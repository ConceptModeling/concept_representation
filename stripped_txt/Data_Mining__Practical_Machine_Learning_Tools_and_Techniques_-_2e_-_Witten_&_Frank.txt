data miningpractical machine learning tools and pm page i the morgan kaufmann series in data management systemsseries editorjim graymicrosoft researchdata mining practical machine learningtools and techniquessecond editionian h.witten and eibe frankfuzzy modeling and genetic algorithms fordata mining and explorationearl coxdata modeling essentialsthird editiongraeme c.simsion and graham c.wittlocation-based servicesjochen schiller and agn s voisarddatabase modeling with microsoft visio forenterprise architectsterry halpinken evanspatrick hallockand bill macleandesigning data-intensive web applicationsstefano ceripiero fraternalialdo bongiomarco brambillasara comaiandmaristella materamining the web discovering knowledgefrom hypertext datasoumen chakrabartiadvanced sql understandingobject-relational and other advancedfeaturesjim meltondatabase tuning principlesexperimentsand troubleshooting techniquesdennis shasha and philippe bonnetsql understanding relationallanguage componentsjim melton and alan r.simoninformation visualization in data miningand knowledge discoveryedited by usama fayyadgeorges g.grinsteinand andreas wiersetransactional information systems theoryalgorithmsand the practice ofconcurrencycontrol and recoverygerhard weikum and gottfried vossenspatial databases with application to gisphilippe rigauxmichel scholland agn svoisardinformation modeling and relationaldatabases from conceptual analysis tological designterry halpincomponent database systemsedited by klaus r.dittrich and andreasgeppertmanaging reference data in enterprisedatabases binding corporate data to thewider worldmalcolm chisholmdata mining concepts and techniquesjiawei han and micheline kamberunderstanding sql and java together aguide to sqljjdbcand relatedtechnologiesjim melton and andrew eisenbergdatabase principlesprogrammingandperformancesecond editionpatrick o neil and elizabeth o neilthe object data standard odmg by r.g.g.cattelldouglas k.barrymark berlerjeffeastmandavidjordancraig russellolafschadowtorsten staniendaand fernando velezdata on the web from relations tosemistructured data and xmlserge abiteboulpeter bunemanand dansuciudata mining practical machine learningtools and techniques with javaimplementationsian h.witten and eibe frankjoe celko s sql for smarties advanced sqlprogrammingsecond editionjoe celkojoe celko s data and databases concepts inpracticejoe celkodeveloping time-oriented databaseapplications in sqlrichard t.snodgrassweb farming for the data warehouserichard d.hackathorndatabase modeling designthird editiontoby j.teoreymanagement ofheterogeneous andautonomous database systemsedited by ahmed elmagarmidmarekrusinkiewiczand amit shethobject-relational dbmss tracking the nextgreat wavesecond editionmichael stonebraker and paul brownwithdorothy moorea complete guide to universaldatabasedon chamberlinuniversal database management a guideto objectrelational technologycynthia maro saraccoreadings in database systemsthird editionedited by michael stonebraker and josephm.hellersteinunderstanding sql s stored procedures acomplete guide to sqlpsmjim meltonprinciples ofmultimedia database systemsv.s.subrahmanianprinciples ofdatabase query processing foradvanced applicationsclement t.yu and weiyi mengadvanced database systemscarlo zaniolostefano cerichristosfaloutsosrichard t.snodgrassv.s.subrahmanianand roberto zicariprinciples oftransaction processing for thesystems professionalphilip a.bernstein and eric newcomerusing the new ibm s object-relationaldatabase systemdon chamberlindistributed algorithmsnancy a.lynchactive database systems triggers and rulesfor advanced database processingedited by jennifer widom and stefano cerimigrating legacy systems gatewaysinterfaces the incremental approachmichael l.brodie and michael stonebrakeratomic transactionsnancy lynchmichael merrittwilliamweihland alan feketequery processing for advanced databasesystemsedited by johann christoph freytagdavidmaierand gottfried vossentransaction processing concepts andtechniquesjim gray and andreas reuterbuilding an object-oriented databasesystem the story by fran ois bancilhonclaudedelobeland paris kanellakisdatabase transaction models for advancedapplicationsedited by ahmed k.elmagarmida guide to developing clientserver sqlapplicationssetrag khoshafianarvola chanannawongand harry k.t.wongthe benchmark handbook for databaseand transaction processing systemssecondeditionedited by jim graycamelot and avalon a distributedtransaction facilityedited by jeffrey l.eppingerlily b.mummertand alfred z.spectorreadings in object-oriented databasesystemsedited by stanley b.zdonik and pm page ii data miningpractical machine learning tools and techniquessecond editionian h. wittendepartment ofcomputer scienceuniversity ofwaikatoeibe frankdepartment ofcomputer scienceuniversity ofwaikatoamsterdam boston heidelberg londonnew york oxford paris san diegosan francisco singapore sydney tokyomorgan kaufmann publishers is an imprint of am page iii publisherdiane cerrapublishing services managersimon crumpproject managerbrandy lillyeditorial assistantasma stephancover designyvo riezebos designcover imagegetty imagescompositionsnp best-set typesetter ltd.hong kongtechnical illustrationdartmouth publishinginc.copyeditorgraphic world inc.proofreadergraphic world inc.indexergraphic world inc.interior printerthe maple-vail book manufacturing groupcover printerphoenix color corpmorgan kaufmann publishers is an imprint sansome streetsuite franciscoca book is printed on acid-free paper. by elsevier inc.all rights reserved.designations used by companies to distinguish their products are often claimed as trademarksor registered trademarks.in all instances in which morgan kaufmann publishers is aware ofaclaimthe product names appear in initial capital or all capital letters.readershowevershouldcontact the appropriate companies for more complete information regarding trademarks andregistration.no part ofthis publication may be reproducedstored in a retrieval systemor transmitted inany form or by any means electronicmechanicalphotocopyingscanningor otherwise without prior written permission ofthe publisher.permissions may be sought directly from elsevier s science technology rights department may also complete your request on-line via the elsevierhomepage by selecting customer support and then obtainingpermissions. library ofcongress cataloging-in-publication datawitteni.h.ian h.data mining machine learning tools and techniques ian h.witteneibe frank. ed.p.cm. kaufmann series in data management systemsincludes bibliographical references and information on all morgan kaufmann publicationsvisit our web site at www.mkp.comor www.books.elsevier.comprinted in the united states together to grow libraries in developing countrieswww.elsevier.com www.bookaid.org pm page iv forewordjim grayseries editormicrosoft researchtechnology now allows us to capture and store vast quantities ofdata.findingpatternstrendsand anomalies in these datasetsand summarizing them with simple quantitative modelsis one ofthe grand challenges ofthe infor-mation age turning data into information and turning information intoknowledge.there has been stunning progress in data mining and machine learning.thesynthesis ofstatisticsmachine learninginformation theoryand computing hascreated a solid sciencewith a firm mathematical baseand with very powerfultools.witten and frank present much ofthis progress in this book and in thecompanion implementation ofthe key algorithms.as suchthis is a milestonein the synthesis ofdata miningdata analysisinformation theoryand machinelearning.ifyou have not been following this field for the last decadethis is agreat way to catch up on this exciting progress.ifyou havethen witten andfrank s presentation and the companion open-source workbenchcalled wekawill be a useful addition to your toolkit.they present the basic theory ofautomatically extracting models from dataand then validating those models.the book does an excellent job ofexplainingthe various models treesassociation ruleslinear modelsclusteringbayes netsneural nets and how to apply them in practice.with this basistheythen walk through the steps and pitfalls ofvarious approaches.they describehow to safely scrub datasetshow to build modelsand how to evaluate a model spredictive quality.most ofthe book is tutorialbut part ii broadly describes howcommercial systems work and gives a tour ofthe publicly available data miningworkbench that the authors provide through a website.this weka workbenchhas a graphical user interface that leads you through data mining tasks and hasexcellent data visualization tools that help understand the models.it is a greatcompanion to the text and a useful and popular tool in its own pm page v this book presents this new discipline in a very accessible formas a text both to train the next generation ofpractitioners and researchers and to informlifelong learners like myself.witten and frank have a passion for simple andelegant solutions.they approach each topic with this mindsetgrounding allconcepts in concrete examplesand urging the reader to consider the simpletechniques firstand then progress to the more sophisticated ones ifthe simpleones prove inadequate.ifyou are interested in databasesand have not been following the machinelearning fieldthis book is a great way to catch up on this exciting progress.ifyou have data that you want to analyze and understandthis book and the asso-ciated weka toolkit are an excellent way to pm page vi contentsforewordvprefacexxiiiupdated and revised contentxxviiacknowledgmentsxxixpart imachine learning tools and s it all mining and machine structural examplesthe weather problem and weather lenses an idealized a classic numeric performance introducing numeric negotiations a more realistic classification a classic machine learning involving and am page vii learning and as the concept mining and concepts instances and s a s in an s in an the the data to know your knowledge with involving for numeric am page viii the basic rudimentary values and numeric values and numeric models for document decision branching algorithmsconstructing versus simple covering versus decision association rules prediction linear classification logistic classification using the classification using distance nearest neighbors distance-based distance am page ix evaluating what s been and data mining loss loss the precision numeric minimum description length the mdl principle to real machine learning error ofdecision tree trees to choices and for choosing valuesnumeric am page x generating good global rules from partial decision with linear maximum margin class vector kernel the number noisy functions for generalized distance the the for model tree from model weighted linear the number em the mixture bayesian am page xi specific structures for fast engineering the input and the attribute numeric discretization versus error-based discrete to numeric useful components to attribute data decision multiple with logistic model output unlabeled for and am page xii on extensions and from massive domain and web data iithe weka machine learning to s in do you use else can you do you get the the data into the a decision the it with things go the and filtering and testing learning it yourself the user a and association attribute instance am page xiii and classifiers for different subset knowledge flow knowledge flow and connecting the an the analyze processing over several am page xiv command-line structure weka.core weka.classifiers machine simple data mining through the new learning example for implementing the am page xv am page xvi list of figuresfigure for the contact lens tree for the contact lens trees for the labor negotiations family tree and two ways ofexpressing the file for the weather a decision tree interactivelya creating arectangular test involving petallengthand petalwidthand resulting decision tree for a simple exclusive-or tree with a replicated for the iris shapes for the cpu performance dataa linear regressionb regression treeand model ways ofpartitioning the instance ways ofrepresenting for stumps for the weather tree stumps for the weather tree for the weather stump for the id algorithma covering the instances and thedecision tree for the same instance space during operation ofa covering for a basic rule regressiona the logit transform and an examplelogistic regression am page xvii figure perceptrona learning rule and representation as a neural winnow algorithma the unbalanced version and the balanced kd-tree for four training instancesa the tree and and a kd-tree to find the nearest neighbor ofthe tree for training instancesa instances and balls andb the out an entire ball based on a target point its current nearest ball treea two cluster centers and their dividing line andb the corresponding hypothetical lift sample roc curves for two learning ofvarying the probability thresholda the error curveand the cost ofsubtree raisingwhere node c is raised tosubsume node the labor negotiations decision for forming rules by incremental algorithm for rule learning and meaning for expanding examples into a partial ofbuilding a partial with exceptions for the iris maximum margin vector regressiona datasets and corresponding versus sigmoida step function and descent using the error function perceptron with a hidden boundary between two rectangular for model tree tree for a dataset with nominal the weather of am page xviii figure clusterings ofthe iris two-class mixture simple bayesian network for the weather bayesian network for the weather weather dataa reduced version and correspondingad space for the weather the temperatureattribute using the result ofdiscretizing the distribution for a two-classtwo-attribute components transform ofa dataseta variance ofeach component and variance ofinternational phone calls from for for for additive logistic option tree for the weather decision tree for the weather explorer dataa spreadsheetb csv formatand weka explorera choosing the explorer interface andb reading in the weather finding it in the classifiers list and from the decision tree the result on the iris dataseta the treeand the classifier object editora the editorb more informationclick moreand choosing a converter a filtera the filtersmenub an object editorandc more information weather data with two attributes the cpu performance data with from the program for numeric the errorsa from and from of am page xix figure on the segmentation data with the user classifiera the data visualizer and the tree a metalearner for boosting decision from the apriori program for association the iris weka s metalearner for discretizationa configuringfilteredclassifierand the menu a bayesian network for the weather data default outputb a version with themaximum number ofparents set to the searchalgorithmand probability distribution table for thewindynode in the parameters for weka s neural-network graphical user selectionspecifying an evaluator and a knowledge flow a data sourcea the right-click menu and the file browser obtained from the configuremenu on the knowledge flow knowledge flow that operates incrementallya theconfiguration and the strip chart experimenta setting it upb the results fileand a spreadsheet with the test results for the experiment in figure up an experiment in advanced and columns offigure row fieldb columnfieldc result ofswapping the row and column selectionsand substituting runfor datasetas javadoca the front page and the class ofthe code for the message code for the decision tree of pm page xx list of tablestable contact lens weather data with some numeric iris cpu performance labor negotiations soybean data as a clustering data with a numeric tree represented as a sister-ofrelation represented in a relation represented as a new iris data for the shapes the attributes in the weather weather data with counts and new numeric weather data with summary new weather data with identification ratio calculations for the tree stumps offigure ofthe contact lens data for which astigmatism ofthe contact lens data for which astigmatism production rate sets for the weather data with coverage or rules for the weather limits for the normal am page xxi table limits for student s distribution with degrees outcomes ofa two-class outcomes ofa three-class predictiona actual andb cost matrixesa a two-class case and a for a lift measures used to evaluate the false positive versus thefalse negative measures for numeric measures for four numeric prediction models in the model a multiclass problem into a two-class onea standard method and error-correcting attribute instance attribute instance algorithms in algorithms in evaluation methods for attribute methods for attribute and evaluation options for learning schemes in options for the decision tree learning schemes in of pm page xxii prefacethe convergence ofcomputing and communication has produced a society thatfeeds on information.yet most ofthe information is in its raw formdata.ifdatais characterized as recorded factsthen informationis the set ofpatternsor expectationsthat underlie the data.there is a huge amount ofinformationlocked up in databases information that is potentially important but has notyet been discovered or articulated.our mission is to bring it forth.data mining is the extraction ofimplicitpreviously unknownand poten-tially useful information from data.the idea is to build computer programs thatsift through databases automaticallyseeking regularities or patterns.strong pat-ternsiffoundwill likely generalize to make accurate predictions on future data.ofcoursethere will be problems.many patterns will be banal and uninterest-ing.others will be spuriouscontingent on accidental coincidences in the par-ticular dataset used.in addition real data is imperfectsome parts will begarbledand some will be missing.anything discovered will be inexacttherewill be exceptions to every rule and cases not covered by any rule.algorithmsneed to be robust enough to cope with imperfect data and to extract regulari-ties that are inexact but useful.machine learning provides the technical basis ofdata mining.it is used toextract information from the raw data in databases information that isexpressed in a comprehensible form and can be used for a variety ofpurposes.the process is one ofabstractiontaking the datawarts and alland inferringwhatever structure underlies it.this book is about the tools and techniques ofmachine learning used in practical data mining for findingand describingstructural patterns in data.as with any burgeoning new technology that enjoys intense commercialattentionthe use ofdata mining is surrounded by a great deal ofhype in thetechnical and sometimes the popular press.exaggerated reports appear ofthe secrets that can be uncovered by setting learning algorithms loose on oceansofdata.but there is no magic in machine learningno hidden am page xxiii alchemy.insteadthere is an identifiable body ofsimple and practical techniquesthat can often extract useful information from raw data.this book describesthese techniques and shows how they work.we interpret machine learning as the acquisition ofstructural descriptionsfrom examples.the kind ofdescriptions found can be used for predictionexplanationand understanding.some data mining applications focus on pre-dictionforecasting what will happen in new situations from data that describewhat happened in the pastoften by guessing the classification ofnew examples.but we are equally perhaps more interested in applications in which theresult of learning is an actual description ofa structure that can be used toclassify examples.this structural description supports explanationunder-standingand prediction.in our experienceinsights gained by the applications users are ofmost interest in the majority ofpractical data mining applicationsindeedthis is one ofmachine learning s major advantages over classical statis-tical modeling.the book explains a variety ofmachine learning methods.some are peda-gogically motivatedsimple schemes designed to explain clearly how the basicideas work.others are practicalreal systems used in applications today.manyare contemporary and have been developed only in the last few years.a comprehensive software resourcewritten in the java languagehas beencreated to illustrate the ideas in the book.called the waikato environment forknowledge analysisor shortit is available as source code on theworld wide web at httpwww.cs.waikato.ac.nzmlweka.it is a fullindustrial-strength implementation ofessentially all the techniques covered in this book.it includes illustrative code and working implementations ofmachine learningmethods.it offers cleanspare implementations ofthe simplest techniquesdesigned to aid understanding ofthe mechanisms involved.it also provides aworkbench that includes fullworkingstate-of-the-art implementations ofmany popular learning schemes that can be used for practical data mining orfor research.finallyit contains a frameworkin the form ofa java class librarythat supports applications that use embedded machine learning and even theimplementation ofnew learning schemes.the objective ofthis book is to introduce the tools and techniques formachine learning that are used in data mining.after reading ityou will under-stand what these techniques are and appreciate their strengths and applicabil-ity.ifyou wish to experiment with your own datayou will be able to do thiseasily with the weka only on the islands ofnew zealandthe wekapronounced to rhyme with meccais a flightless bird with an inquisitive am page xxiv the book spans the gulfbetween the intensely practical approach taken bytrade books that provide case studies on data mining and the more theoreticalprinciple-driven exposition found in current textbooks on machine learning.a briefdescription ofthese books appears in the further readingsection at theend ofchapter this gulfis rather wide.to apply machine learning tech-niques productivelyyou need to understand something about how they workthis is not a technology that you can apply blindly and expect to get good results.different problems yield to different techniquesbut it is rarely obvious whichtechniques are suitable for a given situationyou need to know something aboutthe range ofpossible solutions.we cover an extremely wide range oftechniques.we can do this becauseunlike many trade booksthis volume does not promoteany particular commercial software or approach.we include a large number ofexamplesbut they use illustrative datasets that are small enough to allow youto follow what is going on.real datasets are far too large to show this inany case are usually company confidential.our datasets are chosen not to illustrate actual large-scale practical problems but to help you understand whatthe different techniques dohow they workand what their range ofapplicationis.the book is aimed at the technically aware general reader interested in theprinciples and ideas underlying the current practice ofdata mining.it will also be ofinterest to information professionals who need to become acquaintedwith this new technology and to all those who wish to gain a detailed technicalunderstanding ofwhat machine learning involves.it is written for an eclecticaudience ofinformation systems practitionersprogrammersconsultantsdevelopersinformation technology managersspecification writerspatentexaminersand curious laypeople as well as students and professors whoneed an easy-to-read book with lots ofillustrations that describes what themajor machine learning techniques arewhat they dohow they are usedandhow they work.it is practically orientedwith a strong how to flavorandincludes algorithmscodeand implementations.all those involved in practicaldata mining will benefit directly from the techniques described.the book isaimed at people who want to cut through to the reality that underlies the hypeabout machine learning and who seek a practicalnonacademicunpretentiousapproach.we have avoided requiring any specific theoretical or mathematicalknowledge except in some sections marked by a light gray bar in the margin.these contain optional materialoften for the more technical or theoreticallyinclined readerand may be skipped without loss ofcontinuity.the book is organized in layers that make the ideas accessible to readers whoare interested in grasping the basics and to those who would like more depth oftreatmentalong with full details on the techniques covered.we believe that con-sumers ofmachine learning need to have some idea ofhow the algorithms theyuse work.it is often observed that data models are only as good as the pm page xxv who interprets themand that person needs to know something about how themodels are produced to appreciate the strengthsand limitationsofthe tech-nology.howeverit is not necessary for all data model users to have a deepunderstanding ofthe finer details ofthe algorithms.we address this situation by describing machine learning methods at succes-sive levels ofdetail.you will learn the basic ideasthe topmost levelby readingthe first three chapters.chapter describesthrough exampleswhat machinelearning is and where it can be usedit also provides actual practical applica-tions.chapters and cover the kinds ofinput and output or knowledge representation involved.different kinds ofoutput dictate different styles ofalgorithmand at the next level chapter describes the basic methods ofmachine learningsimplified to make them easy to comprehend.here the prin-ciples involved are conveyed in a variety ofalgorithms without getting into intricate details or tricky implementation issues.to make progress in the appli-cation ofmachine learning techniques to particular data mining problemsit isessential to be able to measure how well you are doing.chapter can beread out ofsequenceequips you to evaluate the results obtained from machinelearningaddressing the sometimes complex issues involved in performanceevaluation.at the lowest and most detailed levelchapter exposes in naked detail thenitty-gritty issues ofimplementing a spectrum ofmachine learning algorithmsincluding the complexities necessary for them to work well in practice.althoughmany readers may want to ignore this detailed informationit is at this level thatthe fullworkingtested implementations ofmachine learning schemes in wekaare written.chapter describes practical topics involved with engineering theinput to machine learning for exampleselecting and discretizing attributes and covers several more advanced techniques for refining and combining theoutput from different learning techniques.the final chapter ofpart i looks tothe future.the book describes most methods used in practical machine learning.howeverit does not cover reinforcement learningbecause it is rarely appliedin practical data mininggenetic algorithm approachesbecause these are justan optimization techniqueor relational learning and inductive logic program-mingbecause they are rarely used in mainstream data mining applications.the data mining system that illustrates the ideas in the book is described inpart ii to clearly separate conceptual material from the practical aspects ofhowto use it.you can skip to part ii directly from chapter ifyou are in a hurry toanalyze your data and don t want to be bothered with the technical details.java has been chosen for the implementations ofmachine learning tech-niques that accompany this book becauseas an object-oriented programminglanguageit allows a uniform interface to learning schemes and methods for pre-and postprocessing.we have chosen java instead ofcsmalltalkor am page xxvi object-oriented languages because programs written in java can be run onalmost any computer without having to be recompiledhaving to undergo com-plicated installation proceduresor worst ofall having to change the code.a java program is compiled into byte-code that can be executed on any com-puter equipped with an appropriate interpreter.this interpreter is called thejava virtual machine.java virtual machines andfor that matterjava compil-ers are freely available for all important platforms.like all widely used programming languagesjava has received its share ofcriticism.although this is not the place to elaborate on such issuesin severalcases the critics are clearly right.howeverofall currently available program-ming languages that are widely supportedstandardizedand extensively docu-mentedjava seems to be the best choice for the purpose ofthis book.its maindisadvantage is speed ofexecution or lack ofit.executing a java program isseveral times slower than running a corresponding program written in c lan-guage because the virtual machine has to translate the byte-code into machinecode before it can be executed.in our experience the difference is a factor ofthree to five ifthe virtual machine uses a just-in-time compiler.instead oftrans-lating each byte-code individuallya just-in-time compilertranslates wholechunks ofbyte-code into machine codethereby achieving significant speedup.howeverifthis is still to slow for your applicationthere are compilers thattranslate java programs directly into machine codebypassing the byte-codestep.this code cannot be executed on other platformsthereby sacrificing oneofjava s most important advantages.updated and revised contentwe finished writing the first edition ofthis book in and nowin april just polishing this second edition.the areas ofdata mining and machinelearning have matured in the intervening years.although the core ofmaterialin this edition remains the samewe have made the most ofour opportunity toupdate it to reflect the changes that have taken place over years.there havebeen errors to fixerrors that we had accumulated in our publicly available erratafile.surprisingly few were foundand we hope there are even fewer in thissecond edition.the errata for the second edition may be found through thebook s home page at httpwww.cs.waikato.ac.nzmlwekabook.html. we havethoroughly edited the material and brought it up to dateand we practicallydoubled the number ofreferences.the most enjoyable part has been addingnew material.here are the highlights.bowing to popular demandwe have added comprehensive information onneural networksthe perceptron and closely related winnow algorithm insection and the multilayer perceptron and backpropagation algorithm am page xxvii in section have included more recent material on implementing nonlinear decision boundaries using both the kernel perceptron and radial basisfunction networks.there is a new section on bayesian networksagain inresponse to readers requestswith a description ofhow to learn classifiers basedon these networks and how to implement them efficiently using all-dimensionstrees.the weka machine learning workbench that accompanies the booka widelyused and popular feature ofthe first editionhas acquired a radical new look inthe form ofan interactive interface or ratherthree separate interactive inter-faces that make it far easier to use.the primary one is the explorerwhichgives access to all ofweka s facilities using menu selection and form filling.theothers are the knowledge flow interfacewhich allows you to design configu-rations for streamed data processingand the experimenterwith which you setup automated experiments that run selected machine learning algorithms withdifferent parameter settings on a corpus ofdatasetscollect performance statis-ticsand perform significance tests on the results.these interfaces lower the barfor becoming a practicing data minerand we include a full description ofhowto use them.howeverthe book continues to stand aloneindependent ofwekaand to underline this we have moved all material on the workbench into a sep-arate part ii at the end ofthe book.in addition to becoming far easier to useweka has grown over the last and matured enormously in its data mining capabilities.it now includesan unparalleled range ofmachine learning algorithms and related techniques.the growth has been partly stimulated by recent developments in the field andpartly led by weka users and driven by demand.this puts us in a position inwhich we know a great deal about what actual users ofdata mining wantandwe have capitalized on this experience when deciding what to include in thisnew edition.the earlier chapterscontaining more general and foundational materialhave suffered relatively little change.we have added more examples offieldedapplications to chapter new subsection on sparse data and a little on stringattributes and date attributes to chapter a description ofinteractive deci-sion tree constructiona useful and revealing technique to help you grapple withyour data using manually built decision treesto chapter addition to introducing linear decision boundaries for classificationtheinfrastructure for neural networkschapter includes new material on multi-nomial bayes models for document classification and on logistic regression.thelast years have seen great interest in data mining for textand this is reflectedin our introduction to string attributes in chapter bayes for doc-ument classification in chapter text transformations in chapter includes a great deal ofnew material on efficient data structures forsearching the instance spacekd-trees and the recently invented ball am page xxviii are used to find nearest neighbors efficiently and to accelerate distance-basedclustering.chapter describes the principles ofstatistical evaluation ofmachine learn-ingwhich have not changed.the main additionapart from a note on the kappastatistic for measuring the success ofa predictoris a more detailed treatmentofcost-sensitive learning.we describe how to use a classifierbuilt withouttaking costs into considerationto make predictions that are sensitive to costalternativelywe explain how to take costs into account during the trainingprocess to build a cost-sensitive model.we also cover the popular new tech-nique ofcost curves.there are several additions to chapter from the previously men-tioned material on neural networks and bayesian network classifiers.moredetails gory details are given ofthe heuristics used in the successful ripperrule learner.we describe how to use model trees to generate rules for numericprediction.we show how to apply locally weighted regression to classificationproblems.finallywe describe the x-means clustering algorithmwhich is a bigimprovement on traditional k-means.chapter on engineering the input and output has changed mostbecausethis is where recent developments in practical machine learning have been con-centrated.we describe new attribute selection schemes such as race search andthe use ofsupport vector machines and new methods for combining modelssuch as additive regressionadditive logistic regressionlogistic model treesandoption trees.we give a full account oflogitboost was mentioned in thefirst edition but not described.there is a new section on useful transforma-tionsincluding principal components analysis and transformations for textmining and time series.we also cover recent developments in using unlabeleddata to improve classificationincluding the co-training and co-em methods.the final chapter ofpart i on new directions and different perspectives hasbeen reworked to keep up with the times and now includes contemporary chal-lenges such as adversarial learning and ubiquitous data mining.acknowledgmentswriting the acknowledgments is always the nicest part! a lot ofpeople havehelped usand we relish this opportunity to thank them.this book has arisenout ofthe machine learning research project in the computer science depart-ment at the university ofwaikatonew zealand.we have received generousencouragement and assistance from the academic staffmembers on that projectjohn clearysally jo cunninghammatt humphreylyn huntbob mcqueenlloyd smithand tony smith.special thanks go to mark hallbernhardpfahringerand above all geoffholmesthe project leader and source am page xxix ration.all who have worked on the machine learning project here have con-tributed to our thinkingwe would particularly like to mention steve garnerstuart inglisand craig nevill-manning for helping us to get the project offtheground in the beginning when success was less certain and things were moredifficult.the weka system that illustrates the ideas in this book forms a crucial com-ponent ofit.it was conceived by the authors and designed and implemented byeibe frankalong with len trigg and mark hall.many people in the machinelearning laboratory at waikato made significant contributions.since the firstedition ofthe book the weka team has expanded considerablyso many peoplehave contributed that it is impossible to acknowledge everyone properly.we aregrateful to remco bouckaert for his implementation ofbayesian networksdalefletcher for many database-related aspectsashrafkibriya and richard kirkbyfor contributions far too numerous to listniels landwehr for logistic modeltreesabdelaziz mahoui for the implementation ofkstefan mutter for asso-ciation rule mininggabi schmidberger and malcolm ware for numerous mis-cellaneous contributionstony voyle for least-median-of-squares regressionyong wang for pace regression and the implementation xin xu forjriplogistic regressionand many other contributions.our sincere thanks goto all these people for their dedicated work and to the many contributors toweka from outside our group at waikato.tucked away as we are in a remote very pretty corner ofthe southernhemispherewe greatly appreciate the visitors to our department who play a crucial role in acting as sounding boards and helping us to develop our thinking.we would like to mention in particular rob holtecarl gutwinandrussell bealeeach ofwhom visited us for several monthsdavid ahawhoalthough he only came for a few days did so at an early and fragile stage oftheproject and performed a great service by his enthusiasm and encouragementand kai ming tingwho worked with us for years on many ofthe topicsdescribed in chapter and helped to bring us into the mainstream ofmachinelearning.students at waikato have played a significant role in the development oftheproject.jamie littin worked on ripple-down rules and relational learning.brentmartin explored instance-based learning and nested instance-based representa-tions.murray fife slaved over relational learningand nadeeka madapathageinvestigated the use offunctional languages for expressing machine learningalgorithms.other graduate students have influenced us in numerous wayspar-ticularly gordon paynteryingying wenand zane braywho have worked withus on text mining.colleagues steve jones and malika mahoui have also madefar-reaching contributions to these and other machine learning projects.morerecently we have learned much from our many visiting students from freiburgincluding peter reutemann and nils am page xxx ian witten would like to acknowledge the formative role ofhis former stu-dents at calgaryparticularly brent krawchukdave maulsbythong phanandtanja mitrovicall ofwhom helped him develop his early ideas in machinelearningas did faculty members bruce macdonaldbrian gainesand davidhill at calgary and john andreae at the university ofcanterbury.eibe frank is indebted to his former supervisor at the university ofkarlsruheklaus-peter huber with sas institutewho infected him withthe fascination ofmachines that learn.on his travels eibe has benefited frominteractions with peter turneyjoel martinand berry de bruijn in canada andwith luc de raedtchristoph helmakristian kerstingstefan kramerulrichr ckertand ashwin srinivasan in germany.diane cerra and asma stephan ofmorgan kaufmann have worked hard toshape this bookand lisa royseour production editorhas made the processgo smoothly.bronwyn webster has provided excellent support at the waikatoend.we gratefully acknowledge the unsung efforts ofthe anonymous reviewersone ofwhom in particular made a great number ofpertinent and constructivecomments that helped us to improve this book significantly.in additionwewould like to thank the librarians ofthe repository ofmachine learning data-bases at the university ofcaliforniairvinewhose carefully collected datasetshave been invaluable in our research.our research has been funded by the new zealand foundation for researchscience and technology and the royal society ofnew zealand marsden fund.the department ofcomputer science at the university ofwaikato has gener-ously supported us in all sorts ofwaysand we owe a particular debt ofgratitude to mark apperley for his enlightened leadership and warm encour-agement.part ofthe first edition was written while both authors were visitingthe university ofcalgarycanadaand the support ofthe computer sciencedepartment there is gratefully acknowledged as well as the positive and helpfulattitude ofthe long-suffering students in the machine learning course on whomwe experimented.in producing the second edition ian was generously supported by canada sinformatics circle ofresearch excellence and by the university oflethbridgein southern albertawhich gave him what all authors yearn for a quiet spacein pleasant and convivial surroundings in which to work.lastand most ofallwe are grateful to our families and partners.pamannaand nikki were all too well aware ofthe implications ofhaving an author in thehouse not again! but let ian go ahead and write the book anyway.julie wasalways supportiveeven when eibe had to burn the midnight oil in the machinelearning laband immo and ollig provided exciting diversions.between us wehail from canadaenglandgermanyirelandand samoanew zealand hasbrought us together and provided an idealeven idyllicplace to do this am page xxxi am page xxxii partimachine learning tools and am page am page human in vitrofertilization involves collecting several eggs from a woman sovarieswhichafter fertilization with partner or donor spermproduce severalembryos.some ofthese are selected and transferred to the woman s uterus.theproblem is to select the best embryos to use the ones that are most likely tosurvive.selection is based on around recorded features ofthe embryos characterizing their morphologyoocytefollicleand the sperm sample.thenumber offeatures is sufficiently large that it is difficult for an embryologist toassess them all simultaneously and correlate historical data with the crucialoutcome ofwhether that embryo did or did not result in a live child.in aresearch project in englandmachine learning is being investigated as a tech-nique for making the selectionusing as training data historical records ofembryos and their outcome.every yeardairy farmers in new zealand have to make a tough business deci-sionwhich cows to retain in their herd and which to sell offto an abattoir.typi-callyone-fifth ofthe cows in a dairy herd are culled each year near the end ofthe milking season as feed reserves dwindle.each cow s breeding and milk s it all am page duction history influences this decision.other factors include age cow isnearing the end ofits productive life at yearshealth problemshistory ofdif-ficult calvingundesirable temperament traits or jumping fencesandnot being in calffor the following season.about attributes for each ofseveral million cows have been recorded over the years.machine learning isbeing investigated as a way ofascertaining what factors are taken into accountby successful farmers not to automate the decision but to propagate their skillsand experience to others.life and death.from europe to the antipodes.family and business.machinelearning is a burgeoning new technology for mining knowledge from dataatechnology that a lot ofpeople are starting to take mining and machine learningwe are overwhelmed with data.the amount ofdata in the worldin our livesseems to go on and on increasing and there s no end in sight.omnipresentpersonal computers make it too easy to save things that previously we wouldhave trashed.inexpensive multigigabyte disks make it too easy to postpone deci-sions about what to do with all this stuff we simply buy another disk and keepit all.ubiquitous electronics record our decisionsour choices in the super-marketour financial habitsour comings and goings.we swipe our way throughthe worldevery swipe a record in a database.the world wide web overwhelmsus with informationmeanwhileevery choice we make is recorded.and all theseare just personal choicesthey have countless counterparts in the world ofcom-merce and industry.we would all testify to the growing gap between the gener-ationofdata and our understandingofit.as the volume ofdata increasesinexorablythe proportion ofit that people understand decreasesalarmingly.lying hidden in all this data is informationpotentially useful informationthatis rarely made explicit or taken advantage of.this book is about looking for patterns in data.there is nothing new aboutthis.people have been seeking patterns in data since human life began.huntersseek patterns in animal migration behaviorfarmers seek patterns in cropgrowthpoliticians seek patterns in voter opinionand lovers seek patterns intheir partners responses.a scientist s job a baby s is to make sense ofdatato discover the patterns that govern how the physical world works and encap-sulate them in theories that can be used for predicting what will happen in newsituations.the entrepreneur s job is to identify opportunitiesthat ispatternsin behavior that can be turned into a profitable businessand exploit them.in data miningthe data is stored electronically and the search is automated or at least augmented by computer.even this is not particularly new.econo-mistsstatisticiansforecastersand communication engineers have long s it all am page with the idea that patterns in data can be sought automaticallyidentifiedvalidatedand used for prediction.what is new is the staggering increase inopportunities for finding patterns in data.the unbridled growth ofdatabasesin recent yearsdatabases on such everyday activities as customer choicesbringsdata mining to the forefront ofnew business technologies.it has been estimatedthat the amount ofdata stored in the world s databases doubles every although it would surely be difficult to justify this figure in anyquantitative sensewe can all relate to the pace ofgrowth qualitatively.as theflood ofdata swells and machines that can undertake the searching becomecommonplacethe opportunities for data mining increase.as the world growsin complexityoverwhelming us with the data it generatesdata mining becomesour only hope for elucidating the patterns that underlie it.intelligently analyzeddata is a valuable resource.it can lead to new insights andin commercial set-tingsto competitive advantages.data mining is about solving problems by analyzing data already present indatabases.supposeto take a well-worn examplethe problem is fickle customerloyalty in a highly competitive marketplace.a database ofcustomer choicesalong with customer profilesholds the key to this problem.patterns ofbehavior offormer customers can be analyzed to identify distinguishing charac-teristics ofthose likely to switch products and those likely to remain loyal.oncesuch characteristics are foundthey can be put to work to identify present cus-tomers who are likely to jump ship.this group can be targeted for special treat-menttreatment too costly to apply to the customer base as a whole.morepositivelythe same techniques can be used to identify customers who might beattracted to another service the enterprise providesone they are not presentlyenjoyingto target them for special offers that promote this service.in today shighly competitivecustomer-centeredservice-oriented economydata is theraw material that fuels business growth ifonly it can be mined.data mining is defined as the process ofdiscovering patterns in data.theprocess must be automatic or usually semiautomatic.the patterns discovered must be meaningful in that they lead to some advantageusually an economic advantage.the data is invariably present in substantial quantities.how are the patterns expressed? useful patterns allow us to make nontrivialpredictions on new data.there are two extremes for the expression ofa patternas a black box whose innards are effectively incomprehensible and as a trans-parent box whose construction reveals the structure ofthe pattern.bothwe areassumingmake good predictions.the difference is whether or not the patternsthat are mined are represented in terms ofa structure that can be examinedreasoned aboutand used to inform future decisions.such patterns we call struc-turalbecause they capture the decision structure in an explicit way.in otherwordsthey help to explain something about the mining and machine am page nowfinallywe can say what this book is about.it is about techniques forfinding and describing structural patterns in data.most ofthe techniques thatwe cover have developed within a field known as machine learning.but first letus look at what structural patterns are.describing structural patternswhat is meant by structural patterns?how do you describe them? and whatform does the input take? we will answer these questions by way ofillustrationrather than by attempting formaland ultimately steriledefinitions.there willbe plenty ofexamples later in this chapterbut let s examine one right now toget a feeling for what we re talking about.look at the contact lens data in table gives the conditions underwhich an optician might want to prescribe soft contact lenseshard contactlensesor no contact lenses at allwe will say more about what the s it all about?table contact lens data.spectacletear am page features mean later.each line ofthe table is one ofthe examples.part ofa struc-tural description ofthis information might be as followsif tear production rate then recommendation if age and astigmatic then recommendation descriptions need not necessarily be couched as rules such as these.decision treeswhich specify the sequences ofdecisions that need to be madeand the resulting recommendationare another popular means ofexpression.this example is a very simplistic one.firstall combinations ofpossiblevalues are represented in the table.there are rowsrepresenting three possi-ble values ofage and two values each for spectacle prescriptionastigmatismand tear production rate rules do not really general-ize from the datathey merely summarize it.in most learning situationsthe setofexamples given as input is far from completeand part ofthe job is to gen-eralize to othernew examples.you can imagine omitting some ofthe rows inthe table for which tear production rate is reducedand still coming up with theruleif tear production rate then recommendation would generalize to the missing rows and fill them in correctly.secondvalues are specified for all the features in all the examples.real-life datasetsinvariably contain examples in which the values ofsome featuresfor somereason or otherare unknown for examplemeasurements were not taken orwere lost.thirdthe preceding rules classify the examples correctlywhereasoftenbecause oferrors or noisein the datamisclassifications occur even on thedata that is used to train the classifier.machine learningnow that we have some idea about the inputs and outputslet s turn to machinelearning.what is learninganyway? what is machine learning? these are philo-sophic questionsand we will not be much concerned with philosophy in thisbookour emphasis is firmly on the practical.howeverit is worth spending afew moments at the outset on fundamental issuesjust to see how tricky theyarebefore rolling up our sleeves and looking at machine learning in practice.our dictionary defines to learn as followsto get knowledge ofby studyexperienceor being taughtto become aware by information or from observationto commit to memoryto be informed ofascertainto receive mining and machine am page these meanings have some shortcomings when it comes to talking about com-puters.for the first twoit is virtually impossible to test whether learning hasbeen achieved or not.how do you know whether a machine has got knowledgeofsomething? you probably can t just ask it questionseven ifyou couldyouwouldn t be testing its ability to learn but would be testing its ability to answerquestions.how do you know whether it has become aware ofsomething? thewhole question ofwhether computers can be awareor consciousis a burningphilosophic issue.as for the last three meaningsalthough we can see what theydenote in human termsmerely committing to memory and receivinginstruction seem to fall far short ofwhat we might mean by machine learning.they are too passiveand we know that computers find these tasks trivial.insteadwe are interested in improvements in performanceor at least in thepotential for performancein new situations.you can commit something tomemory or be informed ofsomething by rote learning without being able toapply the new knowledge to new situations.you can receive instruction withoutbenefiting from it at all.earlier we defined data mining operationally as the process ofdiscoveringpatternsautomatically or semiautomaticallyin large quantities ofdata andthe patterns must be useful.an operational definition can be formulated in thesame way for learningthings learn when they change their behavior in a way that makes themperform better in the future.this ties learning to performance rather than knowledge.you can test learningby observing the behavior and comparing it with past behavior.this is a muchmore objective kind ofdefinition and appears to be far more satisfactory.but there s still a problem.learning is a rather slippery concept.lots ofthingschange their behavior in ways that make them perform better in the futureyetwe wouldn t want to say that they have actually learned.a good example is acomfortable slipper.has it learnedthe shape ofyour foot? it has certainlychanged its behavior to make it perform better as a slipper! yet we would hardlywant to call this learning.in everyday languagewe often use the word train-ing to denote a mindless kind oflearning.we train animals and even plantsalthough it would be stretching the word a bit to talk oftraining objects suchas slippers that are not in any sense alive.but learning is different.learningimplies thinking.learning implies purpose.something that learns has to do sointentionally.that is why we wouldn t say that a vine has learned to grow rounda trellis in a vineyard we d say it has been trained.learning without purposeis merely training.ormore to the pointin learning the purpose is the learner swhereas in training it is the teacher s.thus on closer examination the second definition oflearningin operationalperformance-oriented termshas its own problems when it comes to talking s it all am page computers.to decide whether something has actually learnedyou need to seewhether it intended to or whether there was any purpose involved.that makesthe concept moot when applied to machines because whether artifacts can behavepurposefully is unclear.philosophic discussions ofwhat is really meant by learn-ing like discussions ofwhat is really meant by intention or purpose arefraught with difficulty.even courts oflaw find intention hard to grapple with.data miningfortunatelythe kind oflearning techniques explained in this book do notpresent these conceptual problems they are called machine learning withoutreally presupposing any particular philosophic stance about what learning actu-ally is.data mining is a practical topic and involves learning in a practicalnota theoreticalsense.we are interested in techniques for finding and describingstructural patterns in data as a tool for helping to explain that data and makepredictions from it.the data will take the form ofa set ofexamples examplesofcustomers who have switched loyaltiesfor instanceor situations in whichcertain kinds ofcontact lenses can be prescribed.the output takes the form ofpredictions about new examples a prediction ofwhether a particular customerwill switch or a prediction ofwhat kind oflens will be prescribed under givencircumstances.but because this book is about finding and describingpatternsin datathe output may also include an actual description ofa structure thatcan be used to classify unknown examples to explain the decision.as well asperformanceit is helpful to supply an explicit representation ofthe knowledgethat is acquired.in essencethis reflects both definitions oflearning consideredpreviouslythe acquisition ofknowledge and the ability to use it.many learning techniques look for structural descriptions ofwhat is learneddescriptions that can become fairly complex and are typically expressed as setsofrules such as the ones described previously or the decision trees describedlater in this chapter.because they can be understood by peoplethese descrip-tions serve to explain what has been learned and explain the basis for new pre-dictions.experience shows that in many applications ofmachine learning todata miningthe explicit knowledge structures that are acquiredthe structuraldescriptionsare at least as importantand often very much more importantthan the ability to perform well on new examples.people frequently use datamining to gain knowledgenot just predictions.gaining knowledge from datacertainly sounds like a good idea ifyou can do it.to find out howread examples the weather problem and otherswe use a lot ofexamples in this bookwhich seems particularly appropriate con-sidering that the book is all about learning from examples! there are examplesthe weather problem and am page standard datasets that we will come back to repeatedly.different datasets tendto expose new issues and challengesand it is interesting and instructive to havein mind a variety ofproblems when considering learning methods.in facttheneed to work with different datasets is so important that a corpus containingaround example problems has been gathered together so that different algo-rithms can be tested and compared on the same set ofproblems.the illustrations in this section are all unrealistically simple.serious appli-cation ofdata mining involves thousandshundreds ofthousandsor even mil-lions ofindividual cases.but when explaining what algorithms do and how theyworkwe need simple examples that capture the essence ofthe problem but aresmall enough to be comprehensible in every detail.we will be working with theillustrations in this section throughout the bookand they are intended to be academic in the sense that they will help us to understand what is going on.some actual fielded applications oflearning techniques are discussed in many more are covered in the books mentioned in the further readingsection at the end ofthe chapter.another problem with actual real-life datasets is that they are often propri-etary.no one is going to share their customer and product choice database withyou so that you can understand the details oftheir data mining application andhow it works.corporate data is a valuable assetone whose value has increasedenormously with the development ofdata mining techniques such as thosedescribed in this book.yet we are concerned here with understanding how themethods used for data mining work and understanding the details ofthesemethods so that we can trace their operation on actual data.that is why ourillustrations are simple ones.but they are not simplisticthey exhibit the fea-tures ofreal datasets.the weather problemthe weather problem is a tiny dataset that we will use repeatedly to illustratemachine learning methods.entirely fictitiousit supposedly concerns the con-ditions that are suitable for playing some unspecified game.in generalinstancesin a dataset are characterized by the values offeaturesor attributesthat measuredifferent aspects ofthe instance.in this case there are four attributesoutlooktemperaturehumidityand windy.the outcome is whether to play or not.in its simplest formshown in table four attributes have values thatare symbolic categories rather than numbers.outlook can be sunnyovercastorrainytemperature can be hotmildor coolhumidity can be highor normaland windy can be trueor false.this creates possible combinations are present in the set ofinput examples.a set ofrules learned from this information not necessarily a very goodone might look as s it all am page if outlook and humidity then play outlook and windy play outlook play humidity play none of the abovethen play rules are meant to be interpreted in orderthe first onethen ifit doesn tapply the secondand so on.a set ofrules that are intended to be interpretedin sequence is called a decision list.interpreted as a decision listthe rules correctly classify all ofthe examples in the tablewhereas taken individuallyoutofcontextsome ofthe rules are incorrect.for examplethe rule if humidity then play yesgets one ofthe examples wrong which one.the meaning ofa set ofrules depends on how it is interpreted not surprisingly!in the slightly more complex form shown in table ofthe attributes temperature and humidity have numeric values.this means that any learn-ing method must create inequalities involving these attributes rather thansimple equality testsas in the former case.this is called a numeric-attributeproblem in this casea mixed-attribute problembecause not all attributes arenumeric.now the first rule given earlier might take the following formif outlook and humidity then play slightly more complex process is required to come up with rules that involvenumeric examplesthe weather problem and weather am page the rules we have seen so far are classification rulesthey predict the classifi-cation ofthe example in terms ofwhether to play or not.it is equally possibleto disregard the classification and just look for any rules that strongly associatedifferent attribute values.these are called association rules.many associationrules can be derived from the weather data in table good ones are asfollowsif temperature humidity humidity and windy then play outlook and play humidity windy and play outlook humidity these rules are correct on the given datathey make no false predic-tions.the first two apply to four examples in the datasetthe third to threeexamplesand the fourth to two examples.there are many other rulesin factnearly association rules can be found that apply to two or more examples ofthe weather data and are completely correct on this data.ifyou look for rulesthat are less than correctthen you will find many more.there are somany because unlike classification rulesassociation rules can predict any ofthe attributesnot just a specified classand can even predict more than onething.for examplethe fourth rule predicts both that outlookwill be sunnyandthat humiditywill be s it all about?table data with some numeric am page contact lenses an idealized problemthe contact lens data introduced earlier tells you the kind ofcontact lens to pre-scribegiven certain information about a patient.note that this example isintended for illustration onlyit grossly oversimplifies the problem and shouldcertainly not be used for diagnostic purposes!the first column oftable gives the age ofthe patient.in case you re won-deringpresbyopiais a form oflongsightedness that accompanies the onset ofmiddle age.the second gives the spectacle prescriptionmyopemeans short-sighted and hypermetropemeans longsighted.the third shows whether thepatient is astigmaticand the fourth relates to the rate oftear productionwhichis important in this context because tears lubricate contact lenses.the finalcolumn shows which kind oflenses to prescribehardsoftor none.all possi-ble combinations ofthe attribute values are represented in the table.a sample set ofrules learned from this information is shown in figure is a rather large set ofrulesbut they do correctly classify all the examples.these rules are complete and deterministicthey give a unique prescription forevery conceivable example.generallythis is not the case.sometimes there aresituations in which no rule appliesother times more than one rule may applyresulting in conflicting recommendations.sometimes probabilities or examplesthe weather problem and tear production rate reduced then recommendation noneif age young and astigmatic no and tear production rate normal then recommendation softif age pre-presbyopic and astigmatic no and tear production rate normal then recommendation softif age presbyopic and spectacle prescription myope and astigmatic no then recommendation noneif spectacle prescription hypermetrope and astigmatic no and tear production rate normal then recommendation softif spectacle prescription myope and astigmatic yes and tear production rate normal then recommendation hardif age young and astigmatic yes and tear production rate normal then recommendation hardif age pre-presbyopic and spectacle prescription hypermetrope and astigmatic yes then recommendation noneif age presbyopic and spectacle prescription hypermetrope and astigmatic yes then recommendation for the contact lens am page may be associated with the rules themselves to indicate that some are moreimportantor more reliablethan others.you might be wondering whether there is a smaller rule set that performs aswell.ifsowould you be better offusing the smaller rule set andifsowhy?these are exactly the kinds ofquestions that will occupy us in this book.becausethe examples form a complete set for the problem spacethe rules do no morethan summarize all the information that is givenexpressing it in a different andmore concise way.even though it involves no generalizationthis is often a veryuseful thing to do! people frequently use machine learning techniques to gaininsight into the structure oftheir data rather than to make predictions for newcases.in facta prominent and successful line ofresearch in machine learningbegan as an attempt to compress a huge database ofpossible chess endgamesand their outcomes into a data structure ofreasonable size.the data structurechosen for this enterprise was not a set ofrules but a decision tree.figure shows a structural description for the contact lens data in the formofa decision treewhich for many purposes is a more concise and perspicuousrepresentation ofthe rules and has the advantage that it can be visualized moreeasily.howeverthis decision tree in contrast to the rule set given in classifies two examples incorrectly. the tree calls first for a test on tearproduction rateand the first two branches correspond to the two possible out-comes.iftear production rateis reducedthe left branchthe outcome is none.ifit is normalthe right brancha second test is madethis time on astigma-tism.eventuallywhatever the outcome ofthe testsa leafofthe tree is s it all about?normaltear production ratereducedhypermetropemyopenoneastigmatismsofthardnonespectacle tree for thecontact lens am page that dictates the contact lens recommendation for that case.the question ofwhat is the most natural and easily understood format for the output from amachine learning scheme is one that we will return to in chapter a classic numeric datasetthe iris datasetwhich dates back to seminal work by the eminent statisticianr.a.fisher in the and is arguably the most famous dataset used indata miningcontains examples each ofthree types ofplantiris setosairisversicolorand iris virginica.it is excerpted in table are four attrib-utessepal lengthsepal widthpetal lengthand petal widthall measured in cen-timeters.unlike previous datasetsall attributes have values that are numeric.the following set ofrules might be learned from this datasetif petal length then iris setosaif sepal width then iris versicolorif sepal width and petal length then iris versicolorif sepal width and petal width then iris versicolorif petal length and petal length then iris versicolorif sepal length and petal length then iris examplesthe weather problem and iris data.sepal sepal widthpetal length petal widthlength am page if sepal width and petal length and petal width then iris versicolorif petal length and petal length and petal width then iris versicolorif sepal length and petal length then iris versicolorif sepal width and petal width and sepal length then iris versicolorif sepal length and sepal length and petal length then iris versicolorif petal length then iris virginicaif petal width then iris virginicaif petal width and sepal width then iris virginicaif petal length and petal width then iris virginicathese rules are very cumbersomeand we will see in chapter how morecompact rules can be expressed that convey the same information.cpu performance introducing numeric predictionalthough the iris dataset involves numeric attributesthe outcome the type ofiris is a categorynot a numeric value.table shows some data for whichthe outcome and the attributes are numeric.it concerns the relative perform-ance ofcomputer processing power on the basis ofa number ofrelevant attributeseach row represents different computer configurations.the classic way ofdealing with continuous prediction is to write the outcomeas a linear sum ofthe attribute values with appropriate weightsfor s it all about?table cpu performance data.maincyclememory am page prp myct mmin mmax cach chmin chmax.the abbreviated variable names are given in the second row ofthe table. thisis called a regression equationand the process ofdetermining the weights iscalled regressiona well-known procedure in statistics that we will review inchapter basic regression method is incapable ofdiscoveringnonlinear relationships variants do exist indeedone will bedescribed in section in chapter we will examine different represen-tations that can be used for predicting numeric quantities.in the iris and central processing unit performance dataall the attributes have numeric values.practical situations frequently present a mixtureofnumeric and nonnumeric attributes.labor negotiations a more realistic examplethe labor negotiations dataset in table summarizes the outcome ofcana-dian contract negotiations in and includes all collective agreementsreached in the business and personal services sector for organizations with atleast members staffpoliceetc..each case con-cerns one contractand the outcome is whether the contract is deemed accept-ableor unacceptable.the acceptable contracts are ones in which agreementswere accepted by both labor and management.the unacceptable ones are eitherknown offers that fell through because one party would not accept them oracceptable contracts that had been significantly perturbed to the extent thatinthe view ofexpertsthey would not have been accepted.there are examples in the dataset another which are normallyreserved for test purposes.unlike the other tables heretable presents theexamples as columns rather than as rowsotherwiseit would have to bestretched over several pages.many ofthe values are unknown or missingasindicated by question marks.this is a much more realistic dataset than the others we have seen.it con-tains many missing valuesand it seems unlikely that an exact classification canbe obtained.figure shows two decision trees that represent the dataset.figure simple and approximateit doesn t represent the data exactly.for exampleitwill predict badfor some contracts that are actually marked good.but it doesmake intuitive sensea contract is bad the employee! ifthe wage increasein the first year is too small than first-year wage increase islarger than thisit is good ifthere are lots ofstatutory holidays than ifthere are fewer statutory holidaysit is good ifthe first-year wageincrease is large enough than examplesthe weather problem and am page figure is a more complex decision tree that represents the samedataset.in factthis is a more accurate representation ofthe actual dataset thatwas used to create the tree.but it is not necessarily a more accurate representa-tion ofthe underlying concept ofgood versus bad contracts.look down the leftbranch.it doesn t seem to make sense intuitively thatifthe working hoursexceed contract is bad ifthere is no health-plan contribution or a fullhealth-plan contribution but is good ifthere is a halfhealth-plan contribution.it is certainly reasonable that the health-plan contribution plays a role in thedecision but not ifhalfis good and both full and none are bad.it seems likelythat this is an artifact ofthe particular values used to create the decision treerather than a genuine feature ofthe good versus bad distinction.the tree in figure is more accurate on the data that was used to trainthe classifier but will probably perform less well on an independent set oftestdata.it is overfitted to the training data it follows it too slavishly.the treein figure is obtained from the one in figure by a process ofpruningwhich we will learn more about in chapter classification a classic machine learning successan often-quoted early success story in the application ofmachine learning topractical problems is the identification ofrules for diagnosing soybean diseases.the data is taken from questionnaires describing plant diseases.there are s it all about?table labor negotiations increase increase increase yearpercentage????cost of living adjustmentnone tcf tcnonetcf?noneworking hours per ret-allw empl-cntrnone???standby allowanceyes noyes???statutory avg genavggengenavglong-term disability assistanceyes nono??yesdental plan contributionnone half fullnone?fullfullbereavement assistanceyes nono??yeshealth plan contributionnone half fullnone?fullhalfacceptability of contractgood am page examplesthe weather problem and increase first yearbad holidays increase first year good holidays plan contribution increasefirst yearwage increasefirst yearworking hoursper week trees for the labor negotiations am page exampleseach representing a diseased plant.plants were measured on one having a small set ofpossible values.examples are labeledwith the diagnosis ofan expert in plant biologythere are disease categoriesaltogether horrible-sounding diseases such as diaporthe stem cankerrhizoc-tonia root rotand bacterial blightto mention just a few.table gives the attributesthe number ofdifferent values that each canhaveand a sample record for one particular plant.the attributes are placed intodifferent categories just to make them easier to read.here are two example ruleslearned from this dataifleaf condition is normal andstem condition is abnormal andstem cankers is below soil line andcanker lesion color is brownthendiagnosis is rhizoctonia root rotifleaf malformation is absent andstem condition is abnormal andstem cankers is below soil line andcanker lesion color is brownthendiagnosis is rhizoctonia root rotthese rules nicely illustrate the potential role ofprior knowledge often calleddomain knowledge in machine learningbecause the only difference betweenthe two descriptions is leafcondition is normal versus leafmalformation isabsent.nowin this domainifthe leafcondition is normal then leafmalfor-mation is necessarily absentso one ofthese conditions happens to be a specialcase ofthe other.thus ifthe first rule is truethe second is necessarily true aswell.the only time the second rule comes into play is when leafmalformationis absent but leafcondition is notnormalthat iswhen something other thanmalformation is wrong with the leaf.this is certainly not apparent from a casualreading ofthe rules.research on this problem in the late found that these diagnostic rulescould be generated by a machine learning algorithmalong with rules for everyother disease categoryfrom about training examples.these training examples were carefully selected from the corpus ofcases as being quite differ-ent from one another far apart in the example space.at the same timetheplant pathologist who had produced the diagnoses was interviewedand hisexpertise was translated into diagnostic rules.surprisinglythe computer-generated rules outperformed the expert-derived rules on the remaining testexamples.they gave the correct disease top ranking ofthe time com-pared with only for the expert-derived rules.furthermorenot only s it all am page examplesthe weather problem and soybean data.numberattributeof valuessample valueenvironmenttime of as last yearhail than of fruit spot yellow leaf spot spot mildew soil linecanker lesion fruiting bodies on decay of and drymycelium on am page the learning algorithm find rules that outperformed those ofthe expert collab-oratorbut the same expert was so impressed that he allegedly adopted the dis-covered rules in place ofhis applicationsthe examples that we opened with are speculative research projectsnot pro-duction systems.and the preceding illustrations are toy problemsthey aredeliberately chosen to be small so that we can use them to work through algo-rithms later in the book.where s the beef? here are some applications ofmachine learning that have actually been put into use.being fielded applicationsthe illustrations that follow tend to stress the useoflearning in performance situationsin which the emphasis is on ability toperform well on new examples.this book also describes the use oflearningsystems to gain knowledge from decision structures that are inferred from thedata.we believe that this is as important probably even more important inthe long run a use ofthe technology as merely making high-performance pre-dictions.stillit will tend to be underrepresented in fielded applications becausewhen learning techniques are used to gain insightthe result is not normally asystem that is put to work as an application in its own right.neverthelessinthree ofthe examples that followthe fact that the decision structure is com-prehensible is a key feature in the successful adoption ofthe application.decisions involving judgmentwhen you apply for a loanyou have to fill out a questionnaire that asks for relevant financial and personal information.this information is used by theloan company as the basis for its decision as to whether to lend you money.suchdecisions are typically made in two stages.firststatistical methods are used todetermine clear accept and reject cases.the remaining borderline cases aremore difficult and call for human judgment.for exampleone loan companyuses a statistical decision procedure to calculate a numeric parameter based onthe information supplied in the questionnaire.applicants are accepted ifthisparameter exceeds a preset threshold and rejected ifit falls below a secondthreshold.this accounts for ofcasesand the remaining are referredto loan officers for a decision.on examining historical data on whether appli-cants did indeed repay their loanshoweverit turned out that halfofthe bor-derline applicants who were granted loans actually defaulted.although it wouldbe tempting simply to deny credit to borderline customerscredit industry pro-fessionals pointed out that ifonly their repayment future could be reliably deter-mined it is precisely these customers whose business should be wooedthey tendto be active customers ofa credit institution because their finances remain in s it all am page chronically volatile condition.a suitable compromise must be reached betweenthe viewpoint ofa company accountantwho dislikes bad debtand that ofasales executivewho dislikes turning business away.enter machine learning.the input was training examples ofborderlinecases for which a loan had been made that specified whether the borrower hadfinally paid offor defaulted.for each training exampleabout attributes wereextracted from the questionnairesuch as ageyears with current employeryearsat current addressyears with the bankand other credit cards possessed.amachine learning procedure was used to produce a small set ofclassificationrules that made correct predictions on two-thirds ofthe borderline cases in anindependently chosen test set.not only did these rules improve the success rateofthe loan decisionsbut the company also found them attractive because theycould be used to explain to applicants the reasons behind the decision.althoughthe project was an exploratory one that took only a small development effortthe loan company was apparently so pleased with the result that the rules wereput into use immediately.screening imagessince the early days ofsatellite technologyenvironmental scientists have beentrying to detect oil slicks from satellite images to give early warning ofecolog-ical disasters and deter illegal dumping.radar satellites provide an opportunityfor monitoring coastal waters day and nightregardless ofweather conditions.oil slicks appear as dark regions in the image whose size and shape evolvedepending on weather and sea conditions.howeverother look-alike darkregions can be caused by local weather conditions such as high wind.detectingoil slicks is an expensive manual process requiring highly trained personnel whoassess each region in the image.a hazard detection system has been developed to screen images for subse-quent manual processing.intended to be marketed worldwide to a wide varietyofusers government agencies and companies with different objectivesapplicationsand geographic areasit needs to be highly customizable to indi-vidual circumstances.machine learning allows the system to be trained onexamples ofspills and nonspills supplied by the user and lets the user controlthe tradeoffbetween undetected spills and false alarms.unlike other machinelearning applicationswhich generate a classifier that is then deployed in thefieldhere it is the learning method itselfthat will be deployed.the input is a set ofraw pixel images from a radar satelliteand the outputis a much smaller set ofimages with putative oil slicks marked by a coloredborder.firststandard image processing operations are applied to normalize theimage.thensuspicious dark regions are identified.several dozen attributes are extracted from each regioncharacterizing its am page sharpness and jaggedness ofthe boundariesproximity to other regionsandinformation about the background in the vicinity ofthe region.finallystan-dard learning techniques are applied to the resulting attribute vectors.several interesting problems were encountered.one is the scarcity oftrain-ing data.oil slicks are very rareand manual classification isextremely costly.another is the unbalanced nature ofthe problemofthe manydark regions in the training dataonly a very small fraction are actual oil slicks.a third is that the examples group naturally into batcheswith regions drawnfrom each image forming a single batchand background characteristics varyfrom one batch to another.finallythe performance task is to serve as a filterand the user must be provided with a convenient means ofvarying the false-alarm rate.load forecastingin the electricity supply industryit is important to determine future demandfor power as far in advance as possible.ifaccurate estimates can be made forthe maximum and minimum load for each hourdaymonthseasonand yearutility companies can make significant economies in areas such as setting theoperating reservemaintenance schedulingand fuel inventory management.an automated load forecasting assistant has been operating at a major utilitysupplier over the past decade to generate hourly forecasts days in advance.thefirst step was to use data collected over the previous years to create a sophis-ticated load model manually.this model had three componentsbase load forthe yearload periodicity over the yearand the effect ofholidays.to normalizefor the base loadthe data for each previous year was standardized by subtract-ing the average load for that year from each hourly reading and dividing by thestandard deviation over the year.electric load shows periodicity at three fun-damental frequenciesdiurnalwhere usage has an early morning minimum andmidday and afternoon maximaweeklywhere demand is lower at weekendsand seasonalwhere increased demand during winter and summer for heatingand coolingrespectivelycreates a yearly cycle.major holidays such as thanks-givingchristmasand new year s day show significant variation from thenormal load and are each modeled separately by averaging hourly loads for thatday over the past years.minor official holidayssuch as columbus dayarelumped together as school holidays and treated as an offset to the normaldiurnal pattern.all ofthese effects are incorporated by reconstructing a year sload as a sequence oftypical daysfitting the holidays in their correct positionand denormalizing the load to account for overall growth.thus farthe load model is a static oneconstructed manually from histori-cal dataand implicitly assumes normal climatic conditions over the year.thefinal step was to take weather conditions into account using a technique s it all am page locates the previous day most similar to the current circumstances and uses thehistorical information from that day as a predictor.in this case the predictionis treated as an additive correction to the static load model.to guard againstoutliersthe eight most similar days are located and their additive correctionsaveraged.a database was constructed oftemperaturehumiditywind speedand cloud cover at three local weather centers for each hour ofthe historical recordalong with the difference between the actual load and that predicted by the static model.a linear regression analysis was performed todetermine the relative effects ofthese parameters on loadand the coefficientswere used to weight the distance function used to locate the most similar days.the resulting system yielded the same performance as trained human fore-casters but was far quicker taking seconds rather than hours to generate a dailyforecast.human operators can analyze the forecast s sensitivity to simulatedchanges in weather and bring up for examination the most similar days thatthe system used for weather adjustment.diagnosisdiagnosis is one ofthe principal application areas ofexpert systems.althoughthe handcrafted rules used in expert systems often perform wellmachine learn-ing can be useful in situations in which producing rules manually is too laborintensive.preventative maintenance ofelectromechanical devices such as motors andgenerators can forestall failures that disrupt industrial processes.techniciansregularly inspect each devicemeasuring vibrations at various points to deter-mine whether the device needs servicing.typical faults include shaft misalign-mentmechanical looseningfaulty bearingsand unbalanced pumps.aparticular chemical plant uses more than different devicesranging fromsmall pumps to very large turbo-alternatorswhich until recently were diag-nosed by a human expert with years ofexperience.faults are identified bymeasuring vibrations at different places on the device s mounting and usingfourier analysis to check the energy present in three different directions at eachharmonic ofthe basic rotation speed.this informationwhich is very noisybecause oflimitations in the measurement and recording procedureis studiedby the expert to arrive at a diagnosis.although handcrafted expert system ruleshad been developed for some situationsthe elicitation process would have tobe repeated several times for different types ofmachineryso a learningapproach was investigated.six hundred faultseach comprising a set ofmeasurements along with theexpert s diagnosiswere availablerepresenting years ofexperience in thefield.about halfwere unsatisfactory for various reasons and had to be discardedthe remainder were used as training examples.the goal was not to am page whether or not a fault existedbut to diagnose the kind offaultgiven that onewas there.thus there was no need to include fault-free cases in the training set.the measured attributes were rather low level and had to be augmented by inter-mediate conceptsthat isfunctions ofbasic attributeswhich were defined inconsultation with the expert and embodied some causal domain knowledge.the derived attributes were run through an induction algorithm to produce aset ofdiagnostic rules.initiallythe expert was not satisfied with the rulesbecause he could not relate them to his own knowledge and experience.forhimmere statistical evidence was notby itselfan adequate explanation.further background knowledge had to be used before satisfactory rules weregenerated.although the resulting rules were quite complexthe expert likedthem because he could justify them in light ofhis mechanical knowledge.hewas pleased that a third ofthe rules coincided with ones he used himselfandwas delighted to gain new insight from some ofthe others.performance tests indicated that the learned rules were slightly superior tothe handcrafted ones that had previously been elicited from the expertand thisresult was confirmed by subsequent use in the chemical factory.it is interestingto notehoweverthat the system was put into use not because ofits good per-formance but because the domain expert approved ofthe rules that had beenlearned.marketing and salessome ofthe most active applications ofdata mining have been in the area ofmarketing and sales.these are domains in which companies possess massivevolumes ofprecisely recorded datadata which it has only recently been real-ized is potentially extremely valuable.in these applicationspredictions them-selves are the chiefinterestthe structure ofhow decisions are made is oftencompletely irrelevant.we have already mentioned the problem offickle customer loyalty and thechallenge ofdetecting customers who are likely to defect so that they can bewooed back into the fold by giving them special treatment.banks were earlyadopters ofdata mining technology because oftheir successes in the use ofmachine learning for credit assessment.data mining is now being used toreduce customer attrition by detecting changes in individual banking patternsthat may herald a change ofbank or even life changes such as a move toanother city that could result in a different bank being chosen.it may revealfor examplea group ofcustomers with above-average attrition rate who domost oftheir banking by phone after hours when telephone response is slow.data mining may determine groups for whom new services are appropriatesuch as a cluster ofprofitablereliable customers who rarely get cash advancesfrom their credit card except in november and decemberwhen they are s it all am page pared to pay exorbitant interest rates to see them through the holiday season.inanother domaincellular phone companies fight churnby detecting patterns ofbehavior that could benefit from new servicesand then advertise such servicesto retain their customer base.incentives provided specifically to retain existingcustomers can be expensiveand successful data mining allows them to be pre-cisely targeted to those customers where they are likely to yield maximum benefit.market basket analysisis the use ofassociation techniques to find groups ofitems that tend to occur together in transactionstypically supermarket check-out data.for many retailers this is the only source ofsales information that isavailable for data mining.for exampleautomated analysis ofcheckout datamay uncover the fact that customers who buy beer also buy chipsa discoverythat could be significant from the supermarket operator s point ofviewalthough rather an obvious one that probably does not need a data miningexercise to discover.or it may come up with the fact that on thursdayscus-tomers often purchase diapers and beer togetheran initially surprising resultthaton reflectionmakes some sense as young parents stock up for a weekendat home.such information could be used for many purposesplanning storelayoutslimiting special discounts to just one ofa set ofitems that tend to bepurchased togetheroffering coupons for a matching product when one ofthemis sold aloneand so on.there is enormous added value in being able to iden-tify individual customer s sales histories.in factthis value is leading to a pro-liferation ofdiscount cards or loyalty cards that allow retailers to identifyindividual customers whenever they make a purchasethe personal data thatresults will be far more valuable than the cash value ofthe discount.identifica-tion ofindividual customers not only allows historical analysis ofpurchasingpatterns but also permits precisely targeted special offers to be mailed out toprospective customers.this brings us to direct marketinganother popular domain for data mining.promotional offers are expensive and have an extremely low but highly profitable response rate.any technique that allows a promotional mailout tobe more tightly focusedachieving the same or nearly the same response froma much smaller sampleis valuable.commercially available databases contain-ing demographic information based on zip codes that characterize the associ-ated neighborhood can be correlated with information on existing customersto find a socioeconomic model that predicts what kind ofpeople will turn outto be actual customers.this model can then be used on information gained inresponse to an initial mailoutwhere people send back a response card or callan number for more informationto predict likely future customers.directmail companies have the advantage over shopping-mall retailers ofhaving com-plete purchasing histories for each individual customer and can use data miningto determine those likely to respond to special offers.targeted campaigns arecheaper than mass-marketed campaigns because companies save money am page sending offers only to those likely to want the product.machine learning canhelp companies to find the targets.other applicationsthere are countless other applications ofmachine learning.we briefly mentiona few more areas to illustrate the breadth ofwhat has been done.sophisticated manufacturing processes often involve tweaking controlparameters.separating crude oil from natural gas is an essential prerequisite tooil refinementand controlling the separation process is a tricky job.britishpetroleum used machine learning to create rules for setting the parameters.thisnow takes just minuteswhereas previously human experts took more thana day.westinghouse faced problems in their process for manufacturing nuclearfuel pellets and used machine learning to create rules to control the process.this was reported to save them more than million per year printing company r.r.donnelly applied the same idea to controlrotogravure printing presses to reduce artifacts caused by inappropriate parameter settingsreducing the number ofartifacts from more than eachyear to less than the realm ofcustomer support and servicewe have already described adju-dicating loansand marketing and sales applications.another example ariseswhen a customer reports a telephone problem and the company must decidewhat kind oftechnician to assign to the job.an expert system developed by bellatlantic in to make this decision was replaced in by a set ofruleslearned using machine learningwhich saved more than million per year bymaking fewer incorrect decisions.there are many scientific applications.in biologymachine learning is usedto help identify the thousands ofgenes within each new genome.in biomedi-cineit is used to predict drug activity by analyzing not just the chemical properties ofdrugs but also their three-dimensional structure.this acceleratesdrug discovery and reduces its cost.in astronomymachine learning has been used to develop a fully automatic cataloguing system for celestial objectsthat are too faint to be seen by visual inspection.in chemistryit has been usedto predict the structure ofcertain organic compounds from magnetic resonancespectra.in all these applicationsmachine learning techniques have attainedlevels ofperformance or should we say skill? that rival or surpass humanexperts.automation is especially welcome in situations involving continuous moni-toringa job that is time consuming and exceptionally tedious for humans.eco-logical applications include the oil spill monitoring described earlier.someother applications are rather less consequential for examplemachine learn-ing is being used to predict preferences for tv programs based on past s it all am page and advise viewers about the available channels.still others may save lives.intensive care patients may be monitored to detect changes in variables thatcannot be explained by circadian rhythmmedicationand so onraising an alarm when appropriate.finallyin a world that relies on vulnerable net-worked computer systems and is increasingly concerned about cybersecuritymachine learning is used to detect intrusion by recognizing unusual patterns learning and statisticswhat s the difference between machine learning and statistics? cynicslookingwryly at the explosion ofcommercial interest hype in this areaequatedata mining to statistics plus marketing.in truthyou should not look for adividing line between machine learning and statistics because there is a contin-uum and a multidimensional one at that ofdata analysis techniques.somederive from the skills taught in standard statistics coursesand others are moreclosely associated with the kind ofmachine learning that has arisen out ofcom-puter science.historicallythe two sides have had rather different traditions.ifforced to point to a single difference ofemphasisit might be that statistics hasbeen more concerned with testing hypotheseswhereas machine learning hasbeen more concerned with formulating the process ofgeneralization as a searchthrough possible hypotheses.but this is a gross oversimplificationstatistics isfar more than hypothesis testingand many machine learning techniques do notinvolve any searching at all.in the pastvery similar methods have developed in parallel in machine learn-ing and statistics.one is decision tree induction.four statisticians published a book on classification and regression treesin the throughout the and early a prominent machine learningresearcherj.ross quinlanwas developing a system for inferring classificationtrees from examples.these two independent projects produced quite similarmethods for generating trees from examplesand the researchers only becameaware ofone another s work much later.a second area in which similar methodshave arisen involves the use ofnearest-neighbor methods for classification.these are standard statistical techniques that have been extensively adapted bymachine learning researchersboth to improve classification performance andto make the procedure more efficient computationally.we will examine bothdecision tree induction and nearest-neighbor methods in chapter now the two perspectives have converged.the techniques we willexamine in this book incorporate a great deal ofstatistical thinking.from thebeginningwhen constructing and refining the initial example setstandard sta-tistical methods applyvisualization ofdataselection learning and am page outliersand so on.most learning algorithms use statistical tests when con-structing rules or trees and for correcting models that are overfitted in thatthey depend too strongly on the details ofthe particular examples used toproduce them have already seen an example ofthis in the two decision treesoffigure for the labor negotiations problem.statistical tests are used tovalidate machine learning models and to evaluate machine learning algorithms.in our study ofpractical techniques for data miningwe will learn a great dealabout as searchone way ofvisualizing the problem oflearning and one that distinguishes itfrom statistical approaches is to imagine a search through a space ofpossibleconcept descriptions for one that fits the data.although the idea ofgeneraliza-tion as search is a powerful conceptual tool for thinking about machine learn-ingit is not essential for understanding the practical methods described in thisbook.that is why this section is marked optionalas indicated by the gray barin the margin.supposefor definitenessthat concepts the result oflearning areexpressed as rules such as the ones given for the weather problem in section other concept description languages would do just as well.supposethat we list all possible sets ofrules and then look for ones that satisfy a givenset ofexamples.a big job? yes.an infinitejob? at first glance it seems so becausethere is no limit to the number ofrules there might be.but actually the numberofpossible rule sets is finite.note first that each individual rule is no greaterthan a fixed maximum sizewith at most one term for each attributefor theweather data oftable this involves four terms in all.because the number ofpossible rules is finitethe number ofpossible rule setsis finitetooalthoughextremely large.howeverwe d hardly be interested in sets that contained a verylarge number ofrules.in factwe d hardly be interested in sets that had morerules than there are examples because it is difficult to imagine needing morethan one rule for each example.so ifwe were to restrict consideration to rulesets smaller than thatthe problem would be substantially reducedalthoughstill very large.the threat ofan infinite number ofpossible concept descriptions seems moreserious for the second version ofthe weather problem in table because theserules contain numbers.ifthey are real numbersyou can t enumerate themevenin principle.howeveron reflection the problem again disappears because thenumbers really just represent breakpoints in the numeric values that appear inthe examples.for instanceconsider the temperatureattribute in table the numbers s it all am page ferent numbers.there are possible places in which we might want to put abreakpoint for a rule involving temperature.the problem isn t infinite after all.so the process ofgeneralization can be regarded as a search through an enor-mousbut finitesearch space.in principlethe problem can be solved by enu-merating descriptions and striking out those that do not fit the examplespresented.a positive example eliminates all descriptions that it does not matchand a negative one eliminates those it does match.with each example the setofremaining descriptions shrinks stays the same.ifonly one is leftit is thetarget description the target concept.ifseveral descriptions are leftthey may still be used to classify unknownobjects.an unknown object that matches all remaining descriptions should beclassified as matching the targetifit fails to match any description it should beclassified as being outside the target concept.only when it matches somedescriptions but not others is there ambiguity.in this case ifthe classificationofthe unknown object were revealedit would cause the set ofremainingdescriptions to shrink because rule sets that classified the object the wrong waywould be rejected.enumerating the concept spaceregarding it as search is a good way oflooking at the learning process.howeverthe search spacealthough finiteis extremely bigand it is generally quiteimpractical to enumerate all possible descriptions and then see which ones fit.in the weather problem there are possibilities for eachrule.there are four possibilities for the outlookattributesunnyovercastrainyor it may not participate in the rule at all.similarlythere are four for tempera-turethree for weatherand humidityand two for the class.ifwe restrict the ruleset to contain no more than rules there are examples in the train-ing setthere are around different rule sets.that s a lot toenumerateespecially for such a patently trivial problem.although there are ways ofmaking the enumeration procedure more feasi-blea serious problem remainsin practiceit is rare for the process to convergeon a unique acceptable description.either many descriptions are still in therunning after the examples are processed or the descriptors are all eliminated.the first case arises when the examples are not sufficiently comprehensive toeliminate all possible descriptions except for the correct one.in practicepeople often want a single best descriptionand it is necessary to apply someother criteria to select the best one from the set ofremaining descriptions.thesecond problem arises either because the description language is not expressiveenough to capture the actual concept or because ofnoise in the examples.ifanexample comes in with the wrong classification because ofan error in someofthe attribute values or in the class that is assigned to itthis will likely as am page eliminate the correct description from the space.the result is that the set ofremaining descriptions becomes empty.this situation is very likely to happenifthe examples contain any noise at allwhich inevitably they do except in artificial situations.another way oflooking at generalization as search is to imagine it not as aprocess ofenumerating descriptions and striking out those that don t apply butas a kind ofhill-climbing in description space to find the description that bestmatches the set ofexamples according to some prespecified matching criterion.this is the way that most practical machine learning methods work.howeverexcept in the most trivial casesit is impractical to search the whole spaceexhaustivelymost practical algorithms involve heuristic search and cannotguarantee to find the optimal description.biasviewing generalization as a search in a space ofpossible concepts makes it clearthat the most important decisions in a machine learning system are as follows the concept description language the order in which the space is searched the way that overfitting to the particular training data is avoidedthese three properties are generally referred to as the biasofthe search and arecalled languagebiassearchbiasand overfitting-avoidancebias.you bias thelearning scheme by choosing a language in which to express conceptsby search-ing in a particular way for an acceptable descriptionand by deciding when theconcept has become so complex that it needs to be simplified.language biasthe most important question for language bias is whether the concept descrip-tion language is universal or whether it imposes constraints on what concepts canbe learned.ifyou consider the set ofall possible examplesa concept is really justa division ofit into subsets.in the weather exampleifyou were to enumerate allpossible weather conditionsthe playconcept is a subset ofpossible weather con-ditions.a universal language is one that is capable ofexpressing every possiblesubset ofexamples.in practicethe set ofpossible examples is generally hugeandin this respect our perspective is a theoreticalnot a practicalone.ifthe concept description language permits statements involving logical orthat isdisjunctionsthen any subset can be represented.ifthe description lan-guage is rule baseddisjunction can be achieved by using separate rules.forexampleone possible concept representation is just to enumerate the examplesif outlook and temperature and humidity windy then play s it all am page if outlook and temperature and humidity windy then play outlook and temperature and humidity windy then play outlook and temperature and humidity windy then play none of the above then play is not a particularly enlightening concept descriptionit simply records thepositive examples that have been observed and assumes that all the rest are neg-ative.each positive example is given its own ruleand the concept is the dis-junction ofthe rules.alternativelyyou could imagine having individual rulesfor each ofthe negative examplestoo an equally uninteresting concept.ineither case the concept description does not perform any generalizationitsimply records the original data.on the other handifdisjunction is notallowedsome possible concepts sets ofexamples may not be able to be represented at all.in that caseamachine learning scheme may simply be unable to achieve good performance.another kind oflanguage bias is that obtained from knowledge ofthe par-ticular domain being used.for exampleit may be that some combinations ofattribute values can never happen.this would be the case ifone attributeimplied another.we saw an example ofthis when considering the rules for thesoybean problem described on page would be pointless to even con-sider concepts that involved redundant or impossible combinations ofattributevalues.domain knowledge can be used to cut down the search space.knowl-edge is powera little goes a long wayand even a small hint can reduce thesearch space dramatically.search biasin realistic data mining problemsthere are many alternative concept descrip-tions that fit the dataand the problem is to find the best one according tosome criterion usually simplicity.we use the term fitin a statistical senseweseek the best description that fits the data reasonably well.moreoverit is oftencomputationally infeasible to search the whole space and guarantee that thedescription found really is the best.consequentlythe search procedure isheuristicand no guarantees can be made about the optimality ofthe final result.this leaves plenty ofroom for biasdifferent search heuristics bias the search indifferent ways.for examplea learning algorithm might adopt a greedy search for rules bytrying to find the best rule at each stage and adding it in to the rule set.howeverit may be that the best pairofrules is not just the two rules that are individu-ally found to be the best.or when building a decision treea commitment as am page split early on using a particular attribute might turn out later to be ill consid-ered in light ofhow the tree develops below that node.to get around these prob-lemsa beamsearchcould be used in which irrevocable commitments are notmade but instead a set ofseveral active alternatives whose number is the beamwidth are pursued in parallel.this will complicate the learning algorithmquite considerably but has the potential to avoid the myopia associated with agreedy search.ofcourseifthe beam width is not large enoughmyopia maystill occur.there are more complex search strategies that help to overcome thisproblem.a more general and higher-level kind ofsearch bias concerns whether thesearch is done by starting with a general description and refining itor by starting with a specific example and generalizing it.the former is called ageneral-to-specificsearch biasthe latter a specific-to-generalone.many learningalgorithms adopt the former policystarting with an empty decision treeor avery general ruleand specializing it to fit the examples.howeverit is perfectlypossible to work in the other direction.instance-based methods start with a particular example and see how it can be generalized to cover nearby examplesin the same class.overfitting-avoidance biasoverfitting-avoidance bias is often just another kind ofsearch bias.but becauseit addresses a rather special problemwe treat it separately.recall the disjunc-tion problem described previously.the problem is that ifdisjunction is alloweduseless concept descriptions that merely summarize the data become possiblewhereas ifit is prohibitedsome concepts are unlearnable.to get around thisproblemit is common to search the concept space starting with the simplestconcept descriptions and proceeding to more complex onessimplest-firstordering.this biases the search toward simple concept descriptions.using a simplest-first search and stopping when a sufficiently complexconcept description is found is a good way ofavoiding overfitting.it is some-times called forwardpruningor prepruningbecause complex descriptions arepruned away before they are reached.the alternativebackwardpruningor post-pruningis also viable.herewe first find a description that fits the data well andthen prune it back to a simpler description that also fits the data.this is not asredundant as it soundsoften the only way to arrive at a simple theory is to finda complex one and then simplify it.forward and backward pruning are both akind ofoverfitting-avoidance bias.in summaryalthough generalization as search is a nice way to think aboutthe learning problembias is the only way to make it feasible in practice.dif-ferent learning algorithms correspond to different concept description spacessearched with different biases.this is what makes it s it all am page description languages and biases serve some problems well and other problemsbadly.there is no universal best learning method as every teacher mining and ethicsthe use ofdata particularly data about people for data mining has seriousethical implicationsand practitioners ofdata mining techniques must actresponsibly by making themselves aware ofthe ethical issues that surround theirparticular application.when applied to peopledata mining is frequently used to discriminate who gets the loanwho gets the special offerand so on.certain kinds ofdiscrimination racialsexualreligiousand so on are not only unethical but also illegal.howeverthe situation is complexeverything depends on theapplication.using sexual and racial information for medical diagnosis is certainly ethicalbut using the same information when mining loan paymentbehavior is not.even when sensitive information is discardedthere is a risk that models will be built that rely on variables that can be shown to substitutefor racial or sexual characteristics.for examplepeople frequently live in areas that are associated with particular ethnic identitiesso using an area code in a data mining study runs the risk ofbuilding models that are based onrace even though racial information has been explicitly excluded from thedata.it is widely accepted that before people make a decision to provide personalinformation they need to know how it will be used and what it will be used forwhat steps will be taken to protect its confidentiality and integritywhat the con-sequences ofsupplying or withholding the information areand any rights ofredress they may have.whenever such information is collectedindividualsshould be told these things not in legalistic small print but straightforwardlyin plain language they can understand.the potential use ofdata mining techniques means that the ways in which arepository ofdata can be used may stretch far beyond what was conceived whenthe data was originally collected.this creates a serious problemit is necessaryto determine the conditions under which the data was collected and for whatpurposes it may be used.does the ownership ofdata bestow the right to use itin ways other than those purported when it was originally recorded? clearly inthe case ofexplicitly collected personal data it does not.but in general the situation is complex.surprising things emerge from data mining.for exampleit has beenreported that one ofthe leading consumer groups in france has found thatpeople with red cars are more likely to default on their car loans.what is mining and am page status ofsuch a discovery what information is it based on? under what con-ditions was that information collected? in what ways is it ethical to use it?clearlyinsurance companies are in the business ofdiscriminating amongpeople based on stereotypes young males pay heavily for automobile insur-ance but such stereotypes are not based solely on statistical correlationstheyalso involve common-sense knowledge about the world.whether the precedingfinding says something about the kind ofperson who chooses a red carorwhether it should be discarded as an irrelevancyis a matter for human judgment based on knowledge ofthe world rather than on purely statistical criteria.when presented with datayou need to ask who is permitted to have accessto itfor what purpose it was collectedand what kind ofconclusions is it legit-imate to draw from it.the ethical dimension raises tough questions for thoseinvolved in practical data mining.it is necessary to consider the norms ofthecommunity that is used to dealing with the kind ofdata involvedstandards thatmay have evolved over decades or centuries but ones that may not be known tothe information specialist.for exampledid you know that in the library com-munityit is taken for granted that the privacy ofreaders is a right that is jealously protected? ifyou call your university library and ask who has such-and-such a textbook out on loanthey will not tell you.this prevents a studentfrom being subjected to pressure from an irate professor to yield access to a bookthat she desperately needs for her latest grant application.it also prohibitsenquiry into the dubious recreational reading tastes ofthe university ethicscommittee chairman.those who buildsaydigital libraries may not be awareofthese sensitivities and might incorporate data mining systems that analyzeand compare individuals reading habits to recommend new books perhapseven selling the results to publishers!in addition to community standards for the use ofdatalogical and scientificstandards must be adhered to when drawing conclusions from it.ifyou do comeup with conclusions as red car owners being greater credit risksyou needto attach caveats to them and back them up with arguments other than purelystatistical ones.the point is that data mining is just a tool in the whole processit is people who take the resultsalong with other knowledgeand decide whataction to apply.data mining prompts another questionwhich is really a political onetowhat use are society s resources being put? we mentioned previously the appli-cation ofdata mining to basket analysiswhere supermarket checkout recordsare analyzed to detect associations among items that people purchase.what useshould be made ofthe resulting information? should the supermarket managerplace the beer and chips togetherto make it easier for shoppersor farther apartmaking it less convenient for themmaximizing their time in the storeandtherefore increasing their likelihood ofbeing drawn into unplanned s it all am page purchases? should the manager move the most expensivemost profitablediapers near the beerincreasing sales to harried fathers ofa high-margin itemand add further luxury baby products nearby?ofcourseanyone who uses advanced technologies should consider thewisdom ofwhat they are doing.ifdatais characterized as recorded factstheninformationis the set ofpatternsor expectationsthat underlie the data.youcould go on to define knowledgeas the accumulation ofyour set ofexpectationsand wisdomas the value attached to knowledge.although we will not pursue itfurther herethis issue is worth pondering.as we saw at the very beginning ofthis chapterthe techniques described inthis book may be called upon to help make some ofthe most profound andintimate decisions that life presents.data mining is a technology that we needto take readingto avoid breaking up the flow ofthe main textall references are collected in asection at the end ofeach chapter.this first furtherreadingsection describespapersbooksand other resources relevant to the material covered in human invitrofertilization research mentioned in the opening to thischapter was undertaken by the oxford university computing laboratoryand the research on cow culling was performed in the computer sciencedepartment at the university ofwaikatonew zealand.the example ofthe weather problem is from quinlan and has beenwidely used to explain machine learning schemes.the corpus ofexample prob-lems mentioned in the introduction to section is available from blake et contact lens example is from cendrowska introducedthe prism rule-learning algorithm that we will encounter in chapter irisdataset was described in a classic early paper on statistical inference labor negotiations data is from the collectivebargainingreviewapublication oflabour canada issued by the industrial relations informationservice the soybean problem was first described by michalskiand chilausky ofthe applications in section are covered in an excellent paper thatgives plenty ofother applications ofmachine learning and rule inductionlangley and simon source offielded applications is a specialissue ofthe machinelearningjournalkohavi and provost loancompany application is described in more detail by michie oil slickdetector is from kubat et electric load forecasting work is byjabbour et the application to preventative maintenance ofelectromechanical devices is from saitta and neri descriptions am page ofsome ofthe other projects mentioned in section the figuresofdollars saved and related literature references appear at the web sites ofthealberta ingenuity centre for machine learning and mlneta europeannetwork for machine learning.the book classificationandregressiontreesmentioned in section is bybreiman et the independently derived but similar scheme ofquinlan was described in a series ofpapers that eventually led to a bookquinlan first book on data mining appeared in andfrawley a collection ofpapers presented at a workshop on knowledgediscovery in databases in the late book from the same stable hasappeared since et from a workshop.there followed arash ofbusiness-oriented books on data miningfocusing mainly on practicalaspects ofhow it can be put into practice with only rather superficial descrip-tions ofthe technology that underlies the methods used.they are valuablesources ofapplications and inspiration.for exampleadriaans and from syllogica european systems and database consultancyprovide anearly introduction to data mining.berry and a pennsylva-nia-based company specializing in data warehousing and data mininggive anexcellent and example-studded review ofdata mining techniques for market-ingsalesand customer support.cabena et by people fromfive international ibm laboratoriesoverview the data mining process withmany examples ofreal-world applications.dhar and stein give a busi-ness perspective on data mining and include broad-brushpopularized reviewsofmany ofthe technologies involved.groth for a provider ofdata mining softwaregives a briefintroduction to data mining and then a fairly extensive review ofdata mining software productsthe book includes acd-rom containing a demo version ofhis company s product.weiss andindurkhya look at a wide variety ofstatistical techniques for making predictions from what they call big data. han and kamber cover datamining from a database perspectivefocusing on the discovery ofknowledge inlarge corporate databases.finallyhand et produced an interdiscipli-nary book on data mining from an international group ofauthors who are wellrespected in the field.books on machine learningon the other handtend to be academic textssuited for use in university courses rather than practical guides.mitchell an excellent book that covers many techniques ofmachine learningincluding some notably genetic algorithms and reinforcement learning thatare not covered here.langley offers another good text.although the pre-viously mentioned book by quinlan concentrates on a particular learn-ing we will cover in detail in chapters and is agood introduction to some ofthe problems and techniques ofmachine s it all am page ing.an excellent book on machine learning from a statistical perspective is fromhastie et is quite a theoretically oriented workand is beauti-fully produced with apt and telling illustrations.pattern recognition is a topic that is closely related to machine learningandmany ofthe same techniques apply.duda et offer the second editionofa classic and successful book on pattern recognition and hart and bishop describe the use ofneural networks for patternrecognition.data mining with neural networks is the subject ofa book by ofibmwhich features the ibm neural network utility product that hedeveloped.there is a great deal ofcurrent interest in support vector machineswhichwe return to in chapter and shawe-taylor give a nice intro-ductionand a follow-up work generalizes this to cover additional algorithmskernelsand solutions with applications to pattern discovery problems in fieldssuch as bioinformaticstext analysisand image analysis and cristianini lkopfand smola provide a comprehensive intro-duction to support vector machines and related kernel methods by two youngresearchers who did their phd research in this rapidly developing am page am page before delving into the question ofhow machine learning methods operatewebegin by looking at the different forms the input might take andin the nextchapterthe different kinds ofoutput that might be produced.with any soft-ware systemunderstanding what the inputs and outputs are is far more impor-tant than knowing what goes on in betweenand machine learning is noexception.the input takes the form ofconceptsinstancesand attributes.we call thething that is to be learned a conceptdescription.the idea ofa conceptlike the very idea oflearning in the first placeis hard to pin down preciselyand we won t spend time philosophizing about just what it is and isn t.in a sensewhat we are trying to find the result ofthe learning process is adescription ofthe concept that is intelligiblein that it can be understooddis-cussedand disputedand operationalin that it can be applied to actual exam-ples.the next section explains some distinctions among different kinds oflearning problemsdistinctions that are very concrete and very important inpractical data am page the information that the learner is given takes the form ofa set ofinstances.in the illustrations in chapter instance was an individualindependentexample ofthe concept to be learned.ofcourse there are many things you mightlike to learn for which the raw data cannot be expressed as individualinde-pendent instances.perhaps background knowledge should be taken intoaccount as part ofthe input.perhaps the raw data is an agglomerated mass thatcannot be fragmented into individual instances.perhaps it is a single sequencesaya time sequencethat cannot meaningfully be cut into pieces.howeverthisbook is about simplepractical methods ofdata miningand we focus on situations in which the information can be supplied in the form ofindividualexamples.each instance is characterized by the values ofattributes that measure dif-ferent aspects ofthe instance.there are many different types ofattributesalthough typical data mining methods deal only with numeric and nominalorcategoricalones.finallywe examine the question ofpreparing input for data mining andintroduce a simple format the one that is used by the java code that accom-panies this book for representing the input information as a text s a concept?four basically different styles oflearning appear in data mining applications.inclassificationlearningthe learning scheme is presented with a set ofclassifiedexamples from which it is expected to learn a way ofclassifying unseen exam-ples.in associationlearningany association among features is soughtnot justones that predict a particular classvalue.in clusteringgroups ofexamples thatbelong together are sought.in numericpredictionthe outcome to be predictedis not a discrete class but a numeric quantity.regardless ofthe type oflearninginvolvedwe call the thing to be learned the conceptand the output producedby a learning scheme the conceptdescription.most ofthe examples in chapter are classification problems.the weatherdata and presents a set ofdays together with a decision for eachas to whether to play the game or not.the problem is to learn how to classifynew days as play or don t play.given the contact lens data is to learn how to decide on a lens recommendation for a new patient or more preciselysince every possible combination ofattributes is present inthe datathe problem is to learn a way ofsummarizing the given data.for theirises problem is to learn how to decide whether a new iris floweris setosaversicoloror virginicagiven its sepal length and width and petal lengthand width.for the labor negotiations data problem is to decidewhether a new contract is acceptable or noton the basis ofits am page increase in the firstsecondand third yearscost ofliving adjustmentand soforth.classification learning is sometimes called supervisedbecausein a sensethemethod operates under supervision by being provided with the actual outcomefor each ofthe training examples the play or don t play judgmentthe lens rec-ommendationthe type ofiristhe acceptability ofthe labor contract.thisoutcome is called the classofthe example.the success ofclassification learningcan be judged by trying out the concept description that is learned on an inde-pendent set oftest data for which the true classifications are known but notmade available to the machine.the success rate on test data gives an objectivemeasure ofhow well the concept has been learned.in many practical datamining applicationssuccess is measured more subjectively in terms ofhowacceptable the learned description such as the rules or the decision tree areto a human user.most ofthe examples in chapter can be used equally well for associationlearningin which there is no specified class.herethe problem is to discoverany structure in the data that is interesting. some association rules for theweather data were given in section rules differ from classifica-tion rules in two waysthey can predict any attributenot just the classandthey can predict more than one attribute s value at a time.because ofthis thereare far more association rules than classification rulesand the challenge is toavoid being swamped by them.for this reasonassociation rules are oftenlimited to those that apply to a certain minimum number ofexamples ofthe dataset and have greater than a certain minimum accuracy level say accurate.even thenthere are usually lots ofthemand they have to beexamined manually to determine whether they are meaningful or not.associ-ation rules usually involve only nonnumeric attributesthus you wouldn t nor-mally look for association rules in the iris dataset.when there is no specified classclustering is used to group items that seemto fall naturally together.imagine a version ofthe iris data in which the type ofiris is omittedsuch as in table it is likely that the instances fallinto natural clusters corresponding to the three iris types.the challenge is tofind these clusters and assign the instances to them and to be able to assignnew instances to the clusters as well.it may be that one or more ofthe iris typessplits naturally into subtypesin which case the data will exhibit more than threenatural clusters.the success ofclustering is often measured subjectively in termsofhow useful the result appears to be to a human user.it may be followed by asecond step ofclassification learning in which rules are learned that give anintelligible description ofhow new instances should be placed into the clusters.numeric prediction is a variant ofclassification learning in which theoutcome is a numeric value rather than a category.the cpu performanceproblem is one example.anothershown in table a version ofthe s a am page attributestable data as a clustering problem.sepal lengthsepal widthpetal lengthpetal data with a numeric class.outlooktemperaturehumiditywindyplay time am page data in which what is to be predicted is not play or don t play but rather is thetime minutes to play.with numeric prediction problemsas with othermachine learning situationsthe predicted value for new instances is often ofless interest than the structure ofthe description that is learnedexpressed interms ofwhat the important attributes are and how they relate to the s in an example?the input to a machine learning scheme is a set ofinstances.these instancesare the things that are to be classifiedassociatedor clustered.although until now we have called them exampleshenceforth we will use the more spe-cific term instancesto refer to the input.each instance is an individualinde-pendent example ofthe concept to be learned.in additioneach one ischaracterized by the values ofa set ofpredetermined attributes.this was thecase in all the sample datasets described in the last chapter weathercontactlensirisand labor negotiations problems.each dataset is represented as amatrix ofinstances versus attributeswhich in database terms is a single rela-tionor a flatfile.expressing the input data as a set ofindependent instances is by far the mostcommon situation for practical data mining.howeverit is a rather restrictiveway offormulating problemsand it is worth spending some time reviewingwhy.problems often involve relationships between objects rather than separateindependent instances.supposeto take a specific situationa family tree isgivenand we want to learn the concept sister.imagine your own family treewith your relatives their genders placed at the nodes.this tree is the inputto the learning processalong with a list ofpairs ofpeople and an indication ofwhether they are sisters or not.figure shows part ofa family treebelow which are two tables that eachdefine sisterhood in a slightly different way.a yesin the third column ofthetables means that the person in the second column is a sister ofthe person inthe first column s just an arbitrary decision we ve made in setting up thisexample.the first thing to notice is that there are a lot ofnos in the third column ofthe table on the left because there are people and pairs ofpeople in alland most pairs ofpeople aren t sisters.the table on the rightwhichgives the same informationrecords only the positive instances and assumes thatall others are negative.the idea ofspecifying only positive examples and adopt-ing a standing assumption that the rest are negative is called the closedworldassumption.it is frequently assumed in theoretical studieshoweverit is not s in an am page much practical use in real-life problems because they rarely involve closed worlds in which you can be certain that all cases are covered.neither table in figure is ofany use without the family tree itself.thistree can also be expressed in the form ofa tablepart ofwhich is shown in the problem is expressed in terms oftwo relationships.but these tablesdo not contain independent sets ofinstances because values in the columns ofthe sister-ofrelation refer to rows ofthe familytree relation.we can make them into a single set ofinstances by collapsing thetwo tables into the single one oftable have at last succeeded in transforming the original relational probleminto the form ofinstanceseach ofwhich is an individualindependent attributesfirstpersonsecondpersonpetermpeggyfgracefraympamfianmstevenmgrahammpippafbrianmannafnikkifpeterpeter...stevenstevenstevensteven...lan...anna...nikkipeggysteven......petergrahampamgrace......pippa......nikki.....annasisterof?nonononoyesnoyesyesyesfirstpersonsecondpersonstevengrahamlanbrianannanikkipampampippapippanikkiannasisterof?yesyesyesyesyesyesnoall the family tree and two ways ofexpressing the am page ofthe concept that is to be learned.ofcoursethe instances are not really inde-pendent there are plenty ofrelationships among different rows ofthe table! but they are independent as far as the concept ofsisterhood is concerned.mostmachine learning schemes will still have trouble dealing with this kind ofdataas we will see in section at least the problem has been recast into theright form.a simple rule for the sister-ofrelation is as followsif second person s gender first person s person s sister-of example shows how you can take a relationship between different nodesofa tree and recast it into a set ofindependent instances.in database termsyoutake two relations and join them together to make onea process offlatteningthat is technically called denormalization.it is always possible to do this withany set offinite relations.the structure oftable can be used to describe any relationship betweentwo people grandparenthoodsecond cousins twice removedand so s in an tree represented as a sister-of relation represented in a table.first personsecond of?stevenmalepeterpeggypamfemalepeterpeggyyesgrahammalepeterpeggypamfemalepeterpeggyyesianmalegraceraypippafemalegracerayyesbrianmalegraceraypippafemalegracerayyesannafemalepamiannikkifemalepamianyesnikkifemalepamianannafemalepamianyesall the am page tionships among more people would require a larger table.relationships in whichthe maximum number ofpeople is not specified in advance pose a more seriousproblem.ifwe want to learn the concept ofnuclearfamilyparents and their chil-drenthe number ofpeople involved depends on the size ofthe largest nuclearfamilyand although we could guess at a reasonable maximum number could only be found by scanning the tree itself.neverthelessgivena finite set offinite relations we couldat least in principleform a new superre-lation that contained one row for everycombination ofpeopleand this wouldbe enough to express any relationship between people no matter how many wereinvolved.the computational and storage costs wouldhoweverbe prohibitive.another problem with denormalization is that it produces apparent regular-ities in the data that are completely spurious and are in fact merely reflectionsofthe original database structure.for exampleimagine a supermarket data-base with a relation for customers and the products they buyone for productsand their supplierand one for suppliers and their address.denormalizing thiswill produce a flat file that containsfor each instancecustomerproductsup-plierand supplier address.a database mining tool that seeks structure in thedatabase may come up with the fact that customers who buy beer also buy chipsa discovery that could be significant from the supermarket manager s point ofview.howeverit may also come up with the fact that supplier address can bepredicted exactly from supplier a discovery that will not impress the super-market manager at all.this fact masquerades as a significant discovery from theflat file but is present explicitly in the original database structure.many abstract computational problems involve relations that are not finitealthough clearly any actual set ofinput instances must be finite.concepts suchas ancestor-ofinvolve arbitrarily long paths through a treeand although thehuman raceand hence its family treemay be finite prodigiously largemany artificial problems generate data that truly is infinite.although it maysound abstrusethis situation is the norm in areas such as list processing and logicprogramming and is addressed in a subdiscipline ofmachine learning calledinductivelogicprogramming.computer scientists usually use recursion to dealwith situations in which the number ofpossible instances is infinite.for exampleif is a parent of is an ancestor of is a parent of is an ancestor of is an ancestor of a simple recursive definition ofancestorthat works no matter how distantlytwo people are related.techniques ofinductive logic programming can learnrecursive rules such as these from a finite set ofinstances such as those in am page the real drawbacks ofsuch techniqueshoweverare that they do not copewell with noisy dataand they tend to be so slow as to be unusable on anythingbut small artificial datasets.they are not covered in this booksee bergadanoand gunetti for a comprehensive treatment.in summarythe input to a data mining scheme is generally expressed as atable ofindependent instances ofthe concept to be learned.because ofthisithas been suggesteddisparaginglythat we should really talk offileminingratherthan databasemining.relational data is more complex than a flat file.a finiteset offinite relations can always be recast into a single tablealthough often atenormous cost in space.moreoverdenormalization can generate spurious regularities in the dataand it is essential to check the data for such artifactsbefore applying a learning method.finallypotentially infinite concepts can bedealt with by learning rules that are recursivealthough that is beyond the scopeofthis s in an attribute?each individualindependent instance that provides the input to machine learning is characterized by its values on a fixedpredefined set offeatures orattributes.the instances are the rows ofthe tables that we have shown for theweathercontact lensirisand cpu performance problemsand the attributesare the columns.the labor negotiations data was an exceptionwe presentedthis with instances in columns and attributes in rows for space reasons.the use ofa fixed set offeatures imposes another restriction on the kinds ofproblems generally considered in practical data mining.what s in an relation represented as a table.first personsecond examples hereyesall the am page instances have different features? ifthe instances were transportation vehiclesthen number ofwheels is a feature that applies to many vehicles but not to shipsfor examplewhereas number ofmasts might be a feature that applies to shipsbut not to land vehicles.the standard workaround is to make each possiblefeature an attribute and to use a special irrelevant value flag to indicate that aparticular attribute is not available for a particular case.a similar situation ariseswhen the existence ofone feature s name depends on the value ofanother or single.the value ofan attribute for a particular instance is a measurement ofthequantity to which the attribute refers.there is a broad distinction between quan-tities that are numericand ones that are nominal.numeric attributessometimescalled continuousattributesmeasure numbers either real or integer valued.note that the term continuousis routinely abused in this contextinteger-valuedattributes are certainly not continuous in the mathematical sense.nominalattributes take on values in a prespecifiedfinite set ofpossibilities and are some-times called categorical.but there are other possibilities.statistics texts oftenintroduce levels ofmeasurement such as nominalordinalintervaland ratio.nominal quantities have values that are distinct symbols.the values them-selves serve just as labels or names hence the term nominalwhich comes fromthe latin word for name.for examplein the weather data the attribute outlookhas values sunnyovercastand rainy.no relation is implied among thesethree no ordering or distance measure.it certainly does not make sense to addthe values togethermultiply themor even compare their size.a rule using suchan attribute can only test for equality or inequalityas followsoutlook sunny noovercast yesrainy yesordinal quantities are ones that make it possible to rank order the categories.howeveralthough there is a notion oforderingthere is no notion ofdistance.for examplein the weather data the attribute temperaturehas values hotmildand cool.these are ordered.whether you sayhotmildcoolor hotmildcoolis a matter ofconvention it does not matter which is used as long as consis-tency is maintained.what is important is that mild lies between the other two.although it makes sense to compare two valuesit does not make sense to addor subtract them the difference between hotand mildcannot be comparedwith the difference between mildand cool.a rule using such an attribute mightinvolve a comparisonas followstemperature notemperature am page notice that the distinction between nominal and ordinal quantities is notalways straightforward and obvious.indeedthe very example ofan ordinalquantity that we used previouslyoutlookis not completely clearyou mightargue that the three values dohave an ordering overcastbeing somehow inter-mediate between sunnyand rainyas weather turns from good to bad.interval quantities have values that are not only ordered but also measuredin fixed and equal units.a good example is temperatureexpressed in degreessaydegrees fahrenheit rather than on the nonnumeric scale implied by coolmildand hot.it makes perfect sense to talk about the difference between twotemperaturessay and degreesand compare that with the differencebetween another two temperaturessay and degrees.another example isdates.you can talk about the difference between the years and or even the average ofthe years and it doesn t makemuch sense to consider the sum ofthe years and or threetimes the year the starting pointyear completely arbitrary indeedit has changed many times throughout the course ofhis-tory.children sometimes wonder what the year was called in quantities are ones for which the measurement method inherentlydefines a zero point.for examplewhen measuring the distance from one objectto othersthe distance between the object and itselfforms a natural zero.ratioquantities are treated as real numbersany mathematical operations are allowed.it certainly does make sense to talk about three times the distance and even tomultiply one distance by another to get an area.howeverthe question ofwhether there is an inherently defined zero pointdepends on our scientific knowledge it s culture relative.for exampledanielfahrenheit knew no lower limit to temperatureand his scale is an interval one.nowadayshoweverwe view temperature as a ratio scale based on absolute zero.measurement oftime in years since some culturally defined zero such as not a ratio scaleyears since the big bang is.even the zero point ofmoney where we are usually quite happy to say that something cost twice as much assomething else may not be quite clearly defined for those ofus who constantlymax out our credit cards.most practical data mining systems accommodate just two ofthese four levelsofmeasurementnominal and ordinal.nominal attributes are sometimes calledcategoricalenumeratedor discrete.enumeratedis the standard term used incomputer science to denote a categorical data typehoweverthe strict defini-tion ofthe term namelyto put into one-to-one correspondence with thenatural numbers implies an orderingwhich is specifically not implied in themachine learning context.discretealso has connotations ofordering becauseyou often discretize a continuousnumeric quantity.ordinal attributes are generally called numericor perhaps continuousbut without the implication ofmathematical continuity.a special case ofthe nominal scale is the s in an am page which has only two members often designated as trueand falseor yesand noin the weather data.such attributes are sometimes called boolean.machine learning systems can use a wide variety ofother information aboutattributes.for instancedimensional considerations could be used to restrict thesearch to expressions or comparisons that are dimensionally correct.circularordering could affect the kinds oftests that are considered.for examplein atemporal contexttests on a day attribute could involve next dayprevious daynext weekdayand same day next week.partial orderingsthat isgeneralizationor specialization relationsfrequently occur in practical situations.informationofthis kind is often referred to as metadatadata about data.howeverthe kindsofpractical methods used for data mining are rarely capable oftaking metadatainto accountalthough it is likely that these capabilities will develop rapidly inthe future.we return to this in chapter the inputpreparing input for a data mining investigation usually consumes the bulk ofthe effort invested in the entire data mining process.although this book is notreally about the problems ofdata preparationwe want to give you a feeling forthe issues involved so that you can appreciate the complexities.following thatwe look at a particular input file formatthe attribute-relation file format is used in the java package described in part ii.then we considerissues that arise when converting datasets to such a formatbecause there aresome simple practical points to be aware of.bitter experience shows that realdata is often ofdisappointingly low in qualityand careful checking a processthat has become known as datacleaning pays offmany times over.gathering the data togetherwhen beginning work on a data mining problemit is first necessary to bringall the data together into a set ofinstances.we explained the need to denor-malize relational data when describing the family tree example.although itillustrates the basic issuethis self-contained and rather artificial example doesnot really convey a feeling for what the process will be like in practice.in a realbusiness applicationit will be necessary to bring data together from differentdepartments.for examplein a marketing study data will be needed from thesales departmentthe customer billing departmentand the customer servicedepartment.integrating data from different sources usually presents many challenges not deep issues ofprinciple but nasty realities ofpractice.different departmentswill use different styles ofrecord keepingdifferent conventionsdifferent timeperiodsdifferent degrees ofdata aggregationdifferent primary keysand willhave different kinds oferror.the data must be am page cleaned up.the idea ofcompany wide database integration is known as datawarehousing.data warehouses provide a single consistent point ofaccess to cor-porate or organizational datatranscending departmental divisions.they arethe place where old data is published in a way that can be used to inform busi-ness decisions.the movement toward data warehousing is a recognition ofthefact that the fragmented information that an organization uses to support day-to-day operations at a departmental level can have immense strategic valuewhen brought together.clearlythe presence ofa data warehouse is a very usefulprecursor to data miningand ifit is not availablemany ofthe steps involvedin data warehousing will have to be undertaken to prepare the data for mining.often even a data warehouse will not contain all the necessary dataand youmay have to reach outside the organization to bring in data relevant to theproblem at hand.for exampleweather data had to be obtained in the load forecasting example in the last chapterand demographic data is needed formarketing and sales applications.sometimes called overlaydatathis is not nor-mally collected by an organization but is clearly relevant to the data miningproblem.ittoomust be cleaned up and integrated with the other data that hasbeen collected.another practical question when assembling the data is the degree ofaggre-gation that is appropriate.when a dairy farmer decides which cows to sellthemilk production records which an automatic milking machine records twicea day must be aggregated.similarlyraw telephone call data is oflittle use whentelecommunications companies study their clients behaviorthe data must beaggregated to the customer level.but do you want usage by month or by quarterand for how many months or quarters in arrears? selecting the right type andlevel ofaggregation is usually critical for success.because so many different issues are involvedyou can t expect to get it rightthe first time.this is why data assemblyintegrationcleaningaggregatingandgeneral preparation take so long.arff formatwe now look at a standard way ofrepresenting datasets that consist ofinde-pendentunordered instances and do not involve relationships among instancescalled an arfffile.figure shows an arff file for the weather data in table versionwith some numeric features.lines beginning with a are comments.following the comments at the beginning ofthe file are the name ofthe rela-tion and a block defining the attributes humid-itywindyplay?.nominal attributes are followed by the set ofvalues they cantake onenclosed in curly braces.values can include spacesifsothey must beplaced within quotation marks.numeric values are followed by the the am page attributes arff file for the weather data with some numeric featuresrelation weatherattribute outlook sunny overcast rainy temperature numericattribute humidity numericattribute windy true false play? yes no instancessunny false nosunny true noovercast false yesrainy false yesrainy false yesrainy true noovercast true yessunny false nosunny false yesrainy false yessunny true yesovercast true yesovercast false yesrainy true file for the weather data.although the weather problem is to predict the class value play?from the values ofthe other attributesthe class attribute is not dis-tinguished in any way in the data file.the arff format merely givesa datasetit does not specify which ofthe attributes is the one thatis supposed to be predicted.this means that the same file can be usedfor investigating how well each attribute can be predicted from theothersor to find association rulesor for clustering.following the attribute definitions is an that signals thestart ofthe instances in the dataset.instances are written one per linewith values for each attribute in turnseparated by commas.ifa valueis missing it is represented by a single question mark are am page missing values in this dataset.the attribute specifications in arff files allowthe dataset to be checked to ensure that it contains legal values for all attributesand programs that read arff files do this checking automatically.in addition to nominal and numeric attributesexemplified by the weatherdatathe arff format has two further attribute typesstring attributes and dateattributes.string attributes have values that are textual.suppose you have astring attribute that you want to call description.in the block defining the attrib-utesit is specified as followsattribute description stringthenin the instance datainclude any character string in quotation marks quotation marks in your stringuse the standard convention ofpre-ceding each one by a backslash.strings are stored internally in a string tableand represented by their address in that table.thus two strings that contain thesame characters will have the same value.string attributes can have values that are very long even a whole document.to be able to use string attributes for text miningit is necessary to be able tomanipulate them.for examplea string attribute might be converted into manynumeric attributesone for each word in the stringwhose value is the numberoftimes that word appears.these transformations are described in section attributes are strings with a special format and are introduced like thisattribute today datefor an attribute called today.wekathe machine learning software discussedin part ii ofthis bookuses the combined date and time format yyyy-mm-dd-thhmmsswith four digits for the yeartwo each for the month anddaythen the letter tfollowed by the time with two digits for each ofhoursminutesand the data section ofthe filedates are specified as thecorresponding string representation ofthe date and timefor they are specified as stringsdates are converted tonumeric form when the input file is read.dates can also be converted internallyto different formatsso you can have absolute timestamps in the data file anduse transformations to forms such as time ofday or day ofthe week to detectperiodic behavior.sparse datasometimes most attributes have a value for most the instances.for examplemarket basket data records purchases made by supermarket the contains a mechanism for defining a date attribute to have a different format byincluding a special string in the attribute am page matter how big the shopping expeditioncustomers never purchase more thana tiny portion ofthe items a store offers.the market basket data contains thequantity ofeach item that the customer purchasesand this is zero for almostall items in stock.the data file can be viewed as a matrix whose rows andcolumns represent customers and stock itemsand the matrix is sparse nearly all its elements are zero.another example occurs in text miningin whichthe instances are documents.herethe columns and rows represent documentsand wordsand the numbers indicate how many times a particular word appearsin a particular document.most documents have a rather small vocabularysomost entries are zero.it can be impractical to represent each element ofa sparse matrix explicitlywriting each value in orderas class a class b insteadthe nonzero attributes can be explicitly identified by attribute numberand their value class a class b instance is enclosed in curly braces and contains the index number ofeachnonzero attribute start from and its value.sparse data files have thesame by an the datasection is different and contains specifications in braces such as those shownpreviously.note that the omitted values have a value they are not missing values! ifa value is unknownit must be explicitly represented witha question mark.attribute typesarff files accommodate the two basic data typesnominal and numeric.stringattributes and date attributes are effectively nominal and numericrespectivelyalthough before they are used strings are often converted into a numeric formsuch as a word vector.but how the two basic types are interpreted depends onthe learning method being used.for examplemost methods treat numericattributes as ordinal scales and only use less-than and greater-than comparisonsbetween the values.howeversome treat them as ratio scales and use distancecalculations.you need to understand how machine learning methods workbefore using them for data mining.ifa learning method treats numeric attributes as though they are measuredon ratio scalesthe question ofnormalization arises.attributes are often nor-malized to lie in a fixed rangesayfrom zero to oneby dividing all values bythe maximum value encountered or by subtracting the minimum value am page dividing by the range between the maximum and the minimum values.anothernormalization technique is to calculate the statistical mean and standard deviation ofthe attribute valuessubtract the mean from each valueand dividethe result by the standard deviation.this process is called standardizinga sta-tistical variable and results in a set ofvalues whose mean is zero and standarddeviation is one.some learning methods for examplevarieties ofinstance-based learningand regression methods deal only with ratio scales because they calculate the distance between two instances based on the values oftheir attributes.ifthe actual scale is ordinala numeric distance function must be defined.oneway ofdoing this is to use a two-level distanceone ifthe two values are differ-ent and zero ifthey are the same.any nominal quantity can be treated as numericby using this distance function.howeverit is rather a crude technique and con-ceals the true degree ofvariation between instances.another possibility is to gen-erate several synthetic binary attributes for each nominal attributewe return tothis in section when we look at the use oftrees for numeric prediction.sometimes there is a genuine mapping between nominal quantities andnumeric scales.for examplepostal zip codes indicate areas that could be rep-resented by geographic coordinatesthe leading digits oftelephone numbersmay do sotoodepending on where you live.the first two digits ofa student sidentification number may be the year in which she first enrolled.it is very common for practical datasets to contain nominal values that arecoded as integers.for examplean integer identifier may be used as a code foran attribute such as partnumberyet such integers are not intended for use inless-than or greater-than comparisons.ifthis is the caseit is important tospecify that the attribute is nominal rather than numeric.it is quite possible to treat an ordinal quantity as though it were nominal.indeedsome machine learning methods only deal with nominal elements.forexamplein the contact lens problem the age attribute is treated as nominalandthe rules generated included the followingif age and astigmatic and tear production rate then recommendation age and astigmatic and tear production rate then recommendation in fact agespecified in this wayis really an ordinal quantity for which thefollowing is trueyoung were treated as ordinalthe two rules could be collapsed into oneif age pre-presbyopic and astigmatic and tear production rate then recommendation the am page which is a more compactand hence more satisfactoryway ofsaying the samething.missing valuesmost datasets encountered in practicesuch as the labor negotiations data intable missing values.missing values are frequently indicated by out-of-range entriesperhaps a negative number in a numeric field that isnormally only positive or a in a numeric field that can never normally be nominal attributesmissing values may be indicated by blanks or dashes.sometimes different kinds ofmissing values are distinguished vs.irrelevant values and perhaps represented by different negative integers have to think carefully about the significance ofmissing values.they mayoccur for several reasonssuch as malfunctioning measurement equipmentchanges in experimental design during data collectionand collation ofseveralsimilar but not identical datasets.respondents in a survey may refuse to answercertain questions such as age or income.in an archaeological studya specimensuch as a skull may be damaged so that some variables cannot be measured.in a biologic oneplants or animals may die before all variables have been measured.what do these things meanabout the example under consideration?might the skull damage have some significance in itselfor is it just because ofsome random event? does the plants early death have some bearing on the caseor not?most machine learning methods make the implicit assumption that there isno particular significance in the fact that a certain instance has an attribute valuemissingthe value is simply not known.howeverthere may be a good reasonwhy the attribute s value is unknown perhaps a decision was madeon the evi-dence availablenot to perform some particular test and that might conveysome information about the instance other than the fact that the value is simplymissing.ifthis is the casethen it would be more appropriate to record nottestedas another possible value for this attribute or perhaps as another attribute in thedataset.as the preceding examples illustrateonly someone familiar with the datacan make an informed judgment about whether a particular value being missinghas some extra significance or whether it should simply be coded as an ordinarymissing value.ofcourseifthere seem to be several types ofmissing valuethatis prima facie evidence that something is going on that needs to be investigated.ifmissing values mean that an operator has decided not to make a particu-lar measurementthat may convey a great deal more than the mere fact that thevalue is unknown.for examplepeople analyzing medical databases havenoticed that cases mayin some circumstancesbe diagnosable simply from thetests that a doctor decides to make regardless ofthe outcome ofthe am page a record ofwhich values are missing is all that is needed for a complete diagnosis the actual values can be ignored completely!inaccurate valuesit is important to check data mining files carefully for rogue attributes andattribute values.the data used for mining has almost certainly not been gath-ered expressly for that purpose.when originally collectedmany ofthe fieldsprobably didn t matter and were left blank or unchecked.provided that it doesnot affect the original purpose ofthe datathere is no incentive to correct it.howeverwhen the same database is used for miningthe errors and omissionssuddenly start to assume great significance.for examplebanks do not really needto know the age oftheir customersso their databases may contain many missingor incorrect values.but age may be a very significant feature in mined rules.typographic errors in a dataset will obviously lead to incorrect values.oftenthe value ofa nominal attribute is misspelledcreating an extra possible valuefor that attribute.or perhaps it is not a misspelling but different names for thesame thingsuch as pepsi and pepsi cola.obviously the point ofa definedformat such as arff is to allow data files to be checked for internal consistency.howevererrors that occur in the original data file are often preserved throughthe conversion process into the file that is used for data miningthus the list ofpossible values that each attribute takes on should be examined carefully.typographic or measurement errors in numeric values generally cause out-liers that can be detected by graphing one variable at a time.erroneous valuesoften deviate significantly from the pattern that is apparent in the remainingvalues.sometimeshoweverinaccurate values are hard to findparticularlywithout specialist domain knowledge.duplicate data presents another source oferror.most machine learning toolswill produce different results ifsome ofthe instances in the data files are dupli-catedbecause repetition gives them more influence on the result.people often make deliberate errors when entering personal data into data-bases.they might make minor changes in the spelling oftheir street to try toidentify whether the information they have provided was sold to advertisingagencies that burden them with junk mail.they might adjust the spelling oftheir name when applying for insurance ifthey have had insurance refused inthe past.rigid computerized data entry systems often impose restrictions thatrequire imaginative workarounds.one story tells ofa foreigner renting a vehiclein the united states.being from abroadhe had no zip codeyet the computerinsisted on onein desperation the operator suggested that he use the zip codeofthe rental agency.ifthis is common practicefuture data mining projects maynotice a cluster ofcustomers who apparently live in the same district as the agency!similarlya supermarket checkout operator sometimes uses his own the am page buyer card when the customer does not supply oneeither so that the customercan get a discount that would otherwise be unavailable or simply to accumulatecredit points in the cashier s account.only a deep semantic knowledge ofwhat isgoing on will be able to explain systematic data errors such as these.finallydata goes stale.many items change as circumstances change.forexampleitems in mailing lists namesaddressestelephone numbersand soon change frequently.you need to consider whether the data you are miningis still current.getting to know your datathere is no substitute for getting to know your data.simple tools that show his-tograms ofthe distribution ofvalues ofnominal attributesand graphs ofthevalues ofnumeric attributes sorted or simply graphed against instancenumberare very helpful.these graphical visualizations ofthe data make iteasy to identify outlierswhich may well represent errors in the data file orarcane conventions for coding unusual situationssuch as a missing year as a missing weight as no one has thought to tell you about.domainexperts need to be consulted to explain anomaliesmissing valuesthe signifi-cance ofintegers that represent categories rather than numeric quantitiesandso on.pairwise plots ofone attribute against anotheror each attribute againstthe class valuecan be extremely revealing.data cleaning is a time-consuming and labor-intensive procedure but onethat is absolutely necessary for successful data mining.with a large datasetpeople often give up how can they possibly check it all? insteadyou shouldsample a few instances and examine them carefully.you ll be surprised at whatyou find.time looking at your data is always well readingpyle provides an extensive guide to data preparation for data mining.there is also a great deal ofcurrent interest in data warehousing and the prob-lems it entails.kimball offers the best introduction to these that we knowof.cabena et estimate that data preparation accounts for oftheeffort involved in a data mining applicationand they write at some length aboutthe problems involved.the area ofinductive logic programmingwhich deals with finite and infi-nite relationsis covered by bergadano and gunetti different levelsofmeasurement for attributes were introduced by stevens and are welldescribed in the manuals for statistical packages such as spss et am page most ofthe techniques in this book produce easily comprehensible descriptionsofthe structural patterns in the data.before looking at how these techniquesworkwe have to see how structural patterns can be expressed.there are manydifferent ways for representing the patterns that can be discovered by machinelearningand each one dictates the kind oftechnique that can be used to inferthat output structure from data.once you understand how the output is representedyou have come a long way toward understanding how it can be generated.we saw many examples ofdata mining in chapter these cases the outputtook the form ofdecision trees and classification ruleswhich are basic knowl-edge representation styles that many machine learning methods use.knowledgeis really too imposing a word for a decision tree or a collection ofrulesand byusing it we don t really mean to imply that these structures vie with the realkindofknowledge that we carry in our headsit s just that we need some word torefer to the structures that learning methods produce.there are more complexvarieties ofrules that allow exceptions to be specifiedand ones that can am page relations among the values ofthe attributes ofdifferent instances.special formsoftrees can be used for numeric predictiontoo.instance-based representationsfocus on the instances themselves rather than rules that govern their attributevalues.finallysome learning methods generate clusters ofinstances.these dif-ferent knowledge representation methods parallel the different kinds oflearn-ing problems introduced in chapter tablesthe simplestmost rudimentary way ofrepresenting the output from machinelearning is to make it just the same as the input a decision table.for exampletable is a decision table for the weather datayou just look up the appro-priate conditions to decide whether or not to play.less triviallycreating a deci-sion table might involve selecting some ofthe attributes.iftemperatureisirrelevant to the decisionfor examplea smallercondensed table with thatattribute missing would be a better guide.the problem isofcourseto decidewhich attributes to leave out without affecting the final treesa divide-and-conquer approach to the problem oflearning from a set ofinde-pendent instances leads naturally to a style ofrepresentation called a decisiontree.we have seen some examples ofdecision treesfor the contact lens and labor negotiations datasets.nodes in a decision tree involvetesting a particular attribute.usuallythe test at a node compares an attributevalue with a constant.howeversome trees compare two attributes with eachotheror use some function ofone or more attributes.leafnodes give a classi-fication that applies to all instances that reach the leafor a set ofclassificationsor a probability distribution over all possible classifications.to classify anunknown instanceit is routed down the tree according to the values oftheattributes tested in successive nodesand when a leafis reached the instance isclassified according to the class assigned to the leaf.ifthe attribute that is tested at a node is a nominal onethe number ofchil-dren is usually the number ofpossible values ofthe attribute.in this casebecause there is one branch for each possible valuethe same attribute will notbe retested further down the tree.sometimes the attribute values are dividedinto two subsetsand the tree branches just two ways depending on which subsetthe value lies in the treein that casethe attribute might be tested more thanonce in a path.ifthe attribute is numericthe test at a node usually determines whether itsvalue is greater or less than a predetermined constantgiving a two-way am page alternativelya three-way split may be usedin which case there are several dif-ferent possibilities.ifmissing valueis treated as an attribute value in its ownrightthat will create a third branch.an alternative for an integer-valued attrib-ute would be a three-way split into less thanequal toand greater than.an alter-native for a real-valued attributefor which equal tois not such a meaningfuloptionwould be to test against an interval rather than a single constantagaingiving a three-way splitbelowwithinand above.a numeric attribute is oftentested several times in any given path down the tree from root to leafeach testinvolving a different constant.we return to this when describing the handlingofnumeric attributes in section values pose an obvious problem.it is not clear which branch shouldbe taken when a node tests an attribute whose value is missing.sometimesasdescribed in section valueis treated as an attribute value in its ownright.ifthis is not the casemissing values should be treated in a special wayrather than being considered as just another possible value that the attributemight take.a simple solution is to record the number ofelements in the train-ing set that go down each branch and to use the most popular branch ifthevalue for a test instance is missing.a more sophisticated solution is to notionally split the instance into piecesand send part ofit down each branch and from there right on down to the leavesofthe subtrees involved.the split is accomplished using a numeric weightbetween zero and oneand the weight for a branch is chosen to be proportionalto the number oftraining instances going down that branchall weightssumming to one.a weighted instance may be further split at a lower node.even-tuallythe various parts ofthe instance will each reach a leafnodeand the deci-sions at these leafnodes must be recombined using the weights that havepercolated down to the leaves.we return to this in section is instructive and can even be entertaining to build a decision tree for adataset manually.to do so effectivelyyou need a good way ofvisualizing thedata so that you can decide which are likely to be the best attributes to test andwhat an appropriate test might be.the weka explorerdescribed in part iihasa user classifier facility that allows users to construct a decision tree interac-tively.it presents you with a scatter plot ofthe data against two selected attrib-uteswhich you choose.when you find a pair ofattributes that discriminatesthe classes wellyou can create a two-way split by drawing a polygon around theappropriate data points on the scatter plot.for examplein figure the user is operating on a dataset with threeclassesthe iris datasetand has found two attributespetallengthand petalwidththat do a good job ofsplitting up the classes.a rectangle has been drawnman-uallyto separate out one ofthe classes versicolor.then the user switchesto the decision tree view in figure to see the tree so far.the left-handleafnode contains predominantly irises ofone type am page representationabfigure a decision tree interactivelya creating a rectangular testinvolving petallengthand petalwidthand the resulting decision am page nated by only two virginicasthe right-hand one contains predominantly twotypes setosa and virginicacontaminated by only two versicolors.the userwill probably select the right-hand leafand work on it nextsplitting it furtherwith another rectangle perhaps based on a different pair ofattributesalthoughfrom figure two look pretty good.section explains how to use weka s user classifier facility.most peopleenjoy making the first few decisions but rapidly lose interest thereafterand onevery useful option is to select a machine learning method and let it take over atany point in the decision tree.manual construction ofdecision trees is a goodway to get a feel for the tedious business ofevaluating different combinationsofattributes to split rulesclassification rules are a popular alternative to decision treesand we havealready seen examples for the weather lens soybean datasets.the antecedentor preconditionofa rule is a series oftests just like the tests at nodes in decision treesand the con-sequentor conclusiongives the class or classes that apply to instances coveredby that ruleor perhaps gives a probability distribution over the classes.gener-allythe preconditions are logically anded togetherand all the tests mustsucceed ifthe rule is to fire.howeverin some rule formulations the precondi-tions are general logical expressions rather than simple conjunctions.we oftenthink ofthe individual rules as being effectively logically ored togetherifanyone appliesthe class probability distribution given in its conclusion isapplied to the instance.howeverconflicts arise when several rules with differ-ent conclusions applywe will return to this shortly.it is easy to read a set ofrules directly offa decision tree.one rule is gener-ated for each leaf.the antecedent ofthe rule includes a condition for every nodeon the path from the root to that leafand the consequent ofthe rule is the class assigned by the leaf.this procedure produces rules that are unambigu-ous in that the order in which they are executed is irrelevant.howeveringeneralrules that are read directly offa decision tree are far more complex thannecessaryand rules derived from trees are usually pruned to remove redundanttests.because decision trees cannot easily express the disjunction implied amongthe different rules in a settransforming a general set ofrules into a tree is notquite so straightforward.a good illustration ofthis occurs when the rules havethe same structure but different attributeslikeif a and b then xif c and d then am page then it is necessary to break the symmetry and choose a single test for the rootnode.iffor exampleais chosenthe second rule mustin effectbe repeatedtwice in the treeas shown in figure is known as the replicated subtreeproblem.the replicated subtree problem is sufficiently important that it is worthlooking at a couple more examples.the diagram on the left offigure showsan exclusive-orfunction for which the output is not both.to make this into a treeyou have to split on one attribute firstleading to astructure like the one shown in the center.in contrastrules can faithfully reflectthe true symmetry ofthe problem with respect to the attributesas shown onthe representationabycnxycndynxyndynxynfigure tree for a simple am page in this example the rules are not notably more compact than the tree.in factthey are just what you would get by reading rules offthe tree in the obviousway.but in other situationsrules are much more compact than treesparticu-larly ifit is possible to have a default rule that covers cases not specified by theother rules.for exampleto capture the effect ofthe rules in figure inwhich there are four attributesxyzand wthat can each be requiresthe tree shown on the right.each ofthe three small gray triangles to the upperright should actually contain the whole three-level subtree that is displayed ingraya rather extreme example ofthe replicated subtree problem.this is a dis-tressingly complex description ofa rather simple concept.one reason why rules are popular is that each rule seems to represent an inde-pendent nugget ofknowledge.new rules can be added to an existing rule setwithout disturbing ones already therewhereas to add to a tree structure mayrequire reshaping the whole tree.howeverthis independence is something ofan illusionbecause it ignores the question ofhow the rule set is executed.weexplained earlier page the fact that ifrules are meant to be interpretedin orderas a decision list some ofthemtaken individually and out ofcontextmay be incorrect.on the other handifthe order ofinterpretation is supposedto be immaterialthen it is not clear what to do when different rules lead to dif-ferent conclusions for the same instance.this situation cannot arise for rulesthat are read directly offa decision tree because the redundancy included in thestructure ofthe rules prevents any ambiguity in interpretation.but it does arisewhen rules are generated in other ways.ifa rule set gives multiple classifications for a particular exampleone solu-tion is to give no conclusion at all.another is to count how often each rule fireson the training data and go with the most popular one.these strategies can and then class a if and then class a if and then class b if and then class b figure exclusive-or am page to radically different results.a different problem occurs when an instance isencountered that the rules fail to classify at all.againthis cannot occur withdecision treesor with rules read directly offthembut it can easily happen withgeneral rule sets.one way ofdealing with this situation is to fail to classify suchan exampleanother is to choose the most frequently occurring class as a default.againradically different results may be obtained for these strategies.individ-ual rules are simpleand sets ofrules seem deceptively simple but given justa set ofrules with no additional informationit is not clear how it should beinterpreted.a particularly straightforward situation occurs when rules lead to a class thatis boolean no and when only rules leading to one outcome are expressed.the assumption is that ifa particular instance is not in and then class a if and then class a otherwise class b figure tree with a replicated am page yesthen it must be in class no a form ofclosed world assumption.ifthis isthe casethen rules cannot conflict and there is no ambiguity in rule interpre-tationany interpretation strategy will give the same result.such a set ofrulescan be written as a logic expression in what is called disjunctive normal formthat isas a disjunction ofconjunctive conditions.it is this simple special case that seduces people into assuming rules are veryeasy to deal withbecause here each rule really does operate as a newinde-pendent piece ofinformation that contributes in a straightforward way to thedisjunction.unfortunatelyit only applies to boolean outcomes and requires theclosed world assumptionand both these constraints are unrealistic in mostpractical situations.machine learning algorithms that generate rules invariablyproduce ordered rule sets in multiclass situationsand this sacrifices any possi-bility ofmodularity because the order ofexecution is rulesassociation rules are really no different from classification rules except that theycan predict any attributenot just the classand this gives them the freedom topredict combinations ofattributes too.alsoassociation rules are not intendedto be used together as a setas classification rules are.different association rulesexpress different regularities that underlie the datasetand they generally predictdifferent things.because so many different association rules can be derived from even a tinydatasetinterest is restricted to those that apply to a reasonably large number ofinstances and have a reasonably high accuracy on the instances to which theyapply to.the coverageofan association rule is the number ofinstances for whichit predicts correctly this is often called its support.its accuracy often calledconfidence is the number ofinstances that it predicts correctlyexpressed as aproportion ofall instances to which it applies.for examplewith the ruleif temperature then humidity coverage is the number ofdays that are both cool and have normal humid-ity days in the data oftable the accuracy is the proportion ofcooldays that have normal humidity in this case.it is usual to specifyminimum coverage and accuracy values and to seek only those rules whose cov-erage and accuracy are both at least these specified minima.in the weather datafor examplethere are rules whose coverage and accuracy are at least may also be convenient to specify coverage as a percent-age ofthe total number ofinstances instead.association rules that predict multiple consequences must be interpretedrather carefully.for examplewith the weather data in table we saw this am page if windy and play then outlook humidity is notjust a shorthand expression for the two separate rulesif windy and play then outlook windy and play then humidity indeed implies that these exceed the minimum coverage and accuracyfigures but it also implies more.the original rule means that the number ofexamples that are nonwindynonplayingwith sunny outlook and high humidityis at least as great as the specified minimum coverage figure.it also means thatthe number ofsuch daysexpressed as a proportion ofnonwindynonplaying daysis at least the specified minimum accuracy figure.this implies that the ruleif humidity and windy and play then outlook holdsbecause it has the same coverage as the original ruleand its accu-racy must be at least as high as the original rule s because the number ofhigh-humiditynonwindynonplaying days is necessarily less than that ofnonwindynonplaying days which makes the accuracy greater.as we have seenthere are relationships between particular association rulessome rules imply others.to reduce the number ofrules that are producedin cases where several rules are related it makes sense to present only thestrongest one to the user.in the preceding exampleonly the first rule shouldbe with exceptionsreturning to classification rulesa natural extension is to allow them to haveexceptions.then incremental modifications can be made to a rule set by express-ing exceptions to existing rules rather than reengineering the entire set.forexampleconsider the iris problem described earlier.suppose a new flower wasfound with the dimensions given in table an expert declared it to bean instance ofiris setosa.ifthis flower was classified by the rules given in for this problemit would be misclassified by two representationtable new iris flower.sepal length width length width am page if petal length and petal length then iris versicolorif petal length and petal length and petal width then iris versicolorthese rules require modification so that the new instance can be treated correctly.howeversimply changing the bounds for the attribute-value tests in these rules may not suffice because the instances used to create therule set may then be misclassified.fixing up a rule set is not as simple as itsounds.instead ofchanging the tests in the existing rulesan expert might be con-sulted to explain why the new flower violates themreceiving explanations thatcould be used to extend the relevant rules only.for examplethe first ofthesetwo rules misclassifies the new iris setosaas an instance ofthe genus iris versi-color.instead ofaltering the bounds on any ofthe inequalities in the ruleanexception can be made based on some other attributeif petal length and petal length then iris versicolor exceptif petal width then iris setosathis rule says that a flower is iris versicolorifits petal length is between exceptwhen its petal width is less than which case it isiris setosa.ofcoursewe might have exceptions to the exceptionsexceptions to theseand so ongiving the rule set something ofthe character ofa tree.as well as being used to make incremental changes to existing rule setsrules withexceptions can be used to represent the entire concept description in the firstplace.figure shows a set ofrules that correctly classify all examples in the irisdataset given earlier rules are quite difficult to compre-hend at first.let s follow them through.a default outcome has been chosenirissetosaand is shown in the first line.for this datasetthe choice ofdefault israther arbitrary because there are examples ofeach type.normallythe mostfrequent outcome is chosen as the default.subsequent rules give exceptions to this default.the first if...thenon through a condition that leads to the classification iris versicolor.howeverthere are two exceptions to this rule through we willdeal with in a moment.ifthe conditions on lines and failthe elseclause online is reachedwhich essentially specifies a second exception to the originaldefault.ifthe condition on line holdsthe classification is iris is an exception to this rule lines and return to the exception on lines through overrides the iris ver-sicolorconclusion on line ifeither ofthe tests on lines and holds.as ithappensthese two exceptions both lead to the same conclusioniris with am page and final exception is the one on lines and over-rides the iris virginicaconclusion on line when the condition on line ismetand leads to the classification iris versicolor.you will probably need to ponder these rules for some minutes before itbecomes clear how they are intended to be read.although it takes some time to get used to reading themsorting out the excepts and if...then...elsesbecomes easier with familiarity.people often think ofreal problems interms ofrulesexceptionsand exceptions to the exceptionsso it is often a goodway to express a complex rule set.but the main point in favor ofthis way ofrepresenting rules is that it scales up well.although the whole rule set is a littlehard to comprehendeach individual conclusioneach individual thenstate-mentcan be considered just in the context ofthe rules and exceptions that leadto itwhereas with decision listsall prior rules need to be reviewed to deter-mine the precise effect ofan individual rule.this locality property is crucialwhen trying to understand large rule sets.psychologicallypeople familiar withthe data think ofa particular set ofcasesor kind ofcasewhen looking at anyone conclusion in the exception structureand when one ofthese cases turnsout to be an exception to the conclusionit is easy to add an exceptclause tocater for it.it is worth pointing out that the default...except if...then...structure islogically equivalent to if...then...else...where the elseis unconditional andspecifies exactly what the default did.an unconditional elseisofcourseadefault.note that there are no unconditional elses in the preceding rules. representationdefault if petal-length and petal-length and petal-width then iris-versicolor except if petal-length and petal-width then iris-virginica else if sepal-length and sepal-width then iris-virginica else if petal-length then iris-virginica except if petal-length and sepal-length then iris-versicolor for the iris am page icallythe exception-based rules can very simply be rewritten in terms ofregularif...then...elseclauses.what is gained by the formulation in terms ofexcep-tions is not logicalbut psychological.we assume that the defaults and the teststhat occur early apply more widely than the exceptions further down.ifthis isindeed true for the domainand the user can see that it is plausiblethe expres-sion in terms ofcommon rules and exceptions will be easier to graspthan a differentbut logically involving relationswe have assumed implicitly that the conditions in rules involve testing anattribute value against a constant.such rules are called propositionalbecause theattribute-value language used to define them has the same power as what logi-cians call the propositional calculus.in many classification taskspropositionalrules are sufficiently expressive for conciseaccurate concept descriptions.theweathercontact lens recommendationiris typeand acceptability oflabor con-tract datasets mentioned previouslyfor exampleare well described by propo-sitional rules.howeverthere are situations in which a more expressive form ofrule would provide a more intuitive and concise concept descriptionand theseare situations that involve relationships between examples such as those encoun-tered in section take a concrete examplewe have the set ofeight building blocksofthe various shapes and sizes illustrated in figure we wish to learnthe concept ofstanding.this is a classic two-class problem with classes stand-ingand lying.the four shaded blocks are positive oftheconceptand the unshaded blocks are negative only involving standinglyingfigure shapes am page mation the learning algorithm will be given is the widthheightand number ofsidesofeach block.the training data is shown in table propositional rule set that might be produced for this data isif width and height then lyingif height then standingin case you re is chosen as the breakpoint for widthbecause it ishalfway between the width ofthe thinnest lying blocknamely the widthofthe fattest standing block whose height is less than ischosen as the breakpoint for heightbecause it is halfway between the height ofthe tallest lying blocknamely the shortest standing block whose widthis greater than is common to place numeric thresholds halfwaybetween the values that delimit the boundaries ofa concept.although these two rules work well on the examples giventhey are not verygood.many new blocks would not be classified by either rule withwidth and height it is easy to devise many legitimate blocks that therules would not fit.a person classifying the eight blocks would probably notice that standing blocks are those that are taller than they are wide. this rule does not compare attribute values with constantsit compares attributes with eachotherif width then lyingif height then standingthe actual values ofthe heightand widthattributes are not importantjust theresult ofcomparing the two.rules ofthis form are called relationalbecausethey express relationships between attributesrather than propositionalwhichdenotes a fact about just one representationtable data for the shapes am page standard relations include equality inequality for nominal attributesand less than and greater than for numeric ones.although relational nodescould be put into decision trees just as relational conditions can be put intorulesschemes that accommodate relations generally use the rule rather than thetree representation.howevermost machine learning methods do not considerrelational rules because there is a considerable cost in doing so.one way ofallowing a propositional method to make use ofrelations is to add extrasec-ondary attributes that say whether two primary attributes are equal or notorgive the difference between them ifthey are numeric.for examplewe mightadd a binary attribute is width table attributes are oftenadded as part ofthe data engineering process.with a seemingly rather small further enhancementthe expressive power ofthe relational knowledge representation can be extended very greatly.the trick is to express rules in a way that makes the role ofthe instanceexplicitif widthblock then lyingblockif heightblock then standingblockalthough this does not seem like much ofan extensionit is ifinstances canbe decomposed into parts.for exampleifa toweris a pile ofblocksone on topofthe otherthen the fact that the topmost block ofthe tower is standing canbe expressed byif heighttower.top then standingtower.topheretower.topis used to refer to the topmost block.so farnothing has beengained.but iftower.restrefers to the rest ofthe towerthen the fact that the toweris composed entirelyofstanding blocks can be expressed by the rulesif heighttower.top and standingtower.restthen standingtowerthe apparently minor addition ofthe condition standingtower.restis a recur-sive expression that will turn out to be true only ifthe rest ofthe tower is com-posed ofstanding blocks.a recursive application ofthe same rule will test this.ofcourseit is necessary to ensure that the recursion bottoms out properlyby adding a further rulesuch asif tower then standingtower.topwith this additionrelational rules can express concepts that cannot possibly beexpressed propositionallybecause the recursion can take place over arbitrarilylong lists ofobjects.sets ofrules such as this are called logic programsand thisarea ofmachine learning is called inductive logic programming.we will not betreating it further in this involving am page for numeric predictionthe kind ofdecision trees and rules that we have been looking at are designedfor predicting categories rather than numeric quantities.when it comes to pre-dicting numeric quantitiesas with the cpu performance data in table kind oftree or rule representation can be usedbut the leafnodes ofthetreeor the right-hand side ofthe ruleswould contain a numeric value that isthe average ofall the training set values to which the leafor ruleapplies.because statisticians use the term regressionfor the process ofcomputing anexpression that predicts a numeric quantitydecision trees with averagednumeric values at the leaves are called regression trees.figure shows a regression equation for the cpu performance dataandfigure shows a regression tree.the leaves ofthe tree are numbers thatrepresent the average outcome for instances that reach the leaf.the tree is muchlarger and more complex than the regression equationand ifwe calculate theaverage ofthe absolute values ofthe errors between the predicted and the actualcpu performance measuresit turns out to be significantly less for the tree thanfor the regression equation.the regression tree is more accurate because asimple linear model poorly represents the data in this problem.howeverthetree is cumbersome and difficult to interpret because ofits large size.it is possible to combine regression equations with regression is a tree whose leaves contain linear expressions that isregression equa-tions rather than single predicted values.this is confusingly calleda model tree.figure contains the six linear models that belong at the sixleaveslabeled through model tree approximates continuousfunctions by linear patches a more sophisticated representation than eitherlinear regression or regression trees.although the model tree is smaller andmore comprehensible than the regression treethe average error values on thetraining data are lower.howeverwe will see in chapter that calculating theaverage error on the training set is not in general a good way ofassessing the performance representationthe simplest form oflearning is plain memorizationor rote learning.once aset oftraining instances has been memorizedon encountering a new instancethe memory is searched for the training instance that most strongly resemblesthe new one.the only problem is how to interpret resembles will explainthat shortly.firsthowevernote that this is a completely different way ofrep-resenting the knowledge extracted from a set ofinstancesjust store theinstances themselves and operate by relating new instances whose class am page mmax mmax cach myct mmin cachmmaxchmaxbchmin chmin cach cach chmin chmaxprp chmaxafigure for the cpu performance dataa linear regressionb regressiontreeand model am page unknown to existing ones whose class is known.instead oftrying to create ruleswork directly from the examples themselves.this is known as instance-basedlearning.in a sense all the other learning methods are instance-based toobecause we always start with a set ofinstances as the initial training informa-tion.but the instance-based knowledge representation uses the instances them-selves to represent what is learnedrather than inferring a rule set or decisiontree and storing it instead.in instance-based learningall the real work is done when the time comes toclassify a new instance rather than when the training set is processed.in a sensethenthe difference between this method and the others we have seen is the timeat which the learning takes place.instance-based learning is lazydeferring thereal work as long as possiblewhereas other methods are eagerproducing a gen-eralization as soon as the data has been seen.in instance-based learningeachnew instance is compared with existing ones using a distance metricand theclosest existing instance is used to assign the class to the new one.this is calledthe nearest-neighborclassification method.sometimes more than one nearestneighbor is usedand the majority class ofthe closest kneighbors the dis-tance-weighted averageifthe class is numeric is assigned to the new instance.this is termed the k-nearest-neighbormethod.computing the distance between two examples is trivial when examples havejust one numeric attributeit is just the difference between the two attributevalues.it is almost as straightforward when there are several numeric attributesgenerallythe standard euclidean distance is used.howeverthis assumes thatthe attributes are normalized and are ofequal importanceand one ofthe mainproblems in learning is to determine which are the important features.when nominal attributes are presentit is necessary to come up with a dis-tance between different values ofthat attribute.what are the distances betweensaythe values redgreenand blue?usually a distance ofzero is assigned ifthevalues are identicalotherwisethe distance is one.thus the distance betweenredand redis zero but that between redand greenis one.howeverit may bedesirable to use a more sophisticated representation ofthe attributes.forexamplewith more colors one could use a numeric measure ofhue in colorspacemaking yellow closer to orangethan it is to greenand ochercloser still.some attributes will be more important than othersand this is usuallyreflected in the distance metric by some kind ofattribute weighting.derivingsuitable attribute weights from the training set is a key problem in instance-based learning.it may not be necessaryor desirableto store allthe training instances.forone thingthis may make the nearest-neighbor calculation unbearably slow.foranotherit may consume unrealistic amounts ofstorage.generallysome regionsofattribute space are more stable than others with regard to classand just am page few exemplars are needed inside stable regions.for exampleyou might expectthe required density ofexemplars that lie well inside class boundaries to bemuch less than the density that is needed near class boundaries.deciding whichinstances to save and which to discard is another key problem in instance-basedlearning.an apparent drawback to instance-based representations is that they do notmake explicit the structures that are learned.in a sense this violates the notionof learning that we presented at the beginning ofthis bookinstances do notreally describe the patterns in data.howeverthe instances combine with thedistance metric to carve out boundaries in instance space that distinguish oneclass from anotherand this is a kind ofexplicit representation ofknowledge.for examplegiven a single instance ofeach oftwo classesthe nearest-neigh-bor rule effectively splits the instance space along the perpendicular bisector ofthe line joining the instances.given several instances ofeach classthe space isdivided by a set oflines that represent the perpendicular bisectors ofselectedlines joining an instance ofone class to one ofanother class.figure illus-trates a nine-sided polygon that separates the filled-circle class from the open-circle class.this polygon is implicit in the operation ofthe nearest-neighborrule.when training instances are discardedthe result is to save just a few proto-typical examples ofeach class.figure shows as dark circles only theexamples that actually get used in nearest-neighbor decisionsthe others gray ones can be discarded without affecting the result.these prototypi-cal examples serve as a kind ofexplicit knowledge representation.some instance-based representations go further and explicitly generalize theinstances.typicallythis is accomplished by creating rectangular regions thatenclose examples ofthe same class.figure shows the rectangular regionsthat might be produced.unknown examples that fall within one ofthe rectan-gles will be assigned the corresponding classones that fall outside all rectan-gles will be subject to the usual nearest-neighbor rule.ofcourse this ways ofpartitioning the instance am page different decision boundaries from the straightforward nearest-neighbor ruleas can be seen by superimposing the polygon in figure onto the rectan-gles.any part ofthe polygon that lies within a rectangle will be chopped offandreplaced by the rectangle s boundary.rectangular generalizations in instance space are just like rules with a specialform ofconditionone that tests a numeric variable against an upper and lowerbound and selects the region in between.different dimensions ofthe rectanglecorrespond to tests on different attributes being anded together.choosingsnugly fitting rectangular regions as tests leads to much more conservative rulesthan those generally produced by rule-based machine learning methodsbecause for each boundary ofthe regionthere is an actual instance that lies onor just inside that boundary.tests such as xawhere xis an attribute valueand ais a constant encompass an entire half-space they apply no matter howsmall xis as long as it is less than a.when doing rectangular generalization ininstance space you can afford to be conservative because ifa new example isencountered that lies outside all regionsyou can fall back on the nearest-neigh-bor metric.with rule-based methods the example cannot be classifiedorreceives just a default classificationifno rules apply to it.the advantage ofmoreconservative rules is thatalthough incompletethey may be more perspicuousthan a complete set ofrules that covers all cases.finallyensuring that theregions do not overlap is tantamount to ensuring that at most one rule can applyto an exampleeliminating another ofthe difficulties ofrule-based systems what to do when several rules apply.a more complex kind ofgeneralization is to permit rectangular regions tonest one within another.then a region that is basically all one class can containan inner region ofa different classas illustrated in figure is possibleto allow nesting within nesting so that the inner region can itselfcontain its owninner region ofa different class perhaps the original class ofthe outer region.this is analogous to allowing rules to have exceptions and exceptions to theexceptionsas in section is worth pointing out a slight danger to the technique ofvisualizinginstance-based learning in terms ofboundaries in example spaceit makes theimplicit assumption that attributes are numeric rather than nominal.ifthevarious values that a nominal attribute can take on were laid out along a linegeneralizations involving a segment ofthat line would make no senseeachtest involves either one value for the attribute or all values for it perhaps anarbitrary subset ofvalues.although you can more or less easily imagine extend-ing the examples in figure to several dimensionsit is much harder toimagine how rules involving nominal attributes will look in multidimensionalinstance space.many machine learning situations involve numerous attributesand our intuitions tend to lead us astray when extended to am page clusters rather than a classifier is learnedthe output takes the form ofadiagram that shows how the instances fall into clusters.in the simplest case thisinvolves associating a cluster number with each instancewhich might bedepicted by laying the instances out in two dimensions and partitioning thespace to show each clusteras illustrated in figure clustering algorithms allow one instance to belong to more than oneclusterso the diagram might lay the instances out in two dimensions and drawoverlapping subsets representing each cluster a venn diagram.some algo-rithms associate instances with clusters probabilistically rather than categori-cally.in this casefor every instance there is a probability or degree ofmembership with which it belongs to each ofthe clusters.this is shown infigure particular association is meant to be a probabilistic onesothe numbers for each example sum to one although that is not always the case.other algorithms produce a hierarchical structure ofclusters so that at the top level the instance space divides into just a few clusterseach ofwhichdivides into its own subclusters at the next level downand so on.in this case adiagram such as the one in figure is usedin which elements joinedtogether at lower levels are more tightly clustered than ones joined together ways ofrepresenting am page higher levels.diagrams such as this are called dendrograms.this term meansjust the same thing as tree diagramsthe greek word dendronmeans a tree in clustering the more exotic version seems to be preferred perhapsbecause biologic species are a prime application area for clustering techniquesand ancient languages are often used for naming in biology.clustering is often followed by a stage in which a decision tree or rule set isinferred that allocates each instance to the cluster in which it belongs.thentheclustering operation is just one step on the way to a structural readingknowledge representation is a key topic in classical artificial intelligence and iswell represented by a comprehensive series ofpapers edited by brachman andlevesque are about ways ofrepresenting handcraftednot learned knowledgeand the kind ofrepresentations that can be learned fromexamples are quite rudimentary in comparison.in particularthe shortcomingsofpropositional ruleswhich in logic are referred to as the propositional calcu-lusand the extra expressive power ofrelational rulesor the predicate calculusare well described in introductions to logic such as that in chapter ofthe bookby genesereth and nilsson mentioned the problem ofdealing with conflict among different rules.various ways ofdoing thiscalled conflict resolution strategieshave been devel-oped for use with rule-based programming systems.these are described inbooks on rule-based programmingsuch as that by brownstown et are designed for use with handcrafted rule sets rather thanones that have been learned.the use ofhand-crafted rules with exceptions fora large dataset has been studied by gaines and compton richardsand compton describe their role as an alternative to classic knowledgeengineering.further information on the various styles ofconcept representation can befound in the papers that describe machine learning methods ofinferring con-cepts from examplesand these are covered in the further readingsection ofchapter and the discussionsections ofchapter am page now that we ve seen how the inputs and outputs can be representedit s timeto look at the learning algorithms themselves.this chapter explains the basicideas behind the techniques that are used in practical data mining.we will notdelve too deeply into the trickier issues advanced versions ofthe algorithmsoptimizations that are possiblecomplications that arise in practice.these topicsare deferred to chapter we come to grips with real implementationsofmachine learning methods such as the ones included in data mining toolkitsand used for real-world applications.it is important to understand these moreadvanced issues so that you know what is really going on when you analyze aparticular dataset.in this chapter we look at the basic ideas.one ofthe most instructive lessonsis that simple ideas often work very welland we strongly recommend the adop-tion ofa simplicity-first methodology when analyzing practical datasets.thereare many different kinds ofsimple structure that datasets can exhibit.in onedatasetthere might be a single attribute that does all the work and the othersmay be irrelevant or redundant.in another datasetthe attributes might basic am page contribute independently and equally to the final outcome.a third might havea simple logical structureinvolving just a few attributes that can be capturedby a decision tree.in a fourththere may be a few independent rules that governthe assignment ofinstances to different classes.a fifth might exhibit depend-encies among different subsets ofattributes.a sixth might involve lineardependence among numeric attributeswhere what matters is a weighted sumofattribute values with appropriately chosen weights.in a seventhclassifica-tions appropriate to particular regions ofinstance space might be governed bythe distances between the instances themselves.and in an eighthit might bethat no class values are providedthe learning is unsupervised.in the infinite variety ofpossible datasets there are many different kinds ofstructure that can occurand a data mining tool no matter how capable thatis looking for one class ofstructure may completely miss regularities ofa dif-ferent kindregardless ofhow rudimentary those may be.the result is a baroqueand opaque classification structure ofone kind instead ofa simpleelegantimmediately comprehensible structure ofanother.each ofthe eight examples ofdifferent kinds ofdatasets sketched previouslyleads to a different machine learning method well suited to discovering it.thesections ofthis chapter look at each ofthese structures in rudimentary ruleshere s an easy way to find very simple classification rules from a set ofinstances.called generates a one-level decision tree expressed in the formofa set ofrules that all test one particular is a simplecheap methodthat often comes up with quite good rules for characterizing the structure indata.it turns out that simple rules frequently achieve surprisingly high accu-racy.perhaps this is because the structure underlying many real-world datasetsis quite rudimentaryand just one attribute is sufficient to determine the classofan instance quite accurately.in any eventit is always a good plan to try thesimplest things first.the idea is thiswe make rules that test a single attribute and branch accord-ingly.each branch corresponds to a different value ofthe attribute.it is obviouswhat is the best classification to give each branchuse the class that occurs mostoften in the training data.then the error rate ofthe rules can easily be deter-mined.just count the errors that occur on the training datathat isthe numberofinstances that do not have the majority class.each attribute generates a different set ofrulesone rule for every value ofthe attribute.evaluate the error rate for each attribute s rule set and choosethe best.it s that simple! figure shows the algorithm in the form basic am page to see the method at workconsider the weather data oftable willencounter it many times again when looking at how learning algorithms work.to classify on the final considers four sets ofrulesone for eachattribute.these rules are shown in table asterisk indicates that a randomchoice has been made between two equally likely outcomes.the number oferrors is given for each rulealong with the total number oferrors for the ruleset as a chooses the attribute that produces rules with the smallestnumber oferrors that isthe first and third rule sets.arbitrarily breaking thetie between these two rule sets givesoutlook sunny noovercast yesrainy rudimentary each attribute for each value of that attribute make a rule as follows count how often each class appears find the most frequent class make the rule assign that class to this attribute-value. calculate the error rate of the rules.choose the rules with the smallest error rate.figure for the attributes in the weather data.attributeruleserrorstotal random choice was made between two equally likely am page we noted at the outset that the game for the weather data is unspecified.oddly enoughit is apparently played when it is overcast or rainy but not whenit is sunny.perhaps it s an indoor pursuit.missing values and numeric attributesalthough a very rudimentary learning does accommodate bothmissing values and numeric attributes.it deals with these in simple but effec-tive ways.missingis treated as just another attribute value so thatfor exampleifthe weather data had contained missing values for the outlookattributea ruleset formed on outlookwould specify four possible class valuesone each forsunnyovercastand rainyand a fourth for missing.we can convert numeric attributes into nominal ones using a simple dis-cretization method.firstsort the training examples according to the values ofthe numeric attribute.this produces a sequence ofclass values.for examplesorting the numeric version ofthe weather data according to thevalues oftemperatureproduces the yes yesyes no no yes yes yes no yes yes nodiscretization involves partitioning this sequence by placing breakpoints init.one possibility is to place breakpoints wherever the class changesproducingeight categoriesyes yes yes no yes yes yes breakpoints halfway between the examples on either side placesthem at two instances withvalue cause a problem because they have the same value oftemperaturebutfall into different classes.the simplest fix is to move the breakpoint at upone exampleto a mixed partition in which nois the majorityclass.a more serious problem is that this procedure tends to form a large numberofcategories.the method will naturally gravitate toward choosing an attri-bute that splits into many categoriesbecause this will partition the dataset intomany classesmaking it more likely that instances will have the same class as themajority in their partition.in factthe limiting case is an attribute that has adifferent value for each instance that isan identification codeattribute thatpinpoints instances uniquely and this will yield a zero error rate on the train-ing set because each partition contains just one instance.ofcoursehighlybranching attributes do not usually perform well on test examplesindeedtheidentification codeattribute will never predict any examples outside the trainingset correctly.this phenomenon is known as overfittingwe have basic am page described overfitting-avoidance bias in chapter we willencounter this problem repeatedly in subsequent chapters.for is likely to occur whenever an attribute has a large number ofpossible values.consequentlywhen discretizing a numeric attrib-ute a rule is adopted that dictates a minimum number ofexamples ofthe majority class in each partition.suppose that minimum is set at three.thiseliminates all but two ofthe preceding partitions.insteadthe partitioningprocess beginsyes no yes yes that there are three occurrences ofyesthe majority classin the firstpartition.howeverbecause the next example is also yeswe lose nothing byincluding that in the first partitiontoo.this leads to a new divisionyes no yes yes yes no yes yes yes yes yes nowhere each partition contains at least three instances ofthe majority classexceptthe last onewhich will usually have less.partition boundaries always fallbetween examples ofdifferent classes.whenever adjacent partitions have the same majority classas do the first twopartitions abovethey can be merged together without affecting the meaning ofthe rule sets.thus the final discretization isyes no yes yes yes no no yes yes yes yes yes nowhich leads to the rule settemperature nothe second rule involved an arbitrary choiceas it happensnowas chosen.ifwe had chosen yesinsteadthere would be no need for any breakpoint at all and as this example illustratesit might be better to use the adjacent categoriesto help to break ties.in fact this rule generates five errors on the training setand so is less effective than the preceding rule for outlook.howeverthe sameprocedure leads to this rule for humidityhumidity and yesthis generates only three errors on the training set and is the best forthe data in table numeric attribute has missing valuesan additional category iscreated for themand the preceding discretization procedure is applied just tothe instances for which the attribute s value is rudimentary am page discussionin a seminal paper titled very simple classification rules perform well on mostcommonly used datasets comprehensive study ofthe perform-ance ofthe procedure was reported on datasets frequently used bymachine learning researchers to evaluate their algorithms.throughoutthestudy used cross-validationan evaluation technique that we will explain inchapter ensure that the results were representative ofwhat independenttest sets would yield.after some experimentationthe minimum number ofexamples in each partition ofa numeric attribute was set at sixnot three asused for the preceding illustration.surprisinglydespite its simplicity did astonishingly even embarrass-ingly well in comparison with state-of-the-art learning methodsand the rulesit produced turned out to be just a few percentage points less accurateon almostall ofthe datasetsthan the decision trees produced by a state-of-the-art deci-sion tree induction scheme.these trees werein generalconsiderably largerthan s rules.rules that test a single attribute are often a viable alternative tomore complex structuresand this strongly encourages a simplicity-first meth-odology in which the baseline performance is established using simplerudi-mentary techniques before progressing to more sophisticated learning methodswhich inevitably generate output that is harder for people to interpret.the procedure learns a one-level decision tree whose leaves represent thevarious different classes.a slightly more expressive technique is to use a differ-ent rule for each class.each rule is a conjunction oftestsone for each attribute.for numeric attributes the test checks whether the value lies within a given inter-valfor nominal ones it checks whether it is in a certain subset ofthat attribute svalues.these two types oftests intervals and subset are learned from thetraining data pertaining to each class.for a numeric attributethe endpoints ofthe interval are the minimum and maximum values that occur in the trainingdata for that class.for a nominal onethe subset contains just those values thatoccur for that attribute in the training data for the class.rules representing dif-ferent classes usually overlapand at prediction time the one with the mostmatching tests is predicted.this simple technique often gives a useful firstimpression ofa dataset.it is extremely fast and can be applied to very largequantities modelingthe method uses a single attribute as the basis for its decisions and choosesthe one that works best.another simple technique is to use all attributes andallow them to make contributions to the decision that are equally importantandindependentofone anothergiven the class.this is basic am page makes real-life datasets interesting is that the attributes are certainly not equallyimportant or independent.but it leads to a simple scheme that again works sur-prisingly well in practice.table shows a summary ofthe weather data obtained by counting howmany times each attribute value pair occurs with each value no forplay.for exampleyou can see from table that outlookis sunnyfor five exam-plestwo ofwhich have playyesand three ofwhich have playno.the cellsin the first row ofthe new table simply count these occurrences for all possiblevalues ofeach attributeand the playfigure in the final column counts the totalnumber ofoccurrences ofyesand no.in the lower part ofthe tablewe rewrotethe same information in the form offractionsor observed probabilities.forexampleofthe nine days that playis yesoutlookis sunnyfor twoyielding afraction playthe fractions are differentthey are the proportion ofdays that playis yesand norespectively.now suppose we encounter a new example with the values that are shown intable treat the five features in table outlooktemperaturehumid-itywindyand the overall likelihood that playis yesor no as equally impor-tantindependent pieces ofevidence and multiply the corresponding fractions.looking at the outcome yesgivesthe fractions are taken from the yesentries in the table according to the valuesofthe attributes for the new dayand the final is the overall fraction likelihood of yes weather data with counts and new am page representing the proportion ofdays on which playis yes.a similar calculationfor the outcome noleads tothis indicates that for the new daynois more likely than yes four times morelikely.the numbers can be turned into probabilities by normalizing them sothat they sum to simple and intuitive method is based on bayes s rule ofconditional prob-ability.bayes s rule says that ifyou have a hypothesis hand evidence ethat bearson that hypothesisthenwe use the notation that pra denotes the probability ofan event aand thatprab denotes the probability ofaconditional on another event b.thehypothesis his that playwill besayyesand prhe is going to turn out to as determined previously.the evidence eis the particular combi-nation ofattribute values for the new dayoutlooksunnytemperaturecoolhumidityhighand windytrue.let s call these four pieces ofevidence that these pieces ofevidence are independentgiven the classtheir combined probability is obtained by multiplying theprobabilitiesdon t worry about the denominatorwe will ignore it and eliminate it in thefinal normalizing step when we make the probabilities ofyesand nosum to as we did previously.the pryes at the end is the probability ofa yesoutcome without knowing any ofthe evidence ethat iswithout knowing any-thing about the particular day referenced it s called the prior probabilityofthehypothesis h.in this caseit s just ofthe training exampleshad a yesvalue for play.substituting the fractions in table for the appro-priate evidence probabilities leads toprpryesee prprprprprpryeseeyeseyeseyeseyesyese of of of no basic am page just as we calculated previously.againthe pre in the denominator will dis-appear when we normalize.this method goes by the name ofna ve bayesbecause it s based on bayes srule and na vely assumes independence it is only valid to multiply proba-bilities when the events are independent.the assumption that attributes areindependent the class in real life certainly is a simplistic one.but despitethe disparaging namena ve bayes works very well when tested on actualdatasetsparticularly when combined with some ofthe attribute selection pro-cedures introduced in chapter that eliminate redundantand hence nonin-dependentattributes.one thing that can go wrong with na ve bayes is that ifa particular attributevalue does not occur in the training set in conjunction with everyclass valuethings go badly awry.suppose in the example that the training data was differ-ent and the attribute value outlook always been associated with the outcome no.then the probability ofoutlook a yesthat isproutlooksunnyyeswould be zeroand because the other probabilities aremultiplied by this the final probability ofyeswould be zero no matter how largethey were.probabilities that are zero hold a veto over the other ones.this is nota good idea.but the bug is easily fixed by minor adjustments to the method ofcalculating probabilities from frequencies.for examplethe upper part oftable shows that for playyesoutlook issunnyfor two examplesovercastfor fourand rainyfor threeand the lower partgives these events probabilities add to each numerator and compensate by adding to the denomina-torgiving probabilities will ensure thatan attribute value that occurs zero times receives a probability which is nonzeroalbeit small.the strategy ofadding to each count is a standard technique calledthe laplace estimatorafter the great eighteenth-century french mathematicianpierre laplace.although it works well in practicethere is no particular reasonfor adding to the countswe could instead choose a small constant mand usethe value ofmwhich was set to provides a weight that determineshow influential the a priori values are for each ofthe threepossible attribute values.a large msays that these priors are very important com-pared with the new evidence coming in from the training setwhereas a smallone gives them less influence.finallythere is no particular reason for dividingminto three equalparts in the numeratorswe could am page insteadwhere to three numbers are a prioriprobabilities ofthe values ofthe outlookattribute being sunnyovercastandrainyrespectively.this is now a fully bayesian formulation where prior probabilities have beenassigned to everything in sight.it has the advantage ofbeing completely rigor-ousbut the disadvantage that it is not usually clear just how these prior prob-abilities should be assigned.in practicethe prior probabilities make littledifference provided that there are a reasonable number oftraining instancesand people generally just estimate frequencies using the laplace estimator byinitializing all counts to one instead ofto zero.missing values and numeric attributesone ofthe really nice things about the bayesian formulation is that missingvalues are no problem at all.for exampleifthe value ofoutlookwere missingin the example oftable calculation would simply omit this attributeyieldingthese two numbers are individually a lot higher than they were beforebecauseone ofthe fractions is missing.but that s not a problem because a fraction ismissing in both casesand these likelihoods are subject to a further normal-ization process.this yields probabilities for yesand and value is missing in a training instanceit is simply not included in thefrequency countsand the probability ratios are based on the number ofvaluesthat actually occur rather than on the total number ofinstances.numeric values are usually handled by assuming that they have a normal or gaussian probability distribution.table gives a summary ofthe weatherdata with numeric features from table nominal attributeswe calcu-lated counts as beforeand for numeric ones we simply listed the values thatoccur.thenwhereas we normalized the counts for the nominal attributes intoprobabilitieswe calculated the mean and standard deviation for each class and each numeric attribute.thus the mean value oftemperatureover the yesinstances is its standard deviation is mean is simply the averageofthe preceding valuesthat isthe sum divided by the number ofvalues.thestandard deviation is the square root ofthe sample variancewhich we can cal-culate as followssubtract the mean from each valuesquare the resultsum themtogetherand then divide by one less thanthe number ofvalues.after we havefound this sample variancefind its square root to determine the standard devi-ation.this is the standard way ofcalculating mean and standard deviation ofalikelihood of likelihood of yesno basic am page set ofnumbers one less than is to do with the number ofdegrees offreedom in the samplea statistical notion that we don t want to get into here.the probability density function for a normal distribution with mean mandstandard deviation sis given by the rather formidable expressionbut fear not! all this means is that ifwe are considering a yesoutcome whentemperaturehas a just need to plug into the formula.so the value ofthe probability density function isby the same tokenthe probability density ofa yesoutcome when is calculated in the same waythe probability density function for an event is very closely related to its prob-ability.howeverit is not quite the same thing.iftemperature is a continuousscalethe probability ofthe temperature being or exactlyany othervaluesuch as is zero.the real meaning ofthe density functionfx is that the probability that the quantity lies within a small region around xsaybetween and efx.what we have written above is numeric weather data with summary am page iftemperature is measured to the nearest degree and humidity is measured tothe nearest percentage point.you might think we ought to factor in the accu-racy figure ewhen using these probabilitiesbut that s not necessary.the sameewould appear in both the yesand nolikelihoods that follow and cancel outwhen the probabilities were calculated.using these probabilities for the new day in table yieldswhich leads to probabilitiesthese figures are very close to the probabilities calculated earlier for the newday in table the temperatureand humidityvalues and yieldsimilar probabilities to the cooland highvalues used before.the normal-distribution assumption makes it easy to extend the na ve bayesclassifier to deal with numeric attributes.ifthe values ofany numeric attributesare missingthe mean and standard deviation calculations are based only on theones that are present.bayesian models for document classificationone important domain for machine learning is document classificationinwhich each instance represents a document and the instance s class is the doc-ument s topic.documents might be news items and the classes might be domes-tic newsoverseas newsfinancial newsand sport.documents are characterizedby the words that appear in themand one way to apply machine learning todocument classification is to treat the presence or absence ofeach word as a boolean attribute.na ve bayes is a popular technique for this applicationbecause it is very fast and quite accurate.howeverthis does not take into account the number ofoccurrences ofeachwordwhich is potentially useful information when determining the categoryprobability of of of likelihood of yesno basic methodstable new am page ofa document.insteada document can be viewed as a bag ofwords a set thatcontains all the words in the documentwith multiple occurrences ofa wordappearing multiple times setincludes each ofits members justoncewhereas a bagcan have repeated elements.word frequencies can beaccommodated by applying a modified form ofna ve bayes that is sometimesdescribed as multinominal na ve bayes.suppose the number oftimes word ioccurs in the documentand the probability ofobtaining word iwhen sampling fromall the documents in category h.assume that the probability is independent ofthe word s context and position in the document.these assumptions lead to amultinomial distributionfor document probabilities.for this distributiontheprobability ofa document egiven its class h in other wordsthe formula forcomputing the probability preh in bayes s rule iswhere n the number ofwords in the document.the reasonfor the factorials is to account for the fact that the ordering ofthe occurrencesofeach word is immaterial according to the bag-of-words model.piis estimatedby computing the relative frequency ofword iin the text ofall training docu-ments pertaining to category h.in reality there should be a further term thatgives the probability that the model for category hgenerates a document whoselength is the same as the length ofethat is why we use the symbol insteadofbut it is common to assume that this is the same for all classes and hencecan be dropped.for examplesuppose there are only the two wordsyellowand bluein thevocabularyand a particular document class hhas pryellowh andprblueh might call hthe class ofyellowish green documents.suppose eis the document blue yellow bluewith a length words.thereare four possible bags ofthree words.one is yellow yellowand its prob-ability according to the preceding formula isthe other threewith their am page hereecorresponds to the last case that in a bag ofwords the order isimmaterialthus its probability ofbeing generated by the yellowish green doc-ument model is another classvery bluish green docu-ments it h pryellow probabilitythat eis generated by this model is are the only two classesdoes that mean that eis in the very bluishgreen document class? not necessarily.bayes s rulegiven earliersays that youhave to take into account the prior probability ofeach hypothesis.ifyou knowthat in fact very bluish green documents are twice as rare as yellowish greenonesthis would be just sufficient to outweigh the preceding to disparityand tip the balance in favor ofthe yellowish greenclass.the factorials in the preceding probability formula don t actually need to becomputed because being the same for every class they drop out in the nor-malization process anyway.howeverthe formula still involves multiplyingtogether many small probabilitieswhich soon yields extremely small numbersthat cause underflow on large documents.the problem can be avoided by usinglogarithms ofthe probabilities instead ofthe probabilities themselves.in the multinomial na ve bayes formulation a document s class is determinednot just by the words that occur in it but also by the number oftimes they occur.in general it performs better than the ordinary na ve bayes model for docu-ment classificationparticularly for large dictionary sizes.discussionna ve bayes gives a simple approachwith clear semanticsto representingusingand learning probabilistic knowledge.impressive results can be achievedusing it.it has often been shown that na ve bayes rivalsand indeed outper-formsmore sophisticated classifiers on many datasets.the moral isalways trythe simple things first.repeatedly in machine learning people have eventuallyafter an extended struggleobtained good results using sophisticated learningmethods only to discover years later that simple methods such as and na vebayes do just as well or even better.there are many datasets for which na ve bayes does not do so wellhoweverand it is easy to see why.because attributes are treated as though they were com-pletely independentthe addition ofredundant ones skews the learning process.as an extreme exampleifyou were to include a new attribute with the samevalues as temperatureto the weather datathe effect ofthe temperature attri-bute would be multipliedall ofits probabilities would be squaredgiving it agreat deal more influence in the decision.ifyou were to add such attributesthen the decisions would effectively be made on temperaturealone.dependen-cies between attributes inevitably reduce the power ofna ve bayes to discernwhat is going on.they canhoweverbe ameliorated by using a subset basic am page decision in the decision proceduremaking a careful selection ofwhich onesto use.chapter shows how.the normal-distribution assumption for numeric attributes is anotherrestriction on na ve bayes as we have formulated it here.many features simplyaren t normally distributed.howeverthere is nothing to prevent us from usingother distributions for the numeric attributesthere is nothing magic about thenormal distribution.ifyou know that a particular attribute is likely to followsome other distributionstandard estimation procedures for that distributioncan be used instead.ifyou suspect it isn t normal but don t know the actual distributionthere are procedures for kernel density estimation that do notassume any particular distribution for the attribute values.another possibilityis simply to discretize the data constructing decision treesthe problem ofconstructing a decision tree can be expressed recursively.firstselect an attribute to place at the root node and make one branch for each pos-sible value.this splits up the example set into subsetsone for every value ofthe attribute.now the process can be repeated recursively for each branchusingonly those instances that actually reach the branch.ifat any time all instancesat a node have the same classificationstop developing that part ofthe tree.the only thing left to decide is how to determine which attribute to split ongiven a set ofexamples with different classes.consider the weather data.there are four possibilities for each splitand at the top level they produce treessuch as those in figure is the best choice? the number ofyesand noclasses are shown at the leaves.any leafwith only one class yesor no willnot have to be split furtherand the recursive process down that branch will ter-minate.because we seek small treeswe would like this to happen as soon aspossible.ifwe had a measure ofthe purity ofeach nodewe could choose theattribute that produces the purest daughter nodes.take a moment to look atfigure and ponder which attribute you think is the best choice.the measure ofpurity that we will use is called the informationand is meas-ured in units called bits.associated with a node ofthe treeit represents theexpected amount ofinformation that would be needed to specify whether a newinstance should be classified yesor nogiven that the example reached that node.unlike the bits in computer memorythe expected amount ofinformationusually involves fractions ofa bit and is often less than one! we calculate itbased on the number ofyesand noclasses at the nodewe will look at the detailsofthe calculation shortly.but first let s see how it s used.when evaluating thefirst tree in figure numbers ofyesand noclasses at the leafnodes the information values ofthese nodes am page we can calculate the average information value ofthesetaking into account thenumber ofinstances that go down each branch five down the first and thirdand four down the secondthis average represents the amount ofinformation that we expect would be nec-essary to specify the class ofa new instancegiven the tree structure in basic methodsyesyesnononosunnyyesyesyesyesovercastyesyesyesnonorainyoutlookayesyesyesnonohotyesyesnonoyesyesyesnomildcooltemperatureyesbyesyesyesnonononoyesyesyesyesyesyesnohighnormalhumiditycyesyesyesyesyesyesnonoyesyesyesnononofalsetruewindydfigure stumps for the weather am page decision we created any ofthe nascent tree structures in figure train-ing examples at the root comprised nine yesand five nonodescorrespondingto an information value ofthus the tree in figure is responsible for an information gain ofwhich can be interpreted as the informational value ofcreating a branch on theoutlook attribute.the way forward is clear.we calculate the information gain for each attri-bute and choose the one that gains the most information to split on.in the sit-uation offigure we select outlookas the splitting attribute at the root ofthe tree.hopefullythis accords with your intuition as the best one to select.it is the only choicefor which one daughter node is completely pureand this gives it a considerableadvantage over the other attributes.humidityis the next best choice because itproduces a larger daughter node that is almost completely pure.then we continuerecursively.figure shows the possibilities for a furtherbranch at the node reached when outlookis sunny.clearlya further split onoutlookwill produce nothing newso we only consider the other three attributes.the information gain for each turns out to beso we select humidityas the splitting attribute at this point.there is no need tosplit these nodes any furtherso this branch is finished.continued application ofthe same idea leads to the decision tree for the weather data.ideallythe process terminates when all leafnodes arepurethat iswhen they contain instances that all have the same classification.howeverit might not be possible to reach this happy situation because there isnothing to stop the training set containing two examples with identical sets ofattributes but different classes.consequentlywe stop when the data cannot besplit any am page calculating informationnow it is time to explain how to calculate the information measure that is usedas a basis for evaluating different splits.we describe the basic idea in this sectionthen in the next we examine a correction that is usually made to counter a biastoward selecting splits on attributes with large numbers ofpossible values.before examining the detailed formula for calculating the amount ofinfor-mation required to specify the class ofan example given that it reaches a treenode with a certain number ofyes s and no sconsider first the kind ofproper-ties we would expect this quantity to basic methods......nonoyessunnyhotmildcooloutlooktemperatureyesnoa......nononoyesyessunnyhighnormaloutlookhumidityb......yesyesnonoyesnosunnyfalsetrueoutlookwindycfigure tree stumps for the weather am page decision the number ofeither yes s or no s is zerothe information is the number ofyes s and no s is equalthe information reaches amaximum.moreoverthe measure should be applicable to multiclass situationsnot just totwo-class ones.the information measure relates to the amount ofinformation obtained bymaking a decisionand a more subtle property ofinformation can be derivedby considering the nature ofdecisions.decisions can be made in a single stageor they can be made in several stagesand the amount ofinformation involvedis the same in both cases.for examplethe decision involved incan be made in two stages.first decide whether it s the first case or one oftheother two casesand then decide which ofthe other two cases it isin some cases the second decision will not need to be madenamelywhen the decision turns out to be the first one.taking this into account leads to tree for the weather am page ofcoursethere is nothing special about these particular numbersand a similarrelationship must hold regardless ofthe actual values.thus we can add a furthercriterion to the preceding information must obey the multistage property illustrated previously.remarkablyit turns out that there is only one function that satisfies all thesepropertiesand it is known as the information valueor entropythe reason for the minus signs is that logarithms ofthe fractions negativeso the entropy is actually positive.usually the logarithms areexpressed in base the entropy is in units called bits just the usual kindofbits used with computers.the arguments entropy formula are expressed as fractionsthat add up to oneso thatfor examplethus the multistage decision property can be written in general aswhere ofthe way the log function worksyou can calculate the informationmeasure without having to work out the individual fractionsthis is the way that the information measure is usually calculated in practice.so the information value for the first leafnode ofthe first tree in isas stated on page branching attributeswhen some attributes have a large number ofpossible valuesgiving rise to amultiway branch with many child nodesa problem arises with the informationgain calculation.the problem can best be appreciated in the extreme case whenan attribute has a different value for each instance in the dataset asforexamplean identification codeattribute basic am page decision gives the weather data with this extra attribute.branching on idcodeproduces the tree stump in figure information required to specifythe class given the value ofthis attribute iswhich is zero because each ofthe terms is zero.this is not surprisingthe idcodeattribute identifies the instancewhich determines the class without anyambiguity just as table shows.consequentlythe information gain ofthisattribute is just the information at the bits.this isgreater than the information gain ofany other attributeand so id codewillinevitably be chosen as the splitting attribute.but branching on the identifica-tion code is no good for predicting the class ofunknown instances and tellsnothing about the structure ofthe decisionwhich after all are the twin goals ofmachine weather data with identification codes.id codeoutlooktemperaturehumiditywindyplayasunnyhothighfalsenobsunnyhothightruenocovercasthothighfalseyesdrainymildhighfalseyeserainycoolnormalfalseyesfrainycoolnormaltruenogovercastcoolnormaltrueyeshsunnymildhighfalsenoisunnycoolnormalfalseyesjrainymildnormalfalseyesksunnymildnormaltrueyeslovercastmildhightrueyesmovercasthotnormalfalseyesnrainymildhightruenonoyesnoyesnoid codeabc stump for the id am page the overall effect is that the information gain measure tends to prefer attri-butes with large numbers ofpossible values.to compensate for thisa modifi-cation ofthe measure called the gain ratiois widely used.the gain ratio isderived by taking into account the number and size ofdaughter nodes intowhich an attribute splits the datasetdisregarding any information about theclass.in the situation shown in figure counts have a value theinformation value ofthe split isbecause the same times.this amounts to log bitswhich is a very high value.this is because the information value ofa split is the number ofbits needed to determine to which branch each instanceis assignedand the more branches there arethe greater this value is.the gainratio is calculated by dividing the original information in this caseby the information value ofthe yielding a gain ratio value for the id codeattribute.returning to the tree stumps for the weather data in figure dataset into three subsets ofsize and thus has an intrinsic infor-mation value ofwithout paying any attention to the classes involved in the subsets.as we haveseenthis intrinsic information value is higher for a more highly branchingattribute such as the hypothesizedid code.again we can correct the informa-tion gain by dividing by the intrinsic information value to get the gain ratio.the results ofthese calculations for the tree stumps offigure are sum-marized in table comes out on topbut humidityis now a muchcloser contender because it splits the data into two subsets instead ofthree.inthis particular examplethe hypothetical id codeattributewith a gain ratio still be preferred to any ofthese four.howeverits advantage basic methodstable ratio calculations for the tree stumps of figure info info info info ratio ratio ratio ratio am page algorithmsconstructing reduced.in practical implementationswe can use an ad hoc test to guardagainst splitting on such a useless attribute.unfortunatelyin some situations the gain ratio modification overcompen-sates and can lead to preferring an attribute just because its intrinsic informa-tion is much lower than that for the other attributes.a standard fix is to choosethe attribute that maximizes the gain ratioprovided that the information gainfor that attribute is at least as great as the average information gain for all theattributes examined.discussionthe divide-and-conquer approach to decision tree inductionsometimes calledtop-down induction ofdecision treeswas developed and refined over many yearsby j.ross quinlan ofthe university ofsydneyaustralia.although others haveworked on similar methodsquinlan s research has always been at the very fore-front ofdecision tree induction.the method that has been described using theinformation gain criterion is essentially the same as one known as useofthe gain ratio was one ofmany improvements that were made to overseveral yearsquinlan described it as robust under a wide variety ofcircum-stances.although a robust and practical solutionit sacrifices some ofthe ele-gance and clean theoretical motivation ofthe information gain criterion.a series ofimprovements to culminated in a practical and influentialsystem for decision tree induction called improvements includemethods for dealing with numeric attributesmissing valuesnoisy dataandgenerating rules from treesand they are described in section algorithms constructing rulesas we have seendecision tree algorithms are based on a divide-and-conquerapproach to the classification problem.they work from the top downseekingat each stage an attribute to split on that best separates the classesthen recur-sively processing the subproblems that result from the split.this strategy generates a decision treewhich can ifnecessary be converted into a set ofclas-sification rules although ifit is to produce effective rulesthe conversion is nottrivial.an alternative approach is to take each class in turn and seek a way ofcov-ering all instances in itat the same time excluding instances not in the class.this is called a coveringapproach because at each stage you identify a rule that covers some ofthe instances.by its very naturethis covering approach leadsto a set ofrules rather than to a decision tree.the covering method can readily be visualized in a two-dimensional spaceofinstances as shown in figure first make a rule covering the a am page the first test in the rulesplit the space vertically as shown in the center picture.this gives the beginnings ofa ruleif x then class rule covers many b s as well as a sso a new test is added to therule by further splitting the space horizontally as shown in the third diagramif x and y then class gives a rule covering all but one ofthe a s.it s probably appropriate to leaveit at thatbut ifit were felt necessary to cover the final aanother rule would benecessary perhapsif x and y then class same procedure leads to two rules covering the b sif x then class x and y then class basic methodsfigure algorithma covering the instances and the decision tree forthe same problem.x am page algorithmsconstructing ais erroneously covered by these rules.ifit were necessary to excludeitmore tests would have to be added to the second ruleand additional ruleswould need to be introduced to cover the b s that these new tests exclude.rules versus treesa top-down divide-and-conquer algorithm operates on the same data in amanner that isat least superficiallyquite similar to a covering algorithm.itmight first split the dataset using the xattribute and would probably end upsplitting it at the same the covering algorithmis concerned only with covering a single classthe division would take bothclasses into accountbecause divide-and-conquer algorithms create a singleconcept description that applies to all classes.the second split might also be atthe same to the decision tree in figure treecorresponds exactly to the set ofrulesand in this case there is no difference ineffect between the covering and the divide-and-conquer algorithms.but in many situations there isa difference between rules and trees in termsofthe perspicuity ofthe representation.for examplewhen we described thereplicated subtree problem in section noted that rules can be symmet-ric whereas trees must select one attribute to split on firstand this can lead totrees that are much larger than an equivalent set ofrules.another difference isthatin the multiclass casea decision tree split takes all classes into accounttrying to maximize the purity ofthe splitwhereas the rule-generating methodconcentrates on one class at a timedisregarding what happens to the otherclasses.a simple covering algorithmcovering algorithms operate by adding tests to the rule that is under construc-tionalways striving to create a rule with maximum accuracy.in contrastdivide-and-conquer algorithms operate by adding tests to the tree that is underconstructionalways striving to maximize the separation among the classes.each ofthese involves finding an attribute to split on.but the criterion for thebest attribute is different in each case.whereas divide-and-conquer algorithmssuch as choose an attribute to maximize the information gainthe cover-ing algorithm we will describe chooses an attribute value pair to maximize theprobability ofthe desired classification.figure gives a picture ofthe situationshowing the space containing allthe instancesa partially constructed ruleand the same rule after a new termhas been added.the new term restricts the coverage ofthe rulethe idea is toinclude as many instances ofthe desired class as possible and exclude as manyinstances ofother classes as possible.suppose the new rule will cover a total oftinstancesofwhich pare positive examples ofthe class and t-pare in am page classes that isthey are errors made by the rule.then choose the new term tomaximize the ratio pt.an example will help.for a changewe use the contact lens problem will form rules that cover each ofthe three classeshardsoftand nonein turn.to beginwe seek a ruleif then recommendation the unknown term have nine choicesage prescription prescription production rate production rate numbers on the right show the fraction of correct instances in the setsingled out by that choice.in this casecorrectmeans that the recommendation ishard.for instanceageyoungselects eight instancestwo ofwhich recommendhard contact lensesso the first fraction is follow thisyou will need tolook back at the contact lens data in table on page and count up the entriesin the table. we select the largest choosing betweenthe seventh and the last choice in the preceding listand create the ruleif astigmatism then recommendation rule is an inaccurate onegetting only instances correct out ofthe it coversshown in table we refine it furtherif astigmatism and then recommendation basic methodsfigure instance space during operation ofa covering am page algorithmsconstructing the possibilities for the unknown term yields the seven choicesage prescription prescription production rate production rate the entries in table the last is a clear winnergetting fourinstances correct out ofthe six that it coversand corresponds to the ruleif astigmatism and tear production rate recommendation we stop here? perhaps.but let s say we are going for exact rulesnomatter how complex they become.table shows the cases that are covered bythe rule so far.the possibilities for the next term are nowage prescription prescription need to choose between the first and fourth.so far we have treated the frac-tions numericallybut although these two are equal evaluate to different coverageone selects just two correct instances and the othertable of the contact lens data for which astigmatismyes.agespectacle astigmatismtear production recommended am page selects three.in the event ofa tiewe choose the rule with the greater coveragegiving the final ruleif astigmatism and tear production rate spectacle prescription then recommendation is indeed one ofthe rules given for the contact lens problem.but it onlycovers three ofthe four hardrecommendations.so we delete these three fromthe set ofinstances and start againlooking for another rule ofthe formif then recommendation the same processwe will eventually find that ageyoungis the bestchoice for the first term.its coverage is seventhe reason for the seven is that have been removed from the original setleaving instances alto-gether.the best choice for the second term is astigmatismyesselecting is a tietear production ratenormalis the best for the thirdselecting age and astigmatism and tear production rate then recommendation rule actually covers three ofthe original set ofinstancestwo ofwhich arecovered by the previous rule but that s all right because the recommendationis the same for each rule.now that all the hard-lens cases are coveredthe next step is to proceed withthe soft-lens ones in just the same way.finallyrules are generated for the nonecase unless we are seeking a rule set with a default rulein which case explicitrules for the final outcome are unnecessary.what we have just described is the prism method for constructing rules.itgenerates only correct or perfect rules.it measures the success ofa rule by theaccuracy formula pt.any rule with accuracy less than is incorrect basic methodstable of the contact lens data for which astigmatismyesand tear production ratenormal.agespectacle astigmatismtear am page algorithmsconstructing it assigns cases to the class in question that actually do not have that class.prism continues adding clauses to each rule until it is perfectits accuracy gives a summary ofthe algorithm.the outer loop iterates overthe classesgenerating rules for each class in turn.note that we reinitialize tothe full set ofexamples each time round.then we create rules for that class andremove the examples from the set until there are none ofthat class left.when-ever we create a rulestart with an empty rule covers all the examplesand then restrict it by adding tests until it covers only examples ofthe desiredclass.at each stage choose the most promising testthat isthe one that maxi-mizes the accuracy ofthe rule.finallybreak ties by selecting the test with great-est coverage.rules versus decision listsconsider the rules produced for a particular classthat isthe algorithm in with the outer loop removed.it seems clear from the way that these rulesare produced that they are intended to be interpreted in orderthat isas a deci-sion listtesting the rules in turn until one applies and then using that.this isbecause the instances covered by a new rule are removed from the instance setas soon as the rule is completed the third line from the end ofthe code infigure subsequent rules are designed for instances that are notcoveredby the rule.howeveralthough it appears that we are supposed to check the rulesin turnwe do not have to do so.consider that any subsequent rules generatedfor this class will have the same effect they all predict the same class.thismeans that it does not matter what order they are executed ineither a rule willfor each class c initialize e to the instance set while e contains instances in class c create a rule r with an empty left-hand side that predicts class c until r is perfect there are no more attributes to use do for each attribute a not mentioned in r and each value v consider adding the condition av to the lhs of r select a and v to maximize the accuracy pt ties by choosing the condition with the largest p add av to r remove the instances covered by r from e figure for a basic rule am page be found that covers this instancein which case the class in question is pre-dictedor no such rule is foundin which case the class is not predicted.now return to the overall algorithm.each class is considered in turnandrules are generated that distinguish instances in that class from the others.noordering is implied between the rules for one class and those for another.con-sequentlythe rules that are produced can be executed independent oforder.as described in section rules seem to provide moremodularity by each acting as independent nuggets of knowledge but theysuffer from the disadvantage that it is not clear what to do when conflictingrules apply.with rules generated in this waya test example may receive multi-ple classificationsthat isrules that apply to different classes may accept it.othertest examples may receive no classification at all.a simple strategy to force adecision in these ambiguous cases is to choosefrom the classifications that arepredictedthe one with the most training examples orifno classification is pre-dictedto choose the category with the most training examples overall.thesedifficulties do not occur with decision lists because they are meant to be inter-preted in order and execution stops as soon as one rule appliesthe addition ofa default rule at the end ensures that any test instance receives a classification.it is possible to generate good decision lists for the multiclass case using a slightlydifferent methodas we shall see in section such as prism can be described as separate-and-conqueralgo-rithmsyou identify a rule that covers many instances in the class excludesones not in the classseparate out the covered instances because they are alreadytaken care ofby the ruleand continue the process on those that are left.thiscontrasts nicely with the divide-and-conquer approach ofdecision trees.theseparate step greatly increases the efficiency ofthe method because the instanceset continually shrinks as the operation association rulesassociation rules are like classification rules.you could find them in the samewayby executing a divide-and-conquer rule-induction procedure for each pos-sible expression that could occur on the right-hand side ofthe rule.but not onlymight any attribute occur on the right-hand side with any possible valueasingle association rule often predicts the value ofmore than one attribute.tofind such rulesyou would have to execute the rule-induction procedure oncefor every possible combinationofattributeswith every possible combination ofvalueson the right-hand side.that would result in an enormous number ofassociation ruleswhich would then have to be pruned down on the basis oftheir coveragethe number ofinstances that they predict correctly and basic am page association same number expressed as a proportion ofthe number ofinstances to which the rule applies.this approach is quite infeasible.note thatas we mentioned in section we are calling coverageis often calledsupportand what we are calling accuracy is often called confidence.insteadwe capitalize on the fact that we are only interested in associationrules with high coverage.we ignorefor the momentthe distinction betweenthe left- and right-hand sides ofa rule and seek combinations ofattribute valuepairs that have a prespecified minimum coverage.these are called item setsanattribute value pair is an item.the terminology derives from market basketanalysisin which the items are articles in your shopping cart and the super-market manager is looking for associations among these purchases.item setsthe first column oftable shows the individual items for the weather dataoftable the number oftimes each item appears in the dataset givenat the right.these are the one-item sets.the next step is to generate the two-item sets by making pairs ofone-item ones.ofcoursethere is no point in generating a set containing two different values ofthe same attribute asoutlook outlook that cannot occur in any actualinstance.assume that we seek association rules with minimum coverage wediscard any item sets that cover fewer than two instances.this leaves two-item setssome ofwhich are shown in the second column along with thenumber oftimes they appear.the next step is to generate the three-item setsofwhich have a coverage or greater.there are four-item setsand nofive-item sets for this dataa five-item set with coverage or greater could onlycorrespond to a repeated instance.the first row ofthe tablefor exampleshowsthat there are five days when outlooksunnytwo ofwhich have temperaturemildandin facton both ofthose days humidityhighand playnoas well.association rulesshortly we will explain how to generate these item sets efficiently.but first letus finish the story.once all item sets with the required coverage have been gen-eratedthe next step is to turn each into a ruleor set ofruleswith at least thespecified minimum accuracy.some item sets will produce more than one ruleothers will produce none.for examplethere is one three-item set with a cov-erage oftable windy play set leads to seven potential am page basic methodstable sets for the weather data with coverage or greater.one-item setstwo-item setsthree-item setsfour-item am page association humidity and windy then play humidity and play then windy windy and play then humidity humidity then windy and play windy then humidity and play play then humidity and windy then humidity and windy and play figures at the right show the number ofinstances for which all three con-ditions are true that isthe coverage divided by the number ofinstances forwhich the conditions in the antecedent are true.interpreted as a fractiontheyrepresent the proportion ofinstances on which the rule is correct that isitsaccuracy.assuming that the minimum specified accuracy is the firstofthese rules will make it into the final rule set.the denominators ofthe frac-tions are readily obtained by looking up the antecedent expression in table some are not shown in the table.the final rule above has no condi-tions in the antecedentand its denominator is the total number ofinstances inthe dataset.table shows the final rule set for the weather datawith minimum cov-erage and minimum accuracy by coverage.there are coverage with coverage with coverage have twoconditions in the consequentand none has more than two.the first rule comesfrom the item set described previously.sometimes several rules arise from thesame item set.for examplerules all arise from the four-item setin row oftable humidity windy play setstwo-item setsthree-item setsfour-item am page basic methodstable rules for the weather data.association windy play windy play humidity play windy humidity windyfiplay windy play humidity windy fiplay humidity play temperature windy fiplay temperature play windy play temperature windy play temperature windyfiplay temperature play fiwindy windy play windy play play humidity windy windy play windy play temperature play temperature humidity fiplay temperature play temperature am page association has coverage subsets ofthis item set also have coverage windy humidity windy windy play these lead to rules ofwhich are accurate thetraining data.generating rules efficientlywe now consider in more detail an algorithm for producing association ruleswith specified minimum coverage and accuracy.there are two stagesgenerat-ing item sets with the specified minimum coverageand from each item setdetermining the rules that have the specified minimum accuracy.the first stage proceeds by generating all one-item sets with the givenminimum coverage first column oftable and then using this to gen-erate the two-item sets columnthree-item sets columnand soon.each operation involves a pass through the dataset to count the items ineach setand after the pass the surviving item sets are stored in a hash table a standard data structure that allows elements stored in it to be found veryquickly.from the one-item setscandidate two-item sets are generatedand thena pass is made through the datasetcounting the coverage ofeach two-item setat the end the candidate sets with less than minimum coverage are removedfrom the table.the candidate two-item sets are simply all ofthe one-item setstaken in pairsbecause a two-item set cannot have the minimum coverage unlessboth its constituent one-item sets have minimum coveragetoo.this applies ingenerala three-item set can only have the minimum coverage ifall three ofitstwo-item subsets have minimum coverage as welland similarly for four-itemsets.an example will help to explain how candidate item sets are generated.suppose there are five three-item sets b ca b da c da c eandb c d wherefor examplea is a feature such as outlooksunny.the unionofthe first twoa b c dis a candidate four-item set because its other three-item subsets c d and c d have greater than minimum coverage.ifthethree-item sets are sorted into lexical orderas they are in this listthen we needonly consider pairs whose first two members are the same.for examplewe donot consider c d and c d because b c d can also be generatedfrom b c and b dand ifthese two are not candidate three-item setsthen b c d cannot be a candidate four-item set.this leaves the pairs bc and b dwhich we have already explainedand c d and c e.this second pair leads to the set c d e whose three-item subsets do not allhave the minimum coverageso it is discarded.the hash table assists with thischeckwe simply remove each item from the set in turn and check that am page remaining three-item set is indeed present in the hash table.thus in thisexample there is only one candidate four-item seta b c d.whether or notit actually has minimum coverage can only be determined by checking theinstances in the dataset.the second stage ofthe procedure takes each item set and generates rulesfrom itchecking that they have the specified minimum accuracy.ifonly ruleswith a single test on the right-hand side were soughtit would be simply a matterofconsidering each condition in turn as the consequent ofthe ruledeleting itfrom the item setand dividing the coverage ofthe entire item set by the cov-erage ofthe resulting subset obtained from the hash table to yield the accu-racy ofthe corresponding rule.given that we are also interested in associationrules with multiple tests in the consequentit looks like we have to evaluate theeffect ofplacing each subsetofthe item set on the right-hand sideleaving theremainder ofthe set as the antecedent.this brute-force method will be excessively computation intensive unlessitem sets are smallbecause the number ofpossible subsets grows exponentiallywith the size ofthe item set.howeverthere is a better way.we observed whendescribing association rules in section that ifthe double-consequent ruleif windy and play then outlook humidity with a given minimum coverage and accuracythen both single-consequent rules formed from the same item set must also holdif humidity and windy and play outlook outlook and windy and play humidity or other ofthe single-consequent rules does not holdthereis no point in considering the double-consequent one.this gives a way ofbuild-ing up from single-consequent rules to candidate double-consequent onesfromdouble-consequent rules to candidate triple-consequent onesand so on.ofcourseeach candidate rule must be checked against the hash table to see ifitreally does have more than the specified minimum accuracy.but this generallyinvolves checking far fewer rules than the brute force method.it is interestingthat this way ofbuilding up candidate rules from actual n-consequent ones is really just the same as building up candidate from actual n-item setsdescribed earlier.discussionassociation rules are often sought for very large datasetsand efficient algo-rithms are highly valued.the method described previously makes one basic am page the dataset for each different size ofitem set.sometimes the dataset istoo large to read in to main memory and must be kept on diskthen it may beworth reducing the number ofpasses by checking item sets oftwo consecutivesizes in one go.for exampleonce sets with two items have been generatedallsets ofthree items could be generated from them before going through theinstance set to count the actual number ofitems in the sets.more three-itemsets than necessary would be consideredbut the number ofpasses through theentire dataset would be reduced.in practicethe amount ofcomputation needed to generate association rulesdepends critically on the minimum coverage specified.the accuracy has lessinfluence because it does not affect the number ofpasses that we must makethrough the dataset.in many situations we will want to obtain a certain num-ber ofrules say with the greatest possible coverage at a prespecifiedminimum accuracy level.one way to do this is to begin by specifying the cov-erage to be rather high and to then successively reduce itreexecuting the entirerule-finding algorithm for each coverage value and repeating this until thedesired number ofrules has been generated.the tabular input format that we use throughout this bookand in particu-lar a standard arff file based on itis very inefficient for many association-ruleproblems.association rules are often used when attributes are binary eitherpresent or absent and most ofthe attribute values associated with a giveninstance are absent.this is a case for the sparse data representation describedin section same algorithm for finding association rules modelsthe methods we have been looking at for decision trees and rules work mostnaturally with nominal attributes.they can be extended to numeric attributeseither by incorporating numeric-value tests directly into the decision tree or ruleinduction schemeor by prediscretizing numeric attributes into nominal ones.we will see how in chapters and are methodsthat work most naturally with numeric attributes.we look at simple ones hereones that form components ofmore complex learning methodswhich we willexamine later.numeric prediction linear regressionwhen the outcomeor classis numericand all the attributes are numericlinearregression is a natural technique to consider.this is a staple method in statis-tics.the idea is to express the class as a linear combination ofthe attributeswith predetermined am page where xis the the attribute valuesand weights are calculated from the training data.here the notation gets alittle heavybecause we need a way ofexpressing the attribute values for eachtraining instance.the first instance will have a classsay attribute the superscript denotes that it is the first example.moreoverit is notationally convenient to assume an extra attribute is always predicted value for the first instance s class can be written asthis is the predictednot the actualvalue for the first instance s class.ofinter-est is the difference between the predicted and the actual values.the method oflinear regression is to choose the coefficients wj there are ofthem tominimize the sum ofthe squares ofthese differences over all the traininginstances.suppose there are ntraining instancesdenote the ith one with asuperscript the sum ofthe squares ofthe differences iswhere the expression inside the parentheses is the difference between the ithinstance s actual class and its predicted class.this sum ofsquares is what wehave to minimize by choosing the coefficients appropriately.this is all starting to look rather formidable.howeverthe minimizationtechnique is straightforward ifyou have the appropriate math background.suffice it to say that given enough examples roughly speakingmore examplesthan attributes choosing weights to minimize the sum ofthe squared differ-ences is really not difficult.it does involve a matrix inversion operationbut thisis readily available as prepackaged software.once the math has been accomplishedthe result is a set ofnumeric weightsbased on the training datawhich we can use to predict the class ofnewinstances.we saw an example ofthis when looking at the cpu performancedataand the actual numeric weights are given in figure formulacan be used to predict the cpu performance ofnew test instances.linear regression is an excellentsimple method for numeric predictionandit has been widely used in statistical applications for decades.ofcourselinearmodels suffer from the disadvantage ofwelllinearity.ifthe data exhibits a non-linear dependencythe best-fitting straight line will be foundwhere best isinterpreted as the least mean-squared difference.this line may not fit very well.xwaijjijkin- basic am page models serve well as building blocks for more complex learn-ing methods.linear classification logistic regressionlinear regression can easily be used for classification in domains with numericattributes.indeedwe can use anyregression techniquewhether linear or non-linearfor classification.the trick is to perform a regression for each classsetting the output equal to one for training instances that belong to the classand zero for those that do not.the result is a linear expression for the class.thengiven a test example ofunknown classcalculate the value ofeachlinear expression and choose the one that is largest.this method is sometimescalled multiresponse linear regression.one way oflooking at multiresponse linear regression is to imagine that itapproximates a numeric membership functionfor each class.the membershipfunction is for instances that belong to that class and for other instances.given a new instance we calculate its membership for each class and select thebiggest.multiresponse linear regression often yields good results in practice.howeverit has two drawbacks.firstthe membership values it produces are notproper probabilities because they can fall outside the range to regression assumes that the errors are not only statistically independ-entbut are also normally distributed with the same standard deviationanassumption that is blatantly violated when the method is applied to classifica-tion problems because the observations only ever take on the values and related statistical technique called logistic regressiondoes not suffer fromthese problems.instead ofapproximating the and values directlytherebyrisking illegitimate probability values when the target is overshotlogistic regres-sion builds a linear model based on a transformed target variable.suppose first that there are only two classes.logistic regression replaces theoriginal target variablewhich cannot be approximated accurately using a linear functionwiththe resulting values are no longer constrained to the interval from to butcan lie anywhere between negative infinity and positive infinity.figure the transformation functionwhich is often called the logit transformation.the transformed variable is approximated using a linear function just likethe ones generated by linear regression.the resulting model am page with weights w.figure shows an example ofthis function in one dimen-sionwith two weights and as in linear regressionweights must be found that fit the training datawell.linear regression measures the goodness offit using the squared error.inlogistic regression the log-likelihoodofthe model is used instead.this is basic methods regressiona the logit transform and an example logistic regres-sion am page the xiare either zero or one.the weights wineed to be chosen to maximize the log-likelihood.there areseveral methods for solving this maximization problem.a simple one is to iteratively solve a sequence ofweighted least-squares regression problems untilthe log-likelihood converges to a maximumwhich usually happens in a few iterations.to generalize logistic regression to several classesone possibility is to proceedin the way described previously for multiresponse linear regression by per-forming logistic regression independently for each class.unfortunatelytheresulting probability estimates will not sum to one.to obtain proper probabil-ities it is necessary to couple the individual models for each class.this yields ajoint optimization problemand there are efficient solution methods for this.a conceptually simplerand very generalway to address multiclass problemsis known as pairwise classification.here a classifier is built for every pair ofclassesusing only the instances from these two classes.the output on anunknown test example is based on which class receives the most votes.thismethod generally yields accurate results in terms ofclassification error.it canalso be used to produce probability estimates by applying a method called pair-wise couplingwhich calibrates the individual probability estimates from the dif-ferent classifiers.ifthere are kclassespairwise classification builds a total clas-sifiers.although this sounds unnecessarily computation intensiveit is not.infactifthe classes are evenly populated pairwise classification is at least as fastas any other multiclass method.the reason is that each ofthe pairwise learn-ing problem only involves instances pertaining to the two classes under consid-eration.ifninstances are divided evenly among kclassesthis amounts to per problem.suppose the learning algorithm for a two-class problemwith ninstances takes time proportional to nseconds to execute.then the runtime for pairwise classification is proportional to is other wordsthe method scales linearly with the numberofclasses.ifthe learning algorithm takes more time say proportional to the advantage ofthe pairwise approach becomes even more pronounced.the use oflinear functions for classification can easily be visualized ininstance space.the decision boundary for two-class logistic regression lieswhere the prediction probability is isthis occurs am page because this is a linear equality in the attribute valuesthe boundary is a linearplaneor hyperplanein instance space.it is easy to visualize sets ofpoints thatcannot be separated by a single hyperplaneand these cannot be discriminatedcorrectly by logistic regression.multiresponse linear regression suffers from the same problem.each classreceives a weight vector calculated from the training data.focus for the momenton a particular pair ofclasses.suppose the weight vector for class isand the same for class with appropriate superscripts.thenan instance willbe assigned to class rather than class ifin other wordsit will be assigned to class ifthis is a linear inequality in the attribute valuesso the boundary between eachpair ofclasses is a hyperplane.the same holds true when performing pairwiseclassification.the only difference is that the boundary between two classes isgoverned by the training instances in those classes and is not influenced by theother classes.linear classification using the perceptronlogistic regression attempts to produce accurate probability estimates by max-imizing the probability ofthe training data.ofcourseaccurate probability esti-mates lead to accurate classifications.howeverit is not necessary to performprobability estimation ifthe sole purpose ofthe model is to predict class labels.a different approach is to learn a hyperplane that separates the instances per-taining to the different classes let s assume that there are only two ofthem.ifthe data can be separated perfectly into two groups using a hyperplaneit is saidto be linearly separable.it turns out that ifthe data is linearly separablethereis a very simple algorithm for finding a separating hyperplane.the algorithm is called the perceptron learning rule.before looking at it indetaillet s examine the equation for a hyperplane the attribute valuesand the weightsthat define the hyperplane.we will assume that each training instance extended by an additional attribute always has the value wedid in the case oflinear regression.this extensionwhich is called the basic am page that we don t have to include an additional constant element in the sum.ifthe sum is greater than zerowe will predict the first classotherwisewe willpredict the second class.we want to find values for the weights so that the train-ing data is correctly classified by the hyperplane.figure gives the perceptron learning rule for finding a separatinghyperplane.the algorithm iterates until a perfect solution has been foundbutit will only work properly ifa separating hyperplane existsthat isifthe data islinearly separable.each iteration goes through all the training instances.ifamisclassified instance is encounteredthe parameters ofthe hyperplane arechanged so that the misclassified instance moves closer to the hyperplane ormaybe even across the hyperplane onto the correct side.ifthe instance belongsto the first classthis is done by adding its attribute values to the weight vectorotherwisethey are subtracted from it.set all weights to zerountil all instances in the training data are classified correctly for each instance i in the training data if i is classified incorrectly by the perceptron if i belongs to the first class add it to the weight vector else subtract it from the weight vectora bias perceptrona learning rule and representation as a neural am page to see why this worksconsider the situation after an instance apertainingto the first class has been addedthis means the output for ahas increased bythis number is always positive.thus the hyperplane has moved in the correctdirection for classifying instance aas positive.converselyifan instance belong-ing to the second class is misclassifiedthe output for that instance decreasesafter the modificationagain moving the hyperplane to the correct direction.these corrections are incremental and can interfere with earlier updates.howeverit can be shown that the algorithm converges in a finite number ofiterations ifthe data is linearly separable.ofcourseifthe data is not linearlyseparablethe algorithm will not terminateso an upper bound needs to beimposed on the number ofiterations when this method is applied in practice.the resulting hyperplane is called a perceptronand it s the grandfather ofneural networks return to neural networks in section the perceptron as a graph with nodes and weighted edgesimagina-tively termed a network of neurons. there are two layers ofnodesinput andoutput.the input layer has one node for every attributeplus an extra node thatis always set to one.the output layer consists ofjust one node.every node inthe input layer is connected to the output layer.the connections are weightedand the weights are those numbers found by the perceptron learning rule.when an instance is presented to the perceptronits attribute values serve to activate the input layer.they are multiplied by the weights and summed upat the output node.ifthe weighted sum is greater than the output signal is the first classotherwiseit is the second.linear classification using winnowthe perceptron algorithm is not the only method that is guaranteed to find aseparating hyperplane for a linearly separable problem.for datasets with binaryattributes there is an alternative known as winnowshown in figure structure ofthe two algorithms is very similar.like the perceptronwinnowonly updates the weight vector when a misclassified instance is encountered it is mistake driven.the two methods differ in how the weights are updated.the perceptron ruleemploys an additive mechanism that alters the weight vector by adding sub-tracting the instance s attribute vector.winnow employs multiplicative updatesand alters weights individually by multiplying them by the user-specifiedparameter aor its inverse.the attribute values aiare either or because basic am page some instances are misclassified for every instance a classify a using the current weights if the predicted class is incorrect if a belongs to the first class for each ai that is multiply wi by a ai is leave wi unchanged otherwise for each ai that is divide wi by a ai is leave wi unchangeda while some instances are misclassified for every instance a classify a using the current weights if the predicted class is incorrect if a belongs to the first class for each ai that is multiply wi by a divide wi by a ai is leave wi and wi- unchanged otherwise for for each ai that is multiply wi by a divide wi by a ai is leave wi and wi- unchangedbfigure winnow algorithma the unbalanced version and the am page are working with binary data.weights are unchanged ifthe attribute value is then they do not participate in the decision.otherwisethe multiplieris aifthat attribute helps to make a correct decision and does not.another difference is that the threshold in the linear function is also a user-specified parameter.we call this threshold qand classify an instance as belong-ing to class ifand only ifthe multiplier aneeds to be greater than one.the wiare set to a constant atthe start.the algorithm we have described doesn t allow negative weightswhich depending on the domain can be a drawback.howeverthere is a versioncalled balanced winnowwhich does allow them.this version maintains twoweight vectorsone for each class.an instance is classified as belonging to iffigure shows the balanced algorithm.winnow is very effective in homing in on the relevant features in a dataset therefore it is called an attribute-efficientlearner.that means that it may be agood candidate algorithm ifa dataset has many features and most ofthem are irrelevant.both winnow and the perceptron algorithm can be used inan online setting in which new instances arrive continuouslybecause they canincrementally update their hypotheses as new instances learningin instance-based learning the training examples are stored verbatimand a dis-tance function is used to determine which member ofthe training set is closestto an unknown test instance.once the nearest training instance has beenlocatedits class is predicted for the test instance.the only remaining problemis defining the distance functionand that is not very difficult to doparticularlyifthe attributes are numeric.the distance functionalthough there are other possible choicesmost instance-based learners useeuclidean distance.the distance between an instance with attribute values kis the number ofattributes and one with values defined basic am page comparing distances it is not necessary to perform the square root oper-ationthe sums ofsquares can be compared directly.one alternative to theeuclidean distance is the manhattan or city-block metricwhere the differencebetween attribute values is not squared but just added up taking theabsolute value.others are obtained by taking powers higher than the square.higher powers increase the influence oflarge differences at the expense ofsmalldifferences.generallythe euclidean distance represents a good compromise.other distance metrics may be more appropriate in special circumstances.thekey is to think ofactual instances and what it means for them to be separatedby a certain distance what would twice that distance meanfor example?different attributes are measured on different scalesso ifthe euclidean distance formula were used directlythe effects ofsome attributes might be completely dwarfed by others that had larger scales ofmeasurement.conse-quentlyit is usual to normalize all attribute values to lie between and viis the actual value ofattribute iand the maximum and minimum aretaken over all instances in the training set.these formulae implicitly assume numeric attributes.herethe differencebetween two values is just the numerical difference between themand it is thisdifference that is squared and added to yield the distance function.for nominalattributes that take on values that are symbolic rather than numericthe differ-ence between two values that are not the same is often taken to be onewhereasifthe values are the same the difference is zero.no scaling is required in thiscase because only the values and are used.a common policy for handling missing values is as follows.for nominalattributesassume that a missing feature is maximally different from any otherfeature value.thus ifeither or both values are missingor ifthe values are dif-ferentthe difference between them is taken as onethe difference is zero onlyifthey are not missing and both are the same.for numeric attributesthe dif-ference between two missing values is also taken as one.howeverifjust onevalue is missingthe difference is often taken as either the size ofthe other value or one minus that sizewhichever is larger.this means that ifvalues are missingthe difference is as large as it can possibly be.finding nearest neighbors efficientlyalthough instance-based learning is simple and effectiveit is often slow.theobvious way to find which member ofthe training set is closest to an unknowntest instance is to calculate the distance from every member ofthe training am page and select the smallest.this procedure is linear in the number oftraininginstancesin other wordsthe time it takes to make a single prediction is pro-portional to the number oftraining instances.processing an entire test set takestime proportional to the product ofthe number ofinstances in the training andtest sets.nearest neighbors can be found more efficiently by representing the trainingset as a treealthough it is not quite obvious how.one suitable structure is akd-tree.this is a binary tree that divides the input space with a hyperplane and then splits each partition againrecursively.all splits are made parallel toone ofthe axeseither vertically or horizontallyin the two-dimensional case.the data structure is called a kd-treebecause it stores a set ofpoints in k-dimensional spacekbeing the number ofattributes.figure gives a small example with figure shows thefour training instances it representsalong with the hyperplanes that constitutethe tree.note that these hyperplanes are notdecision boundariesdecisions aremade on a nearest-neighbor basis as explained later.the first split is horizon-tal the point this is the tree s root.the left branch is notsplit furtherit contains the single point is a leafofthe tree.theright branch is split vertically at the point left child is emptyandits right child contains the point this example illustrateseach regioncontains just one point orperhapsno points.sibling branches ofthe tree for examplethe two daughters ofthe root in figure are not neces-sarily developed to the same depth.every point in the training set correspondsto a single nodeand up to halfare basic kd-tree for four training instancesa the tree and instances am page do you build a kd-tree from a dataset? can it be updated efficiently asnew training examples are added? and how does it speed up nearest-neighborcalculations? we tackle the last question first.to locate the nearest neighbor ofa given target pointfollow the tree downfrom its root to locate the region containing the target.figure shows a spacelike that offigure but with a few more instances and an extra bound-ary.the targetwhich is not one ofthe instances in the treeis marked by a star.the leafnode ofthe region containing the target is colored black.this is notnecessarily the target s closest neighboras this example illustratesbut it is agood first approximation.in particularany nearer neighbor must lie closer within the dashed circle in figure determine whether one existsfirstcheck whether it is possible for a closer neighbor to lie within the node s sibling.the black node s sibling is shaded in figure the circle does not inter-sect itso the sibling cannot contain a closer neighbor.then back up to theparent node and check itssibling which here covers everything above the hor-izontal line.in this case it must be exploredbecause the area it covers intersectswith the best circle so far.to explore itfind its daughters original point stwo auntscheck whether they intersect the circle left one does notbutthe right one doesand descend to see whether it contains a closer point a kd-tree to find the nearest neighbor ofthe am page in a typical casethis algorithm is far faster than examining all points to findthe nearest neighbor.the work involved in finding the initial approximatenearest neighbor the black point in figure depends on the depth ofthetreegiven by the logarithm ofthe number amount ofworkinvolved in backtracking to check whether this really is the nearest neighbordepends a bit on the treeand on how good the initial approximation is.but fora well-constructed tree whose nodes are approximately squarerather than longskinny rectanglesit can also be shown to be logarithmic in the number ofnodes.how do you build a good tree for a set oftraining examples? the problemboils down to selecting the first training instance to split at and the direction ofthe split.once you can do thatapply the same method recursively to each childofthe initial split to construct the entire tree.to find a good direction for the splitcalculate the variance ofthe data pointsalong each axis individuallyselect the axis with the greatest varianceand createa splitting hyperplane perpendicular to it.to find a good place for the hyper-planelocate the median value along that axis and select the correspondingpoint.this makes the split perpendicular to the direction ofgreatest spreadwith halfthe points lying on either side.this produces a well-balanced tree.toavoid long skinny regions it is best for successive splits to be along different axeswhich is likely because the dimension ofgreatest variance is chosen at each stage.howeverifthe distribution ofpoints is badly skewedchoosing the medianvalue may generate several successive splits in the same directionyielding longskinny hyperrectangles.a better strategy is to calculate the mean rather than themedian and use the point closest to that.the tree will not be perfectly balancedbut its regions will tend to be squarish because there is a greater chance that dif-ferent directions will be chosen for successive splits.an advantage ofinstance-based learning over most other machine learningmethods is that new examples can be added to the training set at any time.toretain this advantage when using a kd-treewe need to be able to update it incre-mentally with new data points.to do thisdetermine which leafnode containsthe new point and find its hyperrectangle.ifit is emptysimply place the newpoint there.otherwise split the hyperrectanglesplitting it along its longestdimension to preserve squareness.this simple heuristic does not guarantee thatadding a series ofpoints will preserve the tree s balancenor that the hyperrec-tangles will be well shaped for nearest-neighbor search.it is a good idea torebuild the tree from scratch occasionally for examplewhen its depth growsto twice the best possible depth.as we have seenkd-trees are good data structures for finding nearest neigh-bors efficiently.howeverthey are not perfect.skewed datasets present a basicconflict between the desire for the tree to be perfectly balanced and the desirefor regions to be squarish.more importantlyrectangles even squares are notthe best shape to use anywaybecause oftheir corners.ifthe dashed circle basic am page were any biggerwhich it would be ifthe black instance were a littlefurther from the targetit would intersect the lower right-hand corner oftherectangle at the top left and then that rectangle would have to be investigatedtoo despite the fact that the training instances that define it are a long wayfrom the corner in question.the corners ofrectangular regions are awkward.the solution? use hyperspheresnot hyperrectangles.neighboring spheresmay overlap whereas rectangles can abutbut this is not a problem because thenearest-neighbor algorithm for kd-trees described previously does not dependon the regions being disjoint.a data structure called a ball treedefines k-dimensional hyperspheres balls that cover the data pointsand arrangesthem into a tree.figure shows training instances in two-dimensional spaceover-laid by a pattern ofoverlapping circlesand figure shows a tree formedfrom these circles.circles at different levels ofthe tree are indicated by differ-ent styles ofdashand the smaller circles are drawn in shades ofgray.each nodeofthe tree represents a balland the node is dashed or shaded according to thesame convention so that you can identify which level the balls are at.to helpyou understand the treenumbers are placed on the nodes to show how manydata points are deemed to be inside that ball.but be carefulthis is not neces-sarily the same as the number ofpoints falling within the spatial region that theball represents.the regions at each level sometimes overlapbut points that fallinto the overlap area are assigned to only one ofthe overlapping balls does not show which one.instead ofthe occupancy counts in the nodes ofactual ball trees store the center and radius oftheir ballleafnodes record the points they contain as well.to use a ball tree to find the nearest neighbor to a given targetstart by tra-versing the tree from the top down to locate the leafthat contains the target andfind the closest point to the target in that ball.this gives an upper bound forthe target s distance from its nearest neighbor.thenjust as for the kd-treeexamine the sibling node.ifthe distance from the target to the sibling s centerexceeds its radius plus the current upper boundit cannot possibly contain acloser pointotherwise the sibling must be examined by descending the treefurther.in figure the target is marked with a star and the black dot is itsclosest currently known neighbor.the entire contents ofthe gray ball can beruled outit cannot contain a closer point because its center is too far away.proceed recursively back up the tree to its rootexamining any ball that maypossibly contain a point nearer than the current upper bound.ball trees are built from the top downand as with kd-trees the basic problemis to find a good way ofsplitting a ball containing a set ofdata points into two.in practice you do not have to continue until the leafballs contain just two pointsyou can stop earlieronce a predetermined minimum number isreached and the same goes for kd-trees.here is one possible splitting am page basic tree for training instancesa instances and balls and the am page the point in the ball that is farthest from its centerand then a secondpoint that is farthest from the first one.assign all data points in the ball to theclosest one ofthese two cluster centersthen compute the centroid ofeachcluster and the minimum radius required for it to enclose all the data points itrepresents.this method has the merit that the cost ofsplitting a ball contain-ing npoints is only linear in n.there are more elaborate algorithms thatproduce tighter ballsbut they require more computation.we will not describesophisticated algorithms for constructing ball trees or updating them incre-mentally as new training instances are encountered.discussionnearest-neighbor instance-based learning is simple and often works very well.in the method described previously each attribute has exactly the sameinfluence on the decisionjust as it does in the na ve bayes method.anotherproblem is that the database can easily become corrupted by noisy exemplars.one solution is to adopt the k-nearest-neighbor strategywhere some fixedsmallnumber kofnearest neighbors say five are located and used togetherto determine the class ofthe test instance through a simple majority vote.notethat we used kto denote the number ofattributes earlierthis is a differentinde-pendent usage. another way ofproofing the database against noise is to choosethe exemplars that are added to it selectively and judiciouslyimproved proce-duresdescribed in chapter these shortcomings.figure out an entire ball based on a target point and its currentnearest am page the nearest-neighbor method originated many decades agoand statisticiansanalyzed k-nearest-neighbor schemes in the early number oftrain-ing instances is largeit makes intuitive sense to use more than one nearestneighborbut clearly this is dangerous ifthere are few instances.it can be shownthat when kand the number nofinstances both become infinite in such a waythatkn probability oferror approaches the theoretical minimum forthe dataset.the nearest-neighbor method was adopted as a classificationmethod in the early and has been widely used in the field ofpattern recog-nition for more than three decades.nearest-neighbor classification was notoriously slow until kd-trees began tobe applied in the early the data structure itselfwas developedmuch earlier.in practicethese trees become inefficient when the dimension ofthe space increases and are only worthwhile when the number ofattributes issmall up to trees were developed much more recently and are aninstance ofa more general structure sometimes called a metric tree.sophisti-cated algorithms can create metric trees that deal successfully with thousandsofdimensions.instead ofstoring all training instancesyou can compress them into regions.a very simple techniquementioned at the end ofsection to just recordthe range ofvalues observed in the training data for each attribute and cate-gory.given a test instanceyou work out which ranges the attribute values fallinto and choose the category with the greatest number ofcorrect ranges for thatinstance.a slightly more elaborate technique is to construct intervals for eachattribute and use the training set to count the number oftimes each class occursfor each interval on each attribute.numeric attributes can be discretized intointervalsand intervals consisting ofa single point can be used for nominalones.thengiven a test instanceyou can determine which intervals it residesin and classify it by votinga method called voting feature intervals.thesemethods are very approximatebut very fastand can be useful for initial analy-sis oflarge techniques apply when there is no class to be predicted but ratherwhen the instances are to be divided into natural groups.these clusters pre-sumably reflect some mechanism at work in the domain from which instancesare drawna mechanism that causes some instances to bear a stronger resem-blance to each other than they do to the remaining instances.clustering natu-rally requires different techniques to the classification and association learningmethods we have considered so basic am page as we saw in section are different ways in which the result ofclus-tering can be expressed.the groups that are identified may be exclusive so thatany instance belongs in only one group.or they may be overlapping so that aninstance may fall into several groups.or they may be probabilisticwhereby aninstance belongs to each group with a certain probability.or they may be hier-archicalsuch that there is a crude division ofinstances into groups at the topleveland each ofthese groups is refined further perhaps all the way down toindividual instances.reallythe choice among these possibilities should be dic-tated by the nature ofthe mechanisms that are thought to underlie the partic-ular clustering phenomenon.howeverbecause these mechanisms are rarelyknown the very existence ofclusters isafter allsomething that we re tryingto discover and for pragmatic reasons toothe choice is usually dictated by theclustering tools that are available.we will examine an algorithm that forms clusters in numeric domainspar-titioning instances into disjoint clusters.like the basic nearest-neighbor methodofinstance-based learningit is a simple and straightforward technique that has been used for several decades.in chapter we examine newer clusteringmethods that perform incremental and probabilistic clustering.iterative distance-based clusteringthe classic clustering technique is called k-means.firstyou specify in advancehow many clusters are being soughtthis is the parameter k.then kpoints arechosen at random as cluster centers.all instances are assigned to their closestcluster center according to the ordinary euclidean distance metric.next the cen-troidor meanofthe instances in each cluster is calculated this is the means part.these centroids are taken to be new center values for their respective clus-ters.finallythe whole process is repeated with the new cluster centers.itera-tion continues until the same points are assigned to each cluster in consecutiveroundsat which stage the cluster centers have stabilized and will remain thesame forever.this clustering method is simple and effective.it is easy to prove that choos-ing the cluster center to be the centroid minimizes the total squared distancefrom each ofthe cluster s points to its center.once the iteration has stabilizedeach point is assigned to its nearest cluster centerso the overall effect is to min-imize the total squared distance from all points to their cluster centers.but theminimum is a local onethere is no guarantee that it is the global minimum.the final clusters are quite sensitive to the initial cluster centers.completely dif-ferent arrangements can arise from small changes in the initial random choice.in factthis is true ofall practical clustering techniquesit is almost always infea-sible to find globally optimal clusters.to increase the chance offinding a am page minimum people often run the algorithm several times with different initialchoices and choose the best final result the one with the smallest total squareddistance.it is easy to imagine situations in which k-means fails to find a good cluster-ing.consider four instances arranged at the vertices ofa rectangle in two-dimensional space.there are two natural clustersformed by grouping togetherthe two vertices at either end ofa short side.but suppose that the two initialcluster centers happen to fall at the midpoints ofthe longsides.this forms astable configuration.the two clusters each contain the two instances at eitherend ofa long side no matter how great the difference between the long andthe short sides.faster distance calculationsthe k-means clustering algorithm usually requires several iterationseachinvolving finding the distance ofkcluster centers from every instance to deter-mine its cluster.there are simple approximations that speed this up consider-ably.for exampleyou can project the dataset and make cuts along selected axesinstead ofusing the arbitrary hyperplane divisions that are implied by choos-ing the nearest cluster center.but this inevitably compromises the quality oftheresulting clusters.here s a better way ofspeeding things up.finding the closest cluster centeris not so different from finding nearest neighbors in instance-based learning.can the same efficient solutions kd-trees and ball trees be used? yes! indeedthey can be applied in an even more efficient waybecause in each iteration ofk-means all the data points are processed togetherwhereas in instance-basedlearning test instances are processed individually.firstconstruct a kd-tree or ball tree for all the data pointswhich will remainstatic throughout the clustering procedure.each iteration ofk-means producesa set ofcluster centersand all data points must be examined and assigned tothe nearest center.one way ofprocessing the points is to descend the tree fromthe root until reaching a leafand check each individual point in the leafto findits closest cluster center.but it may be that the region represented by a higherinterior node falls entirely within the domain ofa single cluster center.in thatcase all the data points under that node can be processed in one blow!the aim ofthe exerciseafter allis to find new positions for the cluster centersby calculating the centroid ofthe points they contain.the centroid can be cal-culated by keeping a running vector sum ofthe points in the clusterand a countofhow many there are so far.at the endjust divide one by the other to findthe centroid.suppose that with each node ofthe tree we store the vector sumofthe points within that node and a count ofthe number ofpoints.ifthe wholenode falls within the ambit ofa single clusterthe running totals for that basic am page can be updated immediately.ifnotlook inside the node by proceeding recur-sively down the tree.figure shows the same instances and ball tree as figure withtwo cluster centers marked as black stars.because all instances are assigned tothe closest centerthe space is divided in two by the thick line shown in at the root ofthe tree in figure initial values for thevector sum and counts for each clusterall initial values are zero.proceed recur-sively down the tree.when node a is reachedall points within it lie in cluster s sum and count can be updated with the sum and count for nodeaand we need descend no further.recursing back to node bits ball straddlesthe boundary between the clustersso its points must be examined individually.when node c is reachedit falls entirely within cluster can updatecluster immediately and need descend no further.the tree is only examineddown to the frontier marked by the dashed line in figure the advan-tage is that the nodes below need not be opened at leastnot on this particu-lar iteration ofk-means.next timethe cluster centers will have changed andthings may be different.discussionmany variants ofthe basic k-means procedure have been developed.someproduce a hierarchical clustering by applying the algorithm with to theoverall dataset and then repeatingrecursivelywithin each cluster.how do you choose k?often nothing is known about the likely number ofclustersand the whole point ofclustering is to find out.one way is to try dif-ferent values and choose the best.to do this you need to learn how to evaluatethe success ofmachine learningwhich is what chapter is about.we returnto clustering in section readingthe scheme was proposed and thoroughly investigated by holte never really intended as a machine learning method point was moreto demonstrate that very simple structures underlie most ofthe practicaldatasets being used to evaluate machine learning methods at the time and thatputting high-powered inductive inference methods to work on simple datasetswas like using a sledgehammer to crack a nut.why grapple with a complex deci-sion tree when a simple rule will do? the method that generates one simple ruleper class is the result ofwork by lucio de souza coelho ofbrazil and len triggofnew zealandand it has been dubbed hyperpipes.a very simple algorithmit has the advantage ofbeing extremely fast and is quite feasible even with anenormous number am page ball treea two cluster centers and their dividing line and the cor-responding am page bayes was an eighteenth-century english philosopher who set out his theoryofprobability in an essay towards solving a problem in the doctrine ofchances published in the philosophical transactions ofthe royal society oflondonbayes rule that bears his name has been a cornerstone ofprobability theory ever since.the difficulty with the application ofbayes srule in practice is the assignment ofprior probabilities.some statisticiansdubbed bayesianstake the rule as gospel and insist that people make seriousattempts to estimate prior probabilities accurately although such estimates areoften subjective.othersnon-bayesiansprefer the kind ofprior-free analysisthat typically generates statistical confidence intervalswhich we will meet in thenext chapter.with a particular datasetprior probabilities are usually reason-ably easy to estimatewhich encourages a bayesian approach to learning.theindependence assumption made by the na ve bayes method is a great stumblingblockhoweverand some attempts are being made to apply bayesian analysiswithout assuming independence.the resulting models are called bayesian net-worksheckerman et we describe them in section techniques had been used in the field ofpattern recognition hart for years before they were adopted by machine learningresearchers langley et and made to work on datasets withredundant attributes and sage and numeric attributes andlangley label na ve bayesis unfortunate because it is hard to usethis method without feeling simpleminded.howeverthere is nothing na veabout its use in appropriate circumstances.the multinomial na ve bayes modelwhich is particularly appropriate for text classificationwas investigated bymccallum and nigam classic paper on decision tree induction is by quinlan the basic procedure developed in this chapter.a comprehensivedescription ofthe methodincluding the improvements that are embodied in a classic book by quinlan gives a listing ofthecomplete systemwritten in the c programming language.prism wasdeveloped by cendrowska also introduced the contact lens dataset.association rules are introduced and described in the database literaturerather than in the machine learning literature.here the emphasis is very muchon dealing with huge amounts ofdata rather than on sensitive ways oftestingand evaluating algorithms on limited datasets.the algorithm introduced in thischapter is the apriori method developed by agrawal and his associates and srikant survey ofassociation-rulemining appears in an article by chen et regression is described in most standard statistical textsand a partic-ularly comprehensive treatment can be found in a book by lawson and use oflinear models for classification enjoyed a great deal ofpop-ularity in the provides an excellent reference.he am page a linear threshold unitas a binary test ofwhether a linear function is greater orless than zero and a linear machineas a set oflinear functionsone for each classwhose value for an unknown example is compared and the largest chosen as itspredicted class.in the distant pastperceptrons fell out offavor on publicationofan influential book that showed they had fundamental limitations papert complex systems oflinear functions haveenjoyed a resurgence in recent years in the form ofneural networksdescribedin section winnow algorithms were introduced by nick littlestone inhis phd thesis in linear classifiershave found a new application recently for an operation called stackingthat com-bines the output ofother learning algorithmsdescribed in chapter describes the technique ofpairwise classifica-tionf rnkranz further analyzes itand hastie and tibshirani it to estimate probabilities using pairwise coupling.fix and hodges performed the first analysis ofthe nearest-neighbormethodand johns pioneered its use in classification problems.coverand hart obtained the classic theoretical result thatfor large enoughdatasetsits probability oferror never exceeds twice the theoretical minimumdevroye et showed that k-nearest neighbor is asymptotically optimalfor large kand nwithkn methods gained popularity inmachine learning through the work ofaha showed that instance-based learning can be combined with noisy exemplar pruning and attributeweighting and that the resulting methods perform well in comparison withother learning methods.we take this up again in chapter kd-tree data structure was developed by friedman et closely follows an explanation given by andrew moore in his phdthesis with omohundro its use inmachine learning.moore describes sophisticated ways ofconstructingball trees that perform well even with thousands ofattributes.we took our balltree example from lecture notes by alexander gray ofcarnegie-mellon uni-versity.the voting feature intervals method mentioned in the discussion sub-section at the end ofsection is described by demiroz and guvenir k-means algorithm is a classic techniqueand many descriptions andvariations are available hartigan clever use ofkd-trees tospeed up k-means clusteringwhich we chose to illustrate using ball treesinsteadwas pioneered by moore and pelleg in their x-means clusteringalgorithm.that algorithm also contains some other innovationsdescribed insection basic am page evaluation is the key to making real progress in data mining.there are lots ofways ofinferring structure from datawe have encountered many already andwill see further refinementsand new methodsin the next chapter.but to deter-mine which ones to use on a particular problem we need systematic ways toevaluate how different methods work and to compare one with another.eval-uation is not as simple as it might appear at first sight.what s the problem? we have the training setsurely we can just look at howwell different methods do on that.wellnoas we will see very shortlyper-formance on the training set is definitely not a good indicator ofperformanceon an independent test set.we need ways ofpredicting performance bounds inpracticebased on experiments with whatever data can be obtained.when a vast supply ofdata is availablethis is no problemjust make a modelbased on a large training setand try it out on another large test set.but althoughdata mining sometimes involves big data particularly in marketingsalesand customer support applications it is often the case that dataquality datais scarce.the oil slicks mentioned in chapter had to be what s been am page and marked manually a skilled and labor-intensive process before beingused as training data.even in the credit card application out to be only training examples ofthe appropriate type.the elec-tricity supply data went back days but only days and thanksgivingsand just february and presidentialelections.the electromechanical diagnosis application was ableto capitalize on years ofrecorded experiencebut this yielded only usableexamples offaults.marketing and sales applications certainlyinvolve big databut many others do nottraining data frequently relies on spe-cialist human expertise and that is always in short supply.the question ofpredicting performance based on limited data is an inter-estingand still controversialone.we will encounter many different techniquesofwhich one repeated cross-validation is gaining ascendance and is proba-bly the evaluation method ofchoice in most practical limited-data situations.comparing the performance ofdifferent machine learning methods on a givenproblem is another matter that is not so easy as it soundsto be sure that appar-ent differences are not caused by chance effectsstatistical tests are needed.sofar we have tacitly assumed that what is being predicted is the ability to classifytest instances accuratelyhoweversome situations involve predicting the classprobabilities rather than the classes themselvesand others involve predictingnumeric rather than nominal values.different methods are needed in each case.then we look at the question ofcost.in most practical data mining situationsthe cost ofa misclassification error depends on the type oferror it is whetherfor examplea positive example was erroneously classified as negative or viceversa.when doing data miningand evaluating its performanceit is often essen-tial to take these costs into account.fortunatelythere are simple techniques tomake most learning schemes cost sensitive without grappling with the internalsofthe algorithm.finallythe whole notion ofevaluation has fascinating philo-sophical connections.for years philosophers have debated the question ofhow to evaluate scientific theoriesand the issues are brought into sharp focusby data mining because what is extracted is essentially a theory ofthe and testingfor classification problemsit is natural to measure a classifier s performance interms ofthe errorrate.the classifier predicts the class ofeach instanceifit iscorrectthat is counted as a successifnotit is an error.the error rate is just theproportion oferrors made over a whole set ofinstancesand it measures theoverall performance ofthe classifier.ofcoursewhat we are interested in is the likely future performance on newdatanot the past performance on old data.we already know the what s been am page ofeach instance in the training setwhich after all is why we can use it for train-ing.we are not generally interested in learning about those classifications although we might be ifour purpose is data cleansing rather than prediction.so the question isis the error rate on old data likely to be a good indicator ofthe error rate on new data? the answer is a resounding no not ifthe old datawas used during the learning process to train the classifier.this is a surprising factand a very important one.error rate on the train-ing set is notlikely to be a good indicator offuture performance.why? becausethe classifier has been learned from the very same training dataany estimate ofperformance based on that data will be optimisticand may be hopelesslyoptimistic.we have already seen an example ofthis in the labor relations was generated directly from the training dataand figure wasobtained from it by a process ofpruning.the former is likely to be more accu-rate on the data that was used to train the classifier but will probably performless well on independent test data because it is overfitted to the training data.the first tree will look good according to the error rate on the training databetter than the second tree.but this does not reflect how they will perform onindependent test data.the error rate on the training data is called the resubstitutionerrorbecauseit is calculated by resubstituting the training instances into a classifier that wasconstructed from them.although it is not a reliable predictor ofthe true errorrate on new datait is nevertheless often useful to know.to predict the performance ofa classifier on new datawe need to assess itserror rate on a dataset that played no part in the formation ofthe classifier.thisindependent dataset is called the testset.we assume that both the training dataand the test data are representative samples ofthe underlying problem.in some cases the test data might be distinct in nature from the training data.considerfor examplethe credit risk problem from section thebank had training data from branches in new york city and florida and wantedto know how well a classifier trained on one ofthese datasets would perform ina new branch in nebraska.it should probably use the florida data as test datato evaluate the new york-trained classifier and the new york data to evaluatethe florida-trained classifier.ifthe datasets were amalgamated before trainingperformance on the test data would probably not be a good indicator ofper-formance on future data in a completely different state.it is important that the test data was not used inanywayto create the clas-sifier.for examplesome learning methods involve two stagesone to come upwith a basic structure and the second to optimize parameters involved in thatstructureand separate sets ofdata may be needed in the two stages.or youmight try out several learning schemes on the training data and then evaluatethem on a fresh datasetofcourse to see which one works best.but none and am page this data may be used to determine an estimate ofthe future error rate.in suchsituations people often talk about three datasetsthe trainingdatathe valida-tiondataand the testdata.the training data is used by one or more learningmethods to come up with classifiers.the validation data is used to optimizeparameters ofthose classifiersor to select a particular one.then the test datais used to calculate the error rate ofthe finaloptimizedmethod.each ofthethree sets must be chosen independentlythe validation set must be differentfrom the training set to obtain good performance in the optimization or selec-tion stageand the test set must be different from both to obtain a reliable esti-mate ofthe true error rate.it may be that once the error rate has been determinedthe test data isbundled back into the training data to produce a new classifier for actual use.there is nothing wrong with thisit is just a way ofmaximizing the amount ofdata used to generate the classifier that will actually be employed in practice.what is important is that error rates are not quoted based on any ofthis data.alsoonce the validation data has been used maybe to determine the best typeoflearning scheme to use then it can be bundled back into the training datato retrain that learning schememaximizing the use ofdata.iflots ofdata is availablethere is no problemwe take a large sample anduse it for trainingthen anotherindependent large sample ofdifferent data anduse it for testing.provided that both samples are representativethe error rateon the test set will give a true indication offuture performance.generallythelarger the training sample the better the classifieralthough the returns begin todiminish once a certain volume oftraining data is exceeded.and the larger thetest samplethe more accurate the error estimate.the accuracy ofthe error esti-mate can be quantified statisticallyas we will see in the next section.the real problem occurs when there is not a vast supply ofdata available.inmany situations the training data must be classified manually and so must thetest dataofcourseto obtain error estimates.this limits the amount ofdatathat can be used for trainingvalidationand testingand the problem becomeshow to make the most ofa limited dataset.from this dataseta certain amountis held over for testing this is called the holdoutprocedure and the remain-der is used for training ofthat is set aside for validation.there s a dilemma hereto find a good classifierwe want to use as much ofthedata as possible for trainingto obtain a good error estimatewe want to use asmuch ofit as possible for testing.sections and review widely usedmethods for dealing with this performancesuppose we measure the error ofa classifier on a test set and obtain a certainnumeric error rate say this section we refer to success what s been am page rather than error rateso this corresponds to a success rate isonly an estimate.what can you say about the truesuccess rate on the targetpopulation? sureit s expected to be close to how close within it must depend on the size ofthe test set.naturallywe would bemore confident ofthe figure ifit was based on a test set instancesrather than on a test set instances.but how much more confident wouldwe be?to answer these questionswe need some statistical reasoning.in statisticsasuccession ofindependent events that either succeed or fail is called a bernoulliprocess.the classic example is coin tossing.each toss is an independent event.let s say we always predict headsbut rather than heads or tails each toss is considered a success or a failure. let s say the coin is biasedbut we don tknow what the probability ofheads is.thenifwe actually toss the coin and ofthem are headswe have a situation much like the one describedpreviously for a classifier with an observed success rate on a test set.whatcan we say about the true success probability? in other wordsimagine that thereis a bernoulli process a biased coin whose true unknown success rateis p.suppose that out ofntrialssare successesthus the observed success rateis fsn.the question iswhat does this tell you about the true success rate p?the answer to this question is usually expressed as a confidence intervalthatisplies within a certain specified interval with a certain specified confidence.for successes are observed out trialsthis indi-cates that the true success rate must be around how close to itturns out that with confidencethe true success rate plies between successes are observed out trialsthis also indi-cates that the true success rate must be around the experiment issmallerand the confidence interval for pis widerstretching from figures are easy to relate to qualitativelybut how are they derived quan-titatively? we reason as followsthe mean and variance ofa single bernoulli trialwith success rate pare pand are taken from abernoulli processthe expected success rate fsnis a random variable withthe same mean pthe variance is reduced by a factor ofnto nthe distribution ofthis random variable approaches the normal distri-bution.these are all facts ofstatisticswe will not go into how they are derived.the probability that a random variable xwith zero meanlies within acertain confidence range ofwidth a normal distributionvalues ofcand corresponding values ofzare givenin tables printed at the back ofmost statistical texts.howeverthe tabulationsconventionally take a slightly different formthey give the confidence that xwillpr- am page what s been learnedlie outside the rangeand they give it for the upper part ofthe range onlythis is called a one-tailedprobability because it refers only to the upper tail ofthe distribution.normal distributions are symmetricso the probabilities forthe lower tailare just the same.table gives an example.like other tables for the normal distributionthisassumes that the random variable xhas a mean ofzero and a variance ofone.alternativelyyou might say that the zfigures are measured in standarddevia-tionsfromthemean.thus the figure for prx z implies that there is chance that xlies more than standard deviations above the mean.because the distribution is symmetricthe chance that xlies more than deviations from the mean or below is we need do now is reduce the random variable fto have zero mean and unitvariance.we do this by subtracting the mean pand dividing by the standarddeviation this leads tonow here is the procedure for finding confidence limits.given a particular con-fidence figure cconsult table for the corresponding zvalue.to use the tableyou will first have to subtract cfrom and then halve the resultso that for you use the table entry for interpolation can be used for inter-pr--- limits for the normal distribution.prx am page confidence levels.then write the inequality in the preceding expressionas an equality and invert it to find an expression for p.the final step involves solving a quadratic equation.although not hard todoit leads to an unpleasantly formidable expression for the confidence limitsthe in this expression gives two values for pthat represent the upper andlower confidence boundaries.although the formula looks complicatedit is nothard to work out in particular cases.this result can be used to obtain the values in the preceding numericexample.setting that leads to theinterval for pand leads to for the same levelofconfidence.note that the normal distribution assumption is only valid forlarge and leads to confidence but these should be taken with a grain consider what to do when the amount ofdata for training and testing islimited.the holdout method reserves a certain amount for testing and uses theremainder for training sets part ofthat aside for validationifrequired.in practical termsit is common to hold out one-third ofthe data for testingand use the remaining two-thirds for training.ofcourseyou may be unluckythe sample used for training testingmight not be representative.in generalyou cannot tell whether a sample is rep-resentative or not.but there is one simple check that might be worthwhileeachclass in the full dataset should be represented in about the right proportion inthe training and testing sets.ifby bad luckall examples with a certain classwere missing from the training setyou could hardly expect a classifier learnedfrom that data to perform well on the examples ofthat class and the situationwould be exacerbated by the fact that the class would necessarily be overrepre-sented in the test set because none ofits instances made it into the training set!insteadyou should ensure that the random sampling is done in such a way as to guarantee that each class is properly represented in both training and testsets.this procedure is called stratificationand we might speak ofstratifiedholdout.although it is generally well worth doingstratification provides onlya primitive safeguard against uneven representation in training and test sets.a more general way to mitigate any bias caused by the particular samplechosen for holdout is to repeat the whole processtraining and testingseveraltimes with different random samples.in each iteration a certain proportion pfznzfnfnznzn am page say two-thirds ofthe data is randomly selected for trainingpossibly withstratificationand the remainder used for testing.the error rates on the differ-ent iterations are averaged to yield an overall error rate.this is the repeatedholdoutmethod oferror rate estimation.in a single holdout procedureyou might consider swapping the roles ofthetesting and training data that istrain the system on the test data and test iton the training data and average the two resultsthus reducing the effect ofuneven representation in training and test sets.unfortunatelythis is only reallyplausible with a split between training and test datawhich is generallynot ideal it is better to use more than halfthe data for training even at theexpense oftest data.howevera simple variant forms the basis ofan importantstatistical technique called cross-validation.in cross-validationyou decide on afixed number offoldsor partitions ofthe data.suppose we use three.then thedata is split into three approximately equal partitions and each in turn is usedfor testing and the remainder is used for training.that isuse two-thirds fortraining and one-third for testing and repeat the procedure three times so thatin the endevery instance has been used exactly once for testing.this is calledthreefoldcross-validationand ifstratification is adopted as well which it oftenis it is stratifiedthreefoldcross-validation.the standard way ofpredicting the error rate ofa learning technique givena singlefixed sample ofdata is to use stratified cross-validation.thedata is divided randomly into parts in which the class is represented inapproximately the same proportions as in the full dataset.each part is held outin turn and the learning scheme trained on the remaining nine-tenthsthen itserror rate is calculated on the holdout set.thus the learning procedure is exe-cuted a total times on different training sets ofwhich have a lot incommon.finallythe error estimates are averaged to yield an overall errorestimate.why extensive tests on numerous datasetswith different learning tech-niqueshave shown that is about the right number offolds to get the bestestimate oferrorand there is also some theoretical evidence that backs this up.although these arguments are by no means conclusiveand debate continues torage in machine learning and data mining circles about what is the best schemefor cross-validation has become the standard method inpractical terms.tests have also shown that the use ofstratification improvesresults slightly.thus the standard evaluation technique in situations where onlylimited data is available is stratified cross-validation.note that neitherthe stratification nor the division into folds has to be exactit is enough todivide the data into approximately equal sets in which the various class valuesare represented in approximately the right proportion.statistical evaluation isnot an exact science.moreoverthere is nothing magic about the exact or cross-validation is likely to be almost as what s been am page a single cross-validation might not be enough to get a reliable errorestimate.different cross-validation experiments with the same learningmethod and dataset often produce different resultsbecause ofthe effect ofrandom variation in choosing the folds themselves.stratification reduces thevariationbut it certainly does not eliminate it entirely.when seeking an accu-rate error estimateit is standard procedure to repeat the cross-validationprocess times that times cross-validation and average theresults.this involves invoking the learning algorithm times on datasets thatare all nine-tenths the size ofthe original.obtaining a good measure ofper-formance is a computation-intensive estimatestenfold cross-validation is the standard way ofmeasuring the error rate ofalearning scheme on a particular datasetfor reliable times many other methods are used instead.two that are par-ticularly prevalent are leave-one-outcross-validation and the bootstrap.leave-one-outleave-one-out cross-validation is simply n-fold cross-validationwhere nis thenumber ofinstances in the dataset.each instance in turn is left outand thelearning method is trained on all the remaining instances.it is judged by its cor-rectness on the remaining instance one or zero for success or failurerespec-tively.the results ofall njudgmentsone for each member ofthe datasetareaveragedand that average represents the final error estimate.this procedure is an attractive one for two reasons.firstthe greatest possi-ble amount ofdata is used for training in each casewhich presumably increasesthe chance that the classifier is an accurate one.secondthe procedure is deter-ministicno random sampling is involved.there is no point in repeating it repeating it at allthe same result will be obtained each time.set againstthis is the high computational costbecause the entire learning procedure mustbe executed ntimes and this is usually quite infeasible for large datasets.never-thelessleave-one-out seems to offer a chance ofsqueezing the maximum out ofa small dataset and obtaining as accurate an estimate as possible.but there is a disadvantage to leave-one-out cross-validationapart from thecomputational expense.by its very natureit cannot be stratified worse thanthatit guaranteesa nonstratified sample.stratification involves getting thecorrect proportion ofexamples in each class into the test setand this is impos-sible when the test set contains only a single example.a dramaticalthoughhighly artificialillustration ofthe problems this might cause is to imagine acompletely random dataset that contains the same number ofeach am page classes.the best that an inducer can do with random data is to predict themajority classgiving a true error rate in each fold ofleave-one-outthe opposite class to the test instance is in the majority and therefore thepredictions will always be incorrectleading to an estimated error rate bootstrapthe second estimation method we describethe bootstrapis based on the sta-tistical procedure ofsampling withreplacement.previouslywhenever a samplewas taken from the dataset to form a training or test setit was drawn withoutreplacement.that isthe same instanceonce selectedcould not be selectedagain.it is like picking teams for footballyou cannot choose the same persontwice.but dataset instances are not like people.most learning methods can usethe same instance twiceand it makes a difference in the result oflearning ifitis present in the training set twice.mathematical sticklers will notice that weshould not really be talking about sets at all ifthe same object can appear morethan once.the idea ofthe bootstrap is to sample the dataset with replacement to forma training set.we will describe a particular variantmysteriously for areason that will soon become apparent called the thisadataset ofninstances is sampled ntimeswith replacementto give anotherdataset ofninstances.because some elements in this second dataset will be repeatedthere must be some instances in the original dataset thathave not been pickedwe will use these as test instances.what is the chance that a particular instance will not be picked for the train-ing set? it has a ofbeing picked each time and therefore a ofnot being picked.multiply these probabilities togetheraccording to the number ofpicking opportunitieswhich is nand the result isa figure ofwhere eis the base ofnatural the error rate!.this givesthe chance ofa particular instance not being picked at all.thus for a reason-ably large datasetthe test set will contain about ofthe instances and thetraining set will contain about ofthem you can see why it s calledthe instances will be repeated in the training setbring-ing it up to a total size ofnthe same as in the original dataset.the figure obtained by training a learning system on the training set and cal-culating its error over the test set will be a pessimistic estimate ofthe true errorratebecause the training setalthough its size is nnevertheless contains ofthe instanceswhich is not a great deal comparedfor examplewith what s been am page used in cross-validation.to compensate for thiswe combine thetest-set error rate with the resubstitution error on the instances in the trainingset.the resubstitution figureas we warned earliergives a very optimistic esti-mate ofthe true error and should certainly not be used as an error figure on itsown.but the bootstrap procedure combines it with the test error rate to give afinal estimate eas followsthenthe whole bootstrap procedure is repeated several timeswith differentreplacement samples for the training setand the results averaged.the bootstrap procedure may be the best way ofestimating error for verysmall datasets.howeverlike leave-one-out cross-validationit has disadvantagesthat can be illustrated by considering a specialartificial situation.in factthevery dataset we considered previously will doa completely random dataset withtwo classes.the true error rate is for any prediction rule.but a scheme thatmemorized the training set would give a perfect resubstitution score that etraining the bootstrap will mix this in with a to give an overall error rate ofonly is misleadingly data mining methodswe often need to compare two different learning methods on the same problemto see which is the better one to use.it seems simpleestimate the error usingcross-validation any other suitable estimation procedureperhaps repeatedseveral timesand choose the scheme whose estimate is smaller.this is quitesufficient in many practical applicationsifone method has a lower estimatederror than another on a particular datasetthe best we can do is to use the formermethod s model.howeverit may be that the difference is simply caused by esti-mation errorand in some circumstances it is important to determine whetherone scheme is really better than another on a particular problem.this is a stan-dard challenge for machine learning researchers.ifa new learning algorithm isproposedits proponents must show that it improves on the state ofthe art forthe problem at hand and demonstrate that the observed improvement is notjust a chance effect in the estimation process.this is a job for a statistical test that gives confidence boundsthe kind wemet previously when trying to predict true performance from a given test-seterror rate.ifthere were unlimited datawe could use a large amount for train-ing and evaluate performance on a large independent test setobtaining confi-dence bounds just as before.howeverifthe difference turns out to be significantwe must ensure that this is not just because ofthe particular dataset we eee instancestraining data mining am page happened to base the experiment on.what we want to determine is whetherone scheme is better or worse than another on averageacross all possible train-ing and test datasets that can be drawn from the domain.because the amountoftraining data naturally affects performanceall datasets should be the samesizeindeedthe experiment might be repeated with different sizes to obtain alearning curve.for the momentassume that the supply ofdata is unlimited.for definite-nesssuppose that cross-validation is being used to obtain the error estimatesother estimatorssuch as repeated cross-validationare equally viable.for eachlearning method we can draw several datasets ofthe same sizeobtain an accu-racy estimate for each dataset using cross-validationand compute the mean ofthe estimates.each cross-validation experiment yields a differentindependenterror estimate.what we are interested in is the mean accuracy across all possi-ble datasets ofthe same sizeand whether this mean is greater for one schemeor the other.from this point ofviewwe are trying to determine whether the mean ofa set ofsamples cross-validation estimates for the various datasets that wesampled from the domain is significantly greater thanor significantly lessthanthe mean ofanother.this is a job for a statistical device known as the t-testor student st-test.because the same cross-validation experiment can beused for both learning methods to obtain a matched pair ofresults for eachdataseta more sensitive version ofthe t-test known as a pairedt-testcan beused.we need some notation.there is a set ofsamples bysuccessive cross-validations using one learning schemeand a second setofsamples by successive cross-validations usingthe other.each cross-validation estimate is generated using a different datasetbut all datasets are ofthe same size and from the same domain.we will getthe best results ifexactly the same cross-validation partitions are used for bothschemes so that obtained using the same cross-validation splitasare so on.denote the mean ofthe first set ofsamples by x andthe mean ofthe second set by y are trying to determine whether x is sig-nificantly different from y are enough samplesthe mean ofa set ofindependent has a normal distributionregardless ofthe dis-tribution underlying the samples themselves.we will call the true value ofthemean m.ifwe knew the variance ofthat normal distributionso that it could bereduced to have zero mean and unit variancewe could obtain confidence limitson mgiven the mean ofthe samples variance is unknownandthe only way we can obtain it is to estimate it from the set ofsamples.that is not hard to do.the variance ofx can be estimated by dividing thevariance calculated from the samples call it by k.but what s been am page data mining that we have to estimatethe variance changes things somewhat.we canreduce the distribution ofx to have zero mean and unit variance by usingbecause the variance is only an estimatethis does nothave a normal distribu-tion it does become normal for large values ofk.insteadit has whatis called a student this meansin practice is that we have to use a table ofconfidence intervals for student sdistribution rather than the confidence table for the normal distribution givenearlier.for degrees offreedom is the correct number ifwe are usingthe average cross-validations the appropriate confidence limits are shownin table compare them with table you will see that the student sfigures are slightly more conservative for a given degree ofconfidencetheinterval is slightly wider and this reflects the additional uncertainty caused by having to estimate the variance.different tables are needed for differentnumbers ofdegrees offreedomand ifthere are more than degrees offreedom the confidence limits are very close to those for the normal distribu-tion.like table figures in table are for a one-sided confidenceinterval.to decide whether the means x and y an average ofthe same numberkofsamplesare the same or notwe consider the differences dibetween corre-sponding observationsdixi-yi.this is legitimate because the observationsare paired.the mean ofthis difference is just the difference between the twomeansd the means themselvesit has a student s distributionwith degrees offreedom.ifthe means are the samethe difference is zerothis is called the nullhypothesisifthey re significantly differentthe differencewill be significantly different from zero.so for a given confidence levelwe willcheck whether the actual difference exceeds the confidence limits for student s distribution with degrees of freedom.prx am page firstreduce the difference to a zero-meanunit-variance variable called thet-statisticwhere the variance ofthe difference samples.thendecide on a confidencelevel or is used in practice.from this the confidence limit zis determined using table ifkis is nota confidence table ofthestudent s distribution for the kvalue in question is used.a two-tailed test isappropriate because we do not know in advance whether the mean ofthe x s islikely to be greater than that ofthe y s or vice versathus for a test we usethe value corresponding to in table value oftaccording to thepreceding formula is greater than zor less than reject the null hypothe-sis that the means are the same and conclude that there really is a significant dif-ference between the two learning methods on that domain for that dataset size.two observations are worth making on this procedure.the first is technicalwhat ifthe observations were not paired? that iswhat ifwe were unableforsome reasonto assess the error ofeach learning scheme on the same datasets?what ifthe number ofdatasets for each scheme was not even the same? theseconditions could arise ifsomeone else had evaluated one ofthe methods andpublished several different estimates for a particular domain and dataset size or perhaps just their mean and variance and we wished to compare this witha different learning method.then it is necessary to use a regularnonpaired t-test.ifthe means are normally distributedas we are assumingthe differencebetween the means is also normally distributed.instead oftaking the mean ofthe differenced use the difference ofthe meansx s thesame thingthe mean ofthe difference isthe difference ofthe means.but thevariance ofthe difference d is notthe same.ifthe variance ofthe samples the variance ofthe samples best esti-mate ofthe variance ofthe difference ofthe means isit is this variance ratherits square root that should be used as the denom-inator ofthe t-statistic given previously.the degrees offreedomnecessary forconsulting student s confidence tablesshould be taken conservatively to be theminimum ofthe degrees offreedom ofthe two samples.essentiallyknowingthat the observations are paired allows the use ofa better estimate for the vari-ancewhich will produce tighter confidence bounds.the second observation concerns the assumption that there is essentiallyunlimited data so that several independent datasets ofthe right size can be what s been am page practice there is usually only a single dataset oflimited size.what can bedone? we could split the data into subsets and perform a cross-validation on each.howeverthe overall result will only tell us whether a learn-ing scheme is preferable for that particular size perhaps one-tenth oftheoriginal dataset.alternativelythe original dataset could be reused forexamplewith different randomizations ofthe dataset for each resulting cross-validation estimates will not be independentbecause they are not based on independent datasets.in practicethis means thata difference may be judged to be significant when in fact it is not.in factjustincreasing the number ofsamples kthat isthe number ofcross-validation runswill eventually yield an apparently significant difference because the value ofthet-statistic increases without bound.various modifications ofthe standard t-test have been proposed to circum-vent this problemall ofthem heuristic and lacking sound theoretical justifica-tion.one that appears to work well in practice is the correctedresampledt-test.assume for the moment that the repeated holdout method is used instead ofcross-validationrepeated ktimes on different random splits ofthe same datasetto obtain accuracy estimates for two learning methods.each used for training and testingand differences diare computed fromperformance on the test data.the corrected resampled t-test uses the modifiedstatisticin exactly the same way as the standard t-statistic.a closer look at the formulashows that its value cannot be increased simply by increasing k.the same mod-ified statistic can be used with repeated cross-validationwhich is just a specialcase ofrepeated holdout in which the individual test sets for onecross-validation do not overlap.for cross-validation repeated based on probabilitiesthroughout this section we have tacitly assumed that the goal is to maximizethe success rate ofthe predictions.the outcome for each test instance is eithercorrectifthe prediction agrees with the actual value for that instanceor incor-rectifit does not.there are no grayseverything is black or whitecorrect ortdknnd method was advocated in the first edition ofthis am page incorrect.in many situationsthis is the most appropriate perspective.ifthelearning schemewhen it is actually appliedresults in either a correct or anincorrect predictionsuccess is the right measure to use.this is sometimes calleda loss is either zero ifthe prediction is correct or oneifit is not.the use oflossis conventionalalthough a more optimistic termi-nology might couch the outcome in terms ofprofit instead.other situations are softer edged.most learning methods can associate aprobability with each prediction the na ve bayes method does.it might bemore natural to take this probability into account when judging correctness.forexamplea correct outcome predicted with a probability should perhapsweigh more heavily than one predicted with a probability a two-class situationperhaps the latter is not all that much better than an incorrectoutcome predicted with probability it is appropriate to take pre-diction probabilities into account depends on the application.ifthe ultimateapplication really is just a prediction ofthe outcomeand no prizes are awardedfor a realistic assessment ofthe likelihood ofthe predictionit does not seemappropriate to use probabilities.ifthe prediction is subject to further process-inghowever perhaps involving assessment by a personor a cost analysisormaybe even serving as input to a second-level learning process then it maywell be appropriate to take prediction probabilities into account.quadratic loss functionsuppose that for a single instance there are kpossible outcomesor classesandfor a given instance the learning scheme comes up with a probability vector the classes these probabilities sum to actualoutcome for that instance will be one ofthe possible classes.howeverit is con-venient to express it as a vector ith componentwhere iisthe actual classis and all other components are can express the penaltyassociated with this situation as a loss function that depends on both the pvectorand the avector.one criterion that is frequently used to evaluate probabilistic prediction isthe quadraticlossfunctionnote that this is for a single instancethe summation is over possible outputsnot over different instances.just one ofthe a s will be and the rest will be the sum contains contributions the incorrect predictions and the correct one.consequentlyit can be ppijjpajjj- what s been am page iis the correct class.when the test set contains several instancesthe lossfunction is summed over them all.it is an interesting theoretical fact that ifyou seek to minimize the value ofthe quadratic loss function in a situation in which the actual class is generatedprobabilisticallythe best strategy is to choose for the pvector the actual prob-abilities ofthe different outcomesthat ispiprclass true proba-bilities are knownthey will be the best values for p.ifthey are nota systemthat strives to minimize the quadratic loss function will be encouraged to useits best estimate ofprclass as the value for pi.this is quite easy to see.denote the true probabilities by thatpiprclass expected value ofthe quadratic loss function for a testinstance can be rewritten as followsthe first stage just involves bringing the expectation inside the sum and expand-ing the square.for the secondpjis just a constant and the expected value ofajis simply pjmoreoverbecause ajis either or its expected valueis pjtoo.the third stage is straightforward algebra.to minimize the resultingsumit is clear that it is best to choose pjpjso that the squared term disap-pears and all that is left is a term that is just the variance ofthe true distribu-tion governing the actual class.minimizing the squared error has a long history in prediction problems.inthe present contextthe quadratic loss function forces the predictor to be honestabout choosing its best estimate ofthe probabilities orratherit gives prefer-ence to predictors that are able to make the best guess at the true probabilities.moreoverthe quadratic loss function has some useful theoretical properties thatwe will not go into here.for all these reasons it is frequently used as the crite-rion ofsuccess in probabilistic prediction situations.informational loss functionanother popular criterion for the evaluation ofprobabilistic prediction is theinformationallossfunctionwhere the ith prediction is the correct one.this is in fact identical to the nega-tive ofthe log-likelihood function that is optimized by logistic regressiondescribed in section represents the information bits required toexpress the actual class iwith respect to the probability distribution am page pk.in other wordsifyou were given the probability distribution and someonehad to communicate to you which class was the one that actually occurredthisis the number ofbits that person would need to encode the information iftheydid it as effectively as possible.ofcourseit is always possible to use morebits.because probabilities are always less than onetheir logarithms are negativeandthe minus sign makes the outcome positive.for examplein a two-class situa-tion heads or tails with an equal probability ofeach classthe occurrence ofa head would take bit to transmitbecause is expected value ofthe informational loss functionifthe true probabili-ties are the quadratic loss functionthis expression is minimized by choosing pjpjin which case the expression becomes the entropy ofthe true distributionthus the informational loss function also rewards honesty in predictors thatknow the true probabilitiesand encourages predictors that do not to putforward their best guess.the informational loss function also has a gamblinginterpretation in whichyou imagine gambling on the outcomeplacing odds on each possible class andwinning according to the class that comes up.successive instances are like suc-cessive betsyou carry wins losses over from one to the next.the logarithmofthe total amount ofmoney you win over the whole test set is the value oftheinformational loss function.in gamblingit pays to be able to predict the oddsas accurately as possiblein that sensehonesty paystoo.one problem with the informational loss function is that ifyou assign aprobability ofzero to an event that actually occursthe function s value is minusinfinity.this corresponds to losing your shirt when gambling.prudent puntersnever bet everythingon a particular eventno matter how certain it appears.likewiseprudent predictors operating under the informational loss functiondo not assign zero probability to any outcome.this leads to a problem whenno information is available about that outcome on which to base a predictionthis is called the zero-frequencyproblemand various plausible solutions havebeen proposedsuch as the laplace estimator discussed for na ve bayes on are in the business ofevaluating predictions ofprobabilitieswhich ofthetwo loss functions should you use? that s a good questionand there is no uni-versally agreed-upon answer it s really a matter oftaste.both do the what s been am page mental job expected ofa loss functionthey give maximum reward to predic-tors that are capable ofpredicting the true probabilities accurately.howeverthere are some objective differences between the two that may help you forman opinion.the quadratic loss function takes account not only ofthe probability assignedto the event that actually occurredbut also the other probabilities.for examplein a four-class situationsuppose you assigned to the class that actuallycame up and distributed the remainder among the other three classes.thequadratic loss will depend on how you distributed it because ofthe sum ofthe occurs in the expression given earlier for the quadratic loss function.the loss will be smallest ifthe was distributed evenly among the threeclassesan uneven distribution will increase the sum ofthe squares.the infor-mational loss functionon the other handdepends solely on the probabilityassigned to the class that actually occurred.ifyou re gambling on a particularevent coming upand it doeswho cares how you distributed the remainder ofyour money among the other events?ifyou assign a very small probability to the class that actually occurstheinformation loss function will penalize you massively.the maximum penaltyfor a zero probabilityis infinite.the gambling world penalizes mistakes like thisharshlytoo! the quadratic loss functionon the other handis milderbeingbounded bywhich can never exceed ofthe informational loss function point to a generaltheory ofperformance assessment in learning called the minimumdescriptionlengthmdlprinciple.they argue that the size ofthe structures that a schemelearns can be measured in bits ofinformationand ifthe same units are usedto measure the lossthe two can be combined in useful and powerful ways.wereturn to this in section the costthe evaluations that have been discussed so far do not take into account thecost ofmaking wrong decisionswrong classifications.optimizing classificationrate without considering the cost ofthe errors often leads to strange results.inone casemachine learning was being used to determine the exact day that eachcow in a dairy herd was in estrusor in heat. cows were identified by elec-tronic ear tagsand various attributes were used such as milk volume and chem-ical composition automatically by a high-tech milking machineandmilking order for cows are regular beasts and generally arrive in the the am page shed in the same orderexcept in unusual circumstances such as estrus.in amodern dairy operation it s important to know when a cow is readyanimalsare fertilized by artificial insemination and missing a cycle will delay calvingunnecessarilycausing complications down the line.in early experimentsmachine learning methods stubbornly predicted that each cow was neverinestrus.like humanscows have a menstrual cycle ofapproximately dayssothis null rule is correct about ofthe time an impressive degree ofaccu-racy in any agricultural domain! what was wantedofcoursewere rules thatpredicted the in estrus situation more accurately than the not in estrus onethe costs ofthe two kinds oferror were different.evaluation by classificationaccuracy tacitly assumes equal error costs.other examples in which errors cost different amounts include loan deci-sionsthe cost oflending to a defaulter is far greater than the lost-business costofrefusing a loan to a nondefaulter.and oil-slick detectionthe cost offailingto detect an environment-threatening real slick is far greater than the cost ofafalse alarm.and load forecastingthe cost ofgearing up electricity generatorsfor a storm that doesn t hit is far less than the cost ofbeing caught completelyunprepared.and diagnosisthe cost ofmisidentifying problems with a machinethat turns out to be free offaults is less than the cost ofoverlooking problemswith one that is about to fail.and promotional mailingthe cost ofsending junkmail to a household that doesn t respond is far less than the lost-business costofnot sending it to a household that would have responded.why these areall the examples ofchapter in truthyou d be hard pressed to find an appli-cation in which the costs ofdifferent kinds oferror were the same.in the two-class case with classes yesand nolend or not lendmark a suspi-cious patch as an oil slick or notand so ona single prediction has the four dif-ferent possible outcomes shown in table truepositivestp and truenegativestn are correct classifications.a falsepositivefp occurs when theoutcome is incorrectly predicted as yesor positive when it is actually noneg-ative.a falsenegativefn occurs when the outcome is incorrectly predictedas negative when it is actually positive.the truepositiverateis tp divided what s been learnedtable outcomes of a two-class prediction.predicted classyesnoactualyestrue false positivenegativeclassnofalse true am page by the total number ofpositiveswhich is tp falsepositiverateis fp divided by the total number ofnegativesfp overall success rate is the number ofcorrect classifications divided by the total number ofclassificationsfinallythe error rate is one minus this.in a multiclass predictionthe result on a test set is often displayed as a two-dimensional confusion matrixwith a row and column for each class.each matrixelement shows the number oftest examples for which the actual class is the rowand the predicted class is the column.good results correspond to large numbersdown the main diagonal and smallideally zerooff-diagonal shows a numeric example with three classes.in this case the test set instances sum ofthe nine numbers in the matrixand ofthem are predicted correctlyso the success rate is is this a fair measure ofoverall success? how many agreements wouldyou expect by chance?this predictor predicts a total a b sand swhat ifyou had a random predictor that predicted the same total numbersofthe three classes? the answer is shown in table first row dividesthe a s in the test set into these overall proportionsand the second andthird rows do the same thing for the other two classes.ofcoursethe row andcolumn totals for this matrix are the same as before the number ofinstanceshasn t changedand we have ensured that the random predictor predicts thesame number ofa sb sand c s as the actual predictor.this random predictor gets instances correct.a measurecalled the kappa statistictakes this expected figure into account by deducting it from the predictor s successes and expressing the result as a proportion ofthe total for a perfect predictorto yield extra successes out the outcomes of a three-class prediction actual and expected.predicted classpredicted am page ofa possible total maximum value ofkappais the expected value for a random predictor with the same columntotals is zero.in summarythe kappa statistic is used to measure the agreementbetween predicted and observed categorizations ofa datasetwhile correctingfor agreement that occurs by chance.howeverlike the plain success rateit doesnot take costs into account.cost-sensitive classificationifthe costs are knownthey can be incorporated into a financial analysis ofthedecision-making process.in the two-class casein which the confusion matrixis like that oftable two kinds oferror false positives and false nega-tives will have different costslikewisethe two types ofcorrect classificationmay have different benefits.in the two-class casecosts can be summarized inthe form ofa matrix in which the diagonal elements represent the twotypes ofcorrect classification and the off-diagonal elements represent the twotypes oferror.in the multiclass case this generalizes to a square matrix whosesize is the number ofclassesand again the diagonal elements represent the costofcorrect classification.table and shows default cost matrixes for thetwo- and three-class cases whose values simply give the number oferrorsmis-classification costs are all the cost matrix into account replaces the success rate by the averagecost more positivelyprofit per decision.although we will not doso herea complete financial analysis ofthe decision-making process might alsotake into account the cost ofusing the machine learning tool including thecost ofgathering the training data and the cost ofusing the modelor deci-sion structurethat it produces that isthe cost ofdetermining the attributesfor the test instances.ifall costs are knownand the projected number what s been learnedtable cost matrixes a two-class case and a three-class case.predicted predicted am page different outcomes in the cost matrix can be estimated sayusing cross-validation it is straightforward to perform this kind offinancial analysis.given a cost matrixyou can calculate the cost ofa particular learned modelon a given test set just by summing the relevant elements ofthe cost matrix forthe model s prediction for each test instance.herethe costs are ignored whenmaking predictionsbut taken into account when evaluating them.ifthe model outputs the probability associated with each predictionit canbe adjusted to minimize the expected cost ofthe predictions.given a set ofpre-dicted probabilities for each outcome on a certain test instanceone normallyselects the most likely outcome.insteadthe model could predict the class withthe smallest expected misclassification cost.for examplesuppose in a three-class situation the model assigns the classes aband cto a test instance withprobabilities papband pcand the cost matrix is that in table pre-dicts athe expected cost ofthe prediction is obtained by multiplying the firstcolumn ofthe the probability vectorpapbpcyielding pbpcor the three probabilities sum to costs forpredicting the other two classes are this cost matrixchoos-ing the prediction with the lowest expected cost is the same as choosing the onewith the greatest probability.for a different cost matrix it might be different.we have assumed that the learning method outputs probabilitiesas na vebayes does.even ifthey do not normally output probabilitiesmost classifierscan easily be adapted to compute them.in a decision treefor examplethe prob-ability distribution for a test instance is just the distribution ofclasses at thecorresponding leaf.cost-sensitive learningwe have seen how a classifierbuilt without taking costs into considerationcanbe used to make predictions that are sensitive to the cost matrix.in this casecosts are ignored at training time but used at prediction time.an alternative isto do just the oppositetake the cost matrix into account during the trainingprocess and ignore costs at prediction time.in principlebetter performancemight be obtained ifthe classifier were tailored by the learning algorithm to thecost matrix.in the two-class situationthere is a simple and general way to make anylearning method cost sensitive.the idea is to generate training data with a dif-ferent proportion ofyesand noinstances.suppose that you artificially increasethe number ofnoinstances by a factor and use the resulting dataset fortraining.ifthe learning scheme is striving to minimize the number oferrorsitwill come up with a decision structure that is biased toward avoiding errors onthe noinstancesbecause such errors are effectively penalized the am page with the original proportion ofnoinstances is used for testingfewer errors willbe made on these than on yesinstances that isthere will be fewer false posi-tives than false negatives because false positives have been weighted timesmore heavily than false negatives.varying the proportion ofinstances in thetraining set is a general technique for building cost-sensitive classifiers.one way to vary the proportion oftraining instances is to duplicate instancesin the dataset.howevermany learning schemes allow instances to be weighted.as we mentioned in section is a common technique for handlingmissing values. instance weights are normally initialized to one.to build cost-sensitive trees the weights can be initialized to the relative cost ofthe two kindsoferrorfalse positives and false negatives.lift chartsin practicecosts are rarely known with any degree ofaccuracyand people willwant to ponder various scenarios.imagine you re in the direct mailing businessand are contemplating a mass mailout ofa promotional offer to most ofwhom won t respondofcourse.let us say thatbased onprevious experiencethe proportion who normally respond is known to be respondents.suppose a data mining tool is available thatbased onknown information about the householdsidentifies a subset forwhich the response rate is respondents.it may well pay offto restrictthe mailout to these households that depends on the mailing costcompared with the return gained for each response to the offer.in marketingterminologythe increase in response ratea factor offour in this caseis knownas the liftfactor yielded by the learning tool.ifyou knew the costsyou coulddetermine the payoffimplied by a particular lift factor.but you probably want to evaluate other possibilitiestoo.the same datamining schemewith different parameter settingsmay be able to households for which the response rate will be respondentscorresponding to a lift factor oftwo.againwhether this would be a more prof-itable target for the mailout can be calculated from the costs involved.it maybe necessary to factor in the cost ofcreating and using the model includingcollecting the information that is required to come up with the attribute values.after allifdeveloping the model is very expensivea mass mailing may be morecost effective than a targeted one.given a learning method that outputs probabilities for the predicted class ofeach member ofthe set oftest instances na ve bayes doesyour job is tofind subsets oftest instances that have a high proportion ofpositive instanceshigher than in the test set as a whole.to do thisthe instances should be sortedin descending order ofpredicted probability ofyes.thento find a sample ofagiven size with the greatest possible proportion ofpositive instancesjust what s been am page the requisite number ofinstances offthe liststarting at the top.ifeach testinstance s class is knownyou can calculate the lift factor by simply counting thenumber ofpositive instances that the sample includesdividing by the samplesize to obtain a success proportion and dividing by the success proportion forthe complete test set to determine the lift factor.table shows an example for a small dataset with are yesresponses an overall success proportion instances havebeen sorted in descending probability order according to the predicted proba-bility ofa yesresponse.the first instance is the one that the learning schemethinks is most likely to be positivethe second is the next most likelyand so on.the numeric values ofthe probabilities are unimportantrank is the only thingthat matters.with each rank is given the actual class ofthe instance.thus thelearning method was right about items and they are indeed positives butwrong about item turned out to be a negative.nowifyou were seekingthe most promising sample ofsize but only knew the predicted probabilitiesand not the actual classesyour best bet would be the top ten ranking instances.eight ofthese are positiveso the success proportion for this sample is to a lift factor offour.ifyou knew the different costs involvedyou could work them out for eachsample size and choose the most profitable.but a graphical depiction ofthevarious possibilities will often be far more revealing than presenting a single optimal decision.repeating the preceding operation for different-sizedsamples allows you to plot a lift chart like that offigure horizontal axisshows the sample size as a proportion ofthe total possible mailout.the verti-cal axis shows the number ofresponses obtained.the lower left and upper rightpoints correspond to no mailout at allwith a response a full mailoutwith a response diagonal line gives the expected result for the for a lift chart.rankpredicted actual classrankpredicted actual am page sized random samples.but we do not choose random sampleswe choose thoseinstances whichaccording to the data mining toolare most likely to generatea positive response.these correspond to the upper linewhich is derived bysumming the actual responses over the corresponding percentage ofthe instancelist sorted in probability order.the two particular scenarios described previ-ously are markeda mailout that yields respondents and a onethat yields you d like to be in a lift chart is near the upper left-hand corneratthe very responses from a mailout ofjust you send onlyto those households that will respond and are rewarded with a successrate.any selection procedure worthy ofthe name will keep you above the diag-onal otherwiseyou d be seeing a response that was worse than for randomsampling.so the operating part ofthe diagram is the upper triangleand thefarther to the northwest the better.roc curveslift charts are a valuable toolwidely used in marketing.they are closely relatedto a graphical technique for evaluating data mining schemes known as roccurveswhich are used in just the same situation as the preceding onein whichthe learner is trying to select samples oftest instances that have a high propor-tion ofpositives.the acronym stands for receiver operating characteristica termused in signal detection to characterize the tradeoffbetween hit rate and falsealarm rate over a noisy channel.roc curves depict the performance ofa clas-sifier without regard to class distribution or error costs.they plot the what s been sizenumber of respondentsfigure hypothetical lift am page ofpositives included in the sample on the vertical axisexpressed as a percent-age ofthe total number ofpositivesagainst the number ofnegatives includedin the sampleexpressed as a percentage ofthe total number ofnegativesonthe horizontal axis.the vertical axis is the same as that ofthe lift chart exceptthat it is expressed as a percentage.the horizontal axis is slightly different number ofnegatives rather than sample size.howeverin direct marketing sit-uations in which the proportion ofpositives is very small anyway is negligible difference between the size ofa sample and the number ofnegatives it containsso the roc curve and lift chart look very similar.as withlift chartsthe northwest corner is the place to be.figure shows an example roc curve the jagged line for the sample oftest data in table can follow it along with the table.from the origingo up two positivesalong one negativeup five positivesalong one negativeup onealong oneup twoand so on.each point cor-responds to drawing a line at a certain position on the ranked listcounting theyes s and no s above itand plotting them vertically and horizontallyrespectively.as you go farther down the listcorresponding to a larger samplethe numberofpositives and negatives both increase.the jagged roc line in figure depends intimately on the details oftheparticular sample oftest data.this sample dependence can be reduced by apply-ing cross-validation.for each different number ofno s that iseach positionalong the horizontal axis take just enough ofthe highest-ranked instances toinclude that number ofno sand count the number ofyes s they contain.finallyaverage that number over different folds ofthe cross-validation.the result is the sample roc am page smooth curve like that in figure although in reality such curves do notgenerally look quite so smooth.this is just one way ofusing cross-validation to generate roc curves.asimpler approach is to collect the predicted probabilities for all the various testsets there are in a cross-validationalong with the trueclass labels ofthe corresponding instancesand generate a single ranked listbased on this data.this assumes that the probability estimates from the classi-fiers built from the different training sets are all based on equally sized randomsamples ofthe data.it is not clear which method is preferable.howeverthelatter method is easier to implement.ifthe learning scheme does not allow the instances to be orderedyou canfirst make it cost sensitive as described earlier.for each fold ofa cross-validationweight the instances for a selection ofdifferent cost ratiostrain thescheme on each weighted setcount the true positives and false positives in thetest setand plot the resulting point on the roc axes.it doesn t matter whetherthe test set is weighted or not because the axes in the roc diagram are expressedas the percentage oftrue and false positives. howeverfor inherently cost-sensitive probabilistic classifiers such as na ve bayes it is far more costly thanthe method described previously because it involves a separate learning problemfor every point on the curve.it is instructive to look at cross-validated roc curves obtained using differ-ent learning methods.for examplein figure a excels ifa smallfocused sample is soughtthat isifyou are working toward the left-hand sideofthe graph.clearlyifyou aim to cover just ofthe true positives what s been positivestrue positivesabfigure curves for two learning am page should choose method awhich gives a false positive rate ofaround method bwhich gives more than false positives.but method b excelsifyou are planning a large sampleifyou are covering ofthe true positivesmethod b will give a false positive rate as compared with method a shaded area is called the convex hullofthe two curvesand you shouldalways operate at a point that lies on the upper boundary ofthe convex hull.what about the region in the middle where neither method a nor methodb lies on the convex hull? it is a remarkable fact that you can get anywhere inthe shaded region by combining methods a and b and using them at randomwith appropriate probabilities.to see thischoose a particular probability cutofffor method a that gives true and false positive rates oftaand farespectivelyand another cutofffor method b that gives tband fb.ifyou use these twoschemes at random with probability pand qwhere p you will gettrue and false positive rates ofp.taq.tband p.faq.fb.this represents a pointlying on the straight line joining the points and by varying pand qyou can trace out the entire line between these two points.using thisdevicethe entire shaded region can be reached.only ifa particular scheme gen-erates a point that lies on the convex hull should it be used aloneotherwiseitwould always be better to use a combination ofclassifiers corresponding to apoint that lies on the convex hull.recall precision curvespeople have grappled with the fundamental tradeoffillustrated by lift charts androc curves in a wide variety ofdomains.information retrieval is a goodexample.given a querya web search engine produces a list ofhits that repre-sent documents supposedly relevant to the query.compare one system thatlocates ofwhich are relevantwith another that locates ofwhich are relevant.which is better? the answer should nowbe obviousit depends on the relative cost offalse positivesdocuments that arereturned that aren t relevantand false negativesdocuments that are relevantthat aren t returned.information retrieval researchers define parameters calledrecalland precisionfor exampleifthe list ofyes s and no s in table represented a ranked list ofretrieved documents and whether they were relevant or notand the entire collection contained a total relevant documentsthen recall at wouldprecisionnumber of documents retrieved that are relevanttotal number of documents that are retrieved.recallnumber of documents retrieved that are relevanttotal number of documents that are the am page refer to recall for the top ten documentsthat precision would be retrieval experts use recall precisioncurvesthat plot one against the otherfor different numbers ofretrieved docu-mentsin just the same way as roc curves and lift charts except that becausethe axes are differentthe curves are hyperbolic in shape and the desired oper-ating point is toward the upper right.discussiontable summarizes the three different ways we have met ofevaluating thesame basic tradeofftpfptnand fn are the number oftrue positivesfalsepositivestrue negativesand false negativesrespectively.you want to choose aset ofinstances with a high proportion ofyesinstances and a high coverage ofthe yesinstancesyou can increase the proportion by using asmaller coverageor increase the coverage at the expense ofthe pro-portion.different techniques give different tradeoffsand can be plotted as dif-ferent lines on any ofthese graphical charts.people also seek single measures that characterize performance.two that areused in information retrieval are average recallwhich gives the averageprecision obtained at recall values averagerecallwhich gives the average precision obtained at recall values used in informa-tion retrieval is the f-measurewhich what s been learnedtable measures used to evaluate the false positive versus the false negative tradeoff.domainplotaxesexplanation of axeslift chartmarketingtp vs.tpnumber of true positivessubset sizesubset sizeroc curvecommunicationstp rate vs.tp ratefp ratefp raterecall precisioninformationrecall vs.recallsame as tp rate tpcurveretrievalprecisionprecisiontptpfp am page different terms are used in different domains.medicsfor exampletalk aboutthe sensitivityand specificityofdiagnostic tests.sensitivity refers to the propor-tion ofpeople with disease who have a positive test resultthat istp.specificityrefers to the proportion ofpeople without disease who have a negative testresultwhich is the product ofthese is used as an overallmeasurefinallyofcoursethere is our old friend the success rateto summarize roc curves in a single quantitypeople sometimes use the areaunder the curve becauseroughly speaking the larger the area the betterthe model.the area also has a nice interpretation as the probability that theclassifier ranks a randomly chosen positive instance above a randomly chosennegative one.although such measures may be useful ifcosts and class distri-butions are unknown and one method must be chosen to handle all situationsno single number is able to capture the tradeoff.that can only be done by two-dimensional depictions such as lift chartsroc curvesand recall preci-sion diagrams.cost curvesroc curves and their relatives are very useful for exploring the tradeoffs amongdifferent classifiers over a range ofcosts.howeverthey are not ideal for evalu-ating machine learning models in situations with known error costs.forexampleit is not easy to read offthe expected cost ofa classifier for a fixed costmatrix and class distribution.neither can you easily determine the ranges ofapplicability ofdifferent classifiers.for examplefrom the crossover pointbetween the two roc curves in figure it is hard to tell for what cost andclass distributions classifier a outperforms classifier b.cost curvesare a different kind ofdisplay on which a single classifier corre-sponds to a straight line that shows how the performance varies as the class dis-tribution changes.againthey work best in the two-class casealthough you canalways make a multiclass problem into a two-class one by singling out one classand evaluating it against the remaining ones.figure plots the expected error against the probability ofone oftheclasses.you could imagine adjusting this probability by resampling the test setin a nonuniform way.we denote the two classes using diagonalsshow the performance oftwo extreme classifiersone always predicts the am page an expected error ofone ifthe dataset contains no and zero ifall itsinstances are other always predicts the opposite performance.the dashed horizontal line shows the performance ofthe classifier that is alwayswrongand the x-axis itselfrepresents the classifier that is always correct.inpracticeofcourseneither ofthese is realizable.good classifiers have low error ratesso where you want to be is as close to the bottom ofthe diagram aspossible.the line marked a represents the error rate ofa particular classifier.ifyoucalculate its performance on a certain test setits false positive rate fpis itsexpected error on a subsample ofthe test set that contains only negative exam-ples its false negative rate fnis the error on a subsample thatcontains only positive examples are the values ofthe inter-cepts at the left and rightrespectively.you can see immediately from the plotthat ifp is smaller than about a is outperformed by the extremeclassifier that always predicts ifit is larger than about otherextreme classifier is what s been learnedprobability p pick always pick wrongalways ofvarying the probability thresholda the error curve and thecost am page so far we have not taken costs into accountor rather we have used the defaultcost matrix in which all errors cost the same.cost curveswhich do take costinto accountlook very similar very similar indeed but the axes are differ-ent.figure shows a cost curve for the same classifier a that the ver-tical scale has been enlargedfor convenienceand ignore the gray lines for now.it plots the expected cost ofusing a against the probability cost functionwhichis a distorted version ofp that retains the same extremeszero when p and one when p by c- the cost ofpredicting theinstance is actually the reverse by c-.then the axes offigure are assuming here that correct predictions have no costc is not the case the formulas are a little more complex.the maximum value that the normalized expected cost can have is thatis why it is normalized. one nice thing about cost curves is that the extremenormalized expected costprobability cost function the cost function pc am page cost values at the left and right sides ofthe graph are fpand fnjust as they arefor the error curveso you can draw the cost curve for any classifier very easily.figure also shows classifier bwhose expected cost remains the sameacross the range that isits false positive and false negative rates are equal.asyou can seeit outperforms classifier a ifthe probability cost function exceedsabout knowing the costs we could easily work out what this corre-sponds to in terms ofclass distribution.in situations that involve different classdistributionscost curves make it easy to tell when one classifier will outper-form another.in what circumstances might this be useful? to return to the example ofpre-dicting when cows will be in estrustheir cycleor prior probabil-ityis unlikely to vary greatly a genetic cataclysm!.but a particularherd may have different proportions ofcows that are likely to reach estrus inany given weekperhaps synchronized with who knows? the phase ofthemoon.thendifferent classifiers would be appropriate at different times.in theoil spill exampledifferent batches ofdata may have different spill probabilities.in these situations cost curves can help to show which classifier to use when.each point on a lift chartroc curveor recall precision curve represents aclassifiertypically obtained using different threshold values for a method suchas na ve bayes.cost curves represent each classifier using a straight lineand asuite ofclassifiers will sweep out a curved envelope whose lower limit showshow well that type ofclassifier can do ifthe parameter is well indicates this with a few gray lines.ifthe process were continuedit wouldsweep out the dotted parabolic curve.the operating region ofclassifier b ranges from a probability cost value ofabout to a value ofabout this regionclassifier b is outper-formed by the trivial classifiers represented by dashed lines.suppose we decideto use classifier b within this range and the appropriate trivial classifier belowand above it.all points on the parabola are certainly better than this scheme.but how much better? it is hard to answer such questions from an roc curvebut the cost curve makes them easy.the performance difference is negligible ifthe probability cost value is around below a value ofabout andabove it is barely perceptible.the greatest difference occurs at probabilitycost values and and is about ofthe maximum possiblecost numeric predictionall the evaluation measures we have described pertain to classification situa-tions rather than numeric prediction situations.the basic principles using anindependent test set rather than the training set for performance what s been am page holdout methodand cross-validation apply equally well to numeric predic-tion.but the basic quality measure offered by the error rate is no longer appro-priateerrors are not simply present or absentthey come in different sizes.several alternative measuressummarized in table be used to evalu-ate the success ofnumeric prediction.the predicted values on the test instancesare actual values are that pimeans some-thing very different here from what it did in the last sectionthere it was theprobability that a particular prediction was in the ith classhere it refers to thenumeric value ofthe prediction for the ith test instance.mean-squared erroris the principal and most commonly used measuresometimes the square root is taken to give it the same dimensions as the pre-dicted value itself.many mathematical techniques as linear regressionexplained in chapter use the mean-squared error because it tends to be theeasiest measure to manipulate mathematicallyit isas mathematicians say wellbehaved. howeverhere we are considering it as a performance measureall theperformance measures are easy to calculateso mean-squared error has no par-ticular advantage.the question isis it an appropriate measure for the task athand?mean absolute erroris an alternativejust average the magnitude ofthe indi-vidual errors without taking account oftheir sign.mean-squared error tends toexaggerate the effect ofoutliers instances whose prediction error is larger thanthe others but absolute error does not have this effectall sizes oferror aretreated evenly according to their magnitude.sometimes it is the relativerather than absoluteerror values that are ofimpor-tance.for exampleifa error is equally important whether it is an error in a prediction or an error in a prediction averagesofabsolute error will be meaninglessrelative errors are appropriate.this effectwould be taken into account by using the relative errors in the mean-squarederror calculation or the mean absolute error calculation.relative squared errorin table refers to something quite different.theerror is made relative to what it would have been ifa simple predictor had beenused.the simple predictor in question is just the average ofthe actual values from the training data.thus relative squared error takes the total squarederror and normalizes it by dividing by the total squared error ofthe default predictor.the next error measure goes by the glorious name ofrelative absolute errorand is just the total absolute errorwith the same kind ofnormalization.in thesethree relative error measuresthe errors are normalized by the error ofthesimple predictor that predicts average values.the final measure in table is the correlation coefficientwhich measuresthe statistical correlation between the a s and the p s.the correlation coefficientranges from for perfectly correlated resultsthrough when there is no numeric am page relationto when the results are perfectly correlated negatively.ofcoursenegative values should not occur for reasonable prediction methods.correla-tion is slightly different from the other measures because it is scale independentin thatifyou take a particular set ofpredictionsthe error is unchanged ifallthe predictions are multiplied by a constant factor and the actual values are leftunchanged.this factor appears in every term ofspain the numerator and inevery term ofspin the denominatorthus canceling out.this is not true forthe relative error figuresdespite normalizationifyou multiply all the predic-tions by a large constantthen the difference between the predicted and theactual values will change dramaticallyas will the percentage errors. it is alsodifferent in that good performance leads to a large value ofthe correlation coef-ficientwhereas because the other methods measure errorgood performance isindicated by small values.which ofthese measures is appropriate in any given situation is a matter thatcan only be determined by studying the application itself.what are we tryingto minimize? what is the cost ofdifferent kinds oferror? often it is not easy todecide.the squared error measures and root squared error measures weigh what s been learnedtable measures for numeric prediction.performance measureformulamean-squared errorroot mean-squared errormean absolute errorrelative squared errorroot relative squared errorrelative absolute errorcorrelation coefficientpare predicted values and aare actual values.sppnsaanpiiaii---- and ssssppaanpapapaiii where where am page discrepancies much more heavily than small oneswhereas the absolute errormeasures do not.taking the square root mean-squared error just reducesthe figure to have the same dimensionality as the quantity being predicted.therelative error figures try to compensate for the basic predictability or unpre-dictability ofthe output variableifit tends to lie fairly close to its average valuethen you expect prediction to be good and the relative figure compensate forthis.otherwiseifthe error figure in one situation is far greater than that inanother situationit may be because the quantity in the first situation is inher-ently more variable and therefore harder to predictnot because the predictoris any worse.fortunatelyit turns out that in most practical situations the best numericprediction method is still the best no matter which error measure is used.forexampletable shows the result offour different numeric prediction tech-niques on a given datasetmeasured using cross-validation.method d is the bestaccording to all five metricsit has the smallest value for each error measure andthe largest correlation coefficient.method c is the second best by all five metrics.the performance ofmethods a and b is open to disputethey have the samecorrelation coefficientmethod a is better than method b according to bothmean-squared and relative squared errorsand the reverse is true for bothabsolute and relative absolute error.it is likely that the extra emphasis that thesquaring operation gives to outliers accounts for the differences in this case.when comparing two different learning schemes that involve numeric pre-dictionthe methodology developed in section still applies.the only dif-ference is that success rate is replaced by the appropriate performance measuree.g.root mean-squared error when performing the significance minimum description length principlewhat is learned by a machine learning method is a kind of theory ofthedomain from which the examples are drawna theory that is predictive in minimum description length measures for four numeric prediction models.abcdroot mean-squared absolute relative squared absolute am page it is capable ofgenerating new facts about the domain in other wordsthe classofunseen instances.theory is a rather grandiose termwe are using it here onlyin the sense ofa predictive model.thus theories might comprise decision treesor sets ofrules they don t have to be any more theoretical than that.there is a long-standing tradition in science thatother things being equalsimple theories are preferable to complex ones.this is known as occam s razorafter the medieval philosopher william ofoccam ockham.occam s razorshaves philosophical hairs offa theory.the idea is that the best scientific theoryis the smallest one that explains all the facts.as albert einstein is reputed tohave said everything should be made as simple as possiblebut no simpler. ofcoursequite a lot is hidden in the phrase other things being equal and itcan be hard to assess objectively whether a particular theory really does explain all the facts on which it is based that s what controversy in science is all about.in our casein machine learningmost theories make errors.ifwhat is learnedis a theorythen the errors it makes are like exceptionsto the theory.one wayto ensure that other things areequal is to insist that the information embodiedin the exceptions is included as part ofthe theory when its simplicity is judged.imagine an imperfect theory for which there are a few exceptions.not all thedata is explained by the theorybut most is.what we do is simply adjoin theexceptions to the theoryspecifying them explicitly as exceptions.this newtheory is largerthat is a price thatquite justifiablyhas to be paid for its inabil-ity to explain all the data.howeverit may be that the simplicity is it too muchto call it elegance? ofthe original theory is sufficient to outweigh the fact thatit does not quite explain everything compared with a largebaroque theory thatis more comprehensive and accurate.for exampleifkepler s three laws ofplanetary motion did not at the timeaccount for the known data quite so well as copernicus s latest refinement ofthe ptolemaic theory ofepicyclesthey had the advantage ofbeing far lesscomplexand that would have justified any slight apparent inaccuracy.keplerwas well aware ofthe benefits ofhaving a theory that was compactdespite thefact that his theory violated his own aesthetic sense because it depended on ovals rather than pure circular motion.he expressed this in a forcefulmetaphor i have cleared the augean stables ofastronomy ofcycles and spiralsand left behind me only a single cartload ofdung. the minimum description lengthor mdl principle takes the stance that thebest theory for a body ofdata is one that minimizes the size ofthe theory plusthe amount ofinformation necessary to specify the exceptions relative to thetheory the smallest cartload ofdung.in statistical estimation theorythis hasbeen applied successfully to various parameter-fitting problems.it applies tomachine learning as followsgiven a set ofinstancesa learning method infersa theory be it ever so simpleunworthyperhapsto be called a theory fromthem.using a metaphor ofcommunicationimagine that the instances are what s been am page be transmitted through a noiseless channel.any similarity that is detectedamong them can be exploited to give a more compact coding.according to themdl principlethe best generalization is the one that minimizes the number ofbits required to communicate the generalizationalong with the examples fromwhich it was made.now the connection with the informational loss function introduced insection should be starting to emerge.that function measures the error interms ofthe number ofbits required to transmit the instancesgiven the prob-abilistic predictions made by the theory.according to the mdl principle weneed to add to this the size ofthe theory in bitssuitably encodedto obtainan overall figure for complexity.howeverthe mdl principle refers to theinformation required to transmit the examples from which the theory wasformedthat isthe traininginstances not a test set.the overfitting problemis avoided because a complex theory that overfits will be penalized relative to asimple one by virtue ofthe fact that it takes more bits to encode.at one extremeis a very complexhighly overfitted theory that makes no errors on the trainingset.at the other is a very simple theory the null theory which does not helpat all when transmitting the training set.and in between are theories ofinter-mediate complexitywhich make probabilistic predictions that are imperfectand need to be corrected by transmitting some information about the trainingset.the mdl principle provides a means ofcomparing all these possibilities onan equal footing to see which is the best.we have found the holy grailan eval-uation scheme that works on the training set alone and does not need a sepa-rate test set.but the devil is in the detailsas we will see.suppose a learning method comes up with a theory tbased on a trainingset eofexamplesthat requires a certain number ofbits lt to encode the theorythe training set itselfcan be encoded in a certainnumber ofbitslet.let is in fact given by the informational loss func-tion summed over all members ofthe training set.then the total descriptionlength oftheory plus training set isand the mdl principle recommends choosing the theory tthat minimizes thissum.there is a remarkable connection between the mdl principle and basic prob-ability theory.given a training set ewe seek the most likely theory tthat isthe theory for which the a posteriori probability prte the probability afterthe examples have been seen is maximized.bayes s rule ofconditional prob-abilitythe same rule that we encountered in section minimum description length am page taking negative logarithmsmaximizing the probability is the same as minimizing its negative logarithm.now we saw in section the number ofbits required to code somethingis just the negative logarithm ofits probability.furthermorethe final termlogpredepends solely on the training set and not on the learning method.thus choosing the theory that maximizes the probability prte is tantamountto choosing the theory that minimizes in other wordsthe mdl principle!this astonishing correspondence with the notion ofmaximizing the a posteriori probability ofa theory after the training set has been taken intoaccount gives credence to the mdl principle.but it also points out where the problems will sprout when the mdl principle is applied in practice.thedifficulty with applying bayes s rule directly is in finding a suitable prior prob-ability distribution prt for the theory.in the mdl formulationthat trans-lates into finding how to code the theory tinto bits in the most efficient way.there are many ways ofcoding thingsand they all depend on presuppositionsthat must be shared by encoder and decoder.ifyou know in advance that thetheory is going to take a certain formyou can use that information to encodeit more efficiently.how are you going to actually encode t?the devil is in thedetails.encoding ewith respect to tto obtain let seems a little more straight-forwardwe have already met the informational loss function.but actuallywhen you encode one member ofthe training set after anotheryou are encod-ing a sequencerather than a set.it is not necessary to transmit the training setin any particular orderand it ought to be possible to use that fact to reduce thenumber ofbits required.oftenthis is simply approximated by subtracting logn! nis the number ofelements in ewhich is the number ofbitsneeded to specify a particular permutation ofthe training set because thisis the same for all theoriesit doesn t actually affect the comparison betweenthem.but one can imagine using the frequency ofthe individual errors toreduce the number ofbits needed to code them.ofcoursethe more sophisti-cated the method that is used to code the errorsthe less the need for a theoryin the first place so whether a theory is justified or not depends to some extenton how the errors are coded.the detailsthe details.we will not go into the details ofdifferent coding methods here.the wholequestion ofusing the mdl principle to evaluate a learning scheme based solelyon the training data is an area ofactive research and vocal disagreement what s been am page we end this section as we beganon a philosophical note.it is important toappreciate that occam s razorthe preference ofsimple theories over complexoneshas the status ofa philosophical position or axiom rather than some-thing that can be proved from first principles.although it may seem self-evidentto usthis is a function ofour education and the times we live in.a preferencefor simplicity is or may be culture specific rather than absolute.the greek philosopher epicurus enjoyed good food and wine and supposedly advocated sensual pleasure in moderation as the highest goodexpressed almost the opposite sentiment.his principle ofmultiple explanationsadvises ifmore than one theory is consistent with the datakeep them all onthe basis that ifseveral explanations are equally in agreementit may be possi-ble to achieve a higher degree ofprecision by using them together and anywayit would be unscientific to discard some arbitrarily.this brings to mindinstance-based learningin which all the evidence is retained to provide robustpredictionsand resonates strongly with decision combination methods such asbagging and boosting in chapter that actually do gain predictivepower by using multiple explanations the mdl principle to clusteringone ofthe nice things about the mdl principle is that unlike other evaluationcriteriait can be applied under widely different circumstances.although insome sense equivalent to bayes s rule in thatas we saw previouslydevising acoding scheme for theories is tantamount to assigning them a prior probabilitydistributionschemes for coding are somehow far more tangible and easier tothink about in concrete terms than intuitive prior probabilities.to illustrate thiswe will briefly describe without entering into coding details how you mightgo about applying the mdl principle to clustering.clustering seems intrinsically difficult to evaluate.whereas classification orassociation learning has an objective criterion ofsuccess predictions made ontest cases are either right or wrong this is not so with clustering.it seems thatthe only realistic evaluation is whether the result oflearning the clustering proves useful in the application context.it is worth pointing out that really thisis the case for all types oflearningnot just clustering.despite thisclustering can be evaluated from a description length perspec-tive.suppose a cluster-learning technique divides the training set einto kclus-ters.ifthese clusters are natural onesit should be possible to use them to encodeemore efficiently.the best clustering will support the most efficient encoding.one way ofencoding the instances in ewith respect to a given clustering isto start by encoding the cluster centers the average value ofeach attribute overall instances in the cluster.thenfor each instance in etransmit which the mdl principle to am page it belongs to followed by its attribute values with respect to thecluster center perhaps as the numeric difference ofeach attribute value fromthe center.couched as it is in terms ofaverages and differencesthis descrip-tion presupposes numeric attributes and raises thorny questions about how to code numbers efficiently.nominal attributes can be handled in a similarmannerfor each cluster there is a probability distribution for the attributevaluesand the distributions are different for different clusters.the coding issuebecomes more straightforwardattribute values are coded with respect to therelevant probability distributiona standard operation in data compression.ifthe data exhibits extremely strong clusteringthis technique will result ina smaller description length than simply transmitting the elements ofewithoutany clusters.howeverifthe clustering effect is not so strongit will likelyincrease rather than decrease the description length.the overhead oftransmit-ting cluster-specific distributions for attribute values will more than offset theadvantage gained by encoding each training instance relative to the cluster it liesin.this is where more sophisticated coding techniques come in.once the clustercenters have been communicatedit is possible to transmit cluster-specific prob-ability distributions adaptivelyin tandem with the relevant instancestheinstances themselves help to define the probability distributionsand the prob-ability distributions help to define the instances.we will not venture furtherinto coding techniques here.the point is that the mdl formulationproperlyappliedmay be flexible enough to support the evaluation ofclustering.butactually doing it satisfactorily in practice is not readingthe statistical basis ofconfidence tests is well covered in most statistics textswhich also give tables ofthe normal distribution and student s distribution.weuse an excellent course textwild and seber we recommend verystrongly ifyou can get hold ofit. student is the nom de plume ofa statisti-cian called william gossetwho obtained a post as a chemist in the guinnessbrewery in dublinirelandin and invented the t-test to handle smallsamples for quality control in brewing.the corrected resampled t-test was pro-posed by nadeau and bengio is a standard statisticaltechniqueand its application in machine learning has been extensively investi-gated and compared with the bootstrap by kohavi bootstrap tech-nique itselfis thoroughly covered by efron and tibshirani kappa statistic was introduced by cohen has inves-tigated a heuristic way ofgeneralizing to the multiclass case the algorithm givenin section to make two-class learning schemes cost sensitive.lift charts aredescribed by berry and use ofroc analysis in signal what s been am page tion theory is covered by egan work has been extended for visual-izing and analyzing the behavior ofdiagnostic systems and is alsoused in medicine and schultz and fawcett broughtthe idea ofroc analysis to the attention ofthe machine learning and datamining community.witten et explain the use ofrecall and precisionin information retrieval systemsthe f-measure is described by van and holte introduced cost curves and investigatedtheir properties.the mdl principle was formulated by rissanen s discovery ofhis economical three laws ofplanetary motionand his doubts about themarerecounted by koestler s principle ofmultiple explanations is mentioned by li and from asmis am page am page we have seen the basic ideas ofseveral machine learning methods and studiedin detail how to assess their performance on practical data mining problems.now we are well prepared to look at realindustrial-strengthmachine learningalgorithms.our aim is to explain these algorithms both at a conceptual leveland with a fair amount oftechnical detail so that you can understand them fullyand appreciate the key implementation issues that arise.in truththere is a world ofdifference between the simplistic methodsdescribed in chapter and the actual algorithms that are widely used in prac-tice.the principles are the same.so are the inputs and outputs methods ofknowledge representation.but the algorithms are far more complexprincipallybecause they have to deal robustly and sensibly with real-world problems suchas numeric attributesmissing valuesand most challenging ofall noisy data.to understand how the various methods cope with noisewe will have to drawon some ofthe statistical knowledge that we learned in chapter opened with an explanation ofhow to infer rudimentary rules andwent on to examine statistical modeling and decision trees.then we machine learning am page to rule induction and continued with association ruleslinear modelsthenearest-neighbor method ofinstance-based learningand clustering.thepresent chapter develops all these topics except association ruleswhich havealready been covered in adequate detail.we begin with decision tree induction and work up to a full description ofthe systema landmark decision tree program that is probably the machinelearning workhorse most widely used in practice to date.next we describe deci-sion rule induction.despite the simplicity ofthe ideainducing decision rulesthat perform comparably with state-of-the-art decision trees turns out to bequite difficult in practice.most high-performance rule inducers find an initialrule set and then refine it using a rather complex optimization stage that dis-cards or adjusts individual rules to make them work better together.we describethe ideas that underlie rule learning in the presence ofnoiseand then go on tocover a scheme that operates by forming partial decision treesan approach that has been demonstrated to perform as well as other state-of-the-art rulelearners yet avoids their complex and ad hoc heuristics.following thiswe takea brieflook at how to generate rules with exceptionswhich were described insection has been resurgence ofinterest in linear models with the introductionofsupport vector machinesa blend oflinear modeling and instance-based learning.support vector machines select a small number ofcritical boundaryinstances called support vectorsfrom each class and build a linear discriminantfunction that separates them as widely as possible.these systems transcend the limitations oflinear boundaries by making it practical to include extra nonlinear terms in the functionmaking it possible to form quadraticcubicand higher-order decision boundaries.the same techniques can be applied to the perceptron described in section to implement complex decision bound-aries.an older technique for extending the perceptron is to connect unitstogether into multilayer neural networks. all these ideas are described insection next section ofthe chapter describes instance-based learnersdevelop-ing the simple nearest-neighbor method introduced in section and showingsome more powerful alternatives that perform explicit generalization.follow-ing thatwe extend linear regression for numeric prediction to a more sophis-ticated procedure that comes up with the tree representation introduced insection and go on to describe locally weighted regressionan instance-basedstrategy for numeric prediction.next we return to clustering and review somemethods that are more sophisticated than simple k-meansmethods thatproduce hierarchical clusters and probabilistic clusters.finallywe look atbayesian networksa potentially very powerful way ofextending the na ve bayesmethod to make it less na ve by dealing with datasets that have machine learning am page because ofthe nature ofthe material it containsthis chapter differs from theothers in the book.sections can be read independentlyand each section is self-containedincluding the references to further readingwhich are gatheredtogether in a discussionsubsection at the end ofeach treesthe first machine learning scheme that we will develop in detail derives fromthe simple divide-and-conquer algorithm for producing decision trees that wasdescribed in section needs to be extended in several ways before it is readyfor use on real-world problems.first we consider how to deal with numericattributes andafter thatmissing values.then we look at the all-importantproblem ofpruning decision treesbecause although trees constructed by thedivide-and-conquer algorithm as described perform well on the training setthey are usually overfitted to the training data and do not generalize well toindependent test sets.next we consider how to convert decision trees to classi-fication rules.in all these aspects we are guided by the popular decision treealgorithm its commercial successor emerged as theindustry workhorse for off-the-shelfmachine learning.finallywe look at theoptions provided by and themselves.numeric attributesthe method we have described only works when all the attributes are nominalwhereasas we have seenmost real datasets contain some numeric attributes.it is not too difficult to extend the algorithm to deal with these.for a numericattribute we will restrict the possibilities to a two-wayor binarysplit.supposewe use the version ofthe weather data that has some numeric features temperature is being considered for the first splitthe tem-perature values involved values have been collapsed together. there are only possible posi-tions for the breakpoint ifthe breakpoint is not allowed to separate itemsofthe same class.the information gain for each can be calculated in the usualway.for examplethe test produces four yes s and two no swhereas produces five yes s and three no sand so the infor-mation value ofthis test am page it is common to place numeric thresholds halfway between the values thatdelimit the boundaries ofa conceptalthough something might be gained byadopting a more sophisticated policy.for examplewe will see later thatalthough the simplest form ofinstance-based learning puts the dividing linebetween concepts in the middle ofthe space between themother methods thatinvolve more than just the two nearest examples have been suggested.when creating decision trees using the divide-and-conquer methodonce thefirst attribute to split on has been selecteda top-level tree node is created thatsplits on that attributeand the algorithm proceeds recursively on each ofthechild nodes.for each numeric attributeit appears that the subset ofinstancesat each child node must be re-sorted according to that attribute s values andindeedthis is how programs for inducing decision trees are usually written.howeverit is not actually necessary to re-sort because the sort order at a parentnode can be used to derive the sort order for each childleading to a speedierimplementation.consider the temperature attribute in the weather datawhosesort order time including duplicates italicized number below each temperature value gives the number oftheinstance that has that valuethus instance number has temperature value has temperature value so on.suppose we decide to split atthe top level on the attribute outlook.consider the child node for which outlook in fact the examples with this value ofoutlookare numbers italicized sequence is stored with the example set adifferent sequence must be stored for each numeric attribute that contains a pointer to instance points to instance to instance so on then it is a simple matter to read offthe exam-ples for which outlook order.all that is necessary is to scan throughthe instances in the indicated orderchecking the outlookattribute for each andwriting down the ones with the appropriate repeated sorting can be avoided by storing with each subset ofinstancesthe sort order for that subset according to each numeric attribute.the sort ordermust be determined for each numeric attribute at the beginningno furthersorting is necessary thereafter.when a decision tree tests a nominal attribute as described in section is made for each possible value ofthe attribute.howeverwe haverestricted splits on numeric attributes to be binary.this creates an importantdifference between numeric attributes and nominal onesonce you havebranched on a nominal attributeyou have used all the information that it machine learning am page whereas successive splits on a numeric attribute may continue to yield newinformation.whereas a nominal attribute can only be tested once on any pathfrom the root ofa tree to the leafa numeric one can be tested many times.thiscan yield trees that are messy and difficult to understand because the tests onany single numeric attribute are not located together but can be scattered alongthe path.an alternativewhich is harder to accomplish but produces a morereadable treeis to allow a multiway test on a numeric attributetesting againstseveral constants at a single node ofthe tree.a simpler but less powerful solu-tion is to prediscretize the attribute as described in section valuesthe next enhancement to the decision-tree-building algorithm deals with theproblems ofmissing values.missing values are endemic in real-world datasets.as explained in chapter way ofhandling them is to treat themas just another possible value ofthe attributethis is appropriate ifthe fact thatthe attribute is missing is significant in some way.in that case no further actionneed be taken.but ifthere is no particular significance in the fact that a certaininstance has a missing attribute valuea more subtle solution is needed.it istempting to simply ignore all instances in which some ofthe values are missingbut this solution is often too draconian to be viable.instances with missingvalues often provide a good deal ofinformation.sometimes the attributeswhose values are missing play no part in the decisionin which case theseinstances are as good as any other.one question is how to apply a given decision tree to an instance in whichsome ofthe attributes to be tested have missing values.we outlined a solutionin section that involves notionally splitting the instance into piecesusing anumeric weighting methodand sending part ofit down each branch in pro-portion to the number oftraining instances going down that branch.eventu-allythe various parts ofthe instance will each reach a leafnodeand thedecisions at these leafnodes must be recombined using the weights that havepercolated to the leaves.the information gain and gain ratio calculationsdescribed in section can also be applied to partial instances.instead ofhaving integer countsthe weights are used when computing both gain figures.another question is how to partition the training set once a splitting attrib-ute has been chosento allow recursive application ofthe decision tree forma-tion procedure on each ofthe daughter nodes.the same weighting procedureis used.instances for which the relevant attribute value is missing are notion-ally split into piecesone piece for each branchin the same proportion as theknown instances go down the various branches.pieces ofthe instance con-tribute to decisions at lower nodes in the usual way through the informationgain calculationexcept that they are weighted accordingly.they may be am page split at lower nodesofcourseifthe values ofother attributes are unknown aswell.pruningwhen we looked at the labor negotiations problem in chapter found thatthe simple decision tree in figure actually performs better than the morecomplex one in figure and it makes more sense too.now it is time tolearn how to prune decision trees.by building the complete tree and pruning it afterward we are adopting astrategy ofpostpruningsometimes called backward pruning rather thanprepruningor forward pruning.prepruning would involve trying to decideduring the tree-building process when to stop developing subtrees quite anattractive prospect because that would avoid all the work ofdeveloping subtreesonly to throw them away afterward.howeverpostpruning does seem to offersome advantages.for examplesituations occur in which two attributes indi-vidually seem to have nothing to contribute but are powerful predictors whencombined a sort ofcombination-lock effect in which the correct combinationofthe two attribute values is very informative whereas the attributes taken indi-vidually are not.most decision tree builders postpruneit is an open questionwhether prepruning strategies can be developed that perform as well.two rather different operations have been considered for postpruningsubtree replacementand subtree raising.at each nodea learning scheme mightdecide whether it should perform subtree replacementsubtree raisingor leavethe subtree as it isunpruned.subtree replacement is the primary pruning oper-ationand we look at it first.the idea is to select some subtrees and replace themwith single leaves.for examplethe whole subtree in figure internal nodes and four leafnodeshas been replaced by the single leafbad.this will certainly cause the accuracy on the training set to decrease ifthe orig-inal tree was produced by the decision tree algorithm described previouslybecause that continued to build the tree until all leafnodes were pure untilall attributes had been tested.howeverit may increase the accuracy on an inde-pendently chosen test set.when subtree replacement is implementedit proceeds from the leaves andworks back up toward the root.in the figure examplethe whole subtree infigure would not be replaced at once.firstconsideration would be givento replacing the three daughter nodes in the health plan contributionsubtreewith a single leafnode.assume that a decision is made to perform this replace-ment we will explain how this decision is made shortly.thencontinuing towork back from the leavesconsideration would be given to replacing theworking hours per weeksubtreewhich now has just two daughter nodeswith asingle leafnode.in the figure example this replacement was indeed machine learning am page which accounts for the entire subtree in figure being replaced by a singleleafmarked bad.finallyconsideration would be given to replacing the two daughter nodes in the wage increase yearsubtree with a single leafnode.in this case that decision was not madeso the tree remains as shown infigure will examine how these decisions are actually madeshortly.the second pruning operationsubtree raisingis more complexand it is notclear that it is necessarily always worthwhile.howeverbecause it is used in theinfluential decision tree-building system describe it here.subtreeraising does not occur in the figure exampleso use the artificial exampleoffigure for illustration.hereconsideration is given to pruning the tree infigure the result is shown in figure entire subtree fromc downward has been raised to replace the b subtree.note that although thedaughters ofb and c are shown as leavesthey can be entire subtrees.ofcourseifwe perform this raising operationit is necessary to reclassify the examples atthe nodes marked and into the new subtree headed by c.this is why thedaughters ofthat node are marked with to indicate thatthey are not the same as the original daughters but differ by the inclu-sion ofthe examples originally covered by and raising is a potentially time-consuming operation.in actual imple-mentations it is generally restricted to raising the subtree ofthe most popularbranch.that iswe consider doing the raising illustrated in figure providedthat the branch from b to c has more training examples than the branches fromb to node or from b to node example node were themajority daughter ofbwe would consider raising node to replace b andreclassifying all examples under cas well as the examples from node thenew node.estimating error ratesso much for the two pruning operations.now we must address the question ofhow to decide whether to replace an internal node with a leaffor subtreereplacementor whether to replace an internal node with one ofthe nodesbelow it subtree raising.to make this decision rationallyit is necessary toestimate the error rate that would be expected at a particular node given anindependently chosen test set.we need to estimate the error at internal nodesas well as at leafnodes.ifwe had such an estimateit would be clear whetherto replaceor raisea particular subtree simply by comparing the estimated errorofthe subtree with that ofits proposed replacement.before estimating the errorfor a subtree proposed for raisingexamples that lie under siblings ofthe currentnode the examples at nodes and offigure would have to be tem-porarily reclassified into the raised am page it is no use taking the training set error as the error estimatethat would notlead to any pruning because the tree has been constructed expressly for that par-ticular training set.one way ofcoming up with an error estimate is the stan-dard verification techniquehold back some ofthe data originally given and useit as an independent test set to estimate the error at each node.this is calledreduced-errorpruning.it suffers from the disadvantage that the actual tree isbased on less data.the alternative is to try to make some estimate oferror based on the train-ing data itself.that is what doesand we will describe its method here.itis a heuristic based on some statistical reasoningbut the statistical underpin-ning is rather weak and ad hoc.howeverit seems to work well in practice.theidea is to consider the set ofinstances that reach each node and imagine thatthe majority class is chosen to represent that node.that gives a certain numberof errors eout ofthe total number ofinstancesn.now imagine that thetrue probability oferror at the node is qand that the ninstances are generatedby a bernoulli process with parameter qofwhich eturn out to be errors.this is almost the same situation as we considered when looking at theholdout method in section we calculated confidence intervals on the true success probability pgiven a certain observed success rate.there aretwo differences.one is trivialhere we are looking at the error rate qrather than the success rate pthese are simply related by p second is more serioushere the figures eand nare measured from the training datawhereas in section we were considering independent test data instead.because ofthis differencewe make a pessimistic estimate ofthe error rate byusing the upper confidence limit rather than by stating the estimate as a confi-dence machine learning ofsubtree raisingwhere node c is raised to subsume node am page the mathematics involved is just the same as before.given a particular con-fidence cthe default figure used by is c find confidence limitszsuch thatwhere nis the number ofsamplesfenis the observed error rateand qisthe true error rate.as beforethis leads to an upper confidence limit for q.nowwe use that upper confidence limit as a estimate for the error rateeat the nodenote the use ofthe before the square root in the numerator to obtain theupper confidence limit.herezis the number ofstandard deviations corre-sponding to the confidence cwhich for is see how all this works in practicelet s look again at the labor negotiationsdecision tree offigure parts ofwhich are reproduced in figure the number oftraining examples that reach the leaves added.we use thepreceding formula with a confidence figurethat iswith z lower left leaffor which e so these figuresinto the formulathe upper confidence limit is calculated as e meansthat instead ofusing the training set error rate for this leafwhich is use the pessimistic estimate is pessimistic indeedconsideringthat it would be a bad mistake to let the error rate exceed for a two-classproblem.but things are worse for the neighboring leafwhere e and n the upper confidence becomes e third leafhas the same value ofeas the first.the next step is to combine the error estimates forthese three leaves in the ratio ofthe number ofexamples they leads to a combined error estimate we consider the errorestimate for the parent nodehealth plan contribution.this covers nine badexamples and five good onesso the training set error rate is thesevaluesthe preceding formula yields a pessimistic error estimate ofe this is less than the combined error estimate ofthe three childrentheyare pruned away.the next step is to consider the working hours per weeknodewhich now hastwo children that are both leaves.the error estimate for the firstwith for the second it is as we have just seen.com-bining these in the appropriate ratio leads to a value that is higher am page machine learning schemesthe error estimate for the working hoursnodeso the subtree is pruned away andreplaced by a leafnode.the estimated error figures obtained in these examples should be taken witha grain ofsalt because the estimate is only a heuristic one and is based on anumber ofshaky assumptionsthe use ofthe upper confidence limittheassumption ofa normal distributionand the fact that statistics from the train-ing set are used.howeverthe qualitative behavior ofthe error formula is correctand the method seems to work reasonably well in practice.ifnecessarytheunderlying confidence levelwhich we have taken to be be tweaked toproduce more satisfactory results.complexity of decision tree inductionnow that we have learned how to accomplish the pruning operationswe havefinally covered all the central aspects ofdecision tree induction.let s take stockand consider the computational complexity ofinducing decision trees.we willuse the standard order notationon stands for a quantity that grows at mostlinearly with grows at most quadratically with nand so on.suppose that the training data contains ninstances and mattributes.we needto make some assumption about the size ofthe treeand we will assume that itsdepth is on the order oflognthat isologn.this is the standard rate ofgrowth ofa tree with nleavesprovided that it remains bushy and doesn tdegenerate into a few very longstringy branches.note that we are tacitly assum-wage increase first year good plan contribution hoursper weeknonehalffullfigure the labor negotiations decision am page ing that most ofthe instances are different from each otherand this is almostthe same thing that the mattributes provide enough tests to allow theinstances to be differentiated.for exampleifthere were only a few binary attri-butesthey would allow only so many instances to be differentiated and the tree could not grow past a certain pointrendering an in the limit analysismeaningless.the computational cost ofbuilding the tree in the first place isconsider the amount ofwork done for one attribute over all nodes ofthe tree.not all the examples need to be considered at each nodeofcourse.but at eachpossible tree depththe entire set ofninstances must be considered.becausethere are log ndifferent depths in the treethe amount ofwork for this oneattribute is onlogn.at each node all attributes are consideredso the totalamount ofwork is omnlogn.this reasoning makes some assumptions.ifsome attributes are numerictheymust be sortedbut once the initial sort has been done there is no need to re-sort at each tree depth ifthe appropriate algorithm is used earlier onpage initial sort takes onlogn operations for each ofup to mattrib-utesthus the preceding complexity figure is unchanged.ifthe attributes arenominalall attributes do not have to be considered at each tree node becauseattributes that are used further up the tree cannot be reused.howeverifattrib-utes are numericthey can be reused and so they have to be considered at everytree level.nextconsider pruning by subtree replacement.firstan error estimate must be made for every tree node.provided that counts are maintained appropriatelythis is linear in the number ofnodes in the tree.then each node needs to be considered for replacement.the tree has at most nleavesonefor each instance.ifit was a binary treeeach attribute being numeric or two-valuedthat would give it nodesmultiway branches would only serveto decrease the number ofinternal nodes.thus the complexity ofsubtreereplacement isfinallysubtree lifting has a basic complexity equal to subtree replacement.but there is an added cost because instances need to be reclassified during thelifting operation.during the whole processeach instance may have to be reclas-sified at every node between its leafand the rootthat isas many as ologntimes.that makes the total number ofreclassifications onlogn.and reclas-sification is not a single operationone that occurs near the root will take ologn operationsand one ofaverage depth will take halfofthis.thus the totalcomplexity ofsubtree lifting is as am page taking into account all these operationsthe full complexity ofdecision treeinduction isfrom trees to rulesit is possible to read a set ofrules directly offa decision treeas noted in generating a rule for each leafand making a conjunction ofall the testsencountered on the path from the root to that leaf.this produces rules that areunambiguous in that it doesn t matter in what order they are executed.howeverthe rules are more complex than necessary.the estimated error rate described previously provides exactly the mecha-nism necessary to prune the rules.given a particular ruleeach condition in itis considered for deletion by tentatively removing itworking out which ofthetraining examples are now covered by the rulecalculating from this a pes-simistic estimate ofthe error rate ofthe new ruleand comparing this with thepessimistic estimate for the original rule.ifthe new rule is betterdelete thatcondition and carry onlooking for other conditions to delete.leave the rulewhen there are no conditions left that will improve it ifthey are removed.onceall rules have been pruned in this wayit is necessary to see whether there areany duplicates and remove them from the rule set.this is a greedy approach to detecting redundant conditions in a ruleandthere is no guarantee that the best set ofconditions will be removed.animprovement would be to consider all subsets ofconditionsbut this is usuallyprohibitively expensive.another solution might be to use an optimization tech-nique such as simulated annealing or a genetic algorithm to select the bestversion ofthis rule.howeverthe simple greedy solution seems to produce quitegood rule sets.the problemeven with the greedy methodis computational cost.for everycondition that is a candidate for deletionthe effect ofthe rule must be reeval-uated on all the training instances.this means that rule generation from treestends to be very slowand the next section describes much faster methods thatgenerate classification rules directly without forming a decision tree choices and optionswe finish our study ofdecision trees by making a few remarks about practicaluse ofthe landmark decision tree program and its successor devised by j.ross quinlan over a period beginning in the complete description early versionappears as anexcellent and readable book with the full source machine learning am page the more recent available commercially.its decision tree induc-tion seems to be essentially the same as that used by tests show somedifferences but negligible improvements.howeverits rule generation is greatlysped up and clearly uses a different techniquealthough this has not beendescribed in the open works essentially as described in the preceding sections.the default con-fidence value is set at and works reasonably well in most casespossibly itshould be altered to a lower valuewhich causes more drastic pruningiftheactual error rate ofpruned trees on test sets is found to be much higher thanthe estimated error rate.there is one other important parameter whose effectis to eliminate tests for which almost all ofthe training examples have the sameoutcome.such tests are often oflittle use.consequentlytests are not incorpo-rated into the decision tree unless they have at least two outcomes that have atleast a minimum number ofinstances.the default value for this minimum it is controllable and should perhaps be increased for tasks that have a lotofnoisy data.discussiontop-down induction ofdecision trees is probably the most extensivelyresearched method ofmachine learning used in data mining.researchers haveinvestigated a panoply ofvariations for almost every conceivable aspect ofthelearning process for exampledifferent criteria for attribute selection or modified pruning methods.howeverthey are rarely rewarded by substantialimprovements in accuracy over a spectrum ofdiverse datasets.sometimes thesize ofthe induced trees is significantly reduced when a different pruning strat-egy is adoptedbut often the same effect can be achieved by setting spruning parameter to a smaller value.in our description ofdecision treeswe have assumed that only one attribute is used to split the data into subsets at each node ofthe tree.howeverit is possible to allow tests that involve several attributes at a time.for examplewith numeric attributes each test can be on a linear combinationofattribute values.then the final tree consists ofa hierarchy oflinear modelsofthe kind we described in section the splits are no longer restrictedto being axis-parallel.trees with tests involving more than one attribute arecalled multivariatedecision treesin contrast to the simple univariatetrees that we normally use.multivariate tests were introduced with the classificationand regression treescart system for learning decision trees et are often more accurate and smaller than univariate trees but takemuch longer to generate and are also more difficult to interpret.we brieflymention one way ofgenerating them using principal components analysis insection am page ruleswe call the basic covering algorithm for generating rules that was described insection a separate-and-conquer technique because it identifies a rule thatcovers instances in the class excludes ones not in the classseparates themoutand continues on those that are left.such algorithms have been used as thebasis ofmany systems that generate rules.there we described a simple correct-ness-based measure for choosing what test to add to the rule at each stage.howeverthere are many other possibilitiesand the particular criterion that is used has a significant effect on the rules produced.we examine different criteria for choosing tests in this section.we also look at how the basic rule-generation algorithm can be extended to more practical situations by accom-modating missing values and numeric attributes.but the real problem with all these rule-generation schemes is that they tendto overfit the training data and do not generalize well to independent test setsparticularly on noisy data.to be able to generate good rule sets for noisy datait is necessary to have some way ofmeasuring the real worth ofindividual rules.the standard approach to assessing the worth ofrules is to evaluate their errorrate on an independent set ofinstancesheld back from the training setand weexplain this next.after thatwe describe two industrial-strength rule learnersone that combines the simple separate-and-conquer technique with a globaloptimization step and another one that works by repeatedly building partialdecision trees and extracting rules from them.finallywe consider how to gen-erate rules with exceptionsand exceptions to the exceptions.criteria for choosing testswhen we introduced the basic rule learner in section had to figure outa way ofdeciding which ofmany possible tests to add to a rule to prevent itfrom covering any negative examples.for this we used the test that maximizesthe ratiowhere tis the total number ofinstances that the new rule will coverand pisthe number ofthese that are positive that isthat belong to the class in ques-tion.this attempts to maximize the correctness ofthe rule on the basis thatthe higher the proportion ofpositive examples it coversthe more correct a ruleis.one alternative is to calculate an information gainwhere pand tare the number ofpositive instances and the total number ofinstances covered by the new ruleas beforeand pand tare the correspondingpptptloglog- machine learning am page ofinstances that satisfied the rule beforethe new test was added.therationale for this is that it represents the total information gained regarding the current positive exampleswhich is given by the number ofthem that satisfythe new testmultiplied by the information gained regarding each one.the basic criterion for choosing a test to add to a rule is to find one thatcovers as many positive examples as possiblewhile covering as few negativeexamples as possible.the original correctness-based heuristicwhich is just thepercentage ofpositive examples among all examples covered by the ruleattainsa maximum when no negative examples are covered regardless ofthe numberofpositive examples covered by the rule.thus a test that makes the rule exactwill be preferred to one that makes it inexactno matter how few positive exam-ples the former rule covers or how many positive examples the latter covers.forexampleifwe can choose between a test that covers one examplewhich is pos-itivethis criterion will prefer it over a test that covers positive examplesalong with one negative one.the information-based heuristicon the other handplaces far more empha-sis on covering a large number ofpositive examples regardless ofwhether therule so created is exact.ofcourseboth algorithms continue adding tests untilthe final rule produced is exactwhich means that the rule will be finished earlierusing the correctness measurewhereas more terms will have to be added iftheinformation-based measure is used.thus the correctness-based measure mightfind special cases and eliminate them completelysaving the larger picture forlater the more general rule might be simpler because awkward specialcases have already been dealt withwhereas the information-based one will tryto generate high-coverage rules first and leave the special cases until later.it isby no means obvious that either strategy is superior to the other at producingan exact rule set.moreoverthe whole situation is complicated by the fact thatas described laterrules may be pruned and inexact ones tolerated.missing values numeric attributesas with divide-and-conquer decision tree algorithmsthe nasty practical con-siderations ofmissing values and numeric attributes need to be addressed.infactthere is not much more to say.now that we know how these problems canbe solved for decision tree inductionappropriate solutions for rule inductionare easily given.when producing rules using covering algorithmsmissing values can best betreated as though they don t match any ofthe tests.this is particularly suitablewhen a decision list is being produced because it encourages the learning algo-rithm to separate out positive instances using tests that are known to succeed.it has the effect that either instances with missing values are dealt with by rulesinvolving other attributes that are not missingor any decisions about them am page deferred until most ofthe other instances have been taken care ofat which timetests will probably emerge that involve other attributes.covering algorithms fordecision lists have a decided advantage over decision tree algorithms in thisrespecttricky examples can be left until late in the processat which time theywill appear less tricky because most ofthe other examples have already beenclassified and removed from the instance set.numeric attributes can be dealt with in exactly the same way as they are fortrees.for each numeric attributeinstances are sorted according to theattribute s value andfor each possible thresholda binary less-thangreater-thantest is considered and evaluated in exactly the same way that a binary attributewould be.generating good rulessuppose you don t want to generate perfect rules that guarantee to give thecorrect classification on all instances in the training setbut would rather gen-erate sensible ones that avoid overfitting the training set and thereby stand abetter chance ofperforming well on new test instances.how do you decidewhich rules are worthwhile? how do you tell when it becomes counterproduc-tive to continue adding terms to a rule to exclude a few pesky instances ofthewrong typeall the while excluding more and more instances ofthe right typetoo?let s look at a few examples ofpossible rules some good and some bad for the contact lens problem in table first the ruleif astigmatism and tear production rate recommendation gives a correct result for four ofthe six cases that it coversthus its success fraction is we add a further term to make the rule a perfect oneif astigmatism and tear production rate age then recommendation improves accuracy to rule is better? the second one is moreaccurate on the training data but covers only two caseswhereas the first onecovers six.it may be that the second version is just overfitting the training data.for a practical rule learner we need a principled way ofchoosing the appropri-ate version ofa rulepreferably one that maximizes accuracy on future test data.suppose we split the training data into two parts that we will call a growingsetand a pruning set.the growing set is used to form a rule using the basic cov-ering algorithm.then a test is deleted from the ruleand the effect is evaluatedby trying out the truncated rule on the pruning set and seeing whether it machine learning am page better than the original rule.this pruning process repeats until therule cannot be improved by deleting any further tests.the whole procedure isrepeated for each classobtaining one best rule for each classand the overallbest rule is established by evaluating the rules on the pruning set.this rule isthen added to the rule setthe instances it covers removed from the trainingdata from both growing and pruning sets and the process is repeated.why not do the pruning as we build the rule uprather than building up thewhole thing and then throwing parts away? that iswhy not preprune ratherthan postprune? just as when pruning decision trees it is often best to grow thetree to its maximum size and then prune backso with rules it is often best tomake a perfect rule and then prune it.who knows? adding that last term maymake a really good rulea situation that we might never have noticed had weadopted an aggressive prepruning strategy.it is essential that the growing and pruning sets are separatebecause it is mis-leading to evaluate a rule on the very data used to form itthat would lead toserious errors by preferring rules that were overfitted.usually the training set issplit so that two-thirds ofinstances are used for growing and one-third forpruning.a disadvantageofcourseis that learning occurs from instances in thegrowing set onlyand so the algorithm might miss important rules because somekey instances had been assigned to the pruning set.moreoverthe wrong rulemight be preferred because the pruning set contains only one-third ofthe dataand may not be completely representative.these effects can be ameliorated byresplitting the training data into growing and pruning sets at each cycle ofthealgorithmthat isafter each rule is finally chosen.the idea ofusing a separate pruning set for pruning which is applicable todecision trees as well as rule sets is called reduced-error pruning.the variantdescribed previously prunes a rule immediately after it has been grown and iscalled incremental reduced-error pruning.another possibility is to build a fullunpruned rule set firstpruning it afterwards by discarding individual tests.howeverthis method is much slower.ofcoursethere are many different ways to assess the worth ofa rule basedon the pruning set.a simple measure is to consider how well the rule would doat discriminating the predicted class from other classes ifit were the only rulein the theoryoperating under the closed world assumption.ifit gets pinstancesright out ofthe tinstances that it coversand there are pinstances ofthis classout ofa total tofinstances altogetherthen it gets ppositive instances right.the instances that it does not cover include n-nnegative oneswhere n the number ofnegative instances that the rule covers and n thetotal number ofnegative instances.thus the rule has an overall success ratio am page and this quantityevaluated on the test sethas been used to evaluate the successofa rule when using reduced-error pruning.this measure is open to criticism because it treats noncoverage ofnegativeexamples as equally important as coverage ofpositive oneswhich is unrealisticin a situation where what is being evaluated is one rule that will eventually servealongside many others.for examplea rule that gets p instances rightout ofa total coverage gets n wrong is judged as moresuccessful than one that gets p out ofa total coverage wrongbecause the first case but the second.this is counterintuitivethe first rule is clearly lesspredictive than the secondbecause it has as opposed to only chance ofbeing incorrect.using the success rate ptas a measureas in the original formulation ofthecovering algorithm not the perfect solution eitherbecause itwould prefer a rule that got a single instance right out ofa total cover-age n to the far more useful rule that got right out heuristic that has been used is that suffers from exactlythe same problem because and so the resultwhen compar-ing one rule with anotheris just the same as with the success rate.it seems hardto find a simple measure ofthe worth ofa rule that corresponds with intuitionin all cases.whatever heuristic is used to measure the worth ofa rulethe incrementalreduced-error pruning algorithm is the same.a possible rule learning algorithmbased on this idea is given in figure generates a decision listcreating rulesfor each class in turn and choosing at each stage the best version ofthe ruleaccording to its worth on the pruning data.the basic covering algorithm forrule generation is used to come up with good rules for each classchoosing conditions to add to the rule using the accuracy measure ptthat wedescribed earlier.this method has been used to produce rule-induction schemes that canprocess vast amounts ofdata and operate very quickly.it can be accelerated bygenerating rules for the classes in order rather than generating a rule for eachclass at every stage and choosing the best.a suitable ordering is the increasingorder in which they occur in the training set so that the rarest class is processedfirst and the most common ones are processed later.another significantspeedup is obtained by stopping the whole process when a rule ofsufficientlylow accuracy is generatedso as not to spend time generating a lot ofrules atthe end with very small coverage.howeververy simple terminating conditionssuch as stopping when the accuracy for a rule is lower than the default accu-racy for the class it predicts do not give the best performanceand the onlyconditions that have been found that seem to perform well are rather compli-cated ones based on the mdl machine learning am page global optimizationin generalrules generated using incremental reduced-error pruning in thismanner seem to perform quite wellparticularly on large datasets.howeverithas been found that a worthwhile performance advantage can be obtained byperforming a global optimization step on the set ofrules induced.the motiva-tion is to increase the accuracy ofthe rule set by revising or replacing individ-ual rules.experiments show that both the size and the performance ofrule setsare significantly improved by postinduction optimization.on the other handthe process itselfis rather complex.to give an idea ofhow elaborate and heuristic industrial-strength rulelearners becomefigure shows an algorithm called ripperan acronym forrepeated incremental pruning to produce error reduction.classes are examined inincreasing size and an initial set ofrules for the class is generated using incre-mental reduced-error pruning.an extra stopping condition is introduced thatdepends on the description length ofthe examples and rule set.the descriptionlength dlis a complex formula that takes into account the number ofbitsneeded to send a set ofexamples with respect to a set ofrulesthe number ofbits required to send a rule with kconditionsand the number ofbits neededto send the integer k times an arbitrary factor to compensate for pos-sible redundancy in the attributes.having produced a rule set for the classeachrule is reconsidered and two variants producedagain using reduced-errorpruning but at this stageinstances covered by other rules for the class areremoved from the pruning setand success rate on the remaining instances is used as the pruning criterion.ifone ofthe two variants yields a better initialize e to the instance setsplit e into grow and prune in the ratio for each class c for which grow and prune both contain an instance use the basic covering algorithm to create the best perfect rule for class c calculate the worth wr for the rule on prune and of the rule with the final condition omitted wr- while wr- wr remove the final condition from the rule and repeat the previous step from the rules generated select the one with the largest wr print the rule remove the instances covered by the rule from e continuefigure for forming rules by incremental reduced-error am page machine learning schemesa initialize e to the instance setfor each class c from smallest to largest build split e into growing and pruning sets in the ratio repeat until there are no more uncovered examples of c or the description length of ruleset and examples is bits greater than the smallest dl found so far or the error rate exceeds grow phase grow a rule by greedily adding conditions until the rule is accurate by testing every possible value of each attribute and selecting the condition with greatest information gain g prune phase prune conditions in last-to-first order. continue as long as the worth w of the rule increases optimize generate variants for each rule r for class c split e afresh into growing and pruning sets remove all instances from the pruning set that are covered by other rules for c use grow and prune to generate and prune two competing rules from the newly-split data is a new rule rebuilt from scratch is generated by greedily adding antecedents to r. prune using the metric a of w on this reduced data select representative replace r by whichever of r and has the smallest dl. mop up if there are residual uncovered instances of class c return to the build stage to generate more rules based on these instances. clean up calculate dl for the whole ruleset and for the ruleset with each rule in remove instances covered by the rules just generatedturn omitted delete any rule that increases the dlcontinuefigure algorithm for rule learning and meaning am page lengthit replaces the rule.next we reactivate the original buildingphase to mop up any newly uncovered instances ofthe class.a final check ismade to ensure that each rule contributes to the reduction ofdescription lengthbefore proceeding to generate rules for the next class.obtaining rules from partial decision treesthere is an alternative approach to rule induction that avoids global optimiza-tion but nevertheless produces accuratecompactrule sets.the method com-bines the divide-and-conquer strategy for decision tree learning with theseparate-and-conquer one for rule learning.it adopts the separate-and-conquerstrategy in that it builds a ruleremoves the instances it coversand continuescreating rules recursively for the remaining instances until none are left.howeverit differs from the standard approach in the way that each rule iscreated.in essenceto make a single rulea pruned decision tree is built for thecurrent set ofinstancesthe leafwith the largest coverage is made into a ruleand the tree is discarded.the prospect ofrepeatedly building decision trees only to discard most ofthem is not as bizarre as it first seems.using a pruned tree to obtain a ruleinstead ofbuilding it incrementally by adding conjunctions one at a time avoidsa tendency to overprune that is a characteristic problem ofthe basic separate-and-conquer rule learner.using the separate-and-conquer methodology in con-junction with decision trees adds flexibility and speed.it is indeed wasteful tobuild a full decision tree just to obtain a single rulebut the process can be accel-erated significantly without sacrificing the preceding advantages.the key idea is to build a partial decision tree instead ofa fully explored one.a partial decision tree is an ordinary decision tree that contains branches tob dl see text g plogpt logptw p n t accuracy for this rulep number of positive examples covered by this rule positivesn number of negative examples covered by this rule negativest p n total number of examples covered by this rulen n n number of negative examples not covered by this rule negativesp number of positive examples of this classn number of negative examples of this class t p n total number of examples of this class figure am page undefined subtrees.to generate such a treethe construction and pruning oper-ations are integrated in order to find a stable subtree that can be simplified nofurther.once this subtree has been foundtree building ceases and a single ruleis read off.the tree-building algorithm is summarized in figure splits a set ofinstances recursively into a partial tree.the first step chooses a test and dividesthe instances into subsets accordingly.the choice is made using the same infor-mation-gain heuristic that is normally used for building decision trees the subsets are expanded in increasing order oftheir average entropy.the reason for this is that the later subsets will most likely not end up beingexpandedand a subset with low average entropy is more likely to result in asmall subtree and therefore produce a more general rule.this proceeds recur-sively until a subset is expanded into a leafand then continues further by back-tracking.but as soon as an internal node appears that has all its childrenexpanded into leavesthe algorithm checks whether that node is better replacedby a single leaf.this is just the standard subtree replacement operation ofdecision tree pruning is performed the algorithmbacktracks in the standard wayexploring siblings ofthe newly replaced node.howeverifduring backtracking a node is encountered all ofwhose children arenot leaves and this will happen as soon as a potential subtree replacement isnotperformed then the remaining subsets are left unexplored and the corre-sponding subtrees are left undefined.because ofthe recursive structure ofthealgorithmthis event automatically terminates tree generation.figure shows a step-by-step example.during the stages in figure building continues recursively in the normal way except machine learning schemesexpand-subset choose a test t and use it to split the set of examples into subsets sort subsets into increasing order of average entropy while is a subset x that has not yet been expanded and all subsets expanded so far are leaves expand-subsetx if the subsets expanded are leaves and estimated error for subtree estimated error for node undo expansion into subsets and make node a leaffigure for expanding examples into a partial am page each point the lowest-entropy sibling is chosen for expansionnode between stages and elliptical nodes are as yet unexpandedrec-tangular ones are leaves.between stages and rectangular node willhave lower entropy than its siblingnode cannot be expanded furtherbecause it is a leaf.backtracking occurs and node is chosen for expansion.once stage is reachedthere is a node node that has all ofits childrenexpanded into leavesand this triggers pruning.subtree replacement for is considered and acceptedleading to stage node is considered forsubtree replacementand this operation is again accepted.backtracking con-tinuesand node lower entropy than node expanded into twoleaves.now subtree replacement is considered for node that node not replaced.at this pointthe process terminates with the three-leafpartialtree ofstage data is noise-free and contains enough instances to prevent the algo-rithm from doing any pruningjust one path ofthe full decision tree has to beexplored.this achieves the greatest possible performance gain over the na ofbuilding a partial am page method that builds a full decision tree each time.the gain decreases as morepruning takes place.for datasets with numeric attributesthe asymptotic timecomplexity ofthe algorithm is the same as building the full decision treebecause in this case the complexity is dominated by the time required to sortthe attribute values in the first place.once a partial tree has been builta single rule is extracted from it.each leafcorresponds to a possible ruleand we seek the best leafofthose subtrees a small minority that have been expanded into leaves.experimentsshow that it is best to aim at the most general rule by choosing the leafthatcovers the greatest number ofinstances.when a dataset contains missing valuesthey can be dealt with exactly as theyare when building decision trees.ifan instance cannot be assigned to any givenbranch because ofa missing attribute valueit is assigned to each ofthe brancheswith a weight proportional to the number oftraining instances going down thatbranchnormalized by the total number oftraining instances with known valuesat the node.during testingthe same procedure is applied separately to eachrulethus associating a weight with the application ofeach rule to the testinstance.that weight is deducted from the instance s total weight before it ispassed to the next rule in the list.once the weight has reduced to zerothe pre-dicted class probabilities are combined into a final classification according tothe weights.this yields a simple but surprisingly effective method for learning decisionlists for noisy data.its main advantage over other comprehensive rule-generation schemes is simplicitybecause other methods require a complexglobal optimization stage to achieve the same level ofperformance.rules with exceptionsin section we learned that a natural extension ofrules is to allow them tohave exceptionsand exceptions to the exceptionsand so on indeed the wholerule set can be considered as exceptions to a default classification rule that isused when no other rules apply.the method ofgenerating a good ruleusingone ofthe measures described in the previous sectionprovides exactly themechanism needed to generate rules with exceptions.firsta default class is selected for the top-level ruleit is natural to use theclass that occurs most frequently in the training data.thena rule is found per-taining to any class other than the default one.ofall such rules it is natural toseek the one with the most discriminatory powerfor examplethe one with thebest evaluation on a test set.suppose this rule has the formif then class machine learning am page is used to split the training data into two subsetsone containing all instancesfor which the rule s condition is trueand the other containing those for whichit is false.ifeither subset contains instances ofmore than one classthe algo-rithm is invoked recursively on that subset.for the subset for which the condi-tion is truethe default class is the new class as specified by the rulefor the subset for which the condition is falsethe default class remains as it wasbefore.let s examine how this algorithm would work for the rules with exceptionsgiven in section for the iris data oftable will represent the rules inthe graphical form shown in figure is in fact equivalent to the textualrules we gave in figure default ofiris setosais the entry node at the topleft.horizontaldotted paths show exceptionsso the next boxwhich containsa rule that concludes iris versicoloris an exception to the default.below this isan alternativea second exception alternatives are shown by verticalsolidlines leading to the conclusion iris virginica.following the upper path alonghorizontally leads to an exception to the iris versicolorrule that overrides itwhenever the condition in the top right box holdswith the conclusion iris vir-ginica.below this is an alternativeleading it happens to the same conclu-sion.returning to the box at bottom centerthis has its own exceptionthe lowerright boxwhich gives the conclusion iris versicolor.the numbers at the lowerright ofeach box give the coverage ofthe ruleexpressed as the number of-- iris setosa length width length length width length width length length length iris iris iris iris iris arerepresented asdotted paths alternatives assolid ones.figure with exceptions for the iris am page examples that satisfy it divided by the number that satisfy its condition but notits conclusion.for examplethe condition in the top center box applies to ofthe examplesand ofthem are iris versicolor.the strength ofthis represen-tation is that you can get a very good feeling for the effect ofthe rules from theboxes toward the left-hand sidethe boxes at the right cover just a few excep-tional cases.to create these rulesthe default is first set to iris setosaby taking the mostfrequently occurring class in the dataset.this is an arbitrary choice because forthis dataset all classes occur exactly timesas shown in figure this default rule is correct in cases.then the best rule that predicts another classis sought.in this case it isif petal length and petal length and petal width then iris versicolorthis rule covers instancesofwhich are iris versicolor.it divides the datasetinto two subsetsthe instances that do satisfy the condition ofthe rule andthe remaining that do not.we work on the former subset first.the default class for these instances isiris versicolorthere are only three exceptionsall ofwhich happen to be iris virginica.the best rule for this subset that does not predict iris versicoloris identified nextif petal length and petal width then iris virginicait covers two ofthe three iris virginicasand nothing else.again it divides thesubset into twothose instances that satisfy its condition and those that do not.fortunatelyin this caseall instances that satisfy the condition do indeed have the class iris virginicaso there is no need for a further exception.howeverthe remaining instances still include the third iris virginicaalong iris versicolorswhich are the default at this point.again the best rule issoughtif sepal length and sepal width then iris virginicathis rule covers the remaining iris virginicaand nothing elseso it also has noexceptions.furthermoreall remaining instances in the subset that do not satisfyits condition have the class iris versicolorwhich is the defaultso no more needsto be done.return now to the second subset created by the initial rulethe instances thatdo not satisfy the conditionpetal length and petal length and petal width machine learning am page rules for these instances that do not predict the default class iris setosathe best isif petal length then iris virginicait covers all iris virginicasthat are in the example set were removed by thefirst ruleas explained previously.it also covers iris versicolor.this needs tobe taken care ofas an exceptionby the final ruleif petal length and sepal length then iris versicolorfortunatelythe set ofinstances that do notsatisfy its condition are all thedefaultiris setosa.thus the procedure is finished.the rules that are produced have the property that most ofthe examples arecovered by the high-level rules and the lower-level ones really do representexceptions.for examplethe last exception clause in the preceding rules and thedeeply nested elseclause both cover a solitary exampleand removing themwould have little effect.even the remaining nested exception rule covers onlytwo examples.thus one can get an excellent feeling for what the rules do byignoring all the deeper structure and looking only at the first level or two.thatis the attraction ofrules with exceptions.discussionall algorithms for producing classification rules that we have described use thebasic covering or separate-and-conquer approach.for the simplenoise-freecase this produces prism algorithm that is simple andeasy to understand.when applied to two-class problems with the closed worldassumptionit is only necessary to produce rules for one classthen the rulesare in disjunctive normal form and can be executed on test instances withoutany ambiguity arising.when applied to multiclass problemsa separate rule setis produced for each classthus a test instance may be assigned to more thanone classor to no classand further heuristics are necessary ifa unique pre-diction is sought.to reduce overfitting in noisy situationsit is necessary to produce rules thatare not perfect even on the training set.to do this it is necessary to have ameasure for the goodness or worthofa rule.with such a measure it is thenpossible to abandon the class-by-class approach ofthe basic covering algorithmand start by generating the very best ruleregardless ofwhich class it predictsand then remove all examples covered by this rule and continue the process.this yields a method for producing a decision list rather than a set ofinde-pendent classification rulesand decision lists have the important advantage thatthey do not generate ambiguities when am page the idea ofincremental reduced-error pruning is due to f rnkranz and widmer and forms the basis for fast and effective rule induction.the ripper rule learner is due to cohen the publisheddescription appears to differ from the implementation in precisely how the description length affects the stopping condition.what we have pre-sented here is the basic idea ofthe algorithmthere are many more details inthe implementation.the whole question ofmeasuring the value ofa rule has not yet been satis-factorily resolved.many different measures have been proposedsome blatantlyheuristic and others based on information-theoretical or probabilistic grounds.howeverthere seems to be no consensus on what the best measure to use is.an extensive theoretical study ofvarious criteria has been performed byf rnkranz and flach rule-learning method based on partial decision trees was developed byfrank and witten produces rule sets that are as accurate as those gen-erated by and more accurate than other fast rule-induction methods.howeverits main advantage over other schemes is not performance but sim-plicityby combining the top-down decision tree induction method with sepa-rate-and-conquer rule learningit produces good rule sets without any need forglobal optimization.the procedure for generating rules with exceptions was developed as anoption in the induct system by gaines and compton called themripple-downrules.in an experiment with a large medical dataset attributesand classesthey found that people can understandlarge systems ofrules with exceptions more readily than equivalent systems ofregular rules because that is the way that they think about the complex medicaldiagnoses that are involved.richards and compton describe their roleas an alternative to classic knowledge linear modelssection described how simple linear models can be used for classification insituations where all attributes are numeric.their biggest disadvantage is thatthey can only represent linear boundaries between classeswhich makes themtoo simple for many practical applications.support vector machines use linearmodels to implement nonlinear class boundaries.although it is a widely used termsupport vector machinesis something ofa misnomerthese are algorithmsnot machines. how can this be possible? the trick is easytrans-form the input using a nonlinear mappingin other wordstransform theinstance space into a new space.with a nonlinear mappinga straight line inthe new space doesn t look straight in the original instance space.a linear machine learning am page linear in the new space can represent a nonlinear decision boundary inthe original space.imagine applying this idea directly to the ordinary linear models in examplethe original set ofattributes could be replaced by one givingall products ofnfactors that can be constructed from these attributes.anexample for two attributesincluding all products with three factorsisherexis the and the two attribute valuesand there are fourweights wito be learned.as described in section result can be used forclassification by training one linear system for each class and assigning anunknown instance to the class that gives the greatest output x the standardtechnique ofmultiresponse linear and be the attrib-ute values for the test instance.to generate a linear model in the space spannedby these productseach training instance is mapped into the new space by computing all possible three-factor products ofits two attribute values.thelearning algorithm is then applied to the transformed instances.to classify aninstanceit is processed by the same transformation prior to classification.thereis nothing to stop us from adding in more synthetic attributes.for exampleifa constant term were includedthe original attributes and all two-factor prod-ucts ofthem would yield a total ofeight weights to be learned.alternativelyadding an additional attribute whose value was always a constant would havethe same effect. indeedpolynomials ofsufficiently high degree can approxi-mate arbitrary decision boundaries to any required accuracy.it seems too good to be true and it is.as you will probably have guessedproblems arise with this procedure because ofthe large number ofcoefficientsintroduced by the transformation in any realistic setting.the first snag is com-putational complexity.with attributes in the original datasetsuppose wewant to include all products with five factorsthen the learning algorithm willhave to determine more than coefficients.ifits run time is cubic in thenumber ofattributesas it is for linear regressiontraining will be infeasible.that is a problem ofpracticality.the second problem is one ofprincipleover-fitting.ifthe number ofcoefficients is large relative to the number oftraininginstancesthe resulting model will be too nonlinear it will overfit the train-ing data.there are just too many parameters in the model.the maximum margin hyperplanesupport vector machines solve both problems.they are based on an algorithmthat finds a special kind oflinear modelthe maximum margin hyperplane.wealready know what a hyperplane is it s just another word for a linear model.to visualize a maximum margin hyperplaneimagine a two-class dataset am page classes are linearly separablethat isthere is a hyperplane in instance space thatclassifies all training instances correctly.the maximum margin hyperplane isthe one that gives the greatest separation between the classes it comes no closerto either than it has to.an example is shown in figure which the classesare represented by open and filled circlesrespectively.technicallythe convexhullofa set ofpoints is the tightest enclosing convex polygonits outlineemerges when you connect every point ofthe set to every other point.becausewe have supposed that the two classes are linearly separabletheir convex hullscannot overlap.among all hyperplanes that separate the classesthe maximummargin hyperplane is the one that is as far away as possible from both convexhulls it is the perpendicular bisector ofthe shortest line connecting the hullswhich is shown dashed in the figure.the instances that are closest to the maximum margin hyperplane the oneswith minimum distance to it are called support vectors.there is always at leastone support vector for each classand often there are more.the important thingis that the set ofsupport vectors uniquely defines the maximum margin hyper-plane for the learning problem.given the support vectors for the two classeswe can easily construct the maximum margin hyperplane.all other traininginstances are irrelevant they can be deleted without changing the position andorientation ofthe hyperplane.a hyperplane separating the two classes might be machine learning schemesmaximum margin hyperplanesupport vectorsfigure maximum margin am page linear the two-attribute casewhere and the attribute valuesand there arethree weights wito be learned.howeverthe equation defining the maximummargin hyperplane can be written in another formin terms ofthe supportvectors.write the class value yofa training instance as either yesit is in this class or noit is not.then the maximum margin hyperplane ishereyiis the class value oftraining instance aiwhile band aiare numericparameters that have to be determined by the learning algorithm.note that aiand aare vectors.the vector arepresents a test instance just as the vector represented a test instance in the earlier formulation.the vectors aiare the support vectorsthose circled in figure are selected membersofthe training set.the term ai arepresents the dot product ofthe test instancewith one ofthe support vectors.ifyou are not familiar with dot product nota-tionyou should still be able to understand the gist ofwhat followsjust thinkofaias the whole set ofattribute values for the ith support vector.finallyband aiare parameters that determine the hyperplanejust as the weights parameters that determine the hyperplane in the earlier formulation.it turns out that finding the support vectors for the instance sets and deter-mining the parameters band aibelongs to a standard class ofoptimizationproblems known as constrained quadratic optimization.there are off-the-shelfsoftware packages for solving these problems fletcher for a com-prehensive and practical account ofsolution methods.howeverthe com-putational complexity can be reducedand learning can be acceleratedifspecial-purpose algorithms for training support vector machines are applied but the details ofthese algorithms lie beyond the scope ofthis book class boundarieswe motivated the introduction ofsupport vector machines by claiming thatthey can be used to model nonlinear class boundaries.howeverso far we haveonly described the linear case.consider what happens when an attribute trans-formationas described previouslyis applied to the training data before deter-mining the maximum margin hyperplane.recall that there are two problemswith the straightforward application ofsuch transformations to linear modelsinfeasible computational complexity on the one hand and overfitting on theother.with support vectorsoverfitting is unlikely to occur.the reason is that it isinevitably associated with instabilitychanging one or two instance vectors willmake sweeping changes to large sections ofthe decision boundary.but thexbyiiii aaa is support am page maximum margin hyperplane is relatively stableit only moves iftraininginstances are added or deleted that are support vectors and this is true evenin the high-dimensional space spanned by the nonlinear transformation.over-fitting is caused by too much flexibility in the decision boundary.the supportvectors are global representatives ofthe whole set oftraining pointsand thereare usually few ofthemwhich gives little flexibility.thus overfitting is unlikelyto occur.what about computational complexity? this is still a problem.suppose thatthe transformed space is a high-dimensional one so that the transformedsupport vectors and test instance have many components.according to the pre-ceding equationevery time an instance is classified its dot product with allsupport vectors must be calculated.in the high-dimensional space produced bythe nonlinear mapping this is rather expensive.obtaining the dot productinvolves one multiplication and one addition for each attributeand the numberofattributes in the new space can be huge.this problem occurs not only duringclassification but also during trainingbecause the optimization algorithms haveto calculate the same dot products very frequently.fortunatelyit turns out that it is possible to calculate the dot product beforethe nonlinear mapping is performedon the original attribute set.a high-dimensional version ofthe preceding equation is simplywhere nis chosen as the number offactors in the transformation in theexample we used earlier.ifyou expand the term anyou will find that itcontains all the high-dimensional terms that would have been involved ifthetest and training vectors were first transformed by including all products ofnfactors and the dot product was taken ofthe result.ifyou actually do the cal-culationyou will notice that some constant factors binomial coefficients are introduced.howeverthese do not matterit is the dimensionality ofthespace that concerns usthe constants merely scale the axes. because ofthismathematical equivalencethe dot products can be computed in the originallow-dimensional spaceand the problem becomes feasible.in implementationtermsyou take a software package for constrained quadratic optimization andevery time ai ais evaluated you evaluate aninstead.it s as simple as thatbecause in both the optimization and the classification algorithms these vectorsare only ever used in this dot product form.the training vectorsincluding thesupport vectorsand the test instance all remain in the original low-dimensionalspace throughout the calculations.the function ynwhich computes the dot product oftwo vectors xand yand raises the result to the power nis called a polynomial kernel.a good xbyiiin machine learning am page linear ofchoosing the value ofnis to start with linear model and incre-ment it until the estimated error ceases to improve.usuallyquite small valuessuffice.other kernel functions can be used instead to implement different nonlinearmappings.two that are often suggested are the radial basis function kerneland the sigmoid kernel.which one produces the best results depends on theapplicationalthough the differences are rarely large in practice.it is interestingto note that a support vector machine with the rbf kernel is simply a type ofneural network called an rbf networkwhich we describe laterand one withthe sigmoid kernel implements another type ofneural networka multilayerperceptron with one hidden layer described later.throughout this sectionwe have assumed that the training data is linearlyseparable either in the instance space or in the new space spanned by the non-linear mapping.it turns out that support vector machines can be generalized tothe case where the training data is not separable.this is accomplished by placingan upper bound on the preceding coefficients ai.unfortunatelythis parametermust be chosen by the userand the best setting can only be determined byexperimentation.alsoin all but trivial casesit is not possible to determine apriori whether the data is linearly separable or not.finallywe should mention that compared with other methods such as deci-sion tree learnerseven the fastest training algorithms for support vectormachines are slow when applied in the nonlinear setting.on the other handthey often produce very accurate classifiers because subtle and complex deci-sion boundaries can be obtained.support vector regressionthe concept ofa maximum margin hyperplane only applies to classification.howeversupport vector machine algorithms have been developed for numericprediction that share many ofthe properties encountered in the classificationcasethey produce a model that can usually be expressed in terms ofa fewsupport vectors and can be applied to nonlinear problems using kernel func-tions.as with regular support vector machineswe will describe the conceptsinvolved but do not attempt to describe the algorithms that actually perform thework.as with linear regressioncovered in section basic idea is to find afunction that approximates the training points well by minimizing the predic-tion error.the crucial difference is that all deviations up to a user-specifiedparameter eare simply discarded.alsowhen minimizing the errorthe risk ofoverfitting is reduced by simultaneously trying to maximize the flatness ofthefunction.another difference is that what is minimized is normally the am page tions absolute error instead ofthe squared error used in linear regression.there arehoweverversions ofthe algorithm that use the squared errorinstead.a user-specified parameter edefines a tube around the regression functionin which errors are ignoredfor linear support vector regressionthe tube is acylinder.ifall training points can fit within a tube ofwidth algorithmoutputs the function in the middle ofthe flattest tube that encloses them.inthis case the total perceived error is zero.figure shows a regressionproblem with one attributea numeric classand eight instances.in this case ewas set to the width ofthe tube around the regression function dotted lines is shows the outcome ofthe learning processwhen eis set to you can seethe wider tube makes it possible to learn aflatter function.the value ofecontrols how closely the function will fit the training data.toolarge a value will produce a meaningless predictor in the extreme the range ofclass values in the training datathe regression line ishorizontal and the algorithm just predicts the mean class value.on the otherhandfor small values ofethere may be no tube that encloses all the data.inthat case some training points will have nonzero errorand there will be a trade-offbetween the prediction error and the tube s flatness.in figure to and there is no tube ofwidth that encloses all the data.for the linear casethe support vector regression function can be writtenas with classificationthe dot product can be replaced by a kernel function fornonlinear problems.the support vectors are all those points that do not fallstrictly within the tube that isthe points outside the tube and on its border.as with classificationall other points have coefficient and can be deleted fromthe training data without changing the outcome ofthe learning process.in con-trast to the classification casethe aimay be negative.we have mentioned that as well as minimizing the errorthe algorithm simul-taneously tries to maximize the flatness ofthe regression function.in and there is a tube that encloses all the training datathe algo-rithm simply outputs the flattest tube that does so.howeverin figure is no tube with error a tradeoffis struck between the predictionerror and the tube s flatness.this tradeoffis controlled by enforcing an upperlimit con the absolute value ofthe coefficients ai.the upper limit restricts theinfluence ofthe support vectors on the shape ofthe regression function and isa parameter that the user must specify in addition to e.the larger cisthe moreclosely the function can fit the data.in the degenerate case the algorithmsimply performs least-absolute-error regression under the coefficient size con-xbiii aaa is support machine learning am page linear vector regressiona am page straintand all training instances become support vectors.converselyifeislarge enough that the tube can enclose all the datathe error becomes zerothereis no tradeoffto makeand the algorithm outputs the flattest tube that enclosesthe data irrespective ofthe value ofc.the kernel perceptronin section we introduced the perceptron algorithm for learning a linear clas-sifier.it turns out that the kernel trick can also be used to upgrade this algo-rithm to learn nonlinear decision boundaries.to see thiswe first revisit thelinear case.the perceptron algorithm repeatedly iterates through the trainingdata instance by instance and updates the weight vector every time one oftheseinstances is misclassified based on the weights learned so far.the weight vectoris updated simply by adding or subtracting the instance s attribute values to orfrom it.this means that the final weight vector is just the sum ofthe instancesthat have been misclassified.the perceptron makes its predictions based onwhetheris greater or less than zero where wiis the weight for the ith attribute and aithe corresponding attribute value ofthe instance that we wish to classify.insteadwe could useherea is the jth misclassified training instancea its ith attribute valueand yj is its class value or implement this we no longer keeptrack ofan explicit weight vectorwe simply store the instances misclassified sofar and use the preceding expression to make a prediction.it looks like we ve gained nothing in factthe algorithm is much slowerbecause it iterates through all misclassified training instances every time a pre-diction is made.howevercloser inspection ofthis formula reveals that it canbe expressed in terms ofdot products between instances.firstswap the sum-mation signs to yieldthe second sum is just a dot product between two instances and can be writtenasyjjj aa.yjajaiiij machine learning am page linear rings a bell! a similar expression for support vector machines enabled theuse ofkernels.indeedwe can apply exactly the same trick here and use a kernelfunction instead ofthe dot product.writing this function as k... givesin this way the perceptron algorithm can learn a nonlinear classifier simply bykeeping track ofthe instances that have been misclassified during the trainingprocess and using this expression to form each prediction.ifa separating hyperplane exists in the high-dimensional space implicitlycreated by the kernel functionthis algorithm will learn one.howeverit won tlearn the maximum margin hyperplane found by a support vector machine clas-sifier.this means that classification performance is usually worse.on the plussidethe algorithm is easy to implement and supports incremental learning.this classifier is called the kernel perceptron.it turns out that all sorts ofalgo-rithms for learning linear models can be upgraded by applying the kernel trickin a similar fashion.for examplelogistic regression can be turned into kernellogistic regression.the same applies to regression problemslinear regression canalso be upgraded using kernels.a drawback ofthese advanced methods forlinear and logistic regression are done in a straightforward manner isthat the solution is not sparse training instance contributes to the solu-tion vector.in support vector machines and the kernel perceptrononly someofthe training instances affect the solutionand this can make a big differenceto computational efficiency.the solution vector found by the perceptron algorithm depends greatly onthe order in which the instances are encountered.one way to make the algo-rithm more stable is to use all the weight vectors encountered during learningnot just the final oneletting them vote on a prediction.each weight vector con-tributes a certain number ofvotes.intuitivelythe correctness ofa weightvector can be measured roughly as the number ofsuccessive trials after its incep-tion in which it correctly classified subsequent instances and thus didn t have tobe changed.this measure can be used as the number ofvotes given to the weightvectorgiving an algorithm known as the voted perceptronthat performs almostas well as a support vector machine.note thatas previously mentionedthevarious weight vectors in the voted perceptron don t need to be stored explic-itlyand the kernel trick can be applied here too.multilayer perceptronsusing a kernel is not the only way to create a nonlinear classifier based on theperceptron.in factkernel functions are a recent development in machine yjkjj am page learning.previouslyneural network proponents used a different approach for nonlinear classificationthey connected many simple perceptron-like models in a hierarchical structure.this can represent nonlinear decision boundaries.section explained that a perceptron represents a hyperplane in instancespace.we mentioned there that it is sometimes described as an artificial neuron. ofcoursehuman and animal brains successfully undertake verycomplex classification tasks for exampleimage recognition.the functional-ity ofeach individual neuron in a brain is certainly not sufficient to performthese feats.how can they be solved by brain-like structures? the answer lies inthe fact that the neurons in the brain are massively interconnectedallowing aproblem to be decomposed into subproblems that can be solved at the neuronlevel.this observation inspired the development ofnetworks ofartificialneurons neural nets.consider the simple datasets in figure shows a two-dimensional instance space with four instances that have classes and by white and black dotsrespectively.no matter how you draw a straightline through this spaceyou will not be able to find one that separates all theblack points from all the white ones.in other wordsthe problem is not linearlyseparableand the simple perceptron algorithm will fail to generate a separat-ing hyperplane this two-dimensional instance space a hyperplane is just astraight line.the situation is different in figure and figure these problems are linearly separable.the same holds for figure shows two points in a one-dimensional instance space thecase ofone dimension the separating hyperplane degenerates to a separatingpoint.ifyou are familiar with propositional logicyou may have noticed that thefour situations in figure correspond to four types oflogical connectives.figure represents a logical xorwhere the class is ifand only ifexactlyone ofthe attributes has value represents logical andwherethe class is ifand only ifboth attributes have value repre-sents orwhere the class is only ifboth attributes have value notwhere the class is ifand only ifthe attribute has value the last three are linearly separablea perceptron can represent andorand not.indeedperceptrons for the corresponding datasets are shown infigure through respectively.howevera simple perceptron cannotrepresent xorbecause that is not linearly separable.to build a classifier forthis type ofproblem a single perceptron is not sufficientwe need several ofthem.figure shows a network with three perceptronsor unitslabeled aband c.the first two are connected to what is sometimes called the input layerofthe networkrepresenting the attributes in the data.as in a simple machine learning am page bias bias datasets and corresponding am page tronthe input layer has an additional constant input called the bias.howeverthe third unit does not have any connections to the input layer.its input con-sists ofthe output ofunits a and b or and another constant biasunit.these three units make up the hidden layerofthe multilayer perceptron.they are called hidden because the units have no direct connection to the envi-ronment.this layer is what enables the system to represent xor.you can verifythis by trying all four possible combinations ofinput signals.for exampleifattribute has value and value unit a will output b will output unit c will output is the correct answer.closer inspection ofthe behavior ofthe three unitsreveals that the first one represents orthe second represents nand with andand the third represents and.together they representthe expression and is precisely the definitionofxor.as this example illustratesany expression from propositional calculus can beconverted into a multilayer perceptronbecause the three connectives andorand not are sufficient for this and we have seen how each can be representedusing a perceptron.individual units can be connected together to form arbi-trarily complex expressions.hencea multilayer perceptron has the sameexpressive power assaya decision tree.in factit turns out that a two-layer per-ceptron counting the input layer is sufficient.in this caseeach unit in thehidden layer corresponds to a variant ofand a variant because we assumethat it may negate some ofthe inputs before forming the conjunction joinedby an or that is represented by a single unit in the output layer.in other wordseach node in the hidden layer has the same role as a leafin a decision tree or asingle rule in a set ofdecision rules.the big question is how to learn a multilayer perceptron.there are twoaspects to the problemlearning the structure ofthe network and learning theconnection weights.it turns out that there is a relatively simple algorithm fordetermining the weights given a fixed network structure.this algorithm is calledbackpropagationand is described in the next section.howeveralthough thereare many algorithms that attempt to identify network structurethis aspect ofthe problem is commonly solved through experimentation perhaps combinedwith a healthy dose ofexpert knowledge.sometimes the network can be separated into distinct modules that represent identifiable subtasks different components ofan object in an image recognition problemwhich opens up a way ofincorporating domain knowledge into the learningprocess.often a single hidden layer is all that is necessaryand an appropriatenumber ofunits for that layer is determined by maximizing the estimated machine learning am page linear that we have some data and seek a multilayer perceptron that is anaccurate predictor for the underlying classification problem.given a fixednetwork structurewe must determine appropriate weights for the connectionsin the network.in the absence ofhidden layersthe perceptron learning rulefrom section can be used to find suitable values.but suppose there arehidden units.we know what the output unit should predictand could adjustthe weights ofthe connections leading to that unit based on the perceptron rule.but the correct outputs for the hidden units are unknownso the rule cannotbe applied there.it turns out thatroughly speakingthe solution is to modify the weights ofthe connections leading to the hidden units based on the strength ofeach unit scontribution to the final prediction.there is a standard mathematical opti-mization algorithmcalled gradient descentwhich achieves exactly that.unfor-tunatelyit requires taking derivativesand the step function that the simpleperceptron uses to convert the weighted sum ofthe inputs into a predictionis not differentiable.we need to see whether the step function can be replacedwith something else.figure shows the step functionifthe input is smaller than zeroitoutputs zerootherwiseit outputs one.we want a function that is similar inshape but differentiable.a commonly used replacement is shown in neural networks terminology it is called the sigmoidfunctionand itis defined bywe encountered it in section when we described the logit transform usedin logistic regression.in factlearning a multilayer perceptron is closely relatedto logistic regression.to apply the gradient descent procedurethe error function the thing thatis to be minimized by adjusting the weights must also be differentiable.thenumber ofmisclassifications measured by the discrete loss mentioned insection does not fulfill this criterion.insteadmultilayer perceptrons areusually trained by minimizing the squared error ofthe network s outputessentially treating it as an estimate ofthe class probability.other loss func-tions are also applicable.for exampleifthe likelihood is used instead ofthe squared errorlearning a sigmoid-based perceptron is identical to logisticregression.we work with the squared-error loss function because it is most widely used.for a single training instanceit am page machine learning schemeswhere fx is the network s prediction obtained from the output unit and yisthe instance s class label this caseit is assumed to be either or is included just for convenienceand will drop out when we start versus sigmoida step function and sigmoid am page linear descent exploits information given by the derivative ofthe functionthat is to be minimized in this casethe error function.as an examplecon-sider a hypothetical error function that happens to be identical to figure x-axis represents a hypothetical parameter that is to be opti-mized.the derivative is simply crucial observation is thatbased on the derivativewe can figure out the slope ofthe function at any par-ticular point.ifthe derivative is negative the function slopes downward to therightifit is positiveit slopes downward to the leftand the size ofthe deriva-tive determines how steep the decline is.gradient descent is an iterative optimization procedure that uses this information to adjust a function s parameters.it takes the value ofthe derivativemultiplies it by a small constantcalled the learning rateand subtracts the result from the current parametervalue.this is repeated for the new parameter valueand so onuntil a minimumis reached.returning to the exampleassume that the learning rate is set to and thecurrent parameter value xis derivative is double this at this point.multiplying by the learning rate yields subtracting this from gives becomes the new parameter value.repeating the process for so on.the little crosses in figure show the valuesencountered in this process.the process stops once the change in parametervalue becomes too small.in the example this happens when the valueapproaches value corresponding to the location on the x-axis where theminimum ofthe hypothetical error function is descent using the error function am page the learning rate determines the step size and hence how quickly the searchconverges.ifit is too large and the error function has several minimathe searchmay overshoot and miss a minimum entirelyor it may oscillate wildly.ifit istoo smallprogress toward the minimum may be slow.note that gradientdescent can only find a localminimum.ifthe function has several minima and error functions for multilayer perceptrons usually have many it may notfind the best one.this is a significant drawback ofstandard multilayer percep-trons compared withfor examplesupport vector machines.to use gradient descent to find the weights ofa multilayer perceptronthederivative ofthe squared error must be determined with respect to each param-eter that iseach weight in the network.let s start with a simple perceptronwithout a hidden layer.differentiating the preceding error function with respectto a particular weight wiyieldsherefx is the perceptron s output and xis the weighted sum ofthe inputs.to compute the second factor on the right-hand sidethe derivative ofthesigmoid function fx is needed.it turns out that this has a particularly simpleform that can be written in terms offx itselfwe use f to denote this derivative.but we seek the derivative with respectto winot x.becausethe derivative offx with respect to wiisplugging this back into the derivative ofthe error function yieldsthis expression gives all that is needed to calculate the change ofweight wicaused by a particular example vector aextended by to represent the biasasexplained previously.having repeated this computation for each traininginstancewe add up the changes associated with a particular weight wimulti-ply by the learning rateand subtract the result from wi s current value.dedwyfxfxaii- machine learning am page linear far so good.but all this assumes that there is no hidden layer.with ahidden layerthings get a little trickier.suppose fxi is the output ofthe ithhidden unitwijis the weight ofthe connection from input j to the ith hiddenunitand wiis the weight ofthe ith hidden unit to the output unit.the situa-tion is depicted in figure beforefx is the output ofthe single unit inthe output layer.the update rule for the weights wiis essentially the same asaboveexcept that aiis replaced by the output ofthe ith hidden unithoweverto update the weights wijthe corresponding derivatives must be cal-culated.applying the chain rule givesthe first two factors are the same as in the previous equation.to compute thethird factordifferentiate further.becausededwdedxdxdwyfxfxdxdwijijij- akinput perceptron with a hidden am page furthermorethis means that we are finished.putting everything together yields an equationfor the derivative ofthe error function with respect to the weights wijas beforewe calculate this value for every training instanceadd up the changesassociated with a particular weight wijmultiply by the learning rateand sub-tract the outcome from the current value ofwij.this derivation applies to a perceptron with one hidden layer.ifthere are twohidden layersthe same strategy can be applied a second time to update theweights pertaining to the input connections ofthe first hidden layerpropagat-ing the error from the output unit through the second hidden layer to the firstone.because ofthis error propagation mechanismthis version ofthe genericgradient descent strategy is called backpropagation.we have tacitly assumed that the network s output layer has just one unitwhich is appropriate for two-class problems.for more than two classesa sep-arate network could be learned for each class that distinguishes it from theremaining classes.a more compact classifier can be obtained from a singlenetwork by creating an output unit for each classconnecting every unit in thehidden layer to every output unit.the squared error for a particular traininginstance is the sum ofsquared errors taken over all output units.the same tech-nique can be applied to predict several targetsor attribute valuessimultane-ously by creating a separate output unit for each one.intuitivelythis may givebetter predictive accuracy than building a separate classifier for each class attrib-ute ifthe underlying learning tasks are in some way related.we have assumed that weights are only updated after all training instanceshave been fed through the network and all the corresponding weight changeshave been accumulated.this is batchlearningbecause all the training data isprocessed together.but exactly the same formulas can be used to update theweights incrementally after each training instance has been processed.this iscalled stochastic backpropagationbecause the overall error does not necessarilydecrease after every update and there is no guarantee that it will converge to adedwyfxfxwfxaijiii- machine learning am page linear can be used for online learningin which new data arrives in acontinuous stream and every training instance is processed just once.in bothvariants ofbackpropagationit is often helpful to standardize the attributes tohave zero mean and unit standard deviation.before learning startseach weightis initialized to a smallrandomly chosen value based on a normal distributionwith zero mean.like any other learning schememultilayer perceptrons trained with back-propagation may suffer from overfitting especially ifthe network is muchlarger than what is actually necessary to represent the structure ofthe underly-ing learning problem.many modifications have been proposed to alleviate this.a very simple onecalled early stoppingworks like reduced-error pruning inrule learnersa holdout set is used to decide when to stop performing furtheriterations ofthe backpropagation algorithm.the error on the holdout set ismeasured and the algorithm is terminated once the error begins to increasebecause that indicates overfitting to the training data.another methodcalled weight decayadds to the error function a penalty term that consists ofthe squared sum ofall weights in the network.this attempts to limit theinfluence ofirrelevant connections on the network s predictions by penalizinglarge weights that do not contribute a correspondingly large reduction in theerror.although standard gradient descent is the simplest technique for learning theweights in a multilayer perceptronit is by no means the most efficient one.inpracticeit tends to be rather slow.a trick that often improves performance isto include a momentumterm when updating weightsadd to the new weightchange a small proportion ofthe update value from the previous iteration.thissmooths the search process by making changes in direction less abrupt.moresophisticated methods use information obtained from the second derivative ofthe error function as wellthey can converge much more quickly.howevereventhose algorithms can be very slow compared with other methods ofclassifica-tion learning.a serious disadvantage ofmultilayer perceptrons that contain hidden unitsis that they are essentially opaque.there are several techniques that attempt toextract rules from trained neural networks.howeverit is unclear whether theyoffer any advantages over standard rule learners that induce rule sets directlyfrom data especially considering that this can generally be done much morequickly than learning a multilayer perceptron in the first place.although multilayer perceptrons are the most prominent type ofneuralnetworkmany others have been proposed.multilayer perceptrons belong to aclass ofnetworks called feedforward networksbecause they do not contain anycycles and the network s output depends only on the current input instance.recurrentneural networks do have cycles.computations derived from earlierinput are fed back into the networkwhich gives them a kind am page radial basis function networksanother popular type offeedforward network is the radial basis function has two layersnot counting the input layerand differs from a multilayer perceptron in the way that the hidden units perform computations.each hidden unit essentially represents a particular point in input spaceand itsoutputor activationfor a given instance depends on the distance between itspoint and the instance which is just another point.intuitivelythe closer thesetwo pointsthe stronger the activation.this is achieved by using a nonlineartransformation function to convert the distance into a similarity measure.abell-shaped gaussian activation functionwhose width may be different for eachhidden unitis commonly used for this purpose.the hidden units are calledrbfs because the points in instance space for which a given hidden unit pro-duces the same activation form a hypersphere or hyperellipsoid.in a multilayerperceptronthis is a hyperplane.the output layer ofan rbf network is the same as that ofa multilayer per-ceptronit takes a linear combination ofthe outputs ofthe hidden units and in classification problems pipes it through the sigmoid function.the parameters that such a network learns are the centers and widths ofthe rbfs and the weights used to form the linear combination ofthe outputsobtained from the hidden layer.a significant advantage over multilayer per-ceptrons is that the first set ofparameters can be determined independently ofthe second set and still produce accurate classifiers.one way to determine the first set ofparameters is to use clusteringwithoutlooking at the class labels ofthe training instances at all.the simple k-meansclustering algorithm described in section can be appliedclustering eachclass independently to obtain kbasis functions for each class.intuitivelytheresulting rbfs represent prototype instances.then the second set ofparame-ters can be learnedkeeping the first parameters fixed.this involves learning alinear model using one ofthe techniques we have discussed or logis-tic regression.ifthere are far fewer hidden units than training instancesthiscan be done very quickly.a disadvantage ofrbf networks is that they give every attribute the sameweight because all are treated equally in the distance computation.hence theycannot deal effectively with irrelevant attributes in contrast to multilayer per-ceptrons.support vector machines share the same problem.in factsupportvector machines with gaussian kernels rbf kernels are a particular typeofrbf networkin which one basis function is centered on every traininginstanceand the outputs are combined linearly by computing the maximummargin hyperplane.this has the effect that only some rbfs have a nonzeroweight the ones that represent the support machine learning pm page vector machines originated from research in statistical learning theoryvapnik a good starting point for exploration is a tutorial by general descriptionincluding generalization to the case in which thedata is not linearly separablehas been published by cortes and vapnik have introduced the standard version ofsupport vector regressionsch lkopfet present a different version that has one parameter insteadoftwo.smola and sch provide an extensive tutorial on supportvector regression.the kernel perceptron is due to freund and schapire and shawe-taylor provide a nice introduction to support vectormachines and other kernel-based methodsincluding the optimization theoryunderlying the support vector learning algorithms.we have barely skimmed thesurface ofthese learning schemesmainly because advanced mathematics liesjust beneath.the idea ofusing kernels to solve nonlinear problems has beenapplied to many algorithmsfor exampleprincipal components analysisdescribed in section kernel is essentially a similarity function withcertain mathematical propertiesand it is possible to define kernel functionsover all sorts ofstructures for examplesetsstringstreesand probability distributions.shawe-taylor and cristianini cover kernel-based learningin detail.there is extensive literature on neural networksand bishop providesan excellent introduction to both multilayer perceptrons and rbf networks.interest in neural networks appears to have declined since the arrival ofsupportvector machinesperhaps because the latter generally require fewer parametersto be tuned to achieve the same greater accuracy.howevermultilayer per-ceptrons have the advantage that they can learn to ignore irrelevant attributesand rbf networks trained using k-means can be viewed as a quick-and-dirtymethod for finding a nonlinear learningin section we saw how the nearest-neighbor rule can be used to implementa basic form ofinstance-based learning.there are several practical problemswith this simple method.firstit tends to be slow for large training setsbecausethe entire set must be searched for each test instance unless sophisticated datastructures such as kd-trees or ball trees are used.secondit performs badly withnoisy databecause the class ofa test instance is determined by its single nearestneighbor without any averaging to help to eliminate noise.thirdit performsbadly when different attributes affect the outcome to different extents in pm page extreme casewhen some attributes are completely irrelevant because allattributes contribute equally to the distance formula.fourthit does notperform explicit generalizationalthough we intimated in section illus-trated in figure that some instance-based learning systems do indeedperform explicit generalization.reducing the number of exemplarsthe plain nearest-neighbor rule stores a lot ofredundant exemplarsit is almostalways completely unnecessary to save all the examples seen so far.a simplevariant is to classify each example with respect to the examples already seen andto save only ones that are misclassified.we use the term exemplarsto refer tothe already-seen instances that are used for classification.discarding correctlyclassified instances reduces the number ofexemplars and proves to be an effec-tive way to prune the exemplar database.ideallyonly a single exemplar is storedfor each important region ofthe instance space.howeverearly in the learningprocess examples may be discarded that later turn out to be importantpossi-bly leading to some decrease in predictive accuracy.as the number ofstoredinstances increasesthe accuracy ofthe model improvesand so the systemmakes fewer mistakes.unfortunatelythe strategy ofonly storing misclassified instances does notwork well in the face ofnoise.noisy examples are very likely to be misclassifiedand so the set ofstored exemplars tends to accumulate those that are least useful.this effect is easily observed experimentally.thus this strategy is only a step-ping-stone on the way toward more effective instance-based learners.pruning noisy exemplarsnoisy exemplars inevitably lower the performance ofany nearest-neighborscheme that does not suppress them because they have the effect ofrepeatedlymisclassifying new instances.there are two ways ofdealing with this.one is tolocateinstead ofthe single nearest neighborthe knearest neighbors for somepredetermined constant kand assign the majority class to the unknowninstance.the only problem here is determining a suitable value ofk.plainnearest-neighbor learning corresponds to k more noisethe greater theoptimal value ofk.one way to proceed is to perform cross-validation tests withdifferent values and choose the best.although this is expensive in computationtimeit often yields excellent predictive performance.a second solution is to monitor the performance ofeach exemplar that isstored and discard ones that do not perform well.this can be done by keepinga record ofthe number ofcorrect and incorrect classification decisions that eachexemplar makes.two predetermined thresholds are set on the success ratio.when an exemplar s performance drops below the lower oneit is deleted machine learning am page exemplar set.ifits performance exceeds the upper thresholdit is used forpredicting the class ofnew instances.ifits performance lies between the twoitis not used for prediction butwhenever it is the closest exemplar to the newinstance thus would have been used for prediction ifits performancerecord had been good enoughits success statistics are updated as though ithad been used to classify that new instance.to accomplish thiswe use the confidence limits on the success probabilityofa bernoulli process that we derived in section that we took a certainnumber ofsuccesses sout ofa total number oftrials nas evidence on whichto base confidence limits on the true underlying success rate p.given a certainconfidence level can calculate upper and lower bounds and sure that plies between them.to apply this to the problem ofdeciding when to accept a particular exem-plarsuppose that it has been used ntimes to classify other instances and that softhese have been successes.that allows us to estimate boundsat a particularconfidence levelon the true success rate ofthis exemplar.now suppose that theexemplar s class has occurred ctimes out ofa total number noftraininginstances.this allows us to estimate bounds on the default success ratethat isthe probability ofsuccessfully classifying an instance ofthis class without anyinformation about other instances.we insist that the lower confidence boundon its success rate exceeds the upper confidence bound on the default successrate.we use the same method to devise a criterion for rejecting a poorly per-forming exemplarrequiring that the upper confidence bound on its success ratelies below the lower confidence bound on the default success rate.with a suitable choice ofthresholdsthis scheme works well.in a particularimplementationcalled instance-based learner version confidencelevel is used to determine acceptancewhereas a level is usedfor rejection.the lower percentage figure produces a wider confidence intervalwhich makes a more stringent criterion because it is harder for the lower boundofone interval to lie above the upper bound ofthe other.the criterion foracceptance is more stringent than that for rejectionmaking it more difficult foran instance to be accepted.the reason for a less stringent rejection criterion isthat there is little to be lost by dropping instances with only moderately poorclassification accuraciesthey will probably be replaced by a similar instancelater.using these thresholds the method has been found to improve the per-formance ofinstance-based learning andat the same timedramatically reducethe number ofexemplars particularly noisy exemplars that are stored.weighting attributesthe euclidean distance functionmodified to scale all attribute values tobetween and well in domains in which the attributes are equally am page evant to the outcome.such domainshoweverare the exception rather than therule.in most domains some attributes are irrelevantand some relevant onesare less important than others.the next improvement in instance-based learn-ing is to learn the relevance ofeach attribute incrementally by dynamicallyupdating feature weights.in some schemesthe weights are class specific in that an attribute may bemore important to one class than to another.to cater for thisa description isproduced for each class that distinguishes its members from members ofallother classes.this leads to the problem that an unknown test instance may beassigned to several different classesor to no classes at all a problem that is alltoo familiar from our description ofrule induction.heuristic solutions areapplied to resolve these situations.the distance metric incorporates the feature weights eachdimensionin the case ofclass-specific feature weightsthere will be a separate set ofweightsfor each class.all attribute weights are updated after each training instance is classifiedandthe most similar exemplar the most similar exemplar ofeach class is usedas the basis for updating.call the training instance xand the most similar exem-plar y.for each attribute ithe difference a measure ofthe contribu-tion ofthat attribute to the decision.ifthis difference is small then the attributecontributes positivelywhereas ifit is large it may contribute negatively.thebasic idea is to update the ith weight on the basis ofthe size ofthis differenceand whether the classification was indeed correct.ifthe classification is correctthe associated weight is increased and ifit is incorrect it is decreasedthe amountofincrease or decrease being governed by the size ofthe differencelarge ifthedifference is small and vice versa.the weight change is generally followed by arenormalization step.a simpler strategywhich may be equally effectiveis toleave the weights alone ifthe decision is correct and ifit is incorrect to increasethe weights for those attributes that differ most greatlyaccentuating the differ-ence.details ofthese weight adaptation algorithms are described by aha good test ofwhether an attribute weighting method works is to add irrel-evant attributes to all examples in a dataset.ideallythe introduction ofirrele-vant attributes should not affect either the quality ofpredictions or the numberofexemplars stored.generalizing exemplarsgeneralized exemplars are rectangular regions ofinstance spacecalled hyper-rectanglesbecause they are high-dimensional.when classifying new instances machine learning am page necessary to modify the distance function as described below to allow the dis-tance to a hyperrectangle to be computed.when a new exemplar is classifiedcorrectlyit is generalized by simply merging it with the nearest exemplar ofthesame class.the nearest exemplar may be either a single instance or a hyperrec-tangle.in the former casea new hyperrectangle is created that covers the oldand the new instance.in the latterthe hyperrectangle is enlarged to encompassthe new instance.finallyifthe prediction is incorrect and it was a hyperrec-tangle that was responsible for the incorrect predictionthe hyperrectangle sboundaries are altered so that it shrinks away from the new instance.it is necessary to decide at the outset whether overgeneralization caused bynesting or overlapping hyperrectangles is to be permitted or not.ifit is to beavoideda check is made before generalizing a new example to see whether anyregions offeature space conflict with the proposed new hyperrectangle.iftheydothe generalization is aborted and the example is stored verbatim.note thatoverlapping hyperrectangles are precisely analogous to situations in which thesame example is covered by two or more rules in a rule set.in some schemes generalized exemplars can be nested in that they may becompletely contained within one another in the same way thatin some repre-sentationsrules may have exceptions.to do thiswhenever an example is incor-rectly classifieda fallback heuristic is tried using the second nearest neighbor ifit would have produced a correct prediction in a further attempt to performgeneralization.this second-chance mechanism promotes nesting ofhyperrec-tangles.ifan example falls within a rectangle ofthe wrong class that alreadycontains an exemplar ofthe same classthe two are generalized into a new exception hyperrectangle nested within the original one.for nested general-ized exemplarsthe learning process frequently begins with a small number ofseed instances to prevent all examples ofthe same class from being generalizedinto a single rectangle that covers most ofthe problem space.distance functions for generalized exemplarswith generalized exemplars is necessary to generalize the distance function tocompute the distance from an instance to a generalized exemplaras well as toanother instance.the distance from an instance to a hyperrectangle is definedto be zero ifthe point lies within the hyperrectangle.the simplest way to gen-eralize the distance function to compute the distance from an exterior point toa hyperrectangle is to choose the closest instance within it and measure the dis-tance to that.howeverthis reduces the benefit ofgeneralization because it rein-troduces dependence on a particular single example.more preciselywhereasnew instances that happen to lie within a hyperrectangle continue to benefitfrom generalizationsones that lie outside do not.it might be better to use thedistance from the nearest part ofthe hyperrectangle am page figure shows the implicit boundaries that are formed between two rec-tangular classes ifthe distance metric is adjusted to measure distance to thenearest point ofa rectangle.even in two dimensions the boundary contains atotal ofnine regions are numbered for easy identificationthe situationwill be more complex for higher-dimensional hyperrectangles.proceeding from the lower leftthe first regionin which the boundary islinearlies outside the extent ofboth rectangles to the left ofboth borders ofthe larger one and below both borders ofthe smaller one.the second is withinthe extent ofone rectangle to the right ofthe leftmost border ofthe largerrectangle but outside that ofthe other below both borders ofthe smallerone.in this region the boundary is parabolicbecause the locus ofa point thatis the same distance from a given line as from a given point is a machine learning boundary between two rectangular am page region is where the boundary meets the lower border ofthe larger rec-tangle when projected upward and the left border ofthe smaller one when pro-jected to the right.the boundary is linear in this regionbecause it is equidistantfrom these two borders.the fourth is where the boundary lies to the right ofthe larger rectangle but below the bottom ofthat rectangle.in this case theboundary is parabolic because it is the locus ofpoints equidistant from the lowerright corner ofthe larger rectangle and the left side ofthe smaller one.the fifthregion lies between the two rectangleshere the boundary is vertical.the patternis repeated in the upper right part ofthe diagramfirst parabolicthen linearthen parabolic this particular parabola is almost indistinguishablefrom a straight lineand finally linear as the boundary finally escapes from thescope ofboth rectangles.this simple situation certainly defines a complex boundary! ofcourseit isnot necessary to represent the boundary explicitlyit is generated implicitly bythe nearest-neighbor calculation.neverthelessthe solution is still not a verygood one.whereas taking the distance from the nearest instance within a hyper-rectangle is overly dependent on the position ofthat particular instancetakingthe distance to the nearest point ofthe hyperrectangle is overly dependent onthat corner ofthe rectangle the nearest example might be a long way from thecorner.a final problem concerns measuring the distance to hyperrectangles thatoverlap or are nested.this complicates the situation because an instance mayfall within more than one hyperrectangle.a suitable heuristic for use in this caseis to choose the class ofthe most specific hyperrectangle containing the instancethat isthe one covering the smallest area ofinstance space.whether or not overlap or nesting is permittedthe distance function shouldbe modified to take account ofboth the observed prediction accuracy ofexem-plars and the relative importance ofdifferent featuresas described in the pre-ceding sections on pruning noisy exemplars and attribute weighting.generalized distance functionsthere are many different ways ofdefining a distance functionand it is hard tofind rational grounds for any particular choice.an elegant solution is to con-sider one instance being transformed into another through a sequence ofpre-defined elementary operations and to calculate the probability ofsuch asequence occurring ifoperations are chosen randomly.robustness is improvedifall possible transformation paths are consideredweighted by their probabil-itiesand the scheme generalizes naturally to the problem ofcalculating the distance between an instance and a set ofother instances by considering trans-formations to all instances in the set.through such a technique it is possible toconsider each instance as exerting a sphere ofinfluence but a sphere with am page boundaries rather than the hard-edged cutoffimplied by the k-nearest-neighborrulein which any particular example is either in or out ofthe decision.with such a measuregiven a test instance whose class is unknownits dis-tance to the set ofall training instances in each class in turn is calculatedandthe closest class is chosen.it turns out that nominal and numeric attributes canbe treated in a uniform manner within this transformation-based approach bydefining different transformation setsand it is even possible to take account ofunusual attribute types such as degrees ofarc or days ofthe weekwhich aremeasured on a circular scale.discussionnearest-neighbor methods gained popularity in machine learning through thework ofaha showed thatwhen combined with noisy exemplarpruning and attribute weightinginstance-based learning performs well in com-parison with other methods.it is worth noting that although we have describedit solely in the context ofclassification rather than numeric prediction prob-lemsit applies to these equally wellpredictions can be obtained by combiningthe predicted values ofthe knearest neighbors and weighting them by distance.viewed in instance spacethe standard rule- and tree-based representationsare only capable ofrepresenting class boundaries that are parallel to the axesdefined by the attributes.this is not a handicap for nominal attributesbut itis for numeric ones.non-axis-parallel class boundaries can only be approxi-mated by covering the region above or below the boundary with several axis-parallel rectanglesthe number ofrectangles determining the degree ofapproximation.in contrastthe instance-based method can easily representarbitrary linear boundaries.even with just one example ofeach oftwo classesthe boundary implied by the nearest-neighbor rule is a straight line ofarbitraryorientationnamely the perpendicular bisector ofthe line joining the examples.plain instance-based learning does not produce explicit knowledge repre-sentations except by selecting representative exemplars.howeverwhen com-bined with exemplar generalizationa set ofrules can be obtained that may becompared with those produced by other machine learning schemes.the rulestend to be more conservative because the distance metricmodified to incor-porate generalized exemplarscan be used to process examples that do not fallwithin the rules.this reduces the pressure to produce rules that cover the wholeexample space or even all ofthe training examples.on the other handthe incre-mental nature ofmost instance-based learning methods means that rules areformed eagerlyafter only part ofthe training set has been seenand thisinevitably reduces their quality.we have not given precise algorithms for variants ofinstance-based learningthat involve generalization because it is not clear what the best way to do machine learning am page is.salzberg suggested that generalization with nested exem-plars can achieve a high degree ofclassification ofaccuracy on a variety ofdif-ferent problemsa conclusion disputed by wettschereck and dietterich argued that these results were fortuitous and did not hold in otherdomains.martin explored the idea that it is not the generalization butthe overgeneralization that occurs when hyperrectangles nest or overlap that isresponsible for poor performance and demonstrated that ifnesting and over-lapping are avoided excellent results are achieved in a large number ofdomains.the generalized distance function based on transformations is described bycleary and trigg generalization is a rare example ofa learning strategy in which thesearch proceeds from specific to general rather than from general to specific asin the case oftree or rule induction.there is no particular reason why specific-to-general searching should necessarily be handicapped by forcing the examplesto be considered in a strictly incremental fashionand batch-orientedapproaches exist that generate rules using a basic instance-based approach.moreoverit seems that the idea ofproducing conservative generalizations andcoping with instances that are not covered by choosing the closest generaliza-tion is an excellent one that will eventually be extended to ordinary tree andrule predictiontrees that are used for numeric prediction are just like ordinary decision trees except that at each leafthey store either a class value that represents theaverage value ofinstances that reach the leafin which case the tree is called aregression treeor a linear regression model that predicts the class value ofinstances that reach the leafin which case it is called a model tree.in whatfollows we will describe model trees because regression trees are really a specialcase.regression and model trees are constructed by first using a decision treeinduction algorithm to build an initial tree.howeverwhereas most decisiontree algorithms choose the splitting attribute to maximize the information gainit is appropriate for numeric prediction to instead minimize the intrasubsetvariation in the class values down each branch.once the basic tree has beenformedconsideration is given to pruning the tree back from each leafjust aswith ordinary decision trees.the only difference between regression tree andmodel tree induction is that for the lattereach node is replaced by a regressionplane instead ofa constant value.the attributes that serve to define that regres-sion are precisely those that participate in decisions in the subtree that will beprunedthat isin nodes underneath the current am page following an extensive description ofmodel treeswe briefly explain how togenerate rules from model treesand then describe another approach to numericprediction locally weighted linear regression.whereas model trees derive from the basic divide-and-conquer decision tree methodologylocally weightedregression is inspired by the instance-based methods for classification that wedescribed in the previous section.like instance-based learningit performs all learning at prediction time.although locally weighted regression resemblesmodel trees in that it uses linear regression to fit models locally to particularareas ofinstance spaceit does so in quite a different way.model treeswhen a model tree is used to predict the value for a test instancethe tree is fol-lowed down to a leafin the normal wayusing the instance s attribute values tomake routing decisions at each node.the leafwill contain a linear model basedon some ofthe attribute valuesand this is evaluated for the test instance toyield a raw predicted value.instead ofusing this raw value directlyhoweverit turns out to be beneficialto use a smoothing process to compensate for the sharp discontinuities that willinevitably occur between adjacent linear models at the leaves ofthe pruned tree.this is a particular problem for models constructed from a small number oftraining instances.smoothing can be accomplished by producing linear modelsfor each internal nodeas well as for the leavesat the time the tree is built.thenonce the leafmodel has been used to obtain the raw predicted value for a testinstancethat value is filtered along the path back to the rootsmoothing it ateach node by combining it with the value predicted by the linear model for thatnode.an appropriate smoothing calculation iswhere p is the prediction passed up to the next higher nodepis the predictionpassed to this node from belowqis the value predicted by the model at thisnodenis the number oftraining instances that reach the node belowand kisa smoothing constant.experiments show that smoothing substantially increasesthe accuracy ofpredictions.exactly the same smoothing process can be accomplished by incorporatingthe interior models into each leafmodel after the tree has been built.thenduring the classification processonly the leafmodels are used.the disadvan-tage is that the leafmodels tend to be larger and more difficult to comprehendbecause many coefficients that were previously zero become nonzero when theinterior nodes models are incorporated.pnpkqnk machine learning am page the treethe splitting criterion is used to determine which attribute is the best to splitthat portion tofthe training data that reaches a particular node.it is based ontreating the standard deviation ofthe class values in tas a measure ofthe errorat that node and calculating the expected reduction in error as a result oftestingeach attribute at that node.the attribute that maximizes the expected errorreduction is chosen for splitting at the node.the expected error reductionwhich we call sdr for standard deviationreductionis calculated bywhere the sets that result from splitting the node according to thechosen attribute.the splitting process terminates when the class values ofthe instances thatreach a node vary very slightlythat iswhen their standard deviation is only asmall fraction than ofthe standard deviation ofthe originalinstance set.splitting also terminates when just a few instances remainsay fouror fewer.experiments show that the results obtained are not very sensitive tothe exact choice ofthese thresholds.pruning the treeas noted previouslya linear model is needed for each interior node ofthe treenot just at the leavesfor use in the smoothing process.before pruninga modelis calculated for each node ofthe unpruned tree.the model takes the formwhere attribute values.the weights calculatedusing standard regression.howeveronly the attributes that are tested in thesubtree below this node are used in the regressionbecause the other attributesthat affect the predicted value have been taken into account in the tests that leadto the node.note that we have tacitly assumed that attributes are numericwedescribe the handling ofnominal attributes in the next section.the pruning procedure makes use ofan estimateat each nodeoftheexpected error for test data.firstthe absolute difference between the predictedvalue and the actual class value is averaged over each ofthe training instancesthat reach that node.because the tree has been built expressly for this datasetthis average will underestimate the expected error for unseen cases.to com-pensateit is multiplied by the factor nis the number oftraining instances that reach the node and nis the number ofparameters in thelinear model that gives the class value at that am page the expected error for test data at a node is calculated as described previ-ouslyusing the linear model for prediction.because ofthe compensation factornnn-nit may be that the linear model can be further simplified bydropping terms to minimize the estimated error.dropping a term decreases themultiplication factorwhich may be enough to offset the inevitable increase inaverage error over the training instances.terms are dropped one by onegreed-ilyas long as the error estimate decreases.finallyonce a linear model is in place for each interior nodethe tree ispruned back from the leaves as long as the expected estimated error decreases.the expected error for the linear model at that node is compared with theexpected error from the subtree below.to calculate the latterthe error fromeach branch is combined into a singleoverall value for the node by weightingthe branch by the proportion ofthe training instances that go down it and com-bining the error estimates linearly using those weights.nominal attributesbefore constructing a model treeall nominal attributes are transformed intobinary variables that are then treated as numeric.for each nominal attributethe average class value corresponding to each possible value in the enumerationis calculated from the training instancesand the values in the enumeration aresorted according to these averages.thenifthe nominal attribute has kpossi-ble valuesit is replaced by synthetic binary attributesthe ith being ifthe value is one ofthe first iin the ordering and otherwise.thus all splits arebinarythey involve either a numeric attribute or a synthetic binary onetreatedas a numeric attribute.it is possible to prove analytically that the best split at a node for a nominalvariable with kvalues is one ofthe positions obtained by ordering the average class values for each value ofthe attribute.this sorting operationshould really be repeated at each nodehoweverthere is an inevitable increasein noise because ofsmall numbers ofinstances at lower nodes in the tree some cases nodes may not represent all values for some attributesand not much is lost by performing the sorting just oncebefore starting to build amodel tree.missing valuesto take account ofmissing valuesa modification is made to the sdr formula.the final formulaincluding the missing value compensationissdr machine learning am page mis the number ofinstances without missing values for that attributeand tis the set ofinstances that reach this node.tland trare sets that resultfrom splitting on this attribute because all tests on attributes are now binary.when processing both training and test instancesonce an attribute is selectedfor splitting it is necessary to divide the instances into subsets according to theirvalue for this attribute.an obvious problem arises when the value is missing.an interesting technique called surrogate splittinghas been developed to handlethis situation.it involves finding another attribute to split on in place oftheoriginal one and using it instead.the attribute is chosen as the one most highlycorrelated with the original attribute.howeverthis technique is both complexto implement and time consuming to execute.a simpler heuristic is to use the class value as the surrogate attributein thebeliefthata priorithis is the attribute most likely to be correlated with the onebeing used for splitting.ofcoursethis is only possible when processing thetraining setbecause for test examples the class is unknown.a simple solutionfor test examples is simply to replace the unknown attribute value with theaverage value ofthat attribute for the training examples that reach the node which has the effectfor a binary attributeofchoosing the most populoussubnode.this simple approach seems to work well in practice.let s consider in more detail how to use the class value as a surrogate attrib-ute during the training process.we first deal with all instances for which thevalue ofthe splitting attribute is known.we determine a threshold for splittingin the usual wayby sorting the instances according to its value andfor eachpossible split pointcalculating the sdr according to the preceding formulachoosing the split point that yields the greatest reduction in error.only theinstances for which the value ofthe splitting attribute is known are used todetermine the split point.then we divide these instances into the two sets land raccording to thetest.we determine whether the instances in lor r have the greater average classvalueand we calculate the average ofthese two averages.thenan instance forwhich this attribute value is unknown is placed into lor r according to whetherits class value exceeds this overall average or not.ifit doesit goes into whicheverofland rhas the greater average class valueotherwiseit goes into the onewith the smaller average class value.when the splitting stopsall the missingvalues will be replaced by the average values ofthe corresponding attributes ofthe training instances reaching the leaves.pseudocode for model tree inductionfigure gives pseudocode for the model tree algorithm we have described.the two main parts are creating a tree by successively splitting am page machine learning schemesmakemodeltree sd sdinstances for each k-valued nominal attribute convert into synthetic binary attributes root newnode root.instances instances splitroot pruneroot printtreeroot splitnode if sizeofnode.instances or sdnode.instances node.type leaf else node.type interior for each attribute for all possible split positions of the attribute calculate the attributes sdr node.attribute attribute with maximum sdr splitnode.left splitnode.right prunenode if node interior then prunenode.leftchild prunenode.rightchild node.model linearregressionnode if subtreeerrornode errornode then node.type leaf subtreeerrornode l node.left r node.right if node interior then return sizeofr.instancessubtreeerrorrsizeofnode.instances else return errornode figure for model tree am page splitand pruning it from the leaves upwardperformed by prune.the nodedata structure contains a type flag indicating whether it is an internal node ora leafpointers to the left and right childthe set ofinstances that reach thatnodethe attribute that is used for splitting at that nodeand a structure repre-senting the linear model for the node.the sdfunction called at the beginning ofthe main program and again at the beginning ofsplitcalculates the standard deviation ofthe class values ofa set ofinstances.then follows the procedure for obtaining synthetic binary attributes that was described previously.standard procedures for creat-ing new nodes and printing the final tree are not shown.in splitsizeofreturnsthe number ofelements in a set.missing attribute values are dealt with asdescribed earlier.the sdr is calculated according to the equation at the begin-ning ofthe previous subsection.although not shown in the codeit is set toinfinity ifsplitting on the attribute would create a leafwith fewer than twoinstances.in prunethe linearregressionroutine recursively descends thesubtree collecting attributesperforms a linear regression on the instances at thatnode as a function ofthose attributesand then greedily drops terms ifdoingso improves the error estimateas described earlier.finallythe errorfunctionreturnswhere nis the number ofinstances at the node and nis the number ofparam-eters in the node s linear model.figure gives an example ofa model tree formed by this algorithm for aproblem with two numeric and two nominal attributes.what is to be predictedis the rise time ofa simulated servo system involving a servo amplifiermotorlead screwand sliding carriage.the nominal attributes play important roles.four synthetic binary attributes have been created for each ofthe five-valuednominal attributes motorand screwand they are shown in table in termsofthe two sets ofvalues to which they correspond.the ordering ofthesevalues decba for motorand coincidentally decba for screwalso is determined from the training datathe rise time averaged over all examplesfor which motord is less than that averaged over examples for which motorewhich is less than when motorcand so on.it is apparent from the mag-nitude ofthe coefficients in table that motord versus ecba plays aleading role in the modeland motorde versus cba plays a leadingrole in motorand screwalso play minor roles in several ofthe models.the decision tree shows a three-way split on a numeric attribute.first a binary-splitting tree was generated in the usual way.it turned out that the root and oneofits descendants tested the same attributepgainand a simple algorithm wasnnn- nndeviation from predicted class am page used to conflate these two nodes into the slightly more comprehensible tree thatis shown.rules from model treesmodel trees are essentially decision trees with linear models at the leaves.likedecision treesthey may suffer from the replicated subtree problem machine learning tree for a dataset with nominal attributes.table models in the model vs. e c b e vs. c b e c vs. b e c bvs. vs. e c b ascrew e vs. c b e c vs. b d e c b vs. am page section sometimes the structure can be expressed much more con-cisely using a set ofrules instead ofa tree.can we generate rules for numericprediction? recall the rule learner described in section that uses separate-and-conquer in conjunction with partial decision trees to extract decision rulesfrom trees.the same strategy can be applied to model trees to generate deci-sion lists for numeric prediction.first build a partial model tree from all the data.pick one ofthe leaves and make it into a rule.remove the data covered by that leafthen repeat the process with the remaining data.the question ishow to build the partialmodel treethat isa tree with unexpanded nodes? this boils down to the question ofhow to pick which node to expand next.the algorithm offigure picks the node whose entropy for the class attribute issmallest.for model treeswhose predictions are numericsimply use the vari-ance instead.this is based on the same rationalethe lower the variancetheshallower the subtree and the shorter the rule.the rest ofthe algorithm staysthe samewith the model tree learner s split selection method and pruning strategy replacing the decision tree learner s.because the model tree s leaves arelinear modelsthe corresponding rules will have linear models on the right-handside.there is one caveat when using model trees in this fashion to generate rulesetsthe smoothing process that the model tree learner employs.it turns outthat using smoothed model trees does not reduce the error in the final rule set spredictions.this may be because smoothing works best for contiguous databutthe separate-and-conquer scheme removes data covered by previous rulesleaving holes in the distribution.smoothingifit is done at allmust be per-formed after the rule set has been generated.locally weighted linear regressionan alternative approach to numeric prediction is the method oflocally weightedlinear regression.with model treesthe tree structure divides the instance spaceinto regionsand a linear model is found for each ofthem.in effectthe train-ing data determines how the instance space is partitioned.locally weightedregressionon the other handgenerates local models at prediction time bygiving higher weight to instances in the neighborhood ofthe particular testinstance.more specificallyit weights the training instances according to their distance to the test instance and performs a linear regression on theweighted data.training instances close to the test instance receive a high weightthose far away receive a low one.in other wordsa linear model is tailormade for the particular test instance at hand and used to predict the instance sclass am page to use locally weighted regressionyou need to decide on a distance-basedweighting scheme for the training instances.a common choice is to weight theinstances according to the inverse oftheir euclidean distance from the testinstance.another possibility is to use the euclidean distance in conjunction witha gaussian kernel function.howeverthere is no clear evidence that the choiceofweighting function is critical.more important is the selection ofa smooth-ing parameter that is used to scale the distance function the distance is mul-tiplied by the inverse ofthis parameter.ifit is set to a small valueonly instancesvery close to the test instance will receive significant weightifit is largemoredistant instances will also have a significant impact on the model.one way ofchoosing the smoothing parameter is to set it to the distance ofthe kth-nearesttraining instance so that its value becomes smaller as the volume oftrainingdata increases.the best choice ofkdepends on the amount ofnoise in the data.the more noise there isthe more neighbors should be included in the linearmodel.generallyan appropriate smoothing parameter is found using cross-validation.like model treeslocally weighted linear regression is able to approximatenonlinear functions.one ofits main advantages is that it is ideally suited forincremental learningall training is done at prediction timeso new instancescan be added to the training data at any time.howeverlike other instance-based methodsit is slow at deriving a prediction for a test instance.firstthe training instances must be scanned to compute their weightsthenaweighted linear regression is performed on these instances.alsolike otherinstance-based methodslocally weighted regression provides little informationabout the global structure ofthe training dataset.note that ifthe smoothingparameter is based on the kth-nearest neighbor and the weighting function gives zero weight to more distant instancesthe kd-trees and ball trees describedin section can be used to speed up the process offinding the relevant neighbors.locally weighted learning is not restricted to linear regressionit can beapplied with any learning technique that can handle weighted instances.in par-ticularyou can use it for classification.most algorithms can be easily adaptedto deal with weights.the trick is to realize that weights can be simu-lated by creating several copies ofthe same instance.whenever the learningalgorithm uses an instance when computing a modeljust pretend that it isaccompanied by the appropriate number ofidentical shadow instances.thisalso works ifthe weight is not an integer.for examplein the na ve bayes algo-rithm described in section the counts derived from an instance bythe instance s weightand voil you have a version ofna ve bayes that canbe used for locally weighted learning.it turns out that locally weighted na ve bayes works extremely well in prac-ticeoutperforming both na ve bayes itselfand the k-nearest-neighbor machine learning am page also compares favorably with far more sophisticated ways ofenhanc-ing na ve bayes by relaxing its intrinsic independence assumption.locallyweighted learning only assumes independence within a neighborhoodnotglobally in the whole instance space as standard na ve bayes does.in principlelocally weighted learning can also be applied to decision treesand other models that are more complex than linear regression and na ve bayes.howeverit is beneficial here because it is primarily a way ofallowing simplemodels to become more flexible by allowing them to approximate arbitrarytargets.ifthe underlying learning algorithm can already do thatthere is littlepoint in applying locally weighted learning.nevertheless it may improve othersimple models for examplelinear support vector machines and logisticregression.discussionregression trees were introduced in the cart system ofbreiman et classification and regression trees incorporated a decision treeinducer for discrete classes much like that was developed inde-pendentlyand a scheme for inducing regression trees.many ofthe techniquesdescribed in the preceding sectionsuch as the method ofhandling nominalattributes and the surrogate device for dealing with missing valueswereincluded in cart.howevermodel trees did not appear until much morerecentlybeing first described by quinlan model trees for gener-ating rule sets not partial trees has been explored by hall et tree induction is not so commonly used as decision tree inductionpartly because comprehensive descriptions implementations ofthe tech-nique have become available only recently and witten netsare more commonly used for predicting numeric quantitiesalthough theysuffer from the disadvantage that the structures they produce are opaque andcannot be used to help us understand the nature ofthe solution.although thereare techniques for producing understandable insights from the structure ofneural networksthe arbitrary nature ofthe internal representation means thatthere may be dramatic variations between networks ofidentical architecturetrained on the same data.by dividing the function being induced into linearpatchesmodel trees provide a representation that is reproducible and at leastsomewhat comprehensible.there are many variations oflocally weighted learning.for examplestatis-ticians have considered using locally quadratic models instead oflinear ones andhave applied locally weighted logistic regression to classification problems.alsomany different potential weighting and distance functions can be found in theliterature.atkeson et have written an excellent survey on am page weighted learningprimarily in the context ofregression problems.frank et evaluated the use oflocally weighted learning in conjunction with na section we examined the k-means clustering algorithm in which kinitialpoints are chosen to represent initial cluster centersall data points are assignedto the nearest onethe mean value ofthe points in each cluster is computed toform its new cluster centerand iteration continues until there are no changesin the clusters.this procedure only works when the number ofclusters is knownin advanceand this section begins by describing what you can do ifit is not.next we examine two techniques that do not partition instances into disjointclusters as k-means does.the first is an incremental clustering method that wasdeveloped in the late and embodied in a pair ofsystems called cobwebfor nominal attributes and classit numeric attributes.both come up witha hierarchical grouping ofinstances and use a measure ofcluster quality calledcategory utility.the second is a statistical clustering method based on a mixturemodel ofdifferent probability distributionsone for each cluster.it assignsinstances to classes probabilisticallynot deterministically.we explain the basictechnique and sketch the working ofa comprehensive clustering scheme calledautoclass.choosing the number of clusterssuppose you are using k-means but do not know the number ofclusters inadvance.one solution is to try out different possibilities and see which is best that iswhich one minimizes the total squared distance ofall points to theircluster center.a simple strategy is to start from a given minimumperhaps k work up to a small fixed maximumusing cross-validation to find thebest value.because k-means is slowand cross-validation makes it even slowerit will probably not be feasible to try many possible values for k.note that onthe training data the best clustering according to the total squared distancecriterion will always be to choose as many clusters as there are data points! topenalize solutions with many clusters you have to apply something like the mdlcriterion ofsection use cross-validation.another possibility is to begin by finding a few clusters and determiningwhether it is worth splitting them.you could choose k k-meansclustering until it terminatesand then consider splitting each cluster.compu-tation time will be reduced considerably ifthe initial two-way clustering is considered irrevocable and splitting is investigated for each component machine learning am page way to split a cluster is to make a new seedone standarddeviation away from the cluster s center in the direction ofits greatest variationand to make a second seed the same distance in the opposite direction.alternativelyifthis is too slowchoose a distance proportional to the cluster sbounding box and a random direction. then apply k-means to the points in the cluster with these two new seeds.having tentatively split a clusteris it worthwhile retaining the split or is theoriginal cluster equally plausible by itself? it s no good looking at the totalsquared distance ofall points to their cluster center this is bound to be smallerfor two subclusters.a penalty should be incurred for inventing an extra clusterand this is a job for the mdl criterion.that principle can be applied to seewhether the information required to specify the two new cluster centersalongwith the information required to specify each point with respect to themexceeds the information required to specify the original center and all the pointswith respect to it.ifsothe new clustering is unproductive and should be abandoned.ifthe split is retainedtry splitting each new cluster further.continue theprocess until no worthwhile splits remain.additional implementation efficiency can be achieved by combining this iterative clustering process with the kd-tree or ball tree data structure advocatedin section data points are reached by working down the tree from the root.when considering splitting a clusterthere is no need to consider the whole treejust consider those parts ofit that are needed to coverthe cluster.for examplewhen deciding whether to split the lower left cluster infigure on page the thick lineit is only necessary to con-sider nodes a and b ofthe tree in figure node c is irrelevantto that cluster.incremental clusteringwhereas the k-means algorithm iterates over the whole dataset until convergence is reachedthe clustering methods that we examine next workincrementallyinstance by instance.at any stage the clustering forms a tree withinstances at the leaves and a root node that represents the entire dataset.in thebeginning the tree consists ofthe root alone.instances are added one by oneand the tree is updated appropriately at each stage.updating may merely be acase offinding the right place to put a leafrepresenting the new instanceor itmay involve radically restructuring the part ofthe tree that is affected by thenew instance.the key to deciding how and where to update is a quantity calledcategory utilitywhich measures the overall quality ofa partition ofinstancesinto clusters.we defer detailed consideration ofhow this is defined until thenext subsection and look first at how the clustering algorithm am page the procedure is best illustrated by an example.we will use the familiarweather data againbut without the playattribute.to track progress the are labeled abc...nas in table for interest we includethe class yesor noin the label although it should be emphasized that for thisartificial dataset there is little reason to suppose that the two classes ofinstanceshould fall into separate categories.figure shows the situation at salientpoints throughout the clustering procedure.at the beginningwhen new instances are absorbed into the structuretheyeach form their own subcluster under the overall top-level cluster.each newinstance is processed by tentatively placing it into each ofthe existing leaves andevaluating the category utility ofthe resulting set ofthe top-level node s chil-dren to see whether the leafis a good host for the new instance.for each machine learning schemesfigure the weather am page first five instancesthere is no such hostit is betterin terms ofcategoryutilityto form a new leaffor each instance.with the sixth it finally becomesbeneficial to form a clusterjoining the new instance fwith the old one thehost e.ifyou look back at table you will see that the fifth andsixth instances are indeed very similardiffering only in the windyattribute is being ignored here.the next examplegis placed in the samecluster differs from eonly in outlook.this involves another call to the clus-tering procedure.firstgis evaluated to see which ofthe five children oftheroot makes the best hostit turns out to be the rightmostthe one that is alreadya cluster.then the clustering algorithm is invoked with this as the rootand itstwo children are evaluated to see which would make the better host.in this caseit proves bestaccording to the category utility measureto add the new instanceas a subcluster in its own right.ifwe were to continue in this veinthere would be no possibility ofany radicalrestructuring ofthe treeand the final clustering would be excessively depend-ent on the ordering ofexamples.to avoid thisthere is provision for restruc-turingand you can see it come into play when instance his added in the nextstep shown in figure this case two existing nodes are mergedinto a singleclusternodes aand dare merged before the new instance his added.one wayofaccomplishing this would be to consider all pairs ofnodes for merging andevaluate the category utility ofeach pair.howeverthat would be computa-tionally expensive and would involve a lot ofrepeated work ifit were under-taken whenever a new instance was added.insteadwhenever the nodes at a particular level are scanned for a suitablehostboth the best-matching node the one that produces the greatest categoryutility for the split at that level and the runner-up are noted.the best one willform the host for the new instance that new instance is better offin acluster ofits own.howeverbefore setting to work on putting the new instancein with the hostconsideration is given to merging the host and the runner-up.in this caseais the preferred host and dis the runner-up.when a merge ofaand dis evaluatedit turns out that it would improve the category utilitymeasure.consequentlythese two nodes are mergedyielding a version ofthefifth hierarchy offigure before his added.thenconsideration is given tothe placement ofhin the newmerged nodeand it turns out to be best to makeit a subcluster in its own rightas shown.an operation converse to merging is also implementedcalled splittingalthough it does not take place in this particular example.whenever the besthost is identifiedand merging has not proved beneficialconsideration is givento splitting the host node.splitting has exactly the opposite effect ofmergingtaking a node and replacing it with its children.for examplesplitting the right-most node in the fourth hierarchy offigure would raise the efand gleavesup a levelmaking them siblings ofabcand d.merging and splitting am page an incremental way ofrestructuring the tree to compensate for incorrect choicescaused by infelicitous ordering ofexamples.the final hierarchy for all examples is shown at the end offigure are two major clusterseach ofwhich subdivides further into its own subclusters.ifthe playdon t playdistinction really represented an inherentfeature ofthe dataa single cluster would be expected for each outcome.nosuch clean structure is observedalthough a generous eye might discerna slight tendency at lower levels for yesinstances to group togetherand likewisefor noinstances.careful analysis ofthe clustering reveals some anomalies.table will help ifyou want to follow this analysis in detail. for exampleinstances aand bare actually very similar to each otheryet they end up in com-pletely different parts ofthe tree.instance bends up with kwhich is a worsematch than a.instance aends up with dand hand it is certainly not as similarto das it is to b.the reason why aand bbecome separated is that aand dgetmergedas described previouslybecause they form the best and second-besthosts for h.it was unlucky that aand bwere the first two examplesifeither had occurred laterit may well have ended up with the other.subsequent split-ting and remerging may be able to rectify this anomalybut in this case theydidn t.exactly the same scheme works for numeric attributes.category utility isdefined for these as wellbased on an estimate ofthe mean and standard devi-ation ofthe value ofthat attribute.details are deferred to the next subsection.howeverthere is just one problem that we must attend to herewhen estimat-ing the standard deviation ofan attribute for a particular nodethe result willbe zero ifthe node contains only one instanceas it does more often than not.unfortunatelyzero variances produce infinite values in the category utilityformula.a simple heuristic solution is to impose a minimum variance on eachattribute.it can be argued that because no measurement is completely preciseit is reasonable to impose such a minimumit represents the measurement errorin a single sample.this parameter is called acuity.figure showsat the topa hierarchical clustering produced by theincremental algorithm for part ofthe iris dataset from eachclass.at the top level there are two clusters ofthe single noderepresenting the whole dataset.the first contains both iris virginicasand irisversicolorsand the second contains only iris setosas.the iris setosasthemselvessplit into two subclustersone with four cultivars and the other with six.theother top-level cluster splits into three subclusterseach with a fairly complexstructure.both the first and second contain only iris versicolorswith one excep-tiona stray iris virginicain each casethe third contains only iris virginicas.this represents a fairly satisfactory clustering ofthe iris datait shows that thethree genera are not artificial at all but reflect genuine differences in the data.this ishowevera slightly overoptimistic conclusionbecause quite a bit machine learning am page clusterings ofthe iris am page experimentation with the acuity parameter was necessary to obtain such a nicedivision.the clusterings produced by this scheme contain one leaffor every instance.this produces an overwhelmingly large hierarchy for datasets ofany reasonablesizecorrespondingin a senseto overfitting the particular dataset.conse-quentlya second numeric parameter called cutoffis used to suppress growth.some instances are deemed to be sufficiently similar to others to not warrantformation oftheir own child nodeand this parameter governs the similaritythreshold.cutoffis specified in terms ofcategory utilitywhen the increase incategory utility from adding a new node is sufficiently smallthat node is cutoff.figure shows the same iris dataclustered with cutoffin effect.manyleafnodes contain several instancesthese are children ofthe parent node thathave been cut off.the division into the three types ofiris is a little easier to seefrom this hierarchy because some ofthe detail is suppressed.againhoweversome experimentation with the cutoffparameter was necessary to get this resultand in fact a sharper cutoffleads to much less satisfactory clusters.similar clusterings are obtained ifthe full iris dataset instances is used.howeverthe results depend on the ordering ofexamplesfigure wasobtained by alternating the three varieties ofiris in the input file.ifall iris setosasare presented firstfollowed by all iris versicolorsand then all iris virginicastheresulting clusters are quite unsatisfactory.category utilitynow we look at how the category utilitywhich measures the overall quality ofa partition ofinstances into clustersis calculated.in section we learned howthe mdl measure couldin principlebe used to evaluate the quality ofclus-tering.category utility is not mdl based but rather resembles a kind ofquad-ratic loss function defined on conditional probabilities.the definition ofcategory utility is rather formidablewhere the kclustersthe outer summation is over these clus-tersthe next inner one sums over the attributesaiis the ith attributeand ittakes on values are dealt with by the sum over j.note that theprobabilities themselves are obtained by summing over all instancesthus thereis a further implied level ofsummation.this expression makes a great deal ofsense ifyou take the time to examineit.the point ofhaving a cluster is that it will give some advantage in machine learning am page the values ofattributes ofinstances in that cluster that ispraivijc a better estimate ofthe probability that attribute aihas value vijfor an instancein cluster c praivij because it takes account ofthe cluster the instanceis in.ifthat information doesn t helpthe clusters aren t doing much good! sowhat the preceding measure calculatesinside the multiple summationis theamount by which that information doeshelp in terms ofthe differences betweensquares ofprobabilities.this is not quite the standard squared-differencemetricbecause that sums the squares ofthe differences produces a sym-metric resultand the present measure sums the difference ofthe squareswhichappropriatelydoes not produce a symmetric result.the differencesbetween squares ofprobabilities are summed over all attributesand all theirpossible valuesin the inner double summation.then it is summed over all clus-tersweighted by their probabilitiesin the outer summation.the overall division by kis a little hard to justify because the squared differ-ences have already been summed over the categories.it essentially provides a per cluster figure for the category utility that discourages overfitting.other-wisebecause the probabilities are derived by summing over the appropriateinstancesthe very best category utility would be obtained by placing eachinstance in its own cluster.thenpraivijc would be for the value thatattribute aiactually has for the single instance in category c and for all othervaluesand the numerator ofthe category utility formula will end up aswhere nis the total number ofattributes.this is the greatest value that thenumerator can haveand so ifit were not for the additional division by kin thecategory utility formulathere would never be any incentive to form clusterscontaining more than one member.this extra factor is best viewed as a rudi-mentary overfitting-avoidance heuristic.this category utility formula applies only to nominal attributes.howeveritcan easily be extended to numeric attributes by assuming that their distributionis normal with a given mean mand standard deviation s.the prob-ability density function for an attribute aisthe analog ofsumming the squares ofattribute value probabilities iswhere siis the standard deviation ofthe attribute ai.thus for a numeric attrib-utewe estimate the standard deviation from the databoth within the clusterpravfadaiijjiii am page and for the data over all clusters use these in the category utilityformulanow the problem mentioned previously that occurs when the standard devia-tion estimate is zero becomes apparenta zero standard deviation produces aninfinite value ofthe category utility formula.imposing a prespecified minimumvariance on each attributethe acuityis a rough-and-ready solution to theproblem.probability-based clusteringsome ofthe shortcomings ofthe heuristic clustering described previously havealready become apparentthe arbitrary division by kin the category utilityformula that is necessary to prevent overfittingthe need to supply an artificialminimum value for the standard deviation ofclustersthe ad hoc cutoffvalueto prevent every instance from becoming a cluster in its own right.on top ofthis is the uncertainty inherent in incremental algorithmsto what extent is theresult dependent on the order ofexamples? are the local restructuring opera-tions ofmerging and splitting really enough to reverse the effect ofbad initialdecisions caused by unlucky ordering? does the final result represent even a localminimum ofcategory utility? add to this the problem that one never knowshow far the final configuration is from a globalminimum and that the stan-dard trick ofrepeating the clustering procedure several times and choosing thebest will destroy the incremental nature ofthe algorithm.finallydoesn t thehierarchical nature ofthe result really beg the question ofwhich are the bestclusters? there are so many clusters in figure that it is hard to separate thewheat from the chaff.a more principled statistical approach to the clustering problem can over-come some ofthese shortcomings.from a probabilistic perspectivethe goal ofclustering is to find the most likely set ofclusters given the data expectations.because no finite amount ofevidence is enough to make acompletely firm decision on the matterinstances even training instances should not be placed categorically in one cluster or the otherinstead they have a certain probability ofbelonging to each cluster.this helps to eliminatethe brittleness that is often associated with methods that make hard and fastjudgments.the foundation for statistical clustering is a statistical model called finite mix-tures.a mixtureis a set ofkprobability distributionsrepresenting kclustersthat govern the attribute values for members ofthat cluster.in other machine learning am page gives the probability that a particular instance would have a certainset ofattribute values ifit were knownto be a member ofthat cluster.eachcluster has a different distribution.any particular instance really belongs toone and only one ofthe clustersbut it is not known which one.finallytheclusters are not equally likelythere is some probability distribution that reflectstheir relative populations.the simplest finite mixture situation occurs when there is only one numericattributewhich has a gaussian or normal distribution for each cluster butwith different means and variances.the clustering problem is to take a set ofinstances in this case each instance is just a number and a prespecifiednumber ofclustersand work out each cluster s mean and variance and the pop-ulation distribution between the clusters.the mixture model combines severalnormal distributionsand its probability density function looks like a mountainrange with a peak for each component.figure shows a simple example.there are two clustersa and band eachhas a normal distribution with means and standard deviationsmaand saforcluster aand mband sbfor cluster brespectively.samples are taken from thesedistributionsusing cluster a with probability paand cluster b with probabilitypbwhere and resulting in a dataset like that shown.nowimaginebeing given the dataset without the classes just the numbers and being askedto determine the five parameters that characterize the modelmasambsbandpathe parameter pbcan be calculated directly from pa.that is the finitemixture problem.ifyou knew which ofthe two distributions each instance came fromfindingthe five parameters would be easy just estimate the mean and standard devi-ation for the cluster a samples and the cluster b samples separatelyusing theformulasthe use rather than nas the denominator in the second formula is atechnicality ofsamplingit makes little difference in practice ifnis used the samples from the distribution a or b.to estimatethe fifth parameter pajust take the proportion ofthe instances that are in thea cluster.ifyou knew the five parametersfinding the probabilities that a given instancecomes from each distribution would be easy.given an instance xthe probabil-ity that it belongs to cluster a am page where fxmasa is the normal distribution function for cluster athat isthe denominator prx will disappearwe calculate the numerators for bothpra and prb and normalize them by dividing by their sum.this wholeprocedure is just the same as the way numeric attributes are treated in the na vebayes learning scheme ofsection the caveat explained there applieshere toostrictly speakingfxmasa is not the probability prxa becausethe probability ofxbeing any particular real number is zerobut the machine learning schemesdataa a b b a a a a a b a a b a b a a b a b a b b b a a b a a a a a b a a b a a b b a b b a b a sa pa sb pb two-class mixture am page process makes the final result correct.note that the final outcome is nota particular cluster but rather the probabilitieswith which xbelongs to clustera and cluster b.the em algorithmthe problem is that we know neither ofthese thingsnot the distribution thateach training instance came from nor the five parameters ofthe mixture model.so we adopt the procedure used for the k-means clustering algorithm anditerate.start with initial guesses for the five parametersuse them to calculatethe cluster probabilities for each instanceuse these probabilities to reestimatethe parametersand repeat.ifyou preferyou can start with guesses for theclasses ofthe instances instead. this is called the em algorithmfor expecta-tion maximization.the first stepcalculation ofthe cluster probabilities the expected class values is expectation secondcalculation ofthedistribution parametersis maximization ofthe likelihood ofthe distributionsgiven the data.a slight adjustment must be made to the parameter estimation equations toaccount for the fact that it is only cluster probabilitiesnot the clusters them-selvesthat are known for each instance.these probabilities just act like weights.ifwiis the probability that instance ibelongs to cluster athe mean and stan-dard deviation for cluster a are where now the xiare allthe instancesnot just those belonging to cluster a.this differs in a small detail from the estimate for the standard deviation givenon page speakingthis is a maximum likelihood estimator forthe variancewhereas the formula on page is for an unbiased estimator.the difference is not important in practice.now consider how to terminate the iteration.the k-means algorithm stops when the classes ofthe instances don t change from one iteration to thenext a fixed point has been reached.in the em algorithm things are not quiteso easythe algorithm converges toward a fixed point but never actually getsthere.but we can see how close it is by calculating the overall likelihood thatthe data came from this datasetgiven the values for the five parameters.thisoverall likelihood is obtained by multiplying the probabilities ofthe individualinstances am page where the probabilities given the clusters a and b are determined from thenormal distribution function fxms.this overall likelihood is a measure ofthe goodness ofthe clustering and increases at each iteration ofthe em algo-rithm.againthere is a technical difficulty with equating the probability ofaparticular value ofxwith fxmsand in this case the effect does not disap-pear because no probability normalization operation is applied.the upshot isthat the preceding likelihood expression is not a probability and does not nec-essarily lie between zero and oneneverthelessits magnitude still reflects the quality ofthe clustering.in practical implementations its logarithm is calculated insteadthis is done by summing the logarithms ofthe individualcomponentsavoiding all the multiplications.but the overall conclusion stillholdsyou should iterate until the increase in log-likelihood becomes negligi-ble.for examplea practical implementation might iterate until the differencebetween successive values oflog-likelihood is less than successiveiterations.typicallythe log-likelihood will increase very sharply over the firstfew iterations and then converge rather quickly to a point that is virtually stationary.although the em algorithm is guaranteed to converge to a maximumthis is a localmaximum and may not necessarily be the same as the global max-imum.for a better chance ofobtaining the global maximumthe whole proce-dure should be repeated several timeswith different initial guesses for theparameter values.the overall log-likelihood figure can be used to compare thedifferent final configurations obtainedjust choose the largest ofthe localmaxima.extending the mixture modelnow that we have seen the gaussian mixture model for two distributionslet sconsider how to extend it to more realistic situations.the basic method is justthe samebut because the mathematical notation becomes formidable we willnot develop it in full detail.changing the algorithm from two-class problems to multiclass problems iscompletely straightforward as long as the number kofnormal distributions isgiven in advance.the model can be extended from a single numeric attribute per instance tomultiple attributes as long as independence between attributes is assumed.theprobabilities for each attribute are multiplied together to obtain the joint prob-ability for the instancejust as in the na ve bayes method.pxpxiiiabpraprb machine learning am page the dataset is known in advance to contain correlated attributestheindependence assumption no longer holds.insteadtwo attributes can bemodeled jointly using a bivariate normal distributionin which each has its ownmean value but the two standard deviations are replaced by a covariancematrix with four numeric parameters.there are standard statistical techniquesfor estimating the class probabilities ofinstances and for estimating the means and covariance matrix given the instances and their class probabilities.several correlated attributes can be handled using a multivariate distribution.the number ofparameters increases with the square ofthe number ofjointlyvarying attributes.with nindependent attributesthere are and a standard deviation for each.with ncovariant attributesthere parametersa mean for each and an n ncovariance matrix thatis symmetric and therefore involves different quantities.this escala-tion in the number ofparameters has serious consequences for overfittingaswe will explain later.to cater for nominal attributesthe normal distribution must be abandoned.insteada nominal attribute with vpossible values is characterized by vnumbersrepresenting the probability ofeach one.a different set ofnumbers is neededfor every classkvparameters in all.the situation is very similar to the na ve bayes method.the two steps ofexpectation and maximization corre-spond exactly to operations we have studied before.expectation estimatingthe cluster to which each instance belongs given the distribution parameters is just like determining the class ofan unknown instance.maximization estimating the parameters from the classified instances is just like determin-ing the attribute value probabilities from the training instanceswith the small difference that in the em algorithm instances are assigned to classes probabilistically rather than categorically.in section we encountered the problem that probability estimates can turn out to be zeroand the sameproblem occurs here too.fortunatelythe solution is just as simple use thelaplace estimator.na ve bayes assumes that attributes are independent that is why it is called na ve. a pair ofcorrelated nominal attributes with valuesrespectivelycan be replaced with a single covariant attribute with number ofparameters escalates as the number ofdependentattributes increasesand this has implications for probability estimates and over-fitting that we will come to shortly.the presence ofboth numeric and nominal attributes in the data to be clus-tered presents no particular problem.covariant numeric and nominal attrib-utes are more difficult to handleand we will not describe them here.missing values can be accommodated in various different ways.missingvalues ofnominal attributes can simply be left out ofthe probability am page tionsas described in section they can be treated as an addi-tional value ofthe attributeto be modeled as any other value.which is moreappropriate depends on what it means for a value to be missing. exactly thesame possibilities exist for numeric attributes.with all these enhancementsprobabilistic clustering becomes quite sophis-ticated.the em algorithm is used throughout to do the basic work.the usermust specify the number ofclusters to be soughtthe type ofeach attributenumeric or nominalwhich attributes are modeled as covaryingand what to do about missing values.moreoverdifferent distributions than the onesdescribed previously can be used.although the normal distribution is usuallya good choice for numeric attributesit is not suitable for attributes asweight that have a predetermined minimum the case ofweight butno upper boundin this case a log-normal distribution is more appropriate.numeric attributes that are bounded above and below can be modeled by a log-odds distribution.attributes that are integer counts rather than real valuesare best modeled by the poisson distribution.a comprehensive system mightallow these distributions to be specified individually for each attribute.in eachcasethe distribution involves numeric parameters probabilities ofall possi-ble values for discrete attributes and mean and standard deviation for continu-ous ones.in this section we have been talking about clustering.but you may be thinking that these enhancements could be applied just as well to the na vebayes algorithm too and you d be right.a comprehensive probabilisticmodeler could accommodate both clustering and classification learningnominal and numeric attributes with a variety ofdistributionsvarious possi-bilities ofcovariationand different ways ofdealing with missing values.theuser would specifyas part ofthe domain knowledgewhich distributions to usefor which attributes.bayesian clusteringhoweverthere is a snagoverfitting.you might say that ifwe are not sure whichattributes are dependent on each otherwhy not be on the safe side and specifythat allthe attributes are covariant? the answer is that the more parametersthere arethe greater the chance that the resulting structure is overfitted to thetraining data and covariance increases the number ofparameters dramati-cally.the problem ofoverfitting occurs throughout machine learningandprobabilistic clustering is no exception.there are two ways that it can occurthrough specifying too large a number ofclusters and through specifying dis-tributions with too many parameters.the extreme case oftoo many clusters occurs when there is one for everydata pointclearlythat will be overfitted to the training data.in factin machine learning am page modelproblems will occur whenever any ofthe normal distributionsbecomes so narrow that it is centered on just one data point.consequentlyimplementations generally insist that clusters contain at least two different datavalues.whenever there are a large number ofparametersthe problem ofoverfittingarises.ifyou were unsure ofwhich attributes were covariantyou might try outdifferent possibilities and choose the one that maximized the overall probabil-ity ofthe data given the clustering that was found.unfortunatelythe moreparameters there arethe larger the overall data probability will tend to be notnecessarily because ofbetter clustering but because ofoverfitting.the moreparameters there are to play withthe easier it is to find a clustering that seemsgood.it would be nice ifsomehow you could penalize the model for introducingnew parameters.one principled way ofdoing this is to adopt a fully bayesianapproach in which every parameter has a prior probability distribution.thenwhenever a new parameter is introducedits prior probability must be incor-porated into the overall likelihood figure.because this will involve multiplyingthe overall likelihood by a number less than one the prior probability it willautomatically penalize the addition ofnew parameters.to improve the overalllikelihoodthe new parameters will have to yield a benefit that outweighs thepenalty.in a sensethe laplace estimator that we met in section whose usewe advocated earlier to counter the problem ofzero probability estimates fornominal valuesis just such a device.whenever observed probabilities are smallthe laplace estimator exacts a penalty because it makes probabilities that arezeroor close to zerogreaterand this will decrease the overall likelihood ofthedata.making two nominal attributes covariant will exacerbate the problem.instead the number ofpossible valuesthere are now increasing the chance ofa large number ofsmall esti-mated probabilities.in factthe laplace estimator is tantamount to using a par-ticular prior distribution for the introduction ofnew parameters.the same technique can be used to penalize the introduction oflargenumbers ofclustersjust by using a prespecified prior distribution that decayssharply as the number ofclusters increases.autoclass is a comprehensive bayesian clustering scheme that uses the finitemixture model with prior distributions on all the parameters.it allows bothnumeric and nominal attributes and uses the em algorithm to estimate theparameters ofthe probability distributions to best fit the data.because there isno guarantee that the em algorithm converges to the global optimumthe pro-cedure is repeated for several different sets ofinitial values.but that is not all.autoclass considers different numbers ofclusters and can consider differentamounts ofcovariance and different underlying probability distribution am page for the numeric attributes.this involves an additionalouter level ofsearch.forexampleit initially evaluates the log-likelihood for clus-tersafter thatit fits a log-normal distribution to the resulting data and ran-domly selects from it more values to try.as you might imaginethe overallalgorithm is extremely computation intensive.in factthe actual implementa-tion starts with a prespecified time bound and continues to iterate as long astime allows.give it longer and the results may be better!discussionthe clustering methods that have been described produce different kinds ofoutput.all are capable oftaking new data in the form ofa test set and classify-ing it according to clusters that were discovered by analyzing a training set.howeverthe incremental clustering method is the only one that generates anexplicit knowledge structure that describes the clustering in a way that can bevisualized and reasoned about.the other algorithms produce clusters that couldbe visualized in instance space ifthe dimensionality were not too high.ifa clustering method were used to label the instances ofthe training set withcluster numbersthat labeled set could then be used to train a rule or decisiontree learner.the resulting rules or tree would form an explicit description ofthe classes.a probabilistic clustering scheme could be used for the samepurposeexcept that each instance would have multiple weighted labels and therule or decision tree learner would have to be able to cope with weightedinstances as many can.another application ofclustering is to fill in any values ofthe attributes thatmay be missing.for exampleit is possible to make a statistical estimate ofthevalue ofunknown attributes ofa particular instancebased on the class distri-bution for the instance itselfand the values ofthe unknown attributes for otherexamples.all the clustering methods we have examined make a basic assumption ofindependence among the attributes.autoclass does allow the user to specify in advance that two or more attributes are dependent and should be modeledwith a joint probability distribution.there are restrictionshowevernominalattributes may vary jointlyas may numeric attributesbut not both together.moreovermissing values for jointly varying attributes are not catered for. itmay be advantageous to preprocess a dataset to make the attributes more inde-pendentusing a statistical technique such as the principal components trans-form described in section that joint variation that is specific toparticular classes will not be removed by such techniquesthey only removeoverall joint variation that runs across all classes.our description ofhow to modify k-means to find a good value ofkbyrepeatedly splitting clusters and seeing whether the split is worthwhile machine learning am page x-means algorithm ofmoore and pelleg ofthemdl principle they use a probabilistic scheme called the bayes information criterion and wasserman incremental clustering procedurebased on the merging and splitting operationswas introduced in systems calledcobweb for nominal attributes and classit for numeric attributesgennari et are based on a measure ofcategory utility that hadbeen defined previously and corter autoclass program isdescribed by cheeseman and stutz implementations are availablethe original research implementationwritten in lispand a follow-up publicimplementation in c that is or times faster but somewhat morerestricted for exampleonly the normal-distribution model is implementedfor numeric networksthe na ve bayes classifier ofsection and the logistic regression models ofsection both produce probability estimates rather than predictions.for eachclass valuethey estimate the probability that a given instance belongs to thatclass.most other types ofclassifiers can be coerced into yielding this kind ofinformation ifnecessary.for exampleprobabilities can be obtained from adecision tree by computing the relative frequency ofeach class in a leafand froma decision list by examining the instances that a particular rule covers.probability estimates are often more useful than plain predictions.they allow predictions to be rankedand their expected cost to be minimized factthere is a strong argument for treating classification learn-ing as the task oflearning class probability estimates from data.what is beingestimated is the conditional probability distribution ofthe values ofthe classattribute given the values ofthe other attributes.the classification model rep-resents this conditional distribution in a concise and easily comprehensibleform.viewed in this wayna ve bayes classifierslogistic regression modelsdeci-sion treesand so onare just alternative ways ofrepresenting a conditionalprobability distribution.ofcoursethey differ in representational power.na ve bayes classifiers and logistic regression models can only represent simpledistributionswhereas decision trees can represent or at least approximate arbitrary distributions.howeverdecision trees have their drawbacksthey frag-ment the training set into smaller and smaller pieceswhich inevitably yield lessreliable probability estimatesand they suffer from the replicated subtreeproblem described in section sets go some way toward addressing theseshortcomingsbut the design ofa good rule learner is guided by heuristics withscant theoretical am page does this mean that we have to accept our fate and live with these shortcom-ings? no! there is a statistically based alternativea theoretically well-foundedway ofrepresenting probability distributions concisely and comprehensibly ina graphical manner.the structures are called bayesian networks.they are drawnas a network ofnodesone for each attributeconnected by directed edges insuch a way that there are no cycles a directed acyclic graph.in our explanation ofhow to interpret bayesian networks and how to learnthem from datawe will make some simplifying assumptions.we assume thatall attributes are nominal and that there are no missing values.some advancedlearning algorithms can create new attributes in addition to the ones present inthe data so-called hidden attributes whose values cannot be observed.thesecan support better models ifthey represent salient features ofthe underlyingproblemand bayesian networks provide a good way ofusing them at predic-tion time.howeverthey make both learning and prediction far more complexand time consumingso we will not consider them here.making predictionsfigure shows a simple bayesian network for the weather data.it has a nodefor each ofthe four attributes outlooktemperaturehumidityand windyandone for the class attribute play.an edge leads from the playnode to each oftheother nodes.but in bayesian networks the structure ofthe graph is only halfthe story.figure shows a table inside each node.the information in thetables defines a probability distribution that is used to predict the class proba-bilities for any given instance.before looking at how to compute this probability distributionconsider theinformation in the tables.the lower four tables outlooktemperaturehumidityand windy have two parts separated by a vertical line.on the left arethe values ofplayand on the right are the corresponding probabilities for eachvalue ofthe attribute represented by the node.in generalthe left side containsa column for every edge pointing to the nodein this case just the playattrib-ute.that is why the table associated with playitselfdoes not have a left sideithas no parents.in generaleach row ofprobabilities corresponds to one com-bination ofvalues ofthe parent attributesand the entries in the row show theprobability ofeach value ofthe node s attribute given this combination.ineffecteach row defines a probability distribution over the values ofthe node sattribute.the entries in a row always sum to shows a more complex network for the same problemwherethree nodes humidity have two parents.againthereis one column on the left for each parent and as many columns on the right asthe attribute has values.consider the first row ofthe table associated with thetemperaturenode.the left side gives a value for each parent machine learning am page right gives a probability for each value oftemperature.for examplethe first number is the probability oftemperaturetaking on the valuehotgiven that playand outlookhave values yesand sunnyrespectively.how are the tables used to predict the probability ofeach class value for agiven instance? this turns out to be very easybecause we are assuming thatthere are no missing values.the instance specifies a value for each attribute.foreach node in the networklook up the probability ofthe node s attribute valuebased on the row determined by its parents attribute values.then just multi-ply all these probabilities simple bayesian network for the weather am page machine learning bayesian network for the weather am page exampleconsider an instance with values outlook windy calculate the probability for play that the network in figure gives probability from from from from windy.the product is same calculation for play are clearly not the final answerthe final proba-bilities must sum to and don t.they are actually thejoint probabilities prplay and prplay edenotes all theevidence given by the instance s attribute values.joint probabilities measure thelikelihood ofobserving an instance that exhibits the attribute values in eas wellas the respective class value.they only sum to ifthey exhaust the space ofallpossible attribute value combinationsincluding the class attribute.this is cer-tainly not the case in our example.the solution is quite simple already encountered it in section obtain the conditional probabilities pr and pr the joint probabilities by dividing them by their sum.this gives probability for play forplay one mystery remainswhy multiply all those probabilities together? itturns out that the validity ofthe multiplication step hinges on a single assump-tion namely thatgiven values for each ofa node s parentsknowing the valuesfor any other ancestors does not change the probability associated with each ofits possible values.in other wordsancestors do not provide any informationabout the likelihood ofthe node s values over and above the information pro-vided by the parents.this can be writtenwhich must hold for all values ofthe nodes and attributes involved.in statisticsthis property is called conditional independence.multiplication is valid pro-vided that each node is conditionally independent ofits grandparentsgreat-grandparentsand so ongiven its parents.the multiplication step resultsdirectly from the chain rule in probability theorywhich states that the jointprobability ofnattributes aican be decomposed into this productthe decomposition holds for any order ofthe attributes.because our bayesiannetwork is an acyclic graphits nodes can be ordered to give all ancestors ofanode aiindices smaller than i.thenbecause ofthe conditional independenceassumptionprprpr am page which is exactly the multiplication rule that we applied previously.the two bayesian networks in figure and figure are fundamentallydifferent.the first makes stronger independence assumptionsbecause for each ofits nodes the set ofparents is a subset ofthe correspondingset ofparents in the second factfigure is almost identi-cal to the simple na ve bayes classifier ofsection probabilities areslightly different but only because each count has been initialized to to avoidthe zero-frequency problem. the network in figure has more rows in theconditional probability tables and hence more parametersit may be a moreaccurate representation ofthe underlying domain.it is tempting to assume that the directed edges in a bayesian network rep-resent causal effects.but be careful! in our casea particular value ofplaymayenhance the prospects ofa particular value ofoutlookbut it certainly doesn tcause it it is more likely to be the other way round.different bayesian net-works can be constructed for the same problemrepresenting exactly the sameprobability distribution.this is done by altering the way in which the jointprobability distribution is factorized to exploit conditional independencies.thenetwork whose directed edges model causal effects is often the simplest one withthe fewest parameters.hencehuman experts who construct bayesian networksfor a particular domain often benefit by representing causal effects by directededges.howeverwhen machine learning techniques are applied to inducemodels from data whose causal structure is unknownall they can do is con-struct a network based on the correlations that are observed in the data.infer-ring causality from correlation is always a dangerous business.learning bayesian networksthe way to construct a learning algorithm for bayesian networks is to definetwo componentsa function for evaluating a given network based on the dataand a method for searching through the space ofpossible networks.the qualityofa given network is measured by the probability ofthe data given the network.we calculate the probability that the network accords to each instance and multiply these probabilities together over all instances.in practicethis quicklyyields numbers too small to be represented properly arithmetic underflowso we use the sum ofthe logarithms ofthe probabilities rather than their product.the resulting quantity is the log-likelihood ofthe network given the data.assume that the structure ofthe network the set ofedges is given.it s easyto estimate the numbers in the conditional probability tablesjust compute therelative frequencies ofthe associated combinations ofattribute values in thetraining data.to avoid the zero-frequency problem each count is initialized witha constant as described in section exampleto find the probability thathumidity that play temperature last machine learning am page third row ofthe humiditynode s table in figure from that there are three instances with this combination ofattributevalues in the weather dataand no instances with humidity the same values for playand temperature.initializing the counts for the two values ofhumidityto yields the probability for humidity nodes in the network are predeterminedone for each attribute the class.learning the network structure amounts to searching through thespace ofpossible sets ofedgesestimating the conditional probability tables foreach setand computing the log-likelihood ofthe resulting network based onthe data as a measure ofthe network s quality.bayesian network learning algorithms differ mainly in the way in which they search through the space ofnetwork structures.some algorithms are introduced below.there is one caveat.ifthe log-likelihood is maximized based on the trainingdatait will always be better to add more edgesthe resulting network will simplyoverfit.various methods can be employed to combat this problem.one possi-bility is to use cross-validation to estimate the goodness offit.a second is toadd a penalty for the complexity ofthe network based on the number ofparam-etersthat isthe total number ofindependent estimates in all the probabilitytables.for each tablethe number ofindependent probabilities is the totalnumber ofentries minus the number ofentries in the last columnwhich canbe determined from the other columns because all rows must sum to kbe the number ofparametersllthe log-likelihoodand nthe number ofinstances in the data.two popular measures for evaluating the quality ofanetwork are the akaike information criterionaicand the following mdl metricbased on the mdl principlein both cases the log-likelihood is negatedso the aim is to minimize thesescores.a third possibility is to assign a prior distribution over network structuresand find the most likely network by combining its prior probability with theprobability accorded to the network by the data.this is the bayesian approachto network scoring.depending on the priordistribution usedit can takevarious forms.howevertrue bayesians would average over all possible networkstructures rather than singling out a particular network for prediction.unfor-tunatelythis generally requires a great deal ofcomputation.a simplifiedapproach is to average over all network structures that are substructures ofamdl am page given network.it turns out that this can be implemented very efficiently bychanging the method for calculating the conditional probability tables so thatthe resulting probability estimates implicitly contain information from all sub-networks.the details ofthis approach are rather complex and will not bedescribed here.the task ofsearching for a good network structure can be greatly simplifiedifthe right metric is used for scoring.recall that the probability ofa singleinstance based on a network is the product ofall the individual probabilitiesfrom the various conditional probability tables.the overall probability ofthedataset is the product ofthese products for all instances.because terms in aproduct are interchangeablethe product can be rewritten to group together allfactors relating to the same table.the same holds for the log-likelihoodusingsums instead ofproducts.this means that the likelihood can be optimized sep-arately for each node ofthe network.this can be done by addingor removingedges from other nodes to the node that is being optimized the only constraintis that cycles must not be introduced.the same trick also works ifa local scoringmetric such as aic or mdl is used instead ofplain log-likelihood because thepenalty term splits into several componentsone for each nodeand each nodecan be optimized independently.specific algorithmsnow we move on to actual algorithms for learning bayesian networks.onesimple and very fast learning algorithmcalled with a given orderingofthe attributes it processes each node in turn and greedilyconsiders adding edges from previously processed nodes to the current one.ineach step it adds the edge that maximizes the network s score.when there is nofurther improvementattention turns to the next node.as an additional mech-anism for overfitting avoidancethe number ofparents for each node can berestricted to a predefined maximum.because only edges from previously pro-cessed nodes are considered and there is a fixed orderingthis procedure cannotintroduce cycles.howeverthe result depends on the initial orderingso it makessense to run the algorithm several times with different random orderings.the na ve bayes classifier is a network with an edge leading from the classattribute to each ofthe other attributes.when building networks for classifica-tionit sometimes helps to use this network as a starting point for the search.this can be done in by forcing the class variable to be the first one in theordering and initializing the set ofedges appropriately.another potentially helpful trick is to ensure that every attribute in the datais in the markov blanketofthe node that represents the class attribute.a node smarkov blanket includes all its parentschildrenand children s parents.it canbe shown that a node is conditionally independent ofall other nodes machine learning am page for the nodes in its markov blanket.henceifa node is absent from theclass attribute s markov blanketits value is completely irrelevant to the finds a network that does not include a relevant attrib-ute in the class node s markov blanketit might help to add an edge that rectifiesthis shortcoming.a simple way ofdoing this is to add an edge from theattribute s node to the class node or from the class node to the attribute s nodedepending on which option avoids a cycle.a more sophisticated but slower version is not to order the nodes butto greedily consider adding or deleting edges between arbitrary pairs ofnodesall the while ensuring acyclicityofcourse.a further step is to consider invert-ing the direction ofexisting edges as well.as with any greedy algorithmtheresulting network only represents a localmaximum ofthe scoring functionitis always advisable to run such algorithms several times with different randominitial configurations.more sophisticated optimization strategies such as simu-lated annealingtabu searchor genetic algorithms can also be used.another good learning algorithm for bayesian network classifiers is called treeaugmented na ve bayestan.as the name impliesit takes the na ve bayesclassifier and adds edges to it.the class attribute is the single parent ofeachnode ofa na ve bayes networktan considers adding a second parent to eachnode.ifthe class node and all corresponding edges are excluded from consid-erationand assuming that there is exactly one node to which a second parentis not addedthe resulting classifier has a tree structure rooted at the parentlessnode this is where the name comes from.for this restricted type ofnetworkthere is an efficient algorithm for finding the set ofedges that maximizes thenetwork s likelihood based on computing the network s maximum weightedspanning tree.this algorithm is linear in the number ofinstances and quad-ratic in the number ofattributes.all the scoring metrics that we have described so far are likelihood based in the sense that they are designed to maximize the joint probability for each instance.howeverin classificationwhat we reallywant to maximize is the conditional probability ofthe class given the values ofthe other attributes in other wordsthe conditional likelihood.unfortunatelythere is no closed-form solution for the maximum conditional-likelihood prob-ability estimates that are needed for the tables in a bayesian network.on theother handcomputing the conditional likelihood for a given network anddataset is straightforward after allthis is what logistic regression does.henceit has been proposed to use standard maximum likelihood probability estimatesin the networkbut the conditional likelihood to evaluate a particular networkstructure.another way ofusing bayesian networks for classification is to build a sepa-rate network for each class valuebased on the data pertaining to that classandcombine the predictions using bayes s rule.the set ofnetworks is called am page bayesian multinet.to obtain a prediction for a particular class valuetake thecorresponding network s probability and multiply it by the class s prior proba-bility.do this for each class and normalize the result as we did previously.inthis case we would not use the conditional likelihood to learn the network foreach class value.all the network learning algorithms introduced previously are score based.a different strategywhich we will not explain hereis to piece a networktogether by testing individual conditional independence assertions based onsubsets ofthe attributes.this is known as structure learning by conditional inde-pendence tests.data structures for fast learninglearning bayesian networks involves a lot ofcounting.for each network struc-ture considered in the searchthe data must be scanned afresh to obtain thecounts needed to fill out the conditional probability tables.insteadcould theybe stored in a data structure that eliminated the need for scanning the data overand over again? an obvious way is to precompute the counts and store thenonzero ones in a table saythe hash table mentioned in section soany nontrivial dataset will have a huge number ofnonzero counts.againconsider the weather data from table are five attributestwo with three values and three with two values.this gives possible counts.each component ofthe product corre-sponds to an attributeand its contribution to the product is one more than thenumber ofits values because the attribute may be missing from the count.allthese counts can be calculated by treating them as item setsas explained insection setting the minimum coverage to one.but even without storingcounts that are zerothis simple scheme runs into memory problems very quickly.it turns out that the counts can be stored effectively in a structure called anall-dimensions treewhich is analogous to the kd-trees used for nearest-neighbor search described in section simplicitywe illustrate this usinga reduced version ofthe weather data that only has the attributes humiditywindyand play.figure summarizes the data.the number ofpossiblecounts is only ofthem are shown.for examplethecount for play them!.figure shows an ad tree for this data.each node says how manyinstances exhibit the attribute values that are tested along the path from the rootto that node.for examplethe leftmost leafsays that there is one instance withvalues humidity play the rightmost leafsays that there are five instances with play would be trivial to construct a tree that enumerates all counts explic-itly.howeverthat would gain nothing over a plain table and is obviously machine learning am page the tree in figure doesbecause it contains only counts.thereisfor exampleno branch that tests humidity was the tree con-structedand how can all counts be obtained from it?assume that each attribute in the data has been assigned an index.in thereduced version ofthe weather data we give humidityindex playindex ad tree is generated by expanding each node correspon-ding to an attribute iwith the values ofall attributes that have indices jiwithtwo important restrictionsthe most populous expansion for each attribute isomitted ties arbitrarily as are expansions with counts that are zero.the root node is given index for it all attributes are expandedsubject tothe same instanceswindy instancesplay instanceshumidity instanceswindy instancesplay instanceplay instancesplay instancebfigure weather dataa reduced version and corresponding ad am page for examplefigure contains no expansion for windy theroot node because with eight instances it is the most populous expansionthe value falseoccurs more often in the data than the value true.similarlyfromthe node labeled humidity is no expansion for windy falseis the most common value for windyamong all instances withhumidity factin our example the second restriction namelythatexpansions with zero counts are omitted never kicks in because the firstrestriction precludes any path that starts with the tests humidity is the only way to reach the solitary zero in figure node ofthe tree represents the occurrence ofa particular combina-tion ofattribute values.it is straightforward to retrieve the count for a combination that occurs in the tree.howeverthe tree does not explicitly repre-sent many nonzero counts because the most populous expansion for each attribute is omitted.for examplethe combination humidity play three times in the data but has no node in the tree.neverthe-lessit turns out that any count can be calculated from those that the tree storesexplicitly.here s a simple example.figure contains no node for humidity play shows three instances withhumidity windy one ofthem has a value for playthatis different from yes.it follows that there must be two instances for play for a trickier casehow many times does humidity play at first glance it seems impossible to tell because there isno branch for humidity can deduce the number by calcu-lating the count for windy play and subtracting the countfor humidity play gives correctvalue.this idea works for any subset ofattributes and any combination ofattrib-ute valuesbut it may have to be applied recursively.for exampleto obtain thecount for humidity play need the count forwindy and play the count for humidity play obtain the former by subtracting the count for windy play from the count for play the latter bysubtracting the count for humidity play the count for humidity play theremust be instances with humidity play is correct.ad trees only pay offifthe data contains many thousands ofinstances.it ispretty obvious that they do not help on the weather data.the fact that theyyield no benefit on small datasets means thatin practiceit makes little senseto expand the tree all the way down to the leafnodes.usuallya cutoffparam-eter kis employedand nodes covering fewer than kinstances hold a list machine learning am page to these instances rather than a list ofpointers to other nodes.this makesthe trees smaller and more efficient to use.discussionthe algorithm for learning bayesian networks was introduced by cooperand herskovits scoring metrics are covered by heckerman tan algorithm was introduced by friedman et describes multinets.grossman and domingos show how to use theconditional likelihood for scoring networks.guo and greiner present anextensive comparison ofscoring metrics for bayesian network classifiers.bouck-aert describes averaging over subnetworks.ad trees were introduced andanalyzed by moore and lee the same andrew moore whose work onkd-trees and ball trees was mentioned in section a more recent paperkomarek and moore introduce ad trees for incremental learning thatare also more efficient for datasets with many attributes.we have only skimmed the surface ofthe subject oflearning bayesian net-works.we left open questions ofmissing valuesnumeric attributesand hiddenattributes.we did not describe how to use bayesian networks for regressiontasks.bayesian networks are a special case ofa wider class ofstatistical modelscalled graphical modelswhich include networks with undirected edges markov networks.graphical models are attracting great attention in themachine learning community am page am page in the previous chapter we examined a vast array ofmachine learning methodsdecision treesdecision ruleslinear modelsinstance-based schemesnumeric pre-diction techniquesclustering algorithmsand bayesian networks.all are soundrobust techniques that are eminently applicable to practical data mining problems.but successful data mining involves far more than selecting a learning algo-rithm and running it over your data.for one thingmany learning methodshave various parametersand suitable values must be chosen for these.in mostcasesresults can be improved markedly by suitable choice ofparameter valuesand the appropriate choice depends on the data at hand.for exampledecisiontrees can be pruned or unprunedand in the former case a pruning parametermay have to be chosen.in the k-nearest-neighbor method ofinstance-basedlearninga value for kwill have to be chosen.more generallythe learningscheme itselfwill have to be chosen from the range ofschemes that are avail-able.in all casesthe right choices depend on the data itself.it is tempting to try out several learning schemesand several parametervalueson your data and see which works best.but be careful! the best the input and am page is not necessarily the one that performs best on the training data.we haverepeatedly cautioned about the problem ofoverfittingwhere a learned modelis too closely tied to the particular training data from which it was built.it isincorrect to assume that performance on the training data faithfully representsthe level ofperformance that can be expected on the fresh data to which thelearned model will be applied in practice.fortunatelywe have already encountered the solution to this problem inchapter are two good methods for estimating the expected true per-formance ofa learning schemethe use ofa large dataset that is quite separatefrom the training datain the case ofplentiful dataand cross-validationsection is scarce.in the latter casea single cross-validation is typically used in practicealthough to obtain a more reliable estimate the entire procedure should be repeated times.once suitable param-eters have been chosen for the learning schemeuse the whole training set allthe available training instances to produce the final learned model that is tobe applied to fresh data.note that the performance obtained with the chosen parameter value duringthe tuning process is nota reliable estimate ofthe final model s performancebecause the final model potentially overfits the data that was used for tuning.to ascertain how well it will performyou need yet another large dataset that isquite separate from any data used during learning and tuning.the same is truefor cross-validationyou need an inner cross-validation for parameter tuningand an outer cross-validation for error estimation.with cross-validationthis involves running the learning scheme times.to summarizewhen assessing the performance ofa learning schemeany parameter tuningthat goes on should be treated as though it were an integral part ofthe train-ing process.there are other important processes that can materially improve successwhen applying machine learning techniques to practical data mining problemsand these are the subject ofthis chapter.they constitute a kind ofdata engi-neeringengineering the input data into a form suitable for the learning schemechosen and engineering the output model to make it more effective.you canlook on them as a bag oftricks that you can apply to practical data mining prob-lems to enhance the chance ofsuccess.sometimes they workother times theydon t and at the present state ofthe artit s hard to say in advance whetherthey will or not.in an area such as this where trial and error is the most reli-able guideit is particularly important to be resourceful and understand whatthe tricks are.we begin by examining four different ways in which the input can be mas-saged to make it more amenable for learning methodsattribute selectionattribute discretizationdata transformationand data cleansing.consider thefirstattribute selection.in many practical situations there are far too the input and am page attributes for learning schemes to handleand some ofthem perhaps the over-whelming majority are clearly irrelevant or redundant.consequentlythe datamust be preprocessed to select a subset ofthe attributes to use in learning.ofcourselearning methods themselves try to select attributes appropriately andignore irrelevant or redundant onesbut in practice their performance can fre-quently be improved by preselection.for exampleexperiments show thatadding useless attributes causes the performance oflearning schemes such asdecision trees and ruleslinear regressioninstance-based learnersand cluster-ing methods to deteriorate.discretization ofnumeric attributes is absolutely essential ifthe task involvesnumeric attributes but the chosen learning method can only handle categoricalones.even methods that can handle numeric attributes often produce betterresultsor work fasterifthe attributes are prediscretized.the converse situa-tionin which categorical attributes must be represented numericallyalsooccurs less oftenand we describe techniques for this casetoo.data transformation covers a variety oftechniques.one transformationwhich we have encountered before when looking at relational data in and support vector machines in chapter to add newsynthetic attributeswhose purpose is to present existing information in a form that is suitable forthe machine learning scheme to pick up on.more general techniques that donot depend so intimately on the semantics ofthe particular data mining pro-blem at hand include principal components analysis and random projections.unclean data plagues data mining.we emphasized in chapter the neces-sity ofgetting to know your dataunderstanding the meaning ofall the differ-ent attributesthe conventions used in coding themthe significance ofmissingvalues and duplicate datameasurement noisetypographical errorsand thepresence ofsystematic errors even deliberate ones.various simple visualiza-tions often help with this task.there are also automatic methods ofcleansingdataofdetecting outliersand ofspotting anomalieswhich we describe.having studied how to massage the inputwe turn to the question ofengi-neering the output from machine learning schemes.in particularwe examinetechniques for combining different models learned from the data.there aresome surprises in store.for exampleit is often advantageous to take the train-ing data and derive several different training sets from itlearn a model fromeachand combine the resulting models! indeedtechniques for doing this canbe very powerful.it isfor examplepossible to transform a relatively weak learning method into an extremely strong one a precise sense that we willexplain.moreoverifseveral learning schemes are availableit may be advan-tageous not to choose the best-performing one for your dataset cross-validation but to use them all and combine the results.finallythe standardobvious way ofmodeling a multiclass learning situation as a two-class one canbe improved using a simple but subtle am page many ofthese results are counterintuitiveat least at first blush.how can itbe a good idea to use many different models together? how can you possiblydo better than choose the model that performs best? surely all this runs counterto occam s razorwhich advocates simplicity.how can you possibly obtain first-class performance by combining indifferent modelsas one ofthese techniquesappears to do? but consider committees ofhumanswhich often come up withwiser decisions than individual experts.recall epicurus s view thatfaced withalternative explanationsone should retain them all.imagine a group ofspe-cialists each ofwhom excels in a limited domain even though none is competentacross the board.in struggling to understand how these methods workresearchers have exposed all sorts ofconnections and links that have led to evengreater improvements.another extraordinary fact is that classification performance can often beimproved by the addition ofa substantial amount ofdata that is unlabeledinother wordsthe class values are unknown.againthis seems to fly directly inthe face ofcommon senserather like a river flowing uphill or a perpetualmotion machine.but ifit were true and it isas we will show you in it would have great practical importance because there are many situationsin which labeled data is scarce but unlabeled data is plentiful.read on andprepare to be selectionmost machine learning algorithms are designed to learn which are the mostappropriate attributes to use for making their decisions.for exampledecision tree methods choose the most promising attribute to split on at each point and should in theory never select irrelevant or unhelpful attributes.having more features should surely in theory result in more discriminating powernever less. what s the difference between theory and practice? an old question asks. there is no difference the answer goes in theory.but in practicethere is. here there istooin practiceaddingirrelevant or distracting attributes to a dataset often confuses machine learn-ing systems.experiments with a decision tree learner have shown that adding tostandard datasets a random binary attribute generated by tossing an unbiasedcoin affects classification performancecausing it to deteriorate by in the situations tested.this happens because at some point in the treesthat are learned the irrelevant attribute is invariably chosen to branch oncausing random errors when test data is processed.how can this bewhen deci-sion tree learners are cleverly designed to choose the best attribute for splittingat each node? the reason is subtle.as you proceed further down the the input and am page and less data is available to help make the selection decision.at some pointwith little datathe random attribute will look good just by chance.because thenumber ofnodes at each level increases exponentially with depththe chance ofthe rogue attribute looking good somewhere along the frontier multiplies up asthe tree deepens.the real problem is that you inevitably reach depths at whichonly a small amount ofdata is available for attribute selection.ifthe datasetwere bigger it wouldn t necessarily help you d probably just go deeper.divide-and-conquer tree learners and separate-and-conquer rule learnersboth suffer from this effect because they inexorably reduce the amount ofdataon which they base judgments.instance-based learners are very susceptible toirrelevant attributes because they always work in local neighborhoodstakingjust a few training instances into account for each decision.indeedit has beenshown that the number oftraining instances needed to produce a predeter-mined level ofperformance for instance-based learning increases exponentiallywith the number ofirrelevant attributes present.na ve bayesby contrastdoesnot fragment the instance space and robustly ignores irrelevant attributes.itassumes by design that all attributes are independent ofone anotheran assump-tion that is just right for random distracter attributes.but through this verysame assumptionna ve bayes pays a heavy price in other ways because its oper-ation is damaged by adding redundant attributes.the fact that irrelevant distracters degrade the performance ofstate-of-the-art decision tree and rule learners isat firstsurprising.even more surprisingis that relevantattributes can also be harmful.for examplesuppose that in atwo-class dataset a new attribute were added which had the same value as theclass to be predicted most ofthe time and the opposite value the rest ofthe timerandomly distributed among the instances.experiments with standarddatasets have shown that this can cause classification accuracy to deteriorate to in the situations tested.the problem is that the new attribute is chosen for splitting high up in the tree.this has the effect offragment-ing the set ofinstances available at the nodes below so that other choices arebased on sparser data.because ofthe negative effect ofirrelevant attributes on most machine learn-ing schemesit is common to precede learning with an attribute selection stagethat strives to eliminate all but the most relevant attributes.the best way toselect relevant attributes is manuallybased on a deep understanding ofthelearning problem and what the attributes actually mean.howeverautomaticmethods can also be useful.reducing the dimensionality ofthe data by delet-ing unsuitable attributes improves the performance oflearning algorithms.italso speeds them upalthough this may be outweighed by the computationinvolved in attribute selection.more importantlydimensionality reductionyields a more compactmore easily interpretable representation ofthe targetconceptfocusing the user s attention on the most relevant am page scheme-independent selectionwhen selecting a good attribute subsetthere are two fundamentally differentapproaches.one is to make an independent assessment based on general char-acteristics ofthe datathe other is to evaluate the subset using the machinelearning algorithm that will ultimately be employed for learning.the first iscalled the filtermethodbecause the attribute set is filtered to produce the most promising subset before learning commences.the second is the wrappermethodbecause the learning algorithm is wrapped into the selection proce-dure.making an independent assessment ofan attribute subset would be easyifthere were a good way ofdetermining when an attribute was relevant tochoosing the class.howeverthere is no universally accepted measure of rele-vance although several different ones have been proposed.one simple scheme-independent method ofattribute selection is to use justenough attributes to divide up the instance space in a way that separates all thetraining instances.for exampleifjust one or two attributes are usedthere willgenerally be several instances that have the same combination ofattributevalues.at the other extremethe full set ofattributes will likely distinguish theinstances uniquely so that no two instances have the same values for all attrib-utes.this will not necessarily be the casehoweverdatasets sometimes containinstances with the same attribute values but different classes. it makes intuitivesense to select the smallest attribute subset that distinguishes all instancesuniquely.this can easily be found using exhaustive searchalthough at consid-erable computational expense.unfortunatelythis strong bias toward consis-tency ofthe attribute set on the training data is statistically unwarranted andcan lead to overfitting the algorithm may go to unnecessary lengths to repairan inconsistency that was in fact merely caused by noise.machine learning algorithms can be used for attribute selection.for instanceyou might first apply a decision tree algorithm to the full datasetand then selectonly those attributes that are actually used in the tree.although this selectionwould have no effect at all ifthe second stage merely built another treeit willhave an effect on a different learning algorithm.for examplethe nearest-neighbor algorithm is notoriously susceptible to irrelevant attributesand itsperformance can be improved by using a decision tree builder as a filter forattribute selection first.the resulting nearest-neighbor method can alsoperform better than the decision tree algorithm used for filtering.as anotherexamplethe simple scheme described in chapter has been used to selectthe attributes for a decision tree learner by evaluating the effect ofbranchingon different attributes an error-based method such as may not bethe optimal choice for ranking attributesas we will see later when covering therelated problem ofsupervised discretization.often the decision tree performsjust as well when only the two or three top attributes are used for its the input and am page tion and it is much easier to understand.in this approachthe user determineshow many attributes to use for building the decision tree.another possibility is to use an algorithm that builds a linear model forexamplea linear support vector machine and ranks the attributes based on the size ofthe coefficients.a more sophisticated variant applies the learningalgorithm repeatedly.it builds a modelranks the attributes based on the coefficientsremoves the highest-ranked oneand repeats the process until all attributes have been removed.this method ofrecursive feature eliminationhas been found to yield better results on certain datasets iden-tifying important genes for cancer classification than simply ranking attrib-utes based on a single model.with both methods it is important to ensure that the attributes are measured on the same scaleotherwisethe coefficientsare not comparable.note that these techniques just produce a rankinganother method must be used to determine the appropriate number ofattrib-utes to use.attributes can be selected using instance-based learning methodstoo.youcould sample instances randomly from the training set and check neighboringrecords ofthe same and different classes near hits and near misses. ifanear hit has a different value for a certain attributethat attribute appears to beirrelevant and its weight should be decreased.on the other handifa near misshas a different valuethe attribute appears to be relevant and its weight shouldbe increased.ofcoursethis is the standard kind ofprocedure used for attrib-ute weighting for instance-based learningdescribed in section repeat-ing this operation many timesselection takes placeonly attributes with positiveweights are chosen.as in the standard incremental formulation ofinstance-based learningdifferent results will be obtained each time the process isrepeatedbecause ofthe different ordering ofexamples.this can be avoided byusing all training instances and taking into account all near hits and near missesofeach.a more serious disadvantage is that the method will not detect an attributethat is redundant because it is correlated with another attribute.in the extremecasetwo identical attributes would be treated in the same wayeither bothselected or both rejected.a modification has been suggested that appears to gosome way towards addressing this issue by taking the current attribute weightsinto account when computing the nearest hits and misses.another way ofeliminating redundant attributes as well as irrelevant ones isto select a subset ofattributes that individually correlate well with the class buthave little intercorrelation.the correlation between two nominal attributes aand bcan be measured using the symmetric am page where his the entropy function described in section entropies arebased on the probability associated with each attribute valuehabthe jointentropy ofaand bis calculated from the joint probabilities ofall combina-tions ofvalues ofaand b.the symmetric uncertainty always lies between and feature selection determines the goodness ofa set ofattributes usingwhere c is the class attribute and the indices i and j range over all attributes inthe set.ifall mattributes in the subset correlate perfectly with the class and withone anotherthe numerator becomes mand the denominator becomes is also m.hencethe measure is turns out to be the maximumvalue it can attain minimum is this is not idealbecause we wantto avoid redundant attributes.howeverany subset ofthis set will also have using this criterion to search for a good subset ofattributes it makessense to break ties in favor ofthe smallest subset.searching the attribute spacemost methods for attribute selection involve searching the space ofattributesfor the subset that is most likely to predict the class best.figure illustratesthe attribute space for the by now all-too-familiar weather dataset.thenumber ofpossible attribute subsets increases exponentially with the numberofattributesmaking exhaustive search impractical on all but the simplest problems.typicallythe space is searched greedily in one oftwo directionstop tobottom or bottom to top in the figure.at each stagea local change is made tothe current attribute subset by either adding or deleting a single attribute.thedownward directionwhere you start with no attributes and add them one at atimeis called forward selection.the upward onewhere you start with the fullset and delete attributes one at a timeis backward elimination.in forward selectioneach attribute that is not already in the current subsetis tentatively added to it and the resulting set ofattributes is evaluated usingfor examplecross-validation as described in the following section.this evalu-ation produces a numeric measure ofthe expected performance ofthe subset.the effect ofadding each attribute in turn is quantified by this measurethe bestone is chosenand the procedure continues.howeverifno attribute producesan improvement when added to the current subsetthe search ends.this is astandard greedy search procedure and guarantees to find a locally but not nec-essarily globally optimal set ofattributes.backward elimination operates inan entirely analogous fashion.in both cases a slight bias is often the input and am page toward smaller attribute sets.this can be done for forward selection by insist-ing that ifthe search is to continuethe evaluation measure must not onlyincrease but also must increase by at least a small predetermined quantity.asimilar modification works for backward elimination.more sophisticated search methods exist.forward selection and backwardelimination can be combined into a bidirectional searchagain one can eitherbegin with all the attributes or with none ofthem.best-first search is a methodthat does not just terminate when the performance starts to drop but keeps alist ofall attribute subsets evaluated so farsorted in order ofthe performancemeasureso that it can revisit an earlier configuration instead.given enoughtime it will explore the entire spaceunless this is prevented by some kind ofstopping criterion.beam search is similar but truncates its list ofattributesubsets at each stage so that it only contains a fixed number the beam width space for the weather am page ofmost promising candidates.genetic algorithm search procedures are looselybased on the principal ofnatural selectionthey evolve good feature subsetsby using random perturbations ofa current list ofcandidate subsets.scheme-specific selectionthe performance ofan attribute subset with scheme-specific selection is meas-ured in terms ofthe learning scheme s classification performance using justthose attributes.given a subset ofattributesaccuracy is estimated using thenormal procedure ofcross-validation described in section methods such as performance on a holdout set or thebootstrap estimator could equally well be used.the entire attribute selection process is computation intensive.ifeach eval-uation involves a cross-validationthe learning procedure must be exe-cuted times.with kattributesthe heuristic forward selection or backwardelimination multiplies evaluation time by a factor ofup to and for moresophisticated searchesthe penalty will be far greaterup to an exhaustivealgorithm that examines each ofthe subsets.good results have been demonstrated on many datasets.in general termsbackward elimination produces larger attribute setsand better classificationaccuracythan forward selection.the reason is that the performance measureis only an estimateand a single optimistic estimate will cause both ofthesesearch procedures to halt prematurely backward elimination with too manyattributes and forward selection with not enough.but forward selection is usefulifthe focus is on understanding the decision structures involvedbecause it oftenreduces the number ofattributes with only a very small effect on classificationaccuracy.experience seems to show that more sophisticated search techniquesare not generally justified although they can produce much better results incertain cases.one way to accelerate the search process is to stop evaluating a subset ofattributes as soon as it becomes apparent that it is unlikely to lead to higheraccuracy than another candidate subset.this is a job for a paired statistical sig-nificance testperformed between the classifier based on this subset and all theother candidate classifiers based on other subsets.the performance differencebetween two classifiers on a particular test instance can be taken to be depending on whether the first classifier is worsethe same asor better thanthe second on that instance.a paired t-test in section can beapplied to these figures over the entire test seteffectively treating the results foreach instance as an independent estimate ofthe difference in performance.thenthe cross-validation for a classifier can be prematurely terminated as soon as itturns out to be significantly worse than another whichofcoursemay neverhappen.we might want to discard classifiers more aggressively by the input and am page the t-test to compute the probability that one classifier is better than anotherclassifier by at least a small user-specified threshold.ifthis probability becomesvery smallwe can discard the former classifier on the basis that it is very unlikelyto perform substantially better than the latter.this methodology is called race searchand can be implemented with differ-ent underlying search strategies.when used with forward selectionwe race allpossible single-attribute additions simultaneously and drop those that do notperform well enough.in backward eliminationwe race all single-attribute dele-tions.schemata searchis a more complicated method specifically designed forracingit runs an iterative series ofraces that each determine whether or not aparticular attribute should be included.the other attributes for this race areincluded or excluded randomly at each point in the evaluation.as soon as onerace has a clear winnerthe next iteration ofraces beginsusing the winner asthe starting point.another search strategy is to rank the attributes firstusingfor exampletheir information gain they are discreteand then racethe ranking.in this case the race includes no attributesthe top-ranked attrib-utethe top two attributesthe top threeand so on.whatever way you do itscheme-specific attribute selection by no meansyields a uniform improvement in performance.because ofthe complexity ofthe processwhich is greatly increased by the feedback effect ofincluding a targetmachine learning algorithm in the attribution selection loopit is quite hard topredict the conditions under which it will turn out to be worthwhile.as in manymachine learning situationstrial and error using your own particular source ofdata is the final arbiter.there is one type ofclassifier for which scheme-specific attribute selection isan essential part ofthe learning processthe decision table.as mentioned insection entire problem oflearning decision tables consists ofselectingthe right attributes to include.usually this is done by measuring the table scross-validation performance for different subsets ofattributes and choosingthe best-performing subset.fortunatelyleave-one-out cross-validation is verycheap for this kind ofclassifier.obtaining the cross-validation error from a deci-sion table derived from the training data is just a matter ofmanipulating theclass counts associated with each ofthe table s entriesbecause the table s struc-ture doesn t change when instances are added or deleted.the attribute space isgenerally searched by best-first search because this strategy is less likely tobecome stuck in a local maximum than otherssuch as forward selection.let s end our discussion with a success story.one learning method for whicha simple scheme-specific attribute selection approach has shown good results isna ve bayes.although this method deals well with random attributesit has thepotential to be misled when there are dependencies among attributesand par-ticularly when redundant ones are added.howevergood results have beenreported using the forward selection algorithm which is better able to am page when a redundant attribute is about to be added than the backward elimina-tion approach in conjunction with a very simplealmost na ve metric thatdetermines the quality ofan attribute subset to be simply the performance ofthe learned algorithm on the trainingset.as was emphasized in chapter set performance is certainly not a reliable indicator oftest-set performance.neverthelessexperiments show that this simple modification to na ve bayesmarkedly improves its performance on those standard datasets for which it doesnot do so well as tree- or rule-based classifiersand does not have any negativeeffect on results on datasets on which na ve bayes already does well.selectivena ve bayesas this learning method is calledis a viable machine learning tech-nique that performs reliably and well in numeric attributessome classification and clustering algorithms deal with nominal attributes onlyand cannot handle ones measured on a numeric scale.to use them on generaldatasetsnumeric attributes must first be discretized into a small number ofdistinct ranges.even learning algorithms that do handle numeric attributessometimes process them in ways that are not altogether satisfactory.statisticalclustering methods often assume that numeric attributes have a normal distri-bution often not a very plausible assumption in practice and the standardextension ofthe na ve bayes classifier to handle numeric attributes adopts thesame assumption.although most decision tree and decision rule learners canhandle numeric attributessome implementations work much more slowlywhen numeric attributes are present because they repeatedly sort the attributevalues.for all these reasons the question ariseswhat is a good way to discretizenumeric attributes into ranges before any learning takes place?we have already encountered some methods for discretizing numeric attrib-utes.the learning scheme described in chapter uses a simple but effectivetechniquesort the instances by the attribute s value and assign the value intoranges at the points that the class value changes except that a certain minimumnumber ofinstances in the majority class must lie in each ofthe rangeswhich means that any given range may include a mixture ofclass values.thisis a global method ofdiscretization that is applied to all continuous attributesbefore learning starts.decision tree learnerson the other handdeal with numeric attributes on alocal basisexamining attributes at each node ofthe tree when it is being con-structed to see whether they are worth branching on and only at that pointdeciding on the best place to split continuous attributes.although the tree-building method we examined in chapter only considers binary splits ofcon-tinuous attributesone can imagine a full discretization taking place at the input and am page numeric a multiway split on a numeric attribute.the pros and cons ofthe local versus the global approach are clear.local discretization is tailored tothe actual context provided by each tree nodeand will produce different dis-cretizations ofthe same attribute at different places in the tree ifthat seemsappropriate.howeverits decisions are based on less data as tree depth increaseswhich compromises their reliability.iftrees are developed all the way out tosingle-instance leaves before being pruned backas with the normal techniqueofbackward pruningit is clear that many discretization decisions will be basedon data that is grossly inadequate.when using global discretization before applying a learning methodthereare two possible ways ofpresenting the discretized data to the learner.the mostobvious is to treat discretized attributes like nominal oneseach discretizationinterval is represented by one value ofthe nominal attribute.howeverbecausea discretized attribute is derived from a numeric oneits values are orderedandtreating it as nominal discards this potentially valuable ordering information.ofcourseifa learning scheme can handle ordered attributes directlythe solu-tion is obviouseach discretized attribute is declared to be oftype ordered. ifthe learning method cannot handle ordered attributesthere is still a simpleway ofenabling it to exploit the ordering informationtransform each dis-cretized attribute into a set ofbinary attributes before the learning scheme isapplied.assuming the discretized attribute has kvaluesit is transformed binary attributesthe first ofwhich are set to falsewhenever the ithvalue ofthe discretized attribute is present in the data and to trueotherwise.the remaining attributes are set to false.in other wordsthe binaryattribute represents whether the discretized attribute is less than i.ifa decisiontree learner splits on this attributeit implicitly uses the ordering informationit encodes.note that this transformation is independent ofthe particular dis-cretization method being appliedit is simply a way ofcoding an ordered attrib-ute using a set ofbinary attributes.unsupervised discretizationthere are two basic approaches to the problem ofdiscretization.one is to quan-tize each attribute in the absence ofany knowledge ofthe classes ofthe instancesin the training set so-called unsupervised discretization.the other is to takethe classes into account when discretizing supervised discretization.theformer is the only possibility when dealing with clustering problems in whichthe classes are unknown or nonexistent.the obvious way ofdiscretizing a numeric attribute is to divide its range intoa predetermined number ofequal intervalsa fixeddata-independent yardstick.this is frequently done at the time when data is collected.butlike any unsu-pervised discretization methodit runs the risk ofdestroying distinctions am page would have turned out to be useful in the learning process by using gradationsthat are too coarse or by unfortunate choices ofboundary that needlessly lumptogether many instances ofdifferent classes.equal-interval binning often distributes instances very unevenlysome binscontain many instancesand others contain none.this can seriously impair theability ofthe attribute to help to build good decision structures.it is often betterto allow the intervals to be ofdifferent sizeschoosing them so that the samenumber oftraining examples fall into each one.this methodequal-frequencybinningdivides the attribute s range into a predetermined number ofbins basedon the distribution ofexamples along that axis sometimes called histogramequalizationbecause ifyou take a histogram ofthe contents ofthe resultingbins it will be completely flat.ifyou view the number ofbins as a resourcethismethod makes best use ofit.howeverequal-frequency binning is still oblivious to the instances classesand this can cause bad boundaries.for exampleifall instances in a bin haveone classand all instances in the next higher bin have another except for thefirstwhich has the original classsurely it makes sense to respect the class divisions and include that first instance in the previous binsacrificing the equal-frequency property for the sake ofhomogeneity.supervised discretization taking classes into account during the process certainly has advantages.neverthelessit has been found that equal-frequency binning can yield excellentresultsat least in conjunction with the na ve bayes learning schemewhen thenumber ofbins is chosen in a data-dependent fashion by setting it to the squareroot ofthe number ofinstances.this method is called proportional k-intervaldiscretization.entropy-based discretizationbecause the criterion used for splitting a numeric attribute during the forma-tion ofa decision tree works well in practiceit seems a good idea to extend itto more general discretization by recursively splitting intervals until it is timeto stop.in chapter we saw how to sort the instances by the attribute s valueand considerfor each possible splitting pointthe information gain oftheresulting split.to discretize the attributeonce the first split is determined thesplitting process can be repeated in the upper and lower parts ofthe rangeandso onrecursively.to see this working in practicewe revisit the example on page for dis-cretizing the temperature attribute ofthe weather datawhose values the input and am page numeric values have been collapsed together. the information gain for eachofthe possible positions for the breakpoint is calculated in the usual way.for examplethe information value ofthe test splitsthe range into fouryes s and two no s versus fiveyes s and three no sisthis represents the amount ofinformation required to specify the individualvalues ofyesand nogiven the split.we seek a discretization that makes thesubintervals as pure as possiblehencewe choose to split at the point where theinformation value is smallest.this is the same as splitting where the informa-tion gaindefined as the difference between the information value without thesplit and that with the splitis largest. as beforewe place numeric thresholdshalfway between the values that delimit the boundaries ofa concept.the graph labeled a in figure shows the information values at each pos-sible cut point at this first stage.the cleanest division smallest informationvalue is at a temperature bitswhich separates offjust the veryfinal valuea noinstancefrom the preceding list.the instance classes are writtenbelow the horizontal axis to make interpretation easier.invoking the algorithmagain on the lower range oftemperaturesfrom to the graph labeledb.this has a minimum at bitswhich splits offthe next two the temperatureattribute using the entropy am page both yesinstances.again invoking the algorithm on the lower rangenow to the graph labeled c dotted to help distinguish it fromthe others.the minimum is at bitssplitting offanother noinstance.graph d has a minimum at bitssplitting offtwo yesinstances.graph e dashedpurely to make it more easily visiblefor thetemperature range to a minimum at bitswhich splitsofftwo nos and a yes.finallygraph ffor the range to a minimumat bits.the final discretization ofthe temperatureattribute is shown in figure fact that recursion only ever occurs in the first interval ofeach split is anartifact ofthis examplein generalboth the upper and the lower intervals willhave to be split further.underneath each division is the label ofthe graph infigure that is responsible for itand below that is the actual value ofthe splitpoint.it can be shown theoretically that a cut point that minimizes the informa-tion value will never occur between two instances ofthe same class.this leadsto a useful optimizationit is only necessary to consider potential divisions thatseparate instances ofdifferent classes.notice that ifclass labels were assigned tothe intervals based on the majority class in the intervalthere would be no guar-antee that adjacent intervals would receive different labels.you might betempted to consider merging intervals with the same majority class two intervals offigure as we will see later this isnot a good thing to do in general.the only problem left to consider is the stopping criterion.in the tempera-ture example most ofthe intervals that were identified were pure in that alltheir instances had the same classand there is clearly no point in trying to splitsuch an interval.exceptions were the final intervalwhich we tacitly decidednot to splitand the interval from to in generalhoweverthings arenot so the input and result ofdiscretizing the am page numeric good way to stop the entropy-based splitting discretization procedure turnsout to be the mdl principle that we encountered in chapter accordancewith that principlewe want to minimize the size ofthe theory plus the sizeofthe information necessary to specify all the data given that theory.in thiscaseifwe do splitthe theory is the splitting pointand we are comparing thesituation in which we split with that in which we do not.in both cases we assumethat the instances are known but their class labels are not.ifwe do not splittheclasses can be transmitted by encoding each instance s label.ifwe dowe firstencode the split point bitswhere nis the number ofinstancesthen the classes ofthe instances below that pointand then the classes ofthoseabove it.you can imagine that ifthe split is a good one sayall the classes belowit are yesand all those above are no then there is much to be gained by split-ting.ifthere is an equal number ofyesand noinstanceseach instance costs without splitting but hardly more than bits with splitting it is not because the class values associated with the split itselfmust be encodedbutthis penalty is amortized across all the instances.in this caseifthere are manyexamplesthe penalty ofhaving to encode the split point will be far outweighedby the information saved by splitting.we emphasized in section that when applying the mdl principlethedevil is in the details.in the relatively straightforward case ofdiscretizationthesituation is tractable although not simple.the amounts ofinformation can beobtained exactly under certain reasonable assumptions.we will not go into thedetailsbut the upshot is that the split dictated by a particular cut point is worth-while ifthe information gain for that split exceeds a certain value that dependson the number ofinstances nthe number ofclasses kthe entropy oftheinstances ethe entropy ofthe instances in each subinterval thenumber ofclasses represented in each subinterval first component is the information needed to specify the splitting pointthe second is a correction due to the need to transmit which classes correspondto the upper and lower subintervals.when applied to the temperature examplethis criterion prevents any split-ting at all.the first split removes just the final exampleand as you can imaginevery little actual information is gained by this when transmitting the classes in factthe mdl criterion will never create an interval containing just oneexample.failure to discretize temperatureeffectively disbars it from playing anyrole in the final decision structure because the same discretized value will begiven to all instances.in this situationthis is perfectly appropriatethe am page atureattribute does not occur in good decision trees or rules for the weatherdata.in effectfailure to discretize is tantamount to attribute selection.other discretization methodsthe entropy-based method with the mdl stopping criterion is one ofthe bestgeneral techniques for supervised discretization.howevermany other methodshave been investigated.for exampleinstead ofproceeding top-down by recur-sively splitting intervals until some stopping criterion is satisfiedyou couldwork bottom-upfirst placing each instance into its own interval and then con-sidering whether to merge adjacent intervals.you could apply a statistical crite-rion to see which would be the best two intervals to mergeand merge them ifthe statistic exceeds a certain preset confidence levelrepeating the operationuntil no potential merge passes the test.the is a suitable one and hasbeen used for this purpose.instead ofspecifying a preset significance thresholdmore complex techniques are available to determine an appropriate level automatically.a rather different approach is to count the number oferrors that a dis-cretization makes when predicting each training instance s classassuming thateach interval receives the majority class.for examplethe method describedearlier is error based it focuses on errors rather than the entropy.howeverthe best possible discretization in terms oferror count is obtained by using thelargest possible number ofintervalsand this degenerate case should be avoidedby restricting the number ofintervals in advance.for exampleyou might askwhat is the best way to discretize an attribute into kintervals in a way that min-imizes the number oferrors?the brute-force method offinding the best way ofpartitioning an attributeinto kintervals in a way that minimizes the error count is exponential in kandhence infeasible.howeverthere are much more efficient schemes that are basedon the idea ofdynamic programming.dynamic programming applies not justto the error count measure but also to any given additive impurity functionandit can find the partitioning ofninstances into kintervals in a way that mini-mizes the impurity in time proportional to gives a way offinding thebest entropy-based discretizationyielding a potential improvement in thequality ofthe discretization in practice a negligible one over the recursiveentropy-based method described previously.the news for error-based dis-cretization is even betterbecause there is a method that minimizes the errorcount in time linear in n.entropy-based versus error-based discretizationwhy not use error-based discretizationsince the optimal discretization can befound very quickly? the answer is that there is a serious drawback to the input and am page numeric cannot produce adjacent intervals with the same label asthe first two offigure reason is that merging two such intervals willnot affect the error count but it will free up an interval that can be used else-where to reduce the error count.why would anyone want to generate adjacent intervals with the same label?the reason is best illustrated with an example.figure shows the instancespace for a simple two-class problem with two numeric attributes ranging to belong to one class dots iftheir first attribute is lessthan or ifit is less than andtheir second attribute is less than belong to the other class data in figure hasbeen artificially generated according to this rule.now suppose we are trying to discretize both attributes with a view to learn-ing the classes from the discretized attributes.the very best discretization into three intervals through through through into two intervals through and through these distribution for a two-classtwo-attribute am page attributesit will be easy to learn how to tell the classes apart with a simple deci-sion tree or rule algorithm.discretizing is no problem.for and last intervals will have opposite labels trianglerespectively.the second will have whichever label happens to occur most in the region through is in fact dotfor the data in figure waythis labelmust inevitably be the same as one ofthe adjacent labels ofcourse this is truewhatever the class probability happens to be in the middle region.thus this dis-cretization will not be achieved by any method that minimizes the error countsbecause such a method cannot produce adjacent intervals with the same label.the point is that what changes as the value crosses the boundary at not the majority class but the class distribution.the majority class remainsdot.the distributionhoweverchanges markedlyfrom before the bound-ary to just over after it.and the distribution changes again as the bound-ary at is crossedfrom to discretization methodsare sensitive to changes in the distribution even though the majority class doesnot change.error-based methods are not.converting discrete to numeric attributesthere is a converse problem to discretization.some learning algorithms notably the nearest-neighbor instance-based method and numeric predictiontechniques involving regression naturally handle only attributes that arenumeric.how can they be extended to nominal attributes?in instance-based learningas described in section attributes canbe treated as numeric by defining the distance between two nominal valuesthat are the same as and between two values that are different as regard-less ofthe actual values involved.rather than modifying the distance functionthis can be achieved using an attribute transformationreplace a k-valuednominal attribute with ksynthetic binary attributesone for each value indi-cating whether the attribute has that value or not.ifthe attributes have equalweightthis achieves the same effect on the distance function.the distance isinsensitive to the attribute values because only same or different informa-tion is encodednot the shades ofdifference that may be associated with thevarious possible values ofthe attribute.more subtle distinctions can be made ifthe attributes have weights reflecting their relative importance.ifthe values ofthe attribute can be orderedmore possibilities arise.for anumeric prediction problemthe average class value corresponding to eachvalue ofa nominal attribute can be calculated from the training instances andused to determine an ordering this technique was introduced for model trees in section is hard to come up with an analogous way oforderingattribute values for a classification problem. an ordered nominal attribute can be replaced with an integer in the obvious way but this implies not the input and am page useful ordering but also a metric on the attribute s values.the implication ofa metric can be avoided by creating synthetic binary attributes for a k-valued nominal attributein the manner described on page encod-ing still implies an ordering among different values ofthe attribute adjacentvalues differ in just one ofthe synthetic attributeswhereas distant ones differ in several but it does not imply an equal distance between the useful transformationsresourceful data miners have a toolbox full oftechniquessuch as discretiza-tionfor transforming data.as we emphasized in section mining ishardly ever a matter ofsimply taking a dataset and applying a learning algo-rithm to it.every problem is different.you need to think about the data andwhat it meansand examine it from diverse points ofview creatively! toarrive at a suitable perspective.transforming it in different ways can help youget started.you don t have to make your own toolbox by implementing the techniquesyourself.comprehensive environments for data miningsuch as the onedescribed in part ii ofthis bookcontain a wide range ofsuitable tools for youto use.you do not necessarily need a detailed understanding ofhow they areimplemented.what you do need is to understand what the tools do and howthey can be applied.in part ii we listand briefly describeall the transforma-tions in the weka data mining workbench.data often calls for general mathematical transformations ofa set ofattrib-utes.it might be useful to define new attributes by applying specified mathe-matical functions to existing ones.two dateattributes might be subtracted togive a third attribute representing age an example ofa semantic transforma-tion driven by the meaning ofthe original attributes.other transformationsmight be suggested by known properties ofthe learning algorithm.ifa linearrelationship involving two attributesa and bis suspectedand the algorithmis only capable ofaxis-parallel splits most decision tree and rule learnersarethe ratio ab might be defined as a new attribute.the transformations arenot necessarily mathematical ones but may involve world knowledge such asdays ofthe weekcivic holidaysor chemical atomic numbers.they could beexpressed as operations in a spreadsheet or as functions that are implementedby arbitrary computer programs.or you can reduce several nominal attributesto one by concatenating their valuesproducing a single attrib-ute from attributes with converts anumeric attribute to nominaland we saw earlier how to convert in the otherdirection am page as another kind oftransformationyou might apply a clustering procedureto the dataset and then define a new attribute whose value for any given instanceis the cluster that contains it using an arbitrary labeling for clusters.alterna-tivelywith probabilistic clusteringyou could augment each instance with itsmembership probabilities for each clusterincluding as many new attributes asthere are clusters.sometimes it is useful to add noise to dataperhaps to test the robustness ofa learning algorithm.to take a nominal attribute and change a given percent-age ofits values.to obfuscate data by renaming the relationattribute namesand nominal and string attribute values because it is often necessary toanonymize sensitive datasets.to randomize the order ofinstances or produce arandom sample ofthe dataset by resampling it.to reduce a dataset by remov-ing a given percentage ofinstancesor all instances that have certain values fornominal attributesor numeric values above or below a certain threshold.or toremove outliers by applying a classification method to the dataset and deletingmisclassified instances.different types ofinput call for their own transformations.ifyou can inputsparse data files section may need to be able to convert datasetsto a nonsparse formand vice versa.textual input and time series input call fortheir own specialized conversionsdescribed in the subsections that follow.butfirst we look at two general techniques for transforming data with numericattributes into a lower-dimensional form that may be more useful for datamining.principal components analysisin a dataset with knumeric attributesyou can visualize the data as a cloud ofpoints in k-dimensional space the stars in the skya swarm offlies frozen intimea two-dimensional scatter plot on paper.the attributes represent the co-ordinates ofthe space.but the axes you usethe coordinate system itselfis arbi-trary.you can place horizontal and vertical axes on the paper and represent thepoints ofthe scatter plot using those coordinatesor you could draw an arbi-trary straight line to represent the x-axis and one perpendicular to it to repre-sent y.to record the positions ofthe flies you could use a conventionalcoordinate system with a north south axisan east west axisand an up downaxis.but other coordinate systems would do equally well.creatures such as fliesdon t know about northsoutheastand west althoughbeing subject togravitythey may perceive up down as being something special.as for the starsin the skywho s to say what the right coordinate system is? over the centuriesour ancestors moved from a geocentric perspective to a heliocentric one to apurely relativistic oneeach shift ofperspective being accompanied by the input and am page useful religious scientific upheavals and painful reexamination ofhumankind srole in god s universe.back to the dataset.just as in these examplesthere is nothing to stop youtransforming all the data points into a different coordinate system.but unlikethese examplesin data mining there often isa preferred coordinate systemdefined not by some external convention but by the very data itself.whatevercoordinates you usethe cloud ofpoints has a certain variance in each direc-tionindicating the degree ofspread around the mean value in that direction.it is a curious fact that ifyou add up the variances along each axis and thentransform the points into a different coordinate system and do the same thereyou get the same total variance in both cases.this is always true provided thatthe coordinate systems are orthogonalthat iseach axis is at right angles to theothers.the idea ofprincipal components analysis is to use a special coordinatesystem that depends on the cloud ofpoints as followsplace the first axis in thedirection ofgreatest variance ofthe points to maximize the variance along thataxis.the second axis is perpendicular to it.in two dimensions there is nochoice its direction is determined by the first axis but in three dimensionsit can lie anywhere in the plane perpendicular to the first axisand in higherdimensions there is even more choicealthough it is always constrained to beperpendicular to the first axis.subject to this constraintchoose the second axisin the way that maximizes the variance along it.continuechoosing each axisto maximize its share ofthe remaining variance.how do you do this? it s not hardgiven an appropriate computer programand it s not hard to understandgiven the appropriate mathematical tools.tech-nically for those who understand the italicized terms you calculate thecovariance matrixofthe original coordinates ofthe points and diagonalizeit tofind the eigenvectors.these are the axes ofthe transformed spacesorted in orderofeigenvalue because each eigenvalue gives the variance along its axis.figure shows the result oftransforming a particular dataset with attributescorresponding to points in space.imaginethe original dataset as a cloud ofpoints in dimensions we can t draw it!choose the first axis along the direction ofgreatest variancethe second per-pendicular to it along the direction ofnext greatest varianceand so on.thetable gives the variance along each new coordinate axis in the order in whichthe axes were chosen.because the sum ofthe variances is constant regardless ofthe coordinate systemthey are expressed as percentages ofthat total.we callaxes componentsand say that each one accounts for its share ofthe variance.figure plots the variance that each component accounts for against thecomponent s number.you can use all the components as new attributes for dataminingor you might want to choose just the first fewthe principal am page and discard the rest.in this casethree principal components account for variance in the datasetseven account for more than numeric datasets it is common to use principal components analysisbefore data mining as a form ofdata cleanup and attribute generation.forexampleyou might want to replace the numeric attributes with the principalcomponent axes or with a subset ofthem that accounts for a given proportion ofthe variance.note that the scale ofthe attributes affects the input and of numberbfigure components transform ofa dataseta variance ofeach compo-nent and variance am page useful ofprincipal components analysisand it is common practice to stan-dardize all attributes to zero mean and unit variance first.another possibility is to apply principal components analysis recursively ina decision tree learner.at each stage an ordinary decision tree learner choosesto split in a direction that is parallel to one ofthe axes.howeversuppose a prin-cipal components transform is performed firstand the learner chooses an axisin the transformed space.this equates to a split along an oblique line in theoriginal space.ifthe transform is performed afresh before each splitthe resultwill be a multivariate decision tree whose splits are in directions that are notparallel with the axes or with one another.random projectionsprincipal components analysis transforms the data linearly into a lower-dimensional space.but it s expensive.the time taken to find the trans-formation is a matrix comprising the eigenvectors ofthe covariancematrix is cubic in the number ofdimensions.this makes it infeasible fordatasets with a large number ofattributes.a far simpler alternative is to use arandom projection ofthe data into a subspace with a predetermined numberofdimensions.it s very easy to find a random projection matrix.but will it beany good?in facttheory shows that random projections preserve distance relationshipsquite well on average.this means that they could be used in conjunction withkd-trees or ball trees to do approximate nearest-neighbor search in spaces witha huge number ofdimensions.first transform the data to reduce the numberofattributesthen build a tree for the transformed space.in the case ofnearest-neighbor classification you could make the result more stableand less depend-ent on the choice ofrandom projectionby building an ensemble classifier thatuses multiple random matrices.not surprisinglyrandom projections perform worse than ones carefullychosen by principal components analysis when used to preprocess data for arange ofstandard classifiers.howeverexperimental results have shown that thedifference is not too great and that it tends to decrease as the number ofdimensions increase.and ofcourserandom projections are far cheaper computationally.text to attribute vectorsin section we introduced string attributes that contain pieces oftext andremarked that the value ofa string attribute is often an entire document.stringattributes are basically nominalwith an unspecified number ofvalues.iftheyare treated simply as nominal attributesmodels can be built that depend onwhether the values oftwo string attributes are equal or not.but that does am page capture any internal structure ofthe string or bring out any interesting aspectsofthe text it represents.you could imagine decomposing the text in a string attribute into paragraphssentencesor phrases.generallyhoweverthe word is the most useful unit.thetext in a string attribute is usually a sequence ofwordsand is often best repre-sented in terms ofthe words it contains.for exampleyou might transform thestring attribute into a set ofnumeric attributesone for each wordthat repre-sent how often the word appears.the set ofwords that isthe set ofnew attrib-utes is determined from the dataset and is typically quite large.ifthere areseveral string attributes whose properties should be treated separatelythe newattribute names must be distinguishedperhaps by a user-determined prefix.conversion into words tokenization is not such a simple operation as itsounds.tokens may be formed from contiguous alphabetic sequences with non-alphabetic characters discarded.ifnumbers are presentnumeric sequences maybe retained too.numbers may involve contain decimal pointsand may have exponential notation in other wordsthey must be parsedaccording to a defined number syntax.an alphanumeric sequence may beregarded as a single token.perhaps the space character is the token delimiterperhaps white space the tab and new-line characters is the delim-iterand perhaps punctuation istoo.periods can be difficultsometimes theyshould be considered part ofthe word initialstitlesabbreviationsand numbersbut sometimes they should not are sentence delim-iters.hyphens and apostrophes are similarly problematic.all words may be converted to lowercase before being added to the diction-ary.words on a fixedpredetermined list offunction words or stopwords suchas theandand but could be ignored.note that stopword lists are languagedependent.in factso are capitalization conventions capitalizes allnounsnumber syntax use the comma for a decimal pointpunc-tuation conventions has an initial question markandofcoursechar-acter sets.text is complicated!low-frequency words such as hapax often discardedtoo.sometimes it is found beneficial to keep the most frequent kwords after stop-words have been removed or perhaps the top kwords for each class.along with all these tokenization optionsthere is also the question ofwhat the value ofeach word attribute should be.the value may be the wordcount the number oftimes the word appears in the string or it may simplyindicate the word s presence or absence.word frequencies could be normalized togive each document s attribute vector the same euclidean the input and hapax legomenais a word that only occurs once in a given corpus am page useful frequencies fijfor word i in document j can be transformed in various stan-dard ways.one standard logarithmic term frequency measure is log that is widely used in information retrieval is tf idfor term fre-quency times inverse document frequency. herethe term frequency is modu-lated by a factor that depends on how commonly the word is used in otherdocuments.the tf idf metric is typically defined asthe idea is that a document is basically characterized by the words that appearoften in itwhich accounts for the first factorexcept that words used in everydocument or almost every document are useless as discriminatorswhichaccounts for the second.tf idf is used to refer not just to this particularformula but also to a general class ofmeasures ofthe same type.for examplethe frequency factor fijmay be replaced by a logarithmic term such as log seriesin time series dataeach instance represents a different time step and the attrib-utes give values associated with that time such as in weather forecasting or stock market prediction.you sometimes need to be able to replace anattribute s value in the current instance with the corresponding value in some other instance in the past or the future.it is even more common to replacean attribute s value with the difference between the current value and the value in some previous instance.for examplethe difference often called thedelta between the current value and the preceding one is often more informative than the value itself.the first instancein which the time-shiftedvalue is unknownmay be removedor replaced with a missing value.the deltavalue is essentially the first derivative scaled by some constant that depends on the size ofthe time step.successive delta transformations take higher derivatives.in some time seriesinstances do not represent regular samplesbut the timeofeach instance is given by a timestampattribute.the difference between time-stamps is the step size for that instanceand ifsuccessive differences are takenfor other attributes they should be divided by the step size to normalize thederivative.in other cases each attribute may represent a different timeratherthan each instanceso that the time series is from one attribute to the next ratherthan from one instance to the next.thenifdifferences are neededthey mustbe taken between one attribute s value and the next attribute s value for eachinstance.fiijlog.number of documentsnumber of documents that include am page data cleansinga problem that plagues practical data mining is poor quality ofthe data.errorsin large databases are extremely common.attribute valuesand class values tooare frequently unreliable and corrupted.although one way ofaddressing thisproblem is to painstakingly check through the datadata mining techniquesthemselves can sometimes help to solve the problem.improving decision treesit is a surprising fact that decision trees induced from training data can oftenbe simplifiedwithout loss ofaccuracyby discarding misclassified instancesfrom the training setrelearningand then repeating until there are no misclas-sified instances.experiments on standard datasets have shown that this hardlyaffects the classification accuracy standard decision tree inductionscheme.in some cases it improves slightlyin others it deteriorates slightly.thedifference is rarely statistically significant and even when it isthe advantagecan go either way.what the technique does affect is decision tree size.the result-ing trees are invariably much smaller than the original oneseven though theyperform about the same.what is the reason for this? when a decision tree induction method prunesaway a subtreeit applies a statistical test that decides whether that subtree is justified by the data.the decision to prune accepts a small sacrifice in classi-fication accuracy on the training set in the beliefthat this will improve test-setperformance.some training instances that were classified correctly by theunpruned tree will now be misclassified by the pruned one.in effectthe deci-sion has been taken to ignore these training instances.but that decision has only been applied locallyin the pruned subtree.itseffect has not been allowed to percolate further up the treeperhaps resultingin different choices being made ofattributes to branch on.removing the mis-classified instances from the training set and relearning the decision tree is justtaking the pruning decisions to their logical conclusion.ifthe pruning strategyis a good onethis should not harm performance.it may even improve it byallowing better attribute choices to be made.it would no doubt be even better to consult a human expert.misclassifiedtraining instances could be presented for verificationand those that were foundto be wrong could be deleted or better stillcorrected.notice that we are assuming that the instances are not misclassified in anysystematic way.ifinstances are systematically corrupted in both training andtest sets for exampleone class value might be substituted for another it isonly to be expected that training on the erroneous training set would yield betterperformance on the erroneous test the input and am page data enoughit has been shown that when artificial noise is added toattributes than to classestest-set performance is improved ifthe samenoise is added in the same way to the training set.in other wordswhen attrib-ute noise is the problem it is not a good idea to train on a clean set ifper-formance is to be assessed on a dirty one.a learning method can learn tocompensate for attribute noisein some measureifgiven a chance.in essenceit can learn which attributes are unreliable andifthey are all unreliablehowbest to use them together to yield a more reliable result.to remove noise fromattributes for the training set denies the opportunity to learn how best to combatthat noise.but with class noise than attribute noiseit is best to trainon noise-free instances ifpossible.robust regressionthe problems caused by noisy data have been known in linear regression foryears.statisticians often check data for outliers and remove them manually.inthe case oflinear regressionoutliers can be identified visually although it isnever completely clear whether an outlier is an error or just a surprisingbutcorrectvalue.outliers dramatically affect the usual least-squares regressionbecause the squared distance measure accentuates the influence ofpoints faraway from the regression line.statistical methods that address the problem ofoutliers are called robust.oneway ofmaking regression more robust is to use an absolute-value distancemeasure instead ofthe usual squared one.this weakens the effect ofoutliers.another possibility is to try to identify outliers automatically and remove themfrom consideration.for exampleone could form a regression line and thenremove from consideration those ofpoints that lie furthest from the line.a third possibility is to minimize the medianrather than the mean ofthesquares ofthe divergences from the regression line.it turns out that this esti-mator is very robust and actually copes with outliers in the x-direction as well as outliers in the y-direction which is the normal direction one thinks ofoutliers.a dataset that is often used to illustrate robust regression is the graph ofinter-national telephone calls made from belgium from to in data is taken from the belgian statistical survey published by the min-istry ofeconomy.the plot seems to show an upward trend over the yearsbutthere is an anomalous group ofpoints from to turns out thatduring this periodresults were mistakenly recorded in the total number ofminutesofthe calls.the years and are also partially affected.thiserror causes a large fraction ofoutliers in the y-direction.not surprisinglythe usual least-squares regression line is seriously affectedby this anomalous data.howeverthe least medianofsquares line am page remarkably unperturbed.this line has a simple and natural interpretation.geo-metricallyit corresponds to finding the narrowest strip covering halfoftheobservationswhere the thickness ofthe strip is measured in the vertical direc-tion this strip is marked gray in figure need to look closely to see it.the least median ofsquares line lies at the exact center ofthis band.note thatthis notion is often easier to explain and visualize than the normal least-squaresdefinition ofregression.unfortunatelythere is a serious disadvantage tomedian-based regression techniquesthey incur a high computational costwhich often makes them infeasible for practical problems.detecting anomaliesa serious problem with any form ofautomatic detection ofapparently incor-rect data is that the baby may be thrown out with the bathwater.short ofcon-sulting a human expertthere is really no way oftelling whether a particularinstance really is an error or whether it just does not fit the type ofmodel thatis being applied.in statistical regressionvisualizations help.it will usually bevisually apparenteven to the nonexpertifthe wrong kind ofcurve is beingfitted a straight line is being fitted to data that lies on a parabolafor example.the outliers in figure certainly stand out to the eye.but most problemscannot be so easily visualizedthe notion of model type is more subtle than aregression line.and although it is known that good results are obtained on moststandard datasets by discarding instances that do not fit a decision tree modelthis is not necessarily ofgreat comfort when dealing with a particular the input and squaresleast median of squaresyearphone calls of millionsfigure ofinternational phone calls from am page multiple suspicion will remain that perhaps the new dataset is simplyunsuited to decision tree modeling.one solution that has been tried is to use several different learning schemes such as a decision treeand a nearest-neighbor learnerand a linear discrimi-nant function to filter the data.a conservative approach is to ask that all threeschemes fail to classify an instance correctly before it is deemed erroneous andremoved from the data.in some casesfiltering the data in this way and usingthe filtered data as input to a final learning scheme gives better performancethan simply using the three learning schemes and letting them vote on theoutcome.training all three schemes on the filtereddata and letting them votecan yield even better results.howeverthere is a danger to voting techniquessome learning algorithms are better suited to certain types ofdata than othersand the most appropriate method may simply get out-voted! we will examinea more subtle method ofcombining the output from different classifierscalledstackingin the next section.the lessonas usualis to get to know your dataand look at it in many different ways.one possible danger with filtering approaches is that they might con-ceivably just be sacrificing instances ofa particular class group ofclasses to improve accuracy on the remaining classes.although there are no generalways to guard against thisit has not been found to be a common problem inpractice.finallyit is worth noting once again that automatic filtering is a poor sub-stitute for getting the data right in the first place.ifthis is too time consumingand expensive to be practicalhuman inspection could be limited to thoseinstances that are identified by the filter as multiple modelswhen wise people make critical decisionsthey usually take into account theopinions ofseveral experts rather than relying on their own judgment or thatofa solitary trusted adviser.for examplebefore choosing an important newpolicy directiona benign dictator consults widelyhe or she would be ill advisedto follow just one expert s opinion blindly.in a democratic settingdiscussionofdifferent viewpoints may produce a consensusifnota vote may be calledfor.in either casedifferent expert opinions are being combined.in data mininga model generated by machine learning can be regarded asan expert.expertis probably too strong a word! depending on the amountand quality ofthe training dataand whether the learning algorithm is appro-priate to the problem at handthe expert may in truth be regrettably ignorant but we use the term nevertheless.an obvious approach to making decisionsmore reliable is to combine the output ofdifferent models.several am page learning techniques do this by learning an ensemble ofmodels and using themin combinationprominent among these are schemes called baggingboostingand stacking.they can allmore often than notincrease predictive performanceover a single model.and they are general techniques that can be applied tonumeric prediction problems and to classification tasks.baggingboostingand stacking have only been developed over the pastdecadeand their performance is often astonishingly good.machine learningresearchers have struggled to understand why.and during that strugglenewmethods have emerged that are sometimes even better.for examplewhereashuman committees rarely benefit from noisy distractionsshaking up baggingby adding random variants ofclassifiers can improve performance.closeranalysis revealed that boosting perhaps the most powerful ofthe threemethods is closely related to the established statistical technique ofadditivemodelsand this realization has led to improved procedures.these combined models share the disadvantage ofbeing difficult to analyzethey can comprise dozens or even hundreds ofindividual modelsand althoughthey perform well it is not easy to understand in intuitive terms what factorsare contributing to the improved decisions.in the last few years methods havebeen developed that combine the performance benefits ofcommittees withcomprehensible models.some produce standard decision tree modelsothersintroduce new variants oftrees that provide optional paths.we close by introducing a further technique ofcombining models usingerror-correcting output codes.this is more specialized than the other three tech-niquesit applies only to classification problemsand even then only to ones thathave more than three classes.baggingcombining the decisions ofdifferent models means amalgamating the variousoutputs into a single prediction.the simplest way to do this in the case ofclas-sification is to take a vote a weighted votein the case ofnumeric pre-dictionit is to calculate the average a weighted average.bagging andboosting both adopt this approachbut they derive the individual models in dif-ferent ways.in baggingthe models receive equal weightwhereas in boostingweighting is used to give more influence to the more successful ones just asan executive might place different values on the advice ofdifferent expertsdepending on how experienced they are.to introduce baggingsuppose that several training datasets ofthe same sizeare chosen at random from the problem domain.imagine using a particularmachine learning technique to build a decision tree for each dataset.you mightexpect these trees to be practically identical and to make the same predictionfor each new test instance.surprisinglythis assumption is usually quite the input and am page multiple ifthe training datasets are fairly small.this is a rather disturbingfact and seems to cast a shadow over the whole enterprise! the reason for it isthat decision tree induction leastthe standard top-down method describedin chapter is an unstable processslight changes to the training data mayeasily result in a different attribute being chosen at a particular nodewith sig-nificant ramifications for the structure ofthe subtree beneath that node.thisautomatically implies that there are test instances for which some ofthe deci-sion trees produce correct predictions and others do not.returning to the preceding experts analogyconsider the experts to be theindividual decision trees.we can combine the trees by having them vote on eachtest instance.ifone class receives more votes than any otherit is taken as thecorrect one.generallythe more the merrierpredictions made by votingbecome more reliable as more votes are taken into account.decisions rarelydeteriorate ifnew training sets are discoveredtrees are built for themand theirpredictions participate in the vote as well.in particularthe combined classifierwill seldom be less accurate than a decision tree constructed from just one ofthe datasets.improvement is not guaranteedhowever.it can be shown theo-retically that pathological situations exist in which the combined decisions areworse.the effect ofcombining multiple hypotheses can be viewed through a theo-retical device known as the bias variance decomposition.suppose that we couldhave an infinite number ofindependent training sets ofthe same size and usethem to make an infinite number ofclassifiers.a test instance is processed byall classifiersand a single answer is determined by majority vote.in this ideal-ized situationerrors will still occur because no learning scheme is perfecttheerror rate will depend on how well the machine learning method matches theproblem at handand there is also the effect ofnoise in the datawhich cannotpossibly be learned.suppose the expected error rate were evaluated by averag-ing the error ofthe combined classifier over an infinite number ofindepend-ently chosen test examples.the error rate for a particular learning algorithm iscalled its biasfor the learning problem and measures how well the learningmethod matches the problem.this technical definition is a way ofquantifyingthe vaguer notion ofbias that was introduced in section measures the persistent error ofa learning algorithm that can t be eliminated even by takingan infinite number oftraining sets into account.ofcourseit cannot be calcu-lated exactly in practical situationsit can only be approximated.a second source oferror in a learned modelin a practical situationstemsfrom the particular training set usedwhich is inevitably finite and therefore notfully representative ofthe actual population ofinstances.the expected value ofthis component ofthe errorover all possible training sets ofthe given size andall possible test setsis called the varianceofthe learning method for thatproblem.the total expected error ofa classifier is made up ofthe sum am page and variancethis is the bias variance multipleclassifiers decreases the expected error by reducing the variance component.themore classifiers that are includedthe greater the reduction in variance.ofcoursea difficulty arises when putting this voting method into practiceusually there s only one training setand obtaining more data is either impos-sible or expensive.bagging attempts to neutralize the instability oflearning methods by simu-lating the process described previously using a given training set.instead ofsam-pling a freshindependent training dataset each timethe original training datais altered by deleting some instances and replicating others.instances are ran-domly sampledwith replacementfrom the original dataset to create a new one ofthe same size.this sampling procedure inevitably replicates some ofthe instances and deletes others.ifthis idea strikes a chordit is because wedescribed it in chapter when explaining the bootstrap method for estimatingthe generalization error ofa learning method termbaggingstands for bootstrap aggregating.bagging applies the learning scheme for examplea decision tree inducer to each one ofthese artificially deriveddatasetsand the classifiers generated from them vote for the class to be pre-dicted.the algorithm is summarized in figure difference between bagging and the idealized procedure described pre-viously is the way in which the training datasets are derived.instead ofobtain-ing independent datasets from the domainbagging just resamples the originaltraining data.the datasets generated by resampling are different from oneanother but are certainly not independent because they are all based on onedataset.howeverit turns out that bagging produces a combined model thatoften performs significantly better than the single model built from the origi-nal training dataand is never substantially worse.bagging can also be applied to learning methods for numeric prediction for examplemodel trees.the only difference is thatinstead ofvoting on theoutcomethe individual predictionsbeing real numbersare averaged.thebias variance decomposition can be applied to numeric prediction as well bydecomposing the expected value ofthe mean-squared error ofthe predictionson fresh data.bias is defined as the mean-squared error expected when averag-ing over models built from all possible training datasets ofthe same sizeandvariance is the component ofthe expected error ofa single model that is dueto the particular training data it was built from.it can be shown theoreticallythat averaging over multiple models built from independent training sets the input and is a simplified version ofthe full story.several different methods for performing thebias variance decomposition can be found in the literaturethere is no agreed way am page multiple the expected value ofthe mean-squared error.as we mentioned earlierthe analogous result is not true for classification.bagging with costsbagging helps most ifthe underlying learning method is unstable in that smallchanges in the input data can lead to quite different classifiers.indeed it canhelp to increase the diversity in the ensemble ofclassifiers by making the learn-ing method as unstable as possible.for examplewhen bagging decision treeswhich are already unstablebetter performance is often achieved by switchingpruning offwhich makes them even more unstable.another improvement canbe obtained by changing the way that predictions are combined for classifica-tion.as originally formulatedbagging uses voting.but when the models canoutput probability estimates and not just plain classificationsit makes intuitivesense to average these probabilities instead.not only does this often improveclassification slightlybut the bagged classifier also generates probability estimates ones that are often more accurate than those produced by the in-dividual models.implementations ofbagging commonly use this method ofcombining predictions.in section we showed how to make a classifier cost sensitive by mini-mizing the expected cost ofpredictions.accurate probability estimates are nec-essary because they are used to obtain the expected cost ofeach prediction.bagging is a prime candidate for cost-sensitive classification because it producesvery accurate probability estimates from decision trees and other powerfulyetunstableclassifiers.howevera disadvantage is that bagged classifiers are hardto analyze.a method called metacostcombines the predictive benefits ofbagging witha comprehensible model for cost-sensitive prediction.it builds an ensembleclassifier using bagging and uses it to relabel the training data by giving everymodel generationlet n be the number of instances in the training data.for each of t iterations sample n instances with replacement from training data. apply the learning algorithm to the sample. store the resulting model.classificationfor each of the t models predict class of instance using model.return class that has been predicted most often.figure for am page training instance the prediction that minimizes the expected costbased on theprobability estimates obtained from bagging.metacost then discards the orig-inal class labels and learns a single new classifier for examplea single pruneddecision tree from the relabeled data.this new model automatically takescosts into account because they have been built into the class labels! the resultis a single cost-sensitive classifier that can be analyzed to see how predictionsare made.in addition to the cost-sensitive classificationtechnique just mentionedsection also described a cost-sensitive learningmethod that learns a cost-sensitive classifier by changing the proportion ofeach class in the training datato reflect the cost matrix.metacost seems to produce more accurate results thanthis methodbut it requires more computation.ifthere is no need for a com-prehensible modelmetacost s postprocessing step is superfluousit is better touse the bagged classifier directly in conjunction with the minimum expectedcost method.randomizationbagging generates a diverse ensemble ofclassifiers by introducing randomnessinto the learning algorithm s inputoften with excellent results.but there areother ways ofcreating diversity by introducing randomization.some learningalgorithms already have a built-in random component.for examplewhenlearning multilayer perceptrons using the backpropagation algorithm in section the network weights are set to small randomly chosenvalues.the learned classifier depends on the random numbers because the algo-rithm may find a different local minimum ofthe error function.one way tomake the outcome ofclassification more stable is to run the learner several timeswith different random number seeds and combine the classifiers predictions byvoting or averaging.almost every learning method is amenable to some kind ofrandomization.consider an algorithm that greedily picks the best option at every step suchas a decision tree learner that picks the best attribute to split on at each node.it could be randomized by randomly picking one ofthe nbest options insteadofa single winneror by choosing a random subset ofoptions and picking thebest from that.ofcoursethere is a tradeoffmore randomness generates morevariety in the learner but makes less use ofthe dataprobably decreasing theaccuracy ofeach individual model.the best dose ofrandomness can only beprescribed by experiment.although bagging and randomization yield similar resultsit sometimes paysto combine them because they introduce randomness in differentperhaps complementaryways.a popular algorithm for learning random forests buildsa randomized decision tree in each iteration ofthe bagging algorithmand oftenproduces excellent the input and am page multiple demands more work than bagging because the learning algo-rithm must be modifiedbut it can profitably be applied to a greater variety oflearners.we noted earlier that bagging fails with stable learning algorithmswhose output is insensitive to small changes in the input.for exampleit ispointless to bag nearest-neighbor classifiers because their output changes verylittle ifthe training data is perturbed by sampling.but randomization can beapplied even to stable learnersthe trick is to randomize in a way that makesthe classifiers diverse without sacrificing too much performance.a nearest-neighbor classifier s predictions depend on the distances between instanceswhich in turn depend heavily on which attributes are used to compute themso nearest-neighbor classifiers can be randomized by using differentrandomlychosen subsets ofattributes.boostingwe have explained that bagging exploits the instability inherent in learning algo-rithms.intuitivelycombining multiple models only helps when these modelsare significantly different from one another and when each one treats a reason-able percentage ofthe data correctly.ideallythe models complement oneanothereach being a specialist in a part ofthe domain where the other modelsdon t perform very well just as human executives seek advisers whose skillsand experience complementrather than duplicateone another.the boosting method for combining multiple models exploits this insight byexplicitly seeking models that complement one another.firstthe similaritieslike baggingboosting uses voting classification or averaging numericprediction to combine the output ofindividual models.again like baggingitcombines models ofthe same type for exampledecision trees.howeverboost-ing is iterative.whereas in bagging individual models are built separatelyinboosting each new model is influenced by the performance ofthose built previ-ously.boosting encourages new models to become experts for instances handledincorrectly by earlier ones.a final difference is that boosting weights a model scontribution by its performance rather than giving equal weight to all models.there are many variants on the idea ofboosting.we describe a widely usedmethod called is designed specifically for classification.likebaggingit can be applied to any classification learning algorithm.to simplifymatters we assume that the learning algorithm can handle weighted instanceswhere the weight ofan instance is a positive number.we revisit this assump-tion later. the presence ofinstance weights changes the way in which a classi-fier s error is calculatedit is the sum ofthe weights ofthe misclassified instancesdivided by the total weight ofall instancesinstead ofthe fraction ofinstancesthat are misclassified.by weighting instancesthe learning algorithm can beforced to concentrate on a particular set ofinstancesnamelythose with am page weight.such instances become particularly important because there is a greaterincentive to classify them correctly.the algorithmdescribed in an example ofa learning method that can accommodate weightedinstances without modification because it already uses the notion offractionalinstances to handle missing values.the boosting algorithmsummarized in figure by assigning equalweight to all instances in the training data.it then calls the learning algorithmto form a classifier for this data and reweights each instance according to theclassifier s output.the weight ofcorrectly classified instances is decreasedandthat ofmisclassified ones is increased.this produces a set of easy instanceswith low weight and a set of hard ones with high weight.in the next itera-tion and all subsequent ones a classifier is built for the reweighted datawhich consequently focuses on classifying the hard instances correctly.then theinstances weights are increased or decreased according to the output ofthis newclassifier.as a resultsome hard instances might become even harder and easierones might become even easieron the other handother hard instances mightbecome easierand easier ones might become harder all possibilities can occurin practice.after each iterationthe weights reflect how often the instances havebeen misclassified by the classifiers produced so far.by maintaining a measureof hardness with each instancethis procedure provides an elegant way ofgen-erating a series ofexperts that complement one the input and outputmodel generationassign equal weight to each training instance.for each of t iterations apply learning algorithm to weighted dataset and store resulting model. compute error e of model on weighted dataset and store error. if e equal to zero or e greater or equal to terminate model generation. for each instance in dataset if instance classified correctly by model multiply weight of instance by e e. normalize weight of all instances.classificationassign weight of zero to all classes.for each of the t less models add loge e to weight of class predicted by model.return class with highest weight.figure for am page multiple much should the weights be altered after each iteration? the answerdepends on the current classifier s overall error.more specificallyifedenotesthe classifier s error on the weighted data fraction between and are updated byfor correctly classified instancesand the weights remain unchanged for mis-classified ones.ofcoursethis does not increase the weight ofmisclassifiedinstances as claimed previously.howeverafter all weights have been updatedthey are renormalized so that their sum remains the same as it was before.eachinstance s weight is divided by the sum ofthe new weights and multiplied bythe sum ofthe old ones.this automatically increases the weight ofeach mis-classified instance and reduces that ofeach correctly classified one.whenever the error on the weighted training data exceeds or equals procedure deletes the current classifier and does not perform any moreiterations.the same thing happens when the error is then all instanceweights become have explained how the boosting method generates a series ofclassifiers.to form a predictiontheir output is combined using a weighted vote.to deter-mine the weightsnote that a classifier that performs well on the weighted train-ing data from which it was built to should receive a high weightanda classifier that performs badly to should receive a low one.morespecificallywhich is a positive number between and infinity.incidentallythis formulaexplains why classifiers that perform perfectly on the training data must bedeletedbecause when eis the weight is undefined.to make a predictiontheweights ofall classifiers that vote for a particular class are summedand the classwith the greatest total is chosen.we began by assuming that the learning algorithm can cope with weightedinstances.we explained how to adapt learning algorithms to deal with weightedinstances at the end ofsection under locally weighted linear regression.instead ofchanging the learning algorithmit is possible to generate anunweighted dataset from the weighted data by resampling the same techniquethat bagging uses.whereas for bagging each instance is chosen with equal prob-abilityfor boosting instances are chosen with probability proportional to theirweight.as a resultinstances with high weight are replicated frequentlyand oneswith low weight may never be selected.once the new dataset becomes as largeas the original oneit is fed into the learning method instead ofthe weighteddata.it s as simple as am page a disadvantage ofthis procedure is that some instances with low weight don tmake it into the resampled datasetso information is lost before the learningmethod is applied.howeverthis can be turned into an advantage.ifthe learn-ing method produces a classifier whose error exceeds must termi-nate ifthe weighted data is used directlywhereas with resampling it might bepossible to produce a classifier with error below by discarding the resam-pled dataset and generating a new one from a different random seed.sometimesmore boosting iterations can be performed by resampling than when using theoriginal weighted version ofthe algorithm.the idea ofboosting originated in a branch ofmachine learning researchknown as computational learning theory.theoreticians are interested in boost-ing because it is possible to derive performance guarantees.for exampleit canbe shown that the error ofthe combined classifier on the training dataapproaches zero very quickly as more iterations are performed in the number ofiterations.unfortunatelyas explained in section for the training error are not very interesting because they do notnecessarily indicate good performance on fresh data.howeverit can be showntheoretically that boosting only fails on fresh data ifthe individual classifiers aretoo complex for the amount oftraining data present or iftheir training errorsbecome too large too quickly a precise sense explained by schapire et usualthe problem lies in finding the right balance between the indi-vidual models complexity and their fit to the data.ifboosting succeeds in reducing the error on fresh test datait often does soin a spectacular way.one very surprising finding is that performing more boosting iterations can reduce the error on new data long after the error ofthe combined classifier on the training data has dropped to zero.researcherswere puzzled by this result because it seems to contradict occam s razorwhichdeclares that oftwo hypotheses that explain the empirical evidence equally well the simpler one is to be preferred.performing more boosting iterationswithout reducing training error does not explain the training data any betterand it certainly adds complexity to the combined classifier.fortunatelythe con-tradiction can be resolved by considering the classifier s confidence in its pre-dictions.confidence is measured by the difference between the estimatedprobability ofthe true class and that ofthe most likely predicted class other thanthe true class a quantity known as the margin.the larger the marginthe moreconfident the classifier is in predicting the true class.it turns out that boostingcan increase the margin long after the training error has dropped to zero.theeffect can be visualized by plotting the cumulative distribution ofthe marginvalues ofall the training instances for different numbers ofboosting iterationsgiving a graph known as the margin curve.henceifthe explanation ofempir-ical evidence takes the margin into accountoccam s razor remains as sharp the input and am page multiple beautiful thing about boosting is that a powerful combined classifier canbe built from very simple ones as long as they achieve less than error onthe reweighted data.usuallythis is easy certainly for learning problems withtwo classes! simple learning methods are called weaklearnersand boosting con-verts weak learners into strong ones.for examplegood results for two-classproblems can be obtained by boosting extremely simple decision trees that haveonly one level called decision stumps.another possibility is to apply boostingto an algorithm that learns a single conjunctive rule such as a single path in adecision tree and classifies instances based on whether or not the rule coversthem.ofcoursemulticlass datasets make it more difficult to achieve error ratesbelow trees can still be boostedbut they usually need to be morecomplex than decision stumps.more sophisticated algorithms have been devel-oped that allow very simple models to be boosted successfully in multiclass situations.boosting often produces classifiers that are significantly more accurate onfresh data than ones generated by bagging.howeverunlike baggingboostingsometimes fails in practical situationsit can generate a classifier that is signif-icantly less accurate than a single classifier built from the same data.this indi-cates that the combined classifier overfits the data.additive regressionwhen boosting was first investigated it sparked intense interest amongresearchers because it could coax first-class performance from indifferent learn-ers.statisticians soon discovered that it could be recast as a greedy algorithmfor fitting an additive model.additive models have a long history in statistics.broadlythe term refers to any way ofgenerating predictions by summing upcontributions obtained from other models.most learning algorithms for addi-tive models do not build the base models independently but ensure that theycomplement one another and try to form an ensemble ofbase models that opti-mizes predictive performance according to some specified criterion.boosting implements forward stagewise additive modeling.this class ofalgo-rithms starts with an empty ensemble and incorporates new members sequen-tially.at each stage the model that maximizes the predictive performance oftheensemble as a whole is addedwithout altering those already in the ensemble.optimizing the ensemble s performance implies that the next model shouldfocus on those training instances on which the ensemble performs poorly.thisis exactly what boosting does by giving those instances larger weights.here s a well-known forward stagewise additive modeling method fornumeric prediction.first build a standard regression modelfor examplearegression tree.the errors it exhibits on the training data the differencesbetween predicted and observed values are called residuals.then correct am page these errors by learning a second model perhaps another regression tree thattries to predict the observed residuals.to do thissimply replace the originalclass values by their residuals before learning the second model.adding the pre-dictions made by the second model to those ofthe first one automatically yieldslower error on the training data.usually some residuals still remainbecausethe second model is not a perfect oneso we continue with a third model thatlearns to predict the residuals ofthe residualsand so on.the procedure is rem-iniscent ofthe use ofrules with exceptions for classification that we met insection individual models minimize the squared error ofthe predictionsaslinear regression models dothis algorithm minimizes the squared error ofthe ensemble as a whole.in practice it also works well when the base learneruses a heuristic approximation insteadsuch as the regression and model treelearners described in section factthere is no point in using standardlinear regression as the base learner for additive regressionbecause the sum oflinear regression models is again a linear regression model and the regressionalgorithm itselfminimizes the squared error.howeverit is a different story ifthe base learner is a regression model based on a single attributethe one thatminimizes the squared error.statisticians call this simplelinear regressionincontrast to the standard multiattribute methodproperly called multiplelinearregression.in factusing additive regression in conjunction with simple linearregression and iterating until the squared error ofthe ensemble decreases nofurther yields an additive model identical to the least-squares multiple linearregression function.forward stagewise additive regression is prone to overfitting because eachmodel added fits the training data more closely.to decide when to stopusecross-validation.for exampleperform a cross-validation for every number ofiterations up to a user-specified maximum and choose the one that minimizesthe cross-validated estimate ofsquared error.this is a good stopping criterionbecause cross-validation yields a fairly reliable estimate ofthe error on futuredata.incidentallyusing this method in conjunction with simple linear regres-sion as the base learner effectively combines multiple linear regression withbuilt-in attribute selectionbecause the next most important attribute s contri-bution is only included ifit decreases the cross-validated error.for implementation convenienceforward stagewise additive regressionusually begins with a model that simply predicts the mean ofthe classon the training data so that every subsequent model fits residuals.this suggestsanother possibility for preventing overfittinginstead ofsubtracting a model sentire prediction to generate target values for the next modelshrink the pre-dictions by multiplying them by a user-specified constant factor between before subtracting.this reduces the model s fit to the residuals and conse-quently reduces the chance ofoverfitting.ofcourseit may increase the the input and am page multiple needed to arrive at a good additive model.reducing the multipliereffectively damps down the learning processincreasing the chance ofstoppingat just the right moment but also increasing run time.additive logistic regressionadditive regression can also be applied to classification just as linear regressioncan.but we know from section that logistic regression outperforms linearregression for classification.it turns out that a similar adaptation can be madeto additive models by modifying the forward stagewise modeling method toperform additivelogistic regression.use the logit transform to translate theprobability estimation problem into a regression problemas we did in solve the regression task using an ensemble ofmodels for exampleregression trees just as for additive regression.at each stageadd the modelthat maximizes the probability ofthe data given the ensemble classifier.suppose fjis the jth regression model in the ensemble and fja is its predic-tion for instance a.assuming a two-class problemuse the additive model sfjato obtain a probability estimate for the first classthis closely resembles the expression used in section thathere it is abbreviated by using vector notation for the instance aand the origi-nal weighted sum ofattribute values is replaced by a sum ofarbitrarily complexregression models f.figure shows the two-class version ofthe logitboostalgorithmwhich per-forms additive logistic regression and generates the individual models fj.hereyiis for an instance in the first class and for an instance in the second.ineach iteration this algorithm fits a regression model fjto a weighted version model generationfor j to t iterations for each instance ai set the target value for the regression to zi ai ai ai set the weight of instance ai to ai ai fit a regression model fj to the data with class values zi and weights wi.classificationpredict first class if a otherwise predict second class.figure for additive logistic am page the original dataset based on dummy class values ziand weights wi.we assumethat is computed using the fjthat were built in previous iterations.the derivation ofthis algorithm is beyond the scope ofthis bookbut it canbe shown that the algorithm maximizes the probability ofthe data with respectto the ensemble ifeach model fjis determined by minimizing the squared erroron the corresponding regression problem.in factifmultiple linear regressionis used to form the fjthe algorithm converges to the maximum likelihood linear-logistic regression modelit is an incarnation ofthe iteratively reweighted least-squares method mentioned in section looks quite different to adaboostbut the predictorsthey produce differ mainly in that the former optimizes the likelihood directlywhereas the latter optimizes an exponential loss function that can be regardedas an approximation to it.from a practical perspectivethe difference is thatlogitboost uses a regression method as the base learner whereas adaboostworks with classification algorithms.we have only shown the two-class version oflogitboostbut the algorithmcan be generalized to multiclass problems.as with additive regressionthedanger ofoverfitting can be reduced by shrinking the predictions ofthe indi-vidual fjby a predetermined multiplier and using cross-validation to determinean appropriate number ofiterations.option treesbaggingboostingand randomization all produce ensembles ofclassifiers.thismakes it very difficult to analyze what kind ofinformation has been extractedfrom the data.it would be nice to have a single model with the same predictiveperformance.one possibility is to generate an artificial datasetby randomlysampling points from the instance space and assigning them the class labels pre-dicted by the ensemble classifierand then learn a decision tree or rule set fromthis new dataset.to obtain similar predictive performance from the tree as fromthe ensemble a huge dataset may be requiredbut in the limit this strategyshould be able to replicate the performance ofthe ensemble classifier and itcertainly will ifthe ensemble itselfconsists ofdecision trees.another approach is to derive a single structure that can represent an ensem-ble ofclassifiers compactly.this can be done ifthe ensemble consists ofdeci-sion treesthe result is called an option tree.option trees differ from decisiontrees in that they contain two types ofnodedecision nodes and option nodes.figure shows a simple example for the weather datawith only one optionnode.to classify an instancefilter it down through the tree.at a decision nodetake just one ofthe branchesas usualbut at an option node take allthebranches.this means that the instance ends up in more than one leafand theclassifications obtained from those leaves must somehow be combined into the input and am page multiple classification.this can be done simply by votingtaking the majorityvote at an option node to be the prediction ofthe node.in that case it makeslittle sense to have option nodes with only two options in figure there will only be a majority ifboth branches agree.another pos-sibility is to average the probability estimates obtained from the different pathsusing either an unweighted average or a more sophisticated bayesianapproach.option trees can be generated by modifying an existing decision tree learnerto create an option node ifthere are several splits that look similarly usefulaccording to their information gain.all choices within a certain user-specifiedtolerance ofthe best one can be made into options.during pruningthe errorofan option node is the average error ofits options.another possibility is to grow an option tree by incrementally adding nodesto it.this is commonly done using a boosting algorithmand the resulting treesare usually called alternating decision treesinstead ofoption trees.in this contextthe decision nodes are called splitter nodesand the option nodes are called pre-diction nodes.prediction nodes are leaves ifno splitter nodes have been addedto them yet.the standard alternating decision tree applies to two-class prob-lemsand with each prediction node is associated a positive or negative numericvalue.to obtain a prediction for an instancefilter it down all applicablebranches and sum up the values from any prediction nodes that are encoun-teredpredict one class or the other depending on whether the sum is positiveor negative. overcastoutlook overcast false trueyesnoyeswindyoption node normal highnoyeshumidityfigure option tree for the weather am page a simple example tree for the weather data is shown in figure apositive value corresponds to class play a negative one to play classify an instance with outlook windy it down to the corresponding leavesobtainingthe values sum ofthese values is nega-tivehence predict play decision trees always have a predic-tion node at the rootas in this example.the alternating tree is grown using a boosting algorithm for exampleaboosting algorithm that employs a base learner for numeric predictionsuch asthe logitboost method described previously.assume that the base learner pro-duces a single conjunctive rule in each boosting iteration.then an alternatingdecision tree can be generated by simply adding each rule into the tree.thenumeric scores associated with the prediction nodes are obtained from the rules.howeverthe resulting tree would grow large very quickly because the rules fromdifferent boosting iterations are likely to be different.hencelearning algorithmsfor alternating decision trees consider only those rules that extend one oftheexisting paths in the tree by adding a splitter node and two corresponding pre-diction nodes binary splits.in the standard version ofthe the input and output overcastoutlook overcast false true normal decision tree for the weather am page multiple possible location in the tree is considered for additionand a node is addedaccording to a performance measure that depends on the particular boostingalgorithm employed.howeverheuristics can be used instead ofan exhaustivesearch to speed up the learning process.logistic model treesoption trees and alternating trees yield very good classification performancebased on a single structurebut they may still be difficult to interpret when thereare many options nodes because it becomes difficult to see how a particular pre-diction is derived.howeverit turns out that boosting can also be used to buildvery effective decision trees that do not include any options at all.for examplethe logitboost algorithm has been used to induce trees with linear logisticregression models at the leaves.these are called logistic model treesand are interpreted in the same way as the model trees for regression described insection performs additive logistic regression.suppose that each iterationofthe boosting algorithm fits a simple regression function by going through allthe attributesfinding the simple regression function with the smallest errorand adding it into the additive model.ifthe logitboost algorithm is run untilconvergencethe result is a maximum likelihood multiple-logistic regressionmodel.howeverfor optimum performance on future data it is usually unnec-essary to wait for convergence and to do so is often detrimental.an appro-priate number ofboosting iterations can be determined by estimating theexpected performance for a given number ofiterations using cross-validationand stopping the process when performance ceases to increase.a simple extension ofthis algorithm leads to logistic model trees.the boost-ing process terminates when there is no further structure in the data that canbe modeled using a linear logistic regression function.howeverthere may stillbe a structure that linear models can fit ifattention is restricted to subsets ofthe dataobtainedfor exampleby a standard decision tree criterion such asinformation gain.thenonce no further improvement can be obtained byadding more simple linear modelsthe data is split and boosting is resumed sep-arately in each subset.this process takes the logistic model generated so far andrefines it separately for the data in each subset.againcross-validation is run ineach subset to determine an appropriate number ofiterations to perform in thatsubset.the process is applied recursively until the subsets become too small.theresulting tree will surely overfit the training dataand one ofthe standardmethods ofdecision tree learning can be used to prune it.experiments indicatethat the pruning operation is very important.using a strategy that chooses theright tree size using cross-validationthe algorithm produces small but veryaccurate trees with linear logistic models at the am page stackingstacked generalizationor stackingfor shortis a different way ofcombining mul-tiple models.although developed some years agoit is less widely used thanbagging and boostingpartly because it is difficult to analyze theoretically andpartly because there is no generally accepted best way ofdoing it the basic ideacan be applied in many different variations.unlike bagging and boostingstacking is not normally used to combinemodels ofthe same type for examplea set ofdecision trees.instead it isapplied to models built by different learning algorithms.suppose you have adecision tree inducera na ve bayes learnerand an instance-based learningmethod and you want to form a classifier for a given dataset.the usual proce-dure would be to estimate the expected error ofeach algorithm by cross-validation and to choose the best one to form a model for prediction on futuredata.but isn t there a better way? with three learning algorithms availablecan twe use all three for prediction and combine the outputs together?one way to combine outputs is by voting the same mechanism used inbagging.howeverunweighted voting only makes sense ifthe learningschemes perform comparably well.iftwo ofthe three classifiers make predic-tions that are grossly incorrectwe will be in trouble! insteadstacking intro-duces the concept ofametalearnerwhich replaces the voting procedure.theproblem with voting is that it s not clear which classifier to trust.stacking triesto learnwhich classifiers are the reliable onesusing another learning algo-rithm the metalearner to discover how best to combine the output ofthebase learners.the input to the metamodel also called the model are the predic-tions ofthe base modelsor models.a instance has as many attrib-utes as there are learnersand the attribute values give the predictions ofthese learners on the corresponding instance.when the stacked learneris used for classificationan instance is first fed into the modelsand eachone guesses a class value.these guesses are fed into the modelwhichcombines them into the final prediction.there remains the problem oftraining the learner.to do thiswe needto find a way oftransforming the training data for training learners into training data for training the learner.this seems straightforwardlet each model classify a training instanceand attach to their predictions the instance s actual class value to yield a training instance.unfortunatelythis doesn t work well.it would allow rulesto be learned such as always believe the output ofclassifier aand ignore b andc.this rule may well be appropriate for particular base classifiers aband cifsoit will probably be learned.but just because it seems appropriate on thetraining data doesn t necessarily mean that it will work well on the test data the input and pm page multiple it will inevitably learn to prefer classifiers that overfit the training dataover ones that make decisions more realistically.consequentlystacking does not simply transform the training datainto data in this manner.recall from chapter that there are bettermethods ofestimating a classifier s performance than using the error on thetraining set.one is to hold out some instances and use them for an independ-ent evaluation.applying this to stackingwe reserve some instances to form thetraining data for the learner and build classifiers from the remain-ing data.once the classifiers have been built they are used to classify theinstances in the holdout setforming the training data as described pre-viously.because the classifiers haven t been trained on these instancestheir predictions are unbiasedtherefore the training data accuratelyreflects the true performance ofthe learning algorithms.once the data has been generated by this holdout procedurethe learners can bereapplied to generate classifiers from the full training setmaking slightly betteruse ofthe data and leading to better predictions.the holdout method inevitably deprives the model ofsome ofthetraining data.in chapter was introduced as a means ofcir-cumventing this problem for error estimation.this can be applied in conjunc-tion with stacking by performing a cross-validation for every learner.each instance in the training data occurs in exactly one ofthe test folds ofthecross-validationand the predictions ofthe inducers built from the cor-responding training fold are used to build a training instance from it.this generates a training instance for each training instance.ofcourseit is slow because a classifier has to be trained for each fold ofthecross-validationbut it does allow the classifier to make full use ofthetraining data.given a test instancemost learning methods are able to output probabilitiesfor every class label instead ofmaking a single categorical prediction.this canbe exploited to improve the performance ofstacking by using the probabilitiesto form the data.the only difference to the standard procedure is thateach nominal attribute representing the class predicted by a is replaced by several numeric attributeseach representing a classprobability output by the learner.in other wordsthe number ofattrib-utes in the data is multiplied by the number ofclasses.this procedurehas the advantage that the learner is privy to the confidence that learner associates with its predictionsthereby amplifying communica-tion between the two levels oflearning.an outstanding question remainswhat algorithms are suitable for the learner? in principleany learning scheme can be applied.howeverbecausemost ofthe work is already done by the learnersthe classifier isbasically just an arbiter and it makes sense to choose a rather simple am page for this purpose.in the words ofdavid wolpertthe inventor ofstackingit isreasonable that relatively globalsmooth generalizers should performwell.simple linear models or trees with linear models at the leaves usually workwell.stacking can also be applied to numeric prediction.in that casethe and the model all predict numeric values.the basic mechanismremains the samethe only difference lies in the nature ofthe data.inthe numeric caseeach attribute represents the numeric prediction madeby one ofthe modelsand instead ofa class value the numeric targetvalue is attached to training instances.error-correcting output codeserror-correcting output codes are a technique for improving the performanceofclassification algorithms in multiclass learning problems.recall from that some learning algorithms for examplestandard support vectormachines only work with two-class problems.to apply such algorithms tomulticlass datasetsthe dataset is decomposed into several independent two-class problemsthe algorithm is run on each oneand the outputs ofthe result-ing classifiers are combined.error-correcting output codes are a method formaking the most ofthis transformation.in factthe method works so well thatit is often advantageous to apply it even when the learning algorithm can handlemulticlass datasets directly.in section we learned how to transform a multiclass datasetinto several two-class ones.for each classa dataset is generated containing acopy ofeach instance in the original databut with a modified class value.iftheinstance has the class associated with the corresponding dataset it is tagged yesotherwise no.then classifiers are built for each ofthese binary datasetsclassi-fiers that output a confidence figure with their predictions for exampletheestimated probability that the class is yes.during classificationa test instanceis fed into each binary classifierand the final class is the one associated with theclassifier that predicts yesmost confidently.ofcoursethis method is sensitiveto the accuracy ofthe confidence figures produced by the classifiersifsomeclassifiers have an exaggerated opinion oftheir own predictionsthe overallresult will suffer.consider a multiclass problem with the four classes abcand d.the trans-formation can be visualized as shown in table yesand noaremapped to and ofthe original class values is convertedinto a code bit per classand the four classifiers predict the bits independently.interpreting the classification process in terms ofthese code wordserrors occur when the wrong binary bit receives the highest the input and am page multiple do not have to use the particular code words shown.indeedthere is no reason why each class must be represented by bits.look instead atthe code oftable classes are represented by bits.when appliedto a datasetseven classifiers must be built instead offour.to see what that mightbuyconsider the classification ofa particular instance.suppose it belongs toclass aand that the predictions ofthe individual classifiers are this code word with those in table second classifier has made a mistakeit predicted instead the predicted bits with the code word associatedwith each classthe instance is clearly closer to athan to any other class.thiscan be quantified by the number ofbits that must be changed to convert thepredicted code word into those oftable hamming distanceor thediscrepancy between the bit stringsis for the classes abcand drespectively.we can safely conclude that the second classifier made a mistakeand correctly identify aas the instance s true class.the same kind oferror correction is not possible with the code words any predicted string bits other than these four wordshas the same distance to at least two ofthem.the output codes are not errorcorrecting. what determines whether a code is error correcting or not? consider thehamming distance between the code words representing different classes.thenumber oferrors that can possibly be corrected depends on the minimum dis-tance between any pair ofcode wordssay d.the code can guarantee to correctup to errorsbecause ifthis number ofbits ofthe correct code word are flippedit will still be the closest and will therefore be identifiedcorrectly.in table the hamming distance for each pair ofcode words minimum distance dis also we can correct no more errors! howeverin the code oftable the minimum distance is distance is for all pairs.that means it is guaranteed to correct a multiclass problem into a two-class one standard method and error-correcting code.classclass vectorclassclass am page we have identified one property ofa good error-correcting codethe codewords must be well separated in terms oftheir hamming distance.because theycomprise the rows ofthe code tablethis property is called row separation.thereis a second requirement that a good error-correcting code should fulfillcolumnseparation.the hamming distance between every pair ofcolumns must belargeas must the distance between each column and the complement ofeveryother column.in table seven columns are separated from oneanother their complements by at least bit.column separation is necessary because iftwo columns are identical ifone is the complement ofanotherthe corresponding classifiers will make the same errors.error correction is weakened ifthe errors are correlated in other wordsifmany bit positions are simultaneously incorrect.the greater the distance between columnsthe more errors are likely to be corrected.with fewer than four classes it is impossible to construct an effective error-correcting code because good row separation and good column separa-tion cannot be achieved simultaneously.for examplewith three classes there are only eight possible columns ofwhich are complements ofthe other four.moreovercolumns with all zeroes or all ones provide no discrimination.this leaves just three possible columnsand the resultingcode is not error correcting at all.in factit is the standard one-per-class encoding.ifthere are few classesan exhaustive error-correcting code such as the onein table can be built.in an exhaustive code for kclassesthe columnscomprise every possible k-bit stringexcept for complements and the trivial all-zero or all-one strings.each code word contains bits.the code is con-structed as followsthe code word for the first class consists ofall onesthat forthe second class has followed by onesthe third has by followed by followed by onesand soon.the ith code word consists ofalternating runs and onesthelast run being one short.with more classesexhaustive codes are infeasible because the number ofcolumns increases exponentially and too many classifiers have to be built.inthat case more sophisticated methods are employedwhich can build a code withgood error-correcting properties from a smaller number ofcolumns.error-correcting output codes do not work for local learning algorithms suchas instance-based learnerswhich predict the class ofan instance by looking atnearby training instances.in the case ofa nearest-neighbor classifierall outputbits would be predicted using the same training instance.the problem can becircumvented by using different attribute subsets to predict each output bitdecorrelating the the input and am page unlabeled unlabeled datawhen introducing the machine learning process in chapter we drew a sharpdistinction between supervised and unsupervised learning classification andclustering.recently researchers have begun to explore territory between the twosometimes called semisupervised learningin which the goal is classification butthe input contains both unlabeled and labeled data.you can t do classificationwithout labeled dataofcoursebecause only the labels tell what the classes are.but it is sometimes attractive to augment a small amount oflabeled data witha large pool ofunlabeled data.it turns out that the unlabeled data can help youlearn the classes.how can this be?firstwhy would you want it? many situations present huge volumes ofrawdatabut assigning classes is expensive because it requires human insight.textmining provides some classic examples.suppose you want to classify web pagesinto predefined groups.in an academic setting you might be interested in facultypagesgraduate student pagescourse information pagesresearch group pagesand department pages.you can easily download thousandsor millionsofrel-evant pages from university web sites.but labeling the training data is a labo-rious manual process.or suppose your job is to use machine learning to spotnames in textdifferentiating among personal namescompany namesandplace names.you can easily download megabytesor gigabytesoftextbutmaking this into training data by picking out the names and categorizing themcan only be done manually.cataloging news articlessorting electronic maillearning users reading interests applications are legion.leaving text asidesuppose you want to learn to recognize certain famous people in televisionbroadcast news.you can easily record hundreds or thousands ofhours ofnews-castsbut again labeling is manual.in any ofthese scenarios it would be enor-mously attractive to be able to leverage a large pool ofunlabeled data to obtainexcellent performance from just a few labeled examples particularly ifyouwere the graduate student who had to do the labeling!clustering for classificationhow can unlabeled data be used to improve classification? here s a simple idea.use na ve bayes to learn classes from a small labeled datasetand then extendit to a large unlabeled dataset using the em maximization itera-tive clustering algorithm ofsection procedure is this.firsttrain a clas-sifier using the labeled data.secondapply it to the unlabeled data to label itwith class probabilities expectation step.thirdtrain a new classifierusing the labels for all the data maximization step.fourthiterate untilconvergence.you could think ofthis as iterative clusteringwhere starting am page and cluster labels are gleaned from the labeled data.the em procedure guar-antees to find model parameters that have equal or greater likelihood at eachiteration.the key questionwhich can only be answered empiricallyis whetherthese higher likelihood parameter estimates will improve classification accuracy.intuitivelythis might work well.consider document classification.certainphrases are indicative ofthe classes.some occur in labeled documentswhereasothers only occur in unlabeled ones.but there are probably some documentsthat contain bothand the em procedure uses these to generalize the learnedmodel to utilize phrases that do not appear in the labeled dataset.for exampleboth supervisorand phd topicmight indicate a graduate student s home page.suppose that only the former phrase occurs in the labeled documents.em iter-atively generalizes the model to correctly classify documents that contain justthe latter.this might work with any classifier and any iterative clustering algorithm.but it is basically a bootstrapping procedureand you must take care to ensurethat the feedback loop is a positive one.using probabilities rather than harddecisions seems beneficial because it allows the procedure to converge slowlyinstead ofjumping to conclusions that may be wrong.na ve bayes and the prob-abilistic em procedure described in section are particularly apt choicesbecause they share the same fundamental assumptionindependence betweenattributes ormore preciselyconditional independence between attributesgiven the class.ofcoursethe independence assumption is universally violated.even ourlittle example used the two-word phrase phd topicwhereas actual implemen-tations would likely use individual words as attributes and the example wouldhave been far less compelling ifwe had substituted either ofthe single termsphdor topic.the phrase phd studentsis probably more indicative offacultythan graduate student home pagesthe phrase research topicis probably less dis-criminating.it is the very fact that phdand topicare notconditionally inde-pendent given the class that makes the example workit is their combinationthat characterizes graduate student pages.neverthelesscoupling na ve bayes and em in this manner works well in thedomain ofdocument classification.in a particular classification task it attainedthe performance ofa traditional learner using fewer than one-third ofthelabeled training instancesas well as five times as many unlabeled ones.this isa good tradeoffwhen labeled instances are expensive but unlabeled ones are vir-tually free.with a small number oflabeled documentsclassification accuracycan be improved dramatically by incorporating many unlabeled ones.two refinements to the procedure have been shown to improve performance.the first is motivated by experimental evidence that when there are manylabeled documents the incorporation ofunlabeled data may reduce rather thanincrease accuracy.hand-labeled data is should be inherently less noisy the input and am page unlabeled labeled data.the solution is to introduce a weighting parameterthat reduces the contribution ofthe unlabeled data.this can be incorporatedinto the maximization step ofem by maximizing the weighted likelihood ofthelabeled and unlabeled instances.when the parameter is close to zerounlabeleddocuments have little influence on the shape ofem s hill-climbing surfacewhen close to onethe algorithm reverts to the original version in which thesurface is equally affected by both kinds ofdocument.the second refinement is to allow each class to have several clusters.asexplained in section em clustering algorithm assumes that the data isgenerated randomly from a mixture ofdifferent probability distributionsoneper cluster.until nowa one-to-one correspondence between mixture compo-nents and classes has been assumed.in many circumstances this is unrealistic including document classificationbecause most documents address multipletopics.with several clusters per classeach labeled document is initially assignedrandomly to each ofits components in a probabilistic fashion.the maximiza-tion step ofthe em algorithm remains as beforebut the expectation step ismodified to not only probabilistically label each example with the classesbutto probabilistically assign it to the components within the class.the number ofclusters per class is a parameter that depends on the domain and can be set bycross-validation.co-traininganother situation in which unlabeled data can improve classification perform-ance is when there are two different and independent perspectives on the clas-sification task.the classic example again involves documentsthis time webdocumentsin which the two perspectives are the contentofa web page and thelinksto it from other pages.these two perspectives are well known to be bothuseful and differentsuccessful web search engines capitalize on them bothusing secret recipes.the text that labels a link to another web page gives arevealing clue as to what that page is about perhaps even more revealing thanthe page s own contentparticularly ifthe link is an independent one.intuitivelya link labeled my adviseris strong evidence that the target page is a facultymember s home page.the ideacalled co-trainingis this.given a few labeled examplesfirst learna different model for each perspective in this case a content-based and ahyperlink-based model.then use each one separately to label the unlabeledexamples.for each modelselect the example it most confidently labels as pos-itive and the one it most confidently labels as negativeand add these to the pooloflabeled examples.better yetmaintain the ratio ofpositive and negative exam-ples in the labeled pool by choosing more ofone kind than the other.in eithercaserepeat the whole proceduretraining both models on the augmented pooloflabeled examplesuntil the unlabeled pool is am page there is some experimental evidenceusing na ve bayes throughout as thelearnerthat this bootstrapping procedure outperforms one that employs all thefeatures from both perspectives to learn a single model from the labeled data.it relies on having two different views ofan instance that are redundant but notcompletely correlated.various domains have been proposedfrom spottingcelebrities in televised newscasts using video and audio separately to mobilerobots with visionsonarand range sensors.the independence ofthe viewsreduces the likelihood ofboth hypotheses agreeing on an erroneous label.em and co-trainingon datasets with two feature sets that are truly independentexperiments haveshown that co-training gives better results than using em as described previ-ously.even better performancehowevercan be achieved by combining the twointo a modified version ofco-training called co-em.co-training trains two clas-sifiers representing different perspectivesa and band uses both to add newexamples to the training pool by choosing whichever unlabeled examples theyclassify most positively or negatively.the new examples are few in number anddeterministically labeled.co-emon the other handtrains perspective a on thelabeled data and uses it to probabilistically label all unlabeled data.next it trainsclassifier b on both the labeled data and the unlabeled data with classifier a stentative labelsand then it probabilistically relabels all the data for use by clas-sifier a.the process iterates until the classifiers converge.this procedure seemsto perform consistently better than co-training because it does not commit tothe class labels that are generated by classifiers a and b but rather reestimatestheir probabilities at each iteration.the range ofapplicability ofco-emlike co-trainingis still limited by therequirement for multiple independent perspectives.but there is some experi-mental evidence to suggest that even when there is no natural split offeaturesinto independent perspectivesbenefits can be achieved by manufacturing sucha split and using co-training orbetter yetco-em on the split data.thisseems to work even when the split is made randomlyperformance could surelybe improved by engineering the split so that the feature sets are maximally independent.why does this work? researchers have hypothesized that thesealgorithms succeed partly because the split makes them more robust to theassumptions that their underlying classifiers make.there is no particular reason to restrict the base classifier to na ve bayes.support vector machines probably represent the most successful technology fortext categorization today.howeverfor the em iteration to work it is necessarythat the classifier labels the data probabilisticallyit must also be able to useprobabilistically weighted examples for training.support vector machines caneasily be adapted to do both.we explained how to adapt learning algorithms the input and am page deal with weighted instances in section under locally weighted linear regres-sionpage way ofobtaining probability estimates from support vectormachines is to fit a one-dimensional logistic model to the outputeffectivelyperforming logistic regression as described in section on the output.excel-lent results have been reported for text classification using co-em with thesupport vector machine classifier.it outperforms other variants ofsvmand seems quite robust to varying proportions oflabeled and unlabeled data.the ideas ofco-training and em and particularly their combination in the co-em algorithm are interestingthought provokingand have striking poten-tial.but just what makes them work is still controversial and poorly understood.these techniques are the subject ofcurrent researchthey have not yet enteredthe mainstream ofmachine learning and been harnessed for practical readingattribute selectionunder the term feature selectionhas been investigated in thefield ofpattern recognition for decades.backward eliminationfor examplewasintroduced in the early and green surveysthe feature selection algorithms that have been developed for pattern recogni-tion.best-first search and genetic algorithms are standard artificial intelligencetechniques experiments that show the performance ofdecision tree learners deteri-orating when new attributes are added are reported by john givesa nice explanation ofattribute selection.the idea offinding the smallest attrib-ute set that carves up the instances uniquely is from almuallin and and was further developed by liu and setiono andaha and cardie both investigated the use ofdecision tree algo-rithms to identify features for nearest-neighbor learningholmes and nevill-manning used to order features for selection.kira and rendell instance-based methods to select featuresleading to a scheme calledrelieffor recursive elimination offeatures.gilad-bachrach et showhow this scheme can be modified to work better with redundant attributes.thecorrelation-based feature selection method was developed by hall use ofwrapper methods for feature selection is due to john et kohavi and john genetic algorithms have been applied withina wrapper framework by vafaie and dejong and cherkauer and selective na ve bayes learning method is due to langley and et present and evaluate the recursive feature elimina-tion scheme in conjunction with support vector machines.the method ofracedsearch was developed by moore and lee am page dougherty et give a briefaccount ofsupervised and unsuperviseddiscretizationalong with experimental results comparing the entropy-basedmethod with equal-width binning and the method.frank and witten the effect ofusing the ordering information in discretized attributes.proportional k-interval discretization for na ve bayes was proposed by yang andwebb entropy-based method for discretizationincluding the useofthe mdl stopping criterionwas developed by fayyad and irani statistical method using the is due to kerber itsextension to an automatically determined significance level is described by liuand setiono et investigate the use ofdynamic pro-gramming for discretization and derive the quadratic time bound for a generalimpurity function and the linear one for error-based discretiza-tion.the example used for showing the weakness oferror-based discretizationis adapted from kohavi and sahami were the first to clearly iden-tify this phenomenon.principal components analysis is a standard technique that can be found inmost statistics textbooks.fradkin and madigan analyze the performanceofrandom projections.the tf idf metric is described by witten et experiments on using to filter its own training data were reportedby john more conservative approach ofa consensus filter involvingseveral learning algorithms has been investigated by brodley and friedl and leroy describe the detection ofoutliers in sta-tistical regressionincluding the least median ofsquares methodthey alsopresent the telephone data offigure was quinlan who noticedthat removing noise from the training instance s attributes can decrease a classifier s performance on similarly noisy test instancesparticularly at highernoise levels.combining multiple models is a popular research topic in machine learningresearchwith many related publications.the term baggingfor bootstrapaggregating was coined by breiman investigated the propertiesofbagging theoretically and empirically for both classification and numeric pre-diction.domingos introduced the metacost algorithm.randomizationwas evaluated by dietterich and compared with bagging and boosting.bay suggests using randomization for ensemble learning with nearest-neighbor classifiers.random forests were introduced by breiman and schapire developed the boosting algorithmand derived theoretical bounds for its performance.laterthey improved thesebounds using the concept ofmargins and schapire adapted for numeric prediction.the logitboost algorithmwas developed by friedman et describes how tomake boosting more resilient in the presence ofnoisy the input and am page domingos describes how to derive a single interpretable model froman ensemble using artificial training examples.bayesian option trees were intro-duced by buntine majority voting was incorporated into optiontrees by kohavi and kunz and mason introduced alter-nating decision treesexperiments with multiclass alternating decision treeswere reported by holmes et et developed logis-tic model trees using the logitboost algorithm.stacked generalization originated with wolpert presented theidea in the neural network literatureand was applied to numeric prediction bybreiman and witten compared different modelsempirically and found that a simple linear model performs bestthey alsodemonstrated the advantage ofusing probabilities as data.a combina-tion ofstacking and bagging has also been investigated and idea ofusing error-correcting output codes for classification gained wideacceptance after a paper by dietterich and bakiri and aha how to apply such codes to nearest-neighbor classifiers.blum and mitchell pioneered the use ofco-training and developed atheoretical model for the use oflabeled and unlabeled data from different inde-pendent perspectives.nigam and ghani analyzed the effectiveness andapplicability ofco-trainingrelating it to the traditional use ofstandard em tofill in missing values.they also introduced the co-em algorithm.nigam et thoroughly explored how the em clustering algorithm can use unlabeleddata to improve an initial classifier built by na ve bayesas reported in the clustering for classificationsection.up to this pointco-training and co-em were applied mainly to small two-class problemsghani used error-correcting output codes to address multiclass situations with many classes.brefeld and scheffer extended co-em to use a support vector machinerather than na ve bayes.seeger casts some doubt on whether these newalgorithms really do have anything to offer over traditional onesproperly am page am page machine learning is a burgeoning new technology for mining knowledge fromdataa technology that a lot ofpeople are beginning to take seriously.we don twant to oversell it.the kind ofmachine learning we know is not about the big problemsfuturistic visions ofautonomous robot servantsphilosophicalconundrums ofconsciousnessmetaphysical issues offree willevolutionary or theological questions ofwhere intelligence comes fromlinguistic debatesover language learningpsychological theories ofchild developmentor cogni-tive explanations ofwhat intelligence is and how it works.for usit s far moreprosaicmachine learning is about algorithms for inferring structure from dataand ways ofvalidating that structure.these algorithms are not abstruse andcomplicatedbut they re not completely obvious and trivial either.looking forwardthe main challenge ahead is applications.opportunitiesabound.wherever there is datathings can be learned from it.whenever there is too much data for people to pore over themselvesthe mechanics oflearning will have to be automatic.but the inspiration will certainly not be auto-matic! applications will come not from computer programsnor from onextensions and am page learning expertsnor from the data itselfbut from the people who work withthe data and the problems from which it arises.that is why we have written this bookand the weka system described in part ii to empower those whoare not machine learning experts to apply these techniques to the problems thatarise in daily working life.the ideas are simple.the algorithms are here.therest is really up to you!ofcoursedevelopment ofthe technology is certainly not finished.machinelearning is a hot research topicand new ideas and techniques continuallyemerge.to give a flavor ofthe scope and variety ofresearch frontswe close parti by looking at some topical areas in the world ofdata from massive datasetsthe enormous proliferation ofvery large databases in today s companies andscientific institutions makes it necessary for machine learning algorithms tooperate on massive datasets.two separate dimensions become critical when anyalgorithm is applied to very large datasetsspace and time.suppose the data is so large that it cannot be held in main memory.thiscauses no difficulty ifthe learning scheme works in an incremental fashionprocessing one instance at a time when generating the model.an instance canbe read from the input filethe model can be updatedthe next instance can bereadand so on without ever holding more than one training instance in mainmemory.normallythe resulting model is small compared with the dataset sizeand the amount ofavailable memory does not impose any serious constrainton it.the na ve bayes method is an excellent example ofthis kind ofalgorithmthere are also incremental versions ofdecision tree inducers and rule learningschemes.howeverincremental algorithms for some ofthe learning methodsdescribed in this book have not yet been developed.other methodssuch asbasic instance-based schemes and locally weighted regressionneed access to allthe training instances at prediction time.in that casesophisticated caching andindexing mechanisms have to be employed to keep only the most frequentlyused parts ofa dataset in memory and to provide rapid access to relevantinstances in the file.the other critical dimension when applying learning algorithms to massivedatasets is time.ifthe learning time does not scale linearly almost linearlywith the number oftraining instancesit will eventually become infeasible toprocess very large datasets.in some applications the number ofattributes is acritical factorand only methods that scale linearly in the number ofattributesare acceptable.alternativelyprediction time might be the crucial issue.fortu-natelythere are many learning algorithms that scale gracefully during bothtraining and testing.for examplethe training time for na ve bayes is linear onextensions and am page both the number ofinstances and the number ofattributes.for top-down deci-sion tree inducerswe saw in section that training time islinear in the number ofattributes andifthe tree is uniformly bushylog-linearin the number ofinstances raising is not used orifit iswith a further log factor.when a dataset is too large for a particular learning algorithm to be appliedthere are three ways to make learning feasible.the first is trivialinstead ofapplying the scheme to the full datasetuse just a small subset for training.ofcourseinformation is lost when subsampling is employed.howeverthe lossmay be negligible because the predictive performance ofa learned model oftenflattens out long before all the training data is incorporated into it.ifthis is thecaseit can easily be verified by observing the model s performance on a holdouttest set for training sets ofdifferent size.this kind ofbehaviorcalled the law ofdiminishing returnsmay arise becausethe learning problem is a simple oneso that a small volume oftraining data issufficient to learn an accurate model.alternativelythe learning algorithm mightbe incapable ofgrasping the detailed structure ofthe underlying domain.thisis often observed when na ve bayes is employed in a complex domainaddi-tional training data may not improve the performance ofthe modelwhereas a decision tree s accuracy may continue to climb.in this caseofcourseifpredictive performance is the main objective you should switch to the morecomplex learning algorithm.but beware ofoverfitting! take care not to assessperformance on the training data.parallelization is another way ofreducing the time complexity oflearning.the idea is to split the problem into smaller partssolve each using a separateprocessorand combine the results together.to do thisa parallelized version ofthe learning algorithm must be created.some algorithms lend themselvesnaturally to parallelization.nearest-neighbor methodsfor examplecan easilybe distributed among several processors by splitting the data into several partsand letting each processor find the nearest neighbor in its part ofthe trainingset.decision tree learners can be parallelized by letting each processor build asubtree ofthe complete tree.bagging and stacking not boosting arenaturally parallel algorithms.howeverparallelization is only a partial remedybecause with a fixed number ofprocessorsthe algorithm s asymptotic timecomplexity cannot be improved.a simple way to apply any algorithm to a large dataset is to split the data intochunks oflimited size and learn models separately for each onecombining theresult using voting or averaging.either a parallel bagging-like scheme or asequential boosting-like scheme can be employed for this purpose.boosting hasthe advantage that new chunks can be weighted based on the classifiers learnedfrom previous chunksthus transferring knowledge between chunks.in bothcases memory consumption increases linearly with dataset sizehence from massive am page form ofpruning is necessary for very large datasets.this can be done by settingaside some validation data and only adding a model from a new chunk to thecommittee classifier ifit increases the committee s performance on the valida-tion set.the validation set can also be used to identify an appropriate chunksize by running the method with several different chunk sizes in parallel andmonitoring performance on the validation set.the best but most challenging way to enable a learning paradigm to deal withvery large datasets would be to develop new algorithms with lower computa-tional complexity.in some casesit is provably impossible to derive exact algo-rithms with lower complexity.decision tree learners that deal with numericattributes fall into this category.their asymptotic time complexity is dominatedby the sorting process for the numeric attribute valuesa procedure that mustbe performed at least once for any given dataset.howeverstochastic algorithmscan sometimes be derived that approximate the true solution but require a muchsmaller amount oftime.background knowledge can make it possible to vastly reduce the amount ofdata that needs to be processed by a learning algorithm.depending on whichattribute is the classmost ofthe attributes in a huge dataset might turn out tobe irrelevant when background knowledge is taken into account.as usualitpays to carefully engineer the data that is passed to the learning scheme andmake the greatest possible use ofany prior information about the learningproblem at hand.ifinsufficient background knowledge is availablethe attrib-ute filtering algorithms described in section can often drastically reduce theamount ofdata possibly at the expense ofa minor loss in predictive per-formance.some ofthese for exampleattribute selection using decision treesor the learning scheme are linear in the number ofattributes.just to give you a feeling for the amount ofdata that can be handled bystraightforward implementations ofmachine learning algorithms on ordinarymicrocomputerswe ran the decision tree learner on a dataset with attributes numeric and binaryand a class with sevenvalues.we used a pentium processor with a clock and a java virtualmachine with a just-in-time compiler. it took minutes to load the data filebuild the tree using reduced-error pruningand classify all the traininginstances.the tree had nodes.note that this implementation is writtenin javaand executing a java program is often several times slower than runninga corresponding program written in c because the java byte-code must be trans-lated into machine code before it can be executed.in our experience the difference is usually a factor ofthree to five ifthe virtual machine uses a just-in-time compiler.there are datasets today that truly deserve the adjective massive.scientificdatasets from astrophysicsnuclear physicsearth scienceand molecular biologyare measured in hundreds ofgigabytes or even terabytes.so are datasets onextensions and am page containing records offinancial transactions.application ofstandard programsfor machine learning to such datasets in their entirety is a very domain knowledgethroughout this book we have emphasized the importance ofgetting to knowyour data when undertaking practical data mining.knowledge ofthe domainis absolutely essential for success.data about data is often called metadataandone ofthe frontiers in machine learning is the development ofschemes to allowlearning methods to take metadata into account in a useful way.you don t have to look far for examples ofhow metadata might be applied.in chapter we divided attributes into nominal and numeric.but we also notedthat many finer distinctions are possible.ifan attribute is numeric an orderingis impliedbut sometimes there is a zero point and sometimes not timeintervals there isbut for dates there is not.even the ordering may be nonstandardangular degrees have an ordering different from that ofintegersbecause is the same as and is the same as or indeed schemes assume ordinary linear orderingas do learning schemesthat accommodate numeric attributesbut it would be a routine matter toextend them to circular orderings.categorical data may also be ordered.imagine how much more difficult our lives would be ifthere were no conven-tional ordering for letters ofthe alphabet.looking up a listing in the hongkong telephone directory presents an interesting and nontrivial problem! andthe rhythms ofeveryday life are reflected in circular orderingsdays ofthe weekmonths ofthe year.to further complicate matters there are many other kindsoforderingsuch as partial orderings on subsetssubset a may include subsetbsubset b may include subset aor neither may include the other.extendingordinary learning schemes to take account ofthis kind ofinformation in a satisfactory and general way is an open research problem.metadata often involves relations among attributes.three kinds ofrelationscan be distinguishedsemanticcausaland functional.a semanticrelationbetween two attributes indicates that ifthe first is included in a rulethe secondshould betoo.in this caseit is known a priori that the attributes only makesense together.for examplein agricultural data that we have analyzedanattribute called milk productionmeasures how much milk an individual cowproducesand the purpose ofour investigation meant that this attribute had asemantic relationship with three other attributescow-identifierherd-identifierand farmer-identifier.in other wordsa milk production value can only beunderstood in the context ofthe cow that produced the milkand the cow isfurther linked to a specific herd owned by a given farmer.semantic domain am page areofcourseproblem dependentthey depend not just on the dataset but alsoon what you are trying to do with it.causalrelations occur when one attribute causes another.in a system that istrying to predict an attribute caused by anotherwe know that the other attrib-ute must be included to make the prediction meaningful.for examplein theagricultural data mentioned previously there is a chain from the farmerherdand cow identifiersthrough measured attributes such as milk productiondownto the attribute that records whether a particular cow was retained or sold bythe farmer.learned rules should recognize this chain ofdependence.functionaldependencies occur in many databasesand the people who createdatabases strive to identify them for the purpose ofnormalizing the relations inthe database.when learning from the datathe significance ofa functionaldependency ofone attribute on another is that ifthe latter is used in a rule thereis no need to consider the former.learning schemes often rediscover functionaldependencies that are already known.not only does this generate meaninglessor more accurately tautologicalrulesbut also othermore interesting patternsmay be obscured by the functional relationships.howeverthere has been muchwork in automatic database design on the problem ofinferring functionaldependencies from example queriesand the methods developed should proveuseful in weeding out tautological rules generated by learning schemes.taking these kinds ofmetadataor prior domain knowledgeinto accountwhen doing induction using any ofthe algorithms we have met does not seemto present any deep or difficult technical challenges.the only real problem and it is a big one is how to express the metadata in a general and easily under-standable way so that it can be generated by a person and used by the algorithm.it seems attractive to couch the metadata knowledge in just the same repre-sentation as the machine learning scheme generates.we focus on ruleswhichare the norm for much ofthis work.the rules that specify metadata correspondto prior knowledge ofthe domain.given training examplesadditional rules canbe derived by one ofthe rule induction schemes we have already met.in thiswaythe system might be able to combine experience examples with theory domain knowledge.it would be capable ofconfirming andmodifying its programmed-in knowledge based on empirical evidence.looselyputthe user tells the system what he or she knowsgives it some examplesandit figures the rest out for itself!to make use ofprior knowledge expressed as rules in a sufficiently flexiblewayit is necessary for the system to be able to perform logical deduction.otherwisethe knowledge has to be expressed in precisely the right form for thelearning algorithm to take advantage ofitwhich is likely to be too demandingfor practical use.consider causal metadataifa causes b and b causes cthenwe would like the system to deduce that a causes c rather than having to statethat fact explicitly.although in this simple example explicitly stating the onextensions and am page fact presents little problemin practicewith extensive metadatait will be unrealistic to expect the system s users to express all logical consequences oftheir prior knowledge.a combination ofdeduction from prespecified domain knowledge andinduction from training examples seems like a flexible way ofaccommodatingmetadata.at one extremewhen examples are scarce nonexistentdeduc-tion is the prime only means ofgenerating new rules.at the otherwhenexamples are abundant but metadata is scarce nonexistentthe standardmachine learning techniques described in this book suffice.practical situationsspan the territory between.this is a compelling visionand methods ofinductive logic programmingmentioned in section a general way ofspecifying domain knowledgeexplicitly through statements in a formal logic language.howevercurrent logicprogramming solutions suffer serious shortcomings in real-world environ-ments.they tend to be brittle and to lack robustnessand they may be so com-putation intensive as to be completely infeasible on datasets ofany practical size.perhaps this stems from the fact that they use first-order logicthat isthey allowvariables to be introduced into the rules.the machine learning schemes we have seenwhose input and output are represented in terms ofattributes andconstant valuesperform their machinations in propositional logic without variables greatly reducing the search space and avoiding all sorts ofdifficultproblems ofcircularity and termination.some aspire to realize the visionwithout the accompanying brittleness and computational infeasibility offulllogic programming solutions by adopting simplified reasoning systems.othersplace their faith in the general mechanism ofbayesian networksintroduced insection which causal constraints can be expressed in the initial networkstructure and hidden variables can be postulated and evaluated automatically.it will be interesting to see whether systems that allow flexible specification ofdifferent types ofdomain knowledge will become widely and web miningdata mining is about looking for patterns in data.likewisetext mining is aboutlooking for patterns in textit is the process ofanalyzing text to extract infor-mation that is useful for particular purposes.compared with the kind ofdatawe have been talking about in this booktext is unstructuredamorphousanddifficult to deal with.neverthelessin modern western culturetext is the mostcommon vehicle for the formal exchange ofinformation.the motivation fortrying to extract information from it is compelling even ifsuccess is only partial.the superficial similarity between text and data mining conceals real differ-ences.in chapter we characterized data mining as the extraction and web am page previously unknownand potentially useful information from data.with textmininghoweverthe information to be extracted is clearly and explicitly statedin the text.it is not hidden at all most authors go to great pains to make surethat they express themselves clearly and unambiguously.from a human pointofviewthe only sense in which it is previously unknown is that time restric-tions make it infeasible for people to read the text themselves.the problemofcourseis that the information is not couched in a manner that is amenable toautomatic processing.text mining strives to bring it out in a form suitable forconsumption by computers or by people who do not have time to read the full text.a requirement common to both data and text mining is that the informa-tion extracted should be potentially useful.in one sensethis means actionable capable ofproviding a basis for actions to be taken automatically.in the case ofdata miningthis notion can be expressed in a relatively domain-independentwayactionable patterns are ones that allow nontrivial predictions to be madeon new data from the same source.performance can be measured by countingsuccesses and failuresstatistical techniques can be applied to compare differentdata mining methods on the same problemand so on.howeverin many textmining situations it is hard to characterize what actionable means in a waythat is independent ofthe particular domain at hand.this makes it difficult tofind fair and objective measures ofsuccess.as we have emphasized throughout this book potentially useful is oftengiven another interpretation in practical data miningthe key for success is thatthe information extracted must be comprehensiblein that it helps to explain thedata.this is necessary whenever the result is intended for human consumptionrather than as well as for automatic action.this criterion is less applicableto text mining becauseunlike data miningthe input itselfis comprehensible.text mining with comprehensible output is tantamount to summarizing salientfeatures from a large body oftextwhich is a subfield in its own righttext summarization.we have already encountered one important text mining problemdocumentclassificationin which each instance represents a document and the instance sclass is the document s topic.documents are characterized by the words thatappear in them.the presence or absence ofeach word can be treated as aboolean attributeor documents can be treated as bags ofwordsrather thansetsby taking word frequencies into account.we encountered this distinctionin section we learned how to extend na ve bayes to the bag-of-wordsrepresentationyielding the multinomial version ofthe algorithm.there isofcoursean immense number ofdifferent wordsand most ofthemare not very useful for document classification.this presents a classic featureselection problem.some words for examplefunction wordsoften called stopwords can usually be eliminated a prioribut although these occur onextensions and am page frequently there are not all that many ofthem.other words occur so rarely thatthey are unlikely to be useful for classification.paradoxicallyinfrequent wordsare common nearly halfthe words in a typical document or corpus ofdocu-ments occur just once.neverthelesssuch an overwhelming number ofwordsremains after these word classes are removed that further feature selection may benecessary using the methods described in section issue is that thebag- set- of-words model neglects word order and contextual effects.thereis a strong case for detecting common phrases and treating them as single units.document classification is supervised learningthe categories are knownbeforehand and given in advance for each training document.the unsupervisedversion ofthe problem is called document clustering.here there is no predefinedclassbut groups ofcognate documents are sought.document clustering canassist information retrieval by creating links between similar documentswhichin turn allows related documents to be retrieved once one ofthe documents hasbeen deemed relevant to a query.there are many applications ofdocument classification.a relatively easy categorization tasklanguage identificationprovides an important piece ofmetadata for documents in international collections.a simple representationthat works well for language identification is to characterize each document by a profile that consists ofthe n-gramsor sequences ofnconsecutive lettersthat appear in it.the most frequent or so n-grams are highly correlatedwith the language.a more challenging application is authorship ascriptioninwhich a document s author is uncertain and must be guessed from the text.herethe stopwordsnot the content wordsare the giveawaybecause their dis-tribution is author dependent but topic independent.a third problem is theassignment ofkey phrasesto documents from a controlled vocabulary ofpossi-ble phrasesgiven a large number oftraining documents that are tagged fromthis vocabulary.another general class oftext mining problems is metadata extraction.meta-data was mentioned previously as data about datain the realm oftext the termgenerally refers to salient features ofa worksuch as its authortitlesubject clas-sificationsubject headingsand keywords.metadata is a kind ofhighly struc-tured therefore actionable document summary.the idea ofmetadata isoften expanded to encompass words or phrases that stand for objects or enti-ties in the worldleading to the notion ofentity extraction.ordinary documentsare full ofsuch termsphone numbersfax numbersstreet addressesemailaddressesemail signaturesabstractstables ofcontentslists ofreferencestablesfigurescaptionsmeeting announcementsweb addressesand more.inadditionthere are countless domain-specific entitiessuch as international standard book numbers symbolschemical structuresand mathematical equations.these terms act as single vocabulary itemsand manydocument processing tasks can be significantly improved ifthey are and web am page as such.they can aid searchinginterlinkingand cross-referencing betweendocuments.how can textual entities be identified? rote learningthat isdictionarylookupis one ideaparticularly when coupled with existing resources lists ofpersonal names and organizationsinformation about locations from gazetteersor abbreviation and acronym dictionaries.another is to use capitalization andpunctuation patterns for names and acronymstitles prefixes unusual language statistics for foreign names.regularexpressions suffice for artificial constructs such as uniform resource locatorsurlsexplicit grammars can be written to recognize dates and sums ofmoney.even the simplest task opens up opportunities for learning to cope withthe huge variation that real-life documents present.as just one examplewhatcould be simpler than looking up a name in a table? but the name ofthe libyanleader muammar qaddafiis represented in different ways on documents thathave been received by the library ofcongress!many short documents describe a particular kind ofobject or eventcom-bining entities into a higher-level composite that represent the document sentire content.the task ofidentifying the composite structurewhich can oftenbe represented as a template with slots that are filled by individual pieces ofstructured informationis called information extraction.once the entities havebeen foundthe text is parsed to determine relationships among them.typicalextraction problems require finding the predicate structure ofa small set ofpre-determined propositions.these are usually simple enough to be captured byshallow parsing techniques such as small finite-state grammarsalthoughmatters may be complicated by ambiguous pronoun references and attachedprepositional phrases and other modifiers.machine learning has been appliedto information extraction by seeking rules that extract fillers for slots in the template.these rules may be couched in pattern-action formthe patternsexpressing constraints on the slot-filler and words in its local context.these constraints may involve the words themselvestheir part-of-speech tagsandtheir semantic classes.taking information extraction a step furtherthe extracted information canbe used in a subsequent step to learn rules not rules about how to extractinformation but rules that characterize the content ofthe text itself.these rulesmight predict the values for certain slot-fillers from the rest ofthe text.in certaintightly constrained situationssuch as internet job postings for computing-related jobsinformation extraction based on a few manually constructed train-ing examples can compete with an entire manually constructed database interms ofthe quality ofthe rules inferred.the world wide web is a massive repository oftext.almost all ofit differsfrom ordinary plain text because it contains explicit structural onextensions and am page markup is internal and indicates document structure or formatother markupis external and defines explicit hypertext links between documents.these infor-mation sources give additional leverage for mining web documents.web miningis like text mining but takes advantage ofthis extra information and oftenimproves results by capitalizing on the existence oftopic directories and otherinformation on the web.internet resources that contain relational data telephone directories orproduct catalogs use hypertext markup language formatting com-mands to clearly present the information they contain to web users.howeverit is quite difficult to extract data from such resources automatically.to do soexisting software systems use simple parsing modules called wrappersto analyzethe page structure and extract the requisite information.ifwrappers are codedby handwhich they often arethis is a trivial kind oftext mining because itrelies on the pages having a fixedpredetermined structure from which infor-mation can be extracted algorithmically.but pages rarely obey the rules.theirstructures varyweb sites evolve.errors that are insignificant to human readersthrow automatic extraction procedures completely awry.when change occursadjusting a wrapper manually can be a nightmare that involves getting yourhead around the existing code and patching it up in a way that does not causebreakage elsewhere.enter wrapper induction learning wrappers automatically from examples.the input is a training set ofpages along with tuples representing the informa-tion derived from each page.the output is a set ofrules that extracts the tuplesby parsing the page.for exampleit might look for certain html delimiters paragraph boundaries entries boldface that the webpage designer has used to set offkey items ofinformationand learn thesequence in which entities are presented.this could be accomplished by iterat-ing over all choices ofdelimitersstopping when a consistent wrapper is encoun-tered.then recognition will depend only on a minimal set ofcuesprovidingsome defense against extraneous text and markers in the input.alternativelyone might follow epicurus s advice at the end ofsection and seek a robustwrapper that uses multiple cues to guard against accidental variation.the greatadvantage ofautomatic wrapper induction is that when errors are caused bystylistic variants it is simple to add these to the training data and reinduce a newwrapper that takes them into account.wrapper induction reduces recognitionproblems when small changes occur and makes it far easier to produce new setsofextraction rules when structures change radically.a development called the semantic webaims to enable people to publishinformation in a way that makes its structure and semantics explicit so that it can be repurposed instead ofmerely read.this would render wrapper induction superfluous.but ifand when the semantic web is and web am page requirement for manual markup not to mention the huge volumes oflegacypages will likely increase the demand for automatic induction ofinformation structure.text miningincluding web miningis a burgeoning technology that is stillbecause ofits newness and intrinsic difficultyin a fluid state akinperhapsto the state ofmachine learning in the is no real consensusabout what it coversbroadly interpretedall natural language processing comesunder the ambit oftext mining.it is usually difficult to provide general andmeaningful evaluations because the mining task is highly sensitive to the par-ticular text under consideration.automatic text mining techniques have a longway to go before they rival the ability ofpeopleeven without any special domainknowledgeto glean information from large document collections.but they willgo a long waybecause the demand is situationsa prime application ofmachine learning is junk email filtering.as we writethese words late scourge ofunwanted email is a burning issue maybe by the time you read them the beast will have been vanquished or at leasttamed.at first blush junk email filtering appears to present a standard problemofdocument classificationdivide documents into ham and spam on thebasis ofthe text they containguided by training dataofwhich there are copiousamounts.but it is not a standard problem because it involves an adversarialaspect.the documents that are being classified are not chosen randomly froman unimaginably huge set ofall possible documentsthey contain emails thatare carefully crafted to evade the filtering processdesigned specifically to beatthe system.early spam filters simply discarded messages containing spammy wordsthat connote such things as sexlucreand quackery.ofcoursemuch legitimatecorrespondence concerns gendermoneyand medicinea balance must bestruck.so filter designers recruited bayesian text classification schemes thatlearned to strike an appropriate balance during the training process.spammersquickly adjusted with techniques that concealed the spammy words by mis-spelling themoverwhelmed them with legitimate textperhaps printed in white on a white background so that only the filter saw itor simply put thespam text elsewherein an image or a url that most email readers downloadautomatically.the problem is complicated by the fact that it is hard to compare spam detec-tion algorithms objectivelyalthough training data aboundsprivacy issues preclude publishing large public corpora ofrepresentative email.and there arestrong temporal effects.spam changes character rapidlyinvalidating onextensions and am page statistical tests such as cross-validation.finallythe bad guys can also usemachine learning.for exampleifthey could get hold ofexamples ofwhat yourfilter blocks and what it lets throughthey could use this as training data to learnhow to evade it.there areunfortunatelymany other examples ofadversarial learning situa-tions in our world today.closely related to junk email is search engine spamsites that attempt to deceive internet search engines into placing them pro-minently in lists ofsearch results.highly ranked pages yield direct financial benefits to their owners because they present opportunities for advertisingpro-viding strong motivation for profit seekers.then there are the computer viruswarsin which designers ofviruses and virus-protection software react to oneanother s innovations.here the motivation tends to be general disruption anddenial ofservice rather than monetary gain.computer network security is a continually escalating battle.protectorsharden networksoperating systemsand applicationsand attackers find vulnerabilities in all three areas.intrusion detection systems sniffout unusualpatterns ofactivity that might be caused by a hacker s reconnaissance activity.attackers realize this and try to obfuscate their trailsperhaps by working indi-rectly or by spreading their activities over a long time orconverselyby strik-ing very quickly.data mining is being applied to this problem in an attempt todiscover semantic connections among attacker traces in computer network datathat intrusion detection systems miss.this is a large-scale problemaudit logsused to monitor computer network security can amount to gigabytes a day evenin medium-sized organizations.many automated threat detection systems are based on matching current datato known attack types.the u.s.federal aviation administration developed thecomputer assisted passenger pre-screening systemcappswhich screensairline passengers on the basis oftheir flight records and flags individuals foradditional checked baggage screening.although the exact details are unpub-lishedcapps isfor examplethought to assign higher threat scores to cashpayments.howeverthis approach can only spot known or anticipated threats.researchers are using unsupervised approaches such as anomaly and outlierdetection in an attempt to detect suspicious activity.as well as flagging poten-tial threatsanomaly detection systems can be applied to the detection ofillegalactivities such as financial fraud and money laundering.data mining is being used today to sift through huge volumes ofdata in thename ofhomeland defense.heterogeneous information such as financial trans-actionshealth-care recordsand network traffic is being mined to create pro-filesconstruct social network modelsand detect terrorist communications.this activity raises serious privacy concerns and has resulted in the devel-opment ofprivacy-preserving data mining techniques.these algorithms try to discern patterns in the data without accessing the original data am page typically by distorting it with random values.to preserve privacythey mustguarantee that the mining process does not receive enough information toreconstruct the original data.this is easier said than done.on a lighter notenot all adversarial data mining is aimed at combating nefar-ious activity.multiagent systems in complexnoisy real-time domains involveautonomous agents that must both collaborate in a team and compete againstantagonists.ifyou are having trouble visualizing thisthink soccer.robo-socceris a rich and popular domain for exploring how machine learning can be appliedto such difficult problems.players must not only hone low-level skills but mustalso learn to work together and adapt to the behavior patterns ofdifferent opponents.finallymachine learning has been used to solve a historical literary mysteryby unmasking a prolific author who had attempted to conceal his identity.askoppel and schler relateben ish chai was the leading rabbinic scholarin baghdad in the late nineteenth century.among his vast literary legacy are two separate collections ofabout hebrew-aramaic letters written inresponse to legal queries.he is known to have written one collection.althoughhe claims to have found the other in an archivehistorians suspect that he wroteittoobut attempted to disguise his authorship by deliberately altering his style.the problem this case presents to machine learning is that there is no corpus ofwork to ascribe to the mystery author.there were a few known candidatesbutthe letters could equally well have been written by anyone else.a new techniqueappropriately called unmaskingwas developed that creates a model to distin-guish the known author s work a from the unknown author s work xitera-tively removes those features that are most useful for distinguishing the twoandexamines the speed with which cross-validation accuracy degrades as more fea-tures are removed.the hypothesis is that ifwork x is written by work a s authorwho is trying to conceal his identitywhatever differences there are betweenwork x and work a will be reflected in only a relatively small number offea-tures compared with the differences between work x and the works ofa differ-ent authorsay the author ofwork b.in other wordswhen work x is comparedwith works a and bthe accuracy curve as features are removed will declinemuch faster for work a than it does for work b.koppel and schler concludedthat ben ish chai did indeed write the mystery lettersand their technique is astriking example ofthe original and creative use ofmachine learning in anadversarial data miningwe began this book by pointing out that we are overwhelmed with data.nowhere does this affect the lives ofordinary people more than on the worldwide web.at presentthe web contains more than billion onextensions and am page ing perhaps tb and it continues to grow exponentiallydoubling every months or so.most u.s.consumers use the web.none ofthem can keep pace with the information explosion.whereas data mining originated in thecorporate world because that s where the databases aretext mining is movingmachine learning technology out ofthe companies and into the home.when-ever we are overwhelmed by data on the webtext mining promises tools totame it.applications are legion.finding friends and contacting themmain-taining financial portfoliosshopping for bargains in an electronic worldusingdata detectors ofany kind all ofthese could be accomplished automaticallywithout explicit programming.already text mining techniques are being usedto predict what link you re going to click nextto organize documents for youand to sort your mail.in a world where information is overwhelmingdisor-ganizedand anarchictext mining may be the solution we so desperately need.many believe that the web is but the harbinger ofan even greater paradigmshiftubiquitous computing.small portable devices are everywhere mobilephonespersonal digital assistantspersonal stereo and video playersdigitalcamerasmobile web access.already some devices integrate all these functions.they know our location in physical time and spacehelp us communicate insocial spaceorganize our personal planning spacerecall our pastand envelopus in global information space.it is easy to find dozens ofprocessors in amiddle-class home in the u.s.today.they do not communicate with oneanother or with the global information infrastructure yet.but they willandwhen they do the potential for data mining will soar.take consumer music.popular music leads the vanguard oftechnologicaladvance.sony s original walkman paved the way to today s ubiquitous portableelectronics.apple s ipod pioneered large-scale portable storage.napster snetwork technology spurred the development ofpeer-to-peer protocols.recommender systems such as firefly brought computing to social networks.in the near future content-aware music services will migrate to portable devices.applications for data mining in networked communities ofmusic service users will be legiondiscovering musical trendstracking preferences and tastesand analyzing listening behaviors.ubiquitous computing will weave digital space closely into real-world activ-ities.to manyextrapolating their own computer experiences ofextreme frus-trationarcane technologyperceived personal inadequacyand machine failurethis sounds like a nightmare.but proponents point out that it can t be like thatbecauseifit isit won t work.today s visionaries foresee a world of calm com-puting in which hidden machines silently conspire behind the scenes to makeour lives richer and easier.they ll reach beyond the big problems ofcorporatefinance and school homework to the little annoyances such as where are the carkeyscan i get a parking placeand is that shirt i saw last week at macy s still onthe rack? clocks will find the correct time after a power failurethe data am page will download new recipes from the internetand kid s toys will refresh them-selves with new games and new vocabularies.clothes labels will track washingcoffee cups will alert cleaning staffto moldlight switches will save energy ifnoone is in the roomand pencils will digitize everything we draw.where will datamining be in this new world? everywhere!it s hard to point to examples ofa future that does not yet exist.but ad-vances in user interface technology are suggestive.many repetitive tasks indirect-manipulation computer interfaces cannot be automated with standardapplication toolsforcing computer users to perform the same interface actions repeatedly.this typifies the frustrations alluded to previouslywho s incharge me or it? experienced programmers might write a script to carry outsuch tasks on their behalfbut as operating systems accrue layer upon layer ofcomplexity the power ofprogrammers to command the machine is eroded andvanishes altogether when complex functionality is embedded in appliancesrather than in general-purpose computers.research in programming by demonstrationenables ordinary computer usersto automate predictable tasks without requiring any programming knowledgeat all.the user need only know how to perform the task in the usual way to beable to communicate it to the computer.one systemcalled familiarhelps usersautomate iterative tasks involving existing applications on macintosh comput-ers.it works across applications and can work with completely new ones neverbefore encountered.it does this by using apple s scripting language to gleaninformation from each application and exploiting that information to makepredictions.the agent tolerates noise.it generates explanations to inform thecomputer user about its predictionsand incorporates feedback.it s adaptiveitlearns specialized tasks for individual users.furthermoreit is sensitive to eachuser s style.iftwo people were teaching a task and happened to give identicaldemonstrationsfamiliar would not necessarily infer identical programs it stuned to their habits because it learns from their interaction history.familiar employs standard machine learning techniques to infer the user sintent.rules are used to evaluate predictions so that the best one can be pre-sented to the user at each point.these rules are conditional so that users canteach classification tasks such as sorting files based on their type and assigninglabels based on their size.they are learned incrementallythe agent adapts toindividual users by recording their interaction history.many difficulties arise.one is scarcity ofdata.users are loathe to demon-strate several iterations ofa task they think the agent should immediatelycatch on to what they are doing.whereas a data miner would consider a dataset minisculeusers bridle at the prospect ofdemonstrating a taskeven halfa dozen times.a second difficulty is the plethora ofattributes.thecomputer desktop environment has hundreds offeatures that any given actionmight depend upon.this means that small datasets are overwhelmingly onextensions and am page to contain attributes that are apparently highly predictive but nevertheless irrelevantand specialized statistical tests are needed to compare alternativehypotheses.a third is that the iterativeimprovement-driven development stylethat characterizes data mining applications fails.it is impossible in principletocreate a fixed training-and-testing corpus for an interactive problem such asprogramming by demonstration because each improvement in the agent altersthe test data by affecting how users react to it.a fourth is that existing applica-tion programs provide limited access to application and user dataoften the rawmaterial on which successful operation depends is inaccessibleburied deepwithin the application program.data mining is already widely used at work.text mining is starting to bringthe techniques in this book into our own livesas we read our email and surfthe web.as for the futureit will be stranger than we can imagine.the spread-ing computing infrastructure will offer untold opportunities for learning.datamining will be therebehind the scenesplaying a role that will turn out to readingthere is a substantial volume ofliterature that treats the topic ofmassivedatasetsand we can only point to a few references here.fayyad and describe the application ofdata mining to voluminous data from scien-tific experiments.shafer et describe a parallel version ofa top-downdecision tree inducer.a sequential decision tree algorithm for massive disk-resident datasets has been developed by mehta et technique ofapplying any algorithm to a large dataset by splitting it into smaller chunks andbagging or boosting the result is described by breiman et explain the related pruning and selection scheme.despite its importancelittle seems to have been written about the generalproblem ofincorporating metadata into practical data mining.a scheme forencoding domain knowledge into propositional rules and its use for bothdeduction and induction has been investigated by giraud-carrier area ofinductive logic programmingwhich deals with knowledge rep-resented by first-order logic rulesis covered by bergadano and gunetti mining is an emerging areaand there are few comprehensive surveys ofthe area as a wholewitten provides one.a large number offeature selec-tion and machine learning techniques have been applied to text categorizationsebastiani describes applications ofdocument clusteringto information retrieval.cavnar and trenkle show how to use n-gramprofiles to ascertain with high accuracy the language in which a document iswritten.the use ofsupport vector machines for authorship ascription am page described by diederich et same technology was used by dumaiset to assign key phrases from a controlled vocabulary to documentson the basis ofa large number oftraining documents.the use ofmachine learn-ing to extract key phrases from the document text has been investigated byturney and frank et describes many problems ofinformation extraction.manyauthors have applied machine learning to seek rules that extract slot-fillers fortemplatesfor examplesoderland et mooney and nahm and mooney investigatedthe problem ofextracting information from job ads posted on internet newsgroups.an approach to finding information in running text based on compression techniques has been reported by witten et the plethora ofvariations ofmuammar qaddafion documents receivedby the library ofcongress.chakrabarti has written an excellent and comprehensive book ontechniques ofweb mining.kushmerick et developed techniques ofwrapper induction.the semantic web was introduced by tim berners-leeberners-lee et years earlier developed the technology behindthe world wide web.the first paper on junk email filtering was written by sahami et material on computer network security is culled from work by yurcik et information on the capps system comes from the u.s.house ofrepresentatives subcommittee on aviation the use ofunsupervisedlearning for threat detection is described by bay and schwabacher with current privacy-preserving data mining techniques have been identi-fied by datta et and veloso surveyed multiagent systemsofthe kind that are used for playing robo-soccer from a machine learning perspective.the fascinating story ofben ish chai and the technique used tounmask him is from koppel and schler vision ofcalm computingas well as the examples we have mentionedis from weiser and weiser and brown information on dif-ferent methods ofprogramming by demonstration can be found in compendiaby cypher and lieberman et report someexperience with learning apprentices.familiar is described by paynter tests are statistical tests that are suitable for smallsample problemsfrank describes their application in machine onextensions and am page partiithe weka machine am page am page experience shows that no single machine learning scheme is appropriate to alldata mining problems.the universal learner is an idealistic fantasy.as we haveemphasized throughout this bookreal datasets varyand to obtain accuratemodels the bias ofthe learning algorithm must match the structure ofthedomain.data mining is an experimental science.the weka workbench is a collection ofstate-of-the-art machine learningalgorithms and data preprocessing tools.it includes virtually all the algorithmsdescribed in this book.it is designed so that you can quickly try out existingmethods on new datasets in flexible ways.it provides extensive support for thewhole process ofexperimental data miningincluding preparing the input dataevaluating learning schemes statisticallyand visualizing the input data and the result oflearning.as well as a wide variety oflearning algorithmsitincludes a wide range ofpreprocessing tools.this diverse and comprehensivetoolkit is accessed through a common interface so that its users can comparedifferent methods and identify those that are most appropriate for the problemat to am page weka was developed at the university ofwaikato in new zealandand thename stands for waikato environment for knowledge analysis.outside the university the wekapronounced to rhyme with meccais a flightless bird withan inquisitive nature found only on the islands ofnew zealand.the system iswritten in java and distributed under the terms ofthe gnu general publiclicense.it runs on almost any platform and has been tested under linuxwindowsand macintosh operating systems and even on a personal digitalassistant.it provides a uniform interface to many different learning algorithmsalong with methods for pre- and postprocessing and for evaluating the result oflearning schemes on any given s in weka?weka provides implementations oflearning algorithms that you can easily applyto your dataset.it also includes a variety oftools for transforming datasetssuchas the algorithms for discretization described in chapter can preprocessa datasetfeed it into a learning schemeand analyze the resulting classifier andits performance all without writing any program code at all.the workbench includes methods for all the standard data mining problemsregressionclassificationclusteringassociation rule miningand attribute selec-tion.getting to know the data is an integral part ofthe workand many datavisualization facilities and data preprocessing tools are provided.all algorithmstake their input in the form ofa single relational table in the arff formatdescribed in section can be read from a file or generated by a data-base query.one way ofusing weka is to apply a learning method to a dataset and analyzeits output to learn more about the data.another is to use learned models togenerate predictions on new instances.a third is to apply several different learn-ers and compare their performance in order to choose one for prediction.thelearning methods are called classifiersand in the interactive weka interface youselect the one you want from a menu.many classifiers have tunable parameterswhich you access through a property sheet or object editor.a common evalua-tion module is used to measure the performance ofall classifiers.implementations ofactual learning schemes are the most valuable resourcethat weka provides.but tools for preprocessing the datacalled filterscome aclose second.like classifiersyou select filters from a menu and tailor them to your requirements.we will show how different filters can be usedlist the filtering algorithmsand describe their parameters.weka also includes imple-mentations ofalgorithms for learning association rulesclustering data forwhich no class value is specifiedand selecting relevant attributes in the datawhich we describe to am page do you use it?the easiest way to use weka is through a graphical user interface called theexplorer.this gives access to all ofits facilities using menu selection and formfilling.for exampleyou can quickly read in a dataset from an arff file and build a decision tree from it.but learning decision trees is justthe beginningthere are many other algorithms to explore.the explorer inter-face helps you do just that.it guides you by presenting choices as menusbyforcing you to work in an appropriate order by graying out options until theyare applicableand by presenting options as forms to be filled out.helpful tooltipspop up as the mouse passes over items on the screen to explain what theydo.sensible default values ensure that you can obtain results with a minimumofeffort but you will have to think about what you are doing to understandwhat the results mean.there are two other graphical user interfaces to weka.the knowledge flowinterface allows you to design configurations for streamed data processing.afundamental disadvantage ofthe explorer is that it holds everything in mainmemory when you open a datasetit immediately loads it all in.this meansthat it can only be applied to small to medium-sized problems.howeverwekacontains some incremental algorithms that can be used to process very largedatasets.the knowledge flow interface lets you drag boxes representing learn-ing algorithms and data sources around the screen and join them together into the configuration you want.it enables you to specify a data stream by con-necting components representing data sourcespreprocessing toolslearningalgorithmsevaluation methodsand visualization modules.ifthe filters andlearning algorithms are capable ofincremental learningdata will be loaded andprocessed incrementally.weka s third interfacethe experimenteris designed to help you answer abasic practical question when applying classification and regression techniqueswhich methods and parameter values work best for the given problem? thereis usually no way to answer this question a prioriand one reason we developedthe workbench was to provide an environment that enables weka users tocompare a variety oflearning techniques.this can be done interactively usingthe explorer.howeverthe experimenter allows you to automate the process bymaking it easy to run classifiers and filters with different parameter settings ona corpus ofdatasetscollect performance statisticsand perform significancetests.advanced users can employ the experimenter to distribute the comput-ing load across multiple machines using java remote method invocation this way you can set up large-scale statistical experiments and leave them torun.behind these interactive interfaces lies the basic functionality ofweka.thiscan be accessed in raw form by entering textual commandswhich gives do you use am page to all features ofthe system.when you fire up weka you have to choose amongfour different user interfacesthe explorerthe knowledge flowthe experi-menterand the command-line interface.we describe them in turn in the nextchapters.most people choose the explorerat least else can you do?an important resource when working with weka is the online documentationwhich has been automatically generated from the source code and conciselyreflects its structure.we will explain how to use this documentation and howto identify weka s major building blockshighlighting which parts containsupervised learning methodswhich contain tools for data preprocessingandwhich contain methods for other learning schemes.it gives the only completelist ofavailable algorithms because weka is continually growing and beinggenerated automatically from the source code the online documentation isalways up to date.moreoverit becomes essential ifyou want to proceed to thenext level and access the library from your own java programs or write and testlearning schemes ofyour own.in most data mining applicationsthe machine learning component is just asmall part ofa far larger software system.ifyou intend to write a data miningapplicationyou will want to access the programs in weka from inside your owncode.by doing soyou can solve the machine learning subproblem ofyour application with a minimum ofadditional programming.we show you how todo that by presenting an example ofa simple data mining application in java.this will enable you to become familiar with the basic data structures in wekarepresenting instancesclassifiersand filters.ifyou intend to become an expert in machine learning algorithms already are oneyou ll probably want to implement your ownalgorithms without having to address such mundane details as reading the datafrom a fileimplementing filtering algorithmsor providing code to evaluate theresults.ifsowe have good news for youweka already includes all this.to makefull use ofityou must become acquainted with the basic data structures.tohelp you reach this pointwe will describe these structures in more detail andexplain an illustrative implementation ofa do you get it?weka is available from httpwww.cs.waikato.ac.nzmlweka.you can downloadeither a platform-specific installer or an executable java jar file that you run inthe usual way ifjava is installed.we recommend that you download and installit nowand follow through the examples in the upcoming to am page weka s main graphical user interfacethe explorergives access to all its facili-ties using menu selection and form filling.it is illustrated in figure six different panelsselected by the tabs at the topcorresponding to thevarious data mining tasks that weka startedsuppose you have some data and you want to build a decision tree from it.firstyou need to prepare the data then fire up the explorer and load in the data.nextyou select a decision tree construction methodbuild a treeand interpret theoutput.it s easy to do it again with a different tree construction algorithm or a different evaluation method.in the explorer you can flip back and forthbetween the results you have obtainedevaluate the models that have been builton different datasetsand visualize graphically both the models and the datasetsthemselves including any classification errors the models am page preparing the datathe data is often presented in a spreadsheet or database.howeverweka s nativedata storage method is arff format can easily convert froma spreadsheet to arff.the bulk ofan arff file consists ofa list ofthe instancesand the attribute values for each instance are separated by commas spreadsheet and database programs allow you to export data into a file incomma-separated value format as a list ofrecords with commas betweenitems.having done thisyou need only load the file into a text editor or wordprocessoradd the dataset s name using the attribute infor-mation using a save the file as raw text.forexamplefigure shows an excel spreadsheet containing the weather datafrom section data in csv form loaded into microsoft wordand theresult ofconverting it manually into an arff file.howeveryou don t actuallyhave to go through these steps to create the arff file yourselfbecause theexplorer can read csv spreadsheet files directlyas described later.loading the data into the explorerlet s load this data into the explorer and start analyzing it.fire up weka to getthe panel shown in figure explorerfrom the four graphical explorerfigure explorer am page interface choices at the bottom.the others were mentioned earliersimple cliis the old-fashioned command-line interface.what you see next is the main explorer screenshown in figure figure shows what it will look like afteryou have loaded in the weatherdata.the six tabs along the top are the basic operations that the explorer supportsright now we are on preprocess.click the open filebutton to dataa spreadsheetb csv formatand am page bring up a standard dialog through which you can select a file.choose theweather.arfffile.ifyou have it in csv formatchange from arff data filestocsv data files.when you specify a it is automatically converted intoarff format.having loaded the filethe screen will be as shown in figure you about the datasetit has instances and five attributes leftthe attributes are called outlooktemperaturehumiditywindyand playlowerleft.the first attributeoutlookis selected by default can choose othersby clicking them and has no missing valuesthree distinct valuesand no uniquevaluesthe actual values are sunnyovercastand rainyand they occur fivefourand five timesrespectively right.a histogram at the lower right showshow often each ofthe two values ofthe classplayoccurs for each value oftheoutlookattribute.the attribute outlookis used because it appears in the boxabove the histogrambut you can draw a histogram ofany other attributeinstead.here playis selected as the class attributeit is used to color the his-togramand any filters that require a class value use it too.the outlookattribute in figure is nominal.ifyou select a numericattributeyou see its minimum and maximum valuesmeanand standard explorerabfigure weka explorera choosing the explorer interface and reading inthe weather am page deviation.in this case the histogram will show the distribution ofthe class as a function ofthis attribute example appears in figure on page can delete an attribute by clicking its checkbox and using the removebutton.allselects all the attributesnoneselects noneand invertinverts thecurrent selection.you can undo a change by clicking the undobutton.the editbutton brings up an editor that allows you to inspect the datasearch for par-ticular values and edit themand delete instances and attributes.right-clickingon values and column headers brings up corresponding context menus.building a decision treeto see what the decision tree learner described in section does withthis datasetuse the algorithmwhich is weka s implementation ofthis deci-sion tree actually implements a later and slightly improved versioncalled revision was the last public version ofthis family ofalgo-rithms before the commercial implementation was released. click theclassifytab to get a screen that looks like figure figureshows what it will look like afteryou have analyzed the weather data.first select the classifier by clicking the choosebutton at the top leftopeningup the treessection ofthe hierarchical menu in figure finding menu structure represents the organization ofthe weka code into moduleswhich will be described in chapter nowjust open up the hierarchy asnecessary the items you need to select are always at the lowest in the line beside the choosebutton as shown in with its default parameter values.ifyou click that linethe s object editor opens up and you can see what the parameters meanand alter their values ifyou wish.the explorer generally chooses sensibledefaults.having chosen the classifierinvoke it by clicking the startbutton.wekaworks for a briefperiod when it is workingthe little bird at the lower rightoffigure jumps up and dances and then produces the output shownin the main panel offigure the outputfigure shows the full output only gives the lower half.at the beginning is a summary ofthe datasetand the fact that tenfold cross-validation was used to evaluate it.that is the defaultand ifyou look closely atfigure you will see that the cross-validationbox at the left is checked.then comes a pruned decision tree in textual form.the first split is on theoutlookattributeand thenat the second levelthe splits are on humidityandwindyrespectively.in the tree structurea colon introduces the class label am page explorerabfigure finding it in the classifiers list and the am page absolute error root relative squared error total number of instances correctly classified instances incorrectly classified instances kappa statistic mean absolute error mean squared error taken to build model seconds stratified cross-validation summary run information relation weatherinstances outlook temperature humidity windy playtest mode cross-validation classifier model training set pruned tree------------------outlook sunny humidity yes humidity no overcast yes rainy windy true no windy false yes of leaves size of the tree from the decision tree am page has been assigned to a particular leaffollowed by the number ofinstances thatreach that leafexpressed as a decimal number because ofthe way the algorithmuses fractional instances to handle missing values.ifthere were incorrectly clas-sified instances aren t in this example their number would appeartoothus that two instances reached that leafofwhich one is classi-fied incorrectly.beneath the tree structure the number ofleaves is printedthenthe total number ofnodes ofthe tree.there is a way to view decision treesmore graphicallysee pages later in this chapter.the next part ofthe output gives estimates ofthe tree s predictive perform-ance.in this case they are obtained using stratified cross-validation with default in figure you can seemore than oftheinstances out have been misclassified in the cross-validation.this indi-cates that the results obtained from the training data are optimistic comparedwith what might be obtained from an independent test set from the same source.from the confusion matrix at the end in section observe that ofclass yeshave been assigned to class noand ofclass noare assignedto class yes.as well as the classification errorthe evaluation module also outputs thekappa statistic mean absolute errorand the root mean-squarederror ofthe class probability estimates assigned by the tree.the root mean-squared error is the square root ofthe average quadratic loss absolute error is calculated in a similar way using the absolute instead ofthe squared difference.it also outputs relative errorswhich are based on the priorprobabilities obtained by the zerorlearning scheme described later.finallyfor each class it also outputs some statistics from page explorer detailed accuracy by class rate fp rate precision recall f-measure class yes no confusion matrix a b classified as a yes b nofigure am page doing it againyou can easily run again with a different evaluation method.select usetraining setnear the top left in figure and click startagain.the clas-sifier output is quickly replaced to show how well the derived model performson the training setinstead ofshowing the cross-validation results.this evalu-ation is highly optimistic may still be usefulbecause it gener-ally represents an upper bound to the model s performance on fresh data.inthis caseall training instances are classified correctly.in some cases a classi-fier may decide to leave some instances unclassifiedin which case these will belisted as unclassified instances.this does not happen for most learning schemesin weka.the panel in figure has further test optionssupplied test setin whichyou specify a separate file containing the test setand percentage splitwith whichyou can hold out a certain percentage ofthe data for testing.you can outputthe predictions for each instance by clicking the more optionsbutton and check-ing the appropriate entry.there are other useful optionssuch as suppressingsome output and including other statistics such as entropy evaluation measuresand cost-sensitive evaluation.for the latter you must enter a cost matrixtypethe number ofclasses into the classesbox terminate it with the enterorreturnkey to get a default cost matrix edit the values asrequired.the small pane at the lower left offigure contains one high-lighted lineis a history list ofthe results.the explorer adds a new line when-ever you run a classifier.because you have now run the classifier twicethe listwill contain two items.to return to a previous result setclick the correspon-ding line and the output for that run will appear in the classifier output pane.this makes it easy to explore different classifiers or evaluation schemes andrevisit the results to compare them.working with modelsthe result history list is the entry point to some powerful features oftheexplorer.when you right-click an entry a menu appears that allows you to viewthe results in a separate windowor save the result buffer.more importantlyyoucan save the model that weka has generated in the form ofa java object file.you can reload a model that was saved previouslywhich generates a new entryin the result list.ifyou now supply a test setyou can reevaluate the old modelon that new set.several items in the right-click menu allow you to visualize the results invarious ways.at the top ofthe explorer interface is a separate visualizetabbutthat is differentit shows the datasetnot the results for a particular am page right-clicking an entry in the history list you can see the classifier errors.ifthe model is a tree or a bayesian network you can see its structure.you can also view the margin curve and various cost and threshold curvessection cost and threshold curves you must choose a class value froma submenu.the visualize threshold curvemenu item allows you to see the effectofvarying the probability threshold above which an instance is assigned to thatclass.you can select from a wide variety ofcurves that include the roc andrecall precision curves see thesechoose the x- and y-axes appro-priately from the menus given.for exampleset x to false positive rateand y totrue positive ratefor an roc curve or x to recall and y to precisionfor arecall precision curve.figure shows two ways oflooking at the result ofusing to classifythe iris dataset we use this rather than the weather data becauseit produces more interesting pictures.figure shows the tree.right-clicka blank space in this window to bring up a menu enabling you to automaticallyscale the view or force the tree into the window.drag the mouse to pan aroundthe space.it s also possible to visualize the instance data at any nodeifit hasbeen saved by the learning algorithm.figure shows the classifier errors on a two-dimensional plot.you canchoose which attributes to use for x and y using the selection boxes at the top.alternativelyclick one ofthe speckled horizontal strips to the right ofthe plotleft-click for x and right-click for y.each strip shows the spread ofinstancesalong that attribute.x and y appear beside the ones you have chosen for the axes.the data points are colored according to their classblueredand green foriris setosairis versicolorand iris virginicarespectively is a key at thebottom ofthe screen.correctly classified instances are shown as crossesincor-rectly classified ones appear as boxes there are three in figure can click on an instance to bring up relevant detailsits instance numberthe values ofthe attributesits classand the predicted class.when things go wrongbeneath the result history listat the bottom offigure a status linethat sayssimplyok.occasionallythis changes to see error logan indicationthat something has gone wrong.for examplethere may be constraints amongthe various different selections you can make in a panel.most ofthe time theinterface grays out inappropriate selections and refuses to let you choose them.but occasionally the interactions are more complexand you can end up select-ing an incompatible set ofoptions.in this casethe status line changes whenweka discovers the incompatibility typically when you press start.to see theerrorclick the logbutton to the left ofthe weka in the lower right-hand cornerofthe am page the result on the iris dataseta the tree and the clas-sifier am page the explorerwe have briefly investigated two ofthe six tabs at the top ofthe explorer windowin figure and figure summaryhere s what all ofthe tabs the dataset and modify it in various learning schemes that perform classification or regressionand evaluate clusters for the association rules for the data and evaluate attributesselect the most relevant aspects in the different two-dimensional plots ofthe data and interactwith them.each tab gives access to a whole range offacilities.in our tour so farwe havebarely scratched the surface ofthe preprocessand classifypanels.at the bottom ofevery panel is a statusbox and a logbutton.the status box displays messages that keep you informed about what s going on.forexampleifthe explorer is busy loading a filethe status box will say so.right-clicking anywhere inside this box brings up a little menu with two optionsdisplay the amount ofmemory available to wekaand run the java garbage col-lector.note that the garbage collector runs constantly as a background taskanyway.clicking the logbutton opens a textual log ofthe actions that weka has per-formed in this sessionwith timestamps.as noted earlierthe little bird at the lower right ofthe window jumps up anddances when weka is active.the number beside the shows how many con-current processes that are running.ifthe bird is standing but stops movingit ssick! something has gone wrongand you should restart the explorer.loading and filtering filesalong the top ofthe preprocesspanel in figure are buttons for openingfilesurlsand databases.initiallyonly files whose names end in the file browserto see otherschange the formatitem in the file selectionbox.converting files to arffweka has three file format convertersfor spreadsheet files with the extension.csvfor s native file format with the extensions forserialized instances with the extension appropriate converter is usedbased on the extension.ifweka cannot load the datait tries to interpret it asarff.ifthat failsit pops up the box shown in figure am page the is a generic object editorused throughout weka for selecting and configuring objects.for examplewhen you set parameters for a classifieryouuse the same kind ofbox.the csvloaderfor is selected by defaultandthe morebutton gives you more information about itshown in figure is always worth looking at the documentation! in this caseit explains thatthe spreadsheet s first row determines the attribute names.click okto use thisconverter.for a different oneclick chooseto select from the list in arffloaderis the first optionand we reached this point only because itfailed.the csvloaderis the defaultand we clicked choosebecause we want adifferent one.the third option is for the formatin which there are twofiles for a datasetone giving field names and the other giving the actual data.the fourthfor serialized instancesis for reloading a dataset that has been savedas a java serialized object.any java object can be saved in this form and reloaded.as a native java formatit is quicker to load than an arff filewhich must beparsed and checked.when repeatedly reloading a large dataset it may be worthsaving it in this form.further features ofthe generic object editor in figure are savewhichsaves a configured objectand openwhich opens a previously saved one.theseare not useful for this particular kind ofobject.but other generic object editorpanels have many editable propertiesand having gone to some trouble to setthem up you may want to save the configured object to reuse later.abcfigure object editora the editorb more information moreandc choosing a converter am page files on your computer are not the only source ofdatasets for weka.you canopen a urland weka will use the hypertext transfer protocol todownload an arff file from the web.or you can open a database db any database that has a java database connectivity driver and retrieveinstances using the sql selectstatement.this returns a relation that weka readsin as an arff file.to make this work with your databaseyou may need tomodify the file wekaexperimentdatabaseutils.propsin the weka distributionby adding your database driver to it.to access this fileexpand the weka.jarfilein the weka distribution.data can be saved in all these formats using the savebutton in figure from loading and saving datasetsthe preprocesspanel also allows you tofilter them.filters are an important component ofweka.using filtersclicking choosenear the top left in figure gives a list offilters like thatin figure get a collapsed versionclick on an arrow to openup its contents.we will describe how to use a simple filter to delete specifiedattributes from a datasetin other wordsto perform manual attribute selection.the same effect can be achieved more easily by selecting the relevant attributesusing the tick boxes and pressing the removebutton.neverthelesswe describethe equivalent filtering operation explicitlyas an example.removeis an unsupervised attribute filterand to see it you must scroll furtherdown the list.when selectedit appears in the line beside the choosebuttonalong with its parameter values in this case the line reads simply remove. click that line to bring up a generic object editor with which you can examineand alter the filter s properties.you did the same thing earlier by clicking in figure to open the classifier s object editor. the objecteditor for the removefilter is shown in figure learn about itclickmoreto show the information in figure explains that the filterremoves a range ofattributes from the dataset.it has an optionattributeindicesthat specifies the range to act on and another called invertselectionthat deter-mines whether the filter selects attributes or deletes them.there are boxes forboth ofthese in the object editor shown in figure in fact we havealready set them to affect attributes and tem-perature and falseto remove rather than retain them.click okto set theseproperties and close the box.notice that the line beside the choosebutton nowreads remove the command-line version ofthe removefiltertheoption used to specify which attributes to remove.after configuring anobject it s often worth glancing at the resulting command-line formulation thatthe explorer sets up.apply the filter by clicking applyat the right-hand side offigure the screen in figure appears just like the one in am page the but with only three attributeshumiditywindyand play.at this pointthe fourth button in the row near the top becomes active.undoreverses the fil-tering operation and restores the original datasetwhich is useful when youexperiment with different filters.the first attributehumidityis selected and a summary ofits values appearson the right.as a numeric attributethe minimum and maximum valuesmeanand standard deviation are shown.below is a histogram that shows the distri-acbfigure a filtera the filtersmenub an object editorand moreinformation am page bution ofthe playattribute.unfortunatelythis display is impoverished becausethe attribute has so few different values that they fall into two equal-sized bins.more realistic datasets yield more informative histograms.training and testing learning schemesthe classifypanel lets you train and test learning schemes that perform classi-fication or regression.section explained how to interpret the output ofa decision tree learner and showed the performance figures that are auto-matically generated by the evaluation module.the interpretation ofthese is thesame for all models that predict a categorical class.howeverwhen evaluatingmodels for numeric predictionweka produces a different set ofperformancemeasures.as an examplein figure the cpu performance dataset from has been loaded into weka.you can see the histogram ofvaluesofthe first attributevendorat the lower right.in figure the modeltree inducer has been chosen as the classifier by going to the classifypanelclicking the choosebutton at the top leftopening up the treessection ofthehierarchical menu shown in figure clicking start.thehierarchy helps to locate particular classifiers by grouping items with explorerfigure weather data with two attributes am page the shows the output.the pruned model tree is simply a decisionstump with a split on the mmaxattribute and two linear modelsone for eachleaf.both models involve a nominal attributevendoras well as some numericones.the expressionvendoradvisersperryamdahlis interpreted as followsifvendoris either advisersperryor amdahlthen substitute description ofthe model tree is followed by several figures thatmeasure its performance.these are derived from the test option chosen infigure cross-validation stratifiedbecause that doesn tabfigure the cpu performance data with am page mmin mmax run information cpuinstances vendor myct mmin mmax cach chmin chmax classtest mode cross-validation classifier model training set pruned model treeusing smoothed linear modelsmmax num class vendorhoneywelliplibmcdcncrbasfgouldsiemensnasadvisersperryamdahl vendoradvisersperryamdahl vendoramdahl myct cach chmin chmax vendoradvisersperryamdahl vendoramdahl lm num class figure from the program for numeric am page the sense for numeric prediction.section explains themeaning ofthe various measures.ordinary linear regression scheme for numeric pre-dictionis found under linearregressionin the functionssection ofthe menu infigure builds a single linear regression model rather than the two infigure surprisinglyits performance is slightly worse.to get a feel for their relative performancelet s visualize the errors theseschemes makeas we did for the iris dataset in figure theentry in the history list and select visualize classifier errorsto bring up the two-dimensional plot ofthe data in figure points are color coded byclass but in this case the color varies continuously because the class is numeric.in figure the vendorattribute has been selected for the x-axis and theinstance number has been chosen for the y-axis because this gives a good spreadofpoints.each data point is marked by a cross whose size indicates the absolutevalue ofthe error for that instance.the smaller crosses in figure compared with those in figure linear regressionshow that is superior. mmin mmax cach chmin chmax of rules time taken to build model seconds cross-validation summary coefficient absolute error mean squared error absolute error root relative squared error total number of instances myct figure am page do it yourself the user classifierthe user classifier at the end ofsection allows weka users tobuild their own classifiers interactively.it resides in the treessection ofthe hier-archical menu in figure under userclassifier.we illustrate its operationon a new problemsegmenting visual image data into classes such as grassskyfoliagebrickand cementbased on attributes giving average explorerabfigure the errorsa from and from linear am page the various simple textural features.the training data file is suppliedwith the weka distribution and called segment-challenge.arff.having loaded itinselect the user classifier.for evaluation use the special test set called segment-test.arffas the supplied test seton the classifypanel.evaluation by cross-validation is impossible when you have to construct a classifier manually foreach fold.following starta new window appears and weka waits for you to build theclassifier.the tree visualizerand data visualizertabs switch between differentviews.the former shows the current state ofthe classification treeand each nodegives the number ofinstances ofeach class at that node.the aim is to come upwith a tree in which the leafnodes are as pure as possible.initially there is onlyone nodethe rootwhich contains all the data.switch to the data visualizertocreate a split.this shows the same two-dimensional plot that we saw in for the iris dataset and figure for the cpu performance data.theattributes to use for x and y are selected as beforeand the goal here is to find acombination that separates the classes as cleanly as possible.figure a good choiceregion centroid rowfor x and intensity meanfor y.having found a good separationyou must specify a region in the graph.fourtools for this appear in the pull-down menu below the y-axis selector.selectinstanceidentifies a particular instance.rectangleshown in figure you to drag out a rectangle on the graph.with polygonand polylineyoubuild a free-form polygon or draw a free-form polyline to add a vertexand right-click to complete the operation.once an area has been selecteditturns gray.in figure the user has defined a rectangle.the submitbutton creates two new nodes in the treeone holding the selected instances andthe other with all the rest.clearclears the selectionsavesaves the instances inthe current tree node as an arff file.at this pointthe tree visualizershows the tree in figure isa pure node for the skyclassbut the other node is mixed and should be splitfurther.clicking on different nodes determines which subset ofdata is shownby the data visualizer.continue adding nodes until you are satisfied with theresult that isuntil the leafnodes are mostly pure.then right-click on anyblank space in the tree visualizerand choose accept the tree.weka evaluatesyour tree on the test set and outputs performance statistics is a good scoreon this problem.building trees manually is very tedious.but weka can complete the task foryou by building a subtree under any nodejust right-click the node.using a metalearnermetalearners take simple classifiers and turn them into more pow-erful learners.for exampleto boost decision stumps in the explorergo to am page explorerabfigure on the segmentation data with the user classifiera the datavisualizer and the tree am page the and choose the classifier the metasection ofthehierarchical menu.when you configure this classifier by clicking itthe objecteditor shown in figure appears.this has its own classifier fieldwhich weset to decisionstumpas shown.this method could itselfbe configured byclicking that decisionstumphappens to have no editable properties.click okto return to the main classifypanel and startto try out boosting deci-sion stumps up to times.it turns out that this mislabels only ofthe in the iris data good performance considering the rudimentarynature ofdecision stumps and the rather small number ofboosting iterations.clustering and association rulesuse the clusterand associatepanels to invoke clustering algorithms and methods for finding association rules clusteringweka shows the number ofclusters and how many instances each cluster con-tains.for some algorithms the number ofclusters can be specified by setting aparameter in the object editor.for probabilistic clustering methodsweka mea-sures the log-likelihood ofthe clusters on the training datathe larger this quan-titythe better the model fits the data.increasing the number ofclustersnormally increases the likelihoodbut may overfit.the controls on the clusterpanel are similar to those for classify.you canspecify some ofthe same evaluation methods use training setsupplied testsetand percentage split last two are used with the log-likelihood.a furtherfigure a metalearner for boosting decision am page methodclasses to clusters evaluationcompares how well the chosen clustersmatch a preassigned class in the data.you select an attribute must benominal that represents the true class.having clustered the datawekadetermines the majority class in each cluster and prints a confusion matrixshowing how many errors there would be ifthe clusters were used instead ofthe true class.ifyour dataset has a class attributeyou can ignore it during clus-tering by selecting it from a pull-down list ofattributesand see how well theclusters correspond to actual class values.finallyyou can choose whether ornot to store the clusters for visualization.the only reason not to do so is to con-serve space.as with classifiersyou visualize the results by right-clicking on theresult listwhich allows you to view two-dimensional scatter plots like the onein figure have chosen classes to clusters evaluationthe classassignment errors are shown.for the cobwebclustering schemeyou can alsovisualize the tree.the associatepanel is simpler than classifyor cluster.weka contains onlythree algorithms for determining association rules and no methods for evalu-ating such rules.figure shows the output from the apriori program forassociation rules in section on the nominal version oftheweather data.despite the simplicity ofthe dataseveral rules are found.thenumber before the arrow is the mumber ofinstances for which the antecedentis truethat after the arrow is the number ofinstances in which the consequentis true alsoand the confidence parentheses is the ratio between the two.ten rules are found by defaultyou can ask for more by using the object editorto change numrules.attribute selectionthe select attributespanel gives access to several methods for attribute selection.as explained in section involves an attribute evaluator and a outlookovercast playyes temperaturecool humiditynormal humiditynormal windyfalse playyes outlooksunny playno humidityhigh outlooksunny humidityhigh playno outlookrainy playyes windyfalse outlookrainy windyfalse playyes temperaturecool playyes humiditynormal outlooksunny temperaturehot humidityhigh temperaturehot playno outlooksunny from the apriori program for association am page are chosen in the usual way and configured with the object editor.you must also decide which attribute to use as the class.attribute selection canbe performed using the full training set or using cross-validation.in the lattercase it is done separately for each foldand the output shows how many times that isin how many ofthe folds each attribute was selected.the results arestored in the history list.when you right-click an entry here you can visualizethe dataset in terms ofthe selected attributes visualize reduced data.visualizationthe visualizepanel helps you visualize a dataset not the result ofa classifi-cation or clustering modelbut the dataset itself.it displays a matrix oftwo-dimensional scatter plots ofevery pair ofattributes.figure shows theiris dataset.you can select an attribute normally the class for coloring thedata points using the controls at the bottom.ifit is nominalthe coloring is dis-creteifit is numericthe color spectrum ranges continuously from blue to orange values.data points with no class value are shown inblack.you can change the size ofeach plotthe size ofthe pointsand the amountofjitterwhich is a random displacement applied to x and y values to separatepoints that lie on top ofone another.without instances at the samedata point would look just the same as instance.you can reduce the size ofthe matrix ofplots by selecting certain attributesand you can subsample thedata for efficiency.changes in the controls do not take effect until the updatebutton is clicked.click one ofthe plots in the matrix to enlarge it.for exampleclicking onthe top left plot brings up the panel in figure can zoom in on anyarea ofthis panel by choosing rectanglefrom the menu near the top right anddragging out a rectangle on the viewing area like that shown.the submitbuttonnear the top left rescales the rectangle into the viewing algorithmsnow we take a detailed look at the filtering algorithms implemented withinweka.these are accessible from the explorerand also from the knowledge flowand experimenter interfaces described in chapters and filters trans-form the input dataset in some way.when a filter is selected using the choosebuttonits name appears in the line beside that button.click that line to get ageneric object editor to specify its properties.what appears in the line is thecommand-line version ofthe filterand the parameters are specified with minussigns.this is a good way oflearning how to use the weka commands directly.there are two kinds offilterunsupervised and supervised innocuous distinction masks a rather fundamental issue.filters am page often applied to a training dataset and then also applied to the test file.ifthefilter is supervised for exampleifit uses class values to derive good intervalsfor discretization applying it to the test data will bias the results.it is the dis-cretization intervals derived from the trainingdata that must be applied to thetest data.when using supervised filters you must be careful to ensure that the results are evaluated fairlyan issue that does not arise with unsupervisedfilters.we treat weka s unsupervised and supervised filtering methods separately.within each type there is a further distinction between attribute filterswhichwork on the attributes in the datasetsand instance filterswhich work on theinstances.to learn more about a particular filterselect it in the weka explorerafigure the iris am page look at its associated object editorwhich defines what the filter does andthe parameters it takes.unsupervised attribute filterstable lists weka s unsupervised attribute filters.many ofthe operationswere introduced in section and removing attributesaddinserts an attribute at a given positionwhose value is declared to be missingfor all instances.use the generic object editor to specify the attribute s namewhere it will appear in the list ofattributesand its possible values nominalattributes.copycopies existing attributes so that you can preserve them whenexperimenting with filters that overwrite attribute values.several attributes canbe copied together using an expression such as the first three attributesbfigure am page explorertable attribute filters.namefunctionaddadd a new attribute whose values are all marked as missing.addclusteradd a new nominal attribute representing the cluster assigned to each instance by a given clustering algorithm.addexpressioncreate a new attribute by applying a specified mathematical function to existing attributes.addnoisechange a percentage of a given nominal attribute s values.clustermembershipuse a clusterer to generate cluster membership values which then form the new attributes.copycopy a range of attributes in the dataset.discretizeconvert numeric attributes to nominal specify which attributes number of bins whether to optimize the number of bins and output binary attributes. use equal-width or equal-frequency binning.firstorderapply a first-order differencing operator to a range of numeric attributes.makeindicatorreplace a nominal attribute with a boolean attribute. assign value to instances with a particular range of attribute values otherwise assign by default the boolean attribute is coded as numeric.mergetwovaluesmerge two values of a given attribute specify the index of the two values to be merged.nominaltobinarychange a nominal attribute to several binary ones one for each value.normalizescale all numeric values in the dataset to lie within the interval all numeric attributes into binary ones nonzero values become a numeric attribute using any java function.obfuscateobfuscate the dataset by renaming the relation all attribute names and nominal and string attribute values.pkidiscretizediscretize numeric attributes using equal-frequency binning where the number of bins is equal to the square root of the number of values missing values.randomprojectionproject the data onto a lower-dimensional subspace using a random matrix.removeremove attributes.removetyperemove attributes of a given type numeric string or date.removeuselessremove constant attributes along with nominal attributes that vary too much.replacemissingvaluesreplace all missing values for nominal and numeric attributes with the modes and means of the training data.standardizestandardize all numeric attributes to have zero mean and unit variance.stringtonominalconvert a string attribute to nominal.stringtowordvectorconvert a string attribute to a vector that represents word occurrence frequencies you can choose the delimiters and there are many more options.swapvaluesswap two values of an attribute.timeseriesdeltareplace attribute values in the current instance with the difference between the current value and the value in some previous future instance.timeseriestranslatereplace attribute values in the current instance with the equivalent value in some previous future am page attributes selection can beinvertedaffecting all attributes exceptthose specified.these features are sharedby many filters.removehas already been described.similar filters are removetypewhichdeletes all attributes ofa given type dateandremoveuselesswhich deletes constant attributes and nominal attributes whosevalues are different for almost all instances.you can decide how much variationis tolerated before an attribute is deleted by specifying the number ofdistinctvalues as a percentage ofthe total number ofvalues.some unsupervised attrib-ute filters behave differently ifthe menu in the preprocesspanel has been usedto set a class attribute.for exampleremovetypeand removeuselessboth skipthe class attribute.addclusterapplies a clustering algorithm to the data before filtering it.youuse the object editor to choose the clustering algorithm.clusterers are config-ured just as filters are addclusterobject editor contains itsown choosebutton for the clustererand you configure the clusterer by clickingits line and getting anotherobject editor panelwhich must be filled in beforereturning to the addclusterobject editor.this is probably easier to understandwhen you do it in practice than when you read about it in a book! at any rateonce you have chosen a clustereraddclusteruses it to assign a cluster numberto each instanceas a new attribute.the object editor also allows you to ignorecertain attributes when clusteringspecified as described previously for copy.clustermembershipuses a clustereragain specified in the filter s object editorto generate membership values.a new version ofeach instance is created whoseattributes are these values.the class attributeifsetis left unaltered.addexpressioncreates a new attribute by applying a mathematical functionto numeric attributes.the expression can contain attribute references and con-stantsthe arithmetic operators functions logand expabsand sqrtfloorceiland sincosand tanand parentheses.attributesare specified by the prefix afor is the seventh attribute.an exampleexpression isthere is a debug option that replaces the new attribute s value with a postfixparse ofthe supplied expression.whereas addexpressionapplies mathematical functionsnumerictransformperforms an arbitrary transformation by applying a given java function toselected numeric attributes.the function can be anything that takes a doubleasits argument and returns another doublefor examplesqrtin rintfunction rounds to the closest am page one parameter is the name ofthe java class that implements the functionwhich must be a fully qualified nameanother is the name ofthe transfor-mation method itself.normalizescales all numeric values in the dataset to lie between and them to have zero mean and unit variance.both skip theclass attributeifset.changing valuesswapvaluesswaps the positions oftwo values ofa nominal attribute.the orderofvalues is entirely cosmetic it does not affect learning at all but ifthe classis selectedchanging the order affects the layout ofthe confusion matrix.mergetwovaluesmerges values ofa nominal attribute into a single category.thenew value s name is a concatenation ofthe two original onesand every occur-rence ofeither ofthe original values is replaced by the new one.the index ofthe new value is the smaller ofthe original indices.for exampleifyou mergethe first two values ofthe outlookattribute in the weather data in which thereare five sunnyfour overcastand five rainyinstances the new outlookattributewill have values sunny_overcastand rainythere will be nine sunny_overcastinstances and the original five rainyones.one way ofdealing with missing values is to replace them globally beforeapplying a learning scheme.replacemissingvaluesreplaces each missing valuewith the mean for numeric attributes and the mode for nominal ones.ifa classis setmissing values ofthat attribute are not replaced.conversionsmany filters convert attributes from one form to another.discretizeuses equal-width or equal-frequency binning to discretize a range ofnumericattributesspecified in the usual way.for the former method the number ofbinscan be specified or chosen automatically by maximizing the likelihood usingleave-one-out cross-validation.pkidiscretizediscretizes numeric attributesusing equal-frequency binning in which the number ofbins is the square rootofthe number ofvalues missing values.both these filters skip theclass attribute.makeindicatorconverts a nominal attribute into a binary indicator attributeand can be used to transform a multiclass dataset into several two-class ones.itsubstitutes a binary attribute for the chosen nominal onewhose value for eachinstance is ifa particular original value was present and otherwise.the newattribute is declared to be numeric by defaultbut it can be made nominal ifdesired.some learning schemessuch as support vector machinesonly handle binaryattributes.the nominaltobinaryfilter transforms all multivalued am page in a dataset into binary onesreplacing each attribute with kvaluesby kbinary attributes using a simple one-per-value encoding.attributes that arealready binary are left untouched.numerictobinaryconverts all numeric attrib-utes into nominal binary ones the classifset.ifthe value ofthenumeric attribute is exactly new attribute will be ifit is missingthe new attribute will be missingotherwisethe value ofthe new attribute willbe filters also skip the class attribute.firstordertakes a range ofnnumeric attributes and replaces them with numeric attributes whose values are the differences between consecutiveattribute values from the original instances.for exampleifthe original attrib-ute values were new ones will be and conversiona string attribute has an unspecified number ofvalues.stringtonominalcon-verts it to nominal with a set number ofvalues.you should ensure that all stringvalues that will appear in potential test data are represented in the dataset.stringtowordvectorproduces attributes that represent the frequency ofeachword in the string.the set ofwords that isthe new attribute set is deter-mined from the dataset.by default each word becomes an attribute whose valueis or that word s presence in the string.the new attributes can benamed with a user-determined prefix to keep attributes derived from differentstring attributes distinct.there are many options that affect tokenization.words can be formed fromcontiguous alphabetic sequences or separated by a given set ofdelimiter char-acters.they can be converted to lowercase before being added to the diction-aryor all words on a predetermined list ofenglish stopwords can be ignored.words that are not among the top kwords ranked by frequency can be discardedslightly more than kwords will be retained ifthere are ties at the kth position.ifa class attribute has been assignedthe top kwords for each class will be kept.the value ofeach word attribute reflects its presence or absence in the stringbut this can be changed.a count ofthe number oftimes the word appears inthe string can be used instead.word frequencies can be normalized to give eachdocument s attribute vector the same euclidean length this length is notchosen to be avoid the very small numbers that would entailbut to be theaverage length ofall documents that appear as values ofthe original stringattribute.alternativelythe frequencies fijfor word iin document jcan be trans-formed using log or the tf idf measure seriestwo filters work with time series data.timeseriestranslatereplaces the valuesofan attribute attributes in the current instance with the equivalent am page in some other or future instance.timeseriesdeltareplaces attributevalues in the current instance with the difference between the current value andthe value in some other instance.in both cases instances in which the time-shifted value is unknown may be removedor missing values may be used.randomizingother attribute filters degrade the data.addnoisetakes a nominal attribute andchanges a given percentage ofits values.missing values can be retained orchanged along with the rest.obfuscateanonymizes data by renaming the rela-tionattribute namesand nominal and string attribute values.randomprojec-tionprojects the dataset on to a lower-dimensional subspace using a randommatrix with columns ofunit length class attribute is notincluded in the projection.unsupervised instance filtersweka s instance filterslisted in table all instances in a dataset ratherthan all values ofa particular attribute or attributes.randomizing and subsamplingyou can randomizethe order ofinstances in the dataset.normalizetreats allnumeric attributes the class as a vector and normalizes it to a givenlength.you can specify the vector length and the norm to be used.there are various ways ofgenerating subsets ofthe data.use resampletoproduce a random sample by sampling with replacement or removefoldsto explorertable instance filters.namefunctionnonsparsetosparseconvert all incoming instances to sparse format numeric attributes as a vector and normalize it to a given lengthrandomizerandomize the order of instances in a datasetremovefoldsoutput a specified cross-validation fold for the datasetremovemisclassifiedremove instances incorrectly classified according to a specified classifier useful for removing outliersremovepercentageremove a given percentage of a datasetremoverangeremove a given range of instances from a datasetremovewithvaluesfilter out instances with certain attribute valuesresampleproduce a random subsample of a dataset sampling with replacementsparsetononsparseconvert all incoming sparse instances into nonsparse am page into a given number ofcross-validation folds and reduce it to just one ofthem.ifa random number seed is providedthe dataset will be shuffled before thesubset is extracted.removepercentageremoves a given percentage ofinstancesand removerangeremoves a certain range ofinstance numbers.to remove allinstances that have certain values for nominal attributesor numeric valuesabove or below a certain thresholduse removewithvalues.by default allinstances are deleted that exhibit one ofa given set ofnominal attribute valuesifthe specified attribute is nominal or a numeric value below a given thresh-old is numeric.howeverthe matching criterion can be inverted.you can remove outliers by applying a classification method to the datasetspecifying it just as the clustering method was specified previously for addcluster and use removemisclassifiedto delete the instances that it misclassifies.sparse instancesthe nonsparsetosparseand sparsetononsparsefilters convert between theregular representation ofa dataset and its sparse representation filterssupervised filters are available from the explorer s preprocesspaneljust as unsu-pervised ones are.you need to be careful with them becausedespite appear-ancesthey are not really preprocessing operations.we noted this previouslywith regard to discretization the test data splits must not use the test data sclass values because these are supposed to be unknown and it is true for super-vised filters in general.because ofpopular demandweka allows you to invoke supervised filters asa preprocessing operationjust like unsupervised filters.howeverifyou intendto use them for classification you should adopt a different methodology.a meta-learner is provided that invokes a filter in a way that wraps the learning algo-rithm into the filtering mechanism.this filters the test data using the filter thathas been created by the training data.it is also useful for some unsupervisedfilters.for examplein stringtowordvectorthe dictionary will be created fromthe training data alonewords that are novel in the test data will be discarded.to use a supervised filter in this wayinvoke the filteredclassifiermetalearningscheme from in the metasection ofthe menu displayed by the classifypanel schoosebutton.figure shows the object editor for this metalearningscheme.with it you choose a classifier and a filter.figure shows themenu offilters.supervised filterslike unsupervised onesare divided into attribute andinstance filterslisted in table and table am page supervised attribute filtersdiscretizehighlighted in figure the mdl method ofsupervised dis-cretization can specify a range ofattributes or force the dis-cretized attribute to be binary.the class must be nominal.by default fayyadand irani s criterion is usedbut kononenko s method is explorerabfigure weka s metalearner for discretizationa configuring filteredclas-sifierand the menu offilters.table attribute filters.namefunctionattributeselectionprovides access to the same attribute selection methods as the select attributespanelclassorderrandomize or otherwise alter the ordering of class valuesdiscretizeconvert numeric attributes to nominalnominaltobinaryconvert nominal attributes to binary using a supervised method if the class is numerictable instance filters.namefunctionresampleproduce a random subsample of a dataset sampling with replacementspreadsubsampleproduce a random subsample with a given spread between class frequencies sampling with replacementstratifiedremovefoldsoutput a specified stratified cross-validation fold for the am page is a supervised version ofthe nominaltobinaryfilter that transformsall multivalued nominal attributes to binary ones.in this versionthe transfor-mation depends on whether the class is nominal or numeric.ifnominalthesame method as before is usedan attribute with kvalues is transformed into kbinary attributes.ifthe class is numerichoweverthe method described insection is applied.in either case the class itselfis not transformed.classorderchanges the ordering ofthe class values.the user determineswhether the new ordering is random or in ascending or descending order ofclass frequency.this filter must not be used with the filteredclassifiermeta-learning scheme! attributeselectioncan be used for automatic attribute selec-tion and provides the same functionality as the explorer s select attributespaneldescribed later.supervised instance filtersthere are three supervised instance filters.resampleis like the eponymous un-supervised instance filter except that it maintains the class distribution in thesubsample.alternativelyit can be configured to bias the class distributiontowards a uniform one.spreadsubsamplealso produces a random subsamplebut the frequency difference between the rarest and the most common class canbe controlled for exampleyou can specify at most a difference in classfrequencies.like the unsupervised instance filter removefoldsstrati-fiedremovefoldsoutputs a specified cross-validation fold for the datasetexceptthat this time the fold is algorithmson the classifypanelwhen you select a learning algorithm using the choosebutton the command-line version ofthe classifier appears in the line beside thebuttonincluding the parameters specified with minus signs.to change themclick that line to get an appropriate object editor.table lists weka s classi-fiers.they are divided into bayesian classifierstreesrulesfunctionslazy clas-sifiersand a final miscellaneous category.we describe them briefly herealongwith their parameters.to learn morechoose one in the weka explorer inter-face and examine its object editor.a further kind ofclassifierthe metalearneris described in the next section.bayesian classifiersnaivebayesimplements the probabilistic na ve bayes classifier the normal distribution to model numeric attrib-utes.naivebayescan use kernel density estimatorswhich improves perform-ance ifthe normality assumption is grossly incorrectit can also handle am page explorertable algorithms in weka.namefunctionbayesaodeaveraged one-dependence estimatorsbayesnetlearn bayesian netscomplementnaivebayesbuild a complement na ve bayes classifiernaivebayesstandard probabilistic na ve bayes classifiernaivebayesmultinomialmultinomial version of na ve bayesnaivebayessimplesimple implementation of na ve bayesnaivebayesupdateableincremental na ve bayes classifier that learns one instanceat a timetreesadtreebuild alternating decision treesdecisionstumpbuild one-level decision divide-and-conquer decision tree decision tree learner revision logistic model model tree learnernbtreebuild a decision tree with na ve bayes classifiers at theleavesrandomforestconstruct random forestsrandomtreeconstruct a tree that considers a given number of randomfeatures at each nodereptreefast tree learner that uses reduced-error pruninguserclassifierallow users to build their own decision treerulesconjunctiverulesimple conjunctive rule learnerdecisiontablebuild a simple decision table majority classifierjripripper algorithm for fast effective rule rules from model trees built using nngenearest-neighbor method of generating rules usingnonnested generalized classifierpartobtain rules from partial decision trees built using covering algorithm for rulesridorripple-down rule learnerzerorpredict the majority class nominal or the average valueif numericfunctionsleastmedsqrobust regression using the median rather than the meanlinearregressionstandard linear regressionlogisticbuild linear logistic regression modelsmultilayerperceptronbackpropagation neural networkpaceregressionbuild linear regression models using pace regressionrbfnetworkimplements a radial basis function networksimplelinearregressionlearn a linear regression model based on a single attributesimplelogisticbuild linear logistic regression models with built-inattribute selectionsmosequential minimal optimization algorithm for supportvector am page using supervised discretization.naivebayesupdateableis an incre-mental version that processes one instance at a timeit can use a kernel esti-mator but not discretization.naivebayesmultinomialimplements themultinomial bayes classifier a complement na ve bayes classifier as described by rennie et tf idf and length normalization transforms used in this paper can beperformed using the stringtowordvectorfilter.aode estimatorsis a bayesian method that aver-ages over a space ofalternative bayesian models that have weaker independenceassumptions than na ve bayes et algorithm may yieldmore accurate classification than na ve bayes on datasets with nonindependentattributes.bayesnetlearns bayesian networks under the assumptions made in attributes ones are prediscretized and no missing valuesany such values are replaced globally.there are two different algorithms forestimating the conditional probability tables ofthe network.search is doneusing or the tan algorithm or more sophisticated methodsbased on hill-climbingsimulated annealingtabu searchand genetic algo-rithms.optionallysearch speed can be improved using ad trees is also an algorithm that uses conditional independence tests to learn thestructure ofthe networkalternativelythe network structure can be loaded froman xml markup language file.more details on the implementationofbayesian networks in weka can be found in bouckaert can observe the network structure by right-clicking the history item andselecting visualize graph.figure shows the graph for the nominalversion ofthe weather datawhich in fact corresponds to the na ve bayes resulttable minimal optimization algorithm for supportvector regressionvotedperceptronvoted perceptron algorithmwinnowmistake-driven perceptron with multiplicative nearest-neighbor instance-based learneribkk-nearest-neighbor classifierkstarnearest neighbor with generalized distance functionlbrlazy bayesian rules classifierlwlgeneral algorithm for locally weighted learningmisc.hyperpipesextremely simple fast learner based onhypervolumes in instance spacevfivoting feature intervals method simple and am page with all probabilities conditioned on the class value.this is because the searchalgorithm defaults to with the maximum number ofparents ofa node setto one.reconfiguring this to three by clicking on the configuration panelyields the more interesting network in figure on a nodeshows its probability distribution figure is obtained by clicking onthe windynode in figure tree classifiers in table we have already seen how to use see the optionsclick the line beside thechoosebutton in figure to bring up the object editor in figure can build a binary tree instead ofone with multiway branches.you can exploreracbfigure a bayesian network for the weather data versionadefault outputb a version with the maximum number ofparents set to the searchalgorithmand probability distribution table for the windynode in am page confidence threshold for pruning the minimum numberofinstances permissible at a leafdefault ofstandard pruningyou can choose reduced-error pruning numfoldsparameterdefault determines the size ofthe pruning setthe data is divided equallyinto that number ofparts and the last one used for pruning.when visualizingthe tree it is nice to be able to consult the original data pointswhich you can do ifsaveinstancedatahas been turned on is offor falsebydefault to reduce memory requirements.you can suppress subtree raisingyielding a more efficient algorithmforce the algorithm to use the unprunedtree instead ofthe pruned oneor use laplace smoothing for predicted proba-bilities shows many other decision tree the basic algo-rithm explained in chapter for use with the boost-ing methods described laterbuilds one-level binary decision trees for datasetswith a categorical or numeric classdealing with missing values by treating themas a separate value and extending a third branch from the stump.trees built byrandomtreechooses a test based on a given number ofrandom features at eachnodeperforming no pruning.randomforestconstructs random forests bybagging ensembles ofrandom trees a decision or regression tree using information gainvariancereduction and prunes it using reduced-error pruning for speedit only sorts values for numeric attributes oncefigure the parameters for am page deals with missing values by splitting instances intopiecesas does.you can set the minimum number ofinstances per leafmaximum tree depth when boosting treesminimum proportion oftraining set variance for a split classes onlyand number offolds forpruning.nbtreeis a hybrid between decision trees and na ve bayes.it creates treeswhose leaves are na ve bayes classifiers for the instances that reach the leaf.whenconstructing the treecross-validation is used to decide whether a node shouldbe split further or a na ve bayes model should be used instead the model tree learner described in section logisticmodel trees deal with binary and multiclasstarget variablesnumeric and nominal attributesand missing values.whenfitting the logistic regression functions at a nodeit uses cross-validation todetermine how many iterations to run just once and employs the same numberthroughout the tree instead ofcross-validating at every node.this heuristicwhich you can switch off improves the run time considerablywith little effecton accuracy.alternativelyyou can set the number ofboosting iterations to beused throughout the tree.normallyit is the misclassification error that cross-validation minimizesbut the root mean-squared error ofthe probabilities canbe chosen instead.the splitting criterion can be based on s informationgain default or on the logitboost residualsstriving to improve the purityofthe residuals.adtreebuilds an alternating decision tree using boosting and is optimized for two-class problems.the number ofboosting iterations is a parameter that can be tuned to suit the dataset and the desiredcomplexity accuracy tradeoff.each iteration adds three nodes to the tree node and two prediction nodes unless nodes can be merged.the defaultsearch method is exhaustive search all pathsthe others are heuristics andare much faster.you can determine whether to save instance data for visualization.rulestable shows many methods for generating rules.decisiontablebuilds adecision table majority classifier evaluates featuresubsets using best-first search and can use cross-validation for evaluationkohavi option uses the nearest-neighbor method to determine theclass for each instance that is not covered by a decision table entryinstead ofthe table s global majoritybased on the same set offeatures.oneris the with one parameterthe minimum bucket size for dis-cretization.conjunctiverulelearns a single rule that predicts either a numericor a nominal class value.uncovered test instances are assigned the default am page distribution ofthe uncovered training instances.the information gainnominal class or variance reduction class ofeach antecedent is com-putedand rules are pruned using reduced-error pruning.zeroris even simplerit predicts the test data s majority class or average value the elementary covering algorithm for rules rules from partial decision trees the tree using s heuristics with the same user-defined parameters regression rules from model trees built using rules with exceptions by generating the default ruleusing incremental reduced-errorpruning to find exceptions with the smallest error ratefinding the best excep-tions for each exceptionand iterating.jripimplements ripper heuristicglobal optimization ofthe rule set a nearest-neighbormethod for generating rules using nonnested generalized exemplars functions category oftable includes an assorted group ofclassifiersthat can be written down as mathematical equations in a reasonably naturalway.other methodssuch as decision trees and rulescannot are excep-tionsna ve bayes has a simple mathematical formulation.three ofthemimplement linear regression a linearregression model based on a single attribute it chooses the one that yields the smallest squared error.missing values and nonnumeric attributes are notallowed.linearregressionperforms standard least-squares linear regression andcan optionally perform attribute selectioneither by greedily using backwardelimination or by building a full model from all attributes anddropping terms one by one in decreasing order oftheir standardized coefficientsuntil a stopping criteria is reached method was described in a slightly dif-ferent context in section under pruning the treepage methodsuse a version ofthe aic termination criterion ofsection has two further refinementsa mechanism for detectingcollinear attributes can be turned off and a ridgeparameter that stabi-lizes degenerate cases and can reduce overfitting by penalizing large coefficients.technicallylinearregressionimplements ridge regressionwhich is described instandard statistics texts.leastmedsqis a robust linear regression method that minimizes the medianrather than the mean ofthe squares ofdivergences from the regression linesection and leroy repeatedly applies standard am page regression to subsamples ofthe data and outputs the solution that has the small-est median-squared error.smoimplements the sequential minimal optimization algorithm for train-ing a support vector classifier polynomial or gaussiankernels et values are replaced globallynominal attributes are transformed into binary onesand attributes are nor-malized by default note that the coefficients in the output are based on thenormalized data.normalization can be turned offor the input can be stan-dardized to zero mean and unit variance.pairwise classification is used for multiclass problems.logistic regression models can be fitted to the supportvector machine output to obtain probability estimates.in the multiclass case thepredicted probabilities will be coupled pairwise and tibshirani working with sparse instancesturn normalization offfor faster opera-tion.smoregimplements the sequential minimal optimization algorithm forregression problems and sch the voted perceptron algorithm modifies the basic perceptronto use multiplicative updates.the implementation allows for a second multi-plierb different from to be used in place ofthe divisions in figure also provides the balanced version ofthe algorithm.paceregressionbuilds linear regression models using the new technique ofpace regression and witten there are many attributespaceregression is particularly good at determining which ones to discard indeedunder certain regularity conditions it is provably optimal as the number ofattributes tends to infinity.simplelogisticbuilds logistic regression models them using logitboost with simple regression functions as base learnersand determining how many iterations to perform using cross-validation which supports automatic attribute selection et alternative implementation for building and using a multinomial logisticregression model with a ridge estimator to gaurd against overfitting by penal-izing large coefficientsbased on work by le cessie and van houwelingen a gaussian radial basis function network the centers and widths ofhidden units using k-meansand combining the outputs obtained from the hidden layer using logistic regres-sion ifthe class is nominal and linear regression ifit is numeric.the activationsofthe basis functions are normalized to sum to one before they are fed into thelinear models.you can specify kthe number ofclustersthe maximum numberoflogistic regression iterations for nominal-class problemsthe minimum stan-dard deviation for the clustersand the ridge value for regression.ifthe class isnominalk-means is applied separately to each class to derive kclusters for am page networksmultilayerperceptronis a neural network that trains using backpropagationsection listed under functions in table differsfrom the other schemes because it has its own user interface.ifyou load up thenumeric version ofthe weather datainvoke multilayerperceptronset guitotruein its object editorand run the network by clicking starton the classifypanelthe diagram in figure appears in a separate window.this networkhas three layersan input layer on the left with one rectangular box for eachabfigure weka s neural-network graphical user am page attribute greena hidden layer next to it to which all the inputnodes are connectedand an output layer at the right labels at thefar right show the classes that the output nodes represent.output nodes fornumeric classes are automatically converted to unthresholded linear units.before clicking startto run the networkyou can alter its structure by addingnodes and connections.nodes can be selected or deselected.all six nodes in thehidden and output layers in figure are deselectedindicated by the graycolor oftheir center.to select a nodesimply click on it.this changes the colorofits center from gray to bright yellow.to deselect a noderight-click in anempty space.to add a nodeensure that none is selected and left-click anywherein the panelthe new node will be selected automatically.in figure node has been added at the lower center.to connect two nodesselect thestart node and then click on the end one.ifseveral start nodes are selectedtheyare all connected to the end node.ifyou click in empty space insteada newnode is created as the end node.notice that connections are directionalalthough the directions are not shown.the start nodes remain selectedthusyou can add an entire hidden layer with just a few clicksas shown in remove a nodeensure that no nodes are selected and right-clickitthis also removes all connections to it.to remove a single connectionselectone node and right-click the node at the other end.as well as configuring the structure ofthe networkyou can control the learn-ing rateits momentum the number ofpasses it will takethrough the datacalled epochs.the network begins to train when you clickstartand a running indication ofthe epoch and the error for that epoch isshown at the lower left ofthe panel in figure that the error is basedon a network that changes as the value is computed.for numeric classes theerror value depends on whether the class is normalized.the network stops whenthe specified number ofepochs is reachedat which point you can accept theresult or increase the desired number ofepochs and press startagain to con-tinue training.multilayerperceptronneed not be run through the graphical interface.severalparameters can be set from the object editor to control its operation.ifyou areusing the graphical interface they govern the initial network structurewhichyou can override interactively.with autobuildsethidden layers are added andconnected up.the default is to have the one hidden layer shown in without autobuildthis would not appear and there would be noconnections.the hiddenlayersparameter defines the hidden layers present andhow many nodes each one contains.figure is generated by a value hidden layer with four nodesand although figure was createdby adding nodes interactivelyit could have been generated by setting hidden-layersto hidden layer with four nodes and another with five.the valueis a comma-separated list no hidden am page are predefined values that can be used instead ofintegersiis the numberofattributesothe number ofclass valuesathe average ofthe twoand ttheirsum.the defaultawas used to generate figure parameters learningrateand momentumset values for these variableswhich can be overridden in the graphical interface.a decayparameter causesthe learning rate to decrease with timeit divides the starting value by the epochnumber to obtain the current rate.this sometimes improves performance andmay stop the network from diverging.the resetparameter automatically resetsthe network with a lower learning rate and begins training again ifit is diverg-ing from the answer option is only available ifthe graphical user interfaceis notused.the trainingtimeparameter sets the number oftraining epochs.alterna-tivelya percentage ofthe data can be set aside for validation validation-setsizethen training continues until performance on the validation set startsto deteriorate consistently or until the specified number ofepochs is reached.ifthe percentage is set to zerono validation set is used.the validationthresh-oldparameter determines how many consecutive times the validation set errorcan deteriorate before training is stopped.the nominaltobinaryfilterfilter is specified by default in the multilayerper-ceptronobject editorturning it offmay improve performance on data in whichthe nominal attributes are really ordinal.the attributes can be normalized a numeric class can be normalized too normal-izenumericclassboth may improve performance.lazy classifierslazy learners store the training instances and do no real work until classifica-tion a basic instance-based learner which finds thetraining instance closest in euclidean distance to the given test instance and pre-dicts the same class as this training instance.ifseveral instances qualify as theclosestthe first one found is used.ibkis a k-nearest-neighbor classifier that usesthe same distance metric.the number ofnearest neighbors canbe specified explicitly in the object editor or determined automatically usingleave-one-out cross-validationsubject to an upper limit given by the specifiedvalue.predictions from more than one neighbor can be weighted according totheir distance from the test instanceand two different formulas are imple-mented for converting the distance into a weight.the number oftraininginstances kept by the classifier can be restricted by setting the window sizeoption.as new training instances are addedthe oldest ones are removed tomaintain the number oftraining instances at this size.kstaris a nearest-neighbor method with a generalized distance function based on transforma-tions am page lbrfor lazy bayesian rules is a bayesian classifier that defers all process-ing to classification time.for each test instance it selects a set ofattributes forwhich the independence assumption should not be madethe others are treatedas independent ofeach other given the class and the selected set ofattributes.it works well for small test sets and webb a general algorithm for locally weighted learning.it assigns weightsusing an instance-based method and builds a classifier from the weightedinstances.the classifier is selected in lwl s object editora good choice is na vebayes for classification problems and linear regression for regression problemssection can set the number ofneighbors usedwhichdetermines the kernel bandwidthand the kernel shape to use for weighting linearinverseor gaussian.attribute normalization is turned on by default.miscellaneous classifiersthe misc.category includes two simple classifiers that were mentioned at theend ofsection discrete classification problemsrecords the range ofvalues observed in the training data for each attribute andcategory and works out which ranges contain the attribute values ofa test in-stancechoosing the category with the largest number ofcorrect ranges.vfivoting feature intervalsconstructs intervals around each class by discretizingnumeric attributes and using point intervals for nominal onesrecords classcounts for each interval on each attributeand classifies test instances by votingdemiroz and guvenir simple attribute weighting scheme assignshigher weight to more confident intervalswhere confidence is a function ofentropy.vfiis faster than na ve bayes but slower than hyperpipes.neithermethod can handle missing algorithmsmetalearning algorithmslisted in table classifiers and turn them intomore powerful learners.one parameter specifies the base classifierothersspecify the number ofiterations for schemes such as bagging and boosting andan initial seed for the random number generator.we already met filteredclas-sifierin section runs a classifier on data that has been passed through afilterwhich is a parameter.the filter s own parameters are based exclusively onthe training datawhich is the appropriate way to apply a supervised filter totest data.bagging and randomizationbaggingbags a classifier to reduce variance imple-mentation works for both classification and regressiondepending on the am page learner.in the case ofclassificationpredictions are generated by averaging prob-ability estimatesnot by voting.one parameter is the size ofthe bags as a per-centage ofthe training set.another is whether to calculate the out-of-bag errorwhich gives the average error ofthe ensemble members even simplerit builds an ensemble ofbase classifiersand averages their predictions.each one is based on the same data but uses adifferent random number seed only makes senseifthe base classifier is randomizedotherwiseall classifiers would be the algorithms in using the methodadditiveregressionenhance the performance of a regression method byiteratively fitting the residualsattributeselectedclassifierreduce dimensionality of data by attribute selectionbaggingbag a classifier works for regression tooclassificationviaregressionperform classification using a regression methodcostsensitiveclassifiermake its base classifier cost sensitivecvparameterselectionperform parameter selection by cross-validationdecoratebuild ensembles of classifiers by using speciallyconstructed artificial training examplesfilteredclassifierrun a classifier on filtered datagradingmetalearners whose inputs are base-level predictions thathave been marked as correct or incorrectlogitboostperform additive logistic regressionmetacostmake a classifier cost-sensitivemultiboostabcombine boosting and bagging using the multiboosting methodmulticlassclassifieruse a two-class classifier for multiclass datasetsmultischemeuse cross-validation to select a classifier from severalcandidatesordinalclassclassifierapply standard classification algorithms to problems with an ordinal class valueracedincrementallogitboostbatch-based incremental learning by racing logit-boosted committeesrandomcommitteebuild an ensemble of randomizable base classifiersregressionbydiscretizationdiscretize the class attribute and employ a classifierstackingcombine several classifiers using the stacking methodstackingcmore efficient version of stackingthresholdselectoroptimize the f-measure for a probabilistic classifiervotecombine classifiers using average of probability estimates or numeric am page the algorithm described in section can be accelerated by specifying a threshold for weight ifthe base classifier cannot handle weighted instancesyou can also force resampling anyway.multiboostabcombines boosting witha variant ofbagging to prevent overfitting boosting only applies to nominal classesadditiveregressionen-hances the performance ofa regression learner aretwo parametersshrinkagewhich governs the learning rateand the maximumnumber ofmodels to generate.ifthe latter is infinitework continues until theerror stops decreasing.decoratebuilds ensembles ofdiverse classifiers by using specially constructedartificial training examples.this technique is claimed to consistently improveon the base classifier and on the bagging and random forest metalearnersmelville and outperforms boosting on small training setsand rivals it on larger ones.one parameter is the number ofartificial examplesto use as a proportion ofthe training data.another is the desired number ofclassifiers in the ensemblealthough execution may terminate prematurelybecause the number ofiterations can also be capped.larger ensembles usuallyproduce more accurate models but have greater training time and model com-plexity.logitboostperforms additive logistic regression can be accelerated by specifying a threshold for weight pruning.the appropriate number ofiterations can be determined using internal cross-validationthere is a shrinkage parameter that can be tuned to preventoverfittingand you can choose resampling instead ofreweighting.racedincre-mentallogitboostlearns by racing logitboosted committeesand operates incre-mentally by processing the data in batches it useful forlarge datasets et committee member is learned from adifferent batch.the batch size starts at a given minimum and repeatedly doublesuntil it reaches a preset maximum.resampling is used ifthe base classifiercannot handle weighted instances can also force resampling anyway.log-likelihood pruning can be used within each committeethis discards new committee members ifthey decrease the log-likelihood based on the validationdata.you can determine how many instances to hold out for validation.the val-idation data is also used to determine which committee to retain when random forest scheme was mentioned on page is really a metalearnerbut wekaincludes it among the decision tree methods because it is hardwired to a particular am page combining classifiersvoteprovides a baseline method for combining classifiers by averaging theirprobability estimates or numeric predictions the best classifier from a set ofcandidates using cross-validation ofpercentage accuracy or mean-squared errorregression.the number offolds is a parameter.performance on training datacan be used instead.stackingcombines classifiers using stacking for bothclassification and regression problems.you specify the base classifiersthe meta-learnerand the number ofcross-validation folds.stackingcimplements a moreefficient variant for which the metalearner must be a numeric prediction schemeseewald gradingthe inputs to the metalearner are base-level pre-dictions that have been marked graded as correct or incorrect.for eachbase classifiera metalearner is learned that predicts when the base classifier willerr.just as stacking may be viewed as a generalization ofvotinggrading gener-alizes selection by cross-validation and f rnkranz learningthere are two metalearners for cost-sensitive learning costmatrix can be supplied as a parameter or loaded from a file in the directory setby the ondemanddirectorypropertynamed by the relation name and with theextension cost.costsensitiveclassifiereither reweights training instances accord-ing to the total cost assigned to each class learningpage orpredicts the class with the least expected misclassification cost rather than themost likely one classificationpage asingle cost-sensitive classifier from the base learner implementation uses all bagging iterations when reclassifying training datadomingos reports a marginal improvement when using only those iter-ations containing each training instance to reclassify it.you can specify eachbag s size and the number ofbagging iterations.optimizing performancethree metalearners use the wrapper technique to optimize the base classifier sperformance.attributeselectedclassifierselects attributesreducing the data sdimensionality before passing it to the classifier canchoose the attribute evaluator and search method using the select attributespanel described in section performanceby using cross-validation to select parameters.for each parameter you give astring containing its lower and upper bounds and the desired number ofincre-ments.for exampleto vary parameter to in increments number ofcross-validation folds can be am page the third metalearnerthresholdselectoroptimizes the f-measure by selecting a probability threshold on the classifier s output.performancecan be measured on the training dataon a holdout setor by cross-validation.the probabilities returned by the base learner can be rescaled into the full is useful ifthe scheme s probabilities are restricted to a narrow sub-range.the metalearner can be applied to multiclass problems by specifying theclass value for which the optimization is performed first class second class value is least value is most first class named yespositiveor classifiers for different tasksfour metalearners adapt learners designed for one kind oftask to another.clas-sificationviaregressionperforms classification using a regression method bybinarizing the class and building a regression model for each value.regression-bydiscretizationis a regression scheme that discretizes the class attribute into aspecified number ofbins using equal-width discretization and then employs aclassifier.the predictions are the weighted average ofthe mean class value foreach discretized intervalwith weights based on the predicted probabilities forthe intervals.ordinalclassclassifierapplies standard classification algorithms toordinal-class problems and hall problems with two-class classifiers using any ofthese versus all the classification using voting to error-correcting codes selected error-correcting codesrandom code vectors are known to have good error-correcting propertiesaparameter specifies the length ofthe code vector algorithmstable lists weka s clustering algorithmsthe first two and simplekmeansaredescribed in section the emimplementation you can specify how manyclusters to generate or the algorithm can decide using cross-validation inwhich case the number offolds is fixed at there are fewer than instances.you can specify the maximum number ofiterations and setthe minimum allowable standard deviation for the normal density am page simplekmeansclusters data using k-meansthe number ofclusters is specifiedby a parameter.cobwebimplements both the cobweb algorithm for nominalattributes and the classit algorithm for numeric attributes.the ordering andpriority ofthe merging and splitting operators differs between the originalcobweb and classit papers it is somewhat ambiguous.this imple-mentation always compares four different ways oftreating a new instance andchooses the bestadding it to the best hostmaking it into a new leafmergingthe two best hosts and adding it to the merged nodeand splitting the best hostand adding it to one ofthe splits.acuityand cutoffare parameters.farthestfirstimplements the farthest-first traversal algorithm ofhochbaumand shmoys by sanjoy dasgupta fastsimpleapproxi-mate clusterer modeled on k-means.makedensitybasedclustereris a meta-clusterer that wraps a clustering algorithm to make it return a probabilitydistribution and density.to each cluster it fits a discrete distribution or a symmetric normal distribution minimum standard deviation is learnersweka has three association-rule learnerslisted in table the apriori algorithm starts with a minimum support ofthe data items and decreases this in steps until there are at rules with the required minimum confidence or until the support algorithms.namefunctionemcluster using expectation maximizationcobwebimplements the cobweb and classit clustering algorithmsfarthestfirstcluster using the farthest first traversal algorithmmakedensitybasedclustererwrap a clusterer to make it return distribution and densitysimplekmeanscluster using the k-means methodtable learners.namefunctionapriorifind association rules using the apriori algorithmpredictiveapriorifind association rules sorted by predictive accuracytertiusconfirmation-guided discovery of association or classification am page reached a lower bound occurs first.these default values canbe changed. there are four alternative metrics for ranking rulesconfidencewhich is the proportion ofthe examples covered by the premise that are alsocovered by the consequent accuracyin section is deter-mined by dividing the confidence by the support coveragein section is the proportion ofadditional examples covered by both thepremise and the consequent beyond those expected ifthe premise and conse-quent were statistically independentand convictiona measure defined by brinet can also specify a significance leveland rules will be tested forsignificance at this level.predictiveaprioricombines confidence and support into a single measure ofpredictive accuracyscheffer and finds the best nassociation rules in order.internallythe algorithm successively increases the support thresholdbecausethe value ofpredictive accuracy depends on it.tertiusfinds rules according toa confirmation measure and lachiche rules with multipleconditions in the consequentlike aprioribut differing in that these conditionsare or d togethernot anded.it can be set to find rules that predict a singlecondition or a predetermined attribute rules.one parame-ter determines whether negation is allowed in the antecedentthe consequentor bothothers give the number ofrules soughtminimum degree ofconfir-mationminimum coveragemaximum proportion ofcounterinstancesandmaximum rule size.missing values can match any valuenever matchor be sig-nificant and possibly appear in selectionfigure shows that part ofweka s attribute selection panel where youspecify the attribute evaluator and search methodtable and table the choices.attribute selection is normally done by searching the space ofattribute subsetsevaluating each one is achieved by com-bining one ofthe four attribute subset evaluators in table with one oftheseven search methods in table potentially faster but less accurateapproach is to evaluate the attributes individually and sort explorerfigure selectionspecifying an evaluator and a search am page attributes that fall below a chosen cutoffpoint.this is achieved by selecting oneofthe eight single-attribute evaluators in table and using the rankingmethod in table weka interface allows both possibilities by lettingthe user choose a selection method from table and a search method fromtable an error message ifyou select an inappropriate evaluation methods for attribute selection.namefunctionattributecfssubsetevalconsider the predictive value of eachsubset evaluatorattribute individually along with the degree of redundancy among themclassifiersubsetevaluse a classifier to evaluate attribute setconsistencysubsetevalproject training set onto attribute set andmeasure consistency in class valueswrappersubsetevaluse a classifier plus cross-validationsingle-chisquaredattributeevalcompute the chi-squared statistic of eachattribute evaluatorattribute with respect to the classgainratioattributeevalevaluate attribute based on gain ratioinfogainattributeevalevaluate attribute based on information gainonerattributeevaluse oner s methodology to evaluate attributesprincipalcomponentsperform principal components analysis andtransformationrelieffattributeevalinstance-based attribute evaluatorsvmattributeevaluse a linear support vector machine todetermine the value of attributessymmetricaluncertattributeevalevaluate attribute based on symmetricuncertaintytable methods for attribute selection.namefunctionsearchbestfirstgreedy hill-climbing with backtrackingmethodexhaustivesearchsearch exhaustivelygeneticsearchsearch using a simple genetic algorithmgreedystepwisegreedy hill-climbing without backtracking optionallygenerate ranked list of attributesracesearchuse race search methodologyrandomsearchsearch randomlyranksearchsort the attributes and rank promising subsets using anattribute subset evaluatorranking methodrankerrank individual attributes subsets according to their am page nation.the status line refers you to the error log for the message the endofsection subset evaluatorssubset evaluators take a subset ofattributes and return a numeric measure thatguides the search.they are configured like any other weka object.cfssubsetevalassesses the predictive ability ofeach attribute individually and the degree ofredundancy among thempreferring sets ofattributes that are highly correlatedwith the class but have low intercorrelation option iterativelyadds attributes that have the highest correlation with the classprovided thatthe set does not already contain an attribute whose correlation with the attrib-ute in question is even higher.missingcan be treated as a separate valueor itscounts can be distributed among other values in proportion to their frequency.consistencysubsetevalevaluates attribute sets by the degree ofconsistency inclass values when the training instances are projected onto the set.the consis-tency ofany subset ofattributes can never improve on that ofthe full setsothis evaluator is usually used in conjunction with a random or exhaustive searchthat seeks the smallest subset whose consistency is the same as that ofthe fullattribute set.whereas the previously mentioned subset evaluators are filter methods ofattribute selection remainder are wrapper methods.classi-fiersubsetevaluses a classifierspecified in the object editor as a parametertoevaluate sets ofattributes on the training data or on a separate holdout set.wrappersubsetevalalso uses a classifier to evaluate attribute setsbut it employscross-validation to estimate the accuracy ofthe learning scheme for each set.single-attribute evaluatorssingle-attribute evaluators are used with the rankersearch method to generatea ranked list from which rankerdiscards a given number in the nextsubsection.they can also be used in the ranksearchmethod.relieffattribute-evalis instance-basedit samples instances randomly and checks neighboringinstances ofthe same and different classes operates on discreteand continuous class data.parameters specify the number ofinstances tosamplethe number ofneighbors to checkwhether to weight neighbors by dis-tanceand an exponential function that governs how rapidly weights decay withdistance.infogainattributeevalevaluates attributes by measuring their informationgain with respect to the class.it discretizes numeric attributes first using themdl-based discretization method can be set to binarize them instead.thismethodalong with the next threecan treat missingas a separate value or am page tribute the counts among other values in proportion to their frequency.chisquaredattributeevalevaluates attributes by computing the chi-squared sta-tistic with respect to the class.gainratioattributeevalevaluates attributes bymeasuring their gain ratio with respect to the class.symmetricaluncertattribu-teevalevaluates an attribute aby measuring its symmetric uncertainty withrespect to the class csection the simple accuracy measure adopted by the onerclassifier.it can use the training data for evaluationas onerdoesor it can applyinternal cross-validationthe number offolds is a parameter.it adopts oner ssimple discretization methodthe minimum bucket size is a parameter.svmattributeevalevaluates attributes using recursive feature eliminationwith a linear support vector machine areselected one by one based on the size oftheir coefficientsrelearning after eachone.to speed things up a fixed number proportion ofattributes can beremoved at each stage.indeeda proportion can be used until a certain num-ber ofattributes remainthereupon switching to the fixed-number method rapidly eliminating many attributes and then considering each one more intensively.various parameters are passed to the support vector machinecom-plexityepsilontoleranceand the filtering method used.unlike other single-attribute evaluatorsprincipalcomponentstransforms the set ofattributes.the new attributes are ranked in order oftheir eigen-values subset is selected by choosing suf-ficient eigenvectors to account for a given proportion ofthe variance bydefault.you can also use it to transform the reduced data back to the originalspace.search methodssearch methods traverse the attribute space to find a good subset.quality ismeasured by the chosen attribute subset evaluator.each search method can be configured with weka s object editor.bestfirstperforms greedy hill climb-ing with backtrackingyou can specify how many consecutive nonimprov-ing nodes must be encountered before the system backtracks.it can searchforward from the empty set ofattributesbackward from the full setor start atan intermediate point by a list ofattribute indices and search in bothdirections by considering all possible single-attribute additions and deletions.subsets that have been evaluated are cached for efficiencythe cache size is aparameter.greedystepwisesearches greedily through the space ofattribute subsets.likebestfirstit may progress forward from the empty set or backward from the fullset.unlike bestfirstit does not backtrack but terminates as soon as adding am page deleting the best remaining attribute decreases the evaluation metric.in analternative modeit ranks attributes by traversing the space from empty to fullor vice versa and recording the order in which attributes are selected.you canspecify the number ofattributes to retain or set a threshold below which attrib-utes are discarded.racesearchused with classifiersubsetevalcalculates the cross-validationerror ofcompeting attribute subsets using race search fourdifferent searches described on page are implementedforward selectionbackward eliminationschemata searchand rank racing.in the last case a sep-arate attribute evaluator can also be specified is used to generate aninitial ranking.using forward selectionit is also possible to generate a rankedlist ofattributes by continuing racing until all attributes have been selectedtheranking is set to the order in which they are added.as with greedystepwiseyoucan specify the number ofattributes to retain or set a threshold below whichattributes are discarded.geneticsearchuses a simple genetic algorithm population sizenumber ofgenerationsand probabilities ofcrossoverand mutation.you can specify a list ofattribute indices as the starting pointwhich becomes a member ofthe initial population.progress reports can be gen-erated every so many generations.randomsearchrandomly searches the spaceofattribute subsets.ifan initial set is suppliedit searches for subsets thatimprove on equal the starting point and have fewer the same numberof attributes.otherwiseit starts from a random point and reports the bestsubset found.placing all attributes in the initial set yields liu and setiono probabilistic feature selection algorithm.you can determine the fractionofthe search space to explore.exhaustivesearchsearches through the space ofattribute subsetsstarting from the empty setand reports the best subset found.ifan initial set is suppliedit searches backward from this starting point andreports the smallest subset with a better equal evaluation.ranksearchsorts attributes using a single-attribute evaluator and then rankspromising subsets using an attribute subset evaluator.the latter is specified inthe top box offigure usualthe attribute evaluator is specified as aproperty in ranksearch s object editor.it starts by sorting the attributes withthe single-attribute evaluator and then evaluates subsets ofincreasing size usingthe subset evaluator the best attributethe best attribute plus the next best oneand so on reporting the best subset.this procedure has low computationalcomplexitythe number oftimes both evaluators are called is linear in thenumber ofattributes.using a simple single-attribute evaluator selection procedure is very fast.finally we describe rankerwhich as noted earlier is not a search method for attribute subsets but a ranking scheme for individual attributes.it sortsattributes by their individual evaluations and must be used in conjunction am page with one ofthe single-attribute evaluators in the lower part oftable notan attribute subset evaluator.rankernot only ranks attributes but also performsattribute selection by removing the lower-ranking ones.you can set a cutoffthreshold below which attributes are discardedor specify how many attributesto retain.you can specify certain attributes that must be retained regardless oftheir am page am page with the knowledge flow interfaceusers select weka components from a toolbarplace them on a layout canvasand connect them into a directed graph thatprocesses and analyzes data.it provides an alternative to the explorer for thosewho like thinking in terms ofhow data flows through the system.it also allowsthe design and execution ofconfigurations for streamed data processingwhichthe explorer cannot do.you invoke the knowledge flow interface by selectingknowledgeflowfrom the choices at the bottom ofthe panel shown in startedhere is a step-by-step example that loads an arff file and performs a cross-validation using describe how to build up the final configuration shownin figure create a source ofdata by clicking on the datasourcestabrightmost entry in the bar at the top and selecting arffloaderfrom knowledge flow am page toolbar.the mouse cursor changes to crosshairs to signal that you should nextplace the component.do this by clicking anywhere on the canvaswhereupona copy ofthe arff loader icon appears there.to connect it to an arff fileright-click it to bring up the pop-up menu shown in figure con-figureto get the file browser in figure which you select the desiredarff file.the file formatpull-down menu allows you to choose a differenttype ofdata source for examplespreadsheet files.now we specify which attribute is the class using a classassignerobject.thisis on the evaluationpanelso click the evaluationtabselect the classassignerand place it on the canvas.to connect the data source to the class assignerright-click the data source icon and select datasetfrom the menuas shown in rubber-band line appears.move the mouse over the class assignercomponent and left-click.a red line labeled datasetappearsjoining the twocomponents.having connected the class assignerchoose the class by right-clicking itselecting configureand entering the location ofthe class attribute.we will perform cross-validation on the the data flow modelwe first connect the crossvalidationfoldmakerto create the folds on which theclassifier will runand then pass its output to an object representing on the evaluationpanel.select itplace it on the canvasand connect it to the class assigner by right-clicking the latter and knowledge flow interfacefigure knowledge flow am page datasetfrom the menu is similar to that in figure the classifierspanel and place a on the canvas.thereare so many different classifiers that you have to scroll along the toolbar to findit.connect the cross-validation fold maker in the usual waybut makethe connection twiceby first choosing trainingsetand then choosing testsetfromthe pop-up menu for the cross-validation fold maker.the next step is to selecta classifierperformanceevaluatorfrom the evaluationpanel and connect by selecting the batchclassifierentry from the pop-up menu for the visualizationtoolbar we place a textviewercomponent on the canvas.connect the classifier performance evaluator to it by selecting the textentryfrom the pop-up menu for the performance evaluator.at this stage the configuration is as shown in figure except that there isas yet no graph viewer.start the flow ofexecution by selecting start loadingfromthe pop-up menu for the arff loadershown in figure a smalldataset things happen quicklybut ifthe input were large you would see thatsome ofthe icons are animated for s tree would appear to growand the performance evaluator s checkmarks would blink.progress informationappears in the status bar at the bottom ofthe interface.choosing show resultsfrom the text viewer s pop-up menu brings the results ofcross-validation up ina separate windowin the same form as for the explorer.to complete the exampleadd a graphviewerand connect it to s graphoutput to see a graphical representation ofthe trees produced for each fold ofthe cross-validation.once you have redone the cross-validation with this extracomponent in placeselecting show resultsfrom its pop-up menu produces a data sourcea the right-click menu and the file browserobtained from the configuremenu am page list oftreesone for each cross-validation fold.by creating cross-validation foldsand passing them to the classifierthe knowledge flow model provides a way tohook into the results for each fold.the explorer cannot do thisit treats cross-validation as an evaluation method that is applied to the output ofa knowledge flow componentsmost ofthe knowledge flow components will be familiar from the explorer.the classifierspanel contains all ofweka s classifiersthe filterspanel containsthe filtersand the clustererspanel holds the clusterers.possible data sources arearff filescsv files exported from spreadsheetsthe file formatand aserialized instance loader for data files that have been saved as an instance ofajava object.there are data sinks and sources for the file formats supported bythe explorer.there is also a data sink and a data source that can connect to adatabase.the components for visualization and evaluationlisted in table yet been encountered.under visualizationthe datavisualizerpops up apanel for visualizing data in a two-dimensional scatter plot as in figure which you can select the attributes you would like to see.scatterplotmatrixpops up a matrix oftwo-dimensional scatter plots for every pair knowledge flow interfacetable and evaluation components.namefunctionvisualizationdatavisualizervisualize data in a scatter plotscatterplotmatrixmatrix of scatter plotsattributesummarizerset of histograms one for each attributemodelperformancechartdraw roc and other threshold curvestextviewervisualize data or models as textgraphviewervisualize tree-based modelsstripchartdisplay a scrolling plot of dataevaluationtrainingsetmakermake a dataset into a training settestsetmakermake a dataset into a test setcrossvalidationfoldmakersplit a dataset into foldstraintestsplitmakersplit a dataset into training and test setsclassassignerassign one of the attributes to be the classclassvaluepickerchoose a value for the positiveclassclassifierperformanceevaluatorcollect evaluation statistics for batch evaluationincrementalclassifierevaluatorcollect evaluation statistics for incrementalevaluationclustererperformanceevaluatorcollect evaluation statistics for clustererspredictionappenderappend a classifier s predictions to a am page shown in figure a matrix ofhistogramsone for each attributelike that in the lower right-hand corner offigure roc curves and other threshold curves.graphviewerpops up a panel for visualizing tree-based modelsas in beforeyou can zoompanand visualize the instance data at a nodeifit has been saved by the learning algorithm.stripchartis a new visualization component designed for use with in-cremental learning.in conjunction with the incrementalclassifierevaluatordescribed in the next paragraph it displays a learning curve that plots accu-racy both the percentage accuracy and the root mean-squared probabilityerror against time.it shows a fixed-size time window that scrolls horizontallyto reveal the latest results.the evaluationpanel has the components listed in the lower part trainingsetmakerand testsetmakermake a dataset into the corre-sponding kind ofset.the crossvalidationfoldmakerconstructs cross-validationfolds from a datasetthe traintestsplitmakersplits it into training and test setsby holding part ofthe data out for the test set.the classassignerallows you todecide which attribute is the class.with classvaluepickeryou choose a valuethat is treated as the positiveclass when generating roc and other thresholdcurves.the classifierperformanceevaluatorcollects evaluation statisticsit cansend the textual evaluation to a text viewer and the threshold curves to a per-formance chart.the incrementalclassifierevaluatorperforms the same functionfor incremental classifiersit computes running squared errors and so on.thereis also a clustererperformanceevaluatorwhich is similar to the classifierperfor-manceevaluator.the predictionappendertakes a classifier and a dataset andappends the classifier s predictions to the and connecting the componentsyou establish the knowledge flow by configuring the individual components andconnecting them up.figure shows typical operations that are available byright-clicking the various component types.these menus have up to three sec-tionseditconnectionsand actions.the editoperations delete componentsand open up their configuration panel.classifiers and filters are configured justas in the explorer.data sources are configured by opening a file we saw pre-viouslyand evaluation components are configured by setting parameters suchas the number offolds for cross-validation.the actionsoperations are specificto that type ofcomponentsuch as starting to load data from a data source oropening a window to show the results ofvisualization.the connectionsopera-tions are used to connect components together by selecting the type ofcon-nection from the source component and then clicking on the target and connecting the am page all targets are suitableapplicable ones are highlighted.items on the connectionsmenu are disabled out until the component receives other connectionsthat render them applicable.there are two kinds ofconnection from data sourcesdatasetconnectionsand instanceconnections.the former are for batch operations such as classi-fiers like latter are for stream operations such as naivebayesupdateable.a data source component cannot provide both types ofconnectiononce oneis selectedthe other is disabled.when a datasetconnection is made to a batchclassifierthe classifier needs to know whether it is intended to serve as a train-ing set or a test set.to do thisyou first make the data source into a test or train-ing set using the testsetmakeror trainingsetmakercomponents from theevaluationpanel.on the other handan instanceconnection to an incrementalclassifier is made directlythere is no distinction between training and testingbecause the instances that flow update the classifier incrementally.in this casea prediction is made for each incoming instance and incorporated into the testresultsthen the classifier is trained on that instance.ifyou make an instanceconnection to a batch classifier it will be used as a test instance because train-ing cannot possibly be incremental whereas testing always can be.converselyit is quite possible to test an incremental classifier in batch mode using a datasetconnection.connections from a filter component are enabled when it receives input froma data sourcewhereupon follow-on datasetor instanceconnections can bemade.instanceconnections cannot be made to supervised filters or to knowledge flow interfacedata sourcedata sinkfilterclassifiervisualizationevaluationcrossvalidationfoldmakerclassifierperformance-evaluatorfigure on the knowledge flow am page pervised filters that cannot handle data incrementally as discretize.toget a test or training set out ofa filteryou need to put the appropriate kind in.the classifier menu has two types ofconnection.the first typenamelygraphand textconnectionsprovides graphical and textual representations ofthe clas-sifier s learned state and is only activated when it receives a training set input.the other typenamelybatchclassifierand incrementalclassifierconnectionsmakes data available to a performance evaluator and is only activated when atest set input is presenttoo.which one is activated depends on the type oftheclassifier.evaluation components are a mixed bag.trainingsetmakerand testsetmakerturn a dataset into a training or test set.crossvalidationfoldmakerturns adataset into botha training set and a test set.classifierperformanceevaluatorused in the example ofsection generates textual and graphical output forvisualization components.other evaluation components operate like filtersthey enable follow-on datasetinstancetraining setor test setconnectionsdepending on the input a class to a dataset.visualization components do not have connectionsalthough some haveactions such as show resultsand clear learningin most respects the knowledge flow interface is functionally similar to theexploreryou can do similar things with both.it does provide some additionalflexibility for exampleyou can see the tree that for each cross-validation fold.but its real strength is the potential for incremental operation.weka has several classifiers that can handle data incrementallyaodeaversion ofna ve bayes instance-basedlearners metalearner racedincrementallogitboostoperates incrementally filters that work instance by instance areincrementaladdaddexpressioncopyfirstordermakeindicatormerge-twovaluesnonsparsetosparsenumerictobinarynumerictransformobfuscateremoveremovetyperemovewithvaluessparsetononsparseand swapvalues.ifall components connected up in the knowledge flow interface operateincrementallyso does the resulting learning system.it does not read in thedataset before learning startsas the explorer does.insteadthe data source com-ponent reads the input instance by instance and passes it through the knowl-edge flow chain.figure shows a configuration that works incrementally.an instanceconnection is made from the loader to the updatable na ve bayes classifier.the classifier s text output is taken to a viewer that gives a textual am page knowledge flow interfaceabfigure knowledge flow that operates incrementallya the configuration and the stripchart output.ofthe model.alsoan incrementalclassifierconnection is made to the corresponding performance evaluator.this produces an output oftype chartwhich is piped to a strip chart visualization component to generate a scrollingdata plot.figure shows the strip chart output.it plots both the accuracy andthe root mean-squared probability error against time.as time passesthe wholeplot the axes moves leftward to make room for new data at the am page when the vertical axis representing time can move left no fartherit stops andthe time origin starts to increase from to keep pace with the data coming inat the right.thus when the chart is full it shows a window ofthe most recenttime units.the strip chart can be configured to alter the number ofinstancesshown on the x axis.this particular knowledge flow configuration can process input files ofanysizeeven ones that do not fit into the computer s main memory.howeverit alldepends on how the classifier operates internally.for examplealthough theyare incrementalmany instance-based learners store the entire dataset am page am page the explorer and knowledge flow environments help you determine how wellmachine learning schemes perform on given datasets.but serious investigativework involves substantial experiments typically running several learningschemes on different datasetsoften with various parameter settings and theseinterfaces are not really suitable for this.the experimenter enables you to setup large-scale experimentsstart them runningleave themand come backwhen they have finished and analyze the performance statistics that have beencollected.they automate the experimental process.the statistics can be storedin arff formatand can themselves be the subject offurther data mining.youinvoke this interface by selecting experimenterfrom the choices at the bottomofthe panel in figure the knowledge flow transcends limitations ofspace by allowingmachine learning runs that do not load in the whole dataset at oncethe exper-imenter transcends limitations oftime.it contains facilities for advanced wekausers to distribute the computing load across multiple machines using java rmi.you can set up big experiments and just leave them to am page startedas an examplewe will compare the decision tree method with the baselinemethods onerand zeroron the iris dataset.the experimenter has three panelssetuprunand analyze.figure shows the firstyou select the othersfrom the tabs at the top.herethe experiment has already been set up.to dothisfirst click newtoward the right at the top to start a new experiment two buttons in that row save an experiment and open a previously savedone.thenon the line belowselect the destination for the results in this casethe file and choose csv file.underneathselect the datasets wehave only onethe iris data.to the right ofthe datasetsselect the algorithms tobe tested we have three.click add newto get a standard weka object editorfrom which you can choose and configure a classifier.repeat this operation toadd the three classifiers.now the experiment is ready.the other settings shownin figure are all default values.ifyou want to reconfigure a classifier thatis already in the listyou can use the edit selectedbutton.you can also save theoptions for a particular classifier in xml format for later experimenterafigure experimenta setting it upb the results fileand a spreadsheetwith the am page running an experimentto run the experimentclick the runtabwhich brings up a panel that containsa startbutton little elseclick it.a briefreport is displayed when the oper-ation is finished.the file the results.the first two linesare shown in figure are in csv format and can be read directlyinto a spreadsheetthe first part ofwhich appears in figure rowrepresents fold ofa cross-validation the foldcolumn.the cross-validation is run times runcolumn for each classifier schemecolumn.thus the file contains rows for each classifierwhich makes in all the header row.each row contains plenty ofinformation fact including the options supplied to the machine of leaves of the tree am page schemethe number oftraining and test instancesthe number percent-age ofcorrectincorrectand unclassified instancesthe mean absolute errorroot mean-squared errorand many more.there is a great deal ofinformation in the spreadsheetbut it is hard to digest.in particularit is not easy to answer the question posed previouslyhow compare with the baseline methods onerand zeroron this dataset? forthat we need the analyzepanel.analyzing the resultsthe reason that we generated the output in csv format was to show the spread-sheet in figure experimenter normally produces its output in arffformat.you can also leave the file name blankin which case the experimenterstores the results in a temporary file.the analyzepanel is shown in figure analyze the experiment that hasjust been performedclick the experimentbutton at the right near the topotherwisesupply a file that contains the results ofanother experiment.thenclick perform testnear the bottom on the left.the result ofa statistical experimenterfigure test results for the experiment in figure am page cance test ofthe performance ofthe first learning scheme that oftheother two zeror will be displayed in the large panel on the right.we are comparing the percent correct statisticthis is selected by default asthe comparison field shown toward the left offigure three methodsare displayed horizontallynumbered the heading ofa littletable.the labels for the columns are repeated at the bottom rules.zeror in case there is insufficient space for them in theheading.the inscrutable integers beside the scheme names identify whichversion ofthe scheme is being used.they are present by default to avoid con-fusion among results generated using different versions ofthe algorithms.thevalue in parentheses at the beginning ofthe irisrow the number ofexperimental times cross-validation.the percentage correct for the three schemes is shown in figure method for method for method symbolplaced beside a result indicates that it is statistically better worse baseline scheme in this case at the specified significance level corrected resampled t-test from section is used.heremethod is significantly worse than method its success rate is followedby an asterisk.at the bottom ofcolumns and are counts ofthe numberoftimes the scheme was better than same as worse than thebaseline scheme on the datasets used in the experiment.in this case there is onlyone datasetmethod was equivalent to method baseline onceandmethod was worse than it once.the annotation placed at the bottomofcolumn to help you remember the meanings ofthe three counts setupin the setuppanel shown in figure we left most options at their defaultvalues.the experiment is a cross-validation repeated times.you canalter the number offolds in the box at center left and the number ofrepetitionsin the box at center right.the experiment type is classificationyou can specifyregression instead.you can choose several datasetsin which case each algorithmis applied to each datasetand change the order ofiteration using the data setsfirstand algorithm firstbuttons.the alternative to cross-validation is theholdout method.there are two variantsdepending on whether the order ofthedataset is preserved or the data is randomized.you can specify the percentagesplit default is two-thirds training set and one-third test set.experimental setups can be saved and reopened.you can make notes aboutthe setup by pressing the notesbuttonwhich brings up an editor window.serious weka users soon find the need to open up an experiment and rerun itwith some modifications perhaps with a new dataset or a new learning am page algorithm.it would be nice to avoid having to recalculate all the results that havealready been obtained! ifthe results have been placed in a database rather thanan arff or csv filethis is exactly what happens.you can choose jdbc data-basein the results destination selector and connect to any database that has ajdbc driver.you need to specify the database s url and enter a username andpassword.to make this work with your database you may need to modify thewekaexperimentdatabaseutils.propsfile in the weka distribution.ifyou alteran experiment that uses a databaseweka will reuse previously computed resultswhenever they are available.this greatly simplifies the kind ofiterative experi-mentation that typically characterizes data mining setupthe experimenter has an advanced mode.click near the top ofthe panel shownin figure to obtain the more formidable version ofthe panel shown infigure enlarges the options available for controlling the experiment includingfor examplethe ability to generate learning experimenterfigure up an experiment in advanced am page advanced mode is hard to useand the simple version suffices for most pur-poses.for examplein advanced mode you can set up an iteration to test an algorithm with a succession ofdifferent parameter valuesbut the same effect can be achieved in simple mode by putting the algorithm into the listseveral times with different parameter values.something you may need theadvanced mode for is to set up distributed experimentswhich we describe insection analyze panelour walkthrough used the analyzepanel to perform a statistical significance testofone learning scheme two others zeror.the test wason the error rate the comparisonfield in figure statistics can beselected from the drop-down menu insteadpercentage incorrectpercentageunclassifiedroot mean-squared errorthe remaining error measures from various entropy figures.moreoveryou can see the standarddeviation ofthe attribute being evaluated by ticking the show std deviationscheckbox.use the select basemenu to change the baseline scheme from to one ofthe other learning schemes.for exampleselecting onercauses the others to becompared with this scheme.in factthat would show that there is a statisticallysignificant difference between onerand zerorbut not between onerand from the learning schemesthere are two other choices in the select basemenusummaryand ranking.the former compares each learning scheme withevery other scheme and prints a matrix whose cells contain the number ofdatasets on which one is significantly better than the other.the latter ranks theschemes according to the total number ofdatasets that represent wins andlosses and prints a league table.the first column in the output gives the dif-ference between the number ofwins and the number oflosses.the rowand columnfields determine the dimensions ofthe comparisonmatrix.clicking selectbrings up a list ofall the features that have been meas-ured in the experiment in other wordsthe column labels ofthe spreadsheetin figure can select which to use as the rows and columns ofthematrix.the selection does not appear in the selectbox because more than oneparameter can be chosen simultaneously. figure shows which items areselected for the rows and columns offigure two lists show the exper-imental parameters columns ofthe spreadsheet.datasetis selected for therows there is only one in this casethe iris datasetand schemeschemeoptionsand scheme_version_idare selected for the column usual conven-tion ofshift-clicking selects multiple entries.all three can be seen in in factthey are more easily legible in the key at the analyze am page experimenteracbdfigure and columns offigure row fieldb column fieldc resultofswapping the row and column selectionsand substituting runfor am page ifthe row and column selections were swapped and the perform testbuttonwere pressed againthe matrix would be transposedgiving the result in are now three rowsone for each algorithmand one columnforthe single dataset.ifinstead the row ofdatasetwere replaced by runand thetest were performed againthe result would be as in figure the runs ofthe cross-validationofwhich there are there are now number in parentheses after each row label in figure in figure is the number ofresults corresponding to that row inother wordsthe number ofmeasurements that participate in the averages dis-played by the cells in that row.there is also a button that allows you to select asubset ofcolumns to display baseline column is always includedandanother that allows you to select the output formatplain text the latex typesetting systemand csv processing over several machinesa remarkable feature ofthe experimenter is that it can split up an experimentand distribute it across several processors.this is for advanced weka users andis only available from the advanced version ofthe setuppanel.some users avoidworking with this panel by setting the experiment up on the simple version andswitching to the advanced version to distribute itbecause the experiment sstructure is preserved when you switch.howeverdistributing an experiment isan advanced feature and is often difficult.for examplefile and directory per-missions can be tricky to set up.distributing an experiment works best when the results are all sent to acentral database by selecting jdbc databaseas the results destination in thepanel shown in figure uses the rmi facilityand works with any data-base that has a jdbc driver.it has been tested on several freely available data-bases.alternativelyyou could instruct each host to save its results to a differentarff file and merge the files afterwards.to distribute an experimenteach host must have java haveaccess to whatever datasets you are usingand be running the weka.experi-ment.remoteengineexperiment server.ifresults are sent to a central databasethe appropriate jdbc drivers must be installed on each host.getting all thisright is the difficult part ofrunning distributed experiments.to initiate a remote engine experiment server on a host machinefirst copyremoteexperimentserver.jarfrom the weka distribution to a directory on thehost.unpack it withjar xvf processing over several am page it expands to two filesremoteengine.jaran executable jarfile that contains theexperiment serverand remote.policy.the remote.policyfile grants the remote engine permission to perform certainoperationssuch as connecting to ports or accessing a directory.it needs to be edited to specify correct paths in some ofthe permissionsthis is self-explanatory when you examine the file.by defaultit specifies that code can bedownloaded on http port from anywhere on the webbut the remoteengines can also load code from a file url instead.to arrange thisuncommentthe example and replace the pathname appropriately.the remote engines alsoneed to be able to access the datasets used in an experiment the first entryin remote.policy.the paths to the datasets are specified in the experimenteri.e.the clientand the same paths must be applicable in the context oftheremote engines.to facilitate this it may be necessary to specify relative path-names by selecting the use relative pathstick box shown in the setuppanel ofthe experimenter.to start the remote engine servertypejava remoteengine.jarpath_to_any_jdbc_drivers-djava.security.policyremote.policy weka.experiment.remoteenginefrom the directory containing remoteengine.jar.ifall goes well you will see thismessage something like ithost name ml.cs.waikato.ac.nzremoteengineexceptionconnectionrefusedtohostml.cs.waikato.ac.nz nested exception isjava.net.connectexception connection refusedattempting to start rmi registry...remoteengine bound in rmi registrydespite initial appearancesthis is good news! the connection was refusedbecause no rmi registry was running on that serverand hence the remoteengine has started one.repeat the process on all hosts.it does not make senseto run more than one remote engine on a machine.start the experimenter by typingjava url specifies where the remote engines can find the code to be executed.ifthe url denotes a directory that contains the weka directory ratherthan a jarfileit must end with path separator experimenter s advanced setuppanel in figure contains a smallpane at center left that determines whether an experiment will be distributedor not.this is normally inactive.to distribute the experiment click the am page box to activate the hostsbuttona window will pop up asking for the machinesover which to distribute the experiment.host names should be fully qualifiede.g.ml.cs.waikato.ac.nz.having entered the hostsconfigure the rest ofthe experiment in the usualway stillconfigure it before switching to the advanced setup mode.when the experiment is started using the runpanelthe progress ofthe subex-periments on the various hosts is displayedalong with any error messages.distributing an experiment involves splitting it into subexperiments thatrmi sends to the hosts for execution.by defaultexperiments are partitionedby datasetin which case there can be no more hosts than there are datasets.then each subexperiment is self-containedit applies all schemes to a singledataset.an experiment with only a few datasets can be partitioned by runinstead.for examplea times cross-validation would be split into per processing over several am page am page lurking behind weka s interactive interfaces the explorerthe knowledgeflowand the experimenter lies its basic functionality.this can be accessed inraw form through a command-line interface.select simple clifrom the inter-face choices at the bottom offigure to bring up a plain textual panelwith a line at the bottom on which you enter commands.alternativelyuse theoperating system s command-line interface to run the classes in weka.jardirectlyin which case you must first set the classpathenvironment variableas explained in weka s startedat the beginning ofsection we used the explorer to invoke the learneron the weather data.to do the same thing in the command-line interfacetypejava command-line am page into the line at the bottom ofthe text panel.this incantation calls the javavirtual machine the simple clijava is already loaded and instructs it toexecute is organized in packagesthat correspond to a directory hier-archy.the program to be executed is called resides in the treespackagewhich is a subpackage ofclassifierswhich is part ofthe overall wekapackage.the next section gives more details ofthe package structure.the that the next argument is the name ofthe training filewe are assumingthat the weather data resides in a datasubdirectory ofthe directory from which you fired up weka.the result resembles the text shown in figure the simple cli it appears in the panel above the line where you typed structure of wekawe have explained how to invoke filtering and learning schemes with theexplorer and connect them together with the knowledge flow interface.to gofurtherit is necessary to learn something about how weka is put together.detailedup-to-date information can be found in the online documentationincluded in the distribution.this is more technical than the descriptions ofthelearning and filtering schemes given by the morebutton in the explorer andknowledge flow s object editors.it is generated directly from comments in thesource code using sun s javadoc utility.to understand its structureyou need toknow how java programs are organized.classes instances and packagesevery java program is implemented as a class.in object-oriented programminga classis a collection ofvariables along with some methodsthat operate on them.togetherthey define the behavior ofan object belonging to the class.an objectis simply an instantiation ofthe class that has values assigned to all the class svariables.in javaan object is also called an instanceofthe class.unfortunatelythis conflicts with the terminology used in this bookwhere the terms classandinstanceappear in the quite different context ofmachine learning.from nowonyou will have to infer the intended meaning ofthese terms from theircontext.this is not difficult and sometimes we ll use the word objectinsteadofjava s instanceto make things clear.in wekathe implementation ofa particular learning algorithm is encapsu-lated in a class.for examplethe described previously builds a tree.each time the java virtual machine executes creates aninstance ofthis class by allocating memory for building and storing a decisiontree classifier.the algorithmthe classifier it buildsand a procedure for out-putting the classifier are all part ofthat instantiation ofthe command-line am page larger programs are usually split into more than one class.the not actually contain any code for building a decision tree.itincludes references to instances ofother classes that do most ofthe work.whenthere are a lot ofclasses as in weka they become difficult to comprehendand navigate.java allows classes to be organized into packages.a packageis justa directory containing a collection ofrelated classesfor examplethe treespackage mentioned previously contains the classes that implement decisiontrees.packages are organized in a hierarchy that corresponds to the directoryhierarchytreesis a subpackage ofthe classifierspackagewhich is itselfa sub-package ofthe overall wekapackage.when you consult the online documentation generated by javadoc from yourweb browserthe first thing you see is an alphabetical list ofall the packages inwekaas shown in figure we introduce a few ofthem in order ofimportance.the weka.core packagethe corepackage is central to the weka systemand its classes are accessed from almost every other class.you can determine what they are by clicking on the weka.corehyperlinkwhich brings up the web page shown in web page in figure is divided into two partsthe interfacesummaryand the class summary.the latter is a list ofclasses contained withinthe packageand the former lists the interfaces it provides.an interface is similarto a classthe only difference being that it doesn t actually do anything by itself it is merely a list ofmethods without actual implementations.other classes candeclare that they implement a particular interface and then provide code forits methods.for examplethe optionhandlerinterface defines those methodsthat are implemented by all classes that can process command-line optionsincluding all classifiers.the key classes in the corepackage are attributeinstanceand instances.anobject ofclass attributerepresents an attribute.it contains the attribute s nameits typeandin the case ofa nominal or string attributeits possible values.anobject ofclass instancecontains the attribute values ofa particular instanceandan object ofclass instancesholds an ordered set ofinstancesin other wordsadataset.you can learn more about these classes by clicking their hyperlinkswereturn to them in chapter when we show you how to invoke machine learn-ing schemes from other java code.howeveryou can use weka from thecommand line without knowing the details.clicking the overviewhyperlink in the upper left corner ofany documenta-tion page returns you to the listing ofall the packages in weka that is shown infigure structure of am page command-line interfaceabfigure javadoca the front page and the am page the weka.classifiers packagethe classifierspackage contains implementations ofmost ofthe algorithms forclassification and numeric prediction described in this book.numeric predic-tion is included in classifiersit is interpreted as prediction ofa continuous class.the most important class in this package is classifierwhich defines the generalstructure ofany scheme for classification or numeric prediction.classifiercon-tains three methodsbuildclassifierclassifyinstanceand distributionforin-stance.in the terminology ofobject-oriented programmingthe learningalgorithms are represented by subclasses ofclassifierand therefore automati-cally inherit these three methods.every scheme redefines them according to howit builds a classifier and how it classifies instances.this gives a uniform inter-face for building and using classifiers from other java code.hencefor examplethe same evaluation module can be used to evaluate the performance ofanyclassifier in weka.to see an exampleclick on weka.classifiers.treesand then on decisionstumpwhich is a class for building a simple one-level binary decision tree anextra branch for missing values.its documentation pageshown in figure the fully qualified name ofthis classweka.classifiers.trees.decisionstumpnear the top.you have to use this rather lengthy name whenever you build adecision stump from the command line.the class name is sited in a small treestructure showing the relevant part ofthe class hierarchy.as you can seedeci-sionstumpis a subclass ofweka.classifiers.classifierwhich is itselfa subclass ofjava.lang.object.the objectclass is the most general one in javaall classes areautomatically subclasses ofit.after some generic information about the class briefdocumentationitsversionand its author figure gives an index ofthe constructors andmethods ofthis class.a constructoris a special kind ofmethod that is calledwhenever an object ofthat class is createdusually initializing the variables thatcollectively define its state.the index ofmethods lists the name ofeach onethetype ofparameters it takesand a short description ofits functionality.beneaththose indicesthe web page gives more details about the constructors andmethods.we will return to these details later.as you can seedecisionstumpoverwrites the distributionforinstancemethod from classifierthe default implementation ofclassifyinstancein clas-sifierthen uses this method to produce its classifications.in additionit containsthe tostringtosourceand mainmethods.the first returns a textualdescription ofthe classifierused whenever it is printed on the screen.thesecond is used to obtain a source code representation ofthe learned classifier.the third is called when you ask for a decision stump from the command linein other wordsevery time you enter a command beginning withjava structure of am page command-line interfacefigure class ofthe am page the presence ofa mainmethod in a class indicates that it can be run from thecommand line and that all learning methods and filter algorithms implementit.other packagesseveral other packages listed in figure are worth mentioningweka.asso-ciationsweka.clusterersweka.estimatorsweka.filtersand weka.attributeselec-tion.the weka.associationspackage contains association rule learners.thesehave been placed in a separate package because association rules are funda-mentally different from classifiers.the weka.clustererspackage containsmethods for unsupervised learning.the weka.estimatorspackage contains sub-classes ofa generic estimatorclasswhich computes different types ofproba-bility distribution.these subclasses are used by the na ve bayes algorithmamong others.in the weka.filterspackagethe filterclass defines the general structure ofclasses containing filter algorithmswhich are all implemented as subclasses offilter.like classifiersfilters can be used from the command linewe will seehow shortly.the weka.attributeselectionpackage contains several classes for attribute selection.these are used by the attributeselectionfilterinweka.filters.supervised.attributebut can also be invoked structure of am page javadoc indicesas mentioned previouslyall classes are automatically subclasses ofobject.toexamine the tree that corresponds to weka s hierarchy ofclassesselect theoverviewlink from the top ofany page ofthe online documentation.click treeto display the overview as a tree that shows which classes are subclasses or super-classes ofa particular class for examplewhich classes inherit from classifier.the online documentation contains an index ofall publicly accessible vari-ables fields and methods in weka in other wordsall fields andmethods that you can access from your own java code.to view itclick overviewand then index.suppose you want to check which weka classifiers and filters are capable ofoperating incrementally.searching for the word incrementalin the index wouldsoon lead you to the keyword updateableclassifier.in factthis is a java inter-faceinterfaces are listed after the classes in the overview tree.you are lookingfor all classes that implement this interface.clicking any occurrence ofit in thedocumentation brings up a page that describes the interface and lists the clas-sifiers that implement it.finding the filters is a little trickier unless you knowthe keyword streamablefilterwhich is the name ofthe interface that streamsdata through a filteragainits page lists the filters that implement it.you wouldstumble across that keyword ifyou knew any example ofa filter that couldoperate optionsin the preceding examplethe was used in the command line to com-municate the name ofthe training file to the learning algorithm.there are manyother options that can be used with any learning schemeand also scheme-specific ones that apply only to particular schemes.ifyou invoke a schemewithout any command-line options at allit displays the applicable optionsfirstthe general optionsthen the scheme-specific ones.in the command-line inter-facetypejava ll see a list ofthe options common to all learning schemesshown in by those that apply only to in table willexplain the generic options and then briefly review the scheme-specific ones.generic optionsthe options in table determine which data is used for training and testinghow the classifier is evaluatedand what kind ofstatistics are displayed.forexamplethe is used to provide the name ofthe test file when command-line am page ating a learning scheme on an independent test set.by default the class is thelast attribute in an arff filebut you can declare another one to be the classusing by the position ofthe desired for the forthe secondand so on.when cross-validation is performed default ifa testfile is not providedthe data is randomly shuffled first.to repeat the cross-validation several timeseach time reshuffling the data in a different wayset therandom number seed with value a large dataset you maywant to reduce the number offolds for the cross-validation from the defaultvalue using the explorercost-sensitive evaluation is invoked as described in achieve the same effect from the command lineuse the toprovide the name ofa file containing the cost matrix.here is a cost matrix forthe weather number of rows and columns in the if true class yes and prediction no penalty is if true class no and prediction yes penalty is first line gives the number ofrows and columnsthat isthe number ofclassvalues.then comes the matrix ofpenalties.comments introduced by beappended to the end ofany line.it is also possible to save and load models.ifyou provide the name ofanoutput file using saves the classifier generated from the training options for learning schemes in weka.optionfunction-t filespecify training file-t filespecify test file if none a cross-validation is performed on the training data-c indexspecify index of class attribute-s number seedspecify random number seed for cross-validation-x of foldsspecify number of folds for cross-validation-m matrix filespecify file containing cost matrix-d filespecify output file for model-l filespecify input file for model-ooutput statistics only not the classifier-ioutput information retrieval statistics for two-class problems-koutput information-theoretical statistics-p rangeoutput predictions for test instances-voutput no statistics for training data-routput cumulative margin distribution-z nameoutput source representation of classifier-goutput graph representation of am page to evaluate the same classifier on a new batch oftest datayou load it back using-linstead ofrebuilding it.ifthe classifier can be updated incrementallyyoucan provide both a training file and an input fileand weka will load the clas-sifier and update it with the given training instances.ifyou wish only to assess the performance ofa learning schemeuse output ofthe model.use see the performance measures ofpre-cisionrecalland f-measure compute information-theoretical measures from the probabilities derived by a learning schemesection users often want to know which class values the learning scheme actu-ally predicts for each test instance.the prints each test instance snumberits classthe confidence ofthe scheme s predictionand the predictedclass value.it also outputs attribute values for each instance and must be fol-lowed by a specification ofthe range use ifyou don t want anyattribute values.you can also output the cumulative margin distributionfor thetraining datawhich shows how the distribution ofthe margin measure changes with the number ofboosting iterations.finallyyou canoutput the classifier s source representationand a graphical representation ifthe classifier can produce one.scheme-specific optionstable shows the options specific to can force the algorithm to usethe unpruned tree instead ofthe pruned one.you can suppress subtree raisingwhich increases efficiency.you can set the confidence threshold for pruning andthe minimum number ofinstances permissible at any leaf both parameterswere described in section well as s standard command-line interfacetable options for the decision tree learner.optionfunction-uuse unpruned tree-c confidencespecify confidence threshold for pruning-m of instancesspecify minimum number of instances in any leaf-ruse reduced-error pruning-n of foldsspecify number of folds for reduced-error pruning use one fold as pruning set-buse binary splits only-sdon t perform subtree raising-lretain instance information-asmooth the probability estimates using laplace smoothing-qseed for shuffling am page procedurereduced-error pruning can be per-formed.the governs the size ofthe holdout setthe dataset is dividedequally into that number ofparts and the last is held out value smooth the probability estimates using the laplace techniqueset therandom number seed for shuffling the data when selecting a pruning setandstore the instance information for future visualization.finallyto build a binarytree instead ofone with multiway branches for nominal attributesuse am page am page when invoking learning schemes from the graphical user interfaces or thecommand linethere is no need to know anything about programming in java.in this section we show how to access these algorithms from your own code.indoing sothe advantages ofusing an object-oriented programming language willbecome clear.from now onwe assume that you have at least some rudimen-tary knowledge ofjava.in most practical applications ofdata mining the learn-ing component is an integrated part ofa far larger software environment.iftheenvironment is written in javayou can use weka to solve the learning problemwithout writing any machine learning code simple data mining applicationwe present a simple data mining application for learning a model that classi-fies text files into two categorieshitand miss.the application works for arbi-trary documentswe refer to them as messages.the implementation uses machine am page stringtowordvectorfilter mentioned in section to convert themessages into attribute vectors in the manner described in section that the program is called every time a new file is to be processed.iftheweka user provides a class label for the filethe system uses it for trainingifnotit classifies it.the decision tree classifier used to do the through the codefigure shows the source code for the application programimplemented ina class called messageclassifier.the command-line arguments that the mainmethod accepts are the name ofa text file by name ofa fileholding an object ofclass messageclassifier classifica-tion ofthe message in the file user provides a classificationthemessage will be converted into an example for trainingifnotthe message-classifierobject will be used to classify it as hitor miss.mainthe mainmethod reads the message into a java stringbufferand checkswhether the user has provided a classification for it.then it reads a message-classifierobject from the file given by creates a new object ofclass messageclassifierifthis file does not exist.in either case the resulting object iscalled messagecl.after checking for illegal command-line optionsthe programcalls the method updatedatato update the training data stored in messageclifa classification has been providedotherwiseit calls classifymessageto clas-sify it.finallythe messageclobject is saved back into the filebecause it may have changed.in the following sectionswe first describe how a new messageclassifierobject is created by the constructor messageclassifierandthen explain how the two methods updatedataand classifymessagework.messageclassifiereach time a new messageclassifieris createdobjects for holding the filter andclassifier are generated automatically.the only nontrivial part ofthe process iscreating a datasetwhich is done by the constructor messageclassifier.first thedataset s name is stored as a string.then an attributeobject is created for eachattributeone to hold the string corresponding to a text message and the otherfor its class.these objects are stored in a dynamic array oftype fastvector.fastvectoris weka s own implementation ofthe standard java vectorclass andis used throughout weka for historical reasons.attributes are created by invoking one ofthe constructors in the classattribute.this class has a constructor that takes one parameter the attribute sname and creates a numeric attribute.howeverthe constructor we use machine am page through the java program for classifying text messages into two classes. weka.core.attributeimport weka.core.instanceimport weka.core.instancesimport weka.core.fastvectorimport weka.core.utilsimport weka.classifiers.classifierimport weka.filters.filterimport weka.filters.unsupervised.attribute.stringtowordvectorimport java.io.public class messageclassifier implements serializable the training data gathered so far. private instances m_data null the filter used to generate the word counts. private stringtowordvector m_filter new stringtowordvector the actual classifier. private classifier m_classifier new whether the model is up to date. private boolean m_uptodate constructs empty training dataset. public messageclassifier throws exception string nameofdataset create vector of attributes. fastvector attributes new add attribute for holding messages. attributes.addelementnew attributemessage code for the message am page machine learning add class attribute. fastvector classvalues new classvalues.addelementmiss classvalues.addelementhit attributes.addelementnew attributeclass classvalues create dataset with initial capacity of and set index of class. m_data new instancesnameofdataset attributes m_data.setclassindexm_data.numattributes updates data using the given training message. public void updatedatastring message string classvalue throws exception make message into instance. instance instance makeinstancemessage m_data set class value for instance. instance.setclassvalueclassvalue add instance to training data. m_data.addinstance m_uptodate false classifies a given message. public void classifymessagestring message throws exception check whether classifier has been built. if throw new exceptionno classifier available. check whether classifier and filter are up to date. if figure am page through the initialize filter and tell it about the input format. m_filter.setinputformatm_data generate word counts from the training data. instances filtereddata filter.usefilterm_data m_filter rebuild classifier. m_classifier.buildclassifierfiltereddata m_uptodate true make separate little test set so that message does not get added to string attribute in m_data. instances testset m_data.stringfreestructure make message into test instance. instance instance makeinstancemessage testset filter instance. m_filter.inputinstance instance filteredinstance m_filter.output get index of predicted class value. double predicted m_classifier.classifyinstancefilteredinstance output class value. system.err.printlnmessage classified as m_data.classattribute.valueintpredicted method that converts a text message into an instance. private instance makeinstancestring text instances data create instance of length two. instance instance new set value for message attribute attribute messageatt data.attributemessage instance.setvaluemessageatt messageatt.addstringvaluetextfigure am page machine learning give instance access to attribute information from the dataset. instance.setdatasetdata return instance main method. public static void mainstring options try read message file into string. string messagename utils.getoptionm options if throw new exceptionmust provide name of message file. filereader m new filereadermessagename stringbuffer message new stringbuffer int l while m.read message.appendcharl m.close check if class value is given. string classvalue utils.getoptionc options if model file exists read it otherwise create new one. string modelname utils.getoptiono options if throw new exceptionmust provide name of model file. messageclassifier messagecl try objectinputstream modelinobjectfile new objectinputstreamnew fileinputstreammodelname messagecl modelinobjectfile.readobject modelinobjectfile.close catch e messagecl new messageclassifierfigure am page through the check if there are any options left utils.checkforremainingoptionsoptions process message. if messagecl.updatedatamessage.tostring classvalue else messagecl.classifymessagemessage.tostring save message classifier object. objectoutputstream modeloutobjectfile new objectoutputstreamnew fileoutputstreammodelname modeloutobjectfile.writeobjectmessagecl modeloutobjectfile.close catch e e.printstacktrace figure two parametersthe attribute s name and a reference to a fastvector.ifthisreference is nullas in the first application ofthis constructor in our programweka creates an attribute oftype string.otherwisea nominal attribute iscreated.in that case it is assumed that the fastvectorholds the attribute valuesas strings.this is how we create a class attribute with two values hitand missby passing the attribute s name its values stored in a fastvector to attribute.to create a dataset from this attribute informationmessageclassifiermustcreate an object ofthe class instancesfrom the corepackage.the constructor ofinstancesused by messageclassifiertakes three argumentsthe dataset snamea fastvectorcontaining the attributesand an integer indicating thedataset s initial capacity.we set the initial capacity to is expanded auto-matically ifmore instances are added.after constructing the datasetmessage-classifiersets the index ofthe class attribute to be the index ofthe am page updatedatanow that you know how to create an empty datasetconsider how the mes-sageclassifierobject actually incorporates a new training message.the methodupdatedatadoes this job.it first converts the given message into a traininginstance by calling makeinstancewhich begins by creating an object ofclassinstancethat corresponds to an instance with two attributes.the constructor ofthe instanceobject sets all the instance s values to be missingand its weight next step in makeinstanceis to set the value ofthe string attributeholding the text ofthe message.this is done by applying the setvaluemethodofthe instanceobjectproviding it with the attribute whose value needs to bechangedand a second parameter that corresponds to the new value s index inthe definition ofthe string attribute.this index is returned by the addstring-valuemethodwhich adds the message text as a new value to the string attrib-ute and returns the position ofthis new value in the definition ofthe stringattribute.internallyan instancestores all attribute values as double-precision floating-point numbers regardless ofthe type ofthe corresponding attribute.in the caseofnominal and string attributes this is done by storing the index ofthe corre-sponding attribute value in the definition ofthe attribute.for examplethe firstvalue ofa nominal attribute is represented by second by so on.the same method is used for string attributesaddstringvaluereturns the indexcorresponding to the value that is added to the definition ofthe attribute.once the value for the string attribute has been setmakeinstancegives thenewly created instance access to the data s attribute information by passing it areference to the dataset.in wekaan instanceobject does not store the type ofeach attribute explicitlyinsteadit stores a reference to a dataset with the corresponding attribute information.returning to updatedataonce the new instance has been returned frommakeinstanceits class value is set and it is added to the training data.we alsoinitialize m_uptodatea flag indicating that the training data has changed andthe predictive model is hence not up to date.classifymessagenow let s examine how messageclassifierprocesses a message whose class labelis unknown.the classifymessagemethod first checks whether a classifier hasbeen built by determining whether any training instances are available.it thenchecks whether the classifier is up to date.ifnot the training data haschanged it must be rebuilt.howeverbefore doing so the data must be con-verted into a format appropriate for learning using the stringtowordvectorfilter.firstwe tell the filter the format ofthe input data by passing it a reference tothe input dataset using setinputformat.every time this method is machine am page filter is initialized that isall its internal settings are reset.in the next stepthedata is transformed by usefilter.this generic method from the filterclassapplies a filter to a dataset.in this casebecause stringtowordvectorhas just beeninitializedit computes a dictionary from the training dataset and then uses itto form word vectors.after returning from usefilterall the filter s internal set-tings are fixed until it is initialized by another call ofinputformat.this makesit possible to filter a test instance without updating the filter s internal settingsin this casethe dictionary.once the data has been filteredthe program rebuilds the classifier in ourcase a decision tree by passing the training data to its buildclassifiermethod and sets m_uptodateto true.it is an important convention in wekathat the buildclassifiermethod completely initializes the model s internal set-tings before generating a new classifier.hence we do not need to construct anew before we call buildclassifier.having ensured that the model stored in m_classifieris currentwe proceedto classify the message.before makeinstanceis called to create an instanceobject from ita new instancesobject is created to hold the new instance andpassed as an argument to makeinstance.this is done so that makeinstancedoes not add the text ofthe message to the definition ofthe string attribute inm_data.otherwisethe size ofthe m_dataobject would grow every time a newmessage was classifiedwhich is clearly not desirable it should only grow whentraining instances are added.hence a temporary instancesobject is created anddiscarded once the instance has been processed.this object is obtained usingthe method stringfreestructurewhich returns a copy ofm_datawith anempty string attribute.only then is makeinstancecalled to create the newinstance.the test instance must also be processed by the stringtowordvectorfilterbefore being classified.this is easythe inputmethod enters the instance intothe filter objectand the transformed instance is obtained by calling output.then a prediction is produced by passing the instance to the classifier s classi-fyinstancemethod.as you can seethe prediction is coded as a doublevalue.this allows weka s evaluation module to treat models for categorical andnumeric prediction similarly.in the case ofcategorical predictionas in thisexamplethe doublevariable holds the index ofthe predicted class value.tooutput the string corresponding to this class valuethe program calls the valuemethod ofthe dataset s class attribute.there is at least one way in which our implementation could be improved.the classifier and the stringtowordvectorfilter could be combined using the filteredclassifiermetalearner described in section classi-fier would then be able to deal with string attributes directlywithout explicitlycalling the filter to transform the data.we didn t do this because we wanted todemonstrate how filters can be used through the am page am page suppose you need to implement a special-purpose learning algorithm that isnot included in weka.or suppose you are engaged in machine learning researchand want to investigate a new learning scheme.or suppose you just want tolearn more about the inner workings ofan induction algorithm by actually pro-gramming it yourself.this section uses a simple example to show how to makefull use ofweka s class hierarchy when writing classifiers.weka includes the elementary learning schemes listed in table educational purposes.none take any scheme-specific command-lineoptions.they are all useful for understanding the inner workings ofa classifier.as an examplewe describe the imple-ments the decision tree learner from section example classifierfigure gives the source code you cansee from the codeextends the classifierclass.every classifier in weka does sowhether it predicts a nominal class or a numeric new learning am page the first method in globalinfowe mention it here before moving on to the more interesting parts.it simply returns a string that is displayed in weka s graphical user interfaces when this scheme isselected.buildclassifierthe buildclassifiermethod constructs a classifier from a training dataset.inthis case it first checks the data for a nonnominal classmissing attribute valueor any attribute that is not nominalbecause the algorithm cannot handlethese.it then makes a copy ofthe training set avoid changing the originaldata and calls a method from weka.core.instancesto delete all instances withmissing class valuesbecause these instances are useless in the training process.finallyit calls maketreewhich actually builds the decision tree by recursivelygenerating all subtrees attached to the root node.maketreethe first step in maketreeis to check whether the dataset is empty.ifit isaleafis created by setting m_attributeto null.the class value m_classvalueassigned to this leafis set to be missingand the estimated probability for eachofthe dataset s classes in m_distributionis initialized to instancesare presentmaketreefinds the attribute that yields the greatest informationgain for them.it first creates a java enumerationofthe dataset s attributes.iftheindex ofthe class attribute is set as it will be for this dataset the class is auto-matically excluded from the enumeration.inside the enumerationeach attribute s information gain is computed bycomputeinfogainand stored in an array.we will return to this method later.the indexmethod from weka.core.attributereturns the attribute s index in thedatasetwhich is used to index the array.once the enumeration is completetheattribute with the greatest information gain is stored in the instance variablem_attribute.the maxindexmethod from weka.core.utilsreturns the index ofthe greatest value in an array ofintegers or doubles.ifthere is more than oneelement with the maximum valuethe first is returned. the index ofthis new learning schemestable learning schemes in weka.schemedescriptionbook sectionweka.classifiers.bayes.naivebayessimpleprobabilistic tree am page example weka.classifiers.treesimport weka.classifiers.import weka.core.import java.io.import java.util. class implementing an decision tree classifier. class extends classifier the nodes successors. private m_successors attribute used for splitting. private attribute m_attribute class value if node is leaf. private double m_classvalue class distribution if node is leaf. private double m_distribution class attribute of dataset. private attribute m_classattribute returns a string describing the classifier. a description suitable for the gui. public string globalinfo return for constructing an unpruned decision tree based on the can only deal with nominal attributes. no missing values empty leaves may result in unclassified instances. for more see r. quinlan of decision machine learning. pp. figure code for the decision tree am page new learning schemes builds decision tree classifier. data the training data exception if classifier cant be built successfully public void buildclassifierinstances data throws exception if throw new nominal class please. enumeration enumatt data.enumerateattributes while if enumatt.nextelement.isnominal throw new only nominal please. enumeration enum data.enumerateinstances while if enum.nextelement.hasmissingvalue throw new no missing values data new instancesdata data.deletewithmissingclass maketreedata method for building an tree. data the training data exception if decision tree cant be built successfully private void maketreeinstances data throws exception check if no instances have reached this node. if m_attribute nullfigure am page example m_classvalue instance.missingvalue m_distribution new doubledata.numclasses return compute attribute with maximum information gain. double infogains new doubledata.numattributes enumeration attenum data.enumerateattributes while attribute att attenum.nextelement infogainsatt.index computeinfogaindata att m_attribute data.attributeutils.maxindexinfogains make leaf if information gain is zero. otherwise create successors. if m_attribute null m_distribution new doubledata.numclasses enumeration instenum data.enumerateinstances while instance inst instenum.nextelement m_distributionint inst.classvalue utils.normalizem_distribution m_classvalue utils.maxindexm_distribution m_classattribute data.classattribute else instances splitdata splitdatadata m_attribute m_successors new for j j m_attribute.numvalues j m_successorsj new m_successorsj.maketreesplitdataj classifies a given test instance using the decision tree. instance the instance to be classifiedfigure am page new learning schemes if throw new no missing values if null return m_classvalue else return m_successorsint instance.valuem_attribute. classifyinstanceinstance computes class distribution for instance using decision tree. instance the instance for which distribution is to be computed the class distribution for the given instance public double distributionforinstanceinstance instance throws nosupportformissingvaluesexception if throw new no missing values if null return m_distribution else return m_successorsint instance.valuem_attribute. distributionforinstanceinstance the classification public double classifyinstanceinstance instance throws nosupportformissingvaluesexception prints the decision tree using the private tostring method from below. figure am page example a textual description of the classifier public string tostring if null null return no model built yet. return computes information gain for an attribute. data the data for which info gain is to be computed att the attribute the information gain for the given attribute and data private double computeinfogaininstances data attribute att throws exception double infogain computeentropydata instances splitdata splitdatadata att for j j att.numvalues j if infogain splitdataj.numinstances data.numinstances computeentropysplitdataj return infogain computes the entropy of a dataset. data the data for which entropy is to be computed the entropy of the datas class distribution private double computeentropyinstances data throws exception double classcounts new doubledata.numclassesfigure am page new learning schemes enumeration instenum data.enumerateinstances while instance inst instenum.nextelement classcountsint inst.classvalue double entropy for j j data.numclasses j if entropy classcountsj entropy data.numinstances return entropy splits a dataset according to the values of a nominal attribute. data the data which is to be split att the attribute to be used for splitting the sets of instances produced by the split private instances splitdatainstances data attribute att instances splitdata new instancesatt.numvalues for j j att.numvalues j splitdataj new instancesdata data.numinstances enumeration instenum data.enumerateinstances while instance inst instenum.nextelement splitdataint inst.valueatt.addinst for i i splitdata.length i splitdatai.compactify return splitdata outputs a tree at a certain level. figure am page example level the level at which the tree is to be printed private string tostringint level stringbuffer text new stringbuffer if null if text.append null else text.append m_classattribute.valueint m_classvalue else for j j m_attribute.numvalues j text.appendn for i i level i text.append text.appendm_attribute.name m_attribute.valuej text.appendm_successorsj.tostringlevel return text.tostring main method. args the options for the classifier public static void mainstring args try system.out.printlnevaluation.evaluatemodelnew args catch e system.err.printlne.getmessage figure am page ute is passed to the attributemethod from weka.core.instanceswhich returnsthe corresponding attribute.you might wonder what happens to the array field corresponding to the classattribute.we need not worry about this because java automatically initializesall elements in an array ofnumbers to zeroand the information gain is alwaysgreater than or equal to zero.ifthe maximum information gain is zeromake-treecreates a leaf.in that case m_attributeis set to nulland maketreecom-putes both the distribution ofclass probabilities and the class with greatestprobability.the normalizemethod from weka.core.utilsnormalizes an arrayofdoubles to sum to one.when it makes a leafwith a class value assigned to itmaketreestores theclass attribute in m_classattribute.this is because the method that outputs thedecision tree needs to access this to print the class label.ifan attribute with nonzero information gain is foundmaketreesplits thedataset according to the attribute s values and recursively builds subtrees foreach ofthe new datasets.to make the split it calls the method splitdata.thiscreates as many empty datasets as there are attribute valuesstores them in anarray the initial capacity ofeach dataset to the number ofinstances inthe original datasetand then iterates through all instances in the originaldatasetand allocates them to the new dataset that corresponds to the attribute svalue.it then reduces memory requirements by compacting the instancesobjects.returning to maketreethe resulting array ofdatasets is used forbuilding subtrees.the method creates an array for eachattribute valueand calls maketreeon each one by passing it the correspon-ding dataset.computeinfogainreturning to computeinfogainthe information gain associated with an attrib-ute and a dataset is calculated using a straightforward implementation oftheformula in section entropy ofthe dataset is computed.thensplitdatais used to divide it into subsetsand computeentropyis calledon each one.finallythe difference between the former entropy and theweighted sum ofthe latter ones the information gain is returned.themethod computeentropyuses the from weka.core.utilsto obtainthe logarithm base ofa number.classifyinstancehaving seen how constructs a decision treewe now examine how it usesthe tree structure to predict class values and probabilities.every classifier mustimplement the classifyinstancemethod or the distributionforinstancemethod both.the classifiersuperclass contains default new learning am page for both methods.the default implementation ofclassifyinstancecalls distri-butionforinstance.ifthe class is nominalit predicts the class with maximumprobabilityor a missing value ifall probabilities returned by distributionforin-stanceare zero.ifthe class is numericdistributionforinstancemust return asingle-element array that holds the numeric predictionand this is what classi-fyinstanceextracts and returns.converselythe default implementation ofdis-tributionforinstancewraps the prediction obtained from classifyinstanceintoa single-element array.ifthe class is nominaldistributionforinstanceassignsa probability ofone to the class predicted by classifyinstanceand a probabil-ity ofzero to the others.ifclassifyinstancereturns a missing valuethen allprobabilities are set to zero.to give you a better feeling for just what thesemethods dothe overrides them both.let s look first at classifyinstancewhich predicts a class value for a giveninstance.as mentioned in the previous sectionnominal class valueslikenominal attribute valuesare coded and stored in doublevariablesrepresentingthe index ofthe value s name in the attribute declaration.this is used in favorofa more elegant object-oriented approach to increase speed ofexecution.inthe implementation checks whether there aremissing values in the instance to be classifiedifsoit throws an exception.otherwiseit descends the tree recursivelyguided by the instance s attributevaluesuntil a leafis reached.then it returns the class value m_classvaluestoredat the leaf.note that this might be a missing valuein which case the instanceis left unclassified.the method distributionforinstanceworks in exactly thesame wayreturning the probability distribution stored in m_distribution.most machine learning modelsand in particular decision treesserve as amore or less comprehensible explanation ofthe structure found in the data.accordinglyeach ofweka s classifierslike many other java objectsimplementsa tostringmethod that produces a textual representation ofitselfin the formofa s tostringmethod outputs a decision tree in roughlythe same format as recursively prints the tree structure intoa stringvariable by accessing the attribute information stored at the nodes.toobtain each attribute s name and valuesit uses the nameand valuemethodsfrom weka.core.attribute.empty leaves without a class value are indicated by thestring null.mainthe only method in hasn t been described ismainwhich is called whenever the class is executed from the command line.as you can seeit s simpleit basically just tells weka s evaluationclass to eval-uate the given command-line options and prints the resulting string.the one-line expression that does this is enclosed in a try example am page which catches the various exceptions that can be thrown by weka s routines orother java methods.the evaluationmethod in weka.classifiers.evaluationinterprets the genericscheme-independent command-line options described in section and actsappropriately.for exampleit takes the gives the name ofthetraining fileand loads the corresponding dataset.ifthere is no test file it per-forms a cross-validation by creating a classifier object and repeatedly callingbuildclassifier andclassifyinstance or distributionforinstanceon differentsubsets ofthe training data.unless the user suppresses output ofthe model bysetting the corresponding command-line optionit also calls the tostringmethod to output the model built from the full training dataset.what happens ifthe scheme needs to interpret a specific option such as apruning parameter? this is accomplished using the optionhandlerinterface inweka.core.a classifier that implements this interface contains three methodslistoptionssetoptionsand getoptionswhich can be used to list all theclassifier s scheme-specific optionsto set some ofthemand to get the optionsthat are currently set.the evaluationmethod in evaluationautomatically callsthese methods ifthe classifier implements the optionhandlerinterface.oncethe scheme-independent options have been processedit calls setoptionstoprocess the remaining options before using buildclassifierto generate a newclassifier.when it outputs the classifierit uses getoptionsto output a list ofthe options that are currently set.for a simple example ofhow to implementthese methodslook at the source code for weka.classifiers.rules.oner.optionhandlermakes it possible to set options from the command line.toset them from within the graphical user interfacesweka uses the java beansframework.all that is required are set...and get...methods for every param-eter used by the class.for examplethe methods setpruningparameterand get-pruningparameterwould be needed for a pruning parameter.there shouldalso be a pruningparametertiptextmethod that returns a description oftheparameter for the graphical user interface.againsee weka.classifiers.rules.onerfor an example.some classifiers can be incrementally updated as new training instancesarrivethey don t have to process all the data in one batch.in wekaincremen-tal classifiers implement the updateableclassifierinterface in weka.classifiers.this interface declares only one methodnamelyupdateclassifierwhich takesa single training instance as its argument.for an example ofhow to use thisinterfacelook at the source code for weka.classifiers.lazy.ibk.ifa classifier is able to make use ofinstance weightsit should implement theweightedinstanceshandlerinterface from weka.core.then other algorithmssuch as those for boostingcan make use ofthis property.in weka.coreare many other useful interfaces for classifiers for exampleinterfaces for classifiers that are new learning am page graphable.for more information on these and other interfaceslook at thejavadoc for the classes in for implementing classifiersthere are some conventions that you must obey when implementing classifiersin weka.ifyou do notthings will go awry.for exampleweka s evaluationmodule might not compute the classifier s statistics properly when evaluatingit.the first convention has already been mentionedeach time a classifier sbuildclassifiermethod is calledit must reset the model.the checkclassifierclass performs tests to ensure that this is the case.when buildclassifieris calledon a datasetthe same result must always be obtainedregardless ofhow oftenthe classifier has previously been applied to the same or other datasets.howeverbuildclassifiermust not reset instance variables that correspond to scheme-specific optionsbecause these settings must persist through multiple calls ofbuildclassifier.alsocalling buildclassifiermust never change the input data.two other conventions have also been mentioned.one is that when a classifier can t make a predictionits classifyinstancemethod must returninstance.missingvalueand its distributionforinstancemethod must returnprobabilities ofzero for all classes.the implementation in figure doesthis.another convention is that with classifiers for numeric predictionclassi-fyinstancereturns the numeric value that the classifier predicts.some classi-fiershoweverare able to predict nominal classes and their class probabilitiesas well as numeric class values weka.classifiers.lazy.ibkis an example.theseimplement the distributionforinstancemethodand ifthe class is numeric itreturns an array ofsize whose only element contains the predicted numericvalue.another convention not absolutely essential but useful nonetheless is thatevery classifier implements a tostringmethod that outputs a textual descrip-tion for implementing am page am page referencesadriaansp.and mining.harlowenglandaddison-wesley.agrawalr.and algorithms for mining association rules inlarge databases.in j.boccam.jarkeand c.zanioloeditorsproceedings ofthe international conference on very large databasessantiagochile.sanfranciscomorgan mininga performanceperspective.ieee transactions on knowledge and data association rules between sets ofitems in large databases.in p.buneman and s.jajodiaeditorsproceedings ofthe acm sigmod inter-national conference on management ofdatawashingtondc.new noisyirrelevantand novel attributes in instance-based learning algorithms.international journal ofman-machine with many irrelevant features.in proceedings ofthe ninth national conference on artificial intelligenceanaheimca.menlo parkcaaaai algorithms for identifying relevant features.in proceedingsofthe ninth canadian conference on artificial intelligencevancouverbc.sanfranciscomorgan to information extraction technology.tutorialintjoint confon artificial intelligence ijcai kaufmannsan mateo.tutorial notes available at am page scientific method.ithacanycornell university press.atkesonc.g.s.a.schaaland weighted classification from multiple feature subsets.intelligent data linear time detection ofdistance-basedoutliers and applications to security.in proceedings ofthe workshop on datamining for counter terrorism and securitysan francisco.society for industrial and applied essay towards solving a problem in the doctrine ofchances.philosophical transactions ofthe royal society use ofroc curves in test performance eval-uation.archives ofpathology and laboratory logic programming from machinelearning to software engineering.cambridgemamit press.berners-leet.j.hendlerand semantic web.scientific mining techniques for marketingsalesandcustomer support.new yorkjohn mining with neural networks.new yorkmcgraw networks for pattern recognition.new yorkoxford university press.blakec.e.keoghand repository ofmachine learning databaseshttpwww.ics.uci.edumlearnmlrepository.html.depart-ment ofinformation and computer scienceuniversity ofcaliforniairvineca.bli oflabour bargaining reviewnovember.ottawaontariocanadalabour canadabureau oflabourinformation.bluma.and labeled and unlabeled data with co-training.in proceedings ofthe eleventh annual conference on computationallearning theorymadisonwi.san franciscomorgan beliefnetworks from construction to inference.phddissertationcomputer science departmentuniversity am page network classifiers in weka.working paper ofcomputer scienceuniversity ofwaikatonew zealand.brachmanr.j.and in knowledge represen-tation.san franciscomorgan kaufmann.brefeldu.and support vector learning.in r.greiner and d.schuurmanseditorsproceedings ofthe twenty-first international conference on machine learningbanffalbertacanada.new regression.machine predictors.machine small votes for classification in large databases and online.machine forests.machine andregression trees.montereycawadsworth.brins.r.motwanij.d.ullmanand itemset counting and implication rules for market basket data.acm sigmod and eliminating mislabeled train-ing instances.in proceedings ofthe thirteenth national conference on artificial intelligenceportlandor.menlo parkcaaaai expertsystems in classification trees.statistics and tutorial on support vector machines for pattern recogni-tion.data mining and knowledge datamining from concept to implementation.upper saddle rivernjprenticehall.califfm.e.and learning ofpattern-match rules forinformation extraction.in proceedings ofthe sixteenth national conference onartificial intelligenceorlandofl.menlo parkacaaai decision trees to improve case-based learning.in p.utgoffeditorproceedings ofthe tenth international conference on machine learningamherstma.san franciscomorgan am page cavnarw.b.and text categorization.proceed-ings ofthe third symposium on document analysis and information retrieval.las vegasnvunlv algorithm for inducing modular rules.interna-tional journal ofman-machine the web discovering knowledge from hypertext data.sanfranciscocamorgan kaufmann.cheesemanp.and classification andresults.in u.m.fayyadg.piatetsky-shapirop.smythand r.uthurusamyeditorsadvances in knowledge discovery and data mining.menlo parkcaaaai miningan overview from a databaseperspective.ieee transactions on knowledge and data simpler decision trees to facili-tate knowledge discovery.in e.simoudisj.w.hanand u.fayyadeditorsproceedings ofthe second international conference on knowledge discovery anddata miningportlandor.menlo parkcaaaai instance-based learner using an entropicdistance measure.in a.prieditis and s.russelleditorsproceedings ofthetwelfth international conference on machine learningtahoe cityca.sanfranciscomorgan coefficient ofagreement for nominal scales.educational and psychological effective rule induction.in a.prieditis and s.russelleditorsproceedings ofthe twelfth international conference on machine learningtahoe cityca.san franciscomorgan bayesian method for the induction ofprob-abilistic networks from data.machine vector networks.machine pattern classification.ieeetransactions on information introduction to support vector machinesand other kernel-based learning methods.cambridgeukcambridge university am page what i do programming by demonstration.cambridgemamit guarantees for hierarchical clustering.in j.kivinenand r.h.sloaneditorsproceedings ofthe fifteenth annual conference oncomputational learning defenseprivacy-sensi-tive data miningand random value distortion.in proceedings ofthe workshopon data mining for counter terrorism and securitysan francisco.society forinternational and applied mathematicsphiladelphiapa.demirozg.and by voting feature intervals.in m.van someren and g.widmereditorsproceedings ofthe ninth european conference on machine learningpragueczech rfiand probabilistic theory ofpattern recog-nition.new yorkspringer-verlag.dharv.and methods for transforming corporate data into busi-ness intelligence.upper saddle rivernjprentice hall.diederichj.j.kindermanne.leopoldand attribu-tion with support vector machines.applied experimental comparison ofthree methods for con-structing ensembles ofdecision treesbaggingboostingand randomization.machine multiclass learning problems via error-correcting output codes.journal artificial intelligence acquisition from examples via multiple models.ind.h.fisher jr.editorproceedings ofthe fourteenth international conferenceon machine learningnashvilletn.san franciscomorgan general method for making classifiers cost sensitive.inu.m.fayyads.chaudhuriand d.madiganeditorsproceedings ofthe fifthinternational conference on knowledge discovery and data miningsan diegoca.new and unsupervised dis-cretization ofcontinuous features.in a.prieditis and s.russelleditorsproceedings ofthe twelfth international conference on machine learningtahoecityca.san franciscomorgan am page regressors using boosting techniques.in d.h.fishereditorproceedings ofthe fourteenth international conference on machinelearningnashvilletn.san franciscomorgan representing expected costanalternative to roc representation.in r.ramakrishnans.stolfor.bayardoand i.parsaeditorsproceedings ofthe sixth international conference onknowledge discovery and data miningbostonma.new classification and scene analysis.new yorkjohn wiley.dudar.o.p.e.hartand classificationsecond edition.new yorkjohn wiley.dumaiss.t.j.plattd.heckermanand learning algo-rithms and representations for text categorization.in proceedings ofthe acmseventh international conference on information and knowledge managementbethesdamd.new introduction to the bootstrap.londonchapman and detection theory and roc analysis.series in cognition andperception.new yorkacademic press.fayyadu.m.and discretization ofcontinuous-valued attributes for classification learning.in proceedings ofthe thirteenthinternational joint conference on artificial intelligencechamberyfrance.sanfranciscomorgan massive datasets to science catalogsapplications and challenges.in proceedings ofthe workshop on massivedatasets.washingtondcnrccommittee on applied and theoretical statistics.fayyadu.m.g.piatetsky-shapirop.smythand in knowledge discovery and data mining.menlo parkcaaaaipressmit acquisition via incremental conceptual clustering.machine use ofmultiple measurements in taxonomic problems.annual in contributions to mathematical yorkjohn am page fixe.and j.l.hodges analysisnonparametric discrim-inationconsistency properties.technical report schoolofaviation medicinerandolph fieldtexas.flachp.a.and discovery offirst-orderrules with tertius.machine methods ofoptimizationsecond edition.new yorkjohnwiley.fradkind.and with random projections formachine learning.in l.getoort.e.senatorp.domingosand c.faloutsoseditorsproceedings ofthe ninth international conference on knowledge discovery and data miningwashingtondc.new decision trees and lists.phd dissertationdepartment ofcomputer scienceuniversity ofwaikatonew zealand.franke.and simple approach to ordinal classification.in l.deraedt and p.a.flacheditorsproceedings ofthe twelfth european conferenceon machine accurate rule sets without global opti-mization.in j.shavlikeditorproceedings ofthe fifteenth international conference on machine learningmadisonwi.san franciscomorgan better use ofglobal discretization.in i.bratko and s.dze-roskieditorsproceedings ofthe sixteenth international conference on machinelearningbledslovenia.san franciscomorgan weighted na ve bayes.in u.kj rulffand c.meekeditorsproceedings ofthe nineteenth conference onuncertainty in artificial intelligenceacapulcomexico.san committees for largedatasets.in s.lange and k.satohand c.h.smitheditorsproceedings ofthefifth international conference on discovery sciencel key phrase extraction.in proceedings ofthe sixteenth international joint conference on artificial intelligencestockholmsweden.san franciscomorgan learning for information extraction in informal domains.machine learning am page freundy.and alternating decision-tree learning algorithm.in i.bratko and s.dzeroskieditorsproceedings ofthe sixteenth internationalconference on machine learningbledslovenia.san franciscomorgan with a new boosting algorithm.in l.saittaeditorproceedings ofthe thirteenth international conference onmachine learningbariitaly.san franciscomorgan margin classification using the perceptron approach to polychotomous classification.technicalreportdepartment ofstatisticsstanford universitystanfordca. function approximationa gradient boosting machine.annals algorithm for finding bestmatches in logarithmic expected time.acm transactions on logistic regressionastatistical view ofboosting.annals network classifiers.machine algorithms for finding multiwaysplits for decision trees.in a.prieditis and s.russelleditorsproceedings ofthe twelfth international conference on machine learningtahoe cityca.sanfranciscomorgan robin classification.journal ofmachine rnkrantzj.and n rule learningtowards a better under-standing ofcovering algorithms.machine learning rnkrantzj.and reduced-error pruning.in h.hirsh and w.coheneditorsproceedings ofthe eleventh international conference on machine learningnew brunswicknj.san ofripple-down rules applied tomodeling large databases.journal ofintelligent information foundations ofartificial intelli-gence.san franciscomorgan am page gennarij.h.p.langleyand ofincremental concept for-mation.artificial labeled and unlabeled data for multiclass text catego-rization.in c.sammut and a.hoffmanneditorsproceedings ofthe nine-teenth international conference on machine learningsydneyaustralia.sanfranciscomorgan based feature selectiontheory and algorithms.in r.greiner and d.schuurmanseditorsproceed-ings ofthe twenty-first international conference on machine learningbanffalbertacanada.new with prior knowledge.in j.nealon andj.hunteditorsresearch and development in expert systems xiii.cambridgeuksges the utility ofcate-gories.in proceedings ofthe annual conference ofthe cognitive science societyirvineca.hillsdalenjlawrence algorithms in searchoptimizationand machine tests a practical guide to resampling methods for testinghypotheses.springer-verlagnew yorkny.grossmand.and bayesian network classifiers bymaximizing conditional likelihood.in r.greiner and d.schuurmanseditorsproceedings ofthe twenty-first international conference on machine learningbanffalbertacanada.new mining a hands-on approach for business professionals.uppersaddle rivernjprentice hall.guoy.and model selection for beliefnet struc-tures.department ofcomputing ofalbertacanada.guyoni.j.westons.barnhilland selection for cancer classification using support vector machines.machine feature selection for discrete and numeric classmachine learning.in p.langleyeditorproceedings ofthe seventeenth inter-national conference on machine learningstanfordca.san franciscomorgan am page hallm.g.holmesand rule sets from model trees.inn.y.fooeditorproceedings ofthe twelfth australian joint conference on artificial mining concepts and techniques.san franciscomorgan kaufmann.handd.j.h.mannilaand ofdata mining.cambridgemamit algorithms.new yorkjohn wiley.hastiet.and by pairwise coupling.annals elements ofstatistical learning.new yorkspringer-verlag.heckermand.d.geigerand bayesian networksthe combination ofknowledge and statistical data.machine best possible heuristic for the k-centerproblem.mathematics ofoperations selection via the discovery ofsimple classification rules.in g.e.lasker and x.liueditorsproceedings ofthe international symposium on intelligent data analysis.baden-badengermanyinternational institute for advanced studies in systems researchand alter-nating decision trees.in t.elomaah.mannilaand h.toivoneneditorsproceedings ofthe thirteenth european conference on machine simple classification rules perform well on most commonlyused datasets.machine information extraction patterns from examples.ins.wertmere.riloffand g.schelereditorsconnectioniststatisticaland sym-bolic approaches to learning for natural language processingspringer forecasting assistant.ieee transactions on power decision treesremoving outliers from databases.in u.m.fayyad and r.uthurusamyeditorsproceedings ofthe first am page conference on knowledge discovery and data mining.montrealcanada.menlo parkcaaaai to the data mining process.phd dissertationcom-puter science departmentstanford universitystanfordca.johng.h.and continuous distributions in bayesianclassifiers.in p.besnard and s.hankseditorsproceedings ofthe eleventh conference on uncertainty in artificial intelligencemontrealcanada.sanfranciscomorgan features and the subset selection problem.in h.hirsh and w.coheneditorsproceedings oftheeleventh international conference on machine learningnew brunswicknj.san franciscomorgan empirical bayes approach to nonparametric two-way classi-fication.in h.solomoneditorstudies in item analysis and prediction.paloaltocastanford university press.kassr.and reference bayesian test for nested hypothesesand its relationship to the schwarz criterion.journal ofthe american statistical to platt s smo algorithm for svm classifier design.neural ofnumeric attributes.in w.swartouteditorproceedings ofthe tenth national conference on artificial intelligencesan joseca.menlo parkcaaaai representative exemplars ofconceptsan initial case study.in p.langleyeditorproceedings ofthe fourth machine learning workshopirvineca.san franciscomorgan data warehouse toolkit.new yorkjohn wiley.kirak.and practical approach to feature selection.in d.sleeman and p.edwardseditorsproceedings ofthe ninth international workshop on machine learningaberdeenscotland.san set search algorithms.in c.h.cheneditorpattern recog-nition and signal processing.the netherlandssijthoffan act am page study ofcross-validation and bootstrap for accuracy estimationand model selection.in proceedings ofthe fourteenth international joint conference on artificial intelligencemontrealcanada.san power ofdecision tables.in n.lavrac and s.wrobeleditorsproceedings ofthe eighth european conference on machine learningir up the accuracy ofna ve bayes classifiersa decision treehybrid.in e.simoudisj.w.hanand u.fayyadeditorsproceedings ofthesecond international conference on knowledge discovery and data miningportlandor.menlo parkcaaaai for feature subset decision trees with majority votes.in d.fishereditorproceedings ofthe fourteenth international conference onmachine learningnashvilletn.san franciscomorgan learningspecial issue on appli-cations ofmachine learning and the knowledge discovery and entropy-based discretization ofcontinuous features.in e.simoudisj.w.hanand u.fayyadeditorsproceedings ofthe second international conference on knowledge discovery anddata miningportlandor.menlo parkcaaaai dynamic adaptation ofad trees for efficientmachine learning on large data sets.in p.langleyeditorproceedings oftheseventeenth international conference on machine learningstanfordca.sanfranciscomorgan biases in estimating multivalued attributes.in proceedingsofthe fourteenth international joint conference on artificial intelligencemontrealcanada.san franciscomorgan verification as a one-class classificationproblem.in r.greiner and d.schuurmanseditorsproceedings ofthe twenty-first international conference on machine learningbanffalbertacanada.new learning for the detection ofoil spills in satellite radar images.machine am page kushmerickn.d.s.weldand induction for information extraction.in proceedings ofthe fifteenth international joint conference on artificial intelligencenagoyajapan.san model trees.in n.lavracd.gambergerl.todorovskiand h.blockeeleditorsproceedings ofthe four-teenth european conference on machine ofmachine learning.san franciscomorgan kaufmann.langleyp.and ofselective bayesian classifiers.in r.l.de mantaras and d.pooleeditorsproceedings ofthe tenth conference onuncertainty in artificial intelligenceseattlewa.san franciscomorgan ofmachine learning and ruleinduction.communications ofthe analysis ofbayesian classifiers.inw.swartouteditorproceedings ofthe tenth national conference on artificialintelligencesan joseca.menlo parkcaaaai least-squares problems.philadelphiasiam publications.le cessies.and j.c.van estimators in logistic regres-sion.applied reasoning and kolmogorov complex-ity.journal computer and system wish is my command programming by example.san franciscomorgan quickly when irrelevant attributes abounda newlinear-threshold algorithm.machine bounds and logarithmic linear-threshold learning algorithms.phd dissertationuniversity ofcaliforniasanta cruzca.liuh.and probabilistic approach to feature selectiona filtersolution.in l.saittaeditorproceedings ofthe thirteenth international conference on machine learningbariitaly.san franciscomorgan selection via discretization.ieee transactions on knowledgeand data am page research models a guide to classificationcatalogingand com-puters.new yorkoxford university press.marillt.and the effectiveness ofreceptors in recognitionsystems.ieee transactions on information learningnearest neighbour with generalisation.msc thesisdepartment ofcomputer scienceuniversity ofwaikatonewzealand.mccalluma.and comparison ofevent models for na ve bayes text classification.in proceedings ofthe workshop on learning for text categorizationmadisonwi.menlo parkcaaaai fast scalable classifier for datamining.in apersp.m.bouzeghouband g.gardarinproceedings ofthe fifthinternational conference on extending database technologyavignonfrance.new yorkspringer-verlag.melvillep.and diversity in ensembles using artificialdata.information by being told and learning fromexamplesan experimental comparison ofthe two methods ofknowledgeacquisition in the context ofdeveloping an expert system for soybean diseasediagnosis.international journal ofpolicy analysis and information ofcomputer-aided concept formation.in j.r.quinlaneditorapplications ofexpert learning.new yorkmcgraw hill.mitchellt.m.r.caruanad.freitagj.mcdermottand with a learning personal assistant.communications ofthe memory-based learning for robot control.phd dissertationcomputer laboratoryuniversity ofcambridgeuk. anchors hierarchyusing the triangle inequality to survive high-dimensional data.in c.boutilier and m.goldszmidteditorsproceedings ofthe sixteenth conference on uncertainty in artificial intelligencestanfordca.san franciscomorgan am page moorea.w.and algorithms for minimizing cross vali-dation error.in w.w.cohen and h.hirsheditorsproceedings ofthe eleventhinternational conference on machine learningnew brunswicknj.san franciscomorgan sufficient statistics for efficient machine learning with largedatasets.journal artificial intelligence k-means with efficient estimation ofthe number ofclusters.in p.langleyeditorproceedings oftheseventeenth international conference on machine learningstanfordca.sanfranciscomorgan for the generalization information extraction to aid the dis-covery ofprediction rules from texts.proceedings ofthe workshop on textmining at the sixth international conference on knowledge discovery and package for the social sciences.new yorkmcgraw hill.nigamk.and the effectiveness and applicability ofco-training.proceedings ofthe ninth international conference on information andknowledge managementmcleanva.new classification fromlabeled and unlabeled documents using em.machine machines.new yorkmcgraw algorithms with neural network behavior.journalofcomplex iterative tasks with programming by demonstration.phd dissertationdepartment ofcomputer scienceuniversity ofwaikatonew zealand.piatetsky-shapirog.and discovery in data-bases.menlo parkcaaaai pressmit training ofsupport vector machines using sequential minimaloptimization.in b.sch lkopfc.burgesand a.smolaeditorsadvances inkernel methods support vector learning.cambridgemamit press.provostf.and and visualization ofclassifier perform-ancecomparison under imprecise class and cost distributions.in am page heckermanh.mannilad.pregibonand r.uthurusamyeditorsproceedings ofthe third international conference on knowledge discovery anddata mininghuntington beachca.menlo parkcaaaai preparation for data mining.san franciscomorgan ofdecision trees.machine with continuous classes.in n.adams and l.sterlingeditorsproceedings ofthe fifth australian joint conference on artificial intel-ligencehobarttasmania.singaporeworld programs for machine learning.san franciscomorgan kaufmann.renniej.d.m.l.shihj.teevanand the poor assump-tions ofna ve bayes text classifiers.in t.fawcett and n.mishraeditorsproceedings ofthe twentieth international conference on machine learningwashingtondc.menlo parkcaaaai output codes for local learners.inc.nedellec and c.rouveirdeditorsproceedings ofthe european conference onmachine up the situated cognition challenge with ripple-down rules.international journal ofhuman-computer recognition and neural networks.cambridgeukcambridge university minimum description length principle.in s.kotz and n.l.johnsoneditorsencyclopedia ofstatistical regression and outlier detection.newyorkjohn wiley.sahamim.s.dumaisd.heckermanand bayesian approachto filtering junk email.in proceedings ofthe workshop on learningfor text categorizationmadisonwi.menlo parkcaaaai in the real world. machine nearest hyperrectangle learning method.machine the marginanew explanation for the effectiveness ofvoting methods.in am page editorproceedings ofthe fourteenth international conference on machinelearningnashvilletn.san franciscomorgan association rules that trade support optimally against con-fidence.in l.de raedt and a.siebeseditorsproceedings ofthe fifth europeanconference on principles ofdata mining and knowledge lkopfb.and with kernels support vector machinesregularizationoptimizationand beyond.cambridgemamit press.sch lkopfb.p.bartletta.j.smolaand the tubea new support vector regression algorithm.advances in neural informationprocessing learning in automated text categorization.acm computing surveys with labeled and unlabeled data.technical reportinstitute for adaptive and neural computationuniversity to make stacking better and faster while also taking care ofan unknown weakness.proceedings ofthe nineteenth internationalconference on machine learningsydneyaustralia.san j.f evaluation ofgrading classifiers.in f.hoffmannd.j.handn.m.adamsd.h.fisherand g.guimar eseditorsproceedings ofthe fourth international conference on advances in intelligentdata scalable parallel classifier fordata mining.in t.m.vijayaramana.p.buchmannc.mohanand n.l.sardaeditorsproceedings ofthe second international conference on very large databasesmumbai franciscomorgan n methods for pattern analysis.cambridgeukcambridge university press.smolaa.j.and b.sch tutorial on support vector regression.statistics and a con-ceptual dictionary.proceedings ofthe fourteenth international joint conferenceon artificial intelligencemontrealcanada.menlo parkcaaaai the theory ofscales am page stonep.and systemsa survey from a machine learn-ing perspective.autonomous the accuracy ofdiagnostic instance-weighting method to induce cost-sensitive trees.ieeetransactions on knowledge and data generalizationwhen does it work? in proceedings ofthe fifteenth international joint conference on artificial intelligencenagoyajapan.san franciscomorgan bagged and dagged models.in d.h.fishereditorproceedings ofthe fourteenth international conference on machine learningnashvilletn.san franciscomorgan to extract key phrases from text.technical report for information technologynational research council ofcanadaottawacanada.u.s.house ofrepresentatives subcommittee on on avia-tion security with a focus on passenger profilingfebruary algorithms as a tool for feature selectionin machine learning.in proceedings ofthe international conference on toolswith artificial intelligence.arlingtonvaieee computer society nature ofstatistical learning theorysecond edition.new yorkspringer-verlag.wangy.and ofmodel trees for predicting continu-ous classes.in m.van someren and g.widmereditorsproceedings oftheposter papers ofthe european conference on machine learning.pragueuni-versity ofeconomicsfaculty ofinformatics and for optimal probability prediction.in c.sammut and a.hoffmanneditorsproceedings ofthe nineteenth international conference onmachine learningsydneyaustralia.san franciscomorgan technique for combining boosting and wagging.machine am page webbg.i.j.boughtonand so na ve bayesaggregating one-dependence estimators.machine house.reviewthe web magazine ofthe interactivetelecommunications program ofnew york university.march.weiserm.and coming age ofcalm technology.in p.j.denning and r.m.metcalfeeditorsbeyond calculation the next fifty years.new data mining a practical guide.sanfranciscomorgan kaufmann.wettschereckd.and experimental comparison ofthenearest-neighbor and nearest-hyperrectangle algorithms.machine to probability and statistics.depart-ment ofstatisticsuniversity ofaucklandnew mining.in m.p.singheditorpractical handbook ofinternet computing.boca ratonflcrc press.witteni.h.z.braym.mahouiand mininga new fron-tier for lossless compression.in j.a.storer and m.cohneditorsproceedingsofthe data compression conferencesnowbirdut.los alamitoscaieeecomputer society gigabytes compressing and indexing documents and imagessecond edition.san generalization.neural k-interval discretization for na ve bayesclassifiers.in l.de raedt and p.flacheditorsproceedings ofthe twelfth european conference on machine data management alternatives to support data miningheterogeneous logs for computer network security.in proceedings ofthe workshop on data mining for counter terrorism and securitysan francisco.society for international and applied mathematicsphiladelphiapa.zhengz.and learning ofbayesian rules.machine am page am page aactivation logistic methods.seeimplementation real-world schemesadversarial data degree in information criterion ingenuity centre for logistic alsoimplementation real-world schemesassociation rule alsoalgorithms-basic methodsbayesian network in tree examples into partial in in alsolearningalgorithmslinear in learning rule formation-incremental alsoindividual subject headings.all-dimensions decision purchasing am page detection under the curve files the performance oflearning ofkey rules learners in discretization.seediscretizingnumeric evaluation methods in filters in file format.seearff columns in ofvalues in arff evaluation methods in the attribute methods in subset evaluators in components to attribute weighting under the data decision am page over class with methods.seealgorithms-basic methodsbatch classifier.seena ve bayesbayesian ve bayes learning by conditionalindependence augmented na ve network learning option scoring s ish learning is data in assisted passenger pre-screening and regression alsonominalattributescategory am page rule hierarchy in and regression tree decision trees for choosing from association decision subtree rule with in algorithms.see learning algorithmsclassifier errors in text files into two world distance many algorithms in for in multiple logistic with output model am page value data mining learning assisted passenger pre-screeningsystem network software.seeweka description alsoknowledgerepresentationconditional likelihood for scoring resolution quadratic lens alsonumericattributescontinuous discrete to numeric resampled learning in performance roc am page margin distribution in support and farmers alsoautomatic datacleansingdata engineering.seeengineering input andoutputdata ownership to branching s user classifer acyclic numeric discrete to numeric normal experiments in am page diagnosis average and machine input and multiple numeric alsoindividual subject headingsentity alsonominalattributesenumerating the concept output oferrors.seecost oferrorsdecision essay towards solving a problem in thedoctrine ofchancesan data mining alsocost and components in problemscontact lens performance negotiations all success am page the processing over an evaluation format alsolearning algorithmsmetalearning learning attribute instance negative positive positive positive representation attributesfeature alsoattributeselectionfeedforward support and detection and slick maintenance ofelectromechanical format in algorithms in attribute instance am page stagewise additive in ingarbage out.seecost oferrorsdatacleaningerror rategaussian-distribution kernel as concept distance search algorithm search to know your bar in margin oftextbook detection rule literary markup language real-world am page real-world schemescontinuednumeric alsoindividual subject headingsinaccurate alsocost oferrorsdata cleaningerror rateincremental learning in reduced-error logic usage.seeimplementation real-world schemesinferring rudimentary loss the alsoengineering input and outputdata to know your in nearest distance noisy learning filters in care detection combined date and time distance-based email am page s three laws ofplanetary density logistic flow alsoassociationrulesclassification alsoclassification alsoclusteringdecision alsodecision treeinstance-based involving with for numeric negotiations ofdiminishing classifiers in algorithms in margin class vector threshold am page weighted linear weighted na ve model basket basket and margin absolute algorithms in description length classifiers in the tree subtree is am page alternating decision learning na ve linear linear decision decision ve for attribute augmented na ve can go generalized networks in oflearning to numeric class class expected class discrete attributes alsodiscretizingnumeric attributesinstance-based stagewise additive weighted linear am page prediction alsomodel s slick performance in alsoengineering input and outputknowledge alsoknowledge stagewise additive regression in decision instance learning zip am page alsoevaluationpredicting accuracy in maintenance components ofmultiple em cost density workbenchprogramming by k-interval rule loss basis function basis function forest metalearner in basis function basis function applications.seefielded applicationsreal-life implementations.seeimplementation real-world neural feature am page absolute error squared subtree rule mean-squared relative squared squared error ofdecision decision involving with problems.seeexample problemssampling with attribute attribute deviation engine methods in na ve boosting-like minimal optimization am page linear evaluators in holdout weighted linear programs.seeweka workbenchsortingavoiding instance in search loss error deviation reduction deviations from the conversion in learning by conditional s distribution with k degrees s in evaluators in attribute filters in filters in instance filters in am page vector vector machine vector machines with vector classifier vector data input augmented na ve to attribute detection average in induction ofdecision problems.seeexample problemstp and transformationstransforming a multiclass problem into a two-class treelogistic alsomodel treenumeric augmented na ve bayes classifier in negative positive positive positive mixture data decision am page for and attribute filters in instance filters in training large very simple classification rules perform wellon most commonly used datasets components in classifier threshold feature problem rules space a classification a clustering data to arff matrix attributes rules a numeric prediction alsocommand-line interfaceelementary learning machine application text files intotwo alsoexplorerimplementing flow am page wide web classifiers in loss pointinherently am page about the authorsian h.witten is a professor ofcomputer science at the university ofwaikato in new zealand.he is a fellow ofthe association for computingmachinery and the royal society ofnew zealand.he received the ifipnamur awarda biennial honor accorded for outstanding contribution withinternational impact to the awareness ofsocial implications ofinformation andcommunication technology.his books include managing andhow to build a digital he has written many journal articlesand conference papers.eibe frankis a senior lecturer in computer science at the university ofwaikato.he has published extensively in the area ofmachine learning and sits on the edi-torial boards ofthe machine learning journaland the journal ofartificial intel-ligence research.he has also served on the programming committees ofmanydata mining and machine learning conferences.as one ofthe core developersofthe weka machine learning software that accompanies this bookhe enjoysmaintaining and improving am page