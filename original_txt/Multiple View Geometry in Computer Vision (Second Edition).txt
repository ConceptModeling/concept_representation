This page intentionally left blank

Multiple View Geometry in Computer Vision

Second Edition

Richard Hartley

Australian National University,

Canberra, Australia

Andrew Zisserman
University of Oxford, UK

cambridge  university  press
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo

Cambridge  University  Press
The Edinburgh Building, Cambridge cb2 2ru, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9780521540513

© Cambridge University Press 2000, 2003

This publication is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.

First published in print format 

2004

isbn-13    978-0-511-18618-9
isbn-10    0-511-18618-5

eBook (EBL)

eBook (EBL)

isbn-13    978-0-521-54051-3
isbn-10    0-521-54051-8

paperback

paperback

Cambridge University Press has no responsibility for the persistence or accuracy of urls
for external or third-party internet websites referred to in this publication, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.

Dedication

This book is dedicated to Joe Mundy whose vision and constant search for new ideas
led us into this ﬁeld.

Contents

Foreword
Preface
1

Introduction – a Tour of Multiple View Geometry
Introduction – the ubiquitous projective geometry
1.1
Camera projections
1.2
Reconstruction from more than one view
1.3
Three-view geometry
1.4
Four view geometry and n-view reconstruction
1.5
Transfer
1.6
Euclidean reconstruction
1.7
Auto-calibration
1.8
The reward I : 3D graphical models
1.9
1.10 The reward II: video augmentation

page xi
xiii
1
1
6
10
12
13
14
16
17
18
19

PART 0: The Background: Projective Geometry, Transformations and Esti-
mation

23
24
25
25
26
32
37
44
46
47
58
61
62
65
65
66

2

3

Planar geometry
The 2D projective plane
Projective transformations
A hierarchy of transformations
The projective geometry of 1D
Topology of the projective plane
Recovery of afﬁne and metric properties from images

Outline
Projective Geometry and Transformations of 2D
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8 More properties of conics
2.9
2.10 Closure
Projective Geometry and Transformations of 3D
3.1
3.2

Points and projective transformations
Representing and transforming planes, lines and quadrics

Fixed points and lines

v

vi

4

5

Contents

Twisted cubics
The hierarchy of transformations
The plane at inﬁnity
The absolute conic
The absolute dual quadric
Closure

3.3
3.4
3.5
3.6
3.7
3.8
Estimation – 2D Projective Transformations
4.1
4.2
4.3
4.4
4.5
4.6
4.7
4.8
4.9
Algorithm Evaluation and Error Analysis
5.1
5.2
5.3 Monte Carlo estimation of covariance
5.4

The Direct Linear Transformation (DLT) algorithm
Different cost functions
Statistical cost functions and Maximum Likelihood estimation
Transformation invariance and normalization
Iterative minimization methods
Experimental comparison of the algorithms
Robust estimation
Automatic computation of a homography
Closure

Bounds on performance
Covariance of the estimated transformation

Closure

PART I: Camera Geometry and Single View Geometry

6

7

Finite cameras
The projective camera
Cameras at inﬁnity
Other camera models
Closure

Outline
Camera Models
6.1
6.2
6.3
6.4
6.5
Computation of the Camera Matrix P
7.1
7.2
7.3
7.4
7.5

Basic equations
Geometric error
Restricted camera estimation
Radial distortion
Closure

8 More Single View Geometry

8.1
8.2
8.3
8.4
8.5

Action of a projective camera on planes, lines, and conics
Images of smooth surfaces
Action of a projective camera on quadrics
The importance of the camera centre
Camera calibration and the image of the absolute conic

75
77
79
81
83
85
87
88
93
102
104
110
115
116
123
127
132
132
138
149
150

151
152
153
153
158
166
174
176
178
178
180
184
189
193
195
195
200
201
202
208

Contents

Vanishing points and vanishing lines
Afﬁne 3D measurements and reconstruction
Determining camera calibration K from a single view
Single view reconstruction

8.6
8.7
8.8
8.9
8.10 The calibrating conic
8.11 Closure

PART II: Two-View Geometry

9

Outline
Epipolar Geometry and the Fundamental Matrix
9.1
9.2
9.3
9.4
9.5
9.6
9.7

Epipolar geometry
The fundamental matrix F
Fundamental matrices arising from special motions
Geometric representation of the fundamental matrix
Retrieving the camera matrices
The essential matrix
Closure

10 3D Reconstruction of Cameras and Structure

10.1 Outline of reconstruction method
10.2 Reconstruction ambiguity
10.3 The projective reconstruction theorem
10.4 Stratiﬁed reconstruction
10.5 Direct reconstruction – using ground truth
10.6 Closure

11 Computation of the Fundamental Matrix F

11.1 Basic equations
11.2 The normalized 8-point algorithm
11.3 The algebraic minimization algorithm
11.4 Geometric distance
11.5 Experimental evaluation of the algorithms
11.6 Automatic computation of F
11.7 Special cases of F-computation
11.8 Correspondence of other entities
11.9 Degeneracies
11.10 A geometric interpretation of F-computation
11.11 The envelope of epipolar lines
11.12 Image rectiﬁcation
11.13 Closure

12 Structure Computation
12.1 Problem statement
12.2 Linear triangulation methods
12.3 Geometric error cost function
12.4 Sampson approximation (ﬁrst-order geometric correction)

vii

213
220
223
229
231
233

237
238
239
239
241
247
250
253
257
259
262
262
264
266
267
275
276
279
279
281
282
284
288
290
293
294
295
297
298
302
308
310
310
312
313
314

viii

Contents

12.5 An optimal solution
12.6 Probability distribution of the estimated 3D point
12.7 Line reconstruction
12.8 Closure

13 Scene planes and homographies

13.1 Homographies given the plane and vice versa
13.2 Plane induced homographies given F and image correspondences
13.3 Computing F given the homography induced by a plane
13.4 The inﬁnite homography H∞
13.5 Closure

14 Afﬁne Epipolar Geometry

14.1 Afﬁne epipolar geometry
14.2 The afﬁne fundamental matrix
14.3 Estimating FA from image point correspondences
14.4 Triangulation
14.5 Afﬁne reconstruction
14.6 Necker reversal and the bas-relief ambiguity
14.7 Computing the motion
14.8 Closure

PART III: Three-View Geometry

Outline

15 The Trifocal Tensor

15.1 The geometric basis for the trifocal tensor
15.2 The trifocal tensor and tensor notation
15.3 Transfer
15.4 The fundamental matrices for three views
15.5 Closure

16 Computation of the Trifocal Tensor T

16.1 Basic equations
16.2 The normalized linear algorithm
16.3 The algebraic minimization algorithm
16.4 Geometric distance
16.5 Experimental evaluation of the algorithms
16.6 Automatic computation of T
16.7 Special cases of T -computation
16.8 Closure

PART IV: N-View Geometry

Outline

17 N-Linearities and Multiple View Tensors

17.1 Bilinear relations
17.2 Trilinear relations

315
321
321
323
325
326
329
334
338
340
344
344
345
347
353
353
355
357
360

363
364
365
365
376
379
383
387
391
391
393
395
396
399
400
404
406

409
410
411
411
414

Contents

Intersections of four planes

17.3 Quadrilinear relations
17.4
17.5 Counting arguments
17.6 Number of independent equations
17.7 Choosing equations
17.8 Closure

18 N-View Computational Methods

18.1 Projective reconstruction – bundle adjustment
18.2 Afﬁne reconstruction – the factorization algorithm
18.3 Non-rigid factorization
18.4 Projective factorization
18.5 Projective reconstruction using planes
18.6 Reconstruction from sequences
18.7 Closure

19 Auto-Calibration
Introduction

19.1
19.2 Algebraic framework and problem statement
19.3 Calibration using the absolute dual quadric
19.4 The Kruppa equations
19.5 A stratiﬁed solution
19.6 Calibration from rotating cameras
19.7 Auto-calibration from planes
19.8 Planar motion
19.9 Single axis rotation – turntable motion
19.10 Auto-calibration of a stereo rig
19.11 Closure

20 Duality

20.1 Carlsson–Weinshall duality
20.2 Reduced reconstruction
20.3 Closure

21 Cheirality

21.1 Quasi-afﬁne transformations
21.2 Front and back of a camera
21.3 Three-dimensional point sets
21.4 Obtaining a quasi-afﬁne reconstruction
21.5 Effect of transformations on cheirality
21.6 Orientation
21.7 The cheiral inequalities
21.8 Which points are visible in a third view
21.9 Which points are in front of which
21.10 Closure

ix

418
421
422
428
431
432

434
434
436
440
444
447
452
456

458
458
459
462
469
473
481
485
486
490
493
497

502
502
508
513

515
515
518
519
520
521
523
525
528
530
531

x

Contents

22 Degenerate Conﬁgurations
22.1 Camera resectioning
22.2 Degeneracies in two views
22.3 Carlsson–Weinshall duality
22.4 Three-view critical conﬁgurations
22.5 Closure

PART V : Appendices
Appendix 1 Tensor Notation
Appendix 2 Gaussian (Normal) and χ2 Distributions
Appendix 3 Parameter Estimation
Appendix 4 Matrix Properties and Decompositions
Appendix 5 Least-squares Minimization
Appendix 6 Iterative Estimation Methods
Appendix 7 Some Special Plane Projective Transformations
Bibliography
Index

533
533
539
546
553
558

561
562
565
568
578
588
597
628
634
646

Foreword

By Olivier Faugeras

Making a computer see was something that leading experts in the ﬁeld of Artiﬁcial
Intelligence thought to be at the level of difﬁculty of a summer student’s project back
in the sixties. Forty years later the task is still unsolved and seems formidable. A
whole ﬁeld, called Computer Vision, has emerged as a discipline in itself with strong
connections to mathematics and computer science and looser connections to physics,
the psychology of perception and the neuro sciences.

One of the likely reasons for this half-failure is the fact that researchers had over-
looked the fact, perhaps because of this plague called naive introspection, that percep-
tion in general and visual perception in particular are far more complex in animals and
humans than was initially thought. There is of course no reason why we should pattern
Computer Vision algorithms after biological ones, but the fact of the matter is that

(i) the way biological vision works is still largely unknown and therefore hard to

emulate on computers, and

(ii) attempts to ignore biological vision and reinvent a sort of silicon-based vision

have not been so successful as initially expected.

Despite these negative remarks, Computer Vision researchers have obtained some

outstanding successes, both practical and theoretical.

On the side of practice, and to single out one example, the possibility of guiding vehi-
cles such as cars and trucks on regular roads or on rough terrain using computer vision
technology was demonstrated many years ago in Europe, the USA and Japan. This
requires capabilities for real-time three-dimensional dynamic scene analysis which are
quite elaborate. Today, car manufacturers are slowly incorporating some of these func-
tions in their products.

On the theoretical side some remarkable progress has been achieved in the area of
what one could call geometric Computer Vision. This includes the description of the
way the appearance of objects changes when viewed from different viewpoints as a
function of the objects’ shape and the cameras parameters. This endeavour would not
have been achieved without the use of fairly sophisticated mathematical techniques en-
compassing many areas of geometry, ancient and novel. This book deals in particular
with the intricate and beautiful geometric relations that exist between the images of ob-
jects in the world. These relations are important to analyze for their own sake because

xi

xii

0 Foreword

this is one of the goals of science to provide explanations for appearances; they are also
important to analyze because of the range of applications their understanding opens up.
The book has been written by two pioneers and leading experts in geometric Com-
puter Vision. They have succeeded in what was something of a challenge, namely to
convey in a simple and easily accessible way the mathematics that is necessary for
understanding the underlying geometric concepts, to be quite exhaustive in the cover-
age of the results that have been obtained by them and other researchers worldwide, to
analyze the interplay between the geometry and the fact that the image measurements
are necessarily noisy, to express many of these theoretical results in algorithmic form
so that they can readily be transformed into computer code, and to present many real
examples that illustrate the concepts and show the range of applicability of the theory.
Returning to the original holy grail of making a computer see we may wonder
whether this kind of work is a step in the right direction.
I must leave the readers
of the book to answer this question, and be content with saying that no designer of
systems using cameras hooked to computers that will be built in the foreseeable future
can ignore this work. This is perhaps a step in the direction of deﬁning what it means
for a computer to see.

Preface

Over the past decade there has been a rapid development in the understanding and mod-
elling of the geometry of multiple views in computer vision. The theory and practice
have now reached a level of maturity where excellent results can be achieved for prob-
lems that were certainly unsolved a decade ago, and often thought unsolvable. These
tasks and algorithms include:
• Given two images, and no other information, compute matches between the images,
and the 3D position of the points that generate these matches and the cameras that
generate the images.
• Given three images, and no other information, similarly compute the matches be-
tween images of points and lines, and the position in 3D of these points and lines
and the cameras.
• Compute the epipolar geometry of a stereo rig, and trifocal geometry of a trinocular
rig, without requiring a calibration object.
• Compute the internal calibration of a camera from a sequence of images of natural
scenes (i.e. calibration “on the ﬂy”).

The distinctive ﬂavour of these algorithms is that they are uncalibrated — it is not
necessary to know or ﬁrst need to compute the camera internal parameters (such as the
focal length).

Underpinning these algorithms is a new and more complete theoretical understand-
ing of the geometry of multiple uncalibrated views: the number of parameters involved,
the constraints between points and lines imaged in the views; and the retrieval of cam-
eras and 3-space points from image correspondences. For example, to determine the
epipolar geometry of a stereo rig requires specifying only seven parameters, the camera
calibration is not required. These parameters are determined from the correspondence
of seven or more image point correspondences. Contrast this uncalibrated route, with
the previous calibrated route of a decade ago: each camera would ﬁrst be calibrated
from the image of a carefully engineered calibration object with known geometry. The
calibration involves determining 11 parameters for each camera. The epipolar geome-
try would then have been computed from these two sets of 11 parameters.

This example illustrates the importance of the uncalibrated (projective) approach –
using the appropriate representation of the geometry makes explicit the parameters

xiii

xiv

Preface

that are required at each stage of a computation. This avoids computing parameters
that have no effect on the ﬁnal result, and results in simpler algorithms.
It is also
worth correcting a possible misconception. In the uncalibrated framework, entities (for
instance point positions in 3-space) are often recovered to within a precisely deﬁned
ambiguity. This ambiguity does not mean that the points are poorly estimated.

More practically, it is often not possible to calibrate cameras once-and-for-all; for
instance where cameras are moved (on a mobile vehicle) or internal parameters are
changed (a surveillance camera with zoom). Furthermore, calibration information is
simply not available in some circumstances. Imagine computing the motion of a cam-
era from a video sequence, or building a virtual reality model from archive ﬁlm footage
where both motion and internal calibration information are unknown.

The achievements in multiple view geometry have been possible because of develop-
ments in our theoretical understanding, but also because of improvements in estimating
mathematical objects from images. The ﬁrst improvement has been an attention to the
error that should be minimized in over-determined systems – whether it be algebraic,
geometric or statistical. The second improvement has been the use of robust estimation
algorithms (such as RANSAC), so that the estimate is unaffected by “outliers” in the
data. Also these techniques have generated powerful search and matching algorithms.
Many of the problems of reconstruction have now reached a level where we may

claim that they are solved. Such problems include:

(i) Estimation of the multifocal tensors from image point correspondences, par-
ticularly the fundamental matrix and trifocal tensors (the quadrifocal tensor
having not received so much attention).

(ii) Extraction of the camera matrices from these tensors, and subsequent projective

reconstruction from two, three and four views.

Other signiﬁcant successes have been achieved, though there may be more to learn
about these problems. Examples include:

(i) Application of bundle adjustment to solve more general reconstruction prob-

lems.

(ii) Metric (Euclidean) reconstruction given minimal assumptions on the camera

matrices.

(iii) Automatic detection of correspondences in image sequences, and elimination

of outliers and false matches using the multifocal tensor relationships.

Roadplan. The book is divided into six parts and there are seven short appendices.
Each part introduces a new geometric relation: the homography for background, the
camera matrix for single view, the fundamental matrix for two views, the trifocal tensor
for three views, and the quadrifocal tensor for four views.
In each case there is a
chapter describing the relation, its properties and applications, and a companion chapter
describing algorithms for its estimation from image measurements. The estimation
algorithms described range from cheap, simple, approaches through to the optimal
algorithms which are currently believed to be the best available.

Preface

xv

Part 0: Background. This part is more tutorial than the others. It introduces the
central ideas in the projective geometry of 2-space and 3-space (for example
ideal points, and the absolute conic); how this geometry may be represented,
manipulated, and estimated; and how the geometry relates to various objectives
in computer vision such as rectifying images of planes to remove perspective
distortion.

Part 1: Single view geometry. Here the various cameras that model the perspective
projection from 3-space to an image are deﬁned and their anatomy explored.
Their estimation using traditional techniques of calibration objects is described,
as well as camera calibration from vanishing points and vanishing lines.

Part 2: Two view geometry. This part describes the epipolar geometry of two
cameras, projective reconstruction from image point correspondences, methods
of resolving the projective ambiguity, optimal triangulation, transfer between
views via planes.

Part 3: Three view geometry. Here the trifocal geometry of three cameras is de-
scribed, including transfer of a point correspondence from two views to a third,
and similarly transfer for a line correspondence; computation of the geometry
from point and line correspondences, retrieval of the camera matrices.

Part 4: N-views. This part has two purposes. First, it extends three view geometry
to four views (a minor extension) and describes estimation methods applica-
ble to N-views, such as the factorization algorithm of Tomasi and Kanade for
computing structure and motion simultaneously from multiple images. Sec-
ond, it covers themes that have been touched on in earlier chapters, but can
be understood more fully and uniformly by emphasising their commonality.
Examples include deriving multi-linear view constraints on correspondences,
auto-calibration, and ambiguous solutions.

Appendices. These describe further background material on tensors, statistics, pa-
rameter estimation, linear and matrix algebra, iterative estimation, the solution
of sparse matrix systems, and special projective transformations.

Acknowledgements. We have beneﬁted enormously from ideas and discussions with
our colleagues: Paul Beardsley, Stefan Carlsson, Olivier Faugeras, Andrew Fitzgibbon,
Jitendra Malik, Steve Maybank, Amnon Shashua, Phil Torr, Bill Triggs.

If there are only a countable number of errors in this book then it is due to Antonio
Criminisi, David Liebowitz and Frederik Schaffalitzky who have with great energy and
devotion read most of it, and made numerous suggestions for improvements. Similarly
both Peter Sturm and Bill Triggs have suggested many improvements to various chap-
ters. We are grateful to other colleagues who have read individual chapters: David
Capel, Lourdes de Agapito Vicente, Bob Kaucic, Steve Maybank, Peter Tu.

We are particularly grateful to those who have provided multiple ﬁgures: Paul Beard-
sley, Antonio Criminisi, Andrew Fitzgibbon, David Liebowitz, and Larry Shapiro; and
for individual ﬁgures from: Martin Armstrong, David Capel, Lourdes de Agapito Vi-
cente, Eric Hayman, Phil Pritchett, Luc Robert, Cordelia Schmid, and others who are
explicitly acknowledged in ﬁgure captions.

xvi

Preface

At Cambridge University Press we thank David Tranah for his constant source of

advice and patience, and Michael Behrend for excellent copy editing.

A small number of minor errors have been corrected in the reprinted editions, and
we thank the following readers for pointing these out: Luis Baumela, Niclas Borlin,
Mike Brooks, Jun ho. Choi, Wojciech Chojnacki, Carlo Colombo, Nicolas Dano, An-
drew Fitzgibbon, Bogdan Georgescu, Fredrik Kahl, Bob Kaucic, Jae-Hak Kim, Han-
sung Lee, Dennis Maier, Karsten Muelhmann, David Nister, Andreas Olsson, St´ephane
Paris, Frederik Schaffalitzky, Bill Severson, Pedro Lopez de Teruel Alcolea, Bernard
Thiesse, Ken Thornton, Magdalena Urbanek, Gergely Vass, Eugene Vendrovsky, Sui
Wei, and Tom´aˇs Werner.

The second edition. This new paperback edition has been expanded to include some
of the developments since the original version of July 2000. For example, the book
now covers the discovery of a closed form factorization solution in the projective case
when a plane is visible in the scene, and the extension of afﬁne factorization to non-
rigid scenes. We have also extended the discussion of single view geometry (chapter 8)
and three view geometry (chapter 15), and added an appendix on parameter estimation.
In preparing this second edition we are very grateful to colleagues who have made
suggestion for improvements and additions. These include Marc Pollefeys, Bill Triggs
and in particular Tom´aˇs Werner who provided excellent and comprehensive comments.
We also thank Antonio Criminisi, Andrew Fitzgibbon, Rob Fergus, David Liebowitz,
and particularly Josef ˇSivic, for proof reading and very helpful comments on parts of
the new material. As always we are grateful to David Tranah of CUP.

The ﬁgures appearing in this book can be downloaded from

http://www.robots.ox.ac.uk/∼vgg/hzbook.html

This site also includes Matlab code for several of the algorithms, and lists the errata of
earlier printings.

I am never forget the day my ﬁrst book is published. Every chapter I stole from somewhere else. Index
I copy from old Vladivostok telephone directory. This book, this book was sensational!

Excerpts from “Nikolai Ivanovich Lobachevsky” by Tom Lehrer.

1

Introduction – a Tour of Multiple View Geometry

This chapter is an introduction to the principal ideas covered in this book. It gives an
informal treatment of these topics. Precise, unambiguous deﬁnitions, careful algebra,
and the description of well honed estimation algorithms is postponed until chapter 2
and the following chapters in the book. Throughout this introduction we will generally
not give speciﬁc forward pointers to these later chapters. The material referred to can
be located by use of the index or table of contents.

1.1 Introduction – the ubiquitous projective geometry

We are all familiar with projective transformations.When we look at a picture, we see
squares that are not squares, or circles that are not circles. The transformation that
maps these planar objects onto the picture is an example of a projective transformation.
So what properties of geometry are preserved by projective transformations? Cer-
tainly, shape is not, since a circle may appear as an ellipse. Neither are lengths since
two perpendicular radii of a circle are stretched by different amounts by the projective
transformation. Angles, distance, ratios of distances – none of these are preserved,
and it may appear that very little geometry is preserved by a projective transformation.
However, a property that is preserved is that of straightness. It turns out that this is
the most general requirement on the mapping, and we may deﬁne a projective trans-
formation of a plane as any mapping of the points on the plane that preserves straight
lines.

To see why we will require projective geometry we start from the familiar Euclidean
geometry. This is the geometry that describes angles and shapes of objects. Euclidean
geometry is troublesome in one major respect – we need to keep making an exception
to reason about some of the basic concepts of the geometry – such as intersection of
lines. Two lines (we are thinking here of 2-dimensional geometry) almost always meet
in a point, but there are some pairs of lines that do not do so – those that we call parallel.
A common linguistic device for getting around this is to say that parallel lines meet “at
inﬁnity”. However this is not altogether convincing, and conﬂicts with another dictum,
that inﬁnity does not exist, and is only a convenient ﬁction. We can get around this by

1

2

1 Introduction – a Tour of Multiple View Geometry

enhancing the Euclidean plane by the addition of these points at inﬁnity where parallel
lines meet, and resolving the difﬁculty with inﬁnity by calling them “ideal points.”

By adding these points at inﬁnity, the familiar Euclidean space is transformed into a
new type of geometric object, projective space. This is a very useful way of thinking,
since we are familiar with the properties of Euclidean space, involving concepts such as
distances, angles, points, lines and incidence. There is nothing very mysterious about
projective space – it is just an extension of Euclidean space in which two lines always
meet in a point, though sometimes at mysterious points at inﬁnity.

Coordinates. A point in Euclidean 2-space is represented by an ordered pair of real
numbers, (x, y). We may add an extra coordinate to this pair, giving a triple (x, y, 1),
that we declare to represent the same point. This seems harmless enough, since we
can go back and forward from one representation of the point to the other, simply by
adding or removing the last coordinate. We now take the important conceptual step
of asking why the last coordinate needs to be 1 – after all, the others two coordinates
are not so constrained. What about a coordinate triple (x, y, 2).
It is here that we
make a deﬁnition and say that (x, y, 1) and (2x, 2y, 2) represent the same point, and
furthermore, (kx, ky, k) represents the same point as well, for any non-zero value k.
Formally, points are represented by equivalence classes of coordinate triples, where
two triples are equivalent when they differ by a common multiple. These are called the
homogeneous coordinates of the point. Given a coordinate triple (kx, ky, k), we can
get the original coordinates back by dividing by k to get (x, y).

The reader will observe that although (x, y, 1) represents the same point as the co-
ordinate pair (x, y), there is no point that corresponds to the triple (x, y, 0). If we try
to divide by the last coordinate, we get the point (x/0, y/0) which is inﬁnite. This is
how the points at inﬁnity arise then. They are the points represented by homogeneous
coordinates in which the last coordinate is zero.

Once we have seen how to do this for 2-dimensional Euclidean space, extending it
to a projective space by representing points as homogeneous vectors, it is clear that we
can do the same thing in any dimension. The Euclidean space IRn can be extended to
a projective space IPn by representing points as homogeneous vectors. It turns out that
the points at inﬁnity in the two-dimensional projective space form a line, usually called
the line at inﬁnity. In three-dimensions they form the plane at inﬁnity.

Homogeneity. In classical Euclidean geometry all points are the same. There is no
distinguished point. The whole of the space is homogeneous. When coordinates are
added, one point is seemingly picked out as the origin. However, it is important to
realize that this is just an accident of the particular coordinate frame chosen. We could
just as well ﬁnd a different way of coordinatizing the plane in which a different point
is considered to be the origin. In fact, we can consider a change of coordinates for the
Euclidean space in which the axes are shifted and rotated to a different position. We
may think of this in another way as the space itself translating and rotating to a different
position. The resulting operation is known as a Euclidean transform.

A more general type of transformation is that of applying a linear transformation

1.1 Introduction – the ubiquitous projective geometry

3

to IRn, followed by a Euclidean transformation moving the origin of the space. We
may think of this as the space moving, rotating and ﬁnally stretching linearly possibly
by different ratios in different directions. The resulting transformation is known as an
afﬁne transformation.

The result of either a Euclidean or an afﬁne transformation is that points at inﬁn-
ity remain at inﬁnity. Such points are in some way preserved, at least as a set, by
such transformations. They are in some way distinguished, or special in the context of
Euclidean or afﬁne geometry.

From the point of view of projective geometry, points at inﬁnity are not any dif-
ferent from other points. Just as Euclidean space is uniform, so is projective space.
The property that points at inﬁnity have ﬁnal coordinate zero in a homogeneous co-
ordinate representation is nothing other than an accident of the choice of coordinate
frame. By analogy with Euclidean or afﬁne transformations, we may deﬁne a projec-
tive transformation of projective space. A linear transformation of Euclidean space IRn
is represented by matrix multiplication applied to the coordinates of the point. In just
the same way a projective transformation of projective space IPn is a mapping of the
homogeneous coordinates representing a point (an (n + 1)-vector), in which the coor-
dinate vector is multiplied by a non-singular matrix. Under such a mapping, points at
inﬁnity (with ﬁnal coordinate zero) are mapped to arbitrary other points. The points at
inﬁnity are not preserved. Thus, a projective transformation of projective space IPn is
represented by a linear transformation of homogeneous coordinates

X(cid:2)

= H(n+1)×(n+1)X.

In computer vision problems, projective space is used as a convenient way of repre-
senting the real 3D world, by extending it to the 3-dimensional (3D) projective space.
Similarly images, usually formed by projecting the world onto a 2-dimensional repre-
sentation, are for convenience extended to be thought of as lying in the 2-dimensional
projective space. In reality, the real world, and images of it do not contain points at
inﬁnity, and we need to keep our ﬁnger on which are the ﬁctitious points, namely the
line at inﬁnity in the image and the plane at inﬁnity in the world. For this reason, al-
though we usually work with the projective spaces, we are aware that the line and plane
at inﬁnity are in some way special. This goes against the spirit of pure projective ge-
ometry, but makes it useful for our practical problems. Generally we try to have it both
ways by treating all points in projective space as equals when it suits us, and singling
out the line at inﬁnity in space or the plane at inﬁnity in the image when that becomes
necessary.

1.1.1 Afﬁne and Euclidean Geometry
We have seen that projective space can be obtained from Euclidean space by adding
a line (or plane) at inﬁnity. We now consider the reverse process of going backwards.
This discussion is mainly concerned with two and three-dimensional projective space.

Afﬁne geometry. We will take the point of view that the projective space is initially
homogeneous, with no particular coordinate frame being preferred. In such a space,

4

1 Introduction – a Tour of Multiple View Geometry

there is no concept of parallelism of lines, since parallel lines (or planes in the three-
dimensional case) are ones that meet at inﬁnity. However, in projective space, there is
no concept of which points are at inﬁnity – all points are created equal. We say that
parallelism is not a concept of projective geometry. It is simply meaningless to talk
about it.

In order for such a concept to make sense, we need to pick out some particular line,
and decide that this is the line at inﬁnity. This results in a situation where although
all points are created equal, some are more equal than others. Thus, start with a blank
sheet of paper, and imagine that it extends to inﬁnity and forms a projective space
IP2. What we see is just a small part of the space, that looks a lot like a piece of the
ordinary Euclidean plane. Now, let us draw a straight line on the paper, and declare
that this is the line at inﬁnity. Next, we draw two other lines that intersect at this
distinguished line. Since they meet at the “line at inﬁnity” we deﬁne them as being
parallel. The situation is similar to what one sees by looking at an inﬁnite plane. Think
of a photograph taken in a very ﬂat region of the earth. The points at inﬁnity in the
plane show up in the image as the horizon line. Lines, such as railway tracks show
up in the image as lines meeting at the horizon. Points in the image lying above the
horizon (the image of the sky) apparently do not correspond to points on the world
plane. However, if we think of extending the corresponding ray backwards behind the
camera, it will meet the plane at a point behind the camera. Thus there is a one-to-one
relationship between points in the image and points in the world plane. The points at
inﬁnity in the world plane correspond to a real horizon line in the image, and parallel
lines in the world correspond to lines meeting at the horizon. From our point of view,
the world plane and its image are just alternative ways of viewing the geometry of a
projective plane, plus a distinguished line. The geometry of the projective plane and a
distinguished line is known as afﬁne geometry and any projective transformation that
maps the distinguished line in one space to the distinguished line of the other space is
known as an afﬁne transformation.

By identifying a special line as the “line at inﬁnity” we are able to deﬁne parallelism
of straight lines in the plane. However, certain other concepts make sense as well, as
soon as we can deﬁne parallelism. For instance, we may deﬁne equalities of intervals
between two points on parallel lines. For instance, if A, B, C and D are points, and
the lines AB and CD are parallel, then we deﬁne the two intervals AB and CD to
have equal length if the lines AC and BD are also parallel. Similarly, two intervals on
the same line are equal if there exists another interval on a parallel line that is equal to
both.

Euclidean geometry. By distinguishing a special line in a projective plane, we gain
the concept of parallelism and with it afﬁne geometry. Afﬁne geometry is seen as
specialization of projective geometry, in which we single out a particular line (or plane
– according to the dimension) and call it the line at inﬁnity.

Next, we turn to Euclidean geometry and show that by singling out some special
feature of the line or plane at inﬁnity afﬁne geometry becomes Euclidean geometry. In

1.1 Introduction – the ubiquitous projective geometry

5

doing so, we introduce one of the most important concepts of this book, the absolute
conic.

We begin by considering two-dimensional geometry, and start with circles. Note that
a circle is not a concept of afﬁne geometry, since arbitrary stretching of the plane, which
preserves the line at inﬁnity, turns the circle into an ellipse. Thus, afﬁne geometry does
not distinguish between circles and ellipses.

In Euclidean geometry however, they are distinct, and have an important difference.
Algebraically, an ellipse is described by a second-degree equation. It is therefore ex-
pected, and true that two ellipses will most generally intersect in four points. However,
it is geometrically evident that two distinct circles can not intersect in more than two
points. Algebraically, we are intersecting two second-degree curves here, or equiva-
lently solving two quadratic equations. We should expect to get four solutions. The
question is, what is special about circles that they only intersect in two points.

The answer to this question is of course that there exist two other solutions, the two
circles meeting in two other complex points. We do not have to look very far to ﬁnd
these two points.

The equation for a circle in homogeneous coordinates (x, y, w) is of the form

(x − aw)2 + (y − bw)2 = r2w2

This represents the circle with centre represented in homogeneous coordinates as
(x0, y0, w0)T = (a, b, 1)T. It is quickly veriﬁed that the points (x, y, w)T = (1,±i, 0)T
lie on every such circle. To repeat this interesting fact, every circle passes through the
points (1,±i, 0)T, and therefore they lie in the intersection of any two circles. Since
their ﬁnal coordinate is zero, these two points lie on the line at inﬁnity. For obvious
reasons, they are called the circular points of the plane. Note that although the two
circular points are complex, they satisfy a pair of real equations: x2 + y2 = 0; w = 0.
This observation gives the clue of how we may deﬁne Euclidean geometry. Euclidean
geometry arises from projective geometry by singling out ﬁrst a line at inﬁnity and
subsequently, two points called circular points lying on this line. Of course the circular
points are complex points, but for the most part we do not worry too much about
this. Now, we may deﬁne a circle as being any conic (a curve deﬁned by a second-
degree equation) that passes through the two circular points. Note that in the standard
Euclidean coordinate system, the circular points have the coordinates (1,±i, 0)T. In
assigning a Euclidean structure to a projective plane, however, we may designate any
line and any two (complex) points on that line as being the line at inﬁnity and the
circular points.

As an example of applying this viewpoint, we note that a general conic may be
found passing through ﬁve arbitrary points in the plane, as may be seen by counting
the number of coefﬁcients of a general quadratic equation ax2 + by2 + . . . + f w2 = 0.
A circle on the other hand is deﬁned by only three points. Another way of looking at
this is that it is a conic passing through two special points, the circular points, as well
as three other points, and hence as any other conic, it requires ﬁve points to specify it
uniquely.

It should not be a surprise that as a result of singling out two circular points one

6

1 Introduction – a Tour of Multiple View Geometry

obtains the whole of the familiar Euclidean geometry. In particular, concepts such as
angle and length ratios may be deﬁned in terms of the circular points. However, these
concepts are most easily deﬁned in terms of some coordinate system for the Euclidean
plane, as will be seen in later chapters.

3D Euclidean geometry. We saw how the Euclidean plane is deﬁned in terms of
the projective plane by specifying a line at inﬁnity and a pair of circular points. The
same idea may be applied to 3D geometry. As in the two-dimensional case, one may
look carefully at spheres, and how they intersect. Two spheres intersect in a circle,
and not in a general fourth-degree curve, as the algebra suggests, and as two general
ellipsoids (or other quadric surfaces) do. This line of thought leads to the discovery
that in homogeneous coordinates (X, Y, Z, T)T all spheres intersect the plane at inﬁnity
in a curve with the equations: X2 + Y2 + Z2 = 0; T = 0. This is a second-degree curve
(a conic) lying on the plane at inﬁnity, and consisting only of complex points. It is
known as the absolute conic and is one of the key geometric entities in this book, most
particularly because of its connection to camera calibration, as will be seen later.

The absolute conic is deﬁned by the above equations only in the Euclidean coor-
dinate system. In general we may consider 3D Euclidean space to be derived from
projective space by singling out a particular plane as the plane at inﬁnity and specify-
ing a particular conic lying in this plane to be the absolute conic. These entities may
have quite general descriptions in terms of a coordinate system for the projective space.
We will not here go into details of how the absolute conic determines the complete
Euclidean 3D geometry. A single example will serve. Perpendicularity of lines in
space is not a valid concept in afﬁne geometry, but belongs to Euclidean geometry.
The perpendicularity of lines may be deﬁned in terms of the absolute conic, as follows.
By extending the lines until they meet the plane at inﬁnity, we obtain two points called
the directions of the two lines. Perpendicularity of the lines is deﬁned in terms of the
relationship of the two directions to the absolute conic. The lines are perpendicular if
the two directions are conjugate points with respect to the absolute conic (see ﬁgure
3.8(p83)). The geometry and algebraic representation of conjugate points are deﬁned
in section 2.8.1(p58). Brieﬂy, if the absolute conic is represented by a 3× 3 symmetric
matrix Ω∞, and the directions are the points d1 and d2, then they are conjugate with
respect to Ω∞ if dT
Ω∞d2 = 0. More generally, angles may be deﬁned in terms of the
1
absolute conic in any arbitrary coordinate system, as expressed by (3.23–p82).

1.2 Camera projections

One of the principal topics of this book is the process of image formation, namely the
formation of a two-dimensional representation of a three-dimensional world, and what
we may deduce about the 3D structure of what appears in the images.

The drop from three-dimensional world to a two-dimensional image is a projection
process in which we lose one dimension. The usual way of modelling this process is
by central projection in which a ray from a point in space is drawn from a 3D world
point through a ﬁxed point in space, the centre of projection. This ray will intersect a
speciﬁc plane in space chosen as the image plane. The intersection of the ray with the

1.2 Camera projections

7

image plane represents the image of the point. If the 3D structure lies on a plane then
there is no drop in dimension.

This model is in accord with a simple model of a camera, in which a ray of light
from a point in the world passes through the lens of a camera and impinges on a ﬁlm or
digital device, producing an image of the point. Ignoring such effects as focus and lens
thickness, a reasonable approximation is that all the rays pass through a single point,
the centre of the lens.

In applying projective geometry to the imaging process, it is customary to model the
world as a 3D projective space, equal to IR3 along with points at inﬁnity. Similarly
the model for the image is the 2D projective plane IP2. Central projection is simply
a map from IP3 to IP2. If we consider points in IP3 written in terms of homogeneous
coordinates (X, Y, Z, T)T and let the centre of projection be the origin (0, 0, 0, 1)T, then
we see that the set of all points (X, Y, Z, T)T for ﬁxed X, Y and Z, but varying T form
a single ray passing through the point centre of projection, and hence all mapping to
the same point. Thus, the ﬁnal coordinate of (X, Y, Z, T) is irrelevant to where the point
is imaged. In fact, the image point is the point in IP2 with homogeneous coordinates
(X, Y, Z)T. Thus, the mapping may be represented by a mapping of 3D homogeneous
coordinates, represented by a 3 × 4 matrix P with the block structure P = [I3×3|03],
where I3×3 is the 3 × 3 identity matrix and 03 a zero 3-vector. Making allowance for a
different centre of projection, and a different projective coordinate frame in the image,
it turns out that the most general imaging projection is represented by an arbitrary 3× 4
matrix of rank 3, acting on the homogeneous coordinates of the point in IP3 mapping it
to the imaged point in IP2. This matrix P is known as the camera matrix.

In summary, the action of a projective camera on a point in space may be expressed

in terms of a linear mapping of homogeneous coordinates as

 x

y
w

y
w

 = P3×4
 = H3×3




Y
Z
T

 X
 X

Y
T

Furthermore, if all the points lie on a plane (we may choose this as the plane Z = 0)

then the linear mapping reduces to x

which is a projective transformation.

Cameras as points. In a central projection, points in IP3 are mapped to points in IP2,
all points in a ray passing through the centre of projection projecting to the same point
in an image. For the purposes of image projection, it is possible to consider all points
along such a ray as being equal. We can go one step further, and think of the ray
through the projection centre as representing the image point. Thus, the set of all
image points is the same as the set of rays through the camera centre. If we represent

8

1 Introduction – a Tour of Multiple View Geometry

x

3

image plane

x

2

x

1

x

4

C

X

1

X

2

X

3

X4

/

x3

x

3

x

/
2

/

x1

x

2

x

1

x

3

x

4

x

2

x

1

C

X

3

X4

x

3

x

4

x

2

x

1

a

x

4

c

C

X

3

X4

C

X3

X4

x

/
4

x

/
3

π

x /
1

/

x2

/

C

X1

X

1

X

2

x

3

x

4

x

2

x

1

C

X3

X4

X2

e

X1

π

x

/
2

x

/
3

x

/
4

x /
1

/

C

X1

X2

b

X2

d

Fig. 1.1. The camera centre is the essence. (a) Image formation: the image points xi are the inter-
section of a plane with rays from the space points Xi through the camera centre C. (b) If the space
points are coplanar then there is a projective transformation between the world and image planes,
xi = H3×3Xi. (c) All images with the same camera centre are related by a projective transformation,
x(cid:1)
i = H(cid:1)
3×3xi. Compare (b) and (c) – in both cases planes are mapped to one another by rays through
a centre. In (b) the mapping is between a scene and image plane, in (c) between two image planes. (d)
If the camera centre moves, then the images are in general not related by a projective transformation,
unless (e) all the space points are coplanar.

the ray from (0, 0, 0, 1)T through the point (X, Y, Z, T)T by its ﬁrst three coordinates
(X, Y, Z)T, it is easily seen that for any constant k, the ray k(X, Y, Z)T represents the
same ray. Thus the rays themselves are represented by homogeneous coordinates. In

1.2 Camera projections

9

fact they make up a 2-dimensional space of rays. The set of rays themselves may be
thought of as a representation of the image space IP2. In this representation of the
image, all that is important is the camera centre, for this alone determines the set of
rays forming the image. Different camera matrices representing the image formation
from the same centre of projection reﬂect only different coordinate frames for the set
of rays forming the image. Thus two images taken from the same point in space are
projectively equivalent. It is only when we start to measure points in an image, that
a particular coordinate frame for the image needs to be speciﬁed. Only then does it
become necessary to specify a particular camera matrix.
In short, modulo ﬁeld-of-
view which we ignore for now, all images acquired with the same camera centre are
equivalent – they can be mapped onto each other by a projective transformation without
any information about the 3D points or position of the camera centre. These issues are
illustrated in ﬁgure 1.1.

Calibrated cameras. To understand fully the Euclidean relationship between the im-
age and the world, it is necessary to express their relative Euclidean geometry. As
we have seen, the Euclidean geometry of the 3D world is determined by specifying
a particular plane in IP3 as being the plane at inﬁnity, and a speciﬁc conic Ω in that
plane as being the absolute conic. For a camera not located on the plane at inﬁnity, the
plane at inﬁnity in the world maps one-to-one onto the image plane. This is because
any point in the image deﬁnes a ray in space that meets the plane at inﬁnity in a single
point. Thus, the plane at inﬁnity in the world does not tell us anything new about the
image. The absolute conic, however being a conic in the plane at inﬁnity must project
to a conic in the image. The resulting image curve is called the Image of the Absolute
Conic, or IAC. If the location of the IAC is known in an image, then we say that the
camera is calibrated.

In a calibrated camera, it is possible to determine the angle between the two rays
back-projected from two points in the image. We have seen that the angle between two
lines in space is determined by where they meet the plane at inﬁnity, relative to the
absolute conic. In a calibrated camera, the plane at inﬁnity and the absolute conic Ω∞
are projected one-to-one onto the image plane and the IAC, denoted ω. The projective
relationship between the two image points and ω is exactly equal to the relationship
between the intersections of the back-projected rays with the plane at inﬁnity, and Ω∞.
Consequently, knowing the IAC, one can measure the angle between rays by direct
measurements in the image. Thus, for a calibrated camera, one can measure angles
between rays, compute the ﬁeld of view represented by an image patch or determine
whether an ellipse in the image back-projects to a circular cone. Later on, we will see
that it helps us to determine the Euclidean structure of a reconstructed scene.

Example 1.1. 3D reconstructions from paintings
Using techniques of projective geometry, it is possible in many instances to reconstruct
scenes from a single image. This cannot be done without some assumptions being
made about the imaged scene. Typical techniques involve the analysis of features such
as parallel lines and vanishing points to determine the afﬁne structure of the scene, for

10

1 Introduction – a Tour of Multiple View Geometry

a

c

b

d

Fig. 1.2. Single view reconstruction. (a) Original painting – St. Jerome in his study, 1630, Hendrick
van Steenwijck (1580-1649), Joseph R. Ritman Private Collection, Amsterdam, The Netherlands. (b)
(c)(d) Views of the 3D model created from the painting. Figures courtesy of Antonio Criminisi.

example by determining the line at inﬁnity for observed planes in the image. Knowl-
edge (or assumptions) about angles observed in the scene, most particularly orthogonal
lines or planes, can be used to upgrade the afﬁne reconstruction to Euclidean.

It is not yet possible for such techniques to be fully automatic. However, projective
geometric knowledge may be built into a system that allows user-guided single-view
reconstruction of the scene.

Such techniques have been used to reconstruct 3D texture mapped graphical models
derived from old-master paintings. Starting in the Renaissance, paintings with ex-
tremely accurate perspective were produced. In ﬁgure 1.2 a reconstruction carried out
(cid:2)
from such a painting is shown.

1.3 Reconstruction from more than one view

We now turn to one of the major topics in the book – that of reconstructing a scene
from several images. The simplest case is that of two images, which we will consider
ﬁrst. As a mathematical abstraction, we restrict the discussion to “scenes” consisting
of points only.

The usual input to many of the algorithms given in this book is a set of point cor-
respondences. In the two-view case, therefore, we consider a set of correspondences

1.3 Reconstruction from more than one view

11

xi ↔ x(cid:2)
i in two images. It is assumed that there exist some camera matrices, P and P(cid:2)
and a set of 3D points Xi that give rise to these image correspondences in the sense
that PXi = xi and P(cid:2)Xi = x(cid:2)
i. Thus, the point Xi projects to the two given data points.
However, neither the cameras (represented by projection matrices P and P(cid:2)
), nor the
points Xi are known. It is our task to determine them.

It is clear from the outset that it is impossible to determine the positions of the points
uniquely. This is a general ambiguity that holds however many images we are given,
and even if we have more than just point correspondence data. For instance, given
several images of a cube, it is impossible to tell its absolute position (is it located in
a night-club in Addis Ababa, or the British Museum), its orientation (which face is
facing north) or its scale. We express this by saying that the reconstruction is possible
at best up to a similarity transformation of the world. However, it turns out that unless
something is known about the calibration of the two cameras, the ambiguity in the
reconstruction is expressed by a more general class of transformations – projective
transformations.
This ambiguity arises because it is possible to apply a projective transformation (rep-
resented by a 4 × 4 matrix H) to each point Xi, and on the right of each camera matrix
Pj, without changing the projected image points, thus:
PjXi = (PjH−1)(HXi).

(1.1)

There is no compelling reason to choose one set of points and camera matrices over the
other. The choice of H is essentially arbitrary, and we say that the reconstruction has a
projective ambiguity, or is a projective reconstruction.

However, the good news is that this is the worst that can happen. It is possible to
reconstruct a set of points from two views, up to an unavoidable projective ambiguity.
Well, to be able to say this, we need to make a few qualiﬁcations; there must be sufﬁ-
ciently many points, at least seven, and they must not lie in one of various well-deﬁned
critical conﬁgurations.
The basic tool in the reconstruction of point sets from two views is the fundamental
matrix, which represents the constraint obeyed by image points x and x(cid:2)
if they are
to be images of the same 3D point. This constraint arises from the coplanarity of the
camera centres of the two views, the images points and the space point. Given the
fundamental matrix F, a pair of matching points xi ↔ x(cid:2)

i must satisfy

x(cid:2)

i

TFxi = 0

where F is a 3 × 3 matrix of rank 2. These equations are linear in the entries of the
matrix F, which means that if F is unknown, then it can be computed from a set of point
correspondences.

A pair of camera matrices P and P(cid:2)

uniquely determine a fundamental matrix F, and
conversely, the fundamental matrix determines the pair of camera matrices, up to a 3D
projective ambiguity. Thus, the fundamental matrix encapsulates the complete projec-
tive geometry of the pair of cameras, and is unchanged by projective transformation of
3D.

12

1 Introduction – a Tour of Multiple View Geometry

The fundamental-matrix method for reconstructing the scene is very simple, consist-

ing of the following steps:

(i) Given several point correspondences xi ↔ x(cid:2)

equations in the entries of F based on the coplanarity equations x(cid:2)

i across two views, form linear

TFxi = 0.

i

(ii) Find F as the solution to a set of linear equations.
(iii) Compute a pair of camera matrices from F according to the simple formula
given in section 9.5(p253).
) and the corresponding image point pairs xi ↔ x(cid:2)
(iv) Given the two cameras (P, P(cid:2)
i,
ﬁnd the 3D point Xi that projects to the given image points. Solving for X in
this way is known as triangulation.

The algorithm given here is an outline only, and each part of it is examined in de-
tail in this book. The algorithm should not be implemented directly from this brief
description.

1.4 Three-view geometry

In the last section it was discussed how reconstruction of a set of points, and the relative
placement of the cameras, is possible from two views of a set of points. The reconstruc-
tion is possible only up to a projective transformation of space, and the corresponding
adjustment to the camera matrices.

In this section, we consider the case of three views. Whereas for two views, the
basic algebraic entity is the fundamental matrix, for three views this role is played by
the trifocal tensor. The trifocal tensor is a 3 × 3 × 3 array of numbers that relate the
coordinates of corresponding points or lines in three views. Just as the fundamental
matrix is determined by the two camera matrices, and determines them up to projective
transformation, so in three views, the trifocal tensor is determined by the three camera
matrices, and in turn determines them, again up to projective transformation. Thus, the
trifocal tensor encapsulates the relative projective geometry of the three cameras.

For reasons that will be explained in chapter 15 it is usual to write some of the indices
of a tensor as lower and some as upper indices. These are referred to as the covariant
and contravariant indices. The trifocal tensor is of the form T jk
, having two upper and
one lower index.
The most basic relationship between image entities in three views concerns a corre-
spondence between two lines and a point. We consider a correspondence x ↔ l(cid:2) ↔ l(cid:2)(cid:2)
between a point x in one image and two lines l(cid:2)
in the other two images. This
relationship means that there is a point X in space that maps to x in the ﬁrst image, and
to points x(cid:2)
in the other two images. The coordinates
of these three images are then related via the trifocal tensor relationship:

and x(cid:2)(cid:2)

and l(cid:2)(cid:2)

and l(cid:2)(cid:2)

i

lying on the lines l(cid:2)
(cid:7)

(cid:2)(cid:2)
(cid:2)
jl
k

T jk
i = 0.

(1.2)

xil

This relationship gives a single linear relationship between the elements of the tensor.
With sufﬁciently many such correspondences, it is possible to solve linearly for the

ijk

1.5 Four view geometry and n-view reconstruction

13

and x(cid:2)(cid:2)

. In fact, in this situation, one can choose any lines l(cid:2)

elements of the tensor. Fortunately, one can obtain more equations from a point corre-
spondence x ↔ x(cid:2) ↔ x(cid:2)(cid:2)
and l(cid:2)(cid:2)
passing through the points x(cid:2)
and generate a relation of the sort (1.2). Since it
is possible to choose two independent lines passing through x(cid:2)
, and two others passing
through x(cid:2)(cid:2)
, one can obtain four independent equations in this way. A total of seven
point correspondences are sufﬁcient to compute the trifocal tensor linearly in this way.
It can be computed from a minimum of six point correspondences using a non-linear
method.

The 27 elements of the tensor are not independent, however, but are related by a set
of so called internal constraints. These constraints are quite complicated, but tensors
satisfying the constraints can be computed in various ways, for instance by using the
6 point non-linear method. The fundamental matrix (which is a 2-view tensor) also
satisﬁes an internal constraint but a relatively simple one: the elements obey det F = 0.
As with the fundamental matrix, once the trifocal tensor is known, it is possible to
extract the three camera matrices from it, and thereby obtain a reconstruction of the
scene points and lines. As ever, this reconstruction is unique only up to a 3D projective
transformation; it is a projective reconstruction.

Thus, we are able to generalize the method for two views to three views. There are

several advantages to using such a three-view method for reconstruction.

(i) It is possible to use a mixture of line and point correspondences to compute the
projective reconstruction. With two views, only point correspondences can be
used.

(ii) Using three views gives greater stability to the reconstruction, and avoids unsta-
ble conﬁgurations that may occur using only two views for the reconstruction.

1.5 Four view geometry and n-view reconstruction

It is possible to go one more step with tensor-based methods and deﬁne a quadrifocal
tensor relating entities visible in four views. This method is seldom used, however, be-
cause of the relative difﬁculty of computing a quadrifocal tensor that obey its internal
constraints. Nevertheless, it does provide a non-iterative method for computing a pro-
jective reconstruction based on four views. The tensor method does not extend to more
than four views, however, and so reconstruction from more than four views becomes
more difﬁcult.

Many methods have been considered for reconstruction from several views, and we
consider a few of these in the book. One way to proceed is to reconstruct the scene
bit by bit, using three-view or two-view techniques. Such a method may be applied to
any image sequence, and with care in selecting the right triples to use, it will generally
succeed.

There are methods that can be used in speciﬁc circumstances. The task of reconstruc-
tion becomes easier if we are able to apply a simpler camera model, known as the afﬁne
camera. This camera model is a fair approximation to perspective projection whenever
the distance to the scene is large compared with the difference in depth between the
back and front of the scene. If a set of points are visible in all of a set of n views

14

1 Introduction – a Tour of Multiple View Geometry

involving an afﬁne camera, then a well-known algorithm, the factorization algorithm,
can be used to compute both the structure of the scene, and the speciﬁc camera models
in one step using the Singular Value Decomposition. This algorithm is very reliable
and simple to implement. Its main difﬁculties are the use of the afﬁne camera model,
rather than a full projective model, and the requirement that all the points be visible in
all views.

This method has been extended to projective cameras in a method known as projec-
tive factorization. Although this method is generally satisfactory, it can not be proven
to converge to the correct solution in all cases. Besides, it also requires all points to be
visible in all images.

Other methods for n-view reconstruction involve various assumptions, such as
knowledge of four coplanar points in the world visible in all views, or six or seven
points that are visible in all images in the sequence. Methods that apply to speciﬁc mo-
tion sequences, such as linear motion, planar motion or single axis (turntable) motion
have also been developed.

The dominant methodology for the general reconstruction problem is bundle adjust-
ment. This is an iterative method, in which one attempts to ﬁt a non-linear model to
the measured data (the point correspondences). The advantage of bundle-adjustment is
that it is a very general method that may be applied to a wide range of reconstruction
and optimization problems. It may be implemented in such a way that the discovered
solution is the Maximum Likelihood solution to the problem, that is a solution that is in
some sense optimal in terms of a model for the inaccuracies of image measurements.

Unfortunately, bundle adjustment is an iterative process, which can not be guaran-
teed to converge to the optimal solution from an arbitrary starting point. Much research
in reconstruction methods seeks easily computable non-optimal solutions that can be
used as a starting point for bundle adjustment. An initialization step followed by bundle
adjustment is the generally preferred technique for reconstruction. A common impres-
sion is that bundle-adjustment is necessarily a slow technique. The truth is that it is
quite efﬁcient when implemented carefully. A lengthy appendix in this book deals
with efﬁcient methods of bundle adjustment.

Using n-view reconstruction techniques, it is possible to carry out reconstructions
automatically from quite long sequences of images. An example is given in ﬁgure 1.3,
showing a reconstruction from 700 frames.

1.6 Transfer

We have discussed 3D reconstruction from a set of images. Another useful application
of projective geometry is that of transfer: given the position of a point in one (or more)
image(s), determine where it will appear in all other images of the set. To do this, we
must ﬁrst establish the relationship between the cameras using (for instance) a set of
auxiliary point correspondences. Conceptually transfer is straightforward given that a
reconstruction is possible. For instance, suppose the point is identiﬁed in two views (at
x and x(cid:2)
in a third, then this may be computed by
the following steps:

) and we wish to know its position x(cid:2)(cid:2)

1.6 Transfer

15

(b)

(c)

(a)

Fig. 1.3. Reconstruction. (a) Seven frames of a 700 frame sequence acquired by a hand held camera
whilst walking down a street in Oxford. (b)(c) Two views of the reconstructed point cloud and camera
path (the red curve). Figures courtesy of David Capel and 2d3 (www.2d3.com).

16

1 Introduction – a Tour of Multiple View Geometry

Fig. 1.4. Projective ambiguity: Reconstructions of a mug (shown with the true shape in the centre)
under 3D projective transformations in the Z direction. Five examples of the cup with different degrees
of projective distortion are shown. The shapes are quite different from the original.

respondences xi ↔ x(cid:2)

(i) Compute the camera matrices of the three views P, P(cid:2)
(ii) Triangulate the 3D point X from x and x(cid:2)
(iii) Project the 3D point into the third view as x(cid:2)(cid:2)

↔ x(cid:2)(cid:2)
i .

i

, P(cid:2)(cid:2)
using P and P(cid:2)

.

= P(cid:2)(cid:2)X.

from other point cor-

This procedure only requires projective information. An alternative procedure is to use
the multi-view tensors (the fundamental matrix and trifocal tensor) to transfer the point
directly without an explicit 3D reconstruction. Both methods have their advantages.

Suppose the camera rotates about its centre or that all the scene points of interest
lie on a plane. Then the appropriate multiple view relations are the planar projective
transformations between the images. In this case, a point seen in just one image can be
transferred to any other image.

1.7 Euclidean reconstruction

So far we have considered the reconstruction of a scene, or transfer, for images taken
with a set of uncalibrated cameras. For such cameras, important parameters such as
the focal length, the geometric centre of the image (the principal point) and possibly
the aspect ratio of the pixels in the image are unknown. If a complete calibration of
each of the cameras is known then it is possible to remove some of the ambiguity of
the reconstructed scene.

So far, we have discussed projective reconstruction, which is all that is possible with-
out knowing something about the calibration of the cameras or the scene. Projective
reconstruction is insufﬁcient for many purposes, such as application to computer graph-
ics, since it involves distortions of the model that appear strange to a human used to
viewing a Euclidean world. For instance, the distortions that projective transformations
induce in a simple object are shown in ﬁgure 1.4. Using the technique of projective re-
construction, there is no way to choose between any of the possible shapes of the mug
in ﬁgure 1.4, and a projective reconstruction algorithm is as likely to come up with
any one of the reconstructions shown there as any other. Even more severely distorted
models may arise from projective reconstruction.

In order to obtain a reconstruction of the model in which objects have their correct
(Euclidean) shape, it is necessary to determine the calibration of the cameras. It is
easy to see that this is sufﬁcient to determine the Euclidean structure of the scene.
As we have seen, determining the Euclidean structure of the world is equivalent to
specifying the plane at inﬁnity and the absolute conic. In fact, since the absolute conic

1.8 Auto-calibration

17

lies in a plane, the plane at inﬁnity, it is enough to ﬁnd the absolute conic in space.
Now, suppose that we have computed a projective reconstruction of the world, using
calibrated cameras. By deﬁnition, this means that the IAC is known in each of the
images; let it be denoted by ωi in the i-th image. The back-projection of each ωi is a
cone in space, and the absolute conic must lie in the intersection of all the cones. Two
cones in general intersect in a fourth-degree curve, but given that they must intersect in
a conic, this curve must split into two conics. Thus, reconstruction of the absolute conic
from two images is not unique – rather, there are two possible solutions in general.
However, from three or more images, the intersection of the cones is unique in general.
Thus the absolute conic is determined and with it the Euclidean structure of the scene.
Of course, if the Euclidean structure of the scene is known, then so is the position of
the absolute conic. In this case we may project it back into each of the images, produc-
ing the IAC in each image, and hence calibrating the cameras. Thus knowledge of the
camera calibration is equivalent to being able to determine the Euclidean structure of
the scene.

1.8 Auto-calibration

Without any knowledge of the calibration of the cameras, it is impossible to do better
than projective reconstruction. There is no information in a set of feature correspon-
dences across any number of views that can help us ﬁnd the image of the absolute
conic, or equivalently the calibration of the cameras. However, if we know just a little
about the calibration of the cameras then we may be able to determine the position of
the absolute conic.

Suppose, for instance that it is known that the calibration is the same for each of the
cameras used in reconstructing a scene from an image sequence. By this we mean the
following. In each image a coordinate system is deﬁned, in which we have measured
the image coordinates of corresponding features used to do projective reconstruction.
Suppose that in all these image coordinate systems, the IAC is the same, but just where
it is located is unknown. From this knowledge, we wish to compute the position of the
absolute conic.

One way to ﬁnd the absolute conic is to hypothesize the position of the IAC in one
image; by hypothesis, its position in the other images will be the same. The back-
projection of each of the conics will be a cone in space. If the three cones all meet in a
single conic, then this must be a possible solution for the position of the absolute conic,
consistent with the reconstruction.

Note that this is a conceptual description only. The IAC is of course a conic con-
taining only complex points, and its back-projection will be a complex cone. However,
algebraically, the problem is more tractable. Although it is complex, the IAC may be
described by a real quadratic form (represented by a real symmetric matrix). The back-
projected cone is also represented by a real quadratic form. For some value of the IAC,
the three back-projected cones will meet in a conic curve in space.

Generally given three cameras known to have the same calibration, it is possible
to determine the absolute conic, and hence the calibration of the cameras. However,

18

1 Introduction – a Tour of Multiple View Geometry

although various methods have been proposed for this, it remains quite a difﬁcult prob-
lem.

Knowing the plane at inﬁnity. One method of auto-calibration is to proceed in steps
by ﬁrst determining the plane on which it lies. This is equivalent to identifying the plane
at inﬁnity in the world, and hence to determining the afﬁne geometry of the world. In
a second step, one locates the position of the absolute conic on the plane to determine
the Euclidean geometry of space. Assuming one knows the plane at inﬁnity, one can
back-project a hypothesised IAC from each of a sequence of images and intersect the
resulting cones with the plane at inﬁnity. If the IAC is chosen correctly, the intersection
curve is the absolute conic. Thus, from each pair of images one has a condition that
the back-projected cones meet in the same conic curve on the plane at inﬁnity. It turns
out that this gives a linear constraint on the entries of the matrix representing the IAC.
From a set of linear equations, one can determine the IAC, and hence the absolute
conic. Thus, auto-calibration is relatively simple, once the plane at inﬁnity has been
identiﬁed. The identiﬁcation of the plane at inﬁnity itself is substantially more difﬁcult.

Auto-calibration given square pixels in the image.
If the cameras are partially
calibrated, then it is possible to complete the calibration starting from a projective
reconstruction. One can make do with quite minimal conditions on the calibration
of the cameras, represented by the IAC. One interesting example is the square-pixel
constraint on the cameras. What this means is that a Euclidean coordinate system is
known in each image. In this case, the absolute conic, lying in the plane at inﬁnity in
the world must meet the image plane in its two circular points. The circular points in a
plane are the two points where the absolute conic meets that plane. The back-projected
rays through the circular points of the image plane must intersect the absolute conic.
Thus, each image with square pixels determines two rays that must meet the absolute
conic. Given n images, the autocalibration task then becomes that of determining a
space conic (the absolute conic) that meets a set of 2n rays in space. An equivalent
geometric picture is to intersect the set of rays with a plane and require that the set of
intersection points lie on a conic. By a simple counting argument one may see that there
are only a ﬁnite number of conics that meet eight prescribed rays in space. Therefore,
from four images one may determine the calibration, albeit up to a ﬁnite number of
possibilities.

1.9 The reward I : 3D graphical models

We have now described all the ingredients necessary to compute realistic graphics mod-
els from image sequences. From point matches between images, it is possible to carry
out ﬁrst a projective reconstruction of the point set, and determine the motion of the
camera in the chosen projective coordinate frame.

Using auto-calibration techniques, assuming some restrictions on the calibration of
the camera that captured the image sequence, the camera may be calibrated, and the
scene subsequently transformed to its true Euclidean structure.

1.10 The reward II: video augmentation

19

a

b

Fig. 1.5. (a) Three high resolution images (3000 × 2000 pixels) from a set of eleven of the cityhall in
Leuven, Belgium. (b) Three views of a Euclidean reconstruction computed from the image set showing
the 11 camera positions and point cloud.

Knowing the projective structure of the scene, it is possible to ﬁnd the epipolar ge-
ometry relating pairs of images and this restricts the correspondence search for further
matches to a line – a point in one image deﬁnes a line in the other image on which the
(as yet unknown) corresponding point must lie. In fact for suitable scenes, it is possible
to carry out a dense point match between images and create a dense 3D model of the
imaged scene. This takes the form of a triangulated shape model that is subsequently
shaded or texture-mapped from the supplied images and used to generate novel views.
The steps of this process are illustrated in ﬁgure 1.5 and ﬁgure 1.6.

1.10 The reward II: video augmentation

We ﬁnish this introduction with a further application of reconstruction methods to com-
puter graphics. Automatic reconstruction techniques have recently become widely used
in the ﬁlm industry as a means for adding artiﬁcial graphics objects in real video se-
quences. Computer analysis of the motion of the camera is replacing the previously
used manual methods for correctly aligning the artiﬁcial inserted object.

The most important requirement for realistic insertion of an artiﬁcial object in a video

20

1 Introduction – a Tour of Multiple View Geometry

a

c

e

b

d

f

Fig. 1.6. Dense reconstructions. These are computed from the cameras and image of ﬁgure 1.5. (a)
Untextured and (b) textured reconstruction of the full scene. (c) Untextured and (d) textured close up of
the area shown in the white rectangle of (b). (e) Untextured and (f) textured close up of the area shown
in the white rectangle of (d). The dense surface is computed using the three-view stereo algorithm
described in [Strecha-02]. Figures courtesy of Christoph Strecha, Frank Verbiest, and Luc Van Gool.

1.10 The reward II: video augmentation

21

a

d

b

e

c

f

Fig. 1.7. Augmented video. The animated robot is inserted into the scene and rendered using the
computed cameras of ﬁgure 1.3.
(d)-(f) The augmented
frames. Figures courtesy of 2d3 (www.2d3.com).

(a)-(c) Original frames from the sequence.

sequence is to compute the correct motion of the camera. Unless the camera motion
is correctly determined, it is impossible to generate the correct sequences of views of
the graphics model in a way that will appear consistent with the background video.
Generally, it is only the motion of the camera that is important here; we do not need to
reconstruct the scene, since it is already present in the existing video, and novel views
of the scene visible in the video are not required. The only requirement is to be able to
generate correct perspective views of the graphics model.

It is essential to compute the motion of the camera in a Euclidean frame. It is not
enough merely to know the projective motion of the camera. This is because a Eu-
clidean object is to be placed in the scene. Unless this graphics object and the cameras
are known in the same coordinate frame, then generated views of the inserted object
will be seen to distort with respect to the perceived structure of the scene seen in the
existing video.

Once the correct motion of the camera, and its calibration are known the inserted
object may be rendered into the scene in a realistic manner. If the change of the camera
calibration from frame to frame is correctly determined, then the camera may change
focal length (zoom) during the sequence. It is even possible for the principal point to
vary during the sequence through cropping.

In inserting the rendered model into the video, the task is relatively straight-forward
if it lies in front of all the existing scene. Otherwise the possibility of occlusions arises,
in which the scene may obscure parts of the model. An example of video augmentation
is shown in ﬁgure 1.7.

Part 0

The Background: Projective
Geometry, Transformations and

Estimation

La reproduction interdite (The Forbidden Reproduction), 1937, Ren´e Magritte.

Courtesy of Museum Boijmans van Beuningen, Rotterdam.

c(cid:4) ADAGP, Paris, and DACS, London 2000.

Outline

The four chapters in this part lay the foundation for the representations, terminology,
and notation that will be used in the subsequent parts of the book. The ideas and
notation of projective geometry are central to an analysis of multiple view geometry.
For example, the use of homogeneous coordinates enables non-linear mappings (such
as perspective projection) to be represented by linear matrix equations, and points at
inﬁnity to be represented quite naturally avoiding the awkward necessity of taking
limits.

Chapter 2 introduces projective transformations of 2-space. These are the transfor-
mations that arise when a plane is imaged by a perspective camera. This chapter is
more introductory and sets the scene for the geometry of 3-space. Most of the concepts
can be more easily understood and visualized in 2D than in 3D. Specializations of pro-
jective transformations are introduced, including afﬁne and similarity transformations.
Particular attention is focussed on the recovery of afﬁne properties (e.g. parallel lines)
and metric properties (e.g. angles between lines) from a perspective image.

Chapter 3 covers the projective geometry of 3-space. This geometry develops in
much the same manner as that of 2-space, though of course there are extra properties
arising from the additional dimension. The main new geometry here is the plane at
inﬁnity and the absolute conic.

Chapter 4 introduces estimation of geometry from image measurements, which is
one of the main topics of this book. The example of estimating a projective transfor-
mation from point correspondences is used to illustrate the basis and motivation for the
algorithms that will be used throughout the book. The important issue of what should
be minimized in a cost function, e.g. algebraic or geometric or statistical measures, is
described at length. The chapter also introduces the idea of robust estimation, and the
use of such techniques in the automatic estimation of transformations.

Chapter 5 describes how the results of estimation algorithms may be evaluated. In

particular how the covariance of an estimation may be computed.

24

2

Projective Geometry and Transformations of 2D

This chapter introduces the main geometric ideas and notation that are required to un-
derstand the material covered in this book. Some of these ideas are relatively familiar,
such as vanishing point formation or representing conics, whilst others are more es-
oteric, such as using circular points to remove perspective distortion from an image.
These ideas can be understood more easily in the planar (2D) case because they are
more easily visualized here. The geometry of 3-space, which is the subject of the later
parts of this book, is only a simple generalization of this planar case.

In particular, the chapter covers the geometry of projective transformations of the
plane. These transformations model the geometric distortion which arises when a plane
is imaged by a perspective camera. Under perspective imaging certain geometric prop-
erties are preserved, such as collinearity (a straight line is imaged as a straight line),
whilst others are not, for example parallel lines are not imaged as parallel lines in
general. Projective geometry models this imaging and also provides a mathematical
representation appropriate for computations.

We begin by describing the representation of points, lines and conics in homoge-
neous notation, and how these entities map under projective transformations. The line
at inﬁnity and the circular points are introduced, and it is shown that these capture the
afﬁne and metric properties of the plane. Algorithms for rectifying planes are then
given which enable afﬁne and metric properties to be computed from images. We end
with a description of ﬁxed points under projective transformations.

2.1 Planar geometry

The basic concepts of planar geometry are familiar to anyone who has studied math-
ematics even at an elementary level. In fact, they are so much a part of our everyday
experience that we take them for granted. At an elementary level, geometry is the study
of points and lines and their relationships.

To the purist, the study of geometry ought properly to be carried out from a “geomet-
ric” or coordinate-free viewpoint. In this approach, theorems are stated and proved in
terms of geometric primitives only, without the use of algebra. The classical approach
of Euclid is an example of this method. Since Descartes, however, it has been seen that
geometry may be algebraicized, and indeed the theory of geometry may be developed

25

26

2 Projective Geometry and Transformations of 2D

from an algebraic viewpoint. Our approach in this book will be a hybrid approach,
sometimes using geometric, and sometimes algebraic methods. In the algebraic ap-
proach, geometric entities are described in terms of coordinates and algebraic entities.
Thus, for instance a point is identiﬁed with a vector in terms of some coordinate basis.
A line is also identiﬁed with a vector, and a conic section (more brieﬂy, a conic) is
represented by a symmetric matrix. In fact, we often carry this identiﬁcation so far as
to consider that the vector actually is a point, or the symmetric matrix is a conic, at
least for convenience of language. A signiﬁcant advantage of the algebraic approach
to geometry is that results derived in this way may more easily be used to derive algo-
rithms and practical computational methods. Computation and algorithms are a major
concern in this book, which justiﬁes the use of the algebraic method.

2.2 The 2D projective plane

As we all know, a point in the plane may be represented by the pair of coordinates
(x, y) in IR2. Thus, it is common to identify the plane with IR2. Considering IR2 as a
vector space, the coordinate pair (x, y) is a vector – a point is identiﬁed as a vector. In
this section we introduce the homogeneous notation for points and lines on a plane.

Row and column vectors. Later on, we will want to consider linear mappings be-
tween vector spaces, and represent such mappings as matrices. In the usual manner, the
product of a matrix and a vector is another vector, the image under the mapping. This
brings up the distinction between “column” and “row” vectors, since a matrix may be
multiplied on the right by a column and on the left by a row vector. Geometric entities
will by default be represented by column vectors. A bold-face symbol such as x always
represents a column vector, and its transpose is the row vector xT. In accordance with
this convention, a point in the plane will be represented by the column vector (x, y)T,
rather than its transpose, the row vector (x, y). We write x = (x, y)T, both sides of this
equation representing column vectors.

2.2.1 Points and lines
Homogeneous representation of lines. A line in the plane is represented by an equa-
tion such as ax +by +c = 0, different choices of a, b and c giving rise to different lines.
Thus, a line may naturally be represented by the vector (a, b, c)T. The correspondence
between lines and vectors (a, b, c)T is not one-to-one, since the lines ax + by + c = 0
and (ka)x + (kb)y + (kc) = 0 are the same, for any non-zero constant k. Thus, the
vectors (a, b, c)T and k(a, b, c)T represent the same line, for any non-zero k. In fact,
two such vectors related by an overall scaling are considered as being equivalent. An
equivalence class of vectors under this equivalence relationship is known as a homo-
geneous vector. Any particular vector (a, b, c)T is a representative of the equivalence
class. The set of equivalence classes of vectors in IR3 − (0, 0, 0)T forms the projective
space IP2. The notation −(0, 0, 0)T indicates that the vector (0, 0, 0)T, which does not
correspond to any line, is excluded.

2.2 The 2D projective plane

27
Homogeneous representation of points. A point x = (x, y)T lies on the line l =
(a, b, c)T if and only if ax + by + c = 0. This may be written in terms of an inner
product of vectors representing the point as (x, y, 1)(a, b, c)T = (x, y, 1)l = 0; that is
the point (x, y)T in IR2 is represented as a 3-vector by adding a ﬁnal coordinate of 1.
Note that for any non-zero constant k and line l the equation (kx, ky, k)l = 0 if and
only if (x, y, 1)l = 0. It is natural, therefore, to consider the set of vectors (kx, ky, k)T
for varying values of k to be a representation of the point (x, y)T in IR2. Thus, just as
with lines, points are represented by homogeneous vectors. An arbitrary homogeneous
vector representative of a point is of the form x = (x1, x2, x3)T, representing the point
(x1/x3, x2/x3)T in IR2. Points, then, as homogeneous vectors are also elements of IP2.

One has a simple equation to determine when a point lies on a line, namely

Result 2.1. The point x lies on the line l if and only if xTl = 0.
Note that the expression xTl is just the inner or scalar product of the two vectors l
and x. The scalar product xTl = lTx = x.l. In general, the transpose notation lTx
will be preferred, but occasionally, we will use a . to denote the inner product. We
distinguish between the homogeneous coordinates x = (x1, x2, x3)T of a point, which
is a 3-vector, and the inhomogeneous coordinates (x, y)T, which is a 2-vector.

(cid:2)

(cid:2)

(cid:2)

Degrees of freedom (dof). It is clear that in order to specify a point two values must
be provided, namely its x- and y-coordinates. In a similar manner a line is speciﬁed
by two parameters (the two independent ratios {a : b : c}) and so has two degrees
of freedom. For example, in an inhomogeneous representation, these two parameters
could be chosen as the gradient and y intercept of the line.
Intersection of lines. Given two lines l = (a, b, c)T and l(cid:2)
)T, we wish to
ﬁnd their intersection. Deﬁne the vector x = l × l(cid:2)
, where × represents the vector or
cross product. From the triple scalar product identity l.(l × l(cid:2)
) = 0, we
see that lTx = l(cid:2)Tx = 0. Thus, if x is thought of as representing a point, then x lies on
both lines l and l(cid:2)
Result 2.2. The intersection of two lines l and l(cid:2)
Note that the simplicity of this expression for the intersection of the two lines is a direct
consequence of the use of homogeneous vector representations of lines and points.
Example 2.3. Consider the simple problem of determining the intersection of the lines
x = 1 and y = 1. The line x = 1 is equivalent to −1x + 1 = 0, and thus has
homogeneous representation l = (−1, 0, 1)T. The line y = 1 is equivalent to −1y+1 =
= (0,−1, 1)T. From result 2.2 the
0, and thus has homogeneous representation l(cid:2)
intersection point is

, and hence is the intersection of the two lines. This shows:

is the point x = l × l(cid:2)

= (a
, b
) = l(cid:2)

, c
.(l × l(cid:2)

.

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

x = l × l(cid:2)

=

j
0

i
k
−1
1
0 −1 1



(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) =

 1

1
1

which is the inhomogeneous point (1, 1)T as required.

(cid:2)

2 Projective Geometry and Transformations of 2D

28
Line joining points. An expression for the line passing through two points x and x(cid:2)
may be derived by an entirely analogous argument. Deﬁning a line l by l = x × x(cid:2)
, it
may be veriﬁed that both points x and x(cid:2)
Result 2.4. The line through two points x and x(cid:2)

is l = x × x(cid:2)

lie on l. Thus

.

2.2.2 Ideal points and the line at inﬁnity
Intersection of parallel lines. Consider two lines ax+by+c = 0 and ax+by+c
= 0.
These are represented by vectors l = (a, b, c)T and l(cid:2)
)T for which the ﬁrst two
coordinates are the same. Computing the intersection of these lines gives no difﬁculty,
using result 2.2. The intersection is l × l(cid:2)
(cid:2) − c)(b,−a, 0)T, and ignoring the scale
(cid:2) − c), this is the point (b,−a, 0)T.
factor (c
Now if we attempt to ﬁnd the inhomogeneous representation of this point, we ob-
tain (b/0,−a/0)T, which makes no sense, except to suggest that the point of intersec-
tion has inﬁnitely large coordinates. In general, points with homogeneous coordinates
(x, y, 0)T do not correspond to any ﬁnite point in IR2. This observation agrees with the
usual idea that parallel lines meet at inﬁnity.

= (a, b, c

= (c

(cid:2)

(cid:2)

Example 2.5. Consider the two lines x = 1 and x = 2. Here the two lines are parallel,
and consequently intersect “at inﬁnity”. In homogeneous notation the lines are l =
(−1, 0, 1)T, l(cid:2)

= (−1, 0, 2)T, and from result 2.2 their intersection point is

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

x = l × l(cid:2)

=

i
j k
−1 0 1
−1 0 2



(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) =

 0

1
0

which is the point at inﬁnity in the direction of the y-axis.

(cid:2)

Ideal points and the line at inﬁnity. Homogeneous vectors x = (x1, x2, x3)T such
that x3 (cid:5)= 0 correspond to ﬁnite points in IR2. One may augment IR2 by adding points
with last coordinate x3 = 0. The resulting space is the set of all homogeneous 3-
vectors, namely the projective space IP2. The points with last coordinate x3 = 0 are
known as ideal points, or points at inﬁnity. The set of all ideal points may be written
(x1, x2, 0)T, with a particular point speciﬁed by the ratio x1 : x2. Note that this set lies
on a single line, the line at inﬁnity, denoted by the vector l∞ = (0, 0, 1)T. Indeed, one
veriﬁes that (0, 0, 1)(x1, x2, 0)T = 0.
Using result 2.2 one ﬁnds that a line l = (a, b, c)T intersects l∞ in the ideal point
(b,−a, 0)T (since (b,−a, 0)l = 0). A line l(cid:2)
)T parallel to l intersects l∞
in the same ideal point (b,−a, 0)T irrespective of the value of c
. In inhomogeneous
notation (b,−a)T is a vector tangent to the line, and orthogonal to the line normal
(a, b), and so represents the line’s direction. As the line’s direction varies the ideal
point (b,−a, 0)T varies over l∞. For these reasons the line at inﬁnity can be thought of
as the set of directions of lines in the plane.

= (a, b, c

(cid:2)

(cid:2)

Note how the introduction of the concept of points at inﬁnity serves to simplify the
intersection properties of points and lines. In the projective plane IP2, one may state
without qualiﬁcation that two distinct lines meet in a single point and two distinct

2.2 The 2D projective plane

29

x

2

ideal
point

x 1

l

x

O

π

x 3

Fig. 2.1. A model of the projective plane. Points and lines of IP2 are represented by rays and planes,
respectively, through the origin in IR3. Lines lying in the x1x2-plane represent ideal points, and the
x1x2-plane represents l∞.

points lie on a single line. This is not true in the standard Euclidean geometry of IR2,
in which parallel lines form a special case.

The study of the geometry of IP2 is known as projective geometry. In a coordinate-
free purely geometric study of projective geometry, one does not make any distinction
between points at inﬁnity (ideal points) and ordinary points. It will, however, serve
our purposes in this book sometimes to distinguish between ideal points and non-ideal
points. Thus, the line at inﬁnity will at times be considered as a special line in projective
space.

A model for the projective plane. A fruitful way of thinking of IP2 is as a set of
rays in IR3. The set of all vectors k(x1, x2, x3)T as k varies forms a ray through the
origin. Such a ray may be thought of as representing a single point in IP2. In this
model, the lines in IP2 are planes passing through the origin. One veriﬁes that two non-
identical rays lie on exactly one plane, and any two planes intersect in one ray. This
is the analogue of two distinct points uniquely deﬁning a line, and two lines always
intersecting in a point.

Points and lines may be obtained by intersecting this set of rays and planes by the
plane x3 = 1. As illustrated in ﬁgure 2.1 the rays representing ideal points and the
plane representing l∞ are parallel to the plane x3 = 1.

Duality. The reader has probably noticed how the role of points and lines may be
interchanged in statements concerning the properties of lines and points. In particular,
the basic incidence equation lTx = 0 for line and point is symmetric, since lTx = 0
implies xTl = 0, in which the positions of line and point are swapped. Similarly,
result 2.2 and result 2.4 giving the intersection of two lines and the line through two
points are essentially the same, with the roles of points and lines swapped. One may
enunciate a general principle, the duality principle as follows:

30

2 Projective Geometry and Transformations of 2D

Result 2.6. Duality principle. To any theorem of 2-dimensional projective geometry
there corresponds a dual theorem, which may be derived by interchanging the roles of
points and lines in the original theorem.

In applying this principle, concepts of incidence must be appropriately translated as
well. For instance, the line through two points is dual to the point through (that is the
point of intersection of) two lines.

Note that is it not necessary to prove the dual of a given theorem once the original
theorem has been proved. The proof of the dual theorem will be the dual of the proof
of the original theorem.

2.2.3 Conics and dual conics
A conic is a curve described by a second-degree equation in the plane. In Euclidean
geometry conics are of three main types: hyperbola, ellipse, and parabola (apart from
so-called degenerate conics, to be deﬁned later). Classically these three types of conic
arise as conic sections generated by planes of differing orientation (the degenerate con-
ics arise from planes which contain the cone vertex). However, it will be seen that
in 2D projective geometry all non-degenerate conics are equivalent under projective
transformations.

The equation of a conic in inhomogeneous coordinates is

ax2 + bxy + cy2 + dx + ey + f = 0

i.e. a polynomial of degree 2. “Homogenizing” this by the replacements:
x (cid:6)→ x1/x3, y (cid:6)→ x2/x3 gives

ax1

2 + bx1x2 + cx2

2 + dx1x3 + ex2x3 + f x3

2 = 0

or in matrix form

(2.1)

(2.2)

(2.3)

where the conic coefﬁcient matrix C is given by

xTCx = 0

 a

C =

 .

b/2 d/2
e/2
c
f

b/2
d/2 e/2

Note that the conic coefﬁcient matrix is symmetric. As in the case of the homogeneous
representation of points and lines, only the ratios of the matrix elements are important,
since multiplying C by a non-zero scalar does not affect the above equations. Thus C is
a homogeneous representation of a conic. The conic has ﬁve degrees of freedom which
can be thought of as the ratios {a : b : c : d : e : f} or equivalently the six elements of
a symmetric matrix less one for scale.

Five points deﬁne a conic. Suppose we wish to compute the conic which passes
through a set of points, xi. How many points are we free to specify before the conic
is determined uniquely? The question can be answered constructively by providing an

(cid:15)



(cid:16)

 c = 0

2.2 The 2D projective plane

31
algorithm to determine the conic. From (2.1) each point xi places one constraint on the
conic coefﬁcients, since if the conic passes through (xi, yi) then
2 + dxi + eyi + f = 0.

2 + bxiyi + cyi

axi

This constraint can be written as

x2
i xiyi y2

i xi yi 1

c = 0

where c = (a, b, c, d, e, f )T is the conic C represented as a 6-vector.

Stacking the constraints from ﬁve points we obtain

x2
1 x1y1 y2
x2
2 x2y2 y2
x2
3 x3y3 y2
x2
4 x4y4 y2
x2
5 x5y5 y2

1 x1 y1 1
2 x2 y2 1
3 x3 y3 1
4 x4 y4 1
5 x5 y5 1

(2.4)

and the conic is the null vector of this 5 × 6 matrix. This shows that a conic is deter-
mined uniquely (up to scale) by ﬁve points in general position. The method of ﬁtting
a geometric entity (or relation) by determining a null space will be used frequently in
the computation chapters throughout this book.

Tangent lines to conics. The line l tangent to a conic at a point x has a particularly
simple form in homogeneous coordinates:
Result 2.7. The line l tangent to C at a point x on C is given by l = Cx.

Proof. The line l = Cx passes through x, since lTx = xTCx = 0. If l has one-point
contact with the conic, then it is a tangent, and we are done. Otherwise suppose that l
meets the conic in another point y. Then yTCy = 0 and xTCy = lTy = 0. From this
it follows that (x + αy)TC(x + αy) = 0 for all α, which means that the whole line
l = Cx joining x and y lies on the conic C, which is therefore degenerate (see below).

Dual conics. The conic C deﬁned above is more properly termed a point conic, as it
deﬁnes an equation on points. Given the duality result 2.6 of IP2 it is not surprising
that there is also a conic which deﬁnes an equation on lines. This dual (or line) conic
is also represented by a 3 × 3 matrix, which we denote as C∗
. A line l tangent to the
conic C satisﬁes lTC∗l = 0. The notation C∗
indicates that C∗
is the adjoint matrix of C
(the adjoint is deﬁned in section A4.2(p580) of appendix 4(p578)). For a non-singular
symmetric matrix C∗

= C−1 (up to scale).

The equation for a dual conic is straightforward to derive in the case that C has full
rank: From result 2.7, at a point x on C the tangent is l = Cx. Inverting, we ﬁnd the
point x at which the line l is tangent to C is x = C−1l. Since x satisﬁes xTCx = 0 we
obtain (C−1l)TC(C−1l) =l TC−1l = 0, the last step following from C−T = C−1 because
C is symmetric.

Dual conics are also known as conic envelopes, and the reason for this is illustrated

32

2 Projective Geometry and Transformations of 2D

a

b

Fig. 2.2. (a) Points x satisfying xTCx = 0 lie on a point conic. (b) Lines l satisfying lTC∗l = 0 are
tangent to the point conic C. The conic C is the envelope of the lines l.

in ﬁgure 2.2. A dual conic has ﬁve degrees of freedom. In a similar manner to points
deﬁning a point conic, it follows that ﬁve lines in general position deﬁne a dual conic.

Degenerate conics. If the matrix C is not of full rank, then the conic is termed degen-
erate. Degenerate point conics include two lines (rank 2), and a repeated line (rank
1).

Example 2.8. The conic

C = l mT + m lT

is composed of two lines l and m. Points on l satisfy lTx = 0, and are on the conic
since xTCx = (xTl)(mTx) + (xTm)(lTx) = 0. Similarly, points satisfying mTx = 0
also satisfy xTCx = 0. The matrix C is symmetric and has rank 2. The null vector is
x = l × m which is the intersection point of l and m.
(cid:2)
Degenerate line conics include two points (rank 2), and a repeated point (rank 1).
For example, the line conic C∗
= xyT + yxT has rank 2 and consists of lines passing
through either of the two points x and y. Note that for matrices that are not invertible
(C∗

∗ (cid:5)= C.

)

2.3 Projective transformations

In the view of geometry set forth by Felix Klein in his famous “Erlangen Program”,
[Klein-39], geometry is the study of properties invariant under groups of transforma-
tions. From this point of view, 2D projective geometry is the study of properties of
the projective plane IP2 that are invariant under a group of transformations known as
projectivities.

A projectivity is an invertible mapping from points in IP2 (that is homogeneous 3-

vectors) to points in IP2 that maps lines to lines. More precisely,

Deﬁnition 2.9. A projectivity is an invertible mapping h from IP2 to itself such that
three points x1, x2 and x3 lie on the same line if and only if h(x1), h(x2) and h(x3) do.
Projectivities form a group since the inverse of a projectivity is also a projectivity, and
so is the composition of two projectivities. A projectivity is also called a collineation

2.3 Projective transformations

33

(a helpful name), a projective transformation or a homography: the terms are synony-
mous.

In deﬁnition 2.9, a projectivity is deﬁned in terms of a coordinate-free geometric
concept of point line incidence. An equivalent algebraic deﬁnition of a projectivity is
possible, based on the following result.
Theorem 2.10. A mapping h : IP2 → IP2 is a projectivity if and only if there exists a
non-singular 3 × 3 matrix H such that for any point in IP2 represented by a vector x it
is true that h(x) = Hx.

To interpret this theorem, any point in IP2 is represented as a homogeneous 3-vector,
x, and Hx is a linear mapping of homogeneous coordinates. The theorem asserts that
any projectivity arises as such a linear transformation in homogeneous coordinates, and
that conversely any such mapping is a projectivity. The theorem will not be proved in
full here. It will only be shown that any invertible linear transformation of homoge-
neous coordinates is a projectivity.

Proof. Let x1, x2 and x3 lie on a line l. Thus lTxi = 0 for i = 1, . . . ,3 . Let H be a
non-singular 3 × 3 matrix. One veriﬁes that lTH−1Hxi = 0. Thus, the points Hxi all lie
on the line H−Tl, and collinearity is preserved by the transformation.
The converse is considerably harder to prove, namely that each projectivity arises in
this way.

As a result of this theorem, one may give an alternative deﬁnition of a projective

transformation (or collineation) as follows.

Deﬁnition 2.11. Projective transformation. A planar projective transformation is a
linear transformation on homogeneous 3-vectors represented by a non-singular 3 × 3
matrix:

 =

 x

x
x

(cid:2)
(cid:2)
1
(cid:2)
2
3

 h11 h12 h13

h21 h22 h23
h31 h32 h33



 x1

x2
x3

 ,

(2.5)

or more brieﬂy, x(cid:2)

= Hx.

Note that the matrix H occurring in this equation may be changed by multiplication
by an arbitrary non-zero scale factor without altering the projective transformation.
Consequently we say that H is a homogeneous matrix, since as in the homogeneous
representation of a point, only the ratio of the matrix elements is signiﬁcant. There are
eight independent ratios amongst the nine elements of H, and it follows that a projective
transformation has eight degrees of freedom.

A projective transformation projects every ﬁgure into a projectively equivalent ﬁgure,
leaving all its projective properties invariant. In the ray model of ﬁgure 2.1 a projective
transformation is simply a linear transformation of IR3.

34

2 Projective Geometry and Transformations of 2D

O

x /

π /

/

y

/

x

x

π

y

x

Fig. 2.3. Central projection maps points on one plane to points on another plane. The projection
also maps lines to lines as may be seen by considering a plane through the projection centre which inter-
sects with the two planes π and π(cid:1)
. Since lines are mapped to lines, central projection is a projectivity
and may be represented by a linear mapping of homogeneous coordinates x(cid:1) = Hx.

Mappings between planes. As an example of how theorem 2.10 may be applied,
consider ﬁgure 2.3. Projection along rays through a common point (the centre of pro-
jection) deﬁnes a mapping from one plane to another. It is evident that this point-to-
point mapping preserves lines in that a line in one plane is mapped to a line in the other.
If a coordinate system is deﬁned in each plane and points are represented in homoge-
neous coordinates, then the central projection mapping may be expressed by x(cid:2)
= Hx
where H is a non-singular 3× 3 matrix. Actually, if the two coordinate systems deﬁned
in the two planes are both Euclidean (rectilinear) coordinate systems then the mapping
deﬁned by central projection is more restricted than an arbitrary projective transforma-
tion. It is called a perspectivity rather than a full projectivity, and may be represented
by a transformation with six degrees of freedom. We return to perspectivities in section
A7.4(p632).

Example 2.12. Removing the projective distortion from a perspective image of a
plane.

Shape is distorted under perspective imaging. For instance, in ﬁgure 2.4a the win-
dows are not rectangular in the image, although the originals are. In general parallel
lines on a scene plane are not parallel in the image but instead converge to a ﬁnite
point. We have seen that a central projection image of a plane (or section of a plane)
is related to the original plane via a projective transformation, and so the image is a
projective distortion of the original. It is possible to “undo” this projective transforma-
tion by computing the inverse transformation and applying it to the image. The result
will be a new synthesized image in which the objects in the plane are shown with their
correct geometric shape. This will be illustrated here for the front of the building of
ﬁgure 2.4a. Note that since the ground and the front are not in the same plane, the
projective transformation that must be applied to rectify the front is not the same as the
one used for the ground.

Computation of a projective transformation from point-to-point correspondences will
be considered in great detail in chapter 4. For now, a method for computing the trans-

2.3 Projective transformations

35

a

b

Fig. 2.4. Removing perspective distortion. (a) The original image with perspective distortion – the
lines of the windows clearly converge at a ﬁnite point. (b) Synthesized frontal orthogonal view of the
front wall. The image (a) of the wall is related via a projective transformation to the true geometry of the
wall. The inverse transformation is computed by mapping the four imaged window corners to corners
of an appropriately sized rectangle. The four point correspondences determine the transformation. The
transformation is then applied to the whole image. Note that sections of the image of the ground are
subject to a further projective distortion. This can also be removed by a projective transformation.

formation is brieﬂy indicated. One begins by selecting a section of the image corre-
sponding to a planar section of the world. Local 2D image and world coordinates are
selected as shown in ﬁgure 2.3. Let the inhomogeneous coordinates of a pair of match-
ing points x and x(cid:2)
) respectively.
We use inhomogeneous coordinates here instead of the homogeneous coordinates of
the points, because it is these inhomogeneous coordinates that are measured directly
from the image and from the world plane. The projective transformation of (2.5) can
be written in inhomogeneous form as

in the world and image plane be (x, y) and (x

(cid:2)

(cid:2)

, y

(cid:2)

x

=

x
x

(cid:2)
1
(cid:2)
3

(cid:2)
2
(cid:2)
3

=

h11x + h12y + h13
h31x + h32y + h33

,

(cid:2)

y

=

x
x

=

h21x + h22y + h23
h31x + h32y + h33

.

Each point correspondence generates two equations for the elements of H, which

after multiplying out are

(cid:2)
x
(cid:2)
y

(h31x + h32y + h33) =h 11x + h12y + h13
(h31x + h32y + h33) =h 21x + h22y + h23.

These equations are linear in the elements of H. Four point correspondences lead to
eight such linear equations in the entries of H, which are sufﬁcient to solve for H up to
an insigniﬁcant multiplicative factor. The only restriction is that the four points must
be in “general position”, which means that no three points are collinear. The inverse
of the transformation H computed in this way is then applied to the whole image to
undo the effect of perspective distortion on the selected plane. The results are shown
(cid:2)
in ﬁgure 2.4b.

Three remarks concerning this example are appropriate: ﬁrst, the computation of
the rectifying transformation H in this way does not require knowledge of any of the
camera’s parameters or the pose of the plane; second, it is not always necessary to

36

2 Projective Geometry and Transformations of 2D

image 1

 

 

  
 

 


x

 

 


image 2

  

 

R,t

x
 

 

 


 

 


 

 


 

X
 


x
    
 

 
x
 


  

  

  


image 2

image 1

  

  

x

/

x

 

 


 
X

a

 

 


planar surface

b

c

Fig. 2.5. Examples of a projective transformation, x(cid:1) = Hx, arising in perspective images. (a)
The projective transformation between two images induced by a world plane (the concatenation of two
projective transformations is a projective transformation); (b) The projective transformation between
two images with the same camera centre (e.g. a camera rotating about its centre or a camera varying its
focal length); (c) The projective transformation between the image of a plane (the end of the building)
and the image of its shadow onto another plane (the ground plane). Figure (c) courtesy of Luc Van Gool.

know coordinates for four points in order to remove projective distortion: alternative
approaches, which are described in section 2.7, require less, and different types of,
information; third, superior (and preferred) methods for computing projective transfor-
mations are described in chapter 4.

Projective transformations are important mappings representing many more situa-
tions than the perspective imaging of a world plane. A number of other examples are
illustrated in ﬁgure 2.5. Each of these situations is covered in more detail later in the
book.

2.3.1 Transformations of lines and conics
Transformation of lines.
lie on a line l, then the transformed points x(cid:2)
lie on the line l(cid:2)
l(cid:2)Tx(cid:2)
Under the point transformation x(cid:2)

i = lTH−1Hxi = 0. This gives the transformation rule for lines:

= Hx, a line transforms as

It was shown in the proof of theorem 2.10 that if points xi
i = Hxi under a projective transformation
= H−Tl. In this way, incidence of points on lines is preserved, since

l(cid:2)

= H−Tl.

(2.6)
One may alternatively write l(cid:2)T = lTH−1. Note the fundamentally different way
in which lines and points transform. Points transform according to H, whereas lines
(as rows) transform according to H−1. This may be explained in terms of “covariant”
or “contravariant” behaviour. One says that points transform contravariantly and lines
transform covariantly. This distinction will be taken up again, when we discuss tensors
in chapter 15 and is fully explained in appendix 1(p562).

Transformation of conics. Under a point transformation x(cid:2)

= Hx, (2.2) becomes

xTCx = x(cid:2)T[H−1]TCH−1x(cid:2)

= x(cid:2)TH−TCH−1x(cid:2)

2.4 A hierarchy of transformations

37

a

b

c

Fig. 2.6. Distortions arising under central projection. Images of a tiled ﬂoor. (a) Similarity: the
circular pattern is imaged as a circle. A square tile is imaged as a square. Lines which are parallel or
perpendicular have the same relative orientation in the image. (b) Afﬁne: The circle is imaged as an
ellipse. Orthogonal world lines are not imaged as orthogonal lines. However, the sides of the square
tiles, which are parallel in the world are parallel in the image. (c) Projective: Parallel world lines are
imaged as converging lines. Tiles closer to the camera have a larger image than those further away.

which is a quadratic form x(cid:2)TC(cid:2)x(cid:2)
rule for a conic:

with C(cid:2)

= H−TCH−1. This gives the transformation

transformation x(cid:2)

= Hx, a conic C transforms to

= H−TCH−1.

Result 2.13. Under a point
C(cid:2)
The presence of H−1 in this equation may be expressed by saying that a conic transforms
covariantly. The transformation rule for a dual conic is derived in a similar manner.
This gives:
Result 2.14. Under a point transformation x(cid:2)
C∗(cid:2)

= Hx, a dual conic C∗

transforms to

= HC∗HT.

2.4 A hierarchy of transformations

In this section we describe the important specializations of a projective transformation
and their geometric properties. It was shown in section 2.3 that projective transforma-
tions form a group. This group is called the projective linear group, and it will be seen
that these specializations are subgroups of this group.
The group of invertible n × n matrices with real elements is the (real) general linear
group on n dimensions, or GL(n). To obtain the projective linear group the matrices
related by a scalar multiplier are identiﬁed, giving P L(n) (this is a quotient group of
GL(n)). In the case of projective transformations of the plane n = 3.

The important subgroups of P L(3) include the afﬁne group, which is the subgroup
of P L(3) consisting of matrices for which the last row is (0, 0, 1), and the Euclidean
group, which is a subgroup of the afﬁne group for which in addition the upper left hand
2 × 2 matrix is orthogonal. One may also identify the oriented Euclidean group in
which the upper left hand 2 × 2 matrix has determinant 1.
We will introduce these transformations starting from the most specialized, the
isometries, and progressively generalizing until projective transformations are reached.

38

2 Projective Geometry and Transformations of 2D

This deﬁnes a hierarchy of transformations. The distortion effects of various transfor-
mations in this hierarchy are shown in ﬁgure 2.6.

Some transformations of interest are not groups, for example, perspectivities (be-
cause the composition of two perspectivities is a projectivity, not a perspectivity). This
point is covered in section A7.4(p632).

Invariants. An alternative to describing the transformation algebraically, i.e. as a ma-
trix acting on coordinates of a point or curve, is to describe the transformation in terms
of those elements or quantities that are preserved or invariant. A (scalar) invariant of a
geometric conﬁguration is a function of the conﬁguration whose value is unchanged by
a particular transformation. For example, the separation of two points is unchanged by
a Euclidean transformation (translation and rotation), but not by a similarity (e.g. trans-
lation, rotation and isotropic scaling). Distance is thus a Euclidean, but not similarity
invariant. The angle between two lines is both a Euclidean and a similarity invariant.

2.4.1 Class I: Isometries
Isometries are transformations of the plane IR2 that preserve Euclidean distance (from
iso = same, metric = measure). An isometry is represented as

 x

(cid:2)
(cid:2)

y
1

 =

  cos θ − sin θ tx

 sin θ

cos θ



 x



0

0

ty
1

y
1

(cid:17)

(cid:18)

t
R
0T 1

where  = ±1. If  = 1 then the isometry is orientation-preserving and is a Euclidean
transformation (a composition of a translation and rotation). If  = −1 then the isome-
try reverses orientation. An example is the composition of a reﬂection, represented by
the matrix diag(−1, 1, 1), with a Euclidean transformation.
Euclidean transformations model the motion of a rigid object. They are by far the
most important isometries in practice, and we will concentrate on these. However, the
orientation reversing isometries often arise as ambiguities in structure recovery.

A planar Euclidean transformation can be written more concisely in block form as

x(cid:2)

= HEx =

(2.7)
where R is a 2 × 2 rotation matrix (an orthogonal matrix such that RTR = RRT = I),
t a translation 2-vector, and 0 a null 2-vector. Special cases are a pure rotation (when
t = 0) and a pure translation (when R = I). A Euclidean transformation is also known
as a displacement.

x

A planar Euclidean transformation has three degrees of freedom, one for the rotation
and two for the translation. Thus three parameters must be speciﬁed in order to deﬁne
the transformation. The transformation can be computed from two point correspon-
dences.

Invariants. The invariants are very familiar, for instance: length (the distance be-
tween two points), angle (the angle between two lines), and area.

2.4 A hierarchy of transformations

39

Groups and orientation. An isometry is orientation-preserving if the upper left
hand 2 × 2 matrix has determinant 1. Orientation-preserving isometries form a group,
orientation-reversing ones do not. This distinction applies also in the case of similarity
and afﬁne transformations which now follow.

2.4.2 Class II: Similarity transformations
A similarity transformation (or more simply a similarity) is an isometry composed with
an isotropic scaling. In the case of a Euclidean transformation composed with a scaling
(i.e. no reﬂection) the similarity has matrix representation

 .



 x

y
1

 =

 x

(cid:2)
(cid:2)

y
1

 s cos θ −s sin θ tx
(cid:18)

s cos θ

s sin θ

ty
1

(cid:17)

0

0

x(cid:2)

= HSx =

sR t
0T 1

x

This can be written more concisely in block form as

(2.8)

(2.9)

where the scalar s represents the isotropic scaling. A similarity transformation is also
known as an equi-form transformation, because it preserves “shape” (form). A planar
similarity transformation has four degrees of freedom, the scaling accounting for one
more degree of freedom than a Euclidean transformation. A similarity can be computed
from two point correspondences.

Invariants. The invariants can be constructed from Euclidean invariants with suitable
provision being made for the additional scaling degree of freedom. Angles between
lines are not affected by rotation, translation or isotropic scaling, and so are similarity
invariants. In particular parallel lines are mapped to parallel lines. The length between
two points is not a similarity invariant, but the ratio of two lengths is an invariant,
because the scaling of the lengths cancels out. Similarly a ratio of areas is an invariant
because the scaling (squared) cancels out.

Metric structure. A term that will be used frequently in the discussion on reconstruc-
tion (chapter 10) is metric. The description metric structure implies that the structure
is deﬁned up to a similarity.

2.4.3 Class III: Afﬁne transformations
An afﬁne transformation (or more simply an afﬁnity) is a non-singular linear transfor-
mation followed by a translation. It has the matrix representation

 x

(cid:2)
(cid:2)

y
1

 =

 a11 a12

a21 a22
0
0



 x



y
1

tx
ty
1

(2.10)

40

2 Projective Geometry and Transformations of 2D

θ

rotation

a

φ

deformation

b

Fig. 2.7. Distortions arising from a planar afﬁne transformation. (a) Rotation by R(θ). (b) A defor-
mation R(−φ) D R(φ). Note, the scaling directions in the deformation are orthogonal.

(cid:17)

(cid:18)

t
A
0T 1

x

or in block form

x(cid:2)

= HAx =

(2.11)
with A a 2 × 2 non-singular matrix. A planar afﬁne transformation has six degrees of
freedom corresponding to the six matrix elements. The transformation can be com-
puted from three point correspondences.

A helpful way to understand the geometric effects of the linear component A of
an afﬁne transformation is as the composition of two fundamental transformations,
namely rotations and non-isotropic scalings. The afﬁne matrix A can always be decom-
posed as

(2.12)

A = R(θ) R(−φ) D R(φ)

(cid:17)

(cid:18)

where R(θ) and R(φ) are rotations by θ and φ respectively, and D is a diagonal matrix:

D =

0
λ1
0 λ2

.

This decomposition follows directly from the SVD (section A4.4(p585)): writing A =
UDVT = (UVT)(VDVT) = R(θ) (R(−φ) D R(φ)), since U and V are orthogonal matrices.
The afﬁne matrix A is hence seen to be the concatenation of a rotation (by φ); a
scaling by λ1 and λ2 respectively in the (rotated) x and y directions; a rotation back
(by −φ); and ﬁnally another rotation (by θ). The only “new” geometry, compared to
a similarity, is the non-isotropic scaling. This accounts for the two extra degrees of
freedom possessed by an afﬁnity over a similarity. They are the angle φ specifying the
scaling direction, and the ratio of the scaling parameters λ1 : λ2. The essence of an
afﬁnity is this scaling in orthogonal directions, oriented at a particular angle. Schematic
examples are given in ﬁgure 2.7.

2.4 A hierarchy of transformations

41

Invariants. Because an afﬁne transformation includes non-isotropic scaling, the sim-
ilarity invariants of length ratios and angles between lines are not preserved under an
afﬁnity. Three important invariants are:

(i) Parallel lines. Consider two parallel lines. These intersect at a point
(x1, x2, 0)T at inﬁnity. Under an afﬁne transformation this point is mapped
to another point at inﬁnity. Consequently, the parallel lines are mapped to lines
which still intersect at inﬁnity, and so are parallel after the transformation.

(ii) Ratio of lengths of parallel line segments. The length scaling of a line seg-
ment depends only on the angle between the line direction and scaling direc-
tions. Suppose the line is at angle α to the x-axis of the orthogonal scaling
2 sin2 α. This scaling is
direction, then the scaling magnitude is
common to all lines with the same direction, and so cancels out in a ratio of
parallel segment lengths.

λ2
1 cos2 α + λ2

(cid:19)

(iii) Ratio of areas. This invariance can be deduced directly from the decomposi-
tion (2.12). Rotations and translations do not affect area, so only the scalings by
λ1 and λ2 matter here. The effect is that area is scaled by λ1λ2 which is equal to
det A. Thus the area of any shape is scaled by det A, and so the scaling cancels
out for a ratio of areas. It will be seen that this does not hold for a projective
transformation.

An afﬁnity is orientation-preserving or -reversing according to whether det A is positive
or negative respectively. Since det A = λ1λ2 the property depends only on the sign of
the scalings.

2.4.4 Class IV: Projective transformations
A projective transformation was deﬁned in (2.5). It is a general non-singular linear
transformation of homogeneous coordinates. This generalizes an afﬁne transformation,
which is the composition of a general non-singular linear transformation of inhomoge-
neous coordinates and a translation. We have earlier seen the action of a projective
transformation (in section 2.3). Here we examine its block form

(cid:17)

(cid:18)

x(cid:2)

= HPx =

t
A
vT v

x

(2.13)

where the vector v = (v1, v2)T. The matrix has nine elements with only their ratio
signiﬁcant, so the transformation is speciﬁed by eight parameters. Note, it is not always
possible to scale the matrix such that v is unity since v might be zero. A projective
transformation between two planes can be computed from four point correspondences,
with no three collinear on either plane. See ﬁgure 2.4.

Unlike the case of afﬁnities, it is not possible to distinguish between orientation
preserving and orientation reversing projectivities in IP2. We will return to this point
in section 2.6.

42

2 Projective Geometry and Transformations of 2D

Invariants.
The most fundamental projective invariant is the cross ratio of four
collinear points: a ratio of lengths on a line is invariant under afﬁnities, but not un-
der projectivities. However, a ratio of ratios or cross ratio of lengths on a line is a
projective invariant. We return to properties of this invariant in section 2.5.

2.4.5 Summary and comparison
Afﬁnities (6 dof) occupy the middle ground between similarities (4 dof) and projectivi-
ties (8 dof). They generalize similarities in that angles are not preserved, so that shapes
are skewed under the transformation. On the other hand their action is homogeneous
over the plane: for a given afﬁnity the det A scaling in area of an object (e.g. a square)
is the same anywhere on the plane; and the orientation of a transformed line depends
only on its initial orientation, not on its position on the plane. In contrast, for a given
projective transformation, area scaling varies with position (e.g. under perspective a
more distant square on the plane has a smaller image than one that is nearer, as in
ﬁgure 2.6); and the orientation of a transformed line depends on both the orientation
and position of the source line (however, it will be seen later in section 8.6(p213) that
a line’s vanishing point depends only on line orientation, not position).

The key difference between a projective and afﬁne transformation is that the vector
v is not null for a projectivity. This is responsible for the non-linear effects of the
projectivity. Compare the mapping of an ideal point (x1, x2, 0)T under an afﬁnity and
projectivity: First the afﬁne transformation

 =
 A
(cid:18) x1
 A
 =
(cid:18) x1

x2
0

(cid:20)

(cid:21)

x1
x2
0

x1
x2

(cid:20)

(cid:21)

 .
 .

v1x1 + v2x2

t
A
vT v

x2
0

(2.14)

(2.15)

t
A
0T 1

(cid:17)

(cid:17)

Second the projective transformation

In the ﬁrst case the ideal point remains ideal (i.e. at inﬁnity). In the second it is mapped
to a ﬁnite point. It is this ability which allows a projective transformation to model
vanishing points.

2.4.6 Decomposition of a projective transformation
A projective transformation can be decomposed into a chain of transformations, where
each matrix in the chain represents a transformation higher in the hierarchy than the
previous one.

(cid:17)

(cid:18)(cid:17)

(cid:18)(cid:17)

(cid:18)

(cid:17)

(cid:18)

H = HS HA HP =

sR t
0T 1

K 0
0T 1

0
I
vT v

=

t
A
vT v

(2.16)

with A a non-singular matrix given by A = sRK + tvT, and K an upper-triangular matrix
normalized as det K = 1. This decomposition is valid provided v (cid:5)= 0, and is unique if
s is chosen positive.

2.4 A hierarchy of transformations

43

Each of the matrices HS, HA, HP is the “essence” of a transformation of that type (as
indicated by the subscripts S, A, P). Consider the process of rectifying the perspective
image of a plane as in example 2.12: HP (2 dof) moves the line at inﬁnity; HA (2 dof)
affects the afﬁne properties, but does not move the line at inﬁnity; and ﬁnally, HS is a
general similarity transformation (4 dof) which does not affect the afﬁne or projective
properties. The transformation HP is an elation, described in section A7.3(p631).

Example 2.15. The projective transformation

H =

2.707 8.242 2.0
1.0
1.0

 1.707 0.586 1.0

 0.5 1 0



2.0

0
0

2 0
0 1

1
2
1

may be decomposed as

 2 cos 45

◦ −2 sin 45
◦
◦
◦
2 sin 45
2 cos 45

0

0

H =



 1 0 0

0 1 0
1 2 1

 .

(cid:2)

This decomposition can be employed when the objective is to only partially deter-
mine the transformation. For example, if one wants to measure length ratios from the
perspective image of a plane, then it is only necessary to determine (rectify) the trans-
formation up to a similarity. We return to this approach in section 2.7.
S . Since H−1
A H−1
P , H−1
(cid:18)
(cid:18)(cid:17)

S are
still projective, afﬁne and similarity transformations respectively, a general projective
transformation may also be decomposed in the form

Taking the inverse of H in (2.16) gives H−1 = H−1
(cid:18)(cid:17)

A and H−1

P H−1

(cid:17)

H = HP HA HS =

0
I
vT 1

K 0
0T 1

sR t
0T 1

(2.17)

Note that the actual values of K, R, t and v will be different from those of (2.16).

2.4.7 The number of invariants
The question naturally arises as to how many invariants there are for a given geometric
conﬁguration under a particular transformation. First the term “number” needs to be
made more precise, for if a quantity is invariant, such as length under Euclidean trans-
formations, then any function of that quantity is invariant. Consequently, we seek a
counting argument for the number of functionally independent invariants. By consid-
ering the number of transformation parameters that must be eliminated in order to form
an invariant, it can be seen that:

Result 2.16. The number of functionally independent invariants is equal to, or greater
than, the number of degrees of freedom of the conﬁguration less the number of degrees
of freedom of the transformation.

44

2 Projective Geometry and Transformations of 2D

(cid:17)

(cid:17)
(cid:17)
(cid:17)

Group

Projective
8 dof

Afﬁne
6 dof

Similarity
4 dof

Euclidean
3 dof

Matrix

h11 h12 h13
h21 h22 h23
h31 h32 h33

a11 a12
a21 a22
0
0

tx
ty
1

sr11
sr21
0

r11
r21
0

sr12
sr22
0

r12
r22
0

tx
ty
1

tx
ty
1

(cid:18)

(cid:18)
(cid:18)
(cid:18)

Distortion

Invariant properties

Concurrency, collinearity, order of contact:
intersection (1 pt contact); tangency (2 pt con-
tact); inﬂections
(3 pt contact with line); tangent discontinuities
and cusps. cross ratio (ratio of ratio of lengths).

Parallelism, ratio of areas, ratio of lengths on
collinear or parallel lines (e.g. midpoints), lin-
ear combinations of vectors (e.g. centroids).
The line at inﬁnity, l∞.

Ratio of lengths, angle. The circular points, I, J
(see section 2.7.3).

Length, area

Table 2.1. Geometric properties invariant to commonly occurring planar transformations. The
matrix A = [aij] is an invertible 2 × 2 matrix, R = [rij] is a 2D rotation matrix, and (tx, ty) a 2D trans-
lation. The distortion column shows typical effects of the transformations on a square. Transformations
higher in the table can produce all the actions of the ones below. These range from Euclidean, where
only translations and rotations occur, to projective where the square can be transformed to any arbitrary
quadrilateral (provided no three points are collinear).

For example, a conﬁguration of four points in general position has 8 degrees of freedom
(2 for each point), and so 4 similarity, 2 afﬁnity and zero projective invariants since
these transformations have respectively 4, 6 and 8 degrees of freedom.

Table 2.1 summarizes the 2D transformation groups and their invariant properties.
Transformations lower in the table are specializations of those above. A transformation
lower in the table inherits the invariants of those above.

2.5 The projective geometry of 1D

The development of the projective geometry of a line, IP1, proceeds in much the same
way as that of the plane. A point x on the line is represented by homogeneous coordi-
nates (x1, x2)T, and a point for which x2 = 0 is an ideal point of the line. We will use
the notation ¯x to represent the 2-vector (x1, x2)T. A projective transformation of a line
is represented by a 2 × 2 homogeneous matrix,
= H2×2¯x

¯x(cid:2)

and has 3 degrees of freedom corresponding to the four elements of the matrix less one
for overall scaling. A projective transformation of a line may be determined from three
corresponding points.

2.5 The projective geometry of 1D

45

Fig. 2.8. Projective transformations between lines. There are four sets of four collinear points in this
ﬁgure. Each set is related to the others by a line-to-line projectivity. Since the cross ratio is an invariant
under a projectivity, the cross ratio has the same value for all the sets shown.

The cross ratio. The cross ratio is the basic projective invariant of IP1. Given 4 points
¯xi the cross ratio is deﬁned as

Cross(¯x1, ¯x2, ¯x3, ¯x4) =

|¯x1¯x2||¯x3¯x4|
|¯x1¯x3||¯x2¯x4|

(cid:18)

(cid:17)

xi1 xj1
xi2 xj2

.

where

|¯xi¯xj| = det

A few comments on the cross ratio:

(i) The value of the cross ratio is not dependent on which particular homogeneous
representative of a point ¯xi is used, since the scale cancels between numerator
and denominator.
such that x2 = 1, then |¯xi¯xj| represents the signed distance from ¯xi to ¯xj.

(ii) If each point ¯xi is a ﬁnite point and the homogeneous representative is chosen

(iii) The deﬁnition of the cross ratio is also valid if one of the points ¯xi is an ideal

point.
the line: if ¯x(cid:2)

(iv) The value of the cross ratio is invariant under any projective transformation of

= H2×2¯x then
1, ¯x(cid:2)
Cross(¯x(cid:2)

2, ¯x(cid:2)

3, ¯x(cid:2)

4) = Cross(¯x1, ¯x2, ¯x3, ¯x4).

(2.18)

The proof is left as an exercise. Equivalently stated, the cross ratio is invariant
to the projective coordinate frame chosen for the line.

Figure 2.8 illustrates a number of projective transformations between lines with equiv-
alent cross ratios.

Under a projective transformation of the plane, a 1D projective transformation is

induced on any line in the plane.

Concurrent lines. A conﬁguration of concurrent lines is dual to collinear points on
a line. This means that concurrent lines on a plane also have the geometry IP1. In
particular four concurrent lines have a cross ratio as illustrated in ﬁgure 2.9a.

46

l

2 Projective Geometry and Transformations of 2D

x 1

x

2

l

1

l

2

x

3

x

4

a

l

3

l

4

l

x 1

x

1

x

2

c

x2

x 3

x 4

x

3

x

4

b

Fig. 2.9. Concurrent lines. (a) Four concurrent lines li intersect the line l in the four points ¯xi. The
cross ratio of these lines is an invariant to projective transformations of the plane. Its value is given
by the cross ratio of the points, Cross(¯x1, ¯x2, ¯x3, ¯x4). (b) Coplanar points xi are imaged onto a line l
(also in the plane) by a projection with centre c. The cross ratio of the image points ¯xi is invariant to
the position of the image line l.

Note how ﬁgure 2.9b may be thought of as representing projection of points in IP2
into a 1-dimensional image. In particular, if c represents a camera centre, and the line
l represents an image line (1D analogue of the image plane), then the points ¯xi are the
projections of points xi into the image. The cross ratio of the points ¯xi characterizes
the projective conﬁguration of the four image points. Note that the actual position
of the image line is irrelevant as far as the projective conﬁguration of the four image
points is concerned – different choices of image line give rise to projectively equivalent
conﬁgurations of image points.

The projective geometry of concurrent lines is important to the understanding of the

projective geometry of epipolar lines in chapter 9.

2.6 Topology of the projective plane

We make brief mention of the topology of IP2. Understanding of this section is not
required for following the rest of the book.

2 + x2

1 + x2

We have seen that the projective plane IP2 may be thought of as the set of all ho-
mogeneous 3-vectors. A vector of this type x = (x1, x2, x3)T may be normalized by
multiplication by a non-zero factor so that x2
3 = 1. Such a point lies on the
unit sphere in IR3. However, any vector x and −x represent the same point in IP2, since
they differ by a multiplicative factor, −1. Thus, there is a two-to-one correspondence
between the unit sphere S2 in IR3 and the projective plane IP2. The projective plane
may be pictured as the unit sphere with opposite points identiﬁed. In this representa-
tion, a line in IP2 is modelled as a great circle on the unit sphere (as ever, with opposite
points identiﬁed). One may verify that any two distinct (non-antipodal) points on the
sphere lie on exactly one great circle, and any two great circles intersect in one point
(since antipodal points are identiﬁed).

In the language of topology, the sphere S2 is a 2-sheeted covering space of IP2. This
implies that IP2 is not simply-connected, which means that there are loops in IP2 which
cannot be contracted to a point inside IP2. To be technical, the fundamental group of
IP2 is the cyclic group of order 2.

2.7 Recovery of afﬁne and metric properties from images

47

a

b

c

d

Fig. 2.10. Topology of surfaces. Common surfaces may be constructed from a paper square (topo-
logically a disk) with edges glued together. In each case, the matching arrow edges of the square are to
be glued together in such a way that the directions of the arrows match. One obtains (a) a sphere, (b)
a torus, (c) a Klein bottle and (d) a projective plane. Only the sphere and torus are actually realizable
with a real sheet of paper. The sphere and torus are orientable but the projective plane and Klein bottle
are not.

In the model for the projective plane as a sphere with opposite points identiﬁed one
may dispense with the lower hemisphere of S2, since points in this hemisphere are
the same as the opposite points in the upper hemisphere.
In this case, IP2 may be
constructed from the upper hemisphere by identifying opposite points on the equator.
Since the upper hemisphere of S2 is topologically the same as a disk, IP2 is simply
a disk with opposite points on its boundary identiﬁed, or glued together. This is not
physically possible. Constructing topological spaces by gluing the boundary of a disk
is a common method in topology, and in fact any 2-manifold may be constructed in this
way. This is illustrated in ﬁgure 2.10.

A notable feature of the projective plane IP2 is that it is non-orientable. This means
that it is impossible to deﬁne a local orientation (represented for instance by a pair of
oriented coordinate axes) that is consistent over the whole surface. This is illustrated
in ﬁgure 2.11 in which it is shown that the projective plane contains an orientation-
reversing path.

The topology of IP1.
In a similar manner, the 1-dimensional projective line may be
identiﬁed as a 1-sphere S1 (that is, a circle) with opposite points identiﬁed. If we omit
the lower half of the circle, as being duplicated by the top half, then the top half of a
circle is topologically equivalent to a line segment. Thus IP1 is topologically equivalent
to a line segment with the two endpoints identiﬁed – namely a circle, S1.

2.7 Recovery of afﬁne and metric properties from images

We return to the example of projective rectiﬁcation of example 2.12(p34) where the
aim was to remove the projective distortion in the perspective image of a plane to the
extent that similarity properties (angles, ratios of lengths) could be measured on the
original plane. In that example the projective distortion was completely removed by
specifying the position of four reference points on the plane (a total of 8 degrees of
freedom), and explicitly computing the transformation mapping the reference points to
their images. In fact this overspeciﬁes the geometry – a projective transformation has
only 4 degrees of freedom more than a similarity, so it is only necessary to specify 4

48

2 Projective Geometry and Transformations of 2D

a

b

Fig. 2.11. Orientation of surfaces. A coordinate frame (represented by an L in the diagram) may
be transported along a path in the surface eventually coming back to the point where it started. (a)
represents a projective plane. In the path shown, the coordinate frame (represented by a pair of axes) is
reversed when it returns to the same point, since the identiﬁcation at the boundary of the square swaps
the direction of one of the axes. Such a path is called an orientation-reversing path, and a surface that
contains such a path is called non-orientable. (b) shows the well known example of a M¨obius strip
obtained by joining two opposite edges of a rectangle (M.C. Escher’s “Moebius Strip II [Red Ants]”,
1963. c(cid:4)2000 Cordon Art B.V. – Baarn-Holland. All rights reserved). As can be veriﬁed, a path once
around the strip is orientation-reversing.

degrees of freedom (not 8) in order to determine metric properties. In projective geom-
etry these 4 degrees of freedom are given “physical substance” by being associated with
geometric objects: the line at inﬁnity l∞ (2 dof), and the two circular points (2 dof)
on l∞. This association is often a more intuitive way of reasoning about the problem
than the equivalent description in terms of specifying matrices in the decomposition
chain (2.16).

In the following it is shown that the projective distortion may be removed once the
image of l∞ is speciﬁed, and the afﬁne distortion removed once the image of the circu-
lar points is speciﬁed. Then the only remaining distortion is a similarity.

2.7.1 The line at inﬁnity
Under a projective transformation ideal points may be mapped to ﬁnite points (2.15),
and consequently l∞ is mapped to a ﬁnite line. However, if the transformation is an
afﬁnity, then l∞ is not mapped to a ﬁnite line, but remains at inﬁnity. This is evident
directly from the line transformation (2.6–p36):

(cid:17)

l(cid:2)
∞ = H−T

A l∞ =

A−T
0
−tTA−T 1

(cid:18) 0

0
1

 =

 0

0
1

 = l∞.

The converse is also true, i.e. an afﬁne transformation is the most general linear trans-
formation that ﬁxes l∞, and may be seen as follows. We require that a point at inﬁnity,
say x = (1, 0, 0)T, be mapped to a point at inﬁnity. This requires that h31 = 0. Simi-
larly, h32 = 0, so the transformation is an afﬁnity. To summarize,
Result 2.17. The line at inﬁnity, l∞, is a ﬁxed line under the projective transformation
H if and only if H is an afﬁnity.

However, l∞ is not ﬁxed pointwise under an afﬁne transformation: (2.14) showed
that under an afﬁnity a point on l∞ (an ideal point) is mapped to a point on l∞, but

2.7 Recovery of afﬁne and metric properties from images

49

H

P

/
H
P

l = H  (    )

l

P

π

1

π

2

π

3

H

A

Fig. 2.12. Afﬁne rectiﬁcation. A projective transformation maps l∞ from (0, 0, 1)T on a Euclidean
plane π1 to a ﬁnite line l on the plane π2. If a projective transformation is constructed such that l is
mapped back to (0, 0, 1)T then from result 2.17 the transformation between the ﬁrst and third planes
must be an afﬁne transformation since the canonical position of l∞ is preserved. This means that afﬁne
properties of the ﬁrst plane can be measured from the third, i.e. the third plane is within an afﬁnity of the
ﬁrst.

it is not the same point unless A(x1, x2)T = k(x1, x2)T. It will now be shown that
identifying l∞ allows the recovery of afﬁne properties (parallelism, ratio of areas).

2.7.2 Recovery of afﬁne properties from images
Once the imaged line at inﬁnity is identiﬁed in an image of a plane, it is then possible to
make afﬁne measurements on the original plane. For example, lines may be identiﬁed
as parallel on the original plane if the imaged lines intersect on the imaged l∞. This
follows because parallel lines on the Euclidean plane intersect on l∞, and after a pro-
jective transformation the lines still intersect on the imaged l∞ since intersections are
preserved by projectivities. Similarly, once l∞ is identiﬁed a length ratio on a line may
be computed from the cross ratio of the three points specifying the lengths together
with the intersection of the line with l∞ (which provides the fourth point for the cross
ratio), and so forth.

However, a less tortuous path which is better suited to computational algorithms is
simply to transform the identiﬁed l∞ to its canonical position of l∞ = (0, 0, 1)T. The
(projective) matrix which achieves this transformation can be applied to every point
in the image in order to afﬁnely rectify the image, i.e. after the transformation, afﬁne
measurements can be made directly from the rectiﬁed image. The key idea here is
illustrated in ﬁgure 2.12.
If the imaged line at inﬁnity is the line l = (l1, l2, l3)T, then provided l3 (cid:5)= 0 a suitable
projective point transformation which will map l back to l∞ = (0, 0, 1)T is

 1

0
l1

H = HA



0
1
l2

0
0
l3

(2.19)

50

2 Projective Geometry and Transformations of 2D

a

b

c

Fig. 2.13. Afﬁne rectiﬁcation via the vanishing line. The vanishing line of the plane imaged in (a) is
computed (c) from the intersection of two sets of imaged parallel lines. The image is then projectively
warped to produce the afﬁnely rectiﬁed image (b). In the afﬁnely rectiﬁed image parallel lines are now
parallel. However, angles do not have their veridical world value since they are afﬁnely distorted. See
also ﬁgure 2.17.

where HA is any afﬁne transformation (the last row of H is lT). One can verify that under
the line transformation (2.6–p36) H−T(l1, l2, l3)T = (0, 0, 1)T = l∞.

Example 2.18. Afﬁne rectiﬁcation
In a perspective image of a plane, the line at inﬁnity on the world plane is imaged as the
vanishing line of the plane. This is discussed in more detail in chapter 8. As illustrated
in ﬁgure 2.13 the vanishing line l may be computed by intersecting imaged parallel
lines. The image is then rectiﬁed by applying a projective warping (2.19) such that l is
(cid:2)
mapped to its canonical position l∞ = (0, 0, 1)T.

This example shows that afﬁne properties may be recovered by simply specifying a
line (2 dof). It is equivalent to specifying only the projective component of the trans-
formation decomposition chain (2.16). Conversely if afﬁne properties are known, these
may be used to determine points and the line at inﬁnity. This is illustrated in the fol-
lowing example.

Example 2.19. Computing a vanishing point from a length ratio. Given two in-
tervals on a line with a known length ratio, the point at inﬁnity on the line may be
determined. A typical case is where three points a(cid:2)
are identiﬁed on a line in
an image. Suppose a, b and c are the corresponding collinear points on the world line,
and the length ratio d(a, b) : d(b, c) =a : b is known (where d(x, y) is the Euclidean

and c(cid:2)

, b(cid:2)

2.7 Recovery of afﬁne and metric properties from images

51

Fig. 2.14. Two examples of using equal length ratios on a line to determine the point at inﬁnity. The
line intervals used are shown as the thin and thick white lines delineated by points. This construction
determines the vanishing line of the plane. Compare with ﬁgure 2.13c.

distance between the points x and y). It is possible to ﬁnd the vanishing point using
the cross ratio. Equivalently, one may proceed as follows:

(cid:2)

(cid:2)

.

, c(cid:2)

) = a

: b

, b(cid:2)

) : d(b(cid:2)

(i) Measure the distance ratio in the image, d(a(cid:2)
(ii) Points a, b and c may be represented as coordinates 0, a and a+b in a coordinate
frame on the line (cid:8)a, b, c(cid:9). For computational purposes, these points are rep-
resented by homogeneous 2-vectors (0, 1)T, (a, 1)T and (a + b, 1)T. Similarly,
a(cid:2)
, b(cid:2)
, which may also be expressed as
homogeneous vectors.
H2×2 mapping a (cid:6)→ a(cid:2)
, b (cid:6)→ b(cid:2)
vanishing point on the line (cid:8)a(cid:2)

(iv) The image of the point at inﬁnity (with coordinates (1, 0)T) under H2×2 is the

(iii) Relative to these coordinate frames, compute the 1D projective transformation

and c (cid:6)→ c(cid:2)
, c(cid:2)(cid:9).
, b(cid:2)

have coordinates 0, a

(cid:2)
and a

and c(cid:2)

(cid:2)

+ b

(cid:2)

.

An example of vanishing points computed in this manner is shown in ﬁgure 2.14. (cid:2)

Example 2.20. Geometric construction of vanishing points from a length ratio.
The vanishing points shown in ﬁgure 2.14 may also be computed by a purely geometric
construction consisting of the following steps:
, b(cid:2)

three collinear points, a(cid:2)

, in an image corresponding to

(i) Given:
(ii) Draw any line l through a(cid:2)

and c(cid:2)
collinear world points with interval ratio a : b.
points a = a(cid:2)
a : b.

(iii) Join bb(cid:2)
(iv) The line through o parallel to l meets the line a(cid:2)c(cid:2)

and intersect in o.

and cc(cid:2)

), and mark off
, b and c such that the line segments (cid:8)ab(cid:9), (cid:8)bc(cid:9) have length ratio

(not coincident with the line a(cid:2)c(cid:2)

This construction is illustrated in ﬁgure 2.15.

in the vanishing point v(cid:2)

.

(cid:2)

52

2 Projective Geometry and Transformations of 2D

o

c /

b/

v/

a/
a

a

b

b

c

l

Fig. 2.15. A geometric construction to determine the image of the point at inﬁnity on a line given a
known length ratio. The details are given in the text.

2.7.3 The circular points and their dual
Under any similarity transformation there are two points on l∞ which are ﬁxed. These
are the circular points (also called the absolute points) I, J, with canonical coordinates



 1

i
0

I =

J =

 1−i

 .

0

The circular points are a pair of complex conjugate ideal points. To see that they are
ﬁxed under an orientation-preserving similarity:

I(cid:2)

= HSI

=

 s cos θ −s sin θ tx
 = I
 1

s cos θ

s sin θ

ty
1

−iθ

0

0



 1



i
0

= se

i
0

with an analogous proof for J. A reﬂection swaps I and J. The converse is also true,
i.e. if the circular points are ﬁxed then the linear transformation is a similarity. The
proof is left as an exercise. To summarize,

Result 2.21. The circular points, I, J, areﬁxed points under the projective transforma-
tion H if and only if H is a similarity.

The name “circular points” arises because every circle intersects l∞ at the circular
points. To see this, start from equation (2.1–p30) for a conic. In the case that the conic
is a circle: a = c and b = 0. Then

x2
1 + x2

2 + dx1x3 + ex2x3 + f x2

3 = 0

2.7 Recovery of afﬁne and metric properties from images

53
where a has been set to unity. This conic intersects l∞ in the (ideal) points for which
x3 = 0, namely

x2
1 + x2

2 = 0

with solution I = (1, i,0) T, J = (1,−i, 0)T, i.e. any circle intersects l∞ in the circular
points. In Euclidean geometry it is well known that a circle is speciﬁed by three points.
The circular points enable an alternative computation. A circle can be computed using
the general formula for a conic deﬁned by ﬁve points (2.4–p31), where the ﬁve points
are the three points augmented with the two circular points.

In section 2.7.5 it will be shown that identifying the circular points (or equivalently
their dual, see below) allows the recovery of similarity properties (angles, ratios of
lengths). Algebraically, the circular points are the orthogonal directions of Euclidean
geometry, (1, 0, 0)T and (0, 1, 0)T, packaged into a single complex conjugate entity,
e.g.

I = (1, 0, 0)T + i(0, 1, 0)T.

Consequently, it is not so surprising that once the circular points are identiﬁed, orthog-
onality, and other metric properties, are then determined.

The conic dual to the circular points. The conic
C∗
∞ = IJT + JIT

(2.20)

is dual to the circular points. The conic C∗
∞ is a degenerate (rank 2) line conic
(see section 2.2.3), which consists of the two circular points. In a Euclidean coordinate
system it is given by

C∗
∞ =

1 −i 0

+

1 i 0

=

(cid:15)

 1

i
0

(cid:16)

(cid:15)

 1−i

0

(cid:16)

 1 0 0

0 1 0
0 0 0

 .

The conic C∗

∞ is ﬁxed under similarity transformations in an analogous fashion to
the ﬁxed properties of circular points. A conic is ﬁxed if the same matrix results (up to
scale) under the transformation rule. Since C∗
∞ is a dual conic it transforms according to
result 2.14(p37) (C∗(cid:2)
= HC∗HT), and one can verify that under the point transformation
x(cid:2)

= HSx,

(cid:2)

C∗
∞

= HSC∗

S = C∗
∞.

∞HT

The converse is also true, and we have
Result 2.22. The dual conic C∗
only if H is a similarity.
Some properties of C∗

∞ in any projective frame:

∞ is ﬁxed under the projective transformation H if and

(i) C∗

∞ has 4 degrees of freedom: a 3 × 3 homogeneous symmetric matrix has
5 degrees of freedom, but the constraint det C∗
∞ = 0 reduces the degrees of
freedom by 1.

54

2 Projective Geometry and Transformations of 2D

(ii) l∞ is the null vector of C∗

lie on l∞, so that ITl∞ = JTl∞ = 0; then

∞. This is clear from the deﬁnition: the circular points

C∗
∞l∞ = (IJT + JIT)l∞ = I(JTl∞) + J(ITl∞) = 0.

2.7.4 Angles on the projective plane
In Euclidean geometry the angle between two lines is computed from the dot product
of their normals. For the lines l = (l1, l2, l3)T and m = (m1, m2, m3)T with normals
parallel to (l1, l2)T, (m1, m2)T respectively, the angle is

(cid:19)

cos θ =

l1m1 + l2m2
1 + l2

2)(m2

(l2

1 + m2
2)

.

(2.21)

The problem with this expression is that the ﬁrst two components of l and m do not
have well deﬁned transformation properties under projective transformations (they are
not tensors), and so (2.21) cannot be applied after an afﬁne or projective transforma-
tion of the plane. However, an analogous expression to (2.21) which is invariant to
projective transformations is

lTC∗

(cid:19)
(lTC∗∞l)(mTC∗∞m)

∞m

cos θ =

(2.22)

where C∗
∞ is the conic dual to the circular points. It is clear that in a Euclidean co-
It may be veriﬁed that (2.22) is invariant
ordinate system (2.22) reduces to (2.21).
to projective transformations by using the transformation rules for lines (2.6–p36)
(l(cid:2)
= HC∗HT) under the point trans-
formation x(cid:2)

= H−Tl) and dual conics (result 2.14(p37)) (C∗(cid:2)

= Hx. For example, the numerator transforms as
∞HTH−Tm = lTC∗

∞m (cid:6)→ lTH−1HC∗

lTC∗

∞m.

It may also be veriﬁed that the scale of the homogeneous objects cancels between the
numerator and denominator. Thus (2.22) is indeed invariant to the projective frame. To
summarize, we have shown
Result 2.23. Once the conic C∗
angles may be measured by (2.22).

∞ is identiﬁed on the projective plane then Euclidean

∞m = 0.

if l and m satisfy lTC∗

Note, as a corollary,
Result 2.24. Lines l and m are orthogonal if lTC∗
Geometrically,
(see section 2.8.1) with respect to the conic C∗
∞.
Length ratios may also be measured once C∗
∞ is identiﬁed. Consider the triangle
shown in ﬁgure 2.16 with vertices a, b, c. From the standard trigonometric sine rule
the ratio of lengths d(b, c) : d(a, c) = sin α : sin β, where d(x, y) denotes the Eu-
clidean distance between the points x and y. Using (2.22), both cos α and cos β may
be computed from the lines l(cid:2)
for any

then the lines are conjugate

= a(cid:2) × b(cid:2)

= b(cid:2) × c(cid:2)

= c(cid:2) × a(cid:2)

∞m = 0,

and n(cid:2)

, m(cid:2)

2.7 Recovery of afﬁne and metric properties from images

55

c

α

a

β

/

c

/

m

/

n

b

/

a

l /

/
b

Fig. 2.16. Length ratios. Once C∗
measured from the projectively distorted ﬁgure. See text for details.

∞ is identiﬁed the Euclidean length ratio d(b, c) : d(a, c) may be

projective frame in which C∗
the ratio d(a, b) : d(c, a), may be determined from the projectively mapped points.

∞ is speciﬁed. Consequently both sin α, sin β, and thence

2.7.5 Recovery of metric properties from images
A completely analogous approach to that of section 2.7.2 and ﬁgure 2.12, where afﬁne
properties are recovered by specifying l∞, enables metric properties to be recovered
from an image of a plane by transforming the circular points to their canonical position.
Suppose the circular points are identiﬁed in an image, and the image is then rectiﬁed
by a projective transformation H that maps the imaged circular points to their canonical
position (at (1,±i, 0)T) on l∞. From result 2.21 the transformation between the world
plane and the rectiﬁed image is then a similarity since it is projective and the circular
points are ﬁxed.
Metric rectiﬁcation using C∗
∞ neatly packages all the information
required for a metric rectiﬁcation. It enables both the projective and afﬁne components
of a projective transformation to be determined, leaving only similarity distortions.
This is evident from its transformation under a projectivity. If the point transformation
is x(cid:2)
∞ transforms
according to result 2.14(p37) (C∗(cid:2)
= HC∗HT). Using the decomposition chain (2.17–
p43) for H
(cid:15)
(cid:16)
= (HP HA HS) C∗
= (HP HA) C∗
∞

= Hx, where the x-coordinate frame is Euclidean and x(cid:2)
(cid:15)
HS C∗

∞. The dual conic C∗

projective, C∗

= (HP HA)

(cid:16)(cid:15)

HT
A HT

P

C∗
∞

∞HT

S

(cid:16)

(cid:2)

(cid:17)

T

∞ (HP HA HS)
A HT
HT
KKTv

(cid:18)

P

KKT

=

vTKKT vTKKTv

.

(2.23)

∞, but (since C∗

It is clear that the projective (v) and afﬁne (K) components are determined directly from
the image of C∗
∞ is invariant to similarity transformation by result 2.22)
the similarity component is undetermined. Consequently,
Result 2.25. Once the conic C∗
distortion may be rectiﬁed up to a similarity.

∞ is identiﬁed on the projective plane then projective

Actually, a suitable rectifying homography may be obtained directly from the iden-
(cid:2)
in an image using the SVD (section A4.4(p585)): writing the SVD of C∗
∞

tiﬁed C∗
∞

(cid:2)

, m(cid:2)
(cid:15)

(cid:16)(cid:17)

(cid:2)
l
1

(cid:2)
l
2

(cid:2)
l
3

KKT 0
0T
0

 = 0

(cid:18) m

(cid:2)
(cid:2)
1
m
(cid:2)
2
m
3

56

as

2 Projective Geometry and Transformations of 2D

 1 0 0

0 1 0
0 0 0

 UT

(cid:2)

C∗
∞

= U

then by inspection from (2.23) the rectifying projectivity is H = U up to a similarity.

The following two examples show typical situations where C∗

∞ may be identiﬁed in

an image, and thence a metric rectiﬁcation obtained.

Example 2.26. Metric rectiﬁcation I
Suppose an image has been afﬁnely rectiﬁed (as in example 2.18 above), then we re-
quire two constraints to specify the 2 degrees of freedom of the circular points in order
to determine a metric rectiﬁcation. These two constraints may be obtained from two
imaged right angles on the world plane.
line pair l, m on the world plane. From result 2.24 l(cid:2)TC∗
∞
with v = 0

in the afﬁnely rectiﬁed image correspond to an orthogonal
= 0, and using (2.23)

Suppose the lines l(cid:2)

(cid:2)m(cid:2)

which is a linear constraint on the 2 × 2 matrix S = KKT. The matrix S = KKT is
symmetric with three independent elements, and thus 2 degrees of freedom (as the
overall scaling is unimportant). The orthogonality condition reduces to the equation
(l

(cid:2)
(cid:2)
(cid:2)
2)T = 0 which may be written as
2)S(m
1, m

(cid:2)
1, l

(cid:2)
(cid:2)
1m
1, l

(l

(cid:2)
(cid:2)
1m
2 + l

(cid:2)
(cid:2)
2m
1, l

(cid:2)
(cid:2)
2) s = 0,
2m

where s = (s11, s12, s22)T is S written as a 3-vector. Two such orthogonal line pairs
provide two constraints which may be stacked to give a 2 × 3 matrix with s deter-
mined as the null vector. Thus S, and hence K, is obtained up to scale (by Cholesky
decomposition, section A4.2.1(p582)). Figure 2.17 shows an example of two orthog-
onal line pairs being used to metrically rectify the afﬁnely rectiﬁed image computed
(cid:2)
in ﬁgure 2.13.

Alternatively, the two constraints required for metric rectiﬁcation may be obtained from
an imaged circle or two known length ratios. In the case of a circle, the image conic
is an ellipse in the afﬁnely rectiﬁed image, and the intersection of this ellipse with the
(known) l∞ directly determines the imaged circular points.

∞ can alternatively be identiﬁed directly in a perspective image, without

The conic C∗

ﬁrst identifying l∞, as is illustrated in the following example.

Example 2.27. Metric rectiﬁcation II
We start here from the original perspective image of the plane (not the afﬁnely rectiﬁed
image of example 2.26). Suppose lines l and m are images of orthogonal lines on the
world plane; then from result 2.24 lTC∗
∞m = 0, and in a similar manner to constraining

2.7 Recovery of afﬁne and metric properties from images

57

a

b

Fig. 2.17. Metric rectiﬁcation via orthogonal lines I. The afﬁne transformation required to metrically
rectify an afﬁne image may be computed from imaged orthogonal lines. (a) Two (non-parallel) line pairs
identiﬁed on the afﬁnely rectiﬁed image (ﬁgure 2.13) correspond to orthogonal lines on the world plane.
(b) The metrically rectiﬁed image. Note that in the metrically rectiﬁed image all lines orthogonal in the
world are orthogonal, world squares have unit aspect ratio, and world circles are circular.

a

b

Fig. 2.18. Metric rectiﬁcation via orthogonal lines II. (a) The conic C∗
∞ is determined on the per-
spectively imaged plane (the front wall of the building) using the ﬁve orthogonal line pairs shown. The
conic C∗
∞ determines the circular points, and equivalently the projective transformation necessary to
metrically rectify the image (b). The image shown in (a) is the same perspective image as that of ﬁgure
2.4(p35), where the perspective distortion was removed by specifying the world position of four image
points.

a conic to contain a point (2.4–p31), this provides a linear constraint on the elements
of C∗

∞, namely

(l1m1, (l1m2 + l2m1)/2, l2m2, (l1m3 + l3m1)/2, (l2m3 + l3m2)/2, l3m3) c = 0

where c = (a, b, c, d, e, f )T is the conic matrix (2.3–p30) of C∗
∞ written as a 6-vector.
Five such constraints can be stacked to form a 5 × 6 matrix, and c, and hence C∗
∞,
is obtained as the null vector. This shows that C∗
∞ can be determined linearly from
the images of ﬁve line pairs which are orthogonal on the world plane. An example of
(cid:2)
metric rectiﬁcation using such line pair constraints is shown in ﬁgure 2.18.

Stratiﬁcation. Note, in example 2.27 the afﬁne and projective distortions are deter-
mined in one step by specifying C∗
∞. In the previous example 2.26 ﬁrst the projec-
tive and subsequently the afﬁne distortions were removed. This two-step approach is
termed stratiﬁed. Analogous approaches apply in 3D, and are employed in chapter 10

58

2 Projective Geometry and Transformations of 2D

C

y

l

x

Fig. 2.19. The pole–polar relationship. The line l = Cx is the polar of the point x with respect to the
conic C, and the point x = C−1l is the pole of l with respect to C. The polar of x intersects the conic at
the points of tangency of lines from x. If y is on l then yTl = yTCx = 0. Points x and y which satisfy
yTCx = 0 are conjugate.

on 3D reconstruction and chapter 19 on auto-calibration, when obtaining a metric from
a 3D projective reconstruction.

2.8 More properties of conics

We now introduce an important geometric relation between a point, line and conic,
which is termed polarity. Applications of this relation (to the representation of orthog-
onality) are given in chapter 8.

2.8.1 The pole–polar relationship
A point x and conic C deﬁne a line l = Cx. The line l is called the polar of x with
respect to C, and the point x is the pole of l with respect to C.
• The polar line l = Cx of the point x with respect to a conic C intersects the conic in
two points. The two lines tangent to C at these points intersect at x.

This relationship is illustrated in ﬁgure 2.19.

Proof. Consider a point y on C. The tangent line at y is Cy, and this line contains x
if xTCy = 0. Using the symmetry of C, the condition xTCy = (Cx)Ty = 0 is that the
point y lies on the line Cx. Thus the polar line Cx intersects the conic in the point y at
which the tangent line contains x.

As the point x approaches the conic the tangent lines become closer to collinear, and
their contact points on the conic also become closer. In the limit that x lies on C, the
polar line has two-point contact at x, and we have:
• If the point x is on C then the polar is the tangent line to the conic at x.
See result 2.7(p31).

2.8 More properties of conics

59

Example 2.28. A circle of radius r centred on the x-axis at x = a has the equation
(x − a)2 + y2 = r2, and is represented by the conic matrix

 1

C =

 .

−a
0
0
−a 0 a2 − r2

0
1

The polar line of the origin is given by l = C(0, 0, 1)T = (−a, 0, a2 − r2)T. This is a
vertical line at x = (a2 − r2)/a. If r = a the origin lies on the circle. In this case the
(cid:2)
polar line is the y-axis and is tangent to the circle.

It is evident that the conic induces a map between points and lines of IP2. This map is
a projective construction since it involves only intersections and tangency, both prop-
erties that are preserved under projective transformations. A projective map between
points and lines is termed a correlation (an unfortunate name, given its more common
usage).

Deﬁnition 2.29. A correlation is an invertible mapping from points of IP2 to lines of
IP2. It is represented by a 3 × 3 non-singular matrix A as l = Ax.
A correlation provides a systematic way to dualize relations involving points and lines.
It need not be represented by a symmetric matrix, but we will only consider symmetric
correlations here, because of the association with conics.
• Conjugate points. If the point y is on the line l = Cx then yTl = yTCx = 0. Any
two points x, y satisfying yTCx = 0 are conjugate with respect to the conic C.

The conjugacy relation is symmetric:
• If x is on the polar of y then y is on the polar of x.
This follows simply because of the symmetry of the conic matrix – the point x is on
the polar of y if xTCy = 0, and the point y is on the polar of x if yTCx = 0. Since
xTCy = yTCx, if one form is zero, then so is the other. There is a dual conjugacy
relationship for lines: two lines l and m are conjugate if lTC∗m = 0.

2.8.2 Classiﬁcation of conics
This section describes the projective and afﬁne classiﬁcation of conics.

Projective normal form for a conic. Since C is a symmetric matrix it has real eigen-
values, and may be decomposed as a product C = UTDU (see section A4.2(p580)),
where U is an orthogonal matrix, and D is diagonal. Applying the projective trans-
formation represented by U, conic C is transformed to another conic C(cid:2)
= U−TCU−1 =
U−TUTDUU−1 = D. This shows that any conic is equivalent under projective transforma-
tion to one with a diagonal matrix. Let D = diag(1d1, 2d2, 3d3) where i = ±1 or 0
and each di > 0. Thus, D may be written in the form

D = diag(s1, s2, s3)Tdiag(1, 2, 3)diag(s1, s2, s3)

60

2 Projective Geometry and Transformations of 2D

a

l

l

b

l

c

Fig. 2.20. Afﬁne classiﬁcation of point conics. A conic is an (a) ellipse, (b) parabola, or (c) hyperbola;
according to whether it (a) has no real intersection, (b) is tangent to (2-point contact), or (c) has 2 real
intersections with l∞. Under an afﬁne transformation l∞ is a ﬁxed line, and intersections are preserved.
Thus this classiﬁcation is unaltered by an afﬁnity.

i = di. Note that diag(s1, s2, s3)T = diag(s1, s2, s3). Now, transforming once
where s2
more by the transformation diag(s1, s2, s3), the conic D is transformed to a conic with
matrix diag(1, 2, 3), with each i = ±1 or 0. Further transformation by permutation
matrices may be carried out to ensure that values i = 1 occur before values i = −1
which in turn precede values i = 0. Finally, by multiplying by −1 if necessary, one
may ensure that there are at least as many +1 entries as −1. The various types of conics
may now be enumerated, and are shown in table 2.2.

Diagonal
(1, 1, 1)
(1, 1,−1)
(1, 1, 0)
(1,−1, 0)
(1, 0, 0)

Equation

x2 + y2 + w2 = 0
x2 + y2 − w2 = 0

Conic type

Improper conic – no real points.

Circle

x2 + y2 = 0
x2 − y2 = 0

Single real point (0, 0, 1)T

Two lines x = ±y

x2 = 0

Single line x = 0 counted twice.

Table 2.2. Projective classiﬁcation of point conics. Any plane conic is projectively equivalent to one
of the types shown in this table. Those conics for which i = 0 for some i are known as degenerate
conics, and are represented by a matrix of rank less than 3. The conic type column only describes the
real points of the conics – for example as a complex conic x2 + y2 = 0 consists of the line pair x = ±iy.

Afﬁne classiﬁcation of conics. The classiﬁcation of (non-degenerate, proper) conics
in Euclidean geometry into hyperbola, ellipse and parabola is well known. As shown
above in projective geometry these three types of conic are projectively equivalent to
a circle. However, in afﬁne geometry the Euclidean classiﬁcation is still valid because
it depends only on the relation of l∞ to the conic. The relation for the three types of
conic is illustrated in ﬁgure 2.20.

2.9 Fixed points and lines

61

e

1

H

e

1

e

2

e

3

x

e

2

e

3

x /

Fig. 2.21. Fixed points and lines of a plane projective transformation. There are three ﬁxed points,
and three ﬁxed lines through these points. The ﬁxed lines and points may be complex. Algebraically,
the ﬁxed points are the eigenvectors, ei, of the point transformation (x(cid:1) = Hx), and the ﬁxed lines
eigenvectors of the line transformation ( l(cid:1) = H−Tl). Note, the ﬁxed line is not ﬁxed pointwise: under
the transformation, points on the line are mapped to other points on the line; only the ﬁxed points are
mapped to themselves.

2.9 Fixed points and lines

We have seen, by the examples of l∞ and the circular points, that points and lines may
be ﬁxed under a projective transformation. In this section the idea is investigated more
thoroughly.
Here, the source and destination planes are identiﬁed (the same) so that the trans-
formation maps points x to points x(cid:2)
in the same coordinate system. The key idea
is that an eigenvector corresponds to a ﬁxed point of the transformation, since for an
eigenvector e with eigenvalue λ,

He = λe

and e and λe represent the same point. Often the eigenvector and eigenvalue have
physical or geometric signiﬁcance in computer vision applications.
A 3×3 matrix has three eigenvalues and consequently a plane projective transforma-
tion has up to three ﬁxed points, if the eigenvalues are distinct. Since the characteristic
equation is a cubic in this case, one or three of the eigenvalues, and corresponding
eigenvectors, is real. A similar development can be given for ﬁxed lines, which, since
lines transform as (2.6–p36) l(cid:2)

= H−Tl, correspond to the eigenvectors of HT.

The relationship between the ﬁxed points and ﬁxed lines is shown in ﬁgure 2.21.
Note the lines are ﬁxed as a set, not ﬁxed pointwise, i.e. a point on the line is mapped
to another point on the line, but in general the source and destination points will differ.
There is nothing mysterious here: The projective transformation of the plane induces a
1D projective transformation on the line. A 1D projective transformation is represented
by a 2× 2 homogeneous matrix (section 2.5). This 1D projectivity has two ﬁxed points
corresponding to the two eigenvectors of the 2× 2 matrix. These ﬁxed points are those
of the 2D projective transformation.

A further specialization concerns repeated eigenvalues. Suppose two of the eigen-
values (λ2, λ3 say) are identical, and that there are two distinct eigenvectors (e2, e3),
corresponding to λ2 = λ3. Then the line containing the eigenvectors e2, e3 will be
ﬁxed pointwise, i.e. it is a line of ﬁxed points. For suppose x = αe2 + βe3; then

Hx = λ2αe2 + λ2βe3 = λ2x

62

2 Projective Geometry and Transformations of 2D

i.e. a point on the line through two degenerate eigenvectors is mapped to itself (only
differing by scale). Another possibility is that λ2 = λ3, but that there is only one
corresponding eigenvector. In this case, the eigenvector has algebraic dimension equal
to two, but geometric dimension equal to one. Then there is one fewer ﬁxed point (2
instead of 3). Various cases of repeated eigenvalues are discussed further in appendix
7(p628).

We now examine the ﬁxed points and lines of the hierarchy of projective transforma-
tion subgroups of section 2.4. Afﬁne transformations, and the more specialized forms,
have two eigenvectors which are ideal points (x3 = 0), and which correspond to the
eigenvectors of the upper left 2 × 2 matrix. The third eigenvector is ﬁnite in general.

A Euclidean matrix. The two ideal ﬁxed points are the complex conjugate pair of cir-
cular points I, J, with corresponding eigenvalues {eiθ, e
−iθ}, where θ is the rotation an-
gle. The third eigenvector, which has unit eigenvalue, is called the pole. The Euclidean
transformation is equal to a pure rotation by θ about this point with no translation.

A special case is that of a pure translation (i.e. where θ = 0). Here the eigenvalues
are triply degenerate. The line at inﬁnity is ﬁxed pointwise, and there is a pencil of
ﬁxed lines through the point (tx, ty, 0)T which corresponds to the translation direction.
Consequently lines parallel to t are ﬁxed. This is an example of an elation (see section
A7.3(p631)).

A similarity matrix. The two ideal ﬁxed points are again the circular points. The
eigenvalues are {1, seiθ, se
−iθ}. The action can be understood as a rotation and
isotropic scaling by s about the ﬁnite ﬁxed point. Note that the eigenvalues of the
circular points again encode the angle of rotation.

An afﬁne matrix. The two ideal ﬁxed points can be real or complex conjugates, but
the ﬁxed line l∞ = (0, 0, 1)T through these points is real in either case.

2.10 Closure

2.10.1 The literature
A gentle introduction to plane projective geometry, written for computer vision re-
searchers, is given in the appendix of Mundy and Zisserman [Mundy-92]. A more
formal approach is that of Semple and Kneebone [Semple-79], but [Springer-64] is
more readable.

On the recovery of afﬁne and metric scene properties for an imaged plane, Collins
and Beveridge [Collins-93] use the vanishing line to recover afﬁne properties from
satellite images, and Liebowitz and Zisserman [Liebowitz-98] use metric information
on the plane, such as right angles, to recover the metric geometry.

2.10.2 Notes and exercises

(i) Afﬁne transformations.

2.10 Closure

63

(a) Show that an afﬁne transformation can map a circle to an ellipse, but

cannot map an ellipse to a hyperbola or parabola.

(b) Prove that under an afﬁne transformation the ratio of lengths on parallel
line segments is an invariant, but that the ratio of two lengths that are
not parallel is not.

(ii) Projective transformations. Show that there is a three-parameter family of
projective transformations which ﬁx (as a set) a unit circle at the origin, i.e. a
unit circle at the origin is mapped to a unit circle at the origin (hint, use result
2.13(p37) to compute the transformation). What is the geometric interpretation
of this family?

(iii) Isotropies. Show that two lines have an invariant under a similarity transfor-
mation; and that two lines and two points have an invariant under a projective
transformation. In both cases the equality case of the counting argument (result
2.16(p43)) is violated. Show that for these two cases the respective transforma-
tion cannot be fully determined, although it is partially determined.

(iv) Invariants. Using the transformation rules for points, lines and conics show:

(a) Two lines, l1, l2, and two points, x1, x2, not lying on the lines have the

invariant

I =

(lT
1
(lT
1

x1)(lT
2
x2)(lT
2

x2)
x1)

(see the previous question).

(b) A conic C and two points, x1 and x2, in general position have the invari-

ant

I =

(xT
Cx2)2
1
Cx1)(xT
2

Cx2)

.

(xT
1

(c) Show that the projectively invariant expression for measuring an-
gles (2.22) is equivalent to Laguerre’s projectively invariant expression
involving a cross ratio with the circular points (see [Springer-64]).

(v) The cross ratio. Prove the invariance of the cross ratio of four collinear
points under projective transformations of the line (2.18–p45). Hint, start with
the transformation of two points on the line written as ¯x(cid:2)
i = λiH2×2¯xi and
¯x(cid:2)
j = λjH2×2¯xj, where equality is not up to scale, then from the properties of
determinants show that |¯x(cid:2)
| = λiλj det H2×2|¯xi¯xj| and continue from here.
i¯x(cid:2)
An alternative derivation method is given in [Semple-79].

j

(vi) Polarity. Figure 2.19 shows the geometric construction of the polar line for a
point x outside an ellipse. Give a geometric construction for the polar when the
point is inside. Hint, start by choosing any line through x. The pole of this line
is a point on the polar of x.

(vii) Conics. If the sign of the conic matrix C is chosen such that two eigenvalues
are positive and one negative, then internal and external points may be distin-
guished according to the sign of xTCx: the point x is inside/on/outside the conic

64

2 Projective Geometry and Transformations of 2D

C if xTCx is negative/zero/positive respectively. This can seen by example from
a circle C = diag(1, 1,−1). Under projective transformations internality is in-
variant, though its interpretation requires care in the case of an ellipse being
transformed to a hyperbola (see ﬁgure 2.20).

(viii) Dual conics. Show that the matrix [l]×C[l]× represents a rank 2 dual conic
which consists of the two points at which the line l intersects the (point) conic
C (the notation [l]× is deﬁned in (A4.5–p581)).

(ix) Special projective transformations. Suppose points on a scene plane are re-
lated by reﬂection in a line: for example, a plane object with bilateral symmetry.
Show that in a perspective image of the plane the points are related by a pro-
jectivity H satisfying H2 = I. Furthermore, show that under H there is a line
of ﬁxed points corresponding to the imaged reﬂection line, and that H has an
eigenvector, not lying on this line, which is the vanishing point of the reﬂection
direction (H is a planar harmonic homology, see section A7.2(p629)).
Now suppose that the points are related by a ﬁnite rotational symmetry: for
example, points on a hexagonal bolt head. Show in this case that Hn = I,
where n is the order of rotational symmetry (6 for a hexagonal symmetry),
that the eigenvalues of H determine the rotation angle, and that the eigenvector
corresponding to the real eigenvalue is the image of the centre of the rotational
symmetry.

3

Projective Geometry and Transformations of 3D

This chapter describes the properties and entities of projective 3-space, or IP3. Many
of these are straightforward generalizations of those of the projective plane, described
in chapter 2. For example, in IP3 Euclidean 3-space is augmented with a set of ideal
points which are on a plane at inﬁnity, π∞. This is the analogue of l∞ in IP2. Par-
allel lines, and now parallel planes, intersect on π∞. Not surprisingly, homogeneous
coordinates again play an important role, here with all dimensions increased by one.
However, additional properties appear by virtue of the extra dimension. For example,
two lines always intersect on the projective plane, but they need not intersect in 3-space.
The reader should be familiar with the ideas and notation of chapter 2 before read-
ing this chapter. We will concentrate here on the differences and additional geometry
introduced by adding the extra dimension, and will not repeat the bulk of the material
of the previous chapter.

3.1 Points and projective transformations

A point X in 3-space is represented in homogeneous coordinates as a 4-vector. Specif-
ically, the homogeneous vector X = (X1, X2, X3, X4)T with X4 (cid:5)= 0 represents the point
(X, Y, Z)T of IR3 with inhomogeneous coordinates

X = X1/X4, Y = X2/X4, Z = X3/X4.

For example, a homogeneous representation of (X, Y, Z)T is X = (X, Y, Z, 1)T. Homo-
geneous points with X4 = 0 represent points at inﬁnity.
A projective transformation acting on IP3 is a linear transformation on homogeneous
4-vectors represented by a non-singular 4×4 matrix: X(cid:2)
= HX. The matrix H represent-
ing the transformation is homogeneous and has 15 degrees of freedom. The degrees of
freedom follow from the 16 elements of the matrix less one for overall scaling.

As in the case of planar projective transformations, the map is a collineation (lines
are mapped to lines), which preserves incidence relations such as the intersection point
of a line with a plane, and order of contact.

65

66

3 Projective Geometry and Transformations of 3D

3.2 Representing and transforming planes, lines and quadrics

In IP3 points and planes are dual, and their representation and development is analogous
to the point–line duality in IP2. Lines are self-dual in IP3.

3.2.1 Planes
A plane in 3-space may be written as

π1X + π2Y + π3Z + π4 = 0.

(3.1)

Clearly this equation is unaffected by multiplication by a non-zero scalar, so only the
three independent ratios {π1 : π2 : π3 : π4} of the plane coefﬁcients are signiﬁcant. It
follows that a plane has 3 degrees of freedom in 3-space. The homogeneous represen-
tation of the plane is the 4-vector π = (π1, π2, π3, π4)T.
Homogenizing (3.1) by the replacements X (cid:6)→ X1/X4, Y (cid:6)→ X2/X4, Z (cid:6)→ X3/X4 gives

π1X1 + π2X2 + π3X3 + π4X4 = 0

or more concisely

πTX = 0

(3.2)

which expresses that the point X is on the plane π.

The ﬁrst 3 components of π correspond to the plane normal of Euclidean geometry
3-vector notation as n.(cid:22)X + d = 0, where n = (π1, π2, π3)T, (cid:22)X = (X, Y, Z)T, X4 = 1
– using inhomogeneous notation (3.2) becomes the familiar plane equation written in
and d = π4. In this form d/(cid:10)n(cid:10) is the distance of the plane from the origin.

Join and incidence relations.
planes and points and lines. For example,

In IP3 there are numerous geometric relations between

(i) A plane is deﬁned uniquely by the join of three points, or the join of a line and
point, in general position (i.e. the points are not collinear or incident with the
line in the latter case).

(ii) Two distinct planes intersect in a unique line.
(iii) Three distinct planes intersect in a unique point.

These relations have algebraic representations which will now be developed in the
case of points and planes. The representations of the relations involving lines are not
as simple as those arising from 3D vector algebra of IP2 (e.g. l = x × y), and are
postponed until line representations are introduced in section 3.2.2.

Three points deﬁne a plane. Suppose three points Xi are incident with the plane
π. Then each point satisﬁes (3.2) and thus πTXi = 0,
i = 1, . . . ,3 . Stacking these
equations into a matrix gives

(3.3)

 π = 0.

 XT

1
XT
2
XT
3

3.2 Representing and transforming planes, lines and quadrics

67
Since three points X1, X2 and X3 in general position are linearly independent, it fol-
lows that the 3 × 4 matrix composed of the points as rows has rank 3. The plane
π deﬁned by the points is thus obtained uniquely (up to scale) as the 1-dimensional
(right) null-space. If the matrix has only a rank of 2, and consequently the null-space
is 2-dimensional, then the points are collinear, and deﬁne a pencil of planes with the
line of collinear points as axis.
In IP2, where points are dual to lines, a line l through two points x, y can similarly
be obtained as the null-space of the 2 × 3 matrix with xT and yT as rows. However, a
more convenient direct formula l = x × y is also available from vector algebra. In IP3
the analogous expression is obtained from properties of determinants and minors.

We start from the matrix M = [X, X1, X2, X3] which is composed of a general point
X and the three points Xi which deﬁne the plane π. The determinant det M = 0 when
X lies on π since the point X is then expressible as a linear combination of the points
Xi, i = 1, . . . ,3 . Expanding the determinant about the column X we obtain

det M = X1D234 − X2D134 + X3D124 − X4D123

where Djkl is the determinant formed from the jkl rows of the 4×3 matrix [X1, X2, X3].
Since det M = 0 for points on π we can then read off the plane coefﬁcients as

T

.

1

1

X3 =

X2 =

X1 =

π = (D234,−D134, D124,−D123)
This is the solution vector (the null-space) of (3.3) above.
Example 3.1. Suppose the three points deﬁning the plane are

(cid:20) (cid:22)X1
(cid:20) (cid:22)X2
(cid:21)
(cid:21)
where (cid:22)X = (X, Y, Z)T. Then
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) =
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) Y1 − Y3 Y2 − Y3 Y3
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) =
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) Y1 Y2 Y3
Z1 − Z3
(cid:20)
((cid:22)X1 − (cid:22)X3) × ((cid:22)X2 − (cid:22)X3)
−(cid:22)XT
3 ((cid:22)X1 × (cid:22)X2)
plane normal is computed as ((cid:22)X1 − (cid:22)X3) × ((cid:22)X2 − (cid:22)X3).

and similarly for the other components, giving

Z2 − Z3

D234 =

π =

Z3
1

Z1
1

Z2
1

Z3
1

0

0

(3.4)

1

(cid:21)

(cid:20) (cid:22)X3
(cid:15)
((cid:22)X1 − (cid:22)X3) × ((cid:22)X2 − (cid:22)X3)
(cid:21)

(cid:16)

1

.

 X = 0.

 πT

1
πT
2
πT
3

This is the familiar result from Euclidean vector geometry where, for example, the
(cid:2)

Three planes deﬁne a point.
The development here is dual to the case of three
points deﬁning a plane. The intersection point X of three planes πi can be computed
straightforwardly as the (right) null-space of the 3 × 4 matrix composed of the planes
as rows:

(3.5)

68

3 Projective Geometry and Transformations of 3D

Fig. 3.1. A line may be speciﬁed by its points of intersection with two orthogonal planes. Each inter-
section point has 2 degrees of freedom, which demonstrates that a line in IP3 has a total of 4 degrees of
freedom.

A direct solution for X, in terms of determinants of 3 × 3 submatrices, is obtained as
an analogue of (3.4), though computationally a numerical solution would be obtained
by algorithm A5.1(p589).
The two following results are direct analogues of their 2D counterparts.
Projective transformation. Under the point transformation X(cid:2)
forms as

= HX, a plane trans-

π(cid:2)

= H−Tπ.

(3.6)

Parametrized points on a plane. The points X on the plane π may be written as

X = Mx

(3.7)
where the columns of the 4×3 matrix M generate the rank 3 null-space of πT, i.e. πTM =
0, and the 3-vector x (which is a point on the projective plane IP2) parametrizes points
on the plane π. M is not unique, of course. Suppose the plane is π = (a, b, c, d)T and a is
non-zero, then MT can be written as MT = [p| I3×3], where p = (−b/a,−c/a,−d/a)T.
This parametrized representation is simply the analogue in 3D of a line l in IP2
deﬁned as a linear combination of its 2D null-space as x = µa + λb, where lTa =
lTb = 0.

3.2.2 Lines
A line is deﬁned by the join of two points or the intersection of two planes. Lines
have 4 degrees of freedom in 3-space. A convincing way to count these degrees of
freedom is to think of a line as deﬁned by its intersection with two orthogonal planes,
as in ﬁgure 3.1. The point of intersection on each plane is speciﬁed by two parameters,
producing a total of 4 degrees of freedom for the line.

Lines are very awkward to represent in 3-space since a natural representation for an
object with 4 degrees of freedom would be a homogeneous 5-vector. The problem is
that a homogeneous 5 vector cannot easily be used in mathematical expressions to-
gether with the 4-vectors representing points and planes. To overcome this problem

3.2 Representing and transforming planes, lines and quadrics

69

a number of line representations have been proposed, and these differ in their math-
ematical complexity. We survey three of these representations. In each case the rep-
resentation provides mechanisms for a line to be deﬁned by: the join of two points,
a dual version where the line is deﬁned by the intersection of two planes, and also a
map between the two deﬁnitions. The representations also enable join and incidence
relations to be computed, for example the point at which a line intersects a plane.

I. Null-space and span representation. This representation builds on the intuitive
geometric notion that a line is a pencil (one-parameter family) of collinear points, and is
deﬁned by any two of these points. Similarly, a line is the axis of a pencil of planes, and
is deﬁned by the intersection of any two planes from the pencil. In both cases the actual
points or planes are not important (in fact two points have 6 degrees of freedom and
are represented by two 4-vectors – far too many parameters). This notion is captured
mathematically by representing a line as the span of two vectors. Suppose A, B are two
(non-coincident) space points. Then the line joining these points is represented by the
span of the row space of the 2 × 4 matrix W composed of AT and BT as rows:

(cid:17)

(cid:18)

.

AT
BT

W =

Then:

(i) The span of WT is the pencil of points λA + µB on the line.
(ii) The span of the 2-dimensional right null-space of W is the pencil of planes with

the line as axis.

It is evident that two other points, A(cid:2)T and B(cid:2)T, on the line will generate a matrix W(cid:2)
with the same span as W, so that the span, and hence the representation, is independent
of the particular points used to deﬁne it.

To prove the null-space property, suppose that P and Q are a basis for the null-space.
Then WP = 0 and consequently ATP = BTP = 0, so that P is a plane containing the
points A and B. Similarly, Q is a distinct plane also containing the points A and B.
Thus A and B lie on both the (linearly independent) planes P and Q, so the line deﬁned
by W is the plane intersection. Any plane of the pencil, with the line as axis, is given by
the span λ
The dual representation of a line as the intersection of two planes, P, Q, follows in
a similar manner. The line is represented as the span (of the row space) of the 2 × 4
matrix W∗

composed of PT and QT as rows:

(cid:2)P + µ

(cid:2)Q.

(cid:17)

(cid:18)

W∗

=

PT
QT

with the properties

(i) The span of W∗T is the pencil of planes λ
(cid:2)P + µ
(ii) The span of the 2-dimensional null-space of W∗

(cid:2)Q with the line as axis.
is the pencil of points on the

line.

70

3 Projective Geometry and Transformations of 3D

The two representations are related by W∗ WT = W W∗T = 02×2, where 02×2 is a 2 × 2
null matrix.

Example 3.2. The X-axis is represented as

(cid:17)

(cid:18)

(cid:17)

(cid:18)

W =

0 0 0 1
1 0 0 0

W∗

=

0 0 1 0
0 1 0 0

where the points A and B are here the origin and ideal point in the X-direction, and the
(cid:2)
planes P and Q are the XY- and XZ-planes respectively.

Join and incidence relations are also computed from null-spaces.

(i) The plane π deﬁned by the join of the point X and line W is obtained from the

null-space of

M =

from the null-space of

M =

(cid:17)

(cid:17)

(cid:18)

(cid:18)

.

.

W
XT

W∗
πT

If the null-space of M is 2-dimensional then X is on W, otherwise Mπ = 0.

(ii) The point X deﬁned by the intersection of the line W with the plane π is obtained

If the null-space of M is 2-dimensional then the line W is on π, otherwise MX = 0.

These properties can be derived almost by inspection. For example, the ﬁrst is equiva-
lent to three points deﬁning a plane (3.3).

The span representation is very useful in practical numerical implementations where
null-spaces can be computed simply by using the SVD algorithm (see section A4.4-
(p585)) available with most matrix packages. The representation is also useful in es-
timation problems, where it is often not a problem that the entity being estimated is
over-parametrized (see the discussion of section 4.5(p110)).
II. Pl¨ucker matrices. Here a line is represented by a 4 × 4 skew-symmetric homo-
geneous matrix. In particular, the line joining the two points A, B is represented by the
matrix L with elements

lij = AiBj − BiAj

or equivalently in vector notation as

L = ABT − BAT

(3.8)

First a few properties of L:

(i) L has rank 2. Its 2-dimensional null-space is spanned by the pencil of planes

with the line as axis (in fact LW∗T = 0, with 0 a 4 × 2 null-matrix).

3.2 Representing and transforming planes, lines and quadrics

71

(ii) The representation has the required 4 degrees of freedom for a line. This is ac-
counted as follows: the skew-symmetric matrix has 6 independent non-zero ele-
ments, but only their 5 ratios are signiﬁcant, and furthermore because det L = 0
the elements satisfy a (quadratic) constraint (see below). The net number of de-
grees of freedom is then 4.
(iii) The relation L = ABT − BAT is the generalization to 4-space of the vector
product formula l = x × y of IP2 for a line l deﬁned by two points x, y all
represented by 3-vectors.

(iv) The matrix L is independent of the points A, B used to deﬁne it, since if a
different point C on the line is used, with C = A + µB, then the resulting
matrix is

ˆL = ACT − CAT = A(AT + µBT) − (A + µB)AT

= ABT − BAT = L.

(v) Under the point transformation X(cid:2)

= HX, the matrix transforms as L(cid:2)

= HLHT,

i.e. it is a valency-2 tensor (see appendix 1(p562)).

Example 3.3. From (3.8) the X-axis is represented as

(cid:16)

 0 0 0 −1

0 0 0
0 0 0
1 0 0

0
0
0



(cid:15)

 0

0
0
1

(cid:16)

−

(cid:15)

 1

0
0
0

L =

1 0 0 0

0 0 0 1

=

where the points A and B are (as in the previous example) the origin and ideal point in
(cid:2)
the X-direction respectively.

A dual Pl¨ucker representation L∗
two planes P, Q,

is obtained for a line formed by the intersection of

L∗

= PQT − QPT

(3.9)

and has similar properties to L. Under the point transformation X(cid:2)
transforms as L∗(cid:2)
L∗
simple rewrite rule:

= H−TLH−1. The matrix L∗

= HX, the matrix
can be obtained directly from L by a

l12 : l13 : l14 : l23 : l42 : l34 = l

∗
34 : l

∗
42 : l

∗
23 : l

∗
14 : l

∗
13 : l

∗
12.

(3.10)

The correspondence rule is very simple: the indices of the dual and original component
always include all the numbers {1, 2, 3, 4}, so if the original is ij then the dual is those
numbers of {1, 2, 3, 4} which are not ij. For example 12 (cid:6)→ 34.
Join and incidence properties are very nicely represented in this notation:

(i) The plane deﬁned by the join of the point X and line L is

and L∗X = 0 if, and only if, X is on L.

π = L∗X

72

3 Projective Geometry and Transformations of 3D

(ii) The point deﬁned by the intersection of the line L with the plane π is

and Lπ = 0 if, and only if, L is on π.

X = Lπ

The properties of two (or more) lines L1, L2, . . . can be obtained from the null-space
of the matrix M = [L1, L2, . . .]. For example if the lines are coplanar then MT has a
1-dimensional null-space corresponding to the plane π of the lines.

Example 3.4. The intersection of the X-axis with the plane X = 1 is given by X = Lπ
as

 0 0 0 −1

0 0 0
0 0 0
1 0 0

0
0
0



 1

0

0−1

 =



 1

0
0
1

X =

(cid:2)

(3.11)

which is the inhomogeneous point (X, Y, Z)T = (1, 0, 0)T.

III. Pl¨ucker line coordinates.
elements of the 4 × 4 skew-symmetric Pl¨ucker matrix (3.8) L, namely1

The Pl¨ucker line coordinates are the six non-zero

L = {l12, l13, l14, l23, l42, l34}.

This is a homogeneous 6-vector, and thus is an element of IP5. It follows from evaluat-
ing det L = 0 that the coordinates satisfy the equation

l12l34 + l13l42 + l14l23 = 0.

(3.12)
A 6-vector L only corresponds to a line in 3-space if it satisﬁes (3.12). The geometric
interpretation of this constraint is that the lines of IP3 deﬁne a (co-dimension 1) surface
in IP5 which is known as the Klein quadric, a quadric because the terms of (3.12) are
Suppose two lines L, ˆL are the joins of the points A, B and (cid:23)A,(cid:23)B respectively. The
quadratic in the Pl¨ucker line coordinates.
condition for this is that det[A, B,(cid:23)A,(cid:23)B] = 0. It can be shown that the determinant

lines intersect if and only if the four points are coplanar. A necessary and sufﬁcient

expands as

det[A, B,(cid:23)A,(cid:23)B] = l12
= (L| ˆL).

ˆl34 + ˆl12l34 + l13

ˆl42 + ˆl13l42 + l14

ˆl23 + ˆl14l23

(3.13)

Since the Pl¨ucker coordinates are independent of the particular points used to deﬁne
them, the bilinear product (L| ˆL) is independent of the points used in the derivation and
only depends on the lines L and ˆL. Then we have
Result 3.5. Two lines L and ˆL are coplanar (and thus intersect) if and only if (L| ˆL) =
0.

This product appears in a number of useful formulae:
1 The element l42 is conventionally used instead of l24 as it eliminates negatives in many of the subsequent formulae.

3.2 Representing and transforming planes, lines and quadrics

(i) A 6-vector L only represents a line in IP3 if (L|L) = 0. This is simply repeating
(ii) Suppose two lines L, ˆL are the intersections of the planes P, Q and (cid:23)P,(cid:23)Q respec-

the Klein quadric constraint (3.12) above.

73

tively. Then

(L| ˆL) = det[P, Q,(cid:23)P,(cid:23)Q]
and again the lines intersect if and only if (L| ˆL) = 0.

(iii) If L is the intersection of two planes P and Q and ˆL is the join of two points A

and B, then

(L| ˆL) = (PTA)(QTB) − (QTA)(PTB).

(3.14)

Pl¨ucker coordinates are useful in algebraic derivations. They will be used in deﬁning

the map from a line in 3-space to its image in chapter 8.

3.2.3 Quadrics and dual quadrics
A quadric is a surface in IP3 deﬁned by the equation

(3.15)
where Q is a symmetric 4 × 4 matrix. Often the matrix Q and the quadric surface it
deﬁnes are not distinguished, and we will simply refer to the quadric Q.

XTQX = 0

Many of the properties of quadrics follow directly from those of conics in section

2.2.3(p30). To highlight a few:

(i) A quadric has 9 degrees of freedom. These correspond to the ten independent

elements of a 4 × 4 symmetric matrix less one for scale.

(ii) Nine points in general position deﬁne a quadric.
(iii) If the matrix Q is singular, then the quadric is degenerate, and may be deﬁned

by fewer points.

(iv) A quadric deﬁnes a polarity between a point and a plane, in a similar manner
to the polarity deﬁned by a conic between a point and a line (section 2.8.1).
The plane π = QX is the polar plane of X with respect to Q. In the case that Q
is non-singular and X is outside the quadric, the polar plane is deﬁned by the
points of contact with Q of the cone of rays through X tangent to Q. If X lies on
Q, then QX is the tangent plane to Q at X.

(v) The intersection of a plane π with a quadric Q is a conic C. Computing the
conic can be tricky because it requires a coordinate system for the plane. Recall
from (3.7) that a coordinate system for the plane can be deﬁned by the comple-
ment space to π as X = Mx. Points on π are on Q if XTQX = xTMTQMx = 0.
These points lie on a conic C, since xTCx = 0, with C = MTQM.

(vi) Under the point transformation X(cid:2)
Q(cid:2)

= HX, a (point) quadric transforms as
= H−TQH−1.

(3.16)

The dual of a quadric is also a quadric. Dual quadrics are equations on planes: the
= adjoint Q,

tangent planes π to the point quadric Q satisfy πTQ∗π = 0, where Q∗

3 Projective Geometry and Transformations of 3D

74
or Q−1 if Q is invertible. Under the point transformation X(cid:2)
transforms as

= HX, a dual quadric

Q∗(cid:2)

= HQ∗HT.

(3.17)

The algebra of imaging a quadric is far simpler for a dual quadric than a point quadric.
This is detailed in chapter 8.

3.2.4 Classiﬁcation of quadrics
Since the matrix, Q, representing a quadric is symmetric, it may be decomposed as
Q = UTDU where U is a real orthogonal matrix and D is a real diagonal matrix. Further,
by appropriate scaling of the rows of U, one may write Q = HTDH where D is diagonal
with entries equal to 0, 1, or −1. We may further ensure that the zero entries of D
appear last along the diagonal, and that the +1 entries appear ﬁrst. Now, replacement
of Q = HTDH by D is equivalent to a projective transformation effected by the matrix
H (see (3.16)). Thus, up to projective equivalence, we may assume that the quadric is
represented by a matrix D of the given simple form.
The signature of a diagonal matrix D, denoted σ(D), is deﬁned to be the number of
+1 entries minus the number of −1 entries. This deﬁnition is extended to arbitrary
real symmetric matrices Q by deﬁning σ(Q) =σ (D) such that Q = HTDH, where H is
a real matrix. It may be proved that the signature is well deﬁned, being independent
of the particular choice of H. Since the matrix representing a quadric is deﬁned only
up to sign, we may assume that its signature is non-negative. Then, the projective type
of a quadric is uniquely determined by its rank and signature. This will allow us to
enumerate the different projective equivalence classes of quadrics.

A quadric represented by a diagonal matrix diag(d1, d2, d3, d4) corresponds to a set
of points satisfying an equation d1X2 + d2Y2 + d3Z2 + d4T2 = 0. One may set T = 1 to
get an equation for the non-inﬁnite points on the quadric. See table 3.1. Examples of
quadric surfaces are shown in ﬁgure 3.2 – ﬁgure 3.4.

Rank

4

3

2

1

σ

4
2
0

3
1

2
0

1

Diagonal
(1, 1, 1, 1)
(1, 1, 1,−1)
(1, 1,−1,−1)
(1, 1, 1, 0)
(1, 1,−1, 0)
(1, 1, 0, 0)
(1,−1, 0, 0)
(1, 0, 0, 0)

Equation

X2 + Y2 + Z2 + 1 = 0

X2 + Y2 + Z2 = 1
X2 + Y2 = Z2 + 1
X2 + Y2 + Z2 = 0

X2 + Y2 = Z2
X2 + Y2 = 0

X2 = Y2
X2 = 0

Realization

No real points

Sphere

Hyperboloid of one sheet
One point (0, 0, 0, 1)T

Cone at the origin

Single line (Z-axis)
Two planes X = ±Y
The plane X = 0

Table 3.1. Categorization of point quadrics.

3.3 Twisted cubics

75

Fig. 3.2. Non-ruled quadrics. This shows plots of a sphere, ellipsoid, hyperboloid of two sheets and
paraboloid. They are all projectively equivalent.

Fig. 3.3. Ruled quadrics. Two examples of a hyperboloid of one sheet are given. These surfaces are
given by equations X2 + Y2 = Z2 + 1 and XY = Z respectively, and are projectively equivalent. Note
that these two surfaces are made up of two sets of disjoint straight lines, and that each line from one set
meets each line from the other set. The two quadrics shown here are projectively (though not afﬁnely)
equivalent.

Ruled quadrics. Quadrics fall into two classes – ruled and unruled quadrics. A
ruled quadric is one that contains a straight line. More particularly, as shown in
ﬁgure 3.3, the non-degenerate ruled quadric (hyperboloid of one sheet) contains two
families of straight lines called generators. For more properties of ruled quadrics, refer
to [Semple-79].

The most interesting of the quadrics are the two quadrics of rank 4. Note that these
two quadrics differ even in their topological type. The quadric of signature 2 (the
sphere) is (obviously enough) topologically a sphere. On the other hand, the hyper-
boloid of 1 sheet is not topologically equivalent (homeomorphic) to a sphere. In fact,
it is topologically a torus (topologically equivalent to S1 × S1). This gives the clearest
indication that they are not projectively equivalent.

3.3 Twisted cubics

The twisted cubic may be considered to be a 3-dimensional analogue of a 2D conic
(although in other ways it is a quadric surface which is the 3-dimensional analogue of
a 2D conic.)

76

3 Projective Geometry and Transformations of 3D

Fig. 3.4. Degenerate quadrics. The two most important degenerate quadrics are shown, the cone and
two planes. Both these quadrics are ruled. The matrix representing the cone has rank 3, and the null-
vector represents the nodal point of the cone. The matrix representing the two (non-coincident) planes
has rank 2, and the two generators of the rank 2 null-space are two points on the intersection line of the
planes.

A conic in the 2-dimensional projective plane may be described as a parametrized

curve given by the equation

 x1

x2
x3

 = A
 1

 =

θ
θ2

 a11 + a12θ + a13θ2

a21 + a22θ + a23θ2
a31 + a32θ + a33θ2



(3.18)

(3.19)

where A is a non-singular 3 × 3 matrix.
In an analogous manner, a twisted cubic is deﬁned to be a curve in IP3 given in

parametric form as X1

 = A
 1

 =

X2
X3
X4

θ
θ2
θ3

where A is a non-singular 4 × 4 matrix.

 a11 + a12θ + a13θ2 + a14θ3

a21 + a22θ + a23θ2 + a24θ3
a31 + a32θ + a33θ2 + a34θ3
a41 + a42θ + a43θ2 + a44θ3



Since a twisted cubic is perhaps an unfamiliar object, various views of the curve are

shown in ﬁgure 3.5. In fact, a twisted cubic is a quite benign space curve.

Properties of a twisted cubic. Let c be a non-singular twisted cubic. Then c is not
contained within any plane of IP3; it intersects a general plane at three distinct points. A
twisted cubic has 12 degrees of freedom (counted as 15 for the matrix A, less 3 for a 1D
projectivity on the parametrization θ, which leaves the curve unaltered). Requiring the
curve to pass through a point X places two constraints on c, since X = A(1, θ, θ2, θ3)T
is three independent ratios, but only two constraints once θ is eliminated. Thus, there
is a unique c through six points in general position. Finally, all non-degenerate twisted
cubics are projectively equivalent. This is clear from the deﬁnition (3.19): a projective
transformation A−1 maps c to the standard form c(θ
(cid:2)3)T, and since

) = (1, θ

(cid:2)

(cid:2)

(cid:2)2, θ

, θ

3.4 The hierarchy of transformations

77

Fig. 3.5. Various views of the twisted cubic (t3, t2, t,) T. The curve is thickened to a tube to aid in
visualization.

all twisted cubics can be mapped to this curve, it follows that all twisted cubics are
projectively equivalent.

A classiﬁcation of the various special cases of a twisted cubic, such as a conic and
coincident line, are given in [Semple-79]. The twisted cubic makes an appearance as
the horopter for two-view geometry (chapter 9), and plays the central role in deﬁning
the degenerate set for camera resectioning (chapter 22).

3.4 The hierarchy of transformations

There are a number of specializations of a projective transformation of 3-space which
will appear frequently throughout this book. The specializations are analogous to the
strata of section 2.4(p37) for planar transformations. Each specialization is a sub-
group, and is identiﬁed by its matrix form, or equivalently by its invariants. These are
summarized in table 3.2. This table lists only the additional properties of the 3-space
transformations over their 2-space counterparts – the transformations of 3-space also
have the invariants listed in table 2.1(p44) for the corresponding 2-space transforma-
tions.

The 15 degrees of freedom of a projective transformation are accounted for as seven
for a similarity (three for rotation, three for translation, one for isotropic scaling), ﬁve
for afﬁne scalings, and three for the projective part of the transformation.

Two of the most important characterizations of these transformations are parallelism
and angles. For example, after an afﬁne transformation lines which were originally
parallel remain parallel, but angles are skewed; and after a projective transformation
parallelism is lost.

In the following we brieﬂy describe a decomposition of a Euclidean transformation

that will be useful when discussing special motions later in this book.

3.4.1 The screw decomposition
A Euclidean transformation on the plane may be considered as a specialization of a
Euclidean transformation of 3-space with the restrictions that the translation vector t
lies in the plane, and the rotation axis is perpendicular to the plane. However, Euclidean
actions on 3-space are more general than this because the rotation axis and translation
are not perpendicular in general. The screw decomposition enables any Euclidean

78

3 Projective Geometry and Transformations of 3D

Group

Projective
15 dof

(cid:25)

(cid:24)

Matrix

A
vT

t
v

(cid:24)

(cid:24)

(cid:24)

A
0T

sR
0T

R
0T

(cid:25)

(cid:25)

(cid:25)

t
1

t
1

t
1

Afﬁne
12 dof

Similarity
7 dof

Euclidean
6 dof

Distortion

Invariant properties

Intersection and tangency of sur-
faces in contact. Sign of Gaussian
curvature.

Parallelism of planes, volume ra-
tios, centroids. The plane at inﬁn-
ity, π∞, (see section 3.5).

The absolute conic, Ω∞,
(see section 3.6).

Volume.

Table 3.2. Geometric properties invariant to commonly occurring transformations of 3-space. The
matrix A is an invertible 3 × 3 matrix, R is a 3D rotation matrix, t = (tX, tY, tZ)T a 3D translation, v
a general 3-vector, v a scalar, and 0 = (0, 0, 0)T a null 3-vector. The distortion column shows typical
effects of the transformations on a cube. Transformations higher in the table can produce all the actions
of the ones below. These range from Euclidean, where only translations and rotations occur, to projective
where ﬁve points can be transformed to any other ﬁve points (provided no three points are collinear, or
four coplanar).

action (a rotation composed with a translation) to be reduced to a situation almost as
simple as the 2D case. The screw decomposition is that

Result 3.6. Any particular translation and rotation is equivalent to a rotation about a
screw axis together with a translation along the screw axis. The screw axis is parallel
to the rotation axis.

In the case of a translation and an orthogonal rotation axis (termed planar motion), the
motion is equivalent to a rotation alone about the screw axis.

Proof. We will sketch a constructive geometric proof that can easily be visualized.
Consider ﬁrst the 2D case – a Euclidean transformation on the plane.
It is evident
from ﬁgure 3.6 that a screw axis exists for such 2D transformations. For the 3D case,
decompose the translation t into two components t = t(cid:5) + t⊥, parallel and orthogonal
respectively to the rotation axis direction (t(cid:5) = (t.a)a, t⊥ = t − (t.a)a).
Then the Euclidean motion is partitioned into two parts: ﬁrst a rotation about the screw

3.5 The plane at inﬁnity

79

/

y

R(   )θ

/

x

O /

t

y

O

a

x

/

y

S

θ

/

x

O /

y

O

b

x

Fig. 3.6. 2D Euclidean motion and a “screw” axis. (a) The frame {x, y} undergoes a translation t⊥
and a rotation by θ to reach the frame {x
(cid:1)}. The motion is in the plane orthogonal to the rotation
(cid:1)
axis. (b) This motion is equivalent to a single rotation about the screw axis S. The screw axis lies on the
perpendicular bisector of the line joining corresponding points, such that the angle between the lines
joining S to the corresponding points is θ. In the ﬁgure the corresponding points are the two frame
origins and θ has the value 90◦

, y

.

screw
axis

O

t

a

S
θ

a

O /

screw
axis

O

a

S /

O /
t

S

O /

b

Fig. 3.7. 3D Euclidean motion and the screw decomposition. Any Euclidean rotation R and trans-
lation t may be achieved by (a) a rotation about the screw axis, followed by (b) a translation along the
screw axis by t(cid:6). Here a is the (unit) direction of the rotation axis (so that Ra = a), and t is decomposed
as t = t(cid:6) + t⊥, which are vector components parallel and orthogonal respectively to the rotation axis
direction The point S is closest to O on the screw axis (so that the line from S to O is perpendicular to
the direction of a). Similarly S(cid:1)

is the point on the screw axis closest to O(cid:1)

.

axis, which covers the rotation and t⊥; second a translation by t(cid:5) along the screw axis.
The complete motion is illustrated in ﬁgure 3.7.
The screw decomposition can be determined from the ﬁxed points of the 4×4 matrix
representing the Euclidean transformation. This idea is examined in the exercises at the
end of the chapter.

3.5 The plane at inﬁnity

In planar projective geometry identifying the line at inﬁnity, l∞, allowed afﬁne prop-
erties of the plane to be measured. Identifying the circular points on l∞ then allowed

80

3 Projective Geometry and Transformations of 3D

the measurement of metric properties. In the projective geometry of 3-space the corre-
sponding geometric entities are the plane at inﬁnity, π∞, and the absolute conic, Ω∞.

The plane at inﬁnity has the canonical position π∞ = (0, 0, 0, 1)T in afﬁne 3-space.
It contains the directions D = (X1, X2, X3, 0)T, and enables the identiﬁcation of afﬁne
properties such as parallelism. In particular:
• Two planes are parallel if, and only if, their line of intersection is on π∞.
• A line is parallel to another line, or to a plane, if the point of intersection is on π∞.
We then have in IP3 that any pair of planes intersect in a line, with parallel planes
intersecting in a line on the plane at inﬁnity.

The plane π∞ is a geometric representation of the 3 degrees of freedom required
to specify afﬁne properties in a projective coordinate frame. In loose terms, the plane
at inﬁnity is a ﬁxed plane under any afﬁne transformation, but “sees” (is moved by) a
projective transformation. The 3 degrees of freedom of π∞ thus measure the projective
component of a general homography – they account for the 15 degrees of freedom of
this general transformation compared to an afﬁnity (12 dof). More formally:

Result 3.7. The plane at inﬁnity, π∞, is a ﬁxed plane under the projective transforma-
tion H if, and only if, H is an afﬁnity.

The proof is the analogue of the derivation of result 2.17(p48). It is worth clarifying
two points:

(i) The plane π∞ is, in general, only ﬁxed as a set under an afﬁnity; it is not ﬁxed

pointwise.

(ii) Under a particular afﬁnity (for example a Euclidean motion) there may be
planes in addition to π∞ which are ﬁxed. However, only π∞ is ﬁxed under
any afﬁnity.

These points are illustrated in more detail by the following example.

Example 3.8. Consider the Euclidean transformation represented by the matrix

(cid:17)

(cid:18)

HE =

R 0
0T 1

=

 cos θ − sin θ 0 0

cos θ

 .

0 0
1 0
0 1

0
0

sin θ

0
0

(3.20)

This is a rotation by θ about the Z-axis with a zero translation (it is a planar screw
motion, see section 3.4.1). Geometrically it is evident that the family of XY-planes or-
thogonal to the rotation axis are simply rotated about the Z-axis by this transformation.
This means that there is a pencil of ﬁxed planes orthogonal to the Z-axis. The planes
are ﬁxed as sets, but not pointwise as any (ﬁnite) point (not on the axis) is rotated in
horizontal circles by this Euclidean action. Algebraically, the ﬁxed planes of H are the
eigenvectors of HT (refer to section 2.9). In this case the eigenvalues are {eiθ, e
−iθ, 1, 1}

and the corresponding eigenvectors of HT

 1

i
0
0

E1 =

3.6 The absolute conic

 E2 =

 1−i

E are

 E3 =

 0

0
1
0

81

 E4 =

 .

 0

0
0
1

0
0

The eigenvectors E1 and E2 do not correspond to real planes, and will not be discussed
further here. The eigenvectors E3 and E4 are degenerate. Thus there is a pencil of
ﬁxed planes which is spanned by these eigenvectors. The axis of this pencil is the line
of intersection of the the planes (perpendicular to the Z-axis) with π∞, and the pencil
(cid:2)
includes π∞.

The example also illustrates the connection between the geometry of the projective
plane, IP2, and projective 3-space, IP3. A plane π intersects π∞ in a line which is
the line at inﬁnity, l∞, of the plane π. A projective transformation of IP3 induces a
subordinate plane projective transformation on π.

Afﬁne properties of a reconstruction.
In later chapters on reconstruction, for exam-
ple chapter 10, it will be seen that the projective coordinates of the (Euclidean) scene
can be reconstructed from multiple views. Once π∞ is identiﬁed in projective 3-space,
i.e. its projective coordinates are known, it is then possible to determine afﬁne prop-
erties of the reconstruction such as whether geometric entities are parallel – they are
parallel if they intersect on π∞.

A more algorithmic approach is to transform IP3 so that the identiﬁed π∞ is moved
to its canonical position at π∞ = (0, 0, 0, 1)T. After this mapping we then have the
situation that the Euclidean scene, where π∞ has the coordinates (0, 0, 0, 1)T, and our
reconstruction are related by a projective transformation that ﬁxes π∞ at (0, 0, 0, 1)T. It
follows from result 3.7 that the scene and reconstruction are related by an afﬁne trans-
formation. Thus afﬁne properties can now be measured directly from the coordinates
of the entities.

3.6 The absolute conic

The absolute conic, Ω∞, is a (point) conic on π∞. In a metric frame π∞ = (0, 0, 0, 1)T,
and points on Ω∞ satisfy

= 0.

(3.21)

Note that two equations are required to deﬁne Ω∞.

For directions on π∞ (i.e. points with X4 = 0 ) the deﬁning equation can be written

(X1, X2, X3)I(X1, X2, X3)T = 0

so that Ω∞ corresponds to a conic C with matrix C = I. It is thus a conic of purely
imaginary points on π∞.

(cid:26)

X2
1 + X2

2 + X2
3
X4

82

3 Projective Geometry and Transformations of 3D

The conic Ω∞ is a geometric representation of the 5 additional degrees of freedom
that are required to specify metric properties in an afﬁne coordinate frame. A key
property of Ω∞ is that it is a ﬁxed conic under any similarity transformation. More
formally:

Result 3.9. The absolute conic, Ω∞, is a ﬁxed conic under the projective transformation
H if, and only if, H is a similarity transformation.

Proof. Since the absolute conic lies in the plane at inﬁnity, a transformation ﬁxing it
must ﬁx the plane at inﬁnity, and hence must be afﬁne. Such a transformation is of the
form

(cid:17)

(cid:18)

.

HA =

t
A
0T 1

Restricting to the plane at inﬁnity, the absolute conic is represented by the matrix I3×3,
and since it is ﬁxed by HA, one has A−TIA−1 = I (up to scale), and taking inverses gives
AAT = I. This means that A is orthogonal, hence a scaled rotation, or scaled rotation
with reﬂection. This completes the proof.

Even though Ω∞ does not have any real points, it shares the properties of any conic –
such as that a line intersects a conic in two points; the pole–polar relationship etc. Here
are a few particular properties of Ω∞:

(i) Ω∞ is only ﬁxed as a set by a general similarity; it is not ﬁxed pointwise. This
means that under a similarity a point on Ω∞ may travel to another point on Ω∞,
but it is not mapped to a point off the conic.

(ii) All circles intersect Ω∞ in two points. Suppose the support plane of the circle
is π. Then π intersects π∞ in a line, and this line intersects Ω∞ in two points.
These two points are the circular points of π.

(iii) All spheres intersect π∞ in Ω∞.

Metric properties. Once Ω∞ (and its support plane π∞) have been identiﬁed in
projective 3-space then metric properties, such as angles and relative lengths, can be
measured.

Consider two lines with directions (3-vectors) d1 and d2. The angle between these

directions in a Euclidean world frame is given by
(dT
d2)
1
d1)(dT
2

cos θ =

(cid:19)

(dT
1

.

d2)

This may be written as

(cid:19)

(dT
1

(dT
Ω∞d2)
1
Ω∞d1)(dT
2

Ω∞d2)

cos θ =

(3.22)

(3.23)

where d1 and d2 are the points of intersection of the lines with the plane π∞ containing
the conic Ω∞, and Ω∞ is the matrix representation of the absolute conic in that plane.

3.7 The absolute dual quadric

83

d

1

a

Ω

d

2

π

Ω

d

b

l

π

Fig. 3.8. Orthogonality and Ω∞. (a) On π∞ orthogonal directions d1, d2 are conjugate with respect
to Ω∞. (b) A plane normal direction d and the intersection line l of the plane with π∞ are in pole–polar
relation with respect to Ω∞.

The expression (3.23) reduces to (3.22) in a Euclidean world frame where Ω∞ = I.
However, the expression is valid in any projective coordinate frame as may be veriﬁed
from the transformation properties of points and conics (see (iv)(b) on page 63).

There is no simple formula for the angle between two planes computed from the

directions of their surface normals.

Orthogonality and polarity. We now give a geometric representation of orthogo-
nality in a projective space based on the absolute conic. The main device will be the
pole–polar relationship between a point and line induced by a conic.

An immediate consequence of (3.23) is that two directions d1 and d2 are orthogonal
if dT
Ω∞d2 = 0. Thus orthogonality is encoded by conjugacy with respect to Ω∞. The
1
great advantage of this is that conjugacy is a projective relation, so that in a projective
frame (obtained by a projective transformation of Euclidean 3-space) directions can
be identiﬁed as orthogonal if they are conjugate with respect to Ω∞ in that frame (in
general the matrix of Ω∞ is not I in a projective frame). The geometric representation
of orthogonality is shown in ﬁgure 3.8.

This representation is helpful when considering orthogonality between rays in a
camera, for example in determining the normal to a plane through the camera cen-
tre (see section 8.6(p213)). If image points are conjugate with respect to the image of
Ω∞ then the corresponding rays are orthogonal.

Again, a more algorithmic approach is to projectively transform the coordinates so
that Ω∞ is mapped to its canonical position (3.21), and then metric properties can be
determined directly from coordinates.

3.7 The absolute dual quadric

Recall that Ω∞ is deﬁned by two equations – it is a conic on the plane at inﬁnity. The
dual of the absolute conic Ω∞ is a degenerate dual quadric in 3-space called the absolute
dual quadric, and denoted Q∗
∞. Geometrically Q∗
∞ consists of the planes tangent to Ω∞,
so that Ω∞ is the “rim” of Q∗
∞. This is called a rim quadric. Think of the set of planes
tangent to an ellipsoid, and then squash the ellipsoid to a pancake.
∞ is represented by a 4 × 4 homogeneous matrix of rank 3, which in

Algebraically Q∗

84

3 Projective Geometry and Transformations of 3D

metric 3-space has the canonical form
Q∗
∞ =

(cid:17)

(cid:18)

I 0
0T 0

.

(3.24)

We will show that any plane in the dual absolute quadric envelope is indeed tangent
to Ω∞, so the Q∗
∞ is truly a dual of Ω∞. Consider a plane represented by π = (vT, k)T.
This plane is in the envelope deﬁned by Q∗
∞π = 0, which given
the form (3.24) is equivalent to vTv = 0. Now, (see section 8.6(p213)) v represents
the line in which the plane (vT, k)T meets the plane at inﬁnity. This line is tangent to
the absolute conic if and only if vTIv = 0. Thus, the envelope of Q∗
∞ is made up of
just those planes tangent to the absolute conic.

∞ if and only if πTQ∗

Since this is an important fact, we consider it from another angle. Consider the ab-
solute conic as the limit of a series of squashed ellipsoids, namely quadrics represented
by the matrix Q = diag(1, 1, 1, k). As k → ∞, these quadrics become increasingly
close to the plane at inﬁnity, and in the limit the only points they contain are the points
(X1, X2, X3, 0)T with X2
3 = 0, that is points on the absolute conic. However,
1 + X2
the dual of Q is the quadric Q∗
−1), which in the limit becomes
the absolute dual quadric Q∗

= Q−1 = diag(1, 1, 1, k

∞ = diag(1, 1, 1, 0).

2 + X2

∞ is a degenerate quadric and has 8 degrees of freedom (a symmet-
ric matrix has 10 independent elements, but the irrelevant scale and zero determinant
condition each reduce the degrees of freedom by 1). It is a geometric representation of
the 8 degrees of freedom that are required to specify metric properties in a projective
coordinate frame. Q∗
∞ has a signiﬁcant advantage over Ω∞ in algebraic manipulations
because both π∞ and Ω∞ are contained in a single geometric object (unlike Ω∞ which
requires two equations (3.21) in order to specify it). In the following we give its three
most important properties.
Result 3.10. The absolute dual quadric, Q∗
tion H if, and only if, H is a similarity.

∞, is ﬁxed under the projective transforma-

The dual quadric Q∗

Proof. This follows directly from the invariance of the absolute conic under a simi-
larity transform, since the planar tangency relationship between Q∗
∞ and Ω∞ is transfor-
mation invariant. Nevertheless, we give an independent direct proof.
Since Q∗
if and only if Q∗

∞ is a dual quadric, it transforms according to (3.17–p74), so it is ﬁxed under H

∞HT. Applying this with an arbitrary transform

∞ = HQ∗

we ﬁnd

(cid:17)

I 0
0T 0

(cid:18)

(cid:17)
(cid:17)

=

=

(cid:17)

H =

(cid:18)

t
A
vT k

(cid:18)(cid:17)

I 0
0T 0

(cid:18)

t
A
vT k
AAT
Av
vTAT vTv

(cid:18)(cid:17)

(cid:18)

AT v
tT k

3.8 Closure

85
which must be true up to scale. By inspection, this equation holds if and only if v = 0
and A is a scaled orthogonal matrix (scaling, rotation and possible reﬂection). In other
words, H is a similarity transform.
Result 3.11. The plane at inﬁnity π∞ is the null-vector of Q∗
∞.
This is easily veriﬁed when Q∗
∞ has its canonical form (3.24) in a metric frame since
then, with π∞ = (0, 0, 0, 1)T, Q∗
∞π∞ = 0. This property holds in any frame as may
be readily seen algebraically from the transformation properties of planes and dual
= HX, then Q∗
= H Q∗
quadrics: if X(cid:2)
∞ HT, π(cid:2)
∞
∞ = (H Q∗
∞ HT)H−Tπ∞ = HQ∗
(cid:19)

∞π∞ = 0.
Result 3.12. The angle between two planes π1 and π2 is given by

∞ = H−Tπ∞, and

Q∗
∞

(cid:2)π(cid:2)

(cid:2)

cos θ =

(3.25)

Q∗
πT
∞π2
1
Q∗∞π1) (πT

2

(πT
1

.

Q∗∞π2)

Proof.
π2 = (nT
to

Consider two planes with Euclidean coordinates π1 = (nT

1 , d1)T,
∞ has the form (3.24), and (3.25) reduces

2 , d2)T.

In a Euclidean frame, Q∗
(cid:19)

cos θ =

n2

nT
1
n1) (nT
2

n2)

(nT
1

which is the angle between the planes expressed in terms of a scalar product of their
normals.
If the planes and Q∗
∞ are projectively transformed, (3.25) will still determine the angle
between planes due to the (covariant) transformation properties of planes and dual
quadrics.

The details of the last part of the proof are left as an exercise, but are a direct 3D
analogue of the derivation of result 2.23(p54) on the angle between two lines in IP2
computed using the dual of the circular points. Planes in IP3 are the analogue of lines
in IP2, and the absolute dual quadric is the analogue of the dual of the circular points.

3.8 Closure

3.8.1 The literature
The textbooks cited in chapter 2 are also relevant here. See also [Boehm-94] for a gen-
eral background from the perspective of descriptive geometry, and Hilbert and Cohn-
Vossen [Hilbert-56] for many clearly explained properties of curves and surfaces.

An important representation for points, lines and planes in IP3, which is omitted
in this chapter, is the Grassmann–Cayley algebra.
In this representation geometric
operations such as incidence and joins are represented by a “bracket algebra” based on
matrix determinants. A good introduction to this area is given by [Carlsson-94], and
its application to multiple view tensors is illustrated in [Triggs-95].

86

3 Projective Geometry and Transformations of 3D

Faugeras and Maybank [Faugeras-90] introduced Ω∞ into the computer vision liter-
ature (in order to determine the multiplicity of solutions for relative orientation), and
Triggs introduced Q∗

∞ in [Triggs-97] for use in auto-calibration.

3.8.2 Notes and exercises

(i) Pl¨ucker coordinates.

(a) Using Pl¨ucker line coordinates, L, write an expression for the point of
intersection of a line with a plane, and the plane deﬁned by a point and
a line.

(b) Now derive the condition for a point to be on a line, and a line to be on

(c) Show that parallel planes intersect in a line on π∞. Hint, start from (3.9–

a plane.
p71) to determine the line of intersection of two parallel planes L∗

.

(d) Show that parallel lines intersect on π∞.

(ii) Projective transformations. Show that a (real) projective transformation of
3-space can map an ellipsoid to a paraboloid or hyperboloid of two sheets, but
cannot map an ellipsoid to a hyperboloid of one sheet (i.e. a surface with real
rulings).
(iii) Screw decomposition. Show that the 4 × 4 matrix representing the Euclidean
transformation {R, t} (with a the direction of the rotation axis, i.e. Ra = a) has
two complex conjugate eigenvalues, and two equal real eigenvalues, and the
following eigenvector structure:

(a) if a is perpendicular to t, then the eigenvectors corresponding to the real

eigenvalues are distinct;

(b) otherwise, the eigenvectors corresponding to the real eigenvalues are

coincident, and on π∞.

(E.g. choose simple cases such as (3.20), another case is given on page 495).
In the ﬁrst case the two real points corresponding to the real eigenvalues deﬁne
a line of ﬁxed points. This is the screw axis for planar motion. In the second
case, the direction of the screw axis is deﬁned, but it is not a line of ﬁxed points.
What do the eigenvectors corresponding to the complex eigenvalues represent?

4

Estimation – 2D Projective Transformations

In this chapter, we consider the problem of estimation.
In the present context this
will be taken to mean the computation of some transformation or other mathematical
quantity, based on measurements of some nature. This deﬁnition is somewhat vague,
so to make it more concrete, here are a number of estimation problems of the type that
we would like to consider.

(i) 2D homography. Given a set of points xi in IP2 and a corresponding set of
i likewise in IP2, compute the projective transformation that takes each
i. In a practical situation, the points xi and x(cid:2)
i are points in two images

points x(cid:2)
xi to x(cid:2)
(or the same image), each image being considered as a projective plane IP2.

(ii) 3D to 2D camera projection. Given a set of points Xi in 3D space, and a set
of corresponding points xi in an image, ﬁnd the 3D to 2D projective mapping
that maps Xi to xi. Such a 3D to 2D projection is the mapping carried out by a
projective camera, as discussed in chapter 6.
(iii) Fundamental matrix computation. Given a set of points xi in one image,
and corresponding points x(cid:2)
i in another image, compute the fundamental matrix
F consistent with these correspondences. The fundamental matrix, discussed
in chapter 9, is a singular 3 × 3 matrix F satisfying x(cid:2)
(iv) Trifocal tensor computation. Given a set of point correspondences xi ↔
x(cid:2)
i across three images, compute the trifocal tensor. The trifocal tensor,
discussed in chapter 15, is a tensor T jk
relating points or lines in three views.

TFxi = 0 for all i.

i

↔ x(cid:2)(cid:2)

i

i

These problems have many features in common, and the considerations that relate to
one of the problems are also relevant to each of the others. Therefore, in this chapter,
the ﬁrst of these problems will be considered in detail. What we learn about ways of
solving this problem will teach us how to proceed in solving each of the other problems
as well.

Apart from being important for illustrative purposes, the problem of estimating 2D
projective transformations is of importance in its own right. We consider a set of point
correspondences xi ↔ x(cid:2)
i between two images. Our problem is to compute a 3 × 3
matrix H such that Hxi = x(cid:2)
i for each i.

87

88

4 Estimation – 2D Projective Transformations

Number of measurements required. The ﬁrst question to consider is how many
corresponding points xi ↔ x(cid:2)
i are required to compute the projective transformation H.
A lower bound is available by a consideration of the number of degrees of freedom and
number of constraints. On the one hand, the matrix H contains 9 entries, but is deﬁned
only up to scale. Thus, the total number of degrees of freedom in a 2D projective trans-
formation is 8. On the other hand, each point-to-point correspondence accounts for two
constraints, since for each point xi in the ﬁrst image the two degrees of freedom of the
point in the second image must correspond to the mapped point Hxi. A 2D point has
two degrees of freedom corresponding to the x and y components, each of which may
be speciﬁed separately. Alternatively, the point is speciﬁed as a homogeneous 3-vector,
which also has two degrees of freedom since scale is arbitrary. As a consequence, it is
necessary to specify four point correspondences in order to constrain H fully.

Approximate solutions.
It will be seen that if exactly four correspondences are given,
then an exact solution for the matrix H is possible. This is the minimal solution. Such
solutions are important as they deﬁne the size of the subsets required in robust estima-
tion algorithms, such as RANSAC, described in section 4.7. However, since points are
measured inexactly (“noise”), if more than four such correspondences are given, then
these correspondences may not be fully compatible with any projective transformation,
and one will be faced with the task of determining the “best” transformation given the
data. This will generally be done by ﬁnding the transformation H that minimizes some
cost function. Different cost functions will be discussed during this chapter, together
with methods for minimizing them. There are two main categories of cost function:
those based on minimizing an algebraic error; and those based on minimizing a geo-
metric or statistical image distance. These two categories are described in section 4.2.

The Gold Standard algorithm. There will usually be one cost function which is
optimal in the sense that the H that minimizes it gives the best possible estimate of the
transformation under certain assumptions. The computational algorithm that enables
this cost function to be minimized is called the “Gold Standard” algorithm. The results
of other algorithms are assessed by how well they compare to this Gold Standard. In
the case of estimating a homography between two views the cost function is (4.8), the
assumptions for optimality are given in section 4.3, and the Gold Standard is algorithm
4.3(p114).

4.1 The Direct Linear Transformation (DLT) algorithm

i. The transformation is given by the equation x(cid:2)

We begin with a simple linear algorithm for determining H given a set of four 2D to 2D
point correspondences, xi ↔ x(cid:2)
i = Hxi.
Note that this is an equation involving homogeneous vectors; thus the 3-vectors x(cid:2)
i and
Hxi are not equal, they have the same direction but may differ in magnitude by a non-
zero scale factor. The equation may be expressed in terms of the vector cross product
as x(cid:2)

× Hxi = 0. This form will enable a simple linear solution for H to be derived.

i

4.1 The Direct Linear Transformation (DLT) algorithm

89

If the j-th row of the matrix H is denoted by hj T, then we may write

Writing x(cid:2)

i = (x

(cid:2)
i, y

(cid:2)
i, w

(cid:2)
i)T, the cross product may then be given explicitly as

 h1Txi

h2Txi
h3Txi

.

Hxi =

 y

x(cid:2)

i

× Hxi =

h3Txi − w
(cid:2)
(cid:2)
h2Txi
h1Txi − x
(cid:2)
(cid:2)
i
i
h3Txi
w
h2Txi − y
(cid:2)
(cid:2)
i
i
h1Txi
x
i
i

.

 h1
 = 0.

 h1 h2 h3

h2
h3

h4 h5 h6
h7 h8 h9

hj for j = 1, . . . ,3 , this gives a set of three equations in the entries

Since hj Txi = xT
i
of H, which may be written in the form
xT
i



0T
(cid:2)
xT
w
−y
(cid:2)
i
i
xT
i
i

−w
(cid:2)
i
0T
(cid:2)
xT
x
i
i

(cid:2)
xT
y
−x
(cid:2)
i
i
xT
i
i
0T

These equations have the form Aih = 0, where Ai is a 3 × 9 matrix, and h is a 9-vector
made up of the entries of the matrix H,

 ,

 h1

h2
h3

h =

H =

with hi the i−th element of h. Three remarks regarding these equations are in order
here.

(i) The equation Aih = 0 is an equation linear in the unknown h. The matrix

elements of Ai are quadratic in the known coordinates of the points.

(ii) Although there are three equations in (4.1), only two of them are linearly inde-
(cid:2)
pendent (since the third row is obtained, up to scale, from the sum of x
i times
(cid:2)
the ﬁrst row and y
i times the second). Thus each point correspondence gives
two equations in the entries of H. It is usual to omit the third equation in solv-
ing for H ([Sutherland-63]). Then (for future reference) the set of equations
becomes

(cid:17)

0T −w
(cid:2)
(cid:2)
i
xT
0T
w
i
i

xT
i

(cid:2)
xT
y
−x
(cid:2)
i
i
xT
i
i

(cid:18) h1

h2
h3

 = 0.

This will be written

Aih = 0
where Ai is now the 2 × 9 matrix of (4.3).
(cid:2)
(cid:2)
i)T
i, y
of the point x(cid:2)
(cid:2)
i) are the
coordinates measured in the image. Other choices are possible, however, as
will be seen later.

(iii) The equations hold for any homogeneous coordinate representation (x
(cid:2)
i, y

(cid:2)
i = 1, which means that (x

i. One may choose w

(cid:2)
i, w

(4.1)

(4.2)

(4.3)

90

4 Estimation – 2D Projective Transformations

Solving for H
Each point correspondence gives rise to two independent equations in the entries of H.
Given a set of four such point correspondences, we obtain a set of equations Ah = 0,
where A is the matrix of equation coefﬁcients built from the matrix rows Ai contributed
from each correspondence, and h is the vector of unknown entries of H. We seek a
non-zero solution h, since the obvious solution h = 0 is of no interest to us. If (4.1) is
used then A has dimension 12 × 9, and if (4.3) the dimension is 8 × 9. In either case
A has rank 8, and thus has a 1-dimensional null-space which provides a solution for h.
Such a solution h can only be determined up to a non-zero scale factor. However, H is
in general only determined up to scale, so the solution h gives the required H. A scale
may be arbitrarily chosen for h by a requirement on its norm such as (cid:10)h(cid:10) = 1.

4.1.1 Over-determined solution
If more than four point correspondences xi ↔ x(cid:2)
i are given, then the set of equations
Ah = 0 derived from (4.3) is over-determined. If the position of the points is exact
then the matrix A will still have rank 8, a one dimensional null-space, and there is an
exact solution for h. This will not be the case if the measurement of image coordinates
is inexact (generally termed noise) – there will not be an exact solution to the over-
determined system Ah = 0 apart from the zero solution. Instead of demanding an
exact solution, one attempts to ﬁnd an approximate solution, namely a vector h that
minimizes a suitable cost function. The question that naturally arises then is: what
should be minimized? Clearly, to avoid the solution h = 0 an additional constraint is
required. Generally, a condition on the norm is used, such as (cid:10)h(cid:10) = 1. The value of
the norm is unimportant since H is only deﬁned up to scale. Given that there is no exact
solution to Ah = 0, it seems natural to attempt to minimize the norm (cid:10)Ah(cid:10) instead,
subject to the usual constraint, (cid:10)h(cid:10) = 1. This is identical to the problem of ﬁnding
the minimum of the quotient (cid:10)Ah(cid:10)/(cid:10)h(cid:10). As shown in section A5.3(p592) the solution
is the (unit) eigenvector of ATA with least eigenvalue. Equivalently, the solution is the
unit singular vector corresponding to the smallest singular value of A. The resulting
algorithm, known as the basic DLT algorithm, is summarized in algorithm 4.1.

4.1.2 Inhomogeneous solution
An alternative to solving for h directly as a homogeneous vector is to turn the set of
equations (4.3) into a inhomogeneous set of linear equations by imposing a condition
hj = 1 for some entry of the vector h. Imposing the condition hj = 1 is justiﬁed by
the observation that the solution is determined only up to scale, and this scale can be
chosen such that hj = 1. For example, if the last element of h, which corresponds to
H33, is chosen as unity then the resulting equations derived from (4.3) are

(cid:17)

0
xiw

0
(cid:2)
i yiw

0
(cid:2)
i wiw

(cid:2)
i

−xiw

(cid:2)
i

−yiw

(cid:2)
i

−wiw

0

0

0

(cid:2)
i

(cid:2)
xiy
−xix
i

(cid:2)
i

(cid:2)
yiy
−yix
i

(cid:2)
i

˜h =

(cid:18)

(cid:21)

(cid:20) −wiy

(cid:2)
i

(cid:2)
i

wix

where ˜h is an 8-vector consisting of the ﬁrst 8 components of h. Concatenating the
equations from four correspondences then generates a matrix equation of the form

4.1 The Direct Linear Transformation (DLT) algorithm

91

Objective
Given n ≥ 4 2D to 2D point correspondences {xi ↔ x(cid:1)
matrix H such that x(cid:1)
Algorithm

i = Hxi.

i}, determine the 2D homography

rows need be used in general.

i compute the matrix Ai from (4.1). Only the ﬁrst two

(i) For each correspondence xi ↔ x(cid:1)
(ii) Assemble the n 2 × 9 matrices Ai into a single 2n × 9 matrix A.
(iii) Obtain the SVD of A (section A4.4(p585)). The unit singular vector corresponding to
the smallest singular value is the solution h. Speciﬁcally, if A = UDVT with D diagonal
with positive diagonal entries, arranged in descending order down the diagonal, then h
is the last column of V.

(iv) The matrix H is determined from h as in (4.2).

Algorithm 4.1. The basic DLT for H (but see algorithm 4.2(p109) which includes normalization).

M˜h = b, where M has 8 columns and b is an 8-vector. Such an equation may be solved
for ˜h using standard techniques for solving linear equations (such as Gaussian elimina-
tion) in the case where M contains just 8 rows (the minimum case), or by least-squares
techniques (section A5.1(p588)) in the case of an over-determined set of equations.

However, if in fact hj = 0 is the true solution, then no multiplicative scale k can
exist such that khj = 1. This means that the true solution cannot be reached. For this
reason, this method can be expected to lead to unstable results in the case where the
chosen hj is close to zero. Consequently, this method is not recommended in general.

Example 4.1. It will be shown that h9 = H33 is zero if the coordinate origin is mapped
to a point at inﬁnity by H. Since (0, 0, 1)T represents the coordinate origin x0, and
also (0, 0, 1)T represents the line at inﬁnity l, this condition may be written as lTHx0 =
(0, 0, 1)H(0, 0, 1)T = 0, thus H33 = 0. In a perspective image of a scene plane the line
at inﬁnity is imaged as the vanishing line of the plane (see chapter 8), for example the
horizon is the vanishing line of the ground plane. It is not uncommon for the horizon to
pass through the image centre, and for the coordinate origin to coincide with the image
centre. In this case the mapping that takes the image to the world plane maps the origin
to the line at inﬁnity, so that the true solution has H33 = h9 = 0. Consequently, an
(cid:2)
h9 = 1 normalization can be a serious failing in practical situations.

4.1.3 Degenerate conﬁgurations
Consider a minimal solution in which a homography is computed using four point cor-
respondences, and suppose that three of the points x1, x2, x3 are collinear. The question
is whether this is signiﬁcant. If the corresponding points x(cid:2)
3 are also collinear
then one might suspect that the homography is not sufﬁciently constrained, and there
will exist a family of homographies mapping xi to x(cid:2)
i. On the other hand, if the corre-
sponding points x(cid:2)
1, x(cid:2)
3 are not collinear then clearly there can be no transformation
H taking xi to x(cid:2)
i, since a projective transformation must preserve collinearity. Never-

1, x(cid:2)

2, x(cid:2)

2, x(cid:2)

92

4 Estimation – 2D Projective Transformations

= x(cid:2)

4

is that H∗

The equations (4.3) express the condition that x(cid:2)

4. Therefore the condition x(cid:2)
corresponding to H∗

i

theless the set of eight homogeneous equations derived from (4.3) must have a non-zero
solution, giving rise to a matrix H. How is this apparent contradiction to be resolved?
× Hxi = 0 for i = 1, . . . ,4 , and
so the matrix H found by solving the system of 8 equations will satisfy this condition.
Suppose that x1, . . . ,x 3 are collinear and let l be the line that they lie on, so that lTxi =
lT, which is a 3×3 matrix of rank 1. In this case,
0 for i = 1, . . . ,3 . Now deﬁne H∗
one veriﬁes that H∗xi = x(cid:2)
4(lTxi) =0 for i = 1, . . . ,3 , since lTxi = 0. On the other
× H∗xi = 0 is satisﬁed for
hand, H∗x4 = x(cid:2)
4(lTx4) = kx(cid:2)
all i. Note that the vector h∗
is given by h∗T = (x4lT, y4lT, w4lT),
and one easily veriﬁes that this vector satisﬁes (4.3) for all i. The problem with this
solution for H∗
is a rank 1 matrix and hence does not represent a projective
transformation. As a consequence the points H∗xi = 0 for i = 1, . . . ,3 are not well
deﬁned.
lT is a solution to (4.1). There
are two cases: either H∗
is the unique solution (up to scale) or there is a further solution
H. In the ﬁrst case, since H∗
is a singular matrix, there exists no transformation taking
each xi to x(cid:2)
3 are not. In the
second case, where a further solution H exists, then any matrix of the form α H∗
+ β H
is a solution. Thus a 2-dimensional family of transformations exist, and it follows that
the 8 equations derived from (4.3) are not independent.

i. This occurs when x1, . . . ,x 3 are collinear but x(cid:2)

We showed that if x1, x2, x3 are collinear then H∗

1, . . . ,x (cid:2)

i

= x(cid:2)

4

A situation where a conﬁguration does not determine a unique solution for a particu-
lar class of transformation is termed degenerate. Note that the deﬁnition of degeneracy
involves both the conﬁguration and the type of transformation. The degeneracy prob-
lem is not limited to a minimal solution, however. If additional (perfect, i.e. error-free)
correspondences are supplied which are also collinear (lie on l), then the degeneracy is
not resolved.

4.1.4 Solutions from lines and other entities
The development to this point, and for the rest of the chapter, is exclusively in terms of
computing homographies from point correspondences. However, an identical develop-
ment can be given for computing homographies from line correspondences. Starting
from the line transformation li = HTl(cid:2)
i, a matrix equation of the form Ah = 0 can be
derived, with a minimal solution requiring four lines in general position. Similarly, a
homography may be computed from conic correspondences and so forth.

There is the question then of how many correspondences are required to compute the
homography (or any other relation). The general rule is that the number of constraints
must equal or exceed the number of degrees of freedom of the transformation. For
example, in 2D each corresponding point or line generates two constraints on H, in
3D each corresponding point or plane generates three constraints. Thus in 2D the
correspondence of four points or four lines is sufﬁcient to compute H, since 4 × 2 = 8,
with 8 the number of degrees of freedom of the homography. In 3D a homography has
15 degrees of freedom, and ﬁve points or ﬁve planes are required. For a planar afﬁne
transformation (6 dof) only three corresponding points or lines are required, and so on.
A conic provides ﬁve constraints on a 2D homography.

4.2 Different cost functions

93

=

=

Fig. 4.1. Geometric equivalence of point–line conﬁgurations. A conﬁguration of two points and two
lines is equivalent to ﬁve lines with four concurrent, or ﬁve points with four collinear.

Care has to be taken when computing H from correspondences of mixed type. For
example, a 2D homography cannot be determined uniquely from the correspondences
of two points and two lines, but can from three points and one line or one point and three
lines, even though in each case the conﬁguration has 8 degrees of freedom. The case of
three lines and one point is geometrically equivalent to four points, since the three lines
deﬁne a triangle and the vertices of the triangle uniquely deﬁne three points. We have
seen that the correspondence of four points in general position uniquely determines a
homography, which means that the correspondence of three lines and one point also
uniquely determines a homography. Similarly the case of three points and a line is
equivalent to four lines, and again the correspondence of four lines in general position
(i.e. no three concurrent) uniquely determines a homography. However, as a quick
sketch shows (ﬁgure 4.1), the case of two points and two lines is equivalent to ﬁve
lines with four concurrent, or ﬁve points with four collinear. As shown in the previous
section, this conﬁguration is degenerate and a one-parameter family of homographies
map the two-point and two-line conﬁguration to the corresponding conﬁguration.

4.2 Different cost functions

We will now describe a number of cost functions which may be minimized in order to
determine H for over-determined solutions. Methods of minimizing these functions are
described later in the chapter.

4.2.1 Algebraic distance
The DLT algorithm minimizes the norm (cid:10)Ah(cid:10). The vector  = Ah is called the residual
vector and it is the norm of this error vector that is minimized. The components of this
vector arise from the individual correspondences that generate each row of the matrix
A. Each correspondence xi ↔ x(cid:2)
i contributes a partial error vector i from (4.1) or (4.3)
towards the full error vector . This vector i is the algebraic error vector associated
with the point correspondence xi ↔ x(cid:2)
i and the homography H. The norm of this vector
is a scalar which is called the algebraic distance:

dalg(x(cid:2)

i, Hxi)2 = (cid:10)i(cid:10)2 =

0T −w
(cid:2)
(cid:2)
i
xT
0T
w
i
i

xT
i

(cid:2)
xT
y
−x
(cid:2)
i
i
xT
i
i

(cid:17)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

(cid:18)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

h

.

(4.4)

More generally, and brieﬂy, for any two vectors x1 and x2 we may write
2 where a = (a1, a2, a3)T = x1 × x2.

dalg(x1, x2)2 = a2

1 + a2

94

4 Estimation – 2D Projective Transformations

The relation of this distance to a geometric distance is described in section 4.2.4.

Given a set of correspondences, the quantity  = Ah is the algebraic error vector for

the complete set, and one sees that

(cid:7)

(cid:7)

dalg(x(cid:2)

i, Hxi)2 =

(cid:10)i(cid:10)2 = (cid:10)Ah(cid:10)2 = (cid:10)(cid:10)2.

(4.5)

i

i

The concept of algebraic distance originated in the conic-ﬁtting work of Book-
stein [Bookstein-79].
Its disadvantage is that the quantity that is minimized is not
geometrically or statistically meaningful. As Bookstein demonstrated, the solutions
that minimize algebraic distance may not be those expected intuitively. Nevertheless,
with a good choice of normalization (as will be discussed in section 4.4) methods which
minimize algebraic distance do give very good results. Their particular advantages are
a linear (and thus a unique) solution, and computational cheapness. Often solutions
based on algebraic distance are used as a starting point for a non-linear minimization
of a geometric or statistical cost function. The non-linear minimization gives the solu-
tion a ﬁnal “polish”.

4.2.2 Geometric distance
Next we discuss alternative error functions based on the measurement of geometric
distance in the image, and minimization of the difference between the measured and
estimated image coordinates.

Notation. Vectors x represent the measured image coordinates; ˆx represent estimated
values of the points and ¯x represent true values of the points.

Error in one image. We start by considering error only in the second image, with
points in the ﬁrst measured perfectly. Clearly, this will not be true in most practical
situations with images. An example where the assumption is more reasonable is in
estimating the projective transformation between a calibration pattern or a world plane,
where points are measured to a very high accuracy, and its image. The appropriate
quantity to be minimized is the transfer error. This is the Euclidean image distance
in the second image between the measured point x(cid:2)
and the point H¯x at which the
corresponding point ¯x is mapped from the ﬁrst image. We use the notation d(x, y) to
represent the Euclidean distance between the inhomogeneous points represented by x
and y. Then the transfer error for the set of correspondences is

(cid:7)

d(x(cid:2)

i, H¯xi)2.

(4.6)

The estimated homography ˆH is the one for which the error (4.6) is minimized.

i

Symmetric transfer error. In the more realistic case where image measurement errors
occur in both the images, it is preferable that errors be minimized in both images, and
not solely in the one. One way of constructing a more satisfactory error function is to

95
consider the forward (H) and backward (H−1) transformation, and sum the geometric
errors corresponding to each of these two transformations. Thus, the error is

4.2 Different cost functions

(cid:7)

d(xi, H−1x(cid:2)

i)2 + d(x(cid:2)

i, Hxi)2.

(4.7)

i

The ﬁrst term in this sum is the transfer error in the ﬁrst image, and the second term is
the transfer error in the second image. Again the estimated homography ˆH is the one
for which (4.7) is minimized.

4.2.3 Reprojection error – both images
An alternative method of quantifying error in each of the two images involves esti-
mating a “correction” for each correspondence. One asks how much it is necessary
to correct the measurements in each of the two images in order to obtain a perfectly
matched set of image points. One should compare this with the geometric one-image
transfer error (4.6) which measures the correction that it is necessary to make to the
measurements in one image (the second image) in order to get a set of perfectly match-
ing points.
In the present case, we are seeking a homography ˆH and pairs of perfectly matched
points ˆxi and ˆx(cid:2)

i that minimize the total error function

d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)
i)2

subject to ˆx(cid:2)

i = ˆHˆxi ∀i.

(4.8)

(cid:7)

i

i

Minimizing this cost function involves determining both ˆH and a set of subsidiary cor-
respondences {ˆxi} and {ˆx(cid:2)
}. This estimation models, for example, the situation that
wish to estimate a point on the world plane (cid:23)Xi from xi ↔ x(cid:2)
measured correspondences xi ↔ x(cid:2)
i arise from images of points on a world plane. We
i which is then reprojected
to the estimated perfectly matched correspondence ˆxi ↔ ˆx(cid:2)
i.
This reprojection error function is compared with the symmetric error function
in ﬁgure 4.2. It will be seen in section 4.3 that (4.8) is related to the Maximum Likeli-
hood estimation of the homography and correspondences.

4.2.4 Comparison of geometric and algebraic distance
We return to the case of errors only in the second image. Let x(cid:2)
deﬁne a vector (ˆx
becomes

(cid:2)
i)T and
i = H¯xi. Using this notation, the left hand side of (4.3)

i)T = ˆx(cid:2)
(cid:2)

(cid:2)
(cid:2)
i, ˆy
i, ˆw

i = (x

(cid:2)
i, w

(cid:2)
i, y

(cid:20)

Aih = i =

(cid:2)
i ˆw
y
(cid:2)
w
iˆx

(cid:2)
(cid:2)
i
i

− w
(cid:2)
(cid:2)
i ˆy
− x
(cid:2)
(cid:2)
i
i ˆw
i

(cid:21)

.

This vector is the algebraic error vector associated with the point correspondence xi ↔
x(cid:2)
i and the camera mapping H. Thus,
i)2 = (y

dalg(x(cid:2)

− w

− x

i, ˆx(cid:2)

(cid:2)
i)2.

(cid:2)
i ˆw

(cid:2)
iˆx

(cid:2)
i

For points x(cid:2)

i and ˆx(cid:2)
i, ˆx(cid:2)
d(x(cid:2)

(cid:2)
(cid:2)
i)2 + (w
i ˆy
i the geometric distance is

(cid:2)
i ˆw

(cid:2)
i

(cid:15)

i) =

(cid:2)
i/w

(cid:2)
i

(x

− ˆx

(cid:2)
i/ ˆw

(cid:2)
i)2 + (y

(cid:2)
i/w

(cid:2)
i

− ˆy
(cid:2)
i/ ˆw

(cid:2)
i)2

1/2

(cid:16)

96

4 Estimation – 2D Projective Transformations

x

d

x

x

d

image 1

image 1

H

H -1

H
H -1

d/

x /

x /
/d

/x

image 2

image 2

Fig. 4.2. A comparison between symmetric transfer error (upper) and reprojection error (lower) when
estimating a homography. The points x and x(cid:1)
are the measured (noisy) points. Under the estimated
homography the points x(cid:1)
and Hx do not correspond perfectly (and neither do the points x and H−1x(cid:1)
).
However, the estimated points, ˆx and ˆx(cid:1)
= Hˆx. Using
the notation d(x, y) for the Euclidean image distance between x and y, the symmetric transfer error is
d(x, H−1x(cid:1))2 + d(x(cid:1)

, do correspond perfectly by the homography ˆx(cid:1)

, Hx)2; the reprojection error is d(x, ˆx)2 + d(x(cid:1)

, ˆx(cid:1)

)2.

= dalg(x(cid:2)

i, ˆx(cid:2)

i)/ ˆw

(cid:2)
iw

(cid:2)
i.

(cid:2)
i = w

Thus, geometric distance is related to, but not quite the same as, algebraic distance.
Note, though, that if ˆw

(cid:2)
i = 1, then the two distances are identical.

One can always assume that wi = 1, thus expressing the points xi in the usual form
(cid:2)
xi = (xi, yi, 1)T. For one important class of 2D homographies, the values of ˆw
i will
always be 1 as well. A 2D afﬁne transformation is represented by a matrix of the
form (2.10–p39)

 h11 h12 h13

h21 h22 h23
1
0

0

 .

HA =

(4.9)

i = HA¯xi that ˆw

One veriﬁes immediately from ˆx(cid:2)
(cid:2)
i = 1 if wi = 1. This demonstrates
that in the case of an afﬁne transformation geometric distance and algebraic distance are
identical. The DLT algorithm is easily adapted to enforce the condition that the last row
of H has the form (0, 0, 1) by setting h7 = h8 = 0. Hence, for afﬁne transformations,
geometric distance can be minimized by the linear DLT algorithm based on algebraic
distance.

4.2.5 Geometric interpretation of reprojection error
The estimation of a homography between two planes can be thought of as ﬁtting a “sur-
face” to points in a 4D space, IR4. Each pair of image points x, x(cid:2)
deﬁnes a single point
denoted X in a measurement space IR4, formed by concatenating the inhomogeneous
coordinates of x and x(cid:2)
. For a given speciﬁc homography H, the image correspondences
x ↔ x(cid:2)
that satisfy x(cid:2) × (Hx) = 0 deﬁne an algebraic variety1 VH in IR4 which is the
1 A variety is the simultaneous zero-set of one or more multivariate polynomials deﬁned in IRN .

4.2 Different cost functions

97

(cid:2)

(cid:2)

, y

intersection of two quadric hypersurfaces. The surface is a quadric in IR4 because each
. The elements of H determine the
row of (4.1) is a degree 2 polynomial in x, y, x
coefﬁcient of each term of the polynomial, and so H speciﬁes the particular quadric.
The two independent equations of (4.1) deﬁne two such quadrics.
(cid:2)
i)T in IR4, the task of estimating a homography be-
Given points Xi = (xi, yi, x
comes the task of ﬁnding a variety VH that passes (or most nearly passes) through the
points Xi. In general, of course, it will not be possible to ﬁt a variety precisely. In this
Xi, let (cid:23)Xi = (ˆxi, ˆyi, ˆx
case, let VH be some variety corresponding to a transformation H, and for each point
i)T be the closest point to Xi lying on the variety VH. One sees
(cid:2)
(cid:2)
i, ˆy

(cid:2)
i, y

immediately that

(cid:10)Xi − (cid:23)Xi(cid:10)2 = (xi − ˆxi)2 + (yi − ˆyi)2 + (x

(cid:2)
i

− ˆx

(cid:2)
(cid:2)
i)2 + (y
i

− ˆy
(cid:2)
i)2

= d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)

i)2.

Thus geometric distance in IR4 is equivalent to the reprojection error measured in both

the images, and ﬁnding the variety VH and points (cid:23)Xi on VH that minimize the squared
sum of distances to the measured points Xi is equivalent to ﬁnding the homography ˆH
The point (cid:23)X on VH that lies closest to a measured point X is a point where the line
and the estimated points ˆxi and ˆx(cid:2)
i that minimize the reprojection error function (4.8).
between X and (cid:23)X is perpendicular to the tangent plane to VH at (cid:23)X. Thus
where d⊥(X,VH) is the perpendicular distance of the point X to the variety VH. As may
be seen from the conic-ﬁtting analogue discussed below, there may be more than one
such perpendicular from X to VH.
The distance d⊥(X,VH) is invariant to rigid transformations of IR4, and this includes
as a special case rigid transformations of the coordinates (x, y), (x
) of each image
individually. This point is returned to in section 4.4.3.

i)2 = d⊥(Xi,VH)2

d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)

, y

(cid:2)

(cid:2)

Conic analogue. Before proceeding further we will ﬁrst sketch an analogous estima-
tion problem that can be visualized more easily. The problem is ﬁtting a conic to 2D
points, which occupies a useful intermediate position between ﬁtting a straight line
(no curvature, too simple) and ﬁtting a homography (four dimensions, with non-zero
curvature).

Consider the problem of ﬁtting a conic to a set of n > 5 points (xi, yi)T on the
plane such that an error based on geometric distance is minimized. The points may
be thought of as “correspondences” xi ↔ yi. The transfer distance and reprojection
(perpendicular) distance are illustrated in ﬁgure 4.3. It is clear from this ﬁgure that d⊥
is less than or equal to the transfer error.

The algebraic distance of a point x from a conic C is deﬁned as dalg(x, C)2 = xTCx.
i dalg(xi, C)2 with a suitable
A linear solution for C can be obtained by minimizing
normalization on C. There is no linear expression for the perpendicular distance of
a point (x, y) to a conic C, since through each point in IR2 there are up to 4 lines
perpendicular to C. The solution can be obtained from the roots of a quartic. However,
a function d⊥(x, C) may be deﬁned which returns the shortest distance between a conic

(cid:27)

98

4 Estimation – 2D Projective Transformations

y

d

x

d

a

a
d

y

C

b
d

dy

b

x

Fig. 4.3. A conic may be estimated from a set of 2D points by minimizing “symmetric transfer error”
d2
y or the sum of squared perpendicular distances d2⊥. The analogue of transfer error is to consider
x + d2
x as perfect and measure the distance dy to the conic in the y direction, and similarly for dx. For point
a it is clear that d⊥ ≤ dx and d⊥ ≤ dy. Also d⊥ is more stable than dx or dy as illustrated by point b
where dx cannot be deﬁned.

i d⊥(xi, C)2 over the ﬁve
and a point. A conic can then be estimated by minimizing
parameters of C, though this cannot be achieved by a linear solution. Given a conic C
and a measured point x, a corrected point ˆx is obtained simply by choosing the closest
point on C.

(cid:27)

We return now to estimating a homography. In the case of an afﬁne transformation
the variety is the intersection of two hyperplanes, i.e. it is a linear subspace of dimen-
sion 2. This follows from the form (4.9) of the afﬁne matrix which for x(cid:2)
= HAx yields
one linear constraint between x, x
, each of which de-
ﬁnes a hyperplane in IR4. An analogue of this situation is line ﬁtting to points on the
plane. In both cases the relation (afﬁne transformation or line) may be estimated by
minimizing the perpendicular distance of points to the variety. In both cases there is a
closed form solution as discussed in the following section.

, y and another between x, y, y

(cid:2)

(cid:2)

4.2.6 Sampson error
The geometric error (4.8) is quite complex in nature, and minimizing it requires the
simultaneous estimation of both the homography matrix and the points ˆxi, ˆx(cid:2)
i. This
non-linear estimation problem will be discussed further in section 4.5. Its complexity
contrasts with the simplicity of minimizing the algebraic error (4.4). The geometric
interpretation of geometric error given in section 4.2.5 leads to a further cost function
that lies between the algebraic and geometric cost functions in terms of complexity, but
gives a close approximation to geometric error. We will refer to this cost function as
Sampson error since Sampson [Sampson-82] used this approximation for conic ﬁtting.

As described in section 4.2.5, the vector (cid:23)X that minimizes the geometric error (cid:10)X −
(cid:23)X(cid:10)2 is the closest point on the variety VH to the measurement X. This point can not be
estimated directly except via iteration, because of the non-linear nature of the variety
to the point (cid:23)X, assuming that the cost function is well approximated linearly in the
VH. The idea of the Sampson error function is to estimate a ﬁrst-order approximation

neighbourhood of the estimated point. The discussion to follow is related directly to

4.2 Different cost functions

99

the 2D homography estimation problem, but applies substantially unchanged to the
other estimation problems discussed in this book.
)T that lies on VH will satisfy
(cid:2)
For a given homography H, any point X = (x, y, x
the equation (4.3–p89), or Ah = 0. To emphasize the dependency on X we will write
this instead as CH(X) = 0, where CH(X) is in this case a 2-vector. To ﬁrst order, this
cost function may be approximated by a Taylor expansion

, y

(cid:2)

CH(X + δX) = CH(X) +

δX.

(4.10)

If we write δX = (cid:23)X − X and desire (cid:23)X to lie on the variety VH so that CH((cid:23)X) =0 , then
the result is CH(X) + (∂CH/∂X)δX = 0, which we will henceforth write as JδX = −
where J is the partial-derivative matrix, and  is the cost CH(X) associated with X. The
minimization problem that we now face is to ﬁnd the smallest δX that satisﬁes this
equation, namely:
• Find the vector δX that minimizes (cid:10)δX(cid:10) subject to JδX = −.
The standard way to solve problems of this type is to use Lagrange multipliers. A
vector λ of Lagrange multipliers is introduced, and the problem reduces to that of
XδX − 2λT(JδX + ), where the factor 2 is simply introduced
ﬁnding the extrema of δT
for convenience. Taking derivatives with respect to δX and equating to zero gives

∂CH
∂X

X − 2λTJ = 0T

2δT

from which we obtain δX = JTλ. The derivative with respect to λ gives JδX +  = 0,
the original constraint. Substituting for δX leads to
JJTλ = −
which may be solved for λ giving λ = −(JJT)
−1, and so ﬁnally
δX = −JT(JJT)
−1,
and (cid:23)X = X + δX. The norm (cid:10)δX(cid:10)2 is the Sampson error:
−1.

(cid:10)δX(cid:10)2 = δT

XδX = T(JJT)

(4.12)

(4.11)

Example 4.2. Sampson approximation for a conic
We will compute the Sampson approximation to the geometric distance d⊥(x, C) be-
tween a point x and conic C shown in ﬁgure 4.3. In this case the conic variety VC is
deﬁned by the equation xTCx = 0, so that X = (x, y)T is a 2-vector,  = xTCx is a
scalar, and J is the 1 × 2 matrix given by

(cid:17)

(cid:18)

J =

∂(xTCx)

∂x

,

∂(xTCx)

∂y

.

This means that JJT is a scalar. The elements of J may be computed by the chain rule
as

∂(xTCx)

∂x

=

∂(xTCx)

∂x

∂x
∂x

= 2xTC(1, 0, 0)T = 2(Cx)1

100
where (Cx)i denotes the i-th component of the 3-vector Cx. Then from (4.12)

4 Estimation – 2D Projective Transformations

d2⊥ = (cid:10)δX(cid:10)2 = T(JJT)

−1 =

T
JJT =

(xTCx)2

4((Cx)2

1 + (Cx)2
2)

(cid:2)

A few points to note:

measurements are x = (x, y, 1)T and x(cid:2)

)T where the 2D
(i) For the 2D homography estimation problem, X = (x, y, x
(ii)  = CH(X) is the algebraic error vector Aih – a 2-vector – and Ai is deﬁned in
(iii) J = ∂CH(X)/∂X is a 2 × 4 matrix. For example

(4.3–p89).

, 1)T.

= (x

, y

, y

(cid:2)

(cid:2)

(cid:2)

(cid:2)

J11 = ∂(−w

(cid:2)
i

(cid:2)
h2 + y
i

xT
i

xT
i

h3)/∂x = −w

(cid:2)
ih21 + y

(cid:2)
ih31.

(iv) Note the similarity of (4.12) to the algebraic error (cid:10)(cid:10) = T. The Sampson
error may be interpreted as being the Mahalanobis norm (see section A2.1-
(p565)), (cid:10)(cid:10)JJT.
(v) One could alternatively use A deﬁned by (4.1–p89), in which case J has di-
mension 3 × 4 and  is a 3-vector. However, in general the Sampson error,
and consequently the solution δX, will be independent of whether (4.1–p89) or
(4.3–p89) is used.

The Sampson error (4.12) is derived here for a single point pair. In applying this to
the estimation of a 2D homography H from several point correspondences xi ↔ x(cid:2)
i, the
errors corresponding to all the point correspondences must be summed, giving

D⊥ =

i (JiJT
T
i )

−1i

(4.13)

(cid:7)

i

where  and J both depend on H. To estimate H, this expression must be minimized
over all values of H. This is a simple minimization problem in which the set of variable
parameters consists only of the entries (or some other parametrization) of H.

This derivation of the Sampson error assumed that each point had isotropic (circular)
error distribution, the same in each image. The appropriate formulae for more general
Gaussian error distributions are given in the exercises at the end of this chapter.

Linear cost function
The algebraic error vector CH(X) = A(X)h is typically multilinear in the entries of X.
The case where A(X)h is linear is, however, important in its own right. The ﬁrst point
to note is that in this case, the ﬁrst-order approximation to geometric error given by the
Taylor expansion in (4.10) is exact (the higher order terms are zero), which means that
the Sampson error is identical to geometric error.
In addition, the variety VH deﬁned by the equation CH(X) =0 , a set of linear equa-
tions, is a hyperplane depending on H. The problem of ﬁnding H now becomes a
hyperplane ﬁtting problem – ﬁnd the best ﬁt to the data Xi among the hyperplanes
parametrized by H.

4.2 Different cost functions

101

As an example of this idea a linear algorithm which minimizes geometric error (4.8)

for an afﬁne transformation is developed in the exercises at the end of this chapter.

4.2.7 Another geometric interpretation
It was shown in section 4.2.5 that ﬁnding a homography that takes a set of points xi
to another set x(cid:2)
i is equivalent to the problem of ﬁtting a variety of a given type to a
set of points in IR4. We now consider a different interpretation in which the set of all
measurements is represented by a single point in a measurement space IRN .

The estimation problems we consider may all be ﬁtted into a common framework.

In abstract terms the estimation problem has two components,
• a measurement space IRN consisting of measurement vectors X, and
• a model, which in abstract terms may be thought of simply as a subset S of points in
IRN . A measurement vector X that lies inside this subset is said to satisfy the model.
Typically the subspace that satisﬁes the model is a submanifold, or variety in IRN.

Now, given a measurement vector X in IRN , the estimation problem is to ﬁnd the vector

(cid:23)X, closest to X, that satisﬁes the model.

It will now be pointed out how the 2D homography estimation problem ﬁts into this

i

i

framework.
Error in both images. Let {xi ↔ x(cid:2)
} be a set of measured matched points for
i = 1, . . . , n. In all, there are 4n measurements, namely two coordinates in each of
two images for n points. Thus, the set of matched points represents a point in IRN,
where N = 4n. The vector made up of the coordinates of all the matched points in
both images will be denoted X.
Of course, not all sets of point pairs xi ↔ x(cid:2)
i are related via a homography H. A set
of point correspondences {xi ↔ x(cid:2)
} for which there exists a projective transformation
H satisfying x(cid:2)
i = Hxi for all i constitutes the subset of IRN satisfying the model. In
general, this set of points will form a submanifold S in IRN (in fact a variety) of some
dimension. The dimension of this submanifold is equal to the minimal number of
parameters that may be used to parametrize the submanifold.
One may arbitrarily choose n points ˆxi in the ﬁrst image. In addition, a homography
H may be chosen arbitrarily. Once these choices have been made, the points ˆx(cid:2)
i in
the second image are determined by ˆx(cid:2)
i = Hˆxi. Thus, a feasible choice of points is
determined by a set of 2n + 8 parameters: the 2n coordinates of the points ˆxi, plus
the 8 independent parameters (degrees of freedom) of the transformation H. Thus, the
submanifold S ⊂ IRN has dimension 2n + 8, and hence codimension 2n − 8.
and an estimated point (cid:23)X ∈ IRN lying on S, one easily veriﬁes that
Given a set of measured point pairs {xi ↔ x(cid:2)
}, corresponding to a point X in IRN,
Thus, ﬁnding the point (cid:23)X on S lying closest to X in IRN is equivalent to minimizing
the cost function given by (4.8). The estimated correct correspondences ˆxi ↔ ˆx(cid:2)

(cid:10)X − (cid:23)X(cid:10)2 =

(cid:7)

d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)

i)2.

i

i

i are

102

those corresponding to the closest surface point (cid:23)X in IRN . Once (cid:23)X is known H may be

4 Estimation – 2D Projective Transformations

computed.

i

Error in one image only.
In the case of error in one image, one has a set of correspon-
dences {¯xi ↔ x(cid:2)
}. The points ¯xi are assumed perfect. The inhomogeneous coordinates
space has dimension N = 2n. The vector (cid:23)X consists of the inhomogeneous coor-
of the x(cid:2)
i constitute the measurement vector X. Hence, in this case the measurement
vectors satisfying the model is the set (cid:23)X as H varies over the set of all homography
dinates of the mapped perfect points {H¯x1, H¯x2, . . . ,H ¯xn}. The set of measurement
matrices. Once again this subspace is a variety. Its dimension is 8, since this is the total
number of degrees of freedom of the homography matrix H. As with the previous case,
(cid:7)
the codimension is 2n − 8. One veriﬁes that
(cid:10)X − (cid:23)X(cid:10)2 =

d(x(cid:2)

i, H¯xi)2.

Thus, ﬁnding the closest point on S to the measurement vector X is equivalent to min-
imizing the cost function (4.6).

i

4.3 Statistical cost functions and Maximum Likelihood estimation

In section 4.2, various cost functions were considered that were related to geometric
distance between estimated and measured points in an image. The use of such cost
functions is now justiﬁed and then generalized by a consideration of error statistics of
the point measurements in an image.

In order to obtain a best (optimal) estimate of H it is necessary to have a model for
the measurement error (the “noise”). We are assuming here that in the absence of mea-
surement error the true points exactly satisfy a homography, i.e. ¯x(cid:2)
i = H¯xi. A common
assumption is that image coordinate measurement errors obey a Gaussian (or normal)
probability distribution. This assumption is surely not justiﬁed in general, and takes no
account of the presence of outliers (grossly erroneous measurements) in the measured
data. Methods for detecting and removing outliers will be discussed later in section 4.7.
Once outliers have been removed, the assumption of a Gaussian error model, if still not
strictly justiﬁed, becomes more tenable. Therefore, for the present, we assume that
image measurement errors obey a zero-mean isotropic Gaussian distribution. This dis-
tribution is described in section A2.1(p565).

Speciﬁcally we assume that the noise is Gaussian on each image coordinate with
zero mean and uniform standard deviation σ. This means that x = ¯x + ∆x, with ∆x
obeying a Gaussian distribution with variance σ2. If it is further assumed that the noise
on each measurement is independent, then, if the true point is ¯x, the probability density
function (PDF) of each measured point x is

(cid:28)

(cid:29)

Pr(x) =

1

2πσ2

−d(x,¯x)2/(2σ2).

e

(4.14)

Error in one image.
second image. The probability of obtaining the set of correspondences {¯xi ↔ x(cid:2)

First we consider the case where the errors are only in the
} is

i

log Pr({x(cid:2)

i

d(x(cid:2)

i, H¯xi)2 + constant.

(cid:7)

i

}|H) =− 1
2σ2
(cid:7)

d(x(cid:2)

i, H¯xi)2.

4.3 Statistical cost functions and Maximum Likelihood estimation

103

simply the product of their individual PDFs, since the errors on each point are assumed
independent. Then the PDF of the noise-perturbed data is

(cid:28)

(cid:30)

(cid:29)

i

1

}|H) =

Pr({x(cid:2)
(4.15)
}|H) is to be interpreted as meaning the probability of obtaining the
} given that the true homography is H. The log-likelihood of the set

i,H¯xi)2/(2σ2) .

−d(x(cid:1)

2πσ2

e

i

The symbol Pr({x(cid:2)
measurements {x(cid:2)
of correspondences is

i

i

The Maximum Likelihood estimate (MLE) of the homography, ˆH, maximizes this log-
likelihood, i.e. minimizes

Thus, we note that ML estimation is equivalent to minimizing the geometric error func-
tion (4.6).

i

Error in both images.
(cid:28)
correspondences are {¯xi ↔ H¯xi = ¯x(cid:2)
}, then the PDF of the noise-perturbed data is
(cid:30)
}|H,{¯xi}) =

Following a similar development to the above, if the true

Pr({xi, x(cid:2)

−(d(xi,¯xi)2+d(x(cid:1)

i,H¯xi)2)/(2σ2).

(cid:29)

1

e

i

i

2πσ2

i

The additional complication here is that we have to seek “corrected” image measure-
ments that play the role of the true measurements (H¯x above). Thus the ML estimate of
the projective transformation H and the correspondences {xi ↔ x(cid:2)
}, is the homography
ˆH and corrected correspondences {ˆxi ↔ ˆx(cid:2)

} that minimize

i

i

(cid:7)

i

d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)
i)2

with ˆx(cid:2)
reprojection error function (4.8).

i = ˆHˆxi. Note that in this case, the ML estimate is identical with minimizing the

Mahalanobis distance.
In the general Gaussian case, one may assume a vector of
measurements X satisfying a Gaussian distribution function with covariance matrix
Σ. The cases above are equivalent to a covariance matrix which is a multiple of the
identity.

Maximizing the log-likelihood is then equivalent to minimizing the Mahalanobis

distance (see section A2.1(p565))
(cid:10)X − ¯X(cid:10)2

Σ = (X − ¯X)TΣ−1(X − ¯X).

In the case where there is error in each image, but assuming that errors in one image
are independent of the error in the other image, the appropriate cost function is

(cid:10)X − ¯X(cid:10)2

Σ + (cid:10)X(cid:2) − ¯X(cid:2)(cid:10)2
Σ(cid:1)

4 Estimation – 2D Projective Transformations

are the covariance matrices of the measurements in the two images.

Finally, if we assume that the errors for all the points xi and x(cid:2)

i are independent,
i respectively, then the above expression

104
where Σ and Σ(cid:2)
with individual covariance matrices Σi and Σ(cid:2)
expands to

(cid:7)(cid:10)xi − ¯xi(cid:10)2

Σi +

(cid:7)(cid:10)x(cid:2)

− ¯x(cid:2)

i

(cid:10)2
Σ(cid:1)

i

(4.16)

i

This equation allows the incorporation of the type of anisotropic covariance matrices
that arise for point locations computed as the intersection of two non-perpendicular
lines. In the case where the points are known exactly in one of the two images, errors
being conﬁned to the other image, one of the two summation terms in (4.16) disappears.

4.4 Transformation invariance and normalization

We now start to discuss the properties and performance of the DLT algorithm of
section 4.1 and how it compares with algorithms minimizing geometric error. The
ﬁrst topic is the invariance of the algorithm to different choices of coordinates in the
image. It is clear that it would generally be undesirable for the result of an algorithm
to be dependent on such arbitrary choices as the origin and scale, or even orientation,
of the coordinate system in an image.

4.4.1 Invariance to image coordinate transformations
Image coordinates are sometimes given with the origin at the top-left of the image,
and sometimes with the origin at the centre. The question immediately occurs whether
this makes a difference to the results of computing the transformation. Similarly, if
the units used to express image coordinates are changed by multiplication by some
factor, then is it possible that the result of the algorithm changes also? More generally,
to what extent is the result of an algorithm that minimizes a cost function to estimate
a homography dependent on the choice of coordinates in the image? Suppose, for
instance, that the image coordinates are changed by some similarity, afﬁne or even
projective transformation before running the algorithm. Will this materially change the
result?
Formally, suppose that coordinates x in one image are replaced by ˜x = Tx, and
are 3 × 3
coordinates x(cid:2)
T(cid:2)HT−1˜x. This relation implies that (cid:22)H = T(cid:2)HT−1 is the transformation matrix for the
= Hx, we derive the equation ˜x(cid:2)
homographies. Substituting in the equation x(cid:2)
=
point correspondences ˜x ↔ ˜x(cid:2)
taking xi to x(cid:2)
(ii) Find the transformation(cid:22)H from the correspondences ˜xi ↔ ˜x(cid:2)
(iii) Set H = T(cid:2)−1(cid:22)HT.
The transformation matrix H found in this way applies to the original untransformed
point correspondences xi ↔ x(cid:2)
i. What choice should be made for the transformations T
and T(cid:2)
will be left unspeciﬁed for now. The question to be decided now is whether the

(i) Transform the image coordinates according to transformations ˜xi = Txi and

in the other image are replaced by ˜x(cid:2)

. An alternative method of ﬁnding the transformation

i is therefore suggested, as follows.

= T(cid:2)x(cid:2)

, where T and T(cid:2)

˜x(cid:2)
i = T(cid:2)x(cid:2)
i.

i.

4.4 Transformation invariance and normalization

105

outcome of this algorithm is independent of the transformations T and T(cid:2)
being applied.
Ideally it ought to be, at least when T and T(cid:2)
are similarity transformations, since the
choice of a different scale, orientation or coordinate origin in the images should not
materially affect the outcome of the algorithm.

In the subsequent sections it will be shown that an algorithm that minimizes geo-
metric error is invariant to similarity transformations. On the other hand, for the DLT
algorithm as described in section 4.1, the result unfortunately is not invariant to simi-
larity transformations. The solution is to apply a normalizing transformation to the data
before applying the DLT algorithm. This normalizing transformation will nullify the
effect of the arbitrary selection of origin and scale in the coordinate frame of the image,
and will mean that the combined algorithm is invariant to a similarity transformation
of the image. Appropriate normalizing transformations will be discussed later.

i = T(cid:2)x(cid:2)

i yield the trans-

i where ˜xi = Txi and ˜x(cid:2)

4.4.2 Non-invariance of the DLT algorithm
Consider a set of correspondences xi ↔ x(cid:2)
i and a matrix H that is the result of the DLT
i, and let (cid:22)H be deﬁned by
(cid:22)H = T(cid:2)HT−1. Following section 4.4.1, the question to be decided here is the following:
algorithm applied to this set of corresponding points. Consider further a related set
of correspondences ˜xi ↔ ˜x(cid:2)
formation(cid:22)H?
• Does the DLT algorithm applied to the correspondence set ˜xi ↔ ˜x(cid:2)
We will use the following notation: Matrix Ai is the DLT equation matrix (4.3–p89)
derived from a point correspondence xi ↔ x(cid:2)
i, and A is the 2n × 9 matrix formed by
stacking the Ai. Matrix ˜Ai is similarly deﬁned in terms of the correspondences ˜xi ↔ ˜x(cid:2)
i,
where ˜xi = Txi and ˜x(cid:2)
Result 4.3. Let T(cid:2)
be a similarity transformation with scale factor s, and let T be an
arbitrary projective transformation. Further, suppose H is any 2D homography and let
˜H be deﬁned by ˜H = T(cid:2)HT−1. Then (cid:10)˜A˜h(cid:10) = s(cid:10)Ah(cid:10) where h and ˜h are the vectors of
entries of H and ˜H.
× Hxi. Note that Aih is the vector consisting of the
Proof. Deﬁne the vector i = x(cid:2)
ﬁrst two entries of i. Let ˜i be similarly deﬁned in terms of the transformed quantities
as ˜i = ˜x(cid:2)
×(cid:22)H˜xi = T(cid:2)x(cid:2)
× T(cid:2)Hxi = T(cid:2)∗

i for some projective transformations T and T(cid:2)

×(cid:22)H˜xi. One computes:
˜i = ˜x(cid:2)
= T(cid:2)x(cid:2)
= T(cid:2)∗i

× (T(cid:2)HT−1)Txi
× Hxi)
(x(cid:2)

i = T(cid:2)x(cid:2)

.

i

i

i

i

i

i

represents the cofactor matrix of T(cid:2)

where T(cid:2)∗
and the second-last equality follows
from lemma A4.2(p581). For a general transformation T, the error vectors Aih and
˜h (namely the ﬁrst two components of i and ˜i) are not simply related. However, in
˜Ai
the special case where T(cid:2)
where R is a rotation matrix, t is a translation and s is a scaling factor. In this case, we

is a similarity transformation, one may write T(cid:2)

sR t
0T 1

(cid:17)

(cid:18)

=

(cid:17)

= s

106
see that T(cid:2)∗
sees that

(cid:18)

R

0
−tTR s

4 Estimation – 2D Projective Transformations

. Applying T(cid:2)∗

just to the ﬁrst two components of i, one

˜Ai

˜h = (˜i1, ˜i2)T = sR(i1, i2)T = sRAih.

dalg(˜x(cid:2)

error, except for constant scale.

Since rotation does not affect vector norms, one sees that (cid:10)˜A˜h(cid:10) = s(cid:10)Ah(cid:10), as required.
This result may be expressed in terms of algebraic error as
i, Hxi).

i,(cid:22)H˜xi) = sdalg(x(cid:2)
Thus, there is a one-to-one correspondence between H and(cid:22)H giving rise to the same
It may appear therefore that the matrices H and (cid:22)H
minimizing the algebraic error will be related by the formula (cid:22)H = T(cid:2)HT−1, and hence
one may retrieve H as the product T(cid:2)−1(cid:22)HT. This conclusion is false however. For,
although H and (cid:22)H so deﬁned give rise to the same error , the condition (cid:10)H(cid:10) = 1,
imposed as a constraint on the solution, is not equivalent to the condition (cid:10)(cid:22)H(cid:10) = 1.
Speciﬁcally, (cid:10)H(cid:10) and (cid:10)(cid:22)H(cid:10) are not related in any simple manner. Thus, there is no one-
to-one correspondence between H and (cid:22)H giving rise to the same error , subject to the
constraint (cid:10)H(cid:10) = (cid:10)(cid:22)H(cid:10) = 1. Speciﬁcally,
(cid:7)
(cid:7)
dalg(x(cid:2)
(cid:7)
dalg(˜x(cid:2)
dalg(˜x(cid:2)

i, Hxi)2 subject to (cid:10)H(cid:10) = 1
i,(cid:22)H˜xi)2 subject to (cid:10)H(cid:10) = 1
i,(cid:22)H˜xi)2 subject to (cid:10)(cid:22)H(cid:10) = 1.

minimize
⇔ minimize
(cid:5)⇔ minimize

i

i

i

Thus, the method of transformation leads to a different solution for the computed
transformation matrix. This is a rather undesirable feature of the DLT algorithm as it
stands, that the result is changed by a change of coordinates, or even simply a change of
the origin of coordinates. If the constraint under which the norm (cid:10)Ah(cid:10) is minimized is
invariant under the transformation, however, then one sees that the computed matrices
H and ˜H are related in the right way. Examples of minimization conditions for which H
is transformation-invariant are discussed in the exercises at the end of this chapter.

4.4.3 Invariance of geometric error
It will be shown now that minimizing geometric error to ﬁnd H is invariant under sim-
ilarity (scaled Euclidean) transformations. As before, consider a point correspondence
x ↔ x(cid:2)
and a transformation matrix H. Also, deﬁne a related set of correspondences
˜x ↔ ˜x(cid:2)
where ˜x = Tx and ˜x(cid:2)
T and T(cid:2)
,(cid:22)H˜x) = d(T(cid:2)x(cid:2)
represent Euclidean transformations of IP2. One veriﬁes that
, T(cid:2)Hx) = d(x(cid:2)

, and let(cid:22)H be deﬁned by(cid:22)H = T(cid:2)HT−1. Suppose that

, T(cid:2)HT−1Tx) = d(T(cid:2)x(cid:2)

= T(cid:2)x(cid:2)

d(˜x(cid:2)

, Hx)

where the last equality holds because Euclidean distance is unchanged under a Eu-
clidean transformation such as T(cid:2)
. This shows that if H minimizes the geometric error

for a set of correspondences, then(cid:22)H minimizes the geometric error for the transformed

4.4 Transformation invariance and normalization

107

set of correspondences, and so minimizing geometric error is invariant under Euclidean
transformations.

For similarity transformations, geometric error is multiplied by the scale factor of
the transformation, hence the minimizing transformations correspond in the same way
as in the Euclidean transformation case. Minimizing geometric error is invariant to
similarity transformations.

4.4.4 Normalizing transformations
As was shown in section 4.4.2, the result of the DLT algorithm for computing 2D
homographies depends on the coordinate frame in which points are expressed. In fact
the result is not invariant to similarity transformations of the image. This suggests
the question whether some coordinate systems are in some way better than others for
computing a 2D homography. The answer to this is an emphatic yes. In this section a
method of normalization of the data is described, consisting of translation and scaling
of image coordinates. This normalization should be carried out before applying the
DLT algorithm. Subsequently an appropriate correction to the result expresses the
computed H with respect to the original coordinate system.

Apart from improved accuracy of results, data normalization provides a second de-
sirable beneﬁt, namely that an algorithm that incorporates an initial data normalization
step will be invariant with respect to arbitrary choices of the scale and coordinate ori-
gin. This is because the normalization step undoes the effect of coordinate changes,
by effectively choosing a canonical coordinate frame for the measurement data. Thus,
algebraic minimization is carried out in a ﬁxed canonical frame, and the DLT algorithm
is in practice invariant to similarity transformations.

Isotropic scaling. As a ﬁrst step of normalization, the coordinates in each image are
translated (by a different translation for each image) so as to bring the centroid of the
set of all points to the origin. The coordinates are also scaled so that on the average a
point x is of the form x = (x, y, w)T, with each of x, y and w having the same average
magnitude. Rather than choose different scale factors for each coordinate direction, an
isotropic scaling factor is chosen so that the x and y-coordinates of a point are scaled
√
equally. To this end, we choose to scale the coordinates so that the average distance of
a point x from the origin is equal to
2. This means that the “average” point is equal
to (1, 1, 1)T. In summary the transformation is as follows:

(i) The points are translated so that their centroid is at the origin.
(ii) The points are then scaled so that the average distance from the origin is equal

√
2.

to

(iii) This transformation is applied to each of the two images independently.

Why is normalization essential? The recommended version of the DLT algorithm
with data normalization is given in algorithm 4.2. We will now motivate why this

108

4 Estimation – 2D Projective Transformations

version of the algorithm, incorporating data normalization, should be used in prefer-
ence to the basic DLT of algorithm 4.1(p91). Note that normalization is also called
pre-conditioning in the numerical literature.

The DLT method of algorithm 4.1 uses the SVD of A = UDVT to obtain a solution
to the overdetermined set of equations Ah = 0. These equations do not have an exact
solution (since the 2n × 9 matrix A will not have rank 8 for noisy data), but the vector
h, given by the last column of V, provides a solution which minimizes (cid:10)Ah(cid:10) (subject
to (cid:10)h(cid:10) = 1). This is equivalent to ﬁnding the rank 8 matrix ˆA which is closest to A in
Frobenius norm and obtaining h as the exact solution of ˆAh = 0. The matrix ˆA is given
by ˆA = UˆDVT where ˆD is D with the smallest singular value set to zero. The matrix ˆA has
rank 8 and minimizes the difference to A in Frobenius norm because

(cid:10)A − ˆA(cid:10)F = (cid:10)UDVT − UˆDVT(cid:10)F = (cid:10)D − ˆD(cid:10)F.

where (cid:10).(cid:10)F is the Frobenius norm, i.e. the square root of the sum of squares of all
entries.
Without normalization typical image points xi, x(cid:2)

i are of the order (x, y, w)T =
(100, 100, 1)T, i.e., x, y are much larger than w. In A the entries xx
will
, xy
(cid:2)
be of order 104, entries xw
etc. of order 102, and entries ww
will be unity. Re-
placing A by ˆA means that some entries are increased and others decreased such that
the square sum of differences of these changes is minimal (and the resulting matrix has
rank 8). However, and this is the key point, increasing the term ww
by 100 means a
huge change in the image points, whereas increasing the term xx
by 100 means only a
slight change. This is the reason why all entries in A must have similar magnitude and
why normalization is essential.

, yw

(cid:2)

, yx

, yy

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

The effect of normalization is related to the condition number of the set of DLT equa-
tions, or more precisely the ratio d1/dn−1 of the ﬁrst to the second-last singular value
of the equation matrix A. This point is investigated in more detail in [Hartley-97c]. For
the present it is sufﬁcient to say that for exact data and inﬁnite precision arithmetic the
results will be independent of the normalizing transformation. However, in the pres-
ence of noise the solution will diverge from the correct result. The effect of a large
condition number is to amplify this divergence. This is true even for inﬁnite-precision
arithmetic – this is not a round-off error effect.

The effect that this data normalization has on the results of the DLT algorithm is
shown graphically in ﬁgure 4.4. The conclusion to be drawn here is that data normal-
ization gives dramatically better results. The examples shown in the ﬁgure are chosen
to make the effect easily visible. However, a marked advantage remains even in cases
of computation from larger numbers of point correspondences, with points more widely
distributed. To emphasize this point we remark:
• Data normalization is an essential step in the DLT algorithm. It must not be consid-
ered optional.

Data normalization becomes even more important for less well conditioned problems,
such as the DLT computation of the fundamental matrix or the trifocal tensor, which
will be considered in later chapters.

4.4 Transformation invariance and normalization

109

Objective
Given n ≥ 4 2D to 2D point correspondences {xi ↔ x(cid:1)
matrix H such that x(cid:1)
Algorithm

i = Hxi.

i}, determine the 2D homography

(i) Normalization of x: Compute a similarity transformation T, consisting of a translation
and scaling, that takes points xi to a new set of points ˜xi such that the centroid of the
√
points ˜xi is the coordinate origin (0, 0)T, and their average distance from the origin is
2.
image, transforming points x(cid:1)

(ii) Normalization of x(cid:1)
(iii) DLT: Apply algorithm 4.1(p91) to the correspondences ˜xi ↔ ˜x(cid:1)

: Compute a similar transformation T(cid:1)

for the points in the second

phy(cid:22)H.
(iv) Denormalization: Set H = T(cid:1)−1(cid:22)HT.

i to ˜x(cid:1)
i.

i to obtain a homogra-

Algorithm 4.2. The normalized DLT for 2D homographies.

a

b

Fig. 4.4. Results of Monte Carlo simulation (see section 5.3(p149) of computation of 2D homographies).
A set of 5 points (denoted by large crosses) was used to compute a 2D homography. Each of the 5 points
is mapped (in the noise-free case) to the point with the same coordinates, so that homography H is the
identity mapping. Now, 100 trials were made with each point being subject to 0.1 pixel Gaussian noise
in one image. (For reference, the large crosses are 4 pixels across.) The mapping H computed using the
DLT algorithm was then applied to transfer a further point into the second image. The 100 projections
of this point are shown with small crosses and the 95% ellipse computed from their scatter matrix is also
shown. (a) are the results without data normalization, and (b) the results with normalization. The left-
and rightmost reference points have (unnormalized) coordinates (130, 108) and (170, 108).

Non-isotropic scaling. Other methods of scaling are also possible. In non-isotropic
scaling, the centroid of the points is translated to the origin as before. After this trans-
lation the points form a cloud about the origin. Scaling is then carried out so that
the two principal moments of the set of points are both equal to unity. Thus, the set
of points will form an approximately symmetric circular cloud of points of radius 1
about the origin. Experimental results given in [Hartley-97c] suggest that the extra ef-
fort required for non-isotropic scaling does not lead to signiﬁcantly better results than
isotropic scaling.

A further variant on scaling was discussed in [Muehlich-98], based on a statistical
analysis of the estimator, its bias and variance. In that paper it was observed that some
columns of A are not affected by noise. This applies to the third and sixth columns in
(cid:2)
i = 1. Such error-free entries in A should not
(4.3–p89), corresponding to the entry wiw
be varied in ﬁnding ˆA, the closest rank-deﬁcient approximation to A. A method known

110

4 Estimation – 2D Projective Transformations

as Total Least Squares - Fixed Columns is used to ﬁnd the best solution. For estimation
of the fundamental matrix (see chapter 11), [Muehlich-98] reports slightly improved
results compared with non-isotropic scaling.

Scaling with points near inﬁnity. Consider the case of estimation of a homography
between an inﬁnite plane and an image. If the viewing direction is sufﬁciently oblique,
then very distant points in the plane may be visible in the image – even points at inﬁnity
(vanishing points) if the horizon is visible. In this case it makes no sense to normal-
ize the coordinates of points in the inﬁnite plane by setting the centroid at the origin,
since the centroid may have very large coordinates, or be undeﬁned. An approach to
normalization in this case is considered in exercise (iii) on page 128.

4.5 Iterative minimization methods

This section describes methods for minimizing the various geometric cost functions
developed in section 4.2 and section 4.3. Minimizing such cost functions requires
the use of iterative techniques. This is unfortunate, because iterative techniques tend
to have certain disadvantages compared to linear algorithms such as the normalized
DLT algorithm 4.2:

(i) They are slower.
(ii) They generally need an initial estimate at which to start the iteration.
(iii) They risk not converging, or converging to a local minimum instead of the

global minimum.

(iv) Selection of a stopping criterion for iteration may be tricky.

Consequently, iterative techniques generally require more careful implementation.

The technique of iterative minimization generally consists of ﬁve steps:

(i) Cost function. A cost function is chosen as the basis for minimization. Dif-

ferent possible cost functions were discussed in section 4.2.

(ii) Parametrization. The transformation (or other entity) to be computed is ex-
pressed in terms of a ﬁnite number of parameters. It is not in general necessary
that this be a minimum set of parameters, and there are in fact often advantages
to over-parametrization. (See the discussion below.)

(iii) Function speciﬁcation. A function must be speciﬁed that expresses the cost

in terms of the set of parameters.

(iv) Initialization. A suitable initial parameter estimate is computed. This will

generally be done using a linear algorithm such as the DLT algorithm.

(v) Iteration.

Starting from the initial solution, the parameters are iteratively

reﬁned with the goal of minimizing the cost function.

A word about parametrization
For a given cost function, there are often several choices of parametrization. The gen-
eral strategy that guides parametrization is to select a set of parameters that cover the
complete space over which one is minimizing, while at the same time allowing one to

4.5 Iterative minimization methods

111

compute the cost function in a convenient manner. For example, H may be parametrized
by 9 parameters – that is, it is over-parametrized, since there are really only 8 degrees
of freedom, overall scale not being signiﬁcant. A minimal parametrization (i.e. the
same number of parameters as degrees of freedom) would involve only 8 parameters.
In general no bad effects are likely to occur if a minimization problem of this type is
over-parametrized, as long as for all choices of parameters the corresponding object is
of the desired type. In particular for homogeneous objects, such as the 3× 3 projection
matrix encountered here, it is usually not necessary or advisable to attempt to use a
minimal parametrization by removing the scale-factor ambiguity.

The reasoning is the following: it is not necessary to use minimal parametrization
because a well-performing non-linear minimization algorithm will “notice” that it is
not necessary to move in redundant directions, such as the matrix scaling direction.
The algorithm described in Gill and Murray [Gill-78], which is a modiﬁcation of the
Gauss–Newton method, has an effective strategy for discarding redundant combina-
tions of the parameters. Similarly, the Levenberg-Marquardt algorithm (see section
A6.2(p600)) handles redundant parametrizations easily. It is not advisable because it
is found empirically that the cost function surface is more complicated when minimal
parametrizations are used. There is then a greater possibility of becoming stuck in a
local minimum.

One other issue that arises in choosing a parametrization is that of restricting the
transformation to a particular class. For example, suppose H is known to be a homology,
then as described in section A7.2(p629) it may be parametrized as

H = I + (µ − 1)

vaT
vTa

where µ is a scalar, and v and a 3-vectors. A homology has 5 degrees of freedom which
correspond here to the scalar µ and the directions of v and a. If H is parametrized by its
9 matrix entries, then the estimated H is unlikely to exactly be a homology. However,
if H is parametrized by µ, v and a (a total of 7 parameters) then the estimated H is
guaranteed to be a homology. This parametrization is consistent with a homology (it is
also an over-parametrization). We will return to the issues of consistent, local, minimal
and over-parametrization in later chapters. The issues are also discussed further in
appendix A6.9(p623).
Function speciﬁcation
It has been seen in section 4.2.7 that a general class of estimation problems is concerned
with a measurement space IRN containing a model surface S. Given a measurement

X ∈ IRN the estimation task is to ﬁnd the point (cid:23)X lying on S closest to X. In the case

where a non-isotropic Gaussian error distribution is imposed on IRN, the word closest
is to be interpreted in terms of Mahalanobis distance. Iterative minimization methods
will now be described in terms of this estimation model. In iterative estimation through
parameter ﬁtting, the model surface S is locally parametrized, and the parameters are
allowed to vary to minimize the distance to the measured point. More speciﬁcally,

(i) One has a measurement vector X ∈ IRN with covariance matrix Σ.

112

4 Estimation – 2D Projective Transformations

(ii) A set of parameters are represented as a vector P ∈ IRM .
(iii) A mapping f : IRM → IRN is deﬁned. The range of this mapping is (at least
locally) the model surface S in IRN representing the set of allowable measure-
ments.

(iv) The cost function to be minimized is the squared Mahalanobis distance

(cid:10)X − f (P)(cid:10)2

Σ = (X − f (P))TΣ−1(X − f (P)).

In effect, we are attempting to ﬁnd a set of parameters P such that f (P) = X, or fail-
ing that, to bring f (P) as close to X as possible, with respect to Mahalanobis distance.
The Levenberg–Marquardt algorithm is a general tool for iterative minimization, when
the cost function to be minimized is of this type. We will now show how the various
different types of cost functions described in this chapter ﬁt into this format.

Error in one image. Here one ﬁxes the coordinates of points xi in the ﬁrst image, and
varies H so as to minimize cost function (4.6–p94), namely

(cid:7)

d(x(cid:2)

i, H¯xi)2.

i

The measurement vector X is made up of the 2n inhomogeneous coordinates of the
points x(cid:2)
i. One may choose as parameters the vector h of entries of the homography
matrix H. The function f is deﬁned by

f : h (cid:6)→ (Hx1, Hx2, . . . ,H xn)

where it is understood that here, and in the functions below, Hxi indicates the inhomo-
geneous coordinates. One veriﬁes that (cid:10)X − f (h)(cid:10)2 is equal to (4.6–p94).

Symmetric transfer error.

(cid:7)

In the case of the symmetric cost function (4.7–p95)
d(xi, H−1x(cid:2)

i)2 + d(x(cid:2)

i, Hxi)2

i

one chooses as measurement vector X the 4n-vector made up of the inhomogeneous
coordinates of the points xi followed by the inhomogeneous coordinates of the points
x(cid:2)
i. The parameter vector as before is the vector h of entries of H, and the function f is
deﬁned by

f : h (cid:6)→ (H−1x(cid:2)

1, . . . ,H −1x(cid:2)

n, Hx1, . . . ,H xn).

As before, we ﬁnd that (cid:10)X − f (h)(cid:10)2 is equal to (4.7–p95).

Reprojection error. Minimizing the cost function (4.8–p95) is more complex. The
difﬁculty is that it requires a simultaneous minimization over all choices of points ˆxi
as well as the entries of the transformation matrix H. If there are many point corre-
spondences, then this becomes a very large minimization problem. Thus, the problem
may be parametrized by the coordinates of the points ˆxi and the entries of the matrix
ˆH – a total of 2n + 9 parameters. The coordinates of ˆx(cid:2)
i are not required, since they
are related to the other parameters by ˆx(cid:2)
i = ˆHˆxi. The parameter vector is therefore

113
P = (h, ˆx1, . . . , ˆxn). The measurement vector contains the inhomogeneous coordi-
nates of all the points xi and x(cid:2)

4.5 Iterative minimization methods

i. The function f is deﬁned by
1, . . . , ˆxn, ˆx(cid:2)
n)

f : (h, ˆx1, . . . , ˆxn) (cid:6)→ (ˆx1, ˆx(cid:2)

i = Hˆxi. One veriﬁes that (cid:10)X− f (P)(cid:10)2, with X a 4n-vector, is equal to the cost
where ˆx(cid:2)
function (4.8–p95). This cost function must be minimized over all 2n + 9 parameters.

Sampson approximation.
In contrast with 2n + 9 parameters of reprojection error,
minimizing the error in one image (4.6–p94) or symmetric transfer error (4.7–p95)
requires a minimization over the 9 entries of the matrix H only – in general a more
tractable problem. The Sampson approximation to reprojection error enables reprojec-
tion error also to be minimized with only 9 parameters.

This is an important consideration, since the iterative solution of an m-parameter
non-linear minimization problem using a method such as Levenberg–Marquardt in-
volves the solution of an m × m set of linear equations at each iteration step. This is a
problem with complexity O(m3). Hence, it is appropriate to keep the size of m low.
The Sampson error avoids minimizing over the 2n + 9 parameters of reprojection er-
ror because effectively it determines the 2n variables {ˆxi} for each particular choice of
h. Consequently the minimization then only requires the 9 parameters of h.
In prac-
tice this approximation gives excellent results provided the errors are small compared
to the measurements.

Initialization
An initial estimate for the parametrization may be found by employing a linear tech-
nique. For example, the normalized DLT algorithm 4.2 directly provides H and thence
the 9-vector h used to parametrize the iterative minimization. In general if there are
n ≥ 4 correspondences, then all will be used in the linear solution. However, as will be
seen in section 4.7 on robust estimation, when the correspondences contain outliers it
may be advisable to use a carefully selected minimal set of correspondences (i.e. four
correspondences). Linear techniques or minimal solutions are the two initialization
techniques recommended in this book.

An alternative method that is sometimes used (for instance see [Horn-90, Horn-91])
is to carry out a sufﬁciently dense sampling of parameter space, iterating from each
sampled starting point and retaining the best result. This is only possible if the dimen-
sion of the parameter space is sufﬁciently small. Sampling of parameter space may be
done either randomly, or else according to some pattern. Another initialization method
is simply to do without any effective initialization at all, starting the iteration at a given
ﬁxed point in parameter space. This method is not often viable. Iteration is very likely
to fall into a false minimum or not converge. Even in the best case, the number of
iteration steps required will increase the further one starts from the ﬁnal solution. For
this reason using a good initialization method is the best plan.

114

4 Estimation – 2D Projective Transformations

Objective
Given n >4 image point correspondences {xi ↔ x(cid:1)
estimate ˆH of the homography mapping between the images.
The MLE involves also solving for a set of subsidiary points {ˆxi}, which minimize

i}, determine the Maximum Likelihood

(cid:7)

d(xi, ˆxi)2 + d(x(cid:1)

i, ˆx(cid:1)
i)2

where ˆx(cid:1)
Algorithm

i = ˆHˆxi.

i

(ii) Geometric minimization of – either Sampson error:

(i) Initialization: Compute an initial estimate of ˆH to provide a starting point for the ge-
ometric minimization. For example, use the linear normalized DLT algorithm 4.2, or
use RANSAC (section 4.7.1) to compute ˆH from four point correspondences.
• Minimize the Sampson approximation to the geometric error (4.12–p99).
• The cost is minimized using the Newton algorithm of section A6.1(p597) or
Levenberg–Marquardt algorithm of section A6.2(p600) over a suitable parametriza-
tion of ˆH. For example the matrix may be parametrized by its 9 entries.

or Gold Standard error:
• Compute an initial estimate of the subsidiary variables {ˆxi} using the measured
points {xi} or (better) the Sampson correction to these points given by (4.11–p99).
• Minimize the cost

(cid:7)

d(xi, ˆxi)2 + d(x(cid:1)

i, ˆx(cid:1)
i)2

i

over ˆH and ˆxi, i = 1, . . . , n. The cost is minimized using the Levenberg–Marquardt
algorithm over 2n+9 variables: 2n for the n 2D points ˆxi, and 9 for the homography
matrix ˆH.
• If the number of points is large then the sparse method of minimizing this cost func-

tion given in section A6.4(p607) is the recommended approach.

Algorithm 4.3. The Gold Standard algorithm and variations for estimating H from image correspon-
dences. The Gold Standard algorithm is preferred to the Sampson method for 2D homography compu-
tation.

Iteration methods

There are various iterative methods for minimizing the chosen cost function, of which
the most popular are Newton iteration and the Levenberg–Marquardt method. These
methods are described in appendix 6(p597). Other general methods for minimizing
a cost function are available, such as Powell’s method and the simplex method both
described in [Press-88].

Summary. The ideas in this section are collected together in algorithm 4.3, which
describes the Gold Standard and Sampson methods for estimating the homography
mapping between point correspondences in two images.

4.6 Experimental comparison of the algorithms

115

a

b

c

Fig. 4.5. Three images of a plane which are used to compare methods of computing projective transfor-
mations from corresponding points.

Method

Pair 1

ﬁgure 4.5 a & b

Pair 2

ﬁgure 4.5 a & c

Linear normalized
Gold Standard
Linear unnormalized
Homogeneous scaling
Sampson
Error in 1 view
Afﬁne
Theoretical optimal

0.4078
0.4078
0.4080
0.5708
0.4077
0.4077
6.0095
0.5477

0.6602
0.6602
26.2056
0.7421
0.6602
0.6602
2.8481
0.6582

Table 4.1. Residual errors in pixels for the various algorithms.

4.6 Experimental comparison of the algorithms

The algorithms are compared for the images shown in ﬁgure 4.5. Table 4.1 shows the
results of testing several of the algorithms described in this chapter. Residual error is
shown for two pairs of images. The methods used are fairly self-explanatory, with a
few exceptions. The method “afﬁne” was an attempt to ﬁt the projective transformation
with an optimal afﬁne mapping. The “optimal” is the ML estimate assuming a noise
level of one pixel.

The ﬁrst pair of images are (a) and (b) of ﬁgure 4.5, with 55 point correspondences.
It appears that all methods work almost equally well (except the afﬁne method). The
optimal residual is greater than the achieved results, because the noise level (unknown)
is less than one pixel.

Image (c) of ﬁgure 4.5 was produced synthetically by resampling (a), and the second
pair consists of (a) and (c) with 20 point correspondences.
In this case, almost all
methods perform almost optimally, as shown in the table 4.1. The exception is the
afﬁne method (expected to perform badly, since it is not an afﬁne transformation) and
the unnormalized linear method. The unnormalized method is expected to perform
badly (though maybe not this badly). Just why it performs well in the ﬁrst pair and
very badly for the second pair is not understood. In any case, it is best to avoid this
method and use a normalized linear or Gold Standard method.

A further evaluation is presented in ﬁgure 4.6. The transformation to be estimated is
the one that maps the chessboard image shown here to a square grid aligned with the

116

4 Estimation – 2D Projective Transformations

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

2

1.5

1

0.5

r
o
r
r
E

 
l
a
u
d
i
s
e
R

r
o
r
r
e
 
l
a
u
d
i
s
e
R

0

10

20

30

40

50

60

70

Number of Points

b

4

6
Noise Level

8

10

0

0

0.5

1

1.5

2

2.5

3

Noise level

d

r
o
r
r
E

 
l
a
u
d
i
s
e
R

7

6

5

4

3

2

1

0

0

a

2

c

Fig. 4.6. Comparison of the DLT and Gold Standard algorithms to the theoretically optimal resid-
ual error.
(a) The homography is computed between a chessboard and this image. In all three graphs,
the result for the Gold Standard algorithm overlap and are indistinguishable from the theoretical mini-
mum. (b) Residual error as a function of the number of points. (c) The effect of varying noise level for
10 points, and (d) 50 points.

axes. As may be seen, the image is substantially distorted, with respect to a square grid.
For the experiments, randomly selected points in the image were chosen and matched
with the corresponding point on the square grid. The (normalized) DLT algorithm
and the Gold Standard algorithm are compared to the theoretical minimum or residual
error (see chapter 5). Note that for noise up to 5 pixels, the DLT algorithm performs
adequately. However, for a noise level of 10 pixels it fails. Note however that in a 200-
pixel image, an error of 10 pixels is extremely high. For less severe homographies,
closer to the identity map, the DLT performs almost as well as the Gold Standard
algorithm.

4.7 Robust estimation

Up to this point it has been assumed that we have been presented with a set of corre-
spondences, {xi ↔ x(cid:2)
}, where the only source of error is in the measurement of the
point’s position, which follows a Gaussian distribution. In many practical situations
this assumption is not valid because points are mismatched. The mismatched points
are outliers to the Gaussian error distribution. These outliers can severely disturb the

i

4.7 Robust estimation

117

c

a

d

b

b

a

Fig. 4.7. Robust line estimation. The solid points are inliers, the open points outliers. (a) A least-
squares (orthogonal regression) ﬁt to the point data is severely affected by the outliers.
(b) In the
RANSAC algorithm the support for lines through randomly selected point pairs is measured by the num-
ber of points within a threshold distance of the lines. The dotted lines indicate the threshold distance.
For the lines shown the support is 10 for line (cid:8)a, b(cid:9) (where both of the points a and b are inliers); and
2 for line (cid:8)c, d(cid:9) where the point c is an outlier.

estimated homography, and consequently should be identiﬁed. The goal then is to de-
termine a set of inliers from the presented “correspondences” so that the homography
can then be estimated in an optimal manner from these inliers using the algorithms de-
scribed in the previous sections. This is robust estimation since the estimation is robust
(tolerant) to outliers (measurements following a different, and possibly unmodelled,
error distribution).

4.7.1 RANSAC
We start with a simple example that can easily be visualized – estimating a straight
line ﬁt to a set of 2-dimensional points. This can be thought of as estimating a 1-
dimensional afﬁne transformation, x
= ax + b, between corresponding points lying on
two lines.

(cid:2)

The problem, which is illustrated in ﬁgure 4.7a, is the following: given a set of 2D
data points, ﬁnd the line which minimizes the sum of squared perpendicular distances
(orthogonal regression), subject to the condition that none of the valid points deviates
from this line by more than t units. This is actually two problems: a line ﬁt to the data;
and a classiﬁcation of the data into inliers (valid points) and outliers. The threshold t is
set according to the measurement noise (for example t = 3σ), and is discussed below.
There are many types of robust algorithms and which one to use depends to some extent
on the proportion of outliers. For example, if it is known that there is only one outlier,
then each point can be deleted in turn and the line estimated from the remainder. Here
we describe in detail a general and very successful robust estimator – the RANdom
SAmple Consensus (RANSAC) algorithm of Fischler and Bolles [Fischler-81]. The
RANSAC algorithm is able to cope with a large proportion of outliers.

The idea is very simple: two of the points are selected randomly; these points deﬁne
a line. The support for this line is measured by the number of points that lie within a
distance threshold. This random selection is repeated a number of times and the line
with most support is deemed the robust ﬁt. The points within the threshold distance are
the inliers (and constitute the eponymous consensus set). The intuition is that if one of
the points is an outlier then the line will not gain much support, see ﬁgure 4.7b.

118

4 Estimation – 2D Projective Transformations

Furthermore, scoring a line by its support has the additional advantage of favouring
better ﬁts. For example, the line (cid:8)a, b(cid:9) in ﬁgure 4.7b has a support of 10, whereas the
line (cid:8)a, d(cid:9), where the sample points are neighbours, has a support of only 4. Conse-
quently, even though both samples contain no outliers, the line (cid:8)a, b(cid:9) will be selected.
More generally, we wish to ﬁt a model, in this case a line, to data, and the random
sample consists of a minimal subset of the data, in this case two points, sufﬁcient to
determine the model. If the model is a planar homography, and the data a set of 2D
point correspondences, then the minimal subset consists of four correspondences. The
application of RANSAC to the estimation of a homography is described below.

As stated by Fischler and Bolles [Fischler-81] “The RANSAC procedure is opposite
to that of conventional smoothing techniques: Rather than using as much of the data as
possible to obtain an initial solution and then attempting to eliminate the invalid data
points, RANSAC uses as small an initial data set as feasible and enlarges this set with
consistent data when possible”.

The RANSAC algorithm is summarized in algorithm 4.4. Three questions immedi-

ately arise:

Objective

Robust ﬁt of a model to a data set S which contains outliers.

Algorithm

(i) Randomly select a sample of s data points from S and instantiate the model from this

subset.

(ii) Determine the set of data points Si which are within a distance threshold t of the model.

The set Si is the consensus set of the sample and deﬁnes the inliers of S.
(iii) If the size of Si (the number of inliers) is greater than some threshold T ,

re-estimate the model using all the points in Si and terminate.

(iv) If the size of Si is less than T , select a new subset and repeat the above.
(v) After N trials the largest consensus set Si is selected, and the model is

re-estimated using all the points in the subset Si.

Algorithm 4.4. The RANSAC robust estimation algorithm, adapted from [Fischler-81]. A minimum of s
data points are required to instantiate the free parameters of the model. The three algorithm thresholds
t, T , and N are discussed in the text.

1. What is the distance threshold? We would like to choose the distance threshold, t,
such that with a probability α the point is an inlier. This calculation requires the prob-
ability distribution for the distance of an inlier from the model. In practice the distance
threshold is usually chosen empirically. However, if it is assumed that the measure-
ment error is Gaussian with zero mean and standard deviation σ, then a value for t may
be computed. In this case the square of the point distance, d2⊥, is a sum of squared
Gaussian variables and follows a χ2
m distribution with m degrees of freedom, where
m equals the codimension of the model. For a line the codimension is 1 – only the
perpendicular distance to the line is measured. If the model is a point the codimension
is 2, and the square of the distance is the sum of squared x and y measurement errors.

4.7 Robust estimation

119

The probability that the value of a χ2
mulative chi-squared distribution, Fm(k2) =
are described in section A2.2(p566). From the cumulative distribution

m random variable is less than k2 is given by the cu-
m(ξ)dξ. Both of these distributions

 

(cid:31)

k2
0 χ2

d2⊥ < t2

inlier
outlier d2⊥ ≥ t2 with t2 = F

−1
m (α)σ2.

(4.17)

Usually α is chosen as 0.95, so that there is a 95% probability that the point is an inlier.
This means that an inlier will only be incorrectly rejected 5% of the time. Values of t
for α = 0.95 and for the models of interest in this book are tabulated in table 4.2.

Codimension m

Model

1
2
3

line, fundamental matrix

homography, camera matrix

trifocal tensor

t2

3.84 σ2
5.99 σ2
7.81 σ2

Table 4.2. The distance threshold t2 = F
spondence) is an inlier.

−1
m (α)σ2 for a probability of α = 0.95 that the point (corre-

2. How many samples? It is often computationally infeasible and unnecessary to try
every possible sample. Instead the number of samples N is chosen sufﬁciently high
to ensure with a probability, p, that at least one of the random samples of s points is
free from outliers. Usually p is chosen at 0.99. Suppose w is the probability that any
selected data point is an inlier, and thus  = 1− w is the probability that it is an outlier.
Then at least N selections (each of s points) are required, where (1 − ws)N = 1 − p,
so that

N = log(1 − p)/ log(1 − (1 − )s).
Table 4.3 gives examples of N for p = 0.99 for a given s and .

(4.18)

Sample size

Proportion of outliers 

s

2
3
4
5
6
7
8

5% 10% 20% 25% 30% 40% 50%

2
3
3
4
4
4
5

3
4
5
6
7
8
9

5
7
9
12
16
20
26

6
9
13
17
24
33
44

7
11
17
26
37
54
78

11
19
34
57
97
163
272

17
35
72
146
293
588
1177

Table 4.3. The number N of samples required to ensure, with a probability p = 0.99, that at least one
sample has no outliers for a given size of sample, s, and proportion of outliers, .

Example 4.4. For the line-ﬁtting problem of ﬁgure 4.7 there are n = 12 data points, of

120

4 Estimation – 2D Projective Transformations

which two are outliers so that  = 2/12 = 1/6. From table 4.3 for a minimal subset
of size s = 2, at least N = 5 samples are required. This should be compared with
the cost of exhaustively trying every point pair, in which case (12
2 ) = 66 samples are
required (the notation (n
2 ) means the number of choices of 2 among n, speciﬁcally,
(cid:2)
2 ) = n(n − 1)/2).
(n
Note

(i) The number of samples is linked to the proportion rather than number of out-
liers. This means that the number of samples required may be smaller than the
number of outliers. Consequently the computational cost of the sampling can
be acceptable even when the number of outliers is large.

(ii) The number of samples increases with the size of the minimal subset (for a
given  and p). It might be thought that it would be advantageous to use more
than the minimal subset, three or more points in the case of a line, because then
a better estimate of the line would be obtained, and the measured support would
more accurately reﬂect the true support. However, this possible advantage in
measuring support is generally outweighed by the severe increase in computa-
tional cost incurred by the increase in the number of samples.

3. How large is an acceptable consensus set? A rule of thumb is to terminate if the
size of the consensus set is similar to the number of inliers believed to be in the data
set, given the assumed proportion of outliers, i.e. for n data points T = (1 − )n. For
the line-ﬁtting example of ﬁgure 4.7 a conservative estimate of  is  = 0.2, so that
T = (1.0 − 0.2)12 = 10.

Determining the number of samples adaptively.
It is often the case that , the
fraction of data consisting of outliers, is unknown.
In such cases the algorithm is
initialized using a worst case estimate of , and this estimate can then be updated as
larger consistent sets are found. For example, if the worst case guess is  = 0.5 and
a consensus set with 80% of the data is found as inliers, then the updated estimate is
 = 0.2.

This idea of “probing” the data via the consensus sets can be applied repeatedly in
order to adaptively determine the number of samples, N. To continue the example
above, the worst case estimate of  = 0.5 determines an initial N according to (4.18).
When a consensus set containing more than 50% of the data is found, we then know
that there is at least that proportion of inliers. This updated estimate of  determines a
reduced N from (4.18). This update is repeated at each sample, and whenever a con-
sensus set with  lower than the current estimate is found, then N is again reduced. The
algorithm terminates as soon as N samples have been performed. It may occur that a
sample is found for which  determines an N less than the number of samples that have
already been performed. In such a case sufﬁcient samples have been performed and the
algorithm terminates. In pseudo-code the adaptive computation of N is summarized
in algorithm 4.5.

This adaptive approach works very well and in practice covers the questions of both

4.7 Robust estimation

121

• N = ∞, sample count= 0.
• While N > sample count Repeat

– Choose a sample and count the number of inliers.
– Set  = 1 − (number of inliers)/(total number of points)
– Set N from  and (4.18) with p = 0.99.
– Increment the sample count by 1.

• Terminate.

Algorithm 4.5. Adaptive algorithm for determining the number of RANSAC samples.

C

D

B

A

a

C

D

B

A

b

Fig. 4.8. Robust ML estimation. The grey points are classiﬁed as inliers to the line. (a) A line deﬁned
by points (cid:8)A, B(cid:9) has a support of four (from points {A, B, C, D}). (b) The ML line ﬁt (orthogonal
least-squares) to the four points. This is a much improved ﬁt over that deﬁned by (cid:8)A, B(cid:9). 10 points are
classiﬁed as inliers.

the number of samples and terminating the algorithm. The initial  can be chosen
as 1.0, in which case the initial N will be inﬁnite. It is wise to use a conservative
probability p such as 0.99 in (4.18). Table 4.4 on page 127 gives example ’s and N’s
when computing a homography.

4.7.2 Robust Maximum Likelihood estimation
The RANSAC algorithm partitions the data set into inliers (the largest consensus set)
and outliers (the rest of the data set), and also delivers an estimate of the model, M0,
computed from the minimal set with greatest support. The ﬁnal step of the RANSAC
algorithm is to re-estimate the model using all the inliers. This re-estimation should be
optimal and will involve minimizing a ML cost function, as described in section 4.3.
In the case of a line, ML estimation is equivalent to orthogonal regression, and a closed
form solution is available. In general, though, the ML estimation involves iterative
minimization, and the minimal set estimate, M0, provides the starting point.

The only drawback with this procedure, which is often the one adopted, is that the
inlier–outlier classiﬁcation is irrevocable. After the model has been optimally ﬁtted to
the consensus set, there may well be additional points which would now be classiﬁed
as inliers if the distance threshold was applied to the new model. For example, suppose
the line (cid:8)A, B(cid:9) in ﬁgure 4.8 was selected by RANSAC. This line has a support of
four points, all inliers. After the optimal ﬁt to these four points, there are now 10 points
which would correctly be classiﬁed as inliers. These two steps: optimal ﬁt to inliers; re-
classify inliers using (4.17); can then be iterated until the number of inliers converges.

122

4 Estimation – 2D Projective Transformations

A least-squares ﬁt with inliers weighted by their distance to the model is often used at
this stage.
Robust cost function. An alternative to minimizing C =
minimize a robust version including all data. A suitable robust cost function is

i d2⊥i over the inliers is to

(cid:27)

 

(cid:7)

i

D =

γ (d⊥i)

with γ(e) =

e2 e2 < t2
t2

inlier
e2 ≥ t2 outlier

(4.19)

Here d⊥i are point errors and γ(e) is a robust cost function [Huber-81] where outliers
are given a ﬁxed cost. The χ2 motivation for the threshold is the same as that of (4.17),
where t2 is deﬁned. The quadratic cost for inliers arises from the Gaussian error model,
as described in section 4.3. The constant cost for outliers in the robust cost function
arises from the assumption that outliers follow a diffuse or uniform distribution, the log-
likelihood of which is a constant. It might be thought that outliers could be excluded
from the cost function by simply thresholding on d⊥i. The problem with thresholding
alone is that it would result in only outliers being included because they would incur
no cost.
The cost function D allows the minimization to be conducted on all points whether
they are outliers or inliers. At the start of the iterative minimization D differs from C
only by a constant (given by 4 times the number of outliers). However, as the min-
imization progresses outliers can be redesignated inliers, and this typically occurs in
practice. A discussion and comparison of cost functions is given in appendix A6.8-
(p616).

4.7.3 Other robust algorithms
In RANSAC a model instantiated from a minimal set is scored by the number of data
points within a threshold distance. An alternative is to score the model by the me-
dian of the distances to all points in the data. The model with least median is then
selected. This is Least Median of Squares (LMS) estimation, where, as in RANSAC,
minimum size subset samples are selected randomly with the number of samples ob-
tained from (4.18). The advantage of LMS is that it requires no setting of thresholds or
a priori knowledge of the variance of the error. The disadvantage of LMS is that it fails
if more than half the data is outlying, for then the median distance will be to an outlier.
The solution is to use the proportion of outliers to determine the selection distance. For
example if there are 50% outliers then a distance below the median value (the quartile
say) should be used.

Both the RANSAC and LMS algorithms are able to cope with a large proportion of
outliers. If the number of outliers is small, then other robust methods may well be more
efﬁcient. These include case deletion, where each point in turn is deleted and the model
ﬁtted to the remaining data; and iterative weighted least-squares, where a data point’s
inﬂuence on the ﬁt is weighted inversely by its residual. Generally these methods
are not recommended. Both Torr [Torr-95b] and Xu and Zhang [Xu-96] describe and
compare various robust estimators for estimating the fundamental matrix.

4.8 Automatic computation of a homography

123

Objective

Compute the 2D homography between two images.

Algorithm

(i) Interest points: Compute interest points in each image.
(ii) Putative correspondences: Compute a set of interest point matches based on proxim-

ity and similarity of their intensity neighbourhood.

(iii) RANSAC robust estimation: Repeat for N samples, where N is determined adap-

tively as in algorithm 4.5:

(a) Select a random sample of 4 correspondences and compute the homography H.
(b) Calculate the distance d⊥ for each putative correspondence.
(c) Compute the number of inliers consistent with H by the number of correspon-

√

dences for which d⊥ < t =

5.99 σ pixels.

Choose the H with the largest number of inliers. In the case of ties choose the solution
that has the lowest standard deviation of inliers.

(iv) Optimal estimation: re-estimate H from all correspondences classiﬁed as inliers, by
minimizing the ML cost function (4.8–p95) using the Levenberg–Marquardt algorithm
of section A6.2(p600).

(v) Guided matching: Further interest point correspondences are now determined using

the estimated H to deﬁne a search region about the transferred point position.

The last two steps can be iterated until the number of correspondences is stable.

Algorithm 4.6. Automatic estimation of a homography between two images using RANSAC.

4.8 Automatic computation of a homography

This section describes an algorithm to automatically compute a homography between
two images. The input to the algorithm is simply the images, with no other a priori
information required; and the output is the estimated homography together with a set
of interest points in correspondence. The algorithm might be applied, for example, to
two images of a planar surface or two images acquired by rotating a camera about its
centre.

The ﬁrst step of the algorithm is to compute interest points in each image. We are
then faced with a “chicken and egg” problem: once the correspondence between the
interest points is established the homography can be computed; conversely, given the
homography the correspondence between the interest points can easily be established.
This problem is resolved by using robust estimation, here RANSAC, as a “search en-
gine”. The idea is ﬁrst to obtain by some means a set of putative point correspondences.
It is expected that a proportion of these correspondences will in fact be mismatches.
RANSAC is designed to deal with exactly this situation – estimate the homography
and also a set of inliers consistent with this estimate (the true correspondences), and
outliers (the mismatches).

The algorithm is summarized in algorithm 4.6, with an example of its use shown
in ﬁgure 4.9, and the steps described in more detail below. Algorithms with essentially
the same methodology enable the automatic computation of the fundamental matrix
and trifocal tensor directly from image pairs and triplets respectively. This computation
is described in chapter 11 and chapter 16.

124

4 Estimation – 2D Projective Transformations

Determining putative correspondences. The aim, in the absence of any knowledge
of the homography, is to provide an initial point correspondence set. A good proportion
of these correspondences should be correct, but the aim is not perfect matching, since
RANSAC will later be used to eliminate the mismatches. Think of these as “seed”
correspondences. These putative correspondences are obtained by detecting interest
points independently in each image, and then matching these interest points using a
combination of proximity and similarity of intensity neighbourhoods as follows. For
brevity, the interest points will be referred to as ‘corners’. However, these corners need
not be images of physical corners in the scene. The corners are deﬁned by a minimum
of the image auto-correlation function.

For each corner at (x, y) in image 1 the match with highest neighbourhood cross-
correlation in image 2 is selected within a square search region centred on (x, y). Sym-
metrically, for each corner in image 2 the match is sought in image 1. Occasionally
there will be a conﬂict where a corner in one image is “claimed” by more than one
corner in the other. In such cases a “winner takes all” scheme is applied and only the
match with highest cross-correlation is retained.

A variation on the similarity measure is to use Squared Sum of intensity Differences
(SSD) instead of (normalized) Cross-Correlation (CC). CC is invariant to the afﬁne
mapping of the intensity values (i.e. I (cid:6)→ αI+β, scaling plus offset) which often occurs
in practice between images. SSD is not invariant to this mapping. However, SSD is
often preferred when there is small variation in intensity between images, because it is
a more sensitive measure than CC and is computationally cheaper.

RANSAC for a homography. The RANSAC algorithm is applied to the putative
correspondence set to estimate the homography and the (inlier) correspondences which
are consistent with this estimate. The sample size is four, since four correspondences
determine a homography. The number of samples is set adaptively as the proportion of
outliers is determined from each consensus set, as described in algorithm 4.5.

There are two issues: what is the “distance” in this case? and how should the samples

be selected?

(i) Distance measure: The simplest method of assessing the error of a corre-
spondence from a homography H is to use the symmetric transfer error, i.e.
transfer = d(x, H−1x(cid:2)
is the point correspon-
d2
dence. A better, though more expensive, distance measure is the reprojection
error, d2⊥ = d(x, ˆx)2 + d(x(cid:2)
= Hˆx is the perfect correspondence.
This measure is more expensive because ˆx must also be computed. A further
alternative is Sampson error.

, Hx)2, where x ↔ x(cid:2)
)2, where ˆx(cid:2)

)2 + d(x(cid:2)
, ˆx(cid:2)

(ii) Sample selection: There are two issues here. First, degenerate samples should
be disregarded. For example, if three of the four points are collinear then a
homography cannot be computed; second, the sample should consist of points
with a good spatial distribution over the image. This is because of the extrap-
olation problem – an estimated homography will accurately map the region
straddled by the computation points, but the accuracy generally deteriorates

4.8 Automatic computation of a homography

125

with distance from this region (think of four points in the very top corner of the
image). Distributed spatial sampling can be implemented by tiling the image
and ensuring, by a suitable weighting of the random sampler, that samples with
points lying in different tiles are the more likely.

Robust ML estimation and guided matching. The aim of this ﬁnal stage is two-
fold: ﬁrst, to obtain an improved estimate of the homography by using all the inliers in
the estimation (rather than only the four points of the sample); second, to obtain more
inlying matches from the putative correspondence set because a more accurate homog-
raphy is available. An improved estimate of the homography is then computed from
the inliers by minimizing an ML cost function. This ﬁnal stage can be implemented
in two ways. One way is to carry out an ML estimation on the inliers, then recompute
the inliers using the new estimated H, and repeat this cycle until the number of inliers
converges. The ML cost function minimization is carried out using the Levenberg–
Marquardt algorithm described in section A6.2(p600). The alternative is to estimate
the homography and inliers simultaneously by minimizing a robust ML cost function
of (4.19) as described in section 4.7.2. The disadvantage of the simultaneous approach
is the computational effort incurred in the minimization of the cost function. For this
reason the cycle approach is usually the more attractive.

4.8.1 Application domain

The algorithm requires that interest points can be recovered fairly uniformly across the
image, and this in turn requires scenes and resolutions which support this requirement.
Scenes should be lightly textured – images of blank walls are not ideal.

The search window proximity constraint places an upper limit on the image motion
of corners (the disparity) between views. However, the algorithm is not defeated if this
constraint is not applied, and in practice the main role of the proximity constraint is to
reduce computational complexity, as a smaller search window means that fewer corner
matches must be evaluated.

Ultimately the scope of the algorithm is limited by the success of the corner neigh-
bourhood similarity measure (SSD or CC) in providing disambiguation between cor-
respondences. Failure generally results from lack of spatial invariance: the measures
are only invariant to image translation, and are severely degraded by transformations
outside this class such as image rotation or signiﬁcant differences in foreshortening
between images. One solution is to use measures with a greater invariance to the ho-
mography mapping between images, for example measures which are rotationally in-
variant. An alternative solution is to use an initial estimate of the homography to map
between intensity neighbourhoods. Details are beyond the scope of this discussion,
but are provided in [Pritchett-98, Schmid-98]. The use of robust estimation confers
moderate immunity to independent motion, changes in shadows, partial occlusions etc.

126

4 Estimation – 2D Projective Transformations

a

c

e

g

b

d

f

h

Fig. 4.9. Automatic computation of a homography between two images using RANSAC. The mo-
tion between views is a rotation about the camera centre so the images are exactly related by a homog-
raphy. (a) (b) left and right images of Keble College, Oxford. The images are 640 × 480 pixels. (c) (d)
detected corners superimposed on the images. There are approximately 500 corners on each image. The
following results are superimposed on the left image: (e) 268 putative matches shown by the line linking
corners, note the clear mismatches; (f) outliers – 117 of the putative matches; (g) inliers – 151 corre-
spondences consistent with the estimated H; (h) ﬁnal set of 262 correspondences after guided matching
and MLE.

4.9 Closure

127

4.8.2 Implementation and run details
Interest points are obtained using the Harris [Harris-88] corner detector. This detec-
tor localizes corners to sub-pixel accuracy, and it has been found empirically that the
correspondence error is usually less than a pixel [Schmid-98].

When obtaining seed correspondences, in the putative correspondence stage of the
algorithm, the threshold on the neighbourhood similarity measure for match acceptance
is deliberately conservative to minimize incorrect matches (the SSD threshold is 20).
For the guided matching stage this threshold is relaxed (it is doubled) so that additional
putative correspondences are available.

1 − 

Number of
inliers

Adaptive

N

6
10
44
58
73
151

2% 20,028,244
3%
2,595,658
6,922
16%
2,291
21%
911
26%
56%
43

Table 4.4. The results of the adaptive algorithm 4.5 used during RANSAC to compute the homography
for ﬁgure 4.9. N is the total number of samples required as the algorithm runs for p = 0.99 probability
of no outliers in the sample. The algorithm terminated after 43 samples.

For the example of ﬁgure 4.9 the images are 640×480 pixels, and the search window
±320 pixels, i.e. the entire image. Of course a much smaller search window could have
been used given the actual point disparities in this case. Often in video sequences a
search window of ±40 pixels sufﬁces (i.e. a square of side 80 centred on the current
position). The inlier threshold was t = 1.25 pixels.

A total of 43 samples were required, with the sampling run as shown in table 4.4.
The guided matching required two iterations of the MLE–inlier classiﬁcation cycle.
The RMS values for d⊥ pixel error were 0.23 before the MLE and 0.19 after. The
Levenberg–Marquardt algorithm required 10 iterations.

4.9 Closure

This chapter has illustrated the issues and techniques that apply to estimating the ten-
sors representing multiple view relations. These ideas will reoccur in each of the com-
putation chapters throughout the book. In each case there are a minimal number of
correspondences required; degenerate conﬁgurations that should be avoided; algebraic
and geometric errors that can be minimized when more than the minimal number of
correspondences are available; parametrizations that enforce internal constraints on the
tensor etc.

4.9.1 The literature
The DLT algorithm dates back at least to Sutherland [Sutherland-63]. Sampson’s clas-
sic paper on conic ﬁtting (an improvement on the equally classic Bookstein algorithm)

128

4 Estimation – 2D Projective Transformations

appeared in [Sampson-82]. Normalization was made public in the Computer Vision
literature by Hartley [Hartley-97c].

Related reading on numerical methods may be found in the excellent Numerical
Recipes in C [Press-88], and also Gill and Murray [Gill-78] for iterative minimization.
Fischler and Bolles’ [Fischler-81] RANSAC was one of the earliest robust algo-
rithms, and in fact was developed to solve a Computer Vision problem (pose from
3 points). The original paper is very clearly argued and well worth reading. Other
background material on robustness may be found in Rousseeuw [Rousseeuw-87]. The
primary application of robust estimation in computer vision was to estimating the fun-
damental matrix (chapter 11), by Torr and Murray [Torr-93] using RANSAC, and,
Zhang et al. [Zhang-95] using LMS. The automatic ML estimation of a homography
was described by Torr and Zisserman [Torr-98].

4.9.2 Notes and exercises

(i) Computing homographies of IPn. The derivation of (4.1–p89) and (4.3–p89)
assumed that the dimension of x(cid:2)
i is three, so that the cross-product is deﬁned.
However, (4.3) may be derived in a way that generalizes to all dimensions.
(cid:2)
Assuming that w
i = 1, we may solve for the unknown scale factor explicitly by
writing Hxi = k(xi, yi, 1)T. From the third coordinate we obtain k = h3Txi,
and substituting this into the original equation gives
(cid:2)
h3Txi
x
(cid:2)
i
h3Txi
y
i

h1Txi
h2Txi

(cid:20)

(cid:21)

(cid:20)

(cid:21)

=

which leads directly to (4.3).

(ii) Computing homographies for ideal points.

i is an
(cid:2)
ideal point, so that w
i = 0, then the pair of equations (4.3) collapses to a single
equation although (4.1) does contain two independent equations. To avoid such
degeneracy, while including only the minimum number of equations, a good
way to proceed is as follows. We may rewrite the equation x(cid:2)

i = Hxi as

If one of the points x(cid:2)

[x(cid:2)
⊥Hxi = 0
i]

is a matrix with rows orthogonal to x(cid:2)
i = 0. Each
leads to a separate linear equation in the entries of H. The matrix
may be obtained by deleting the ﬁrst row of an orthogonal matrix M satis-
i = (1, 0, . . . ,0) T. A Householder matrix (see section A4.1.2(p580))

where [x(cid:2)
⊥
i]
row of [x(cid:2)
⊥
i]
[x(cid:2)
⊥
i]
fying Mx(cid:2)
is an easily constructed matrix with the desired property.

i so that [x(cid:2)
⊥x(cid:2)
i]

(iii) Scaling unbounded point sets.

In the case of points at or near inﬁnity in a
plane, it is neither reasonable nor feasible to normalize coordinates using the
isotropic (or non-isotropic) scaling schemes presented in this chapter, since the
centroid and scale are inﬁnite or near inﬁnite. A method that seems to give
good results is to normalize the set of points xi = (xi, yi, wi)T such that
i = 1∀i

x2
i + y2

(cid:7)

(cid:7)

(cid:7)

(cid:7)

yi = 0 ;

i = 2

; x2

i + y2

i + w2

xi =

w2
i

i

i

i

i

4.9 Closure

129

Note that the coordinates xi and yi appearing here are the homogeneous co-
ordinates, and the conditions no longer imply that the centroid is at the origin.
Investigate methods of achieving this normalization, and evaluate its properties.
(iv) Transformation invariance of DLT. We consider computation of a 2D ho-
mography by minimizing algebraic error (cid:10)Ah(cid:10) (see (4.5–p94)) subject to vari-
ous constraints. Prove the following cases:

(a) If (cid:10)Ah(cid:10) is minimized subject to the constraint h9 = H33 = 1, then the
result is invariant under change of scale but not translation of coordi-
nates.

(b) If instead the constraint is H2

31 + H2

32 = 1 then the result is similarity

invariant.

(c) Afﬁne case: The same is true for the constraint H31 = H32 = 0; H33 = 1.

(v) Expressions for image coordinate derivatives.

For the map x(cid:2)
(cid:2)
(cid:2)
, ˜y
)T are the inhomogeneous coordinates of the image point):

)T = Hx, derive the following expressions (where ˜x(cid:2)
(cid:2)
(cid:2)
/w

, y
/w

, w
(cid:2)
, y

= (˜x

(x
(x

(cid:2)
(cid:2)

(cid:2)

(cid:2)

=
)T =

(cid:17)

(a) Derivative wrt x

∂ ˜x(cid:2)

1
/∂x =
w(cid:2)
where hj T is the j−th row of H.
(cid:17)

(b) Derivative wrt H

∂ ˜x(cid:2)

1
w(cid:2)
with h as deﬁned in (4.2–p89).

/∂h =

(cid:18)

h1T − ˜x
h2T − ˜y

(cid:2)h3T
(cid:2)h3T

(cid:18)

0 −˜x
xT
0 xT −˜y

(cid:2)xT
(cid:2)xT

(4.20)

(4.21)

(vi) Sampson error with non-isotropic error distributions. The derivation of
Sampson error in section 4.2.6(p98) assumed that points were measured with
circular error distributions. In the case where the point X = (x, y, x
) is
measured with covariance matrix ΣX it is appropriate instead to minimize the
Mahalanobis norm (cid:10)δX(cid:10)2
X δX. Show that in this case the formulae
corresponding to (4.11–p99) and (4.12–p99) are
δX = −ΣXJT(JΣXJT)

ΣX = δT

XΣ−1

(4.22)

−1

, y

(cid:2)

(cid:2)

and

(cid:10)δX(cid:10)2

ΣX = T(JΣXJ T)

−1.

(4.23)

Note that if the measurements in the two images are independent, then the co-
variance matrix ΣX will be block-diagonal with two 2 × 2 diagonal blocks cor-
responding to the two images.

(vii) Sampson error programming hint.

In the case of 2D homography estima-
tion, and in fact every other similar problem considered in this book, the cost
function CH(X) = A(X)h of section 4.2.6(p98) is multilinear in the coordinates

130

4 Estimation – 2D Projective Transformations

Objective
Given n ≥ 4 image point correspondences {xi ↔ x(cid:1)
which minimizes reprojection error in both images (4.8–p95).

i}, determine the afﬁne homography HA

Algorithm

(a) Express points as inhomogeneous 2-vectors. Translate the points xi by a translation t
i by a translation t(cid:1)
.

so that their centroid is at the origin. Do the same to the points x(cid:1)
Henceforth work with the translated coordinates.

(b) Form the n × 4 matrix A whose rows are the vectors

XT

i = (xT

i , x(cid:1)

i

(cid:1)
T) = (xi, yi, x
i, y

(cid:1)
i).

(c) Let V1 and V2 be the right singular-vectors of A corresponding to the two largest (sic)
(d) Let H2×2 = CB−1, where B and C are the 2 × 2 blocks such that

singular values.

(e) The required homography is

(cid:24)

(cid:25)

B
C

.

(cid:25)

,

[V1V2] =

HA =

(cid:24)
(cid:23)Xi = (V1VT

H2×2 H2×2t − t(cid:1)
0T

1

1 + V2VT

2 )Xi

and the corresponding estimate of the image points is given by

Algorithm 4.7. The Gold Standard Algorithm for estimating an afﬁne homography HA from image cor-
respondences.

of X. This means that the partial derivative ∂CH(X)/∂X may be very simply
computed. For instance, the derivative

∂CH(x, y, x

(cid:2)

(cid:2)

, y

)/∂x = CH(x + 1, y, x

(cid:2)

(cid:2)

, y

) − CH(x, y, x

(cid:2)

(cid:2)

)

, y

is exact, not a ﬁnite difference approximation. This means that for pro-
gramming purposes, one does not need to code a special routine for taking
derivatives – the routine for computing CH(X) will sufﬁce. Denoting by Ei
the vector containing 1 in the i-th position, and otherwise 0, one sees that
∂CH(X)/∂Xi = CH(X + Ei) − CH(X), and further

JJT =

(CH(X + Ei) − CH(X)) (CH(X + Ei) − CH(X))

T

.

(cid:7)

i

Also note that computationally it is more efﬁcient to solve JJTλ = − directly
for λ, rather than take the inverse as λ = −(JJT)
(viii) Minimizing geometric error for afﬁne transformations. Given a set of
correspondences (xi, yi) ↔ (x
(cid:2)
i), ﬁnd an afﬁne transformation HA that mini-
mizes geometric error (4.8–p95). We will step through the derivation of a linear
algorithm based on Sampson’s approximation which is exact in this case. The
complete method is summarized in algorithm 4.7.

−1.

(cid:2)
i, y

4.9 Closure

131
(a) Show that the optimum afﬁne transformation takes the centroid of the xi
to the centroid of x(cid:2)
i, so by translating the points to have their centroid
at the origin, the translation part of the transformation is determined. It
is only necessary then to determine the upper-left 2 × 2 submatrix H2×2
of HA, which represents the linear part of the transformation.
T)T lies on VH if and only if [H2×2|− I2×2]X = 0.
(b) The point Xi = (xT
So VH is a codimension-2 subspace of IR4.
(c) Any codimension-2 subspace may be expressed as [H2×2| − I]X = 0
for suitable H2×2. Thus given measurements Xi, the estimation task is
equivalent to ﬁnding the best-ﬁtting codimension-2 subspace.

i , x(cid:2)

i

(d) Given a matrix M with rows XT

i , the best-ﬁtting subspace to the Xi is
spanned by the singular vectors V1 and V2 corresponding to the two
largest singular values of M.
by solving the equations [H2×2| − I][V1V2] = 0.

(e) The H2×2 corresponding to the subspace spanned by V1 and V2 is found

(ix) Computing homographies of IP3 from line correspondences.

Consider
computing a 4 × 4 homography H from lines correspondences alone, assuming
the lines are in general position in IP3. There are two questions: how many
correspondences are required?, and how to formulate the algebraic constraints
to obtain a solution for H? It might be thought that four line correspondences
would be sufﬁcient because each line in IP3 has four degrees of freedom, and
thus four lines should provide 4 × 4 = 16 constraints on the 15 degrees of
freedom of H. However, a conﬁguration of four lines is degenerate (see section
4.1.3(p91)) for computing the transformation, as there is a 2D isotropy sub-
group. This is discussed further in [Hartley-94c]. Equations linear in H can be
obtained in the following way:

πT
i

HXj = 0 ,

i = 1, 2, j = 1, 2 ,

where H transfers a line deﬁned by the two points (X1, X2) to a line deﬁned
by the intersection of the two planes (π1, π2). This method was derived in
[Oskarsson-02], where more details are to be found.

5

Algorithm Evaluation and Error Analysis

This chapter describes methods for assessing and quantifying the results of estima-
tion algorithms. Often it is not sufﬁcient to simply have an estimate of a variable or
transformation. Instead some measure of conﬁdence or uncertainty is also required.

Two methods for computing this uncertainty (covariance) are outlined here. The
ﬁrst is based on linear approximations and involves concatenating various Jacobian
expressions. The second is the easier to implement Monte Carlo method.

5.1 Bounds on performance

Once an algorithm has been developed for the estimation of a certain type of trans-
formation it is time to test its performance. This may be done by testing it on real or
on synthetic data. In this section, testing on synthetic data will be considered, and a
methodology for testing will be sketched.

We recall the notational convention:
• A quantity such as x represents a measured image point.
• Estimated quantities are represented by a hat, such as ˆx or ˆH.
• True values are represented by a bar, such as ¯x or ¯H.
Typically, testing will begin with the synthetic generation of a set of image corre-
spondences ¯xi ↔ ¯x(cid:2)
i between two images. The number of such correspondences will
vary. Corresponding points will be chosen in such a way that they correspond via a
given ﬁxed projective transformation H, and the correspondence is exact, in the sense
that ¯x(cid:2)

i = H¯xi precisely, up to machine accuracy.

Next, artiﬁcial Gaussian noise will be added to the image measurements by perturb-
ing both the x- and y-coordinates of the point by a zero-mean Gaussian random vari-
able with known variance. The resulting noisy points are denoted xi and x(cid:2)
i. A suitable
Gaussian random number generator is given in [Press-88]. The estimation algorithm
is then run to compute the estimated quantity. For the 2D projective transformation
problem considered in chapter 4, this means the projective transformation itself, and
also perhaps estimates of the correct original noise-free image points. The algorithm
is then evaluated according to how closely the computed model matches the (noisy)
input data, or alternatively, how closely the estimated model agrees with the original

132

5.1 Bounds on performance

133

noise-free data. This procedure should be carried out many times with different noise
(i.e. a different seed for the random number generator, though each time with the same
noise variance) in order to obtain a statistically meaningful performance evaluation.

5.1.1 Error in one image

To illustrate this, we continue our investigation of the problem of 2D homography es-
timation. For simplicity we consider the case where noise is added to the coordinates
of the second image only. Thus, xi = ¯xi for all i. Let xi ↔ x(cid:2)
i be a set of noisy
matched points between two images, generated from a perfectly matched set of data by
injection of Gaussian noise with variance σ2 in each of the two coordinates of the sec-
ond (primed) image. Let there be n such matched points. From this data, a projective
transformation ˆH is estimated using any one of the algorithms described in chapter 4.
Obviously, the estimated transformation ˆH will not generally map xi to x(cid:2)
i, nor ¯xi to ¯x(cid:2)
precisely, because of the injected noise in the coordinates of x(cid:2)
i
i. The RMS (root-mean-
squared) residual error

res =

d(x(cid:2)

i, ˆx(cid:2)
i)2

(5.1)

(cid:20)

n(cid:7)

i=1

1
2n

(cid:21)

1/2

measures the average difference between the noisy input data (x(cid:2)
i) and the estimated
points ˆx(cid:2)
i = ˆH¯xi. It is therefore appropriately called residual error. It measures how
well the computed transformation matches the input data, and as such is a suitable
quality measure for the estimation procedure.

The value of the residual error is not in itself an absolute measure of the quality of the
solution obtained. For instance, consider the 2D projectivity problem in the case where
the input data consists of just 4 matched points. Since a projective transformation is
deﬁned uniquely and exactly by 4 point correspondences, any reasonable algorithm
will compute an ˆH that matches the points exactly, in the sense that x(cid:2)
i = ˆHxi. This
means that the residual error is zero. One cannot expect any better performance from
an algorithm than this.
i, and not to the original
noise-free data, ¯x(cid:2)
i. In fact, since the difference between the noise-free and the noisy
coordinates has variance σ2, in the minimal four-point case the residual difference be-
tween projected points ˆHxi and the noise-free data ¯x(cid:2)
i also has variance σ2. Thus, in the
case of 4 points, the model ﬁts the noisy input data perfectly (i.e. the residual is zero),
but does not give a very close approximation to the true noise-free values.

Note that ˆH matches the projected points to the input data x(cid:2)

With more than 4 point matches, the value of the residual error will increase. In
fact, intuitively, one expects that as the number of measurements (matched points)
increases, the estimated model should agree more and more closely with the noise-free
true values. Asymptotically, the variance should decrease in inverse proportion to the
number of point matches. At the same time, the residual error will increase.

134

5 Algorithm Evaluation and Error Analysis

P

f

2
2

0
0

-2
-2

2

1

0

-1

-2

-2
-2

0
0

X

2
2

Fig. 5.1. As the values of the parameters P vary, the function image traces out a surface SM through the
true value X.

5.1.2 Error in both images
In the case of error in both images, the residual error is

res =

d(xi, ˆxi)2 +
i are estimated points such that ˆx(cid:2)

i=1

i = ˆHˆxi.

where ˆxi and ˆx(cid:2)

(cid:20)
n(cid:7)

1√
4n

n(cid:7)

i=1

d(x(cid:2)

i, ˆx(cid:2)
i)2

(cid:21)

1/2

(5.2)

5.1.3 Optimal estimators (MLE)
Bounds for estimation performance will be considered in a general framework, and
then specialized to the two cases of error in one or both images. The goal is to derive
formulae for the expected residual error of the Maximum Likelihood Estimate (MLE).
As described previously, minimization of geometric error is equivalent to MLE, and
so the goal of any algorithm implementing minimization of geometric error should be
to achieve the theoretical bound given for the MLE. Another algorithm minimizing a
different cost function (such as algebraic error) can be judged according to how close
it gets to the bound given by the MLE.

A general estimation problem is concerned with a function f from IRM to IRN as
described in section 4.2.7(p101), where IRM is a parameter space, and IRN is a space
of measurements. Consider now a point X ∈ IRN for which there exists a vector of
parameters P ∈ IRM such that f (P) = X (i.e. a point X in the range of f with preimage
P). In the context of 2D projectivities with measurements in the second image only,
this corresponds to a noise-free set of points ¯x(cid:2)
i = H¯xi. The x- and y-components of the
n points ¯x(cid:2)
i, i = 1, . . . , n constitute the N-vector X with N = 2n, and the parameters
of the homography constitute the vector P which may be an 8- or 9-vector depending
on the parametrization of H.

Let X be a measurement vector chosen according to an isotropic Gaussian distribu-
tion with mean the true measurement X and variance N σ2 (this notation means that
each of the N components has variance σ2). As the value of the parameter vector P
varies in a neighbourhood of the point P, the value of the function f (P) traces out a
surface SM in IRN through the point X. This is illustrated in ﬁgure 5.1. The surface SM

5.1 Bounds on performance

135

n

X

X

X

S

Fig. 5.2. Geometry of the errors in measurement space using the tangent plane approximation to SM.

The estimated point (cid:23)X is the closest point on SM to the measured point X. The residual error is the
distance between the measured point X and (cid:23)X. The estimation error is the distance from (cid:23)X to the true

point X.

the point on SM closest to X. The ML estimator is the one that returns this closest point

is given by the range of f. The dimension of the surface as a submanifold of IRN is
equal to d, where d is the number of essential parameters (that is the number of degrees
of freedom, or minimum number of parameters). In the single-image error case, this
equals 8, since the mapping determined by the matrix H is independent of scale.

Now, given a measurement vector X, the maximum likelihood (ML) estimate (cid:23)X is
to X that lies on this surface. Denote this ML estimate by (cid:23)X.
(cid:23)X is the foot of the perpendicular from X onto the tangent plane. The residual error is
the distance from the point X to the estimated value (cid:23)X. Furthermore, the distance from
(cid:23)X to (the unknown) X is the distance from the optimally estimated value to the true

We now assume that in the neighbourhood of X, the surface is essentially planar and
is well approximated by the tangent surface – at least for neighbourhoods around X of
the order of magnitude of noise variance. In this linear approximation, the ML estimate

value as seen in ﬁgure 5.2. Our task is to compute the expected value of these errors.

Computing the expected ML residual error has now been abstracted to a geomet-
ric problem as follows. The total variance of an N-dimensional Gaussian distribution
is the trace of the covariance matrix, that is the sum of variances in each of the ax-
ial directions. This is, of course, unchanged by a change of orthogonal coordinate
frame. For an N-dimensional isotropic Gaussian distribution with independent vari-
ances σ2 in each variable, the total variance is N σ2. Now, given an isotropic Gaussian
random variable deﬁned on IRN with total variance N σ2 and mean the true point X,
we wish to compute the expected distance of the random variable from a dimension
d hyperplane passing through X. The projection of a Gaussian random variable in
IRN onto the d-dimensional tangent plane gives the distribution of the estimation er-
ror (the difference between the estimated value and the true result). Projection onto the

136

5 Algorithm Evaluation and Error Analysis

(N −d)-dimensional normal to the tangent surface gives the distribution of the residual
error.

By a rotation of axes if necessary, one may assume, without loss of generality, that
Integration over the

the tangent surface coincides with the ﬁrst d coordinate axes.
remaining axial directions provides the following result.

Result 5.1. The projection of an isotropic Gaussian distribution deﬁned on IRN with to-
tal variance N σ2 onto a subspace of dimension s is an isotropic Gaussian distribution
with total variance sσ2.

The proof of this is straightforward, and is omitted. We apply this in the two cases
where s = d and s = N − d to obtain the following results.

Result 5.2. Consider an estimation problem where N measurements are to be modelled
by a function depending on a set of d essential parameters. Suppose the measurements
are subject to independent Gaussian noise with standard deviation σ in each measure-
ment variable.

(i) The RMS residual error (distance of the measured from the estimated value)

for the ML estimator is

res = E[(cid:10)(cid:23)X − X(cid:10)2/N ]1/2 = σ(1 − d/N )1/2

(5.3)

(ii) The RMS estimation error (distance of the estimated from the true value) for

the ML estimator is

est = E[(cid:10)(cid:23)X − X(cid:10)2/N ]1/2 = σ(d/N )1/2

where X, (cid:23)X and X are respectively the measured, estimated and true values of the

(5.4)

measurement vector.

Result 5.2 follows directly from result 5.1 by dividing by N to get the variance per
measurement, then taking a square root to get standard deviation, instead of variance.
These values give lower bounds for residual error against which a particular estima-

tion algorithm may be measured.

2D homography – error in one image. For the 2D projectivity estimation problem
considered in this chapter, assuming error in the second image only, we have d = 8 and
N = 2n, where n is the number of point matches. Thus, we have for this problem

res = σ (1 − 4/n)1/2
est = σ (4/n)1/2 .

(5.5)

Graphs of these errors as n varies are shown in ﬁgure 5.3.

5.1 Bounds on performance

137

r
o
r
r
E

 
/
 
l
a
u
d
i
s
e
R

r
o
r
r
E

 
/
 
l
a
u
d
i
s
e
R

Number of Points

a

Number of Points

b

Fig. 5.3. Optimal error when noise is present in (a) one image, and in (b) both images as the number of
points varies. An error level of one pixel is assumed. The descending curve shows the estimation error
est and the ascending curve shows the residual error res.

Error in both images.
In this case, N = 4n and d = 2n + 8. As before, assuming
linearity of the tangent surface in the neighbourhood of the true measurement vector

(cid:23)X, result 5.2 gives the following expected errors.
n − 4
2n

res = σ

(cid:28)
(cid:28)

(cid:29)1/2
(cid:29)1/2

est = σ

.

(5.6)

n + 4

2n

Graphs of these errors as n varies are also shown in Figure 5.3.

√

An interesting observation to be made from this graph is that the asymptotic error
2, and not 0 as in the case of error in one image.
with respect to the true values is σ/
This result is expected, since in effect, one has two measurements of the position of
each point, one in each image, related by the projective transformation. With two
√
measurements of a point the variance in the estimate of the point position decreases
by a factor of
2. By contrast, in the previous case where errors occur in one image
only, one has one exact measurement for each point (i.e. in the ﬁrst image). Thus, as the
transformation H is estimated with greater and greater accuracy, the exact position of the
point in the second image becomes known with uncertainty asymptotically approaching
0.

Mahalanobis distance. The formulae quoted above were derived under the assump-
tion that the error distribution in measurement space was an isotropic Gaussian distri-
bution, meaning that errors in each coordinate were independent. This assumption is
not essential. We may assume any Gaussian distribution of error, with covariance ma-
trix Σ. The formulae of result 5.2 remain true with  being replaced with the expected
Σ/N ]1/2. The standard deviation σ also disappears,

Mahalanobis distance E[(cid:10)(cid:23)X − X(cid:10)2

since it is taken care of by the Mahalanobis distance.

This follows from a simple change of coordinates in the measurement space IRN
In this new coordinate frame,

to make the covariance matrix equal to the identity.
Mahalanobis distance becomes the same as Euclidean distance.

138

5 Algorithm Evaluation and Error Analysis

5.1.4 Determining the correct convergence of an algorithm
The relations given in (5.3) and (5.4) give a simple way of determining correct conver-
gence of an estimation algorithm, without the need to determine the number of degrees
of freedom of the problem. As seen in ﬁgure 5.2, the measurement space corre-
sponding to the model speciﬁed by the parameter vector P forms a surface SM. If near
the noise-free data X the surface is nearly planar, then it may be approximated by its

tangent plane, and the three points (cid:23)X, X and X form a right-angled triangle. In most

estimation problems this assumption of planarity will be very close to correct at the
scale set by typical noise magnitude. In this case, the Pythagorean equality may be
written as

(cid:10)X − X(cid:10)2 = (cid:10)X − (cid:23)X(cid:10)2 + (cid:10)X − (cid:23)X(cid:10)2

(5.7)

In evaluating an algorithm with synthetic data, this equality allows a simple test to see

whether the algorithm has converged to the optimal value. If the estimated value (cid:23)X

satisﬁes this equality, then it is a strong indication that the algorithm has found the
true global minimum. Note that it is unnecessary in applying this test to determine the
number of degrees of freedom of the problem. A few more properties are listed:
• This test can be used to determine on a run-by-run basis whether the algorithm has
succeeded. Thus, with repeated runs, it allows an estimate of the percentage success
rate for the algorithm.
• This test can only be used for synthetic data, or at least data for which the true
measurements X are known.
• The equality (5.7) depends on the assumption that the surface SM consisting of valid
measurements is locally planar. If the equality is not satisﬁed for a particular run of
the estimation algorithm, then this is because the surface is not planar, or (far more
• The test (5.7) is a test for the algorithm ﬁnding the global, not a local solution. If (cid:23)X
likely) because the algorithm is failing to ﬁnd the best solution.
chance if the algorithm ﬁnds the incorrect point (cid:23)X.

settles to a local cost minimum, then the right-hand-side of (5.7) is likely to be much
larger than the left-hand-side. The condition is unlikely to be satisﬁed entirely by

5.2 Covariance of the estimated transformation

In the previous section the ML estimate was considered, and how its expected average
error may be computed. Comparing the achieved residual error or estimation error of
an algorithm against the ML error is a good way of evaluating the performance of a
particular estimation algorithm, since it compares the results of the algorithm against
the best that may be achieved (the optimum estimate) in the absence of any other prior
information.

Nevertheless, the chief concern is how accurately the transformation itself has been
computed. The uncertainty of the estimated transformation depends on many factors,
including the number of points used to compute it, the accuracy of the given point
matches, as well as the conﬁguration of the points in question. To illustrate the impor-
tance of the conﬁguration suppose the points used to compute the transformation are

5.2 Covariance of the estimated transformation

139

close to a degenerate conﬁguration; then the transformation may not be computed with
great accuracy. For instance, if the transformation is computed from a set of points that
lie close to a straight line, then the behaviour of the transformation in the dimension
perpendicular to that line is not accurately determined. Thus, whereas the achievable
residual error and estimation error were seen to be dependent only on the number of
point correspondences and their accuracy, by contrast, the accuracy of the computed
transformation is dependent on the particular points. The uncertainty of the computed
transformation is conveniently captured in the covariance matrix of the transformation.
Since H is a matrix with 9 entries, its covariance matrix will be a 9 × 9 matrix. In this
section it will be seen how this covariance matrix may be computed.

5.2.1 Forward propagation of covariance
The covariance matrix behaves in a pleasantly simple manner under afﬁne transforma-
tions, as described in the following theorem.

Result 5.3. Let v be a random vector in IRM with mean ¯v and covariance matrix Σ, and
suppose that f : IRM → IRN is an afﬁne mapping deﬁned by f (v) = f (¯v) + A(v − ¯v).
Then f (v) is a random variable with mean f (¯v) and covariance matrix AΣAT.

Note that it is not assumed that A is a square matrix. Instead of giving a proof of this
theorem, we give an example.

(cid:17)

Example 5.4. Let x and y be independent random variables with mean 0 and standard
deviations of 1 and 2 respectively. What are the mean and standard deviation of x
=
f (x, y) = 3x + 2y − 7?
(cid:18)
= f (0, 0) = −7. Next we compute the variance of x
(cid:2)
The mean is ¯x
1 0
and A is the matrix [3 2]. Thus, the variance of x
0 4

is the matrix
Thus 3x + 2y − 7 has standard deviation 5.
= 3x−2y. Find the covariance matrix of (x
(cid:18)
(cid:18)
Example 5.5. Let x
given that x and y have the same distribution as before.

. In this case, Σ
(cid:2)
is AΣAT = 25.
(cid:2)
(cid:2)

= 3x+2y and y

(cid:17)

(cid:17)

, y

),

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

25 −7
−7 25

3
2
3 −2

(cid:2)

(cid:2)

(cid:2)

(cid:2)

y

and y

] = −7.

. One computes AΣAT =

In this case, the matrix A =

have variance 25 (standard deviation 5), whereas x

one sees that both x
are negatively correlated, with covariance E[x

. Thus,
(cid:2)
(cid:2)
and y
(cid:2)
If v is a random vector in IRM and f : IRM → IRN is a
Non-linear propagation.
non-linear function acting on v, then we may compute an approximation to the mean
and covariance of f (v) by assuming that f is approximately afﬁne in the vicinity of the
mean of the distribution. The afﬁne approximation to f is f (v) ≈ f (¯v) + J(v − ¯v),
where J is the partial derivative (Jacobian) matrix ∂f /∂v evaluated at ¯v. Note that J
has dimension N × M. Then we have the following result.
Result 5.6. Let v be a random vector in IRM with mean ¯v and covariance matrix Σ,

140

5 Algorithm Evaluation and Error Analysis

and let f : IRM → IRN be differentiable in a neighbourhood of ¯v. Then up to a ﬁrst-
order approximation, f (v) is a random variable with mean f (¯v) and covariance JΣJT,
where J is the Jacobian matrix of f, evaluated at ¯v.

The extent to which this result gives a good approximation to the actual mean and
variance of f (¯v) depends on how closely the function f is approximated by a linear
function in a region about ¯v commensurate in size with the support of the probability
distribution of v.
Example 5.7. Let x = (x, y)T be a Gaussian random vector with mean (0, 0)T and
= f (x, y) =x 2 + 3x − 2y + 5. Then one may
covariance matrix σ2diag(1, 4). Let x
compute the true values of the mean and standard deviation of f (x, y) according to the
formulae

(cid:2)

! ! ∞
! ! ∞
−∞ P (x, y)f (x, y)dxdy
−∞ P (x, y)(f (x, y) − ¯x

(cid:2)

¯x

=

σ2
x(cid:1) =

(cid:2)

)2dxdy

where

P (x, y) =

−(x2+y2/4)/2σ2

1
4πσ2 e

is the Gaussian probability distribution (A2.1–p565). One obtains

(cid:2)

= 5 + σ2

¯x
σ2
x(cid:1) = 25σ2 + 2σ4.

Applying the approximation given by result 5.6, and noting that J = [3 − 2], we ﬁnd
that the estimated values are

(cid:17)

(cid:18)

= 5

(cid:2)
¯x
x(cid:1) = σ2[3 − 2]
σ2

1

4

[3 − 2]T = 25σ2.

Thus, as long as σ is small, this is a close approximation to the correct values of the
mean and variance of x
. The following table shows the true and approximated values
for the mean and standard deviation of f (x, y) for two different values of σ.

(cid:2)

σ = 0.25
(cid:2)
σx(cid:1)

¯x

(cid:2)

σ = 0.5
σx(cid:1)

¯x

estimate
true

5.0000
5.0625

1.25000
1.25312

5.00
5.25

2.5000
2.5249

For reference, in the case σ = 0.25, one sees that as long as |x| < 2σ (about 95%
of the total distribution) the value f (x, y) = x2 + 3x − 2y + 5 differs from its linear
approximation 3x − 2y + 5 by no more than x2 < 0.25.
(cid:2)

5.2 Covariance of the estimated transformation

141

Example 5.8. More generally, assuming that x and y are independent zero-mean Gaus-
sian random variables, one may compute that for the function f (x, y) = ax2 + bxy +
cy2 + dx + ey + f,

mean = aσ2

variance = 2a2σ4

x + cσ2

y + f
x + b2σ2
xσ2

y + 2c2σ4

y + d2σ2

x + e2σ2
y

which are close to the estimated values mean = f, variance = d2σ2
σx and σy are small.

x + e2σ2

y as long as
(cid:2)

5.2.2 Backward propagation of covariance
The material in this and the following section 5.2.3 is more advanced. The examples in
section 5.2.4 show the straightforward application of the results of these sections, and
can be read ﬁrst.

Consider a differentiable mapping f from a “parameter space”, IRM to a “measure-
ment space” IRN , and let a Gaussian probability distribution be deﬁned on IRN with
covariance matrix Σ. Let SM be the image of the mapping f. We assume that M < N
and that SM has the same dimension M as the parameter space IRM. Thus we are
not considering the over-parametrized case at present. A vector P in IRM represents
a parametrization of the point f (P) on SM. Finding the point on SM closest in Maha-
lanobis distance to a given point X in IRN deﬁnes a map from IRN to the surface SM.
We call this mapping η : IRN → SM. Now, f is by assumption invertible on the surface
SM, and we deﬁne f
By composing the map η : IRN → SM and f
−1 : SM → IRM we have a mapping
parameters P corresponding to the ML estimate (cid:23)X. In principle we may propagate the
−1 ◦ η : IRN → IRM. This mapping assigns to a measurement vector X, the set of

−1 : SM → IRM to be the inverse function.

f

covariance of the probability distribution in the measurement space IRN to compute a
covariance matrix for the set of parameters P corresponding to ML estimation. Our
goal is to apply result 5.3 or result 5.6.

We consider ﬁrst the case where the mapping f is an afﬁne mapping from IRM into
−1 ◦ η is also an afﬁne mapping, and
IRN. We will show next that the mapping f
compute the covariance of the estimated parameters (cid:23)P = f
−1 ◦ η, thereby allowing us to apply result 5.3 to
a speciﬁc form will be given for f
Since f is afﬁne, we may write f (P) = f (P) +J (P − P), where f (P) = X is the
mean of the probability distribution on IRN . Since we are assuming that the surface
a measurement vector X, the ML estimate (cid:23)X minimizes (cid:10)X − (cid:23)X(cid:10)Σ = (cid:10)X − f ((cid:23)P)(cid:10)Σ.
SM = f (IRM ) has dimension M, the rank of J is equal to its column dimension. Given
Thus, we seek (cid:23)P to minimize this latter quantity. However,

−1 ◦ η(X).

and this is minimized (see (A5.2–p591) in section A5.2.1(p591)) when

(cid:10)X − f ((cid:23)P)(cid:10)Σ = (cid:10)(X − X) − J((cid:23)P − P)(cid:10)Σ
((cid:23)P − P) = (JTΣ−1J)

−1JTΣ−1(X − X) .

5 Algorithm Evaluation and Error Analysis

142
Writing P = f

f

−1X and (cid:23)P = f
−1 ◦ η(X) = (cid:23)P

−1(cid:23)X, we see that
= (JTΣ−1J)
= (JTΣ−1J)

This shows that f

result 5.3, we see that the covariance matrix for (cid:23)P is
−1 ◦ η is afﬁne and (JTΣ−1J)
#
"
(JTΣ−1J)

(JTΣ−1J)

−1JTΣ−1

−1JTΣ−1

"

Σ

−1(X)
−1 ◦ η(X) .

−1JTΣ−1(X − X) + f
−1JTΣ−1(X − X) + f
#T

−1JTΣ−1 is its linear part. Applying

= (JTΣ−1J)
= (JTΣ−1J)

−1JTΣ−1ΣΣ−1J(JTΣ−1J)
−1,

−1

recalling that Σ is symmetric. We have proved the following theorem.
Result 5.9 Backward transport of covariance – afﬁne case. Let f : IRM → IRN be
an afﬁne mapping of the form f (P) =f (P) +J (P − P), where J has rank M. Let
X be a random variable in IRN with mean X = f (P) and covariance matrix Σ. Let
parameters corresponding to the ML estimate (cid:23)X. Then (cid:23)P = f
−1 ◦ η : IRN → IRM be the mapping that maps a measurement X to the set of
f
−1 ◦ η(X) is a random

variable with mean P and covariance matrix
ΣP = (JTΣ−1

X J)

−1.

(5.8)

In the case where f is not afﬁne, an approximation to the mean and covariance may

be obtained by approximating f by an afﬁne function in the usual way, as follows.
Result 5.10 Backward transport of covariance – non-linear case. Let f : IRM →
IRN be a differentiable mapping and let J be its Jacobian matrix evaluated at a point
P. Suppose that J has rank M. Then f is one-to-one in a neighbourhood of P. Let
X be a random variable in IRN with mean X = f (P) and covariance matrix ΣX. Let
rameters corresponding to the ML estimate (cid:23)X. Then to ﬁrst-order, (cid:23)P = f
−1 ◦ η : IRN → IRM be the mapping that maps a measurement X to the set of pa-
f
−1 ◦ η(X) is a
random variable with mean P and covariance matrix (JTΣ−1

−1.

X J)

5.2.3 Over-parametrization
One may generalize result 5.9 and result 5.10 to the case of redundant sets of parame-
ters – the over-parametrized case. In this case, the mapping f from the parameter space
IRM to measurement space IRN is not locally one-to-one. For instance, in the case of
estimating a 2D homography as discussed in section 4.5(p110) there is a mapping f (P)
where P is a 9-vector representing the entries of the homography matrix H. Since the
homography has only 8 degrees of freedom, the mapping f is not one-to-one. In par-
ticular, for any constant k, the matrix kH represents the same map, and so the image
coordinate vectors f (P) and f (kP) are equal.
In the general case of a mapping f : IRM → IRN the Jacobian matrix J does not
have full rank M, but rather a smaller rank d < M. This rank d is called the number
of essential parameters. The matrix JTΣ−1
X J in this case has dimension M but rank

5.2 Covariance of the estimated transformation

143

1

0.8

0.6
0.4
0.2
0
-1
-1

P

-0.5
-0.5

S

0
0

1

0.5

0

-0.5

f

2
2

0
0

-2
-2

2

1

0

-1

-2

0.5
0.5

1 -1

-2
-2

0
0

X

X

2
2

Fig. 5.4. Back propagation (over-parametrized). Mapping f maps constrained parameter surface to
measurement space. A measurement X is mapped (by a mapping η) to the closest point on the surface
−1 to the parameter space, providing the ML estimate of the parameters. The
f(SP) and then back via f
covariance of X is transferred via f

−1 ◦ η to a covariance of the parameters.

−1, clearly does not hold, since the matrix

d < M. The formula (5.8), ΣP = (JTΣ−1
on the right side is not invertible.

X J)

In fact, it is clear that without any further restriction, the elements of the estimated

vector (cid:23)P may vary without bound, namely through multiplication by an arbitrary con-
stant k. Hence the elements have inﬁnite variance. It is usual to restrict the estimated
homography matrix H or more generally the parameter vector P by imposing some con-
straint. The usual constraint is that (cid:10)P(cid:10) = 1 though other constraints are possible, such
as demanding that the last parameter should equal 1 (see section 4.4.2(p105)). Thus,
the parameter vector P is constrained to lie on a surface in the parameter space IR9, or
generally IRM . In the ﬁrst case the surface (cid:10)P(cid:10) = 1 is the unit sphere in IRM. The
constraint Pm = 1 represents a plane in IRM . In the general case we may assume that
the estimated vector P lies on some submanifold of IRM as in the following theorem.

Result 5.11. Backward transport of covariance – over-parametrized case.
Let
f : IRM → IRN be a differentiable mapping taking a parameter vector P to a mea-
surement vector X. Let SP be a smooth manifold of dimension d embedded in IRM
passing through point P, and such that the map f is one-to-one on the manifold SP in
a neighbourhood of P, mapping SP locally to a manifold f (SP) in IRN . The function f
−1, restricted to the surface f (SP) in a neighbourhood of
has a local inverse, denoted f
X. Let a Gaussian distribution on IRN be deﬁned with mean X and covariance matrix
ΣX and let η : IRN → f (SP) be the mapping that takes a point in IRN to the closest
−1 ◦ η the probability
point on f (SP) with respect to Mahalanobis norm (cid:10) · (cid:10)ΣX. Via f
distribution on IRN with covariance matrix ΣX induces a probability distribution on
IRM with covariance matrix, to ﬁrst-order equal to

ΣP = (JTΣ−1

X J)+A = A(ATJTΣ−1

(5.9)
where A is any m × d matrix whose column vectors span the tangent space to SP at P.
This is illustrated in ﬁgure 5.4. The notation (JTΣ−1
X J)+A, deﬁned by (5.9), is dis-

X JA)

−1AT

cussed further in section A5.2(p590).

144

5 Algorithm Evaluation and Error Analysis

Proof. The proof of result 5.11 is straightforward. Let d be the number of essential
parameters. One deﬁnes a map g : IRd → IRM mapping an open neighbourhood
U in IRd to an open set of SP containing the point P. Then the combined mapping
f ◦ g : IRd → IRN is one-to-one on the neighbourhood U. Let us denote the partial
derivative matrices of f by J and of g by A. The matrix of partial derivatives of f ◦ g is
then JA. Result 5.10 now applies, and one sees that the probability distribution function
with covariance matrix Σ on IRN may be transported backwards to a covariance matrix
(ATJTΣ−1JA)
−1 on IRd. Transporting this forwards again to IRM, applying result 5.6,
we arrive at the covariance matrix A(ATJTΣ−1JA)
−1AT on SP. This matrix, which will
be denoted here by (JTΣ−1J)+A, is related to the pseudo-inverse of (JTΣ−1J) as deﬁned
in section A5.2(p590). The expression (5.9) is not dependent on the particular choice of
the matrix A as long as the column span of A is unchanged. In particular, if A is replaced
by AB for any invertible d × d matrix B, then the value of (5.9) does not change. Thus,
any matrix A whose columns span the tangent space of SP at P will do.

Note that the proof gives a speciﬁc way of computing a matrix A spanning the tangent
space – namely the Jacobian matrix of g. In many instances, as we will see, there are
easier ways of ﬁnding A. Note that the covariance matrix (5.9) is singular. In particular,
it has dimension M and rank d < M. This is because the variance of the estimated
parameter set in directions orthogonal to the constraint surface SP is zero – there can
be no variation in that direction. Note that whereas JTΣ−1J is non-invertible, the d × d
matrix ATJTΣ−1JA has rank d and is invertible.

An important case occurs when the constraint surface is locally orthogonal to the
null-space of the Jacobian matrix. Denote by NL(X) the left null-space of matrix X,
namely the space of all vectors x such that xTX = 0. Then (as shown in section A5.2-
(p590)), the pseudo-inverse X+ is given by

X+ = X+A = A(ATXA)

−1AT

if and only if NL(A) =N L(X). The following result then derives directly from
result 5.11.
Result 5.12. Let f : IRM → IRN be a differentiable mapping taking P to X, and let J
be the Jacobian matrix of f. Let a Gaussian distribution on IRN be deﬁned at X with
−1 ◦ η : IRN → IRM as in result 5.11 be the mapping
covariance matrix ΣX and let f
taking a measurement X to the MLE parameter vector P constrained to lie on a surface
−1 ◦ η induces a distribution on
SP locally orthogonal to the null-space of J. Then f
IRM with covariance matrix, to ﬁrst-order equal to

ΣP = (JTΣ−1

X J)+.

(5.10)

Note that the restriction that P be constrained to lie on a surface locally orthogonal
to the null-space of J is in many cases the natural constraint. For instance, if P is
a homogeneous parameter vector (such as the entries of a homogeneous matrix), the
restriction is satisﬁed for the usual constraint (cid:10)P(cid:10) = 1. In such a case, the constraint
surface is the unit sphere, and the tangent plane at any point is perpendicular to the
parameter vector. On the other hand, since P is a homogeneous vector, the function

5.2 Covariance of the estimated transformation

145

f (P) is invariant to changes of scale, and so J has a null-vector in the radial direction,
thus perpendicular to the constraint surface.

In other cases, it is often not critical what restriction we place on the parameter set
for the purpose of computing the covariance matrix of the parameters. In addition,
since the pseudo-inversion operation is its own inverse, we can retrieve the original
matrix from its pseudo-inverse, according to JTΣ−1
P. One can then compute the
covariance matrix corresponding to any other subspace, according to

X J = Σ+

(JTΣ−1

X J)+A = (Σ+

P)+A

where the columns of A span the constrained subspace of parameter space.

5.2.4 Application and examples
Error in one image. Let us consider the application of this theory to the problem of
ﬁnding the covariance of an estimated 2D homography H. First, we look at the case
where the error is limited to the second image. The 3 × 3 matrix H is represented by
a 9-dimensional parameter vector which will be denoted by h instead of P so as to
remind us that it is made up of the entries of H. The covariance of the estimated ˆh
is a 9 × 9 symmetric matrix. We are given a set of matched points ¯xi ↔ x(cid:2)
i. The
points ¯xi are ﬁxed true values, and the points x(cid:2)
i are considered as random variables
subject to Gaussian noise with variance σ2 in each component, or if desired, with a
more general covariance. The function f : IR9 → IR2n is deﬁned as mapping a 9-
vector h representing a matrix H to the 2n-vector made up of the coordinates of the
points x(cid:2)
i make up a composite vector in IRN , which we
denote by X(cid:2)
. As we have seen, as h varies, the point f (h) traces out an 8-dimensional
surface SP in IR2n. Each point X(cid:2)
i consistent
closest point (cid:23)X
with the ﬁrst-image points ¯xi. Given a vector of measurements X(cid:2)
, one selects the
−1((cid:23)X
(cid:2)
on the surface SP with respect to Mahalanobis distance. The pre-image
), subject to constraint (cid:10)h(cid:10) = 1, represents the estimated homography
ˆh = f
matrix ˆH, estimated using the ML estimator. From the probability distribution of values
of X(cid:2)
one wishes to derive the distribution of the estimated ˆh. The covariance matrix Σh
is given by result 5.12. This covariance matrix corresponds to the constraint (cid:10)h(cid:10) = 1.
Thus, a procedure for computing the covariance matrix of the estimated transforma-

on the surface represents a set of points x(cid:2)

i = H¯xi. The coordinates of x(cid:2)

(cid:2)

tion is as follows.

(i) Estimate the transformation ˆH from the given data.
(ii) Compute the Jacobian matrix Jf = ∂X(cid:2)
(iii) The covariance matrix of the estimated h is given by (5.10): Σh = (JT
f

/∂h, evaluated at ˆh.

Σ−1
X(cid:1) Jf )+.

We investigate the two last steps of this method in slightly more detail.

Computation of the derivative matrix.
J = ∂X(cid:2)
n)T where Ji = ∂x(cid:2)
(JT

the Jacobian matrix
/∂h. This matrix has a natural decomposition into blocks so that J =
i/∂h is given in

i/∂h. A formula for ∂x(cid:2)

Consider ﬁrst

i , . . . ,J T

1 , JT

2 , . . . ,J T

146

(4.21–p129):

5 Algorithm Evaluation and Error Analysis

(cid:17)

˜xT
i
0T ˜xT
i

0T −x
(cid:2)
i˜xT
−y
(cid:2)
i˜xT

i

i

(cid:18)

(5.11)

Ji = ∂x(cid:2)

1
(cid:2)
w
i
i represents the vector (xi, yi, 1).

i/∂h =

/∂h. An important case is when the image measurements x(cid:2)

where ˜xT
Stacking these matrices on top of each other for all points xi gives the derivative
matrix ∂X(cid:2)
i are independent
random vectors. In this case Σ = diag(Σ1, . . . ,Σ n) where each Σi is the 2× 2 covariance
matrix of the i-th measured point x(cid:2)
Σh = (JTΣ−1

i. Then one computes
Σ−1

(cid:20)(cid:7)

X(cid:1) J)+ =

(5.12)

(cid:21)

+

.

Ji

JT
i

i

i

Example 5.13. We consider the simple numerical example of a point correspondence
containing just 4 points as follows:

x1 = (1, 0)T ↔ (1, 0)T = x(cid:2)
x2 = (0, 1)T ↔ (0, 1)T = x(cid:2)
x3 = (−1, 0)T ↔ (−1, 0)T = x(cid:2)
x4 = (0,−1)T ↔ (0,−1)T = x(cid:2)

1

2

3

4

namely, the identity map on the points of a projective basis. We assume that points xi
are known exactly, and points x(cid:2)
i have one pixel standard deviation in each coordinate
direction. This means that the covariance matrix Σx(cid:1)
Obviously, the computed homography will be the identity map. For simplicity we
normalize (scale it) so that it is indeed the identity matrix, and hence (cid:10)H(cid:10)2 = 3 instead
of the usual normalization (cid:10)H(cid:10) = 1. In this case, all the w
(cid:2)
i in (5.11) are equal to 1. The
matrix J is easily computed from (5.11) to equal

i is the identity.

JTJ =

(5.13)

J =

Then




1
0
0
0
0
1
0
0
−1
0
0
0
0 −1
0
0

1
0
0
0
0
1
0
1
0
0
1
0
0
1
0
0 −1
0
0
0
1
0 −1
0

0 −1
0 −1
0
1
0
0
0
0
0
0
0 −1 −1
1
0 −1
1
0
0
0
1
0
0
0
0
0
0 −1
1
1

0 −2
2 0
0 0
0
0
0
0 2
0 0
0
0
0
0
0
0 −2
0 0
4 0
0
0
0
0 0
0 2
0
0
0
0
0
0 −2
0 0
0 0
2
0
0
0 −2
0 0
0 0
0
0
4
0 0 −2 0
0
0
0
2
0
0 −2
0 0
0 0
0
2
0
−2 0
0 0 −2
0
0
0
4

 .
 .

5.2 Covariance of the estimated transformation

147

To take the pseudo-inverse of this matrix, we may use (5.9) where A is a matrix with
columns spanning the tangent plane to the constraint surface. Since H is computed
subject to the condition (cid:10)H(cid:10)2 = 3, which represents a hypersphere, the constraint sur-
face is perpendicular to the vector h corresponding to the computed homography H.
A Householder matrix A (see section A4.1.2(p580)) corresponding to the vector h has
the property that Ah = (0, . . . ,0 , 1)T, so the ﬁrst 8 columns of A (denoted A1)are per-
pendicular to h as required. This allows the pseudo-inverse to be computed exactly
without using SVD. Applying (5.9) the pseudo-inverse is computed to be



5 0 0 0 −4
0 9 0 0
0 0 9 0
0 0 0 9
−4 0 0 0
0 0 0 0
0 0 9 0
0 0 0
0
−1 0 0 0 −1

0
0
0
0 0
9
0 0
0
0 0
0
5 0
0
0 9
0 0 18
0 9
0

 .

0 −1
0
0
0
0
0
0
0 −1
0
9
0
0
0
0 18
2
0
0
(5.14)
(cid:2)

Σh = (JTJ)+A1 = A1(AT

1 (JTJ)A1)

−1AT

1 =

1
18

The diagonals give the individual variances of the entries of H.

This computed covariance is used to assess the accuracy of point transfer in

example 5.14.

5.2.5 Error in both images
In the case of error in both images, computation of the covariance of the transformation
is a bit more complicated. As seen in section 4.2.7(p101), one may deﬁne a set of 2n+8
parameters, where 8 parameters describe the transformation matrix and 2n parameters
ˆxi represent estimates of the points in the ﬁrst image. More conveniently, one may
over-parametrize by using 9 parameters for the transformation H. The Jacobian matrix
naturally splits up into two parts as J = [A | B] where A and B are the derivatives with
respect to the camera parameters and the points xi respectively. Applying (5.10) one
computes

(cid:17)

JTΣ−1

X J =

ATΣ−1
BTΣ−1

X A ATΣ−1
X B
X A BTΣ−1
X B

(cid:18)

.

The pseudo-inverse of this matrix is the covariance of the parameter set and the top-left
block of this pseudo-inverse is the covariance of the entries of H. A detailed discussion
of this is given in section A6.4.1(p608), where it is shown how one can make use of
the block structure of the Jacobian to simplify the computation.
In example 5.13 on estimating the covariance of H from four points in the previous
section, the covariance turns out to be Σh = 2(JTΣ−1
X(cid:1) J)+, namely twice the covariance
computed for noise in one image only. This assumes that points are measured with the
same covariance in both images. This simple relationship between the covariances in
the one and two-image cases does not generally hold.

148

5 Algorithm Evaluation and Error Analysis

5.2.6 Using the covariance matrix in point transfer
Once one has the covariance, one may compute the uncertainty associated with a given
point transfer. Consider a new point x in the ﬁrst image, not used in the computation
of the transformation, H. The corresponding point in the second image is x(cid:2)
= Hx.
However, because of the uncertainty in the estimation of H, the correct location of the
point x(cid:2)
will also have associated uncertainty. One may compute this uncertainty from
the covariance matrix of H.

The covariance matrix for the point x(cid:2)

is given by the formula

Σx(cid:1) = JhΣhJT
h

(5.15)

where Jh = ∂x(cid:2)

/∂h. A formula for ∂x(cid:2)

/∂h is given in (4.21–p129).

If in addition, the point x itself is measured with some uncertainty, then one has

instead

Σx(cid:1) = JhΣhJT

h + JxΣxJT

x

(5.16)

assuming that there is no cross-correlation between x and h, which is reasonable, since
point x is assumed to be a new point not used in the computation of the transformation
H. A formula for the Jacobian matrix Jx = ∂x(cid:2)

/∂x is given in (4.20–p129).

The covariance matrix Σx(cid:1) given by (5.15) is expressed in terms of the covariance
matrix Σh of the transformation H. We have seen that this covariance matrix Σh depends
on the particular constraint used in estimating H, according to (5.9). It may therefore
appear that Σx(cid:1) also depends on the particular method used to constrain H.
It may
however be veriﬁed that these formulae are independent of the particular constraint A
used to compute the covariance matrix ΣP = (JTΣ−1
Example 5.14. To continue example 5.13, let the computed 2D homography H be given
by the identity matrix, with covariance matrix Σh as in (5.14). Consider an arbitrary
point (x, y) mapped to the point x(cid:2)
= Hx. In this case the covariance matrix Σx(cid:1) =
JhΣhJT

h may be computed symbolically to equal

X J)+A.

Σx(cid:1) =

σx(cid:1)x(cid:1) σx(cid:1)y(cid:1)
σx(cid:1)y(cid:1) σy(cid:1)y(cid:1)

=

1
4

2 − x2 + x4 + y2 + x2y2

xy(x2 + y2 − 2)

xy(x2 + y2 − 2)

2 − y2 + y4 + x2 + x2y2

(cid:17)

(cid:18)

(cid:17)

(cid:18)

.

Note that σx(cid:1)x(cid:1) and σy(cid:1)y(cid:1) are even functions of x and y, whereas σx(cid:1)y(cid:1) is an odd func-
tion. This is a consequence of the symmetry about the x and y axes of the point set
used to compute H. Also note that σx(cid:1)x(cid:1) and σy(cid:1)y(cid:1) differ by swapping x and y, which is
a further consequence of the symmetry of the deﬁning point set.

As may be seen, the variance σx(cid:1)x(cid:1) varies as the fourth power of x, and hence the
standard deviation varies as the square. This shows that extrapolating the values of
transformed points x(cid:2)
(cid:19)
= Hx far beyond the set of points used to compute H is not
reliable. More speciﬁcally, the RMS uncertainty of the position of x(cid:2)
is equal to
(σx(cid:1)x(cid:1) + σy(cid:1)y(cid:1))1/2 =
trace(Σx(cid:1)) which one ﬁnds is equal to (1 + (x2 + y2)2)1/2 =
(1 + r4)1/2, where r is the radial distance from the origin. Note the interesting fact that
the RMS error is only dependent on the radial distance. In fact, one may verify that
the probability distribution for point x(cid:2)
, its

depends only on the radial distance of x(cid:2)

5.3 Monte Carlo estimation of covariance

149

8

6

4

2

0.5

1

1.5

2

2.5

3

Fig. 5.5. RMS error in the position of a projected point x(cid:1)
from the
origin. The homography H is computed from 4 evenly spaced points on a unit circle around the origin
with errors in the second image only. The RMS error is proportional to the assumed error in the points
used to compute H, and the vertical axis is calibrated in terms of this assumed error.

as a function of radial distance of x(cid:1)

two principal axes pointing radially and tangentially. Figure 5.5 shows the graph of the
(cid:2)
RMS error in x(cid:2)

as a function of r.

This example has computed the covariance of a transferred point in the minimal case
of four point correspondences. For more than four correspondences, the situation is
not substantially different. Extrapolation beyond the set of points used to compute the
homography is unreliable. In fact, one may show that if H is computed from n points
evenly spaced around a unit circle (instead of 4 as in the computation above) then the
RMS error is equal to σx(cid:1)x(cid:1) + σy(cid:1)y(cid:1) = 4(1 + r4)/n, so the error exhibits the same
quadratic growth.

5.3 Monte Carlo estimation of covariance

The method of estimating covariance discussed in the previous sections has relied on
an assumption of linearity. In other words, it has been assumed that the surface f (h)
is locally ﬂat in the vicinity of the estimated point, at least over a region corresponding
to the approximate extent of the noise distribution. It has also been assumed that the
method of estimation of the transformation H was the Maximum Likelihood Estimate.
If the surface is not entirely ﬂat then the estimate of covariance may be incorrect. In
addition, a particular estimation method may be inferior to the ML estimate, thereby
introducing additional uncertainty in the values of the estimated transformation H.

A general (though expensive) method of getting an estimate of the covariance is
by exhaustive simulation. Assuming that the noise is drawn from a given noise dis-
tribution, one starts with a set of point matches corresponding perfectly to a given
transformation. One then adds noise to the points and computes the corresponding
transformation using the chosen estimation procedure. The covariance of the trans-
formation H or a further transferred point is then computed statistically from multiple
trials with noise drawn from the assumed noise distribution. This is illustrated for the
case of the identity mapping in ﬁgure 5.6.

Both the analytical and the Monte Carlo methods of estimating covariance of the
transformation H may be applied to the estimation of covariance from real data for
which one does not know the true value of H. From the given data, an estimate of
H and the corresponding true values of the points x(cid:2)
i and xi are computed. Then the

150

5 Algorithm Evaluation and Error Analysis

Fig. 5.6. Transfer of a point under the identity mapping for the normalized and unnormalized DLT
algorithm. See also ﬁgure 4.4(p109) for further explanation.

covariance is computed as if the estimated values were the true values of the matched
data points and the transformation. The resulting covariance matrix is assumed to be
the covariance of the true transformation. This identiﬁcation is based on the assumption
that the true values of the data points are sufﬁciently close to the estimated values that
the covariance matrix is essentially unaffected.

5.4 Closure

An extended discussion of bias and variance of estimated parameters is given in
appendix 3(p568).

5.4.1 The literature
The derivations throughout this chapter have been considerably simpliﬁed by only us-
ing ﬁrst-order Taylor expansions, and assuming Gaussian error distributions. Similar
ideas (ML, covariance . . . ) can be developed for other distributions by using the Fisher
Information matrix. Related reading may be found in Kanatani [Kanatani-96], Press et
al. [Press-88], and other statistical textbooks.

Criminisi et al.

[Criminisi-99b] give many examples of the computed covariances
in point transfer as the correspondences used to determine the homography vary in
number and position.

5.4.2 Notes and exercises

(i) Consider the problem of computing a best line ﬁt to a set of 2D points in the
plane using orthogonal regression. Suppose that N points are measured with
independent standard deviations of σ in each coordinate. What is the expected
RMS distance of each point from a ﬁtted line? Answer : σ ((n − 2)/n)1/2.

(ii) (Harder) : In section 18.5.2(p450) a method is given for computing a projective
reconstruction from a set of n+4 point correspondences across m views, where
4 of the point correspondences are presumed to be known to be from a plane.
Suppose the 4 planar correspondences are known exactly, and the other n image
points are measured with 1 pixel error (each coordinate in each image). What
is the expected residual error of (cid:10)xi

− ˆPiXj(cid:10)?

j

Part I

Camera Geometry and Single View

Geometry

The Cyclops, c. 1914 (oil on canvas) by Odilon Redon (1840-1916)

Rijksmuseum Kroller-Muller, Otterlo, Netherlands /Bridgeman Art Library

Outline

This part of the book concentrates on the geometry of a single perspective camera. It
contains three chapters.

Chapter 6 describes the projection of 3D scene space onto a 2D image plane. The
camera mapping is represented by a matrix, and in the case of mapping points it is a
3 × 4 matrix P which maps from homogeneous coordinates of a world point in 3-space
to homogeneous coordinates of the imaged point on the image plane. This matrix has in
general 11 degrees of freedom, and the properties of the camera, such as its centre and
focal length, may be extracted from it. In particular the internal camera parameters,
such as the focal length and aspect ratio, are packaged in a 3 × 3 matrix K which
is obtained from P by a simple decomposition. There are two particularly important
classes of camera matrix: ﬁnite cameras, and cameras with their centre at inﬁnity such
as the afﬁne camera which represents parallel projection.

Chapter 7 describes the estimation of the camera matrix P, given the coordinates
of a set of corresponding world and image points. The chapter also describes how
constraints on the camera may be efﬁciently incorporated into the estimation, and a
method of correcting for radial lens distortion.

Chapter 8 has three main topics. First, it covers the action of a camera on geometric
objects other than ﬁnite points. These include lines, conics, quadrics and points at
inﬁnity. The image of points/lines at inﬁnity are vanishing points/lines. The second
topic is camera calibration, in which the internal parameters K of the camera matrix
are computed, without computing the entire matrix P. In particular the relation of the
internal parameters to the image of the absolute conic is described, and the calibration
of a camera from vanishing points and vanishing lines. The ﬁnal topic is the calibrating
conic. This is a simple geometric device for visualizing camera calibration.

152

6

Camera Models

A camera is a mapping between the 3D world (object space) and a 2D image. The
principal camera of interest in this book is central projection. This chapter develops a
number of camera models which are matrices with particular properties that represent
the camera mapping.

It will be seen that all cameras modelling central projection are specializations of
the general projective camera. The anatomy of this most general camera model is
examined using the tools of projective geometry. It will be seen that geometric entities
of the camera, such as the projection centre and image plane, can be computed quite
simply from its matrix representation. Specializations of the general projective camera
inherit its properties, for example their geometry is computed using the same algebraic
expressions.

The specialized models fall into two major classes – those that model cameras with
a ﬁnite centre, and those that model cameras with centre “at inﬁnity”. Of the cam-
eras at inﬁnity the afﬁne camera is of particular importance because it is the natural
generalization of parallel projection.

This chapter is principally concerned with the projection of points. The action of a

camera on other geometric entities, such as lines, is deferred until chapter 8.

6.1 Finite cameras

In this section we start with the most specialized and simplest camera model, which is
the basic pinhole camera, and then progressively generalize this model through a series
of gradations.

The models we develop are principally designed for CCD like sensors, but are also
applicable to other cameras, for example X-ray images, scanned photographic nega-
tives, scanned photographs from enlarged negatives, etc.

The basic pinhole model. We consider the central projection of points in space onto a
plane. Let the centre of projection be the origin of a Euclidean coordinate system, and
consider the plane Z = f, which is called the image plane or focal plane. Under the
pinhole camera model, a point in space with coordinates X = (X, Y, Z)T is mapped to
the point on the image plane where a line joining the point X to the centre of projection
meets the image plane. This is shown in ﬁgure 6.1. By similar triangles, one quickly

153

154

Y

6 Camera Models

X

x

y

p

x

X

Z

Y

C

principal axis

f

image plane

C

camera
centre

f Y / Z

p

Z

Fig. 6.1. Pinhole camera geometry. C is the camera centre and p the principal point. The camera
centre is here placed at the coordinate origin. Note the image plane is placed in front of the camera
centre.

computes that the point (X, Y, Z)T is mapped to the point (f X/Z, f Y/Z, f )T on the
image plane. Ignoring the ﬁnal image coordinate, we see that

(X, Y, Z)T (cid:6)→ (f X/Z, f Y/Z)T

(6.1)

describes the central projection mapping from world to image coordinates. This is a
mapping from Euclidean 3-space IR3 to Euclidean 2-space IR2.

The centre of projection is called the camera centre. It is also known as the optical
centre. The line from the camera centre perpendicular to the image plane is called the
principal axis or principal ray of the camera, and the point where the principal axis
meets the image plane is called the principal point. The plane through the camera
centre parallel to the image plane is called the principal plane of the camera.

Central projection using homogeneous coordinates. If the world and image points
are represented by homogeneous vectors, then central projection is very simply ex-
pressed as a linear mapping between their homogeneous coordinates.
In particular,
(6.1) may be written in terms of matrix multiplication as

 X

Y
Z
1

 (cid:6)→

 f X

f Y
Z

 =

 f

.



 X

Y
Z
1

f

0
0
1 0

(6.2)

The matrix in this expression may be written as diag(f, f, 1)[I | 0] where
diag(f, f, 1) is a diagonal matrix and [I | 0] represents a matrix divided up into a 3 × 3
block (the identity matrix) plus a column vector, here the zero vector.

We now introduce the notation X for the world point represented by the homoge-
neous 4-vector (X, Y, Z, 1)T, x for the image point represented by a homogeneous 3-
vector, and P for the 3×4 homogeneous camera projection matrix. Then (6.2) is written
compactly as

x = PX

which deﬁnes the camera matrix for the pinhole model of central projection as

P = diag(f, f, 1) [I | 0].

6.1 Finite cameras

155

y
cam

p

x cam

y
0
y

x

x

0

Fig. 6.2. Image (x, y) and camera (xcam, ycam) coordinate systems.

Principal point offset. The expression (6.1) assumed that the origin of coordinates in
the image plane is at the principal point. In practice, it may not be, so that in general
there is a mapping

(X, Y, Z)T (cid:6)→ (f X/Z + px, f Y/Z + py)T

where (px, py)T are the coordinates of the principal point. See ﬁgure 6.2. This equation
may be expressed conveniently in homogeneous coordinates as

 (cid:6)→

 X

Y
Z
1

 f

f Y + Zpy

 =
 f X + Zpx
 f

K =

Z

px
f py
1

.



 X

Y
Z
1

px 0
f py 0
0

1



(6.3)

(6.4)

(6.5)

Now, writing

then (6.3) has the concise form

x = K[I | 0]Xcam.

The matrix K is called the camera calibration matrix.
In (6.5) we have written
(X, Y, Z, 1)T as Xcam to emphasize that the camera is assumed to be located at the
origin of a Euclidean coordinate system with the principal axis of the camera pointing
straight down the Z-axis, and the point Xcam is expressed in this coordinate system.
Such a coordinate system may be called the camera coordinate frame.

Camera rotation and translation. In general, points in space will be expressed in
terms of a different Euclidean coordinate frame, known as the world coordinate frame.
The two coordinate frames are related via a rotation and a translation. See ﬁgure 6.3.

If (cid:22)X is an inhomogeneous 3-vector representing the coordinates of a point in the world
coordinate frame, and (cid:22)Xcam represents the same point in the camera coordinate frame,
then we may write (cid:22)Xcam = R((cid:22)X−(cid:22)C), where (cid:22)C represents the coordinates of the camera

Xcam =

0

X.

(6.6)

(cid:18)

R −R(cid:22)C

0

1

(cid:17)

R −R(cid:22)C

(cid:17)

 =
(cid:18) X
x = KR[I | −(cid:22)C]X

Y
Z
1

1

156

6 Camera Models

Y

cam

C

Xcam

Zcam

R, t

Z

O

X

Y

Fig. 6.3. The Euclidean transformation between the world and camera coordinate frames.

centre in the world coordinate frame, and R is a 3 × 3 rotation matrix representing the
orientation of the camera coordinate frame. This equation may be written in homoge-
neous coordinates as

Putting this together with (6.5) leads to the formula

(6.7)

contained in K are called the internal camera parameters, or the internal orientation

where X is now in a world coordinate frame. This is the general mapping given by a

pinhole camera. One sees that a general pinhole camera, P = KR[I | −(cid:22)C], has 9 degrees
of freedom: 3 for K (the elements f, px, py), 3 for R, and 3 for (cid:22)C. The parameters
of the camera. The parameters of R and (cid:22)C which relate the camera orientation and
the world to image transformation as (cid:22)Xcam = R(cid:22)X + t. In this case the camera matrix is
where from (6.7) t = −R(cid:22)C.

position to a world coordinate system are called the external parameters or the exterior
orientation.

It is often convenient not to make the camera centre explicit, and instead to represent

P = K[R | t]

simply

(6.8)

CCD cameras. The pinhole camera model just derived assumes that the image coor-
dinates are Euclidean coordinates having equal scales in both axial directions. In the
case of CCD cameras, there is the additional possibility of having non-square pixels. If
image coordinates are measured in pixels, then this has the extra effect of introducing
unequal scale factors in each direction. In particular if the number of pixels per unit



 .

 αx

K =

x0
αy y0
1

 αx

K =

s
x0
αy y0
1

P = KR[I | −(cid:22)C]

(6.9)

(6.10)

(6.11)

6.1 Finite cameras

157

distance in image coordinates are mx and my in the x and y directions, then the trans-
formation from world coordinates to pixel coordinates is obtained by multiplying (6.4)
on the left by an extra factor diag(mx, my, 1). Thus, the general form of the calibration
matrix of a CCD camera is

where αx = f mx and αy = f my represent the focal length of the camera in terms
of pixel dimensions in the x and y direction respectively. Similarly, ˜x0 = (x0, y0)
is the principal point in terms of pixel dimensions, with coordinates x0 = mxpx and
y0 = mypy. A CCD camera thus has 10 degrees of freedom.

Finite projective camera. For added generality, we can consider a calibration matrix
of the form

The added parameter s is referred to as the skew parameter. The skew parameter
will be zero for most normal cameras. However, in certain unusual instances which are
described in section 6.2.4, it can take non-zero values.

A camera

for which the calibration matrix K is of the form (6.10) will be called a ﬁnite projective
camera. A ﬁnite projective camera has 11 degrees of freedom. This is the same number
of degrees of freedom as a 3 × 4 matrix, deﬁned up to an arbitrary scale.
Note that the left hand 3× 3 submatrix of P, equal to KR, is non-singular. Conversely,
any 3 × 4 matrix P for which the left hand 3 × 3 submatrix is non-singular is the
KR[I | −(cid:22)C]. Indeed, letting M be the left 3 × 3 submatrix of P, one decomposes M as
camera matrix of some ﬁnite projective camera, because P can be decomposed as P =
therefore be written P = M[I | M−1p4] = KR[I | −(cid:22)C] where p4 is the last column of P.
In short
• The set of camera matrices of ﬁnite projective cameras is identical with the set of
homogeneous 3× 4 matrices for which the left hand 3× 3 submatrix is non-singular.

a product M = KR where K is upper-triangular of the form (6.10) and R is a rotation
matrix. This decomposition is essentially the RQ matrix decomposition, described in
section A4.1.1(p579), of which more will be said in section 6.2.4. The matrix P can

General projective cameras. The ﬁnal step in our hierarchy of projective cameras is
to remove the non-singularity restriction on the left hand 3 × 3 submatrix. A general
projective camera is one represented by an arbitrary homogeneous 3× 4 matrix of rank
3. It has 11 degrees of freedom. The rank 3 requirement arises because if the rank is

158

6 Camera Models

Camera centre. The camera centre is the 1-dimensional right null-space C of P, i.e. PC = 0.

(cid:21) Finite camera (M is not singular) C =
(cid:21) Camera at inﬁnity (M is singular) C =

i.e. Md = 0.

(cid:29)

(cid:28) −M−1p4
(cid:28)
(cid:29)

1

d
0

where d is the null 3-vector of M,

Column points. For i = 1, . . . , 3, the column vectors pi are vanishing points in the image
corresponding to the X, Y and Z axes respectively. Column p4 is the image of the
coordinate origin.

Principal plane. The principal plane of the camera is P3, the last row of P.
Axis planes. The planes P1 and P2 (the ﬁrst and second rows of P) represent planes in space
through the camera centre, corresponding to points that map to the image lines x = 0
and y = 0 respectively.

Principal point. The image point x0 = Mm3 is the principal point of the camera, where m3T

is the third row of M.

Principal ray. The principal ray (axis) of the camera is the ray passing through the camera
centre C with direction vector m3T. The principal axis vector v = det(M)m3 is
directed towards the front of the camera.

Table 6.1. Summary of the properties of a projective camera P. The matrix is represented by the block
form P = [M | p4].

less than this then the range of the matrix mapping will be a line or point and not the
whole plane; in other words not a 2D image.

6.2 The projective camera

A general projective camera P maps world points X to image points x according to
x = PX. Building on this mapping we will now dissect the camera model to reveal
how geometric entities, such as the camera centre, are encoded. Some of the properties
that we consider will apply only to ﬁnite projective cameras and their specializations,
whilst others will apply to general cameras. The distinction will be evident from the
context. The derived properties of the camera are summarized in table 6.1.

6.2.1 Camera anatomy
A general projective camera may be decomposed into blocks according to P = [M | p4],
where M is a 3 × 3 matrix. It will be seen that if M is non-singular, then this is a ﬁnite
camera, otherwise it is not.

Camera centre. The matrix P has a 1-dimensional right null-space because its rank
is 3, whereas it has 4 columns. Suppose the null-space is generated by the 4-vector
C, that is PC = 0. It will now be shown that C is the camera centre, represented as a
homogeneous 4-vector.

Consider the line containing C and any other point A in 3-space. Points on this line

may be represented by the join

X(λ) =λ A + (1− λ)C .

6.2 The projective camera

159

C

p
3

p
2

p

1

O

Z

Y

X

Fig. 6.4. The three image points deﬁned by the columns pi, i = 1, . . . , 3, of the projection matrix are
the vanishing points of the directions of the world axes.

Under the mapping x = PX points on this line are projected to
x = PX(λ) =λ PA + (1− λ)PC = λPA

since PC = 0. That is all points on the line are mapped to the same image point PA,
which means that the line must be a ray through the camera centre. It follows that C
is the homogeneous representation of the camera centre, since for all choices of A the
line X(λ) is a ray through the camera centre.

This result is not unexpected since the image point (0, 0, 0)T = PC is not deﬁned,
the case of ﬁnite cameras the result may be established directly, since C = ((cid:22)CT
and the camera centre is the unique point in space for which the image is undeﬁned. In
is clearly the null-vector of P = KR[I | −(cid:22)C]. The result is true even in the case where
, 1)T
the ﬁrst 3× 3 submatrix M of P is singular. In this singular case, though, the null-vector
has the form C = (dT, 0)T where Md = 0. The camera centre is then a point at inﬁnity.
Camera models of this class are discussed in section 6.3.

Column vectors. The columns of the projective camera are 3-vectors which have a
geometric meaning as particular image points. With the notation that the columns of P
are pi, i = 1, . . . ,4 , then p1, p2, p3 are the vanishing points of the world coordinate X,
Y and Z axes respectively. This follows because these points are the images of the axes’
directions. For example the x-axis has direction D = (1, 0, 0, 0)T, which is imaged at
p1 = PD. See ﬁgure 6.4. The column p4 is the image of the world origin.

Row vectors. The rows of the projective camera (6.12) are 4-vectors which may be
interpreted geometrically as particular world planes. These planes are examined next.
We introduce the notation that the rows of P are PiT so that

 p11 p12 p13 p14

p21 p22 p23 p24
p31 p32 p33 p34

 =

P =

 .

 P1T

P2T
P3T

(6.12)

160

6 Camera Models

y

x

2

P

P 3

y

x

principal plane

Fig. 6.5. Two of the three planes deﬁned by the rows of the projection matrix.

The principal plane. The principal plane is the plane through the camera centre par-
allel to the image plane. It consists of the set of points X which are imaged on the line
at inﬁnity of the image. Explicitly, PX = (x, y, 0)T. Thus a point lies on the principal
plane of the camera if and only if P3TX = 0. In other words, P3 is the vector repre-
senting the principal plane of the camera. If C is the camera centre, then PC = 0, and
so in particular P3TC = 0. That is C lies on the principal plane of the camera.

Axis planes. Consider the set of points X on the plane P1. This set satisﬁes P1TX = 0,
and so is imaged at PX = (0, y, w)T which are points on the image y-axis. Again it
follows from PC = 0 that P1TC = 0 and so C lies also on the plane P1. Consequently
the plane P1 is deﬁned by the camera centre and the line x = 0 in the image. Similarly
the plane P2 is deﬁned by the camera centre and the line y = 0.

Unlike the principal plane P3, the axis planes P1 and P2 are dependent on the image
x- and y-axes, i.e. on the choice of the image coordinate system. Thus they are less
tightly coupled to the natural camera geometry than the principal plane. In particular
the line of intersection of the planes P1 and P2 is a line joining the camera centre and
image origin, i.e. the back-projection of the image origin. This line will not coincide
in general with the camera principal axis. The planes arising from Pi are illustrated
in ﬁgure 6.5.

The camera centre C lies on all three planes, and since these planes are distinct (as
the P matrix has rank 3) it must lie on their intersection. Algebraically, the condition
for the centre to lie on all three planes is PC = 0 which is the original equation for the
camera centre given above.

The principal point. The principal axis is the line passing through the camera centre
C, with direction perpendicular to the principal plane P3. The axis intersects the image
plane at the principal point. We may determine this point as follows. In general, the
normal to a plane π = (π1, π2, π3, π4)T is the vector (π1, π2, π3)T. This may alterna-
tively be represented by a point (π1, π2, π3, 0)T on the plane at inﬁnity. In the case of
the principal plane P3 of the camera, this point is (p31, p32, p33, 0)T, which we denote

(cid:23)P3. Projecting that point using the camera matrix P gives the principal point of the

camera P(cid:23)P3. Note that only the left hand 3 × 3 part of P = [M | p4] is involved in this

6.2 The projective camera

161

formula. In fact the principal point is computed as x0 = Mm3 where m3T is the third
row of M.

The principal axis vector. Although any point X not on the principal plane may be
mapped to an image point according to x = PX, in reality only half the points in space,
those that lie in front of the camera, may be seen in an image. Let P be written as
P = [M | p4]. It has just been seen that the vector m3 points in the direction of the
principal axis. We would like to deﬁne this vector in such a way that it points in the
direction towards the front of the camera (the positive direction). Note however that P
is only deﬁned up to sign. This leaves an ambiguity as to whether m3 or −m3 points
in the positive direction. We now proceed to resolve this ambiguity.

We start by considering coordinates with respect to the camera coordinate frame.
According to (6.5), the equation for projection of a 3D point to a point in the image
is given by x = PcamXcam = K[I | 0]Xcam, where Xcam is the 3D point expressed in
camera coordinates. In this case observe that the vector v = det(M)m3 = (0, 0, 1)T
points towards the front of the camera in the direction of the principal axis, irrespective
of the scaling of Pcam. For example, if Pcam → kPcam then v → k4v which has the
If the 3D point is expressed in world coordinates then P = kK[R | −R(cid:22)C] = [M | p4],
same direction.

where M = kKR. Since det(R) > 0 the vector v = det(M)m3 is again unaffected by
scaling. In summary,
• v = det(M)m3 is a vector in the direction of the principal axis, directed towards the
front of the camera.

6.2.2 Action of a projective camera on points
Forward projection. As we have already seen, a general projective camera maps a
point in space X to an image point according to the mapping x = PX. Points D =
(dT, 0)T on the plane at inﬁnity represent vanishing points. Such points map to

and thus are only affected by M, the ﬁrst 3 × 3 submatrix of P.

x = PD = [M | p4]D = Md

Back-projection of points to rays. Given a point x in an image, we next determine
the set of points in space that map to this point. This set will constitute a ray in space
passing through the camera centre. The form of the ray may be speciﬁed in several
ways, depending on how one wishes to represent a line in 3-space. A Pl¨ucker represen-
tation is postponed until section 8.1.2(p196). Here the line is represented as the join of
two points.

We know two points on the ray. These are the camera centre C (where PC = 0)
and the point P+x, where P+ is the pseudo-inverse of P. The pseudo-inverse of P is the
−1, for which PP+ = I (see section A5.2(p590)). Point P+x lies
matrix P+ = PT(PPT)

162

6 Camera Models

X

C

m 3

X . m 3

Fig. 6.6. If the camera matrix P = [M | p4] is normalized so that (cid:10)m3(cid:10) = 1 and det M > 0, and
x = w(x, y, 1)T = PX, where X = (X, Y, Z, 1)T, then w is the depth of the point X from the camera
centre in the direction of the principal ray of the camera.

on the ray because it projects to x, since P(P+x) =I x = x. Then the ray is the line
formed by the join of these two points

X(λ) =P +x + λC.

(6.13)

P = [M | p4], the camera centre is given by (cid:22)C = −M−1p4. An image point x back-

In the case of ﬁnite cameras an alternative expression can be developed. Writing
projects to a ray intersecting the plane at inﬁnity at the point D = ((M−1x)T, 0)T, and
D provides a second point on the ray. Again writing the line as the join of two points
on the ray,

(cid:20)

M−1x

0

(cid:21)

+

(cid:20) −M−1p4

(cid:21)

=

1

(cid:20)

X(µ) =µ

(cid:21)

M−1(µx − p4)

1

.

(6.14)

6.2.3 Depth of points
Next, we consider the distance a point lies in front of or behind the principal
X = (X, Y, Z, 1)T = ((cid:22)XT
plane of the camera. Consider a camera matrix P = [M | p4], projecting a point
C = ((cid:22)C, 1)T be the camera centre. Then w = P3TX = P3T(X − C) since PC = 0 for
, 1)T in 3-space to the image point x = w(x, y, 1)T = PX. Let
the camera centre C. However, P3T(X − C) = m3T((cid:22)X − (cid:22)C) where m3 is the principal
ray direction, so w = m3T((cid:22)X−(cid:22)C) can be interpreted as the dot product of the ray from
the camera centre to the point X, with the principal ray direction. If the camera matrix
is normalized so that det M > 0 and (cid:10)m3(cid:10) = 1, then m3 is a unit vector pointing in the
positive axial direction. Then w may be interpreted as the depth of the point X from the
camera centre C in the direction of the principal ray. This is illustrated in ﬁgure 6.6.

Any camera matrix may be normalized by multiplying it by an appropriate factor.
However, to avoid having always to deal with normalized camera matrices, the depth
of a point may be computed as follows:
Result 6.1. Let X = (X, Y, Z, T)T be a 3D point and P = [M | p4] be a camera matrix
for a ﬁnite camera. Suppose P(X, Y, Z, T)T = w(x, y, 1)T. Then

depth(X; P) =

sign(det M)w

T(cid:10)m3(cid:10)

(6.15)

6.2 The projective camera

163

is the depth of the point X in front of the principal plane of the camera.
This formula is an effective way to determine if a point X is in front of the camera. One
veriﬁes that the value of depth(X; P) is unchanged if either the point X or the camera
matrix P is multiplied by a constant factor k. Thus, depth(X; P) is independent of the
particular homogeneous representation of X and P.

6.2.4 Decomposition of the camera matrix
Let P be a camera matrix representing a general projective camera. We wish to ﬁnd the
camera centre, the orientation of the camera and the internal parameters of the camera
from P.

Finding the camera centre. The camera centre C is the point for which PC = 0.
Numerically this right null-vector may be obtained from the SVD of P, see section
A4.4(p585). Algebraically, the centre C = (X, Y, Z, T)T may be obtained as (see (3.5–
p67))

X = det([p2, p3, p4]) Y = − det([p1, p3, p4])
Z = det([p1, p2, p4]) T = − det([p1, p2, p3]).

Finding the camera orientation and internal parameters. In the case of a ﬁnite
camera, according to (6.11),

P = [M | −M(cid:22)C] = K[R | −R(cid:22)C].

We may easily ﬁnd both K and R by decomposing M as M = KR using the RQ-
decomposition. This decomposition into the product of an upper-triangular and orthog-
onal matrix is described in section A4.1.1(p579). The matrix R gives the orientation of
the camera, whereas K is the calibration matrix. The ambiguity in the decomposition is
removed by requiring that K have positive diagonal entries.

The matrix K has the form (6.10)

K =

 αx

s

x0
0 αy y0
0
1

0



where
• αx is the scale factor in the x-coordinate direction,
• αy is the scale factor in the y-coordinate direction,
• s is the skew,
• (x0, y0)T are the coordinates of the principal point.
The aspect ratio is αy/αx.
Example 6.2. The camera matrix



P =

3.39645 e+2 2.77744 e+2 −1.44946 e+6
3.53553 e+2
−1.03528 e+2
2.33212 e+1 4.59607 e+2 −6.32525 e+5
7.07107 e−1 −3.53553 e−1 6.12372 e−1 −9.18559 e+2



164

6 Camera Models

with P = [M | −M(cid:22)C], has centre (cid:22)C = (1000.0, 2000.0, 1500.0)T, and the matrix M
 .

 468.2

decomposes as





M = KR =

91.2 300.0
427.2 200.0
1.0

0.41380
0.90915 0.04708
−0.57338
0.22011 0.78917
0.70711 −0.35355 0.61237

(cid:2)
When is s (cid:5)= 0? As was shown in section 6.1 a true CCD camera has only four internal
camera parameters, since generally s = 0. If s (cid:5)= 0 then this can be interpreted as
a skewing of the pixel elements in the CCD array so that the x- and y-axes are not
perpendicular. This is admittedly very unlikely to happen.

In realistic circumstances a non-zero skew might arise as a result of taking an image
of an image, for example if a photograph is re-photographed, or a negative is enlarged.
Consider enlarging an image taken by a pinhole camera (such as an ordinary ﬁlm cam-
era) where the axis of the magnifying lens is not perpendicular to the ﬁlm plane or the
enlarged image plane.

The most severe distortion that can arise from this “picture of a picture” process is a
planar homography. Suppose the original (ﬁnite) camera is represented by the matrix P,
then the camera representing the picture of a picture is HP, where H is the homography
matrix. Since H is non-singular, the left 3 × 3 submatrix of HP is non-singular and can
be decomposed as the product KR – and K need not have s = 0. Note however that the
K and R are no longer the calibration matrix and orientation of the original camera.

On the other hand, one veriﬁes that the process of taking a picture of a picture does
not change the apparent camera centre. Indeed, since H is non-singular, HPC = 0 if and
only if PC = 0.

Where is the decomposition required? If the camera P is constructed from (6.11)
then the parameters are known and a decomposition is clearly unnecessary. So the
question arises – where would one obtain a camera for which the decomposition is not
known? In fact cameras will be computed in myriad ways throughout this book and
decomposing an unknown camera is a frequently used tool in practice. For example
cameras can be computed directly by calibration – where the camera is computed from
a set of world to image correspondences (chapter 7) – and indirectly by computing a
multiple view relation (such as the fundamental matrix or trifocal tensor) and subse-
quently computing projection matrices from this relation.

A note on coordinate orientation.
In the derivation of the camera model and its
parametrization (6.10) it is assumed that the coordinate systems used in both the image
and the 3D world are right handed systems, as shown in ﬁgure 6.1(p154). However,
a common practice in measuring image coordinates is that the y-coordinate increases
in the downwards direction, thus deﬁning a left handed coordinate system, contrary to
ﬁgure 6.1(p154). A recommended practice in this case is to negate the y-coordinate of
the image point so that the coordinate system again becomes right handed. However, if

6.2 The projective camera

165

the image coordinate system is left handed, then the consequences are not grave. The
relationship between world and image coordinates is still expressed by a 3 × 4 camera
matrix. Decomposition of this camera matrix according to (6.11) with K of the form
(6.10) is still possible with αx and αy positive. The difference is that R now represents
the orientation of the camera with respect to the negative Z-axis. In addition, the depth
of points given by (6.15) will be negative instead of positive for points in front of the
camera. If this is borne in mind then it is permissible to use left handed coordinates in
the image.

6.2.5 Euclidean vs projective spaces
The development of the sections to this point has implicitly assumed that the world
and image coordinate systems are Euclidean. Ideas have been borrowed from projec-
tive geometry (such as directions corresponding to points on π∞) and the convenient
notation of homogeneous coordinates has allowed central projection to be represented
linearly.

In subsequent chapters of the book we will go further and use a projective coordinate
frame. This is easily achieved, for suppose the world coordinate frame is projective;
then the transformation between the camera and world coordinate frame (6.6) is again
represented by a 4 × 4 homogeneous matrix, Xcam = HX, and the resulting map from
projective 3-space IP3 to the image is still represented by a 3 × 4 matrix P with rank 3.
In fact, at its most general the projective camera is a map from IP3 to IP2, and covers
the composed effects of a projective transformation of 3-space, a projection from 3-
space to an image, and a projective transformation of the image. This follows simply
by concatenating the matrices representing these mappings:

 1 0 0 0

0 1 0 0
0 0 1 0

 [4 × 4 homography]

P = [3 × 3 homography]

which results in a 3 × 4 matrix.
However, it is important to remember that cameras are Euclidean devices and simply
because we have a projective model of a camera it does not mean that we should eschew
notions of Euclidean geometry.
Euclidean and afﬁne interpretations. Although a (ﬁnite) 3× 4 matrix can always be
decomposed as in section 6.2.4 to obtain a rotation matrix, a calibration matrix K, and
so forth, Euclidean interpretations of the parameters so obtained are only meaningful if
the image and space coordinates are in an appropriate frame. In the decomposition case
a Euclidean frame is required for both image and 3-space. On the other hand, the in-
terpretation of the null-vector of P as the camera centre is valid even if both frames are
projective – the interpretation requires only collinearity, which is a projective notion.
The interpretation of P3 as the principal plane requires at least afﬁne frames for the im-
age and 3-space. Finally, the interpretation of m3 as the principal ray requires an afﬁne
image frame but a Euclidean world frame in order for the concept of orthogonality (to
the principal plane) to be meaningful.

166

6 Camera Models

perspective 

weak perspective

increasing  focal length

increasing distance from camera

Fig. 6.7. As the focal length increases and the distance between the camera and object also increases,
the image remains the same size but perspective effects diminish.

6.3 Cameras at inﬁnity

We now turn to consider cameras with centre lying on the plane at inﬁnity. This means
that the left hand 3 × 3 block of the camera matrix P is singular. The camera centre
may be found from PC = 0 just as with ﬁnite cameras.

Cameras at inﬁnity may be broadly classiﬁed into two different types, afﬁne cameras
and non-afﬁne cameras. We consider ﬁrst of all the afﬁne class of cameras which are
the most important in practice.

Deﬁnition 6.3. An afﬁne camera is one that has a camera matrix P in which the last row
P3T is of the form (0, 0, 0, 1).

It is called an afﬁne camera because points at inﬁnity are mapped to points at inﬁnity.

6.3.1 Afﬁne cameras
Consider what happens as we apply a cinematographic technique of tracking back
while zooming in, in such a way as to keep objects of interest the same size1. This
is illustrated in ﬁgure 6.7. We are going to model this process by taking the limit as
both the focal length and principal axis distance of the camera from the object increase.

In analyzing this technique, we start with a ﬁnite projective camera (6.11). The

1 See ‘Vertigo’ (Dir. Hitchcock, 1958) and ‘Mishima’ (Dir. Schrader, 1985).

camera matrix may be written as

6.3 Cameras at inﬁnity

P0 = KR[I | −(cid:22)C] = K

 r1T −r1T(cid:22)C
r2T −r2T(cid:22)C
r3T −r3T(cid:22)C



167

(6.16)

has orientation denoted by matrix R and internal parameters matrix K of the form given
in (6.10–p157). From section 6.2.1 the principal ray of the camera is in the direction

where riT is the i-th row of the rotation matrix. This camera is located at position (cid:22)C and
of the vector r3, and the value d0 = −r3T(cid:22)C is the distance of the world origin from the
(cid:22)C − tr3. Replacing (cid:22)C in (6.16) by (cid:22)C − tr3 gives the camera matrix at time t:


Now, we consider what happens if the camera centre is moved backwards along the
principal ray at unit speed for a time t, so that the centre of the camera is moved to

 r1T −r1T(cid:22)C
r2T −r2T(cid:22)C

camera centre in the direction of the principal ray.

 r1T −r1T((cid:22)C − tr3)
r2T −r2T((cid:22)C − tr3)
r3T −r3T((cid:22)C − tr3)

 = K

where the terms riTr3 are zero for i = 1, 2 because R is a rotation matrix. The scalar

dt = −r3T(cid:22)C + t is the depth of the world origin with respect to the camera centre in
the direction of the principal ray r3 of the camera. Thus
• The effect of tracking along the principal ray is to replace the (3,4) entry of the matrix
by the depth dt of the camera centre from the world origin.

r3T

dt

Pt = K

(6.17)

Next, we consider zooming such that the camera focal length is increased by a factor
k. This magniﬁes the image by a factor k. It is shown in section 8.4.1(p203) that the
effect of zooming by a factor k is to multiply the calibration matrix K on the right by
diag(k, k, 1).

Now, we combine the effects of tracking and zooming. We suppose that the magni-
ﬁcation factor is k = dt/d0 so that the image size remains ﬁxed. The resulting camera
matrix at time t, derived from (6.17), is

 dt/d0

Pt = K

dt/d0



 r1T −r1T(cid:22)C
r2T −r2T(cid:22)C

1

r3T

−r1T(cid:22)C
−r2T(cid:22)C



r1T
r2T

r3Td0/dt

d0

and one can ignore the factor dt/d0. When t = 0, the camera matrix Pt corresponds
with (6.16). Now, in the limit as dt tends to inﬁnity, this matrix becomes

K

dt

dt
d0

 =
 r1T −r1T(cid:22)C
r2T −r2T(cid:22)C

0T

d0





P∞ = lim

t→∞ Pt = K

(6.18)

which is just the original camera matrix (6.16) with the ﬁrst three entries of the last row
set to zero. From deﬁnition 6.3 P∞ is an instance of an afﬁne camera.

168

6 Camera Models

6.3.2 Error in employing an afﬁne camera
It may be noted that the image of any point on the plane through the world origin
perpendicular to the principal axis direction r3 is unchanged by this combined zooming
and motion. Indeed, such a point may be written as

One then veriﬁes that P0X = PtX = P∞X for all t, since r3T(αr1 + βr2) = 0.

For points not on this plane the images under P0 and P∞ differ, and we will now
investigate the extent of this error. Consider a point X which is at a perpendicular
distance ∆ from this plane. The 3D point can be represented as

(cid:21)

.

(cid:21)

X =

αr1 + βr2

1

(cid:20)

X =

αr1 + βr2 + ∆r3

1

(cid:20)

(cid:17)

and is imaged by the cameras P0 and P∞ at


 ˜x
where ˜x = α − r1T(cid:22)C, ˜y = β − r2T(cid:22)C. Now, writing the calibration matrix as

 xafﬁne = P∞X = K

xproj = P0X = K



d0 + ∆

˜y
d0

˜x
˜y

(cid:18)

,

K =

K2×2 ˜x0
˜0T
1

(cid:21)

where K2×2 is an upper-triangular 2 × 2 matrix, gives

(cid:20)

xproj =

K2×2˜x + (d0 + ∆)˜x0

d0 + ∆

xafﬁne =

(cid:21)

(cid:20)

K2×2˜x + d0˜x0

d0

The image point for P0 is obtained by dehomogenizing, by dividing by the third
element, as ˜xproj = ˜x0 + K2×2˜x/(d0 + ∆), and for P∞ the inhomogeneous image point
is ˜xafﬁne = ˜x0 + K2×2˜x/d0. The relationship between the two points is therefore

˜xafﬁne − ˜x0 =

d0 + ∆

d0

(˜xproj − ˜x0)

which shows that
• The effect of the afﬁne approximation P∞ to the true camera matrix P0 is to move the
image of a point X radially towards or away from the principal point ˜x0 by a factor
equal to (d0 + ∆)/d0 = 1 + ∆/d0.

This is illustrated in ﬁgure 6.8.

169
Afﬁne imaging conditions. From the expressions for ˜xproj and ˜xafﬁne we can deduce
that

6.3 Cameras at inﬁnity

˜xafﬁne − ˜xproj =

(˜xproj − ˜x0)

∆
d0

(6.19)

which shows that the distance between the true perspective image position and the
position obtained using the afﬁne camera approximation P∞ will be small provided:

(i) The depth relief (∆) is small compared to the average depth (d0), and
(ii) The distance of the point from the principal ray is small.

The latter condition is satisﬁed by a small ﬁeld of view. In general, images acquired
using a lens with a longer focal length tend to satisfy these conditions as both the ﬁeld
of view and the depth of ﬁeld are smaller than those obtained by a short focal length
lens with the same CCD array.

For scenes at which there are many points at different depths, the afﬁne camera is
not a good approximation. For instance where the scene contains close foreground as
well as background objects, an afﬁne camera model should not be used. However, a
different afﬁne model can be used for each region in these circumstances.

6.3.3 Decomposition of P∞
The camera matrix (6.18) may be written as
K2×2 ˜x0
ˆ0T
1

P∞ =

(cid:17)

(cid:18)(cid:17)

(cid:18)

ˆt
ˆR
0T d0

where ˆR consists of the two ﬁrst rows of a rotation matrix, ˆt is the vector

(−r1T(cid:22)C,−r2T(cid:22)C)T, and ˆ0 the vector (0, 0)T. The 2 × 2 matrix K2×2 is upper-triangular.

(cid:18)(cid:17)

(cid:18)

(cid:17)

(cid:18)(cid:17)

(cid:18)

d

−1
K2×2 ˜x0
0
ˆ0T
1

ˆt
ˆR
0T 1

P∞ =

K2×2 ˜x0
ˆ0T
1

ˆt
ˆR
0T d0

One quickly veriﬁes that

−1
so we may replace K2×2 by d
0
product gives

K2×2 and assume that d0 = 1. Multiplying out this

(cid:18)(cid:17)

(cid:18)

ˆR
0T

ˆt + K−1
2×2˜x0
1

P∞ =

=

K2×2ˆR K2×2ˆt + ˜x0
ˆ0T
K2×2 K2×2ˆt + ˜x0
ˆ0T

1

1

K2×2 ˆ0
ˆ0T
1

(cid:18)

ˆ0
ˆR
0T 1

.

Thus, making appropriate substitutions for ˆt or ˜x0, we can write the afﬁne camera
matrix in one of the two forms
K2×2 ˆ0
ˆ0T
1

K2×2 ˜x0
ˆ0T
1

ˆ0
ˆR
0T 1

ˆt
ˆR
0T 1

(cid:18)(cid:17)

(cid:18)(cid:17)

P∞ =

(6.20)

(cid:18)

(cid:17)

(cid:18)

.

(cid:17)

=

Consequently, the camera P∞ can be interpreted in terms of these decompositions in

(cid:17)

(cid:17)
(cid:17)

=

(cid:17)

(cid:18)
(cid:18)(cid:17)

=

170

6 Camera Models

∆

X

weak

perspective

C

perspective

f

d

0

Fig. 6.8. Perspective vs weak perspective projection. The action of the weak perspective camera is
equivalent to orthographic projection onto a plane (at Z = d0), followed by perspective projection from
the plane. The difference between the perspective and weak perspective image point depends both on
the distance ∆ of the point X from the plane, and the distance of the point from the principal ray.

one of two ways, either with ˜x0 = 0 or with ˆt = ˆ0. Using the second decomposition
of (6.20), the image of the world origin is P∞(0, 0, 0, 1)T = (˜xT
0 , 1)T. Consequently,
the value of ˜x0 is dependent on the particular choice of world coordinates, and hence
is not an intrinsic property of the camera itself. This means that the camera matrix P∞
does not have a principal point. Therefore, it is preferable to use the ﬁrst decomposition
of P∞ in (6.20), and write

(cid:17)

(cid:18)(cid:17)

(cid:18)

P∞ =

K2×2 ˆ0
ˆ0T
1

ˆt
ˆR
0T 1

(6.21)

where the two matrices represent the internal camera parameters and external camera
parameters of P∞.

(cid:18)

(cid:17)

Parallel projection. In summary the essential differences between P∞ and a ﬁnite
camera are:
• The parallel projection matrix
trix [I | 0] of a ﬁnite camera (6.5–p155).
• The calibration matrix
K2×2 ˆ0
ˆ0T
1
• The principal point is not deﬁned.

replaces K of a ﬁnite camera (6.10–p157) .

replaces the canonical projection ma-

1 0 0 0
0 1 0 0
0 0 0 1

(cid:18)

(cid:17)

6.3.4 A hierarchy of afﬁne cameras
In a similar manner to the development of the ﬁnite projection camera taxonomy
in section 6.1 we can start with the basic operation of parallel projection and build
a hierarchy of camera models representing progressively more general cases of parallel
projection.

6.3 Cameras at inﬁnity

171

Orthographic projection. Consider projection along the Z-axis. This is represented
by a matrix of the form

 1 0 0 0

0 1 0 0
0 0 0 1

 .

P =

t
R
0T 1

(cid:17)
 r1T t1

r2T t2
0T
1

.

(cid:18)
 .

H =

P =

(6.22)

(6.23)

(6.25)

This mapping takes a point (X, Y, Z, 1)T to the image point (X, Y, 1)T, dropping the
Z-coordinate.

For a general orthographic projection mapping, we precede this map by a 3D Eu-

clidean coordinate change of the form

Writing t = (t1, t2, t3)T, we see that a general orthographic camera is of the form

An orthographic camera has ﬁve degrees of freedom, namely the three parameters de-
scribing the rotation matrix R, plus the two offset parameters t1 and t2. An orthographic
projection matrix P = [M | t] is characterized by a matrix M with last row zero, with the
ﬁrst two rows orthogonal and of unit norm, and t3 = 1.

Scaled orthographic projection. A scaled orthographic projection is an orthographic
projection followed by isotropic scaling. Thus, in general, its matrix may be written in
the form

 k

P =

k



 r1T t1

r2T t2
0T
1

 =

 r1T

r2T
0T

1

 .

t1
t2
1/k

It has six degrees of freedom. A scaled orthographic projection matrix P = [M | t] is
characterized by a matrix M with last row zero, and the ﬁrst two rows orthogonal and of
equal norm.

(6.24)

Weak perspective projection. Analogous to a ﬁnite CCD camera, we may consider
the case of a camera at inﬁnity for which the scale factors in the two axial image
directions are not equal. Such a camera has a projection matrix of the form

 αx

P =



 r1T t1

r2T t2
0T
1

 .

αy

1

It has seven degrees of freedom. A weak perspective projection matrix P = [M | t]
is characterized by a matrix M with last row zero, and ﬁrst two rows orthogonal (but
they need not have equal norm as is required in the scaled orthographic case). The
geometric action of this camera is illustrated in ﬁgure 6.8.

172
The afﬁne camera, PA. As has already been seen in the case of P∞, a general camera
matrix of the afﬁne form, and with no restrictions on its elements, may be decomposed
as

6 Camera Models

 αx

PA =



 r1T t1

r2T t2
0T
1

 .

s
αy

1

It has eight degrees of freedom, and may be thought of as the parallel projection version
of the ﬁnite projective camera (6.11–p157).

In full generality an afﬁne camera has the form

 m11 m12 m13

m21 m22 m23
0

0

0

 .

t1
t2
1

PA =

It has eight degrees of freedom corresponding to the eight non-zero and non-unit matrix
elements. We denote the top left 2 × 3 submatrix by M2×3. The sole restriction on the
afﬁne camera is that M2×3 has rank 2. This arises from the requirement that the rank of
P is 3.

The afﬁne camera covers the composed effects of an afﬁne transformation of 3-space,
an orthographic projection from 3-space to an image, and an afﬁne transformation of
the image. This follows simply by concatenating the matrices representing these map-
pings:

PA = [3 × 3 aﬃne]

0 1 0 0
0 0 0 1
which results in a 3 × 4 matrix of the afﬁne form.
Projection under an afﬁne camera is a linear mapping on inhomogeneous coordinates

 1 0 0 0

 [4 × 4 aﬃne]

composed with a translation:

(cid:20)

(cid:21)

(cid:17)

x
y

=

m11 m12 m13
m21 m22 m23

(cid:20)

 +

(cid:21)

(cid:18) X
(cid:22)X + ˜t .

Y
Z

t1
t2

(6.26)

which is written more concisely as

˜x = M2×3

The point ˜t = (t1, t2)T is the image of the world origin.

The camera models of this section are seen to be afﬁne cameras satisfying additional
constraints, thus the afﬁne camera is an abstraction of this hierarchy. For example, in
the case of the weak perspective camera the rows of M2×3 are scalings of rows of a
rotation matrix, and thus are orthogonal.

6.3.5 More properties of the afﬁne camera
The plane at inﬁnity in space is mapped to points at inﬁnity in the image. This is easily
seen by computing PA(X, Y, Z, 0)T = (X, Y, 0)T. Extending the terminology of ﬁnite

6.3 Cameras at inﬁnity

173

projective cameras, we interpret this by saying that the principal plane of the camera is
the plane at inﬁnity. The optical centre, since it lies on the principal plane, must also
lie on the plane at inﬁnity. From this we have

(i) Conversely, any projective camera matrix for which the principal plane is the

plane at inﬁnity is an afﬁne camera matrix.

(ii) Parallel world lines are projected to parallel image lines. This follows because
parallel world lines intersect at the plane at inﬁnity, and this intersection point
is mapped to a point at inﬁnity in the image. Hence the image lines are parallel.
(iii) The vector d satisfying M2×3d = 0 is the direction of parallel projection, and

(cid:20)

(cid:21)

(dT, 0)T the camera centre since PA

= 0.

d
0

Any camera which consists of the composed effects of afﬁne transformations (ei-
ther of space, or of the image) with parallel projection will have the afﬁne form. For
example, para-perspective projection consists of two such mappings: the ﬁrst is par-
allel projection onto a plane π through the centroid and parallel to the image plane.
The direction of parallel projection is the ray joining the centroid to the camera centre.
This parallel projection is followed by an afﬁne transformation (actually a similarity)
between π and the image. Thus a para-perspective camera is an afﬁne camera.

6.3.6 General cameras at inﬁnity
An afﬁne camera is one for which the principal plane is the plane at inﬁnity. As such,
its camera centre lies on the plane at inﬁnity. However, it is possible for the camera
centre to lie on the plane at inﬁnity without the whole principal plane being the plane
at inﬁnity.
A camera centre lies at inﬁnity if P = [M | p4] with M a singular matrix. This is
clearly a weaker condition than insisting that the last row of M is zero, as is the case
for afﬁne cameras. If M is singular, but the last row of M is not zero, then the camera is
not afﬁne, but not a ﬁnite projective camera either. Such a camera is rather a strange
object, however, and will not be treated in detail in this book. We may compare the
properties of afﬁne and non-afﬁne inﬁnite cameras:

Afﬁne camera Non-afﬁne camera

Camera centre on π∞
Principal plane is π∞
Image of points on π∞ on l∞

yes
yes
yes

yes
no

no in general

In both cases the camera centre is the direction of projection. Furthermore, in the case
of an afﬁne camera all non-inﬁnite points are in front of the camera. For a non-afﬁne
camera space is partitioned into two sets of points by the principal plane.

A general camera at inﬁnity could arise from a perspective image of an image pro-
duced by an afﬁne camera. This imaging process corresponds to left-multiplying the

174

6 Camera Models

Line of motion

y

Perspective

axis

Image plane

x

Orthographic

axis

Instantaneous
view plane

Fig. 6.9. Acquisition geometry of a pushbroom camera.

afﬁne camera by a general 3 × 3 matrix representing the planar homography. The re-
sulting 3 × 4 matrix is still a camera at inﬁnity, but it does not have the afﬁne form,
since parallel lines in the world will in general appear as converging lines in the image.

6.4 Other camera models

6.4.1 Pushbroom cameras
The Linear Pushbroom (LP) camera is an abstraction of a type of sensor common in
satellites, for instance the SPOT sensor.
In such a camera, a linear sensor array is
used to capture a single line of imagery at a time. As the sensor moves the sensor
plane sweeps out a region of space (hence the name pushbroom), capturing the image
a single line at a time. The second dimension of the image is provided by the motion of
the sensor. In the linear pushbroom model, the sensor is assumed to move in a straight
line at constant velocity with respect to the ground. In addition, one assumes that the
orientation of the sensor array with respect to the direction of travel is constant. In
the direction of the sensor, the image is effectively a perspective image, whereas in the
direction of the sensor motion it is an orthographic projection. The geometry of the LP
camera is illustrated in ﬁgure 6.9. It turns out that the mapping from object space into
the image may be described by a 3 × 4 camera matrix, just as with a general projective
camera. However, the way in which this matrix is used is somewhat different.
• Let X = (X, Y, Z, 1)T be an object point, and let P be the camera matrix of the
LP camera. Suppose that PX = (x, y, w)T. Then the corresponding image point
(represented as an inhomogeneous 2-vector) is (x, y/w)T.

One must compare this with the projective camera mapping. In that case the point
represented by (x, y, w)T is (x/w, y/w)T. Note the difference that in the LP case, the
coordinate x is not divided by the factor w to get the image coordinate. In this formula,
the x-axis in the image is the direction of the sensor motion, whereas the y-axis is in
the direction of the linear sensor array. The camera has 11 degrees of freedom.

Another way of writing the formula for LP projection is

6.4 Other camera models

˜x = x = P1TX ˜y = y/z =

P2TX
P3TX

175

(6.27)

where (˜x, ˜y) is the image point.

Note that the ˜y-coordinate behaves projectively, whereas the ˜x is obtained by orthog-
onal projection of the point X on the direction perpendicular to the plane P1. The vector
P1 represents the sweep plane of the camera at time t = 0 – that is the moment when
the line with coordinates ˜x = 0 is captured.

Mapping of lines. One of the novel features of the LP camera is that straight lines in
space are not mapped to straight lines in the image (they are mapped to straight lines
in the case of a projective camera – see section 8.1.2). The set of points X lying on a
3D line may be written as X0 + αD, where X0 = (X, Y, Z, 1)T is a point on the line and
D = (DX, DY, DZ, 0)T is the intersection of this line with the plane at inﬁnity. In this
case, we compute from (6.27)

˜x = P1T(X0 + tD)
P2T(X0 + tD)
P3T(X0 + tD)

˜y =

.

This may be written as a pair of equations ˜x = a+bt and (c+dt)˜y = e+f t. Eliminating
t from these equations leads to an equation of the form α˜x˜y + β ˜x + γ ˜y + δ = 0, which
is the equation of a hyperbola in the image plane, asymptotic in one direction to the
line α˜x + γ = 0, and in the other direction to the line α˜y + β = 0. A hyperbola is
made up of two curves. However, only one of the curves making up the image of a line
actually appears in the image – the other part of the hyperbola corresponds to points
lying behind the camera.

6.4.2 Line cameras
This chapter has dealt with the central projection of 3-space onto a 2D image. An
analogous development can be given for the central projection of a plane onto a 1D line
contained in the plane. See ﬁgure 22.1(p535). The camera model for this geometry is

 = P2×3x

(cid:18) X

Y
Z

(cid:17)

(cid:18)

(cid:17)

x
y

=

p11 p12 p13
p21 p22 p23

which is a linear mapping from homogeneous representation of the plane to a homo-
geneous representation of the line. The camera has 5 degrees of freedom. Again the
null-space, c, of theP 2×3 projection matrix is the camera centre, and the matrix can be
decomposed in a similar manner to the ﬁnite projective camera (6.11–p157) as

P2×3 = K2×2R2×2[I2×2 | −˜c]

176
where ˜c is the inhomogeneous 2-vector representing the centre (2 dof), R2×2 is a rota-
tion matrix (1 dof), and

6 Camera Models

(cid:17)

(cid:18)

K2×2 =

αx x0
1

the internal calibration matrix (2 dof).

6.5 Closure

This chapter has covered camera models, their taxonomy and anatomy. The subsequent
chapters cover the estimation of cameras from a set of world to image correspondences,
and the action of a camera on various geometric objects such as lines and quadrics.
Vanishing points and vanishing lines are also described in more detail in chapter 8.

6.5.1 The literature
[Aloimonos-90] deﬁned a hierarchy of camera models including para-perspective.
Mundy and Zisserman [Mundy-92] generalized this with the afﬁne camera. Faugeras
developed properties of the projective camera in his textbook [Faugeras-93]. Further
details on the linear pushbroom camera are given in [Gupta-97], and on the 2D camera
in [Quan-97b].

6.5.2 Notes and exercises

(i) Let I0 be a projective image, and I1 be an image of I0 (an image of an image).
Let the composite image be denoted by I
. Show that the apparent camera
centre of I
is the same as that of I0. Speculate on how this explains why a
portrait’s eyes “follow you round the room.” Verify on the other hand that all
other parameters of I

and I0 may be different.

(cid:2)

(cid:2)

(cid:2)

(ii) Show that the ray back-projected from an image point x under a projective

camera P (as in (6.14–p162)) may be written as

L∗

= PT[x]×P

(6.28)

where L∗

is the dual Pl¨ucker representation of a line (3.9–p71).

(iii) The afﬁne camera.

(a) Show that the afﬁne camera is the most general linear mapping on ho-
mogeneous coordinates that maps parallel world lines to parallel image
lines. To do this consider the projection of points on π∞, and show that
only if P has the afﬁne form will they map to points at inﬁnity in the
image.

(b) Show that for parallel lines mapped by an afﬁne camera the ratio of
lengths on line segments is an invariant. What other invariants are there
under an afﬁne camera?

(iv) The rational polynomial camera is a general camera model, used extensively

6.5 Closure

177

in the satellite surveillance community. Image coordinates are deﬁned by the
ratios

x = Nx(X)/Dx(X) y = Ny(X)/Dy(X)

where the functions Nx, Dx, Ny, Dy are homogeneous cubic polynomials in the
3-space point X. Each cubic has 20 coefﬁcients, so that overall the camera has
78 degrees of freedom. All of the cameras surveyed in this chapter (projective,
afﬁne, pushbroom) are special cases of the rational polynomial camera. Its dis-
advantage is that it is severely over-parametrized for these cases. More details
are given in Hartley and Saxena [Hartley-97e].
camera (6.22) by applying a 4 × 4 homography H on the right such that

(v) A ﬁnite projective camera (6.11–p157) P may be transformed to an orthographic

PH = KR[I | −(cid:22)C]H =

 1 0 0 0

0 1 0 0
0 0 0 1

 = Porthog .

(the last row of H is chosen so that H has rank 4). Then since
x = P(HH−1)X = (PH)(H−1X) = PorthogX(cid:2)

imaging under P is equivalent to ﬁrst transforming the 3-space points X to X(cid:2)
=
H−1X and then applying an orthographic projection. Thus the action of any
camera may be considered as a projective transformation of 3-space followed
by orthographic projection.

7

Computation of the Camera Matrix P

This chapter describes numerical methods for estimating the camera projection matrix
from corresponding 3-space and image entities. This computation of the camera matrix
is known as resectioning. The simplest such correspondence is that between a 3D
point X and its image x under the unknown camera mapping. Given sufﬁciently many
correspondences Xi ↔ xi the camera matrix P may be determined. Similarly, P may
be determined from sufﬁciently many corresponding world and image lines.

If additional constraints apply to the matrix P, such as that the pixels are square, then
a restricted camera matrix subject to these constraints may be estimated from world to
image correspondences.

Throughout this book it is assumed that the map from 3-space to the image is linear.
This assumption is invalid if there is lens distortion. The topic of radial lens distortion
correction is dealt with in this chapter.

The internal parameters K of the camera may be extracted from the matrix P by the
decomposition of section 6.2.4. Alternatively, the internal parameters can be computed
directly, without necessitating estimating P, by the methods of chapter 8.

7.1 Basic equations

We assume a number of point correspondences Xi ↔ xi between 3D points Xi and
2D image points xi are given. We are required to ﬁnd a camera matrix P, namely a
3 × 4 matrix such that xi = PXi for all i. The similarity of this problem with that of
computing a 2D projective transformation H, treated in chapter 4, is evident. The only
difference is the dimension of the problem. In the 2D case the matrix H has dimension
3× 3, whereas in the present case, P is a 3× 4 matrix. As one may expect, much of the
material from chapter 4 applies almost unchanged to the present case.
As in section 4.1(p88) for each correspondence Xi ↔ xi we derive a relationship



0T
wiXT
−yiXT
i
i

i

−wiXT
0T
xiXT
i

yiXT
−xiXT
i
i
0T



 = 0.

 P1

P2
P3

(7.1)

where each PiT is a 4-vector, the i-th row of P. Alternatively, one may choose to use

178

only the ﬁrst two equations:

(cid:17)

0T −wiXT
0T
wiXT
i

i

yiXT
−xiXT
i
i

7.1 Basic equations

 = 0

179

(7.2)

(cid:18) P1

P2
P3

since the three equations of (7.1) are linearly dependent. From a set of n point corre-
spondences, we obtain a 2n × 12 matrix A by stacking up the equations (7.2) for each
correspondence. The projection matrix P is computed by solving the set of equations
Ap = 0, where p is the vector containing the entries of the matrix P.

Minimal solution. Since the matrix P has 12 entries, and (ignoring scale) 11 degrees
of freedom, it is necessary to have 11 equations to solve for P. Since each point corre-
spondence leads to two equations, at a minimum 5 1
2 such correspondences are required
to solve for P. The 1
2 indicates that only one of the equations is used from the sixth
point, so one needs only to know the x-coordinate (or alternatively the y-coordinate)
of the sixth image point.

Given this minimum number of correspondences, the solution is exact, i.e. the space
points are projected exactly onto their measured images. The solution is obtained by
solving Ap = 0 where A is an 11 × 12 matrix in this case. In general A will have rank
11, and the solution vector p is the 1-dimensional right null-space of A.

Over-determined solution.
If the data is not exact, because of noise in the point
coordinates, and n ≥ 6 point correspondences are given, then there will not be an exact
solution to the equations Ap = 0. As in the estimation of a homography a solution for
P may be obtained by minimizing an algebraic or geometric error.
In the case of algebraic error the approach is to minimize (cid:10)Ap(cid:10) subject to some
normalization constraint. Possible constraints are

(i) (cid:10)p(cid:10) = 1;
(ii) (cid:10)ˆp3(cid:10) = 1, where ˆp3 is the vector (p31, p32, p33)T, namely the ﬁrst three entries

in the last row of P.

The ﬁrst of these is preferred for routine use and will be used for the moment. We
will return to the second normalization constraint in section 7.2.1. In either case, the
residual Ap is known as the algebraic error. Using these equations, the complete DLT
algorithm for computation of the camera matrix P proceeds in the same manner as that
for H given in algorithm 4.1(p91).

Degenerate conﬁgurations. Analysis of the degenerate conﬁgurations for estimation
of P is rather more involved than in the case of the 2D homography. There are two
types of conﬁgurations in which ambiguous solutions exist for P. These conﬁgurations
will be investigated in detail in chapter 22. The most important critical conﬁgurations
are as follows:

(i) The camera and points all lie on a twisted cubic.

180

7 Computation of the Camera Matrix P

(ii) The points all lie on the union of a plane and a single straight line containing

the camera centre.

For such conﬁgurations, the camera cannot be obtained uniquely from the images
of the points. Instead, it may move arbitrarily along the twisted cubic, or straight line
respectively. If data is close to a degenerate conﬁguration then a poor estimate for P is
obtained. For example, if the camera is distant from a scene with low relief, such as a
near-nadir aerial view, then this situation is close to the planar degeneracy.

Data normalization.
It is important to carry out some sort of data normalization just
as in the 2D homography estimation case. The points xi in the image are appropriately
normalized in the same way as before. Namely the points should be translated so that
√
their centroid is at the origin, and scaled so that their RMS (root-mean-squared) dis-
tance from the origin is
2. What normalization should be applied to the 3D points
Xi is a little more problematical. In the case where the variation in depth of the points
from the camera is relatively slight it makes sense to carry out the same sort of normal-
ization. Thus, the centroid of the points is translated to the origin, and their coordinates
are scaled so that the RMS distance from the origin is
3 (so that the “average” point
has coordinates of magnitude (1, 1, 1, 1)T). This approach is suitable for a compact
distribution of points, such as those on the calibration object of ﬁgure 7.1.

√

In the case where there are some points that lie at a great distance from the camera,
the previous normalization technique does not work well. For instance, if there are
points close to the camera, as well as points that lie at inﬁnity (which are imaged as
vanishing points) or close to inﬁnity, as may occur in oblique views of terrain, then
it is not possible or reasonable to translate the points so that their centroid is at the
origin. The normalization method described in exercise (iii) on page 128 would be
more appropriately used in such a case, though this has not been thoroughly tested.

With appropriate normalization the estimate of P is carried out in the same manner

as algorithm 4.2(p109) for H.

Line correspondences.
It is a simple matter to extend the DLT algorithm to take
account of line correspondences as well. A line in 3D may be represented by two points
X0 and X1 through which the line passes. Now, according to result 8.2(p197) the plane
formed by back-projecting from the image line l is equal to PTl. The condition that the
point Xj lies on this plane is then

lTPXj = 0 for j = 0, 1.

(7.3)

Each choice of j gives a single linear equation in the entries of the matrix P, so two
equations are obtained for each 3D to 2D line correspondence. These equations, being
linear in the entries of P, may be added to the equations (7.1) obtained from point
correspondences and a solution to the composite equation set may be computed.

As in the case of 2D homographies (chapter 4), one may deﬁne geometric error. Sup-
pose for the moment that world points Xi are known far more accurately than the

7.2 Geometric error

7.2 Geometric error

181

(cid:27)
Objective
Given n ≥ 6 world to image point correspondences {Xi ↔ xi}, determine the Maxi-
mum Likelihood estimate of the camera projection matrix P, i.e. the P which minimizes

i d(xi, PXi)2.

Algorithm

(i) Linear solution.

Compute an initial estimate of P using a linear method such as

algorithm 4.2(p109):

(a) Normalization: Use a similarity transformation T to normalize the image
points, and a second similarity transformation U to normalize the space points.
Suppose the normalized image points are ˜xi = Txi, and the normalized space
points are ˜Xi = UXi.
(b) DLT: Form the 2n × 12 matrix A by stacking the equations (7.2) generated by
each correspondence ˜Xi ↔ ˜xi. Write p for the vector containing the entries of
the matrix ˜P. A solution of Ap = 0, subject to (cid:10)p(cid:10) = 1, is obtained from the
unit singular vector of A corresponding to the smallest singular value.

(ii) Minimize geometric error. Using the linear estimate as a starting point minimize the

geometric error (7.4):

(cid:7)

i

d(˜xi, ˜P ˜Xi)2

over ˜P, using an iterative algorithm such as Levenberg–Marquardt.

(iii) Denormalization. The camera matrix for the original (unnormalized) coordinates is

obtained from ˜P as

P = T−1˜PU.

Algorithm 7.1. The Gold Standard algorithm for estimating P from world to image point correspon-
dences in the case that the world points are very accurately known.

measured image points. For example the points Xi might arise from an accurately
machined calibration object. Then the geometric error in the image is

d(xi, ˆxi)2

(cid:7)

i

(cid:7)

min

P

i

where xi is the measured point and ˆxi is the point PXi, i.e. the point which is the exact
image of Xi under P. If the measurement errors are Gaussian then the solution of

d(xi, PXi)2

(7.4)

is the Maximum Likelihood estimate of P.

Just as in the 2D homography case, minimizing geometric error requires the use of
iterative techniques, such as Levenberg–Marquardt. A parametrization of P is required,
and the vector of matrix elements p provides this. The DLT solution, or a minimal
solution, may be used as a starting point for the iterative minimization. The complete
Gold Standard algorithm is summarized in algorithm 7.1.

Example 7.1. Camera estimation from a calibration object
We will compare the DLT algorithm with the Gold Standard algorithm 7.1 for data

182

7 Computation of the Camera Matrix P

Fig. 7.1. An image of a typical calibration object. The black and white checkerboard pattern (a “Tsai
grid”) is designed to enable the positions of the corners of the imaged squares to be obtained to high
accuracy. A total of 197 points were identiﬁed and used to calibrate the camera in the examples of this
chapter.

fy

fx/fy

skew

x0

y0

residual

linear
iterative

1673.3
1675.5

1.0063
1.0063

1.39
1.43

379.96
379.79

305.78
305.25

0.365
0.364

Table 7.1. DLT and Gold Standard calibration.

from the calibration object shown in ﬁgure 7.1. The image points xi are obtained from
the calibration object using the following steps:

(i) Canny edge detection [Canny-86].
(ii) Straight line ﬁtting to the detected linked edges.
(iii) Intersecting the lines to obtain the imaged corners.

If sufﬁcient care is taken the points xi are obtained to a localization accuracy of far
better than 1/10 of a pixel. A rule of thumb is that for a good estimation the number
of constraints (point measurements) should exceed the number of unknowns (the 11
camera parameters) by a factor of ﬁve. This means that at least 28 points should be
used.

Table 7.1 shows the calibration results obtained by using the linear DLT method
and the Gold Standard method. Note that the improvement achieved using the Gold
Standard algorithm is very slight. The difference of residual of one thousandth of a
(cid:2)
pixel is insigniﬁcant.

Errors in the world points
It may be the case that world points are not measured with “inﬁnite” accuracy. In this
case one may choose to estimate P by minimizing a 3D geometric error, or an image
geometric error, or both.

If only errors in the world points are considered then the 3D geometric error is de-

ﬁned as

(cid:7)

i

d(Xi,(cid:23)Xi)2

7.2 Geometric error

183

C

x
i

f

d

/

X
i

X
i

∆

w

Fig. 7.2. The DLT algorithm minimizes the sum of squares of geometric distance ∆ between the point
Xi and the point X(cid:1)
i mapping exactly onto xi and lying in the plane through Xi parallel to the principal
plane of the camera. A short calculation shows that wd = f∆.

where (cid:23)Xi is the closest point in space to Xi that maps exactly onto xi via xi = P(cid:23)Xi.
this requires that one augment the set of parameters by including parameters (cid:23)Xi, the

More generally, if errors in both the world and image points are considered, then a
weighted sum of world and image errors is minimized. As in the 2D homography case,

estimated 3D points. One minimizes

n(cid:7)

dMah(xi, P(cid:23)Xi)2 + dMah(Xi,(cid:23)Xi)2

i=1

where dMah represents Mahalanobis distance with respect to the known error covari-
ance matrices for each of the measurements xi and Xi. In the simplest case, the Maha-
lanobis distance is simply a weighted geometric distance, where the weights are chosen
to reﬂect the relative accuracy of measurements of the image and 3D points, and also
the fact that image and world points are typically measured in different units.

7.2.1 Geometric interpretation of algebraic error
Suppose all the points Xi in the DLT algorithm are normalized such that
Xi = (Xi, Yi, Zi, 1)T, and xi = (xi, yi, 1)T. In this case, it was seen in section 4.2.4-
i( ˆwid(xi, ˆxi))2,
(p95) that the quantity being minimized by the DLT algorithm is
where ˆwi(ˆxi, ˆyi, 1)T = PXi. However, according to (6.15–p162),

(cid:27)

ˆwi = ±(cid:10)ˆp3(cid:10) depth(X; P) .

Thus, the value ˆwi may be interpreted as the depth of the point Xi from the camera in
the direction along the principal ray, provided the camera is normalized so that (cid:10)ˆp3(cid:10)2 =
33 = 1. Referring to ﬁgure 7.2 one sees that ˆwid(xi, ˆxi) is proportional to
p2
31 + p2
f d(X(cid:2)
i is a point mapping to xi and lying in a
plane through Xi parallel to the principal plane of the camera. Thus, the algebraic error
being minimized is equal to f

32 + p2
, X), where f is the focal length and X(cid:2)

i) is the correction that needs to be made to the measured 3D
points in order to correspond precisely with the measured image points xi. The restric-
tion is that the correction must be made in the direction perpendicular to the principal
axis of the camera. Because of this restriction, the point X(cid:2)
i is not the same as the clos-

The distance d(Xi, X(cid:2)
est point (cid:23)Xi to Xi that maps to xi. However, for points Xi not too far from the principal

i d(Xi, X(cid:2)

(cid:27)

i)2.

7 Computation of the Camera Matrix P

i) is a reasonable approximation to the distance

184
d(Xi,(cid:23)Xi). The DLT slightly weights the points farther away from the principal ray by
ray of the camera, the distance d(Xi, X(cid:2)
i), which is slightly larger than d(Xi,(cid:23)Xi). In
minimizing the squared sum of d(Xi, X(cid:2)
addition, the presence of the focal length f in the expression for algebraic error sug-
gests that the DLT algorithm will be biased towards minimizing focal length at a cost
of a slight increase in 3D geometric error.
Transformation invariance. We have just seen that by minimizing (cid:10)Ap(cid:10) subject
to the constraint (cid:10)ˆp3(cid:10) = 1 one may interpret the solution in terms of minimizing 3D
geometric distances. Such an interpretation is not affected by similarity transformations
in either 3D space or the image space. Thus, one is led to expect that carrying out
translation and scaling of the data, either in the image or in 3D point coordinates, will
not have any effect on the solutions. This is indeed the case as may be shown using the
arguments of section 4.4.2(p105).

7.2.2 Estimation of an afﬁne camera
The methods developed above for the projective cameras can be applied directly to
afﬁne cameras. An afﬁne camera is one for which the projection matrix has last row
(0, 0, 0, 1). In the DLT estimation of the camera in this case one minimizes (cid:10)Ap(cid:10) sub-
ject to this condition on the last row of P. As in the case of computing 2D afﬁne trans-
formations, for afﬁne cameras, algebraic error and geometric image error are equal.
This means that geometric image distances may be minimized by a linear algorithm.

Suppose as above that all the points Xi are normalized such that Xi = (Xi, Yi, Zi, 1)T,
and xi = (xi, yi, 1)T, and also that the last row of P has the afﬁne form. Then (7.2) for
a single correspondence reduces to
0T −XT
i
0T
XT
i

(cid:18)(cid:20)

P1
P2

(7.5)

(cid:21)

(cid:20)

(cid:21)

(cid:17)

+

which shows that the squared algebraic error in this case equals the squared geometric
error

(cid:15)
xi − P1TXi

(cid:16)

2

+

(cid:15)
yi − P2TXi

(cid:7)

i

(cid:10)Ap(cid:10)2 =

yi−xi
(cid:16)

= 0

(cid:7)

2

=

i

d(xi, ˆxi)2.

This result may also be seen geometrically by comparison of ﬁgure 6.8(p170) and
ﬁgure 7.2.

A linear estimation algorithm for an afﬁne camera which minimizes geometric error
is given in algorithm 7.2. Under the assumption of Gaussian measurement errors this
is the Maximum Likelihood estimate of PA.

7.3 Restricted camera estimation

The DLT algorithm, as it has so far been described, computes a general projective
camera matrix P from a set of 3D to 2D point correspondences. The matrix P with

centre at a ﬁnite point may be decomposed as P = K[R | −R(cid:22)C] where R is a 3 × 3

7.3 Restricted camera estimation

185

(cid:27)

Objective
Given n ≥ 4 world to image point correspondences {Xi ↔ xi}, determine the Maximum
Likelihood Estimate of the afﬁne camera projection matrix PA, i.e. the camera P which mini-
mizes

i d(xi, PXi)2 subject to the afﬁne constraint P3T = (0, 0, 0, 1).

Algorithm

(i) Normalization: Use a similarity transformation T to normalize the image points, and a
second similarity transformation U to normalize the space points. Suppose the normal-
ized image points are ˜xi = Txi, and the normalized space points are ˜Xi = UXi, with
unit last component.

(cid:29)
(ii) Each correspondence ˜Xi ↔ ˜xi contributes (from (7.5)) equations

(cid:18)(cid:20)

(cid:21)

(cid:28)

(cid:17)

˜XT
i
0T

0T
˜XT
i

˜P1
˜P2

=

˜xi
˜yi

which are stacked into a 2n × 8 matrix equation A8p8 = b, where p8 is the 8-vector
containing the ﬁrst two rows of ˜PA.

(iii) The solution is obtained by the pseudo-inverse of A8 (see section A5.2(p590))

(iv) Denormalization: The camera matrix for the original (unnormalized) coordinates is

and ˜P3T = (0, 0, 0, 1).
obtained from ˜PA as

8 b
p8 = A+

PA = T−1˜PAU

Algorithm 7.2. The Gold Standard Algorithm for estimating an afﬁne camera matrix PA from world to
image correspondences.

rotation matrix and K has the form (6.10–p157):

 αx

K =

s
x0
αy y0
1

 .

(7.6)

The non-zero entries of K are geometrically meaningful quantities, the internal cali-
bration parameters of P. One may wish to ﬁnd the best-ﬁt camera matrix P subject to
restrictive conditions on the camera parameters. Common assumptions are

(i) The skew s is zero.
(ii) The pixels are square: αx = αy.
(iii) The principal point (x0, y0) is known.
(iv) The complete camera calibration matrix K is known.

In some cases it is possible to estimate a restricted camera matrix with a linear algo-
rithm (see the exercises at the end of the chapter).

As an example of restricted estimation, suppose that we wish to ﬁnd the best pinhole
camera model (that is projective camera with s = 0 and αx = αy) that ﬁts a set of
point measurements. This problem may be solved by minimizing either geometric or
algebraic error, as will be discussed next.

186

7 Computation of the Camera Matrix P

Minimizing geometric error. To minimize geometric error, one selects a set of pa-
rameters that characterize the camera matrix to be computed. For instance, suppose we
wish to enforce the constraints s = 0 and αx = αy. One can parametrize the camera
matrix using the remaining 9 parameters. These are x0, y0, α, plus 6 parameters rep-

resenting the orientation R and location (cid:22)C of the camera. Let this set of parameters be

denoted collectively by q. The camera matrix P may then be explicitly computed in
terms of the parameters.

The geometric error may then be minimized with respect to the set of parameters
using iterative minimization (such as Levenberg–Marquardt). Note that in the case
of minimization of image error only, the size of the minimization problem is 9 × 2n
(supposing 9 unknown camera parameters). In other words the LM minimization is
minimizing a function f : IR9 → IR2n. In the case of minimization of 3D and 2D error,
the function f is from IR3n+9 → IR5n, since the 3D points must be included among the
measurements and minimization also includes estimation of the true positions of the
3D points.

Minimizing algebraic error.
It is possible to minimize algebraic error instead, in
which case the iterative minimization problem becomes much smaller, as will be ex-
plained next. Consider the parametrization map taking a set of parameters q to the

corresponding camera matrix P = K[R | −R(cid:22)C]. Let this map be denoted by g. Ef-
fectively, one has a map p = g(q), where p is the vector of entries of the matrix P.
Minimizing algebraic error over all point matches is equivalent to minimizing (cid:10)Ag(q)(cid:10).
In general, the 2n × 12 matrix A may have a
The reduced measurement matrix.
very large number of rows. It is possible to replace A by a square 12× 12 matrix ˆA such
that (cid:10)Ap(cid:10) = pTATAp = (cid:10)ˆAp(cid:10) for any vector p. Such a matrix ˆA is called a reduced
measurement matrix. One way to do this is using the Singular Value Decomposition
(SVD). Let A = UDVT be the SVD of A, and deﬁne ˆA = DVT. Then

ATA = (VDUT)(UDVT) = (VD)(DVT) = ˆA

TˆA

as required. Another way of obtaining ˆA is to use the QR decomposition A = QˆA, where
Q has orthogonal columns and ˆA is upper triangular and square.
Note that the mapping q (cid:6)→ ˆAg(q) is a mapping from IR9 to IR12. This is a simple
parameter-minimization problem that may be solved using the Levenberg–Marquardt
method. The important point to note is the following:
• Given a set of n world to image correspondences, Xi ↔ xi, the problem of ﬁnd-
(cid:27)
ing a constrained camera matrix P that minimizes the sum of algebraic distances
i dalg(xi, PXi)2 reduces to the minimization of a function IR9 → IR12, independent
of the number n of correspondences.
P = K[R | −R(cid:22)C] with K as in (7.6) then P satisﬁes the condition p2
Minimization of (cid:10)ˆAg(q)(cid:10) takes place over all values of the parameters q. Note that if
33 = 1, since
these entries are the same as the last row of the rotation matrix R. Thus, minimizing
Ag(q) will lead to a matrix P satisfying the constraints s = 0 and αx = αy and scaled

32 +p2

31+p2

7.3 Restricted camera estimation

187

such that p2
all point correspondences.

32 + p2

31 + p2

33 = 1, and which in addition minimizes the algebraic error for

Initialization. One way of ﬁnding camera parameters to initialize the iteration is as
follows.

(i) Use a linear algorithm such as DLT to ﬁnd an initial camera matrix.
(ii) Clamp ﬁxed parameters to their desired values (for instance set s = 0 and set

αx = αy to the average of their values obtained using DLT).

(iii) Set variable parameters to their values obtained by decomposition of the initial

camera matrix (see section 6.2.4).

Ideally, the assumed values of the ﬁxed parameters will be close to the values ob-
tained by the DLT. However, in practice this is not always the case. Then altering these
parameters to their desired values results in an incorrect initial camera matrix that may
lead to large residuals, and difﬁculty in converging. A method which works better in
practice is to use soft constraints by adding extra terms to the cost function. Thus, for
the case where s = 0 and αx = αy, one adds extra terms ws2 + w(αx − αy)2 to the cost
function. In the case of geometric image error, the cost function becomes

(cid:7)

d(xi, PXi)2 + ws2 + w(αx − αy)2 .

i

One begins with the values of the parameters estimated using the DLT. The weights
begin with low values and are increased at each iteration of the estimation procedure.
Thus, the values of s and the aspect ratio are drawn gently to their desired values.
Finally they may be clamped to their desired values for a ﬁnal estimation.

Exterior orientation.
Suppose that all the internal parameters of the camera are
known, then all that remains to be determined are the position and orientation (or pose)
of the camera. This is the “exterior orientation” problem, which is important in the
analysis of calibrated systems.

To compute the exterior orientation a conﬁguration with accurately known position
in a world coordinate frame is imaged. The pose of the camera is then sought. Such
a situation arises in hand–eye calibration for robotic systems, where the position of
the camera is required, and also in model-based recognition using alignment where the
position of an object relative to the camera is required.

There are six parameters that must be determined, three for the orientation and three
for the position. As each world to image point correspondence generates two con-
straints it would be expected that three points are sufﬁcient. This is indeed the case,
and the resulting non-linear equations have four solutions in general.

Experimental evaluation
Results of constrained estimation for the calibration grid of example 7.1 are given in
table 7.2.

Both the algebraic and geometric minimization involve an iterative minimization

188

7 Computation of the Camera Matrix P

fy

fx/fy

skew

x0

y0

residual

algebraic
geometric

1633.4
1637.2

1.0
1.0

0.0
0.0

371.21
371.32

293.63
293.69

0.601
0.601

Table 7.2. Calibration for a restricted camera matrix.

over 9 parameters. However, the algebraic method is far quicker, since it minimizes
only 12 errors, instead of 2n = 396 in the geometric minimization. Note that ﬁxing
skew and aspect ratio has altered the values of the other parameters (compare table 7.1)
and increased the residual.

Covariance estimation. The techniques of covariance estimation and propagation of
the errors into an image may be handled in just the same way as in the 2D homography
case (chapter 5). Similarly, the minimum expected residual error may be computed
as in result 5.2(p136). Assuming that all errors are in the image measurements, the
expected ML residual error is equal to

res = σ(1 − d/2n)1/2 .

where d is the number of camera parameters being ﬁtted (11 for a full pinhole camera
model). This formula may also be used to estimate the accuracy of the point mea-
surements, given a residual error.
In the case of example 7.1 where n = 197 and
res = 0.365 this results in a value of σ = 0.37. This value is greater than expected.
The reason, as we will see later, lies in the camera model – we are ignoring radial
distortion.

Example 7.2. Covariance ellipsoid for an estimated camera
Suppose that the camera is estimated using the Maximum Likelihood (Gold Stan-
dard) method, optimizing over a set of camera parameters. The estimated covari-
ance of the point measurements can then be used to compute the covariance of
the camera model by back-propagation, according to result 5.10(p142). This gives
Σcamera = (JTΣ−1
−1 where J is the Jacobian matrix of the measured points in
terms of the camera parameters. Uncertainty in 3D world points may also be taken
into account in this way. If the camera is parametrized in terms of meaningful param-
eters (such as camera position), then the variance of each parameter can be measured
directly from the diagonal entries of the covariance matrix.

points

J)

Knowing the covariance of the camera parameters, error bounds or ellipsoids can
be computed. For instance, from the covariance matrix for all the parameters we may
extract the subblock representing the 3 × 3 covariance matrix of the camera position,
ΣC. A conﬁdence ellipsoid for the camera centre is then deﬁned by

(C − ¯C)TΣ−1

C (C − ¯C) =k 2

where k2 is computed from the inverse cumulative χ2
sired conﬁdence level α: namely k2 = F

n distribution in terms of the de-
−1
n (α) (see ﬁgure A2.1(p567)). Here n is the

7.4 Radial distortion

189

a

b

Fig. 7.3. Camera centre covariance ellipsoids. (a) Five images of Stanislas square (Nancy, France),
for which 3D calibration points are known. (b) Camera centre covariance ellipsoids corresponding to
each image, computed for cameras estimated from the imaged calibration points. Note, the typical cigar
shape of the ellipsoid aligned towards the scene data. Figure courtesy of Vincent Lepetit, Marie-Odile
Berger and Gilles Simon.

number of variables – that is 3 in the case of the camera centre. With the chosen level
of certainty α, the camera centre lies inside the ellipsoid.

Figure 7.3 shows an example of ellipsoidal uncertainty regions for computed camera
centres. Given the estimated covariance matrix for the computed camera, the tech-
niques of section 5.2.6(p148) may be used to compute the uncertainty in the image
(cid:2)
positions of further 3D world points.

7.4 Radial distortion

The assumption throughout these chapters has been that a linear model is an accurate
model of the imaging process. Thus the world point, image point and optical centre
are collinear, and world lines are imaged as lines and so on. For real (non-pinhole)
lenses this assumption will not hold. The most important deviation is generally a radial
distortion. In practice this error becomes more signiﬁcant as the focal length (and price)
of the lens decreases. See ﬁgure 7.4.

The cure for this distortion is to correct the image measurements to those that would
have been obtained under a perfect linear camera action. The camera is then effectively
again a linear device. This process is illustrated in ﬁgure 7.5. This correction must

190

7 Computation of the Camera Matrix P

a

b

Fig. 7.4. (a) Short vs (b) long focal lengths. Note the curved imaged lines at the periphery in (a) which
are images of straight scene lines.

radial distortion

linear image

correction

Fig. 7.5. The image of a square with signiﬁcant radial distortion is corrected to one that would have
been obtained under a perfect linear lens.

be carried out in the right place in the projection process. Lens distortion takes place
during the initial projection of the world onto the image plane, according to (6.2–p154).
Subsequently, the calibration matrix (7.6) reﬂects a choice of afﬁne coordinates in the
image, translating physical locations in the image plane to pixel coordinates.

We will denote the image coordinates of a point under ideal (non-distorted) pinhole
projection by (˜x, ˜y), measured in units of focal-length. Thus, for a point X we have
(see (6.5–p155))

(˜x, ˜y, 1)T = [I | 0]Xcam

where Xcam is the 3D point in camera coordinates, related to world coordinates by (6.6–
p156). The actual projected point is related to the ideal point by a radial displacement.
Thus, radial (lens) distortion is modelled as

(cid:20)

(cid:21)

(cid:20)

(cid:21)

xd
yd

= L(˜r)

˜x
˜y

(7.7)

where
• (˜x, ˜y) is the ideal image position (which obeys linear projection).
• (xd, yd) is the actual image position, after radial distortion.
• ˜r is the radial distance
• L(˜r) is a distortion factor, which is a function of the radius ˜r only.

˜x2 + ˜y2 from the centre for radial distortion.

√

7.4 Radial distortion

191

Correction of distortion.

In pixel coordinates the correction is written

ˆx = xc + L(r)(x − xc)

ˆy = yc + L(r)(y − yc).

where (x, y) are the measured coordinates, (ˆx, ˆy) are the corrected coordinates, and
(xc, yc) is the centre of radial distortion, with r2 = (x − xc)2 + (y − yc)2. Note, if the
aspect ratio is not unity then it is necessary to correct for this when computing r. With
this correction the coordinates (ˆx, ˆy) are related to the coordinates of the 3D world
point by a linear projective camera.

Choice of the distortion function and centre. The function L(r) is only deﬁned for
positive values of r and L(0) = 1. An approximation to an arbitrary function L(r)
may be given by a Taylor expansion L(r) = 1 + κ1r + κ2r2 + κ3r3 + . . .. The coef-
ﬁcients for radial correction {κ1, κ2, κ3, . . . , xc, yc} are considered part of the interior
calibration of the camera. The principal point is often used as the centre for radial
distortion, though these need not coincide exactly. This correction, together with the
camera calibration matrix, speciﬁes the mapping from an image point to a ray in the
camera coordinate system.

Computing the distortion function. The function L(r) may be computed by mini-
mizing a cost based on the deviation from a linear mapping. For example, algorithm
7.1(p181) estimates P by minimizing geometric image error for calibration objects such
as the Tsai grids of ﬁgure 7.1. The distortion function may be included as part of the
imaging process, and the parameters κi computed together with P during the iterative
minimization of the geometric error. Similarly, the distortion function may be com-
puted when estimating the homography between a single Tsai grid and its image.

A simple and more general approach is to determine L(r) by the requirement that
images of straight scene lines should be straight. A cost function is deﬁned on the
imaged lines (such as the distance between the line joining the imaged line’s ends and
its mid-point) after the corrective mapping by L(r). This cost is iteratively minimized
over the parameters κi of the distortion function and the centre of radial distortion. This
is a very practical method for images of urban scenes since there are usually plenty of
lines available. It has the advantage that no special calibration pattern is required as the
scene provides the calibration entities.

Example 7.3. Radial correction. The function L(r) is computed for the image of
ﬁgure 7.6a by minimizing a cost based on the straightness of imaged scene lines. The
image is 640×480 pixels and the correction and centre are computed as κ1 = 0.103689,
κ2 = 0.00487908, κ3 = 0.00116894, κ4 = 0.000841614, xc = 321.87, yc = 241.18
pixels, where pixels are normalized by the average half-size of the image. This is a
correction by 30 pixels at the image periphery. The result of warping the image is
(cid:2)
shown in ﬁgure 7.6b.

Example 7.4. We continue with the example of the calibration grid shown in ﬁgure 7.1
and discussed in example 7.1(p181). Radial distortion was removed by the straight line

192

7 Computation of the Camera Matrix P

a

b

Fig. 7.6. Radial distortion correction.
(a) The original image with lines which are straight in the
world, but curved in the image. Several of these lines are annotated by dashed curves. (b) The image
warped to remove the radial distortion. Note that the lines in the periphery of the image are now straight,
but that the boundary of the image is curved.

method, and then the camera calibrated using the methods described in this chapter.
The results are given in table 7.3.

Note that the residuals after radial correction are substantially smaller. Estimation of
the error of point measurements from the residual leads to a value of σ = 0.18 pixels.
Since radial distortion involves selective stretching of the image, it is quite plausible
(cid:2)
that the effective focal length of the image is changed, as seen here.

fy

fx/fy

skew

x0

y0

residual

linear
iterative
algebraic
iterative

linear
iterative
algebraic
iterative

1580.5
1580.7
1556.0
1556.6

1673.3
1675.5
1633.4
1637.2

1.0044
1.0044
1.0000
1.0000

1.0063
1.0063
1.0000
1.0000

0.75
0.70
0.00
0.00

1.39
1.43
0.00
0.00

377.53
377.42
372.42
372.41

379.96
379.79
371.21
371.32

299.12
299.02
291.86
291.86

305.78
305.25
293.63
293.69

0.179
0.179
0.381
0.380

0.365
0.364
0.601
0.601

Table 7.3. Calibration with and without radial distortion correction. The results above the line
are after radial correction – the results below for comparison are without radial distortion (from the
previous tables). The upper two methods in each case solve for the general camera model, the lower two
are for a constrained model with square pixels.

In correcting for radial distortion, it is often not actually necessary to warp the image.
Measurements can be made in the original image, for example the position of a corner
feature, and the measurement simply mapped according to (7.7). The question of where
features should be measured does not have an unambiguous answer. Warping the im-
age will distort noise models (because of averaging) and may well introduce aliasing
effects. For this reason feature detection on the unwarped image will often be prefer-
able. However, feature grouping, such as linking edgels into straight line primitives,

is best performed after warping since thresholds on linearity may well be erroneously
exceeded in the original image.

7.5 Closure

193

7.5 Closure

7.5.1 The literature
The original application of the DLT in [Sutherland-63] was for camera computation.
Estimation by iterative minimization of geometric errors is a standard procedure of
photogrammetrists, e.g. see [Slama-80].

A minimal solution for a calibrated camera (pose from the image of 3 points) was the
original problem studied by Fischler and Bolles [Fischler-81] in their RANSAC paper.
Solutions to this problem reoccur often in the literature; a good treatment is given in
[Wolfe-91] and also [Haralick-91]. Quasi-linear solutions for one more than the mini-
mum number of point correspondences Xi ↔ xi are in [Quan-98] and [Triggs-99a].
Another class of methods, which are not covered here, is the iterative estimation of a
projective camera starting from an afﬁne one. The algorithm of “Model based pose in
25 lines of code” by Dementhon and Davis [Dementhon-95] is based on this idea. A
similar method is used in [Christy-96].

Devernay and Faugeras [Devernay-95] introduced a straight line method for com-
In the photogrammetry

puting radial distortion into the computer vision literature.
literature the method is known as “plumb line correction”, see [Brown-71].

7.5.2 Notes and exercises

(i) Given 5 world-to-image point correspondences, Xi ↔ xi, show that there are
in general four solutions for a camera matrix P with zero skew that exactly maps
the world to image points.
(ii) Given 3 world-to-image point correspondences, Xi ↔ xi, show that there are
in general four solutions for a camera matrix P with known calibration K that
exactly maps the world to image points.

(iii) Find a linear algorithm for computing the camera matrix P under each of the

following conditions:

(a) The camera location (but not orientation) is known.
(b) The direction of the principal ray of the camera is known.
(c) The camera location and the principal ray of the camera are known.
(d) The camera location and complete orientation of the camera are known.
(e) The camera location and orientation are known, as well as some subset

of the internal camera parameters (αx, αy, s, x0 and y0).

(iv) Conﬂation of focal length and position on principal axis. Compare the im-
aged position of a point of depth d before and after an increase in camera focal
length ∆f, or a displacement ∆t3 of the camera backwards along the principal
axis. Let (x, y)T and (x
)T be the image coordinates of the point before and

, y

(cid:2)

(cid:2)

194

7 Computation of the Camera Matrix P

after the change. Following a similar derivation to that of (6.19–p169), show
that

(cid:21)

(cid:20)

(cid:21)

(cid:20)

(cid:20)

(cid:2)
x
(cid:2)
y

(cid:21)

=

+ k

x
y

x − x0
y − y0

where kf = ∆f /f for a focal length change, or kt3 = −∆t3/d for a displace-
ment (here skew s = 0, and αx = αy = f).
For a set of calibration points Xi with depth relief (∆i) small compared to the
average depth (d0),
i = −∆t3/di = −∆t3/(d0 + ∆i) ≈ −∆t3/d0
kt3

i.e. kt3
is approximately constant across the set. It follows that in calibrating
i
from such a set, similar image residuals are obtained by changing the focal
length kf or displacing the camera kt3. Consequently, the estimated parameters
of focal length and position on the principal axis are correlated.

(v) Pushbroom camera computation.

The pushbroom camera, described in
section 6.4.1, may also be computed using a DLT method. The x (orthographic)
part of the projection matrix has 4 degrees of freedom which may be determined
by four or more point correspondences Xi ↔ xi; the y (perspective) part of the
projection matrix has 7 degrees of freedom and may be determined from 7 cor-
respondences. Hence, a minimal solution requires 7 points. Details are given
in [Gupta-97].

8

More Single View Geometry

Chapter 6 introduced the projection matrix as the model for the action of a camera
on points. This chapter describes the link between other 3D entities and their images
under perspective projection. These entities include planes, lines, conics and quadrics;
and we develop their forward and back-projection properties.

The camera is dissected further, and reduced to its centre point and image plane.
Two properties are established: images acquired by cameras with the same centre are
related by a plane projective transformation; and images of entities on the plane at
inﬁnity, π∞, do not depend on camera position, only on camera rotation and internal
parameters, K.

The images of entities (points, lines, conics) on π∞ are of particular importance. It
will be seen that the image of a point on π∞ is a vanishing point, and the image of
a line on π∞ a vanishing line; their images depend on both K and camera rotation.
However, the image of the absolute conic, ω, depends only on K; it is unaffected by the
camera’s rotation. The conic ω is intimately connected with camera calibration, K, and
−1 is established. It follows that ω deﬁnes the angle between
the relation ω = (KKT)
rays back-projected from image points.

These properties enable camera relative rotation to be computed from vanishing
points independently of camera position. Further, since K enables the angle between
rays to be computed from image points, in turn K may be computed from the known
angle between rays. In particular K may be determined from vanishing points corre-
sponding to orthogonal scene directions. This means that a camera can be calibrated
from scene features, without requiring known world coordinates.

A ﬁnal geometric entity introduced in this chapter is the calibrating conic, which

enables a geometric visualization of K.

8.1 Action of a projective camera on planes, lines, and conics

In this section (and indeed in most of this book) it is only the 3× 4 form and rank of the
camera projection matrix P that is important in determining its action. The particular
properties and relations of its elements are often not relevant.

195

196

8 More Single View Geometry

C

x

Z

X

x π
Y

π

Fig. 8.1. Perspective image of points on a plane. The XY-plane of the world coordinate frame is
aligned with the plane π. Points on the image and scene planes are related by a plane projective
transformation.

8.1.1 On planes
The point imaging equation x = PX is a map from a point in a world coordinate frame,
to a point in image coordinates. We have the freedom to choose the world coordinate
frame. Suppose it is chosen such that the XY-plane corresponds to a plane π in the
scene, so that points on the scene plane have zero Z-coordinate as shown in ﬁgure 8.1
(it is assumed that the camera centre does not lie on the scene plane). Then, if the
columns of P are denoted as pi, the image of a point on π is given by

x = PX =

p1 p2 p3 p4

p1 p2 p4

"

 =

"

# X

Y
0
1

 .

# X

Y
1

So that the map between points xπ = (X, Y, 1)T on π and their image x is a general
planar homography (a plane to plane projective transformation): x = Hxπ, with H a
3 × 3 matrix of rank 3. This shows that:
• The most general transformation that can occur between a scene plane and an image
plane under perspective imaging is a plane projective transformation.

If the camera is afﬁne, then a similar derivation shows that the scene and image planes
are related by an afﬁne transformation.
Example 8.1. For a calibrated camera (6.8–p156) P = K[R | t], the homography be-
tween a world plane at Z = 0 and the image is

where ri are the columns of R.

H = K [r1, r2, t]

(8.1)
(cid:2)

8.1.2 On lines
Forward projection. A line in 3-space projects to a line in the image. This is easily
seen geometrically – the line and camera centre deﬁne a plane, and the image is the
intersection of this plane with the image plane (ﬁgure 8.2) – and is proved algebraically

8.1 Action of a projective camera on planes, lines, and conics

197

L

π

l

C

Fig. 8.2. Line projection. A line L in 3-space is imaged as a line l by a perspective camera. The image
line l is the intersection of the plane π, deﬁned by L and the camera centre C, with the image plane.
Conversely an image line l back-projects to a plane π in 3-space. The plane is the “pull-back” of the
line.

by noting that if A, B are points in 3-space, and a, b their images under P, then a point
X(µ) = A + µB on a line which is the join of A, B in 3-space projects to a point

x(µ) =P (A + µB) = PA + µPB

= a + µb

which is on the line joining a and b.

Back-projection of lines. The set of points in space which map to a line in the image
is a plane in space deﬁned by the camera centre and image line, as shown in ﬁgure 8.2.
Algebraically,
Result 8.2. The set of points in space mapping to a line l via the camera matrix P is the
plane PTl.

Proof. A point x lies on l if and only if xTl = 0. A space point X maps to a point
PX, which lies on the line if and only if XTPTl = 0. Thus, if PTl is taken to represent a
plane, then X lies on this plane if and only if X maps to a point on the line l. In other
words, PTl is the back-projection of the line l.
Geometrically there is a star (two-parameter family) of planes through the camera cen-
tre, and the three rows of the projection matrix PiT (6.12–p159) are a basis for this star.
The plane PTl is a linear combination of this basis corresponding to the element of the
star containing the camera centre and the line l. For example, if l = (0, 1, 0)T then the
plane is P2, and is the back projection of the image x-axis.

Pl¨ucker line representation. Understanding this material on Pl¨ucker line mapping is
not required for following the rest of the book.

We now turn to forward projection of lines. If a line in 3-space is represented by
Pl¨ucker coordinates then its image can be expressed as a linear map on these coordi-
nates. We will develop this map for both the 4 × 4 matrix and 6-vector line representa-
tions.

198

8 More Single View Geometry

Result 8.3. Under the camera mapping P, a line in 3-space represented as a Pl¨ucker
matrix L, as deﬁned in (3.8–p70), is imaged as the line l where

[l]× = PLPT.

(8.2)

where the notation [l]× is deﬁned in (A4.5–p581).

Proof. Suppose as above that a = PA, b = PB. The Pl¨ucker matrix L for the line
through A, B in 3-space is L = ABT − BAT. Then the matrix M = PLPT = abT − baT
is 3 × 3 and antisymmetric, with null-space a × b. Consequently, M = [a × b]×, and
since the line through the image points is given by l = a × b, this completes the proof.

It is clear from the form of (8.2) that there is a linear relation between the image line
coordinates li and the world line coordinates Ljk, but that this relation is quadratic
in the elements of the point projection matrix P. Thus, (8.2) may be rearranged such
that the map between the Pl¨ucker line coordinates, L (a 6-vector), and the image line
coordinates l (a 3-vector) is represented by a single 3 × 6 matrix. It can be shown that
Deﬁnition 8.4. The line projection matrix P is the 3 × 6 matrix of rank 3 given by

 P2 ∧ P3

P3 ∧ P1
P1 ∧ P2



P =

(8.3)

(8.4)

where PiT are the rows of the point camera matrix P, and Pi ∧ Pj are the Pl¨ucker line
coordinates of the intersection of the planes Pi and Pj.

Then the forward line projection is given by
Result 8.5. Under the line projection matrix P, a line in IP3 represented by Pl¨ucker
line coordinates L, as deﬁned in (3.11–p72), is mapped to the image line

 (P2 ∧ P3|L)

(P3 ∧ P1|L)
(P1 ∧ P2|L)



l = PL =

where the product (L| ˆL) is deﬁned in (3.13–p72).

Proof. Suppose the line in 3-space is the join of the points A and B, and these project
to a = PA, b = PB respectively. Then the image line l = a × b = (PA) × (PB).
Consider the ﬁrst element

l1 = (P2TA)(P3TB) − (P2TB)(P3TA)

= (P2 ∧ P3|L)

where the second equality follows from (3.14–p73). The other components follow in a
similar manner.

8.1 Action of a projective camera on planes, lines, and conics

199

The line projection matrix P plays the same role for lines as P does for points. The rows
of P may be interpreted geometrically as lines, in a similar manner to the interpretation
of the rows of the point camera matrix P as planes given in section 6.2.1(p158). The
rows PiT of P are the principal plane and axis planes of the camera. The rows of P
are the lines of intersection of pairs of these camera planes. For example, the ﬁrst
row of P is P2 ∧ P3, and this is the 6-vector Pl¨ucker line representation of the line of
intersection of the y = 0 axis plane, P2, and the principal plane, P3. The three lines
corresponding to the three rows of P intersect at the camera centre. Consider lines L
in 3-space for which PL = 0. These lines are in the null-space of P. Since each row
of P is a line, and from result 3.5(p72) the product (L1|L2) = 0 if two lines intersect,
if follows that L intersects each of the lines represented by the rows of P. These lines
are the intersection of the camera planes, and the only point on all 3 camera planes is
the camera centre. Thus we have
• The lines L in IP3 for which PL = 0 pass through the camera centre.
The 3 × 6 matrix P has a 3-dimensional null-space. Allowing for the homogeneous
scale factor, this null-space is a two-parameter family of lines containing the camera
centre. This is to be expected since there is a star (two parameter family) of lines in IP3
concurrent with a point.

8.1.3 On conics
Back-projection of conics. A conic C back-projects to a cone. A cone is a degenerate
quadric, i.e. the 4× 4 matrix representing the quadric does not have full rank. The cone
vertex, in this case the camera centre, is the null-vector of the quadric matrix.

Result 8.6. Under the camera P the conic C back-projects to the cone

Qco = PTCP.

Proof. A point x lies on C if and only if xTCx = 0. A space point X maps to a point
PX, which lies on the conic if and only if XTPTCPX = 0. Thus, if Qco = PTCP is taken
to represent a quadric, then X lies on this quadric if and only if X maps to a point on
the conic C. In other words, Qco is the back-projection of the conic C.

Note the camera centre C is
QcoC = PTC(PC) =0 .
Example 8.7. Suppose that P = K[I | 0]; then the conic C back-projects to the cone

the degenerate quadric since

the vertex of

(cid:18)

.

(cid:17)

(cid:18)

(cid:17)

Qco =

KT
0T

C [K | 0] =

KTCK 0
0T
0

The matrix Qco has rank 3. Its null-vector is the camera centre C = (0, 0, 0, 1)T.

(cid:2)

200

8 More Single View Geometry

n

k

x

n

X

Contour
generator

a

Γ

Apparent
γ
contour

C

b

Fig. 8.3. Contour generator and apparent contour. (a) for parallel projection; (b) for central projec-
tion. The ray from the camera centre through x is tangent to the surface at X. The set of such tangent
points X deﬁnes the contour generator, and their image deﬁnes the apparent contour. In general the
contour generator is a space curve. Figure courtesy of Roberto Cipolla and Peter Giblin.

8.2 Images of smooth surfaces

The image outline of a smooth surface S results from surface points at which the imag-
ing rays are tangent to the surface, as shown in ﬁgure 8.3. Similarly, lines tangent to
the outline back-project to planes which are tangent planes to the surface.

Deﬁnition 8.8. The contour generator Γ is the set of points X on S at which rays are
tangent to the surface. The corresponding image apparent contour γ is the set of points
x which are the image of X, i.e. γ is the image of Γ.

The apparent contour is also called the “outline” and “proﬁle”. If the surface is viewed
in the direction of X from the camera centre, then the surface appears to fold, or to have
a boundary or occluding contour.

It is evident that the contour generator Γ depends only on the relative position of the
camera centre and surface, not on the image plane. However, the apparent contour γ
is deﬁned by the intersection of the image plane with the rays to the contour generator,
and so does depend on the position of the image plane.

In the case of parallel projection with direction k, consider all the rays parallel to k
which are tangent to S, see ﬁgure 8.3a. These rays form a “cylinder” of tangent rays,
and the curve along which this cylinder is tangent to S is the contour generator Γ. The
curve in which the cylinder meets the image plane is the apparent contour γ. Note that
both Γ and γ depend in an essential way on k. The set Γ slips over the surface as the
direction of k changes. For example, with S a sphere, Γ is the great circle orthogonal
to k. In this case, the contour generator Γ is a plane curve, but in general Γ is a space
curve.

We next describe the projection properties of quadrics. For this class of surface
algebraic expressions can be developed for the contour generator and apparent contour.

8.3 Action of a projective camera on quadrics

201

Γ

c

Fig. 8.4. The cone of rays for a quadric. The vertex of the cone is the camera centre. (a) The contour
generator Γ of a quadric is a plane curve (a conic) which is the intersection of the quadric with the
polar plane of the camera centre, C.

8.3 Action of a projective camera on quadrics

A quadric is a smooth surface and so its outline curve is given by points where the
back-projected rays are tangent to the quadric surface as shown in ﬁgure 8.4.

Suppose the quadric is a sphere, then the cone of rays between the camera centre
and quadric is right-circular, i.e. the contour generator is a circle, with the plane of
the circle orthogonal to the line joining the camera and sphere centres. This can be
seen from the rotational symmetry of the geometry about this line. The image of the
sphere is obtained by intersecting the cone with the image plane. It is clear that this is a
classical conic section, so that the apparent contour of a sphere is a conic. In particular
if the sphere centre lies on the principal (Z) camera axis, then the conic is a circle.

Now consider a 3-space projective transformation of this geometry. Under this map
the sphere is transformed to a quadric and the apparent contour to a conic. However,
since intersection and tangency are preserved, the contour generator is a (plane) conic.
Consequently, the apparent contour of a general quadric is a conic, and the contour gen-
erator is also a conic. We will now give algebraic representations for these geometric
results.
Forward projection of quadrics. Since the outline arises from tangency, it is not
surprising that the dual of the quadric, Q∗
, is important here since it deﬁnes the tangent
planes to the quadric Q.

Result 8.9. Under the camera matrix P the outline of the quadric Q is the conic C given
by

C∗

= PQ∗PT.

(8.5)

Proof. This expression is simply derived from the observation that lines l tangent to
the conic outline satisfy lTC∗l = 0. These lines back-project to planes π = PTl that are
tangent to the quadric and thus satisfy πTQ∗π = 0. Then it follows that for each line

πTQ∗π = lTPQ∗PTl
= lTC∗l = 0

202

8 More Single View Geometry

and since this is true for all lines tangent to C the result follows.

Note the similarity of (8.5) with the projection of a line represented by a Pl¨ucker ma-
trix (8.2). An expression for the projection of the point quadric Q can be derived
from (8.5) but it is quite complicated. However, the plane of the contour generator
is easily expressed in terms of Q:
• The plane of Γ for a quadric Q and camera with centre C is given by πΓ = QC.
This result follows directly from the pole–polar relation for a point and quadric of
section 3.2.3(p73). Its proof is left as an exercise. Note, the intersection of a quadric
and plane is a conic. So Γ is a conic and its image γ, which is the apparent contour, is
also a conic as has been seen above.

We may also derive an expression for the cone of rays formed by the camera centre

and quadric. This cone is a degenerate quadric of rank 3.
Result 8.10. The cone with vertex V and tangent to the quadric Q is the degenerate
quadric

Qco = (VTQV)Q − (QV)(QV)T.

Note that QcoV = 0, so that V is the vertex of the cone as required. The proof is
omitted.
Example 8.11. We write the quadric in block form:

(cid:17)

Q =

Q3×3 q
qT
q44

(cid:18)

.

Then if V = (0, 0, 0, 1)T, which corresponds to the cone vertex being at the centre of
the world coordinate frame,

(cid:17)

Qco =

q44Q3×3 − qqT 0

0T

(cid:18)

which is clearly a degenerate quadric.

0

(cid:2)

8.4 The importance of the camera centre

An object in 3-space and camera centre deﬁne a set of rays, and an image is obtained
by intersecting these rays with a plane. Often this set is referred to as a cone of rays,
even though it is not a classical cone. Suppose the cone of rays is intersected by two
planes, as shown in ﬁgure 8.5, then the two images, I and I
, are clearly related by a
perspective map. This means that images obtained with the same camera centre may
be mapped to one another by a plane projective transformation, in other words they are
projectively equivalent and so have the same projective properties. A camera can thus
be thought of as a projective imaging device – measuring projective properties of the
cone of rays with vertex the camera centre.
(cid:2)
The result that the two images I and I

are related by a homography will now be
derived algebraically to obtain a formula for this homography. Consider two cameras

(cid:2)

P = KR[I | −(cid:22)C], P(cid:2)

[I | −(cid:22)C]

= K(cid:2)R(cid:2)

8.4 The importance of the camera centre

203

C

x /

x

X

Fig. 8.5. The cone of rays with vertex the camera centre. An image is obtained by intersecting this
cone with a plane. A ray between a 3-space point X and the camera centre C pierces the planes in the
image points x and x(cid:1)

. All such image points are related by a planar homography, x(cid:1) = Hx.

with the same centre. Note that since the cameras have a common centre there is a
simple relation between them, namely P(cid:2)
−1P. It then follows that the
images of a 3-space point X by the two cameras are related as

= (K(cid:2)R(cid:2)

)(KR)

x(cid:2)

= P(cid:2)X = (K(cid:2)R(cid:2)

)(KR)

−1PX = (K(cid:2)R(cid:2)

)(KR)

−1x.

That is, the corresponding image points are related by a planar homography (a 3 × 3
matrix) as x(cid:2)

= Hx, where H = (K(cid:2)R(cid:2)

)(KR)

−1.

We will now investigate several cases of moving the image plane whilst ﬁxing the
camera centre. For simplicity the world coordinate frame will be chosen to coincide
with the camera’s, so that P = K[I | 0] (and it will be assumed that the image plane
never contains the centre, as the image would then be degenerate).

8.4.1 Moving the image plane
Consider ﬁrst an increase in focal length. To a ﬁrst approximation this corresponds
to a displacement of the image plane along the principal axis. The image effect is a
simple magniﬁcation. This is only a ﬁrst approximation because with a compound
lens zooming will perturb both the principal point and the effective camera centre.
Algebraically, if x, x(cid:2)

are the images of a point X before and after zooming, then

x = K[I | 0]X
x(cid:2)

= K(cid:2)

[I | 0]X = K(cid:2)K−1 (K[I | 0]X) =K (cid:2)K−1x

so that x(cid:2)
a short calculation shows that

= Hx with H = K(cid:2)K−1. If only the focal lengths differ between K and K(cid:2)

then

(cid:17)

K(cid:2)K−1 =

kI (1 − k)˜x0
0T

1

(cid:18)

.

where ˜x0 is the inhomogeneous principal point, and k = f
/f is the magniﬁcation
factor. This result follows directly from similar triangles: the effect of zooming by a

(cid:2)

204

8 More Single View Geometry

a

b

c

Fig. 8.6. Between images (a) and (b) the camera rotates about the camera centre. Corresponding points
(that is images of the same 3D point) are related by a plane projective transformation. Note that 3D
points at different depths which are coincident in image (a), such as the mug lip and cat body, are also
coincident in (b), so there is no motion parallax in this case. However, between images (a) and (c) the
camera rotates about the camera centre and translates. Under this general motion coincident points of
differing depth in (a) are imaged at different points in (c), so there is motion parallax in this case due to
the camera translation.

factor k is to move the image point ˜x on a line radiating from the principal point ˜x0 to
= k˜x + (1− k)˜x0. Algebraically, using the most general form (6.10–p157)
the point ˜x(cid:2)
of the calibration matrix K, we may write

(cid:18)(cid:17)

(cid:18)

(cid:17)
(cid:17)

K(cid:2)

=

=

(cid:18)

kI (1 − k)˜x0
(cid:17)
0T
kA ˜x0
0T
1

= K

1

(cid:18)

(cid:17)

K =

kI

.

1

kI (1 − k)˜x0
(cid:18)
0T

1

A
0T

˜x0
1

This shows that
• The effect of zooming by a factor k is to multiply the calibration matrix K on the right
by diag(k, k, 1).

8.4.2 Camera rotation
A second common example is where the camera is rotated about its centre with
no change in the internal parameters. Examples of this “pure” rotation are given
in ﬁgure 8.6 and ﬁgure 8.9. Algebraically, if x, x(cid:2)
are the images of a point X be-
fore and after the pure rotation

x = K[I | 0]X
x(cid:2)

= K [R | 0] X = KRK−1K[I | 0]X = KRK−1x

= Hx with H = KRK−1. This homography is a conjugate rotation and is
so that x(cid:2)
discussed further in section A7.1(p628). For now, we mention a few of its properties
by way of an example.

Example 8.12. Properties of a conjugate rotation
The homography H = KRK−1 has the same eigenvalues (up to scale) as the rotation
matrix, namely {µ, µeiθ, µe
−iθ}, where µ is an unknown scale factor (if H is scaled such
that det H = 1, then µ = 1). Consequently the angle of rotation between views may be
computed directly from the phase of the complex eigenvalues of H. Similarly, it can be

8.4 The importance of the camera centre

205

a

b

c

Fig. 8.7. Synthetic views. (a) Source image. (b) Fronto-parallel view of the corridor ﬂoor generated
from (a) using the four corners of a ﬂoor tile to compute the homography. (c) Fronto-parallel view of the
corridor wall generated from (a) using the four corners of the door frame to compute the homography.

shown (see exercises) that the eigenvector of H corresponding to the real eigenvalue is
the vanishing point of the rotation axis.

For example, between images (a) and (b) of ﬁgure 8.6 there is a pure rotation
of the camera. The homography H is computed by algorithm 4.6(p123), and from
this the angle of rotation is estimated as 4.66
, and the axis vanishing point as
(−0.0088, 1, 0.0001)T, i.e. virtually at inﬁnity in the y direction, so the rotation axis
(cid:2)
is almost parallel to the y-axis.
The transformation H = KRK−1 is an example of the inﬁnite homography mapping
H∞, that will appear many times through this book. It is deﬁned in section 13.4(p338).
The conjugation property is used for auto-calibration in chapter 19.

◦

8.4.3 Applications and examples
The homographic relation between images with the same camera centre can be ex-
ploited in several ways. One is the creation of synthetic images by projective warping.
Another is mosaicing, where panoramic images can be created by using planar homo-
graphies to “sew” together views obtained by a rotating camera.

Example 8.13. Synthetic views
New images corresponding to different camera orientations (with the same camera
centre) can be generated from an existing image by warping with planar homographies.
In a fronto-parallel view a rectangle is imaged as a rectangle, and the world and
image rectangle have the same aspect ratio. Conversely, a fronto-parallel view can be
synthesized by warping an image with the homography that maps a rectangle imaged
as a quadrilateral to a rectangle with the correct aspect ratio. The algorithm is:

(i) Compute the homography H which maps the image quadrilateral to a rectangle

with the correct aspect ratio.

(ii) Projectively warp the source image with this homography.

Examples are shown in ﬁgure 8.7.

(cid:2)

206

8 More Single View Geometry

  

  


 

  

 
  


 

  

 

 

 


  

 

 

 


Fig. 8.8. Three images acquired by a rotating camera may be registered to the frame of the middle one,
as shown, by projectively warping the outer images to align with the middle one.

Example 8.14. Planar panoramic mosaicing
Images acquired by a camera rotating about its centre are related to each other by a
planar homography. A set of such images may be registered with the plane of one of
the images by projectively warping the other images, as illustrated in ﬁgure 8.8.

Fig. 8.9. Planar panoramic mosaicing. Eight images (out of thirty) acquired by rotating a camcorder
about its centre. The thirty images are registered (automatically) using planar homographies and com-
posed into the single panoramic mosaic shown. Note the characteristic “bow tie” shape resulting from
registering to an image at the middle of the sequence.

In outline the algorithm is:

(i) Choose one image of the set as a reference.
(ii) Compute the homography H which maps one of the other images of the set to

this reference image.

8.4 The importance of the camera centre

207

(iii) Projectively warp the image with this homography, and augment the reference

image with the non-overlapping part of the warped image.

(iv) Repeat the last two steps for the remaining images of the set.

The homographies may be computed by identifying (at least) four corresponding
points, or by using the automatic method of algorithm 4.6(p123). An example mo-
(cid:2)
saic is shown in ﬁgure 8.9.

8.4.4 Projective (reduced) notation
It will be seen in chapter 20 that if canonical projective coordinates are chosen for
world and image points, i.e.

X1 = (1, 0, 0, 0)T, X2 = (0, 1, 0, 0)T, X3 = (0, 0, 1, 0)T, X4 = (0, 0, 0, 1)T,

and

x1 = (1, 0, 0)T, x2 = (0, 1, 0)T, x3 = (0, 0, 1)T, x4 = (1, 1, 1)T,

then the camera matrix

 a 0 0 −d



P =

0 b 0 −d
0 0 c −d
−1)T = 0, which means
−1, b
satisﬁes xi = PXi, i = 1, . . . ,4 , and also that P(a
−1)T. This is known as the reduced
−1, d
that the camera centre is C = (a
camera matrix, and it is clearly completely speciﬁed by the 3 degrees of freedom of
the camera centre C. This is a further illustration of the fact that all images acquired
by cameras with the same camera centre are projectively equivalent – the camera has
been reduced to its essence: a projective device whose action is to map IP3 to IP2 with
only the position of the camera centre affecting the result. This camera representation
is used in establishing duality relations in chapter 20.

−1, d

−1, c

−1, b

−1, c

(8.6)

8.4.5 Moving the camera centre
The cases of zooming and camera rotation illustrate that moving the image plane, whilst
ﬁxing the camera centre, induces a transformation between images that depends only
on the image plane motion, but not on the 3-space structure. Conversely, no information
on 3-space structure can be obtained by this action. However, if the camera centre is
moved then the map between corresponding image points does depend on the 3-space
structure, and indeed may often be used to (partially) determine the structure. This is
the subject of much of the remainder of this book.

How can one determine from the images alone whether the camera centre has
moved? Consider two 3-space points which have coincident images in the ﬁrst view,
i.e. the points are on the same ray. If the camera centre is moved (not along that ray)
the image coincidence is lost. This relative displacement of previously coincident im-
age points is termed parallax, and is illustrated in ﬁgure 8.6 and shown schematically
in ﬁgure 8.10. If the scene is static and motion parallax is evident between two views
then the camera centre has moved. Indeed, a convenient method for obtaining a camera

208

8 More Single View Geometry

X 1

L

x

C

X 2

x/
2

x /
1

C /

Fig. 8.10. Motion parallax. The images of the space points X1 and X2 are coincident when viewed by
the camera with centre C. However, when viewed by a camera with centre C(cid:1)
, which does not lie on the
line L through X1 and X2, the images of the space points are not coincident. In fact the line through
the image points x(cid:1)
2 is the image of the ray L, and will be seen in chapter 9 to be an epipolar
line. The vector between the points x(cid:1)

2 is the parallax.

1 and x(cid:1)

1 and x(cid:1)

motion that is only a rotation about its centre (for example for a camera mounted on a
robot head) is to adjust the motion until there is no parallax.

An important special case of 3-space structure is when all scene points are coplanar.
In this case the images of corresponding points are related by a planar homography
even if the camera centre is moved. The map between images in this case is discussed
in detail in chapter 13 on planes. In particular vanishing points, which are images of
points on the plane π∞, are related by a planar homography for any camera motion.
We return to this in section 8.6.

8.5 Camera calibration and the image of the absolute conic

Up to this point we have discussed projective properties of the forward and back-
projection of various entities (point, lines, conics . . . ). These properties depend only
on the 3× 4 form of the projective camera matrix P. Now we describe what is gained if
the camera internal calibration, K, is known. It will be seen that Euclidean properties,
such as the angle between two rays, can then be measured.

What does calibration give? An image point x back-projects to a ray deﬁned by x and
points on the ray are written as (cid:22)X = λd in the camera Euclidean coordinate frame, then
the camera centre. Calibration relates the image point to the ray’s direction. Suppose
these points map to the point x = K[I | 0](λdT, 1)T = Kd up to scale. Conversely the
direction d is obtained from the image point x as d = K−1x. Thus we have established:
Result 8.15. The camera calibration matrix K is the (afﬁne) transformation between
x and the ray’s direction d = K−1x measured in the camera’s Euclidean coordinate
frame.
Note, d = K−1x is in general not a unit vector.

8.5 Camera calibration and the image of the absolute conic

209

C

θ

x1

x2

d1

d

2

Fig. 8.11. The angle θ between two rays.

The angle between two rays, with directions d1, d2 corresponding to image points
x1, x2 respectively, may be obtained from the familiar cosine formula for the angle
between two vectors:

cos θ =

=

dT
1
d1

dT
1

(cid:19)
(cid:19)

(cid:19)

d2

(cid:19)
(K−1x1)T(K−1x1)
dT
d2
(cid:19)
2
1 (K−TK−1)x2
xT

=

.

(K−1x1)T(K−1x2)

(cid:19)
(K−1x2)T(K−1x2)

1 (K−TK−1)x1
xT

2 (K−TK−1)x2
xT

(8.7)

The formula (8.7) shows that if K, and consequently the matrix K−TK−1, is known,
then the angle between rays can be measured from their corresponding image points.
A camera for which K is known is termed calibrated. A calibrated camera is a direction
sensor, able to measure the direction of rays – like a 2D protractor.

The calibration matrix K also provides a relation between an image line and a scene

plane:

Result 8.16. An image line l deﬁnes a plane through the camera centre with normal
direction n = KTl measured in the camera’s Euclidean coordinate frame.
Note, the normal n will not in general be a unit vector.
Proof. Points x on the line l back-project to directions d = K−1x which are orthogonal
to the plane normal n, and thus satisfy dTn = xTK−Tn = 0. Since points on l satisfy
xTl = 0, it follows that l = K−Tn, and hence n = KTl.

8.5.1 The image of the absolute conic
We now derive a very important result which relates the calibration matrix K to the
image of the absolute conic, ω. First we must determine the map between the plane
at inﬁnity, π∞, and the camera image plane. Points on π∞ may be written as X∞ =

(dT, 0)T, and are imaged by a general camera P = KR[I | −(cid:22)C] as

x = PX∞ = KR[I | −(cid:22)C]

(cid:20)

(cid:21)

d
0

= KRd.

This shows that

210

8 More Single View Geometry

• the mapping between π∞ and an image is given by the planar homography x = Hd
with

H = KR.

(8.8)

Note that this map is independent of the position of the camera, C, and depends only
on the camera internal calibration and orientation with respect to the world coordinate
frame.

Now, since the absolute conic Ω∞ (section 3.6(p81)) is on π∞ we can compute its

image under H, and ﬁnd

of

the

absolute

image
−1 = K−TK−1.

Result 8.17. The
ω = (KKT)
Proof. From result 2.13(p37) under a point homography x (cid:6)→ Hx a conic C maps as
C (cid:6)→ H−TCH−1. It follows that Ω∞, which is the conic C = Ω∞ = I on π∞, maps to
ω = (KR)

−1 = K−TRR−1K−1 = (KKT)

−1. So the IAC ω = (KKT)

−TI(KR)

conic

conic

−1.

(the

IAC)

is

the

Like Ω∞ the conic ω is an imaginary point conic with no real points. For the moment it
may be thought of as a convenient algebraic device, but it will be used in computations
later in this chapter, and also in chapter 19 on camera auto-calibration.
A few remarks here:

(i) The image of the absolute conic, ω, depends only on the internal parameters K

of the matrix P; it does not depend on the camera orientation or position.

(ii) It follows from (8.7) that the angle between two rays is given by the simple

expression

cos θ =

(cid:19)

xT
1

(cid:19)

xT
1
ωx1

ωx2
xT
2

.

ωx2

(8.9)

This expression is independent of the projective coordinate frame in the image,
that is, it is unchanged under projective transformation of the image. To see this
consider any 2D projective transformation, H. The points xi are transformed
to Hxi, and ω transforms (as any image conic) to H−TωH−1. Thus, (8.9) is
unchanged, and hence holds in any projective coordinate frame in the image.

(iii) A particularly important specialization of (8.9) is that if two image points x1

and x2 correspond to orthogonal directions then

xT
1

ωx2 = 0.

(8.10)

This equation will be used at several points later in the book as it provides a
linear constraint on ω.

(iv) We may also deﬁne the dual image of the absolute conic (the DIAC) as

ω∗

= ω−1 = KKT.

(8.11)

This is a dual (line) conic, whereas ω is a point conic (though it contains no real
points). The conic ω∗

∞ and is given by (8.5) ω∗

is the image of Q∗

= PQ∗

∞PT.

8.5 Camera calibration and the image of the absolute conic

211

(v) Result 8.17 shows that once ω (or equivalently ω∗

) is identiﬁed in an image
then K is also determined. This follows because a symmetric matrix ω may be
uniquely decomposed into a product ω∗
= KKT of an upper-triangular matrix
with positive diagonal entries and its transpose by the Cholesky factorization
(see result A4.5(p582)).

(vi) It was seen in chapter 3 that a plane π intersects π∞ in a line, and this line
intersects Ω∞ in two points which are the circular points of π. The imaged
circular points lie on ω at the points at which the vanishing line of the plane π
intersects ω.

These ﬁnal two properties of ω are the basis for a calibration algorithm, as shown in
the following example.

Example 8.18. A simple calibration device
The image of three squares (on planes which are not parallel, but which need not be
orthogonal) provides sufﬁciently many constraints to compute K. Consider one of the
squares. The correspondences between its four corner points and their images deﬁne
the homography H between the plane π of the square and the image. Applying this
homography to circular points on π determines their images as H(1,±i, 0)T. Thus we
have two points on the (as yet unknown) ω. A similar procedure applied to the other
squares generates a total of six points on ω, from which it may be computed (since ﬁve
points are required to determine a conic). In outline the algorithm has the following
steps:

(i) For each square compute the homography H that maps its corner points,
(0, 0)T, (1, 0)T, (0, 1)T, (1, 1)T, to their imaged points. (The alignment of the
plane coordinate system with the square is a similarity transformation and does
not affect the position of the circular points on the plane).
(ii) Compute the imaged circular points for the plane of that square as H(1,±i, 0)T.
Writing H = [h1, h2, h3], the imaged circular points are h1 ± ih2.
(iii) Fit a conic ω to the six imaged circular points. The constraint that the imaged
circular points lie on ω may be rewritten as two real constraints. If h1 ± ih2
T ω (h1 ± ih2) = 0, and the imaginary and real parts
lies on ω then (h1 ± ih2)
give respectively:

hT
1

ωh2 = 0 and hT

1

ωh1 = hT

2

ωh2

(8.12)

which are equations linear in ω. The conic ω is determined up to scale from
ﬁve or more such equations.
−1 using the Cholesky factorization.

(iv) Compute the calibration K from ω = (KKT)

Figure 8.12 shows a calibration object consisting of three planes imprinted with
squares, and the computed matrix K. For the purpose of internal calibration, the squares
have the advantage over a standard calibration object (e.g. ﬁgure 7.1(p182)) that no
(cid:2)
measured 3D co-ordinates are required.

212

8 More Single View Geometry

(cid:17)

K =

1108.3 −9.8
1097.8

0
0

0

(cid:18)

525.8
395.9

1

a

b

Fig. 8.12. Calibration from metric planes. (a) Three squares provide a simple calibration object. The
planes need not be orthogonal. (b) The computed calibration matrix using the algorithm of example 8.18.
The image size is 1024 × 768 pixels.

x
1

a

ω

x

2

image

ω

x

b

l

image

Fig. 8.13. Orthogonality represented by conjugacy and pole–polar relationships. (a) Image points
x1, x2 back-project to orthogonal rays if the points are conjugate with respect to ω, i.e. xT
1 ωx2 = 0. (b)
The point x and line l back-project to a ray and plane that are orthogonal if x and l are pole–polar with
respect to ω, i.e. l = ωx. For example (see section 8.6.3), the vanishing point of the normal direction to
a plane and the vanishing line of the plane are pole–polar with respect to ω.

We will return to camera calibration in section 8.8, where vanishing points and lines
provide constraints on K. The geometric constraints that are used in example 8.18 are
discussed further in section 8.8.1.

8.5.2 Orthogonality and ω
The conic ω is a device for representing orthogonality in an image. It has already been
seen (8.10) that if two image points x1 and x2 back-project to orthogonal rays, then the
points satisfy xT
1

ωx2 = 0. Similarly, it may be shown that

Result 8.19. A point x and line l back-projecting to a ray and plane respectively that
are orthogonal are related by l = ωx.

Geometrically these relations express that image points back-projecting to orthogonal
rays are conjugate with respect to ω (xT
ωx2 = 0), and that a point and line back-
1
projecting to an orthogonal ray and plane are in a pole–polar relationship (l = ωx).
See section 2.8.1(p58). A schematic representation of these two relations is given
in ﬁgure 8.13.

8.6 Vanishing points and vanishing lines

213

These geometric representations of orthogonality, and indeed the projective repre-
sentation (8.9) of the angle between two rays measured from image points, are simply
specializations and a recapitulation of relations derived earlier in the book. For ex-
ample, we have already developed a projective representation (3.23–p82) of the angle
between two lines in 3-space, namely

(cid:19)

cos θ =

dT
1

(cid:19)

Ω∞d2
dT
2

dT
1

Ω∞d1

Ω∞d2

where d1 and d2 are the directions of the lines (which are the points at which the lines
intersect π∞). Rays are lines in 3-space which are coincident at the camera centre, and
so (3.23–p82) may be applied directly to rays. This is precisely what (8.9) does – it is
simply (3.23–p82) computed in the image.
Under the map (8.8) H = KR, which is the homography between the plane π∞ in
the world coordinate frame and the image plane, Ω∞ (cid:6)→ HTωH = (KR)Tω(KR) and
−1xi. Substituting these relations into (3.23–p82) gives (8.9).
di = H−1xi = (KR)
Similarly the conjugacy and pole–polar relations for orthogonality in the image are
a direct image of those on π∞, as can be seen by comparing ﬁgure 3.8(p83) with
ﬁgure 8.13.

In practice these orthogonality results ﬁnd greatest application in the case of vanish-

ing points and vanishing lines.

8.6 Vanishing points and vanishing lines

One of the distinguishing features of perspective projection is that the image of an
object that stretches off to inﬁnity can have ﬁnite extent. For example, an inﬁnite scene
line is imaged as a line terminating in a vanishing point. Similarly, parallel world lines,
such as railway lines, are imaged as converging lines, and their image intersection is
the vanishing point for the direction of the railway.

8.6.1 Vanishing points
The perspective geometry that gives rise to vanishing points is illustrated in ﬁgure 8.14.
It is evident that geometrically the vanishing point of a line is obtained by intersecting
the image plane with a ray parallel to the world line and passing through the camera
centre. Thus a vanishing point depends only on the direction of a line, not on its
position. Consequently a set of parallel world lines have a common vanishing point, as
illustrated in ﬁgure 8.16.

Algebraically the vanishing point may be obtained as a limiting point as follows:
Points on a line in 3-space through the point A and with direction D = (dT, 0)T are
written as X(λ) = A + λD, see ﬁgure 8.14b. As the parameter λ varies from 0 to ∞ the
point X(λ) varies from the ﬁnite point A to the point D at inﬁnity. Under a projective
camera P = K[I | 0], a point X(λ) is imaged at

x(λ) = PX(λ) =P A + λPD = a + λKd

where a is the image of A. Then the vanishing point v of the line is obtained as the

214

8 More Single View Geometry

x

v

x
/

v
/

C

D

X1

2

X
a

X

3

X

4

X

d

X( λ )

A

X(  )λ

d

C

v

b

(a) Plane to line camera. The points Xi, i = 1, . . . , 4 are
Fig. 8.14. Vanishing point formation.
equally spaced on the world line, but their spacing on the image line monotonically decreases. In the
limit X → ∞ the world point is imaged at x = v on the vertical image line, and at x(cid:1) = v(cid:1)
on the
inclined image line. Thus the vanishing point of the world line is obtained by intersecting the image
plane with a ray parallel to the world line through the camera centre C. (b) 3-space to plane camera.
The vanishing point, v, of a line with direction d is the intersection of the image plane with a ray parallel
to d through C. The world line may be parametrized as X(λ) = A + λD, where A is a point on the
line, and D = (dT, 0)T.

limit

v = lim

λ→∞ x(λ) = lim

λ→∞ (a + λKd) = Kd.

From result 8.15, v = Kd means that the vanishing point v back-projects to a ray with
direction d. Note that v depends only on the direction d of the line, not on its position
speciﬁed by A.

In the language of projective geometry this result is obtained directly: In projective
3-space the plane at inﬁnity π∞ is the plane of directions, and all lines with the same
direction intersect π∞ in the same point (see chapter 3). The vanishing point is simply
the image of this intersection. Thus if a line has direction d, then it intersects π∞ in
the point X∞ = (dT, 0)T. Then v is the image of X∞

(cid:20)

(cid:21)

d
0

v = PX∞ = K[I | 0]

= Kd.

To summarize:

Result 8.20. The vanishing point of lines with direction d in 3-space is the intersection

8.6 Vanishing points and vanishing lines

215
v of the image plane with a ray through the camera centre with direction d, namely
v = Kd.
Note, lines parallel to the image plane are imaged as parallel lines, since v is at inﬁnity
in the image. However, the converse – that parallel image lines are the image of parallel
scene lines – does not hold since lines which intersect on the principal plane are imaged
as parallel lines.

Example 8.21. Camera rotation from vanishing points
Vanishing points are images of points at inﬁnity, and provide orientation (attitude) in-
formation in a similar manner to that provided by the ﬁxed stars. Consider two images
of a scene obtained by calibrated cameras, where the two cameras differ in orientation
and position. The points at inﬁnity are part of the scene and so are independent of the
camera. Their images, the vanishing points, are not affected by the change in camera
position, but are affected by the camera rotation. Suppose both cameras have the same
calibration matrix K, and the camera rotates by R between views.
Let a scene line have vanishing point vi in the ﬁrst view, and v(cid:2)

i in the second. The
vanishing point vi has direction di measured in the ﬁrst camera’s Euclidean coordi-
nate frame, and the corresponding vanishing point v(cid:2)
i measured in the
second camera’s Euclidean coordinate frame. These directions can be computed from
the vanishing points, for example di = K−1vi/(cid:10)K−1vi(cid:10), where the normalizing fac-
tor (cid:10)K−1vi(cid:10) is included to ensure that di is a unit vector. The directions di and d(cid:2)
are related by the camera rotation as d(cid:2)
i = Rdi, which represents two independent con-
straints on R. Thus the rotation matrix R can be computed from two such corresponding
(cid:2)
directions.

i has direction d(cid:2)

i

The angle between two scene lines. We have seen that the vanishing point of a scene
line back-projects to a ray parallel to the scene line. Consequently (8.9), which de-
termines the angle between rays back-projected from image points, enables the angle
between the directions of two scene lines to be measured from their vanishing points:
Result 8.22. Let v1 and v2 be the vanishing points of two lines in an image, and let ω
be the image of the absolute conic in the image. If θ is the angle between the two line
directions, then

.

ωv2

(8.13)

(cid:19)

cos θ =

(cid:19)

vT
1
ωv1

ωv2
vT
2

vT
1

A note on computing vanishing points
Often vanishing points are computed from the image of a set of parallel line segments,
though they may be determined in other ways for example by using equal length in-
tervals on a line as described in example 2.18(p50) and example 2.20(p51).
In the
case of imaged parallel line segments the objective is to estimate their common image
intersection – which is the image of the direction of the parallel scene lines. Due to
measurement noise the imaged line segments will generally not intersect in a unique

216

8 More Single View Geometry

v

a

b

c

Fig. 8.15. ML estimate of a vanishing point from imaged parallel scene lines. (a) Estimating the
vanishing point v involves ﬁtting a line (shown thin here) through v to each measured line (shown thick
here). The ML estimate of v is the point which minimizes the sum of squared orthogonal distances
between the ﬁtted lines and the measured lines’ end points. (b) Measured line segments are shown in
white, and ﬁtted lines in black. (c) A close-up of the dashed square in (b). Note the very slight angle
between the measured and ﬁtted lines.

point. Commonly the vanishing point is then computed by intersecting the lines pair-
wise and using the centroid of these intersections, or ﬁnding the closest point to all the
measured lines. However, these are not optimal procedures.

Under the assumption of Gaussian measurement noise, the maximum likelihood es-
timate (MLE) of the vanishing point and line segments is computed by determining a
set of lines that do intersect in a single point, and which minimize the sum of squared
orthogonal distances from the endpoints of the measured line segments as shown in
ﬁgure 8.15(a). This minimization may be computed numerically using the Levenberg–
Marquardt algorithm (section A6.2(p600)). Note that if the lines are deﬁned by ﬁtting
to many points, rather than just their end points, one can use the method described in
section 16.7.2(p404) to reduce each line to an equivalent pair of weighted end points
which can then be used in this algorithm. Figure 8.15(b)(c) shows an example of a
vanishing point computed in this manner. It is evident that the residuals between the
measured and ﬁtted lines are very small.

8.6.2 Vanishing lines
Parallel planes in 3-space intersect π∞ in a common line, and the image of this line
is the vanishing line of the plane. Geometrically the vanishing line is constructed,
as shown in ﬁgure 8.16, by intersecting the image with a plane parallel to the scene
plane through the camera centre. It is clear that a vanishing line depends only on the
orientation of the scene plane; it does not depend on its position. Since lines parallel
to a plane intersect the plane at π∞, it is easily seen that the vanishing point of a
line parallel to a plane lies on the vanishing line of the plane. An example is shown
in ﬁgure 8.17.

If the camera calibration K is known then a scene plane’s vanishing line may be used

to determine information about the plane, and we mention three examples here:

(i) The plane’s orientation relative to the camera may be determined from its van-
ishing line. From result 8.16 a plane through the camera centre with normal

8.6 Vanishing points and vanishing lines

217

v

1

v

2

l

a

l

b

n

C

n

π

Fig. 8.16. Vanishing line formation. (a) The two sets of parallel lines on the scene plane converge to
the vanishing points v1 and v2 in the image. The line l through v1 and v2 is the vanishing line of the
plane. (b) The vanishing line l of a plane π is obtained by intersecting the image plane with a plane
through the camera centre C and parallel to π.

a

b

c

Fig. 8.17. Vanishing points and lines. The vanishing line of the ground plane (the horizon) of the
corridor may be obtained from two sets of parallel lines on the plane. (a) The vanishing points of lines
which are nearly parallel to the image plane are distant from the ﬁnite (actual) image. (b) Note the
monotonic decrease in the spacing of the imaged equally spaced parallel lines corresponding to the
sides of the ﬂoor tiles. (c) The vanishing point of lines parallel to a plane (here the ground plane) lies
on the vanishing line of the plane.

218

8 More Single View Geometry

direction n intersects the image plane in the line l = K−Tn. Consequently, l
is the vanishing line of planes perpendicular to n. Thus a plane with vanishing
line l has orientation n = KTl in the camera’s Euclidean coordinate frame.

(ii) The plane may be metrically rectiﬁed given only its vanishing line. This can be
seen by considering a synthetic rotation of the camera in the manner of example
8.13(p205). Since the plane normal is known from the vanishing line, the cam-
era can be synthetically rotated by a homography so that the plane is fronto-
parallel (i.e. parallel to the image plane). The computation of this homography
is discussed in exercise (ix).

(iii) The angle between two scene planes can be determined from their vanishing
lines. Suppose the vanishing lines are l1 and l2, then the angle θ between the
planes is given by

(cid:19)

(cid:19)
ω∗l2
lT
2

lT
1
ω∗l1

lT
1

.

ω∗l2

(8.14)

cos θ =

The proof is left as an exercise.

Computing vanishing lines
A common way to determine a vanishing line of a scene plane is ﬁrst to determine
vanishing points for two sets of lines parallel to the plane, and then to construct the
line through the two vanishing points. This construction is illustrated in ﬁgure 8.17.
Alternative methods of determining vanishing points are shown in example 2.19(p51)
and example 2.20(p51).

However, the vanishing line may be determined directly, without using vanishing
points as an intermediate step. For example, the vanishing line may be computed given
an imaged set of equally spaced coplanar parallel lines. This is a useful method in
practice because such sets commonly occur in man-made structures, such as: stairs,
windows on the wall of a building, fences, radiators and zebra crossings. The following
example illustrates the projective geometry involved.
Example 8.23. The vanishing line given the image of three coplanar equally spaced
parallel lines
A set of equally spaced lines on the scene plane may be represented as ax
+
λ = 0, where λ takes integer values. This set (a pencil) of lines may be written as
l(cid:2)
n = (a, b, n)T = (a, b, 0)T + n(0, 0, 1)T, where (0, 0, 1)T is the line at inﬁnity on the
scene plane. Under perspective imaging the point transformation is x = Hx(cid:2)
, and the
corresponding line map is ln = H−Tl(cid:2)
n = l0 + nl, where l, the image of (0, 0, 1)T, is the
vanishing line of the plane. The imaged geometry is shown in ﬁgure 8.18(c). Note all
lines ln intersect in a common vanishing point (which is given by li× lj, for i (cid:5)= j) and
the spacing decreases monotonically with n. The vanishing line l may be determined
from three lines of the set provided their index (n) is identiﬁed. For example, from
the image of three equally spaced lines, l0, l1 and l2, the closed form solution for the
vanishing line is:

+ by

(cid:2)

(cid:2)

l =

(l0 × l2)T(l1 × l2)

l1 + 2

(l0 × l1)T(l2 × l1)

l2.

(8.15)

(cid:16)

(cid:15)

(cid:16)

(cid:15)

8.6 Vanishing points and vanishing lines

219

l

a

l

3

l

n

l

2

l

1

l

0

l

c

b

v
 

 


Fig. 8.18. Determining a planes vanishing line from imaged equally spaced parallel lines. (a) Image
of a vertical fence with equally spaced bars. (b) The computed vanishing line l from three equally spaced
bars (12 apart). Note the vanishing point of the horizontal lines lies on this vanishing line. (c) The
spacing between the imaged lines ln monotonically decreases with n.

The proof is left as an exercise. Figure 8.18(b) shows a vanishing line computed in this
(cid:2)
way.

8.6.3 Orthogonality relationships amongst vanishing points and lines
It is often the case in practice that the lines and planes giving rise to vanishing points
are orthogonal. In this case there are particularly simple relationships amongst their
vanishing points and lines involving ω, and furthermore these relations can be used to
(partially) determine ω, and consequently the camera calibration K as will be seen in
section 8.8.

It follows from (8.13) that the vanishing points, v1, v2, of two perpendicular world
lines satisfy vT
ωv2 = 0. This means that the vanishing points are conjugate with
1
respect to ω, as illustrated in ﬁgure 8.13. Similarly it follows from result 8.19 that the
vanishing point v of a direction perpendicular to a plane with vanishing line l satisﬁes
l = ωv. This means that the vanishing point and line are in a pole–polar relation with
respect to ω, as is also illustrated in ﬁgure 8.13. Summarizing these image relations:

(i) The vanishing points of lines with perpendicular directions satisfy

vT
1

ωv2 = 0.

(8.16)

(ii) If a line is perpendicular to a plane then their respective vanishing point v and

vanishing line l are related by

l = ωv

(8.17)

and inversely v = ω∗l.

(iii) The vanishing lines of two perpendicular planes satisfy lT
1

ω∗l2 = 0.

220

8 More Single View Geometry

  

  


 

 


C

 v

image 
plane

l

a

π

 

 

 


 
l

 

 


b

π

  

 

 


 

  

  


Fig. 8.19. Geometry of a vertical vanishing point and ground plane vanishing line. (a) The vertical
vanishing point v is the image of the vertical “footprint” of the camera centre on the ground plane π. (b)
The vanishing line l partitions all points in scene space. Any scene point projecting onto the vanishing
line is at the same distance from the plane π as the camera centre; if it lies “above” the line it is farther
from the plane, and if “below” then it is closer to the plane than the camera centre.

For example, suppose the vanishing line l of the ground plane (the horizon) is identiﬁed
in an image, and the internal calibration matrix K is known, then the vertical vanishing
point v (which is the vanishing point of the normal direction to the plane) may be
obtained from v = ω∗l.

8.7 Afﬁne 3D measurements and reconstruction

It has been seen in section 2.7.2(p49) that identifying a scene plane’s vanishing line
allows afﬁne properties of the scene plane to be measured. If in addition a vanishing
point for a direction not parallel to the plane is identiﬁed, then afﬁne properties can
be computed for the 3-space of the perspectively imaged scene. We will illustrate this
idea for the case where the vanishing point corresponds to a direction orthogonal to
the plane, although orthogonality is not necessary for the construction. The method
described in this section does not require that the internal calibration of the camera K
be known.

It will be convenient to think of the scene plane as the horizontal ground plane, in
which case the vanishing line is the horizon. Similarly, it will be convenient to think of
the direction orthogonal to the scene plane as vertical, so that v is the vertical vanishing
point. This situation is illustrated in ﬁgure 8.19.

Suppose we wish to measure the relative lengths of two line segments in the vertical

direction as shown in ﬁgure 8.20(a). We will show the following result:

Result 8.24. Given the vanishing line of the ground plane l and the vertical vanishing
point v, then the relative length of vertical line segments can be measured provided
their end point lies on the ground plane.

Clearly the relative lengths cannot be measured directly from their imaged lengths
because as a vertical line recedes deeper into the scene (i.e. further from the camera)
then its imaged length decreases. The construction to determine the relative lengths
proceeds in two steps:

8.7 Afﬁne 3D measurements and reconstruction

221

 
T2
 


L2

 

 


2B

l

 

 


u
 

 


image

 

 


T1
L1

 

B1
 


a

v
  

l1

t1

 

l2
 

 

  

  


2t

t1

  

  


1b

  

2b

c

  

T2
  

 

 


T1

d
2

  

2B

b

l 3
2t
1t

  

 

  

2b

d

image

T1
  

d
1

  

B1
  


  

v
  


l1

l2

  

2t
  

  

  

1t

t1

 

 


 

1b

Fig. 8.20. Computing length ratios of parallel scene lines. (a) 3D geometry: The vertical line seg-
ments L1 = (cid:8)B1, T1(cid:9) and L2 = (cid:8)B2, T2(cid:9) have length d1 and d2 respectively. The base points B1, B2
are on the ground plane. We wish to compute the scene length ratio d1 : d2 from the imaged conﬁgu-
ration. (b) In the scene the length of the line segment L1 may be transferred to L2 by constructing a

line parallel to the ground plane to generate the point (cid:22)T1. (c) Image geometry: l is the ground plane
˜t1 (the image of (cid:22)T1) by the intersection of l2 and the line (cid:8)t1, u(cid:9). (d) The line l3 is parallel to l1 in

vanishing line, and v the vertical vanishing point. A corresponding parallel line construction in the
image requires ﬁrst determining the vanishing point u from the images bi of Bi, and then determining
the image. The points ˆt1 and ˆt2 are constructed by intersecting l3 with the lines (cid:8)t1, ˜t1(cid:9) and (cid:8)t1, t2(cid:9)
respectively. The distance ratio d(b2, ˆt1) : d(b2, ˆt2) is the computed estimate of d1 : d2.

Step 1: Map the length of one line segment onto the other.
In 3D the length of
L1 may be compared to L2 by constructing a line parallel to the ground plane in the

direction (cid:8)B1, B2(cid:9) that transfers T1 onto L2. This transferred point will be denoted (cid:22)T1
(see ﬁgure 8.20(b)). In the image a corresponding construction is carried out by ﬁrst
determining the vanishing point u which is the intersection of (cid:8)b1, b2(cid:9) with l. Now
any scene line parallel to (cid:8)B1, B2(cid:9) is imaged as a line through u, so in particular the
intersection of the line (cid:8)t1, u(cid:9) with l2 deﬁnes the image ˜t1 of the transferred point (cid:22)T1
image of the line through T1 parallel to (cid:8)B1, B2(cid:9) is the line through t1 and u. The

(see ﬁgure 8.20(c)).

Step 2: Determine the ratio of lengths on the scene line. We now have four
collinear points on an imaged scene line and wish to determine the actual length ra-
tio in the scene. The four collinear image points are b2, ˜t1, t2 and v. These may be
treated as images of scene points at distances 0, d1, d2 and ∞, respectively, along the
scene line. The afﬁne ratio d1 : d2 may be obtained by applying a projective transfor-

222

8 More Single View Geometry

Objective
Given the vanishing line of the ground plane l and the vertical vanishing point v and the top
(t1, t2) and base (b1, b2) points of two line segments as in ﬁgure 8.20, compute the ratio of
lengths of the line segments in the scene.

Algorithm

(i) Compute the vanishing point u = (b1 × b2) × l.
(ii) Compute the transferred point ˜t1 = (t1 × u) × l2 (where l2 = v × b2).
(iii) Represent the four points b2, ˜t1, t2 and v on the image line l1 by their distance from

b2, as0, ˜t1, t2 and v respectively.
(iv) Compute a 1D projective transformation H2×2 mapping homogeneous coordinates
(0, 1) (cid:6)→ (0, 1) and (v, 1) (cid:6)→ (1, 0) (which maps the vanishing point v to inﬁnity).
A suitable matrix is given by

(v) The (scaled) distance of the scene points (cid:22)T1 and T2 from B2 on L2 may then be ob-

tained from the position of the points H2×2(˜t1, 1)T and H2×2(t2, 1)T. Their distance
ratio is then given by

H2×2 =

1
0
1 −v

(cid:24)

(cid:25)

.

˜t1(v − t2)
t2(v − ˜t1)

d1
d2

=

Algorithm 8.1. Computing scene length ratios from a single image.

mation to the image line which maps v to inﬁnity. A geometric construction of this
projectivity is shown in ﬁgure 8.20(d) (see example 2.20(p51)).

Details of the algorithm to carry out these two steps are given in algorithm 8.1.
Note, no knowledge of the camera calibration K or pose is necessary to apply the
algorithm. In fact, the position of the camera centre relative to the ground plane can
also be computed. The algorithm is well conditioned even when the vanishing point
and/or line are at inﬁnity in the image. For example, under afﬁne image conditionings,
or if the image plane is parallel to the vertical scene direction (so that v is at inﬁnity).
In these cases the distance ratio simpliﬁes to d1

d2 = ˜t1
t2 .

Example 8.25. Measuring a person’s height in a single image
Suppose we have an image which contains sufﬁcient information to compute the
ground plane vanishing line and the vertical vanishing point, and also one object of
known height for which the top and base are imaged. Then the height of a person
standing on the ground plane can be measured anywhere in the scene provided that
their head and feet are both visible. Figure 8.21(a) shows an example. The scene con-
tains plenty of horizontal lines from which to compute a horizontal vanishing point.
Two such vanishing points determine the vanishing line of the ﬂoor (which is the hori-
zon for this image). The scene also contains plenty of vertical lines from which to
compute a vertical vanishing point (ﬁgure 8.21(c)). Assuming that the two people
are standing vertically, then their relative height may be computed directly from their
length ratio using algorithm 8.1. Their absolute height may be determined by comput-

8.8 Determining camera calibration K from a single view

223

a

c

b

d

Fig. 8.21. Height measurements using afﬁne properties. (a) The original image. We wish to measure
the height of the two people. (b) The image after radial distortion correction (see section 7.4(p189)).
(c) The vanishing line (shown) is computed from two vanishing points corresponding to horizontal di-
rections. The lines used to compute the vertical vanishing points are also shown. The vertical vanishing
point is not shown since it lies well below the image. (d) Using the known height of the ﬁling cabinet on
the left of the image, the absolute height of the two people are measured as described in algorithm 8.1.
The measured heights are within 2cm of ground truth. The computation of the uncertainty is described
in [Criminisi-00].

ing their height relative to an object on the ground plane with known height. Here the
known height is provided by the ﬁling cabinet. The result is shown in ﬁgure 8.21(d).(cid:2)

8.8 Determining camera calibration K from a single view

We have seen that once ω is known the angle between rays can be measured. Con-
versely if the angle between rays is known then a constraint is placed on ω. Each
known angle between two rays gives a constraint of the form (8.13) on ω. Unfor-
tunately, for arbitrary angles, and known v1 and v2, this gives a quadratic constraint
on the entries of ω. If the lines are perpendicular, however, (8.13) reduces to (8.16)
vT
ωv2 = 0, and the constraint on ω is linear.
1
A linear constraint on ω also results from a vanishing point and vanishing line arising
from a line and its orthogonal plane. A common example is a vertical direction and
horizontal plane as in ﬁgure 8.19. From (8.17) l = ωv. Writing this as l × (ωv) =0

224

8 More Single View Geometry

Condition

vanishing points v1, v2
corresponding to orthogonal lines
vanishing point v and vanishing
line l corresponding to
orthogonal line and plane

constraint

vT
1 ωv2 = 0

[l]×ωv = 0

metric plane imaged with known
homography H = [h1, h2, h3]

hT
1 ωh2 = 0
hT
1 ωh1 = hT

2 ωh2

zero skew

square pixels

ω12 = ω21 = 0

ω12 = ω21 = 0
ω11 = ω22

type

# constraints

linear

linear

linear

linear

linear

1

2

2

1

2

Table 8.1. Scene and internal constraints on ω.

removes the homogeneous scaling factor and results in three homogeneous equations
linear in the entries of ω. These are equivalent to two independent constraints on ω.

All these conditions provide linear constraints on ω. Given a sufﬁcient number of
such constraints ω may be computed and hence the camera calibration K also follows
since ω = (KKT)

−1.

The number of entries of ω that need be determined from scene constraints of this
sort can be reduced if the calibration matrix K has a more specialized form than (6.10–
In the case where K is known to have zero skew (s = 0), or square pixels
p157).
(αx = αy and s = 0), we can take advantage of this condition to help ﬁnd ω. In
particular, it is quickly veriﬁed by direct computation that:

Result 8.26. If s = K12 = 0 then ω12 = ω21 = 0 . If in addition αx = K11 = K22 = αy,
then ω11 = ω22.

Thus, in solving for the image of the absolute conic, one may easily take into account
the zero-skew or square-aspect ratio constraint on the camera, if such a constraint is
known to exist. One may also verify that no such simple connection as result 8.26
exists between the entries of K and those of ω∗

= KKT.
We have now seen three sources of constraints on ω:

(i) metric information on a plane imaged with a known homography, see (8.12–

p211)

(ii) vanishing points and lines corresponding to perpendicular directions and

planes, (8.16)

(iii) “internal constraints” such as zero skew or square pixels, as in result 8.26

These constraints are summarized in table 8.1. We now describe how these constraints
may be combined to estimate ω and thence K.

Since all the above constraints (including the internal constraints) are described al-
gebraically as linear equations on ω, it is a simple matter to combine them as rows of

8.8 Determining camera calibration K from a single view

225

Objective
Compute K via ω by combining scene and internal constraints.

Algorithm

(i) Represent ω as a homogeneous 6-vector w = (w1, w2, w3, w4, w5, w6)T where:

(cid:17)

(cid:18)

ω =

w1 w2 w4
w2 w3 w5
w4 w5 w6

(ii) Each available constraint from table 8.1 may be written as aTw = 0. For exam-
ple, for the orthogonality constraint uTωv = 0, where u = (u1, u2, u3)T and
v = (v1, v2, v3)T, the 6-vector a is given by

a = (v1u1, v1u2 + v2u1, v2u2, v1u3 + v3u1, v2u3 + v3u2, v3u3)T.

Similar constraints vectors are obtained from the other sources of scene and internal
constraints. For example a metric plane generates two such constraints.
n × 6 matrix for n constraints.

(iii) Stack the equations aTw = 0 from each constraint in the form Aw = 0, where A is a

(iv) Solve for w using the SVD as in algorithm 4.2(p109). This determines ω.
(v) Decompose ω into K using matrix inversion and Cholesky factorization (see section

A4.2.1(p582)).

Algorithm 8.2. Computing K from scene and internal constraints.

a constraint matrix. All constraints may be collected together so that for n constraints
the system of equations may be written as Aw = 0, where A is a n × 6 matrix and w
is a 6-vector containing the six distinct homogeneous entries of ω. With a minimum
of 5 constraint equations an exact solution is found. With more than ﬁve equations, a
least-squares solution is found by algorithm A5.4(p593). The method is summarized
in algorithm 8.2.

With more than the minimum required ﬁve constraints, we have the option to apply
some of the constraints as hard constraints – that is, constraints that will be satisﬁed
exactly. This can be done by parametrizing ω so that the constraints are satisﬁed ex-
plicitly (for instance setting ω21 = ω12 = 0 for the zero skew constraint, and also
ω11 = ω22 for the square-pixel constraint). The minimization method of algorithm
A5.5(p594) may also be used to enforce hard constraints. Otherwise, treating all con-
straints as soft constraints and using algorithm A5.4(p593) will produce a solution in
which the constraints are not satisﬁed exactly in the presence of noise – for instance,
pixels may not be quite square.

Finally, an important issue in practice is that of degeneracy. This occurs when the
combined constraints are not independent and results in the matrix A dropping rank.
If the rank is less than the number of unknowns, then a parametrized family of so-
lutions for ω (and hence K) is obtained. Also, if conditions are near degenerate then
the solution is ill-conditioned and the particular member of the family is determined
by “noise”. These degeneracies can often be understood geometrically – for example
in example 8.18 if the three metric planes are parallel then the three pairs of imaged

226

8 More Single View Geometry

a

b


  

  

  

  

  


c


 

 

 

Fig. 8.22. For the case that image skew is zero and the aspect ratio unity the principal point is the
orthocentre of an orthogonal triad of vanishing points. (a) Original image. (b) Three sets of parallel
lines in the scene, with each set having direction orthogonal to the others. (c) The principal point is the
orthocentre of the triangle with the vanishing points as vertices.

circular points are coincident and only provide a total of two constraints instead of six.
A pragmatic solution to the problem of degeneracy, popularized by Zhang [Zhang-00],
is to image a metric plane many times in varying positions. This reduces the chances
of degeneracy occurring, and also provides a very over-determined solution.

Example 8.27. Calibration from three orthogonal vanishing points
Suppose that it is known that the camera has zero skew, and that the pixels are square
(or equivalently their aspect ratio is known). A triad of orthogonal vanishing point
directions supplies three more constraints. This gives a total of 5 constraints – sufﬁcient
to compute ω, and hence K.

In outline the algorithm has the following steps:

(i) In the case of square pixels ω has the form

 w1

ω =

0 w2
0 w1 w3
w2 w3 w4

 .

(ii) Each pair of vanishing points vi, vj generates an equation vT
i

ωvj = 0, which
is linear in the elements of ω. The constraints from the three pairs of vanishing
points are stacked together to form an equation Aw = 0, where A is a 3 × 4
matrix.

(iii) The vector w is obtained as the null vector of A, and this determines ω. The ma-
−1 by Cholesky factorization of ω, followed

trix K is obtained from ω = (KKT)
by inversion.

An example is shown in ﬁgure 8.22(a). Vanishing points are computed corre-
sponding to the three perpendicular directions shown in ﬁgure 8.22(b). The image
is 1024 × 768 pixels, and the calibration matrix is computed to be

(cid:2)

 1163

0
0

K =

 .

0

548
1163 404
1

0

8.8 Determining camera calibration K from a single view

227

image
plane

l

2

v
3

l

p

l

1

n

v
3

l

C

p

x

3l

π

a

v
1

x

l

3
b

v
2

Fig. 8.23. Geometric construction of the principal point. The vanishing line l3 back-projects to a
plane π with normal n. The vanishing point v3 back-projects to a line orthogonal to the plane π. (a)
The normal n of the plane π through the camera centre C and the principal axis deﬁne a plane, which
intersects the image in the line l = (cid:8)v3, x(cid:9). The line l3 is the intersection of π with the image plane,
and is also its vanishing line. The point v3 is the intersection of the normal with the image plane, and
is also its vanishing point. Clearly the principal point lies on l, and l and l3 are perpendicular on the
image plane. (b) The principal point may be determined from three such constraints as the orthocentre
of the triangle.

v3

p

x

α

C

a

α

a

v3

l

p

x

b

α

b

Fig. 8.24. Geometric construction of the focal length. (a) Consider the plane deﬁned by the camera
centre C, principal point and one of the vanishing points, e.g. v3 as shown in ﬁgure 8.23(a). The rays
from C to v3 and x are perpendicular to each other. The focal length, α, is the distance from the camera
centre to the image plane. By similar triangles, α2 = d(p, v3)d(p, x), where d(u, v) is the distance
between the points u and v. (b) In the image a circle is drawn with diameter the line between v3 and
x. A line through p perpendicular to (cid:8)v3, x(cid:9) meets the circle in two points a and b. The focal length
equals the distance d(p, a).

The principal point and focal length may also be computed geometrically in this
case. The principal point is the orthocentre of the triangle with vertices the vanishing
points. Figure 8.23 shows that the principal point lies on the perpendicular line from
one triangle side to the opposite vertex. A similar construction for the other two sides
shows that the principal point is the orthocentre. An algebraic derivation of this result
is left to the exercises. The focal length can also be computed geometrically as shown
in ﬁgure 8.24.

As a cautionary note, this estimation method is degenerate if one of the vanishing

228

8 More Single View Geometry

a

b

Fig. 8.25. Plane rectiﬁcation via partial internal parameters (a) Original image. (b) Rectiﬁcation
assuming the camera has square pixels and principal point at the centre of the image. The focal length
is computed from the single orthogonal vanishing point pair. The aspect ratio of a window in the rectiﬁed
image differs from the ground truth value by 3.7%. Note that the two parallel planes, the upper building
facade and the lower shopfront, are both mapped to fronto-parallel planes.

points, say the vertical, is at inﬁnity. In this case A drops rank to two, and there is a
one-parameter family of solutions for ω and correspondingly for K. This degeneracy
can be seen geometrically from the orthocentre construction of ﬁgure 8.23. If v3 is at
inﬁnity then the principal point p lies on the line l3 = (cid:8)v1, v2(cid:9), but its x position is not
deﬁned.

Example 8.28. Determining the focal length when the other internal parameters
are known
We consider a further example of calibration from a single view. Suppose that it is
known that the camera has zero skew, that the pixels are square (or equivalently their
aspect ratio is known), and also that the principal point is at the image centre. Then only
the focal length is unknown. In this case, the form of ω is very simple: it is a diagonal
matrix diag(1/f 2, 1/f 2, 1) with only one degree of freedom. Using algorithm 8.2, the
focal length f may be determined from one further constraint, such as the one arising
from two vanishing points corresponding to orthogonal directions.

An example is shown in ﬁgure 8.25(a). Here the vanishing points used in the con-
straint are computed from the horizontal edges of the windows and pavement, and the
vertical edges of the windows. These vanishing points also determine the vanishing
line l of the building facade. Given K and the vanishing line l, the camera can be syn-
thetically rotated such that the facade is fronto-parallel by mapping the image with a
homography as in example 8.13(p205). The result is shown in ﬁgure 8.25(b). Note,
in example 8.13 it was necessary to know the aspect ratio of a rectangle on the scene
plane in order to rectify the plane. Here it is only necessary to know the vanishing
line of the plane because the camera calibration K provides the additional information
(cid:2)
required for the homography.

8.8.1 The geometry of the constraints
Although the algebraic constraints given in table 8.1 appear to arise from distinct
sources, they are in fact all equivalent to one of two simple geometric relations: two
points lying on the conic ω, or conjugacy of two points with respect to ω.

For example, the zero skew constraint is an orthogonality constraint: it speciﬁes that

8.9 Single view reconstruction

229

the image x and y axes are orthogonal. These axes correspond to rays with directions
in the camera’s Euclidean coordinate frame, (1, 0, 0)T and (0, 1, 0)T, respectively, that
are imaged at vx = (1, 0, 0)T and vy = (0, 1, 0)T (since the rays are parallel to the
image plane). The zero skew constraint ω12 = ω21 = 0 is just another way of writing
the orthogonality constraint (8.16) vT
ωvx = 0. Geometrically skew zero is equivalent
y
to conjugacy of the points (1, 0, 0)T and (0, 1, 0)T with respect to ω.

The square pixel constraint may be interpreted in two ways. A square has the prop-
erty of deﬁning two sets of orthogonal lines: adjacent edges are orthogonal, and so
are the two diagonals. Thus, the square pixel constraint may be interpreted as a pair
of orthogonal line constraints. The diagonal vanishing points of a square pixel are
(1, 1, 0)T and (−1, 1, 0)T. The resulting orthogonality constraints lead to the square
pixel constraints given in table 8.1.

Alternatively, the square pixel constraint can be interpreted in terms of two known
points lying on the IAC. If the image plane has square pixels, then it has a Euclidean
coordinate system and the circular points have known coordinates (1,±i, 0)T. It may be
veriﬁed that the two square pixel equations are equivalent to (1,±i, 0)ω(1,±i, 0)T = 0.
This is the most important geometric equivalence. In essence an image plane with
square pixels acts as a metric plane in the scene. A square pixel image plane is equiv-
alent to a metric plane imaged with a homography given by the identity. Indeed if
the homography H in the “metric plane imaged with known homography” constraint of
table 8.1 is replaced by the identity then the square pixel constraints are immediately
obtained.

Thus, we see that all the constraints given in table 8.1 are derived either from known
points lying on ω, or from pairs of points that are conjugate with respect to ω. Deter-
mining ω may therefore be viewed as a conic ﬁtting problem, given points on the conic
and conjugate point pairs.

It is well to bear in mind that conic ﬁtting is a delicate problem, often unstable
([Bookstein-79]) if the points are not well distributed on the conic. The same observa-
tion is true of the present problem, which we have seen is equivalent to conic ﬁtting.
The method given in algorithm 8.2 for ﬁnding the calibration from vanishing points
amounts to minimization of algebraic error, and therefore does not give an optimal so-
lution. For greater accuracy, the methods of chapter 4, for instance the Sampson error
method of section 4.2.6(p98) should be used.

8.9 Single view reconstruction

As an application of the methods developed in this chapter we demonstrate now the
3D reconstruction of a texture mapped piecewise planar graphical model from a single
image. The camera calibration methods of section 8.8 and the rectiﬁcation method of
example 8.28 may be combined to back project image regions to texture the planes of
the model.

The method will be illustrated for the image of ﬁgure 8.26(a), where the scene con-
tains three dominant and mutually orthogonal planes: the building facades on the left
and right and the ground plane. The parallel line sets in three orthogonal directions de-

230

8 More Single View Geometry

a

b

c

Fig. 8.26. Single view reconstruction. (a) Original image of the Fellows quad, Merton College, Oxford.
(b) (c) Views of the 3D model created from the single image. The vanishing line of the roof planes is
computed from the repetition of the texture pattern.

ﬁne three vanishing points and together with the constraint of square pixels the camera
calibration may be computed using the method described in section 8.8. From the van-
ishing lines of the three planes, likewise determined by the vanishing points, together
with the computed ω, homographies may be computed to texture map the appropriate
image regions onto the orthogonal planes of the model.

In more detail taking the left facade as a reference plane in ﬁgure 8.26(a), its correctly
proportioned width and height are determined by the rectiﬁcation. The right facade
and ground planes deﬁne 3D planes orthogonal to the reference (we have assumed
the orthogonality of the planes in computing the camera, so relative orientations are
deﬁned). Scaling of the right and ground planes is computed from the points common
to the planes and this completes a three orthogonal plane model.

Having computed the calibration, the relative orientation of planes in the scene that
are not orthogonal (such as the roof) can be computed if their vanishing lines can be
found using (8.14–p218). Their relative positions and dimensions can be determined
if the intersection of a pair of planes is visible in the image, so that there are points
common to both planes. Relative size can be computed from the rectiﬁcation of a
distance between common points using the homographies of both planes. Views of the
model, with texture mapped correctly to the planes, appear in ﬁgure 8.26(b) and (c).

8.10 The calibrating conic

8.10 The calibrating conic

231

The image of the absolute conic (IAC) is an imaginary conic in an image, and hence
is not visible. Sometimes it is useful for visualization purposes to consider a different
conic that is closely related to the calibration of the camera. Such a conic is the cal-
ibrating conic, which is the image of a cone with apex angle 45
and axis coinciding
with the principal axis of the camera.

◦

◦

We wish to compute a formula for this cone in terms of the calibration matrix of the
camera. Since the 45
cone moves with the camera, its image is clearly independent
of the orientation and position of the camera. Thus, we may assume that the camera is
located at the origin and oriented directly along the Z-axis. Thus, let the camera matrix
be P = K[I | 0]. Now, any point on the 45
◦
cone satisﬁes X2 + Y2 = Z2. Points on this
cone map to points on the conic

 1

C = K−T

 K−1

1 −1

(8.18)

as one easily veriﬁes from result 8.6(p199). This conic will be referred to as the cal-
ibrating conic of the camera. For a calibrated camera with identity calibration matrix
K = I, the calibrating conic is a unit circle centred at the origin (which is the principal
point of the image). The conic of (8.18) is simply this unit circle transformed by an
afﬁne transformation according to the conic transformation rule of result 2.13(p37):
(C (cid:6)→ H−TCH−1). Thus the calibrating conic of a camera with calibration matrix K is the
afﬁne transformation of a unit circle centred on the origin by the matrix K.

The calibration parameters are easily read from the calibrating conic. The principal
point is the centre of the conic, and the scale factors and skew are easily identiﬁed, as in
ﬁgure 8.27. In the case of zero skew, the calibrating conic has its principal axes aligned
with the image coordinate axes. An example on a real image is shown in ﬁgure 8.29.

Example 8.29. Suppose K = diag(f, f, 1), which is the calibration matrix for a camera
of focal length f pixels, with no skew, square pixels, and image origin coincident
with the principal point. Then from (8.18) the calibrating conic is C = diag(1, 1,−f 2),
(cid:2)
which is a circle of radius f centred on the principal point.

In particular the rays corresponding to two points x and x(cid:2)

Orthogonality and the calibrating conic
A formula was given in (8.9–p210) for the angle between the rays corresponding to
two image points.
are
perpendicular when x(cid:2)Tωx = 0. As shown in ﬁgure 8.13(p212) this may be interpreted
as the point x(cid:2)
We wish to carry out a similar analysis in terms of the calibrating conic. Writing
C = K−TDK−1, where D = diag(1, 1,−1), we ﬁnd

lying on the line ωx, which is the polar of x with respect to the IAC.

C = (K−TK−1)(KDK−1) =ω S

where S = KDK−1. However, for any point x, the product Sx represents the reﬂection of

232

8 More Single View Geometry

α y

αx

( x   , y  )
0

0

s

αy

α x

( x   , y  )
0

0

a

b

Fig. 8.27. Reading the internal camera parameters K from the calibrating conic. (a) Skew s is zero.
(b) Skew s is non-zero. The skew parameter of K (see (6.10–p157), is given by the x-coordinate of the
highest point of the conic.

l

x

.
x

C

Fig. 8.28. To construct the line perpendicular to the ray through image point x proceed as follows: (i)
Reﬂect x through the centre of C to get point ˙x (i.e. at the same distance from the centre as x). (ii) The
desired line is the polar of ˙x.

the point x through the centre of the conic C, that is, the principal point of the camera.
Representing this reﬂected point by ˙x, one ﬁnds that
x(cid:2)Tωx = x(cid:2)TC ˙x .

(8.19)

This leads to the following geometric result:

Result 8.30. The line in an image corresponding to the plane perpendicular to a ray
through image point x is the polar C ˙x of the reﬂected point ˙x with respect to the cali-
brating conic.

This construction is illustrated in ﬁgure 8.28.

Example 8.31. The calibrating conic given three orthogonal vanishing points
The calibrating conic can be drawn directly for the example of ﬁgure 8.22. Again
assume there is no skew and square pixels, then the calibrating conic is a circle. Now
given three mutually perpendicular vanishing points, one can ﬁnd the calibrating conic
by direct geometric construction as shown in ﬁgure 8.29.

(i) First, construct the triangle with vertices the three vanishing points v1, v2 and

v3.

(ii) The centre of C is the orthocentre of the triangle.

8.11 Closure

233

calibrating 

conic

v1

v3

c

a

v1

v 2

calibrating 

conic

 

 

 


v

1

 v3

 
 

 


c

b

 

 


2v

Fig. 8.29. The calibrating conic computed from three orthogonal vanishing points. (a) The geometric
construction. (b) The calibrating conic for the image of ﬁgure 8.22.

(iii) Reﬂect one of the vanishing points (say v1) in the centre to get ˙v1.
(iv) The radius of C is determined by the condition that the polar of ˙v1 is the line

passing though v2 and v3.

(cid:2)

8.11 Closure

8.11.1 The literature
Faugeras and Mourrain [Faugeras-95a], and Faugeras and Papadopoulo [Faugeras-97]
develop the projection of lines using Pl¨ucker coordinates. Koenderink [Koenderink-84,
Koenderink-90], and Giblin and Weiss [Giblin-87] give many properties of the contour
generator and apparent contour, and their relation to the differential geometry of sur-
faces.

[Kanatani-92] gives an alternative, calibrated, treatment of vanishing points and
lines, and of the result that images acquired by cameras with the same centre are re-
lated by a planar homography. Mundy and Zisserman [Mundy-92] showed this result
geometrically, and [Hartley-94a] gave a simple algebraic derivation based on camera
projection matrices. [Faugeras-92b] introduced the projective (reduced) camera ma-
trix. The link between the image of the absolute conic and camera calibration was ﬁrst
given in [Faugeras-92a].

The computation of panoramic mosaics is described in [Capel-98, Sawhney-98,
Szeliski-97]. The ML method of computing vanishing points is given in Liebowitz &
Zisserman [Liebowitz-98]. Applications of automatic vanishing line estimation from
coplanar equally spaced lines are given in [Schaffalitzky-00b] and also [Se-00]. Afﬁne
3D measurements from a single view is described in [Criminisi-00, Proesmans-98].

The result that K may be computed from multiple scene planes on which metric
structure (such as a square) is known was given in [Liebowitz-98]. Algorithms for
this computation are given in [Liebowitz-99a, Sturm-99c, Zhang-00]. The advantage
in using ω, rather than ω∗
, when imposing the skew zero constraint was ﬁrst noted
in [Armstrong-96b]. The method of internal calibration using three vanishing points
for orthogonal directions was given by Caprile and Torre [Caprile-90], though there

234

8 More Single View Geometry

is an earlier reference to this result in the photogrammetry literature [Gracie-68]. A
simple formula for the focal length in this case is given in [Cipolla-99, Hartley-02b].
A discussion of the degeneracies that arise when combining multiple constraints is
given in [Liebowitz-99b, Liebowitz-01]. Single view reconstruction is investigated
in [Criminisi-99a, Criminisi-01, Horry-97, Liebowitz-99a, Sturm-99a].

8.11.2 Notes and exercises

(i) Homography from a world plane. Suppose H is computed (e.g. from the
correspondence between four or more known world points and their images)
and K known, then the pose of the camera {R, t} may be computed from the
camera matrix [r1, r2, r1 × r2, t], where

[r1, r2, t] = ±K−1H/(cid:10)K−1H(cid:10).

Note that there is a two-fold ambiguity. This result follows from (8.1–p196)
which gives the homography between a world plane and calibrated camera P =
Show that the homography x = H(cid:22)X between points on a world plane (nT, d)T
K[R | t].
have coordinates (cid:22)X = (X, Y, Z)T.
and the image may be expressed as H = K(R − tnT/d). The points on the plane

(ii) Line projection.

the map (8.2–p198), i.e. it is projected to the line l = 0.

(a) Show that any line containing the camera centre lies in the null-space of
(b) Show that the line L = P Tx in IP3 is the ray through the image point x
and the camera centre. Hint: start from result 3.5(p72), and show that
the camera centre C lies on L.

(c) What is the geometric interpretation of the columns of P?

(iii) Contour generator of a quadric. The contour generator Γ of a quadric con-
sists of the set of points X on Q for which the tangent planes contain the camera
centre, C. The tangent plane at a point X on Q is given by π = QX, and the
condition that C is on π is CTπ = CTQX = 0. Thus points X on Γ satisfy
CTQX = 0, and thus lie on the plane πΓ = QC since πT
ΓX = CTQX = 0. This
shows that the contour generator of a quadric is a plane curve and furthermore,
since πΓ = QC, that the plane of Γ is the polar plane of the camera centre with
respect to the quadric.
(iv) Apparent contour of an algebraic surface. Show that the apparent contour
of a homogeneous algebraic surface of degree n is a curve of degree n(n − 1).
For example, if n = 2 then the surface is a quadric and the apparent contour
a conic. Hint: write the surface as F (X, Y, Z, W) = 0, then the tangent plane
contains the camera centre C if

∂F
∂Y
which is a surface of a degree n − 1.

∂F
∂X

CX

+ CY

+ CZ

∂F
∂Z

+ CW

∂F
∂W

= 0

8.11 Closure

235
(v) Rotation axis vanishing point for H = KRK−1. The homography of a conjugate
rotation H = KRK−1 has an eigenvector Ka, where a is the direction of the
rotation axis, since HKa = KRa = Ka. The last equality follows because Ra =
1a, i.e. a is the unit eigenvector of R. It follows that (a) Ka is a ﬁxed point under
the homography H; and (b) from result 8.20(p215) v = Ka is the vanishing
point of the rotation axis.

(vi) Synthetic rotations. Suppose, as in example 8.12(p205), that a homography
is estimated between two images related by a pure rotation about the camera
centre. Then the estimated homography will be a conjugate rotation, so that
H = KR(θ)K−1 (though K and R are unknown). However, H2 applied to the
ﬁrst image generates the image that would have been obtained by rotating the
camera about the same axis through twice the angle, since H2 = KR2K−1 =
KR(2θ)K−1.
More generally we may write Hλ to represent a rotation through any fractional
angle λθ. To make sense of Hλ, observe that the eigenvalue decomposition of H
−iθ) U−1, and both θ and U may be computed from the
is H(θ) = U diag(1, eiθ, e
estimated H. Then

Hλ = U diag(1, eiλθ, e

−iλθ) U−1 = KR(λθ)K−1.

which is the conjugate of a rotation through angle λθ. Writing φ instead of λθ,
we may use this homography to generate synthetic images rotated through any
angle φ. The images are interpolated between the original images (if 0 < φ <
θ), or extrapolated (if φ > θ).

(vii) Show that the imaged circular points of a perspectively imaged plane may be
computed if any of the following are on the plane: (i) a square grid; (ii) two
rectangles arranged such that the sides of one rectangle are not parallel to the
sides of the other; (iii) two circles of equal radius; (iv) two circles of unequal
radius.

(viii) Show that in the case of zero skew, ω is the conic

(cid:28)

x − x0
αx

(cid:20)

(cid:29)

2

+

(cid:21)

2

y − y0
αy

+ 1 = 0

which may be interpreted as an ellipse aligned with the axes, centred on the
principal point, and with axes of length iαx and iαy in the x and y directions
respectively.

(ix) If the camera calibration K and the vanishing line l of a scene plane are known
then the scene plane can be metric rectiﬁed by a homography corresponding
to a synthetic rotation H = KRK−1 that maps l to l∞, i.e. it is required that
H−Tl = (0, 0, 1)T. This condition arises because if the plane is rotated such that
its vanishing line is l∞ then it is fronto-parallel. Show that H−Tl = (0, 0, 1)T is
equivalent to Rn = (0, 0, 1)T, where n = KTl is the normal to the scene plane.
This is the condition that the scene normal is rotated to lie along the camera Z
axis. Note the rotation is not uniquely deﬁned since a rotation about the plane’s

236

8 More Single View Geometry

normal does not affect its metric rectiﬁcation. However, the last row of R equals
n, so that R = [r1, r2, n]T where n, r1 and r2 are a triad of orthonormal vectors.

(x) Show that the angle between two planes with vanishing lines l1 and l2 is

(cid:19)

lT
1

(cid:19)
ω∗l2
lT
2

lT
1
ω∗l1

cos θ =

.

ω∗l2

(xi) Derive (8.15–p218). Hint, the line l lies in the pencil deﬁned by l1 and l2, so
it can be expressed as l = αl1 + βl2. Then use the relations ln = l0 + nl for
n = 1, 2 to solve for α and β.

(xii) For the case of vanishing points arising from three orthogonal directions, and
for an image with square pixels, show algebraically that the principal point is
the orthocentre of the triangle with vertices the vanishing points. Hint: suppose
the vanishing point at one vertex of the triangle is v and the line of the opposite
side (through the other two vanishing points) is l. Then from (8.17–p219) v =
ω∗l since v and l arise from an orthogonal line and plane respectively. Show
that the principal point lies on the line from v to l which is perpendicular in
the image to l. Since this result is true for any vertex the principal point is the
orthocentre of the triangle.

(xiii) Show that the vanishing points of an orthogonal triad of directions are the ver-

tices of a self-polar triangle [Springer-64] with respect to ω.

(xiv) If a camera has square pixels, then the apparent contour of a sphere centred on
the principal axis is a circle. If the sphere is translated parallel to the image
plane, then the apparent contour deforms from a circle to an ellipse with the
principal point on its major axis.

(a) How can this observation be used as a method of internal parameter

calibration?

(b) Show by a geometric argument that the aspect ratio of the ellipse does

not depend on the distance of the sphere from the camera.

If the sphere is now translated parallel to the principal axis the apparent contour
can deform to a hyperbola, but only one branch of the hyperbola is imaged.
Why is this?

(xv) Show that for a general camera the apparent contour of a sphere is related to the

IAC as:

ω = C + vvT

where C is the conic outline of the imaged sphere, and v is a 3-vector that
depends on the position of the sphere. A proof is given in [Agrawal-03]. Note
this relation places two constraints on ω, so that in principle ω, and hence
the calibration K, may be computed from three images of a sphere. However,
in practice this is not a well conditioned method for computing K because the
deviation of the sphere’s outline from a circle is small.

Part II

Two-View Geometry

The Birth of Venus (detail), c. 1485 (tempera on canvas) by Sandro Botticelli (1444/5-1510)

Galleria degli Ufﬁzi, Florence, Italy/Bridgeman Art Library

Outline

This part of the book covers the geometry of two perspective views. These views may be acquired
simultaneously as in a stereo rig, or acquired sequentially, for example by a camera moving relative to
the scene. These two situations are geometrically equivalent and will not be differentiated here. Each
view has an associated camera matrix, P, P(cid:1)
indicates entities associated with the second view,
and a 3-space point X is imaged as x = PX in the ﬁrst view, and x(cid:1) = P(cid:1)X in the second. Image points
x and x(cid:1)
correspond because they are the image of the same 3-space point. There are three questions
that will be addressed:

, where

(cid:1)

(i) Correspondence geometry. Given an image point x in the ﬁrst view, how does this constrain

the position of the corresponding point x(cid:1)
{xi ↔ x(cid:1)

(ii) Camera geometry (motion). Given a set of corresponding image points
for the two views?
(iii) Scene geometry (structure). Given corresponding image points x ↔ x(cid:1)

i}, i = 1, . . . , n, what are the cameras P and P(cid:1)
what is the position of (their pre-image) X in 3-space?

in the second view?

and cameras P, P(cid:1)

,

Chapter 9 describes the epipolar geometry of two views, and directly answers the ﬁrst question: a
point in one view deﬁnes an epipolar line in the other view on which the corresponding point lies. The
epipolar geometry depends only on the cameras – their relative position and their internal parameters.
It does not depend at all on the scene structure. The epipolar geometry is represented by a 3 × 3
matrix called the fundamental matrix F. The anatomy of the fundamental matrix is described, and its
computation from camera matrices P and P(cid:1)
may be computed from
F up to a projective ambiguity of 3-space.

given. It is then shown that P and P(cid:1)

Chapter 10 describes one of the most important results in uncalibrated multiple view geometry – a
reconstruction of both cameras and scene structure can be computed from image point correspondences
alone; no other information is required. This answers both the second and third questions simultaneously.
The reconstruction obtained from point correspondences alone is up to a projective ambiguity of 3-space,
and this ambiguity can be resolved by supplying well deﬁned additional information on the cameras or
scene. In this manner an afﬁne or metric reconstruction may be computed from uncalibrated images. The
following two chapters then ﬁll in the details and numerical algorithms for computing this reconstruction.
Chapter 11 describes methods for computing F from a set of corresponding image points {xi ↔
i}, even though the structure (3D pre-image Xi) of these points is unknown and the camera matrices
x(cid:1)
are unknown. The cameras P and P(cid:1)
may then be determined, up to a projective ambiguity, from the
computed F.

Chapter 12 then describes the computation of scene structure by triangulation given the cameras
and corresponding image points – the point X in 3-space is computed as the intersection of rays back-
projected from the corresponding points x and x(cid:1)
. Similarly, the 3D
position of other geometric entities, such as lines or conics, may also be computed given their image
correspondences.

via their associated cameras P, P(cid:1)

Chapter 13 covers the two-view geometry of planes. It provides an alternative answer to the ﬁrst
question: if scene points lie on a plane, then once the geometry of this plane is computed, the image x of
a point in one image determines the position of x(cid:1)
in the other image. The points are related by a plane
projective transformation. This chapter also describes a particularly important projective transformation
between views – the inﬁnite homography, which is the transformation arising from the plane at inﬁnity.
are
afﬁne. This case has a number of simpliﬁcations over the general projective case, and provides a very
good approximation in many practical situations.

Chapter 14 describes two-view geometry in the specialized case that the two cameras P and P(cid:1)

238

9

Epipolar Geometry and the Fundamental Matrix

The epipolar geometry is the intrinsic projective geometry between two views. It is
independent of scene structure, and only depends on the cameras’ internal parameters
and relative pose.
The fundamental matrix F encapsulates this intrinsic geometry. It is a 3 × 3 matrix
of rank 2. If a point in 3-space X is imaged as x in the ﬁrst view, and x(cid:2)
in the second,
then the image points satisfy the relation x(cid:2)TFx = 0.

We will ﬁrst describe epipolar geometry, and derive the fundamental matrix. The
properties of the fundamental matrix are then elucidated, both for general motion of
the camera between the views, and for several commonly occurring special motions. It
is next shown that the cameras can be retrieved from F up to a projective transformation
of 3-space. This result is the basis for the projective reconstruction theorem given in
chapter 10. Finally, if the camera internal calibration is known, it is shown that the Eu-
clidean motion of the cameras between views may be computed from the fundamental
matrix up to a ﬁnite number of ambiguities.

The fundamental matrix is independent of scene structure. However, it can be com-
puted from correspondences of imaged scene points alone, without requiring knowl-
edge of the cameras’ internal parameters or relative pose. This computation is de-
scribed in chapter 11.

9.1 Epipolar geometry

The epipolar geometry between two views is essentially the geometry of the inter-
section of the image planes with the pencil of planes having the baseline as axis (the
baseline is the line joining the camera centres). This geometry is usually motivated by
considering the search for corresponding points in stereo matching, and we will start
from that objective here.
Suppose a point X in 3-space is imaged in two views, at x in the ﬁrst, and x(cid:2)
in the
second. What is the relation between the corresponding image points x and x(cid:2)
? As
shown in ﬁgure 9.1a the image points x and x(cid:2)
, space point X, and camera centres
are coplanar. Denote this plane as π. Clearly, the rays back-projected from x and x(cid:2)
intersect at X, and the rays are coplanar, lying in π. It is this latter property that is of
most signiﬁcance in searching for a correspondence.

239

240

9 Epipolar Geometry and the Fundamental Matrix

X

epipolar plane  

 π

X ?

X

X ?

x

C

/

x

C /

x

e

/

l

/

e

epipolar line
for x

a

b

and image planes. The camera centres, 3-space point X, and its images x and x(cid:1)

Fig. 9.1. Point correspondence geometry. (a) The two cameras are indicated by their centres C and
C(cid:1)
lie in a common
plane π. (b) An image point x back-projects to a ray in 3-space deﬁned by the ﬁrst camera centre, C,
and x. This ray is imaged as a line l(cid:1)
in the second view. The 3-space point X which projects to x must
lie on this ray, so the image of X in the second view must lie on l(cid:1)

.

π

l

/

l

e

/

e

baseline

a

e

baseline

X

b

/

e

Fig. 9.2. Epipolar geometry. (a) The camera baseline intersects each image plane at the epipoles e
and e(cid:1)
. Any plane π containing the baseline is an epipolar plane, and intersects the image planes in
corresponding epipolar lines l and l(cid:1)
. (b) As the position of the 3D point X varies, the epipolar planes
“rotate” about the baseline. This family of planes is known as an epipolar pencil. All epipolar lines
intersect at the epipole.

Supposing now that we know only x, we may ask how the corresponding point x(cid:2)

is
constrained. The plane π is determined by the baseline and the ray deﬁned by x. From
above we know that the ray corresponding to the (unknown) point x(cid:2)
lies in π, hence
the point x(cid:2)
of π with the second image plane. This line
l(cid:2)
is the image in the second view of the ray back-projected from x. It is the epipolar
line corresponding to x. In terms of a stereo correspondence algorithm the beneﬁt is
that the search for the point corresponding to x need not cover the entire image plane
but can be restricted to the line l(cid:2)

lies on the line of intersection l(cid:2)

.

The geometric entities involved in epipolar geometry are illustrated in ﬁgure 9.2.

The terminology is
• The epipole is the point of intersection of the line joining the camera centres (the
baseline) with the image plane. Equivalently, the epipole is the image in one view

9.2 The fundamental matrix F

241

e

e /

a

b

c

Fig. 9.3. Converging cameras. (a) Epipolar geometry for converging cameras. (b) and (c) A pair of
images with superimposed corresponding points and their epipolar lines (in white). The motion between
the views is a translation and rotation. In each image, the direction of the other camera may be inferred
from the intersection of the pencil of epipolar lines. In this case, both epipoles lie outside of the visible
image.

of the camera centre of the other view. It is also the vanishing point of the baseline
(translation) direction.
• An epipolar plane is a plane containing the baseline. There is a one-parameter
family (a pencil) of epipolar planes.
• An epipolar line is the intersection of an epipolar plane with the image plane. All
epipolar lines intersect at the epipole. An epipolar plane intersects the left and right
image planes in epipolar lines, and deﬁnes the correspondence between the lines.

Examples of epipolar geometry are given in ﬁgure 9.3 and ﬁgure 9.4. The epipolar
geometry of these image pairs, and indeed all the examples of this chapter, is computed
directly from the images as described in section 11.6(p290).

9.2 The fundamental matrix F

The fundamental matrix is the algebraic representation of epipolar geometry. In the
following we derive the fundamental matrix from the mapping between a point and its
epipolar line, and then specify the properties of the matrix.
there exists a corresponding epipolar line l(cid:2)
second image matching the point x must lie on the epipolar line l(cid:2)

Given a pair of images, it was seen in ﬁgure 9.1 that to each point x in one image,
in the
. The epipolar line

in the other image. Any point x(cid:2)

242

9 Epipolar Geometry and the Fundamental Matrix

e  at

infinity

/

e  at

infinity

a

b

c

Fig. 9.4. Motion parallel to the image plane. In the case of a special motion where the translation is
parallel to the image plane, and the rotation axis is perpendicular to the image plane, the intersection
of the baseline with the image plane is at inﬁnity. Consequently the epipoles are at inﬁnity, and epipolar
lines are parallel. (a) Epipolar geometry for motion parallel to the image plane. (b) and (c) a pair of
images for which the motion between views is (approximately) a translation parallel to the x-axis, with
no rotation. Four corresponding epipolar lines are superimposed in white. Note that corresponding
points lie on corresponding epipolar lines.

is the projection in the second image of the ray from the point x through the camera
centre C of the ﬁrst camera. Thus, there is a map

x (cid:6)→ l(cid:2)

from a point in one image to its corresponding epipolar line in the other image. It is
the nature of this map that will now be explored. It will turn out that this mapping
is a (singular) correlation, that is a projective mapping from points to lines, which is
represented by a matrix F, the fundamental matrix.

9.2.1 Geometric derivation
We begin with a geometric derivation of the fundamental matrix. The mapping from
a point in one image to a corresponding epipolar line in the other image may be de-
composed into two steps. In the ﬁrst step, the point x is mapped to some point x(cid:2)
in
the other image lying on the epipolar line l(cid:2)
is a potential match for the
point x. In the second step, the epipolar line l(cid:2)
to the
epipole e(cid:2)

is obtained as the line joining x(cid:2)

. This point x(cid:2)

.

Step 1: Point transfer via a plane. Refer to ﬁgure 9.5. Consider a plane π in space
not passing through either of the two camera centres. The ray through the ﬁrst camera
centre corresponding to the point x meets the plane π in a point X. This point X is
then projected to a point x(cid:2)
in the second image. This procedure is known as transfer
via the plane π. Since X lies on the ray corresponding to x, the projected point x(cid:2)
must lie on the epipolar line l(cid:2)
corresponding to the image of this ray, as illustrated in

9.2 The fundamental matrix F

243

X

π

x

e

H

π

/

l

/x

/

e

Fig. 9.5. A point x in one image is transferred via the plane π to a matching point x(cid:1)
image. The epipolar line through x(cid:1)
write x(cid:1) = Hπx and l(cid:1) = [e(cid:1)]×x(cid:1) = [e(cid:1)]×Hπx = Fx where F = [e(cid:1)]×Hπ is the fundamental matrix.

is obtained by joining x(cid:1)

to the epipole e(cid:1)

in the second
. In symbols one may

ﬁgure 9.1b. The points x and x(cid:2)
are both images of the 3D point X lying on a plane.
The set of all such points xi in the ﬁrst image and the corresponding points x(cid:2)
i in the
second image are projectively equivalent, since they are each projectively equivalent to
(cid:0) mapping each xi to x(cid:2)
the planar point set Xi. Thus there is a 2D homography H
i.
Step 2: Constructing the epipolar line. Given the point x(cid:2)
= e(cid:2) × x(cid:2)
through x(cid:2)
may be written as x(cid:2)
is deﬁned in (A4.5–p581)). Since x(cid:2)
l(cid:2)
= [e(cid:2)

the epipolar line l(cid:2)
= [e(cid:2)
= H

can be written as l(cid:2)

]×x(cid:2)
x, we have

and the epipole e(cid:2)

(the notation [e(cid:2)

passing
]×

x = Fx

(cid:0)

]×H

(cid:0)

]×H

where we deﬁne F = [e(cid:2)
(cid:0), the fundamental matrix. This shows
Result 9.1. The fundamental matrix F may be written as F = [e(cid:2)
transfer mapping from one image to another via any plane π. Furthermore, since [e(cid:2)
has rank 2 and H(cid:0) rank 3, F is a matrix of rank 2.

(cid:0), where H

]×H

(cid:0) is the
]×

Geometrically, F represents a mapping from the 2-dimensional projective plane IP2
of the ﬁrst image to the pencil of epipolar lines through the epipole e(cid:2)
. Thus, it rep-
resents a mapping from a 2-dimensional onto a 1-dimensional projective space, and
hence must have rank 2.

Note, the geometric derivation above involves a scene plane π, but a plane is not
required in order for F to exist. The plane is simply used here as a means of deﬁning a
point map from one image to another. The connection between the fundamental matrix
and transfer of points from one image to another via a plane is dealt with in some depth
in chapter 13.

9.2.2 Algebraic derivation
The form of the fundamental matrix in terms of the two camera projection matri-
ces, P, P(cid:2)
, may be derived algebraically. The following formulation is due to Xu and
Zhang [Xu-96].

244

9 Epipolar Geometry and the Fundamental Matrix

The ray back-projected from x by P is obtained by solving PX = x. The one-

parameter family of solutions is of the form given by (6.13–p162) as

X(λ) =P +x + λC

where P+ is the pseudo-inverse of P, i.e. PP+ = I, and C its null-vector, namely the
camera centre, deﬁned by PC = 0. The ray is parametrized by the scalar λ.
In
particular two points on the ray are P+x (at λ = 0), and the ﬁrst camera centre C
(at λ = ∞). These two points are imaged by the second camera P(cid:2)
at P(cid:2)P+x and P(cid:2)C
respectively in the second view. The epipolar line is the line joining these two projected
= (P(cid:2)C)× (P(cid:2)P+x). The point P(cid:2)C is the epipole in the second image,
points, namely l(cid:2)
namely the projection of the ﬁrst camera centre, and may be denoted by e(cid:2)
. Thus,
= [e(cid:2)
l(cid:2)

]×(P(cid:2)P+)x = Fx, where F is the matrix

F = [e(cid:2)

]×P(cid:2)P+.

(9.1)

This is essentially the same formula for the fundamental matrix as the one derived in
(cid:0) = P(cid:2)P+ in terms
the previous section, the homography H(cid:0) having the explicit form H
of the two camera matrices. Note that this derivation breaks down in the case where
the two camera centres are the same for, in this case, C is the common camera centre
, and so P(cid:2)C = 0. It follows that F deﬁned in (9.1) is the zero matrix.
of both P and P(cid:2)
Example 9.2. Suppose the camera matrices are those of a calibrated stereo rig with the
world origin at the ﬁrst camera

P = K[I | 0]

(cid:17)

K−1
0T

P+ =

P(cid:2)

[R | t].
= K(cid:2)
(cid:20)
(cid:21)

(cid:18)

C =

0
1

Then

and

F = [P(cid:2)C]×P(cid:2)P+

= [K(cid:2)t]×K(cid:2)RK−1 = K(cid:2)−T[t]×RK−1 = K(cid:2)−TR[RTt]×K−1 = K(cid:2)−TRKT[KRTt]× (9.2)
where the various forms follow from result A4.3(p582). Note that the epipoles (deﬁned
as the image of the other camera centre) are

= KRTt

e(cid:2)

= P(cid:2)

= K(cid:2)t.

(9.3)

(cid:20) −RTt

(cid:21)

e = P

1

(cid:20)

(cid:21)

0
1

Thus we may write (9.2) as

F = [e(cid:2)

]×K(cid:2)RK−1 = K(cid:2)−T[t]×RK−1 = K(cid:2)−TR[RTt]×K−1 = K(cid:2)−TRKT[e]×.

(9.4)
(cid:2)
The expression for the fundamental matrix can be derived in many ways, and indeed
will be derived again several times in this book. In particular, (17.3–p412) expresses F
in terms of 4 × 4 determinants composed from rows of the camera matrices for each
view.

9.2 The fundamental matrix F

245

9.2.3 Correspondence condition
Up to this point we have considered the map x → l(cid:2)
the most basic properties of the fundamental matrix.

deﬁned by F. We may now state

Result 9.3. The fundamental matrix satisﬁes the condition that for any pair of corre-
sponding points x ↔ x(cid:2)

in the two images

x(cid:2)TFx = 0.

correspond, then x(cid:2)
This is true, because if points x and x(cid:2)
lies on the epipolar line
l(cid:2)
= x(cid:2)TFx. Conversely,
= Fx corresponding to the point x. In other words 0 = x(cid:2)Tl(cid:2)
if image points satisfy the relation x(cid:2)TFx = 0 then the rays deﬁned by these points are
coplanar. This is a necessary condition for points to correspond.

The importance of the relation of result 9.3 is that it gives a way of characterizing
the fundamental matrix without reference to the camera matrices, i.e. only in terms of
corresponding image points. This enables F to be computed from image correspon-
dences alone. We have seen from (9.1) that F may be computed from the two camera
matrices, P, P(cid:2)
, and in particular that F is determined uniquely from the cameras, up
to an overall scaling. However, we may now enquire how many correspondences are
required to compute F from x(cid:2)TFx = 0, and the circumstances under which the ma-
trix is uniquely deﬁned by these correspondences. The details of this are postponed
until chapter 11, where it will be seen that in general at least 7 correspondences are
required to compute F.

9.2.4 Properties of the fundamental matrix
Deﬁnition 9.4. Suppose we have two images acquired by cameras with non-coincident
centres, then the fundamental matrix F is the unique 3×3 rank 2 homogeneous matrix
which satisﬁes

x(cid:2)TFx = 0

(9.5)

for all corresponding points x ↔ x(cid:2)
We now brieﬂy list a number of properties of the fundamental matrix. The most

.

important properties are also summarized in table 9.1.

(i) Transpose: If F is the fundamental matrix of the pair of cameras (P, P(cid:2)

), then

(ii) Epipolar lines: For any point x in the ﬁrst image, the corresponding epipolar
represents the epipolar line corresponding

= Fx. Similarly, l = FTx(cid:2)

FT is the fundamental matrix of the pair in the opposite order: (P(cid:2)
line is l(cid:2)
to x(cid:2)
the epipole e(cid:2)
e(cid:2)TF = 0, i.e. e(cid:2)
null-vector of F.

= Fx contains
satisﬁes e(cid:2)T(Fx) = (e(cid:2)TF)x = 0 for all x. It follows that
is the left null-vector of F. Similarly Fe = 0, i.e. e is the right

(iii) The epipole: for any point x (other than e) the epipolar line l(cid:2)

in the second image.

. Thus e(cid:2)

, P).

x(cid:1)TFx = 0.

• Epipolar lines:

(cid:21) l(cid:1) = Fx is the epipolar line corresponding to x.
(cid:21) l = FTx(cid:1)
is the epipolar line corresponding to x(cid:1)
• Epipoles:
(cid:21) Fe = 0.
(cid:21) FTe(cid:1) = 0.

.

• Computation from camera matrices P, P(cid:1)

:

(cid:21) General cameras,

246

9 Epipolar Geometry and the Fundamental Matrix

• F is a rank 2 homogeneous matrix with 7 degrees of freedom.
• Point correspondence: If x and x(cid:1)

are corresponding image points, then

F = [e(cid:1)]×P(cid:1)P+, where P+ is the pseudo-inverse of P, and e(cid:1) = P(cid:1)C, with PC = 0.

(cid:21) Canonical cameras, P = [I | 0], P(cid:1) = [M | m],

F = [e(cid:1)]×M = M−T[e]×, where e(cid:1) = m and e = M−1m.

(cid:21) Cameras not at inﬁnity P = K[I | 0], P(cid:1) = K(cid:1)[R | t],

F = K(cid:1)−T[t]×RK−1 = [K(cid:1)t]×K(cid:1)RK−1 = K(cid:1)−TRKT[KRTt]×.

Table 9.1. Summary of fundamental matrix properties.

(iv) F has seven degrees of freedom: a 3×3 homogeneous matrix has eight indepen-
dent ratios (there are nine elements, and the common scaling is not signiﬁcant);
however, F also satisﬁes the constraint det F = 0 which removes one degree of
freedom.

= Fx, which is the epipolar line of x. If l and l(cid:2)

(v) F is a correlation, a projective map taking a point to a line (see deﬁnition 2.29-
(p59)). In this case a point in the ﬁrst image x deﬁnes a line in the second
l(cid:2)
are corresponding epipolar
lines (see ﬁgure 9.6a) then any point x on l is mapped to the same line l(cid:2)
. This
means there is no inverse mapping, and F is not of full rank. For this reason, F
is not a proper correlation (which would be invertible).

9.2.5 The epipolar line homography
The set of epipolar lines in each of the images forms a pencil of lines passing through
the epipole. Such a pencil of lines may be considered as a 1-dimensional projective
space. It is clear from ﬁgure 9.6b that corresponding epipolar lines are perspectively
related, so that there is a homography between the pencil of epipolar lines centred at e
in the ﬁrst view, and the pencil centred at e(cid:2)
in the second. A homography between two
such 1-dimensional projective spaces has 3 degrees of freedom.
for e, 2 for e(cid:2)
line through e(cid:2)
Here we give an explicit formula for this mapping.

The degrees of freedom of the fundamental matrix can thus be counted as follows: 2
, and 3 for the epipolar line homography which maps a line through e to a
. A geometric representation of this homography is given in section 9.4.

9.3 Fundamental matrices arising from special motions

247

l3

l
2
1l

3l/

/
2l
l/
1

e

e /

p

a

b

Fig. 9.6. Epipolar line homography. (a) There is a pencil of epipolar lines in each image centred on the
epipole. The correspondence between epipolar lines, li ↔ l(cid:1)
i, is deﬁned by the pencil of planes with axis
the baseline. (b) The corresponding lines are related by a perspectivity with centre any point p on the
baseline. It follows that the correspondence between epipolar lines in the pencils is a 1D homography.

]×l(cid:2)

.

are related by l(cid:2)

.

are corresponding epipolar lines, and k is any line not
= F[k]×l. Symmetrically,

Result 9.5. Suppose l and l(cid:2)
passing through the epipole e, then l and l(cid:2)
l = FT[k(cid:2)
Proof. The expression [k]×l = k × l is the point of intersection of the two lines k and
l, and hence a point on the epipolar line l – call it x. Hence, F[k]×l = Fx is the epipolar
line corresponding to the point x, namely the line l(cid:2)
Furthermore a convenient choice for k is the line e, since kTe = eTe (cid:5)= 0, so that
the line e does not pass through the point e as is required. A similar argument holds
for the choice of k(cid:2)

. Thus the epipolar line homography may be written as

= e(cid:2)

l(cid:2)

= F[e]×l

l = FT[e(cid:2)

]×l(cid:2)

.

9.3 Fundamental matrices arising from special motions

A special motion arises from a particular relationship between the translation direction,
t, and the direction of the rotation axis, a. We will discuss two cases: pure translation,
where there is no rotation; and pure planar motion, where t is orthogonal to a (the
signiﬁcance of the planar motion case is described in section 3.4.1(p77)). The ‘pure’
indicates that there is no change in the internal parameters. Such cases are important,
ﬁrstly because they occur in practice, for example a camera viewing an object rotating
on a turntable is equivalent to planar motion for pairs of views; and secondly because
the fundamental matrix has a special form and thus additional properties.

9.3.1 Pure translation
In considering pure translations of the camera, one may consider the equivalent situ-
ation in which the camera is stationary, and the world undergoes a translation −t. In
this situation points in 3-space move on straight lines parallel to t, and the imaged in-
tersection of these parallel lines is the vanishing point v in the direction of t. This is
illustrated in ﬁgure 9.7 and ﬁgure 9.8. It is evident that v is the epipole for both views,
and the imaged parallel lines are the epipolar lines. The algebraic details are given in
the following example.

248

9 Epipolar Geometry and the Fundamental Matrix

parallel
lines

e

vanishing

point

image

camera
centre

Fig. 9.7. Under a pure translational camera motion, 3D points appear to slide along parallel rails. The
images of these parallel lines intersect in a vanishing point corresponding to the translation direction.
The epipole e is the vanishing point.

C

e

/

e

/

C

a

b

c

Fig. 9.8. Pure translational motion. (a) under the motion the epipole is a ﬁxed point, i.e. has the same
coordinates in both images, and points appear to move along lines radiating from the epipole. The
epipole in this case is termed the Focus of Expansion (FOE). (b) and (c) the same epipolar lines are
overlaid in both cases. Note the motion of the posters on the wall which slide along the epipolar line.

Example 9.6. Suppose the motion of the cameras is a pure translation with no rotation
and no change in the internal parameters. One may assume that the two cameras are

P = K[I | 0] and P(cid:2)

9.3 Fundamental matrices arising from special motions

= K[I | t]. Then from (9.4) (using R = I and K = K(cid:2)

)

249

]×.
If the camera translation is parallel to the x-axis, then e(cid:2)

F = [e(cid:2)

]×KK−1 = [e(cid:2)

= (1, 0, 0)T, so

 0 0

0
0 0 −1
0
0 1

 .

F =

The relation between corresponding points, x(cid:2)TFx = 0, reduces to y = y
, i.e. the
epipolar lines are corresponding rasters. This is the situation that is sought by image
(cid:2)
rectiﬁcation described in section 11.12(p302).

(cid:2)

Indeed if the image point x is normalized as x = (x, y, 1)T, then from
x = PX = K[I | 0]X, the space point’s (inhomogeneous) coordinates are (X, Y, Z)T =
ZK−1x, where Z is the depth of the point X (the distance of X from the camera centre
measured along the principal axis of the ﬁrst camera). It then follows from x(cid:2)
= P(cid:2)X =
K[I | t]X that the mapping from an image point x to an image point x(cid:2)

is

x(cid:2)

= x + Kt/Z.

(9.6)

The motion x(cid:2)
= x + Kt/Z of (9.6) shows that the image point “starts” at x and then
moves along the line deﬁned by x and the epipole e = e(cid:2)
= v. The extent of the motion
depends on the magnitude of the translation t (which is not a homogeneous vector here)
and the inverse depth Z, so that points closer to the camera appear to move faster than
those further away – a common experience when looking out of a train window.

Note that in this case of pure translation F = [e(cid:2)

]× is skew-symmetric and has only
2 degrees of freedom, which correspond to the position of the epipole. The epipolar
line of x is l(cid:2)
and
e = e(cid:2)
are collinear (assuming both images are overlaid on top of each other). This
collinearity property is termed auto-epipolar, and does not hold for general motion.

= Fx = [e]×x, and x lies on this line since xT[e]×x = 0, i.e. x, x(cid:2)

General motion. The pure translation case gives additional insight into the general
motion case. Given two arbitrary cameras, we may rotate the camera used for the ﬁrst
image so that it is aligned with the second camera. This rotation may be simulated
by applying a projective transformation to the ﬁrst image. A further correction may
be applied to the ﬁrst image to account for any difference in the calibration matrices
of the two images. The result of these two corrections is a projective transformation
H of the ﬁrst image. If one assumes these corrections to have been made, then the
effective relationship of the two cameras to each other is that of a pure translation.
Consequently, the fundamental matrix corresponding to the corrected ﬁrst image and
]×, satisfying x(cid:2)TˆFˆx = 0, where ˆx = Hx is the
the second image is of the form ˆF = [e(cid:2)
corrected point in the ﬁrst image. From this one deduces that x(cid:2)T[e(cid:2)
]×Hx = 0, and so
the fundamental matrix corresponding to the initial point correspondences x ↔ x(cid:2)
is
F = [e(cid:2)

]×H. This is illustrated in ﬁgure 9.9.

250

9 Epipolar Geometry and the Fundamental Matrix

H

C

e

/e

/

C

Fig. 9.9. General camera motion. The ﬁrst camera (on the left) may be rotated and corrected to
simulate a pure translational motion. The fundamental matrix for the original pair is the product F =
[e(cid:1)]×H, where [e(cid:1)]× is the fundamental matrix of the translation, and H is the projective transformation
corresponding to the correction of the ﬁrst camera.

= K(cid:2)

Example 9.7. Continuing from example 9.2, assume again that the two cameras are
P = K[I | 0] and P(cid:2)
[R | t]. Then as described in section 8.4.2(p204) the requisite
projective transformation is H = K(cid:2)RK−1 = H∞, where H∞ is the inﬁnite homography
(see section 13.4(p338)), and F = [e(cid:2)
(X, Y, Z)T = ZK−1x, and from x = P(cid:2)X = K(cid:2)
point x to an image point x(cid:2)

If the image point x is normalized as x = (x, y, 1)T, as in example 9.6, then
[R | t]X the mapping from an image

]×H∞.

is

x(cid:2)

= K(cid:2)RK−1x + K(cid:2)t/Z.

(9.7)

The mapping is in two parts: the ﬁrst term depends on the image position alone, i.e.
x, but not the point’s depth Z, and takes account of the camera rotation and change
of internal parameters; the second term depends on the depth, but not on the image
position x, and takes account of camera translation. In the case of pure translation
(cid:2)
(R = I, K = K(cid:2)

) (9.7) reduces to (9.6).

9.3.2 Pure planar motion
In this case the rotation axis is orthogonal to the translation direction. Orthogonality
imposes one constraint on the motion, and it is shown in the exercises at the end of
this chapter that if K(cid:2)
= K then Fs, the symmetric part of F, has rank 2 in this planar
motion case (note, for a general motion the symmetric part of F has full rank). Thus,
the condition that det Fs = 0 is an additional constraint on F and reduces the number
of degrees of freedom from 7, for a general motion, to 6 degrees of freedom for a pure
planar motion.

9.4 Geometric representation of the fundamental matrix

This section is not essential for a ﬁrst reading and the reader may optionally skip to
section 9.5.

In this section the fundamental matrix is decomposed into its symmetric and skew-
symmetric parts, and each part is given a geometric representation. The symmetric and

9.4 Geometric representation of the fundamental matrix

251

(cid:15)

(cid:16)

(cid:15)
F − FT

(cid:16)

/2

skew-symmetric parts of the fundamental matrix are

Fs =

F + FT

/2

Fa =

so that F = Fs + Fa.

To motivate the decomposition, consider the points X in 3-space that map to the
same point in two images. These image points are ﬁxed under the camera motion
so that x = x(cid:2)
. Clearly such points are corresponding and thus satisfy xTFx = 0,
which is a necessary condition on corresponding points. Now, for any skew-symmetric
matrix A the form xTAx is identically zero. Consequently only the symmetric part of
F contributes to xTFx = 0, which then reduces to xTFsx = 0. As will be seen below
the matrix Fs may be thought of as a conic in the image plane.
Geometrically the conic arises as follows. The locus of all points in 3-space for
which x = x(cid:2)
is known as the horopter curve. Generally this is a twisted cubic curve in
3-space (see section 3.3(p75)) passing through the two camera centres [Maybank-93].
The image of the horopter is the conic deﬁned by Fs. We return to the horopter in
chapter 22.

Symmetric part. The matrix Fs is symmetric and is of rank 3 in general. It has 5
degrees of freedom and is identiﬁed with a point conic, called the Steiner conic (the
name is explained below). The epipoles e and e(cid:2)
lie on the conic Fs. To see that the
epipoles lie on the conic, i.e. that eTFse = 0, start from Fe = 0. Then eTFe = 0 and
so eTFse + eTFae = 0. However, eTFae = 0, since for any skew-symmetric matrix
S, xTSx = 0. Thus eTFse = 0. The derivation for e(cid:2)

follows in a similar manner.

Skew-symmetric part. The matrix Fa is skew-symmetric and may be written as
Fa = [xa]×, where xa is the null-vector of Fa. The skew-symmetric part has 2
degrees of freedom and is identiﬁed with the point xa.
The relation between the point xa and conic Fs is shown in ﬁgure 9.10a. The polar
of xa intersects the Steiner conic Fs at the epipoles e and e(cid:2)
(the pole–polar relation is
described in section 2.2.3(p30)). The proof of this result is left as an exercise.

Epipolar line correspondence.
It is a classical theorem of projective geometry due
to Steiner [Semple-79] that for two line pencils related by a homography, the locus
of intersections of corresponding lines is a conic. This is precisely the situation here.
The pencils are the epipolar pencils, one through e and the other through e(cid:2)
. The
epipolar lines are related by a 1D homography as described in section 9.2.5. The locus
of intersection is the conic Fs.

The conic and epipoles enable epipolar lines to be determined by a geometric con-
struction as illustrated in ﬁgure 9.10b. This construction is based on the ﬁxed point
property of the Steiner conic Fs. The epipolar line l = x × e in the ﬁrst view deﬁnes
an epipolar plane in 3-space which intersects the horopter in a point, which we will
call Xc. The point Xc is imaged in the ﬁrst view at xc, which is the point at which l
intersects the conic Fs (since Fs is the image of the horopter). Now the image of Xc is
also xc in the second view due to the ﬁxed-point property of the horopter. So xc is the

252

9 Epipolar Geometry and the Fundamental Matrix

F
s

e

/e

l

a

x a
a

xc

Fs

/

l

/

e

x

b

e

Fig. 9.10. Geometric representation of F. (a) The conic Fs represents the symmetric part of F, and the
point xa the skew-symmetric part. The conic Fs is the locus of intersection of corresponding epipolar
lines, assuming both images are overlaid on top of each other. It is the image of the horopter curve. The
line la is the polar of xa with respect to the conic Fs. It intersects the conic at the epipoles e and e(cid:1)
.
(b) The epipolar line l(cid:1)
corresponding to a point x is constructed as follows: intersect the line deﬁned by
the points e and x with the conic. This intersection point is xc. Then l(cid:1)
is the line deﬁned by the points
xc and e(cid:1)

.

image in the second view of a point on the epipolar plane of x. It follows that xc lies
on the epipolar line l(cid:2)

of x, and consequently l(cid:2)

may be computed as l(cid:2)

= xc × e(cid:2)

.

The conic together with two points on the conic account for the 7 degrees of freedom
of F: 5 degrees of freedom for the conic and one each to specify the two epipoles on
the conic. Given F, then the conic Fs, epipoles e, e(cid:2)
and skew-symmetric point xa are
deﬁned uniquely. However, Fs and xa do not uniquely determine F since the identity
of the epipoles is not recovered, i.e. the polar of xa determines the epipoles but does
not determine which one is e and which one e(cid:2)

.

9.4.1 Pure planar motion
We return to the case of planar motion discussed above in section 9.3.2, where Fs has
rank 2. It is evident that in this case the Steiner conic is degenerate and from section
2.2.3(p30) is equivalent to two non-coincident lines:
s + lslT

Fs = lhlT

as depicted in ﬁgure 9.11a. The geometric construction of the epipolar line l(cid:2)
corre-
sponding to a point x of section 9.4 has a simple algebraic representation in this case.
As in the general motion case, there are three steps, illustrated in ﬁgure 9.11b: ﬁrst
the line l = e × x joining e and x is computed; second, its intersection point with the
“conic” xc = ls × l is determined; third the epipolar line l(cid:2)
= e(cid:2) × xc is the join of xc
and e(cid:2)

. Putting these steps together we ﬁnd

h

l(cid:2)

= e(cid:2) × [ls × (e × x)] = [e(cid:2)

]×[ls]×[e]×x.

It follows that F may be written as

F = [e(cid:2)

]×[ls]×[e]×.

(9.8)

The 6 degrees of freedom of F are accounted for as 2 degrees of freedom for each of
the two epipoles and 2 degrees of freedom for the line.

9.5 Retrieving the camera matrices

253

l s

e

x

s

/

e

l

h

x
a

image

a

x c

l

x

l s

l /

e

e/

image

b

Fig. 9.11. Geometric representation of F for planar motion. (a) The lines ls and lh constitute the
Steiner conic for this motion, which is degenerate. Compare this ﬁgure with the conic for general
corresponding to a point x is constructed as
motion shown in ﬁgure 9.10.
follows: intersect the line deﬁned by the points e and x with the (conic) line ls. This intersection point
is xc. Then l(cid:1)

(b) The epipolar line l(cid:1)
is the line deﬁned by the points xc and e(cid:1)

.

The geometry of this situation can be easily visualized: the horopter for this motion
is a degenerate twisted cubic consisting of a circle in the plane of the motion (the plane
orthogonal to the rotation axis and containing the camera centres), and a line parallel
to the rotation axis and intersecting the circle. The line is the screw axis (see section
3.4.1(p77)). The motion is equivalent to a rotation about the screw axis with zero
translation. Under this motion points on the screw axis are ﬁxed, and consequently
their images are ﬁxed. The line ls is the image of the screw axis. The line lh is the
intersection of the image with the plane of the motion. This geometry is used for auto-
calibration in chapter 19.

9.5 Retrieving the camera matrices

To this point we have examined the properties of F and of image relations for a point
correspondence x ↔ x(cid:2)
. We now turn to one of the most signiﬁcant properties of F,
that the matrix may be used to determine the camera matrices of the two views.

9.5.1 Projective invariance and canonical cameras
It is evident from the derivations of section 9.2 that the map l(cid:2)
= Fx and the correspon-
dence condition x(cid:2)TFx = 0 are projective relationships: the derivations have involved
only projective geometric relationships, such as the intersection of lines and planes, and
in the algebraic development only the linear mapping of the projective camera between
world and image points. Consequently, the relationships depend only on projective co-
ordinates in the image, and not, for example on Euclidean measurements such as the
angle between rays. In other words the image relationships are projectively invariant:
under a projective transformation of the image coordinates ˆx = Hx, ˆx(cid:2)
, there is a
(cid:2)
= ˆFˆx with ˆF = H(cid:2)−TFH−1 the corresponding rank 2 fundamental
corresponding map ˆl
matrix.

= H(cid:2)x(cid:2)

Similarly, F only depends on projective properties of the cameras P, P(cid:2)

. The camera
matrix relates 3-space measurements to image measurements and so depends on both
the image coordinate frame and the choice of world coordinate frame. F does not

254

9 Epipolar Geometry and the Fundamental Matrix

depend on the choice of world frame, for example a rotation of world coordinates
changes P, P(cid:2)
, but not F. In fact, the fundamental matrix is unchanged by a projective
transformation of 3-space. More precisely,
Result 9.8. If H is a 4 × 4 matrix representing a projective transformation of 3-space,
then the fundamental matrices corresponding to the pairs of camera matrices (P, P(cid:2)
)
and (PH, P(cid:2)H) are the same.
Proof. Observe that PX = (PH)(H−1X), and similarly for P(cid:2)
are
matched points with respect to the pair of cameras (P, P(cid:2)
), corresponding to a 3D point
X, then they are also matched points with respect to the pair of cameras (PH, P(cid:2)H),
corresponding to the point H−1X.
Thus, although from (9.1–p244) a pair of camera matrices (P, P(cid:2)
) uniquely determine a
fundamental matrix F, the converse is not true. The fundamental matrix determines the
pair of camera matrices at best up to right-multiplication by a 3D projective transfor-
mation. It will be seen below that this is the full extent of the ambiguity, and indeed the
camera matrices are determined up to a projective transformation by the fundamental
matrix.

. Thus if x ↔ x(cid:2)

Canonical form of camera matrices. Given this ambiguity, it is common to deﬁne
a speciﬁc canonical form for the pair of camera matrices corresponding to a given
fundamental matrix in which the ﬁrst matrix is of the simple form [I | 0], where I is
the 3 × 3 identity matrix and 0 a null 3-vector. To see that this is always possible, let
P be augmented by one row to make a 4 × 4 non-singular matrix, denoted P∗
. Now
letting H = P∗−1, one veriﬁes that PH = [I | 0] as desired.
The following result is very frequently used

Result 9.9. The fundamental matrix corresponding to a pair of camera matrices P =
[I | 0] and P(cid:2)
This is easily derived as a special case of (9.1–p244).

= [M | m] is equal to [m]×M.

9.5.2 Projective ambiguity of cameras given F
It has been seen that a pair of camera matrices determines a unique fundamental matrix.
This mapping is not injective (one-to-one) however, since pairs of camera matrices that
differ by a projective transformation give rise to the same fundamental matrix. It will
now be shown that this is the only ambiguity. We will show that a given fundamental
matrix determines the pair of camera matrices up to right multiplication by a projective
transformation. Thus, the fundamental matrix captures the projective relationship of
the two cameras.
Theorem 9.10. Let F be a fundamental matrix and let (P, P(cid:2)
) be two pairs of
camera matrices such that F is the fundamental matrix corresponding to each of these
pairs. Then there exists a non-singular 4 × 4 matrix H such that ˜P = PH and ˜P
= P(cid:2)H.

) and (˜P, ˜P

(cid:2)

(cid:2)

9.5 Retrieving the camera matrices

255

(cid:2)

) and (˜P, ˜P

Proof. Suppose that a given fundamental matrix F corresponds to two different pairs
of camera matrices (P, P(cid:2)
). As a ﬁrst step, we may simplify the problem
by assuming that each of the two pair of camera matrices is in canonical form with
P = ˜P = [I | 0], since this may be done by applying projective transformations to
each pair as necessary. Thus, suppose that P = ˜P = [I | 0] and that P(cid:2)
= [A | a]
= [˜A | ˜a]. According to result 9.9 the fundamental matrix may then be written
and ˜P
F = [a]×A = [˜a]×˜A.
We will need the following lemma:

(cid:2)

Lemma 9.11. Suppose the rank 2 matrix F can be decomposed in two different ways
−1(A + avT) for some non-zero
as F = [a]×A and F = [˜a]×˜A; then ˜a = ka and ˜A = k
constant k and 3-vector v.

(cid:15)
(cid:16)
k˜A − A

= 0, and so k˜A − A = avT for some v. Hence, ˜A = k

Proof. First, note that aTF = aT[a]×A = 0, and similarly, ˜aTF = 0. Since F has
rank 2, it follows that ˜a = ka as required. Next, from [a]×A = [˜a]×˜A it follows that
−1(A + avT) as
[a]×
required.
= [A | a] and
Applying this result to the two camera matrices P(cid:2)
shows that P(cid:2)
(cid:18)
(cid:17)
(cid:2)
−1(A+avT) | ka] if they are to generate the same F. It only remains now to show
˜P
.

that these camera pairs are projectively related. Let H be the matrix H =

(cid:2)
and ˜P

= [k

−1I
0
k
−1vT k
k

Then one veriﬁes that PH = k

−1[I | 0] = k

−1˜P, and furthermore,

P(cid:2)H = [A | a]H = [k

−1(A + avT) | ka] = [˜A | ˜a] = ˜P

(cid:2)

so that the pairs P, P(cid:2)

and ˜P, ˜P

(cid:2)

are indeed projectively related.

This can be tied precisely to a counting argument: the two cameras P and P(cid:2)

each
have 11 degrees of freedom, making a total of 22 degrees of freedom. To specify a
projective world frame requires 15 degrees of freedom (section 3.1(p65)), so once the
degrees of freedom of the world frame are removed from the two cameras 22− 15 = 7
degrees of freedom remain – which corresponds to the 7 degrees of freedom of the
fundamental matrix.

9.5.3 Canonical cameras given F
We have shown that F determines the camera pair up to a projective transformation of
3-space. We will now derive a speciﬁc formula for a pair of cameras with canonical
form given F. We will make use of the following characterization of the fundamental
matrix F corresponding to a pair of camera matrices:

if and only if P(cid:2)TFP is skew-symmetric.

Result 9.12. A non-zero matrix F is the fundamental matrix corresponding to a pair of
camera matrices P and P(cid:2)
Proof. The condition that P(cid:2)TFP is skew-symmetric is equivalent to XTP(cid:2)TFPX = 0
= P(cid:2)X and x = PX, this is equivalent to x(cid:2)TFx = 0, which is the
for all X. Setting x(cid:2)
deﬁning equation for the fundamental matrix.

256

9 Epipolar Geometry and the Fundamental Matrix

One may write down a particular solution for the pairs of camera matrices in canon-

ical form that correspond to a fundamental matrix as follows:

Result 9.13. Let F be a fundamental matrix and S any skew-symmetric matrix. Deﬁne
the pair of camera matrices

P = [I | 0]

and P(cid:2)

= [SF | e(cid:2)

],

is the epipole such that e(cid:2)TF = 0, and assume that P(cid:2)

where e(cid:2)
so deﬁned is a valid
camera matrix (has rank 3). Then F is the fundamental matrix corresponding to the
pair (P, P(cid:2)

).

To demonstrate this, we invoke result 9.12 and simply verify that

(cid:17)

(cid:18)

(cid:17)

[SF | e(cid:2)

]TF[I | 0] =

FTSTF 0
e(cid:2)TF
0

=

FTSTF 0
0

0T

(cid:18)

(9.9)

which is skew-symmetric.
The skew-symmetric matrix S may be written in terms of its null-vector as S = [s]×.
Then [[s]×F | e(cid:2)
(cid:5)= 0, according to the following argument.
] has rank 3 provided sTe(cid:2)
Since e(cid:2)F = 0, the column space (span of the columns) of F is perpendicular to e(cid:2)
. But
if sTe(cid:2) (cid:5)= 0, then s is not perpendicular to e(cid:2)
, and hence not in the column space of F.
Now, the column space of [s]×F is spanned by the cross-products of s with the columns
of F, and therefore equals the plane perpendicular to s. So [s]×F has rank 2. Since e(cid:2)
is not perpendicular to s, it does not lie in this plane, and so [[s]×F | e(cid:2)
] has rank 3, as
required.
As suggested by Luong and Vi´eville [Luong-96] a good choice for S is S = [e(cid:2)
in this case e(cid:2)Te(cid:2) (cid:5)= 0, which leads to the following useful result.

]×, for

].

= [[e(cid:2)

]×F | e(cid:2)

has left 3 × 3 submatrix [e(cid:2)

Result 9.14. The camera matrices corresponding to a fundamental matrix F may be
chosen as P = [I | 0] and P(cid:2)
Note that the camera matrix P(cid:2)
]×F which has rank 2. This
corresponds to a camera with centre on π∞. However, there is no particular reason to
avoid this situation.
The proof of theorem 9.10 shows that the four parameter family of camera pairs in
canonical form ˜P = [I | 0], ˜P
= [A + avT | ka] have the same fundamental matrix as
the canonical pair, P = [I | 0], P(cid:2)
= [A | a]; and that this is the most general solution.
To summarize:

(cid:2)

Result 9.15. The general formula for a pair of canonic camera matrices corresponding
to a fundamental matrix F is given by
P = [I | 0] P(cid:2)

]×F + e(cid:2)vT | λe(cid:2)

= [[e(cid:2)

(9.10)

]

where v is any 3-vector, and λ a non-zero scalar.

9.6 The essential matrix

9.6 The essential matrix

257

The essential matrix is the specialization of the fundamental matrix to the case of
normalized image coordinates (see below). Historically, the essential matrix was in-
troduced (by Longuet-Higgins) before the fundamental matrix, and the fundamental
matrix may be thought of as the generalization of the essential matrix in which the
(inessential) assumption of calibrated cameras is removed. The essential matrix has
fewer degrees of freedom, and additional properties, compared to the fundamental ma-
trix. These properties are described below.
Normalized coordinates. Consider a camera matrix decomposed as P = K[R | t],
and let x = PX be a point in the image. If the calibration matrix K is known, then we
may apply its inverse to the point x to obtain the point ˆx = K−1x. Then ˆx = [R | t]X,
where ˆx is the image point expressed in normalized coordinates. It may be thought of
as the image of the point X with respect to a camera [R | t] having the identity matrix I
as calibration matrix. The camera matrix K−1P = [R | t] is called a normalized camera
matrix, the effect of the known calibration matrix having been removed.
Now, consider a pair of normalized camera matrices P = [I | 0] and P(cid:2)
= [R | t]. The
fundamental matrix corresponding to the pair of normalized cameras is customarily
called the essential matrix, and according to (9.2–p244) it has the form

E = [t]×R = R [RTt]×.

Deﬁnition 9.16. The deﬁning equation for the essential matrix is

ˆx(cid:2)TEˆx = 0

(9.11)

in terms of the normalized image coordinates for corresponding points x ↔ x(cid:2)
Substituting for ˆx and ˆx(cid:2)
gives x(cid:2)TK(cid:2)−TEK−1x = 0. Comparing this with the relation
x(cid:2)TFx = 0 for the fundamental matrix, it follows that the relationship between the
fundamental and essential matrices is

.

E = K(cid:2)TFK.

(9.12)

9.6.1 Properties of the essential matrix
The essential matrix, E = [t]×R, has only ﬁve degrees of freedom: both the rotation
matrix R and the translation t have three degrees of freedom, but there is an overall
scale ambiguity – like the fundamental matrix, the essential matrix is a homogeneous
quantity.

The reduced number of degrees of freedom translates into extra constraints that are
satisﬁed by an essential matrix, compared with a fundamental matrix. We investigate
what these constraints are.
Result 9.17. A 3×3 matrix is an essential matrix if and only if two of its singular values
are equal, and the third is zero.

9 Epipolar Geometry and the Fundamental Matrix

258
Proof. This is easily deduced from the decomposition of E as [t]×R = SR, where S is
skew-symmetric. We will use the matrices

 0 −1 0

 and Z =

 0

W =

1
0

0
0

0
1

 .

1 0
−1 0 0
0 0
0

(9.13)

It may be veriﬁed that W is orthogonal and Z is skew-symmetric. From Result A4.1-
(p581), which gives a block decomposition of a general skew-symmetric matrix, the
3 × 3 skew-symmetric matrix S may be written as S = kUZUT where U is orthogonal.
Noting that, up to sign, Z = diag(1, 1, 0)W, then up to scale, S = U diag(1, 1, 0)WUT, and
E = SR = U diag(1, 1, 0)(WUTR). This is a singular value decomposition of E with two
equal singular values, as required. Conversely, a matrix with two equal singular values
may be factored as SR in this way.
Since E = U diag(1, 1, 0)VT,

it may seem that E has six degrees of freedom
and not ﬁve, since both U and V have three degrees of freedom. However, be-
cause the two singular values are equal, the SVD is not unique – in fact there is
a one-parameter family of SVDs for E.
Indeed, an alternative SVD is given by
E = (U diag(R2×2, 1)) diag(1, 1, 0)(diag(RT

2×2, 1))VT for any 2 × 2 rotation matrix R.

9.6.2 Extraction of cameras from the essential matrix
The essential matrix may be computed directly from (9.11) using normalized image
coordinates, or else computed from the fundamental matrix using (9.12). (Methods
of computing the fundamental matrix are deferred to chapter 11). Once the essential
matrix is known, the camera matrices may be retrieved from E as will be described next.
In contrast with the fundamental matrix case, where there is a projective ambiguity, the
camera matrices may be retrieved from the essential matrix up to scale and a four-fold
ambiguity. That is there are four possible solutions, except for overall scale, which
cannot be determined.
We may assume that the ﬁrst camera matrix is P = [I | 0]. In order to compute the
second camera matrix, P(cid:2)
, it is necessary to factor E into the product SR of a skew-
symmetric matrix and a rotation matrix.
Result 9.18. Suppose that the SVD of E is U diag(1, 1, 0)VT. Using the notation of
(9.13), there are (ignoring signs) two possible factorizations E = SR as follows:

S = UZUT

R = UWVT or UWTVT .

(9.14)

Proof. That the given factorization is valid is true by inspection. That there are no
other factorizations is shown as follows. Suppose E = SR. The form of S is determined
by the fact that its left null-space is the same as that of E. Hence S = UZUT. The
rotation R may be written as UXVT, where X is some rotation matrix. Then

U diag(1, 1, 0)VT = E = SR = (UZUT)(UXVT) = U(ZX)VT

from which one deduces that ZX = diag(1, 1, 0). Since X is a rotation matrix, it follows
that X = W or X = WT, as required.

9.7 Closure

259

√

The factorization (9.14) determines the t part of the camera matrix P(cid:2)
, up to scale,
from S = [t]×. However, the Frobenius norm of S = UZUT is
2, which means
that if S = [t]× including scale then (cid:10)t(cid:10) = 1, which is a convenient normaliza-
tion for the baseline of the two camera matrices. Since St = 0, it follows that
t = U (0, 0, 1)T = u3, the last column of U. However, the sign of E, and consequently t,
cannot be determined. Thus, corresponding to a given essential matrix, there are four
possible choices of the camera matrix P(cid:2)
, based on the two possible choices of R and
two possible signs of t. To summarize:
Result 9.19. For a given essential matrix E = U diag(1, 1, 0)VT, and ﬁrst camera matrix
P = [I | 0], there are four possible choices for the second camera matrix P(cid:2)

, namely

P(cid:2)

= [UWVT | +u3] or [UWVT | −u3] or [UWTVT | +u3] or [UWTVT | −u3].

9.6.3 Geometrical interpretation of the four solutions
It is clear that the difference between the ﬁrst two solutions is simply that the direction
of the translation vector from the ﬁrst to the second camera is reversed.

The relationship of the ﬁrst and third solutions in result 9.19 is a little more compli-

cated. However, it may be veriﬁed that

(cid:17)

(cid:18)

[UWTVT | u3] = [UWVT | u3]

VWTWTVT

1
and VWTWTVT = V diag(−1,−1, 1)VT is a rotation through 180
◦
about the line joining
the two camera centres. Two solutions related in this way are known as a “twisted
pair”.

The four solutions are illustrated in ﬁgure 9.12, where it is shown that a reconstructed
point X will be in front of both cameras in one of these four solutions only. Thus, testing
with a single point to determine if it is in front of both cameras is sufﬁcient to decide
between the four different solutions for the camera matrix P(cid:2)
Note. The point of view has been taken here that the essential matrix is a homogeneous
quantity. An alternative point of view is that the essential matrix is deﬁned exactly by
the equation E = [t]×R, (i.e. including scale), and is determined only up to indetermi-
nate scale by the equation x(cid:2)TEx = 0. The choice of point of view depends on which
of these two equations one regards as the deﬁning property of the essential matrix.

.

9.7 Closure

9.7.1 The literature
The essential matrix was introduced to the computer vision community by Longuet-
Higgins [LonguetHiggins-81], with a matrix analogous to E appearing in the pho-
togrammetry literature, e.g. [VonSanden-08]. Many properties of the essential matrix
have been elucidated particularly by Huang and Faugeras [Huang-89], [Maybank-93],
and [Horn-90].

The realization that the essential matrix could also be applied in uncalibrated situa-
tions, as it represented a projective relation, developed in the early part of the 1990s,

260

9 Epipolar Geometry and the Fundamental Matrix

A

B

B

A

(a)

(b)

A

B /

B /

A

(c)

(d)

Fig. 9.12. The four possible solutions for calibrated reconstruction from E. Between the left and
right sides there is a baseline reversal. Between the top and bottom rows camera B rotates 180◦
about
the baseline. Note, only in (a) is the reconstructed point in front of both cameras.

and was published simultaneously by Faugeras [Faugeras-92b, Faugeras-92a], and
Hartley et al. [Hartley-92a, Hartley-92c].

The special case of pure planar motion was examined by [Maybank-93] for the
essential matrix. The corresponding case for the fundamental matrix is investigated
by Beardsley and Zisserman [Beardsley-95a] and Vi´eville and Lingrand [Vieville-95],
where further properties are given.

9.7.2 Notes and exercises

(i) Fixating cameras. Suppose two cameras ﬁxate on a point in space such that
their principal axes intersect at that point. Show that if the image coordinates
are normalized so that the coordinate origin coincides with the principal point
then the F33 element of the fundamental matrix is zero.

(ii) Mirror images. Suppose that a camera views an object and its reﬂection in a
plane mirror. Show that this situation is equivalent to two views of the object,
and that the fundamental matrix is skew-symmetric. Compare the fundamental
matrix for this conﬁguration with that of: (a) a pure translation, and (b) a pure
planar motion. Show that the fundamental matrix is auto-epipolar (as is (a)).

(iii) Show that if the vanishing line of a plane contains the epipole then the plane is

parallel to the baseline.

(iv) Steiner conic. Show that the polar of xa intersects the Steiner conic Fs at the
epipoles (ﬁgure 9.10a). Hint, start from Fe = Fse + Fae = 0. Since e lies on

261
the conic Fs, then l1 = Fse is the tangent line at e, and l2 = Fae = [xa]×e =
xa × e is a line through xa and e.

9.7 Closure

(v) The afﬁne type of the Steiner conic (hyperbola, ellipse or parabola as given in
section 2.8.2(p59)) depends on the relative conﬁguration of the two cameras.
For example, if the two cameras are facing each other then the Steiner conic
is a hyperbola. This is shown in [Chum-03] where further results on oriented
epipolar geometry are given.

(vi) Planar motion. It is shown by [Maybank-93] that if the rotation axis direction
is orthogonal or parallel to the translation direction then the symmetric part of
the essential matrix has rank 2. We assume here that K = K(cid:2)
. Then from (9.12),
F = K−TEK−1, and so

Fs = (F + FT)/2 = K−T(E + ET)K−1/2 = K−TEsK−1.

It follows from det(Fs) = det(K−1)2 det(Es) that the symmetric part of F is
also singular. Does this result hold if K (cid:5)= K(cid:2)
camera matrices (P, P(cid:2)
for the canonical cameras depends only on the rank 2 property of F.

(vii) Any matrix F of rank 2 is the fundamental matrix corresponding to some pair of
) This follows directly from result 9.14 since the solution

?

(viii) Show that the 3D points determined from one of the ambiguous reconstructions
obtained from E are related to the corresponding 3D points determined from
another reconstruction by either (i) an inversion through the second camera
centre; or (ii) a harmonic homology of 3-space (see section A7.2(p629)), where
the homology plane is perpendicular to the baseline and through the second
camera centre, and the vertex is the ﬁrst camera centre.

(ix) Following a similar development to section 9.2.2, derive the form of the fun-
damental matrix for two linear pushbroom cameras. Details of this matrix are
given in [Gupta-97] where it is shown that afﬁne reconstruction is possible from
a pair of images.

10

3D Reconstruction of Cameras and Structure

This chapter describes how and to what extent the spatial layout of a scene and the
cameras can be recovered from two views. Suppose that a set of image correspondences
xi ↔ x(cid:2)
i are given. It is assumed that these correspondences come from a set of 3D
points Xi, which are unknown. Similarly, the position, orientation and calibration of
the cameras are not known. The reconstruction task is to ﬁnd the camera matrices P
and P(cid:2)

, as well as the 3D points Xi such that

xi = PXi x(cid:2)

i = P(cid:2)Xi

for all i.

Given too few points, this task is not possible. However, if there are sufﬁciently many
point correspondences to allow the fundamental matrix to be computed uniquely, then
the scene may be reconstructed up to a projective ambiguity. This is a very signiﬁcant
result, and one of the major achievements of the uncalibrated approach.

The ambiguity in the reconstruction may be reduced if additional information is sup-
plied on the cameras or scene. We describe a two-stage approach where the ambiguity
is ﬁrst reduced to afﬁne, and second to metric; each stage requiring information of the
appropriate class.

We describe a method for reconstruction from two views as follows.

10.1 Outline of reconstruction method

(i) Compute the fundamental matrix from point correspondences.
(ii) Compute the camera matrices from the fundamental matrix.
(iii) For each point correspondence xi ↔ x(cid:2)

i, compute the point in space that

projects to these two image points.

Many variants on this method are possible. For instance, if the cameras are calibrated,
then one will compute the essential matrix instead of the fundamental matrix. Further-
more, one may use information about the motion of the camera, scene constraints or
partial camera calibration to obtain reﬁnements of the reconstruction.

Each of the steps of this reconstruction method will be discussed brieﬂy in the fol-
lowing paragraphs. The method described is no more than a conceptual approach to
reconstruction. The reader is warned not to implement a reconstruction method based
solely on the description given in this section. For real images where measurements

262

10.1 Outline of reconstruction method

263

X

x

/

x

Fig. 10.1. Triangulation. The image points x and x(cid:1)
x(cid:1)TFx = 0 is satisﬁed, then these two rays lie in a plane, and so intersect in a point X in 3-space.

back project to rays. If the epipolar constraint

are “noisy” preferred methods for reconstruction, based on this general outline, are
described in chapter 11 and chapter 12.
Computation of the fundamental matrix. Given a set of correspondences xi ↔ x(cid:2)
in two images the fundamental matrix F satisﬁes the condition x(cid:2)
i
Fxi = 0 for all i. With
the xi and x(cid:2)
i known, this equation is linear in the (unknown) entries of the matrix F. In
fact, each point correspondence generates one linear equation in the entries of F. Given
at least 8 point correspondences it is possible to solve linearly for the entries of F up to
scale (a non-linear solution is available for 7 point correspondences). With more than
8 equations a least-squares solution is found. This is the general principle of a method
for computing the fundamental matrix.

i

Recommended methods of computing the fundamental matrix from a set of point

correspondences will be described later in chapter 11.

Computation of the camera matrices. A pair of camera matrices P and P(cid:2)
corre-
sponding to the fundamental matrix F are easily computed using the direct formula in
result 9.14.

, let x and x(cid:2)

Triangulation. Given the camera matrices P and P(cid:2)
be two points in the
two images that satisfy the epipolar constraint, x(cid:2)TFx = 0. As shown in chapter 9 this
constraint may be interpreted geometrically in terms of the rays in space corresponding
to the two image points. In particular it means that x(cid:2)
lies on the epipolar line Fx. In
turn this means that the two rays back-projected from image points x and x(cid:2)
lie in a
common epipolar plane, that is, a plane passing through the two camera centres. Since
the two rays lie in a plane, they will intersect in some point. This point X projects
via the two cameras to the points x and x(cid:2)
in the two images. This is illustrated in
ﬁgure 10.1.

The only points in 3-space that cannot be determined from their images are points on
the baseline between the two cameras. In this case the back-projected rays are collinear
(both being equal to the baseline) and intersect along their whole length. Thus, the point

10 3D Reconstruction of Cameras and Structure

264
X cannot be uniquely determined. Points on the baseline project to the epipoles in both
images.
of the two rays back-projected from x and x(cid:2)

Numerically stable methods of actually determining the point X at the intersection

will be described later in chapter 12.

10.2 Reconstruction ambiguity

In this section we discuss the inherent ambiguities involved in reconstruction of a scene
from point correspondences. This topic will be discussed in a general context, without
reference to a speciﬁc method of carrying out the reconstruction.

Without some knowledge of a scene’s placement with respect to a 3D coordinate
frame, it is generally not possible to reconstruct the absolute position or orientation
of a scene from a pair of views (or in fact from any number of views). This is true
independently of any knowledge which may be available about the internal parameters
of the cameras, or their relative placement. For instance the exact latitude and longitude
of the scene in ﬁgure 9.8(p248) (or any scene) cannot be computed, nor is it possible to
determine whether the corridor runs north-south or east-west. This may be expressed
by saying that the scene is determined at best up to a Euclidean transformation (rotation
and translation) with respect to the world frame.

Only slightly less obvious is the fact that the overall scale of the scene cannot be
determined. Considering ﬁgure 9.8(p248) once more, it is impossible based on the
images alone to determine the width of the corridor. It may be two metres, one metre.
It is even possible that this is an image of a doll’s house and the corridor is 10 cm
wide. Our common experience leads us to expect that ceilings are approximately 3m
from the ﬂoor, which allows us to perceive the real scale of the scene. This extra
information is an example of subsidiary knowledge of the scene not derived from image
measurements. Without such knowledge therefore the scene is determined by the image
only up to a similarity transformation (rotation, translation and scaling).
To give a mathematical basis to this observation, let Xi be a set of points and P, P(cid:2)
be a pair of cameras projecting Xi to image points xi and x(cid:2)
i. The points Xi and the
camera pair constitute a reconstruction of the scene from the image correspondences.
Now let

HS =

t
R
0T λ

(cid:17)

(cid:18)

be any similarity transformation: R is a rotation, t a translation and λ
all scaling. Replacing each point Xi by HSXi and cameras P and P(cid:2)
respectively does not change the observed image points, since PXi = (PH−1
Furthermore, if P is decomposed as P = K[RP | tP], then one computes

−1 represents over-
S and P(cid:2)H−1
by PH−1
S )(HSXi).

S

PH−1

S = K[RPR−1 | t(cid:2)

]

for some t(cid:2)
that we do not need to compute more exactly. This result shows that multi-
plying by H−1
S does not change the calibration matrix of P. Consequently this ambiguity
of reconstruction exists even for calibrated cameras. It was shown by Longuet-Higgins

10.2 Reconstruction ambiguity

265

Similarity

Projective

a

b

Fig. 10.2. Reconstruction ambiguity. (a) If the cameras are calibrated then any reconstruction must
respect the angle between rays measured in the image. A similarity transformation of the structure
and camera positions does not change the measured angle. The angle between rays and the baseline
(epipoles) is also unchanged. (b) If the cameras are uncalibrated then reconstructions must only respect
the image points (the intersection of the rays with the image plane). A projective transformation of the
structure and camera positions does not change the measured points, although the angle between rays
is altered. The epipoles are also unchanged (intersection with baseline).

([LonguetHiggins-81]) that for calibrated cameras, this is the only ambiguity of recon-
struction. Thus for calibrated cameras, reconstruction is possible up to a similarity
transformation. This is illustrated in ﬁgure 10.2a.

Projective ambiguity.
If nothing is known of the calibration of either camera, nor the
placement of one camera with respect to the other, then the ambiguity of reconstruction
is expressed by an arbitrary projective transformation. In particular, if H is any 4 × 4
invertible matrix, representing a projective transformation of IP3, then replacing points
Xi by HXi and matrices P and P(cid:2)
by PH−1 and P(cid:2)H−1 (as in the previous paragraph)
does not change the image points. This shows that the points Xi and the cameras can
be determined at best only up to a projective transformation. It is an important result,
proved in this chapter (section 10.3), that this is the only ambiguity in the reconstruction
of points from two images. Thus reconstruction from uncalibrated cameras is possible
up to a projective transformation. This is illustrated in ﬁgure 10.2b.

Other types of reconstruction ambiguity result from certain assumptions on the types

of motion, or partial knowledge of the cameras. For instance,

(i) If the two cameras are related via a translational motion, without change of

calibration, then reconstruction is possible up to an afﬁne transformation.

(ii) If the two cameras are calibrated apart from their focal lengths, then reconstruc-

tion is still possible up to a similarity transformation.

These two cases will be considered later in section 10.4.1 and example 19.8(p472),
respectively.

Terminology.
point correspondences xi ↔ x(cid:2)
tual points ¯Xi and actual cameras ¯P, ¯P

In any reconstruction problem derived from real data, consisting of
i, there exists a true reconstruction consisting of the ac-
that generated the measured observations. The

(cid:2)

10 3D Reconstruction of Cameras and Structure

266
reconstructed point set Xi and cameras differ from the true reconstruction by a trans-
formation belonging to a given class or group (for instance a similarity, projective or
afﬁne transformation). One speaks of projective reconstruction, afﬁne reconstruction,
similarity reconstruction, and so on, to indicate the type of transformation involved.
However, the term metric reconstruction is normally used in preference to similarity
reconstruction, being identical in meaning. The term indicates that metric properties,
such as angles between lines and ratios of lengths, can be measured on the reconstruc-
tion and have their veridical values (since these are similarity invariants). In addition,
the term Euclidean reconstruction is frequently used in the published literature to mean
the same thing as a similarity or metric reconstruction, since true Euclidean recon-
struction (including determination of overall scale) is not possible without extraneous
information.

10.3 The projective reconstruction theorem

In this section the basic theorem of projective reconstruction from uncalibrated cameras
is proved. Informally, the theorem may be stated as follows.
• If a set of point correspondences in two views determine the fundamental matrix
uniquely, then the scene and cameras may be reconstructed from these correspon-
dences alone, and any two such reconstructions from these correspondences are pro-
jectively equivalent.

Points lying on the line joining the two camera centres must be excluded, since such
points cannot be reconstructed uniquely even if the camera matrices are determined.
The formal statement is:
Theorem 10.1 (Projective reconstruction theorem). Suppose that xi ↔ x(cid:2)
i is a set
of correspondences between points in two images and that the fundamental matrix F
1,{X1i}) and
is uniquely determined by the condition x(cid:2)
(P2, P(cid:2)
i. Then there
H−1 and X2i = HX1i for
exists a non-singular matrix H such that P2 = P1H−1, P(cid:2)
all i, except for those i such that Fxi = x(cid:2)

TFxi = 0 for all i. Let (P1, P(cid:2)
2,{X2i}) be two reconstructions of the correspondences xi ↔ x(cid:2)

2 = P(cid:2)

TF = 0.

1

i

i

2 = P(cid:2)

1) and also to (P2, P(cid:2)

Proof. Since the fundamental matrix is uniquely determined by the point correspon-
dences, one deduces that F is the fundamental matrix corresponding to the camera pair
(P1, P(cid:2)
2). According to theorem 9.10(p254) there is a projective
transformation H such that P2 = P1H−1 and P(cid:2)
As for the points, one observes that P2(HX1i) = P1H−1HX1i = P1X1i = xi. On the
other hand P2X2i = xi, soP 2(HX1i) =P 2X2i. Thus both HX1i and X2i map to the same
point xi under the action of the camera P2. It follows that both HX1i and X2i lie on the
same ray through the camera centre of P2. Similarly, it may be deduced that these two
points lie on the same ray through the camera centre of P(cid:2)
2. There are two possibilities:
either X2i = HX1i as required, or they are distinct points lying on the line joining the
two camera centres. In this latter case, the image points xi and x(cid:2)
i coincide with the
epipoles in the two images, and so Fxi = x(cid:2)

H−1 as required.

TF = 0.

1

i

10.4 Stratiﬁed reconstruction

267

a

b

Fig. 10.3. Projective reconstruction.
(b) 2 views of a 3D projective re-
construction of the scene. The reconstruction requires no information about the camera matrices, or
information about the scene geometry. The fundamental matrix F is computed from point correspon-
dences between the images, camera matrices are retrieved from F, and then 3D points are computed by
triangulation from the correspondences. The lines of the wireframe link the computed 3D points.

(a) Original image pair.

This is an enormously signiﬁcant result, since it implies that one may compute a
projective reconstruction of a scene from two views based on image correspondences
alone, without knowing anything about the calibration or pose of the two cameras in-
volved. In particular the true reconstruction is within a projective transformation of the
projective reconstruction. Figure 10.3 shows an example of 3D structure computed as
part of a projective reconstruction from two images.
E,{XEi}) and the
,{Xi}), then the reconstructions are related by a non-
projective reconstruction is (P, P(cid:2)
singular matrix H such that

In more detail suppose the true Euclidean reconstruction is (PE, P(cid:2)

PE = PH−1, P(cid:2)

E = P(cid:2)H−1, and XEi = HXi

(10.1)
where H is a 4 × 4 homography matrix which is unknown but the same for all points.
For some applications projective reconstruction is all that is required. For example,
questions such as “at what point does a line intersect a plane?”, “what is the mapping
between two views induced by particular surfaces, such as a plane or quadric?” can be
dealt with directly from the projective reconstruction. Furthermore it will be seen in
the sequel that obtaining a projective reconstruction of a scene is the ﬁrst step towards
afﬁne or metric reconstruction.

10.4 Stratiﬁed reconstruction

The “stratiﬁed” approach to reconstruction is to begin with a projective reconstruction
and then to reﬁne it progressively to an afﬁne and ﬁnally a metric reconstruction, if

268

10 3D Reconstruction of Cameras and Structure

possible. Of course, as has just been seen, afﬁne and metric reconstruction are not
possible without further information either about the scene, the motion or the camera
calibration.

10.4.1 The step to afﬁne reconstruction
The essence of afﬁne reconstruction is to locate the plane at inﬁnity by some means,
since this knowledge is equivalent to an afﬁne reconstruction. This equivalence is
explained in the 2D case in section 2.7(p47). To see this equivalence for reconstruction,
suppose we have determined a projective reconstruction of a scene, consisting of a
,{Xi}). Suppose further that by some means a certain plane π has been
triple (P, P(cid:2)
identiﬁed as the true plane at inﬁnity. The plane π is expressed as a 4-vector in the
coordinate frame of the projective reconstruction. In the true reconstruction, π has
coordinates (0, 0, 0, 1)T, and we may ﬁnd a projective transformation that maps π to
(0, 0, 0, 1)T. Considering the way a projective transformation acts on planes, we want
to ﬁnd H such that H−Tπ = (0, 0, 0, 1)T. Such a transformation is given by

(cid:17)

(cid:18)

I | 0
πT

H =

.

(10.2)

it is immediately veriﬁed that HT(0, 0, 0, 1)T = π, and thus H−Tπ =
Indeed,
(0, 0, 0, 1)T, as desired. The transformation H is now applied to all points and the two
cameras. Notice, however that this formula will not work if the ﬁnal coordinate of πT
is zero. In this case, one may compute a suitable H by computing H−T as a Householder
matrix (A4.2–p580) such that H−Tπ = (0, 0, 0, 1)T.

At this point, the reconstruction that one has is not necessarily the true reconstruction
– all one knows is that the plane at inﬁnity is correctly placed. The present reconstruc-
tion differs from the true reconstruction by a projective transformation that ﬁxes the
plane at inﬁnity. However, according to result 3.7(p80), a projective transformation
ﬁxing the plane at inﬁnity is an afﬁne transformation. Hence the reconstruction differs
by an afﬁne transformation from the true reconstruction – it is an afﬁne reconstruction.
An afﬁne reconstruction may well be sufﬁcient for some applications. For example,
the mid-point of two points and the centroid of a set of points may now be computed,
and lines constructed parallel to other lines and to planes. Such computations are not
possible from a projective reconstruction.

As has been stated, the plane at inﬁnity cannot be identiﬁed unless some extra in-
formation is given. We will now give several examples of the type of information that
sufﬁces for this identiﬁcation.

Translational motion
Consider the case where the camera is known to undergo a purely translational motion.
In this case, it is possible to carry out afﬁne reconstruction from two views. A simple
way of seeing this is to observe that a point X on the plane at inﬁnity will map to the
same point in two images related by a translation. This is easily veriﬁed formally. It is
also part of our common experience that as one moves in a straight line (for instance in

10.4 Stratiﬁed reconstruction

269

a car on a straight road), objects at a great distance (such as the moon) do not appear
to move – only the nearby objects move past the ﬁeld of view. This being so, one may
invent any number of matched points xi ↔ xi where a point in one image corresponds
with the same point in the other image. Note that one does not actually have to observe
such a correspondence in the two images – any point and the same point in the other
image will do. Given a projective reconstruction, one may then reconstruct the point
Xi corresponding to the match xi ↔ xi. Point Xi will lie on the plane at inﬁnity.
From three such points one can get three points on the plane at inﬁnity – sufﬁcient to
determine it uniquely.

Although this argument gives a constructive proof that afﬁne reconstruction is pos-
sible from a translating camera, this does not mean that this is the best way to pro-
ceed numerically. In fact in this case, the assumption of translational motion implies
a very restricted form for the fundamental matrix – it is skew-symmetric as shown in
section 9.3.1. This special form should be taken into account when solving for the
fundamental matrix.

Result 10.2. Suppose the motion of the cameras is a pure translation with no rotation
and no change in the internal parameters. As shown in example 9.6(p249) F = [e]× =
]×, and for an afﬁne reconstruction one may choose the two cameras as P = [I | 0]
[e(cid:2)
and P(cid:2)

= [I | e(cid:2)

].

Scene constraints
Scene constraints or conditions may also be used to obtain an afﬁne reconstruction.
As long as three points can be identiﬁed that are known to lie on the plane at inﬁn-
ity, then that plane may be identiﬁed, and the reconstruction transformed to an afﬁne
reconstruction.

Parallel lines. The most obvious such condition is the knowledge that 3D lines are in
reality parallel. The intersection of the two parallel lines in space gives a point on the
plane at inﬁnity. The image of this point is the vanishing point of the line, and is the
point of intersection of the two imaged lines. Suppose that three sets of parallel lines
can be identiﬁed in the scene. Each set intersects in a point on the plane at inﬁnity.
Provided each set has a different direction, the three points will be distinct. Since three
points determine a plane, this information is sufﬁcient to identify the plane π.

The best way of actually computing the intersection of lines in space is a somewhat
delicate problem, since in the presence of noise, lines that are intended to intersect
rarely do. It is discussed in some detail in chapter 12. Correct numerical procedures
for computing the plane are given in chapter 13. An example of an afﬁne reconstruction
computed from three sets of parallel scene lines is given in ﬁgure 10.4.

Note that it is not necessary to ﬁnd the vanishing point in both images. Suppose
the vanishing point v is computed from imaged parallel lines in the ﬁrst image, and
l(cid:2)
is a corresponding line in the second image. Vanishing points satisfy the epipolar
constraint, so the corresponding vanishing point v(cid:2)
in the second image may be com-
puted as the intersection of l(cid:2)
and the epipolar line Fv of v. The construction of the

270

10 3D Reconstruction of Cameras and Structure

a

b

Fig. 10.4. Afﬁne reconstruction. The projective reconstruction of ﬁgure 10.3 may be upgraded to afﬁne
using parallel scene lines. (a) There are 3 sets of parallel lines in the scene, each set with a different
direction. These 3 sets enable the position of the plane at inﬁnity, π∞, to be computed in the projective
reconstruction. The wireframe projective reconstruction of ﬁgure 10.3 is then afﬁnely rectiﬁed using the
homography (10.2). (b) Shows two orthographic views of the wireframe afﬁne reconstruction. Note that
parallel scene lines are parallel in the reconstruction, but lines that are perpendicular in the scene are
not perpendicular in the reconstruction.

3-space point X can be neatly expressed algebraically as the solution of the equations
([v]×P)X = 0 and (l(cid:2)TP(cid:2)
)X = 0. These equations expresses the fact that X maps to v
in the ﬁrst image, and to a point on l(cid:2)

in the second image.

Distance ratios on a line. An alternative to computing vanishing points as the in-
tersection of imaged parallel scene lines is to use knowledge of afﬁne length ratios in
the scene. For example, given two intervals on a line with a known length ratio, the
point at inﬁnity on the line may be determined. This means that from an image of a
line on which a world distance ratio is known, for example that three points are equally
spaced, the vanishing point may be determined. This computation, and other means of
computing vanishing points and vanishing lines, are described in section 2.7(p47).

The inﬁnite homography
Once the plane at inﬁnity has been located, so that we have an afﬁne reconstruction,
then we also have an image-to-image map called the “inﬁnite homography”. This map,
which is a 2D homography , is described in greater detail in chapter 13. Brieﬂy, it is
the map that transfers points from the P image to the P(cid:2)
image via the plane at inﬁnity
as follows: the ray corresponding to a point x is extended to meet the plane at inﬁnity
in a point X; this point is projected to a point x(cid:2)
in the other image. The homography
from x to x(cid:2)
Having an afﬁne reconstruction is equivalent to knowing the inﬁnite homography as
will now be shown. Given two cameras P = [M | m] and P(cid:2)
] of an afﬁne
reconstruction, the inﬁnite homography is given by H∞ = M(cid:2)M−1. This is because a
= M(cid:2) ˜X
point X = ( ˜XT
= M(cid:2)M−1x for points on π∞. Furthermore, it may be veriﬁed that
in the other, so x(cid:2)

, 0)T on the plane at inﬁnity maps to x = M ˜X in one image and x(cid:2)

= [M(cid:2) | m(cid:2)

is written as x(cid:2)

= H∞x.

10.4 Stratiﬁed reconstruction

271

this is unchanged by a 3-space afﬁne transformation of the cameras. Hence, the inﬁnite
homography may be computed explicitly from an afﬁne reconstruction, and vice versa:

= [M(cid:2) | e(cid:2)

].

= [H∞ | e(cid:2)

Result 10.3. If an afﬁne reconstruction has been obtained in which the camera matrices
are P = [I | 0] and P(cid:2)
], then the inﬁnite homography is given by H∞ = M(cid:2)
.
Conversely, if the inﬁnite homography H∞ has been obtained, then the cameras of an
afﬁne reconstruction may be chosen as P = [I | 0] and P(cid:2)
The inﬁnite homography may be computed directly from corresponding image en-
tities, rather than indirectly from an afﬁne reconstruction. For example, H∞ can be
computed from the correspondence of three vanishing points together with F, or the
correspondence of a vanishing line and vanishing point, together with F. The correct
numerical procedure for these computations is given in chapter 13. However, such
direct computations are completely equivalent to determining π∞ in a projective re-
construction.
One of the cameras is afﬁne
Another important case in which afﬁne reconstruction is possible is when one of the
cameras is known to be an afﬁne camera as deﬁned in section 6.3.1(p166). To see that
this implies that afﬁne reconstruction is possible, refer to section 6.3.5(p172) where it
was shown that the principal plane of an afﬁne camera is the plane at inﬁnity. Hence to
convert a projective reconstruction to an afﬁne reconstruction, it is sufﬁcient to ﬁnd the
principal plane of the camera supposed to be afﬁne and map it to the plane (0, 0, 0, 1)T.
Recall (section 6.2(p158)) that the principal plane of a camera is simply the third row
of the camera matrix. For example, consider a projective reconstruction with camera
matrices P = [I | 0] and P(cid:2)
, for which the ﬁrst camera is supposed to be afﬁne. To map
the third row of P to (0, 0, 0, 1) it is sufﬁcient to swap the last two columns of the two
camera matrices, while at the same time swapping the 3rd and 4th coordinates of each
Xi. This is a projective transformation corresponding to a permutation matrix H. This
shows:
,{Xi}) be a projective reconstruction from a set of point corre-
Result 10.4. Let (P, P(cid:2)
spondences for which P = [I | 0]. Suppose in truth, P is known to be an afﬁne camera,
then an afﬁne reconstruction is obtained by swapping the last two columns of P and P(cid:2)
and the last two coordinates of each Xi.
Note that the condition that one of the cameras is afﬁne places no restriction on
the fundamental matrix, since any canonical camera pair P = [I | 0] and P(cid:2)
can be
transformed to a pair in which P is afﬁne. If both the cameras are known to be afﬁne,
then it will be seen that the fundamental matrix has the restricted form given in (14.1–
p345). In this case, for numerical stability, one must solve for the fundamental matrix
enforcing this special form of the fundamental matrix.

Of course there is no such thing as a real afﬁne camera – the afﬁne camera model
is an approximation which is only valid when the set of points seen in the image has
small depth variation compared with the distance from the camera. Nevertheless, an
assumption of an afﬁne camera may be useful to effect the signiﬁcant restriction from
projective to afﬁne reconstruction.

272

10 3D Reconstruction of Cameras and Structure

10.4.2 The step to metric reconstruction
Just as the key to afﬁne reconstruction is the identiﬁcation of the plane at inﬁnity, the
key to metric reconstruction is the identiﬁcation of the absolute conic (section 3.6-
(p81)). Since the absolute conic, Ω∞, is a planar conic, lying in the plane at inﬁnity,
identifying the absolute conic implies identifying the plane at inﬁnity.

In a stratiﬁed approach, one proceeds from projective to afﬁne to metric reconstruc-
tion, so one knows the plane at inﬁnity before ﬁnding the absolute conic. Suppose one
has identiﬁed the absolute conic on the plane at inﬁnity. In principle the next step is
to apply an afﬁne transformation to the afﬁne reconstruction such that the identiﬁed
absolute conic is mapped to the absolute conic in the standard Euclidean frame (it will
3 = 0, on π∞). The resulting reconstruction is
then have the equation X2
then related to the true reconstruction by a projective transformation which ﬁxes the
absolute conic. It follows from result 3.9(p82) that the projective transformation is a
similarity transformation, so we have achieved a metric reconstruction.

1 + X2

2 + X2

In practice the easiest way to accomplish this is to consider the image of the absolute
conic in one of the images. The image of the absolute conic (as any conic) is a conic
in the image. The back-projection of this conic is a cone, which will meet the plane at
inﬁnity in a single conic, which therefore deﬁnes the absolute conic. Remember that
the image of the absolute conic is a property of the image itself, and like any image
point, line or other feature, is not dependent on any particular reconstruction, hence it
is unchanged by 3D transformations of the reconstruction.
Suppose that in the afﬁne reconstruction the image of the absolute conic as seen
by the camera with matrix P = [M | m] is a conic ω. We will show how ω may be
used to deﬁne the homography H which transforms the afﬁne reconstruction to a metric
reconstruction:

Result 10.5. Suppose that the image of the absolute conic is known in some image to
be ω, and one has an afﬁne reconstruction in which the corresponding camera matrix
is given by P = [M | m]. Then, the afﬁne reconstruction may be transformed to a metric
reconstruction by applying a 3D transformation of the form

(cid:17)

(cid:18)

H =

A−1

1

where A is obtained by Cholesky factorization from the equation AAT = (MTωM)

−1.

Proof. Under the transformation H, the camera matrix P is transformed to a matrix
(cid:17)
PM = PH−1 = [MM | mM]. If H−1 is of the form

(cid:18)

H−1 =

A 0
0T 1

then MM = MA. However, the image of the absolute conic is related to the camera matrix
PM of a Euclidean frame by the relationship

ω∗

= MMMT

M .

10.4 Stratiﬁed reconstruction

273

This is because the camera matrix may be decomposed as MM = KR, and from (8.11–
p210) ω∗
= ω−1 = KKT. Combining this with MM = MA gives ω−1 = MAATMT, which
−1. A particular value of A that satisﬁes this
may be rearranged as AAT = (MTωM)
−1. This latter
relationship is found by taking the Cholesky factorization of (MTωM)
matrix is guaranteed to be positive-deﬁnite (see result A4.5(p582)), otherwise no such
matrix A will exist, and metric reconstruction will not be possible.

This approach to metric reconstruction relies on identifying the image of the absolute
conic. There are various ways of doing this and these are discussed next. Three sources
of constraint on the image of the absolute conic are given, and in practice a combination
of these constraints is used.

1. Constraints arising from scene orthogonality. Pairs of vanishing points, v1 and
v2, arising from orthogonal scene lines place a single linear constraint on ω:

vT
1

ωv2 = 0.

Similarly, a vanishing point v and a vanishing line l arising from a direction and plane
which are orthogonal place two constraints on ω:

l = ωv.

A common example is the vanishing point for the vertical direction and a vanishing line
from the horizontal ground plane. Finally an imaged scene plane containing metric
information, such as a square grid, places two constraints on ω.

2. Constraints arising from known internal parameters.
If the calibration matrix
of a camera is equal to K, then the image of the absolute conic is ω = K−TK−1. Thus,
knowledge of the internal parameters (6.10–p157) contained in K may be used to con-
strain or determine the elements of ω. In the case where K is known to have zero skew
(s = 0),

and if the pixels are square (zero skew and αx = αy) then

ω12 = ω21 = 0

ω11 = ω22.

These ﬁrst two sources of constraint are discussed in detail in section 8.8(p223) on
single view calibration, where examples are given of calibrating a camera solely from
such information. Here there is an additional source of constraints available arising
from the multiple views.

3. Constraints arising from the same cameras in all images. One of the properties
of the absolute conic is that its projection into an image depends only on the calibration
matrix of the camera, and not on the position or orientation of the camera. In the case
where both cameras P and P(cid:2)
have the same calibration matrix (usually meaning that
both the images were taken with the same camera with different pose) one has that
ω = ω(cid:2)
, that is the image of the absolute conic is the same in both images. Given

274

10 3D Reconstruction of Cameras and Structure

a

b

Fig. 10.5. Metric reconstruction. The afﬁne reconstruction of ﬁgure 10.4 is upgraded to metric by
computing the image of the absolute conic. The information used is the orthogonality of the directions
of the parallel line sets shown in ﬁgure 10.4, together with the constraint that both images have square
pixels. The square pixel constraint is transferred from one image to the other using H∞.
(a) Two
views of the metric reconstruction. Lines which are perpendicular in the scene are perpendicular in the
reconstruction and also the aspect ratio of the sides of the house is veridical. (b) Two views of a texture
mapped piecewise planar model built from the wireframes.

sufﬁciently many images, one may use this property to obtain a metric reconstruction
from an afﬁne reconstruction. This method of metric reconstruction, and its use for
self-calibration of a camera, will be treated in greater detail in chapter 19. For now, we
give just the general principle.

Since the absolute conic lies on the plane at inﬁnity, its image may be transferred
from one view to the other via the inﬁnite homography. This implies an equation (see
result 2.13(p37))

ω(cid:2)

= H−T∞ ωH−1∞

(10.3)

where ω and ω(cid:2)
are images of Ω∞ in the two views. In forming these equations it is
necessary to have an afﬁne reconstruction already, since the inﬁnite homography must
be known. If ω = ω(cid:2)
, then (10.3) gives a set of linear equations in the entries of ω. In
general this set of linear equations places four constraints on ω, and since ω has 5 de-
grees of freedom it is not completely determined. However, by combining these linear
equations with those above provided by scene orthogonality or known internal param-
eters, ω may be determined uniquely. Indeed (10.3) may be used to transfer constraints
on ω to constraints on ω(cid:2)
. Figure 10.5 shows an example of a metric reconstruction
computed by combining constraints in this manner.

10.5 Direct reconstruction – using ground truth

275

10.4.3 Direct metric reconstruction using ω
The previous discussion showed how knowledge of the image of the absolute conic
(IAC) may be used to transform an afﬁne to a metric reconstruction. However, knowing
ω it is possible to proceed directly to metric reconstruction, given at least two views.
This can be accomplished in at least two different ways. The most evident approach
is to use the IAC to compute calibration of each of the cameras, and then carry out a
calibrated reconstruction.

This method relies on the connection of ω to the calibration matrix K, namely
−1. Thus one can compute K from ω by inverting it and then applying
ω = (KKT)
Cholesky factorization to obtain K.
If the IAC is known in each image, then both
cameras may be calibrated in this way. Next with calibrated cameras, a metric recon-
struction of the scene may be computed using the essential matrix, as in section 9.6.
Note that four possible solutions may result. Two of these are just mirror images, but
the other two are different, forming a twisted pair. (Though all solutions but one may
be ruled out by consideration of points lying in front of the cameras.)

A more conceptual approach to metric reconstruction is to use knowledge of the
IAC to directly determine the plane at inﬁnity and the absolute conic. Knowing the
camera matrices P and P(cid:2)
in a projective frame, and a conic (speciﬁcally the image of
the absolute conic) in each image, then Ω∞ may be explicitly computed in 3-space.
This is achieved by back-projecting the conics to cones, which must intersect in the
absolute conic. Thus, Ω∞ and its support plane π∞ are determined (see exercise (x)
on page 342 for an algebraic solution). However, two cones will in general intersect in
two different plane conics, each lying in a different support plane. Thus there are two
possible solutions for the absolute conic, which one can identify as belonging to the
two different reconstructions constituting the twisted pair ambiguity.

10.5 Direct reconstruction – using ground truth

It is possible to jump directly from a projective reconstruction to a metric reconstruc-
tion if “ground control points” (that is points with known 3D locations in a Euclidean
world frame) are given. Suppose we have a set of n such ground control points {XEi}
which are imaged at xi ↔ x(cid:2)
i. We wish to use these points to transform the projective
reconstruction to metric.
The 3D location {Xi} of the control points in the projective reconstruction may be
computed from their image correspondences xi ↔ x(cid:2)
i. Since the projective reconstruc-
tion is related by a homography to the true reconstruction we then have from (10.1) the
equations:

XEi = HXi,

i = 1, . . . , n.

Each point correspondence provides 3 linearly independent equations on the elements
of H, and since H has 15 degrees of freedom a linear solution is obtained provided
n ≥ 5 (and no four of the control points are coplanar). This computation, and the
proper numerical procedures, are described in chapter 4.

Alternatively, one may bypass the computation of the Xi and compute H by relating

276

10 3D Reconstruction of Cameras and Structure

a

b

c

Fig. 10.6. Direct reconstruction. The projective reconstruction of ﬁgure 10.3 may be upgraded to
metric by specifying the position of ﬁve (or more) world points: (a) the ﬁve points used; (b) the cor-
responding points on the projective reconstruction of ﬁgure 10.3; (c) the reconstruction after the ﬁve
points are mapped to their world positions.

the known ground control points directly to image measurements. Thus as in the DLT
algorithm for camera resection (section 7.1(p178)) the equation

xi = PH−1XEi

provides two linearly independent equations in the entries of the unknown H−1, all other
quantities being known. Similarly equations may be derived from the other image if
x(cid:2)
i is known. It is not necessary for the ground control points to be visible in both
images. Note however that if both xi and x(cid:2)
i are visible for a given control point XEi
then because of the coplanarity constraint on x and x(cid:2)
, the four equations generated in
this way contain only three independent ones.

Once H has been computed it may be used to transform the cameras P, P(cid:2)

of the
projective reconstruction to their true Euclidean counterparts. An example of metric
structure computed by this direct method is shown in ﬁgure 10.6.

10.6 Closure

In this chapter we have overviewed the steps necessary to produce a metric reconstruc-
tion from a pair of images. This overview is summarized in algorithm 10.1, and the
computational procedures for these steps are described in the following chapters. As
usual the general discussion has been restricted mainly to points, but the ideas (trian-
gulation, ambiguity, stratiﬁcation) apply equally to other image features such as lines,
conics etc.

It has been seen that for a metric reconstruction it is necessary to identify two enti-
ties in the projective frame; these are the plane at inﬁnity π∞ (for afﬁne), together with
the absolute conic Ω∞ (for metric). Conversely, given F and a pair of calibrated cam-
eras then π∞ and Ω∞ may be explicitly computed in 3-space. These entities each have
an image-based counterpart: speciﬁcation of the inﬁnite homography, H∞, is equiva-
lent to specifying π∞ in 3-space; and specifying the image of the absolute conic, ω,
in each view is equivalent to specifying π∞ and Ω∞ in 3-space. This equivalence is
summarized in table 10.1.

Finally, it is worth noting that if metric precision is not the goal then an accept-
able metric reconstruction is generally obtained directly from the projective if approx-
imately correct internal parameters are guessed. Such a “quasi-Euclidean reconstruc-
tion” is often suitable for visualization purposes.

10.6 Closure

277

Objective
M,{XMi}) of the cam-
Given two uncalibrated images compute a metric reconstruction (PM, P(cid:1)
eras and scene structure, i.e. a reconstruction that is within a similarity transformation of the
true cameras and scene structure.

Algorithm

(i) Compute a projective reconstruction (P, P(cid:1)

,{Xi}):

tween the images.

(a) Compute the fundamental matrix from point correspondences xi ↔ x(cid:1)
(b) Camera retrieval: compute the camera matrices P, P(cid:1)
(c) Triangulation: for each point correspondence xi ↔ x(cid:1)

matrix.

i, compute the point Xi

from the fundamental

i be-

in space that projects to these two image points.

(ii) Rectify the projective reconstruction to metric:

• either Direct method: Compute the homography H such that XEi = HXi from ﬁve
or more ground control points XEi with known Euclidean positions. Then the metric
reconstruction is

PM = PH−1, P(cid:1)

M = P(cid:1)H−1, XMi = HXi.

• or Stratiﬁed method:

(a) Afﬁne reconstruction: Compute the plane at inﬁnity, π∞, as described
in section 10.4.1, and then upgrade the projective reconstruction to an afﬁne
reconstruction with the homography

H =

I | 0
πT∞

(cid:24)

(cid:24)

(cid:25)

.

(cid:25)

H =

A−1

1

(b) Metric reconstruction: Compute the image of the absolute conic, ω, as
described in section 10.4.2, and then upgrade the afﬁne reconstruction to a
metric reconstruction with the homography

where A is obtained by Cholesky factorization from the equation AAT =
(MTωM)−1, and M is the ﬁrst 3 × 3 submatrix of the camera in the afﬁne
reconstruction for which ω is computed.

Algorithm 10.1. Computation of a metric reconstruction from two uncalibrated images.

10.6.1 The literature

Koenderink and van Doorn [Koenderink-91] give a very elegant discussion of stratiﬁca-
tion for afﬁne cameras. This was extended to perspective in [Faugeras-95b], with devel-
opments given by Luong and Vi´eville [Luong-94, Luong-96]. The possibility of projec-
tive reconstruction given F appeared in [Faugeras-92b] and Hartley et al. [Hartley-92c].
The method of computing afﬁne reconstruction from pure translation ﬁrst appeared
in Moons et al. [Moons-94]. Combining scene and internal parameter constraints over
multiple views is described in [Faugeras-95c, Liebowitz-99b, Sturm-99c].

278

10 3D Reconstruction of Cameras and Structure

Image information
provided

View relations and
projective objects

3-space Reconstruction
objects

ambiguity

Point correspondences

Point correspondences
including vanishing points

Point correspondences and
internal camera calibration

F

F, H∞

F, H∞
ω, ω(cid:1)

Projective

Afﬁne

Metric

π∞

π∞
Ω∞

Table 10.1. The two-view relations, image entities, and their 3-space counterparts for various classes
of reconstruction ambiguity.

10.6.2 Notes and exercises

(i) Using only (implicit) image relations (i.e. without an explicit 3D reconstruc-
tion) and given the images of a line L and point X (not on L) in two views,
together with H∞ between the views, compute the image of the line in 3-space
parallel to L and through X. Other examples of this implicit approach to com-
putation are given in [Zeller-96].

11

Computation of the Fundamental Matrix F

This chapter describes numerical methods for estimating the fundamental matrix given
a set of point correspondences between two images. We begin by describing the equa-
tions on F generated by point correspondences in two images, and their minimal solu-
tion. The following sections then give linear methods for estimating F using algebraic
distance, and then various geometric cost functions and solution methods including the
MLE (“Gold Standard”) algorithm, and Sampson distance.

An algorithm is then described for automatically obtaining point correspondences,
so that F may be estimated directly from an image pair. We discuss the estimation of F
for special camera motions.

The chapter also covers a method of image rectiﬁcation based on the computed F.

11.1 Basic equations

The fundamental matrix is deﬁned by the equation
x(cid:2)TFx = 0

(11.1)

for any pair of matching points x ↔ x(cid:2)
in two images. Given sufﬁciently many point
matches xi ↔ x(cid:2)
i (at least 7), equation (11.1) can be used to compute the unknown
matrix F. In particular, writing x = (x, y, 1)T and x(cid:2)
, 1)T each point match
gives rise to one linear equation in the unknown entries of F. The coefﬁcients of this
equation are easily written in terms of the known coordinates x and x(cid:2)
. Speciﬁcally,
(cid:2)
the equation corresponding to a pair of points (x, y, 1) and (x
, 1) is

= (x

, y

, y

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)
(11.2)
x
xf11 + x
Denote by f the 9-vector made up of the entries of F in row-major order. Then (11.2)

f23 + xf31 + yf32 + f33 = 0.

yf12 + x

xf21 + y

yf22 + y

f13 + y

(cid:2)

(cid:2)

(cid:2)

(cid:2)

can be expressed as a vector inner product

(cid:2)

(x

(cid:2)

(cid:2)

(cid:2)

, y

y, x

x, x

x, y

(cid:2)

y, y

(cid:2)

, x, y, 1) f = 0.

From a set of n point matches, we obtain a set of linear equations of the form

 x

x

Af =

(cid:2)
1x1 x
...
(cid:2)
nxn x

(cid:2)
(cid:2)
1y1 x
1
...
...
(cid:2)
(cid:2)
nyn x
n y

(cid:2)
1x1
y
...
(cid:2)
nxn y

(cid:2)
(cid:2)
1y1
y
y
1 x1 y1 1
...
...
(cid:2)
(cid:2)
nyn y
n xn yn 1

...

...

279

 f = 0.

(11.3)

280

11 Computation of the Fundamental Matrix F

a

b

Fig. 11.1. Epipolar lines. (a) the effect of a non-singular fundamental matrix. Epipolar lines computed
as l(cid:1) = Fx for varying x do not meet in a common epipole. (b) the effect of enforcing singularity using
the SVD method described here.

This is a homogeneous set of equations, and f can only be determined up to scale. For
a solution to exist, matrix A must have rank at most 8, and if the rank is exactly 8, then
the solution is unique (up to scale), and can be found by linear methods – the solution
is the generator of the right null-space of A.

If the data is not exact, because of noise in the point coordinates, then the rank of
A may be greater than 8 (in fact equal to 9, since A has 9 columns). In this case, one
ﬁnds a least-squares solution. Apart from the speciﬁc form of the equations (compare
(11.3) with (4.3–p89)) the problem is essentially the same as the estimation problem
considered in section 4.1.1(p90). Refer to the algorithm 4.1(p91). The least-squares
solution for f is the singular vector corresponding to the smallest singular value of
A, that is, the last column of V in the SVD A = UDVT. The solution vector f found
in this way minimizes (cid:10)Af(cid:10) subject to the condition (cid:10)f(cid:10) = 1. The algorithm just
described is the essence of a method called the 8-point algorithm for computation of
the fundamental matrix.

11.1.1 The singularity constraint
An important property of the fundamental matrix is that it is singular, in fact of rank
2. Furthermore, the left and right null-spaces of F are generated by the vectors rep-
resenting (in homogeneous coordinates) the two epipoles in the two images. Most
applications of the fundamental matrix rely on the fact that it has rank 2. For instance,
if the fundamental matrix is not singular then computed epipolar lines are not coinci-
dent, as is demonstrated by ﬁgure 11.1. The matrix F found by solving the set of linear
equations (11.3) will not in general have rank 2, and we should take steps to enforce
this constraint. The most convenient way to do this is to correct the matrix F found
by the SVD solution from A. Matrix F is replaced by the matrix F(cid:2)
that minimizes the
Frobenius norm (cid:10)F − F(cid:2)(cid:10) subject to the condition det F(cid:2)
= 0. A convenient method of

11.2 The normalized 8-point algorithm

281
doing this is to again use the SVD. In particular, let F = UDVT be the SVD of F, where D
is a diagonal matrix D = diag(r, s, t) satisfying r ≥ s ≥ t. Then F(cid:2)
= Udiag(r, s, 0)VT
minimizes the Frobenius norm of F − F(cid:2)
Thus, the 8-point algorithm for computation of the fundamental matrix may be for-

.

mulated as consisting of two steps, as follows.

(i) Linear solution. A solution F is obtained from the vector f corresponding to
(ii) Constraint enforcement. Replace F by F(cid:2)

the smallest singular value of A, where A is deﬁned in (11.3).

, the closest singular matrix to F

under a Frobenius norm. This correction is done using the SVD.

The algorithm thus stated is extremely simple, and readily implemented, assuming
that appropriate linear algebra routines are available. As usual normalization is re-
quired, and we return to this in section 11.2.

i

11.1.2 The minimum case – seven point correspondences
The equation x(cid:2)
TFxi = 0 gives rise to a set of equations of the form Af = 0. If A has
rank 8, then it is possible to solve for f up to scale. In the case where the matrix A has
rank seven, it is still possible to solve for the fundamental matrix by making use of the
singularity constraint. The most important case is when only 7 point correspondences
are known (other cases are discussed in section 11.9). This leads to a 7 × 9 matrix A,
which generally will have rank 7.
The solution to the equations Af = 0 in this case is a 2-dimensional space of the form
αF1 +(1− α)F2, where α is a scalar variable. The matrices F1 and F2 are obtained as the
matrices corresponding to the generators f 1 and f 2 of the right null-space of A. Now, we
use the constraint that det F = 0. This may be written as det(αF1 + (1− α)F2) = 0.
Since F1 and F2 are known, this leads to a cubic polynomial equation in α. This poly-
nomial equation may be solved to ﬁnd the value of α. There will be either one or three
real solutions (the complex solutions are discarded [Hartley-94c]). Substituting back
in the equation F = αF1 + (1 − α)F2 gives one or three possible solutions for the
fundamental matrix.

This method of computing one or three fundamental matrices for the minimum num-
ber of points (seven) is used in the robust algorithm of section 11.6. We return to the
issue of the number of solutions in section 11.9.

11.2 The normalized 8-point algorithm

The 8-point algorithm is the simplest method of computing the fundamental matrix,
involving no more than the construction and (least-squares) solution of a set of linear
equations.
If care is taken, then it can perform extremely well. The original algo-
rithm is due to Longuet-Higgins [LonguetHiggins-81]. The key to success with the
8-point algorithm is proper careful normalization of the input data before constructing
the equations to solve. The subject of normalization of input data has applications to
many of the algorithms of this book, and is treated in general terms in section 4.4-
(p104). In the case of the 8-point algorithm, a simple transformation (translation and

282

11 Computation of the Fundamental Matrix F

Objective
Given n ≥ 8 image point correspondences {xi ↔ x(cid:1)
such that x(cid:1)
Algorithm

TFxi = 0.

i

i}, determine the fundamental matrix F

(i) Normalization: Transform the image coordinates according to ˆxi = Txi and ˆx(cid:1)

i =
are normalizing transformations consisting of a translation and

i, where T and T(cid:1)

(cid:1)
(ii) Find the fundamental matrix ˆF

corresponding to the matches ˆxi ↔ ˆx(cid:1)

i by

T(cid:1)x(cid:1)
scaling.

(a) Linear solution: Determine ˆF from the singular vector corresponding to the
smallest singular value of ˆA, where ˆA is composed from the matches ˆxi ↔ ˆx(cid:1)
as deﬁned in (11.3).

i

(cid:1)
(b) Constraint enforcement: Replace ˆF by ˆF

such that det ˆF

= 0 using the SVD

(cid:1)

(cid:1)
(iii) Denormalization: Set F = T(cid:1)TˆF

(see section 11.1.1).
to the original data xi ↔ x(cid:1)
i.

T. Matrix F is the fundamental matrix corresponding

Algorithm 11.1. The normalized 8-point algorithm for F.

scaling) of the points in the image before formulating the linear equations leads to an
enormous improvement in the conditioning of the problem and hence in the stability of
the result. The added complexity of the algorithm necessary to do this transformation
is insigniﬁcant.

The suggested normalization is a translation and scaling of each image so that the
√
centroid of the reference points is at the origin of the coordinates and the RMS distance
of the points from the origin is equal to
2. This is carried out for essentially the same
reasons as in chapter 4. The basic method is analogous to algorithm 4.2(p109) and is
summarized in algorithm 11.1.

Note that it is recommended that the singularity condition should be enforced before

denormalization. For a justiﬁcation of this, refer to [Hartley-97c].

11.3 The algebraic minimization algorithm

The normalized 8-point algorithm includes a method for enforcing the singularity con-
straint on the fundamental matrix. The initial estimate F is replaced by the singular
that minimizes the difference (cid:10)F(cid:2) − F(cid:10). This is done using the SVD, and has
matrix F(cid:2)
the advantage of being simple and rapid.

Numerically, however, this method is not optimal, since all the entries of F do not
have equal importance, and indeed some entries are more tightly constrained by the
point-correspondence data than others. A more correct procedure would be to compute
a covariance matrix from the entries of F in terms of the input data, and then to ﬁnd
the singular matrix F(cid:2)
closest to F in terms of Mahalanobis distance with respect to this
covariance. Unfortunately, minimization of the Mahalanobis distance (cid:10)F− F(cid:2)(cid:10)Σ cannot
be done linearly for a general covariance matrix Σ, so this approach is unattractive.
An alternative procedure is to ﬁnd the desired singular matrix F(cid:2)
directly. Thus, just
as F is computed by minimizing the norm (cid:10)Af(cid:10) subject to (cid:10)f(cid:10) = 1, so one should aim

11.3 The algebraic minimization algorithm

283

that minimizes (cid:10)Af(cid:2)(cid:10) subject to (cid:10)f(cid:2)(cid:10) = 1. It turns out
to ﬁnd the singular matrix F(cid:2)
not to be possible to do this by linear non-iterative means, chieﬂy because det F(cid:2)
= 0
is a cubic, rather than a linear constraint. Nevertheless, it will be seen that a simple
iterative method is effective.
An arbitrary singular 3× 3 matrix, such as the fundamental matrix F, may be written
as a product F = M[e]× where M is a non-singular matrix and [e]× is any skew-symmetric
matrix, with e corresponding to the epipole in the ﬁrst image.
Suppose we wish to compute the fundamental matrix F of the form F = M[e]× that
minimizes the algebraic error (cid:10)Af(cid:10) subject to the condition (cid:10)f(cid:10) = 1. Let us assume
for now that the epipole e is known. Later we will let e vary, but for now it is ﬁxed.
The equation F = M[e]× can be written in terms of the vectors f and m comprising the
entries of F and M as an equation f = Em where E is a 9 × 9 matrix. Supposing that
f and m contain the entries of the corresponding matrices in row-major order, then it
can be veriﬁed that E has the form

 [e]×

E =

 .

[e]×

[e]×

(11.4)

Since f = Em, the minimization problem becomes: 1

Minimize (cid:10)AEm(cid:10) subject to the condition (cid:10)Em(cid:10) = 1.

(11.5)

This minimization problem is solved using algorithm A5.6(p595). For the purposes of
this algorithm one observes that rank(E) = 6, since each of its diagonal blocks has rank
2.

11.3.1 Iterative estimation
The minimization (11.5) gives a way of computing an algebraic error vector Af given
a value for the epipole e. This mapping e (cid:6)→ Af is a map from IR3 to IR9. Note that the
value of Af is unaffected by scaling e. Starting from an estimated value of e derived as
the generator of the right null-space of an initial estimate of F, one may iterate to ﬁnd
the ﬁnal F that minimizes algebraic error. The initial estimate of F may be obtained
from the 8-point algorithm, or any other simple algorithm. The complete algorithm for
computation of F is given in algorithm 11.2.

Note the advantage of this method of computing F is that the iterative part of the
algorithm consists of a very small parameter minimization problem, involving the es-
timation of only three parameters (the homogeneous coordinates of e). Despite this,
the algorithm ﬁnds the fundamental matrix that minimizes the algebraic error for all
matched points. The matched points themselves do not come into the ﬁnal iterative
estimation.
1 It does not do to minimize (cid:5)AEm(cid:5) subject to the condition (cid:5)m(cid:5) = 1, since a solution to this occurs when m is a unit vector

in the right null-space of E. In this case, Em = 0, and hence (cid:5)AEm(cid:5) = 0.

284

11 Computation of the Fundamental Matrix F

Objective
Find the fundamental matrix F that minimizes the algebraic error (cid:10)Af(cid:10) subject to (cid:10)f(cid:10) = 1 and
det F = 0.

Algorithm

(i) Find a ﬁrst approximation F0 for the fundamental matrix using the normalized 8-point

algorithm 11.1. Then ﬁnd the right null-vector e0 of F0.
(ii) Starting with the estimate ei = e0 for the epipole, compute the matrix Ei according to
(11.4), then ﬁnd the vector f i = Eimi that minimizes (cid:10)Af i(cid:10) subject to (cid:10)f i(cid:10) = 1. This
is done using algorithm A5.6(p595).
(iii) Compute the algebraic error i = Af i. Since f i and hence i is deﬁned only up to sign,
i ei−1 > 0 for
(iv) The previous two steps deﬁne a mapping IR3 → IR9 mapping ei (cid:6)→ i. Now use
the Levenberg–Marquardt algorithm (section A6.2(p600)) to vary ei iteratively so as to
minimize (cid:10)i(cid:10).

correct the sign of i (multiplying by minus 1 if necessary) so that eT
i > 0. This is done to ensure that i varies smoothly as a function of ei.

(v) Upon convergence, f i represents the desired fundamental matrix.

Algorithm 11.2. Computation of F with det F = 0 by iteratively minimizing algebraic error.

11.4 Geometric distance

This section describes three algorithms which minimize a geometric image distance.
The one we recommend, which is the Gold Standard method, unfortunately requires the
most effort in implementation. The other algorithms produce extremely good results
and are easier to implement, but are not optimal under the assumption that the image
errors are Gaussian. Two important issues for each of the algorithms are the intitial-
ization for the non-linear minimization, and the parametrization of the cost function.
The algorithms are generally initialized by one of the linear algorithms of the previous
section. An alternative, which is used in the automatic algorithm, is to select 7 corre-
spondences and thus generate one or three solutions for F. Various parametrizations
are discussed in section 11.4.2. In all cases we recommend that the image points be
normalized by a translation and scaling. This normalization does not skew the noise
characteristics, so does not interfere with the optimality of the Gold Standard algo-
rithm, which is described next.

11.4.1 The Gold Standard method
The Maximum Likelihood estimate of the fundamental matrix depends on the assump-
tion of an error model. We make the assumption that noise in image point measure-
ments obeys a Gaussian distribution.
In that case the ML estimate is the one that
minimizes the geometric distance (which is reprojection error)

d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)
i)2

(11.6)

(cid:7)

i

where xi ↔ x(cid:2)
correspondences that satisfy ˆx(cid:2)
mated fundamental matrix.

i

i are the measured correspondences, and ˆxi and ˆx(cid:2)

i are estimated “true”
TFˆxi = 0 exactly for some rank-2 matrix F, the esti-

11.4 Geometric distance

285

Objective
Given n ≥ 8 image point correspondences {xi ↔ x(cid:1)
estimate ˆF of the fundamental matrix.
The MLE involves also solving for a set of subsidiary point correspondences {ˆxi ↔ ˆx(cid:1)
that ˆx(cid:1)

TˆFˆxi = 0, and which minimizes(cid:7)

i}, determine the Maximum Likelihood

i}, such

i

d(xi, ˆxi)2 + d(x(cid:1)

i, ˆx(cid:1)

i)2.

Algorithm

i

from ˆF.

(i) Compute an initial rank 2 estimate of ˆF using a linear algorithm such as algorithm 11.1.
(ii) Compute an initial estimate of the subsidiary variables {ˆxi, ˆx(cid:1)

i} as follows:
(a) Choose camera matrices P = [I | 0] and P(cid:1) = [[e(cid:1)]×ˆF | e(cid:1)], where e(cid:1)
(b) From the correspondence xi ↔ x(cid:1)
triangulation method of chapter 12.

i and ˆF determine an estimate of (cid:23)Xi using the
(c) The correspondence consistent with ˆF is obtained as ˆxi = P(cid:23)Xi, ˆx(cid:1)
i = P(cid:1)(cid:23)Xi.
over ˆF and (cid:23)Xi, i = 1, . . . , n. The cost is minimized using the Levenberg–Marquardt
algorithm over 3n + 12 variables: 3n for the n 3D points (cid:23)Xi, and 12 for the camera
matrix P(cid:1) = [M | t], with ˆF = [t]×M, and ˆxi = P(cid:23)Xi, ˆx(cid:1)

i = P(cid:1)(cid:23)Xi.

d(xi, ˆxi)2 + d(x(cid:1)

(iii) Minimize the cost

(cid:7)

is obtained

i, ˆx(cid:1)
i)2

i

Algorithm 11.3. The Gold Standard algorithm for estimating F from image correspondences.

i will satisfy ˆx(cid:2)

i = P(cid:2)Xi, one varies P(cid:2)

This error function may be minimized in the following manner. A pair of camera
matrices P = [I | 0] and P(cid:2)
= [M | t] are deﬁned. In addition one deﬁnes 3D points
Xi. Now letting ˆxi = PXi and ˆx(cid:2)
and the points Xi so as to
minimize the error expression. Subsequently F is computed as F = [t]×M. The vectors
ˆxi and ˆx(cid:2)
TFˆxi = 0. Minimization of the error is carried out using the
Levenberg–Marquardt algorithm described in section A6.2(p600). An initial estimate
of the parameters is computed using the normalized 8-point algorithm, followed by
projective reconstruction, as described in chapter 12. Thus, estimation of the funda-
mental matrix using this method is effectively equivalent to projective reconstruction.
The steps of the algorithm are summarized in algorithm 11.3.

i

It may seem that this method for computing F will be expensive in computing cost.
However, the use of the sparse LM techniques means that it is not much more expensive
than other iterative techniques, and details of this are given in section A6.5(p609).

11.4.2 Parametrization of rank-2 matrices
The non-linear minimization of the geometric distance cost functions requires a
parametrization of the fundamental matrix which enforces the rank 2 property of the
matrix. We describe three such parametrizations.

286

11 Computation of the Fundamental Matrix F

Over-parametrization. One way that we have already seen for parametrizing F is to
write F = [t]×M, where M is an arbitrary 3 × 3 matrix. This ensures that F is singular,
since [t]× is. This way, F is parametrized by the nine entries of M and the three entries
of t – a total of 12 parameters, more than the minimum number of parameters, which
is 7. In general this should not cause a signiﬁcant problem.

Epipolar parametrization. An alternative way of parametrizing F is by specifying
the ﬁrst two columns of F, along with two multipliers α and β such that the third
column may be written as a linear combination f 3 = αf 1 + βf 2. Thus, the fundamental
matrix is parametrized as

 a b αa + βb

c d αc + βd
e f αe + βf

 .

F =

(11.7)

This has a total of 8 parameters. To achieve a minimum set of parameters, one of the
elements, for instance f, may be set to 1. In practice whichever of a, . . . , f has greatest
absolute value is set to 1. This method ensures a singular matrix F, while using the
minimum number of parameters. The main disadvantage is that it has a singularity –
it does not work when the ﬁrst two columns of F are linearly dependent, for then it is
not possible to write column 3 in terms of the ﬁrst two columns. This problem can be
signiﬁcant, since it will occur in the case where the right epipole lies at inﬁnity. For then
Fe = F(e1, e2, 0)T = 0 and the ﬁrst two columns are linearly dependent. Nevertheless,
this parametrization is widely used and works well if steps are taken to avoid this
singularity. Instead of using the ﬁrst two columns as a basis, another pair of columns
can be used, in which case the singularity occurs when the epipole is on one of the
coordinate axes. In practice such singularities can be detected during the minimization
and the parametrization switched to one of the alternative parametrizations.
Note that (α, β,−1)T is the right epipole for this fundamental matrix – the coor-
dinates of the epipole occur explicitly in the parametrization. For best results, the
parametrization should be chosen so that the largest entry (in absolute value) of the
epipole is the one set to 1.

Note how the complete manifold of possible fundamental matrices is not covered by
a single parametrization, but rather by a set of minimally parametrized patches. As a
path is traced out through the manifold during a parameter minimization procedure, it
is necessary to switch from one patch to another as the boundary between patches is
crossed. In this case there are actually 18 different parameter patches, depending on
which of a, . . . , f is greatest, and which pair of columns are taken as the basis.

Both epipoles as parameters. The previous parametrization uses one of the epipoles
as part of the parametrization. For symmetry one may use both the epipoles as param-
eters. The resulting form of F is

F =

a
c

a + β

b
d

b + β

(cid:2)

(cid:2)

c α

(cid:2)

α

(cid:2)

(cid:2)

d α

αa + α

αa + βb
αc + βd
(cid:2)
(cid:2)
βb + β

αc + β

(cid:2)

βd

(11.8)

 .



11.4 Geometric distance

287

,−1)T. As above, one can set one of
The two epipoles are (α, β,−1)T and (α
a, b, c, d to 1. To avoid singularities, one must switch between different choices of the
two rows and two columns to use as the basis. Along with four choices of which of
a, b, c, d to set to 1, there are a total of 36 parameter patches used to cover the complete
manifold of fundamental matrices.

, β

(cid:2)

(cid:2)

11.4.3 First-order geometric error (Sampson distance)
The concept of Sampson distance was discussed at length in section 4.2.6(p98). Here
the Sampson approximation is used in the case of the variety deﬁned by x(cid:2)TFx = 0 to
provide a ﬁrst-order approximation to the geometric error.

The general formula for the Sampson cost function is given in (4.13–p100). In the
case of fundamental matrix estimation, the formula is even simpler, since there is only
one equation per point correspondence (see also example 4.2(p100)). The partial-
derivative matrix J has only one row, and hence JJT is a scalar and (4.12–p99) becomes

T
JJT =

(x(cid:2)

TFxi)2
i
JJT

.

From the deﬁnition of J and the explicit form of Ai = x(cid:2)
of (11.2), we obtain

i

TFxi given in the left hand side

JJT = (Fxi)2

1 + (Fxi)2

2 + (FTx(cid:2)
1 + (FTx(cid:2)
i)2
i)2
2

where for instance (Fxi)2

the cost function is (cid:7)

j represents the square of the j-th entry of the vector Fxi. Thus,

(x(cid:2)
TFxi)2
2 + (FTx(cid:2)
1 + (FTx(cid:2)
1 + (Fxi)2
i)2
i)2
2

i

(Fxi)2

.

(11.9)

i

This gives a ﬁrst-order approximation to geometric error, which may be expected to
give good results if higher order terms are small in comparison to the ﬁrst. The ap-
proximation has been used successfully in estimation algorithms by [Torr-97, Torr-98,
Zhang-98]. Note that this approximation is undeﬁned at the point in IR4 determined by
the two epipoles, as here JJT is zero. This point should be avoided in any numerical
implementation.

The key advantage of approximating the geometric error in this way is that the result-
ing cost function only involves the parameters of F. This means that to ﬁrst-order the
Gold Standard cost function (11.6) is minimized without introducing a set of subsidiary
variables, namely the coordinates of the n space points Xi. Consequently a minimiza-
tion problem with 7 + 3n degrees of freedom is reduced to one with only 7 degrees of
freedom.

Symmetric epipolar distance. Equation (11.9) is similar in form to another cost

function (cid:7)

i

d(x(cid:2)

i, Fxi)2 + d(xi, FTx(cid:2)
i)2

288

(cid:7)

i

=

(cid:20)

11 Computation of the Fundamental Matrix F
(x(cid:2)

TFxi)2

+

1

1
(FTx(cid:2)
1 + (FTx(cid:2)
i)2
i)2
2

i

(Fxi)2

1 + (Fxi)2

2

(cid:21)

(11.10)

which minimizes the distance of a point from its projected epipolar line, computed in
each of the images. However, this cost function seems to give slightly inferior results
to (11.9) (see [Zhang-98]), and hence is not discussed further.

11.5 Experimental evaluation of the algorithms

Three of the algorithms of the previous sections are now compared by estimating F
from point correspondences for a number of image pairs. The algorithms are:

(i) The normalized 8-point algorithm (algorithm 11.1).
(ii) Minimization of algebraic error whilst imposing the singularity constraint

(algorithm 11.2).

(iii) The Gold Standard geometric algorithm (algorithm 11.3).
The experimental procedure was as follows. For each pair of images, a number n of
matched points were chosen randomly from the matches and the fundamental matrix
estimated and residual error (see below) computed. This experiment was repeated 100
times for each value of n and each pair of images, and the average residual error plotted
against n. This gives an idea of how the different algorithms behave as the number of
points is increased. The number of points used, n, ranged from 8 up to three-quarters
of the total number of matched points.

Residual error
The error is deﬁned as

N(cid:7)

i

1
N

d(x(cid:2)

i, Fxi)2 + d(xi, FTx(cid:2)
i)2

where d(x, l) here is the distance (in pixels) between a point x and a line l. The error is
the squared distance between a point’s epipolar line and the matching point in the other
image (computed for both points of the match), averaged over all N matches. Note the
error is evaluated over all N matched points, and not just the n matches used to com-
pute F. The residual error corresponds to the epipolar distance deﬁned in (11.10). Note
that this particular error is not minimized directly by any of the algorithms evaluated
here.

The various algorithms were tried with 5 different pairs of images. The images are
presented in ﬁgure 11.2 and show the diversity of image types, and placement of the
epipoles. A few of the epipolar lines are shown in the images. The intersection of the
pencil of lines is the epipole. There was a wide variation in the accuracy of the matched
points for the different images, though mismatches were removed in a pre-processing
step.

Results. The results of these experiments are shown and explained in ﬁgure 11.3. They
show that minimizing algebraic error gives essentially indistinguishable results from
minimizing geometric error.

11.5 Experimental evaluation of the algorithms

289

Houses Images

Statue image

Grenoble Museum

Corridor scene

Calibration rig

Fig. 11.2. Image pairs used for the algorithm comparison. In the top two the epipoles are far from the
image centres. In the middle two the epipoles are close (Grenoble) and in the image (Corridor). For the
calibration images the matched points are known extremely accurately.

11.5.1 Recommendations

Several methods of computing the fundamental matrix have been discussed in this
chapter, and some pointers on which method to use are perhaps desirable. Brieﬂy,
these are our recommendations:
• Do not use the unnormalized 8-point algorithm.
• For a quick method, easy to implement, use the normalized 8-point algorithm 11.1.
This often gives adequate results, and is ideal as a ﬁrst step in other algorithms.
• If more accuracy is desired, use the algebraic minimization method, either with or
without iteration on the position of the epipole.
• As an alternative that gives excellent results, use an iterative-minimization method
that minimizes the Sampson cost function (11.9). This and the iterative algebraic
method give similar results.
• To be certain of getting the best results, if Gaussian noise is a viable assumption,
implement the Gold Standard algorithm.

290

11 Computation of the Fundamental Matrix F

5

4

3

2

1

0

r
o
r
r

E

houses

25

20

15

10

5

0

r
o
r
r

E

statue

3

2.5

2

r
o
r
r

E

1.5

1

0.5

5

10

20

15
25
Number of points

30

35

6

8

10

12

14

16

18

20

22

5

10

Number of Points

museum

20

15
Number of Points

25

30

35

2

1.5

r
o
r
r

E

1

0.5

0

Corridor

6

8

10

12

14

16

18

20

22

r
o
r
r

E

0.5

0.4

0.3

0.2

0.1

0

5

calibration

10

15

20

25

30

35

Number of Points

Number of Points

Fig. 11.3. Results of the experimental evaluation of the algorithms. In each case, three methods of
computing F are compared. Residual error is plotted against the number of points used to compute F.
In each graph, the top (solid line) shows the results of the normalized 8-point algorithm. Also shown
are the results of minimizing geometric error (long dashed line) and iteratively minimizing algebraic
error subject to the determinant constraint (short dashed line). In most cases, the result of iteratively
minimizing algebraic error is almost indistinguishable from minimizing geometric error. Both are no-
ticeably better than the non-iterative normalized 8-point algorithm, though that algorithm also gives
good results.

11.6 Automatic computation of F

This section describes an algorithm to compute the epipolar geometry between two
images automatically. The input to the algorithm is simply the pair of images, with no
other a priori information required; and the output is the estimated fundamental matrix
together with a set of interest points in correspondence.

The algorithm uses RANSAC as a search engine in a similar manner to its use in
the automatic computation of a homography described in section 4.8(p123). The ideas
and details of the algorithm are given there, and are not repeated here. The method is
summarized in algorithm 11.4, with an example of its use shown in ﬁgure 11.4.

A few remarks on the method:

(i) The RANSAC sample. Only 7 point correspondences are used to estimate F.
This has the advantage that a rank 2 matrix is produced, and it is not necessary
to coerce the matrix to rank 2 as in the linear algorithms. A second reason for
using 7 correspondences, rather than 8 say with a linear algorithm, is that the
number of samples that must be tried in order to ensure a high probability of no
outliers is exponential in the size of the sample set. For example, from table 4.3-
(p119) for a 99% conﬁdence of no outliers (when drawing from a set containing
50% outliers) twice as many samples are required for 8 correspondences as for
7. The slight disadvantage in using 7 correspondences is that it may result in 3
real solutions for F, and all 3 must be tested for support.

11.6 Automatic computation of F

291

Objective Compute the fundamental matrix between two images.

Algorithm

(i) Interest points: Compute interest points in each image.
(ii) Putative correspondences: Compute a set of interest point matches based on proxim-

ity and similarity of their intensity neighbourhood.

(iii) RANSAC robust estimation: Repeat for N samples, where N is determined adap-

tively as in algorithm 4.5(p121):

(a) Select a random sample of 7 correspondences and compute the fundamental
matrix F as described in section 11.1.2. There will be one or three real solutions.

(b) Calculate the distance d⊥ for each putative correspondence.
(c) Compute the number of inliers consistent with F by the number of correspon-

dences for which d⊥ < t pixels.

(d) If there are three real solutions for F the number of inliers is computed for each

solution, and the solution with most inliers retained.

Choose the F with the largest number of inliers. In the case of ties choose the solution
that has the lowest standard deviation of inliers.

(iv) Non-linear estimation: re-estimate F from all correspondences classiﬁed as inliers
by minimizing a cost function, e.g. (11.6), using the Levenberg–Marquardt algorithm
of section A6.2(p600).

(v) Guided matching: Further interest point correspondences are now determined using

the estimated F to deﬁne a search strip about the epipolar line.

The last two steps can be iterated until the number of correspondences is stable.

Algorithm 11.4. Algorithm to automatically estimate the fundamental matrix between two images using
RANSAC.

(ii) The distance measure. Given a current estimate of F (from the RANSAC
sample) the distance d⊥ measures how closely a matched pair of points satis-
ﬁes the epipolar geometry. There are two clear choices for d⊥: reprojection
error, i.e. the distance minimized in the cost function (11.6) (the value may
be obtained using the triangulation algorithm of section 12.5); or the Sampson
approximation to reprojection error (d2⊥ is given by (11.9)). If the Sampson
approximation is used, then the Sampson cost function should be used to itera-
tively estimate F. Otherwise distances used in RANSAC and elsewhere in the
algorithm will be inconsistent.

(iii) Guided matching. The current estimate of F deﬁnes a search band in the sec-
ond image around the epipolar line Fx of x. For each corner x a match is sought
within this band. Since the search area is restricted a weaker similarity thresh-
old can be employed, and it is not necessary to enforce a “winner takes all”
scheme.
(iv) Implementation and run details. For the example of ﬁgure 11.4, the search
window was ±300 pixels. The inlier threshold was t = 1.25 pixels. A total
of 407 samples were required. The RMS pixel error after RANSAC was 0.34
(for 99 correspondences), and after MLE and guided matching it was 0.33 (for
157 correspondences). The guided matching MLE required 10 iterations of the
Levenberg–Marquardt algorithm.

292

11 Computation of the Fundamental Matrix F

a

c

e

g

b

d

f

h

Fig. 11.4. Automatic computation of the fundamental matrix between two images using RANSAC.
(a) (b) left and right images of Keble College, Oxford. The motion between views is a translation and
rotation. The images are 640 × 480 pixels. (c) (d) detected corners superimposed on the images. There
are approximately 500 corners on each image. The following results are superimposed on the left image:
(e) 188 putative matches shown by the line linking corners, note the clear mismatches; (f) outliers – 89
of the putative matches. (g) inliers – 99 correspondences consistent with the estimated F; (h) ﬁnal set of
157 correspondences after guided matching and MLE. There are still a few mismatches evident, e.g. the
long line on the left.

11.7 Special cases of F-computation

293

e

image

Fig. 11.5. For a pure translation the epipole can be estimated from the image motion of two points.

11.7 Special cases of F-computation

Certain special cases of motion, or partially known camera calibration, allow computa-
tion of the fundamental matrix to be simpliﬁed. In each case the number of degrees of
freedom of the fundamental matrix is less than the 7 of general motion. We give three
examples.

11.7.1 Pure translational motion
This is the simplest possible case. The matrix can be estimated linearly whilst si-
multaneously imposing the constraints that the matrix must satisfy, namely that it is
skew-symmetric (see section 9.3.1(p247)), and thus has the required rank of 2. In this
case F = [e(cid:2)
]×, and has two degrees of freedom. It may be parametrized by the three
entries of e(cid:2)
.

Each point correspondence provides one linear constraint on the homogeneous pa-
rameters, as is clear from ﬁgure 11.5. The matrix can be computed uniquely from two
point correspondences.

Note, in the general motion case if all 3D points are coplanar, which is a structure
degeneracy (see section 11.9), the fundamental matrix cannot be determined uniquely
from image correspondences. However, for pure translational motion this is not a prob-
lem (two 3D points are always coplanar). The only degeneracy is if the two 3D points
are coplanar with both camera centres.

This special form also simpliﬁes the Gold Standard estimation, and correspondingly
triangulation for structure recovery. The Gold Standard estimation of the epipole from
point correspondences under pure translation is identical to the estimation of a vanish-
ing point given the end points of a set of imaged parallel lines, see section 8.6.1(p213).

11.7.2 Planar motion
In the case of planar motion, described in section 9.3.2(p250), we require that the
symmetric part of F has rank 2, in addition to the standard rank 2 condition for
the full matrix.
It can be veriﬁed that the parametrization of (9.8–p252), namely
F = [e(cid:2)
]×[ls]×[e]×, satisﬁes both these conditions. If unconstrained 3-vectors are used
to represent e(cid:2)
, ls and e then 9 parameters are used, whereas the fundamental matrix
for planar motion has only 6 degrees of freedom. As usual this over-parametrization is
not a problem.

294

11 Computation of the Fundamental Matrix F

An alternative parametrization with similar properties is

(cid:15)

(cid:16)

F = α[xa]× + β

lslT

h + lhlT

s

with xT

alh = 0

where α and β are scalars, and the meaning of the 3-vectors xa, ls and lh is evident
from ﬁgure 9.11(p253)(a).

i

11.7.3 The calibrated case
In the case of calibrated cameras normalized image coordinates may be used, and the
essential matrix E computed instead of the fundamental matrix. As with the fundamen-
tal matrix, the essential matrix may be computed using linear techniques from 8 points
or more, since corresponding points satisfy the deﬁning equation x(cid:2)

TExi = 0.

Where the method differs from the computation of the fundamental matrix is in the
enforcement of the constraints. For, whereas the fundamental matrix satisﬁes det F =
0, the essential matrix satisﬁes the additional condition that its two singular values are
equal. This constraint may be handled by the following result, which is offered here
without proof.
Result 11.1. Let E be a 3 × 3 matrix with SVD given by E = UDVT, where D =
diag(a, b, c) with a ≥ b ≥ c. Then the closest essential matrix to E in Frobenius
norm is given by ˆE = UˆDVT, where ˆD = diag((a + b)/2, (a + b)/2, 0).
If the goal is to compute the two normalized camera matrices P and P(cid:2)
as part of a
reconstruction process, then it is not actually necessary to compute ˆE by multiplying out
ˆE = UˆDVT. Matrix P(cid:2)
can be computed directly from the SVD according to result 9.19-
(p259). The choice between the four solutions for P(cid:2)
is determined by the consideration
that the visible points must lie in front of the two cameras, as explained in section 9.6.3-
(p259).

11.8 Correspondence of other entities

So far in this chapter only point correspondences have been employed, and the question
naturally arises: can F be computed from the correspondence of image entities other
than points? The answer is yes, but not from all types of entities. We will now discuss
some common examples.

Lines. The correspondence of image lines between views places no constraint at all on
F. Here a line is an inﬁnite line, not a line segment. Consider the case of corresponding
image points: the points in each image back-project to rays, one through each camera
centre, and these rays intersect at the 3-space point. Now in general two lines in 3-
space are skew (i.e. they do not intersect); so the condition that the rays intersect places
a constraint on the epipolar geometry. In contrast in the case of corresponding image
lines, the back-projection is a plane from each view. However, two planes in 3-space
always intersect so there is no constraint on the epipolar geometry (there is a constraint
in the case of 3-views).

In the case of parallel lines, the correspondence of vanishing points does provide a

11.9 Degeneracies

295

X

x

x/

l

/

l

C

e

e /

/

C

e

/

e

a

b

Fig. 11.6. Epipolar tangency. (a) for a surface; (b) for a space curve – ﬁgure after Porrill and Pol-
lard [Porrill-91]. In (a) the epipolar plane CC(cid:1)X is tangent to the surface at X. The imaged outline
is tangent to the epipolar lines at x and x(cid:1)
in the two views. The dashed curves on the surface are the
contour generators. In (b) the epipolar plane is tangent to the space curve. The corresponding epipolar
lines l ↔ l(cid:1)

are tangent to the imaged curve.

constraint on F. However, a vanishing point has the same status as any ﬁnite point, i.e.
it provides one constraint.

Space curves and surfaces. As illustrated in ﬁgure 11.6, at points at which the epipo-
lar plane is tangent to a space curve the imaged curve is tangent to the corresponding
epipolar lines. This provides a constraint on the 2 view geometry, i.e. if an epipolar line
is tangent to an imaged curve in one view, then the corresponding epipolar line must
be tangent to the imaged curve in the other view. Similarly, in the case of surfaces,
at points at which the epipolar plane is tangent to the surface the imaged outline is
tangent to the corresponding epipolar lines. Epipolar tangent points act effectively as
point correspondences and may be included in estimation algorithms as described by
[Porrill-91].

Particularly important cases are those of conics and quadrics which are algebraic
objects and so algebraic solutions can be developed. Examples are given in the notes
and exercises at the end of this chapter.

11.9 Degeneracies

A set of correspondences {xi ↔ x(cid:2)
i, i = 1, . . . , n} is geometrically degenerate with
respect to F if it fails to uniquely deﬁne the epipolar geometry, or equivalently if there
exist linearly independent rank-2 matrices, Fj, j = 1, 2, such that

x(cid:2)

i

TF1xT

i = 0 and x(cid:2)

i

TF2xi = 0

(1 ≤ i ≤ n) .

The subject of degeneracy is investigated in detail in chapter 22. However, a brief
preview is given now for the two important cases of scene points on a ruled quadric, or
on a plane.
Provided the two camera centres are not coincident the epipolar geometry is uniquely
deﬁned. It can always be computed from the camera matrices P, P(cid:2)
as in (9.1–p244)
for example. What is at issue here are conﬁgurations where the epipolar geometry
cannot be estimated from point correspondences. An awareness of the degeneracies of

296

11 Computation of the Fundamental Matrix F

dim(N) = 1: Unique solution – no degeneracy.

Arises from n ≥ 8 point correspondences in general position. If n > 8 then the point
correspondences must be perfect (i.e. noise-free).

dim(N) = 2: 1 or 3 solutions.

Arises in the case of seven point correspondences, and also in the case of n >7
perfect point correspondences where the 3D points and camera centres lie on a ruled
quadric referred to as a critical surface. The quadric may be non-degenerate (a hyper-
boloid of one sheet) or degenerate.

dim(N) = 3: Two-parameter family of solutions.

Arises if n ≥ 6 perfect point correspondences are related by a homography, x(cid:1)
• Rotation about the camera centre (a degenerate motion).
• All world points on a plane (a degenerate structure).

i = Hxi.

Table 11.1. Degeneracies in estimating F from point correspondences, classiﬁed by the dimension of the
null-space N of A in (11.3–p279).

estimation algorithms is important because conﬁgurations “close to” degenerate ones
are likely to lead to a numerically ill-conditioned estimation. The degeneracies are
summarized in table 11.1.

11.9.1 Points on a ruled quadric
It will be shown in chapter 22 that degeneracy occurs if both camera centres and
all the 3D points lie on a (ruled) quadric surface referred to as the critical sur-
face [Maybank-93]. A ruled quadric may be non-degenerate (a hyperboloid of one
sheet – a cooling tower) or degenerate (for instance two planes, cones, and cylinders)
– see section 3.2.4(p74); but a critical surface cannot be an ellipsoid or hyperboloid
of two sheets. For a critical surface conﬁguration there are three possible fundamental
matrices.

Note that in the case of just 7 point correspondences, together with the two camera
centres there are 9 points in total. A general quadric has 9 degrees of freedom, and one
may always construct a quadric through 9 points. In the case where this quadric is a
ruled quadric it will be a critical surface, and there will be three possible solutions for
F. The case where the quadric is not ruled corresponds to the case where there is only
one real solution for F.

11.9.2 Points on a plane
In this case, all the
An important degeneracy is when all the points lie in a plane.
points plus the two camera centres lie on a ruled quadric surface, namely the degenerate
quadric consisting of two planes – the plane through the points, plus a plane passing
through the two camera centres.
Two views of a planar set of points are related via a 2D projective transformation
H. Thus, suppose that a set of correspondences xi ↔ x(cid:2)
i =
Hxi. Any number of points xi and the corresponding points x(cid:2)
i = Hxi may be given.

i is given for which x(cid:2)

11.10 A geometric interpretation of F-computation

297

i

i

T(FH−1)x(cid:2)

The fundamental matrix corresponding to the pair of cameras satisﬁes the equation
x(cid:2)
TFxi = x(cid:2)
i = 0. This set of equations is satisﬁed whenever FH−1 is skew-
symmetric. Thus, the solution for F is any matrix of the form F = SH, where S is
skew-symmetric. Now, a 3 × 3 skew-symmetric matrix S may be written in the form
S = [t]×, for any 3-vector t. Thus, S has three degrees of freedom, and consequently so
does F. More precisely, the correspondences xi ↔ x(cid:2)
i lead to a three-parameter family
of possible fundamental matrices F (note, one of the parameters accounts for scaling
the matrix so there is only a two-parameter family of homogeneous matrices). The
equation matrix A derived from the set of correspondences must therefore have rank no
greater than 6.
From the decomposition of F = SH, it follows from result 9.9(p254) that the pair
of camera matrices [I | 0] and [H | t] correspond to the fundamental matrix F. Here,
the vector t may take on any value. If point xi = (xi, yi, 1)T and x(cid:2)
i = Hxi, then one
veriﬁes that the point Xi = (x, y, 1, 0)T maps to xi and x(cid:2)
i through the two cameras.
Thus, the points Xi constitute a reconstruction of the scene.

11.9.3 No translation
If the two camera centres are coincident then the epipolar geometry is not deﬁned. In
addition, formulae such as result 9.9(p254) give a value of 0 for the fundamental matrix.
In this case the two images are related by a 2D homography (see section 8.4.2(p204)).
If one attempts to ﬁnd the fundamental matrix then, as shown above, there will be
at least a 2-parameter family of solutions for F. Even if the camera motion involves
no translation, then a method such as the 8-point algorithm used to compute the fun-
damental matrix will still produce a matrix F satisfying x(cid:2)
TFxi = 0, where F has the
form F = SH, H is the homography relating the points, and S is an essentially arbitrary
skew-symmetric matrix. Points xi and x(cid:2)

i related by H will satisfy this relationship.

i

i

(cid:2)

(cid:2)

, y

11.10 A geometric interpretation of F-computation
The estimation of F from a set of image correspondences {xi ↔ x(cid:2)
} has many sim-
ilarities with the problem of estimating a conic from a set of 2D points {xi, yi} (or a
quadric from a set of 3D points).
The equation x(cid:2)TFx = 0 is a single constraint in x, y, x
and so deﬁnes a surface
(variety) V of codimension 1 (dimension 3) in IR4. The surface is a quadric because
the equation is quadratic in the coordinates x, y, x
of IR4. There is a natural map-
ping from projective 3-space to the variety V that takes any 3D point to the quadruple
)T of the corresponding image points in the two views. The quadric form is
(x, y, x
evident if x(cid:2)TFx = 0 is rewritten as
0
0
f31
f32
0
0
f13
f11 f12
f23
f21 f22
f31 f32 f13 f23 2f33

 = 0 .

f11 f21
f12 f22
0
0
0
0



x
y
(cid:2)
x
(cid:2)
y
1





x y x

(cid:15)

(cid:2)

(cid:2)

, y

(cid:16)

(cid:2)

(cid:2)

, y

(cid:2)

(cid:2)

y

1

The case of conic ﬁtting is a good (lower-dimensional) model of F estimation. To

298

11 Computation of the Fundamental Matrix F

tangent
line

Fig. 11.7. Estimating a conic from point data (shown as •) may be poorly conditioned. All of the conics
shown have residuals within the point error distribution. However, even though there is ambiguity in the
estimated conic, the tangent line is well deﬁned, and can be computed from the points.

bring out the analogy between the two estimation problems: a point (xi, yi) places one
constraint on the 5 degrees of freedom of a conic as described in section 2.2.3(p30):

ax2

i + bxiyi + cy2

i + dxi + eyi + f = 0.

Similarly, a point correspondence (xi, yi, x
of freedom of F as (11.2–p279):

(cid:2)
i, y

(cid:2)
i) places one constraint on the (8) degrees

(cid:2)
ixif11 + x

(cid:2)
iyif12 + x

x

(cid:2)
if13 + y

(cid:2)
ixif21 + y

(cid:2)
iyif22 + y

(cid:2)
if23 + xif31 + yif32 + f33 = 0.

It is not quite an exact analogy, since the deﬁning relationship expressed by the fun-
damental matrix is bilinear in the two sets of indices, as is also evident from the zeros
in the quadric matrix above, whereas in the case of a conic section the equation is an
arbitrary quadratic. Also the surface deﬁned by F must satisfy an additional constraint
arising from det(F) = 0, and there is no such constraint in the conic ﬁtting analogue.

The problems of extrapolation when data has only been ﬁtted to a small section of a
conic are well known, and similar issues arise in ﬁtting the fundamental matrix to data.
Indeed, there are cases where the data is sufﬁcient to determine an accurate tangent line
to the conic, but insufﬁcient to determine the conic itself, see ﬁgure 11.7. In the case of
the fundamental matrix the tangent plane to the quadric in IR4 is the afﬁne fundamental
matrix (chapter 14), and this approximation may be ﬁtted when perspective effects are
small.

11.11 The envelope of epipolar lines

One of the uses of the fundamental matrix is to determine epipolar lines in a second
image corresponding to points in a ﬁrst image. For instance, if one is seeking matched
points between two images, the match of a given point x in the ﬁrst image may be
found by searching along the epipolar line Fx in the second image. In the presence
of noise, of course, the matching point will not lie precisely on the line Fx because
the fundamental matrix will be known only within certain bounds, expressed by its
covariance matrix. In general, instead of searching along the epipolar line only, it will
be necessary to search in a region on either side of the line Fx. We will now consider
how the covariance matrix of the fundamental matrix may be used to determine the
region in which to search.

Let x be a point and F be a fundamental matrix for which one has computed a co-
variance matrix ΣF. The point x corresponds to an epipolar line l = Fx, and one

11.11 The envelope of epipolar lines

299

may transfer the covariance matrix ΣF to a covariance matrix Σl according to result 5.6-
(p139). Also by result 5.6(p139), the mean value of the epipolar line is given by ¯l = ¯Fx.
To avoid singular cases, the vector l representing an epipolar line is normalized so that
(cid:10)l(cid:10) = 1. Then the mapping x (cid:6)→ l is given by l = (Fx)/(cid:10)Fx(cid:10). If J is the Jacobian
matrix of this mapping with respect to the entries of F, then J is a 3 × 9 matrix, and
Σl = JΣFJT.
Though the constraint (cid:10)l(cid:10) = 1 is the most convenient constraint, the following analy-
sis applies for any constraint used to conﬁne the vector representing the epipolar line to
vary on a 2-dimensional surface in IR3. In this case, the covariance matrix Σl is singular,
having rank 2, since no variation is allowed in the direction normal to the constraint
surface. For a particular instance of l, the deviation from the mean, ¯l − l, must be
along the constraint surface, and hence (in the linear approximation) perpendicular to
the null-space of Σl.

For the remainder of this derivation, ¯l, the vector representing the mean epipolar line,
will be denoted by m, so as to avoid confusing notation. Now, assuming a Gaussian
distribution for the vectors l representing the epipolar line, the set of all lines having a
given likelihood is given by the equation
(l − m)TΣ+

(11.11)

where k is some constant. To analyze this further, we apply an orthogonal change of
coordinates such that Σl becomes diagonal. Thus, one may write

l (l − m) =k 2
(cid:18)

(cid:17)

(cid:2)
l 0
˜Σ
0T 0

UΣlUT = Σ(cid:2)

l =

(cid:2)
l is a 2 × 2 non-singular diagonal matrix. Applying the same transformation
where ˜Σ
= Ul. Since l(cid:2) − m(cid:2)
to the lines, one deﬁnes 2-vectors m(cid:2)
is orthogonal
to the null-space (0, 0, 1)T of Σ(cid:2)
have the same third coordinate. By
multiplying U by a constant as necessary, one may assume that this coordinate is 1.
and ˜m(cid:2)
Thus we may write l(cid:2)
.
Then, one veriﬁes that

and l(cid:2)
(cid:2)
= ( ˜m(cid:2)T, 1)T for certain 2-vectors ˜l

l, both m(cid:2)
(cid:2)T, 1)T and m(cid:2)

= Um and l(cid:2)

= (˜l

l (l − m)
k2 = (l − m)TΣ+
l (l(cid:2) − m(cid:2)
)TΣ(cid:2)+
(cid:2) − ˜m(cid:2)
(cid:2)−1
)T˜Σ
(˜l
l

= (l(cid:2) − m(cid:2)
(cid:2) − ˜m(cid:2)
= (˜l

)

).

(cid:2)−1
l ˜l

(cid:2)T˜Σ
˜l

This equation expands out to
(cid:2) − ˜m(cid:2)T˜Σ
(cid:17)

which may be written as
(cid:2)T 1)
(˜l

(cid:2)−1
l ˜l

(cid:2) − ˜l

(cid:2)T˜Σ

(cid:2)−1
l

˜m(cid:2)

+ ˜m(cid:2)T˜Σ

(cid:2)−1
˜Σ
l
− ˜m(cid:2)T˜Σ

(cid:2)−1
−˜Σ
˜m(cid:2)
l
(cid:2)−1
˜m(cid:2) − k2
˜m(cid:2)T˜Σ
l

(cid:2)−1
l

(cid:2)
˜l
1

(cid:2)−1
l

˜m(cid:2) − k2 = 0
(cid:18)(cid:20)

(cid:21)

= 0

300

11 Computation of the Fundamental Matrix F

or equivalently (as one may verify)

(cid:17)

(cid:2)T 1)
(˜l

˜m(cid:2)

(cid:2)
˜m(cid:2)T − k2˜Σ
l
˜m(cid:2)T

˜m(cid:2)
1

(cid:18)−1

(cid:20)

(cid:21)

(cid:2)
˜l
1

= 0.

(11.12)

Finally, this is equivalent to

l(cid:2)T[m(cid:2)m(cid:2)T − k2Σ(cid:2)
−1l(cid:2)
l]

= 0.

(11.13)

This shows that the lines satisfying (11.11) form a line conic deﬁned by the matrix
(m(cid:2)m(cid:2)T − k2Σ(cid:2)
−1. The corresponding point conic, which forms the envelope of the
l)
lines, is deﬁned by the matrix m(cid:2)m(cid:2)T− k2Σ(cid:2)
l. One may now transform back to the orig-
inal coordinate system to determine the envelope of the lines in the original coordinate
system. The transformed conic is

C = UT(m(cid:2)m(cid:2)T − k2Σ(cid:2)

l)U = mmT − k2Σl.

(11.14)

(cid:31)

k2
0 χ2

n(ξ)dξ represents the probability that the value of a χ2

Note that when k = 0, the conic C degenerates to mmT, which represents the set of
points lying on the line m. As k increases, the conic becomes a hyperbola the two
branches of which lie on opposite sides of the line m.
Suppose we want to choose k so that some fraction α of the epipolar lines lie in-
side the region bounded by this hyperbola. The value k2 = (l − m)TΣ+
l (l − m)
of (11.11) follows a χ2
n distribution, and the cumulative chi-squared distribution
n random vari-
Fn(k2) =
able is less than k2 (the χ2
n and Fn distributions are deﬁned in section A2.2(p566)).
Applying this to a random line l, one sees that in order to ensure that a fraction α of
lines lie within region bounded by the hyperbola deﬁned by (11.14), one must choose
k2 such that F2(k2) = α (n = 2 since the covariance matrix Σl has rank 2). Thus,
−1
2 (α), and for a value of α = 0.95, for instance, one ﬁnds that k2 = 5.9915.
k2 = F
The corresponding hyperbola given by (11.14) is C = mmT − 5.9915 Σl. To sum up
this discussion:
Result 11.2. If l is a random line obeying a Gaussian distribution with mean ¯l and
covariance matrix Σl of rank 2, then the plane conic
C = ¯l ¯lT − k2Σl

(11.15)

represents an equal-likelihood contour bounding some fraction of all instances of l. If
−1
F2(k2) represents the cumulative χ2
2 (k2) =
α, then a fraction α of all lines lie within the region bounded by C. In other words with
probability α the lines lie within this region.

2 distribution, and k2 is chosen such that F

In applying this formula, one must be aware that it represents only an approxima-
tion, since epipolar lines are not normally distributed. We have consistently made the
assumption that the distributions may be correctly transformed using the Jacobian, that
is an assumption of linearity. This assumption will be most reasonable for distribu-
tions with small variance, and close to the mean. Here, we are applying it to ﬁnd the
region in which as many as 95% of samples fall, namely almost the whole of the error

11.11 The envelope of epipolar lines

301

distribution. In this case, the assumption of a Gaussian distribution of errors is less
tenable.

11.11.1 Veriﬁcation of epipolar line covariance
We now present some examples of epipolar line envelopes, conﬁrming and illustrating
the theory developed above. Before doing this, however, a direct veriﬁcation of the
theory will be given, concerning the covariance matrix of epipolar lines. Since the
3 × 3 covariance matrix of a line is not easily understood quantitatively, we consider
the variance of the direction of epipolar lines. Given a line l = (l1, l2, l3)T, the angle
representing its direction is given by θ = arctan(−l1/l2). Letting J equal the 1 × 3
Jacobian matrix of the mapping l → θ, one ﬁnds the variance of the angle θ to be
θ = JΣlJT. This result may be veriﬁed by simulation, as follows.
σ2
One considers a pair of images for which point correspondences have been identiﬁed.
The fundamental matrix is computed from the point correspondences and the points are
then corrected so as to correspond precisely under the epipolar mapping (as described
in section 12.3). A set of n of these corrected correspondences are used to compute
the covariance matrix of the fundamental matrix F. Then, for a further set of “test”
corrected points xi in the ﬁrst image, the mean and covariance of the corresponding
epipolar line l(cid:2)
i = Fxi are computed, and subsequently the mean and variance of the
orientation direction of this line are computed. This gives the theoretical values of these
quantities.

Next, Monte Carlo simulation is done, in which Gaussian noise is added to the co-
ordinates of the points used to compute F. Using the computed F, the epipolar lines
corresponding to each of the test points are computed, and subsequently their angle,
and the deviation of the angle from the mean. This is done many times, and the stan-
dard deviation of angle is computed, and ﬁnally compared with the theoretical value.
The results of this are shown in ﬁgure 11.8 for the statue image pair of ﬁgure 11.2-
(p289).

Epipolar envelopes for statue image. The statue image pair of ﬁgure 11.2(p289)
is interesting because of the large depth variation across the image. There are close
points (on the statue) and distant points (on the building behind) in close proximity
in the images. The fundamental matrix was computed from several points. A point
in the ﬁrst image (see ﬁgure 11.9) was selected and Monte Carlo simulation was used
to compute several possible epipolar lines corresponding to a noise level of 0.5 pixels
in each matched point coordinate. To test the theory, the mean and covariance of the
epipolar line were next computed theoretically. The 95% envelope of the epipolar lines
was computed and drawn in the second image. The results are shown in ﬁgure 11.10
for different numbers of points used to compute F. The 95% envelope for n = 15
corresponds closely to the simulated envelope of the lines.

The results shown in ﬁgure 11.10 show the practical importance of computing the
epipolar envelopes in point matching. Thus, suppose one is attempting to ﬁnd a match
for the foreground point in ﬁgure 11.9.
If the epipolar line is computed from just
10 point matches, then epipolar search is unlikely to succeed, given the width of the

302

11 Computation of the Fundamental Matrix F

10

8

6

4

2

0

0

2

4

a

8

6

10
Point Number

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

2

4

12

14

16

12

14

16

8

6

10
Point Number
b

Fig. 11.8. Comparison of theoretical and Monte Carlo simulated values of orientation angle of epipolar
lines for 15 test points form the statue image pair of ﬁgure 11.2(p289). The horizontal axis represents
the point number (1 to 15) and the vertical axis the standard deviation of angle. (a) the results when
the epipolar structure (fundamental matrix) is computed from 15 points. (b) the results when 50 point
matches are used. Note : the horizontal axis of these graphs represent discrete points numbered 1 to 15.
The graphs are shown as a continuous curve only for visual clarity.

envelope. Even for n = 15, the width of the envelope at the level of the correct match
is several tens of pixels. For n = 25, the situation is more favourable. Note that this
instability is inherent in the problem, and not the result of any speciﬁc algorithm for
computing F.

An interesting point concerns the location of the narrowest point of the envelope.
In this case, it appears to be close to the correct match position for the background
point in ﬁgure 11.9. The match for the foreground point (leg of statue) lies far from
the narrowest point of the envelope. Though the precise location of the narrow point of
the envelope is not fully understood, it appears that in this case, most points used to the
computation of F are on the background building. This biases towards the supposition
that other matched points lie close to the plane of the building. The match for a point
at signiﬁcantly different depth is less precisely known.

Matching points close to the epipole – the corridor scene.
In the case where the
points to be matched are close to the epipole, then the determination of the epipolar
line is more unstable, since any uncertainty in the position of the epipole results in
uncertainty in the slope of the epipolar line. In addition, as one approaches this unstable
position, the linear approximations implicit in the derivation of (11.14) become less
tenable.
In particular, the distribution of the epipolar lines deviates from a normal
distribution.

11.12 Image rectiﬁcation

This section gives a method for image rectiﬁcation, the process of resampling pairs
of stereo images taken from widely differing viewpoints in order to produce a pair
of “matched epipolar projections”. These are projections in which the epipolar lines
run parallel with the x-axis and match up between views, and consequently disparities
between the images are in the x-direction only, i.e. there is no y disparity.

11.12 Image rectiﬁcation

303

a

b

Fig. 11.9. (a) The point in the ﬁrst image used to compute the epipolar envelopes in the second images.
Note the ambiguity of which point is to be found in the second image. The marked point may represent
the point on the statue’s leg (foreground) or the point on the building behind the statue (background).
In the second image, these two points are quite separate, and the epipolar line must pass through them
both. (b) Computed corresponding epipolar lines computed from n = 15 point matches. The different
lines correspond to different instances of injected noise in the matched points. Gaussian noise of 0.5
pixels in each coordinate was added to the ideal matched point positions before computing the epipolar
line corresponding to the selected point. The ML estimator (Gold Standard algorithm) was used to
compute F. This experiment shows the basic instability of the computation of the epipolar lines from
small numbers of points. To ﬁnd the point matching the selected point in the image at left, one needs to
search over the regions covered by all these epipolar lines.

The method is based on the fundamental matrix. A pair of 2D projective transfor-
mations are applied to the two images in order to match the epipolar lines. It is shown
that the two transformations may be chosen in such a way that matching points have
approximately the same x-coordinate as well. In this way, the two images, if overlaid
on top of each other, will correspond as far as possible, and any disparities will be
parallel to the x-axis. Since the application of arbitrary 2D projective transformations
may distort the image substantially, the method for ﬁnding the pair of transformations
subjects the images to a minimal distortion.

In effect, transforming the two images by the appropriate projective transformations
reduces the problem to the epipolar geometry produced by a pair of identical cameras
placed side by side with their principal axes parallel. Many stereo matching algorithms
described in previous literature have assumed this geometry. After this rectiﬁcation the
search for matching points is vastly simpliﬁed by the simple epipolar structure and by
the near-correspondence of the two images. It may be used as a preliminary step to
comprehensive image matching.

11.12.1 Mapping the epipole to inﬁnity
In this section we will discuss the question of ﬁnding a projective transformation H of
an image that maps the epipole to a point at inﬁnity. In fact, if epipolar lines are to
be transformed to lines parallel with the x-axis, then the epipole should be mapped to

304

11 Computation of the Fundamental Matrix F

n = 10

n = 15

n = 25

n = 50

Fig. 11.10. The 95% envelopes of epipolar lines are shown for a noise level of 0.5 pixels, with F being
computed from n = 10, 15, 25 and 50 points. In each case, Monte Carlo simulated results agreed closely
with these results (though not shown here). For the case n = 15, compare with ﬁgure 11.9. Note that
for n = 10, the epipolar envelope is very wide (> 90 degrees), showing that one can have very little
conﬁdence in an epipolar line computed from 10 points in this case. For n = 15, the envelope is still
quite wide. For n = 25 and n = 50, the epipolar line is known with quite good precision. Of course,
the precise shape of the envelope depends strongly on just what matched points are used to compute the
epipolar structure.

the particular inﬁnite point (1, 0, 0)T. This leaves many degrees of freedom (in fact
four) open for H, and if an inappropriate H is chosen, severe projective distortion of the
image can take place. In order that the resampled image should look somewhat like the
original image, we may put closer restrictions on the choice of H.

One condition that leads to good results is to insist that the transformation H should
act as far as possible as a rigid transformation in the neighbourhood of a given selected
point x0 of the image. By this is meant that to ﬁrst-order the neighbourhood of x0 may
undergo rotation and translation only, and hence will look the same in the original and
resampled images. An appropriate choice of point x0 may be the centre of the image.
For instance, this would be a good choice in the context of aerial photography if the
view is known not to be excessively oblique.

For the present, suppose x0 is the origin and the epipole e = (f, 0, 1)T lies on the

x-axis. Now consider the following transformation

11.12 Image rectiﬁcation

 .

G =

1
0 0
0
1 0
−1/f 0 1

305

(11.16)



(cid:17)

This transformation takes the epipole (f, 0, 1)T to the point at inﬁnity (f, 0, 0)T as
required. A point (x, y, 1)T is mapped by G to the point (ˆx, ˆy, 1)T = (x, y, 1 − x/f )T.
If |x/f| < 1 then we may write

(ˆx, ˆy, 1)T = (x, y, 1 − x/f )T = (x(1 + x/f + . . .), y(1 + x/f + . . .), 1)T.

The Jacobian is

∂(ˆx, ˆy)
∂(x, y)

=

1 + 2x/f

0

y/f

1 + x/f

(cid:18)

plus higher order terms in x and y. Now if x = y = 0 then this is the identity map. In
other words, G is approximated (to ﬁrst-order) at the origin by the identity mapping.

For an arbitrarily placed point of interest x0 and epipole e, the required mapping H
is a product H = GRT where T is a translation taking the point x0 to the origin, R is a
rotation about the origin taking the epipole e(cid:2)
to a point (f, 0, 1)T on the x-axis, and G
is the mapping just considered taking (f, 0, 1)T to inﬁnity. The composite mapping is
to ﬁrst-order a rigid transformation in the neighbourhood of x0.

(cid:2)

(cid:2)

11.12.2 Matching transformations
In the previous section it was shown how the epipole in one image may be mapped to
inﬁnity. Next, it will be seen how a map may be applied to the other image to match
. The intention is to resample
up the epipolar lines. We consider two images J and J
these two images according to transformations H to be applied to J and H(cid:2)
to be applied
to J
. The resampling is to be done in such a way that an epipolar line in J is matched
. More speciﬁcally, if l and l(cid:2)
with its corresponding epipolar line in J
are any pair of
corresponding epipolar lines in the two images, then H−Tl = H(cid:2)−Tl(cid:2)
. (Recall that H−T is
the line map corresponding to the point map H.) Any pair of transformations satisfying
this condition will be called a matched pair of transformations.
ﬁrst to be
some transformation that sends the epipole e(cid:2)
to inﬁnity as described in the previous
section. We then seek a matching transformation H chosen so as to minimize the sum-
of-squared distances

Our strategy in choosing a matched pair of transformations is to choose H(cid:2)

(cid:2)

(cid:7)

d(Hxi, H(cid:2)x(cid:2)

i)2.

(11.17)

The ﬁrst question to be determined is how to ﬁnd a transformation matching H(cid:2)
question is answered in the following result.

. That

i

Result 11.3. Let J and J
a projective transformation of J

be images with fundamental matrix F = [e(cid:2)

]×M, and let H(cid:2)

. A projective transformation H of J matches H(cid:2)

be
if and

(cid:2)

(cid:2)

306

11 Computation of the Fundamental Matrix F

only if H is of the form

H = (I + H(cid:2)e(cid:2)aT)H(cid:2)M

(11.18)

for some vector a.
Proof. If x is a point in J, then e×x is the epipolar line in the ﬁrst image, and Fx is the
epipolar line in the second image. Transformations H and H(cid:2)
are a matching pair if and
only if H−T(e× x) = H(cid:2)−TFx. Since this must hold for all x we may write equivalently
H−T[e]× = H(cid:2)−TF = H(cid:2)−T[e(cid:2)

]×M or, applying result A4.3(p582),

[He]×H = [H(cid:2)e(cid:2)

]×H(cid:2)M.

(11.19)

In view of lemma 9.11(p255), this implies H = (I + H(cid:2)e(cid:2)aT)H(cid:2)M as required.
To prove the converse, if (11.18) holds, then

He = (I + H(cid:2)e(cid:2)aT)H(cid:2)Me = (I + H(cid:2)e(cid:2)aT)H(cid:2)e(cid:2)

= (1 + aTH(cid:2)e(cid:2)

)H(cid:2)e(cid:2)

= H(cid:2)e(cid:2)

.

This, along with (11.18), is sufﬁcient for (11.19) to hold, and so H and H(cid:2)
transformations.

are matching

We are particularly interested in the case when H(cid:2)

is a transformation taking the
to a point at inﬁnity (1, 0, 0)T. In this case, I + H(cid:2)e(cid:2)aT = I + (1, 0, 0)TaT is

epipole e(cid:2)
of the form

 a b

c
0 1 0
0 0 1



HA =

(11.20)

(11.21)

which represents an afﬁne transformation. Thus, a special case of result 11.3 is

(cid:2)

be images with fundamental matrix F = [e(cid:2)

be a projective transformation of J

Corollary 11.4. Let J and J
H(cid:2)
(1, 0, 0)T. A transformation H of J matches H(cid:2)
where H0 = H(cid:2)M and HA is an afﬁne transformation of the form (11.20).

]×M, and let
to the inﬁnite point
if and only if H is of the form H = HAH0,

mapping the epipole e(cid:2)

(cid:2)

Given H(cid:2)

mapping the epipole to inﬁnity, we may use this corollary to make the
choice of a matching transformation H to minimize the disparity. Writing ˆx(cid:2)
i = H(cid:2)x(cid:2)
i
and ˆxi = H0xi, the minimization problem (11.17) is to ﬁnd HA of the form (11.20) such
that

(cid:7)

i

d(HAˆxi, ˆx(cid:2)
i)2

is minimized.
In particular, let ˆxi = (ˆxi, ˆyi, 1)T, and let ˆx(cid:2)
these vectors may be computed from the matched points xi ↔ x(cid:2)
be minimized (11.21) may be written as

(cid:2)
(cid:2)
i, 1)T. Since H(cid:2)
i, ˆy

i = (ˆx

and M are known,
i. Then the quantity to

(cid:7)

i

(aˆxi + bˆyi + c − ˆx

i)2 + (ˆyi − ˆy
(cid:2)
(cid:2)
i)2.

Since (ˆyi − ˆy
(cid:2)
i)2 is a constant, this is equivalent to minimizing

11.12 Image rectiﬁcation

(cid:7)
(aˆxi + bˆyi + c − ˆx

(cid:2)
i)2.

307

i

This is a simple linear least-squares parameter minimization problem, and is easily
solved using linear techniques (see section A5.1(p588)) to ﬁnd a, b and c. Then HA
is computed from (11.20) and H from (11.18). Note that a linear solution is possible
because HA is an afﬁne transformation. If it were simply a projective transformation,
this would not be a linear problem.

11.12.3 Algorithm outline
The resampling algorithm will now be summarized. The input is a pair of images
containing a common overlap region. The output is a pair of images resampled so that
the epipolar lines in the two images are horizontal (parallel with the x-axis), and such
that corresponding points in the two images are as close to each other as possible. Any
remaining disparity between matching points will be along the the horizontal epipolar
lines. A top-level outline of the algorithm is as follows.

(i) Identify a seed set of image-to-image matches xi ↔ x(cid:2)

i between the two images.
Seven points at least are needed, though more are preferable. It is possible to
ﬁnd such matches by automatic means.

images.

(ii) Compute the fundamental matrix F and ﬁnd the epipoles e and e(cid:2)
(iii) Select a projective transformation H(cid:2)
(cid:7)

inﬁnity, (1, 0, 0)T. The method of section 11.12.1 gives good results.

that maps the epipole e(cid:2)

distance

(iv) Find the matching projective transformation H that minimizes the least-squares

to the point at

in the two

d(Hxi, H(cid:2)x(cid:2)
i).

(11.22)

i

(v) Resample the ﬁrst image according to the projective transformation H and the

The method used is a linear method described in section 11.12.2.
second image according to the projective transformation H(cid:2)

.

Example 11.5. Model house images
Figure 11.11(a) shows a pair of images of some wooden block houses. Edges and ver-
tices in these two images were extracted automatically and a small number of common
vertices were matched by hand. The two images were then resampled according to the
methods described here. The results are shown in ﬁgure 11.11(b). In this case, because
of the wide difference in viewpoint, and the three-dimensional shape of the objects, the
two images even after resampling look quite different. However, it is the case that any
point in the ﬁrst image will now match a point in the second image with the same y-
coordinate. Therefore, in order to ﬁnd further point matches between the images only
(cid:2)
a 1-dimensional search is required.

308

11 Computation of the Fundamental Matrix F

a

b

Fig. 11.11. Image rectiﬁcation. (a) A pair of images of a house. (b) Resampled images computed
from (a) using a projective transformation computed from F. Note, corresponding points in (b) match
horizontally.

a

b

Fig. 11.12. Image rectiﬁcation using afﬁnities. (a) A pair of original images and (b) a detail of the
images rectiﬁed using afﬁne transformations. The average y-disparity after rectiﬁcation is of the order
of 3 pixels in a 512 × 512 image. (For correctly rectiﬁed images the y-disparity should be zero.)

11.12.4 Afﬁne rectiﬁcation
The theory discussed in this section can equally be applied to afﬁne resampling. If
the two cameras can be well approximated by afﬁne cameras, then one can rectify the
images using just afﬁne transformations. To do this, one uses the afﬁne fundamental
matrix (see section 14.2(p345)) instead of the general fundamental matrix. The above
method with only minor variations can then be applied to compute a pair of match-
ing afﬁne transformations. Figure 11.12 shows a pair of images rectiﬁed using afﬁne
transformations.

11.13 Closure

11.13.1 The literature
The basic idea behind the computation of the fundamental matrix is given in
[LonguetHiggins-81], which is well worth reading. It addresses the case of calibrated
matrices only, but the principles apply to the uncalibrated case as well. A good refer-
ence for the uncalibrated case is [Zhang-98] which considers most of the best methods.
In addition, that paper considers the uncertainty envelopes of epipolar lines, following
earlier work by Csurka et al. [Csurka-97]. A more detailed study of the 8-point algo-
rithm in the uncalibrated case is given in [Hartley-97c]. Weng et al. [Weng-89] used
Sampson approximation for the fundamental matrix cost function. The SVD method
of coercing the estimated F to have rank 2 was suggested by Tsai & Huang [Tsai-84].
There is a wealth of literature on conic ﬁtting – minimizing algebraic distance

[Bookstein-79]; approximating geometric distance [Sampson-82, Pratt-87, Taubin-91];
optimal ﬁtting [Kanatani-94]; and ﬁtting special forms [Fitzgibbon-99].

11.13 Closure

309

11.13.2 Notes and exercises

(i) Six point correspondences constrain e and e(cid:2)

to a plane cubic in each image
([Faugeras-93], page 298). The cubic also passes through the six points in
each image. A sketch derivation of these results follows. Given six correspon-
dences, the null-space of A in (11.3–p279) will be 3-dimensional. Then the
solution is F = α1F1 + α2F2 + α3F3, where Fi denotes the matrices correspond-
ing to the vectors spanning the null-space. The epipole satisﬁes Fe = 0, so
that [(F1e), (F2e), (F3e)](α1, α2, α3)T = 0. Since this equation has a solution it
follows that det[(F1e), (F2e), (F3e)] = 0 which is a cubic in e.

(ii) Show that the image correspondence of four coplanar points and a quadric
outline determines the fundamental matrix up to a two-fold ambiguity (Hint,
see algorithm 13.2(p336)).

(iii) Show that the corresponding images of a (plane) conic are equivalent to two

constraints on F. See [Kahl-98b] for details.

(iv) Suppose that a stereo pair of images is acquired by a camera translating forward
along its principal axis. Can the geometry of image rectiﬁcation described in
section 11.12 be applied in this case? See [Pollefeys-99a] for an alternative
rectiﬁcation geometry.

12

Structure Computation

This chapter describes how to compute the position of a point in 3-space given its image
in two views and the camera matrices of those views. It is assumed that there are errors
only in the measured image coordinates, not in the projection matrices P, P(cid:2)

.

Under these circumstances na¨ıve triangulation by back-projecting rays from the mea-
sured image points will fail, because the rays will not intersect in general. It is thus
necessary to estimate a best solution for the point in 3-space.

A best solution requires the deﬁnition and minimization of a suitable cost function.
This problem is especially critical in afﬁne and projective reconstruction in which there
is no meaningful metric information about the object space. It is desirable to ﬁnd a
triangulation method that is invariant to projective transformations of space.

In the following sections we describe the estimation of X and of its covariance. An
optimal (MLE) estimator for the point is developed, and it is shown that a solution can
be obtained without requiring numerical minimization.
Note, this is the scenario where F is given a priori and then X is determined. An
alternative scenario is where F and {Xi} are estimated simultaneously from the im-
age point correspondences {xi ↔ x(cid:2)
}, but this is not considered in this chapter. It
may be solved using the Gold Standard algorithm of section 11.4.1, using the method
considered in this chapter as an initial estimate.

i

12.1 Problem statement

It is supposed that the camera matrices, and hence the fundamental matrix, are pro-
vided; or that the fundamental matrix is provided, and hence a pair of consistent cam-
era matrices can be constructed (as in section 9.5(p253)). In either case it is assumed
that these matrices are known exactly, or at least with great accuracy compared with a
pair of matching points in the two images.

Since there are errors in the measured points x and x(cid:2)

, the rays back-projected from
the points are skew. This means that there will not be a point X which exactly satisﬁes
x = PX, x(cid:2)
= P(cid:2)X; and that the image points do not satisfy the epipolar constraint
x(cid:2)TFx = 0. These statements are equivalent since the two rays corresponding to a
matching pair of points x ↔ x(cid:2)
will meet in space if and only if the points satisfy the
epipolar constraint. See ﬁgure 12.1.

310

12.1 Problem statement

311

x

C

x /

/

C

l = F /x

x

image 1

e

a

b

/

x

/l = F x

e /

image 2

Fig. 12.1. (a) Rays back-projected from imperfectly measured points x, x(cid:1)
eral. (b) The epipolar geometry for x, x(cid:1)
The epipolar line l(cid:1) = Fx is the image of the ray through x, and l = FTx(cid:1)
x(cid:1)
. Since the rays do not intersect, x(cid:1)

are skew in 3-space in gen-
. The measured points do not satisfy the epipolar constraint.
is the image of the ray through

, and x does not lie on l.

does not lie on l(cid:1)

A desirable feature of the method of triangulation used is that it should be invariant
under transformations of the appropriate class for the reconstruction – if the camera
matrices are known only up to an afﬁne (or projective) transformation, then it is clearly
desirable to use an afﬁne (resp. projective) invariant triangulation method to compute
the 3D space points. Thus, denote by τ a triangulation method used to compute a 3D
space point X from a point correspondence x ↔ x(cid:2)
and a pair of camera matrices P
and P(cid:2)

. We write

).

X = τ (x, x(cid:2)

, P, P(cid:2)

The triangulation is said to be invariant under a transformation H if

τ (x, x(cid:2)

, P, P(cid:2)

) =H −1τ (x, x(cid:2)

, PH−1, P(cid:2)H−1).

This means that triangulation using the transformed cameras results in the transformed
point.

It is clear, particularly for projective reconstruction, that it is inappropriate to min-
imize errors in the 3D projective space, IP3. For instance, the method that ﬁnds the
midpoint of the common perpendicular to the two rays in space is not suitable for
projective reconstruction, since concepts such as distance and perpendicularity are not
valid in the context of projective geometry. In fact, in projective reconstruction, this
method will give different results depending on which particular projective reconstruc-
tion is considered – the method is not projective-invariant.

Here we will give a triangulation method that is projective-invariant. The key idea

312

projects as

ˆx = P(cid:23)X ˆx(cid:2)

= P(cid:2)(cid:23)X

12 Structure Computation

is to estimate a 3D point (cid:23)X which exactly satisﬁes the supplied camera geometry, so it
and the aim is to estimate (cid:23)X from the image measurements x and x(cid:2)
the point (cid:23)X which minimizes the reprojection error – the (summed squared) distances
between the projections of (cid:23)X and the measured image points.
which are the projections of (cid:23)X do not depend on
the projective frame in which (cid:23)X is deﬁned, i.e. a different projective reconstruction will
minimized, and the points ˆx and ˆx(cid:2)

. As described
in section 12.3 the maximum likelihood estimate, under Gaussian noise, is given by

Such a triangulation method is projective-invariant because only image distances are

project to the same points.

In the following sections simple linear triangulation methods are given. Then the
MLE is deﬁned, and it is shown that an optimal solution can be obtained via the root of
a sixth-degree polynomial, thus avoiding a non-linear minimization of a cost function.

12.2 Linear triangulation methods

In this section, we describe simple linear triangulation methods. As usual the estimated
point does not exactly satisfy the geometric relations, and is not an optimal estimate.
The linear triangulation method is the direct analogue of the DLT method described
in section 4.1(p88). In each image we have a measurement x = PX, x(cid:2)
= P(cid:2)X, and
these equations can be combined into a form AX = 0, which is an equation linear in X.
First the homogeneous scale factor is eliminated by a cross product to give three
equations for each image point, of which two are linearly independent. For example
for the ﬁrst image, x × (PX) = 0 and writing this out gives
x(p3TX) − (p1TX) = 0
y(p3TX) − (p2TX) = 0
x(p2TX) − y(p1TX) = 0

where piT are the rows of P. These equations are linear in the components of X.

An equation of the form AX = 0 can then be composed, with

 xp3T − p1T

yp3T − p2T
(cid:2)p(cid:2)3T − p(cid:2)1T
x
(cid:2)p(cid:2)3T − p(cid:2)2T
y



A =

where two equations have been included from each image, giving a total of four equa-
tions in four homogeneous unknowns. This is a redundant set of equations, since the
solution is determined only up to scale. Two ways of solving the set of equations of the
form AX = 0 were discussed in section 4.1(p88) and will be considered again here.

Homogeneous method (DLT). The method of section 4.1.1(p90) ﬁnds the solution
as the unit singular vector corresponding to the smallest singular value of A, as shown

12.3 Geometric error cost function

313

in section A5.3(p592). The discussion in section 4.1.1 on the merits of normalization,
and of including two or three equations from each image, applies equally well here.

Inhomogeneous method.
In section 4.1.2(p90) the solution of this system as a set
of inhomogeneous equations is discussed. By setting X = (X, Y, Z, 1)T the set of
homogeneous equations, AX = 0, is reduced to a set of four inhomogeneous equations
in three unknowns. The least-squares solution to these inhomogeneous equations is
described in section A5.1(p588). As explained in section 4.1.2, however, difﬁculties
arise if the true solution X has last coordinate equal or close to 0. In this case, it is not
legitimate to set it to 1 and instabilities can occur.

Discussion. These two methods are quite similar, but in fact have quite different prop-
erties in the presence of noise. The inhomogeneous method assumes that the solution
point X is not at inﬁnity, for otherwise we could not assume that X = (x, y, z, 1)T.
This is a disadvantage of this method when we are seeking to carry out a projective re-
construction, where reconstructed points may lie on the plane at inﬁnity. Furthermore,
neither of these two linear methods is quite suitable for projective reconstruction, since
they are not projective-invariant. To see this, suppose that camera matrices P and P(cid:2)
are replaced by PH−1 and P(cid:2)H−1. One sees that in this case the matrix of equations,
A, becomes AH−1. A point X such that AX =  for the original problem corresponds
to a point HX satisfying (AH−1)(HX) =  for the transformed problem. Thus, there is
a one-to-one correspondence between points X and HX giving the same error. How-
ever, neither the condition (cid:10)X(cid:10) = 1 for the homogeneous method, nor the condition
X = (X, Y, Z, 1)T for the inhomogeneous method, is invariant under application of the
projective transformation H. Thus, in general the point X solving the original problem
will not correspond to a solution HX for the transformed problem.
For afﬁne transformations, on the other hand, the situation is different. In fact, al-
though the condition (cid:10)X(cid:10) = 1 is not preserved under afﬁne transformations, the condi-
tion X = (X, Y, Z, 1)T is preserved, since for an afﬁne transformation, H(X, Y, Z, 1)T =
, 1)T. This means that there is a one-to-one correspondence between a vector
(X
X = (X, Y, Z, 1)T such that A(x, y, z, 1)T =  and the vector HX = (X
, 1)T such
that (AH−1)(X
, 1)T = . The error is the same for corresponding points. Thus,
the points that minimize the error (cid:10)(cid:10) correspond as well. Hence, the inhomogeneous
method is afﬁne-invariant, whereas the homogeneous method is not.

, Y

, Y

, Y

, Z

, Z

(cid:2)

(cid:2)

, Z

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

In the remainder of this chapter we will describe a method for triangulation that is
invariant to the projective frame of the cameras, and minimizes a geometric image error.
This will be the recommended triangulation method. Nevertheless, the homogeneous
linear method described above often provides acceptable results. Furthermore, it has
the virtue that it generalizes easily to triangulation when more than two views of the
point are available.

12.3 Geometric error cost function

A typical observation consists of a noisy point correspondence x ↔ x(cid:2)
which does
not in general satisfy the epipolar constraint. In reality, the correct values of the cor-

314

12 Structure Computation

X

x

x

d

x /
/
d

/x

C

e

e /

/

C

satisfy the epipolar constraint, unlike the measured points

. The corresponding image points ˆx, ˆx(cid:1)

Fig. 12.2. Minimization of geometric error. The estimated 3-space point(cid:23)X projects to the two images

. The point (cid:23)X is chosen so that the reprojection error d2 + d

at ˆx, ˆx(cid:1)
x and x(cid:1)
responding image points should be points ¯x ↔ ¯x(cid:2)
x ↔ x(cid:2)
We seek the points ˆx and ˆx(cid:2)
subject to ˆx(cid:2)TFˆx = 0
(12.1)
mizing the reprojection error for a point (cid:23)X which is mapped to ˆx and ˆx(cid:2)
where d(∗,∗) is the Euclidean distance between the points. This is equivalent to mini-
by projection

and satisfying the epipolar constraint ¯x(cid:2)TF¯x = 0 exactly.

lying close to the measured points

that minimize the function

(cid:1)2 is minimized.

C(x, x(cid:2)

) = d(x, ˆx)2 + d(x(cid:2)

, ˆx(cid:2)

)2

matrices consistent with F, as illustrated in ﬁgure 12.2.
ˆx(cid:2)
spondences. Once ˆx(cid:2)
method, since the corresponding rays will meet precisely in space.

As explained in section 4.3(p102), assuming a Gaussian error distribution, the points
and ˆx are Maximum Likelihood Estimates (MLE) for the true image point corre-

and ˆx are found, the point (cid:23)X may be found by any triangulation

This cost function could, of course, be minimized using a numerical minimization
method such as Levenberg–Marquardt (section A6.2(p600)). A close approximation
to the minimum may also be found using a ﬁrst-order approximation to the geometric
cost function, namely the Sampson error, as described in the next section. However,
in section 12.5 it is shown that the minimum can be obtained non-iteratively by the
solution of a sixth-degree polynomial.

12.4 Sampson approximation (ﬁrst-order geometric correction)

Before deriving the exact polynomial solution we develop the Sampson approxima-
tion, which is valid when the measurement errors are small compared with the mea-
surements. The Sampson approximation to the geometric cost function in the case
of the fundamental matrix has already been discussed in section 11.4.3. Here we are
concerned with computing the correction to the measured points.

The Sampson correction δX to the measured point X = (x, y, x

)T (note, in this
section X does not denote a homogeneous 3-space point) is shown in section 4.2.6(p98)

, y

(cid:2)

(cid:2)

12.5 An optimal solution

315

l = F

/x

x

d

x

/

d

/

x

/x

/l = F

x

θ

(t)

e

e /

(t)

θ/

image 1

of an estimated 3D point (cid:23)X lie on a pair of corresponding epipolar
(cid:23)X is reduced to a one-parameter search for corresponding epipolar lines so as to minimize the squared

Fig. 12.3. The projections ˆx and ˆx(cid:1)
lines. The optimal ˆx and ˆx(cid:1)
will lie at the foot of the perpendiculars from the measured points x and
x(cid:1)
. Parametrizing the corresponding epipolar lines as a one-parameter family, the optimal estimation of

image 2

sum of perpendicular distances d2 + d

(cid:1)2.

to be (4.11–p99)

δX = −JT(JJT)

−1

and the corrected point is (cid:23)X = X + δX = X − JT(JJT)

−1.

As shown in section 11.4.3 in the case of the variety deﬁned by x(cid:2)TFx = 0, the error
 = x(cid:2)TFx, and the Jacobian is

J = ∂/∂x = [(FTx(cid:2)
)1, (FTx(cid:2)
(cid:2)
+f31, etc. Then the ﬁrst-order approximation
+f21y

)2, (Fx)1, (Fx)2]

)1 = f11x

(cid:2)

where for instance (FTx(cid:2)
to the corrected point is simply

 ˆx

ˆy
(cid:2)
ˆx
(cid:2)
ˆy

 =

 x

y
(cid:2)
x
(cid:2)
y

 −

x(cid:2)TFx
2 + (FTx(cid:2))2

1 + (FTx(cid:2))2

2

(Fx)2

1 + (Fx)2

 .

 (FTx(cid:2)

(FTx(cid:2)
(Fx)1
(Fx)2

)1
)2

The approximation is accurate if the correction in each image is small (less than
a pixel), and is cheap to compute. Note, however, that the corrected points will not
satisfy the epipolar relation ˆx(cid:2)TFˆx = 0 exactly. The method of the following section
computes the points ˆx, ˆx(cid:2)
which do exactly satisfy the epipolar constraint, but is more
costly.

12.5 An optimal solution

In this section, we describe a method of triangulation that ﬁnds the global minimum of
the cost function (12.1) using a non-iterative algorithm. If the Gaussian noise model
can be assumed to be correct, this triangulation method is then provably optimal.

12.5.1 Reformulation of the minimization problem
Given a measured correspondence x ↔ x(cid:2)
and ˆx that mini-
mize the sum of squared distances (12.1) subject to the epipolar constraint ˆx(cid:2)TFˆx = 0.
The following discussion relates to ﬁgure 12.3. Any pair of points satisfying the

, we seek a pair of points ˆx(cid:2)

316

12 Structure Computation

epipolar constraint must lie on a pair of corresponding epipolar lines in the two images.
Thus, in particular, the optimum point ˆx lies on an epipolar line l and ˆx(cid:2)
lies on the
corresponding epipolar line l(cid:2)
. On the other hand, any other pair of points lying on the
lines l and l(cid:2)
will also satisfy the epipolar constraint. This is true in particular for the
point x⊥ on l lying closest to the measured point x, and the correspondingly deﬁned
. Of all pairs of points on the lines l and l(cid:2)
⊥ on l(cid:2)
point x(cid:2)
⊥ minimize
the squared distance sum of (12.1). It follows that ˆx(cid:2)
⊥ and ˆx = x⊥, where x⊥ and
⊥ are deﬁned with respect to a pair of matching epipolar lines l and l(cid:2)
x(cid:2)
. Consequently,
we may write d(x, ˆx) =d(x , l), where d(x, l) represents the perpendicular distance
from the point x to the line l. A similar expression holds for d(x(cid:2)

, the points x⊥ and x(cid:2)
= x(cid:2)

, ˆx(cid:2)

).

In view of the previous paragraph, we may formulate the minimization problem dif-

ferently as follows. We seek to minimize

d(x, l)2 + d(x(cid:2)

, l(cid:2)

)2

(12.2)

where l and l(cid:2)
then the closest point on the line l to the point x and the point ˆx(cid:2)

range over all choices of corresponding epipolar lines. The point ˆx is

is similarly deﬁned.

Our strategy for minimizing (12.2) is as follows:

an epipolar line in the ﬁrst image may be written as l(t).

(i) Parametrize the pencil of epipolar lines in the ﬁrst image by a parameter t. Thus
(ii) Using the fundamental matrix F, compute the corresponding epipolar line l(cid:2)
(iii) Express the distance function d(x, l(t))2 + d(x(cid:2)

(t))2 explicitly as a function

in the second image.

, l(cid:2)

(t)

of t.

(iv) Find the value of t that minimizes this function.

In this way, the problem is reduced to that of ﬁnding the minimum of a function of a

single variable t, i.e.

min(cid:23)X

C = d(x, ˆx)2 + d(x(cid:2)

, ˆx(cid:2)

)2 = min

t

C = d(x, l(t))2 + d(x(cid:2)

, l(cid:2)

(t))2.

It will be seen that for a suitable parametrization of the pencil of epipolar lines the
distance function is a rational polynomial function of t. Using techniques of elementary
calculus, the minimization problem reduces to ﬁnding the real roots of a polynomial of
degree 6.

12.5.2 Details of the minimization
If both of the image points correspond with the epipoles, then the point in space lies
on the line joining the camera centres. In this case it is impossible to determine the
position of the point in space. If only one of the corresponding points lies at an epipole,
then we conclude that the point in space must coincide with the other camera centre.
Consequently, we assume that neither of the two image points x and x(cid:2)
corresponds
with an epipole.
image in order to place both points x and x(cid:2)

In this case, we may simplify the analysis by applying a rigid transformation to each
at the origin, (0, 0, 1)T in homogeneous

(cid:2)

12.5 An optimal solution

317
coordinates. Furthermore, the epipoles may be placed on the x-axis at points (1, 0, f )T
)T respectively. A value f equal to 0 means that the epipole is at inﬁn-
and (1, 0, f
ity. Applying these two rigid transforms has no effect on the sum-of-squares distance
function in (12.1), and hence does not change the minimization problem.
Thus, in future we assume that in homogeneous coordinates, x = x(cid:2)

and that the two epipoles are at points (1, 0, f )T and (1, 0, f
F(1, 0, f )T = (1, 0, f

)F = 0, the fundamental matrix has a special form

(cid:2)

= (0, 0, 1)T
)T. In this case, since

(cid:2)

 f f

(cid:2)
−f b
−f d

d −f

(cid:2)

c −f
a
c

(cid:2)

d
b
d

 .

F =

(12.3)

Consider an epipolar line in the ﬁrst image passing through the point (0, t,1) T (still in
homogeneous coordinates) and the epipole (1, 0, f )T. We denote this epipolar line by
l(t). The vector representing this line is given by the cross product (0, t,1) ×(1, 0, f ) =
(tf, 1,−t), so the squared distance from the line to the origin is

d(x, l(t))2 =

t2

1 + (tf )2 .

Using the fundamental matrix to ﬁnd the corresponding epipolar line in the other image,
we see that

l(cid:2)

(t) =F (0, t,1) T = (−f

(cid:2)

(ct + d), at + b, ct + d)T.

(12.4)

This is the representation of the line l(cid:2)
tance of this line from the origin is equal to

(t) as a homogeneous vector. The squared dis-

d(x(cid:2)

, l(cid:2)

(t))2 =

(ct + d)2

(at + b)2 + f(cid:2)2(ct + d)2 .

The total squared distance is therefore given by

s(t) =

t2

1 + f 2t2 +

(ct + d)2

(at + b)2 + f(cid:2)2(ct + d)2 .

(12.5)

Our task is to ﬁnd the minimum of this function.

We may ﬁnd the minimum using techniques of elementary calculus, as follows. We

compute the derivative

(cid:2)

s

(t) =

2t

(1 + f 2t2)2

− 2(ad − bc)(at + b)(ct + d)
((at + b)2 + f(cid:2)2(ct + d)2)2 .

(12.6)

(cid:2)

Maxima and minima of s(t) will occur when s
(cid:2)
s

(t) over a common denominator and equating the numerator to 0 gives a condition

(t) = 0. Collecting the two terms in

g(t) =t((at + b)2 + f

(cid:2)2(ct + d)2)2

−(ad − bc)(1 + f 2t2)2(at + b)(ct + d)

= 0.

(12.7)

The minima and maxima of s(t) will occur at the roots of this polynomial. This is a

318

12 Structure Computation

Objective
Given a measured point correspondence x ↔ x(cid:1)
corrected correspondences ˆx ↔ ˆx(cid:1)
epipolar constraint ˆx(cid:1)TFˆx = 0.
Algorithm
(cid:17)

(i) Deﬁne transformation matrices

(cid:18)

, and a fundamental matrix F, compute the
that minimize the geometric error (12.1) subject to the

(cid:17)

(cid:17)

(cid:18)

.

(cid:1)

(cid:18)

1

1

−x
(cid:1)
1 −y
(cid:1)
1
These are the translations that take x = (x, y, 1)T and x(cid:1) = (x
(cid:1)

−x
1 −y
1

and T(cid:1)

T =

=

1

, y
(ii) Replace F by T(cid:1)−TFT−1. The new F corresponds to translated coordinates.
(iii) Compute the right and left epipoles e = (e1, e2, e3)T and e(cid:1) = (e
(cid:1)
(cid:1)
3)T such that
2, e
1 + e2
2 = 1 and do

, 1)T to the origin.
(cid:1)
1, e
e(cid:1)TF = 0 and Fe = 0. Normalize (multiply by a scale) e such that e2
the same to e(cid:1)
.
(iv) Form matrices

(cid:17)

(cid:18)

R =

e1
−e2

e2
e1

1

and R(cid:1)

=

(cid:1)
e
−e
(cid:1)
1
2

(cid:1)
(cid:1)
2
1

e
e

and observe that R and R(cid:1)
R(cid:1)e(cid:1) = (1, 0, e

(cid:1)
3)T.
(cid:1) = e

(v) Replace F by R(cid:1)FRT. The resulting F must have the form (12.3).
(cid:1)
(vi) Set f = e3, f
3, a = F22, b = F23, c = F32 and d = F33.
(vii) Form the polynomial g(t) as a polynomial in t according to (12.7). Solve for t to get 6

are rotation matrices, and Re = (1, 0, e3)T and

roots.

(ix) Evaluate the two lines l = (tf, 1,−t) and l(cid:1)

(viii) Evaluate the cost function (12.5) at the real part of each of the roots of g(t) (alterna-
tively evaluate at only the real roots of g(t)). Also, ﬁnd the asymptotic value of (12.1)
for t = ∞, namely 1/f 2 + c2/(a2 + f
(cid:1)2c2). Select the value tmin of t that gives the
smallest value of the cost function.
given by (12.4) at tmin and ﬁnd ˆx and ˆx(cid:1)
as the closest points on these lines to the origin. For a general line (λ, µ, ν), the formula
for the closest point on the line to the origin is (−λν,−µν, λ2 + µ2).
(xi) The 3-space point(cid:23)X may then be obtained by the homogeneous method of section 12.2.
T(cid:1)−1R(cid:1)Tˆx(cid:1)

(x) Transfer back to the original coordinates by replacing ˆx by T−1RTˆx and ˆx(cid:1)

by

.

Algorithm 12.1. The optimal triangulation method.

polynomial of degree 6, which may have up to 6 real roots, corresponding to 3 minima
and 3 maxima of the function s(t). The absolute minimum of the function s(t) may be
found by ﬁnding the roots of g(t) and evaluating the function s(t) given by (12.5) at
each of the real roots. More simply, one checks the value of s(t) at the real part of each
root (complex or real) of g(t), which saves the trouble of determining if a root is real
or complex. One should also check the asymptotic value of s(t) as t → ∞ to see if the
minimum distance occurs when t = ∞, corresponding to an epipolar line f x = 1 in
the ﬁrst image.

The overall method is summarized in algorithm 12.1.

12.5 An optimal solution

319

1.6

1.4

1.2

-1.5

-1

-0.5

0.5

1

1.5

0.8

a

1.2

1

0.8

0.6

0.4

0.2

-1.5

-1

-0.5

0.5

1

1.5

b

Fig. 12.4. (a) Example of a cost function with three minima. (b) This is the cost function for a perfect
point match, which nevertheless has two minima.

12.5.3 Local minima
The fact that g(t) in (12.7) has degree 6 means that s(t) may have as many as three
minima. In fact, this is indeed possible, as the following case shows. Setting f = f
=
1 and

(cid:2)

 4 −3 −4



2
3

3
4

−3
−4

F =

gives a function

s(t) =

t2

1 + t2 +

(3t + 4)2

(2t + 3)2 + (3t + 4)2

with graph as shown in ﬁgure 12.4a1. The three minima are clearly shown.

As a second example, we consider the case where f = f

= 1, and

(cid:2)

 0 −1

1
0

0
2 −1
0
1

 .

F =

In this case, the function s(t) is given by

s(t) =

t2

t2 + 1

+

t2

t2 + (2t − 1)2

and both terms of the cost function vanish for a value of t = 0, which means that
the corresponding points x and x(cid:2)
exactly satisfy the epipolar constraint. This can
be veriﬁed by observing that x(cid:2)TFx = 0. Thus the two points are exactly matched.
A graph of the cost function s(t) is shown in ﬁgure 12.4b. Apart from the absolute
minimum at t = 0 there is also a local minimum at t = 1. Thus, even in the case of
perfect matches local minima may occur. This example shows that an algorithm that
attempts to minimize the cost function in (12.1), or equivalently (12.2), by an iterative
1 In this graph and also ﬁgure 12.4b we make the substitution t = tan(θ) and plot for θ in the range −π/2 ≤ θ ≤ π/2, so as

to show the whole inﬁnite range for t.

320

12 Structure Computation

search beginning from an arbitrary initial point is in danger of ﬁnding a local minimum,
even in the case of perfect point matches.

12.5.4 Evaluation on real images

An experiment was carried out using the calibration cube images shown in ﬁgure 11.2-
(p289) with the goal of determining how the triangulation method effects the accuracy
of reconstruction. A Euclidean model of the cubes, to be used as ground truth, was es-
timated and reﬁned using accurate image measurements. The measured pixel locations
were corrected to correspond exactly to the Euclidean model, requiring coordinate cor-
rections averaging 0.02 pixels.

At this stage we had a model and a set of matched points corresponding exactly to the
model. Next, a projective reconstruction of the points was computed and a projective
transformation H computed that brought the projective reconstruction into agreement
with the Euclidean model. Controlled zero-mean Gaussian noise was introduced into
the point coordinates, and triangulation using two methods was carried out in the pro-
jective frame, the transformation H applied, and the error of each method was measured
in the Euclidean frame. Figure 12.5 shows the results of this experiment for the two tri-
angulation methods. The graph shows the average reconstruction error over all points
in 10 separate runs at each chosen noise level. It clearly shows that the optimal method
gives superior reconstruction results.

In this pair of images the two epipoles are distant from the image. For cases where
the epipoles are close to the images, results on synthetic images show that the advantage
of the polynomial method will be more pronounced.

r
o
r
r
E
 
n
o
i
t
c
u
r
t
s
n
o
c
e
R

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1

0.15

Noise

Fig. 12.5. Reconstruction error comparison of triangulation methods. The graph shows the recon-
struction error obtained using two triangulation methods: (i) selection of the midpoint of the common
perpendicular to the rays in the projective frame (top curve), and (ii) the optimal polynomial method
(lower curve). On the horizontal axis is the noise, on the vertical axis the reconstruction error. The units
for reconstruction error are relative to a unit distance equal to the side of one of the dark squares in the
calibration cube image ﬁgure 11.2(p289). Even for the best method the error is large for higher noise
levels, because there is little movement between the images.

12.6 Probability distribution of the estimated 3D point

321

Fig. 12.6. Uncertainty of reconstruction. The shaded region in each case illustrates the shape of the
uncertainty region, which depends on the angle between the rays. Points are less precisely localized
along the ray as the rays become more parallel. Forward motion in particular can give poor reconstruc-
tions since rays are almost parallel for much of the ﬁeld of view.

12.6 Probability distribution of the estimated 3D point

An illustration of the distribution of the reconstructed point is given in ﬁgure 12.6.
A good rule of thumb is that the angle between the rays determines the accuracy of
reconstruction. This is a better guide than simply considering the baseline, which is the
more commonly used measure.

More formally the probability of a particular 3D point X depends on the probability
of obtaining its image in each view. We will consider a simpliﬁed example where the
objective is to estimate the probability that a point on a plane has position X = (X, Y)T
(X) in two line cameras. (The projections f and
given its images x = f (X) and x
(cid:2)
2×3 respectively –
f
see section 6.4.2(p175)). The imaging geometry is shown in ﬁgure 12.7(a).

are expressible in terms of 2 × 3 projection matrices P2×3 and P(cid:2)

= f

(cid:2)

(cid:2)

Suppose that the measured image point is at x in the ﬁrst image, and the measure-
ment process is corrupted by Gaussian noise with mean zero and variance σ2, then the
probability of obtaining x, given that the true image point is f (X), is given by

p(x|X) = (2πσ2)

−1/2 exp

(cid:15)
−|f (X) − x|2/(2σ2)

(cid:16)

.

with a similar expression for p(x
p(X|x, x

(cid:2)|X). We wish to compute the a posteriori distribution:
(cid:2)
) = p(x, x

(cid:2)|X)p(X)/p(x, x

).

(cid:2)

Assuming a uniform prior probability p(X), and independent image measurements in
the two images, it follows that
p(X|x, x

(cid:2)|X) = p(x|X)p(x

) ∼ p(x, x

(cid:2)|X).

(cid:2)

Figure 12.7 shows an example of this Probability Density Function (PDF). The bias
and variance of this example is discussed in appendix 3(p568).

12.7 Line reconstruction

Suppose a line in 3-space is projected to lines in two views as l and l(cid:2)
. The line in
3-space can be reconstructed by back-projecting each line to give a plane in 3-space,
and intersecting the planes.

12 Structure Computation

322

1C

C 2

a

b

c

Fig. 12.7. PDF for a triangulated point. (a) The camera conﬁguration. There are two line cameras
with centres at C1 and C2. The image lines are the left and lower edge of the square. The bar indicates
the 2σ range of the noise. The plots show the PDF for a triangulated point computed from the two
perspective projections. A large noise variance σ2 is chosen to emphasize the effect. (b) The PDF shown
as an image with white representing a higher value. (b) A contour plot of the PDF. Note that the PDF is
not a Gaussian.

π

l

e

C

L

π /

l /

/

e

/

C

epipolar
plane

Fig. 12.8. Line reconstruction. The image lines l, l(cid:1)
respectively. The
plane intersection determines the line L in 3-space. If the line in 3-space lies on an epipolar plane then
its position in 3-space cannot be determined from its images. In this case the epipoles lie on the image
lines.

back-project to planes π, π(cid:1)

The planes deﬁned by the lines are π = PTl and π(cid:2)
. It is often quite con-
venient in practice to parametrize the line in 3-space by the two planes deﬁned by the
image lines, i.e. to represent the line as the 2 × 4 matrix

= P(cid:2)Tl(cid:2)

(cid:17)

(cid:18)

L =

lTP
l(cid:2)TP(cid:2)

as described in the span representation of section 3.2.2(p68). Then for example a point
X lies on the line if LX = 0.

In the case of corresponding points the pre-image (i.e. the point in 3-space that
projects to the image points) is over-determined since there are four measurements

12.8 Closure

323

on the three degrees of freedom of the 3-space point. In contrast in the case of lines
the pre-image is exactly determined because a line in 3-space has four degrees of free-
dom, and the image line provides two measurements in each view. Note, here we are
considering the lines as inﬁnite, and not using their endpoints.

Degeneracy. As illustrated in ﬁgure 12.8 lines in 3-space lying on epipolar planes
cannot be determined from their images in two views. Such lines intersect the cam-
era baseline. In practice, when there is measurement error, lines which are close to
intersecting the baseline can be poorly localized in a reconstruction.

The degeneracy for lines is far more severe than for points: in the case of points
there is a one-parameter family of points on the baseline which cannot be recovered.
For lines there is a three-parameter family: one parameter for position on the baseline,
and the other two for the star of lines through each point on the baseline.

Intersection of more than two planes
In later chapters (particularly chapter 15) we will be considering reconstruction from
three or more views. To reconstruct the line that results from the intersection of several
planes it is appropriate to proceed as follows. Represent each plane πi by a 4-vector
and form an n × 4 matrix A for n planes with rows πT
i . Let A = UDVT be the singular
value decomposition. The two columns of V corresponding to the two largest singular
values span the best rank 2 approximation to A and may be used to deﬁne the line of
intersection of the planes. If the planes are deﬁned by back-projecting image lines,
then the Maximum Likelihood estimate of the line L in 3-space is found by minimizing
a geometric image distance between L projected into each image and the measured line
in that image. This is discussed in section 16.4.1(p396).

12.8 Closure

It is not evident how to extend the polynomial method of triangulation to 3 or more
views. However, the linear method extends in an obvious manner. More interestingly,
the Sampson method also may be extended to 3 or more views, as is described in
[Torr-97]. The disadvantage is that the computational cost (and also coding effort)
increases noticeably with more views.

12.8.1 The literature
The optimal triangulation method was given by Hartley & Sturm [Hartley-97b].

12.8.2 Notes and exercises

(i) Derive a method for triangulation in the case of pure translational motion of the
cameras. Hint, see ﬁgure 12.9. A closed form solution for the parameter θ is
possible. This method was used in [Armstrong-94].

(ii) Adapt the polynomial triangulation method to a pair of afﬁne cameras (or more
generally, to cameras with the same principal plane). In this case, the funda-
mental matrix has a simple form, (14.1–p345), and the method reduces to a
linear algorithm.

324

12 Structure Computation

/

d

x /

/x

x

d

x

images 1 & 2

e = e

/

θ

Fig. 12.9. The epipolar geometry for pure translation. In this case, corresponding epipolar lines are
(cid:1)2 can
identical (see section 11.7.1(p293)). The epipolar line (parametrized by θ) that minimizes d2 + d
be computed directly.

(iii) Show that the Sampson method (section 12.4) is invariant under Euclidean co-

(iv) Derive the analogue of the polynomial solution for triangulation in the case of
, compute

ordinate changes in the images (and the corresponding change in F).
a planar homography, i.e. given a measured correspondence x ↔ x(cid:2)
points ˆx and ˆx(cid:2)

that minimize the function
, ˆx(cid:2)

) = d(x, ˆx)2 + d(x(cid:2)

C(x, x(cid:2)

subject to ˆx(cid:2)

)2

= Hˆx.

See [Sturm-97b], where it is shown that the solution is a degree 8 polynomial
in one variable.

13

Scene planes and homographies

This chapter describes the projective geometry of two cameras and a world plane.

Images of points on a plane are related to corresponding image points in a second
view by a (planar) homography as shown in ﬁgure 13.1. This is a projective relation
since it depends only on the intersections of planes with lines. It is said that the plane
induces a homography between the views. The homography map transfers points from
one view to the other as if they were images of points on the plane.

There are then two relations between the two views: ﬁrst, through the epipolar ge-
ometry a point in one view determines a line in the other which is the image of the ray
through that point; and second, through the homography a point in one view determines
a point in the other which is the image of the intersection of the ray with a plane. This
chapter ties together these two relations of 2-view geometry.

Two other important notions are described here: the parallax with respect to a plane,

and the inﬁnite homography.

π

x

π

H

/

x

/

C

x

C

Fig. 13.1. The homography induced by a plane. The ray corresponding to a point x is extended to
meet the plane π in a point xπ; this point is projected to a point x(cid:1)
in the other image. The map from
x to x(cid:1)
is the homography induced by the plane π. There is a perspectivity, x = H1πxπ, between the
world plane π and the ﬁrst image plane; and a perspectivity, x(cid:1) = H2πxπ, between the world plane and
second image plane. The composition of the two perspectivities is a homography, x(cid:1) = H2πH−1
1π x = Hx,
between the image planes.

325

326

13 Scene planes and homographies

13.1 Homographies given the plane and vice versa

We start by showing that for planes in general position the homography is determined
uniquely by the plane and vice versa. General position in this case means that the plane
does not contain either of the camera centres. If the plane does contain one of the
camera centres then the induced homography is degenerate.

Suppose a plane π in 3-space is speciﬁed by its coordinates in the world frame. We

ﬁrst derive an explicit expression for the induced homography.

Result 13.1. Given the projection matrices for the two views

P = [I | 0]

P(cid:2)

= [A | a]

and a plane deﬁned by πTX = 0 with π = (vT, 1)T, then the homography induced by
the plane is x(cid:2)

= Hx with

H = A − avT.

(13.1)

We may assume that π4 = 1 since the plane does not pass through the centre of the ﬁrst
camera at (0, 0, 0, 1)T.

Note, there is a three-parameter family of planes in 3-space, and correspondingly a
three-parameter family of homographies between two views induced by planes in 3-
space. These three parameters are speciﬁed by the elements of the vector v, which is
not a homogeneous 3-vector.

Proof. To compute H we back-project a point x in the ﬁrst view and determine the
intersection point X of this ray with the plane π. The 3D point X is then projected into
the second view.
For the ﬁrst view x = PX = [I | 0]X and so any point on the ray X = (xT, ρ)T projects
to x, where ρ parametrizes the point on the ray. Since the 3D point X is on π it satisﬁes
πTX = 0. This determines ρ, and X = (xT,−vTx)T. The 3D point X projects into the
second view as

x(cid:2)

(cid:15)
= P(cid:2)X = [A | a]X
A−avT
= Ax − avTx =

(cid:16)

x

as required.

Example 13.2. A calibrated stereo rig.

Suppose the camera matrices are those of a calibrated stereo rig with the world origin

at the ﬁrst camera

PE = K[I | 0]

P(cid:2)
E = K(cid:2)

[R | t],

and the world plane πE has coordinates πE = (nT, d)T so that for points on the plane

nT(cid:22)X + d = 0. We wish to compute an expression for the homography induced by the

plane.

13.1 Homographies given the plane and vice versa

327

From result 13.1, with v = n/d, the homography for the cameras
P = [I | 0], P(cid:2)

= [R | t] is

Applying the transformations K and K(cid:2)
K[I | 0], P(cid:2)

[R | t] and the resulting induced homography is

E = K(cid:2)

to the images we obtain the cameras PE =

K−1.

(13.2)

H = R − tnT/d.
(cid:16)

R − tnT/d

H = K(cid:2)(cid:15)

This is a three-parameter family of homographies, parametrized by n/d. It is deﬁned
(cid:2)
by the plane, and the camera internal and relative external parameters.

13.1.1 Homographies compatible with epipolar geometry
Suppose four points Xi are chosen on a scene plane. Then the correspondence xi ↔ x(cid:2)
i
of their images between two views deﬁnes a homography H, which is the homography
induced by the plane. These image correspondences also obey the epipolar constraint,
i.e. x(cid:2)
TFxi = 0, since they arise from images of scene points. Indeed, the correspon-
dence x ↔ x(cid:2)
are
images of a scene point, in this case the point given by intersecting the scene plane
with the ray back-projected from x. The homography H is said to be consistent or
compatible with F.

= Hx obeys the epipolar constraint for any x, since again x and x(cid:2)

i

Now suppose four arbitrary image points are chosen in the ﬁrst view and four ar-
bitrary image points chosen in the second. Then a homography ˜H may be computed
which maps one set of points into the other (provided no three are collinear in either
view). However, correspondences x ↔ x(cid:2)
= ˜Hx may not obey the epipolar constraint.
If the correspondence x ↔ x(cid:2)
= ˜Hx does not obey the epipolar constraint then there
does not exist a scene plane which induces ˜H.

The epipolar geometry determines the projective geometry between two views, and
can be used to deﬁne conditions on homographies which are induced by actual scene
planes. Figure 13.2 illustrates several relations between epipolar geometry and scene
planes which can be used to deﬁne such conditions. For example, since correspon-
dences x ↔ Hx obey the epipolar constraint if H is induced by a plane, then from
x(cid:2)TFx = 0

(Hx)TFx = xTHTFx = 0.

This is true for all x, so:
• A homography H is compatible with a fundamental matrix F if and only if the matrix
HTF is skew-symmetric:

HTF + FTH = 0

(13.3)

The argument above showed that the condition was necessary. The fact that this is a
sufﬁcient condition was shown by Luong and Vi´eville [Luong-96]. Counting degrees
of freedom, (13.3) places six homogeneous (ﬁve inhomogeneous) constraints on the 8
degrees of freedom of H. There are therefore 8 − 5 = 3 degrees of freedom remaining

328

13 Scene planes and homographies

π

baseline

e

a

e /

l

e

π

x

e

π

l

e

l /
e

e

/

e

b

l /
e

/

x

Hx
e /

c

Fig. 13.2. Compatibility constraints. The homography induced by a plane is coupled to the epipolar
geometry and satisﬁes constraints. (a) The epipole is mapped by the homography, as e(cid:1) = He, since
the epipoles are images of the point on the plane where the baseline intersects π. (b) Epipolar lines
are mapped by the homography as HTl(cid:1)
e = le. (c) Any point x mapped by the homography lies on its
corresponding epipolar line l(cid:1)

e = Fx = x(cid:1) × (Hx).

e, sol (cid:1)

for H; these 3 degrees of freedom correspond to the three-parameter family of planes in
3-space.

The compatibility constraint (13.3) is an implicit equation in H and F. We now de-
velop an explicit expression for a homography H induced by a plane given F which is
more suitable for a computational algorithm.

Result 13.3. Given the fundamental matrix F between two views, the three-parameter
family of homographies induced by a world plane is
H = A − e(cid:2)vT

(13.4)

where [e(cid:2)

]×A = F is any decomposition of the fundamental matrix.

Proof. Result 13.1 has shown that given the camera matrices for the view pair P =
= [A | a] a plane π induces a homography H = A− avT where π = (vT, 1)T.
[I | 0], P(cid:2)
However, according to result 9.9(p254), for the fundamental matrix F = [e(cid:2)
]×A one can
choose the two cameras to be [I | 0] and [A | e(cid:2)

].

Remark.
The above derivation, which is based on the projection of points on a
plane, ensures that the homographies are compatible with the epipolar geometry. Al-
gebraically, the homography (13.4) is compatible with the fundamental matrix since it
obeys the necessary and sufﬁcient condition (13.3) that FTH is skew-symmetric. This

13.2 Plane induced homographies given F and image correspondences

329

(cid:15)
A − e(cid:2)vT

(cid:16)

= AT[e(cid:2)

]×A

follows from

FTH = AT[e(cid:2)

]×

using [e(cid:2)

]×e(cid:2)

= 0, since AT[e(cid:2)

]×A is skew-symmetric.

Comparing (13.4) with the general decomposition of the fundamental matrix, as
given in lemma 9.11(p255) or (9.10–p256) it is evident that they involve an iden-
tical formula (except for signs).
In fact there is a one-to-one correspondence be-
tween decompositions of the fundamental matrix (up to the scale factor ambiguity k
in lemma 9.11) and homographies induced by world planes, as stated here.

Corollary 13.4. A transformation H is the homography between two images induced by
some world plane if and only if the fundamental matrix F for the two images has a
decomposition F = [e(cid:2)
This choice in the decomposition simply corresponds to the choice of projective world
In fact, H is the transformation with respect to the plane with coordinates
frame.
(0, 0, 0, 1)T in the reconstruction with P = [I | 0] and P(cid:2)
Finding the plane that induces a given homography is a simple matter given a pair of

= [H | e(cid:2)

]×H.

].

camera matrices, as follows.
Result 13.5. Given the cameras in the canonical form P = [I | 0], P(cid:2)
= [A | a], then
the plane π that induces a given homography H between the views has coordinates
π = (vT, 1)T where v may be obtained linearly by solving the equations λH = A−avT,
which are linear in the entries of v and λ.

Note, these equations have an exact solution only if H satisﬁes the compatibility con-
straint (13.3) with F. For a homography computed numerically from noisy data this
will not normally be true, and the linear system is over-determined.

13.2 Plane induced homographies given F and image correspondences

A plane in 3-space can be speciﬁed by three points, or by a line and a point, and so forth.
In turn these 3D elements can be speciﬁed by image correspondences. In section 13.1
the homography was computed from the coordinates of the plane. In the following the
homography will be computed directly from the corresponding image elements that
specify the plane. This is a quite natural mechanism to use in applications.

We will consider two cases: (i) three points; (ii) a line and a point. In each case the
corresponding elements are sufﬁcient to determine a plane in 3-space uniquely. It will
be seen that in each case:

(i) The corresponding image entities have to satisfy consistency constraints with

the epipolar geometry.

(ii) There are degenerate conﬁgurations of the 3D elements and cameras for which
the homography is not deﬁned. Such degeneracies arise from collinearities and
coplanarities of the 3D elements and the epipolar geometry. There may also be
degeneracies of the solution method, but these can be avoided.

The three-point case is covered in more detail.

330

13 Scene planes and homographies

l

e

π

X
1

X

3

X

2

e

/

e

/
e

l

Fig. 13.3. Degenerate geometry for an implicit computation of the homography. The line deﬁned
by the points X1 and X2 lies in an epipolar plane, and thus intersects the baseline. The images of X1
and X2 are collinear with the epipole, and H cannot be computed uniquely from the correspondences
xi ↔ x(cid:1)

. This conﬁguration is not degenerate for the explicit method.

i, i ∈ {1, . . . , 3}, e ↔ e(cid:1)

13.2.1 Three points
We suppose that we have the images in two views of three (non-collinear) points Xi,
and the fundamental matrix F. The homography H induced by the plane of the points
may be computed in principle in two ways:

First,

= He, from which H may be computed.

the position of the points Xi is recovered in a projective reconstruction
(chapter 12). Then the plane π through the points is determined (3.3–p66), and the
homography computed from the plane as in result 13.1. Second, the homography may
be computed from four corresponding points, the four points in this case being the
images of the three points Xi on the plane together with the epipole in each view.
The epipole may be used as the fourth point since it is mapped between the views
by the homography as shown in ﬁgure 13.2. Thus we have four correspondences,
i = Hxi, i ∈ {1, . . . ,3}, e(cid:2)
x(cid:2)
We thus have two alternative methods to compute H from three point correspon-
dences, the ﬁrst involving an explicit reconstruction, the second an implicit one where
the epipole provides a point correspondence. It is natural to ask if one has an advantage
over the other, and the answer is that the implicit method should not be used for com-
putation as it has signiﬁcant degeneracies which are not present in the explicit method.
Consider the case when two of the image points are collinear with the epipole (we
assume for the moment that the measurements are noise-free). A homography H cannot
be computed from four correspondences if three of the points are collinear (see section
4.1.3(p91)), so the implicit method fails in this case. Similarly if the image points are
close to collinear with the epipole then the implicit method will give a poorly con-
ditioned estimate for H. The explicit method has no problems when two points are
collinear or close to collinear with the epipole – the corresponding image points deﬁne
points in 3-space (the world points are on the same epipolar plane, but this is not a
degenerate situation) and the plane π and hence homography can be computed. The
conﬁguration is illustrated in ﬁgure 13.3.

We now develop the algebra of the explicit method in more detail. It is not neces-

13.2 Plane induced homographies given F and image correspondences

331
sary to actually determine the coordinates of the points Xi, all that is important is the
constraint they place on the three-parameter family of homographies compatible with
F (13.4), H = A − e(cid:2)vT, parametrized by v. The problem is reduced to that of solving
for v from the three point correspondences. The solution may be obtained as:
Result 13.6. Given F and the three image point correspondences xi ↔ x(cid:2)
raphy induced by the plane of the 3D points is
H = A − e(cid:2)

i, the homog-

where A = [e(cid:2)

(M−1b)T,
]×F and b is a 3-vector with components
)/(cid:10)x(cid:2)

× (Axi))T(x(cid:2)

bi = (x(cid:2)

× e(cid:2)

i

i

× e(cid:2)(cid:10)2,

i

and M is a 3 × 3 matrix with rows xT
i .
Proof. According to result 9.14(p256), F may be decomposed as F = [e(cid:2)
(13.4) gives H = A − e(cid:2)vT, and each correspondence xi ↔ x(cid:2)
constraint on v as

]×A. Then
i generates a linear

From (13.5) the vectors x(cid:2)
zero:
× (Axi − e(cid:2)

x(cid:2)

i

i = Hxi = Axi − e(cid:2)
x(cid:2)
i and Axi − e(cid:2)
(vTxi)) = (x(cid:2)

i

Forming the scalar product with the vector x(cid:2)

(13.5)

i = 1, . . . ,3 .

(vTxi),
(vTxi) are parallel, so their vector product is
× Axi) − (x(cid:2)
gives
× e(cid:2)
× e(cid:2))

)(vTxi) =0 .

× e(cid:2)

(13.6)

= bi

)

i

× e(cid:2)
× (Axi))T(x(cid:2)
(x(cid:2)
× e(cid:2))T(x(cid:2)
(x(cid:2)

i

i

i

i

i

xT
i

v =

which is linear in v. Note, the equation is independent of the scale of x(cid:2)
occurs
the same number of times in the numerator and denominator. Each correspondence
generates an equation xT
i

v = bi, and collecting these together we have Mv = b.

, since x(cid:2)

Note, a solution cannot be obtained if MT = [x1, x2, x3] is not of full rank. Alge-
braically, det M = 0 if the three image points xi are collinear. Geometrically, three
collinear image points arise from collinear world points, or coplanar world points
where the plane contains the ﬁrst camera centre. In either case a full rank homography
is not deﬁned.

Consistency conditions. Equation (13.5) is equivalent to six constraints since each
point correspondence places two constraints on a homography. Determining v requires
only three constraints, so there are three constraints remaining which must be satisﬁed
for a valid solution. These constraints are obtained by taking the cross product of (13.5)
with e(cid:2)

, which gives

e(cid:2) × x(cid:2)

i = e(cid:2) × Axi = Fxi.

332

13 Scene planes and homographies

Objective
Given F and three point correspondences xi ↔ x(cid:1)
determine the homography x(cid:1) = Hx induced by the plane of Xi.
Algorithm

i which are the images of 3D points Xi,

(i) For each correspondence xi ↔ x(cid:1)

i compute the corrected correspondence ˆxi ↔ ˆx(cid:1)

i

using algorithm 12.1(p318).

(ii) Choose A = [e(cid:1)]×F and solve linearly for v from Mv = b as in result 13.6.
(iii) Then H = A − e(cid:1)vT.

Algorithm 13.1. The optimal estimate of the homography induced by a plane deﬁned by three points.

i = Fxi is a consistency constraint between xi and x(cid:2)

The equation e(cid:2) × x(cid:2)
i, since it
is independent of v. It is simply a (disguised) epipolar constraint on the correspon-
dence xi ↔ x(cid:2)
i, and the RHS is Fxi which is
the epipolar line for xi in the second image, i.e. the equation enforces that x(cid:2)
i lie on
the epipolar line of xi, and hence the correspondence is consistent with the epipolar
geometry.

i: the LHS is the epipolar line through x(cid:2)

Estimation from noisy points. The three point correspondences which determine the
plane and homography must satisfy the consistency constraint arising from the epipo-
lar geometry. Generally measured correspondences xi ↔ x(cid:2)
i will not exactly satisfy
this constraint. We therefore require a procedure for optimally correcting the measured
points so that the estimated points ˆxi ↔ ˆx(cid:2)
i satisfy the epipolar constraint. Fortunately,
such a procedure has already been given in the triangulation algorithm 12.1(p318),
which can be adopted here directly. We then have a Maximum Likelihood estimate of
H and the 3D points under Gaussian image noise assumptions. The method is summa-
rized in algorithm 13.1.

13.2.2 A point and line
In this section an expression is derived for a plane speciﬁed by a point and line corre-
spondence. We start by considering only the line correspondence and show that this
reduces the three-parameter family of homographies compatible with F (13.4) to a 1-
parameter family. It is then shown that the point correspondence uniquely determines
the plane and corresponding homography.

The correspondence of two image lines determines a line in 3-space, and a line in
3-space lies on a one parameter family (a pencil) of planes, see ﬁgure 13.4. This pencil
of planes induces a pencil of homographies between the two images, and any member
of this family will map the corresponding lines to each other.

Result 13.7. The homography for the pencil of planes deﬁned by a line correspondence
l ↔ l(cid:2)

is given by

]×F + µe(cid:2)lT
provided l(cid:2)Te(cid:2) (cid:5)= 0, where µ is a projective parameter.

H(µ) = [l(cid:2)

(13.7)

13.2 Plane induced homographies given F and image correspondences
( µ )

L

π

l

e

C

π

π /

/l

/

e

/

C

C

a

l

e

L

b

333

/l

/

e

/

C

Fig. 13.4. (a) Image lines l and l(cid:1)
respectively. The intersection of these
planes deﬁnes the line L in 3-space. (b) The line L in 3-space is contained in a one parameter family
of planes π(µ). This family of planes induces a one parameter family of homographies between the
images.

determine planes π and π(cid:1)

Proof. From result 8.2(p197) the line l back-projects to a plane PTl through the ﬁrst
camera centre, and l(cid:2)
through the second, see ﬁgure 13.4a.
These two planes are the basis for a pencil of planes parametrized by µ. As in the proof
of result 13.3 we may choose P = [I | 0], P(cid:2)
(cid:20)

back-projects to a plane P(cid:2)Tl(cid:2)
= [A | e(cid:2)
(cid:21)
(cid:20)
π(µ) = µPTl + P(cid:2)Tl(cid:2)

], then the pencil of planes is

(cid:21)

= µ

+

l
0

ATl(cid:2)
e(cid:2)Tl(cid:2)

From result 13.1 the induced homography is H(µ) =A − e(cid:2)v(µ)T, with

(cid:15)
= −

Using the decomposition A = [e(cid:2)
H =

(cid:15)
(cid:16)
(e(cid:2)Tl(cid:2)I − e(cid:2)l(cid:2)T)[e(cid:2)
]×F + µe(cid:2)lT
[l(cid:2)

]×F − µe(cid:2)lT
/(e(cid:2)Tl(cid:2)

)

v(µ) = (µl + ATl(cid:2)
]×F we obtain
/(e(cid:2)Tl(cid:2)

(cid:16)

)/(e(cid:2)Tl(cid:2)
(cid:15)

) = −

)

(cid:16)

(13.8)

/(e(cid:2)Tl(cid:2)

)

]×[e(cid:2)

]×[e(cid:2)

]×F + µe(cid:2)lT

[l(cid:2)

where the last equality follows from result A4.4(p582) that [e(cid:2)
equivalent to (13.7) up to scale.

]×[e(cid:2)

]×F = F. This is

The homography for a corresponding point and line. From the line correspondence
we have that H(µ) = [l(cid:2)
]×F+µe(cid:2)lT, and now solve for µ using the point correspondence
x ↔ x(cid:2)
Result 13.8. Given F and a corresponding point x ↔ x(cid:2)
phy induced by the plane of the 3-space point and line is

and line l ↔ l(cid:2)

, the homogra-

.

H = [l(cid:2)

]×F +

(x(cid:2) × e(cid:2)

T

(x(cid:2) × ((Fx) × l(cid:2)

)
(cid:10)x(cid:2) × e(cid:2)(cid:10)2(lTx)

))

e(cid:2)lT.

The derivation is analogous to that of result 13.6. As in the three-point case, the im-
age point correspondence must be consistent with the epipolar geometry. This means

334

13 Scene planes and homographies

l

l

e

x

e

/

e

/

l

x/

/

l

e

image 1

image 2
Fig. 13.5. The epipolar geometry induces a homography between corresponding lines l ↔ l(cid:1)
the images of a line L in 3-space. The points on l are mapped to points on l(cid:1)
and x(cid:1)

are the images of the intersection of L with the epipolar plane corresponding to le and l(cid:1)
e.

which are
as x(cid:1) = [l(cid:1)]×Fx, where x

that the measured (noisy) points must be corrected using algorithm 12.1(p318) be-
fore result 13.8 is applied. There is no consistency constraint on the line, and no cor-
rection is available.

Geometric interpretation of the point map H(µ). It is worth exploring the map H(µ)
further. Since H(µ) is compatible with the epipolar geometry, a point x in the ﬁrst
view is mapped to a point x(cid:2)
= H(µ)x in the second view on the epipolar line Fx
corresponding to x. In general the position of the point x(cid:2)
= H(µ)x on the epipolar line
varies with µ. However, if the point x lies on l (so that lTx = 0) then

x(cid:2)

= H(µ)x = ([l(cid:2)

]×F + µe(cid:2)lT)x = [l(cid:2)

]×Fx

which is independent of the value of µ, depending only on F. Thus as shown in
ﬁgure 13.5 the epipolar geometry deﬁnes a point-to-point map for points on the line.

Degenerate homographies. As has already been stated, if the world plane contains
one of the camera centres, then the induced homography is degenerate. The matrix
representing the homography does not have full rank, and points on one plane are
mapped to a line (if rank H = 2) or a point (if rank H = 1). However, an explicit
expression can be obtained for a degenerate homography from (13.7). The degenerate
(singular) homographies in this pencil are at µ = ∞ and µ = 0. These correspond to
planes through the ﬁrst and second camera centres respectively. Figure 13.6 shows the
case where the plane contains the second camera centre, and intersects the image plane
in the line l(cid:2)

at the point x(cid:2)

where

. A point x in the ﬁrst view is imaged on l(cid:2)
]×Fx.

= l(cid:2) × Fx = [l(cid:2)

x(cid:2)
]×F. This is a rank 2 matrix.

The homography is thus H = [l(cid:2)

13.3 Computing F given the homography induced by a plane

Up to now it has been assumed that F is given, and the objective is to compute H when
various additional information is provided. We now reverse this, and show that if H is
given then F may be computed when additional information is provided. We start by
introducing an important geometric idea, that of parallax relative to a plane, which will
make the algebraic development straightforward.

13.3 Computing F given the homography induced by a plane

335

/

π

X

/

l

/

x

x

C

/

C

C

x

e

/

l

/

x

e /

Fx

/

C

a

b

Fig. 13.6. A degenerate homography. (a) The map induced by a plane through the second camera
centre is a degenerate homography H = [l(cid:1)]×F. The plane π(cid:1)
intersects the second image plane in the
line l(cid:1)
in the second. (b) A point x in the ﬁrst view
is imaged at x(cid:1)

with the epipolar line Fx of x, so that x(cid:1) = l(cid:1) × Fx.

. All points in the ﬁrst view are mapped to points on l(cid:1)

, the intersection of l(cid:1)

X

π

Xπ

x

H

l

/
x

/x

/x

C

e

/

e

/

C

Fig. 13.7. Plane induced parallax. The ray through X intersects the plane π at the point Xπ. The
images of X and Xπ are coincident points at x in the ﬁrst view. In the second view the images are
the points x(cid:1)
= Hx respectively. These points are not coincident (unless X is on π), but both
are on the epipolar line l(cid:1)
is the parallax relative to the
homography induced by the plane π. Note that if X is on the other side of the plane, then ˜x(cid:1)
will be on
the other side of x(cid:1)

x of x. The vector between the points x(cid:1)

and ˜x(cid:1)

.

and ˜x(cid:1)

Plane induced parallax. The homography induced by a plane generates a virtual par-
allax (see section 8.4.5(p207)) as illustrated schematically in ﬁgure 13.7 and by exam-
ple in ﬁgure 13.8. The important point here is that in the second view x(cid:2)
, the image of
the 3D point X, and ˜x(cid:2)
= Hx, the point mapped by the homography, are on the epipolar
line of x; since both are images of points on the ray through x. Consequently, the line
x(cid:2)×(Hx) is an epipolar line in the second view and provides a constraint on the position
of the epipole. Once the epipole is determined (two such constraints sufﬁce), then as
shown in result 9.1(p243) F = [e(cid:2)
]×H where H is the homography induced by any plane.
Similarly it can be shown that F = H−T[e]×.

As an application of virtual parallax it is shown in algorithm 13.2 that F can be
computed uniquely from the images of six points, four of which are coplanar and two
are off the plane. The images of the four coplanar points deﬁne the homography, and
the two points off the plane provide constraints sufﬁcient to determine the epipole. The

336

13 Scene planes and homographies

a

b

c

Fig. 13.8. Plane induced parallax. (a) (b) Left and right images. (c) The left image is superimposed
on the right using the homography induced by the plane of the Chinese text. The transferred and imaged
planes exactly coincide. However, points off the plane (such as the mug) do not coincide. Lines joining
corresponding points off the plane in the “superimposed” image intersect at the epipole.

six-point result is quite surprising since for seven points in general position there are 3
solutions for F (see section 11.1.2(p281)).

Objective
Given six point correspondences xi ↔ x(cid:1)
four 3-space points i ∈ {1, . . . , 4} coplanar, determine the fundamental matrix F.
Algorithm

i which are the images of 3-space Xi, with the ﬁrst

(i) Compute the homography H, such that x(cid:1)
(ii) Determine the epipole e(cid:1)
(iii) Then F = [e(cid:1)]×H.

i = Hxi, i ∈ {1, . . . , 4}.

as the intersection of the lines (Hx5) × x(cid:1)

5 and (Hx6) × x(cid:1)
6.

See ﬁgure 13.9.

Algorithm 13.2. Computing F given the correspondences of six points of which four are coplanar.

X 2

X 1

π

X

3

X

4

x

2

x1

x

C

X

x

3

e
x

4

a

e/

x

/
3

x

/
4

/

C

x/

/
x2

x/

/

e

1x
/

/x
5

/x

5

/x

6

/x

6

b

Fig. 13.9. The fundamental matrix is deﬁned uniquely by the image of six 3D points, of which four
are coplanar. (a) The parallax for one point X. (b) The epipole determined by the intersection of two
parallax lines: the line joining ˜x(cid:1)

5, and the join of ˜x(cid:1)

6 = Hx6 to x(cid:1)
6.

5 = Hx5 to x(cid:1)

13.3 Computing F given the homography induced by a plane

337

a

d

b

e

c

f

Fig. 13.10. Binary space partition. (a) (b) Left and right images. (c) Points whose correspondence is
known. (d) A triplet of points selected from (c). This triplet deﬁnes a plane. The points in (c) can then
be classiﬁed according to their side of the plane. (e) Points on one side. (f) Points on the other side.

Projective depth. A world point X = (xT, ρ)T is imaged at x in the ﬁrst view and at

x(cid:2)

= Hx + ρe(cid:2)

(13.9)

, e(cid:2)

in the second. Note that x(cid:2)
and Hx are collinear. The scalar ρ is the parallax relative
to the homography H, and may be interpreted as a “depth” relative to the plane π. If
ρ = 0 then the 3D point X is on the plane, otherwise the “sign” of ρ indicates which
‘side’ of the plane π the point X is (see ﬁgure 13.7 and ﬁgure 13.8). These statements
should be taken with care because in the absence of oriented projective geometry the
sign of a homogeneous object, and the side of a plane have no meaning.

Example 13.9. Binary space partition. The sign of the virtual parallax (sign(ρ)) may
be used to compute a partition of 3-space by the plane π. Suppose we are given F and
three space points are speciﬁed by their corresponding image points. Then the plane
deﬁned by the three points can be used to partition all other correspondences into sets
on either side of (or on) the plane. Figure 13.10 shows an example. Note, the three
points need not actually correspond to images of physical points so the method can
be applied to virtual planes. By combining several planes a region of 3-space can be
(cid:2)
identiﬁed.

Two planes. Suppose there are two planes, π1, π2, in the scene which induce ho-
mographies H1, H2 respectively. With the idea of parallax in mind it is clear that be-
cause each plane provides off-plane information about the other, the two homographies

338

13 Scene planes and homographies

π

1

X 2

X 1

π

2

x

Hx

C

/

x

C /

H
1

H
2

Fig. 13.11. The action of the map H = H−1
as
though it were the image of the 3D point X1, and then map it back to the ﬁrst image as though it were
the image of the 3D point X2. Points in the ﬁrst view which lie on the imaged line of intersection of the
two planes will be mapped to themselves, so are ﬁxed points under this action. The epipole e is also a
ﬁxed point under this map.

2 H1 on a point x in the ﬁrst image is ﬁrst to transfer it to x(cid:1)

should be sufﬁcient to determine F. Indeed F is over-determined by this conﬁguration
which means that the two homographies must satisfy consistency constraints.

Consider ﬁgure 13.11. The homography H = H−1

H1 is a mapping from the ﬁrst image
onto itself. Under this mapping the epipole e is a ﬁxed point, i.e. He = e, so may be
determined from the (non-degenerate) eigenvector of H. The fundamental matrix may
then be computed from result 9.1(p243) as F = [e(cid:2)
= Hie for i = 1 or 2.
The map H has further properties which may be seen from ﬁgure 13.11. The map has
a line of ﬁxed points and a ﬁxed point not on the line (see section 2.9(p61) for ﬁxed
points and lines). This means that two of the eigenvalues of H are equal. In fact H is
a planar homology (see section A7.2(p629)). In turn, these properties of H = H−1
H1
deﬁne consistency constraints on H1 and H2 in order that their composition has these
properties.

2

2

]×Hi, where e(cid:2)

Up to this point the results of this chapter have been entirely projective. Now an

afﬁne element is introduced.

13.4 The inﬁnite homography H∞

The plane at inﬁnity is a particularly important plane, and the homography induced by
this plane is distinguished by a special name:

Deﬁnition 13.10. The inﬁnite homography, H∞, is the homography induced by the
plane at inﬁnity, π∞.

The form of the homography may be derived by a limiting process starting from (13.2–
p327), H = K(cid:2)
K−1, where d is the orthogonal distance of the plane from

(cid:15)
R − tnT/d

(cid:16)

13.4 The inﬁnite homography H∞

339

v1

H

v /
1

v2

image 1

H

image 2

Fig. 13.12. The inﬁnite homography H∞ maps vanishing points between the images.

/v
2

the ﬁrst camera:

H∞ = lim

d→∞ H = K(cid:2)RK−1.

This means that H∞ does not depend on the translation between views, only on the
rotation and camera internal parameters. Alternatively, from (9.7–p250) corresponding
image points are related as
x(cid:2)

= K(cid:2)RK−1x + K(cid:2)t/Z = H∞x + K(cid:2)t/Z

(13.10)

where Z is the depth measured from the ﬁrst camera. Again it can be seen that points
at inﬁnity (Z = ∞) are mapped by H∞. Note also that H∞ is obtained if the translation
t is zero in (13.10), which corresponds to a rotation about the camera centre. Thus H∞
is the homography that relates image points of any depth if the camera rotates about its
centre (see section 8.4(p202)).

Since e(cid:2)

= K(cid:2)t, (13.10) can be written as x(cid:2)

/Z, and comparison
with (13.9) shows that (1/Z) plays the role of ρ. Thus Euclidean inverse depth can
be interpreted as parallax relative to π∞.

= H∞x + e(cid:2)

Vanishing points and lines. Images of points on π∞ are mapped by H∞. These images
are vanishing points, and so H∞ maps vanishing points between images, i.e. v(cid:2)
= H∞v,
where v(cid:2)
and v are corresponding vanishing points. See ﬁgure 13.12. Consequently,
H∞ can be computed from the correspondence of three (non-collinear) vanishing points
together with F using result 13.6. Alternatively, H∞ can be computed from the corre-
spondence of a vanishing line and the correspondence of a vanishing point (not on the
line), together with F, as described in section 13.2.2.

Afﬁne and metric reconstruction. As we have seen in chapter 10, specifying π∞
enables a projective reconstruction to be upgraded to an afﬁne reconstruction. Not
surprisingly, because of its association with π∞, H∞ arises naturally in the rectiﬁcation.
Indeed, if the camera matrices are chosen as P = [I | 0] and P(cid:2)
] then the
reconstruction is afﬁne.

= [H∞ | λe(cid:2)

Conversely, suppose the world coordinate system is afﬁne (i.e. π∞ has its canonical
position at π∞ = (0, 0, 0, 1)T); then H∞ may be determined directly from the camera
projection matrices. Suppose M, M(cid:2)
respectively.

are the ﬁrst 3× 3 submatrix of P and P(cid:2)

340

13 Scene planes and homographies

π

x

X

e

C

correspondence

on this segment

H

x /

/e

/

C

Fig. 13.13. Reducing the search region using H∞. Points in 3-space are no ‘further’ away than π∞.
H∞ captures this constraint and limits the search on the epipolar line in one direction. The baseline
between the cameras partitions each epipolar plane into two. A point on one “side” of the epipolar
line in the left image will be imaged on the corresponding “side” of the epipolar line in the right image
(indicated by the solid line in the ﬁgure). The epipole thus bounds the search region in the other direction.

Then a point X = (xT∞, 0)T on π∞ is imaged at x = PX = Mx∞ and x(cid:2)
in the two views. Consequently x(cid:2)

= M(cid:2)M−1x and so
H∞ = M(cid:2)M−1.

= P(cid:2)X = M(cid:2)x∞

(13.11)

The homography H∞ may be used to propagate camera calibration from one view to
another. The absolute conic Ω∞ resides on π∞, and its image, ω, is mapped between
images by H∞ according to result 2.13(p37): ω(cid:2)
−1
is speciﬁed in one view, then ω(cid:2)
, the image of Ω∞ in a second view, can be computed
via H∞, and the calibration for that view determined from ω(cid:2)
−1. Section
19.5.2(p475) describes applications of H∞ to camera auto-calibration.

= H−T∞ ωH−1∞ . Thus if ω = (KKT)

= (K(cid:2)K(cid:2)T)

Stereo correspondence. H∞ limits the search region when searching for correspon-
dences. The region is reduced from the entire epipolar line to a bounded line.
See ﬁgure 13.13. However, a correct application of this constraint requires oriented
projective geometry.

13.5 Closure

This chapter has illustrated a raft of projective techniques for a plane that may be ap-
plied to many other surfaces. A plane is a simple parametrized surface with 3 degrees
of freedom. A very similar development can be given for other surfaces where the
degrees of freedom are determined from images of points on the surface. For exam-
ple in the case of a quadric the surface can be determined both from images of points
on its surface, and/or (an extension not possible for planes) from its outline in each
view [Cross-98, Shashua-97]. The ideas of surface induced transfer, families of sur-
faces when the surface is not fully determined from its images, surface induced paral-
lax, consistency constraints, implicit computations, degenerate geometries etc. all carry
over to other surfaces.

13.5 Closure

341

13.5.1 The literature
The compatibility of epipolar geometry and induced homographies is investigated thor-
oughly by Luong & Vi´eville [Luong-96]. The six-point solution for F appeared in
Beardsley et al. [Beardsley-92] and [Mohr-92]. The solution for F given two planes
appeared in Sinclair [Sinclair-92]. [Zeller-96] gives many examples of conﬁgurations
whose properties may be determined using only epipolar geometry and their image
projections. He also catalogues their degenerate cases.

13.5.2 Notes and exercises

(i) Homography induced by a plane (13.1–p326).

(a) The inverse of the homography H is given by

(cid:20)

H−1 = A−1

I +

avTA−1
1 − vTA−1a

(cid:21)

provided A−1 exists. This is sometimes called the Sherman-Morrison
formula.
(b) Show that the homography H is degenerate if the plane contains the
second camera centre. Hint, in this case vTA−1a = 1, and note that
H = A(I − A−1avT).

(ii) Show that if the camera undergoes planar motion, i.e. the translation is parallel
to the plane and the rotation is parallel to the plane normal, then the homog-
raphy induced by the plane is conjugate to a planar Euclidean transformation.
Show that the ﬁxed points of the homography are the images of the plane’s
circular points.

(iii) Using (13.2–p327) show that if a camera undergoes a pure translation then the
homography induced by the plane is a planar homology (as deﬁned in section
A7.2(p629)), with a line of ﬁxed points corresponding to the vanishing line of
the plane. Show further that if the translation is parallel to the plane then the
homography is an elation (as deﬁned in section A7.3(p631)).
coplanar is (l(cid:2)

2)TF(l1 × l2) = 0. Why is it not a sufﬁcient condition?

(iv) Show that a necessary, but not sufﬁcient, condition for two space lines to be

× l(cid:2)

(v) Intersections of lines and planes. Verify each of the following results by
sketching the conﬁguration assuming general position. In each case determine
the degenerate conﬁgurations for which the result is not valid.
(a) Suppose the line L in 3-space is imaged as l and l(cid:2)

1

, and the plane π
= Hx. Then the point of intersection of
=

) in the ﬁrst image, and at x(cid:2)

induces the homography x(cid:2)
L with π is imaged at x = l × (HTl(cid:2)
l(cid:2) × (H−Tl) in the second.
a line seen in two images. If l and l(cid:2)
images, and v, v(cid:2)
(HT∞l(cid:2)

= l(cid:2) × (H−T∞ l).

), v(cid:2)

(b) The inﬁnite homography may be used to ﬁnd the vanishing point of
are corresponding lines in two
their vanishing points in each image, then v = l ×

342

13 Scene planes and homographies

(c) Suppose the planes π1 and π2 induce homographies x(cid:2)

=
H2x respectively. Then the image of the line of intersection of π1 with
π2 in the ﬁrst image obeys HT
l = l and may be determined from the
1
real eigenvector of the planar homology HT
1

= H1x and x(cid:2)

(see ﬁgure 13.11).

H−T

H−T

2

2

(vi) Coplanarity of four points.

Suppose F is known, and four corresponding
image points xi ↔ x(cid:2)
i are supplied. How can it be determined whether their
pre-images are coplanar? One possibility is to use three of the points to deter-
mine a homography via result 13.6(p331), and then measure the transfer error
of the fourth point. A second possibility is to compute lines joining the im-
age points, and determine if the line intersection obeys the epipolar constraint
(see [Faugeras-92b]). A third possibility is to compute the cross-ratio of the four
lines from the epipole to the image points – if the four scene points are copla-
nar then this cross-ratio will be the same in both images. Thus this equality is a
necessary condition for co-planarity, but is it a sufﬁcient condition also? What
statistical tests should be applied when there is measurement error (noise)?

= Hx induced by a plane π = ( ˜πT, π4)T is given by

(viii) Starting from the camera matrices P = [M | m], P(cid:2)

(vii) Show that the epipolar geometry can be computed uniquely from the images of
four coplanar lines and two points off the plane of the lines. If two of the lines
are replaced by points can the epipolar geometry still be computed?
homography x(cid:2)
H = M(cid:2)

(I−tvT)M−1 with t = (M(cid:2)−1m(cid:2)−M−1m), and v = ˜π/(π4− ˜πTM−1m).
(ix) Show that the homography computed as in result 13.6(p331) is independent of
the scale of F. Start by choosing an arbitrary ﬁxed scale for F, so that F is no
longer a homogeneous quantity, but a matrix ˜F with ﬁxed scale. Show that if
H = [e(cid:2)
T(˜Fxi), then replacing ˜F by λ˜F simply
scales H by λ.

(M−1 ˜b)T with ˜bi = c(cid:2)

= [M(cid:2) | m(cid:2)

] show that the

]×˜F − e(cid:2)

i

(x) Given two perspective images of a (plane) conic and the fundamental matrix be-
tween the views, then the plane of the conic (and consequently the homography
induced by this plane) is deﬁned up to a two-fold ambiguity. Suppose the image
conics are C and C(cid:2)
(Ce)T,
with the two values of µ obtained from

, then the induced homography is H(µ) = [C(cid:2)e(cid:2)

]×F−µe(cid:2)

#

"

(eTCe)C − (Ce)(Ce)T
Details are given in [Schmid-98].

µ2

(e(cid:2)TC(cid:2)e(cid:2)

) =−F T[C(cid:2)e(cid:2)

]×C(cid:2)

[C(cid:2)e(cid:2)

]×F.

(a) By considering the geometry, show that to be compatible with the
epipolar geometry the conics must satisfy the consistency constraint
that epipolar tangents are corresponding epipolar lines (see ﬁgure 11.6-
(p295)). Now derive this result algebraically starting from H(µ) above.
(b) The algebraic expressions are not valid if the epipole lies on the conic
(since then eTCe = e(cid:2)TC(cid:2)e(cid:2)
= 0). Is this a degeneracy of the geometry
or of the expression alone?

13.5 Closure

343

(xi) Fixed points of a homography induced by a plane. A planar homography H
has up to three distinct ﬁxed points corresponding to the three eigenvectors of
the 3 × 3 matrix (see section 2.9(p61)). The ﬁxed points are images of points
on the plane for which x(cid:2)
= Hx = x. The horopter is the locus of all points in
3-space for which x = x(cid:2)
. It is a twisted cubic curve passing through the two
camera centres. A twisted cubic intersects a plane in three points, and these are
the three ﬁxed points of the homography induced by that plane.

(xii) Estimation. Suppose n > 3 points Xi lie on a plane in 3-space and we wish
to optimally estimate the homography induced by the plane given F and their
image correspondences xi ↔ x(cid:2)
i. Then the ML estimate of the homography
estimating the plane ˆπ (3 dof) and the n points (cid:23)Xi (2 dof each, since they lie on
(assuming independent Gaussian measurement noise as usual) is obtained by

a plane) which minimizes reprojection error for the n points.

14

Afﬁne Epipolar Geometry

This chapter recapitulates the developments and objectives of the previous chapters
on two-view geometry, but here with afﬁne cameras replacing projective cameras. The
afﬁne camera is an extremely usable and well conditioned approximation in many prac-
tical situations. Its great advantage is that, because of its linearity, many of the optimal
algorithms can be implemented by linear algebra (matrix inverses, SVD etc.), whereas
in the projective case solutions either involve high order polynomials (such as for trian-
gulation) or are only possible by numerical minimization (such as in the Gold Standard
estimation of F).

We ﬁrst describe properties of the epipolar geometry of two afﬁne cameras, and its
optimal computation from point correspondences. This is followed by triangulation,
and afﬁne reconstruction. Finally the ambiguities in reconstruction that result from
parallel projection are sketched, and the non-ambiguous motion parameters are com-
puted from the epipolar geometry.

14.1 Afﬁne epipolar geometry

In many respects the epipolar geometry of two afﬁne cameras is identical to that of
two perspective cameras, for example a point in one view deﬁnes an epipolar line
in the other view, and the pencil of such epipolar lines intersect at the epipole. The
difference is that because the cameras are afﬁne their centres are at inﬁnity, and there
is parallel projection from scene to image. This leads to certain simpliﬁcations in the
afﬁne epipolar geometry:

Epipolar lines. Consider two points, x1, x2, in the ﬁrst view. These points back-
project to rays which are parallel in 3-space, since all projection rays are parallel. In
the second view an epipolar line is the image of a back-projected ray. The images of
these two rays in the second view are also parallel since an afﬁne camera maps parallel
scene lines to parallel images lines. Consequently, all epipolar lines are parallel, as are
the epipolar planes.

The epipoles. Since epipolar lines intersect in the epipole, and all epipolar lines are
parallel, it follows that the epipole is at inﬁnity.

344

14.2 The afﬁne fundamental matrix

345

X
1

X

2

3X

X

?

?

?

x

/

l

x /

x

1

x

2

x

3

parallel
epipolar
planes

x

/
3

/

3

l

l /
2

/
l 1

centre lies 
at infinity

a

b

Fig. 14.1. Afﬁne epipolar geometry. (a) Correspondence geometry: Projection rays are parallel and
intersect at inﬁnity. A point x back-projects to a ray in 3-space deﬁned by the ﬁrst camera centre (at
inﬁnity) and x. This ray is imaged as a line l(cid:1)
in the second view. The 3-space point X which projects
to x lies on this ray, so the image of X in the second view lies on l(cid:1)
. (b) Epipolar lines and planes are
parallel.

These points are illustrated schematically in ﬁgure 14.1, and examples on images are

shown in ﬁgure 14.2.

14.2 The afﬁne fundamental matrix

The afﬁne epipolar geometry is represented algebraically by a matrix termed the afﬁne
fundamental matrix, FA. It will be seen in the following that:

Result 14.1. The fundamental matrix resulting from two cameras with the afﬁne form
(i.e. the third row is (0, 0, 0, 1)) has the form

 0 0 ∗

0 0 ∗
∗ ∗ ∗

FA =

 0 0 a

0 0 b
c d e


 .

where ∗ indicates a non-zero entry.
It will be convenient to write the ﬁve non-zero entries as

FA =

Note that in general FA has rank 2.

(14.1)

14.2.1 Derivation
Geometric derivation. This derivation is the analogue of that given in section 9.2.1-
(p242) for a pair of projective cameras. The map from a point in one image to the
corresponding epipolar line in the other image is decomposed into two steps, as illus-
trated in ﬁgure 14.5 on page 352:

346

14 Afﬁne Epipolar Geometry

a

c

e

b

d

f

Fig. 14.2. Afﬁne epipolar lines. (a), (b) Two views of a hole punch acquired under afﬁne imaging con-
ditions. For the points marked in (c) the epipolar lines are superimposed on (d). Note that corresponding
points lie on their epipolar lines, and that all epipolar lines are parallel. The epipolar geometry is com-
puted from point correspondences using algorithm 14.1. (e) and (f) show the “ﬂow” for selected points
in the image (the lines link a point in one image to the point’s position in the other image). This demon-
strates that even though the epipolar lines are parallel the movement of imaged points between the views
contains both rotational and translational components.

(i) Point transfer via a plane π. Since both cameras are afﬁne, points are mapped
between an image and a scene plane by parallel projection, so the map between
π and the images is a planar afﬁne transformation; the composition of the afﬁne
transformations between the ﬁrst view and π, and π and the second view, is also
an afﬁne transformation, i.e. x(cid:2)

= HAx.

(ii) Constructing the epipolar line. The epipolar line is obtained as the line

= e(cid:2) × HAx = FAx, so that FA = [e(cid:2)

]×HA.

through x(cid:2)

and the epipole e(cid:2)

, i.e. l(cid:2)

14.3 Estimating FA from image point correspondences

347

We now take note of the special forms of the afﬁne matrix HA, and the skew matrix
[e(cid:2)

is at inﬁnity, and so has a zero last element:

]× when e(cid:2)

 0 0 ∗

0 0 ∗
∗ ∗ 0



 ∗ ∗ ∗

∗ ∗ ∗
0 0 1

 =

 0 0 ∗

0 0 ∗
∗ ∗ ∗



FA = [e(cid:2)

]×HA =

(14.2)

where ∗ indicates a non-zero entry. This derives the afﬁne form of F using only the
geometric properties that the camera centres are on the plane at inﬁnity.

Algebraic derivation.
In the case that the cameras are both afﬁne, the afﬁne form
of the fundamental matrix is obtained directly from the expression (9.1–p244) for F in
= P(cid:2)C, with C the camera
terms of the pseudo-inverse, namely F = [e(cid:2)
centre which is the null-vector of P. Details are left as an exercise. An elegant deriva-
tion of FA in terms of determinants formed from rows of the afﬁne camera matrices is
given in section 17.1.2(p413).

]×P(cid:2)P+, where e(cid:2)

14.2.2 Properties
The afﬁne fundamental matrix is a homogeneous matrix with ﬁve non-zero elements,
it thus has 4 degrees of freedom. These are accounted as: one for each of the two
epipoles (the epipoles lie on l∞, so only their direction need be speciﬁed); and two for
the 1D afﬁne transformation mapping the pencil of epipolar lines from one view to the
other.

The geometric entities (epipoles etc.) are encoded in FA in the same manner as their
encoding in F. However, often the expressions are far simpler and so can be given
explicitly.

The epipoles. The epipole in the ﬁrst view is the right null-vector of FA, i.e. FAe = 0.
This determines e = (−d, c, 0)T, which is a point (direction) on l∞. Since all epipolar
lines intersect the epipole this shows that all epipolar lines are parallel.

Epipolar lines. The epipolar line in the second view corresponding to x in the ﬁrst is
l(cid:2)
= FAx = (a, b, cx + dy + e)T. Again it is evident that all epipolar lines are parallel
since the line orientation, (a, b), is independent of (x, y).
These properties, and others, are summarized in table 14.1.

14.3 Estimating FA from image point correspondences
in two images. Given sufﬁciently many point matches xi ↔ x(cid:2)

The fundamental matrix is deﬁned by the equation x(cid:2)TFAx = 0 for any pair of matching
points x ↔ x(cid:2)
i, this
equation can be used to compute the unknown matrix FA. In particular, writing xi =
(xi, yi, 1)T and x(cid:2)

(cid:2)
i, 1)T each point match gives rise to one linear equation

i = (x

(cid:2)
i, y

(cid:2)
i + by

(cid:2)
i + cxi + dyi + e = 0

ax

(14.3)

in the unknown entries {a, b, c, d, e} of FA.

348

14 Afﬁne Epipolar Geometry

• FA is a rank 2 homogeneous matrix with 4 degrees of freedom. It has the form

(cid:17)

FA =

0 a
0
0
0
b
c d e

(cid:18)

.

are corresponding image points under an afﬁne camera,

• Point correspondence: If x and x(cid:1)
then x(cid:1)TFAx = 0. For ﬁnite points
(cid:1)
ax

• Epipolar lines:

+ by

(cid:1)

+ cx + dy + e = 0.

(cid:21) l(cid:1) = FAx = (a, b, cx + dy + e)T is the epipolar line corresponding to x.
(cid:21) l = FA
Tx(cid:1) = (c, d, ax + by + e)T is the epipolar line corresponding to x(cid:1)
• Epipoles:

.

(cid:21) From FAe = 0, e = (−d, c, 0)T.
(cid:21) From FA

Te(cid:1) = 0, e(cid:1) = (−b, a, 0)T.

• Computation from camera matrices PA, P(cid:1)
A:

(cid:21) General cameras,
AP+
AC, where C is the centre of the ﬁrst camera.

A is the pseudo-inverse of PA, and e(cid:1)

FA = [e(cid:1)]×P(cid:1)
e(cid:1) = P(cid:1)

A , where P+

(cid:21) Canonical cameras,

(cid:17)

(cid:18)

(cid:24)

(cid:25)

PA =

1
0
0

0
1
0

0
0
0

0
0
1

P(cid:1)
A =

M2×3

0

t
0 1

0

is the epipole deﬁned by

c = m13m21 − m11m23
a = m23,
d = m13m22 − m12m23, e = m13t2 − m23t1

b = −m13,

Table 14.1. Summary of afﬁne fundamental matrix properties.

14.3.1 The linear algorithm
In the usual manner a solution for FA may be obtained by rewriting (14.3) as

(cid:2)
i, y

(cid:2)
i, xi, yi, 1) f = 0

(x

with f = (a, b, c, d, e)T. From a set of n point matches, we obtain a set of linear
equations of the form Af = 0, where A is a n × 5 matrix:

 x

(cid:2)
(cid:2)
1 x1 y1 1
1 y
...
...
...
(cid:2)
(cid:2)
n y
n xn yn 1
x

...

...

 f = 0.

A minimal solution is obtained for n = 4 point correspondences as the right null-
space of the 4 × 5 matrix A. Thus FA can be computed uniquely from only 4 point

14.3 Estimating FA from image point correspondences

349

correspondences, provided the 3-space points are in general position. The conditions
for general position are described in section 14.3.3 below.

If there are more than 4 correspondences, and the data is not exact, then the rank
of A may be greater than 4. In this case, one may ﬁnd a least-squares solution, sub-
ject to (cid:10)f(cid:10) = 1, in essentially the same manner as that of section 4.1.1(p90), as the
singular vector corresponding to the smallest singular value of A. Refer to algorithm
4.2(p109) for details. This linear solution is the equivalent of the 8-point algorithm
11.1(p282) for the computation of a general fundamental matrix. We do not recom-
mend this approach for estimating FA because the Gold Standard algorithm described
below may be implemented with equal computational ease, and in general will have
superior performance.

The singularity constraint. The form (14.1) of FA ensures that the matrix has rank
no greater than 2. Consequently, if FA is estimated by the linear method above it is
not necessary to subsequently impose a singularity constraint. This is a considerable
advantage over the estimation of a general F by the linear 8-point algorithm, where
the estimated matrix is not guaranteed to have rank 2, and thus must be subsequently
corrected.

Geometric interpretation. As has been seen at several points in this book, comput-
ing a two-view relation from point correspondences is equivalent to ﬁtting a surface
in IR4. In the case of the equation x(cid:2)TFAx = 0 the relation
(variety) to points x, y, x
i + cxi + dyi + e = 0 is linear in the coordinates, and the variety VFA deﬁned
(cid:2)
ax
by the afﬁne fundamental matrix is a hyperplane.

(cid:2)
i + by

, y

(cid:2)

(cid:2)

This results in two simpliﬁcations: ﬁrst, ﬁnding the best estimate of FA may be for-
mulated as a (familiar) plane-ﬁtting problem; second, the Sampson error is identical
to the geometric error, whereas in the case of a general (non-afﬁne) fundamental ma-
trix (11.9–p287) it is only a ﬁrst-order approximation. As discussed in section 4.2.6-
(p98) this latter property arises generally with afﬁne (linear) relations because the tan-
gent plane of the Sampson approximation is equivalent to the surface.

14.3.2 The Gold Standard algorithm
Given a set of n corresponding image points {xi ↔ x(cid:2)
}, we seek the Maximum Like-
lihood estimate of FA under the assumption that the noise in the image measurements
obeys an isotropic, homogeneous Gaussian distribution. This estimate is obtained by
minimizing a cost function on geometric image distances:
i, ˆx(cid:2)
i)2

d(xi, ˆxi)2 + d(x(cid:2)

(cid:7)

(14.4)

i

min
{FA,ˆxi,ˆx(cid:1)
i}

i

i are the measured correspondences, and ˆxi and ˆx(cid:2)

where as usual xi ↔ x(cid:2)
i are estimated
“true” correspondences that satisfy ˆx(cid:2)
TFAˆxi = 0 exactly for the estimated afﬁne fun-
damental matrix. The distances are illustrated in ﬁgure 14.3. The true correspondences
are subsidiary variables that must also be estimated.

i

As discussed above, and in section 4.2.5(p96), minimizing the cost function (14.4)

14 Afﬁne Epipolar Geometry

(
n =

c
d

)

/

y

350

y

d

x

x

epipolar line
for x

/

/

n =(

a
b

)

/

x

d/

/

x

epipolar line
for x

x

/

x

Fig. 14.3. The MLE of FA from a set of measured corresponding points {xi ↔ x(cid:1)
the ﬁve parameters a, b, c, d, e together with a set of correspondences {ˆxi ↔ ˆx(cid:1)
ˆx(cid:1)
TFAˆxi = 0. There is a linear solution to this problem.

i

i} involves estimating
i} which exactly satisfy

y

n

x

d

x

x

Fig. 14.4. In 2D a line is the analogue of the hyperplane deﬁned by FA, and the problem of estimating
the true correspondence given a measured correspondence, is the problem of determining the closest
point (ˆx, ˆy) on the line ax + by + c to a measurement point (x, y). The normal to the line has direction
(a, b), and the perpendicular distance of the point (x, y) from the line is d⊥ = (ax + by + c)/
a2 + b2,
so that (ˆx, ˆy)T = (x, y)T − d⊥ˆn, where ˆn = (a, b)/

a2 + b2.

√

√

estimated points (cid:23)Xi = (ˆx(cid:2)
written as ((cid:23)XT
i, ˆy(cid:2)

is equivalent to ﬁtting a hyperplane to set of points Xi = (x
i)T satisfy the equation ˆx(cid:2)
i

(cid:2)
(cid:2)
i, xi, yi)T in IR4. The
i, y
TFAˆxi = 0 which may be
i , 1)f = 0, where f = (a, b, c, d, e)T. This is the equation of a point in IR4
on the plane f. We seek the plane f which minimizes the squared distance between the
measured and estimated points, and consequently which minimizes the sum of squared
perpendicular distances to the points Xi = (x

(cid:2)
i, xi, yi)T.

i, ˆxi, ˆy

Geometrically the solution is very simple, and an analogue for line ﬁtting in 2D is
(cid:2)
i)T

illustrated in ﬁgure 14.4. The perpendicular distance of a point Xi = (xi, yi, x
from the plane f is

(cid:2)
i, y

(cid:2)
i, y

d⊥(Xi, f ) =

ax

(cid:2)
√
i + by

(cid:2)
i + cxi + dyi + e

a2 + b2 + c2 + d2

.

Then the matrix FA which minimizes (14.4) is determined by minimizing the cost func-
tion

d⊥(Xi, f )2 =

1

a2 + b2 + c2 + d2

i

(cid:2)
i + by

(cid:2)
i + cxi + dyi + e)2

(ax

(14.5)

(cid:7)

i

C =

(cid:7)

14.3 Estimating FA from image point correspondences

351

Objective
Given n ≥ 4 image point correspondences {xi ↔ x(cid:1)
Likelihood estimate FA of the afﬁne fundamental matrix.

i}, i = 1, . . . , n, determine the Maximum

Algorithm
A correspondence is represented as Xi = (x

(cid:1)
i, y

(cid:1)
i, xi, yi)T.

(cid:27)

(i) Compute the centroid X = 1
(ii) Compute the n × 4 matrix A with rows ∆XT
i .
(iii) Then N = (a, b, c, d)T is the singular vector corresponding to the smallest singular

i Xi and centre the vectors ∆Xi = Xi − X.

value of A, and e = −NTX. The matrix FA has the form (14.1).

n

Algorithm 14.1. The Gold Standard algorithm for estimating FA from image correspondences.

over the 5 parameters {a, b, c, d, e} of f. Writing N = (a, b, c, d)T for the normal to the
hyperplane then

(cid:7)

(cid:15)

i

C =

1(cid:10)N(cid:10)2

NTXi + e

2

.

This cost function can be minimized by a very simple linear algorithm, equivalent to
the classical problem of orthogonal regression to a plane. There are two steps:
The ﬁrst step is to minimize C over the parameter e. We obtain

(cid:16)

(cid:16)

2

∂C
∂e

=

2(NTXi + e) = 0

(cid:7)
1(cid:10)N(cid:10)2
(cid:7)
(NTXi) =− NTX

i

e = − 1
n

i

(cid:7)

(cid:15)

i

C =

1(cid:10)N(cid:10)2

NT∆Xi

and hence

so the solution hyperplane passes through the data centroid X. Substituting for e in the
cost function reduces C to

where ∆Xi = Xi − X is the vector Xi relative to the data centroid X.
The second step is to minimize this reduced cost function over N. Writing A for the

matrix with rows ∆XT

i , it is evident that

C = (cid:10)AN(cid:10)2/(cid:10)N(cid:10)2.

Minimizing this expression is equivalent to minimizing (cid:10)AN(cid:10) subject to (cid:10)N(cid:10) = 1,
which is our usual homogeneous minimization problem solved by the SVD. These
steps are summarized in algorithm 14.1.

It is worth noting that the Gold Standard algorithm produces an identical estimate of
FA to that obtained by the factorization algorithm 18.1(p437) for an afﬁne reconstruc-
tion from n point correspondences.

352

14 Afﬁne Epipolar Geometry

X 2

X 1

X

4

X3
π

x /
2

x/
3

x /
4

4AxH

H

A

/
x 1

x

2

x1

x4

x3

Fig. 14.5. Computing the afﬁne epipolar line for a minimal conﬁguration of four points. The line is
computed by the virtual parallax induced by the plane π. Compare ﬁgure 13.7(p335).

Objective
Given four image point correspondences {xi ↔ x(cid:1)
mental matrix.

i}, i = 1, . . . , 4, compute the afﬁne funda-

Algorithm
The ﬁrst three 3-space points Xi, i = 1, . . . , 3 deﬁne a plane π. See ﬁgure 14.5.
(i) Compute the afﬁne transformation matrix HA, such that x(cid:1)
(ii) Determine the epipolar line in the second view from l(cid:1) = (HAx4) × x(cid:1)
4. The epipole
(iii) Then for any point x the epipolar line in the second view is e(cid:1) × (HAx) = FAx. Hence

i = HAxi, i = 1, . . . , 3.

e(cid:1) = (−l
(cid:1)
2, l
FA = [(−l
(cid:1)
2, l

(cid:1)
1, 0)T.
(cid:1)
1, 0)T]×HA.

Algorithm 14.2. The computation of FA for a minimal conﬁguration of four point correspondences.

14.3.3 The minimal conﬁguration
We return to the minimal conﬁguration for estimating FA, namely the corresponding
images of four points in 3-space in general position. A geometric computation method
of FA for this conﬁguration is described in algorithm 14.2. This minimal solution is
useful in the case of robust estimation algorithms, such as RANSAC, and will be used
here to illustrate degenerate conﬁgurations. Note that for this minimal conﬁguration an
exact solution is obtained for FA, and the linear algorithm of section 14.3.1, the Gold
Standard algorithm 14.1, and the minimal algorithm 14.2 give an identical result.

General position. The conﬁguration of four points shown in ﬁgure 14.5 demonstrates
the conditions necessary for general position of the 3-space points when computing FA.
Conﬁgurations for which FA cannot be computed are degenerate. These fall into two
classes: ﬁrst, degenerate conﬁgurations depending only on the structure, for example
if the four points are coplanar (so there is no parallax), or if the ﬁrst three points are
collinear (so that HA can’t be computed); second, those degeneracies which depend

14.4 Triangulation

353

only on the cameras, for example if the two cameras have the same viewing direction
(and so have common centres on the plane at inﬁnity).

Note once again the importance of parallax – as the point X4 approaches the plane
deﬁned by the other three points in ﬁgure 14.5 the parallax vector, which determines the
epipolar line direction, is monotonically reduced in length. Consequently, the accuracy
of the line direction is correspondingly reduced. This result for the minimal conﬁgura-
tion is true also of the Gold Standard algorithm 14.1: as relief reduces to zero, i.e. the
point set approaches coplanarity, the covariance of the estimated FA will increase.

14.4 Triangulation

Suppose we have a measured correspondence (x, y) ↔ (x
) and the afﬁne funda-
mental matrix FA. We wish to determine the Maximum Likelihood estimate of the true
correspondence, (ˆx, ˆy) ↔ (ˆx
), under the usual assumption that image measure-
ment error is Gaussian. The 3D point may then be determined from the ML estimate
correspondence.

(cid:2)
, ˆy

, y

(cid:2)

(cid:2)

(cid:2)

As we have seen earlier

the MLE involves determining a
i.e.
, 1)FA(ˆx, ˆy, 1)T = 0, and also minimizes the image distance to the measured

“true” correspondence which exactly obeys the afﬁne epipolar geometry,
(cid:2)
(ˆx
, ˆy
points,

in chapter 12,

(cid:2)

(x − ˆx)2 + (y − ˆy)2 + (x

(cid:2) − ˆx

(cid:2)

)2 + (y

(cid:2) − ˆy
(cid:2)

)2.

(cid:2)

(cid:2)

Geometrically the solution is very simple, and is illustrated in 2D in ﬁgure 14.4. We
seek the closest point on the hyperplane deﬁned by FA to the measured correspondence
, x, y)T in IR4. Again, the Sampson correction (4.11–p99) is exact in this
X = (x
case. Algebraically, the normal to the hyperplane has direction N = (a, b, c, d)T and the
perpendicular distance of a point X to the hyperplane is given by d⊥ = (NTX+e)/(cid:10)N(cid:10),
so that

, y

N
(cid:10)N(cid:10)

(cid:23)X = X − d⊥
 − (ax

+ by

(cid:2)

(cid:2)

+ cx + dy + e)

(a2 + b2 + c2 + d2)

or in its full detail ˆx

(cid:2)
(cid:2)
ˆy
ˆx
ˆy

 =

 x

(cid:2)
(cid:2)

y
x
y

 .

 a

b
c
d

14.5 Afﬁne reconstruction

Suppose we have n ≥ 4 image point correspondences xi ↔ x(cid:2)
, i = 0, . . . , n−1, which
for the moment will be assumed noise-free, then we may compute a reconstruction of
the 3D points and cameras. In the case of projective cameras (with n ≥ 7 points) the
reconstruction was projective. In the afﬁne case, not surprisingly, the reconstruction is
afﬁne. We will now give a simple constructive derivation of this result.

An afﬁne coordinate frame in 3-space may be speciﬁed by four ﬁnite non-coplanar
basis points Xi, i = 0, . . . ,3 . As illustrated in ﬁgure 14.6 one point X0 is chosen as the

354

14 Afﬁne Epipolar Geometry

X

3

E

3

X

2

2E

X

0

1E

X

1

a

X

= (X,Y,Z)

Y

2E

E3

X

Z

E1

b

Fig. 14.6. Afﬁne coordinates. (a) four non-coplanar points in 3-space (X1, X2, X3 and origin X0)
deﬁne a set of axes in terms of which other points X can be assigned afﬁne coordinates (X, Y, Z). (b)
Each afﬁne coordinate is deﬁned by a ratio of lengths in parallel directions (which is an afﬁne invariant).

For example, X may be computed by the following two operations: ﬁrst, X is projected parallel to(cid:22)E2
onto the plane spanned by(cid:22)E1 and(cid:22)E3. Second, this projected point is projected parallel to(cid:22)E3 onto the
(cid:22)E1 axis. The value of the coordinate X is the ratio of the length from the origin of this ﬁnal projected
point to the length of(cid:22)E1.

x

0

e
1

x

e 3

x

3

Image 1

e

2

x

1

x

2

x

/
1

x

/
2

/

x

/
e 1

/

e2

x

/
3

x

/
0

/e
3

Image 2

Fig. 14.7. Reconstruction from two images. The afﬁne coordinates of the 3D point X with image x, x(cid:1)
in two views may be computed linearly from the projection of the basis points xi and basis vectors ˜ei of
ﬁgure 14.6.

origin, and the three other points then deﬁne basis vectors (cid:22)Ei = (cid:22)Xi − (cid:22)X0, i = 1, . . . ,3 ,
where (cid:22)Xi is the inhomogeneous 3-vector corresponding to Xi. The position of a point
and (X, Y, Z) are the afﬁne coordinates of (cid:22)X with respect to this basis. This means that

X may then be speciﬁed by simple vector addition as

(cid:22)X = (cid:22)X0 + X(cid:22)E1 + Y(cid:22)E2 + Z(cid:22)E3
 0
 1
 (cid:22)X1 =

 (cid:22)X2 =

 (cid:22)X3 =

the basis points Xi have the canonical coordinates (X, Y, Z)T:

(cid:22)X0 =

 0

0
0

 .

 0

0
1

0
0

1
0

(14.6)

Given the afﬁne projection of the four basis points in two views, the 3D afﬁne co-
ordinates of any other point can be directly recovered from its image, as will now be
demonstrated (see ﬁgure 14.7).

14.6 Necker reversal and the bas-relief ambiguity

355

Projection with an afﬁne camera may be represented as (6.26–p172)

(cid:22)X + ˜t

˜x = M2×3

where ˜x = (x, y)T is the inhomogeneous 2-vector corresponding to x. Differences of
vectors eliminate ˜t. For example the basis vectors project as ˜ei = M2×3
Consequently for any point X, its image in the ﬁrst view is
˜x − ˜x0 = X˜e1 + Y˜e2 + Z˜e3
(cid:22)X + ˜t
= M(cid:2)
2×3
˜x(cid:2) − ˜x(cid:2)
0 = X˜e(cid:2)

) in the second view is
1 + Y˜e(cid:2)

and similarly the image (˜x(cid:2)

2 + Z˜e(cid:2)
3.

(14.7)

(14.8)

(cid:2)

(cid:22)Ei, i = 1, . . . ,3 .

Each equation (14.7) and (14.8) imposes two linear constraints on the unknown afﬁne
coordinates X, Y, Z of the space point X. All the other terms in the equations are known
from the projection of the four basis points (cid:22)Xi, i = 0, . . . ,3 ). Thus, there are four lin-
from image measurements (for example the image basis vectors ˜ei, ˜e(cid:2)
i are computed

ear simultaneous equations in the three unknowns X, Y, Z, and the solution is straight-
forward. This demonstrates that the afﬁne coordinates of a point X may be computed
from its image in two views.

between the 3-space points (cid:22)Xi, with coordinates given in (14.6), and their measured
images. For example, PA is computed from the correspondence ˜xi ↔ (cid:22)Xi, i = 0, . . . ,3 .

The cameras for the two views, PA, P(cid:2)

A, may be computed from the correspondences

The above development is not optimal, because the basis points are treated as exact,
and all measurement error associated with the ﬁfth point X. An optimal reconstruction
algorithm, where reprojection error is minimized over all points, is very straightforward
in the afﬁne case. However, its description is postponed until section 18.2(p436) be-
cause the factorization algorithm described there is applicable to any number of views.

Example 14.2. Afﬁne reconstruction
A 3D reconstruction is computed for the hole punch images of ﬁgure 14.2 by choosing
four points as the afﬁne basis, and then computing the afﬁne coordinates of each of
the remaining points in turn by the linear method above. Two views of the resulting
reconstruction are shown in ﬁgure 14.8. Note, however, that this ﬁve-point method
is not recommended. Instead the optimal afﬁne reconstruction algorithm 18.1(p437)
(cid:2)
should be used.

14.6 Necker reversal and the bas-relief ambiguity

We have seen in the previous section that in the absence of any calibration informa-
tion, an afﬁne reconstruction is obtained from point correspondences alone. In this
section we show that even if the camera calibration is known there remains a family of
reconstruction ambiguities which cannot be resolved in the two-view case.

This situation differs from that of perspective projection where once the internal
calibration is determined the camera motion is determined up to a ﬁnite number of

356

14 Afﬁne Epipolar Geometry

a

b

c

Fig. 14.8. Afﬁne reconstruction. (a)(b) Wireframe outline of the hole punch from the two images of
ﬁgure 14.2. The circles show the points selected as the afﬁne basis. The lines are for visualization only.
(c) Two views of the 3D afﬁne structure computed from the vertices of the wireframe.

ambiguities (from the essential matrix, see section 9.6(p257)). For parallel projection
there are two important additional ambiguities: a ﬁnite reﬂection ambiguity (Necker
reversal); and a one-parameter family rotation ambiguity (the bas-relief ambiguity).

Necker reversal ambiguity.
This arises because an object rotating by ρ and its
mirror image rotating by −ρ generate the same image under parallel projection, see
ﬁgure 14.9(a). Thus, structure is only recovered up to a reﬂection about the frontal
plane. This ambiguity is absent in the perspective case because the points have differ-
ent depths in the two interpretations and so do not project to coincident image points.

This is illustrated in ﬁgure 14.9(b).

The bas-relief ambiguity.
Imagine a set of
parallel rays from one camera, and consider adjusting a set of parallel rays from a
second camera until each ray intersects its corresponding ray. The rays lie in a family
of parallel epipolar planes, and there remains the freedom to rotate one camera about
the normal to these planes whilst maintaining incidence of the rays. This bas-relief (or
depth–turn) ambiguity is a one-parameter family of solutions for the rotation angle and
depth. The parameters of depth, ∆Z, and rotation, sin ρ, are confounded and cannot be
determined individually – only their product can be computed. Consequently, a shallow
object experiencing a large turn (i.e. small ∆Z and large ρ) generates the same image
as a deep object experiencing a small turn (i.e. large ∆Z and small ρ). The name arises
from bas-relief sculptures. Fixing the depth or the angle ρ determines the structure and
the motion uniquely. Extra points cannot resolve this ambiguity, but an additional view
(i.e. three views) will in general resolve it.

14.7 Computing the motion

357

Z

Z

−ρ
ρ

Frontal plane

−ρ
ρ

parallel

ρ

Z∆ 1

b

X

?

?

?

X

perspective

a

ρ

l

l

A

A’

x

/

x

X

c

Fig. 14.9. Motion ambiguities under parallel projection. (a) Necker reversal: a rotating object gen-
erates the same image as its mirror object rotating in the opposite sense. Under perspective projection
the images are different. (b) The cameras can rotate (by ρ) and still preserve ray intersections. This
cannot happen for perspective cameras. (c) The bas-relief ambiguity: consider a rod of length l, which
(cid:1) − x = l sin ρ. This bas–relief (or depth–turn) ambiguity is so-
rotates through an angle ρ. That is x
named because a shallow object experiencing a large turn (i.e. small l and big ρ) generates the same
image as a deep object experiencing a small turn (i.e. large l and small ρ).

This ambiguity casts light on the stability of reconstruction from two perspective
cameras: as imaging conditions approach afﬁne the rotation angle will be poorly esti-
mated, but the product of the rotation angle and depth will be stable.

14.7 Computing the motion

In this section expressions for computing the camera motion from FA will be given for
the case of two weak perspective cameras (section 6.3.4(p170)). These cameras may
be chosen as

 α

(cid:2)
x

P(cid:2)

=



 r1T t1

r2T t2
0T
1



(cid:2)
α
y

1

 αx

P =



 1 0 0 0

0 1 0 0
0 0 0 1



αy

1

where r1 and r2 are the ﬁrst and second rows of the rotation matrix R between the
views. We will assume that the aspect ratio αy/αx is known in both cameras, but that
(cid:2)
the relative scaling s = α
x/αx is unknown. s > 1 for a “looming” object and s < 1 for
one that is “receding”. As has been seen, the complete rotation R cannot be computed
from two weak perspective views, resulting in the bas-relief ambiguity. Nevertheless

358

14 Afﬁne Epipolar Geometry

Y

Y

Z

θ

Z

image

a

X

ρ
Φ

φ

image

X

b

Fig. 14.10. The rotation representation. (a) rotation by θ about the Z-axis; (b) subsequent rotation by
ρ about a fronto–parallel axis Φ angled at φ to the X-axis. The Φ-axis has components (cos φ, sin φ, 0)T.

ρ

AxisΦ

π

I1

l

X

I 2

/

l

φ

l

a

I

1

φ

/

l

2I

ρ

Axis Φ

π

X

I 2

/

l

φ−θ

l

φ

l
/

2I
b

I1

l

I

1

Fig. 14.11. The camera rotates about the axis Φ which is parallel to the image plane. The intersection
of the epipolar plane π with the image planes gives epipolar lines l and l(cid:1)
, and the projections of Φ in
the images are orthogonal to these epipolar lines: (a) no cyclotorsion occurs (θ = 0◦
); (b) the camera
counter-rotates by θ in I1, so the orientation of the epipolar lines changes by θ.

the remaining motion parameters can be computed from FA, and their computation is
straightforward.

To represent the motion we will use a rotation representation introduced by Koen-
derink and van Doorn [Koenderink-91]. As will be seen, this has the advantage that it
isolates the parameter ρ of the bas-relief ambiguity, which cannot be computed from
the afﬁne epipolar geometry. In this representation the rotation R between the views is
decomposed into two rotations (see ﬁgure 14.10),

R = Rρ Rθ.

(14.9)

First, there is a cyclo-rotation Rθ in the image plane through an angle θ (i.e. about the
line of sight). This is followed by a rotation Rρ through an angle ρ about an axis Φ with
direction parallel to the image plane, and angled at φ to the positive X-axis, i.e. a pure
rotation out of the image plane.

14.7 Computing the motion

359

Ι 1

Parallel
epipolar
planes

Ι

2b

Axis

ρ
b

Ι

2a

ρ
a

π1
π2
π3

Fig. 14.12. The scene can be sliced into parallel epipolar planes. The magnitude of ρ has no effect on
the epipolar geometry (provided ρ (cid:5)= 0), so it is indeterminate from two views.

θ

ρ

a

b

φ

c

d

Fig. 14.13. The effect of scale and rotation angles on the epipolar lines for an object moving relative to
a stationary camera. This also illustrates the assumed sequence of events accounting for the transition
from I1 to I2: (a) I1; (b) cyclotorsion (θ); (c) rotation out of the plane (φ and ρ); (d) scaling, giving I2.

Solving for s, φ and θ.
It is now shown that the scale factor (s), the projection of the
axis of rotation (φ) and the cyclo-rotation angle (θ) may be computed directly from the
afﬁne epipolar geometry. The solution is preceded by a geometric explanation of how
the epipolar lines relate to the unknown motion parameters.

. Consequently:

Consider a camera rotating about an axis Φ lying parallel to the image plane
(ﬁgure 14.11(a)). The epipolar plane π is perpendicular to both this axis and the two
images, and intersects the images in the epipolar lines l and l(cid:2)
• The projection of the axis of rotation Φ is perpendicular to the epipolar lines.
This relation still holds if there is additionally a cyclotorsion θ in the image plane
(ﬁgure 14.11(b)); the axis Φ and intersection l(cid:2)
remain ﬁxed in space, and are sim-
ply observed at a new angle in the image, maintaining the orthogonality between the
epipolar lines and the projected axis. The orientations of the epipolar lines in the two
images therefore differ by θ. Importantly, changing the magnitude of the turn angle ρ
doesn’t alter the epipolar geometry in any way (ﬁgure 14.12). This angle is therefore
indeterminate from two views, a consequence of the bas-relief ambiguity.

360

14 Afﬁne Epipolar Geometry

a

b

Fig. 14.14. Computing motion from afﬁne epipolar geometry. (a)(b) Two views of a buggy rotating
on a turntable. The computed rotation axis is superimposed on the image, drawn to pass through the
image centre. The ground truth axis is, of course, perpendicular to the turntable in the world.

Figure 14.13 illustrates the effect of scale. Consider a 3D object to be sliced into
parallel epipolar planes, with each plane constraining how a particular slice of the
object moves. Altering the effective size of the object (e.g. by moving closer to it)
simply changes the relative spacing between successive epipolar planes.

In summary, cyclotorsion simply rotates the epipolar lines, rotation out of the plane
causes foreshortening along the epipolar lines (orthogonal to Φ), and a scale change
uniformly alters the epipolar line spacing (ﬁgure 14.13).

It can be shown (and is left as exercise) that s, θ and φ can be computed directly from

the afﬁne epipolar geometry as

tan φ =

b
a

,

d
c

and s2 =

tan(φ − θ) =

c2 + d2
a2 + b2 ,
with s > 0 (by deﬁnition). Note that φ is the angle of projection in I2 of the axis of
rotation out of the plane, while (φ − θ) is its angle of projection in I1.
Example 14.3. Motion computed from the afﬁne fundamental matrix
ﬁgure 14.14 shows two images of a buggy rotating on a turntable. The image is
256 × 256 pixels with an aspect ratio of 0.65. The afﬁne fundamental matrix is com-
puted using algorithm 14.1, and the motion parameters computed from FA using (14.10)
(cid:2)
above. The computed rotation axis is superimposed on the image.

(14.10)

14.8 Closure

14.8.1 The literature
Koenderink and van Doorn [Koenderink-91] set the scene for afﬁne reconstruction
from two afﬁne cameras. This paper should be read by all. The afﬁne fundamental
matrix was ﬁrst deﬁned in [Zisserman-92]. The computation of the motion parameters
from FA is described in Shapiro et al. [Shapiro-95], and in particular the cases where
a third view does not resolve the bas–relief ambiguity. A helpful eigenvector analysis
of the ambiguity is given in [Szeliski-96]. The three view afﬁne motion case is treated
quite elegantly in [Shimshoni-99].

14.8.2 Notes and exercises

14.8 Closure

361

(i) A scene plane induces an afﬁne transformation between two afﬁne cameras.
There is a three-parameter family of such afﬁnities deﬁned by the three-
parameter family of planes in IR3. Given FA, this family of afﬁnities may be
written as (result 13.3(p328)) HA = [e(cid:2)
= 0, and
the 3-vector v parametrizes the family of planes. Conversely, show that given
HA, the homography induced by a scene plane, then FA is determined up to a
one-parameter ambiguity.

]×FA + e(cid:2)vT, where FA

Te(cid:2)

(ii) Consider a perspective camera, i.e. the matrix does not have the afﬁne form.
Show that if the camera motion consists of a translation parallel to the image
plane, and a rotation about the principal axis, then F has the afﬁne form. This
shows that a fundamental matrix with afﬁne form does not imply that the imag-
ing conditions are afﬁne. Are there other camera motions which generate a
fundamental matrix with the afﬁne form?

(iii) Two afﬁne cameras, PA, P(cid:2)

A, uniquely deﬁne an afﬁne fundamental matrix FA
by (9.1–p244). Show that if the cameras are transformed on the right by a
common afﬁne transformation, i.e. PA (cid:6)→ PAHA, P(cid:2)
AHA, the transformed
cameras deﬁne the original FA. This shows that the afﬁne fundamental matrix
is invariant to an afﬁne transformation of the world coordinates.

A (cid:6)→ P(cid:2)

(iv) Suppose one of the cameras is afﬁne and the other is a perspective camera.
(v) The 4 × 4 permutation homography

Show that in general in this case the epipoles in both views are ﬁnite.

 1

0
0
0

H =



0
0
0
1

0
0
1
0

0
1
0
0

(cid:17)

maps the canonical matrix of a ﬁnite projective camera P = [I | 0] into the
canonical matrix of parallel projection PA:

PA = [I | 0] H =

1 0 0 0
0 1 0 0
0 0 0 1

(cid:18)

.

Show, by applying this transformation to a pair of ﬁnite projective camera ma-
trices, that the results of this chapter (such as the properties listed in table 14.1-
(p348)) can be generated directly from their non-afﬁne counterparts of the pre-
vious chapters. In particular derive an expression for a pair of afﬁne cameras
PA, P(cid:2)

A consistent with FA.

Part III

Three-View Geometry

Lord Shiva, c. 1920-40 (print).

Shiva is depicted as having three eyes. The third eye in the centre of the
forehead symbolizes spiritual knowledge and power.
Image courtesy of http://www.healthyplanetonline.com

Outline

This part contains two chapters on the geometry of three-views. The scene is imaged
with three cameras perhaps simultaneously in a trinocular rig, or sequentially from a
moving camera.

Chapter 15 introduces a new multiple view object – the trifocal tensor. This has anal-
ogous properties to the fundamental matrix of two-view geometry: it is independent of
scene structure depending only on the (projective) relations between the cameras. The
camera matrices may be retrieved from the trifocal tensor up to a common projec-
tive transformation of 3-space, and the fundamental matrices for view-pairs may be
retrieved uniquely.

The new geometry compared with the two-view case is the ability to transfer from
two views to a third: given a point correspondence over two views the position of the
point in the third view is determined; and similarly, given a line correspondence over
two views the position of the line in the third view is determined. This transfer property
is of great beneﬁt when establishing correspondences over multiple views.

If the essence of the epipolar constraint over two views is that rays back-projected
from corresponding points are coplanar, then the essence of the trifocal constraint over
three views is the geometry of a point–line–line correspondence arising from the image
of a point on a line in 3-space: corresponding image lines in two views back-project
to planes which intersect in a line in 3-space, and the ray back-projected from a corre-
sponding image point in a third view must intersect this line.

Chapter 16 describes the computation of the trifocal tensor from point and line cor-
respondences over three-views. Given the tensor, and thus the retrieved camera matri-
ces, a projective reconstruction may be computed from correspondences over multiple
views. The reconstruction may be upgraded to similarity or metric as additional infor-
mation is provided in the same manner as in the two view case.

It is in reconstruction that there is another gain over two-view geometry. Given the
cameras, in the two-view case each point correspondence provided four measurements
on the three degrees of freedom (the position) of the point in 3-space. In three views
there are six measurements on, again, three degrees of freedom. However, it is for lines
that there is the more signiﬁcant gain. In two-views the number of measurements equals
the number of degrees of freedom of the line in 3-space, namely four. Consequently,
there is no possibility of removing the effects of measurement errors. However, in
three views there are six measurements on four degrees of freedom, so a scene line is
over-determined and can be estimated by a suitable minimization over measurement
errors.

364

15

The Trifocal Tensor

The trifocal tensor plays an analogous role in three views to that played by the funda-
mental matrix in two. It encapsulates all the (projective) geometric relations between
three views that are independent of scene structure.

We begin this chapter with a simple introduction to the main geometric and algebraic
properties of the trifocal tensor. A formal development of the trifocal tensor and its
properties involves the use of tensor notation. To start, however, it is convenient to
use standard vector and matrix notation, thus obtaining some geometric insight into
the trifocal tensor without the additional burden of dealing with a (possibly) unfamiliar
notation. The use of tensor notation will therefore be deferred until section 15.2.

The three principal geometric properties of the tensor are introduced in section 15.1.
These are the homography between two of the views induced by a plane back-projected
from a line in the other view; the relations between image correspondences for points
and lines which arise from incidence relations in 3-space; and the retrieval of the fun-
damental and camera matrices from the tensor.

The tensor may be used to transfer points from a correspondence in two views to the
corresponding point in a third view. The tensor also applies to lines, and the image of
a line in one view may be computed from its corresponding images in two other views.
Transfer is described in section 15.3.

The tensor only depends on the motion between views and the internal parameters
of the cameras and is deﬁned uniquely by the camera matrices of the views. However,
it can be computed from image correspondences alone without requiring knowledge of
the motion or calibration. This computation is described in chapter 16.

15.1 The geometric basis for the trifocal tensor

There are several ways that the trifocal tensor may be approached, but in this section
the starting point is taken to be the incidence relationship of three corresponding lines.

Incidence relations for lines. Suppose a line in 3-space is imaged in three views, as
in ﬁgure 15.1, what constraints are there on the corresponding image lines? The planes
back-projected from the lines in each view must all meet in a single line in space, the
3D line that projects to the matched lines in the three images. Since in general three
arbitrary planes in space do not meet in a single line, this geometric incidence condition

365

366

15 The Trifocal Tensor

L

l

C

l /

/

C

l / /

/ /

C

Fig. 15.1. A line L in 3-space is imaged as the corresponding triplet l ↔ l(cid:1) ↔ l(cid:1)(cid:1)
by their centres, C, C(cid:1)
ﬁrst, second and third images all intersect in a single 3D line in space.

in three views indicated
, and image planes. Conversely, corresponding lines back-projected from the

, C(cid:1)(cid:1)

↔ l(cid:2)(cid:2)

i

= [A | a4], P(cid:2)(cid:2)

provides a genuine constraint on sets of corresponding lines. We will now translate this
geometric constraint into an algebraic constraint on the three lines.
We denote a set of corresponding lines as li ↔ l(cid:2)
i . Let the camera matrices for
the three views be P = [I | 0], as usual, and P(cid:2)
= [B | b4], where A and
B are 3 × 3 matrices, and the vectors ai and bi are the i-th columns of the respective
camera matrices for i = 1, . . . ,4 .
• a4 and b4 are the epipoles in views two and three respectively, arising from the ﬁrst
camera. These epipoles will be denoted by e(cid:2)
throughout this chapter, with
e(cid:2)
= P(cid:2)(cid:2)C, where C is the ﬁrst camera centre. (For the most part we will
not be concerned with the epipoles between the second and third views).
• A and B are the inﬁnite homographies from the ﬁrst to the second and third cameras
respectively.

= P(cid:2)C, e(cid:2)(cid:2)

and e(cid:2)(cid:2)

As has been seen in chapter 9, any set of three cameras is equivalent to a set with
P = [I | 0] under projective transformations of space. In this chapter we will be con-
cerned with properties (such as image coordinates and 3D incidence relations) that are
invariant under 3D projective transforms, so we are free to choose the cameras in this
form.

Now, each image line back-projects to a plane, as shown in ﬁgure 15.1. From result

8.2(p197) these three planes are

π(cid:2)

= P(cid:2)Tl(cid:2)

=

π(cid:2)(cid:2)

= P(cid:2)(cid:2)Tl(cid:2)(cid:2)

=

(cid:20)

(cid:21)

l
0

π = PTl =

(cid:20)

(cid:21)

ATl(cid:2)
l(cid:2)
aT
4

(cid:20)

(cid:21)

.

BTl(cid:2)(cid:2)
l(cid:2)(cid:2)
bT
4

Since the three image lines are derived from a single line in space, it follows that
these three planes are not independent but must meet in this common line in 3-space.
This intersection constraint can be expressed algebraically by the requirement that the
4 × 3 matrix M = [π, π(cid:2)
] has rank 2. This may be seen as follows. Points on the
line of intersection may be represented as X = αX1 + βX2, with X1 and X2 linearly
independent. Such points lie on all three planes and so πTX = π(cid:2)TX = π(cid:2)(cid:2)TX = 0. It

, π(cid:2)(cid:2)

367
follows that MTX = 0. Consequently M has a 2-dimensional null-space since MTX1 = 0
and MTX2 = 0.

15.1 The geometric basis for the trifocal tensor

This intersection constraint induces a relation amongst the image lines l, l(cid:2)

, l(cid:2)(cid:2)
the rank of M is 2, there is a linear dependence between its columns mi. Denoting

. Since

(cid:18)

(cid:17)

l ATl(cid:2) BTl(cid:2)(cid:2)
l(cid:2) bT
l(cid:2)(cid:2)
0 aT
4

4

M = [m1, m2, m3] =

the linear relation may be written m1 = αm2 + βm3. Then noting that the bottom
left hand element of M is zero, it follows that α = k(bT
) for
4
some scalar k. Applying this to the top 3-vectors of each column shows that (up to a
homogeneous scale factor)

) and β = −k(aT

l(cid:2)(cid:2)

l(cid:2)

4

l = (bT
4

l(cid:2)(cid:2)

)ATl(cid:2) − (aT

l(cid:2)

)BTl(cid:2)(cid:2)

= (l(cid:2)(cid:2)Tb4)ATl(cid:2) − (l(cid:2)Ta4)BTl(cid:2)(cid:2)

.

4

The i-th coordinate li of l may therefore be written as
= l(cid:2)T(aibT

i )l(cid:2) − l(cid:2)T(a4bT

i )l(cid:2)(cid:2)

li = l(cid:2)(cid:2)T(b4aT
and introducing the notation

4 )l(cid:2)(cid:2) − l(cid:2)T(a4bT

i )l(cid:2)(cid:2)

Ti = aibT

4

− a4bT

i

(15.1)

the incidence relation can be written

(15.2)
Deﬁnition 15.1. The set of three matrices {T1, T2, T3} constitute the trifocal tensor in
matrix notation.

.

li = l(cid:2)TTil(cid:2)(cid:2)

We introduce a further notation1. Denoting the ensemble of the three matrices Ti by

[T1, T2, T3], or more brieﬂy [Ti], this last relation may be written as

lT = l(cid:2)T[T1, T2, T3]l(cid:2)(cid:2)

(15.3)

).

i]l(cid:2)(cid:2)

, l(cid:2)TT3l(cid:2)(cid:2)

i] and [T(cid:2)(cid:2)

, l(cid:2)TT2l(cid:2)(cid:2)
i ]l(cid:2)
and l(cid:2)(cid:2)T = lT[T(cid:2)(cid:2)

is understood to represent the vector (l(cid:2)TT1l(cid:2)(cid:2)

where l(cid:2)T[T1, T2, T3]l(cid:2)(cid:2)
Of course there is no intrinsic difference between the three views, and so by analogy
with (15.3) there will exist similar relations l(cid:2)T = lT[T(cid:2)
. The three
tensors [Ti], [T(cid:2)
i ] exist, but are distinct. In fact, although all three tensors may
be computed from any one of them, there is no very simple relationship between them.
Thus, in fact there are three trifocal tensors existing for a given triple of views. Usually
one will be content to consider only one of them. However, a method of computing the
other trifocal tensors [T(cid:2)
i ] given [Ti] is outlined in exercise (viii) on page 389.
Note that (15.3) is a relationship between image coordinates only, not involving 3D
coordinates. Hence (as remarked previously), although it was derived under the as-
sumption of a canonical camera set (that is P = [I | 0]), the value of the matrix ele-
ments [Ti] is independent of the form of the cameras. The particular simple formula
(15.1) for the trifocal tensor given the camera matrices holds only in the case where

i] and [T(cid:2)(cid:2)

1 This notation is somewhat cumbersome, and its meaning is not quite self-evident. It is for this reason that tensor notation is

introduced in section 15.2.

368

15 The Trifocal Tensor

/

C

image 2

/

l

C

x

image 1

x //

//

C

X

image 3

π

/

Fig. 15.2. Point transfer. A line l(cid:1)
x in the ﬁrst image deﬁnes a ray in 3-space which intersects π(cid:1)
imaged as the point x(cid:1)(cid:1)
third views, deﬁned by its back-projected plane π(cid:1)

in the third view. Thus, any line l(cid:1)

.

in the second view back-projects to a plane π(cid:1)

in 3-space. A point
in the point X. This point X is then
induces a homography between the ﬁrst and

P = [I | 0], but a general formula (17.12–p415) for the trifocal tensor corresponding to
any three cameras will be derived later.

Degrees of freedom. The trifocal tensor consists of three 3 × 3 matrices, and thus
has 27 elements. There are therefore 26 independent ratios apart from the (common)
overall scaling of the matrices. However, the tensor has only 18 independent degrees
of freedom. In other words once 18 parameters are speciﬁed, all 27 elements of the
tensor are determined up to a common scale. The number of degrees of freedom may
be computed as follows. Each of 3 camera matrices has 11 degrees of freedom, which
makes 33 in total. However, 15 degrees of freedom must be subtracted to account for
the projective world frame, thus leaving 18 degrees of freedom. The tensor therefore
satisﬁes 26 − 18 = 8 independent algebraic constraints. We return to this point in
chapter 16.

15.1.1 Homographies induced by a plane
A fundamental geometric property encoded in the trifocal tensor is the homography
between the ﬁrst view and the third induced by a line in the second image. This is
illustrated in ﬁgure 15.2 and ﬁgure 15.3. A line in the second view deﬁnes (by back-
projection) a plane in 3-space, and this plane induces a homography between the ﬁrst
and third views.

We now derive the algebraic representation of this geometry in terms of the trifocal
tensor. The homography map between the ﬁrst and third images, deﬁned by the plane
π(cid:2)
= Hx and (2.6–p36) l = HTl(cid:2)(cid:2)
in ﬁgure 15.2 and ﬁgure 15.3, may be written as x(cid:2)(cid:2)
respectively. Notice that the three lines l, l(cid:2)
in ﬁgure 15.3 are a corresponding
line triple, the projections of the 3D line L. Therefore, they satisfy the line incidence
relationship li = l(cid:2)TTil(cid:2)(cid:2)
shows that

of (15.2). Comparison of this formula and l = HTl(cid:2)(cid:2)

and l(cid:2)(cid:2)

H = [h1, h2, h3] with hi = TT

i

l(cid:2)

.

15.1 The geometric basis for the trifocal tensor

369

/

C

image 2

/

l

L

l//

//

C

image 3

π

/

C

l

image 1

Fig. 15.3. Line transfer. The action on lines of the homography deﬁned by ﬁgure 15.2 may similarly be
visualized geometrically. A line, l, in the ﬁrst image deﬁnes a plane in 3-space, which intersects π(cid:1)
in
the line L. This line L is then imaged as the line l(cid:1)(cid:1)

in the third view.

Thus, H deﬁned by the above formula represents the (point) homography H13 between
views one and three speciﬁed by the line l(cid:2)

in view two.

The second and third views play similar roles, and the homography between the ﬁrst
and second views deﬁned by a line in the third can be derived in a similar manner.
These ideas are formalized in the following result.
Result 15.2. The homography from the ﬁrst to the third image induced by a line l(cid:2)
the second image (see ﬁgure 15.2) is given by x(cid:2)(cid:2)
1 , TT

= H13(l(cid:2)
3 ]l(cid:2)
2 , TT

) x, where

in

H13(l(cid:2)

) = [TT

.

Similarly, a line l(cid:2)(cid:2)
ﬁrst to the second views, given by

in the third image deﬁnes a homography x(cid:2)

= H12(l(cid:2)(cid:2)

) x from the

H12(l(cid:2)(cid:2)

) = [T1, T2, T3]l(cid:2)(cid:2)

.

Once this mapping is understood the algebraic properties of the tensor are straight-
forward and can easily be generated. In the following section we deduce a number of
incidence relations between points and lines based on (15.3) and result 15.2.

15.1.2 Point and line incidence relations
It is easy to deduce various linear relationships between lines and points in three im-
ages involving the trifocal tensor. We have seen one such relationship already, namely
(15.3). This relation holds only up to scale since it involves homogeneous quantities.
We may eliminate the scale factor by taking the vector cross product of both sides,
which must be zero. This leads to the formula
(l(cid:2)T[T1, T2, T3]l(cid:2)(cid:2)

)[l]× = 0T,

(15.4)

where we have used the matrix [l]× to denote the cross product (see (A4.5–p581)), or
more brieﬂy (l(cid:2)T[Ti]l(cid:2)(cid:2)
– swapping the

)[l]× = 0T. Note the symmetry between l(cid:2)

and l(cid:2)(cid:2)

370

15 The Trifocal Tensor

)[l]× = 0T.

roles of these two lines is accounted for by transposing each Ti, resulting in a relation
(l(cid:2)(cid:2)T[TT
i ]l(cid:2)
(cid:27)

Now, a point x on the line l must satisfy
i xili = 0 (using upper indices for the point coordinates, foreshadowing the

Consider again ﬁgure 15.3.

xTl =
(cid:7)
use of tensor notation). Since li = l(cid:2)TTil(cid:2)(cid:2)

, this may be written as
xiTi)l(cid:2)(cid:2)

= 0

l(cid:2)T(

i

and l(cid:2)(cid:2)

(cid:27)
(15.5)
i xiTi) is simply a 3 × 3 matrix). This is an incidence relation in the
(note that (
ﬁrst image: the relationship will hold for a point–line–line correspondence – that is
whenever some 3D line L maps to l(cid:2)
in the second and third images, and to a
line passing through x in the ﬁrst image. An important equivalent deﬁnition of a point–
line–line correspondence for which (15.5) holds results from an incidence relation in
3-space – there exists a 3D point X mapping to x in the ﬁrst image, and to points on
the lines l(cid:2)

in the second and third images as shown in ﬁgure 15.4(a).

in the second
and third images. Consider a point–line–point correspondence as in ﬁgure 15.4(b) so
that

From result 15.2 we may obtain relations involving points x(cid:2)
(cid:7)

= H13(l(cid:2)
which is valid for any line l(cid:2)
in the second image. The homogeneous
scale factor may be eliminated by (post-)multiplying the transpose of both sides by
[x(cid:2)(cid:2)

) x = [TT
1
passing through x(cid:2)
(cid:7)

i )l(cid:2)
xiTT

and x(cid:2)(cid:2)

]× to give

and l(cid:2)(cid:2)

] x = (

, TT
2

x(cid:2)(cid:2)

l(cid:2)

l(cid:2)

l(cid:2)

, TT
3

i

x(cid:2)(cid:2)T[x(cid:2)(cid:2)

]× = l(cid:2)T(

xiTi)[x(cid:2)(cid:2)

]× = 0T,

(15.6)

i

A similar analysis may be undertaken with the roles of the second and third images
swapped.

Finally, for a 3-point correspondence as shown in ﬁgure 15.4(c), there is a relation

(cid:7)

i

]×(

]× = 03×3.

xiTi)[x(cid:2)(cid:2)

[x(cid:2)
(cid:27)
i xiTi)[x(cid:2)(cid:2)
and so is independent of y(cid:2)

in (15.6) passes through x(cid:2)
The line l(cid:2)
]×y(cid:2)
for some point y(cid:2)
]×(
through x(cid:2)

= [x(cid:2)
]× = y(cid:2)T[x(cid:2)

on l(cid:2)

and so may be written
from (15.6)
]× = 0T. However, the relation (15.6) is
. The relation (15.7) then

Consequently,

.

(15.7)

(cid:27)
Proof.
= x(cid:2) × y(cid:2)
as l(cid:2)
l(cid:2)T(
i xiTi)[x(cid:2)(cid:2)
true for all lines l(cid:2)
follows.

The various relationships between lines and points in three views are summarized in
table 15.1, and their properties are investigated further in section 15.2.1, once tensor
notation has been introduced. Note that there are no relations listed for point–line–line
correspondence in which the point is in the second or third view. Such simple relations
do not exist in terms of the trifocal tensor in which the ﬁrst view is the special view.
It is also worth noting that satisfying an image incidence relation does not guarantee
incidence in 3-space, as illustrated in ﬁgure 15.5.

15.1 The geometric basis for the trifocal tensor

371

x

x

x

C

C

C

l//

x//

x//

x / /

L

X

/

l

/x

/

C

(a) point–line–line

l/

x/

/

C

(b) point–line–point

L

X

X

x /

/

C

(c) point–point–point

/ /

C

/ /

C

/ /

C

are any two lines through x(cid:1)

and x(cid:1)(cid:1)

(a) Consider a 3-view point correspondence x ↔ x(cid:1) ↔ x(cid:1)(cid:1)

.

If l(cid:1)
Fig. 15.4. Incidence relations.
and l(cid:1)(cid:1)
forms a point–line–line
correspondence, corresponding to a 3D line L. Consequently, (15.5) holds for any choice of lines l(cid:1)
through x(cid:1)
. (b) The space point X is incident with the space line L. This deﬁnes an
incidence relation x ↔ l(cid:1) ↔ x(cid:1)(cid:1)
arising
from the image of a space point X.

between their images. (c) The correspondence x ↔ x(cid:1) ↔ x(cid:1)(cid:1)

respectively, then x ↔ l(cid:1) ↔ l(cid:1)(cid:1)

and l(cid:1)(cid:1)

through x(cid:1)(cid:1)

We now begin to extract the two-view geometry, the epipoles and fundamental ma-

trix, from the trifocal tensor.

15.1.3 Epipolar lines
A special case of a point–line–line correspondence occurs when the plane π(cid:2)
projected from l(cid:2)

back-
is an epipolar plane with respect to the ﬁrst two cameras, and hence

372

15 The Trifocal Tensor

(i) Line–line–line correspondence
l(cid:1)T[T1, T2, T3]l(cid:1)(cid:1)
(ii) Point–line–line correspondence
xiTi)l(cid:1)(cid:1)

l(cid:1)T(

(iii) Point–line–point correspondence

$

l(cid:1)T[T1, T2, T3]l(cid:1)(cid:1)%

= lT

or

[l]× = 0T

= 0 for a correspondence x ↔ l(cid:1) ↔ l(cid:1)(cid:1)

(cid:7)
(cid:7)

i

i

(cid:7)

i

l(cid:1)T(

xiTi)[x(cid:1)(cid:1)

]× = 0T for a correspondence x ↔ l(cid:1) ↔ x(cid:1)(cid:1)

(iv) Point–point–line correspondence
xiTi)l(cid:1)(cid:1)

[x(cid:1)

]×(

(v) Point–point–point correspondence

= 0 for a correspondence x ↔ x(cid:1) ↔ l(cid:1)(cid:1)
(cid:7)

[x(cid:1)

]×(

xiTi)[x(cid:1)(cid:1)

]× = 03×3

i

Table 15.1. Summary of trifocal tensor incidence relations using matrix notation.

L

X

x

C

/

l

/

C

x//

/ /

C

Fig. 15.5. Non-incident conﬁguration. The imaged points and lines of this conﬁguration satisfy the
point–line–point incidence relation of table 15.1. However, the space point X and line L are not incident.
Compare with ﬁgure 15.4.

back-projected from a line l(cid:2)(cid:2)

; then the ray deﬁned by X and C lies in this plane, and l(cid:2)

passes through the camera centre C of the ﬁrst camera. Suppose X is a point on the
plane π(cid:2)
is the epipolar line
corresponding to the point x, the image of X. This is shown in ﬁgure 15.6.
The plane π(cid:2)(cid:2)
in the third image will intersect the plane
in a line L. Further, since the ray corresponding to x lies entirely in the plane π(cid:2)
π(cid:2)
it
must intersect the line L. This gives a 3-way intersection between the ray and planes
and l(cid:2)(cid:2)
back-projected from point x and lines l(cid:2)
(cid:27)
, and so they constitute a point–line–line
correspondence, satisfying l(cid:2)T(
i xiTi)l(cid:2)(cid:2)
= 0. The important point now is that this is
, and it follows that l(cid:2)T(
true for any line l(cid:2)(cid:2)
i xiTi) =0 T. The same argument holds
with the roles of l(cid:2)
and l(cid:2)(cid:2)

reversed. To summarize:

(cid:27)

15.1 The geometric basis for the trifocal tensor

373

/

π

X

x

C

/

l

C /

e /

/

l /

C / /

Fig. 15.6. If the plane π(cid:1)
third view gives a point–line–line incidence.

deﬁned by l(cid:1)

is an epipolar plane for the ﬁrst two views, then any line l(cid:1)(cid:1)

in the

are the corresponding epipolar lines in the

Result 15.3. If x is a point and l(cid:2)
second and third images, then

and l(cid:2)(cid:2)

(cid:7)

xiTi) = 0T and (

xiTi)l(cid:2)(cid:2)

= 0.

(cid:7)

i

l(cid:2)T(

Consequently, the epipolar lines l(cid:2)
left and right null-vectors of the matrix

(cid:27)
and l(cid:2)(cid:2)

i

corresponding to x may be computed as the
i xiTi.

(cid:27)

As the point x varies, the corresponding epipolar lines vary, but all epipolar lines in
one image pass through the epipole. Thus, one may compute this epipole by computing
the intersection of the epipolar lines for varying values of x. Three convenient choices
of x are the points represented by homogeneous coordinates (1, 0, 0)T, (0, 1, 0)T and
(0, 0, 1)T, with
i xiTi equal to T1, T2 and T3 respectively for these three choices of x.
From this we deduce the following important result:
Result 15.4. The epipole e(cid:2)
in the second image is the common intersection of the
epipolar lines represented by the left null-vectors of the matrices Ti, i = 1, . . . ,3 .
Similarly the epipole e(cid:2)(cid:2)
is the common intersection of lines represented by the right
null-vectors of the Ti.
Note that the epipoles involved here are the epipoles in the second and third images
corresponding to the ﬁrst image centre C.

The usefulness of this result may not be apparent at present. However, it will be seen
below that it is an important step in computing the camera matrices from the trifocal
tensor, and in chapter 16 in the accurate computation of the trifocal tensor.

Algebraic properties of the Ti matrices. This section has established a number of
algebraic properties of the Ti matrices. We summarize these here:
• Each matrix Ti has rank 2. This is evident from (15.1) since Ti = aie(cid:2)(cid:2)T − e(cid:2)bT
sum of two outer products.
• The right null-vector of Ti is l(cid:2)(cid:2)
for the point x = (1, 0, 0)T, (0, 1, 0)T or (0, 0, 1)T, as i = 1, 2 or 3 respectively.

i is the
i = e(cid:2)(cid:2) × bi, and is the epipolar line in the third view

374

15 The Trifocal Tensor

i for i = 1, 2, 3.

is the common intersection of the epipolar lines l(cid:2)

is the common intersection of the epipolar lines l(cid:2)(cid:2)

i = e(cid:2) × ai, and is the epipolar line in the second view

• The epipole e(cid:2)(cid:2)
• The left null-vector of Ti is l(cid:2)
for the point x = (1, 0, 0)T, (0, 1, 0)T or (0, 0, 1)T, as i = 1, 2 or 3 respectively.
• The epipole e(cid:2)
i for i = 1, 2, 3.
• The sum of the matrices M(x) = (
of M(x) is the epipolar line l(cid:2)(cid:2)
epipolar line l(cid:2)
It’s worth emphasizing again that although a particular canonical form of the camera
is used in the derivation, the epipolar properties of the Ti matrices

i xiTi) also has rank 2. The right null-vector
of x in the third view, and its left null-vector is the

of x in the second view.

(cid:27)

matrices P, P(cid:2)
are independent of this choice.

and P(cid:2)(cid:2)

15.1.4 Extracting the fundamental matrices
It is simple to compute the fundamental matrices F21 and F31 between the ﬁrst1 and the
other views from the trifocal tensor. It was seen in section 9.2.1(p242) that the epipolar
line corresponding to some point can be derived by transferring the point to the other
view via a homography and joining the transferred point to the epipole. Consider a
point x in the ﬁrst view. According to ﬁgure 15.2 and result 15.2, a line l(cid:2)(cid:2)
in the
third view induces a homography from the ﬁrst to the second view given by x(cid:2)
=
) x. The epipolar line corresponding to x is then found by joining x(cid:2)
([T1, T2, T3]l(cid:2)(cid:2)
to
the epipole e(cid:2)
. This gives l(cid:2)

) x, from which it follows that

= [e(cid:2)

]× ([T1, T2, T3]l(cid:2)(cid:2)
F21 = [e(cid:2)

]×[T1, T2, T3]l(cid:2)(cid:2)

.

This formula holds for any vector l(cid:2)(cid:2)
generate condition where l(cid:2)(cid:2)
since as has been seen e(cid:2)(cid:2)
the formula

, but it is important to choose l(cid:2)(cid:2)

to avoid the de-
lies in the null-space of any of the Ti. A good choice is e(cid:2)(cid:2)
is perpendicular to the right null-space of each Ti. This gives

F21 = [e(cid:2)
A similar formula holds for F31 = [e(cid:2)(cid:2)

]×[T1, T2, T3]e(cid:2)(cid:2)
3 ]e(cid:2)
1 , TT
]×[TT
.

2 , TT

.

(15.8)

15.1.5 Retrieving the camera matrices
It was remarked that the trifocal tensor, since it expresses a relationship between im-
age entities only, is independent of 3D projective transformations. Conversely, this
implies that the camera matrices may be computed from the trifocal tensor only up to
a projective ambiguity. It will now be shown how this may be done.
Just as in the case of reconstruction from two views, because of the projective am-
biguity, the ﬁrst camera may be chosen as P = [I | 0]. Now, since F21 is known (from
(15.8)), we can make use of result 9.9(p254) to derive the form of the second camera
as

P(cid:2)

= [[T1, T2, T3]e(cid:2)(cid:2) | e(cid:2)

]

and the camera pair {P, P(cid:2)} then has the fundamental matrix F21. It might be thought
1 The fundamental matrix F21 satisﬁes x(cid:1)TF21x = 0 for corresponding points x ↔ x(cid:1)

. The subscript notation refers to

ﬁgure 15.8.

15.1 The geometric basis for the trifocal tensor

375

Given the trifocal tensor written in matrix notation as [T1, T2, T3].

(i) Retrieve the epipoles e(cid:1)

, e(cid:1)(cid:1)

Let ui and vi be the left and right null-vectors respectively of Ti, i.e. uT
i Ti = 0T,
Tivi = 0. Then the epipoles are obtained as the null-vectors to the following 3 × 3
matrices:

e(cid:1)T[u1, u2, u3] = 0 and e(cid:1)(cid:1)T[v1, v2, v3] = 0.

(ii) Retrieve the fundamental matrices F21, F31

F21 = [e(cid:1)

]×[T1, T2, T3]e(cid:1)(cid:1)

and F31 = [e(cid:1)(cid:1)
(with P = [I | 0])

]×[TT

1 , TT

2 , TT

3 ]e(cid:1)

.

(iii) Retrieve the camera matrices P(cid:1)

, P(cid:1)(cid:1)

Normalize the epipoles to unit norm. Then
] and P(cid:1)(cid:1)

= [[T1, T2, T3]e(cid:1)(cid:1) | e(cid:1)

P(cid:1)

= [(e(cid:1)(cid:1)e(cid:1)(cid:1)T − I)[TT

1 , TT

2 , TT

3 ]e(cid:1) | e(cid:1)(cid:1)

].

Algorithm 15.1. Summary of F and P retrieval from the trifocal tensor. Note, F21 and F31 are
determined uniquely. However, P(cid:1)
are determined only up to a common projective transformation
of 3-space.

and P(cid:1)(cid:1)

2 , TT

1 , TT

= [[TT

, P(cid:2)(cid:2)} is inconsistent.

3 ]e(cid:2) | e(cid:2)(cid:2)
that the third camera could be chosen in a similar manner as P(cid:2)(cid:2)
],
but this is incorrect. This is because the two camera pairs {P, P(cid:2)} and {P, P(cid:2)(cid:2)} do not
necessarily deﬁne the same projective world frame; although each pair is correct by
itself, the triple {P, P(cid:2)
The third camera cannot be chosen independently of the projective frame of the ﬁrst
two. To see this, suppose the camera pair {P, P(cid:2)} is chosen and points Xi reconstructed
from their image correspondences xi ↔ x(cid:2)
i. Then the coordinates of Xi are speciﬁed
in the projective world frame deﬁned by by {P, P(cid:2)}, and a consistent camera P(cid:2)(cid:2)
may
be computed from the correspondences Xi ↔ x(cid:2)(cid:2)
depends on the frame
deﬁned by {P, P(cid:2)}. However, it is not necessary to explicitly reconstruct 3D structure,
a consistent camera triplet can be recovered from the trifocal tensor directly.
The pair of camera matrices P = [I | 0] and P(cid:2)
] are not the only
ones compatible with the given fundamental matrix F21. According to (9.10–p256), the
most general form for P(cid:2)

= [[T1, T2, T3]e(cid:2)(cid:2) | e(cid:2)

i . Clearly, P(cid:2)(cid:2)

is

P(cid:2)

= [[T1, T2, T3]e(cid:2)(cid:2)

+ e(cid:2)vT|λe(cid:2)
]
for some vector v and scalar λ. A similar choice holds for P(cid:2)(cid:2)
matrices compatible with the trifocal tensor, we need to ﬁnd the correct values of P(cid:2)
P(cid:2)(cid:2)

. To ﬁnd a triple of camera
and
from these families so as to be compatible with the form (15.1) of the trifocal tensor.
= [[T1, T2, T3]e(cid:2)(cid:2)|e(cid:2)
Because of the projective ambiguity, we are free to choose P(cid:2)
],
is now deﬁned
and b4 =

. This choice ﬁxes the projective world frame so that P(cid:2)(cid:2)

thus ai = Tie(cid:2)(cid:2)
uniquely (up to scale). Then substituting into (15.1) (observing that a4 = e(cid:2)
e(cid:2)(cid:2)

)

from which it follows that e(cid:2)bT

Ti = Tie(cid:2)(cid:2)e(cid:2)(cid:2)T − e(cid:2)bT
i = Ti(e(cid:2)(cid:2)e(cid:2)(cid:2)T − I). Since the scale may be chosen such

i

376

that (cid:10)e(cid:2)(cid:10) = e(cid:2)Te(cid:2)

15 The Trifocal Tensor

= 1, we may multiply on the left by e(cid:2)T and transpose to get

= [(e(cid:2)(cid:2)e(cid:2)(cid:2)T − I)[TT

so P(cid:2)(cid:2)
the camera matrices from the trifocal tensor is given in algorithm 15.1.

1 , TT

2 , TT

]. A summary of the steps involved in extracting

bi = (e(cid:2)(cid:2)e(cid:2)(cid:2)T − I)TT
3 ]e(cid:2)|e(cid:2)(cid:2)

i

e(cid:2)

We have seen that the trifocal tensor may be computed from the three camera matri-
ces, and that conversely the three camera matrices may be computed, up to projective
equivalence, from the trifocal tensor. Thus, the trifocal tensor completely captures the
three cameras up to projective equivalence.

15.2 The trifocal tensor and tensor notation

The style of notation that has been used up to now for the trifocal tensor is derived
from the standard matrix–vector notation. Since a matrix has two indices only, it is
possible to distinguish between the two indices using the devices of matrix transposi-
tion and right or left multiplication, and in dealing with matrices and vectors, one can
do without writing the indices explicitly. Because the trifocal tensor has three indices,
instead of the two indices that a matrix has, it becomes increasingly cumbersome to
persevere with this style of matrix notation, and we now turn to using standard tensor
notation when dealing with the trifocal tensor. For those unfamiliar with tensor nota-
tion a gentle introduction is given in appendix 1(p562). This appendix should be read
before proceeding with this chapter.

Image points and lines are represented by homogeneous column and row 3-vectors,
respectively, i.e. x = (x1, x2, x3)T and l = (l1, l2, l3). The ij-th entry of a matrix A is
denoted by ai
j, index i being the contravariant (row) index and j being the covariant
(column) index. We observe the convention that indices repeated in the contravariant
and covariant positions imply summation over the range (1, . . . ,3 ) of the index. For
example, the equation x(cid:2)
jxj, which may be written
(cid:2)i = ai
x
We begin with the deﬁnition of the trifocal tensor given in (15.1). Using tensor

= Ax is equivalent to x

(cid:2)i =

(cid:27)

jxj.

j ai

notation, this becomes

T jk
i = aj

i bk
4

− aj
4bk
i .

(15.9)

The positions of the indices in T jk
(two contravariant and one covariant) are dictated by
the positions of the indices on the right side of the equation. Thus, the trifocal tensor is
a mixed contravariant–covariant tensor. In tensor notation, the basic incidence relation
(15.3) becomes

i

.

(15.10)

li = l

(cid:2)(cid:2)
(cid:2)
jl
k

i

T jk
(cid:7)

(cid:7)

Note that when multiplying tensors the order of the entries does not matter, in contrast
with standard matrix notation. For instance the right side of the above expression is

(cid:2)(cid:2)
(cid:2)
l
jl
k

T jk
i =

(cid:2)(cid:2)
(cid:2)
l
jl
k

T jk
i =

(cid:2)
l
j

T jk

i

(cid:2)
(cid:2)(cid:2)
l
k = l
j

T jk

i

(cid:2)(cid:2)
l
k .

j,k

j,k

15.2 The trifocal tensor and tensor notation

377

Deﬁnition. The trifocal tensor T is a valency 3 tensor T jk
i with two contravariant and one
covariant indices. It is represented by a homogeneous 3 × 3 × 3 array (i.e. 27 elements). It has
18 degrees of freedom.

Computation from camera matrices.
P = [I | 0], P(cid:1)

If the canonical 3 × 4 camera matrices are
j], P(cid:1)(cid:1)

= [bi
j]

= [ai

then

T jk
i = a

4 − a
j
i bk

j
4bk
i .

See (17.12–p415) for computation from three general camera matrices.

Line transfer from corresponding lines in the second and third views to
the ﬁrst.

Transfer by a homography.

li = l

(cid:1)
jl

kT jk
(cid:1)(cid:1)

i

(i) Point transfer from ﬁrst to third view via a plane in the second

The contraction l
induced by a plane deﬁned by the back-projection of the line l(cid:1)

is a homography mapping between the ﬁrst and third views

in the second view.

i

jT jk
(cid:1)

kT jk
(cid:1)(cid:1)

i

(cid:1)(cid:1)k = hk

i xi where hk

i = l

x

j T jk
(cid:1)

i

(ii) Point transfer from ﬁrst to second view via a plane in the third

The contraction l
induced by a plane deﬁned by the back-projection of the line l(cid:1)(cid:1)

is a homography mapping between the ﬁrst and second views

in the third view.

(cid:1)j = h
x

j
j
i xi where h
i = l

k T jk
(cid:1)(cid:1)

i

Table 15.2. Deﬁnition and transfer properties of the trifocal tensor.

The homography maps of ﬁgure 15.2 and ﬁgure 15.3 may be deduced from the in-
cidence relation (15.10). In the case of the plane deﬁned by back-projecting the line
l(cid:2)
,

(cid:2)(cid:2)
(cid:2)
jl
k

T jk
i = l

(cid:2)
(cid:2)(cid:2)
k(l
j

T jk

i

li = l

) = l

(cid:2)(cid:2)
k hk

i where hk

(cid:2)
i = l
j

T jk

i

i are the elements of the homography matrix H. This homography maps points

and hk
between the ﬁrst and third view as

(cid:2)(cid:2)k = hk

i xi.

x

Note that the homography is obtained from the tensor by contraction with a line (i.e. a
summation over one contravariant (upper) index of the tensor, and the covariant (lower)
extracts a 3 × 3 matrix from the tensor – think of the trifocal
index of the line), i.e. l(cid:2)
tensor as an operator which takes a line and produces a homography matrix. Table 15.2
summarizes the deﬁnition and transfer properties of the trifocal tensor.

A pair of particularly important tensors are ijk and its contravariant counterpart ijk,
deﬁned in section A1.1(p563). This tensor is used to represent the vector product. For

378

15 The Trifocal Tensor

(i) Line–line–line correspondence

(ii) Point–line–line correspondence

(lrris)l

(cid:1)
jl

kT jk
(cid:1)(cid:1)

i = 0s

(cid:1)
jl

kT jk
(cid:1)(cid:1)

i = 0

xil

(iii) Point–line–point correspondence
xil

(iv) Point–point–line correspondence

(cid:1)
j(x

(cid:1)(cid:1)k kqs)T jq

i = 0s

xi(x

(cid:1)jjpr)l

k T pk
(cid:1)(cid:1)

i = 0r

(v) Point–point–point correspondence

(cid:1)j jpr)(x

(cid:1)(cid:1)k kqs)T pq

xi(x

i = 0rs

Table 15.3. Summary of trifocal tensor incidence relations – the trilinearities.

instance, the line joining two points xi and yj is equal to the cross product xiyjijk =
lk, and the skew-symmetric matrix [x]× is written as xiirs in tensor notation. It is
now relatively straightforward to write down the basic incidence results involving the
trifocal tensor given in table 15.1. The results are summarized in table 15.3. In this
table, a notation such as 0r represents an array of zeros.
The form of the relations in table 15.3 is more easily understood if one observes
that three indices i, j and k in T jk
correspond to entities in the ﬁrst, second and third
views respectively. Thus for instance a partial expression such as l
cannot occur,
because the index j belongs to the second view, and hence does not belong on the line
l(cid:2)(cid:2)
in the third view. Repeated indices (indicating summation) must occur once as a
contravariant (upper) index and once as a covariant (lower) index. Thus, we cannot
write x
, since the index j occurs twice in the upper position. Think of the  tensor
as being used to raise or lower indices, for instance by replacing l(cid:2)
j by xiijk. However,
this may not be done arbitrarily, as pointed out in exercise (x) on page 389.

(cid:2)jT jk

T jk

(cid:2)(cid:2)
j

i

i

i

15.2.1 The trilinearities
The incidence relations in table 15.3 are trilinear relations or trilinearities in the co-
ordinates of the image elements (points and lines). Tri- since every monomial in
the relation involves a coordinate from each of the three image elements involved;
and linear because the relations are linear in each of the algebraic entities (i.e. the
three “arguments” of the tensor). For example in the point–point–point relation,
i = 0rs, suppose both x1 and x2 satisfy the relation, then so
xi(x
does x = αx1 + βx2, i.e. the relation is linear in its ﬁrst argument. Similarly, the
relation is linear in the second and third argument. This multi-linearity is a standard
T jk
(cid:2)(cid:2)
(cid:2)
property of tensors, and follows directly from the form xil
i = 0 which is a con-
jl
k
traction of the tensor over all three of its indices (arguments).

(cid:2)(cid:2)k kqs)T pq

(cid:2)j jpr)(x

We will now describe the point–point–point trilinearities in more detail. There are

15.3 Transfer

379

nine of these trilinearities arising from the three choices of r and s. Geometrically these
trilinearities arise from special choices of the lines in the second and third image for
the point–line–line relation (see ﬁgure 15.4(a)). Choosing r = 1, 2 or 3 corresponds to
a line parallel to the image x-axis, parallel to the image y-axis, or through the image
coordinate origin (the point (0, 0, 1)T), respectively. For example, choosing r = 1 and
expanding x

(cid:2)jjpr results in

(cid:2)
p = x
l

(cid:2)jjp1 = (0,−x

(cid:2)2)
which is a horizontal line in the second view through x(cid:2)
y(cid:2)
third view results in the vertical line through x(cid:2)(cid:2)
(cid:2)(cid:2)kkq2 = (x

(cid:2)3)T satisfy y(cid:2)Tl(cid:2)

(cid:2)(cid:2)3, 0,−x

(cid:2)1 + λ, x

(cid:2)(cid:2)
l
q = x

(cid:2)3, x

(cid:2)2, x

(cid:2)(cid:2)1)

= (x

(since points of the form
= 0 for any λ). Similarly, choosing s = 2 in the

and the trilinear point relation expands to
(cid:2)(cid:2)k jp1kq2T pq
− x
(cid:2)3(x

(cid:2)jx
= xi[−x

0 = xix

(cid:2)(cid:2)3 T 21

i

i

(cid:2)(cid:2)1 T 23

i

(cid:2)2(x

(cid:2)(cid:2)3 T 31

i

− x

(cid:2)(cid:2)1 T 33

i

)].

) +x

Of these nine trilinearities, four are linearly independent. This means that from
a basis of four trilinearities all nine can be generated by linear combinations. The
four degrees of freedom may be traced back to those of the point-line-line relation
T jk
i = 0 and are counted as follows. There is a one-parameter family of lines
xil
through x(cid:2)(cid:2)
are two members of this family, then any
other line through x(cid:2)(cid:2)

can be obtained from a linear combination of these:

in the third view. If m(cid:2)(cid:2)

and n(cid:2)(cid:2)

(cid:2)
(cid:2)(cid:2)
jl
k

l(cid:2)(cid:2)
The incidence relation is linear in l(cid:2)(cid:2)
(cid:2)
(cid:2)(cid:2)
l
jm
k
(cid:2)
(cid:2)(cid:2)
l
jn
k

+ βn(cid:2)(cid:2)
= αm(cid:2)(cid:2)
, so that given

.

T jk
i xi = 0
T jk
i xi = 0

then the incidence relation for any other line l(cid:2)(cid:2)
can be generated by a linear combi-
nation of these two. Consequently, there are only two linearly independent incidence
relations for l(cid:2)(cid:2)
, and the
incidence relation is also linear in lines l(cid:2)
. Thus, there are a total of four
linearly independent incidence relations between a point in the ﬁrst view and lines in
the second and third.

. Similarly there is a one-parameter family of lines through x(cid:2)

through x(cid:2)

The main virtue of the trilinearities is that they are linear, otherwise their properties

are often subsumed by transfer, as described in the following section.

15.3 Transfer

Given three views of a scene and a pair of matched points in two views one may wish
to determine the position of the point in the third view. Given sufﬁcient information
about the placement of the cameras, it is usually possible to determine the location of

380

image 1

x

F31

epipolar line 
from image 1

15 The Trifocal Tensor

image 2

image 3

a

x/

F 32

epipolar line 
from image 2

/

F x
32

F x
31

/ /

x

e

31

b

e

32

image 3

Fig. 15.7. Epipolar transfer. (a) The image of X in the ﬁrst two views is the correspondence x ↔ x(cid:1)
.
The image of X in the third view may be computed by intersecting the epipolar lines F31x and F32x(cid:1)
.
(b) The conﬁguration of the epipoles and transferred point x(cid:1)(cid:1)
as seen in the third image. Point x(cid:1)(cid:1)
is
computed as the intersection of epipolar lines passing through the two epipoles e31 and e32. However,
if x(cid:1)(cid:1)
lies on the line through the two epipoles, then its position cannot be determined. Points close to
the line through the epipoles will be estimated with poor precision.

the point in the third view without reference to image content. This is the point transfer
problem. A similar transfer problem arises for lines.

In principle the problem can generally be solved given the cameras for the three
views. Rays back-projected from corresponding points in the ﬁrst and second view
intersect and thus determine the 3D point. The position of the corresponding point
in the third view is computed by projecting this 3D point onto the image. Similarly
lines back-projected from the ﬁrst and second image intersect in the 3D line, and the
projection of this line in 3-space to the third image determines its image position.

15.3.1 Point transfer using fundamental matrices
The transfer problem may be solved using knowledge of the fundamental matrices
only. Thus, suppose we know the three fundamental matrices F21, F31 and F32 relating
the three views, and let points x and x(cid:2)
in the ﬁrst two views be a matched pair. We
wish to ﬁnd the corresponding point x(cid:2)(cid:2)
in the third image.

matches point x in the ﬁrst image, and consequently must lie
on the epipolar line corresponding to x. Since we know F31, this epipolar line may be
computed, and is equal to F31x. By a similar argument, x(cid:2)(cid:2)
must lie on the epipolar line
F32x(cid:2)
. Taking the intersection of the epipolar lines gives

The required point x(cid:2)(cid:2)

x(cid:2)(cid:2)

= (F31x) × (F32x(cid:2)

) .

See ﬁgure 15.7a.

Note that the fundamental matrix F21 is not used in this expression. The question
naturally arises whether we can gain anything by knowledge of F21, and the answer is
yes. In the presence of noise, the points x ↔ x(cid:2)
will not form an exact matched pair,
meaning that they will not satisfy the equation x(cid:2)TF21x = 0 exactly. Given F21 one may
use optimal triangulation as in algorithm 12.1(p318) to correct x and x(cid:2)
, resulting in a
pair ˆx ↔ ˆx(cid:2)
that satisﬁes this relation. The transferred point may then be computed as
x(cid:2)(cid:2)
). This method of point transfer using the fundamental matrices
will be called epipolar transfer.

= (F31ˆx) × (F32ˆx(cid:2)

15.3 Transfer

image 2
C

2

e23

e

21

381

C

1

e13

12e

image 1

e32

image 3

C

3

e31

Fig. 15.8. The trifocal plane is deﬁned by the three camera centres. The notation for the epipoles is
eij = PiCj. Epipolar transfer fails for any point X on the trifocal plane. If the three camera centres
are collinear then there is a one-parameter family of planes containing the three centres.

Though at one time used for point transfer, epipolar transfer has a serious deﬁciency
that rules it out as a practical method. This deﬁciency is due to the degeneracy that
can be seen from ﬁgure 15.7(b): epipolar transfer fails when the two epipolar lines in
the third image are coincident (and becomes increasingly ill-conditioned as the lines
become less “transverse”). The degeneracy condition that x(cid:2)(cid:2)
, e31 and e32 are collinear
in the third image means that the camera centres C and C(cid:2)
and the 3D point X lie in
a plane through the centre C(cid:2)(cid:2)
of the third camera; thus X lies on the trifocal plane
deﬁned by the three camera centres, see ﬁgure 15.8. Epipolar transfer will fail for
points X lying on the trifocal plane and will be inaccurate for points lying near that
plane. Note, in the special case that the three camera centres are collinear the trifocal
plane is not uniquely deﬁned, and epipolar transfer fails for all points. In this case
e31 = e32.

15.3.2 Point transfer using the trifocal tensor
The degeneracy of epipolar transfer is avoided by use of the trifocal tensor. Consider a
correspondence x ↔ x(cid:2)
is chosen in the second
view, then the corresponding point x(cid:2)(cid:2)
may be computed by transferring the point x
(cid:2)(cid:2)k = xil
from the ﬁrst to the third view using x
, from table 15.2. It is clear from
ﬁgure 15.4(p371)(b) that this transfer is not degenerate for general points X lying on
the trifocal plane.

passing through the point x(cid:2)

. If a line l(cid:2)

T jk

(cid:2)
j

i

However, note from result 15.3 and ﬁgure 15.6 that if l(cid:2)

T jk
i = 0k, so the point x(cid:2)(cid:2)

is the epipolar line corre-
(cid:2)
sponding to x, then xil
is undeﬁned. Consequently, the
choice of line l(cid:2)
j
is important. To avoid choosing only an epipolar line, one possibility
(cid:2)rrjp for the
is to use two or three different lines passing through x(cid:2)
three choices of p = 1, . . . ,3 . For each such line, one computes the value of x(cid:2)(cid:2)
and
retains the one that has the largest norm (i.e. is furthest from being zero). An alternative
method entirely for ﬁnding x(cid:2)(cid:2)
is as the least-squares solution of the system of linear
equations xi(x
i = 0rs, but this method is probably an overkill.
The method we recommend is the following. Before attempting to compute the point
transferred from a pair of points x ↔ x(cid:2)
x(cid:2)(cid:2)
, ﬁrst correct the pair of points using the
fundamental matrix F21, as described above in the case of epipolar transfer. If ˆx and ˆx(cid:2)

(cid:2)(cid:2)k kqs)T pq

(cid:2)j jpr)(x

(cid:2)
jp = x

, namely l

382

15 The Trifocal Tensor

B12

e

21

C

2

l /

image 2

B 23

C

3

//

x

X

π /

image 3

e

12

C
1

x

image 1

Fig. 15.9. Degeneracy for point transfer using the trifocal tensor. The 3D point X is deﬁned by the
intersection of the ray through x with the plane π(cid:1)
. A point X on the baseline B12 between the ﬁrst and
second views cannot be deﬁned in this manner. So a 3D point on the line B12 cannot be transferred to
the third view via a homography deﬁned by a line in the second view. Note that a point on the line B12
projects to e12 in the ﬁrst image and e21 in the second image. Apart from the line B12 any point can
be transferred. In particular there is not a degeneracy problem for points on the baseline B23, between
views two and three, or for any other point on the trifocal plane.

(cid:2)(cid:2)k = ˆxil

(cid:2)
j

T jk

chosen passing through ˆx(cid:2)

does not depend on the
are an exact match, then the transferred point x
line l(cid:2)
(as long as it is not the epipolar line). This may be
veriﬁed geometrically by referring to ﬁgure 15.2(p368). A good choice is always given
by the line perpendicular to F21ˆx.
To summarize, a measured correspondence x ↔ x(cid:2)
steps:

is transferred by the following

i

(i) Compute F21 from the trifocal tensor (by the method given in algorithm 15.1),
using algorithm 12.1-

to the exact correspondence ˆx ↔ ˆx(cid:2)

and correct x ↔ x(cid:2)
(p318).
(ii) Compute the line l(cid:2)
e = (l1, l2, l3)T and ˆx(cid:2)
l(cid:2)

(iii) The transferred point is x

through ˆx(cid:2)
= (ˆx1, ˆx2, 1)T, then l(cid:2)
(cid:2)
(cid:2)(cid:2)k = ˆxil
j

and perpendicular to l(cid:2)
T jk

e = F21ˆx.
= (l2,−l1,−ˆx1l2 + ˆx2l1)T.

.

i

If

Degenerate conﬁgurations. Consider transfer to the third view via a plane, as shown
in ﬁgure 15.9. The 3D point X is only undeﬁned if it lies on the baseline joining the
ﬁrst and second camera centres. This is because rays through x and x(cid:2)
are collinear
for such 3D points and so their intersection is not deﬁned. In such a case, the points x
and x(cid:2)
correspond with the epipoles in the two images. However, there is no problem
transferring a point lying on the baseline between views two and three, or anywhere
else on the trifocal plane. This is the key difference between epipolar transfer and
transfer using the trifocal tensor. The former is undeﬁned for any point on the trifocal
plane.

15.3.3 Line transfer using the trifocal tensor
Using the trifocal tensor, it is possible to transfer lines from a pair of images to a third
T jk
according to the line-transfer equation li = l
i of table 15.2. This gives an explicit

(cid:2)(cid:2)
(cid:2)
jl
k

15.4 The fundamental matrices for three views

383

are known in the ﬁrst and second views then l(cid:2)(cid:2)

formula for the line in the ﬁrst view, given lines in the other two views. Note however
that if the lines l and l(cid:2)
may be computed
T jk
by solving the set of linear equations (lrris)l
i = 0s, thereby transferring it into
the third image. Similarly one may transfer lines into the second image. Line transfer
is not possible using only the fundamental matrices.

(cid:2)(cid:2)
(cid:2)
jl
k

Degeneracies. Consider the geometry of ﬁgure 12.8(p322). The line L in 3-space is
deﬁned by the intersection of the planes through l and l(cid:2)
, namely π and π(cid:2)
respectively.
This line is clearly undeﬁned when the planes π and π(cid:2)
are coincident, i.e. in the case
of epipolar planes. Consequently, lines cannot be transferred between the ﬁrst and third
image if both l and l(cid:2)
are corresponding epipolar lines for the ﬁrst and second views.
T jk
i = 0, and the equation
Algebraically, the line-transfer equation gives li = l
T jk
used to solve for l(cid:2)(cid:2)
(cid:2)
becomes zero. It is quite common for lines to
matrix (lrris)l
j
be near epipolar, and their transfer is then inaccurate, so this condition should always
be checked for. There is an equivalent degeneracy for line transfer between views one
and two deﬁned by a line in view three. Again, it occurs if the lines in views one and
three are corresponding epipolar lines for these two views.

(cid:2)(cid:2)
(cid:2)
jl
k

i

In general the epipolar geometries between views one and two, and one and three
will differ, for instance the epipole e12 arising in the ﬁrst view from view two will not
coincide with the epipole e13 arising in the ﬁrst view from view three. Thus an epipolar
line in the ﬁrst view for views one and two will not coincide with an epipolar line for
views one and three. Consequently, when line transfer into the third view is degenerate,
line transfer into the second view will not in general be degenerate. However, for lines
in the trifocal plane transfer is degenerate (i.e. undeﬁned) always.

15.4 The fundamental matrices for three views

The three fundamental matrices F21, F31, F32 are not independent, but satisfy three rela-
tions:

eT
23

31

32

F21 e13 = eT

F31 e12 = 0.
These relations are easily seen from ﬁgure 15.8. For example, eT
F31 e12 = 0 follows
32
from the observation that e32 and e12 are matching points, corresponding to the centre
of camera number 2.

F32 e21 = eT

(15.11)

Projectively, the three-camera conﬁguration has 18 degrees of freedom counting 11
for each camera less 15 for an overall projective ambiguity. Alternatively, this may be
accounted for as 21 for the 3 × 7 degrees of freedom of the fundamental matrices less
3 for the relations. The trifocal tensor also has 18 degrees of freedom and fundamental
matrices computed from the trifocal tensor will automatically satisfy the three relations.
The counting argument implies that the three relations of (15.11) are sufﬁcient to
ensure consistency of three fundamental matrices. The counting argument alone is not
a convincing proof of this, however, so a proof is given below.

Deﬁnition 15.5. Three fundamental matrices F21, F31 and F32 are said to be compatible
if they satisfy the conditions (15.11).

384

15 The Trifocal Tensor

In most cases, these conditions are sufﬁcient to ensure that the three fundamental ma-
trices correspond to some geometric conﬁguration of cameras.

Theorem 15.6. Let a set of three fundamental matrices F21, F31 and F32 be given sat-
isfying the conditions (15.11). Assume also that e12 (cid:5)= e13, e21 (cid:5)= e23, and e31 (cid:5)= e32.
Then there exist three camera matrices P1, P2, P3 such that Fij is the fundamental matrix
corresponding to the pair (Pi, Pj).
(cid:5)= eik in this theorem ensure that the three cameras
are non-collinear. For this reason they will be referred to here as the non-collinearity
conditions. One may show by example (left to the reader) that these conditions are
necessary for the truth of the theorem.

Note that the conditions eij

Proof.
In this proof, the indices i, j and k are intended to be distinct. We begin by
choosing three points xi; i = 1, . . . ,3 , consistent with the three fundamental matrices.
In other words, we require that xT
Fijxj = 0 for all pairs (i, j). This is easily done
i
by choosing ﬁrst x1 and x2 to satisfy xT
F21x1 = 0, and then deﬁning x3 to be the
2
intersection of the two epipolar lines F32x2 and F31x1.
In a similar manner, we choose a second set of points yi; i = 1, . . . ,3 satisfying
yT
Fijyj = 0. This is done in such a way that the four points xi, yi, eij, eik in each
i
image i are in general position – that is no three are collinear. This is possible by the
assumption that the two epipoles in each image are distinct.
Next we choose ﬁve world points C1, C2, C3, X, Y in general position. For example,
one could take the usual projective basis. We may now deﬁne the three camera matri-
ces. Let the i-th camera matrix Pi satisfy the conditions

PiCi = 0; PiCj = eij; PiCk = eik; PiX = xi; PiY = yi.

In other words, the i-th camera has centre at Ci and maps the four other world points
Cj, Ck, X, Y to the four image points eij, eik, xi, yi. This uniquely determines the cam-
era matrix since the points are in general position. To see this, recall that the camera
matrix deﬁnes a homography between the image and the rays through the camera cen-
tre (a 2D projective space). The images of four points specify this homography com-
pletely. Let ˆFij be the fundamental matrix deﬁned by the pair of camera matrices Pi
and Pj. The proof is completed by proving that ˆFij = Fij for all i, j.
The epipoles of ˆFij and Fij are the same, by the way that Pi and Pj are constructed.
Consider the pencil of epipolar lines through eij in image i. This pencil forms a 1-
dimensional projective space of lines, and the fundamental matrix Fij induces a one-
to-one correspondence (in fact a homography) between this pencil and the pencil of
lines through eji in image j. The fundamental matrix ˆFij also induces a homography
between the same pencils. The two fundamental matrices are the same if the homogra-
phies they induce are the same.
Two 1-dimensional homographies are the same if they agree on three points (or in this
case epipolar lines). The relation xT
Fijxj = 0 means that the epipolar lines through
i
xi in image i and xj in image j correspond under the homography induced by Fij. By
construction xiˆFijxj = 0 as well, since xi and xj are the projections of the point X in

15.4 The fundamental matrices for three views

385

the two images. Thus, both homographies agree on this pair of epipolar lines. In the
same way, the homographies induced by Fij and ˆFij agree on the epipolar lines corre-
sponding to the pairs yi ↔ yj and eik ↔ ejk. The two homographies therefore agree
on three lines in the pencil and hence are equal; so are the corresponding fundamental
matrices. (We are grateful to Frederik Schaffalitzky for this proof).

15.4.1 Uniqueness of camera matrices given three fundamental matrices
The proof just given shows that there is at least one set of cameras corresponding to
three compatible fundamental matrices (provided they satisfying the non-collinearity
condition). It is important to know that the three fundamental matrices determine the
conﬁguration of the three cameras uniquely, at least up to the unavoidable projective
ambiguity. This will be shown next.

The ﬁrst two camera matrices P and P(cid:2)

may be determined from the fundamental
matrix F21 by two-view techniques (chapter 9). It remains to determine the third camera
matrix P(cid:2)(cid:2)

in the same projective frame. In principle, this may be done as follows.

(i) Select a set of matching points xi ↔ x(cid:2)

i in the ﬁrst two images, satisfying
TF21xi = 0, and use triangulation to determine the corresponding 3D points

(ii) Use epipolar transfer to determine the corresponding points x(cid:2)(cid:2)

i in the third im-

x(cid:2)
i
Xi.

(iii) Solve for the camera matrix P(cid:2)(cid:2)

age, using the fundamental matrices F31 and F32.
Xi ↔ x(cid:2)(cid:2)
i .

from the set of 3D–2D correspondences

The second step in this algorithm will fail in the case where the point Xi lies in the
trifocal plane. Such a point Xi is easily detected and discarded, since it projects into
the ﬁrst image as a point xi lying on the line joining the two epipoles e12 and e13. Since
there are inﬁnitely many possible matched points, we can compute sufﬁciently many
such points to compute P(cid:2)(cid:2)

.

The only situation in which this method will fail is when all space points Xi lie in a
trifocal plane. This can occur only in the degenerate situation in which the three camera
centres are collinear, in which case the trifocal plane is not uniquely determined. Thus,
we see that unless the three camera centres are collinear, the three camera matrices
may be determined from the fundamental matrices. On the other hand, if the three
cameras are collinear, then there is no way to determine the relative spacings of the
cameras along the line of their centres. This is because the length of the baseline
cannot be determined from the fundamental matrices, and the three baselines (distances
between the camera centres) may be arbitrarily chosen and remain consistent with the
fundamental matrices. Thus we have demonstrated the following fact:

Result 15.7. Given three compatible fundamental matrices F21, F31 and F32 satisfying
and P(cid:2)(cid:2)
the non-collinearity condition, the three corresponding camera matrices P, P(cid:2)
are unique up to the choice of a 3D projective coordinate frame.

386

15 The Trifocal Tensor

15.4.2 Computation of camera matrices from three fundamental matrices
Given three compatible fundamental matrices, there exists a simple method for com-
puting a corresponding set of three camera matrices. From the fundamental matrix
F21, one can compute a corresponding pair of camera matrices (P, P(cid:2)
) using result 9.14-
(p256). Next, according to result 9.12(p255) the third camera matrix P(cid:2)(cid:2)
must satisfy
the condition that P(cid:2)(cid:2)TF31P and P(cid:2)(cid:2)TF32P(cid:2)
be skew-symmetric. Each of these matrices
gives rise to 10 linear equations in the entries of P(cid:2)(cid:2)
, a total of 20 equations in the 12
entries of P(cid:2)(cid:2)

may be computed linearly.

. From these, P(cid:2)(cid:2)

If the three fundamental matrices are compatible in the sense of deﬁnition 15.5 and
the non-collinearity condition of theorem 15.6 holds, then there will exist a solution,
and it will be unique.
If however the three fundamental matrices are computed in-
dependently from point correspondences, then they will not satisfy the compatibility
conditions exactly. In this case it will be necessary to compute a least-squares solution
to ﬁnd P(cid:2)(cid:2)
. The error being minimized is not geometrically based. It is best to use this
algorithm only when the fundamental matrices are known to be compatible.

One can think of doing three-view reconstruction by estimating the three fundamen-
tal matrices using pairwise point correspondences, then using the above algorithm to
estimate the three camera matrices. This is not a very good strategy, for the following
reasons.

(i) The method for computing the three camera matrices from the fundamental
matrices assumes that the fundamental matrices are compatible. Otherwise, a
least-squares problem involving a non-geometrically justiﬁed cost function is
involved.

(ii) Although result 15.7 shows that three fundamental matrices may determine the
camera geometry, and hence the trifocal tensor, this is only true when the cam-
eras are not collinear. As they approach collinearity, the estimate of the relative
camera placement becomes unstable.

The trifocal tensor is preferable to a triple of compatible fundamental matrices as a
means of determining the geometry of three views. This is because the difﬁculty with
the views being collinear is not an issue with the trifocal tensor. It is well deﬁned and
uniquely determines the geometry even for collinear cameras. The difference is that the
fundamental matrices do not contain a direct constraint on the relative displacements
between the three cameras, whereas this is built into the trifocal tensor.

Since the projective structure of the three cameras may be computed explicitly from
the trifocal tensor, it follows that all three fundamental matrices for the three view pairs
are determined by the trifocal tensor. In fact simple formulae, given in algorithm 15.1-
(p375) exist for the two fundamental matrices F21 and F31. The fundamental matrices
determined from the trifocal tensor will satisfy the compatibility conditions (15.11).

15.4.3 Camera matrices compatible with two fundamental matrices
Suppose we are given only two fundamental matrices F21 and F31. To what extent do
these ﬁx the geometry of the three cameras? It will be shown here that there are four

15.5 Closure

387

) compatible with the pair of fundamental matrices.

From F21 one may compute a pair of camera matrices (P, P(cid:2)

), and from F31 the pair
). In both cases we may choose P = [I | 0], resulting in a triple of camera matrices
, P(cid:2)(cid:2)

degrees of freedom in the solution for the camera matrices, beyond the usual projective
ambiguity.
(P, P(cid:2)(cid:2)
(P, P(cid:2)
However, the choice of the three camera matrices is not unique, since for any
matrices H1 and H2 representing 3D projective transforms, the pairs (PH1, P(cid:2)H1) and
(PH2, P(cid:2)(cid:2)H2) are also compatible with the same fundamental matrices. In order to pre-
serve the condition that P is equal to [I | 0] in each case, the form of Hi must be re-
stricted to:

(cid:17)

(cid:18)

.

Hi =

I
vT
i

0
ki

) and (P, P(cid:2)(cid:2)

We may now ﬁx on a particular choice of the ﬁrst two camera matrices (P, P(cid:2)

) com-
patible with F21. This is equivalent to ﬁxing on a speciﬁc projective coordinate frame.
The general solution for the camera matrices is then (P, P(cid:2)
, P(cid:2)(cid:2)H2), where H2 is of the
form given above and the two pairs (P, P(cid:2)
) are compatible with the two
fundamental matrices.
Allowing also for the overall projective ambiguity, the most general solution is
(PH, P(cid:2)H, P(cid:2)(cid:2)H2H), which gives a total of 19 degrees of freedom, 15 for the projective
transformation H and 4 for the degrees of freedom of H2. The same number of degrees
of freedom may be found using a counting argument as follows: two fundamental ma-
trices have 7 degrees of freedom each, for a total of 14. Three arbitrary camera matrices
on the other hand have 3 × 11 = 33 degrees of freedom. The 14 constraints imposed
by the two fundamental matrices leave 19 remaining degrees of freedom for the three
camera matrices.

15.5 Closure

The development of three-view geometry proceeds in an analogous manner to that of
two-view geometry covered in part II of this book. The trifocal tensor may be computed
from image correspondences over three views, and a projective reconstruction of the
cameras and 3D scene then follows. This computation is described in chapter 16.
The projective ambiguity may be reduced to afﬁne or metric by supplying additional
information on the scene or cameras in the same manner as that of chapter 10. A similar
development to that of chapter 13 may be given for the relations between homographies
induced by scene planes and the trifocal tensor.

15.5.1 The literature
With hindsight, the discovery of the trifocal tensor may be traced to [Spetsakis-91]
and [Weng-88], where it was used for scene reconstruction from lines in the case of
calibrated cameras. It was later shown in [Hartley-94d] to be equally applicable to
projective scene reconstruction in the uncalibrated case. At this stage matrix notation
was used, but [Vieville-93] used tensor notation for this problem.

388

15 The Trifocal Tensor

Meanwhile in independent work, Shashua introduced trilinearity conditions relat-
ing the coordinates of corresponding points in three views with uncalibrated cameras
[Shashua-94, Shashua-95a]. [Hartley-95b, Hartley-97a] then showed that Shashua’s re-
lation for points and scene reconstruction from lines both arise from a common tensor,
and the trifocal tensor was explicitly identiﬁed.

In subsequent work properties of

the tensor have been investigated, e.g.
[Shashua-95b]. In particular [Triggs-95] described the mixed covariant–contravariant
behaviour of the indices, and [Zisserman-96] described the geometry of the homogra-
phies encoded by the tensor. Faugeras and Mourrain [Faugeras-95a] gave enlightening
new derivations of the trifocal tensor equations and considered the trifocal tensor in
the context of general linear constraints involving multiple views. This approach will
be discussed in chapter 17. Further geometric properties of the tensor were given in
Faugeras & Papadopoulo [Faugeras-97].

Epipolar point transfer was described by [Barrett-92, Faugeras-94], and its deﬁcien-

cies pointed out by [Zisserman-94], amongst others.

The trifocal tensor has been used for various applications including establish-
ing correspondences in image sequences [Beardsley-96], independent motion detec-
tion [Torr-95a], and camera self-calibration [Armstrong-96a].

15.5.2 Notes and exercises

(i) The trifocal tensor is invariant to 3D projective transforms. Verify explicitly
that if H4×4 is a transform preserving the ﬁrst camera matrix P = [I | 0], then
the tensor deﬁned by (15.1–p367) is unchanged.

= [A | a4], P(cid:2)(cid:2)

(ii) In this chapter the starting point for the trifocal tensor derivation was the inci-
dence property of three corresponding lines. Show that alternatively the starting
point may be the homography induced by a plane.
Here is a sketch derivation: choose the camera matrices to be a canonical set
P = [I | 0], P(cid:2)
= [B | b4] and start from the homography H13 be-
tween the ﬁrst and third views induced by a plane π(cid:2)
. From result 13.1(p326)
this homography may be written as H13 = B − b4vT, where π(cid:2)T = (vT, 1). In
this case the plane is deﬁned by a line l(cid:2)
. Show
that result 15.2(p369) follows.
(iii) Homographies involving the ﬁrst view are simply expressed in terms of the
trifocal tensor T jk
as given by result 15.2(p369). Investigate whether a sim-
ple formula exists for the homography H23 from the second to the third view,
induced by a line l in the ﬁrst image.
is a 3 × 3 matrix. Show that this may be interpreted as
a correlation (see deﬁnition 2.29(p59)) mapping between the second and third
views induced by the line which is the back-projection of the point x in the ﬁrst
view.

(iv) The contraction xiT jk

in the second view as π(cid:2)

= P(cid:2)Tl(cid:2)

i

i

(v) Plane plus parallax over three views. There is a rich geometry associated
with the plane plus two points conﬁguration (see ﬁgure 13.9(p336)) over three
views: suppose the points off the (reference) plane are X and Y. Project the

, x(cid:2)(cid:2)

15.5 Closure

, and similarly project the point Y to the triangle y, y(cid:2)

389
point X onto the reference plane from each of the three camera centres to form
a triangle x, x(cid:2)
, y(cid:2)(cid:2)
.
Then the two triangles form a Desargues’s conﬁguration and are related by a
planar homology (see section A7.2(p629)). A simple sketch shows that the
lines joining corresponding triangle vertices, (x, y), (x(cid:2)
), are con-
current, and their intersection is the point at which the line joining X and Y
pierces the reference plane. Similarly the intersection points of corresponding
triangle sides are collinear, and the line so formed is the intersection of the tri-
focal plane of the cameras with the reference plane. Further details are given
in [Criminisi-98, Irani-98, Triggs-00b].

, y(cid:2)

), (x(cid:2)(cid:2)

, y(cid:2)(cid:2)

(vi) In the case where two of the three cameras have the same camera centre, the

trifocal tensor may be related to simpler entities. There are two cases.

the second and third camera have the same centre,

then
(a) If
T jk
i = FriHk
s rjs, where Fri is the fundamental matrix for the ﬁrst two
views, and H is the homography from the second to the third view in-
duced by the fact that they have the same centre.
e(cid:2)(cid:2)k,
i = Hj
where H is the homography from the ﬁrst to the second view and e(cid:2)(cid:2)
is
the epipole in the third image.

(b) If the ﬁrst and the second views have the same centre, then T jk

i

Prove these relationships using the approach of chapter 17.

(vii) Consider the case of a small baseline between the cameras and derive a differ-

ential form of the trifocal tensor, see [Astrom-98, Triggs-99b].

(viii) There are actually three different trifocal tensors relating three views, depend-
ing on which of the three cameras corresponds to the covariant index. Given
one such tensor [Ti], verify that the tensor [T(cid:2)
i] may be computed in several steps,
as follows:

(a) Extract the three camera matrices P = [I | 0], P(cid:2)
from the trifocal
tensor.
(b) Find a 3D projective transformation H such that P(cid:2)H = [I | 0], and apply
it to each of P and P(cid:2)(cid:2)
(c) Compute the tensor [T(cid:2)

as well.
i] by applying (15.1–p367).

and P(cid:2)(cid:2)

(ix) Investigate the form and properties (e.g. rank of the matrices Ti) of the trifocal
tensor for the special motions (pure translation, planar motion) described in
section 9.3(p247) for the fundamental matrix.

(x) Comparison of the incidence relationships of table 15.3(p378) indicates that
(cid:2)
one may replace a line l
r, and proceed similarly with
(cid:2)(cid:2)
k. Also, one gets a three-view line equation by replacing xi by irsli. Can both
l
of these operations be carried out at once to obtain an equation

(cid:2)
j by the expression jrsx

(cid:15)

(cid:16)(cid:15)

(cid:16)(cid:15)

(cid:2)j

iruli

jsvx

Why, or why not?

(cid:16)

(cid:2)(cid:2)k

ktwx

T st
r = 0u

vw?

390

15 The Trifocal Tensor

(xi) Afﬁne trifocal tensor. If the three cameras P, P(cid:2)

and P(cid:2)(cid:2)

are all afﬁne (deﬁnition
6.3(p166)), then the corresponding tensor TA is the afﬁne trifocal tensor. This
afﬁne specialization of the tensor has 12 degrees of freedom and 16 non-zero
entries. The afﬁne trifocal tensor was ﬁrst deﬁned in [Torr-95b], and has been
studied in [Kahl-98a, Quan-97a, Thorhallsson-99].
It shares with the afﬁne
fundamental matrix (chapter 14) very stable numerical estimation behaviour. It
has been shown to perform very well in tracking applications where the object
of interest (for example a car) has small relief compared to the depth of the
scene [Hayman-03, Tordoff-01].

16

Computation of the Trifocal Tensor T

This chapter describes numerical methods for estimating the trifocal tensor given a set
of point and line correspondences across three views. The development will be very
similar to that for the fundamental matrix, using much the same techniques as those of
chapter 11. In particular, ﬁve methods will be discussed:

(i) A linear method based on direct solution of a set of linear equations (after ap-

propriate data normalization) (section 16.2).

(ii) An iterative method, that minimizes the algebraic error, while satisfying all

appropriate constraints on the tensor (section 16.3).

(iii) An iterative method that minimizes geometric error (the “Gold Standard”

method) (section 16.4.1).

(iv) An iterative method that minimizes the Sampson approximation to geometric

error (section 16.4.3).

(v) Robust estimation based on RANSAC (section 16.6).

16.1 Basic equations

A complete set of the (tri-)linear equations involving the trifocal tensor is given in
table 16.1. All of these equations are linear in the entries of the trifocal tensor T .

Correspondence

three points

two points, one line

one point, two lines

xix

(cid:1)jx
xix

three lines

lpl

(cid:1)
ql

i = 0st

Relation

(cid:1)(cid:1)kjqskrtT qr
r jqsT qr
(cid:1)jl
(cid:1)(cid:1)
rT qr
(cid:1)
(cid:1)(cid:1)
xil
ql
r piwT qr
(cid:1)(cid:1)

i = 0
i = 0w

i = 0s

Number of equations

4

2

1

2

Table 16.1. Trilinear relations between point and line coordinates in three views. The ﬁnal column
denotes the number of linearly independent equations. The notation 0st means a 2-dimensional tensor
with all zero entries. Thus, the ﬁrst line in this table corresponds to a set of 9 equations, one for each
choice of s and t. However, among this set of 9 equations, only 4 are linearly independent.

391

392

16 Computation of the Trifocal Tensor T

Given several point or line correspondences between three images, the complete set
of equations generated is of the form At = 0, where t is the 27-vector made up of the
entries of the trifocal tensor. From these equations, one may solve for the entries of
the tensor. Note that equations involving points may be combined with those involving
lines – in general all available equations from table 16.1 may be used simultaneously.
Since T has 27 entries, 26 equations are needed to solve for t up to scale. With more
than 26 equations, a least-squares solution is computed. As with the fundamental ma-
trix, one minimizes (cid:10)At(cid:10) subject to the constraint (cid:10)t(cid:10) = 1 using algorithm A5.4(p593).
This gives a bare outline of a linear algorithm for computing the trifocal tensor.
However, in order to build a practical algorithm out of this several issues, such as
normalization, need to be addressed. In particular the tensor that is estimated must
obey various constraints, and we consider these next.

16.1.1 The internal constraints

The most notable difference between the fundamental matrix and the trifocal tensor is
the greater number of constraints that apply to the trifocal tensor. The fundamental
matrix has a single constraint, namely det(F) = 0, leaving 7 degrees of freedom, dis-
counting the arbitrary scale factor. The trifocal tensor, on the other hand, has 27 entries,
but 18 parameters only are required to specify the equivalent camera conﬁguration, up
to projectivity. The elements of the tensor therefore satisfy 8 independent algebraic
constraints. This condition is conveniently stated as follows.
Deﬁnition 16.1. A trifocal tensor T jk
internal constraints” if there exist three camera matrices P = [I | 0], P(cid:2)
T jk
corresponds to the three camera matrices according to (15.9–p376).

is said to be “geometrically valid” or “satisfy all
such that

i

i

and P(cid:2)(cid:2)

Just as with the fundamental matrix it is important to enforce these constraints in
some way so as to arrive at a geometrically valid trifocal tensor. If the tensor does not
satisfy the constraints, there are consequences similar to a fundamental matrix which
is not of rank 2 – where epipolar lines, computed as Fx for varying x, do not intersect
in a single point (see ﬁgure 11.1(p280)). For example, if the tensor does not satisfy the
internal constraints and is used to transfer a point to a third view, given a correspon-
dence over two views as described in section 15.3, then the position of the transferred
point will vary depending on which set of equations from table 16.1 is used. In the
following the objective is always to estimate a geometrically valid tensor.

The constraints satisﬁed by the trifocal tensor elements are not so simply expressed
(as det = 0), and some have thought this an impediment to accurate computation of
the trifocal tensor. However, in reality, in order to work with or compute the trifocal
tensor it is not necessary to express these constraints explicitly – rather they are im-
plicitly enforced by an appropriate parametrization of the trifocal tensor, and rarely
cause any trouble. We will return to the issue of parametrization in section 16.3 and
section 16.4.2.

16.2 The normalized linear algorithm

393

16.1.2 The minimum case – 6 point correspondences
A geometrically valid trifocal tensor may be computed from images of a 6 point con-
ﬁguration, provided the scene points are in general position. There are one or three real
solutions. The tensor is computed from the three camera matrices which are obtained
using algorithm 20.1(p511), as described in section 20.2.4(p510). This minimal six
point solution is used in the robust algorithm of section 16.6.

16.2 The normalized linear algorithm

In forming the matrix equation At = 0 from the equations on T in table 16.1 it is
not necessary to use the complete set of equations derived from each correspondence,
since not all of these equations are linearly independent. For instance in the case of a
point–point–point correspondence (ﬁrst row of table 16.1) all choices of s and t lead to
a set of 9 equations, but only 4 of these equations are linearly independent, and these
may be obtained by choosing two values for each of s and t, for instance 1 and 2. This
point is discussed in more detail in section 17.7(p431).

The reader may verify that the three points equation obtained from table 16.1 for a

given choice of s and t may be expanded as
(cid:2)ix

(cid:2)(cid:2)mT jl

(cid:2)(cid:2)mT il

− x

− x

(cid:2)jx

(cid:2)ix

(cid:2)(cid:2)lT jm

k

k

xk(x

k + x

(16.1)
when i, j (cid:5)= s and l, m (cid:5)= t. Equation (16.1) collapses for i = j or l = m, and swapping
i and j (or l and m) simply changes the sign of the equation. One choice of the four
independent equations is obtained by setting j = m = 3, and letting i and l range
(cid:2)(cid:2)3 may be set to 1 to obtain a relationship between
freely. The coordinates x3, x
the observed image coordinates. Equation (16.1) then becomes

k ) = 0ijlm .

(cid:2)3 and x

(cid:2)jx

(cid:2)(cid:2)lT im

xk(x

(cid:2)ix

(cid:2)(cid:2)lT 33

k

− x

(cid:2)(cid:2)lT i3

k

− x

(cid:2)iT 3l

k + T il

k ) = 0.

(16.2)

The four different choices of i, l = 1, 2 give four different equations in terms of the
observed image coordinates.

How to represent lines
The three lines correspondence equation of table 16.1 may be written in the form

li = l

(cid:2)(cid:2)
(cid:2)
jl
k

T jk

i

,

where, as usual with homogeneous entities, the equality is up to scale. In the presence
of noise, this relationship will only be approximately satisﬁed by the measured lines
l, l(cid:2)
that are close to the
measured lines.

(cid:2)
, but will be satisﬁed exactly for three lines ˆl, ˆl

(cid:2)(cid:2)
and ˆl

and l(cid:2)(cid:2)

The question is whether two sets of homogeneous coordinates that differ by a small
amount represent lines that are close to each other in some geometric sense. Consider
the two vectors l1 = (0.01, 0, 1)T and l2 = (0, 0.01, 1)T. Clearly as vectors they are
not very different, and in fact (cid:10)l1 − l2(cid:10) is small. On the other hand, l1 represents the
line x = 100, and l2 represents the line y = 100. Thus in a geometric sense, these lines
are totally different. Note that this problem is alleviated by scaling. If coordinates are

394

16 Computation of the Trifocal Tensor T

Objective
Given n ≥ 7 image point correspondences across 3 images, or at least 13 line correspondences,
or a mixture of point and line correspondences, compute the trifocal tensor.

Algorithm

and H(cid:1)(cid:1)

to apply to the three images.

i lj. Points and lines in the second and third image transform in the same way.

(i) Find transformation matrices H, H(cid:1)
jxj, and lines according to li (cid:6)→ ˆli =
(ii) Transform points according to xi (cid:6)→ ˆxi = Hi
(iii) Compute the trifocal tensor ˆT linearly in terms of the transformed points and lines
using the equations in table 16.1 by solving a set of equation of the form At = 0, using
algorithm A5.4(p593).
t ˆT st
i (H(cid:1)−1)j
Hr
r .

(iv) Compute the trifocal tensor corresponding to the original data according to T jk

(H−1)j

s(H(cid:1)(cid:1)−1)k

i =

Algorithm 16.1. The normalized linear algorithm for computation of T .

scaled by a factor of 0.01, then the coordinates for the lines become l1 = (1, 0, 1)T and
l2 = (0, 1, 1)T, which are quite different.
Nevertheless, this observation indicates that care is needed when representing lines.
Suppose one is given a correspondence between three lines l, l(cid:2)
. Two points
x1 and x2 lying on l are selected. Each of these points provides a correspondence
xs ↔ l(cid:2) ↔ l(cid:2)(cid:2)
, for s = 1, 2, between the three views, in the sense that there exists a
3D line that maps to l(cid:2)
in the second and third images and to a line (namely l)
T jk
passing through xs in the ﬁrst image. Two equations of the form xi
i = 0s for
sl
s = 1, 2 result from these correspondences. In this way one avoids the use of lines in
the ﬁrst image, though not the other images. Often lines in images are deﬁned naturally
by a pair of points, possibly the two endpoints of the lines. Even lines that are deﬁned
as the best ﬁt to a set of edge points in an image may be treated as if they were deﬁned
by just two points, as will be described in section 16.7.2.

and l(cid:2)(cid:2)

and l(cid:2)(cid:2)

(cid:2)(cid:2)
(cid:2)
jl
k

Normalization
As in all algorithms of this type, it is necessary to carry out prenormalization of the
input data before forming and solving the linear equation system. Subsequently, it is
necessary to correct for this normalization to ﬁnd the trifocal tensor for the original
data. The recommended normalization is much the same as that given for the compu-
tation of the fundamental matrix. A translation is applied to each image such that the
√
centroid of the points is at the origin, and then a scaling is applied so that the average
(RMS) distance of the points from the origin is
2. In the case of lines, the transforma-
tion should be deﬁned by considering each line’s two endpoints (or some representative
line points visible in the image). The transformation rule for the trifocal tensor under
these normalizing transformations is given in section A1.2(p563). The normalized
linear algorithm for computing T is summarized in algorithm 16.1.
This algorithm does not consider the constraints discussed in section 16.1.1 that
should be applied to T . These constraints ought to be enforced before the denormal-

16.3 The algebraic minimization algorithm

395

ization step (ﬁnal step) in the above algorithm. Methods of enforcing these constraints
will be considered next.

16.3 The algebraic minimization algorithm

The linear algorithm 16.1 will give a tensor not necessarily corresponding to any ge-
ometric conﬁguration, as discussed in section 16.1.1. The next task is to correct the
tensor to satisfy all required constraints.
Our task will be to compute a geometrically valid trifocal tensor T jk
from a set of
image correspondences. The tensor computed will minimize the algebraic error asso-
ciated with the input data. That is, we minimize (cid:10)Aˆt(cid:10) subject to (cid:10)ˆt(cid:10) = 1, where ˆt
is the vector of entries of a geometrically valid trifocal tensor. The algorithm is quite
similar to the algebraic algorithm (section 11.3(p282)) for computation of the funda-
mental matrix. Just as with the fundamental matrix, the ﬁrst step is the computation of
the epipoles.

i

Retrieving the epipoles
Let e(cid:2)
and e(cid:2)(cid:2)
be the epipoles in the second and third images corresponding to (that
is being images of) the ﬁrst camera centre. Recall from result 15.4(p373) that the
two epipoles e(cid:2)
and e(cid:2)(cid:2)
are the common perpendicular to the left (respectively right)
null-vectors of the three Ti. In principle then, the epipoles may be computed from the
trifocal tensor using the algorithm outlined in algorithm 15.1(p375). However, in the
presence of noise, this translates easily into an algorithm for computing the epipoles
based on four applications of algorithm A5.4(p593).

(i) For each i = 1, . . . ,3 ﬁnd the unit vector vi that minimizes (cid:10)Tivi(cid:10), where
(ii) Compute the epipole e(cid:2)(cid:2)

as the unit vector that minimizes (cid:10)Ve(cid:2)(cid:2)(cid:10).

i . Form the matrix V, the i-th row of which is vT
i .

Ti = T ··

The epipole e(cid:2)

is computed similarly, using TT

i instead of Ti.

, P(cid:2)(cid:2)

4 and e(cid:2)(cid:2)k = bk

Algebraic minimization
Having computed the epipoles the next step is to determine the remaining elements of
the camera matrices P(cid:2)
from which the trifocal tensor can be calculated. This step
is linear.
From the form (15.9–p376) of the trifocal tensor, it may be seen that once the
epipoles e(cid:2)j = aj
4 are known, the trifocal tensor may be expressed linearly
in terms of the remaining entries of the matrices aj
i . This relationship may be
written linearly as t = Ea where a is the vector of the remaining entries ai
j, t is
the vector of entries of the trifocal tensor, and E is the linear relationship expressed by
(15.9–p376). We wish to minimize the algebraic error (cid:10)At(cid:10) = (cid:10)AEa(cid:10) over all choices
of a constrained such that (cid:10)t(cid:10) = 1, that is (cid:10)Ea(cid:10) = 1. This minimization problem is
solved by algorithm A5.6(p595). The solution t = Ea represents a trifocal tensor sat-
isfying all constraints, and minimizing the algebraic error, subject to the given choice
of epipoles.

i and bk

j and bi

396

Objective

16 Computation of the Trifocal Tensor T

Given a set of point and line correspondences in three views, compute the trifocal tensor.
Algorithm

i

and e(cid:1)(cid:1)

i

i

(i) From the set of point and line correspondences compute the set of equations of the form

At = 0, from the relations given in table 16.1.
trifocal tensor T jk

.

from T jk

, a is the vector representing entries of a

(ii) Solve these equations using algorithm A5.4(p593) to ﬁnd an initial estimate of the
(iii) Find the two epipoles e(cid:1)
as the common perpendicular to the left
(respectively right) null-vectors of the three Ti.
(iv) Construct the 27 × 18 matrix E such that t = Ea where t is the vector of entries of
T jk
i , and where E expresses the linear
relationship T jk
(v) Solve the minimization problem: minimize (cid:10)AEa(cid:10) subject to (cid:10)Ea(cid:10) = 1, using
algorithm A5.6(p595). Compute the error vector  = AEa.
, e(cid:1)(cid:1)) (cid:6)→  is a mapping from IR6 to IR27. Iterate on the
(vi) Iteration: The mapping (e(cid:1)
last two steps with varying e(cid:1)
and e(cid:1)(cid:1)
using the Levenberg–Marquardt algorithm to ﬁnd
the optimal e(cid:1)

. Hence ﬁnd the optimal t = Ea containing the entries of T jk

(cid:1)(cid:1)k − e

(cid:1)jbk
i .

i = a

j
i e

j
i and bk

, e(cid:1)(cid:1)

.

i

Algorithm 16.2. Computing the trifocal tensor minimizing algebraic error. The computation should
be carried out on data normalized in the manner of algorithm 16.1. Normalization and denormalization
steps are omitted here for simplicity. This algorithm ﬁnds the geometrically valid trifocal tensor that
minimizes algebraic error. At the cost of a slightly inferior solution, the last iteration step may be
omitted, providing a fast non-iterative algorithm.

Iterative method
The two epipoles used to compute a geometrically valid tensor T jk
are determined
using the estimate of T jk
obtained from the linear algorithm. Analogous to the case
) (cid:6)→ AEa is a mapping IR6 → IR27.
of the fundamental matrix, the mapping (e(cid:2)
An application of the Levenberg–Marquardt algorithm to optimize the choice of the
epipoles will result in an optimal (in terms of algebraic error) estimate of the trifocal
tensor. Note that the iteration problem is of modest size, since only 6 parameters, the
homogeneous coordinates of the epipoles, are involved in the iteration problem.

, e(cid:2)(cid:2)

i

i

This contrasts with an iterative estimation of the optimal trifocal tensor in terms of
geometric error, considered later. This latter problem requires estimating the param-
eters of the three cameras, plus the coordinates of all the points, a large estimation
problem.

The complete algebraic method for estimating the trifocal tensor is summarized in

algorithm 16.2.

16.4 Geometric distance

16.4.1 The Gold Standard method for the trifocal tensor
As with the computation of the fundamental matrix, best results may be expected from
the maximum likelihood (or “Gold Standard”) solution. Since this has been adequately
described for the case of the fundamental matrix computation, little needs to be added
for the three-view case.

16.4 Geometric distance

397

Objective
Given n ≥ 7 image point correspondences {xi ↔ x(cid:1)
Likelihood Estimate of the trifocal tensor.
The MLE involves also solving for a set of subsidiary point correspondences {ˆxi ↔ ˆx(cid:1)
which exactly satisfy the trilinear relations of the estimated tensor and which minimize

i ↔ x(cid:1)(cid:1)

i }, determine the Maximum

i ↔ ˆx(cid:1)(cid:1)
i },

d(xi, ˆxi)2 + d(x(cid:1)

i, ˆx(cid:1)

i)2 + d(x(cid:1)(cid:1)

i , ˆx(cid:1)(cid:1)
i )2

(cid:7)

i

Algorithm

i, ˆx(cid:1)(cid:1)

algorithm 16.2.

and P(cid:1)(cid:1)
i ↔ x(cid:1)(cid:1)

(i) Compute an initial geometrically valid estimate of T using a linear algorithm such as
(ii) Compute an initial estimate of the subsidiary variables {ˆxi, ˆx(cid:1)

i } as follows:

i = P(cid:1)(cid:1)(cid:23)Xi.

from T .
i and P = [I | 0], P(cid:1)

i = P(cid:1)(cid:23)Xi, ˆx(cid:1)(cid:1)

(a) Retrieve the camera matrices P(cid:1)
(b) From the correspondence xi ↔ x(cid:1)
(c) The correspondence consistent with T is obtained as

estimate of (cid:23)Xi using the triangulation method of chapter 12.
ˆxi = P(cid:23)Xi, ˆx(cid:1)
(iii) Minimize the cost (cid:7)
over T and (cid:23)Xi, i = 1, . . . , n. The cost is minimized using the Levenberg–Marquardt
algorithm over 3n + 24 variables: 3n for the n 3D points (cid:23)Xi, and 24 for the elements

d(xi, ˆxi)2 + d(x(cid:1)

i)2 + d(x(cid:1)(cid:1)

determine an

i , ˆx(cid:1)(cid:1)
i )2

i, ˆx(cid:1)

, P(cid:1)(cid:1)

i

of the camera matrices P(cid:1)

, P(cid:1)(cid:1)

.

Algorithm 16.3. The Gold Standard algorithm for estimating T from image correspondences.

Given a set of point correspondences {xi ↔ x(cid:2)
function to be minimized is(cid:7)

i

↔ x(cid:2)(cid:2)

i

} in three views, the cost

d(xi, ˆxi)2 + d(x(cid:2)

i, ˆx(cid:2)

i)2 + d(x(cid:2)(cid:2)

i , ˆx(cid:2)(cid:2)
i )2

(16.3)

i

i, ˆx(cid:2)(cid:2)

where the points ˆxi, ˆx(cid:2)
i satisfy a trifocal constraint (as in table 16.1) exactly for
the estimated trifocal tensor. As in the case of the fundamental matrix one needs to
introduce further variables corresponding to 3D points Xi and parametrize the trifocal
tensor by the entries of the matrices P(cid:2)
(see below). The cost function is then
minimized over the position of the 3D points Xi and the two camera matrices P(cid:2)
and
P(cid:2)(cid:2)
i = P(cid:2)(cid:2)Xi. Essentially one is carrying out
bundle adjustment over three views. The sparse matrix techniques of section A6.3-
(p602) should be used.

and P(cid:2)(cid:2)
i = P(cid:2)Xi, and ˆx(cid:2)(cid:2)

with ˆxi = [I | 0]Xi, ˆx(cid:2)

A good way to ﬁnd an initial estimate is the algebraic algorithm 16.2, though the
ﬁnal iterative step can be omitted. This algorithm gives a direct estimate of the entries
of P(cid:2)
. The initial estimate of the 3D points Xi may be obtained using the linear
triangulation method of section 12.2(p312). The steps of the algorithm are summarized
in algorithm 16.3.

and P(cid:2)(cid:2)

398

16 Computation of the Trifocal Tensor T

The technique can be extended to include line correspondences. To do this, one
needs to ﬁnd a representation of a 3D line convenient for computation. Given a 3-view
line correspondence l ↔ l(cid:2) ↔ l(cid:2)(cid:2)
, the lines being perhaps deﬁned by their endpoints in
each image, a very convenient way to represent the 3D line during the LM parameter
(cid:2)
minimization is by its projections ˆl
in the second and third views. Given a
candidate trifocal tensor, one can easily compute the projection of the 3D line into
(cid:2)
the ﬁrst view using the line transfer equation ˆli = ˆl
. Then one minimizes the
j

(cid:2)(cid:2)
and ˆl

(cid:2)(cid:2)
ˆl
k

sum-of-squares line distance(cid:7)

T jk

i

d(li,ˆli)2 + d(l(cid:2)

(cid:2)(cid:2)
(cid:2)
i)2 + d(l(cid:2)(cid:2)
i ,ˆl
i,ˆl
i )2

i

(cid:2)
for some appropriate interpretation of the distance d(l(cid:2)
i,ˆl
i)2 between the measured and
estimated line.
If the measured line is speciﬁed by its endpoints, then the obvious
distance metric to use is the distance of the estimated line from the measured endpoints.
In general a Mahalanobis distance may be used.

16.4.2 Parametrization of the trifocal tensor
If the tensor is parametrized simply by its 27 entries, then the estimated tensor will not
satisfy the internal constraints. A parametrization which ensures that the tensor does
satisfy its constraints, and so is geometrically valid, is termed consistent.
Since, from deﬁnition 16.1, a tensor is geometrically valid if it is generated from
three camera matrices P = [I | 0], P(cid:2)
by (15.9–p376), it follows that the three cam-
era matrices give a consistent parametrization. Note that this is an overparametrization
since it requires 24 parameters to be speciﬁed, namely the 12 entries each of the ma-
= [B|b4]. There is no need to attempt to deﬁne a minimal
trices P(cid:2)
set of (18) parameters, which is a difﬁcult task. Any choice of cameras is a consistent
parametrization, the particular projective reconstruction has no effect on the tensor.

= [A|a4] and P(cid:2)(cid:2)

, P(cid:2)(cid:2)

Another consistent parametrization is obtained by computing the tensor from six
point correspondences across three views as in section 20.2(p508). Then the position
of the points in each image is the parametrization – a total of 6 (points) ×2 (for x, y) ×
3 (images) = 36 parameters. However, only a subset of the points need be varied during
the minimization, or the movement of the points can be restricted to be perpendicular
to the variety of trifocal tensors.

16.4.3 First-order geometric error (Sampson distance)
The trifocal tensor may be computed using a geometric cost function based on the
Sampson approximation in a manner entirely analogous to the Sampson method used
to compute the fundamental matrix (section 11.4.3(p287)). Again the advantage is that
it is not necessary to introduce a set of subsidiary variables, as this ﬁrst-order geometric
error requires a minimization only over the parametrization of the tensor (e.g. only
24 parameters if P(cid:2)
is used as above). The minimization can be carried out with
a simple iterative Levenberg–Marquardt algorithm, and the method initialized by the
iterative algebraic algorithm 16.2.

, P(cid:2)(cid:2)

16.5 Experimental evaluation of the algorithms

399

The Sampson cost function is a little more complex computationally than the cor-
responding cost function for the fundamental matrix (11.9–p287), because each point
correspondence gives four equations, instead of just one for the fundamental matrix.
The more general case was discussed in section 4.2.6(p98). The error function (4.13–
p100) in the present case is

i (JiJT
T
i )

−1i

(16.4)

(cid:7)

i

i

where i is the algebraic error vector Ait corresponding to a single 3-view correspon-
dence (a 4-vector in the case of 4 equations per point), and J is the 4 × 6 matrix of
partial derivatives of  with respect to the coordinates of each of the corresponding
points xi ↔ x(cid:2)
i . As in the programming hint given in exercise (vii) on page 129,
the computation of the partial derivative matrix J may be simpliﬁed by observing that
the cost function is multilinear in the coordinates of the points xi, x(cid:2)

↔ x(cid:2)(cid:2)

i, x(cid:2)(cid:2)
i .

The Sampson error method has various advantages:
• It gives a good approximation to actual geometric error (the optimum), using a rela-
tively simple iterative algorithm.
• As in the case of actual geometric error, non-isotropic and unequal error distribu-
tions may be speciﬁed for each of the points without signiﬁcantly complicating the
algorithm. See exercises in chapter 4.

16.5 Experimental evaluation of the algorithms

A brief comparison is now given of the results of the (iterative) algebraic algorithm 16.2
along with the Gold Standard algorithm 16.3 for computing the trifocal tensor. The
algorithms are run on synthetic data with controlled levels of noise. This allows a
comparison with the theoretically optimal ML results, and a determination of how well
these algorithms are able to approximate the theoretical lower bound on residual error,
achieved by an optimal ML algorithm.

Computer-generated data sets of 10, 15 and 20 points were used to test the algorithm,
and the cameras were placed at random angles around the cloud of points. The camera
parameters were chosen to approximate a standard 35mm camera, and the scale was
chosen so that the size of the image was 600 × 600 pixels.
For a given level of added Gaussian noise in the image measurement, one may com-
pute the expected residual achieved by an ML algorithm, according to result 5.2(p136).
In this case, if n is the number of points, then the number of measurements is N = 6n,
and the number of degrees of freedom in the ﬁtting is d = 18 + 3n, where 18 represents
the number of degrees of freedom of the three cameras (3 × 11 less 15 to account for
projective ambiguity) and 3n represents the number of degrees of freedom of n points
in space. Hence the ML residual is

res = σ(1 − d/N )1/2 = σ

(cid:28)

n − 6
2n

(cid:29)1/2

.

16 Computation of the Trifocal Tensor T

15 points

r
o
r
r
E

7

6

5

4

3

2

1

0

20 points

7

6

5

4

3

2

1

0

r
o
r
r
E

0

2

4

6

8

10

12

Noise

400

r
o
r
r
E

6

5

4

3

2

1

0

10 points

0

2

4

6

Noise

8

10

12

0

2

4

6

Noise

8

10

12

Fig. 16.1. Comparison of trifocal tensor estimation algorithms. The residual error RMS-averaged
over 100 runs is plotted against the noise level, for computation of the trifocal tensor using 10, 15
and 20 points. Each graph contains three curves. The top curve is the result of the algebraic error
minimization, whereas the lower two curves, actually indistinguishable in the graphs, represent the
theoretical minimum error, and the error obtained by the Gold Standard algorithm using the algebraic
minimization as a starting point. Note that the residual errors are almost exactly proportional to added
noise, as they should be.

16.5.1 Results and recommendations
The results are shown in ﬁgure 16.1. We learn two things from these results. Minimiza-
tion of the algebraic error achieves residual errors within about 15% of the optimal and
using this estimate as a starting point for minimizing geometric error achieves a virtu-
ally optimal estimate.

All the algorithms developed above, except the linear method of section 16.1, enforce
the internal constraints on the tensor. The linear method is not recommended for use
on its own, but is necessary for initialization in most of the other methods. As in the
case of estimating the fundamental matrix our recommendations are to use the iterative
algebraic algorithm 16.2 or the Sampson geometric approximation of section 16.4.3.
Both give excellent results. Again to be certain of getting the best results, if Gaussian
noise is a viable assumption, implement the Gold Standard algorithm 16.3.

16.6 Automatic computation of T

This section describes an algorithm to compute the trifocal geometry between three
images automatically. The input to the algorithm is simply the triplet of images, with
no other a priori information required; and the output is the estimated trifocal tensor
together with a set of interest points in correspondence across the three images.

The fact that the trifocal tensor may be used to determine the exact image position
of a point in a third view, given its image position in the other two views, means that
there are fewer mismatches over three views than there are over two. In the two view
case there is only the weaker geometric constraint of an epipolar line against which to
verify a possible match.

The three-view algorithm uses RANSAC as a search engine in a similar manner
to its use in the automatic computation of a homography described in section 4.8-
(p123). The ideas and details of the algorithm are given there, and are not repeated
here. The method is summarized in algorithm 16.4, with an example of its use shown

16.6 Automatic computation of T

401

Objective Compute the trifocal tensor between three images.

Algorithm

(i) Interest points: Compute interest points in each image.
(ii) Two-view correspondences: Compute interest point correspondences (and F) between

views 1 & 2, and 2 & 3 using algorithm 11.4(p291).

(iii) Putative three-view correspondences: Compute a set of interest point correspon-

dences over three views by joining the two-view match sets.

(iv) RANSAC robust estimation: Repeat for N samples, where N is determined adap-

tively as in algorithm 4.5(p121):

(a) Select a random sample of 6 correspondences and compute the trifocal tensor

using algorithm 20.1(p511). There will be one or three real solutions.
ety described by T , as in section 16.6.
dences for which d⊥ < t.

(b) Calculate the distance d⊥ in IR6 from each putative correspondence to the vari-
(c) Compute the number of inliers consistent with T by the number of correspon-
(d) If there are three real solutions for T the number of inliers is computed for each
Choose the T with the largest number of inliers. In the case of ties choose the solution
that has the lowest standard deviation of inliers.
(v) Optimal estimation: Re-estimate T from all correspondences classiﬁed as inliers us-

solution, and the solution with most inliers retained.

(vi) Guided matching: Further interest point correspondences are now determined using

ing the Gold Standard algorithm 16.3 or the Sampson approximation to this.
the estimated T as described in the text.

The last two steps can be iterated until the number of correspondences is stable.

Algorithm 16.4. Algorithm to automatically estimate the trifocal tensor over three images using
RANSAC.

in ﬁgure 16.2, and additional explanation of the steps given below. Figure 16.3 shows
a second example which includes automatically computed line matches.
The distance measure – reprojection error. Given the match x ↔ x(cid:2) ↔ x(cid:2)(cid:2)
and the
current estimate of T we need to determine the minimum of the reprojection error –
d2⊥ = d2(x, ˆx) + d2(x(cid:2)
are consistent
a 3-space point (cid:23)X
with T . As usual the consistent images points may be obtained from the projection of
by determining the point (cid:23)X which minimizes the image distance between the measured
are extracted from T . The distance d2⊥ is then obtained

, ˆx(cid:2)
, ˆx(cid:2)(cid:2)
ˆx = [I | 0](cid:23)X,

), where the image points ˆx, ˆx(cid:2)

= P(cid:2)(cid:23)X,

= P(cid:2)(cid:2)(cid:23)X

) + d2(x(cid:2)(cid:2)

, ˆx(cid:2)(cid:2)

ˆx(cid:2)(cid:2)

where the camera matrices P(cid:2)
points x, x(cid:2)

, x(cid:2)(cid:2)

, P(cid:2)(cid:2)

and the projected points.

ˆx(cid:2)

Another way of obtaining this distance is to use the Sampson error (16.4), which is
a ﬁrst-order approximation to the geometric error. However, in practice it is quicker
to estimate the error directly by non-linear least-squares iteration (a small Levenberg–

Marquardt problem). Starting from an initial estimate of (cid:23)X, one iterates varying the
coordinates of (cid:23)X to minimize the reprojection error.

402

16 Computation of the Trifocal Tensor T

a

d

b

e

c

f

g

i

h

j

Fig. 16.2. Automatic computation of the trifocal tensor between three images using RANSAC. (a
- c) raw images of Keble College, Oxford. The motion between views consists of a translation and
rotation. The images are 640 × 480 pixels. (d - f) detected corners superimposed on the images. There
are approximately 500 corners on each image. The following results are superimposed on the (a) image:
(g) 106 putative matches shown by the line linking corners, note the clear mismatches; (h) outliers – 18
of the putative matches. (i) inliers – 88 correspondences consistent with the estimated T ; (j) ﬁnal set of
95 correspondences after guided matching and MLE. There are no mismatches.

Guided matching. We have an initial estimate of T and wish to use this to generate
and assess additional point correspondences across the three-views. The ﬁrst step is
to extract the fundamental matrix F12 between views 1 & 2 from T . Then two-view

16.6 Automatic computation of T

403

a

b

Fig. 16.3. Image triplet matching. The trifocal tensor is computed automatically from interest points
using algorithm 16.4, and subsequently used to match line features across views. (a) Three images of
a corridor sequence. (b) Automatically matched line segments. The matching algorithm is described in
[Schmid-97].

guided matches are computed using loose thresholds on matching. Each two-view
match is corrected using F12 to give points ˆx, ˆx(cid:2)
which are consistent with F12. These
corrected two-view matches (together with T ) deﬁne a small search window in the third
view in which the corresponding point is sought. Any three-view point correspondence
is assessed by computing d⊥, as described above. The match accepted if d⊥ is less
than the threshold t. Note, the same threshold is used for inlier detection within the
RANSAC stage and guided matching.

In practice it is found that the stage of guided matching is more signiﬁcant here, in
that it generates additional correspondences, than in the case of homography estima-
tion.

Implementation and run details. For the example of ﬁgure 16.2, the search window
was ±300 pixels. The inlier threshold was t = 1.25 pixels. A total of 26 samples were
required. The RMS pixel error after RANSAC was 0.43 (for 88 correspondences),
after MLE it was 0.23 (for 88 correspondences), and after MLE and guided matching it
was 0.19 (for 95 correspondences). The MLE required 10 iterations of the Levenberg–
Marquardt algorithm.

Note, RANSAC has to do far less work than in algorithm 11.4(p291) to estimate
F and correspondences, because the two-view algorithm has already removed many
outliers before the putative correspondences over three views are generated.

404

16 Computation of the Trifocal Tensor T
16.7 Special cases of T -computation

i

from a plane plus parallax

16.7.1 Computing T jk
We describe here the computation of T jk
from the image of a special conﬁguration
consisting of a world plane (from which a homography between views can be com-
puted) and two points off the plane. Of course, it is not necessary for the plane to
actually be present. It may be virtual, or the homography may simply be speciﬁed by
the images of four coplanar points or four coplanar lines. The method is the analogue
of algorithm 13.2(p336) for the fundamental matrix.

i

The solution is obtained by constructing the three camera matrices (up to a common
projective transformation of 3-space) and then computing the trifocal tensor from these
matrices according to (15.9–p376). The homography induced by the world (reference)
plane between the ﬁrst and second view is H12, and between the ﬁrst and third views is
H13. As shown in section 13.3(p334) the epipole e(cid:2)
may be computed directly from the
two point correspondences off the plane for the ﬁrst and second views, and the camera
matrices chosen as P = [I | 0], P(cid:2)
], where µ is a scalar. Note the scale of
both H12 and e(cid:2)
is considered ﬁxed here, so they are no longer homogeneous quantities.
Similarly, e(cid:2)(cid:2)
may be determined from the two point correspondences for views one
and three and the camera matrices chosen as P = [I | 0], P(cid:2)(cid:2)
], where λ is
a scalar.

= [H13 | λe(cid:2)(cid:2)

= [H12 | µe(cid:2)

It is then easily veriﬁed that a consistent set of cameras for the three views (see the

discussion on consistent camera triplets on page 375) is given by
= [H13 | λe(cid:2)(cid:2)

P = [I | 0],

= [H12 | e(cid:2)

P(cid:2)

P(cid:2)(cid:2)

],

]

(16.5)

where µ has been set to unity. The value of λ is determined from one of the point
correspondences over three views, and this is left as an exercise. For more on plane-
plus-parallax reconstruction, see section 18.5.2(p450).

Note that

the estimation of the trifocal

tensor for this conﬁguration is over-
determined. In the case of the fundamental matrix over two views the homographies
determine all but 2 degrees of freedom (the epipole), and each of the point correspon-
dence provides one constraint, so that the number of constraints equals the number of
degrees of freedom of the matrix. In the case of the trifocal tensor the homography
determines all but 5 degrees of freedom (the two epipoles and their relative scaling).
However, each point correspondence provides three constraints (there are six coordi-
nate measurements less three for the point’s position in 3-space), so that there are six
constraints on 5 degrees of freedom. Since there are more measurements than degrees
of freedom in this case, the tensor should be estimated by minimizing a cost function
based on geometric error.

16.7.2 Lines speciﬁed by several points
In describing the reconstruction algorithm from lines, we have considered the case
where lines are speciﬁed by their two endpoints. Another common way that lines may

16.7 Special cases of T -computation

405

be speciﬁed in an image is as the best line ﬁt to several points. It will be shown now
how that case may easily be reduced to the case of a line deﬁned by two endpoints.
Consider a set of points xi in an image, normalized to have third component equal to 1.
Let l = (l1, l2, l3)T be a line, which we suppose is normalized such that l2
2 = 1. In
this case, the distance from a point xi to the line l is equal to xT
l. The squared distance
i
may be written as d2 = lTxixT
i

1 + l2

l, and the sum-of-squares of all distances is
lTxixT

l = lT(

xixT

i )l .

i

(cid:7)

(cid:7)

i

i

(cid:27)

xixT

The matrix E = (
Lemma 16.2. Matrix (E − 0J) is positive-semideﬁnite, where J is the matrix
diag(1, 1, 0) and 0 is the smallest solution to the equation det(E − J) = 0.

i ) is positive-deﬁnite and symmetric.

i

1 +x2

1 + x2

Proof. We start by computing the vector x = (x1, x2, x3)T that minimizes xTEx
subject to the condition x2
2 = 1. Using the method of Lagrange multipliers, this
comes down to ﬁnding the extrema of xTEx−ξ(x2
2), where ξ denotes the Lagrange
coefﬁcient. Taking the derivative with respect to x and setting it to zero, we ﬁnd that
2Ex− ξ(2x1, 2x2, 0)T = 0. This may be written as (E− ξJ)x = 0. It follows that ξ is a
root of the equation det(E− ξJ) = 0 and x is the generator of the null-space of E− ξJ.
Since xTEx = ξxTJx = ξ(x2
2) = ξ, it follows that to minimize xTEx one must
choose ξ to be the minimum root ξ0 of the equation det(E−ξJ) = 0. In this case one has
Ex0 − ξ0 = 0 for the minimizing vector x0. For any other vector x, not necessarily
xT
the minimizing vector, one has xTEx− ξ0 ≥ 0. Then, xT(E− ξ0J)x = xTEx− ξ0 ≥ 0,
0
and so E − ξ0J is positive-semideﬁnite.
Since the matrix E − ξ0J is symmetric it may be written in the form E − ξ0J =
Vdiag(r, s, 0)VT where V is an orthogonal matrix and r and s are positive. It follows
that

1 + x2

E − ξ0J = Vdiag(r, 0, 0)VT + Vdiag(0, s,0)V T

= rv1vT

1 + sv2vT

2

where vi is the i-th column of V. Therefore E = ξ0J + rv1vT
line l satisfying l2

2 = 1 we have

1 + l2

1 + sv2vT

2 . Then for any

(cid:7)

(xT
i

l)2 = lTEl

i

= ξ0 + r(vT

1

l)2 + s(vT
2

l)2.

Thus, we have replaced the sum-of-squares of several points by a constant value ξ0,
which is not capable of being minimized, plus the weighted sum-of-squares of the
distances to two points v1 and v2. To summarize: when forming the trifocal tensor
equations involving a line deﬁned by points xi, formulate two point equations expressed
in terms of the points v1 and v2 with weights

√
r and

s respectively.

√

406

16 Computation of the Trifocal Tensor T

(cid:27)

Orthogonal regression. In the proof of lemma 16.2 above, it was shown that the line
l that minimizes the sum of squared distances to the set of all points xi = (xi, yi, 1)T is
obtained as follows.

xixT

i

i and J = diag(1, 1, 0).

(i) Deﬁne matrices E =
(ii) Let ξ0 be the minimum root of the equation det(E − ξJ) = 0.
(iii) The required line l is the right null-vector of the matrix E − ξ0J.
This gives a least-squares best ﬁt of a line to a set of points. This process is known
as orthogonal regression and it extends in an obvious way to higher-dimensional ﬁtting
of a hyperplane to a set of points in a way that minimizes the sum of squared distances
to the points.

16.8 Closure

16.8.1 The literature
A linear method for computing the trifocal tensor was ﬁrst given in [Hartley-97a],
where further experimental results of estimation using both point and line correspon-
dences on real data are reported. An iterative algebraic method for estimating a consis-
tent tensor was given in [Hartley-98d].
Torr and Zisserman [Torr-97] developed an automatic algorithm for estimating a
consistent tensor T from three images. This paper also compared several parametriza-
tions of the iterative minimization. Several methods of representing and imposing the
constraints on the tensor are given by Faugeras and Papadopoulo [Faugeras-97].

[Oskarsson-02] gives minimal solutions for reconstruction for the two cases of “four

points and three lines in three views”, and “two points and six lines in three views”.

16.8.2 Notes and exercises

(i) Consider the problem of estimating the 3-space point X which minimizes re-
projection error from measured image points x, x(cid:2)
, given the trifocal tensor.
This is the analogue of the triangulation problem of chapter 12. Show that for
general motion the one parameter family parametrization of epipolar lines de-
veloped in chapter 12 does not extend from two views to three. However, in the
case that the three camera centres are collinear the two-view parametrization
can be extended to three and a minimum determined by solving a polynomial
in one variable. What is the degree of this polynomial?

, x(cid:2)(cid:2)

(ii) An afﬁne trifocal tensor may be computed from a minimal conﬁguration of 4
points in general position. The computation is similar to that of algorithm 14.2-
(p352), and the resulting tensor satisﬁes the internal constraints for an afﬁne
trifocal tensor. How many constraints are there in the afﬁne case?
If more than 4 point correspondences are used in the estimation then a geo-
metrically valid tensor is estimated using the factorization algorithm of section
18.2(p436).

(iii) The transformation rule for tensors is T jk

i = Ar

i (B−1)j

s(C−1)k

t

ˆT st
r . This may be

computed easily as

16.8 Closure

407

Binv = B.inverse();
Cinv = C.inverse();

for (i=1; i<=3; i++) for (j=1; j<=3; j++) for (k=1; k<=3; k++)
{

T[i][j][k] = 0.0;

for (r=1; r<=3; r++) for (s=1; s<=3; s++) for (t=1; t<=3; t++)

T[i][j][k] +=

A[r][i] * Binv[j][s] * Cinv[k][t] * T_hat[r][s][t];

}

How many multiplications and loop iterations does this involve? Find a better
way of computing this transformation.

(iv) In the computation of

tensor using plane plus parallax
(section 16.7.1), show that if ρ is the projective depth of one of the points off
the plane (i.e. x(cid:2)
see (13.9–p337)), then the scalar λ in (16.5) may
be computed from the equation x(cid:2)(cid:2)

= H12x + ρe(cid:2)

the trifocal

= H13x + ρλe(cid:2)(cid:2)

.

Part IV

N-View Geometry

Untitled 1947, (Oil on sackcloth) by Asger Jorn (1914-1973)

c(cid:4) 2003 Artists Rights Society (ARS), New York / COPY-DAN, Copenhagen

Outline

This part is partly a recapitulation and partly new material.

Chapter 17 is the recapitulation. We return to two- and three-view geometry but now within a more
general framework which naturally extends to four- and n-views. The fundamental projective relations
over multiple views arise from the intersection of lines (back-projected from points) and planes (back-
projected from lines). These intersection properties are represented by the vanishing of determinants
formed from the camera matrices of the views. The fundamental matrix, the trifocal tensor, and a new
tensor for four views – the quadrifocal tensor – arise naturally from these determinants as the multiple
view tensors for two, three, and four views respectively. The tensors are what remains when the 3D
structure and non-essential part of the camera matrices are eliminated. The tensors stop at four views.

These tensors are unique for each set of views, and generate relationships which are multi-linear in
the coordinates of the image measurements. The tensors can be computed from sets of image correspon-
dences, and subsequently a camera matrix for each view can be computed from the tensor. Finally, the
3D structure can be computed from the retrieved cameras and image correspondences.

Chapter 18 covers the computation of a reconstruction from multiple views. In particular the impor-
tant factorization algorithm is given for reconstruction from afﬁne views. It is important because the
algorithm is optimal, but is also non-iterative.

Chapter 19 describes the auto-calibration of a camera. These are a set of methods for computing the
internal parameters of a camera based on constraints over multiple images. In contrast to the traditional
approach to calibration described in chapter 7, no explicit scene calibration object is used, but simply
constraints such as that the internal parameters are common across the images, or that the camera rotates
about its centre and does not change aspect ratio.

Chapter 20 emphasises the duality between points and cameras, and how this links various conﬁgura-
tions and algorithms that have been given throughout this book. This chapter contains an algorithm for
computing a reconstruction of six points imaged in 3-views.

Chapter 21 investigates the issue of whether points are in front of or behind one or more cameras.
This is an issue that goes beyond the homogeneous representation used throughout the book which does
not distinguish the direction of a ray.

Chapter 22 covers the important topic of those conﬁgurations for which the estimation algorithms
described in this book will fail. An example is for resectioning, where the camera matrix cannot be
computed if all the 3D points and the camera centre lie on a twisted cubic.

410

17

N -Linearities and Multiple View Tensors

This chapter introduces the quadrifocal tensor Qijkl between four views, which is the
analogue of the fundamental matrix for two and the trifocal tensor for three views. The
quadrifocal tensor encapsulates the relationships between imaged points and lines seen
in four views.

It is shown that multiple view relations may be derived directly and uniformly from
the intersection properties of back-projected lines and points. From this analysis the
fundamental matrix F, trifocal tensor T jk
, and quadrifocal tensor Qijkl appear in a
common framework involving matrix determinants. Speciﬁc formulae are given for
each of these tensors in terms of the camera matrices.

i

We also develop general counting arguments for the degrees of freedom of the tensors
and the number of point and line correspondences required for tensor computation.
These are given for conﬁgurations in general position and for the important special
case where four or more of the elements are coplanar.

17.1 Bilinear relations

We consider ﬁrst the relationship that holds between the coordinates of a point seen
in two separate views. Thus, let x ↔ x(cid:2)
be a pair of corresponding points which are
the images of the same point X in space as seen in the two separate views. It will be
convenient, for clarity of notation, to represent the two camera matrices by A and B,
instead of the usual notation, P and P(cid:2)
. The projection from space to image can now be
expressed as kx = AX and k
are two undetermined constants.
This pair of equations may be written down as one equation

= BX where k and k

(cid:2)x(cid:2)

(cid:17)

A x 0
B 0 x(cid:2)

(cid:2)

 = 0

(cid:18) X−k−k

(cid:2)

and it may easily be veriﬁed that this is equivalent to the two equations. This can be
written in a more detailed form by denoting the i-th row of the matrix A by ai, and
similarly the i-th row of the matrix B by bi. We also write x = (x1, x2, x3)T and

411

412
x(cid:2)

= (x

(cid:2)1, x

(cid:2)2, x

(cid:2)3)T. The set of equations is now

17 N-Linearities and Multiple View Tensors



a1 x1
a2 x2
a3 x3

b1
b2
b3

(cid:2)1
(cid:2)2
(cid:2)3

x
x
x



 = 0.

 X−k−k

(cid:2)

(17.1)

(cid:2)

Now, this is a 6 × 6 set of equations which by hypothesis has a non-zero solution, the
vector (XT,−k,−k
)T. It follows that the matrix of coefﬁcients in (17.1) must have
zero determinant. It will be seen that this condition leads to a bilinear relationship
between the entries of the vectors x and x(cid:2)
expressed by the fundamental matrix F. We
will now look speciﬁcally at the form of this relationship.

Consider the matrix appearing in (17.1). Denote it by X. The determinant of X may
(cid:2)i. Notice that the entries
(cid:2)i appear in only two columns of X. This implies that the determinant of X may
(cid:2)i. In fact, since all the
(cid:2)j.
(cid:2)ix
(cid:2)i, the determinant of X is a bilinear expression. The

be written as an expression in terms of the quantities xi and x
xi and x
be expressed as a quadratic expression in terms of the xi and x
entries xi appear in the same column, there can be no terms of the form xixj or x
Brieﬂy, in terms of the xi and x
fact that the determinant is zero may be written as an equation
(cid:2)ixjFij = 0

(cid:2)3)F(x1, x2, x3)T = x

where F is a 3 × 3 matrix, the fundamental matrix.
We may compute a speciﬁc formula for the entries of the matrix F as follows. The
(cid:2)ixj in the expansion of the determinant
entry Fij of F is the coefﬁcient of the term x
of X. In order to ﬁnd this coefﬁcient, we must eliminate the rows and columns of the
(cid:2)i and xj, take the determinant of the resulting matrix and multiply
matrix containing x
by ±1 as appropriate. For instance, the coefﬁcient of x
(cid:2)1x1 is obtained by eliminating
two rows and the last two columns of the matrix X. The remaining matrix is

(cid:2)2, x

(cid:2)1, x

(17.2)

(x



 a2

a3
b2
b3

(cid:2)1x1 is equal to the determinant of this 4 × 4 matrix. In general,

and the coefﬁcient of x
we may write

(cid:18)

(cid:17) ∼ai

∼bj

Fji = (−1)i+j det

(17.3)
In this expression, the notation ∼ai has been used to denote the matrix obtained from
A by omitting the row ai. Thus the symbol ∼ may be read as omit, and ∼ai represents
two rows of A. The determinant appearing on the right side of (17.3) is therefore a 4× 4
determinant.

.

A different way of writing the expression for Fji makes use of the tensor rst (deﬁned

17.1 Bilinear relations

413

in section A1.1(p563)) as follows:1

(cid:28)

Fji =

(cid:29)

1
4

ipqjrs det

.

 ap

aq
br
bs

(17.4)

To see this, note that Fji is deﬁned in (17.4) in terms of a sum of determinants over all
values of p, q, r and s. However, for a given value of i, the tensor ipq is zero unless p
and q are different from i and from each other. This leaves only two remaining choices
of p and q (for example if i = 1, then we may choose p = 2, q = 3 or p = 3, q = 2).
Similarly, there are only two different choices of r and s giving rise to non-zero terms.
Thus the sum consists of four non-zero terms only. Furthermore, the determinants
appearing in these four terms consist of the same four rows of the matrices A and B and
hence have equal values, except for sign. However, the value of ipqjrs is such that the
four terms all have the same sign and are equal. Thus, the sum (17.4) is equal to the
single term appearing in (17.3).

17.1.1 Epipoles as tensors
The expression (17.3) for the fundamental matrix involves determinants of matrices
containing two rows from each of A and B. If we consider instead the determinants of
matrices containing all three rows from one matrix and one row from the other matrix,
the resulting determinants represent the epipoles. Speciﬁcally we have

(cid:17)

(cid:18)

ei = det

(cid:2)j = det

e

A
bj

(17.5)

(cid:17)

(cid:18)

ai
B

where e and e(cid:2)
are the epipoles in the two images. To see this, note that the epipole is
deﬁned by ei = aiC(cid:2)
= 0.
The formula (17.5) is now obtained by expanding the determinant by cofactors along
the ﬁrst row (in a similar manner to the derivation of (3.4–p67)).

is the centre of the second camera, deﬁned by BC(cid:2)

, where C(cid:2)

17.1.2 Afﬁne specialization
In the case where both the cameras are afﬁne cameras, the fundamental matrix has a
particularly simple form. Recall that an afﬁne camera matrix is one for which the ﬁnal
row is (0, 0, 0, 1). Now, note from (17.3) that if neither i nor j is equal to 3, then the
third rows of both A and B are present in this expression for Fij. The determinant has
two equal rows, and hence equals zero. Thus, F is of the form



FA =

a
b
c d e



1 Of course the factor 1/4 is inessential since F is deﬁned only up to scale. It is included here just to show the relationship to

(17.3).

414

17 N-Linearities and Multiple View Tensors

with all other entries being zero. Thus the afﬁne fundamental matrix has just 5 non-
zero entries, and hence 4 degrees of freedom. Its properties are described in section
14.2(p345).

Note that this argument relies solely on the fact that both cameras have the same
third row. Since the third row of a camera matrix represents the principal plane of
the camera (see section 6.2.1(p158)), it follows that the fundamental matrix for two
cameras sharing the same principal plane is of the above form.

17.2 Trilinear relations

The determinant method of deriving the fundamental matrix can be used to derive rela-
tionships between the coordinates of points seen in three views. This analysis results in
a formula for the trifocal tensor. Unlike the fundamental matrix, the trifocal tensor re-
lates both lines and points in the three images. We begin by describing the relationships
for corresponding points.

17.2.1 Trifocal point relations
Consider a point correspondence across three views: x ↔ x(cid:2) ↔ x(cid:2)(cid:2)
. Let the third
camera matrix be C and let ci be its i-th row. Analogous to (17.1) we can write an
equation describing the projection of a point X into the three images as

 A x

B
C

x(cid:2)

x(cid:2)(cid:2)

 = 0.



 X−k−k

−k

(cid:2)
(cid:2)(cid:2)

(17.6)

(17.7)

This matrix, which as before we will call X, has 9 rows and 7 columns. From the
existence of a solution to this set of equations, we deduce that its rank must be at most
6. Hence any 7 × 7 minor has zero determinant. This fact gives rise to the trilinear
relationships that hold between the coordinates of the points x, x(cid:2)
There are essentially two different types of 7 × 7 minors of X. In choosing 7 rows of
X, we may choose either

and x(cid:2)(cid:2)

.

(i) Three rows from each of two camera matrices and one row from the third, or
(ii) Three rows from one camera matrix and two rows from each of the two others.

Let us consider the ﬁrst type. A typical such 7 × 7 minor of X is of the form

(cid:2)(cid:2)i. Expanding
Note that this matrix contains only one entry in the last column, namely x
the determinant by cofactors down this last column reveals that the determinant is equal
to

(cid:17)

 A x

B
ci

x(cid:2)

(cid:2)(cid:2)i

x

 .

(cid:18)

(cid:2)(cid:2)i det

x

A x

B

x(cid:2)

.

17.2 Trilinear relations

415
(cid:2)(cid:2)i, this just leads to the bilinear relationship expressed by the
Apart from the factor x
fundamental matrix, as discussed in section 17.1.
The other sort of 7 × 7 minor is of more interest. An example of such a determinant
is of the form



det

(cid:2)j
x
(cid:2)l
x

A x
bj
bl
ck
cm

(cid:2)(cid:2)k
x
(cid:2)(cid:2)m
x

By the same sort of argument as with the bilinear relations it is seen that setting the
determinant to zero leads to a trilinear relation of the form f (x, x(cid:2)
) = 0. By ex-
panding this determinant down the column containing xi, we can ﬁnd a speciﬁc formula
as follows.

, x(cid:2)(cid:2)

 .

am
bq
cr

 al
.

 = 0uv

(17.8)

(17.9)

(17.10)

(17.11)

(17.12)

where u and v are free indices corresponding to the rows omitted from the matrices B
and C to produce (17.8). We introduce the tensor

det Xuv = −1

2

(cid:2)jx

xix

(cid:2)(cid:2)kilmjqukrv det

 al

am
bq
cr

T qr
i =

1
2

ilm det

The trilinear relationship (17.9) may then be written

(cid:2)jx

(cid:2)(cid:2)kjqukrvT qr

xix

i = 0uv.

i

The tensor T qr
is the trifocal tensor, and (17.11) is a trilinear relation such as those
discussed in section 15.2.1(p378). The indices u and v are free indices, and each choice
of u and v leads to a different trilinear relation.
Just as in the case of the fundamental matrix, one may write the formula for the
tensor T qr

in a slightly different way

i

T qr
i = (−1)i+1 det

 .

 ∼ai

bq
cr

As in section 17.1, the expression ∼ai means the matrix A with row i omitted. Note
that we omit row i from the ﬁrst camera matrix, but include rows q and r from the other
two camera matrices.
In the often-considered case where the ﬁrst camera matrix A has the canonical form
[I | 0], the expression (17.12) for the trifocal tensor may be written simply as

T qr
i = bq

i cr
4

− bq
4cr
i .

(17.13)

416

17 N-Linearities and Multiple View Tensors

Note that there are in fact 27 possible trilinear relations that may be formed in this
way (refer to (17.8)). Speciﬁcally, note that each relation arises from taking all three
rows from one camera matrix along with two rows from each of the other two matrices.
This gives the following computation.
• 3 ways to choose the ﬁrst camera matrix from which to take all three rows.
• 3 ways to choose the row to omit from the second camera matrix.
• 3 ways to choose the row to omit from the third camera matrix.
This gives a total of 27 trilinear relations. However, among the 9 ways of choosing
two rows from the second and third camera matrices, only 4 are linearly independent
(we return to this in section 17.6). This means that there are a total of 12 linearly
independent trilinear relations.

It is important to distinguish between the number of trilinear relations, however,
and the number of different trifocal tensors. As is shown by (17.11), several different
trilinear relations may be expressed in terms of just one trifocal tensor. In (17.11) each
distinct choice of the free indices u and v gives rise to a different trilinear relation, all
of which are expressible in terms of the same trifocal tensor T qr
. On the other hand,
in the deﬁnition of the trifocal tensor given in (17.10), the camera matrix A is treated
differently from the other two, in that A contributes two rows (after omitting row i) to
the determinant deﬁning any given entry of T qr
, whereas the other two camera matrices
contribute just one row. This means that there are in fact three different trifocal tensors
corresponding to the choice of which of the three camera matrices contributes two
rows.

i

i

17.2.2 Trifocal line relations
A line in an image is represented by a covariant vector li, and the condition for a point
x to lie on the line is that lixi = 0. Let Xj represent a point X in space, and ai
j represent
a camera matrix A. The 3D point Xj is mapped to the image point as xi = ai
j Xj. It
follows that the condition for the point Xj to project to a point on the line li is that
j represents a plane consisting of
liai
all points that project onto the line li.

j Xj = 0. Another way of looking at this is that liai

Consider the situation where a point Xj maps to a point xi in one image and to some

(cid:2)
q and l

(cid:2)(cid:2)
r in two other images. This may be expressed by equations

point on lines l

xi = kai

j Xj

(cid:2)
qbq
l

j Xj = 0

(cid:2)(cid:2)
r cr
l

j Xj = 0.

These may be written as a single matrix equation of the form

 A

x
(cid:2)
bq 0
l
q
(cid:2)(cid:2)
cr 0
l
r

(cid:20)



(cid:21)

X−k

= 0.

(17.14)

Since this set of equations has a solution, it follows that det X = 0, where X is the
matrix on the left of the equation. Expanding this determinant down the last column

gives

0 = − det X =

1
2

xiilm det

17.2 Trilinear relations

 =

 al

am
(cid:2)
bq
l
(cid:2)(cid:2)
q
cr
l
r

417



 al

am
bq
cr

(cid:2)
ql

(cid:2)(cid:2)
r ilm det

xil

1
2

T qr

(cid:2)(cid:2)
(cid:2)
ql
r

i

.

= xil

(17.15)
(cid:2)
This shows the connection of the trifocal tensor with sets of lines. The two lines l
(cid:2)(cid:2)
q
and l
r back-project to planes meeting in a line in space. The image of this line in the
ﬁrst image is a line, which may be represented by li. For any point xi on that line the
relation (17.15) holds. It follows that l
is the representation of the line li. Thus,
we see that for three corresponding lines in the three images

T qr

(cid:2)(cid:2)
(cid:2)
ql
r

i

(cid:2)(cid:2)
(cid:2)
ql
r

T qr

p

lp = l

(17.16)

(17.17)

(17.18)

where, of course, the two sides are equal only up to a scale factor. Since the two sides
of the relation (17.16) are vectors, this may be interpreted as meaning that the vector
product of the two sides vanishes. Expressing this vector product using the tensor ijk,
we arrive at an equation

r ipwT qr
(cid:2)
(cid:2)(cid:2)
ql

i = 0w.

lpl

In an analogous manner to the derivation of (17.11) and (17.15) we may derive a
relationship between corresponding points in two images and a line in a third image.
(cid:2)j in the ﬁrst two images,
In particular, if a point Xj in space maps to points xi and x
and to some point on a line l

(cid:2)(cid:2)
r in the third image, then the relation is

r jquT qr
(cid:2)jl
(cid:2)(cid:2)

xix

i = 0u.

In this relation, the index u is free, and there is one such relation for each choice of
u = 1, . . . ,3 , of which two are linearly independent.

We summarize the results of this section in table 17.1, where the ﬁnal column denotes

the number of linearly independent equations.

Correspondence

three points

two points, one line

one point, two lines

xix

i = 0uv

Relation

(cid:1)jx
xix

(cid:1)(cid:1)kjqukrvT qr
r jquT qr
(cid:1)jl
(cid:1)(cid:1)
rT qr
(cid:1)(cid:1)
(cid:1)
xil
ql
r piwT qr
(cid:1)(cid:1)

i = 0
i = 0w

i = 0u

(cid:1)
ql

Number of equations

4

2

1

2

three lines

lpl

Table 17.1. Trilinear relations (see also table 16.1(p391)).

Note how the different equation sets are related to each other. For instance, the
(cid:2)(cid:2)
r and

second line of the table is derived from the ﬁrst by replacing x
deleting the free index v.

(cid:2)(cid:2)kkrv by the line l

418

17 N-Linearities and Multiple View Tensors

17.2.3 Relations between two views and the trifocal tensor

(cid:2)j and x

(cid:2)j and x

(cid:2)(cid:2)kjqukrvT qr
(cid:2)jx

To this point we have considered correspondences across three views and the trifo-
cal tensor. Here we describe the constraints that arise if the correspondence is only
across two views. From the two view case, where point correspondences constrain the
fundamental matrix, we would expect some constraint on T .
(cid:2)(cid:2)k in the second and third images.
Consider the case of corresponding points x
(cid:2)(cid:2)k. The point
This means that there is a point X in space mapping to the points x
X also maps to some point xi in the ﬁrst image, but xi is not known. Nevertheless, there
(cid:2)jx
exists a relationship xix
i = 0uv between these points. For each choice
of u and v, denote Ai,uv = x
. The entries of Ai,uv are linear expressions
in the entries of T qr
(cid:2)j
that may be determined explicitly in terms of the known points x
(cid:2)(cid:2)k. There exists a point x such that xiAi,uv = 0. For each choice of u, v we may
and x
consider Ai,uv as being a 3-vector indexed by i, and for the different choices of u and v,
there are 4 linearly independent such expressions. Thus, A may be considered as a 3×4
matrix, and the condition that xiAi,uv = 0 means that Ai,uv has rank 2. This means that
every 3×3 subdeterminant of A is zero, which leads to cubic constraints on the elements
of the trifocal tensor. For geometric reasons, it appears that the equations xiAi,uv are
not algebraically independent for the four choices of u and v. Consequently we obtain
a single cubic constraint on T jk
from a two-view point correspondence. Details are left
to the reader.

(cid:2)(cid:2)kjqukrvT qr

i

i

i

In the case where the point correspondence is between the ﬁrst and second (or third)
views the analysis is slightly different. However, the result in each case is that although
a point correspondence across two views leads to a constraint on the trifocal tensor,
this constraint is not a linear constraint as it is in the case of correspondences across
three views.

17.2.4 Afﬁne trifocal tensor

is zero. This is the case for elements T j3

In the case where all three cameras are afﬁne, the trifocal tensor satisﬁes certain con-
straints. A camera matrix is afﬁne if the last row is (0, 0, 0, 1). It follows that if two
of the rows in the matrix in (17.12) are of this form, then the corresponding element of
T jk
3 – a total of 11
i
elements. Thus the trifocal tensor contains 16 non-zero entries, deﬁned up to scale. As
in the case of the afﬁne fundamental matrix, this analysis is equally valid for the case
of cameras sharing the same principal plane.

and T 33

1 ,T 3k

2 ,T 3k

1 ,T j3

2

17.3 Quadrilinear relations

Similar arguments work in the case of four views. Once more, consider a point corre-
spondence across 4 views: x ↔ x(cid:2) ↔ x(cid:2)(cid:2) ↔ x(cid:2)(cid:2)(cid:2)
. With camera matrices A, B, C and D,

17.3 Quadrilinear relations

the projection equations may be written as

 A x

B
C
D

x(cid:2)

x(cid:2)(cid:2)

x(cid:2)(cid:2)(cid:2)

 = 0.

419

(17.19)





X−k−k
(cid:2)
−k
(cid:2)(cid:2)
−k
(cid:2)(cid:2)(cid:2)

Since this equation has a solution, the matrix X on the left has rank at most 7, and
so all 8 × 8 determinants are zero. As in the trilinear case, any determinant containing
only one row from one of the camera matrices gives rise to a trilinear or bilinear relation
between the remaining views. A different case occurs when we consider 8× 8 determi-
nants containing two rows from each of the camera matrices. Such a determinant leads
to a new quadrilinear relationship of the form

(cid:2)jx

(cid:2)(cid:2)kx

(cid:2)(cid:2)(cid:2)lipwjqxkrylszQpqrs = 0wxyz

xix

(17.20)

where each choice of the free variables w, x, y and z gives a different equation, and the
4-dimensional quadrifocal tensor Qpqrs is deﬁned by

Qpqrs = det

(17.21)

.

 ap

bq
cr
ds

Note that the four indices of the four-view tensor are contravariant, and there is no dis-
tinguished view as there is in the case of the trifocal tensor. There is only one four-view
tensor corresponding to four given views, and this one tensor gives rise to 81 different
quadrilinear relationships, of which 16 are linearly independent (see section 17.6).

As in the case of the trifocal tensor, there are also relations between lines and points
in the case of the four-view tensor. Equations relating points are really just special
cases of the relationship for lines. In the case of a 4-line correspondence, however,
something different happens, as will now be explained. The relationship between a set
of four lines and the quadrifocal tensor is given by the formula

lpl

(cid:2)(cid:2)
(cid:2)
ql
r l
(cid:2)(cid:2)(cid:2)
(cid:2)
for any set of corresponding lines lp, l
s . However, the derivation shows that
q, l
this condition will hold as long as there is a single point in space that projects onto the
four image lines. It is not necessary that the four image lines correspond (in the sense
that they are the image of a common line in space). This conﬁguration is illustrated in
ﬁgure 17.1a.

(cid:2)(cid:2)(cid:2)
s Qpqrs = 0
(cid:2)(cid:2)
r and l

(17.22)

Now, consider the case where three of the lines (for instance l

(cid:2)(cid:2)(cid:2)
s ) correspond
by deriving from a single 3D line (ﬁgure 17.1b). Now let lp be any arbitrary line in the
ﬁrst image. The back-projection of this line is a plane, which will meet the 3D line in
a single point, X, and the conditions are present for (17.22) to hold. Since this is true
(cid:2)(cid:2)(cid:2)
(cid:2)
for any arbitrary line lp, it must follow that l
s Qpqrs = 0p. This gives three linearly
ql
(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)
independent equations involving l
r and l
s . However, given a set of corresponding

(cid:2)(cid:2)
r and l

(cid:2)
q, l

(cid:2)
q, l

(cid:2)(cid:2)
r l

420

17 N-Linearities and Multiple View Tensors

l

l

C

C

l///

C/ //

l / /

l///

C/ //

l / /

X

L

X

/l

/

l

/

C

a

/

C

b

/ /

C

/ /

C

Fig. 17.1. Four line “correspondence”. The four lines l, l(cid:1)
satisfy the quadrilinear relation
(17.22) since their back-projections intersect in a common point X. (a) No three planes intersect in a
common line. (b) The three lines l(cid:1) ↔ l(cid:1)(cid:1) ↔ l(cid:1)(cid:1)(cid:1)

are the image of the same line L in 3-space.

, l(cid:1)(cid:1)(cid:1)

, l(cid:1)(cid:1)

lines in four images, as above, we may choose a subset of three lines, and for each line-
triplet obtain three equations in this way. Since there are four choices of line-triplets,
the total number of equations is thus 12.

= l(cid:2)(cid:2)

= l(cid:2)(cid:2)(cid:2)

However only 9 of these equations are independent, and this may be seen as follows.
Suppose that l = l(cid:2)
= (1, 0, 0)T. This is equivalent to the general situation
since by applying projective transformations to each of the four images we can trans-
(cid:2)(cid:2)(cid:2)
form an arbitrary line correspondence to this case. Now the equation l
s Qpqrs = 0p
means that any element Qp111 = 0. Applying this argument to all four triplets of three
views, we ﬁnd that Qpqrs = 0 whenever at least three of the indices are 1. There are
a total of 9 such elements. Since the set of equations generated by the line correspon-
dence is equivalent to setting each of these elements to zero, among the total of 12
equations there are just 9 independent ones.

(cid:2)
ql

(cid:2)(cid:2)
r l

The four-view relations are summarized in table 17.2. No equation is given here for
the case of three lines and one point, since this gives no more restrictions on the tensor
than just the three-line correspondence.

17.4 Intersections of four planes

421

Relation

Number of equations

Correspondence

four points

three points, one line

two points, two lines

xix

(cid:1)jx
xix

(cid:1)(cid:1)kx
(cid:1)jx
xix

(cid:1)(cid:1)(cid:1)lipwjqxkrylszQpqrs = 0wxyz
(cid:1)(cid:1)kl
(cid:1)(cid:1)(cid:1)
s ipwjqxkryQpqrs = 0wxy
(cid:1)(cid:1)
(cid:1)jl
r l

(cid:1)(cid:1)(cid:1)
s ipwjqxQpqrs = 0wx

three lines

four lines

(cid:1)
ql

lpl

(cid:1)
ql
(cid:1)(cid:1)
r Qpqrs = 0s,

lpl

(cid:1)(cid:1)
r Qpqrs = 0s

(cid:1)
ql

(cid:1)(cid:1)(cid:1)
s Qpqrs = 0r,

lpl

. . .

16

8

4

3

9

Table 17.2. Quadrilinear relations.

17.4 Intersections of four planes

The multi-view tensors may be given a different derivation, which sheds a little more
light on their meaning. In this interpretation, the basic geometric property is the inter-
section of four planes. Four planes in space will generally not meet in a common point.
A necessary and sufﬁcient condition for them to do so is that the determinant of the
4 × 4 matrix formed from the vectors representing the planes should vanish.

In this section only we shall represent the determinant of a 4 × 4 matrix
Notation.
with rows a, b, c and d by a∧ b∧ c∧ d. In a more general context, the symbol ∧ rep-
resents the meet (or intersection) operator in the double algebra (see literature section
of this chapter). However, for the present purposes the reader need only consider it as
a shorthand for the determinant.
We start with the quadrifocal tensor for which the derivation is easiest. Consider four
lines l, l(cid:2)
in images formed from four cameras with camera matrices A, B,
C and D. The back projection of a line l through camera A is written as the plane liai,
with notation as in (17.14). The condition that these four planes are coincident may be
written as

and l(cid:2)(cid:2)(cid:2)

, l(cid:2)(cid:2)

(lpap) ∧ (l

(cid:2)
q

bq) ∧ (l

(cid:2)(cid:2)
r

cr) ∧ (l

(cid:2)(cid:2)(cid:2)
s

ds) = 0.

However, since the determinant is linear in each row, this may be written as

0 = lpl

(cid:2)
ql

s (ap ∧ bq ∧ cr ∧ ds) def= lpl
(cid:2)
(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)
ql
r l

(cid:2)(cid:2)
r l

(cid:2)(cid:2)(cid:2)
s Qpqrs.

(17.23)

This corresponds to the deﬁnition (17.21) and line relation (17.22) for the quadrifocal
tensor. The basic geometric property is the intersection of the four planes in space.

tensor derivation.
↔ l

Trifocal
Consider now a point–line–line relationship
xi ↔ l
(cid:2)
p and l2
q be two lines in the ﬁrst image that pass
j
through the image point x. The planes back-projected from the four lines meet in a
point (see ﬁgure 17.2). So we can write:

(cid:2)(cid:2)
k for three views and let l1

r (al ∧ am ∧ bq ∧ cr) = 0 .
(cid:2)
(cid:2)(cid:2)
ql

l l2
l1
ml

422

17 N-Linearities and Multiple View Tensors

L

x

C

l /

/

C

l / /

/ /

C

Fig. 17.2. A point–line–line correspondence x ↔ l(cid:1) ↔ l(cid:1)(cid:1)
involving three images may be interpreted as
follows. Two arbitrary lines are chosen to pass through the point x in the ﬁrst image. The four lines then
back-project to planes that meet in a point in space.

(cid:16)

= 0 .

al ∧ am ∧ bq ∧ cr
(cid:16)

The next step is an algebraic trick – to multiply this equation by the ilmilm. This is a
scalar value (in fact equal to 6, the number of permutations of (ilm)). The result after
regrouping is

(cid:15)

(cid:16)

(cid:15)

l1
l l2

milm

(cid:2)
l
ql

(cid:2)(cid:2)
r ilm

l l2

milm is simply the cross-product of the two lines ll and lm, in

Now the expression l1
other words their intersection point, xi. Thus ﬁnally we can write
(cid:2)(cid:2)
(cid:2)
ql
r

ilm(al ∧ am ∧ bq ∧ cr)

0 = xil

def= xil

(cid:2)(cid:2)
(cid:2)
ql
r

(cid:15)

T qr

i

(17.24)

which are the deﬁnition (17.10) and basic incidence relation (17.15) for the trifocal
tensor.

Fundamental matrix. We can derive the fundamental matrix in the same manner.
Given a correspondence x ↔ x(cid:2)
q passing through x, and
(cid:2)1
. The back-projected planes all meet in a point, so we
r and l
l
write

(cid:2)2
s passing through x(cid:2)

, select pairs of lines l1

p and l2

(cid:2)1
r l

s (ap ∧ aq ∧ br ∧ bs) = 0 .
(cid:2)2

p l2
l1
q l

Multiplying by (ipqipq)(jrsjrs) and proceeding as before leads to the coplanarity
constraint

0 = xix

(cid:2)j (ipqjrs(ap ∧ aq ∧ br ∧ bs)) def= xix

(cid:2)jFji

(17.25)

which can be compared with (17.4).

17.5 Counting arguments

In this section we specify the number of points or lines required to carry out reconstruc-
tion from several views. This analysis is related to counting the number of degrees of
freedom of the associated tensors. However, in doing this it is necessary to distinguish
between the number of degrees of freedom of the tensor viewed as an unconstrained

17.5 Counting arguments

423

algebraic object, and the number of degrees of freedom that arises from a conﬁguration
of cameras and their camera matrices.
For instance, consider the fundamental matrix. At one level, the fundamental matrix
may be considered as a homogeneous 3×3 matrix, and hence has 8 degrees of freedom
(9 minus 1 for the indeterminate scale). On the other hand, the fundamental matrix aris-
ing from a pair of camera matrices according to (17.3–p412) must satisfy the additional
constraint det F = 0. Hence, such a fundamental matrix has just 7 degrees of freedom.
Since the camera matrices may be determined up to a 3D projectivity from the funda-
mental matrix (and vice versa), we may count the number of degrees of freedom of a
fundamental matrix by counting the degrees of freedom of the camera matrices. Two
camera matrices have a total of 22 degrees of freedom (two homogeneous 3 × 4 ma-
trices). A 3D homography is represented by a 4 × 4 homogeneous matrix, and so has
15 degrees of freedom. This gives a total of 7 = 22 − 15 degrees of freedom for the
conﬁguration of the two cameras, modulo a projective transformation. This agrees with
the 7 degrees of freedom of the fundamental matrix, providing a check on the previous
calculation.
Similarly, the trifocal tensor encodes the projective structure of three camera matri-
ces, and hence has 18 = 3∗11−15 degrees of freedom. In the same way the quadrifocal
tensor has 29 = 4 ∗ 11 − 15 degrees of freedom. Generally for m cameras, we have

# dof = 11m − 15.

Since the trifocal and quadrifocal tensors, considered just as homogeneous algebraic
arrays have 26 and 80 degrees of freedom respectively, they must satisfy an additional
set of constraints, dictated by the camera geometry – 8 constraints for the trifocal tensor
and 51 constraints for the quadrifocal tensor.

In computing geometric structure, we may use linear algebraic methods based on
estimating the multifocal tensor by solving the constraints from the multi-linearities.
The number of correspondences required is determined from the number of equations
generated by each point or line correspondence. This linear method takes no account
of the constraints imposed on the tensors by their geometry.

On the other hand, the number of correspondences required may be determined by
counting the number of geometric constraints given by each correspondence and com-
paring with the total number of degrees of freedom of the system. Consider a conﬁgu-
ration of n points in m views. The total number of degrees of freedom of this system
is 11m− 15 + 3n, since each of the n 3D points has 3 degrees of freedom. To estimate
the projective structure the available data are the images of the n points in m images,
a total of 2mn measurements (each 2D point having two coordinates). Thus for recon-
struction to be possible, we require 2mn ≥ 11m− 15 + 3n, or (2m− 3)n ≥ 11m− 15.
Thus the required number of points is

n ≥ 11m − 15
2m − 3

= 5 +

m

2m − 3

.

One may also think of this by observing that each point correspondence contributes

424

17 N-Linearities and Multiple View Tensors

# views

tensor

# elems

# dof

linear

# points

# lines

non-linear

# points

# lines

2

3

4

F
T

Q

9

27

81

7

18

29

8

7

6

—

13

9

∗
7
∗

6

6

—
∗
9

?

8?

Table 17.3. Projective degrees of freedom and constraints. The linear column indicates the minimum
number of correspondences across all views required to solve linearly for the tensor (up to scale). The
non-linear is the minimum number of correspondences required. A star indicates multiple solutions, and
a question-mark indicates that no practical reconstruction method is known.

2m − 3 constraints on the cameras, that is 2m for the coordinates of the points in each
view, less 3 for the added degree of freedom of the 3D point.

An analogous argument applies for line correspondences. A line has four degrees
of freedom in 3-space, and its image line is described by 2 degrees of freedom, so
that each line correspondence provides 2m − 4 constraints and the number of lines l
required is

l ≥ 11m − 15
2m − 4

.

In either of these cases, if the number of constraints (equations) is equal to the num-
ber of degrees of freedom (unknowns) of the cameras and points or lines conﬁguration,
then we can in general expect multiple solutions, except in the linear case. However,
if there are more equations than unknowns, the system is over-determined, and in the
generic case there will exist a single solution.

Table 17.3 summarizes the number of correspondences required for reconstruction.
A star (*) indicates that multiple solutions are expected. For the case of non-linear solu-
tions, actual methods of solution are not always known, other than by brute-force gen-
eration and solution of a set of simultaneous polynomial equations. Situations where
no simple method is known are marked with a question-mark. Note that not much is
known about non-linear solutions for lines. Speciﬁc non-linear algorithms are:

(i) 7 points in 2 views: see section 11.1.2(p281). Three solutions are possible.
(ii) 6 points in 3 views: see section 20.2.4(p510). Three solutions are possible.

This is the dual (see chapter 20) of the previous problem.

(iii) 6 points in 4 views. Solve using 6 points in 3 views, then use the camera resec-
tion DLT algorithm (section 7.1(p178)) to ﬁnd the fourth view and eliminate all
but one solution. However, unlike the previous two cases, the case of 6 points
in 4 views is overdetermined, and this solution is only valid for perfect data.
The estimation for noisy data is discussed in chapter 20.

17.5 Counting arguments

425

# views

tensor

# non-zero
elements

# dof

linear

# points

# lines

non-linear

# points

# lines

2

3

4

FA
TA

QA

5

16

48

4

12

20

4

4

4

—

8

6

4

4

4

—
∗
6
∗
5

?

?

Table 17.4. Afﬁne degrees of freedom and constraints. The camera for each view is afﬁne. See
caption of table 17.3 for details.

17.5.1 Afﬁne cameras
For afﬁne cameras, the reconstruction problem requires fewer correspondences. The
plane at inﬁnity may be identiﬁed in a reconstruction, being the principal plane of all
of the cameras, and the reconstruction ambiguity is afﬁne, not projective. The number
of degrees of freedom for m afﬁne cameras is

# dof = 8m − 12.

Each view adds 8 degrees of freedom for the new 3 × 4 afﬁne camera matrix, and we
subtract 12 for the degrees of freedom of a 3D afﬁne transformation.
As in the projective case a point correspondence imposes 2m − 3 constraints, and a
line correspondence 2m − 4 constraints. As before the number of required points may
be computed as n(2m − 3) ≥ 8m − 12, i.e.

n ≥ 8m − 12
2m − 3

= 4.

For lines the result is

l ≥ 8m − 12
2m − 4

= 4 +

2

m − 2

.

As for linear methods, the number of elements in the tensor is 3m.

In the afﬁne
case the tensor is, as always, only deﬁned up to scale, but additionally many of the
elements are zero, as seen in section 17.1.2 and section 17.2.4. This cuts down on the
required number of correspondences. Counting results are given in table 17.4. Note
that for point correspondences, the linear algorithms apply with the minimum number
of correspondences given by the above equation. Hence, the non-linear algorithms are
the same as the linear ones.

17.5.2 Knowing four coplanar points – plane plus parallax
The previous counting arguments were for the case of points and lines in general po-
sition. Here we consider the important case that four or more of the 3D points are
coplanar. It will be see that the computation of the tensors, and consequently projec-
tive structure, is signiﬁcantly simpliﬁed. This discussion is given in terms of knowing

426

17 N-Linearities and Multiple View Tensors

# views

tensor

# dof

# points

points

# constraints

# lines

lines
# constraints

2

3

4

F
T

Q

2

5

8

2

2

2

2

5

8

—

5

4

5

8

Table 17.5. Number of additional correspondences required to compute projective structure given 2D
homographies between the views, induced by a plane. The homographies may be computed from four or
more point matches derived from coplanar points, or by any other means. Points used to compute the
homographies are not counted in this table.

four coplanar points in the images. However, all that is really important here is that
inter-image homographies induced by a plane should be known.

Computation of the fundamental matrix knowing four coplanar points was consid-
ered in section 13.3(p334) (see algorithm 13.2(p336)), and for the trifocal tensor in
section 16.7.1. Now, we consider the four-view case. From the correspondence of 4
(or more) points derived from 3D points in a plane, we can compute the homographies
H(cid:2)
, H(cid:2)(cid:2)
from the ﬁrst view to the second, third and fourth views respectively,
induced by the plane. In a projective reconstruction, we choose the plane containing
the points as the plane at inﬁnity, in which case H(cid:2)
are the inﬁnite homogra-
phies. Assuming further that the ﬁrst image is at the origin, the four camera matrices
may now be written as

and H(cid:2)(cid:2)(cid:2)

and H(cid:2)(cid:2)(cid:2)

, H(cid:2)(cid:2)

, t(cid:2)(cid:2)

A = [I | 0] B = [H(cid:2)|t(cid:2)
, t(cid:2)(cid:2)(cid:2)

] D = [H(cid:2)(cid:2)(cid:2)|t(cid:2)(cid:2)(cid:2)
may be determined only up to a common scale.

] C = [H(cid:2)(cid:2)|t(cid:2)(cid:2)

The vectors t(cid:2)
Since the left hand 3 × 3 blocks of the camera matrices are now known, it is easily
seen from (17.21) that the entries of Qpqrs are linear in the remaining entries t(cid:2)
, t(cid:2)(cid:2)
and t(cid:2)(cid:2)(cid:2)
of the camera matrices. Using (17.21) we can write down an expression for
the entries of Qpqrs in terms of the entries of t(cid:2)
. In fact, we may write this
explicitly as q = Mt, where M is an 81×9 matrix and q and t are vectors representing the
entries of Q and t(cid:2)
. Thus, the quadrifocal tensor may be linearly parametrized
in terms of 9 homogeneous coordinates and hence has 8 degrees of freedom.

, . . . ,t (cid:2)(cid:2)(cid:2)

and t(cid:2)(cid:2)(cid:2)

]

, t(cid:2)(cid:2)

Now, given a set of equations Eq = 0 derived from correspondences as in table 17.2,
they may be rewritten in terms of the minimal parameter set t by substituting q = Mt
arriving at EMt = 0. This allows a linear solution for t and hence the camera matrices,
and (if desired) the tensor, q = Mt. Note the important advantage here that the tensor
so obtained automatically corresponds to a set of camera matrices, and so satisﬁes all
geometric constraints – the troublesome 51 constraints that must be satisﬁed by the
quadrifocal tensor disappear into thin air.

The above analysis was for the computation of the quadrifocal tensor, but it applies

equally well to the fundamental matrix and trifocal tensor as well.

17.5 Counting arguments

427

Counting argument. We return to the general case of m views and consider the situa-
tion from a geometric point of view. The number of degrees of freedom parametrizing
the tensor is equal to the number of geometric degrees of freedom remaining in the
camera matrices, namely 3m − 4 where m is the number of views. This arises from
3(m − 1) for the last column of each camera matrix apart from the ﬁrst, less one for
the common scaling.

Counting the number of constraints imposed by a point or a line correspondence
is a little tricky, however, and shows that one must always be careful with counting
arguments not to neglect hidden dependencies. First, consider a line correspondence
l ↔ l(cid:2) ↔ l(cid:2)(cid:2)
across three views – the argument will hold in general for m ≥ 3 views.
The question to be resolved is just how much information can be derived from the
measurement of the image lines. Surprisingly, knowledge of the plane homographies
between the images reduces the amount of information that a line correspondence pro-
vides. For simplicity of argument, we may assume that planar homographies have been
applied to the images so that the four coplanar reference points map to the same point
in each image. As a result any further point in the reference plane will map to the same
point in each image. Now the 3D line L from which the line correspondence is de-
rived must meet the reference plane in a point X. Since it lies on the reference plane, X
, l(cid:2)(cid:2)
projects to the same point x in all images, and x must lie on all three image lines l, l(cid:2)
.
Thus, corresponding lines in the three views cannot be arbitrary – they must all pass
through a common image point. In the general case the number of degrees of freedom
of the measurements of the lines in m views is m + 2. To specify the point x requires
2 degrees of freedom, and each line through the point then has one remaining degree
of freedom (its orientation). Subtracting 4 for the four added degrees of freedom of the
line in space, we see
• Each line correspondence across m views generates m − 2 constraints on the re-
maining degrees of freedom of the cameras.

Note how the condition that the image lines must meet in a point restricts the number
of equations generated by the line correspondence. Without observing this condition,
one would expect 2m − 4 constraints from each line. However, with perfect data the
set of 2m− 4 equations will have rank only m− 2. With noisy data the image lines will
not be exactly coincident, and the system may have full rank, but this will be entirely
due to noise, and the smallest singular values of the system will be essentially random.
One should, however, include all available equations in solving the system, since this
will diminish the noise effects.

For point correspondences the argument is similar. The line through any two image
points is the image of a line in space. The projections of this 3D line are constrained
as discussed in the preceding discussion, and this imposes constraints on the matching
points. The measurements have 3m + 2 degrees of freedom: 2 for the intersection
point of the line with the plane and, in each view, one for the line orientation and two
corresponding to the position of each of the two points on the line. Subtracting 6 for
the degrees of freedom of the two points in 3-space, the result is

428

17 N-Linearities and Multiple View Tensors

• Two point correspondences across m views generate 3m − 4 constraints on the re-
maining degrees of freedom of the cameras.

Since this is the same as the number of degrees of freedom of the cameras, 2 points are
sufﬁcient to compute structure.

Since the number of geometric constraints on the cameras is the same as the number
of degrees of freedom of the tensor, there is no distinction between constraints on the
tensor and constraints on the geometry. Thus, point and line correspondences may
be used to generate linear constraints on the tensor. There is no need for non-linear
methods. The number of required correspondences is summarized in table 17.5.

17.6 Number of independent equations

It was asserted in table 17.2 that each four-view point correspondence gives rise to 16
linearly independent equations in the entries of the quadrifocal tensor. We now examine
this point more closely.

Given sufﬁciently many point matches across four views, one may solve for the
tensor Qpqrs. Once Q is known, it is possible to solve for the camera matrices and
hence compute projective structure. This was shown in [Heyden-95b, Heyden-97c,
Hartley-98c] but is not discussed further in this book. A curious phenomenon oc-
curs however when one counts the number of point matches necessary to do this. As
indicated above, it appears to be the case that each point match gives 16 linearly in-
dependent equations in the entries of the tensor Qpqrs, and it seems unlikely that the
equations derived from two totally unrelated sets of point correspondences could have
any dependencies. It would therefore appear that from ﬁve point correspondences one
obtains 80 equations, which is enough to solve for the 81 entries of Qpqrs up to scale.
From this argument it would seem that it is possible to solve for the tensor from only
ﬁve point matches across four views, and thence one may solve for the camera matri-
ces, up to the usual projective ambiguity. This conclusion however is contradicted by
the following remark.
• It is not possible to determine the positions of four (or any number of) cameras from
the images of ﬁve points.

This follows from the counting arguments of section 17.5. Obviously there is some
error in our counting of equations. The truth is contained in the following.

Result 17.1. The full set of 81 linear equations (17.20) derived from a single point
correspondence x ↔ x(cid:2) ↔ x(cid:2)(cid:2) ↔ x(cid:2)(cid:2)(cid:2)
across four views contains 16 independent
constraints on Qpqrs. Furthermore, let the equations be written as Aq = 0 where A
is an 81 × 81 matrix and q is a vector containing the entries of Qpqrs. Then the 16
non-zero singular values of A are all equal.

What this result is saying is that indeed as expected one obtains 16 linearly indepen-
dent equations from one point correspondence, and in fact it is possible to reduce this
set of equations by an orthogonal transformation (multiplication of the equation matrix
A on the left by an orthogonal matrix U) to a set of 16 orthogonal equations. The proof

17.6 Number of independent equations

429

is postponed until the end of this section. The surprising fact however is that the equa-
tion sets corresponding to two unrelated point correspondences have a dependency, as
stated in the following result.
Result 17.2. The set of equations (17.20–p419) derived from a set of n general point
correspondences across four views has rank 16n − (n
2 ) = n(n −
The notation (n
1)/2. Thus for 5 points there are only 70 independent equations, not enough to solve
for Qpqrs. For n = 6 points, 16n − (n
2 ) = 81, and we have enough equations to solve
for the 81 entries of Qpqrs.

2 ) means the number of choices of 2 among n, speciﬁcally, (n

2 ), for n ≤ 5.

We now prove the two results above. The key point in the proof of result 17.1 con-

cerns the singular values of a skew-symmetric matrix (see result A4.1(p581)).
Result 17.3. A 3 × 3 skew-symmetric matrix has two equal non-zero singular values.
The rest of the proof of result 17.1 is quite straightforward as long as one does not get
lost in notation.

Proof. (Result 17.1) The full set of 81 equations derived from a single point correspon-
(cid:2)(cid:2)(cid:2)llszQpqrs = 0wxyz. A total of 81 equations
dence is of the form xiipwx
are generated by varying w, x, y, z over the range 1, . . . ,3 . Thus, the equation matrix
A may be written as

(cid:2)(cid:2)kkryvx

(cid:2)jjqxx

A(wxyz)(pqrs) = xiipwx

(cid:2)jjqxx

(cid:2)(cid:2)kkryx

(cid:2)(cid:2)(cid:2)llsz

(17.26)

where the indices (wxyz) index the row and (pqrs) index the column of A. We will
consider a set of indices, such as (wxyz) in this case, as a single index for the row
or column of a matrix. This situation will be indicated by enclosing the indices in
parentheses as here, and referring to them as a combined index.
We consider now the expression xiipw. This may be considered as a matrix indexed
by the free indices p and w. Furthermore, since xiipw = −xiiwp we see that it is a
skew-symmetric matrix, and hence has equal singular values. We denote this matrix by
Swp. Writing result 17.3 using tensor notation, we have

Uw
a

SwpVp

e = kDae

(17.27)

where the diagonal matrix D is as in result 17.3. The matrix A in (17.26) may be written
as A(wxyz)(pqrs) = SwpS(cid:2)
U(cid:2)(cid:2)(cid:2)z

S(cid:2)(cid:2)(cid:2)
zs. Consequently, applying (17.27) we may write

U(cid:2)(cid:2)y

V(cid:2)(cid:2)r

U(cid:2)x

S(cid:2)(cid:2)

(cid:2)(cid:2)(cid:2)DaeDbf DcgDdh.

yr

xq
A(wxyz)(pqrs)Vp
e

V(cid:2)(cid:2)(cid:2)s
h = kk

(cid:2)

(17.28)

(cid:2)(cid:2)

k

k

(cid:2)q
V
f

c

d

g

Uw
a

b
Now, writing

ˆU(wxyz)
(abcd) = Uw
a
(cid:2)(cid:2)

(cid:2)

and ˆk = kk

k

k

U(cid:2)x
b
(cid:2)(cid:2)(cid:2)

U(cid:2)(cid:2)y

c

U(cid:2)(cid:2)(cid:2)z

d

ˆV(pqrs)
(ef gh) = Vp
e

(cid:2)q
V
f

V(cid:2)(cid:2)r

g

V(cid:2)(cid:2)(cid:2)s

h

ˆD(abcd)(ef gh) = DaeDbf DcgDdh

we see that (17.28) may be written as

ˆU(wxyz)
(abcd)

A(wxyz)(pqrs)ˆV(pqrs)

(ef gh) = ˆkˆD(abcd)(ef gh).

(17.29)

As a matrix, D(abcd)(ef gh) is diagonal with 16 non-zero diagonal entries, all equal to

430

17 N-Linearities and Multiple View Tensors

unity. To show that (17.29) is the SVD of the matrix A(wxyz)(pqrs), and hence to complete
the proof, it remains only to show that U(wxyz)
(ef gh) are orthogonal matrices. It is
necessary only to show that the columns are orthonormal. This is straightforward and
is left as an exercise.

(abcd) and V(pqrs)

(cid:2)i ↔ y

(cid:2)i ↔ x

(cid:2)(cid:2)i ↔ y

(cid:2)(cid:2)i ↔ x

(cid:2)(cid:2)(cid:2)i and yi ↔ y

Proof.
(Result 17.2) We consider two point correspondences across four views,
namely xi ↔ x
(cid:2)(cid:2)(cid:2)i. These give rise to
two sets of equations Axq = 0 and Ayq = 0 of the form (17.26). Each of these ma-
trices has rank 16, and if the rows of Ax are independent of the rows of Ay, then the
rank of the combined set of equations is 32. However, if there is a linear dependence
between the rows of Ax and those of Ay then their combined rank is at most 31.
We deﬁne a vector sx with combined index (pqrs) by s(pqrs)
sy is similarly deﬁned. We will demonstrate that sT
y Ax = sT
rows of Ax and Ay are linearly dependent.
Expanding sT

(cid:2)(cid:2)(cid:2)s. A vector
= xpx
x Ay, which means that the

y Ax gives

(cid:2)(cid:2)rx

(cid:2)qx

x

y

sT
Ax
y Ax = s(wxyz)
(wxyz)(pqrs)
(cid:2)(cid:2)(cid:2)yy
(cid:2)(cid:2)xy
= (ywy
(cid:2)(cid:2)(cid:2)kx
(cid:2)(cid:2)jx
= (xix
= sT
x Ay.

(cid:2)(cid:2)(cid:2)(cid:2)z)xiipwx
(cid:2)(cid:2)(cid:2)(cid:2)l)ywwpiy

(cid:2)jjqxx
(cid:2)xxqjy

(cid:2)(cid:2)kkryx
(cid:2)(cid:2)yyrky

(cid:2)(cid:2)(cid:2)llsz
(cid:2)(cid:2)(cid:2)zzsl

, x(cid:2)(cid:2)

, y(cid:2)(cid:2)

, x(cid:2)(cid:2)(cid:2)

, y, y(cid:2)

and y(cid:2)(cid:2)(cid:2)

This demonstrates that the rows of Ax and Ay are dependent, and their combined rank
is at most 31. We now consider the possibility that the combined rank is less than
31. In such a case, all 31 × 31 subdeterminants of the matrix [AxT, AyT]T must vanish.
These subdeterminants may be expressed as polynomial expressions in the coefﬁcients
of the points x, x(cid:2)
. These 24 coefﬁcients together generate a
24-dimensional space. Thus, there is a function f : IR24 → IRN for some N (equal to
the number of such 31 × 31 subdeterminants), such that the equation matrix has rank
less than 31 only on the set of zeros of the function f. Any arbitrarily chosen example
(omitted) may be used to show that the function f is not identically zero. It follows that
the set of point correspondences for which the set of equations has rank less than 31 is
a variety in IR24, and hence is nowhere dense. Thus, the set of equations generated by
a general pair of point correspondences across four views has rank 31.
We now turn to the general case of n point correspondences across all four views. Note
that the linear relationship that holds for two point correspondences is non-generic, but
depends on the pair of correspondences. In general, therefore, given n point correspon-
dences, there will be (n
2 ) such relationships. This reduces the dimension of the space
spanned by the set of equations to 16n − (n

2 ) as required.

The three-view case. Similar arguments hold in the three-view case. It can be proved
in the same manner that the nine equations arising from a point match contain four
independent equations. This is left to an exercise (see page 432).

17.7 Choosing equations

17.7 Choosing equations

431

In section 17.6 a proof was given that the singular values of the full set of equations
derived from four point equations are all equal. The proof may easily be adapted to
the three-view case as well (see exercise, page 432). The key point in the argument is
that the two non-zero singular values of a 3× 3 skew-symmetric matrix are equal. This
proof may clearly be extended to apply to any of the other sets of equations derived
from line or point correspondences given in sections 17.2 and 17.3.

We consider still the four-view case. The results on singular values show that it is in
general advisable to include all 81 equations derived from this correspondence, rather
than selecting just 16 independent equations. This will avoid difﬁculties with near
singular situations. This conclusion is supported by experimental observation. Indeed,
numerical examples show that the condition of a set of equations derived from several
point correspondences is substantially better when all equations are included for each
point correspondence. In this context, the condition of the equation set is given by the
ratio of the ﬁrst (largest) to the n-th singular value, where n is the number of linearly
independent equations.

Including all 81 equations rather than just 16 means that the set of equations is larger,
leading to increased complexity of solution. This can be remedied as follows. The
basis for the equality of the singular values is that Swp = xiipw and the other similarly
deﬁned terms are skew symmetric matrices, and hence have equal singular values. The
same effect can be achieved by any other matrix S with equal singular values. We
require only that the columns of S should represent lines passing through the point x (
otherwise stated Swpxw = 0p). Matrix S will have equal singular values if its columns
are orthonormal. These conditions may be achieved with S being a 3× 2 matrix. If this
is done for the point in each view the total number of equations will be reduced from
34 = 81 to 24 = 16, and the 16 equations will be orthogonal. A convenient way of
choosing S is to use a Householder matrix (see section A4.1.2(p580)) as shown below.
This discussion also applies to the trifocal tensor, allowing us to reduce the number
of equations from 9 to 4, while retaining equal singular values. Summarizing this
discussion for the trifocal tensor case:
• Given a point correspondence x ↔ x(cid:2) ↔ x(cid:2)(cid:2)
of the form

across three views, generate equations

(cid:2)
xiˆl
qx

(cid:2)(cid:2)
ˆl
ry

T qr
i = 0xy for x, y = 1, 2

q2 are two lines through x(cid:2)
(cid:2)
(cid:2)
where ˆl
q1 and ˆl
(cid:2)(cid:2)
(cid:2)
(cid:2)(cid:2)
similarly ˆl
qx and ˆl
ry). A convenient way to ﬁnd ˆl
ry is as follows.
(cid:2)qh
(cid:2)
qx = δ3x and x

(cid:2)
(cid:2)(cid:2)
ry such that x
qx and h

(i) Find Householder matrices h

represented by orthonormal vectors (and

(cid:2)(cid:2)rh

(cid:2)
ry =

δ3y.

(cid:2)
(ii) For x, y = 1, 2 set ˆl
qx = h

(cid:2)(cid:2)
(cid:2)
qx and ˆl
ry = h

(cid:2)(cid:2)
ry.

It is evident that essentially this method will work for all the types of equations

summarized in table 17.1(p417) and table 17.2(p421).

432

17 N-Linearities and Multiple View Tensors

(cid:2)(cid:2)
(cid:2)
ql
r

This chapter suggests that the most basic relations are the point–line–line correspon-
T qr
dence equation xil
i = 0 in the three-view case, and the line-correspondence
(cid:2)(cid:2)(cid:2)
equation lpl
s Qpqrs = 0 for four views. Indeed numerical robustness may be en-
hanced by reducing other correspondences to this type of correspondence, for carefully
selected lines.

(cid:2)
ql

(cid:2)(cid:2)
r l

17.8 Closure

17.8.1 The literature
Although using a slightly different approach, this chapter summarizes previous results
of [Triggs-95] and Faugeras and Mourrain [Faugeras-95a] on the derivation of multi-
linear relationships between corresponding image coordinates. The formulae for re-
lations between mixed point and line correspondences are extensions of the results of
[Hartley-95b, Hartley-97a].

The enumeration of the complete set of multilinear relations given in table 17.1-
(p417) and table 17.2(p421), formulae for the multifocal tensors, and the analysis of
the number of independent equations derived from point correspondences, are adapted
from [Hartley-95a]. A similar analysis of the multifocal tensors has also appeared in
[Heyden-98].

The quadrifocal tensor was probably ﬁrst discovered by [Triggs-95]. The quadri-
linear constraints and their associated tensor have been described in several papers
[Triggs-95, Faugeras-95a, Shashua-95b, Heyden-95b, Heyden-97c].

The double (or Grassmann–Cayley) algebra was introduced into the computer vision
literature in [Carlsson-93], see also [Faugeras-95a, Faugeras-97] for further applica-
tions.

An algorithm for computing the quadrifocal tensor and an algorithm for reconstruct-
ing it based on the reduced tensor was given in ([Heyden-95b, Heyden-97c]). A later
paper [Hartley-98c] reﬁned this algorithm.

17.8.2 Notes and exercises

(i) Determine the properties of the afﬁne quadrifocal tensor, i.e. the quadrifocal
tensor computed from afﬁne camera matrices.
In particular using the deter-
minant deﬁnition of the tensor (17.21–p419), verify the number of non-zero
elements given in table 17.4.
(ii) Show that the 9 linear equations (17.11–p415) derived from a single point cor-
respondence x ↔ x(cid:2) ↔ x(cid:2)(cid:2)
across three views contains 4 linearly independent
equations. Furthermore, let the equations be written as At = 0 where A is a
9 × 27 matrix. Then the 4 non-zero singular values of A are equal. Unlike the
four-view case, the equations resulting from different point matches are linearly
independent, so n point matches produce 4n independent equations.

(iii) If a canonical afﬁne basis is chosen for the image coordinates such that three
corresponding points have coordinates (0, 0), (1, 0), (0, 1) in each view, then the
resulting tensors have a simpler form. These “reduced tensors” have a greater
number of zero elements than the general form of the tensors. For example, in

17.8 Closure

433

the case of the reduced fundamental matrix the diagonal elements are zero, and
the reduced trifocal tensor has only 15 nonzero entries. Also the tensors are
speciﬁed by fewer parameters, e.g. four in the case of the reduced fundamental
matrix, as effectively the basis points specify the other parameters. Further
details are given in [Heyden-95b, Heyden-95a].

(iv) Show that if the four camera centres are coplanar then the quadrifocal tensor

has 28 geometric degrees of freedom.

18

N -View Computational Methods

This chapter describes computational methods for estimating a projective or afﬁne re-
construction from a set of images – in particular where the number of views is large.

We start with the most general case which is that of bundle adjustment for a projec-
tive reconstruction. This is then specialized to afﬁne cameras and the important fac-
torization algorithm introduced. A generalization of this algorithm to non-rigid scenes
is given. A second specialization of bundle adjustment is then described for the case
of scenes containing planes. Finally we discuss methods for obtaining point corre-
spondences throughout an image sequence and a projective reconstruction from these
correspondences.

18.1 Projective reconstruction – bundle adjustment

Consider a situation in which a set of 3D points Xj is viewed by a set of cameras with
matrices Pi. Denote by xi
j the coordinates of the j-th point as seen by the i-th camera.
We wish to solve the following reconstruction problem: given the set of image coor-
dinates xi
j ﬁnd the set of camera matrices, Pi, and the points Xj such that PiXj = xi
j.
Without further restriction on the Pi or Xj, such a reconstruction is a projective recon-
struction, because the points Xj may differ by an arbitrary 3D projective transformation
from the true reconstruction.

Bundle adjustment.
If the image measurements are noisy then the equations
xi
j = PiXj will not be satisﬁed exactly.
In this case we seek the Maximum Like-
lihood (ML) solution assuming that the measurement noise is Gaussian: we wish to

estimate projection matrices ˆPi and 3D points (cid:23)Xj which project exactly to image points
j = ˆPi(cid:23)Xj, and also minimize the image distance between the reprojected point

j for every view in which the 3D point appears,

j as ˆxi
ˆxi
and detected (measured) image points xi
i.e.

(cid:7)

ij

,(cid:23)Xj

min
ˆP

i

d(ˆPi(cid:23)Xj, xi

j)2

(18.1)

where d(x, y) is the geometric image distance between the homogeneous points x and
y. This estimation involving minimizing the reprojection error is known as bundle
adjustment – it involves adjusting the bundle of rays between each camera centre and

434

18.1 Projective reconstruction – bundle adjustment

435

the set of 3D points (and equivalently between each 3D point and the set of camera
centres).

Bundle adjustment should generally be used as a ﬁnal step of any reconstruction
algorithm. This method has the advantages of being tolerant of missing data while
providing a true ML estimate. At the same time it allows assignment of individual
covariances (or more general PDFs) to each measurement and may also be extended to
include estimates of priors and constraints on camera parameters or point positions. In
short, it would seem to be an ideal algorithm, except for the fact that: (i) it requires a
good initialization to be provided, and (ii) it can become an extremely large minimiza-
tion problem because of the number of parameters involved. We will discuss brieﬂy
these two points.

Iterative minimization. Since each camera has 11 degrees of freedom and each 3-
space point 3 degrees of freedom, a reconstruction involving n points over m views
requires minimization over 3n + 11m parameters. In fact, since entities are often over-
parametrized (e.g. using 12 parameters for the homogeneous P matrix) this may be a
lower bound. If the Levenberg–Marquardt algorithm is used to minimize (18.1) then
matrices of dimension (3n + 11m) × (3n + 11m) must be factored (or sometimes in-
verted). As m and n increase this becomes extremely costly, and eventually impossible.
There are several solutions to this problem:

(i) Reduce n and/or m. Do not include all the views or all the points, and ﬁll these
in later by resectioning or triangulation respectively; or, partition the data into
several sets, bundle adjust each set separately and then merge. Such strategies
are discussed further in section 18.6.

(ii) Interleave. Alternate minimizing reprojection error by varying the cameras
with minimizing reprojection error by varying the points. Since each point is
estimated independently given ﬁxed cameras, and similarly each camera is esti-
mated independently from ﬁxed points, the largest matrix that must be inverted
is the 11 × 11 matrix used to estimate one camera.
Interleaving minimizes
the same cost function as bundle adjustment, so the same solution should be
obtained (provided there is a unique minimum), but it may take longer to con-
verge. Interleaving is compared with bundle adjustment in [Triggs-00a].

(iii) Sparse methods. These are described in appendix 6(p597).

Initial solution. Several methods for initialization are described in the following sec-
tions. If the problem is restricted to afﬁne cameras then factorization (section 18.2)
gives a closed form optimal solution provided points are imaged in every view. Even
with projective cameras there is an (iterative) factorization method (section 18.4) avail-
able provided points are imaged in every view. If there is more information available
on the data, for example that it is partly coplanar, then again a closed form solution
is possible (section 18.5). Finally, hierarchical methods can be used as described in
section 18.6 for the case where points are not visible in every view.

436

18 N-View Computational Methods

18.2 Afﬁne reconstruction – the factorization algorithm

In this section we describe reconstruction from a set of image point correspondences for
images acquired by afﬁne cameras. As described in section 17.5.1 the reconstruction
in this case is afﬁne.

The factorization algorithm of Tomasi and Kanade [Tomasi-92] to be presented be-

low and summarized in algorithm 18.1 has the following property:
• Under an assumption of isotropic mean-zero Gaussian noise independent and equal
for each measured point, factorization achieves a Maximum Likelihood afﬁne recon-
struction.

This fact was ﬁrst pointed out by Reid and Murray [Reid-96]. However, the method
requires a measurement of each point in all views. This is a limitation in practice, since
matched points may be absent in some views.

An afﬁne camera may be characterized by having its last row equal to (0, 0, 0, 1).
In this section, however, we will denote it somewhat differently, separating out the
translation and the pure linear transformation part of the camera map. Thus we write

(cid:20)

(cid:21)

x
y

= M

 + t

 X

Y
Z

where M is a 2 × 3 matrix and t a 2-vector. From here on for ease of readability
x represents an inhomogeneous image point x = (x, y)T, and X an inhomogeneous
world point X = (X, Y, Z)T.
Our goal is to ﬁnd a reconstruction to minimize geometric error in image coordinate
measurements. That is, we wish to estimate cameras {Mi, ti} and 3D points {Xj} such
that the distance between the estimated image points ˆxi
j = MiXj + ti and measured
image points xi

j is minimized

(cid:7)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− ˆxi

j

min
Mi,ti,Xj

ij

= min
Mi,ti,Xj

ij

(cid:7)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− (MiXj + ti)

.

(18.2)

As is common in such minimization problems the translation vector ti can be elimi-
nated in advance by choosing the centroid of the points as the origin of the coordinate
system. This is a consequence of the geometric fact that an afﬁne camera maps the cen-
troid of a set of 3D points to the centroid of their projections. Thus, if the coordinate
origin is chosen as the centroid of the 3D points and of each set of image points then it
follows that ti = 0. This step requires that the same n points be imaged in all views,
i.e. that there are no views in which the image coordinates of any point are unknown.
An analytical derivation of this result goes like this. The minimization with respect to
ti requires that

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xk

j

(cid:7)

kj

∂
∂ti

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− (MkXj + tk)

= 0

which after a brief calculation reduces to ti = (cid:8)xi(cid:9) −M i(cid:8)X(cid:9), where the centroids are

18.2 Afﬁne reconstruction – the factorization algorithm

437

Objective
Given n ≥ 4 image point correspondences over m views xi
j, j = 1, . . . , n; i = 1, . . . , m,
determine afﬁne camera matrices {Mi, ti} and 3D points {Xj} such that the reprojection error

(cid:7)

(cid:8)(cid:8)(cid:8)(cid:8)xi

j − (MiXj + ti)

(cid:8)(cid:8)(cid:8)(cid:8)2

is minimized over {Mi, ti, Xj}, with Mi a 2 × 3 matrix, Xj a 3-vector, and xi
ti are 2-vectors.

j = (xi

j, yi

j)T and

ij

Algorithm

(i) Computation of translations. Each translation ti is computed as the centroid of points

in image i, namely

(ii) Centre the data. Centre the points in each image by expressing their coordinates with

respect to the centroid:

(cid:7)

j

ti = (cid:8)xi(cid:9) =

1
n

xi
j.

j ← xi
xi

j − (cid:8)xi(cid:9).

Henceforth work with these centred coordinates.

(iii) Construct the 2m × n measurement matrix W from the centred data, as deﬁned
(iv) Then the matrices Mi are obtained from the ﬁrst three columns of U multiplied by the

in (18.5), and compute its SVD W = UDVT.

singular values:

 = [ σ1u1 σ2u2 σ3u3 ] .

 M1

M2
...
Mm

The vectors ti are as computed in step (i) and the 3D structure is read from the ﬁrst three
columns of V

[ X1 X2

. . . Xn ] = [ v1 v2 v3 ]T

.

(cid:27)

(cid:27)

Algorithm 18.1. The factorization algorithm to determine the MLE for an afﬁne reconstruction from n
image correspondences over m views (under Gaussian image noise).

(cid:8)xi(cid:9) = 1
chosen to coincide with the centroid (cid:8)X(cid:9), in which case (cid:8)X(cid:9) = 0 and

j and (cid:8)X(cid:9) = 1

Xj. The origin of the 3D frame is arbitrary, so may be

xi

n

n

j

j

ti = (cid:8)xi(cid:9).

(18.3)

It follows that if we measure the image coordinates with respect to a coordinate
origin based at the centroid of the projected points, then ti = 0. Thus, we replace each
− (cid:8)xi(cid:9). Henceforth we will assume that this has been done, and work with
xi
j by xi
the centred coordinates. With respect to these new coordinates ti = 0, and so (18.2)
reduces to

j

(cid:7)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

min
Mi,Xj

ij

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− ˆxi

j

(cid:7)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− MiXj

= min
Mi,Xj

ij

.

(18.4)

438

18 N-View Computational Methods

The minimization problem now has a very simple form when written as a matrix.
The measurement matrix W is the 2m × n matrix composed of the centred coordinates
of the measured image points

(18.5)

Since each xi

j = MiXj, the complete set of equations may be written as

x1
2
x2
2
...
xm
2

. . . x1
n
. . . x2
n
...
...
. . . xm
n

 .

#

W =

X1 X2

. . . Xn

.



W =



M1
M2
...
Mm

x1
1
x2
1
...
xm
1

"


 =

(cid:16)

2

In the presence of noise this equation will not be satisﬁed exactly, so instead we seek a
matrix ˆW as close as possible to W in Frobenius norm, such that ˆW may be decomposed
as


(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)W − ˆW
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

ˆx1
1
ˆx2
1
...
ˆxm
1

ˆW =

ˆx1
. . .
n
ˆx2
. . .
n
...
...
. . . ˆxm
n
(cid:15)
In this case it may be veriﬁed that
Wij − ˆWij

ˆx1
2
ˆx2
2
...
ˆxm
2
(cid:7)

=

F

ij

"




(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

M1
M2
...
Mm
(cid:7)

=

ij

#

X1 X2

. . . Xn

.

(18.6)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

=

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

(cid:7)

ij

− ˆxi

j

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− MiXj

Comparing this with (18.4) we ﬁnd that minimizing the required geometric error is
equivalent to ﬁnding such a ˆW as close as possible to W in Frobenius norm.
Note that a matrix ˆW satisfying (18.6) is the product of a 2m × 3 motion matrix ˆM,
and a 3 × n structure matrix ˆX; consequently ˆW = ˆMˆX has rank 3. In other words we
seek a rank 3 matrix which is closest to W in Frobenius norm. Such a matrix may
be determined by the SVD of W truncated to rank 3. In more detail, if the SVD of
W = UDVT then ˆW = U2m×3D3×3VT
3×n is the rank 3 matrix which is closest to W in the
Frobenius norm, where U2m×3 consists of the ﬁrst 3 columns of U, VT
3×n consists of the
ﬁrst 3 rows of VT, and D3×3 is the diagonal matrix containing the ﬁrst 3 singular values,
D3×3 = diag(σ1, σ2, σ3).

Note that the choice of ˆM and ˆX is not unique. For example ˆM may be chosen as
3×n since in either case

3×n, or as ˆM = U2m×3, ˆX = D3×3VT

ˆM = U2m×3D3×3 and ˆX = VT
ˆW = ˆMˆX = U2m×3D3×3VT

3×n.

Afﬁne ambiguity. In fact for any such choice there is an additional ambiguity since an
arbitrary 3 × 3 rank 3 matrix A may be inserted in the decomposition as ˆW = ˆMAA−1ˆX =
(ˆMA)(A−1ˆX). This means that the camera matrices Mi, which are obtained from ˆM, and

18.2 Afﬁne reconstruction – the factorization algorithm

439
the 3D points Xj, which are obtained from ˆX, are determined up to multiplication by a
common matrix A. In other words the MLE reconstruction is afﬁne.

This afﬁne reconstruction may be upgraded to a metric reconstruction by supplying
metric information on the scene as described in section 10.4.2(p272), or by using auto-
calibration methods as described in chapter 19, or a combination of the two. Note that
in the case of afﬁne cameras only three internal parameters need be speciﬁed (compared
to ﬁve for projective cameras) and the auto-calibration task is correspondingly simpler.

18.2.1 Afﬁne multiple view tensors
The factorization algorithm provides an optimal method for computing the afﬁne mul-
tiview tensors from image point correspondences. These tensors are the afﬁne funda-
mental matrix, afﬁne trifocal tensor, and afﬁne quadrifocal tensor. In each case the al-
gorithm determines the camera matrices up to an overall afﬁne ambiguity. The tensors
may then be computed directly from the camera matrices (as for instance in chapter 17).
The afﬁne ambiguity of 3-space is irrelevant when computing the tensors since they are
unaffected by afﬁne transformations of 3-space. In fact it is not necessary to compute
the full SVD of W since only the U part of the decomposition is required. If the number
of points n is large compared with the number of views then very great savings can be
made in the computation of the SVD by not determining V (see table A4.1(p587)).

An alternative to using the SVD is to use the eigenvalue decomposition of WWT, since
WWT = (UDVT)(UDVT)T = UD2UT. In the case of three views (computation of the trifocal
tensor) the matrix WWT has dimension only 9 × 9. Thus this approach can mean signif-
icant savings. However, it is numerically inferior, since forming WWT causes the con-
dition number of the matrix to be squared (see the discussion of SVD in [Golub-89]).
Since we need just the three largest eigenvectors, that may not be such a problem in
this case. However, the savings of this approach will not be so great given an imple-
mentation of the SVD that avoids computing V.

The factorization method may be used to compute any of the multiple-view afﬁne
tensors. For the afﬁne fundamental matrix, however, algorithm 14.1(p351) described
in chapter 14 is more direct. The results of both the methods are identical.

18.2.2 Triangulation and reprojection using subspaces
The factorization algorithm also provides an optimal method for computing the images
of new points or of points not observed in all views. Again the afﬁne ambiguity of
3-space is irrelevant.

A column of W is the set of all corresponding image points for the point Xj and is
referred to as a point’s trajectory. The rank 3 decomposition (18.6) of ˆW as ˆW = ˆMˆX
shows that all trajectories lie in a 3 dimensional subspace. In particular the trajectory
(i.e. all image projections) of a new point X may be obtained as ˆMX. This is simply a
linear weighting of the three columns of ˆM.

Suppose we have observed a new point X in some (not all) views, and wish to pre-
dict its projection in the other views. This is carried out in two steps: ﬁrst triangu-
lation to ﬁnd the pre-image X, and then reprojection as ˆMX to generate its image in

440

18 N-View Computational Methods

all views. Note that the projected points will not coincide exactly with the measured
(noisy) points. In the triangulation step we wish to ﬁnd the point X that minimizes re-
projection error, and this corresponds to ﬁnding the point in the linear subspace spanned
by the columns of ˆM closest to the trajectory. This closest point is found by projecting
the trajectory onto the subspace (in a similar manner to algorithm 4.7(p130)).
In more detail suppose we have computed a set of afﬁne cameras {Mi, ti} then the
triangulation problem may be solved linearly for any number of views. The image
points xi = MiX + ti give a pair of linear equations MiX = xi − ti in the entries of
X. Given sufﬁciently many such equations (arising from known values of xi) one can
ﬁnd the linear least-squares solution for X, using algorithm A5.1(p589), the pseudo-
inverse (see result A5.1(p590)) or algorithm A5.3(p591). Note that if the data xi is
centred using the same transformation applied in step (ii) of algorithm 18.1, then the
translation vectors ti in the afﬁne triangulation method may be taken to be zero.

In practice triangulation and reprojection provides a method of ‘ﬁlling in’ points that

are missed during tracking or multiple view matching.

18.2.3 Afﬁne Reconstruction by Alternation
Suppose a set of image coordinates xi
j are given as in algorithm 18.1, and we wish
to perform afﬁne reconstruction. We have seen that afﬁne triangulation may be carried
out linearly. Thus, if the afﬁne camera matrices represented by {Mi, ti} are known, then
the optimal point positions Xj may be computed by a linear least-squares method such
as the normal equations method of algorithm A5.3(p591).
Conversely, if the points Xj are known, then the equations xi
j = MiXj + ti are linear
in Mi and ti. So it is once again possible simply to solve for {Mi, ti} by linear least-
squares.
This suggests a method of afﬁne reconstruction in which linear least-squares methods
are used to solve alternately for the points Xj and the cameras {Mi, ti}. This method
of alternation is not to be recommended as a general method for reconstruction, or for
solving optimization problems in general. In the case of afﬁne reconstruction, however,
it can be proven to converge rapidly to the optimal solution, starting from a random
starting point. This method of afﬁne reconstruction has the advantage of working with
missing data, or with covariance-weighted data which algorithm 18.1 will not, though
in the missing data or covariance-weighted case, global optimal convergence is not
guaranteed in all cases.

18.3 Non-rigid factorization

Throughout the book it has been assumed that we are viewing a rigid scene and that
only the relative motion between the camera and scene is to be modelled. In this section
we relax this assumption and consider the problem of recovering a reconstruction for
a deforming object. It will be shown that if the deformation is modelled as a linear
combination over basis shapes then the reconstruction and the basis shapes may be
recovered with a simple modiﬁcation of the factorization algorithm of section 18.2.

An example where this type of situation arises is in a sequence of images of a per-

18.3 Non-rigid factorization

441

Fig. 18.1. Shape basis. A face template is represented by N equally spaced 2D points (here N = 140).
The central face of the seven is the mean shape and the faces to the left or right are generated by adding
or subtracting, respectively, the basis shape that accounts for the maximum variation in the training set.
In this case the basis spans expressions from surprised to disgruntled. Facial expressions are learnt by
tracking the face of an actor with the template whilst he changes expression but does not vary his head
pose. Each frame of the training sequence generates a set of N 2D points, and the coordinates of these
are rewritten as a 2N-vector. A 2N × f matrix is then composed from these vectors, where f is the
number of training frames, and the basis shapes are computed from the singular vectors of this matrix.
Figure courtesy of Richard Bowden.

son’s head which moves and also changes expression. The motion of the head may
be modelled as a rigid rotation, and the change of expression relative to the ﬁxed head
may be modelled as a linear combination over basis sets. For example the mouth out-
line may be represented by a set of points.

Suppose the set of n scene points Xj may be represented as a linear combination of

l basis shapes Bk so that at a particular time i:

Xi
1

Xi
2

. . . Xi
n

B1k B2k

. . . Bnk

j and the basis points Bjk are inhomogeneous points
where here both the scene points Xi
represented by 3-vectors, and Bk is a 3×n matrix. Typically the number of basis shapes,
l, is much smaller than the number of points, n. The coefﬁcients αi
k may be different
at each time i, and the resulting differing combination of basis shapes generates the
deformation. An example is shown in ﬁgure 18.1.

In the forward model of image generation each view i is acquired by an afﬁne camera

and gives the image points

xi
j = Mi

αi
k

Bjk + ti.

(cid:7)

k

It will again be assumed that image point matches are available for all views. Our
goal is to estimate cameras {Mi, ti} and 3D structure {αi
k, Bjk} from the measured
(cid:27)
image points {xi
j =
(cid:7)

}, such that the distance between the estimated image points ˆxi
(cid:7)

k αi
k

Mi

j

Bjk + ti and measured image points is minimized
− (Mi

(cid:7)

− ˆxi

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

Bjk + ti)

.

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

j

j

αi
k

min
Mi,ti,αi

k,Bjk

ij

= min
Mi,ti,αi

k,Bjk

ij

k

As in afﬁne factorization the translation may be eliminated by centring the measured
image points, and it will be assumed from here on that this has been done. Then the

"

#

l(cid:7)

"

αi
k

=

k=1

#

(cid:7)

αi
k

Bk

=

k

18 N-View Computational Methods

442

problem reduces to

min

Mi,αi

k,Bjk

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi

j

(cid:7)

ij

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2

− ˆxi

j

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)xi
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

j

(cid:7)

ij

(cid:7)

k

− Mi

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)2
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

.

αi
k

Bjk

(18.7)

= min

Mi,αi

k,Bjk

(cid:27)

k αi
k

 =











The complete set of equations ˆxi

j = Mi

Bjk may be written

ˆW =

Bl)
Bl)

B2 + . . . α1
B1 + α1
l
2
B1 + α2
B2 + . . . α2
l
2
...
B1 + αm
2

B2 + . . . αm
l

M1 (α1
1
M2 (α2
1

B1
B2
...
Bl
(18.8)
This rearrangement shows that the 2m × n matrix ˆW may be decomposed as a product
of a 2m × 3l motion matrix ˆM and 3l × n structure matrix ˆB, and consequently ˆW has
maximum rank 3l.

M1
M1
M1
. . . α1
α1
α1
l
2
1
M2
M2
M2
. . . α2
α2
α2
l
2
1
...
...
...
...
Mm
Mm . . . αm
Mm αm
αm
l
2
1

Mm (αm
1

Bl)

As in rigid factorization a rank 3l decomposition may be obtained from the measure-
ment matrix W by truncating the SVD of W to rank 3l. Also, as in rigid factorization, the
decomposition ˆW = ˆMˆB is not uniquely deﬁned since an arbitrary 3l × 3l rank 3l matrix
A may be inserted in the decomposition as ˆW = ˆMAA−1ˆB = (ˆMA)(A−1ˆB). In the rigid
case this resulted in a straightforward afﬁne ambiguity in the reconstruction. However,
in the non-rigid case there is the additional requirement that the motion matrix has the
replicated block structure of (18.8), and we return to this below. This block structure is
not required for determining a point’s image motion, as will now be discussed.

18.3.1 Subspaces and tensors
In the case of rigid factorization (18.6), as discussed in section 18.2.2, the trajecto-
ries lie in a 3 dimensional subspace, and any trajectory may be generated as a linear
combination of the columns of ˆM (the 2m × 3 motion matrix). Similarly in the case of
non-rigid factorization, (18.8), the trajectories lie in a 3l dimensional subspace, and any
trajectory may be generated as a linear combination of the columns of ˆM (the 2m × 3l
motion matrix).

Suppose we observe a new point in a subset of the views, how many images are
required before its position can be predicted in all the other views? This is simply a
question of triangulation: in rigid factorization a 3-space point has 3 degrees of free-
dom, and must be observed in two views to obtain the necessary 3 measurements. In
non-rigid factorization we need to specify 3l degrees of freedom (the number of rows
in the ˆB matrix), and this requires 3l/2 images. For example, if l = 2 the subspace is six
dimensional (the columns of the ˆB matrix are 6-vectors), and given the image position
in three views, the image position in all views is then determined by an analogue of
afﬁne triangulation (section 18.2.2) even though the object is deforming.

Independently moving objects. Low-rank factorization methods also arise when
there are independently moving objects in the scene. For instance suppose the scene
is divided into two objects, each moving independently of the other, and viewed by

18.3 Non-rigid factorization

443

a

b

Fig. 18.2. Non-rigid motion sequence. Top row: alternate frames from a sequence in which a giraffe
gracefully walks and ﬂexes its neck, whilst the camera pans to match its speed. Bottom row: point
tracks showing the motion over (a) the 10 previous, and (b) the 10 forthcoming frames. These tracks are
computed using non-rigid factorization and lie in a six dimensional subspace. Note the very different
trajectories of the (rigid) background from the (deforming) foreground. Yet these are all spanned by the
six basis vectors of the motion matrix. The rank can be accounted for as follows: the sequence motion
is effectively that of two planes of points moving independently relative to the camera. The background
is a rigid object represented by a plane and contributes 2 to the rank. The giraffe in the foreground is
represented as a non-rigid object by a set of l = 2 planar basis shapes and contributes 4 to the rank.
Figures courtesy of Andrew Fitzgibbon and Aeron Morgan.

a moving afﬁne camera. In this case, the columns of the measurement matrix corre-
sponding to points on one object will have rank 3, and those corresponding to the other
object will also have rank 3. The total rank of the measurement matrix will be 6. In
degenerate conﬁgurations in which one object’s points all lie in a plane, its contribution
to the rank will be only 2. This multibody factorization problem has been studied in
some depth in [Costeira-98].

An example of point tracks residing in a low dimensional subspace is shown in

ﬁgure 18.2.

The existence of the analogue of the fundamental matrix and trifocal tensor depends
on the dimension of the subspace. For example suppose the subspace has odd di-
mension (e.g. l = 3 so it is 9 dimensional) then given point measurements in (cid:25)3l/2(cid:26)
views (e.g. 4 views) the corresponding point in any other view is constrained to a line,
the analogue of an epipolar line, since there is one fewer measurement than degrees of
freedom of the subspace. However, if the dimension is even (e.g. l = 2 so it is 6 dimen-
sional) then given point measurements in l/2 views (e.g. 3 views) the corresponding
point in any other view is completely determined. Multi-view tensors can be built for

444

18 N-View Computational Methods

the non-rigid l > 1 subspaces using methods similar to those developed in chapter 17
for l = 1 3-space points.

18.3.2 Recovering the camera motion
In rigid-factorization the camera matrices are obtained relatively easily from the motion
matrix ˆM – all that is required is to remove a global afﬁne ambiguity speciﬁed by a 3× 3
matrix A as described on page 438.

In the non-rigid case, the analogous problem is not so straightforward. It is a simple

matter to obtain the motion matrix:

(i) Construct the 2m × n measurement matrix W from the centred data, as deﬁned

in (18.5), and compute its SVD W = UDVT.

(ii) Then the motion matrix ˆM is obtained from the ﬁrst 3l columns of U multiplied

by the singular values as ˆM =

σ1u1 σ2u2

. . . σ3lu3l

,

"

#

but the matrix obtained by this route will not in general have the block structure
of (18.8). As in the case of rigid-factorization the motion matix is determined up to
post-multiplication by a matrix A, which here is 3l × 3l. The task is then to deter-
mine A such that ˆMA has the required block structure of (18.8) and also to remove the
usual afﬁne ambiguity such that each block conforms to any available constraints on
the camera calibration (for example identical internal parameters over all views).

Various methods for determining A have been investigated (see [Brand-01,
Torresani-01]), but these do not impose the full block structure, and currently there
is not a satisfactory solution to this problem. Once an initial solution has been obtained
by some means, then the correct form can be imposed by bundle adjustment of (18.7).

18.4 Projective factorization

The afﬁne factorization method does not apply directly to projective reconstruction. It
was observed in [Sturm-96], however, that if one knows the “projective depth” of each
of the points then the structure and camera parameters may be estimated by a simple
factorization algorithm similar in style to the afﬁne factorization algorithm.

Consider a set of image points xi

j = PiXj. This equation representing the projective
mapping is to be interpreted as true only up to a constant factor. Writing these constant
xi
j = PiXj. In this equation, and henceforth in the de-
factors explicitly, we have λi
j
scription of the projective factorization algorithm, the notation xi
j means the 3-vector
j, 1)T representing an image point. Thus the third coordinate is equal to unity,
(xi
j, yi
and xi
j are the actual measured image coordinates. Provided that each point is
j and yi
visible in every view, so that xi
j is known for all i, j, the complete set of equations may
be written as a single matrix equation as follows:



x1
λ1
1
1
x2
λ2
1
1
...
xm
λm
1
1

x1
λ1
2
2
x2
λ2
2
2
...
xm
λm
2
2

x1
. . . λ1
n
n
x2
. . . λm
n
n
...
...
xm
. . . λm
n
n

 =



P1
P2
...
Pm

 [X1, X2, . . . , Xn] .

(18.9)

18.4 Projective factorization

445

Objective
Given a set of n image points seen in m views:

xi
j ; i = 1, . . . , m, j = 1, . . . , n

compute a projective reconstruction.

Algorithm

(i) Normalize the image data using isotropic scaling as in section 4.4.4(p107).
(ii) Start with an initial estimate of the projective depths λi

j. This may be obtained by

techniques such as an initial projective reconstruction, or else by setting all λi

j = 1.

(iii) Normalize the depths λi
j by multiplying rows and columns by constant factors. One
method is to do a pass setting the norms of all rows to 1, then a similar pass on columns.
(iv) Form the 3m × n measurement matrix on the left of (18.9), ﬁnd its nearest rank-4 ap-
proximation using the SVD and decompose to ﬁnd the camera matrices and 3D points.
(v) Optional iteration. Reproject the points into each image to obtain new estimates of

the depths and repeat from step (ii).

Algorithm 18.2. Projective reconstruction through factorization.

This equation is true only if the correct weighting factors λi

j are applied to each of
the measured points xi
j. For the present, let us assume that these depths are known. As
with the afﬁne factorization algorithm, we would like the matrix on the left – denote it
by W – to have rank 4, since it is the product of two matrices with 4 columns and rows
respectively. The actual measurement matrix can be corrected to have rank 4 by using
the SVD. Thus, if W = UDVT, all but the ﬁrst four diagonal entries of D are set to zero
resulting in ˆD. The corrected measurement matrix is ˆW = UˆDVT. The camera matrices
are retrieved from [PT
m]T = UˆD and the points from [X1, X2, . . . , Xn] = VT.
Note that this factorization is not unique, and in fact we may interpose an arbitrary
4 × 4 projective transformation H and its inverse between the two matrices on the right
of (18.9), reﬂecting the fact that reconstruction has a projective ambiguity.

2 , . . . ,P T

1 , PT

The steps of the projective factorization method are summarized in algorithm 18.2.

18.4.1 Choosing the depths
The weighting factors λi
j are called the projective depths of the points. The justiﬁcation
of this terminology is the relation of these λi
j to the actual depths if camera matrices are
known in a Euclidean frame. Refer to section 6.2.3(p162) and in particular ﬁgure 6.6-
(p162). The main difﬁculty with this projective factorization algorithm is that we need
to know these projective depths up front, but we do not have this knowledge. There are
various techniques to estimate the depths.

(i) Start with an initial projective reconstruction obtained by other means, such as
j by reprojecting the 3D

those discussed in section 18.6 below. Then compute λi
points.

(ii) Start with initial depths all equal to 1, compute the reconstruction and reproject
to obtain a new estimate of the depths. This step may be repeated to obtain

446

18 N-View Computational Methods

improved estimates. However, there is no guarantee that the procedure will
converge to a global minimum.

The original paper [Sturm-96] gives a method of computing the depths by stringing
together pairwise estimates of the depth obtained from the fundamental matrix, or the
trifocal tensor. This method is quite similar to obtaining an initial projective recon-
struction by stringing together triples of images (see section 18.6), whilst ensuring that
the scale factors are consistent for a common projective reconstruction.

(cid:7)

ij

(cid:10)λi

j

xi
j

18.4.2 What is being minimized?
In the case of noise, or incorrect values for λi
j, the equations (18.9) are not satisﬁed ex-
actly. We determine a corrected measurement matrix ˆW that is closest to W in Frobenius
norm, subject to having rank 4. Denoting the entries of this matrix as ˆλi
j, then the
computed solution minimizes the expression
(cid:10)W − ˆW(cid:10)2 =
jxi
j

− ˆλi
j)2
(18.10)
Because of the last term, at a minimum ˆλi
j. Assuming they are
(cid:10) is the geomet-
− ˆxi
− ˆxi
equal, (18.10) reduces to
ric distance between the measured and estimated points, what is being minimized is a
weighted sum-of-squares geometric distance, where each point is being weighted by
j are close to equal, then the factorization method
j. If all the geometric depths λi
λi
minimizes an approximation to geometric distance scaled by the common value of λi
j.

j must be close to λi
(cid:10)2. Noting that (cid:10)xi

− ˆλi
j ˆxi
(cid:27)

j)2 + (λi

jyi
j

− ˆλi
j ˆxi

− ˆλi
j ˆyi

j)2(cid:10)xi

j

ij(λi

(cid:10)2 =

j

j)2 + (λi
j

(cid:7)

ij

(λi

j ˆxi

j

j

j

18.4.3 Normalizing the depths
Projective depths as deﬁned here are not unique. Indeed suppose that λi
j
we replace Pi by αiPi and Xj by βjXj, then we ﬁnd that

xi
j = PiXj. If

(αiβjλi

j)xi
j = (αiPi)(βjXj) .
In other words, the projective depths λi
j may be replaced by multiplying the i-th row
of (18.9) by a factor αi and the j-th column by a factor βj. In the light of the previous
paragraph, the closer all λi
j are to unity, the more exactly the error expression represents
geometric distance. Therefore, it is advantageous to renormalize the values of the λi
j
so that they are as close to unity as possible, by multiplying rows and columns of the
measurement matrix by constant values αi and βj. A simple heuristic manner of doing
this is to multiply each row by a factor αi so that it has unit norm, followed by a similar
pass normalizing the columns. The row and column passes may be iterated.

18.4.4 Normalizing the image coordinates
As with most numerical algorithms involving homogeneous representations of image
coordinates described in this book, it is important to normalize the image coordinates.
A reasonable scheme is the isotropic normalization method described in section 4.4.4-
(p107). One can see the necessity of normalization quite clearly in this case. Consider
two image points x = (200, 300, 1)T and ˆx = (250, 375, 1)T. Obviously these points

18.5 Projective reconstruction using planes

447

are very far apart in a geometric sense. However, the error expression (18.10) mea-
sures not geometric error, but the distance between homogeneous vectors (cid:10)λx − ˆλˆx(cid:10).
Choosing λ = 1.25 and ˆλ = 1.0, the error is (cid:10)(250, 375, 1.25)T − (250, 375, 1)T(cid:10)
which is proportionally quite small. On the other hand, the distance between points
x = (200, 300, 1)T and ˆx = (199, 301, 1)T, which are much closer geometrically can
not be made so small by choice of λ and ˆλ (except for small values). The reader may
observe that if the points are scaled down by a factor of 200, then this anomalous sit-
uation no longer occurs. In short, with normalized coordinates, the error is a closer
approximation to geometric error.

j(xi

j, yi

32 + p2

18.4.5 When is the assumption λi
j = 1 reasonable?
if camera matrices are normalized such that
According to result 6.1(p162),
33 = 1, and 3D points are normalized to have last coordinate T = 1, then
p2
31 + p2
j, 1) = PiXj are the true depths of the points from the camera
λij deﬁned by λi
in a Euclidean frame. If all points are equidistant from the cameras throughout a se-
quence then we may reasonably assume that each λi
j = 1, for (18.9) will have at least
the solution where Pi and Xj are the true cameras and points, normalized in the manner
just stated. More generally, suppose that points are located at different depths, but each
point Xj remains at approximately the same depth dj from the cameras through the
whole sequence. In this case a solution will exist with all λi
j = 1 in which the com-
−1
j (Xj, Yj, Zj, 1)T. Similarly, by allowing multiplication of the camera
puted Xj = d
matrices by a factor, we ﬁnd
• If the ratios of true depths of the different 3D points Xj remain approximately con-
stant during a sequence, then the assumption λi
j = 1 is a good ﬁrst approximation
to projective depth.

This is for instance the case of an aerial image camera pointing straight down from
constant altitude.

18.5 Projective reconstruction using planes

It was seen in section 17.5.2 that if four points visible in each view are known to
be coplanar then the computation of the multifocal tensors relating the image points
becomes signiﬁcantly more simple. A major advantage is that a tensor satisfying all
its constraints may be computed using a linear algorithm. We now continue with that
particular line of investigation, and show that the use of linear techniques extends to
estimation of motion and structure for any number of views.

The condition that four of the image correspondences are derived from coplanar
points is equivalent to knowing the homographies between the images induced by a
plane in space, since a homography may be computed from the four points. It is only
the homographies that are important in the following approach. These homographies
may be computed from four or more point correspondences, or line correspondences,
or estimated directly from the images by direct correlation methods.

448

18 N-View Computational Methods

What do the plane-plane homographies tell us? The key to projective reconstruction
using planes is the observation that knowledge of homographies between the images
means we know the ﬁrst 3 × 3 part of the camera matrices:

 M

P =



(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8) t1

t2
t3

Hence, it remains only to compute their last columns, namely the vectors t.

Since we are interested at this point only in obtaining a projective reconstruction
of the scene, we may suppose that the plane inducing the homographies is the plane
at inﬁnity, with points Xj = (Xj, Yj, Zj, 0)T. Camera matrices may be written in the
form Pi = [Mi|ti], where M is a 3 × 3 matrix and ti is a column vector. A reasonable
assumption is that the camera centres do not lie on the plane inducing the homographies
(for otherwise the homographies will be degenerate). This means that the matrix M
is non-singular. For simplicity, the ﬁrst camera may be assumed to have the form
P1 = [I | 0], where I is the identity matrix.
Now, if xi

j is the point in image i corresponding to the 3D point Xj = (Xj, Yj, Zj, 0)T

lying on the homography-inducing plane, then

whereas

j = P1(Xj, Yj, Zj, 0)T = (Xj, Yj, Zj)T
x1

j = Mi(Xj, Yj, Zj)T = Mix1
xi
j .

Thus Mi represents the homography from the ﬁrst image to the i-th image induced by
the plane. Conversely, if Mi is the known plane-induced homography that maps a point
in the ﬁrst image to its matching point in the i-th image, then the set of camera matrices
can be assumed to have the form Pi = [Mi|ti], where the Mi are known and their scale is
ﬁxed, but the ﬁnal columns ti are not.

Known camera orientation. We have just shown that knowledge of homographies
implies the knowledge of the left-hand 3 × 3 submatrix of each camera matrix. The
same will hold if we know the orientation (and calibration) of all the cameras. For in-
stance, a reasonable approach to reconstruction, knowing the calibration of each cam-
era, is to estimate the orientation of each camera separately from the translation (for
example from two or more scene vanishing points). Once the orientation (Ri) and cal-
ibration (Ki) of each camera is known, the left-hand block of each camera matrix is
KiRi.

18.5.1 Direct solution for structure and translation
We describe two separate methods for computation of the projective structure given
plane-induced homographies between images. The ﬁrst method solves for the 3D
points and the camera motion simultaneously by solving a single linear system. Sup-
pose point X = (X, Y, Z, 1)T is not on the plane at inﬁnity, that is, the plane inducing
the homographies.

18.5 Projective reconstruction using planes

449

where the (unknown) scale factor λ has been explicitly written. More precisely, we
may write

The equation for point projection is

λx = PX = [M|t]X = [M|t]

 x

 =

 mT

λ

t1
t2
t3
i is the i-th row of the matrix M.

1
mT
2
mT
3

y
1

(cid:21)

(cid:20) (cid:22)X

1

=

where mT

1

(cid:21)

(cid:20) (cid:22)X
 mT

1
mT
2
mT
3



(cid:22)X + t1
(cid:22)X + t2
(cid:22)X + t3

The unknown scale factor λ may be eliminated by taking the vector product of the

two sides of this equation, resulting in

1
mT
2
mT
3
This provides two independent equations

y
1

 = 0.
 mT
 ×
 x
(cid:22)X + t1
(cid:22)X + t2
(cid:22)X + t3
(cid:22)X + t1) = 0
(cid:22)X + t3) − (mT
(cid:22)X + t3) − (mT
(cid:22)X + t2) = 0
 = 0.
(cid:18)

−1
x
0 −1 y

− mT
− mT

(cid:22)X

0

1

1

2

x(mT
3
y(mT
3

(cid:17)

xmT
3
ymT
3

t1
t2
t3

which are linear in the unknowns (cid:22)X = (X, Y, Z)T and t = (t1, t2, t3)T. The equations

2

may be written as

Thus, each measured point xi

j = PiXj generates a pair of equations, and with m
views involving n points a 2nm set of equations in 3n + 3m unknowns is generated in
this way. These equations may be solved by linear or linear least-squares techniques to
obtain the structure and motion.

A few remarks about this method are offered.

(i) In contrast to factorization methods (section 18.2) we do not need all points to
be visible in all views. Only equations corresponding to the measured points
are used.

(ii) Since it is assumed that points have ﬁnal coordinate equal to one, it is neces-
sary to exclude points that lie on the plane at inﬁnity (the plane inducing the
homography) which have ﬁnal coordinate equal to zero. A test to detect points
lying on or close to the plane is necessary.

(iii) Both points and cameras are computed at once. For a large number of points
and cameras this may be a very large estimation problem. However, if the point
tracks have a banded form, then sparse solution techniques may be used to solve
the equation set efﬁciently, as in section A6.7(p613).

450

18 N-View Computational Methods

This method and its implementation are discussed in depth in [Rother-01,
Rother-03]. The details given here are different from those given in [Rother-01], where
the structure and motion computation is carried out in a speciﬁc projective frame re-
lated to the matched points on the plane, involving a coordinate change in the images.

18.5.2 Direct motion estimation
The second method for planar reconstruction knowing homographies solves for the
camera matrices ﬁrst and subsequently computes the point positions.
We start from the set of camera matrices which again can be assumed to have the
form Pi = [Hi|ti], where the Hi are known and their scale is ﬁxed, but the ﬁnal columns
ti are not. We may assume that P1 = [I | 0], so that t1 = 0. The set of all remaining ti
have 3m − 4 degrees of freedom, since the ti are deﬁned only up to a common scale.
Now assume that several point or line correspondences across two or more views are
known (three views are required for lines). These correspondences must derive from
3D points or lines that do not lie in the reference plane (used to compute the Hi). Each
point correspondence across two views leads to a linear equation in the entries of the
fundamental matrix. Similarly, correspondences of points or lines across three or four
views lead to linear equations in the entries of the trifocal or quadrifocal tensor.

The key point (as explained in section 17.5.2) is that we may express the entries of
the fundamental matrix (or trifocal or quadrifocal tensor) linearly in the entries of the
vectors ti. Therefore each linear relation induced by a point or line correspondence
may be related back to a linear relationship in terms of the entries of the ti. Thus,
for example, a correspondence across views i, j and k gives rise to a set of linear
equations in the entries of the three vectors ti, tj and tk. A set of correspondences
across many views can be broken down into correspondences across sets of consecutive
views. Thus, for example, a single point correspondence across m >4 views will give
a set of equations of the form

where each row represents a set of equations derived from a quadrifocal tensor rela-
tionship. Each black square represents a block with 3 columns corresponding to one
of the vectors ti. In the diagram above, we choose to wrap the equations around from
the last to the ﬁrst view to add greater rigidity. Otherwise, the values of the ti can drift
from the ﬁrst to the last view. Other schemes for selecting groups of views are possible,
and it is not necessary to restrict to consecutive views.

Linear relations may be generated between any subset of sufﬁciently many images
(2, 3 or 4 depending on which tensor is used to generate the equations). One must trade
off the added stability of the solution against the added computational cost of adding
more equations. A mixture of bifocal, trifocal and quadrifocal constraints may be used

18.5 Projective reconstruction using planes

451

in generating the set of all equations, and it is not necessary that all points be visible in
all views.

Numbers of equations generated. Let the total number of views be m. Consider
a subset of s views (s = 2, 3 or 4) and let n point correspondences be given between
these views. We brieﬂy consider the problem of reconstruction from this subset of s
views in isolation. From these point correspondences we can generate a set of equations
At(cid:2)
between the entries t(cid:2)
of the s views, and thence estimate the values of the 3s entries
of t(cid:2)
. In doing this, we can assume that the ﬁrst view has t = 0, and the vectors t
from the remaining s − 1 views are only determined up to a common scale. Thus,
the A occurring in the equation set At(cid:2)
has a right null-space of dimension at least 4,
corresponding to the 4 degrees of freedom of the solution. In general, then:

Result 18.1. Ignoring the effects of noisy data, the total rank of the set of equations
generated from n ≥ 2 point correspondences in s views is 3s − 4. This is independent
of the number of point (or line) correspondences used to generate them.
To be exact, the argument above showed that the rank was at most 3s−4. For 2-view,
3-view and 4-view correspondences this is equal to 2, 5 or 8 respectively. However, as
long as there are two correspondences the maximum rank is achieved. This is because
two points are sufﬁcient for reconstruction from s = 2, 3 or 4 views as shown by the
counting arguments of section 17.5.2.
Now consider the total set of m views. The total number of retrievable parameters of
all the ti is 3m − 4. Therefore, for a solution to be possible, the number of equations
must exceed 3m − 4, which gives the following result.
Result 18.2. If S subsets of sk views are chosen from among m views, then in order
to solve for all the vectors ti representing ﬁnal columns of the camera matrices, it is
necessary that

S(cid:7)

(3sk − 4) ≥ 3m − 4 .

k=1

One can verify that if 2-view correspondences are to be used,
tions derived from the fundamental matrix constraints,
ﬁcient

to use just pairs of consecutive views

involving equa-
is not suf-
in a conﬁguration such as

then it

for in this case, the total number of equations generated is m(3s− 4) = m(3· 2− 4) =
2m, whereas the total number of equations required is 3m − 4. Thus for m >4 there
are not enough equations. This is related to the fact that the fundamental matrices
between consecutive views are not sufﬁcient to deﬁne the structure of the sequence of
views. It is necessary to add additional constraints from non-consecutive views, such as

452

18 N-View Computational Methods

.

Note though that the discussion of section 15.4(p383) suggests that it is preferable to
use trifocal or quadrifocal constraints over triplets or quadruplets of views. Implemen-
tation details for this method are given in [Kaucic-01].

18.6 Reconstruction from sequences

In this ﬁnal section we bring together several ideas from earlier in the book. The
objective here is to compute a reconstruction from a sequence of frames provided by
a video. There are three stages to this problem: (i) compute corresponding features
throughout the sequence; (ii) compute an initial reconstruction which may be used as a
starting point for (iii) bundle adjustment (as described in section 18.1).

Here the features we will consider are interest points, though others such as lines
could equally well be used. The correspondence problem is exacerbated because an
interest point feature will generally not appear in all of the images, and often will be
missing from consecutive images. Bundle adjustment, however, is not hindered by
missing correspondences.

There are several advantages of a video sequence over an arbitrary set of images: (i)
there is an ordering on the images; (ii) the distance between camera centres (the base-
line) for successive frames is small. The small baseline is important because it enables
possible feature matches between successive images to be obtained and assessed more
easily. Matches are more easily obtained because the image points do not move “far”
between views so a proximity search region can be employed; matches are more eas-
ily assessed (as to whether they arise from the same point in 3-space) because nearby
images are similar in appearance. The disadvantage of a small baseline is that the 3D
structure is estimated poorly. However, this disadvantage is mitigated by tracking over
many views in the sequence so that the effective baseline is large.

An overview of the method is given in algorithm 18.3. There are several strategies
that may be used to obtain the initial reconstruction, though this area is still to some
extent a black art. Three possibilities are:

1. Extending the baseline. Suppose a reasonable number of scene points are visible
throughout the sequence. Correspondences may be carried through from the ﬁrst to
the last frame using the pairwise matches (from F), or the triplet matched points (from
T ). Indeed if the baseline between consecutive frames is small (compared to the struc-
ture depth), then pairwise matches may be obtained using homography computation
(algorithm 4.6(p123)) – this provides a stronger matching constraint (point to point)
than F (point to line).

A trifocal tensor can then be estimated from corresponding points in the ﬁrst, middle
(say), and end frames of the sequence. This tensor determines a projective reconstruc-

18.6 Reconstruction from sequences

453

Objective
Given a sequence of frames in a video, compute correspondences and a reconstruction of the
scene structure and the camera for each frame.

Algorithm

(i) Interest points: Compute interest points in each image.
(ii) 2 view correspondences: Compute interest point correspondences and F between con-
secutive frames using algorithm 11.4(p291) (frames may be omitted if the baseline
motion is too small).
(iii) 3 view correspondences: Compute interest point correspondences and T between all

consecutive image triplets using algorithm 16.4(p401).

(iv) Initial reconstruction: See text.
(v) Bundle adjust the cameras and 3D structure for the complete sequence.
(vi) Auto-calibration: see chapter 19 (optional).

Algorithm 18.3. Overview of reconstruction from a sequence of images.

tion for those points and frames. The cameras for the intermediate frames may then
be estimated by resectioning, and the scene points not visible throughout the sequence
estimated by triangulation.

2. Hierarchical merging of sub-sequences. The idea here is to partition the sequence
into manageable sub-sequences (there can be several hierarchical layers of partition-
ing). A projective reconstruction is then computed for each sub-sequence and these
reconstructions are “zipped” (merged) together.

Consider the problem of merging two triplets which overlap by two views. It is a
simple matter to extend the correspondences over the views: a correspondence which
exists across the triplet 1-2-3 and also across the triplet 2-3-4 may be extended to the
frames 1-2-3-4, since the pair 2-3 overlaps for the triplets. The camera matrices and 3D
structure are then computed for the frames 1-2-3-4, for example by ﬁrst resectioning
and then bundle adjustment. This process is extended by merging neighbouring groups
of frames until camera matrices and correspondences are established throughout the
sequence. In this manner error can be distributed evenly over the sequence.

3. Incremental bundle adjustment. A fresh bundle adjustment is carried out as the
correspondences from each new frame are added. The disadvantage of this method is
the computational expense and also the possibility that error systematically accumu-
lates.

Of course these three methods may be combined together. For example, the se-
quence can be partitioned into a sub-sequence where common points are visible, and
a reconstruction built for the sub-sequence using the extended baseline method. These
sub-sequences may then be combined hierarchically.

In this manner structure and cameras may be computed automatically for sequences
consisting of hundreds of frames. These reconstructions may form the basis for such
tasks as navigation (determining the camera/ego-position) and virtual model genera-

454

18 N-View Computational Methods

a

c

b

d

Fig. 18.3. Corridor sequence. (a) A three dimensional reconstruction of points and lines in the scene,
and (b) cameras (represented by their image planes) computed automatically from the images. A texture
mapped triangulated graphical model is then automatically constructed as described in [Baillard-99].
(c) A rendering of the scene from a novel viewpoint, different from any in the sequence. (d) VRML model
of the scene with the cameras represented by their image planes (texture mapped with the original images
from the sequence).

tion. Often it is necessary ﬁrst to compute a metric reconstruction from the projective
one, using the methods described in chapter 10 and chapter 19. Metric reconstruction
and virtual model generation is illustrated in the following examples.

Example 18.3. Corridor sequence
A camera is mounted on a mobile vehicle for this sequence. The vehicle moves along
the ﬂoor turning to the left. The forward translation in this sequence makes structure
recovery difﬁcult, due to the small baseline for triangulation. In this situation, the ben-
eﬁt of using all frames in the sequence is signiﬁcant. Figure 18.3 shows the recovered
(cid:2)
structure.

18.6 Reconstruction from sequences

455

Fig. 18.4. Wilshire: 3D points and cameras for 350 frames of a helicopter shot. Cameras are shown
for just the start and end frames for clarity, with the camera path plotted between.

Example 18.4. “Wilshire” sequence
This is a helicopter shot of Wilshire Boulevard, Los Angeles. In this case reconstruc-
tion is hampered by the repeated structure in the scene – many of the feature points
(for example those on the skyscraper windows) have very similar intensity neighbour-
hoods, so correlation-based tracking produces many false candidates. However, the
robust geometry-guided matching successfully rejects the incorrect correspondences.
(cid:2)
Figure 18.4 shows the structure.

456

18 N-View Computational Methods

18.7 Closure

It is probably fair to say that no fully satisfactory technique for reconstruction from a
sequence of projective images is known, and many ad-hoc techniques have been used,
with reasonable success. Four views is the limit for the closed-form solutions based
on multiview tensors. For larger numbers of views there is no such neat mathematical
formulation of the problem. One exception to this is the m-view technique based on
duality (see chapter 20), but this techniques is limited to six to eight points, depending
on which dual tensor (fundamental matrix, trifocal tensor or quadrifocal tensor) is used.
Most sequences will contain many more matched points than this.

18.7.1 Literature
The Tomasi–Kanade algorithm was ﬁrst proposed for orthographic projection,
[Tomasi-92], but was later extended to paraperspective [Poelman-94]. It has been ex-
tended to lines and conics, e.g. [Kahl-98a], but the MLE property no longer applies,
and it is unclear what is being minimized in the afﬁne reconstruction. Others have in-
vestigated subspace methods for multiple afﬁne views in the case of planes [Irani-99],
and the case of multiple objects moving independently [Boult-91, Gear-98]. Non-rigid
factorization was formulated by [Brand-01, Torresani-01], though the elements of the
idea are present in [Bascle-98]. Afﬁne reconstruction with uncertainty (covariance-
weighted data) has been discussed by Irani and Anandan [Irani-00, Anandan-02] The
method of afﬁne reconstruction by alternating triangulation and camera estimation is
mentioned in [Huynh-03], under the name “PowerFactorization.”

The extension of factorization to projective cameras is due to Sturm and Triggs
[Sturm-96]. Methods of iteration using this approach have been proposed by
[Heyden-97a, Triggs-96].

A method of computing multiple cameras based on a plane homography was em-

ployed in [Cross-99], initializing the ti vectors using planar auto-calibration.

Methods for obtaining an initial projective reconstruction from a sequence are
described in [Avidan-98, Beardsley-94, Beardsley-96, Fitzgibbon-98a, Laveau-96a,
Nister-00, Sturm-97b]. [Torr-99], and more recently [Pollefeys-02], discuss the im-
portant problem of scene and motion degeneracies that may be encounted in sequence
reconstruction.

18.7.2 Notes and exercises

(i) The afﬁne factorization algorithm can be applied to obtain a reconstruction in
situations where a set of cameras {Pi} have a common third row, even though
the cameras are not afﬁne. The third row is the principal plane of the camera
(see section 6.2(p158)) and the condition of a common third row is equivalent
to coplanar principal planes. For example if a camera translates in a direction
perpendicular to its principal axis, then all the camera centres will lie on a plane,
and the principal planes are coplanar. The afﬁne factorization algorithm can be
applied in this case because the set of cameras can be transformed as PiH4×4 to

18.7 Closure

457

the afﬁne form by a 4× 4 homography H satisfying P3H4×4 = (0, 0, 0, 1), where
P3 is the last row of Pi.
More generally, if the camera centres are restricted to a plane then the images
may be synthetically rotated such that the cameras effectively have coplanar
principal planes. For example in the case of planar motion (section 19.8) or
single axis rotation (section 19.9(p490)) if all the images are rotated such that
the principal axis is parallel to the rotation axis (by applying a homography to
each image which maps the horizon to inﬁnity in the case of a vertical rotation
axis), then the principal planes of all the cameras are parallel. However, if the
cameras are not actually afﬁne, then the algorithm will not give the ML estimate
of the reconstruction.

19

Auto-Calibration

Auto-calibration is the process of determining internal camera parameters directly from
multiple uncalibrated images. Once this is done, it is possible to compute a metric re-
construction from the images. Auto-calibration avoids the onerous task of calibrating
cameras using special calibration objects. This gives great ﬂexibility since, for ex-
ample, a camera can be calibrated directly from an image sequence despite unknown
motion and changes in some of the internal parameters.

The root of the method is that a camera moves rigidly, so the absolute conic is ﬁxed
under the motion. Conversely, then, if a unique ﬁxed conic in 3-space can be deter-
mined in some way from the images, this identiﬁes Ω∞. As we have seen in earlier
chapters, once Ω∞ is identiﬁed, the metric geometry can be computed. An array of
auto-calibration methods are available for this task of identifying Ω∞.

This chapter has four main parts. First we lay out the algebraic structure of the auto-
calibration problem, and show how the auto-calibration equations are generated from
constraints on the internal or external parameters. Second, we describe several direct
methods for auto-calibration which involve computing the absolute conic or its image.
These include estimating the absolute dual quadric over many views, or the Kruppa
equations from view pairs. Third, are stratiﬁed methods for auto-calibration which
involve two steps – ﬁrst solving for the plane at inﬁnity, then using this to solve for the
absolute conic. The fourth part covers a number of special conﬁgurations including a
camera rotating about its centre, a camera undergoing planar motion, and the motion
of a stereo rig.

19.1 Introduction

Auto- (or self-) calibration is the computation of metric properties of the cameras
and/or the scene from a set of uncalibrated images. This differs from conventional
calibration where the camera calibration matrix K is determined from the image of a
known calibration grid (chapter 7) or properties of the scene, such as vanishing points
of orthogonal directions (chapter 8). Instead, in auto-calibration the metric properties
are determined directly from constraints on the internal and/or external parameters.

For example, suppose we have a set of images acquired by a camera with ﬁxed
internal parameters, and that a projective reconstruction is computed from point cor-

458

19.2 Algebraic framework and problem statement

459

respondences across the image set. The reconstruction computes a projective camera
matrix Pi for each view. Our constraint is that for the actual cameras the internal pa-
rameter matrix K is the same (but unknown) for each view. Now, each camera Pi of
the projective reconstruction may be decomposed as Pi = Ki[Ri | ti] but in general the
calibration matrix Ki will differ for each view. Thus the constraint will not be satisﬁed
by the projective reconstruction.

However, we have the freedom to vary our projective reconstruction by transforming
the camera matrices by a homography H. Since the actual cameras have ﬁxed internal
parameters, there will exist a homography (or a family of homographies) such that the
transformed cameras PiH do decompose as PiH = KRi[I | ti], with the same calibration
matrix for each camera, so the reconstruction is consistent with the constraint. Pro-
vided there are sufﬁciently many views and the motion between the views is general
(see later), then this consistency constrains H to the extent that the reconstruction trans-
formed by H is within a similarity transformation of the actual cameras and scene, i.e.
we achieve a metric reconstruction.

Although the particular constraints used to achieve a metric reconstruction may dif-

fer, this example illustrates the general approach:
(i) Obtain a projective reconstruction {Pi, Xj}.
(ii) Determine a rectifying homography H from auto-calibration constraints, and

transform to a metric reconstruction {PiH, H−1Xj}.

Various ﬂavours of auto-calibration will be covered in the following sections. They
differ in the constraints used, and the methods whereby the homography H is deter-
mined. The methods may be divided into two classes: those that directly determine H;
and those that are stratiﬁed, determining ﬁrst the projective and then the afﬁne compo-
nents of H. The advantage of the latter approach is that once an afﬁne reconstruction is
achieved, i.e. π∞ is known, the solution for a metric reconstruction is linear.

If camera calibration rather than metric scene reconstruction is the goal, then it is not
always necessary to compute an explicit projective reconstruction, and sometimes the
camera calibration may be computed more directly than via a rectifying transformation.
This is the case, for instance, if a camera rotates about its centre without translation, as
is discussed in section 19.6.

19.2 Algebraic framework and problem statement

Suppose we have a projective reconstruction {Pi, Xj}; then based on constraints on the
cameras’ internal parameters or motion we wish to determine a rectifying homography
H such that {PiH, H−1Xj} is a metric reconstruction.
We start from the true metric situation with calibrated cameras, and structure repre-
sented in a Euclidean world frame. Thus in actuality there are m cameras Pi
M which
project a 3D point XM to an image point xi = Pi
MXM in each view, where the subscript
M indicates that the cameras are calibrated and the world frame is Euclidean. The
cameras may be written as Pi

M = Ki[Ri | ti] for i = 1, . . . , m.

460

19 Auto-Calibration

In a projective reconstruction we obtain cameras Pi which are related to Pi

M by

Pi
M = PiH i = 1, . . . , m

(19.1)

where H is an unknown 4 × 4 homography of 3-space. Our goal is to determine H.
To be precise we are not concerned with the absolute rotation, translation and scale
of the reconstruction, and we will now factor out this similarity component. We choose
the world frame to coincide with the ﬁrst camera, so that R1 = I and t1 = 0. Then Ri, ti
speciﬁes the Euclidean transformation between the i-th camera and the ﬁrst, and P1
M =
K1[I | 0]. Similarly, in the projective reconstruction we choose the usual canonical
camera for the ﬁrst view, so that P1 = [I | 0]. Then writing H as

M = P1H from (19.1) becomes [K1 | 0] = [I | 0]H, which implies that
the condition P1
A = K1 and t = 0. In addition, since H is non-singular, k must be non-zero, so we may
assume k = 1 (this ﬁxes the scale of the reconstruction). This shows that H is of the
form

(cid:17)

(cid:17)

H =

t
A
vT k

H =

K1 0
vT 1

(cid:18)

(cid:18)

.

This has factored out the similarity component.

The vector v, together with K1, speciﬁes the plane at inﬁnity in the projective recon-

struction since the coordinates of π∞ are

(cid:17)

 =

 0

0
0
1

π∞ = H−T

−T −(K1)
1

(K1)
0

−Tv

 =

(cid:18)  0

0
0
1

(cid:20) −(K1)

−Tv

1

(cid:21)

.

We will write π∞ = (pT, 1)T so that p = −(K1)
shown:
Result 19.1. A projective reconstruction {Pi, Xj} in which P1 = [I | 0] can be trans-
formed to a metric reconstruction {PiH, H−1Xj} by a matrix H of the form

−Tv. To summarize so far we have

(cid:17)

(cid:18)

H =

K

0
−pTK 1

(19.2)

where K is an upper triangular matrix. Furthermore,

(i) K = K1 is the calibration matrix of the ﬁrst camera.
(ii) The coordinates of the plane at inﬁnity in the projective reconstruction are given

by π∞ = (pT, 1)T.

Conversely, if the plane at inﬁnity in the projective frame and the calibration matrix of
the ﬁrst camera are known, then the transformation H that converts the projective to a
metric reconstruction is given by (19.2).

19.2 Algebraic framework and problem statement

461

From this result it follows that to transform a projective to a metric reconstruction it is
sufﬁcient to specify 8 parameters – the three entries of p and ﬁve entries of K1. This
agrees with a geometric counting argument. Finding metric structure is equivalent to
specifying the plane at inﬁnity and the absolute conic (three and ﬁve degrees of freedom
respectively). In a metric reconstruction the calibration Ki of each camera, its rotation
Ri relative to the ﬁrst camera, and its translation ti relative to the ﬁrst camera up to a
single common scaling, i.e. ti (cid:6)→ sti, are all determined.
We now develop the basic auto-calibration equations. We denote the cameras of the
projective reconstruction as Pi = [Ai | ai]. Multiplying out (19.1) using (19.2) gives

(cid:15)

(cid:16)

KiRi =

(cid:15)
K1
Ai − aipT
which may be rearranged as Ri = (Ki)
(cid:16)
(cid:15)
rotation Ri may be eliminated using RRT = I, leaving

Ai − aipT
−1

(cid:16)

(cid:15)
Ai − aipT

(cid:16)T

Ai − aipT

for i = 2, . . . , m

(19.3)

K1

i = 2, . . . , m. Finally, the

KiKiT =
Note now that KiKiT = ω∗
(8.11–p210). Making this substitution gives the basic equations for auto-calibration:

, the dual image of the absolute conic (or DIAC) – see

K1K1T

.

(cid:15)
(cid:15)
Ai − aipT
Ai − aipT

(cid:16)
(cid:16)−T

ω∗1

(cid:15)

(cid:16)T
(cid:16)
(cid:15)
Ai − aipT
Ai − aipT

ω1

−1

ω∗i =
ωi =

(19.4)
the second equation being simply the inverse of the ﬁrst, with ω the image of the
absolute conic (or IAC). These equations relate the unknown entries of ω∗i or ωi i =
1, . . . , m and unknown parameters p with the known entries of the projective cameras
Ai, ai.

The art of auto-calibration is to use constraints on the Ki, such as that one of the
elements of Ki is zero, to generate equations on the eight parameters of p and K1
from (19.4). All auto-calibration methods are variations on solving these equations,
and in the following sections we describe several of these methods. Generally meth-
ods proceed by computing ωi or ω∗i ﬁrst and extracting the values of the calibra-
tion matrices Ki from these – though iterative methods (such as bundle adjustment)
may be parametrized by Ki directly. The equations (19.4) have a geometric interpreta-
tion as mappings on the absolute conic, and we will return to this in section 19.3 and
section 19.5.2.

We start with a simple example to illustrate how equations on the eight parameters

are generated from (19.4).

Example 19.2. Auto-calibration equations for identical Ki
Suppose that all the cameras have the same internal parameters, so Ki = K, then (19.4)
becomes

(cid:16)

(cid:15)

(cid:15)

(cid:16)T

KKT =

Ai − aipT

KKT

Ai − aipT

i = 2, . . . , m.

(19.5)

Each view i = 2, . . . , m provides an equation, and we can develop a counting argument

462

19 Auto-Calibration

for the number of views required (in principle) in order to be able to determine the 8
unknowns. Each view other than the ﬁrst imposes 5 constraints since each side is a
3×3 symmetric matrix (i.e. 6 independent elements) and the equation is homogeneous.
Assuming these constraints are independent for each view, a solution is determined
provided 5(m − 1) ≥ 8. Consequently, provided m ≥ 3 a solution is obtained, at
least in principle. Clearly, if m is much larger than 3 the unknowns K and p are very
(cid:2)
over-determined.
One could imagine using (19.5) as a basis for a direct estimation of the rectifying
transformation H. This may be framed as a parametrized minimization problem in
which the eight parameters of (19.2) are allowed to vary with the purpose of minimizing
a cost function on how well the equations (19.4) are satisﬁed or measuring closeness
to metric structure. Of course, a method of obtaining an initial solution would also
be required.
In essence these two steps, initial solution and iterative minimization,
are what will be investigated in the following sections – though under constraints less
restrictive than identical internal parameters.

19.3 Calibration using the absolute dual quadric
The absolute dual quadric, Q∗
a 4 × 4 homogeneous matrix of rank 3. Its importance here is that Q∗
π∞ and Ω∞ in a very concise fashion, for instance π∞ is the null-vector of Q∗
has an algebraically simple image projection:
= PQ∗

∞ is a degenerate dual (i.e. plane) quadric represented by
∞ encodes both
∞, and it

(19.6)

ω∗

∞PT

The idea of auto-calibration based on Q∗
to a constraint on Q∗

which is simply the projection (8.5–p201) of a (dual) quadric. In words, Q∗
the dual image of the absolute conic ω∗
ω∗
representing Q∗
on Ki, as will be seen below. Indeed, Q∗
for auto-calibration in [Triggs-97].

∞ is to use (19.6) to transfer a constraint on
∞ via the (known) camera matrix Pi. In this manner the matrix
∞ may be determined in the projective reconstruction from constraints
∞ was introduced as a convenient representation

∞ projects to

= KKT.

∞ has been determined, then the rectifying homography (19.2) H that we
seek is also determined as shown below. Thus we have a general framework for auto-
calibration based on specifying constraints on Ki to determine Q∗
∞ de-
termining H. This general approach is summarized in algorithm 19.1. In section 19.3.1
we will concentrate on the second step of this algorithm, estimation of Q∗
∞. We ﬁrst ﬁll
in some details.

∞, and then from Q∗

Once Q∗

Simple properties of the absolute dual quadric. Section 3.7(p83) gives a full de-
scription of Q∗
∞. For the purposes of auto-calibration particularly important properties
are summarized here. In a Euclidean frame Q∗

∞ has the canonical form

(cid:17)

(cid:18)

˜I =

I3×3 0
0T
0

.

(19.7)

19.3 Calibration using the absolute dual quadric

463

Objective

Given a set of matched points across several views and constraints on the calibration matrices
Ki, compute a metric reconstruction of the points and cameras.
Algorithm

Pi and points Xj.
Q∗
∞.

(i) Compute a projective reconstruction from a set of views, resulting in camera matrices
(ii) Use (19.6) along with constraints on the form of the ω∗i arising from Ki to estimate
(iii) Decompose Q∗
(iv) Apply H−1 to the points and H to the cameras to get a metric reconstruction.
(v) Use iterative least-squares minimization to improve the solution (see section 19.3.3).

∞ as H˜IHT, where ˜I is the the matrix diag(1, 1, 1, 0).

Alternatively, the calibration matrix of each camera may be computed directly:

(i) Compute ω∗i for all i using (19.6).
(ii) Compute the calibration matrix Ki from the equation ω∗ = KKT by Cholesky factoriza-

tion.

Algorithm 19.1. Auto-calibration based on Q∗
∞.

∞ has the form Q∗

In a projective coordinate frame Q∗
∞ = H˜IHT, where ˜I is the matrix
in (19.7). This follows from the projective transformation rule (3.17–p74) for dual
quadrics, Q∗
Result 19.3. In an arbitrary projective frame, the dual absolute quadric is represented
by a symmetric 4 × 4 matrix with the following properties.

∞ (cid:6)→ HQ∗

∞HT. Consequently:

(i) It is singular of rank 3, since Q∗
(ii) Its null space is the vector representing the plane at inﬁnity, since Q∗
∞π∞ = 0.
(iii) It is positive semi-deﬁnite (or negative – depending on the homogeneous scale).

∞ is a degenerate conic.

∞ in its canonical form in a Euclidean frame, and

These properties are immediate for Q∗
easily extend to an arbitrary frame.
Extracting the rectifying homography from Q∗
∞ in a pro-
jective coordinate frame we wish to determine the homography H. Extracting H is a
simple matter of decomposing the expression as follows.
∞ = H˜IHT (see notation above), then H−1 is a 3D
Result 19.4. If Q∗
(point) homography that takes the projective coordinate frame to a Euclidean frame.

∞. Given an estimated Q∗

∞ is decomposed as Q∗

Note that a camera is transformed by the inverse of the transformation applied to points,
so H is the correct matrix to apply to a camera to give PM = PH. Thus H is the rectifying
transformation to apply to cameras. A decomposition of Q∗
∞ as H˜IHT may be easily
computed from its eigenvalue decomposition (see section A4.2(p580) for Jacobi’s al-
gorithm for this).
Equivalence to auto-calibration equations. Equations (19.6) which describe the
image projection of Q∗
∞ are simply a geometric representation of the auto-calibration
equations (19.4) as will now be demonstrated.

464

19 Auto-Calibration

The forms of ω = (KKT)−1 and ω∗ = ω−1 = KKT for a camera with calibration matrix K as
in (6.10–p157) are

 α2

ω∗

=

x + s2 + x2
0
sαy + x0y0

x0

sαy + x0y0 x0
y0
1

α2
y + y2
0
y0





and

ω = 1
α2

xα2

y

−sαy
α2
−sαy
y
x + s2
α2
y + y0sαy αysx0 − α2
xy0 − s2y0 α2

xα2

−x0α2
αysx0 − α2
y + α2

xy2

y + y0sαy
xy0 − s2y0
0 + (αyx0 − sy0)2

−x0α2

If the skew is zero, i.e. s = 0, then the expressions simplify to

 α2

x + x2
0

x0y0

x0y0

α2
y + y2
0

ω∗

=

and



ω =

1
α2
xα2
y

x0

y0

α2
y

0

α2
0
x
−α2
yx0 −α2

xy0 α2

xα2

y + α2

yx2

0 + α2

xy2
0



x0

y0

1

−α2
−α2

yx0

xy0

(19.9)



(19.10)

(19.11)

(19.12)



Table 19.1. The image of the absolute conic, ω, and dual image of the absolute conic, ω∗
terms of the camera internal parameters.

, written in

(cid:17)

(cid:18)

(cid:18)

(cid:17)

We have seen that in a projective frame Q∗

∞ has the form H˜IHT. The projective recon-

struction is related to the metric reconstruction by (19.2), so in detail

.

=

(cid:16)

(cid:15)

ω∗1

∞PiT =

Q∗
∞ = H˜IHT =

K1K1T

−K1K1Tp
−pTK1K1T pTK1K1Tp

ω∗1

−ω∗1p
−pTω∗1 pTω∗1p
(cid:15)
Ai − aipT

(cid:16)T

(19.8)
On applying (19.6) with Pi = [Ai | ai] we obtain once again the auto-calibration equa-
tions (19.4)

ω∗i = PiQ∗

∞ is a ﬁxed quadric under the Euclidean

Ai − aipT
This is a geometric interpretation of (19.4) – Q∗
motion of the camera, and the DIAC ω∗i is the image of Q∗
19.3.1 Linear solutions for Q∗
The objective here is to estimate Q∗
∞ in a projective reconstruction directly from con-
straints on the internal parameters. We will start by describing three cases for which a
linear solution may be obtained. It is convenient at this point to summarize the forms
of the DIAC and also the IAC. Refer to table 19.1.
Specifying linear constraints on Q∗
∞ may be obtained if
the principal point is known. Assume that this point is known, then we may change

∞. Linear constraints on Q∗

∞ from a set of images

∞ in each view.

.

19.3 Calibration using the absolute dual quadric

465

the image coordinate system so that the origin coincides with the principal point. Then
x0 = 0, y0 = 0, and from table 19.1 the DIAC becomes
x + s2 sαy 0
sαy
0
0
1

 α2

 .

(19.13)

α2
y
0

ω∗

=

The linear equations on Q∗
applying the projection equation (19.6) ω∗
two equations

∞ are then generated from the zero entries in (19.13) by
∞PT to each view i. For example the

= PQ∗

∞PiT)23 = 0

∞PiT)13 = 0 and (PiQ∗
13 = ω∗i

(PiQ∗
follow immediately from ω∗i
If there are additional constraints on Ki which result in further relationship between
the entries of ω∗
, then these may provide additional linear equations. For instance, an
assumption that skew is zero means that the (1,2)-entries of (19.13) vanish, which pro-
vides one more linear equation on the entries of Q∗
∞ similar to (19.14). Known aspect
ratio provides a further constraint. Table 19.2 summarizes the possible constraints that
may be used.

23 = 0.

(19.14)

Condition

constraint

type

# constraints

zero skew

principal point (p.p.) at origin

zero skew (p.p. at origin)

33 = ω∗

13ω∗

23

ω∗
12ω∗
ω∗
13 = ω∗
ω∗
12 = 0

23 = 0

ﬁxed (unknown) aspect ratio
(zero skew and p.p. at origin)

ω∗ i
11
ω∗ i
22

= ω∗ j
11
ω∗ j
22

known aspect ratio r = αy/αx (zero
skew and p.p. at origin)

r2ω∗

11 = ω∗

22

quadratic

linear

linear

quadratic

m

2m

m
m − 1

linear

m

Table 19.2. Auto-calibration constraints derived from the DIAC. The number of constraints column
gives the total number of constraints over m views, assuming the constraint is true for each view. Each
additional item of information generates additional equations. For example, if the principal point is
known and skew is zero then there are 3 constraints per view.

Linear solution. Since it is symmetric, Q∗
∞ may be parametrized linearly by 10 ho-
mogeneous parameters, namely the 10 diagonal and above-diagonal entries. These 10
entries may be represented by a 10-vector x. In the usual manner the linear equations
on Q∗
∞ may be assembled into a matrix equation of the form Ax = 0, and a least-squares
solution for x obtained via the SVD. For example, the two equations (19.14) provide
two rows of the matrix from each view. From ﬁve images a total of 10 equations are
obtained (assuming only that principal point is known), and a linear solution is pos-
sible. From four images eight equations are generated. In the same way as with the

466

19 Auto-Calibration

computation of the fundamental matrix from seven points there is a 2-parameter family
of solutions. The condition det Q∗
∞ = 0 gives a fourth-degree equation and so up to
four solutions for Q∗
∞.
Example 19.5. Linear solution for variable focal length
Suppose the camera is calibrated apart from the focal length – the principal point is
known, the aspect ratio is unity (if it is not the equations can be transformed so that it is
unity from the known value), and skew is zero – the focal length is unknown and may
vary between views. In this case from table 19.2 there are four linear constraints on Q∗
∞
available from each view. In the case of two views there are eight linear constraints and
∞ = 0. If m ≥ 3 a unique
up to four solutions are obtained using the condition det Q∗
(cid:2)
linear solution exists.

More will be said about determination of focal lengths in this minimal case in
example 19.8.
19.3.2 Non-linear solutions for Q∗
∞
We now describe various non-linear equations that can be obtained from the form of
(19.6). It has been seen that each element of ω∗i = PiQ∗
∞PiT is expressible as a linear
expression in terms of the parameters of Q∗
∞. It follows that any relationship between
the entries of the various ω∗i translates into an equation involving the entries of Q∗
∞. In
particular, linear or quadratic relationships between entries of ω∗
generate respectively
linear or quadratic relationships between entries of Q∗
∞. Given sufﬁciently many such
equations, we may solve for Q∗
∞.

Constant internal parameters.
If the internal parameters of all cameras are the same,
then ω∗i = ω∗j for all i and j, which expands to PiQ∗
∞Pj T. However, since
these are homogeneous quantities, the equality holds only up to an unknown scale. A
set of ﬁve equations are generated:
12 = ω∗i

∞PiT = PjQ∗

13 = ω∗i

11/ω∗j

11 = ω∗i

22/ω∗j

22 = ω∗i

13/ω∗j

23/ω∗j

23 = ω∗i

33/ω∗j
33.

ω∗i

12/ω∗j

This gives a set of quadratic equations in the entries of Q∗
of 10 equations result, which may be solved to ﬁnd Q∗
∞.

∞. Given three views, a total

Calibration assuming zero skew. Under the assumption of zero skew in each of the
cameras, the form of the DIAC is simpliﬁed, as given in (19.11). In particular, we
obtain the following constraints between the entries of ω∗

in the zero-skew case

ω∗

12

ω∗
33 = ω∗

13

ω∗
23.

(19.15)

This gives a single quadratic equation in the entries of Q∗
obtain m quadratics. However there is also one extra equation det Q∗
the fact that the absolute dual quadric is degenerate. Since Q∗
linear parameters, it may be computed (at least in principle) from 8 views.
These different calibration constraints are also summarized in table 19.2.

∞. From a set of m views we
∞ = 0 derived from
∞ has 10 homogeneous

19.3 Calibration using the absolute dual quadric

467

19.3.3 Iterative methods
As we have seen on many occasions throughout this book there is a choice between
minimizing an algebraic or geometric error. In this case a suitable algebraic error is
provided by (19.4) In previous cases, such as (4.1–p89), the unknown scale factor has
been eliminated by forming a cross product. Here the scale factor can be eliminated by
using a matrix norm. The cost function is

(cid:7)

(cid:10)KiKiT − PiQ∗

∞PiT(cid:10)2

F

(19.16)

i

where (cid:10)M(cid:10)F is the Frobenius norm of M, and KiKiT and PiQ∗
∞PiT are both normalized
to have unit Frobenius norm. The cost function is parametrized by the (at most eight)
∞, and the unknown elements of each ω∗i = KiKiT. It is possible
unknown elements of Q∗
to use the expansion (19.8) to parametrize the absolute dual quadric. For example, in
the case of example 19.5 where the focal length is the only unknown per view, (19.16)
would be minimized over m+3 parameters. These are the focal length f i of each view,
and the three components of p. Note that this parametrization ensures that Q∗
∞ has rank
3 throughout the minimization.

Since the above cost function has no particular geometric meaning, it is advisable
to follow this up with a complete bundle adjustment.
In fact, given a good initial
linear estimate one can proceed directly to bundle adjustment. There is no difﬁculty in
incorporating assumptions on calibration parameters into a full bundle adjustment as
described in section 18.1(p434).

Example 19.6. Metric reconstruction for general motion
Figure 19.1(a-c) shows views of an Indian temple acquired by a hand held camera. A
projective reconstruction is computed from image point correspondences as described
in section 18.6, and a metric reconstruction obtained using algorithm 19.1 under the
constraint of constant camera parameters with known principal point. The computed
(cid:2)
cameras and 3D point cloud are shown in ﬁgure 19.1(d) and (e).

19.3.4 A counting argument
The constraints we have seen have been of two types: a parameter has a known value;
or a parameter is ﬁxed across views but its value is unknown. The actual constraints that
apply depend on the physical circumstances of the image production from acquisition
by the camera, through digitization and cropping, to the ﬁnal image. For example, for
an image sequence in which the lens is zoomed it might be the case that the skew and
aspect ratio are ﬁxed (but unknown), but that the focal length and principal point vary
through the sequence. Often it is the case that the pixels are square or have a known
aspect ratio, so that both the skew (which is zero) and aspect ratio are known.

We will now consider the number of constraints that are required to determine a

metric reconstruction fully.

The number of parameters that must be computed to perform calibration is 8. This
is equal to the number of essential parameters of the absolute dual quadric, including
the scale ambiguity and rank-3 constraint. Consider m views and suppose that k of the

468

19 Auto-Calibration

a

b

c

d

e

f

g

Fig. 19.1. Metric reconstruction for general motion. (a)-(c) 3 views (of 5) acquired by a hand held
camera. (d) and (e) Two views of a metric reconstruction computed from interest points matches over
the ﬁve views. The cameras are represented by pyramids with apex at the computed camera centre. (f)
and (g) two views of a texture mapped 3D model computed from the original images and reconstructed
cameras using an area based stereo algorithm. Figures courtesy of Marc Pollefeys, Reinhard Koch, and
Luc Van Gool.

internal parameters are known in all views, and f are ﬁxed over the views but unknown
(where k + f ≤ 5). A ﬁxed and known calibration parameter provides one constraint
per view via the condition ω∗i = PiQ∗
∞PiT, for a total of mk constraints. A ﬁxed but
unknown calibration parameter provides one fewer constraint, since just the value of
the unknown parameter is missing. Thus f ﬁxed parameters provide a total of f (m−1)
constraints. The requirement for calibration is then that
mk + (m − 1)f ≥ 8.

Table 19.3 gives values for m for several combinations of constraints. It is important to
remember that degenerate conﬁgurations are of course possible in which some of the
constraints are dependent. This will increase the number of required views.

19.3.5 Limitations of the absolute quadric approach to calibration
The following considerations apply to calibration using this method.

Limitations of least-squares algebraic solution.
Since the least-squares solution
(e.g. of Ax = 0 in the linear solution for the ω∗
) minimizes but does not enforce
constraints, the solution obtained will not precisely satisfy the required conditions.
This observation holds in over-constrained cases. For instance in the case of estimating
focal lengths in example 19.5, the entries ω∗i
22 will not be in the required

11 and ω∗i

19.4 The Kruppa equations

469

Condition

ﬁxed

known

views

Constant internal parameters

Aspect ratio and skew known,
focal length and principal point vary

Aspect ratio and skew constant,
focal length and principal point vary

Skew zero, all other parameters vary

p.p. known all other parameters vary

p.p. known skew zero

p.p., skew and aspect ratio known

f

5

0

2

0

0

0

0

k

0

2

0

1

2

3

4

m

3

4*

5*

8*

4*, 5(linear)

3(linear)

2, 3(linear)

Table 19.3. The number of views m required under various conditions in order for there to be enough
constraints for auto-calibration. For those cases marked with an asterisk there may be multiple solu-
tions, even for general motion between views.

ratio, nor will the off-diagonal entries be precisely zero. This means that the Ki will
not be precisely of the desired form. The absolute dual quadric computed by linear
means will not have rank 3 in general, since this is not enforced by the linear equations.
A rank 3 matrix for Q∗
∞ can be obtained by setting the smallest eigenvalue to zero
in its eigenvalue decomposition (in a similar manner to using the SVD to obtain a
rank 2 matrix for F in the 8-point algorithm in section 11.1.1(p280)). This rank 3
matrix may then be directly decomposed to obtain the rectifying homography (19.2)
using result 19.4. Alternatively, the rank 3 matrix can provide the starting point for an
iterative minimization as described in section 19.3.3.

The positive-deﬁniteness condition. The most troublesome failing of this method is
the difﬁculty in enforcing the condition that Q∗
∞ is positive semi-deﬁnite, (or negative
semi-deﬁnite if the sign is reversed). This is related to the condition that ω∗
∞PT
should be positive-deﬁnite. If ω∗
is not positive-deﬁnite, then it can not be decom-
posed using Cholesky factorization to compute the calibration matrix. This is a recur-
ring problem with auto-calibration methods based on estimation of the IAC or DIAC.
If the data is noisy, then this problem may occur indicating that the data are not con-
sistent with metric reconstruction. It is not appropriate if this occurs to seek the closest
positive-deﬁnite solution, since this will generally be a boundary case leading to a spu-
rious calibration.

= PQ∗

19.4 The Kruppa equations

A different method of auto-calibration involves the use of the Kruppa equations,
which were originally introduced into computer vision by Faugeras, Luong and May-
bank [Faugeras-92a] and historically are seen as the ﬁrst auto-calibration method. They

470

19 Auto-Calibration

π

C

W

X

x

C

e

O

C /

/
x

/

e

/O

C

x

2

l

2

x1

l

1

e

l

/
1

/

e

x

/
1

/
2

l

/

C

/x
2

Fig. 19.2. Epipolar tangency for a conic. A conic CW on a world plane π is imaged as corresponding
conics C ↔ C(cid:1)
in two views. The imaged conics are consistent with the epipolar geometry of the view
pair. Upper: An epipolar plane tangent to the world conic CW deﬁnes corresponding epipolar lines
which are tangent to the imaged conics. Lower: epipolar lines, l1, l2 tangent to the imaged conic in the
ﬁrst view correspond to the epipolar lines l(cid:1)
2, respectively, tangent to the imaged conic in the second.

1, l(cid:1)

are two-view constraints that require only F to be known, and consist of two indepen-
dent quadratic equations in the elements of ω∗

.

The Kruppa equations are an algebraic representation of the correspondence of
epipolar lines tangent to a conic. The geometry of this correspondence is illustrated
in ﬁgure 19.2. Suppose the conics C and C(cid:2)
are the images of a conic CW on a world
plane in the ﬁrst and second views respectively, and that C∗
are their duals. In
the ﬁrst view the two epipolar tangent lines l1 and l2 may be combined into a single de-
generate point conic (see example 2.8(p32)) as Ct = [e]×C∗
[e]×. (It may be veriﬁed that
any point x on the lines l1 and l2 satisﬁes xTCtx = 0). Similarly, in the second view
]×C∗(cid:2)
t = [e(cid:2)
the corresponding epipolar lines l(cid:2)
]×. The
epipolar tangent lines correspond under the homography H induced by any world plane
t = H−TCtH−1,
π. Since Ct is a point conic it transforms according to result 2.13(p37) C(cid:2)
and the correspondence of the lines requires that

2 may be written as C(cid:2)

1 and l(cid:2)

and C∗(cid:2)

[e(cid:2)

[e(cid:2)

]×C∗(cid:2)

[e(cid:2)

]× = H−T[e]×C∗

[e]×H−1

= FC∗FT

(19.17)
the last equality following from F = H−T[e]× (see page 335). Note, this equation does
not enforce the condition that the epipolar tangent lines map individually to their cor-
responding lines, only that their symmetric product maps to their symmetric product.
The development to this point applies to any conic. However, in the case of interest

here the world conic is the absolute conic on the plane at inﬁnity, so that C∗
ω∗(cid:2)

, (and H = H∞), and (19.17) specializes to

19.4 The Kruppa equations

[e(cid:2)

]×ω∗(cid:2)

[e(cid:2)

]× = FωFT

471

= ω∗

, C∗(cid:2)

=

(19.18)

[e(cid:2)

]×ω∗

If the internal parameters are constant over the views then ω∗(cid:2)
and so
]× = Fω∗FT, which are the Kruppa equations in a form originally given
[e(cid:2)
by Vi´eville [Vieville-95]. On eliminating the homogeneous scale factor, one obtains
equations quadratic in the elements of ω∗

= ω∗

.

Although (19.18) concisely expresses the Kruppa equations, it is not in a form that
can be easily applied. A succinct and easily usable form of the Kruppa equations is
now given. We show that the null-space of [e(cid:2)
]×, which is common to both sides of
(19.18), can be eliminated leaving an equation between two 3-vectors.
Result 19.7. The Kruppa equations (19.18) are equivalent to

 uT

−uT
1
uT
1

2

ω∗(cid:2)u2
ω∗(cid:2)u2
ω∗(cid:2)u1

 ×

 σ2

1

vT
1
σ1σ2vT
1
vT
σ2
2
2

ω∗v1
ω∗v2
ω∗v2

 = 0

(19.19)

where ui, vi and σi are the columns and singular values of the SVD of F. This provides
three quadratic equations in the elements ω∗

, of which two are independent.

ij of ω∗

Proof. The fundamental matrix has rank 2, and thus has an SVD expansion

 σ1

 VT

σ2

0

F = UDVT = U

where the null-vectors are FTu3 = 0 and Fv3 = 0. This means that the epipoles are
e = v3 and e(cid:2)

= u3. Substituting this expansion into (19.18) we obtain

(19.20)

We now use the property that U is an orthogonal matrix. On pre-multiplying (19.20) by
UT, and post-multiplying by U, the LHS becomes
u2 −u1 0
−uT

u2 −u1 0

UT[u3]×ω∗(cid:2)

[u3]×U =



#

2

=

ω∗(cid:2)"

#T
ω∗(cid:2)u2 −uT
ω∗(cid:2)u2
2
uT
1
0

[u3]×ω∗(cid:2)
[u3]× = UDVTω∗VDUT.
"
 uT
 σ1
 σ2

 VTω∗V

 σ1

0
ω∗v1
ω∗v2

σ2

=

1

1

vT
1
σ1σ2vT
1
0

σ2

ω∗v2 0
σ1σ2vT
ω∗v2
1
vT
σ2
0
2
2
0
0

ω∗(cid:2)u1 0
ω∗(cid:2)u1
0
0
0


 .

0

and the RHS of (19.20) becomes

DVTω∗VD =

472

19 Auto-Calibration

It is evident that both sides have reduced to symmetric matrices each with three in-
dependent elements. These three elements may be represented by a homogeneous 3-
vector on each side:

(cid:15)
(cid:15)

xT

LHS =

xT

RHS =

ω∗(cid:2)u2, −uT
ω∗v1, σ1σ2vT
vT
1

1

1

uT
2

σ2
1

ω∗(cid:2)u2, uT

1

ω∗v2, σ2

ω∗(cid:2)u1
vT
2

2

(cid:16)
ω∗v2

(cid:16)

.

The two sides are only equal up to a scale factor. However, equalities are obtained in
the usual way using a vector cross-product, xLHS × xRHS = 0. An alternative derivation
is given in [Hartley-97d].

Note, the Kruppa equations involve the DIAC, rather than the IAC, since they arise
from tangent line constraints, and line constraints are more simply expressed using a
dual (line) conic.

We now discuss the solution of the Kruppa equations, beginning with a simple ex-
ample where all the internal parameters are known apart from the focal length. An
alternative method for solving this problem using the absolute dual quadric was given
in example 19.5.

Example 19.8. Focal lengths for a view pair
Suppose two cameras have zero skew and known principal point and aspect ratio, but
unknown and different focal lengths (as in example 19.5). Then from (19.11) by a
suitable change of coordinates their DIACs may be written as
(cid:2)2, α

= diag(α2, α2, 1), ω∗(cid:2)

= diag(α

(cid:2)2, 1)

ω∗

(cid:2)

are the unknown focal lengths of the ﬁrst and second view, respectively.

where α, α
Writing the Kruppa equations (19.19) as
= − uT

1

ω∗(cid:2)u2
ω∗v2

ω∗(cid:2)u2
uT
2
ω∗v1
vT
σ2
1
1

it is evident that each numerator is linear in terms of α
in α2. Cross-multiplying provides two simple quadratic equations in α2 and α
are easily solved. The values of α and α
if the internal parameters are the same for the two views (that is α = α
equation of result 19.7 provides a quadratic in the single unknown α2.

ω∗(cid:2)u1
uT
1
ω∗v2
vT
σ2
2
2
(cid:2)2 and each denominator linear
(cid:2)2 which
are found by taking the square roots. Note,
) then each
(cid:2)

σ1σ2vT
1

=

(cid:2)

(cid:2)

Extending the Kruppa equations to multiple views.
In the absence of knowledge
of the internal parameters, other than that the parameters are constant across views, the
Kruppa equations provide two independent constraints on the ﬁve unknown parame-
ters. Thus given three views, with F known between each pair, there are in principle
six quadratic constraints, which is sufﬁcient to determine ω∗
. Using any ﬁve of these
equations results in ﬁve quadratics in ﬁve unknowns, a total of 25 possible solutions.
Solving this set of equations is not a particularly promising approach, although solu-
tions have been obtained by homotopy continuation [Luong-92] and by minimizing
algebraic residuals using every view pair for a sequence of images [Zeller-96].

19.5 A stratiﬁed solution

473

Ambiguities.
vide no constraint on ω∗
translation, reduces to [e(cid:2)

If there is no rotation between views then the Kruppa equations pro-
. This may be seen from (19.18) which, in the case of pure
]×ω∗

]×, since F = [e(cid:2)

[e(cid:2)

[e(cid:2)

]× = [e(cid:2)

]×ω∗

]×.

The Kruppa equations are closely related to the calibration constraint provided by the
transfer of the IAC under the inﬁnite homography, as discussed later in section 19.5.2.
It will follow from that discussion that the constraint placed on ω∗
by the Kruppa
equations for a pair of views is weaker than that placed by the inﬁnite homography
constraint (19.25). Consequently ambiguities of ω∗
imposed by the Kruppa equations
are a superset of those imposed by (19.25).

The application of the Kruppa equations to three or more views provides weaker
constraints than those obtained by other methods such as the modulus constraint
(section 19.5.1) or the absolute dual quadric (section 19.3.1). This is because the
Kruppa constraints are a view-pair constraint for conics obtained as a projection of
a 3D (dual) quadric. They do not enforce that the (dual) quadric is degenerate, or
equivalently do not enforce a common support plane for Ω∞ over the multiple views.
Consequently, there are additional ambiguous solutions as described in [Sturm-97b].

Although the application of the Kruppa equations to auto-calibration is chronologi-
cally the ﬁrst example in the literature, the difﬁculty of their solution and the problem
with ambiguities has seen them losing favour in the face of more tractable methods
such as the dual quadric formulation. However, if only two views are given then the
Kruppa equations are the constraint available on ω∗

.

19.5 A stratiﬁed solution

Determining a metric reconstruction involves simultaneously obtaining both the cam-
era calibration K and the plane at inﬁnity, π∞. An alternative approach is ﬁrst to obtain
by some means π∞, or equivalently an afﬁne reconstruction. The subsequent deter-
mination of K is then relatively simple because there exists a linear solution. This
approach will now be described starting with methods of determining π∞, and hence
H∞, and followed by methods of computing K given H∞.

19.5.1 Afﬁne reconstruction – determining π∞
For general motion and constant internal parameters, (19.3–p461) can be rearranged
into providing a constraint only on π∞, known as the modulus constraint. This allows
the coordinates p of π∞ to be solved for directly, and is described below.

The modulus constraint
The modulus constraint is a polynomial equation in the coordinates of π∞. Assume
the internal parameters are constant; then from (19.3–p461) with Ki = K

A − apT = µKRK−1

(19.21)

where the scale factor µ is explicitly included, and for clarity the superscripts are omit-
ted. Since KRK−1 is conjugate to a rotation, it has eigenvalues {1, eiθ, e
−iθ}. Conse-
quently, the eigenvalues of (A− apT) are {µ, µeiθ, µe
−iθ}, and thus have equal moduli.
This is the modulus constraint on the coordinates of the plane at inﬁnity, p.

474

19 Auto-Calibration

To develop this constraint further consider the characteristic polynomial of A − apT
which is

det(λI − A + apT) = (λ − λ1)(λ − λ2)(λ − λ3)

= λ3 − f1λ2 + f2λ − f3

where λi are the three eigenvalues, and

f1 = λ1 + λ2 + λ3 = µ(1 + 2 cos θ)
f2 = λ1λ2 + λ1λ3 + λ2λ3 = µ2(1 + 2 cos θ)
f3 = λ1λ2λ3 = µ3.

Eliminating the scalar µ and angle θ we obtain

f3f 3

1 = f 3
2 .

Looking more closely at the characteristic polynomial we observe that p appears
only as part of the rank-1 term apT. This means that the elements of p appear only
linearly in the determinant det(λI − A + apT), and hence linearly in each of f1, f2, f3.
Hence the modulus constraint may be written as a quartic polynomial in the three
elements pi of p. This polynomial equation is only a necessary condition for the eigen-
values to have equal moduli, not sufﬁcient.

Each view pair generates a quartic equation in the coordinates of π∞. Thus, in
principle, three views determine π∞, but only as the intersection of three quartics
in three variables – a possible 43 solutions. However, for three views an additional
cubic equation is available from the modulus constraint, and this equation can be
used to eliminate many of the spurious solutions. This cubic equation is developed
in [Schaffalitzky-00a]. The modulus constraint may also be combined with scene in-
formation. For example, if a corresponding vanishing line is available in two views,
then π∞ is determined up to a one-parameter ambiguity. Applying the modulus con-
straint resolves this ambiguity, and results in a quartic equation in one variable.
Kruppa equations are equations on ω∗
ulus constraint is an equation on π∞ which does not involve ω∗
π∞ is known the other can subsequently be determined.

The modulus constraint may be considered the cousin of the Kruppa equations: the
which do not involve π∞; conversely, the mod-
or

. Once one of ω∗

Other methods of ﬁnding π∞
Because of the problem with solving sets of simultaneous quartic equations, the modu-
lus constraint is not very satisfactory as a practical means of ﬁnding the plane at inﬁnity.
In fact, ﬁnding the plane at inﬁnity is the hardest part of auto-calibration and the place
where one is most likely to run into difﬁculties.

The plane at inﬁnity may be identiﬁed by various other methods. Several of these
are described in chapter 10. One straightforward method (which is outside the province
of pure auto-calibration) is to use properties of the scene geometry. For example, the
correspondence of a vanishing point between two views determines a point on π∞,
and three such correspondences determine π∞ in a projective reconstruction. Indeed,

19.5 A stratiﬁed solution

475
a good approximation for π∞ may well be obtained from the correspondence of three
points that are distant in the scene. A second method is to employ a pure translation
between two views, i.e. the camera translates but does not rotate or change internal
parameters; π∞ is determined uniquely by such a motion.

As seen in result 19.3(p463) the plane at inﬁnity may also be computed from the
absolute dual quadric and this method is quite attractive if the principal point is known.
Methods of bounding the position of π∞ using cheiral inequalities will be described in
chapter 21. These use information about which points are in front of the cameras to get
a quasi-afﬁne reconstruction, which is close to an afﬁne reconstruction. A method for
ﬁnding the plane at inﬁnity by iterative search from this initial quasi-afﬁne reconstruc-
tion is described in [Hartley-94b]. More recently, the bounds imposed by cheirality
have been used [Hartley-99] to deﬁne a rectangular region of 3D parameter space in-
side which the vector p representing the plane at inﬁnity must lie. Then an exhaustive
search is undertaken to ﬁnd the elusive plane at inﬁnity inside this region.

19.5.2 Afﬁne to metric conversion – determining K given π∞
Once the plane at inﬁnity has been determined, an afﬁne reconstruction is effectively
known. The remaining step is to transform from afﬁne to metric. It turns out that this
is a far easier step than the step from projective to afﬁne. In fact, a linear algorithm is
available based on the transformation of the IAC or its dual.

The inﬁnite homography. The inﬁnite homography H∞ is the plane projective trans-
formation between two images induced by the plane at inﬁnity π∞ (see chapter 13).
If the plane at inﬁnity π∞ = (pT, 1)T and camera matrices [Ai | ai] are known in any
projective coordinate frame, an explicit formula for the inﬁnite homography can be
derived from result 13.1(p326)

Hi∞ = Ai − aipT

(19.22)
where Hi∞ represents the homography from a camera [I | 0] to the camera [Ai | ai].
So Hi∞ may be computed from a projective reconstruction once the plane at inﬁnity is
known.
If the ﬁrst camera is not in the canonical form [I | 0], then one can still compute the
homography Hi∞ from the ﬁrst image to the i-th by writing
A1 − a1pT

Ai − aipT

(cid:16)(cid:15)

(cid:16)−1

.

(cid:15)

Hi∞ =

(19.23)

This is not strictly necessary, however, since one can invent a new view that does have
camera matrix in the canonical form [I | 0], and express the inﬁnite homographies with
respect to this view. In the following discussion, we write K and ω (without super-
scripts) to refer either to this reference view, or to the ﬁrst view, if it is in canonical
form.

The absolute conic lies on π∞ so its image is mapped between views by H∞. Under
the point transformation xi = Hi∞x, where Hi∞ is the inﬁnite homography between the
reference view and view i, the transformation rules for a dual conic (result 2.14(p37))

476

19 Auto-Calibration

and a point conic (result 2.13(p37)) lead to the relations

ω∗i = Hi∞ω∗Hi∞

T

and ωi = (Hi∞)

−Tω (Hi∞)

−1.

(19.24)

where ωi is the IAC in the i-th view. It may be veriﬁed that these equations are precisely
the auto-calibration equations (19.4–p461), and this is another geometric interpretation
of those equations.

These are among the most important relationships for auto-calibration. They are the
basis for obtaining metric reconstruction from afﬁne reconstruction, and also for cali-
brating a non-translating camera, as will be seen later in section 19.6. The signiﬁcance
of this relation for auto-calibration is that if Hi∞ is known, then these are linear relations
between ωi and ω (and similarly for ω∗
). This means that constraints placed on ωi in
one view can easily be transferred to another and in this manner sufﬁcient constraints
may be assembled to determine ω by linear means. Once ω is determined, then K fol-
lows by the Cholesky decomposition. We will illustrate this approach for the example
of ﬁxed internal parameters.

Sketch solution for identical internal parameters.
constant over m views then Ki = K and ω∗i = ω∗
equation of (19.24) becomes

If the internal parameters are
for i = 1, . . . , m, and the ω∗

ω∗

= Hi∞ω∗Hi∞T.

(19.25)

A very important point here relates to the scale factors in the equation (19.25).
• Although (19.25) is a relationship between homogeneous quantities, the scale factor
in the homogeneous equation can be chosen as unity provided Hi∞ is normalized as
det Hi∞ = 1.

This results in six equations for the independent elements of the symmetric matrix
. Then (19.25) can be written in a homogeneous linear form

ω∗

Ac = 0,

(19.26)
where A is a 6 × 6 matrix composed from the elements of Hi∞, and c is the conic ω∗
written as a 6-vector. As discussed below, c is not uniquely determined by one such
equation since A has at most rank 4. However, if linear equations (19.26) from m ≥ 2
view pairs are combined, so that A is now a 6m × 6 matrix, and provided the rotations
between the views are about different axes, then in general c is determined uniquely.

Related to the issue of uniqueness is the issue of numerical stability. Under a single
motion the linear computation of K from H∞ is extremely sensitive to the accuracy of
H∞. If H∞ is inaccurate it is not always possible to obtain a positive-deﬁnite matrix ω
(or ω∗
) and thus to apply the Cholesky decomposition to obtain K. This sensitivity is
reduced if further motions are made, and ω obtained from the combined constraints of
a number of H∞’s.

477

19.5 A stratiﬁed solution
In an analogous manner to the linear solution for ω∗

Advantage of using the IAC.
, a
linear solution may be obtained for ω starting from (19.24). In fact using the equations
involving the IAC is attractive for the following reason.
In the zero-skew case the
form of (19.12–p464) for the IAC is simpler and more clearly reﬂects the role of each
calibration parameter than does the corresponding formula (19.11–p464) for the DIAC.
In order to obtain linear equations using the DIAC equations (19.11) it is necessary to
assume that the principal point is known. This is not necessary for equations deriving
from the IAC. An assumption of zero skew is quite natural and is a safe assumption for
most imaging conditions. However, an assumption of known principal point is much
less tenable. For this reason it is usually preferable to use the IAC constraints of (19.24)
for auto-calibration rather than using the DIAC constraints.

Other calibration constraints. The algorithm just described was for constant but
arbitrary internal parameters. If more is known about K, such as the value of the aspect
ratio or that the skew is zero, the corresponding constraints may be simply added to the
set of equations on ω (or ω∗
), and imposed as soft constraints. The possible constraints
are shown in table 19.2(p465) for the DIAC (the same constraints that are used to
compute Q∗

∞ in section 19.3) and table 19.4 for the IAC.

As mentioned above, the constraints derived from the IAC are generally linear,
whereas the constraints derived from the DIAC, are linear only under the assumption
that the principal point is known (and at the origin).

Just as with the absolute dual quadric method, it is possible to allow varying internal
parameters for the cameras, as long as sufﬁciently many constraints are imposed. The
constraint of constant internal parameters for the cameras imposed a total of 5(m − 1)
constraints on the calibration parameters of the m views. We can make do with fewer
constraints, letting certain parameters vary. The method of calibration with varying
internal parameters is quite analogous to that used in the case of the absolute dual
quadric. Each entry of ωi is expressible as a linear expression in the entries of ω
according to (19.24). A linear constraint on some entry of ωi therefore maps back to a
linear constraint on the entries of ω.

Note: To avoid treating the ﬁrst image differently, any constraints imposed on the
ﬁrst camera, such as ω1
12 = 0 (camera 1 has zero skew) should be treated by adding this
equation to the complete equation set, rather than by decreasing the number of parame-
ters used to describe ω1. The latter method would cause the zero-skew constraint to be
enforced exactly in the ﬁrst image (a hard constraint), but it would be a soft constraint
in the other images.

Algorithm 19.2 summarizes the stratiﬁed method for both constant and varying
parameters. One could imagine implementing this algorithm directly for a camera
mounted on a robot: the camera is ﬁrst moved by a pure translation in order to deter-
mine π∞; and in subsequent motions the camera may both translate and rotate until
sufﬁcient rotations have accumulated to determine K uniquely.

478

19 Auto-Calibration

Condition

zero skew

constraint
ω12 = 0

principal point at origin

ω13 = ω23 = 0

known aspect ratio r = αy/αx
(assuming zero skew)

ﬁxed (unknown) aspect ratio
(assuming zero skew)

ω11 = r2ω22

ωi

11/ωi

22 = ωj

11/ωj

22

type

linear

linear

linear

# constraints

m

2m

m

quadratic

m − 1

Table 19.4. Auto-calibration constraints derived from the IAC. These constraints are derived directly
from the form of (19.10–p464) and (19.12–p464). The number of constraints column gives the total
number of constraints over m views, assuming the constraint is true for each view.

Using hard constraints. Algorithm 19.2 comes down to solving a homogeneous
set of equations of the form Ac = 0, where c represents ω arranged as a 6-vector.
Generally the supplied information on K, such as that the skew is zero, will not be
satisﬁed exactly. As discussed in section 8.8(p223) known information can be imposed
as a hard constraint by parametrizing ωi in each view to satisfy this constraint. For
instance if the camera is known to have square pixels then the remaining parameters for
the IAC of each view can be represented by a homogeneous 4-vector. Linear equations
for the unknown parameters in each view may again be obtained from (19.24). A
homogeneous set of equations of the form Ac = 0 may then be assembled, where c
now represents the unknown parameters of ωi over all views, and a solution which
minimizes (cid:10)Ac(cid:10) obtained in the usual manner via the SVD. An alternative is to include
all the parameters in each view and use algorithm A5.5(p594) to minimize (cid:10)Ac(cid:10), while
satisfying constraints Cc = 0 exactly.

19.5.3 The ambiguities in using the inﬁnite homography relation
In this section we describe the ambiguities in determining the internal parameters
from (19.25) that occur if only a single rotation axis is used. It will be assumed that the
internal parameters are unknown but ﬁxed.
A rotation matrix R has an eigenvector dr with unit eigenvalue, Rdr = 1dr, where dr
is the direction of the axis of the rotation. Consequently, the matrix Hi∞ = KRiK−1 also
has an eigenvector with unit eigenvalue (provided Hi∞ is normalized as det Hi∞ = 1).
This eigenvector is vr = Kdr, and the image point vr corresponds to the vanishing point
of the rotation axis direction. Suppose ω∗
; then, it may be veriﬁed that
if ω∗
true satisﬁes (19.25) with Hi∞ = KRiK−1, then so does the one-parameter family of
(dual) conics

true is the true ω∗

ω∗

(µ) =ω ∗

true + µvrvT

r

(19.27)

where µ parametrizes the family. In a similar manner there is a one parameter family
(pencil) of solutions for the IAC equation of (19.24). This argument indicates that

19.5 A stratiﬁed solution

479

Objective
Given a projective reconstruction {Pi, Xj}, where Pi = [Ai | ai], determine a metric recon-
struction via an intermediate afﬁne reconstruction.

Algorithm

(i) Afﬁne rectiﬁcation: Determine the vector p that deﬁnes π∞, using one of the methods
described in section 19.5.1. At this point an afﬁne reconstruction may be obtained as
{PiHP, H−1

P Xj} with

(ii) Inﬁnite homography: Compute the inﬁnite homography between the reference view

and the others as

(iii) Compute ω:

Normalize the matrix so that det Hi∞ = 1.
• In

constant

case

of

the

calibration:

equations
ω = (Hi∞)−Tω(Hi∞)−1, i = 1, . . . , m as Ac = 0 with A a 6m × 6 matrix,
and c the elements of the conic ω arranged as a 6-vector, or
• For variable calibration parameters, use the equation ωi = (Hi∞)−Tω(Hi∞)−1 to
express linear constraints on entries of ωi (e.g. zero skew) as linear equations in the
entries of ω.

rewrite

the

(iv) Obtain a least-squares solution to Ac = 0 via SVD.
(v) Metric rectiﬁcation: Determine the camera matrix K from the Cholesky decomposi-
tion ω = (KKT)−1. Then a metric reconstruction is obtained as {PiHPHA, (HPHA)−1Xj}
with

(cid:24)

(cid:24)

$

HP =

I
−pT

0
1

Hi∞ =

Ai − aipT

HA =

K
0T

0
1

(cid:25)

%

.

.

(cid:25)

.

(vi) Use iterative least-squares minimization to improve the solution (see section 19.3.3).

Algorithm 19.2. Stratiﬁed auto-calibration algorithm using IAC constraints.

although the inﬁnite homography constraint seemingly provides six constraints on the
ﬁve degrees of freedom of ω∗
, only four of these constraints are linearly independent.

Removing the ambiguity. The one-parameter ambiguity may be resolved in several
ways. First, if there is another view available related by a rotation around an axis with a
direction different to dr, then the combination of both sets of constraints will not have
this ambiguity. A linear solution is easily obtained in the manner of (19.26). Thus with
a minimum of three views (i.e. more than one rotation) a unique solution can generally
be obtained. A second method of resolving the ambiguity is to make assumptions on
the internal parameters of the cameras: for instance an assumption of zero skew (see
table 19.4). The equations enforcing zero skew may be added as hard constraints to the
set of equations being solved.

An alternative (but equivalent) method enforces the constraints a posteriori in the
following manner. An ambiguity in solving for c, from the linear equation system

480
Ac = 0, occurs when A has a 2-dimensional (or greater) right null-space. In this case
in solving for ω there would be a family of solutions of the form

19 Auto-Calibration

ω(α) =ω 1 + αω2.

Here ω1 and ω2 are known from the null-space generators, and α must be determined.
It remains simply to ﬁnd the value of α that leads to a solution satisfying the chosen
constraint condition in table 19.4. This is solved linearly. One could do the same thing
solving for the DIAC, but then the constraint condition would be quadratic (see table
19.2(p465)), and one of the solutions would be spurious.

In certain cases, these additional constraints do not resolve the ambiguity. For exam-
ple, skew-zero does not resolve the ambiguity if the rotation is about the image x- or
y-axes. Such exceptions are described in more detail in [Zisserman-98], and we give a
few commonly occurring examples now.

Typical ambiguities.
The one-parameter family of solutions given in (19.27) for
ω∗
(µ) corresponds to a one-parameter family of calibration matrices obtained from
(µ) as ω∗
ω∗
(µ) = K(µ)K(µ)T. For simplicity we will assume that the true camera K
(which is a member of this family) has skew zero, so K has four unknown parameters.
If the rotation axis is parallel to the camera X-axis, then dr = (1, 0, 0)T and vr =
Kdr = αx(1, 0, 0)T. From the form (19.11–p464) of ω∗
with no skew, the family
(19.27) is

ω∗

(µ) =ω ∗

true + µvrvT

r =

 α2

x(1 + µ) +x 2
0

x0y0
x0

 .

x0y0
y + y2
α2
y0

x0
0 y0
1

(19.28)

Note that the entire family has skew-zero, and in this case only the element ω∗
11 is
varying. This means that the principal point and αy are unambiguously determined
– since they may be read-off from elements which are unaffected by the ambiguity.
However, it is apparent that αx cannot be determined because it only appears in the
varying element ω∗
• If K is computed from the inﬁnite homography relation (19.25) assuming a zero-
skew camera, then for some motions, there remains one undetermined calibration
parameter. For rotation about various axes this ambiguity is as follows.

11(µ). To summarize this, and two other canonical cases:

(i) X-axis: αx is undetermined;
(ii) Y-axis: αy is undetermined;
(iii) Z-axis (principal axis): αx and αy are undetermined, but their ratio αy/αx

is determined.

Geometric note. These ambiguities are not limited to calibration from a pair of views,
but apply to complete sequences. For instance if the set of rotations in a camera motion
are all about the X-axis of the camera, then there will be a reconstruction ambiguity, and

19.6 Calibration from rotating cameras

481

the same is true for Y-axis rotations. One can see this geometrically as follows. Con-
sider a metric reconstruction of a scene from a sequence of images with only Y-axis ro-
tations of the camera. One can deﬁne a coordinate system in which the world Y-axis is
aligned with the direction of the camera’s y-axis. Now, consider “squashing” the whole
reconstruction (points and camera positions) so that their Y coordinate is multiplied by
some factor k. From the imaging geometry, it is easy to see that this will have the effect
of multiplying the y image coordinate of any imaged point by the same factor k, but
not affecting the x-coordinate. However, this effect can be undone by multiplying the
−1, thereby leaving
scale factor αy of coordinates in the image by the inverse factor k
image coordinates unchanged. This shows that αy is not unambiguously determined,
in fact it is unconstrained. In summary there is a one-parameter ambiguity parallel to
the rotation axis in the metric reconstruction and a corresponding one-parameter ambi-
guity in the internal parameters. This argument shows that the problem of ambiguity is
intrinsic to the motion, and not to any particular auto-calibration algorithm.
Relationship to the Kruppa equations. Writing (19.24) for two views as ω∗(cid:2)
H∞ω∗HT∞ and multiplying before and after by the matrix [e(cid:2)

]× leads to

=

[e(cid:2)

]×ω∗(cid:2)

]× = Fω∗FT

[e(cid:2)

]× = [e(cid:2)

]×H∞ω∗HT∞[e(cid:2)

since F = [e(cid:2)
]×H∞. This is simply the Kruppa equations (19.18–p471), which shows
that they follow immediately from the inﬁnite homography constraint. Since [e(cid:2)
]× is
not invertible, one can not go the other direction and derive the inﬁnite homography
constraint from the Kruppa equations. Thus, the Kruppa equations are a weaker con-
straint.

However the difference is that to apply (19.24) one needs to know the plane at inﬁnity
(and hence afﬁne structure), since it is true only for the inﬁnite homography, and not for
an arbitrary H. The Kruppa equations, on the other hand, do not involve any knowledge
of afﬁne structure of the scene. Nevertheless, this relationship shows that for a sequence
of images, any calibration ambiguity under the inﬁnite homography relation is also an
ambiguity of the Kruppa equations.

19.6 Calibration from rotating cameras

In this section, we begin consideration of calibration under special imaging conditions.
The situation considered here is the one in which the camera rotates about its centre
but does not translate. We will consider both the case of ﬁxed internal parameters, and
the case of some parameters known and ﬁxed whilst others are unknown and varying.
This situation is one that occurs frequently. Examples include: pan–tilt and zoom
surveillance cameras; cameras used for broadcasts of sporting events which are almost
invariably ﬁxed in location but free to rotate and zoom; and hand-held camcorders
which are very often panned from a single viewpoint. Even though the rotation is not
exactly about the centre, in practice the translation is generally negligible compared to
the distance of scene points, and a ﬁxed centre is an excellent approximation.

The calibration problem from rotating cameras is mathematically identical with the
afﬁne-to-metric calibration step in stratiﬁed reconstruction, as given in section 19.5.2.

482

19 Auto-Calibration

Objective
Given m ≥ 2 views acquired by a camera rotating about its centre with ﬁxed or varying internal
parameters, compute the parameters of each camera. It is assumed that the rotations are not all
about the same axis.

Algorithm

(i) Inter-image homographies: Compute the homography Hi between each view i and a
reference view such that xi = Hix using, for example, algorithm 4.6(p123). Normalize
the matrices such that det Hi = 1.

(ii) Compute ω:

• In

the

case

of

constant

calibration:

equations
ω = (Hi)−Tω(Hi)−1, i = 1, . . . , m as Ac = 0 where A is a 6m × 6 matrix,
and c the elements of the conic ω arranged as a 6-vector, or
• For variable calibration parameters, use the equation ωi = (Hi)−Tω(Hi)−1 to ex-
press linear constraints on entries of ωi in table 19.4 (e.g. unit aspect ratio) as linear
equations in the entries of ω.

rewrite

the

(iii) Compute K: Determine the Cholesky decomposition of ω as ω = UUT, and thence

K = U−T.

(iv) Iterative improvement: (Optional) Reﬁne the linear estimate of K by minimizing

(cid:7)

d(xi

j, KRiK−1xj)2

i=2,m; j=1,n

over K and Ri, where xj, xi
j are the position of the j-th point measured in the ﬁrst and
i-th images respectively. Initial estimates for the minimization are obtained from K and
Ri = K−1HiK.

Algorithm 19.3. Calibration for a camera rotating about its centre.

From a non-translating camera it is impossible to achieve an afﬁne (or any) reconstruc-
tion, because there is no way to resolve depth. Nevertheless, we may compute the
inﬁnite homography between the images, which is all that is needed to determine the
camera calibration.

As has been shown earlier (section 8.4(p202)) the images of two cameras with a
common centre are related by a plane projective transformation. Indeed, if xi and x are
−1,
corresponding image points then they are related by xi = Hix, where Hi = KiRi(K)
and Ri is the rotation between view i and the reference view. Furthermore, since this
map is independent of the depth of the points imaged at x, it applies also to points at
inﬁnity, so as shown in section 13.4(p338),

Hi = Hi∞ = KiRi(K)

−1.

Thus we have a convenient means of measuring H∞ directly from images.

Given H∞ a solution for the calibration matrices Ki of all the images in the set ac-
quired by the rotating camera may be obtained as described in section 19.5.2. The
method may be applied to either ﬁxed or variable internal parameters and is summa-
rized in algorithm 19.3. We will illustrate this by a number of examples.

19.6 Calibration from rotating cameras

483

Fig. 19.3. Calibrating a camera rotating about its centre. (upper) Five images of the US Capitol
acquired by approximately rotating the camera about its centre. (lower) A mosaic image constructed
from the ﬁve images (see example 8.14(p207)). The mosaic image shows very clearly the distortion
effect of the inﬁnite homography between the images. Analysis of this distortion provides the basis for
the auto-calibration algorithm. The calibration is computed as described in algorithm 19.3.

Example 19.9. Rotation about centre with ﬁxed internal parameters
The images in ﬁgure 19.3 were obtained using a 35mm camera with ordinary black and
white ﬁlm to produce negatives. The camera was hand-held, and no particular care was
taken to ensure that the camera centre remained stationary.

Prints enlarged from these negatives were digitized using a ﬂat-bed scanner. The
enlargement process can lead to a non-zero value of s and unequal values of αx and αy
if the negative and print paper are not precisely parallel. The resulting image size was
776 × 536 pixels.
The constraint applied here is that the internal parameters are constant. The camera

484

19 Auto-Calibration

FOCAL LENGTH

linear
non linear

10

15

Frame

20

25

PAN

linear
non linear

)
s
l
e
x
i
p
(
 
h
t
g
n
e
l
 
l
a
c
o
F

)
g
e
d
(
 
n
a
p

1200

1100

1000

900

800

700

600

500

0
-10
-20
-30
-40
-50
-60
-70
-80

0

5

0

5

a

c

PRINCIPAL POINT

linear
non linear

100 200 300 400 500 600 700

u_o (pixels)
b

TILT

linear
non linear

)
s
l
e
x
i
p
(
 
o
_
v

)
g
e
d
(
 
t
l
i
t

500

400

300

200

100

0

0

10

8

6

4

2

0

10

15

frame

20

25

0

5

15

20

25

frame

10
d

Fig. 19.4. Rotation for varying internal parameters assuming square pixels. These are for the
panned sequence of ﬁgure 8.9(p206). (a) Focal length. (b) Principal point. (c) Pan angle. (d) Tilt
angle. Figures courtesy of Lourdes de Agapito Vicente.

matrix computed as described in algorithm 19.3 is

 964.4 −4.9 392.8

966.4 282.0



Klinear =

 956.8 −6.4 392.0

959.3 281.4

 .

1

Kiterative =

1

There is little difference between the linear and iterative estimates, and the computed
(cid:2)
aspect ratio (virtually unity) and principal point are very reasonable.

Example 19.10. Rotation about centre with varying internal parameters
The images in ﬁgure 8.9(p206) were acquired by panning a camcorder approximately
about its centre. The camera was not zoomed, but due to auto-focus there might be
slight variations in the focal length and principal point.

In this example the constraint used is that the pixels are square: i.e. that the skew is
zero and the aspect ratio unity; but the focal length and principal point are unknown and
not ﬁxed. Then from table 19.4 we have two linear constraints on ω from each view.
−1)11 =
From zero skew (Hi∞
−1)22. These constraints are assembled to give a linear system of equations
(Hi∞
on ω as described in algorithm 19.3.

−1)12 = 0, and from unit aspect ratio (Hi∞

−T ω Hi∞

−T ω Hi∞

−T ω Hi∞

The internal parameters of the computed camera matrix for each view are shown
It is evident that the recovered focal length and principal point are
in ﬁgure 19.4.
quite constant (even though this was not imposed), and the pan and tilt angles are very
(cid:2)
reasonable for this hand-held sequence.

19.7 Auto-calibration from planes

485

19.7 Auto-calibration from planes

For a set of images of a planar scene, the two-step approach of estimating a projec-
tive reconstruction followed by computation of a rectifying transformation to take it
to metric does not work. This is because it is not possible to determine the cameras
without depth relief. As seen in section 17.5.2(p425), a minimum of two points not
lying on the plane are required. Nevertheless, auto-calibration from scene planes is
possible. This was shown by [Triggs-98] who gave a solution in the case of constant
internal parameters. The method is especially interesting from the point of view of po-
tential applications. Scenes consisting of planes are extremely common in man-made
environments, e.g. the ground plane. Furthermore, in aerial images acquired by a high-
ﬂying aircraft or a satellite, the depth-relief of the scene is small compared with the
extent of the image, and the scene may be accurately approximated as a plane and the
auto-calibration method will apply.

The starting point of the algorithm is a set of image–to–image homographies induced
by the world plane. These can all be related back to the ﬁrst image, providing a set of
homographies Hi. Geometrically auto-calibration from planes is then a marriage of two
ideas. First, the circular points on the plane, which are the intersection of the absolute
conic with the plane, are mapped from image to image via the homographies. Second,
as we have seen in example 8.18(p211), the calibration matrix K may be determined
from the imaged circular points of a plane (with two constraints provided by each
image).

Thus suppose the images of the circular points (4 dof) are determined in the ﬁrst
image (by some means), then they may be transferred to the other views by the known
Hi. In each view then there are two constraints on ω, since the two imaged circular
points lie on ω. In detail, if we denote the (at this stage unknown) imaged circular
points in the ﬁrst view by cj, j = 1, 2. Then the auto-calibration equations are

(Hicj)Tωi(Hicj) = 0, i = 1, . . . , m j = 1, 2

(19.29)

where H1 = I. In solving these equations the unknowns are the coordinates of the
circular points in the ﬁrst image, and some number of unknown calibration parameters.
Although the circular points are complex points, they are complex conjugates of each
other, so 4 parameters sufﬁce to describe them. If in addition there are a total of v
unknowns in the internal parameters Ki of all m views, then a solution is possible
provided 2m ≥ v + 4, since each view provides two equations.
Restrictions on the internal parameters of the cameras lead to further algebraic con-
straints according to table 19.4 and these are used to supplement the constraints im-
posed by (19.29). Various cases are considered in table 19.5. In most cases, calibration
from a plane is a non-linear problem and nothing more will be said here about computa-
tional aspects of how to ﬁnd the solution. Iterative methods are necessary, minimizing
some cost function. See [Triggs-98] for more details of minimization methods.

Implementation. A considerable implementational advantage of this method is that
it only requires the homography between planes, and not the point correspondences

486

19 Auto-Calibration

Condition

Unknown but constant internal parameters

Constant known skew and aspect ratio. Constant unknown principal point
and focal length

All internal parameters known except varying focal length

Varying focal length, all other internal
parameters ﬁxed but unknown

dof(v)

views

5

3

m

m + 4

5

4

4

8

Table 19.5. The number of views required for calibration from a plane under various conditions.
Calibration is (in principle) possible if 2m ≥ v + 4.

arising from 3-space points that are generally required to estimate multiple view ten-
sors, such as the fundamental matrix. The matching transformation between planes is
a much simpler, stabler and accurate computation because of the constrained nature of
the inter-image transformation which is point-to-point. The method of algorithm 4.6-
(p123) may be used to estimate this transformation between two images. Alternatively,
correlation-based methods that estimate the parametrized homography directly from
image intensity may be used.

Including additional information. If additional information is available on the plane
or the motion then the complexity of auto-calibration using scene planes may be re-
duced. For example, if the vanishing line of the imaged plane can be determined then
only two parameters are required to specify the circular points since the imaged circu-
lar points lie on the vanishing line. Indeed if the plane provides sufﬁcient information
to estimate its imaged circular points directly (such as a square grid) then the problem
reduces to that of calibration from scene constraints discussed in section 8.8(p223).

Similarly constraints on the motion may be used to simplify the problem. One partic-
ular example is where the rotation axis describing the camera motion is parallel to the
scene plane normal. In this case the imaged circular points may be computed directly
from the ﬁxed points of the homography and two linear constraints placed on ω. This
situation is discussed further in Note (vii) at the end of the chapter. An example of such
a motion is planar motion which is discussed in detail in the following section.

19.8 Planar motion

A case of some practical importance is that of a camera moving in a plane and rotating
about an axis perpendicular to that plane. This is the case for a roaming vehicle moving
on a ground plane, with a camera ﬁxed with respect to the vehicle body. In this case,
the camera must move in a plane parallel to the (horizontal) ground, and as the vehicle
turns, the camera will rotate about a vertical axis. It is not assumed that the camera is
pointing horizontally, or is in any other particular orientation with respect to the vehicle.
However, we assume constant internal calibration for the camera. The constrained
nature of the motion makes the calibration task signiﬁcantly simpler.

19.8 Planar motion

487

It will be shown that given three or more images from a sequence of planar motions
an afﬁne reconstruction may be computed. To do this, we need to determine the plane
at inﬁnity. This will be done by identifying three points on the plane at inﬁnity, thereby
deﬁning the plane. These points will be identiﬁed as being ﬁxed points in the sequence
of images.

Fixed image points under planar motion. According to section 3.4.1(p77), any
rigid motion (for instance the camera motion) can be interpreted as a rotation about
screw axis along with a translation along the direction of the axis. For a planar motion,
the rotation axis is perpendicular to the plane of motion, and the translational part of
the screw motion is zero. Think of the vehicle as being swung horizontally around the
screw axis. The position of the screw axis with respect to the camera remains ﬁxed, and
so it will constitute a line of ﬁxed points in the image. If a second motion takes place
about a different axis, then the image of the second axis will be ﬁxed in the second pair
of images. The images of the two axes will in general be different, but will intersect in
the image of the point where the two screw axes meet. Since the two axes are vertical,
they are perforce parallel, and so will meet at their common direction on the plane at
inﬁnity. This direction projects into the images at the intersection of the images of the
screw axes, which is the vanishing point of the screw axes direction. This image point
is called the apex. We now have one ﬁxed point over the views, and this will be used
to determine one point on the plane at inﬁnity in a projective reconstruction.

As we have seen the image of the screw axis is a line of ﬁxed points for an image
pair. There is also a ﬁxed line which is identiﬁable from a pair of images as follows.
Because the plane of motion of the camera (which will be called the ground plane) is
ﬁxed, the set of points in the plane is mapped to the same line in all the images. This
line is called the horizon line and is the vanishing line of the ground plane. Since each
of the cameras lie on the ground plane their epipoles must all lie on the horizon line.
Unlike the image of the screw axis, the horizon line is a ﬁxed line, but not a line of
ﬁxed points.

Although the horizon line is not ﬁxed pointwise, it contains two points that are ﬁxed
in the image pair, namely the image of the two circular points on the ground plane.
These circular points are the intersection of the absolute conic with the ground plane.
Since the image of the absolute conic is ﬁxed under rigid motion, and the image of
the ground plane is ﬁxed, the images of two circular points must be ﬁxed. In fact,
they will be ﬁxed in all images from the planar motion sequence. This is illustrated in
ﬁgure 19.5.

So far we have described the ﬁxed points of the motion sequence. Computing these
ﬁxed points is equivalent to afﬁne reconstruction, since we can back-project from the
ﬁxed image points to ﬁnd the corresponding 3D points on the plane at inﬁnity. Al-
though the apex may be computed from two views; it requires three views to compute
the imaged circular points.

Computing the ﬁxed points. The set of points in space that map to the same image
point in two images is called the horopter. In general, the horopter is a twisted cubic,

488

19 Auto-Calibration

v

image of screw axis

c
i

e

e

/

a

horizon

c

j

image

v

b

imaged screw axes

horizon

image

Fig. 19.5. Fixed image entities for planar motion. (a) For two views the imaged screw axis is a line
of ﬁxed points in the image under the motion. The horizon is a ﬁxed line under the motion. The epipoles
e, e(cid:1)
and imaged circular points of the ground plane ci, cj lie on the horizon. (b) The relation between
the ﬁxed lines obtained pairwise for three images under planar motion. The image horizon lines for each
pair are coincident, and the imaged screw axes for each pair intersect in the apex v. All the epipoles lie
on the horizon line.

but in the case of planar motion it degenerates to a line (the screw axis) and a conic
on the ground plane. The image of the horopter is the conic deﬁned by F + FT, where
F is the fundamental matrix (see section 9.4(p250)). In the case of planar motion this
will be a degenerate conic consisting of two lines, the image of the screw axis and the
horizon line (see ﬁgure 9.11(p253)). By decomposing the conic, these two lines are
determined. From three images we can compute the image of the horopter for each of
three pairs, and thereby obtain three sets of horizon lines and imaged axes. The horizon
line will be a common component of these sets, and the other components (the images
of the screw axes) will intersect in the image at the apex.

Now we turn to computing the circular points. It is useful to understand the geom-
etry of a pair of horopters corresponding to two pairs of images. Let C12 be a conic
representing the portion of the horopter for images 1 and 2, lying in the ground plane.
This conic will pass through the two circular points, the two camera centres and the
intersection of the screw axis with the ground plane. Since the conic contains the cir-
cular points it is a circle. Let C23 be the corresponding circle deﬁned from images 2
and 3. The two circular points and the centre of the second camera will lie on both
circles. Since two conics intersect in general in four points, there must be a further
(real) intersection point. However, this can be discarded, since the points of interest are
the two complex intersection points, namely the circular points on the ground plane.

In implementations, authors have chosen different ways of ﬁnding the two circular
points. In [Armstrong-96b] the method is based on ﬁnding ﬁxed lines in three views
through the apex using the trifocal tensor. In [Faugeras-98, Sturm-97b] the method in-
volves computing the trifocal tensor of a 1D camera, applied to imaging the 2D ground
plane to a 1D image. In both cases the positions of the circular points on the horizon
are obtained as the solution of a cubic in one variable.

The main steps of this afﬁne calibration method are summarized in algorithm 19.4.

19.8 Planar motion

489

Objective

Given three (or more) images acquired by a camera with constant internal parameters under-
going planar motion compute an afﬁne reconstruction.

Algorithm

The trifocal tensor may be computed by, e.g. algorithm 16.4(p401).

(i) Compute a projective reconstruction. from the trifocal tensor T for the three views.
(ii) Compute pairwise fundamental matrices from T . See algorithm 15.1(p375). De-
compose the symmetric part of each fundamental matrix into two lines, a horizon and
the image of the screw axes. See section 9.4.1(p252).

(iii) Compute the apex. Intersect the three imaged screw axes to determine the apex v.
(iv) Compute the horizon for the triplet. Obtain the six epipoles from the three funda-
mental matrices, and determine the horizon by an orthogonal regression ﬁt to these
epipoles.

(v) Compute the imaged circular points. Compute the position of the imaged circular

(vi) Compute the plane at inﬁnity. Triangulate points on the plane at inﬁnity from the
for each of v, ci, cj. This determines

points on the horizon (see text) ci, cj.
corresponding image points x ↔ x(cid:1)
three points on π∞, and hence determines the plane.

, with x = x(cid:1)

(vii) Compute an afﬁne reconstruction. Rectify the projective reconstruction using the

computed π∞ as in algorithm 19.2.

Algorithm 19.4. Afﬁne calibration for planar motion.

Metric reconstruction. Once the apex and two circular points are found, the plane
at inﬁnity is computed, and the inﬁnite homographies between the images can be com-
puted. Calibration and metric reconstruction now proceeds in the usual way. How-
ever, it must be noted that the constrained nature of the motion means that there is
a one-parameter family of solutions for the calibration because all the camera rota-
tions are about the same axis. We have the sort of calibration ambiguity considered
in section 19.5.3. It is necessary to make an assumption about internal calibration in
order to ﬁnd a unique result. If the y-axis of the camera is parallel with the rotation
axis (which may be true in a practical situation), then we have seen that the zero-skew
constraint is not sufﬁcient. The best plan is to enforce a zero-skew and known aspect
ratio constraint (e.g. if the pixels are square).

Example 19.11. Metric reconstruction for planar motion.
Figure 19.6(a) shows four of seven images of a planar motion sequence. For this se-
quence the elevation angle of the camera is approximately 20
. The computed im-
aged screw axes and horizon lines using all possible pairs are shown in ﬁgure 19.6(b),
with the resulting estimated apex and horizon line. The positions of the imaged cir-
cular points were estimated as x = 104 ± 362i, y = −86 ∓ 2i. Assuming an as-
pect ratio of 1.1, the internal parameters of the calibration matrix K were computed
as αx = 330, αy = 363, x0 = 123, y0 = 50. The accuracy of the metric recon-
struction was assessed by measuring metric invariants. Typical results are shown in
(cid:2)
ﬁgure 19.6(c).

◦

490

19 Auto-Calibration

a

5000

4000

3000

2000

1000

0

−1000

−2000

−3000

−4000

)
s
l
e
x
i
p
(

−5000

−5000

0

(pixels)

b

5000

c

Fig. 19.6. Planar motion sequence. (a) Four images (of seven used) acquired from a camera mounted
on a vehicle moving on a plane. (b) The computed epipoles (×), horizon lines (grey solid), and imaged
screw axes (grey dashed) for all image pairs. (c) Euclidean invariants measured in the metric recon-
struction the right angle is measured as 89◦
, the ratio of the non-parallel lengths is measured as 0.61
(compared to the approximate veridical value of 0.65).

19.9 Single axis rotation – turntable motion

In this section we discuss auto-calibration of single axis motions where the relative
motion between scene and cameras is equivalent to a rotation about a single ﬁxed axis.
This is a specialization of the planar motion case of section 19.8 where here the screw
axes for each motion are coincident. This situation occurs, for example, in the case of
a static camera viewing an object rotating on a turntable. A second example is that of
a camera rotating about a ﬁxed axis (offset from its centre). A third example is that of
a camera viewing a rotating mirror.

We will consider here turntable motion and for ease of discussion assume that the
axis is vertical so that the motion occurs in a horizontal plane. Again it is not assumed
that the camera is pointing horizontally, or is in any other particular orientation with
respect to the axis. It is assumed that the internal calibration of the camera is constant.
The ﬁxed image points under a sequence of single axis rotation are those of planar
motion described in section 19.8, and shown in ﬁgure 19.5(a), with the addition that
the imaged screw axis is a line of ﬁxed points. The constraint of ﬁgure 19.5(b) is not
available here as all the imaged screw axes are coincident. The consequence is that
it is not possible to determine the apex v directly, and only the two circular points on
π∞ may be recovered from imaged ﬁxed points. This means that the reconstruction,
in the absence of constraints on the internal parameters, is not afﬁne, but a particular

19.9 Single axis rotation – turntable motion

491

parametrized projective transformation. Metric structure is known in the horizontal
planes (since the circular points of these planes is known) but there is a 1D projective
transformation in the vertical (Z) direction. The resulting ambiguity is the transforma-
tion XP = HXE where

 1 0 0 0

0 1 0 0
0 0 γ 0
0 0 δ 1



H =

(19.30)

and γ and δ are scalars which determine the intersection of the Z axis with π∞ and
the relative scaling between the horizontal and vertical directions. An example of the
projective transformations represented by family of mappings is shown in ﬁgure 1.4-
(p16).

Computing the ﬁxed points. One way to proceed is to determine the imaged circular
points directly. As is evident from ﬁgure 19.7(b) the point tracks are ellipses which are
images of circles. In 3-space these circles lie in parallel horizontal planes and intersect
in the circular points on π∞. In the image, ellipses may be ﬁtted to these tracks, and the
common intersection points of the image conics are the (complex conjugate) imaged
circular points. The 3D circular points may then be determined by triangulation from
two or more views. This is the approach taken by [Jiang-02].
An alternative more algebraic way to proceed is to model the camera matrices as
Pi = H3×3[RZ(θi) | t] where

 h1 h2 h3



 cos θi

sin θi

Pi =

− sin θi
cos θi

0

0

0
0
1

t
0
0



(19.31)

with hk the columns of H3×3. This division of the internal and external parameters
means that H3×3 and t are ﬁxed over the sequence, and only the angle of rotation, θi,
about the Z axis varies for each camera Pi.

Given this parametrization, the estimation problem can then be precisely stated as
determining the common matrix H3×3 and the angles θi in order to estimate the set of
cameras Pi for the sequence. Thus a total of 8 + m parameters must be estimated for m
views, where 8 is the number of degrees of freedom of the homography H3×3. This is a
considerable saving over the 11m that would be required for a projective reconstruction
of a general motion sequence.

For single axis motion the fundamental matrix has the special form described in
section 9.4.1(p252), and writing the matrix in terms of the camera matrices (19.31)
gives

F = α[h2]× + β

(h1 × h3)(h1 × h2)T + (h1 × h2)(h1 × h3)T

(cid:15)

(cid:16)

which means that the columns of H3×3 are partially determined once the fundamental
matrix is computed. This is the approach taken by [Fitzgibbon-98b] where a funda-
mental matrix of the special form is ﬁtted to point correspondences (see section 11.7.2-
(p293)). It may be shown that from 3 or more views the ﬁrst two columns h1, h2 of

492

19 Auto-Calibration

a

b

e
m
a
r
F

20

40

60

500

1000

1500
c

2000

2500

3000

Fig. 19.7. Dinosaur sequence and tracking: (a) six frames from a sequence of 36 of a dinosaur rotating
on a turntable (Image sequence courtesy of the University of Hannover [Niem-94]). (b) A subset of the
point tracks – only the 200 tracks which survived for longer than 7 successive views are shown. (c) Track
lifetimes: Each vertical bar corresponds to a single point track, extending from the ﬁrst to last frame in
which that point was seen. The horizontal ordering is according to where the point ﬁrst appeared in the
sequence. The measurement matrix is relatively sparse, and few points survive longer than 15 frames.

H3×3 and the angle θi can be fully determined, but h3 is only determined up to the
two-parameter family corresponding to the ambiguity of (19.30).

Metric reconstruction. The two parameter ambiguity in the reconstruction given by
(19.30) can be resolved by providing additional information on the internal parameters,
for example that the pixels are square. However, if the camera is horizontal with im-
age y axis parallel to the rotation axis, then square pixels only provides one additional
constraint (from the aspect ratio, as the skew zero constraint does not provide an addi-
tional constraint). In this case further information on the camera is required (e.g. the y
coordinate of the principal point) or the aspect ratio of the scene may be used.

Example 19.12. Reconstruction from a turntable sequence
Figure 19.7 shows frames from a sequence of a model dinosaur rotating on a turntable,
and the resulting image point tracks. Feature extraction is performed on the luminance
component of the colour signal. The projective geometry of the turntable motion is de-

19.10 Auto-calibration of a stereo rig

493

a

b

Fig. 19.8. Dinosaur: (a) 3D scene points (about 5000) and camera positions for the Dinosaur se-
quence.
(b) The automatic computation of a 3D graphical model for this sequence is described in
[Fitzgibbon-98b].

termined from these tracks (and no other information) and the resulting reconstruction
of cameras and 3D points is shown in ﬁgure 19.8. Effectively the camera circumnav-
igates the object. A 3D texture mapped graphical model may then be computed, in
principle, by back-projecting cones deﬁned by the dinosaur silhouette in each frame,
(cid:2)
and intersecting the set of cones to determine the visual hull of the 3D object.

19.10 Auto-calibration of a stereo rig

In this section we describe a stratiﬁed method for calibrating a “ﬁxed” two-camera
stereo rig. Fixed here means that the relative orientation of the cameras on the rig
is unchanged during the motion, and the internal parameters of each camera are also

494

19 Auto-Calibration

unchanged. It will be shown that from a single motion of the rig the plane at inﬁnity is
determined uniquely.
Suppose a ﬁxed stereo rig undergoes a general motion. The projective structure of
the scene can be obtained before (X) and after (X(cid:2)
are
two projective reconstructions of the same scene they are related by a 4 × 4 projective
transformation HP, as

) the motion. Since X and X(cid:2)

X(cid:2)

= HPX.

However, the actual motion of the rig is Euclidean, and it follows (see below) that
the homography HP is conjugate to the Euclidean transformation representing the mo-
tion. Conjugacy is the key result because under a conjugacy relation ﬁxed entities are
mapped to ﬁxed entities. Consequently the ﬁxed entities of the Euclidean motion (in
particular the plane at inﬁnity) can be accessed from the ﬁxed entities of the projective
motion represented by HP.

Conjugacy relation. Suppose XE represents a point in 3-space in a Euclidean coor-
dinate frame attached to the rig, and X(cid:2)
E represents the same point after the motion of
the rig. Then the points are related as

X(cid:2)
E = HEXE

(19.32)
where HE is a non-singular 4 × 4 Euclidean transformation matrix that represents the
rotation and translation of the rig. Suppose also that the point is represented in a pro-
jective coordinate frame attached to the rig (and which is obtained by a projective
reconstruction); then

XE = HEPX X(cid:2)

E = HEPX(cid:2)

(19.33)
where HEP is a non-singular 4 × 4 matrix that relates projective to metric structure. An
essential point to note here is that the two projective reconstructions, before and after
the camera must be in the same projective coordinate frame, in other words, the same
pair of cameras matrices must be used before and after.

From (19.32) and (19.33) if follows that
HP = H−1

EP HE HEP

(19.34)

so that HP is conjugate to a Euclidean transformation. There are two important proper-
ties of this conjugacy relation:

(i) HP and HE have the same eigenvalues.
(ii) If E is an eigenvector of HE then the corresponding eigenvector of HP, with the
same eigenvalue, is (H−1
EP E), i.e. the eigenvectors of HE are mapped to the eigen-
vectors of HP by the point transformation (19.33). This follows from (19.34),
for if HEE = λE then HEPHPH−1
EP gives the
desired result.

EP E = λE, and pre-multiplying by H−1

19.10 Auto-calibration of a stereo rig

495

Fixed points of a Euclidean transformation. Consider the Euclidean transformation
represented by the matrix

(cid:17)

(cid:18)

HE =

t
R
0T 1

=

 cos θ − sin θ 0 0

cos θ

sin θ

 .

0
0

0
0

0 0
1 1
0 1

This is a rotation by θ about the Z-axis together with a unit translation along the Z-
axis (it is a general screw motion). The eigenvectors of HE are the ﬁxed points un-
der the transformation (refer to section 2.9(p61)).
In this case the eigenvalues are
{eiθ, e

−iθ, 1, 1} and the corresponding eigenvectors of HE are

 1

i
0
0

E1 =

 E2 =

 1−i

 E3 =

0
0

 0

0
1
0

 E4 =

 .

 0

0
1
0

All the eigenvectors lie on π∞. This means that π∞ is ﬁxed as a set, but is not a
plane of ﬁxed points. The eigenvectors E1 and E2 are the circular points for planes
perpendicular to the Z (rotation) axis. The other two (identical) eigenvectors E3 and E4
are the direction of the rotation axis.

E

E

E . The eigenvectors of H−T

Computing π∞.
If the point transformation matrix is HE then from (3.6–p68) the
plane transformation matrix is H−T
are the ﬁxed planes under
the motion. The matrix H−T
also has two equal, unit eigenvalues and a single eigen-
vector corresponding to these which is the plane π∞ as may easily be veriﬁed. The
eigenvectors of H−T
P , in the same manner as the mapping of
eigenvectors of HE to those of HP described above. Consequently, π∞ in the projective
reconstruction is the eigenvector corresponding to the (double) real eigenvalue of H−T
P .
Thus,
• π∞ may be computed uniquely as the real eigenvector of H−T
more simply, as the real eigenvector of HT
P .

are mapped to those of H−T

P , or equivalently, and

E

We observe here that although the real eigenvalue has algebraic multiplicity of two,
its geometric multiplicity (in the case of non-planar motions) is one. This is what
enables us to ﬁnd the plane at inﬁnity.

The procedure for afﬁne calibration is summarized in algorithm 19.5.

Metric calibration and ambiguities. Once π∞ is identiﬁed, the metric calibration
may proceed as described in the stratiﬁed algorithm 19.2(p479). Since the rig is ﬁxed,
the parameters of the left camera are unchanged during the motion (and similarly for
the right camera). From a single motion the internal parameters of each camera are
determined up to the one-parameter family resulting from a single rotation as described
in section 19.5.3.

As usual the ambiguity from a single motion is removed by additional motions or

496

Objective

19 Auto-Calibration

Given two (or more) stereo pairs of images acquired by a ﬁxed stereo rig undergoing general
motions (i.e. both R and t are non-zero, and t not perpendicular to the axis of R), compute an
afﬁne reconstruction.

Algorithm

(i) Compute an initial projective reconstruction X: Using the ﬁrst stereo pair compute a
projective reconstruction (PL, PR,{Xj}) as described in chapter 10. This involves com-
puting the fundamental matrix F and point correspondences between the images of the ﬁrst
pair xL

j ↔ xR

j , e.g. use algorithm 11.4(p291).
(ii) Compute a projective reconstruction X(cid:1)

after the motion: Compute correspondences
j ↔ x
(cid:1)R
(cid:1)L
between the images of the second stereo pair x
j . Since both the internal and
relative external parameters of the cameras are ﬁxed, the second stereo pair has the same
fundamental matrix F as the ﬁrst. The same camera matrices PL, PR are used for triangu-
lating points X(cid:1)
in the second
stereo pair.

j in 3-space from the computed correspondences x

j ↔ x
(cid:1)L

(cid:1)R
j

: Compute correspondences between
j ↔ x
(cid:1)L
the left images of the two stereo pairs xL
j (e.g. again use algorithm 11.4(p291)). This
establishes correspondences between the space points Xj ↔ X(cid:1)
j. The homography HP
may be estimated linearly from ﬁve or more of these 3-space point correspondences, and
then the estimate reﬁned by minimizing a suitable cost function over HP. For example,
j)2) minimizes the distance between the
j)2 + d(xr
minimizing
measured and reprojected image points.

j, PRHX(cid:1)

j , PLHX(cid:1)

(cid:27)

j(d(xL

(iii) Compute the 4×4 matrix HP which relates X to X(cid:1)

(iv) Afﬁne reconstruction: Compute π∞ from the real eigenvector of HT

P and thence an afﬁne

reconstruction.

Algorithm 19.5. Afﬁne calibration of a ﬁxed stereo rig.

by supplying additional constraints, such as that the pixels are square.
If there are
additional motions then an improved estimate of π∞ may also be computed. The
outcome of metric calibration is the complete calibration of the rig (i.e. the relative
external orientation of the cameras and their internal parameters).

Planar motion.
In the special case of orthogonal (planar) motion, where the transla-
tion is orthogonal to the rotation axis direction, the space of eigenvectors corresponding
to the repeated (real) eigenvalue is two-dimensional. Consequently, π∞ is determined
only up to a one-parameter family. We are therefore unable to ﬁnd the plane at inﬁnity
uniquely (this is examined in detail in example 3.8(p81)). The ambiguity may be re-
solved by a second orthogonal motion about an axis with a different direction from the
ﬁrst.

Example 19.13. Auto-calibration from two stereo pairs
Figure 19.9(a)(b) shows the two stereo pairs used for the afﬁne calibration of the stereo
rig following the procedure of algorithm 19.5. The accuracy of the calibration is as-
sessed by computing a vanishing point in the right image in two ways: ﬁrst, as the
intersection of imaged parallel lines; second, by determining the corresponding van-
ishing point in the left image (from images of the same parallel lines), and mapping
this vanishing point to the right image using the inﬁnite homography computed from

19.11 Closure

497

a

b

c

d

e

Fig. 19.9. Auto-calibration of a stereo rig. The input stereo pairs before (a) and after (b) the motion of
the rig. The stereo rig moves left by about 20 cm, pans by about 10◦
and changes elevation by about 10◦
.
The accuracy of the computed H∞ is assessed on another stereo pair acquired by the same rig as follows:
In (c), the left image (of a stereo pair), a vanishing point is computed by intersecting the imaged sides of
the table (which are parallel in the scene). In (d), the right image (of a stereo pair), the corresponding
vanishing point is computed. The white square (near the line intersection) is the vanishing point from
the left image mapped to the right using the computed H∞. In the absence of error the points should be
identical. (e) Following metric calibration the computed angle between the desk sides (shown in white)
from the 3D reconstruction is 90.7◦

, in very good agreement with the veridical value.

the eigenvector of HT
P . The discrepancy between the vanishing points is a measure of
the accuracy of the computed H∞. The metric calibration uses the zero skew constraint
to resolve the one-parameter ambiguity. Angles in the resulting metric reconstruction
(cid:2)
are accurate to within 1

◦

.

19.11 Closure

Research in the area of auto-calibration is still quite active, and better methods than
those described in this chapter may yet be developed. There is still a lack of closed form
solutions from multiview tensors, and of algorithms to automatically detect critical
motion sequences (see below).

Critical motion sequences.
It has been seen in this chapter that for certain classes
of motion it is not possible to completely determine the rectifying homography H. The
resulting reconstruction is then at some level between metric and projective. For ex-
ample, for constant internal parameters in the case of planar motion there is a one-
parameter scaling ambiguity parallel to the rotation axis; and for pure translation under
constant internal parameters the reconstruction is afﬁne. Sequences of camera motions
for which such ambiguities arise are termed “Critical motion sequences” and have been
systematically classiﬁed by Sturm [Sturm-97a, Sturm-97b] in the case of constant in-
ternal parameters. This classiﬁcation has been extended to more general calibration
constraints, such as varying focal lengths [Pollefeys-99b, Sturm-99b]. For recent work
see [Kahl-99, Kahl-01b, Ma-99, Sturm-01].

498

19 Auto-Calibration

Recommendations.
It may seem that auto-calibration offers a complete solution to
metric reconstruction. Calibrated cameras are not necessary and we can do with con-
straints as weak as the zero-skew constraint on the camera. Unfortunately one must be
wary of putting complete trust in auto-calibration. Auto-calibration can work well in
the right circumstances, but used recklessly it will fail. Several speciﬁc recommenda-
tions can be made.

(i) Take care to avoid ambiguous motion sequences. It has been seen that calibra-
tion degeneracies occur if the motion is too restricted, such as being about a
single axis. The motion should not be too small, or cover too small a ﬁeld of
view. Auto-calibration often comes down to estimating the inﬁnite homogra-
phy, the effects of which are not apparent on small ﬁelds of view.

(ii) Use as much information as you have. Although it is possible to calibrate from
minimal information such as zero skew, this should be avoided if other infor-
mation is available. For instance the known aspect ratio constraint should be
used if it is valid, as should the knowledge of the principal point. Even if the
values are known only imprecisely, this information can be incorporated into a
linear auto-calibration method by including an equation, but with low weight.
(iii) This recommendation relates to bundle adjustment as well. Generally it is best
to ﬁnish off with a bundle adjustment. In doing this, it is recommended that the
internal parameters of the camera not be left to ﬂoat unbounded. For instance,
even if the principal point is not known exactly, it is usually known within some
reasonable bounds (it is not close to inﬁnity for instance). Similarly, aspect
ratio normally lies between 0.5 and 3. This knowledge should be incorporated
in a bundle adjustment by adding further constraints to the cost function, with
small weights (standard deviations) if necessary. This can give an enormous
improvement in results where auto-calibration is poorly conditioned (and hence
unstable) by preventing the solution from wandering off into remote regions of
parameter space in quest of a minor and insigniﬁcant improvement in the cost
function.

(iv) Methods that use restricted motions usually are more reliable than those that
allow general motion. For instance the methods that involve a rotating but non-
translating camera are generally much more reliable than general motion meth-
ods. The same is true of afﬁne reconstruction from a translational motion.

19.11.1 The literature
The idea of auto calibrating a camera originated in Faugeras et al.
[Faugeras-92a]
where the Kruppa equations were used. The early papers considered the case of con-
stant internal parameters.
[Mohr-93] investigated
bundle-adjustment like methods for more than two views.

[Hartley-94b] and Mohr et al.

The afﬁne reconstruction solution for the case of pure translation was given by
Moons et al.
[Moons-94], and was extended to a combination of pure trans-
lation followed by rotation for a full metric reconstruction by Armstrong et al.
[Armstrong-94]. The case of auto-calibration for a camera rotating about its centre

19.11 Closure

499

was given by [Hartley-94a]. The modulus constraint was ﬁrst published by Pollefeys
et al. [Pollefeys-96].

The original method for auto-calibration of a stereo rig was given by Zisserman
[Zisserman-95b], with alternative parametrizations given in Devernay and
et al.
Faugeras [Devernay-96], and Horaud and Csurka [Horaud-98]. The special case of
planar motion of a stereo rig is covered in [Beardsley-95b, Csurka-98]. For planar
motion of a monocular camera the original method was published by Armstrong et al.
[Armstrong-96b], and an alternative numerical solution was given in Faugeras et al.
[Faugeras-98].

In more recent papers less restrictive constraints than constant internal parameters
have been investigated. A number of “existence proofs” have been given: Heyden
and ˚Astr¨om [Heyden-97b] showed that metric reconstruction is possible knowing only
skew and aspect ratio, and [Pollefeys-98, Heyden-98] showed that skew-zero alone
was sufﬁcient.

Triggs [Triggs-97] introduced the absolute (dual) quadric as a numerical device for
formulating auto-calibration problems, and applied both linear methods and sequential-
quadratic programming to solve for Q∗
∞. Pollefeys et al. [Pollefeys-98] showed that
computations based on Q∗
∞ could be used to compute metric reconstructions for varying
focal length under general motion for real image sequences.

For the case of a rotating camera de Agapito et al.

[DeAgapito-98] gave a non-
linear solution for varying internal parameters based on the use of the DIAC. This was
modiﬁed in [DeAgapito-99] to an IAC-based linear method.

19.11.2 Notes and exercises

(i) [Hartley-92a] ﬁrst gave a solution for the extraction of focal lengths from the
fundamental matrix, but the algorithm given there is unwieldy. A simple elegant
formula is given in [Bougnoux-98]:

α2 = −p(cid:2)T[e(cid:2)

]×˜IFppTFTp(cid:2)
p(cid:2)T[e(cid:2)]×˜IF˜IFTp(cid:2)

(19.35)

are the principal points in the
(cid:2)2

where ˜I is the matrix diag(1, 1, 0) and p and p(cid:2)
two images. Unit aspect ratio and zero skew are assumed. The formula for α
is given by reversing the roles of the two images (and transposing F).
Note that the ﬁnal step of the algorithm is to take a square root. It is assured
(cid:2)2 as computed by (19.35) are positive, given good data and a
that α2 and α
good guess of the principal point. However in practice this does not always
pertain, and negative values can result. This is the same problem as mentioned
previously in section 19.3.5(p468). In addition, as ﬂagged in [Newsam-96], the
method has an intrinsic degeneracy when the principal rays of the two cameras
meet in space, in which case it is impossible to compute the focal lengths inde-
pendently. This occurs when both cameras are trained on the same point, quite
a common occurrence.
A further degeneracy occurs when the plane deﬁned by the baseline and the

500

19 Auto-Calibration

principal ray of one camera is perpendicular to the plane deﬁned by the baseline
and principal ray of the other camera. Generally speaking, our opinion is that
this method is of doubtful value as a means of computing focal lengths.
(ii) Show that if the internal parameters are constant then the constraints on Q∗
∞
obtained from two views (19.6–p462) are equivalent to the Kruppa equa-
tions (19.18–p471). Hint, from (9.10–p256) the cameras may be chosen as
P1 = [I | 0], P2 = [[e(cid:2)

]×F | e(cid:2)

].

(iii) Show from (19.21–p473) that in the case of a camera translating with constant
internal parameters and without rotating, then the plane at inﬁnity may be com-
puted directly from a projective reconstruction.

(iv) The inﬁnite homography relation (19.24–p476) may be derived in two lines
−1, in section 13.4(p338). This may
simply from the deﬁnition Hij∞ = KiRij(Kj)
be rearranged as Hij∞Kj = KiRij. Eliminating the rotation using orthogonality as
RijRij T = I gives Hij∞(KjKj T)Hij∞T = (KiKiT).
(v) Under HE, points on π∞ (i.e. with X4 = 0) are mapped to points on π∞ by the
3× 3 homography x∞ (cid:6)→ Rx∞. Under this point transformation a conic on π∞
maps according to result 2.13(p37) C (cid:6)→ R−TCR−1 = RCRT. The absolute conic
Ω∞ is ﬁxed since RIRT = I. Now, denote by a the direction of the rotation axis,
so that Ra = 1a. The (degenerate) point conic aaT is also ﬁxed. It follows that
there is a pencil of ﬁxed conics C∞(µ) =I + µaaT under the mapping since

R(Ω∞ + µaaT)RT = RIRT + µRaaTRT

= Ω∞ + µaaT.

The scalar µ parametrizes the pencil. This shows that under a particular simi-
larity there is a one-parameter family of ﬁxed conics on π∞. However, it is the
case that Ω∞ is the only conic ﬁxed under any similarity.

(vi) A further calibration ambiguity exists for a common type of robot head, namely
a pan-tilt (or alt-azimuth) head.
In [DeAgapito-99] it was shown that since
the camera may rotate about its X or Y axes its set of orientations form only
a 2-parameter family, rather than a 3-parameter family of general rotations.
This limitation causes an ambiguity in the aspect ratio αx of the camera, and
consequently of x0 as well.

(vii) The method of auto-calibration from planes is generally non-linear. However,
for special motions linear constraints on ω can be obtained. Suppose we have
two images of a plane that induces a planar homography H between views, and
imagine that the motion of the camera relative to the plane is a general screw
motion but with the screw axis parallel to the plane normal.
Consider the action of this screw motion on the plane. Since this action (a ro-
tation about the plane’s normal and a translation) does not change the plane’s
orientation, its line of intersection with the plane at inﬁnity is unchanged (as
a set). The absolute conic is ﬁxed (as a set) under Euclidean motion. Con-
sequently, the plane’s intersection with the absolute conic, which deﬁnes the
circular points for the plane, is also unchanged under the motion.

19.11 Closure

501

Consider now applying the action to the camera viewing the plane. Since the
two circular points are ﬁxed (as 3-space points) they have the same image be-
fore and after the motion. As the homography H maps points on the plane
between images, the imaged circular points must correspond to two of its ﬁxed
points (see section 2.9(p61)) and can thus be determined directly from the ho-
mography. Each circular points places a linear constraint on ω. Further details
of this method are given in [Knight-03].

20

Duality

It has been known since the work of Carlsson [Carlsson-95] and Weinshall et al.
[Weinshall-95] that there is a dualization principle that allows one to interchange the
role of points being viewed by several cameras and the camera centres themselves. In
principle this implies the possibility of dualizing projective reconstruction algorithms
to obtain new algorithms. In this chapter, this theme is developed to outline an ex-
plicit method for dualizing any projective reconstruction algorithm. At the practical
implementation level, however, it is shown that there are difﬁculties which need to be
overcome in order to allow application of this dualization method to produce working
algorithms.

20.1 Carlsson–Weinshall duality

Let E1 = (1, 0, 0, 0)T, E2 = (0, 1, 0, 0)T, E3 = (0, 0, 1, 0)T and E4 = (0, 0, 0, 1)T
form part of a projective basis for IP3. Similarly, let e1 = (1, 0, 0)T, e2 = (0, 1, 0)T,
e3 = (0, 0, 1)T and e4 = (1, 1, 1)T be a projective basis for the projective image plane
IP2.

Now, consider a camera with matrix P. We assume that the camera centre C does
not sit on any of the axial planes, that is none of the four coordinates of C is zero.
In this case, no three of the points PEi for i = 1, . . . ,4 are collinear in the image.
Consequently, one may apply a projective transformation H to the image so that ei =
HPEi. We assume that this has been done, and henceforth denote HP simply by P. Since
PEi = ei, the form of the matrix P is

 a

P =

 .

b

d
d
c d

Deﬁnition 20.1. A camera matrix P is called a reduced camera matrix if it maps Ei to
ei for each i = 1, . . . ,4 . In other words ei = PEi.

502

20.1 Carlsson–Weinshall duality
Now, for any point X = (X, Y, Z, T)T one veriﬁes that

 a

P =

 a

b

d
d
c d

Y
Z
T



 X
 =

 X

b

d
d
c d



 X

Y
Z
T

Notice the symmetry in this equation between the entries of the camera matrix and the
coordinates of the point. They may be interchanged as follows

 =

 aX + dT

bY + dT
cZ + dT

.
 a

b
c
d



 .

Y

T
T
Z T

503

(20.1)

(20.2)

The interchange of the roles of cameras and points may be interpreted as a form of du-
ality, which will be referred to as Carlsson–Weinshall duality, or more brieﬂy Carlsson
duality. The consequences of this duality will be explored in the rest of this chapter.

20.1.1 Dual algorithms
First of all, we will use it for deriving a dual algorithm from a given projective recon-
struction algorithm. Speciﬁcally, it will be shown that if one has an algorithm for doing
projective reconstruction from n views of m + 4 points, then there is an algorithm for
doing projective reconstruction from m views of n + 4 points. This result, observed by
Carlsson [Carlsson-95], will be made speciﬁc by explicitly describing the steps of the
dual algorithm.
We consider a projective reconstruction problem, which will be referred to as
P(m, n). It is the problem of doing reconstruction from m views of n points. We
denote image points by xi
j, which represents the image of the j-th object space point
in the i-th view. Thus, the upper index indicates the view number, and the lower index
represents the point number. Such a set of points {xi
} is called realizable if there are a
set of camera matrices Pi and a set of 3D points Xj such that xi
j = PiXj. The projective
reconstruction problem P(m, n) is that of ﬁnding such camera matrices Pi and points
} for m views of n points. The set of camera matrices and
Xj given a realizable set {xi
3D points together are called a realization (or projective realization) of the set of point
correspondences.
Let A(n, m + 4) represent an algorithm for solving the projective reconstruction
problem P(n, m + 4). An algorithm will now be exhibited for solving the projective
reconstruction P(m, n + 4). This algorithm will be denoted A∗
(m, n + 4), the dual of
the algorithm A(n, m + 4).
Initially, the steps of the algorithm will be given without proof. In addition, difﬁcul-
ties will be glossed over so as to give the general idea without getting bogged down in
details. In the description of this algorithm it is important to keep track of the range of
the indices, and whether they index the cameras or the points. Thus, the following may
help to keep track.

j

j

504

20 Duality

Views (i)

 m

xi
j

1 x2
x1
1
x1
x2
2
2

x2
n
xn+1
2

x1
n
xn+1
1
xn+2
1
xn+3
1
xn+4
1

n

4

xm
1
xm
2

xm
n
xn+1
m
xn+2
m
xn+3
m
xn+4
m

Points (j)

Points (j)

Views (i)

 m

x' i
j

x' 1
x' 1
2

1 x' 2
1
x' 2
2

x' 1
n
e1
e2
e3
e4

x' 2
n
e1
e2
e3
e4

n

4

x' m
1
x' m
2

x' m
n
e1
e2
e3
e4

T1 T2

Ti

Tm

Fig. 20.1. Left: Input to algorithm A∗(m, n + 4). Right: Input data after transformation.

• Upper indices represent the view number.
• Lower indices represent the point number.
• i ranges from 1 to m.
• j ranges from 1 to n.
• k ranges from 1 to 4.

The dual algorithm
Given an algorithm A(n, m + 4) the goal is to exhibit a dual algorithm A∗

(m, n + 4).

Input:
The input to the algorithm A∗
in m views. This set of points can be arranged in a table as in ﬁgure 20.1(left).

(m, n + 4) consists of a realizable set of n + 4 points seen

In this table, the points xi

receive special treatment.

n+k are separated from the other points xi

j, since they will

Step 1: Transform
The ﬁrst step is to compute, for each i, a transformation Ti that maps the points
xi
n+k, k = 1, . . . ,4 in the i-th view to the points ek of a canonical basis for projec-
tive 2-space IP2. The transformation Ti is applied also to each of the points xi
j to pro-
duce transformed points x(cid:2)i
j. The result is the transformed point array shown
in ﬁgure 20.1(right). A different transformation Ti is computed and applied to each
column of the array, as indicated.

j = Tixi

20.1 Carlsson–Weinshall duality

505

Points (i)

m

Views (j)

 n

^
^
1 x2
x1
1
^
^
x1
x2
2
2

^
x j
i

^
^
m x2
x1
m

^
x j
i

=

x' i
j

^
xn
1
^
xn
2

^
xn
m

Points (i)

Views (j)

 n

^
x j
i

^
^
1 x2
x1
1
^
^
x1
x2
2
2

^
^
m x2
x1
m
e1
e1
e2
e2
e3
e3
e4
e4

m

4

^
xn
1
^
xn
2

^
xn
m
e1
e2
e3
e4

Fig. 20.2. Left: Transposed data. Right: Transposed data extended by addition of extra points.

Step 2: Transpose
The last four rows of the array are dropped, and the remaining block of the array is
i = x(cid:2)i
transposed. One deﬁnes ˆxj
j . At the same time, one does a mental switch of
points and views. Thus the point ˆxj
i is now conceived as being the image of the i-th
point in the j-th view, whereas the point x(cid:2)i
j was the image of the j-th point in the i-th
view. What is happening here effectively is that the roles of points and cameras are
being swapped – the basic concept behind Carlsson duality expressed by (20.2). The
resulting transposed array is shown in ﬁgure 20.2(left).

Step 3: Extend
The array of points is now extended by the addition of four extra rows containing points
ek in all positions of the (m + k)-th row of the array, as shown in ﬁgure 20.2(right).
The purpose of this extension will be explained in section 20.1.2.

Step 4: Solve
The array of points resulting from the last step has m + 4 rows and n columns, and may
be regarded as the positions of m + 4 points seen in n views. As such, it is a candidate
for solution by the algorithm A(n, m + 4), which we have assumed is given. Essential
here is that the points in the array form a realizable set of point correspondences. Jus-
of cameras ˆPj and points (cid:23)Xi such that ˆxj
i = ˆPj(cid:23)Xi. In addition, corresponding to the last
tiﬁcation of this is deferred for now. The result of the algorithm A(n, m + 4) is a set
four rows of the array, there are points (cid:23)Xm+k such that ek = ˆPj(cid:23)Xm+k for all j.

Step 5: 3D transformation
Since the reconstruction obtained in the last step is a projective reconstruction, one may
transform it (equivalently, choose a projective coordinate frame) such that the points

506

(cid:23)Xm+k are the four points Ek of a partial canonical basis for IP3. The only requirement
is that the points (cid:23)Xm+k obtained in the projective reconstruction are not coplanar. This
At this point, one sees that ek = ˆPj(cid:23)Xm+k = ˆPjEk. From this it follows that ˆPj has

assumption is validated later.

20 Duality

the special form

ˆPj

=

 aj

bj

dj
dj
cj dj

(20.3)

 .

 .

Step 6: Dualize

Let (cid:23)Xi = (Xi, Yi, Zi, Ti)T, and ˆPj be as given in (20.3). Now deﬁne points

Xj = (aj, bj, cj, dj)T and cameras

 Xi

P(cid:2)i =

Yi

Ti
Ti
Zi Ti

Then one veriﬁes that

P(cid:2)iXj = (Xiaj + Tidj, Yibj + Tidj, Zicj + Tidj)T

= ˆPj(cid:23)Xi

= ˆxj
i
= x(cid:2)i

j

.

If, in addition, one deﬁnes Xn+k = Ek for k = 1, . . . ,4 , then P(cid:2)iXn+k = ek. It is then
evident that the cameras P(cid:2)i and points Xj and Xn+k form a projective realization of the
transformed data array obtained in step 1 of this algorithm.

Step 7: Reverse transformation
−1P(cid:2)i, and with the points Xj and Xn+k obtained in the
Finally, deﬁning Pi = (Ti)
previous step, one has a projective realization of the original data. Indeed, one veriﬁes

PiXj = (Ti)

−1P(cid:2)iXj = (Ti)

−1x(cid:2)i

j = xi

j

.

This completes the description of the algorithm. One can see that it takes place in

various stages.

(i) In step 1, the data is transformed into canonical image reference frames based

(ii) In steps 2 and 3 the problem is mapped into the dual domain, resulting in a dual

on the selection of four distinguished points.
problem P(n, m + 4).

(iii) The dual problem is solved in steps 4 and 5.
(iv) Step 6 maps the solution back into the original domain.
(v) Step 7 undoes the effects of the initial transformation.

20.1 Carlsson–Weinshall duality

507

20.1.2 Justiﬁcation of the algorithm
To justify this algorithm, one needs to be sure that at step 4 there indeed exists a solution
to the transformed problem. Before considering this, it is necessary to explain the
purpose of step 3, which extends the data by the addition of rows of image points ek,
and step 5, which transforms the arbitrary projective solution to one in which four
points are equal to the 3D basis points Ek.

The purpose of these steps is to ensure that one obtains a solution to the dual re-
construction problem in which ˆPj has the special form given by (20.3) in which the
camera matrix is parametrized by only 4 values. The dual algorithm is described in this
manner so that it will work with any algorithm A(n, m + 4) whatever. However, both
steps 3 and 5 may be eliminated if the known algorithm A(n, m + 4) has the capability
of enforcing this constraint on the camera matrices directly. Algorithms based on the
fundamental matrix, trifocal or quadrifocal tensors may easily be modiﬁed in this way,
as will be seen.

In the mean time, since ˆPj of the form (20.3) is called a reduced camera matrix, we
call any reconstruction of a set of image correspondences in which each camera matrix
is of this form a reduced reconstruction. Not all sets of realizable point correspon-
dences allow a reduced realization, however. The following result characterizes sets of
point correspondences that do have this property.
Result 20.2. A set of image points {xi
i = 1, . . . , m ; j = 1, . . . , n} admits a
reduced realization if and only if it may be augmented with supplementary correspon-
dences xi

n+k = ek for k = 1, . . . ,4 such that

:

j

(i) The total set of image correspondences is realizable, and
(ii) The reconstructed points Xn+k corresponding to the supplementary image cor-

respondences are non-coplanar.

Proof. The proof is straightforward enough. Suppose the set permits a reduced real-
ization, and let Pi be the set of reduced camera matrices. Let points Xn+k = Ek for
k = 1, . . . ,4 be projected into the m images. The projections are xi
n+k = PiXn+k =
PiEk = ek for all i.
Conversely, suppose the augmented set of points is realizable and the points Xn+k are
non-coplanar. In this case, a projective basis may be chosen such that Xn+k = Ek.
Then for each view, one has ek = PiEk for all k. From this it follows that each Pi has
the desired form (20.3).

One other remark must be made before proving the correctness of the algorithm.

Result 20.3. If a set of image points {xi
reduced realization then so does the transposed set {ˆxj
where ˆxj

j for all i and j.

i = xi

:

j

i = 1, . . . , m ; j = 1, . . . , n} permits a
: j = 1, . . . , n ; i = 1, . . . , m}

i

This is the basic duality property, effectively proved by the construction given in step
6 of the algorithm above. Now it is possible to prove the correctness of the algorithm.

508
Result 20.4. Let xi
correspondences, and suppose

j and xi

20 Duality

n+k as in ﬁgure 20.1(left) be a set of realizable image point

(i) for each i, the four points xi
n+k do not include three collinear points.
(ii) the four points Xn+k in a projective reconstruction are non-coplanar.

Then the algorithm of section 20.1.1 will succeed.

Proof. Because of the ﬁrst condition, transformations Ti exist for each i, transforming
the input data to the form shown in ﬁgure 20.1(right). This transformed data is also
realizable, since the transformed data differs only by a projective transformation of the
image.
Now, according to result 20.2 applied to ﬁgure 20.1(right), the correspondences x(cid:2)i
j
admit a reduced realization. By result 20.3 the transposed data ﬁgure 20.2(left) also
admits a reduced realization. Applying result 20.2 once more shows that the extended

data ﬁgure 20.2(right) is realizable. Furthermore, the points (cid:23)Xm+k are non-coplanar,

and so step 5 is valid. The subsequent steps 6 and 7 go forward without problems.

The ﬁrst condition may be checked from the image correspondences xi

j. It may be
thought that to check the second condition requires reconstruction to be carried out. It
is, however, possible to check whether the reconstructed points will be coplanar without
carrying out the reconstruction. This is left as an exercise for the reader (page 342).

20.2 Reduced reconstruction

In this section, we concentrate on and reevaluate steps 3 – 5 of the algorithm described
in the preceding section. To recapitulate, the purpose of these steps is to obtain a
reduced reconstruction from a set of image correspondences. Thus, the input is a set of
image correspondences ˆxj
i admitting a reduced realization (see ﬁgure 20.2(left)). The
i for all

output is a set of reduced camera matrices ˆPj and points (cid:23)Xi such that ˆPj(cid:23)Xi = ˆxj

i, j.

As we have seen, one way to do this (as in steps 3 – 5) of the given algorithm is
to augment the points by the addition of four extra synthetic point correspondences
ˆxj
m+k, carrying out projective reconstruction, and then applying a 3D homography so

that the 3D points (cid:23)Xm+k are mapped to the points Ek of a projective basis for IP3. The

problem with this is that in the presence of noise, the projective reconstruction is not
exact. Thus, the camera matrices obtained by this method will map points Ek to points
close to, but not identical with ek. This means that the camera matrices are not exactly
in reduced form. Therefore, we now consider methods of computing a realization of
the point correspondences in which the cameras are exactly reduced.

20.2.1 The reduced fundamental matrix
The most evident applications of these dual methods are to dualize the reconstruction
algorithms involving the fundamental matrix and trifocal tensor. These will lead to
reconstruction algorithms for 6 or 7 points (respectively) across N views. In this sec-
tion, we consider reconstruction from 6 points. The dual of a reconstruction problem

20.2 Reduced reconstruction

509

P(N, 6) is a problem P(2, N + 4), namely reconstruction from N + 4 points in 2
views. The method of chapter 10 involving the fundamental matrix is a standard way
of solving such a problem.

To this end we deﬁne a reduced fundamental matrix:

Deﬁnition 20.5. A fundamental matrix ˆF is called a reduced fundamental matrix if it
satisﬁes the condition eT
i

ˆFei = 0 for i = 1, . . . ,4 .

It is evident that since a reduced fundamental matrix already satisﬁes constraints
derived from four point correspondences, it may be computed from a small number of
additional points, in fact linearly from four points, or non-linearly from three points.

20.2.2 Computation of the reduced fundamental matrix
For a reduced fundamental matrix, the condition eT
ˆFei = 0 for i = 1, . . . ,3 implies
i
that the diagonal entries of ˆF are zero. The requirement that (1, 1, 1)ˆF(1, 1, 1)T = 0
gives the additional condition that the sum of entries of ˆF is zero. Thus one may write
ˆF in the form

ˆF =

q
r
s
t −(p + q + r + s + t) 0

p
0

(20.4)

 0



thereby parametrizing a fundamental matrix satisfying all the linear constraints (though
not the condition det ˆF = 0). Now, a further point correspondence x ↔ x(cid:2)
satisfying
x(cid:2)TˆFx = 0 is easily seen to provide a linear equation in the parameters p, . . . , t of
ˆF. Given at least four such correspondences, one may solve for these parameters, up
to an inconsequential scale. Given only three such correspondences, the extra con-
straint det ˆF = 0 may be used to provide the extra constraint necessary to determine
ˆF. There may be one or three solutions. This computation is analogous to the method
used in section 11.1.2(p281) to compute the fundamental matrix from seven point cor-
respondences. With four correspondences xi ↔ x(cid:2)
i or more, one ﬁnds a least-squares
solution.

20.2.3 Retrieving reduced camera matrices
Computing a pair of reduced camera matrices that correspond to a reduced fundamen-
tal matrix is surprisingly tricky. One can not assume that the ﬁrst camera is [I | 0] as
in the usual projective camera case, since this is non-generic, the camera centre corre-
sponding with the basis point E4 = (0, 0, 0, 1)T. However, we may instead assume that
the pair of cameras are of the form

 1

P =

1

1
1
1 1

 and P(cid:2)

=

 a



b

d
d
c d

(20.5)

since the centre of the ﬁrst camera is (1, 1, 1, 1)T which is a generic point with respect
to the basis points E1, . . . , E4. Further, if d (cid:5)= 0, then we may assume that d = 1, but
we prefer not to do this.

510

20 Duality

The reduced fundamental matrix corresponding to this pair of cameras is



ˆF =

b(d − c) −c(d − b)
c(d − a)

0

0

−a(d − c)
a(d − b) −b(d − a)

0



(20.6)

(20.7)

which the reader may verify satisﬁes the four linear constraints as well as the zero-
determinant condition. The task at hand is to retrieve the values of (a, b, c, d) given
the fundamental matrix. This seemingly requires a solution of simultaneous quadratic
equations, but there is a linear method as follows.

(i) The ratio a : b : c may be found by solving the set of homogeneous linear

equations

 f12 f21

f13
0

0
f31
0
f23 f32

 = 0



 a

b
c

where fij is the ij-th entry of ˆF. The matrix appearing here clearly has the
same rank as ˆF (namely 2), so there is a unique solution (up to scale) to this
equation set. The solution a : b : c = A : B : C provides a set of homogeneous
equations in a, b and c, namely Ba = Ab, Ca = Ac and Cb = Bc, of which
two are linearly independent.
(ii) Similarly, one may ﬁnd the ratio d − a : d − b : d − c by solving (d − a, d −
b, d − c)ˆF = 0. Once again, the solution is unique. This provides two more
linear equations in the values of a, b, c, d.

(iii) From the set of equations one may solve for (a, b, c, d) up to scale, and hence

reconstruct the second camera matrix according to (20.5).

20.2.4 Solving for six points in three views
The minimal case involving the computation of the reduced fundamental matrix in-
volves three point, in which case there may be three solutions. By dualization, this
leads to a solution to the reconstruction problem for six points in three views. Its use in
outlier detection using the trifocal tensor has been described in algorithm 16.4(p401).
Because of its intrinsic interest as a minimum-case solution, and because of its prac-
tical use, the algorithm is given explicitly as algorithm 20.1. The algorithm consists
essentially of putting together what has been described previously. However there are
a few minor variations.

In algorithm 20.1, the ﬁnal estimation of the camera matrices takes place in the
domain of the original point measurements. As an alternative, one could apply a DLT
algorithm in the dual domain, as in the basic algorithm of section 20.1.1. However, the
present method seems simpler. It has the advantage of avoiding the need for applying
the inverse transformations T, T(cid:2)
. More signiﬁcantly the ﬁnal computation of the
camera matrices is carried out using the original data. This is important, because the
transformations can severely skew the noise characteristics of the data.

, T(cid:2)(cid:2)

The basis for this algorithm and the n-view case to follow is the dual fundamental

20.2 Reduced reconstruction
Objective Given a set of six point correspondences xj ↔ x(cid:1)
pute a reconstruction from these points, consisting of the three camera matrices P, P(cid:1)
and the six 3D points X1, . . . , X6.
Algorithm

j ↔ x(cid:1)(cid:1)

j across three views, com-
and P(cid:1)(cid:1)

511

(i) Select a set of four points no three of which are collinear in any of the three views. Let

these be the correspondences x2+k ↔ x(cid:1)
T to the other two points x1 and x2 resulting in points ˆx1 = Tx1 and ˆx2 = Tx2.

(ii) For the ﬁrst view, ﬁnd a projective transformation T that maps each x2+k to ek. Apply
(iii) From the dual correspondence ˆx1 ↔ ˆx2 derive an equation in the entries p, q, r, s, t
of the reduced fundamental matrix ˆF as in (20.4). The equation is derived from the
relationship ˆxT

2+k for k = 1, . . . , 4.

2+k ↔ x(cid:1)(cid:1)

2 ˆFˆx1 = 0.

(iv) In the same way as in the last two steps, form two more equations from the points in the
other two views. This results in a set of three homogeneous equations in ﬁve unknowns.
There will be a two-parameter family of solutions for the reduced fundamental matrix,
generated by two independent solutions ˆF1 and ˆF2.

(v) The general solution is ˆF = λˆF1 + µˆF2. From the requirement det ˆF = 0 one derives a
homogeneous cubic equation in (λ, µ), which one may solve to ﬁnd (λ, µ), and hence
ˆF. There will be 1 or 3 real solutions. Further steps are applied to each solution in turn.
(vi) Use the method of section 20.2.3 to extract the parameters (a, b, c, d)T of the second

(cid:1)
reduced camera matrix ˆP

deﬁned in (20.5).

(vii) We complete the reconstruction in the original measurement domain. The dual of cam-
(cid:1)
deﬁned by (a, b, c, d) is the point X2 = (a, b, c, d)T. Thus the six 3D points are
era ˆP
X1 = (1, 1, 1, 1)T, X2 = (a, b, c, d)T and X2+k = Ek for k = 1, . . . , 4. This gives the
structure of the reconstructed scene. One may then compute the three camera matrices
using the DLT algorithm for camera calibration described in section 7.1(p178). Since
we require the camera matrices deﬁned with respect to the original camera coordinates
here, we use the original coordinates to solve for P, P(cid:1)
such that PXj = xj, etc.
Since the solution will be exact, the DLT solution will be sufﬁcient.

and P(cid:1)(cid:1)

Algorithm 20.1. Algorithm for computing a projective reconstruction from six points in three views.

matrix, ˆF. Note how the dual fundamental matrix expresses a relationship between
points in the same image. Indeed the equations used to solve for ˆF are constructed
from points in the same image. This contrasts with the standard fundamental matrix
where the relationships being encoded are between points seen in separate images.

20.2.5 Six points in n views

The method for six points in three views can be applied with little modiﬁcation to the
case of six points in many views. The main difference is that the reduced fundamental
matrix ˆF will be uniquely determined by the data. Speciﬁcally at step 4 of the algorithm
of section 20.2.4, each view will contribute one equation. With four views or more, this
will be sufﬁcient to determine ˆF.

In this redundant-data case, one must be careful with the effects of noise. For this
reason, it appears preferable to carry out the last step of the algorithm, as shown, with
original untransformed points. This mitigates the effect of noise distortion that would
result from working with transformed points.

512

20 Duality

20.2.6 Seven points in n views
The problem of seven points in n views is dual to the case of three views of n+4 points
and is solved by computing the reduced trifocal tensor from n point correspondences.
Deﬁnition 20.6. A trifocal tensor T is called a reduced trifocal tensor if it satisﬁes
the linear constraints imposed by synthetic point correspondences ek ↔ e(cid:2)
k for
i = 1, . . . ,4 .

↔ e(cid:2)(cid:2)

k

The general method for reconstruction from 7 points is similar to the method for six
points in n views, except that the reduced trifocal tensor is used instead of the reduced
fundamental matrix. There are, however some minor differences.
In computing the reduced trifocal tensor, the constraints corresponding to the syn-
thetic correspondences ej ↔ ej ↔ ej should be satisﬁed exactly, whereas the other
correspondences used to compute the tensor are subject to noise, and will only be sat-
isﬁed approximately. Otherwise the computed tensor will not be exactly in reduced
form. In the case of the reduced fundamental matrix this was handled by giving a spe-
ciﬁc parametrization of the reduced fundamental matrix. That is, it was parametrized
in such a way that the constraints generated by the synthetic correspondences were au-
tomatically satisﬁed (see (20.4)). In the case of the trifocal tensor, it is not obvious that
such a convenient parametrization is possible. The synthetic constraints are of the form

eiepeqjprkqsT jk

i = 0rs

which is rather more complicated than the linear constraints on the reduced fundamen-
tal matrix. Instead, one may proceed in the following manner.

In the usual linear method of computing the trifocal tensor, one must solve a set of
linear equations of the form At = 0, or more precisely, ﬁnd the vector t that mini-
mizes (cid:10)At(cid:10) subject to (cid:10)t(cid:10) = 1. In solving for the reduced trifocal tensor, the matrix
A may be divided into two parts, corresponding to those constraints from the synthetic
correspondences, which should be satisﬁed exactly, and the constraints from real corre-
spondences, which must be satisﬁed in a least-squares sense. The ﬁrst set of constraints
are of the form Ct = 0, and the second set may be written as ˆAt = 0. The problem
becomes: ﬁnd t that minimizes (cid:10)ˆAt(cid:10) subject to (cid:10)t(cid:10) = 1 and Ct = 0. An algorithm for
solving this problem is given as algorithm A5.5(p594).

For the problem of extracting the three camera matrices from the reduced tensor no
simple method seems to be available, similar to that described in section 20.2.3 for the
fundamental matrix. Instead, one may use the method described in steps 5 and 6 of the
general dual algorithm (see page 505).

The minimal conﬁguration of this type is seven points in two views. In this case, the
problem is best solved directly by the method of section 11.1.2(p281), rather than by
dualization to the case of three points in six views.

20.2.7 Performance issues
Dual reconstruction algorithms based on the reduced fundamental matrix and trifocal
tensor have been implemented and tested. The results of these tests were contained

20.3 Closure

513

in a student report presented in August 1996 by Gilles Debunne. Since this report is
effectively unavailable, the results are summarized here.

The most serious difﬁculty is the distortion of the noise distribution by the applica-
tion of projective transformations Ti to the images. Application of projective transfor-
mations to the image data has the effect of distorting any noise distribution that may
apply to the data. The problem is related to the need to choose four points that are non-
collinear in any of the images. If the points are close to collinear in any of the images,
then the projective transformation applied to the image in step 1 of the algorithm may
entail extreme distortion of the image. This sort of distortion can degrade performance
of the algorithm severely.

Without special attention being paid to the noise-distortion, performance of the algo-
rithms was generally unsatisfactory, Despite great care being taken to minimize errors
due to noise in steps 4–6 of the algorithm (page 505f f), when the inverse projective
transformations are applied in step 7, the average error became very large. Some points
retained quite small error, whereas in those images where distortion was signiﬁcant,
quite large errors resulted.

Normalization in the sense of section 4.4.4(p107) is also a problem.

It has been
shown to be essential for performance of the linear reconstruction algorithms to ap-
ply data normalization. However, what sort of normalization should be applied to the
transformed data of ﬁgure 20.1(right), which is geometrically unrelated to actual image
measurements, is a mystery.

To get good results, it would seem that one would need to propagate assumed error
distributions forward in step 1 of the algorithm to get assumed error distributions for
the transformed data ﬁgure 20.1(right), and then during reconstruction to minimize
residual error relative to this propagated error distribution. Equivalently, cost functions
being minimized during reconstruction must be related back to measurement error in
the original image points. Recent work reported in [Hartley-00a, Schaffalitzky-00c]
has shown that this indeed gives signiﬁcantly improved results.

20.3 Closure

20.3.1 The literature
The idea behind Carlsson–Weinshall duality was ﬁrst described in a pair of papers ap-
pearing simultaneously: [Carlsson-95] and Weinshall [Weinshall-95] and subsequently
in a joint paper [Carlsson-98]. The treatment given here for the general method of du-
alizing an algorithm was given in [Hartley-98b], derived from these earlier papers.
Details of methods for handling with noise propagation were given in [Hartley-00a],
building on earlier implementations contained in an unavailable report by Gilles De-
bunne (August 1996).

The problem of reconstruction from six points in three views was perhaps ﬁrst treated
in a technical report [Hartley-92b] (later published as [Hartley-94c]) where the exis-
tence of up to eight solutions was shown. The problem was given a complete solution
in [Quan-94] where it was shown that only three solutions are possible. This was also
pointed out in [Ponce-94]. The paper [Carlsson-95] established that this problem is

514

20 Duality

dual to the two-view, seven point problem for which a solution was known. This en-
abled the formulation of the method given in this chapter. The minimum six point
conﬁguration was used in [Torr-97] for robust estimation of the trifocal tensor. An al-
ternative method for computing a reconstruction from six points in n ≥ 3 views is given
in [Schaffalitzky-00c]. This method does not require that images are ﬁrst projectively
transformed to a canonical basis.

20.3.2 Notes and Exercises

(i) In the dual algorithm of section 20.1.1 it was noted that the method will only
work if the four points used to deﬁne the image transformations are non-
coplanar. However, note that in this case, the algorithm of section 18.5.2(p450)
will compute a projective reconstruction linearly.
(ii) If the four chosen points are coplanar, then the homographies Ti will map the
plane to a common coordinate system. The transformed points x(cid:2)i
j will then
satisfy the condition of section 17.5.2(p425), namely the lines joining any pair
of points x(cid:2)i
k (j and k ﬁxed and a different line for each i) will meet in a
common point. The duality in the case is described in [Criminisi-98, Irani-98].
(iii) Still in the case of the four chosen points being coplanar: after applying the Ti,
any point on the plane of the four points will map to the same point in all im-
ages. Thus, the fundamental matrix consistent with the point correspondences
will be skew-symmetric.

j and x(cid:2)i

21

Cheirality

When a projective reconstruction of a scene is carried out from a set of point corre-
spondences, an important piece of information is typically ignored – if the points are
visible in the images, then they must have been in front of the camera. In general, a
projective reconstruction of a scene will not bear a close resemblance to the real scene
when interpreted as if the coordinate frame were Euclidean. The scene is often split
across the plane at inﬁnity, as is illustrated in two dimensions by ﬁgure 21.1. It is pos-
sible to come much closer to at least an afﬁne reconstruction of the scene by taking this
simple constraint into account. The resulting reconstruction is called “quasi-afﬁne”
in that it lies part way between a projective and afﬁne reconstruction. Scene objects
are no longer split across the plane at inﬁnity, though they may still suffer projective
distortion.

Converting a projective reconstruction to quasi-afﬁne is extremely simple if one ne-
glects the cameras and requires only that the scene be of the correct quasi-afﬁne form –
in fact it can be accomplished in about two lines of programming (see corollary 21.9).
To handle the cameras as well requires the solution of a linear programming problem.

21.1 Quasi-afﬁne transformations

A subset B of IRn is called convex if the line segment joining any two points in B
also lies entirely within B. The convex hull of B, denoted B, is the smallest convex
set containing B. Our main concern will be with 3-dimensional point sets, so n = 3.
We view IR3 as being a subset of IP3, consisting of all non-inﬁnite points. The inﬁnite
points constitute the plane at inﬁnity, denoted π∞. Thus, IP3 = IR3 ∪ π∞. A subset
of IP3 will be called convex if and only if it is contained in IR3 and is convex in IR3.
Hence, according to this deﬁnition, a convex set does not contain any inﬁnite points.
Deﬁnition 21.1. Consider a point set {Xi} ⊂ IR3 ⊂ IP3. A projective mapping h :
IP3 → IP3 is said to preserve the convex hull of the points {Xi} if

(i) h(Xi) is a ﬁnite point for all i, and
(ii) h maps the convex hull of points {Xi} bijectively onto the convex hull of the

points {h(Xi)}.

515

516

21 Cheirality

a

b

Fig. 21.1. (a) an image of a comb, and (b) the result of applying a projective transformation to the
image. The projective transformation does not however preserve the convex hull of the set of points
constituting the comb. In the original image, the convex hull of the comb is a ﬁnite set contained within
the extent of the visible image. However, some of the points in this convex hull are mapped to inﬁnity by
the transformation.

An example, shown in ﬁgure 21.1, may help in understanding this deﬁnition. The
example deals with 2D point sets, but the principle is the same. The ﬁgure shows an
image of a comb and the image resampled according to a projective mapping. The
projective mapping does not however preserve the convex hull of the comb. Most
people will agree that the resampled image is unlike any view of a comb seen by camera
or human eye.

the last coordinate is equal to 1.

The property of preserving the convex hull of a set of points may be characterized in
various different ways, as is shown by the theorem to be given shortly. In order to state
this theorem, we introduce a new notation.

Notation. The symbol (cid:23)X denotes a homogeneous representation of a point X in which
3D. Thus for instance if H is a projective transformation, we may write H(cid:23)X = T
(cid:2)(cid:23)X
mean that the transformation takes point (cid:23)X to point (cid:23)X

Commonly in this chapter we will be interested in the exact equalities (not equalities
up to scale) between vectors representing homogeneous quantities, such as points in
to
is

, but that the scale factor T

(cid:2)
(cid:2)

(cid:2)

required to make this equality exact.

Now for the theorem.
Theorem 21.2. Consider a projective transformation h : IP3 → IP3 and a set of points
{Xi}. Let π∞ be the plane mapped to inﬁnity by h. The following statements are
equivalent.

(i) h preserves the convex hull of the points {Xi}.
(ii) ˜X ∩ π∞ = ∅ for any point ˜X in the convex hull of the points {Xi}.
H(cid:23)Xi = T
(iii) Let H be a matrix representing the transformation h, and suppose that

(cid:2)
i. Then the constants T

(cid:2)
i all have the same sign.

(cid:23)X

(cid:2)
i

21.1 Quasi-afﬁne transformations

517

T

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

, ˜Y

, ˜Z

(cid:2)(cid:23)X

is equal to zero. This means that H ˜X = (˜X

Proof. This will be proved by showing that (i) ⇒ (ii) ⇒ (iii) ⇒ (i).
(i) ⇒ (ii). If h preserves the convex hull of the points, then h( ˜X) is a ﬁnite point for
any point ˜X in the convex hull of the Xi. So ˜X ∩ π∞ = ∅.
(ii) ⇒ (iii). Consider the chord joining two points Xi and Xj and suppose that T
(cid:2)
i and
(cid:2)
(cid:2)
i is a continuous function
j (as in part (iii) of the theorem) have opposite signs. Since T
T
of the coordinates of Xi, there must be a point ˜X lying on the chord from Xi to Xj for
(cid:2)
, 0)T. Since ˜X is in the
which T
convex hull of the points {Xi}, this contradicts (ii).
(cid:23)X
(iii) ⇒ (i). We assume that there exist constants T
(cid:2)
H(cid:23)X = T
(cid:2)
i

i all of the same sign such that H(cid:23)Xi =
(cid:2)
i. Let S be the subset of IRn consisting of all points X satisfying the condition
i. The set S contains {Xi}.
(cid:2)
It
If Xi and Xj are points in S with corresponding
j. To see this, consider a point (cid:23)X = α(cid:23)Xi+(1−α)(cid:23)Xj
(cid:2)
(cid:2)
j, then any intermediate point X between Xi and Xj must have T
(cid:2)
(cid:23)X
4 (α(cid:23)Xi + (1 − α)(cid:23)Xj)
= hT
(cid:23)Xi + (1− α)hT
(cid:23)Xj
4
= hT
= αhT
4
i + (1− α)T
(cid:2)
= αT

will be shown that S is convex.
constants T
value intermediate between T
where 0 ≤ α ≤ 1. This point lies between Xi and Xj. Denote by hT
Then,

has the same sign as the T

4 the last row of H.

(cid:2)
i and T

such that T

(cid:2)
i and T

(cid:2)

T

4

(cid:2)
j

(cid:2)

must have the

(cid:2)
j as claimed. Consequently, the value of T

(cid:2)
i and T
(cid:2)
j, and so X lies in S also. This shows that S is convex.

which lies between T
(cid:2)
same sign as T
i and T
Now, let ˜S be a convex subset of S. It will be shown that h( ˜S) is also convex. This is
easily seen to be true, since h maps a line segment in ˜S to a line segment that does not
cross the plane at inﬁnity. Thus, h maps any convex set ˜S such that S ⊃ ˜S ⊃ {Xi} to
}. However, if H satisﬁes condition (iii), then
(cid:2) ⊃ {X(cid:2)
a convex set ˜S
it is easily seen that H−1 does also. From this it follows that the above correspondence
is bijective. Since the convex hull of {Xi} (or {X(cid:2)
}
between convex sets ˜S and ˜S
i
respectively) is the intersection of all such convex sets, it follows that h preserves the
convex hull of the points.

(cid:2) ⊃ ˜S
(cid:2)

such that S

(cid:2)

i

The projective transformations that preserve the convex hull of a given set of points

form an important class, and will be called quasi-afﬁne transformations.

Deﬁnition 21.3. Let B be a subset of IRn and let h be a projectivity of IPn. The projec-
tivity h is said to be “quasi-afﬁne” with respect to the set B if h preserves the convex
hull of the set B.

−1 is quasi-afﬁne
It may be veriﬁed that if h is quasi-afﬁne with respect to B, then h
with respect to h(B). Furthermore, if h is quasi-afﬁne with respect to B and g is
quasi-afﬁne with respect to h(B), then g ◦ h is quasi-afﬁne with respect to B. Thus,
quasi-afﬁne projectivities may be composed in this fashion. Strictly speaking, however,
quasi-afﬁne projectivities with respect to a given ﬁxed set of points do not form a group.

518

21 Cheirality

We will be considering sets of points {Xi} and {X(cid:2)
} that correspond via a projectiv-
ity. When we speak of the projectivity being quasi-afﬁne, we will mean with respect to
the set {Xi}.
Two-dimensional quasi-afﬁne mappings
Two-dimensional quasi-afﬁne mappings arise as transformations between planar point
sets in 3D and their image under a projective camera mapping, as stated formally below.

i

Theorem 21.4. If B is a point set in a plane (the “object plane”) in IR3 and B lies
entirely in front of a projective camera, then the mapping from the object plane to the
image plane deﬁned by the camera is quasi-afﬁne with respect to B.

Proof. That there is a projectivity h mapping the object plane to the image plane is
well known. What is to be proved is that the projectivity is quasi-afﬁne with respect
to B. Let L be the line in which the principal plane of the camera meets the object
plane. Since B lies entirely in front of the camera, L does not meet the convex hull
of B. However, by deﬁnition of the principal plane h(L) = L∞, where L∞ is the line
at inﬁnity in the image plane. Thus, one deduces that h(B) ∩ L∞ = ∅, and hence by
theorem 21.2 the transformation is quasi-afﬁne with respect to B.

Note that if points xi are visible in an image, then the corresponding object points
must lie in front of the camera. Applying theorem 21.4 to a sequence of imaging
operations (for instance, a picture of a picture of a picture, etc.), it follows that the
original and ﬁnal images in the sequence are connected by a planar projectivity which
is quasi-afﬁne with respect to any point set in the object plane visible in the ﬁnal image.
Similarly, if two images are taken of a set of points {Xi} in a plane, {xi} and {x(cid:2)
}
i
being corresponding points in the two images, then there is a quasi-afﬁne mapping
(with respect to the xi) mapping each xi to x(cid:2)
i, and so theorem 21.2 applies, yielding
the following:
Result 21.5. If {xi} and {x(cid:2)
} are corresponding points in two views of a set of object
points {Xi} lying in a plane, then there is a matrix H representing a planar projectivity
such that Hˆxi = wiˆx(cid:2)

i and all wi have the same sign.

i

21.2 Front and back of a camera

The depth of a point X = (X, Y, Z, T)T with respect to a camera was shown in (6.15–
p162) to be given by

sign(det M)w

depth(X; P) =

(21.1)
where M is the left hand 3 × 3 block of P, m3 is the third row of M, and PX = wˆx. This
expression is not dependent on the particular homogeneous representation of X or M,
that is it is unchanged by multiplication by non-zero scale factors. This deﬁnition of
depth is used to determine whether a point is in front of a camera or not.

T(cid:10)m3(cid:10)

Result 21.6. The point X lies in front of the camera P if and only if depth(X; P) > 0.

21.3 Three-dimensional point sets

519

In fact, depth is positive for points in front of the camera, negative for points be-
hind the camera, inﬁnite on the plane at inﬁnity and zero on the principal plane of the
camera. If the camera centre or the point X is at inﬁnity, then depth is not deﬁned.

Usually, in this section we will only be concerned with the sign of depth and not its

magnitude. We may then write

depth(X; P)

.
= wT det M

(21.2)

where the symbol .
= indicates equality of sign. The quantity sign(depth(X; P)) will be
referred to as the cheirality of the point X with respect to the camera P. The cheirality
of a point is said to be reversed by a transformation if it is swapped from 1 to −1 or
vice versa.

21.3 Three-dimensional point sets

In this section the connection between cheirality of points with respect to a camera and
convex hulls of point sets will be explained. The main result is stated now.
Theorem 21.7. Let PE and P(cid:2)E be two cameras, XE
cameras, and xi and x(cid:2)
Euclidean.)

i a set of points lying in front of both
i the corresponding image points. (The superscript E stands for
,{Xi}) be any projective reconstruction from the image correspon-
dences xi ↔ x(cid:2)
(cid:2)
i has the same
(ii) If each Xi is a ﬁnite point, and P(cid:23)Xi = wiˆxi with wi having the same sign for all
sign for all i.

i, and let PXi = wiˆxi and P(cid:2)Xi = w

iˆx(cid:2)
(cid:2)
i. Then wiw

(i) Let (P, P(cid:2)

i, then there exists a quasi-afﬁne transformation H taking each Xi to XE
i .

Note that the condition that each wiw

Of course, the existence of a projective transformation taking each Xi to XE
i is guaran-
teed by theorem 10.1(p266). The current theorem gives the extra information that the
transformation is quasi-afﬁne, and hence one has a quasi-afﬁne reconstruction.
(cid:2)
i have the same sign is unaffected by multiply-
ing P, P(cid:2)
or any of the points Xi by a scale factor, and hence is invariant of the choice of
homogeneous representative for any of these quantities. In particular, if P is multiplied
(cid:2)
by a negative constant, then so is wi for all i. Thus the sign of wiw
i is inverted for each
i, but they still all have the same sign. Similarly, if one point Xi is multiplied by a neg-
(cid:2)
ative constant, then both wi and w
i is unchanged.
In the same way, the condition that each wi (in part (ii) of the theorem) have the same
sign for all i is unaffected if the camera matrix is multiplied by a negative (or of course
positive) constant.

(cid:2)
i change signs, and so the sign of wiw

Proof. The points XE
depth with respect to these cameras. According to (21.2)

i lie in front of the cameras PE and P(cid:2)E, and hence have positive

Hence, det(ME)wiTE
Multiplying these expressions together, and cancelling TE

i > 0 for all i. Similarly for the second camera, det(M(cid:2)E)w

i > 0.
i because it appears twice,

(cid:2)
iTE

.
= det(ME)wiTE
i .

depth(XE

i ; PE)

520

21 Cheirality

(cid:2)
i has

i ), and hence wiw

i = (PEH−1)(HXE

i det ME det M(cid:2)E > 0. Since det ME det M(cid:2)E is constant, this shows that wiw
(cid:2)

gives wiw
constant sign.
This was shown in terms of the true conﬁguration. Note however that for any H, one has
(cid:2)
wiˆxi = PEXE
i has the same sign for the projective
}). Since any projective reconstruction is of this
reconstruction (PEH−1, P(cid:2)EH−1,{HXE
(cid:2)
form (except for homogeneous scale factors), and the condition that wiw
i has the same
i , PE and P(cid:2)E, it
sign is independent of choice of homogeneous representatives of XE
(cid:2)
follows that in any projective reconstruction, wiw
i has the same sign for all i. This
To show the second part, suppose in the projective reconstruction wiˆxi = P(cid:23)X all the wi
proves the ﬁrst part of the theorem.
(cid:23)XE
tion represented by H such that H(cid:23)Xi = ηi
have the same sign. Since this is a projective reconstruction, there exists a transforma-
i and PH−1 = PE for some constants ηi and

i

. Then,

and so

wiˆxi = P(cid:23)Xi = (PH−1)(H(cid:23)Xi) = (PE)(ηi

(cid:23)XE

i ) .

PE(cid:23)XE

i = (wi/ηi)ˆxi

for all i. However, since depth(XE
i , PE) > 0, one has det(ME)wi/ηi > 0 for all i. Since
det(ME)/ is constant, and by hypothesis wi has the same sign for all i, it follows that ηi
i is a quasi-afﬁne

has the same sign for all i. Thus the mapping H such that H(cid:23)Xi = ηi
map with respect to the points (cid:23)Xi, according to theorem 21.2.
one of the cameras only. However, deﬁning P(cid:2)(cid:23)Xi = w
Note that the condition that wi have the same sign for all i needs to be checked for
iˆx(cid:2)
(cid:2)
i, according to part (i) of the
(cid:2)
i has the same sign for all i. Thus, if all wi have the same sign, then so do

(cid:23)XE

theorem, wiw
all w

(cid:2)
i.

According to theorem 21.7, any projective reconstruction in which P(cid:23)Xi = wiˆxi and

21.4 Obtaining a quasi-afﬁne reconstruction

wi has the same sign for all i is a quasi-afﬁne reconstruction. The advantage of a
quasi-afﬁne reconstruction is that it gives a closer approximation to the true shape
of the object than does an arbitrary projective transformation.
It may be used as a
stepping stone on the way to a metric reconstruction of the scene, as in [Hartley-94b].
In addition, one may retrieve the convex hull of the object or determine such questions
as whether two points lie on the same side of a plane.

It turns out that quasi-afﬁne reconstruction is extremely simple, given a projective

reconstruction, as shown in the following theorem.

Theorem 21.8. Any projective reconstruction in which one of the cameras is an afﬁne
camera is a quasi-afﬁne reconstruction.

Proof. Recall that an afﬁne camera is one for which the last row is of the form

(0, 0, 0, 1). In this case, writing wiˆxi = P(cid:23)Xi, one immediately veriﬁes that wi = 1

for all i, and in particular they all have the same sign. According to theorem 21.2 this

21.5 Effect of transformations on cheirality

521

means that the reconstruction differs by a quasi-afﬁne transformation from the truth.

The following result follows immediately.

Corollary 21.9. Let (P, P(cid:2)
P = [I | 0]. Then by swapping the last two columns of P and of P(cid:2)
two coordinates of each Xi, one obtains a quasi-afﬁne reconstruction of the scene.

,{Xi}) be a projective reconstruction of a scene in which
, as well as the last

This is similar to result 10.4(p271), in which it was shown that if the camera P is
known in reality to be an afﬁne camera, then the above procedure provides an afﬁne
reconstruction.

21.5 Effect of transformations on cheirality

At this point, it is desirable to derive a slightly different form of the formula for depth
deﬁned in (21.2). Let P be a camera matrix. The centre of P is the unique point C such
that PC = 0. One can write an explicit formula for C as follows.
Deﬁnition 21.10. Given a camera matrix P, we deﬁne CT
where

P to be the vector (c1, c2, c3, c4),

ci = (−1)i det ˆP(i)

and ˆP(i) is the matrix obtained by removing the i-th column of P.
We denote by [P/VT] the 4 × 4 matrix made up of a 3 × 4 camera matrix P augmented
with an ﬁnal row VT. Deﬁnition 21.10 leads to a simple formula for det[P/VT]. Cofac-
tor expansion of the determinant along the last row gives det[P/VT] = VTCP for any
row vector VT. As a special case, if pT

i is the i-th row of P, then

pT
i

CP = det[P/pT

i ] = 0

where the last equality is true because the matrix has a repeated row. Since this is true
for all i, it follows that PCP = 0, and so CP is the camera centre, as claimed.
Note that submatrix ˆP(4) is the same as matrix M in the decomposition P = [M | v],
and so det M = c4. This allows us to reformulate (21.2), as follows.

.
= w(ET
4

X)(ET
4

CP)

depth(X; P)

X = 0.
We now consider a projective transformation represented by matrix H. If P(cid:2)

(21.3)
where ET
4 is the vector (0, 0, 0, 1). It is signiﬁcant to note here that E4 is the vector
representing the plane at inﬁnity – a point X lies on the plane at inﬁnity if and only if
ET
= PH−1
4
and X(cid:2)
= HX then the image correspondences are preserved by this transformation.
When speaking of a projective transformation being applied to a set of points and to
a camera, it is meant that a point X is transformed to HX and the camera matrix is
transformed to PH−1.

In this section we will consider such projective transformations and their effect on
the cheirality of points with respect to a camera. First, we wish to determine what

522
happens to CP when P is transformed to PH−1. To answer that question, consider an
arbitrary 4-vector V. We see that

21 Cheirality

VTH−1CPH−1 = det(PH−1/VTH−1) = det(P/VT) det H−1 = VTCP det H−1.

Since this is true for all vectors V, it follows that H−1CPH−1 = CP det H−1, or

CPH−1 = HCP det H−1.

(21.4)

At one level, this formula is saying that the transformation H takes the camera centre
C = CP to the new location CPH−1 ≈ HC. However, we are interested in the exact
coordinates of CPH−1, especially the sign of the last coordinate c4 which appears in
(21.3). Thus, the factor det H−1 is signiﬁcant.

Now, applying (21.4) to (21.3) gives
.
= w(ET
4
.
= w(ET
4

depth(HX; PH−1)

HX)(ET
4
HX)(ET
4

CPH−1)
HCP) detH −1 .

One may interpret the expression ET
4

H as being the plane π∞ mapped to inﬁnity by
H. This is because a point X lies on π∞ if and only if the last coordinate of HX is zero
– that is E4HX = 0. On the other hand, X lies on π∞ if and only if πT∞X = 0. Finally,
denoting the fourth row of the transformation matrix H by hT
4 , and sign(det H) by δ, we
obtain
Result 21.11. If π∞ is the plane mapped to inﬁnity by a projective transformation H
and δ = sign(det H), then

depth(HX; PH−1)

.
= w(πT∞X)(πT∞CP)δ .

This equation will be used extensively. It may be considered to be a generalization of
(21.3). It will be seen in the next section that δ = sign(det H) is an indicator of whether
H is an orientation-reversing or orientation-preserving transformation. Thus, the effect
on cheirality of applying a transformation H is determined only by the position of the
plane mapped to inﬁnity π∞, and whether H preserves or reverses orientation.

We now consider the effect of different transformations on the cheirality of points

with respect to a camera. The effect of an afﬁne transformation is considered ﬁrst.

Result 21.12. An afﬁne transformation with positive determinant preserves the cheiral-
ity of any point with respect to a camera. An afﬁne transformation with negative deter-
minant reverses cheirality.

Proof. An afﬁne transformation preserves the plane at inﬁnity, hence π∞ = E4. The
result then follows by comparing (21.3) and result 21.11.

We now determine how an arbitrary projective transformation affects cheirality.

Result 21.13. Let H represent a projective transformation with positive determinant,
and let π∞ be the plane in space mapped to inﬁnity by H. The cheirality of a point X
is preserved by H if and only if X lies on the same side of the plane π∞ as the camera
centre.

21.6 Orientation

523
.
Proof. Since det H > 0, we see from (21.3) and result 21.11 that depth(X; P)
=
depth(HX; PH−1) if and only if (πT∞X)(πT∞C)
C). Suppose the point
X and the camera P are located at ﬁnite points so that the cheirality is well deﬁned,
and let them be scaled so that both X and C have ﬁnal coordinate equal to 1.
In
this case, (ET
C) = 1 and we see that cheirality is preserved if and only if
4
(πT∞X)(πT∞C)
= πT∞C. This condition may be
interpreted as meaning that the points C and X both lie on the same side of the plane
π∞. Hence, the cheirality of a point X is preserved by a transformation H, if and only
if it lies on the same side of the plane π∞ as the camera centre.

X)(ET
4
.
= 1, or otherwise expressed πT∞X .

.
= (ET
4

x)(ET
4

21.6 Orientation

We now consider the question of image orientation. A mapping h from IRn to itself is
called orientation-preserving at points X where the Jacobian of h (the determinant of
the matrix of partial derivatives) is positive, and orientation-reversing at points where
the Jacobian is negative. Reﬂection of points of IRn with respect to a hyperplane (that
is mirror imaging) is an example of an orientation-reversing mapping. A projectivity
h from Pn to itself restricts to a mapping from IRn − π∞ to IRn, where π∞ is the
hyperplane (line, plane) mapped to inﬁnity by H. Consider the case n = 3 and let H be
a 4 × 4 matrix representing the projectivity h. We wish to determine at which points X
Mathematica [Mathematica-92]) that if H(cid:23)X = w(cid:23)X
in IRn − π∞ the map h is orientation-preserving. It may be veriﬁed (quite easily using
and J is the Jacobian matrix of h

(cid:2)

evaluated at X, then det(J) = det(H)/w4. This gives the following result.
Result 21.14. A projectivity h of IP3 represented by a matrix H is orientation-preserving
at any point in IR3 − π∞ if and only if det(H) > 0.
Of course, the concept of orientability may be extended to the whole of IP3, and it may
be shown that h is orientation-preserving on the whole of IP3 if and only if det(H) > 0.
The essential feature here is that as a topological manifold, IP3 is orientable.
Two sets of points {Xi} and {Xi} that correspond via a quasi-afﬁne transformation
are said to be oppositely oriented if the transformation is orientation-reversing. As an
example, consider the transformation given by a diagonal matrix H = diag(1, 1,−1, 1).
This transformation has negative determinant, and hence is orientation-reversing. On
the other hand, it is afﬁne, and hence quasi-afﬁne. Therefore, it is possible always to
construct oppositely oriented quasi-afﬁne reconstructions of a scene. It may appear
therefore that the orientation of a scene may not be determined from a pair of images.
Although this is sometimes true, it is sometimes possible to rule out one of the op-
positely oriented quasi-afﬁne reconstructions of a scene, and hence determine the true
orientation of the scene.

Common experience provides some clues here. In particular a stereo pair may be
viewed by presenting one image to one eye and the other image to the other eye. If this
is done correctly, then the brain perceives a 3D reconstruction of the scene If, however,
the two images are swapped and presented to the opposite eyes, then the perspective
will be reversed – hills become valleys and vice versa. In effect, the brain is able to
compute two oppositely oriented reconstructions of the image pair. It seems, therefore,

524

21 Cheirality

Fig. 21.2. Stereo pairs of images that may be viewed by cross-fusing (the eyes are crossed so that the
left eye sees the right image and vice versa). The two bottom images are the same as the top pair, except
that they are swapped. In the top pair of images one sees an L-shaped region raised above a planar
background. In the bottom pair of images the L-shaped region appears as an indentation. The two
“reconstructions” differ by reﬂection in the background plane. This demonstrates that the same pair of
images may give rise to two differently oriented projective reconstructions.

that in certain circumstances, two oppositely oriented realizations of an image pair
exist. This is illustrated in ﬁgure 21.2.

It may be surprising to discover that this is not always the case, as is shown in the
following theorem. As used in this theorem and elsewhere in this chapter, a projective
realization of a set of point correspondences is known as a strong realization if the
reconstructed 3D points Xi are in front of all the cameras.
,{Xi}) be a strong realization of a uniquely realizable set of
Theorem 21.15. Let (P, P(cid:2)
point correspondences. There exists a different oppositely oriented strong realization
,{Xi}) if and only if there exists a plane in IR3 such that the perspective centres
(P, P
of both cameras P and P(cid:2)
lie on one side of the plane, and the points Xi lie on the other
side.

(cid:2)

Proof. Consider one strong realization of the conﬁguration. By deﬁnition, all the
points lie in front of both cameras. Suppose that there exists a plane separating the
points from the two camera centres. Let G be a projective transformation mapping
the given plane to inﬁnity and let A be an afﬁne transformation. Suppose further that

21.7 The cheiral inequalities

525

det G > 0 and det A < 0. Let H be the composition H = AG. According to result 21.13
the transformation G is cheirality-reversing for the points, since the points are on the
opposite side of the plane from the camera centres. According to result 21.12 A is also
cheirality-reversing, since det A < 0. The composition H must therefore be cheirality-
preserving, and it transforms the strong conﬁguration to a different strong conﬁgura-
tion. Since H has negative determinant, however, it is orientation-reversing, so the two
strong realizations have opposite orientations.
Conversely, suppose that two strong oppositely oriented realizations exist and let H be
the transformation taking one to the other. Since H is orientation-reversing, det H < 0.
The mapping H is by deﬁnition cheirality-preserving on all points, with respect to both
cameras. If π∞ is the plane mapped to inﬁnity by H, then according to result 21.13 the
points X must lie on the opposite side of the plane π∞ from both camera centres.

21.7 The cheiral inequalities

In section 21.4 a very simple method was given for obtaining a quasi-afﬁne reconstruc-
tion of a scene directly from a projective reconstruction. However, the reconstruction
obtained there did not respect the condition that the points must lie in front of all cam-
eras. In fact, the ﬁrst camera in this construction is an afﬁne camera, for which front and
back are not well deﬁned. By taking full advantage of the fact that visible points must
lie in front of the camera, it is possible to constrain the reconstruction more tightly,
leading to a closer approximation to a true afﬁne reconstruction of the scene.
The method will be given for the case of a reconstruction derived from several im-
ages. One is given a set of image points {xj
i is the projection of the i-th
point in the j-th image. Not all points are visible in each image, so for some (i, j) the
point xj
i is not given, in which case it is not known whether the i-th point lies in front
of the j-th camera or not. On the other hand, the existence of an image point xj
i implies
that the point lies in front of the camera.

}, where xj

i

We start from an assumed projective reconstruction of the scene, consisting of a set
≈ PjXi. Writing the implied scalar
of 3D points Xi and cameras Pj such that xj
i
constant explicitly in this equation gives wj
i ˆxj
i = PjXi. In this equation, Pj and Xi
are arbitrarily chosen homogeneous representatives of the respective matrix or vector.
Related to theorem 21.7 one may state for several views:
Result 21.16. Consider a set of points XE
deﬁned for some indices (i, j) such that point XE
be a projective reconstruction from xj
˜Xi = ±Xi such that for each (i, j) for which xj

i be
i lies in front of camera PjE. Let (Pj; Xi)
i . Then there are camera matrices ˜Pj = ±Pj and

i and cameras PjE, and let xj

i is deﬁned, one has

i = PjEXE

i with wj
Brieﬂy stated, one can always adjust a projective reconstruction, multiplying camera
matrices and points by −1 if necessary, so that wj
i is positive whenever image point xj
exists. The simple proof is omitted. To ﬁnd the matrices ˜Pj and points ˜Xi, one may
assume that one of the cameras ˜P1 = P1, for otherwise all points and cameras may be
multiplied by −1. The condition P1 ˜Xi = w1
i > 0 determines whether to

x1
i with w1

i > 0.

i

˜Pj ˜Xi = wj

i ˆxj

i

526

21 Cheirality

choose ˜Xi = Xi or −Xi for all i such that x1
i is deﬁned. Each known ˜Xi determines ˜Pj
i is deﬁned. Continuing in this way, one easily ﬁnds the factor ±1
for all j such that xj
to apply to each Pj and Xj to ﬁnd ˜Pj and ˜Xi. We assume that this has been done, and
replace each Pj by ˜Pj and Xi by ˜Xi. In future we drop the tildes and continue to work
with the corrected Pj and Xi. We now know that wj
i is
given.

i > 0 whenever image point xj

Now, we seek a transformation H that will transform the projective reconstruction to
a quasi-afﬁne reconstruction for which all points lie in front of the cameras as appro-
priate. Denoting by 4-vector v the plane π∞ mapped to inﬁnity by H, this condition
may be written as (see result 21.11):

depth(Xi; Pj)

.
= (vTXi)(vTC)δ > 0

where δ = sign(det H). This condition holds for all pairs (i, j) where xj
Since we are free to multiply v by −1 if necessary, we may assume that (vTC1)δ > 0
for the centre of the camera P1. The following inequalities now follow easily:

i is given.

XT
i

v > 0 for all i
δCj Tv > 0 for all j.

(21.5)

These equations (21.5) may be called the cheiral inequalities. Since the values of
each Xi, C and C(cid:2)
are known, they form a set of inequalities in the entries of v. The
value of δ is not known a priori, and so it is necessary to seek a solution for each of the
two cases δ = 1 and δ = −1.
To ﬁnd the required transformation H, ﬁrst of all we solve the cheiral inequalities to
ﬁnd a value of v, either for δ = 1 or δ = −1. The required matrix H is any matrix
having vT as its last row and satisfying the condition det H .
= δ. If the last component
of v is non-zero, then H can be chosen to have the simple form in which the ﬁrst three
rows are of the form ±[I | 0].
If a Euclidean reconstruction (or more speciﬁcally a quasi-afﬁne reconstruction) is
possible, then there must be a solution either for δ = 1 or δ = −1. In some cases
there will exist solutions of the cheiral inequalities for both δ = 1 and δ = −1. This
will mean that two oppositely oriented strong realizations exist. The conditions under
which this may occur were discussed in section 21.6.

Solving the cheiral inequalities
Naturally, the cheiral inequalities may be solved using techniques of linear program-
ming. As they stand however, if v is a solution then so is αv for any positive factor α.
In order to restrict the solution domain to be bounded, one may add additional inequal-
ities. For instance, if v = (v1, v2, v3, v4)T, then the inequalities −1 < vi < 1 serve to
restrict the solution domain to be a bounded polyhedron.

To achieve a unique solution we need to specify some goal function to be linearized.
An appropriate strategy is to seek to maximize the extent to which each of the inequali-
ties is satisﬁed. To do this, we introduce one further variable, d. Each of the inequalities
aTv of the form (21.5) for appropriate a is replaced by an inequality aTv > d. We seek

21.7 The cheiral inequalities

527

to maximize d while satisfying all the inequalities. This is a standard linear program-
ming problem, for which many methods of solution exist, such as the simplex method
([Press-88]).1 If a solution is found for which d >0 then this will be a desired solution.

Summary of the algorithm
Now, we give the complete algorithm for computing a quasi-afﬁne reconstruction of a
scene using the cheiral inequalities. The algorithm as outlined above was discussed for
the case of two views. In the present case it will be presented for an arbitrary number
of views. The extension to more views is straightforward.

Objective

Given a set of 3D points Xi and camera matrices Pj constituting a projective reconstruction
from a set of image points, compute a projective transformation H transforming the projective
to a quasi-afﬁne reconstruction.

Algorithm

(i) For each pair (i, j) such that point xj
(ii) Replace some cameras Pj by −Pj and some points Xi by −Xi as required to ensure

i is given let PjXi = w

j

i ˆxj
i .

that each w

j
i > 0.

(iii) Form the cheiral inequalities (21.5) where Cj = CPj is deﬁned by deﬁnition 21.10.
(iv) For each of the values δ = ±1, choose a solution (if it exists) to the set of cheiral
inequalities. Let the solution be vδ. A solution must exist for at least one value of δ,
sometimes for both values of δ.
(v) Deﬁne a matrix Hδ having last row equal to vδ and such that det(H) .= δ. The matrix
Hδ is the required transformation matrix. If both H+ and H− exist, then they lead to two
oppositely oriented quasi-afﬁne reconstructions.

Algorithm 21.1. Computing a quasi-afﬁne reconstruction.

Bounding the plane at inﬁnity
A quasi-afﬁne reconstruction is of course not unique, being deﬁned only up to a quasi-
afﬁne transformation with respect to the points and camera centres. However, once one
has been found, it is possible to set bounds on the coordinates of the plane at inﬁnity.
Thus, let Pj and Xi constitute a quasi-afﬁne reconstruction of a scene. One may choose
the sign of Pj and Xi such that the last coordinates of Xi and the determinants of each
Mj are positive. One may apply a translation to the points and cameras so that the
coordinate origin lies inside the convex hull of the points and camera centres. For
simplicity, the centroid of these points may be placed at the origin.

It is possible to apply a further quasi-afﬁne transformation H to obtain an alternative
reconstruction. Let π∞ be the plane mapped to inﬁnity by H. We conﬁne our interest
to orientation-preserving transforms, and wish to ﬁnd constraints on the coordinates
of π∞ such that H is quasi-afﬁne. A plane π∞ has this property if and only if it lies
entirely outside the convex hull of the points and camera centres. Since the plane π∞

1 The Simplex algorithm given in [Press-88] is not suitable for use as stands, since it makes the unnecessary assumption that all

variables are non-negative. It needs to be modiﬁed to be used for this problem.

21 Cheirality

528
cannot cross the convex hull, it cannot pass through the origin. Representing π∞ by
the vector v, this says that the last coordinate of v is non-zero. One may then write
v = (v1, v2, v3, 1)T. Since the origin lies on the same side of the plane as all the points,
the cheiral inequalities become

XT
v > 0 for all i
i
Cj Tv > 0 for all j

(21.6)

for v = (v1, v2, v3, 1)T. One may ﬁnd upper and lower bounds for each vi by solving
the linear programming problem to maximize vi or −vi subject to these constraints.
None of the vi can be unbounded, since otherwise the plane π∞ represented by the
vector v could lie arbitrarily close to the origin.

Before solving for this system, good practice is to apply an afﬁne transformation to
normalize the set of points and camera centres so that their centroid is at the origin and
their principal moments are all equal to 1.

The complete algorithm for computing the bounds on the position of the plane at

inﬁnity is given in algorithm 21.2.

Objective

Given a quasi-afﬁne reconstruction of a scene, establish bounds on the coordinates of the plane
at inﬁnity.

Algorithm

so that det Mj = 1.

(i) Normalize the points Xi = (Xi, Yi, Zi, Ti)T so that Ti = 1, and cameras Pj = [Mj | tj]
(ii) Further normalize by replacing Xi by H−1Xi and Pj by PjH, where H is an afﬁne trans-
formation moving the centroid to the origin and scaling in the principal axis directions
so that is has equal principal axes.

(iii) Letting v = (v1, v2, v3, 1)T, form cheiral inequalities (21.6). Any orientation-
preserving transformation H mapping the reconstruction to an afﬁne reconstruction of
the image must have the form

(cid:24)

(cid:25)

H =

I

v1 v2 v3

0
1

for a vector v satisfying these inequalities.

(iv) Upper and lower bounds for each vi may be found by running a linear programming
problem six times. The coordinates of a desired transformation H must lie inside the
box deﬁned by these bounds.

Algorithm 21.2. Establishing bounds on the plane at inﬁnity.

21.8 Which points are visible in a third view

Consider a scene reconstructed from two views. We consider now the question of
determining which points are visible in a third view. Such a question arises when one
is given two uncalibrated views of a scene and one seeks to synthesize a third view.
This can be done by carrying out a projective reconstruction of the scene from the

21.8 Which points are visible in a third view

529

π∞

P1

P2

P3

X

Fig. 21.3. The point set X is in front of all three cameras as shown. However, if an orientation-
preserving projective transformation H is applied taking the plane π∞ to inﬁnity, then the point set
will subsequently lie in front of the cameras P1 and P2 but behind the camera P3. Thus, suppose the
point set X is reconstructed from images captured by cameras P1 and P2, and let P3 be any other cam-
era matrix. If a plane exists separating the centre of camera P3 from the other camera centres, and not
meeting the convex hull of the point set X, then it cannot be determined whether the points lie in front
of P3.

ﬁrst two views and then projecting into the third view. In this case, it is important to
determine if a point lies in front of the third camera and is hence visible, or not.

If the third view is given simply by specifying the camera matrix with respect to
the frame of reference of some given reconstruction, then it may be impossible to
determine whether points are in front of the third camera or behind it in the true scene.
The basic ambiguity is illustrated in ﬁgure 21.3. Knowledge of a single point known
to be visible in the third view serves to break the ambiguity, however, as the following
result shows. By applying theorem 21.7(p519) to the ﬁrst and third views, one obtains
the following criterion.

Result 21.17. Let points (P1, P2,{Xi}) be a realization of a set of correspondences
j ˆxi = PiXj
j has the same sign for all points Xj visible in the third

i . Let P3 be the camera matrix of a third view and suppose that wi

↔ x2

x1
i
for i = 1, . . . ,3 . Then w1
view.

j w3

In practice, it will usually be the case that one knows at least one point X0 visible in
0, and any other point Xj will be in
0w3
0.
0w3

the third view. This serves to deﬁne the sign w1
.
front of the camera P3 if and only if w1
= w1

j w3
j

As an example, once a projective reconstruction has been carried out using two
views, the camera matrix of the third camera may be determined from the images of
six or more points known to be in front of it by solving directly for the matrix P3 given
the correspondences x3
i = P3Xi where points Xi are the reconstructed points. Then one
can determine unambiguously which other points are in front of P3.

530

21 Cheirality

x2

x1

π∞

Fig. 21.4. As shown, the point x1 is closer to the camera than x2. However, if an orientation-reversing
projectivity is applied, taking π∞ to inﬁnity, then both x1 and x2 remain in front of the camera, but x2
is closer to the camera than x1.

21.9 Which points are in front of which

When we are attempting to synthesize a new view of a scene that has been recon-
structed from two or more uncalibrated views it is sometimes necessary to consider
the possibility of points being obscured by other points. This leads to the question:
given two points that project to the same point in the new view, which one is closer
to the camera, and hence obscures the other? In the case where the possibility exists
of oppositely oriented quasi-afﬁne reconstructions it may once again be impossible to
determine which of a pair of points is closer to the new camera. This is illustrated in
ﬁgure 21.4. If a plane exists, separating the camera centres from the point set, then two
oppositely oriented reconstructions exist, and one cannot determine which points are
in front of which. The sort of ambiguity shown in ﬁgure 21.4 can only occur in the
case where there exists a plane π∞ that separates the camera centres from the set of all
visible points. If this is not the case, then one can compute a quasi-afﬁne reconstruc-
tion and the problem is easily solved. To avoid the effort of computing a quasi-afﬁne
reconstruction, however, we would like to solve this problem using only a projective
reconstruction of the scene. How this may be done is explained next.
−1(X; P) = 1/depth(X; P).
This inverse depth function is inﬁnite on the principal plane of the camera, zero on the
plane at inﬁnity, positive for points in front of the camera and negative for points behind
the camera. For notational simplicity, we write χ(X; P) instead of depth

One may invert (21.1–p518) to get an expression for depth

−1(X; P).

For points X lying on a ray through the camera centre, the value of χ(X; P) decreases
monotonically along this ray, from zero at the camera centre, decreasing through posi-
tive values to zero at the plane at inﬁnity, thence continuing to decrease through nega-
tive values to −∞ at the camera centre. A point X1 lies closer to the front of the camera
than X2 if and only if χ(X1) > χ(X2). This is illustrated in ﬁgure 21.5.

(cid:2)
deﬁned by χ

Now, if the conﬁguration undergoes an orientation-preserving transformation H tak-
ing the plane π∞ to inﬁnity, then the parameter χ will be replaced by a new parameter
(cid:2)
must also vary monotonically
χ
along the ray. Since H is orientation-preserving, points just in front of the camera cen-
tre will remain in front of the camera after the transformation, because of result 21.13.

(cid:2)
(X) =χ (HX; PH−1). The value of χ

531

21.10 Closure

χ'=0

χ<0

χ<0

χ'<0

x2

χ'>0

x1

π∞

 χ >0

Fig. 21.5. The parameter χ.

(cid:2)
Thus both χ and χ
and X2 are two points on the line, then χ

decrease monotonically in the same direction along the ray. If X1
(X2) if and only if χ(X1) > χ(X2).
In the case where the projective transformation has negative determinant, then the
front and back of the camera are reversed locally. In this case the direction of increase
(X2) if and only if
of the parameter χ
χ(X1) < χ(X2).

will be reversed. Consequently, χ

(cid:2)

(cid:2)
(X1) > χ

(cid:2)

(cid:2)

(cid:2)
(X1) > χ

In the case where the projective transformation transforms the scene to the “true”
scene, of two points that project to the same point in the image, the one with the higher
(cid:2)
value of χ
is closer to the camera. This leads to the following result that allows us to
determine from an arbitrary projective reconstruction which of two points is closer to
the front of the camera.
Result 21.18. Suppose that X1 and X2 are two points that map to the same point in
an image. Consider a projective reconstruction of the scene and let the parameter χ
be deﬁned (by formula (21.3–p521)) in the frame of the projective reconstruction. If
the projective reconstruction has the same orientation as the true scene, then the point
that lies closer to the front of the camera in the true scene is the one that has the
greater value of χ. On the other hand, if the projective transformation has the opposite
orientation, then the point with smaller value of χ will lie closer to the front of the
camera in the true scene.

As remarked previously, unless there exists a plane separating the point set from
the cameras used for the reconstruction, the orientation of the scene is uniquely de-
termined, and one can determine whether the projective transformation of result 21.18
has positive or negative determinant. However, to do this may require one to compute a
strong realization of the conﬁguration by the linear programming method as described
in section 21.7. If differently oriented strong realizations exist, then as illustrated by
ﬁgure 21.4, there is an essential ambiguity. However, this ambiguity may be resolved
by knowledge of the relative distance from the camera of a single pair of points.

21.10 Closure

21.10.1 The literature
The topic of this chapter belongs to Oriented projective geometry, which is treated
in a standard and readable text [Stolﬁ-91]. Laveau and Faugeras apply the ideas of

532

21 Cheirality

oriented projective geometry in [Laveau-96b]. The concepts of front and back of the
camera were also used in [Robert-93] to compute convex hulls in projective recon-
structions. This chapter derives from the paper [Hartley-98a] which also treats such
topics of invariants for quasi-afﬁne mappings and conditions under which arbitrary
correspondences allow quasi-afﬁne reconstructions.

Cheirality, and speciﬁcally the cheirality inequalities have been useful in determin-
ing quasi-afﬁne reconstructions as an intermediate step towards afﬁne and metric recon-
struction in [Hartley-94b, Hartley-99]. More recently Werner and Pajdla in [Werner-01]
have used oriented projective geometry to eliminate spurious line correspondences, and
to constrain correspondences of ﬁve points [Werner-03], over two views.

22

Degenerate Conﬁgurations

In past chapters we have given algorithms for the estimation of various quantities as-
sociated with multiple images – the projection matrix, the fundamental matrix and the
trifocal tensor. In each of these cases, linear and iterative algorithms were given, but
little consideration was given to the possibility that these algorithms could fail. We
now consider under what conditions this might happen.

Typically, if sufﬁciently many point correspondences are given in some sort of “gen-
eral position” then the quantities in question will be uniquely determined, and the algo-
rithms we have given will succeed. However, if there are too few point correspondences
given, or else all the points lie in certain critical conﬁgurations, then there will not be
a unique solution. Sometimes there will be a ﬁnite number of different solutions, and
sometimes a complete family of solutions.

This chapter will concentrate on three of the main estimation problems that we have
encountered in this book, camera resectioning, reconstruction from two views and re-
construction from three views. Some of the results given here are classical, particularly
the camera resectioning and two-view critical surface problems. Others are more recent
results. We consider the different estimation problems in turn.

22.1 Camera resectioning

We begin by considering the problem of computing the camera projection matrix, given
a set of points in space and the corresponding set of points in the image. Thus, one is
given a set of points Xi in space that are mapped to points xi in the image by a camera
with projection matrix P. The coordinates of the points in space and the image are
known, and the matrix P is to be computed. This problem was considered in chapter 7.
Before considering the critical conﬁgurations for this problem, we will digress to look
at an abstraction of the camera projection.

Cameras as points
Suppose the existence of a set of correspondences Xi ↔ xi. Let us suppose that there
is a unique camera matrix P such that xi = PXi. Now, let H be a matrix representing
a projective transformation of the image, and let x(cid:2)
i = Hxi be the set of transformed
image coordinates. Then it is clear that there is a unique camera matrix P(cid:2)
such that

533

22 Degenerate Conﬁgurations

534
i = P(cid:2)Xi, namely the camera matrix P(cid:2)
x(cid:2)
= HP. Conversely, if there exists more than
one camera matrix P mapping Xi to xi, then there exists more than one camera matrix
P(cid:2)
mapping Xi to x(cid:2)
i. Thus, the existence or not of a unique solution to the problem of
determining the projection matrix P is dependent on the image points xi only up to a
projective transformation H of the image.

Next, observe that applying a projective transformation H to a camera matrix P does
not change the camera centre. Speciﬁcally, the point C is the camera centre if and
only if PC = 0. However PC = 0 if and only if HPC = 0. Thus, the camera centre
is preserved by a projective transformation of the image. Next, we show that this is
essentially the only property of the camera that is preserved.
Result 22.1. Let P and P(cid:2)
be two camera matrices with the same centre. Then there
exists a projective image transformation represented by a non-singular matrix H such
that P(cid:2)

= HP.

Proof. If the centre C is not at inﬁnity, then the camera matrices are of the form P =
[M | −Mc] and P(cid:2)
= [M(cid:2) | −M(cid:2)c], where c is the inhomogeneous 3-vector representing the
a 3D projective transformation G such that GC is a ﬁnite point, say (cid:23)C. In this case, the
camera centre. Then clearly, P(cid:2)
= M(cid:2)M−1P. If C is a point at inﬁnity, then one chooses
two camera matrices PG−1 and P(cid:2)G−1 both have the same centre, namely (cid:23)C. It follows

that P(cid:2)G = HPG for some H. Cancelling G gives P(cid:2)

= HP.

This result may be interpreted as saying that an image is determined up to projectivity
by the camera centre alone. Thus, we see that in considering the problem of uniqueness
of the camera matrix, one may ignore all the parameters of the camera, except the
camera centre, since this alone determines the projectivity type of the image, and hence
the uniqueness or not of a solution.

Images as equivalence classes of rays
To gain insight into the critical conﬁgurations of camera resectioning, we turn ﬁrst to
consider 2-dimensional cameras, mapping IP2 to IP1. Consider a camera centre c and a
set of points xi in space. The rays cxi intersect an image line l at a set of points ¯xi; thus
points ¯xi are the images of the points xi. The projection of the points xi to points ¯xi
in the 1D image may be described by a 2 × 3 projection matrix as described in section
6.4.2(p175).

As shown in chapter 2, the projective equivalence type of the set of rays cxi is the
same as that of the image points ¯xi. This is illustrated in ﬁgure 22.1. Thus, instead of
considering an image as being the set of points on the image line, the image may be
thought of as the projective equivalence class of the set of rays from the camera centre
through each of the image points. In the case of just 4 image points, the cross ratio
of the points ¯xi (or equivalently, the rays) characterizes their projective equivalence
class. To give a speciﬁc notation, we denote by (cid:8)c; x1, . . . ,x i, . . . xn(cid:9) the projective
equivalence class of the set of rays cxi.
The same remarks are valid for projections of 3D points into a 2-dimensional image.
One may also extend the above notation by writing (cid:8)C; X1, . . . , Xn(cid:9) to represent the

22.1 Camera resectioning

l

x 1

535

x2

x

1

x

2

x

3

x

4

x 3

x 4

c

Fig. 22.1. Projection of points in the plane using a 2D camera. A 1D image is formed by the intersection
of the rays li = cxi with the image line l. The set of image points {¯xi} is projectively equivalent to the
set of rays {li}. For four points, the projective equivalence class of the image is determined by the cross
ratio of the points.

projective equivalence class of all the rays CXi. As in the 2-dimensional case, this is
an abstraction of the projection of the points Xi relative to a camera with centre at C.
We will be considering conﬁgurations of camera centres and 3D points, which will be
denoted by {C1, . . . , Cm; X1, . . . , Xn} or variations thereof. Implicit is that the symbols
appearing before the semicolon are camera centres, and those that come after are 3D
points. In order to make the statements of derived results simple, the concept of image
equivalence is deﬁned.

Deﬁnition 22.2. Two conﬁgurations

{C1, . . . , Cm; X1, . . . , Xn} and {C(cid:2)

1, . . . , C(cid:2)
are called image-equivalent if (cid:8)Ci; X1, . . . , Xn(cid:9) = (cid:8)C(cid:2)
1, . . . , m.

n

n

m; X(cid:2)
i; X(cid:2)

1, . . . , X(cid:2)
1, . . . , X(cid:2)

}
(cid:9) for all i =

The concept of image equivalence is distinct from projective equivalence of the sets
of points and camera centres involved. Indeed, the relevance of this to reconstruction
ambiguity is that if a conﬁguration {C1, . . . , Cm; X1, . . . , Xn} allows another image-
equivalent set which is not projective equivalent, then this amounts to an ambiguity of
the projective reconstruction problem, since the projective structure of the points and
cameras is not uniquely deﬁned by the set of images. In this case, we say that the
conﬁguration {C1, . . . , Cm; X1, . . . , Xn} allows an alternative reconstruction.

22.1.1 Ambiguity in 2D – Chasles’ theorem
Before considering the usual 3D cameras, we discuss the simpler case of 2D cameras.
The analysis of the uniqueness of 2D camera projections involves planar conics. Ambi-
guity in determining the camera centre from the projection of a set of known points xi
means that the projection of the points is the same from two different camera centres
c and c(cid:2)
. The question is for what conﬁgurations of the points this may occur. The
answer to this question is given by Chasles’ Theorem.

536

22 Degenerate Conﬁgurations

x2

x1

x3

xn

c'

c

Fig. 22.2. The points x1, . . . , xn project in equivalent ways to the two camera centres c and c(cid:1)

.

Theorem 22.3 Chasles’ Theorem. Let xi be a set of n points and c and c(cid:2)
centres, all lying in a plane. Then

two camera

(cid:8)c; x1, . . . ,x n(cid:9) = (cid:8)c(cid:2)

; x1, . . . ,x n(cid:9)

if and only if one of the following conditions holds

(i) The points c, c(cid:2)
(ii) All points lie on the union of two lines (a degenerate conic), both camera centres

and all xi all lie on a non-degenerate conic, or

lying on the same line.

Note that as a simple corollary of this theorem, if c(cid:2)(cid:2)

These two conﬁgurations are shown in ﬁgure 22.2.
is any other camera centre lying
on the same component of a conic (degenerate or non-degenerate) as c, c(cid:2)
and the xi,
then the projection of the points to the centre c(cid:2)(cid:2)
is equivalent to their projection to the
original camera centres. Furthermore, any number of extra points xj may be added
without breaking the ambiguity, as long as they lie on the same conic.

22.1.2 Ambiguity for 3D cameras
We now address the problem of ambiguity of camera resection in the case of full 3D
cameras. Twisted cubics (described in section 3.3(p75)) play an analogous role in the
3D case to that played by conics in the case of 2D cameras. The degenerate form
of a twisted cubic consisting of a conic plus a line intersecting the conic arises in this
context; so does the degenerate cubic consisting of three lines. As in the 2D case, when
ambiguity arises from points lying on a degenerate cubic, the camera centres must both
lie on the same component.

A complete classiﬁcation of the point and camera conﬁgurations leading to ambigu-
ous camera resectioning is given in ﬁgure 22.3 and also in the following deﬁnition. For
the present we describe the geometric conﬁgurations. The exact relevance to ambigu-
ous camera conﬁgurations will be given afterwards.

Deﬁnition 22.4.
A critical set for camera resectioning consists of two parts:

(i) An algebraic curve C containing the camera centres, plus any number of the 3D

22.1 Camera resectioning

537

points. This curve may be

(a) a non-degenerate twisted cubic (degree 3),
(b) a planar conic (degree 2),
(c) a straight line (degree 1), or
(d) a single point (degree 0).

(ii) A union of linear subspaces L (lines or planes) containing any number of 3D

points.

The curve C and the linear subspaces satisfy three conditions:
(i) Each of the linear subspaces must meet the curve C.
(ii) The sum of the degree of the curve C and the dimensions of the linear subspaces
(iii) Except in the case where C is a single point, the cameras do not lie at the inter-

is at most 3.
section point of C and the linear subspaces.

The various possibilities are shown in ﬁgure 22.3, and it is easily veriﬁed that these
completely enumerate all conﬁgurations in accordance with deﬁnition 22.4.

Result 22.5. The different possible critical sets for camera resectioning are:

ﬁrst line.

(i) A non-degenerate twisted cubic C (degree 3).
(ii) A plane conic C (degree 2), plus a line (dimension 1) that meets the conic.
(iii) A line C (degree 1) plus up to two other lines (total dimension 2) that meet the
(iv) A line C (degree 1), plus a plane (dimension 2).
(v) A point C (degree 0) and up to three lines passing through the point (total di-
(vi) A point C (degree 0) and a line and a plane both passing through this point.
The exact relationship of these critical sets to ambiguous camera resectioning is

mension 3).

given by the following result.
Result 22.6. Let P and P(cid:2)
be two different camera matrices with camera centres C0 and
C1. Then the two camera centres and the points Xi that satisfy PXi = P(cid:2)Xi all lie on a
critical set as given by deﬁnition 22.4.
Furthermore, if Pθ = P + θP(cid:2)
has rank 31, then the camera centre of the camera
deﬁned by Pθ lies on the component C of the critical set containing the two original
camera centres C0 and C1, and PθX = PX = P(cid:2)X for any point X in the critical set.

Conversely, let P be a camera matrix with centre C0. Let C0 and a set of 3D points
Xi lie in a critical set for camera resectioning. Let C1 be any other point lying on
the component C of the critical set, with C1 (cid:5)= C0 unless C consists of a single point.
Then there exists a camera matrix P(cid:2)
(different from P) with camera centre C1 such that
PXi = P(cid:2)Xi for all points Xi.
1 We include the case P∞ = P(cid:1)

.

538

22 Degenerate Conﬁgurations

Fig. 22.3. The different critical conﬁgurations for camera resectioning from a single view. The open
circles represent centres of projection and the ﬁlled circles represent points. Each case consists of an
algebraic curve (or single point) C containing the camera centres, plus a set of linear subspaces (lines
or planes).

Proof. An outline of the proof will be given, with details left for the reader to ﬁll in.
We temporarily need to distinguish between equality of homogeneous quantities up to
a scale factor (which will be denoted by ≈) and absolute equality (which is denoted
by =). Suppose that X maps to the same image point with respect to cameras P and
. One can write PX ≈ P(cid:2)X. Taking account of the scale factor, this can be written
P(cid:2)
as PX = −θP(cid:2)X for some constant θ. From this it follows that (P + θP(cid:2)
)X = 0.
)X = 0. for some θ. It follows that PX = −θP(cid:2)X,
Conversely, suppose that (P + θP(cid:2)
and so PX ≈ P(cid:2)X. Thus, the critical set is the set of points X in the right null-space of
P + θP(cid:2)
Deﬁne Pθ = P + θP(cid:2)
. The rest of the proof involves ﬁnding the set of all points X
satisfying PθX = 0 as θ runs over all values. If Pθ is a camera matrix (has rank 3) then
such an X is the centre of the camera Pθ. If however Pθ is rank-deﬁcient for some value
X = 0 is a linear space. The total critical set
of θi, then the set of points X such that Pθi
therefore consists of two parts:

for some θ.

22.2 Degeneracies in two views

539

(i) The locus of the camera centre of Pθ over all values θ for which Pθ has full rank
(that is rank 3). This is a curve C containing the two camera centres C0 and C1.
(ii) A linear space (line or plane) corresponding to each value of θ for which Pθ has
rank 2 or less. If Pθ has rank 2, the points such that PθX = 0 form a line, and if
it has rank 1, then they form a plane.

θ

θ

is the matrix Pθ with the i-th column removed. Since each P(i)
θ

Let the 4-vector Cθ be deﬁned by Cθ = (c1, c2, c3, c4)T, where ci = (−1)i det P(i)
is a 3 × 3
and P(i)
matrix, and the entries of Pθ are linear in θ, we see that each ci = (−1)i det P(i)
θ
is a
cubic polynomial in θ. According to the discussion following deﬁnition 21.10(p521),
PθCθ = 0, hence if Cθ (cid:5)= 0 then it is the camera centre of Pθ, and as θ varies the point
Cθ traces out the curve C. Since the coordinates of Cθ are cubic polynomials, this is
in general a twisted cubic. If however the four components of Cθ have a simultaneous
root θi, then the degree of the curve Cθ is diminished. In this case Pθi is rank-deﬁcient,
X = 0. Thus the linear subspace
and there exists a linear space of points X such that Pθi
components of the critical set correspond to values of θ where Cθ vanishes. Clearly
there can be at most three such values. Further details are left to the reader.
The last part of the theorem provides a converse result – namely that if the points and
one camera centre lie in a critical conﬁguration, then there exist alternative resection
solutions with the camera placed at any location in C. The exact form of the camera
matrix P is not important here, only its camera centre, as has been observed above.
For most of the conﬁgurations in ﬁgure 22.3 it is clear enough geometrically that the
image of the critical set is unchanged (up to projectivity) by moving the camera along
the locus C. In the case where C is a planar conic, this follows fairly easily from the 1D
camera case (theorem 22.3). The exception is the twisted cubic case. It is illustrated
graphically in ﬁgure 22.4. We leave this proof for now, and come back to it later (result
22.25(p551)).

22.2 Degeneracies in two views

Notation. For the rest of this chapter, the camera matrices are represented by P and Q,
3D points by P and Q. Thus cameras and 3D points are distinguished only by their type-
face. This may appear to be a little confusing, but the alternative of using subscripts or
primes proved to be much more confusing. In the context of ambiguous reconstructions
from image coordinates we distinguish the two reconstructions by using P and P for
one, and Q and Q for the other.

Now we turn to the case of two views of an object. Given sufﬁciently many points
placed in “general position” one may determine the placement of the two cameras, and
reconstruct the point set up to a projective transformation. This may be done using one
of the projective reconstruction algorithms discussed in chapter 10. We now wish to
determine under what conditions this technique may fail.
Thus, we consider a set of image correspondences xi ↔ x(cid:2)
set of correspondences consists of a pair of camera matrices P and P(cid:2)
points Pi such that xi = PPi and x(cid:2)

i = P(cid:2)Pi for all i. Two realizations {P, P(cid:2)

i. A realization of this
and a set of 3D
, Pi}

540

22 Degenerate Conﬁgurations

Fig. 22.4. Separate views of a set of points on the twisted cubic (t3, t2, t,1) T as viewed from a centre
of projection. The visible points are viewed from the points with t = 3, 4, 5, 10, 50, 1000 with the image
suitably magniﬁed to prevent the point set from becoming too small. As is plausible from the image, the
sets of points differ by a projective transformation. From a viewpoint on a twisted cubic, the twisted
cubic has the appearance of a conic, in this particular case a parabola.

and {Q, Q(cid:2)
represented by a matrix H such that Q = PH−1, Q(cid:2)

, Qi} are projectively equivalent if there is a 3D projective transformation,

= P(cid:2)H−1, and Qi = HPi for all i.

Because of a technicality, this deﬁnition of equivalence is not quite appropriate to the
present discussion. Recall from the projective reconstruction theorem, theorem 10.1-
(p266), that one cannot determine the position of a point lying on the line joining the
two camera centres. Hence, non-projectively-equivalent reconstructions will always
exist if some points lie on the line of camera centres, the two reconstructions differing
only by the position of the points Pi and Qi on this line. This type of reconstruction
ambiguity is not of great interest, and so we will modify the notion of equivalence
by deﬁning two reconstructions to be equivalent if H exists such that Q = PH−1 and
Q(cid:2)
= P(cid:2)H−1. As in the proof of the projective reconstruction theorem, such an H will
also map Qi to Pi, except possibly for reconstructed points Qi lying on the line of the
camera centres. According to theorem 9.10(p254), this condition is also equivalent to
the condition that FP(cid:1)P = FQ(cid:1)Q, where FP(cid:1)P and FQ(cid:1)Q are the fundamental matrices corre-
sponding to the camera pairs (P, P(cid:2)
). Accordingly, we make the following
deﬁnition.
Deﬁnition 22.7. Two conﬁgurations of cameras and points {P, P(cid:2)
are said to be conjugate conﬁgurations provided

, Pi} and {Q, Q(cid:2)

) and (Q, Q(cid:2)

, Qi}

(i) PPi = QQi and P(cid:2)Pi = Q(cid:2)Qi for all i.
(ii) The fundamental matrices FQ(cid:1)Q and FQ(cid:1)Q corresponding to the two camera matrix

pairs (P, P(cid:2)

) and (Q, Q(cid:2)

) are different.

A conﬁguration {P, P(cid:2)

, Pi} that allows a conjugate conﬁguration is called critical.

An important remark is that being a critical conﬁguration depends only on the camera

22.2 Degeneracies in two views

541

respectively, then {ˆP, ˆP

(cid:2)
, Pi} is a critical conﬁguration and ˆP and ˆP

centres and the points, and not on the particular cameras.
Result 22.8. If {P, P(cid:2)
the same centres as P and P(cid:2)
well.
Proof. This is easily seen as follows. Since {P, P(cid:2)
, Pi} is a critical conﬁguration there
exists an alternative conﬁguration {Q, Q(cid:2)
, Qi} such that PPi = QQi and P(cid:2)Pi = Q(cid:2)Qi
for all i. However, since P and ˆP have the same camera centre, ˆP = HP according to
result 22.1 and similarly ˆP

are two cameras with
, Pi} is a critical conﬁguration as

(cid:2)

(cid:2)

. Therefore

= H(cid:2)P(cid:2)
ˆPPi = HPPi = HQQi and
(cid:2)
Pi = H(cid:2)P(cid:2)Pi = H(cid:2)Q(cid:2)Qi.
ˆP

It follows that {HQ, H(cid:2)Q(cid:2)
fore critical.

(cid:2)
, Qi} is an alternative conﬁguration to {ˆP, ˆP

, Pi}, which is there-

The goal of this section is to determine under what conditions non-equivalent re-
alizations of a set of point correspondences may occur. This question is completely
resolved by the following theorem, which will be proved incrementally.

, Ri}.

(i) Conjugate conﬁgurations of cameras and points generically
, Pi} has two conjugates
, Qi} and {R, R(cid:2)
, Pi} is a critical conﬁguration, then all the points Pi and the two cam-

Theorem 22.9.
come in triples. Thus a critical conﬁguration {P, P(cid:2)
{Q, Q(cid:2)
(ii) If {P, P(cid:2)
(iii) Conversely, suppose that the camera centres of P, P(cid:2)

and a set of 3D points Pi
lie on a ruled quadric (excluding the quadrics (v) and (viii) in result 22.11),
then {P, P(cid:2)

era centres CP and CP(cid:1) lie on a ruled quadric surface SP.

, Pi} is a critical conﬁguration.

By the words “ruled quadric” in this context is meant any quadric surface that con-
tains a straight line. This includes various degenerate quadrics, as will be seen. A gen-
eral discussion and classiﬁcation of quadric surfaces has been given in section 3.2.4-
(p74). A quadric is usually deﬁned to be the set of points X such that XTSX = 0,
where S is a symmetric 4 × 4 matrix. However, if S is any 4 × 4 matrix, not neces-
sarily symmetric, then one sees that for any point X, one has XTSX = (XTSsymX),
where Ssym = (S + ST)/2 is the symmetric part of S. Thus, XTSX = 0 if and only
if XTSsymX = 0, and so S deﬁnes the same quadric surface as Ssym. In investigating
reconstruction ambiguity, it will often be convenient to use non-symmetric matrices S
to represent quadrics.
Proof. We begin by proving the ﬁrst part of the theorem. Let F and F(cid:2)
be two distinct
fundamental matrices satisfying the relation x(cid:2)
F(cid:2)xi = 0 for all correspon-
dences x(cid:2)
TFθxi = 0. However,
Fθ is a fundamental matrix only if det Fθ = 0. Now, det F(θ) is generally a polynomial
of degree 3 in θ. This polynomial has roots θ = 0 and θ = 1 corresponding to F and

↔ xi. Deﬁne Fθ = F + θF(cid:2)

. One easily veriﬁes that x(cid:2)

TFxi = x(cid:2)

i

i

i

i

22 Degenerate Conﬁgurations

542
F(cid:2)
respectively. The third root corresponds to a third fundamental matrix, and hence
a third non-equivalent reconstruction. In special cases, det F(θ) has degree 2 in θ and
there are only two conjugate conﬁgurations.

The second part of the theorem is concluded by proving the following lemma.

Lemma 22.10. Consider two pairs of cameras (P, P(cid:2)
), with corresponding
different fundamental matrices FP(cid:1)P and FQ(cid:1)Q. Deﬁne quadrics SP = P(cid:2)TFQ(cid:1)QP, and SQ =
Q(cid:2)TFP(cid:1)PQ.

) and (Q, Q(cid:2)

(i) The quadric SP contains the camera centres of P and P(cid:2)

the camera centres of Q and Q(cid:2)

.

. Similarly, SQ contains

(ii) SP is a ruled quadric.
(iii) If P and Q are 3D points such that PP = QQ and P(cid:2)P = Q(cid:2)Q, then P lies on the

(iv) Conversely, if P is a point lying on the quadric SP, then there exists a point Q

quadric SP, and Q lies on SQ.
lying on SQ such that PP = QQ and P(cid:2)P = Q(cid:2)Q.

Proof. Recall that according to result 9.12(p255) the matrix F is the fundamen-
tal matrix corresponding to a pair of cameras (P, P(cid:2)
) if and only if P(cid:2)TFP is skew-
symmetric. Since FP(cid:1)P (cid:5)= FQ(cid:1)Q, however, the matrices SP and SQ deﬁned here are not
skew-symmetric, and hence represent non-trivial quadrics.
We adopt a convention that a camera represented by a matrix such as P or P(cid:2)
centre denoted by CP or CP(cid:1).

has camera

(i) The camera centre of P satisﬁes PCP = 0. Then
P (P(cid:2)TFQ(cid:1)QP)CP = CT

P SPCP = CT

CT

P (P(cid:2)TFQ(cid:1)Q)PCP = 0

since PCP = 0. So, CP lies on the quadric SP. In a similar manner, CP(cid:1) lies on
SP.

(ii) Let eQ be the epipole deﬁned by FQ(cid:1)QeQ = 0, and consider the ray passing
through CP consisting of all points such that eQ = PX. Then for any such point,
one veriﬁes that SPX = P(cid:2)TFQ(cid:1)QPX = P(cid:2)TFQ(cid:1)QeQ = 0. Thus the ray lies on SP and
so SP is a ruled quadric.

(iii) Under the given conditions one sees that

PTSPP = PTP(cid:2)TFQ(cid:1)QPP = QT(Q(cid:2)TFQ(cid:1)QQ)Q = 0

(iv) Let P lie on SP and deﬁne x = PP and x(cid:2)

since Q(cid:2)TFQ(cid:1)QQ is skew-symmetric. Thus, P lies on the quadric SP. By a similar
argument, Q lies on SQ.
= P(cid:2)P. Then, from PTSPP = 0
we deduce 0 = PTP(cid:2)TFQ(cid:1)QPP = x(cid:2)TFQ(cid:1)Qx, and so x ↔ x(cid:2)
are a corresponding
pair of points with respect to FQ(cid:1)Q. Therefore, there exists a point Q such that
QQ = x = PP, and Q(cid:2)Q = x(cid:2)
= P(cid:2)P. From part (iii) of this lemma, Q must lie
on SQ.

22.2 Degeneracies in two views

543

This lemma completely describes the sets of 3D points giving rise to ambiguous
image correspondences. Note that any two arbitrarily chosen camera pairs can give
rise to ambiguous image correspondences, provided that the world points lie on the
given quadrics.

22.2.1 Examples of ambiguity
At this point it remains to prove the converse of theorem 22.9. This needs to be done
for all types of ruled quadrics and any placement of the two camera centres on the
quadric. The different types of ruled quadrics, including degenerate cases, are (see
section 3.2.4(p74)): a hyperboloid of one sheet; a cone; two (intersecting) planes; a
single plane; a single line. A complete enumeration of the types of placement of two
camera centres on a ruled quadric is given in the following result.

Result 22.11. The possible conﬁgurations of two distinct points (particularly camera
centres) on a ruled quadric surface are as enumerated:

(i) hyperboloid of one sheet, two points not on the same generator
(ii) hyperboloid of one sheet, two points on the same generator
(iii) cone, one point at the vertex and one not
(iv) cone, two points on different generators, neither one at the vertex
(v) cone, two points on the same generator, neither one at the vertex
(vi) pair of planes, two points on the intersection of the two planes
(vii) pair of planes, one point at the intersection and one not
(viii) pair of planes, neither point at the intersection, but points on different planes
(ix) pair of planes, neither point on the intersection, both points on the same plane
(x) single plane with two points
(xi) single line with two points

Any two quadric/point-pairs in the same class are projectively equivalent.

It is clear by enumeration of cases that this list is complete. The only case in which
it is not immediately obvious that any two conﬁgurations in the same category are
projectively equivalent is in the non-degenerate case of the hyperboloid of one sheet.
The proof of this fact is left as exercise (i) at the end of the chapter (page 559).
Now, consider an example of a critical conﬁguration {P, P(cid:2)
on a quadric SP also containing the two camera centres. The quadric and two camera
centres belong to one of the categories given in result 22.11.

(cid:2)

,(cid:23)Pi} in which all Pi lie
) and a set of points (cid:23)Pi lie on a quadric
,(cid:23)Pi} is a critical conﬁguration,

Let the centres of two new cameras (ˆP, ˆP

. Since the points (cid:23)Pi lie on SP, it follows that {P, P(cid:2)

ˆSP. Suppose that ˆSP and the two camera centres lie in the same category as the given
example. Since two conﬁgurations in the same category are projectively equivalent, we
may assume that ˆSP = SP and the centres of P and ˆP are the same, as are the centres of P(cid:2)
(cid:2)
and ˆP
and hence by result 22.8 so is {ˆP, ˆP
This shows that to demonstrate the converse section of theorem 22.9 it is sufﬁcient
merely to demonstrate an example of a critical conﬁguration belonging to each of the
categories given in result 22.11 (except for cases (v) and (viii)). Examples will be given

,(cid:23)Pi}.

(cid:2)

544

22 Degenerate Conﬁgurations

for several of the categories, though not all. The fact that cases (v) and (viii) are not
critical is not shown here. The remaining cases are left to the motivated reader.

Examples of critical conﬁgurations
Consider the case P = Q = [I | 0] and P(cid:2)

(cid:17)

(cid:18)

= [I | t0]. In this case, one sees that

SP =

Consequently,

I
tT
0

FQ(cid:1)Q[I | 0] =
(cid:17)

SPsym =

1
2

FQ(cid:1)Q + FT
FQ(cid:1)Q

Q(cid:1)Q FT
Q(cid:1)Qt0
0

tT
0

(cid:17)

(cid:18)

I
tT
0

[FQ(cid:1)Q | 0] .

(cid:18)

.

Given the fundamental matrix FQ(cid:1)Q of rank 2, and the camera matrix Q = [I | 0], one
may easily ﬁnd the other camera matrix Q(cid:2)
. This is done by using the formula of result
9.13(p256). We now consider several examples of critical conﬁgurations belonging to
different categories of result 22.11.

Example 22.12. Hyperboloid of one sheet – two centres not on the same generator. We

choose FQ(cid:1)Q =

ﬁnd that

 1

0
1

1
0
0
−1 0 −1

 and t0 = (0, 2, 0)T. Then tT
 .
 1

1−1

1

0

Ssym =

1

0

FQ(cid:1)Q = (0, 2, 0) and we

This is the quadric with equation X2 + Y2 +2Y− Z2 = 0, or X2 +(Y +1)2− Z2 = 1. This
is a hyperboloid of one sheet. Note that in this case, the camera centres are CP = CQ =
(0, 0, 0)T (in inhomogeneous coordinates). The camera centre CP(cid:1) = (0, 2, 0). We note
that the line from CP to CP(cid:1) does not lie on the quadric surface. The camera centre
CQ(cid:1) is not uniquely determined by the information already given, since the fundamental
matrix does not uniquely determine the two cameras. However, the epipole e such that
FQ(cid:1)Qe = 0 is e = (1, 0,−1). Since e = QCQ(cid:1), we see that CQ(cid:1) = (1, 0,−1, k
−1). In
inhomogeneous coordinates CQ(cid:1) = k(1, 0,−1) for any k. We verify that the complete
line from CP to CQ(cid:1) lies on the quadric, but CQ(cid:1) may lie anywhere along this line. (cid:2)
Example 22.13. Hyperboloid of one sheet, both centres on the same generator. We
choose the same FQ(cid:1)Q as in the previous example, and and t0 = (3, 4, 5)T. Then tT
FQ(cid:1)Q =
(−2, 4,−2) and we ﬁnd that
0

 1

−1
2
−1 −1
0

1

−1 2 −1

 .

Ssym =

This is the quadric with equation (X−1)2 +(Y+2)2−(Z+1)2 = 4, which is once again

22.2 Degeneracies in two views

545

a hyperboloid of one sheet. It may be veriﬁed that the line (X, Y, Z) = (3t, 4t, 5t) lies

entirely on the quadric and contains the two camera centres, (0, 0, 0)T and (3, 4, 5)T.(cid:2)
Example 22.14. Cone – one centre at the vertex of the cone. We choose FQ(cid:1)Q to be the
same as in the previous example, but t0 = (1, 0, 1)T. In this case we see that tT
FQ(cid:1)Q =
0T, and so Ssym = diag(1, 1,−1, 0). This is an example of a cone. The camera centre
0
(cid:2)
of both P and Q is at the vertex of the cone, namely the point CP = (0, 0, 0, 1)T.

Example 22.15. Cone – neither centre at the vertex of the cone. We choose FQ(cid:1)Q =
diag(1, 1, 0) and t0 = (0, 2, 0)T. In this case, we see that

 1



Ssym =

1

1

0

1

0

This is the quadric with equation x2 + y2 + 2y = 0, or x2 + (y + 1)2 = 1, which is a
cylinder parallel with the Z-axis, thus projectively equivalent to the cone with vertex at
(cid:2)
the inﬁnite point (0, 0, 1, 0)T. Neither of the camera centres lies at the vertex.
Example 22.16. Two planes. We choose FQ(cid:1)Q = diag(1,−1, 0) and t0 = (0, 0, 1)T,
FQ(cid:1)Q = 0T. In this case, we see that SP = Ssym = diag(1,−1, 0, 0), which
so that tT
0
represents a pair of planes. In this case the camera centres are on the intersection of the
(cid:2)
two planes

 1

1 0
−1 0 0
0 0
0

 and t0 = (0, 0, 1)T.

Example 22.17. Single plane. We choose FQ(cid:1)Q =

(cid:2)
Then Ssym = diag(1, 0, 0, 0), which represents a single plane, x = 0.
Example 22.18. Single line. We choose FQ(cid:1)Q = diag(1, 1, 0) and t0 = (0, 0, 1)T. In this
case, we see that Ssym = diag(1, 1, 0, 0), which represents a single line – the Z-axis.
(cid:2)
All the points and the two camera centres lie on this single line.

Apart for the impossible cases (v) and (viii) this gives examples of all possible de-
generate conﬁgurations except for cases (vii) and (ix) in result 22.11. These remaining
cases are left for the reader.

Minimal case – seven points As seen in section 11.1.2(p281), seven is the minimum
number of points in general position from which one can do projective reconstruction.
The method comes down to solving a cubic equation, for which either one or three
real solutions exist. Looked at from the point of view of critical surfaces, the seven
points and two camera centres in one conﬁguration must lie on a quadric surface (since
9 points lie on a quadric). If this quadric is ruled, then there will be three conjugate
solutions. On the other hand, if it is not a ruled quadric (for instance an ellipsoid) then
there will be only one solution. This shows that the distinction between the cases where
the cubic equation has one or three solutions arises from the difference between the

546

22 Degenerate Conﬁgurations

points and camera centres lying on a ruled or unruled quadric – a pleasing connection
between the algebra and geometry.

22.3 Carlsson–Weinshall duality

The duality explored in chapter 20 between cameras and points may be exploited so
as to dualize degeneracy results, as will be explained in this section. We give a more
formal treatment of Carlsson–Weinshall duality here.

The basis of Carlsson–Weinshall duality is the equation
−T
−T
Z −T

−d
−d
c −d

Y

b

 =

 X

 a



 X

Y
Z
T

 .



 a

b
c
d

camera

matrix
−1, b

The
on
to
−1)T. We are interested in describing cameras
−1, d
a camera with centre (a
in terms of their camera centres. As this shows, in swapping camera centres with 3D
points, one must invert the coordinates of the camera centre and point. For instance,
the point (X, Y, Z, T)T is dual to a camera with camera centre (X

corresponds

−1)T.

−1, Y

−1, Z

−1, T

−1, c

left

the

We denote the reduced camera matrix

 a

−1

P =

−1

b



−d
−d
−1 −d

−1
−1
−1

c

(22.1)

(22.2)

−1, Z

−1, Y

with centre C = (a, b, c, d)T by PC.
−1, d
−1, c
(X
PCX = PXC

−1)T and C = (a

−1, T

−1, b

Now, deﬁning the points X =
−1)T, one immediately veriﬁes that

Thus, this transformation interchanges the results of 3D points and camera centres.
Thus, a camera with centre C acting on X gives the same result as camera with centre
X acting on C.

This observation leads to the following deﬁnition

Deﬁnition 22.19. The mapping of IP3 to itself given by

(X, Y, Z, T)T (cid:6)→ (YZT, ZTX, TXY, XYZ)T

will be called the Carlsson–Weinshall map, and will be denoted by Γ. We denote the
image of a point X under Γ by X. The image of an object under Γ is sometimes referred
to as the dual object.

The Carlsson–Weinshall map is an example of a Cremona transformation. For more
information on Cremona transformations, the reader is referred to Semple and Knee-
bone ([Semple-79]).
Note. If none of the coordinates of X is zero then we may divide X by XYZT. Then Γ
is equivalent to (X, Y, Z, T)T (cid:6)→ (X
−1)T. This is the form of the mapping

−1, Y

−1, Z

−1, T

22.3 Carlsson–Weinshall duality

547
that we will usually use. In the case where one of the coordinates of X is zero, then
the mapping will be interpreted as in the deﬁnition. Note that any point (0, Y, Z, T)T is
mapped to the point (1, 0, 0, 0)T by Γ, provided none of the other coordinates is zero.
Thus, the mapping is not one-to-one.

If two of the coordinates of X are zero, then X = (0, 0, 0, 0)T, which is an unde-
ﬁned point. Thus, Γ is not deﬁned at all points. In fact, there is no way to extend Γ
continuously to such points.

Deﬁne the reference tetrahedron to be the tetrahedron with vertices E1 =
(1, 0, 0, 0)T, E2 = (0, 1, 0, 0)T, E3 = (0, 0, 1, 0)T and E4 = (0, 0, 0, 1)T. As we have
just seen, Γ is one-to-one other than on the faces of the reference tetrahedron. It maps
a face of the reference tetrahedron to the opposite vertex, and is undeﬁned on the edges
of the reference tetrahedron. Next, we investigate the way in which Γ acts on other
geometric objects.

Theorem 22.20. The Carlsson–Weinshall map, Γ acts in the following manner:

(i) It maps a line passing through two general points X0 and X1 to the twisted

cubic passing through X0, X1 and the four reference vertices E1, . . . , E4.

(ii) It maps a line passing through any of the points Ei to a line passing through
the same Ei. We exclude the lines lying on the face of the reference tetrahedron,
since such lines will be mapped to a single point.

(iii) It maps a quadric S passing through the four points Ei, i = 1, . . . ,4 to a
If S is a ruled

quadric (denoted ¯S) passing through the same four points.
quadric, then so is ¯S. If S is degenerate then so is ¯S.

Proof.
(i) A line has parametric equation (X0 + aθ, Y0 + bθ, Z0 + cθ, T0 + dθ)T, and a point
on this line is taken by the Carlsson–Weinshall map to the point

T
((Y0 + bθ)(Z0 + cθ)(T0 + dθ), . . . ,( X0 + aθ)(Y0 + bθ)(Z0 + cθ))

.

Thus, the entries of the vector are cubic functions of θ, and the curve is a twisted cubic.
Now, setting θ = −X0/a, the term (X0 + aθ) vanishes, and the corresponding dual
T ≈ (1, 0, 0, 0)T. The ﬁrst entry is the
point is ((Y0 + bθ)(Z0 + cθ)(T0 + dθ), 0, 0, 0)
only one that does not contain (X0 + aθ), and hence the only one that does not vanish.
This shows that the reference vertex E1 = (1, 0, 0, 0)T is on the twisted cubic. By
similar arguments, the other points E2, . . . , E4 lie on the twisted cubic also. Note that a
twisted cubic is deﬁned by six points, and this twisted cubic is deﬁned by the given six
points Ei, X0, X1 that lie on it, where X0 and X1 are any two points deﬁning the line.
(ii) We prove this for lines passing through the point E0 = (1, 0, 0, 0)T. An analogous
proof holds for the other points Ei. Choose another point X = (X, Y, Z, T)T on the
line, such that X does not lie on any face of the reference tetrahedron. Thus X has no
zero coordinate. Points on a line passing through (1, 0, 0, 0)T and X = (X, Y, Z, T)T
are all of the form (α, Y, Z, T)T for varying values of α. These points are mapped by
−1)T. This represents a line passing through the
−1, Z
the transformation to (α
−1, Z
two points (1, 0, 0, 0)T and X = (X

−1, T
−1, Y

−1)T.

−1, Y

−1, T

22 Degenerate Conﬁgurations

548
(iii) Since the quadric S passes through all the points Ei, the diagonal entries of S must
all be zero. This means that there are no terms involving a squared coordinate (such as
X2) in the equation for the quadric. Hence the equation for the quadric contains only
mixed terms (such as XY, YZ or XT). Therefore, the quadric S may be deﬁned by an
equation aXY + bXZ + cXT + dYZ + eYT + f ZT = 0. Dividing this equation by XYZT,
−1 = 0.
−1 + cY
we obtain aZ
−1)T, this is a quadratic equation in the entries of X. Thus
Since X = (X
Γ maps quadric to quadric. Speciﬁcally, suppose S is represented by the matrix

−1 + bY
−1, Z

−1T
−1, T

−1T
−1, Y

−1 + f X

−1Y

−1Z

−1Z

−1 + dX

−1T

−1 + eX



 0 a b

 then ¯S =

 0 f

S =

e f

e d
b
0 c
f
e
c 0 a
d b a 0

c
a 0 d e
b d 0 f
0
c
and XTSX = 0 implies XT¯SX = 0.
If S is ruled, then it contains two generators
passing through any point, and in particular through each Ei. By part (ii), these are
mapped to straight lines, which must lie on ¯S. Thus ¯S is a ruled quadric. One may
further verify that det S = det ¯S, which implies that if S is a non-degenerate quadric
(that is det S (cid:5)= 0), then so is ¯S. In this non-degenerate case, if S is a hyperboloid
of one sheet, then det S > 0, from which it follows that det ¯S > 0. Thus ¯S is also a
hyperboloid of one sheet.

The action of Γ on other geometric entities is investigated in exercises (page 559);
We wish to interpret duality equation (22.2) in a coordinate-free manner. The matrix
PC has by deﬁnition the form given in (22.1), and maps Ei to ei for i = 1, . . . ,4 . The
image PCX is may be thought of as a representation of the projection of X relative
to the projective basis ei in the image. Alternatively, PCX represents the projective
equivalence class of the set of the ﬁve rays CE1, . . . , CE4, CX. Thus PCX = PC(cid:1)X(cid:2)
if
and only if the set of rays from C to X and the four vertices of the reference tetrahedron
is projectively equivalent to the set of rays from C(cid:2)
and the four reference vertices.
In terms of the notation introduced earlier, we may write (22.2) in a different form as

to X(cid:2)

(cid:8)C; E1, . . . , E4, X(cid:9) = (cid:8)X; E1, . . . , E4, C(cid:9).

(22.3)

The duality principle
The basis of duality is (22.2) which states that PCX = PXC, with notation as in (22.2)
The notation PCX represents the coordinates of the projection of point X with re-
spect to the canonical image coordinate frame deﬁned by the projections of the corners
of the reference tetrahedron. Equivalently, PC may be considered as representing the
projective equivalence class of the ﬁve projected points PCEi and PCX. In the notation
of this chapter, this is (cid:8)C; E1, . . . , E4, X(cid:9). Thus the duality relation may be written as
(22.4)

(cid:8)C; E1, . . . , E4, X(cid:9) = (cid:8)X; E1, . . . , E4, C(cid:9)

where the bar represents the Carlsson–Weinshall map.

Although PC was deﬁned in terms of the canonical projective basis, there is nothing
special about the four points E1, . . . , E4 used as vertices of the reference tetrahedron,

22.3 Carlsson–Weinshall duality

549

other than the fact that they are non-coplanar. Given any four non-coplanar points, one
may deﬁne a projective coordinate system in which these four points are the points
Ei forming part of a projective basis. The Carlsson–Weinshall map may then be de-
ﬁned with respect to this coordinate frame. The resulting map is called the Carlsson–
Weinshall map with respect to the given reference tetrahedron.

To be more precise, it should be observed that ﬁve points (not four) deﬁne a projec-
tive coordinate frame in IP3. In fact, there is more than one projective frame (in fact a
3-parameter family) for which four non-coplanar points have coordinates Ei. Thus the
Carlsson–Weinshall map with respect to a given reference tetrahedron is not unique.
However, the mapping given by deﬁnition 22.19 with respect to any such coordinate
frame may be used.

Given a statement or theorem concerning projections of sets of points with respect
to one or more projection centres one may derive a dual statement. One requires that
among the four points being projected, there are four non-coplanar points that may
form a reference tetrahedron. Under a general duality mapping with respect to the
reference tetrahedron

(i) Points (other than those belonging to the reference tetrahedron) are mapped to

centres of projection.

(ii) Centres of projection are mapped to points.
(iii) Straight lines are mapped to twisted cubics.
(iv) Ruled quadrics containing the reference tetrahedron are mapped to ruled
quadrics containing the reference tetrahedron. If the original quadric is non-
degenerate, so is its image under the duality mapping.

Points lying on an edge of the reference tetrahedron should be avoided, since the
Carlsson–Weinshall mapping is undeﬁned for such points. Using this as a sort of trans-
lation table, one may dualize existing theorems about point projection, giving new
theorems for which a separate proof is not needed.
Note.
It is important to observe that only those points not belonging to the reference
tetrahedron are mapped to camera centres by duality. The vertices of the reference
tetrahedron remain points. In practice, in applying the duality principle, one may select
any four points to form the reference tetrahedron, as long as they are non-coplanar. In
general, in the results stated in the next section there will be an assumption (not always
stated explicitly) that point sets considered contain four non-coplanar points, which
may be taken as the reference tetrahedron.

22.3.1 Single view ambiguity
It will be shown in this section how various ambiguous reconstruction results may be
derived simply from known or obvious geometric statements by applying duality.

Camera resectioning from ﬁve points
Five 3D–2D point correspondences are not sufﬁcient for camera resectioning for pro-
jective cameras. It is interesting to know what one can determine from ﬁve point cor-
respondences, however.

550

22 Degenerate Conﬁgurations

C

X

X

C

Fig. 22.5. Left: Any point on the line passing through C and X is projected to the same point from
projection centre C. Right: The dual statement – from any centre of projection C lying on a twisted
cubic passing through X and the vertices of the reference tetrahedron, the ﬁve points are projected in
the same way (up to projective equivalence). Thus a camera is constrained to lie on a twisted cubic by
its image of ﬁve known points.

As a simple example of what can be deduced using Carlsson duality, consider the
following simple question: when do two points project to the same point in an image?
The answer is obviously when the two points lie on the same ray (straight line) through
the camera centre. Dualizing this simple observation, ﬁgure 22.5 shows that the centre
of the camera constrained by ﬁve point 3D–2D correspondences must lie on a twisted
cubic passing through the ﬁve 3D points.

The horopter
In a similar manner one may compute the form of the horopter determined by two
cameras. The horopter is the set of space points that map to the same point in two
images. The argument is illustrated in ﬁgure 22.6 and begins with a simple observation
concerning straight lines.
Result 22.21. Given points X and X(cid:2)

, the locus of camera centres C such that

(cid:8)C; E1, . . . , E4, X(cid:9) = (cid:8)C; E1, . . . , E4, X(cid:2)(cid:9)

.

is the straight line passing through X and X(cid:2)
This is illustrated in ﬁgure 22.6(left) The dual of this statement determines the horopter
for a pair of cameras (see ﬁgure 22.6(right)).
Result 22.22. Given projection centres C and C(cid:2)
, non-collinear with the four points
Ei of a reference tetrahedron, the set of points X such that (cid:8)C; E1, . . . , E4, X(cid:9) =
(cid:8)C(cid:2)
; E1, . . . , E4, X(cid:9) is a twisted cubic passing through E1, . . . , E4 and the two projec-
tion centres C and C(cid:2)

.

Ambiguity of camera resection
Finally, we consider ambiguity of resection. This is very closely related to the horopter.
To visualize this, the reader may refer again to ﬁgure 22.6, though it is not exactly
pertinent in this situation.
Result 22.23. Consider a set of camera centres C1, . . . , Cm and a point X0 all lying on

22.3 Carlsson–Weinshall duality

551

C

C'

X

X'

X

X'

C

C'

, . . . lying on the line passing through X and X(cid:1)
Fig. 22.6. Left: From any centre of projection C, C(cid:1)
,
are projected to the same ray. That is, (cid:8)C; Ei, X(cid:9) = (cid:8)C; Ei, X(cid:1)(cid:9) for all C on the
the points X and X(cid:1)
line. Right: The dual statement – all points on the twisted cubic passing through C and C(cid:1)
and the
vertices of the reference tetrahedron are projected in the same way relative to the two projection centres.
That is, (cid:8)C; Ei, X(cid:9) = (cid:8)C(cid:1); Ei, X(cid:9) for all X on the twisted cubic. This curve is called the horopter for
the two centres of projection.

a single straight line and let Ei, i = 1, . . . ,4 be the vertices of a reference tetrahedron.
Let X be another point. Then the two conﬁgurations

{C1, . . . , Cm; E1, . . . , E4, X} and {C1, . . . , Cm; E1, . . . , E4, X0}

are image-equivalent conﬁgurations if and only if X lies on the same straight line as
X0 and the cameras.

In passing to the dual statement, according to theorem 22.20 the straight line be-
comes a twisted cubic through the four vertices of the reference tetrahedron. Thus the
dual statement to result 22.23 is:
Result 22.24. Consider a set of points Xi and a camera centre C0 all lying on a single
twisted cubic also passing through four reference vertices Ei. Let C be any other
camera centre. Then the conﬁgurations

{C; E1, . . . , E4, X1, . . . , Xm} and {C0; E1, . . . , E4, X1, . . . , Xm}

are image-equivalent if and only if C lies on the same twisted cubic.

Since the points Ei may be any four non-coplanar points, and a twisted cubic cannot

contain 4 coplanar points, one may state this last result in the following form:
Result 22.25. Let X1, . . . , Xm be a set of points and C0 a camera centre all lying on a
twisted cubic. Then for any other camera centre C the conﬁgurations

{C; X1, . . . , Xm} and {C0; X1, . . . , Xm}

are image-equivalent if and only if C lies on the same twisted cubic.

This is illustrated in ﬁgure 22.6 (right). It shows that camera pose cannot be uniquely
determined whenever all points and a camera centre lie on a twisted cubic. This gives
an independent proof of result 22.6(p537) covering the case that was left unﬁnished
previously.

Using similar methods one can show that this is one of only two possible ambiguous
situations. The other case in which ambiguity occurs is when all points and the two

552

22 Degenerate Conﬁgurations

camera centres lie in the union of a plane and a line. This arises as the dual of the
case when the straight line through the camera centres meets one of the vertices of the
reference tetrahedron. In this case, the dual of this line is also a straight line through
the same reference vertex (see theorem 22.20), and all points must lie on this line or
the opposite face of the reference tetrahedron.

Note in both these examples how the use of duality has taken intuitively obvious
statements concerning projections of collinear points and derived a result somewhat
less obvious about points lying on a twisted cubic.

22.3.2 Two-view ambiguity
The basic result theorem 22.9(p541) about critical surfaces from two views may be
stated as follows.
Theorem 22.26. A conﬁguration {C1, C2; X1, . . . , Xn} of two camera centres and n
points allows an alternative reconstruction if and only if both camera centres C1, C2
and all the points Xj lie on a ruled quadric surface. If the quadric is non-degenerate (a
hyperboloid of one sheet), then there will always exist a third distinct reconstruction.

One may write down the dual statement straight away as follows.
Theorem 22.27. A conﬁguration {C1, . . . , Cn; X1, . . . , X6} of any number of cameras
and six points allows an alternative reconstruction if and only if all camera centres
C1, . . . , Cn and all the points X1, . . . , X6 lie on a ruled quadric surface.1 If the quadric
is non-degenerate (a hyperboloid of one sheet) then there will always exist a third
distinct reconstruction.

This result was originally proved in [Maybank-98]. Just to emphasize how a duality

proof works, a proof for theorem 22.27 will be given.
Proof. Consider the conﬁguration {C1, . . . , Cn; X1, . . . , X6}. One renumbers the
points so that the conﬁguration is denoted by {C1, . . . , Cn; E1, . . . , E4, X1, X2} where
E1, . . . , E4 are four non-collinear points, taken to be the vertices of a reference tetra-
hedron. If this conﬁguration has an alternative reconstruction, then there exists an-
other conﬁguration {C(cid:2)
} such that for all i = 1, . . . , n and
n; E1, . . . , E4, X(cid:2)
j = 1, 2, one has (cid:8)Ci; E1, . . . , E4, Xj(cid:9) = (cid:8)C(cid:2)
(cid:9). Dualizing this using
(22.3) yields

1, X(cid:2)
i; E1, . . . , E4, X(cid:2)
(cid:9) for all j = 1, 2 and i = 1, . . . , n.

(cid:8)Xj; E1, . . . , E4, Ci(cid:9) = (cid:8)X(cid:2)

j; E1, . . . , E4, C(cid:2)

i

1, . . . , C(cid:2)

2

j

Now, theorem 22.26 applies, and one deduces that the two camera centres Xj, the
reference vertices E1, . . . , E4 and the points Ci all lie on a ruled quadric surface ¯S.
Applying the reverse dualization, using theorem 22.20(iii), one sees that the points
X1, X2 and the camera centres Ci all lie on the quadric surface S. This proves the
forward implication of the theorem. The reverse implication is handled in a similar
manner.
1 In this statement, it is assumed that the set of points X1, . . . , X6 includes four non-coplanar points forming a reference
tetrahedron and that none of the other two Xj nor any of the camera centres Ci lies on a face of this tetrahedron. Whether or
not this condition is essential is not resolved.

22.4 Three-view critical conﬁgurations

553

The existence of a third distinct solution follows from the fact that the dual of a non-
degenerate quadric is non-degenerate.

The minimum interesting case of theorem 22.27 is when n = 3, as studied in section
20.2.4(p510). In this case there are nine points in total (three cameras and six points).
One can construct a quadric surface passing through these nine points (a quadric is de-
ﬁned by nine points). If the quadric is a ruled quadric (a hyperboloid of one sheet in the
non-degenerate case), then there are three possible distinct reconstructions. Otherwise
the reconstruction is unique. As with reconstruction from seven points in two views,
algorithm 20.1(p511) for six points in three views requires the solution of a cubic equa-
tion. As with seven points, the distinction between cases where the cubic has one or
three real solutions is explained as the difference between whether the quadric is ruled
or not.

22.4 Three-view critical conﬁgurations

We now turn to consider the ambiguous conﬁgurations that may arise in the three-view
case.
In this section, to distinguish the three cameras, we use superscripts instead of
primes. Thus, let P0, P1, P2 be three cameras and {Pi} be a set of points. We ask
under what circumstances there exists another conﬁguration consisting of three other
camera matrices Q0, Q1 and Q2 and points {Qi} such that PjPi = QjQi for all i and j.
We require that the two conﬁgurations be projectively inequivalent.

Various special ambiguous conﬁgurations exist.

Points in a plane
If all the points lie in a plane, and Pi = Qi for all i, then any of the cameras may be
moved without changing the projective equivalence class of the projected points. It is
possible to choose Pj and Qj with centres at any two preassigned locations in such a
way that PjPi = QjQi.

Points on a twisted cubic
A similar ambiguous situation arises when all the points plus one of the cameras, say
P2, lie on a twisted cubic. In this case, we may choose Q0 = P0 and Q1 = P1 and
the points Qi = Pi for all i. Then according to the ambiguity of camera resectioning
for points on a twisted cubic, (section 22.1.2), for any point C2
Q on the twisted cubic a
camera matrix Q2 may be chosen with centre at C2

Q such that P2Pi = Q2Qi for all i.

These examples of ambiguity are not very interesting, since they are no more than
extensions of the one-view camera resectioning ambiguity. In the above examples, the
points Pi and Qi are the same in each case, and the ambiguity lies only in the placement
of the cameras with respect to the points. More interesting ambiguities may also occur,
as we consider next.

554

22 Degenerate Conﬁgurations

General three-view ambiguity
Suppose that the camera matrices (P0, P1, P2) and (Q0, Q1, Q2) are ﬁxed, and we wish to
ﬁnd the set of all points such that PiP = QiQ for i = 0, 1, 2. Note that we are trying
here to copy the two-view case in which both sets of camera matrices are chosen in
advance. Later, we will turn to the less restricted case in which just one set of cameras
is chosen in advance.

A simple observation is that a critical conﬁguration for three views is also a critical
set for each of the pairs of views. Thus one is led naturally to assume that the set of
points for which {P0, P1, P2, Pi} is a critical conﬁguration is simply the intersection of
the point sets for which each of {P0, P1, Pi}, {P1, P2, Pi} and {P0, P2, Pi} is a critical
conﬁguration. Since by lemma 22.10(p542) each of these point sets is a ruled quadric,
one is led to assume that the critical point set in the three-view case is simply an in-
tersection of three quadrics. Although this is not far from the truth, the reasoning is
somewhat fuzzy. The crucial point missing in this argument is that the corresponding
conjugate points may not be the same for each of the three pairs.
More precisely, corresponding to the critical conﬁguration {P0, P1, Pi}, there exists
a conjugate conﬁguration {Q0, Q1, Q01
} for which PjPi = QjQ01
for j = 0, 1. Simi-
larly, for the critical conﬁguration {P0, P2, Pi}, there exists a conjugate conﬁguration
{Q0, Q2, Q02
} for which PjPi = QjQ02
i are not
necessarily the same as Q01
, so we cannot conclude that there exist points Qi such that
i
PjPi = QjQi for all i and j = 0, 1, 2 – at least not immediately.

for j = 0, 2. However, the points Q02

i

i

i

i

We now consider this a little more closely. Considering just the ﬁrst pairs of cameras
(P0, P1) and (Q0, Q1), lemma 22.10(p542) tells us that if P and Q are points such that
PjP = QjQ, then P must lie on a quadric surface S01
P determined by these camera
matrices. Similarly, point Q lies on a quadric S01
Q . Likewise considering the camera
pairs (P0, P2) and (Q0, Q2) one ﬁnds that the point P must lie on a second quadric S02
P
deﬁned by these two camera pairs. Similarly, there exists a further quadric deﬁned by
the camera pairs (P1, P2) and (Q1, Q2) on which the point P must lie. Thus for points
P and Q to exist such that PjP = QjQ for j = 0, 1, 2 it is necessary that P lie on the
intersection of the three quadrics: P ∈ S01
P . It will now be seen that this is
almost a necessary and sufﬁcient condition.

P ∩ S02

P ∩ S12

Result 22.28. Let (P0, P1, P2) and (Q0, Q1, Q2) be two triplets of camera matrices and
assume P0 = Q0. For each of the pairs (i, j) = (0, 1), (0, 2) and (1, 2), let Sij
P and
Sij
Q be the ruled quadric critical surfaces deﬁned for camera matrix pairs (Pi, Pj) and
(Qi, Qj) as in lemma 22.10(p542).

P ∩ S02

P ∩ S12

lie on the intersection S01

Q ∩ S02
(iii) Conversely, if P is a point lying on the intersection of quadrics S01
Q and C2

(ii) If there exist points P and Q such that PiP = QiQ for all i = 0, 1, 2, then P must
Q ∩ S12
Q .
P ∩ S02
P ∩ S12
P ,
Q, then there
Q such that PiP = QiQ for all i = 0, 1, 2.

but not on a plane containing the three camera centres C0
exists a point Q lying on S01

P and Q must lie on S01

P ∩ S02

P ∩ S12

Q ∩ S02

Q ∩ S12

Q, C1

(i) The centre of camera P0 lies on S01

P , P1 lies on S01

P , and P2 lies on

P ∩ S02
P .

S12

22.4 Three-view critical conﬁgurations

555

Note that the condition that P0 = Q0 is not any restriction of generality, since the pro-
jective frames for the two conﬁgurations (P0, P1, P2) and (Q0, Q1, Q2) are independent.
One may easily choose a projective frame for the second conﬁguration in which this
condition is true. This assumption is made simply so that one may consider the point
P in relation to the projective frame of the second set of cameras.

The extra condition that the point P does not lie on the plane of camera centres Ci
Q
is necessary, however the reader is referred to [Hartley-00b] for justiﬁcation of this
claim. Note that this case will usually not arise, however, since the intersection point
of the three quadrics with the trifocal plane will be empty, or in special cases consist of
a ﬁnite number of points. Where it does arise is through the possibility that the three
camera centres C0
Q are collinear, in which case any other point is coplanar
with these three camera centres.

Q and C2

Q, C1

Proof. The ﬁrst statement follows directly from lemma 22.10(p542). For the second
part, the fact that the points P and Q lie on the intersections of the three quadrics
follows (as pointed out before the statement of the theorem) from lemma 22.10(p542)
applied to each pair of cameras in turn.
To prove the ﬁnal assertion, suppose that P lies on the intersection of the three quadrics.
Then from lemma 22.10(p542), applied to each of the three quadrics Sij
P , there exist
points Qij such that the following conditions hold:

P0P = Q0Q01
P0P = Q0Q02
P1P = Q1Q12

P1P = Q1Q01
P2P = Q2Q02
P2P = Q2Q12.

It is easy to be confused by the superscripts here, but the main point is that each line is
precisely the result of lemma 22.10(p542) applied to one of the three pairs of camera
matrices at a time. These equations may be rearranged as

P0P = Q0Q01 = Q0Q02
P1P = Q1Q01 = Q1Q12
P2P = Q2Q02 = Q2Q12.

Now, the condition that Q1Q01 = Q1Q12 means that the points Q01 and Q12 are collinear
with the camera centre C1
Q of Q1. Thus, assuming that the points Qij are distinct, they
must lie in a conﬁguration as shown in ﬁgure 22.7. One sees from the diagram that
if two of the points are the same, then the third one is the same as the other two. If
the three points are distinct, then the three points Qij and the three camera centres Ci
Q
are coplanar, since they all lie in the plane deﬁned by Q01 and the line joining Q02 to
Q12. Thus the three points all lie in the plane of the camera centres Ci
Q. However, since
P0P = Q0Q01 = Q0Q02 it follows that P must lie along the same line as Q01 and Q02, and
hence must lie in the same plane as the camera centres Ci
Q.

Thus, this result shows that the points in a 3-view critical conﬁguration lie on the
intersection of three quadrics, whereas the camera centres lie on the intersections of

556

22 Degenerate Conﬁgurations

P 0

Q 0=

Q1

Q 12

Q 01

Q 02

P

Q 2

Fig. 22.7. Conﬁguration of the three camera centres and the three ambiguous points. If the three points
Qij are distinct, then they all lie in the plane of the camera centres Ci
Q.

pairs of the quadrics.
In general, the intersection of three quadrics will consist of
eight points. In this case, the critical set with respect to the two triplets of camera
matrices will consist of these eight points, less any such points that lie on the plane of
the three cameras Qi. In fact, it has been shown (in a longer unpublished version of
[Maybank-98]) that of the eight points of intersection of three quadrics, only seven are
critical, since the eighth point lies on the plane of the three cameras.

P = αS01

P P = 0 and PTS02

P + βS02
P P = 0 will also satisfy PTS12

In some cases, however, the camera matrices may be chosen such that the three
quadric surfaces meet in a curve. This will occur if the three quadrics Sij
P are lin-
P , then any point P that satisﬁes
early dependent. For instance if S12
PTS01
P P = 0. Thus the intersection of
the three quadrics is the same as the intersection of two of them. In this case, the three
cameras must also lie on the same intersection curve. We deﬁne a non-degenerate ellip-
tic quartic to be the intersection curve of two non-degenerate ruled quadrics, consisting
of a single curve. This is a fourth-degree space curve. Other ways that two quadrics
can intersect include a twisted-cubic plus a line, or two conics. Examples of elliptic
quartics are shown in ﬁgure 22.8.

Example 22.29. Three-view critical conﬁguration – the elliptic quartic.
We consider the quadrics represented by matrices A and B = ˜B + ˜BT
−s − u
−s + u

s − t
s + t

 and ˜B =

 0 1

 p q

A =

0 r
0 0 −p − q − r
0 0

0

0
0

0
0
1 0
0
0
0 −1
0 0
0 0 −1
0

, where

 .

(22.5)

Thus, B is a member of a 5-parameter family of quadrics generated by {p, q, r, s, t, u},
(remembering that scale is irrelevant). The camera matrices

P0 = [ I| 0 ], P1 = [ I | (−1,−1,−1)T ] and P2 = [ I | (1, 1,−1)T ]

have centres lying on the intersection of these three quadrics.

We show that a conﬁguration consisting of these three cameras, and any number
of points on the intersection of the two quadrics is critical. This is demonstrated by
explicitly exhibiting two alternative conﬁgurations consisting of three cameras Qi and
for each point P lying on the intersection of the two quadrics, a conjugate point Q such

22.4 Three-view critical conﬁgurations

557

4

3

2

1

0

6

5

4

3

2

1

0

−1

−2

−3

−1
−1

−0.5

0

0.5

1

1.5

2

−4

−2

−3

1

0

−1

−15

−10

−5

0

5

4

2

0

−2

−4

−6

−8

−10

−12

−14

1.5

1

0.5

0

−0.5

−1

−1.5
2

5

0

−5
−8

−6

2

0

−4

−2

0

2

0

1

2

4

6

3

−2

−1

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2

Fig. 22.8. Examples of elliptic quartic curves generated as the intersection of two ruled quadrics.

that PiP = QiQ. In fact, two different conjugate conﬁgurations are given in table 22.1
and table 22.2.

It may be veriﬁed directly that PiP = QiQ for all points P = (x, y, xy, 1)T and
corresponding points Q, provided that P lies on the quadric B. (It always lies on quadric
A). The easiest way to see this is to verify that (PiP)× (QiQ) = 0 for all such points. In
fact for i = 0, 1, the cross-product is always zero, whereas for i = 2 it may be veriﬁed
by direct computation that

(P2P) × (Q2Q) =

for the ﬁrst solution, and

(P2P) × (Q2Q) =

PTBP

PTBP

(4,−4x, 4)T

(−4y, 4, 4)T

(cid:15)
(cid:15)

(cid:16)
(cid:16)

(cid:2)

for the second solution. Thus P2P = Q2Q if and only if P lies on B.

and B(cid:2)

Note that in this example, A is the matrix representing the quadric S01
P .
This example is quite general, since if A(cid:2)

are two non-degenerate ruled
quadrics containing the centres of three cameras, then by a projective transformation,
we may map A(cid:2)
to A and the three camera centres to those of the given Pi, provided only
that no two of the camera centres lie on the same generator of A(cid:2)
. In addition, B(cid:2)
will
map to A + λB for some λ. Thus, the pencil generated by A(cid:2)
and B(cid:2)
, and hence also their
intersection curve, are projectively equivalent to those generated by A and B.

The possibility that two of the camera centres lie on the same generator of A(cid:2)

is not
a difﬁculty, since if the line of the camera centres lies on all quadrics in the pencil,
then the quadric intersection can not be an non-degenerate elliptic quartic. Otherwise,

22 Degenerate Conﬁgurations

(cid:18)

1
0
0

0
1
0

0
0
1

0
0
0

, Q1 =

(cid:17) −4

0
0

(cid:18)

0
2

0
0
0
1
0 −2 1

558

The camera matrices are

(cid:17)

Q0 =

and Q2 =(cid:17) −4 (2p + q − t + u)

0
8p

8 (r + s − u)

8r
−8r

4 (p + q + 2r + s + t)

−2 (p + q − s − t)
−2 (2p + q − 2s + 3t + 3u) 2p + q − 2s − t − u

−2 (q − t + u)

−q + t − u

The conjugate point to P = (x, y, xy, 1)T is

Q = ((x − 1)x, (x − 1)y, (x − 1)xy,−2x (−2 + y + xy))T

.

Table 22.1. First conjugate solution to reconstruction problem for cameras Pi and points on the inter-
section of quadrics A and B given in (22.5).

(cid:17)

(cid:18)

(cid:17)

1 0 0 0
0 1 0 0
0 0 1 0

, Q1 =

0 0− 2 1
0
0 4
0 0
1

0
2

(cid:18)

(cid:18)

(cid:18)

The camera matrices are

Q0 =

and Q2 =(cid:17) −8 (p + s + u)

−8p
8p

4 (q + 2r + t − u) −4 (2p + q + r + s − t)
2 (q + 2r − 2s − 3t − 3u)

0
−8r

2 (q + t − u)

−q − t + u

−2 (q + r − s + t)
q + 2r − 2s + t + u

The conjugate point to P = (x, y, xy, 1)T is

Q = ((y − 1)x, (y − 1)y, (y − 1)xy, 2y (−2 + x + xy))T

.

Table 22.2. Second conjugate solution to reconstruction problem.

we can choose A(cid:2)
centres. This demonstrates

to be one of the quadrics not containing the line of the two camera

Result 22.30. Any conﬁguration consisting of three cameras and any number of points
lying on a non-degenerate elliptic quartic is critical.

22.5 Closure

22.5.1 The literature
The twisted cubic as the critical curve for camera resectioning was brought to the atten-
tion of the computer-vision community by [Buchanan-88]. For more about critical sets
of two views, the reader is referred to [Maybank-90] and the book [Maybank-93]. In
both these cases, the critical point sets were known much earlier. In fact [Buchanan-88]
refers the reader to the German photogrammetric literature [Krames-42, Rinner-72].
For two views, the result that (v) and (viii) are not critical in theorem 22.9(p541) is due
to Fredrik Kahl (unpublished).

22.5 Closure

559

The discussion of critical conﬁgurations for three views given in this chapter is only a
part of what is known about this topic. More can be found in [Hartley-00b, Hartley-02a,
Kahl-01a]. In particular, the elliptic-quartic conﬁguration is extended to any number of
cameras in [Kahl-01a]. A critical conﬁguration for any number of cameras, consisting
of points on a twisted cubic and cameras on a straight line is considered in [Hartley-03].
Earlier work on this area includes an investigation of the critical camera positions for
sets of six points in [Maybank-98], and an unpublished report [Shashua-96] deals with
critical conﬁgurations in three views.

Nothing has been said here about critical conﬁgurations of lines in three or more
views, but this topic has been treated in [Buchanan-92]. In addition, critical conﬁgura-
tion for linear reconstruction from lines (the linear line complex) have been identiﬁed
in [Stein-99].

22.5.2 Notes and exercises

(i) Fill out the details of the following sketch to prove that any two conﬁgurations
consisting of a hyperboloid of one sheet and two points on the hyperboloid are
projectively equivalent (via a projectivity of IP3) provided that the points in both
pairs either do or do not lie on the same generator.
Since any hyperboloid of one sheet is projectively equivalent to X2 + Y2 −
Z2 = 1, any two hyperboloids of one sheet are projectively equivalent to each
other, and also to the hyperboloid given by Z = XY. Deﬁne a 1D projective
transformation hX(X) = X

= (aX + b)/(cX + d). One computes that

(cid:2)

 a

c



 X

Y
XY
1

d c
b a

b

d

 =

 X

(cid:2)

Y
(cid:2)
Y
X
1

 .

This is a 3D projective transformation taking the surface Z = XY to itself.
Composing this with a similar transformation of Y one ﬁnds a projective trans-
formation that takes (X, Y, XY, 1)T to (X
, 1)T while ﬁxing the quadric
Z = XY. Since hX and hY are arbitrary 1D projective transformations, this
gives enough freedom to map any two points to two others.

, Y

, X

(cid:2)

(cid:2)

(cid:2)

(cid:2)

Y

(ii) Show that Γ maps a line that meets one of the edges of the reference tetrahedron

to a conic.

(iii) Show that Γ maps a straight line meeting two opposite edges of the reference

tetrahedron to a straight line meeting the same two edges.

(iv) How are these conﬁgurations related to degenerate conﬁgurations for camera

resectioning, as shown in ﬁgure 22.3(p538).

(v) Two-view degeneracy occurs when all points and the two cameras lie on a ruled
quadric. Given eight points on the corners of a Euclidean cube and two camera
centres, show that these 10 points always lie on a quadric. If this is a ruled
quadric, then the conﬁguration is degenerate, and reconstruction is not possible
from 8 points. Investigate under what conditions the quadric is ruled. Hint:

560

22 Degenerate Conﬁgurations

there is a two-parameter family of quadrics passing through the cube vertices.
What does this two-parameter family look like?

(vi) Extend result 22.30 by showing that a conﬁguration of any number of cameras
and points lying on a non-degenerate elliptic quartic is critical. This does not
require complex computations. If stuck, refer to [Kahl-01a].

Part V

Appendices

Portrait of the Countess D’Haussonville, 1845 (oil on canvas)

by Jean-Auguste-Dominique Ingres (1780-1867)

The Frick Collection, New York

Appendix 1

Tensor Notation

Since tensor notation is not commonly used in computer vision, it seems appropri-
ate to give a brief introduction to its use. For more details, the reader is referred to
[Triggs-95]. For simplicity, these concepts will be explained here in the context of
low-dimensional projective spaces, rather than in their full generality. However, the
ideas apply in arbitrary dimensional vector spaces.

Consider a set of basis vectors ei, i = 1, . . . ,3 for a 2-dimensional projective space
IP2. For reasons to become clear, we will write the indices as subscripts. With respect
to this basis, a point in IP2 is represented by a set of coordinates xi, which represents
i=1 xi ei. We write the coordinates with an upper index, as shown. Let x
the point
represent the triple of coordinates, x = (x1, x2, x3)T.

(cid:27)

3

(cid:27)

Now, consider a change of coordinate axes in which the basis vectors ei are replaced
by a new basis set ˆej, where ˆej =
ei, and H is the basis transformation matrix
j. If ˆx = (ˆx1, ˆx2, ˆx3)T are the coordinates of the vector with respect to
with entries H i
the new basis, then we may verify that ˆx = H−1x. Thus, if the basis vectors transform
according to H the coordinates of points transform according to the inverse transforma-
tion H−1.

i H i
j

Next, consider a line in IP2 represented by coordinates l with respect to the original
basis. With respect to the new basis, it may be veriﬁed that the line is represented by
a new set of coordinates ˆl = HTl. Thus coordinates of the line transform according to
HT.

As a further example, let P be a matrix representing a mapping between projective
(or vector) spaces. If G and H represent basis transformations in the domain and range
spaces, then with respect to the new bases, the mapping is represented by a new matrx
ˆP = H−1PG. Note in these examples that sometimes the matrix H or HT is used in the
transformation, and sometimes H−1.

These three examples of coordinate transformations may be written explicitly as fol-

lows.

ˆxi = (H

−1)i

j xj

ˆli = H j

i lj

ˆP i

j = (H

−1)i

k Gl

j P k
l

where we use the tensor summation convention that an index repeated in upper and
lower positions in a product represents summation over the range of the index. Note
that those indices that are written as superscripts transform according to H−1, whereas

562

A1.1 The tensor rst

563

those that are written as subscripts transform as H (or G). Note that there is no distinc-
tion in tensor notation between indices that are transformed by H, and those that are
transformed by HT. In general, tensor indices will transform by either H or H−1 – in fact
this is the characteristic of a tensor. Those indices that transform according to H are
known as covariant indices and are written as subscripts. Those indices that transform
according to H−1 are known as contravariant indices, and are written as superscripts.
The number of indices is the valency of the tensor. The sum over an index, e.g. H j
i lj,
is referred to as a contraction, in this case the tensor H j
i is contracted with the line lj.

The tensor rst is deﬁned for r, s, t = 1, . . . ,3 as follows:

A1.1 The tensor rst

 0 unless r, s and t are distinct

+1 if rst is an even permutation of 123
−1 if rst is an odd permutation of 123

rst =

The tensor ijk (or its contravariant counterpart, ijk) is connected with the cross prod-
uct of two vectors. If a and b are two vectors, and c = a × b is their cross product,
then the following formula may easily be veriﬁed.

ci = (a × b)i = ijkajbk.

Related to this is the expression (A4.5–p581) for the skew-symmetric matrix [a]×.

Using tensor notation one writes this as

([a]×)ik = ijkaj.

Thus, one sees that if a is a contravariant vector, then [a]× is a matrix with two covariant
indices. A similar formula holds for [v]× where v is covariant, namely ([v]×)ik =
ijkvj.
Finally, the tensor ijk is related to determinants: for three contravariant tensors ai,
bj and ck, one veriﬁes that aibjckijk is the determinant of the 3 × 3 matrix with rows
ai, bj and ck.

A1.2 The trifocal tensor

i

The trifocal tensor T jk
has one covariant and two contravariant indices. For vectors
and matrices, such as xi, li and P i
j , it is possible to write the transformation rules using
standard linear algebra notation, e.g. x(cid:2)
= Hx. However, for tensors with three or more
indices, this cannot conveniently be done. There is really no choice but to use tensor
notation when dealing with the trifocal tensor.

Transformation rule. The arrangement of indices for the trifocal tensor implies a
transformation rule

−1)k
t
with respect to changes of basis in the three images.
It is worthwhile pointing out
one possible source of confusion here. The transformation rule (A1.1) shows how the

ˆT jk
i = F r

(A1.1)

−1)j

i (G

s (H

T st

r

564

Appendix 1 Tensor Notation

l1

l

2

l

3

/ /
l 1

l

/ /
2

l

/ /
3

T1

/ /
l 1

l

/ /
2

l

/ /
3

T
2

/ /
l 1

l

/ /
2

l

/ /
3

T
3

/

l 1

/

l 1

/

l 1

l

/
2

l

/
2

l

/
2

l

/
3

l

/
3

l

/
3

Fig. A1.1. A 3-dimensional representation of the trifocal tensor – ﬁgure after Faugeras and Pa-
padopoulo [Faugeras-97]. The picture represents li = l
, which is the contraction of the tensor
with the lines l(cid:1)
to produce a line l. In pseudo-matrix notation this can be written as li = l(cid:1)TTil(cid:1)(cid:1)T,
and l(cid:1)(cid:1)
where (Ti)jk = T jk
.

kT jk
(cid:1)(cid:1)

(cid:1)
jl

i

i

tensor is transformed in terms of basis transformations in the three images. Often,
we are concerned instead with point coordinate transformations. Thus, if F(cid:2)
, G(cid:2)
and H(cid:2)
(cid:2)j
represent coordinate transformations in the images, in the sense that ˆxj = F
i xi, and
G(cid:2)
are similarly deﬁned for the other images, then the transformation rule may
be written as

and H(cid:2)

ˆT jk
i = (F

(cid:2)−1)r

i G

(cid:2)k
(cid:2)j
s H
t

T st
r .

Picture of tensors. A vector x may be thought of as a set of numbers arranged in a
column or row, and a matrix H as a 2D array of numbers. Similarly, a tensor with three
indices may be thought of as a 3D array of numbers. In particular the trifocal tensor is
a 3 × 3 × 3 cube of cells as illustrated in ﬁgure A1.1.

Appendix 2

Gaussian (Normal) and χ2 Distributions

A2.1 Gaussian probability distribution

Given a vector X of random variables xi for i = 1, . . . , N, with mean X = E[X], where
E[·] represents the expected value, and ∆X = X − X, the covariance matrix Σ is an
N × N matrix given by

Σ = E[∆X ∆XT]

so that Σij = E[∆xi∆xj]. The diagonal entries of the matrix Σ are the variances of
the individual variables xi, whereas the off-diagonal entries are the cross-covariance
values.

The variables xi are said to conform to a joint Gaussian distribution, if the probability

distribution of X is of the form

−N/2 det(Σ−1)1/2 exp

P (X + ∆X) = (2π)

(A2.1)
for some positive-semideﬁnite matrix Σ−1. It may be veriﬁed that X and Σ are the mean
and covariance of the distribution. A Gaussian distribution is uniquely determined by
−N/2 det(Σ−1)1/2 is just the normalizing factor
its mean and covariance. The factor (2π)
necessary to make the total integral of the distribution equal to 1.
(cid:20)
− N(cid:7)

In the special case where Σ is a scalar matrix Σ = σ2I the Gaussian PDF takes a

√
P (X) = (

(xi − ¯xi)2/2σ2

simple form

−N exp

2πσ)

(cid:21)

(cid:15)

(cid:16)
−(∆X)TΣ−1(∆X)/2

i=1

(cid:15)(cid:27)

(cid:16)

where X = (x1, x2, . . . , xN )T. This distribution is called an isotropic Gaussian distri-
bution.

Mahalanobis distance. Note that in this case the value of the PDF at a point X is
of the point X from
simply a function of the Euclidean distance
the mean X = (¯x1, . . . , ¯xN )T. By analogy with this one may deﬁne the Mahalanobis
distance between two vectors X and Y to be

1/2

N

i=1(xi − ¯xi)2
(cid:16)

(cid:15)
(X − Y)TΣ−1(X − Y)

(cid:10)X − Y(cid:10)Σ =

1/2

.

565

566

Appendix 2 Gaussian (Normal) and χ2 Distributions

One veriﬁes that for a positive-deﬁnite matrix Σ, this deﬁnes a metric on IRN. Using
this notation, the general form of the Gaussian PDF may be written as

(cid:15)
−(cid:10)X − X(cid:10)2

Σ/2

(cid:16)

P (X) ≈ exp

where the normalizing factor has been omitted. Thus, the value of the Gaussian PDF is
a function of the Mahalanobis distance of the point X from the mean.

exp

1, σ2

2, . . . , σ2

= UX and X(cid:2)

= UX, and substituting in (A2.1), leads to

Change of coordinates. Since Σ is symmetric and positive-deﬁnite, it may be written
as Σ = UTDU, where U is an orthogonal matrix and D = (σ2
(cid:15)
(cid:16)
N ) is diagonal.
Writing X(cid:2)
−(X − X)TΣ−1(X − X)/2

(cid:15)
(cid:15)
(cid:16)
−(X(cid:2) − X(cid:2)
)TUΣ−1UT(X(cid:2) − X(cid:2)
−(X(cid:2) − X(cid:2)
)TD−1(X(cid:2) − X(cid:2)
)/2
Thus, the orthogonal change of coordinates from X to X(cid:2)
= UX transforms a general
Gaussian PDF into one with diagonal covariance matrix. A further scaling by σi in
each coordinate direction may be applied to transform it to an isotropic Gaussian dis-
tribution. Equivalently stated, a change of coordinates may be applied to transform
Mahalanobis distance to ordinary Euclidean distance.

= exp

= exp

(cid:16)

)/2

A2.2 χ2 distribution

n distribution is the distribution of the sum of squares of n independent Gaussian
The χ2
random variables. As applied to a Gaussian random vector v with non-singular covari-
ance matrix Σ, the value of (v − ¯v)TΣ−1(v − ¯v) satisﬁes a χ2
n distribution, where n is
the dimension of v. If the covariance matrix Σ is singular, then we must replace Σ−1
with the pseudo-inverse Σ+. In this case
• If v is a Gaussian random vector with mean ¯v and covariance matrix Σ, then the
value of (v − ¯v)TΣ+(v − ¯v) satisﬁes a χ2
The cumulative chi-squared distribution is deﬁned as Fn(k2) =

n(ξ)dξ. This
represents the probability that the value of the χ2
n random variable is less than k2.
Graphs of the χ2
n distributions for n = 1, . . . ,4
are shown in ﬁgure A2.1 A program for computing the cumulative chi-squared distri-
bution Fn(k2) is given in [Press-88]. Since it is a monotonically increasing function,
one may compute the inverse function by any simple technique such as subdivision,
and values are tabulated in table A2.1 (compare with ﬁgure A2.1).

r distribution, where r = rankΣ.
k2
0 χ2

n distribution and inverse cumulative χ2

(cid:31)

A2.2 χ2 distribution

567

0.5

0.4

0.3

0.2

0.1

14

12

10

8

6

4

2

2

4

6

8

10

0.2

0.4

0.6

0.8

1

Fig. A2.1. The χ2
1, . . . , 4. In both cases, graphs are for n = 1, . . . , 4 bottom to top (at middle point of horizontal axis).

n distribution (left) and inverse cumulative χ2

−1
n distribution F
n

(right) for n =

n α = 0.95 α = 0.99

1
2
3
4

3.84
5.99
7.81
9.49

6.63
9.21
11.34
13.28

Table A2.1. Values of k2 for which Fn(k2), the cumulative χ2 distribution with n degrees of freedom,
equals α, i.e. k2 = F

−1
n (α), where α is the probability.

Appendix 3

Parameter Estimation

There is much theory about parameter estimation, dealing with properties such as the
bias and variance of the estimate. This theory is based on analysis of the probability
density functions of the measurements and the parameter space. In this appendix, we
discuss such topics as bias of an estimator, the variance, the Cram´er-Rao lower bound
on the variance, and the posterior distribution. The treatment will be largely informal,
based on examples, and exploring these concepts in the context of reconstruction.

The general lesson to be learnt from this discussion is that many of these concepts
depend strongly on the particular parametrization of the model. In problems such as 3D
projective reconstruction, where there is no preferred parametrization, these concepts
are not well deﬁned, or depend very strongly on assumed noise models.

A simple geometric estimation problem. The problem we shall consider is related
to the triangulation problem of determining a point in space from its projection into two
images. To simplify this problem, however, we consider its 2-dimensional analog. In
addition, we ﬁx one of the rays reducing the problem to one of estimating the position
of a point along a known line from observing it in a single image.

Thus, consider a line camera (that is, one forming a 1D image as in section 6.4.2-
(p175)) observing points on a single line. Let the camera be located at the origin (0, 0)
and point in the positive Y direction. Further, assume that it has unit focal length. Thus,
the camera matrix for this camera is simply [I2×2|0]. Now, suppose that the camera is
observing points on the line Y = X+1 (the “world line”). A point (X, X+1) on this line
will be mapped to the image point x = X/(X + 1). However, we assume that a point is
measured with a certain inaccuracy, which may be modelled with a probability density
function (PDF). The usual practice is to model the noise using a Gaussian distribution.
Let us assume at least that the mode (maximum) of the distribution is at zero. The
imaging setup is illustrated in ﬁgure A3.1.

The estimation problem we consider is the following: given the image coordinates of
a point, estimate the position of the “world point” on the line y = x + 1. To consider a
speciﬁc scenario, we may think of the line as a scintillator being ﬂooded with gamma-
rays. A camera is used to measure the location of each scintillation and determine its
position along the line. The problem seems to be ridiculously simple, but it will turn
out that there are some surprises.

568

A3.1 Bias

Y

θ

569

x

C

X

Fig. A3.1. The imaging setup for a simple estimation problem. A point on the line Y = X + 1 is imaged
by a line camera. The projection mapping is given by f : θ (cid:6)→ x = θ/(1 + θ), where θ parametrizes the
points on the line Y = X + 1. Measurement is subject to noise with a zero-mode distribution.

Probability density function. We start by parametrizing the world line Y = X + 1
by a parameter θ, where the most convenient parametrization is θ = X so that the 2D
point parametrized by θ is (θ, θ + 1). This point projects to θ/(θ + 1). We denote this
projection function from the world line to the image line by f, so that f (θ) =θ/ (θ+1).
The measurement of this point is corrupted by noise, resulting in a random variable x
with probability distribution given by p(x|θ) = g(x − f (θ)). For instance, if g is a
zero-mean Gaussian distribution with variance σ2, then

(cid:15)

p(x|θ) = (2πσ2)

−1/2 exp

−(x − f (θ))2/2σ2

.

(cid:16)

Maximum Likelihood estimate. An estimate of the parameter vector θ given a mea-
sured value x is a function denoted ˆθ(x), assigning a parameter vector θ to a measure-
ment x. The maximum likelihood (ML) estimate is given by

ˆθM L = arg max

θ

p(x|θ).

In the current estimation problem it is easily seen that the ML estimate is obtained
simply by back-projecting the measured point x and selecting its intersection with the
world line, according to the formula
ˆθ(x) =f

−1(x) =x/ (1 − x).

This is the ML estimate, because the resulting point, with parameter ˆθ(x), projects
forward to x, hence p(x|ˆθ) = g(x−x) =g (0) which by assumption gives the maximum
(mode) of the probability density function g. Any other choice of parameter θ will give
a smaller value of p(x|θ).

A3.1 Bias

A desirable property of an estimator is that it can be expected to give the right answer
on the average. Given a parameter θ, or equivalently in our case a point on the world
line, we consider all possible measurements x and from them reestimate the parame-
ter θ, namely ˆθ(x). The estimator is known as unbiased if on the average we obtain

570

Appendix 3 Parameter Estimation

-2

-1

4

2

-2

-4

1

2

3

−1(x) = x/(1 − x) for different
Fig. A3.2. The ML estimate of the world-point position ˆθ(x) = f
measurements of the image point x. Note that for values of x greater than 1, the ML estimate switches
to “behind” the camera.

the original parameter θ (the true value). In forming this average, we weight the mea-
surements x according to their probability. More formally, the bias of the estimator is
deﬁned as

Eθ[ˆθ(x)] − θ =

p(x|θ) ˆθ(x)dx − θ

!

x

and the estimator is unbiased if Eθ[ˆθ(x)] = θ for all θ. Here Eθ stands for the expected
value given θ, deﬁned as shown.

Another way of thinking of bias is in terms of repeating an experiment many times
with identical model parameters and a different instance of the noise at each trial. The
bias is the difference between the average value of the estimated parameters and the true
parameter value. It is worth noting that for the bias to be deﬁned, it is not necessary
that the parameter space have an a priori distribution deﬁned on it, not even that it be
a measure space. It is necessary, however that it have some afﬁne structure so that the
average (or integral) can be formed.

Now, we determine whether the ML estimate of θ is unbiased in the case where

(cid:21)

f (θ) = θ/(θ + 1). The integral becomes

!

(cid:20)

p(x|θ) ˆθ(x)dx =

1√
2πσ

exp

x

(x − θ/(θ + 1))2

2σ2

x
1 − x

dx

!

x

It turns out that this integral diverges, and hence the bias is undeﬁned. The difﬁculty is
that with an assumed Gaussian distribution of noise, for any value of θ, there is always
a ﬁnite (though perhaps very small) probability p(x|θ) that x > 1. For values of x > 1,
the corresponding ray does not meet the world line in front of the camera (since the ray
is parallel to the world line at x = 1). The estimate ˆθ(x) as a function of x is shown in
ﬁgure A3.2, showing how it results in estimated world-points behind the camera. Even
if values of ˆθ(x) behind the camera are ignored, the ML estimator has inﬁnite bias, as
is explained in ﬁgure A3.3.
Limiting the range of parameters. Since the range of the parameter θ is from −1 to
∞, it makes sense to limit its range. In fact, we may have knowledge that all “events”
on the world-line lie in a more restricted range. As an example, suppose that we assume
that θ lies in the range −1 ≤ θ ≤ 1, and hence noise-free projected points are in the
range −∞ < x < 1/2. In this case, the ML estimate for any image point x > 1/2 will

2

1.75

1.5

1.25

1

0.75

0.5

0.25

a

-1

-0.5

A3.1 Bias

571

5

4

3

2

1

0.4

0.3

0.2

0.1

-1

-0.5

0.5

1

-1

-0.5

0.5

1

0.5

1

-1

b

-0.1

-0.2

c

Fig. A3.3. The reason that the ML estimate with Gaussian noise model has inﬁnite bias. (a) The
distribution of possible values of the image measurement given a world-point at x = 0, y = 1 assuming
Gaussian noise distribution with σ = 0.4 – in symbols p(x|θ = 0). (b) The ML estimate of the world
point for different values of the image point, ˆθ(x) = x/(1− x). Note that as the image point approaches
1, the estimated point on the world-line recedes to inﬁnity. (c) Product ˆθ(x) p(x|θ = 0). The integral
of this function from x = −∞ to x = 1 gives the bias of the ML estimator. Note that as x approaches
1, the graph increases abruptly to inﬁnity. The integral is unbounded, meaning that the estimator has
inﬁnite bias.

0.0005

0.00025

-0.4

-0.2

0.2

0.4

0.6

0.8

1

-0.00025

-0.0005

-0.00075

b

(cid:31)

1.5

1

0.5

-1

-0.5

0.5

1

-0.5

-1

a

Fig. A3.4. (a) If the range of possible values of the world-point parameter θ is limited to the range
θ ≤ 1, then the ML estimate of any point x > 1/2 will be θ = 1. This will prevent inﬁnite bias in the
estimate, but there will still be bias. (b) The bias Eθ[ˆθ(x)] − θ =
ˆθ(x)p(x|θ)dx − θ as a function of
θ. Measurement noise is Gaussian with σ = 0.01.

x

be at ˆθ(x) = 1. With this restriction on the parameter θ, the ML estimate is still biased,
as shown in ﬁgure A3.4.

If the noise-distribution has ﬁnite support, then the bias will also be ﬁnite for most
values of θ, even if the range of the parameter θ is unrestricted. This is shown in
ﬁgure A3.5. One learns from this that the bias of the estimator can be very dependent
on the noise-model – a factor that is usually not within our control.

Dependency of bias on parametrization. The reason for the inﬁnite bias for the ML
estimator with Gaussian noise model in this example is the projective mapping between
the world line and the image line. It is possible to parametrize the world line differently
in a way that will change the bias.
Let the world line Y = X + 1 be parametrized in such a way that parameter θ repre-
sents the point (θ/(1 − θ), 1/(1 − θ)) on the line. The part of the line in front of the
camera (having positive y coordinate) is parametrized by θ in the range −∞ < θ < 1.
Under the projection (X, Y) (cid:6)→ X/Y, the projection map f is given by f (θ) = X/Y = θ,
thus the point with parameter θ maps to θ. In other words, points on the world line are

572

Appendix 3 Parameter Estimation

0.0002

0.00015

0.0001

0.00005

25

20

15

10

5

-0.75 -0.5 -0.25

0.25

0.5

0.75

1

20

40

a

80

60

b

Fig. A3.5. If the noise-model has ﬁnite support then the bias will be ﬁnite. In this example the noise
model is 3(1 − (x/σ)2)/4σ2, for −σ ≤ x ≤ σ, where σ = 0.01. (a) Bias as a function of θ for
small values of θ. (b) Percentage bias (Eθ[ˆθ(x)] − θ)/θ. The bias is relatively small for small values
of θ, but larger (up to 20% ) for large values. The position of the point along the world-line is always
over-estimated.

parametrized by the coordinate of the point that they project to under the camera map-
ping.
−1(x) =x , and the bias is

Now, in this case, the ML estimate is given by ˆθ(x) =f

!

!
!
!

ˆθ(x)p(x|θ)dx − θ =

x

=

xg(x − f (θ))dx − θ
!
(x + f (θ))g(x)dx − θ

x

x

=
xg(x)dx + f (θ)
= 0 + f (θ) − θ = 0

x

x

g(x)dx − θ

Assuming that the distribution g(x) is zero-mean. Note, this shows that the estimate of
the original measurement x is unbiased.

Lessons about bias. By a change of parametrization of the world line, the bias has
been changed from inﬁnite to zero. In the present example, there is a natural afﬁne
parametrization of the world line, for which we have seen that the bias is inﬁnite.
However, if we are working in a projective context, then the parameter space has no
natural afﬁne parametrization. In this case, it is somewhat meaningless to speak of any
absolute measurement of bias. A second lesson from the above example is that the ML
estimate of the corrected measurement (as opposed to the world point) is unbiased.

It was also seen that the value of bias is strongly dependent on the noise distribution.
Even the very small tails of the Gaussian distribution have a very large effect on the
computed bias. Of course, a Gaussian distribution of the noise is only a convenient
model for modelling image measurement errors. The exact distribution of image mea-
surement noise is generally unknown, and the conclusion is inescapable that one can
not theoretically compute an exact value for the bias of a given estimator, except for
synthetic data with known noise model.

A3.2 Variance of an estimator

573

A3.2 Variance of an estimator

The other important attribute of an estimator is its variance. Consider an experiment
being repeated many times with the same model parameters, but a different instantia-
tion of the noise at each trial. Applying our estimator to the measured data, we obtain
an estimate for each of these trials. The variance of the estimator is the variance (or
covariance matrix) of the estimated values. More precisely, we can deﬁne the variance
for an estimation problem involving a single parameter as

Varθ(ˆθ) =E θ[(ˆθ(x) − ¯θ)2] =

(ˆθ(x) − ¯θ)2p(x|θ)dx

!

x

!

where

¯θ = Eθ[ˆθ(x)] =

x

ˆθ(x)p(x|θ)dx = θ + bias(ˆθ)

In the case where the parameters θ form a vector, Varθ(ˆθ) is the covariance matrix

Varθ(ˆθ) =E θ[(ˆθ(x) − ¯θ)(ˆθ(x) − ¯θ)T]

(A3.1)

In many cases we might be more interested in the variability of the estimate with respect
to the original parameter θ, which is the mean-squared error of the estimator. This is
easily computed from

Eθ[(ˆθ(x) − θ)(ˆθ(x) − θ)T] = Varθ(ˆθ) +bias( ˆθ) bias(ˆθ)T.

It should be noted that, as with the bias, the variance of an estimator makes good sense
only when there is a natural afﬁne structure on the parameter set, at least locally.

Most estimation algorithms will give the right answer if there is no noise. If an algo-
rithm performs badly when noise is added, this means that either the bias or variance
of the algorithm is high. This is the case, for instance with the DLT algorithm 4.1(p91),
or the unnormalized 8-point algorithm discussed in section 11.1(p279). The variance
of the algorithm grows quickly with added noise.

The Cram´er-Rao lower bound. It is evident that by adding noise to a set of mea-
surements information is lost. Consequently, it is not to be expected that any estimator
can have zero bias and variance in the presence of noise on the measurements. For
unbiased estimators, this notion is formalized in the Cram´er-Rao lower bound, which
is a bound on the variance of an unbiased estimator. To explain the Cram´er-Rao bound,
we need a few deﬁnitions. Given a probability distribution p(x|θ), the Fisher score is
deﬁned as Vθ(x) = ∂θ log p(x|θ). The Fisher Information Matrix is deﬁned to be

F (θ) = Eθ[Vθ(x)Vθ(x)T]

Vθ(x)Vθ(x)Tp(x|θ)dx.

!

=

x

The relevance of the Fisher Information Matrix is expressed in the following result.
Result A3.1. Cram´er-Rao lower bound. For an unbiased estimator ˆθ(x),

det(E[(ˆθ − θ)(ˆθ − θ)T]) ≥ 1/ det F (θ).

A Cram´er-Rao lower bound may also be given in the case of a biased estimator.

574

Appendix 3 Parameter Estimation

A3.3 The posterior distribution

An alternative to the ML estimate is to consider the probability distribution for the
parameters, given the measurements, namely p(θ|x). This is known as the posterior
distribution, namely the distribution for the parameters after the measurements have
been taken. To compute it, we need a prior distribution p(θ) for the parameters before
any measurement has been taken. The posterior distribution can then be computed
from Bayes Law

p(θ|x) =

p(x|θ) p(θ)

.

p(x)

Since the measurement x is ﬁxed, so is its probability p(x), so we may ignore it, leading
to p(θ|x) ≈ p(x|θ) p(θ). The maximum of the posterior distribution is known as the
Maximum A Posteriori (MAP) estimate.

Note. Though the MAP estimate may seem like a good idea, it is important to realize
that it depends on the parametrization of the parameter space. The posterior proba-
bility distribution is proportional to p(x|θ)p(θ). However, p(θ) is dependent on the
parametrization of θ. For instance if p(θ) is a uniform distribution in one parametriza-
tion, it will not be a uniform distribution in a different parametrization that differs
by a non-afﬁne transformation. On the other hand, p(x|θ) does not depend on the
parametrization of θ. Therefore, the result of a change of parametrization is to alter
the posterior distribution in such a way that its maximum will change. If the parameter
space does not have a natural afﬁne coordinate system (for instance if the parameter
space is projective) then the MAP estimate does not really make a lot of sense.
Other estimates based on the posterior distribution are also possible. Given a mea-
surement x, and the posterior distribution p(θ|x), we may wish to make a different
estimate of the parameter θ. One sensible choice is the estimate that minimizes the
expected squared error in the estimate, namely

!

!

ˆθ(x) = argminY E[(cid:10)Y − θ(cid:10)2] = argminY

which is the mean of the posterior distribution.

(cid:10)Y − θ(cid:10)2p(θ|x)dθ,

A further alternative is to minimize the expected absolute error

ˆθ(x) = argminY E[(cid:10)Y − θ(cid:10)] = argminY

(cid:10)Y − θ(cid:10)p(θ|x)dθ,

which is the median of the posterior distribution. Examples of these estimates are
shown in ﬁgure A3.6 and ﬁgure A3.7.

More properties of these estimates are listed in the notes at the end of this appendix.

A3.4 Estimation of corrected measurements

We have seen that in geometric estimation problems, particularly those that involve
projective models, concepts such as bias and variance of the estimator are dependent
on the particular parametrization of the model, for instance a particular projective coor-
dinate frame chosen. Even in cases where a natural afﬁne parametrization of the model

A3.4 Estimation of corrected measurements

575

2

1.75

1.5

1.25

1

0.75

0.5

0.25

0.4

0.2

-0.4

-0.2

0.2

0.4

0.6

0.8

1

-0.2

-0.4

-0.4

-0.2

0.2

0.4

0.6

0.8

1

a

b

Fig. A3.6. Different estimators for θ. (a) For the imaging setup of ﬁgure A3.1: the a posteriori distri-
bution p(θ|x = 0) assuming a Gaussian noise distribution with σ = 0.2, and a prior distribution for
θ, uniform on the interval [−1/2, 1]. The mode (maximum) of this distribution is the Maximal Apriori
(MAP) estimate of θ, which is identical with the ML estimate, because of the assumed uniform distri-
bution for θ. The mean of this distribution (θ = 0.1386) is the estimate that minimizes the expected
squared error E[(ˆθ(x) − ¯θ)2] with respect to the true measurement ¯θ. (b) The cumulative a posteriori
distribution (offset by −0.5). The zero point of this graph is the median of the distribution, namely the
estimate that minimizes E[|ˆθ(x) − ¯θ|]. The median lies at θ = 0.09137.

2

1.75

1.5

1.25

1

0.75

0.5

0.25

0.4

0.2

-0.2

0.2

0.4

0.6

-0.2

-0.4

-0.4

-0.2

0.2

0.4

0.6

0.8

1

a

b

Fig. A3.7. Different estimators with the parabolic noise model. The two graphs show the a posteriori
distribution of θ and its cumulative distribution. In this example, the noise model is 3(1− (x/σ)2)/4σ2,
for −σ ≤ x ≤ σ, where σ = 0.4. The mode of the distribution (θ = 0) is the MAP estimate, identical
with the ML estimate, the mean (θ = 0.1109) minimizes the expected squared error in θ and the median
(θ = 0.0911) minimizes the expected absolute error.

exists, it may be difﬁcult to ﬁnd an unbiased estimator. For instance, the ML estimator
for problems such as triangulation is biased.

We saw however in the example of 1D back-projection discussed in section A3.1,
that if instead of attempting to compute the model (namely the back-projected point)
we estimate the corrected measurement instead, then the ML estimator is unbiased. We
explore this notion further in this section, and show that in a general setting, the ML
estimator of the corrected measurements is not only unbiased but attains the Cram´er-
Rao lower bound, when the noise-model is Gaussian.

Consider an estimation problem that involves ﬁtting a parametrized model to a set of
image measurements. As seen in section 5.1.3(p134) this problem may be viewed as
an estimation problem in a high-dimensional Euclidean space IRN, which is the space
of all image measurements. This is illustrated in ﬁgure 5.2(p135). The estimation
problem is, given a measurement vector X, to ﬁnd the closest point lying on a surface

representing the set of all allowable exact measurements. The vector (cid:23)X represents the

576

Appendix 3 Parameter Estimation

set of “corrected” image measurements that conform to the given model. The model it-
self may depend on a set of parameters θ, such as the fundamental matrix, hypothesized
3D points or other parameters appropriate to the problem.

The estimation of the parameters θ is subject to bias in the same way as we have
seen with the simple problem discussed in section A3.1, and the exact degree of bias
is dependent on the precise parametrization. Generally (for instance in projective-
reconstruction problems) there is no natural afﬁne coordinate system for the model
parameters, though there is a natural afﬁne coordinate system for the image plane.

measurement vector (cid:23)X, then we ﬁnd a more favourable situation. The measurements

If we think of the problem differently, as the problem of directly ﬁnding the corrected

are carried out in the images, which have a natural afﬁne coordinate system, and so
questions of bias in estimating the corrected measurements make more sense. We will
show that, provided the measurement surface is well approximated by its tangent plane,
the ML estimate of the corrected measurement vector is unbiased, provided the noise
is zero-mean. In addition, if the noise is Gaussian and isotropic, then the ML estimate
meets the Cram´er-Rao lower-bound.

The geometric situation is follows. A point X lies on a measurement surface, as
shown in ﬁgure 5.2(p135). Noise is added to this point to obtain a measured point X.

The estimate (cid:23)X of the true point X is obtained by selecting the closest point on the mea-

surement surface to the measured point. We make an assumption that the measurement
surface is effectively planar near X.

is spanned by the ﬁrst d coordinates. We may write X = (XT
a d-vector. The measured point may similarly be written as X = (XT

We may choose a coordinate system in which the measurement surface close to X
N−d)T, where X1 is
2 )T, and its
1 , 0T)T. We suppose that the

projection onto the tangent plane is (cid:23)X = (XT
noise-distribution is given by p(X|X) = g(X − X). Now, the bias of the estimate (cid:23)X is

1 , 0T)T = ((cid:23)XT

1 , XT

1 , 0T

E[((cid:23)X − X)] =

((cid:23)X − X)g(X − X)dX

!

X

((cid:23)X − X)p(X|X)dX =
!
J(X − X)g(X − X)dX
(X − X)g(X − X)dX

X

X

=

= J
= 0

X

!
!

!
!

((cid:23)X − X)((cid:23)X − X)Tp(X|X)dX =
!
J(X − X)(X − X)TJTg(X − X)dX
(X − X)(X − X)Tg(X − X)dXJT

X

X

X

=

= J
= JΣgJT

X

where J is the matrix [Id×d|0d×(N−d)]. This shows that the estimate of X is unbiased as
E[((cid:23)X − X)((cid:23)X − X)T] =
long at g has zero-mean. The variance of the estimate is equal to

((cid:23)X − X)((cid:23)X − X)Tg(X − X)dX

!

where Σg is the covariance matrix of g.

A3.5 Notes and exercises

577

We now compute the Cram´er-Rao lower bound for this estimator, supposing that the
distribution g(X) is Gaussian deﬁned by g(X) = k exp(−(cid:10)X(cid:10)2/2σ2). In this case, the
variance of the estimator is simply σ2Id×d.

We next compute the Fisher information matrix. The probability distribution is

p(X|X) = g(X − X) =k exp(−(cid:10)X − X(cid:10)2/2σ2)

= k exp(−(cid:10)X1 − X1(cid:10)2/2σ2) exp(−(cid:10)X2(cid:10)2/2σ2).

Taking logarithms and derivatives with respect to X1 gives
∂X1 log p(X|X) = −(X1 − X1)/σ2.

The Fisher information matrix is then

1/σ2

(X1 − X1)(X1 − X1)Tg(X1 − X1)g(X2)dX1dX2 = Id×d/σ2.

!

Thus, the Fisher information matrix is the inverse of the covariance matrix for the
estimator. Thus, for the case where the noise distribution is Gaussian, the ML estimator
meets the Cram´er-Rao lower bound, to the extent that the measurement surface is ﬂat.
It should be noticed that the Fisher Information Matrix does not depend on the spe-
ciﬁc shape of the measurement surface, but only on its ﬁrst-order approximation, the
tangent plane. The properties of the estimate does however depend on the shape of the
measurement surface, both as regards its bias and variance. It may also be shown that
if the Cram´er-Rao bound is met, then the noise distribution must be Gaussian. In other
words, if the noise distribution is not Gaussian, then we can not meet the lower bound.

A3.5 Notes and exercises

(i) Show by a speciﬁc example that the a posteriori distribution is altered by a
change of coordinates in the parameter space. Show also that the mean of the
distribution may be altered by such a coordinate change. Thus the mode (MAP
estimate) and mean of the posterior distribution are dependent on the choice of
coordinates for the parameter space.

(ii) Show for any PDF p(θ) deﬁned on IRn, that argminY

mean of the distribution.

(iii) Show for any PDF p(θ) deﬁned on IR, that argminY

median of the distribution. In higher dimensions, show that
is a convex function of Y , and hence has a single minimum. The value of Y
that minimizes this is a higher-dimensional generalization of the median of a
1-dimensional distribution.

(iv) Show that the median of a PDF deﬁned on IR is invariant to reparametrization
of IR. Show by an example that this is not true in higher dimensions, however.

(cid:31) (cid:10)Y − θ(cid:10)2p(θ)dθ is the
(cid:31) |Y − θ|p(θ)dθ is the
(cid:31) (cid:10)Y − θ(cid:10)p(θ)dθ

Appendix 4

Matrix Properties and Decompositions

In this appendix we discuss matrices with particular forms that occur throughout the
book, and also various matrix decompositions.

A4.1 Orthogonal matrices

A square matrix U is known as orthogonal if its transpose is its inverse – symbolically
UTU = I, where I is the identity matrix. This means that the column vectors of U are all
of unit norm and are orthogonal. This may be written uT
uj = δij. From the condition
i
UTU = I one easily deduces that UUT = I. Hence the row vectors of U are also of
unit norm and are orthogonal. Consider once more the equation UTU = I. Taking
determinants leads to the equation (det U)2 = 1, since det U = det UT. Thus if U is
orthogonal, then det U = ±1.
One easily veriﬁes that the orthogonal matrices of a given ﬁxed dimension form a
group, denoted On, since if U and V are orthogonal, then (UV)TUV = VTUTUV = I.
Furthermore, the orthogonal matrices of dimension n with positive determinant form a
group, called SOn. An element of SOn is called an n-dimensional rotation.

Norm-preserving properties of orthogonal matrices. Given a vector x, the notation
(cid:10)x(cid:10) represents its Euclidean length. This can be written as (cid:10)x(cid:10) = (xTx)1/2. An
important property of orthogonal matrices is that multiplying a vector by an orthogonal
matrix preserves its norm. This is easily seen by computing

(Ux)T(Ux) = xTUTUx = xTx.

By the QR decomposition of a matrix is usually meant the decomposition of the
matrix A into a product A = QR, where Q is orthogonal, and R is an upper-triangular
matrix. The letter R stands for Right, meaning upper-triangular. Similar to the QR
decomposition, there are also QL, LQ and RQ decompositions, where L denotes a Left
or lower-triangular matrix. In fact, the RQ decomposition of a matrix is the one that
will be of most use in this book, and which will therefore be discussed here. The most
important case is the decomposition of a 3 × 3 matrix and we will concentrate on this
in the following section.

578

A4.1 Orthogonal matrices

579

A4.1.1 Givens rotations and RQ decomposition
A 3-dimensional Givens rotation is a rotation about one of the three coordinate axes.
The three Givens rotations are

 1

Qx =



 c

c −s
c
s

Qy =

1−s



s

c

 c −s

s

Qz =

c



1

(A4.1)

where c = cos(θ) and s = sin(θ) for some angle θ and blank entries represent zeros.
Multiplying a 3× 3 matrix A on the right by (for instance) Qz has the effect of leaving
the last column of A unchanged, and replacing the ﬁrst two columns by linear combi-
nations of the original two columns. The angle θ may be chosen so that any given entry
in the ﬁrst two columns becomes zero.
For instance, to set the entry A21 to zero, we need to solve the equation ca21 + sa22 =
0. The solution to this is c = −a22/(a2
21)1/2. It is
required that c2 + s2 = 1 since c = cos(θ) and s = sin(θ), and the values of c and s
given here satisfy that requirement.
The strategy of the RQ algorithm is to clear out the lower half of the matrix one entry
at a time by multiplication by Givens rotations. Consider the decomposition of a 3 × 3
matrix A as A = RQ where R is upper-triangular and Q is a rotation matrix. This may
take place in three steps. Each step consists of multiplication on the right by a Givens
rotation to set a chosen entry of the matrix A to zero. The sequence of multiplications
must be chosen in such a way as not to disturb the entries that have already been set to
zero. An implementation of the RQ decomposition is given in algorithm A4.1.

21)1/2 and s = a21/(a2

22 + a2

22 + a2

Objective
Carry out the RQ decomposition of a 3 × 3 matrix A using Givens rotations.
Algorithm

(i) Multiply by Qx so as to set A32 to zero.
(ii) Multiply by Qy so as to set A31 to zero. This multiplication does not change the second

column of A, hence A32 remains zero.

(iii) Multiply by Qz so as to set A21 to zero. The ﬁrst two columns are replaced by linear

combinations of themselves. Thus, A31 and A32 remain zero.

Algorithm A4.1. RQ decomposition of a 3 × 3 matrix.

Other sequences of Givens rotations may be chosen to give the same result. As
a result of these operations, we ﬁnd that AQxQyQz = R where R is upper-triangular.
QT
Consequently, A = RQT
x is a rotation. In
z
addition, the angles θx, θy and θz associated with the three Givens rotations provide a
parametrization of the rotation by three Euler angles, otherwise known as roll, pitch
and yaw angles.

QT
x , and so A = RQ where Q = QT

QT
y

QT
y

z

It should be clear from this description of the decomposition algorithm how similar
QR, QL and LQ factorizations may be carried out. Furthermore, the algorithm is easily
generalized to higher dimensions.

580

Appendix 4 Matrix Properties and Decompositions

A4.1.2 Householder matrices and QR decomposition
For matrices of larger dimension, the QR decomposition is more efﬁciently carried out
using Householder matrices. The symmetric matrix
Hv = I − 2vvT/vTv
v Hv = I, and so Hv is orthogonal.

has the property that HT
Let e1 be the vector (1, 0, . . . ,0) T, and let x be any vector. Let v = x ± (cid:10)x(cid:10)e1. One
easily veriﬁes that Hvx = ∓(cid:10)x(cid:10)e1; thus Hv is an orthogonal matrix that transforms the
vector x to a multiple of e1. Geometrically Hv is a reﬂection in the plane perpendicular
to v, and v = x ± (cid:10)x(cid:10)e1 is a vector that bisects x and ±(cid:10)x(cid:10)e1. Thus reﬂection in the
v direction takes x to ∓(cid:10)x(cid:10)e1. For reasons of stability, the sign ambiguity in deﬁning
v should be resolved by setting

(A4.2)

v = x + sign(x1)(cid:10)x(cid:10)e1.

(A4.3)

If A is a matrix, x is the ﬁrst column of A, and v is deﬁned by (A4.3), then forming
the product HvA will clear out the ﬁrst column of the matrix, replacing the ﬁrst column
by ((cid:10)x(cid:10), 0, 0, . . . ,0) T. One continues left multiplication by orthogonal Householder
matrices to clear out the below-diagonal part of the matrix A. In this way, one ﬁnds
that eventually QA = R, where Q is a product of orthogonal matrices and R is an upper-
triangular matrix. Therefore, one has A = QTR. This is the QR decomposition of the
matrix A.

When multiplying by Householder matrices it is inefﬁcient to form the Householder

matrix explicitly. Multiplication by a vector a may be carried out most efﬁciently as

Hva = (I − 2vvT/vTv)a = a − 2v(vTa)/vvT

(A4.4)

and the same holds for multiplication by a matrix A. For more about Householder
matrices and the QR decomposition, the reader is referred to [Golub-89].

Note.

In the QR or RQ decomposition, R refers to an upper-triangular matrix and
Q refers to an orthogonal matrix. In the notation used elsewhere in this book, R refers
usually to a rotation (hence orthogonal) matrix.

A4.2 Symmetric and skew-symmetric matrices

Symmetric and skew-symmetric matrices play an important role in this book. A ma-
trix is called symmetric if AT = A and skew-symmetric if AT = −A. The eigenvalue
decompositions of these matrices are summarized in the following result.

Result A4.1. Eigenvalue decomposition.

(i) If A is a real symmetric matrix, then A can be decomposed as A = UDUT, where U
is an orthogonal matrix and D is a real diagonal matrix. Thus, a real symmetric
matrix has real eigenvalues, and the eigenvectors are orthogonal.

(ii) If S is real and skew-symmetric, then S = UBUT where B is a block-diagonal

A4.2 Symmetric and skew-symmetric matrices

(cid:17)

(cid:18)

581

matrix of the form diag(a1Z, a2Z, . . . , amZ, 0, . . . ,0), where Z =
The eigenvectors of S are all purely imaginary, and a skew-symmetric matrix of
odd order is singular.

.

0

1−1 0

A proof of this result is given in [Golub-89].

Jacobi’s method.
In general, eigenvalue extraction from arbitrary matrices is a dif-
ﬁcult numerical problem. For real symmetric matrices however, a very stable method
exists: Jacobi’s method. An implementation of this algorithm is given in [Press-88].

Cross products
Of particular interest are 3 × 3 skew-symmetric matrices. If a = (a1, a2, a3)T is a
3-vector, then one deﬁnes a corresponding skew-symmetric matrix as follows:

 0 −a3

a3
−a2

a2
0 −a1
a1

0

 .

[a]× =

(A4.5)

Note that any skew-symmetric 3 × 3 matrix may be written in the form [a]× for a
suitable vector a. Matrix [a]× is singular, and a is its null-vector (right or left). Hence,
a 3 × 3 skew-symmetric matrix is deﬁned up to scale by its null-vector.
The cross product (or vector product) of two 3-vectors a × b (sometimes written
a ∧ b) is the vector (a2b3 − a3b2, a3b1 − a1b3, a1b2 − a2b1)T. The cross product is
related to skew-symmetric matrices according to

(cid:15)

(cid:16)T

a × b = [a]×b =

aT[b]×

.

(A4.6)

Cofactor and adjoint matrices. Let M be a square matrix. By M∗
is meant the matrix
is equal to (−1)i+j det ˆMij,
of cofactors of M. That is, the (ij)-th entry of the matrix M∗
where ˆMij is the matrix obtained from M by striking out the i-th row and j-th column.
The transpose of the cofactor matrix M∗
is known as the adjoint of M, and denoted
adj(M).

If M is invertible, then it is well known that

(A4.7)
where M−T is the inverse transpose of M. This formula does not hold for non-invertible
matrices, but adj(M) M = M adj(M) = det(M) I is always valid.

M∗

= det(M) M−T

The cofactor matrix is related to the way matrices distribute with respect to the cross

product.
Lemma A4.2. If M is any 3 × 3 matrix (invertible or not), and x and y are column
vectors, then

(Mx) × (My) =M ∗

(x × y).

(A4.8)

582

Appendix 4 Matrix Properties and Decompositions

This equation may be written as [Mx]×M = M∗

[x]×, dropping y which is inessential.
Now, putting t = Mx and assuming M is invertible, one obtains a rule for commuting a
skew-symmetric matrix [t]× with any non-singular matrix M. One may write (A4.8) as
follows.

Result A4.3. For any vector t and non-singular matrix M one has
[M−1t]× = M−T[M−1t]× (up to scale).

[t]×M = M∗

Note (see result 9.9(p254)) that [t]×M is the form of the fundamental matrix for a pair
of cameras P = [I | 0] and P(cid:2)
= [M|t]. The formula of result A4.3 is used in deriving
alternative forms (9.2–p244) for the fundamental matrix.
A curious property of 3 × 3 skew-symmetric matrices is that up to scale,
[a]× = [a]×[a]×[a]× (including scale [a]3× = −(cid:10)a(cid:10)2[a]×). This is easily veriﬁed, since
the right hand side is clearly skew-symmetric and its null-space generator is a. The
next result follows immediately:
Result A4.4. If F = [e(cid:2)
]×[e(cid:2)
[e(cid:2)
M = [e(cid:2)

]×F = F (up to scale). Hence one may decompose F as F = [e(cid:2)
]×F.

]×M is a fundamental matrix (a 3 × 3 singular matrix), then
]×M, where

A4.2.1 Positive-deﬁnite symmetric matrices
The special class of real symmetric matrices that have positive real eigenvalues are
called positive-deﬁnite symmetric matrices. We list some of the important properties
of a positive-deﬁnite symmetric real matrix.

Result A4.5. Positive-deﬁnite symmetric real matrix.

(i) A symmetric matrix A is positive-deﬁnite if and only if xTAx > 0 for any non-

zero vector x.

(ii) A positive-deﬁnite symmetric matrix A may be uniquely decomposed as A = KKT

where K is an upper-triangular real matrix with positive diagonal entries.

Proof. The ﬁrst part of this result follows almost immediately from the decomposition
A = UDUT. As for the second part, since A is symmetric and positive-deﬁnite, it may
be written as A = UDUT where D is diagonal, real and positive and U is orthogonal. We
may take the square root of D, writing D = EET where E is diagonal. Then A = VVT
where V = UE. The matrix V is not upper-triangular. However, we may apply the RQ-
decomposition (section A4.1.1) to write V = KQ where K is upper-triangular and Q is
orthogonal. Then A = VVT = KQQTKT = KKT. This is the Cholesky factorization of A.
One may ensure that the diagonal entries of K are all positive by multiplying K on the
right by a diagonal matrix with diagonal entries equal to ±1. This will not change the
product KKT.

A4.3 Representations of rotation matrices

583

1 = K2KT

Now, we prove uniqueness of the factorization. Speciﬁcally, if K1 and K2 are two upper-
triangular matrices satisfying K1KT
. Since the left side of
this equation is upper-triangular, and the right side is lower-triangular, they must both
in fact be diagonal. Thus D = K−1
K1 is the inverse transpose
of KT
, and so D is equal to its own inverse transpose, and hence is a diagonal matrix
with diagonal entries equal to ±1. If both K1 and K2 have positive diagonal entries then
2
D = I, and K1 = K2.

K1 = KT
2
. However, K−1

2 then K−1
K−T

2

K1 = KT
2

K−T

1

K−T

1

2

1

2

The above proof gives a constructive method for computing the Cholesky factoriza-
tion. There is, however, a very simple and more efﬁcient direct method for computing
the Cholesky factorization. See [Press-88] for an implementation.

A4.3 Representations of rotation matrices

A4.3.1 Rotations in n-dimensions
Given a matrix T, we deﬁne eT to be the sum of the series

eT = I + T + T2/2! + . . . + Tk/k! + . . .

This series converges absolutely for all values of T. Now, we consider powers
of a skew-symmetric matrix. According to result A4.1 a skew-symmetric ma-
trix can be written as S = UBUT where B is block diagonal of the form B =
diag(a1Z, a2Z, . . . , amZ, 0, . . . ,0), matrix U is orthogonal, and Z2 = −I2×2. We ob-
serve that the powers of Z are

Z2 = −I ; Z3 = −Z ; Z4 = I

and so on. Thus,

eZ = I + Z − I/2! − Z/3! + . . . = cos(1)I + sin(1)Z = R2×2(1)

Where R2×2(1) means the 2 × 2 matrix representing a rotation through 1 radian. More
generally,

eaZ = cos(a)I + sin(a)Z = R2×2(a) .

With S = UBUT as above, it now follows that

eS = UeBUT = U diag(R(a1), R(a2), . . . ,R (am), 1, . . . ,1) UT

Thus eS is a rotation matrix. On the other hand, any rotation matrix may be written
in the block-diagonal form U diag(R(a1), R(a2), . . . ,R (am), 1, . . . ,1) UT, and it follows
that the matrices eS where S is an n × n skew-symmetric are exactly the set of n-
dimensional rotation matrices.

A4.3.2 Rotations in 3-dimensions
If t is a 3-vector, then [t]× is a skew-symmetric matrix, and any 3 × 3 skew-symmetric
matrix is of this form. Consequently, any 3-dimensional rotation can be written as e[t]×.
We seek to describe the rotation e[t]×.

584

Appendix 4 Matrix Properties and Decompositions

Let [t]× = U diag(aZ, 0) UT. Then by matching the Frobenius norms of the matrices
on both sides, we see that a = (cid:10)t(cid:10). Thus,

e[t]× = U diag(R((cid:10)t(cid:10)), 1) UT .

Thus, e[t]× represents a rotation through an angle (cid:10)t(cid:10). It is easily veriﬁed that u3, the
3-rd column of U is the eigenvector of U diag(R, 1) UT with unit eigenvector, hence the
axis of rotation. However, [t]×u3 = Udiag(aZ, 0)UTu3 = U diag(aZ, 0)(0, 0, 1)T = 0.
Since u3 is the generator of the null space of [t]×, it must be that u3 is a unit vector in
the direction of t. We have shown
Result A4.6. The matrix e[t]× is a rotation matrix representing a rotation through an
angle (cid:10)t(cid:10) about the axis represented by the vector t.
This representation of a rotation is called the angle-axis representation.
We may write a speciﬁc formula for the rotation matrix corresponding to e[t]×. We
observe that [t]3× = −(cid:10)t(cid:10)2 [t]× = −(cid:10)t(cid:10)3 [ˆt]×, where ˆt represents a unit vector in the
direction t. Then, with sinc(θ) representing sin(θ)/θ, we have

e[t]× = I + [t]× + [t]2×/2! + [t]3×/3! + [t]4×/4! + . . .

= I + (cid:10)t(cid:10) [ˆt]× + (cid:10)t(cid:10)2 [ˆt]2×/2! − (cid:10)t(cid:10)3 [ˆt]×/3! − (cid:10)t(cid:10)4 [ˆt]2×/4! + . . .
= I + sin(cid:10)t(cid:10) [ˆt]× + (1 − cos(cid:10)t(cid:10)) [ˆt]2×
= I + sinc(cid:10)t(cid:10) [t]× +
(cid:10)t(cid:10)2
[t]2×
= cos(cid:10)t(cid:10)I + sinc(cid:10)t(cid:10) [t]× +

1 − cos(cid:10)t(cid:10)

1 − cos(cid:10)t(cid:10)

(cid:10)t(cid:10)2

ttT

(A4.9)

where the last line follows from the identity [t]2× = ttT − (cid:10)t(cid:10)2I.
Some properties of these representations:

(i) Extraction of the axis and rotation angle from a rotation matrix R is just a little
tricky. The unit rotation axis v can be found as the eigenvector corresponding
to the unit eigenvalue – that is by solving (R − I)v = 0. Next, it is easily seen
from (A4.9) that the rotation angle φ satisﬁes
2 cos(φ) = (trace(R) − 1)
2 sin(φ)v = (R32 − R23, R13 − R31, R21 − R12)T.

(A4.10)

Writing this second equation as 2 sin(φ)v = ˆv, we can then compute 2 sin(φ) =
vT ˆv. Now, the angle φ can be computed from sin(φ) and cos(φ) using a two-
argument arctan function (such as the C-language function atan2(y, x)).
It has often been written that φ can be computed directly from (A4.10) using
arccos or arcsin. However, this method is not numerically accurate, and fails
to ﬁnd the axis when φ = π.

(ii) To apply a rotation R(t) to some vector x, it is not necessary to construct the

matrix representation of t. In fact

A4.4 Singular value decomposition

(cid:20)
I + sinc(cid:10)t(cid:10)[t]× +
= x + sinc(cid:10)t(cid:10)t × x +

R(t)x =

(cid:21)

1 − cos(cid:10)t(cid:10)
1 − cos(cid:10)t(cid:10)

(cid:10)t(cid:10)2

(cid:10)t(cid:10)2

x

[t]2×
t × (t × x)

585

(A4.11)

(iii) If t is written as t = θˆt, where ˆt is a unit vector in the direction of the axis,
and θ = (cid:10)t(cid:10) is the angle of rotation, then (A4.9) is equivalent to the Rodrigues
formula for a rotation matrix:

R(θ, ˆt) = I + sin θ[ˆt]× + (1 − cos θ)[ˆt]2×

(A4.12)

A4.3.3 Quaternions
Three-dimensional rotations may also be represented by unit quaternions. A unit
quaternion is a 4-vector and may be written in the form q = (v sin(θ/2), cos(θ/2))T, as
may indeed any unit 4-vector. Such a quaternion represents a rotation about the vector
v through the angle θ. This is a 2-to-1 representation in that both q and −q represent
the same rotation. To check this, note that −q = (v sin(θ/2 + π), cos(θ/2 + π))T,
which represents a rotation through θ + 2π = θ.

The relationship between the angle-axis representation of a rotation and the quater-
nion representation is easily determined. Given a vector t, the angle-axis representation
of a rotation, the corresponding quaternion is easily seen to be
t ↔ q = (sinc((cid:10)t(cid:10)/2)t, cos((cid:10)t(cid:10)/2))T

A4.4 Singular value decomposition

The singular value decomposition (SVD) is one of the most useful matrix decomposi-
tions, particularly for numerical computations. Its most common application is in the
solution of over-determined systems of equations.

Given a square matrix A, the SVD is a factorization of A as A = UDVT, where U and V
are orthogonal matrices, and D is a diagonal matrix with non-negative entries. Note that
it is conventional to write VT instead of V in this decomposition. The decomposition
may be carried out in such a way that the diagonal entries of D are in descending order,
and we will assume that this is always done. Thus a circumlocutory phrase such as
“the column of V corresponding to the smallest singular value” is replaced by “the last
column of V.”
The SVD also exists for non-square matrices A. Of most interest is the case where A
has more rows than columns. Speciﬁcally, let A be an m × n matrix with m ≥ n. In
this case A may be factored as A = UDVT where U is an m × n matrix with orthogonal
columns, D is an n × n diagonal matrix and V is an n × n orthogonal matrix. The fact
that U has orthogonal columns means that UTU = In×n. Furthermore U has the norm-
preserving property that (cid:10)Ux(cid:10) = (cid:10)x(cid:10) for any vector x, as one readily veriﬁes. On the
other hand, UUT is in general not the identity unless m = n.

Not surprisingly, one can also deﬁne a singular value decomposition for matrices

586

Appendix 4 Matrix Properties and Decompositions

with more columns than rows, but generally this will not be of interest to us. Instead
on the occasional instances in this book where we need to take the singular value de-
composition of a matrix A with m < n, it is appropriate to extend A by adding rows of
zeros to obtain a square matrix, and then take the SVD of this resulting matrix. Usually
this will be done without special remark.
Common implementations of the SVD, such as the one in [Press-88], assume that
m ≥ n. Since in this case the matrix U has the same dimension m × n as the input,
matrix A may be overwritten by the output matrix U.

Implementation of the SVD. A description of the singular value decomposition al-
gorithm, or a proof of its existence, is not given in this book. For a description of how
the algorithm works, the reader is referred to [Golub-89]. A practical implementation
of the SVD is given in [Press-88]. However, the implementation of the SVD given in
the ﬁrst edition of “Numerical Recipes in C” can sometimes give incorrect results. The
version of the algorithm given in the second edition [Press-88] of “Numerical Recipes
in C” corrects mistakes in the earlier version.

Singular values and eigenvalues. The diagonal entries of matrix D in the SVD are
non-negative. These entries are known as the singular values of the matrix A. They are
not the same thing as eigenvalues. To see the connection of the singular values of A with
eigenvalues, we start with A = UDVT. From this it follows that ATA = VDUTUDVT =
VD2VT. Since V is orthogonal, VT = V−1, and so ATA = VD2V−1. This is the deﬁning
equation for eigenvalues, indicating that the entries of D2 are the eigenvalues of ATA
and the columns of V are the eigenvectors of ATA. In short, the singular values of A are
the square-roots of the eigenvalues of ATA.

Note, ATA is symmetric and positive-semi-deﬁnite (see section A4.2.1 above), so
its eigenvalues are real and non-negative. Consequently, singular values are real and
non-negative.

Computational complexity of the SVD
The computational complexity of the SVD depends on how much information needs
to be returned. For instance in algorithm A5.4 to be considered later, the solution to
the problem is the last column of the matrix V in the SVD. The matrix U is not used,
and does not need to be computed. On the other hand, algorithm A5.1 of section A5.1
requires the complete SVD to be computed. For systems of equations with many more
rows than columns, the extra effort required to compute the matrix U is substantial.
Approximate numbers of ﬂoating-point operations (ﬂops) required to compute the
SVD of an m × n matrix are given in [Golub-89]. To ﬁnd matrices U, V and D, a total
of 4m2n + 8mn2 + 9n3 ﬂops are needed. However, if only the matrices V and D are
required, then only 4mn2 + 8n3 ﬂops are required. This is an important distinction,
since this latter expression does not contain any term in m2. Speciﬁcally, the number
of operations required to compute U varies as the square of m, the number of rows.
On the other hand, the complexity of computing D and V is linear in m. For cases
where there are many more rows than columns, therefore, it is important (supposing

A4.4 Singular value decomposition

587

computation time is an issue) to avoid computing the matrix U unless it is needed. To
illustrate this point we consider the DLT algorithm for camera resection, described in
chapter 7. In this algorithm a 3×4 camera matrix is computed from a set of n 3D to 2D
point matches. The solution involves using algorithm A5.4, to solve a set of equations
Ap = 0 where A is a 2n × 12 matrix. The solution vector p is the last column of the
matrix V in an SVD, A = UDVT. Thus the matrix U is not required. Table A4.1 gives the
total number of ﬂops for carrying out the SVD for six (the minimum number), 100 or
1000 point correspondences.

# points

# equations
per point

# equations
(m)

#unknowns

(n)

# operations
not computing U

# operations
computing U

6
100
1000

2
2
2

12
200
2000

12
12
12

20,736
129,024
1,165,824

36,288
2,165,952
194,319,552

Table A4.1. Comparison of the number of ﬂops required to compute the SVD of a matrix of size m × n,
for varying values of m and for n = 12. Note that the computational complexity increases sub-linearly
in the number of equations when U is not computed. On the other hand, the extra computational burden
of computing U is very large, especially for large numbers of equations.

Further reading. Two invaluable text books for this area are [Golub-89] and
[Lutkepohl-96].

Appendix 5

Least-squares Minimization

In this appendix we discuss numerical algorithms for solving linear systems of equa-
tions under various constraints. As will be seen such problems are conveniently solved
using the SVD.

A5.1 Solution of linear equations

Consider a system of equations of the form Ax = b. Let A be an m × n matrix. There
are three possibilities:

(i) If m < n there are more unknowns than equations. In this case, there will not

be a unique solution, but rather a vector space of solutions.

(ii) If m = n there will be a unique solution as long as A is invertible.

(iii) If m > n then there are more equations than unknowns. In general the system
will not have a solution unless by chance b lies in the span of the columns of A.

Least-squares solutions: full-rank case. We consider the case m ≥ n and assume
for the present that A is known to be of rank n. If a solution does not exist, then in many
cases it still makes sense to seek a vector x that is closest to providing a solution to the
system Ax = b. In other words, we seek x such that (cid:10)Ax − b(cid:10) is minimized, where
(cid:10) · (cid:10) represents the vector norm. Such an x is known as the least-squares solution to
the over-determined system. The least-squares solution is conveniently found using the
SVD as follows.
We seek x that minimizes (cid:10)Ax−b(cid:10) = (cid:10)UDVTx−b(cid:10). Because of the norm-preserving
property of orthogonal transforms, (cid:10)UDVTx − b(cid:10) = (cid:10)DVTx − UTb(cid:10), and this is the
quantity that we want to minimize. Writing y = VTx and b(cid:2)
= UTb, the problem
becomes one of minimizing (cid:10)Dy − b(cid:2)(cid:10) where D is an m × n matrix with vanishing

588



d1

d2

dn

...

0





 =

y1
y2
...
yn



(cid:2)
b
(cid:2)
1
b
2
...
(cid:2)
b
n
(cid:2)
n+1

...
(cid:2)
b
m
(cid:2)
1, b

b

589

 .

off-diagonal entries. This set of equations is of the form

A5.1 Solution of linear equations

Clearly, the nearest Dy can approach to b(cid:2)
(cid:2)
n, 0, . . . ,0) T, and
(cid:2)
this is achieved by setting yi = b
i/di for i = 1, . . . , n. Note that the assumption
rankA = n ensures that di (cid:5)= 0. Finally, one retrieves x from x = Vy. The complete
algorithm is

is the vector (b

(cid:2)
2, . . . , b

Objective
Find the least-squares solution to the m × n set of equations Ax = b, where m > n and
rank A = n.

Algorithm

(i) Find the SVD A = UDVT.
(ii) Set b(cid:1) = UTb.
(iii) Find the vector y deﬁned by yi = b
(iv) The solution is x = Vy.

(cid:1)
i/di, where di is the i-th diagonal entry of D.

Algorithm A5.1. Linear least-squares solution to an over-determined full-rank set of linear equations.

Deﬁcient-rank systems. Sometimes one is called upon to solve a system of equations
that is expected not to be of full column rank. Thus, let r = rankA < n, where n is the
number of columns of A. It is possible that because of noise corruption, the matrix A
actually has rank greater than r, but we wish to enforce the rank r constraint because
of theoretical considerations, derived from the particular problem being considered. In
this case, there will be an (n− r)-parameter family of solutions to the set of equations,
where r = rankA < n. This family of solutions is appropriately solved using the SVD,
as follows:
This algorithm gives an (n−r)-parameter family (parametrized by the indeterminate
values λi) of least-squares solutions to the deﬁcient-rank system. The justiﬁcation of
this algorithm is similar to that of algorithm A5.1 for the least-squares solution of full-
rank systems.

Systems of unknown rank.
In most cases encountered in this book, the rank of a
system of linear equations will be known theoretically in advance of solution. If the
rank of the system of equations is not known, then one must guess at its rank. In this
case, it is appropriate to set singular values that are small compared with the largest

590

Appendix 5 Least-squares Minimization

Objective
Find the general solution to a set of equations Ax = b where A is an m × n matrix of rank
r < n.

Algorithm

order.

(i) Find the SVD A = UDVT, where the diagonal entries di of D are in descending numerical
(ii) Set b(cid:1) = UTb.
(iii) Find the vector y deﬁned by yi = b
(iv) The solution x of minimum norm (cid:10)x(cid:10) is Vy.
(v) The general solution is x = Vy + λr+1vr+1 + . . . + λnvn, where vr+1, . . . , vn are

(cid:1)
i/di for i = 1, . . . , r, and yi = 0 otherwise.

the last n − r columns of V.

Algorithm A5.2. General solution to deﬁcient-rank system

singular value to zero. Thus, if di/d0 < δ where δ is a small constant of the order of
the machine precision1 then one sets yi = 0. A least-squares solution is then given by
x = Vy as before.

A5.2 The pseudo-inverse

Given a square diagonal matrix D, we deﬁne its pseudo-inverse to be the diagonal ma-
trix D+ such that

 

D+

ii =

0
−1
D
ii

if Dii = 0
otherwise.

Now, consider an m × n matrix A with m ≥ n. Let the SVD of A be A = UDVT. We
deﬁne the pseudo-inverse of A to be the matrix

A+ = VD+UT.

where b(cid:2)

(A5.1)
One very simply veriﬁes that the vector y in algorithm A5.1 or algorithm A5.2 is
nothing more than D+b(cid:2)
Result A5.1. The least-squares solution to an m × n system of equations Ax = b of
rank n is given by x = A+b. In the case of a deﬁcient-rank system, x = A+b is the
solution that minimizes (cid:10)x(cid:10).
As remarked when discussing the SVD, if A has fewer rows than columns, then this
result may be applied after extending A to a square matrix by adding rows of zeros.

= UTb. Thus,

Symmetric matrices.
For symmetric matrices, one may generalize the pseudo-
inverse as follows. This generalization was used in chapter 5, section 5.2.3(p142) for
discussing singular covariance matrices. If A is a non-invertible symmetric matrix, and
−1XT. One can see that this depends
XTAX is invertible, then we write A+X def= X(XTAX)
only on the span of the columns of X. In other words, if X is replaced by XB for any
invertible matrix B, then A+X = A+XB. Otherwise stated, A+X depends only on the (left)
1 Machine precision is the largest ﬂoating point value  such that 1.0 +  = 1.0.

A5.2 The pseudo-inverse

591

null-space of X, namely the space of vectors perpendicular to the columns of X. Deﬁne
the null-space NLX = {xT | xTX = 0}. One ﬁnds that under a simple condition, A+X is
the pseudo-inverse of A:

Result A5.2. Let A be a symmetric matrix, then A+X def= X(XTAX)
if NL(X) =N L(A).

−1XT = A+ if and only

Only a sketch proof is given. The necessity is obvious, since NL(X) and NL(A) are
the null-spaces of the left and right sides of the equation. To prove the converse, one
may assume that the columns of X are orthonormal, since as shown above, only the
null-space of X is of importance. Thus, X may be extended by adding further columns
X(cid:2)
]. Now, the rows of X(cid:2)T span the null-space
of X, and hence, by assumption, of A. Now, the proof is completed in a few lines by
−1XT with the deﬁnition (A5.1) of the pseudo-
comparing the deﬁnition A+X def= X(XTAX)
inverse.

to form an orthogonal matrix U = [X|X(cid:2)

A5.2.1 Linear least-squares using normal equations
The linear least-squares problem may also be solved by a method involving the so-
called normal equations. Once more, we consider the set of linear equations Ax = b
where A is an m × n matrix with m > n. In general, no solution x will exist for
this set of equations. Consequently, the task is to ﬁnd the vector x that minimizes the
norm (cid:10)Ax − b(cid:10). As the vector x varies over all values, the product Ax varies over the
complete column space of A, that is, the subspace of IRm spanned by the columns of
A. The task therefore is to ﬁnd the closest vector to b that lies in the column space of
A, where closeness is deﬁned in terms of vector norm. Let x be the solution to this
problem; thus Ax is the closest point to b. In this case, the difference Ax − b must be
a vector orthogonal to the column space of A. This means, explicitly, that Ax − b is
perpendicular to each of the columns of A, and hence AT(Ax− b) = 0. Multiplying out
and separating terms gives an equation

(ATA)x = ATb.

(A5.2)
This is a square n × n set of linear equations, called the normal equations. This set
of equations may be solved to ﬁnd the least-squares solution to the problem Ax = b.
Even if A is not of full rank (rank n), this set of equations should have a solution, since
ATb lies in the column space of ATA. In the case where A has rank n, the matrix ATA is

Objective
Find x that minimizes (cid:10)Ax − b(cid:10).
Algorithm

(i) Solve the normal equations ATAx = ATb.
(ii) If ATA is invertible, then the solution is x = (ATA)−1ATb.

Algorithm A5.3. Linear least-squares using the normal equations.

Appendix 5 Least-squares Minimization

−1AT.

−1ATb. Since x = A+b, this implies

592
invertible, and so x may be found by x = (ATA)
the following result, which is also easily veriﬁed directly:
Result A5.3. If A is an m × n matrix of rank n, then A+ = (ATA)
This result is useful in theoretical analysis, as well as being a computationally simpler
method than using the SVD to compute a pseudo-inverse if n is small compared with
m (so that computing the inverse of (ATA) is inexpensive compared to computing the
SVD of A).
Vector space norms. One sometimes wishes to minimize Ax − b with respect to a
different norm on the vector space IRn. The usual norm in a vector space IRn is given in
terms of the usual inner product. Thus, for two vectors a and b in IRn one may deﬁne
the inner product a · b to be aTb. The norm of a vector a is then (cid:10)a(cid:10) = (a · a)1/2 =
(aTa)1/2. One notes the properties:

(i) The inner product is a symmetric bilinear form on IRn.
(ii) (cid:10)a(cid:10) > 0 for all non-zero vectors a ∈ IRn.

We say that the inner product is a positive-deﬁnite symmetric bilinear form. It is pos-
sible to deﬁne other inner products on a vector space IRn. Let C be a real symmetric
positive-deﬁnite matrix, and deﬁne a new inner product C(a, b) =a TCb. The symme-
try of the inner product follows from the symmetry of C. A norm may be deﬁned by
(cid:10)a(cid:10)C = (aTCa)1/2, and this is deﬁned and positive-deﬁnite, because C is assumed to be
a positive-deﬁnite matrix.

Weighted linear least-squares problems. Sometimes one desires to solve a weighted
least-squares problem of the form Ax− b = 0 by minimizing the C-norm (cid:10)Ax− b(cid:10)C of
the error. Here C is a positive-deﬁnite symmetric matrix deﬁning an inner product and
a norm (cid:10) · (cid:10)C on IRn. As before, one can argue that the minimum error vector Ax − b
must be orthogonal in the inner product deﬁned by C to the column space of A. This
leads to a requirement ATC(Ax − b) = 0. Rearranging this one obtains the weighted
normal equations:

(ATCA)x = ATCb.

(A5.3)

The most common weighting will be where C is a diagonal matrix, corresponding to
independent weights in each of the axial directions in IRn. However, general weighting
matrices C may be used also.

A5.3 Least-squares solution of homogeneous equations

Similar to the previous problem is that of solving a set of equations of the form Ax = 0.
This problem comes up frequently in reconstruction problems. We consider the case
where there are more equations than unknowns – an over-determined set of equations.
The obvious solution x = 0 is not of interest – we seek a non-zero solution to the set
of equations. Observe that if x is a solution to this set of equations, then so is kx for
any scalar k. A reasonable constraint would be to seek a solution for which (cid:10)x(cid:10) = 1.

A5.4 Least-squares solution to constrained systems

593

In general, such a set of equations will not have an exact solution. Suppose A has
dimension m× n then there is an exact solution if and only if rank(A) < n – the matrix
A does not have full column rank. In the absence of an exact solution we will normally
seek a least-squares solution. The problem may be stated as
• Find the x that minimizes (cid:10)Ax(cid:10) subject to (cid:10)x(cid:10) = 1 .
This problem is solvable as follows. Let A = UDVT. The problem then requires us to
minimize (cid:10)UDVTx(cid:10). However, (cid:10)UDVTx(cid:10) = (cid:10)DVTx(cid:10) and (cid:10)x(cid:10) = (cid:10)VTx(cid:10). Thus, we need
to minimize (cid:10)DVTx(cid:10) subject to the condition (cid:10)VTx(cid:10) = 1. We write y = VTx, and
the problem is: minimize (cid:10)Dy(cid:10) subject to (cid:10)y(cid:10) = 1. Now, D is a diagonal matrix with
its diagonal entries in descending order. It follows that the solution to this problem is
y = (0, 0, . . . ,0 , 1)T having one non-zero entry, 1 in the last position. Finally x = Vy
is simply the last column of V. The method is summarized in algorithm A5.4.

Objective
Given a matrix A with at least as many rows as columns, ﬁnd x that minimizes (cid:10)Ax(cid:10) subject to
(cid:10)x(cid:10) = 1.
Solution
x is the last column of V, where A = UDVT is the SVD of A.

Algorithm A5.4. Least-squares solution of a homogeneous system of linear equations.

As mentioned in section A4.4 the last column of V may alternatively be described as

the eigenvector of ATA corresponding to the smallest eigenvalue.

A5.4 Least-squares solution to constrained systems

In the previous section, we considered a method of least-squares solution of equations
of the form Ax = 0. Such problems may arise from situations where measurements
are made on a set of image features. With exact measurements and an exact imaging
model the mathematical model predicts an exact solution to this system. In the case of
inexact image measurements, or noise, there will not be an exact solution. In this case,
it makes sense to ﬁnd a least-squares solution.

On other occasions, however, some of the equations represented by rows of the ma-
trix A are derived from precise mathematical constraints, and should be satisﬁed ex-
actly. This set of constraints may be described by a matrix equation Cx = 0, which
should be satisﬁed exactly. Others of the equations are derived from image measure-
ments and are subject to noise. This leads to a problem of the following sort:
• Find the x that minimizes (cid:10)Ax(cid:10) subject to (cid:10)x(cid:10) = 1 and Cx = 0 .
This problem can be solved in the following manner. The condition that x satisﬁes
Cx = 0 means that x lies perpendicular to each of the rows of C. The set of all such x
is a vector space called the orthogonal complement of the row space of C. We wish to
ﬁnd this orthogonal complement.

594

Appendix 5 Least-squares Minimization

First, if C has fewer rows than columns, then extend it to a square matrix by adding
rows of zero elements. This has no effect on the set of constraints Cx = 0. Now, let
C = UDVT be the Singular Value Decomposition of C, where D is a diagonal matrix
with r non-zero diagonal entries. In this case, C has rank r and the row-space of C
is generated by the ﬁrst r rows of VT. The orthogonal complement of the row-space
of C consists of the remaining rows of VT. Deﬁne C⊥
to be the matrix V with the ﬁrst
r columns deleted. Then CC⊥
= 0, and so the set of vectors x satisfying Cx = 0 is
spanned by the columns of C⊥
and we may write any such x as x = C⊥x(cid:2)
for suitable
has orthogonal columns, one observes that (cid:10)x(cid:10) = (cid:10)C⊥x(cid:2)(cid:10) = (cid:10)x(cid:2)(cid:10). The
. Since C⊥
x(cid:2)
minimization problem now becomes
that minimizes (cid:10)AC⊥x(cid:2)(cid:10) subject to (cid:10)x(cid:2)(cid:10) = 1 .
• Find the x(cid:2)
This is simply an instance of the problem discussed in section A5.3, solved by
algorithm A5.4. The complete algorithm for solution of the constrained minimization
problem is given as algorithm A5.5.

Objective
Given an m × n matrix A with m ≥ n, ﬁnd the vector x that minimizes (cid:10)Ax(cid:10) subject to
(cid:10)x(cid:10) = 1 and Cx = 0.
Algorithm

(i) If C has fewer rows than columns, then add zero-ﬁlled rows to C to make it square.
Compute the SVD C = UDVT where diagonal entries of D are sorted with non-zero ones
ﬁrst. Let C⊥
be the matrix obtained from V by deleting the ﬁrst r columns of V, where
r is the number of non-zero entries in D (the rank of C).
(ii) Find the solution to the minimization problem AC⊥x(cid:1) = 0 using algorithm A5.4. The
solution is given by x = C⊥x(cid:1)

.

Algorithm A5.5. Algorithm for constrained minimization.

A5.4.1 More constrained minimization
A further constrained minimization problem arises in the algebraic estimation method
used frequently throughout this book – for instance, for computation of the fundamental
matrix (section 11.3(p282)) or the trifocal tensor (section 16.3(p395)).
The problem is:
• Minimize (cid:10)Ax(cid:10) subject to (cid:10)x(cid:10) = 1 and x = Gˆx for a given matrix G and some
unknown vector ˆx.
Note that this is very similar to the previous minimization problem of section A5.4,
which was reduced to the form of the present problem in which the matrix G had or-
thonormal columns. The condition that x = Gˆx for some ˆx means nothing more than
that x lies in the span of the columns of G. Thus, to solve the present problem using
algorithm A5.5, we need only to replace G by a matrix with the same column space (i.e.
the space spanned by the columns), but with orthonormal columns. If G = UDVT where
D has r non-zero entries (that is G has rank r), then let U(cid:2)
be the matrix consisting of the

A5.4 Least-squares solution to constrained systems

595

.

If ˆx is also required, then it may be obtained by solving Gˆx = x = U(cid:2)x(cid:2)

have the same column space. As in section A5.4 the
to be the unit vector that minimizes (cid:10)AU(cid:2)x(cid:2)(cid:10), then setting

ﬁrst r columns of U. Then G and U(cid:2)
solution is found by setting x(cid:2)
x = U(cid:2)x(cid:2)
. The solution
is expressed in terms of the pseudo-inverse (section A5.2) as ˆx = G+x = G+U(cid:2)x(cid:2)
; it
may not be unique if G does not have full column rank. Since G+ = VD+UT, we may
write ˆx = VD+UTU(cid:2)x(cid:2)
consists of the ﬁrst r
and D(cid:2)
columns of V(cid:2)

, which simpliﬁes to ˆx = V(cid:2)D(cid:2)−1x(cid:2)
is the upper r × r block of D.

where V(cid:2)

The complete method is summarized in algorithm A5.6.

Objective
Find the vector x that minimizes (cid:10)Ax(cid:10) subject to the conditions (cid:10)x(cid:10) = 1 and x = Gˆx, where
G has rank r.

Algorithm

diagonal.

(i) Compute the SVD G = UDVT, where the non-zero values of D appear ﬁrst down the
(ii) Let U(cid:1)
(iii) Find the unit vector x(cid:1)
(iv) The required solution is x = U(cid:1)x(cid:1)
(v) If desired, one may compute ˆx as V(cid:1)D(cid:1)−1x(cid:1)

that minimizes (cid:10)AU(cid:1)x(cid:1)(cid:10), using algorithm A5.4.

be the matrix comprising the ﬁrst r columns of U.

consists of the ﬁrst r columns of

, where V(cid:1)

V and D(cid:1)

is the upper r × r block of D.

.

Algorithm A5.6. Algorithm for constrained minimization, subject to a span-space constraint.

A5.4.2 Yet another minimization problem
A very similar problem is
• Minimize (cid:10)Ax(cid:10) subject to a condition (cid:10)Cx(cid:10) = 1.
This problem comes up for instance in the solution to the DLT camera calibration
problem (section 7.3(p184)). In general it will be the case that rankC < n where n is
the dimension of the vector x. Geometrically the problem may be thought of as ﬁnding
the “lowest” point on a quadratic surface (speciﬁed by xTATAx), with the constraint
that the point must lie on the (inhomogeneous) “conic” xTCTCx = 1.
We start by taking the SVD of the matrix C, obtaining C = UDVT. The condition
(cid:10)UDVTx(cid:10) = 1 is equivalent to (cid:10)DVTx(cid:10) = 1, and it is not necessary to compute U
= VTx, the problem becomes: minimize (cid:10)AVx(cid:2)(cid:10) subject to
explicitly. Then writing x(cid:2)
the condition (cid:10)Dx(cid:2)(cid:10) = 1 Writing A(cid:2)
= AV, this becomes: minimize (cid:10)A(cid:2)x(cid:2)(cid:10) subject to
(cid:10)Dx(cid:2)(cid:10) = 1. Thus we have reduced to the case where the constraint matrix is a diagonal
matrix, D.

We suppose that D has r non-zero diagonal entries, and s zero entries, where r + s =
(cid:2)
i of x(cid:2)
n, the non-zero entries appearing ﬁrst on the diagonal of D. Then the entries x
(cid:10), since the corresponding diagonal entries of
for i > r do not affect the value of (cid:10)Dx(cid:2)
(cid:2)
(cid:2)
D are zero. Then, for a speciﬁc choice of the x
i for i = 1, . . . , r, the other entries x
i,

i

596

Appendix 5 Least-squares Minimization

1

i

| A(cid:2)

and A(cid:2)

= [A(cid:2)
, and let x(cid:2)

1 consists of the ﬁrst r columns of A(cid:2)

2] where A(cid:2)
2 consist of the remaining s elements of x(cid:2)

i = r + 1, . . . , n should be chosen so as to minimize the value of (cid:10)A(cid:2)x(cid:2)
A(cid:2)
remaining s columns. Similarly, let x(cid:2)
of x(cid:2)
diagonal matrix consisting of the ﬁrst r diagonal entries of D. Then A(cid:2)x(cid:2)
and the minimization problem is to minimize
x(cid:2)
1 + A(cid:2)

(cid:10). We write
2 consists of the
1 be an r-vector consisting of the ﬁrst r elements
. Further, let D1 be the r × r
x(cid:2)
2,

(cid:10) = 1. Now, temporarily ﬁxing x(cid:2)

subject to the condition (cid:10)D1x(cid:2)
1, (A5.4) takes the
form of a least-squares minimization problem of the type discussed in section A5.1.
According to result A5.1, the value of x(cid:2)
x(cid:2)
1.
Substituting this in (A5.4) gives (cid:10)(A(cid:2)
(cid:10), which we are required to minimize,
subject to the condition (cid:10)D1x(cid:2)
1, the problem reduces
at last to a problem of the form of the familiar minimization problem of algorithm A5.4.
• Minimize (cid:10)(A(cid:2)
+ − I)A(cid:2)
1
We now summarize the algorithm.

2 that minimizes (A5.4) is x(cid:2)

(cid:10) = 1. Finally, writing x(cid:2)(cid:2)

(cid:10), subject to (cid:10)x(cid:2)(cid:2)(cid:10) = 1.

2 = −A(cid:2)

+−I)A(cid:2)

x(cid:2)
1 +A(cid:2)

= D1x(cid:2)

= A(cid:2)

(A5.4)

D−1

(cid:10)A(cid:2)

+A(cid:2)

x(cid:2)(cid:2)

x(cid:2)

x(cid:2)

A(cid:2)

A(cid:2)

(cid:10)

1

2

2

2

2

2

1

1

2

2

1

1

2

1

1

1

1

Objective
Minimize (cid:10)Ax(cid:10) subject to (cid:10)Cx(cid:10) = 1.
Algorithm

A(cid:1)

1 | A(cid:1)

, and A(cid:1)

2] where A(cid:1)

2 is formed from the remaining columns.

(i) Compute the SVD C = UDVT, and write A(cid:1) = AV.
(ii) Suppose rankD = r and let A(cid:1) = [A(cid:1)
(iii) Let D1 be the upper r × r minor of D.
(cid:29)
1 . This is an n × r matrix.
+ − I)A(cid:1)
1D−1
(iv) Compute A(cid:1)(cid:1) = (A(cid:1)
(v) Minimize (cid:10)A(cid:1)(cid:1)x(cid:1)(cid:10) subject to (cid:10)x(cid:1)(cid:1)(cid:10) = 1 using algorithm A5.4.
2 = −A(cid:1)
(vi) Set x(cid:1)
.
(vii) The solution is given by x = Vx(cid:1)

1. Let x(cid:1) =

1 = D−1

, and x(cid:1)

1 x(cid:1)(cid:1)

(cid:28)

1x(cid:1)

x(cid:1)
x(cid:1)

+A(cid:1)

2A(cid:1)

2

2

1

2

.

1 consists of the ﬁrst r columns of

Algorithm A5.7. Least-squares solution of homogeneous equations subject to the constraint (cid:10)Cx(cid:10) = 1.

Appendix 6

Iterative Estimation Methods

In this appendix we describe the various components involved in building an efﬁcient
and robust iterative estimation algorithm.

We start with two of the most common iterative parameter minimization meth-
ods, namely Newton iteration (and the closely related Gauss-Newton method) and
Levenberg–Marquardt iteration. The general idea of Newton iteration is familiar to
most students of numerical methods as a way of ﬁnding the zeros of a function of
a single variable.
Its generalization to several variables and application to ﬁnding
least-squares solutions rather than exact solutions to sets of equations is relatively
straightforward. The Levenberg–Marquardt method is a simple variation on Newton
iteration designed to provide faster convergence and regularization in the case of over-
parametrized problems. It may be seen as a hybrid between Newton iteration and a
gradient descent method.

For the type of problem considered in this book, important reductions of computa-
tional complexity are obtained by dividing the set of parameters into two parts. The two
parts generally consist of a set of parameters representing camera matrices or homo-
graphies, and a set of parameters representing points. This leads to a sparse structure
to the problem that is described starting at section A6.3.

We discuss two further implementation issues – the choice of cost function, with re-
spect to their robustness to outliers and convexity (section A6.8); and the parametriza-
tion of rotations, and homogeneous and constrained vectors (section A6.9). Finally,
those readers who want to learn more about iterative techniques and bundle-adjustment
are referred to [Triggs-00a] for more details.

A6.1 Newton iteration

Suppose we are given a hypothesized functional relation X = f (P) where X is a mea-
surement vector and P is a parameter vector in Euclidean spaces IRN and IRM respec-
tively. A measured value of X approximating the true value X is provided, and we wish

to ﬁnd the vector (cid:23)P that most nearly satisﬁes this functional relation. More precisely,
we seek the vector (cid:23)P satisfying X = f ((cid:23)P) −  for which (cid:10)(cid:10) is minimized. Note that

the linear least-squares problem considered in section A5.1 is exactly of this type, the
function f being deﬁned as a linear function f(P) = AP.

597

598

Appendix 6 Iterative Estimation Methods

To solve the case where f is not a linear function, we may start with an initial es-
timated value P0, and proceed to reﬁne the estimate under the assumption that the
function f is locally linear. Let 0 be deﬁned by 0 = f (P0) − X. We assume that the
function f is approximated at P0 by f (P0 + ∆) = f(P0) + J∆, where J is the linear
mapping represented by the Jacobian matrix J = ∂f /∂P. We seek a point f (P1), with
P1 = P0 + ∆, which minimizes f (P1) − X = f(P0) +J ∆ − X = 0 + J∆. Thus, it is
required to minimize (cid:10)0 + J∆(cid:10) over ∆, which is a linear minimization problem. The
vector ∆ is obtained by solving the normal equations (see (A5.2))
or by using the pseudo-inverse ∆ = −J+0. Thus, the solution vector (cid:23)P is obtained

JTJ∆ = −JT0

(A6.1)

by starting with an estimate P0 and computing successive approximations according to
the formula

where ∆i is the solution to the linear least-squares problem

Pi+1 = Pi + ∆i

this algorithm will converge to the required least-squares solution (cid:23)P. Unfortunately, it
Matrix J is the Jacobian ∂f /∂P evaluated at Pi and i = f(Pi) − X. One hopes that

J∆i = −i.

is possible that this iteration procedure converges to a local minimum value, or does
not converge at all. The behaviour of the iteration algorithm depends very strongly on
the initial estimate P0.

Weighted iteration. As an alternative to all the dependent variables being equally
weighted it is possible to provide a weight matrix specifying the weights of the depen-
dent variables X. To be more precise, one may assume that the measurement X satisﬁes
a Gaussian distribution with covariance matrix ΣX, and one wishes to minimize the Ma-

halanobis distance (cid:10)f ((cid:23)P) − X(cid:10)Σ. This covariance matrix may be diagonal, specifying
that the individual coordinates of X are independent, or more generally it may be an
arbitrary symmetric and positive deﬁnite matrix. In this case, the normal equations
become JTΣ−1J∆i = −JTΣ−1i. The rest of the algorithm remains unchanged.

Newton’s method and the Hessian. We pass now to consideration of ﬁnding minima
of functions of many variables. For the present, we consider an arbitrary scalar-valued
function g(P) where P is a vector. The optimization problem is simply to minimize
g(P) over all values of P. We make two assumptions: that g(P) has a well-deﬁned
minimum value, and that we know a point P0 reasonably close to this minimum.

We may expand g(P) about P0 in a Taylor series to get

g(P0 + ∆) = g + gP∆ + ∆TgPP∆/2 + . . .

where subscript P denotes differentiation, and the right hand side is evaluated at P0.

599
We wish to minimize this quantity with respect to ∆. To this end, we differentiate with
respect to ∆ and set the derivative to zero, arriving at the equation gP + gPP∆ = 0 or

A6.1 Newton iteration

gPP∆ = −gP.

(A6.2)

In this equation, gPP is the matrix of second derivatives, the Hessian of g, the (i, j)-th
entry of which is ∂2g/∂pi∂pj, and pi and pj are the i-th and j-th parameters. Vector
gP is the gradient of g. The method of Newton iteration consists in starting at an initial
value of the parameters, P0, and iteratively computing parameter increments ∆ using
(A6.2) until convergence occurs.

Now we turn to the sort of cost function that arises in the least-squares minimization
problem considered above. Speciﬁcally, g(P) is the squared norm of an error function

g(P) =

1
2

(cid:10)(P)(cid:10)2 = (P)T(P)/2

where (P) is a vector-valued function of the parameter vector P. In particular, (P) =
f(P) − X. The factor 1/2 is present for simplifying the succeeding computations.
The gradient vector gP is easily computed to be T

P. However, we may write P =
f P = J using the notation introduced previously. In short gP = JT. Differentiating
gP = T

P a second time, we compute the following formula for the Hessian.1

gPP = T

PP + T

PP.

(A6.3)

Now, under the assumption that f (P) is linear, the second term on the right vanishes,
PP = JTJ. Now, substituting for the gradient and Hessian in (A6.2)
leaving gPP = T
yields JTJ∆ = −JT which is nothing more than the normal equations (A6.1). Thus
we have arrived at the same iterative procedure as previously solving the parameter
estimation problem, under the assumption that JTJ = T
PP is a reasonable approxima-
tion for the Hessian of the function g(P). This procedure in which JTJ is used as an
approximation for the Hessian is known as the Gauss-Newton method.
Gradient descent. The negative (or down-hill) gradient vector −gP = −T
P deﬁnes
the direction of most rapid decrease of the cost function. A strategy for minimization
of g is to move iteratively in the gradient direction. This is known as gradient descent.
The length of the step may be computed by carrying out a line search for the function
minimum in the negative gradient direction. In this case, the parameter increment ∆ is
computed from the equation λ∆ = −gP, where λ controls the length of the step.
We may consider this as related to Newton iteration as expressed by the update equa-
tion (A6.2) in which the Hessian is approximated (somewhat arbitrarily) by the scalar
matrix λI. Gradient descent by itself is not a very good minimization strategy, typ-
ically characterized by slow convergence due to zig-zagging.
(See [Press-88] for a
closer analysis.) It will be seen in the next section, however, that it can be useful in
conjunction with Gauss-Newton iteration as a way of getting out of tight corners. The

1 The last term in this formula needs some clariﬁcation. Since (cid:1) is a vector, (cid:1)PP is a 3-dimensional array (a tensor). The sum
i(i)PPi where i is the

T
PP (cid:1) is over the components of (cid:1). It may be written more precisely as

implied by the product (cid:1)
i-th components of the vector (cid:1), and (i)PP is its Hessian.

(cid:27)

600

Appendix 6 Iterative Estimation Methods

Levenberg–Marquardt method is essentially a Gauss-Newton method that transitions
smoothly to gradient descent when the Gauss-Newton updates fail.
To summarize, we have so far considered three methods of minimization of a cost
function g(P) = (cid:10)(P)(cid:10)2/2:

(i) Newton. Update equation:

gPP∆ = −gP

PP + T

PP and gP = T

where gPP = T
P. Newton iteration is based on the
assumption of an approximately quadratic cost function near the minimum, and
will show rapid convergence if this condition is met. The disadvantage of this
approach is that the computation of the Hessian may be difﬁcult. In addition, far
from the minimum the assumption of quadratic behaviour is probably invalid,
so a lot of extra work is done with little beneﬁt.

(ii) Gauss-Newton. Update equation:
PP∆ = −T
T
P

This is equivalent to Newton iteration in which the Hessian is approximated by
T
PP. Generally this is a good approximation, particularly close to a minimum,
or when  is nearly linear in P.

(iii) Gradient descent. Update equation:

λ∆ = −T

P = −gP.

The Hessian in Newton iteration is replaced by a multiple of the identity ma-
trix. Each update is in the direction of most rapid local decrease of the function
value. The value of λ may be chosen adaptively, or by a line search in the
downward gradient direction. Generally, gradient descent by itself is not rec-
ommended, but in conjunction with Gauss-Newton it yields the commonly used
Levenberg–Marquardt method.

A6.2 Levenberg–Marquardt iteration

The Levenberg–Marquardt (abbreviated LM) iteration method is a slight variation on
the Gauss-Newton iteration method. The normal equations JTJ∆ = −JT are replaced
by the augmented normal equations (JTJ + λI)∆ = −JT, for some value of λ that
varies from iteration to iteration. Here I is the identity matrix. A typical initial value
of λ is 10

−3 times the average of the diagonal elements of N = JTJ.

If the value of ∆ obtained by solving the augmented normal equations leads to a
reduction in the error, then the increment is accepted and λ is divided by a factor (typi-
cally 10) before the next iteration. On the other hand if the value ∆ leads to an increased
error, then λ is multiplied by the same factor and the augmented normal equations are
solved again, this process continuing until a value of ∆ is found that gives rise to a
decreased error. This process of repeatedly solving the augmented normal equations
for different values of λ until an acceptable ∆ is found constitutes one iteration of the
LM algorithm. An implementation of the LM algorithm is given in [Press-88].

A6.2 Levenberg–Marquardt iteration

601

Justiﬁcation of LM. To understand the reasoning behind this method, consider what
happens for different values of λ. When λ is very small, the method is essentially the
same as Gauss-Newton iteration. If the error function (cid:10)(cid:10)2 = (cid:10)f (P) − X(cid:10)2 is close to
being quadratic in P, then this method will converge fast to the minimum value. On the
other hand when λ is large then the normal equation matrix is approximated by λI, and
the normal equations become λ∆ = −JT. Recalling that JT is simply the gradient
vector of (cid:10)(cid:10)2, we see that the direction of the parameter increment ∆ approaches
that given by gradient descent. Thus, the LM algorithm moves seamlessly between
Gauss-Newton iteration, which will cause rapid convergence in the neighbourhood of
the solution, and a gradient descent approach, which will guarantee a decrease in the
cost function when the going is difﬁcult. Indeed, as λ becomes larger and larger, the
length of the increment step ∆ decreases and eventually it will lead to a decrease of the
cost function (cid:10)(cid:10)2.
To demonstrate that the parameter increment ∆ obtained by solving the augmented
normal equations for all values of λ is in the direction of decreasing cost, we will
show that the inner product of ∆ and the negative gradient direction for the function
g(P) = (cid:10)(P)(cid:10)2 is positive. This results from the following computation.

−gP · ∆ = −gT
P∆

= (JT)T(JTJ + λI)

−1JT

However, JTJ+λI is positive-deﬁnite for any value of λ, and so therefore is its inverse.
−1JT is positive, unless JT is zero.
By deﬁnition, this means that (JT)T(JTJ + λI)
Thus, the increment ∆ is in a direction of locally decreasing cost, unless of course the
gradient JT is zero.

A different augmentation.
In some implementations of Levenberg-Marquardt, no-
tably that given in [Press-88], a different method of augmenting the normal equations
is used. The augmented normal equation matrix N(cid:2)
is deﬁned in terms of the matrix
ij = Nij for i (cid:5)= j. Thus the diagonal of N is
N = JTJ by N(cid:2)
augmented by a multiplicative factor (1 + λ) instead of an additive factor. As before,
a small value of λ, results essentially in a Gauss-Newton update. For large λ, the off-
diagonal entries of the normal equation matrix become insigniﬁcant with respect to the
diagonal ones.

ii = (1 + λ)Nii and N(cid:2)

The i-th diagonal entry of N(cid:2)

is simply (1 + λ)JT
i

Ji where Ji = ∂f /∂pi, and pi
is the i-th parameter. The update equation is then (1 + λ)JT
 where δi is
i
the increment in the i-th parameter. Apart from the factor (1 + λ), this is the normal
equation that would result from minimizing the cost by varying only the i-th parameter
δi. Thus, in the limit as λ becomes large, the increment to the parameters is the direction
that would result from minimizing each one separately.

Jiδi = JT
i

With this sort of augmentation, the parameter increments for large λ are not the
same as for gradient descent. Nevertheless, the same analysis as before shows that the
resulting increment is still in the down-hill direction for any value of λ.
One small problem may arise: if some parameter pi has no effect on the value of
the function f, then Ji = ∂f /∂pi is zero, and the i-th diagonal entry of N and hence N(cid:2)

602
is zero. The augmented normal equation matrix N(cid:2)
trouble. In practice, this is a rare occurrence, but it can occur.

Appendix 6 Iterative Estimation Methods

is then singular, which can cause

Implementation of LM. To carry out Levenberg–Marquardt minimization in its sim-
plest form it is necessary only to provide a routine to compute the function being min-

imized, a goal vector (cid:23)X of observed or desired values of the function, and an initial

estimate P0. Computation of the Jacobian matrix J may be carried out either numeri-
cally, or by providing a custom routine.

Numerical differentiation may be carried out as follows. Each independent variable
xi is incremented in turn to xi + δ, the resulting function value is computed using the
routine provided for computing f and the derivative is computed as a ratio. Good re-
sults have been found by setting the value δ to the maximum of |10
−6.
This choice seemingly gives a good approximation to the derivative. In practice, there
seems to be little disadvantage in using numerical differentiation, though for simple
functions f one may prefer to provide a routine to compute J, partly for aesthetic rea-
sons, partly because of a possible slightly improved convergence and partly for speed.

−4 × xi| and 10

A6.3 A sparse Levenberg–Marquardt algorithm

The LM algorithm as described above in section A6.2 is quite suitable for minimization
with respect to a small number of parameters. Thus, in the case of 2D homography
estimation (see chapter 4), for the simple cost functions (4.6–p94) and (4.7–p95) which
are minimized with respect only to the entries of the homography matrix H the LM
algorithm works well. However, for minimizing cost functions with respect to large
numbers of parameters, the bare LM algorithm is not very suitable. This is because
the central step of the LM algorithm, the solution of the normal equations (A5.2), has
complexity N 3 in the number of parameters, and this step is repeated many times.
However, in solving many estimation problems of the type considered in this book, the
normal equation matrix has a certain sparse block structure that one may take advantage
of to realize very great time savings.

An example of the situation in which this method is useful is the problem of es-
timating a 2D homography between two views assuming errors in both images, by
minimizing the cost function (4.8–p95). This problem may be parametrized in terms
of a set of parameters characterizing the 2D homography (perhaps the 9 entries of the
homography matrix), plus parameters for each of the n points in the ﬁrst view, a total
of 2n + 9 parameters.

Another instance where these methods are useful is the reconstruction problem in
which one has image correspondences across two or more (let us say m) views and one
wishes to estimate the camera parameters of all the cameras and also the 3D positions
of all the points. One can assume arbitrary projective cameras or fully or partially
calibrated cameras. Furthermore, to remove some of the incidental degrees of freedom
one can ﬁx one of the cameras. For instance in the projective reconstruction problem,
the problem may be parametrized by the entries of all the camera matrices (a total of
12m or 11m parameters depending on how the cameras are parametrized), plus a set
of 3n parameters for the coordinates of the 3D points.

A6.3 A sparse Levenberg–Marquardt algorithm

603

The sparse LM algorithm is often perceived as complex and difﬁcult to implement.
To help overcome this, the algorithms are given cook-book style. Given a suitable
library of standard matrix manipulation routines, a reader should be able to implement
the algorithm without difﬁculty.
• Notation:
one after the other in a single column vector is denoted by (aT
similar notation is used for matrices.

If a1, a2, . . . ,a n are vectors, then the vector obtained by placing them
n)T. A

2 , . . . ,a T

1 , aT

A6.3.1 Partitioning the parameters in the LM method
The sparse LM method will be described primarily in terms of the reconstruction prob-
lem, since this is the archetypal problem to which this method relates. The estimation
problem will be treated in general terms ﬁrst of all, since this will illuminate the gen-
eral approach without excessive detail. At this level of abstraction, the beneﬁts of this
approach may not be apparent, but they will become clearer in section A6.3.3. We start
with the simple observation that the set of parameters in this problem may be divided
up into two sets: a set of parameters describing the cameras, and a set of parameters de-
scribing the points. More formally, the “parameter vector” P ∈ IRM may be partitioned
into parameter vectors a and b so that P = (aT, bT)T. We are given a “measurement
vector”, denoted by X in a space IRN . In the reconstruction problem, this consists of
the vector of all image point coordinates. In addition let ΣX be the covariance matrix
the parameter vector P to the estimated measurement vector (cid:23)X = f (P). Denoting by 
for the measurement vector.1 We consider a general function f : IRM → IRN taking
the difference X − (cid:23)X between the measured and estimated quantities, one seeks the set
of parameters that minimize the squared Mahalanobis distance (cid:10)(cid:10)2
J = [∂(cid:23)X/∂P] has a block structure of the form J = [A|B], where Jacobian submatrices
Corresponding to the division of parameters P = (aT, bT)T, the Jacobian matrix

ΣX = TΣ−1
X .

are deﬁned by

and

A =

B =

The set of equations Jδ =  solved as the central step in the LM algorithm (see

section A6.2) now has the form

.

"
#
∂(cid:23)X/∂a
#
"
∂(cid:23)X/∂b
 = .
 δa
 ATΣ−1
 =
 δa



δb

 .

X 
BTΣ−1
X 

Jδ = [A|B]

δb
X Jδ = JTΣ−1

Then, the normal equations JTΣ−1

algorithm are of the form ATΣ−1

X A ATΣ−1
X B
X A BTΣ−1
X B

BTΣ−1

1 In the absence of other knowledge, one usually assumes that ΣX is the identity matrix.

X  to be solved at each step of the LM

(A6.4)

(A6.5)

604

Appendix 6 Iterative Estimation Methods

At this point in the LM algorithm the diagonal blocks of this matrix are augmented
by multiplying their diagonal entries by a factor 1 + λ, for the varying parameter λ.
This augmentation alters the matrices ATΣ−1
X A and BTΣ−1
X B. The resulting matrices
∗
will be denoted by (ATΣ−1
X B)
. The reader may wish to look ahead to
ﬁgure A6.1 and ﬁgure A6.2 which shows graphically the form of the Jacobian and
normal equations in an estimation problem involving several cameras and points.

X A)

∗

The set of equations (A6.5) may now be written in block form as

and (BTΣ−1
(cid:18)(cid:20)

U∗
W
WT V∗

δa
δb

(cid:17)

(cid:21)

(cid:20)

(cid:21)

A
B

=

.

(A6.6)

As a ﬁrst step to solving these equations, both sides are now multiplied on the left by

(cid:18)

(cid:17)

I −WV∗−1
0

I

(cid:17)

resulting in
U∗ − WV∗−1WT

WT

(cid:18)(cid:20)

(cid:21)

(cid:20)

=

δa
δb

0
V∗

A − WV∗−1B

B

(cid:21)

.

(A6.7)

This results in the elimination of the top right hand block. The top half of this set of
equations is

(U∗ − WV∗−1WT)δa = A − WV∗−1B.

These equations may be solved to ﬁnd δa. Subsequently, the value of δb may be found
by back-substitution, giving

(A6.8)

(A6.9)

V∗

δb = B − WTδa.

As in section A6.2, if the newly computed value of the parameter vector P = ((a +
δa)T, (b + δb)T)T results in a diminished value of the error function, then one accepts
the new parameter vector P, diminishes the value of λ by a factor of 10, and proceeds
to the next iteration. On the other hand if the error value is increased, then one rejects
the new P and tries again with a new value of λ, increased by a factor of 10.

The

complete

partitioned Levenberg–Marquardt

algorithm is

given

in

algorithm A6.1.

Although in this method we solve ﬁrst for δa and subsequently for δb based on the
new value of a, it should not be thought that the method amounts to no more than
independent iterations over a and b. If one were to solve for a holding b constant,
then the normal equations used to solve for δa would take the simpler form Uδa = A,
compared with (A6.8). The method of alternating between solving for δa and δb is not
recommended, however, because of potential slow convergence.

A6.3.2 Covariance
It was seen in result 5.12(p144) that the covariance matrix of the estimated parameters
is given by

ΣP = (JTΣ−1

X J)+.

(A6.10)

A6.3 A sparse Levenberg–Marquardt algorithm

605

of parameters P = (aT, bT)T and a function f : P (cid:6)→ (cid:23)X taking the parameter vector to an

Given A vector of measurements X with covariance matrix ΣX, an initial estimate of a set

estimate of the measurement vector.
Objective Find the set of parameters P that minimizes TΣ−1
Algorithm

(ii) Compute the derivative matrices A = [∂(cid:23)X/∂a] and B = [∂(cid:23)X/∂b] and the error vector

(i) Initialize a constant λ = 0.001 (typical value).

X  where  = X − (cid:23)X.

.

(iii) Compute intermediate expressions

U = ATΣ−1
X A
A = ATΣ−1
X 

V = BTΣ−1
X B
B = BTΣ−1
X 

W = ATΣ−1
X B

of V∗

(iv) Augment U and V by multiplying their diagonal elements by 1 + λ.
(v) Compute the inverse V∗−1, and deﬁne Y = WV∗−1. The inverse may overwrite the value
(vi) Find δa by solving (U∗ − YWT)δa = A − YB.
(vii) Find δb by back-substitution: δb = V∗−1(B − WTδa).
(viii) Update the parameter vector by adding the incremental vector (δT

which will not be needed again.

b)T and compute

a , δT

the new error vector.

(ix) If the new error is less than the old error, then accept the new values of the parameters,
diminish the value of λ by a factor of 10, and start again at step (ii), or else terminate.
(x) If the new error is greater than the old error, then revert to the old parameter values,

increase the value of λ by a factor of 10, and try again from step (iv).

Algorithm A6.1. A partitioned Levenberg–Marquardt algorithm.

In the over-parametrized case, the covariance matrix ΣP given by (A6.10) is singular,
and in particular, no variation in the parameters is allowed in directions perpendicular
to the constraint surface – the variance is zero in these directions.

In the case where the parameter set is partitioned as P = (aT, bT)T, matrix (JTΣ−1

X J)
has the block form given in (A6.5) and (A6.6) (but without augmentation, represented
by stars). Thus we may write

(cid:18)

(cid:17)

(cid:18)

(cid:17)

JTΣ−1

X J =

ATΣ−1
BTΣ−1

X A ATΣ−1
X B
X A BTΣ−1
X B

=

U
W
WT V

.

(A6.11)

The covariance matrix ΣP is the pseudo-inverse of this matrix. Under the assumption
that V is invertible, redeﬁne Y = WV−1. Then the matrix may be diagonalized according
to

(cid:18)

(cid:17)

(cid:18)(cid:17)

(cid:18)(cid:17)

(cid:18)

(cid:17)

JTΣ−1

X J =

W
U
WT V

=

I Y
0 I

U − WV−1WT 0
V

0

I
0
YT I

(A6.12)

For matrices G and H with G invertible, we assume an identity

(GHGT)+ = G−TH+G−1

This identity is valid under conditions explored in the exercise at the end of this ap-

606

Appendix 6 Iterative Estimation Methods

Objective Compute the covariance of the parameters a and b estimated using algorithm A6.1.
Algorithm

(i) Compute matrices U, V and W as in algorithm A6.1, and also Y = WV−1.
(ii) Σa = (U − WV−1WT)+.
(iii) Σb = YTΣaY + V−1
(iv) The cross-covariance Σab = −ΣaY.

Algorithm A6.2. Computation of the covariance matrix of the LM parameters.

pendix. Applying this to (A6.12) and multiplying out provides a formula for the pseudo
inverse

(cid:17)

(cid:18)

− XY

YTXY + V−1

ΣP = (JTΣ−1

X J)+ =

X

−YTX

(A6.13)

where X = (U − WV−1WT)+.
The condition for this to be true is that span(A) ∩ span(B) = ∅, where span(·) rep-
resents the span of the columns of the matrix. Here A and B are as in (A6.11). Proof
of this fact and interpretation of the condition span(A) ∩ span(B) = ∅ is outlined in
exercise (i) on page 626.
The division of matrix JTΣ−1

X J into blocks as in (A6.11) corresponds in (A6.5) to
the division of P into parameters a and b. Truncation of the covariance matrix for
parameters P gives covariance matrices for parameters a and b separately. The result
is summarized in algorithm A6.2.

1 , XT

2 , . . . , XT

A6.3.3 General sparse LM method
In the previous pages, a method has been described for carrying out LM iteration and
computing covariances of the solution in the case where the parameter vector may be
partitioned into two sub-vectors a and b. It is not clear from the foregoing discussion
that the methods described there actually give any computational advantage in the gen-
eral case. However, such methods become important when the Jacobian matrix obeys
a certain sparseness condition, as will be described now.
We suppose that the “measurement vector” X ∈ IRN may broken up into pieces as
n)T. Similarly, suppose that the “parameter vector” P ∈ IRM may
X = (XT
to a given assignment of parameters will be denoted by (cid:23)Xi. We make the sparseness
be divided up as P = (aT, bT
n)T. The estimated values of Xi corresponding
assumption that each (cid:23)Xi is dependent on a and bi only, but not on the other parameters
bj. In this case, ∂(cid:23)Xi/∂bj = 0 for i (cid:5)= j. No assumption is made about ∂(cid:23)Xi/∂a. This
in which bi is the vector of parameters of the i-th point, and (cid:23)Xi is the vector of mea-
a point does not depend on any other point, one sees that ∂(cid:23)Xi/∂bj = 0, as required,
Corresponding to this division, the Jacobian matrix J = [∂(cid:23)X/∂P] has a sparse block

surements of the image of this point in all the views. In this case, since the image of

situation arises in the reconstruction problem described at the start of this discussion,

1 , bT

2 , . . . ,b T

unless i = j.

structure. We deﬁne Jacobian matrices by

A6.4 Application of sparse LM to 2D homography estimation

607

"

#

∂(cid:23)Xi/∂a

Ai =

"

Bi =

#

∂(cid:23)Xi/∂bi
n)T = X − (cid:23)X the set of equations

.

Given an error vector of the form  = (T

1 , . . . , T

Jδ =  now has the form



Jδ =

A1 B1
A2
...
An

B2

...

Bn





δa

δb1
...
δbn

 =

 .

 1

...
n

(A6.14)

We suppose further that all of the measurements Xi are independent with covariance
matrices ΣXi. Thus the covariance matrix for the complete measurement vector ΣX has
the block-diagonal form ΣX = diag(ΣX1, . . . ,Σ Xn).

In the notation of algorithm A6.1, we have
1 , AT

n]T

2 , . . . ,A T

A = [AT
B = diag(B1, B2, . . . ,B n)
ΣX = diag(ΣX1, . . . ,Σ Xn)
δb = (δT
bn)T
 = (T

b2, . . . ,δ T
n)T
2 , . . . , T

b1, δT
1 , T

Now, it is a straightforward task to substitute these formulae into algorithm A6.1.
The result of this substitution is given in algorithm A6.3, representing the computation
of one step of the LM algorithm. The important observation is that in this form each
step of the algorithm requires computation time linear in n. Without the advantage
resulting from the sparse structure, the algorithm (for instance a blind adherence to
algorithm A6.1) would have complexity of order n3.

A6.4 Application of sparse LM to 2D homography estimation

i , x(cid:2)

i

We apply the foregoing discussion to the estimation of a 2D homography H given a
set of corresponding image points xi ↔ x(cid:2)
i in two images. The points in each image
are subject to noise, and one seeks to minimize the cost function (4.8–p95). We deﬁne
a measurement vector Xi = (xT
T)T. In this case, the parameter vector P may be
divided up as P = (hT, ˆxT
n)T, where the values ˆxi are the estimated values
of the image points in the ﬁrst image, and h is a vector of entries of the homography
H. Thus, one must simultaneously estimate the homography H and the parameters of
n)T, where each

each point in the ﬁrst image. The function f maps P to ((cid:23)XT
1 ,(cid:23)XT
2 , . . . ,(cid:23)XT
(cid:23)Xi = (ˆxT
(cid:18)

The Jacobian matrices have a special form in this case, since one observes that

T)T. Then algorithm A6.3 applies directly.

i )T = (ˆxT

2 , . . . , ˆxT

i , HˆxT

i , ˆx(cid:2)

1 , ˆxT

(cid:17)

i

Ai = ∂(cid:23)Xi/∂h =

0
∂ ˆx(cid:2)
i/∂h

608

Appendix 6 Iterative Estimation Methods

1 , . . . , bT

n)T, and the measurement vector as X = (XT

Objective Formulate LM algorithm in the case where the parameter vector is partitioned
as P = (aT, bT
n)T, such that

∂(cid:23)Xi/∂bj = 0 for i (cid:5)= j.
(i) Compute the derivative matrices Ai = [∂(cid:23)Xi/∂a] and Bi = [∂(cid:23)Xi/∂bi] and the error
vectors i = Xi − (cid:23)Xi.

Algorithm Steps (ii) to (vii) of algorithm A6.1 become:

1 , . . . , XT

(ii) Compute the intermediate values

(cid:7)

i

U =

i Σ−1
AT

Xi

Ai

V = diag(V1, . . . , Vn) where Vi = BT
W = [W1, W2, . . . , Wn] where Wi = AT
A =

(cid:7)

i Σ−1
AT

i

Xi

i Σ−1
Bi
Xi
i Σ−1
Bi

Xi

i
B = (T
B2, . . . , T
B1, T
Yi = WiV∗−1
.

Bn)T where Bi = BT

i Σ−1

Xi

i

i
(iii) Compute δa from the equation
(U∗ −

(cid:7)

i

i )δa = A −

YiWT

(cid:7)

i

YiBi .

(iv) Compute each δbi in turn from the equation
δbi = V∗−1

i

(Bi − WT

i δa).

Covariance

(ii) Σa = (U −(cid:27)

(i) Redeﬁne Yi = WiV−1
i
i YiWT
i )+

(iii) Σbibj = YT
(iv) Σabi = −ΣaYi

i ΣaYj + δijV−1

i

Algorithm A6.3. A sparse Levenberg–Marquardt algorithm.

since ˆxi is independent of h. Also,

Bi = ∂(cid:23)Xi/∂ ˆxi =
because of the form of (cid:23)Xi = (ˆxT
i , ˆx(cid:2)

T)T.

i

(cid:18)

(cid:17)

∂ ˆx(cid:2)

I
i/∂ ˆxi

A6.4.1 Computation of the covariance
As an example of covariance computation, we consider the same problem as in section
5.2.4(p145), in which a homography is estimated from point correspondences. As in
that example, we consider a case in which the estimated homography is actually the
identity mapping, H = I. For the purposes of this example, the number of points or
their distribution is not important. It will be assumed, however, that the errors of all

A6.5 Application of sparse LM to fundamental matrix estimation

609

(cid:15)(cid:27)

(cid:16)

point measurements are independent. We recall from (5.12–p146) that in the case of
errors in the second image only, Σh =

, where Ji = [∂ ˆx(cid:2)

xi = Σ−1
x(cid:1)

We now proceed to compute the covariance of the camera parameter vector h in the
case where the points in the ﬁrst image are subject to noise as well. We assume further
that Σ−1
, and denote them by Si. In this case, the inverse covariance matrix Σ−1
X
xi , Σ−1
is block-diagonal, Σ−1
). Then, applying the steps of algorithm A6.3
x(cid:1)
to compute the covariance matrix for h,

X = diag(Σ−1

i/∂h].

Σ−1

JT
i

Ji

+

i

i

i

i

i ]T

(cid:7)

Ai = [0T, JT
Bi = [IT, IT]T since H = I
AT
i diag(Si, Si)Ai =
U =

(cid:7)

JT
i

SiJi

U −(cid:7)

i

WiV−1

i

Vi = BT
Wi = AT
WT
i =

i

(cid:7)

i
(cid:7)
i diag(Si, Si)Bi = 2Si
i diag(Si, Si)Bi = JT
Si
i (Si − Si/2)Ji =
(cid:20)(cid:7)
JT

(cid:21)

+

i

i

i

Σh = 2

JT
i

SiJi

.

JT
i

SiJi/2

i

Hence, the covariance matrix for h is just twice the value of the covariance matrix for
the case of error in one image. This is not generally true, and results from the fact that
H is the identity mapping. As exercises, one may verify the following.
• If H represents a scaling by a factor s, then Σh = (s2 + 1)
• If H is an afﬁne transformation and D is the upper 2 × 2 part of H (the non-
translational part), and if Si = I for all i (isotropic and independent noise), then
Σh =

I − D(I + DTD)

(cid:15)(cid:27)

(cid:15)(cid:27)

−1DT

SiJi

(cid:16)

(cid:15)

(cid:16)

(cid:16)

JT
i

+

+

.

i

.

Ji

JT
i

i

A6.5 Application of sparse LM to fundamental matrix estimation

In estimating the fundamental matrix and a set of 3D points, the algorithm is effec-
tively that described in section A6.3.3, a sparse LM algorithm for the estimation of 2D
homographies, but slightly modiﬁed to the present case. The analogy with the 2D ho-
mography estimation problem is as follows: in estimating 2D homographies, one has a
mapping H that takes points xi to corresponding points x(cid:2)
i; in the present problem, one
has a mapping represented by a pair of camera matrices P and P(cid:2)
that map a 3D point
to a pair of corresponding points (xi, x(cid:2)
i).

For convenience, the notation of section A6.3.3 will be used here. In particular, note
that in section A6.3.3, and here the notation X is used to denote the total measure-
ment vector (in this case (x1, x(cid:2)
n)) and not a 3D point. Also, be careful to
distinguish between P the parameter vector and P the camera matrix.

1, . . . ,x n, x(cid:2)

The parameter vector P is partitioned as P = (aT, bT
(i) a = p(cid:2)

is made up of the entries of camera matrix P(cid:2)

1 , . . . ,b T
, and

n)T, where

610

Appendix 6 Iterative Estimation Methods

(ii) bi = (Xi, Yi, Ti)T is a 3-vector, parametrizing the i-th 3D point (Xi, Yi, 1, Ti).
Thus, there are a total of 3n+12 parameters, where n is the number of points. Parameter
vector a provides a parametrization for the camera P(cid:2)
and the other camera P is taken
as [I | 0]. Note also that it is convenient and permissible to set the third coordinate of
the 3D space point to 1 as here, since a point (Xi, Yi, 0, Ti)T maps to (Xi, Yi, 0)T which
is an inﬁnite point, not anywhere close to the measured point (xi, yi, 1)T.

T)T, the measured images of the i-th point.

The measurement vector X is partitioned as X = (XT

Now, the Jacobian matrices Ai = ∂(cid:23)Xi/∂a and Bi = ∂(cid:23)Xi/∂bi may be computed and

Xi = (xT
algorithm A6.3 applied to estimate the parameters, and hence P(cid:2)
computed.

, from which F may be

n)T, where each

2 , . . . , XT

i , x(cid:2)

1 , XT

i

Partial derivative matrices

Since (cid:23)Xi = (ˆxT
i , ˆx(cid:2)

i

Jacobian matrices in section A6.4:
0
∂ ˆx(cid:2)
i/∂a

Ai =

(cid:17)

(cid:18)

(cid:17)

Bi =

I2×2|0
∂ ˆx(cid:2)
i/∂bi

(cid:18)

.

T)T one may compute that Ai and Bi have a form similar to the

The covariance matrix ΣXi breaks up into diagonal blocks diag(Si, S(cid:2)

i), where
. Now, calculating the intermediate expressions in step 2 of

xi and S(cid:2)

i = Σ−1
Si = Σ−1
x(cid:1)
algorithm A6.3, we ﬁnd

i

Vi = BT

i diag(Si, S(cid:2)

i)Bi = [I2×2 | 0]TSi[I2×2 | 0] + (∂ ˆx(cid:2)

i/∂b)TS(cid:2)

i(∂ ˆx(cid:2)

i/∂b)

(A6.15)

The abstract form of AT
Σ−1
i
expressions Wi = AT
i
The estimation procedure otherwise proceeds precisely as in algorithm A6.3.

Ai is the same as in the 2D homography case, and the other
i may easily be computed.

i, and Ai = AT
i

Σ−1

Σ−1

Xi

Xi

Xi

Σ−1
Xi
Bi, Bi = BT
i

Covariance of F
According to the discussion of section A6.3.3 and in particular algorithm A6.3, the
covariance matrix of the camera parameters, namely the entries of P(cid:2)

, is given by

WiV−1

i

WT
i )+

(A6.16)

ΣP(cid:1) = (U −(cid:7)

with notation as in algorithm A6.3.

i

In computing this pseudo-inverse, it is useful to know the expected rank of ΣP(cid:1). In
this case, this rank is 7, since the total number of degrees of freedom in the solution
involving two cameras and n point matches is 3n + 7. Looked at another way, P(cid:2)
is
= [M | m], then any other matrix [M + tmT | αm]
not determined uniquely, since if P(cid:2)
also determines the same fundamental matrix. Thus, in computing the pseudo-inverse
appearing in the right hand side of (A6.16), one should set ﬁve of the singular values
to zero.
of P(cid:2)
is a simple formula for the entries of F in terms of the entries of P(cid:2)

The foregoing discussion shows how to compute the covariance matrix of the entries
. We desire to compute the covariance matrix of the entries of F. However, there
= [M | m], namely

A6.6 Application of sparse LM to multiple image bundle adjustment

611
F = [m]×M. If one desires to compute the covariance matrix of F normalized so that
(cid:10)F(cid:10) = 1, then one writes F = [m]×M/((cid:10)[m]×M(cid:10)). Therefore, one may express the
entries of F as a simple function in terms of the entries of P(cid:2)
. Let J be the Jacobian
matrix of this function. The covariance of F is then computed by propagating the
covariance of P(cid:2)

using result 5.6(p139) to get

ΣF = JΣP(cid:1)JT = J(U −(cid:7)

WiV−1

i

WT
i )+JT

(A6.17)

where ΣP(cid:1) is given by (A6.16). This is the covariance of the fundamental matrix esti-
mated according to an ML algorithm from the given point correspondences.

i

A6.6 Application of sparse LM to multiple image bundle adjustment

In the previous section, the application of the sparse Levenberg–Marquardt algorithm to
the computation of the fundamental matrix was considered. This is essentially a recon-
struction problem from two views. It should be clear how this may easily be extended
to the computation of the trifocal tensor and the quadrifocal tensor. More generally, one
may apply it to the simultaneous estimation of multiple camera and points to compute
projective structure, or perhaps afﬁne or metric structure given appropriate constraints.
This technique is called bundle adjustment.

In the case of multiple cameras, one may also take advantage of the lack of inter-
action between parameters of the different cameras, as will be shown now.
In the
following discussion, for simplicity of notation it will be assumed that each point is
visible in all the views. This is not at all necessary – points may in general be visible
in some arbitrary subset of the available views.

i1, xT

i2, . . . ,x T

We use the same notation as in section A6.3.3. The measurement data may be ex-
pressed as a vector X, which may be divided up into parts Xi, representing the measured
image coordinates of some 3D point in all views. One may further subdivide Xi writing
Xi = (xT
im)T where xij is the image of the i-th point in the j-th image.
The parameter vector a (camera parameters) may correspondingly be partitioned as
a = (aT
m)T, where aj are the parameters of the j-th camera. Since the
image point xij does not depend on the parameters of any but the j-th camera, one
observes that ∂ ˆxij/∂ak = 0 unless j = k. In a similar way for derivatives with respect
to the parameters bk of the k-th 3D point, one has ∂ ˆxij/∂bk = 0 unless i = k.

1 , aT

2 , . . . ,a T

The form of the Jacobian matrix J for this problem and the resulting normal equa-
tions JTJδ = JT are shown schematically in ﬁgure A6.1 and ﬁgure A6.2. Referring

to the Jacobian matrices deﬁned in algorithm A6.3, one sees that Ai = [∂(cid:23)Xi/∂a] is a
trix Bi = [∂(cid:23)Xi/∂bi] decomposes as Bi = [BT

block diagonal matrix Ai = diag(Ai1, . . . ,A im), where Aij = ∂ ˆxij/∂aj. Similarly, ma-
im]T, where Bij = ∂ ˆxij/∂bi. It may
normally be assumed also that ΣXi has a diagonal structure ΣXi = diag(Σxi1, . . . ,Σ xim),
meaning that the measurements of the projected points in separate images are indepen-
dent (or more precisely, uncorrelated). With these assumptions, one is easily able to
adapt algorithm A6.3, as shown in algorithm A6.4, as the reader is left to verify.

i1, . . . ,B T

612

Appendix 6 Iterative Estimation Methods

P1

P2

P3 X1 X2 X3 X4

1
jx

2
jx

3
jx

Camera

parameters

Feature

parameters

Fig. A6.1. Form of the Jacobian matrix for a bundle-adjustment problem consisting of 3 cameras and 4
points.

U1

U2

W

U3

V1

WT

V2

V3

V4

∆(P1)

∆(P2)

∆(P3)

∆(X1)
∆(X2)
∆(X3)
∆(X4)

ε(P1)

ε(P2)

ε(P3)

ε(X1)
ε(X2)
ε(X3)
ε(X4)

Fig. A6.2. Form of the normal equations for the bundle-adjustment problem consisting of 3 cameras
and 4 points.

Missing data. Typically, in a bundle-adjustment problem some points are not visible
in every image. Thus, some measurement xij may not exist, meaning that i-th point
is not visible in the j-th image. Algorithm A6.4 is easily adapted to this situation,
by ignoring terms subscripted by ij where the measurement xij does not exist. Such
missing terms are simply omitted from the relevant summations. This includes all of
Aij, Bij, Σ−1
xij , Wij and Yij. It may be seen that this is equivalent to setting Aij and Bij to
zero, thus effectively giving zero weight to the missing measurements.

This is convenient when programming this algorithm, since the above quantities sub-
scripted by ij may be associated only with existing measurements in a common data
structure.

A6.7 Sparse methods for equation solving

613

(i) Compute the derivative matrices Aij = [∂ˆxij/∂aj] and Bij = [∂ˆxij/∂bi] and the error

(ii) Compute the intermediate values

vectors ij = xij − ˆxij.
(cid:7)
(cid:7)

Uj =

i

aj =

ijΣ−1
AT

xij

Aij

Vi =

Bij

Wij = AT

ijΣ−1

xij

Bij

ijΣ−1
AT

xij

ij

bi =

ijΣ−1
BT

xij

ij

Yij = WijV∗−1

i

i

j

(cid:7)
ijΣ−1
BT
(cid:7)

xij

j

(cid:7)
(cid:7)

i

YijWT

YijWT

(cid:7)
(cid:7)

i

(iii) Compute δa = (δT

where in each case i = 1, . . . , n and j = 1, . . . , m.
am)T from the equation
1 , . . . , eT

Sδa = (eT

a1, . . . , δT

m)T

where S is an m × m block matrix with blocks Sjk deﬁned by

Sjj = −
Sjk = −

j

ij + U∗
ik if j (cid:5)= k

and

i

ej = aj −

Yijbi

(iv) Compute each δbi in turn from the equation
(bi −

δbi = V∗−1

i

WT
ijδaj )

j

Covariance

(i) Redeﬁne Yij = WijV−1
(ii) Σa = S+, where S is deﬁned as above, without the augmentation represented by the ∗.
(iii) Σbibj = YT
(iv) Σabi = −ΣaYi.

i ΣaYj + δijV−1

.

.

i

i

Algorithm A6.4. General sparse Levenberg–Marquardt algorithm.

A6.7 Sparse methods for equation solving

In a long sequence of images, it is rare for a point to be tracked through the whole
sequence, and usually point tracks disappear and new ones start, causing the set of
point tracks to have a banded structure, as seen in ﬁgure 19.7(p492)(c). This banded
structure of the point track set leads to a banded structure for the set of equations
that are solved to compute structure and motion – we refer here to the matrix S in
algorithm A6.4. Thus, for bundle-adjustment problems with banded track structure,
sparseness can appear at two levels, ﬁrst at the level of independence of the individual
point measurements, as exploited in algorithm A6.4, and secondly arising from the
banded track structure as will be explained in this section.

Another similar context in which this will occur is solution for structure and motion

614

Appendix 6 Iterative Estimation Methods

in section 18.5.1(p448) or simply motion, as in section 18.5.2(p450). In both these
methods, large sparse sets of equations may arise. In order for large problems of this
kind to be tractable, it is necessary to take advantage of this sparse structure in order
to minimize the amount of storage and computation involved. In this section, we will
consider sparse matrix techniques that are valuable in this context.

trix with off-diagonal blocks of the form Sjk = −(cid:27)

A6.7.1 Banded structure in bundle-adjustment
The time-consuming step in ﬁnding the parameter increments in the iterative step of
bundle-adjustment, as given in algorithm A6.4 is the solution of the equations Sδa = e
in step (iii) of the algorithm. As shown there, the matrix S is a symmetric ma-
WT
ik. We see that the
block Sjk is non-zero only when for some i, both Wij and Wik are non-zero. Since
−1
Wij = [∂ ˆxij/∂aj]TΣ
xij [∂ ˆxij/∂bi] it follows that Wij is non-zero only when the corre-
sponding measurement ˆxij depends on parameters aj and bi. To be more concrete, if
aj represents the parameters of the j-th camera, and bi represents the parameters of the
i-th point, then Wij is non-zero only when the i-th point is visible in the j-th image, and
xij is its measured image position.

i

WijV∗−1

i

The condition that for some i both Wij and Wik are non-zero means that there exists
some point with index i that is visible in both the j-th and k-th images. To summarize,
• The block Sjk is non-zero only if there exists a point that is visible in both the j-th
and k-th images.

Thus, if point tracks extend only over consecutive views, then the matrix S will be
banded. In particular, if no point track extends over more than B views (B representing
bandwidth), then the block Sjk is zero unless |j − k| < B.
Consider tracking points over a long sequence, for instance along a path that may
loop back and cross itself. In this case, it may be possible to recognize a point that
had been seen previously in the sequence, and pick up its track again. The set of views
in which a 3D point is seen will not be a consecutive set of views. This will destroy
the banded nature of the matrix S, by introducing non-zero blocks possibly far away
from the central band. Nevertheless, if there is not too much ﬁlling-in of off-diagonal
blocks, sparse solution techniques may still be utilized as we shall see later.

A6.7.2 Solution of symmetric linear equations
In solving a set of linear equations Ax = b in which the matrix A is symmetric, it is best
not to use a general purpose equation solver, such as Gaussian-elimination, but rather
to take advantage of the symmetry of the matrix A. One way of doing this is the use the
LDLT decomposition of the matrix A. This relies on the following observation:
Result A6.1. Any positive-deﬁnite symmetric matrix A can be factored as A = LDLT, in
which L is a lower-triangular matrix with unit diagonal entries, and D is diagonal.

The reader is advised to consult [Golub-89] for details of efﬁcient implementation and
numeric properties of LDLT decomposition. The normal equations derived from struc-
ture and motion problems are always at least positive semi-deﬁnite, and with stabiliza-

A6.7 Sparse methods for equation solving

615

a

b

Fig. A6.3. (a) A sparse matrix and (b) its corresponding “skyline” structure. Non-zero entries in the
original matrix are shown in black. In the skyline format, all non-zero entries lie in the shaded area. For
each row i, there is an integer mi representing the ﬁrst non-zero entry in that row. Note that non-zero
off-diagonal elements cause “ﬁll-in” of the skyline format for that row (or symmetric above-diagonal
column). If such entries are relatively rare, the skyline format will remain sparse, and techniques for
skyline matrices may be usefully applied. The LDLT decomposition of a matrix in skyline format has the
same skyline structure. Thus, the LDLT decomposition of the original sparse matrix shown in (a), will
have non-zero entries only within the shaded area of (b).

tion through enhancement, they are positive-deﬁnite, and symmetric factorization is
the recommended method of solution.
solved for x in three steps as (i) Lx(cid:2)

Given the LDLT factorization, the set of linear equations Ax = LDLTx = b may be
Solving the equations Lx(cid:2)

= b is carried out by the process of “forward-substitution.”
In particular (bearing in mind that L has unit diagonal entries), the components of x
may be computed in order as

= b, (ii) x(cid:2)(cid:2)

= D−1x(cid:2)

, (iii) LTx = x(cid:2)(cid:2)

.

forward-substitution: x(cid:2)

i = bi − i−1(cid:7)

j=1

Lijx(cid:2)
j.

Since LT is upper-triangular, the second set of equations is solved in a similar fashion,
except that the values xi are computed in inverse order in a process known as “back-
substitution.”

back-substitution:

xi = x(cid:2)(cid:2)

i

Ljixj.

− n(cid:7)

j=i+1

The number of operations involved in this computation is given in [Golub-89], and

is equal to n3/3, where n is the dimension of the matrix.

A6.7.3 Solution of sparse symmetric linear systems
We consider a special type of sparse structure for a symmetric matrix, known as “sky-
line” format. This is illustrated in ﬁgure A6.3. An n× n symmetric matrix A in skyline
format is characterized by the existence of an array of integers mi for i = 1, . . . , n such
that Aij = 0 for j < mi. A diagonally banded matrix is a special case of a matrix with
skyline structure.

616

Appendix 6 Iterative Estimation Methods

Although a diagonally banded or skyline matrix may be sparse, its inverse is not,
and in fact will be completely ﬁlled out with non-zero elements. Thus, it is a very bad
idea actually to compute the inverse of A to ﬁnd the solution x = A−1b to the set of
equations. However, the importance of matrices in skyline (or banded) form is that
skyline structure of the matrix is preserved by the LDLT decomposition, as expressed
in the following result.
Result A6.2. Let A be a symmetric matrix such that Aij = 0 for j < mi. Let A = LDLT.
Then Lij = 0 for j < mi.
In other words, the skyline structure of L is the same as that of A.

Proof. Suppose that j is the smallest index such that Lij = 0. Then in multiplying out
A = LDLT, it may be veriﬁed that only one product contributes to the (i, j)-th element
of Aij. Speciﬁcally, Aij = LijDjjLjj (cid:5)= 0.
Thus, in computing the LDLT decomposition of A having a sparse skyline structure,
we know in advance that many of the entries of L will be zero. The algorithm for
computing the LDLT decomposition for such a matrix is much the same as the that for
a full symmetric matrix, except that we do not need to consider the zero elements.

Forward and back substitution involving a matrix L with skyline structure easily take

advantage of the sparse structure. In fact the forward substitution formula becomes

i = bi − i−1(cid:7)

x(cid:2)

Lijx(cid:2)
j.

Back-substitution is left for the reader to work out. More details of implementation are
given in [Bathe-76].

j=mi

A6.8 Robust cost functions

In estimation problems of the Newton or Levenberg-Marquardt type, an important de-
cision to make is the precise form of the cost function. As we have seen, an assumption
of Gaussian noise without outliers implies that the the Maximum Likelihood estimate
is given by a least-squares cost function involving the predicted errors in the measure-
ments, where the noise is introduced.

negative logarithm gives − log(p(δ1, . . . , δn)) = −(cid:27)

The same analysis may be carried out for other assumed probability models for the
measurements. Thus, if all measurements are assumed to be independent, and f (δ) is
the probability distribution of an error δ in the measurement, then the probability of a
n
set of measurement with errors δi is given by p(δ1, . . . , δn) =
i=1 f (δi). Taking the
n
i=1 log(f (δi)) and the right-hand
side of this expression is a suitable cost function for a set of measurements. It is usually
appropriate to set the cost of an exact measurement to be zero, by subtracting log(f (0)),
though this is not strictly necessary if our purpose is cost minimization. Graphs of
various speciﬁc cost functions to be discussed next are shown in ﬁgure A6.4.

*

A6.8 Robust cost functions

617

Cost function

PDF

Attenuation factor

12

10

8

6

4

2

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

12

10

8

6

4

2

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

12

10

8

6

4

2

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

12

10

8

6

4

2

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

12

10

8

6

4

2

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

2

4

-20

-15

-10

-5

5

10

15

20

1

0.8

0.6

0.4

0.2

2

4

-20

-15

-10

-5

5

10

15

20

1

0.8

0.6

0.4

0.2

2

4

-20

-15

-10

-5

5

10

15

20

1

0.8

0.6

0.4

0.2

2

4

-20

-15

-10

-5

5

10

15

20

2

1.75

1.5

1.25

1

0.75

0.5

0.25

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

2

4

-20

-15

-10

-5

5

10

15

20

12

10

8

6

4

2

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

12

10

8

6

4

2

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

1

0.8

0.6

0.4

0.2

2

4

-20

-15

-10

-5

5

10

15

20

1

0.8

0.6

0.4

0.2

Squared-
error

Blake-
Zisserman

corrupted
Gaussian

Cauchy

L1

Huber

pseudo-
Huber

-7.5

-5

-2.5

2.5

5

7.5

-4

-2

2

4

-20

-15

-10

-5

5

10

15

20

Fig. A6.4. A comparison of different cost functions, C(δ), for robust estimation. Their corresponding
PDFs, exp(−C(δ)), and attenuation factors (w = C(δ)1/2/δ see text) are also shown.

618

Appendix 6 Iterative Estimation Methods

Statistically based cost functions. Determination of a suitable cost function may
be approached by estimating or guessing the distribution of errors for the particular
measurement process involved, such as point extraction in an image. In the following
list, for simplicity, we ignore the normalization constant for Gaussian distributions
(2πσ2)

−1/2, and assume that 2σ2 = 1.

(i) Squared error. Assuming the data is Gaussian distributed, the Probability
Distribution Function (PDF) is p(δ) = exp(−δ2) which leads to a cost function

C(δ) = δ2.

(ii) Blake-Zisserman. The data is assumed to have a Gaussian distribution for
inliers with a uniform distribution of outliers. The PDF is taken to be of the
form p(δ) = exp(−δ2) + . This is not actually a PDF, since it integrates to
inﬁnity. Nevertheless, it leads to a cost function of the form

C(δ) =− log(exp(−δ2) + ).

For inliers (small δ), this approximates δ2, whereas for outliers (large δ) the
asymptotic cost is − log . Thus, the crossover point from inliers to outliers is
given approximately by δ2 = − log . The actual cost function used by Blake
and Zisserman in [Blake-87] was min(δ2, α2) and  = exp(−α2).

(iii) Corrupted Gaussian.

The previous example has the theoretical disadvan-
tage that it is not actually a PDF. An alternative is to model the outliers by a
Gaussian with larger standard deviation, leading to a mixture model probability
distribution of the form p(δ) = α exp(−δ2) + (1 − α) exp(δ2/w2)/w where
w is the ratio of standard deviations of the outliers to the inliers, and α is the
expected fraction of inliers. Then

C(δ) =− log(α exp(−δ2) + (1− α) exp(−δ2/w2)/w).

Heuristic cost functions. Next we consider cost functions justiﬁed more by heuris-
tics and required noise-immuneness properties than by adherence to a speciﬁc noise-
distribution model. For this reason they will be introduced directly as a cost function,
rather than a PDF.

(i) Cauchy cost function. The cost function is given by

C(δ) =b 2 log(1 + δ2/b2)

for some constant b. For small values of δ, this curve approximates δ2, and the
value of b determines for what range of δ this approximation is close. The cost
function is derived from the Cauchy distribution p(δ) = 1/(π(1 + δ2)), which
is a bell-curve similar to the Gaussian, but with heavier tails.

(ii) The L1 cost function. Instead of using the sum of squares, we use the sum of

absolute errors. Thus,

C(δ) = 2b|δ|

A6.8 Robust cost functions

619

where 2b is some positive constant (which normally could just be 1). This cost
function is known as the total variation.

(iii) Huber cost function.

This cost function is a hybrid between the L1 and

least-squares cost function. Thus, we deﬁne

C(δ) =δ

2 for |δ| < b

= 2b|δ| − b2 otherwise

This cost function is continuous with continuous ﬁrst derivative. The value of
the threshold b is chosen approximately to equal the outlier threshold.

(iv) Pseudo-Huber cost function. The cost function

(cid:19)
1 + (δ/b)2 − 1)

C(δ) = 2b2(

is very similar to the Huber cost function, but has continuous derivatives of all
orders. Note that it approximates δ2 for small δ and is linear with slope 2b for
large δ.

A6.8.1 Properties of the different cost functions
Squared error. The most basic cost function is the squared error C(δ) =δ 2. Its
main drawback is that it is not robust to outliers in the measurements, as we shall see.
Because of the rapid growth of the quadratic curve, distant outliers exert an excessive
inﬂuence, and can draw the cost minimum well away from the desired value.

Non-convex cost functions. The Blake-Zisserman, corrupted Gaussian and Cauchy
cost functions seek to mitigate the deleterious effect of outliers by giving them dimin-
ished weight. As is seen in the plot of the ﬁrst two of these, once the error exceeds a
certain threshold, it is classiﬁed as an outlier, and the cost remains substantially con-
stant. The Cauchy cost function also seeks to deemphasize the cost of outliers, but
this is done more gradually. These three cost functions are non-convex, which has
important effects as we will see.

(cid:27)

i

Asymptotically linear cost functions. The L1 cost function measures the absolute
value of the error. The main effect of this is to give outliers less weight compared with
the squared error. The key to understanding the performance of this cost function is to
observe that it acts to ﬁnd the median of a set of data. Consider a set of real valued data
{ai} and a cost function deﬁned by C(x) =
|x− ai|. The minimum of this function
is at the median of the set {ai}. To see this, note that the derivative of |x − ai| with
respect to x is +1 when x > ai and −1 when x < ai. Thus, the derivative is zero when
there are as many values of ai less than x as there are greater than x. Thus, the cost is
minimized at the median of the values ai. Note that the median is immune to changes
in the values of data ai that lie far from the median. The value of the cost function
changes, but not the position of its minimum.
For higher dimensional data ai ∈ IRn, the minimum of the cost function C(x) =
(cid:10)x − ai(cid:10) has similar stability properties. Note that (cid:10)x − ai(cid:10) is a convex function

(cid:27)

i

620

Appendix 6 Iterative Estimation Methods

(cid:27)

of x, and therefore so is a sum of such terms,
function has a single minimum (as do all convex functions).

i

(cid:10)x − ai(cid:10). Consequently, this cost

The Huber cost function takes the form of a quadratic for small values of the error,
δ, and becomes linear for values of δ beyond a given threshold. As such, it retains the
outlier stability of the L1 cost function, while for inliers it reﬂects the property that the
squared-error cost function gives the Maximum Likelihood estimate.

The Pseudo-Huber cost function is also near-quadratic for small δ, and linear for
large δ. Thus, it may be used as a smooth approximation to the Huber cost function,
and gives similar results. It is important to note that each of these three cost functions
has the very desirable property of being convex.

(cid:27)
A6.8.2 Performance of the different cost functions
To illustrate the properties of the different cost functions we will evaluate the cost
i C(x − ai) for two synthetic example data sets {ai}. Of the group of asymptotically
linear cost functions, only the Huber cost function will be shown, since the other two
(L1 and pseudo-Huber) give very similar results.
The data {ai} may be thought of as the outcome of an experiment to measure some
quantity, with repeated measurements. The measurements are subject to random Gaus-
sian noise, with outliers. The purpose of the estimation process is to estimate the value
of the quantity by minimizing a cost function. The experiments and the results for the
two data sets are described in the captions of ﬁgure A6.5 and ﬁgure A6.6.

Summary of ﬁndings. The squared-error cost function is generally very susceptible
to outliers, and may be regarded as unusable as long as outliers are present. If outliers
have been thoroughly eradicated, using for instance RANSAC, then it may be used.

The non-convex cost functions, though generally having a stable minimum, not much
effected by outliers have the signiﬁcant disadvantage of having local minima, which
can make convergence to a global minimum chancy. The estimate is not strongly at-
tracted to the minimum from outside of its immediate neighbourhood. Thus, they are
not useful, unless (or until) the estimate is close to the ﬁnal correct value.

The Huber cost function has the pleasant property of being convex, which makes
convergence to a global minimum more reliable. The minimum is quite immune to the
baleful inﬂuence of outliers since it represents a compromise between the Maximum
Likelihood estimate of the inliers and the median of the outliers. The pseudo-Huber
cost function is a good alternative to Huber, but use of L1 should be approached with
care, because of its non-differentiablity at the origin.

These ﬁndings were illustrated on one-dimensional data, but they carry over to higher

dimensional data also.

Parameter minimization. We have seen that the Huber and related cost functions are
convex, and hence have a single minimum. We refer here to the cost C(δ) as a function
of the error δ. In general in problems such as structure from motion, the error δ itself is
a non-linear function of the parameters (such as camera and point positions). For this
reason, the total cost expressed as a function of the motion and structure parameters

A6.8 Robust cost functions

621

0.3

0.25

0.2

0.15

0.1

0.05

70

60

50

40

30

20

10

14

12

10

8

6

4

2

-30

-20

-10

10

20

30

-10

-5

5

10

-10

-5

5

10

7

6

5

4

3

2

1

6

5

4

3

2

1

-10

-5

5

10

-10

-5

5

10

4.4

4.2

-30

-20

-10

10

20

30

3.8

3.6

20

17.5

15

12.5

10

7.5

5

2.5

-10

-5

5

10

(cid:27)
Fig. A6.5. The data {ai} (illustrated in the top left graph) consists of a set of measurements centred
around 0 with unit Gaussian noise, plus 10% of outliers biased towards the right of the true value. The
i C(|x − ai|) correspond (left-to-right and top-to-bottom) to the cost functions Squared
graphs of
error, Cauchy, corrupted-Gaussian, Blake-Zisserman, a zoom of the Blake-Zisserman, and Huber cost
functions. Note that the minimum of the squared-error cost function is pulled signiﬁcantly to the right
by the outliers, whereas the other cost-functions are relatively outlier-independent.
The Blake-Zisserman cost function, which is based most nearly on the distribution of the data, has a very
clear minimum. However, close examination (the zoomed plot) shows an undesirable characteristic,
which is the presence of local minima near each of the outliers. An iterative method to ﬁnd the cost
minimum will fail if it is initiated outside the narrow basin of attraction surrounding the minimum.
By contrast, the Huber cost function is convex, which means the estimate will be drawn towards the
single minimum from any initialization point.

can not be expected to be convex, and local minima are inevitable. Nevertheless, an
important principle is:
• choose a parameterization in which the error is as close as possible to being a linear
function of the parameters, at least locally.

Observing this principle will lead to simpler cost surfaces with fewer local minima,
and generally quicker convergence.

Cost functions and least-squares. Usually cost functions of the type we have
discussed are used in the context of a parameter minimization procedure, such as
Levenberg-Marquardt or Newton iteration. Commonly, these procedures seek to min-
imize the norm of some vector ∆ depending on a set of parameters p. Thus, they
minimize (cid:10)∆(p)(cid:10)2 over all choices of the parameter vector p. For instance in a struc-
(cid:10)δi(cid:10)2 = (cid:10)∆(cid:10)2
ture and motion problem we may seek to minimize
where the values xi are measured image coordinates, the ˆxi are the predicted values de-

(cid:10)xi − ˆxi(cid:10)2 =

(cid:27)

(cid:27)

i

i

622

Appendix 6 Iterative Estimation Methods

0.4

0.3

0.2

0.1

250

200

150

100

50

20

17.5

15

12.5

10

7.5

5

2.5

-30

-20

-10

10

20

30

-30

-20

-10

10

20

30

-30

-20

-10

10

20

30

20

17.5

15

12.5

10

7.5

5

2.5

6

5

4

3

2

1

70

60

50

40

30

20

10

-10

-5

5

10

15

20

-30

-20

-10

10

20

30

-30

-20

-10

10

20

30

Fig. A6.6. In this experiment, as in ﬁgure A6.5, the main part of the data (70%) is centred at the origin,
with 30% of “outliers” concentrated in a block away from the origin (see top left graph). This type of
measurement distribution is quite realistic in many imaging scenarios, for instance where point or edge
measurement is confused by ghost edges. The cost functions in order from the top are: Square error,
Cauchy, corrupted Gaussian, Blake-Zisserman and Huber.
The Squared error cost function ﬁnds the mean of the measurement distribution, which is signiﬁcantly
pulled to the right by the block of outliers. The effect of the outlier block on the non-convex cost functions
is also shown clearly here. Because of the non-convexity, the total cost function does not have a single
minimum, but rather two minima around the separate blocks of measurements. Because of its convexity,
the Huber cost function has a single minimum, which is located close to the median of the data, and is
hardly inﬂuenced by the presence of the 30% of outliers.

rived from the current parameter values, and ∆ is the vector formed by concatenating
the individual error vectors δi.
Since minimization of a squared vector norm (cid:10)∆(cid:10)2 is built into most implementa-
tions of Levenberg-Marquardt, we need to see how to apply the robust cost function in
this case. The answer is to replace each vector δi by a weighted vector δ(cid:2)
i = wiδi such
that

(cid:27)
i C((cid:10)δi(cid:10)) =

(cid:27)

i

for then

(cid:10)2 = w2

i

i

(cid:10)δi(cid:10)2 = C((cid:10)δi(cid:10))

(cid:10)δ(cid:2)
(cid:10)δi(cid:10)2 as desired. From this equality, we ﬁnd

wi = C((cid:10)δi(cid:10))1/2/(cid:10)δi(cid:10).

(A6.18)

Thus, the minimization problem is to minimize (cid:10)∆(cid:2)(cid:10)2 where ∆(cid:2)
is the vector obtained
by concatenating the vectors δ(cid:2)
i = wiδi, and each wi is computed from (A6.18). Note
that wi is a function of (cid:10)δi(cid:10), which normally seeks to attenuate the cost of the outliers.
This attenuation function is shown in the ﬁnal column of ﬁgure A6.4 for the different
cost functions. For the Squared error cost function, the attenuation factor is 1, meaning
no attenuation occurs. For the other cost functions, there is little attenuation within an
inlier region, and points outside this region are attenuated to different degrees.

A6.9 Parametrization

A6.9 Parametrization

623

In the sort of iterative estimate problems we are considering here, an important consid-
eration is how the solution space is to be parametrized. Most of the geometric entities
that we consider do not have a simple Euclidean parametrization. For example, in
solving a geometric optimization problem, we often wish to iterate over all camera
orientations, represented by rotations. Rotations are most conveniently represented for
computational purposes by rotation matrices, but this is a major overparametrization.
The set of 3D rotations is naturally 3-dimensional (in fact, it forms a 3-dimensional Lie
group, SO(3)), and we may wish to parametrize it with only 3 parameters.

Homogeneous vectors or matrices are another common type of representation. It is
tempting to use a parametrization of a homogeneous n-vector just by using the compo-
nents of the vector itself. However, there are really only n − 1 degrees of freedom in
a homogeneous n-vector, and it is sometimes advantageous to parametrize it with only
n − 1 parameters.

Gauge freedom. There has been much attention paid to gauge freedom and gauge in-
dependence in recent papers (for instance [McLauchlan-00]). In this context, the word
gauge means a coordinate system for a parameter set, and gauge-freedom essentially
refers to a change in the representation of the parameter set that does not essentially
change the underlying geometry, and hence has no effect on the cost function. The
most important gauge freedoms commonly encountered are projective or other ambi-
guities, such as those arising in reconstruction problems. However, the scale ambiguity
of homogeneous vectors can be counted as gauge freedom also. Gauge freedoms in the
parametrization of an optimization problem cause the normal equations to be singu-
lar, and hence allow multiple solutions. This problem is avoided by the regularization
(or enhancement) step in Levenberg-Marquardt, but there is evidence that excessive
gauge freedoms, causing slop in the parametrization, can lead to slower convergence.
In addition, when gauge freedoms are present the covariance matrix of the estimated
parameters is troublesome, in that there will be inﬁnite variance in unconstrained pa-
rameter directions. For instance, it makes no sense to talk of the covariance matrix of an
estimated homogeneous vector, unless the scale of the vector is constrained. Therefore,
in the following pages, we will give some methods for obtaining minimal parametriza-
tions for certain common geometric parameter sets.

What makes a good parametrization?
The foremost requirement of a good
parametrization is that it be singularity-free, at least in the areas that are visited during
the course of an iterative optimization. This means that the parametrization should be
locally continuous, differentiable and one-to-one – in short a diffeomorphism. A sim-
ple example of what is meant by this is given by the latitude-longitude parametrization
of a sphere, that is, spherical-polar coordinates. This has a singularity at the poles,
where the mapping from the lat-long coordinates to a neighbourhood of the pole is not
one-to-one. The difﬁculty is that the point with coordinates (latitude = 89
, longitude
= 0
) – they are both points
very close to the pole. However, they are a long way apart in parameter space. To see

) is very close to the point (latitude = 89

◦

◦

◦

, longitude = 90

◦

624

Appendix 6 Iterative Estimation Methods

the effect of this, suppose that an optimization is taking place on the sphere, tracking
down a minimum of a cost function, which exists at the point (89
). If the current
estimate is proceeding in the general direction of this minimum, up the line of zero
longitude, when it gets near the pole it will ﬁnd that although close to the minimum,
it can not get there without a long detour in lat-long parameter space. The difﬁculty is
that arbitrarily close points on the sphere, in the neighbourhood of the singularity (the
pole) can have large differences in parameter values. The same sort of thing happens
with representations of rotations using Euler angles.

◦
, 90

◦

Now, we move on to consider some speciﬁc parametrizations.

A6.9.1 Parametrization of 3D rotations
Using the angle-axis formula of (A4.9–p584) we may parametrize a 3 × 3 rotation by
a 3-vector t. This represents a rotation through an angle (cid:10)t(cid:10) about the axis determined
by the vector t. We denote the corresponding rotation matrix by R(t).

Regarding this representation, we may make certain simple observations:

(i) The identity map (no rotation) is represented by the zero vector t = 0.
(ii) If some rotation R is represented by a vector t, then the inverse rotation is rep-

resented by −t. In symbols: R(t)

−1 = R(−t).

(iii) If t is small, then the rotation matrix is approximated by I + [t]×.
(iv) For small rotations represented by t1 and t2, the composite rotation is repre-
sented to ﬁrst-order by t1 + t2. In other words R(t1)R(t2) ≈ R(t1 + t2). Thus
the mapping t (cid:6)→ R(t) is to ﬁrst order a group isomorphism for small t. In fact,
for small t, the map is an isometry (distance preserving map) in terms of the
distance between two rotations R1 and R2 deﬁned to equal to the angle of the
rotation R1R−1
2 .
(v) Any rotation may be represented as R(t) for some t such that (cid:10)t(cid:10) ≤ π. That
is, any rotation is a rotation through an angle of at most π radians about some
axis. The mapping t (cid:6)→ R(t) is one-to-one for (cid:10)t(cid:10) < π and two-to-one for
(cid:10)t(cid:10) < 2π. If (cid:10)t(cid:10) = 2π, then R(t) is the identity map, regardless of t. Thus, the
parametrization has a singularity at (cid:10)t(cid:10) = 2π.
In parametrizing rotations by a vector t it is best to maintain
the condition that (cid:10)t(cid:10) ≤π , in order to keep away from the singularity when
(cid:10)t(cid:10) = 2π. If (cid:10)t(cid:10) > π, then it may be replaced by the vector ((cid:10)t(cid:10)−2π)t/(cid:10)t(cid:10) =
t(1 − 2π/(cid:10)t(cid:10)), which represents the same rotation.

(vi) Normalization:

A6.9.2 Parametrization of homogeneous vectors
The quaternion representation of a rotation (section A4.3.3(p585)) is a redundant rep-
resentation in that it contains 4 parameters where 3 will sufﬁce. The angle-axis repre-
sentation on the other hand is a minimum parametrization. Many entities in projective
geometry are represented by homogeneous quantities, either vectors or matrices, for
instance points in projective space or the fundamental matrix to name a few. For com-
putational purposes, it is possible to represent such quantities as vectors with a min-

A6.9 Parametrization

625

imum number of parameters in a similar way to which the angle-axis representation
gives an alternative to the quaternion representation of a rotation.
Let v be a vector of any dimension, and represent by ¯v the unit vector
(sinc((cid:10)v(cid:10)/2)vT, cos((cid:10)v(cid:10)/2))T. This mapping v (cid:6)→ ¯v maps the disk of radius π
(that is, the set of vectors of length at most π) smoothly and one-to-one onto the set
of unit vectors ¯v with non-negative ﬁnal coordinate. Thus, it provides a mapping onto
the set of homogeneous vectors. The only difﬁcult point with this mapping is that it
takes any vector of length 2π to the same vector (0,−1)T. However, this singular point
may be avoided by renormalizing any vector v of length (cid:10)v(cid:10) > π, replacing it with
((cid:10)v(cid:10) −2 π)v/(cid:10)v(cid:10) which represents the same homogeneous vector ¯v.

A6.9.3 Parametrization of the n-sphere
Commonly it occurs in geometric optimization problems that some vector of parame-
ters is required to lie on a unit sphere. As an example, consider a complete Euclidean
bundle-adjustment with two views. The two cameras may be taken as P = [I | 0] and
= [R|t], where R is a rotation and t is a translation. In addition, 3D points Xi are
P(cid:2)
deﬁned, which are to map via the two camera matrices to image points. This deﬁnes an
optimization problem in which the parameters are R, t and the points Xi, and the cost to
be minimized is a geometric residual of the projections with respect to the image mea-
surements. In this problem, there is an overall scale ambiguity, and this is conveniently
resolved by requiring that (cid:10)t(cid:10) = 1.
A minimum parametrization for the rotation matrix R is given by the rota-
tion parametrization of section A6.9.1. Similarly, the points Xi are conveniently
parametrized as homogeneous 4-vectors, using the parametrization of section A6.9.2.
We consider in this section how one may parametrize the unit vector t. Note that we can
not simply parametrize t as a homogeneous vector, since change of sign of t changes
the projection P(cid:2)X = [R|t]X.
The same problem arises in multiple-view Euclidean bundle-adjustment. In this case
we have many camera matrices Pi = [Ri|ti] and we may ﬁx P0 = [I | 0]. The set
of translations ti for all i > 0 are subject to scale ambiguity. We can minimally
parametrize the translations by requiring that (cid:10)T(cid:10) = 1, where T is the vector formed
by concatenating all the ti for i > 0.

There may be several ways of parametrizing a unit vector. We consider one particular
parametrization here based on a local parametrization of the tangent plane to the unit
sphere. We consider a sphere of dimension n, which consists of the set of (n + 1)-
vectors of unit length. Let x be a such a vector. Let Hv(x) be a Householder matrix (see
section A4.1.2(p580)) such that Hv(x)x = (0, . . . ,0 , 1)T. Thus, we have transformed
the vector x to lie along the coordinate axis. Now, we consider a parametrization of the
unit sphere in the vicinity of (0, . . . ,0 , 1)T. Such a parametrization is a map IRn → Sn
that is well behaved in the vicinity of the origin. There are many choices, of which two
possibilities are

(i) f (y) = ˆy/(cid:10)ˆy(cid:10) where ˆy = (yT, 1)T
(ii) f (y) = (sinc((cid:10)y(cid:10)/2)yT, cos((cid:10)y(cid:10))/2)T .

Appendix 6 Iterative Estimation Methods

626
Both these functions map the origin (0, . . . ,0) T to (0, . . . ,0 , 1)T, and their Jacobian is
∂f /∂y = [I|0]T. Note that although we are interested in these functions just as local
parametrizations, the ﬁrst provides a parametrization for half the sphere, whereas for
(cid:10)y(cid:10) ≤π the second map parametrizes the whole of the sphere with no singularity
except at (cid:10)y(cid:10) = 2π.
The composite map y (cid:6)→ Hv(x)f (y) provides a local parametrization for a neigh-
bourhood of the point x on the sphere (note here that we should write H−1
v(x), but
v(x)). The Jacobian of this map is simply Hv(x)[I|0]T, which consists of
Hv(x) = H−1
the ﬁrst n columns of the Householder matrix, and hence is easy to compute.

In minimization problems, we usually need to compute a Jacobian matrix ∂C/∂y,
of a vector valued cost function C with respect to a set of parameters y. In the case
where the parameters are constrainted to lie on a sphere S n in IRn+1 the cost function is
nonetheless usually deﬁned for all values of the parameter x in IRn+1. As an example,
in the Euclidean bundle-adjustment problem considered at the start of this section, the
cost function (for instance residual reprojection error) can be deﬁned for all pairs of
cameras P = [I | 0] and P(cid:2)
= [R|t] with t taking any value. Nevertheless, we may wish
to minimize the cost function constraining t to lie on a sphere.
Thus, consider the case where a cost function C(x) is deﬁned for x ∈ IRn+1, but we
parametrize x to lie on a sphere by setting x = Hv(x)f (y) with y ∈ IRn. In this case,
we see that

Hv(x)[I|0]T.

J =

∂C
∂y =

∂C
∂x

∂x
∂y =

∂C
∂x

In summary, by using local parametrizations, a parameter vector may be constrained
to lie on an n-sphere with a modest added computational cost compared with the over-
parametrization of allowing the vector to vary over the whole of IRn+1. The key points
of the method are as follows.

(i) Store the parameter vector x ∈ IRn+1, satisfying (cid:10)x(cid:10) = 1.
(ii) In forming the linear update equations, compute the Jacobian matrix ∂C/∂x,
and multiply it by Hv(x)[I|0]T to obtain the Jacobian with respect to a minimal
parameter set y. Multiplication of ∂C/∂x by Hv(x)[I|0]T is efﬁciently carried
out by the method of (A4.4–p580).

(iii) The iteration step provides an increment parameter vector δy. Compute the

new value of x = Hv(x)f (δy).

Essentially the same method of using local-parametrizations may be used more gen-
erally in other situations where a minimal parametrization is required. For instance,
in section 11.4.2(p285) it was seen how the fundamental matrix may be parametrized
locally with a minimum number of parameters, but no minimal parametrization can
cover the whole set of fundamental matrices.

A6.10 Notes and exercises

(i) We prove in various steps the form of the pseudo-inverse of a block matrix

given in (A6.13–p606).

A6.10 Notes and exercises

627
−1GT if and only if NL(G) = NL(H) (see
(a) Recall that H+ = G(GTHG)
(b) Let G be invertible. Then (GHGT)+ = G−TH+G−1 if and only if

section A5.2(p590)).
NL(H)GT = NL(H)G−1.
(c) Applying this condition to (A6.12–p605) the necessary and sufﬁcient
condition for (A6.13–p606) to be valid is that NL(U − WV−1WT) ⊆
NL(Y) = NL(W).
equivalent to the condition span(A) ∩ span(B) =∅ .

(d) With U, V and W deﬁned in terms of A and B as in (A6.11–p605) this is

(ii) Investigate conditions under which the condition span(A)∩ span(B) = ∅ is true.
It may be interpreted as meaning that the effects of varying the parameters a
(for instance camera parameters) and the effects of varying b (point parameters)
can not be complementary. Clearly this is not the case with (for instance) un-
constrained projective reconstruction where both cameras and points may vary
without affecting the measurements. In such a case, the variance of parameters
a and b is inﬁnite in directions δa and δb such that Aδa = Bδb.

Appendix 7

Some Special Plane Projective Transformations

Projective transformations (homographies) can be classiﬁed according to the algebraic
and geometric multiplicity of their eigenvalues. The algebraic multiplicity of an eigen-
value is the number of times the root is repeated in the characteristic equation. The
geometric multiplicity may be determined from the rank of the matrix (H − λI), where
H is the homography and λ the eigenvalue. A complete classiﬁcation is given in projec-
tive geometry textbooks such as [Springer-64]. Here we mention several special cases
which are important in practical situations, and occur at several points throughout this
book. The description will be for plane transformations where H is a 3 × 3 matrix, but
the generalization to 3-space transformations is straightforward.

The special forms are signiﬁcant because H satisﬁes a number of relationships (re-
member the only restriction a general projective transformation is that it has full rank).
Since H satisﬁes constraints it has fewer degrees of freedom, and consequently can be
computed from fewer correspondences than a general projective transformation. The
special transformations also have richer geometry and invariants than in the general
case.

Note that unlike the special forms (afﬁne etc.) discussed in chapter 2, which form
subgroups, the following special projectivities do not form subgroups in general since
they are not closed under multiplication. They do form a subgroup if all the elements
have coincident ﬁxed points and lines (i.e. differing only in their eigenvalues).

A7.1 Conjugate rotations

A rotation matrix R has eigenvalues {1, eiθ, e
−iθ}, with corresponding eigenvectors
{a, I, J}, where a is the rotation axis, i.e. Ra = a, θ is the angle of rotation about
the axis, and I and J (which are complex conjugate) are the circular points for the plane
orthogonal to a. Suppose a projective transformation between two planes has the form

H = T R T−1

with T a general projective transformation; then H is a conjugate rotation. Eigenval-
ues are preserved under a conjugate relationship1 so the eigenvalues of the projective
transformation H are also {1, eiθ, e

−iθ} up to a common scale.

1 Conjugacy is also referred to as a “similarity transformation”. This meaning of “similarity” is unrelated to its use in this book

as an isometry plus scaling transformation.

628

A7.2 Planar homologies

629

e 1

v = vertex (fixed point)

fixed
 lines

a = axis (line of fixed points)

e 3

e

2

Fig. A7.1. A planar homology. A planar homology is a plane projective transformation which has a
line a of ﬁxed points, called the axis, and a distinct ﬁxed point v, not on the line, called the centre or
vertex of the homology. There is a pencil of ﬁxed lines through the vertex. Algebraically, two of the
eigenvalues of the transformation matrix are equal, and the ﬁxed line corresponds to the 2D invariant
space of the matrix (here the repeated eigenvalues are λ2 and λ3).

Consider two images obtained by a camera rotating about its centre (as in ﬁgure 2.5-
(p36)b); then as shown in section 8.4.2(p204), the images are related by a conjugate
rotation. In this case the complex eigenvalues determine the angle (θ) through which
the camera rotates, and the eigenvector corresponding to the real eigenvalue is the
vanishing point of the rotation axis. Note that θ – a metric invariant – can be measured
directly from the projective transformation.

A7.2 Planar homologies

A plane projective transformation H is a planar homology if it has a line of ﬁxed points
(called the axis), together with a ﬁxed point (called the vertex) not on the line, see
ﬁgure A7.1. Algebraically, the matrix has two equal and one distinct eigenvalues and
the eigenspace corresponding to the equal eigenvalues is two-dimensional. The axis
is the line through the two eigenvectors (i.e. points) spanning this eigenspace. The
vertex corresponds to the other eigenvector. The ratio of the distinct eigenvalue to the
repeated one is a characteristic invariant µ of the homology (i.e. the eigenvalues are, up
to a common scale factor, {µ, 1, 1}).
Properties of a planar homology include:
• Lines joining corresponding points intersect at the vertex, and corresponding lines
(i.e. lines through two pairs of corresponding points) intersect on the axis. This is an
example of Desargues’ Theorem. See ﬁgure A7.2a.
• The cross ratios deﬁned by the vertex, a pair of corresponding points, and the inter-
section of the line joining these points with the line of ﬁxed points, are the same for
all points related by the homology. See ﬁgure A7.2b.
• For curves related by a planar homology, corresponding tangents (the limit of neigh-
bouring points deﬁning corresponding lines) intersect on the axis.
• The vertex (2 dof), axis (2 dof) and invariant (1 dof) are sufﬁcient to deﬁne the
homology completely. A planar homology thus has 5 degrees of freedom.

630

Appendix 7 Some Special Plane Projective Transformations

vertex  v

/

x1

/x
2

x 2

a

H

x 1

v

x

/
1

x

/
2

axis  a

x

1

p
12

x

2

i

2

a

b

i

1

Fig. A7.2. Homology transformation. (a) Under the transformation points on the axis are mapped to
themselves; each point off the axis lies on a ﬁxed line through v intersecting a and is mapped to another
point on the line. Consequently, corresponding point pairs x ↔ x(cid:1)
and the vertex of the homology
are collinear. Corresponding lines – i.e. lines through pairs of corresponding points – intersect on the
axis: for example, the lines (cid:8)x1, x2(cid:9) and (cid:8)x(cid:1)
(b) The cross ratio deﬁned by the vertex v, the
corresponding points x, x(cid:1)
and the intersection of their join with the axis i, is thecharacteristic invariant
of the homology, and is the same for all corresponding points. For example, the cross ratios of the
four points {v, x(cid:1)
2, x2, i2} are equal since they are perspectively related by lines
concurrent at p12. It follows that the cross ratio is the same for all points related by the homology.
• 3 matched points are sufﬁcient to compute a planar homology. The 6 degrees of free-
dom of the point matches over-constrain the 5 degrees of freedom of the homology.

1, x1, i1} and {v, x(cid:1)

2(cid:9).
1, x(cid:1)

A planar homology arises naturally in an image of two planes related by a perspectiv-
ity of 3-space (i.e. lines joining corresponding points on the two planes are concurrent).
An example is the transformation between the image of a planar object and the image
of its shadow on a plane. In this case the axis is the imaged intersection of the two
planes, and the vertex is the image of the light source, see ﬁgure 2.5(p36)c.

Parametrization. The projective transformation representing the homology can be
parametrized directly in terms of the 3-vectors representing the axis a and vertex v,
and the characteristic ratio µ, as

where I is the identity. It is straightforward to verify that the inverse transformation is
given by

vaT
vTa

H = I + (µ − 1)
(cid:21)
− 1

(cid:20)

H−1 = I +

1
µ

vaT
vTa.

The eigenvectors are

{e1 = v, e2 = a⊥

1 , e3 = a⊥

2

}

A7.3 Elations

631

with corresponding eigenvalues

{λ1 = µ, λ2 = 1, λ3 = 1}

i are two vectors that span the space orthogonal to the 3-vector a, i.e. aTa⊥

i = 0

where a⊥
and a = a⊥

1

× a⊥
2 .

If the axis or the vertex is at inﬁnity then the homology is an afﬁne transformation.
Algebraically, if a = (0, 0, 1)T, then the axis is at inﬁnity; or if v = (v1, v2, 0)T, then
the vertex is at inﬁnity; and in both cases the transformation matrix H has last row
(0, 0, 1).

Planar harmonic homology. A specialization of a planar homology is the case that
the cross ratio is harmonic (µ = −1). This planar homology is called a planar harmonic
homology and has 4 degrees of freedom since the invariant is known. The transforma-
tion matrix H obeys H2 = I, i.e. the transformation is a square root of the identity, which
is called an involution (also a collineation of period 2). The eigenvalues are, up to a
common scale factor, {−1, 1, 1}. Two pairs of point correspondences determine H.
In a perspective image of a plane object with a bilateral symmetry, corresponding
points in the image are related by a planar harmonic homology. The axis of the ho-
mology is the image of the symmetry axis. Algebraically, H is a conjugate reﬂection
where the conjugating element is a plane projective transformation. In an afﬁne image
(generated by an afﬁne camera) the resulting transformation is a skewed symmetry, and
the conjugating element is a plane afﬁne transformation. For a skewed symmetry the
vertex is at inﬁnity, and the lines joining corresponding points are parallel.

The harmonic homology can be parametrized as
H = H−1 = I − 2

vaT
vTa.

Again, if the axis or vertex is at inﬁnity then the transformation is afﬁne.

A7.3 Elations

An elation has a line of ﬁxed points (the axis), and a pencil of ﬁxed lines intersecting
in a point (the vertex) on the axis. It may be thought of as the limit of a homology
where the vertex is on the line of ﬁxed points. Algebraically, the matrix has three equal
eigenvalues, but the eigenspace is 2-dimensional. It may be parametrized as

H = I + µvaT with aTv = 0

(A7.1)

where a is the axis, and v the vertex. The eigenvalues are all unity. The invariant space
of H is spanned by a⊥
2 . This is a line (pencil) of ﬁxed points (which includes v
since aTv = 0). The invariant space of HT is spanned by vectors v⊥
2 orthogonal to
v. This is a pencil of ﬁxed lines, l = αv⊥
2 , for which lTv = 0, i.e. all lines of
the pencil are concurrent at the point v.

1 + βv⊥

1 , v⊥

1 , a⊥

An elation has 4 degrees of freedom: one less than a homology due to the constraint
aTv = 0. It is deﬁned by the axis a (2 dof), the vertex v on a (1 dof) and the parameter
µ (1 dof). It can be determined from 2 point correspondences.

632

Appendix 7 Some Special Plane Projective Transformations

/

x

c

b

a

A

B

C

x

Fig. A7.3. A line perspectivity. The lines joining corresponding points (a, A etc.) are concurrent.
Compare with ﬁgure A7.4.

Elations often arise in practice as conjugate translations. Consider a pattern on a
plane that repeats by a translation t = (tx, ty)T, for example identical windows on the
wall of a building. This action is represented on the plane of the wall as

(cid:17)

(cid:18)

HE =

t
I
0T 1

which is an elation where v = (tx, ty, 0)T is the translation direction of the repetition,
and a = (0, 0, 1)T is the line at inﬁnity. In an image of the wall the windows are related
by a conjugate translation H = T HE T−1, where T is the projectivity which maps the
plane of the wall to the image. The image transformation H is also an elation. The
vertex of this elation is the vanishing point of the translation direction, and the axis is
the vanishing line of the wall plane.

A7.4 Perspectivities

One other special case of a projectivity is a perspectivity, which is shown in ﬁgure A7.3
for a 1D projectivity on the plane. The distinctive property of a perspectivity is that
lines joining corresponding points are concurrent. The difference between a perspec-
tivity and projectivity is made clear by considering the composition of two perspectiv-
ities. As shown in ﬁgure A7.4 the composition of two perspectivities is not in general
a perspectivity. However, the composition is a projectivity because a perspectivity is
a projectivity, and projectivities form a group (closed), so that the composition of two
projectivities is a projectivity. To summarize:
• The composition of two (or more) perspectivities is a projectivity, but not, in general,
a perspectivity.

A central projection image of a world plane, as in ﬁgure 2.3(p34), is an example of
a 2D perspectivity between different planes. Notice that identifying the projectivity as
a perspectivity requires the embedding of the planes in 3-space.

Finally, imagine that the planes and camera centre of ﬁgure 2.3(p34) are mapped
(by another perspectivity) onto one of the planes. Then this imaged perspectivity is
now a map between points on the same plane, and is seen to be a planar homology
(section A7.2).

A7.4 Perspectivities

633

x

c

b

a

image 1

p

r

q

x /

/

a

/
b

/

c

image 2

A

B

C

Fig. A7.4. A line projectivity. Points {a, b, c} are related to points {A, B, C} by a line–to–line per-
spectivity. Points {a(cid:1)
, c(cid:1)} are also related to points {A, B, C} by a perspectivity. However, points
{a, b, c} are related to points {a(cid:1)
, c(cid:1)} by a projectivity; they are not related by a perspectivity be-
cause lines joining corresponding points are not concurrent. In fact the pairwise intersections result in
three distinct points {p, q, r}.

, b(cid:1)

, b(cid:1)

Further reading. [Springer-64] classiﬁes projectivities and covers special cases,
e.g. planar homologies. Planar homologies appear in many guises: modelling im-
aged shadow relations in [VanGool-98]; modelling imaged extruded surfaces in
[Zisserman-95a]; and modelling relations for planar pose recovery in [Basri-99].
The parametrization of planar homologies is given in Vi´eville and Lingrand
[Vieville-95]. Elations appear in the grouping of imaged repeated patterns on a plane
[Schaffalitzky-99, Schaffalitzky-00b] and in 3-space they appear in the generalized
bas-relief ambiguity [Kriegman-98].

Bibliography

[Agrawal-03] M. Agrawal and L. Davis. Camera calibration using spheres: A dual-space approach.

Research Report CAR-TR-984, Center for Automation Research, University of Maryland, 2003.

[Aloimonos-90] J. Y. Aloimonos. Perspective approximations. Image and Vision Computing, 8(3):177–

192, August 1990.

[Anandan-02] P. Anandan and M. Irani. Factorization with uncertainty. International Journal of Com-

puter Vision, 49(2/3):101–116, 2002.

[Armstrong-94] M. Armstrong, A. Zisserman, and P. Beardsley. Euclidean reconstruction from uncali-

brated images. In Proc. British Machine Vision Conference, pages 509–518, 1994.

[Armstrong-96a] M. Armstrong. Self-Calibration from Image Sequences. PhD thesis, University of

Oxford, England, 1996.

[Armstrong-96b] M. Armstrong, A. Zisserman, and R. Hartley. Self-calibration from image triplets. In
Proc. European Conference on Computer Vision, LNCS 1064/5, pages 3–16. Springer-Verlag, 1996.
[Astrom-98] K. ˚Astr¨om and A. Heyden. Continuous time matching constraints for image streams. Inter-

national Journal of Computer Vision, 28(1):85–96, 1998.

[Avidan-98] S. Avidan and A. Shashua. Threading fundamental matrices. In Proc. 5th European Con-

ference on Computer Vision, Freiburg, Germany, pages 124–140, 1998.

[Baillard-99] C. Baillard and A. Zisserman. Automatic reconstruction of piecewise planar models from
multiple views. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 559–
565, June 1999.

[Barrett-92] E. B. Barrett, M. H. Brill, N. N. Haag, and P. M. Payton. Invariant linear methods in pho-
togrammetry and model-matching. In J. L. Mundy and A. Zisserman, editors, Geometric invariance
in computer vision. MIT Press, Cambridge, 1992.

[Bascle-98] B. Bascle and A. Blake. Separability of pose and expression in facial tracing and animation.

In Proc. International Conference on Computer Vision, pages 323–328, 1998.

[Basri-99] R. Basri and D. Jacobs. Projective alignment with regions. In Proc. 7th International Confer-

ence on Computer Vision, Kerkyra, Greece, pages 1158–1164, 1999.

[Bathe-76] K-J. Bathe and E. Wilson. Numerical methods in ﬁnite element analysis. Prentice Hall, 1976.
Insight
[Beardsley-92] P. A. Beardsley, D. Sinclair, and A. Zisserman. Ego-motion from six points.

meeting, Catholic University Leuven, February 1992.

[Beardsley-94] P. A. Beardsley, A. Zisserman, and D. W. Murray. Navigation using afﬁne structure and
motion. In Proc. European Conference on Computer Vision, LNCS 800/801, pages 85–96. Springer-
Verlag, 1994.

[Beardsley-95a] P. A. Beardsley and A. Zisserman. Afﬁne calibration of mobile vehicles. In Europe–
China workshop on Geometrical Modelling and Invariants for Computer Vision, pages 214–221.
Xidan University Press, Xi’an, China, 1995.

[Beardsley-95b] P. A. Beardsley, I. D. Reid, A. Zisserman, and D. W. Murray. Active visual navigation
In Proc. International Conference on Computer Vision, pages 58–64,

using non-metric structure.
1995.

634

Bibliography

635

[Beardsley-96] P. A. Beardsley, P. H. S. Torr, and A. Zisserman. 3D model aquisition from extended
image sequences. In Proc. 4th European Conference on Computer Vision, LNCS 1065, Cambridge,
pages 683–695, 1996.

[Blake-87] A. Blake and A. Zisserman. Visual Reconstruction. MIT Press, Cambridge, USA, August

1987.

[Boehm-94] W. Boehm and H. Prautzsch. Geometric Concepts for Geometric Design. A. K. Peters,

1994.

[Bookstein-79] F. Bookstein. Fitting conic sections to scattered data. Computer Graphics and Image

Processing, 9:56–71, 1979.

[Bougnoux-98] S. Bougnoux. From Projective to Euclidean space under any practical situation, a criti-
cism of self-calibration. In Proc. 6th International Conference on Computer Vision, Bombay, India,
pages 790–796, January 1998.

[Boult-91] I. E. Boult and L. Gottesfeld Brown. Factorisation-based segmentation of motions. In Proc.

IEEE Workshop on Visual Motion, 1991.

[Brand-01] M. Brand. Morphable 3d models from video. In Proc. IEEE Conference on Computer Vision

and Pattern Recognition, pages II: 456–463, 2001.

[Brown-71] D. C. Brown. Close-range camera calibration. Photogrammetric Engineering, 37(8):855–

866, 1971.

[Buchanan-88] T. Buchanan. The twisted cubic and camera calibration. Computer Vision, Graphics and

Image Processing, 42:130–132, 1988.

[Buchanan-92] T. Buchanan. Critical sets for 3D reconstruction using lines. In Proc. European Confer-

ence on Computer Vision, LNCS 588, pages 730–738. Springer-Verlag, 1992.

[Canny-86] J. F. Canny. A computational approach to edge detection. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 8(6):679–698, 1986.

[Capel-98] D. Capel and A. Zisserman. Automated mosaicing with super-resolution zoom.

In Proc.
IEEE Conference on Computer Vision and Pattern Recognition, Santa Barbara, pages 885–891, June
1998.

[Caprile-90] B. Caprile and V. Torre. Using vanishing points for camera calibration. International Jour-

nal of Computer Vision, 4:127–140, 1990.

[Carlsson-93] S. Carlsson. Multiple image invariance using the double algebra. In Applications of In-

variance in Computer Vision, volume SLN Comp. Science vol 825, pages 335–350, 1993.

[Carlsson-94] S. Carlsson. Multiple image invariance using the double algebra. In J. Mundy, A. Zisser-
man, and D. Forsyth, editors, Applications of Invariance in Computer Vision LNCS 825. Springer-
Verlag, 1994.

[Carlsson-95] S. Carlsson. Duality of reconstruction and positioning from projective views. In IEEE

Workshop on Representation of Visual Scenes, Boston, 1995.

[Carlsson-98] S. Carlsson and D. Weinshall. Dual computation of projective shape and camera positions

from multiple images. International Journal of Computer Vision, 27(3):227–241, 1998.

[Christy-96] S. Christy and R. Horaud. Euclidean shape and motion from multiple perspective views by
afﬁne iteration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(11):1098–1104,
November 1996.

[Chum-03] O. Chum, T. Werner, and T. Pajdla. On joint orientation of epipoles. Research Report CTU–
CMP–2003–10, Center for Machine Perception, K333 FEE Czech Technical University, Prague,
Czech Republic, April 2003.

[Cipolla-99] R. Cipolla, T. Drummond, and D. Robertson. Camera calibration from vanishing points in

images of architectural scenes. In Proc. British Machine Vision Conference, September 1999.

[Collins-93] R. T. Collins and J. R. Beveridge. Matching perspective views of coplanar structures using
projective unwarping and similarity matching. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, 1993.

[Costeira-98] J.P. Costeira and T. Kanade. A multibody factorization method for independently moving

objects. International Journal of Computer Vision, 29(3):159–179, 1998.

[Criminisi-98] A. Criminisi, I. Reid, and A. Zisserman. Duality, rigidity and planar parallax. In Proc.

636

Bibliography

European Conference on Computer Vision, pages 846–861. Springer-Verlag, June 1998.

[Criminisi-99a] A. Criminisi, I. Reid, and A. Zisserman. Single view metrology. In Proc. 7th Interna-

tional Conference on Computer Vision, Kerkyra, Greece, pages 434–442, September 1999.

[Criminisi-99b] A. Criminisi, I. Reid, and A. Zisserman. A plane measuring device. Image and Vision

Computing, 17(8):625–634, 1999.

[Criminisi-00] A. Criminisi, I. Reid, and A. Zisserman. Single view metrology. International Journal of

Computer Vision, 40(2):123–148, November 2000.

[Criminisi-01] A. Criminisi. Accurate Visual Metrology from Single and Multiple Uncalibrated Images.

Distinguished Dissertation Series. Springer-Verlag London Ltd., July 2001. ISBN: 1852334681.

[Cross-98] G. Cross and A. Zisserman. Quadric surface reconstruction from dual-space geometry. In
Proc. 6th International Conference on Computer Vision, Bombay, India, pages 25–31, January 1998.
[Cross-99] G. Cross, A. W. Fitzgibbon, and A. Zisserman. Parallax geometry of smooth surfaces in
multiple views. In Proc. 7th International Conference on Computer Vision, Kerkyra, Greece, pages
323–329, September 1999.

[Csurka-97] G. Csurka, C. Zeller, Z. Zhang, and O. D. Faugeras. Characterizing the uncertainty of the

fundamental matrix. Computer Vision and Image Understanding, 68(1):18–36, October 1997.

[Csurka-98] G. Csurka, D. Demirdjian, A. Ruf, and R. Horaud. Closed-form solutions for the euclidean
calibration of a stereo rig. In Proc. 5th European Conference on Computer Vision, Freiburg, Germany,
pages 426–442, June 1998.

[DeAgapito-98] L. de Agapito, E. Hayman, and I. Reid. Self-calibration of a rotating camera with varying

intrinsic parameters. In Proc. 9th British Machine Vision Conference, Southampton, 1998.

[DeAgapito-99] L. de Agapito, R. I. Hartley, and E. Hayman. Linear self-calibration of a rotating and
In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages

zooming camera.
15–21, 1999.

[Dementhon-95] D. Dementhon and L. Davis. Model based pose in 25 lines of code.

International

Journal of Computer Vision, 15(1/2):123–141, 1995.

[Devernay-95] F. Devernay and O. D. Faugeras. Automatic calibration and removal of distortion from

scenes of structured environments. In SPIE, volume 2567, San Diego, CA, July 1995.

[Devernay-96] F. Devernay and O. D. Faugeras. From projective to euclidean reconstruction. In Proc.

IEEE Conference on Computer Vision and Pattern Recognition, pages 264–269, 1996.

[Faugeras-90] O. D. Faugeras and S. J. Maybank. Motion from point matches: Multiplicity of solutions.

International Journal of Computer Vision, 4:225–246, 1990.

[Faugeras-92a] O. D. Faugeras, Q. Luong, and S. Maybank. Camera self-calibration: Theory and exper-
iments. In Proc. European Conference on Computer Vision, LNCS 588, pages 321–334. Springer-
Verlag, 1992.

[Faugeras-92b] O. D. Faugeras. What can be seen in three dimensions with an uncalibrated stereo rig? In
Proc. European Conference on Computer Vision, LNCS 588, pages 563–578. Springer-Verlag, 1992.
[Faugeras-93] O. D. Faugeras. Three-Dimensional Computer Vision: a Geometric Viewpoint. MIT Press,

1993.

[Faugeras-94] O. D. Faugeras and L. Robert. What can two images tell us about a third one. In J. O.
Eckland, editor, Proc. 3rd European Conference on Computer Vision, Stockholm, pages 485–492.
Springer-Verlag, 1994.

[Faugeras-95a] O. D. Faugeras and B. Mourrain. On the geometry and algebra of point and line cor-
In Proc. International Conference on Computer Vision, pages

respondences between N images.
951–962, 1995.

[Faugeras-95b] O. D. Faugeras. Stratiﬁcation of three-dimensional vision: projective, afﬁne, and metric

representation. Journal of the Optical Society of America, A12:465–484, 1995.

[Faugeras-95c] O. D. Faugeras, S. Laveau, L. Robert, G. Csurka, and C. Zeller. 3-D reconstruction of

urban scenes from sequences of images. Technical report, INRIA, 1995.

[Faugeras-97] O. D. Faugeras and T. Papadopoulo. Grassmann-Cayley algebra for modeling systems
of cameras and the algebraic equations of the manifold of trifocal tensors. Technical Report 3225,
INRIA, Sophia-Antipolis, France, 1997.

Bibliography

637

[Faugeras-98] O. D. Faugeras, L. Quan, and P. Sturm. Self-calibration of a 1D projective camera and
its application to the self-calibration of a 2D projective camera. In Proc. European Conference on
Computer Vision, pages 36–52, 1998.

[Fischler-81] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model ﬁt-
ting with applications to image analysis and automated cartography. Comm. Assoc. Comp. Mach.,
24(6):381–395, 1981.

[Fitzgibbon-98a] A. W. Fitzgibbon and A. Zisserman. Automatic camera recovery for closed or open
In Proc. European Conference on Computer Vision, pages 311–326. Springer-

image sequences.
Verlag, June 1998.

[Fitzgibbon-98b] A. W. Fitzgibbon, G. Cross, and A. Zisserman. Automatic 3D model construction for
turn-table sequences. In R. Koch and L. Van Gool, editors, 3D Structure from Multiple Images of
Large-Scale Environments, LNCS 1506, pages 155–170. Springer-Verlag, June 1998.

[Fitzgibbon-99] A. W. Fitzgibbon, M. Pilu, and R. B. Fisher. Direct least-squares ﬁtting of ellipses. IEEE

Transactions on Pattern Analysis and Machine Intelligence, 21(5):476–480, May 1999.

[Gear-98] C. W. Gear. Multibody grouping from motion images.

International Journal of Computer

Vision, 29(2):133–150, 1998.

[Giblin-87] P. Giblin and R. Weiss. Reconstruction of surfaces from proﬁles. In Proc. 1st International

Conference on Computer Vision, London, pages 136–144, London, 1987.

[Gill-78] P. E. Gill and W. Murray. Algorithms for the solution of the nonlinear least-squares problem.

SIAM J Num Anal, 15(5):977–992, 1978.

[Golub-89] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press,

Baltimore, MD, second edition, 1989.

[Gracie-68] G. Gracie. Analytical photogrammetry applied to single terrestrial photograph mensuration.

In XIth International Conference of Photogrammetry, Lausanne, Switzerland, July 1968.

[Gupta-97] R. Gupta and R. I. Hartley. Linear pushbroom cameras.

IEEE Transactions on Pattern

Analysis and Machine Intelligence, September 1997.

[Haralick-91] R. M. Haralick, C. Lee, K. Ottenberg, and M. N¨olle. Analysis and solutions of the three
point perspective pose estimation problem. In Proc. IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 592–598, 1991.

[Harris-88] C. J. Harris and M. Stephens. A combined corner and edge detector. In Proc. 4th Alvey Vision

Conference, Manchester, pages 147–151, 1988.

[Hartley-92a] R. I. Hartley. Estimation of relative camera positions for uncalibrated cameras. In Proc.

European Conference on Computer Vision, LNCS 588, pages 579–587. Springer-Verlag, 1992.

[Hartley-92b] R. I. Hartley. Invariants of points seen in multiple images. GE internal report, GE CRD,

Schenectady, NY 12301, USA, May 1992.

[Hartley-92c] R. I. Hartley, R. Gupta, and T. Chang. Stereo from uncalibrated cameras. In Proc. IEEE

Conference on Computer Vision and Pattern Recognition, 1992.

[Hartley-94a] R. I. Hartley. Self-calibration from multiple views with a rotating camera. In Proc. Euro-

pean Conference on Computer Vision, LNCS 800/801, pages 471–478. Springer-Verlag, 1994.

[Hartley-94b] R. I. Hartley. Euclidean reconstruction from uncalibrated views.

In J. Mundy, A. Zis-
serman, and D. Forsyth, editors, Applications of Invariance in Computer Vision, LNCS 825, pages
237–256. Springer-Verlag, 1994.

[Hartley-94c] R. I. Hartley. Projective reconstruction and invariants from multiple images. IEEE Trans-

actions on Pattern Analysis and Machine Intelligence, 16:1036–1041, October 1994.

[Hartley-94d] R. I. Hartley. Projective reconstruction from line correspondence. In Proc. IEEE Confer-

ence on Computer Vision and Pattern Recognition, 1994.

[Hartley-95a] R. I. Hartley. Multilinear relationships between coordinates of corresponding image points
and lines. In Proceedings of the Sophus Lie Symposium, Nordfjordeid, Norway (not published yet),
1995.

[Hartley-95b] R. I. Hartley. A linear method for reconstruction from lines and points. In Proc. Interna-

tional Conference on Computer Vision, pages 882–887, 1995.

[Hartley-97a] R. I. Hartley. Lines and points in three views and the trifocal tensor. International Journal

638

Bibliography

of Computer Vision, 22(2):125–140, 1997.

[Hartley-97b] R. I. Hartley and P. Sturm. Triangulation. Computer Vision and Image Understanding,

68(2):146–157, November 1997.

[Hartley-97c] R. I. Hartley.

In defense of the eight-point algorithm.
Analysis and Machine Intelligence, 19(6):580 – 593, October 1997.

IEEE Transactions on Pattern

[Hartley-97d] R. I. Hartley. Kruppa’s equations derived from the fundamental matrix. IEEE Transactions

on Pattern Analysis and Machine Intelligence, 19(2):133–135, 1997.

[Hartley-97e] R. I. Hartley and T. Saxena. The cubic rational polynomial camera model. In Proc. DARPA

Image Understanding Workshop, pages 649 – 653, 1997.

[Hartley-98a] R. I. Hartley. Chirality. International Journal of Computer Vision, 26(1):41–61, 1998.
[Hartley-98b] R. I. Hartley. Dualizing scene reconstruction algorithms. In R. Koch and L. Van Gool,
editors, 3D Structure from Multiple Images of Large-Scale Environments, LNCS 1506, pages 14–31.
Springer-Verlag, June 1998.

[Hartley-98c] R. I. Hartley. Computation of the quadrifocal tensor. In Proc. European Conference on

Computer Vision, LNCS 1406, pages 20–35. Springer-Verlag, 1998.

[Hartley-98d] R. I. Hartley. Minimizing algebraic error in geometric estimation problems.

In Proc.

International Conference on Computer Vision, pages 469–476, 1998.

[Hartley-99] R. Hartley, L. de Agapito, E. Hayman, and I. Reid. Camera calibration and the search for
inﬁnity. In Proc. 7th International Conference on Computer Vision, Kerkyra, Greece, pages 510–517,
September 1999.

[Hartley-00a] R. I. Hartley and N. Y. Dano. Reconstruction from six-point sequences. In Proc. IEEE

Conference on Computer Vision and Pattern Recognition, pages II–480 – II–486, 2000.

[Hartley-00b] R. I. Hartley. Ambiguous conﬁgurations for 3-view projective reconstruction. In Proc.
6th European Conference on Computer Vision, Part I, LNCS 1842, Dublin, Ireland, pages 922–935,
2000.

[Hartley-02a] R. Hartley and F. Kahl. Critical curves and surfaces for euclidean reconstruction. In Proc.
7th European Conference on Computer Vision, Part II, LNCS 2351, Copenhagen, Denmark, pages
447–462, 2002.

[Hartley-02b] R. Hartley and R. Kaucic. Sensitivity of calibration to principal point position. In Proc.
7th European Conference on Computer Vision, Copenhagen, Denmark, volume 2, pages 433–446.
Springer-Verlag, 2002.

[Hartley-03] R. Hartley and F. Kahl. A critical conﬁguration for reconstruction from rectilinear motion.

In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2003.

[Hayman-03] E. Hayman, T. Th´orhallsson, and D.W. Murray. Tracking while zooming using afﬁne
International Journal of Computer Vision, 51(1):37–62, January

transfer and multifocal tensors.
2003.

[Heyden-95a] A. Heyden. Reconstruction from image sequences by means of relative depths. In E. Grim-
son, editor, Proc. 5th International Conference on Computer Vision, Boston, Cambridge, MA, June
1995.

[Heyden-95b] A. Heyden. Geometry and Algebra of Multiple Projective Transformations. PhD thesis,

Department of Mathematics, Lund University, Sweden, December 1995.

[Heyden-97a] A. Heyden. Projective structure and motion from image sequences using subspace meth-

ods. In Scandinavian Conference on Image Analysis, Lappenraanta, pages 963–968, 1997.

[Heyden-97b] A. Heyden and K. ˚Astr¨om. Euclidean reconstruction from image sequences with varying
and unknown focal length and principal point. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, 1997.

[Heyden-97c] A. Heyden. Reconstruction from multiple images by means of using relative depths. In-

ternational Journal of Computer Vision, 24(2):155–161, 1997.

[Heyden-98] A. Heyden. Algebraic varieties in multiple view geometry. In Proc. 5th European Confer-

ence on Computer Vision, Freiburg, Germany, pages 3–19, 1998.

[Hilbert-56] D. Hilbert and S. Cohn-Vossen. Geometry and the Imagination. Chelsea, NY, 1956.
[Horaud-98] R. Horaud and G. Csurka. Self-calibration and Euclidean reconstruction using motions of a

Bibliography

639

stereo rig. In Proc. 6th International Conference on Computer Vision, Bombay, India, pages 96–103,
January 1998.

[Horn-90] B. K. P. Horn. Relative orientation. International Journal of Computer Vision, 4:59–78, 1990.
[Horn-91] B. K. P. Horn. Relative orientation revisited. Journal of the Optical Society of America,

8(10):1630–1638, 1991.

[Horry-97] Y. Horry, K. Anjyo, and K. Arai. Tour into the picture: Using a spidery mesh interface
In Proceedings of the ACM SIGGRAPH Conference on

to make animation from a single image.
Computer Graphics, pages 225–232, 1997.

[Huang-89] T. S. Huang and O. D. Faugeras. Some properties of the E-matrix in two-view motion
estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11:1310 – 1312, 1989.

[Huber-81] P. J. Huber. Robust Statistics. John Wiley and Sons, 1981.
[Huynh-03] D.Q. Huynh, R. Hartley, and A Heyden. Outlier correction of image sequences for the afﬁne

camera. In Proc. 9th International Conference on Computer Vision, Vancouver, France, 2003.

[Irani-98] M. Irani, P. Anandan, and D. Weinshall. From reference frames to reference planes: Multi-
view parallax geometry and applications. In Proc. European Conference on Computer Vision, 1998.
[Irani-99] M. Irani. Multi-frame optical ﬂow estimation using subspace contraints. In Proc. International

Conference on Computer Vision, 1999.

[Irani-00] Michal Irani and P. Anandan. Factorization with uncertainty. In Proc. 6th European Confer-

ence on Computer Vision, Part I, LNCS 1842, Dublin, Ireland, pages 539 – 553, 2000.

[Jiang-02] G. Jiang, H. Tsui, L. Quan, and A. Zisserman. Single axis geometry by ﬁtting conics. In Proc.
7th European Conference on Computer Vision, Copenhagen, Denmark, volume 1, pages 537–550.
Springer-Verlag, 2002.

[Kahl-98a] F. Kahl and A. Heyden. Structure and motion from points, lines and conics with afﬁne
cameras. In Proc. 5th European Conference on Computer Vision, Freiburg, Germany, pages 327–
341, 1998.

[Kahl-98b] F. Kahl and A. Heyden. Using conic correspondences in two images to estimate epipolar
geometry. In Proc. 6th International Conference on Computer Vision, Bombay, India, pages 761–
766, 1998.

[Kahl-99] F. Kahl. Critical motions and ambiguous euclidean reconstructions in auto-calibration.
Proc. 7th International Conference on Computer Vision, Kerkyra, Greece, pages 469–475, 1999.

[Kahl-01a] F. Kahl, R. Hartley, and K. ˚Astr¨om. Critical conﬁgurations for n-view projective reconstruc-
tion. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages II–158 – II–163,
2001.

In

[Kahl-01b] F. Kahl. Geometry and Critical Conﬁgurations of Multiple Views. PhD thesis, Lund Institute

of Technology, 2001.

[Kanatani-92] K. Kanatani. Geometric computation for machine vision. Oxford University Press, Ox-

ford, 1992.

[Kanatani-94] K. Kanatani. Statistical bias of conic ﬁtting and renormalization. IEEE Transactions on

Pattern Analysis and Machine Intelligence, 16(3):320–326, 1994.

[Kanatani-96] K. Kanatani. Statistical Optimization for Geometric Computation: Theory and Practice.

Elsevier Science, Amsterdam, 1996.

[Kaucic-01] R. Kaucic, R. I. Hartley, and N. Y. Dano. Plane-based projective reconstruction. In Proc.

8th International Conference on Computer Vision, Vancouver, Canada, pages I–420–427, 2001.

[Klein-39] F. Klein. Elementary Mathematics from an Advanced Standpoint. Macmillan, New York,

1939.

[Knight-03] J. Knight, A. Zisserman, and I. Reid. Linear auto-calibration for ground plane motion. In

Proc. IEEE Conference on Computer Vision and Pattern Recognition, June 2003.

[Koenderink-84] J. J. Koenderink. What does the occluding contour tell us about solid shape? Perception,

13:321–330, 1984.

[Koenderink-90] J. Koenderink. Solid Shape. MIT Press, 1990.
[Koenderink-91] J. J. Koenderink and A. J. van Doorn. Afﬁne structure from motion. Journal of the

Optical Society of America, 8(2):377–385, 1991.

640

Bibliography

[Krames-42] J. Krames.

¨Uber die bei der Hauptaufgabe der Luftphotogrammetrie auftretenden
“gef¨ahrlichen” Fl¨achen. Bildmessung und Luftbildwesen (Beilage zur Allg. Vermessungs-Nachr.),
17, Heft 1/2:1–18, 1942.

[Kriegman-98] D. J. Kriegman and P. Belhumeur. What shadows reveal about object structure. In Proc.

European Conference on Computer Vision, pages 399–414, 1998.

[Laveau-96a] S. Laveau. G´eom´etrie d’un syst`eme de N cam´eras. Th´eorie, estimation et applications.

PhD thesis, INRIA, 1996.

[Laveau-96b] S. Laveau and O. D. Faugeras. Oriented projective geometry in computer vision. In Proc.
4th European Conference on Computer Vision, LNCS 1065, Cambridge, pages 147–156, Springer–
Verlag, 1996. Buxton B. and Cipolla R.

[Liebowitz-98] D. Liebowitz and A. Zisserman. Metric rectiﬁcation for perspective images of planes. In

Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 482–488, June 1998.

[Liebowitz-99a] D. Liebowitz, A. Criminisi, and A. Zisserman. Creating architectural models from im-

ages. In Proc. EuroGraphics, volume 18, pages 39–50, September 1999.

[Liebowitz-99b] D. Liebowitz and A. Zisserman. Combining scene and auto-calibration constraints. In

Proc. 7th International Conference on Computer Vision, Kerkyra, Greece, September 1999.

[Liebowitz-01] D. Liebowitz. Camera Calibration and Reconstruction of Geometry from Images. PhD

thesis, University of Oxford, Dept. Engineering Science, June 2001. D.Phil. thesis.

[LonguetHiggins-81] H. C. Longuet-Higgins. A computer algorithm for reconstructing a scene from two

projections. Nature, 293:133–135, September 1981.

[Luong-92] Q. Luong. Matrice Fondamentale et Autocalibration en Vision par Ordinateur. PhD thesis,

Universit´e de Paris-Sud, France, 1992.

[Luong-94] Q. T. Luong and T. Vi´eville. Canonic representations for the geometries of multiple projec-
tive views. In Proc. 3rd European Conference on Computer Vision, Stockholm, pages 589–599, May
1994.

[Luong-96] Q. T. Luong and T. Vi´eville. Canonical representations for the geometries of multiple pro-

jective views. Computer Vision and Image Understanding, 64(2):193–229, September 1996.

[Lutkepohl-96] H. Lutkepohl. Handbook of Matrices. Wiley, ISBN 0471970158, 1996.
[Ma-99] Y. Ma, S. Soatto, J. Kosecka, and S. Sastry. Euclidean reconstruction and reprojection up to
subgroups. In Proc. 7th International Conference on Computer Vision, Kerkyra, Greece, pages 773–
780, 1999.

[Mathematica-92] S. Wolfram. Mathematica A System for Doing Mathematics by Computer second

edition. Addison-Wesley, 1992.

[Maybank-90] S. J. Maybank. The projective geometry of ambiguous surfaces. Philosophical Transac-

tions of the Royal Society of London, SERIES A, A 332:1–47, 1990.

[Maybank-93] S. J. Maybank. Theory of reconstruction from image motion. Springer-Verlag, Berlin,

1993.

[Maybank-98] S. J. Maybank and A. Shashua. Ambiguity in reconstruction from images of six points.

In Proc. 6th International Conference on Computer Vision, Bombay, India, pages 703–708, 1998.

[McLauchlan-00] P. F. McLauchlan. Gauge independence in optimization algorithms for 3D vision. In
W. Triggs, A. Zisserman, and R. Szeliski, editors, Vision Algorithms: Theory and Practice, volume
1883 of LNCS, pages 183–199. Springer, 2000.

[Mohr-92] R. Mohr. Projective geometry and computer vision. In C. H. Chen, L. F. Pau, and P. S. P.

Wang, editors, Handbook of Pattern Recognition and Computer Vision. World Scientiﬁc, 1992.

[Mohr-93] R. Mohr, F. Veillon, and L. Quan. Relative 3D reconstruction using multiple uncalibrated
images. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 543–548,
1993.

[Moons-94] T. Moons, L. Van Gool, M. Van Diest, and E. Pauwels. Afﬁne reconstruction from perspec-
tive image pairs. In J. Mundy, A. Zisserman, and D. Forsyth, editors, Applications of Invariance in
Computer Vision, LNCS 825. Springer-Verlag, 1994.

[Muehlich-98] M. M¨uhlich and R. Mester. The role of total least squares in motion analysis. In Proc.
5th European Conference on Computer Vision, Freiburg, Germany, pages 305–321. Springer-Verlag,

Bibliography

641

1998.

[Mundy-92] J. Mundy and A. Zisserman. Geometric Invariance in Computer Vision. MIT Press, 1992.
[Newsam-96] G. Newsam, D. Q. Huynh, M. Brooks, and H. P. Pan. Recovering unknown focal lengths
in self-calibration: An essentially linear algorithm and degenerate conﬁgurations. In Int. Arch. Pho-
togrammetry & Remote Sensing, volume XXXI-B3, pages 575–80, Vienna, 1996.

[Niem-94] W. Niem and R. Buschmann. Automatic modelling of 3D natural objects from multiple views.
In European Workshop on Combined Real and Synthetic Image Processing for Broadcast and Video
Production, Hamburg, Germany, 1994.

[Nister-00] D. Nister. Reconstruction from uncalibrated sequences with a hierarchy of trifocal tensors.

In Proc. European Conference on Computer Vision, 2000.

[Oskarsson-02] M. Oskarsson, A. Zisserman, and K. ˚Astr¨om. Minimal projective reconstruction for
combinations of points and lines in three views. In Proc. British Machine Vision Conference, pages
62–72, 2002.

[Poelman-94] C. Poelman and T. Kanade. A paraperspective factorization method for shape and motion
recovery. In Proc. 3rd European Conference on Computer Vision, Stockholm, volume 2, pages 97–
108, 1994.

[Pollefeys-96] M. Pollefeys, L. Van Gool, and A. Oosterlinck. The modulus constraint: a new constraint

for self-calibration. In Proc. International Conference on Pattern Recognition, pages 31–42, 1996.

[Pollefeys-98] M. Pollefeys, R. Koch, and L. Van Gool. Self calibration and metric reconstruction in
spite of varying and unknown internal camera parameters. In Proc. 6th International Conference on
Computer Vision, Bombay, India, pages 90–96, 1998.

[Pollefeys-99a] M. Pollefeys, R. Koch, and L. Van Gool. A simple and efﬁcient rectiﬁcation method for

general motion. In Proc. International Conference on Computer Vision, pages 496–501, 1999.

[Pollefeys-99b] M. Pollefeys. Self-calibration and metric 3D reconstruction from uncalibrated image

sequences. PhD thesis, ESAT-PSI, K.U.Leuven, 1999.

[Pollefeys-02] M. Pollefeys, F. Verbiest, and L. J. Van Gool. Surviving dominant planes in uncalibrated

structure and motion recovery. In ECCV (2), pages 837–851, 2002.

[Ponce-94] J. Ponce, D. H. Marimont, and T. A. Cass. Analytical methods for uncalibrated stereo and
motion measurement. In Proc. 3rd European Conference on Computer Vision, Stockholm, volume 1,
pages 463–470, 1994.

[Porrill-91] J. Porrill and S. B. Pollard. Curve matching and stereo calibration. Image and Vision Com-

puting, 9(1):45–50, 1991.

[Pratt-87] V. Pratt. Direct least-squares ﬁtting of algebraic surfaces. Computer Graphics, 21(4):145–151,

1987.

[Press-88] W. Press, B. Flannery, S. Teukolsky, and W. Vetterling. Numerical Recipes in C. Cambridge

University Press, 1988.

[Pritchett-98] P. Pritchett and A. Zisserman. Wide baseline stereo matching. In Proc. 6th International

Conference on Computer Vision, Bombay, India, pages 754–760, January 1998.

[Proesmans-98] M. Proesmans, T. Tuytelaars, and L. J. Van Gool. Monocular image measurements.

Technical Report Improofs-M12T21/1/P, K.U.Leuven, 1998.

[Quan-94] L. Quan. Invariants of 6 points from 3 uncalibrated images. In J. O. Eckland, editor, Proc.

3rd European Conference on Computer Vision, Stockholm, pages 459–469. Springer-Verlag, 1994.

[Quan-97a] L. Quan and T. Kanade. Afﬁne structure from line correspondences with uncalibrated afﬁne
cameras. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(8):834–845, August
1997.

IEEE Conference on Computer Vision and Pattern Recognition, pages 60–65, 1997.

[Quan-97b] L. Quan. Uncalibrated 1D projective camera and 3D afﬁne reconstruction of lines. In Proc.
[Quan-98] L. Quan and Z. Lan. Linear n ≥ 4-point pose determination.
Conference on Computer Vision, Bombay, India, pages 778–783, 1998.

In Proc. 6th International

[Reid-96] I. D. Reid and D. W. Murray. Active tracking of foveated feature clusters using afﬁne structure.

International Journal of Computer Vision, 18(1):41–60, 1996.

[Rinner-72] K. Rinner and R. Burkhardt. Photogrammetrie. In Handbuch der Vermessungskunde, volume

642

Bibliography

Band III a/3. Jordan, Eggert, Kneissel, Stuttgart: J.B. Metzlersche Verlagsbuchhandlung, 1972.

[Robert-93] L. Robert and O. D. Faugeras. Relative 3D positioning and 3D convex hull computation
In Proc. 4th International Conference on Computer Vision,

from a weakly calibrated stereo pair.
Berlin, pages 540–544, 1993.

[Rother-01] C. Rother and S. Carlsson. Linear multi view reconstruction and camera recovery. In Proc.

8th International Conference on Computer Vision, Vancouver, Canada, pages I–42–49, 2001.

[Rother-03] C. Rother. Multi-View Reconstruction and Camera Recovery using a Real or Virtual Refer-
ence Plane. PhD thesis, Computational Vision and Active Perception Laboratory, Kungl Tekniska
H¨ogskolan, 2003.

[Rousseeuw-87] P. J. Rousseeuw. Robust Regression and Outlier Detection. Wiley, New York, 1987.
[Sampson-82] P. D. Sampson. Fitting conic sections to ‘very scattered’ data: An iterative reﬁnement of

the Bookstein algorithm. Computer Vision, Graphics, and Image Processing, 18:97–108, 1982.

[Sawhney-98] H. S. Sawhney, S. Hsu, and R. Kumar. Robust video mosaicing through topology inference
and local to global alignment. In Proc. European Conference on Computer Vision, pages 103–119.
Springer-Verlag, 1998.

[Schaffalitzky-99] F. Schaffalitzky and A. Zisserman. Geometric grouping of repeated elements within
In D.A. Forsyth, J.L. Mundy, V. Di Gesu, and R. Cipolla, editors, Shape, Contour and

images.
Grouping in Computer Vision, LNCS 1681, pages 165–181. Springer-Verlag, 1999.

[Schaffalitzky-00a] F. Schaffalitzky. Direct solution of modulus constraints. In Proceedings of the Indian
Conference on Computer Vision, Graphics and Image Processing, Bangalore, pages 314–321, 2000.
[Schaffalitzky-00b] F. Schaffalitzky and A. Zisserman. Planar grouping for automatic detection of van-

ishing lines and points. Image and Vision Computing, 18:647–658, 2000.

[Schaffalitzky-00c] F. Schaffalitzky, A. Zisserman, R. I. Hartley, and P. H. S. Torr. A six point solution for
structure and motion. In Proc. European Conference on Computer Vision, pages 632–648. Springer-
Verlag, June 2000.

[Schmid-97] C. Schmid and A. Zisserman. Automatic line matching across views. In Proc. IEEE Con-

ference on Computer Vision and Pattern Recognition, pages 666–671, 1997.

[Schmid-98] C. Schmid and A. Zisserman. The geometry and matching of curves in multiple views. In

Proc. European Conference on Computer Vision, pages 394–409. Springer-Verlag, June 1998.

[Se-00] S. Se. Zebra-crossing detection for the partially sighted. In Proc. IEEE Conference on Computer

Vision and Pattern Recognition, pages 211–217, 2000.

[Semple-79] J. G. Semple and G. T. Kneebone. Algebraic Projective Geometry. Oxford University Press,

1979.

[Shapiro-95] L. S. Shapiro, A. Zisserman, and M. Brady. 3D motion recovery via afﬁne epipolar geom-

etry. International Journal of Computer Vision, 16(2):147–182, 1995.

[Shashua-94] A. Shashua. Trilinearity in visual recognition by alignment. In Proc. 3rd European Con-

ference on Computer Vision, Stockholm, volume 1, pages 479–484, May 1994.

[Shashua-95a] A. Shashua. Algebraic functions for recognition. IEEE Transactions on Pattern Analysis

and Machine Intelligence, 17(8):779–789, August 1995.

[Shashua-95b] A. Shashua and M. Werman. On the trilinear tensor of three perspective views and its

underlying geometry. In Proc. 5th International Conference on Computer Vision, Boston, 1995.

[Shashua-96] A. Shashua and S. J. Maybank. Degenerate N-point conﬁgurations of three views: Do
critical surfaces exist? Technical Report TR 96-19, Hebrew University, Computer Science, November
1996.

[Shashua-97] A. Shashua and S. Toelg. The quadric reference surface: Theory and applications. Inter-

national Journal of Computer Vision, 23(2):185–198, 1997.

[Shimshoni-99] I. Shimshoni, R. Basri, and E. Rivlin. A geometric interpretation of weak-perspective

motion. Technical report, Technion, 1999.

[Sinclair-92] D. A. Sinclair. Experiments in Motion and Correspondence. PhD thesis, University of

Oxford, 1992.

[Slama-80] C. Slama. Manual of Photogrammetry. American Society of Photogrammetry, Falls Church,

VA, USA, 4th edition, 1980.

Bibliography

643

[Spetsakis-91] M. E. Spetsakis and J. Aloimonos. A multi-frame approach to visual motion perception.

International Journal of Computer Vision, 16(3):245–255, 1991.

[Springer-64] C. E. Springer. Geometry and Analysis of Projective Spaces. Freeman, 1964.
[Stein-99] G. Stein and A. Shashua. On degeneracy of linear reconstruction from three views: Linear
IEEE Transactions on Pattern Analysis and Machine Intelligence,

line complex and applications.
21(3):244–251, 1999.

[Stolﬁ-91] J. Stolﬁ. Oriented Projective Geometry. Academic Press, 1991.
[Strecha-02] C.Strecha and L. Van Gool. PDE-based multi-view depth estimation. 1st Int. Symp. of 3D

Data Processing Visualization and Transmission, pages 416–425, 2002.

[Sturm-96] P. Sturm and W. Triggs. A factorization based algorithm for multi-image projective structure
and motion. In Proc. 4th European Conference on Computer Vision, Cambridge, pages 709–720,
1996.

[Sturm-97a] P. Sturm. Critical motion sequences for monocular self-calibration and uncalibrated Eu-
In Proc. IEEE Conference on Computer Vision and Pattern Recognition,

clidean reconstruction.
Puerto Rico, pages 1100–1105, June 1997.

[Sturm-97b] P. Sturm. Vision 3D non calibr´ee: Contributions `a la reconstruction projective et ´etude des

mouvements critiques pour l’auto calibrage. PhD thesis, INRIA Rhˆone-Alpes, 1997.

[Sturm-99a] P. Sturm and S. J. Maybank. A method for interactive 3D reconstruction of piecewise planar

objects from single images. In Proc. 10th British Machine Vision Conference, Nottingham, 1999.

[Sturm-99b] P. Sturm. Critical motion sequences for the self-calibration of cameras and stereo systems
In Proc. 10th British Machine Vision Conference, Nottingham, pages

with variable focal length.
63–72, 1999.

[Sturm-99c] P. Sturm and S. Maybank. On plane based camera calibration: A general algorithm, singu-
larities, applications. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages
432–437, June 1999.

[Sturm-01] P. Sturm. On focal length calibration from two views. In Proc. IEEE Conference on Computer

Vision and Pattern Recognition, pages 145–150, 2001.

[Sutherland-63] I. E. Sutherland. Sketchpad: A man-machine graphical communications system. Techni-
cal Report 296, MIT Lincoln Laboratories, 1963. Also published by Garland Publishing, New York,
1980.

[Szeliski-96] R. Szeliski and S. B. Kang. Shape ambiguities in structure from motion. In B. Buxton and
Cipolla R., editors, Proc. 4th European Conference on Computer Vision, LNCS 1064, Cambridge,
pages 709–721. Springer–Verlag, 1996.

[Szeliski-97] R. Szeliski and S. Heung-Yeung. Creating full view panoramic image mosaics and envi-

ronment maps. In Proceedings of the ACM SIGGRAPH Conference on Computer Graphics, 1997.

[Taubin-91] G. Taubin. Estimation of planar curves, surfaces, and nonplanar space curves deﬁned by
implicit equations with applications to edge and range image segmentation. PAMI, 13(11):1115–
1138, 1991.

[Thorhallsson-99] T. Thorhallsson and D.W. Murray. The tensors of three afﬁne views. In Proc. IEEE

Conference on Computer Vision and Pattern Recognition, 1999.

[Tomasi-92] C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: A

factorization approach. International Journal of Computer Vision, 9(2):137–154, November 1992.

[Tordoff-01] B. Tordoff and D.W. Murray. Reactive zoom control while tracking using an afﬁne camera.

In Proc. British Machine Vision Conference, volume 1, pages 53–62, 2001.

[Torr-93] P. H. S. Torr and D. W. Murray. Outlier detection and motion segmentation. In Proc SPIE

Sensor Fusion VI, pages 432–443, Boston, September 1993.

[Torr-95a] P. H. S. Torr, A. Zisserman, and D. W. Murray. Motion clustering using the trilinear constraint
over three views. In R. Mohr and C. Wu, editors, Europe–China Workshop on Geometrical Modelling
and Invariants for Computer Vision, pages 118–125. Xidan University Press, 1995.

[Torr-95b] P. H. S. Torr. Motion segmentation and outlier detection. PhD thesis, Dept. of Engineering

Science, University of Oxford, 1995.

[Torr-97] P. H. S. Torr and A. Zisserman. Robust parameterization and computation of the trifocal tensor.

644

Bibliography

Image and Vision Computing, 15:591–605, 1997.

[Torr-98] P. H. S. Torr and A. Zisserman. Robust computation and parameterization of multiple view
relations. In Proc. 6th International Conference on Computer Vision, Bombay, India, pages 727–732,
January 1998.

[Torr-99] P. H. S. Torr, A. W. Fitzgibbon, and A. Zisserman. The problem of degeneracy in structure
and motion recovery from uncalibrated image sequences. International Journal of Computer Vision,
32(1):27–44, August 1999.

[Torresani-01] L. Torresani, D. Yang, G. Alexander, and C. Bregler. Tracking and modelling non-rigid
objects with rank constraints. In Proc. IEEE Conference on Computer Vision and Pattern Recognition,
pages I: 493–500, 2001.

[Triggs-95] W. Triggs. The geometry of projective reconstruction i: Matching constraints and the joint

image. In Proc. International Conference on Computer Vision, pages 338–343, 1995.

[Triggs-96] W. Triggs. Factorization methods for projective structure and motion. In Proc. IEEE Con-

ference on Computer Vision and Pattern Recognition, pages 845–851, 1996.

[Triggs-97] W. Triggs. Auto-calibration and the absolute quadric. In Proc. IEEE Conference on Com-

puter Vision and Pattern Recognition, pages 609–614, 1997.

[Triggs-98] W. Triggs. Autocalibration from planar scenes. In Proc. 5th European Conference on Com-

puter Vision, Freiburg, Germany, 1998.

[Triggs-99a] W. Triggs. Camera pose and calibration from 4 or 5 known 3D points. In Proc. International

Conference on Computer Vision, pages 278–284, 1999.

[Triggs-99b] W. Triggs. Differential matching constraints. In Proc. International Conference on Com-

puter Vision, pages 370–376, 1999.

[Triggs-00a] W. Triggs, P. F. McLauchlan, R. I. Hartley, and A. Fitzgibbon. Bundle adjustment for

structure from motion. In Vision Algorithms: Theory and Practice. Springer-Verlag, 2000.

[Triggs-00b] W Triggs. Plane + parallax, tensors and factorization. In Proc. European Conference on

Computer Vision, pages 522–538, 2000.

[Tsai-84] R. Y. Tsai and T. S. Huang. The perspective view of three points. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 6:13–27, 1984.

[VanGool-98] L. Van Gool, M. Proesmans, and A. Zisserman. Planar homologies as a basis for grouping

and recognition. Image and Vision Computing, 16:21–26, January 1998.

[Vieville-93] T. Vi´eville and Q. Luong. Motion of points and lines in the uncalibrated case. Technical

Report 2054, I.N.R.I.A., 1993.

[Vieville-95] T. Vi´eville and D. Lingrand. Using singular displacements for uncalibrated monocular

vision systems. Technical Report 2678, I.N.R.I.A., 1995.

[VonSanden-08] H. von Sanden. Die Bestimmung der Kernpunkte in der Photogrammetrie. PhD thesis,

Univ. G¨ottingen, December 1908.

[Weinshall-95] D. Weinshall, M. Werman, and A. Shashua. Shape descriptors: Bilinear, trilinear and
quadrilinear relations for multi-point geometry and linear projective reconstruction algorithms. In
IEEE Workshop on Representation of Visual Scenes, Boston, pages 58–65, 1995.

[Weng-88] J. Weng, N. Ahuja, and T. S. Huang. Closed-form solution and maximum likelihood : A
robust approach to motion and structure estimation. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition, 1988.

[Weng-89] J. Weng, T. S. Huang, and N. Ahuja. Motion and structure from two perspective views:
algorithms, error analysis and error estimation. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 11(5):451–476, 1989.

[Werner-01] T. Werner and T. Pajdla. Oriented matching constraints. In T Cootes and C Taylor, editors,
Proc. British Machine Vision Conference, pages 441–450, London, UK, September 2001. British
Machine Vision Association.

[Werner-03] T. Werner. A constraint on ﬁve points in two images. In Proc. IEEE Conference on Computer

Vision and Pattern Recognition, June 2003.

[Wolfe-91] W. J. Wolfe, D. Mathis, C. Weber Sklair, and M. Magee. The perspective view of three points.

IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(1):66–73, January 1991.

Bibliography

645

[Xu-96] G. Xu and Z. Zhang. Epipolar Geometry in Stereo, Motion and Object Recognition. Kluwer

Academic Publishers, 1996.

[Zeller-96] C. Zeller. Projective, Afﬁne and Euclidean Calibration in Computer Vision and the Applica-
tion of Three Dimensional Perception. PhD thesis, RobotVis Group, INRIA Sophia-Antipolis, 1996.
[Zhang-95] Z. Zhang, R. Deriche, O. D. Faugeras, and Q. Luong. A robust technique for matching two
uncalibrated images through the recovery of the unknown epipolar geometry. Artiﬁcial Intelligence,
78:87–119, 1995.

[Zhang-98] Z. Zhang. Determining the epipolar geometry and its uncertainty – a review. International

Journal of Computer Vision, 27(2):161–195, March 1998.

[Zhang-00] Z. Zhang. A ﬂexible new technique for camera calibration. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 22(11):1330–1334, November 2000.

[Zisserman-92] A. Zisserman. Notes on geometric invariance in vision. Tutorial, British Machine Vision

Conference, 1992.

[Zisserman-94] A. Zisserman and S. Maybank. A case against epipolar geometry. In J. Mundy, A. Zis-
serman, and D. Forsyth, editors, Applications of Invariance in Computer Vision LNCS 825. Springer-
Verlag, 1994.

[Zisserman-95a] A. Zisserman, J. Mundy, D. Forsyth, J. Liu, N. Pillow, C. Rothwell, and S. Utcke.
Class-based grouping in perspective images. In Proc. International Conference on Computer Vision,
1995.

[Zisserman-95b] A. Zisserman, P. Beardsley, and I. Reid. Metric calibration of a stereo rig. In IEEE

Workshop on Representation of Visual Scenes, Boston, pages 93–100, 1995.

[Zisserman-96] A. Zisserman. A users guide to the trifocal tensor. Dept. of Engineering Science, Uni-

versity of Oxford, 1996.

[Zisserman-98] A. Zisserman, D. Liebowitz, and M. Armstrong. Resolving ambiguities in auto-
calibration. Philosophical Transactions of the Royal Society of London, SERIES A, 356(1740):1193–
1211, 1998.

Index

χ2 566

A
absolute conic 5–6, 9, 16–18, 78, 80–84, 195, 209–210, 224,

272–277, 340, 458, 461, 464, 471, 475, 485, 487, 500

relation to orthogonality 83

absolute dual quadric 83–85, 458, 462, 466–467, 469,

472–473, 475, 477

absolute dual quadric 83–84, 463
absolute dual quadric

extracting the rectifying transform 463
properties 462
simple properties 462

adjoint matrix 31, 581
afﬁne camera 14, 166, 169, 172–174, 176, 184–185, 271, 344,

347, 355, 413, 425, 432, 436–437, 440–441, 520

afﬁne camera

counting arguments 425
decomposition 169
deﬁnition 166
deﬁnition 166
error in employing 168
estimation of 184
hierarchy of 170
more properties 172

afﬁne fundamental matrix 298, 308, 345, 347–349, 351–353,

360–361, 390, 413, 439

afﬁne fundamental matrix
algebraic derivation 347
derivation 345
geometric derivation 345
geometric interpretation 349
Gold Standard algorithm 349
linear algorithm 348
minimal conﬁguration 352
properties 347
singularity constraint 349

afﬁne geometry. 3
afﬁne imaging 169
afﬁne imaging conditions 346
afﬁne imaging model 169
afﬁne matrix 40, 98, 347
afﬁne matrix 62
afﬁne multiple view tensors

computation 439

afﬁne properties 43, 49–50, 55, 62, 79–81, 220, 223
afﬁne properties of reconstruction 81
afﬁne properties

recovery from image 49

afﬁne reconstruction 10, 261, 266, 268–272, 274, 277, 339,
351, 355–356, 360, 434, 436–437, 439–440, 456, 459,

473, 475–476, 479, 487, 489, 496, 498, 515, 521, 525,
528

afﬁne reconstruction by alternation 440
afﬁne transformation 39
algebraic distance 93–94, 96–97, 186, 279, 308
algebraic distance

comparison with geometric distance 95

algebraic error

minimizing 186

algebraic minimization 107, 289, 400
algebraic solution
limitations 468

ambiguities

auto-calibration 478

ambiguity 480
ambiguity

afﬁne 438–439, 442, 444
auto-calibration 473
bas-relief 356–359, 633
examples 543
for 2D cameras 535
for 3D cameras 536
for three views 554
Necker reversal 356
of auto-calibration 473, 479–480
of auto-calibration

for a sequence 480

projective 265, 387
single view 549
two views 552

angle

between two lines in image 215
between two planes 83, 85, 218, 236
between two rays 9, 195, 209–210, 213, 215, 223, 265
between two scene lines 215
of rotation 204–205, 628

angle-axis parametrization 584–585, 624–625
augmentation 601
auto-calibration – see also reconstruction metric, 18, 58, 86,
205, 210, 253, 340, 439, 453, 456, 458–459, 461–465,
469, 473–474, 476–479, 481, 483, 485–486, 490,
496–500

auto-calibration 274
auto-calibration

ambiguities 478
critical motion sequences 497
ﬁnding plane at inﬁnity 473
from planar motion 496
implementation 485
iterative methods 467
limitations of absolute dual quadric method 468

646

metric from afﬁne 475
numbers of views needed 467
of stereo rig 494
problems if Q∗∞ not positive-deﬁnite 469
removing ambiguity 479
removing the ambiguity 479
solving for Q∗∞
linear 464
non-linear 466
solving forQ∗∞

non-linear 466

typical ambiguities 480
using Q∗∞ 463
varying parameters 477

auto-epipolar 249, 260
axis planes of camera 160, 199

B
back-projection
of conics 199
of lines 197
of points to rays 161

banded data 614
bas-relief ambiguity 356
bias 109, 150, 184, 302, 321, 568, 570–577

dependency on parametrization 571
lessons 572

bundle adjustment 14, 397, 434–435, 444, 452–453, 461, 467,

498, 611

bundle adjustment 434
bundle adjustment

banded structure 614
incremental 453
initial solution 435
sparse methods 435
with missing data 611

C
calibrating conic 195
orthogonality 231

calibration constraints 477
calibration constraints

known internal parameters 273
same camera in all images 273

calibration matrix 157, 163–165, 167–168, 170, 190, 204, 209,

212, 215, 224, 226, 231, 257, 264, 273, 275, 459–460,
463–464, 469, 485, 489

calibration

internal 176, 185, 210, 220, 233, 239, 355, 486, 489–490
what does it give? 208

camera anatomy 158
camera calibration 6, 17, 21, 155, 185, 191, 195, 208, 212,
216, 219, 222, 224, 228–230, 233, 235, 262, 268, 278,
293, 340, 355, 444, 458–459, 473, 482, 511, 595

camera centre 163
camera centre

ﬁnding it 163
moving 207
camera matrices

compatible with two fundamental matrices 386
computation from F 263
computed from three fundamental matrices 386
deﬁned by three fundamental matrices

uniqueness 385

camera matrix

canonical form 254
computation 263
decomposition 163
in projective space 165
reduced – see reduced camera matrix

Index

647

camera motion

recovering it 444

camera orientation 156, 205, 210, 623

ﬁnding it 163

camera parameters 147, 156, 164, 170, 182, 185–188, 193,

232, 399, 435, 444, 458, 467, 602, 610–611, 627

camera resectioning

covariance 188
degenerate conﬁgurations 179
from ﬁve points 549
minimal solution 179
over-determined solution 179

camera rotation 155
camera translation 204, 249–250
camera translation 155
camera

afﬁne – see afﬁne camera
afﬁne 172
at inﬁnity 173
basic pinhole model 153
calibrated 193, 209, 231, 234
CCD 156
centre 158
ﬁnite 170
ﬁnite projective 157
general projective 157
known orientation 448
line 175
pinhole model 153
projective 7, 153, 157–159, 161, 165–166, 173–176,
184–185, 193, 208, 213, 253, 361, 459, 509, 518

projective

action on points 161

pushbroom 174
resection ambiguity 550
rotation 195, 204, 207, 215, 250, 489
rotation and translation 155

cameras

as points 533

Carlsson–Weinshall duality 505, 513, 546
Carlsson–Weinshall map 546–547, 549
Carlsson–Weinshall map

deﬁnition 546

caution 227
central projection 6–7, 34, 37, 153–154, 165, 175, 200, 632
central projection 154
change of coordinates 2, 106, 137, 299, 472, 566, 577
change of coordinates 566
Chasles’ theorem 535
cheiral inequalities

solving 526

Cholesky factorization 211, 225–226, 272–273, 275, 277, 469,

582

choosing the depths 445
circular points 5–6, 18, 25, 44, 48, 52–57, 61–63, 79, 82, 85,

211, 226, 229, 235, 341, 485–491, 495, 500–501, 628

circular points 52
cofactor matrix 581
collineation – see also projective transformation, 32–33, 65,

631

column vector 26, 143, 154, 158, 448, 578, 581, 603
column vector 159
compatibility conditions 385
computing a plane

from point and line correspondence 332
from three point correspondences 330

computing vanishing lines 218
concurrent lines 45–46
concurrent lines 45
condition number 108, 439

648

conic 30
conic ﬁtting 127, 229, 297
conic

afﬁne classiﬁcation 60
analog for fundamental matrix 97
classiﬁcation 59
deﬁned by 5 points 30
degenerate 32
dual – see also dual conic
dual 31
dual to circular points 53
projective normal form 59
projective normal form of 59
tangent lines 31
transformations of 36

conjugacy 59, 83, 212–213, 228–229, 494, 628
conjugacy relation 494
conjugate conﬁgurations – deﬁnition 540
consistency conditions 331
constraints

from internal parameters 273
from same amera 273
from scene orthogonality 273
contour 200–201, 234, 300, 322
contour generator 200–202, 233–234, 295
contour generator – deﬁnition 200
contour

apparent 200–202, 233–234, 236
occluding 200

convex hull 515
coordinate orientation 164
coordinates. 2
coplanarity constraint 276, 422
correlation 59, 124, 148, 242, 246, 388, 447, 455, 486
correlation – deﬁnition 59
correspondence

of curves and surfaces 295
of lines 294

correspondences

determining 124
corridor scene 289
corridor scene 302
cosine formula 209
cost function 88, 90, 93–95, 98–99, 101–104, 110–114,

121–123, 125, 129, 134, 187, 191, 284, 287–289, 291,
308, 310, 312, 314–315, 318–319, 349–351, 386,
397–399, 404, 435, 462, 467, 485, 496, 498, 513, 597,
599–602, 607, 616–624, 626

cost function 88, 102, 110, 112, 279, 285, 602, 619, 621
cost function

heuristic 618
Huber 617, 619–622
least squares 621
non-convex 619
performance 620
properties 619
robust 122
squared error 619
statistically based 618

cost functions

asymptotically linear 619
linear 100
robust 122
summary 620

counting argument 18, 43, 63, 255, 383, 387, 411, 425,

427–428, 451, 461
counting argument 427
covariance estimation 608
covariance estimation

camera resectioning 188

Index

covariance

backward propagation 141
backward propagation

over-parametrized case 142

estimation 188
forward propagation 139
matrix 103, 111, 129, 137, 139–146, 148, 150, 188–189,

282, 298–301, 565–566, 573, 576–577, 598, 603–607,
609–611, 623

of epipolar lines 301
use in point transfer 148

Cram´er-Rao lower bound 573
critical conﬁgurations 11, 179, 533–534, 538, 544, 554, 559

examples 544

critical motion sequences 497
critical set 537

for 2d camera

deﬁnition 536

for 2D cameras 535

critical surface

for three views 553

cross product 581
cross ratio 42, 44–46, 49, 51, 63, 534–535, 629–631
cross ratio 44

D
deﬁcient-rank systems. 589
degeneracy 92, 226, 228, 293, 295–296, 323, 342, 381–383,

499, 546, 559

degeneracy 225, 234, 295–296, 329–330, 352, 456, 498
degeneracy

of triangulation 323
transfer 383

degenerate conﬁguration

for resectioning 179
for transfer 382

degenerate conﬁgurations – see also camera resectioning,

transfer, homography 2D estimation

degenerate conﬁgurations 127, 179, 341, 352, 443, 468, 545,

559

degenerate conﬁgurations

2D homography 91

degenerate homographies 334
degrees of freedom (dof) 27
degrees of freedom (dof)
of trifocal tensor 368

degrees of freedom

trifocal 368

depth of points 162
derivative matrix

computation 145

determining the correct convergence of an algorithm 138
determining the intersection 27
DIAC – see dual image of the absolute conic
direct metric reconstruction using (cid:2) 275
direct motion estimation 450
direct solution for structure and translation 448
distance ratio 270
distance ratios

on a line 270

distortion correction 191
distortion function 191

choice 191
computing 191

distortion

correction 191

DLT – see also homography computation, transformation

invariance, triangulation

DLT 90–91, 93, 96, 104–110, 116, 127, 150, 179–184, 187,

193–194, 276, 312, 424, 510–511, 573, 587, 595

DLT algorithm

non-invariance 105
using line correspondences 180

dual absolute quadric

computing

assuming zero skew 466
limitations 468
with constrained parameters 466

equivalence to calibration 463
linear solution 465
specifying linear constraints 464

dual circular points 52
dual conic 31–32, 37, 53–55, 64, 475
dual conic 30
dual image of the absolute conic 464–466, 477
dual image of the absolute conic

deﬁnition 210

dual quadric 73
dual reconstruction algorithm 504–506
dual reconstruction algorithm 503
dual reconstruction algorithm

justiﬁcation 507

dual

of absolute conic – see absolute dual quadric
of circular points 53

duality 29–31, 66, 207, 456, 503, 507, 514, 546, 548–550, 552
duality 29
duality principle 548

E
eigenvalue extraction

Jacobi’s method 581

elation 43, 62, 341, 631–633
elliptic quartic 556–558, 560
epipolar distance 288
epipolar envelope 301–303
epipolar line – see also epipolar lines, 46, 208, 240–249,

251–253, 263, 269, 288, 291, 295, 298–299, 301–306,
308, 311, 315–318, 324, 328, 332, 334–335, 340,
344–348, 352–353, 358–360, 372–374, 380–385, 400,
406, 443, 470

epipolar line 347
epipolar line correspondence 251
epipolar line homography 246
epipolar line
afﬁne 344

epipolar lines 240–243, 245–247, 249, 280, 298, 300–303,
305, 307, 315–316, 328, 342, 345–347, 358–360, 371,
373–374, 380, 384–385, 392, 470

afﬁne cameras 344

epipolar transfer

degeneracies 380

epipoles 240–242, 244, 251–252, 260, 264–266, 280,

286–287, 289, 307, 317–318, 320, 322, 328, 347, 361,
366, 371, 373, 375, 380–382, 384–385, 395–396, 404,
413, 471, 487–490

epipoles

as parameters 286
as tensors 413
for afﬁne cameras 344
retrieval from trifocal tensor 395
retrieving 395

error

algebraic 88, 93–95, 98, 100, 106, 134, 179, 183–187, 229,

283–284, 288, 391, 395–396, 399–400, 467

algebraic

camera resectioning 186
geometric interpretation 183

geometric 98, 100–101, 103, 105, 107, 114, 130, 134,

181–182, 184, 186, 287, 318, 391, 398–399, 436, 438

Index

649

in both images 101, 103, 134, 137, 147
in one image 94, 102, 112–113, 133, 136, 145
minimizing algebraic 129, 186, 288, 290
minimizing geometric 106–107, 130, 181, 290, 400
reproduction 112
reprojection 95–97, 103, 112–113, 130, 291, 312, 314, 343,

355, 401, 406, 434–435, 437

residual 115–116, 133–139, 150, 188, 288, 290, 400, 513
transfer 94–96, 98, 342

essential matrix 257–262, 275, 294, 356

computation of 294
deﬁnition 257
extraction of cameras from 258
four solutions 259
properties 257

estimation error 135–139
estimation problem 568
estimator 109, 117, 122, 135–136, 145, 303, 310, 568–575,

577

Euclidean and afﬁne interpretations 165
euclidean geometry. 4
Euclidean transformation

ﬁxed points 495

extending the baseline 452
exterior orientation 187
exterior orientation of camera 187
extracting the fundamental matrices 374

F
factorization 14, 211, 258–259, 351, 355, 406, 434–437,

439–446, 449, 456, 463, 579, 583, 585, 615

non-rigid 442–443, 456

feature detection 192
Fisher information matrix 573, 577
ﬁxed points 487, 491
ﬁxed points of a Euclidean transformation 495
ﬁxed points

of Euclidean transformation 495

focal length 16, 21, 36, 157, 166–167, 169, 183–184,
189–190, 192–194, 203, 227–228, 231, 234, 265,
466–469, 472, 484, 486, 497, 499–500, 568

forward projection 197
forward projection 161
forward projection

of lines 196
of quadrics 201

Frobenius norm 108, 259, 280–281, 294, 467
fundamental matrix

afﬁne – see afﬁne fundamental matrix
algebraic derivation 243
algorithm results and recommendations 400
both epipoles as parameters 286
computation 263
computation from seven points
connection to quadrics 545

computation

from seven points 281
from two planes 337
Gold Standard algorithm 284
Maximum likelihood 284

condition for point correspondence 245
consistency conditions 331
covariance 610
deﬁnition 245
degeneracies

no translation 297
points on a plane 553
points on a ruled quadric 296
derivation from four planes 422
epipolar parametrization 286

650

Index

extraction of canonical cameras 255
for planar motion

computation of 14, 78, 86, 247, 250, 252–253, 260–261,

293, 341, 389, 457–458, 486–490, 495, 497, 499

form of 250, 252

for translation

computation of 247
form of 247

geometric derivation 242–243
geometric derivation 242
iterative estimation 283
over-parametrization 286
parametrization of 285
parametrization of

over-parametrization 286

projective ambiguity given F 254
projective invariance of 253
properties 245
reduced 433, 508–512
reduced

computation of 509

Sampson method for estimation 287, 398
singularity constraint 280
symmetric part 251

G
gauge freedom 623
Gaussian

distribution 102–103, 135, 137, 143–144, 299–301, 565,

569–570, 572, 598, 618

distribution

isotropic 134–136

error 100, 102, 111, 116, 122, 150, 314
noise 109, 132–133, 136, 145, 289, 301, 303, 315,

320–321, 399–400, 436, 571, 575, 616

general motion 204, 239, 249–250, 252–253, 293, 406,

467–469, 473, 491, 494, 496, 498–499

general position 31–32, 35, 44, 63, 66–67, 73, 76, 92–93, 131,

296, 326, 336, 341, 349, 352, 384, 393, 406, 411, 425,
533, 539, 545

general position 352
generator

of quadric 75

geometric distance 94
geometric error – see also error geometric

camera resectioning 186
invariance to coordinate transforms 106

geometric interpretation 63, 72, 98, 234, 461, 464, 476
geometric interpretation 334, 349
geometric interpretation

of estimation 101

geometric minimization 114, 187–188
geometric representation 80, 82–84, 213, 246, 250, 252–253,

463

Givens rotation 579
Gold Standard algorithm 88, 114–116, 130, 181–182, 185,
284–285, 287–289, 293, 303, 310, 344, 349, 351, 397,
399–401

Gold Standard algorithm

for afﬁne fundamental matrix 349
for afﬁne homography 130
for fundamental matrix 284
for homography 114
for trifocal tensor 396

gradient descent 599
ground truth 223, 228, 320, 360
groups of projectivities 39
guided matching 125, 401

H

Hessian 599–600
hierarchical merging 453
homogeneity. 2
homogeneous representation 27, 30, 33, 65–66, 159, 163, 175,

446, 516, 518

of lines 26
of points 27

homogeneous scaling 115, 224
homogeneous vector 2, 27–28, 51, 65, 88, 90, 144, 154, 249,

317, 447, 623, 625

homographies

what do they say about the camera matrices? 448

homography estimation

approximate solution 88
conic analogue 97
degenerate conﬁgurations 91
DLT 90
error in both images 134, 147
error in one image 94, 102, 112–113, 133, 145
errors in world points 182
experimental evaluation 187
from noisy points 332
function speciﬁcation 111
Gold Standard algorithm 88
inhomogeneous solution 90
initialization 14, 110, 113–114, 400, 435, 621
iterative methods 114
linear cost function 100
over-determined solution 90
parametrization 110
symmetric transfer error 94, 112
using RANSAC 124

homography

2D 91, 93, 99–101, 105, 109, 114, 123, 129, 133, 142, 145,

148, 180–181, 183, 188, 243, 270, 297, 602, 607,
609–610
3D 423, 508
compatible with epipolar geometry 327
degenerate 334
induced by a plane 368

homology

parametrization of 630
planar harmonic 631

horopter 77, 251–253, 343, 487–488, 550–551
horopter 550
Householder matrix 128, 147, 268, 431, 580, 625–626
Huber cost function 617, 619–622
hyperboloid – see quadric, 74–75, 86, 296, 543–545, 548,

552–553, 559

I
IAC – see image of the absolute conic
IAC

advantage over DIAC 477

ideal point 28
image of the absolute conic 9, 17–18, 210, 215, 231, 233, 236,

272–273, 275, 461, 469, 473, 475–479, 487

image of the absolute conic

connection to calibration 209

image plane

moving it 203

image-equivalent conﬁgurations

deﬁnition 535

incidence relations 66, 367, 370–372, 376–377, 379, 422
incidence relations

for lines 365

independently moving objects. 442
inﬁnite homography 205, 250, 270–271, 274, 276, 325,

338–339, 341, 473, 475, 479–483, 496, 498, 500

inﬁnite homography 270, 475

Index

651

inﬁnite homography constraint

relationship to Kruppa equations 481

inﬁnite homography

deﬁnition 338

inhomogeneous method 313
initialization

for camera resectioning 187

interest points 123–125, 290–291, 400–401, 403, 452–453,

468

internal parameters 163, 167, 178, 187, 195, 204, 210, 228,
239, 247–248, 250, 264, 269, 273–274, 276, 339, 365,
439, 444, 458–459, 461–462, 464, 466, 468–469,
471–473, 475–479, 481–486, 489–490, 492–493,
495–500
of camera

ﬁnding them 163

intersection

of lines 27, 253, 269, 373
of parallel lines 28

invariance to image coordinate transformations 104
invariants 38–39, 41, 43–44, 63, 77, 176, 266, 489–490, 532,

628

invariants 38–40, 42
invariants

of a transformation 38–40, 42

isometries 38
iteration

weighted 598

iterative algorithms 533
iterative methods 114, 461, 485
iterative minimization 110–113, 121–122, 128, 181, 186–187,

191, 193, 406, 462, 469

J
Jacobi’s method 581
Jacobian 132, 140, 142, 144–145, 147–148, 188, 299, 301,
305, 315, 523, 598, 602–604, 606–607, 610–612, 626

K
Klein

Erlangen Program 32

known principal point 467, 472, 477
Kruppa equations 458, 469–474, 481, 498, 500
Kruppa equations

relationship to inﬁnite homography 481

L
least squares 110
least squares solution 589, 597
least-squares problems 592
least-squares solution 589, 597
least-squares solution

of constrained systems 594–595
using normal equations 591
weighted 592

least-squares solutions

full rank case 588

length ratios 6, 41, 43, 51, 54–56, 221–222, 270
Levenberg-Marquardt 111, 186, 285, 398, 600–604, 606–609,

616, 621

Levenberg-Marquardt

covariance 604
implementation 602
justiﬁcation 601
sparse 603, 606

line at inﬁnity 2–6, 10, 25, 28–29, 43–44, 48–50, 62, 79, 81,

91, 160, 218, 518, 632

line at inﬁnity 28, 48
line correspondences 92, 131, 180, 391–392, 394, 396, 398,

406, 411, 424, 428, 432, 447, 450, 532

line estimation 117, 233
line projection matrix

deﬁnition 198

line reconstruction 323
line transfer 377
line transfer 382
linear equations

systems of unknown rank 589

linear solution 465
lines

concurrent 45
parallel 269
representation 68, 393
transformations of 36

M
Mahalanobis

distance 103, 112, 137, 141, 183, 282, 398, 565–566, 598,

603

norm 100, 129, 143

matching

using trifocal tensor 123, 126–127, 291–292, 401–403, 455

matrix

Euclidean 62
orthogonal

norm-preserving properties 578

skew-symmetric part 251
symmetric 590
symmetric part 251

Maximum likelihood estimate 114, 127, 134, 144, 181,
184–185, 216, 279, 284–285, 291, 312, 323, 332,
349–351, 353, 397, 403, 437, 439, 456, 569, 616, 620

Maximum likelihood estimate

robust 121, 125

Maxmimum likelihood estimate

robust 125

measurement matrix

reduced 186

metric calibration 495
metric properties 25, 48, 53, 55, 79, 82–84, 266, 458
metric properties of reconstruction 82
metric properties

recovery from images 55

metric reconstruction

via the inﬁnite homography 475
metric rectiﬁcation 55–57, 236, 479
metric rectiﬁcation 55
metric structure 39
metric structure : structure

metric 39

metric-calibration

of a stereo rig 495

minimal solution 91–92, 113, 181, 193–194, 279, 348, 352,

406

minimization

iterative 435

MLE – see Maximum likelihood estimate
modulus constraint 473–474, 499
modulus constraint 473
multiview tensor

afﬁne 439

N
Necker reversal 356
Necker reversal ambiguity 356
Newton iteration 111, 114, 597, 599–601, 616, 621
newton’s method and the hessian. 598
non-collinearity conditions 386
non-isotropic scaling – see scaling – non-isotropic
non-rigid factorization 442–443, 456

652

non-zero skew 164
norm

in vector space 592

normal equations 440, 591, 598–604, 611–612, 614, 623
normalization 91, 94, 97, 107–110, 127, 129, 146, 179–181,
185, 259, 281–282, 284, 313, 391–392, 394, 396, 446,
513, 618, 624

for camera resectioning 180
for trifocal tensor 394
why is it essential? 107
normalized coordinates 257
normalized coordinates

for calibrated camera 257

normalizing transformation 105, 107, 282, 394
note. 574
null-space 67–70, 72, 76, 90, 144, 158, 175, 179, 198–199,
234, 258, 280–281, 283, 296, 299, 309, 348, 367, 374,
405, 451, 471, 480, 538, 582, 591

numbers of equations generated. 451

O
orientation – see also camera orientation
orientation 11, 30, 37–39, 41–42, 47–48, 105, 156, 163–165,

167, 174, 186–187, 193, 210, 215–216, 218, 230–231,
262, 264, 273, 301–302, 358, 427, 448, 486, 490, 493,
496, 523, 531
orthogonal matrices

norm-preserving properties 578

orthogonal regression 351, 489
orthogonal regression 406
orthogonality and (cid:2) 212
orthogonality in the image 213
orthogonality relationships 219
orthographic projection 170–172, 174, 177, 456
over-parametrization 142
over-parametrization 70, 141–143, 177, 286, 435, 597, 605

P
parallax 204, 207–208, 325, 334–337, 339–340, 352–353,

388, 404, 407

induced by a plane 335

parallel lines 1–2, 4, 9, 25, 28–29, 34, 39, 41, 44, 49–50, 65,

86, 174, 176, 215, 217–219, 226, 247–248, 269–270,
293–294, 496

parallel projection 153, 170, 172–173, 200, 344, 346,

356–357, 361

parameter minimization 620
parameter space 113, 134, 141–143, 145, 475, 498, 568, 570,

572, 574, 577, 623–624

parameters

constant internal 466
limit range 570

parametrization 76, 100, 110–111, 113–114, 127, 134, 141,
164, 181, 186, 284–286, 293–294, 316, 392, 398, 406,
467, 491, 499, 512, 568–569, 572, 574, 576, 579, 597,
610, 623–626, 633

epipolar 286
of 3D rotations 624
of homogeneous vectors 624
of homology 630
of the n-sphere 625
what makes a good one? 623

Pl¨ucker

coordinates 72–73, 86, 197
line coordinates 72, 198–199, 233
line representation 197
matrix 70, 198

planar harmonic homology 64, 631
planar harmonic homology 631
planar homology

Index

parametrization 630

planar motion

ﬁxed image points 487

planar projective transformation

deﬁnition 33

plane at inﬁnity 2–4, 6, 9, 16–18, 78, 80, 82–85, 160–162,
166, 172–173, 175, 195, 209, 214, 268–272, 274–277,
313, 338, 347, 353, 425–426, 448–449, 458, 460–461,
463, 471, 473–475, 481, 487, 489, 494–496, 500, 515,
517, 519, 521–522, 527–528, 530

computation 495
methods of ﬁnding 474

plane induced parallax 335–336
plane induced parallax 335
plane plus parallax 404
plane plus parallax reconstruction

counting arguments 425

plane

deﬁned by three points 66
homogeneous vector representation 66

point equations

recommended method 431

point transfer

using fundamental matrix 380
using trifocal tensor 381

point

deﬁned by three planes 67

points on a plane 296
polarity 83
pole–polar relationship 58
positive deﬁnite matrix 598
positive-deﬁnite matrix

symmetric 582

preserve the convex hull – deﬁnition 515
principal axis vector 158
principal axis vector 161
principal plane 154, 158, 160–163, 165, 173, 183, 199, 215,

271, 323, 414, 418, 425, 456–457, 518–519, 530

principal plane 160
principal plane

of camera 160

principal point 16, 21, 154–155, 157–158, 160–161, 163, 168,

170, 185, 191, 203–204, 226–228, 231–232, 235–236,
260, 464–467, 469, 475, 477–478, 480, 484, 486, 492,
498–499

principal point 160
principal point offset 155
probability density function 569
proﬁle

of a surface 200

programming hint 129, 399
projection

orthographic 171
parallel 170
scaled orthographic 171
weak perspective 171
projective ambiguity 265
projective camera

action on conics 199
action on lines 196
action on planes 196
deﬁnition 157

projective depth 407, 444–447
projective depth 336
projective factorization

normalizing image coordinates 446
normalizing the weights 446
what is being minimized 446

projective plane model 29, 47
projective plane

model 29
topology 47

projective realization – see realization – projective
projective reconstruction

dual algorithm 504
from reduced cameras matrices 509
from seven points in n views 512
from six points 510
from six points in n views 511

projective transformation 68
projective transformation

decomposition of 42

projectivity

deﬁnition 32

pseudo-inverse 144–145, 147, 161, 185, 244, 246, 347–348,

440, 566, 590–592, 595, 598, 605, 610, 626

Q
QR decomposition 186, 578–580
quadric 6, 72–76, 83–85, 97, 176, 195, 199–202, 234, 267,

295–298, 309, 340, 462–464, 473, 499, 541–549,
552–560

dual – see dual quadric
ruled 543, 549, 557
ruled

deﬁnition 75

quadrics 73
quadrics

classiﬁcation of 74

quadrifocal tensor 13, 411, 421, 423, 426, 432–433, 450
quadrifocal tensor

derivation from four planes 421

quartic 97, 474, 559
quasi-afﬁne mapping

deﬁnition 517
two-dimensional 518

quasi-afﬁne reconstruction

algorithm 527
bounds on (cid:0)∞ 527

quaternions 585
quaternions 585, 624–625

R
radial distortion 188–193, 223

estimating 191

RANSAC 88, 114, 117–118, 121–124, 126–128, 193,

290–292, 352, 391, 400–403, 620

RANSAC

adaptive 120
application domain 125
distance threshold 118
for 2D homography 124
for homography 124
number of samples 119
run details 125
size of consensus set 120

realizable points
deﬁnition 503

realization 503
realization

deﬁnition 503
strong 524

realizations

equivalent

deﬁnition 540

non-equivalent 541
oppositely oriented 526

recommendations 289
reconstruction

afﬁne – see afﬁne reconstruction

Index

653

afﬁne 339
afﬁne properties 81
afﬁne

from afﬁne camera 271
from scene constraints 269
from translation 268

Euclidean 19, 266–267, 526
metric – see also auto-calibration
metric 268, 272–277, 339, 439, 454, 458–461, 463–464,

467–468, 473, 476, 479, 481, 488–490, 492, 497–499,
520, 532

metric properties 82
projective 13, 16–18, 150, 239, 266–268, 270–271,

275–277, 310–312, 320, 330, 339, 387, 398, 434,
445–446, 448, 453, 456, 458–464, 467, 475, 479, 485,
489, 491, 495–496, 502–503, 506, 508, 511, 514–515,
519–521, 525–531, 535, 539–540, 602, 627

quasi-afﬁne 475, 515, 517–521, 523, 525–528, 530, 532

rectifying homography 463
reduced camera matrix 207
reduced camera matrix

deﬁnition 502

reduced fundamental matrix – see fundamental matrix –

reduced

reduced measurement matrix 186
reduced measurement matrix 186
reduced trifocal tensor – see trifocal tensor – reduced
relation

bilinear 419
quadrilinear 420
trilinear 391, 397, 414–417

reprojection error 401
reprojection error
both images 95
geometric interpretation 96

resection 276, 424, 435, 453, 533–534, 536–539, 549–550,

553, 558, 587

resectioning – see camera resectioning, 77, 178, 453, 536–537,

559

covariance estimation 188
degenerate conﬁguration 179
initialization 187
normalization 180

residuals 187, 192, 194, 216, 298, 472
retrieving the camera matrices 374
robust algorithms 122
robust methods 88, 113, 117–118, 121–123, 125, 128, 281,
291, 352, 391, 393, 401, 455, 514, 597, 617, 619, 622

Rodrigues formula 585
rotations

in n dimensions 583
in 3 dimensions 583
row vector 26, 521, 578
row vector 159

S
Sampson approximation 99, 113–114, 287, 291, 308, 314,

349, 391, 398, 401
Sampson distance 287
Sampson error 98
scaled orthographic projection 171
scaling

isotropic 62, 107, 445
non-isotropic 109
with points near inﬁnity 110

scene constraints
distance ratios

on a line 270
parallel lines 269
scene orthogonality 273

654

Index

screw axis 78–79, 86, 253, 487–488, 490, 500
screw decomposition of a rigid motion 77–79
similarity matrix 62
similarity transformation 39, 43, 53, 107, 184
Singular Value Decomposition 40, 55, 70, 91, 108, 163, 186,
225, 258, 280–282, 294, 308, 344, 430, 437–439, 442,
444–445, 469, 471, 585–590, 592–596

Singular Value Decomposition

computational complexity 586
implementation 439, 586

singular values

relation to eigenvalues 586

singularity constraint 281–282, 288, 349
singularity constraint 349
skew-symmetric part 250–252
skyline structure 615–616
solution

over-determined 179

space curve 295
span representation 70, 322
sparse linear systems

solution 615
special points 5
stability 469
stereo correspondence 240
stereo correspondence 340
stereo rectiﬁcation 47, 49–50, 228–230, 249, 279, 302–303,

308–309, 339, 479

stereo rectiﬁcation

afﬁne 308
algorithm outline 307
mapping the epipole to inﬁnity 303
matching transforms 305

stereo-rig

metric calibration 495

stratiﬁcation 276–277
stratiﬁcation 57
stratiﬁed reconstruction

afﬁne 268
metric 272

structure

afﬁne 9, 356, 481, 570, 573
metric 233, 276, 461, 491, 611
projective 19, 386, 423, 426, 448, 494, 535

subspaces and tensors 442
SVD – see Singular Value Decomposition
symmetric epipolar distance 287
symmetric linear equations 614
symmetric linear equations

sparse 615

symmetric matrices 590
symmetric part 250–252, 261, 293, 489, 541
symmetric transfer error 94, 112
synthetic data 132, 138, 399, 572

T
tangent line

to conic 31

tensor notation 376
tensor

pictorial representation 564

terminology 172, 240, 445
topology of IP1 47
total least squares 110
transfer 14, 16, 94–98, 109, 113, 123–124, 131, 143, 147–150,

221–222, 242–243, 270, 274, 299, 318, 325, 336, 338,
340, 346, 365, 368–369, 374, 377, 379–383, 385, 388,
392, 398, 462, 473, 476, 485

transfer

degenerate conﬁgurations 382–383

transformation invariance 129
transformation invariance 184
transformation invariance

of DLT 184

transformation rule 36–37, 53–54, 63, 231, 394, 406, 475,

563–564
transformation
of conic 36
of conics 36
of lines 36
projective 1, 3–4, 7–9, 11–13, 16, 25, 30, 33–37, 41–49,

51–55, 57, 59, 61–65, 74, 76–77, 80–84, 86–88,
91–92, 94, 101, 103–105, 115, 132–133, 137, 165,
177–178, 195–196, 201–202, 204, 210, 222, 239,
249–250, 253–255, 265–268, 271–272, 296, 303,
305–308, 310, 313, 320, 366, 374–375, 387, 389, 404,
420, 423, 434, 445, 463, 475, 482, 491, 494, 502, 508,
511, 513, 516–517, 519–522, 524, 527, 529, 531,
533–534, 539–540, 557, 559, 628–631

similarity 39, 43, 53, 107, 184

translational motion 293
triangulation 12, 263, 267, 276–277, 285, 291, 293, 310–315,

318, 320, 323–324, 332, 344, 380, 385, 397, 406, 435,
439–440, 442, 453–454, 456, 491, 568, 575

triangulation 263
triangulation

DLT method 312
evaluation on real images 320
for afﬁne cameras 353
for afﬁne cameras 439–440, 442
inhomogenous method 313
local minima 319
minimizing the cost function 316
problem formulation 315

trifocal tensor 12–13, 16, 87, 108, 119, 123, 164, 365,

367–378, 381–383, 386–407, 411, 414–419, 422–423,
426, 431, 439, 443, 446, 452, 456, 488–489, 508, 510,
512, 514, 533, 563–564, 594, 611

trifocal tensor

afﬁne 390, 406, 418
algebraic properties 373
computation from camera matrices 377
computation

from 6 points 393
Gold Standard algorithm 396
iterative 396
reprojection error 401
results and recommendations 400
Sampson distance 287, 398
specifying lines 404
using algebraic minimization 395

constraints 392
deﬁnition 367
deﬁnition 377
derivation from four planes 421
geometrically valid 392
line relations 416
parametrization 398
picture 564
point and line relations 369
point relations 414
reduced 433, 512
relation to two views 418
transformation rule 563

trilinearities – see also relations – trilinear
trilinearities 378
twisted cubic 75–77, 179–180, 251, 253, 343, 487, 536–537,

539–540, 547, 549–553, 558–559

properties 76

twisted pair 259, 275

Index

655

V
vanishing lines 176, 213, 216, 218–219, 230, 236, 270
vanishing lines 339
vanishing points 9, 42, 51, 110, 158–159, 161, 176, 180, 195,

208, 212–213, 215, 217–219, 222–224, 226–230,
232–233, 236, 269–271, 273, 278, 294, 339, 341, 448,
458, 497

vanishing points 339
vanishing points

computing 215

variance 102, 109, 122, 132–137, 139–141, 143–145,

147–148, 150, 188, 300–301, 321–322, 565, 568–569,
573–574, 576–577, 605, 623, 627

vector geometry 67
vector space norms 592

W
weak perspective 170–172, 357

Z
zero skew 164, 193, 224–226, 228–229, 231, 235, 273,

465–466, 472, 477–479, 484, 497–499

zero skew 466

