Bayesian Reasoning and Machine Learning

David Barber c(cid:13)2007,2008,2009,2010

Notation List
V
dom(x)

x = x

p(x = tr)

p(x = fa)

p(x, y)

p(x ∩ y)
p(x ∪ y)
(cid:82)
p(x|y)
x f(x)
I [x = y]

pa (x)

ch (x)

ne (x)

X ⊥⊥Y|Z
X(cid:62)(cid:62)Y|Z
dim x

(cid:104)f(x)(cid:105)p(x)
δ(a, b)

dim x

(cid:93) (x = s, y = t)

D
n

N

(cid:93)x
y

S

σ(x)

erf(x)

i ∼ j
Im

II

a calligraphic symbol typically denotes a set of random variables . . . . . . . . 3

Domain of a variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

The variable x is in the state x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

probability of event/variable x being in the state true . . . . . . . . . . . . . . . . . . . 3

probability of event/variable x being in the state false . . . . . . . . . . . . . . . . . . . 3

probability of x and y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

probability of x and y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

probability of x or y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

The probability of x conditioned on y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

For continuous variables this is shorthand for(cid:82) f(x)dx and for discrete vari-
ables means summation over the states of x,(cid:80)

x f(x) . . . . . . . . . . . . . . . . . . . 7
Indicator : has value 1 if x = y, 0 otherwise . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

The parents of node x.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

The children of node x.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

Neighbours of node x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

Variables X are independent of variables Y conditioned on variables Z. 33
. . . . .33
Variables X are dependent on variables Y conditioned variables Z.
For a discrete variable x, this denotes the number of states x can take . .43

The average of the function f(x) with respect to the distribution p(x). 139
Delta function. For discrete a, b, this is the Kronecker delta, δa,b and for
continuous a, b the Dirac delta function δ(a − b) . . . . . . . . . . . . . . . . . . . . . . 142
The dimension of the vector/matrix x. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .150
The number of times variable x is in state s and y in state t simultaneously.
172
Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .251

Data index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

Number of Dataset training points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

The number of times variable x is in state y . . . . . . . . . . . . . . . . . . . . . . . . . . 265

Sample Covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283

The logistic sigmoid 1/(1 + exp(−x) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
The (Gaussian) error function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319

The set of unique neighbouring edges on a graph . . . . . . . . . . . . . . . . . . . . . .529

The m × m identity matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546

DRAFT March 9, 2010

Preface

Machine Learning

The last decade has seen considerable growth in interest in Artiﬁcial Intelligence and Machine Learning.
In the broadest sense, these ﬁelds aim to ‘learn something useful’ about the environment within which the
organism operates. How gathered information is processed leads to the development of algorithms – how
to process high dimensional data and deal with uncertainty. In the early stages of research in Machine
Learning and related areas, similar techniques were discovered in relatively isolated research communities.
Whilst not all techniques have a natural description in terms of probability theory, many do, and it is the
framework of Graphical Models (a marriage between graph and probability theory) that has enabled the
understanding and transference of ideas from statistical physics, statistics, machine learning and informa-
tion theory. To this extent it is now reasonable to expect that machine learning researchers are familiar
with the basics of statistical modelling techniques.

This book concentrates on the probabilistic aspects of information processing and machine learning. Cer-
tainly no claim is made as to the correctness or that this is the only useful approach. Indeed, one might
counter that this is unnecessary since “biological organisms don’t use probability theory”. Whether this
is the case or not, it is undeniable that the framework of graphical models and probability has helped
with the explosion of new algorithms and models in the machine learning community. One should also be
clear that Bayesian viewpoint is not the only way to go about describing machine learning and information
processing. Bayesian and probabilistic techniques really come into their own in domains where uncertainty
is a necessary consideration.

The structure of the book

One aim of part I of the book is to encourage Computer Science students into this area. A particular diﬃ-
culty that many modern students face is a limited formal training in calculus and linear algebra, meaning
that minutiae of continuous and high-dimensional distributions can turn them away. In beginning with
probability as a form of reasoning system, we hope to show the reader how ideas from logical inference
and dynamical programming that they may be more familiar with have natural parallels in a probabilistic
context. In particular, Computer Science students are familiar with the concept of algorithms as core.
However, it is more common in machine learning to view the model as core, and how this is implemented
is secondary. From this perspective, understanding how to translate a mathematical model into a piece of
computer code is central.

Part II introduces the statistical background needed to understand continuous distributions and how learn-
ing can be viewed from a probabilistic framework. Part III discusses machine learning topics. Certainly
some readers will raise an eyebrow to see their favourite statistical topic listed under machine learning. A
diﬀerence viewpoint between statistics and machine learning is what kinds of systems we would ultimately

III

like to construct (machines capable of ‘human/biological information processing tasks) rather than in some
of the techniques. This section of the book is therefore what I feel would be useful for machine learners
to know.

Part IV discusses dynamical models in which time is explicitly considered. In particular the Kalman Filter
is treated as a form of graphical model, which helps emphasise what the model is, rather than focusing on
it as a ‘ﬁlter’, as is more traditional in the engineering literature.

Part V contains a brief introduction to approximate inference techniques, including both stochastic (Monte
Carlo) and deterministic (variational) techniques.

The references in the book are not generally intended as crediting authors with ideas, nor are they always
to the most authoritative works. Rather, the references are largely to works which are at a level reasonably
consistent with the book and which are readily available.

Whom this book is for

My primary aim was to write a book for ﬁnal year undergraduates and graduates without signiﬁcant expe-
rience in calculus and mathematics that gave an inroad into machine learning, much of which is currently
phrased in terms of probabilities and multi-variate distributions. The aim was to encourage students that
apparently unexciting statistical concepts are actually highly relevant for research in making intelligent
systems that interact with humans in a natural manner. Such a research programme inevitably requires
dealing with high-dimensional data, time-series, networks, logical reasoning, modelling and uncertainty.

Other books in this area

Whilst there are several excellent textbooks in this area, none currently meets the requirements that I per-
sonally need for teaching, namely one that contains demonstration code and gently introduces probability
and statistics before leading on to more advanced topics in machine learning. This lead me to build on my
lecture material from courses given at Aston, Edinburgh, EPFL and UCL and expand the demonstration
software considerably. The book is due for publication by Cambridge University Press in 2010.

The literature on machine learning is vast, as is the overlap with the relevant areas of statistics, engineering
and other physical sciences. In this respect, it is diﬃcult to isolate particular areas, and this book is an
attempt to integrate parts of the machine learning and statistics literature. The book is written in an
informal style at the expense of rigour and detailed proofs. As an introductory textbook, topics are
naturally covered to a somewhat shallow level and the reader is referred to more specialised books for
deeper treatments. Amongst my favourites are:

• Graphical models

– Graphical models by S. Lauritzen, Oxford University Press, 1996.
– Bayesian Networks and Decision Graphs by F. Jensen and T. D. Nielsen, Springer Verlag, 2007.
– Probabilistic Networks and Expert Systems by R. G. Cowell, A. P. Dawid, S. L. Lauritzen and

D. J. Spiegelhalter, Springer Verlag, 1999.

– Probabilistic Reasoning in Intelligent Systems by J. Pearl, Morgan Kaufmann, 1988.
– Graphical Models in Applied Multivariate Statistics by J. Whittaker, Wiley, 1990.
– Probabilistic Graphical Models: Principles and Techniques by D. Koller and N. Friedman, MIT

Press, 2009.

• Machine Learning and Information Processing

– Information Theory, Inference and Learning Algorithms by D. J. C. MacKay, Cambridge Uni-

versity Press, 2003.

IV

DRAFT March 9, 2010

– Pattern Recognition and Machine Learning by C. M. Bishop, Springer Verlag, 2006.
– An Introduction To Support Vector Machines, N. Cristianini and J. Shawe-Taylor, Cambridge

University Press, 2000.

– Gaussian Processes for Machine Learning by C. E. Rasmussen and C. K. I. Williams, MIT

press, 2006.

How to use this book

Part I would be suitable for an introductory course on Graphical Models with a focus on inference. Part
II contains enough material for a short lecture course on learning in probabilistic models. Part III is
reasonably self-contained and would be suitable for a course on Machine Learning from a probabilistic
perspective, particularly combined with the dynamical models material in part IV. Part V would be
suitable for a short course on approximate inference.

Accompanying code

The MATLAB code is provided to help readers see how mathematical models translate into actual code.
The code is not meant to be an industrial strength research tool, rather a reasonably lightweight toolbox
that enables the reader to play with concepts in graph theory, probability theory and machine learning.
In an attempt to retain readability, no extensive error and/or exception handling has been included.
The code contains at the moment basic routines for manipulating discrete variable distributions, along
with a set of routines that are more concerned with continuous variable machine learning. One could
in principle extend the ‘graphical models’ part of the code considerably to support continuous variables.
Limited support for continuous variables is currently provided so that, for example, inference in the linear
dynamical system may be written in conducted of operations on Gaussian potentials. However, in general,
potentials on continuous variables need to be manipulated with care and often specialised routines are
required to ensure numerical stability.

Acknowledgements

Many people have helped this book along the way either in terms of reading, feedback, general insights,
allowing me to present their work, or just plain motivation. Amongst these I would like to thank Massim-
iliano Pontil, Mark Herbster, John Shawe-Taylor, Vladimir Kolmogorov, Yuri Boykov, Tom Minka, Simon
Prince, Silvia Chiappa, Bertrand Mesot, Robert Cowell, Ali Taylan Cemgil, David Blei, Jeﬀ Bilmes, David
Cohn, David Page, Peter Sollich, Chris Williams, Marc Toussaint, Amos Storkey, Zakria Hussain, Seraf´ın
Moral, Milan Studen´y, Tristan Fletcher, Tom Furmston, Ed Challis and Chris Bracegirdle. I would also
like to thank the many students that have helped improve the material during lectures over the years. I’m
particularly grateful to Tom Minka for allowing parts of his Lightspeed toolbox to be bundled with the
BRMLtoolbox and am similarly indebted to Taylan Cemgil for his GraphLayout package.

A ﬁnal thankyou to my family and friends.

Website

The code along with an electronic version of the book is available from

http://www.cs.ucl.ac.uk/staff/D.Barber/brml

Instructors seeking solutions to the exercises can ﬁnd information at the website, along with additional
teaching material. The website also contains a feedback form and errata list.

DRAFT March 9, 2010

V

VI

DRAFT March 9, 2010

Contents

I

Inference in Probabilistic Models

1

1 Probabilistic Reasoning

3
3
1.1 Probability Refresher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.1 Probability Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.1.2
Interpreting Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Probabilistic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.3 Prior, Likelihood and Posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.1 Two dice : what were the individual scores? . . . . . . . . . . . . . . . . . . . . . . . 10
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.4 Further worked examples
1.5 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.5.1 Basic Probability code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.5.2 General utilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.5.3 An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

1.6 Notes
1.7 Exercises

2 Basic Graph Concepts

2.1.1

19
2.1 Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Spanning tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Numerically Encoding Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2.1 Edge list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2.2 Adjacency matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2.3 Clique matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.3 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.3.1 Utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

2.4 Exercises

3 Belief Networks

25
3.1 Probabilistic Inference in Structured Distributions
. . . . . . . . . . . . . . . . . . . . . . . 25
3.2 Graphically Representing Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2.1 Constructing a simple Belief network : wet grass . . . . . . . . . . . . . . . . . . . . 26
3.2.2 Uncertain evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.3 Belief Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.3.1 Conditional independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.3.2 The impact of collisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
d-Separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.3.3
3.3.4
d-Connection and dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.3.5 Markov equivalence in belief networks . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.3.6 Belief networks have limited expressibility . . . . . . . . . . . . . . . . . . . . . . . . 39

VII

CONTENTS

CONTENTS

3.4.1
3.4.2
3.4.3 Learning the direction of arrows

3.4 Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Simpson’s paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Inﬂuence diagrams and the do-calculus . . . . . . . . . . . . . . . . . . . . . . . . . . 42
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.5 Parameterising Belief Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.7 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.7.1 Naive inference demo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.7.2 Conditional independence demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.7.3 Utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

3.8 Exercises

4 Graphical Models

49
4.1 Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.2 Markov Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.2.1 Markov properties
4.2.2 Gibbs networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.2.3 Markov random ﬁelds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.2.4 Conditional independence using Markov networks . . . . . . . . . . . . . . . . . . . . 53
4.2.5 Lattice Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.3 Chain Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.4 Expressiveness of Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.5 Factor Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.5.1 Conditional independence in factor graphs . . . . . . . . . . . . . . . . . . . . . . . . 59
4.6 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.7 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.8 Exercises

5 Eﬃcient Inference in Trees

5.2 Other Forms of Inference

63
5.1 Marginal Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.1 Variable elimination in a Markov chain and message passing . . . . . . . . . . . . . . 63
5.1.2 The sum-product algorithm on factor graphs
. . . . . . . . . . . . . . . . . . . . . . 66
5.1.3 Computing the marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.1.4 The problem with loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.2.1 Max-Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.2.2 Finding the N most probable states
. . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.2.3 Most probable path and shortest path . . . . . . . . . . . . . . . . . . . . . . . . . . 75
5.2.4 Mixed inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Inference in Multiply-Connected Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.3.1 Bucket elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.3.2 Loop-cut conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.4 Message Passing for Continuous Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.5 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.6.1 Factor graph examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.6.2 Most probable and shortest path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.6.3 Bucket elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.6.4 Message passing on Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

5.3

5.7 Exercises

VIII

DRAFT March 9, 2010

CONTENTS

CONTENTS

6 The Junction Tree Algorithm

6.2 Clique Graphs

6.3 Junction Trees

6.1 Clustering Variables

85
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.1.1 Reparameterisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
6.2.1 Absorption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
6.2.2 Absorption schedule on clique trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
6.3.1 The running intersection property . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.4 Constructing a Junction Tree for Singly-Connected Distributions . . . . . . . . . . . . . . . 92
6.4.1 Moralisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
6.4.2 Forming the clique graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
6.4.3 Forming a junction tree from a clique graph . . . . . . . . . . . . . . . . . . . . . . . 92
6.4.4 Assigning potentials to cliques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
6.5 Junction Trees for Multiply-Connected Distributions . . . . . . . . . . . . . . . . . . . . . . 93
6.5.1 Triangulation algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
6.6 The Junction Tree Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
6.6.1 Remarks on the JTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
6.6.2 Computing the normalisation constant of a distribution . . . . . . . . . . . . . . . . 99
6.6.3 The marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.7 Finding the Most Likely State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.8 Reabsorption : Converting a Junction Tree to a Directed Network . . . . . . . . . . . . . . 102
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.9 The Need For Approximations
6.9.1 Bounded width junction trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.10 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.10.1 Utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

6.11 Exercises

7 Making Decisions

Syntax of inﬂuence diagrams

7.4 Solving Inﬂuence Diagrams

7.6 Temporally Unbounded MDPs

107
7.1 Expected Utility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1.1 Utility of money . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.2 Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.3 Extending Bayesian Networks for Decisions
. . . . . . . . . . . . . . . . . . . . . . . . . . . 111
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.4.1 Eﬃcient inference
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.4.2 Using a junction tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
7.5 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
7.5.1 Maximising expected utility by message passing . . . . . . . . . . . . . . . . . . . . . 120
7.5.2 Bellman’s equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.6.1 Value iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.6.2 Policy iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
7.6.3 A curse of dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.7 Probabilistic Inference and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.7.1 Non-stationary Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.7.2 Non-stationary probabilistic inference planner . . . . . . . . . . . . . . . . . . . . . . 125
7.7.3
Stationary planner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.7.4 Utilities at each timestep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
7.9 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Sum/Max under a partial order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Junction trees for inﬂuence diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

7.8.1 Partially observable MDPs
7.8.2 Restricted utility functions
7.8.3 Reinforcement learning

7.8 Further Topics

7.3.1

7.9.1
7.9.2

DRAFT March 9, 2010

IX

CONTENTS

CONTENTS

7.9.3 Party-Friend example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.9.4 Chest Clinic with Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.9.5 Markov decision processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

7.10 Exercises

II Learning in Probabilistic Models

137

8 Statistics for Machine Learning:

8.2.1 Estimator bias
8.3 Discrete Distributions
8.4 Continuous Distributions

139
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.1 Distributions
8.2 Summarising distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
8.4.1 Bounded distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
8.4.2 Unbounded distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
8.5 Multivariate Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8.6 Multivariate Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
8.6.1 Conditioning as system reversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
8.6.2 Completing the square . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
8.6.3 Gaussian propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8.6.4 Whitening and centering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8.6.5 Maximum likelihood training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8.6.6 Bayesian Inference of the mean and variance
. . . . . . . . . . . . . . . . . . . . . . 153
8.6.7 Gauss-Gamma distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.7 Exponential Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.7.1 Conjugate priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
8.8 The Kullback-Leibler Divergence KL(q|p)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
8.8.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
8.9 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
8.10 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

9 Learning as Inference

165
9.1 Learning as Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.1.1 Learning the bias of a coin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.1.2 Making decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
9.1.3 A continuum of parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
9.1.4 Decisions based on continuous intervals
. . . . . . . . . . . . . . . . . . . . . . . . . 168
9.2 Maximum A Posteriori and Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . 169
9.2.1
Summarising the posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.2.2 Maximum likelihood and the empirical distribution . . . . . . . . . . . . . . . . . . . 170
9.2.3 Maximum likelihood training of belief networks . . . . . . . . . . . . . . . . . . . . . 171
9.3 Bayesian Belief Network Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
9.3.1 Global and local parameter independence . . . . . . . . . . . . . . . . . . . . . . . . 174
9.3.2 Learning binary variable tables using a Beta prior
. . . . . . . . . . . . . . . . . . . 176
9.3.3 Learning multivariate discrete tables using a Dirichlet prior . . . . . . . . . . . . . . 178
9.3.4 Parents
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
9.3.5
Structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
9.3.6 Empirical independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
9.3.7 Network scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
9.4 Maximum Likelihood for Undirected models . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
9.4.1 The likelihood gradient
9.4.2 Decomposable Markov networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
9.4.3 Non-decomposable Markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
. . . . . . . . . . . . . . . . . . . . . . 189
9.4.4 Constrained decomposable Markov networks

X

DRAFT March 9, 2010

CONTENTS

CONTENTS

9.4.5
Iterative scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
9.4.6 Conditional random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.4.7 Pseudo likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
9.4.8 Learning the structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
9.5 Properties of Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
9.5.1 Training assuming the correct model class . . . . . . . . . . . . . . . . . . . . . . . . 196
9.5.2 Training when the assumed model is incorrect . . . . . . . . . . . . . . . . . . . . . . 197
9.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
9.6.1 PC algorithm using an oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
9.6.2 Demo of empirical conditional independence . . . . . . . . . . . . . . . . . . . . . . . 197
9.6.3 Bayes Dirichlet structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198

9.7 Exercises

10 Naive Bayes

203
10.1 Naive Bayes and Conditional Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.2 Estimation using Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
10.2.1 Binary attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
10.2.2 Multi-state variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
10.2.3 Text classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
10.3 Bayesian Naive Bayes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
10.4 Tree Augmented Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
10.4.1 Chow-Liu Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
10.4.2 Learning tree augmented Naive Bayes networks . . . . . . . . . . . . . . . . . . . . . 212
10.5 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
10.6 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213

11 Learning with Hidden Variables

217
11.1 Hidden Variables and Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
11.1.1 Why hidden/missing variables can complicate proceedings . . . . . . . . . . . . . . . 217
11.1.2 The missing at random assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
11.1.3 Maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
11.1.4 Identiﬁability issues
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
11.2 Expectation Maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
11.2.1 Variational EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
11.2.2 Classical EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
11.2.3 Application to Belief networks
11.2.4 Application to Markov networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.2.5 Convergence
11.3 Extensions of EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.3.1 Partial M step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.3.2 Partial E step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.4 A Failure Case for EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
11.5 Variational Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
11.5.1 EM is a special case of variational Bayes . . . . . . . . . . . . . . . . . . . . . . . . . 233
11.5.2 Factorising the parameter posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
11.6 Bayesian Methods and ML-II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
11.7 Optimising the Likelihood by Gradient Methods
. . . . . . . . . . . . . . . . . . . . . . . . 236
11.7.1 Directed models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
11.7.2 Undirected models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
11.8 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
11.9 Exercises

DRAFT March 9, 2010

XI

CONTENTS

CONTENTS

12 Bayesian Model Selection

241
12.1 Comparing Models the Bayesian Way . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
12.2 Illustrations : coin tossing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
12.2.1 A discrete parameter space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
12.2.2 A continuous parameter space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
12.3 Occam’s Razor and Bayesian Complexity Penalisation . . . . . . . . . . . . . . . . . . . . . 244
12.4 A continuous example : curve ﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.5 Approximating the Model Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
12.5.1 Laplace’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
12.5.2 Bayes information criterion (BIC)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247

12.6 Exercises

III Machine Learning

249

13 Machine Learning Concepts

13.1 Styles of Learning

251
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
13.1.1 Supervised learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
13.1.2 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
13.1.3 Anomaly detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
13.1.4 Online (sequential) learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
13.1.5 Interacting with the environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
13.1.6 Semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
13.2 Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
13.2.1 Utility and Loss
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
13.2.2 What’s the catch? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
13.2.3 Using the empirical distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
13.2.4 Bayesian decision approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
13.2.5 Learning lower-dimensional representations in semi-supervised learning . . . . . . . . 261
13.2.6 Features and preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
13.3 Bayes versus Empirical Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
13.4 Representing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
13.4.1 Categorical
13.4.2 Ordinal
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
13.4.3 Numerical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
13.5 Bayesian Hypothesis Testing for Outcome Analysis . . . . . . . . . . . . . . . . . . . . . . . 263
13.5.1 Outcome analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
13.5.2 Hdiﬀ : model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
13.5.3 Hsame : model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
13.5.4 Dependent outcome analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
13.5.5 Is classiﬁer A better than B? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
13.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
13.7 Notes
13.8 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270

14 Nearest Neighbour Classiﬁcation

273
14.1 Do As Your Neighbour Does . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
14.2 K-Nearest Neighbours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
14.3 A Probabilistic Interpretation of Nearest Neighbours . . . . . . . . . . . . . . . . . . . . . . 275
14.3.1 When your nearest neighbour is far away . . . . . . . . . . . . . . . . . . . . . . . . 277
14.4 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
14.4.1 Utility Routines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
14.4.2 Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

14.5 Exercises

XII

DRAFT March 9, 2010

CONTENTS

CONTENTS

15 Unsupervised Linear Dimension Reduction

15.4 Latent Semantic Analysis

15.4.1 LSA for information retrieval

279
15.1 High-Dimensional Spaces – Low Dimensional Manifolds
. . . . . . . . . . . . . . . . . . . . 279
15.2 Principal Components Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
15.2.1 Deriving the optimal linear reconstruction . . . . . . . . . . . . . . . . . . . . . . . . 280
15.2.2 Maximum variance criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
15.2.3 PCA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
15.2.4 PCA and nearest neighbours
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
15.2.5 Comments on PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
15.3 High Dimensional Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
15.3.1 Eigen-decomposition for N < D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
15.3.2 PCA via Singular value decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 286
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
15.5 PCA With Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
15.5.1 Finding the principal directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
15.5.2 Collaborative ﬁltering using PCA with missing data . . . . . . . . . . . . . . . . . . 291
15.6 Matrix Decomposition Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
15.6.1 Probabilistic latent semantic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
15.6.2 Extensions and variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
15.6.3 Applications of PLSA/NMF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
15.7 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
15.8 Canonical Correlation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
15.8.1 SVD formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
15.9 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
15.10Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
15.11Exercises

16 Supervised Linear Dimension Reduction

303
16.1 Supervised Linear Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
16.2 Fisher’s Linear Discriminant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
16.3 Canonical Variates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
16.3.1 Dealing with the nullspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
16.4 Using non-Gaussian Data Distributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
16.5 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
16.6 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308

17 Linear Models

17.3 The Dual Representation and Kernels

311
17.1 Introduction: Fitting A Straight Line
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
17.2 Linear Parameter Models for Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
17.2.1 Vector outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
17.2.2 Regularisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
17.2.3 Radial basis functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
17.3.1 Regression in the dual-space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
17.3.2 Positive deﬁnite kernels (covariance functions)
. . . . . . . . . . . . . . . . . . . . . 318
17.4 Linear Parameter Models for Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
17.4.1 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
17.4.2 Maximum likelihood training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
17.4.3 Beyond ﬁrst order gradient ascent
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
17.4.4 Avoiding overconﬁdent classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
17.4.5 Multiple classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
17.5 The Kernel Trick for Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
17.6 Support Vector Machines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
17.6.1 Maximum margin linear classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
17.6.2 Using kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328

DRAFT March 9, 2010

XIII

CONTENTS

CONTENTS

17.6.3 Performing the optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
17.6.4 Probabilistic interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
17.7 Soft Zero-One Loss for Outlier Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
17.8 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
17.9 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
17.10Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330

18 Bayesian Linear Models

333
18.1 Regression With Additive Gaussian Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
18.1.1 Bayesian linear parameter models
18.1.2 Determining hyperparameters: ML-II
. . . . . . . . . . . . . . . . . . . . . . . . . . 335
18.1.3 Learning the hyperparameters using EM . . . . . . . . . . . . . . . . . . . . . . . . . 336
18.1.4 Hyperparameter optimisation : using the gradient
. . . . . . . . . . . . . . . . . . . 337
18.1.5 Validation likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
18.1.6 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
18.1.7 The relevance vector machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
18.2 Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
18.2.1 Hyperparameter optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
18.2.2 Laplace approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
18.2.3 Making predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
18.2.4 Relevance vector machine for classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . 344
18.2.5 Multi-class case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
18.3 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
18.4 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345

19 Gaussian Processes

347
19.1 Non-Parametric Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
19.1.1 From parametric to non-parametric
. . . . . . . . . . . . . . . . . . . . . . . . . . . 347
19.1.2 From Bayesian linear models to Gaussian processes . . . . . . . . . . . . . . . . . . . 348
19.1.3 A prior on functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
19.2 Gaussian Process Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
19.2.1 Regression with noisy training outputs . . . . . . . . . . . . . . . . . . . . . . . . . . 350
19.3 Covariance Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
19.3.1 Making new covariance functions from old . . . . . . . . . . . . . . . . . . . . . . . . 352
19.3.2 Stationary covariance functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
19.3.3 Non-stationary covariance functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . 355
19.4 Analysis of Covariance Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
19.4.1 Smoothness of the functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
19.4.2 Mercer kernels
19.4.3 Fourier analysis for stationary kernels
. . . . . . . . . . . . . . . . . . . . . . . . . . 358
19.5 Gaussian Processes for Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
19.5.1 Binary classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
19.5.2 Laplace’s approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
19.5.3 Hyperparameter optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
19.5.4 Multiple classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
19.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
19.7 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
19.8 Exercises

20 Mixture Models

365
20.1 Density Estimation Using Mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
. . . . . . . . . . . . . . . . . . . . . . . . . 366
20.2 Expectation Maximisation for Mixture Models
20.2.1 Unconstrained discrete tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
20.2.2 Mixture of product of Bernoulli distributions . . . . . . . . . . . . . . . . . . . . . . 368
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370

20.3 The Gaussian Mixture Model

XIV

DRAFT March 9, 2010

CONTENTS

CONTENTS

20.3.1 EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
20.3.2 Practical issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
20.3.3 Classiﬁcation using Gaussian mixture models . . . . . . . . . . . . . . . . . . . . . . 373
20.3.4 The Parzen estimator
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
20.3.5 K-Means
20.3.6 Bayesian mixture models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
20.3.7 Semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
20.4 Mixture of Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
20.5 Indicator Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
. . . . . . . . . . . . . . . . . . . . . . . . 378
20.5.1 Joint indicator approach: factorised prior
20.5.2 Joint indicator approach : Polya prior . . . . . . . . . . . . . . . . . . . . . . . . . . 378
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
20.6.1 Latent Dirichlet allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
20.6.2 Graph based representations of data . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
20.6.3 Dyadic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
20.6.4 Monadic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
20.6.5 Cliques and adjacency matrices for monadic binary data . . . . . . . . . . . . . . . . 383
20.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
20.8 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
20.9 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387

20.6 Mixed Membership Models

21 Latent Linear Models

389
21.1 Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
21.1.1 Finding the optimal bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
21.2 Factor Analysis : Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
21.2.1 Direct likelihood optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
21.2.2 Expectation maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
21.3 Interlude: Modelling Faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
. . . . . . . . . . . . . . . . . . . . . . . . . . 397
21.4 Probabilistic Principal Components Analysis
21.5 Canonical Correlation Analysis and Factor Analysis
. . . . . . . . . . . . . . . . . . . . . . 398
21.6 Independent Components Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
21.7 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
21.8 Exercises

22 Latent Ability Models

403
22.1 The Rasch Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
22.1.1 Maximum Likelihood training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
22.1.2 Bayesian Rasch models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
22.2 Competition Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
22.3 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
22.4 Exercises

22.2.1 Bradly-Terry-Luce model
22.2.2 Elo ranking model
22.2.3 Glicko and TrueSkill

IV Dynamical Models

409

23 Discrete-State Markov Models

411
23.1 Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
23.1.1 Equilibrium and stationary distribution of a Markov chain . . . . . . . . . . . . . . . 412
23.1.2 Fitting Markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
23.1.3 Mixture of Markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416

23.2.1 The classical inference problems

23.2 Hidden Markov Models

DRAFT March 9, 2010

XV

CONTENTS

CONTENTS

23.3 Learning HMMs

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
23.2.2 Filtering p(ht|v1:t)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
23.2.3 Parallel smoothing p(ht|v1:T )
23.2.4 Correction smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
23.2.5 Most likely joint state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
23.2.6 Self localisation and kidnapped robots . . . . . . . . . . . . . . . . . . . . . . . . . . 421
23.2.7 Natural language models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
23.3.1 EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
23.3.2 Mixture emission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
23.3.3 The HMM-GMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
23.3.4 Discriminative training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
23.4 Related Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
23.4.1 Explicit duration model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
23.4.2 Input-Output HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
23.4.3 Linear chain CRFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
23.4.4 Dynamic Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
23.5 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
23.5.1 Object tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
23.5.2 Automatic speech recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
23.5.3 Bioinformatics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
23.5.4 Part-of-speech tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
23.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
23.7 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432

24 Continuous-state Markov Models

24.1.1 Stationary distribution with noise

437
24.1 Observed Linear Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
24.2 Auto-Regressive Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
24.2.1 Training an AR model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
24.2.2 AR model as an OLDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
24.2.3 Time-varying AR model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
24.3 Latent Linear Dynamical Systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
24.4 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
24.4.1 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
24.4.2 Smoothing : Rauch-Tung-Striebel correction method . . . . . . . . . . . . . . . . . . 446
24.4.3 The likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
24.4.4 Most likely state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
24.4.5 Time independence and Riccati equations . . . . . . . . . . . . . . . . . . . . . . . . 448
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
24.5.1 Identiﬁability issues
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
24.5.2 EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
24.5.3 Subspace Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
24.5.4 Structured LDSs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
24.5.5 Bayesian LDSs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
24.6.1 Inference
24.6.2 Maximum Likelihood Learning using EM . . . . . . . . . . . . . . . . . . . . . . . . 453
24.7 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
24.7.1 Autoregressive models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

24.5 Learning Linear Dynamical Systems

24.6 Switching Auto-Regressive Models

24.8 Exercises

XVI

DRAFT March 9, 2010

CONTENTS

CONTENTS

25 Switching Linear Dynamical Systems

457
25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
25.2 The Switching LDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
25.2.1 Exact inference is computationally intractable . . . . . . . . . . . . . . . . . . . . . . 458
25.3 Gaussian Sum Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
25.3.1 Continuous ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
25.3.2 Discrete ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
25.3.3 The likelihood p(v1:T ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
25.3.4 Collapsing Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
25.3.5 Relation to other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
25.4 Gaussian Sum Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
25.4.1 Continuous smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
25.4.2 Discrete smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
25.4.3 Collapsing the mixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
25.4.4 Using mixtures in smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
25.4.5 Relation to other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
25.5 Reset Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
25.5.1 A Poisson reset model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
25.5.2 HMM-reset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
25.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
25.7 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472

26 Distributed Computation

475
26.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
26.2 Stochastic Hopﬁeld Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
26.3 Learning Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
26.3.1 A single sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
26.3.2 Multiple sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
26.3.3 Boolean networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
26.3.4 Sequence disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
26.4 Tractable Continuous Latent Variable Models . . . . . . . . . . . . . . . . . . . . . . . . . . 482
26.4.1 Deterministic latent variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
26.4.2 An augmented Hopﬁeld network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
26.5.1 Stochastically spiking neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
26.5.2 Hopﬁeld membrane potential
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
26.5.3 Dynamic synapses
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
26.5.4 Leaky integrate and ﬁre models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
26.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
26.7 Exercises

26.5 Neural Models

V Approximate Inference

489

27 Sampling

491
27.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
27.1.1 Univariate sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
27.1.2 Multi-variate sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
27.2 Ancestral Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
27.2.1 Dealing with evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
27.2.2 Perfect sampling for a Markov network . . . . . . . . . . . . . . . . . . . . . . . . . 495
27.3 Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
27.3.1 Gibbs sampling as a Markov chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
27.3.2 Structured Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
27.3.3 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498

DRAFT March 9, 2010

XVII

CONTENTS

CONTENTS

27.4 Markov Chain Monte Carlo (MCMC)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
27.4.1 Markov chains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
27.4.2 Metropolis-Hastings sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
27.5 Auxiliary Variable Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
27.5.1 Hybrid Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
27.5.2 Swendson-Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
27.5.3 Slice sampling
27.6 Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
27.6.1 Sequential importance sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
27.6.2 Particle ﬁltering as an approximate forward pass . . . . . . . . . . . . . . . . . . . . 509
27.7 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
27.8 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512

28 Deterministic Approximate Inference

515
28.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
28.2 The Laplace approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
28.3 Properties of Kullback-Leibler Variational Inference
. . . . . . . . . . . . . . . . . . . . . . 516
28.3.1 Bounding the normalisation constant . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
28.3.2 Bounding the marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
. . . . . . . . . . . . . . . . . . . . . 517
28.3.3 Gaussian approximations using KL divergence
. . . . . . . . . . . . . . . . . . 518
28.3.4 Moment matching properties of minimising KL(p|q)
28.4 Variational Bounding Using KL(q|p) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
28.4.1 Pairwise Markov random ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
28.4.2 General mean ﬁeld equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
28.4.3 Asynchronous updating guarantees approximation improvement
. . . . . . . . . . . 522
28.4.4 Intractable energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
28.4.5 Structured variational approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 524
28.5 Mutual Information Maximisation : A KL Variational Approach . . . . . . . . . . . . . . . 524
28.5.1 The information maximisation algorithm . . . . . . . . . . . . . . . . . . . . . . . . 525
28.5.2 Linear Gaussian decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526
28.6 Loopy Belief Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526
28.6.1 Classical BP on an undirected graph . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
28.6.2 Loopy BP as a variational procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
28.7 Expectation Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
28.8 MAP for MRFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533
28.8.1 MAP assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533
28.8.2 Attractive binary MRFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
28.8.3 Potts model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
28.9 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
28.10Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
28.11Exercises

A Background Mathematics

543
A.1 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
A.1.1 Vector algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
A.1.2 The scalar product as a projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
A.1.3 Lines in space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
A.1.4 Planes and hyperplanes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
A.1.5 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
A.1.6 Linear transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
A.1.7 Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
A.1.8 Matrix inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
A.1.9 Computing the matrix inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
A.1.10 Eigenvalues and eigenvectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
A.1.11 Matrix decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550

XVIII

DRAFT March 9, 2010

CONTENTS

CONTENTS

A.5.1 Critical points

A.2 Matrix Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
A.3 Multivariate Calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
A.3.1 Interpreting the gradient vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552
A.3.2 Higher derivatives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552
A.3.3 Chain rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
A.3.4 Matrix calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
A.4 Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
A.4.1 Convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
A.4.2 Jensen’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
A.5 Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
A.6 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
A.6.1 Gradient descent with ﬁxed stepsize . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
A.6.2 Gradient descent with momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
A.6.3 Gradient descent with line searches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
A.6.4 Exact line search condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
A.7 Multivariate Minimization: Quadratic functions . . . . . . . . . . . . . . . . . . . . . . . . . 557
A.7.1 Minimising quadratic functions using line search . . . . . . . . . . . . . . . . . . . . 557
A.7.2 Gram-Schmidt construction of conjugate vectors
. . . . . . . . . . . . . . . . . . . . 558
A.7.3 The conjugate vectors algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
A.7.4 The conjugate gradients algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
A.7.5 Newton’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
A.7.6 Quasi-Newton methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
A.7 Constrained Optimisation using Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . . . 562

DRAFT March 9, 2010

XIX

CONTENTS

CONTENTS

XX

DRAFT March 9, 2010

Part I

Inference in Probabilistic Models

1

CHAPTER 1

Probabilistic Reasoning

1.1 Probability Refresher

Variables, States and Notational Shortcuts

Variables will be denoted using either upper case X or lower case x and a set of variables will typically be
denoted by a calligraphic symbol, for example V = {a, B, c} .
The domain of a variable x is written dom(x), and denotes the states x can take. States will typically
be represented using sans-serif font. For example, for a coin c, we might have dom(c) = {heads, tails} and
p(c = heads) represents the probability that variable c is in state heads.

The meaning of p(state) will often be clear, without speciﬁc reference to a variable. For example, if we are
discussing an experiment about a coin c, the meaning of p(heads) is clear from the context, being short-
x f(x), the

hand for p(c = heads). When summing (or performing some other operation) over a variable(cid:80)
interpretation is that all states of x are included, i.e. (cid:80)

(cid:80)

x f(x) ≡

s∈dom(x) f(x = s).

For our purposes, events are expressions about random variables, such as Two heads in 6 coin tosses.
Two events are mutually exclusive if they cannot both simultaneously occur. For example the events
The coin is heads and The coin is tails are mutually exclusive. One can think of deﬁning a new variable
named by the event so, for example, p(The coin is tails) can be interpreted as p(The coin is tails = true).
We use p(x = tr) for the probability of event/variable x being in the state true and p(x = fa) for the
probability of event/variable x being in the state false.

The Rules of Probability

Deﬁnition 1 (Rules of Probability (Discrete Variables)).

The probability of an event x occurring is represented by a value between 0 and 1.

p(x) = 1 means that we are certain that the event does occur.

Conversely, p(x) = 0 means that we are certain that the event does not occur.

The summation of the probability over all the states is 1:

(cid:88)

p(x = x) = 1

(1.1.1)

x

3

Such probabilities are normalised. We will usually more conveniently write(cid:80)

x p(x) = 1.

Probability Refresher

Two events x and y can interact through

p(x or y) = p(x) + p(y) − p(x and y)

(1.1.2)

We will use the shorthand p(x, y) for p(x and y). Note that p(y, x) = p(x, y) and p(x or y) = p(y or x).

Deﬁnition 2 (Set notation). An alternative notation in terms of set theory is to write

p(x or y) ≡ p(x ∪ y),

p(x, y) ≡ p(x ∩ y)

(1.1.3)

Deﬁnition 3 (Marginals). Given a joint distribution p(x, y) the distribution of a single variable is given
by

p(x, y)

(1.1.4)

p(x) =(cid:88)

y

Here p(x) is termed a marginal of the joint probability distribution p(x, y). The process of computing a
marginal from a joint distribution is called marginalisation. More generally, one has

p(x1, . . . , xi−1, xi+1, . . . , xn) =(cid:88)

p(x1, . . . , xn)

(1.1.5)

xi

An important deﬁnition that will play a central role in this book is conditional probability.

Deﬁnition 4 (Conditional Probability / Bayes’ Rule). The probability of event x conditioned on knowing
event y (or more shortly, the probability of x given y) is deﬁned as

p(x|y) ≡

p(x, y)
p(y)

If p(y) = 0 then p(x|y) is not deﬁned.

Probability Density Functions

(1.1.6)

Deﬁnition 5 (Probability Density Functions). For a single continuous variable x, the probability density
p(x) is deﬁned such that

(cid:90) ∞
p(x) ≥ 0

p(x)dx = 1

−∞

4

(1.1.7)

(1.1.8)

DRAFT March 9, 2010

Probability Refresher

(cid:90) b

a

(cid:90) 1.7

−1

p(a < x < b) =

As shorthand we will sometimes write(cid:82) b

p(x)dx

x=a p(x), particularly when we want an expression to be valid for
either continuous or discrete variables. The multivariate case is analogous with integration over all real
space, and the probability that x belongs to a region of the space deﬁned accordingly.

(1.1.9)

For continuous variables, formally speaking, events are deﬁned for the variable occurring within a deﬁned
region, for example

p(x ∈ [−1, 1.7]) =

f(x)dx

(1.1.10)

where here f(x) is the probability density function (pdf) of the continuous random variable x. Unlike
probabilities, probability densities can take positive values greater than 1.

appear strange, the nervous reader may simply replace our p(X = x) notation for(cid:82)

Formally speaking, for a continuous variable, one should not speak of the probability that x = 0.2 since the
probability of a single value is always zero. However, we shall often write p(x) for continuous variables,
thus not distinguishing between probabilities and probability density function values. Whilst this may
x∈∆ f(x)dx, where ∆
is a small region centred on x. This is well deﬁned in a probabilistic sense and, in the limit ∆ being very
small, this would give approximately ∆f(x). If we consistently use the same ∆ for all occurrences of pdfs,
then we will simply have a common prefactor ∆ in all expressions. Our strategy is to simply ignore these
values (since in the end only relative probabilities will be relevant) and write p(x). In this way, all the
standard rules of probability carry over, including Bayes’ Rule.

Interpreting Conditional Probability

Imagine a circular dart board, split into 20 equal sections, labelled from 1 to 20 and Randy, a dart thrower
who hits any one of the 20 sections uniformly at random. Hence the probability that a dart thrown by
Randy occurs in any one of the 20 regions is p(region i) = 1/20. A friend of Randy tells him that he
hasn’t hit the 20 region. What is the probability that Randy has hit the 5 region? Conditioned on this
information, only regions 1 to 19 remain possible and, since there is no preference for Randy to hit any
of these regions, the probability is 1/19. The conditioning means that certain states are now inaccessible,
and the original probability is subsequently distributed over the remaining accessible states. From the
rules of probability :

p(region 5|not region 20) = p(region 5, not region 20)

p(not region 20)

= p(region 5)

p(not region 20)

=

1/20
19/20

=

1
19

giving the intuitive result.
p(region 5).

In the above p(region 5, not region 20) = p(region {5 ∩ 1 ∩ 2∩, . . . ,∩19}) =

An important point to clarify is that p(A = a|B = b) should not be interpreted as ‘Given the event B = b
has occurred, p(A = a|B = b) is the probability of the event A = a occurring’. In most contexts, no such
explicit temporal causality is implied1 and the correct interpretation should be ‘ p(A = a|B = b) is the
probability of A being in state a under the constraint that B is in state b’.

constant since p(A = a, B = b) is not a distribution in A – in other words,(cid:80)
make it a distribution we need to divide : p(A = a, B = b)/(cid:80)

The relation between the conditional p(A = a|B = b) and the joint p(A = a, B = b) is just a normalisation
a p(A = a, B = b) (cid:54)= 1. To
a p(A = a, B = b) which, when summed

over a does sum to 1. Indeed, this is just the deﬁnition of p(A = a|B = b).

1We will discuss issues related to causality further in section(3.4).

DRAFT March 9, 2010

5

Probability Refresher

Deﬁnition 6 (Independence).

Events x and y are independent if knowing one event gives no extra information about the other event.
Mathematically, this is expressed by

p(x, y) = p(x)p(y)

Provided that p(x) (cid:54)= 0 and p(y) (cid:54)= 0 independence of x and y is equivalent to

p(x|y) = p(x) ⇔ p(y|x) = p(y)

(1.1.11)

(1.1.12)

If p(x|y) = p(x) for all states of x and y, then the variables x and y are said to be independent. If

p(x, y) = kf(x)g(y)

(1.1.13)

for some constant k, and positive functions f(·) and g(·) then x and y are independent.

Deterministic Dependencies

Sometimes the concept of independence is perhaps a little strange. Consider the following : variables x
and y are both binary (their domains consist of two states). We deﬁne the distribution such that x and y
are always both in a certain joint state:

p(x = a, y = 1) = 1
p(x = a, y = 2) = 0
p(x = b, y = 2) = 0
p(x = b, y = 1) = 0

Are x and y dependent? The reader may show that p(x = a) = 1, p(x = b) = 0 and p(y = 1) = 1,
p(y = 2) = 0. Hence p(x)p(y) = p(x, y) for all states of x and y, and x and y are therefore independent.
This may seem strange – we know for sure the relation between x and y, namely that they are always in
the same joint state, yet they are independent. Since the distribution is trivially concentrated in a single
joint state, knowing the state of x tells you nothing that you didn’t anyway know about the state of y,
and vice versa.

This potential confusion comes from using the term ‘independent’ which, in English, suggests that there is
no inﬂuence or relation between objects discussed. The best way to think about statistical independence
is to ask whether or not knowing the state of variable y tells you something more than you knew before
about variable x, where ‘knew before’ means working with the joint distribution of p(x, y) to ﬁgure out
what we can know about x, namely p(x).

1.1.1 Probability Tables

Based on the populations 60776238, 5116900 and 2980700 of England (E), Scotland (S) and Wales (W),
the a priori probability that a randomly selected person from these three countries would live in England,
Scotland or Wales, would be approximately 0.88, 0.08 and 0.04 respectively. We can write this as a vector
(or probability table) :

 p(Cnt = E)

p(Cnt = S)
p(Cnt = W)

 =

 0.88

0.08
0.04



whose component values sum to 1. The ordering of the components in this vector is arbitrary, as long as
it is consistently applied.

6

DRAFT March 9, 2010

(1.1.14)

Probability Refresher

For the sake of simplicity, let’s assume that only three Mother Tongue languages exist : English (Eng),
Scottish (Scot) and Welsh (Wel), with conditional probabilities given the country of residence, England
(E), Scotland (S) and Wales (W). We write a (ﬁctitious) conditional probability table

p(M T = Eng|Cnt = E) = 0.95 p(M T = Scot|Cnt = E) = 0.04 p(M T = Wel|Cnt = E) = 0.01
p(M T = Eng|Cnt = S) = 0.7
p(M T = Wel|Cnt = S) = 0.0
p(M T = Wel|Cnt = W) = 0.4
p(M T = Eng|Cnt = W) = 0.6
(1.1.15)

p(M T = Scot|Cnt = S) = 0.3
p(M T = Scot|Cnt = W) = 0.0

From this we can form a joint distribution p(Cnt, M T ) = p(M T|Cnt)p(Cnt). This could be written as a
3 × 3 matrix with (say) rows indexed by country and columns indexed by Mother Tongue:

 0.95 × 0.88 0.7 × 0.08 0.6 × 0.04

0.04 × 0.88 0.3 × 0.08 0.0 × 0.04
0.01 × 0.88 0.0 × 0.08 0.4 × 0.04

 =

 0.836

0.056 0.024

0.0352 0.024
0.0088

0

0

0.016



(1.1.16)

The joint distribution contains all the information about the model of this environment. By summing a
column of this table, we have the marginal p(Cnt). Summing the row gives the marginal p(M T ). Similarly,
one could easily infer p(Cnt|M T ) ∝ p(Cnt|M T )p(M T ) from this joint distribution.
For joint distributions over a larger number of variables, xi, i = 1, . . . , D, with each variable xi taking
i=1 Ki entries. Explicitly storing
tables therefore requires space exponential in the number of variables, which rapidly becomes impractical
for a large number of variables.

Ki states, the table describing the joint distribution is an array with(cid:81)D

A probability distribution assigns a value to each of the joint states of the variables. For this reason,
p(T, J, R, S) is considered equivalent to p(J, S, R, T ) (or any such reordering of the variables), since in each
case the joint setting of the variables is simply a diﬀerent index to the same probability. This situation is
more clear in the set theoretic notation p(J ∩ S ∩ T ∩ R). We abbreviate this set theoretic notation by
using the commas – however, one should be careful not to confuse the use of this indexing type notation
with functions f(x, y) which are in general dependent on the variable order. Whilst the variables to the
left of the conditioning bar may be written in any order, and equally those to the right of the conditioning
bar may be written in any order, moving variables across the bar is not generally equivalent, so that
p(x1|x2) (cid:54)= p(x2|x1).
1.1.2

Interpreting Conditional Probability

Together with the rules of probability, conditional probability enables one to reason in a rational, logical
and consistent way. One could argue that much of science deals with problems of the form : tell me
something about the parameters θ given that I have observed data D and have some knowledge of the
underlying data generating mechanism. From a modelling perspective, this requires

p(θ|D) = p(D|θ)p(θ)
p(D)

(cid:82)
= p(D|θ)p(θ)
θ p(D|θ)p(θ)

(1.1.17)

This shows how from a forward or generative model p(D|θ) of the dataset, and coupled with a prior
belief p(θ) about which parameter values are appropriate, we can infer the posterior distribution p(θ|D)
of parameters in light of the observed data.

This use of a generative model sits well with physical models of the world which typically postulate how to
generate observed phenomena, assuming we know the correct parameters of the model. For example, one
might postulate how to generate a time-series of displacements for a swinging pendulum but with unknown
mass, length and damping constant. Using this generative model, and given only the displacements, we
could infer the unknown physical properties of the pendulum, such as its mass, length and friction damping
constant.

DRAFT March 9, 2010

7

Probabilistic Reasoning

Subjective Probability

Probability is a contentious topic and we do not wish to get bogged down by the debate here, apart from
pointing out that it is not necessarily the axioms of probability that are contentious rather what interpre-
tation we should place on them. In some cases potential repetitions of an experiment can be envisaged so
that the ‘long run’ (or frequentist) deﬁnition of probability in which probabilities are deﬁned with respect
to a potentially inﬁnite repetition of ‘experiments’ makes sense. For example, in coin tossing, the proba-
bility of heads might be interpreted as ‘If I were to repeat the experiment of ﬂipping a coin (at ‘random’),
the limit of the number of heads that occurred over the number of tosses is deﬁned as the probability of
a head occurring.

Here’s another problem that is typical of the kind of scenario one might face in a machine learning
situation. A ﬁlm enthusiast joins a new online ﬁlm service. Based on expressing a few ﬁlms a user likes
and dislikes, the online company tries to estimate the probability that the user will like each of the 10000
ﬁlms in their database. If we were to deﬁne probability as a limiting case of inﬁnite repetitions of the same
experiment, this wouldn’t make much sense in this case since we can’t repeat the experiment. However,
if we assume that the user behaves in a manner consistent with other users, we should be able to exploit
the large amount of data from other users’ ratings to make a reasonable ‘guess’ as to what this consumer
likes. This degree of belief or Bayesian subjective interpretation of probability sidesteps non-repeatability
issues – it’s just a consistent framework for manipulating real values consistent with our intuition about
probability[145].

1.2 Probabilistic Reasoning

The axioms of probability, combined with Bayes’ rule make for a complete reasoning system, one which
includes traditional deductive logic as a special case[145].

Remark 1. The central paradigm of probabilistic reasoning is to identify all relevant variables
x1, . . . , xN in the environment, and make a probabilistic model p(x1, . . . , xN ) of their interaction.
Reasoning (inference) is then performed by introducing evidence 2 that sets variables in known states,
and subsequently computing probabilities of interest, conditioned on this evidence.

Example 1 (Hamburgers). Consider the following ﬁctitious scientiﬁc information: Doctors ﬁnd that peo-
ple with Kreuzfeld-Jacob disease (KJ) almost invariably ate hamburgers, thus p(Hamburger Eater|KJ ) =
0.9. The probability of an individual having KJ is currently rather low, about one in 100,000.

1. Assuming eating lots of hamburgers is rather widespread, say p(Hamburger Eater) = 0.5, what is

the probability that a hamburger eater will have Kreuzfeld-Jacob disease?
This may be computed as

p(KJ |Hamburger Eater) = p(Hamburger Eater, KJ )

p(Hamburger Eater)

= p(Hamburger Eater|KJ )p(KJ )

p(Hamburger Eater)

=

9

10 × 1

100000
1
2

= 1.8 × 10−5

(1.2.1)

(1.2.2)

2. If the fraction of people eating hamburgers was rather small, p(Hamburger Eater) = 0.001, what is
the probability that a regular hamburger eater will have Kreuzfeld-Jacob disease? Repeating the
above calculation, this is given by

9

10 × 1

100000
1

1000

8

≈ 1/100

(1.2.3)

DRAFT March 9, 2010

Probabilistic Reasoning

Intuitively, this is much higher than in scenario (1) since here we can be more sure that eating
hamburgers is related to the illness. In this case only a small number of people in the population
eat hamburgers, and most of them get ill.

Example 2 (Inspector Clouseau). Inspector Clouseau arrives at the scene of a crime. The victim lies
dead in the room and the inspector quickly ﬁnds the murder weapon, a Knife (K). The Butler (B) and
Maid (M) are his main suspects. The inspector has a prior belief of 0.8 that the Butler is the murderer,
and a prior belief of 0.2 that the Maid is the murderer. These probabilities are independent in the sense
that p(B, M) = p(B)p(M). (It is possible that both the Butler and the Maid murdered the victim or
neither). The inspector’s prior criminal knowledge can be formulated mathematically as follows:

dom(B) = dom(M) = {murderer, not murderer} , dom(K) = {knife used, knife not used}
p(B = murderer) = 0.8,
p(knife used|B = not murderer, M = not murderer) = 0.3
= 0.2
p(knife used|B = not murderer, M = murderer)
M = not murderer) = 0.6
p(knife used|B = murderer,
p(knife used|B = murderer,
M = murderer)
= 0.1

p(M = murderer) = 0.2

(1.2.4)

(1.2.5)

(1.2.6)

What is the probability that the Butler is the murderer? (Remember that it might be that neither is the
murderer). Using b for the two states of B and m for the two states of M,

p(B|K) =(cid:88)

p(B, m|K) =(cid:88)

m

m

Plugging in the values we have

p(B = murderer|knife used) =

8
10

= p(B)(cid:80)
(cid:80)
b p(b)(cid:80)
(cid:0) 2
(cid:1) + 2
10 × 1
10 × 6

10 + 8

10

10

m p(K|B, m)p(m)
m p(K|b, m)p(m)
(cid:1)
(cid:0) 2
10 × 6
10 × 2

10 + 8

10

10 × 3

10

p(B, m, K)

p(K)

(cid:0) 2
10 × 1

8
10

10 + 8

(1.2.7)

(cid:1) =

200
228 ≈ 0.877 (1.2.8)

The role of p(knife used) in the Inspector Clouseau example can cause some confusion. In the above,

p(knife used|b, m)p(m)

(1.2.9)

p(knife used) =(cid:88)

p(b)(cid:88)

b

m

is computed to be 0.456. But surely, p(knife used) = 1, since this is given in the question! Note that the
quantity p(knife used) relates to the prior probability the model assigns to the knife being used (in the
absence of any other information). If we know that the knife is used, then the posterior

p(knife used|knife used) = p(knife used, knife used)

p(knife used)

= p(knife used)
p(knife used)

= 1

which, naturally, must be the case.

Another potential confusion is the choice

p(B = murderer) = 0.8,

p(M = murderer) = 0.2

(1.2.10)

(1.2.11)

which means that p(B = not murderer) = 0.2, p(M = not murderer) = 0.8. These events are not exclusive
and it’s just ‘coincidence’ that the numerical values are chosen this way. For example, we could have also
chosen

p(B = murderer) = 0.6,

p(M = murderer) = 0.9

which means that p(B = not murderer) = 0.4, p(M = not murderer) = 0.1

DRAFT March 9, 2010

(1.2.12)

9

p(θ)

(cid:124)(cid:123)(cid:122)(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
p(D|θ)
(cid:124)(cid:123)(cid:122)(cid:125)
p(D)

prior

likelihood

evidence

(cid:124) (cid:123)(cid:122) (cid:125)
p(θ|D)

posterior

=

(1.3.1)

The evidence is also called the marginal likelihood.

The term likelihood is used for the probability that a model generates observed data. More fully, if we
condition on the model M, we have

p(θ|D, M) = p(D|θ, M)p(θ|M)

p(D|M)

where we see the role of the likelihood p(D|θ, M) and marginal likelihood p(D|M). The marginal
likelihood is also called the model likelihood.

The most probable a posteriori
argmax

p(θ|D, M).

θ

(MAP) setting is that which maximises the posterior, θ∗ =

Bayes’ rule tells us how to update our prior knowledge with the data generating mechanism. The prior
distribution p(θ) describes the information we have about the variable before seeing any data. After data
D arrives, we update the prior distribution to the posterior p(θ|D) ∝ p(D|θ)p(θ).
1.3.1 Two dice : what were the individual scores?

Two fair dice are rolled. Someone tells you that the sum of the two scores is 9. What is the probability
distribution of the two dice scores3?

The score of die a is denoted sa with dom(sa) = {1, 2, 3, 4, 5, 6} and similarly for sb. The three variables
involved are then sa, sb and the total score, t = sa + sb. A model of these three variables naturally takes
the form

(cid:123)(cid:122)
(cid:125)
p(t, sa, sb) = p(t|sa, sb)

(cid:124)

likelihood

(cid:124) (cid:123)(cid:122) (cid:125)

p(sa, sb)

prior

1.3 Prior, Likelihood and Posterior

The prior, likelihood and posterior are all probabilities. They are assigned these names due to their role
in Bayes’ rule, described below.

Prior, Likelihood and Posterior

Deﬁnition 7. Prior Likelihood and Posterior

For data D and variable θ, Bayes’ rule tells us how to update our prior beliefs about the variable θ in light
of the data to a posterior belief:

The prior p(sa, sb) is the joint probability of score sa
and score sb without knowing anything else. Assum-
ing no dependency in the rolling mechanism,

p(sa, sb) = p(sa)p(sb)

(1.3.3)

Since the dice are fair both p(sa) and p(sb) are uni-
form distributions, p(sa = s) = 1/6.

3This example is due to Taylan Cemgil.

sa = 1
1/36
1/36
1/36
1/36
1/36
1/36

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

p(sa)p(sb):

sa = 2
1/36
1/36
1/36
1/36
1/36
1/36

sa = 3
1/36
1/36
1/36
1/36
1/36
1/36

sa = 4
1/36
1/36
1/36
1/36
1/36
1/36

10

DRAFT March 9, 2010

(1.3.2)

sa = 5
1/36
1/36
1/36
1/36
1/36
1/36

sa = 6
1/36
1/36
1/36
1/36
1/36
1/36

Further worked examples

Here the likelihood term is

p(t|sa, sb) = I [t = sa + sb]

(1.3.4)

which states that the total score is given by sa +
sb. Here I [x = y] is the indicator function deﬁned as
I [x = y] = 1 if x = y and 0 otherwise.

sa = 1

0
0
0
0
0
0

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

p(t = 9|sa, sb):

sa = 3

sa = 2

sa = 4

sa = 5

sa = 6

0
0
0
0
0
0

0
0
0
0
0
1

0
0
0
0
1
0

0
0
0
1
0
0

0
0
1
0
0
0

Hence, our complete model is

p(t, sa, sb) = p(t|sa, sb)p(sa)p(sb)

(1.3.5)

where the terms on the right are explicitly deﬁned.

Our interest is then obtainable using Bayes’ rule,

(1.3.6)

where

p(t = 9)

p(sa, sb|t = 9) = p(t = 9|sa, sb)p(sa)p(sb)
p(t = 9) = (cid:88)
The term p(t = 9) =(cid:80)

sa,sb

p(t = 9|sa, sb)p(sa)p(sb) (1.3.7)

p(t = 9|sa, sb)p(sa)p(sb):

sa = 2

sa = 3

sa = 1

sa = 4

sa = 5

0
0
0
0
0
0

0
0
0
0
0
0

0
0
0
0
0

1/36

0
0
0
0

1/36

0

0
0
0

1/36

0
0

sa = 6

0
0

1/36

0
0
0

p(sa, sb|t = 9):

sa = 3

sa = 2

sa = 4

sa = 5

sa = 6

0
0
0
0
0
0

0
0
0
0
0
1/4

0
0
0
0
1/4
0

0
0
0
1/4
0
0

0
0
1/4
0
0
0

sa = 1

0
0
0
0
0
0

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

equal mass in only 4 non-zero elements, as shown.

sa,sb

p(t = 9|sa, sb)p(sa)p(sb) = 4 × 1/36 = 1/9. Hence the posterior is given by

1.4 Further worked examples

Example 3 (Who’s in the bathroom?). Consider a household of three people, Alice, Bob and Cecil.
Cecil wants to go to the bathroom but ﬁnds it occupied. He then goes to Alice’s room and sees she is
there. Since Cecil knows that only either Alice or Bob can be in the bathroom, from this he infers that
Bob must be in the bathroom.

To arrive at the same conclusion in a mathematical framework, let’s deﬁne the following events

A = Alice is in her bedroom, B = Bob is in his bedroom, O = Bathroom occupied

(1.4.1)

We can encode the information that if either Alice or Bob are not in their bedrooms, then they must be
in the bathroom (they might both be in the bathroom) as

p(O = tr|A = fa, B) = 1,

p(O = tr|A, B = fa) = 1

(1.4.2)

The ﬁrst term expresses that the bathroom is occupied if Alice is not in her bedroom, wherever Bob is.
Similarly, the second term expresses bathroom occupancy as long as Bob is not in his bedroom. Then

p(B = fa|O = tr, A = tr) = p(B = fa, O = tr, A = tr)

p(O = tr, A = tr)

where

= p(O = tr|A = tr, B = fa)p(A = tr, B = fa)

p(O = tr, A = tr)

p(O = tr, A = tr) = p(O = tr|A = tr, B = fa)p(A = tr, B = fa)

+ p(O = tr|A = tr, B = tr)p(A = tr, B = tr)

DRAFT March 9, 2010

(1.4.3)

(1.4.4)

11

Further worked examples

Using the fact p(O = tr|A = tr, B = fa) = 1 and p(O = tr|A = tr, B = tr) = 0, which encodes that if if
Alice is in her room and Bob is not, the bathroom must be occupied, and similarly, if both Alice and Bob
are in their rooms, the bathroom cannot be occupied,

p(B = fa|O = tr, A = tr) = p(A = tr, B = fa)

p(A = tr, B = fa)

= 1

(1.4.5)

This example is interesting since we are not required to make a full probabilistic model in this case thanks
to the limiting nature of the probabilities (we don’t need to specify p(A, B)). The situation is common in
limiting situations of probabilities being either 0 or 1, corresponding to traditional logic systems.

Example 4 (Aristotle : Resolution). We can represent the statement ‘All apples are fruit’ by p(F = tr|A =
tr) = 1. Similarly, ‘All fruits grow on trees’ may be represented by p(T = tr|F = tr) = 1. Additionally
we assume that whether or not something grows on a tree depends only on whether or not it is a fruit,
p(T|A, F ) = P (T|F ). From these, we can compute

p(T = tr|A = tr) =(cid:88)
(cid:124)

(cid:125)
= p(T = tr|F = fa) p(F = fa|A = tr)

p(T = tr|F, A = tr)p(F|A = tr) =(cid:88)
(cid:124)
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:125)
p(F = tr|A = tr)
+ p(T = tr|F = tr)

p(T = tr|F )p(F|A = tr)
(cid:123)(cid:122)

(1.4.6)

(1.4.7)

= 1

F

=0

=1

F

(cid:123)(cid:122)

=1

In other words we have deduced that ‘All apples grow on trees’ is a true statement, based on the information
presented. (This kind of reasoning is called resolution and is a form of transitivity : from the statements
A ⇒ F and F ⇒ T we can infer A ⇒ T ).

Example 5 (Aristotle : Inverse Modus Ponens). According to Logic, from the statement : ‘If A is true then
B is true’, one may deduce that ‘if B is false then A is false’. Let’s see how this ﬁts in with a probabilistic
reasoning system. We can express the statement : ‘If A is true then B is true’ as p(B = tr|A = tr) = 1.
Then we may infer

p(A = fa|B = fa) = 1 − p(A = tr|B = fa)

= 1 −

p(B = fa|A = tr)p(A = tr)

p(B = fa|A = tr)p(A = tr) + p(B = fa|A = fa)p(A = fa)

= 1

(1.4.8)

This follows since p(B = fa|A = tr) = 1 − p(B = tr|A = tr) = 1 − 1 = 0, annihilating the second term.

Both the above examples are intuitive expressions of deductive logic. The standard rules of Aristotelian
logic are therefore seen to be limiting cases of probabilistic reasoning.

Example 6 (Soft XOR Gate).
A standard XOR logic gate is given by the table on the right. If we
observe that the output of the XOR gate is 0, what can we say about
A and B? In this case, either A and B were both 0, or A and B were
both 1. This means we don’t know which state A was in – it could
equally likely have been 1 or 0.

A B A xor B
0
0
1
1

0
1
0
1

0
1
1
0

12

DRAFT March 9, 2010

Further worked examples

Consider a ‘soft’ version of the XOR gate given on the right,
with additionally p(A = 1) = 0.65, p(B = 1) = 0.77. What is
p(A = 1|C = 0)?

p(A = 1, C = 0) =(cid:88)

p(A = 1, B, C = 0) =(cid:88)

B

B

p(C = 0|A = 1, B)p(A = 1)p(B)

A B p(C = 1|A, B)
0
0
1
1

0.1
0.99
0.8
0.25

0
1
0
1

= p(A = 1) (p(C = 0|A = 1, B = 0)p(B = 0) + p(C = 0|A = 1, B = 1)p(B = 1))
= 0.65 × (0.2 × 0.23 + 0.75 × 0.77) = 0.4053

(1.4.9)

p(A = 0, C = 0) =(cid:88)

p(A = 0, B, C = 0) =(cid:88)

B

B

p(C = 0|A = 0, B)p(A = 0)p(B)

= p(A = 0) (p(C = 0|A = 0, B = 0)p(B = 0) + p(C = 0|A = 0, B = 1)p(B = 1))
= 0.35 × (0.9 × 0.23 + 0.01 × 0.77) = 0.0751

Then

p(A = 1|C = 0) =

p(A = 1, C = 0)

p(A = 1, C = 0) + p(A = 0, C = 0)

=

0.4053

0.4053 + 0.0751

= 0.8437

(1.4.10)

Example 7 (Larry). Larry is typically late for school. If Larry is late, we denote this with L = late,
otherwise, L = not late. When his mother asks whether or not he was late for school he never admits to
being late. The response Larry gives RL is represented as follows

p(RL = not late|L = not late) = 1,

p(RL = late|L = late) = 0

The remaining two values are determined by normalisation and are

p(RL = late|L = not late) = 0,

p(RL = not late|L = late) = 1

(1.4.11)

(1.4.12)

Given that RL = not late, what is the probability that Larry was late, i.e. p(L = late|RL = not late)?
Using Bayes’ we have

p(L = late|RL = not late) = p(L = late, RL = not late)

p(RL = not late)

=

p(L = late, RL = not late)

p(L = late, RL = not late) + p(L = not late, RL = not late)

In the above

(cid:125)
p(L = late, RL = not late) = p(RL = not late|L = late)

(cid:123)(cid:122)

(cid:124)

=1

p(L = late)

(1.4.13)

(1.4.14)

and

(cid:125)
p(L = not late, RL = not late) = p(RL = not late|L = not late)

(cid:123)(cid:122)

(cid:124)

=1

p(L = not late)

(1.4.15)

Hence

p(L = late|RL = not late) =

p(L = late)

p(L = late) + p(L = not late)

= p(L = late)

(1.4.16)

DRAFT March 9, 2010

13

Where we used normalisation in the last step, p(L = late) + p(L = not late) = 1. This result is intuitive –
Larry’s mother knows that he never admits to being late, so her belief about whether or not he really was
late is unchanged, regardless of what Larry actually says.

Further worked examples

Example 8 (Larry and Sue). Continuing the example above, Larry’s sister Sue always tells the truth to
her mother as to whether or not Larry was late for School.

p(RS = not late|L = not late) = 1,

p(RS = late|L = late) = 1
The remaining two values are determined by normalisation and are

p(RS = late|L = not late) = 0,

p(RS = not late|L = late) = 0
We also assume p(RS, RL|L) = p(RS|L)p(RL|L). We can then write

p(RL, RS, L) = p(RL|L)p(RS|L)p(L)

Given that RS = late, what is the probability that Larry was late?

Using Bayes’ rule, we have

(1.4.17)

(1.4.18)

(1.4.19)

p(L = late|RL = not late, RS = late) =

1
Z

p(RS = late|L = late)p(RL = not late|L = late)p(L = late)

where the normalisation Z is given by

p(RS = late|L = late)p(RL = not late|L = late)p(L = late)

+ p(RS = late|L = not late)p(RL = not late|L = not late)p(L = not late)

Hence

p(L = late|RL = not late, RS = late) =

1 × 1 × p(L = late)

1 × 1 × p(L = late) + 0 × 1 × p(L = not late)

= 1

(1.4.20)

This result is also intuitive – Since Larry’s mother knows that Sue always tells the truth, no matter what
Larry says, she knows he was late.

Example 9 (Luke). Luke has been told he’s lucky and has won a prize in the lottery. There are 5 prizes
available of value £10, £100, £1000, £10000, £1000000. The prior probabilities of winning these 5 prizes
are p1, p2, p3, p4, p5, with p0 being the prior probability of winning no prize. Luke asks eagerly ‘Did I win
£1000000?!’. ‘I’m afraid not sir’, is the response of the lottery phone operator. ‘Did I win £10000?!’ asks
Luke. ‘Again, I’m afraid not sir’. What is the probability that Luke has won £1000?

Note ﬁrst that p0 + p1 + p2 + p3 + p4 + p5 = 1. We denote W = 1 for the ﬁrst prize of £10, and W = 2, . . . , 5
for the remaining prizes and W = 0 for no prize. We need to compute

p(W = 3|W (cid:54)= 5, W (cid:54)= 4, W (cid:54)= 0) = p(W = 3, W (cid:54)= 5, W (cid:54)= 4, W (cid:54)= 0)

=

14

p(W (cid:54)= 5, W (cid:54)= 4, W (cid:54)= 0)
p(W = 1 or W = 2 or W = 3)

p(W = 3)

=

p3

p1 + p2 + p3

(1.4.21)

DRAFT March 9, 2010

Code

where the term in the denominator is computed using the fact that the events W are mutually exclusive
(one can only win one prize). This result makes intuitive sense : once we have removed the impossible
states of W , the probability that Luke wins the prize is proportional to the prior probability of that prize,
with the normalisation being simply the total set of possible probability remaining.

1.5 Code

The BRMLtoolbox code accompanying this book is intended to give the reader some insight into repre-
senting discrete probability tables and performing simple inference. The MATLAB code is written with
only minimal error trapping to keep the code as short and hopefully reasonably readable4.

1.5.1 Basic Probability code
At the simplest level, we only need two basic routines. One for multiplying probability tables together
(called potentials in the code), and one for summing a probability table. Potentials are represented using
a structure. For example, in the code corresponding to the Inspector Clouseau example demoClouseau.m,
we deﬁne a probability table as

>> pot(1)
ans =

variables: [1 3 2]

table: [2x2x2 double]

This says that the potential depends on the variables 1, 3, 2 and the entries are stored in the array given
by the table ﬁeld. The size of the array informs how many states each variable takes in the order given
by variables. The order in which the variables are deﬁned in a potential is irrelevant provided that
one indexes the array consistently. A routine that can help with setting table entries is setstate.m. For
example,

>> pot(1) = setstate(pot(1),[2 1 3],[2 1 1],0.3)

means that for potential 1, the table entry for variable 2 being in state 2, variable 1 being in state 1 and
variable 3 being in state 1 should be set to value 0.3.

The philosophy of the code is to keep the information required to perform computations to a minimum.
Additional information about the labels of variables and their domains can be useful to check results, but
is not actually required to carry out computations. One may also specify the name and domain of each
variable, for example

>>variable(3)
ans =

domain: {’murderer’ ’not murderer’}

name: ’butler’

The variable name and domain information in the Clouseau example is stored in the structure variable,
which can be helpful to display the potential table:

>> disptable(pot(1),variable);
knife
knife
knife
knife
knife
knife
knife
knife

used
not used
used
not used
used
not used
used
not used

=
=
=
=
=
=
=
=

maid
maid
maid
maid
maid
maid
maid
maid

= murderer
= murderer
= not murderer
= not murderer
= murderer
= murderer
= not murderer
= not murderer

butler
butler
butler
butler
butler
butler
butler
butler

=
=
=
=
=
=
=
=

murderer
murderer
murderer
murderer
not murderer
not murderer
not murderer
not murderer

0.100000
0.900000
0.600000
0.400000
0.200000
0.800000
0.300000
0.700000

4At the time of writing, some versions of MATLAB suﬀer from serious memory indexing bugs, some of which may appear
in the array structures used in the code provided. To deal with this, turn oﬀ the JIT accelerator by typing feature accel
off.

DRAFT March 9, 2010

15

Multiplying Potentials

In order to multiply potentials (as for arrays) the tables of each potential must be dimensionally consistent
– that is the number of states of variable i in potential 1 must match the number of states of variable i
in any other potential. This can be checked using potvariables.m. This consistency is also required for
other basic operations such as summing potentials.
multpots.m: Multiplying two or more potentials
divpots.m: Dividing a potential by another

Code

Summing a Potential

sumpot.m: Sum (marginalise) a potential over a set of variables
sumpots.m: Sum a set of potentials together

Making a conditional Potential

condpot.m: Make a potential conditioned on variables

Setting a Potential

setpot.m: Set variables in a potential to given states
setevpot.m: Set variables in a potential to given states and return also an identity potential on the given
states

The philosophy of BRMLtoolbox is that all information about variables is local and is read oﬀ from a
potential. Using setevpot.m enables one to set variables in a state whilst maintaining information about
the number of states of a variable.

Maximising a Potential

maxpot.m: Maximise a potential over a set of variables

See also maxNarray.m and maxNpot.m which return the N-highest values and associated states.

Other potential utilities

setstate.m: Set the a potential state to a given value
table.m: Return a table from a potential
whichpot.m: Return potentials which contain a set of variables
potvariables.m: Variables and their number of states in a set of potentials
orderpotfields.m: Order the ﬁelds of a potential structure
uniquepots.m: Merge redundant potentials and return only unique ones
numstates.m: Number of states of a variable in a domain
squeezepots.m: Remove redundant potentials by merging
normpot.m: Normalise a potential to form a distribution

1.5.2 General utilities
condp.m: Return a table p(x|y) from p(x, y)
condexp.m: Form a conditional distribution from a log value
logsumexp.m: Compute the log of a sum of exponentials in a numerically precise way
normp.m: Return a normalised table from an unnormalised table

16

DRAFT March 9, 2010

Exercises

assign.m: Assign values to multiple variables
maxarray.m: Maximize a multi-dimensional array over a subset

1.5.3 An example

The following code highlights the use of the above routines in solving the Inspector Clouseau, example(2).
demoClouseau.m: Solving the Inspector Clouseau example

1.6 Notes

The interpretation of probability is contentious and we refer the reader to [145, 183, 179] for detailed discus-
sions. A useful website that relates to understanding probability and Bayesian reasoning is understandinguncertainty.org.

1.7 Exercises

Exercise 1. Prove

p(x, y|z) = p(x|z)p(y|x, z)

and also

p(x|y, z) = p(y|x, z)p(x|z)

p(y|z)

Exercise 2. Prove the Bonferroni inequality

p(a, b) ≥ p(a) + p(b) − 1

(1.7.1)

(1.7.2)

(1.7.3)

Exercise 3 (Adapted from [167]). There are two boxes. Box 1 contains three red and ﬁve white balls and
box 2 contains two red and ﬁve white balls. A box is chosen at random p(box = 1) = p(box = 2) = 0.5 and
a ball chosen at random from this box turns out to be red. What is the posterior probability that the red
ball came from box 1?

Exercise 4 (Adapted from [167]). Two balls are placed in a box as follows: A fair coin is tossed and a
white ball is placed in the box if a head occurs, otherwise a red ball is placed in the box. The coin is tossed
again and a red ball is placed in the box if a tail occurs, otherwise a white ball is placed in the box. Balls
are drawn from the box three times in succession (always with replacing the drawn ball back in the box). It
is found that on all three occasions a red ball is drawn. What is the probability that both balls in the box
are red?

Exercise 5 (From David Spiegelhalter understandinguncertainty.org). A secret government agency
has developed a scanner which determines whether a person is a terrorist. The scanner is fairly reliable;
95% of all scanned terrorists are identiﬁed as terrorists, and 95% of all upstanding citizens are identiﬁed
as such. An informant tells the agency that exactly one passenger of 100 aboard an aeroplane in which
you are seated is a terrorist. The agency decide to scan each passenger and the shifty looking man sitting
next to you is the ﬁrst to test positive. What are the chances that this man is a terrorist?

Exercise 6 (The Monty Hall problem). On a gameshow there are three doors. Behind one door is a prize.
The gameshow host asks you to pick a door. He then opens a diﬀerent door to the one you chose and
shows that there is no prize behind it. Is is better to stick with your original guess as to where the prize
is, or better to change your mind?

Exercise 7. Consider three variable distributions which admit the factorisation

p(a, b, c) = p(a|b)p(b|c)p(c)

(1.7.4)

where all variables are binary. How many parameters are needed to specify distributions of this form?

DRAFT March 9, 2010

17

Exercise 8. Repeat the Inspector Clouseau scenario, example(2), but with the restriction that either the
Maid or the Butler is the murderer, but not both. Explicitly, the probability of the Maid being the Murder
and not the Butler is 0.04, the probability of the Butler being the Murder and not the Maid is 0.64. Modify
demoClouseau.m to implement this.

Exercise 9. Prove

Exercises

p(a, (b or c)) = p(a, b) + p(a, c) − p(a, b, c)

Exercise 10. Prove

p(x|z) =(cid:88)

p(x|y, z)p(y|z) =(cid:88)

y

y,w

p(x|w, y, z)p(w|y, z)p(y|z)

(1.7.5)

(1.7.6)

Exercise 11. As a young man Mr Gott visits Berlin in 1969. He’s surprised that he cannot cross into
East Berlin since there is a wall separating the two halves of the city. He’s told that the wall was erected
8 years previously. He reasons that : The wall will have a ﬁnite lifespan; his ignorance means that he
arrives uniformly at random at some time in the lifespan of the wall. Since only 5% of the time one would
arrive in the ﬁrst or last 2.5% of the lifespan of the wall he asserts that with 95% conﬁdence the wall will
survive between 8/0.975 ≈ 8.2 and 8/0.025 = 320 years. In 1989 the now Professor Gott is pleased to ﬁnd
that his prediction was correct and promotes his prediction method in elite journals. This ‘delta-t’ method
is widely adopted and used to form predictions in a range of scenarios about which researchers are ‘totally
ignorant’. Would you ‘buy’ a prediction from Prof. Gott? Explain carefully your reasoning.

Exercise 12. Implement the soft XOR gate, example(6) using BRMLtoolbox. You may ﬁnd condpot.m
of use.

Exercise 13. Implement the hamburgers, example(1) (both scenarios) using BRMLtoolbox. To do so you
will need to deﬁne the joint distribution p(hamburgers, KJ) in which dom(hamburgers) = dom(KJ) =
{tr, fa}.
Exercise 14. Implement the two-dice example, section(1.3.1) using BRMLtoolbox.

Exercise 15. A redistribution lottery involves picking the correct four numbers from 1 to 9 (without
replacement, so 3,4,4,1 for example is not possible). The order of the picked numbers is irrelevant. Every
week a million people play this game, each paying £1 to enter, with the numbers 3,5,7,9 being the most
popular (1 in every 100 people chooses these numbers). Given that the million pounds prize money is split
equally between winners, and that any four (diﬀerent) numbers come up at random, what is the expected
amount of money each of the players choosing 3,5,7,9 will win each week? The least popular set of numbers
is 1,2,3,4 with only 1 in 10,000 people choosing this. How much do they proﬁt each week, on average? Do
you think there is any ‘skill’ involved in playing this lottery?

Exercise 16. In a test of ‘psychometry’ the car keys and wrist watches of 5 people are given to a medium.
The medium then attempts to match the wrist watch with the car key of each person. What is the expected
number of correct matches that the medium will make (by chance)? What is the probability that the medium
will obtain at least 1 correct match?

18

DRAFT March 9, 2010

CHAPTER 2

Basic Graph Concepts

2.1 Graphs

Deﬁnition 8 (Graph). A graph G consists of vertices (nodes) and edges (links) between the vertices.
Edges may be directed (they have an arrow in a single direction) or undirected. A graph with all edges
directed is called a directed graph, and one with all edges undirected is called an undirected graph.

Deﬁnition 9 (Path, Ancestors, Descendants). A path A (cid:55)→ B from node A to node B is a sequence of
vertices A0 = A, A1, . . . , An−1, An = B, with (An, An+1) an edge in the graph, thereby connecting A to
B. For a directed graph this means that a path is a sequence of nodes which when we follow the direction
of the arrows leads us from A to B.

The vertices A such that A (cid:55)→ B and B (cid:54)
(cid:55)→ A are the descendants of A[168].
and B (cid:54)

(cid:55)→ A are the ancestors of B. The vertices B such that A (cid:55)→ B

Deﬁnition 10 (Directed Acyclic Graph (DAG)). A DAG is a graph G with directed edges (arrows on
each link) between the vertices (nodes) such that by following a path of vertices from one node to another
along the direction of each edge no path will revisit a vertex. In a DAG the ancestors of B are those
nodes who have a directed path ending at B. Conversely, the descendants of A are those nodes who have
a directed path starting at A.

Deﬁnition 11 (Relationships in a DAG).

x1

x8

x2

x4

x3

x7

x5

x6

The children of
The parents of x4 are pa (x4) = {x1, x2, x3}.
The family of a node is itself and its
x4 are ch (x4) = {x5, x6}.
parents. The Markov blanket of a node is itself, its parents, children
and the parents of its children. In this case, the Markov blanket of
x4 is x1, x2, . . . , x7.

19

c

a

f

b

g

d

(a)

e

c

b

g

e

a

f

d

(b)

Graphs

Figure 2.1:
(b) Multiply-Connected graph.

(a) Singly-Connected graph.

Deﬁnition 12 (Undirected graph).

A

D

B

C

E

An undirected graph G consists of undirected edges between nodes.

Deﬁnition 13 (Neighbour). For an undirected graph G the neighbours of x, ne (x) are those nodes
directly connected to x.

Deﬁnition 14 (Connected graph). An undirected graph is connected if there is a path between every
set of vertices (i.e. there are no isolated islands). For a graph which is not connected, the connected
components are those subgraphs which are connected.

Deﬁnition 15 (Clique).

A

D

B

C

E

Given an undirected graph, a clique is a maximally connected subset of
vertices. All the members of the clique are connected to each other; further-
more there is no larger clique that can be made from a clique. For example
this graph has two cliques, C1 = {A, B, C, D} and C2 = {B, C, E}. Whilst
A, B, C are fully connected, this is a non-maximal clique since there is a
larger fully connected set, A, B, C, D that contains this. A non-maximal
clique is sometimes called a cliquo.

Deﬁnition 16 (Singly-Connected Graph). A graph is singly-connected if there is only one path from a
vertex a to another vertex b. Otherwise the graph is multiply-connected. This deﬁnition applies regardless
of whether or not the edges in the graph are directed. An alternative name for a singly-connected graph
is a tree. A multiply-connected graph is also called loopy.

20

DRAFT March 9, 2010

Numerically Encoding Graphs

2.1.1 Spanning tree

Deﬁnition 17 (Spanning Tree).

A spanning tree of an undirected graph G is a singly-connected
subset of the existing edges such that the resulting singly-
connected graph covers all vertices of G. On the right is a graph
and an associated spanning tree. A maximum weight spanning
tree is a spanning tree such that the sum of all weights on the
edges of the tree is larger than for any other spanning tree of G.

Finding the maximal weight spanning tree

A simple algorithm to ﬁnd a spanning tree with maximal weight is as follows: Start by picking the edge
with the largest weight and add this to the edge set. Then pick the next candidate edge which has the
largest weight and add this to the edge set – if this results in an edge set with cycles, then reject the
candidate edge and ﬁnd the next largest edge weight. Note that there may be more than one maximal
weight spanning tree.

2.2 Numerically Encoding Graphs

To express the structure of GMs we need to numerically encode the links on the graphs. For a graph of
N vertices, we can describe the graph structure in various equivalent ways.

2.2.1 Edge list

As the name suggests, an edge list simply lists which vertex-vertex pairs are in the graph. For ﬁg(2.2a), an
edge list is L = {(1, 2), (2, 1), (1, 3), (3, 1), (2, 3), (3, 2), (2, 4), (4, 2), (3, 4), (4, 3)} where an undirected edge
is represented by a bidirectional edge.

2.2.2 Adjacency matrix

An alternative is to use an adjacency matrix

 0 1 1 0

1 0 1 1
1 1 0 1
0 1 1 0



A =

(2.2.1)

where Aij = 1 if there is an edge from variable i to j in the graph, and 0 otherwise. Some authors include
self-connections in this deﬁnition. An undirected graph has a symmetric adjacency matrix.

Provided that the vertices are labelled in ancestral order (parents always come before children) a directed
graph ﬁg(2.2b) can be represented as a triangular adjacency matrix:

(2.2.2)

21

 0 1 1 0

0 0 1 1
0 0 0 1
0 0 0 0



T =

DRAFT March 9, 2010

Numerically Encoding Graphs

1

2

3

(a)

4

1

2

3

(b)

4

Figure 2.2: (a): An undirected graph can be represented
as a symmetric adjacency matrix. (b): A directed graph
with vertices labelled in ancestral order corresponds to a
triangular adjacency matrix.

Adjacency matrix powers

For an N × N adjacency matrix A, powers of the adjacency matrix(cid:2)Ak(cid:3)
If we include 1’s on the diagonal of A then (cid:2)AN(cid:3)
in the graph. If A corresponds to a DAG the non-zero entries of the jth row of(cid:2)AN(cid:3) correspond to the

ij is non-zero when there is a path connecting j to i

are from node i to node j in k edge hops.

ij specify how many paths there

descendants of node j.

2.2.3 Clique matrix
For an undirected graph with N vertices and maximal cliques C1, . . . , CK a clique matrix is an n × K
matrix in which each column ck has zeros expect for ones on entries describing the clique. A cliquo matrix
relaxes the constraint that cliques are required to be maximal1. For example

1 1
1 1
0 1


 1 0
 1 1 0 0 0

1 0 1 1 0
0 1 1 0 1
0 0 0 1 1



C =

Cinc =

(2.2.3)

(2.2.4)

is a clique matrix for ﬁg(2.2a). A cliquo matrix containing only two-dimensional maximal cliques is called
an incidence matrix. For example

is an incidence matrix for ﬁg(2.2b).

It is straightforward to show that CincCT
inc is equal to the adjacency matrix except that the diagonals now
contain the degree of each vertex (the number of edges it touches). Similarly, for any cliquo matrix the
diagonal entry of [CCT]ii expresses the number of cliquos (columns) that vertex i occurs in. Oﬀ diagonal
elements [CCT]ij contain the number of cliquos that vertices i and j jointly inhabit.

Remark 2 (Graph Confusions). Graphs are widely used, but diﬀer markedly in what they represent.
Two potential pitfalls are described below.

State Transition Diagrams Such graphical representations are common in Markov Chains and Fi-
nite State Automata. A set of states is written as set of nodes(vertices) of a graph, and a directed
edge between node i and node j (with an associated weight pij) represents that a transition from
state i to state j can occur with probability pij. From the graphical models perspective we
would simply write down a directed graph x(t) → x(t + 1) to represent this Markov Chain. The
state-transition diagram simply provides a graphical description of the conditional probability
table p(x(t + 1)|x(t)).

1The term ‘cliquo’ for a non-maximal clique is attributed to Julian Besag.

22

DRAFT March 9, 2010

Exercises

Neural Networks Neural networks also have vertices and edges. In general, however, neural net-
works are graphical representations of functions, whereas as graphical models are representations
of distributions (a richer formalism). Neural networks (or any other parametric description) may
be used to represent the conditional probability tables, as in sigmoid belief networks[204].

2.3 Code

2.3.1 Utility routines

ancestors.m: Find the ancestors of a node in a DAG
edges.m: Edge list from an adjacency matrix
ancestralorder.m: Ancestral order from a DAG
connectedComponents.m: Connected Components

parents.m: Parents of a node given an adjacency matrix
children.m: Children of a node given an adjacency matrix
neigh.m: Neighbours of a node given an adjacency matrix

A connected graph is a tree if the number of edges plus 1 is equal to the number of nodes. However, for a
possibly disconnected graph this is not the case. The code below deals with the possibly disconnected case.
The routine is based on the observation that any singly-connected graph must always possess a simplical
node (a leaf node) which can be eliminated to reveal a smaller singly-connected graph.
istree.m: If graph is singly connected return 1 and elimination sequence
spantree.m: Return a spanning tree from an ordered edge list
singleparenttree.m: Find a directed tree with at most one parent from an undirected tree

Additional routines for basic manipulations in graphs are given at the end of chapter(6).

2.4 Exercises

j in one timestep, and 0 otherwise. Show that the matrix(cid:2)Ak(cid:3)

Exercise 17. Consider an adjacency matrix A with elements [A]ij = 1 if one can reach state i from state
ij represents the number of paths that lead
from state j to i in k timesteps. Hence derive an algorithm that will ﬁnd the minimum number of steps
to get from state j to state i.
Exercise 18. For an N × N symmetric adjacency matrix A, describe an algorithm to ﬁnd the connected
components. You may wish to examine connectedComponents.m.

Exercise 19. Show that for a connected graph that is singly-connected, the number of edges E must be
equal to the number of vertices minus 1, E = V − 1. Give an example graph with E = V − 1 that is not
singly-connected.

DRAFT March 9, 2010

23

Exercises

24

DRAFT March 9, 2010

CHAPTER 3

Belief Networks

3.1 Probabilistic Inference in Structured Distributions

Consider an environment composed of N variables, with a corresponding distribution p(x1, . . . , xN ). Writ-
ing E as the set of evidential variables and using evidence = {xe = xe, e ∈ E} to denote all available evidence,
then inference and reasoning can be carried out automatically by the ‘brute force’ method1

(cid:80)
x{\E,\i} p({xe = xe, e ∈ E} , xi = xi, x{\E,\i})

(cid:80)

x\E p({xe = xe, e ∈ E} , x\E)

p(xi = xi|evidence) =

(3.1.1)

If all variables are binary (take two states), these summations require O(2N−|E|) operations. Such expo-
nential computation is impractical and techniques that reduce this burden by exploiting any structure in
the joint probability table are the topic of our discussions on eﬃcient inference.

Naively specifying all the entries of a table p(x1, . . . , xN ) over binary variables xi takes O(2N ) space.
We will need to deal with large numbers of variables in machine learning and related application areas,
with distributions on potentially hundreds if not millions of variables. The only way to deal with such
large distributions is to constrain the nature of the variable interactions in some manner, both to render
speciﬁcation and ultimately inference in such systems tractable. The key idea is to specify which variables
are independent of others, leading to a structured factorisation of the joint probability distribution. Belief
Networks are a convenient framework for representing such factorisations into local conditional distribu-
tions. We will discuss Belief Networks more formally in section(3.3), ﬁrst discussing their natural graphical
representations of distributions.

D(cid:89)

i=1

Deﬁnition 18 (Belief Network). A Belief Network is a distribution of the form

p(x1, . . . , xD) =

p(xi|pa (xi))

(3.1.2)

where pa (xi) represent the parental variables of variable xi. Written as a Directed Graph, with an arrow
pointing from a parent variable to child variable, a Belief Network is a Directed Acyclic Graph (DAG),
with the ith vertex in the graph corresponding to the factor p(xi|pa (xi)).

1The extension to continuous variables is straightforward, replacing summation with integration over pdfs; we defer
treatment of this to later chapters, since our aim is to here outline more the intuitions without needing to deal with integration
of high dimensional distributions.

25

Graphically Representing Distributions

3.2 Graphically Representing Distributions

Belief Networks (also called Bayes’ Networks or Bayesian Belief Networks) are a way to depict the inde-
pendence assumptions made in a distribution [148, 168]. Their application domain is widespread, ranging
from troubleshooting[50] and expert reasoning under uncertainty to machine learning. Before we more
formally deﬁne a BN, an example will help motivate the development2.

3.2.1 Constructing a simple Belief network : wet grass

One morning Tracey leaves her house and realises that her grass is wet. Is it due to overnight rain or did
she forget to turn oﬀ the sprinkler last night? Next she notices that the grass of her neighbour, Jack,
is also wet. This explains away to some extent the possibility that her sprinkler was left on, and she
concludes therefore that it has probably been raining.

Making a model

We can model the above situation using probability by following a general modelling approach. First we
deﬁne the variables we wish to include in our model. In the above situation, the natural variables are

R ∈ {0, 1} (R = 1 means that it has been raining, and 0 otherwise).
S ∈ {0, 1} (S = 1 means that Tracey has forgotten to turn oﬀ the sprinkler, and 0 otherwise).
J ∈ {0, 1} (J = 1 means that Jack’s grass is wet, and 0 otherwise).
T ∈ {0, 1} (T = 1 means that Tracey’s Grass is wet, and 0 otherwise).
A model of Tracey’s world then corresponds to a probability distribution on the joint set of the variables
of interest p(T, J, R, S) (the order of the variables is irrelevant).

Since each of the variables in this example can take one of two states, it would appear that we naively
have to specify the values for each of the 24 = 16 states, e.g. p(T = 1, J = 0, R = 1, S = 1) = 0.7 etc.
However, since there are normalisation conditions for probabilities, we do not need to specify all the state
probabilities. To see how many states need to be speciﬁed, consider the following decomposition. Without
loss of generality (wlog) and repeatedly using Bayes’ rule, we may write

p(T, J, R, S) = p(T|J, R, S)p(J, R, S)

= p(T|J, R, S)p(J|R, S)p(R, S)
= p(T|J, R, S)p(J|R, S)p(R|S)p(S)

(3.2.1)
(3.2.2)
(3.2.3)

That is, we may write the joint distribution as a product of conditional distributions. The ﬁrst term
p(T|J, R, S) requires us to specify 23 = 8 values – we need p(T = 1|J, R, S) for the 8 joint states of J, R, S.
The other value p(T = 0|J, R, S) is given by normalisation : p(T = 0|J, R, S) = 1 − p(T = 1|J, R, S).
Similarly, we need 4 + 2 + 1 values for the other factors, making a total of 15 values in all. In general, for
a distribution on n binary variables, we need to specify 2n − 1 values in the range [0, 1]. The important
point here is that the number of values that need to be speciﬁed in general scales exponentially with the
number of variables in the model – this is impractical in general and motivates simpliﬁcations.

Conditional independence

The modeler often knows constraints on the system. For example, in the scenario above, we may assume
that Tracey’s grass is wet depends only directly on whether or not is has been raining and whether or not
her sprinkler was on. That is, we make a conditional independence assumption

p(T|J, R, S) = p(T|R, S)

2The scenario is adapted from [219].

26

(3.2.4)

DRAFT March 9, 2010

Graphically Representing Distributions

R

S

B

E

J

T

(a)

R

A

(b)

Figure 3.1: (a): Belief network structure for the
‘wet grass’ example. Each node in the graph
represents a variable in the joint distribution,
and the variables which feed in (the parents) to
another variable represent which variables are
(b): BN
to the right of the conditioning bar.
for the Burglar model.

Similarly, since whether or not Jack’s grass is wet is inﬂuenced only directly by whether or not it has been
raining, we write

p(J|R, S) = p(J|R)

and since the rain is not directly inﬂuenced by the sprinkler,

p(R|S) = p(R)

which means that our model now becomes :

p(T, J, R, S) = p(T|R, S)p(J|R)p(R)p(S)

(3.2.5)

(3.2.6)

(3.2.7)

We can represent these conditional independencies graphically, as in ﬁg(3.1a). This reduces the number
of values that we need to specify to 4 + 2 + 1 + 1 = 8, a saving over the previous 15 values in the case
where no conditional independencies had been assumed.

To complete the model, we need to numerically specify the values of each conditional probability table
(CPT). Let the prior probabilities for R and S be p(R = 1) = 0.2 and p(S = 1) = 0.1. We set the remaining
probabilities to p(J = 1|R = 1) = 1, p(J = 1|R = 0) = 0.2 (sometimes Jack’s grass is wet due to unknown
eﬀects, other than rain), p(T = 1|R = 1, S) = 1, p(T = 1|R = 0, S = 1) = 0.9 (there’s a small chance that
even though the sprinkler was left on, it didn’t wet the grass noticeably), p(T = 1|R = 0, S = 0) = 0.

Inference

Now that we’ve made a model of an environment, we can perform inference. Let’s calculate the probability
that the sprinkler was on overnight, given that Tracey’s grass is wet: p(S = 1|T = 1).
To do this, we use Bayes’ rule:

p(S = 1|T = 1) = p(S = 1, T = 1)

(cid:80)
(cid:80)

=

J,R p(T = 1, J, R, S = 1)
(cid:80)
J,R,S p(T = 1, J, R, S)
p(T = 1)
(cid:80)
J,R p(J|R)p(T = 1|R, S = 1)p(R)p(S = 1)
(cid:80)
J,R,S p(J|R)p(T = 1|R, S)p(R)p(S)
(cid:80)
R p(T = 1|R, S = 1)p(R)p(S = 1)
R,S p(T = 1|R, S)p(R)p(S)

=

=

=

0.9 × 0.8 × 0.1 + 1 × 0.2 × 0.1

0.9 × 0.8 × 0.1 + 1 × 0.2 × 0.1 + 0 × 0.8 × 0.9 + 1 × 0.2 × 0.9

(3.2.8)

(3.2.9)

(3.2.10)

= 0.3382

(3.2.11)

so the belief that the sprinkler is on increases above the prior probability 0.1, due to the fact that the
grass is wet.

Let us now calculate the probability that Tracey’s sprinkler was on overnight, given that her grass is wet
and that Jack’s grass is also wet, p(S = 1|T = 1, J = 1). We use Bayes’ rule again:
DRAFT March 9, 2010

27

Graphically Representing Distributions

p(S = 1|T = 1, J = 1) = p(S = 1, T = 1, J = 1)

(cid:80)
(cid:80)
p(T = 1, J = 1)
R p(T = 1, J = 1, R, S = 1)
(cid:80)
R,S p(T = 1, J = 1, R, S)
(cid:80)
R p(J = 1|R)p(T = 1|R, S = 1)p(R)p(S = 1)
R,S p(J = 1|R)p(T = 1|R, S)p(R)p(S)
= 0.1604

0.0344
0.2144

=

=

=

(3.2.12)

(3.2.13)

(3.2.14)

(3.2.15)

The probability that the sprinkler is on, given the extra evidence that Jack’s grass is wet, is lower than
the probability that the grass is wet given only that Tracey’s grass is wet. That is, that the grass is wet
due to the sprinkler is (partly) explained away by the fact that Jack’s grass is also wet – this increases the
chance that the rain has played a role in making Tracey’s grass wet.

Naturally, we don’t wish to carry out such inference calculations by hand all the time. General purpose
algorithms exist for this, such as the Junction Tree Algorithm, and we shall introduce these in later
chapters.

Example 10 (Was it the Burglar?). Here’s another example using binary variables, adapted from [219].
Sally comes home to ﬁnd that the burglar alarm is sounding (A = 1). Has she been burgled (B = 1), or
was the alarm triggered by an earthquake (E = 1)? She turns the car radio on for news of earthquakes
and ﬁnds that the radio broadcasts an earthquake alert (R = 1).

Using Bayes’ rule, we can write, without loss of generality,

p(B, E, A, R) = p(A|B, E, R)p(B, E, R)

We can repeat this for p(B, E, R), and continue

p(B, E, A, R) = p(A|B, E, R)p(R|B, E)p(E|B)p(B)

(3.2.16)

(3.2.17)

However, the alarm is surely not directly inﬂuenced by any report on the Radio – that is, p(A|B, E, R) =
p(A|B, E). Similarly, we can make other conditional independence assumptions such that

p(B, E, A, R) = p(A|B, E)p(R|E)p(E)p(B)
Specifying conditional probability tables

Alarm = 1 Burglar Earthquake

0.9999
0.99
0.99
0.0001

1
1
0
0

1
0
1
0

(3.2.18)

Radio = 1 Earthquake

1
0

1
0

The remaining tables are p(B = 1) = 0.01 and p(E = 1) = 0.000001. The tables and graphical structure
fully specify the distribution. Now consider what happens as we observe evidence.

Initial Evidence: The Alarm is sounding

p(B = 1|A = 1) =

=

(cid:80)
(cid:80)
E,R p(B = 1, E, A = 1, R)
(cid:80)
B,E,R p(B, E, A = 1, R)
(cid:80)
E,R p(A = 1|B = 1, E)p(B = 1)p(E)p(R|E)
B,E,R p(A = 1|B, E)p(B)p(E)p(R|E)

≈ 0.99

(3.2.19)

(3.2.20)

28

DRAFT March 9, 2010

Graphically Representing Distributions

Additional Evidence: The Radio broadcasts an Earthquake warning: A similar calculation
gives p(B = 1|A = 1, R = 1) ≈ 0.01. Thus, initially, because the Alarm sounds, Sally thinks that
she’s been burgled. However, this probability drops dramatically when she hears that there has been an
Earthquake. That is, the Earthquake ‘explains away’ to an extent the fact that the Alarm is ringing. See
demoBurglar.m.

3.2.2 Uncertain evidence

In soft or uncertain evidence, the variable is in more than one state, with the strength of our belief about
each state being given by probabilities. For example, if x has the states dom(x) = {red, blue, green} the
vector (0.6, 0.1, 0.3) represents the probabilities of the respective states. In contrast, for hard evidence we
are certain that a variable is in a particular state. In this case, all the probability mass is in one of the
vector components, for example (0, 0, 1).

Performing inference with soft-evidence is straightforward and can be achieved using Bayes’ rule. Writing
the soft evidence as ˜y, we have

p(x|y)p(y|˜y)

(3.2.21)

p(x|˜y) =(cid:88)

y

where p(y = i|˜y) represents the probability that y is in state i under the soft-evidence. This is a gener-
alisation of hard-evidence in which the vector p(y|˜y) has all zero component values, except for all but a
single component.

Note that the soft evidence p(y = i|˜y) does not correspond to the marginal p(y = i) in the original joint
distribution p(x, y). A procedure to form a joint distribution, known as Jeﬀrey’s rule is to begin with an
original distribution p1(x, y), from which we can deﬁne

(cid:80)

p1(x|y) = p1(x, y)
x p1(x, y)

Using the soft evidence p(y|˜y) we then deﬁne a new joint distribution

p2(x, y|˜y) = p1(x|y)p(y|˜y)

In the BN we use a dashed circle to represent that a variable is in a soft-evidence state.

(3.2.22)

(3.2.23)

Example 11 (soft-evidence). Revisiting the earthquake scenario, example(10), imagine that we think we
hear the burglar alarm sounding, but are not sure, speciﬁcally we are only 70% sure we heard the alarm.
For this binary variable case we represent this soft-evidence for the states (1, 0) as ˜A = (0.7, 0.3). What is
the probability of a burglary under this soft-evidence?

p(B = 1|A)p(A| ˜A) = p(B = 1|A = 1) × 0.7 + p(B = 1|A = 0) × 0.3

(3.2.24)

p(B = 1| ˜A) =(cid:88)

A

The probabilities p(B = 1|A = 1) ≈ 0.99 and p(B = 1|A = 0) ≈ 0.0001 are calculated using Bayes’ rule as
before to give

p(B = 1| ˜A) ≈ 0.6930

Uncertain evidence versus unreliable modelling

An entertaining example of uncertain evidence is given by Pearl[219]:

DRAFT March 9, 2010

(3.2.25)

29

Graphically Representing Distributions

B

A

G

W

B

A

H

W

B

A

G

W

J

N

B

A

G

W

(a)

(b)

(c)

(d)

Figure 3.2: (a): Mr Holmes’ burglary worries as given in [219]: (B)urglar, (A)larm, (W)atson, Mrs
(c): Modiﬁed problem. Mrs
(Gibbon).
(d):
Gibbon is not drinking but somewhat deaf; we represent such uncertain (soft-evidence) by a circle.
Holmes gets additional information from his neighbour Mrs (N)osy and informant Dodgy (J)oe.

(b): Virtual Evidence can be represented by a dashed line.

Mr Holmes receives a telephone call from his neighbour Dr Watson, who states that he hears
the sound of a burglar alarm from the direction of Mr Holmes’ house. While preparing to rush
home, Mr Holmes recalls that Dr Watson is known to be a tasteless practical joker, and he
decides to ﬁrst call another neighbour, Mrs Gibbon who, despite occasional drinking problems,
is far more reliable.
When Mr Holmes calls Mrs Gibbon, he soon realises that she is somewhat tipsy. Instead of
answering his question directly, she goes on and on about her latest back operation and about
how terribly noisy and crime-ridden the neighbourhood has become. When he ﬁnally hangs
up, all Mr Holmes can glean from the conversation is that there is probably an 80% chance
that Mrs Gibbon did hear an alarm sound from her window.

A BN for this scenario is depicted in ﬁg(3.2a) which deals with four binary variables: House is (B)urgled,
(A)larm has sounded, (W)atson hears alarm, and Mrs (G)ibbon hears alarm3:

p(B, A, G, W ) = p(A|B)p(B)p(W|A)p(G|A)

(3.2.26)

Holmes is interested in the likelihood that his house has been burgled. Naively, Holmes’ might calculate4

p(B = tr|W = tr, G = tr)

(3.2.27)

However, after ﬁnding out about Mrs Gibbon’s state, Mr Holmes no longer ﬁnds the above model reliable.
He wants to ignore the eﬀect that Mrs Gibbon’s evidence has on the inference, and replace it with his own
belief as to what Mrs Gibbon observed. Mr Holmes can achieve this by replacing the term p(G = tr|A) by
a so-called virtual evidence term

p(G = tr|A) → p(H|A),

where p(H|A) =

(cid:26) 0.8 A = tr

0.2 A = fa

(3.2.28)

(3.2.29)

Here the state H is arbitrary and ﬁxed. This is used to modify the joint distribution to

p(B, A, H, W ) = p(A|B)p(B)p(W|A)p(H|A),

see ﬁg(3.2b). When we then compute p(B = tr|W = tr, H) the eﬀect of Mr Holmes’ judgement will count
for a factor of 4 times more in favour of the Alarm sounding than not. The values of the table entries
are irrelevant up to normalisation since any constants can be absorbed into the proportionality constant.
Note also that p(H|A) is not a distribution in A, and hence no normalisation is required. This form of
evidence is also called likelihood evidence.

A twist on Pearl’s scenario is that Mrs Gibbon has not been drinking. However, she is a little deaf and
cannot be sure herself that she heard the alarm. She is 80% sure she heard it. In this case, Holmes would

3One might be tempted to include an additional (T)ipsy variable as a parent of G. This would then require us to specify
the joint distribution p(G|T, A) for the 4 parental joint states of T and A. Here we assume that we do not have access to
such information.

4The notation tr is equivalent to 1 and fa to 0 from example(10).

30

DRAFT March 9, 2010

trust the model – however, the observation itself is now uncertain, ﬁg(3.2c). This can be dealt with using
the soft evidence technique. From Jeﬀrey’s rule, one uses the original model equation (3.2.26) to compute

(cid:80)
(cid:80)
A p(G|A)p(W = tr|A)p(A|B = tr)
B,A p(G|A)p(W = tr|A)p(A|B)

Graphically Representing Distributions

p(B = tr|W = tr, G) = p(B = tr, W = tr, G)

p(W = tr, G)

=

and then uses the soft-evidence

(cid:26) 0.8 G = tr

0.2 G = fa

p(G| ˜G) =

to compute

p(B = tr|W = tr, ˜G) = p(B = tr|W = tr, G = tr)p(G = tr| ˜G) + p(B = tr|W = tr, G = fa)p(G = fa| ˜G)
(3.2.32)

The reader may show that an alternative way to represent an uncertain observation such as Mrs Gibbon
being non-tipsy but hard-of-hearing above is to use a virtual evidence child from G.

Uncertain evidence within an unreliable model

To highlight uncertain evidence in an unreliable model we introduce two additional characters. Mrs Nosy
lives next door to Mr Holmes and is completely deaf, but nevertheless an incorrigible curtain-peeker who
seems to notice most things. Unfortunately, she’s also rather prone to imagining things. Based on his
conversation with her, Mr Holmes counts her story as 3 times in favour of there not being a burglary to
there being a burglary, and therefore uses a virtual evidence term

(3.2.30)

(3.2.31)

(3.2.33)

(3.2.34)

(cid:26) 1 B = tr

3 B = fa

(cid:26) 1 B = tr

5 B = fa

p(Nosy|B) =

p(Joe|B) =

Mr Holmes also telephones Dodgy Joe, his contact in the criminal underworld to see if he’s heard of any
planned burglary on Mr Holmes’ home. He summarises this information using a virtual evidence term

When all this information is combined : Mrs Gibbon is not tipsy but somewhat hard of hearing, Mrs Nosy,
and Dodgy Joe, we ﬁrst deal with the unreliable model

p(B, A, W = tr, G, Nosy, Joe) ∝ p(B)p(Nosy|B)p(Joe|B)p(A|B)p(W = tr|A)p(G|A)

(3.2.35)

from which we can compute

p(B = tr|W = tr, G, Nosy, Joe)

Finally we perform inference with the soft-evidence

p(B = tr|W = tr, ˜G, Nosy, Joe) =(cid:88)

p(B = tr|W = tr, G, Nosy, Joe)p(G| ˜G)

G

(3.2.36)

(3.2.37)

An important consideration above is that the virtual evidence does not replace the prior p(B) with another
prior distribution – rather the virtual evidence terms modify the prior through the inclusion of extra
factors. The usual assumption is that each virtual evidence acts independently, although one can consider
dependent scenarios if required.

DRAFT March 9, 2010

31

Belief Networks

x1

x2

x3

x4

x3

x4

x1

x2

(a)

(b)

Figure 3.3: Two BNs for a 4 variable distribution. Both graphs (a) and (b) represent the same distribution
p(x1, x2, x3, x4). Strictly speaking they represent the same (lack of) independence assumptions – the graphs
say nothing about the content of the CPTs. The extension of this ‘cascade’ to many variables is clear and
always results in a Directed Acyclic Graph.

3.3 Belief Networks

In the Wet Grass and Burglar examples, we had a choice as to how we recursively used Bayes’ rule. In a
general 4 variable case we could choose the factorisation,

p(x1, x2, x3, x4) = p(x1|x2, x3, x4)p(x2|x3, x4)p(x3|x4)p(x4)

An equally valid choice is (see ﬁg(3.3))

p(x1, x2, x3, x4) = p(x3|x4, x1, x2)p(x4|x1, x2)p(x1|x2)p(x2).

(3.3.1)

(3.3.2)

In general, two diﬀerent graphs may represent the same independence assumptions, as we will discuss
further in section(3.3.1). If one wishes to make independence assumptions, then the choice of factorisation
becomes signiﬁcant.

The observation that any distribution may be written in the cascade form, ﬁg(3.3), gives an algorithm for
constructing a BN on variables x1, . . . , xn : write down the n−variable cascade graph; assign any ordering
of the variables to the nodes; you may then delete any of the directed connections. More formally, this
corresponds to an ordering of the variables which, without loss of generality, we may write as x1, . . . , xn.
Then, from Bayes’ rule, we have

p(x1, . . . , xn) = p(x1|x2, . . . , xn)p(x2, . . . , xn)

= p(x1|x2, . . . , xn)p(x2|x3, . . . , xn)p(x3, . . . , xn)
= p(xn)

p(xi|xi+1, . . . , xn)

n−1(cid:89)

i=1

(3.3.3)
(3.3.4)

(3.3.5)

The representation of any BN is therefore a Directed Acyclic Graph (DAG).

Every probability distribution can be written as a BN, even though it may correspond to a fully con-
nected ‘cascade’ DAG. The particular role of a BN is that the structure of the DAG corresponds to a set
of conditional independence assumptions, namely which parental variables are suﬃcient to specify each
conditional probability table. Note that this does not mean that non-parental variables have no inﬂu-
ence. For example, for distribution p(x1|x2)p(x2|x3)p(x3) with DAG x1 ← x2 ← x3, this does not imply
p(x2|x1, x3) = p(x2|x3). The DAG speciﬁes conditional independence statements of variables on their
ancestors – namely which ancestors are ‘causes’ for the variable.

The DAG corresponds to a statement of conditional independencies in the model. To complete the speci-
ﬁcation of the BN we need to deﬁne all elements of the conditional probability tables p(xi|pa (xi)). Once
the graphical structure is deﬁned, the entries of the conditional probability tables (CPTs) p(xi|pa (xi))
can be expressed. For every possible state of the parental variables pa (xi), a value for each of the states
of xi needs to be speciﬁed (except one, since this is determined by normalisation). For a large number
of parents, writing out a table of values is intractable, and the tables are usually parameterised in a low
dimensional manner. This will be a central topic of our discussion on the application of BNs in machine
learning.

32

DRAFT March 9, 2010

Belief Networks

3.3.1 Conditional independence

Whilst a BN corresponds to a set of conditional independence assumptions, it is not always immediately
clear from the DAG whether a set of variables is conditionally independent of a set of other variables. For
example, in ﬁg(3.4) are x1 and x2 independent, given the state of x4? The answer is yes, since we have

p(x1, x2|x4) =

p(x1, x2, x3, x4) =

p(x1|x4)p(x2|x3, x4)p(x3)p(x4)

(3.3.6)

(3.3.7)

(3.3.8)

(3.3.9)

(3.3.10)

(3.3.11)

(cid:88)

x1,x3

1

p(x4)

p(x1|x4)p(x2|x3, x4)p(x3)p(x4)

(cid:88)

1

p(x4)

x3

p(x2|x3, x4)p(x3)

1

x3

p(x4)

(cid:88)
= p(x1|x4)(cid:88)
(cid:88)

x3

Now

p(x2|x4) =

p(x4)

1

=(cid:88)

x3

x1,x3

p(x2|x3, x4)p(x3)

p(x1, x2, x3, x4) =

Combining the two results above we have

p(x1, x2|x4) = p(x1|x4)p(x2|x4)

so that x1 and x2 are indeed independent conditioned on x4.

Deﬁnition 19 (Conditional Independence).

X ⊥⊥Y|Z

denotes that the two sets of variables X and Y are independent of each other provided we know the state
of the set of variables Z. For full conditional independence, X and Y must be independent given all states
of Z. Formally, this means that
p(X ,Y|Z) = p(X|Z)p(Y|Z)

(3.3.12)

for all states of X ,Y,Z. In case the conditioning set is empty we may also write X ⊥⊥Y for X ⊥⊥Y|∅, in
which case X is (unconditionally) independent of Y.
If X and Y are not conditionally independent, they are conditionally dependent. This is written

X(cid:62)(cid:62)Y|Z

(3.3.13)

To develop intuition about conditional independence consider the three variable distribution p(x1, x2, x3).
We may write this in any of the 6 ways

p(x1, x2, x3) = p(xi1|xi2, xi3)p(xi2|xi3)p(xi3)

(3.3.14)

where (i1, i2, i3) is any of the 6 permutations of (1, 2, 3). Whilst all diﬀerent DAGs, they represent the
same distribution, namely that which makes no conditional independence statements.

To make an independence statement, we need to drop one of the links. This gives rise to the 4 DAGs
in ﬁg(3.5). Are any of these graphs equivalent, in the sense that they represent the same distribution?

x1

x2

x3

x4

Figure 3.4: p(x1, x2, x3, x4) = p(x1|x4)p(x2|x3, x4)p(x3)p(x4).

DRAFT March 9, 2010

33

Belief Networks

x1

x2

x1

x2

x1

x2

x1

x2

x3

(a)

x3

(b)

x3

(c)

x3

(d)

Figure 3.5: By dropping say the connection between variables x1 and x2, we reduce the 6 possible BN
graphs amongst three variables to 4.
(The 6 fully connected ‘cascade’ graphs correspond to (a) with
x1 → x2, (a) with x2 → x1, (b) with x1 → x2, (b) with x2 → x1, (c) with x1 → x3 and (d) with x2 → x1.
Any other graphs would be cyclic and therefore not distributions).

x

y

x

y

x

y

z

(a)

z

(b)

z

(c)

x

y

w

z

(d)

(c): Variable z is a collider. Graphs
Figure 3.6: In graphs (a) and (b), variable z is not a collider.
(a) and (b) represent conditional independence x⊥⊥ y| z. In graphs (c) and (d), x and y are ’graphically’
conditionally dependent given variable z.

Applying Bayes’ rule gives :

(cid:124)
(cid:125)
p(x2|x3)p(x3|x1)p(x1)

(cid:123)(cid:122)

graph(c)

= p(x2, x3)p(x3, x1)/p(x3) = p(x1|x3)p(x2, x3)
(cid:124)
(cid:125)
= p(x1|x3)p(x3|x2)p(x2)

(cid:124)
(cid:125)
= p(x1|x3)p(x2|x3)p(x3)

(cid:123)(cid:122)

(cid:123)(cid:122)

graph(d)

graph(b)

(3.3.15)

(3.3.16)

so that DAGs (b),(c) and (d) represent the same CI assumptions namely that, given the state of variable
x3, variables x1 and x2 are independent, x1⊥⊥ x2| x3.
However, graph (a) represents something fundamentally diﬀerent, namely: p(x1, x2) = p(x1)p(x2). There
is no way to transform the distribution p(x3|x1, x2)p(x1)p(x2) into any of the others.

Remark 3 (Graphical Dependence). Belief networks are good for encoding conditional independence,
but are not well suited to describing dependence. For example, consider the trivial network p(a, b) =
p(b|a)p(a) which has the DAG representation a → b. This may appear to encode that a and b are
dependent. However, there are certainly instances when this is not the case. For example, it may
be that the conditional is such that p(b|a) = p(b), so that p(a, b) = p(a)p(b). In this case, although
generally there may appear to be a ‘graphical’ dependence from the DAG, there can be instances
of the distributions for which dependence does not follow. The same holds for Markov networks,
section(4.2), in which p(a, b) = φ(a, b). Whilst this suggests ‘graphical’ dependence between a and b,
for φ(a, b) = φ(a)φ(b), the variables are independent.

3.3.2 The impact of collisions
In a general BN, how could we check if x⊥⊥ y| z? In ﬁg(3.6)(a,b), x and y are independent when conditioned
on z. In ﬁg(3.6)(c) they are dependent; in this situation, variable z is called a collider – the arrows of its

34

DRAFT March 9, 2010

Belief Networks

a

b

e

c

d

Figure 3.7: The variable d is a collider along the path a−b−d−c,
but not along the path a − b − d − e. Is a ⊥⊥ e| b? a and b are
not d-connected since there are no colliders on the only path
between a and e, and since there is a non-collider b which is in
the conditioning set. Hence a and b are d-separated, i.e. a⊥⊥ e| b.

neighbours are pointing towards it. What about ﬁg(3.6)(d)? In (d), when we condition on z, x and y will
be ‘graphically’ dependent, since

(cid:88)

w

p(x, y|z) = p(x, y, z)

p(z)

=

1

p(z)

p(z|w)p(w|x, y)p(x)p(y) (cid:54)= p(x|z)p(y|z)

(3.3.17)

– intuitively, variable w becomes dependent on the value of z, and since x and y are conditionally depen-
dent on w, they are also conditionally dependent on z.

Roughly speaking, if there is a non-collider z which is conditioned on along the path between x and y (as
in ﬁg(3.6)(a,b)), then this path does not make x and y dependent. Similarly, if there is a path between x
and y which contains a collider, provided that this collider is not in the conditioning set (and neither are
any of its descendants) then this path does not make x and y dependent. If there is a path between x and
y which contains no colliders and no conditioning variables, then this path ‘d-connects’ x and y.

Note that a collider is deﬁned relative to a path. In ﬁg(3.7), the variable d is a collider along the path
a− b− d− c, but not along the path a− b− d− e (since, relative to this path, the two arrows do not point
inwards to d).

Consider the BN: A → B ← C. Here A and C are (unconditionally) independent. However, conditioning
of B makes them ‘graphically’ dependent. Intuitively, whilst we believe the root causes are independent,
given the value of the observation, this tells us something about the state of both the causes, coupling
them and making them (generally) dependent.

3.3.3 d-Separation

The DAG concepts of d-separation and d-connection are central to determining conditional independence
in any BN with structure given by the DAG[284].

Deﬁnition 20 (d-connection, d-separation). If G is a directed graph in which X , Y and Z are disjoint
sets of vertices, then X and Y are d-connected by Z in G if and only if there exists an undirected path
U between some vertex in X and some vertex in Y such that for every collider C on U, either C or a
descendent of C is in Z, and no non-collider on U is in Z.
X and Y are d-separated by Z in G if and only if they are not d-connected by Z in G.
One may also phrase this as follows. For every variable x ∈ X and y ∈ Y, check every path U between x
and y. A path U is said to be blocked if there is a node w on U such that either

1. w is a collider and neither w nor any of its descendants is in Z.
2. w is not a collider on U and w is in Z.

DRAFT March 9, 2010

35

Belief Networks

If all such paths are blocked then X and Y are d-separated by Z.
If the variable sets X and Y are d-separated by Z, they are independent conditional on Z in all probability
distributions such a graph can represent.

The Bayes Ball algorithm[241] provides a linear time complexity algorithm which given a set of nodes X
and Z determines the set of nodes Y such that X ⊥⊥ Y| Z. Y is called the set of irrelevant nodes for X
given Z.
3.3.4 d-Connection and dependence

Given a DAG we can imply with certainty that two variables are (conditionally) independent, provided
they are d-separated. Can we infer that they are dependent, provided they are d-connected? Consider the
following situation

p(a, b, c) = p(c|a, b)p(a)p(b)

(3.3.18)

for which we note that a and b are d-connected by c. For concreteness, we assume c is binary with states
1,2. The question is whether a and b are dependent, conditioned on c, a⊥⊥ b| c. To answer this, consider
(3.3.19)

p(a, b|c = 1) = p(c = 1|a, b)p(a)p(b)
a,b p(c = 1|a, b)p(a)p(b)

(cid:80)

In general, the ﬁrst term p(c = 1|a, b) does not need to be a factored function of a and b and therefore a
and b are conditionally ‘graphically’ dependent. However, we can construct cases where this is not so. For
example, let

p(c = 1|a, b) = φ(a)ψ(b), and p(c = 2|a, b) = 1 − p(c = 1|a, b)
where φ(a) and ψ(b) are arbitrary potentials between 0 and 1. Then

(3.3.20)

Z =(cid:88)

φ(a)p(a)(cid:88)

a

b

p(a, b|c = 1) =

1
Z

φ(a)p(a)ψ(b)p(b),

ψ(b)p(b)

(3.3.21)

which shows that p(a, b|c = 1) is a product of a function in a and function in b, so that a and b are
independent, conditioned on c = 1.

A second example is given by the distribution

p(a, b, c) = p(c|b)p(b|a)p(a)

(3.3.22)

in which a and c are d-connected by b. The question is, are a and c dependent, a⊥⊥ c|∅? For simplicity
we assume b takes the two states 1,2. Then

p(a, c) = p(a)(cid:88)

b

p(a, c) = p(a)γ

= p(a)p(b = 1|a)

p(c|b)p(b|a) = p(a) (p(c|b = 1)p(b = 1|a) + p(c|b = 2)p(b = 2|a))

(cid:18)

(cid:20)
(cid:19)(cid:21)
p(c|b = 1) + p(c|b = 2)
p(b = 1|a) − 1
(cid:19)(cid:19)

1

(cid:18)
p(c|b = 1) + p(c|b = 2)

(cid:18) 1

γ − 1

For the setting p(b = 1|a) = γ, for some constant γ for all states of a, then

(3.3.23)

(3.3.24)

(3.3.25)

which is a product of a function of a and a function of c. Hence a and c are independent.

36

DRAFT March 9, 2010

Belief Networks

c

e

a

c

e

a

b

t

d

f

s

b

(a)

g

(a)

b

d

(b)

f

s

b

t

g

u

(b)

Figure 3.8: Examples for d-separation.
(a): b d-separates a from e. The joint vari-
ables {b, d} d-connect a and e. (b): c and
e are (unconditionally) d-connected. b d-
connects a and e.

Figure 3.9: (a): t and f are d-connected by g. (b):
b and f are d-separated by u.

The moral of the story is that d-separation necessarily implies independence. However, d-connection
does not necessarily imply dependence. It might be that there are numerical settings for which variables
are independent, even though they are d-connected. For this reason we use the term ‘graphical’ depen-
dence when the graph would suggest that variables are dependent, even though there may be numerical
instantiations were dependence does not hold, see deﬁnition(21).

Example 12. Consider ﬁg(3.8a).
Is a ⊥⊥ e| b? If we sum out variable d, then we see that a and e
are independent given b, since the variable e will appear as an isolated factor independent of all other
variables, hence indeed a⊥⊥ e| b. Whilst b is a collider which is in the conditioning set, we need all colliders
on the path to be in the conditioning set (or their descendants) for d-connectedness.

In ﬁg(3.8b), if we sum out variable d, then c and e become intrinsically linked and p(a, b, c, e) will not
factorise into a function of a multiplied by a function of e – hence they are dependent.

Example 13. Consider the graph in ﬁg(3.9a).

1. Are the variables t and f unconditionally independent, i.e. t ⊥⊥ f| ∅? Here there are two colliders,
namely g and s – however, these are not in the conditioning set (which is empty), and hence they
are d-separated and therefore unconditionally independent.

2. What about t⊥⊥ f| g? There is a collider on the path between t and f which is in the conditioning
set. Hence t and f are d-connected by g, and therefore t and f are not independent conditioned on
g.

3. What about b⊥⊥ f| s? Since there is a collider s in the conditioning set on the path between t and

f, then b and f are ‘graphically’ conditionally dependent given s.

Example 14. Is {b, f} ⊥⊥ u| ∅? in ﬁg(3.9b). Since the conditioning set is empty and every path from
either b or f to u contains a collider, b, f are unconditionally independent of u.

3.3.5 Markov equivalence in belief networks

DRAFT March 9, 2010

37

Deﬁnition 21 (Some properties of Belief Networks).

Belief Networks

B

B

B

B

B

B

B

A

A

A

A

A

A

A

C

C

C

C

C

C

C

p(A, B, C) = p(C|A, B)p(A)p(B)

(3.3.26)

A and B are (unconditionally) independent : p(A, B) = p(A)p(B).
A and B are conditionally dependent on C : p(A, B|C) (cid:54)= p(A|C)p(B|C).
→ A

B

Marginalising over C makes A and B independent.

→ A

B

Conditioning on C makes A and B (graphically) de-
pendent.

p(A, B, C) = p(A|c)p(B|C)p(C)

(3.3.27)

A and B are (unconditionally) dependent : p(A, B) (cid:54)= p(A)p(B).
A and B are conditionally independent on C : p(A, B|C) = p(A|C)p(B|C).
→ A

Marginalising over C makes A and B (graphically)
dependent.

B

→ A

= A
=

B

B

Conditioning on C makes A and B independent.

= A

B

C

C

Deﬁnition 22 (Markov Equivalence). Two graphs are Markov equivalent if they both represent the same
set of conditional independence statements.

Deﬁne the skeleton of a graph by removing the directions on the arrows. Deﬁne an immorality in a DAG
as a conﬁguration of three nodes, A,B,C such that C is a child of both A and B, with A and B not directly
connected. Two DAGs represent the same set of independence assumptions (they are Markov equivalent)
if and only if they have the same skeleton and the same set of immoralities [74].

Using this rule we see that in ﬁg(3.5), BNs (b,c,d) have the same skeleton with no immoralities and are
therefore equivalent. However BN (a) has an immorality and is therefore not equivalent to DAGS (b,c,d).

38

DRAFT March 9, 2010

Causality

3.3.6 Belief networks have limited expressibility

t1

y1

t2

y2

h

(a)

t2

y2

t1

y1

(b)

Figure 3.10: (a): Two treatments t1, t2 and correspond-
ing outcomes y1, y2. The health of a patient is represented
by h. This DAG embodies the conditional independence
statements t1 ⊥⊥ t2, y2 | ∅, t2 ⊥⊥ t1, y1 | ∅, namely that
the treatments have no eﬀect on each other. (b): One
could represent the marginalised latent variable using a
bi-directional edge.

Consider the DAG in ﬁg(3.10a), (from [232]). This DAG could be used to represent two successive
experiments where t1 and t2 are two treatments and y1 and y2 represent two outcomes of interest; h is
the underlying health status of the patient; the ﬁrst treatment has no eﬀect on the second outcome hence
there is no edge from y1 to y2. Now consider the implied independencies in the marginal distribution
p(t1, t2, y1, y2), obtained by marginalising the full distribution over h. There is no DAG containing only
the vertices t1, y1, t2, y2 which represents the independence relations and does not also imply some other
independence relation that is not implied by ﬁg(3.10a). Consequently, any DAG on vertices t1, y1, t2, y2
alone will either fail to represent an independence relation of p(t1, t2, y1, y2), or will impose some additional
independence restriction that is not implied by the DAG. In the above example

p(t1, t2, y1, y2) = p(t1)p(t2)(cid:88)

h

p(y1|t1, h)p(y2|t2, h)p(h)

(3.3.28)

cannot in general be expressed as a product of functions deﬁned on a limited set of the variables. However,
it is the case that the conditional independence conditions t1⊥⊥ t2, y2|∅, t2⊥⊥ t1, y1|∅ hold in p(t1, t2, y1, y2)
– they are there, encoded in the form of the conditional probability tables.
It is just that we cannot
‘see’ this independence since it is not present in the structure of the marginalised graph (though one can
naturally infer this in the larger graph p(t1, t2, y1, y2, h)).

This example demonstrates that BNs cannot express all the conditional independence statements that
could be made on that set of variables (the set of conditional independence statements can be increased by
considering extra latent variables however). This situation is rather general in the sense that any graphical
model has limited expressibility in terms of independence statements[265]. It is worth bearing in mind
that Belief Networks may not always be the most appropriate framework to express one’s independence
assumptions and intuitions.

A natural consideration is to use a bi-directional arrow when a latent variable is marginalised. For
ﬁg(3.10a), one could depict the marginal distribution using a bi-directional edge, ﬁg(3.10b). Similarly a
BN with a latent conditioned variable can be represented using an undirected edge. For a discussion of
these and related issues, see [232].

3.4 Causality

Causality is a contentious topic and the purpose of this section is make the reader aware of some pitfalls
that can occur and which may give rise to erroneous inferences. The reader is referred to [220] and [74]
for further details.

The word ‘causal’ is contentious particularly in cases where the model of the data contains no explicit
temporal information, so that formally only correlations or dependencies can be inferred. For a distri-
bution p(a, b), we could write this as either (i) p(a|b)p(b) or (ii) p(b|a)p(a). In (i) we might think that b
‘causes’ a, and in (ii) a ‘causes’ b. Clearly, this is not very meaningful since they both represent exactly
the same distribution. Formally Belief Networks only make independence statements, not causal ones.
Nevertheless, in constructing BNs, it can be helpful to think about dependencies in terms of causation

DRAFT March 9, 2010

39

Causality

A

B

A

B

R

W

R

W

(a)

(b)

(c)

(d)

(c):
Figure 3.11: Both (a) and (b) represent the same distribution p(a, b) = p(a|b)p(b) = p(b|a)p(a).
The graph represents p(rain, grasswet) = p(grasswet|rain)p(rain). (d): We could equally have written
p(rain|grasswet)p(grasswet), although this appears to be causally non-sensical.

G

D

R

FD

(a)

G

D

(b)

(a): A DAG for the relation be-
Figure 3.12:
tween Gender (G), Drug (D) and Recovery (R), see
(b): Inﬂuence diagram. No decision
table(3.1).
variable is required for G since G has no parents.

R

since our intuitive understanding is usually framed in how one variable ‘inﬂuences’ another. First we
discuss a classic conundrum that highlights potential pitfalls that can arise.

3.4.1 Simpson’s paradox

Simpson’s ‘paradox’ is a cautionary tale in causal reasoning in BNs. Consider a medical trial in which
patient treatment and outcome are recovered. Two trials were conducted, one with 40 females and one
with 40 males. The data is summarised in table(3.1). The question is : Does the drug cause increased
recovery? According to the table for males, the answer is no, since more males recovered when they were
not given the drug than when they were. Similarly, more females recovered when not given the drug than
recovered when given the drug. The conclusion appears that the drug cannot be beneﬁcial since it aids
neither subpopulation.

However, ignoring the gender information, and collating both the male and female data into one combined
table, we ﬁnd that more people recovered when given the drug than when not. Hence, even though the
drug doesn’t seem to work for either males or females, it does seem to work overall! Should we therefore
recommend the drug or not?

Resolution of the paradox

The ‘paradox’ occurs since we are asking a causal (or interventional) question. The question we are in-
tuitively asking is, if we give someone the drug, what happens? However, the calculation we performed
above was only an observational calculation. The calculation we really want is to ﬁrst intervene, setting

Males

Recovered Not Recovered Rec. Rate

Given Drug

Not Given Drug

Females

Given Drug

Not Given Drug

Combined
Given Drug

Not Given Drug

18
7

12
3

60%
70%

Recovered Not Recovered Rec. Rate

2
9

8
21

20%
30%

Recovered Not Recovered Rec. Rate

20
16

20
24

50%
40%

Table 3.1: Table for Simpson’s Paradox (from [220])

40

DRAFT March 9, 2010

Causality

the drug state, and then observe what eﬀect this has on recovery. Pearl[220] describes this as the diﬀerence
between ‘given that we see’ (observational evidence), versus ‘given that we do’ (interventional evidence).

A model of the Gender, Drug and Recovery data (which makes no conditional independence assumptions)
is

p(G, D, R) = p(R|G, D)p(D|G)p(G)

(3.4.1)

An observational calculation concerns computing p(R|G, D) and p(R|D).
interpretation,
however, if we intervene and give the drug, then the term p(D|G) in equation (3.4.1) should play no role in
the experiment (otherwise the distribution models that given the gender we select a drug with probability
p(D|G), which is not the case – we decide to give the drug or not, independent of gender). In the causal
case we are modelling the causal experiment; in this case the term p(D|G) needs to be replaced by a term
that reﬂects the setup of the experiment. In an atomic intervention a single variable is set in a particular
state5. In our atomic causal intervention in setting D, we are dealing with the modiﬁed distribution

In a causal

˜p(G, R|D) = p(R|G, D)p(G)

(3.4.2)

where the terms on the right hand side of this equation are taken from the original BN of the data. To
denote an intervention we use ||:

(cid:80)
p(R||G, D) ≡ ˜p(R|G, D) = p(R|G, D)p(G)
R p(R|G, D)p(G)

= p(R|G, D)

(3.4.3)

(One can also consider here G as being interventional – in this case it doesn’t matter since the fact that
the variable G has no parents means that for any distribution conditional on G, the prior factor p(G) will
not be present). Using equation (3.4.3), for the males given the drug 60% recover, versus 70% recovery
when not given the drug. For the females given the drug 20% recover, versus 30% recovery when not given
the drug.

Similarly,

p(R||D) ≡ ˜p(R|D) =

(cid:80)
(cid:80)
G p(R|G, D)p(G)
R,G p(R|G, D)p(G)

=(cid:88)

G

p(R|G, D)p(G)

Using the above post intervention distribution we have

p(recovery|drug) = 0.6 × 0.5 + 0.2 × 0.5 = 0.4

and

p(recovery|no drug) = 0.7 × 0.5 + 0.3 × 0.5 = 0.5

(3.4.4)

(3.4.5)

(3.4.6)

Hence we correctly infer that the drug is overall not helpful, as we intuitively expect, and is consistent
with the results from both subpopulations.

Here p(R||D) means that we ﬁrst choose either a Male or Female patient at random, and then give them
the drug, or not depending on the state of D. The point is that we do not randomly decide whether or
not to give the drug, hence the absence of the term p(D|G) from the joint distribution. One way to think
about such models is to consider how to draw a sample from the joint distribution of the random variables
– in most cases this should clarify the role of causality in the experiment.

In contrast to the interventional calculation, the observational calculation makes no conditional indepen-
dence assumptions. This means that, for example, the term p(D|G) plays a role in the calculation (the
reader might wish to verify that the result given in the combined data in table(3.1) is equivalent to inferring
with the full distribution equation (3.4.1)).

5More general experimental conditions can be modelled by replacing p(D|G) by an intervention distribution π(D|G)

DRAFT March 9, 2010

41

Deﬁnition 23 (Pearl’s Do Operator).

Causality

In a causal inference, in which the eﬀect of setting variables Xc1, . . . , XcK , ck ∈ C, in states xc1, . . . , xcK , is
to be inferred, this is equivalent to standard evidential inference in the post intervention distribution:

(cid:81)K
p(X|do(Xc1 = xc1), . . . , do(XcK = xcK )) = p(X1, . . . , Xn|xc1, . . . , xcK )
i=1 p(Xci|pa (Xci))

p (Xj|pa (Xj))

(3.4.7)

=(cid:89)

j(cid:54)∈C

where any parental states of pa (Xj) of Xj are set in their evidential states. An alternative notation is
p(X||xc1, . . . , xcK ).
In words, for those variables for which we causally intervene and set in a particular state, the corresponding
terms p(Xci|pa (Xci)) are removed from the original Belief Network. For variables which are evidential but
non-causal, the corresponding factors are not removed from the distribution. The interpretation is that
the post intervention distribution corresponds to an experiment in which the causal variables are ﬁrst set
and non-causal variables are subsequently observed.

3.4.2 Inﬂuence diagrams and the do-calculus

In making causal inferences we must adjust the model to reﬂect any causal experimental conditions. In
setting any variable into a particular state we need to surgically remove all parental links of that vari-
able. Pearl calls this the do operator, and contrasts an observational (‘see’) inference p(x|y) with a causal
(‘make’ or ‘do’) inference p(x|do(y)).
A useful alternative representation is to append variables X upon which an intervention can possibly be
made with a parental decision variable FX[74]. For example6

˜p(D, G, R, FD) = p(D|FD, G)p(G)p(R|G, D)p(FD)

(3.4.8)

where

p(D|FD = ∅, G) ≡ p(D|pa (D))
p(D|FD = d, G) = 1 for D = d and 0 otherwise

Hence, if the decision variable FD is set to the empty state, the variable D is determined by the standard
observational term p(D|pa (D)). If the decision variable is set to a state of D, then the variable puts all
its probability in that single state of D = d. This has the eﬀect of replacing the conditional probability
term a unit factor and any instances of D set to the variable in its interventional state7.

A potential advantage of the inﬂuence diagram approach over the do-calculus is that deriving conditional
independence statements can be made based on standard techniques for the augmented BN. Additionally,
for parameter learning, standard techniques apply in which the decision variables are set to the condition
under which each data sample was collected (a causal or non-causal sample).

Example 15 (Drivers and Accidents: A causal Belief Network).

6Here the Inﬂuence Diagram is a distribution over variables in including decision variables, in contrast to the application

of IDs in chapter(7).

7More general cases can be considered in which the variables are placed in a distribution of states [74].

42

DRAFT March 9, 2010

Parameterising Belief Networks

x1

x2

x3

x4

x5

x1

x2

x3

x4

x5

x1

x2

x3

x4

x5

z1

z2

z1

z2

z3

z4

z5

y

(a)

y

(b)

y

(c)

Figure 3.13: (a): If all variables are binary 25 = 32 states are required to specify p(y|x1, . . . , x5).
Here only 16 states are required. (c): Noisy logic gates.

(b):

FD

FA

D

A

Consider the following CPT entries p(D = bad) = 0.3, p(A =
tr|D = bad) = 0.9. If we intervene and use a bad driver, what
is the probability of an accident?

p(A = tr|D = bad, FD = bad, FA = ∅) = p(A = tr|D = bad) = 0.9

(3.4.9)

On the other hand, if we intervene and make an accident, what is the probability the driver involved is
bad? This is

p(D = bad||A = tr, FD = ∅, FA = tr) = p(D = bad) = 0.3

3.4.3 Learning the direction of arrows

In the absence of data from causal experiments, one should be justiﬁably sceptical about learning ‘causal’
networks. Nevertheless, one might prefer a certain direction of a link based on assumptions of the ‘sim-
plicity’ of the CPTs. This preference may come from a ‘physical intuition’ that whilst root ‘causes’ may
be uncertain, the relationship from cause to eﬀect is clear. In this sense a measure of the complexity of a
CPT is required, such as entropy. Such heuristics can be numerically encoded and the ‘directions’ learned
in an otherwise Markov equivalent graph.

3.5 Parameterising Belief Networks

(dim (y) − 1)(cid:81)

Consider a variable y with many parental variables x1, . . . , xn, ﬁg(3.13a). Formally, the structure of the
graph implies nothing about the form of the parameterisation of the table p(y|x1, . . . , xn). If each parent
xi has dim (xi) states, and there is no constraint on the table, then the table p(y|x1, . . . , xn) contains
i dim (xi) entries. If stored explicitly for each state, this would require potentially huge
storage. An alternative is to constrain the table to have a simpler parametric form. For example, one
might write a decomposition in which only a limited number of parental interactions are required (this is
called divorcing parents in [148]). For example, in ﬁg(3.13b), assuming all variables are binary, the number
of states requiring speciﬁcation is 23 + 22 + 22 = 16, compared to the 25 = 32 states in the unconstrained
case. The distribution

p(y|x1, . . . , x5) = (cid:88)

z1,z2

p(y|z1, z2)p(z1|x1, x2, x3)p(z2|x4, x5)

can be stored using only 16 independent parameters.

DRAFT March 9, 2010

(3.5.1)

43

Logic gates

Exercises

Another technique to constrain CPTs uses simple classes of conditional tables. For example, in ﬁg(3.13c),
one could use a logical OR gate on binary zi, say

(cid:26) 1 if at least one of the zi is in state 1

p(y|z1, . . . , z5) =

0 otherwise

(3.5.2)

We can then make a CPT p(y|x1, . . . , x5) by including the additional terms p(zi = 1|xi). When each xi is
binary there are in total only 2 + 2 + 2 + 2 + 2 = 10 quantities required for specifying p(y|x). In this case,
ﬁg(3.13c) can be used to represent any noisy logic gate, such as the noisy OR or noisy AND, where the
number of parameters required to specify the noisy gate is linear in the number of parents x.

The noisy-OR is particularly common in disease-symptom networks in which many diseases x can give
rise to the same symptom y– provided that at least one of the diseases is present, the probability that the
symptom will be present is high.

3.6 Further Reading

An introduction to Bayesian Networks and graphical models in expert systems is to be found in [258],
which also discusses general inference techniques which will be discussed during later chapters.

3.7 Code

3.7.1 Naive inference demo

demoBurglar.m: Was it the Burglar demo
demoChestClinic.m: Naive Inference on Chest Clinic

3.7.2 Conditional independence demo
The following demo determines whether X ⊥⊥ Y| Z for the Chest Clinic network, and checks the result
numerically8. The independence test is based on the Markov method of section(4.2.4). This is preferred
over the d-separation method since it is arguably simpler to code and also more general in that it deals
also with conditional independence in Markov Networks as well as Belief Networks.

Running the demo code below, it may happen that the numerical dependence is very low – that is

p(X ,Y|Z) ≈ p(X|Z)p(Y|Z)

(3.7.1)

even though X(cid:62)(cid:62)Y|Z. This highlights the diﬀerence between ‘structural’ and ‘numerical’ independence.
condindepPot.m: Numerical measure of conditional independence
demoCondindep.m: Demo of conditional independence (using Markov method)

3.7.3 Utility routines

dag.m: Find the DAG structure for a Belief Network

3.8 Exercises

Exercise 20 (Party Animal). The party animal problem corresponds to the network in ﬁg(3.14). The
boss is angry and the worker has a headache – what is the probability the worker has been to a party? To

8The code for (structural) conditional independence is given in chapter(4).

44

DRAFT March 9, 2010

Exercises

D

U

H

P

A

Figure 3.14: Party animal. Here all variables are binary. When set to
1 the statements are true: P = Been to Party, H = Got a Headache,
D = Demotivated at work, U = Underperform at work, A =Boss Angry.
Shaded variables are observed in the true state.

a

t

x

l

e

b

s

d

x = Positive X-ray
d = Dyspnea (Shortness of breath)
e = Either Tuberculosis or Lung Cancer
t = Tuberculosis
l = Lung Cancer
b = Bronchitis
a = Visited Asia
s = Smoker

Figure 3.15: Belief network structure for the Chest Clinic example.

complete the speciﬁcations, the probabilities are given as follows:
p(U = tr|P = tr, D = tr) = 0.999
p(U = tr|P = fa, D = tr) = 0.9
p(U = tr|P = fa, D = fa) = 0.01
p(U = tr|P = tr, D = fa) = 0.9
Exercise 21. Consider the distribution p(a, b, c) = p(c|a, b)p(a)p(b). (i) Is a⊥⊥ b|∅?. (ii) Is a⊥⊥ b| c?
Exercise 22. The Chest Clinic network [170] concerns the diagnosis of lung disease (tuberculosis, lung
cancer, or both, or neither). In this model a visit to Asia is assumed to increase the probability of tuber-
culosis. State if the following conditional independence relationships are true or false

1. tuberculosis⊥⊥ smoking| shortness of breath,
2. lung cancer⊥⊥ bronchitis| smoking,
3. visit to Asia⊥⊥ smoking| lung cancer
4. visit to Asia⊥⊥ smoking| lung cancer, shortness of breath.

Exercise 23 ([128]). Consider the network in ﬁg(3.16), which concerns the probability of a car starting.

p(f = empty) = 0.05

p(b = bad) = 0.02
p(g = empty|b = good, f = not empty) = 0.04 p(g = empty|b = good, f = empty) = 0.97
p(g = empty|b = bad, f = not empty) = 0.1
p(g = empty|b = bad, f = empty) = 0.99
p(t = fa|b = good) = 0.03
p(t = fa|b = bad) = 0.98
p(s = fa|t = tr, f = empty) = 0.92
p(s = fa|t = tr, f = not empty) = 0.01
p(s = fa|t = fa, f = not empty) = 1.0
p(s = fa|t = fa, f = empty) = 0.99

Calculate P (f = empty|s = no), the probability of the fuel tank being empty conditioned on the observation
that the car does not start.
Exercise 24. Consider the Chest Clinic Bayesian Network in ﬁg(3.15) [170]. Calculate by hand the values
for p(D), p(D|S = tr), p(D|S = fa). The table values are:

p(a = tr)
p(t = tr|a = tr)
p(l = tr|s = tr)
p(b = tr|s = tr)
p(x = tr|e = tr)
p(d = tr|e = tr, b = tr) = 0.9
p(d = tr|e = fa, b = tr) = 0.8

= 0.01 p(s = tr)
= 0.5
= 0.05 p(t = tr|a = fa)
= 0.01
= 0.1
p(l = tr|s = fa)
= 0.01
= 0.6
p(b = tr|s = fa)
= 0.3
= 0.98 p(x = tr|e = fa)
= 0.05
p(d = tr|e = tr, b = fa) = 0.7
p(d = tr|e = fa, b = fa) = 0.1

p(e = tr|t, l) = 0 only if both t and l are fa, 1 otherwise.
DRAFT March 9, 2010

45

Exercises

Battery

Fuel

Gauge

Figure 3.16: Belief Network of car not starting[128], see exercise(23).

Turn Over

Start

Exercise 25. If we interpret the Chest Clinic network exercise(24) causally, how can we help a doctor
answer the question ‘If I could cure my patients of Bronchitis, how would this aﬀect my patients’s chance
of being short of breath?’. How does this compare with p(d = tr|b = fa) in a non-causal interpretation, and
what does this mean?

Exercise 26. There is a synergistic relationship between Asbestos (A) exposure, Smoking (S) and Cancer
(C). A model describing this relationship is given by

p(A, S, C) = p(C|A, S)p(A)p(S)

(3.8.1)

1. Is A⊥⊥ S|∅?
2. Is A⊥⊥ S| C?
3. How could you adjust the model to account for the fact that people who work in the building industry

have a higher likelihood to also be smokers and also a higher likelihood to asbestos exposure?

Exercise 27. Consider the three variable distribution

p(a, b, c) = p(a|b)p(b|c)p(c)

(3.8.2)

where all variables are binary. How many parameters are needed to specify distributions of this form?

Exercise 28.
Consider the Belief Network on the right which represents Mr
Holmes’ burglary worries as given in ﬁg(3.2a) :
(B)urglar,
(A)larm, (W)atson, Mrs (Gibbon).
All variables take the two states {tr, fa}. The table entries are

= 0.01

p(B = tr)
p(A = tr|B = tr) = 0.99 p(A = tr|B = fa) = 0.05
p(W = tr|A = tr) = 0.9
p(W = tr|A = fa) = 0.5
p(G = tr|A = tr) = 0.7
p(G = tr|A = fa) = 0.2

1. Compute ‘by hand’ (i.e. show your working) :

(a) p(B = tr|W = tr)
(b) p(B = tr|W = tr, G = fa)

B

A

G

W

(3.8.3)

2. Consider the same situation as above, except that now the evidence is uncertain. Mrs Gibbon thinks
that the state is G = fa with probability 0.9. Similarly, Dr Watson believes in the state W = fa with
value 0.7. Compute ‘by hand’ the posteriors under these uncertain (soft) evidences:
(a) p(B = tr| ˜W )
(b) p(B = tr| ˜W , ˜G)

Exercise 29. A doctor gives a patient a (D)rug (drug or no drug) dependent on their (A)ge (old or young)
and (G)ender (male or female). Whether or not the patient (R)ecovers (recovers or doesn’t recover) depends
on all D, A, G. In addition A⊥⊥ G|∅.

1. Write down the Belief Network for the above situation.

46

DRAFT March 9, 2010

Exercises

2. Explain how to compute p(recover|drug).
3. Explain how to compute p(recover|do(drug), young).

Exercise 30. Implement the Wet Grass scenario numerically using the BRMLtoolbox.
Exercise 31 (LA Burglar). Consider the Burglar scenario, example(10). We now wish to model the fact
that in Los Angeles the probability of being burgled increases if there is an earthquake. Explain how to
include this eﬀect in the model.
Exercise 32. Given two Belief Networks represented as DAGs with associated adjacency matrices A and
B, write a MATLAB function MarkovEquiv(A,B).m that returns 1 if A and B are Markov equivalent,
and zero otherwise.
Exercise 33. The adjacency matrices of two Belief Networks are given below (see ABmatrices.mat).
State if they are Markov equivalent.



A =

0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0

1
1
0
0
1
0
0
0
0

1
0
0
0
0
1
0
0
0

0
1
0
0
0
0
0
0
0

1
0
0
0
0
0
0
0
0

0
0
1
0
1
0
0
0
0

0
0
0
1
0
1
0
0
0

0
0
0
1
0
0
1
0
0

 ,



B =



0
0
0
0
0
1
0
0
0

0
0
0
0
1
0
0
0
0

1
1
0
0
1
0
0
0
0

1
0
0
0
0
1
0
0
0

0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0

0
0
1
0
1
0
0
0
0

0
0
0
1
0
1
0
0
0

0
0
0
1
0
0
1
0
0

(3.8.4)

Exercise 34. There are three computers indexed by i ∈ {1, 2, 3}. Computer i can send a message in one
timestep to computer j if Cij = 1, otherwise Cij = 0. There is a fault in the network and the task is to
ﬁnd out some information about the communication matrix C (C is not necessarily symmetric). To do
this, Thomas, the engineer, will run some tests that reveal whether or not computer i can send a message
to computer j in t timesteps, t ∈ {1, 2}. This is expressed as Cij(t), with Cij(1) ≡ Cij. For example,
he might know that C13(2) = 1, meaning that according to his test, a message sent from computer 1 will
arrive at computer 3 in at most 2 timesteps. Note that this message could go via diﬀerent routes – it
might go directly from 1 to 3 in one timestep, or indirectly from 1 to 2 and then from 2 to 3, or both.
You may assume Cii = 1. A priori Thomas thinks there is a 10% probability that Cij = 1. Given the test
information C = {C12(2) = 1, C23(2) = 0}, compute the a posteriori probability vector

[p(C12 = 1|C), p(C13 = 1|C), p(C23 = 1|C), p(C32 = 1|C), p(C21 = 1|C), p(C31 = 1|C)]

(3.8.5)
Exercise 35. A Belief Network models the relation between the variables oil, inf, eh, bp, rt which stand for
the price of oil, inﬂation rate, economy health, British Petroleum Stock price, retailer stock price. Each
variable takes the states low, high, except for bp which has states low, high, normal. The Belief Network
model for these variables has tables

p(eh=low)=0.2
p(bp=low|oil=low)=0.9
p(bp=low|oil=high)=0.1
p(oil=low|eh=low)=0.9
p(rt=low|inf=low,eh=low)=0.9
p(rt=low|inf=high,eh=low)=0.1
p(inf=low|oil=low,eh=low)=0.9
p(inf=low|oil=high,eh=low)=0.1

p(bp=normal|oil=low)=0.1
p(bp=normal|oil=high)=0.4
p(oil=low|eh=high)=0.05
p(rt=low|inf=low,eh=high)=0.1
p(rt=low|inf=high,eh=high)=0.01
p(inf=low|oil=low,eh=high)=0.1
p(inf=low|oil=high,eh=high)=0.01

1. Draw a Belief Network for this distribution.

2. Given that BP stock price is normal and the retailer stock price is high, what is the probability that

inﬂation is high?

Exercise 36. There are a set of C potentials with potential c deﬁned on a subset of variables Xc. If
Xc ⊆ Xd then can merge (multiply) potentials c and d since c is contained within d. With reference to
suitable graph structures, describe an eﬃcient algorithm to merge a set of potentials so that for the new
set of potentials no potential is contained within the other.

DRAFT March 9, 2010

47

Exercises

48

DRAFT March 9, 2010

CHAPTER 4

Graphical Models

4.1 Graphical Models

Graphical Models (GMs) are depictions of independence/dependence relationships for distributions. Each
form of GM is a particular union of graph and probability constructs and details the form of independence
assumptions represented. GMs are useful since they provide a framework for studying a wide class of
probabilistic models and associated algorithms. In particular they help to clarify modelling assumptions
and provide a uniﬁed framework under which inference algorithms in diﬀerent communities can be related.

It needs to be emphasised that all forms of GM have a limited ability to graphically expresses condi-
tional (in)dependence statements[265]. As we’ve seen, Belief Networks are useful for modelling ancestral
conditional independence.
In this chapter we’ll introduce other types of GM that are more suited to
representing diﬀerent assumptions. Markov Networks, for example, are particularly suited to modelling
marginal dependence and conditional independence. Here we’ll focus on Markov Networks, Chain Graphs
(which marry Belief and Markov networks) and Factor Graphs. There are many more inhabitants of the
zoo of Graphical Models, see [70, 293].

The general viewpoint we adopt is to describe the problem environment using a probabilistic model, after
which reasoning corresponds to performing probabilistic inference. This is therefore a two part process :

Modelling After identifying all potentially relevant variables of a problem environment, our task is to
describe how these variables can interact. This is achieved using structural assumptions as to the
form of the joint probability distribution of all the variables, typically corresponding to assumptions
of independence of variables. Each class of graphical model corresponds to a factorisation property
of the joint distribution.

Inference Once the basic assumptions as to how variables interact with each other is formed (i.e. the
probabilistic model is constructed) all questions of interest are answered by performing inference on
the distribution. This can be a computationally non-trivial step so that coupling GMs with accurate
inference algorithms is central to successful graphical modelling.

Whilst not a strict separation, GMs tend to fall into two broad classes – those useful in modelling, and
those useful in representing inference algorithms. For modelling, Belief Networks, Markov Networks, Chain
Graphs and Inﬂuence Diagrams are some of the most popular. For inference one typically ‘compiles’ a
model into a suitable GM for which an algorithm can be readily applied. Such inference GMs include
Factor Graphs, Junction Trees and Region Graphs.

49

x2

x2

x1

x3

x1

x3

x1

x4

(a)

x4

(b)

x5

x6

x3

x2

x4

(c)

4.2 Markov Networks

Markov Networks

(a):

4.1:

Figure
pa
φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)/Za.
(c):
pb = φ(x1, x2, x3, x4)/Zb.
φ(x1, x2, x4)φ(x2, x3, x4)φ(x3, x5)φ(x3, x6)/Zc.

=
(b):
pc =

Belief Networks correspond to a special kind of factorisation of the joint probability distribution in which
each of the factors is itself a distribution. An alternative factorisation is, for example

and Z is a constant which ensures normalisation, called the

(4.2.1)

p(a, b, c) =

1
Z

φ(a, b)φ(b, c)

where φ(a, b) and φ(b, c) are potentials
partition function

Z =(cid:88)

φ(a, b)φ(b, c)

(4.2.2)

a,b,c

We will typically use the convention that the ordering of the variables in the potential is not relevant (as
for a distribution) – the joint variables simply index an element of the potential table. Markov Networks
are deﬁned as products of non-negative functions deﬁned on maximal cliques of an undirected graph – see
ﬁg(4.1).

case of a potential satisfying normalisation, (cid:80)

Deﬁnition 24 (Potential). A potential φ(x) is a non-negative function of the variable x, φ(x) ≥ 0. A
joint potential φ(x1, . . . , xn) is a non-negative function of the set of variables. A distribution is a special
x φ(x) = 1. This holds similarly for continuous variables,

with summation replaced by integration.

p(x1, . . . , xn) =

1
Z

C(cid:89)

c=1

Deﬁnition 25 (Markov Network). For a set of variables X = {x1, . . . , xn} a Markov network is deﬁned
as a product of potentials on subsets of the variables Xc ⊆ X :

φc(Xc)

(4.2.3)

Graphically this is represented by an undirected graph G with Xc, c = 1, . . . , C being the maximal cliques
of G. The constant Z ensures the distribution is normalised. The graph is said to satisfy the factorisation
property. In the special case that the graph contains cliques of only size 2, the distribution is called a
pairwise Markov Network, with potentials deﬁned on each link between two variables.

For the case in which clique potentials are strictly positive, this is called a Gibbs distribution.

Remark 4 (Pairwise Markov network). Whilst a Markov network is formally deﬁned on maximal
cliques, in practice authors often use the term to refer to non-maximal cliques.
For example, in the graph on the right, the maximal cliques are x1, x2, x3
and x2, x3, x4, so that the graph describes a distribution p(x2, x2, x3, x4) =
In a pairwise network though the poten-
φ(x1, x2, x3)φ(x2, x3, x4)/Z.
tials are assumed to be over
two-cliques, giving p(x2, x2, x3, x4) =
φ(x1, x2)φ(x1, x3)φ(x2, x3)φ(x2, x4)φ(x3, x4)/Z.

x1

x2

x3

x4

50

DRAFT March 9, 2010

Markov Networks

Deﬁnition 26 (Properties of Markov Networks).

B

B

B

A

A

A

C

C

C

p(A, B, C) = φAC(A, C)φBC(B, C)/Z

(4.2.4)

A and B are unconditionally dependent : p(A, B) (cid:54)= p(A)p(B).
A and B are conditionally independent on C : p(A, B|C) = p(A|C)p(B|C).
→ A

Marginalising over C makes A and B (graphically)
dependent.

B

→ A

B

Conditioning on C makes A and B independent.

4.2.1 Markov properties

We here state some of the most useful results. The reader is referred to [168] for proofs and more
detailed discussion. Consider the Markov Network in ﬁg(4.2a). Here we use the shorthand p(1) ≡ p(x1),
φ(1, 2, 3) ≡ φ(x1, x2, x3) etc. We will use this undirected graph to demonstrate conditional independence
properties.

Local Markov property

Deﬁnition 27 (Local Markov Property).

p(x|X\x) = p(x|ne (x))

When conditioned on its neighbours, x is independent of the remaining variables of the graph.

(4.2.5)

The conditional distribution p(4|1, 2, 3, 5, 6, 7) is

p(4|1, 2, 3, 5, 6, 7) = φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)φ(5, 6, 7)
4 φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)φ(5, 6, 7)

(cid:80)

(cid:80)
= φ(2, 3, 4)φ(4, 5, 6)
4 φ(2, 3, 4)φ(4, 5, 6)

= p(4|2, 3, 5, 6)
(4.2.6)

The last line above follows since the variable x4 only appears in the cliques that border x4. The general-
isation of the above example is clear: a MN with positive clique potentials φ, deﬁned with respect to an
undirected graph G entails1 p(xi|x\i) = p(xi|ne (xi)).

Pairwise Markov property

Deﬁnition 28 (Pairwise Markov Property). For any non-adjacent vertices x and y

x⊥⊥ y|X\{x, y}

1The notation x\i is shorthand for the set of all variables X excluding variable xi, namely X\xi in set notation.

DRAFT March 9, 2010

(4.2.7)

51

Markov Networks

1

2

3

5

6

4

(a)

7

1

5

6

2

3

4

(b)

7

4.2:

(a):
Figure
(b):
φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)φ(5, 6, 7).
By the global Markov property,
since
every path from 1 to 7 passes through 4,
then 1⊥⊥7|4.

p(1, 4|2, 3, 5, 6, 7) = φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)φ(5, 6, 7)
1,4 φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)φ(5, 6, 7)

(cid:80)

(cid:80)
= φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)

1,4 φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)

(4.2.8)
(4.2.9)

= p(1|2, 3, 4, 5, 6, 7)p(4|1, 2, 3, 5, 6, 7)

where the last line follows since for ﬁxed 2, 3, 5, 6, 7, the function φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6) is a product
of a function on 1 and a function on 4, implying independence.

Global Markov property

Deﬁnition 29 (Separation). A subset S separates a subset A from a subset B if every path from any
member of A to any member of B passes though S.

Deﬁnition 30 (Global Markov Property). For a disjoint subset of variables, (A,B,S) where S separates
A from B in G, then A⊥⊥B|S.

2,3,5,6

(cid:88)
= (cid:88)
(cid:88)

2,3,5,6

=

2,3

p(1, 7|4) ∝

p(1, 2, 3, 4, 5, 6, 7)

φ(1, 2, 3)φ(2, 3, 4)φ(4, 5, 6)φ(5, 6, 7)



(cid:88)

5,6



φ(1, 2, 3)φ(2, 3, 4)

φ(4, 5, 6)φ(5, 6, 7)

(4.2.10)

(4.2.11)

(4.2.12)

This implies that p(1, 7|4) = p(1|4)p(7|4).

Example 16 (Boltzmann machine). A Boltzmann machine is a MN on binary variables dom(xi) = {0, 1}
of the form

i bixi

(4.2.13)

(cid:80)

i<j wij xixj +(cid:80)

p(x) =

1

Z(w, b) e

where the interactions wij are the ‘weights’ and the bi the biases. This model has been studied in the
machine learning community as a basic model of distributed memory and computation[2]. The graphical
model of the BM is an undirected graph with a link been nodes i and j for wij (cid:54)= 0. Consequently, for all
but specially constrained W, the graph is multiply-connected and inference will be typically intractable.

4.2.2 Gibbs networks

For simplicity we assume that the potentials are strictly positive in which case MNs are also termed Gibbs
Networks. In this case, a GN satisﬁes the following independence relations:

52

DRAFT March 9, 2010

Markov Networks

x1

x3

x2

x1

x2

x1

x4

x3

x4

x3

x4

x3

x4

x3

(a)

(b)

(c)

(d)

(e)

x2

x4

Figure 4.3: (a-d): Local distributions. (e): The Markov network consistent with the local distributions.
If the local distributions are positive, by the Hammersley-Cliﬀord theorem, the only joint distribution that
can be consistent with the local distributions must be a Gibbs distribution with structure given by (e).

4.2.3 Markov random ﬁelds

Deﬁnition 31 (Markov Random Field). A MRF is deﬁned by a set of distributions p(xi|ne (xi)) where
i ∈ {1, . . . , n} indexes the distributions and ne (xi) are the neighbours of variable xi, namely that subset
of the variables x1, . . . , xn that the distribution of variable xi depends on. The term Markov indicates
that this is a proper subset of the variables.

A distribution is an MRF with respect to an undirected graph G if

p(xi|x\i) = p(xi|ne (xi))

(4.2.14)

where ne (xi) are the neighbouring variables of variable xi, according to the undirected graph G.

Hammersley Cliﬀord Theorem

The Hammersley-Cliﬀord theorem helps resolve questions as to when a set of positive local distributions
p(xi|ne (xi)) could ever form a consistent joint distribution p(x1, . . . , xn). Local distributions p(xi|ne (xi))
can form a consistent joint distribution if and only if p(x1, . . . , xn) factorises according to

(cid:32)

(cid:88)

(cid:33)

p(x1, . . . , xn) =

exp

−

Vc(Xc)

1
Z

(4.2.15)

indexed by c. Equation (4.2.15) is equivalent to(cid:81)

c

where the sum is over all cliques and Vc(Xc) is a real function deﬁned over the variables in the clique

c φ(Xc), namely a MN on positive clique potentials.
The graph over which the cliques are deﬁned is an undirected graph with a link between xi and xj if

p(xi|x\i) (cid:54)= p(xi|x\(i,j))

(4.2.16)

That is, if xj has an eﬀect on the conditional distribution of xi, then add an undirected link between xi
and xj. This is then repeated over all the variables xi[35, 203], see ﬁg(4.3). Note that the HC theorem does
not mean that given a set of conditional distributions, we can always form a consistent joint distribution
from them – rather it states what the functional form of a joint distribution for the conditionals to be
consistent with the joint, see exercise(45).

4.2.4 Conditional independence using Markov networks
For X ,Y,Z each being collections of variables, in section(3.3.3) we discussed an algorithm to determine
X ⊥⊥Y|Z. An alternative and more general method (since it handles directed and undirected graphs) uses
the following steps: (see [74, 169])

DRAFT March 9, 2010

53

a

c

e

h

i

j

(a)

b

d

f

k

g

b

d

f

a

c

e

i

(b)

Markov Networks

Figure 4.4: (a): Belief Network for which we are interested
(b):
in checking conditional independence a ⊥⊥ b| {d, i}.
Ancestral moralised graph for a ⊥⊥ b| {d, i}. Every path
from a red to green node passes through a yellow node,
so a and b are independent given d, i. Alternatively, if we
consider a ⊥⊥ b| i, the variable d is uncoloured, and we
can travel from the red to the green without encountering
In this case a is
a yellow node (using the e − f path).
dependent on b, conditioned on i.

Ancestral Graph Remove from the DAG any node which is neither in X ∪ Y ∪ Z nor an ancestor of a

node in this set, together with any edges in or out of such nodes.

Moralisation Add a line between any two remaining nodes which have a common child, but are not

already connected by an arrow. Then remove remaining arrowheads.

Separation In the undirected graph so constructed, look for a path which joins a node in X to one in Y

but does not intersect Z. If there is no such path deduce that X ⊥⊥Y|Z.

For Markov Networks only the ﬁnal separation criterion needs to be applied. See ﬁg(4.4) for an example.

4.2.5 Lattice Models

Undirected models have a long history in diﬀerent branches of science, especially statistical mechanics
on lattices and more recently as models in visual processing in which the models encourage neighbouring
variables to be in the same states[35, 36, 106].

Consider a model in which our desire is that states of the binary valued vari-
ables x1, . . . , x9, arranged on a lattice (right) should prefer their neighbouring
variables to be in the same state

(cid:89)

i∼j

x1

x4

x7

x2

x5

x8

x3

x6

x9

p(x1, . . . , x9) =

1
Z

φij(xi, xj)

(4.2.17)

where i ∼ j denotes the set of indices where i and j are neighbours in the
undirected graph.

The Ising model

A set of potentials for equation (4.2.17) that encourages neighbouring variables to have the same state is

φij(xi, xj) = e

− 1
2T (xi−xj )2

(4.2.18)

This corresponds to a well-known model of the physics of magnetic systems, called the Ising model which
consists of ‘mini-magnets’ which prefer to be aligned in the same state, depending on the temperature

Figure 4.5: Onsagar magnetisation. As the temperature T decreases
towards the critical temperature Tc a phase transition occurs in which
a large fraction of the variables become aligned in the same state.

54

DRAFT March 9, 2010

00.511.5200.51T/TcMChain Graphical Models

a

c

(a)

b

d

a

b

a

cd

(b)

b

e

g

d

f

c

c

aedf

bg

(c)

(d)

Figure 4.6: Chain graphs. The chain components are identiﬁed by deleting the directed edges and iden-
(a): Chain components are (a),(b),(c, d), which can be
tifying the remaining connected components.
written as a BN on the cluster variables in (b). (c): Chain components are (a, e, d, f), (b, g), (c), which
has the cluster BN representation (d). (From [168])

T . For high T the variables behave independently so that no global magnetisation appears. For low T ,
there is a strong preference for neighbouring mini-magnets to become aligned, generating a strong macro-
magnet. Remarkably, one can show that, in a very large two-dimensional lattice, below the so-called Curie
(cid:80)N
temperature, Tc ≈ 2.269 (for ±1 variables), the system admits a phase change in that a large fraction
of the variables become aligned – above Tc, on average, the variables are unaligned. This is depicted in
i=1 xi|/N is the average alignment of the variables. That this phase change happens
ﬁg(4.5) where M = |
for non-zero temperature has driven considerable research in this and related areas[40]. Global coherence
eﬀects such as this that arise from weak local constraints are present in systems that admit emergent
behaviour. Similar local constraints are popular in image restoration algorithms to clean up noise, under
the assumption that noise will not show any local spatial coherence, whilst ‘signal’ will. An example is
given in section(28.8) where we discuss algorithms for inference under special constraints on the MRF.

4.3 Chain Graphical Models

Deﬁnition 32 (Chain Component). The chain components of a graph G are obtained by :

1. Forming a graph G(cid:48) with directed edges removed from G.
2. Then each connected component in G(cid:48) constitutes a chain component.

Chain Graphs (CGs) contain both directed and undirected links. To develop the intuition, consider
ﬁg(4.6a). The only terms that we can unambiguously specify from this depiction are p(a) and p(b) since
there is no mixed interaction of directed and undirected edges at the a and b vertices. By probability,
therefore, we must have

p(a, b, c, d) = p(a)p(b)p(c, d|a, b)

Looking at the graph, we might expect the interpretation to be

p(c, d|a, b) = φ(c, d)p(c|a)p(d|b)

(4.3.1)

(4.3.2)

(cid:88)

c,d

However, to ensure normalisation, and also to retain generality, we interpret this chain component as

p(c, d|a, b) = φ(c, d)p(c|a)p(d|b)φ(a, b), with φ(a, b) ≡ 1/

φ(c, d)p(c|a)p(d|b)

(4.3.3)

This leads to the interpretation of a CG as a DAG over the chain components. Each chain component
represents a distribution over the variables of the component, conditioned on the parental components.
The conditional distribution is itself a product over the cliques of the undirected component and moralised
parental components, including also a factor to ensure normalisation over the chain component.

DRAFT March 9, 2010

55

Expressiveness of Graphical Models

Deﬁnition 33 (Chain Graph distribution). The distribution associated with a chain graph G is found by
ﬁrst identifying the chain components, τ. Then

p(x) =(cid:89)

τ

and

p (Xτ|pa (Xτ ))
(cid:89)

p (Xτ|pa (Xτ )) ∝

φ (XCτ )

c∈Cτ

where Cτ denotes the union of the cliques in component τ together with the moralised parental components
of τ, with φ being the associated functions deﬁned on each clique. The proportionality factor is determined
implicitly by the constraint that the distribution sums to 1.

BNs are CGs in which the connected components are singletons. MNs are CGs in which the chain com-
ponents are simply the connected components of the undirected graph.

CGs can be useful since they are more expressive of CI statements than either Belief Networks or Markov
Networks alone. The reader is referred to [168] and [99] for further details.

Example 17 (Chain Graphs are more expressive than Belief or Markov Networks). Consider the chain
graph in ﬁg(4.7a), which has chain component decomposition

(4.3.4)

(4.3.5)

(4.3.6)

(4.3.7)

(4.3.8)

(4.3.9)

p(a, b, c, d, e, f) = p(a)p(b)p(c, d, e, f|a, b)

where

p(c, d, e, f|a, b) = φ(a, c)φ(c, e)φ(e, f)φ(d, f)φ(d, b)φ(a, b)

with the normalisation requirement

φ(a, b) ≡ 1/

φ(a, c)φ(c, e)φ(e, f)φ(d, f)φ(d, b)

The marginal p(c, d, e, f) is given by

φ(c, e)φ(e, f)φ(d, f)(cid:88)
(cid:124)

a,b

φ(a, b)p(a)p(b)φ(a, c)φ(d, b)

(cid:123)(cid:122)

φ(c,d)

(cid:125)

(cid:88)

c,d,e,f

Since the marginal distribution of p(c, d, e, f) is an undirected 4-cycle, no DAG can express the CI state-
ments contained in the marginal p(c, d, e, f). Similarly no undirected distribution on the same skeleton as
ﬁg(4.7a) could express that a and b are independent (unconditionally), i.e. p(a, b) = p(a)p(b).

4.4 Expressiveness of Graphical Models

It is clear that directed distributions can be represented as undirected distributions since one can asso-
ciate each (normalised) factor in a directed distribution with a potential. For example, the distribution
p(a|b)p(b|c)p(c) can be factored as φ(a, b)φ(b, c), where φ(a, b) = p(a|b) and φ(b, c) = p(b|c)p(c), with
Z = 1. Hence every Belief Network can be represented as some MN by simple identiﬁcation of the fac-
tors in the distributions. However, in general, the associated undirected graph (which corresponds to the

56

DRAFT March 9, 2010

Expressiveness of Graphical Models

a

c

e

b

d

f

c

e

d

f

c

e

d

f

(a)

(b)

(c)

Figure 4.7: The CG (a) expresses a ⊥⊥ b | ∅ and
d ⊥⊥ e | (c, f). No directed graph could express
both these conditions since the marginal distribu-
tion p(c, d, e, f) is an undirected four cycle, (b). Any
DAG on a 4 cycle must contain a collider, as in (c)
and therefore express a diﬀerent set of CI statements
than (b). Similarly, no connected Markov network
can express unconditional independence and hence
(a) expresses CI statements that no Belief Network
or Markov Network alone can express.

moralised directed graph) will contain additional links and independence information can be lost. For
example, the MN of p(c|a, b)p(a)p(b) if a single clique φ(a, b, c) from which one cannot graphically infer
that a⊥⊥ b|∅.
The converse question is whether every undirected model can be represented by a BN with a readily derived
link structure? Consider the example in ﬁg(4.8). In this case, there is no directed model with the same
link structure that can express the (in)dependencies in the undirected graph. Naturally, every probability
distribution can be represented by some BN though it may not necessarily have a simple structure and
be simply a ‘fully connected’ cascade style graph. In this sense the DAG cannot graphically represent the
independence/dependence relations true in the distribution.

Deﬁnition 34 (Independence Maps). A graph is an independence map (I-map) of a given distribution P if
every conditional independence statement that one can derive from the graph G is true in the distribution
P . That is

X ⊥⊥Y|Z G ⇒ X ⊥⊥Y|Z P

(4.4.1)

for all disjoint sets X ,Y,Z.
Similarly, a graph is a dependence map (D-map) of a given distribution P if every conditional independence
statement that one can derive from P is true in the graph G. That is

X ⊥⊥Y|Z G ⇐ X ⊥⊥Y|Z P

for all disjoint sets X ,Y,Z.
A graph G which is both an I-map and a D-map for P is called a perfect map and

X ⊥⊥Y|Z G ⇔ X ⊥⊥Y|Z P

(4.4.2)

(4.4.3)

for all disjoint sets X ,Y,Z. In this case, the set of all conditional independence and dependence statements
expressible in the graph G are consistent with P and vice versa.

Due to Inverse Modus Ponens, example(5), a dependence map is equivalent to

X(cid:62)(cid:62)Y|Z G ⇒ X(cid:62)(cid:62)Y|Z P

(4.4.4)

although this is less useful since standard graphical model representations cannot express dependence.

Note that the above deﬁnitions are not dependent on the graph being directed or undirected. Indeed,
some distributions may have a perfect directed map, but no perfect undirected map. For example

p(x, y, z) = p(z|x, y)p(x)p(y)

DRAFT March 9, 2010

(4.4.5)

57

Factor Graphs

b

b

a

c

a

c

d

(a)

d

(b)

Figure 4.8: (a): An undirected model for which we wish to ﬁnd
a directed equivalent. (b): Every DAG with the same structure
as the undirected model must have a situation where two arrows
will point to a node, such as node d. Summing over the states of
variable d will leave a DAG on the variables a, b, c with no link
between a and c. This cannot represent the undirected model
since when one marginals over d in the undirected this adds a
link between a and c.

has a directed perfect map x → z ← y (assuming that p(z|x, y) (cid:54)= φx(x)φy(y)), but no perfect undirected
map.

Example 18. Consider the distribution deﬁned on variables t1, t2, y1, y2[232]:

p(t1, t2, y1, y2) = p(t1)p(t2)(cid:88)

h

The BN

p(y2|y1, t2)p(y1|t1)p(t1)p(t2)

p(y1|t1, h)p(y2|t2, h)p(h)

(4.4.6)

(4.4.7)

is an I-MAP for distribution (4.4.6) since every independence statement in the BN is true for the corre-
sponding graph. However, it is not a D-MAP since t1(cid:62)(cid:62)t2| y2 cannot be inferred from the BN. Similarly no
undirected graph can represent all independence statements true in (4.4.6). In this case no perfect MAP
(a BN or a MN) can represent (4.4.6).

4.5 Factor Graphs

Factor Graphs (FGs) are mainly used as part of inference algorithms2.

Deﬁnition 35 (Factor Graph). Given a function

f(x1, . . . , xn) =(cid:89)

(cid:0)

X i(cid:1)

ψi

i

(4.5.1)

The FG has a node (represented by a square) for each factor ψi, and a variable node (represented by a
circle) for each variable xj. For each xj ∈ X i an undirected link is made between factor ψi and variable xj.
For a factor ψi
parents to the factor node, and a directed link from the factor node to the child. This has the same
structure as an (undirected) FG, but preserves the information that the factors are distributions.

X i(cid:1) which is a conditional distribution p(xi|pa (xi)), we may use directed links from the

(cid:0)

Factor Graphs are useful since they can preserve more information about the form of the distribution than
either a Belief Network or a Markov Network (or Chain Graph) can do alone.
Consider the distribution

p(a, b, c) = φ(a, b)φ(a, c)φ(b, c)

(4.5.2)

2Formally a FG is an alternative graphical depiction of a hypergraph[81] in which the vertices represent variables, and a
hyperedge a factor as a function of the variables associated with the hyperedge. A FG is therefore a hypergraph with the
additional interpretation that the graph represents a function deﬁned as products over the associated hyperedges. Many
thanks to Robert Cowell for this observation.

58

DRAFT March 9, 2010

Exercises

a

a

a

a

a

c

b

c

b

c

b

c

b

c

b

(a)

(b)

(c)

(d)

(e)

a

c

b

d

(f)

(b): φ(a, b)φ(b, c)φ(c, a).

Figure 4.9: (a): φ(a, b, c).
(c): φ(a, b, c). Both (a) and (b) have the same
undirected graphical model, (c). (e): Directed FG of the BN in (d). (a) is an undirected FG of (d). The
advantage of (e) over (a) is that information regarding the marginal independence of variables b and c is
clear from graph (e), whereas one could only ascertain this by examination of the numerical entries of the
(f): A partially directed FG of p(a|b, c)φ(d, c)φ(b, d). No directed, undirected or
factors in graph (a).
chain graph can represent both the conditional and marginal independence statements expressed by this
graph and also the factored structure of the undirected terms.

The MN representation is given in ﬁg(4.9c). However, ﬁg(4.9c) could equally represent some unfactored
clique potential φ(a, b, c).
In this sense, the FG representation in ﬁg(4.9b) more precisely conveys the
form of distribution equation (4.5.2). An unfactored clique potential φ(a, b, c) is represented by the FG
ﬁg(4.9a). Hence diﬀerent FGs can have the same MN since information regarding the structure of the
clique potential is lost in the MN.

4.5.1 Conditional independence in factor graphs

A rule which works with both directed and undirected (and partially directed) FGs is as follows[96]. To
determine whether two variables are independent given a set of conditioned variables, consider all paths
connecting the two variables. If all paths are blocked, the variables are conditionally independent.

A path is blocked if any one or more of the following conditions are satisﬁed:

• One of the variables in the path is in the conditioning set.
• One of the variables or factors in the path has two incoming edges that are part of the path, and

neither the variable or factor nor any of its descendants are in the conditioning set.

4.6 Notes

A detailed discussion of the axiomatic and logical basis of conditional independence is given in [45] and
[264].

4.7 Code

condindep.m: Conditional Independence test p(X, Y |Z) = p(X|Z)p(Y |Z)?

4.8 Exercises

Exercise 37.

1. Consider the pairwise Markov Network,

p(x) = φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)

Express in terms of φ the following:
p(x2|x1, x3),

p(x1|x2, x4),

DRAFT March 9, 2010

p(x3|x2, x4),

p(x4|x1, x3)

(4.8.1)

(4.8.2)

59

Exercises

2. For a set of local distributions deﬁned as

p1(x1|x2, x4),

p2(x2|x1, x3),

p3(x3|x2, x4),

p4(x4|x1, x3)

(4.8.3)

is it always possible to ﬁnd a joint distribution p(x1, x2, x3, x4) consistent with these local conditional
distributions?

Exercise 38. Consider the Markov network

p(a, b, c) = φab(a, b)φbc(b, c)

(4.8.4)

Nominally, by summing over b, the variables a and c are dependent. For binary b, explain a situation in
which this is not the case, so that marginally, a and c are independent.

Exercise 39. Show that for the Boltzmann machine

p(x) =

1

Z(W, b) exTWx+xTb

one may assume, without loss of generality, W = WT.

Exercise 40.

(4.8.5)

The restricted Boltzmann machine (or Harmonium[253]) is a specially constrained
Boltzmann machine on a bipartite graph, consisting of a layer of visible variables
v = (v1, . . . , vV ) and hidden variables h = (h1, . . . , hH):

h1

h2

1

p(v, h) =

Z(W, a, b) evTWh+aTv+bTh
All variables are binary taking states 0, 1.

(4.8.6)

v1

v2

v3

1. Show that the distribution of hidden units conditional on the visible units factorises as

bi +(cid:88)

j



Wjivj

(4.8.7)

(4.8.8)

(4.8.9)

p(h|v) =(cid:89)

i

p(hi|v),

with p(hi|v) = σ

where σ(x) = ex/(1 + ex).

2. By symmetry arguments, write down the form of the conditional p(v|h).
3. Is p(h) factorised?

4. Can the partition function Z(W, a, b) be computed eﬃciently for the RBM?

Exercise 41. Consider

p(x) = φ(x1, x100)

99(cid:89)

i=1

φ(xi, xi+1)

Is it possible to compute argmax
x1,...,x100

p(x) eﬃciently?

Exercise 42. You are given that

x⊥⊥ y|(z, u) ,

u⊥⊥ z|∅

Derive the most general form of probability distribution p(x, y, z, u) consistent with these statements. Does
this distribution have a simple graphical model?

60

DRAFT March 9, 2010

Exercises

Exercise 43. The undirected graph
represents a Markov Network with nodes x1, x2, x3, x4, x5, count-
ing clockwise around the pentagon with potentials φ(xi, x1+mod(i,5)). Show that the joint distribution can
be written as

p(x1, x2, x3, x4, x5) = p(x1, x2, x5)p(x2, x4, x5)p(x2, x3, x4)

p(x2, x5)p(x2, x4)

(4.8.10)

and express the marginal probability tables explicitly as functions of the potentials φ(xi, xj).

Exercise 44.

Consider the Belief Network on the right.

1. Write down a Markov Network of p(x1, x2, x3).

2. Is your Markov Network a perfect map of p(x1, x2, x3)?

h1

h2

x1

x2

x3

Exercise 45. Two research labs work independently on the relationship between discrete variables x and
y. Lab A proudly announces that they have ascertained distribution pA(x|y) from data. Lab B proudly
announces that they have ascertained pB(y|x) from data.

1. Is it always possible to ﬁnd a joint distribution p(x, y) consistent with the results of both labs?

2. Is it possible to deﬁne consistent marginals p(x) and p(y), in the sense that p(x) =(cid:80)
and p(y) =(cid:80)

y pA(x|y)p(y)

x pB(y|x)p(x)? If so, explain how to ﬁnd such marginals. If not, explain why not.

Exercise 46. Research lab A states its ﬁndings about a set of variables x1, . . . , xn as a list LA of conditional
independence statements. Lab B similarly provides a list of conditional independence statements LB.

1. Is it possible to ﬁnd a distribution which is consistent with LA and LB?

2. If the lists also contain dependence statements, how could one attempt to ﬁnd a distribution that is

consistent with both lists?

Exercise 47.
Consider the distribution

p(x, y, w, z) = p(z|w)p(w|x, y)p(x)p(y)

(4.8.11)

1. Write p(x|z) using a formula involving (all or some of) p(z|w), p(w|x, y), p(x), p(y).
2. Write p(y|z) using a formula involving (all or some of) p(z|w), p(w|x, y), p(x), p(y).
3. Using the above results, derive an explicit condition for x ⊥⊥ y| z and explain if this is satisﬁed for

this distribution.

Exercise 48. Consider the distribution

p(t1, t2, y1, y2, h) = p(y1|t1, h)p(y2|t2, h)p(t1)p(t2)p(h)

1. Draw a Belief Network for this distribution.

2. Can the distribution

p(t1, t2, y1, y2) =(cid:88)

h

p(y1|t1, h)p(y2|t2, h)p(t1)p(t2)p(h)

be written as a (‘non-complete’) Belief Network?

3. Show that for p(t1, t2, y1, y2) as deﬁned above t1⊥⊥ y2|∅.

DRAFT March 9, 2010

(4.8.12)

(4.8.13)

61

Exercise 49. Consider the distribution

p(a, b, c, d) = φab(a, b)φbc(b, c)φcd(c, d)φda(d, a)

where the φ are potentials.

1. Draw a Markov Network for this distribution.

Exercises

(4.8.14)

2. Explain if the distribution can be represented as a (‘non-complete’) Belief Network.
3. Derive explicitly if a⊥⊥ c|∅.

Exercise 50. Show how for any singly-connected Markov network, one may construct a Markov equivalent
Belief Network.
Exercise 51. Consider a pairwise binary Markov network deﬁned on variables si ∈ {0, 1}, i = 1, . . . , N,
ij∈E φij(si, sj), where E is a given edge set and the potentials φij are arbitrary. Explain how
to translate such a Markov network into a Boltzmann machine.

with p(s) =(cid:81)

62

DRAFT March 9, 2010

CHAPTER 5

Eﬃcient Inference in Trees

5.1 Marginal Inference

Given a distribution p(x1, . . . , xn), inference is the process of computing functions of the distribution. For
example, computing a marginal conditioned on a subset of variables being in a particular state would be
an inference task. Similarly, computing the mean of a variable can be considered an inference task. The
main focus of this chapter is on eﬃcient inference algorithms for marginal inference in singly-connected
structures. An eﬃcient algorithm for multiply-connected graphs will be considered in chapter(6). Marginal
inference is concerned with the computation of the distribution of a subset of variables, possibly conditioned
on another subset. For example, given a joint distribution p(x1, x2, x3, x4, x5), a marginal inference given
evidence calculation is

p(x1 = tr, x2, x3, x4, x5)

(5.1.1)

(cid:88)

p(x5|x1 = tr) ∝

x2,x3,x4

Marginal inference for discrete models involves summation and will be the focus of our development. In
principle the algorithms carry over to continuous variable models although the lack of closure of most
continuous distributions under marginalisation (the Gaussian being a notable exception) can make the
direct transference of these algorithms to the continuous domain problematic.

5.1.1 Variable elimination in a Markov chain and message passing

A key concept in eﬃcient inference is message passing in which information from the graph is summarised
by local edge information. To develop this idea, consider the four variable Markov chain (Markov chains
are discussed in more depth in section(23.1))

p(a, b, c, d) = p(a|b)p(b|c)p(c|d)p(d)

(5.1.2)

as given in ﬁg(5.1), for which our task is to calculate the marginal p(a). For simplicity, we assume that
each of the variables has domain {0, 1}. Then

(cid:88)

p(a = 0) =

p(a = 0, b, c, d) =

b∈{0,1},c∈{0,1},d∈{0,1}

b∈{0,1},c∈{0,1},d∈{0,1}

p(a = 0|b)p(b|c)p(c|d)p(d) (5.1.3)

(cid:88)

a

b

c

d

Figure 5.1: A Markov chain is of the form p(xT )(cid:81)T−1

t=1 p(xt|xt+1)
for some assignment of the variables to labels xt. Variable Elim-
ination can be carried out in time linear in the number of vari-
ables in the chain.

63

We could carry out this computation by simply summing each of the probabilities for the 2 × 2 × 2 = 8
states of the variables b,c and d.

Marginal Inference

A more eﬃcient approach is to push the summation over d as far to the right as possible:

p(a = 0) = (cid:88)

b∈{0,1},c∈{0,1}

p(a = 0|b)p(b|c) (cid:88)
(cid:124)

d∈{0,1}

(cid:125)
(cid:123)(cid:122)
p(c|d)p(d)

γd(c)

(5.1.4)

where γd (c) is a (two state) potential. Similarly, we can distribute the summation over c as far to the
right as possible:

p(a = 0) = (cid:88)

b∈{0,1}

p(a = 0|b) (cid:88)
(cid:124)

c∈{0,1}

Then, ﬁnally,

p(a = 0) = (cid:88)

b∈{0,1}

p(a = 0|b)γc (b)

(cid:125)
(cid:123)(cid:122)
p(b|c)γd (c)

γc(b)

(5.1.5)

(5.1.6)

By distributing the summations we have made 2 + 2 + 2 = 6 additions, compared to 8 from the naive
approach. Whilst this saving may not appear much, the important point is that the number of computa-
tions for a chain of length T would scale linearly with T , as opposed to exponentially for the naive approach.

This procedure is naturally enough called variable elimination, since each time we sum over the states of
a variable, we eliminate it from the distribution. We can always perform variable elimination in a chain
eﬃciently since there is a natural way to distribute the summations, working inwards from the edges. Note
that in the above case, the potentials are in fact always distributions – we are just recursively computing
the marginal distribution of the right leaf of the chain.

One can view the elimination of a variable as passing a message (information) to a neighbouring vertex
on the graph. We can calculate a univariate-marginal of any singly-connected graph by starting at a
leaf of the tree, eliminating the variable there, and then working inwards, nibbling oﬀ each time a leaf
of the remaining tree. Provided we perform elimination from the leaves inwards, then the structure of
the remaining graph is simply a subtree of the original tree, albeit with the conditional probability table
entries modiﬁed to potentials which update under recursion. This is guaranteed to enable us to calculate
any marginal p(xi) using a number of summations which scales linearly with the number of variables in
the graph.

Finding conditional marginals for a chain

Consider the following inference problem, ﬁg(5.1) : Given

p(a, b, c, d) = p(a|b)p(b|c)p(c|d)p(d),
ﬁnd p(d|a). This can be computed using

(cid:88)

b,c

(cid:88)

b,c

p(d|a) ∝

p(a, b, c, d) ∝

p(a|b)p(b|c)p(c|d)p(d) ∝

(5.1.7)

p(c|d)p(d) ≡ γc (d)

(5.1.8)

(cid:88)

c

(cid:88)
(cid:124)

b

(cid:123)(cid:122)
(cid:125)
p(a|b)p(b|c)

γb(c)

the fact that(cid:80)

The missing proportionality constant is found by repeating the computation for all states of variable d.
Since we know that p(d|a) = kγc (d), where γc (d) is the unnormalised result of the summation, we can use

d p(d|a) = 1 to infer that k = 1/(cid:80)

d γc (d).

64

DRAFT March 9, 2010

Marginal Inference

In this example, the potential γb (c) is not a distribution in c, nor is γc (d). In general, one may view variable
elimination as the passing of messages in the form of potentials from nodes to their neighbours. For Belief
Networks, variable elimination passes messages that are distributions when following the direction of the
edge, and non-normalised potentials when passing messages against the direction of the edge.

Remark 5. Variable elimination in trees as matrix multiplication

Variable Elimination is related to the associativity of matrix multiplication. For equation (5.1.2)
above, we can deﬁne matrices

[Mab]i,j = p(a = i|b = j),
[Mcd]i,j = p(c = i|d = j),

[Mbc]i,j = p(b = i|c = j),
[Md]i = p(d = i),

[Ma]i = p(a = i)

Then the marginal Ma can be written

Ma = MabMbcMcdMd = Mab(Mbc(McdMd))

(5.1.9)

(5.1.10)

since matrix multiplication is associative. This matrix formulation of calculating marginals is called
the transfer matrix method, and is particularly popular in the physics literature[26].

Example 19 (Where will the ﬂy be?).

You live in a house with three rooms, labelled 1, 2, 3. There is a door between rooms 1 and 2 and another
between rooms 2 and 3. One cannot directly pass between rooms 1 and 3 in one time-step. An annoying
ﬂy is buzzing from one room to another and there is some smelly cheese in room 1 which seems to attract
the ﬂy more. Using xt for which room the ﬂy is in at time t, with dom(xt) = {1, 2, 3}, the movement of
the ﬂy can be described by a transition

p(xt+1 = i|xt = j) = Mij
where M is a transition matrix

 0.7 0.5

0
0.3 0.3 0.5
0
0.2 0.5



M =

(cid:80)3

T−1(cid:89)

t=1

The transition matrix is stochastic in the sense that, as required of a conditional probability distribution
i=1 Mij = 1. Given that the ﬂy is in room 1 at time 1, what is the probability of room occupancy at

time t = 5? Assume a Markov chain which is deﬁned by the joint distribution

(5.1.11)

(5.1.12)

(5.1.13)

(5.1.14)

p(x1, . . . , xT ) = p(x1)

p(xt+1|xt)

We are asked to compute p(x5|x1 = 1) which is given by

p(x5|x4)p(x4|x3)p(x3|x2)p(x2|x1 = 1)

(cid:88)

x4,x3,x2

Since the graph of the distribution is a Markov chain, we can easily distribute the summation over the
terms. This is most easily done using the transfer matrix method, giving

p(x5 = i|x1 = 1) = [M4v]i

DRAFT March 9, 2010

(5.1.15)

65

where v is a vector with components (1, 0, 0)T, reﬂecting the evidence that at time 1 the ﬂy is in room 1.
Computing this we have (to 4 decimal places of accuracy)

Marginal Inference

(5.1.16)

 0.5746

0.3180
0.1074



M4v =

p(xt+1) =(cid:88)

xt

Similarly, after 5 time-steps, the occupancy probabilities are (0.5612, 0.3215, 0.1173). The room occupancy
probability is converging to a particular distribution – the stationary distribution of the Markov chain.
One might ask where the ﬂy is after an inﬁnite number of time-steps. That is, we are interested in the
large t behaviour of

p(xt+1|xt)p(xt)

(5.1.17)

At convergence p(xt+1) = p(xt). Writing p for the vector describing the stationary distribution, this means

p = Mp

(5.1.18)

In other words, p is the eigenvector of M with eigenvalue 1[122]. Computing this numerically, the station-
ary distribution is (0.5435, 0.3261, 0.1304). Note that software packages usually return eigenvectors with
eTe = 1 – the unit eigenvector therefore will usually require normalisation to make this a probability.

5.1.2 The sum-product algorithm on factor graphs

Both Markov and belief networks can be represented using factor graphs. For this reason it is convenient
to derive a marginal inference algorithm for the FG since this then applies to both Markov and belief
networks. This is termed the sum-product algorithm since to compute marginals we need to distribute the
sum over variable states over the product of factors. In older texts, this is referred to as belief propagation.

Non-branching graphs : variable to variable messages

Consider the distribution

p(a, b, c, d) = f1 (a, b) f2 (b, c) f3 (c, d) f4 (d)

(5.1.19)

which has the factor graph represented in ﬁg(5.2) with factors as deﬁned by the f above. To compute the
marginal p(a, b, c), since the variable d only occurs locally, we use

p(a, b, c) =(cid:88)

p(a, b, c, d) =(cid:88)

d

d

f1 (a, b) f2 (b, c) f3 (c, d) f4 (d) = f1 (a, b) f2 (b, c)(cid:88)
(cid:124)

d

Similarly,

p(a, b) =(cid:88)

c

p(a, b, c) = f1 (a, b)(cid:88)
(cid:124)

c

Hence

µc→b (b) =(cid:88)

c

f2 (b, c) µd→c (c)

f2 (b, c) µd→c (c)

(cid:123)(cid:122)

µc→b(b)

(cid:125)

It is clear how one can recurse this deﬁnition of messages so that for a chain of length n variables the
marginal of the ﬁrst node can be computed in time linear in n. The term µc→b (b) can be interpreted as

66

DRAFT March 9, 2010

f3 (c, d) f4 (d)

(cid:123)(cid:122)

µd→c(c)

(cid:125)

(5.1.20)

(5.1.21)

(5.1.22)

Marginal Inference

f1

a

b

f2

f3

c

d

f4

Figure 5.2: For singly-connected structures with-
out branches, simple messages from one variable to
its neighbour may be deﬁned to form an eﬃcient
marginal inference scheme.

carrying marginal information from the graph beyond c.

For any singly-connected structure the factors at the edge of the graph can be replaced with messages
that reﬂect marginal information from the graph beyond that factor. For simple linear structures with no
branching, messages from variables to variables are suﬃcient. However, as we will see below, it is useful
in more general structures with branching to consider two types of messages, namely those from variables
to factors and vice versa.

General singly-connected factor graphs

The slightly more complex example,

p(a|b)p(b|c, d)p(c)p(d)p(e|d)

has the factor graph, ﬁg(5.3)

f1 (a, b) f2 (b, c, d) f3 (c) f4 (d, e) f5 (d)

(5.1.23)

(5.1.24)

If the marginal p(a, b) is to be represented by the amputated graph with messages on the edges, then

f4 (d, e)

(cid:125)

(5.1.25)

e

In this case it is natural to consider messages from factors to variables. Similarly, we can break the message
from the factor f2 into messages arriving from the two branches through c and d, namely

p(a, b) = f1 (a, b)(cid:88)
(cid:124)

c,d

f2 (b, c, d) f3 (c) f5 (d)(cid:88)

(cid:123)(cid:122)

µf2→b(b)

µf2→b (b) =(cid:88)

(cid:124)(cid:123)(cid:122)(cid:125)

f2 (b, c, d) f3 (c)
µc→f2 (c)

c,d

f5 (d)(cid:88)
(cid:124)
(cid:123)(cid:122)

e

µd→f2 (d)

f4 (d, e)

(cid:125)

Similarly, we can interpret

(cid:124)(cid:123)(cid:122)(cid:125)

µd→f2 (d) = f5 (d)
µf5→d(d)

(cid:88)
(cid:124)

e

f4 (d, e)

(cid:123)(cid:122)

(cid:125)

µf4→d(d)

(5.1.26)

(5.1.27)

To complete the interpretation we identify µc→f2 (c) ≡ µf3→c (c). In a non-branching link, one can more
simply use a variable to variable message.

a

f1

b

f2

f3

e

f4

c

d

f5

Figure 5.3: For a branching singly-connected graph,
it is useful to deﬁne messages from both factors to
variables, and variables to factors.

DRAFT March 9, 2010

67

For consistency of interpretation, one also can view the above as

A convenience of this approach is that the messages can be reused to evaluate other marginal inferences.
For example, it is clear that p(b) is given by

Marginal Inference

(5.1.28)

(5.1.29)

(5.1.30)

(5.1.31)

To compute the marginal p(a), we then have

f1 (a, b) µf2→b (b)

(cid:123)(cid:122)

µf1→a(a)

(cid:125)

(cid:123)(cid:122)

(cid:125)

f1 (a, b) µf2→b (b)
µb→f1 (b)

b

(cid:124)

f1 (a, b)

µf2→b (b)

p(a) =(cid:88)
(cid:124)

b

µf1→a (a) =(cid:88)

a

p(b) =(cid:88)
(cid:124)

(cid:123)(cid:122)
µf2→c (c) =(cid:88)

µf1→b(b)

(cid:125)

If we additionally desire p(c), we need to deﬁne the message from f2 to c,

f2 (b, c, d) µb→f2 (b) µd→f2 (d)

b,d

where µb→f2 (b) ≡ µf1→b (b). This demonstrates the reuse of already computed message from d to f2 to
compute the marginal p(c).

Deﬁnition 36 (Message schedule). A message schedule is a speciﬁed sequence of message updates. A
valid schedule is that a message can be sent from a node only when that node has received all requisite
messages from its neighbours. In general, there is more than one valid updating schedule.

Sum-Product algorithm

The sum-product algorithm is described below in which messages are updated as a function of incoming
messages. One then proceeds by computing the messages in a schedule that allows the computation of a
new message based on previously computed messages, until all messages from all factors to variables and
vice-versa have been computed.

Deﬁnition 37 (Sum-Product messages on Factor Graphs).

(cid:81)

(cid:0)

X f(cid:1), provided the

Given a distribution deﬁned as a product on subsets of the variables, p(X ) = 1
factor graph is singly-connected we can carry out summation over the variables eﬃciently.

f φf

Z

Initialisation Messages from extremal (simplical) node factors are initialised to the factor. Messages

from extremal (simplical) variable nodes are set to unity.

Variable to Factor message

µx→f (x) = (cid:89)

g∈{ne(x)\f}

µg→x (x)

f2

f1

f3

µ
f
1→

x(x)

µf2→x (x)

x ( x)

→

f

3

µ

µx→f (x)

f

x

68

DRAFT March 9, 2010

Marginal Inference

Factor to Variable message

µf→x (x) = (cid:88)

φf (X f ) (cid:89)

y∈X f\x

y∈{ne(f )\x}

µy→f (y)

We write(cid:80)

y∈X f\x to emphasise that we sum over all states in

the set of variables X f\x.

Marginal

(cid:89)

f∈ne(x)

p(x) ∝

µf→x (x)

y1

µ
y
1→

f(y

1)

y2

f1

f2

µy2→f (y2)
( y

→

f

y

3

µ

y3

µf1→x(x)

( x )

µ f 2 → x

f

µf→x (x)

x

3)

x

µx←f3(x)

f3

For marginal inference, the important information is the relative size of the message states so that we may
renormalise messages as we wish. Since the marginal will be proportional to the incoming messages for
that variable, the normalisation constant is trivially obtained using the fact that the marginal must sum
to 1. However, if we wish to also compute any normalisation constant using these messages, we cannot
normalise the messages since this global information will then be lost. To resolve this one may work with
log messages to avoid numerical under/overﬂow problems.

The sum-product algorithm is able to perform eﬃcient marginal inference in both Belief and Markov
Networks, since both are expressible as factor graphs. This is the reason for the preferred use of the Factor
Graph since it requires only a single algorithm and is agnostic to whether or not the graph is a locally or
globally normalised distribution.

5.1.3 Computing the marginal likelihood

For a distribution deﬁned as products over potentials φf

(cid:89)

f

φf

(cid:16)

φf

X f(cid:17)
(cid:16)
X f(cid:17)

p(x) =

1
Z

Z =(cid:88)

(cid:89)

X

f

the normalisation is given by

(cid:0)

X f(cid:1)

(5.1.32)

(5.1.33)

To compute this summation eﬃciently we take the product of all incoming messages to an arbitrarily
chosen variable x and then sum over the states of that variable:

Z =(cid:88)

(cid:89)

x

f∈ne(x)

µf→x (x)

If the factor graph is derived from setting a subset of variables of a BN in evidential states

p(X ,V) = p(X|V)
p(V)

then the summation over all non-evidential variables will yield the marginal on the visible (evidential)
variables, p(V).
For this method to work, the absolute (not relative) values of the messages are required, which prohibits
renormalisation at each stage of the message passing procedure. However, without normalisation the

DRAFT March 9, 2010

69

(5.1.34)

(5.1.35)

Marginal Inference

f4

a

d

f1

f3

(a)

b

c

f2

a

f1

f5

(b)

b

c

f2

Figure 5.4: (a) Factor graph with a loop. (b) Eliminating
the variable d adds an edge between a and c, demonstrating
that, in general, one cannot perform marginal inference in
loopy graphs by simply passing messages along existing
edges in the original graph.

numerical value of messages can become very small, particularly for large graphs, and numerical precision
issues can occur. A remedy in this situation is to work with log messages,

For this, the variable to factor messages

become simply

λ = log µ

λg→x (x)

µg→x (x)

g∈{ne(x)\f}

g∈{ne(x)\f}

µx→f (x) = (cid:89)
λx→f (x) = (cid:88)
µf→x (x) = (cid:88)
φf (X f ) (cid:89)
 (cid:88)

λf→x (x) = log

φf (X f )e

y∈X f\x

Naively, one may write

More care is required for the factors to variable messages, which are deﬁned by

y∈X f\x

y∈{ne(f )\x}

µy→f (y)



y∈{ne(f )\x} λy→f (y)

(cid:80)

(5.1.36)

(5.1.37)

(5.1.38)

(5.1.39)

(5.1.40)

However, the exponentiation of the log messages will cause potential numerical precision problems. A
solution to this numerical diﬃculty is obtained by ﬁnding the largest value of the incoming log messages,

λ

∗
y→f = max

y∈{ne(f )\x} λy→f (y)

Then

λf→x (x) = λ

 (cid:88)

y∈X f\x

∗
y→f + log
(cid:80)

(cid:80)

φf (X f )e

y∈{ne(f )\x} λy→f (y)−λ∗

y→f



(5.1.41)

(5.1.42)

By construction the terms e
contributions to the summation are computed accurately.

y∈{ne(f )\x} λy→f (y)−λ∗

y→f will be ≤ 1. This ensures that the dominant numerical

Log marginals are readily found using

log p(x) = (cid:88)

f∈{ne(x)}

λf→x (x)

(5.1.43)

70

DRAFT March 9, 2010

Other Forms of Inference

5.1.4 The problem with loops

Loops cause a problem with variable elimination (or message passing) techniques since once a variable is
eliminated the structure of the ‘amputated’ graph in general changes. For example, consider the FG

p(a, b, c, d) = f1 (a, b) f2 (b, c) f3 (c, d) f4 (a, d)

The marginal p(a, b, c) is given by

p(a, b, c) = f1 (a, b) f2 (b, c)(cid:88)
(cid:124)

d

f3 (c, d) f4 (a, d)

(cid:123)(cid:122)

f5(a,c)

(cid:125)

(5.1.44)

(5.1.45)

which adds a link ac in the amputated graph, see ﬁg(5.4). This means that one cannot account for
information from variable d by simply updating potentials on links in the original graph – one needs to
account for the fact that the structure of the graph changes. The Junction Tree algorithm, chapter(6) is a
widely used technique to deal with this and essentially combines variables together in order to make a new
singly-connected graph for which the graph structure remains singly-connected under variable elimination.

5.2 Other Forms of Inference

5.2.1 Max-Product

A common interest is the most likely state of distribution. That is

argmax
x1,x2,...,xn

p (x1, x2, . . . , xn)

(5.2.1)

To compute this eﬃciently we exploit any factorisation structure of the distribution, analogous to the
sum-product algorithm. That is, we aim to distribute the maximization so that only local computations
are required.

To develop the algorithm, consider a function which can be represented as an undirected chain,

f(x1, x2, x3, x4) = φ(x1, x2)φ(x2, x3)φ(x3, x4)

(5.2.2)
for which we wish to ﬁnd the joint state x∗ which maximises f. Firstly, we calculate the maximum value
of f. Since potentials are non-negative, we may write

max

x

f(x) = max

x1,x2,x3,x4

φ(x1, x2)φ(x2, x3)φ(x3, x4) = max
x1,x2,x3

φ(x1, x2)φ(x2, x3) max
x4

φ(x3, x4)

(cid:124)

(cid:124)

(cid:125)

(cid:123)(cid:122)
(cid:123)(cid:122)

γ(x1)

γ(x3)
φ(x1, x2)γ(x2)

(cid:125)

= max
x1,x2

φ(x1, x2) max
x3

(cid:124)

(cid:123)(cid:122)

γ(x2)

(cid:125)

φ(x2, x3)γ(x3)

= max
x1,x2

φ(x1, x2)γ(x2) = max
x1

max
x2

The ﬁnal equation corresponds to solving a single variable optimisation and determines both the optimal
value of the function f and also the optimal state x∗
γ(x1). Given x∗
1, the optimal x2 is given
by x∗
2, x3)γ(x3), and so on. This procedure is
called backtracking. Note that we could have equally started at the other end of the chain by deﬁning
messages γ that pass information from xi to xi+1.

1, x2)γ(x2), and similarly x∗

1 = argmax
φ(x∗

2 = argmax

3 = argmax

φ(x∗

x1

x3

x2

The chain structure of the function ensures that the maximal value (and its state) can be computed in
time which scales linearly with the number of factors in the function. There is no requirement here that
the function f corresponds to a probability distribution (though the factors must be non-negative).

DRAFT March 9, 2010

71

Other Forms of Inference

(5.2.3)

Example 20. Consider a distribution deﬁned over binary variables:

p(a, b, c) ≡ p(a|b)p(b|c)p(c)

with

p(a = tr|b = tr) = 0.3, p(a = tr|b = fa) = 0.2, p(b = tr|c = tr) = 0.75
p(b = tr|c = fa) = 0.1, p(c = tr) = 0.4

What is the most likely joint conﬁguration, argmax

p(a, b, c)?

a,b,c

Naively, we could evaluate p(a, b, c) over all the 8 joint states of a, b, c and select that states with highest
probability. A message passing approach is to deﬁne

γ(b) ≡ max

c

p(b|c)p(c)

For the state b = tr,

p(b = tr|c = tr)p(c = tr) = 0.75 × 0.4,

p(b = tr|c = fa)p(c = fa) = 0.1 × 0.6

Hence, γ(b = tr) = 0.75 × 0.4 = 0.3. Similarly, for b = fa,

p(b = fa|c = tr)p(c = tr) = 0.25 × 0.4

p(b = fa|c = fa)p(c = fa) = 0.9 × 0.6

Hence, γ(b = fa) = 0.9 × 0.6 = 0.54.
We now consider

γ(a) ≡ max

b

p(a|b)γ(b)

For a = tr, the state b = tr has value

p(a = tr|b = tr)γ(b = tr) = 0.3 × 0.3 = 0.09

and state b = fa has value

p(a = tr|b = fa)γ(b = fa) = 0.2 × 0.54 = 0.108

Hence γ(a = tr) = 0.108. Similarly, for a = fa, the state b = tr has value

p(a = fa|b = tr)γ(b = tr) = 0.7 × 0.3 = 0.21

and state b = fa has value

p(a = fa|b = fa)γ(b = fa) = 0.8 × 0.54 = 0.432

giving γ(a = fa) = 0.432. Now we can compute the optimal state

∗ = argmax
a

a

γ(a) = fa

Given this optimal state, we can backtrack, giving

∗ = argmax
b

b

p(a = fa|b)γ(b) = fa, c

∗ = argmax

c

p(b = fa|c)p(c) = fa

(5.2.4)

(5.2.5)

(5.2.6)

(5.2.7)

(5.2.8)

(5.2.9)

(5.2.10)

(5.2.11)

(5.2.12)

(5.2.13)

Note that in the backtracking process, we already have all the information required from the computation
of the messages γ.

72

DRAFT March 9, 2010

Other Forms of Inference

Using a factor graph

One can also use the factor graph to compute the joint most probable state. Provided that a full schedule
of message passing has occurred, the product of messages into a variable equals the maximum value of the
joint function with respect to all other variables. One can then simply read oﬀ the most probable state
by maximising this local potential.

One then proceeds in computing the messages in a schedule that allows the computation of a new message
based on previously computed messages, until all messages from all factors to variables and vice-versa
have been computed. The message updates are given below.

Deﬁnition 38 (Max-Product messages on Factor Graphs).

(cid:81)

(cid:0)

X f(cid:1), provided the

Given a distribution deﬁned as a product on subsets of the variables, p(X ) = 1
factor graph is singly-connected we can carry out maximisation over the variables eﬃciently.

f φf

Z

Initialisation Messages from extremal (simplical) node factors are initialised to the factor. Messages

from extremal (simplical) variable nodes are set to unity.

Variable to Factor message

µx→f (x) = (cid:89)

g∈{ne(x)\f}

µg→x (x)

f1

µ
f
1→

x(x)

f2

µf2→x (x)

µx→f (x)

f

x

x ( x)

→

f

3

µ

f3

y1

Factor to Variable message

µf→x (x) = max
y∈X f\x

φf (X f ) (cid:89)

y∈{ne(f )\x}

Maximal State

x∗ = argmax

x

(cid:89)

f∈ne(x)

µf→x (x)

µy→f (y)

y2

f1

f2

µy2→f (y2)
( y

→

f

y

3

µ

y3

µf1→x(x)

( x )

µ f 2 → x

µ
y
1→

f(y

1)

f

µf→x (x)

x

3)

x

µx←f3(x)

f3

In earlier literature, this algorithm is called belief revision.

5.2.2 Finding the N most probable states

It is often of interest to calculate not just the most likely joint state, but the N most probable states,
particularly in cases where the optimal state is only slightly more probable than other states. This is an
interesting problem in itself and can be tackled with a variety of methods. A general technique is given by
Nilson[210] which is based on the Junction Tree formalism, chapter(6), and the construction of candidate
lists, see for example [69].

DRAFT March 9, 2010

73

Other Forms of Inference

4

6

2

3

5

1

8

7

9

Figure 5.5: State transition diagram (weights not shown). The shortest
(unweighted) path from state 1 to state 7 is 1 − 2 − 7. Considered as a
Markov chain (random walk), the most probable path from state 1 to state
7 is 1− 8− 9− 7. The latter path is longer but more probable since for the
path 1 − 2 − 7, the probability of exiting from state 2 into state 7 is 1/5
(assuming each transition is equally likely). See demoMostProbablePath.m

For singly-connected structures, several approaches have been developed. For the hidden Markov model,
section(23.2) a simple algorithm is the N-Viterbi approach which stores the N-most probable messages at
each stage of the propagation, see for example [256]. A special case of Nilsson’s approach is available for
hidden Markov models[211] which is the particularly eﬃcient for large state spaces.

For more general singly-connected graphs one can extend the max-product algorithm to an N-max-product
algorithm by retaining at each stage the N most probable messages, see below. These techniques require
N to be speciﬁed a-priori compared to anytime alternatives, [298]. An alternative approach for singly-
connected networks was developed in [269]. Of particular interest is the application of the singly-connected
algorithms as an approximation when for example Nilsson’s approach on a multiply-connected graph is
intractable[298].

N-max-product

The algorithm for N-max-product is a straightforward modiﬁcation of the standard algorithms. Compu-
tationally, a straightforward way to accomplish this is to introduce an additional variable for each message
that is used to index the most likely messages. For example, consider the distribution

p(a, b, c, d) = φ(a, b)φ(b, c)φ(b, d)

for which we wish to ﬁnd the two most probable values. Using the notation

imax

x

f(x)

for the ith highest value of f(x), the maximisation over d can be expressed using the message

γd(b, 1) = 1max

d

φ(b, d),

γd(b, 2) = 2max

d

φ(b, d)

(5.2.14)

(5.2.15)

(5.2.16)

Using a similar message for the maximisation over c, the 2 most likely states of p(a, b, c, d) can be computed
using

1:2max

a,b,c,d

= 1:2max

a,b,mb,md

φ(a, b)γd(b, md)γc(b, mc)

(5.2.17)

where mc and md index the highest values. At the ﬁnal stage we now have a table with dim a × dim b × 4
entries, from which we compute the highest two states.

The generalisation of this to the factor graph formalism is straightforward and contained in maxNprodFG.m.
Essentially the only modiﬁcation required is to deﬁne extended messages which contain the N-most likely
messages computed at each stage. At a junction of the factor graph, all the messages from the neighbours,
along with their N-most probable tables are multiplied together into a large table. For a factor to variable
message the N-most probable messages are retained, see maxNprodFG.m. The N-most probable states for
each variable can then be read oﬀ by ﬁnding the variable state that maximises the product of incoming
extended messages.

74

DRAFT March 9, 2010

Other Forms of Inference

5.2.3 Most probable path and shortest path

What is the most likely path from state a to state b for an N state Markov chain? Note that this is not
necessarily the same as the shortest path, as explained in ﬁg(5.5).

If assume that a length T path exists, this has probability

p(s2|s1 = a)p(s3|s2) . . . p(sT = b|sT−1)

(5.2.18)

Finding the most probable path can then be readily solved using the max-product (or max-sum algorithm
for the log-transitions) on a simple serial factor graph. To deal with the issue that we don’t know the
optimal T , one approach is to redeﬁne the probability transitions such that the desired state b is an ab-
sorbing state of the chain (that is, one can enter this state but not leave it). With this redeﬁnition, the
most probable joint state will correspond to the most probable state on the product of N transitions –
once the absorbing state is reached the chain will stay in this state, and hence the most probable path
can be read oﬀ from the sequence of states up to the ﬁrst time the chain hits the absorbing state. This
approach is demonstrated in demoMostProbablePath.m, along with the more direct approaches described
below.

An alternative, cleaner approach is as follows: for the Markov chain we can dispense with variable-to-factor
and factor-to-variable messages and use only variable-to-variable messages. If we want to ﬁnd the most
likely set of states a, s2, . . . , sT−1, b to get us there, then this can be computed by deﬁning the maximal
path probability E (a → b, T ) to get from a to b in T -timesteps:

E (a → b, T ) = max
= max

s2,...,sT−1

s3,...,sT−1

p(s2|s1 = a)p(s3|s2)p(s4|s3) . . . p(sT = b|sT−1)
(cid:124)
max
s2

(cid:125)
p(s2|s1 = a)p(s3|s2)

p(s4|s3) . . . p(sT = b|sT−1)

(cid:123)(cid:122)

γ2→3(s3)

To compute this eﬃciently we deﬁne messages
γt−1→t (st) p(st+1|st),

γt→t+1 (st+1) = max

st

t ≥ 2,

γ1→2 (s2) = p(s2|s1 = a)

until the point

E (a → b, T ) = max

sT−1

γT−2→T−1 (sT−1) p(sT = b|sT−1) = γT−1→T (sT = b)

(5.2.19)

(5.2.20)

(5.2.21)

(5.2.22)

We can now proceed to ﬁnd the maximal path probability for timestep T + 1. Since the messages up to
time T − 1 will be the same as before, we need only compute one additional message, γT−1→T (sT ), from
which

E (a → b, T + 1) = max

sT

γT−1→T (sT ) p(sT +1 = b|sT ) = γT→T +1 (sT +1 = b)

(5.2.23)

We can proceed in this manner until we reach E (a → b, N) where N is the number of nodes in the graph.
We don’t need to go beyond this number of steps since those that do must necessarily contain non-simple
paths. (A simple path is one that does not include the same state more than once.) The optimal time t∗ is
then given by which of E (a → b, 2) , . . . , E (a → b, N) is maximal. Given t∗ one can begin to backtrack1.

Since

E (a → b, t

∗) = max
st∗−1

γt∗−2→t∗−1 (st∗−1) p(st∗ = b|st∗−1)

we know the optimal state

s∗
t∗−1 = argmax
st∗−1

γt∗−2→t∗−1 (st∗−1) p(st∗ = b|st∗−1)

(5.2.24)

(5.2.25)

1An alternative to ﬁnding t∗ is to deﬁne self-transitions with probability 1, and then use a ﬁxed time T = N . Once the
desired state b is reached, the self-transition then preserves the chain in state b for the remaining timesteps. This procedure
is used in mostprobablepathmult.m

DRAFT March 9, 2010

75

We can then continue to backtrack:

s∗
t∗−2 = argmax
st∗−2

γt∗−3→t∗−2 (st∗−2) p(s∗

t∗−1|st∗−2)

and so on. See mostprobablepath.m.

Other Forms of Inference

(5.2.26)

• In the above derivation we do not use any properties of probability, except that p must be non-
negative (otherwise sign changes can ﬂip a whole sequence ‘probability’ and the local message recur-
sion no longer applies). One can consider the algorithm as ﬁnding the optimal ‘product’ path from
a to b.

• It is straightforward to modify the algorithm to solve the (single-source, single-sink) shortest weighted
path problem. One way to do this is to replace the Markov transition probabilities with edge weights
exp(−u(st|st−1)), where u(st|st−1) is inﬁnite if there is no edge from st−1 to st. This approach is
taken in shortestpath.m which is able to deal with either positive or negative edge weights. This
method is therefore more general than the well-known Dijkstra’s algorithm [111] which requires
weights to be positive. If a negative edge cycle exists, the code returns the shortest weighted length
N path, where N is the number of nodes in the graph. See demoShortestPath.m.

• The above algorithm is eﬃcient for the single-source, single-sink scenario, since the messages contain

only N states, meaning that the overall storage is O(N 2).

• As it stands, the algorithm is numerically impractical since the messages are recursively multiplied
by values usually less than 1 (at least for the case of probabilities). One will therefore quickly run
into numerical underﬂow (or possibly overﬂow in the case of non-probabilities) with this method.

To ﬁx the ﬁnal point above, it is best to work by deﬁning the logarithm of E. Since this is a monotonic
transformation, the most probable path deﬁned through log E is the same as that obtained from E. In
this case

L (a → b, T ) = max

s2,...,sT−1

= max

s2,...,sT−1

(cid:34)
log [p(s2|s1 = a)p(s3|s2)p(s4|s3) . . . p(sT = b|sT−1)]
log p(s2|s1 = a) +

(cid:35)
log p(st|st−1) + log p(sT = b|sT−1)

T−1(cid:88)

t=2

We can therefore deﬁne new messages

λt→t+1 (st+1) = max

st

[λt−1→t (st) + log p(st+1|st)]

One then proceeds as before by ﬁnding the most probable t∗ deﬁned on L, and backtracks.

(5.2.27)

(5.2.28)

(5.2.29)

graphical model corresponding to this simple Markov chain is the Belief Network (cid:81)

Remark 6. A possible confusion is that optimal paths can be eﬃciently found ‘when the graph is
loopy’. Note that the graph in ﬁg(5.5) is a state-transition diagram, not a graphical model. The
t p(st|st−1), a
linear serial structure. Hence the underlying graphical model is a simple chain, which explains why
computation is eﬃcient.

Most probable path (multiple-source, multiple-sink)

If we need the most probable path between all states a and b, one could re-run the above single-source-
single-sink algorithm for all a and b. A computationally more eﬃcient approach is to observe that one can
deﬁne a message for each starting state a:

γt→t+1 (st+1|a) = max

st

γt−1→t (st|a) p(st+1|st)

76

(5.2.30)

DRAFT March 9, 2010

Other Forms of Inference

Algorithm 1 Compute marginal p(x1|evidence) from distribution p(x) = (cid:81)
1: procedure Bucket Elimination(p(x) =(cid:81)

evidential variables are ordered x1, . . . , xn.

f φf ({x}f ).)

Initialize all Bucket potentials to unity.
while There are potentials left in the distribution do

For each potential φf , its highest variable xj (according to the ordering).
Multiply φf with the potential in Bucket j and remove φf the distribution.

end while
for i = Bucket n to 1 do

For Bucket i sum over the states of variable xi and call this potential γi
Identify the highest variable xh of potential γi
Multiply the existing potential in Bucket h by γi

end for
The marginal p(x1|evidence) is proportional to γ1.
return p(x1|evidence)

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end procedure

f φf ({x}f ). Assumes non-

(cid:46) Fill Buckets

(cid:46) Empty Buckets

(cid:46) The conditional marginal.

and continue until we ﬁnd the maximal path probability matrix for getting from any state a to any state
b in T timesteps:

E (a → b, T ) = max

sT−1

γT−2→T−1 (sT−1|a) p(sT = b|sT−1)

(5.2.31)

Since we know the message γT−2→T−1 (sT−1|a) for all states a, we can readily compute the most probable
path from all starting states a to all states b after T steps. This requires passing an N × N matrix message
γ. We can then proceed to the next timestep T + 1. Since the messages up to time T − 1 will be the same
as before, we need only compute one additional message, γT−1→T (sT ), from which

E (a → b, T + 1) = max

sT

γT−1→T (sT|a) p(sT +1 = b|sT )

(5.2.32)

In this way one can then eﬃciently compute the optimal path probabilities for any starting state a and
end state b after t timesteps. To ﬁnd the optimal corresponding path, backtracking proceeds as before,
see mostprobablepathmult.m. One can also use the same algorithm to solve the multiple-source, multiple
sink shortest path problem. This algorithm is a variant of the Floyd-Warshall-Roy algorithm[111] for
ﬁnding shortest weighted summed paths on a directed graph (the above algorithm enumerates through
time, whereas the FWR algorithm enumerates through states).

5.2.4 Mixed inference

An often encountered situation is to infer the most likely state of a joint marginal, possibly given some
evidence. For example, given a distribution p(x1, . . . , xn), ﬁnd

(cid:88)

argmax
x1,x2,...,xm

p(x1, x2, . . . , xm) = argmax
x1,x2,...,xm

xm+1,...,xn

p(x1, . . . , xn)

(5.2.33)

In general, even for tree structured p(x1, . . . , xn), the optimal marginal state cannot be computed eﬃciently.
One way to see this is that due to the summation the resulting joint marginal does not have a structured
factored form as products of simpler functions of the marginal variables. Finding the most probable
joint marginal then requires a search over all the joint marginal states – a task exponential in m. An
approximate solution is provided by the EM algorithm (see section(11.2) and exercise(57)).

DRAFT March 9, 2010

77

Inference in Multiply-Connected Graphs

e

c

b

g

a

d

f

p(e)p(g|d, e)

p(c|a)

p(b)p(d|a, b)

p(b)p(d|a, b)

γE (d, g)

γE (d, g)

p(a)

p(f|d)

p(a)

p(a)γB (d, a)

p(a)γB (d, a)

p(f|d)

p(f|d)

p(f|d)γG (d)

p(f|d)γG (d) γA (d)

γD (f)

node is eliminated from the graph. The second stage of eliminating c is trivial since (cid:80)

Figure 5.6: The bucket elimination algorithm applied to the graph ﬁg(2.1). At each stage, at least one
c p(c|a) = 1 and

has therefore been skipped over since this bucket does not send any message.

5.3 Inference in Multiply-Connected Graphs

5.3.1 Bucket elimination

We consider here a general conditional marginal variable elimination method that works for any distribu-
tion (including multiply connected graphs). The algorithm assumes the distribution is in the form

(cid:89)

f

p(x1, . . . , xn) ∝

φ(Xf )

(5.3.1)

(5.3.2)

and that the task is to compute p(x1|evidence). For example, for

p(x1, x2, x3, x4) = p(x1|x2)p(x2|x3)p(x3|x4)p(x4)

we could use

φ1(x1, x2) = p(x1|x2), φ2(x2, x3) = p(x2|x3), φ3(x3, x4) = p(x3|x4)p(x4)

(5.3.3)
The sets of variables here are X1 = (x1, x2),X2 = (x2, x3),X3 = (x3, x4). In general, the construction of
potentials for a distribution is not unique. The task of computing a marginal in which a set of variables
xn+1, . . . are clamped to their evidential states is

p(x1|evidence) ∝ p(x1, evidence) = (cid:88)

(cid:89)

x2,...,xn

f

φf (Xf )

(5.3.4)

The algorithm is given in algorithm(11) and can be considered a way to organise the distributed summation[79].
The algorithm is best explained by a simple example, as given below.

Example 21 (Bucket Elimination). Consider the problem of calculating the marginal p(f) of

p(a, b, c, d, e, f, g) = p(f|d)p(g|d, e)p(c|a)p(d|a, b)p(a)p(b)p(e),

see ﬁg(2.1).

p(f) = (cid:88)

p(a, b, c, d, e, f, g) = (cid:88)

a,b,c,d,e,g

a,b,c,d,e,g

p(f|d)p(g|d, e)p(c|a)p(d|a, b)p(a)p(b)p(e)

(5.3.5)

(5.3.6)

78

DRAFT March 9, 2010

Inference in Multiply-Connected Graphs

p(f) = (cid:88)

terms, we can write

p(f) = (cid:88)

a,d,g

p(f) =(cid:88)
p(f) =(cid:88)

d

d

We can distribute the summation over the various terms as follows: e,b and c are end nodes, so that we
can sum over their values:

(cid:32)(cid:88)

b

(cid:33)(cid:32)(cid:88)

(cid:33)
p(g|d, e)p(e)

(cid:33)(cid:32)(cid:88)
b p(d|a, b)p(b) ≡ γB (a, d), (cid:80)

e

p(f|d)p(a)

a,d,g

For convenience, lets write the terms in the brackets as (cid:80)
γE (d, g). The term(cid:80)

c

p(d|a, b)p(b)

p(c|a)

e p(g|d, e)p(e) ≡
c p(c|a) is equal to unity, and we therefore eliminate this node directly. Rearranging

(5.3.7)

p(f|d)p(a)γB (a, d) γE (d, g)

(5.3.8)

If we think of this graphically, the eﬀect of summing over b, c, e is eﬀectively to remove or ‘eliminate’ those
variables. We can now carry on summing over a and g since these are end points of the new graph:

(cid:32)(cid:88)

(cid:33)(cid:32)(cid:88)

(cid:33)

p(f|d)

p(a)γB (a, d)

γE (d, g)

a

g

Again, this deﬁnes new functions γA (d), γG (d), so that the ﬁnal answer can be found from

(5.3.9)

(5.3.10)

p(f|d)γA (d) γG (d)

We illustrate this in ﬁg(5.6). Initially, we deﬁne an ordering of the variables, beginning with the one that
we wish to ﬁnd the marginal for – a suitable ordering is therefore, f, d, a, g, b, c, e. Then starting with the
highest bucket e (according to our ordering f, d, a, g, b, c, e), we put all the functions that mention e in the
e bucket. Continuing with the next highest bucket, c, we put all the remaining functions that mention c
in this c bucket, etc. The result of this initialisation procedure is that terms (conditional distributions) in
the DAG are distributed over the buckets, as shown in the left most column of ﬁg(5.6). Eliminating then
the highest bucket e, we pass a message to node g. Immediately, we can also eliminate bucket c since this
sums to unity. In the next column, we have now two less buckets, and we eliminate the highest remaining
bucket, this time b, passing a message to bucket a.

There are some important observations we can make about bucket elimination:

1. To compute say p(x2|evidence) we need to re-order the variables (so that the required marginal
variable is labelled x1) and repeat Bucket Elimination. Hence each query (calculation of a marginal
in this case) requires re-running the algorithm. It would be more eﬃcient to reuse messages, rather
than recalculating them each time.

2. In general, Bucket Elimination constructs multi-variable messages γ from Bucket to Bucket. The
storage requirements of a multi-variable message are exponential in the number of variables of the
message.

3. For trees we can always choose a variable ordering to render the computational complexity to be
linear in the number of variables. Such an ordering is called perfect, deﬁnition(49), and indeed it
can be shown that a perfect ordering can always easily be found for singly-connected graphs (see
[86]). However, orderings exist for which Bucket Elimination will be extremely ineﬃcient.

5.3.2 Loop-cut conditioning

For distributions which contain a loop (there is more than one path between two nodes in the graph when
the directions are removed), we run into some diﬃculty with the message passing routines such as the
sum-product algorithm which are designed to work on singly-connected graphs only. One way to solve

DRAFT March 9, 2010

79

c

a

f

b

g

d

(a)

e

c

b

g

e

a

f

d

(b)

Notes

5.7:

Figure
A multiply-
connected graph (a) reduced to
a singly-connected graph (b) by
conditioning on the variable c.

the diﬃculties of multiply connected (loopy) graphs is to identify nodes that, when removed, would reveal
a singly-connected subgraph[219]. Consider the example if ﬁg(5.7). Imagine that we wish to calculate a
marginal, say p(d). Then

p(d) =(cid:88)

(cid:88)

c

a,b,e,f,g

(cid:124)
(cid:123)(cid:122)
(cid:125)
p(c|a)p(a)

p∗(a)

(cid:124) (cid:123)(cid:122) (cid:125)
p(d|a, b)p(b) p(f|c, d)
p∗(f|d)

p(g|d, e)

(5.3.11)

where the p∗ deﬁnitions are not necessarily distributions. For each state of c, the form of the products of
factors remaining as a function of a, b, e, f, g is singly-connected, so that standard singly-connected message
passing can be used to perform inference. We will need to do perform inference for each state of variable
c, each state deﬁning a new singly-connected graph (with the same structure) but with modiﬁed potentials.

More generally, we can deﬁne a set of variables C, called the loop cut set and run singly-connected inference
for each joint state of the cut-set variables C. This can also be used for ﬁnding the most likely state of a
multiply-connected joint distribution as well. Hence, for a computational price exponential in the loop-
cut size, we can calculate the marginals (or the most likely state) for a multiply-connected distribution.
However, determining a small cut set is in general diﬃcult, and there is no guarantee that this will anyway
be small for a given graph. Whilst this method is able to handle loops in a general manner, it is not
particularly elegant since the concept of messages now only applies conditioned on the cut set variables,
and how to re-use messages for inference of additional quantities of interest becomes unclear. We will
discuss an alternative method for handling multiply connected distributions in chapter(6).

5.4 Message Passing for Continuous Distributions

For parametric continuous distributions p(x|θx), message passing corresponds to passing parameters θ of
the distributions. For the sum-product algorithm, this requires that the operations of multiplication and
integration over the variables are closed with respect to the family of distributions. This is the case, for
example, for the Gaussian distribution – the marginal (integral) of a Gaussian is another Gaussian, and
the product of two Gaussians is a Gaussian, see section(8.6). This means that we can then implement the
sum-product algorithm based on passing mean and covariance parameters. To implement this requires
some tedious algebra to compute the appropriate message parameter updates. At this stage, the complex-
ities from performing such calculations are a potential distraction, though the interested reader may refer
to demoSumprodGaussMoment.m, demoSumprodGaussCanon.m and demoSumprodGaussCanonLDS.m and also
chapter(24) for examples of message passing with Gaussians. For more general exponential family dis-
tributions, message passing is essentially straightforward, though again the speciﬁcs of the updates may
be tedious to work out.
In cases where the operations of marginalisation and products are not closed
within the family, the distributions need to be projected back to the chosen message family. Expectation
propagation, section(28.7) is relevant in this case.

5.5 Notes

A take-home message from this chapter is that (non-mixed) inference in singly-connected structures is
usually computationally tractable. Notable exceptions are when the message passing operations are not-
closed within the message family, or representing messages explicitly requires an exponential amount of
space. This happens for example when the distribution can contain both discrete and continuous variables,

80

DRAFT March 9, 2010

Code

such as the Switching Linear Dynamical system, which we discuss in chapter(25).

Broadly speaking, inference in multiply-connected structures is more complex and may be intractable.
However, we do not want to give the impression that this is always the case. Notable exceptions are:
ﬁnding the MAP state in an attractive pairwise MRF, section(28.8); ﬁnding the MAP and MPM state in
a binary planar MRF with pure interactions, see for example [115, 243]. For N variables in the graph,

a naive use of the junction tree algorithm for these inferences would result in an O(cid:0)2N(cid:1) computation,
whereas clever algorithms are able to return the exact results in O(cid:0)N 3(cid:1) operations. Of interest is bond

propagation[177] which is an intuitive node elimination method to arrive at the MPM inference in pure-
interaction Ising models.

5.6 Code

The code below implements message passing on a tree structured factor graph. The FG is stored as an
adjacency matrix with the message between FG node i and FG node j given in Ai,j.
FactorGraph.m: Return a Factor Graph adjacency matrix and message numbers
sumprodFG.m: Sum-Product algorithm on a Factor Graph

In general it is recommended to work in log-space in the Max-Product case, particularly for large graphs
since the produce of messages can become very small. The code provided does not work in log space and
as such may not work on large graphs; writing this using log-messages is straightforward but leads to less
readable code. An implementation based on log-messages is left as an exercise for the interested reader.
maxprodFG.m: Max-Product algorithm on a Factor Graph
maxNprodFG.m: N-Max-Product algorithm on a Factor Graph

5.6.1 Factor graph examples

For the distribution from ﬁg(5.3), the following code ﬁnds the marginals and most likely joint states. The
number of states of each variable is chosen at random.
demoSumprod.m: Test the Sum-Product algorithm
demoMaxprod.m: Test the Max-Product algorithm
demoMaxNprod.m: Test the Max-N-Product algorithm

5.6.2 Most probable and shortest path

mostprobablepath.m: Most Probable Path
demoMostProbablePath.m: Most probable versus shortest path demo

The shortest path demo works for both positive and negative edge weights. If negative weight cycles exist,
the code ﬁnds the best length N shortest path. demoShortestPath.m: Shortest path demo
mostprobablepathmult.m: Most Probable Path – multi-source, multi-sink
demoMostProbablePathMult.m: Demo of most probable path – multi-source, multi-sink

5.6.3 Bucket elimination

The eﬃcacy of Bucket Elimination depends critically on the elimination sequence chosen. In the demon-
stration below we ﬁnd the marginal of a variable in the Chest Clinic exercise using a randomly chosen
elimination order. The desired marginal variable is speciﬁed as the last to be eliminated. For comparison
we use an elimination sequence based on decimating a triangulated graph of the model, as discussed in
section(6.5.1), again under the constraint that the last variable to be ‘decimated’ is the marginal variable of

DRAFT March 9, 2010

81

Exercises

interest. For this smarter choice of elimination sequence, the complexity of computing this single marginal
is roughly the same as that for the Junction Tree algorithm, using the same triangulation.
bucketelim.m: Bucket Elimination
demoBucketElim.m: Demo Bucket Elimination

5.6.4 Message passing on Gaussians

The following code hints at how message passing may be implemented for continuous distributions. The
reader is referred to the BRMLtoolbox for further details and also section(8.6) for the algebraic manipu-
lations required to perform marginalisation and products of Gaussians. The same principal holds for any
family of distributions which is closed under products and marginalisation, and the reader may wish to
implement speciﬁc families following the method outlined for Gaussians.
demoSumprodGaussMoment.m: Sum-product message passing based on Gaussian Moment parameterisation

5.7 Exercises

Exercise 52. Given a pairwise singly connected Markov Network of the form

φ (xi, xj)

(5.7.1)

(cid:89)

i∼j

p(x) =

1
Z

explain how to eﬃciently compute the normalisation factor (also called the partition function) Z as a
function of the potentials φ.

Exercise 53. You are employed by a web start up company that designs virtual environments, in which
players can move between rooms. The rooms which are accessible from another in one time step is given
by the 100 × 100 matrix M, stored in virtualworlds.mat, where Mij = 1 means that there is a door
between rooms i and j (Mij = Mji). Mij = 0 means that there is no door between rooms i and j. Mii = 1
meaning that in one time step, one can stay in the same room. You can visualise this matrix by typing
imagesc(M).

1. Write a list of rooms which cannot be reached from room 2 after 10 time steps.

2. The manager complains that takes at least 13 time steps to get from room 1 to room 100. Is this

true?

3. Find the most likely path (sequence of rooms) to get from room 1 to room 100.

4. If a single player were to jump randomly from one room to another (or stay in the same room),
with no preference between rooms, what is the probability at time t (cid:29) 1 the player will be in room
1? Assume that eﬀectively an inﬁnite amount of time has passed and the player began in room 1 at
t = 1.

5. If two players are jumping randomly between rooms (or staying in the same room), explain how to
compute the probability that, after an inﬁnite amount of time, at least one of them will be in room
1? Assume that both players begin in room 1.

Exercise 54. Consider the hidden Markov model:

p(v1, . . . , vT , h1, . . . , hT ) = p(h1)p(v1|h1)

p(vt|ht)p(ht|ht−1)

(5.7.2)

in which dom(ht) = {1, . . . , H} and dom(vt) = {1, . . . , V } for all t = 1, . . . , T .

1. Draw a Belief Network representation of the above distribution.

2. Draw a Factor Graph representation of the above distribution.

82

DRAFT March 9, 2010

T(cid:89)

t=2

Exercises

3. Use the Factor Graph to derive a Sum-Product algorithm to compute marginals p(ht|v1, . . . , vT ).

Explain the sequence order of messages passed on your Factor Graph.

4. Explain how to compute p(ht, ht+1|v1, . . . , vT ).

Exercise 55. For a singly connected Markov Network, p(x) = p(x1, . . . , xn), the computation of a marginal
p(xi) can be carried out eﬃciently. Similarly, the most likely joint state x∗ = arg maxx1,...,xn p(x) can be
computed eﬃciently. Explain when the most likely joint state of a marginal can be computed eﬃciently,
i.e. under what circumstances could one eﬃciently (in O (m) time) compute argmax
p(x1, . . . , xm) for
x1,x2,...,xm
m < n?
Exercise 56. Consider the internet with webpages labelled 1, . . . , N. If webpage j has a link to webpage i,
then we place an element of the matrix Lij = 1, otherwise Lij = 0. By considering a random jump from
webpage j to webpage i to be given by the transition probability

(5.7.3)

Mij = Lij(cid:80)

i Lij

T(cid:89)

t=2

what is the probability that after an inﬁnite amount of random surﬁng, one ends up on webpage i? How
could you relate this to the potential ‘relevance’ of a webpage in terms of a search engine?
Exercise 57. A special time-homogeneous hidden Markov model is given by

p(x1, . . . , xT , y1, . . . , yT , h1, . . . , hT ) = p(x1|h1)p(y1|h1)p(h1)

p(ht|ht−1)p(xt|ht)p(yt|ht)

(5.7.4)

The variable xt has 4 states, dom(xt) = {A, C, G, T} (numerically labelled as states 1,2,3,4). The variable
yt has 4 states, dom(yt) = {A, C, G, T}. The hidden or latent variable ht has 5 states, dom(ht) = {1, . . . , 5}.
The HMM models the following (ﬁctitious) process:

In humans, Z-factor proteins are a sequence on states of the variables x1, x2, . . . , xT .
In bananas Z-
factor proteins are also present, but represented by a diﬀerent sequence y1, y2, . . . , yT . Given a sequence
x1, . . . , xT from a human, the task is to ﬁnd the corresponding sequence y1, . . . , yT in the banana by ﬁrst
ﬁnding the most likely joint latent sequence, and then the most likely banana sequence given this optimal
latent sequence. That is, we require
∗
T )

∗
1, . . . , h

(5.7.5)

argmax
y1,...,yT

p(y1, . . . , yT|h

where

∗
1, . . . , h
h

∗
T = argmax
h1,...,hT

p(h1, . . . , hT|x1, . . . , xT )

(5.7.6)

The ﬁle banana.mat contains the emission distributions pxgh (p(x|h)), pygh (p(y|h)) and transition
phtghtm (p(ht|ht−1)). The initial hidden distribution is given in ph1 (p(h1)). The observed x sequence is
given in x.

1. Explain mathematically and in detail how to compute the optimal y-sequence, using the two-stage

procedure as stated above.

2. Write a MATLAB routine that computes and displays the optimal y-sequence, given the observed

x-sequence. Your routine must make use of the Factor Graph formalism.

3. Explain whether or not it is computationally tractable to compute

arg max
y1,...,yT

p(y1, . . . , yT|x1, . . . , xT )

(5.7.7)

4. Bonus question: By considering y1, . . . , yT as parameters, explain how the EM algorithm may be
used to ﬁnd most likely marginal states. Implement this approach with a suitable initialisation for
the optimal parameters y1, . . . , yT .

DRAFT March 9, 2010

83

Exercises

84

DRAFT March 9, 2010

CHAPTER 6

The Junction Tree Algorithm

6.1 Clustering Variables

In chapter(5) we discussed eﬃcient inference for singly-connected graphs, for which variable elimination
and message passing schemes are appropriate. In the multiply-connected case, however, one cannot in
general perform inference by passing messages only along existing links in the graph. The idea behind
the junction tree algorithm is to form a new representation of the graph in which variables are clustered
together, resulting in a singly-connected graph in the cluster variables (albeit on a diﬀerent graph). The
main focus of the development will be on marginal inference, though similar techniques apply to diﬀerence
inferences, such as ﬁnding the maximal state of the distribution.

At this stage it is important to point out that the junction tree algorithm is not a magic method to
deal with intractabilities resulting from multiply connected graphs; it is simply a way to perform correct
inference on a multiply connected graph by transforming to a singly connected structure. Carrying out
the inference on the resulting junction tree may still be computationally intractable. For example, the
junction tree representation of a general two-dimensional Ising model is a single supernode containing all
the variables. Inference in this case is exponentially complex in the number of variables. Nevertheless,
even in cases where implementing the JTA (or any other exact inference algorithm) may be intractable, the
JTA provides useful insight into the representation of distributions that can form the basis for approximate
inference. In this sense the JTA is key to understanding issues related to representations and complexity
of inference and is central to the development of eﬃcient inference algorithms.

6.1.1 Reparameterisation

Consider the chain

p(a, b, c, d) = p(a|b)p(b|c)p(c|d)p(d)

Using Bayes’ rule, we can reexpress this as

p(a, b, c, d) = p(a, b)
p(b)

p(b, c)
p(c)

p(c, d)

p(d) p(d) = p(a, b)p(b, c)p(c, d)

p(b)p(c)

(6.1.1)

(6.1.2)

A useful insight is that the distribution can therefore be written as a product of marginal distribu-
tions, divided by a product of the intersection of the marginal distributions: Looking at the numerator
p(a, b)p(b, c)p(c, d) this cannot be a distribution over a, b, c, d since we are overcounting b and c, where this
overcounting of b arises from the overlap between the sets a, b and b, c, which have b as their intersection.
Similarly, the overcounting of c arises from the overlap between the sets b, c and c, d. Roughly speaking
we need to correct for this overcounting by dividing by the distribution on the intersections. Given the
transformed representation, a marginal such as p(a, b) can be read oﬀ directly from the factors in the new

85

Clique Graphs

a

b

c

(a)

d

expression.

abc

bc

(b)

bcd

6.1:

Figure
φ(a, b, c)φ(b, c, d).
graph of (a).

(a) Markov

network
(b) Equivalent clique

The aim of the junction tree algorithm is to form a representation of the distribution which contains
the marginals explicitly. We want to do this in a way that works for Belief and Markov networks, and
also deals with the multiply-connected case. In order to do so, an appropriate way to parameterise the
distribution is in terms of a clique graph, as described in the next section.

6.2 Clique Graphs

Deﬁnition 39 (Clique Graph). A clique graph consists of a set of potentials, φ1(X 1), . . . , φn(X n) each
deﬁned on a set of variables X i. For neighbouring cliques on the graph, deﬁned on sets of variables X i
and X j, the intersection X s = X i ∩ X j is called the separator and has a corresponding potential φs(X s).
A clique graph represents the function

(6.2.1)

(cid:81)
(cid:81)
c φc(X c)
s φs(X s)

For notational simplicity we will usually drop the clique potential index c. Graphically clique potentials
are represented by circles/ovals, and separator potentials by squares.

X 1

X 1 ∩ X 2

X 2

The graph on the left represents φ(X 1)φ(X 2)/φ(X 1 ∩ X 2).

Clique graphs translate Markov networks into structures convenient for carrying out inference. Consider
the Markov network in ﬁg(6.1a)

p(a, b, c, d) = φ(a, b, c)φ(b, c, d)

Z

(6.2.2)

which contains two clique potentials sharing the variables b, c. An equivalent representation is given by
the clique graph in ﬁg(6.1b), deﬁned as the product of the numerator clique potentials, divided by the
product of the separator potentials. In this case the separator potential may be set to the normalisation
constant Z. By summing we have

Zp(a, b, c) = φ(a, b, c)(cid:88)
(cid:32)
φ(a, b, c)(cid:88)

φ(b, c, d),

Z2p(a, b, c)p(b, c, d) =

Multiplying the two expressions, we have

d

Zp(b, c, d) = φ(b, c, d)(cid:88)
(cid:33)(cid:32)
φ(b, c, d)(cid:88)

a

φ(a, b, c)

φ(b, c, d)

d

a

In other words

p(a, b, c, d) = p(a, b, c)p(b, c, d)

p(c, b)

86

φ(a, b, c)

(6.2.3)

(cid:33)

= Z2p(a, b, c, d)(cid:88)

a,d

p(a, b, c, d)

(6.2.4)

(6.2.5)

DRAFT March 9, 2010

Clique Graphs

The important observation is that the distribution can be written in terms of its marginals on the variables
in the original cliques and that, as a clique graph, it has the same structure as before. All that has changed
is that the original clique potentials have been replaced by the marginals of the distribution and the
separator by the marginal deﬁned on the separator variables φ(a, b, c) → p(a, b, c), φ(b, c, d) → p(b, c, d),
Z → p(c, b). The usefulness of this representation is that if we are interested in the marginal p(a, b, c),
this can be read oﬀ from the transformed clique potential. To make use of this representation, we require
a systematic way of transforming the clique graph potentials so that at the end of the transformation the
new potentials contain the marginals of the distribution.

Remark 7. Note that, whilst visually similar, a Factor Graph and a Clique Graph are diﬀerent
representations. In a Clique Graph the nodes contain sets of variables, which may share variables with
other nodes.

6.2.1 Absorption
Consider neighbouring cliques V and W, sharing the variables S in common. In this case, the distribution
on the variables X = V ∪ W is

p(X ) = ψ (V)ψ (W)

ψ (S)

and our aim is to ﬁnd a new representation

(6.2.6)

(6.2.7)

ψ (V)

ψ (S)

ψ (W)

ψ∗ (V)

ψ∗ (S)

ψ∗ (W)

in which the potentials are given by

∗ (W) = p(W), ψ

∗ (S) = p(S)

(6.2.8)

In this example, we can explicitly work out the new potentials as function of the old potentials by computing
the marginals as follows:

(cid:80)V\S ψ (V)
(cid:80)W\S ψ (W)

ψ (S)

ψ (S)

ψ (V)ψ (W)

ψ (S)

= ψ (W)

ψ (V)ψ (W)

ψ (S)

= ψ (V)

p(X ) = ψ∗ (V)ψ∗ (W)

ψ∗ (S)
∗ (V) = p(V), ψ

ψ

p(W) =(cid:88)
p(V) = (cid:88)

V\S

W\S

p(X ) =(cid:88)
p(X ) = (cid:88)

V\S

W\S

∗ (S) =(cid:88)

V\S

ψ

ψ (V)

and

and the reﬁne the W potential using

ψ

∗ (W) = ψ (W) ψ∗ (S)
ψ (S)

(6.2.9)

(6.2.10)

(6.2.11)

(6.2.12)

There is a symmetry present in the two equations above – they are the same under interchanging V and
W. One way to describe these equations is through ‘absorption’. We say that the cluster W ‘absorbs’
information from cluster V by the following updating procedure. First we deﬁne a new separator

The advantage of this interpretation is that the new representation is still a valid clique graph represen-
tation of the distribution since

ψ (V)ψ∗ (W)

ψ∗ (S)

=

ψ (V)ψ (W) ψ∗(S)

ψ(S)

ψ∗ (S)

DRAFT March 9, 2010

= ψ (V)ψ (W)

ψ (S)

= p(X )

(6.2.13)

87

A

3

4
→ ←

C

6

←

1

→ 2 →

←

1

0

E

D

8

9
→←

Junction Trees

Figure 6.2: An example absorption schedule on a
clique tree. Many valid schedules exist under the con-
straint that messages can only be passed to a neigh-
bour when all other messages have been received.

B

7 →

←

5

F

After W absorbs information from V then ψ∗ (W) contains the marginal p(W). Similarly, after V absorbs
information from W then ψ∗ (V) contains the marginal p(V). After the separator S has participated in
absorption along both directions, then the separator potential will contain p(S) (this is not the case after
only a single absorption). To see this, consider

ψ

∗∗ (S) = (cid:88)

∗ (W) = (cid:88)

= (cid:88)
Continuing, we have the new potential ψ∗ (V) given by

ψ (W)ψ∗ (S)
ψ (V)(cid:80)W\S ψ (W)ψ∗ (S)/ψ (s)

{W∪V}\S

ψ (S)

W\S

ψ

W\S

ψ

∗ (V) = ψ (V)ψ∗∗ (S)
ψ∗ (S)

=

ψ∗ (S)

ψ (W)ψ (V)

ψ (S)

= p(S)

(6.2.14)

(cid:80)W\S ψ (V)ψ (W)

=

ψ (S)

= p(V)

(6.2.15)

Deﬁnition 40 (Absorption).

ψ (V)

ψ∗ (S)

ψ∗ (W)

Let V and W be neighbours in a clique graph, let S be their
separator, and let ψ (V), ψ (W) and ψ (S) be their potentials.
Absorption from V to W through S replaces the tables ψ∗ (S)
and ψ∗ (W) with

ψ (V)

ψ

∗ (W) = ψ (W) ψ∗ (S)
ψ (S)

(6.2.16)

∗ (S) =(cid:88)

V\S

ψ

We say that clique W absorbs information from clique V.

6.2.2 Absorption schedule on clique trees

Having deﬁned the local message propagation approach, we need to deﬁne an update ordering for absorp-
tion. In general, a node V can send exactly one message to a neighbour W, and it may only be sent when
V has received a message from each of its other neighbours. We continue this sequence of absorptions until
a message has been passed in both directions along every link. See, for example, ﬁg(6.2). Note that the
message passing scheme is not unique.

Deﬁnition 41 (Absorption Schedule). A clique can send a message to a neighbour, provided it has already
received messages from all other neighbours.

6.3 Junction Trees

There are a few stages we need to go through in order to transform a distribution into an appropriate
structure for inference. Initially we explain how to do this for singly-connected structures before moving

88

DRAFT March 9, 2010

Junction Trees

x1

x4

x1, x4

x1, x4

x4

x4

x4

x4

x3

x2

x3, x4

(a)

x4

(b)

x2, x4

x3, x4

x2, x4

(c)

Figure 6.3: (a): Singly-connected Markov network. (b): Clique graph. (c): Clique tree.

onto the multiply-connected case.

Consider the singly-connected Markov network, ﬁg(6.3a)

p(x1, x2, x3, x4) = φ(x1, x4)φ(x2, x4)φ(x3, x4)

(6.3.1)

The clique graph of this singly-connected Markov network is multiply-connected, ﬁg(6.3b), where the
separator potentials are all set to unity. Nevertheless, let’s try to reexpress the Markov network in terms
of marginals. First we have the relations

p(x1, x4) = (cid:88)
p(x2, x4) = (cid:88)
p(x3, x4) = (cid:88)

x1,x3

x2,x3

p(x1, x2, x3, x4) = φ(x1, x4)(cid:88)
p(x1, x2, x3, x4) = φ(x2, x4)(cid:88)
p(x1, x2, x3, x4) = φ(x3, x4)(cid:88)

x2

x1

x1,x2

x1

Taking the product of the three marginals, we have

p(x1, x4)p(x2, x4)p(x3, x4) = φ(x1, x4)φ(x2, x4)φ(x3, x4)

φ(x3, x4)

φ(x3, x4)

φ(x2, x4)

x3

x3

φ(x2, x4)(cid:88)
φ(x1, x4)(cid:88)
φ(x1, x4)(cid:88)
(cid:32)(cid:88)
(cid:124)

x1

x2

φ(x1, x4)(cid:88)

x2

φ(x2, x4)(cid:88)
(cid:123)(cid:122)

x3

p(x4)2

(6.3.2)

(6.3.3)

(6.3.4)

(cid:33)2
(cid:125)

φ(x3, x4)

(6.3.5)

(6.3.6)

This means that the Markov network can be expressed in terms of marginals as

p(x1, x2, x3, x4) = p(x1, x4)p(x2, x4)p(x3, x4)

p(x4)p(x4)

Hence a valid clique graph is also given by the representation ﬁg(6.3c). Indeed, if a variable (here x4)
occurs on every separator in a clique graph loop, one can remove that variable from an arbitrarily chosen
separator in the loop. This leaves an empty separator, which we can simply remove. This shows that
in such cases we can transform the clique graph into a clique tree (i.e. a singly-connected clique graph).
Provided that the original Markov network is singly-connected, one can always form a clique tree in this
manner.

6.3.1 The running intersection property

Sticking with the above example, consider the clique tree in ﬁg(6.3)

φ(x3, x4)φ(x1, x4)φ(x2, x4)

φ1(x4)φ2(x4)

DRAFT March 9, 2010

(6.3.7)

89

as a representation of the distribution (6.3.1) where we set φ1(x4) = φ2(x4) = 1 to make this match. Now
perform absorption on this clique tree:
We absorb (x3, x4) (cid:32) (x1, x4). The new separator is

Junction Trees

1(x4) =(cid:88)

∗

φ

φ(x3, x4)

x3

and the new potential is

∗(x1, x4) = φ(x1, x4) φ∗
2(x4) =(cid:88)

∗(x1, x4)

φ

∗

φ

φ

1(x4)
φ1(x4)

= φ(x1, x4)φ

∗
1(x4)

Now (x1, x4) (cid:32) (x2, x4). The new separator is

x1

and the new potential is

∗(x2, x4) = φ(x2, x4) φ∗

2(x4)
φ2(x4)

φ

= φ(x2, x4)φ

∗
2(x4)

Since we’ve ‘hit the buﬀers’ in terms of message passing, the potential φ(x2, x4) cannot be updated further.
Let’s examine more carefully the value of this new potential,
∗(x1, x4)

∗(x2, x4) = φ(x2, x4)φ

(6.3.12)

φ

φ

∗

2(x4) = φ(x2, x4)(cid:88)
φ(x1, x4)(cid:88)

x1

φ(x3, x4) = (cid:88)

= φ(x2, x4)(cid:88)

p(x1, x2, x3, x4) = p(x2, x4)

(6.3.13)

Hence the new potential φ∗(x2, x4) contains the marginal p(x2, x4).

x1

x3

x1,x3

To complete a full round of message passing we need to have passed messages in a valid schedule along
both directions of each separator. To do so, we continue as follows:
We absorb (x2, x4) (cid:32) (x1, x4). The new separator is

(6.3.8)

(6.3.9)

(6.3.10)

(6.3.11)

(6.3.14)

(6.3.15)

(6.3.16)

(6.3.17)

2 (x4) =(cid:88)

∗∗

x2

∗(x2, x4)

φ

∗∗(x1, x4) = φ

2 (x4) =(cid:80)
1 (x4) =(cid:88)

∗∗

φ

x1

∗(x1, x4) φ∗∗
2 (x4)
x2 φ∗(x2, x4) =(cid:80)
φ∗
2(x4)

∗∗(x1, x4) = p(x4)

and

φ

φ

φ

and

Note that φ∗∗
x2 p(x2, x4) = p(x4) so that now, after absorbing through both
directions, the separator contains the marginal p(x4). The reader may show that φ∗∗(x1, x4) = p(x1, x4).
Finally, we absorb (x1, x4) (cid:32) (x3, x4). The new separator is

∗(x3, x4) = φ(x3, x4) φ∗∗
1 (x4)
φ∗
1(x4)

φ

= p(x3, x4)

Hence, after a full round of message passing, the new potentials all contain the correct marginals.

90

DRAFT March 9, 2010

Junction Trees

(cid:88)

V\I

ψ (V) = (cid:88)

W\I

The new representation is consistent in the sense that for any (not necessarily neighbouring) cliques V
and W with intersection I, and corresponding potentials ψ (V) and ψ (W),

ψ (W)

(6.3.18)

Note that bidirectional absorption guarantees consistency for neighbouring cliques, as in the example
above, provided that we started with a clique tree which is a correct representation of the distribution.

In general, the only possible source of non-consistency is if a variable occurs in two non-neighbouring
cliques and is not present in all cliques on any path connection them. An extreme example would be if
we removed the link between cliques (x3, x4) and (x1, x4). In this case this is still a Clique Tree; however
global consistency could not be guaranteed since the information required to make clique (x3, x4) consis-
tent with the rest of the graph cannot reach this clique.

Formally, the requirement for the propagation of local to global consistency is that the clique tree is a
junction tree, as deﬁned below.

Deﬁnition 42 (Junction Tree). A Clique Tree is a Junction Tree if, for each pair of nodes, V and W,
all nodes on the path between V and W contain the intersection V ∩ W. This is also called the running
intersection property.

From this deﬁnition local consistency will be passed on to any neighbours and the distribution will be
globally consistent. Proofs for these results are contained in [148].

Example 22 (A consistent Junction Tree). To gain some intuition about the meaning of consistency,
consider the junction tree in ﬁg(6.4d). After a full round of message passing on this tree, each link is
consistent, and the product of the potentials divided by the product of the separator potentials is just the
original distribution itself. Imagine that we are interested in calculating the marginal for the node abc.
That requires summing over all the other variables, def gh. If we consider summing over h then, because
the link is consistent,
∗ (e, h) = ψ

(cid:88)

(6.3.19)

∗ (e)

ψ

h

so that the ratio (cid:80)

ψ∗(e,h)
ψ∗(e)

(cid:88)

∗ (d, c, e) = ψ

∗ (c)

ψ

h

is unity, and the eﬀect of summing over node h is that the link between eh
and dce can be removed, along with the separator. The same happens for the link between node eg and
dce, and also for cf to abc. The only nodes remaining are now dce and abc and their separator c, which
have so far been unaﬀected by the summations. We still need to sum out over d and e. Again, because
the link is consistent,

de

so that the ratio (cid:80)
summation over the other variables in that potential, for example p(f) =(cid:80)

ψ∗(d,c,e)
ψ∗(c) = 1. The result of the summation of all variables not in abc therefore
produces unity for the cliques and their separators, and the summed potential representation reduces
simply to the potential ψ∗ (a, b, c) which is the marginal p(a, b, c).
It is clear that a similar eﬀect will
happen for other nodes. We can then obtain the marginals for individual variables by simple brute force

de

c ψ∗ (c, f).

(6.3.20)

DRAFT March 9, 2010

91

Constructing a Junction Tree for Singly-Connected Distributions

6.4 Constructing a Junction Tree for Singly-Connected Distributions

6.4.1 Moralisation

For Belief Networks, an initial step is required, which is not required in the case of undirected graphs.

Deﬁnition 43 (Moralisation). For each variable x add an undirected link between all parents of x and
replace the directed link from x to its parents by undirected links. This creates a ‘moralised’ Markov
network.

6.4.2 Forming the clique graph

The clique graph is formed by identifying the cliques in the Markov network and adding a link between
cliques that have a non-empty intersection. Add a separator between the intersecting cliques.

6.4.3 Forming a junction tree from a clique graph

For a singly-connected distribution, any maximal weight spanning tree of a clique graph is a junction tree.

Deﬁnition 44 (Junction Tree). A junction tree is obtained by ﬁnding a maximal weight spanning tree
of the clique graph. The weight of the tree is deﬁned as the sum of all the separator weights of the tree,
where the separator weight is the number of variables in the separator.

If the clique graph contains loops, then all separators on the loop contain the same variable. By continuing
to remove loop links until you have a tree is revealed, we obtain a junction tree.

Example 23 (Forming a Junction Tree). Consider the Belief Network in ﬁg(6.4a). The moralisation
procedure gives ﬁg(6.4b). Identifying the cliques in this graph, and linking them together gives the clique
graph in ﬁg(6.4c). There are several possible junction trees one could obtain from this clique graph, and
one is given in ﬁg(6.4d).

6.4.4 Assigning potentials to cliques

of a set of potentials ψ(cid:0)

Deﬁnition 45 (Clique Potential Assignment). Given a junction tree and a function deﬁned as the product

X 1(cid:1), . . . , ψ (X n), a valid clique potential assignment places potentials in JT cliques

whose variables can contain them such that the product of the JT clique potentials, divided by the JT
separator potentials, is equal to the function.

A simple way to achieve this assignment is to list all the potentials and order the JT cliques arbitrarily.
Then, for each potential, search through the JT cliques until the ﬁrst is encountered for which the potential
variables are a subset of the JT clique variables. Subsequently the potential on each JT clique is taken as
the product of all clique potentials assigned to the JT clique. Lastly, we assign all JT separators to unity.
This approach is taken in jtassignpot.m.

92

DRAFT March 9, 2010

Junction Trees for Multiply-Connected Distributions

c

h

c

a

e

(a)

abc

c

eh

d

g

c

e

(c)

b

f

d

g

a

e

c

h

(b)

cf

dce

e

eg

b

f

c

e

(d)

abc

c

cf

eh

dce

e

eg

e

Figure 6.4: (a): Belief Network.
(d): A
junction tree. This satisﬁes the running intersection property that for any two nodes which contain a
variable in common, any clique on the path linking the two nodes also contains that variable.

(b): Moralised version of (a).

(c): Clique Graph of (b).

Example 24. For the belief network of ﬁg(6.4a), we wish to assign its potentials to the junction tree
ﬁg(6.4d). In this case the assignment is unique and is given by

ψ (abc) = p(a)p(b)p(c|a, b)
ψ (dce) = p(d)p(e|d, c)
ψ (cf) = p(f|c)
ψ (eg) = p(g|e)
ψ (eh) = p(h|e)

(6.4.1)

All separator potentials are initialised to unity. Note that in some instances it can be that a junction tree
clique is assigned to unity.

6.5 Junction Trees for Multiply-Connected Distributions

When the distribution contains loops, the construction outlined in section(6.4) does not result in a junction
tree. The reason is that, due to the loops, variable elimination changes the structure of the remaining
graph. To see this, consider the following distribution,

p(a, b, c, d) = φ(a, b)φ(b, c)φ(c, d)φ(d, a)

(6.5.1)

as shown in ﬁg(6.5a). Let’s ﬁrst try to make a clique graph. We have a choice about which variable ﬁrst
to marginalise over. Let’s choose d:

p(a, b, c) = φ(a, b)φ(b, c)(cid:88)

DRAFT March 9, 2010

d

φ(c, d)φ(d, a)

(6.5.2)

93

Junction Trees for Multiply-Connected Distributions

a

d

a

b

c

b

c

a

d

b

c

a

d

b

c

(a)

(b)

(c)

(d)

abc

ac

(e)

acd

Figure 6.5: (a): An undirected graph with a loop.
c in the subgraph.
representation. (e): Junction tree for (a).

(c): The induced representation for the graph in (a).

(b): Eliminating node d adds a link between a and
(d): Equivalent induced

The remaining subgraph therefore has an extra connection between a and c, see ﬁg(6.5b). We can express
the joint in terms of the marginals using

To continue the transformation into marginal form, let’s try to replace the numerator terms with proba-
bilities. We can do this by considering

(6.5.3)

(6.5.4)

(6.5.5)

(6.5.6)

p(a, b, c, d) =

(cid:80)
p(a, b, c)
d φ(c, d)φ(d, a) φ(c, d)φ(d, a)

p(a, c, d) = φ(c, d)φ(d, a)(cid:88)

φ(a, b)φ(b, c)

b

Plugging this into the above equation, we have

p(a, b, c, d) =

d φ(c, d)φ(d, a)(cid:80)
(cid:80)

p(a, b, c)p(a, c, d)

b φ(a, b)φ(b, c)

We recognise that the denominator is simply p(a, c), hence

p(a, b, c, d) = p(a, b, c)p(a, c, d)

p(a, c)

.

This means that a valid clique graph for the distribution ﬁg(6.5a) must contain cliques larger than those
in the original distribution. To form a JT based on products of cliques divided by products of separators,
we could start from the induced representation ﬁg(6.5c). Alternatively, we could have marginalised over
variables a and c, and ended up with the equivalent representation ﬁg(6.5d).

Generally, the result from variable elimination and re-representation in terms of the induced graph is that
a link is added between any two variables on a loop (of length 4 or more) which does not have a chord. This
is called triangulation. A Markov network on a triangulated graph can always be written in terms of the
product of marginals divided by the product of separators. Armed with this new induced representation,
we can form a junction tree.

a

f

b

e

(a)

c

a

d

f

b

e

(b)

c

d

(a):

Figure 6.6:
Markov network.
resentation.

Loopy ‘ladder’
(b): Induced rep-

94

DRAFT March 9, 2010

Junction Trees for Multiply-Connected Distributions

a

a

f

f

b

j

b

j

c

d

e

a

g

h

i

e

a

k

d

l

(a)

c

g

h

i

k

l

(d)

f

f

b

j

b

j

c

d

e

a

g

h

i

e

a
a

k

d

l

(b)

c

g

h

i

k

l

(e)

f

f
f

b

j

b
b

j
j

c

d

e

g

h

i

k

d
d

e
e

l

(c)

c
c

g
g

h
h

i
i

k
k

l
l

(f)

Figure 6.7: (a): Markov network for which we seek a triangulation via greedy variable elimination. We
(b): We then eliminate variables b, d since these only add
ﬁrst eliminate the simplical nodes a, e, l.
(d):
(c): f and i are now simplical and are eliminated.
a single extra link to the induced graph.
(e): The remaining variables {c, j, k}
We eliminate g and h since this adds only single extra links.
(f): Final triangulation. The variable elimination (partial) order is
may be eliminated in any order.
{a, e, l} ,{b, d} ,{f, i} ,{g, h} ,{c, j, k} where the brackets indicate that the order in which the variables
inside the bracket are eliminated is irrelevant. Compared with the triangulation produced by the max-
cardinality checking approach in ﬁg(6.9d), this triangulation is more parsimonious.

Example 25. A slightly more complex loopy distribution is depicted in ﬁg(6.6a),

p(a, b, c, d, e, f) = φ(a, b)φ(b, c)φ(c, d)φ(d, e)φ(e, f)φ(a, f)φ(b, e)

(6.5.7)

There are diﬀerent induced representations depending on which variables we decide to eliminate. The
reader may convince herself that one such induced representation is given by ﬁg(6.6b).

Deﬁnition 46 (Chord). This is a link joining two non-consecutive vertices of a loop.

Deﬁnition 47 (Triangulated (Decomposable) Graph). An undirected graph is triangulated if every loop
of length 4 or more has a chord. An equivalent term is that the graph is decomposable or chordal. An
undirected graph is triangulated if and only if its clique graph has a junction tree.

6.5.1 Triangulation algorithms

When a variable is eliminated from a graph, links are added between all the neighbours of the eliminated
variable. A triangulation algorithm is one that produces a graph for which there exists a variable elimi-
nation order that introduces no extra links in the graph.

DRAFT March 9, 2010

95

Junction Trees for Multiply-Connected Distributions

Figure 6.8:
Junction tree formed from the triangulation
ﬁg(6.7)f. One verify that this satisﬁes the running intersection
property.

abf

bf

bcf g

cdhi

di

dei

cf g

cf gj

chi

chik

cj

ck

cjk

jk

jkl

For discrete variables the complexity of inference scales exponentially with clique sizes in the triangulated
graph since absorption requires computing tables on the cliques. It is therefore of some interest to ﬁnd
a triangulated graph with small clique sizes. However, ﬁnding the triangulated graph with the smallest
maximal clique is an NP-hard problem for a general graph, and heuristics are unavoidable. Below we
describe two simple algorithms that are generically reasonable, although there may be cases where an
alternative algorithm may be considerably more eﬃcient[53, 28, 191].

Remark 8 (Triangulation does not mean putting ‘triangles’ on the original graph). Note that a
triangulated graph is not one in which ‘squares in the original graph have triangles within them in
the triangulated graph’. Whilst this is the case for ﬁg(6.6b), this is not true for ﬁg(6.9d). The term
triangulation refers to the fact that every ‘square’ (i.e. loop of length 4) must have a ‘triangle’, with
edges added until this criterion is satisﬁed.

Greedy variable elimination

An intuitive way to think of triangulation is to ﬁrst start with simplical nodes, namely those which, when
eliminated do not introduce any extra links in the remaining graph. Next consider a non-simplical node of
the remaining graph that has the minimal number of neighbours. Then add a link between all neighbours
of this node and ﬁnally eliminate this node from the graph. Continue until all nodes have been eliminated.
(This procedure corresponds to Rose-Tarjan Elimination[233] with a particular node elimination choice).
By labelling the nodes eliminated in sequence, we obtain a perfect ordering (see below) in reverse. In
the case that (discrete) variables have diﬀerent numbers of states, a more reﬁned version is to choose the
non-simplical node i which, when eliminated, leaves the smallest clique table size (the product of the size
of all the state dimensions of the neighbours of node i). See ﬁg(6.7) for an example.

Deﬁnition 48 (Variable Elimination). In Variable Elimination, one simply picks any non-deleted node
x in the graph, and then adds links to all the neighbours of x. Node x is then deleted. One repeats this
until all nodes have been deleted[233].

Whilst this procedure guarantees a triangulated graph, its eﬃciency depends heavily on the sequence of
nodes chosen to be eliminated. Several heuristics for this have been proposed, including the one below,
which corresponds to choosing x to be the node with the minimal number of neighbours.

Maximum cardinality checking

Algorithm(2) terminates with success if the graph is triangulated. Not only is this a suﬃcient condition
for a graph to be triangulated, but is also necessary [271]. It processes each node and the time to process
a node is quadratic in the number of adjacent nodes. This triangulation checking algorithm also suggests

96

DRAFT March 9, 2010

The Junction Tree Algorithm

1

3

2

6

5

7

10

1

4

8

9

3

11

2

6

5

7

10

1

4

8

9

3

11

2

6

5

7

10

4

8

9

11

(a)

(b)

(c)

(d)

Figure 6.9: Starting with the Markov network in (a), the maximum cardinality check algorithm proceeds
until (b). where an additional link is required (c). One continues the algorithm until the fully triangulated
graph (d) is found.

a triangulation construction algorithm – we simply add a link between the two neighbours that caused the
algorithm to FAIL, and then restart the algorithm. The algorithm is restarted from the beginning, not
just continued from the current node. This is important since the new link may change the connectivity
between previously labelled nodes. See ﬁg(6.9) for an example1.

Deﬁnition 49 (Perfect Elimination Order). Let the n variables in a Markov network be ordered from 1
to n. The ordering is perfect if, for each node i, the neighbours of i that are later in the ordering, and
i itself, form a (maximal) clique. This means that when we eliminate the variables in sequence from 1
to n, no additional links are induced in the remaining marginal graph. A graph which admits a perfect
elimination order is decomposable, and vice versa.

Algorithm 2 A check if a graph is decomposable (triangulated). The graph is triangulated if, after cycling
through all the n nodes in the graph, the FAIL criterion is not encountered.

1: Choose any node in the graph and label it 1.
2: for i = 2 to n do
3:
4:
5: end for
Where there is more than one node with the most labeled neighbours, the tie may be broken arbitrarily.

Choose the node with the most labeled neighbours and label it i.
If any two labeled neighbours of i are not adjacent to each other, FAIL.

6.6 The Junction Tree Algorithm

We now have all the steps required for inference in multiply-connected graphs:

Moralisation Marry the parents. This is required only for directed distributions.

Triangulation Ensure that every loop of length 4 or more has a chord.

Junction Tree Form a junction tree from cliques of the triangulated graph, removing any unnecessary
links in a loop on the cluster graph. Algorithmically, this can be achieved by ﬁnding a tree with
maximal spanning weight with weight wij given by the number of variables in the separator between
cliques i and j. Alternatively, given a clique elimination order (with the lowest cliques eliminated
ﬁrst), one may connect each clique i to the single neighbouring clique j > i with greatest edge weight
wij.

1This example is due to David Page www.cs.wisc.edu/∼dpage/cs731

DRAFT March 9, 2010

97

The Junction Tree Algorithm

a

c

f

b

e

d

g

b

e

a

c

f

d

g

h

i

h

i

(a)

(b)

(a): Original

Figure 6.10:
loopy Belief Network.
(b): The moralisation links (dashed) are between
nodes e and f and between nodes f and g. The other
additional links come from triangulation. The clique
size of the resulting clique tree (not shown) is four.
From [55].

Potential Assignment Assign potentials to junction tree cliques and set the separator potentials to

unity.

Message Propagation Carry out absorption until updates have been passed along both directions of

every link on the JT.

The clique marginals can then be read oﬀ from the JT. An example is given in ﬁg(6.10).

6.6.1 Remarks on the JTA

• The algorithm provides an upper bound on the computation required to calculate marginals in the
graph. There may exist more eﬃcient algorithms in particular cases, although generally it is believed
that there cannot be much more eﬃcient approaches than the JTA since every other approach must
perform a triangulation[147, 173]. One particular special case is that of marginal inference for a
binary variable MRF on a two-dimensional lattice containing only pure quadratic interactions. In

this case the complexity of computing a marginal inference is O(cid:0)n3(cid:1) where n is the number of

variables in the distribution. This is in contrast to the pessimistic exponential complexity suggested
by the JTA. Such cases are highly specialised and it is unlikely that a general purpose algorithm
that could consistently outperform the JTA exists.

• One might think that the only class of distributions for which essentially a linear time algorithm is
available are singly-connected distributions. However, there are decomposable graphs for which the
cliques have limited size meaning that inference is tractable. For example an extended version of the
‘ladder’ in ﬁg(6.6a) has a simple induced decomposable representation ﬁg(6.6b), for which marginal
inference would be linear in the number of rungs in the ladder. Eﬀectively these structures are hyper
trees in which the complexity is then related to the tree width of the graph[81].

• Ideally, we would like to ﬁnd a triangulated graph which has minimal clique size. However, it can
be shown to be a hard-computation problem (N P -hard) to ﬁnd the most eﬃcient triangulation.
In practice, most general purpose triangulation algorithms are somewhat heuristic and chosen to
provide reasonable, but clearly not optimal, generic performance.

• Numerical over/under ﬂow issues can occur in large cliques, where many probability values are mul-
tiplied together. Similarly in long chains since absorption will tend to reduce the numerical size of
potential entries in a clique.
If we only care about marginals we can avoid numerical diﬃculties
by normalising potentials at each step; these missing normalisation constants can always be found
under the normalisation constraint. If required one can always store the values of these local renor-
malisations, should, for example, the global normalisation constant of a distribution be required, see
section(6.6.2).

• After clamping variables in evidential states, running the JTA returns the joint distribution on the
non-evidential variables in a clique with all the evidential variables clamped in their evidential states.
From this conditionals are straightforward to calculate.

• Imagine that we have run the JT algorithm and want to afterwards ﬁnd the marginal p(X|evidence).
We could do so by clamping the evidential variables. However, if both X and the set of evidential
DRAFT March 9, 2010

98

The Junction Tree Algorithm

variables are all contained within a single clique of the JT, then we may use the consistent JT cliques
to compute p(X|evidence). The reason is that since the JT clique contains the marginal on the set
of variables which includes X and the evidential variables, we can obtain the required marginal by
considering the single JT clique alone.

• Representing the marginal distribution of a set of variables X which are not contained within a single
clique is in general computationally diﬃcult. Whilst the probability of any state of p(X ) may be
computed eﬃciently, there are in general an exponential number of such states. A classical example
in this regard is the HMM, section(23.2) with singly-connected joint distribution p(V,H). However
the marginal distribution p(H) is fully connected. This means that for example whilst the entropy
of p(V,H) is straightforward to compute, the entropy of the marginal p(H) is intractable.

6.6.2 Computing the normalisation constant of a distribution

For a Markov network

(cid:89)

p(X ) =

1
Z

how can we ﬁnd Z eﬃciently? If we used the JTA on the unnormalised distribution(cid:81)

i

φ(Xi)

have the equivalent representation:

(6.6.1)

i φ(Xi), we would

(6.6.2)

(6.6.3)

1
Z

(cid:81)
(cid:81)
C φ(XC)
S φ(XS)
(cid:81)
(cid:81)
C φ(XC)
S φ(XS)

p(X ) =

Z =(cid:88)

x

Z =(cid:88)

XC

Since the distribution must normalise, we can obtain Z from

For a consistent JT, summing ﬁrst over the variables of a simplical JT clique (not including the separator
variables), the marginal clique will cancel with the corresponding separator to give a unity term so that the
clique and separator can be removed. This forms a new JT for which we then eliminate another simplical
clique. Continuing in this manner we will be left with a single numerator potential so that

φ(XC)

(6.6.4)

This is true for any clique C, so it makes sense to choose one with a small number of states so that
the resulting raw summation is eﬃcient. Hence in order to compute the normalisation constant of a
distribution one runs the JT algorithm on an unnormalised distribution and the global normalisation is
then given by the local normalisation of any clique. Note that if the graph is disconnected (there are
isolated cliques), the normalisation is the product of the connected component normalisation constants. A
computationally convenient way to ﬁnd this is to compute the product of all clique normalisations divided
by the product of all separator normalisations.

6.6.3 The marginal likelihood
Our interest here is the computation of p(V) where V is a subset of the full variable set. Naively, one could
carry out this computation by summing over all the non-evidential variables (hidden variables H = X\V)
explicitly. In cases where this is computationally impractical an alternative is to use

p(H|V) = p(V,H)
p(V)

(6.6.5)

One can view this as a product of clique potentials divided by the normalisation p(V), for which the general
method of section(6.6.2) may be directly applied. See demoJTree.m.

DRAFT March 9, 2010

99

The Junction Tree Algorithm

Example 26 (A simple example of the JTA).
Consider running the JTA on the simple graph

p(a, b, c) = p(a|b)p(b|c)p(c)

(6.6.6)

The moralisation and triangulation steps are trivial, and the JTA is given
immediately by the ﬁgure on the right. A valid assignment is

a

ab

b

b

c

bc

To ﬁnd a marginal p(b) we ﬁrst run the JTA:

ψ (a, b) = p(a|b), ψ (b) = 1, ψ (b, c) = p(b|c)p(c)

• Absorbing from ab through b, the new separator is ψ∗ (b) =(cid:80)

(6.6.7)

a ψ (a, b) =(cid:80)

a p(a|b) = 1.

• The new potential on (b, c) is given by

= p(b|c)p(c) × 1

1

ψ (b)

∗ (b, c) = ψ (b, c)ψ∗ (b)
∗ (b, c) =(cid:88)
∗∗ (b) =(cid:88)

ψ

ψ

ψ

c

c

• Absorbing from bc through b, the new separator is

p(b|c)p(c)

= p(a|b)(cid:80)

• The new potential on (a, b) is given by

∗ (a, b) = ψ (a, b)ψ∗∗ (b)

c p(b|c)p(c)
1

ψ

ψ∗ (b)

This is therefore indeed equal to the marginal since(cid:80)
∗∗ (b) =(cid:88)

The new separator ψ∗∗ (b) contains the marginal p(b) since

p(b|c)p(c) =(cid:88)

p(b, c) = p(b)

ψ

c

c

c p(a, b, c) = p(a, b).

(6.6.8)

(6.6.9)

(6.6.10)

(6.6.11)

(6.6.12)

(6.6.13)

(6.6.14)

Example 27 (Finding a conditional marginal). Continuing with the distribution in example(26), we
consider how to compute p(b|a = 1, c = 1). First we clamp the evidential variables in their states. Then
we claim that the eﬀect of running the JTA is to produce on a set of clique variables X the marginals on
the cliques p(X ,V). We demonstrate this below:

a p(a|b) = 1. However, since
a is clamped in state a = 1, then the summation is not carried out over a, and we have instead

• In general, the new separator is given by ψ∗ (b) = (cid:80)
ψ∗ (b) = p(a = 1|b).
• The new potential on the (b, c) clique is given by

a ψ (a, b) = (cid:80)

ψ (b)

∗ (b, c) = ψ (b, c)ψ∗ (b)
∗ (b, c) =(cid:88)
∗∗ (b) =(cid:88)

ψ

ψ

ψ

c

c

• The new separator is normally given by

p(b|c)p(c)

= p(b|c = 1)p(c = 1)p(a = 1|b)

1

However, since c is clamped in state 1, we have instead

ψ

∗∗ (b) = p(b|c = 1)p(c = 1)p(a = 1|b)

100

DRAFT March 9, 2010

Finding the Most Likely State

• The new potential on (a, b) is given by

∗ (a, b) = ψ (a, b)ψ∗∗ (b)

ψ

ψ∗ (b)

= p(a = 1|b)p(b|c = 1)p(c = 1)p(a = 1|b)

p(a = 1|b)

= p(a = 1|b)p(b|c = 1)p(c = 1)
(6.6.15)

The eﬀect of clamping a set of variables V in their evidential states and running the JTA is that, for a clique
i which contains the set of non-evidential variables Hi, the consistent potential from the JTA contains the
marginal p(Hi,V). Finding a conditional marginal is then straightforward by ensuring normalisation.

Example 28 (ﬁnding the likelihood p(a = 1, c = 1)). The eﬀect of clamping the variables in their
evidential states and running the JTA produces the joint marginals, such as ψ∗ (a, b) = p(a = 1, b, c = 1).
Then calculating the likelihood is easy since we just sum out over the non-evidential variables of any

converged potential : p(a = 1, c = 1) =(cid:80)

b ψ∗ (a, b) =(cid:80)

b p(a = 1, b, c = 1).

6.7 Finding the Most Likely State

A quantity of interest is the most likely joint state of a distribution:

argmax
x1,...,xn

p(X )

(6.7.1)

and it is natural to wonder how this can be eﬃciently computed in the case of a loopy distribution. Since
the development of the JTA is based around a variable elimination procedure and the max operator dis-
tributes over the distribution as well, eliminating a variable by maximising over that variable will have
the same eﬀect on the graph structure as summation did. This means that a junction tree is again an
appropriate structure on which to perform max operations. Once a JT has been constructed, one then
uses the Max Absorption procedure (see below), to perform maximisation over the variables. After a full
round of absorption has been carried out, the cliques contain the distribution on the variables of the clique
with all remaining variables set to their optimal states. The optimal local states can be found by explicit
optimisation of each clique potential separately.

Note that this procedure holds also for non-distributions – in this sense this is an example of a more general
dynamic programming procedure applied in a case where the underlying graph is multiply-connected.
This demonstrates how to eﬃciently compute the optimum of a multiply-connected function deﬁned as
the product on potentials.

Deﬁnition 50 (Max Absorption).

ψ (V)

ψ∗ (S)

ψ∗ (W)

Let V and W be neighbours in a clique graph, let S be their
separator, and let ψ (V), ψ (W) and ψ (S) be their potentials.
Absorption replaces the tables ψ (S) and ψ (W) with
∗ (W) = ψ (W) ψ∗ (S)
ψ (S)

∗ (S) = maxV\S ψ (V)

ψ

ψ

Once messages have been passed in both directions over all separators, according to a valid schedule,
the most-likely joint state can be read oﬀ from maximising the state of the clique potentials. This is
implemented in absorb.m and absorption.m where a ﬂag is used to switch between either sum or max
absorption.

DRAFT March 9, 2010

101

Reabsorption : Converting a Junction Tree to a Directed Network

abc

c

c

e

dce

e

abc

c

c

e

cf

dce

e

cf

a b c

e d

f

eg

eh

eg

(a)

eh

(b)

g

h

(c)

Figure 6.11: (a): Junction tree.
(b): Directed junction tree in which all edges are consistently oriented
away from the clique (abc). (c): A set chain formed from the junction tree by reabsorbing each separator
into its child clique.

6.8 Reabsorption : Converting a Junction Tree to a Directed Network

It is sometimes useful to be able to convert the JT back to a BN of a desired form. For example, if
one wishes to draw samples from a Markov network, this can be achieved by ancestral sampling on an
equivalent directed structure, see section(27.2.2).

Revisiting the example from ﬁg(6.4), we have the JT given in ﬁg(6.11a). To ﬁnd a valid directed represen-
tation we ﬁrst orient the JT edges consistently away from a chosen root node (see singleparenttree.m),
thereby forming a directed JT which has the property that each clique has at most one parent clique.

Deﬁnition 51 (Reabsorption).

V

S

W

⇒

V

W

Let V and W be neighbouring cliques in a directed JT in which each clique in the tree has at most one
parent. Furthermore, let S be their separator, and ψ (V), ψ (W) and ψ (S) be the potentials. Reabsorption
into W removes the separator and forms a (set) conditional distribution

p(W|V) = ψ (W)
ψ (S)

We say that clique W reabsorbs the separator S.

(6.8.1)

In ﬁg(6.11) where one amongst many possible directed representations is formed from the JT. Speciﬁcally,
ﬁg(6.11a) represents

p(a, b, c, d, e, f, g, h) = p(e, g)p(d, c, e)p(a, b, c)p(c, f)p(e, h)

p(e)p(c)p(c)p(e)

We now have many choices as to which clique re-absorbs a separator. One such choice would give

p(a, b, c, d, e, f, g, h) = p(g|e)p(d, e|c)p(a, b, c, )p(f|c)p(h|e)

(6.8.2)

(6.8.3)

This can be represented using a so-called set chain[170] in ﬁg(6.11c) (set chains generalise Belief Networks
to a product of clusters of variables conditioned on parents). By writing each of the set conditional
probabilities as local conditional BNs, one may also write full BN. For example, one such would be given
from the decomposition

p(c|a, b)p(b|a)p(a)p(g|e)p(f|c)p(h|e)p(d|e, c)p(e|c)

102

(6.8.4)

DRAFT March 9, 2010

Code

d1

d2

d3

d4

d5

s1

s2

s3

Figure 6.12: 5 diseases giving rise to 3 symptoms. Assuming the symptoms
are all instantiated, the triangulated graph of the diseases is a 5 clique.

6.9 The Need For Approximations

The JTA provides an upper bound on the complexity of (marginal) inference and attempts to exploit the
structure of the graph to reduce computations. However, in a great deal of interesting applications the
use of the JTA algorithm would result in clique-sizes in the triangulated graph that are prohibitively large.

A classical situation in which this can arise are disease-symptom networks. For example, for the graph in
ﬁg(6.12), the triangulated graph of the diseases is fully coupled, meaning that no simpliﬁcation can occur
in general. This situation is common in such bipartite networks, even when the children only have a small
number of parents.
Intuitively, as one eliminates each parent, links are added between other parents,
mediated via the common children. Unless the graph is highly regular, analogous to a form of hidden
Markov model, this ﬁll-in eﬀect rapidly results in large cliques and intractable computations.

Dealing with large clique in the triangulated graph is an active research topic and we’ll discuss strategies
to approximate the computations in chapter(28).

6.9.1 Bounded width junction trees

In some applications we may be at liberty to choose the structure of the Markov network. For example,
if we wish to ﬁt a Markov network to data, we may wish to use as complex a Markov network as we
can computationally aﬀord.
In such cases we desire that the clique sizes of the resulting triangulated
Markov network are smaller than a speciﬁed ‘tree width’ (considering the corresponding junction tree as a
hypertree). Constructing such bounded width or ‘thin’ junction trees is an active research topic. A simple
way to do this is to start with a graph and include a randomly chosen edge provided that the size of all
cliques in the resulting triangulated graph is below a speciﬁed maximal width. See demoThinJT.m and
makeThinJT.m which assumes an initial graph G and a graph of candidate edges C, iteratively expanding
G until a maximal tree width limit is reached. See also [11] for a discussion on learning an appropriate
Markov structure based on data.

6.10 Code

absorb.m: Absorption update V → S → W
absorption.m: Full Absorption Schedule over Tree
jtree.m: Form a Junction Tree
triangulate.m: Triangulation based on simple node elimination

6.10.1 Utility routines

Knowing if an undirected graph is a tree, and returning a valid elimination sequence is useful. A connected
graph is a tree if the number of edges plus 1 is equal to the number of nodes. However, for a possibly
disconnected graph this is not the case. The code deals with the possibly disconnected case, returning
a valid elimination sequence if the graph is singly-connected. The routine is based on the observation
that any singly-connected graph must always possess a simplical node which can be eliminated to reveal
a smaller singly-connected graph.
istree.m: If graph is singly connected return 1 and elimination sequence
elimtri.m: Vertex/Node Elimination on a Triangulated Graph, with given end node
demoJTree.m: Junction Tree : Chest Clinic

DRAFT March 9, 2010

103

6.11 Exercises

Exercise 58. Show that the Markov network 1
elimination labelling for this graph.

4

2

Exercise 59. Consider the following distribution:

Exercises

3

is not perfect elimination ordered and give a perfect

p(x1, x2, x3, x4) = φ(x1, x2)φ(x2, x3)φ(x3, x4)

(6.11.1)

1. Draw a clique graph that represents this distribution and indicate the separators on the graph.

2. Write down an alternative formula for the distribution p(x1, x2, x3, x4) in terms of the marginal

probabilities p(x1, x2), p(x2, x3), p(x3, x4), p(x2), p(x3)

Exercise 60. Consider the distribution

p(x1, x2, x3, x4) = φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)

(6.11.2)

1. Write down a junction tree for the above graph.

2. Carry out the absorption procedure and demonstrate that this gives the correct result for the marginal

p(x1).

Exercise 61. Consider the distribution

p(a, b, c, d, e, f, g, h, i) = p(a)p(b|a)p(c|a)p(d|a)p(e|b)p(f|c)p(g|d)p(h|e, f)p(i|f, g)

(6.11.3)

1. Draw the Belief Network for this distribution.

2. Draw the moralised graph.

3. Draw the triangulated graph. Your triangulated graph should contain cliques of the smallest size

possible.

4. Draw a junction tree for the above graph and verify that it satisﬁes the running intersection property.

5. Describe a suitable initialisation of clique potentials.

6. Describe the absorption procedure and write down an appropriate message updating schedule.

Exercise 62. This question concerns the distribution

p(a, b, c, d, e, f) = p(a)p(b|a)p(c|b)p(d|c)p(e|d)p(f|a, e)

1. Draw the Belief Network for this distribution.

2. Draw the moralised graph.

(6.11.4)

3. Draw the triangulated graph. Your triangulated graph should contain cliques of the smallest size

possible.

4. Draw a junction tree for the above graph and verify that it satisﬁes the running intersection property.

5. Describe a suitable initialisation of clique potentials.

6. Describe the Absorption procedure and an appropriate message updating schedule.

7. Show that the distribution can be expressed in the form

p(a|f)p(b|a, c)p(c|a, d)p(d|a, e)p(e|a, f)p(f)

104

(6.11.5)

DRAFT March 9, 2010

Exercises

Exercise 63.

For the undirected graph on the square lattice as shown, draw a triangulated graph
with the smallest clique sizes possible.

Exercise 64.

Consider a binary variable Markov Random Field p(x) = Z−1(cid:81)

i>j φ(xi, xj), deﬁned
I[xi=xj ] for i a neighbour of j on the lattice
on the n × n lattice with φ(xi, xj) = e
and i > j. A naive way to perform inference is to ﬁrst stack all the variables in the
tth column and call this cluster variable Xt, as shown. The resulting graph is then
singly-connected. What is the complexity of computing the normalisation constant
based on this cluster representation? Compute log Z for n = 10.

X1

X2

X3

Exercise 65. Given a consistent junction tree on which a full round of message passing has occurred,
explain how to form a belief network from the junction tree.

Exercise 66. The ﬁle diseaseNet.mat contains the potentials for a disease bi-partite belief network, with
20 diseases d1, . . . , d20 and 40 symptoms, s1, . . . , s40. Each disease and symptom is a binary variable, and
each symptom connects to 3 parent diseases.

1. Using the BRMLtoolbox, construct a junction tree for this distribution and use it to compute all

the marginals of the symptoms, p(si = 1).

2. On most standard computers, computing the marginal p(si = 1) by raw summation of the joint
distribution is computationally infeasible. Explain how to compute the marginals p(si = 1) in a
tractable way without using the junction tree formalism. By implementing this method, compare it
with the results from the junction tree algorithm.

3. Consider the (unlikely) scenario in which all the 40 symptom variables are instantiated. Using
the junction tree, estimate an upper bound on the number of seconds that computing a marginal
p(d1|s1:40) takes, assuming that for a two clique table containing S (joint) states, absorption takes
O (Sδ) seconds, for an unspeciﬁed δ. Compare this estimate with the time required to compute the
marginal by raw summation of the instantiated Belief network.

DRAFT March 9, 2010

105

Exercises

106

DRAFT March 9, 2010

CHAPTER 7

Making Decisions

7.1 Expected Utility

This chapter concerns situations in which decisions need to be taken under uncertainty. Consider the
following scenario : you are asked if you wish to take a bet on the outcome of tossing a fair coin. If you
bet and win, you gain £100. If you bet and lose, you lose £200. If you don’t bet, the cost to you is zero.
We can set this up using a two state variable x, with dom(x) = {win, lose}, a decision variable d with
dom(d) = {bet, no bet} and utilities as follows:

U(win, bet) = 100, U(lose, bet) = −200, U(win, no bet) = 0, U(lose, no bet) = 0

(7.1.1)

Since we don’t know the state of x, in order to make a decision about whether or not to bet, arguably
the best we can do is work out our expected winnings/losses under the situations of betting and not
betting[240]. If we bet, we would expect to gain

U(bet) = p(win) × U(win, bet) + p(lose) × U(lose, bet) = 0.5 × 100 − 0.5 × 200 = −50

If we don’t bet, the expected gain is zero, U(no bet) = 0. Based on taking the decision which maximises
expected utility, we would therefore be advised not to bet.

Deﬁnition 52 (Subjective Expected Utility). The utility of a decision is

U(d) = (cid:104)U(d, x)(cid:105)p(x)

where p(x) is the distribution of the outcome x and d represents the decision.

(7.1.2)

7.1.1 Utility of money

You are a wealthy individual, with £1, 000, 000 in your bank account. You are asked if you would like to
participate in a fair coin tossing bet in which, if you win, your bank account will become £1, 000, 000, 000.
However, if you lose, your bank account will contain only £1000. Assuming the coin is fair, should you
take the bet?

If we take the bet our expected bank balance would be

U(bet) = 0.5 × 1, 000, 000, 000 + 0.5 × 1000 = 500, 000, 500.00

(7.1.3)

If we don’t bet, our bank balance will remain at £1, 000, 000. Based on expected utility, we are therefore
advised to take the bet. (Note that if one considers instead the amount one will win or lose, one may show

107

Decision Trees

that the diﬀerence in expected utility between betting and not betting is the same, exercise(73)).

Whilst the above is a correct mathematical derivation, few people who are millionaires are likely to be
willing to risk losing almost everything in order to become a billionaire. This means that the subjective
utility of money is not simply the quantity of money. In order to better reﬂect the situation, the utility
of money would need to be a non-linear function of money, growing slowly for large quantities of money
and decreasing rapidly for small quantities of money, exercise(68).

7.2 Decision Trees

Decision trees are a way to graphically organise a sequential decision process. A decision tree contains
decision nodes, each with branches for each of the alternative decisions. Chance nodes (random variables)
also appear in the tree, with the utility of each branch computed at the leaf of each branch. The expected
utility of any decision can then be computed on the basis of the weighted summation of all branches from
the decision to all leaves from that branch.

Example 29 (Party). Consider the decision problem as to whether or not to go ahead with a fund-raising
garden party. If we go ahead with the party and it subsequently rains, then we will lose money (since very
few people will show up); on the other hand, if we don’t go ahead with the party and it doesn’t rain we’re
free to go and do something else fun. To characterise this numerically, we use:

p(Rain = rain) = 0.6, p(Rain = no rain) = 0.4

(7.2.1)

The utility is deﬁned as

U (party, rain) = −100, U (party, no rain) = 500, U (no party, rain) = 0, U (no party, no rain) = 50
(7.2.2)

We represent this situation in ﬁg(7.1). The question is, should we go ahead with the party? Since we
don’t know what will actually happen to the weather, we compute the expected utility of each decision:

U(party, Rain)p(Rain) = −100 × 0.6 + 500 × 0.4 = 140
U(no party, Rain)p(Rain) = 0 × 0.6 + 50 × 0.4 = 20

(7.2.3)

(7.2.4)

U (party) = (cid:88)
U (no party) = (cid:88)

Rain

Rain

(cid:88)

Rain

max
P arty

Based on expected utility, we are therefore advised to go ahead with the party. The maximal expected
utility is given by (see demoDecParty.m)

p(Rain)U(P arty, Rain) = 140

(7.2.5)

Example 30 (Party-Friend). An extension of the Party problem is that if we decide not to go ahead
with the party, we have the opportunity to visit a friend. However, we’re not sure if this friend will be in.
The question is should we still go ahead with the party?

We need to quantify all the uncertainties and utilities. If we go ahead with the party, the utilities are as
before:

Uparty (party, rain) = −100, Uparty (party, no rain) = 500

108

(7.2.6)

DRAFT March 9, 2010

Decision Trees

y e s

( 0 . 6 )
(0.4)no

Rain

yes

P arty

n

o

y e s

( 0 . 6 )
(0.4)no

Rain

−100

500

0

50

with

Figure 7.1: A decision tree containing chance nodes (denoted
with ovals), decision nodes (denoted with squares) and utility
nodes (denoted with diamonds). Note that a decision tree is not
a graphical representation of a Belief Network with additional
nodes. Rather, a decision tree is an explicit enumeration of the
possible choices that can be made, beginning with the leftmost
decision node, with probabilities on the links out of ‘chance’
nodes.

p(Rain = rain) = 0.6, p(Rain = no rain) = 0.4

(7.2.7)

If we decide not to go ahead with the party, we will consider going to visit a friend. In making the decision
not to go ahead with the party we have utilities

Uparty (no party, rain) = 0, Uparty (no party, no rain) = 50

The probability that the friend is in depends on the weather according to

p(F riend = in|rain) = 0.8, p(F riend = in|no rain) = 0.1,

The other probabilities are determined by normalisation. We additionally have

Uvisit (friend in, visit) = 200, Uvisit (friend out, visit) = −100

(7.2.8)

(7.2.9)

(7.2.10)

with the remaining utilities zero. The two sets of utilities add up so that the overall utility of any decision
sequence is Uparty + Uvisit. The decision tree for the Party-Friend problem is shown is ﬁg(7.2). For each
decision sequence the utility of that sequence is given at the corresponding leaf of the DT. Note that the
leaves contain the total utility Uparty + Uvisit. Solving the DT corresponds to ﬁnding for each decision
node the maximal expected utility possible (by optimising over future decisions). At any point in the
tree choosing that action which leads to the child with highest expected utility will lead to the optimal
strategy. Using this, we ﬁnd that the optimal expected utility has value 140 and is given by going ahead
with the party, see demoDecPartyFriend.m.

• In DTs the same nodes are often repeated throughout the tree. For a longer sequence of decisions,
the number of branches in the tree can grow exponentially with the number of decisions, making
this representation impractical.

• In this example the DT is asymmetric since if we decide to go ahead with the party we will not visit

the friend, curtailing the further decisions present in the lower half of the tree.

(cid:88)

Rain

(cid:88)

F riend

Mathematically, we can express the optimal expected utility U for the Party-Visit example by summing
over un-revealed variables and optimising over future decisions:

max
P arty

p(Rain) max
V isit

p(F riend|Rain) [Uparty(P arty, Rain) + Uvisit(V isit, F riend)I [P arty = no]]
(7.2.11)
where the term I [P arty = no] has the eﬀect of curtailing the DT if the party goes ahead. To answer the
question as to whether or not to go ahead with the party, we take that state of P arty that corresponds to

DRAFT March 9, 2010

109

-100

-100

s
e
y

P arty

s
e
y

(0.6)

Rain

(

0

.

4

)

n

o

500

yes

V isit

n

o

s
e
y

(0.6)

Rain

200

(0.8)in

F riend

(
0.2
)
o

u

t

−100

n

o

0

(0.8)in

F riend

(
0.2
)
o

u

t

0

250

(0.1)in

(

0

.

4

)

n

o

F riend

(
0.9
)
o

u

t

−50

yes

V isit

50

n

o

(0.1)in

F riend

(
0.9
)
o

u

t

50

s
e
y

Rain(140)

s
e
y

n

o

500

200

in

P arty(140)

F riend(140)

o

u

t

−100

yes

V isit(140)

n

o

n

o

0

s
e
y

in

F riend(0)

Rain(104)

o

u

t

0

250

in

n

o

F riend(−20)

o

u

t

−50

yes

V isit(50)

n

o

50

in

F riend(50)

o

u

t

50

(a)

(b)

Decision Trees

Figure 7.2:
Solving a Decision Tree.
(a): Decision Tree for the Party-Visit,
(b): Solving the DT cor-
example(30).
responds to making the decision with the
highest expected future utility. This can
be achieved by starting at the leaves (util-
ities). For a chance parent node x, the
utility of the parent is the expected util-
ity of that variable. For example, at the
top of the DT we have the Rain vari-
able with the children −100 (probability
0.6) and 500 (probability 0.4). Hence
the expected utility of the Rain node is
−100 × 0.6 + 500 × 0.4 = 140. For a
decision node, the value of the node is
the optimum of its child values. One re-
curses thus backwards from the leaves to
the root. For example, the value of the
Rain chance node in the lower branch is
given by 140 × 0.6 + 50 × 0.4 = 104. The
optimal decision sequence is then given at
each decision node by ﬁnding which child
node has the maximal value. Hence the
overall best decision is to decide to go
ahead with the party. If we decided not to
do so, and it does not rain, then the best
decision we could take would be to not visit
the friend (which has an expected utility
of 50). A more compact description of this
problem is given by the inﬂuence diagram,
ﬁg(7.4). See also demoDecPartyFriend.m.

the maximal expected utility above. The way to read equation (7.2.11) is to start from the last decision
that needs to be taken, in this case V isit. When we are at the V isit stage we assume that we will have
previously made a decision about P arty and also will have observed whether or not is is raining. However,
we don’t know whether or not our friend will be in, so we compute the expected utility by averaging over
this unknown. We then take the optimal decision by maximising over V isit. Subsequently we move to the
next-to-last decision, assuming that what we will do in the future is optimal. Since in the future we will
have taken a decision under the uncertain F riend variable, the current decision can then be taken under
uncertainty about Rain and maximising this expected optimal utility over P arty. Note that the sequence
of maximisations and summations matters – changing the order will in general result in a diﬀerent problem
with a diﬀerent expected utility1.

1If one only had a sequence of summations, the order of the summations is irrelevant – likewise for the case of all

maximisations. However, summation and maximisation operators do not in general commute.

110

DRAFT March 9, 2010

Extending Bayesian Networks for Decisions

P arty

Rain

U tility

Figure 7.3: An inﬂuence diagram which contains random vari-
ables (denoted with ovals/circles) Decision nodes (denoted with
squares) and Utility nodes (denoted with diamonds). Con-
trasted with ﬁg(7.1) this is a more compact representation of
the structure of the problem. The diagram represents the ex-
pression p(rain)u(party, rain). In addition the diagram denotes
an ordering of the variables with party ≺ rain (according to the
convention given by equation (7.3.1)).

7.3 Extending Bayesian Networks for Decisions

An inﬂuence diagram is a Bayesian Network with additional Decision nodes and Utility nodes [137, 148,
162]. The decision nodes have no associated distribution; the utility nodes are deterministic functions of
their parents. The utility and decision nodes can be either continuous or discrete; for simplicity, in the
examples here the decisions will be discrete.

A beneﬁt of decision trees is that they are general and explicitly encode the utilities and probabilities
associated with each decision and event. In addition, we can readily solve small decision problems using
decision trees. However, when the sequence of decisions increases, the number of leaves in the decision
tree grows and representing the tree can become an exponentially complex problem. In such cases it can
be useful to use an Inﬂuence Diagram (ID). An ID states which information is required in order to make
each decision, and the order in which these decisions are to be made. The details of the probabilities
and rewards are not speciﬁed in the ID, and this can enable a more compact description of the decision
problem.

7.3.1 Syntax of inﬂuence diagrams

Information Links An information link from a random variable into a decision node:

X

D

indicates that the state of the variable X will be known before decision D is taken. Information
links from another decision node d in to D similarly indicate that decision d is known before decision
D is taken. We use a dashed link to denote that decision D is not functionally related to its parents.

Random Variables Random Variables may depend on the states of parental random variables (as in

Belief Networks), but also Decision Node states:

D

X

Y

As decisions are taken, the states of some random variables will be revealed. To emphasise this we
typically shade a node to denote that its state will be revealed during the sequential decision process.

Utilities A utility node is a deterministic function of its parents. The parents can be either random

variables or decision nodes.

In the party example, the BN trivially consists of a single node, and the Inﬂuence Diagram is given in
ﬁg(7.3). The more complex Party-Friend problem is depicted in ﬁg(7.4). The ID generally provides a
more compact representation of the structure of problem than a DT, although details about the speciﬁc
probabilities and utilities are not present in the ID.

DRAFT March 9, 2010

111

Extending Bayesian Networks for Decisions

P arty

V isit

Rain

F riend

Uparty

Uvisit

Figure 7.4: An inﬂuence diagram for the party-visit prob-
lem, example(30). The partial ordering is P arty∗
≺ Rain ≺
V isit∗
≺ F riend. The dashed-link from party to visit is not
strictly necessary but retained in order to satisfy the conven-
tion that there is a directed path connecting all decision nodes.

Partial ordering
An ID deﬁnes a partial ordering of the nodes. We begin by writing those variables X0 whose states are
known (evidential variables) before the ﬁrst decision D1. We then ﬁnd that set of variables X1 whose
states are revealed before the second decision D2. Subsequently the set of variables Xt is revealed before
decision Dt+1. The remaining fully-unobserved variables are placed at the end of the ordering:

X0 ≺ D1 ≺ X1 ≺ D2, . . . ,≺ Xn−1 ≺ Dn ≺ Xn

(7.3.1)
with Xk being the variables revealed between decision Dk and Dk+1. The term ‘partial’ refers to the fact
that there is no order implied amongst the variables within the set Xn. For notational clarity, at points
below we will indicate decision variables with ∗ to reinforce that we maximise over these variables, and
sum over the non-starred variables. Where the sets are empty we omit writing them. For example, in
∗
ﬁg(7.5a) the ordering is T est

∗

≺ Oil.
The optimal ﬁrst decision D1 is determined by computing

≺ Seismic ≺ Drill

Uj (pa (uj))

(7.3.2)

(cid:88)

X1

(cid:88)

Xn−1

(cid:88)

(cid:89)

Xn

i∈I

p (xi|pa (xi))(cid:88)

j∈J

max
Dn

U(D1|X0) ≡

max
D2

. . .

for each state of the decision D1, given X0. In equation (7.3.2) above I denotes the set of indices for the
random variables, and J the indices for the utility nodes. For each state of the conditioning variables, the
optimal decision D1 is found using

argmax

D1

U(D1|X0)

(7.3.3)

Remark 9 (Reading oﬀ the partial ordering). Sometimes it can be tricky to read the partial ordering
from the ID. A method is to identify the ﬁrst decision D1 and then any variables X0 that need to
be observed to make that decision. Then identify the next decision D2 and the variables X1 that are
revealed after decision D1 is taken and before decision D2 is taken, etc. This gives the partial ordering
X0 ≺ D1 ≺ X1 ≺ D2, . . .. Place any unrevealed variables at the end of the ordering.

Implicit and explicit information links

The information links are a potential source of confusion. An information link speciﬁes explicitly which
quantities are known before that decision is taken2. We also implicitly assume the no forgetting principle
that all past decisions and revealed variables are available at the current decision (the revealed variables
are necessarily the parents of all past decision nodes). If we were to include all such information links, IDs
would get potentially rather messy. In ﬁg(7.5), both explicit and implicit information links are demon-
strated. We call an information link fundamental if its removal would alter the partial ordering.

2Some authors prefer to write all information links where possible, and others prefer to leave them implicit. Here we largely
take the implicit approach. For the purposes of computation, all that is required is a partial ordering; one can therefore view
this as ‘basic’ and the information links as superﬁcial (see [69]).

112

DRAFT March 9, 2010

Extending Bayesian Networks for Decisions

T est

Oil

U2

T est

Oil

U2

U1

Seismic

Drill

U1

Seismic

Drill

(a)

(b)

∗
Figure 7.5: (a): The partial ordering is T est
≺ Oil. The explicit information links
from T est to Seismic and from Seismic to Drill are both fundamental in the sense that removing either
results in a diﬀerent partial ordering. The shaded node emphasises that the state of this variable will be
revealed during the sequential decision process. Conversely, the non-shaded node will never be observed.
(b): Based on the ID in (a), there is an implicit link from T est to Drill since the decision about T est is
taken before Seismic is revealed.

≺ Seismic ≺ Drill

∗

Causal consistency

For an Inﬂuence Diagram to be consistent a current decision cannot aﬀect the past. This means that any
random variable descendants of a decision D in the ID must come later in the partial ordering. Assuming
the no-forgetting principle, this means that for any valid ID there must be a directed path connecting all
decisions. This can be a useful check on the consistency of an ID.

Asymmetry

IDs are most convenient when the corresponding DT is symmetric. However, some forms of asymmetry
are relatively straightforward to deal with in the ID framework. For our party-visit example, the DT is
asymmetric. However, this is easily dealt with in the ID by using a link from P arty to Uvisit which removes
the contribution from Uvisit when the party goes ahead.

More complex issues arise when the set of variables than can be observed depends on the decision sequence
taken. In this case the DT is asymmetric. In general, Inﬂuence Diagrams are not well suited to modelling
such asymmetries, although some eﬀects can be mediated either by careful use of additional variables, or
extending the ID notation. See [69] and [148] for further details of these issues and possible resolutions.

Example 31 (Should I do a PhD?). Consider a decision whether or not to do PhD as part of our education
(E). Taking a PhD incurs costs, UC both in terms of fees, but also in terms of lost income. However, if
we have a PhD, we are more likely to win a Nobel Prize (P ), which would certainly be likely to boost our
Income (I), subsequently beneﬁtting our ﬁnances (UB). This setup is depicted in ﬁg(7.6a). The ordering
is (eliding empty sets)

∗

E

≺ {I, P}

and

(7.3.4)

dom(E) = (do PhD, no PhD) , dom(I) = (low, average, high) , dom(P ) = (prize, no prize)

(7.3.5)

The probabilities are

p(win Nobel prize|no PhD) = 0.0000001

p(win Nobel prize|do PhD) = 0.001

(7.3.6)

p(low|do PhD, no prize) = 0.1 p(average|do PhD, no prize) = 0.5 p(high|do PhD, no prize) = 0.4
p(low|no PhD, no prize) = 0.2 p(average|no PhD, no prize) = 0.6 p(high|no PhD, no prize) = 0.2
p(low|do PhD, prize) = 0.01
p(high|do PhD, prize) = 0.95
p(low|no PhD, prize) = 0.01
p(high|no PhD, prize) = 0.95

p(average|do PhD, prize) = 0.04
p(average|no PhD, prize) = 0.04

DRAFT March 9, 2010

113

S

US

P

I

UB

E

UC

(a)

E

UC

P

I

UB

(b)

Extending Bayesian Networks for Decisions

Figure 7.6: (a): Education E incurs some
cost, but also gives a chance to win a pres-
tigious science prize. Both of these af-
fect our likely Incomes, with correspond-
ing long term ﬁnancial beneﬁts. (b): The
start-up scenario.

The utilities are

UC (do PhD) = −50000, UC (no PhD) = 0,
UB (low) = 100000, UB (average) = 200000, UB (high) = 500000

The expected utility of Education is

U(E) =(cid:88)

I,P

p(I|E, P )p(P|E) [UC(E) + UB(I)]

(7.3.7)

(7.3.8)
(7.3.9)

(7.3.10)

so that U(do phd) = 260174.000, whilst not taking a PhD is U(no phd) = 240000.0244, making it on
average beneﬁcial to do a PhD. See demoDecPhD.m.

Example 32 (PhDs and Start-up companies). Inﬂuence Diagrams are particularly useful when a
sequence of decisions is taken. For example, in ﬁg(7.6b) we model a new situation in which someone has
ﬁrst decided whether or not to take a PhD. Ten years later in their career they decide whether or not
to make a start-up company. This decision is based on whether or not they won the Nobel Prize. The
start-up decision is modelled by S with dom(S) = (tr, fa).
If we make a start-up, this will cost some
money in terms of investment. However, the potential beneﬁt in terms of our income could be high.

We model this with (the other required table entries being taken from example(31)):

p(low|start up, no prize) = 0.1
p(low|no start up, no prize) = 0.2 p(average|no start up, no prize) = 0.6 p(high|no start up, no prize) = 0.2
p(low|start up, prize) = 0.005
p(low|no start up, prize) = 0.05

p(average|start up, no prize) = 0.5
p(average|start up, prize) = 0.005
p(average|no start up, prize) = 0.15

p(high|start up, no prize) = 0.4
p(high|start up, prize) = 0.99
p(high|no start up, prize) = 0.8

(7.3.11)

and

US (start up) = −200000,

US (no start up) = 0

(7.3.12)

Our interest is to advise whether or not it is desirable (in terms of expected utility) to take a PhD, now
bearing in mind that later one may or may not win the Nobel Prize, and may or may not make a start-up
company.

The ordering is (eliding empty sets)

∗

E

≺ P ≺ S

∗

≺ I

114

(7.3.13)

DRAFT March 9, 2010

Solving Inﬂuence Diagrams

d1

x1

d2

x2

u2

d3

x3

u3

x4

u4

Figure 7.7: Markov Decision Process. These can be
used to model planning problems of the form ‘how do
I get to where I want to be incurring the lowest to-
tal cost?’. They are readily solvable using a message
passing algorithm.

The expected optimal utility for any state of E is

U(E) =(cid:88)

max

S

P

(cid:88)

I

p(I|S, P )p(P|E) [US(S) + UC(E) + UB(I)]

(7.3.14)

where we assume that the optimal decisions are taken in the future. Computing the above, we ﬁnd

U(do PhD) = 190195.00,

U(no PhD) = 240000.02

(7.3.15)

Hence, we are better oﬀ not doing a PhD. See demoDecPhd.m.

7.4 Solving Inﬂuence Diagrams

Solving an inﬂuence diagram means computing the optimal decision or sequence of decisions. Here we
focus on ﬁnding the optimal ﬁrst decision. The direct approach is to take equation (7.3.2) and perform
the required sequence of summations and maximisations explicitly. However, we may be able to exploit
the structure of the problem to for computational eﬃciency. To develop this we ﬁrst derive an eﬃcient
algorithm for a highly structured ID, the Markov Decision Process, which we will discuss further in
section(7.5).

7.4.1 Eﬃcient inference

Consider the following function from the ID of ﬁg(7.7)

φ(x4, x3, d3)φ(x3, x2, d2)φ(x2, x1, d1) (u(x2) + u(x3) + u(x4))

(7.4.1)

where the φ represent conditional probabilities and the u are utilities. We write this in terms of potentials
since this will facilitate the generalisation to other cases. Our task is to take the optimal ﬁrst decision,
based on the expected optimal utility

φ(x4, x3, d3)φ(x3, x2, d2)φ(x2, x1, d1) (u(x2) + u(x3) + u(x4))

(7.4.2)

(cid:88)

x3

(cid:88)

x4

max
d2

max
d3

x2

Whilst we could carry out the sequence of maximisations and summations naively, our interest is to derive
a computationally eﬃcient approach. Let’s see how to distribute these operations ‘by hand’. Since only
u(x4) depends on x4 explicitly we can write

(cid:88)
(cid:88)

x3

x3

(cid:88)

x3

(cid:88)

x4

(cid:88)
(cid:88)

x4

x4

φ(x2, x1, d1) max
d2

φ(x2, x1, d1) max
d2

φ(x3, x2, d2) max
d3

φ(x4, x3, d3)u(x4)

φ(x3, x2, d2)u(x3) max
d3

φ(x2, x1, d1)u(x2) max
d2

φ(x3, x2, d2) max
d3

φ(x4, x3, d3)

φ(x4, x3, d3)

(7.4.3)

115

U(d1) =(cid:88)

U(d1) =(cid:88)
+(cid:88)
+(cid:88)

x2

x2

x2

DRAFT March 9, 2010

Solving Inﬂuence Diagrams

(cid:88)
(cid:88)

x4

x4

(cid:88)
(cid:88)

x3

Starting with the ﬁrst line and carrying out the summation over x4 and max over d3, this gives a new
function of x3,

u3←4(x3) ≡ max

d3

φ(x4, x3, d3)u(x4)

In addition we deﬁne the message (which in our particular example will be unity)

(7.4.4)

(7.4.5)

φ3←4(x3) ≡ max
Using this we can write

d3

U(d1) =(cid:88)

φ(x4, x3, d3)

(cid:88)

x3

φ(x2, x1, d1) max
d2

x2

φ(x3, x2, d2) [u(x3)φ3←4(x3) + u3←4(x3)]

+(cid:88)

x2

(cid:88)

x3

φ(x2, x1, d1)u(x2) max
d2

φ(x3, x2, d2)φ3←4(x3)

(7.4.6)

Now we carry out the sum over x3 and max over d2 for the ﬁrst row above and deﬁne a utility message

u2←3(x2) ≡ max

d2

x3
and probability message3

φ2←3(x2) ≡ max

d2

φ(x3, x2, d2) [u(x3)φ3←4(x3) + u3←4(x3)]

φ(x3, x2, d2)φ3←4(x3)

(7.4.7)

(7.4.8)

The optimal decision for d1 can be obtained from

φ(x2, x1, d1) [u(x2)φ2←3(x2) + u2←3(x2)]

U(d1) =(cid:88)
U(d1) =(cid:88)

x2

x2

Since the probability message φ2←3(x2) represents information about the distribution passed to x2 via x3,
it is more intuitive to write

(cid:20)

(cid:21)

φ(x2, x1, d1)φ2←3(x2)

u(x2) + u2←3(x2)
φ2←3(x2)

which has the interpretation of the average of a utility with respect to a distribution.

It is intuitively clear that we can continue along this line for richer structures than chains. Indeed, provided
we have formed an appropriate junction tree, we can pass potential and utility messages from clique to
neighbouring clique, as described in the following section.

7.4.2 Using a junction tree

In complex IDs computational eﬃciency in carrying out the series of summations and maximisations may
be an issue and one therefore seeks to exploit structure in the ID. It is intuitive that some form of junction
tree style algorithm is applicable. We can ﬁrst represent an ID using decision potentials which consist of
two parts, as deﬁned below.

Deﬁnition 53 (Decision Potential). A decision potential on a clique C contains two potentials: a prob-
ability potential ρC and a utility potential µC. The joint potentials for the junction tree are deﬁned
as

µC

(7.4.9)

ρ = (cid:89)

C∈C

ρC,

µ =(cid:88)

C∈C

with the junction tree representing the term ρµ.

3For our MDP example all these probability messages are unity.

116

DRAFT March 9, 2010

Solving Inﬂuence Diagrams

In this case there are constraints on the triangulation, imposed by the partial ordering which restricts the
variables elimination sequence. This results in a so-called strong Junction Tree. The treatment here is
inspired by [146]; a related approach which deals with more general chain graphs is given in [69]. The
sequence of steps required to construct a JT for an ID is as follows:

Remove Information Edges Parental links of decision nodes are removed4.

Moralization Marry all parents of the remaining nodes.

Remove Utility Nodes Remove the utility nodes and their parental links.

Strong Triangulation Form a triangulation based on an elimination order which obeys the partial or-

dering of the variables.

Strong Junction Tree From the strongly triangulated graph, form a junction tree and orient the edges

towards the strong root (the clique that appears last in the elimination sequence).

The cliques are ordered according to the sequence in which they are eliminated. The separator probability
cliques are initialised to the identity, with the separator utilities initialised to zero. The probability cliques
are then initialised by placing conditional probability factors into the lowest available clique (according to
the elimination order) that can contain them, and similarly for the utilities. Remaining probability cliques
are set to the identity and utility cliques to zero.

Example 33 (Junction Tree). An example of a junction tree for an ID is given in ﬁg(7.8a). The
moralisation and triangulation links are given in ﬁg(7.8b). The orientation of the edges follows the
partial ordering with the leaf cliques being the ﬁrst to disappear under the sequence of summations and
maximisations.

A by-product of the above steps is that the cliques describe the fundamental dependencies on previous
decisions and observations. In ﬁg(7.8a), for example, the information link from f to D2 is not present
in the moralised-triangulated graph ﬁg(7.8b), nor in the associated cliques of ﬁg(7.8c). This is because
once e is revealed, the utility U4 is independent of f, giving rise to the two-branch structure in ﬁg(7.8b).
Nevertheless, the information link from f to D2 is fundamental since it speciﬁes that f will be revealed –
removing this link would therefore change the partial ordering.

Absorption

By analogy with the deﬁnition of messages in section(7.4.1), for two neighbouring cliques C1 and C2, where
C1 is closer to the strong root of the JT (the last clique deﬁned through the elimination order), we deﬁne

∗(cid:88)

C2\S

ρS =

ρC2,

µS =

∗(cid:88)

C2\S

ρC2µC2

(7.4.10)

(7.4.11)

ρnew
C1 = ρC1ρS,

In the above (cid:80)∗

C1 = µC1 + µS
µnew
ρS

C is a ‘generalised marginalisation’ operation – it sums over those elements of clique C
which are random variables and maximises over the decision variables in the clique. The order of this
sequence of sums and maximisations follows the partial ordering deﬁned by ≺.
Absorption is then computed from the leaves inwards to the root of the strong Junction Tree. The optimal
setting of a decision D1 can then be computed from the root clique. Subsequently backtracking may be

4Note that for the case in which the domain is dependent on the parental variables, such links must remain.

DRAFT March 9, 2010

117

l

U4

U3

a

b

D1

c

d

D4

i

h

D3

j

k

U2

g

D2

(a)

a

b

D1

U1

c

d

e

f

b, c, a

b, c

Solving Inﬂuence Diagrams

D4

i

h

D3

l

j

k

g

D2

e

f

(b)

b, e, d, c

e, D2, g

D2, g

D2, g, D4, i

D4, i

D4, i, l

b, e, d

b, D1, e, f, d

e

f

f, D3, h

D3, h

D3, h, k

h, k

h, k, j

(c)

Figure 7.8: (a): Inﬂuence Diagram, adapted from [146]. Causal consistency is satisﬁed since there is a
directed path linking the all decisions in sequence. The partial ordering is b ≺ D1 ≺ (e, f) ≺ D2 ≺ (·) ≺
D3 ≺ g ≺ D4 ≺ (a, c, d, h, i, j, k, l). (b): Moralised and strongly triangulated graph. Moralisation links are
in green, strong triangulation links are in red. (c): Strong Junction Tree. Absorption passes information
from the leaves of the tree towards the root.

applied to infer the optimal decision trajectory. The optimal decision for D can be obtained by working
with the clique containing D which is closest to the strong root and setting any previously taken decisions
and revealed observations into their evidential states. See demoDecAsia.m for an example.

Example 34 (Absorption on a chain). For the ID of ﬁg(7.7), the moralisation and triangulation steps
are trivial and give the JT:

3: x1, x2, d1

x2

2: x2, x3, d2

x3

1: x3, x4, d3

where the cliques are indexed according the elimination order. The probability and utility cliques are
initialised to

ρ3 (x1, x2, d1) = p(x2|x1, d1) µ3 (x1, x2, d1) = 0
ρ2 (x2, x3, d2) = p(x3|x2, d2) µ2 (x2, x3, d2) = u(x2)
ρ1 (x3, x4, d3) = p(x4|x3, d3) µ1 (x3, x4, d3) = u(x3) + u(x4)

(7.4.12)

118

DRAFT March 9, 2010

Updating the separator we have the new probability potential

ρ1 (x3, x4, d3) = 1

Solving Inﬂuence Diagrams

with the separator cliques initialised to

ρ1−2 (x3) = 1 µ1−2 (x3) = 0
ρ2−3 (x2) = 1 µ2−3 (x2) = 0

ρ1−2 (x3)∗ = max

d3

and utility potential

µ1−2 (x3)∗ = max

d3

= max
d3

x4

(cid:88)
(cid:88)
(cid:32)
u(x3) +(cid:88)

x4

x4

(cid:88)

x4

p(x4|x3, d3) (u(x3) + u(x4))

(7.4.15)

ρ1 (x3, x4, d3) µ1 (x3, x4, d3) = max
d3

(cid:33)
p(x4|x3, d3)u(x4)

(7.4.16)

(7.4.17)

(cid:33)

p(x4|x3, d3)u(x4)

(7.4.18)

(7.4.13)

(7.4.14)

(7.4.19)

(7.4.20)

(7.4.21)

(7.4.22)

(7.4.23)

(7.4.24)

(7.4.25)

At the next step we update the probability potential

ρ2 (x2, x3, d2)∗ = ρ2 (x2, x3, d2) ρ1−2 (x3)∗ = 1

and utility potential

µ2 (x2, x3, d2)∗ = µ2 (x2, x3, d2) + µ1−2 (x3)∗

ρ1−2 (x3)

= u(x2) + max
d3

(cid:32)

u(x3) +(cid:88)

x4

The next separator decision potential is

ρ2−3 (x2)∗ = max

d2

µ2−3 (x2)∗ = max

d2

= max
d2

ρ2 (x2, x3, d2)∗ = 1

ρ2 (x2, x3, d2) µ2 (x2, x3, d2)∗
(cid:32)
u(x3) +(cid:88)

(cid:32)

u(x2) + max
d3

p(x3|x2, d2)

x4

(cid:33)(cid:33)
p(x4|x3, d3)u(x4)

x3

(cid:88)
(cid:88)
(cid:88)

x3

x3

Finally we end up with the root decision potential

ρ3 (x1, x2, d1)∗ = ρ3 (x1, x2, d1) ρ2−3 (x2)∗ = p(x2|x1, d1)

and

µ3 (x1, x2, d1)∗ = µ3 (x2, x1, d1) + µ2−3 (x2)∗
(cid:32)
ρ2−3 (x2)∗

= max
d2

p(x3|x2, d2)

u(x2) + max
d3

(cid:88)

x3

(cid:32)
u(x3) +(cid:88)

x4

(cid:33)(cid:33)
p(x4|x3, d3)u(x4)

From the ﬁnal decision potential we have the expression

ρ3 (x1, x2, d1)∗

µ3 (x1, x2, d1)∗

which is equivalent to that which would be obtained by simply distributing the summations and maximi-
sations over the original ID. At least for this special case, we therefore have veriﬁed that the JT approach
yields the correct root clique potentials.

DRAFT March 9, 2010

119

Markov Decision Processes

7.5 Markov Decision Processes

Consider a Markov chain with transition probabilities p(xt+1 = j|xt = i). At each time t we consider an
action (decision), which aﬀects the state at time t + 1. We describe this by

p(xt+1 = i|xt = j, dt = k)

(7.5.1)
Associated with each state xt = i is a utility u(xt = i), and is schematically depicted in ﬁg(7.7). One use
of such an environment model would be to help plan a sequence of actions (decisions) required to reach a
goal state in minimal total summed cost.

More generally one could consider utilities that depend on transitions and decisions, u(xt+1 = i, xt = j, dt =
k) and also time dependent versions of all of these, pt(xt+1 = i|xt = j, dt = k), ut(xt+1 = i, xt = j, dt = k).
We’ll stick with the time-independent (stationary) case here since the generalisations are conceptually
straightforward at the expense of notational complexity.

MDPs can be used to solve planning tasks such as how can one get to a desired goal state as quickly as
possible. By deﬁning the utility of being in the goal state as high, and being in the non-goal state as a low
value, at each time t, we have a utility u(xt) of being in state xt. For positive utilities, the total utility of
any state-decision path x1:T , d1:T is deﬁned as (assuming we know the initial state x1)

and the probability with which this happens is given by

T(cid:88)

t=2

U(x1:T ) ≡

u(xt)

T−1(cid:89)
p(x2:T|x1, d1:T−1) =
(cid:88)

(cid:88)

t=1

U(d1) ≡

max
d2

max
d3

x2

x3

x4

p(xt+1|xt, dt)
(cid:88)
(cid:88)

. . . max
dT−1

xT

At time t = 1 we want to make that decision d1 that will lead to maximal expected total utility

p(x2:T|x1, d1:T−1)U(x1:T )

Our task is to compute U(d1) for each state of d1 and then choose that state with maximal expected
total utility. To carry out the summations and maximisations eﬃciently, we could use the junction tree
approach, as described in the previous section. However, in this case, the ID is suﬃciently simple that a
direct message passing approach can be used to compute the expected utility.

7.5.1 Maximising expected utility by message passing

(7.5.2)

(7.5.3)

(7.5.4)

(7.5.5)

(7.5.6)

(7.5.7)

(7.5.8)

Consider the MDP

T−1(cid:89)

t=1

p(xt+1|xt, dt)

T(cid:88)

t=2

u(xt)

For the speciﬁc example in ﬁg(7.7) the joint model of the BN and utility is

p(x4|x3, d3)p(x3|x2, d2)p(x2|x1, d1) (u(x2) + u(x3) + u(x4))

To decide on how to take the ﬁrst optimal decision, we need to compute

max
d2

max
d3

p(x4|x3, d3)p(x3|x2, d2)p(x2|x1, d1) (u(x2) + u(x3) + u(x4))

Since only u(x4) depends on x4 explicitly, we can write

max
d3

p(x4|x3, d3)p(x3|x2, d2)p(x2|x1, d1)u(x4)

p(x3|x2, d2)p(x2|x1, d1)u(x3)

(cid:88)
(cid:88)

x4

x4

x2

U(d1) =(cid:88)
U(d1) =(cid:88)
+(cid:88)
+(cid:88)

x2

x2

x3

(cid:88)
(cid:88)
(cid:88)

x3

x3

max
d2

max
d2

p(x2|x1, d1)u(x2)

x2

120

DRAFT March 9, 2010

(cid:88)

x4

p(x3|x2, d2) max
p(x3|x2, d2)u(x3)

d3

p(x4|x3, d3)u(x4)

(7.5.9)

We now start with the ﬁrst line and carry out the summation over x4 and maximisation over d3. This
gives a new function of x3,

Markov Decision Processes

For each line we distribute the operations:

U(d1) =(cid:88)
+(cid:88)
+(cid:88)

x2

x2

x2

d2

p(x2|x1, d1) max
p(x2|x1, d1) max
p(x2|x1, d1)u(x2)

d2

u3←4(x3) ≡ max

d3

p(x4|x3, d3)u(x4)

which we can incorporate in the next line

U(d1) =(cid:88)
+(cid:88)

x2

x2

p(x2|x1, d1) max
p(x2|x1, d1)u(x2)

d2

(cid:88)
(cid:88)

x3

x3

(cid:88)

x3

(cid:88)

x4

(cid:88)

x3

(7.5.10)

(7.5.11)

(7.5.12)

(7.5.13)

(7.5.14)

(7.5.15)

(7.5.16)

(7.5.17)

(7.5.18)

(7.5.19)

121

p(x3|x2, d2) [u(x3) + u3←4(x3)]

Similarly, we can now carry out the sum over x3 and max over d2 to deﬁne a new function

u2←3(x2) ≡ max

d2

p(x3|x2, d2) [u(x3) + u3←4(x3)]

to give

U(d1) =(cid:88)

x2

p(x2|x1, d1) [u(x2) + u2←3(x2)]

Given U(d1) above, we can then ﬁnd the optimal decision d1 by

d1
2? Bear in mind that when we come to make decision d2 we will have observed x1, x2 and

3. In general, the optimal decision is given by

∗
1 = argmax
d

U(d1)

What about d∗
d2. We can then ﬁnd d∗

2 by

(cid:88)

d2

argmax

p(x3|x2, d2) [u(x3) + u3←4(x3)]
(cid:88)
Subsequently we can backtrack further to ﬁnd d∗

x3

∗
t−1 = argmax
d

p(xt|xt−1, dt−1) [u(xt) + ut←t+1(xt)]

dt−1

xt

7.5.2 Bellman’s equation

(cid:88)

xt

ut−1←t(xt−1) ≡ max

dt−1

p(xt|xt−1, dt−1) [u(xt) + ut←t+1(xt)]

It is more common to deﬁne the value of being in state xt as

vt(xt) ≡ u(xt) + ut←t+1(xt),

vT (xT ) = u(xT )

and write then the equivalent recursion

vt−1(xt−1) = u(xt−1) + max
dt−1

DRAFT March 9, 2010

p(xt|xt−1, dt−1)vt(xt)

(cid:88)

xt

In a Markov Decision Process, as above, we can deﬁne utility messages recursively as

Temporally Unbounded MDPs

The optimal decision d∗

t is then given by

(cid:88)

xt+1

∗
t = argmax
d

dt

p(xt+1|xt, dt)v(xt+1)

(7.5.20)

Equation(7.5.19) is called Bellman’s equation[30]5.

7.6 Temporally Unbounded MDPs

In the previous discussion about MDPs we assumed a given end time, T , from which one can propagate
messages back from the end of the chain. The inﬁnite T case would appear to be ill-deﬁned since the sum
of utilities

u(x1) + u(x2) + . . . + u(xT )

(7.6.1)
will in general be unbounded. There is a simple way to avoid this diﬃculty. If we let u∗ = maxs u(s)
be the largest value of the utility and consider the sum of modiﬁed utilities for a chosen discount factor
0 < γ < 1

T(cid:88)

t=1

∗ T(cid:88)

t=1

γtu(xt) ≤ u

γt = γu

∗ 1 − γT
1 − γ

(7.6.2)

where we used the result for a geometric series. In the limit T → ∞ this means that the summed modiﬁed
utility γtu(xt) is ﬁnite. The only modiﬁcation required to our previous discussion is to include a factor γ
in the message deﬁnition. Assuming that we are at convergence, we deﬁne a value v(xt = s) dependent
only on the state s, and not the time. This means we replace the time-dependent Bellman’s value recursion
equation (7.5.19) with the stationary equation

v(s) ≡ u(s) + γ max

d

p(xt = s(cid:48)

|xt−1 = s, dt−1 = d)v(s(cid:48))

(7.6.3)

(cid:88)

s(cid:48)

We then need to solve equation (7.6.3) for the value v(s) for all states s. The optimal decision policy when
one is in state xt = s is then given by
p(xt+1 = s(cid:48)

d∗(s) = argmax

(cid:88)

(7.6.4)

|xt = s, dt = d)v(s(cid:48))

d

s(cid:48)

For a deterministic transition p (i.e. for each decision d, only one state s(cid:48) is available), this means that
the best decision is the one that takes us to the accessible state with highest value.

Equation(7.6.3) seems straightforward to solve. However, the max operation means that the equations
are non-linear in the value v and no closed form solution is available. Two popular techniques for solving
equation (7.6.3), are Value and Policy iteration, which we describe below. When the number of states S
is very large, approximate solutions are required. Sampling and state-dimension reduction techniques are
described in [58].

7.6.1 Value iteration

A naive procedure is to iterate equation (7.6.3) until convergence, assuming some initial guess for the
values (say uniform). One can show that this value iteration procedure is guaranteed to converge to a
unique optimum[34]. The convergence rate depends somewhat on the discount γ – the smaller γ is, the
faster is the convergence. An example of value iteration is given in ﬁg(7.10).

5The continuous-time analog has a long history in physics and is called the Hamilton-Jacobi equation and enables one
to solve MDPs by message passing, this being a special case of the more general junction tree approach described earlier in
section(7.4.2).

122

DRAFT March 9, 2010

Temporally Unbounded MDPs

Figure 7.9: States deﬁned on a two dimensional grid. In each square the top
left value is the state number, and the bottom right is the utility of being
in that state. An ‘agent’ can move from a state to a neighbouring state,
as indicated. The task is to solve this problem such that for any position
(state) one knows how to move optimally to maximise the expected utility.
This means that we need to move towards the goal states (states with non-
zero utility). See demoMDP.

Figure 7.10: Value Iteration on a set of 225 states,
corresponding to a 15× 15 two dimensional grid. De-
terministic transitions are allowed to neighbours on
the grid, {stay, left, right, up, down}. There are three
goal states, each with utility 1 – all other states have
utility 0. Plotted is the value v(s) for γ = 0.9 after 30
updates of Value Iteration, where the states index a
point on the x− y grid. The optimal decision for any
state on the grid is to go to the neighbouring state
with highest value. See demoMDP.

7.6.2 Policy iteration
In policy iteration we ﬁrst assume we know the optimal decision d∗(s) for any state s. We may use this in
equation (7.6.3) to give

v(s) = u(s) + γ

p(xt = s(cid:48)

|xt−1 = s, d∗(s))v(s(cid:48))

(7.6.5)

(cid:88)

s(cid:48)

The maximisation over d has disappeared since we have assumed we already know the optimal decision for
each state s. For ﬁxed d∗(s), equation (7.6.5) is now linear in the value. Deﬁning the value v and utility
u vectors and transition matrix P,
[u]s = u(s),

[v]s = v(s),

(7.6.6)

[P]s(cid:48),s = p(s’|s, d∗ (s))

in matrix notation, equation (7.6.5) becomes

(cid:16)

I − γPT(cid:17)

(cid:16)

I − γPT(cid:17)−1

v = u + γPTv ⇔

v = u ⇔ v =

u

(7.6.7)

These linear equations are readily solved with Gaussian Elimination. Using this, the optimal policy is
recomputed using equation (7.6.4). The two steps of solving for the value, and recomputing the policy are
iterated until convergence.
In Policy Iteration we guess an initial d∗(s), then solve the linear equations (7.6.5) for the value, and then
recompute the optimal decision. See demoMDP.m for a comparison of value and policy iteration, and also
an EM style approach which we discuss in the next section.

Example 35 (A grid-world MDP). A set of states deﬁned on a grid, utilities for being in a grid state
is given in ﬁg(7.9), for which the agent deterministically moves to a neighbouring grid state at each time
step. After initialising the value of each grid state to unity, the converged value for each state is given in
ﬁg(7.10). The optimal policy is then given by moving to the neighbouring grid state with highest value.

DRAFT March 9, 2010

123

1020314050607080901001101201301401501601701811902002102202302402502602702802903003103203303403503603703803904004104204304404504614704804905005115205305405505605705805906006106206306406506606706806907017107207317407507607707807908008108208308408508608708808909019109219309409509609709809901000123456789101112131415123456789101112131415024d1

x1

d2

x2

(a)

d1

x3

x1

u3

d2

x2

(b)

x3

u3

Probabilistic Inference and Planning

Figure 7.11: (a): A Markov Decision Pro-
(b): The corresponding probabilis-
cess.
tic inference planner.

7.6.3 A curse of dimensionality

Consider the following Tower of Hanoi problem. There are 4 pegs a, b, c, d and 10 disks numbered from 1
to 10. You may move a single disk from one peg to another – however, you are not allowed to put a bigger
numbered disk on top of a smaller numbered disk. Starting with all disks on peg a, how can you move
them all to peg d in the minimal number of moves?

This would appear to be a straightforward Markov decision process in which the transitions are allowed
disk moves. If we use x to represent the state of the disks on the 4 pegs, this has 410 = 1048576 states
(some are equivalent up to permutation of the pegs, which reduces this by a factor of 2). This large number
of states renders this naive approach computationally problematic.

Many interesting real-world problems suﬀer from this large number of states issue so that a naive approach
based as we’ve described is computationally infeasible. Finding eﬃcient exact and also approximate state
representations is a key aspect to solving large scale MDPs, see for example [193].

7.7 Probabilistic Inference and Planning

An alternative to the classical MDP solution methods is to make use of the standard methods for training
probabilistic models, such as the Expectation-Maximisation algorithm. In order to do so we ﬁrst need to
write the problem of maximising expected utility in a form that is suitable. To do this we ﬁrst discuss
how a MDP can be expressed as the maximisation of a form of Belief Network in which the parameters
to be found relate to the policy.

7.7.1 Non-stationary Markov Decision Process

Consider the MDP in ﬁg(7.11a) in which, for simplicity, we assume we know the initial state x1 = x1.
Our task is then to ﬁnd the decisions that maximise the expected utility, based on a sequential decision
process. The ﬁrst decision d1 is given by maximising the expected utility:

U(d1) =(cid:88)

x2

(cid:88)

x3

p(x2|x1, d1) max

d2

p(x3|x2, d2)u3(x3)

More generally, this utility can be computed eﬃciently using a standard message passing routine:

(cid:88)

xt+1

ut←t+1(xt) ≡ max

dt

where

uT←T +1(xT ) = uT (xT )

p(xt+1|xt, dt)ut+1←t+2(xt+1)

(7.7.1)

(7.7.2)

(7.7.3)

124

DRAFT March 9, 2010

Probabilistic Inference and Planning

7.7.2 Non-stationary probabilistic inference planner

As an alternative to the above MDP description, consider the Belief Network ﬁg(7.11b) in which we have
a utility associated with the last time-point[279]. Then the expected utility is given by

U(π1, π2) = (cid:88)
=(cid:88)

d1,d2,x2,x3

p(d1|x1, π1)p(x2|x1, d1)p(d2|x2, π2)p(x3|x2, d2)u3(x3)

p(d1|x1, π1)(cid:88)

p(x2|x1, d1)(cid:88)

p(d2|x2, π2)(cid:88)

d1

x2

d2

x3

p(x3|x2, d2)u3(x3)

(7.7.4)

(7.7.5)

Here the terms p(dt|xt, πt) are the ‘policy distributions’ that we wish to learn and πt are the parameters
of the tth policy distribution. Let’s assume that we have one per time so that πt is a function that maps a
state x to a probability distribution over decisions. Our interest is to ﬁnd the policy distributions π1, π2
that maximise the expected utility. Since each time-step has its own πt and for each state x2 = x2 we have
a separate unconstrained distribution p(d2|x2, π2) to optimise over and we can write

max
π1,π2

U(π1, π2) = max
π1

p(x2|x1, d1) max

π2

p(x3|x2, d2)u3(x3) (7.7.6)

(cid:88)

d1

p(d1|x1, π1)(cid:88)

x2

(cid:88)

d2

p(d2|x2, π2)(cid:88)

x3

This shows that provided there are no constraints on the policy distributions (there is a separate one for
each timepoint), we are allowed to distribute the maximisations over the individual policies inside the
summation.

More generally, for a ﬁnite time T one can deﬁne messages to solve for the optimal policy distributions

ut←t+1(xt) ≡ max

πt

p(xt+1|xt, dt)ut+1←t+2(xt+1)

(cid:88)

dt

p(dt|xt, πt)(cid:88)

xt+1

with

uT +1←T (xT ) = uT (xT )

Deterministic policy

For a deterministic policy, only a single state is allowed, so that

(7.7.7)

(7.7.8)

where d∗
function for each time t equation (7.7.7) reduces to

∗
p(dt|xt, πt) = δ (dt, d
t (xt))
t (x) is a policy function that maps a state x to a single decision d. Since we have a separate policy
(cid:88)

(7.7.9)

ut←t+1(xt) ≡ max

d∗
t (xt)

xt+1

∗
t (xt))ut+1←t+2(xt+1)
p(xt+1|xt, d

(7.7.10)

which is equivalent to equation (7.7.2).

This shows that solving the MDP is equivalent to maximising a standard expected utility deﬁned in terms
of a Belief Network under the assumption that each time point has its own policy distribution, and that
this is deterministic.

7.7.3 Stationary planner

If we reconsider our simple example, ﬁg(7.11b) but now constrain the policy distributions to be the same
for all time, p(dt|xt, πt) = p(dt|xt, π) (or more succinctly πt = π), then equation (7.7.5) becomes

p(d1|x1, π)(cid:88)

p(x2|x1, d1)(cid:88)

p(d2|x2, π)(cid:88)

p(x3|x2, d2)u3(x3)

(7.7.11)

U(π) =(cid:88)

d1

x2

d2

x3

In this case we cannot distribute the maximisation over the policy π over the individual terms of the
product. However, computing the expected utility for any given policy π is straightforward, using message
passing. One may thus optimise the expected utility using standard numerical optimisation procedures,
or alternatively an EM style approach as we discuss below.

DRAFT March 9, 2010

125

A variational training approach

Probabilistic Inference and Planning

Without loss of generality, we assume that the utility is positive and deﬁne a distribution

(cid:80)
d1,d2,d3,x2,x3 p(d1|x1, π)p(x2|x1, d1)p(d2|x2, π)p(x3|x2, d2)u3(x3)

p(d1|x1, π)p(x2|x1, d1)p(d2|x2, π)p(x3|x2, d2)u3(x3)

(7.7.12)

˜p(d1, d2, d3, x2, x3) =

Then for any variational distribution q(d1, d2, d3, x2, x3),

KL(q(d1, d2, d3, x2, x3)|˜p(d1, d2, d3, x2, x3)) = (cid:104)log q(d1, d2, d3, x2, x3)(cid:105)q(d1,d2,d3,x2,x3)

− (cid:104)log ˜p(d1, d2, d3, x2, x3)(cid:105)q(d1,d2,d3,x2,x3) ≥ 0 (7.7.13)
Using the deﬁnition of ˜p(d1, d2, d3, x2, x3) and the fact that the denominator in equation (7.7.12) is equal
to U(π) we obtain the bound

log U(π) ≥ −(cid:104)log q(d1, d2, d3, x2, x3)(cid:105)q(d1,d2,d3,x2,x3)

+ (cid:104)log p(d1|x1, π)p(x2|x1, d1)p(d2|x2, π)p(x3|x2, d2)u3(x3)(cid:105)q(d1,d2,d3,x2,x3)

(7.7.14)

This then gives a two-stage EM style procedure:

M-step Isolating the dependencies on π, for a given variational distribution qold, maximising the bound

equation (7.7.14) is equivalent to maximising

E(π) ≡ (cid:104)log p(d1|x1, π)(cid:105)qold(d1) + (cid:104)log p(d2|x2, π)(cid:105)qold(d2,x2)

One then ﬁnds a policy πnew which maximises E(π):

πnew = argmax

π

E(π)

E-step For ﬁxed π the best q is given by the update

qnew ∝ p(d1|x1, π)p(x2|x1, d1)p(d2|x2, π)p(x3|x2, d2)u3(x3)

(7.7.15)

(7.7.16)

(7.7.17)

From this joint distribution, in order to determine the M-step updates, we only require the marginals
q(d1) and q(d2, x2), both of which are straightforward to obtain since q is simply a ﬁrst order
Markov Chain in the joint variables xt, dt. For example one may write the q-distribution as a simple
chain Factor Graph for which marginal inference can be performed readily using the sum-product
algorithm.

This procedure is analogous to the standard EM procedure, section(11.2). The usual guarantees therefore
carry over so that ﬁnding a policy that increases E(π) is guaranteed to improve the expected utility.

In complex situations in which, for reasons of storage, the optimal q cannot be used, a structured con-
strained variational approximation may be applied. In this case, as in generalised EM, only a guaranteed
improvement on the lower bound of the expected utility is achieved. Nevertheless, this may be of consid-
erable use in practical situations, for which general techniques of approximate inference may be applied.

The deterministic case

For the special case that the policy π is deterministic, π simply maps each state x to single decision d.
Writing this policy map as d∗(x) equation (7.7.11) reduces to

∗) =(cid:88)

x2

U(d

∗(x1))(cid:88)

x3

p(x2|x1, d

p(x3|x2, d

∗(x2))u3(x3)

We now deﬁne a variational distribution only over x2, x3,

q(x2, x3) ∝ p(x2|x1, d

∗(x1))p(x3|x2, d

∗(x2))u3(x3)

126

DRAFT March 9, 2010

(7.7.18)

(7.7.19)

Probabilistic Inference and Planning

and the ‘energy’ term becomes

E(d

∗) ≡ (cid:104)log p(x2|x1, d
(cid:32)(cid:88)

(cid:88)

E(d(s)) ≡

s(cid:48)

t

and

d∗(s) = argmax

E(d(s))

d(s)

∗(x1))(cid:105)q(x2) + (cid:104)log p(x3|x2, d

∗(x2))(cid:105)q(x2,x3)

(7.7.20)

For a more general problem in which the utility is at the last time point T and no starting state is given
(cid:33)
we have (for a stationary transition p(xt+1|xt, dt))

q(xt = s, xt+1 = s(cid:48))

log p(x

(cid:48) = s(cid:48)

|x = s, d(s) = d)

(7.7.21)

(7.7.22)

This shows how to train a stationary MDP using EM in which there is a utility deﬁned only at the last
time-point. Below we generalise this to the case of utilities at each time for both the stationary and
non-stationary cases.

7.7.4 Utilities at each timestep

Consider a generalisation in which we have an additive utility associated with each time-point.

Non-stationary policy

To help develop the approach, let’s look at simply including utilities at times t = 1, 2 for the previous
example. The expected utility is given by

p(d1|x1, π1)p(x2|x1, d1)p(d2|x2, π2)p(x3|x2, d2)u3(x3)

(7.7.23)

d1,x2

d1,d2,x1,x2,x3

U(π1, π2) = (cid:88)
+ (cid:88)
= u1(x1) + (cid:88)
vπ2(x2) = u2(x2) + (cid:88)
vπ1(x1) = u1(x1) + (cid:88)

d2,x3

d1,x2

Deﬁning value messages

and

p(d1|x1, π1)p(x2|x1, d1)u2(x2) + u1(x1)

u2(x2) + (cid:88)

d2,x3

p(d1|x1, π1)p(x2|x1)

p(d2|x2, π2)p(x3|x2, d2)u3(x3)

p(d1|x1, π1)p(x2|x1, d1)vπ2(x2)

d1,x2



p(d2|x2, π2)p(x3|x2, d2)u3(x3)

(7.7.24)

(7.7.25)

(7.7.26)

U(π1, π2) = vπ1(x1)

(7.7.27)
For a more general case deﬁned over T timesteps, we have analogously an expected utility U(π1:T ), and
our interest is to maximise this expected utility with respect to all the policies

max
π1:T

U(π1:T )

(7.7.28)

As before, since each timestep has its own policy distribution for each state, we may distribute the
maximisation using the recursion

vt,t+1(xt) ≡ ut(xt) + max

πt

with

vT,T +1(xT ) ≡ u(xT )

DRAFT March 9, 2010

p(dt|xt, πt)p(xt+1|xt, dt)vt+1,t+2(xt+1)

(7.7.29)

(7.7.30)

127

(cid:88)

dt,xt+1

Stationary deterministic policy

Probabilistic Inference and Planning

For an MDP the optimal policy is deterministic[267], so that methods which explicitly seek for deterministic
policies are of interest. For a stationary deterministic policy π we have the expected utility

T(cid:88)

(cid:88)

ut(xt) (cid:88)

t(cid:89)

t=1

xt

x1:t−1

τ =1

U(π) =

p(xτ|xτ−1, d(xτ−1))

(7.7.31)

with the convention p(x1|x0, d(x0)) = p(x1). Viewed as a Factor Graph, this is simply a chain, so that for
any policy d, the expected utility can be computed easily. In principle one could then attempt to optimise
U with respect to the decisions directly. An alternative is to use an EM style procedure[100]. To do this
we need to deﬁne a (trans-dimensional) distribution

ˆp(x1:t, t) = ut(xt)
Z(d)

T(cid:88)

(cid:88)

t(cid:89)

p(xτ|xτ−1, d(xτ−1))
T(cid:88)

(cid:88)

The normalisation constant Z(d) of this distribution is

ut(xt)

p(xτ|xτ−1, d(xτ−1)) =

t=1

x1:t

τ =1

t=1

x1:t

ut(xt)

(7.7.32)

t(cid:89)

τ =1

p(xτ|xτ−1, d(xτ−1)) = U(π)

(7.7.33)

t(cid:89)

τ =1

If we now deﬁne a variational distribution q(x1:t, t), and consider

KL(q(x1:t, t)|ˆp(x1:t, t)) ≥ 0

this gives the lower bound

(cid:42)

log U(π) ≥ −H(q(x1:T , t)) +

log ut(xt)

(cid:43)
p(xτ|xτ−1, d(xτ−1))

q(x1:t,t)

t(cid:89)

τ =1

In terms of an EM algorithm, the M-step requires the dependency on d alone, which is

T(cid:88)
T(cid:88)

t=1

t(cid:88)
t(cid:88)

τ =1

t=1

τ =1

E(d) =

=

(cid:104)log p(xτ|xτ−1, d(xτ−1))(cid:105)q(xτ ,xτ−1,t)

q(xτ = s(cid:48)

, xτ−1 = s, t) log p(xτ = s(cid:48)

|xτ−1 = s, d(xτ−1) = d)

(7.7.34)

(7.7.35)

(7.7.36)

(7.7.37)

(7.7.38)

For each given state s we now attempt to ﬁnd the optimal decision d, which corresponds to maximising

(cid:41)

q(xτ = s(cid:48)

, xτ−1 = s, t)

(cid:40) T(cid:88)

t(cid:88)

t=1

τ =1

ˆE(d|s) =(cid:88)
t(cid:88)
T(cid:88)

s(cid:48)

q(s(cid:48)

|s) ∝

t=1

τ =1

Deﬁning

q(xτ = s(cid:48)

, xτ−1 = s, t)

log p(s(cid:48)

|s, d)

(7.7.39)

(7.7.40)

we see that for given s, up to a constant, ˆE(d|s) is the Kullback-Leibler divergence between q(s(cid:48)
p(s(cid:48)
aligned with q(s(cid:48)

|s, d) so that the optimal decision d is given by the index of the distribution p(s(cid:48)
|s):
d∗(s) = argmin

KL(cid:0)q(s(cid:48)

|s) and
|s, d) most closely

(7.7.41)

|s, d)(cid:1)

|s)|p(s(cid:48)

d

128

DRAFT March 9, 2010

Further Topics

d1

h1

v1

d2

h2

v2

u2

d3

h3

v3

u3

h4

v4

u4

Figure 7.12: An example Partially Observable
Markov Decision Process (POMDP). The ‘hidden’
variables h are never observed.
In solving the In-
ﬂuence Diagram we are required to ﬁrst sum over
variables that are never observed; doing so will cou-
ple together all past observed variables and decisions
that means any decision at time t will depend on all
previous decisions. Note that the no-forgetting prin-
ciple means that we do not need to explicitly write
that each decision depends on all previous observa-
tions – this is implicitly assumed.

The E-step concerns the computation of the marginal distributions required in the M-step. The optimal
q distribution is proportional to ˆp evaluated at the previous decision function d:

t(cid:89)

τ =1

q(x1:t, t) ∝ ut(xt)

p(xτ|xτ−1, d(xτ−1))

(7.7.42)

(7.7.43)

(7.7.44)

For a constant discount factor γ at each time-step and an otherwise stationary utility6

ut(xt) = γtu(xt)

using this

q(x1:t, t) ∝ γtu(xt)

t(cid:89)

τ =1

p(xτ|xτ−1, d(xτ−1))

For each t this is a simple Markov chain for which the pairwise transition marginals required for the
M-step, equation (7.7.40) are straightforward. This requires inference in a series of Markov models
of diﬀerent lengths. This can be done eﬃciently using a single forward and backward pass[279]. See
MDPemDeterministicPolicy.m which also deals with the more general case of utilities dependent on the
decision(action) as well as the state.

Note that this EM algorithm formally fails in the case of a deterministic environment (the transition
p(xt|xt−1, dt−1) is deterministic) – see exercise(75) for an explanation and exercise(76) for a possible
resolution.

7.8 Further Topics

7.8.1 Partially observable MDPs

In a POMDP there are states that are not observed. This seemingly innocuous extension of the MDP
case can lead however to computational diﬃculties. Let’s consider the situation in ﬁg(7.12), and attempt
to compute the optimal expected utility based on the sequence of summations and maximisations:

U = max
d1

max
d2

max
d3

p(h4|h3, d3)p(v3|h3)p(h3|h2, d2)p(v2|h2)p(h2|h1, d1)p(v1|h1)p(h1)

(cid:88)

v2

(cid:88)

v3

(cid:88)

h1:3

The sum over the hidden variables h1:3 couples all the decisions and observations, meaning that we no
longer have a simple chain structure for the remaining maximisations. For a POMDP of length t, this
leads to intractable problem with complexity exponential in t. An alternative view is to recognise that all
past decisions and observations v1:t, d1:t−1, can be summarised in terms of a belief in the current latent
state, p(ht|v1:t, d1:t−1). This suggests that instead of having an actual state, as in the MDP case, we need
6In the standard MDP framework it is more common to deﬁne ut(xt) = γt−1u(xt) so that for comparison with the standard

Policy/Value routines one needs to divide the expected utility by γ.

DRAFT March 9, 2010

129

Further Topics

to use a distribution over states to represent our current knowledge. One can therefore write down an
eﬀective MDP albeit over belief distributions, as opposed to ﬁnite states. Approximate techniques are
required to solve the resulting ‘inﬁnite’ state MDPs, and the reader is referred to more specialised texts
for a study of approximation procedures. See for example [148, 151].

7.8.2 Restricted utility functions

An alternative to solving MDPs is to consider restricted utilities such that the policy can be found easily.
Recently eﬃcient solutions have been developed for classes of MDPs with utilities restricted to Kullback-
Leibler divergences [152, 278].

7.8.3 Reinforcement learning

Reinforcement Learning deals mainly with stationary Markov Decision Processes. The added twist is that
the transition p(s(cid:48)
|s, d) (and possibly the utility) is unknown. Initially an ‘agent’ begins to explore the set
of states and utilities (rewards) associated with taking decisions. The set of accessible states and their
rewards populates as the agent traverses its environment. Consider for example a maze problem with a
given start and goal state, though with an unknown maze structure. The task is to get from the start
to the goal in the minimum number of moves on the maze. Clearly there is a balance required between
curiosity and acting to maximise the expected reward. If we are too curious (don’t take optimal decisions
given the currently available information about the maze structure) and continue exploring the possible
maze routes, this may be bad. On the other hand, if we don’t explore the possible maze states, we might
never realise that there is a much more optimal short-cut to follow than that based on our current knowl-
edge. This exploration-exploitation tradeoﬀ is central to the diﬃculties of RL. See [267] for an extensive
discussion of reinforcement learning.

For a given set of environment data X (observed transitions and utilities) one aspect of RL problem can
be considered as ﬁnding the policy that maximises expected reward, given only a prior belief about the
environment and observed decisions and states. If we assume we know the utility function but not the
transition, we may write

U(π|X ) = (cid:104)U(π|θ)(cid:105)p(θ|X )

where θ represents the environment state transition,

θ = p(xt+1|xt, dt)

Given a set of observed states and decisions,

p(θ|X ) ∝ p(X|θ)p(θ)

(7.8.1)

(7.8.2)

(7.8.3)

where p(θ) is a prior on the transition. Similar techniques to the EM style training can be carried through
in this case as well[77, 279]. Rather than the policy being a function of the state and the environment
θ, optimally one needs to consider a policy p(dt|xt, b(θ)) as a function of the state and the belief in
the environment. This means that, for example, if the belief in the environment has high entropy, the
agent can recognise this and explicitly carry out decisions/actions to explore the environment. A further
complication in RL is that the data collected X depends on the policy π. If we write t for an ‘episode’ in
which policy πt is followed and data Xt collected, then the utility of the policy π given all the historical
information is

U(π|π1:t,X1:t) = (cid:104)U(π|θ)(cid:105)p(θ|X1:t,π1:t)

(7.8.4)

Depending on the priors on the environment, and also on how long each episode is, we will have diﬀerent
posteriors for the environment parameters. If we then set

πt+1 = argmax

π

U(π|π1:t,X1:t)

(7.8.5)

this aﬀects the data we collect at the next episode Xt+1. In this way, the trajectory of policies π1, π2, . . .
can be very diﬀerent depending on these episode lengths and priors.

130

DRAFT March 9, 2010

Code

7.9 Code

7.9.1 Sum/Max under a partial order

maxsumpot.m: Generalised elimination operation according to a partial ordering
sumpotID.m: Sum/max an ID with probability and decision potentials
demoDecParty.m: Demo of summing/maxing an ID

7.9.2 Junction trees for inﬂuence diagrams

There is no need to specify the information links provided that a partial ordering is given. In the code
jtreeID.m no check is made that the partial ordering is consistent with the inﬂuence diagram. In this
case, the ﬁrst step of the junction tree formulation in section(7.4.2) is not required. Also the moralisation
and removal of utility nodes is easily dealt with by deﬁning utility potentials and including them in the
moralisation process.

The strong triangulation is found by a simple variable elimination scheme which seeks to eliminate a
variable with the least number of neighbours, provided that the variable may be eliminated according to
the speciﬁed partial ordering.

The junction tree is constructed based only on the elimination clique sequence C1, . . . ,CN . obtained from
the triangulation routine. The junction tree is then obtained by connecting a clique CI to the ﬁrst clique
j > i that is connected to this clique. Clique Ci is then eliminated from the graph. In this manner a
junction tree of connected cliques is formed. We do not require the separators for the inﬂuence diagram
absorption since these can be computed and discarded on the ﬂy.

Note that the code only computes messages from the leaves to the root of the junction tree, which is
suﬃcient for taking decisions at the root. If one desires an optimal decision at a non-root, one would need
to absorb probabilities into a clique which contains the decision required. These extra forward probability
absorptions are required because information about any unobserved variables can be aﬀected by decisions
and observations in the past. This extra forward probability schedule is not given in the code and left as
an exercise for the interested reader.
jtreeID.m: Junction Tree for an Inﬂuence Diagram
absorptionID.m: Absorption on an Inﬂuence Diagram
triangulatePorder.m: Triangulation based on a partial ordering
demoDecPhD.m: Demo for utility of Doing PhD and Startup

7.9.3 Party-Friend example

The code below implements the Party-Friend example in the text. To deal with the asymmetry the V isit
utility is zero if P arty is in state yes.
demoDecPartyFriend.m: Demo for Party-Friend

7.9.4 Chest Clinic with Decisions

The table for the Chest Clinic Decision network, ﬁg(7.13) is taken from exercise(24), see [119, 69]. There
If an x-ray is taken, then information about x is
is a slight modiﬁcation however to the p(x|e) table.
available. However, if the decision is not to take an x-ray no information about x is available. This is a
form of asymmetry. A straightforward approach in this case is to make dx a parent of the x variable and

DRAFT March 9, 2010

131

a

t

x

dx

Ux

l

e

s

d

b

dh

Uh

Code

s = Smoking
x = Positive X-ray
d = Dyspnea (Shortness of breath)
e = Either Tuberculosis or Lung Cancer
t = Tuberculosis
l = Lung Cancer
b = Bronchitis
a = Visited Asia
dh = Hospitalise?
dx = Take X-ray?

Figure 7.13: Inﬂuence Diagram for the ‘Chest Clinic’ Decision example.

set the distribution of x to be uninformative if dx = fa.

p(a = tr) = 0.01
p(s = tr) = 0.5
p(t = tr|a = tr) = 0.05
p(t = tr|a = fa) = 0.01
p(l = tr|s = tr) = 0.1
p(l = tr|s = fa) = 0.01
p(b = tr|s = tr) = 0.6
p(b = tr|s = fa) = 0.3
p(x = tr|e = tr, dx = tr) = 0.98 p(x = tr|e = fa, dx = tr) = 0.05
p(x = tr|e = fa, dx = fa) = 0.5
p(x = tr|e = tr, dx = fa) = 0.5
p(d = tr|e = tr, b = tr) = 0.9
p(d = tr|e = tr, b = fa) = 0.3
p(d = tr|e = fa, b = tr) = 0.2
p(d = tr|e = fa, b = fa) = 0.1

(7.9.1)

The two utilities are designed to reﬂect the costs and beneﬁts of taking an x-ray and hospitalising a patient:

l = tr
180
t = tr
dh = tr
l = fa 120
t = tr
dh = tr
t = fa l = tr
160
dh = tr
t = fa l = fa 15
dh = tr
l = tr
2
dh = fa t = tr
l = fa 4
dh = fa t = tr
0
dh = fa t = fa l = tr
dh = fa t = fa l = fa 40

(7.9.2)

t = tr
0
dx = tr
t = fa 1
dx = tr
dx = fa t = tr
10
dx = fa t = fa 10

(7.9.3)

We assume that we know whether or not the patient has been to Asia, before deciding on taking an x-ray.
The partial ordering is then

a ≺ dx ≺ {d, x} ≺ dh ≺ {b, e, l, s, t}

The demo demoDecAsia.m produces the results:

utility table:
asia = yes takexray = yes 49.976202
takexray = yes 46.989441
asia = no
48.433043
asia = yes takexray = no
asia = no
takexray = no
47.460900

(7.9.4)

which shows that optimally one should take an x-ray only if the patient has been to Asia.
demoDecAsia.m: Junction Tree Inﬂuence Diagram demo

132

DRAFT March 9, 2010

Exercises

7.9.5 Markov decision processes

In demoMDP.m we consider a simple two dimensional grid in which an ‘agent’ can move to a grid square
either above, below, left, right of the current square, or stay in the current square. We deﬁned goal states
(grid squares) that have high utility, with others having zero utility.
demoMDPclean.m: Demo of Value and Policy Iteration for a simple MDP
MDPsolve.m: MDP solver using Value or Policy Iteration

MDP solver using EM and assuming a deterministic policy

The following code7 is not fully documented in the text, although the method is reasonably straightforward
and follows that described in section(7.7.3). The inference is carried out using a simple α−β style recursion.
This could also be implemented using the general Factor Graph code, but was coded explicitly for reasons
of speed. The code also handles the more general case of utilities (rewards) as a function of both the state
and the action u(xt, dt).
MDPemDeterministicPolicy.m: MDP solver using EM and assuming a deterministic policy
EMqTranMarginal.m: Marginal information required for the transition term of the energy
EMqUtilMarginal.m: Marginal information required for the utility term of the energy
EMTotalBetaMessage.m: Backward information required for inference in the MDP
EMminimizeKL.m: Find the optimal decision
EMvalueTable.m: Return the expected value of the policy

7.10 Exercises

Exercise 67. You play a game in which you have a probability p of winning. If you win the game you
gain an amount £S and if you lose the game you lose an amount £S. Show that the expected gain from
playing the game is £(2p − 1)S.
Exercise 68. It is suggested that the utility of money is based, not on the amount, but rather how much
we have relative to other peoples. Assume a distribution p(i), i = 1, . . . , 10 of incomes using a histogram
with 10 bins, each bin representing an income range. Use a histogram to roughly reﬂect the distribution
of incomes in society, namely that most incomes are around the average with few very wealthy and few
extremely poor people. Now deﬁne the utility of an income x as the chance that income x will be higher
than a randomly chosen income y (under the distribution you deﬁned) and relate this to the cumulative
distribution of p. Write a program to compute this probability and plot the resulting utility as a function
of income. Now repeat the coin tossing bet of section(7.1.1) so that if one wins the bet one’s new income
will be placed in the top histogram bin, whilst if one loses one’s new income is in the lowest bin. Compare
the optimal expect utility decisions under the situations in which one’s original income is (i) average, and
(ii) much higher than average.

Exercise 69.

Derive a partial ordering for the ID on the right, and
explain how this ID diﬀers from that of ﬁg(7.5).

T est

Oil

U2

U1

Seismic

Drill

Exercise 70. This question follows closely demoMDP.m, and represents a problem in which a pilot wishes
to land an airplane.

The matrix U(x, y) in the ﬁle airplane.mat contains the utilities of being in position x, y and is a very
crude model of a runway and taxiing area.

7Thanks to Tom Furmston for coding this.

DRAFT March 9, 2010

133

Exercises

The airspace is represented by an 18×15 grid (Gx = 18, Gy = 15 in the notation employed in demoMDP.m).
The matrix U(8, 4) = 2 represents that position (8, 4) is the desired parking bay of the airplane (the vertical
height of the airplane is not taken in to account). The positive values in U represent runway and areas
where the airplane is allowed. Zero utilities represent neutral positions. The negative values represent
unfavourable positions for the airplane. By examining the matrix U you will see that the airplane should
preferably not veer oﬀ the runway, and also should avoid two small villages close to the airport.

At each timestep the plane can perform one of the following actions stay up down left right:
For stay, the airplane stays in the same x, y position.
For up, the airplane moves to the x, y + 1 position.
For down, the airplane moves to the x, y − 1 position.
For left, the airplane moves to the x − 1, y position.
For right, the airplane moves to the x + 1, y position.

A move that takes the airplane out of the airspace is not allowed.

1. The airplane begins in at point x = 1, y = 13. Assuming that an action deterministically results in
the intended grid move, ﬁnd the optimal xt, yt sequence for times t = 1, . . . , for the position of the
aircraft.

2. The pilot tells you that there is a fault with the airplane. When the pilot instructs the plane to go
right with probability 0.1 it actually goes up (provided this remains in the airspace). Assuming again
that the airplane begins at point x = 1, y = 13, return the optimal xt, yt sequence for times t = 1, . . . ,
for the position of the aircraft.

Exercise 71.

The inﬂuence diagram depicted describes the ﬁrst stage of a game. The decision
variable dom(d1) = {play, not play}, indicates the decision to either play the ﬁrst stage
or not. If you decide to play, there is a cost c1(play) = C1, but no cost otherwise,
c1(no play) = 0. The variable x1 describes if you win or lose the game, dom(x1) =
{win, lose}, with probabilities:

p(x1 = win|d1 = play) = p1,
The utility of winning/losing is

p(x1 = win|d1 = no play) = 0

(7.10.1)

u1(x1 = win) = W1,

u1(x1 = lose) = 0

(7.10.2)

c1

d1

x1

u1

Show that the expected utility gain of playing this game is

U(d1 = play) = p1W1 − C1

(7.10.3)

If you win the
Exercise 72. Exercise(71) above describes the ﬁrst stage of a new two-stage game.
ﬁrst stage x1 = win, you have to make a decision d2 as to whether or not play in the second stage
dom(d2) = {play, not play}. If you do not win the ﬁrst stage, you cannot enter the second stage.
If you decide to play the second stage, you win with probability p2:

p(x2 = win|x1 = win, d2 = play) = p2

If you decide not to play the second stage there is no chance to win:

p(x2 = win|x1 = win, d2 = not play) = 0

The cost of playing the second stage is

c2(d2 = play) = C2,

c2(d2 = no play) = 0

(7.10.4)

(7.10.5)

(7.10.6)

134

DRAFT March 9, 2010

Exercises

and the utility of winning/losing the second stage is

u2(x2 = win) = W2,

u2(x2 = lose) = 0

1. Draw an Inﬂuence Diagram that describes this two-stage game.

(7.10.7)

2. A gambler needs to decide if he should even enter the ﬁrst stage of this two-stage game. Show that

based on taking the optimal future decision d2 the expected utility based on the ﬁrst decision is:

U(d1 = play) =

(cid:26) p1(p2W2 − C2) + p1W1 − C1

p1W1 − C1

if p2W2 − C2 ≥ 0
if p2W2 − C2 ≤ 0

(7.10.8)

Exercise 73. You have £B in your bank account. You are asked if you would like to participate in a bet
in which, if you win, your bank account will become £W . However, if you lose, your bank account will
contain only £L. You win the bet with probability pw.

1. Assuming that the utility is given by the number of pounds in your bank account, write down a
formula for the expected utility of taking the bet, U(bet) and also the expected utility of not taking
the bet, U(no bet).

2. The above situation can be formulated diﬀerently. If you win the bet you gain £(W − B). If you lose
the bet you lose £(B − L). Compute the expected amount of money you gain if you bet Ugain(bet)
and if you don’t bet Ugain(no bet).

3. Show that U(bet) − U(no bet) = Ugain(bet) − Ugain(no bet).

Exercise 74. Consider the Party-Friend scenario, example(30). An alternative is to replace the link from
P arty to Uvisit by an information link from P arty to V isit with the constraint that V isit can be in state
yes only if P arty is in state no.

1. Explain how this constraint can be achieved by including an additional additive term to the utilities

and modify demoDecPartyFriend.m accordingly to demonstrate this.

2. For the case in which utilities are all positive, explain how the same constraint can be achieved using

a multiplicative factor.

Exercise 75. Consider an objective

F (θ) =(cid:88)

x

U(x)p(x|θ)

(7.10.9)

for a positive function U(x) and that our task is to maximise F with respect to θ. An Expectation-
Maximisation style bounding approach (see section(11.2)) can be derived by deﬁning the auxiliary distri-
bution

˜p(x|θ) = U(x)p(x|θ)

F (θ)

so that by considering KL(q(x)|˜p(x)) for some variational distribution q(x) we obtain the bound

log F (θ) ≥ −(cid:104)log q(x)(cid:105)q(x) + (cid:104)log U(x)(cid:105)q(x) + (cid:104)log p(x|θ)(cid:105)q(x)

The M-step states that the optimal q distribution is given by

(7.10.10)

(7.10.11)

(7.10.12)

q(x) = ˜p(x|θold)

At the E-step of the algorithm the new parameters θnew are given by maximising the ‘energy’ term

θnew = argmax

(cid:104)log p(x|θ)(cid:105)˜p(x|θold)
Show that for a deterministic distribution

θ

p(x|θ) = δ (x, f(θ))

the E-step fails, giving θnew = θold.

DRAFT March 9, 2010

(7.10.13)

(7.10.14)

135

Exercise 76. Consider an objective

F(θ) =(cid:88)

U(x)p(x|θ)

x

for a positive function U(x) and

p(x|θ) = (1 − )δ (x, f(θ)) + n(x), 0 ≤  ≤ 1

Exercises

(7.10.15)

(7.10.16)

and an arbitrary distribution n(x). Our task is to maximise F with respect to θ. As the previous exercise
showed, if we attempt an EM algorithm in the limit of a deterministic model  = 0, then no-updating
occurs and the EM algorithm fails to ﬁnd θ that optimises F0(θ).

Show that

F(θ) = (1 − )F0(θ) + 

and hence

(cid:88)

x

n(x)U(x)

F(θnew) − F(θold) = (1 − ) [F0(θnew) − F0(θold]

(7.10.17)

(7.10.18)

Show that if for  > 0 we can ﬁnd a θnew such that F(θnew) > F(θold), then necessarily F0(θnew) >
F0(θold).

Using this result, derive an EM-style algorithm that guarantees to increase F(θ) (unless we are already at
the optimum) for  > 0 and therefore guarantees to increase F0(θ). Hint: use

˜p(x|θ) = U(x)p(x|θ)

F(θ)

and consider

KL(q(x)|˜p(x))

for some variational distribution q(x).

(7.10.19)

(7.10.20)

Exercise 77. The ﬁle IDjensen.mat contains probability and utility tables for the inﬂuence diagram
of ﬁg(7.8a). Using BRMLtoolbox, write a program that returns the maximal expected utility for this
ID using a strong junction tree approach, and check the result by explicit summation and maximisation.
Similarly, your program should output the maximal expected utility for both states of d1, and check that
the computation using the strong junction tree agrees with the result from explicit elimination summation
and maximisation.

Exercise 78. For a POMDP, explain the structure of the strong junction tree, and relate this to the
complexity of inference in the POMDP.

Exercise 79.

(i) Deﬁne a partial order for the ID diagram depicted. (ii) Draw a (strong)
junction tree for this ID.

i

b

f

u2

d

g

a

e

u1

c

h

136

DRAFT March 9, 2010

Part II

Learning in Probabilistic Models

137

CHAPTER 8

Statistics for Machine Learning:

8.1 Distributions

Deﬁnition 54 (Cumulative distribution function). For a univariate distribution p(x), the CDF is deﬁned
as

cdf(y) ≡ p(x ≤ y) = (cid:104)I [x ≤ y](cid:105)p(x)

For an unbounded domain, cdf(−∞) = 0 and cdf(∞) = 1.

8.2 Summarising distributions

(8.1.1)

Deﬁnition 55 (Mode). The mode x∗ of a distribution p(x) is the state of x at which the distribution takes
its highest value, x∗ = argmax
p(x). A distribution could have more than one node (be multi-modal). A
widespread abuse of terminology is to refer to any isolated local maximum of p(x) to be a mode.

x

Deﬁnition 56 (Averages and Expectation).

(cid:104)f(x)(cid:105)p(x)

(8.2.1)

denotes the average or expectation of f(x) with respect to the distribution p(x). A common alternative
notation is
E(x)

(8.2.2)

When the context is clear, one may drop the notational dependency on p(x). The notation

(cid:104)f(x)|y(cid:105)

(8.2.3)

is shorthand for the average of f(x) conditioned on knowing the state of variable y, i.e. the average of
f(x) with respect to the distribution p(x|y).

139

An advantage of the expectation notations is that they hold whether the distribution is over continuous
or discrete variables. In the discrete case

Summarising distributions

(cid:88)
(cid:90) ∞

x

(cid:104)f(x)(cid:105) ≡

f(x = x)p(x = x)

and for continuous variables,

(cid:104)f(x)(cid:105) ≡

−∞

f(x)p(x)dx

(8.2.4)

(8.2.5)

(8.2.6)

(8.2.7)

(8.2.8)

(8.2.9)

The reader might wonder what (cid:104)x(cid:105) means when x is discrete.
if dom(x) =
{apple, orange, pear}, with associated probabilities p(x) for each of the states, what does (cid:104)x(cid:105) refer
to? Clearly, (cid:104)f(x)(cid:105) makes sense if f(x = x) maps the state x to a numerical value. For example
f(x = apple) = 1, f(x = orange) = 2, f(x = pear) = 3 for which (cid:104)f(x)(cid:105) is meaningful. Unless the
states of the discrete variable are associated with a numerical value, then (cid:104)x(cid:105) has no meaning.

For example,

Deﬁnition 57 (Moments). The kth moment of a distribution is given by the average of xk under the
distribution:

For k = 1, we have the mean, typically denoted by µ,

µ ≡ (cid:104)x(cid:105)

Deﬁnition 58 (Variance and Correlation).

(cid:68)

xk(cid:69)

p(x)

p(x)

(x − (cid:104)x(cid:105))2(cid:69)
(cid:68)
(cid:10)x2(cid:11)

− (cid:104)x(cid:105)2

σ2 ≡

σ2 ≡

The square root of the variance, σ is called the standard deviation. The notation var(x) is also used to
emphasise for which variable the variance is computed. The reader may show that an equivalent expression
is

For a multivariate distribution the matrix with elements

Σij = (cid:104)(xi − µi) (xj − µj)(cid:105)

(8.2.10)
where µi = (cid:104)xi(cid:105) is called the covariance matrix. The diagonal entries of the covariance matrix contain the
variance of each variable. An equivalent expression is

The correlation matrix has elements

Σij = (cid:104)xixj(cid:105) − (cid:104)xi(cid:105)(cid:104)xj(cid:105)

(cid:28)(xi − µi)

σi

ρij =

(cid:29)

(xj − µj)

σj

(8.2.11)

(8.2.12)

where σi is the deviation of variable xi. The correlation is a normalised form of the covariance so that
each element is bounded −1 ≤ ρij ≤ 1.

140

DRAFT March 9, 2010

Summarising distributions

For independent variables xi and xj, xi⊥⊥ xj|∅ the covariance Σij is zero. Similarly independent variables
have zero correlation – they are ‘uncorrelated’. Note however that the converse is not generally true – two
variables can be uncorrelated but dependent. A special case is for when xi and xj are Gaussian distributed
then independence is equivalent to being uncorrelated, see exercise(81).

Deﬁnition 59 (Skewness and Kurtosis). The skewness is a measure of the asymmetry of a distribution:

(cid:68)

(x − (cid:104)x(cid:105))3(cid:69)

σ3

p(x)

γ1 ≡

(cid:68)

(x − (cid:104)x(cid:105))4(cid:69)

σ4

p(x)

γ2 ≡

where σ2 is the variance of x with respect to p(x). A positive skewness means the distribution has a
heavy tail to the right. Similarly, a negative skewness means the distribution has a heavy tail to the left.

The kurtosis is a measure of how peaked around the mean a distribution is:

(8.2.13)

− 3

(8.2.14)

A distribution with positive kurtosis has more mass around its mean than would a Gaussian with the same
mean and variance. These are also called super Gaussian. Similarly a negative kurtosis (sub Gaussian)
distribution has less mass around its mean than the corresponding Gaussian. The kurtosis is deﬁned such
that a Gaussian has zero kurtosis (which accounts for the -3 term in the deﬁnition).

Deﬁnition 60 (Empirical Distribution). For a set of datapoints x1, . . . , xN , which are states of a random
variable x, the empirical distribution has probability mass distributed evenly over the datapoints, and
zero elsewhere.

For a discrete variable x the empirical distribution is

N(cid:88)

n=1

N(cid:88)

n=1

p(x) =

1
N

I [x = xn]

where N is the number of datapoints.

For a continuous distribution we have

p(x) =

1
N

δ (x − xn)

where δ (x) is the Dirac Delta function.

The sample mean of the datapoints is given by the

and the sample variance is given by the

ˆµ =

1
N

xn

N(cid:88)
N(cid:88)

n=1

n=1

ˆσ2 =

1
N

(xn − ˆµ)2

DRAFT March 9, 2010

(8.2.15)

(8.2.16)

(8.2.17)

(8.2.18)

141

Summarising distributions

For vectors the sample mean vector has elements

N(cid:88)

n=1

xn
i

ˆµi =

1
N

and sample covariance matrix has elements

N(cid:88)

n=1

i − ˆµi)(cid:0)xn

(xn

j − ˆµj

(cid:1)

ˆΣij =

1
N

Deﬁnition 61 (Delta function). For continuous x, we deﬁne the Dirac delta function

which is zero everywhere expect at x0, where there is a spike. (cid:82) ∞

δ(x − x0)
(cid:90) ∞

δ(x − x0)f(x)dx = f(x0)

−∞

−∞ δ(x − x0)dx = 1 and

One can view the Dirac delta function as an inﬁnitely narrow Gaussian: δ(x − x0) = limσ→0 N
The Kronecker delta,

(8.2.19)

(8.2.20)

(8.2.21)

(8.2.22)

(cid:0)x x0, σ2(cid:1).

δx,x0

(8.2.23)
is similarly zero everywhere, except for δx0,x0 = 1. The Kronecker delta is equivalent to δx,x0 = I [x = x0].
We use the expression δ (x, x0) to denote either the Dirac or Kronecker delta, depending on the context.

8.2.1 Estimator bias

Deﬁnition 62 (Unbiased estimator). Given data X = x1, . . . , xN , from a distribution p(x|θ) we can use
the data X to estimate the parameter θ that was used to generate the data. The estimator is a function
of the data, which we write ˆθ(X ). For an unbiased estimator

(8.2.24)

(cid:69)

(cid:68)ˆθ(X )

p(X|θ)

= θ

(cid:69)
(cid:68) ˆψ(X )

p(x)

More generally, one can consider any estimating function ˆψ(X ) of data. This is an unbiased estimator of
a quantity ψ if

= ψ.

1

2

3

4

Figure 8.1: Empirical distribution over a discrete variable with
4 states. The empirical samples consist of n samples at each
of states 1, 2, 4 and 2n samples at state 3 where n > 0. On
normalising this gives a distribution with values 0.2, 0.2, 0.4, 0.2
over the 4 states.

142

DRAFT March 9, 2010

Discrete Distributions

A classical example for estimator bias are those of the mean and variance. Let

N(cid:88)

n=1

xn

ˆµ(X ) =

1
N

This is an unbiased estimator of the mean (cid:104)x(cid:105)p(x) since

(cid:104)xn(cid:105)p(x) =

1
N

N (cid:104)x(cid:105)p(x) = (cid:104)x(cid:105)p(x)

On the other hand, consider the estimator of the variance,

N(cid:88)

n=1

1
N

(cid:104)ˆµ(X )(cid:105)p(x) =
N(cid:88)

ˆσ2(X ) =

1
N

n=1

(cid:10)ˆσ2(X )(cid:11)

p(x) =

1
N

This is biased since (omitting a few lines of algebra)

(xn − ˆµ(X ))2
N(cid:88)

(cid:68)

(xn − ˆµ(X ))2(cid:69)

n=1

= N − 1

N

σ2

(8.2.25)

(8.2.26)

(8.2.27)

(8.2.28)

8.3 Discrete Distributions

Deﬁnition 63 (Bernoulli Distribution). The Bernoulli distribution concerns a discrete binary variable x,
with dom(x) = {0, 1}. The states are not merely symbolic, but real values 0 and 1.

p(x = 1) = θ

From normalisation, it follows that p(x = 0) = 1 − θ. From this

(cid:104)x(cid:105) = 0 × p(x = 0) + 1 × p(x = 1) = θ
The variance is given by var(x) = θ (1 − θ).

(8.3.1)

(8.3.2)

Deﬁnition 64 (Categorical Distribution). The categorical distribution generalises the Bernoulli distri-
bution to more than two (symbolic) states. For a discrete variable x, with symbolic states dom(x) =
{1, . . . , C},

p(x = c) = θc,

θc = 1

(8.3.3)

(cid:88)

The Dirichlet is conjugate to the categorical distribution.

c

Deﬁnition 65 (Binomial Distribution). The Binomial describes the distribution of a discrete two-state
variable x, with dom(x) = {1, 0} where the states are symbolic. The probability that in n Bernoulli Trials
(independent samples), k ‘success’ states 1 will be observed is

(cid:18)n

k

(cid:19)
θk (1 − θ)n−k

p(y = k|θ) =

DRAFT March 9, 2010

(8.3.4)

143

where(cid:0)n

(cid:1) is the binomial coeﬃcient. The mean and variance are

k

(cid:104)y(cid:105) = nθ,

var(x) = nθ (1 − θ)

The Beta distribution is the conjugate prior for the Binomial distribution.

(8.3.5)

Continuous Distributions

Deﬁnition 66 (Multinomial Distribution). Consider a multi-state variable x, with dom(x) = {1, . . . , K},
with corresponding state probabilities θ1, . . . , θK. We then draw n samples from this distribution. The
probability of observing the state 1 y1 times, state 2 y2 times, . . . , state K yK times in the n samples is

(8.3.6)

(8.3.7)

n(cid:89)

θyi
i

n!

y1! . . . , yK!

i=1

p(y|θ) =

where n =(cid:80)n

i=1 yi.

(cid:104)yi(cid:105) = nθi, var(yi) = nθi (1 − θi) ,

(cid:104)yiyj(cid:105) − (cid:104)yi(cid:105)(cid:104)yj(cid:105) = −nθiθj (i (cid:54)= j)

The Dirichlet distribution is the conjugate prior for the multinomial distribution.

Deﬁnition 67 (Poisson Distribution). The Poisson distribution can be used to model situations in which
the expected number of events scales with the length of the interval within which the events can occur.
If λ is the expected number of events per unit interval, then the distribution of the number of events x
within an interval tλ is

p(x = k|λ) =

−λt (λt)k ,

1
k! e

k = 0, 1, 2, . . .

For a unit length interval (t = 1),

(cid:104)x(cid:105) = λ, var(x) = λ

(8.3.8)

(8.3.9)

The Poisson distribution can be derived as a limiting case of a Binomial distribution in which the success
probability scales as θ = λ/n, in the limit n → ∞.

8.4 Continuous Distributions

8.4.1 Bounded distributions

Deﬁnition 68 (Uniform distribution). For a variable x, the distribution is uniform if p(x) = const. over
the domain of the variable.

Deﬁnition 69 (Exponential Distribution). For x ≥ 0,

−λx

p(x|λ) ≡ λe

One can show that for rate λ

(cid:104)x(cid:105) =

1
λ

,

var(x) =

1
λ2

(8.4.1)

(8.4.2)

144

DRAFT March 9, 2010

Continuous Distributions

(a): Exponential
Figure 8.2:
(b): Laplace
distribution.
(double exponential) distribu-
tion.

(a)

(b)

The alternative parameterisation b = 1/λ is called the scale.

Deﬁnition 70 (Gamma Distribution).

(cid:18) x

(cid:19)α−1

Gam (x|α, β) =
(cid:90) ∞

Γ(a) =

ta−1e

−tdt

1

βΓ(γ)

β

− x
β ,

e

x ≥ 0, α > 0, β > 0

α is called the shape parameter, β is the scale parameter and

The parameters are related to the mean and variance through

0

(cid:16) µ

(cid:17)2

s

α =

,

β = s2
µ

(8.4.3)

(8.4.4)

(8.4.5)

where µ is the mean of the distribution and s is the standard deviation. The mode is given by (α − 1) β,
for α ≥ 1.
An alternative parameterisation uses the inverse scale

Gamis (x|α, b) = Gam (x|α, 1/b) ∝ xα−1e

−bx

Deﬁnition 71 (Inverse Gamma distribution).

InvGam (x|α, β) = βα

Γ (α)

−β/x

1
xα+1 e

This has mean β/(α − 1) for α > 1 and variance

β2

(α−1)2(α−2)

for α > 2.

Deﬁnition 72 (Beta Distribution).
1

p(x|α, β) = B (x|α, β) =

B(α, β) xα−1 (1 − x)β−1 , 0 ≤ x ≤ 1

where the beta function is deﬁned as

B(α, β) =

Γ(α)Γ(β)
Γ(α + β)

DRAFT March 9, 2010

(8.4.6)

(8.4.7)

(8.4.8)

(8.4.9)

145

−101234500.511.5  λ=0.2λ=0.5λ=1λ=1.5−50500.20.40.60.81  λ=0.2λ=0.5λ=1λ=1.5and Γ(x) is the gamma function. Note that the distribution can be ﬂipped by interchanging x for 1 − x,
which is equivalent to interchanging α and β.

Continuous Distributions

The mean is given by

(cid:104)x(cid:105) = α
α + β

,

var(x) =

αβ

(α + β)2 (α + β + 1)

8.4.2 Unbounded distributions

Deﬁnition 73 (Laplace (Double Exponential) Distribution).

p(x|λ) ≡ λe

− 1
b |x−µ|

For scale b

(cid:104)x(cid:105) = µ,

var(x) = 2b2

Univariate Gaussian distribution

(8.4.10)

(8.4.11)

(8.4.12)

The Gaussian distribution is an important distribution in science. It’s technical description is given in
deﬁnition(74).

Deﬁnition 74 (Univariate Gaussian Distribution).

(cid:0)x µ, σ2(cid:1)

p(x|µ, σ2) = N

1

√2πσ2

≡

2σ2 (x−µ)2
− 1

e

(8.4.13)

where µ is the mean of the distribution, and σ2 the variance. This is also called the normal distribution.

One can show that the parameters indeed correspond to

µ = (cid:104)x(cid:105)N (x µ,σ2) ,

σ2 =

N (x µ,σ2)

(cid:68)

(x − µ)2(cid:69)

For µ = 0 and σ = 1, the Gaussian is called the standard normal distribution.

Deﬁnition 75 (Student’s t-distribution).

p(x|µ, λ, ν) =

Γ( ν+1
2 )
Γ( ν
2 )

1 + λ (x − µ)2

ν

(cid:34)

(cid:19) 1

2

(cid:18) λ

νπ

(cid:35)− ν+1

2

(8.4.14)

(8.4.15)

(a)

(b)

146

DRAFT March 9, 2010

Figure 8.3: (a): Gamma distri-
bution with varying β for ﬁxed
(b): Gamma distribution
α.
with varying α for ﬁxed β.

012345012345  α=1 β=0.2α=2 β=0.2α=5 β=0.2α=10 β=0.2012345012345  α=2 β=0.1α=2 β=0.5α=2 β=1α=2 β=2Multivariate Distributions

Figure 8.4: Top: 200 datapoints x1, . . . , x200 drawn from a
Gaussian distribution. Each vertical line denotes a datapoint
at the corresponding x value on the horizontal axis. Middle:
Histogram using 10 equally spaced bins of the datapoints. Bot-
tom: Gaussian distribution N (x µ = 5, σ = 3) from which the
In the limit of an inﬁnite amount of
datapoints were drawn.
data, and limitingly small bin size, the normalised histogram
tends to the Gaussian probability density function.

where µ is the mean, ν the degrees of freedom, and λ scales the distribution. The variance is given by

var(x) =

ν

λ (ν − 2) , for ν > 2

(8.4.16)

For ν → ∞ the distribution tends to a Gaussian with mean µ and variance 1/λ. As ν decreases the tails
of the distribution become fatter.

The t-distribution can be derived from a scaled mixture

p(x|a, b) =
=

(cid:90) ∞
(cid:16) τ

τ =0 N

(cid:17) 1

2

(cid:0)x µ, τ
(cid:90) ∞

2π
= ba
Γ(a)

τ =0
Γ(a + 1
2)
√2π

2 (x−µ)2
− τ
e

−1(cid:1) Gamis (τ|a, b) dτ
2 (x − µ)2(cid:17)a+ 1
(cid:16)

b + 1

bae

−bτ τ a−1 1

1

2

Γ(a) dτ

It is conventional to reparameterise using ν = 2a and λ = a/b.

8.5 Multivariate Distributions

(8.4.17)

(8.4.18)

(8.4.19)

Deﬁnition 76 (Dirichlet Distribution). The Dirichlet distribution is a distribution on probability distri-
butions:

(8.5.1)

(8.5.2)

(8.5.3)

147

(cid:32) Q(cid:88)

(cid:33) Q(cid:89)

q=1

uq−1
α
q

p(α) =

where

Z(u) =

1
Z(u) δ

i=1

(cid:81)Q
(cid:16)(cid:80)Q

q=1 Γ(uq)
q=1 uq

Γ

αi − 1
(cid:17)

It is conventional to denote the distribution as

Dirichlet (α|u)

DRAFT March 9, 2010

−5051015−50510150102030−505101500.10.2Multivariate Gaussian

(a)

(b)

(c)

(d)

Figure 8.5: Dirichlet distribution with parameter (u1, u2, u3) displayed on the simplex x1, x2, x3 ≥ 0, x1 +
x2 + x3 = 1. Black denotes low probability and white high probability. (a): (3, 3, 3) (b): (0.1, 1, 1). (c):
(4, 3, 2). (d): (0.05, 0.05, 0.05).

The parameter u controls how strongly the mass of the distribution is pushed to the corners of the
simplex. Setting uq = 1 for all q corresponds to a uniform distribution, ﬁg(8.5). In the binary case Q = 2,
this is equivalent to a Beta distribution.

The product of two Dirichlet distributions is

Marginal of a Dirichlet:

Dirichlet (θ|u1) Dirichlet (θ|u2) = Dirichlet (θ|u1 + u2)
(cid:90)

(cid:1)

θj

Dirichlet (θ|u) = Dirichlet(cid:0)θ\j|u\j
θi|ui,

(cid:88)



uj

j(cid:54)=i

p(θi) = B

The marginal of a single component θi is a Beta distribution:

(8.5.4)

(8.5.5)

(8.5.6)

8.6 Multivariate Gaussian

The multivariate Gaussian plays a central role throughout this book and as such we discuss its properties
in some detail.

Deﬁnition 77 (Multivariate Gaussian Distribution).

p(x|µ, Σ) = N (x µ, Σ) ≡

1(cid:112)det (2πΣ)

− 1
2 (x−µ)TΣ−1(x−µ)

e

(8.6.1)

where µ is the mean vector of the distribution, and Σ the covariance matrix. The inverse covariance Σ−1
is called the precision.

One may show

µ = (cid:104)x(cid:105)N (x µ,Σ) ,

Σ =

(cid:68)

(x − µ) (x − µ)T(cid:69)

N (x µ,Σ)

(8.6.2)

148

DRAFT March 9, 2010

00.5100.5100.51x3x2x100.5100.5100.51x3x2x100.5100.5100.51x3x2x100.5100.5100.51x3x2x1Multivariate Gaussian

(a)

(b)

Figure 8.6: (a): Bivariate Gaussian with mean (0, 0) and covariance [1, 0.5; 0.5, 1.75]. Plotted on the
(b): Probability density contours for the same
vertical axis is the probability density value p(x).
bivariate Gaussian. Plotted are the unit eigenvectors scaled by the square root of their eigenvalues, √λi.

The multivariate Gaussian is given in deﬁnition(77). Note that det (ρM) = ρDdet (M), where M is a D×D
matrix, which explains the dimension independent notation in the normalisation constant of deﬁnition(77).

The moment representation uses µ and Σ to parameterise the Gaussian. The alternative canonical repre-
sentation

p(x|b, M, c) = ce

− 1

2 xTMx+xTb

is related to the moment representation via

Σ = M−1,

µ = M−1b,

1(cid:112)det (2πΣ)

1

2 bTM−1b

= ce

(8.6.3)

(8.6.4)

The multivariate Gaussian is widely used and it is instructive to understand the geometric picture. This
can be obtained by view the distribution in a diﬀerent co-ordinate system. First we use that every real
symmetric matrix D × D has an eigen-decomposition

Σ = EΛET

(8.6.5)

where ETE = I and Λ = diag (λ1, . . . , λD). In the case of a covariance matrix, all the eigenvalues λi are
positive. This means that one can use the transformation

y = Λ 1

2 ET (x − µ)

so that

(x − µ)T Σ (x − µ) = (x − µ)T EΛET (x − µ) = yTy

(8.6.6)

(8.6.7)

Under this transformation, the multivariate Gaussian reduces to a product of D univariate zero-mean unit
variance Gaussians (since the Jacobian of the transformation is a constant). This means that we can view
a multivariate Gaussian as a shifted, scaled and rotated version of an isotropic Gaussian in which the
centre is given by the mean, the rotation by the eigenvectors, and the scaling by the square root of the
eigenvalues, as depicted in ﬁg(8.6b).

Isotropic means ‘same under rotation’. For any isotropic distribution, contours of equal probability are
spherical around the origin.

Some useful properties of the Gaussian are as follows:

DRAFT March 9, 2010

149

−4−2024−4−202400.020.040.060.080.10.120.14−4−3−2−101234−4−3−2−101234  0.020.040.060.080.10.12Deﬁnition 78 (Partitioned Gaussian). For a distribution N (z µ, Σ) deﬁned jointly over two vectors x
and y of potentially diﬀering dimensions,

Multivariate Gaussian

(cid:19)
(cid:18) x
(cid:18) µx

y

µy

(cid:19)

z =

µ =

with corresponding mean and partitioned covariance

(cid:18) Σxx Σxy

Σyx Σyy

(cid:19)

Σ =

where Σyx ≡ ΣT

xy. The marginal distribution is given by

p(x) = N (x µx, Σxx)

and conditional

p(x|y) = N

(cid:0)x µx + ΣxyΣ−1

yy

(cid:0)y − µy

(cid:1), Σxx − ΣxyΣ−1

yy Σyx

(cid:1)

(8.6.8)

(8.6.9)

(8.6.10)

(8.6.11)

(8.6.12)

(8.6.13)

(8.6.14)

(8.6.15)

Deﬁnition 79 (Product of two Gaussians). The product of two Gaussians is another Gaussian, with a
multiplicative factor, exercise(114):

N (x µ1, Σ1)N (x µ2, Σ2) = N (x µ, Σ)

exp

(cid:112)det (2πS)
2 (µ1 − µ2)T S−1 (µ1 − µ2)
− 1

(cid:16)

(cid:17)

where S ≡ Σ1 + Σ2 and the mean and covariance are given by

µ = Σ1S−1µ2 + Σ2S−1µ1

Σ = Σ1S−1Σ2

Deﬁnition 80 (Linear Transform of a Gaussian). For the linear transformation

y = Ax + b

where x ∼ N (x µ, Σ),

(cid:16)

y Aµ + b, AΣAT(cid:17)

y ∼ N

Deﬁnition 81 (Entropy of a Gaussian). The diﬀerential entropy of a multivariate Gaussian p(x) =
N (x µ, Σ) is

H(x) ≡ −(cid:104)log p(x)(cid:105)p(x) =

1
2

log det (2πΣ) + D
2

where D = dim x. Note that the entropy is independent of the mean µ.

(8.6.16)

150

DRAFT March 9, 2010

Multivariate Gaussian

(a)

(b)

Figure 8.7: Beta distribution. The
parameters α and β can also be wit-
ting in terms of the mean and vari-
ance,
leading to an alternative pa-
rameterisation, see exercise(94).

8.6.1 Conditioning as system reversal
For a joint distribution p(x, y), consider the conditional p(x|y). The statistics of p(x|y) can be obtained
using a linear system of the form

where

x = ←−Ay + ←−η
(cid:16)

(cid:17)
←−η ←−µ ,←−Σ

←−η ∼ N

and this reversed noise is uncorrelated with y.

(8.6.17)

(8.6.18)

To show this, we need to make the statistics of x under this linear system match those given by the
conditioning operation, (8.6.11). The mean of the linear system is given by

µx = ←−Aµy + ←−µ

and the covariances by (note that covariance of y remains unaﬀected by the system reversal)

Σxx = ←−AΣyy←−AT + ←−Σ
Σxy = ←−AΣyy

From equation (8.6.21) we have

←−A = ΣxyΣ−1

yy

which, using in equation (8.6.20), gives

Using equation (8.6.19) we similarly obtain

←−Σ = Σxx − ←−AΣyy←−AT = Σxx − ΣxyΣ−1
←−µ = µx − ←−Aµy = µx − ΣxyΣ−1

yy Σyx

(8.6.24)
This means that we can write an explicit linear system of the form equation (8.6.17) where the parameters
are given in terms of the statistics of the original system. These results are just a restatement of the
conditioning results but shows how it may be interpreted as a linear system. This is useful in deriving
results in inference with Linear Dynamical Systems.

yy µy

(8.6.19)

(8.6.20)

(8.6.21)

(8.6.22)

(8.6.23)

(8.6.25)

(8.6.26)

(8.6.27)

(8.6.28)

151

8.6.2 Completing the square

A useful technique in manipulating Gaussians is completing the square. For example, the expression

− 1
e

2 xTAx+bTx

can be transformed as follows. First we complete the square:

1
2
Hence

xTAx − bTx =

1
2

2 xTAx−bTx = N
− 1
e
− 1

(cid:90)

From this one can derive

e

2 xTAx+bTxdx =

(cid:0)x − A−1b(cid:1)T A(cid:0)x − A−1b(cid:1)
(cid:0)x A−1b, A−1(cid:1)(cid:113)
det(cid:0)2πA−1(cid:1)e
(cid:113)
det(cid:0)2πA−1(cid:1)e

2 bTA−1b

−

1

bTA−1b

1
2

1

2 bTA−1b

DRAFT March 9, 2010

00.20.40.60.8100.511.522.53  α=0.1 β=0.1α=1 β=1α=2 β=2α=5 β=500.20.40.60.8100.511.522.53  α=0.1 β=2α=1 β=2α=2 β=2α=5 β=28.6.3 Gaussian propagation

Let y be linearly related to x through

y = Mx + η

where η ∼ N (µ, Σ), and x ∼ N (µx, Σx).

Then the marginal p(y) =(cid:82)

x p(y|x)p(x) is a Gaussian

(cid:17)

(cid:16)

p(y) = N

y Mµx + µ, MΣxMT + Σ

Multivariate Gaussian

(8.6.29)

(8.6.30)

8.6.4 Whitening and centering

For a set of data x1, . . . , xN , with dim xn = D, we can transform this data to y1, . . . , yN with zero mean
using centering:

where the mean m of the data is given by

yn = xn − m
N(cid:88)

m =

1
N

xn

n=1

(8.6.31)

(8.6.32)

Furthermore, we can transform to a values z1, . . . , zN that have zero mean and unit covariance using
whitening

where the covariance S of the data is given by

zn = S− 1

2 (xn − m)
N(cid:88)

S =

1
N

n=1

(xn − m) (xn − m)T

Y =(cid:2)y1, . . . , yN(cid:3)

USVT = Y,

then

An equivalent approach is to compute the SVD decomposition of the matrix of centered datapoints

(8.6.33)

(8.6.34)

(8.6.35)

(8.6.36)

Z = √Ndiag (1/S1,1, . . . , 1/SD,D) UTY

has zero mean and unit covariance, see exercise(111).

8.6.5 Maximum likelihood training

Given a set of training data X =(cid:8)x1, . . . , xN(cid:9), drawn from a Gaussian N (x µ, Σ) with unknown mean

µ and covariance Σ, how can we ﬁnd these parameters? Assuming the data are drawn i.i.d. the log
likelihood is

N(cid:88)

n=1

log p(x|µ, Σ) = −

1
2

N(cid:88)

n=1

L(µ, Σ) ≡

152

(xn − µ)T Σ−1 (xn − µ) −

N
2

log det (2πΣ)

(8.6.37)

DRAFT March 9, 2010

Taking the partial derivative with respect to the vector µ we obtain the vector derivative

Equating to zero gives that at the optimum of the log likelihood,

The derivative of L with respect to the matrix Σ requires more work.
dependence on the covariance, and also parameterise using the inverse covariance, Σ−1,

It is convenient to isolate the

 + N

2

log det(cid:0)2πΣ−1(cid:1)

(cid:125)
(xn − µ) (xn − µ)T

(cid:123)(cid:122)

≡M

(8.6.38)

(8.6.39)

(8.6.40)

(8.6.41)

(8.6.42)

(8.6.43)

Multivariate Gaussian

Optimal µ

N(cid:88)

n=1

Σ−1 (xn − µ)

∇µL(µ, Σ) =
N(cid:88)

n=1

Σ−1xn = N µΣ−1

and therefore, optimally

N(cid:88)

n=1

xn

µ =

1
N

Optimal Σ

Using M = MT, we obtain

Σ−1

N(cid:88)
(cid:124)

n=1

1
2

M + N
2

Σ

L = −

1
2

trace

∂
∂Σ−1 L = −
N(cid:88)

Σ =

1
N

n=1

(xn − µ) (xn − µ)T

Equating the derivative to the zero matrix and solving for Σ gives

Equations (8.6.40) and (8.6.43) deﬁne the Maximum Likelihood solution mean and covariance for training
data X . Consistent with our previous results, in fact these equations simply set the parameters to their
sample statistics of the empirical distribution. That is, the mean is set to the sample mean of the data
and the covariance to the sample covariance.

8.6.6 Bayesian Inference of the mean and variance

For simplicity here we deal with the univariate case. Assuming i.i.d. data the likelihood is

p(X|µ, σ2) =

(2πσ2)N/2 e

1

− 1
2σ2

(cid:80)N
n=1(xn−µ)2

For a Bayesian treatment, we require the posterior of the parameters

p(µ, σ2|X ) ∝ p(X|µ, σ2)p(µ, σ2) = p(X|µ, σ2)p(µ|σ2)p(σ2)

(8.6.44)

(8.6.45)

Our aim is to ﬁnd conjugate priors for the mean and variance. A convenient choice for a prior on the
mean µ is that it is a Gaussian centred on µ0:

p(µ|µ0, σ2

0) =

1(cid:112)2πσ2

0

DRAFT March 9, 2010

(µ0−µ)2

− 1
2σ2
e
0

(8.6.46)

153

The posterior is then

p(µ, σ2|X ) ∝

1(cid:112)

1

(σ2)N/2 e

σ2
0

(cid:80)

(µ0−µ)2− 1
2σ2

n(xn−µ)2

p(σ2)

− 1
2σ2
0

It is convenient to write this in the form

p(µ, σ2|X ) = p(µ|σ2,X )p(σ2|X )

Multivariate Gaussian

(8.6.47)

(8.6.48)

Since equation (8.6.47) has quadratic contributions in µ in the exponent, the conditional posterior p(µ|σ2,X )
is Gaussian. To identify this Gaussian we multiply out the terms in the exponent to arrive at

We encounter a diﬃculty in attempting to ﬁnd a conjugate prior for σ2 because the term b2/a is not a
simple expression of σ2. For this reason we constrain

If we therefore use an inverse gamma distribution we will have a conjugate prior for σ2. For a Gauss-
Inverse-Gamma prior:

(cid:33)(cid:33)

(cid:32)

1
2

˜b2
˜a

˜c −

(8.6.57)

(8.6.58)

2 , β +

154

DRAFT March 9, 2010

(cid:0)aµ2 − 2bµ + c(cid:1)

σ2 , b = µ0
+ N
σ2
0
(cid:18)

(cid:80)

+

n xn
σ2

(cid:19)2

b
a

+

µ −

with

exp−

1
2

a =

1
σ2
0

Using the identity

we can write

aµ2 − 2bµ + c = a
(cid:124)
p(µ, σ2|X ) ∝ √ae

, c = µ2
0
σ2
0
(cid:19)
(cid:18)

b2
a

c −
(cid:16)

− 1

2

+(cid:88)

n

(xn)2
σ2

(cid:123)(cid:122)

− 1
2 a(µ− b
p(µ|X ,σ2)

a)2

(cid:125)

e

1
√a

(cid:124)

c− b2

a

(cid:17) 1(cid:112)
(cid:123)(cid:122)

σ2
0
p(σ2|X )

1

(cid:125)
(σ2)N/2 p(σ2)

for some ﬁxed hyperparameter γ. Deﬁning the constants

+(cid:88)

n

(xn)2

xn, ˜c = µ2
0
γ

σ2
0 ≡ γσ2

+(cid:88)
(cid:33)

n

˜a =

1
γ

+ N, ˜b = µ0
γ

we have

b2
a

=

1
σ2

c −

(cid:32)

˜b2
˜a

˜c −

Using this expression in equation (8.6.52) we obtain

e

(cid:17)

(cid:16)

˜c− ˜b2

˜a

− 1
2σ2

p(σ2)

(cid:0)σ2(cid:1)−N/2
(cid:0)µ µ0, γσ2(cid:1) InvGam(cid:0)σ2|α, β(cid:1)
(cid:32)
(cid:32)
σ2|α + N

InvGam

(cid:33)

σ2
˜a

˜b
˜a

µ

,

p(σ2|X ) ∝

p(µ, σ2) = N

p(µ, σ2|X ) = N

the posterior is also Gauss-Inverse-Gamma with

(8.6.49)

(8.6.50)

(8.6.51)

(8.6.52)

(8.6.53)

(8.6.54)

(8.6.55)

(8.6.56)

Exponential Family

8.6.7 Gauss-Gamma distribution

It is common to to use a prior on the precision, deﬁned as the inverse variance

1
σ2

λ ≡

If we then use a Gamma prior

p(λ|α, β) = Gam (λ|α, β) =

1

βαΓ(α) λα−1e

−λ/β

(cid:17)
λ|α + N/2, ˜β

The posterior will be

(cid:16)
p(λ|X , α, β) = Gam
(cid:33)

(cid:32)
˜c −

1
2

1
˜β

=

+

1
β

˜b2
˜a

where

The Gauss-Gamma prior distribution

p(µ, λ|µ0, α, β, γ) = N

p(µ, λ|X , µ0, α, β, γ) = N

(8.6.59)

(8.6.60)

(8.6.61)

(8.6.62)

(8.6.63)

(8.6.64)

is the conjugate prior for a Gaussian with unknown mean µ and precision λ.

The posterior for this prior is a Gauss-Gamma distribution with parameters

(cid:0)µ µ0, γλ

−1(cid:1) Gam (λ|α, β)
(cid:33)
(cid:16)

Gam

(cid:32)

˜b
˜a

µ

,

1
˜aλ

(cid:17)
λ|α + N/2, ˜β

The marginal p(µ|X , µ0, α, β, γ) is a Student’s t-distribution. An example of a Gauss-Gamma prior/posterior
is given in ﬁg(8.8).

The Maximum Likelihood solution is recovered in the limit of a ‘ﬂat’ (improper) prior µ0 = 0, γ → ∞, α =
1/2, β → ∞, see exercise(102). The unbiased estimators for the mean and variance are given using the
proper prior µ0 = 0, γ → ∞, α = 1, β → ∞, exercise(103).
For the multivariate case, the extension of these techniques uses a multivariate Gaussian distribution
for the conjugate prior on the mean, and an Inverse Wishart distribution for the conjugate prior on the
covariance[124].

8.7 Exponential Family

A theoretically convenient class of distributions are the exponential family, which contains many standard
distributions, including the Gaussian, Gamma, Poisson, Dirichlet, Wishart, Multinomial, Markov Random
Field.

Deﬁnition 82 (Exponential Family). For a distribution on a (possibly multidimensional) variable x
(continuous or discrete) an exponential family model is of the form

i ηi(θ)Ti(x)−ψ(θ)

(8.7.1)

θ are the parameters, Ti(x) the test statistics, and ψ (θ) is the log partition function that ensure normal-
isation

(cid:80)

p(x|θ) = h(x)e
(cid:90)

ψ (θ) = log

(cid:80)

h(x)e

x

DRAFT March 9, 2010

i ηi(θ)Ti(x)

(8.7.2)

155

Exponential Family

(a) Prior

(b) Posterior

Figure 8.8: Bayesian approach to inferring the mean and precision (inverse variance) of a Gaussian based
(a): A Gauss-Gamma prior with µ0 = 0, α = 2, β = 1, γ = 1.
on N = 10 randomly drawn datapoints.
(b): Gauss-Gamma posterior conditional on the data. For comparison, the sample mean of the data is
1.87 and Maximum Likelihood optimal variance is 1.16 (computed using the N normalisation). The 10
datapoints were drawn from a Gaussian with mean 2 and variance 1. See demoGaussBayes.m.

One can always transform the parameters to the form η (θ) = θ in which case the distribution is in
canonical form:

p(x|θ) = h(x)eθTT(x)−ψ(θ)

For example the univariate Gaussian can be written

1

√2πσ2

2σ2 (x−µ)2
− 1
e

= e

− 1

2σ2 x2+ µ

σ2 x− µ2

2σ2 − 1

2 log πσ2

Deﬁning t1(x) = x, t2(x) = −x2/2 and , θ1 = µ, θ2 = σ2, h(x) = 1, then

η1(θ) = θ1
θ2

,

η2(θ) =

1
θ2

,

ψ(θ) =

1
2

+ log πθ2

(cid:18) θ2

1
θ2

(cid:19)

(8.7.3)

(8.7.4)

(8.7.5)

Note that the parameterisation is not necessarily unique – we can for example rescale the functions Ti(x)
and inversely scale ηi by the same amount to arrive at an equivalent representation.

8.7.1 Conjugate priors

In principle, Bayesian learning for the exponential family is straightforward. In canonical form

p(x|θ) = h(x)eθTT(x)−ψ(θ)

For a prior with hyperparameters α, γ,

p(θ|α, γ) ∝ eθTα−γψ(θ)

the posterior is

p(θ|x) ∝ p(x|θ)p(θ)

∝ h(x)eθTT(x)−ψ(θ)eθTα−γψ(θ)
∝ eθT[T(x)+α]−[γ+1]ψ(θ)

(8.7.6)

(8.7.7)

(8.7.8)
(8.7.9)
(8.7.10)

so that the prior equation (8.7.7) is conjugate for the exponential family likelihood equation (8.7.6). Whilst
the likelihood is in the exponential family, the conjugate prior is not necessarily in the exponential family.

156

DRAFT March 9, 2010

The Kullback-Leibler Divergence KL(q|p)
8.8 The Kullback-Leibler Divergence KL(q|p)
The Kullback-Leibler divergence KL(q|p) measures the ‘diﬀerence’ between distributions q and p[68].

Deﬁnition 83. KL divergence For two distributions q(x) and p(x)

KL(q|p) ≡ (cid:104)log q(x) − log p(x)(cid:105)q(x) ≥ 0

(8.8.1)

where (cid:104)f(x)(cid:105)r(x) denotes average of the function f(x) with respect to the distribution r(x).

The KL divergence is ≥ 0
The KL divergence is widely used and it is therefore important to understand why the divergence is positive.

To see this, consider the following linear bound on the function log(x)

log(x) ≤ x − 1

(8.8.2)

as plotted in the ﬁgure on the right. Replacing x by p(x)/q(x) in the above bound

p(x)

q(x) − 1 ≥ log p(x)

q(x)

(8.8.3)

Since probabilities are non-negative, we can multiply both sides by q(x) to obtain

We now integrate (or sum in the case of discrete variables) both sides. Using(cid:82) p(x)dx = 1,(cid:82) q(x)dx = 1,

p(x) − q(x) ≥ q(x) log p(x) − q(x) log q(x)

(8.8.4)

1 − 1 ≥ (cid:104)log p(x) − log q(x)(cid:105)q(x)

Rearranging gives

(cid:104)log q(x) − log p(x)(cid:105)q(x) ≡ KL(q|p) ≥ 0

The KL divergence is zero if and only if the two distributions are exactly the same.

8.8.1 Entropy

For both discrete and continuous variables, the entropy is deﬁned as

H(p) ≡ −(cid:104)log p(x)(cid:105)p(x)

(8.8.5)

(8.8.6)

(8.8.7)

For continuous variables, this is also called the diﬀerential entropy, see also exercise(113). The entropy is
a measure of the uncertainty in a distribution. One way to see this is that

H(p) = −KL(p|u) + const.

(8.8.8)

where u is a uniform distribution. Since the KL(p|u) ≥ 0, the less like a uniform distribution p is, the
smaller will be the entropy. Or, vice versa, the more similar p is to a uniform distribution, the greater
will be the entropy. Since the uniform distribution contains the least information a prior about which
state p(x) is in, the entropy is therefore a measure of the a priori uncertainty in the state occupancy.
For a discrete distribution we can permute the state labels without changing the entropy. For a discrete
distribution the entropy is positive, whereas the diﬀerential entropy can be negative.

DRAFT March 9, 2010

157

1234−4−3−2−101238.9 Code

demoGaussBayes.m: Bayesian ﬁtting of a univariate Gaussian
logGaussGamma.m: Plotting routine for a Gauss-Gamma distribution

Exercises

8.10 Exercises

Exercise 80. In a public lecture, the following phrase was uttered by a Professor of Experimental Psy-
chology: ‘In a recent data survey, 90% of people claim to have above average intelligence, which is clearly
nonsense!’ [Audience Laughs]. Is it theoretically possible for 90% of people to have above average intelli-
gence? If so, give an example, otherwise explain why not. What about above median intelligence?

Exercise 81. Consider the distribution deﬁned on real variables x, y:

−x2−y2

,

p(x, y) ∝ (x2 + y2)2e

dom(x) = dom(y) = {−∞ . . .∞}

(8.10.1)
Show that (cid:104)x(cid:105) = (cid:104)y(cid:105) = 0. Furthermore show that x and y are uncorrelated, (cid:104)xy(cid:105) = (cid:104)x(cid:105)(cid:104)y(cid:105). Whilst x and
y are uncorrelated, show that they are nevertheless dependent.
Exercise 82. For a variable x with dom(x) = {0, 1}, and p(x = 1) = θ, show that in n independent draws
x1, . . . , xn from this distribution, the probability of observing k states 1 is the Binomial distribution

(cid:18)n

(cid:19)

k

I =

θk (1 − θ)n−k
(cid:90) ∞
(cid:90) ∞

− 1
e

−∞

2 x2

dx

− 1

2 x2

e

−∞

I 2 =

dx

By considering

Exercise 83 (Normalisation constant of a Gaussian). The normalisation constant of a Gaussian distri-
bution is related to the integral

(cid:90) ∞

−∞

− 1
e

2 y2

dy =

(cid:90) ∞

(cid:90) ∞

−∞

−∞

− 1
e

2 x2+y2

dxdy

(8.10.3)

(8.10.4)

(8.10.2)

(8.10.5)

(8.10.6)

and transforming to polar coordinates, show that

1. I = √2π
− 1
2σ2 (x−µ)2

2. (cid:82) ∞

−∞ e

dx = √2πσ2

Exercise 84. For a univariate Gaussian distribution, show that

(cid:90)

1. µ = (cid:104)x(cid:105)N (x µ,σ2)
2. σ2 =

N (x µ,σ2)

(cid:68)
(x − µ)2(cid:69)
Dirichlet (θ|u) = Dirichlet(cid:0)θ\j|u\j
xk(cid:69)

= B(α + k, β)
B(α, β)

(cid:68)

θj

(cid:1)

Exercise 86. For a Beta distribution, show that

Exercise 85. Show that the marginal of a Dirichlet distribution is another Dirichlet distribution:

and, using Γ(x + 1) = xΓ(x), derive an explicit expression for the kth moment of a Beta distribution.

158

DRAFT March 9, 2010

Exercises

Exercise 87. Deﬁne the moment generating function as

(cid:10)etx(cid:11)

p(x)

g(t) ≡
Show that

(cid:68)

xk(cid:69)

p(x)

lim
t→0

dk
dtk g(t) =

(8.10.7)

(8.10.8)

Exercise 88 (Change of variables). Consider a one dimensional continuous random variable x with
corresponding p(x). For a variable y = f(x), where f(x) is a monotonic function, show that the distribution
of y is

(cid:18) df

(cid:19)−1

dx

p(y) = p(x)

, x = f

−1(y)

Exercise 89 (Normalisation of a Multivariate Gaussian). Consider

(cid:90) ∞

−∞

I =

2 (x−µ)TΣ−1(x−µ)dx
− 1
e

By using the transformation

show that

z = Σ− 1

2 (x − µ)

I =(cid:112)det (2πΣ)
(cid:19)
(cid:18) A B

M =

C D

Exercise 90. Consider the partitioned matrix

(8.10.9)

(8.10.10)

(8.10.11)

(8.10.12)

(8.10.13)

for which we wish to ﬁnd the inverse M−1. We assume that A is m × m and invertible, and D is n × n

and invertible. By deﬁnition, the partitioned inverse

(cid:19)

(8.10.14)

(8.10.15)

(cid:19)

=

(cid:18) Im 0

0

In

where in the above Im is the m × m identity matrix of the same dimension as A, and 0 the zero matrix of
the same dimension as D. Using the above, derive the results

(cid:19)
(cid:18) P Q
(cid:19)(cid:18) P Q

R S

M−1 =

(cid:18) A B

C D

R S

must satisfy

P =(cid:0)A − BD−1C(cid:1)−1
Q = −A−1B(cid:0)D − CA−1B(cid:1)−1
R = −D−1C(cid:0)A − BD−1C(cid:1)−1
S =(cid:0)D − CA−1B(cid:1)−1

(cid:0)x µ, σ2(cid:1) the skewness and kurtosis are both

Exercise 91. Show that for Gaussian distribution p(x) = N
zero.

Exercise 92. Consider a small interval of time δt and let the probability of an event occurring in this
small interval be θδt. Derive a distribution that expresses the probability of at least one event in an interval
from 0 to t.

DRAFT March 9, 2010

159

Exercise 93. Consider a vector variable x = (x1, . . . , xn) and set of functions deﬁned on each component
of x, φi(xi). For example for x = (x1, x2) we might have

Exercises

(8.10.16)

(8.10.17)

(8.10.18)

φ1(x1) = −|x1|, φ2(x2) = −x2

2

Consider the distribution

1
Z

eθTφ(x)

p(x|θ) =
(cid:90) ∞

−∞

eθiφi(xi)dxi

where φ(x) is a vector function with ith component φi(xi), and θ is a parameter vector. Each component
is tractably integrable in the sense that

can be computed either analytically or to an acceptable numerical accuracy. Show that

1. xi⊥⊥ xj|∅.
2. The normalisation constant Z can be tractably computed.

3. Consider the transformation

x = My

(8.10.19)
for an invertible matrix M. Show that the distribution p(y|M, θ) is tractable (its normalisation
constant is known), and that, in general, yi(cid:62)(cid:62)yj | ∅. Explain the signiﬁcance of this is deriving
tractable multivariate distributions.

Exercise 94. Show that we may reparameterise the Beta distribution, deﬁnition(72) by writing the pa-
rameters α and β as functions of the mean m and variance s using

α = βγ,
1

β =

1 + γ

(cid:18)

(cid:19)
γ ≡ m/(1 − m)
s (1 + γ)2 − 1

γ

Exercise 95. Consider the function

f(γ + α, β, θ) ≡ θγ+α−1 (1 − θ)β−1

show that

lim
γ→0

∂
∂γ

f(γ + α, β, θ) = θα−1 (1 − θ)β−1 log θ

and hence that

(cid:90)

(cid:90)

θα−1 (1 − θ)β−1 log θdθ = lim

γ→0
Using this result, show therefore that

∂
∂γ

f(γ + α, β, θ)dθ = ∂
∂α

(cid:104)log θ(cid:105)B(θ|α,β) = ∂

∂α

log B(α, β)

where B(α, β) is the Beta function. Show additionally that

(cid:104)log (1 − θ)(cid:105)B(θ|α,β) = ∂

∂β

log B(α, β)

Using the fact that

B(α, β) =

Γ(α)Γ(β)
Γ(α + β)

(cid:90)

(8.10.20)

(8.10.21)

(8.10.22)

(8.10.23)

f(α, β, θ)dθ

(8.10.24)

(8.10.25)

(8.10.26)

(8.10.27)

where Γ(x) is the gamma function, relate the above averages to the digamma function, deﬁned as

ψ(x) = d
dx

log Γ(x)

160

(8.10.28)

DRAFT March 9, 2010

Exercises

Exercise 96. Using a similar ‘generating function’ approach as in exercise(95), explain how to compute

(cid:104)log θi(cid:105)Dirichlet(θ|u)

Exercise 97. Consider the function

θui−1

(cid:33)(cid:89)
(cid:27)

i

f(x) =

0

δ

θi − x

(cid:82) ∞
0 e−sxf(x)dx is
Show that the Laplace transform of f(x), ˜f(s) ≡
n(cid:89)

dθ1 . . . dθn

i

−sθiθui−1
e

i

dθi

=

˜f(s) =

(cid:80)

1
i ui

s

Γ (ui)

i=1

By taking the inverse Laplace transform, show that

(cid:90) ∞

(cid:32) n(cid:88)
(cid:26)(cid:90) ∞

i=1

0

i=1

n(cid:89)
(cid:81)n
Γ ((cid:80)

(cid:80)

i ui−1

i=1 Γ (ui)
i ui − 1) x

f(x) =

(cid:81)n
Γ ((cid:80)

i=1 Γ (ui)
i ui)

Hence show that the normalisation constant of a Dirichlet distribution with parameters u is given by

Exercise 98. By using the Laplace transform, as in exercise(97), show that the marginal of a Dirichlet
distribution is a Dirichlet distribution.

Exercise 99. Derive the formula for the diﬀerential entropy of a multi-variate Gaussian.
Exercise 100. Show that for a gamma distribution Gam (x|α, β) the mode is given by

x

∗ = (α − 1) β
provided that α ≥ 1.
Exercise 101. Consider a distribution p(x|θ) and a distribution with θ changed by a small amount, δ.
Take the Taylor expansion of
KL(p(x|θ)|p(x|θ + δ))
(cid:29)

for small δ and show that this is equal to

(8.10.34)

(8.10.35)

(cid:28) ∂2

∂θ2 log p(x|θ)

p(θ)

More generally for a distribution parameterised by a vector θi + δi, show that a small change in the
parameter results in

(8.10.29)

(8.10.30)

(8.10.31)

(8.10.32)

(8.10.33)

(8.10.36)

(8.10.37)

(8.10.38)

δ2
2

−

(cid:88)

i,j

δiδj
2 Fij

(cid:28) ∂2
(cid:28) ∂

where the Fisher Information matrix is deﬁned as

Fij = −

∂θi∂θj

log p(x|θ)

(cid:29)

p(θ)

(cid:29)
log p(x|θ)

Show that the Fisher information matrix is positive (semi) deﬁnite by expressing it equivalently as

Fij =

log p(x|θ) ∂

∂θj

∂θi

DRAFT March 9, 2010

p(θ)

(8.10.39)

161

Exercise 102. Consider the joint prior distribution

p(µ, λ|µ0, α, β, γ) = N

(cid:88)

n

µ∗ =

1
N

xn,

σ2∗ =

1
N

(cid:0)µ µ0, γλ

−1(cid:1) Gam (λ|α, β)
(cid:88)

(xn − µ∗)2

n

Show that for µ0 = 0, γ → ∞, β → ∞, then the prior distribution becomes ‘ﬂat’ (independent of µ and
λ) for α = 1/2. Show that for these settings the mean and variance that jointly maximise the posterior
equation (8.6.64) are given by the standard Maximum Likelihood settings

Exercises

(8.10.40)

(8.10.41)

(8.10.44)

Exercise 103. Show that in the limit µ0 = 0, γ → ∞, α = 1, β → ∞, the jointly optimal mean and
variance obtained from

argmax

µ,λ

is given by

µ∗ =

1
N

p(µ, λ|X , α, β, γ)
(cid:88)

xn,

σ2∗ =

n

(cid:88)

n

1

N + 1

(xn − µ∗)2

(8.10.42)

(8.10.43)

where σ2∗ = 1/λ∗. Note that these correspond to the standard ‘unbiased’ estimators of the mean and
variance.
Exercise 104. For the Gauss-Gamma posterior p(µ, λ|µ0, α, β,X ) given in equation (8.6.64) compute the
marginal posterior p(µ|µ0, α, β,X ). What is the mean of this distribution?
Exercise 105. Derive equation (8.6.30).
Exercise 106. Consider the multivariate Gaussian distribution p(x) ∼ N (x µ, Σ) on the vector x with
components x1, . . . , xn:

p(x) =

− 1
2 (x−µ)TΣ−1(x−µ)
e

1(cid:112)det (2πΣ)
(cid:1) and p(yi|x) ∼ N
(cid:0)x 0, σ2

0

Calculate p(xi|x1, . . . , xi−1, xi+1, . . . , xn).
Exercise 107. Observations y0, . . . , yn−1 are noisy i.i.d. measurements of an underlying variable x with
p(x) ∼ N
with mean

(cid:0)yi x, σ2(cid:1) for i = 0, . . . , n−1. Show that p(x|y0, . . . , yn−1) is Gaussian

µ = nσ2

0

nσ2

0 + σ2 y

where y = (y0 + y1 + . . . + yn−1)/n and variance σ2

n such that

1
σ2
n

= n

σ2 +

1
σ2
0

.

(8.10.45)

(8.10.46)

Exercise 108. Consider a set of data x1, . . . , xN drawn from a Gaussian with know mean µ and unknown
variance σ2. Assume a gamma distribution prior on τ = 1/σ2,

p(τ) = Gamis (τ|a, b)

1. Show that the posterior distribution is

(cid:32)
τ|a + N

2 , b +

1
2

(cid:33)

N(cid:88)

n=1

(xn − µ)2

(8.10.47)

(8.10.48)

p(τ|X ) = Gamis

162

DRAFT March 9, 2010

Exercises

2. Show that the distribution for x is

(cid:90)

p(x|X ) =

p(x|τ)p(τ|X )dτ = Student

(cid:16)

x|µ, λ = a

b

(cid:17)

, ν = 2a

(8.10.49)

Exercise 109. The Poisson distribution is a discrete distribution on the non-negative integers, with

P (x) = e−λλx
x!

x = 0, 1, 2, . . .

(8.10.50)

You are given a sample of n observations x1, . . . , xn drawn from this distribution. Determine the maximum
likelihood estimator of the Poisson parameter λ.

Exercise 110. For a Gaussian mixture model

piN (x µi, Σi) ,

pi > 0,

(cid:88)

i

show that p(x) has mean

i

p(x) =(cid:88)
(cid:104)x(cid:105) =(cid:88)
(cid:16)
and covariance(cid:88)

i

piµi

pi

Σi + µiµT
i

i

(cid:17)

(cid:88)

i

−

(cid:88)

j

piµi

pjµT
j

pi = 1

(8.10.51)

(8.10.52)

(8.10.53)

Exercise 111. Show that for the whitened data matrix, given in equation (8.6.36), ZZT = NI.

Exercise 112. Consider a uniform distribution pi = 1/N deﬁned on states i = 1, . . . , N. Show that the
entropy of this distribution is

N(cid:88)

i=1

H = −

pi log pi = log N

(8.10.54)

and that there for as the number of states N increases to inﬁnity, the entropy diverges to inﬁnity.
Exercise 113. Consider a continuous distribution p(x), x ∈ [0, 1]. We can form a discrete approximation
with probabilities pi to this continuous distribution by identifying a continuous value i/N for each state
i = 1, . . . , N. With this

(8.10.55)

(8.10.56)

(8.10.57)

(8.10.58)

163

(cid:80)

pi = p(i/N)
i p(i/N)

(cid:88)
show that the entropy H = −

1(cid:80)

i p(i/N)

(cid:80)
p(i/N) log p(i/N) + log(cid:88)

i pi log pi is given by

i

i

p(i/N)

Since for a continuous distribution

p(x)dx = 1

H = −
(cid:90) 1

0

1
N

N(cid:88)

i=1

p(i/N) = 1

DRAFT March 9, 2010

a discrete approximation of this integral into bins of size 1/N gives

Hence show that for large N,

(cid:90) 1

H ≈ −

0

p(x) log p(x)dx + const.

Exercises

(8.10.59)

where the constant tends to inﬁnity as N → ∞. Note that this result says that as a continuous distribu-
tion has essentially an inﬁnite number of states, the amount of uncertainty in the distribution is inﬁnite
(alternatively, we would need an inﬁnite number of bits to specify a continuous value). This motivates the
deﬁnition of the diﬀerential entropy, which neglects the inﬁnite constant of the limiting case of the discrete
entropy.
Exercise 114. Consider two multivariate Gaussians N (x µ1, Σ1) and N (x µ2, Σ2).

1. Show that the log product of the two Gaussians is given by

1 µ1 + Σ−1

2 µ2

1 Σ−1
µT

1 µ1 + µT

2 Σ−1

2 µ2

1
2

−

log det (2πΣ1) det (2πΣ2)

2

1 + Σ−1

(cid:1) x+xT(cid:0)Σ−1
xT(cid:0)Σ−1
(cid:0)x − A−1b(cid:1)T A(cid:0)x − A−1b(cid:1)+

1
−
2
2. Deﬁning A = Σ−1
1
2

1 + Σ−1

−

(cid:16)

1
2

−

(cid:1)
1 µ1 + Σ−1
(cid:16)

2 and b = Σ−1

log det (2πΣ1) det (2πΣ2)
Writing Σ = A−1 and µ = A−1b show that the product of Gaussians is a Gaussian with covariance

1 µ1 + µT

2 µ2

−

1
2

bTA−1b−

1
2

2 µ2 we can write the above as
1 Σ−1
µT

2 Σ−1

1
2

(cid:17)

(cid:17)

Σ = Σ1 (Σ1 + Σ2)−1 Σ2

mean

µ = Σ1 (Σ1 + Σ2)−1 µ2 + Σ2 (Σ1 + Σ2)−1 µ1

and log prefactor

1
2

bTA−1b −

1
2

(cid:16)

1 Σ−1
µT

1 µ1 + µT

2 Σ−1

2 µ2

(cid:17)

1
2

−

3. Show that this can be written as

N (x µ1, Σ1)N (x µ2, Σ2) = N (x µ, Σ)

exp

Exercise 115. Show that

∂
∂θ (cid:104)log p(x|θ)(cid:105)p(x|θ0) |θ=θ0 = 0

1
2

log det (2πΣ1) det (2πΣ2) +

(cid:16)

(cid:17)
(cid:112)det (2πS)
2 (µ1 − µ2)T S−1 (µ1 − µ2)
− 1

(8.10.60)

(8.10.61)

log det (2πΣ)

(8.10.62)

(8.10.63)

164

DRAFT March 9, 2010

CHAPTER 9

Learning as Inference

9.1 Learning as Inference

In previous chapters we largely assumed that all distributions are fully speciﬁed for the inference tasks.
In Machine Learning and related ﬁelds, however, the distributions need to be learned on the basis of data.
Learning is then the problem of integrating data with domain knowledge of the model environment.

Deﬁnition 84 (Priors and Posteriors). Priors and posteriors typically refer to the parameter distributions
before (prior to) and after (posterior to) seeing the data. Formally, Bayes’ rule relates these via

p(θ|V) = p(V|θ)p(θ)
p(V)

where θ is the parameter of interest and V represents the observed (visible) data.

(9.1.1)

9.1.1 Learning the bias of a coin

Consider data expressing the results of tossing a coin. We write vn = 1 if on toss n the coin comes
up heads, and vn = 0 if it is tails. Our aim is to estimate the probability θ that the coin will be a
head, p(vn = 1|θ) = θ – called the ‘bias’ of the coin. For a fair coin, θ = 0.5. The variables in this
environment are v1, . . . , vN and θ and we require a model of the probabilistic interaction of the variables,
p(v1, . . . , vN , θ). Assuming there is no dependence between the observed tosses, except through θ, we have
the Belief Network

N(cid:89)

n=1

p(v1, . . . , vN , θ) = p(θ)

p(vn|θ)

(9.1.2)

which is depicted in ﬁg(9.1). The assumption that each observation is identically and independently dis-
tributed is called the i.i.d. assumption.

Learning refers to using the observations v1, . . . , vN to infer θ. In this context, our interest is

p(θ|v1, . . . , vN ) = p(v1, . . . , vN , θ)

p(v1, . . . , vN )

= p(v1, . . . , vN|θ)p(θ)

p(v1, . . . , vN )

(9.1.3)

We still need to fully specify the prior p(θ). To avoid complexities resulting from continuous variables,
we’ll consider a discrete θ with only three possible states, θ ∈ {0.1, 0.5, 0.8}. Speciﬁcally, we assume

p(θ = 0.1) = 0.15,

p(θ = 0.5) = 0.8,

p(θ = 0.8) = 0.05

(9.1.4)

165

θ

v3

(a)

···

vN

v1

v2

θ

vn

(b)

N

Learning as Inference

Figure 9.1: (a): Belief Network for coin
tossing model. (b): Plate notation equiv-
alent of (a). A plate replicates the quanti-
ties inside the plate a number of times as
speciﬁed in the plate.

as shown in ﬁg(9.2a). This prior expresses that we have 80% belief that the coin is ‘fair’, 5% belief the
coin is biased to land heads (with θ = 0.8), and 15% belief the coin is biased to land tails (with θ = 0.1).
The distribution of θ given the data and our beliefs is

N(cid:89)
N(cid:89)
p(vn|θ) = p(θ)
(cid:80)N
(cid:80)N
I[vn=1] (1 − θ)

n=1

n=1

θ

p(θ|v1, . . . , vN ) ∝ p(θ)
∝ p(θ)θ
(9.1.6)
I [vn = 1] is the number of occurrences of heads, which we more conveniently denote

(9.1.5)

n=1

I[vn=1] (1 − θ)
I[vn=0]

I[vn=0]

n=1

In the above(cid:80)N
as NH. Likewise,(cid:80)N

n=1

I [vn = 0] is the number of tails, NT . Hence

n=1

p(θ|v1, . . . , vN ) ∝ p(θ)θNH (1 − θ)NT

For an experiment with NH = 2, NT = 8, the posterior distribution is

p(θ = 0.1|V) = k × 0.15 × 0.12 × 0.98 = k × 6.46 × 10−4
p(θ = 0.5|V) = k × 0.8 × 0.52 × 0.58 = k × 7.81 × 10−4
p(θ = 0.8|V) = k × 0.05 × 0.82 × 0.28 = k × 8.19 × 10−8

(9.1.7)

(9.1.8)
(9.1.9)
(9.1.10)

where V is shorthand for v1, . . . , vN . From the normalisation requirement we have 1/k = 6.46 × 10−4 +
7.81 × 10−4 + 8.19 × 10−8 = 0.0014, so that

p(θ = 0.1|V) = 0.4525,

p(θ = 0.5|V) = 0.5475,

p(θ = 0.8|V) = 0.0001

(9.1.11)

as shown in ﬁg(9.2b). These are the ‘posterior’ parameter beliefs. In this case, if we were asked to choose
a single a posteriori most likely value for θ, it would be θ = 0.5, although our conﬁdence in this is low
since the posterior belief that θ = 0.1 is also appreciable. This result is intuitive since, even though we
observed more Tails than Heads, our prior belief was that it was more likely the coin is fair.

Repeating the above with NH = 20, NT = 80, the posterior changes to

p(θ = 0.1|V) = 1−1.93×10−6,

p(θ = 0.8|V) = 2.13×10−35 (9.1.12)
ﬁg(9.1c), so that the posterior belief in θ = 0.1 dominates. This is reasonable since in this situation, there
are so many more tails than heads that this is unlikely to occur from a fair coin. Even though we a priori
thought that the coin was fair, a posteriori we have enough evidence to change our minds.

p(θ = 0.5|V) = 1.93×10−6,

0.8

0.1

0.1

0.5
θ

(a)

0.8

0.1

0.5
θ

(b)

0.8

0.5
θ

(c)

Figure 9.2: (a): Prior encoding our be-
liefs about the amount the coin is biased
(b): Posterior having seen 2
to heads.
(c): Posterior having
heads and 8 tails.
seen 20 heads and 80 tails.

166

DRAFT March 9, 2010

Learning as Inference

9.1.2 Making decisions

In itself, the Bayesian posterior merely represents our beliefs and says nothing about how best to sum-
marise these beliefs.
In situations in which decisions need to be taken under uncertainty we need to
additionally specify what the utility of any decision is, as in chapter(7).

In the coin tossing scenario where θ is assumed to be either 0.1, 0.5 or 0.8, we setup a decision problem
as follows: If we correctly state the bias of the coin we gain 10 points; being incorrect, however, loses 20
points. We can write this using

U(θ, θ0) = 10I(cid:2)θ = θ0(cid:3)

− 20I(cid:2)θ (cid:54)= θ0(cid:3)

(9.1.13)

where θ0 is the true value for the bias. The expected utility of the decision that the coin is θ = 0.1 is

U(θ = 0.1) = U(θ = 0.1, θ0 = 0.1)p(θ0 = 0.1|V)

+ U(θ = 0.1, θ0 = 0.5)p(θ0 = 0.5|V) + U(θ = 0.1, θ0 = 0.8)p(θ0 = 0.8|V)

Plugging in the numbers from equation (9.1.11), we obtain

U(θ = 0.1) = 10 × 0.4525 − 20 × 0.5475 − 20 × 0.0001 = −6.4270

Similarly

U(θ = 0.5) = 10 × 0.5475 − 20 × 0.4525 − 20 × 0.0001 = −3.5770

and

U(θ = 0.8) = 10 × 0.0001 − 20 × 0.4525 − 20 × 0.5475 = −19.999
So that the best decision is to say that the coin is unbiased, θ = 0.5.

Repeating the above calculations for NH = 20, NT = 80, we arrive at

U(θ = 0.1) = 10 × (1 − 1.93 × 10−6) − 20(cid:0)1.93 × 10−6 + 2.13 × 10−35(cid:1) = 9.9999
U(θ = 0.5) = 10 × 1.93 × 10−6 − 20(cid:0)1 − 1.93 × 10−6 + 2.13 × 10−35(cid:1)
U(θ = 0.8) = 10 × 2.13 × 10−35 − 20(cid:0)1 − 1.93 × 10−6 + 1.93 × 10−6(cid:1)

≈ −20.0
≈ −20.0

so that the best decision in this case is to choose θ = 0.1.

(9.1.14)

(9.1.15)

(9.1.16)

(9.1.17)

(9.1.18)

(9.1.19)

(9.1.20)

As more information about the distribution p(v, θ) becomes available the posterior p(θ|V) becomes in-
creasingly peaked, aiding our decision making process.

9.1.3 A continuum of parameters

In section(9.1.1) we considered only three possible values for θ. Here we discuss a continuum of parameters.

Using a ﬂat prior

We ﬁrst examine the case of a ‘ﬂat’ or uniform prior p(θ) = k for some constant k. For continuous
variables, normalisation requires

(cid:90)
(cid:90) 1

p(θ)dθ = 1

Since 0 ≤ θ ≤ 1,

p(θ)dθ = k = 1

0

DRAFT March 9, 2010

(9.1.21)

(9.1.22)

167

Learning as Inference

Figure 9.3: Posterior p(θ|V) assuming a ﬂat prior on θ. (red)
NH = 2, NT = 8 and (blue) NH = 20, NT = 80. In both cases,
the most probable state of the posterior is 0.2, which makes
intuitive sense, since the fraction of Heads to Tails in both cases
is 0.2. Where there is more data, the posterior is more certain
and sharpens around the most probable value. The Maximum
A Posteriori setting is θ = 0.2 in both cases, this being the value
of θ for which the posterior attains its highest value.

Repeating the previous calculations with this ﬂat continuous prior, we have

1
c

θNH (1 − θ)NT

p(θ|V) =
(cid:90) 1

c =

where c is a constant to be determined by normalisation,

θNH (1 − θ)NT dθ ≡ B(NH + 1, NT + 1)

0

(9.1.23)

(9.1.24)

where B(α, β) is the Beta function.

Deﬁnition 85 (conjugacy). If the posterior is of the same parametric form as the prior, then we call the
prior the conjugate distribution for the likelihood distribution.

Using a conjugate prior

Determining the normalisation constant of a continuous distribution requires that the integral of the
unnormalised posterior can be carried out. For the coin tossing case, it is clear that if the prior is of the
form of a Beta distribution, then the posterior will be of the same parametric form:

p(θ) =

1

B(α, β) θα−1 (1 − θ)β−1

the posterior is

p(θ|V) ∝ θα−1 (1 − θ)β−1 θNH (1 − θ)NT

so that

p(θ|V) =

1

B(α + NH , β + NT ) θα+NH−1 (1 − θ)β+NT −1 ≡ B(θ|α + NH , β + NT )

(9.1.25)

(9.1.26)

(9.1.27)

The prior and posterior are of the same form (both Beta distributions) but simply with diﬀerent parame-
ters. Hence the Beta distribution is ‘conjugate’ to the Binomial distribution.

9.1.4 Decisions based on continuous intervals

The result of a coin tossing experiment is NH = 2 heads and NT = 8 tails. You now need to make a deci-
sion : you win 10 dollars if your guess that the coin is more likely to come up heads than tails is correct. If
your guess is incorrect, you lose a million dollars. What is your decision? (Assume an uninformative prior).

We need two quantities, θ for our guess and θ0 for the truth. Then the utility of saying Heads is

U(θ > 0.5, θ0 > 0.5)p(θ0 > 0.5|V) + U(θ > 0.5, θ0 < 0.5)p(θ0 < 0.5|V)

168

(9.1.28)

DRAFT March 9, 2010

00.20.40.60.810510θMaximum A Posteriori and Maximum Likelihood

In the above,

(cid:90) 0.5

0

p(θ0|V)dθ0

p(θ0 < 0.5|V) =
=
B(α + NH , β + NT )
≡ I0.5(α + NH , β + NT )

1

(cid:90) 0.5

0

θα+NH−1 (1 − θ)β+NT −1 dθ

(9.1.29)

(9.1.30)

(9.1.31)

where Ix(a, b) is the regularised incomplete Beta function. For the former case of NH = 2, NT = 8, under
a ﬂat prior,

p(θ0 < 0.5|V) = I0.5(NH + 1, NT + 1) = 0.9673

Since the events are exclusive, p(θ0 ≥ 0.5|V) = 1 − 0.9673 = 0.0327.
Hence the expected utility of saying heads is more likely is

10 × 0.0327 − 1000000 × 0.9673 = −9.673 × 105.

Similarly, the utility of saying tails is more likely is

10 × 0.9673 − 1000000 × 0.0327 = −3.269 × 104.

(9.1.32)

(9.1.33)

(9.1.34)

So we are better oﬀ taking the decision that the coin is more likely to come up tails.

If we modify the above so that we lose 100 million dollars if we guess tails when in fact it as heads, the
expected utility of saying tails would be −3.27× 106 in which case we would be better of saying heads. In
this case, even though we are more conﬁdent that the coin is likely to come up tails, we would pay such a
penalty of making a mistake in saying tails, that it is fact better to say heads.

9.2 Maximum A Posteriori and Maximum Likelihood

9.2.1 Summarising the posterior

Deﬁnition 86 (Maximum Likelihood and Maximum a Posteriori). Maximum Likelihood sets parameter
θ, given data V, using
θM L = argmax

(9.2.1)

p(V|θ)

θ

Maximum A Posteriori uses that setting θ that maximises the posterior distribution of the parameter,

θM AP = argmax

θ

p(V|θ)p(θ)

where p(θ) is the prior distribution.

(9.2.2)

δ(cid:0)θ, θM AP(cid:1). In making such an approximation, potentially useful information concerning the reliability

A crude summary of the posterior is given by a distribution with all its mass in a single most likely state,

of the parameter estimate is lost.
possibilities and their associated credibilities.

In contrast the full posterior reﬂects our beliefs about the range of

One can motivate MAP from a decision theoretic perspective. If we assume a utility that is zero for all
but the correct θ,

U(θtrue, θ) = I [θtrue = θ]

DRAFT March 9, 2010

(9.2.3)

169

θa

an

θs

sn

cn

n = 1 : N

θc

(b)

a

s

c

(a)

Maximum A Posteriori and Maximum Likelihood

Figure 9.4: (a): A model for the relationship between
(b):
lung Cancer, Asbestos exposure and Smoking.
Plate notation replicating the observed n datapoints
and placing priors over the CPTs, tied across all dat-
apoints.

then the expected utility of θ is

U(θ) = (cid:88)

θtrue

I [θtrue = θ] p(θtrue|V) = p(θ|V)

(9.2.4)

This means that the maximum utility decision is to return that θ with the highest posterior value.

When a ‘ﬂat’ prior p(θ) = const. is used the MAP parameter assignment is equivalent to the Maximum
Likelihood setting

θM L = argmax

θ

p(V|θ)

(9.2.5)

The term Maximum Likelihood refers to the parameter θ for which the observed data is most likely to be
generated by the model.

Since the logarithm is a strictly increasing function, then for a positive function f(θ)

θopt = argmax

θ

f(θ) ⇔ θopt = argmax

θ

log f(θ)

(9.2.6)

so that the MAP parameters can be found either by optimising the MAP objective or, equivalently, its
logarithm,

log p(θ|V) = log p(V|θ) + log p(θ) − log p(V)

where the normalisation constant, p(V), is not a function of θ.
The log likelihood is convenient since under the i.i.d. assumption it is a summation of data terms,

log p(θ|V) =(cid:88)

n

log p(vn|θ) + log p(θ) − log p(V)

(9.2.7)

(9.2.8)

so that quantities such as derivatives of the log-likelihood w.r.t. θ are straightforward to compute.

Example 36. In the coin-tossing experiment of section(9.1.1) the ML setting is θ = 0.2 in both NH =
2, NT = 8 and NH = 20, NT = 80.

9.2.2 Maximum likelihood and the empirical distribution

Given a dataset of discrete variables X =(cid:8)x1, . . . , xN(cid:9) we deﬁne the empirical distribution as

I [x = xn]

(9.2.9)

DRAFT March 9, 2010

N(cid:88)

n=1

q(x) =

1
N

170

Maximum A Posteriori and Maximum Likelihood

a
1
1
0
0
1
0
1

s
1
0
1
1
1
0
0

c
1
0
1
0
1
0
1

Figure 9.5: A database containing information about the Asbestos expo-
sure (1 signiﬁes exposure), being a Smoker (1 signiﬁes the individual is
a smoker), and lung Cancer (1 signiﬁes the individual has lung Cancer).
Each row contains the information for an individual, so that there are 7
individuals in the database.

in the case that x is a vector of variables,

I [x = xn] =(cid:89)

I [xi = xn
i ]

(9.2.10)

i

The Kullback-Leibler divergence between the empirical distribution q(x) and a distribution p(x) is

KL(q|p) = (cid:104)log q(x)(cid:105)q(x) − (cid:104)log p(x)(cid:105)q(x)

(9.2.11)
Our interest is the functional dependence of KL(q|p) on p. Since the entropic term (cid:104)log q(x)(cid:105)q(x) is inde-
pendent of p(x) we may consider this constant and focus on the second term alone. Hence

N(cid:88)

n=1

1
N

log p(xn) + const.

(9.2.12)

KL(q|p) = −(cid:104)log p(x)(cid:105)q(x) + const. = −

We recognise(cid:80)N

n=1 log p(xn) as the log likelihood under the model p(x), assuming that the data is i.i.d.
This means that setting parameters by maximum likelihood is equivalent to setting parameters by minimis-
ing the Kullback-Leibler divergence between the empirical distribution and the parameterised distribution.
In the case that p(x) is unconstrained, the optimal choice is to set p(x) = q(x), namely the maximum
likelihood optimal distribution corresponds to the empirical distribution.

9.2.3 Maximum likelihood training of belief networks

Consider the following model of the relationship between exposure to asbestos (a), being a smoker (s) and
the incidence of lung cancer (c)
p(a, s, c) = p(c|a, s)p(a)p(s)

(9.2.13)
which is depicted in ﬁg(9.4a). Each variable is binary, dom(a) = {0, 1}, dom(s) = {0, 1}, dom(c) = {0, 1}.
We assume that there is no direct relationship between Smoking and exposure to Asbestos. This is the
kind of assumption that we may be able to elicit from medical experts. Furthermore, we assume that
we have a list of patient records, ﬁg(9.5), where each row represents a patient’s data. To learn the table
entries p(c|a, s) we can do so by counting the number of c is in state 1 for each of the 4 parental states of
a and s:

p(c = 1|a = 0, s = 0) = 0,
p(c = 1|a = 0, s = 1) = 0.5
p(c = 1|a = 1, s = 0) = 0.5 p(c = 1|a = 1, s = 1) = 1

(9.2.14)

Similarly, based on counting, p(a = 1) = 4/7, and p(s = 1) = 4/7. These three CPTs then complete the
full distribution speciﬁcation.

Setting the CPT entries in this way by counting the relative number of occurrences corresponds mathe-
matically to maximum likelihood learning under the i.i.d. assumption, as we show below.

Maximum likelihood corresponds to counting

For a BN there is a constraint on the form of p(x), namely

K(cid:89)

i=1

p(x) =

p(xi|pa (xi))

DRAFT March 9, 2010

(9.2.15)

171

Maximum A Posteriori and Maximum Likelihood

To compute the Maximum Likelihood setting of each term p(xi|pa (xi)), as shown in section(9.2.2), we can
equivalently minimise the Kullback-Leibler divergence between the empirical distribution q(x) and p(x).
For the BN p(x), and empirical distribution q(x) we have

(cid:42) K(cid:88)

i=1

(cid:43)
log p (xi|pa (xi))

K(cid:88)

i=1

+ const. = −

q(x)

KL(q|p) = −

(cid:104)log p (xi|pa (xi))(cid:105)q(xi,pa(xi)) + const.

(9.2.16)

(9.2.17)

This follows using the general result

(cid:104)f(Xi)(cid:105)q(X ) = (cid:104)f(Xi)(cid:105)q(Xi)

which says that if the function f only depends on a subset of the variables, we only need to know the
marginal distribution of this subset of variables in order to carry out the average.

Since q(x) is ﬁxed, we can add on entropic terms in q and equivalently mimimize

(cid:105)

(cid:104)

K(cid:88)
K(cid:88)

i=1

i=1

KL(q|p) =

=

(cid:104)log q(xi|pa (xi))(cid:105)q(xi,pa(xi)) − (cid:104)log p (xi|pa (xi))(cid:105)q(xi,pa(xi))

(cid:104)KL(q(xi|pa (xi))|p(xi|pa (xi)))(cid:105)q(pa(xi))

(9.2.18)

(9.2.19)

The ﬁnal line is a positive weighted sum of individual Kullback-Leibler divergences. The minimal Kullback-
Leibler setting, and that which corresponds to Maximum Likelihood, is therefore

In terms of the original data, this is

p(xi|pa (xi)) = q(xi|pa (xi))
N(cid:88)

p(xi = s|pa (xi) = t) ∝

n=1

i = s] (cid:89)

I [xn

xj∈pa(xi)

I(cid:2)xn
j = tj(cid:3)

(9.2.20)

(9.2.21)

This expression corresponds to the intuition that the table entry p(xi|pa (xi)) can be set by counting the
number of times the state {xi = s, pa (xi) = t} occurs in the dataset (where t is a vector of parental states).
The table is then given by the relative number of counts of being in state s compared to the other states
s(cid:48), for ﬁxed joint parental state t.

An alternative method to derive this intuitive result is to use Lagrange multipliers, see exercise(120). For
reader less comfortable with the above Kullback-Leibler derivation, a more direct example is given below
which makes use of the notation

(cid:93) (x1 = s1, x2 = s2, x3 = s3, . . .)

(9.2.22)

to denote the number of times that states x1 = s1, x2 = s2, x3 = s3, . . . occur together in the training
data.

Example 37. We wish to learn the table entries of the distribution p(x1, x2, x3) = p(x1|x2, x3)p(x2)p(x3).
We address here how to ﬁnd the CPT entry p(x1 = 1|x2 = 1, x3 = 0) using Maximum Likelihood. For
i.i.d. data, the contribution from p(x1|x2, x3) to the log likelihood is

(cid:88)

n

log p(xn

2 , xn
3 )

1|xn

The number of times p(x1 = 1|x2 = 1, x3 = 0) occurs in the log likelihood is (cid:93) (x1 = 1, x2 = 1, x3 = 0), the
number of such occurrences in the training set. Since (by the normalisation constraint) p(x1 = 0|x2 =

172

DRAFT March 9, 2010

Maximum A Posteriori and Maximum Likelihood

xn−1

xn

x1

x2

···

y

Figure 9.6: A variable y with a large number of
parents x1, . . . , xn requires the speciﬁcation of an
exponentially large number of entries in the con-
ditional probability p(y|x1, . . . , xn). One solution
to this diﬃculty is to parameterise the conditional,
p(y|x1, . . . , xn, θ).

1, x3 = 0) = 1 − p(x1 = 1|x2 = 1, x3 = 0), the total contribution of p(x1 = 1|x2 = 1, x3 = 0) to the log
likelihood is

(cid:93) (x1 = 1, x2 = 1, x3 = 0) log p(x1 = 1|x2 = 1, x3 = 0)

+ (cid:93) (x1 = 0, x2 = 1, x3 = 0) log (1 − p(x1 = 1|x2 = 1, x3 = 0))

Using θ ≡ p(x1 = 1|x2 = 1, x3 = 0) we have

(cid:93) (x1 = 1, x2 = 1, x3 = 0) log θ + (cid:93) (x1 = 0, x2 = 1, x3 = 0) log (1 − θ)
Diﬀerentiating the above expression w.r.t. θ and equating to zero gives

(cid:93) (x1 = 1, x2 = 1, x3 = 0)

−
The solution for optimal θ is then

θ

(cid:93) (x1 = 0, x2 = 1, x3 = 0)

= 0

1 − θ

p(x1 = 1|x2 = 1, x3 = 0) =

(cid:93) (x1 = 1, x2 = 1, x3 = 0)

(cid:93) (x1 = 1, x2 = 1, x3 = 0) + (cid:93) (x1 = 0, x2 = 1, x3 = 0) ,

corresponding to the intuitive counting procedure.

(9.2.23)

(9.2.24)

(9.2.25)

(9.2.26)

Conditional probability functions

Consider a binary variable y with n binary parental variables, x = (x1, . . . , xn). There are 2n entries in
the CPT of p(y|x) so that it is infeasible to explicitly store these entries for even moderate values of n. To
reduce the complexity of this CPT we may constrain the form of the table. For example, one could use a
function

p(y = 1|x, w) =

1

1 + e−wTx

where we only need to specify the n-dimensional parameter vector w.

(9.2.27)

In this case, rather than using Maximum Likelihood to learn the entries of the CPTs directly, we instead
learn the value of the parameter w. Since the number of parameters in w is small (n, compared with 2n
in the unconstrained case), we also have some hope that with a small number of training examples we can
learn a reliable value for w.

Example 38. Consider the following 3 variable model p(x1, x2, x3) = p(x1|x2, x3)p(x2)p(x3), where xi ∈
{0, 1} , i = 1, 2, 3. We assume that the CPT is parameterised using

p(x1 = 1|x2, x3, θ) ≡ e

−θ2

1−θ2

2(x2−x3)2

(9.2.28)

One may verify that the above probability is always positive and lies between 0 and 1. Due to normalisation,
we must have

p(x1 = 0|x2, x3) = 1 − p(x1 = 1|x2, x3)

DRAFT March 9, 2010

(9.2.29)

173

Bayesian Belief Network Training

For unrestricted p(x2) and p(x3), the Maximum Likelihood setting is p(x2 = 1) ∝ (cid:93) (x2 = 1), and p(x3 =
1) ∝ (cid:93) (x3 = 1). The contribution to the log likelihood from the term p(x1|x2, x3, θ), assuming i.i.d. data,
is

1 = 1](cid:0)

I [xn

N(cid:88)

n=1

3 )2(cid:1) + I [xn

(cid:16)

−θ2

1 − θ2

2(xn

2 − xn

1 = 0] log

1 − e

3 )2(cid:17)

−θ2

1−θ2

2 −xn

2(xn

(9.2.30)

This objective function needs to be optimised numerically to ﬁnd the best θ1 and θ2. The gradient is

L(θ1, θ2) =

N(cid:88)
N(cid:88)

n=1

n=1

dL
dθ1

=

dL
dθ2

=

−2I [xn

1 = 1] θ1 + 2I [xn

−2I [xn

1 = 1] θ2 (xn

2 − xn

2 −xn
3 )2
2 −xn
3 )2

2(xn
2(xn

1 = 0] θ1e−θ2
1−θ2
1−θ2
1 − e−θ2
3 )2 + 2θ2I [xn

1 = 0]

(xn

3 )2e−θ2
1−θ2

2 − xn
1 − e−θ2

1−θ2
2(xn

2 −xn
2(xn
3 )2
2 −xn
3 )2

(9.2.31)

(9.2.32)

The gradient can be used as part of a standard optimisation procedure (such as conjugate gradients, see
Appendix (A)) to aid ﬁnding the Maximum Likelihood parameters θ1, θ2.

9.3 Bayesian Belief Network Training

An alternative to Maximum Likelihood training of a BN is to use a Bayesian approach in which we maintain
a distribution over parameters. We continue with the Asbestos, Smoking, Cancer scenario,

p(a, c, s) = p(c|a, s)p(a)p(s)

(9.3.1)

which can be represented as a Belief Network, ﬁg(9.4a). So far we’ve only speciﬁed the independence
structure, but not the entries of the tables p(c|a, s), p(a), p(s). Given a set of visible observations,
V = {(an, sn, cn) , n = 1, . . . , N}, we would like to learn appropriate distributions for the table entries.
To begin we need a notation for the table entries. With all variables binary we have parameters such as

p(a = 1|θa) = θa, p(c = 1|a = 0, s = 1, θc) = θ0,1
c
, θ1,0

and similarly for the remaining parameters θ1,1

, θ0,0

c

c

c

θa, θs, θ0,0

c

, θ0,1

c

, θ1,0

c

, θ1,1

c

(cid:124)

(cid:123)(cid:122)

θc

(cid:125)

. For our example, the parameters are

(9.3.2)

(9.3.3)

9.3.1 Global and local parameter independence

In Bayesian learning of BNs, we need to specify a prior on the joint table entries. Since in general dealing
with multi-dimensional continuous distributions is computationally problematic, it is useful to specify only
uni-variate distributions in the prior. As we show below, this has a pleasing consequence that for i.i.d.
data the posterior also factorises into uni-variate distributions.

Global parameter independence

A convenient assumption is that the prior factorises over parameters. For our Asbestos, Smoking, Cancer
example, we assume

Assuming the data is i.i.d., we then have the joint model

p(θa, θs, θc) = p(θa)p(θs)p(θc)

p(θa, θs, θc,V) = p(θa)p(θs)p(θc)(cid:89)

n

p(an|θa)p(sn|θs)p(cn|sn, an, θc)

(9.3.4)

(9.3.5)

174

DRAFT March 9, 2010

Bayesian Belief Network Training

θa

an

θs

sn

cn

n = 1 : N

θa,s
c

(a, s) ∈ P

Figure 9.7: A Bayesian parameter model for the relationship between lung
Cancer, Asbestos exposure and Smoking with factorised parameter pri-
ors. The global parameter independence assumption means that the prior
over tables factorises into priors over each conditional probability table.
The local independence assumption, which in this case comes into ef-
c ), where

fect only for p(c|a, s), means that p(θc) factorises in(cid:81)

a,s∈P p(θa,s

P = {(0, 0), (0, 1), (1, 0), (1, 1)}.

the Belief Network for which is given in ﬁg(9.7.) Learning then corresponds to inference of

A convenience of the factorised prior for a BN is that the posterior also factorises, since

= p(V|θa, θs, θc)p(θa)p(θs)p(θc)

p(V)

p(θa, θs, θc|V) = p(V|θa, θs, θc)p(θa, θs, θc)
(cid:41)(cid:40)
p(θs)(cid:89)

p(θa, θs, θc|V) ∝ p(θa, θs, θc,V)

(cid:40)
p(θa)(cid:89)

p(an|θa)

∝
= p(θa|Va)p(θs|Vs)p(θc|V)

n

p(V)
(cid:41)(cid:40)
p(θc)(cid:89)
p(sn|θs)

n

n

p(cn|sn, an, θc)

(cid:41)

(9.3.6)

(9.3.7)

so that one can consider each parameter posterior separately. In this case, ‘learning’ involves computing
the posterior distributions p(θi|Vi) where Vi is the set of training data restricted to the family of variable
i.

The global independence assumption conveniently results in a posterior distribution that factorises over
the conditional tables. However, the parameter θc is itself 4 dimensional. To simplify this we need to make
a further assumption as to the structure of each local table.

Local parameter independence

If we further assume that the prior for the table factorises over all states a, c:

p(θc) = p(θ0,0

c )p(θ1,0

c )p(θ0,1

c )p(θ1,1
c )

(9.3.8)

then the posterior

(cid:3)(cid:93)(a=0,s=0)
p(θc|V) ∝ p(V|θc)p(θ0,0
(cid:123)(cid:122)

(cid:2)θ0,0

(cid:124)
= ∝

c

(cid:2)θ0,1

c )p(θ0,1
c )p(θ1,1
c )
p(θ0,0
c )
∝

(cid:3)(cid:93)(a=0,s=1)
(cid:123)(cid:122)

(cid:125)

(cid:124)

c

c )p(θ1,0

p(θ0,0

c

|V)

p(θ0,1

c

|V)

(cid:2)θ1,0

c

(cid:124)

∝

p(θ0,1
c )

(cid:125)

(cid:3)(cid:93)(a=1,s=0)
(cid:123)(cid:122)

p(θ1,0

c

|V)

(cid:2)θ1,1

c

(cid:124)

∝

p(θ1,0
c )

(cid:125)

(9.3.9)

(cid:3)(cid:93)(a=1,s=1)
(cid:123)(cid:122)

|V)

c

p(θ1,1
(9.3.10)

p(θ1,1
c )

so that the posterior also factorises over the parental states of the local conditional table.

Posterior marginal table

A marginal probability table is given by, for example,

(cid:90)

p(c = 1|a = 1, s = 0,V) =

DRAFT March 9, 2010

θc

p(c = 1|a = 1, s = 0, θ1,0

c )p(θc|V)

(9.3.11)

175

Bayesian Belief Network Training

(cid:90)

The integral over all the other tables in equation (9.3.11) is unity, and we are left with

p(c = 1|a = 1, s = 0,V) =

p(c = 1|a = 1, s = 0, θ1,0

c )p(θ1,0
c

|V)

(9.3.12)

θ1,0
c

9.3.2 Learning binary variable tables using a Beta prior

We continue the example of section(9.3.1) where all variables are binary, but using a continuous valued
table prior. The simplest case is to start with p(a|θa) since this requires only a univariate prior distribution
p(θa). The likelihood depends on the table variable via

p(a = 1|θa) = θa

so that the total likelihood term is

θ(cid:93)(a=1)
a

(1 − θa)(cid:93)(a=0)
The posterior is therefore

(9.3.13)

(9.3.14)

p(θa|Va) ∝ p(θa)θ(cid:93)(a=1)

(9.3.15)
a (1 − θa)β then conjugacy will hold, and the mathematics
This means that if the prior is also of the form θα
of integration will be straightforward. This suggests that the most convenient choice is a Beta distribution,

(1 − θa)(cid:93)(a=0)

a

p(θa) = B (θa|αa, βa) =

1

B(αa, βa) θαa−1

a

(1 − θa)βa−1

for which the posterior is also a Beta distribution:

p(θa|Va) = B (θa|αa + (cid:93) (a = 1) , βa + (cid:93) (a = 0))

The marginal table is given by

(cid:90)

p(a = 1|Va) =

θa

p(θa|Va)θa =

αa + (cid:93) (a = 1)

αa + (cid:93) (a = 1) + βa + (cid:93) (a = 0)

(9.3.16)

(9.3.17)

(9.3.18)

using the result for the mean of a Beta distribution, deﬁnition(72).

The situation for the table p(c|a, s) is slightly more complex since we need to specify a prior for each of
the parental tables. As above, this is most convenient if we specify a Beta prior, one for each of the (four)
parental states. Let’s look at a speciﬁc table

p(c = 1|a = 1, s = 0)

Assuming the local independence property, we have p(θ1,0

|αc(a = 1, s = 0) + (cid:93) (c = 1, a = 1, s = 0) , βc(a = 1, s = 0) + (cid:93) (c = 0, a = 1, s = 0)(cid:1)

|Vc) given by

B(cid:0)θ1,0

c

c

As before, the marginal probability table is then given by

p(c = 1|a = 1, s = 0,Vc) =

αc(a = 1, s = 0) + (cid:93) (c = 1, a = 1, s = 0)

αc(a = 1, s = 0) + βc(a = 1, s = 0) + (cid:93) (a = 1, s = 0)

since (cid:93) (a = 1, s = 0) = (cid:93) (c = 0, a = 1, s = 0) + (cid:93) (c = 1, a = 1, s = 0).

(9.3.19)

(9.3.20)

(9.3.21)

The prior parameters αc(a, s) are called hyperparameters. If one had no preference, one could set all of the
αc(a, s) to be equal to the same value α and similarly for β. A complete ignorance prior would correspond
to setting α = β = 1, see ﬁg(8.7).

176

DRAFT March 9, 2010

Bayesian Belief Network Training

No data limit N → 0 In the limit of no data, the marginal probability table corresponds to the prior,

which is given in this case by

p(c = 1|a = 1, s = 0) =

αc(a = 1, s = 0)

αc(a = 1, s = 0) + βc(a = 1, s = 0)

(9.3.22)

For a ﬂat prior α = β = 1 for all states a, c, this would give a prior probability of p(c = 1|a = 1, s =
0) = 0.5.

Inﬁnite data limit N → ∞ In this limit the marginal probability tables are dominated by the data
counts, since these will typically grow in proportion to the size of the dataset. This means that in
the inﬁnite (or very large) data limit,

p(c = 1|a = 1, s = 0,V) →

(cid:93) (c = 1, a = 1, s = 0)

(cid:93) (c = 1, a = 1, s = 0) + (cid:93) (c = 0, a = 1, s = 0)

(9.3.23)

which corresponds to the Maximum Likelihood solution.

This eﬀect that the large data limit of a Bayesian procedure corresponds to the Maximum Likelihood
solution is general unless the prior has a pathologically strong eﬀect.

Example 39 (Asbestos-Smoking-Cancer).

Consider the binary variable network

p(c, a, s) = p(c|a, s)p(a)p(s)

(9.3.24)

The data V is given in ﬁg(9.5). Using a ﬂat Beta prior α = β = 1 for all conditional probability tables,
the marginal posterior tables are given by

p(a = 1|V) =

1 + (cid:93) (a = 1)

2 + N

=

1 + 4
2 + 7

=

5
9 ≈ 0.556

(9.3.25)

By comparison, the Maximum Likelihood setting is 4/7 = 0.571. The Bayesian result is a little more
cautious than the Maximum Likelihood, which squares with our prior belief that any setting of the
probability is equally likely, pulling the posterior towards 0.5.

Similarly,

p(s = 1|V) =

1 + (cid:93) (s = 1)

2 + N

=

1 + 4
2 + 7

=

5
9 ≈ 0.556

and

p(c = 1|a = 1, s = 1,V) =

1 + (cid:93) (c = 1, a = 1, s = 1)

2 + (cid:93) (c = 1, a = 1, s = 1) + (cid:93) (c = 0, a = 1, s = 1)

p(c = 1|a = 1, s = 0,V) =

p(c = 1|a = 0, s = 1,V) =

p(c = 1|a = 0, s = 0,V) =

1 + (cid:93) (c = 1, a = 1, s = 0)

2 + (cid:93) (c = 1, a = 1, s = 0) + (cid:93) (c = 0, a = 1, s = 0)

1 + (cid:93) (c = 1, a = 0, s = 1)

2 + (cid:93) (c = 1, a = 0, s = 1) + (cid:93) (c = 0, a = 0, s = 1)

1 + (cid:93) (c = 1, a = 0, s = 0)

2 + (cid:93) (c = 1, a = 0, s = 0) + (cid:93) (c = 0, a = 0, s = 0)

=

=

=

=

1 + 2
2 + 2

1 + 1
2 + 1
1 + 1
2 + 2
1 + 0
2 + 1

=

=

=

=

3
4

2
3
1
2
1
3

DRAFT March 9, 2010

(9.3.26)

(9.3.27)

(9.3.28)

(9.3.29)

(9.3.30)

177

Bayesian Belief Network Training

9.3.3 Learning multivariate discrete tables using a Dirichlet prior

The natural generalisation to more than two-state variables is given by using a Dirichlet prior, again
assuming i.i.d. data and the local and global parameter prior independencies. Since under the global
parameter independence assumption the posterior factorises over variables (as in equation (9.3.7)), we can
concentrate on the posterior of a single variable.

No parents
Let’s consider the contribution of a variable v with dom(v) = {1, . . . , I}. The contribution to the posterior
from a datapoint vn is

I[vn=i]
i

,

θ

θi = 1

I(cid:88)

i=1

I(cid:89)

i=1

p(vn|θ) =
I(cid:89)
N(cid:89)

p(θ)

so that the posterior is proportional to

I(cid:89)

(cid:80)N

n=1

θ

i

I[vn=i]

I[vn=i]
i

θ

= p(θ)

n=1

i=1

i=1

For a Dirichlet prior distribution with hyperparameters u

I(cid:89)

i=1

p(θ) ∝

θui−1

i

Using this prior the posterior becomes

I(cid:89)

I(cid:89)

(cid:80)N

n=1

θ

i

θui−1

i

i=1

i=1

I[vn=i]

=

ui−1+(cid:80)N

I[vn=i]

n=1

I(cid:89)

i=1

θ

i

p(θ|V) ∝

which means that the posterior is given by

p(θ|V) = Dirichlet (θ|u + c)

where c is a count vector with components

N(cid:88)

ci =

I [vn = i]

n=1

being the number of times state i was observed in the training data.

The marginal table is given by integrating

(cid:90)

p(v = i|V) =

p(v = i|θ)p(θ|V) =

θ

θip(θi|V)

θi

(cid:90)

(cid:80)

p(v = i|V) = ui + ci
j uj + cj

(9.3.31)

(9.3.32)

(9.3.33)

(9.3.34)

(9.3.35)

(9.3.36)

(9.3.37)

(9.3.38)

Since the single-variable marginal distribution of a Dirichlet is a Beta distribution, section(8.5), the
marginal table is the mean of a Beta distribution. Given that the marginal p(θ|V) is Beta distribution

with parameters α = ui + ci, β =(cid:80)

j(cid:54)=i uj + cj, the marginal table is given by

which generalises the binary state formula equation (9.3.18).

178

DRAFT March 9, 2010

Bayesian Belief Network Training

9.3.4 Parents

To deal with the general case of a variable v with parents pa (v) we denote the probability of v being in
state i, conditioned on the parents being in state j as

p(v = i|pa (v) = j, θ) = θi(v; j)

where(cid:80)

the number of states j will be exponential in K.

i θi(v; j) = 1. This forms the components of a vector θ(v; j). Note that if v has K parents then

(9.3.39)

(9.3.40)

(9.3.41)

Local (state) independence means

p(θ(v; j))

p(θ(v)) =(cid:89)
p(θ) =(cid:89)

j

p(θ(v))

And global independence means

v

where θ = (θ(v), v = 1, . . . , V ) represents the combined table of all the variables. We drop the explicit
sans-serif font on the states from here on in.

Parameter posterior

Thanks to the global parameter independence the posterior distribution over the tables θ factorises, with
one posterior table per variable. Each posterior table for a variable v depends only on the information
local to the family of each variable D(v). Assuming a Dirichlet distribution prior

p(θ(v; j)) = Dirichlet (θ(v; j)|u(v; j))

the posterior is proportional to the joint distribution

(cid:89)
p(θ(v),D(v)) = p(θ(v))p(D(v)|θ(v))
(cid:89)

=(cid:89)
=(cid:89)

Z(u(v; j))

1

1

i

Z(u(v; j))

i

j

j

θi(v; j)ui(v;j)−1(cid:89)

(cid:89)

(cid:89)

θi(v; j)I[vn=i,pa(vn)=j]

n

j

i

θi(v; j)ui(v;j)−1+(cid:93)(v=i,pa(v)=j)

where Z(u) is the normalisation constant of a Dirichlet distribution.
Hence the posterior is

p(θ(v)|D(v)) =(cid:89)

Dirichlet(cid:0)θ(v; j)|u(cid:48)(v; j)(cid:1)

j

where the hyperparameter prior term is updated by the observed counts,

(cid:48)
i(v; j) ≡ ui(v; j) + (cid:93) (v = i, pa (v) = j)
u

(9.3.42)

(9.3.43)

(9.3.44)

(9.3.45)

(9.3.46)

(9.3.47)

By analogy with the no-parents case, the marginal table is given by (writing the states explicitly)

(cid:48)
i(v; j)
p(v = i|pa (v) = j,D(v)) ∝ u

DRAFT March 9, 2010

(9.3.48)

179

Bayesian Belief Network Training

a
1
1
0
0
1
0
1

s
1
0
1
1
1
0
0

c
2
0
1
0
2
0
1

Figure 9.8: A database of patient records about the Asbestos exposure (1
signiﬁes exposure), being a Smoker (1 signiﬁes the individual is a smoker),
and lung Cancer (0 signiﬁes no cancer, 1 signiﬁes early stage cancer, 2
signiﬁes late state cancer). Each row contains the information for an indi-
vidual, so that there are 7 individuals in the database.

Example 40. Consider the p(c|a, s)p(s)p(a) asbestos example with dom(a) = dom(s) = {0, 1}, except
now with the variable c taking three states, dom(c) = {0, 1, 2}, accounting for diﬀerent kinds of cancer.
The marginal table under a Dirichlet prior is then given by

p(c = 0|a = 1, s = 1,V) =

u0(a = 1, s = 1) + (cid:93) (c = 0, a = 1, s = 1)

i∈{0,1,2} ui(a = 1, s = 1) + (cid:93) (c = i, a = 1, s = 1)

(cid:80)

(9.3.49)

(9.3.50)

(9.3.51)

(9.3.52)

(9.3.53)

(9.3.54)

Assuming a ﬂat Dirichlet prior, which corresponds to setting all components of u to 1, this gives

p(c = 0|a = 1, s = 1,V) =

p(c = 1|a = 1, s = 1,V) =
p(c = 2|a = 1, s = 1,V) =

1 + 0
3 + 2
1 + 0
3 + 2
1 + 2
3 + 2

=

=

=

1
5
1
5
3
5

and similarly for the other three tables p(c|a = 1, s = 0), p(c|a = 0, s = 1), p(c|a = 1, s = 1).

Model likelihood

For a Belief Network M, the joint probability of all variables factorises into the local probabilities of each
variable conditioned on its parents:

p(V|M) =(cid:89)
p(D|M) =(cid:89)

v

For i.i.d. data D, the likelihood under the network M is

p(v|pa (v) , M)
(cid:89)

p(vn|pa (vn) , M) =(cid:89)

(cid:89)

v

n

v

j

Z(u(cid:48)(v; j))
Z(u(v; j))

where u are the Dirichlet hyperparameters and u(cid:48) is given by equation (9.3.47). Expression (9.3.54) can
be written explicitly in terms of Gamma functions, see exercise(125). In the above expression in general
the number of parental states diﬀers for each variable v, so that implicit in the above formula is that the
state product over j goes from 1 to the number of parental states of variable v. Due to the local and
global parameter independence assumptions, the logarithm of the model likelihood splits into terms, one
for each variable v and parental conﬁguration. This is called the likelihood decomposable property.

9.3.5 Structure learning

Up to this point we have assumed that we are given both the structure of the distribution and a dataset‘
D. A more complex task is when we need to learn the structure of the network as well. We’ll consider
the case in which the data is complete (i.e. there are no missing observations). Since for D variables,
there is an exponentially large number (in D) of BN structures, it’s clear that we cannot search over all
possible structures. For this reason structure learning is a computationally challenging problem and we
must rely on constraints and heuristics to help guide the search. Furthermore, for all but the sparsest

180

DRAFT March 9, 2010

Bayesian Belief Network Training

Algorithm 3 PC algorithm for skeleton learning.

1: Start with a complete undirected graph G on the set V of all vertices.
2: i = 0
3: repeat
4:
5:

for x ∈ V do

for y ∈ Adj {x} do

Determine if there a subset S of size i of the neighbours of x (not including y) for which
x⊥⊥ y|S. If this set exists remove the x − y link from the graph G and set Sxy = S.

6:

7:
8:
9:

end for

end for
i = i + 1.

10: until all nodes have ≤ i neighbours.

networks, estimating the dependencies to any accuracy requires a large amount of data, making testing
of dependencies diﬃcult.
Indeed, for a ﬁnite amount of data, two variables will always have non-zero
mutual information, so that a threshold needs to be set to decide if the measured dependence is signiﬁ-
cant under the ﬁnite sample, see section(9.3.6). Other complexities arise from the concern that a Belief
or Markov Network on the visible variables alone may not be a parsimonious way to represent the ob-
served data if, for example, there may be latent variables which are driving the observed dependencies. For
these reasons we will not discuss this topic in detail here and limit the discussion to two central approaches.

A special case that is computationally tractable is when the network is constrained to have at most one
parent. We defer discussion of this to section(10.4.1).

PC algorithm

The PC algorithm[259] ﬁrst learns the skeleton of a graph, after which edges may be oriented to form a
(partially oriented) DAG.

The PC algorithm begins at the ﬁrst round with a complete skeleton G and attempts to remove as many
links as possible. At the ﬁrst step we test all pairs x⊥⊥ y|∅. If an x and y pair are deemed independent
then the link x− y is removed from the complete graph. One repeats this for all the pairwise links. In the
second round, for the remaining graph, one examines each x− y link and conditions on a single neighbour
z of x. If x⊥⊥ y| z then remove the link x − y. One repeats in this way through all the variables. At each
round the number of neighbours in the conditioning set is increased by one. See algorithm(3), ﬁg(9.9)1 and
demoPCoracle.m. A reﬁnement of this algorithm, known as NPC for necessary path PC[261] attempts to
limit the number of independence checks which may otherwise result in inconsistencies due to the empirical
estimates of conditional mutual information. Given a learned skeleton, a partial DAG can be constructed
using algorithm(4). Note that this is necessary since the undirected graph G is a skeleton – not a Belief
Network of the independence assumptions discovered. For example, we may have a graph G with x− z − y
in which the x − y link was removed on the basis x ⊥⊥ y| ∅ → Sxy = ∅. As a MN the graph x − z − y
implies x(cid:62)(cid:62)y, although this is inconsistent with the discovery in the ﬁrst round x⊥⊥ y. This is the reason
for the orientation part: for consistency, we must have x → z ← y, for which x ⊥⊥ y and x(cid:62)(cid:62)y| z. Note
that in algorithm(4) we have for the ‘unmarried collider’ test, z (cid:54)∈ ∅, which in this case is true, resulting
in a collider forming. See also ﬁg(9.10).

Example 41 (Skeleton orienting).

z

x

y

x⊥⊥ y|∅ ⇒

z

x

y

If x is (unconditionally) independent of y,
it
must be that z is a collider since otherwise
marginalising over z would introduce a depen-
dence between x and y.

1This example appears in [148] and [208] – thanks also to Seraf´ın Moral for his online notes.

DRAFT March 9, 2010

181

Bayesian Belief Network Training

x

z

x

z

t

(a)

t

(j)

y

x

y

x

y

x

y

x

y

x

y

x

y

x

y

x

w

z

w

z

w

z

w

z

w

z

w

z

w

z

w

z

t

(b)

t

(c)

t

(d)

t

(e)

t

(f)

t

(g)

t

(h)

y

x

y

x

y

x

y

x

y

x

y

x

y

x

y

x

w

z

w

z

w

z

w

z

w

z

w

z

w

z

w

z

t

(k)

t

(l)

t

(m)

t

(n)

t

(o)

t

(p)

t

(q)

y

w

y

w

t

(i)

t

(r)

(a): The BN from which data is assumed generated and against which
Figure 9.9: PC algorithm.
conditional independence tests will be performed. (b): The initial skeleton is fully connected. (c-l): In
the ﬁrst round (i = 0) all the pairwise mutual informations x⊥⊥ y|∅ are checked, and the link between x
(m-o): i = 1. We now look at connected subsets on
and y removed if deemed independent (green line).
the three variables x, y, z of the remaining graph, removing the link x − y if x⊥⊥ y| z is true. Not all steps
(p,q): i = 2. We now examine all x⊥⊥ y|{a, b}. The algorithm terminates after this round
are shown.
(r): Final skeleton.
(when i gets incremented to 3) since there are no nodes with 3 or more neighbours.
During this process the sets Sx,y = ∅, Sx,w = ∅, Sz,w = y, Sx,t = {z, w} , Sy,t = {z, w} were found. See also
demoPCoracle.m

z

x

y

x⊥⊥ y| z ⇒

z

x

y

If x is independent of y conditioned on z, z must
not be a collider. Any other orientation is ap-
propriate.

9.3.6 Empirical independence
Given a data set D, containing variables x, y, z, our interest is to measure if x⊥⊥ y| z. One approach is to
use the conditional mutual information which is the average of conditional Kullback-Leibler divergences.

Deﬁnition 87 (Mutual Information).

MI(x; y|z) ≡ (cid:104)KL(p(x, y|z)|p(x|z)p(y|z))(cid:105)p(z) ≥ 0

(9.3.55)

where this expression is equally valid for sets of variables. If x⊥⊥ y| z is true, then MI(x; y|z) is zero, and
vice versa. When z = ∅, the average over p(z) is absent and one writes MI(x; y).

Given data we can obtain an estimate of the conditional mutual information by using the empirical
distribution p(x, y, z) estimated by simply counting occurrences in the data.
In practice, however, we
only have a ﬁnite amount of data to estimate the empirical distribution so that for data sampled from
distribution for which the variables truly are independent, the empirical mutual information will typically
be greater than zero. An issue therefore is what threshold to use for the empirical conditional mutual
information to decide if this is suﬃciently far from zero to be caused by dependence. A frequentist
approach is to compute the distribution of the conditional mutual information and then see where the
sample value is compared to the distribution. According to [164] 2NMI(x; y|z) is Chi-square distributed
182
DRAFT March 9, 2010

Bayesian Belief Network Training

Algorithm 4 Skeleton orientation algorithm (returns a DAG).

1: Unmarried Collider: Examine all undirected links x − z − y. If z (cid:54)∈ Sxy set x → z ← y.
2: repeat
3:
4:
5:
6: until No more edges can be oriented.
7: The remaining edges can be arbitrarily oriented provided that the graph remains a DAG and no additional

x → z − y ⇒ x → z → y
For x − y, if there is a directed path from x to y orient x → y
If for x − z − y there is a w such that x → w, y → w, z − w then orient z → w

colliders are introduced.

with (X−1)(Y −1)Z degrees of freedom, although this test does not work well in the case of small amounts
of data. An alternative pragmatic approach is to estimate the threshold based on empirical samples of
the MI under controlled independent/dependent conditions – see demoCondindepEmp.m for a comparison
of these approaches.

Bayesian conditional independence test

A Bayesian approach to testing for independence can be made by comparing the likelihood of the data under
the independence hypothesis, versus the likelihood under the dependent hypothesis. For the independence
hypothesis we have a joint distribution over variables and parameters:

p(x, y, z, θ|Hindep) = p(x|z, θx|z)p(y|z, θy|z)p(z|θz)p(θx|z)p(θy|z)p(θz)

(9.3.56)

For categorical distributions, it is convenient to use a prior Dirichlet (θ|u) on the parameters θ, assum-
ing also local as well as global parameter independence. For a set of assumed i.i.d. data (X ,Y,Z) =
(xn, yn, zn) , n = 1, . . . , N, the likelihood is then given by integratingover the parameters θ:

p(X ,Y,Z|Hindep) = Z(uz + (cid:93) (z))

Z(uz)

Z(ux|z + (cid:93) (x, z))

Z(uy|z + (cid:93) (y, z))

Z(ux|z)

z

Z(uy|z)

(9.3.57)

where ux|z is a hyperparameter matrix of pseudo counts for each state of x given each state of z. Z(v) is
the normalisation constant of a Dirichlet distribution with vector parameter v.

For the dependent hypothesis we have

p(x, y, z, θ|Hdep) = p(x, y, z|θx,y,z)p(θx,y,z)

The likelihood is then

p(X ,Y,Z|Hdep) = Z(ux,y,z + (cid:93) (x, y, z))

Z(ux,y,z)

(9.3.58)

(9.3.59)

x

z

y

x

y

x

y

x

w

z

w

z

w

z

y

w

t

(a)

t

(b)

t

(c)

t

(d)

(a):
Figure 9.10: Skeleton orientation algorithm.
The skeleton along with Sx,y = ∅, Sx,w = ∅, Sz,w =
(b): z (cid:54)∈ Sx,y,
y, Sx,t = {z, w} , Sy,t = {z, w}.
(c): t (cid:54)∈ Sz,w, so form collider.
so form collider.
(d): Final partially oriented DAG. The remaining
edge may be oriented as desired, without violating
the DAG condition. See also demoPCoracle.m.

DRAFT March 9, 2010

183

p(X ,Y,Z|Hindep) =

p(xn, yn, zn, θ|Hindep)

Thanks to conjugacy, this is straightforward and gives the expression

(cid:90)

(cid:89)

θ

n

(cid:89)

Bayesian Belief Network Training

θz

zn

xn

yn

N

θx|z

θy|z

θx,y,z

xn, yn, zn

N

Figure 9.11: Bayesian conditional independence test
(a): A
using Dirichlet priors on the tables.
model Hindep for conditional independence x ⊥⊥ y |
(b): A model Hdep for conditional depen-
z.
dence x(cid:62)(cid:62)y| z. By computing the likelihood of the
data under each model, a numerical score for the
whether the data is more consistent with the condi-
tional independence assumption can be formed. See
demoCondindepEmp.m.

(a)

(b)

Figure 9.12: Conditional independence test of x ⊥⊥ y | z1, z2 with x, y, z1, z2
having 3, 2, 4, 2 states respectively. From the oracle Belief network shown, in
each experiment the tables are drawn at random and 20 examples are sampled
to form a dataset. For each dataset a test is carried out to determine if x and
y are independent conditioned on z1, z2 (the correct answer being that they are
independent). Over 500 experiments, the Bayesian conditional independence
test correctly states that the variables are conditionally independent 74% of the
time, compared with only 50% accuracy using the chi-square mutual information
test. See demoCondindepEmp.m.

Assuming each hypothesis is equally likely, for a Bayes’ Factor

p(X ,Y,Z|Hindep)
p(X ,Y,Z|Hdep)

(9.3.60)

greater than 1, we assume that conditional independence holds, otherwise we assume the variables are
conditionally dependent. demoCondindepEmp.m suggests that the Bayesisan hypothesis test tends to out-
perform the conditional mutual information approach, particularly in the small sample size case, see
ﬁg(9.12).

9.3.7 Network scoring

An alternative to local methods such as the PC algorithm is to evaluate the whole network structure. In
a probabilistic context, given a model structure M, we wish to compute p(M|D) ∝ p(D|M)p(M). Some
care is needed here since we have to ﬁrst ‘ﬁt’ each model with parameters θ, p(V|θ, M) to the data D. If
we do this using Maximum Likelihood alone, with no constraints on θ, we will always end up favouring
that model M with the most complex structure (assuming p(M) = const.). This can be remedied by using
the Bayesian technique

(cid:90)

p(D|M) =

θ

p(D|θ, M)p(θ|M)

(9.3.61)

In the case of directed networks, however, as we saw in section(9.3), the assumptions of local and global
parameter independence make the integrals tractable. For a discrete state network and Dirichlet priors,
we have p(D|M) given explicitly by the Bayesian Dirichlet score equation (9.3.54). First we specify
the hyperparameters u(v; j), and then search over structures M, to ﬁnd the one with the best score
p(D|M). The simplest setting for the hyperparameters is set them all to unity[66]. Another setting is the
‘uninformative prior’[52]

ui(v; j) =

α

dim v dim pa (v)

184

(9.3.62)

DRAFT March 9, 2010

xyz1z2Maximum Likelihood for Undirected models

(a)

(b)

(c)

(a): The correct structure in which all
Figure 9.13: Learning the structure of a Bayesian network.
variables are binary. The ancestral order is 2, 1, 5, 4, 3, 8, 7, 6. The dataset is formed from 1000 samples
(b): The learned structure based on the PC algorithm using the Bayesian empirical
from this network.
conditional independence test. Undirected edges may be oriented arbitrarily. (c): The learned structure
based on the Bayes Dirichlet network scoring method. See demoPCdata.m and demoBDscore.m.

where dim x is the number of states of the variable(s) x, giving rise to the BDeu score, for an ‘equivalent
sample size’ parameter α. A discussion of these settings is given in [129] under the concept of likelihood
equivalence, namely that two networks which are Markov equivalent should have the same score. How
dense the resulting network is can be sensitive to α[263, 250, 262]. Including an explicit prior p(M) on
the networks to favour those with sparse connections is also a sensible idea, for which one modiﬁed the
score to p(D|M)p(M).
Searching over structures is a computationally demanding task. However, since the log-score decomposes
into terms involving each family of v, we can compare two networks diﬀering in a single arc eﬃciently.
Search heuristics based on local addition/removal/reversal of links [66, 129] that increase the score are
popular[129].
In learnBayesNet.m we simplify the problem for demonstration purposes in which we
assume we know the ancestral order of the variables, and also the maximal number of parents of each
variable.

Example 42 (PC algorithm versus network scoring). In ﬁg(9.13) we compare the PC algorithm with
BD network scoring based (with Dirichlet hyperparameters set to unity) on 1000 samples from a known
Belief Network. The PC algorithm conditional independence test is based on the Bayesian factor (9.3.60)
in which Dirichlet priors with α = 0.1 were used throughout. In ﬁg(9.13) the network scoring technique
outperforms the PC algorithm. This is partly explained by the network scoring technique being provided
with the correct ancestral order and the constraint that each variable has maximally two parents.

9.4 Maximum Likelihood for Undirected models

Consider a Markov network distribution p(X ) deﬁned on (not necessarily maximal) cliques Xc ⊆ X

p(X|θ) =

1

Z(θ)

φc(Xc|θc)

(cid:89)

c

where

Z(θ) =(cid:88)

X

(cid:89)

c

φc(Xc|θc)

DRAFT March 9, 2010

(9.4.1)

(9.4.2)

185

123456781234567812345678Maximum Likelihood for Undirected models

ensures normalisation. Given a set of data, X n, n = 1, . . . , N, and assuming i.i.d. data, the log likelihood
is

log φc(X n

c |θc) − N log Z(θ)

(9.4.3)

L(θ) =(cid:88)

(cid:88)

n

c

In general learning the optimal parameters θc, c = 1, . . . , C is awkward since they are coupled via Z(θ).
Unlike the BN, the objective function does not split into a set of isolated parameter terms and in general
we need to resort to numerical methods. In special cases, however, exact results still apply, in particular
when the MN is decomposable and no constraints are placed on the form of the clique potentials, as we
discuss in section(9.4.2). More generally, however, gradient based techniques may be used and also give
insight into properties of the Maximum Likelihood solution.

9.4.1 The likelihood gradient

L(θ) =(cid:88)

∂
∂θc

∂
∂θc

log Z(θ) =

where we used the result

c |θc) − N

(cid:28) ∂
φc(Xc|θc)(cid:89)

c(cid:54)=c(cid:48)

∂θc

∂
∂θc

n

log φc(X n
(cid:88)

∂
∂θc

1

Z(θ)

X

(cid:29)
log φc(Xc|θc)
(cid:28) ∂

p(XC|θ)

φc(cid:48)(Xc(cid:48)|θc(cid:48)) =

log φc(Xc|θc)

∂θc

(cid:29)

p(Xc|θ)

The gradient can then be used as part of a standard numerical optimisation package.

Exponential form potentials

A common form of parameterisation is to use an exponential form

φc(Xc) = eθTψc(XC)

(9.4.4)

(9.4.5)

(9.4.6)

where θ are the parameters and ψc(XC) is a ﬁxed ‘feature function’ deﬁned on the variables of clique c.
Diﬀerentiating with respect to θ and equating to zero, we obtain that the Maximum Likelihood solution
satisﬁes that the empirical average of a feature function matches the average of the feature function with
respect to the model:

(cid:104)ψc(XC)(cid:105)(Xc) = (cid:104)ψc(XC)(cid:105)p(Xc)

(9.4.7)

1
N

(cid:93) (Xc)

(Xc) =

(9.4.8)
where (cid:93) (Xc) is the number of times the clique state Xc is observed in the dataset. An intuitive interpretation
is to sample states X from the trained model p(X ) and use these to compute the average of each feature
function. In the limit of an inﬁnite number of samples, for a Maximum Likelihood optimal model, these
sample averages will match those based on the empirical average.

Unconstrained potentials

For unconstrained potentials we have a separate table for each of the states deﬁned on the clique. This
means we may write

φ(Yc)I[Yc=X n

c ]

(9.4.9)

c ) =(cid:89)

Yc

φc(X n

where the product is over all states of potential c. This expression follows since the indicator is zero for
all but the single observed state X n
I [Yc = X n

c ] log φc(Yc) − N log Z(φ)

L(θ) =(cid:88)

c . The log likelihood is then

(cid:88)

(cid:88)

(9.4.10)

c

Yc

n

186

DRAFT March 9, 2010

Maximum Likelihood for Undirected models

ψ (x1, x2)

ψ (x2)

ψ (x2, x3, x5)

ψ (x5)

ψ (x5, x6)

x1

x2

x3

x4

(a)

x5

x6

ψ (x2, x5)

ψ (x2, x4, x5)

(b)

x1

x2, x3, x5

x6

x4

(c)

Figure 9.14: (a): A decomposable Markov network.
(c): Set chain for
(a) formed by choosing clique x2, x3, x5 as root and orienting edges consistently away from the root. Each
separator is absorbed into its child clique to form the set chain.

(b): A junction tree for (a).

where

(cid:89)

c

φc(Yc)

Z(φ) =(cid:88)
(cid:88)

Y

I [Yc = X n
c ]

n

p(Yc)
φc(Yc)

1
φc(Yc) − N
(cid:88)

1
N

n

p(Yc) = (Yc) ≡

I [Yc = X n
c ]

Diﬀerentiating the log likelihood with respect to a speciﬁc table entry φ(Yc) we obtain

Equating to zero, the Maximum Likelihood solution is obtained when

That is, the unconstrained optimal Maximum Likelihood solution is given by setting the clique potentials
such that the marginal distribution on each clique p(Yc) matches the empirical distribution on each clique
(Yc).

9.4.2 Decomposable Markov networks

In the case that there is no constraint placed on the form of the factors φc and if the MN corresponding
to these potentials is decomposable, then we know (from the junction tree representation) that we can
express the distribution in the form of a product of local marginals divided by the separator distributions

(9.4.11)

(9.4.12)

(9.4.13)

(9.4.14)

(9.4.15)

p(X ) =

(cid:81)
(cid:81)
c p(XC)
s p(XS)
p(X ) =(cid:89)

c

p(XC|XS)

By reabsorbing the separators into the numerator terms, we can form a set chain distribution, section(6.8)

Since this is directed, the Maximum Likelihood solution to learning the tables is trivial since we assign
each set chain factor p(XC|XS) by counting the instances in the dataset[168], see learnMarkovDecom.m.
The procedure is perhaps best explained by an example, as given below. See algorithm(5) for a general
description.

Example 43. Given a dataset V = {X n, n = 1, . . . , N}, we wish to ﬁt by Maximum Likelihood a MN of
the form

p(x1, . . . , x6) =

1
Z

φ(x1, x2)φ(x2, x3, x5)φ(x2, x4, x5)φ(x5, x6)

DRAFT March 9, 2010

(9.4.16)

187

Maximum Likelihood for Undirected models

Algorithm 5 Learning of an unconstrained decomposable Markov network using Maximum Likelihood.
We have a triangulated (decomposable) Markov network on cliques φc(Xc), c = 1, . . . , C and the empirical
marginal distributions on all cliques and separators, (Xc), (Xs)
1: Form a junction tree from the cliques.
2: Initialise each clique ψc(Xc) to (Xc) and each separator ψs(Xs) to (Xs).
3: Choose a root clique on the junction tree and orient edges consistently away from this root.
4: For this oriented junction tree, divide each clique by its parent separator.
5: Return the new potentials on each clique as the Maximum Likelihood solution.

where the potentials are unconstrained tables, see ﬁg(9.14a). Since the graph is decomposable, we know
it admits a factorisation of clique potentials divided by the separators:

p(x1, . . . , x6) = p(x1, x2)p(x2, x3, x5)p(x2, x4, x5)p(x5, x6)

p(x2)p(x2, x5)p(x5)

(9.4.17)

We can convert this to a set chain by reabsorbing the denominators into numerator terms, see section(6.8).
For example, by choosing the clique x2, x3, x5 as root,we can write

(cid:124) (cid:123)(cid:122) (cid:125)
p(x1, . . . , x6) = p(x1|x2)

(cid:124)

(cid:123)(cid:122)

p(x2, x3, x5)

ψ(x1,x2)

ψ(x2,x3,x5)

(cid:125)

(cid:124)
(cid:125)
p(x4|x2, x5)

(cid:123)(cid:122)

ψ(x2,x4,x5)

(cid:124) (cid:123)(cid:122) (cid:125)
p(x6|x5)

ψ(x5,x6)

(9.4.18)

where we identiﬁed the factors with clique potentials, and the normalisation constant Z is unity, see
ﬁg(9.14b). The advantage is that in this representation, the clique potentials are independent since the
distribution is a BN on cluster variables. The log likelihood for an i.i.d. dataset X = {xn, n = 1, . . . , N}
is

log p(xn

1|xn

2 ) + log p(xn

2 , xn

3 , xn

5 ) + log p(xn

2 , xn

5 ) + log p(xn

4|xn

6|xn
5 )

(9.4.19)

L =(cid:88)

n

where each of the terms is an independent parameter of the model. The Maximum Likelihood solution
then corresponds (as for the BN case) to simply setting each factor to the datacounts. For example

φ(x2, x4, x5) = p(x4|x2, x5) = (cid:93) (x2, x4, x5)

(cid:93) (x2, x5)

,

φ(x2, x3, x5) = p(x2, x3, x5) = (cid:93) (x2, x3, x5)

N

(9.4.20)

9.4.3 Non-decomposable Markov networks

In the non-decomposable or constrained case, no closed form Maximum Likelihood solution generally
exists and one needs to resort to numerical methods. According to equation (9.4.13) the Maximum
Likelihood solution is such that the clique marginals match the empirical marginals. Assuming that we
can absorb the normalisation constant into an arbitrarily chosen clique, we can drop explicitly representing
the normalisation constant. For a clique c, the requirement that the marginal of p matches the empirical
marginal on the variables in the clique is

φ(Xd) = (Xc)

(9.4.21)

φ(Xc)(cid:88)

X\c

(cid:89)

d(cid:54)=c

Given an initial setting for the potentials we can then update φ(Xc) to satisfy the above marginal require-
ment,

(cid:80)X\c

(cid:81)
(Xc)
d(cid:54)=c φ(Xd)

φnew(Xc) =

188

(9.4.22)

DRAFT March 9, 2010

Maximum Likelihood for Undirected models

which is required for each of the states of Xc. By multiplying and dividing the right hand side by φ(Xc)
this is equivalent to

φnew(Xc) = φ(Xc)(Xc)
p(Xc)

One can view this IPF update as coordinate-wise optimisation of the log likelihood in which the coor-
dinate corresponds to φc(Xc), with all other parameters ﬁxed. In this case this conditional optimum is
analytically given by the above setting. One proceeds by selecting another potential to update. Note that
in general, with each update, the marginal p(Xc) need to be recomputed. Computing these marginals may
be expensive unless the width of the junction tree formed from the graph is suitably limited.

Example 44 (Boltzmann Machine learning). We deﬁne the BM as

1

1

2 vTWv,

p(v|W) =

Z(W) e

for symmetric W and binary variables dom(vi) = {0, 1}. Given a set of training data, D =(cid:8)v1, . . . , vN(cid:9),

e

v

(9.4.24)

1

2 vTWv

the log likelihood is

Z(W) =(cid:88)

(9.4.23)

(9.4.25)

(9.4.26)

(9.4.27)

N(cid:88)

n=1

N(cid:88)

(cid:16)

n=1

L(W) =

1
2

(vn)T Wvn − N log Z(W)

Diﬀerentiating w.r.t. wij, i (cid:54)= j we have the gradient

∂L
∂wij

=

vn
i vn

j − (cid:104)vivj(cid:105)p(v|W)

(cid:17)

A simple algorithm to optimise the weight matrix W is to use gradient ascent,

N(cid:88)

(cid:16)

(cid:17)

wnew

ij = wold

ij + η

vn
i vn

j − (cid:104)vivj(cid:105)p(v|W)

n=1

the second order statistics of the model (cid:104)vivj(cid:105)p(v|W) match those of the empirical distribution,(cid:80)

for a learning rate η > 0. The intuitive interpretation is that learning will stop (the gradient is zero) when
j /N.
BM learning however is diﬃcult since (cid:104)vivj(cid:105)p(v|W) is typically computationally intractable for an arbitrary
interaction matrix W and therefore needs to be approximated. Indeed, one cannot compute the likelihood
L(W) exactly so that monitoring performance is also diﬃcult.

n vn

i vn

9.4.4 Constrained decomposable Markov networks

If there are no constraints on the forms of the maximal clique potentials of the Markov network, as we’ve
seen, learning is straightforward. Here our interest is when the functional form of the maximal clique is
constrained to be a product of potentials on smaller cliques2:

c(X i
φi
c)

(9.4.28)

φc(Xc) =(cid:89)

i

with no constraint being placed on the non-maximal clique potentials φi
In general, in this case
one cannot write down directly the Maximum Likelihood solution for the non-maximal clique potentials
c).
c(X i
φi

c).
c(X i

2A Boltzmann machine is of this form since any unconstrained binary pairwise potentials can be converted into a BM. For

other cases in which the φi

c are constrained, then Iterative scaling may be used in place of IPF.

DRAFT March 9, 2010

189

Maximum Likelihood for Undirected models

x2

x4

x1

(a)

x5

x3

φ1,4φ1,5

x1,4

φ1,2φ1,4

x2,4

φ2,3φ2,4φ3,4

(b)

(a):

Interpreted as

9.15:
φ(x1, x4, x5)φ(x1, x2, x4)φ(x2, x4, x3).

the distri-
Figure
represents
bution
(b): A junction tree for the pair-
φ(x4, x5)φ(x1, x4)φ(x4, x5)φ(x1, x2)φ(x2, x4)φ(x2, x3)φ(x3, x4).
wise MN in (a). We have a choice were to place the pairwise cliques, and this is one valid choice, using
the shorthand φa,b = φa,b(xa, xb) and xa,b = {xa, xb}.

graph represents
graph

a Markov network,

pairwise MN,

the

the

As

a

Consider the graph in ﬁg(9.15). In the constrained case, in which we interpret the graph as a pairwise
MN, IPF may be used to learn the pairwise tables. Since the graph is decomposable, there are however,
computational savings that can be made in this case[11]. For an empirical distribution , Maximum
Likelihood requires that all the pairwise marginals of the MN match the corresponding marginals obtained
from . As explained in ﬁg(9.15) we have a choice as to which junction tree clique each potential is assigned
to, with one valid choice being given in ﬁg(9.15b). Keeping the potentials of the cliques φ1,4φ1,5 and
φ2,3φ2,4φ3,4 ﬁxed we can update the potentials of clique φ1,2φ1,4. Using a bar to denote ﬁxed potentials,
we the marginal requirement that the MN matches the empirical marginal (x1, x2) can be written in
shorthand as

which can be expressed as

x3,x4,x5

p(x1, x2) = (cid:88)
(cid:32)(cid:88)
(cid:88)
(cid:124)

¯φ1,5 ¯φ4,5

(cid:123)(cid:122)

x4

x5

γ1,4

(cid:33)
(cid:125)

¯φ1,5 ¯φ4,5φ1,4φ1,2 ¯φ2,4 ¯φ2,3 ¯φ3,4 = (x1, x2)

(cid:32)(cid:88)
(cid:124)

x3

¯φ2,4 ¯φ2,3 ¯φ3,4

(cid:123)(cid:122)

γ2,4

(cid:33)
(cid:125)

= (x1, x2)

φ1,4φ1,2

The ‘messages’ γ1,4 and γ1,2 are the boundary separator tables when we choose the central clique as root
and carry out absorption towards the root. Given these ﬁxed messages we can then perform updates of
the root clique using

(9.4.29)

(9.4.30)

(9.4.31)

(9.4.32)

(9.4.33)

After making this update, we can subsequently update φ1,4 similarly using the constraint

¯φ1,5 ¯φ4,5

φ1,4φ1,2

= (x1, x4)

(cid:33)
(cid:125)

(cid:32)(cid:88)
(cid:124)

x3

(cid:33)
(cid:125)

¯φ2,4 ¯φ2,3 ¯φ3,4

(cid:123)(cid:122)

γ2,4

φnew
1,2 =

(x1, x2)
x4 γ1,4φ1,4γ2,4

(cid:80)
(cid:32)(cid:88)
(cid:124)

x5

(cid:88)

γ1,4

(cid:123)(cid:122)
(cid:80)

x2

so that

φnew
1,4 =

(x1, x4)
x2 γ1,4φ1,2γ2,4

Given converged updates for this clique, we can choose another clique as root, propagate towards the
root and compute the separator cliques on the boundary of the root. Given these ﬁxed boundary clique
potentials we perform IPF within the clique.

190

DRAFT March 9, 2010

Maximum Likelihood for Undirected models

Algorithm 6 Eﬃcient Iterative Proportional Fitting. Given a set of φi, i = 1, . . . , I and a corresponding
set of reference (empirical) marginal distributions on the variables of each potential, i, we aim to set all
φ such that all marginals of the Markov network match the given empirical marginals.

1: Given a Markov network on potentials φi, i = 1, . . . , I, triangulate the graph and form the cliques

C1, . . . ,CC.

Choose a clique c as root.
Propagate messages towards the root and compute the separators on the boundary of the root.
repeat

2: Assign potentials to cliques. Thus each clique has a set of associated potentials Fc
3: Initialise all potentials (for example to unity).
4: repeat
5:
6:
7:
8:
9:
10:
11: until All Markov network marginals converge to the reference marginals.

Choose a potential φi in clique c, i ∈ Fc.
Perform an IPF update for φi, given ﬁxed boundary separators and other potentials in c.

until Potentials in clique c converge.

This ‘eﬃcient’ IPF procedure is described more generally in algorithm(6) for an empirical distribution .
More generally, IPF minimises the Kullback-Leibler divergence between a given reference distribution 
and the Markov network. See demoIPFeff.m and IPF.m.

Example 45. In ﬁg(9.17) 36 examples of 18 × 14 = 252 binary pixel handwritten twos are presented,
forming the training set from which we wish to ﬁt a Markov network. First all pairwise empirical entropies
H(xi, xj), i, j = 1, . . . , 252 were computed and used to rank edges, with highest entropy edges ranked ﬁrst.
Edges were included in a graph G, highest ranked ﬁrst, provided the triangulated G had all cliques less than
size 15. This resulted in 238 unique cliques and an adjacency matrix for the triangulated G as presented in
ﬁg(9.16a). In ﬁg(9.16b) the number of times that a pixel appears in the 238 cliques is shown, and indicates
the degree of importance of each pixel in distinguishing between the 36 examples. Two models were then
trained and used to compute the most likely reconstruction based on missing data p(xmissing|xvisible). The
ﬁrst model was a Markov network on the maximal cliques of the graph, for which essentially no training
is required, and the settings for each clique potential can be obtained as explained in algorithm(5). The
model makes 3.8% errors in reconstruction of the missing pixels. Note that the unfortunate eﬀect of
reconstructing a white pixel surrounded by black pixels is an eﬀect of the limited training data. With
larger amounts of data the model would recognise that such eﬀects do not occur. In the second model,
the same maximal cliques were used, but the maximal clique potentials restricted to be the product of
all pairwise two-cliques within the maximal clique. This is equivalent to using a Boltzmann machine, and
was trained using the eﬃcient IPF approach of algorithm(6). The corresponding reconstruction error is
20%. This performance is worse than the unconstrained network since the Boltzmann machine is a highly
constrained Markov network. See demoLearnDecMN.m.

Figure 9.16: (a): Based on the pairwise
empirical entropies H(xi, xj) edges are or-
dered, high entropy edges ﬁrst. Shown
is the adjacency matrix of the resulting
Markov network whose junction tree has
cliques ≤ 15 in size (white represents an
(b): Indicated are the number of
edge).
cliques that each pixel is a member of, in-
dicating a degree of importance. Note that
the lowest clique membership value is 1, so
that each pixel is a member of at least one
clique.

(a)

(b)

DRAFT March 9, 2010

191

5010015020025050100150200250  24681012142468101214161820406080100120140160180200220Maximum Likelihood for Undirected models

Figure 9.17: Learning digits (from Simon Lucas’ algoval system) using a Markov network. Top row: the
36 training examples. Each example is a binary image on 18 × 14 pixels. Second row: the training data
with 50% missing pixels (grey represents a missing pixel). Third row: Reconstructions from the missing
data using a thin-junction-tree MN with maximum clique size 15. Bottom row: Reconstructions using a
thin-junction-tree Boltzmann machine with maximum clique size 15, trained using eﬃcient IPF.

9.4.5 Iterative scaling

We consider Markov networks of the exponential form

eθcfc(Vc)

(9.4.34)

where fc(Vc) ≥ 0 and c ranges of the non-maximal cliques Vc ⊂ V. The normalisation requirement is

(cid:89)

c

eθcfc(Vc)

p(V|θ) =

1

Z(θ)

Z(θ) =(cid:88)

(cid:89)

V

c

A Maximum Likelihood training algorithm for a Markov network, somewhat analogous to the EM ap-
proach of section(11.2) can be derived as follows[32]:

Consider the bound, for positive x:

log x ≤ x − 1 ⇒ − log x ≥ 1 − x

Hence

Z(θ)
Z(θold) ⇒ − log Z(θ) ≥ − log Z(θold) + 1 −

Z(θ)
Z(θold)

− log Z(θ)

Z(θold) ≥ 1 −
(cid:88)

1
N

L(θ) ≥

1
N

c,n

Then we can write a bound on the log likelihood

θcfc(V n

c ) − log Z(θold) + 1 −

Z(θ)
Z(θold)

(9.4.35)

(9.4.36)

(9.4.37)

(9.4.38)

(9.4.39)

(9.4.40)

(9.4.41)

As it stands, the bound (9.4.38) is in general not straightforward to optimise since the parameters of each
potential are coupled through the Z(θ) term. For convenience it is useful to ﬁrst reparmameterise and
write

Then

αc

c

c

(cid:124)

+θold

(cid:123)(cid:122)
(cid:125)
θc = θc − θold
Z(θ) =(cid:88)
(cid:80)
(cid:80)

c fc(Vc)θc =(cid:88)
(cid:80)
(cid:80)
c αcfc(Vc) = e

c pc[αc

V

e

V

e

d fd(Vd)]

(cid:80)

e

(cid:80)

c fc(Vc)θold
c e

c fc(Vc)αc

One can decouple this using an additional bound derived by ﬁrst considering:

192

DRAFT March 9, 2010

c pc = 1 we may apply Jensen’s inequality to give

Maximum Likelihood for Undirected models

e

where

pc ≡
(cid:80)

fc(Vc)(cid:80)
Since pc ≥ 0 and(cid:80)
d fd(Vd)
(cid:88)
c αcfc(Vc) ≤
(cid:88)
(cid:80)
(cid:40)
(cid:124)

Z(θ) ≤

L(θ) ≥

(cid:88)

Hence

1
N

V

c

1
N

d fd(Vd)αc

pce

(cid:80)
c (cid:88)
(cid:88)

c

fc(V n

c )θc −

c

n

Plugging this bound into (9.4.38) we have

c fc(Vc)θold

e

(cid:80)

f fd(Vc)

pceαc

(cid:80)

d fd(Vc)(cid:69)

p(V|θold)

(cid:41)
(cid:125)

+1 − log Z(θold)

(cid:68)

pceαc

(cid:123)(cid:122)

LB(θc)

(9.4.42)

(9.4.43)

(9.4.44)

(9.4.45)

The term in curly brackets contains the potential parameters θc in an uncoupled fashion. Diﬀerentiating
with respect to θc the gradient of each lower bound is given by

∂LB(θc)

∂θc

=

1
N

fc(V n

c ) −

(cid:88)

n

(cid:68)

c )(cid:80)
fc(Vc)e(θc−θold

d fd(Vd)(cid:69)

p(V|θold)

(9.4.46)

This can be used as part of a gradient based optimisation procedure to learn the parameters θc. A potential
advantage over IPF is that all the parameters may be updated simultaneously, whereas in IPF they must
be updated sequentially. Intuitively, the parameters converge when the empirical average of the functions
f match the average of the functions with respect to samples drawn from the distribution, in line with our
general condition for Maximum Likelihood optimal solution.

c fc(Vc) = 1, the zero of the gradient can be found

In the special case that the functions sum to 1, (cid:80)

analytically, giving the update

θc = θold

c + log

1
N

(cid:88)

n

fc(V n

c ) − log (cid:104)fc(Vc)(cid:105)p(Vc|θold)

(9.4.47)

The constraint that the features fc need to be non-negative can be relaxed at the expense of additional
variational parameters, see exercise(129). In cases where the zero of the gradient cannot be computed
analytically, there may be little advantage in general in using IS over standard gradient based procedures
on the log likelihood directly [196].

If the junction tree formed from this exponential form Markov network has limited tree width, compu-
tational savings can be made by performing IPF over the cliques of the junction tree and updating the
parameters θ within each clique using IS[11]. This is a modiﬁed version of the constrained decomposable
case. See also [274] for a uniﬁed treatment of propagation and scaling on junction trees.

9.4.6 Conditional random ﬁelds

For an input x and output y, a CRF is deﬁned by a conditional distribution [266, 166]

φk(y, x)

(9.4.48)

for (positive) potentials φk(y, x).To make learning more straightforward, the potentials are usually deﬁned
as eλkfk(y,x) for ﬁxed functions f(y, x) and parameters λk.
In this case the distribution of the output
conditioned on the input is

p(y|x) =

1

Z(x)

(cid:89)

k

(cid:89)

p(y|x, λ) =

1

Z(x, λ)

k

DRAFT March 9, 2010

eλkfk(y,x)

(9.4.49)

193

Maximum Likelihood for Undirected models

For an i.i.d. dataset of input-outputs, D = {(xn, yn), n = 1, . . . , N}, training based on conditional Maxi-
mum Likelihood requires the maximisation of

N(cid:88)

n=1

N(cid:88)

(cid:88)

n=1

k

L(λ) ≡

log p(yn|xn, λ) =

λkfk(yn, xn) − log Z(xn, λ)

(9.4.50)

In general no closed form solution for the optimal λ exists and this needs to be determined numerically.
First we note that equation (9.4.49) is equivalent to equation (9.4.34) where the parameters θ are here
denoted by λ and the variables v are here denoted by y. In the CRF case the inputs simply have the eﬀect
of determining the feature fk(y, x). In this sense iterative scaling, or any related method for Maximum
Likelihood training of constrained Markov networks, may be readily adapted, taking advantage also of any
computational savings from limited width junction trees.

As an alternative here we brieﬂy describe gradient based training. The gradient has components

L =(cid:88)

(cid:16)

n

∂
∂λi

(cid:17)

fi(yn, xn) − (cid:104)fi(y, xn)(cid:105)p(y|xn,λ)

(9.4.51)

The terms (cid:104)fi(y, xn)(cid:105)p(y|xn,λ) can be problematic and their tractability depends on the structure of the
potentials. For a multivariate y, provided the structure of the cliques deﬁned on subsets of y is singly-
connected, then computing the average is generally tractable. More generally, provided the cliques of the
resulting junction tree have limited width, then exact marginals are avaiable. An example of this is given
for a linear-chain CRF in section(23.4.3) – see also example(46) below.

Another quantity often useful for numerical optimisation is the Hessian which has components

L =(cid:88)

n

∂2

∂λi∂λj

((cid:104)fi(y, xn)(cid:105)(cid:104)fj(y, xn)(cid:105) − (cid:104)fi(y, xn)fj(y, xn)(cid:105))

(9.4.52)

where the averages above are with respect to p(y|xn, λ). This expression is a (negated) sum of covariance
elements, and is therefore negative (semi) deﬁnite. Hence the function L(λ) is concave and has only a
single global optimum. Whilst no closed form solution for the optimal λ exists, the optimal solutions can
be found easily using a numerical technique such as conjugate gradients.

In practice regularisation terms are often added to prevent overﬁtting (see section(13.2.3) for a discussion
of regularisation) . Using a term

kλ2
c2
k

(9.4.53)

(cid:88)

k

−

for positive regularisation constants c2
k discourages the weights λ from being too large. This term is also
negative deﬁnite and hence the overall objective function remains concave. Iterative Scaling may also be
used to train a CRF though in practice gradient based techniques are to be preferred[196].
Once trained a CRF can be used for predicting the output distribution for a novel input x∗. The most
likely output y∗ is equivalently given by
∗) = argmax

∗ = argmax

(cid:88)

(9.4.54)

λkfk(y, x

, λ)

y

∗) − log Z(x

∗

Since the normalisation term is independent of y, ﬁnding the most likely output is equivalent to

y

y

k

log p(y|x
(cid:88)

k

∗)
λkfk(y, x

y

∗ = argmax

y

194

(9.4.55)

DRAFT March 9, 2010

Maximum Likelihood for Undirected models

(a)

(b)

Figure 9.18: (a): Training results for a linear chain CRF. There are 5 training sequences, one per subpanel.
In each the top row corresponds to the input sequence x1:20, xt ∈ {1, . . . , 5} (each state represented by a
diﬀerent colour) the middle row, the correct output sequence y1:20, yt ∈ {1, 2, 3} (each state represented
by a diﬀerent colour). Together the input and output sequences make the training data D. The bottom
(b):
row contains the most likely output sequence given the trained CRF, arg maxy1:20 p(y1:20|x1:20,D).
Five additional test sequences along with the correct output and predicted output sequence.

Natural language processing

In a natural language processing application, xt might represent a word and yt a corresponding linguistic
tag (‘noun’,‘verb’, etc. ). A more suitable form in this case is to constrain the CRF to be of the form

(cid:32)(cid:88)

exp

µkgk(yt, yt−1) +(cid:88)

(cid:33)

ρlhl(yt, xt)

(9.4.56)

k

l

for binary functions gk and hl and parameters µk and ρl. The grammatical structure of tag-tag transitions
is encoded in gk(yt, yt−1) and linguistic tag information in hk(yt, xt), with the importance of these being
determined by the corresponding parameters[166]. In this case inference of the marginals (cid:104)ytyt−1|x1:T(cid:105) is
straightforward since the factor graph corresponding to the inference problem is a linear chain.

Variants of the linear chain CRF are used heavily in natural language processing, including part-of-speech
tagging and machine translation (in which the input sequence x represents a sentence say in English and
the output sequence y the corresponding translation into French). See, for example, [213].

Example 46 (Linear chain CRF). We consider a CRF with X = 5 input states and Y = 3 output states
of the form

T(cid:89)

t=2

k µkgk(yt,yt−1)+(cid:80)
(cid:80)

e

p(y1:T|x1:T ) =

l ρlhl(yt,xt)

(9.4.57)

Here the binary functions gk(yt, yt−1) = I [yt = ak] I [yt−1 = bk], k = 1, . . . , 9 model the transitions between
two consecutive outputs. The binary functions hl(yt, xt) = I [yt = al] I [xt = cl], l = 1, . . . , 15 model the
translation of the input to the output. There are therefore 9 + 15 = 24 parameters in total. In ﬁg(9.18)
we plot the training and test results based on a small set of data. The training of the CRF is obtained
using 50 iterations of gradient ascent with a learning rate of 0.1. See demoLinearCRF.m.

DRAFT March 9, 2010

195

2468101214161820123246810121416182012324681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123Properties of Maximum Likelihood

9.4.7 Pseudo likelihood

Consider a MN on variables x with dim x = D of the form

φc(Xc|θc)

(9.4.58)

For all but specially constrained φc, the partition function Z will be intractable and the likelihood of a
set of i.i.d. data intractable as well. A surrogate is to use the pseudo likelihood of the probability of
each variable conditioned on all other variables (which is equivalent to conditioning on only the variable’s
neighbours for a MN)

(cid:89)

c

p(x|θ) =

1
Z

N(cid:88)

D(cid:88)

n=1

i=1

(cid:48)(θ) =
L

log p(xn

i |xn\i|θ)

(9.4.59)

i |xn\i|θ) are usually straightforward to work out since they require ﬁnding the normalisation
The terms p(xn
of a univariate distribution only.
In this case the gradient can be computed exactly, and learning of
the parameters θ carried out. At least for the case of the Boltzmann machine, this forms a consistent
estimator[139].

9.4.8 Learning the structure

Learning the structure of a Markov network can also be based on independence tests, as for Belief net-
works. A criterion for ﬁnding a MN on a set of nodes V is to use the fact that no edge exits between x
and y if, conditioned on all other nodes, x and y are deemed independent. This is the pairwise Markov
property described in section(4.2.1). By checking x⊥⊥ y|V\{x, y} for every pair of variables x and y, this
edge deletion approach in principle reveals the structure of the network[219]. For learning the structure
from an oracle, this method is sound. However, a practical diﬃculty in the case where the independencies
are determined from data is that checking if x ⊥⊥ y| V\{x, y} requires in principle enormous amounts of
data. The reason for this is that the conditioning selects only those parts of the dataset consistent with the
conditioning. In practice this will result in very small numbers of remaining datapoints, and estimating
independencies on this basis is unreliable.

The Markov boundary criterion[219] uses the local Markov property, section(4.2.1), namely that condi-
tioned on its neighbours, a variable is independent of all other variables in the graph. By starting with
a variable x and an empty neighbourhood set, one can progressively include neighbours, testing if their
inclusion renders the remaining non-neighbours independent of x. A diﬃcultly with this is that, if one
doesn’t have the correct Markov boundary, then including a variable in the neighbourhood set may be
deemed necessary. To see this, consider a network which corresponds to a linear chain and that x is at the
edge of the chain. In this case, only the nearest neighbour of x is in the Markov boundary of x. However,
if this nearest neighbour were not currently in the set, then any other non-nearest neighbour would be
included, even though this is not strictly required. To counter this, the neighbourhood variables included
in the neighbourhood of x may be later removed if they are deemed superﬂuous to the boundary[102].

In cases where speciﬁc constraints are imposed, such as learning structures whose resulting triangulation
has a bounded tree-width, whilst still formally diﬃcult, approximate procedures are available[260].

In terms of network scoring methods for undirected networks, computing a score is hampered by the fact
that the parameters of each clique become coupled in the normalisation constant of the distribution. This
issue can be addressed using hyper Markov priors[75].

9.5 Properties of Maximum Likelihood

9.5.1 Training assuming the correct model class
Consider a dataset X = {xn, n = 1, . . . , N} generated from an underlying parametric model p(x|θ0). Our
interest is to ﬁt a model p(x|θ) of the same form as the correct underlying model p(x|θ0) and examine
196
DRAFT March 9, 2010

Code

whether if, in the limit of a large amount of data, the parameter θ learned by Maximum Likelihood matches
the correct parameter θ0. Our derivation below is non-rigorous, but highlights the essence of the argument.

Assuming the data is i.i.d., the log likelihood L(θ) ≡ log p(X|θ) is

N(cid:88)

n=1

L(θ) =

1
N

log p(xn|θ)

(9.5.1)

In the limit N → ∞, the sample average can be replaced by an average with respect to the distribution
generating the data

(cid:104)log p(x|θ)(cid:105)p(x|θ0) = −KL(cid:0)p(x|θ0)|p(x|θ)(cid:1) +(cid:10)log p(x|θ0)(cid:11)

L(θ) =N→∞

p(x|θ0)

(9.5.2)

Up to a negligible constant, this is the Kullback-Leibler divergence between two distributions in x, just
with diﬀerent parameter settings. The θ that maximises L(θ) is that which minimises the Kullback-
Leibler divergence, namely θ = θ0. In the limit of a large amount of data we can, in principle, learn the
correct parameters (assuming we know the correct model class). The property of an estimator such that
the parameter θ converges to the true model parameter θ0 as the sequence of data increase is termed a
consistency.

9.5.2 Training when the assumed model is incorrect
We write q(x|θ) for the assumed model, and p(x|φ) for the correct generating model. Repeating the above
calculations in the case of the assumed model being correct, we have that, in the limit of a large amount
of data, the likelihood is

L(θ) = (cid:104)log q(x|θ)(cid:105)p(x|φ) = −KL(p(x|φ)|q(x|θ)) + (cid:104)log p(x|φ)(cid:105)p(x|φ)

(9.5.3)

Since q and p are not of the same form, setting θ to φ does not necessarily minimise KL(p(x|φ)|q(x|θ)),
and therefore does not necessarily optimize L(θ).

9.6 Code

condindepEmp.m: Bayes test and Mutual Information for empirical conditional independence
condMI.m: Conditional Mutual Information
condMIemp.m: Conditional Mutual Information of Empirical distribution
MIemp.m: Mutual Information of Empirical distribution

9.6.1 PC algorithm using an oracle
This demo uses an oracle to determine x ⊥⊥ y | z, rather than using data to determine the empirical
dependence. The oracle is itself a Belief Network. For the partial orientation only the ﬁrst ‘unmarried
collider’ rule is implemented.
demoPCoracle.m: Demo of PC algorithm with an oracle
PCskeletonOracle.m: PC algorithm using an oracle
PCorient.m: Orient a skeleton

9.6.2 Demo of empirical conditional independence
For half of the experiments, the data is drawn from a distribution for which x⊥⊥ y| z is true. For the other
half of the experiments, the data is drawn from a random distribution for which x⊥⊥ y| z is false. We then
measure the fraction of experiments for which the Bayes test correctly decides x⊥⊥ y| z. We also measure
the fraction of experiments for which the Mutual Information test correctly decides x ⊥⊥ y| z, based on
197
DRAFT March 9, 2010

Fuse

Drum

Toner

Paper

Roller

Burning

Quality

Wrinkled

Mult. Pages

Paper Jam

Exercises

Figure 9.19: Printer Nightmare Belief Network. All variables are binary. The upper variables without
parents are possible problems (diagnoses), and the lower variables consequences of problems (faults).

setting the threshold equal to the median of all the empirical conditional mutual information values. A
similar empirical threshold can also be obtained for the Bayes’ factor (although this is not strictly kosher
in the pure Bayesian spirit since one should in principle set the threshold to zero). The test based on
the assumed chi-squared distributed MI is included for comparison, although it seems to be impractical
in these small data cases.
demoCondIndepEmp.m: Demo of empirical conditional independence based on data

9.6.3 Bayes Dirichlet structure learning

It is interesting to compare the result of demoPCdata.m with demoBDscore.m.
PCskeletonData.m: PC algorithm using empirical conditional independence
demoPCdata.m: Demo of PC algorithm with data
BDscore.m: Bayes Dirichlet (BD) score for a node given parents
learnBayesNet.m: Given an ancestral order and maximal parents, learn the network
demoBDscore.m: Demo of structure learning

9.7 Exercises

Exercise 116 (Printer Nightmare). Cheapco is, quite honestly, a pain in the neck. Not only did they buy
a dodgy old laser printer from StopPress and use it mercilessly, but try to get away with using substandard
components and materials. Unfortunately for StopPress, they have a contract to maintain Cheapco’s old
warhorse, and end up frequently sending the mechanic out to repair the printer. After the 10th visit, they
decide to make a statistical model of Cheapco’s printer, so that they will have a reasonable idea of the fault
based only on the information that Cheapco’s secretary tells them on the phone. In that way, StopPress
hopes to be able to send out to Cheapco only a junior repair mechanic, having most likely diagnosed the
fault over the phone.

Based on the manufacturer’s information, StopPress has a good idea of the dependencies in the printer,
and what is likely to directly aﬀect other printer components. The Belief Network in ﬁg(9.19) represents
these assumptions. However, the speciﬁc way that Cheapco abuse their printer is a mystery, so that the
exact probabilistic relationships between the faults and problems is idiosyncratic to Cheapco. StopPress has
the following table of faults for each of the 10 visits. Each column represents a visit.

fuse assembly malfunction

drum unit
toner out

poor paper quality

worn roller
burning smell

poor print quality

wrinkled pages

multiple pages fed

paper jam

0
0
1
1
0
0
1
0
0
0

0
0
1
0
0
0
1
0
0
0

0
0
0
1
0
0
1
1
1
1

1
0
0
0
0
1
0
0
0
1

0
1
0
1
0
0
1
0
0
0

0
0
1
0
0
0
1
0
0
0

0
0
0
1
1
0
0
0
1
1

0
1
1
0
0
0
1
0
0
1

0
0
0
1
0
0
0
1
1
1

0
0
0
1
0
0
0
0
0
1

0
1
0
0
0
0
1
0
0
0

0
1
1
1
0
0
1
0
0
0

1
0
0
1
0
1
0
1
0
0

0
0
0
0
1
0
0
1
0
1

1
0
0
0
1
0
0
1
1
0

198

DRAFT March 9, 2010

Exercises

1. The above table is contained in printer.mat. Learn all table entries on the basis of Maximum

Likelihood.

2. Program the Belief Network using the tables Maximum Likelihood tables and BRMLtoolbox. Com-
pute the probability that there is a fuse assembly malfunction given that the secretary complains there
is a burning smell and that the paper is jammed, and that there are no other problems.

3. Repeat the above calculation using a Bayesian method in which a ﬂat Beta prior is used on all tables.

4. Given the above information from the secretary, what is the most likely joint diagnosis over the
diagnostic variables – that is the joint most likely p(F use, Drum, T oner, P aper, Roller|evidence)?
Use the max-absorption method on the associated junction tree.

5. Compute the joint most likely state of the distribution

p(F use, Drum, T oner, P aper, Roller|burning smell, paper jammed)
Explain how to compute this eﬃciently using the max-absorption method.

(cid:80)N

Exercise 117. Explain how to use a factorised Beta prior in the case of learning table entries in Belief
Networks in which each variable has maximally a single parent. Consider the issues around Bayesian
Learning of binary table entries when the number of parental variables is not restricted.

Exercise 118. Consider data xn, n = 1, . . . , N. Show that for a Gaussian distribution, the Maximum
Likelihood estimator of the mean is ˆm = 1
N

n=1 xn and variance is ˆσ2 = 1

N

Exercise 119. A training set consists of one dimensional examples from two classes. The training exam-
ples from class 1 are

0.5, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.35, 0.25

and from class 2 are

0.9, 0.8, 0.75, 1.0

(9.7.1)

(9.7.2)

(cid:80)N
n=1(xn − ˆm)2.

Fit a (one dimensional) Gaussian using Maximum Likelihood to each of these two classes. Also estimate
the class probabilities p1 and p2 using Maximum Likelihood. What is the probability that the test point
x = 0.6 belongs to class 1?
Exercise 120. For a set of N observations (training data), X = x1, . . . , xN , and independently gathered
observations, the log likelihood for a Belief network to generate X is

N(cid:88)

K(cid:88)

n=1

i=1

log p(X ) =

log p (xn

i |pa (xn
i ))

We deﬁne the notation

(9.7.3)

(9.7.4)

(9.7.5)

(9.7.6)

199

meaning variable xi is in state s, and the parents of variable xi are in the vector of states t. Using a
Lagrangian

θi
s(t) = p(xi = s|pa (xi) = t)
N(cid:88)

K(cid:88)

L ≡

n=1

i=1

log p (xn

i |pa (xn

i )) +

(cid:80)N
(cid:80)N

n=1

I(cid:104)
I(cid:104)
(cid:80)

s

n=1

j = s
xn

(cid:105) I(cid:104)
(cid:105) I(cid:104)

pa

j = s
xn

pa

s (tj) =
θj

K(cid:88)
(cid:16)

i=1

(cid:16)

xn
j

(cid:32)

ti

λi
ti

(cid:88)
= tj(cid:105)
(cid:17)
(cid:17)
= tj(cid:105)

xn
j

Show that the Maximum Likelihood setting of θi

s(t) is

(cid:0)ti(cid:1)(cid:33)

(cid:88)

s

θi
s

1 −

DRAFT March 9, 2010

Exercises

N(cid:88)

n=1

CL(θ) =

1
N

Exercise 121 (Conditional Likelihood training). Consider a situation in which we partition observable
variables into disjoint sets x and y and that we want to ﬁnd the parameters that maximize the conditional
likelihood,

p(yn|xn, θ),

(9.7.7)

for a set of training data {(xn, yn) , n = 1, . . . , N}. All data is assumed generated from the same distribu-
tion p(x, y|θ0) = p(y|x, θ0)p(x|θ0) for some unknown parameter θ0. In the limit of a large amount of i.i.d.
training data, does CL(θ) have an optimum at θ0?

Exercise 122 (Moment Matching). One way to set parameters of a distribution is to match the moments
of the distribution to the empirical moments. This sometimes corresponds to Maximum Likelihood (for the
Gaussian distribution for example), though generally this is not consistent with Maximum Likelihood.
For data with mean m and variance s, show that to ﬁt a Beta distribution by moment matching, we use

α = m2(1 − m)

s

1 − m
m

(9.7.8)

Exercise 123. For data 0 ≤ xn ≤ 1, n = 1, . . . , N, generated from a Beta distribution B (x|a, b), show
that the log likelihood is given by

− m, β = α
N(cid:88)

n=1

N(cid:88)

n=1

N(cid:88)

n=1

L(a, b) ≡ (a − 1)

log xn + (b − 1)

log(1 − xn) − N log B(a, b)

(9.7.9)

where B(a, b) is the Beta function. Show that the derivatives are

∂
∂a

L =

log xn − ψ(a) − ψ(a + b),

∂
∂b

L =

log(1 − xn) − ψ(b) − ψ(a + b)

(9.7.10)

where ψ(x) ≡ d log Γ(x)/dx is the digamma function, and suggest a method to learn the parameters a,b.
Exercise 124. Consider the Boltzmann machine as deﬁned in example(44).

1. Derive the gradient with respect to the ‘biases’ wii.

2. Write down the pseudo likelihood for a set of i.i.d. data v1, . . . , vN and derive the gradient of this

N(cid:88)

n=1

Exercise 125. Show that the model likelihood equation (9.3.54) can be written explicitly as

with respect to wij, i (cid:54)= j.

p(D|M) =(cid:89)

(cid:89)

v

j

Γ ((cid:80)
Γ ((cid:80)

(cid:20)Γ (u(cid:48)

(cid:89)

(cid:21)

i ui(v; j))
i u(cid:48)
i(v; j))

i

i(v; j))
Γ (ui(v; j))

(9.7.11)

Exercise 126. Deﬁne the set N as consisting of 8 node Belief Networks in which each node has at most
2 parents. For a given ancestral order a, the restricted set is written Na

1. How many Belief Networks are in Na?
2. What is the computational time to ﬁnd the optimal member of Na using the Bayesian Dirichlet score,
assuming that computing the BD score of any member of Na takes 1 second and bearing in mind the
decomposability of the BD score.

3. What is the time to ﬁnd the optimal member of N ?

Exercise 127. For the Markov network

p(x, y, z) =

1
Z

φ1(x, y)φ2(y, z)

(9.7.12)

derive an iterative scaling algorithm to learn the unconstrained tables φ1(x, y) and φ2(x, y) based on a set
of i.i.d. data X ,Y,Z.
200

DRAFT March 9, 2010

Exercise 128. Given training data x1, . . . , xn, derive an iterative scaling algorithm for Maximum Likeli-
hood training of CRFs of the form

1

p(x|λ) =

eλcfc(x)

Z(λ)

where Z(λ) = (cid:80)
cannot all be zero for any given x).
(cid:81)
Exercise 129. For data X 1, . . . ,X N , consider Maximum Likelihood learning of a Markov network p(X ) =
c φc(Xc) with potentials of the form

c eλcfc(x) and non-negative features, fc(x) ≥ 0 (you may assume that the features

x

with fc(Xc) being general real valued functions and θc real valued parameters. By considering

(9.7.13)

(9.7.14)

Exercises

(cid:89)
(cid:81)

c

φc(Xc) = eθcfc(Xc)
(cid:88)

θcfc(Xc) =(cid:88)

c

θcfc(Xc)

for auxiliary variables pc > 0 such that(cid:80)

pc

pc

c

algorithm in which each parameter θc can be learned separately.

c pc = 1, explain how to derive a form of iterative scaling training

DRAFT March 9, 2010

201

Exercises

202

DRAFT March 9, 2010

CHAPTER 10

Naive Bayes

10.1 Naive Bayes and Conditional Independence

Naive Bayes (NB) is a popular classiﬁcation method and aids our discussion of conditional independence,
overﬁtting and Bayesian methods. In NB, we form a joint model of observations x and the corresponding
class label c using a Belief network of the form

p(x, c) = p(c)

p(xi|c)

(10.1.1)

whose Belief Network is depicted in ﬁg(10.1a). Coupled with a suitable choice for each conditional distri-

bution p(xi|c), we can then use Bayes’ rule to form a classiﬁer for a novel attribute vector x∗:

D(cid:89)

i=1

p(c|x∗) = p(x∗

|c)p(c)
p(x∗)

(cid:80)
= p(x∗
c p(x∗

|c)p(c)
|c)p(c)

(10.1.2)

In practice it is common to consider only two classes dom(c) = {0, 1}. The theory we describe below is
valid for any number of classes c, though our examples are restricted to the binary class case. Also, the
attributes xi are often taken to be binary, as we shall do initially below as well. The extension to more
than two attribute states, or continuous attributes is straightforward.

Example 47. EZsurvey.org considers Radio station listeners conveniently fall into two groups – the
‘young’ and ‘old’. They assume that, given the knowledge that a customer is either ‘young’ or ‘old’, this
is suﬃcient to determine whether or not a customer will like a particular Radio station, independent of
their likes or dislikes for any other stations:

p(R1, R2, R3, R4|age) = p(R1|age)p(R2|age)p(R3|age)p(R4|age)

(10.1.3)

where each of the variables R1, R2, R3, R4 can take the states either like or dislike, and the ‘age’ variable
can take the value either young or old. Thus the information about the age of the customer determines
the individual product preferences without needing to know anything else. To complete the speciﬁcation,
given that a customer is young, she has a 95% chance to like Radio1, a 5% chance to like Radio2, a 2%
chance to like Radio3 and a 20% chance to like Radio4. Similarly, an old listener has a 3% chance to like
Radio1, an 82% chance to like Radio2, a 34% chance to like Radio3 and a 92% chance to like Radio4.
They know that 90% of the listeners are old.

203

Estimation using Maximum Likelihood

c

cn

xn
i

θc

θi,c

i = 1 : D

x1

x2

x3

n = 1 : N

(a)

(b)

Figure 10.1: Naive Bayes classifer. (a): The central
assumption is that given the class c, the attributes xi
(b): Assuming the data is i.i.d.,
are independent.
Maximum Likelihood learns the optimal parameters
of the distribution p(c) and the class-dependent at-
tribute distributions p(xi|c).

Given this model, and a new customer that likes Radio1, and Radio3, but dislikes Radio2 and Radio4,
what is the probability that they are young? This is given by

p(age = young|R1 = like, R2 = dislike, R3 = like, R4 = dislike)

(cid:80)

= p(R1 = like, R2 = dislike, R3 = like, R4 = dislike|age = young)p(age = young)

age p(R1 = like, R2 = dislike, R3 = like, R4 = dislike|age)p(age)

Using the Naive Bayes structure, the numerator above is given by

p(R1 = like|age = young)p(R2 = dislike|age = young)

× p(R3 = like|age = young)p(R4 = dislike|age = young)p(age = young)

Plugging in the values we obtain

0.95 × 0.95 × 0.02 × 0.8 × 0.1 = 0.0014

(10.1.4)

(10.1.5)

The denominator is given by this value plus the corresponding term evaluated under assuming the customer
is old,

0.03 × 0.18 × 0.34 × 0.08 × 0.9 = 1.3219 × 10−4

Which gives

p(age = young|R1 = like, R2 = dislike, R3 = like, R4 = dislike) =

0.0014

0.0014 + 1.3219 × 10−4 = 0.9161 (10.1.6)

10.2 Estimation using Maximum Likelihood

Learning the table entries for NB is a straightforward application of the more general BN learning dis-
cussed in section(9.2.3). For a fully observed dataset, Maximum Likelihood learning of the table entries
corresponds to counting the number of occurrences in the training data, as we show below.

10.2.1 Binary attributes
Consider a dataset {xn, n = 1, . . . , N} of binary attributes, xn
i ∈ {0, 1}, i = 1, . . . , D. Each datapoint
xn has an associated class label cn. The number of datapoints from class c = 0 is n0 and the num-
ber from class c = 1 denoted is n1. For each attribute of the two classes, we need to estimate the
values p(xi = 1|c) ≡ θc
i . The other probability, p(xi = 0|c) is given by the normalisation requirement,
p(xi = 0|c) = 1 − p(xi = 1|c) = 1 − θc
i .

204

DRAFT March 9, 2010

Based on the NB conditional independence assumption the probability of observing a vector x can be
compactly written

Estimation using Maximum Likelihood

D(cid:89)

i=1

D(cid:89)

i=1

p(x|c) =

p(xi|c) =

(θc
i )xi(1 − θc

i )1−xi

In the above expression, xi is either 0 or 1 and hence each i term contributes a factor θc
i if xi = 1 or 1− θc
i
if xi = 0. Together with the assumption that the training data is i.i.d. generated, the log likelihood of the
attributes and class labels is

log p(xn, cn) =(cid:88)

log p(cn)(cid:89)

n

i

p(xn

i |cn)

i log θcn
xn

i + (1 − xn

i ) log(1 − θcn

i ) + n0 log p(c = 0) + n1 log p(c = 1)

This can be written more explicitly in terms of the parameters as

n

L =(cid:88)
=(cid:88)
L =(cid:88)

i,n

i,n

(cid:8)I [xn

i = 1, cn = 0] log θ0

i + I [xn

i = 0, cn = 0] log(1 − θ0

i ) + I [xn

i = 1, cn = 1] log θ1
i

i )(cid:9) + n0 log p(c = 0) + n1 log p(c = 1)

We can ﬁnd the Maximum Likelihood optimal θc

i by diﬀerentiating w.r.t. θc

i and equating to zero, giving

+ I [xn

i = 0, cn = 1] log(1 − θ1
(cid:80)

i = 1, cn = c]

I [xn

n

(cid:80)

θc
i = p(xi = 1|c) =
=

I [xn

i = 0, cn = c] + I [xn

n

i = 1, cn = c]

number of times xi = 1 for class c
number of datapoints in class c

(10.2.1)

(10.2.2)

(10.2.3)

(10.2.4)

(10.2.5)

(10.2.6)

(10.2.7)

(10.2.8)

(10.2.9)

Similarly, optimising equation (10.2.3) with respect to p(c) gives

p(c) =

number of times class c occurs
total number of data points

Classiﬁcation boundary
We classify a novel input x∗ as class 1 if

p(c = 1|x∗) > p(c = 0|x∗)

Using Bayes’ rule and writing the log of the above expression, this is equivalent to

(cid:88)
(cid:8)x

i

log p(x∗

|c = 1) + log p(c = 1) − log p(x∗) > log p(x∗

|c = 0) + log p(c = 0) − log p(x∗)

From the deﬁnition of the classiﬁer, this is equivalent to (the normalisation constant − log p(x∗) can be

dropped from both sides)

∗
i|c = 1) + log p(c = 1) >
log p(x

log p(x

(cid:88)

i

∗
i|c = 0) + log p(c = 0)
(cid:88)

∗
i log θ1

(cid:88)
Using the binary encoding xi ∈ {0, 1}, we classify x∗ as class 1 if
(cid:8)x
∗
i log θ0
This decision rule can be expressed in the form: classify x∗ as class 1 if(cid:80)

i )(cid:9)+log p(c = 1) >

∗
i ) log(1 − θ1

i + (1 − x

i

i

i + (1 − x

i wix∗

i + a > 0 for some suitable
choice of weights wi and constant a, see exercise(133). The interpretation is that w speciﬁes a hyperplane
in the attribute space and x∗ is classiﬁed as 1 if it lies on the positive side of the hyperplane.

(10.2.10)

i )(cid:9)+log p(c = 0)

∗
i ) log(1 − θ0

(10.2.11)

DRAFT March 9, 2010

205

Estimation using Maximum Likelihood

0
0
1
1
1

1
0
1
1
0

1
1
0
0
0

1
1
0
0
1
(a)

0
1
0
0
1

0
0
0
1
0

1
0
0
1
1

1
1
0
0
1

1
1
1
1
0

1
1
0
1
0
(b)

1
1
0
1
1

1
0
1
1
0

1
0
1
0
0

Figure 10.2: (a): English tastes over attributes
(shortbread, lager, whiskey, porridge, f ootball).
Each column represents the tastes of an indi-
vidual. (b): Scottish tastes.

Example 48 (Are they Scottish?). Consider the following vector of attributes:

(likes shortbread, likes lager, drinks whiskey, eats porridge, watched England play football) (10.2.12)

A vector x = (1, 0, 1, 1, 0)T would describe that a person likes shortbread, does not like lager,
drinks whiskey, eats porridge, and has not watched England play football. Together with each vector
x, there is a label nat describing the nationality of the person, dom(nat) = {scottish, english}, see ﬁg(10.2).
We wish to classify the vector x = (1, 0, 1, 1, 0)T as either scottish or english. We can use Bayes’ rule to
calculate the probability that x is Scottish or English:

p(scottish|x) = p(x|scottish)p(scottish)

p(x)

=

p(x|scottish)p(scottish)

p(x|scottish)p(scottish) + p(x|english)p(english)

(10.2.13)

By Maximum Likelihood the ‘prior’ class probability p(scottish) is given by the fraction of people in the
database that are Scottish, and similarly p(english) is given as the fraction of people in the database that
are English. This gives p(scottish) = 7/13 and p(english) = 6/13.

For p(x|nat) under the Naive Bayes assumption:

p(x|nat) = p(x1|nat)p(x2|nat)p(x3|nat)p(x4|nat)p(x5|nat)

(10.2.14)

so that knowing whether not someone is Scottish, we don’t need to know anything else to calculate the
probability of their likes and dislikes. Based on the table in ﬁg(10.2) and using Maximum Likelihood we
have:

p(x1 = 1|english) = 1/2
p(x2 = 1|english) = 1/2
p(x3 = 1|english) = 1/3
p(x4 = 1|english) = 1/2
p(x5 = 1|english) = 1/2
For x = (1, 0, 1, 1, 0)T, we get

p(x1 = 1|scottish) = 1
p(x2 = 1|scottish) = 4/7
p(x3 = 1|scottish) = 3/7
p(x4 = 1|scottish) = 5/7
p(x5 = 1|scottish) = 3/7

p(scottish|x) =

1 × 3

7 × 3

7 × 5

1 × 3
7 × 4

7 × 3
7 × 7

7 × 5
13 + 1

7 × 4
2 × 1

7 × 7
2 × 1

13

3 × 1

2 × 1

2 × 6

13

(10.2.15)

= 0.8076

(10.2.16)

Since this is greater than 0.5, we would classify this person as being Scottish.

Small data counts

In example(48), consider trying to classify the vector x = (0, 1, 1, 1, 1)T. In the training data, all Scottish
people say they like shortbread. This means that for this particular x, p(x, scottish) = 0, and therefore
that we make the extremely conﬁdent classiﬁcation p(scottish|x) = 0. This demonstrates a diﬃculty using
Maximum Likelihood with sparse data. One way to ameliorate this is to smooth the probabilities, for
example by adding a certain small number to the frequency counts of each attribute. This ensures that

206

DRAFT March 9, 2010

Estimation using Maximum Likelihood

there are no zero probabilities in the model. An alternative is to use a Bayesian approach that discourages
extreme probabilities, as discussed in section(10.3).

Potential pitfalls with encoding

In many oﬀ-the-shelf packages implementing Naive Bayes, binary attributes are assumed.
In practice,
however, the case of non-binary attributes often occurs. Consider the following attribute : age.
In a
survey, a person’s age is marked down using the variable a ∈ 1, 2, 3. a = 1 means the person is between
0 and 10 years old, a = 2 means the person is between 10 and 20 years old, a = 3 means the person is
older than 20. One way to transform the variable a into a binary representation would be to use three
binary variables (a1, a2, a3) with (1, 0, 0), (0, 1, 0), (0, 0, 1) representing a = 1, a = 2, a = 3 respectively.
This is called 1 − of − M coding since only 1 of the binary variables is active in encoding the M states.
By construction, means that the variables a1, a2, a3 are dependent – for example, if we know that a1 = 1,
we know that a2 = 0 and a3 = 0. Regardless of any class conditioning, these variables will always be
dependent, contrary to the assumption of Naive Bayes. A correct approach is to use variables with more
than two states, as explained in section(10.2.2).

10.2.2 Multi-state variables
For a variable xi with more than two states, dom(xi) = {1, . . . , S}, the likelihood of observing a state
xi = s is denoted

s(c)

with(cid:80)
p(xi = s|c) = θi
s p(xi = s|c) = 1. For a set of data vectors xn,n = 1, . . . N, belonging to class c, under the i.i.d.
N(cid:89)

assumption, the likelihood of the NB model generating data from class c is

N(cid:89)

D(cid:89)

S(cid:89)

C(cid:89)

(10.2.17)

I[xn

i =s]I[cn=c]

θi
s(c)

(10.2.18)

n=1

n=1

i=1

s=1

c=1

which gives the class conditional log-likelihood

L =

I [xn

i = s] I [cn = c] log θi

s(c)

(10.2.19)

p(xn|cn) =
S(cid:88)
D(cid:88)
N(cid:88)

C(cid:88)

n=1

i=1

s=1

c=1

N(cid:88)

D(cid:88)

S(cid:88)

C(cid:88)

We can optimize with respect to the parameters θ using a Lagrange multiplier (one for each of the attributes
i and classes c) to ensure normalisation:

L(θ) =

I [xn

i = s] I [cn = c] log θi

s(c) +

n=1

i=1

s=1

c=1

c=1

i=1

θi
s(c)

(10.2.20)

To ﬁnd the optimum of this function we may diﬀerentiate with respect to θi
the resulting equation we obtain

s(c) and equate to zero. Solving

C(cid:88)

D(cid:88)

(cid:32)
1 −

S(cid:88)

s=1

λc
i

(cid:33)

N(cid:88)

n=1

I [xn

i = s] I [cn = c]

s(c)
θi

= λc
i

Hence, by normalisation,

θi
s(c) = p(xi = s|c) =

(cid:80)
s(cid:48),n(cid:48) I(cid:2)xn(cid:48)
(cid:80)

I [xn

n

i = s] I [cn = c]

i = s(cid:48)(cid:3) I [cn(cid:48) = c]

(10.2.21)

(10.2.22)

The Maximum Likelihood setting for the parameter p(xi = s|c) equals the relative number of times that
attribute i is in state s for class c.

DRAFT March 9, 2010

207

Bayesian Naive Bayes

Figure 10.3: Bayesian Naive Bayes with a factorised prior on the class
conditional attribute probabilities p(xi = s|c). For simplicity we assume
that the class probability θc ≡ p(c) is learned with Maximum Likelihood,
so that no distribution is placed over this parameter.

θc

n = 1 : N

cn

xn
i

θi,c

c = 1 : C

i = 1 : D

10.2.3 Text classiﬁcation

Consider a set of documents about politics, and another set about sport. Our interest is to make a
method that can automatically classify a new document as pertaining to either sport or politics. We
search through both sets of documents to ﬁnd the 100 most commonly occurring words. Each document
is then represented by a 100 dimensional vector representing the number of times that each of the words
occurs in that document – the so called bag of words representation (this is a crude representation of the
document since it discards word order). A Naive Bayes model speciﬁes a distribution of these number of
occurrences p(xi|c), where xi is the count of the number of times word i appears in documents of type c.
One can achieve this using either a multistate representation (as discussed in section(10.2.2)) or using a
continuous xi to represent the frequency of word i in the document. In this case p(xi|c) could be conve-
niently modelled using for example a Beta distribution.

Intuitively a
Despite the simplicity of Naive Bayes, it can classify documents surprisingly well[125].
potential justiﬁcation for the conditional independence assumption is that if we know a document is about
politics, this is a good indication of the kinds of other words we will ﬁnd in the document. Because Naive
Bayes is a reasonable classiﬁer in this sense, and has minimal storage and fast training, it has been applied
to time-storage critical applications, such as automatically classifying webpages into types[289], and spam
ﬁltering[9].

10.3 Bayesian Naive Bayes

To predict the class c of an input x we use

p(c|x,D) ∝ p(x,D, c)p(c|D) ∝ p(x|D, c)p(c|D)

For convenience we will simply set p(c|D) using Maximum Likelihood

(cid:88)

n

p(c|D) =

1
N

I [cn = c]

(10.3.1)

(10.3.2)

However, as we’ve seen, setting the parameters of p(x|D, c) using Maximum Likelihood training can yield
over-conﬁdent predictions in the case of sparse data. A Bayesian approach that addresses this diﬃculty
s(c) that discourage extreme values. The model is
is to use priors on the probabilities p(xi = s|c) ≡ θi
depicted in ﬁg(10.3).

The prior

p(θ) =(cid:89)

i,c

208

We will use a prior on the table entries and make the global factorisation assumption (see section(9.3))

p(θi(c))

(10.3.3)

DRAFT March 9, 2010

Bayesian Naive Bayes

We consider discrete xi each of which take states from 1, . . . , S. In this case p(xi = s|c) corresponds to
a multinomial distribution, for which the conjugate prior is a Dirichlet distribution. Under the factorised
prior assumption (10.3.3) we deﬁne a prior for each attribute i and class c,

p(θi(c)) = Dirichlet(cid:0)θi(c)|ui(c)(cid:1)

where ui(c) is the hyperparameter vector of the Dirichlet distribution for table p(xi|c).
The posterior

First let’s see how the Bayesian approach is used to classify a novel point x∗. Let D denote the training
data (xn, cn), n = 1, . . . , N. From equation (10.3.14), the term p(x∗,D|c∗) is computed using the following

decomposition:

(cid:90)

(cid:90)

p(x∗

∗) =

,D|c

p(x∗

∗) =

,D, θ|c

θ

p(x∗

|θ,  D, c

∗)p(θ,D|  c

∗) ∝

θ

p(x∗

|θ(c

∗))p(θ(c

∗)|D)

(10.3.5)

θ(c∗)

(cid:90)

Hence in order to make a prediction, we require the parameter posterior. Consistent with our general
Bayesian BN training result in section(9.3), the parameter posterior factorises

For Dirichlet hyperparameters ui(c∗) the above equation updates the hyperparameter by the number of
times variable i is in state s for class c∗ data. A common default setting is to take all components of u to
be 1.

p(x

∗
∗)
i|D, c

(10.3.10)

p(x

∗
i = s|θ(c

∗))p(θ(c

∗)|D) =

(cid:90)

θ(c∗)

∗)p(θ(c

θi
s(c

∗)|D)

θ(c∗)

(10.3.4)

(10.3.6)

(10.3.7)

(10.3.8)

(10.3.9)

(10.3.11)

(10.3.12)

(10.3.13)

209

∗)|D) =(cid:89)

i

p(θ(c

where

p(θi(c

p(θi(c

∗)|D)
∗)) (cid:89)
∗)|D) ∝ p(θi(c
∗)|D) = Dirichlet(cid:0)θi(c
∗) + (cid:88)
(cid:2)ˆui(c
∗)(cid:3)

∗)(cid:1)
∗)|ˆui(c
where the vector ˆui(c∗) has components

n:cn=c∗

p(θi(c

I [xn

i = s]

s = ui

s(c

p(xn

i |θi(c

n:cn=c∗

∗))

By conjugacy, the posterior for class c∗ is a Dirichlet distribution,

Classiﬁcation

The class distribution is given by

∗
p(c

|x∗

To compute p(x∗

|D, c

∗
|D)p(x∗
,D) ∝ p(c
(cid:90)
|D, c∗) we use
∗) =
∗
i = s, θ(c

p(x

θ(c∗)

p(x

∗
i = s|D, c
(cid:90)

Using the general identity

|D)(cid:89)
(cid:90)

i

∗) =

∗) = p(c
∗

∗)|D, c
(cid:90) (cid:89)

s(cid:48)

θsDirichlet (θ|u) dθ =

1

Z(u)

θus(cid:48)−1+I[s(cid:48)=s]dθ = Z(u(cid:48))
Z(u)

(cid:26) us

(cid:48)
s =
u

s (cid:54)= s(cid:48)
us + 1 s = s(cid:48)

DRAFT March 9, 2010

where Z(u) is the normalisation constant of the Dirichlet distribution Dirichlet (·|u) and

x1

x2

x3

x4

Figure 10.4: A Chow-Liu Tree in which each variable xi has at most one
parent. The variables may be indexed such that 1 ≤ i ≤ D.

Tree Augmented Naive Bayes

we obtain

∗
p(c

|x∗

∗
,D) ∝ p(c

|D)(cid:89)

i

Z(u∗i(c∗))
Z(ˆui(c∗))

where

∗i
s (c
u

∗) = ˆui

s(c

∗) + I [x

∗
i = s]

(10.3.14)

(10.3.15)

Example 49 (Bayesian Naive Bayes). Repeating the previous analysis for the ‘Are they Scottish?’ data
from example(48), the probability under a uniform Dirichlet prior for all the tables, gives a value of 0.236
for the probability that (1, 0, 1, 1, 0) is Scottish, compared with a value of 0.192 under the standard Naive
Bayes assumption.

10.4 Tree Augmented Naive Bayes

A natural extension of Naive Bayes is to relax the assumption that the attributes are independent given
the class:

p(x|c) (cid:54)=

p(xi|c)

(10.4.1)

The question then arises – which structure should we choose for p(x|c)? As we saw in section(9.3.5),
learning a structure is computationally infeasible for all but very small numbers of attributes. A practical
algorithm requires a speciﬁc form of constraint on the structure. To do this we ﬁrst make a digression into
the Maximum Likelihood learning of trees constrained to have at most a single parent.

10.4.1 Chow-Liu Trees

Consider a multivariate distribution p(x) that we wish to approximate with a distribution q(x). Further-
more, we constrain the approximation q(x) to be a Belief Network in which each node has at most one
parent. First we assume that we have chosen a particular labelling of the variables 1 ≤ i ≤ D, for which
the DAG single parent constraint means

D(cid:89)

i=1

D(cid:89)

i=1

D(cid:88)

(cid:10)log q(xi|xpa(i))(cid:11)

q(x) =

q(xi|xpa(i)),

pa(i) < i, or pa(i) = ∅

(10.4.2)

where pa(i) is the single parent index of node i. To ﬁnd the best approximating distribution q in this
constrained class, we may minimise the Kullback-Leibler divergence

KL(p|q) = (cid:104)log p(x)(cid:105)p(x) −

Since p(x) is ﬁxed, the ﬁrst term is constant. By adding a term(cid:10)log p(xi|xpa(i))(cid:11)

p(xi,xpa(i))

i=1

on p(x) alone, we can write

(10.4.3)

p(xi,xpa(i)) that depends

(cid:68)(cid:10)log q(xi|xpa(i))(cid:11)

D(cid:88)

i=1

(cid:10)log p(xi|xpa(i))(cid:11)

(cid:69)

p(xi|xpa(i)) −

p(xi|xpa(i))

(10.4.4)

p(xpa(i))

KL(p|q) = const. −

210

DRAFT March 9, 2010

Tree Augmented Naive Bayes

Algorithm 7 Chow-Liu Trees

end for

for j = 1 to D do

Compute the Mutual Information for the pair of variables xi, xj: wij = MI(xi; xj)

1: for i = 1 to D do
2:
3:
4:
5: end for
6: For the undirected graph G with edge weights w, ﬁnd a maximum weight undirected spanning tree T
7: Choose an arbitrary variable as the root node of the tree T .
8: Form a directed tree by orienting all edges away from the root node.

This enables us to recognise that, up to a negligible constant, the overall Kullback-Leibler divergence is a
positive sum of individual Kullback-Leibler divergences so that the optimal setting is therefore

(10.4.5)
Plugging this solution into equation (10.4.3) and using log p(xi|xpa(i)) = log p(xi, xpa(i)) − log p(xpa(i)) we
obtain

q(xi|xpa(i)) = p(xi|xpa(i))
D(cid:88)

KL(p|q) = const. −

i=1

(cid:10)log p(xi, xpa(i))(cid:11)

p(xi,xpa(i)) +

p(xpa(i))

(10.4.6)

(cid:10)log p(xpa(i))(cid:11)

D(cid:88)

i=1

We still need to ﬁnd the optimal parental structure pa(i) that minimises the above expression. If we add
and subtract an entropy term we can write

(cid:10)log p(xi, xpa(i))(cid:11)

(cid:10)log p(xpa(i))(cid:11)

D(cid:88)

i=1

D(cid:88)

i=1

KL(p|q) = −

p(xi,xpa(i)) +

p(xpa(i)) +

(cid:104)log p(xi)(cid:105)p(xi)

D(cid:88)

i=1

−

(cid:104)log p(xi)(cid:105)p(xi) + const.

(10.4.7)

For two variables xi and xj and distribution p(xi, xj), the mutual information deﬁnition(87) can be written
as

MI(xi; xj) =

log p(xi, xj)
p(xi)p(xj)

p(xi,xj )

(10.4.8)

(cid:29)

which can be seen as the Kullback-Leibler divergence KL(p(xi, xj)|p(xi)p(xj)) and is therefore non-
negative. Using this, equation (10.4.7) is

D(cid:88)

i=1

(cid:28)

D(cid:88)

MI(cid:0)xi; xpa(i)

(cid:1)

D(cid:88)

D(cid:88)

MI(cid:0)xi; xpa(i)

(cid:1)

KL(p|q) = −

Since our task is to ﬁnd the parental indices pa(i), and the entropic term(cid:80)

(cid:104)log p(xi)(cid:105)p(xi) + const.

−

i=1

i=1

i (cid:104)log p(xi)(cid:105)p(xi) is independent
of this mapping, ﬁnding the optimal mapping is equivalent to maximising the summed mutual informations

(10.4.9)

(10.4.10)

i=1

under the constraint that pa(i) ≤ i. Since we also need to choose the optimal initial labelling of the
variables as well, the problem is equivalent to computing all the pairwise mutual informations

wij = MI(xi; xj)

(10.4.11)

and then ﬁnding a maximal spanning tree for the graph with edge weights w (see spantree.m). This can
be thought of as a form of breadth-ﬁrst-search[61]. Once found, we need to identify a directed tree with
at most one parent. This is achieved by choosing an arbitrary node and then orienting edges consistently
away from this node.

DRAFT March 9, 2010

211

Tree Augmented Naive Bayes

c

x1

x2

x3

x4

Figure 10.5: Tree Augmented Naive (TAN) Bayes. Each variable xi has
at most one parent. The Maximum Likelihood optimal TAN structure is
computed using a modiﬁed Chow-Liu algorithm in which the conditional
mutual information MI(xi; xj|c) is computed for all i, j. A maximum
weight spanning tree is then found and turned into a directed graph by
orienting the edges outwards from a chosen root node. The table entries
can then be read oﬀ using the usual Maximum Likelihood counting argu-
ment.

Maximum likelihood Chow-Liu trees

N(cid:88)

n=1

If p(x) is the empirical distribution

p(x) =

1
N

then

δ (x, xn)

KL(p|q) = const. −

(cid:88)

n

1
N

log q(xn)

(10.4.12)

(10.4.13)

(10.4.14)

Hence the approximation q that minimises the Kullback-Leibler divergence between the empirical distri-
bution and p is equivalent to that which maximises the likelihood of the data. This means that if we use
the mutual information found from the empirical distribution, with

p(xi = a, xj = b) ∝ (cid:93) (xi = a, xj = b)

then the Chow-Liu tree produced corresponds to the Maximum Likelihood solution amongst all single-
parent trees. An outline of the procedure is given in algorithm(7). An eﬃcient algorithm for sparse data
is also available[190].

Remark 10 (Learning Tree structured Belief Networks). The Chow-Liu algorithm pertains to the
discussion in section(9.3.5) on learning the structure of Belief networks from data. Under the special
constraint that each variable has at most one parent, the Chow-Liu algorithm returns the Maximum
Likelihood structure to ﬁt the data.

10.4.2 Learning tree augmented Naive Bayes networks
For a distribution p(x|c) of the form of a tree structure with a single-parent constraint we can readily ﬁnd
the class conditional Maximum Likelihood solution by computing the Chow-Liu tree for each class. One
then adds links from the class node c to each variable and learns the class conditional probabilities from
c to x, which can be read oﬀ for Maximum Likelihood using the usual counting argument. Note that this
would generally result in a diﬀerent Chow-Liu tree for each class.

Practitioners typically constrain the network to have the same structure for all classes. The Maximum
Likelihood objective under the TAN constraint then corresponds to maximising the conditional mutual
information[97]

MI(xi; xj|c) = (cid:104)KL(p(xi, xj|c)|p(xi|c)p(xj|c))(cid:105)p(c)

(10.4.15)

see exercise(136). Once the structure is learned one subsequently sets parameters by Maximum Likelihood
counting. Techniques to prevent overﬁtting are discussed in [97] and can be addressed using Dirichlet
priors, as for the simpler Naive Bayes structure.

One can readily consider less restrictive structures than single-parent Belief Networks. However, the
complexity of ﬁnding optimal BN structures is generally computationally infeasible and heuristics are
required to limit the search space.

212

DRAFT March 9, 2010

Exercises

10.5 Code

NaiveBayesTrain.m: Naive Bayes trained with Maximum Likelihood
NaiveBayesTest.m: Naive Bayes test
NaiveBayesDirichletTrain.m: Naive Bayes trained with Bayesian Dirichlet
NaiveBayesDirichletTest.m: Naive Bayes testing with Bayesian Dirichlet

demoNaiveBayes.m: Demo of Naive Bayes

10.6 Exercises

Exercise 130. A local supermarket specializing in breakfast cereals decides to analyze the buying patterns
of its customers. They make a small survey asking 6 randomly chosen people their age (older or younger
than 60 years) and which of the breakfast cereals (Cornﬂakes, Frosties, Sugar Puﬀs, Branﬂakes) they like.
Each respondent provides a vector with entries 1 or 0 corresponding to whether they like or dislike the
cereal. Thus a respondent with (1101) would like Cornﬂakes, Frosties and Branﬂakes, but not Sugar Puﬀs.
The older than 60 years respondents provide the following data (1000), (1001), (1111), (0001). The younger
than 60 years old respondents responded (0110), (1110). A novel customer comes into the supermarket and
says she only likes Frosties and Sugar Puﬀs. Using Naive Bayes trained with maximum likelihood, what is
the probability that she is younger than 60?

Exercise 131. A psychologist does a small survey on ‘happiness’. Each respondent provides a vector with
entries 1 or 0 corresponding to whether they answer ‘yes’ to a question or ‘no’, respectively. The question
vector has attributes

x = (rich, married, healthy)

(10.6.1)

Thus, a response (1, 0, 1) would indicate that the respondent was ‘rich’, ‘unmarried’, ‘healthy’. In addition,
each respondent gives a value c = 1 if they are content with their lifestyle, and c = 0 if they are not. The fol-
lowing responses were obtained from people who claimed also to be ‘content’ : (1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1)
and for ‘not content’: (0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0).

1. Using Naive Bayes, what is the probability that a person who is ‘not rich’, ‘married’ and ‘healthy’ is

‘content’?

2. What is the probability that a person who is ‘not rich’ and ‘married’ is ‘content’? (That is, we do

not know whether or not they are ‘healthy’).

3. Consider the following vector of attributes :

x1 = 1 if customer is younger than 20 ; x1 = 0 otherwise

x2 = 1 if customer is between 20 and 30 years old ; x2 = 0 otherwise

x3 = 1 if customer is older than 30 ; x3 = 0 otherwise

x4 = 1 if customer walks to work ; x4 = 0 otherwise

(10.6.2)

(10.6.3)

(10.6.4)

(10.6.5)

Each vector of attributes has an associated class label ‘rich’ or ‘poor’. Point out any potential
diﬃculties with using your previously described approach to training using Naive Bayes. Hence
describe how to extend your previous Naive Bayes method to deal with this dataset.

Exercise 132. Whizzco decide to make a text classiﬁer. To begin with they attempt to classify documents
as either sport or politics. They decide to represent each document as a (row) vector of attributes describing
the presence or absence of words.

x = (goal, football, golf, defence, oﬀence, wicket, oﬃce, strategy)

DRAFT March 9, 2010

(10.6.6)

213

Exercises

Training data from sport documents and from politics documents is represented below in MATLAB using
a matrix in which each row represents the 8 attributes.

xP=[1 0 1 1 1 0 1 1; % Politics

0 0 0 1 0 0 1 1;
1 0 0 1 1 0 1 0;
0 1 0 0 1 1 0 1;
0 0 0 1 1 0 1 1;
0 0 0 1 1 0 0 1]

xS=[1 1 0 0 0 0 0 0; % Sport

0 0 1 0 0 0 0 0;
1 1 0 1 0 0 0 0;
1 1 0 1 0 0 0 1;
1 1 0 1 1 0 0 0;
0 0 0 1 0 1 0 0;
1 1 1 1 1 0 1 0]

Using a Naive Bayes classiﬁer, what is the probability that the document x = (1, 0, 0, 1, 1, 1, 1, 0) is about
politics?
Exercise 133. A Naive Bayes Classiﬁer for binary attributes xi ∈ {0, 1} is parameterised by θ1
i = p(xi =
1|class = 1), θ0
i = p(xi = 1|class = 0), and p1 = p(class = 1) and p0 = p(class = 0). Show that the
decision boundary to classify a datapoint x can be written as wTx + b > 0, and state explicitly w and b as
a function of θ1, θ0, p1, p0.
Exercise 134. This question concerns spam ﬁltering. Each email is represented by a vector

x = (x1, . . . , xD)

(10.6.7)
where xi ∈ {0, 1}. Each entry of the vector indicates if a particular symbol or word appears in the email.
The symbols/words are

money, cash, !!!, viagra, . . . , etc.

(10.6.8)

So that x2 = 1 if the word ‘cash’ appears in the email. The training dataset consists of a set of vectors
along with the class label c, where c = 1 indicates the email is spam, and c = 0 not spam. Hence, the
training set consists of a set of pairs (xn, cn), n = 1, . . . , N. The Naive Bayes model is given by

p(c, x) = p(c)

p(xi|c)

(10.6.9)

1. Draw a Belief Network for this distribution.

2. Derive expressions for the parameters of this model in terms of the training data using Maximum

Likelihood. Assume that the data is independent and identically distributed

D(cid:89)

i=1

N(cid:89)

p(c1, . . . , cN , x1, . . . , xN ) =

p(cn, xn)

Explicitly, the parameters are

n=1

p(c = 1), p(xi = 1|c = 1), p(xi = 1|c = 0), i = 1, . . . , D

(10.6.10)

(10.6.11)

3. Given a trained model p(x, c), explain how to form a classiﬁer p(c|x).
4. If ‘viagra’ never appears in the spam training data, discuss what eﬀect this will have on the classiﬁ-

cation for a new email that contains the word ‘viagra’.

5. Write down an expression for the decision boundary

and show that it can be written in the form

p(c = 1|x) = p(c = 0|x)
D(cid:88)

udxd − b = 0

d=1

for suitably deﬁned u and b.

214

(10.6.12)

(10.6.13)

DRAFT March 9, 2010

Exercises

Exercise 135. For a distribution p(x, c) and an approximation q(x, c), show that when p(x, c) corresponds
to the empirical distribution, ﬁnding q(x, c) that minimises the Kullback-Leibler divergence

KL(p(x, c)|q(x, c))

corresponds to Maximum Likelihood training of q(x, c).

Exercise 136. Consider a distribution p(x, c) and a Tree Augmented approximation

q(x, c) = q(c)(cid:89)

(10.6.14)

(10.6.15)

i

q(xi|xpa(i), c),
(cid:28)

(cid:88)

pa(i) < i or pa(i) = ∅

(cid:29)

Show that for the optimal q(x, c) constrained as above, the solution q(x, c) that minimises KL(p(x, c)|q(x, c))
when plugged back into the Kullback-Leibler expression gives, as a function of the parental structure,

KL(p(x, c)|q(x, c)) = −

i

log

p(xi, xpa(i)|c)
p(xpa(i)|c)p(xi|c)

+ const.

p(xi,xpa(i),c)

(10.6.16)

This shows that under the single-parent constraint and that each tree q(x|c) has the same structure, min-
imising the Kullback-Leibler divergence is equivalent to maximising the sum of conditional mutual infor-
mation terms.
Exercise 137. Write a MATLAB routine A = ChowLiu(X) where X is a D × N data matrix containing
a multivariate datapoint on each column that returns a Chow-Liu Maximum Likelihood tree for X. The
tree structure is to be returned in the sparse matrix A. You may ﬁnd the routine spantree.m useful. The
ﬁle ChowLiuData.mat contains a data matrix for 10 variables. Use your routine to ﬁnd the Maximum
Likelihood Chow Liu tree, and draw a picture of the resulting DAG with edges oriented away from variable
1.

DRAFT March 9, 2010

215

Exercises

216

DRAFT March 9, 2010

CHAPTER 11

Learning with Hidden Variables

11.1 Hidden Variables and Missing Data

In practice data entries are often missing resulting in incomplete information to specify a likelihood. Ob-
servational variables may be split into visible (those for which we actually know the state) and missing
(those whose states would nominally be known but are missing for a particular datapoint).

Another scenario in which not all variables in the model are observed are the so-called hidden or latent
variable models. In this case there are variables which are essential for the model description but never
observed. For example, the underlying physics of a model may contain latent processes which are essential
to describe the model, but cannot be directly measured.

11.1.1 Why hidden/missing variables can complicate proceedings

In learning the parameters of models as previously described in chapter(9), we assumed we have complete
information to deﬁne all variables of the joint model of the data p(v|θ). Consider the Asbestos-Smoking-
Cancer network of section(9.2.3). In the case of complete data, the likelihood is

p(vn|θ) = p(an, sn, cn|θ) = p(cn|an, sn, θc)p(an|θa)p(sn|θs)

(11.1.1)

which is factorised in terms of the table entry parameters. We exploited this property to show that table
entries θ can be learned by considering only local information, both in the Maximum Likelihood and
Bayesian frameworks.

Now consider the case that for some of the patients, only partial information is available. For example,
for patient n with record vn = {c = 1, s = 1} it is known that the patient has cancer and is a smoker, but
whether or not they had exposure to asbestos is unknown. Since we can only use the ‘visible’ available
information is it would seem reasonable to assess parameters using the marginal likelihood

p(cn|a, sn, θc)p(a|θa)p(sn|θs)

(11.1.2)

p(vn|θ) =(cid:88)

p(a, sn, cn|θ) =(cid:88)

a

a

We will discuss when this approach is valid in section(11.1.2). Using the marginal likelihood may result
in computational diﬃculties since equation (11.1.2) is not factorised over the tables. This means that the
likelihood function cannot be written as a product of functions, one for each separate parameter. In this
case the maximisation of the likelihood is more complex since the parameters of diﬀerent tables are coupled.

A similar complication holds for Bayesian learning. As we saw in section(13), under a prior factorised over
each CPT θ, the posterior is also factorised. However, in the case of unknown asbestos exposure, a term

217

Hidden Variables and Missing Data

θ

θ

xvis

xinv

xvis

xinv

minv

(a)

minv

(b)

Figure 11.1: (a): Missing at random assumption.
Missing completely at random assumption.

(b):

is introduced of the form

p(vn|θ) =(cid:88)

p(cn|a, sn, θc)p(a|θa)p(sn|θs) = p(sn|θs)(cid:88)

a

p(cn|a, sn, θc)p(a|θa)

(11.1.3)

a

which cannot be written as a product of a functions of fs(θs)fa(θa)fc(θc). The missing variable therefore
introduces dependencies in the posterior parameter distribution, making the posterior more complex.

In both the Maximum Likelihood and Bayesian cases, one has a well deﬁned likelihood function of the
table parameters/posterior. The diﬃculty is therefore not conceptual, but rather computational – how
are we to ﬁnd the optimum of the likelihood/summarise the posterior?

Note that missing data does not always make the parameter posterior non-factorised. For example, if
the cancer state is unobserved above, because cancer is a collider with no descendants, the conditional
distribution simply sums to 1, and one is left with a factor dependent on a and another on s.

11.1.2 The missing at random assumption

Under what circumstances is it valid to use the marginal likelihood to assess parameters? We partition
the variables x into those that are ‘visible’, xvis and ‘invisible’, xinv, so that the set of all variables can be
written x = [xvis, xinv]. For the visible variables we have an observed state xvis = v, whereas the state of
the invisible variables is unknown. We use an indicator minv = 1 to denote that the state of the invisible
variables is unknown. Then for a datapoint which contains both visible and invisible information,

p(xvis = v, minv = 1|θ) =(cid:88)
=(cid:88)

xinv

xinv

p(xvis = v, xinv, minv = 1|θ)
p(minv = 1|xvis = v, xinv, θ)p(xvis = v, xinv|θ)

(11.1.4)

(11.1.5)

(11.1.6)

(11.1.7)

(11.1.8)

If we assume that the mechanism which generates invisible data has the form

then

p(minv = 1|xvis = v, xinv, θ) = p(minv = 1|xvis = v)

p(xvis = v, minv = 1|θ) = p(minv = 1|xvis = v)(cid:88)

p(xvis = v, xinv|θ)

xinv

= p(minv = 1|xvis = v)p(xvis = v|θ)

Only the term p(xvis = v|θ) conveys information about the model. Therefore, provided the mechanism by
which the data is missing depends only on the visible states, we may simply use the marginal likelihood
to assess parameters. This is called the missing at random assumption.

Example 50 (Not missing at random). EZsurvey.org stop men on the street and ask them their favourite
colour. All men whose favourite colour is pink decline to respond to the question – for any other colour,
all men respond to the question. Based on the data, EZsurvey.org produce a histogram of men’s favourite
colour, based on the likelihood of the visible data alone, conﬁdently stating that none of them likes pink.

218

DRAFT March 9, 2010

Hidden Variables and Missing Data

For simplicity, assume there are only three colours, blue, green and pink. EZsurvey.org attempts to ﬁnd the
histogram with probabilities θb, θg, θp with θb + θg + θp = 1. Each respondent produces a visible response
xc with dom(xc) = {blue, green, pink}, otherwise mc = 1 if there is no response. Three men are asked their
favourite colour, giving data

(cid:8)x1

c, x2

c, x3
c

(cid:9) = {blue, missing, green}

(11.1.9)

(11.1.10)

Based on the likelihood of the visible data alone we have the log likelihood for i.i.d. data

L(θb, θg, θp) = log θb + log θg + λ (1 − θb − θg − θp)

where the last Lagrange term ensures normalisation. Maximising the expression we arrive at (see
exercise(145))

θb =

1
2 , θg =

1
2 , θp = 0

(11.1.11)

The unreasonable result that EZsurvey.org produce is due to not accounting correctly for the mechanism
which produces the data.

The correct mechanism that generates the data (including the missing data is)

p(c1 = blue|θ)p(m2

c = 1|θ)p(c3 = green|θ) = θbθpθg = θb (1 − θb − θg) θg

(11.1.12)

where we used p(m2
probability that the favourite colour is pink. Maximising the likelihood, we arrive at

c = 1|θ) = θp since the probability that a datapoint is missing is the same as the

θb =

1
3 , θg =

1
3 , θp =

1
3

(11.1.13)

as we would expect. On the other hand if there is another visible variable, t, denoting the time of day,
and the probability that men respond to the question depends only on the time t alone (for example the
missing probability is high during rush hour), then we may indeed treat the missing data as missing at
random.

A stronger assumption than MAR is

p(minv = 1|xvis = v, xinv, θ) = p(minv = 1)

(11.1.14)

which is called missing completely at random. This applies for example to latent variable models in which
the variable state is always missing, independent of anything else.

11.1.3 Maximum likelihood

Throughout the remaining discussion we will assume any missing data is MAR or missing completely at
random. This means that we can treat any unobserved variables by summing (or integrating) over their
states. For Maximum Likelihood we learn model parameters θ by optimising the marginal likelihood

p(v, h|θ)

(11.1.15)

p(v|θ) =(cid:88)

h
with respect to θ.

11.1.4 Identiﬁability issues
The marginal likelihood objective function depends on the parameters only through p(v|θ), so that equiv-
alent parameter solutions may exist. For example, consider a latent variable problem with distribution

p(x1, x2|θ) = θx1,x2
DRAFT March 9, 2010

(11.1.16)

219

Expectation Maximisation

p(x1|θ) = (cid:80)
(cid:88)

θ

x1,x2 =(cid:88)

(cid:48)

in which variable x2 is never observed. This means that the marginal likelihood only depends on the entry
x2 θx1,x2. Given a Maximum Likelihood solution θ∗, we can then always ﬁnd an equivalent
Maximum Likelihood solution θ(cid:48) provided (see exercise(146))

∗
x1,x2

θ

(11.1.17)

x2

x2

In other cases there is an inherent symmetry in the parameter space of the marginal likelihood. For
example, consider the network over binary variables

p(c, a, s) = p(c|a, s)p(a)p(s)

Our aim is to learn the table

ˆp(a = 1)

and the four tables

(11.1.18)

(11.1.19)

ˆp(c = 1|a = 1, s = 1), ˆp(c = 1|a = 1, s = 0), ˆp(c = 1|a = 0, s = 1), ˆp(c = 1|a = 0, s = 0) (11.1.20)

where we used a ‘ˆ’ to denote that these are parameter estimates.

We assume that we have missing data such that the states of variable a are never observed. In this case
an equivalent solution (in the sense that it has the same marginal likelihood) is given by interchanging the
states of a:

(cid:48)(a = 0) = ˆp(a = 1)
ˆp

and the four tables

(11.1.21)

ˆp
ˆp

(cid:48)(c = 1|a = 0, s = 1) = ˆp(c = 1|a = 1, s = 1),
(cid:48)(c = 1|a = 1, s = 1) = ˆp(c = 1|a = 0, s = 1),

ˆp
ˆp

(cid:48)(c = 1|a = 0, s = 0) = ˆp(c = 1|a = 1, s = 0)
(cid:48)(c = 1|a = 1, s = 0) = ˆp(c = 1|a = 0, s = 0)

A similar situation occurs in a more general setting in which the state of a variable is consistently unob-
served (mixture models are a case in point) yielding an inherent symmetry in the solution space. A well
known characteristic of Maximum Likelihood algorithms is that ‘jostling’ occurs in the initial stages of
training in which these symmetric solutions compete.

11.2 Expectation Maximisation

The EM algorithm is a convenient and general purpose iterative approach to maximising the likelihood
under missing data/hidden variables[187]. It is generally straightforward to implement and can achieve
large jumps in parameter space, particularly in the initial iterations.

11.2.1 Variational EM

The key feature of the EM algorithm is to form an alternative objective function for which the parameter
coupling eﬀect discussed in section(11.1.1) is removed, meaning that individual parameter updates can be
achieved, akin to the case of fully observed data. The way this works is to replace the marginal likelihood
with a lower bound – it is this lower bound that has the decoupled form.

We ﬁrst consider a single variable pair (v, h), where v stands for ‘visible’ and h for ‘hidden’. To derive
the bound on the marginal likelihood, consider the Kullback-Leibler divergence between a ‘variational’
distribution q(h|v) and the parametric model p(h|v, θ):

KL(q(h|v)|p(h|v, θ)) ≡ (cid:104)log q(h|v) − log p(h|v, θ)(cid:105)q(h|v) ≥ 0

220

(11.2.1)

DRAFT March 9, 2010

Expectation Maximisation

The term ‘variational’ refers to the fact that this distribution will be a parameter of an optimisation
problem. Using Bayes’ rule, p(h|v, θ) = p(h, v|θ)/p(v|θ) and the fact that p(v|θ) does not depend on h,

(cid:104)log q(h|v)(cid:105)q(h|v) − (cid:104)log p(h, v|θ)(cid:105)q(h|v) + log p(v|θ) = KL(q(h|v)|p(h|v, θ)) ≥ 0

(cid:124)

(cid:123)(cid:122)

Rearranging, we obtain a bound on the marginal likelihood1

(cid:125)
log p(v|θ) ≥ −(cid:104)log q(h|v)(cid:105)q(h|v)

(cid:125)
(cid:124)
+(cid:104)log p(h, v|θ)(cid:105)q(h|v)
V =(cid:8)v1, . . . , vN(cid:9) is the sum of the individual log likelihoods:

(cid:123)(cid:122)

Entropy

Energy

The bound is potentially useful since it is similar in form to the fully observed case, except that terms with
missing data have their log likelihood weighted by a prefactor. Equation(11.2.3) is a marginal likelihood
bound for a single training example. Under the i.i.d. assumption, the log likelihood of all training data

log p(V|θ) =

N(cid:88)

n=1

log p(vn|θ)
N(cid:88)

n=1

N(cid:88)

n=1

(11.2.2)

(11.2.3)

(11.2.4)

(11.2.5)

Summing over the training data, we obtain a bound on the log (marginal) likelihood

log p(V|θ) ≥ −

(cid:104)log q(hn|vn)(cid:105)q(hn|vn) +

(cid:104)log p(hn, vn|θ)(cid:105)q(hn|vn)

Note that the bound is exact (that is, the right hand side is equal to the log likelihood) when we set
q(hn|vn) = p(hn|vn, θ), n = 1, . . . , N.
The bound suggests an iterative procedure to optimise θ:
E-step For ﬁxed θ, ﬁnd the distributions q(hn|vn) that maximise equation (11.2.5).
M-step For ﬁxed {q(hn|vn), n = 1, . . . , N}, ﬁnd the parameters θ that maximise equation (11.2.5).
11.2.2 Classical EM

In the variational E-step above, the fully optimal setting is

q(hn|vn) = p(hn|vn, θ)

(11.2.6)

Since q does not depend on θnew the M-step is equivalent to maximising the energy term alone, see
algorithm(8).

Example 51 (A one-parameter one-state example). We consider a model small enough that we can plot
fully the evolution of the EM algorithm. The model is on a single visible variable v and single two-state
hidden variable h ∈ {1, 2}. We deﬁne a model p(v, h) = p(v|h)p(h) with

p(v|h, θ) =

1

√2πσ2

− 1
2σ2 (v−θh)2
e

(11.2.7)

and p(h = 1) = p(h = 2) = 0.5. For an observation v = 2.75 and σ2 = 0.5 our interest is to ﬁnd the
parameter θ that optimises the likelihood

−(2.75−θh)2

e

(11.2.8)

p(v = 2.75|θ) =

1
2√π

(cid:88)

h=1,2

1This is analogous to a standard partition function bound in statistical physics, from where the terminology ‘energy’ and

‘entropy’ hails.

DRAFT March 9, 2010

221

Expectation Maximisation

Algorithm 8 Expectation Maximisation. Compute Maximum Likelihood value for data with hidden
variables. Input: a distribution p(x|θ) and dataset V. Returns ML setting of θ.
1: t = 0
2: Choose an initial setting for the parameters θ0.
3: while θ not converged (or likelihood not converged) do
4:
5:
6:
7:
8:
9: end while
10: return θt

(cid:80)N
n=1 (cid:104)log p(hn, vn|θ)(cid:105)qn

t (hn|vn) = p(hn|vn, θt−1)

(cid:46) Run over all datapoints
(cid:46) E step

t ← t + 1
for n = 1 to N do

qn

end for
θt = arg maxθ

(cid:46) The max likelihood parameter estimate.

t (hn|vn)

(cid:46) M step

(cid:46) Iteration counter
(cid:46) Initialisation

(a)

(b)

(c)

Figure 11.2: (a): The log likelihood for the model described in example(51). (b): Contours of the lower
bound LB(q(h = 2), θ). For an initial choice q(h = 2) = 0.5 and θ = 1.9, successive updates of the E
(c): Starting at θ = 1.95, the EM algorithm converges
(vertical) and M (horizontal) steps are plotted.
to a local optimum.

The log likelihood is plotted in ﬁg(11.2a) with optimum at θ = 1.325. The EM procedure iteratively
optimises the lower bound

log p(v = 2.75|θ) ≥ LB(q(h = 2), θ)

≡ −q(h = 1) log q(h = 1) − q(h = 2) log q(h = 2) −

q(h) (2.75 − θh)2 + log 2 (11.2.9)

(cid:88)

h=1,2

where q(h = 1) = 1 − q(h = 2). From an initial starting θ, the EM algorithm ﬁnds the q distribution that
optimises L(q, θ) (E-step) and then updates θ (M-step). Depending on the initial θ, the solution found is
either a global or local optimum of the likelihood, see ﬁg(11.2).

The M-step is easy to work out analytically in this case with θnew = v (cid:104)h(cid:105)q(h) /(cid:10)h2(cid:11)

E-step sets qnew(h) = p(h|v, θ) so that

qnew(h = 2) = p(v = 2.75|h = 2, θ)p(h = 2)

p(v = 2.75)

=

where we used

e−(2.75−2θ)2

e−(2.75−2θ)2 + e−(2.75−θ)2

p(v = 2.75) = p(v = 2.75|h = 1, θ)p(h = 1) + p(v = 2.75|h = 2, θ)p(h = 2)

q(h). Similarly, the

(11.2.10)

(11.2.11)

222

DRAFT March 9, 2010

11.522.53−1.55−1.5−1.45−1.4−1.35−1.3−1.25−1.2−1.15−1.1−1.05θlogp(v|θ)θq(h=2)11.522.530.10.20.30.40.50.60.70.80.9θq(h=2)11.522.530.10.20.30.40.50.60.70.80.9Expectation Maximisation

Example 52. Consider a simple model

p(x1, x2|θ)

where dom(x1) = dom(x2) = {1, 2}. Assuming an unconstrained distribution

p(x1, x2|θ) = θx1,x2,

θ1,1 + θ1,2 + θ2,1 + θ2,2 = 1

(11.2.12)

(11.2.13)

our aim is to learn θ from the data x1 = (1, 1) , x2 = (1, ?) , x3 = (?, 2). The energy term for the classical
EM is

log p(x1 = 1, x2 = 1|θ) + (cid:104)log p(x1 = 1, x2|θ)(cid:105)p(x2|x1=1,θold) + (cid:104)log p(x1, x2 = 2|θ)(cid:105)p(x1|x2=2,θold)

Writing out fully each of the above terms on a separate line gives the energy

log θ1,1
+p(x2 = 1|x1 = 1, θold) log θ1,1 + p(x2 = 2|x1 = 1, θold) log θ1,2
+p(x1 = 1|x2 = 2, θold) log θ1,2 + p(x1 = 2|x2 = 2, θold) log θ2,2

(11.2.14)

(11.2.15)
(11.2.16)
(11.2.17)

This expression resembles the standard log likelihood of fully observed data except that terms with missing
data have their weighted log parameters. The parameters are conveniently decoupled in this bound (apart
from the trivial normalisation constraint) so that ﬁnding the optimal parameters is straightforward. This
is achieved by the M-step update which gives

θ1,1 ∝ 1 + p(x2 = 1|x1 = 1, θold) θ1,2 ∝ p(x2 = 2|x1 = 1, θold) + p(x1 = 1|x2 = 2, θold)
θ2,1 = 0

θ2,2 ∝ p(x1 = 2|x2 = 2, θold)

(11.2.18)

where p(x2|x1, θold) ∝ θold

x1,x2 (E-step) etc. The E and M-steps are iterated till convergence.

The EM algorithm increases the likelihood

Whilst, by construction, the EM algorithm cannot decrease the bound on the likelihood, an important
question is whether or not the log likelihood itself is necessarily increased by this procedure.
We use θ(cid:48) for the new parameters, and θ for the previous parameters in two consecutive iterations. Using
q(hn|vn) = p(hn|vn, θ) we see that as a function of the parameters, the lower bound for a single variable
pair (v, h) depends on θ and θ(cid:48):

|θ) ≡ −(cid:104)log p(h|v, θ)(cid:105)p(h|v,θ) +(cid:10)log p(h, v|θ

(cid:48))(cid:11)

p(h|v,θ)

(11.2.19)

(11.2.20)

(cid:48)

LB(θ

and

That is, the Kullback-Leibler divergence is the diﬀerence between the lower bound and the true likelihood.
We may write

(cid:48)) = LB(θ

(cid:48)

log p(v|θ

|θ) + KL(p(h|v, θ)|p(h|v, θ

(cid:48)))

(cid:125)
log p(v|θ) = LB(θ|θ) + KL(p(h|v, θ)|p(h|v, θ))

(cid:124)

Hence

log p(v|θ

(cid:48)) − log p(v|θ) = LB(θ

(cid:124)

(cid:48)

DRAFT March 9, 2010

0

(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:125)
|θ) − LB(θ|θ)

≥0

(cid:124)
+ KL(p(h|v, θ)|p(h|v, θ

(cid:123)(cid:122)

≥0

(cid:48)))

(cid:125)

(11.2.21)

(11.2.22)

223

Expectation Maximisation

s
1
0
1
1
1
0
0

c
1
0
1
0
1
0
1

Figure 11.3: A database containing information about being a Smoker (1 signiﬁes the
individual is a smoker), and lung Cancer (1 signiﬁes the individual has lung Cancer).
Each row contains the information for an individual, so that there are 7 individuals
in the database.

The ﬁrst assertion is true since, by deﬁnition of the M-step, we search for a θ(cid:48) which has a higher value for
the bound than our starting value θ. The second assertion is true by the property of the Kullback-Leibler
divergence.

For more than a single datapoint, we simply sum each individual bound for log p(vn|θ). Hence we reach
the important conclusion that the EM algorithm increases, not only the lower bound on the marginal
likelihood, but the marginal likelihood itself (more correctly, the EM cannot decrease these quantities).

Shared parameters and tables

The case of tables sharing parameters is essentially straightforward. According to the energy term, we need
to identify all those terms in which the shared parameter occurs. The objective for the shared parameter
is then the sum over all energy terms containing the shared parameter.

11.2.3 Application to Belief networks

Conceptually, the application of EM to training Belief Networks with missing data is straightforward.
The battle is more notational than conceptual. We begin the development with an example, from which
intuition about the general case can be gleaned.

Example 53. Consider the network.

p(a, c, s) = p(c|a, s)p(a)p(s)

(11.2.23)

for which we have a set of data, but that the states of variable a are never observed, see ﬁg(11.3). Our
goal is to learn the CPTs p(c|a, s) and p(a) and p(s). To apply EM, algorithm(8) to this case, we ﬁrst
assume initial parameters θ0

a, θ0

s, θ0
c .

The ﬁrst E-step, for iteration t = 1 then deﬁnes a set of distributions on the hidden variables (here the
hidden variable is a)

qn=1
t=1 (a) = p(a|c = 1, s = 1, θ0),

qn=2
t=1 (a) = p(a|c = 0, s = 0, θ0)

(11.2.24)

and so on for the 7 training examples, n = 2, . . . , 7. For notational convenience, we write qn
of qn

t (a) in place

t (a|vn).

We now move to the ﬁrst M-step. The energy term for any iteration t is:

7(cid:88)
7(cid:88)

n=1

n=1

E(θ) =

=

(cid:104)log p(cn|an, sn) + log p(an) + log p(sn)(cid:105)qn
(cid:110)
(cid:104)log p(cn|an, sn)(cid:105)qn

t (a) + (cid:104)log p(an)(cid:105)qn

t (a)

t (a) + log p(sn)

(cid:111)

(11.2.25)

(11.2.26)

The ﬁnal term is the log likelihood of the variable s, and p(s) appears explicitly only in this term. Hence,
the usual maximum likelihood rule applies, and p(s = 1) is simply given by the relative number of times

224

DRAFT March 9, 2010

Expectation Maximisation

that s = 1 occurs in the database, giving p(s = 1) = 4/7, p(s = 0) = 3/7.

(cid:88)
n {qn
log p(a = 0)(cid:88)

The parameter p(a = 1) occurs in the terms

t (a = 0) log p(a = 0) + qn

t (a = 1) log p(a = 1)}

which, using the normalisation constraint is

qn

t (a = 0) + log(1 − p(a = 0))(cid:88)
(cid:80)
(cid:88)
t (a = 0) +(cid:80)

t (a = 0)
n qn

t (a = 1)

n qn

1
N

=

n

n

n qn

n

(cid:80)

qn
t (a = 1)

p(a = 0) =

qn
t (a = 0)

Diﬀerentiating with respect to p(a = 0) and solving for the zero derivative we get

(11.2.27)

(11.2.28)

(11.2.29)

That is, whereas in the standard Maximum Likelihood estimate, we would have the real counts of the
t (a = 1).
data in the above formula, here they have been replaced with our guessed values qn

t (a = 0) and qn

A similar story holds for p(c = 1|a = 0, s = 1). The contribution of this term to the energy is

(cid:88)

t (a = 0) log p(c = 1|a = 0, s = 1) + (cid:88)

qn

qn
t (a = 0) log (1 − p(c = 1|a = 0, s = 1))

n:cn=0,sn=1

n:cn=1,sn=1

which is

log p(c = 1|a = 0, s = 1) (cid:88)
(cid:80)
(cid:80)

p(c = 1|a = 0, s = 1) =

p(c = 1|a = 0, s = 1) =

n

t (a = 0)+log(1−p(c = 1|a = 0, s = 1)) (cid:88)

qn

n:cn=0,sn=1

qn
t (a = 0) (11.2.30)

n:cn=1,sn=1

(cid:80)
(cid:80)

n

I [cn = 1] I [sn = 1] qn

t (a = 0) +(cid:80)
I [cn = 1] I [sn = 1] I [an = 0] +(cid:80)

n

n

I [cn = 1] I [sn = 1] qn

t (a = 0)

I [cn = 0] I [sn = 1] qn

t (a = 0)

n

I [cn = 1] I [sn = 1] I [an = 0]

I [cn = 0] I [sn = 1] I [an = 0]

n

(11.2.31)

(11.2.32)

Optimising with respect to p(c = 1|a = 0, s = 1) gives

For comparison, the setting in the complete data case is

There is an intuitive relationship between these updates: in the missing data case we replace the indicators
by the assumed distributions q.

Iterating the E and M steps, these equations will converge to a local likelihood optimum.

To minimise the notational burden, we assume that the structure of the missing variables is ﬁxed through-
out, this being equivalent therefore to a latent variable model. The form of the energy term for Belief

Networks is(cid:88)

n (cid:104)log p(xn)(cid:105)qt(hn|vn) =(cid:88)

n

(cid:88)

i

It is useful to deﬁne the following notation:

t (x) = qt(h|vn)δ(v, vn)
qn

(cid:104)log p(xn

i |pa (xn

i ))(cid:105)qt(hn|vn)

(11.2.33)

(11.2.34)

t (x) sets the visible
where x = (v, h) represents all the variables in the distribution. This means that qn
variables in the observed state, and deﬁnes a conditional distribution on the unobserved variables. We

DRAFT March 9, 2010

225

Expectation Maximisation

Algorithm 9 EM for Belief Networks.
tables, and dataset on the visible variables V. Returns the Maximum Likelihood setting of tables.
1: t = 1
2: Set pt (xi|pa (xi)) to initial values.
3: while p (xi|pa (xi)) not converged (or likelihood not converged) do

Input: a BN structure p(xi|pa (xi)), i = 1, . . . , K with empty

(cid:46) Iteration counter
(cid:46) Initialisation

t (x) = pt (hn|vn) δ(v, vn)
qn
(cid:80)N

pt+1(xi|pa (xi)) = 1

N

n=1 qn

t (xi|pa (xi))

t ← t + 1
for n = 1 to N do

end for
for i = 1 to K do

4:
5:
6:
7:
8:
9:
end for
10:
11: end while
12: return pt(xi|pa (xi))
N(cid:88)

qt(x) =

qn
t (x)

1
N

n=1

then deﬁne the mixture distribution

(cid:46) Run over all datapoints
(cid:46) E step

(cid:46) Run over all variables
(cid:46) M step

(cid:46) The max likelihood parameter estimate.

The energy term in equation (11.2.5) can be written more compactly as

(cid:88)
n (cid:104)log p(xn)(cid:105)qt(h|vn) = N (cid:104)log p(x)(cid:105)qt(x)
(cid:88)

N (cid:104)log p(x)(cid:105)qt(x) = N

To see this consider the right hand side of the above

qt(h|vn)δ(v, vn) =(cid:88)

x

1
N

[log p(x)]

(cid:88)
(cid:104)log p(xi|pa (xi))(cid:105)qt(x) =(cid:88)

n

Using the structure of the Belief Network, we have

(cid:104)log p(x)(cid:105)qt(x) =(cid:88)
(cid:68)
(cid:88)
(cid:104)log qt(xi|pa (xi))(cid:105)qt(xi|pa(xi)) − (cid:104)log p(xi|pa (xi))(cid:105)qt(xi|pa(xi))

This means that maximising the energy is equivalent to minimising

i

i

(cid:68)

i

(cid:69)

qt(pa(xi))

n (cid:104)log p(xn)(cid:105)qt(h|vn)
(cid:69)

(cid:104)log p(xi|pa (xi))(cid:105)qt(xi|pa(xi))

qt(pa(xi))

where we added the constant ﬁrst term to make this into the form of a Kullback-Leibler divergence. Since
this is a sum of independent Kullback-Leibler divergences, optimally the M-step is given by setting

p(xi|pa (xi)) = qt(xi|pa (xi))

(11.2.40)

In practice, storing the qt(x) over the states of all variables x is prohibitively expense. Fortunately, since
the M-step only requires the distribution on the family of each variable xi, one only requires the local
distributions qn

old(xi|pa (xi)). We may therefore dispense with the global qold(x) and equivalently use

(cid:80)
(cid:80)

pnew(xi|pa (xi)) =

old(xi, pa (xi))
n qn
n(cid:48) qn(cid:48)
old(pa (xi))

Using the EM algorithm, the optimal setting for the E-step is to use qt(hn|vn) = pold(hn|vn). With this
notation, the EM algorithm can be compactly stated as in algorithm(9). See also EMbeliefnet.m.

226

DRAFT March 9, 2010

(11.2.35)

(11.2.36)

(11.2.37)

(11.2.38)

(11.2.39)

(11.2.41)

Expectation Maximisation

Example 54 (More General Belief Networks). Consider a ﬁve variable distribution with discrete variables,

p(x1, x2, x3, x4, x5) = p(x1|x2)p(x2|x3)p(x3|x4)p(x4|x5)p(x5)

(11.2.42)

in which the variables x2 and x4 are consistently hidden in the training data, and training data for x1, x3, x5
are always present. The distribution can be represented as a Belief network

x1

x2

x3

x4

x5

In this case, the contributions to the energy have the form

which may be written as

(cid:88)
n (cid:104)log p(xn
(cid:88)
+(cid:88)
n (cid:104)log p(xn
n (cid:104)log p(xn

1|x2)p(x2|xn

3 )p(xn

3|x4)p(x4|xn

5 )p(xn

5(cid:105)qn(x2,x4|x1,x3,x5)

1|x2)(cid:105)qn(x2,x4|x1,x3,x5) +(cid:88)

3|x4)(cid:105)qn(x2,x4|x1,x3,x5) +(cid:88)

n (cid:104)log p(x2|xn

3 )(cid:105)qn(x2,x4|x1,x3,x5)

5 )(cid:105)qn(x2,x4|x1,x3,x5) +(cid:88)

n

n (cid:104)log p(x4|xn

(11.2.43)

log p(xn
5 )

(11.2.44)

A useful property can now be exploited, namely that each term depends on only those hidden variables
in the family that that term represents. Thus we may write

(cid:88)
n (cid:104)log p(xn

1|x2)(cid:105)qn(x2|x1,x3,x5) +(cid:88)

+(cid:88)
n (cid:104)log p(xn

n (cid:104)log p(x2|xn

3|x4)(cid:105)qn(x4|x1,x3,x5) +(cid:88)

3 )(cid:105)qn(x2|x1,x3,x5)

n (cid:104)log p(x4|xn

5 )(cid:105)qn(x4|x1,x3,x5) +(cid:88)

n

log p(xn
5 )

The ﬁnal term can be set using Maximum Likelihood. Let us consider therefore a more diﬃcult table,
p(x1|x2). When will the table entry p(x1 = i|x2 = j) occur in the energy? This happens whenever xn
1
is in state i. Since there is a summation over all the states of variables x2 (due to the average), there
is also a term with variable x2 in state j. Hence the contribution to the energy from terms of the form
p(x1 = i|x2 = j) is

I [xn

1 = i] qn(x2 = j|x1, x3, x5) log p(x1 = i|x2 = j)

where the indicator function I [xn
isation of the table, we add a Lagrange term:

1 = i] equals 1 if xn

1 is in state i and is zero otherwise. To ensure normal-

(cid:40)
1 −

(cid:88)

k

(11.2.45)

(cid:41)

p(x1 = k|x2 = j)

(11.2.46)

(11.2.47)

(11.2.48)

(11.2.49)

227

(cid:88)

n

(cid:88)

n

(cid:88)

I [xn

1 = i] qn(x2 = j|x1, x3, x5) log p(x1 = i|x2 = j) + λ

Diﬀerentiating with respect to p(x1 = i|x2 = j) we get

= λ

n

or

Hence

I [xn

1 = i] qn(x2 = j|x1, x3, x5)
p(x1 = i|x2 = j)
(cid:88)
(cid:80)
(cid:80)

I [xn

I [xn
I [xn

n

n

n,k

p(x1 = i|x2 = j) =

p(x1 = i|x2 = j) ∝

1 = i] qn(x2 = j|x1, x3, x5).

1 = i] qn(x2 = j|x1, x3, x5)
1 = k] qn(x2 = j|x1, x3, x5)

DRAFT March 9, 2010

Expectation Maximisation

Figure 11.4: Evolution of the log-likelihood versus iterations un-
der the EM training procedure (from solving the Printer Night-
mare with missing data, exercise(138). Note how rapid progress
is made at the beginning, but convergence can be slow.

Using the EM algorithm, we have

qn(x2 = j|x1, x3, x5) = p(x2 = j|xn

1 , xn

3 , xn
5 )

(11.2.50)

This optimal distribution is easy to compute since this is the marginal on the family, given some evidential
variables. Hence, the M-step update for the table is

What about the table p(x2 = i|x3 = j)? To ensure normalisation of the table, we add a Lagrange term:

I [xn
I [xn

n

n,k

1 = i] pold(x2 = j|xn
1 = k] pold(x2 = j|xn

1 , xn
1 , xn

5 )
3 , xn
5 )
3 , xn

(cid:40)
1 −

(cid:88)

k

(cid:41)
p(x2 = k|x3 = j)

(11.2.51)

(11.2.52)

(11.2.53)

(11.2.54)

(11.2.55)

pnew(x1 = i|x2 = j) =
(cid:88)

I [xn

n

3 = j] qn(x2 = i|x1, x3, x5) log p(x2 = i|x3 = j) + λ

As before, diﬀerentiating, and using the EM settings, we have

pnew(x2 = i|x3 = j) =

I [xn
I [xn

3 = j] pold(x2 = i|xn
3 = j] pold(x2 = k|xn

1 , xn
1 , xn

5 )
3 , xn
5 )
3 , xn

n

n,k

pnew(x1 = i|x2 = j) ∝

I [xn

1 = i] I [xn

2 = j]

and equation (11.2.53) would be

pnew(x2 = i|x3 = j) ∝

I [xn

3 = j] I [xn

2 = i]

(cid:80)
(cid:80)

(cid:80)
(cid:80)

(cid:88)
(cid:88)

n

n

There is a simple intuitive pattern to equation (11.9.2) and equation (11.2.53) : If there were no hidden
data, equation (11.9.2) would read

All that we do, therefore, in the general EM case, is to replace those deterministic functions such as
I [xn
5 ). This is merely a restatement of the
general update given in equation (11.2.41) under the deﬁnition (11.2.34).

2 = i] by their missing variable equivalents pold(x2 = i|xn

3 , xn

1 , xn

11.2.4 Application to Markov networks
For a MN deﬁned over visible and hidden variables p(v, h|θ) = 1
is

Z(θ)

log p(v|θ) ≥ −H(q(h)) +(cid:88)

c (cid:104)log φc(h, v|θ)(cid:105)q(h) − log Z(θ)

(cid:81)

c φc(h, v) the EM variational bound

(11.2.56)

Whilst the bound decouples the parameters in the second term, the parameters are nevertheless coupled in
the normalisation Z(θ). Because of this we cannot optimise the above bound on a parameter by parameter
basis. One approach is to use an additional bound log Z(θ) from above, as for iterative scaling.

228

DRAFT March 9, 2010

024681012−120−110−100−90−80−70−60−50log likelihoodExtensions of EM

11.2.5 Convergence

Convergence of EM can be slow, particularly when the number of missing observations is greater than the
number of visible observations. In practice, one often combines the EM with gradient based procedures to
improve convergence, see section(11.7). Note also that the log likelihood is typically a non-convex function
of the parameters. This means that there may be multiple local optima and the solution found often
depends on the initialisation.

11.3 Extensions of EM

11.3.1 Partial M step

It is not necessary to ﬁnd the full optimum of the energy term at each iteration. As long as one ﬁnds a
parameter θ(cid:48) which has a higher energy than that of the current parameter θ, then the conditions required
in section(11.2.2) still hold, and the likelihood cannot decrease at each iteration.

11.3.2 Partial E step

N(cid:88)

n=1

N(cid:88)

n=1

(cid:104)log p(hn, vn|θ)(cid:105)q(hn|vn)

The E-step requires us to ﬁnd the optimum of

log p(V|θ) ≥ −

(cid:104)log q(hn|vn)(cid:105)q(hn|vn) +

with respect to q(hn|vn). The fully optimal setting is

q(hn|vn) = p(hn|vn)

(11.3.1)

(11.3.2)

(11.3.3)

For a guaranteed increase in likelihood at each iteration, from section(11.2.2) we required that the fully
optimal setting of q is used. Unfortunately, therefore, one cannot in general guarantee that such a partial
E step would always increase the likelihood. Of course, it is guaranteed to increase the lower bound on
the likelihood, though not the likelihood itself.

Intractable energy

The EM algorithm assumes that we can calculate

(cid:104)log p(h, v|θ)(cid:105)q(h|v)

of distributions - for example, factorised distributions q(h|v) =(cid:81)
simpler class of distributions, Q, e.g. Q = factorised q(h|v) =(cid:81)

However, in general, it may be that we can only carry out the average over q for a very restricted class
j q(hj|v). In such cases one may use a
i q(hi|v), for which the averaging required

for the energy may be simpler.

We can ﬁnd the best distribution in class Q by minimising the KL divergence between q(h|v, θQ) and
p(h|v, θ) numerically using a non-linear optimisation routine:

qopt = argmin
q∈Q

KL(q(h)|p(h|v, θ))

(11.3.4)

Alternatively, one can assume a certain structured form for the q distribution, and learn the optimal
factors of the distribution by free form functional calculus.

Viterbi training
An extreme case is to restrict q(hn|vn) to a delta-function. In this case, the entropic term (cid:104)log q(hn|vn)(cid:105)q(hn|vn)
is constant, so that the optimal delta function q is to set

q(hn|vn) = δ (hn, hn∗ )

DRAFT March 9, 2010

(11.3.5)

229

N(cid:88)

n=1

where

hn∗ = argmax

h

p(h, vn|θ)

A Failure Case for EM

(11.3.6)

This is called Viterbi training and is common in training HMMs, see section(23.2). EM training with this
restricted class of q distribution is therefore guaranteed to increase the lower bound on the log likelihood,
though not the likelihood itself. A practical advantage of Viterbi training is that the energy is always
tractable to compute, becoming simply

log p(hn∗ , vn|θ)

(11.3.7)

which is amenable to optimisation.

Provided there is suﬃcient data, one might hope that the likelihood as a function of the parameter θ will
be sharply peaked around the optimum value. This means that at convergence the approximation of the
posterior p(h|v, θopt) by a delta function will be reasonable, and an update of EM using Viterbi training
will produce a new θ approximately the same as θopt. For any highly suboptimal θ, however, p(h|v, θ)
will be far from a delta function, and therefore a Viterbi update is less reliable in terms of leading to
an increase in the likelihood itself. This suggests that the initialisation of θ for Viterbi training is more
critical than for the standard EM.

Stochastic training
Another approximate q(hn|vn) distribution would be to use an empirical distribution formed by samples
from the fully optimal distribution p(hn|vn, θ). That is one draws samples (see chapter(27) for a discussion
on sampling) hn

1 , . . . , hn

L from p(hn|vn, θ) and forms a q distribution
L(cid:88)

1
L

l=1

δ (hn, hn
l )

The energy becomes then proportional to

q(hn|vn) =
L(cid:88)
N(cid:88)

n=1

l=1

log p(hn

l , vn|θ)

(11.3.8)

(11.3.9)

so that, as in Viterbi training, the energy is always computationally tractable for this restricted q class.
Provided that the samples from p(hn|vn) are reliable, stochastic training will produce an energy function
with (on average) the same characteristics as the true energy under the classical EM algorithm. This
means that the solution obtained from stochastic training should tend to that from the classical EM as
the number of samples increases.

p(v|θ) =

h

δ (v, f(h|θ)) p(h)

(11.4.1)

If we attempt an EM approach for this, this will fail (see also exercise(75)). For a more general model of
the form

11.4 A Failure Case for EM

Consider a likelihood of the form

(cid:90)

(cid:90)

p(v|θ) =
The E-step is

p (v|h, θ) p(h)

h

q(h|θold) ∝ p (v|h, θold) p(h)

230

(11.4.2)

(11.4.3)

DRAFT March 9, 2010

Variational Bayes

and the M-step sets

θnew = argmax

θ

(cid:104)log p(v, h|θ)(cid:105)p(h|θold) = argmax

θ

(cid:104)log p(v|h, θ)(cid:105)p(h|θold)

(11.4.4)

where we used the fact that for this model p(h) is independent of θ. In the case that p (v|h, θ) = δ (v, f(h|θ))
then

p(h|θold) ∝ δ (v, f(h|θ)) p(h)

so that optimising the energy requires

θnew = argmax

θ

(cid:104)log δ (v, f(h|θ))(cid:105)p(h|θold)

(11.4.5)

(11.4.6)

Since p(h|θold) is zero everywhere expect that h for which v = f(h|θ), then the energy is eﬀectively negative
inﬁnity if θ (cid:54)= θold. However, when θ = θold the energy is maximal2 . This is therefore the optimum of the
energy, and represents therefore a failure in updating for EM. This situation occurs in practice, and has
been noted in particular in the context of Independent Component Analysis[222].

One can attempt to heal this behaviour by deriving an EM algorithm based on the distribution

p(v|h, θ) = (1 − )δ (v, f(h|θ)) + n(h), 0 ≤  ≤ 1

where n(h) is an arbitrary distribution on the hidden variable h. The original deterministic model corre-
sponds to p0(v|h, θ). Deﬁning
p(v|h, θ)p(h)

p(v|θ) =

(11.4.8)

(cid:90)

h

we have

p(v|θ) = (1 − )p0(v|θ) + (cid:104)n(h)(cid:105)p(h)

An EM algorithm for p(v|θ), 0 <  < 1 satisﬁes

p(v|θnew) − p(v|θold) = (1 − ) (p0(v|θnew) − p0(v|θold)) > 0

which implies

p0(v|θnew) − p0(v|θold) > 0

(11.4.7)

(11.4.9)

(11.4.10)

(11.4.11)

This means that the EM algorithm for the non-deterministic case 0 <  < 1 is guaranteed to increase
the likelihood under the deterministic model p0(v|θ) at each iteration (unless we are at convergence). See
[100] for an application of this ‘antifreeze’ technique to learning Markov Decision Processes with EM.

11.5 Variational Bayes

As discussed in section(9.2) Maximum Likelihood corresponds to a form of Bayesian approach in which
the parameter posterior distribution (under a ﬂat prior) is approximated with a delta function p(θ|V) ≈
δ(θ, θopt). Variational Bayes is analogous to EM in that it attempts to deal with hidden variables but
using a distribution that better represents the posterior distribution than given by Maximum Likelihood.

To keep the notation simple, we’ll initially assume only a single datapoint with observation v. Our interest
is then the parameter posterior

p(θ|v) ∝ p(v|θ)p(θ) ∝

p(v, h|θ)p(θ)

(11.5.1)

(cid:88)

h

The VB approach assumes a factorised approximation of the joint hidden and parameter posterior:

p(h, θ|v) ≈ q(h)q(θ)

(11.5.2)

2For discrete variables and the Kronecker delta, the energy attains the maximal value of zero when θ = θold. In the case
of continuous variables, however, the log of the Dirac delta function is not well deﬁned. Considering the delta function as the
limit of a narrow width Gaussian, for any small but ﬁnite width, the energy is largest when θ = θold.

DRAFT March 9, 2010

231

A bound on the marginal likelihood

By minimising the KL divergence,

KL(q(h)q(θ)|p(h, θ|v)) = (cid:104)log q(h)(cid:105)q(h) + (cid:104)log q(θ)(cid:105)q(θ) − (cid:104)log p(h, θ|v)(cid:105)q(h)q(θ) ≥ 0

we arrive at the bound

log p(v) ≥ −(cid:104)log q(h)(cid:105)q(h) − (cid:104)log q(θ)(cid:105)q(θ) + (cid:104)log p(v, h, θ)(cid:105)q(h)q(θ)

For ﬁxed q(θ) if we minimize the Kullback-Leibler divergence, we get the tightest lower bound on log p(v).
If then for ﬁxed q(h) we minimise the Kullback-Leibler divergence w.r.t. q(θ) we are maximising the term
−(cid:104)log q(θ)(cid:105)q(θ) + (cid:104)log p(v, h, θ)(cid:105)q(h)q(θ) and hence pushing up the bound on the marginal likelihood. This
simple co-ordinate wise procedure in which we ﬁrst ﬁx the q(θ) and solve for q(h) and then vice versa is
analogous to the E and M step of the EM algorithm:
E-step

(cid:16)

(cid:17)
q(h)qold(θ)|p(h, θ|v)

qnew(h) = argmin

q(h)

M-step

KL

qnew(θ) = argmin

q(θ)

KL(qnew(h)q(θ)|p(h, θ|v))

Variational Bayes

(11.5.3)

(11.5.4)

(11.5.5)

(11.5.6)

In full generality for a set of observations V and hidden variables H, algorithm(7). For distributions
q(H) and q(θ) which are parametersised/constrained, the best distributions in the minimal KL sense are
returned. In general, each iteration of VB is guaranteed to increase the bound on the marginal likelihood,
but not the marginal likelihood itself. Like the EM algorithm, VB can (and often does) suﬀer from local
maxima issues. This means that the converged solution can be dependent on the initialisation.

Unconstrained approximations

For ﬁxed q(θ) the contribution to the KL divergence is

(cid:104)log q(h)(cid:105)q(h) − (cid:104)log p(v, h, θ)(cid:105)q(h)q(θ) = KL(q(h)|˜p(h)) + const.

where

˜p(h) ≡

1
˜Z

(cid:104)log p(v,h,θ)(cid:105)q(θ)
e

where ˜Z is a normalising constant. Hence, for ﬁxed q(θ), the optimal q(h) is given by ˜p,

(cid:104)log p(v,h,θ)(cid:105)q(θ)

q(h) ∝ e

Similarly, for ﬁxed q(h), optimally

(cid:104)log p(v,h,θ)(cid:105)q(h)

q(θ) ∝ e

(11.5.7)

(11.5.8)

(11.5.9)

(11.5.10)

log p(V|θ) ≥

q(h1, . . . , hN ) =(cid:89)

i.i.d. Data

Under the i.i.d. assumption, we obtain a bound on the marginal likelihood for the whole dataset:

(cid:88)

n

(cid:110)
−(cid:104)log q(hn)(cid:105)q(hn) − (cid:104)log q(θ)(cid:105)q(θ) + (cid:104)log p(vn, hn, θ)(cid:105)q(hn)q(θ)

(cid:111)

(11.5.11)

The bound holds for any q(hn) and q(θ) but is tightest for the converged estimates from the VB procedure.

For an i.i.d. dataset, it is straightforward to show that without loss of generality we may assume

q(hn)

(11.5.12)

Under this we arrive at algorithm(11).

n

232

DRAFT March 9, 2010

Variational Bayes

Algorithm 10 Variational Bayes.

1: t = 0
2: Choose an initial distribution q0(θ).
3: while θ not converged (or likelihood bound not converged) do
4:
5:
6:
7: end while
8: return qn

t ← t + 1
t (H) = arg minq(H) KL(q(H)qt−1(θ)|p(H, θ|V))
qn
t (θ) = arg minq(θ) KL(qn
qn

t (H)q(θ)|p(H, θ|V))

t (θ)

(cid:46) Iteration counter
(cid:46) Initialisation

(cid:46) E step
(cid:46) M step

(cid:46) The posterior parameter approximation.

hn

vn

N

(a)

θ

hn

N

θ

(b)

Figure 11.5: (a): Generic form of a model
(b): A fac-
with hidden variables.
torised posterior approximation uses in
Variational Bayes.

11.5.1 EM is a special case of variational Bayes

If we wish to ﬁnd a summary of the parameter distribution corresponding to only the most likely point θ,
then

q(θ) = δ (θ, θ∗)

(11.5.13)
where θ∗ is the single optimal value of the parameter. If we plug this assumption into equation (11.5.4)
we obtain the bound

The M-step is then given by

log p(v|θ∗) ≥ −(cid:104)log q(h)(cid:105)q(h) + (cid:104)log p(v, h, θ∗)(cid:105)q(h) + const.

(cid:16)

(cid:17)
(cid:104)log p(v|h, θ)p(h|θ)(cid:105)q(h) + log p(θ)

θ∗ = argmax

θ

(11.5.14)

(11.5.15)

For a ﬂat prior p(θ) = const., this is therefore equivalent to energy maximisation in the EM algorithm.
Using this single optimal value in the VB update for q(hn) we have

qn
t (h) ∝ p(v, h|θ∗) ∝ p(h|v, θ∗)

(11.5.16)
which is the standard E-step of EM. Hence EM is a special case of VB, under a ﬂat prior p(θ) = const.
and a delta function approximation of the parameter posterior.

11.5.2 Factorising the parameter posterior

Let’s reconsider Bayesian learning in the binary variable network

p(a, c, s) = p(c|a, s)p(a)p(s)

in which we use a factorised parameter prior

(11.5.17)

p(θc)(θa)p(θs)

(11.5.18)
When all the data is observed, the parameter posterior factorises. As we discussed in section(11.1.1) if the
state of a is not observed, the parameter posterior no longer factorises:

p(θa, θs, θc|V) ∝ p(θa)p(θs)p(θc)p(V|θa, θs, θc)

∝ p(θa)p(θs)p(θc)(cid:89)
∝ p(θa)p(θs)p(θc)(cid:89)

n

n

p(vn|θa, θs, θc)

p(sn|θs)(cid:88)

an

p(cn|sn, an, θc)p(an|θa)

DRAFT March 9, 2010

(11.5.19)
(11.5.20)

(11.5.21)

233

Algorithm 11 Variational Bayes (i.i.d. data).

1: t = 0
2: Choose an initial distribution q0(θ).
3: while θ not converged (or likelihood bound not converged) do
4:
5:

t ← t + 1
for n = 1 to N do

(cid:104)log p(vn,hn,θ)(cid:105)qt−1(θ)
t (hn) ∝ e
qn
(cid:80)
end for
qt(θ) ∝ p(θ)e

n(cid:104)log p(vn,hn|θ)(cid:105)qn

t (hn)

6:
7:
8:
9: end while
10: return qn

t (θ)

θs

s

N

θa

a

c

θc

(a)

θs

θa

a

N

θc

(b)

Variational Bayes

(cid:46) Iteration counter
(cid:46) Initialisation

(cid:46) Run over all datapoints
(cid:46) E step

(cid:46) M step

(cid:46) The posterior parameter approximation.

Figure 11.6: (a): A model for the rela-
tionship between lung Cancer, Asbestos
exposure and Smoking with factorised pa-
rameter priors. Variables c and s are ob-
served, but variable a is consistently miss-
(b): A factorised parameter poste-
ing.
rior approximation.

where the summation over a prevents the factorisation into a product of the individual table parameters.

Since it is convenient in terms of representations to work with factorised posteriors, we can apply VB but
with a factorised constraint on the form of the q. In VB we deﬁne a distribution over the visible and
hidden variables. In this case the hidden variables are the an and the visible are sn, cn. The joint posterior
over all unobserved variables (parameters and missing observations) is

p(θa, θs, θc, a1, . . . , aN|V) ∝ p(θa)p(θs)p(θc)(cid:89)
p(θa, θs, θc, a1, . . . , aN|V) ≈ q(θa)q(θc)q(θs)(cid:89)

n

To make a factorised posterior approximation we use

n

p(cn|sn, an, θc)p(sn|θs)p(an|θa)

(11.5.22)

q(an)

(11.5.23)

and minimise the Kullback-Leibler divergence between the left and right of the above.

M-step

Hence

q(θa) ∝ p(θa)(cid:89)

n

(cid:104)log p(an|θa)(cid:105)q(an)
e

(cid:104)log p(an|θa)(cid:105)q(an) = q(an = 1) log θa + q(an = 0) log (1 − θa)

Hence

e

(cid:104)log p(an|θa)(cid:105)q(an) = θq(an=1)

(1 − θa)q(an=0)
It is convenient to use a Beta distribution prior,

a

p(θa) ∝ θα−1

a

(1 − θa)β−1

234

(11.5.24)

(11.5.25)

(11.5.26)

(11.5.27)

DRAFT March 9, 2010

Variational Bayes

(cid:32)
θa|α +(cid:88)
(cid:32)
θs|α +(cid:88)

n

q(θa) = B

n
A similar calculation gives

q(θs) = B

q(θc(a = 0, s = 1)) = B

(cid:33)

q(an = 0

(cid:33)

I [sn = 0]

n

q(an = 1), β +(cid:88)
I [sn = 1] , β +(cid:88)
(cid:32)
θc|α +(cid:88)

n

n

I [sn = 1] q(an = 0), β +(cid:88)

n

(11.5.28)

(11.5.29)

(11.5.30)

(cid:33)

I [sn = 0] q(an = 1)

since the posterior approximation is then also a Beta distribution:

and four tables, one for each of the parental states of c. For example

These are reminiscent of the standard Bayesian equations, equation (9.3.17) except that the counts have
been replaced by q’s.

E-step

We still need to determine q(an). The optimal value is given by minimising the Kullback-Leibler divergence
with respect to q(an). This gives the solution that optimally,

(cid:104)log p(cn|sn,an,θc)(cid:105)q(θc)+(cid:104)log p(an|θa)(cid:105)q(θa)

q(an) ∝ e

For example, if assume that for datapoint n, s is in state 1 and c in state 0, then

(cid:104)log(1−θc(s=1,a=1))(cid:105)q(θc(s=1,a=1))+(cid:104)log θa(cid:105)q(θa)

q(an = 1) ∝ e

and

(11.5.31)

(11.5.32)

(cid:104)log(1−θc(s=1,a=0))(cid:105)q(θc(s=1,a=1))+(cid:104)log(1−θa)(cid:105)q(θa)

q(an = 0) ∝ e

(11.5.33)
To compute such quantities explicitly, we need the values (cid:104)log θ(cid:105)B(θ|α,β) and (cid:104)log (1 − θ)(cid:105)B(θ|α,β). For a
Beta distribution, these are straightforward to compute, see exercise(95).

The complete VB procedure is then given by iterating equations (11.5.28,11.5.29,11.5.30) and (11.5.32,11.5.33)
until convergence.

Given a converged factorised approximation, computing a marginal table p(a = 1|V) is then straightforward
under the approximation

(cid:90)

(cid:90)

p(a = 1|V) ≈

q(a = 1|θa)q(θa|V) =

θa

θa

θaq(θa|V)

(11.5.34)

Since q(θa|V) is a Beta distribution B (θa|α, β), the mean is straightforward. Using this for both states of
a leads to

α +(cid:80)
n q(an = 0) + β +(cid:80)

n q(an = 1)

α +(cid:80)

n q(an = 1)

p(a = 1|V) =

(11.5.35)

The application of VB to learning the tables in arbitrarily structured BNs is a straightforward extension
of the technique outlined here. Under the factorised approximation, q(h, θ) = q(h)q(θ), one will always
obtain a simple updating equation analogous to the full data case, but with the missing data replaced by
variational approximations. Nevertheless, if a variable has many missing parents, the number of states in
the average with respect to the q distribution can become intractable, and further constraints on the form
of the approximation, or additional bounds are required.

One may readily extend the above to the case of Dirichlet distributions on multinomial variables, see
exercise(142). Indeed, the extension to the exponential family is straightforward.

DRAFT March 9, 2010

235

Optimising the Likelihood by Gradient Methods

Figure 11.7: (a): Standard ML learning. The best parameter θ is found
by maximising the probability that the model generates the observed data
(b): ML-II learning. In cases where we have a
θopt = arg maxθ p(v|θ).
prior preference for the parameters θ, but with unspeciﬁed hyperparameter
θ(cid:48), we can ﬁnd θ(cid:48) by θ(cid:48)

opt = arg maxθ(cid:48) p(v|θ(cid:48)) = arg maxθ(cid:48) (cid:104)p(v|θ)(cid:105)p(θ|θ(cid:48)).

θ(cid:48)

θ

v

(b)

θ

v

(a)

11.6 Bayesian Methods and ML-II

Consider a parameterised distribution p(v|θ), for which we wish to the learn the optimal parameters θ
given some data. The model p(v|θ) is depicted in ﬁg(11.7a), where a dot indicates that no distribution is
present on that variable. For a single observed datapoint v, setting θ by Maximum Likelihood corresponds
to ﬁnding the parameter θ that maximises p(v|θ).
In some cases we may have an idea about which parameters θ are more appropriate and can express
this prior preference using a distribution p(θ). If the prior were fully speciﬁed, then there is nothing to
‘learn’ since p(θ|v) is now fully known. However, in many cases in practice, we are unsure of the exact
parameter settings of the prior, and hence specify a parametersised prior using a distribution p(θ|θ(cid:48)) with
hyperparameter θ(cid:48). This is depicted in ﬁg(11.7b). The learning corresponds to ﬁnding the optimal θ(cid:48)
θ p(v|θ)p(θ|θ(cid:48)). This is known as an ML-II procedure since it
corresponds to maximum likelihood, but at the higher, hyperparameter level[33, 183]. This is a form of
approximate Bayesian analysis since, although θ(cid:48) is set using maximum likelihood, after training, we have
a distribution over parameters, p(θ|v, θ(cid:48)).
11.7 Optimising the Likelihood by Gradient Methods

that maximises the likelihood p(v|θ(cid:48)) = (cid:82)

11.7.1 Directed models

The EM algorithm typically works well when the amount of missing information is small compared to
the complete information.
In this case EM exhibits approximately the same convergence as Newton
based gradient method[237]. However, if the fraction of missing information approaches unity, EM can
converge very slowly. In the case of continuous parameters θ, an alternative is to compute the gradient of
the likelihood directly and use this as part of a standard continuous variable optimisation routine. The
gradient is straightforward to compute using the following identity. Consider the log likelihood

L(θ) = log p(v|θ)

The derivative can be written

(11.7.1)

(11.7.2)

(11.7.3)

∂θL(θ) =

p(v, h|θ)
At this point, we take the derivative inside the integral

p(v|θ) ∂θp(v|θ) =

h

1

1

(cid:90)

(cid:90)

1
p(v|θ) ∂θ
(cid:90)

∂θL(θ) =

p(v|θ)

h

∂θp(v, h|θ) =

h

p(h|v, θ)∂θ log p(v, h|θ) = (cid:104)∂θ log p(v, h|θ)(cid:105)p(h|v,θ)

where we used ∂ log f(x) = (1/f(x))∂f(x). The right hand side is the average of the derivative of the
log complete likelihood. This is closely related to the derivative of the energy term in the EM algorithm,
though note that the average here is performed with respect the current distribution parameters θ and not
θold as in the EM case. Used in this way, computing the derivatives of latent variable models is relatively
straightforward. These derivatives may then be used as part of a standard optimisation routine such as
conjugate gradients[237].

236

DRAFT March 9, 2010

Exercises

11.7.2 Undirected models

Consider an undirected model which contains both hidden and visible variables

p(v, h|θ) =

1
Z(θ) eφ(v,h)

For i.i.d. data, the log likelihood on the visible variables is (assuming discrete v and h)

which has gradient

L(θ) =(cid:88)

n

L =(cid:88)

n

∂
∂θ

h

log(cid:88)


(cid:28) ∂
(cid:124)

∂θ

eφ(vn,h|θ) − log(cid:88)

h,v



eφ(v,h|θ)

(cid:29)
(cid:123)(cid:122)
φ(vn, h|θ)

clamped average

p(h|vn)

(cid:28) ∂
(cid:124)

∂θ

−

(cid:29)

(cid:123)(cid:122)
φ(v, h|θ)

free average



(cid:125)

p(h,v)

(cid:125)

(11.7.4)

(11.7.5)

(11.7.6)

For a Markov Network that is intractable (the partition function Z cannot be computed eﬃciently), the
gradient is particularly diﬃcult to estimate since it is the diﬀerence of two quantities, each of which needs
to be estimated. Even getting the sign of the gradient correct can therefore be computationally diﬃcult.
For this reason learning in models, such as the Boltzmann machine with hidden units, is particularly
diﬃcult.

11.8 Code

In the demo code we take the original Chest Clinic network [170] and draw data samples from this network.
Our interest is then to see if we can use the EM algorithm to estimate the tables based on the data (with
some parts of the data missing at random). We assume that we know the correct BN structure, only that
the CPTs are unknown. We assume the logic gate table is known, so we do not need to learn this.
demoEMchestclinic.m: Demo of EM in learning the Chest Clinic Tables

The following code implements Maximum Likelihood learning of BN tables based on data with possibly
missing values.
EMbeliefnet.m: EM training of a Belief Network

11.9 Exercises

Exercise 138 (Printer Nightmare continued). Continuing with the BN given in ﬁg(9.19), the following
table represents data gathered on the printer, where ? indicates that the entry is missing. Each column
represents a datapoint. Use the EM algorithm to learn all CPTs of the network.

fuse assembly malfunction

drum unit
toner out

poor paper quality

worn roller
burning smell

poor print quality

wrinkled pages

multiple pages fed

paper jam

?
?
1
1
0
0
1
0
0
?

?
0
1
0
0
?
1
0
?
0

?
?
0
1
?
?
1
1
1
1

1
0
?
0
?
1
0
0
0
1

0
1
?
1
?
0
1
0
?
?

0
0
1
?
0
0
1
0
0
0

?
0
0
1
1
0
0
?
1
1

0
1
1
0
?
0
1
0
0
1

?
?
0
1
0
0
0
1
1
1

0
?
?
1
0
?
0
?
?
1

0
1
0
?
?
0
1
0
0
0

?
1
1
1
0
?
1
0
0
?

1
?
?
1
?
1
?
1
?
0

?
0
0
?
1
0
?
1
0
1

1
0
?
0
1
?
0
1
1
?

DRAFT March 9, 2010

237

The table is contained in EMprinter.mat, using states 1, 2, nan in place of 0, 1, ? (since BRMLtoolbox
requires states to be numbered 1,2,....). Given no wrinkled pages, no burning smell and poor print quality,
what is the probability there is a drum unit problem?

Exercise 139. Consider the following distribution over discrete variables,

p(x1, x2, x3, x4, x5) = p(x1|x2)p(x2|x3)p(x3|x4)p(x4|x5)p(x5),

(11.9.1)

Exercises

in which the variables x2 and x4 are consistently hidden in the training data, and training data for x1, x3, x5
are always present. Show that the EM update for the table p(x1|x2) is given by

pnew(x1 = i|x2 = j) =

I [xn
I [xn

n

n,k

1 = i] pold(x2 = j|xn
1 = k] pold(x2 = j|xn

1 , xn
1 , xn

5 )
3 , xn
5 )
3 , xn

Exercise 140. Consider a simple two variable BN

p(y, x) = p(y|x)p(x)

(11.9.3)
where both y and x are binary variables, dom(x) = {1, 2}, dom(y) = {1, 2}. You have a set of training
data {(yn, xn) , n = 1, . . . , N}, in which for some cases xn may be missing. We are speciﬁcally interested
in learning the table p(x) from this data. A colleague suggests that one can set p(x) by simply looking at
datapoints where x is observed, and then setting p(x = 1) to be the fraction of observed x that is in state
1. Explain how this suggested procedure relates to Maximum Likelihood and EM.

Exercise 141. Assume that a sequence is generated by a Markov chain. For a single chain of length T ,
we have

(cid:80)
(cid:80)

T−1(cid:89)

t=1

(11.9.2)

(11.9.4)

(11.9.5)

(11.9.6)

p(v1, . . . , vT ) = p(v1)

p(vt+1|vt)

For simplicity, we denote the sequence of visible variables as

v = (v1, . . . , vT )

For a single Markov chain labelled by h,

T−1(cid:89)

t=1

p(v|h) = p(v1|h)

p(vt+1|vt, h)

H(cid:88)

h=1

In total there are a set of H such Markov chains (h = 1, . . . , H). The distribution on the visible variables
is therefore

p(v) =

p(v|h)p(h)

(11.9.7)

1. There are a set of training sequences, vn, n = 1, . . . , N. Assuming that each sequence vn is inde-
pendently and identically drawn from a Markov chain mixture model with H components, derive the
Expectation Maximisation algorithm for training this model.

2. Write a general MATLAB function in the form

function [q,ph,pv,A]=mchain_mix(v,V,H,num_em_loops)

to perform EM learning for any set of (the same length) sequences of integers vn
t ∈ [1 : V ],
t = 1, . . . , T . v is a cell array of the training data: v{2}(4) is the 4th time element of the sec-
ond training sequence. Each element, say v{2}(4) must be an integer from 1 to V . V is the
number of states of the visible variables (in the bio-sequence case below, this will be 4). H is the

238

DRAFT March 9, 2010

Exercises

number of mixture components. num_em_loops is the number of EM iterations. A is the transi-
tion matrix A{h}(i,j)=p(v(t+1)=i|v(t)=j,h). pv is the prior state of the ﬁrst visible variable,
pv{h}(i)=p(v(t=1)=i|h). ph is a vector of prior probabilities for the mixture state ph(h)=p(h).
q is the cell array of posterior probabilities q{mu}(h)=p(h|v{mu}). Your routine must also display,
for each EM iteration, the value of the log likelihood. As a check on your routine, the log likelihood
must increase at each iteration.

3. The ﬁle sequences.mat contains a set of ﬁctitious bio-sequence in a cell array sequences{mu}(t).
Thus sequences{3}(:) is the third sequence, GTCTCCTGCCCTCTCTGAAC which consists of 20 timesteps.
There are 20 such sequences in total. Your task is to cluster these sequences into two clusters,
assuming that each cluster is modelled by a Markov chain. State which of the sequences belong
together by assigning a sequence vn to that state for which p(h|vn) is highest.

Exercise 142. Write a general purpose routine VBbeliefnet(pot,x,pars) along the lines of EMbeliefnet.m
that performs Variational Bayes under a Dirichlet prior, using a factorised parameter approximation. As-
sume both global and local parameter independence for the prior and the approximation q, section(9.3.1).

Exercise 143. Consider a 3 ‘layered’ Boltzmann Machine which has the form

p(v, h1, h2, h3|θ) =

1
Z

φ(v, h1|θ1)φ(h1, h2|θ2)φ(h2, h3|θ3)

(11.9.8)

where dim v = dim h1 = dim h2 = dim h3 = V

(cid:80)V
φ(x, y|θ) = e

All variables are binary with states 0, 1 and the parameters for each layer l are θl =(cid:8)Wl, Al, Bl(cid:9).

i,j=1 Wij xiyj +Aij xixj +Bij yiyj

(11.9.9)

1. In terms of ﬁtting the model to visible data v1, . . . , vN , is the 3 layered model above any more powerful

than ﬁtting a two-layered model (the factor φ(h2, h3|θ3) is not present in the two-layer case)?

2. If we use a restricted potential

(cid:80)

φ(x, y|θ) = e

i,j Wij xiyj

(11.9.10)

is the three layered model more powerful in being able to ﬁt the visible data than the two-layered
model?

Exercise 144. The sigmoid Belief Network is deﬁned by the layered network

p(xL)

p(xl−1|xl)

(11.9.11)

where vector variables have binary components xl ∈ {0, 1}wl and the width of layer l is given by wl. In
addition

L(cid:89)

l=1

wl(cid:89)

i=1

p(xl−1|xl) =

p(xl−1

i

|xl)

and

p(xl−1

i = 1|xl) = σ

(cid:16)

i,lxl(cid:17)

wT

,

σ(x) = 1/(1 + e

−x)

(11.9.12)

(11.9.13)

for a weight vector wi,l describing the interaction from the parental layer. The top layer, p(xL) describes
a factorised distribution p(xL

1 ), . . . , p(xL

wL).

1. Draw the Belief Network structure of this distribution.

2. For the layer x0, what is the computational complexity of computing the likelihood p(x0), assuming

that all layers have equal width w?

DRAFT March 9, 2010

239

3. Assuming a fully factorised approximation for an equal width network,

p(x1, . . . , xL|x0) ≈

q(xl
i)

L(cid:89)

w(cid:89)

l=1

i=1

Exercises

(11.9.14)

write down the energy term of the Variational EM procedure for a single data observation x0, and
discuss the tractability of computing the energy.

Exercise 146. A 2 × 2 probability table, p(x1 = i, x2 = j) = θi,j, with 0 ≤ θi,j ≤ 1,(cid:80)2

Exercise 145. Show how to ﬁnd the components 0 ≤ (θb, θg, θp) ≤ 1 that maximise equation (11.1.10).
j=1 θi,j = 1 is

learned using maximal marginal likelihood in which x2 is never observed. Show that if

(cid:80)2

i=1

(cid:18) 0.3 0.3
(cid:18) 0.2 0.4

0.2 0.2

0.4

0

(cid:19)
(cid:19)

θ =

θ =

is given as a maximal marginal likelihood solution, then

(11.9.15)

(11.9.16)

has the same marginal likelihood score.

240

DRAFT March 9, 2010

CHAPTER 12

Bayesian Model Selection

12.1 Comparing Models the Bayesian Way

Given two models M1 and M2 with parameters θ1, θ2 and associated parameter priors,

p(x, θ1|M1) = p(x|θ1, M1)p(θ1|M1),

p(x, θ2|M2) = p(x|θ2, M2)p(θ2|M2)

(12.1.1)

how can we compare the performance of the models in ﬁtting a set of data D = {x1, . . . , xN}? The
application of Bayes’ rule to models gives a framework for answering questions like this – a form of
Bayesian Hypothesis testing, applied at the model level. More generally, given an indexed set of models
M1, . . . , Mm, and associated prior beliefs in the appropriateness of each model p(Mi), our interest is the
model posterior probability

p(Mi|D) = p(D|Mi)p(Mi)

p(D)

where

p(D) =

m(cid:88)

i=1

p(D|Mi)p(Mi)
(cid:90)

Model Mi is parameterised by θi, and the model likelihood is given by

p(D|Mi) =

p(D|θi, Mi)p(θi|Mi)dθi

(12.1.2)

(12.1.3)

(12.1.4)

In discrete parameter spaces, the integral is replaced with summation. Note that the number of parameters
dim (θi) need not be the same for each model.

A point of caution here is that p(Mi|D) only refers to the probability relative to the set of models speciﬁed
M1, . . . , Mm. This is not the absolute probability that model M ﬁts ‘well’. To compute such a quantity
would require one to specify all possible models. Whilst interpreting the posterior p(Mi|D) requires some
care, comparing two competing model hypotheses Mi and Mj is straightforward and only requires the
Bayes’ factor

p(Mi|D)
p(Mj|D)

(cid:124)
(cid:123)(cid:122)
(cid:125)
= p(D|Mi)
p(D|Mj)

Bayes’ Factor

p(Mi)
p(Mj)

which does not require integration/summation over all possible models.

241

(12.1.5)

Illustrations : coin tossing

(a)

(b)

Figure 12.1: (a): Discrete prior model of a ‘fair’ coin. (b): Prior for a biased ‘unfair’ coin. In both cases
we are making explicit choices here about what we consider to be a ‘fair’ and and ‘unfair’.

12.2 Illustrations : coin tossing

We’ll consider two illustrations. The ﬁrst uses a discrete parameter space to keep the mathematics simple.
In the second we use a continuous parameter space.

12.2.1 A discrete parameter space

A simple choice would be to consider two competing models, one corresponding to a fair coin, and the other
a biased coin. The bias of the coin, namely the probability that the coin will land heads, is speciﬁed by θ,
so that a truly fair coin has θ = 0.5. For simplicity we assume dom(θ) = {0.1, 0.2, . . . , 0.9}. For the fair
coin we use the distribution p(θ|Mf air) in ﬁg(12.1a) and for the biased coin the distribution p(θ|Mbiased)
in ﬁg(12.1b).

For each model M, the likelihood is given by

p(D|M) =(cid:88)

p(D|θ, M)p(θ|M) =(cid:88)

θ

θNH (1 − θ)NT p(θ|M)

θ

= 0.1NH (1 − 0.1)NT p(θ = 0.1|M) + . . . + 0.9NH (1 − 0.9)NT p(θ = 0.9|M)

(12.2.1)

(12.2.2)

Assuming that p(Mf air) = p(Mbiased) the Bayes’ factor is given by the ratio of the two model likelihoods.

Example 55 (Discrete parameter space).

5 Heads and 2 Tails Here p(D|Mf air) = 0.00786 and p(D|Mbiased) = 0.0072. The Bayes’ factor is

p(Mf air|D)
p(Mbiased|D)

= 1.09

(12.2.3)

indicating that there is little to choose between the two models.

50 Heads and 20 Tails Here p(D|Mf air) = 1.5 × 10−20 and p(D|Mbiased) = 1.4 × 10−19. The Bayes’

factor is

p(Mf air|D)
p(Mbiased|D)

= 0.109

(12.2.4)

indicating that have around 10 times the belief in the biased model as opposed to the fair model.

12.2.2 A continuous parameter space

Here we repeat the above calculation but for continuous parameter spaces.

242

DRAFT March 9, 2010

00.10.20.30.40.50.60.70.80.9100.20.40.60.800.10.20.30.40.50.60.70.80.9100.050.10.15Illustrations : coin tossing

(a)

(b)

Figure 12.2: Probability density priors on the probability of a Head p(θ). (a): For a fair coin, p(θ|Mf air) =
(b): For an biased coin, p(θ|Mbiased) = 0.5 (B (θ|3, 10) + B (θ|10, 3)). Note the diﬀerent
B (θ|50, 50).
vertical scales in the two cases.

Fair coin

For the fair coin, a uni-modal prior is appropriate. We use Beta distribution

p(θ) = B (θ|a, b) ,

B (θ|a, b) ≡

1

B(a, b) θa−1 (1 − θ)b−1

(12.2.5)

for convenience since as this is conjugate to the binomial distribution the required integrations are trivial.
A reasonable choice for a fair coin is a = 50, b = 50, as shown in ﬁg(12.2a).

(cid:90)

1

B(a, b)

θ

θa−1 (1 − θ)b−1 θNH (1 − θ)NT

(cid:90)

p(θ)θNH (1 − θ)NT =
1

B(a, b)

θ

θNH +a−1 (1 − θ)NT +b−1 = B(NH + a, NT + b)

B(a, b)

(cid:90)

θ

In general,

p(D|Mf air) =
=

Biased coin

(12.2.6)

(12.2.7)

For the biased coin, we use a bimodal distribution formed, for convenience, as a mixture of two Beta
distributions:

1
2

[B (θ|a1, b1) + B (θ|a2, b2)]

(12.2.8)

as shown in ﬁg(12.2b). The model likelihood p(D|Mbiased) is given by
(cid:90)

(cid:90)

θ

p(θ|Mbiased) =
(cid:90)

1

(cid:26)
p(θ|Mbiased)θNH (1 − θ)NT
1
(cid:26) B(NH + a1, NT + b1)
2

B(a1, b1)

θ

B(a1, b1)

=

=

1
2

θa1−1 (1θ)b1−1 θNH (1 − θ)NT +

B(a2, b2)

θ

θa2−1 (1 − θ)b2−1 θNH (1 − θ)NT

1

(cid:27)

+ B(NH + a2, NT + b2)

B(a2, b2)

(12.2.9)

(cid:27)

(12.2.10)

(12.2.11)

Assuming no prior preference for either a fair or biased coin p(M) = const., and repeating the above
scenario in the discrete parameter case:

Example 56 (Continuous parameter space).

DRAFT March 9, 2010

243

00.20.40.60.810246800.20.40.60.8100.511.5Occam’s Razor and Bayesian Complexity Penalisation

Figure 12.3: The likelihood of the total dice score, p(t|n) for n = 1 (top) to n = 5 (bottom) die. Plotted
along the horizontal axis is the total score t. The vertical line marks the comparison for p(t = 9|n) for the
diﬀerent number of die. The more complex models, which can reach more states, have lower likelihood,
due to normalisation over t.

5 Heads and 2 Tails Here p(D|Mf air) = 0.0079 and p(D|Mbiased) = 0.00622. The Bayes’ factor is

p(Mf air|D)
p(Mbiased|D)

= 1.27

(12.2.12)

indicating that there is little to choose between the two models.

50 Heads and 20 Tails Here p(D|Mf air) = 9.4 × 10−21 and p(D|Mbiased) = 1.09 × 10−19. The Bayes’

factor is

p(Mf air|D)
p(Mbiased|D)

= 0.087

(12.2.13)

indicating that have around 11 times the belief in the biased model as opposed to the fair model.

12.3 Occam’s Razor and Bayesian Complexity Penalisation

We return to the dice scenario of section(1.3.1). There we assumed there are two die whose scores s1 and
s2 are not known. Only the sum of the two scores t = s1 + s2 is known. We then computed the posterior
joint score distribution p(s1, s2|t = 9) for the two die. We repeat the calculation but now for multiple dice
and with the twist that we don’t know how many dice there are1, only that the sum of the scores is 9.
i=1 si and are given the value t = 9. However, we are not told the number of die

That is, we know t =(cid:80)n

involved n. Assuming that any number n is equally likely, what is the posterior distribution over n?

From Bayes’ rule, we need to compute the posterior distribution over models

In the above

p(t)

p(n|t) = p(t|n)p(n)
p(t|n) = (cid:88)

s1,...,sn

p(t, s1, . . . , sn|n) = (cid:88)

s1,...,sn

p(t|s1, . . . , sn)(cid:89)

i

p(si) = (cid:88)

s1,...,sn

(cid:34)

I

t =

(cid:35)(cid:89)

i

si

n(cid:88)

i=1

(12.3.1)

p(si)

(12.3.2)

1This description of Occam’s razor is due to Taylan Cemgil.

244

DRAFT March 9, 2010

000.10.2000.10.2000.10.2000.10.212345678910111213141516171819202122232425262728293000.10.2A continuous example : curve ﬁtting

0.5

0

1

2

3

4

5

6

n

Figure 12.4: The posterior distribution p(n|t = 9) of
the number of die given the observed summed score
of 9.

where p(si) = 1/6 for all scores si. By enumerating all 6n states, we can explicitly compute p(t|n), as
displayed in ﬁg(12.3). The important observation is that as the models explaining the data become more
‘complex’ (n increases), more states become accessible and the probability mass typically reduces. We see
this eﬀect at p(t = 9|n) where, apart from n = 1, the value of p(t = 9|n) decreases with increasing n since
the higher n have mass in more states, becoming more spread out. Assuming p(n) = const., the posterior
p(n|t = 9) is plotted in ﬁg(12.4). A posteriori, there are only 3 plausible models, namely n = 2, 3, 4 since
the rest are either too complex, or impossible. This demonstrates the Occam’s razor eﬀect which penalises
models which are over complex.

12.4 A continuous example : curve ﬁtting

Consider an additive set of periodic functions

y0 = w0 + w1 cos(x) + w2 cos(2x) + . . . + wK cos(Kx)

(12.4.1)

This can be conveniently written in vector form

y0 = wTφ(x)

(12.4.2)
where φ(x) is a K + 1 dimensional vector with elements (1, cos(x), cos(2x), . . . , cos(Kx))T and the vector
w contains the weights of the additive function. We are given a set of data D = {(xn, yn), n = 1, . . . , N}
drawn from this distribution, where y is the clean y0(x) corrupted with additive zero mean Gaussian noise
with variance σ2,

yn = y0(xn) + n,

n ∼ N

see ﬁg(12.5). Assuming i.i.d. data, we are interested in the posterior probability of the number of coeﬃ-
cients, given the observed data:

(cid:0)n 0, σ2(cid:1)
= p(K)(cid:81)
(cid:90)

p(D)

(12.4.3)

(12.4.4)

(12.4.5)

We will assume p(K) = const. The likelihood term above is given by the integral

p(K|D) = p(D|K)p(K)

p(D)

n p(xn)

p(y1, . . . , yN|x1, . . . , xN , K) =

p(w|K)

w

p(y1, . . . , yN|x1, . . . , xN , K)
N(cid:89)

p(yn|xn, w, K)

n=1

For p(w|K) = N (w 0, IK/α), the integrand is a Gaussian in w for which it is straightforward to evaluate
the integral, (see section(8.6) and exercise(149))

2 log p(y1, . . . , yN|x1, . . . , yN , K) = N log(cid:0)2πσ2(cid:1)

−

N(cid:88)

n=1

(yn)2
σ2 + bTA−1b− log det (2πA) + K log (2πα)

K

w

xn

yn
0

yn

N

(cid:82)

for regression under the i.i.d.

Figure 12.5: Belief Network representation of a Hierarchical Bayesian
Model
data assumption. Note that
0 are included to highlight the role of the
the intermediate nodes on yn
p(y|y0)p(y0|w, x) =
‘clean’ underlying model.
y0 N
with the intermediate node y0 and place directly arrows from w and xn to
yn.

Since p(y|w, x) = (cid:82)
(cid:0)y wTx, σ2(cid:1), we can if desired do away

(cid:0)y y0, σ2(cid:1) δ(cid:0)y0 − wTx(cid:1) = N

y0

DRAFT March 9, 2010

245

Approximating the Model Likelihood

(a)

(b)

(c)

Figure 12.6: (a) The data generated with additive Gaussian noise σ = 0.5 from a K = 5 component model.
(b) The posterior p(K|D). (c) The reconstruction of the data using (cid:104)w(cid:105)T φ(x) where (cid:104)w(cid:105) is the mean
posterior vector of the optimal dimensional model p(w|D, K = 5). Plotted in the continuous line is the
reconstruction. Plotted in dots is the true underlying clean data.

where

A ≡ αI +

1
σ2

N(cid:88)

n=1

φ(xn)φT(xn),

N(cid:88)

n=1

b ≡

1
σ2

ynφ(xn)

(12.4.6)

(12.4.7)

Assuming α = 1 and σ = 0.5, we sampled some data from a model with K = 5 components, ﬁg(12.6a).
We assume that we know the correct noise level σ. The posterior p(K|D) plotted in ﬁg(12.6b) is sharply
peaked at K = 5, which is the ‘correct’ value used to generate the data. The clean reconstructions for
K = 5 are plotted in ﬁg(12.6c).

12.5 Approximating the Model Likelihood

For a model with continuous parameter vector θ, dim (θ) = K and data D, the model likelihood is

(cid:90)

p(D|M) =

p(D|θ, M)p(θ|M)dθ

θ

For a generic expression

p(D|θ, M)p(θ|M) = e

−f (θ)

(12.5.1)

(12.5.2)

(12.5.3)

(12.5.4)

unless f is of a particularly simple form (quadratic in θ for example), one cannot compute the integral in
(12.5.1) and approximations are required.

12.5.1 Laplace’s method

A simple approximation of (12.5.1) is given by Laplace’s method, section(28.2),

log det(cid:0)2πH−1(cid:1)

log p(D|M) ≈ −f(θ∗) +
where θ∗ is the MAP solution

1
2

θ∗ = argmax

p(D|θ, M)p(θ|M)
and H is the Hessian of f(θ) at θ∗.

θ

246

DRAFT March 9, 2010

−10−50510−3−2−101231234567891000.20.40.60.81−10−50510−3−2−10123Exercises

For data D =(cid:8)x1, . . . , xN(cid:9) that is i.i.d. generated the above specialises to

(cid:90)

p(D|M) =

p(θ|M)

θ

N(cid:89)

n=1

p(xn|θ, M)dθ
N(cid:88)

−f(θ) = log p(θ|M) +

log p(xn|θ, M)

n=1

In this case Laplace’s method computes the optimum of the function

(12.5.5)

(12.5.6)

12.5.2 Bayes information criterion (BIC)

For i.i.d. data the Hessian scales with the number of training examples, N, and a crude approximation
is to set H ≈ NIK where K = dim θ. In this case one may take as a model comparison procedure the
function

log p(D|M) ≈ log p(D|θ∗

, M) + log p(θ∗

|M) + K
2

log 2π −

K
2

log N

(12.5.7)

For a simple prior that penalises the length of the parameter vector, p(θ|M) = N (θ 0, I), the above
reduces to

log p(D|M) ≈ log p(D|θ∗

, M) −

1
2

(θ∗)T θ∗

K
2

−

log N

The Bayes Information Criterion[244] approximates (12.5.7) by ignoring the penalty term, giving

BIC = log p(D|θ∗

, M) −

K
2

log N

(12.5.8)

(12.5.9)

The BIC criterion may be used as an approximate way to compare models, where the term − K
2 log N
penalises model complexity. In general, the Laplace approximation, equation (12.5.3), is to be preferred
to the BIC criterion since it more correctly accounts for the uncertainty in the posterior parameter esti-
mate. Other techniques that aim to improve on the Laplace method are discussed in section(28.3) and
section(28.7).

12.6 Exercises

Exercise 147. Write a program to implement the fair/biased coin tossing model selection example of
section(12.2.1) using a discrete domain for θ. Explain how to overcome potential numerical issues in
dealing with large NH and NT (of the order of 1000).

Exercise 148. You work at Dodder’s hedge fund and the manager wants to model next day ‘returns’ yt+1
based on current day information xt. The vector of ‘factors’ each day, xt captures essential aspects of the
market. He argues that a simple linear model

K(cid:88)

k=1

yt+1 =

wkxkt

(12.6.1)

should be reasonable and asks you to ﬁnd the weight vector w, based on historical information D =
{(xt, yt+1), t = 1, . . . , T − 1}. In addition he also gives you a measure of the ‘volatility’ σ2
t for each day.

1. Under the assumption that the returns are i.i.d. Gaussian distributed

T(cid:89)

t=2

(cid:16)

T(cid:89)

t=2

N

(cid:17)

p(y1:T|x1:T , w) =

p(yt|xt−1, w) =

yt wTxt−1, σ2
t

explain how to set the weight vector w by Maximum Likelihood.

DRAFT March 9, 2010

(12.6.2)

247

Exercises

2. Your hedge fund manager is however convinced that some of the factors are useless for prediction
and wishes to remove as many as possible. To do this you decide to use a Bayesian model selection
method in which you use a prior

p(w|M) = N (w 0, I)

(12.6.3)

where M = 1, . . . , 2K − 1 indexes the model. Each model uses only a subset of the factors. By
translating the integer M into a binary vector representation, the model describes which factors are
to be used. For example if K = 3, there would be 7 models

{0, 0, 1} ,{0, 1, 0} ,{1, 0, 0} ,{0, 1, 1} ,{1, 0, 1} ,{1, 1, 0} ,{1, 1, 1}

(12.6.4)

where the ﬁrst model is yt = w3x3 with weight prior p(w3) = N (w3 0, 1). Similarly model 7 would
be yt = w1x1 + w2x2 + w3x3 with p(w1, w2, w3) = N ((w1, w2, w3) (0, 0, 0), I3). You decide to use
a ﬂat prior p(M) = const. Draw the hierarchical Bayesian network for this model and explain how
to ﬁnd the best model for the data using Bayesian model selection by suitably adapting equation
(12.4.6).

3. Using the data dodder.mat, perform Bayesian model selection as above for K = 6 and ﬁnd which

of the factors x1, . . . , x6 are most likely to explain the data.

Exercise 149. Here we will derive the expression (12.4.6) and also an alternative form.

1. Starting from

N(cid:89)

n=1

p(yn|w, xn, K) = N (w 0, I/α)(cid:89)

p(w)

(cid:16)

yn wTφ(xn), σ2(cid:17)
(cid:80)
n(yn−wTφ(xn))2

− 1
2σ2

1

(2πσ2)N/2 e

n N
2 wTw

− α

e

Show that this can be expressed as

1

=

√2πα−1
(cid:80)

n(yn)2

− 1

e

− 1
2σ2

1

√2πα−1

where

1

(2πσ2)N/2 e
(cid:88)

1
σ2

n

A = αI +

φ(xn)φT(xn)

2 wTAw+bTw

(cid:88)

n

b =

1
σ2

ynφ(xn)

(12.6.5)

(12.6.6)

(12.6.7)

(12.6.8)

2. By completing the square (see section(8.6.2)), derive (12.4.6).

3. Since each yn, n = 1, . . . , N is linearly related through w and w is Gaussian distributed, the joint
vector y1, . . . , yN is Gaussian distributed. Using the Gaussian propagation results, section(8.6.3),
derive an alternative expression for log p(y1, . . . , yN|x1, . . . , xN ).

248

DRAFT March 9, 2010

Part III

Machine Learning

249

CHAPTER 13

Machine Learning Concepts

13.1 Styles of Learning

Broadly speaking the main two subﬁelds of machine learning are supervised learning and unsupervised
learning. In supervised learning the focus is on accurate prediction, whereas in unsupervised learning the
aim is to ﬁnd accurate compact descriptions of the data.

Particularly in supervised learning, one is interested in methods that perform well on previously unseen
data. That is, the method ‘generalises’ to unseen data. In this sense, one distinguishes between data that
is used to train a model, and data that is used to test the performance of the trained model, see ﬁg(13.1).

13.1.1 Supervised learning

Consider a database of face images, each represented by a vector1 x. Along with each image x is an
output class y ∈ {male, female} that states if the image is of a male or female. A database of 10000 such
image-class pairs is available, D = {(xn, yn) , n = 1, . . . , 10000}. The task is to make an accurate predictor
y(x∗) of the sex of a novel image x∗. This is an example application that would be hard to program in
a traditional ‘programming’ manner since formally specifying how male faces diﬀer from female faces is
diﬃcult. An alternative is to give examples faces and their gender labels and let a machine automatically
‘learn’ a rule to diﬀerentiate male from female faces.

Deﬁnition 88 (Supervised Learning). Given a set of data D = {(xn, yn) , n = 1, . . . , N} the task is to
‘learn’ the relationship between the input x and output y such that, when given a new input x∗ the
predicted output y∗ is accurate. To specify explicitly what accuracy means one deﬁnes a loss function
L(ypred, ytrue) or, conversely, a utility function U = −L.
In supervised learning our interest is describing y conditioned on knowing x. From a probabilistic modelling
perspective, we are therefore concerned primarily with the conditional distribution p(y|x,D). The term
‘supervised’ indicates that there is a ‘supervisor’ specifying the output y for each input x in the available
data D. The output is also called a ‘label’, particularly when discussing classiﬁcation.

Predicting tomorrow’s stock price y(T +1) based on past observations y(1), . . . , y(T ) is a form of supervised
learning. We have a collection of times and prices D = {(t, y(t)) , t = 1, . . . , T} where time t is the ‘input’
and the price y(t) is the output.
1For an m × n face image with elements Fmn we can form a vector by stacking the entries of the matrix. In MATLAB

one may achieve this using x=F(:).

251

Train

Test

Styles of Learning

Figure 13.1: In training and evaluating a model, conceptually
there are two sources of data. The parameters of the model are
set on the basis of the train data only. If the test data is gener-
ated from the same underlying process that generated the train
data, an unbiased estimate of the generalisation performance
can be obtained by measuring the test data performance of the
trained model.
Importantly, the test performance should not
be used to adjust the model parameters since we would then no
longer have an independent measure of the performance of the
model.

Example 57. A father decides to teach his young son what a sports car is. Finding it diﬃcult to explain
in words, he decides to give some examples. They stand on a motorway bridge and, as each car passes
underneath, the father cries out ‘that’s a sports car!’ when a sports car passes by. After ten minutes, the
father asks his son if he’s understood what a sports car is. The son says, ‘sure, it’s easy’. An old red VW
Beetle passes by, and the son shouts – ‘that’s a sports car!’. Dejected, the father asks – ‘why do you say
that?’. ‘Because all sports cars are red!’, replies the son.

This is an example scenario for supervised learning. Here the father plays the role of the supervisor, and his
son is the ‘student’ (or ‘learner’). It’s indicative of the kinds of problems encountered in machine learning
in that it is not really clear anyway what a sports car is – if we knew that, then we wouldn’t need to go
through the process of learning. This example also highlights the issue that there is a diﬀerence between
performing well on training data and performing well on novel test data. The main interest in supervised
learning is to discover an underlying rule that will generalise well, leading to accurate prediction on new
inputs.

For an input x, if the output is one of a discrete number of possible ‘classes’, this is called a classiﬁcation
problem. In classiﬁcation problems we will generally use c for the output.

For an input x, if the output is continuous, this is called a regression problem. For example, based on
historical information of demand for sun-cream in your supermarket, you are asked to predict the demand
for the next month. In some cases it is possible to discretise a continuous output and then consider a
corresponding classiﬁcation problem. However, in other cases it is impractical or unnatural to do this; for
example if the output y is a high dimensional continuous valued vector, or if the ordering of states of the
variable is meaningful.

13.1.2 Unsupervised learning

Deﬁnition 89 (Unsupervised learning). Given a set of data D = {xn, n = 1, . . . , N} in unsupervised
learning we aim to to ‘learn’ a plausible compact description of the data. An objective is used to quantify
the accuracy of the description.

In unsupervised learning there is no special ‘prediction’ variable. From a probabilistic perspective we are
interested in modelling the distribution p(x). The likelihood of the data under the i.i.d. assumption, for
example, would be one objective measure of the accuracy of the description.

252

DRAFT March 9, 2010

Styles of Learning

Example 58. A supermarket chain wishes to discover how many diﬀerent basic consumer buying
behaviours there are based on a large database of supermarket checkout data.
Items brought by a
customer on a visit to a checkout are represented by a (very sparse) 10,000 dimensional vector x which
contains a 1 in the ith element if the customer bought product i and 0 otherwise. Based on 10 million

such checkout vectors from stores across the country, D = (cid:8)xn, n = 1, . . . , 107(cid:9) the supermarket chain

wishes to discover patterns of buying behaviour.

In the table each column represents the buying patterns of a
customer (7 customer records and just the ﬁrst 6 of the 10,000
products are shown). A 1 indicates that the customer bought
that item. We wish to ﬁnd common patterns in the data, such
as if someone buys coﬀee they are also likely to buy milk.

coﬀee
tea
milk
beer

diapers
aspirin

1
0
1
0
0
0

0
0
0
0
0
1

0
1
1
0
1
0

1
0
1
1
0
0

0
0
0
1
1
1

0
0
1
0
0
0

0
0
1
1
1
1

· ·
· ·
· ·
· ·
· ·
· ·

Example 59 (Clustering).

The table on the right represents a collection of unla-
belled two-dimensional points. We can visualise this
data by plotting it in 2 dimensions.

x1
x2

-2
7

-6
22

-1
1

11
1

-1
-8

46
52

33
40

42
33

32
54

45
39

By simply eye-balling the data, we can see that there are two
apparent clusters here, one centred around (0,5) and the other
around (35,45). A reasonable model to describe this data might
therefore be to describe it as two clusters, centred at (0,0) and
(35,35), each with a standard deviation of around 10.

13.1.3 Anomaly detection

A baby processes a mass of initially confusing sensory data. After a while the baby begins to understand
her environment in the sense that novel sensory data from the same environment is familiar or expected.
When a strange face presents itself, the baby recognises that this is not familiar and may be upset. The
baby has learned a representation of the familiar and can distinguish the expected from the unexpected;
this is an example of unsupervised learning. Models that can detect irregular events are used in plant
monitoring and require a model of normality which will in most cases be based on unlabelled data.

13.1.4 Online (sequential) learning
In the above situations, we assumed that the data D was given beforehand. In online learning data arrives
sequentially and we want to continually update our model as new data becomes available. Online learning
may occur in either a supervised or unsupervised context.

13.1.5 Interacting with the environment

In many real-world situations, an agent is able to interact in some manner with its environment.

Query (Active) Learning Here the agent has the ability to request data from the environment. For
example, a predictor might recognise that it is less conﬁdently able to predict in certain regions of
the space x and therefore requests more training data in this region. Active Learning can also be
considered in an unsupervised context in which the agent might request information in regions where
p(x) looks uninformative or ‘ﬂat’.

DRAFT March 9, 2010

253

−100102030405001020304050Supervised Learning

Reinforcement Learning One might term this also ‘survival learning’. One has in mind scenarios
such as encountered in real-life where an organism needs to learn the best actions to take in its
environment in order to survive as long as possible. In each situation in which the agent ﬁnds itself
it needs to take an action. Some actions may eventually be beneﬁcial (lead to food for example),
whilst others may be disastrous (lead to being eaten for example). Based on accumulated experience,
the agent needs to learn which action to take in a given situation in order to obtain a desired long
term goal. Essentially actions that lead to long term rewards need to reinforced. Reinforcement
learning has connections with control theory, Markov decision processes and game theory. Whilst
we discussed MDPs and brieﬂy mentioned how an environment can be learned based on delayed
rewards in section(7.8.3), we will not discuss this topic further in this book.

13.1.6 Semi-supervised learning

In machine learning, a common scenario is to have a small amount of labelled and a large amount of
unlabelled data. For example, it may be that we have access to many images of faces; however, only a
small number of them may have been labelled as instances of known faces. In semi-supervised learning,
one tries to use the unlabelled data to make a better classiﬁer than that based on the labelled data alone.

13.2 Supervised Learning

Supervised and unsupervised learning are mature ﬁelds with a wide range of practical tools and associated
theoretical analyses. Our aim here is to give a brief introduction to the issues and ‘philosophies’ behind
the approaches. We focus here mainly on supervised learning and classiﬁcation in particular.

13.2.1 Utility and Loss

To more fully specify a supervised problem we need to be clear what ‘cost’ is involved in making a correct
or incorrect prediction. In a two class problem dom(c) = {1, 2}, we assume here that everything we know
about the environment is contained in a model p(x, c). Given a new input x∗, the optimal prediction also
depends on how costly making an error is. This can be quantiﬁed using a loss function (or conversely a
utility). In forming a decision function c(x∗) that will produce a class label for the new input x∗, we don’t
know the true class, only our presumed distribution p(c|x∗). The expected utility for the decision function

is

∗)) = (cid:88)

ctrue

U(c(x

U(ctrue, c(x

∗))p(ctrue|x

∗)

(13.2.1)

and the optimal decision is that which maximises the expected utility.

Zero-one loss

0 if c∗

U(ctrue, c

∗) =

(cid:54)= ctrue
For the two class case, we then have

A ‘count the correct predictions’ measure of prediction performance is based on the ‘zero-one’ utility (or
conversely the zero-one loss):

(cid:26) 1 if c∗ = ctrue
(cid:26) p(ctrue = 1|x∗) for c(x∗) = 1
p(ctrue = 2|x∗) for c(x∗) = 2
Hence, in order to have the highest expected utility, the decision function c(x∗) should correspond to
(cid:26) 1 if p(c = 1|x∗) ≥ 0.5
selecting the highest class probability p(c|x∗):
2 if p(c = 2|x∗) ≥ 0.5

∗) =

c(x

U(c(x

∗)) =

(13.2.2)

(13.2.3)

(13.2.4)

In the case of a tie, either class is selected at random with equal probability.

254

DRAFT March 9, 2010

Supervised Learning

General loss functions

In general, for a two-class problem, we have

(cid:26) U(ctrue = 1, c∗ = 1)p(ctrue = 1|x∗) + U(ctrue = 2, c∗ = 1)p(ctrue = 2|x∗) for c(x∗) = 1
U(ctrue = 1, c∗ = 2)p(ctrue = 1|x∗) + U(ctrue = 2, c∗ = 2)p(ctrue = 2|x∗) for c(x∗) = 2

U(c(x

∗)) =

(13.2.5)

and the optimal decision function c(x∗) chooses that class with highest expected utility.

One can readily generalise this to multiple-class situations using a utility matrix with elements

Ui,j = U(ctrue = i, cpred = j)

(13.2.6)

where the i, j element of the matrix contains the utility of predicting class j when the true class is i.
Conversely one could think of a loss-matrix with entries Lij = −Uij. The expected loss with respect to
p(c|x) is then termed the risk.
In some applications the utility matrix is highly non-symmetric. Consider a medical scenario in which we
are asked to predict whether or not the patient has cancer dom(c) = {cancer, benign}. If the true class is
cancer yet we predict benign, this could have terrible consequences for the patient. On the other hand,
if the class is benign yet we predict cancer, this may be less disastrous for the patient. Such asymmetric
utilities can bias the predictions in favour of conservative decisions – in the cancer case, we would be more
inclined to decide the sample is cancerous than benign, even if the predictive probability of the two classes
is equal.

13.2.2 What’s the catch?

In solving for the optimal decision function c(x∗) above we are assuming that the model p(c|x) is ‘correct’.

The catch is therefore that in practice :

• We typically don’t know the correct model underlying the data – all we have is a dataset of examples
D = {(xn, yn) , n = 1, . . . , N} and our domain knowledge.
• We want our method to perform well not just on a speciﬁcally chosen x∗, but any new input that
could come along – that is we want it to generalise to novel inputs. This means we also need a
model for p(x) in order to measure what the expected performance of our decision function would
be. Hence we require knowledge of the joint distribution p(c, x) = p(c|x)p(x).

We therefore need to form a distribution p(x, c|D) which should ideally be close to the true but unknown
joint data distribution. Communities of researchers in machine learning form around diﬀerent strategies
to address the lack of knowledge about the true p(c, x).

13.2.3 Using the empirical distribution

A direct approach to not knowing the correct model ptrue(c, x) is to replace it with the empirical distribution

p(x, c|D) =

1
N

N(cid:88)

n=1

δ (x, xn) δ (c, cn)

(13.2.7)

(cid:88)

1
N

That is, we assume that the underlying distribution is approximated by placing equal mass on each of the
points (xn, cn) in the dataset. Using this gives the empirical utility

(cid:104)U(c, c(x))(cid:105)p(c,x|D) =

n
or conversely the empirical risk

(cid:88)

n

R =

1
N

L(cn, c(xn))

DRAFT March 9, 2010

U(cn, c(xn))

(13.2.8)

(13.2.9)

255

Supervised Learning

Train

Validate

Test

Figure 13.2: Models can be trained using the train data based on
diﬀerent regularisation parameters. The optimal regularisation
parameter is determined by the empirical performance on the
validation data. An independent measure of the generalisation
performance is obtained by using a separate test set.

Assuming the loss is minimal when the correct class is predicted, the optimal decision c(x) for any input

in the train set is trivially given by c(xn) = cn. However, for any new x∗ not contained in D then c(x∗) is

undeﬁned. In order to deﬁne the class of a novel input, one may use a parametric function

For example for a two class problem dom(c) = {1, 2}, a linear decision function is given by

c(x) = f(x|θ)

(cid:26) 1 if θTx + θ0 ≥ 0

2 if θTx + θ0 < 0

f(x|θ) =

If the vector input x is on the positive side of a hyperplane deﬁned by the vector θ and bias θ0, we assign
it to class 1, otherwise to class 2. (We return to the geometric interpretation of this in chapter(17)). The
empirical risk then becomes a function of the parameters θ = {θ, θ0},

(cid:88)

n

R(θ|D) =

1
N

L(cn, f(xn|θ))

The optimal parameters θ are given by minimising the empirical risk with respect to θ,

θopt = argmin

θ

R(θ|D)

The decision for a new datapoint x∗ is then given by f(x∗

|θopt).

In this empirical risk minimisation approach, as we make the decision function f(x|θ) more complex, the
empirical risk goes down. If we make f(x|θ) too complex we will have no conﬁdence f(x|θ) will perform
well on a novel input x∗. To constrain the complexity of f(x|θ) we may minimise the penalised empirical
risk

R

(cid:48)(θ|D) = R(θ|D) + λP (θ)

For the linear decision function above, it is reasonable to penalise wildly changing classiﬁcations in the
sense that if we change the input x by only a small amount we expect (on average) minimal change in the

class label. The squared diﬀerence in θTx + θ0 for two inputs x1 and x2 is(cid:0)θT∆x(cid:1)2 where ∆x ≡ x2 − x1.

By constraining the length of θ to be small we would then limit the ability of the classiﬁer to change class
for only a small change in input space2. This motivates a penalised risk of the form

(13.2.10)

(13.2.11)

(13.2.12)

(13.2.13)

(13.2.14)

(13.2.15)

R

(cid:48)(θ, θ0|D) = R(θ, θ0|D) + λθTθ

where λ is a regularising constant,. We subsequently minimise this penalised empirical risk with respect
to θ, θ0. We discuss how to ﬁnd an appropriate setting for the regularisation constant λ below.

Validation

In penalised empirical risk minimisation we need to set the regularisation parameter λ. This can be
achieved by evaluating the performance of the learned classiﬁer f(x|θ) on validation data Dvalidate for sev-
eral diﬀerent λ values, and choosing the one with the best performance. It’s important that the validation
data is not the data on which the model was trained since we know that the optimal setting for λ in that
case is zero, and again we will have no conﬁdence in the generalisation ability.

2Assuming the distance between two datapoints is distributed according to an isotropic multivariate Gaussian with zero
= σ2θTθ, motivating the choice of the Euclidean squared

mean and covariance σ2I, the average squared change is
length of the parameter θ as the penalty term.

(cid:68)(cid:0)θT∆x(cid:1)2(cid:69)

256

DRAFT March 9, 2010

Supervised Learning

Algorithm 12 Setting regularisation parameters using cross-validation.

2: Choose a set of training and validation set splits(cid:8)

1: Choose a set of regularisation parameters λ1, . . . , λA
Di
train,Di

(cid:9) , i = 1, . . . , I

validate

3: for a = 1 to A do
4:
5:

for i = 1 to I do
a = argmin
θi

(cid:2)R(θ|Di

train) + λaP (θ)(cid:3)

θ

(cid:80)I

L(λa)

end for
L(λa) = 1
I

6:
7:
8: end for
9: λopt = argmin

λa

i=1 R(θi

validate)

a|Di

train and validation Di

Given an original dataset D we split this into disjoint parts, Dtrain,Dvalidate, where the size of the vali-
dation set is usually chosen to be smaller than the train set. For each parameter λa one then ﬁnds the
minimal empirical risk parameter θa. This splitting procedure is repeated, each time producing a separate
training Di
validation set, along with an optimal penalised empirical risk parameter θi
a
and associated (unregularised) validation performance R(θi
validate). The performance of regularisation
parameter λa is taken as the average of the validation performances over i. The best regularisation param-
eter is then given as that with the minimal average validation error, see algorithm(12) and ﬁg(13.2). Using
the optimal regularisation parameter λ, many practitioners retrain θ on the basis of the whole dataset D.
In cross-validation a dataset is partitioned into training and validation sets multiple times with validation
results obtained for each partition. More speciﬁcally, in K-fold cross validation the data D is split into K
validate. This gives a total of
equal sized disjoint parts D1, . . . ,DK. Then Di
K diﬀerent training-validation sets over which performance is averaged, see ﬁg(13.3). In practice 10-fold
cross validation is popular, as is leave-one-out cross validation in which the validation sets consist of only
a single example.

validate = Di and Di

train = D\Di

a|Di

Beneﬁts of the empirical risk approach

For a utility U(ctrue, cpred) and penalty P (θ), the empirical risk approach is summarised in ﬁg(13.4).

• In the limit of a large amount of training data the empirical distribution will tend towards the correct

distribution.

• The discriminant function is chosen on the basis of minimal risk, which is the quantity we are

ultimately interested in.

• The procedure is conceptually straightforward.

Train

Validate

Train

Validate

Train

Validate

Train

Test

DRAFT March 9, 2010

Figure 13.3: In cross-validation the original dataset is split into
several train-validation sets. Depicted is 3-fold cross-validation.
For a range of regularisation parameters, the optimal regular-
isation parameter is found based on the empirical validation
performance averaged across the diﬀerent splits.

257

X ,C

p(x, c)

x∗

θ

c∗ = f(x∗

|θ)

U(c, f(x|θ)) − λP (θ)

Supervised Learning

Figure 13.4: Empirical risk approach. Given
the dataset X ,C, a model of the data p(x, c)
is made, usually using the empirical distribu-
tion. For a classiﬁer f(x|θ), the parameter θ
is learned by maximising the penalised empir-
ical utility (or minimising empirical risk) with
respect to θ. The penalty parameter λ is set by
validation. A novel input x∗ is then assigned to
class f(x∗

|θ), given this optimal θ.

(a)

(b)

Figure 13.5: (a): The unregularised ﬁt (λ = 0) to training given by ×. Whilst the training data is well
(b): The regularised ﬁt (λ = 0.5). Whilst the
ﬁtted, the error on the validation examples, + is high.
train error is high, the validation error (which is all important) is low. The true function which generated
this noisy data is the dashed line; the function learned from the data is given by the solid line.

Drawbacks of the empirical risk approach

• It seems extreme to assume that the data follows the empirical distribution, particularly for small
amounts of training data. To generalise well, we need to make sensible assumptions as to p(x) – that
is the distribution for all x that could arise.

• If the utility (or loss) function changes, the discriminant function needs to be retrained.
• Some problems require an estimate of the conﬁdence of the prediction. Whilst there may be heuristic

ways to evaluating conﬁdence in the prediction, this is not inherent in the framework.

• When there are multiple penalty parameters, performing cross validation in a discretised grid of the

parameters becomes infeasible.

• It seems a shame to discard all those trained models in cross-validation – can’t they be combined in

some manner and used to make a better predictor?

Example 60 (Finding a good regularisation parameter). In ﬁg(13.5), we ﬁt the function a sin(wx) to
data, learning the parameters a and w. The unregularised solution ﬁg(13.5a) badly overﬁts the data, and
has a high validation error. To encourage a smoother solution, a regularisation term Ereg = w2 is used.
The validation error based on several diﬀerent values of the regularisation parameter λ was computed,
ﬁnding that λ = 0.5 gave a low validation error. The resulting ﬁt to novel data, ﬁg(13.5b) is reasonable.

13.2.4 Bayesian decision approach
An alternative to using the empirical distribution is to ﬁt a model p(c, x|θ) to the train data D. Given
this model, the decision function c(x) is automatically determined from the maximal expected utility (or

258

DRAFT March 9, 2010

−3−2−10123−1.5−1−0.500.511.5−3−2−10123−1.5−1−0.500.511.5Supervised Learning

X ,C

θ

p(x, c|θ)

x∗

p(c|x∗, θ)

U(c, c∗)

c∗

Figure 13.6: Bayesian decision approach. A
model p(x, c|θ) is ﬁtted to the data. After lean-
ing, this model is used to compute p(c|x, θ). For
a novel x∗, we then ﬁnd the distribution of the
assumed ‘truth’, p(c|x∗, θ). The prediction (de-
cision) is then given by that c∗ which maximises
the expected utility (cid:104)U(c, c∗)(cid:105)p(c|x∗,θ).

minimal risk), with respect to this model, as in equation (13.2.5), in which the unknown p(ctrue|x) is
replaced with p(c|x, θ). This approach therefore divorces learning the parameters θ of p(c, x|θ) from the
utility (or loss).

Beneﬁts of the Bayesian decision approach

• This is a conceptually ‘clean’ approach, in which one tries ones best to model the environment,
independent of the subsequent decision process. In this case learning the environment is separated
from the ultimate eﬀect this will have on the expected utility.

• The ultimate decision c∗ for a novel input x∗ can be a highly complex function of x∗ due to the

maximisation operation.

Drawbacks of the Bayesian decision approach

• If the environment model p(c, x|θ) is poor, the prediction c∗ could be highly inaccurate since mod-

elling the environment is divorced from prediction.

• To avoid fully divorcing the learning of the model p(c, x|θ) from its eﬀect on decisions, in practice one
often includes regularisation terms in the environment model p(c, x|θ) which are set by validation
based on an empirical utility.

There are two main approaches to ﬁtting p(c, x|θ) to data D. We could parameterise the joint distribution
using

p(c, x|θ) = p(c|x, θc|x)p(x|θx)

discriminative approach

or

p(c, x|θ) = p(x|c, θx|c)p(c|θc)

generative approach

(13.2.16)

(13.2.17)

We’ll consider these two approaches below in the context of trying to make a system that can distinguish
between a male and female face. We have a database of face images in which each image is represented as
a real-valued vector xn, n = 1, . . . , N), along with a label cn ∈ {0, 1} stating if the image is male or female.

Generative approach p(x, c|θ) = p(x|c, θx|c)p(c|θc)
For simplicity we use Maximum Likelihood training for the parameters θ. Assuming the data D is i.i.d.,
we have a log likelihood

log p(D|θ) =(cid:88)

log p(xn|cn, θx|c) +(cid:88)

n

n

log p(cn|θc)

(13.2.18)

As we see the dependence on θx|c occurs only in the ﬁrst term, and θc only occurs in the second. This
means that learning the optimal parameters is equivalent to isolating the data for the male-class and

DRAFT March 9, 2010

259

Supervised Learning

θc

cn

θx|c

θc|x

θx

xn

N

cn

xn

N

(a)

(b)

Figure 13.7: Two generic strategies for probabilis-
(a): Class dependent generative
tic classiﬁcation.
model of x. After learning parameters, classiﬁca-
tion is obtained by making x evidential and inferring
(b): A discriminative classiﬁcation method
p(c|x).
p(c|x).

ﬁtting a model p(x|c = male, θx|male). We similarly isolate the female data and ﬁt a separate model
p(x|c = female, θx|female). The class distribution p(c|θc) can be easily set according to the ratio of
males/females in the set of training data.
To make a classiﬁcation of a new image x∗ as either male or female, we may use

p(c = male|x∗) =

p(x∗, c = male|θx|male)

p(x∗, c = male|θx|male) + p(x∗, c = female|θx|female)

(13.2.19)

Based on zero-one loss, if this probability is greater than 0.5 we classify x∗ as male, otherwise female. More
generally, we may use this probability as part of a decision process, as in equation (13.2.5).

Advantages Prior information about the structure of the data is often most naturally speciﬁed through
a generative model p(x|c). For example, for male faces, we would expect to see heavier eyebrows, a
squarer jaw, etc.

Disadvantages The generative approach does not directly target the classiﬁcation model p(c|x) since
the goal of generative training is rather to model p(x|c). If the data x is complex, ﬁnding a suitable
generative data model p(x|c) is a diﬃcult task. On the other hand it might be that making a model
of p(c|x) is simpler, particularly if the decision boundary between the classes has a simple form, even
if the data distribution of each class is complex, see ﬁg(13.8). Furthermore, since each generative
model is separately trained for each class, there is no competition amongst the models to explain
the data.

Discriminative approach p(c, x) = p(c|x, θc|x)p(x|θx)
Assuming i.i.d. data, the log likelihood is

log p(D|θ) =(cid:88)

log p(cn|xn, θc|x) +(cid:88)

(cid:88)

n

n

n

log p(xn|θx)

(13.2.20)

The parameters are isolated in the two terms so that Maximum Likelihood training is equivalent to ﬁnding
the parameters of θc|x that will best predict the class c for a given training input x. The parameters θx for
modelling the data occur separately in the second term above, and setting them can therefore be treated
as a separate unsupervised learning problem. This approach therefore isolates modelling the decision
boundary from modelling the input distribution, see ﬁg(13.8).
Classiﬁcation of a new point x∗ is based on

p(c|x, θopt
c|x )

(13.2.21)

As for the generative case, this approach still learns a joint distribution p(c, x) = p(c|x)p(x) which can be
used as part of a decision process if required.
Advantages The discriminative approach directly addresses making an accurate classiﬁer based on p(c|x),
modelling the decision boundary, as opposed to the class conditional data distribution in the gener-
ative approach. Whilst the data from each class may be distributed in a complex way, it could be
that the decision boundary between them is relatively easy to model.

260

DRAFT March 9, 2010

Supervised Learning

Figure 13.8: Each point represents a high dimensional vector with an
associated class label, either male or female. The point x∗ is a new
point for which we would like to predict whether this should be male or
female. In the generative approach, a male model p(x|male) generates
data similar to the ‘m’ points. Similarly, the female model p(x|female)
generates points that are similar to the ‘f’ points above. We then
use Bayes’ rule to calculate the probability p(male|x∗) using the two
we directly make a model of p(male|x∗), which cares less about how

the points ‘m’ or ‘f’ are distributed, but more about describing the
boundary which can separate the two classes, as given by the line.

ﬁtted models, as given in the text.

In the discriminative approach,

θc|h

θh

θx|h

cn

hn

xn

N

Figure 13.9: A strategy for semi-supervised learning. When cn is
missing, the term p(cn|hn) is absent. The large amount of training
data helps the model learn a good lower dimension/compressed rep-
resentation h of the data x. Fitting then a classiﬁcation model p(c|h)
using this lower dimensional representation may be much easier than
ﬁtting a model directly from the complex data to the class, p(c|x).

Disadvantages Discriminative approaches are usually trained as ‘black-box’ classiﬁers, with little prior
knowledge built in as to how data for a given class might look. Domain knowledge is often more
easily expressed using the generative framework.

Hybrid generative-discriminative approaches
One could use a generative description, p(x|c), building in prior information, and use this to form a
joint distribution p(x, c), from which a discriminative model p(c|x) may be formed, using Bayes’ rule.
Speciﬁcally, we can use

(cid:80)
p(x|c, θx|c)p(c|θc)
c p(x|c, θx|c)p(c|θc)

p(c|x, θ) =

and use a separate model for p(x|θx). Subsequently the parameters θ =(cid:0)θx|c, θc

(13.2.22)

(cid:1), for this hybrid model

can be found by maximising the probability of being in the correct class. This approach would appear to
leverage the advantages of both the discriminative and generative frameworks since we can more readily
incorporate domain knowledge in the generative model p(x|c, θx|c) yet train this in a discriminative way.
This approach is rarely taken in practice since the resulting functional form of the likelihood depends in a
complex manner on the parameters. In this case no separation occurs (as was previously the case for the
generative and discriminative approaches).

13.2.5 Learning lower-dimensional representations in semi-supervised learning

One way to exploit a large amount of unlabelled training data to improve classiﬁcation modelling is to try
to ﬁnd a lower dimensional representation h of the data x. Based on this, the mapping from h to c may
be rather simpler to learn than a mapping from x to c directly. To do so we can form the likelihood using,
see ﬁg(13.9),

p(C,X ,H|θ) =(cid:89)
(cid:8)p(cn|hn, θc|h)(cid:9)I[cn(cid:54)=∅]
(cid:88)

n

θopt = argmax

p(C,X ,H|θ)

θ

H

DRAFT March 9, 2010

and then set any parameters for example by using Maximum Likelihood

p(xn|hn, θx|h)p(h|θh)

(13.2.23)

(13.2.24)

261

*mmmmmmmffffffffffxBayes versus Empirical Decisions

13.2.6 Features and preprocessing

It is often the case that when attempting to make a predictive model, transforming the raw input x into
a form that more directly captures the relevant label information can greatly improve performance. For
example, in the male-female classiﬁcation case, it might be that building a classiﬁer directly in terms of the
elements of the face vector x is diﬃcult. However, using ‘features’ which contain geometric information
such as the distance between eyes, width of mouth, etc. may make ﬁnding a classiﬁer easier. In practice
data is often preprocessed to remove noise, centre an image etc.

13.3 Bayes versus Empirical Decisions

The empirical risk and Bayesian approaches are at the extremes of the philosophical spectrum. In the
empirical risk approach one makes a seemingly over-simplistic data generating assumption. However
decision function parameters are set based on the task of making decisions. On the other hand, the
Bayesian approach attempts to learn p(c, x) without regard to its ultimate use as part of a larger decision
process. What ‘objective’ criterion can we use to learn p(c, x), particularly if we are only interested in
classiﬁcation with a low test-risk? The following example is intended to recapitulate the two generic Bayes
and empirical risk approaches we’ve been considering.

Example 61 (The two generic decision strategies). Consider a situation in which, based on patient
information x, we need to take a decision d as whether or not to operate. The utility of operating u(d, c)
depends on whether or not the patient has cancer. For example

u(operate, cancer) = 100
u(don’t operate, cancer) = 0 u(don’t operate, benign) = 70

u(operate, benign) = 30

(13.3.1)

We have independent true assessments of whether or not a patient had cancer, giving rise to a set of
historical records D = {(xn, cn), n = 1, . . . , N}. Faced with a new patient with information x, we need to
make a decision whether or not to operate.

In the Bayesian decision approach one would ﬁrst make a model p(c|x,D) (for example Logistic regression).
Using this model the decision is given by that which maximises the expected utility

d = argmax

d

[p(cancer|x,D)u(d, cancer) + p(benign|x,D)u(d, benign)]

(13.3.2)

In this approach learning the model p(c|x,D) is divorced from the ultimate use of the model in the
decision making process. An advantage of this approach is that, from the viewpoint of expected utility,
it is optimal – provided the model p(c|x,D) is ‘correct’. Unfortunately, this is rarely the case. Given the
limited model resources, it might make sense to focus on ensuring the prediction of cancer is correct since
this has a more signiﬁcant eﬀect on the utility. However, formally, this is not possible in this framework.

The alternative empirical utility approach recognises that the task can be stated as to translate patient
information x into an operation decision d. To do so one could parameterise this as d(x) = f(x|θ) and
then learn θ under maximising the empirical utility

u(f(xn|θ), cn)

(13.3.3)

For example, if x is a vector representing the patient information and θ the parameter, we might use a
linear decision function such as

(cid:26) θTx ≥ 0 d = operate

θTx < 0 d = don’t operate

u(θ) =(cid:88)

n

f(x|θ) =

The advantage of this approach is that the parameters of the decision are directly related to the utility
of making the decision. A disadvantage is that we cannot easily incorporate domain knowledge into the
decision function. It may be that we have a good model of p(c|x) and would wish to make use of this.

262

DRAFT March 9, 2010

(13.3.4)

Bayesian Hypothesis Testing for Outcome Analysis

Both approaches are heavily used in practice and which is to be preferred depends very much on the
problem. Whilst the Bayesian approach appears formally optimal, it is prone to model mis-speciﬁcation.
A pragmatic alternative Bayesian approach is to ﬁt a parameterised distribution p(c, x|λ) to the data D,
where λ penalises ‘complexity’ of the ﬁtted distribution, setting λ using validation on the risk. This has the
potential advantage of allowing one to incorporate sensible prior information about p(c, x) whilst assessing
competing models in the light of their actual predictive risk. Similarly, for the empirical risk approach,
one can modify the extreme empirical distribution assumption by using a more plausible model p(x, c) of
the data.

13.4 Representing Data

The numeric encoding of data can have a signiﬁcant eﬀect on performance and an understanding of the
options for representing data is therefore of considerable importance.

13.4.1 Categorical

For categorical (or nominal) data, the observed value belongs to one of a number of classes, with no intrinsic
ordering of the classes. An example of a categorical variable would be the description of the type of job
that someone does, e.g. healthcare, education, ﬁnancial services, transport, homeworker, unemployed,
engineering etc.

One way to transform this data into numerical values would be to use 1-of-m encoding. Here’s an example:
There are 4 kinds of jobs: soldier, sailor, tinker, spy. A soldier is represented as (1,0,0,0), a sailer as (0,1,0,0),
a tinker as (0,0,1,0) and a spy as (0,0,0,1). In this encoding the distance between the vectors representing
two diﬀerent professions is constant. It is clear that 1-of-m encoding induces dependencies in the profession
attributes since if one of the profession attributes is 1, the others must be zero.

13.4.2 Ordinal

An ordinal variable consists of categories with an ordering or ranking of the categories, e.g. cold, cool,
warm, hot. In this case, to preserve the ordering we could perhaps use -1 for cold, 0 for cool, +1 for warm
and +2 for hot. This choice is somewhat arbitrary, and one should bear in mind that results will generally
be dependent on the numerical coding used.

13.4.3 Numerical

Numerical data takes on values that are real numbers, e.g. a temperature measured by a thermometer, or
the salary that someone earns.

13.5 Bayesian Hypothesis Testing for Outcome Analysis

How can we assess whether two classiﬁers are performing diﬀerently? For techniques which are based on
Bayesian classiﬁers p(c, θ|D, M) there will always be, in principle, a direct way to estimate the suitability
of the model M by computing p(M|D). We consider here the less fortunate situation where the only
information presumed available is the test performance of the two classiﬁers.

To outline the basic issue, let’s consider two classiﬁers A and B which predict the class of 55 test exam-
ples. Classiﬁer A makes 20 errors, and 35 correct classiﬁcations, whereas classiﬁer B makes 23 errors and
32 correct classiﬁcations. Is classiﬁer A better than classiﬁer B? Our lack of conﬁdence in pronouncing
that A is better than B results from the small number of test examples. On the other hand if classiﬁer
A makes 200 errors and 350 correct classiﬁcations, whilst classiﬁer B makes 230 errors and 320 correct
classiﬁcations, intuitively, we would be more conﬁdent that classiﬁer A is better than classiﬁer B.

Perhaps the most practically relevant question from a Machine Learning perspective is the probability
that classiﬁer A outperforms classiﬁer B, given the available test information. Whilst this question can

DRAFT March 9, 2010

263

Bayesian Hypothesis Testing for Outcome Analysis

be addressed using a Bayesian procedure, section(13.5.5), we ﬁrst focus on a simpler question, namely
whether classiﬁer A and B are the same[16].

13.5.1 Outcome analysis

The treatment in this section refers to outcomes and quantiﬁes if data is likely to come from the same
multinomial distribution. In the main we will apply this to assessing if two classiﬁers are essentially per-
forming the same, although one should bear in mind that the method applies more generally to assessing
if outcomes are likely to have been generated from the same or diﬀerent underlying processes.

Consider a situation where two classiﬁers A and B have been tested on some data, so that we have, for
each example in the test set, an outcome pair

(oa(n), ob(n)) , n = 1, . . . , N

(13.5.1)

where N is the number of test data points, and oa ∈ {1, . . . , Q} (and similarly for ob). That is, there are
Q possible types of outcomes that can occur.

For example, for binary classiﬁcation we will typically have the four cases

dom(o) = {TruePositive, FalsePositive, TrueNegative, FalseNegative}

(13.5.2)

If the classiﬁer predicts class c ∈ {true, false} and the truth is class t ∈ {true, false} these are deﬁned as

TruePositive
FalsePositive
TrueNegative
FalseNegative

c = true
c = true
c = false
c = false

t = true
t = false
t = false
t = true

(13.5.3)

We call oa = {oa(n), n = 1, . . . , N}, the outcomes for classiﬁer A, and similarly for ob = {ob(n), n = 1, . . . , N}
for classiﬁer B. To be speciﬁc we have two hypotheses we wish to test:

1. Hdiﬀ : oa and ob are from diﬀerent categorical distributions.

2. Hsame : oa and ob are from the same categorical distribution.

In both cases we will use categorical models p(oc = q|γ, H) = γc
q, with unknown parameters α (hypothesis
2 will correspond to using the same parameters γa = γb for both classiﬁers, and hypothesis 1 to using
diﬀerent parameters, as we will discuss below). In the Bayesian framework, we want to ﬁnd how likely it
is that a model/hypothesis is responsible for generating the data. For any hypothesis H calculate

p(H|oa, ob) = p(oa, ob|H)p(H)

p(oa, ob)

(13.5.4)

where p(H) is our prior belief that H is the correct hypothesis. Note that the normalising constant
p(oa, ob) does not depend on the hypothesis.

Under all hypotheses we will make the independence of trials assumption

N(cid:89)

n=1

p(oa, ob|H) =

p(oa(n), ob(n)|H).

(13.5.5)

To make further progress we need to state what the speciﬁc hypotheses mean.

264

DRAFT March 9, 2010

Bayesian Hypothesis Testing for Outcome Analysis

α

oa

β

α

ob

oa

ob

(a)

(b)

P

oa, ob

(c)

Figure 13.10: (a): Hdiﬀ : Corresponds to the out-
comes for the two classiﬁers being independently gen-
(b): Hsame: both outcomes are generated
erated.
(c): Hdep : the out-
from the same distribution.
comes are dependent (‘correlated’).

13.5.2 Hdiﬀ : model likelihood

We now use the above assumptions to compute the Hypothesis likelihood:

p(Hdiﬀ|oa, ob) = p(oa, ob|Hdiﬀ)p(Hdiﬀ)

p(oa, ob)

(13.5.6)

The outcome model for classiﬁer A is speciﬁed using parameters, α, giving p(oa|α, Hdiﬀ), and similarly
we use β for classiﬁer B. The ﬁnite amount of data means that we are uncertain as to these parameter
values, and therefore the joint term in the numerator above is

p(oa, ob)p(Hdiﬀ|oa, ob) =

p(oa, ob|α, β, Hdiﬀ)p(α, β|Hdiﬀ)p(Hdiﬀ)dαdβ

(13.5.7)

(cid:90)

(cid:90)

= p(Hdiﬀ)

where we assumed

p(oa|α, Hdiﬀ)p(α|Hdiﬀ)dα

p(ob|β, Hdiﬀ)p(β|Hdiﬀ)dβ (13.5.8)

p(α, β|Hdiﬀ) = p(α|Hdiﬀ)p(β|Hdiﬀ) and p(oa, ob|α, β, Hdiﬀ) = p(oa|α, Hdiﬀ)p(ob|β, Hdiﬀ)

(13.5.9)
Note that one might expect there to be a speciﬁc constraint that the two models A and B are diﬀerent.
However since the models are assumed independent and each has parameters sampled from an eﬀectively
inﬁnite set (α and β are continuous), the probability that sampled parameters α and β of the two models
are the same is zero.

Since we are dealing with categorical distributions, it is convenient to use the Dirichlet prior, which is
conjugate to the categorical distribution:

(cid:89)

1

Z(u)

q

p(α|Hdiﬀ) =

uq−1
α
q

,

Z(u) =

(cid:81)Q
(cid:16)(cid:80)Q

q=1 Γ(uq)
q=1 uq

(cid:17)

Γ

(13.5.10)

The prior hyperparameter u controls how strongly the mass of the distribution is pushed to the corners
of the simplex, see ﬁg(8.5). Setting uq = 1 for all q corresponds to a uniform prior. The likelihood of oa

(cid:90)

is given by(cid:90)

(cid:90) (cid:89)

(cid:89)

p(oa|α, Hdiﬀ)p(α|Hdiﬀ)dα =

1

(cid:93)a
q
q

α

Z(u)

q

q

uq−1
q

α

dα = Z(u + (cid:93)a)

Z(u)

(13.5.11)

where (cid:93)a is a vector with components (cid:93)a
Hence

q being the number of times that variable a is is state q in the data.

p(Hdiﬀ|oa, ob) = p(Hdiﬀ) Z(u + (cid:93)a)

Z(u)

Z(u + (cid:93)b)

Z(u)

where Z(u) is given by equation (13.5.10).

(13.5.12)

13.5.3 Hsame : model likelihood
In Hsame, the hypothesis is that the outcomes for the two classiﬁers are generated from the same categorical
distribution. Hence

(cid:90)

p(oa, ob)p(Hsame|oa, ob) = p(Hsame)

p(oa|α, Hsame)p(ob|α, Hsame)p(α|Hsame)dα

= p(Hsame) Z(u + (cid:93)a + (cid:93)b)

Z(u)

DRAFT March 9, 2010

(13.5.13)

(13.5.14)

265

Bayesian Hypothesis Testing for Outcome Analysis

Bayes’ factor

If we assume that we have no prior preference for either hypothesis (p(Hdiﬀ) = p(Hsame)), then

p(Hdiﬀ|oa, ob)
p(Hsame|oa, ob)

= Z(u + (cid:93)a)Z(u + (cid:93)b)
Z(u)Z(u + (cid:93)a + (cid:93)b)

(13.5.15)

This is the evidence to suggest that the data were generated by two diﬀerent categorical distributions.

Example 62. Two people classify the expression of each image into happy, sad or normal, using states
1, 2, 3 respectively. Each column of the data below represents an image classed by the two people (person
1 is the top row and person 2 the second row). Are the two people essentially in agreement?

1
1

3
3

1
1

3
2

1
2

1
3

3
3

2
3

2
2

3
3

1
3

1
2

1
2

1
2

1
2

1
1

1
2

1
1

1
3

2
2

To help answer this question, we perform a Hdiﬀ versus Hsame test. From this data, the count vector for
person 1 is [13, 3, 4] and for person 2, [4, 9, 7]. Based on a ﬂat prior for the categorical distribution and
assuming no prior preference for either hypothesis, we have the Bayes’ factor

p(persons 1 and 2 classify diﬀerently)
p(persons 1 and 2 classify the same)

= Z([14, 4, 5])Z([5, 10, 8])
Z([1, 1, 1])Z([18, 13, 12])

= 12.87

(13.5.16)

where the Z function is given in equation (13.5.10). This is strong evidence the two people are classifying
the images diﬀerently.

Below we discuss some further examples for the Hdiﬀ versus Hsame test. As above, the only quantities
we need for this test are the vector counts from the data. Let’s assume that there are three kinds of
outcomes, Q = 3. For example dom(o) = {good, bad, ugly} are our set of outcomes and we want to test if
two classiﬁers are essentially producing the same outcome distributions, or diﬀerent.

Example 63 (Hdiﬀ versus Hsame).

• We have the two outcome counts (cid:93)a = [39, 26, 35] and (cid:93)b = [63, 12, 25]. Then, the Bayes’ factor

equation (13.5.15) is 20.7 – strong evidence in favour of the two classiﬁers being diﬀerent.

• Alternatively, consider the two outcome counts (cid:93)a = [52, 20, 28] and (cid:93)b = [44, 14, 42]. Then, the
Bayes’ factor equation (13.5.15) is 0.38 – weak evidence against the two classiﬁers being diﬀerent.
• As a ﬁnal example, consider counts (cid:93)a = [459, 191, 350] and (cid:93)b = [465, 206, 329]. This gives a Bayes’
factor equation (13.5.15) of 0.008 – strong evidence that the two classiﬁers are statistically the same.

In all cases the results are consistent with the model in fact used to generate the count data – the two
outcomes for A and B were indeed from diﬀerent categorical distributions. The more test data we have,
the more conﬁdent we are in our statements.

13.5.4 Dependent outcome analysis

Here we consider the (perhaps more common) case that outcomes are dependent. For example, it is often
the case that if classiﬁer A works well, then classiﬁer B will also work well. Thus we want to evaluate the

266

DRAFT March 9, 2010

Bayesian Hypothesis Testing for Outcome Analysis

hypothesis:

Hdep : the outcomes that the two classiﬁers make are dependent

To do so we assume a categorical distribution over the joint states:

p(oa(n), ob(n)|P, Hdep)

[P ]ij = p(oa = i, ob = j)

Here P is a Q × Q matrix of probabilities:
(cid:90)

(cid:90)

so [P ]ij is the probability that A makes outcome i, and B makes outcome j. Then,

p(o|Hdep) =

p(o, P|Hdep)dP =

p(o|P, Hdep)p(P|Hdep)dP

where, for convenience, we write o = (oa, ob). Assuming a Dirichlet prior on P, with hyperparameters U,
we have

p(o)p(Hdep|o) = p(Hdep) Z(vec (U + (cid:93)))

Z(vec(U))

(13.5.20)

where vec(D) is a vector formed from concatenating the rows of the matrix D. Here (cid:93) is the count matrix,
with [(cid:93)]ij equal to the number of times that joint outcome (oa = i, ob = j) occurred in the N datapoints.
As before, we can then use this in a Bayes factor calculation. For the uniform prior, [U]ij = 1,∀i, j.
Testing for dependencies in the outcomes: Hdep versus Hdiﬀ
To test whether or not the outcomes of the classiﬁers are dependent Hdep against the hypothesis that they
are independent Hdiﬀ we may use, assuming p(Hdiﬀ) = p(Hdep),

(13.5.17)

(13.5.18)

(13.5.19)

(13.5.21)

(13.5.22)

(13.5.23)

(13.5.24)

(13.5.25)

267

p(Hdiﬀ|o)
p(Hdep|o)

= Z(u + (cid:93)a)

Z(u)

Z(u + (cid:93)b)

Z(vec(U))

Z(u)

Z(vec (U + (cid:93)))

Example 64 (Hdep versus Hdiﬀ).

• Consider the outcome count matrix (cid:93)

 98

7

93
168 13 163
245 12 201

so that (cid:93)a = [511, 32, 457], and (cid:93)b = [198, 344, 458]. Then





p(Hdiﬀ|o)
p(Hdep|o)

= 3020

 82

120 83
107 162
4
170 203 70

- strong evidence that the classiﬁers perform independently.

• Consider the outcome count matrix (cid:93)

so that (cid:93)a = [359, 485, 156], and (cid:93)b = [284, 273, 443]. Then

p(Hdiﬀ|o)
p(Hdep|o)

= 2 × 10−18

- strong evidence that the classiﬁers perform dependently.

These results are in fact consistent with the way the data was generated in each case.

DRAFT March 9, 2010

Bayesian Hypothesis Testing for Outcome Analysis

Testing for dependencies in the outcomes: Hdep versus Hsame
In practice, it is reasonable to believe that dependencies are quite likely in the outcomes that classiﬁers.
For example two classiﬁers will often do well on ‘easy’ test examples, and badly on ‘diﬃcult’ examples.
Are these dependencies strong enough to make us believe that the outcomes are coming from the same
process? In this sense, we want to test

p(Hsame|o)
p(Hdep|o)

= Z(u + (cid:93)a + (cid:93)b)

Z(u)

Z(vec(U))

Z(vec (U + (cid:93)))

Example 65 (Hdep versus Hsame).

• Consider an experiment which gives the test outcome count matrix (cid:93)



 105

42
45

172
42
29
192 203 170

so that (cid:93)a = [339, 290, 371], and (cid:93)b = [319, 116, 565]. Then

p(Hsame|o)
p(Hdep|o)

= 4.5 × 10−38

– strong evidence that the classiﬁers are performing diﬀerently.

• Consider an experiment which gives the test outcome count matrix (cid:93)

 15

8
4

10
8
5
13 12 25



so that (cid:93)a = [33, 24, 43], and (cid:93)b = [33, 17, 50]. Then

p(Hsame|o)
p(Hdep|o)

= 42

– strong evidence that the classiﬁers are performing the same.

These results are in fact consistent with the way the data was generated.

(13.5.26)

(13.5.27)

(13.5.28)

(13.5.29)

(13.5.30)

13.5.5 Is classiﬁer A better than B?

We return to the question with which we began this outcome analysis. Given the common scenario of
observing a number of errors for classiﬁer A on a test set and a number for B, can we say which classiﬁer
is better? This corresponds to the special case of binary classes Q = 2 with dom(e) = {correct, incorrect}.
Under the Hdiﬀ for this special case it makes sense to use a Beta distribution (which corresponds to the
Dirichlet when Q = 2). Then for θa being the probability that classiﬁer A generates a correct label we
have

p(oA|θa) = θ

Similarly

(cid:93)a
correct
a

(1 − θa)(cid:93)a

incorrect

p(oB|θb) = θ

(cid:93)b
correct
b

(1 − θb)(cid:93)b

incorrect

We assume independent identical Beta distribution priors

p(θa) = B (θa|u1, u2) ,

p(θb) = B (θb|u1, u2)

(13.5.31)

(13.5.32)

(13.5.33)

268

DRAFT March 9, 2010

Code

(a)

(b)

Figure 13.11: Two classiﬁers A and B and their posterior distributions of the probability that they
(a): For A with 35 correct and 20 incorrect labels,
classify correctly (using a uniform Beta prior).
B (x|1 + 35, 1 + 20) (solid curve); B with 32 correct 23 incorrect B (y|1 + 32, 1 + 23) (dashed curve). p(x >
y) = 0.719 (b): For A with 350 correct and 200 incorrect labels (solid curve), B (x|1 + 350, 1 + 200); B
with 320 correct 230 incorrect B (y|1 + 320, 1 + 230) (dashed curve), p(x > y) = 0.968. As the amount
of data increases the overlap between the distributions decreases and the certainty that one classiﬁer is
better than the other correspondingly increases.

where a ﬂat prior corresponds to using the hyperparameter setting u1 = u2 = 1. The posterior distribu-
tions for θa and θb are independent:

(cid:16)

(cid:17)

(13.5.34)

(13.5.35)

p(θa|oA) = B (θa|(cid:93)a

correct + u1, (cid:93)a

correct + u1, (cid:93)b

incorrect + u2

θb|(cid:93)b

The question of whether A is better than B can then be addressed by computing

incorrect + u2) , p(θb|oB) = B
(cid:90) 1

(cid:90) 1

xa−1 (1 − x)b−1

1

B(a, b)B(c, d)

0

yc−1 (1 − y)d−1 dydx

x

p(θa > θb|oA, oB) =

where

a = (cid:93)a

correct + u1,

b = (cid:93)a

incorrect + u2,

c = (cid:93)b

correct + u1,

d = (cid:93)b

incorrect + u2

(13.5.36)

Example 66. Classiﬁer A makes 20 errors, and 35 correct classiﬁcations, whereas classiﬁer B makes 23
errors and 32 correct classiﬁcations. Using a ﬂat prior this gives

p(θa > θb|oA, oB) = betaXbiggerY(1+35,1+20,1+32,1+23) = 0.719

(13.5.37)

On the other hand if classiﬁer A makes 200 errors and 350 correct classiﬁcations, whilst classiﬁer B makes
230 errors and 320 correct classiﬁcations, we have

p(θa > θb|oA, oB) = betaXbiggerY(1+350,1+200,1+320,1+230) = 0.968

(13.5.38)

This demonstrates the intuitive eﬀect that even though the proportion of correct/incorrect classiﬁcations
doesn’t change for the two scenarios, as we have more data our conﬁdence in determining the better
classiﬁer increases.

13.6 Code

demoBayesErrorAnalysis.m: Demo for Bayesian Error Analysis
betaXbiggerY.m: p(x > y) for x ∼ B (x|a, b), y ∼ B (y|c, d)

DRAFT March 9, 2010

269

00.10.20.30.40.50.60.70.80.910123456700.10.20.30.40.50.60.70.80.910510152013.7 Notes

A general introduction to machine learning is given in [200]. An excellent reference for Bayesian decision
theory is [33]. Approaches based on empirical risk are discussed in [282].

Exercises

(cid:0)x µ2, σ2

2

(cid:1), with

1

1, σ2

13.8 Exercises
Exercise 150. Given the distributions p(x|class1) = N
corresponding prior occurrence of classes p1 and p2 (p1 + p2 = 1), calculate the decision boundary explicitly
as a function of µ1, µ2, σ2
2, p1, p2. How many solutions are there to the decision boundary, and are they
all reasonable?
Exercise 151. Suppose that instead of using the Bayes’ decision rule to choose class k if p(Ck|x) > p(Cj|x)
for all j (cid:54)= k, we use a randomized decision rule, choosing class j with probability Q(Cj|x). Calculate the
error for this decision rule, and show that the error is minimized by using Bayes’ decision rule.

(cid:1) and p(x|class2) = N

(cid:0)x µ1, σ2

(cid:0)x m1, σ2(cid:1) and class 2 has the distribution p(x|c = 2) ∼ N

Exercise 152. Consider datapoints generated from two diﬀerent classes. Class 1 has the distribution
p(x|c = 1) ∼ N
probabilities of each class are p(c = 1) = p(c = 2) = 1/2. Show that the posterior probability p(c = 1|x) is
of the form

(cid:0)x m2, σ2(cid:1). The prior

p(c = 1|x) =

1

1 + exp−(ax + b)

and determine a and b in terms of m1, m2 and σ2.

(13.8.1)

Exercise 153. WowCo.com is a new startup prediction company. After years of failures, they eventually
ﬁnd a neural network with a trillion hidden units that achieves zero test error on every learning problem
posted on the internet up last week. Each learning problem included a training and test set. Proud of their
achievement, they market their product aggressively with the claim that it ‘predicts perfectly on all known
problems’. Would you buy this product? Justify your answer.

Exercise 154. Three people classify images into 1 of three categories. Each column in the table below
represents the classiﬁcations of each image, with the top row being the class from person 1, the middle
from person 2 and the bottom from person 3.

1
1
1

3
3
2

1
1
1

3
2
1

1
2
1

1
3
3

3
3
2

2
3
2

2
2
2

3
3
3

1
3
1

1
2
2

1
2
1

1
2
2

1
2
1

1
1
1

1
2
2

1
1
3

1
3
3

2
2
2

Assuming no prior preference amongst hypotheses and a uniform prior on counts, compute

p(persons 1, 2 and 3 classify diﬀerently)
p(persons 1, 2 and 3 classify the same)

(13.8.2)

Exercise 155 (Better than random guessing?). Consider a classiﬁer that makes R correct classiﬁcations
and W wrong classiﬁcations. Is the classiﬁer better than random guessing? Let D be the fact that there
are R right and W wrong answers. Assume also that the classiﬁcations are i.i.d.

1. Show that under the hypothesis the data is generated purely at random,

p(D|Hrandom) = 0.5R+W

2. Deﬁne θ to be the probability that the classiﬁer makes an error. Then

p(D|θ) = θR (1 − θ)W

270

(13.8.3)

(13.8.4)

DRAFT March 9, 2010

Exercises

Then consider

p(D|Hnon random) =

(cid:90)

θ

p(D|θ)p(θ)

Show that for a Beta prior, p(θ) = B (θ|a, b)
p(D|Hnon random) = B(R + a, W + b)

B(a, b)

where B(a, b) is the beta-function.

3. Considering the random and non-random Hypotheses as a priori equally likely, show that

p(Hrandom|D) =

0.5R+W

0.5R+W + B(R+a,W +b)

B(a,b)

(13.8.5)

(13.8.6)

(13.8.7)

4. For a ﬂat prior a = b = 1 compute the probability that for 10 correct and 12 incorrect classiﬁcations,
the data is from a purely random distribution (according to equation (13.8.7)). Repeat this for 100
correct and 120 incorrect classiﬁcations.

5. Show that the standard deviation in the number of errors of a random classiﬁer is 0.5√R + W and

relate this to the above computation.

Exercise 156. For a prediction model ˜p(y|x) and true data generating distribution p(x, y), we deﬁne the
accuracy as

(cid:90)

A =

p(x, y)˜p(y|x)

x,y

2. You are given a set of training data D = {xn, yn, n = 1, . . . , N}. By taking q(x, y) to be the empirical

This shows that the prediction accuracy is lower bounded by the training accuracy and the ‘gap’ be-
tween the empirical distribution and the unknown true data generating mechanism. In theories such
as PAC Bayes, one may bound this gap, resulting in a bound on the predictive accuracy. According
to this naive bound, the best thing to do to increase the prediction accuracy is to increase the training
accuracy (since the ﬁrst Kullback-Leibler term is independent of the predictor). As N increases, the
ﬁrst term Kullback-Leibler term becomes small, and minimising the training error is justiﬁable.

DRAFT March 9, 2010

271

1. By deﬁning

p(x, y)˜p(y|x)

A

ˆp(x, y) ≡
and considering

KL(q(x, y)|ˆp(x, y)) ≥ 0

show that for any distribution q(x, y),

log A ≥ −KL(q(x, y)|p(x, y)) + (cid:104)log ˜p(y|x)(cid:105)q(x,y)

distribution

q(x, y) =

1
N

show that

n(cid:88)

n=1

δ (x, xn) δ (y, yn)

log A ≥ −KL(q(x, y)|p(x, y)) +

n(cid:88)

n=1

1
N

log ˜p(yn|xn)

(13.8.8)

(13.8.9)

(13.8.10)

(13.8.11)

(13.8.12)

(13.8.13)

Assuming that the training data are drawn from a distribution p(y|x) which is deterministic, show
that

Exercises

n(cid:88)

n=1

log A ≥ −KL(q(x)|p(x)) +

1
N

log ˜p(yn|xn)

(13.8.14)

and hence that, provided the training data is correctly predicted, (q(yn|xn) = p(yn|xn), the accuracy
can be related to the empirical input distribution and true input distribution by

−KL(q(x)|p(x))

A ≥ e

(13.8.15)

272

DRAFT March 9, 2010

CHAPTER 14

Nearest Neighbour Classiﬁcation

14.1 Do As Your Neighbour Does

Successful prediction typically relies on smoothness in the data – if the class label can change arbitrarily
as we move a small amount in the input space, the problem is essentially random and no algorithm will
generalise well. machine learning researchers therefore construct appropriate measures of smoothness for
the problem they have at hand. Nearest neighbour methods are a good starting point since they readily
encode basic smoothness intuitions and are easy to program, forming a useful baseline method.

In a classiﬁcation problem each input vector x has a corresponding class label, cn ∈ {1, . . . , C}. Given
a dataset of N such training examples, D = {xn, cn} , n = 1, . . . , N, and a novel x, we aim to return
the correct class c(x). A simple, but often eﬀective, strategy for this supervised learning problem can be
stated as: for novel x, ﬁnd the nearest input in the training set and use the class of this nearest input,
algorithm(13).
For vectors x and x(cid:48) representing two diﬀerent datapoints, we measure ‘nearness’ using a dissimilarity
function d(x, x(cid:48)). A common dissimilarity is the squared Euclidean distance

d(x, x(cid:48)) = (x − x(cid:48))T(x − x(cid:48))

which can be more conveniently written (x − x(cid:48))2. Based on the squared Euclidean distance, the decision
boundary is determined by the lines which are the perpendicular bisectors of the closest training points
with diﬀerent training labels, see ﬁg(14.1). This is called a Voronoi tessellation.

The nearest neighbour algorithm is simple and intuitive. There are, however, some issues:

(14.1.1)

(14.1.2)

• How should we measure the distance between points? Whilst the Euclidean square distance is
popular, this may not always be appropriate. A fundamental limitation of the Euclidean distance is
that it does not take into account how the data is distributed. For example if the length scales of
x vary greatly the largest length scale will dominate the squared distance, with potentially useful
class-speciﬁc information in other components of x lost. The Mahalanobis distance

d(x, x(cid:48)) =(cid:0)x − x(cid:48)(cid:1)T Σ−1(cid:0)x − x(cid:48)(cid:1)

where Σ is the covariance matrix of the inputs (from all classes) can overcome some of these problems
since it rescales all length scales to be essentially equal.

• The whole dataset needs to be stored to make a classiﬁcation. This can be addressed by a method
called data editing in which datapoints which have little or no eﬀect on the decision boundary are
removed from the training dataset.

273

K-Nearest Neighbours

Figure 14.1: In nearest neighbour classiﬁcation a new
vector is assigned the label of the nearest vector in
the training set. Here there are three classes, with
training points given by the circles, along with their
class. The dots indicate the class of the nearest train-
ing vector. The decision boundary is piecewise linear
with each segment corresponding to the perpendicu-
lar bisector between two datapoints belonging to dif-
ferent classes, giving rise to a Voronoi tessellation of
the input space.

Algorithm 13 Nearest neighbour algorithm to classify a new vector x, given a set of training data
D = {(xn, cn), n = 1, . . . , N}:
1: Calculate the dissimilarity of the test point x to each of the stored points, dn = d (x, xn), n = 1, . . . , N .
2: Find the training point xn∗

which is nearest to x :

n

∗ = argmin

n

d (x, xn)

3: Assign the class label c(x) = cn∗
4: In the case that there are two or more ‘equidistant’ neighbours with diﬀerent class labels, the most numerous
If there is no one single most numerous class, we use the K-nearest-neighbours case

.

class is chosen.
described in the next section.

• Each distance calculation can be expensive if the datapoints are high dimensional. Principal Com-
ponents Analysis, see chapter(15), is one way to address this and replaces xn with a low dimensional

projection p. The Euclidean distance of two datapoints(cid:0)xa − xb(cid:1)2 is then approximately given by
(cid:0)pa − pb(cid:1)2, see section(15.2.4). This is both faster to compute and can also improve classiﬁcation

accuracy since only the large scale characteristics of the data are retained in the PCA projections.

• It is not clear how to deal with missing data or incorporate prior beliefs and domain knowledge.
• For large databases, computing the nearest neighbour of a novel point x∗ can be very time-consuming
since x∗ needs to be compared to each of the training points. Depending on the geometry of the
training points, ﬁnding the nearest neighbour can accelerated by examining the values of each of the
components xi of x in turn. Such an axis-aligned space-split is called a KD-tree[202] and can reduce
the possible set of candidate nearest neighbours in the training set to the novel x∗, particularly in
low dimensions.

14.2 K-Nearest Neighbours

Basing the classiﬁcation on only the single nearest neighbour can lead to inaccuracies. If your neighbour
is simply mistaken (has an incorrect training class label), or is not a particularly representative example of
his class, then these situations will typically result in an incorrect classiﬁcation. By including more than
the single nearest neighbour, we hope to make a more robust classiﬁer with a smoother decision boundary
(less swayed by single neighbour opinions). For datapoints which are somewhat anomalous compared with

274

DRAFT March 9, 2010

A Probabilistic Interpretation of Nearest Neighbours

Figure 14.2: In K-nearest neighbours, we centre a hypersphere around
the point we wish to classify (here the central dot). The inner circle corre-
sponds to the nearest neighbour. However, using the 3 nearest neighbours,
we ﬁnd that there are two ‘blue’ classes and one ‘red’ – and we would there-
fore class the point as ‘blue’ class. In the case of a tie, one can extend K
until the tie is broken.

neighbours from the same class, their inﬂuence will be outvoted.

If we assume the Euclidean distance as the dissimilarity measure, the K-Nearest Neighbour algorithm
considers a hypersphere centred on the test point x. We increase the radius r until the hypersphere
contains exactly K points in the training data. The class label c(x) is then given by the most numerous
class within the hypersphere.

Choosing K

Whilst there is some sense in making K > 1, there is certainly little sense in making K = N (N being
the number of training points). For K very large, all classiﬁcations will become the same – simply assign
each novel x to the most numerous class in the training data. This suggests that there is an ‘optimal’
intermediate setting of K which gives the best generalisation performance. This can be determined using
cross-validation, as described in section(13.2.3).

Example 67 (Handwritten Digit Example). Consider two classes of handwritten digits, zeros and ones.
Each digit contains 28 × 28 = 784 pixels. The training data consists of 300 zeros, and 300 ones, a subset
of which are plotted in ﬁg(14.3a,b). To test the performance of the nearest neighbour method (based on
Euclidean distance) we use an independent test set containing a further 600 digits. The nearest neighbour
method, applied to this data, correctly predicts the class label of all 600 test points. The reason for
the high success rate is that examples of zeros and ones are suﬃciently diﬀerent that they can be easily
distinguished.

A more diﬃcult task is to distinguish between ones and sevens. We repeat the above experiment, now
using 300 training examples of ones, and 300 training examples of sevens, ﬁg(14.3b,c). Again, 600 new
test examples (containing 300 ones and 300 sevens) were used to assess the performance. This time, 18
errors are found using nearest neighbour classiﬁcation – a 3% error rate for this two class problem. The
18 test points on which the nearest neighbour method makes errors are plotted in ﬁg(14.4). If we use
K = 3 nearest neighbours, the classiﬁcation error reduces to 14 – a slight improvement. As an aside, the
best machine learning methods classify real world digits (over all 10 classes) to an error of less than 1%
(yann.lecun.com/exdb/mnist) – better than the performance of an ‘average’ human.

14.3 A Probabilistic Interpretation of Nearest Neighbours

Consider the situation where we have (for simplicity) data from two classes – class 0 and class 1. We make
the following mixture model for data from class 0:

−(x−xn)2/(2σ2)
e

(14.3.1)

(cid:88)

(cid:0)x xn, σ2I(cid:1) =

p(x|c = 0) =

1
N0

N

n∈ class 0

(cid:88)

1

1
N0

(2πσ2)D/2

n∈ class 0

where D is the dimension of a datapoint x and N0 are the number of training datapoints of class 0, and
σ2 is the variance. This is a Parzen estimator, which models the data distribution as a uniform weighted
sum of distributions centred on the training points, ﬁg(14.5).

DRAFT March 9, 2010

275

A Probabilistic Interpretation of Nearest Neighbours

(a)

(b)

(c)

Figure 14.3: Some of the training examples of the digit zero and (a), one (b) and seven (c). There are 300
training examples of each of these three digit classes.

Figure 14.4: ‘1’ versus ‘7’ classiﬁcation us-
ing the NN method. (Top) The 18 out of
600 test examples that are incorrectly clas-
siﬁed; (Bottom) the nearest neighbours in
the training set corresponding to each test-
point above.

(cid:88)

1

(2πσ2)D/2

n∈ class 1

−(x−xn)2/(2σ2)

e

(14.3.2)

Similarly, for data from class 1:

(cid:88)

(cid:0)x xn, σ2I(cid:1) =

1
N1

p(x|c = 1) =

1
N1
To classify a new datapoint x∗, we use Bayes’ rule
|c = 0)p(c = 0)

n∈ class 1

p(x∗

N

p(c = 0|x∗) =

p(x∗

|c = 0)p(c = 0) + p(x∗

|c = 1)p(c = 1)

The Maximum Likelihood setting of p(c = 0) is N0/(N0 + N1), and p(c = 1) = N1/(N0 + N1). An
analogous expression to equation (14.3.3) holds for p(c = 1|x∗). To see which class is most likely we may

use the ratio

(14.3.3)

(14.3.4)

p(c = 0|x∗)
p(c = 1|x∗)

= p(x∗
p(x∗

|c = 0)p(c = 0)
|c = 1)p(c = 1)

If this ratio is greater than one, we classify x∗ as 0, otherwise 1.
Equation(14.3.4) is a complicated function of x∗. However, if σ2 is very small, the numerator, which is
a sum of exponential terms, will be dominated by that term for which datapoint xn0 in class 0 is closest
to the point x∗. Similarly, the denominator will be dominated by that datapoint xn1 in class 1 which is
closest to x∗. In this case, therefore,

p(c = 0|x∗)
p(c = 1|x∗) ≈

e−(x∗−xn0 )2/(2σ2)p(c = 0)/N0
e−(x∗−xn1 )2/(2σ2)p(c = 1)/N1

= e−(x∗−xn0 )2/(2σ2)
e−(x∗−xn1 )2/(2σ2)

(14.3.5)

Taking the limit σ2 → 0, with certainty we classify x∗ as class 0 if x∗ has a point in the class 0 data which

is closer than the closest point in the class 1 data. The nearest (single) neighbour method is therefore
recovered as the limiting case of a probabilistic generative model, see ﬁg(14.5).

The motivation of using K nearest neighbours is to produce a result that is robust against unrepresentative
nearest neighbours. To ensure a similar kind of robustness in the probabilistic interpretation, we may use
a ﬁnite value σ2 > 0. This smoothes the extreme probabilities of classiﬁcation and means that more points
(not just the nearest) will have an eﬀective contribution in equation (14.3.4). The extension to more than

276

DRAFT March 9, 2010

Exercises

Figure 14.5: A probabilistic interpretation of nearest neighbours. For each
class we use a mixture of Gaussians to model the data from that class
p(x|c), placing at each training point an isotropic Gaussian of width σ2.
In the limit
The width of each Gaussian is represented by the circle.
σ2 → 0 a novel point (black) is assigned the class of its nearest neighbour.
For ﬁnite σ2 > 0 the inﬂuence of non-nearest neighbours has an eﬀect,
resulting in a soft version of nearest neighbours.

two classes is straightforward, requiring a class conditional generative model for each class.

To go beyond nearest neighbour methods, we can relax the assumption of using a Parzen estimator, and
use a richer generative model. We will examine such cases in some detail in later chapters, in particular
chapter(20).

14.3.1 When your nearest neighbour is far away
For a novel input x∗ that is far from all training points, Nearest Neighbours, and its soft probabilistic
variant will conﬁdently classify x∗ as belonging to the class of the nearest training point. This is arguably
opposite to what we would like, namely that the classiﬁcation should tend to the prior probabilities of the
class based on the number of training data per class. A way to avoid this problem is, for each class, to
include a ﬁctitious mixture component at the mean of all the data with large variance, equal for each class.
For novel inputs close to the training data, this extra ﬁctitious datapoint will have no appreciable eﬀect.
However, as we move away from the high density regions of the training data, this additional ﬁctitious
component will dominate. Since the distance from x∗ to each ﬁctitious class point is the same, in the limit
that x∗ is far from the training data, the eﬀect is that no class information from the position of x∗ occurs.
See section(20.3.3) for an example.

14.4 Code

nearNeigh.m: K Nearest Neighbour

14.4.1 Utility Routines

majority.m: Find the majority entry in each column of a matrix

14.4.2 Demonstration

demoNearNeigh.m: K Nearest Neighbour Demo

14.5 Exercises

Exercise 157. The ﬁle NNdata.mat contains training and test data for the handwritten digits 5 and 9.
Using leave one out cross-validation, ﬁnd the optimal K in K-nearest neighours, and use this to compute
the classiﬁcation accuracy of the method on the test data.

Exercise 158. Write a routine SoftNearNeigh(xtrain,xtest,trainlabels,sigma) to implement soft
nearest neighbours, analogous to nearNeigh.m. Here sigma is the variance σ2 in equation (14.3.1). As
above, the ﬁle NNdata.mat contains training and test data for the handwritten digits 5 and 9. Using
leave one out cross-validation, ﬁnd the optimal σ2 and use this to compute the classiﬁcation accuracy of
the method on the test data. Hint: you may have numerical diﬃculty with this method. To avoid this,

consider using the logarithm, and how to numerically compute log(cid:0)ea + eb(cid:1) for large (negative) a and b.

See also logsumexp.m.

DRAFT March 9, 2010

277

Exercise 159. In the text we suggested the use of the Mahalanobis distance

d(x, y) = (x − y)T Σ−1 (x − y)

Exercises

as a way to improve on the Euclidean distance, with Σ the covariance matrix of the combined data from
both classes. Consider a modiﬁcation based on using a mixture model

(cid:88)
(cid:88)

n∈class 0

n∈class 1

and

p(x|c = 0) =

p(x|c = 1) =

1
N0

1
N1

N (x xn, Σ0)

N (x xn, Σ1)

1. Explain how the soft Nearest Neighbours algorithm can deal with the issue that the distribution of

data from the diﬀerent classes can be very diﬀerent.

2. For the case Σ0 = γ2Σ(cid:48)

(cid:18) p(c = 0|x∗)

p(c = 1|x∗)

log

(cid:19)
0 and Σ1 = γ2Σ(cid:48)

1 and γ2 small, derive a simple expression that approximates

Exercise 160. The editor at YoMan! (a ‘mens’ magazine) has just had a great idea. Based on the success
of a recent national poll to test IQ, she decides to make a ‘Beauty Quotient’ (BQ) test. She collects as
many images of male faces as she can, taking care to make sure that all the images are scaled to roughly
the same size and under the same lighting conditions. She then gives each male face a BQ score from 0
(‘Severely Aesthetically Challenged’) to 100 (‘Generously Aesthetically Gifted’). Thus, for each image x,
there is an associated value b in the range 0 to 100. In total she collects N images and associated scores,
{(xn, bn) , n = 1, . . . , N}, where each image is represented by a D-dimensional real-valued vector x. One
morning, she bounces into your oﬃce and tells you the good news : it is your task to make a test for the
male nation to determine their Beauty Quotient. The idea, she explains, is that a man can send online an
image of their face x∗, to YoMan! and will ‘instantly’ receive an automatic BQ response b∗.

1. As a ﬁrst step, you decide to use the K nearest neighbour method (KNN) to assign a BQ score b∗ to

a novel test image x∗.
Describe how to determine the optimal number of neighbours K to use.

2. Your line manager is pleased with your algorithm but is disappointed that it does not provide any

simple explanation of Beauty that she can present in a future version of YoMan! magazine.
To address this, you decide to make a model based on linear regression. That is

b = wTx

(14.5.1)

where w is a parameter (weight) vector chosen to minimise

E(w) =(cid:88)

n

(cid:16)

bn − wTxn(cid:17)2

(a) After training (ﬁnding a suitable w), how can YoMan! explain to its readership in a simple way

what facial features are important for determining one’s BQ?

(b) Describe fully and mathematically a method to train this linear regression model. Your expla-

nation must be detailed enough so that a programmer can directly implement it.

(c) Discuss any implications of the situation D > N.
(d) Discuss any advantages/disadvantages of using the linear regression model compared with using

the KNN approach.

278

DRAFT March 9, 2010

CHAPTER 15

Unsupervised Linear Dimension Reduction

15.1 High-Dimensional Spaces – Low Dimensional Manifolds

In machine learning problems data is often high dimensional – images, bag-of-word descriptions, gene-
expresssions etc. In such cases we cannot expect the training data to densely populate the space, meaning
that there will be large parts in which little is known about the data. For the hand-written digits from
chapter(14), the data is 784 dimensional. For binary valued pixels the possible number of images that could
ever exist is 2784 ≈ 10236. Nevertheless, we would expect that only a handful of examples of a digit should
be suﬃcient (for a human) to understand how to recognise a 7. Digit-like images must therefore occupy
a highly constrained subspace of the 784 dimensions and we expect only a small number of directions to
be relevant for describing the data to a reasonable accuracy. Whilst the data vectors may be very high
dimensional, they will therefore typically lie close to a much lower dimensional ‘manifold’ (informally, a
two-dimensional manifold corresponds to a warped sheet of paper embedded in a high dimensional space),
meaning that the distribution of the data is heavily constrained.

Here we concentrate on linear dimension reduction techniques for which there exist computationally eﬃ-
cient approaches. In this approach a high dimensional datapoint x is ‘projected down’ to a lower dimen-
sional vector y by

y = Fx + const.

(15.1.1)
where the non-square matrix F has dimensions dim (y) × dim (x), with dim (y) < dim (x). The methods
in this chapter are largely non-probabilistic, although many have natural probabilistic interpretations. For
example, PCA is closely related to Factor Analysis, described in chapter(21).

15.2 Principal Components Analysis

If data lies close to a hyperplane, as in ﬁg(15.1) we can accurately approximate each data point by using
vectors that span the hyperplane alone. Eﬀectively, we are trying to discover a low dimensional co-ordinate
system in which we can approximately represent the data. We express the approximation for datapoint
xn as

M(cid:88)

i=1

xn ≈ c +

i bi ≡ ˜xn
yn

(15.2.1)

Here the vector c is a constant and deﬁnes a point in the hyperplane and the bi deﬁne vectors in the hy-
perplane (also known as ‘principal component coeﬃcients’ or ‘loadings’). The yn
i are the low dimensional
co-ordinates of the data. Equation(15.2.1) expresses how to ﬁnd the reconstruction ˜xn given the lower
dimensional representation yn (which has components yn
i , i = 1, . . . , M). For a data space of dimension

279

Principal Components Analysis

Figure 15.1: In linear dimension reduction a hyper-
plane is ﬁtted such that the average distance between
datapoints (red rings) and their projections onto the
plane (black dots) is minimal.

N(cid:88)

D(cid:88)

dim (x) = D, we hope to accurately describe the data using only a small number M (cid:28) D of co-ordinates y.
To determine the best lower dimensional representation it is convenient to use the square distance error
between x and its reconstruction ˜x:

E(B, Y, c) =

i ]2
i − ˜xn
[xn

It is straightforward to show that the optimal bias c is given by the mean of the data (cid:80)
therefore assume that the data has been centred (has zero mean(cid:80)

n xn/N. We
n xn = 0), so that we can set c to zero,

n=1

i=1

(15.2.2)

and concentrate on ﬁnding the optimal basis B below.

M(cid:88)

2

E(B, Y) =

15.2.1 Deriving the optimal linear reconstruction

To ﬁnd the best basis vectors B =(cid:2)b1, . . . , bM(cid:3) (deﬁning [B]i,j = bj
coordinates Y = (cid:2)y1, . . . , yN(cid:3), we may minimize the sum of squared diﬀerences between each vector x

i ) and corresponding low dimensional

and its reconstruction ˜x:

xn
D(cid:88)
N(cid:88)
where X =(cid:2)x1, . . . , xN(cid:3).
Consider a transformation Q of the basis B so that ˜B ≡ BQ is an orthonormal matrix, ˜BT ˜B = I. Since
Q is invertible, we may write BY = ˜BQ−1Y ≡ ˜B ˜Y, which is of then same form as BY, albeit with
an orthonormality constraint on ˜B. Hence, without loss of generality, we may consider equation (15.2.3)
under the orthonormality constraint BTB = I, namely that the basis vectors are mutually orthogonal and
of unit length.

(X − BY)T (X − BY)

(15.2.3)

= trace

j bj
yn

i

i −

(cid:16)

(cid:17)

n=1

i=1

j=1

By diﬀerentiating equation (15.2.3) with respect to yn

k we obtain (using the orthonormality constraint)

E(B, Y) =(cid:88)

xn

i −

(cid:88)

 bk
i =(cid:88)

j bj
yn

i

i

j

i

1
2

−

∂
∂yn
k

(cid:88)

j

=(cid:88)

i

(cid:88)
(cid:124) (cid:123)(cid:122) (cid:125)

bj
i bk
i

i

δjk

xn
i bk

i −

yn
j

The squared error E(B, Y) therefore has zero derivative when

k =(cid:88)

yn

bk
i xn
i

i

280

i bk
xn
i − yn

k

(15.2.4)

DRAFT March 9, 2010

−2024−2024−10123We now substitute this solution into equation (15.2.3) to write the squared error only as a function of B.

Bi,jBk,jxn

k = [BBTxn]i

Principal Components Analysis

Using(cid:88)

j

j bj
yn

i =(cid:88)
(cid:16)(cid:16)
E(B) =(cid:88)

j,k

The objective E(B) becomes

j,k

kxn

bj
i bj

k =(cid:88)
xn(cid:17)2
I − BBT(cid:17)
I − BBT(cid:17)
(xn)T(cid:16)
(cid:104)

n

E(B) =(cid:88)

Since(cid:0)I − BBT(cid:1)2 = I − BBT, (using BTB = I)
(cid:32)(cid:88)
SBBT(cid:17)(cid:105)

Hence the objective becomes

xn = trace

(cid:16)

n

n

E(B) = (N − 1)

where S is the sample covariance matrix of the data1

trace (S) − trace
N(cid:88)

1

N(cid:88)

n=1

m =

1
N

xn, S =

(xn − m)(xn − m)T

n=1

N − 1
(cid:16)

L

(cid:16)

SBBT(cid:17)

−trace

+ trace

(cid:16)

(cid:17)(cid:17)
BTB − I

(xn) (xn)T(cid:16)

I − BBT(cid:17)(cid:33)

To minimise equation (15.2.8) under the constraint BTB = I we use a set of Lagrange multipliers L, so
that the objective is to minimize

(15.2.5)

(15.2.6)

(15.2.7)

(15.2.8)

(15.2.9)

(15.2.10)

(15.2.11)

(neglecting the constant prefactor N − 1). Since the constraint is symmetric, we can assume that L is also
symmetric. Diﬀerentiating with respect to B and equating to zero we obtain that at the optimum

SB = BL

whose columns are the corresponding eigenvectors of S. In this case, trace(cid:0)SBBT(cid:1) = trace (L), which is

This is a form of eigen-equation so that a solution is given by taking L to be diagonal and B as the matrix

the sum of the eigenvalues corresponding to the eigenvectors forming B. Since we wish to minimise E(B),
we take the eigenvectors with largest corresponding eigenvalues.

If we order the eigenvalues λ1 ≥ λ2, . . ., the squared error is given by, from equation (15.2.8)

1

N − 1 E(B) = trace (S) − trace (L) =

M(cid:88)

D(cid:88)

i=1

λi −

D(cid:88)

i=1

i=M +1

λi =

λi

(15.2.12)

Whilst the solution to this eigen-problem is unique, this only serves to deﬁne the solution subspace since
one may rotate and scale B and Y such that the value of the squared loss is exactly the same. The
justiﬁcation for choosing the non-rotated eigen solution is given by the additional requirement that the
principal components corresponds to directions of maximal variance, as explained in section(15.2.2).

1Here we use the unbiased sample covariance, simply because this is standard in the literature. If we were to replace this
with the sample covariance as deﬁned in chapter(8), the only change required is to replace N − 1 by N throughout, which
has no eﬀect on the form of the solutions found by PCA.

DRAFT March 9, 2010

281

yn =(cid:88)

i

(cid:88)

(cid:16)

bTxn(cid:17)2

(cid:34)(cid:88)

(cid:35)

Principal Components Analysis

Figure 15.2: Projection of two dimensional data using
one dimensional PCA. Plotted are the original dat-
apoints x (larger rings) and their reconstructions ˜x
(small dots) using 1 dimensional PCA. The lines rep-
resent the orthogonal projection of the original dat-
apoint onto the ﬁrst eigenvector. The arrows are the
two eigenvectors scaled by the square root of their
corresponding eigenvalues. The data has been cen-
tred to have zero mean. For each ‘high dimensional’
datapoint x, the ‘low dimensional’ representation y is
given in this case by the distance (possibly negative)
from the origin along the ﬁrst eigenvector direction
to the corresponding orthogonal projection point.

15.2.2 Maximum variance criterion

We search ﬁrst for the single direction b such that, when the data is projected onto this direction, the
variance of this projection is maximal amongst all possible such projections. Using equation (15.2.4) for
a single vector b we have

bixn
i

(15.2.13)

The projection of a datapoint onto a direction b is bTxn for a unit length vector b. Hence the sum of
squared projections is

= bT

xn (xn)T

b = (N − 1)bTSb = λ(N − 1)

(15.2.14)

n

n

which, ignoring constants, is simply the negative of equation (15.2.8) for a single retained eigenvector
b (with Sb = λb). Hence the optimal single b which maximises the projection variance is given by
the eigenvector corresponding to the largest eigenvalue of S. Under the criterion that the next optimal
direction should be orthonormal to the ﬁrst, one can readily show that b(2) is given by the second largest
eigenvector, and so on. This explains why, despite the squared loss equation (15.2.8) being invariant with
respect to arbitrary rotation (and scaling) of the basis vectors, the ones given by the eigen-decomposition
have the additional property that they correspond to directions of maximal variance. These maximal
variance directions found by PCA are called the principal directions.

15.2.3 PCA algorithm

The routine for PCA is presented in algorithm(14). In the notation of y = Fx, the projection matrix F
corresponds to ET. Similarly for the reconstruction equation (15.2.1), the coordinate yn corresponds to
ETxn and bi corresponds to ei. The PCA reconstructions are orthogonal projections of the data onto
the subspace spanned by the eigenvectors corresponding to the M largest eigenvalues of the covariance
matrix, see ﬁg(15.2).

Figure 15.3: Top row : a selection of the
digit 5 taken from the database of 892 ex-
amples. Plotted beneath each digit is the
reconstruction using 100, 30 and 5 eigen-
vectors (from top to bottom). Note how
the reconstructions for fewer eigenvectors
express less variability from each other,
and resemble more a mean 5 digit.

DRAFT March 9, 2010

282

−2−10123−2−1.5−1−0.500.511.52Principal Components Analysis

Algorithm 14 Principal Components Analysis to form an M-dimensional approximation of a dataset
{xn, n = 1, . . . , N}, with dim xn = D.
1: Find the D × 1 sample mean vector and D × D covariance matrix

N(cid:88)

n=1

N(cid:88)

n=1

m =

1
N

xn, S =

1

N − 1

(xn − m)(xn − m)T

2: Find the eigenvectors e1, . . . , eM of the covariance matrix S, sorted so that the eigenvalue of ei is larger

than ej for i < j. Form the matrix E = [e1, . . . , eM ].

3: The lower dimensional representation of each data point xn is given by

yn = ET(xn − m)

4: The approximate reconstruction of the original datapoint xn is

5: The total squared error over all the training data made by the approximation is

xn ≈ m + Eyn
N(cid:88)

(xn − ˜xn)2 = (N − 1)

n=1

D(cid:88)

λj

j=M +1

(15.2.15)

(15.2.16)

(15.2.17)

where λM +1 . . . λN are the eigenvalues discarded in the projection.

Example 68 (Reducing the dimension of digits). We have 892 examples of handwritten 5’s, where each
image consists of 28 × 28 real-values pixels, see ﬁg(15.3). Each image matrix is stacked to form a 784
dimensional vector, giving a 784 × 892 dimensional data matrix X. The covariance matrix of this data
has eigenvalue spectrum as plotted in ﬁg(15.4), where we plot only the 100 largest eigenvalues. Note how
after around 40 components, the mean squared reconstruction error is small, indicating that the data
indeed lie close to a 40 dimensional hyperplane. The eigenvalues are computed using pca.m.

The reconstructions using diﬀerent numbers of eigenvectors (100, 30 and 5) are plotted in ﬁg(15.3). Note
how using only a small number of eigenvectors, the reconstruction more closely resembles the mean image.

Example 69 (Eigenfaces). In ﬁg(15.5) we present example images for which we wish to ﬁnd a lower
dimensional representation. Using PCA the ﬁrst 49 ‘eigenfaces’ are presented along with reconstructions
of the original data using these eigenfaces, see ﬁg(15.6).

Figure 15.4: For the digits data consisting of 892 examples of
the digit 5, each image being represented by a 784 dimensional
vector. Plotted as the largest 100 eigenvalues (scaled so that
the largest eigenvalue is 1) of the sample covariance matrix.

DRAFT March 9, 2010

283

02040608010000.20.40.60.81eigenvalue numbereigenvaluePrincipal Components Analysis

Figure 15.5: 100 of the 120 training images (40 people, with 3
images of each person). Each image consists of 92×112 = 10304
greyscale pixels. The training data is scaled so that, represented
as an image, the components of each image sum to 1. The

average value of each pixel across all images is 9.70×10−5. This

is a subset of the 400 images in the full Olivetti Research Face
Database.

Figure 15.6: (a): SVD reconstruc-
tion of the images in ﬁg(15.5) using a
combination of the 49 eigen-images.
(b): The eigen-images are found us-
ing SVD of the ﬁg(15.5) and tak-
ing the 49 eigenvectors with largest
eigenvalue. The images correspond-
ing to the largest eigenvalues are con-
tained in the ﬁrst row, and the next
7 in the row below, etc. The root
mean square reconstruction error is
1.121 × 10−5, a small improvement

over PLSA (see ﬁg(15.14)).

(a)

(b)

15.2.4 PCA and nearest neighbours

For high-dimensional data computing the squared Euclidean distance between vectors can be expensive,
and also sensitive to noise. It is therefore often useful to project the data to form a lower dimensional
representation ﬁrst. For example, in making a classiﬁer to distinguish between the digit 1 and the digit 7,
example(67), we can form a lower dimensional representation ﬁrst by ignoring the class label (to make a
dataset of 1200 training points). Each of the training points xn is then projected to a lower dimensional
PCA representation yn. Subsequently, any distance calculations (xa − xb)2 are replaced by (ya − yb)2. To
justify this, consider

(xa − xb)T(xa − xb) ≈ (Eya + m − Eyb − m)T(Eya + m − Eyb − m)

= (ya − yb)TETE(ya − yb)
= (ya − yb)T(ya − yb)

where the last equality is due to the orthonormality of eigenvectors : ETE = I.

(15.2.18)

Using 19 principal components (see example(70) why this number was chosen) and the nearest neighbour
rule to classify ones and sevens gave a test-set error of 14 in 600 examples, compared to 18 from the standard
method on the non-projected data. How can it be that the classiﬁcation performance has improved? A
plausible explanation is that the new PCA representation of the data is more robust since only the large
scale change directions in the space are retained, with low variance directions discarded.

284

DRAFT March 9, 2010

High Dimensional Data

Figure 15.7: Finding the optimal PCA dimension to use for
classifying hand-written digits using nearest neighbours. 400
training examples are used, and the validation error plotted on
200 further examples. Based on the validation error, we see that
a dimension of 19 is reasonable.

Example 70 (Finding the best PCA dimension). There are 600 examples of the digit 1 and 600 examples
of the digit 7. We will use half the data for training and the other half for testing. The 600 training
examples were further split into a training set of 400 examples, and a separate validation set of 200
examples. PCA was used to reduce the dimensionality of the inputs, and then nearest neighbours used to
classify the 200 validation examples. Diﬀerent reduced dimensions were in vestigated and, based on the
validation results, 19 was selected as the optimal number of PCA components retained, see ﬁg(15.7). The
independent test error on 600 independent examples using 19 dimensions is 14.

15.2.5 Comments on PCA

The ‘intrinsic’ dimension of data

How many dimensions should the linear subspace have? From equation (15.2.12), the reconstruction error
is proportional to the sum of the discarded eigenvalues. If we plot the eigenvalue spectrum (the set of
eigenvalues ordered by decreasing value), we might hope to see a few large values and many small values.
If the data does lie close to an M dimensional hyperplane, we would see M large eigenvalues with the rest
being very small. This would give an indication of the number of degrees of freedom in the data, or the
intrinsic dimensionality. Directions corresponding to the small eigenvalues are then interpreted as ‘noise’.

Non-linear dimension reduction

In PCA we are presupposing that the data lies close to a hyperplane. Is this really a good description?
More generally, we would expect data to lie on low dimensional curved manifolds. Also, data is often
clustered – examples of handwritten ‘4’s look similar to each other and form a cluster, separate from the
‘8’s cluster. Nevertheless, since linear dimension reduction is computationally relatively straightforward,
this is one of the most common dimensionality reduction techniques.

15.3 High Dimensional Data

The computational complexity of computing an eigen-decomposition of a D × D matrix is O(cid:0)D3(cid:1). You
exceed 500. One can exploit this fact to bound the complexity by O(cid:0)min(D, N)3(cid:1), as described below.

might be wondering therefore how it is possible to perform PCA on high dimensional data. For example, if
we have 500 images each of 1000× 1000 = 106 pixels, the covariance matrix will be 106 × 106 dimensional.
It would appear a signiﬁcant computational challenge to compute the eigen-decomposition of this matrix.
In this case, however, since there are only 500 such vectors, the number of non-zero eigenvalues cannot

DRAFT March 9, 2010

285

020406080100024681012number of eigenvaluesnumber of errors15.3.1 Eigen-decomposition for N < D

High Dimensional Data

First note that for zero mean data, the sample covariance matrix can be expressed as

N(cid:88)

1

[S]ij =

xn
i xn
j

N − 1

n=1

(15.3.1)

(15.3.2)

(15.3.3)

In matrix notation this can be written

S =

1

N − 1

XXT

where the D × N matrix X contains all the data vectors:

X =(cid:2)x1, . . . , xN(cid:3)

Since the eigenvectors of a matrix M are equal to those of γM for scalar γ, one can consider more simply
the eigenvectors of XXT. Writing the D× N matrix of eigenvectors as E (this is a non-square thin matrix
since there will be fewer eigenvalues than data dimensions) and the eigenvalues as an N × N diagonal
matrix Λ, the eigen-decomposition of the covariance S is

XXTE = EΛ ⇒ XTXXTE = XTEΛ ⇒ XTX ˜E = ˜EΛ

where we deﬁned ˜E = XTE. The ﬁnal expression above represents the eigenvector equation for XTX.

This is a matrix of dimensions N ×N so that calculating the eigen-decomposition takes O(cid:0)N 3(cid:1) operations,
compared with O(cid:0)D3(cid:1) operations in the original high-dimensional space . We then can therefore calculate

the eigenvectors ˜E and eigenvalues Λ of this matrix more easily. Once found, we use the fact that the
eigenvalues of S are given by the diagonal entries of Λ and the eigenvectors by

(15.3.4)

E = X ˜EΛ−1

(15.3.5)

15.3.2 PCA via Singular value decomposition

An alternative to using an eigen-decomposition routine to ﬁnd the PCA solution is to make use of the
Singular Value Decomposition (SVD) of an D × N dimensional matrix X. This is given by

X = UDVT

(15.3.6)

where UTU = ID and VTV = IN and D is a diagonal matrix of the (positive) singular values. We
assume that the decomposition has ordered the singular values so that the upper left diagonal element of
D contains the largest singular value. The matrix XXT can then be written as

XXT = UDVTVDUT = UD2UT

(15.3.7)

Since UD2UT is in the form of an eigen-decomposition, the PCA solution is equivalently given by per-
forming the SVD decomposition of X, for which the eigenvectors are then given by U, and corresponding
eigenvalues by the square of the singular values.

Equation(15.3.6) shows that PCA is a form of matrix decomposition method:

X = UDVT ≈ UM DM VT

M

(15.3.8)

where UM , DM , VM correspond to taking only the ﬁrst M singular values of the full matrices.

286

DRAFT March 9, 2010

Latent Semantic Analysis

Figure 15.8: Document data for a dictionary contain-
ing 10 words and 1200 documents. Black indicates
that a word was present in a document. The data
consists of two ‘similar’ topics (diﬀering only in their
usage of the ﬁrst two words) and a random back-
ground topic.

15.4 Latent Semantic Analysis

In the document analysis literature PCA is also called Latent Semantic Analysis and is concerned with
analysing a set of N documents, where each document dn, n = 1, . . . , N is represented by a vector

xn = (xn

1 , . . . , xn
D)

(15.4.1)

1 might count how many times the word ‘cat’ appears
of word occurrences. For example the ﬁrst element xn
2 the number of occurrences of ‘dog’, etc. This bag of words 2 is formed by ﬁrst choosing
in document n, xn
is the (possibly normalised) number of occurrences of the
a dictionary of D words. The vector element xn
i
word i in the document n. Typically D will be large, of the order 106, and x will be very sparse since any
document contains only a small fraction of the available words in the dictionary. Given a set of documents
D, the aim in LSA is to form a lower dimensional representation of each document. The whole document
database is represented by the so-called term-document matrix

X =(cid:2)x1, . . . , xN(cid:3)

(15.4.2)

which has dimension D × N, see for example ﬁg(15.8). In this small example the term-document matrix
is ‘short and fat’, whereas in practice most often the matrix will be ‘tall and thin’.

Example 71 (Latent Topic). We have a small dictionary containing the words inﬂuenza, ﬂu, headache,
nose, temperature, bed, cat, tree, car, foot. The database contains a large number of articles that discuss
ailments, and articles which seem to talk about the eﬀects of inﬂuenza, in addition to some background
documents that are not speciﬁc to ailments. Some of the more formal documents exclusively use the
term inﬂuenza, whereas the other more ‘tabloid’ documents use the informal term ﬂu. Each document is
represented by a simple bag-of-words style description, namely a 10-dimensional vector in which element
i of that vector is set to 1 if word i occurs in the document, and 0 otherwise. The data is represented in
ﬁg(15.8). The data is generated using the artiﬁcial mechanism described in demoLSI.m.

The result of using PCA on this data is represented in ﬁg(15.9) where we plot the eigenvectors, scaled by
their eigenvalue. The ﬁrst eigenvector groups all the ‘inﬂuenza’ words together, and the second deals with
the diﬀerent usage of the terms inﬂuenza and ﬂu.

Rescaling

In LSA it is common to scale the transformation so that the projected vectors have approximately unit
covariance (assuming centred data). Using

the covariance of the projections is obtained from

y = √N − 1D−1

M UT

M x

(cid:88)

1

N − 1

n

(cid:88)
(cid:124)

n

yn (yn)T = D−1

M UT

M

UM D−1

M = D−1

M UT

M UD2UTUM D−1

M ≈ I

xn (xn)T

(cid:123)(cid:122)

XXT

(cid:125)

(15.4.3)

2More generally one can consider term-counts, in which terms can can be single words, or sets of words, or even sub-words.

DRAFT March 9, 2010

287

20040060080010001200246810Latent Semantic Analysis

Figure 15.9: Hinton Diagram of the eigenvector matrix E where each eigen-
vector column is scaled by the corresponding eigenvalue. Red indicates
positive and green negative (the area of each square corresponds to the
magnitude), showing that there are only a few large eigenvalues. Note
that the overall sign of any eigenvector is irrelevant. The ﬁrst eigenvector
corresponds to a topic in which the words inﬂuenza, ﬂu, headache, nose,
temperature, bed are prevalent. The second eigenvector denotes that there
is negative correlation between the occurrence of inﬂuenza and ﬂu.

Given y, the approximate reconstruction ˜x is

The Euclidean distance between two points xa and xb is then approximately

(15.4.4)

(cid:16)

ya − yb(cid:17)

D2
M

(cid:16)

ya − yb(cid:17)T

(cid:16)

ya − yb(cid:17)

≈

1

N − 1

˜x =

UM DM y

1

√N − 1
˜xa, ˜xb(cid:17)

=

(cid:16)

d

(cid:16)

ya − yb(cid:17)T

1

N − 1

DM UT

M UM DM

It is common to ignore the D2
the projected space just to be the Euclidean distance between the y vectors.

M term (and 1/ (N − 1) factor), and consider a measure of dissimilarity in

15.4.1 LSA for information retrieval
Consider a large collection of documents from the web, creating a database D. Our interest it to ﬁnd
the most similar document to a speciﬁed query document. Using a bag-of-words style representation for
document n, xn, and similarly for the query document, x∗ we address this task by ﬁrst deﬁning a measure
of dissimilarity between documents, for example

d(xn, xm) = (xn − xm)T (xn − xm)

One then searches for the document that minimises this dissimilarity:

nopt = argmin

n

d(xn, x∗)

(15.4.5)

(15.4.6)

and returns document xnopt as the result of the search query. A diﬃculty with this approach is that the
bag-of-words representation will have mostly zeros (i.e. be very sparse). Hence diﬀerences may be due
to ‘noise’ rather than any real similarity between the query and database document. LSA alleviates this
problem by using a lower dimensional representation y of the high-dimensional x. The y capture the main
variations in the data and are less sensitive to random uncorrelated noise. Using the dissimilarity deﬁned
in terms of the lower dimensional y is therefore more robust and likely to retrieve more useful documents.

The squared diﬀerence between two documents can also be written

(cid:0)x − x(cid:48)(cid:1)T(cid:0)x − x(cid:48)(cid:1) = xTx + x(cid:48)Tx(cid:48)

− 2xTx(cid:48)

If, as is commonly done, the bag-of-words representations are scaled to have unit length,

ˆx =

x

√xTx

so that ˆxTˆx = 1, the distance is

(cid:0)ˆx − ˆx(cid:48)(cid:1)T(cid:0)ˆx − ˆx(cid:48)(cid:1) = 2

(cid:16)

1 − ˆxTˆx(cid:48)(cid:17)

(15.4.7)

(15.4.8)

(15.4.9)

288

DRAFT March 9, 2010

1234567891010987654321PCA With Missing Data

Figure 15.10: (a): Two bag-of-word vectors. The Euclidean distance be-
tween the two is large. (b): Normalised vectors. The Euclidean distance
is now related directly to the angle between the vectors. In this case two
documents which have the same relative frequency of words will both have
the same dissimilarly, even though the number of occurrences of the words
is diﬀerent.

(a)

(b)

Figure 15.11: Top: original data matrix X. Black is
missing, white present. The data is constructed from
a set of only 5 basis vectors. Middle : X with missing
data (80% sparsity). Bottom : reconstruction found
using svdm.m, SVD for missing data. This problem is
essentially easy since, despite there being many miss-
ing elements, the data is indeed constructed from a
model for which SVD is appropriate. Such techniques
have application in collaborative ﬁltering and recom-
mender systems where one wishes to ‘ﬁll in’ missing
values in a matrix.

and one may equivalently consider the cosine similarity

s(ˆx, ˆx(cid:48)) = ˆxTˆx(cid:48) = cos (θ)

where θ is the angle between the unit vectors ˆx and ˆx(cid:48), ﬁg(15.10).

(15.4.10)

PCA is arguably suboptimal for document analysis since we would expect the presence of a latent topic
to contribute only positive counts to the data. A related version of PCA in which the decomposition is
constrained to have positive elements is called PLSA, and discussed in section(15.6).

Example 72. Continuing the Inﬂuenza example, someone who uploads a query document which uses
the term ‘ﬂu’ might also be interested in documents about ‘inﬂuenza’. However, the search query term
‘ﬂu’ does not contain the word ‘inﬂuenza’, so how can one retrieve such documents? Since the ﬁrst
component using PCA (LSA) groups all ‘inﬂuenza’ terms together, if we use only the ﬁrst component of
the representation y to compare documents, this will retrieve documents independent of whether the term
‘ﬂu’ or ‘inﬂuenza’ is used.

15.5 PCA With Missing Data

When values of the data matrix X are missing, the standard PCA algorithm as described cannot be
implemented. Unfortunately, there is no ‘quick ﬁx’ PCA solution when some of the xn
i are missing and
more complex numerical procedures need to invoked. A naive approach in this case is to require the

DRAFT March 9, 2010

289

501001502002503002040608010050100150200250300204060801005010015020025030020406080100squared reconstruction error to be small only for the existing elements of X. That is

N(cid:88)

D(cid:88)

n=1

i=1

xn

(cid:88)

j

E(B, Y) =

γn
i

i −

j bj
yn

i

2

(15.5.1)

PCA With Missing Data

where γn
we ﬁnd that the optimal weights satisfy (assuming BTB = I),

i = 1 if the ith entry of the nth vector is available, and is zero otherwise. Diﬀerentiating, as before,

γn
i xn

i bk
i

(15.5.2)

n =(cid:88)

yk

i

One then substitutes this expression into the squared error, and minimises the error with respect to B
under the orthonormality constraint. An alternative iterative optimisation procedure is as follows: First
select a random D × M matrix ˆB. Then iterate until convergence the following two steps:
Optimize Y for ﬁxed B

For ﬁxed ˆB the above E( ˆB, Y) is a quadratic function of the matrix Y, which can be optimised
directly. By diﬀerentiating and equating to zero, one obtains the ﬁxed point condition

2

(cid:88)

j

i −

yn
j

ˆbj

i

N(cid:88)

D(cid:88)

n=1

i=1

(cid:88)

l

γn
i

xn
(cid:33)
M(n)(cid:105)
(cid:104)

ˆbl

i

xn
i −

yn
l

ˆbk
i = 0

E( ˆB, Y) =

(cid:32)
(cid:88)
w(n)(cid:105)
Deﬁning(cid:104)

γn
i

i

= yl
n,

l

=(cid:88)

i

kl

ˆbl

i

ˆbk
i γn
i ,

[cn]k =(cid:88)

i

γn
i xn
i

ˆbk
i ,

(15.5.3)

(15.5.4)

(15.5.5)

in matrix notation, we then have a set of linear systems:

n = 1, . . . , N

c(n) = M(n)y(n),

(15.5.6)
One may solve each linear system using Gaussian elimination (one can avoid explicit matrix inversion
by using the \ operator in MATLAB). It can be that one or more of the above linear systems is
underdetermined– this can occur when there are less observed values in the nth data column of X
than there are components M. In this case one may use the pseudo-inverse to provide a minimal
length solution.

Optimize B for ﬁxed Y

One now freezes ˆY and considers the function

N(cid:88)

D(cid:88)

n=1

i=1

xn

(cid:88)

j

γn
i

i −

j bj
ˆyn

i

2

E(B, ˆY) =

(15.5.7)

For ﬁxed ˆY the above expression is quadratic in the matrix B, which can again be optimised using
linear algebra. This corresponds to solving a set of linear systems for the ith row of B:

m(i) = F(i)b(i)

m(i)(cid:105)

=(cid:88)

where(cid:104)
Mathematically, this is b(i) = F(i)−1m(i).

(cid:104)
F(i)(cid:105)

γn
i xn

i ˆyn
k ,

n

k

kj

=(cid:88)

n

γn
i ˆyn

j ˆyn
k

(15.5.8)

(15.5.9)

In this manner one is guaranteed to iteratively decrease the value of the squared error loss until a minimum
is reached. This technique is implemented in svdm.m. Note that eﬃcient techniques based on updating
the solution as a new column of X arrives one at a time (‘online’ updating) are available, see for example
[49].

290

DRAFT March 9, 2010

PCA With Missing Data

15.5.1 Finding the principal directions

For the missing data case the basis B found using the above technique is based only on minimising the
squared reconstruction error and therefore does not necessarily satisfy the maximal variance (or principal
directions) criterion, namely that the columns of B point along the eigen-directions. For a given B, Y
with approximate decomposition X ≈ BY we can return a new orthonormal basis U by performing SVD
on the completed data, BY = USVT to return an orthonormal basis B → U. In general, however, this
is potentially computationally expensive. If only the principal directions are required, an alternative is to
explicitly transform the solution B using an invertible matrix Q:

X ≈ BQQ−1Y

(15.5.10)

Calling the new basis ˜B = BQ, for a solution to be aligned with the principal directions, we need

˜BT ˜B = I

In other words

QTBTBQ = I

Forming the SVD of B,

B = UDVT

and substituting in equation (15.5.12), we have the requirement

QTVDUTUDVTQ = I

Since UTU = I and VTV = VVT = I, we can use

Q = VTD−1

Hence, given a solution B, we can ﬁnd the principal directions from the SVD of B using

˜B = UTD−1B

(15.5.11)

(15.5.12)

(15.5.13)

(15.5.14)

(15.5.15)

(15.5.16)

If the D × M matrix B is non-square M < D, then the matrix D will be non-square and non-invertible.
To make the above well deﬁned, one may append D with the columns of the identity:

D(cid:48) = [D, IM +1, . . . , ID]

(15.5.17)

where IK is the Kth column of the identity matrix, and use D(cid:48) in place of D above.

15.5.2 Collaborative ﬁltering using PCA with missing data

entry in the vector x speciﬁes the rating the user gives to the ith ﬁlm. The matrix X =(cid:2)x1, . . . , xN(cid:3) for all

A database contains a set of vectors, each describing the ﬁlm ratings for a user in the database. The ith

the N users, has many missing values since any single user will only have given a rating for a small selection
of the possible D ﬁlms. In a practical example one might have D = 10, 000 ﬁlms and N = 1, 000, 000
users. For any user n the task is to predict reasonable values for the missing entries of their rating vector
xn, thereby providing a suggestion as to which ﬁlms they might like to view. Viewed as a missing data
problem, one can ﬁt B and Y using svdm.m as above. Given B and Y we can form a reconstruction on
all the entries of X, by using

˜X = BY

giving therefore a prediction for the missing values.

DRAFT March 9, 2010

(15.5.18)

291

Matrix Decomposition Methods

Figure 15.12: (a): Under-complete representation. There are too few basis
(b): Over-complete representation.
vectors to represent the datapoints.
There are too many basis vectors to form a unique representation of a
datapoint in terms of a linear combination of the basis vectors.

(a)

(b)

z

(a)

x

y

x

y

z

(b)

Figure 15.13: (a): Joint PLSA.
(b): Conditional
PLSA. Whilst written as a graphical model, some
care is required in the interpretation, see text.

15.6 Matrix Decomposition Methods

Given a data matrix X for which each column represents a datapoint, an approximate matrix decompo-
sition is of the form X ≈ BY into a basis matrix B and weight (or coordinate) matrix Y. Symbolically,
matrix decompositions are of the form



(cid:124)

X : Data

(cid:123)(cid:122)

D×N



(cid:125)

 B : Basis

(cid:123)(cid:122)

D×M

≈

(cid:124)



(cid:125)


(cid:124)

Y : Weights/Components

(cid:123)(cid:122)

M×N


(cid:125)

(15.6.1)

In this section we will consider some common matrix decomposition methods.

Under-complete decompositions

When M < D, there are fewer basis vectors than dimensions, ﬁg(15.12a). The matrix B is then called
‘tall’ or ‘thin’. In this case the matrix Y forms a lower dimensional approximate representation of the
data X, PCA being a classic example.

Over-complete decompositions

For M > D the basis is over-complete, there being more basis vectors than dimensions, ﬁg(15.12b). In such
cases additional constraints are placed on either the basis or components. For example, one might require
that only a small number of the large number of available basis vectors is used to form the representation
for any given x. Such sparse-representations are common in theoretical neurobiology where issues of energy
eﬃciency, rapidity of processing and robustness are of interest[214, 155, 252].

15.6.1 Probabilistic latent semantic analysis
Consider two objects, x and y, where dom(x) = {1, . . . , I} and dom(y) = {1, . . . , J}. We have a count
matrix with elements Cij which describes the number of times that x = i, y = j was observed. We can
transform this count matrix into a ‘frequency’ matrix p with elements

p(x = i, y = j) = Cij(cid:80)
(cid:88)

p(x = i, y = j)

ij Cij

(cid:124)

(cid:123)(cid:122)

Xij

Our interest is to ﬁnd a decomposition of this frequency matrix of the form in ﬁg(15.13a)

(cid:125)

≈

k

(cid:124)
(cid:125)
˜p(x = i|z = k)

(cid:123)(cid:122)

(cid:124)
(cid:125)
˜p(y = j|z = k)˜p(z = k)

(cid:123)(cid:122)

Bik

Ykj

≡ ˜p(x = i, y = j)

292

DRAFT March 9, 2010

(15.6.2)

(15.6.3)

Matrix Decomposition Methods

which is a form of matrix decomposition into basis B and coordinates Y. This has the interpretation of
discovering latent topics z that describe the joint behaviour of x and y.

An EM style training algorithm

In order to ﬁnd the approximate decomposition we ﬁrst need a measure of diﬀerence between the matrix
with elements pij and the approximation with elements ˜pij. Since all elements are bounded between 0
and 1 and sum to 1, we may interpret p as a joint probability and ˜p as an approximation to this. For
probabilities, a useful measure of discrepancy is the Kullback-Leibler divergence

Since p is ﬁxed, minimising the Kullback-Leibler divergence with respect to the approximation ˜p is equiv-
alent to maximising the ‘likelihood’ term (cid:104)log ˜p(cid:105)p. This is

(15.6.4)

(15.6.5)

p(x, y) log ˜p(x, y)

KL(p|˜p) = (cid:104)log p(cid:105)p − (cid:104)log ˜p(cid:105)p
(cid:88)
KL(q(z|x, y)|˜p(z|x, y)) =(cid:88)
where(cid:80)

x,y

z

It’s convenient to derive an EM style algorithm to learn ˜p(x|z), ˜p(y|z) and ˜p(z). To do this, consider

q(z|x, y) log q(z|x, y) −

q(z|x, y) log ˜p(z|x, y) ≥ 0

(15.6.6)

(cid:88)

z

z implies summation over all states of the variable z. Rearranging, this gives the bound,

q(z|x, y) log ˜p(z, x, y)

(15.6.7)

log ˜p(x, y) ≥ −
(cid:88)

Plugging this into the ‘likelihood’ term above, we have the bound

p(x, y) log ˜p(x, y) ≥ −

x,y

q(z|x, y) log q(z|x, y)

(cid:88)

z

q(z|x, y) log q(z|x, y) +(cid:88)
p(x, y)(cid:88)
+(cid:88)

(cid:88)

p(x, y)(cid:88)

x,y

z

z

x,y

z

M-step
For ﬁxed ˜p(x|z), ˜p(y|z), the contribution to the bound from ˜p(z) is

q(z|x, y) [log ˜p(x|z) + log ˜p(y|z) + log ˜p(z)]

(15.6.8)

(15.6.9)

(15.6.10)

. Similarly, for ﬁxed ˜p(y|z), ˜p(z),

(15.6.11)

(15.6.12)

(15.6.13)

293

It is straightforward to see that the optimal setting of ˜p(z) is

(cid:16)(cid:80)

(cid:17)
x,y q(z|x, y)p(x, y)|˜p(z)

q(z|x, y) log ˜p(z)

q(z|x, y)p(x, y)

x,y

(cid:88)
p(x, y)(cid:88)
˜p(z) =(cid:88)

x,y

z

(cid:88)

x,y

z

p(x, y)(cid:88)
(cid:88)
(cid:88)

y

since equation (15.6.9) is, up to a constant, KL
the contribution to the bound from ˜p(x|z) is

q(z|x, y) log ˜p(y|z)

Therefore, optimally

˜p(x|z) ∝

p(x, y)q(z|x, y)

and similarly,
˜p(y|z) ∝

p(x, y)q(z|x, y)

x

DRAFT March 9, 2010

Matrix Decomposition Methods

the

images

Figure 15.14: (a) Conditional PLSA
reconstruction of
in
ﬁg(15.5) using a positive (convex)
combination of the 49 positive base
images in (b).
The root mean
square reconstruction error is 1.391×
10−5. The base images tend to be
more ‘localised’ than the correspond-
ing eigen-images ﬁg(15.6b). Here one
sees local structure such as foreheads,
chins, etc.

(15.6.14)

(a)

(b)

E-step

The optimal setting for the q distribution at each iteration is

q(z|x, y) = ˜p(z|x, y)

which is ﬁxed throughout the M-step.

The procedure is given in algorithm(15) and a demonstration is in demoPLSA.m. The ‘likelihood’ equation
(15.6.5) is guaranteed to increase (and the Kullback-Leibler divergence equation (15.6.4) decrease) under
iterating between the E and M-steps, since the method is analogous to an EM procedure Generalisations,
such as using simpler q distributions, (corresponding to generalised EM procedures) are immediate based
on modifying the above derivation.

A related probabilistic model
For variables x, y, z with z hidden and dom(x) = {1, . . . , I} , dom(y) = {1, . . . , J} , dom(z) = {1, . . . , K},
consider a distribution

˜p(x, y, z) = ˜p(x|z)˜p(y|z)˜p(z)

(15.6.15)

and data D = {(xn, yn) , n = 1, . . . , N}. Assuming the data are i.i.d. draws from equation (15.6.15) the
log likelihood is

log ˜p(D) =

N(cid:88)
˜p(xn, yn) =(cid:88)

n=1

where

log ˜p(xn, yn)

˜p(xn|zn)˜p(yn|zn)˜p(zn)

zn

(15.6.16)

(15.6.17)

If xn, yn are sampled from a distribution p(x, y) then, in the limit of an inﬁnite number of samples, N → ∞,
equation (15.6.16) becomes

log ˜p(D) = (cid:104)log ˜p(x, y)(cid:105)p(x,y)

(15.6.18)

which is equation (15.6.5). From this viewpoint, even though we started out with a set of samples, in the
limit, only the distribution of the observed data, p(x, y) is relevant. The generic code for the ﬁnite sample
case, trained with EM is given in demoMultinomialpXYgZ.m. See also exercise(168).

A fully probabilistic interpretation of PLSA can be made via Poisson processes[56]. A related probabilistic
model is Latent Dirichlet Allocation, which is described in section(20.6.1).

294

DRAFT March 9, 2010

Matrix Decomposition Methods

Algorithm 15 PLSA: Given a frequency matrix p(x = i, y = j), return a decomposition(cid:80)

k)˜p(y = j|z = k)˜p(z = k). See plsa.m
1: Initialise ˜p(z), ˜p(x|z), ˜p(y|z).
2: while Not Converged do
Set q(z|x, y) = ˜p(z|x, y)
3:
Set ˜p(x|z) ∝
4:
Set ˜p(y|z) ∝
5:
6: end while

(cid:80)
(cid:80)
y p(x, y)q(z|x, y)
x p(x, y)q(z|x, y)
x,y p(x, y)q(z|x, y)

k ˜p(x = i|z =

(cid:46) E-step
(cid:46) M-Steps

7: Set ˜p(z) =(cid:80)
(cid:80)
Algorithm 16 Conditional PLSA: Given a frequency matrix p(x = i|y = j), return a decomposition
k ˜p(x = i|z = k)˜p(z = k|y = j). See plsaCond.m
1: Initialise ˜p(x|z), ˜p(z|y).
(cid:80)
2: while Not Converged do
(cid:80)
3:
y p(x|y)q(z|x, y)
4:
x p(x|y)q(z|x, y)
5:
6: end while

Set q(z|x, y) = ˜p(z|x, y)
Set ˜p(x|z) ∝
Set ˜p(z|y) ∝

(cid:46) E-step
(cid:46) M-Steps

Conditional PLSA

In some cases it is more natural to consider a conditional frequency matrix

p(x = i|y = j)
(cid:124)
(cid:125)
p(x = i|y = j)

(cid:123)(cid:122)

and seek an approximate decomposition
(cid:124)
(cid:125)
˜p(x = i|z = k)

(cid:123)(cid:122)

≈

(cid:88)

k

Xij

Bik

(cid:124)
(cid:125)
˜p(z = k|y = j)

(cid:123)(cid:122)

Ykj

(15.6.19)

(15.6.20)

as depicted in ﬁg(15.13b). Deriving an EM style algorithm for this is straightforward (see exercise(167)),
and is presented in algorithm(16), being equivalent to the non-negative matrix factorisation algorithm of
[171].

is unity, (cid:80)

Example 73 (Discovering the basis). A set of images is give in ﬁg(15.15a). These were created by ﬁrst
deﬁning 4 base images ﬁg(15.15b). Each base image is positive and scaled so that the sum of the pixels
i p(x = i|z = k) = 1, where k = 1, . . . , 4 and x indexes the pixels, see ﬁg(15.15). We then
sum each of these images using a randomly chosen positive set of 4 weights (under the constraint that the
weights sum to 1) to generate a training image with elements p(x = i|y = j) and j indexes the training
image. This is repeated 144 times to form the full training set, ﬁg(15.15a). The task is, given only the
training set images, to reconstruct the basis from which the images were formed. We assume that we
know the correct number of base images, namely 4. The results of using conditional PLSA on this task
are presented in ﬁg(15.15c) and using SVD in ﬁg(15.15d). In this case PLSA ﬁnds the correct ‘natural’
basis, corresponding to the way the images were generated. The eigenbasis is just as good in terms of
being able to represent any of the training images, but in this case does not correspond to the constraints
under which the data was generated

15.6.2 Extensions and variations

Non-negative matrix factorisation

Non-negative Matrix factorisation (NMF) considers a decomposition in which both the basis and weight
matrices have non-negative entries. An early example of this work is as a form of constrained Factor

DRAFT March 9, 2010

295

Matrix Decomposition Methods

(a)

(b)

(c)

(d)

Figure 15.15: (a): Training data, consisting of a positive (convex) combination of the base images. (b):
(c): Basis learned using conditional
The chosen base images from which the training data is derived.
(d): Eigenbasis
PLSA on the training data. This is virtually indistinguishable from the true basis.
(sometimes called ‘eigenfaces’).

Analysis[217]. Closely related works are [171] which is a generalisation of PLSA (since there is no require-
ment that the basis or components sum to unity). In all cases EM-style training algorithms exist, although
their convergence can be slow. A natural relaxation is when only one of the factors in the decomposition
is constrained to be non-negative. We will encounter similar models in the discussion on Independent
Component Analysis, section(21.6).

Gradient based training

EM style algorithms are easy to derive and implement but can exhibit poor convergence. Gradient based
methods to simultaneously optimize with respect to the basis and the components have been developed,
but require a parameterisation that ensures positivity of the solutions[217].

Array decompositions

It is straightforward to extend the method to the decomposition of multidimensional arrays, based also
on more than one basis. For example

˜p(s, t|u, v)˜p(u|w)˜p(v)˜p(w)

(15.6.21)

(cid:88)

v,w

p(s, t, u) ≈

˜p(s, t, u|v, w)˜p(v, w) =(cid:88)

v,w

Such extensions require only additional bookkeeping.

15.6.3 Applications of PLSA/NMF

Physical models

Non-negative decompositions can arise naturally in certain physical situations. For example, in acoustics,
positive amounts of energy combine linearly from diﬀerent signal sources to form the observed signal. Let’s
imagine that two kinds of signals are present in an acoustic signal, say a piano and a singer. Using NMF
one can learn two separate bases for these cases, and then reconstruct a given signal using only one of
the bases. This means that one could potentially remove the singer from a recording, leaving only the
piano. See also [285] for a more standard probabilistic model in acoustics. This would be analogous to
reconstructing the images in ﬁg(15.15a) using say only one of the learned basis images, see example(73).

Modelling citations

We have a collection of research documents which cite other documents. For example, document 1 might
cite documents 3, 2, 10, etc. Given only the list of citations for each document, can we identify key research
papers and the communities that cite them? Note that this is not the same question as ﬁnding the most
cited documents – rather we want to identify documents with communities and ﬁnd their relevance for a

296

DRAFT March 9, 2010

Matrix Decomposition Methods

(Reinforcement Learning)
Learning to predict by the methods of temporal diﬀerences. Sutton.
Neuronlike adaptive elements that can solve diﬃcult learning control problems. Barto et al.
Practical Issues in Temporal Diﬀerence Learning. Tesauro.
(Rule Learning)
Explanation-based generalization: a unifying view. Mitchell et al.
Learning internal representations by error propagation. Rumelhart et al.
Explanation-Based Learning: An Alternative View. DeJong et al.
(Neural Networks)
Learning internal representations by error propagation. Rumelhart et al.
Neural networks and the bias-variance dilemma. Geman et al.
The Cascade-Correlation learning architecture. Fahlman et al.
(Theory)
Classiﬁcation and Regression Trees. Breiman et al.
Learnability and the Vapnik-Chervonenkis dimension. Blumer et al.
Learning Quickly when Irrelevant Attributes Abound. Littlestone.
(Probabilistic Reasoning)
Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Pearl.

factor 1
0.0108
0.0066
0.0065
factor 2
0.0038
0.0037
0.0036
factor 3
0.0120
0.0061
0.0049
factor 4
0.0093
0.0066
0.0055
factor 5
0.0118
0.0094 Maximum likelihood from incomplete data via the em algorithm. Dempster et al.
0.0056
factor 6
0.0157
0.0132
0.0096
factor 7
0.0063
0.0054
0.0033

Local computations with probabilities on graphical structures. Lauritzen et al.
(Genetic Algorithms)
Genetic Algorithms in Search, Optimization, and Machine Learning. Goldberg.
Adaptation in Natural and Artiﬁcial Systems. Holland.
Genetic Programming: On the Programming of Computers by Means of Natural Selection. Koza.
(Logic)
Eﬃcient induction of logic programs. Muggleton et al.
Learning logical deﬁnitions from relations. Quinlan.
Inductive Logic Programming Techniques and Applications. Lavrac et al.

Table 15.1: Highest ranked documents according to p(c|z). The factor topic labels are manual assignments
based on similarities to the Cora topics. Reproduced from [63].

community.

We use the variable d ∈ {1, . . . , D} to index documents and c ∈ {1, . . . , D} to index citations (both d and
c have the same domain, namely the index of a research article). If document d = i cites article c = j
then we set the entry of the matrix Cij = 1. If there is no citation, Cij is set to zero. We can form a
‘distribution’ over documents and citations using

p(d = i, c = j) = Cij(cid:80)

ij Cij

(15.6.22)

Example 74 (Modelling citations). The Cora corpus (www.cs.umass.edu/∼mccallum) contains an
archive of around 30,000 computer science research papers. From this archive the authors in [63] extracted
the papers in the machine learning category, consisting of 4220 documents and 38,372 citations. Using
these the distribution equation (15.6.22) was formed. The documents have additionally been categorised
by hand into 7 topics: Case-based reasoning, Genetic Algorithms, Neural Networks, Probabilistic
methods, Reinforcement Learning, Rule Learning and Theory.

In [63] the joint PLSA method is ﬁtted to the data using z = 7 topics. From the trained model the
expression p(c = j|z = k) deﬁnes how authoritative paper j is according to community z = k. The results
are presented in table(15.1) and show how the method discovers intuitively meaningful topics.

Modelling the web

Consider a collection of websites, indexed by i. If website j points to website i, one sets Cij = 1 giving a
directed graph of website-to-website links. Since a website will discuss usually only of a small number of

DRAFT March 9, 2010

297

‘topics’ we might be able to explain why there is a link between two websites using a PLSA decomposition.
These algorithms have proved useful for internet search for example to determine the latent topics of
websites and identify the most authoritative websites. See [64] for a discussion.

Kernel PCA

15.7 Kernel PCA

Kernel PCA is a non-linear extension of PCA designed to discover non-linear manifolds. Here we only
brieﬂy describe the approach and refer the reader to [242] for details. In kernel PCA, we replace each x by
a ‘feature’ vector ˜x ≡ φ(x). Note that the use of ˜x here does not have the interpretation we used before as
the approximate reconstruction. The feature map φ takes a vector x and produces a higher dimensional
vector ˜x. For example we could map a two dimensional vector x = [x1, x2]T using

φ(x) =(cid:2)x1, x2, x2

1, . . .(cid:3)T

1, x2

2, x1x2, x3

(15.7.1)

The idea is then to perform PCA on these higher dimensional feature vectors, subsequently mapping back
the eigenvectors to the original space x. The main challenge is to write this without explicitly computing
PCA in the potentially very high dimensional feature vector space. As a reminder, in standard PCA, for
zero mean data, one forms an eigen-decomposition of the sample matrix3

S =

1
N

˜X ˜XT

(15.7.2)

For simplicity, we concentrate here on ﬁnding the ﬁrst principal component ˜e which satisﬁes

˜X ˜XT˜e = λ

(cid:48)˜e

(15.7.3)
for corresponding eigenvalue λ (writing λ(cid:48) = N λ). The ‘dual’ representation is obtained by pre-multiplying
by ˜XT, so that in terms of ˜f ≡ ˜XT˜e, the standard PCA eigen-problem reduces to solving:

(15.7.4)

(15.7.5)

(15.7.6)

˜XT ˜X˜f = λ

(cid:48)˜f

(cid:48)˜e

˜X˜f = λ

(cid:104) ˜XT ˜X
(cid:105)
(cid:104) ˜XT ˜X
(cid:105)

N(cid:88)

n=1

yi =

1
N λi

The feature eigenvector ˜e is then recovered using

We note that matrix ˜XT ˜X has elements

= φ(xm)Tφ(xn)

mn

and recognise this as the scalar product between vectors. This means that the matrix is positive (semi)
deﬁnite and we may equivalently use a positive deﬁnite kernel, see section(19.3),

= k(xm, xn) = Kmn

mn

(15.7.7)

Then equation (15.7.4) can be written as

K˜f = λ

(cid:48)˜f

(15.7.8)
One then solves this eigen-equation to ﬁnd the N dimensional principal dual feature vector ˜f. The
projection of the feature ˜x is given by

y = ˜xT˜e =

˜xT ˜X˜f

1
λ

(15.7.9)

More generally, for a larger number of components, the ith kernel PCA projection yi can be expressed in
terms of the kernel directly as

k(x, xn) ˜f i

n

(15.7.10)

3We use the normalisation N as opposed to N − 1 just for notational convenience – in practice, there is little diﬀerence.

298

DRAFT March 9, 2010

Kernel PCA

(a)

(b)

(c)

Figure 15.16: Canonical Correlation Analysis. (a): Training data. The top panel contains the X matrix
of 1000, 15 dimensional points, and the bottom the corresponding 30 dimensional Y matrix.
(b): The
data in (a) was produced using X = Ah, Y = Bh where A is a 15 × 1 matrix, and B is a 30 × 1 matrix.
(c): Matrices A and B learned by CCA. Note that they are close to the true A and B up to rescaling
and sign changes. See demoCCA.m.

where i is the eigenvalue label.

The above derivation implicitly assumed zero mean features ˜x. Even if the original data x is zero mean,
due to the non-linear mapping, the features may not be zero mean. To correct for this one may show that
the only modiﬁcation required is to replace the matrix K in equation (15.7.8) above with

(cid:48)
mn = k(xm, xn) −
K

1
N

k(xd, xn) −

1
N

k(xm, xd) +

1
N 2

k(xd(cid:48)

, xd)

(15.7.11)

N(cid:88)

d=1

N(cid:88)

d=1

N(cid:88)

d=1,d(cid:48)=1

Finding the reconstructions

The above gives a procedure for ﬁnding the KPCA projection y. However, in many cases we would also
like to have an approximate reconstruction using the lower dimensional y. This is not straightforward
since the mapping from y to x is in general highly non-linear. Here we outline a procedure for achieving
this.
First we ﬁnd the reconstruction ˜x∗ of the feature space ˜x. Now

˜f n
i φ(xn)

(15.7.12)

˜x∗ =(cid:88)

yi˜ei =(cid:88)

i

i

yi

1
λi

(cid:88)

n

Given ˜x∗ we try to ﬁnd that point x(cid:48) in the original data space that maps to ˜x∗. This can be found by
minimising

E(x(cid:48)) =(cid:0)φ(x(cid:48)) − ˜x∗(cid:1)2
, x(cid:48)) − 2(cid:88)

E(x(cid:48)) = k(x(cid:48)

(cid:88)

yi
λi

i

n

Up to negligable constants this is

i k(xn, x(cid:48))
˜f n

One then ﬁnds x(cid:48) by minimising E(x(cid:48)) numerically.
.

DRAFT March 9, 2010

(15.7.13)

(15.7.14)

299

training data2004006008001000510152004006008001000102030051015−202true model0102030−202051015−0.200.2learned model0102030−0.200.2Canonical Correlation Analysis

15.8 Canonical Correlation Analysis

Consider x and y which have dimensions dim (x) and dim (y) respectively. For example x might represent
a segment of video and y the corresponding audio. Given then a collection (xn, yn) , n = 1, . . . , N, an
interesting challenge is to identify which parts of the audio and video ﬁles are strongly correlated. One
might expect, for example, that the mouth region of the video is strongly correlated with the audio.

One way to achieve this is to project each x and y to one dimension using aTx and bTy such that the
correlation between the projections is maximal. The unnormalised correlation between the projections

aTx and bTy is(cid:88)

(cid:34)(cid:88)

(cid:35)

aTxnbTyn = aT

xnynT

b

n

n

and the normalised correlation is

(cid:112)aTSxxa(cid:112)bTSyyb

aTSxyb

(15.8.1)

(15.8.2)

(15.8.4)

(15.8.5)

(15.8.6)

where Sxy is the sample x, y cross correlation matrix. When the joint covariance of the stacked vectors
zn = [xn, yn] is considered Sxx, Sxy, Syx, Syy are the blocks of the joint covariance matrix.

Since equation (15.8.2) is invariant with respect to length scaling of a and also b, we can consider the
equivalent objective

E(a, b) = aTSxyb

(15.8.3)

subject to aTSxxa = 1 and bTSyyb = 1. To ﬁnd the optimal projections a, b, under the constraints, we
use the Lagrangian,

L (a, b, λa, λb) ≡ aTSxyb + λa
2

1 − aTSxxa
from which we obtain the zero derivative criteria

(cid:16)

(cid:17)

(cid:16)

(cid:17)
1 − bTSyyb

+ λb
2

Sxyb = λaSxxa,

Syxa = λbSyyb

Hence

aTSxyb = λaaTSxxa = λa,

bTSyxa = λbbTSyyb = λb

Since aTSxyb = bTSyxa we must have λa = λb = λ at the optimum. If we assume that Syy is invertible,

b =

S−1
yy Syxa

1
λ

Using this to eliminate b in equation (15.8.5) we have

SxyS−1

yy Syxa = λ2Sxxa

which is a generalised eigen-problem. Assuming that Sxx is invertible we can equivalently write

xx SxyS−1
S−1

yy Syxa = λ2a

(15.8.7)

(15.8.8)

(15.8.9)

which is a standard eigen-problem (albeit with λ2 as the eigenvalue). Once this is solved we can ﬁnd b
using equation (15.8.7).

300

DRAFT March 9, 2010

Exercises

15.8.1 SVD formulation

It is straightforward to show that we can ﬁnd a by ﬁrst computing the SVD of

− 1
xx SxyS

2

− 1
2
yy

S

(15.8.10)

in the form UDVT and extracting the maximal singular vector u1 of U (the ﬁrst column on U). Then a
− 1
yy v1, where v1 is the ﬁrst column of V. In this way,
is optimally S

the extension to ﬁnding M multiple directions A = (cid:2)a1, . . . , aM(cid:3) and B = (cid:2)b1, . . . , bM(cid:3) is clear – one

− 1
xx u1, and similarly, b is optimally S

2

2

takes the corresponding ﬁrst M singular values accordingly. Doing so maximises the criterion

trace(cid:0)ATSxyB(cid:1)

(cid:112)trace (ATSxxA)(cid:112)trace (BTSyyB)

(15.8.11)

This approach is taken in cca.m – see ﬁg(15.16) for a demonstration. One can also show that CCA cor-
responds to the probabilistic Factor Analysis model under a block restriction on the form of the factor
loadings, see section(21.2.1).

CCA and related kernel extensions have been applied in machine learning contexts, for example to model
the correlation between images and text in order to improve image retrieval from text queries, see [126].

15.9 Notes

PCA is also known as the Karhunen-Lo`eve decomposition, particularly in the engineering literature.

15.10 Code

pca.m: Principal Components Analysis
demoLSI.m: Demo of Latent Semantic Indexing/Analysis
svdm.m: Singular Value Decomposition with missing data
demoSVDmissing.m: Demo SVD with missing data

plsa.m: Probabilistic Latent Semantic Analysis
plsaCond.m: Conditional Probabilistic Latent Semantic Analysis
demoPLSA.m: Demo of PLSA
demoMultnomialpXYgZ.m: Demo of ‘ﬁnite sample’ PLSA

cca.m: Canonical Correlation Analysis (CCA)
demoCCA.m: Demo of Canonical Correlation Analysis

15.11 Exercises

Exercise 161. Consider a dataset in two dimensions where the data lies on the circumference of a circle
of unit radius. What would be the eﬀect of using PCA on this dataset, in which we attempt to reduce the
dimensionality to 1? Suggest an alternative one dimensional representation of the data.

Exercise 162. Consider two vectors xa and xb and their corresponding PCA approximations c+(cid:80)M
and c +(cid:80)M

i=1 aiei
i=1 biei, where the eigenvectors ei, i = 1, . . . , M are mutually orthogonal and have unit length.
The eigenvector ei has corresponding eigenvalue λi. Approximate (xa − xb)2 by using the PCA represen-
tations of the data, and show that this is equal to (a − b)2.
Exercise 163. Show how the solution for a to the CCA problem in equation (15.8.8) can be transformed
into the form expressed by equation (15.8.10), as claimed in the text.

DRAFT March 9, 2010

301

Exercises

(15.11.3)

(15.11.4)

(cid:16)

xa − xb(cid:17)T

S−1(cid:16)

xa − xb(cid:17)

Exercise 164. Let S be the covariance matrix of the data. The Mahalanobis distance between xa and xb
is deﬁned as

.

(15.11.1)

Explain how to approximate this distance using M-dimensional PCA approximations.
Exercise 165 (PCA with external inputs). In some applications, one may suspect that certain external
variables v have a strong inﬂuence on how the data x is distributed. For example, if x represents an image,
it might be that we know the lighting condition v under which the image was made – this will have a large
eﬀect on the image. It would make sense therefore to include the known lighting condition in forming
a lower dimensional representation of the image. Note that we don’t want to form a lower dimensional
representation of the joint x, v, rather we want to form a lower dimensional representation of x alone,
bearing in mind that some of the variability observed may be due to v.
We therefore assume an approximation

k ck
vn

(15.11.2)

j

k
where the coeﬃcients yn
i , i = 1, . . . , N, n = 1, . . . , N and basis vectors bj, j = 1, . . . , J and ck, k =
1, . . . , K are to be determined. The external inputs v1, . . . , vN are given. The sum squared error loss
between the xn and their linear reconstruction equation (15.11.2) is

(cid:88)

j bj +(cid:88)

yn

xn ≈

xn

E =(cid:88)

n,i

(cid:88)

j

2

(cid:88)

k

i −

j bj
yn

i −

k ck
vn
i

Find the parameters that minimise E.
Exercise 166. Consider the following 3-dimensional datapoints:

(1.3, 1.6, 2.8)(4.3,−1.4, 5.8)(−0.6, 3.7, 0.7)(−0.4, 3.2, 5.8)(3.3,−0.4, 4.3)(−0.4, 3.1, 0.9)

Perform Principal Components Analysis by:

1. Calculating the mean, c, of the data.

2. Calculating the covariance matrix S = 1
6
3. Finding the eigenvalues and eigenvectors ei of the covariance matrix.

(cid:80)6
n=1 xn(xn)T − ccT of the data.

You should ﬁnd that only two eigenvalues are large, and therefore that the data can be well represented
using two components only. Let e1 and e2 be the two eigenvectors with largest eigenvalues.

1. Calculate the two dimensional representation of each datapoint (e1·(xn−c), e2·(xn−c)), n = 1, . . . , 6.
2. Calculate the reconstruction of each datapoint c + (e1T(xn − c))e1 + (e2T(xn − c))e2, n = 1, . . . , 6.

Exercise 167. Consider a ‘conditional frequency matrix’

p(x = i|y = j)

(cid:88)

k

Show how to derive an EM style algorithm for an approximate decomposition of this matrix in the form

p(x = i|y = j) ≈

˜p(x = i|z = k)˜p(z = k|y = j)

(15.11.6)

where k = 1, . . . , Z, i = 1, . . . , X, j = 1, . . . , Y .
Exercise 168. For the multinomial model ˜p(x, y, z) described in equation (15.6.15), derive explicitly the
EM algorithm and implement this in MATLAB. For randomly chosen values for the conditional proba-
bilities, draw 10000 samples from this model for X = 5, Y = 5, Z = 4 and compute from this the matrix
with elements

(15.11.5)

(15.11.7)

(cid:80)X

(cid:80)Y

pij =

(cid:93) (x = i, y = j)

i=1

j=1 (cid:93) (x = i, y = j)

Now run PLSA (use plsa.m) with the settings X = 5, Y = 5, Z = 4 to learn and compare your results
with those obtained from the ﬁnite sample model equation (15.6.15).

302

DRAFT March 9, 2010

CHAPTER 16

Supervised Linear Dimension Reduction

16.1 Supervised Linear Projections

In chapter(15) we discussed dimension reduction using an unsupervised procedure. In cases where class
information is available, and our ultimate interest is to reduce dimensionality for improved classiﬁcation,
it makes sense to use the available class information in forming the projections. Exploiting the class label
information to improve the projection is a form of supervised dimension reduction. Let’s consider data
from two diﬀerent classes. For class 1, we have a set of data N1 datapoints,

(cid:110)
(cid:110)

(cid:111)
(cid:111)

X1 =

1, . . . , xN1
x1
1

and similarly for class 2, we have a set of N2 datapoints

X2 =

2, . . . , xN2
x1
2

Our interest is then to ﬁnd a linear projection,

y = WTx

(16.1.1)

(16.1.2)

(16.1.3)

where dim W = D×L, L < D, such that for datapoints xi, xj in the same class, the distance between their
projections yi, yj should be small. Conversely, for datapoints in diﬀerent classes, the distance between
their projections should be large. This may be useful for classiﬁcation purposes since for a novel point x∗,
if its projection

y∗ = WTx∗

(16.1.4)
is close to class 1 projected data, we would expect x∗ to belong to class 1. In forming the supervised
projection, only the class discriminative parts of the data are retained, so that the procedure can be
considered a form of supervised feature extraction.

16.2 Fisher’s Linear Discriminant

We restrict attention to binary class data. Also, for simplicity, we project the data down to one dimension.
The canonical variates algorithm of section(16.3) deals with the generalisations.

Gaussian assumption

We model the data from each class with a Gaussian. That is

p(x1) = N (x1 m1, S1) ,

p(x2) = N (x2 m2, S2)

303

(16.2.1)

Fisher’s Linear Discriminant

(a)

(b)

Figure 16.1: The large crosses represent data from class 1, and the large circles from class 2. Their pro-
jections onto 1 dimension are represented by their small counterparts. (a): Fisher’s Linear Discriminant
(b): Unsupervised dimension reduction
Analysis. Here there is little class overlap in the projections.
using Principal Components Analysis for comparison. There is considerable class overlap in the projec-
tion. In both (a) and (b) the one dimensional projection is the distance along the line, measured from an
arbitrary chosen ﬁxed point on the line.

where m1 is the sample mean of class 1 data, and S1 the sample covariance; similarly for class 2. The
projections of the points from the two classes are then given by

Because the projections are linear, the projected distributions are also Gaussian,

1 = wTxn
yn
1 ,

2 = wTxn
yn
2

(cid:0)y1 µ1, σ2
(cid:0)y2 µ2, σ2

1

2

(cid:1) ,
(cid:1) ,

p(y1) = N
p(y2) = N

µ1 = wTm1,
µ2 = wTm2,

1 = wTS1w
σ2
2 = wTS2w
σ2

(16.2.2)

(16.2.3)
(16.2.4)

We search for a projection w such that the projected distributions have minimal overlap. This can be
achieved if the projected Gaussian means are maximally separated, (µ1 − µ2)2 is large. However, if the
variances σ2
2 are also large, there could be a large overlap still in the classes. A useful objective function
therefore is

1, σ2

(µ1 − µ2)2
1 + π2σ2
π1σ2
2

(16.2.5)

where πi represents the fraction of the dataset in class i.
equation (16.2.5) is

In terms of the projection w, the objective

F (w) =

wT (m1 − m2) (m1 − m2)T w

wT (π1S1 + π2S2) w

=

wTAw
wTBw

where

The optimal w can be found by diﬀerentiating equation (16.2.6) with respect to w. This gives

A = (m1 − m2) (m1 − m2)T ,
(cid:104)(cid:16)
(cid:17)

wTAw
wTBw

∂
∂w

(cid:16)

(cid:17)

(cid:16)

=

Aw −
and therefore the zero derivative requirement is

(wBw)2

wTBw

(cid:17)

2

(cid:16)

wTBw

Aw =

wTAw

Bw

B = π1S1 + π2S2

(cid:17)

(cid:105)

wTAw

Bw

(16.2.6)

(16.2.7)

(16.2.8)

(16.2.9)

304

DRAFT March 9, 2010

−202468−5−4−3−2−1012345−202468−5−4−3−2−1012345Canonical Variates

Multiplying by the inverse of B we have
B−1 (m1 − m2) (m1 − m2)T w =

wTAw
wTBw

w

This means that the optimal projection is explicitly given by

w ∝ B−1 (m1 − m2)

(16.2.10)

(16.2.11)

Although the proportionality factor depends on w, we may take it to be constant since the objective
function F (w) of equation (16.2.6) is invariant to rescaling of w. We may therefore take

w = kB−1 (m1 − m2)

It is common to rescale w to have unit length, wTw = 1, such that

k =

(cid:113)
(m1 − m2)T B−2 (m1 − m2)

1

(16.2.12)

(16.2.13)

An illustration of the method is given in ﬁg(16.1), which demonstrates how supervised dimension reduc-
tion can produce lower dimensional representations more suitable for subsequent classiﬁcation than an
unsupervised method such as PCA.

One can also arrive at the equation (16.2.12) from a diﬀerent starting objective. By treating the projection
as a regression problem y = wTx+b in which the outputs y are deﬁned as y1 and y2 for classes 1 and class 2
respectively, one may show that, for suitably chosen y1 and y2, the solution using a least squares criterion
is given by equation (16.2.12) [83, 42]. This also suggests a way to regularise LDA, see exercise(171).
Kernel extensions of LDA are possible, see for example [78, 248].

When the naive method breaks down

The above derivation relied on the existence of the inverse of B.
In practice, however, B may not be
invertible, and the above procedure requires modiﬁcation. A case where B is not invertible is when there
are fewer datapoints N1 + N2 than dimensions D. Another case is when there are elements of the input
vectors that never vary. For example, in the hand-written digits case, the pixels at the corner edges are
actually always zero. Let’s call this corner pixel z. The matrix B will then have a zero entry for [B]z,z
(indeed the whole zth row and column will be zero) so that for any vector

w = (0, 0, . . . , wz, 0, 0, . . . , 0) ⇒ wTBw = 0

(16.2.14)

This shows that the denominator of Fisher’s objective can become zero, and the objective ill deﬁned. We
will deal with these issues section(16.3.1).

16.3 Canonical Variates

Canonical Variates generalises Fisher’s method to projections in more than one dimension and more than
two classes. The projection of any point is given by

y = WTx

where W is a D × L matrix. Assuming that the data x from class c is Gaussian distributed,

p(x) = N (x mc, Sc)

the projections y are also Gaussian

p(y) = N

y WTmc, WTScW

(cid:16)

(cid:17)

To extend to more than two classes, we deﬁne the following matrices:

DRAFT March 9, 2010

(16.3.1)

(16.3.2)

(16.3.3)

305

Between class Scatter Find m the mean of the whole dataset and mc, the mean of the each class c.

Canonical Variates

where Nc is the number of datapoints in class c, c = 1, . . . , C.

Within class Scatter For each class c form a covariance matrix Sc and mean mc. Deﬁne

Form

A ≡

Nc (mc − m) (mc − m)T

C(cid:88)

c=1

C(cid:88)

c=1

B ≡

NcSc

This naturally gives rise to a Raleigh quotient objective

trace(cid:0)WTAW(cid:1)

trace (WTBW)

F (W) ≡

˜BT ˜B = B

Then deﬁning

the objective can be written in terms of ˜W:

(cid:17)

trace

(cid:16) ˜WT ˜B−TA ˜B−1 ˜W
(cid:17)

˜W = ˜BW ⇒ W = ˜B−1 ˜W
(cid:16) ˜WT ˜W
(cid:17)
(cid:16) ˜WTC ˜W

F ( ˜W) ≡

trace

F ( ˜W) ≡ trace

, subject to ˜WT ˜W = I

(16.3.4)

(16.3.5)

(16.3.6)

(16.3.7)

(16.3.8)

(16.3.9)

(16.3.10)

(16.3.11)

(16.3.12)

(16.3.13)

Assuming B is invertible (see section(16.3.1) otherwise), we can deﬁne the Cholesky factor ˜B, with

If we assume an orthonormality constraint on ˜W, then we equivalently require the maximisation of

where

C ≡ ˜B−TA ˜B−1

Since C is symmetric and positive semi-deﬁnite, it has a real eigen-decomposition

C = EΛET

where Λ = diag (λ1, λ2, . . . , λD) is diagonal with non-negative entries containing the eigenvalues, sorted
by decreasing order, λ1 ≥ λ2 ≥ . . . and ETE = I. Hence

(cid:16) ˜WTEΛET ˜W

(cid:17)

F ( ˜W) = trace

By setting ˜W = [e1, . . . , eL], where el is the lth eigenvector, the objective becomes the sum of the ﬁrst L
eigenvalues. This setting maximises the objective function since forming ˜W from any other columns of E
would give a lower sum. We then return

W = ˜B−1 ˜W

(16.3.14)

as the projection matrix. The procedure is outlined in algorithm(17). Note that since A has rank C,
there can be no more than C − 1 non-zero eigenvalues and corresponding directions.
306

DRAFT March 9, 2010

Canonical Variates

Algorithm 17 Canonical Variates

1: Compute the between and within class scatter matrices A, equation (16.3.4) and B, equation (16.3.5).
2: Compute the Cholesky factor ˜B of B.
3: Compute the L principal eigenvectors [e1, . . . , eL] of ˜B−TA ˜B−1.
4: ˜W = [e1, . . . , eL]
5: Return W = ˜B−1 ˜W as the projection matrix.

16.3.1 Dealing with the nullspace

The above derivation of Canonical Variates (and also Fisher’s LDA) requires the invertibility of the matrix
B. However, as we discussed in section(16.2) one may encounter situations where B is not invertible. A
solution is to require that W lies only in the subspace spanned by the data (that is there can be no
contribution from the nullspace). To do this we ﬁrst concatenate the training data from all classes into
one large matrix X. A basis for X can be found using, for example, the thin-SVD technique which returns
an orthonormal basis Q. We then require the solution W to be expressed in this basis:

W = QW(cid:48)

(16.3.15)
for some matrix W(cid:48). Substituting this in the Canonical Variates objective equation (16.3.6), we obtain

(cid:16)

W(cid:48)TQTAQW(cid:48)(cid:17)
trace(cid:0)W(cid:48)TQTBQW(cid:48)(cid:1)

trace

F (W(cid:48)) ≡

(16.3.16)

This is of the same form as the standard quotient, equation (16.3.6), on replacing the between-scatter A
with

A(cid:48)

≡ QTAQ

and the within-scatter B with

(16.3.17)

B(cid:48)

≡ QTBQ

(16.3.18)
In this case B(cid:48) is guaranteed invertible, and one may carry out Canonical Variates, as in section(16.3)
above. This will return a matrix W(cid:48). We then return

W = QW(cid:48)

See also CanonVar.m.

(16.3.19)

Example 75 (Using canonical variates on the Digits Data). We apply canonical variates to project the
digit data onto two dimensions, see ﬁg(16.3). There are 800 examples of a three, 800 examples of a ﬁve and
800 examples of a seven. Thus, overall, there are 2400 examples lying in a 784 (28× 28 pixels) dimensional
space. Note how the canonical variates projected data onto two dimensions has very little class overlap,
see ﬁg(16.3a).
In comparison the projections formed from PCA, which discards the class information,
displays a high degree of class overlap. The diﬀerent scales of the canonical variates and PCA projections

q1

q2

Figure 16.2: Each three dimensional datapoint lies in a
two-dimensional plane, meaning that the matrix B is not
full rank, and therefore not invertible. A solution is given
by ﬁnding vectors q1, q2 that span the plane, and express-
ing the Canonical Variates solution in terms of these vec-
tors alone.

DRAFT March 9, 2010

307

Exercises

(a)

(b)

Figure 16.3: (a): Canonical Variates projection of examples of handwritten digits 3(‘+’), 5(‘o’) and 7(di-
amond). There are 800 examples from each digit class. Plotted are the projections down to 2 dimensions.
(b): PCA projections for comparison.

In PCA W is unitary; in canonical
is due to the diﬀerent constraints on the projection matrices W.
variates WTBW = I, meaning that W will scale with the inverse square root of the largest eigenvalues
of the within class scatter matrix. Since the canonical variates objective is independent of linear scaling,
W can be rescaled with an arbitrary scalar prefactor γW, as desired.

16.4 Using non-Gaussian Data Distributions

The applicability of canonical variates depends on our assumption that a Gaussian is a good description
of the data. Clearly, if the data is multimodal, using a single Gaussian to model the data in each class
is a poor assumption. This may result in projections with a large class overlap. In principle, there is no
conceptual diﬃculty in using more complex distributions, with say more general criteria such as Kullback-
Leibler divergence between projected distributions used as the objective. However, such criteria typically
result in diﬃcult optimisation problems. Canonical variates is popular due to its simplicity and lack of
local optima issues in constructing the projection.

16.5 Code

CanonVar.m: Canonical Variates
demoCanonVarDigits.m: Demo for Canonical Variates

16.6 Exercises

Exercise 169. What happens to Fisher’s Linear Discriminant if there are less datapoints than dimensions?

Exercise 170. Modify demoCanonVarDigits.m to project and visualise the digits data in 3 dimensions.

Exercise 171. Consider N1 class 1 datapoints xn1, n1 = 1, . . . , N1 and class 2 datapoints xn2, n2 =
1, . . . , N2. We will make a linear predictor for the data,

y = wTx + b

308

(16.6.1)

DRAFT March 9, 2010

−0.1−0.0500.050.10.15−0.06−0.04−0.0200.020.040.060.080.10.120.14−3000−2500−2000−1500−1000−500−1000−50005001000Exercises

with the aim to predict value y1 for data from class 1 and y2 for data from class two. A measure of the ﬁt
is given by

N1(cid:88)

n1=1

(cid:16)

(cid:17)2
y1 − wTxn1 − b

+

N2(cid:88)

n2=1

(cid:16)

(cid:17)2
y2 − wTxn2 − b

Show that by setting y1 = (N1 + N2)/N1 and y2 = (N1 + N2)/N2 the w which minimises E corresponds to
Fisher’s LDA solution. Hint: ﬁrst show that the two zero derivative conditions are

E(w, b|y1, y2) =
(cid:16)
(cid:88)
(cid:16)
and (cid:88)

n1

y1 − b − wTxn1

n1

y1 − b − wTxn1
(cid:18)

(cid:17)
(cid:17)

(cid:16)
+(cid:88)
n1 +(cid:88)

xT

n2

n2

(cid:17)

= 0

y2 − b − wTxn2
(cid:16)

y2 − b − wTxn2

(cid:17)

xT
n2 = 0

which can be reduced to the single equation

N (m1 − m2) =

(m1 − m2) (m1 − m2)T
where B is as deﬁned for LDA in the text, equation (16.2.7).

N

NB + N1N2

(cid:19)

w

Note that this suggests a way to regularise LDA, namely by adding on a term λwTw to E(w, b|y1, y2).
This can be absorbed into redeﬁning equation (16.3.5) as

B(cid:48) = B + λI

(16.6.6)
In other words, one can increase the covariance B by an additive amount λI. The optimal regularising
constant λ may be set by cross-validation. More generally one can consider the use of a regularising matrix
λR, where R is positive deﬁnite.
Exercise 172. Consider the digit data of 892 ﬁves digit5.mat and 1028 sevens digit7.mat. Make a
training set which consists of the ﬁrst 500 examples from each digit class. Use Canonical Variates to ﬁrst
project the data down to 50 dimensions and compute the Nearest Neighbour performance on the remaining
digits. Compare the classiﬁcation accuracy to using Nearest Neighbours the projections from PCA using
50 components.
Exercise 173. Consider an objective function of the form

F (w) ≡

A(w)
B(w)

(16.6.7)

where A(w) and B(w) are positive functions, and our task is to maximise F (w) with respect to w. It may
be that this objective does not have a simple algebraic solution, even though A(w) and B(w) are simple
functions.

(16.6.2)

(16.6.3)

(16.6.4)

(16.6.5)

(16.6.8)

(16.6.9)

(16.6.10)

We can consider an alternative objective, namely

J(w, λ) = A(w) − λB(w)

where λ is a constant scalar. Choose an initial point wold at random and set

λold ≡ A(wold)/B(wold)

In that case J(wold, λold) = 0. Now choose a w such that

J(w, λold) = A(w) − λoldB(w) ≥ 0

This is certainly possible since J(wold, λold) = 0. If we can ﬁnd a w such that J(w, λold) > 0, then

A(w) − λoldB(w) > 0

(16.6.11)
Show that for such a w, F (w) > F (wold), and suggest an iterative optimisation procedure for objective
functions of the form F (w).

DRAFT March 9, 2010

309

Exercises

310

DRAFT March 9, 2010

CHAPTER 17

Linear Models

17.1 Introduction: Fitting A Straight Line

Given training data {(xn, yn) , n = 1, . . . , N}, for scalar input xn and scalar output yn, a linear regression
ﬁt is

y(x) = a + bx

(17.1.1)

To determine the best parameters a, b, we use a measure of the discrepancy between the observed outputs
and the linear regression ﬁt such as the sum squared training error. This is also called ordinary least
squares and minimises the average vertical projection of the points y to ﬁtted line:

N(cid:88)

n=1

E(a, b) =

N(cid:88)

n=1

(yn − a − bxn)2

[yn − y(xn)]2 =
N(cid:88)

Our task is to ﬁnd the parameters a and b that minimise E(a, b). Diﬀerentiating with respect to a and b
we obtain

∂
∂a

E(a, b) = −2

(yn − a − bxn),

n=1

∂
∂b

E(a, b) = −2

(yn − a − bxn)xn

Dividing by N and equating to zero, the optimal parameters are given from the solution to the two linear
equations

N(cid:88)

n=1

(17.1.2)

(17.1.3)

(17.1.4)

(17.1.5)

(17.1.6)

(17.1.7)

(cid:104)y(cid:105) − a − b(cid:104)x(cid:105) = 0,

(cid:104)xy(cid:105) − a(cid:104)x(cid:105) − b(cid:10)x2(cid:11) = 0
(cid:80)N
− (cid:104)x(cid:105)2(cid:105)

(cid:104)(cid:10)x2(cid:11)

where we used the notation (cid:104)f(x, y)(cid:105) to denote 1
to determine a and b:
a = (cid:104)y(cid:105) − b(cid:104)x(cid:105)

b(cid:10)x2(cid:11) = (cid:104)yx(cid:105) − (cid:104)x(cid:105) ((cid:104)y(cid:105) − b(cid:104)x(cid:105)) ⇒ b

N

Hence

= (cid:104)xy(cid:105) − (cid:104)x(cid:105)(cid:104)y(cid:105)

b = (cid:104)xy(cid:105) − (cid:104)x(cid:105)(cid:104)y(cid:105)
(cid:104)x2(cid:105) − (cid:104)x(cid:105)2

and a is found by substituting this value for b into equation (17.1.5).

n=1 f(xn, yn). We can readily solve the equations(17.1.4)

In contrast to ordinary least squares regression, PCA from chapter(15) minimises the orthogonal projection
of y to the line and is known as orthogonal least squares – see example(76).

311

Linear Parameter Models for Regression

Figure 17.1: Data from crickets – the number of
chirps per second, versus the temperature in Fahren-
heit.

Example 76. Consider the data in ﬁg(17.1), in which we plot the number of chirps c per second for
crickets, versus the temperature t in degrees Fahrenheit. A biologist believes that there is a simple
relation between the number of chirps and the temperature of the form

c = a + bt

(17.1.8)

where she needs to determine the parameters a and b. For the cricket data, the ﬁt is plotted in ﬁg(17.2a).
For comparison we plot the ﬁt from the PCA, ﬁg(17.2b), which minimises the sum of the squared orthogonal
projections from the data to the line. In this case there is little numerical diﬀerence between the two ﬁts.

17.2 Linear Parameter Models for Regression

We can generalise on the idea of ﬁtting straight lines to ﬁtting linear functions of vector inputs. For a
dataset {(xn, yn) , n = 1, . . . , N}, a linear parameter regression model (LPM) is deﬁned by1

y(x) = wTφ(x)

(17.2.1)

where φ(x) is a vector valued function of the input vector x. For example, in the case of a straight line
ﬁt, with a scalar input and output, section(17.1), we have

φ(x) = (1, x)T,

w = (a, b)T,

(17.2.2)

We deﬁne the train error as the sum of squared diﬀerences between the observed outputs and the predictions
under the linear model:

E(w) =

(yn − wTφn)2,

where φn ≡ φ (xn)

(17.2.3)

We now wish to determine the parameter vector w that minimises E(w). Writing out the error in terms
of the components of w,

N(cid:88)

n=1

N(cid:88)

n=1

(17.2.4)

(17.2.5)

(17.2.6)

E(w) =

(yn −

wiφn

i )(yn −

wjφn
j )

Diﬀerentiating with respect to wk, and equating to zero gives

(cid:88)

j

(cid:88)

i

(cid:88)

n

ynφn

wi

i φn
φn
k

or, in matrix notation,

N(cid:88)

n=1

N(cid:88)

k =(cid:88)
N(cid:88)

i

ynφn =

φn(φn)Tw

n=1

n=1

1Note that the model is linear in the parameter w – not necessarily linear in x.

312

DRAFT March 9, 2010

65707580859095100121416182022chirps per sectemperature (F)Linear Parameter Models for Regression

(a)

(b)

Figure 17.2: (a): Straight line regression ﬁt to the cricket data.
(b): PCA ﬁt to the data. In regression
we minimize the residuals – the vertical distances from datapoints to the line. In PCA the ﬁt minimizes
the orthogonal projections to the line. In this case, there is little diﬀerence in the ﬁtted lines. Both go
through the mean of the data; the linear regression ﬁt has slope 0.121 and the PCA ﬁt has slope 0.126.

These are called the normal equations, for which the solution is

(cid:32) N(cid:88)

(cid:33)−1 N(cid:88)

w =

φn(φn)T

ynφn

(17.2.7)

n=1

n=1

Although we write the solution using matrix inversion, in practice one ﬁnds the numerical solution using
Gaussian elimination[117] since this is faster and more numerically stable.

Example 77. A cubic polynomial ﬁt

A cubic polynomial is given by

y(x) = w1 + w2x + w3x2 + w4x3

As a LPM, this can be expressed using

φ(x) =(cid:0)1, x, x2, x3(cid:1)T

(17.2.8)

(17.2.9)

The ordinary least squares solution has the form given in equation (17.2.17). The ﬁtted cubic polynomial
is plotted in ﬁg(17.3). See also demoCubicPoly.m.

5(cid:88)

Example 78 (Predicting return). In ﬁg(17.4) we present ﬁtting an LPM with vector inputs x to a scalar
output y. The vector x represents factors that are believed to aﬀect the stock price of a company, with the
stock price return given by the scalar y. A hedge fund manager believes that the returns may be linearly
related to the factors:

yt =

wixit

(17.2.10)

i=1

and wishes to ﬁt the parameters w in order to use the model to predict future stock returns. This is
straightforward using ordinary least squares, this being simply an LPM with a linear φ function. See

Figure 17.3: Cubic polynomial ﬁt to the cricket data.

DRAFT March 9, 2010

313

65707580859095100121416182022chirps per sectemperature (F)65707580859095100121416182022chirps per sectemperature (F)65707580859095100121416182022chirps per sectemperature (F)Linear Parameter Models for Regression

on yt =(cid:80)

Figure 17.4: Predicting stock return using a linear LPM. The
top ﬁve panels present the inputs x1, . . . , x5 for 20 train days
(blue) and 5 test days (red). The corresponding train out-
put (stock return) y for each day is given in the bottom
panel. The predictions y21, . . . , y25 are the predictions based
i wixit with w trained using ordinary least squares.
With a regularisation term 0.01wTw, the OLS learned w is
[1.42, 0.62, 0.27,−0.26, 1.54]. Despite the simplicity of these
models, their application in the ﬁnance industry is widespread,
with signiﬁcant investment made on collating factors x that may
be indicative of future return. See demoLPMhedge.m.

ﬁg(17.4) for an example. Such models also form the basis for more complex models in ﬁnance, see for
example [194].

17.2.1 Vector outputs

It is straightforward to generalise the above framework to vector outputs y. Using a separate weight vector
wi for each output component yi, we have

The mathematics follows similarly to before, and we may deﬁne a training error per output as

yi(x) = wT

i φ(x)

E(w) =(cid:88)

E(wi) =(cid:88)

(cid:88)

(cid:16)

i

i

n

i φn(cid:17)2

i − wT
yn

(17.2.11)

(17.2.12)

Since the training error decomposes into individual terms, one for each output, the weights for each
output can be trained separately. In other words, the problem decomposes into a set of independent scalar
output problems. In case the parameters w are tied or shared amongst the outputs, the training is still
straightforward since the objective function remains linear in the parameters, and this is left as an exercise
for the interested reader.

17.2.2 Regularisation

For most purposes, our interest is not just to ﬁnd the function that best ﬁts the training data but one that
that will generalise well. To control the complexity of the ﬁtted function we may add an extra ‘regularising’
(or ‘penalty’) term to the training error to penalise rapid changes in the output. For example a regularising
term that can be added to equation (17.2.3) is

(17.2.13)

(cid:105)2

y(xn) − y(xn(cid:48)

)

N(cid:88)

n=1

λ

e

(cid:16)

−γ

xn−xn(cid:48)(cid:17)2(cid:104)
(cid:105)2

N(cid:88)
(cid:104)
y(xn) − y(xn(cid:48)
xn−xn(cid:48)(cid:17)2

n(cid:48)=1

)

The factor
−γ

(cid:16)

factor e
xn(cid:48)
regularising term.

penalises large diﬀerences in the outputs corresponding to two inputs. The

has the eﬀect of weighting more heavily terms for which two input vectors xn and
are close together; γ is a ﬁxed length-scale parameter and λ determines the overall strength of the

314

DRAFT March 9, 2010

051015202500.510510152025−0.500.50510152025−0.500.50510152025−2.5−2−1.50510152025−1010510152025024Linear Parameter Models for Regression

Figure 17.5: A set of ﬁxed-width (α = 1) radial basis functions,
2(x−mi)2
− 1
, with the centres mi evenly spaced. By taking a linear combi-
e
nation of these functions we can form a ﬂexible function class.

Since y = wTφ(x), expression (17.2.13) can be written as

wTRw

where

R ≡ λ

xn−xn(cid:48)(cid:17)2(cid:16)
(cid:16)

φn − φn(cid:48)(cid:17)(cid:16)

φn − φn(cid:48)(cid:17)T

−γ

e

The regularised train error is then

(cid:48)(w) =

E

(yn − wTφn)2 + wTRw

n(cid:48)=1

n=1

N(cid:88)
N(cid:88)
N(cid:88)
(cid:32)(cid:88)

n=1

n

(cid:88)

(cid:33)−1 N(cid:88)

n=1

(17.2.14)

(17.2.15)

(17.2.16)

By diﬀerentiating the regularised training error and equating to zero, we ﬁnd the optimal w is given by

w =

φn(φn)T + R

ynφn

(17.2.17)

In practice it is common to use a regulariser that penalises the sum square length of the weights

λwTw = λ

w2
i

(17.2.18)

i

which corresponds to setting R = λI. Regularising pararameters such as λ, γ may be determined using a
validation set, section(13.2.3).

A popular LPM is given by the non-linear function φ(x) with components

17.2.3 Radial basis functions

(cid:18)

1
2α2

−

(cid:0)x − mi(cid:1)2(cid:19)

φi(x) = exp

(17.2.19)

These basis functions are bump shaped, with the center of the bump i being given by mi and the width
by α. An example is given in ﬁg(17.5) in which several RBFs are plotted with diﬀerent centres. In LPM
regression, we can then use a linear combination of these ‘bumps’ to ﬁt the data.

One can apply the same approach using vector inputs. For vector x and centre m, the radial basis function
depends on the distance between x and the centre m, giving a ‘bump’ in input space, ﬁg(17.8).

Example 79 (Setting α). Consider ﬁtting the data in ﬁg(17.6) using 16 radial basis functions uniformly
spread over the input space, with width parameter α and regularising term λwTw. The generalisation
performance on the test data depends heavily on the width and regularising parameter λ. In order to ﬁnd
reasonable values for these parameters we may use a validation set. For simplicity we set the regularisation
parameter to λ = 0.0001 and use the validation set to determine a suitable α. In ﬁg(17.7) we plot the
validation error as a function of α. Based on this graph, we can ﬁnd the best value of α; that which
minimises the validation error. The predictions are also given in ﬁg(17.6).

DRAFT March 9, 2010

315

00.20.40.60.8100.10.20.30.40.50.60.70.80.91xThe Dual Representation and Kernels

Figure 17.6: The × are the training points, and the +
are the validation points. The solid line is the correct
underlying function sin(10x) which is corrupted with
a small amount of additive noise to form the train
data. The dashed line is the best predictor based on
the validation set.

A curse of dimensionality

If the data has non-trivial behaviour over some region in x, then we need to cover the region of x space
fairly densely with ‘bump’ type functions.
In the above case, we used 16 basis functions for this one
dimensional space. In 2 dimensions if we wish to cover each dimension to the same discretisation level, we
would need 162 = 256 basis functions. Similarly, for 10 dimensions we would need 1610 ≈ 1012 functions.
To ﬁt such an LPM would require solving a linear system in more than 1012 variables. This explosion in
the number of basis functions with the input dimension is a ‘curse of dimensionality’.

A possible remedy is to make the basis functions very broad so that each covers more of the high dimen-
sional space. However, this will mean a lack of ﬂexibility of the ﬁtted function since it is constrained to be
smooth. Another approach is to place basis functions centred on the training input points and add some
more basis functions randomly placed close to the training inputs. The rational behind this is that when
we come to do prediction, we will most likely see novel x that are close to the training points – we do
not need to make ‘accurate’ predictions over all the space. A further approach is to make the positions of
the basis functions adaptive, allowing them to be moved around in the space to minimise the error. This
approach motivates the neural network models[41]. An alternative is to reexpress the problem of ﬁtting
an LPM by reparameterising the problem, as discussed below.

17.3 The Dual Representation and Kernels
Consider a set of training data with inputs, X = {xn, n = 1, . . . , N} and corresponding outputs yn, n =
1, . . . , N. For an LPM of the form

f(x) = wTx

(17.3.1)

our interest is to ﬁnd the ‘best ﬁt’ parameters w. We assume that we have found an optimal parameter
w∗. The nullspace of X are those x⊥ which are orthogonal to all the inputs in X . That is,

(17.3.2)

(cid:16)

(cid:16)

xn = 0,

x⊥(cid:17)T
w∗ + x⊥(cid:17)T

for all n. If we then consider the vector w∗ with an additional component in the direction orthogonal to
the space spanned by X ,

xn = wT∗ xn

(17.3.3)

Figure 17.7: The validation error as a function of the basis
function width for the validation data in ﬁg(17.6) and RBFs in
ﬁg(17.5). Based on the validation error, the optimal setting of
the basis function width parameter is α = 0.25.

316

DRAFT March 9, 2010

00.10.20.30.40.50.60.70.80.91−1.5−1−0.500.511.500.20.40.60.81012345678alphavalidation errorThe Dual Representation and Kernels

(a)

(b)

(cid:0)x − m1(cid:1)2

(a): The output of an RBF
Figure 17.8:
/α2). Here m1 =
function exp(− 1
(b): The combined
(0, 0.3)T and α = 0.25.
output for two RBFs with m1 as above and
m2 = (0.5,−0.5)T .

2

This means that adding a contribution to w∗ outside of the space spanned by X , has no eﬀect on the
predictions on the train data. If the training criterion depends only on how well the LPM predicts the
train data, there is therefore no need to consider contributions to w from outside of X . That is, without
loss of generality we may consider the representation

w =

anxn

(17.3.4)

The parameters a = (a1, . . . , aN ) are called the dual parameters. We can then write the output of the
LPM directly in terms of the dual parameters,

wTxn =

am (xm)T xn

(17.3.5)

More generally, for a vector function φ(x), the solution will lie in the space spanned by φ(x1), . . . , φ(xN ),

N(cid:88)

n=1

N(cid:88)

N(cid:88)

m=1

w =

anφ (xn)

n=1

and we may write

wTφ(xn) =

N(cid:88)

m=1

amφ (xm)T φ (xn) =

N(cid:88)

m=1

amK (xm, xn)

where we have deﬁned a kernel function

K (xm, xn) ≡ φ (xm)T φ (xn) ≡ [K]m,n

In matrix form, the output of the LPM on a training input x is then

wTφ(xn) = [Ka]n = aTkn

where kn is the nth column of the Gram matrix K.

17.3.1 Regression in the dual-space

For ordinary least squares regression, using equation (17.3.9), we have a train error

N(cid:88)

n=1

(cid:16)

yn − aTkn(cid:17)2

E(a) =

Equation(17.3.10) is analogous to the standard regression equation (17.2.3) on interchanging a for w and
kn for φ(xn). Similarly, the regularisation term can be expressed as

N(cid:88)

n,m=1

DRAFT March 9, 2010

wTw =

anamφ (xn) φ (xm) = aTKa

(17.3.6)

(17.3.7)

(17.3.8)

(17.3.9)

(17.3.10)

(17.3.11)

317

−1−0.500.51−1−0.8−0.6−0.4−0.200.20.40.60.8100.51x(1)x(2)−1−0.500.51−1−0.500.5100.511.5x(1)x(2)The Dual Representation and Kernels

By direct analogy the optimal solution for a is therefore

a =

kn (kn)T + λK

ynkn

We can express the above solution more conveniently by ﬁrst writing

a =

K−1kn (kn)T + λI

ynK−1kn

(cid:32) N(cid:88)
(cid:32) N(cid:88)

n=1

(cid:33)−1 N(cid:88)
(cid:33)−1 N(cid:88)

n=1

(17.3.12)

(17.3.13)

n=1

n=1

Since kn is the nth column of K then K−1kn is the nth column of the identity matrix. With a little
thought, we can rewrite equation (17.3.13) more simply as

a = (K + λI)−1 y

where y is the vector with components formed from the training inputs y1, . . . , yN .
Using this, the prediction for a new input x∗ is given by

y(x∗) = kT∗ (K + λI)−1 y

where the vector k∗ has components

(17.3.14)

(17.3.15)

[k∗]m = K (x∗

, xm)

(17.3.16)
This dual space solution shows that predictions can be expressed purely in terms of the kernel K (x, x(cid:48)).
This means that we may dispense with deﬁning the vector functions φ(x) and deﬁne a kernel function di-
rectly. This approach is also used in Gaussian Processes, chapter(19) and enables us to use eﬀectively very
large (even inﬁnite) dimensional vectors φ without ever explicitly needing to compute them. Note that the
Gram matrix K has dimension N × N, which means that the computational complexity of performing the
be prohibitively expensive, and numerical approximations are required. This is in contrast to the compu-
tational complexity of solving the normal equations in the original weight space viewpoint is O
.

matrix inversion in equation (17.3.16) is O(cid:0)N 3(cid:1). For moderate to large N (greater than 5000), this will
dim (φ)3(cid:17)

(cid:16)

To an extent, the dual parameterisation helps us with the curse of dimensionality since the complexity of
learning in the dual parameterisation scales cubically with the number of training points – not cubically
with the dimension of the φ vector.

17.3.2 Positive deﬁnite kernels (covariance functions)
The kernel K (x, x(cid:48)) in (17.3.8) was deﬁned as the scalar product between two vectors φ(x) and φ(x(cid:48)).
For any set of points x1, . . . , xM , the resulting matrix

[K]m,n = φ(xm)Tφ(xn) =(cid:88)
(cid:32)(cid:88)
zmKmnzn =(cid:88)
zTKz =(cid:88)

i

φm
i φn
i

is positive semi-deﬁnite since for any z

(cid:33)(cid:32)(cid:88)

(cid:33)

(cid:32)(cid:88)

=(cid:88)

znφn
i

zmφm
i

(17.3.17)

(cid:33)2

zmφm
i

≥ 0

(17.3.18)

m,n

i

m

n

i

m

Instead of specifying high-dimensional φ(x) vectors, we may instead specify a function K(x, x(cid:48)) that
produces a positive deﬁnite matrix K for any inputs x, x(cid:48). Such a function is called a covariance function,
or a positive kernel. For example a popular choice is

−λ|x−x(cid:48)|ν
e

, 0 < ν ≤ 2, λ ≥ 0

(17.3.19)

For ν = 2 this is commonly called the squared exponential kernel. For ν = 1 this is known as the
Ornstein-Uhlenbeck kernel. Covariance functions are discussed in more detail in section(19.3).

318

DRAFT March 9, 2010

Linear Parameter Models for Classiﬁcation

Figure 17.9: The logistic sigmoid function σβ(x) = 1/(1+e−βx).
The parameter β determines the steepness of the sigmoid. The
full (blue) line is for β = 1 and the dashed (red) for β = 10.
As β → ∞, the logistic sigmoid tends to a Heaviside step func-
tion. The dotted curve (magenta) is the error function (probit)
0.5 (1 + erf(λx)) for λ = √π/4, which closely matches the stan-
dard logistic sigmoid with β = 1.

17.4 Linear Parameter Models for Classiﬁcation
In a binary classiﬁcation problem we are given some training data, D = {(xn, cn), n = 1 . . . , N}, where
the targets c ∈ {0, 1}. Inspired by the LPM regression model, we can assign the probability that a novel
input x belongs to class 1 using

p(c = 1|x) = f(xTw)

(17.4.1)

where 0 ≤ f(x) ≤ 1. In the statistics literature, f(x) is termed a mean function – the inverse function
f−1(x) is the link function.

Two popular choices for the function f(x) are the and probit functions. The logit is given by

f(x) = ex

1 + ex =

1

1 + e−x

(17.4.2)

which is also called the logistic sigmoid and written σ(x), ﬁg(17.9). The scaled version is deﬁned as

σβ(x) = σ(βx)

(17.4.3)

A closely related model is probit regression which uses in place of the logistic sigmoid the error function
the cumulative distribution of the standard normal distribution

f(x) =

1
√2π

− 1

2 t2

e

dt =

1
2

(1 + erf(x))

−∞

This can also be written in terms of the standard error function,

(cid:90) x
(cid:90) x

0

erf(x) ≡

2
√π

−t2
e

dt

(17.4.4)

(17.4.5)

The shape of the probit and logistic functions are similar under rescaling, see ﬁg(17.9). We focus below
on the logit function. Similar derivations carry over in a straightforward manner to any monotonic mean
function.

17.4.1 Logistic regression

Logistic regression corresponds to the model

p(c = 1|x) = σ(b + xTw)

(17.4.6)

where b is a scalar, and w is a vector. As the argument b + xTw of the sigmoid function increases, the
probability x belongs to class 1 increases.

The decision boundary
The decision boundary is deﬁned as that set of x for which p(c = 1|x) = p(c = 0|x) = 0.5. This is given
by the hyperplane

b + xTw = 0

DRAFT March 9, 2010

(17.4.7)

319

−5−4−3−2−101234500.10.20.30.40.50.60.70.80.91Linear Parameter Models for Classiﬁcation

w

Figure 17.10: The decision boundary p(c = 1|x) = 0.5 (solid
line). For two dimensional data, the decision boundary is a line.
If all the training data for class 1 (ﬁlled circles) lie on one side
of the line, and for class 0 (open circles) on the other, the data
is said to be linearly separable.

On the side of the hyperplane for which b + xTw > 0, inputs x are classiﬁed as 1’s, and on the other
side they are classiﬁed as 0’s. The ‘bias’ parameter b simply shifts the decision boundary by a constant
amount. The orientation of the decision boundary is determined by w, the normal to the hyperplane, see
ﬁg(17.10).

To clarify the geometric interpretation, let x be a point on the decision boundary and consider a new point
x∗ = x + w⊥, where w⊥ is a vector perpendicular to w, so that wTw⊥ = 0. Then

b + wTx∗ = b + wT(cid:16)

x + w⊥(cid:17)

= b + wTx + wTw⊥ = b + wTx = 0

(17.4.8)

Thus if x is on the decision boundary, so is x plus any vector perpendicular to w. In D dimensions, the
space of vectors that are perpendicular to w occupy a D − 1 dimensional hyperplane. For example, if
the data is two dimensional, the decision boundary is a one dimensional hyperplane, a line, as depicted in
ﬁg(17.10).

Linear separability and linear independence

Deﬁnition 90 (Linear separability). If all the training data for class 1 lies on one side of a hyperplane,
and for class 0 on the other, the data is said to be linearly separable.

For D dimensional data, provided there are no more than D training points, then these are linearly
separable provided they are linearly independent. To see this, let cn = +1 if xn is in class 1, and cn = −1
if xn is in class 0. For the data to be linearly separable we require

wTxn + b = cn,

n = 1, . . . , N

(17.4.9)

where  is an arbitrarily small positive constant. The above equations state that each input is just the
correct side of the decision boundary. If there are N = D datapoints, the above can be written in matrix
form as

Xw + b = c

(17.4.10)

where X is a square matrix whose nth column contains xn. Provided that X is invertible the solution is

w = X−1 (c − b)

(17.4.11)

The bias b can be set arbitrarily. This shows that provided the xn are linearly independent, we can always
ﬁnd a hyperplane that linearly separates the data. Provided the data are not-collinear (all occupying the
same D − 1 dimensional subspace) the bias can be used to improve this to enabling D + 1 arbitrarily
labelled points to be linearly separated in D dimensions.

A dataset that is not linearly separable is given by the following four training points and class labels

{([0, 0], 0), ([0, 1], 1), ([1, 0], 1), ([1, 1], 0)}

320

(17.4.12)

DRAFT March 9, 2010

Linear Parameter Models for Classiﬁcation

Figure 17.11: The XOR problem. This is not linearly separable.

This data represents the XOR function, and is plotted in ﬁg(17.11). This function is not linearly separable
since no straight line has all inputs from one class on one side and the other class on the other.

Classifying data which is not linearly separable can only be achieved using a non-linear decision boundary.
It might be that data is non-linearly separable in the original data space. However, by mapping to a
higher dimension using a non-linear vector function, we generate a set of non-linearly dependent high-
dimensional vectors, which can then be separated using a high-dimensional hyperplane. We will discuss
this in section(17.5).

The perceptron
The perceptron assigns x to class 1 if b + wTx ≥ 0, and to class 0 otherwise. That is

p(c = 1|x) = θ(b + xTw)

where the step function is deﬁned as

θ(x) =

If we consider the logistic regression model

(cid:17)

p(c = 1|x) = σβ

b + xTw

(cid:26) 1 x > 0
0 x ≤ 0
(cid:16)
 1

and take the limit β → ∞, we have the perceptron like classiﬁer

p(c = 1|x) =

b + xTw > 0
0.5 b + xTw = 0
b + xTw < 0
0

(17.4.13)

(17.4.14)

(17.4.15)

(17.4.16)

The only diﬀerence between this ‘probabilistic perceptron’ and the standard perceptron is in the technical
deﬁnition of the value of the step function at 0. The perceptron may therefore essentially be viewed as a
limiting case of logistic regression.

17.4.2 Maximum likelihood training
Given a data set D, how can we learn the weights to obtain good classiﬁcation? Probabilistically, if we
assume that each data point has been drawn independently from the same distribution that generates
the data (the standard i.i.d. assumption), the likelihood of the observed data is (writing explicitly the
conditional dependence on the parameters b, w)

p(D|b, w) =

p(cn|xn, b, w)p(xn) =

p(c = 1|xn, b, w)cn (1 − p(c = 1|xn, b, w))1−cn

p(xn) (17.4.17)

where we have used the fact that cn ∈ {0, 1}. For this discriminative model, we do not model the input
distribution p(x) so that we may equivalently consider the log likelihood of the output class variables
conditioned on the training inputs: For logistic regression this gives

N(cid:89)

n=1

N(cid:89)

n=1

N(cid:88)

n=1

L(w, b) =

cn log σ(b + wTxn) + (1 − cn) log

DRAFT March 9, 2010

(17.4.18)

321

(cid:16)

(cid:17)
1 − σ(b + wTxn)

Gradient ascent

Linear Parameter Models for Classiﬁcation

There is no closed form solution to the maximisation of L(w, b) which needs to be carried out numerically.
One of the simplest methods is gradient ascent for which the gradient is given by

N(cid:88)

n=1

N(cid:88)

n=1

∇wL =

(cn − σ(wTxn + b))xn

Here we made use of the derivative relation

dσ(x)/dx = σ(x)(1 − σ(x))

dL
db

=

(cn − σ(wTxn + b))

for the logistic sigmoid. The derivative with respect to the bias is

The gradient ascent procedure then corresponds to updating the weights and bias using

wnew = w + η∇wL,

bnew = b + η

dL
db

where η, the learning rate is a scalar chosen small enough to ensure convergence. The application of the
above rule will lead to a gradual increase in the log likelihood.

Batch training

Writing the updates (17.4.22) explicitly gives

wnew = w + η

(cn − σ(wTxn + b))xn,

N(cid:88)

n=1

N(cid:88)

bnew = b + η

(cn − σ(wTxn + b))

n=1

(17.4.23)

This is called a batch update since the parameters w and b are updated only after passing through the
whole (batch) of training data. This batch version ‘converges’ in all cases since the error surface is bowl
shaped (see below). For linearly separable data, however, the optimal setting is for the weights to become
inﬁnitely large, since then the logistic sigmoids will saturate to 1 or 0 (see below). A stopping criterion
based on either minimal changes to the log likelihood or the parameters is therefore required to halt the
optimisation routine. For non-linearly separable data, the likelihood has a maximum at ﬁnite w so the
algorithm converges. However, the predictions will be less certain, reﬂected in a broad conﬁdence interval
– see ﬁg(17.12).

In batch training, the zero gradient criterion is

(cn − σ(wTxn + b))xn = 0

n=1

ment that for a zero gradient, cn = σ(cid:0)wTxn + b(cid:1), meaning that the weights must tend to inﬁnity for this

In the case that the inputs xn, n = 1, . . . , N are linearly independent, we immediately have the require-

(17.4.24)

condition to hold.

For linearly separable data, we can also show that the weights must become inﬁnite at convergence. Taking
the scalar product of equation (17.4.24) with w, we have the zero gradient requirement

N(cid:88)

N(cid:88)

(17.4.19)

(17.4.20)

(17.4.21)

(17.4.22)

n=1

(cn − σn) wTxn = 0

where σn ≡ σ(cid:0)wTxn + b(cid:1). For simplicity we assume b = 0. For linearly separable data we have

(cid:26) > 0 c = 1

< 0 c = 0

wTxn

322

(17.4.25)

(17.4.26)

DRAFT March 9, 2010

Linear Parameter Models for Classiﬁcation

(a)

(b)

(cid:26)

Then, using the fact that 0 ≤ σn ≤ 1, we have

(cn − σn) wTxn

≥ 0 c = 1
≥ 0 c = 0

Figure 17.12: The decision boundary p(c = 1|x) =
0.5 (solid line) and conﬁdence boundaries p(c =
1|x) = 0.9 and p(c = 1|x) = 0.1 and 10000 itera-
(a):
tions of batch gradient ascent with η = 0.1.
(b): Non-linearly separa-
Linearly separable data.
ble data. Note how the conﬁdence interval remains
broad, see demoLogReg.m.

(17.4.27)

Each term (cn − σn) wTxn is non-negative and the zero gradient condition requires the sum of these terms
to be zero. This can only happen if all the terms are zero, implying that cn = σn, requiring the sigmoid
to saturate, for which the weights must be inﬁnite.

Online training

In practice it is common to update the parameters after each training example has been considered:

wnew = w + η
N

(cn − σ(wTxn + b))xn,

bnew = b + η
N

(cn − σ(wTxn + b))

(17.4.28)

An advantage of online training is that the dataset does not need to be stored since only the performance
on the current input is required. Provided that the data is linearly separable, the above online procedure
converges (provided η is not too large). However, if the data is not linearly separable, the online version
will not converge since the opposing class labels will continually pull the weights one way and then the
other as each conﬂicting example is used to form an update. For the limiting case of the perceptron
(replacing σ(x) with θ(x)) and linearly separable data, online updating converges in a ﬁnite number of
steps[212, 41], but does not converge for non-linearly separable data.

Geometry of the error surface

The Hessian of the log likelihood L(w) is the matrix with elements2

(cid:88)

n

= −
(cid:88)

i,j,n

(cid:88)

ij

Hij ≡

∂2L
∂wiwj

xn
i xn

j σn(1 − σn)

This is negative (semi) deﬁnite since for any z

ziHijzj = −

zixn

i zjxn

j σn(1 − σn) ≤ −

2

zixn
i

(cid:88)

i,n

≤ 0

(17.4.29)

(17.4.30)

This means that the error surface is concave (an upside down bowl) and gradient ascent is guaranteed to
converge to the optimal solution, provided the learning rate η is small enough.

Example 80 (Classifying Handwritten Digits). We apply logistic regression to the 600 handwritten digits
of example(67), in which there are 300 ones and 300 sevens in the train data. Using gradient ascent
training with a suitably chosen stopping criterion, the number of errors made on the 600 test points is
12, compared with 14 errors using Nearest Neighbour methods. See ﬁg(17.13) for a visualisation of the
learned w.

2For simplicity we ignore the bias b. This can readily be dealt with by extending x to a D + 1 dimensional vector ˆx with

a 1 in the D + 1 component. Then for a D + 1 dimensional ˆw = (w, wD+1), we have ˆwT ˆx = wTx + wD+1.

DRAFT March 9, 2010

323

00.20.40.60.8100.10.20.30.40.50.60.70.80.9100.20.40.60.8100.10.20.30.40.50.60.70.80.91The Kernel Trick for Classiﬁcation

Figure 17.13: Logistic regression for classifying hand written digits 1 and 7.
Displayed is a Hinton diagram of the 784 learned weight vector w, plotted
as a 28 × 28 image for visual interpretation. Green are positive and an
input x with a (positive) value in this component will tend to increase the
probability that the input is classed as a 7. Similarly, inputs with positive
contributions in the red regions tend to increase the probability as being
classed as a 1 digit. Note that the elements of each input x are either
positive or zero.

17.4.3 Beyond ﬁrst order gradient ascent

Since the surface has a single optimum, a Newton update

wnew = wold + ηH−1wold

(17.4.31)

where H is the Hessian matrix as above and 0 < η < 1, will typically converge much faster than gradient
ascent. For large scale problems, the inversion of the Hessian is computationally demanding and limited
memory BFGS or conjugate gradient methods may be considered as more practical alternatives, see
section(A.5).

17.4.4 Avoiding overconﬁdent classiﬁcation

Provided the data is linearly separable the weights will continue to increase and the classiﬁcations will
become extreme. This is undesirable since the classiﬁcations will be over-conﬁdent. This can be prevented
by adding a penalty term to the objective function

L

(cid:48)(w, b) = L(w, b) − αwTw.

(17.4.32)

The scalar constant α > 0 encourages smaller values of w (remember that we wish to maximise the log
likelihood). An appropriate value for α can be determined using validation data.

17.4.5 Multiple classes

For more than two classes, one may use the softmax function

p(c = i|x) =

(cid:80)C

ewT
i x+bi
j=1 ewT

j x+bj

(17.4.33)

where C is the number of classes. When C = 2 this reduced to the logistic sigmoid. One can show that
the likelihood for this case is also concave, see exercise(176) and [297].

17.5 The Kernel Trick for Classiﬁcation

A drawback of logistic regression as described above is the simplicity of the decision surface – a hyperplane.
One way to extend the method to more complex non-linear decision boundaries is to consider mapping
the inputs x in a non-linear way to φ(x):

(cid:16)

(cid:17)

p(c = 1|x) = σ

wTφ(x) + b

For example, the one-dimensional input x could get mapped to a two dimensional vector (x2, sin(x)). Map-
ping into a higher dimensional space makes it easier to ﬁnd a separating hyperplane since any set of points
that are linearly independent can be linearly separated provided we have as many dimensions as datapoints.

324

DRAFT March 9, 2010

(17.5.1)

Support Vector Machines

Figure 17.14: Logistic regression p(c = 1|x) = σ(wTφ(x)) us-
ing a quadratic function φ(x) = (1, x1, x2, x2
2, x1x2)T. 1000
iterations of gradient ascent training were performed with a
learning rate η = 0.1. Plotted are the datapoints for the two
classes (red cross) and (blue circle) and the equal probability
contours. The decision boundary is the 0.5-probability contour.
See demoLogRegNonLinear.m.

1, x2

For the Maximum Likelihood criterion, we can use exactly the same algorithm as before on replacing x
with φ(x). See ﬁg(17.14) for a demonstration using a quadratic function.

Since only the scalar product between the φ vectors plays a role the dual representation may be used in
which we assume the weight can be expressed in the form

αnφn

(17.5.2)

w =(cid:88)

n

We then subsequently ﬁnd a solution in terms of the dual parameters αn. This is potentially advantageous
since there may be less training points than dimensions of φ. The classiﬁer depends only on scalar products
which can be written in terms of a positive deﬁnite kernel,

(cid:33)

(cid:32)(cid:88)
(cid:16)

n

(cid:17)

p(c = 1|x) = σ

anK(x, xn)

For convenience, we can write the above as

p(c = 1|x) = σ

aTk(x)

(17.5.3)

(17.5.4)

where the N dimensional vector k(x) has elements [k(x)]n = K(x, xn). Then the above is of exactly the
same form as the original speciﬁcation of logistic regression, namely as a function of a linear combination
of vectors. Hence the same training algorithm to maximise the likelihood can be employed, simply on
replacing xn with k(xn). The details are left to the interested reader, and follow closely the treatment of
Gaussian Processes for classiﬁcation, section(19.5).

17.6 Support Vector Machines

Like kernel logistic regression, SVMs are a form of kernel linear classiﬁer. However, the SVM uses an
objective which more explicitly encourages good generalisation performance. SVMs do not ﬁt comfortably
within a probabilistic framework and as such we describe them here only brieﬂy, referring the reader to
the wealth of excellent literature on this topic3. The description here is inspired largely by [71].

17.6.1 Maximum margin linear classiﬁer
In the SVM literature it is common to use +1 and −1 to denote the two classes. For a hyperplane deﬁned
by weight w and bias b, a linear discriminant is given by

wTx + b ≥ 0
wTx + b < 0

class +1
class -1

3http://www.support-vector.net

DRAFT March 9, 2010

(17.6.1)
(17.6.2)

325

0.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.40.40.40.40.40.40.50.50.50.50.50.60.60.60.60.60.70.70.70.70.70.80.80.80.80.80.90.90.90.90.9−0.500.511.5−0.500.511.5Support Vector Machines

Figure 17.15: SVM classiﬁcation of data from two
classes (open circles and ﬁlled circles). The decision
boundary wTx + b = 0 (solid line). For linearly
separable data the maximum margin hyperplane is
equidistant from the closest opposite class points.
These support vectors are highlighted in blue and the
margin in red. The distance of the decision boundary

from the origin is −b/√wTw, and the distance of a
general point x from the origin along the direction w
is xTw/√wTw.

w

origin

For a point x that is close to the decision boundary at wTx + b = 0, a small change in x can lead to a
change in classiﬁcation. To make the classiﬁer more robust we therefore impose that for the training data
at least, the decision boundary should be separated from the data by some ﬁnite margin (assuming in the
ﬁrst instance that the data is linearly separable):

wTx + b ≥ 2
wTx + b < −2

class +1
class -1

(17.6.3)
(17.6.4)

Since w, b and 2 can all be rescaled arbitrary, we need to ﬁx the scale of the above to break this invariance.
It is convenient to set  = 1 so that a point x+ from class +1 that is closest to the decision boundary
satisﬁes

wTx+ + b = 1

and a point x− from class -1 that is closest to the decision boundary satisﬁes

wTx− + b = −1

(17.6.5)

(17.6.6)

From vector algebra, ﬁg(17.15), the distance from the origin along the direction w to a point x is given by

wTx
√wTw

(17.6.7)

The margin between the hyperplanes for the two classes is then the diﬀerence between the two distances
along the direction w which is

wT
√wTw

(x+ − x−) =

2

√wTw

(17.6.8)

To set the distance between the two hyperplanes to be maximal, we need to minimise the length wTw.
Given that for each xn we have a corresponding label yn ∈ {+1,−1}, in order to classify the training
labels correctly and maximise the margin, the optimisation problem is therefore equivalent to:

subject to yn(cid:16)

(cid:17)

minimise

1
2

wTw

yn(cid:16)

(cid:17)

This is a quadratic programming problem. Note that the factor 0.5 is just for convenience.

wTxn + b

≥ 1,

n = 1, . . . , N

(17.6.9)

To account for potentially mislabelled training points (or for data that is not linearly separable), we relax
the exact classiﬁcation constraint and use instead

wTxn + b

≥ 1 − ξn

(17.6.10)

where ξn ≥ 0. Here each ξn measures how far xn is from the correct margin. For 0 < ξn < 1 datapoint xn
is on the correct side of the decision boundary. However for ξn > 1, the datapoint is assigned the opposite
class to its training label. Ideally we want to limit the size of these ‘violations’ ξn. Here we brieﬂy describe
two standard approaches.

326

DRAFT March 9, 2010

Support Vector Machines

2-Norm soft-margin

ξn

w

origin

Figure 17.16: Slack Margin. The term ξn measures
how far a variable is from the wrong side of the mar-
gin for its class.
If ξn > 1 then the point will be
misclassiﬁed – treated as an outlier.

The 2-norm soft-margin objective is

minimise

1
2

wTw + C
2

(ξn)2

(cid:88)

n

subject to yn(cid:16)

(cid:17)

wTxn + b

≥ 1 − ξn, n = 1, . . . , N (17.6.11)

where C controls the number of mislabellings of the training data. The constant C needs to be determined
empirically using a validation set. The optimisation problem expressed by (17.6.11) can be formulated
using the Lagrangian

(cid:88)

L(w, b, ξ, α) =

− 1 + ξn(cid:105)
For points xn on the ‘correct’ side of the decision boundary yn(cid:0)wTxn + b(cid:1)

which is to be minimised with respect to x, b, ξ and maximised with respect to α.

wTw + C
2

yn(cid:16)

αn(cid:104)

(ξn)2−

(cid:88)

wTxn + b

(cid:17)

n

n

1
2

−1+ ξn > 0 so that maximising
L with respect to α requires the corresponding αn to be set to zero. Only training points that are support
vectors lying on the decision boundary will have non-zero αn.

,

αn ≥ 0, ξn ≥ 0 (17.6.12)

Diﬀerentiating the Lagrangian and equating to zero, we have the conditions

(cid:88)
(cid:88)
L(w, b, ξ, α) = wi −

L(w, b, ξ, α) = −

n

n

∂
∂wi

∂
∂b

αnynxn

i = 0

αnyn = 0

∂
∂ξn L(w, b, ξ, α) = Cξn − αn = 0

From this we see that the solution for w is given by

w =(cid:88)

n

αnynxn

(17.6.13)

(17.6.14)

(17.6.15)

(17.6.16)

Since only the support vectors have non-zero αn, the solution for w will typically depend on only a small
number of the training data. Using these conditions and substituting back into the original problem, the
objective is equivalent to minimising

n

(17.6.17)

(17.6.18)

327

(cid:88)
L(α) =(cid:88)
subject to (cid:88)

αn −

1
2

n,m

n

1
ynymαnαm (xn)T xm −
2C
αn ≥ 0
ynαn = 0,

n

(cid:88)

(αn)2

If we deﬁne

K(xn, xm) = (xn)T xm

DRAFT March 9, 2010

Support Vector Machines

Figure 17.17: SVM training. The solid red and solid blue circles represent
training data from diﬀerent classes. The support vectors are highlighted in
green. For the unﬁlled test points, the class assigned to them by the SVM is
given by the colour. See demoSVM.m

The optimisation problem is

maximize (cid:88)
subject to (cid:88)

n

(cid:88)

1
2

αn −
n,m
ynαn = 0,

n

(cid:18)

K(xn, xm) +

(cid:19)

1
C

δn,m

ynymαnαm

αn ≥ 0

Optimising this objective is discussed in section(17.6.3).

1-norm soft-margin (box constraint)

In the 1-norm soft-margin version, one uses a 1-norm penalty

(cid:88)

n

C

ξn

(17.6.19)

(17.6.20)

to give the optimisation problem:

minimise

1
2

wTw+C

subject to yn(cid:16)

(cid:17)

wTxn + b

≥ 1−ξn, ξn ≥ 0,

n = 1, . . . , N (17.6.21)

where C is an empirically determined penalty factor that controls the number of mislabellings of the
training data. To reformulate the optimisation problem we use the Lagrangian

L(w, b, ξ) =

1
2

wTw+C

(cid:88)

αn(cid:104)
yn(cid:16)

ξn−

n

(cid:17)

− 1 + ξn(cid:105)

(cid:88)

−

n

wTxn + b

(cid:88)

n

ξn

(cid:88)

n

rnξn,

αn ≥ 0, ξn ≥ 0, rn ≥ 0
(17.6.22)

The variables rn are introduced in order to give a non-trivial solution (otherwise αn = C). Following a
similar argument as for the 2-norm case, by diﬀerentiating the Lagrangian and equating to zero, we arrive
at the optimisation problem

maximize (cid:88)
subject to (cid:88)

n

(cid:88)

1
2

αn −
n,m
ynαn = 0,

n

ynymαnαmK(xn, xm)

(17.6.23)

0 ≤ αn ≤ C

which is closely related to the 2-norm problem except that we now have the box-constraint 0 ≤ αn ≤ C.
17.6.2 Using kernels
The ﬁnal objectives (17.6.19) and (17.6.17) depend on the inputs xn only via the scalar product (xn)T xn.
If we map x to a vector function of x, then we can write

K(xn, xm) = φ(xn)Tφ(xm)

(17.6.24)

This means that we can use any positive (semi) deﬁnite kernel K and make a non-linear classiﬁer. See
section(19.3).

328

DRAFT March 9, 2010

−2−1012−2.5−2−1.5−1−0.500.511.52Soft Zero-One Loss for Outlier Robustness

17.6.3 Performing the optimisation

Both of the above soft-margin SVM optimisations problems (17.6.19) and (17.6.17) are quadratic programs

for which the exact computational cost scales as O(cid:0)N 3(cid:1). Whilst these can be solved with general purpose
mal Optimisation algorithm[224], whose practical performance is typically O(cid:0)N 2(cid:1) or better. A variant of

routines, speciﬁcally tailored routines that exploit the structure of the problem are preferred in practice.
Of particular practical interest are ‘chunking’ techniques that optimise over a subset of the α. In the limit
of updating only two components of α, this can be achieved analytically, resulting in the Sequential Mini-

this algorithm [92] is provided in SVMtrain.m.

Once the optimal solution α∗ is found the decision function for a new point x is

αn∗ ynK(xn, x) + b∗

assign to class 1
assign to class -1

The optimal b∗ is determined using the maximum margin condition, equations(17.6.5,17.6.6):

(cid:88)

n

(cid:20)

(cid:26) > 0

< 0

(cid:21)

b∗ =

1
2

min
yn=1

wT∗ xn − max
yn=−1

wT∗ xn

(17.6.25)

(17.6.26)

17.6.4 Probabilistic interpretation

Kernelised logistic-regression has some of the characteristics of the SVM but does not express the large
margin requirement. Also the sparse data usage of the SVM is similar to that of the Relevance Vector
Machine we discuss in section(18.2.4). However, a probabilistic model whose MAP assignment matches
exactly the SVM is hampered by the normalisation requirement for a probability distribution. Whilst,
arguably, no fully satisfactory direct match between the SVM and a related probabilistic model has been
achieved, approximate matches have been obtained[255].

17.7 Soft Zero-One Loss for Outlier Robustness

Both the support vector machine and logistic regression are potentially mislead by outliers. For the SVM,
a mislabelled datapoint that is far from the correct side of the decision boundary would require a large
slack ξ. However, since exactly such large ξ are discouraged, it is unlikely that the SVM would admit
such a solution. For logistic regression, the probability of generating a mislabelled point far from the cor-
rect side of the decision boundary is so exponentially small that this will never happen in practice. This
means that the model trained with Maximum Likelihood will never present such a solution. In both cases
therefore mislabelled points (or outliers) have a signiﬁcant impact on the location of the decision boundary.

A robust technique to deal with outliers is to use the zero-one loss in which a mislabeled point contributes
only a relatively small loss. Soft variants of this are obtained by using the objective

N(cid:88)

n=1

(cid:104)
σβ(b + wTxn) − cn(cid:105)2

+ λwTw

(17.7.1)

which is to be minimised with respect to w and b. For β → ∞ the ﬁrst term above tends to the zero-one
loss. The second term represents a penalty on the length of w and prevents overﬁtting. Kernel extensions
of this soft zero-one loss are straightforward.

Unfortunately, the objective (17.7.1) is highly non-convex and ﬁnding the optimal w, b is computationally
diﬃcult. A simple-minded scheme is to ﬁx all components of w except one, wi and then perform a nu-
merical one-dimensional optimisation over this single parameter wi. At the next step, another parameter
wj is chosen, and the procedure repeated until convergence. As usual, λ can be set using validation.
The practical diﬃculties of minimising non-convex high-dimensional objective functions means that these
approaches are rarely used in practice. A discussion of practical attempts in this area is given in [286].

DRAFT March 9, 2010

329

Exercises

Figure 17.18: Soft zero-one loss decision boundary (solid line) versus logis-
tic regression (dotted line). The number of mis-classiﬁed training points
using the soft zero-one loss is 2, compared to 3 for logistic regression. The
penalty λ = 0.01 was used for the soft-loss, with β = 10. For logistic re-
gression, no penalty term was used. The outliers have a signiﬁcant impact
on the decision boundary for logistic regression, whilst the soft zero-one
loss essentially gives up on the outliers and ﬁts a large margin classiﬁer
between the remaining points. See demoSoftLoss.m.

An illustration of the diﬀerence between logistic regression and this soft zero-one loss is given in ﬁg(17.18),
which demonstrates how logistic regression is inﬂuenced by the mass of the data points, whereas the
zero-one loss attempts to mimimise the number of mis-classiﬁcations whilst maintaining a large margin.

17.8 Notes

The perceptron has a long history in artiﬁcial intelligence and machine learning. Rosenblatt discussed the
perceptron as a model for human learning, arguing that its distributive nature (the input-output ‘patterns’
are stored in the weight vector) is closely related to the kind of information storage believed to be present
in biological systems[234]. To deal with non-linear decision boundaries, the main thrust of research in
the ensuing neural network community was on the use of multilayered structures in which the outputs
of perceptrons are used as the inputs to other perceptrons, resulting in potentially highly non-linear
discriminant functions. This line of research was largely inspired by analogies to biological information
processing in which layered structures are prevalent. Such multilayered artiﬁcial neural networks are
fascinating and, once trained, are extremely fast in forming their decisions. However, reliably training
these systems is a highly complex task and probabilistic generalisations in which priors are placed on the
parameters lead to computational diﬃculties. Whilst perhaps less inspiring from a biological viewpoint,
the alternative route of using the kernel trick to boost the power of a linear classiﬁer has the advantage
of ease of training and generalisation to probabilistic variants. More recently, however, there has been a
resurgence of interest in the multilayer systems, with new heuristics aimed at improving the diﬃculties in
training, see for example [135].

17.9 Code

demoCubicPoly.m: Demo of Fitting a Cubic Polynomial
demoLogReg.m: Demo Logistic Regression
LogReg.m: Logistic Regression Gradient Ascent Training
demoLogRegNonLinear.m: Demo of Logistic regression with a non-linear φ(x)
SVMtrain.m: SVM training using the SMO algorithm
demoSVM.m: SVM demo
demoSoftLoss.m: softloss demo
softloss.m: softloss function

17.10 Exercises

Exercise 174.

1. Give an example of a two-dimensional dataset for which the data are linearly separable, but not

linearly independent.

2. Can you ﬁnd a dataset which is linearly independent but not linearly separable?

330

DRAFT March 9, 2010

−4−202468−4−2024681012Exercises

the ﬁtted lines go through the point(cid:80)N

n=1(xn, yn)/N.

Exercise 175. Show that for both Ordinary and Orthogonal Least Squares regression ﬁts to data {(xn, yn), n = 1, . . . , N)}

Exercise 176. Consider the softmax function for classifying an input vector x into one of c = 1, . . . , C
classes using

c x(cid:80)C

ewT
c(cid:48)=1 ewT
c(cid:48) x

p(c|x) =

(17.10.1)

A set of input-class examples is given by D = {(xn, cn), n = 1, . . . , N}.

1. Write down the log-likelihood L of the classes conditional on the inputs, assuming that the data is

i.i.d.

2. Compute the Hessian with elements

Hij = ∂2L(D)
∂wiwj

where w is is the stacked vector

(cid:16)

(cid:17)T

w =

wT

1 , . . . , wT
C

(17.10.2)

(17.10.3)

and show that the Hessian is positive semi-deﬁnite, that is zTHz ≥ 0 for any z.

Exercise 177. Derive from equation (17.6.11) the dual optimisation problem equation (17.6.17).

Exercise 178. A datapoint x is projected to a lower dimensional vector ´x using

´x = Mx

(17.10.4)
where M is a fat matrix. For a set of data xn, n = 1, . . . , N and corresponding binary class labels yn ∈
{0, 1}, using logistic regression on the projected datapoints ´xn corresponds to a form of constrained logistic
regression in the original higher dimensional space x. Explain if it is reasonable to use an algorithm such
as PCA to ﬁrst reduce the data dimensionality before using logistic regression.

Exercise 179. The logistic sigmoid function is deﬁned as σ(x) = ex/(1+ex). What is the inverse function,
σ−1(x)?
Exercise 180. Given a dataset D = {(xn, cn), n = 1, . . . , N}, where cn ∈ {0, 1}, logistic regression uses
the model p(c = 1|x) = σ(wTx + b). Assuming the data is drawn independently and identically, show that
the derivative of the log likelihood L with respect to w is

N(cid:88)

(cid:16)

n=1

∇wL =

cn − σ

(cid:16)

(cid:17)(cid:17)

wTxn + b

xn

(17.10.5)

Exercise 181. Consider a dataset D = {(xn, cn), n = 1, . . . , N}, where cn ∈ {0, 1}, and x is a D dimen-
sional vector.

1. Show that if the training data is linearly separable with the hyperplane wTx + b, the data is also

separable with the hyperplane ˜wTx + ˜b, where ˜w = λw, ˜b = λb for any scalar λ > 0.

2. What consequence does the above result have for maximum likelihood training of linearly separable

data?

Exercise 182. Consider a dataset D = {(xn, cn), n = 1, . . . , N}, where cn ∈ {0, 1}, and x is a N dimen-
sional vector. (Hence we have N datapoints in a N dimensional space). In the text we showed that we can
ﬁnd a hyperplane (parameterised by (w, b)) that linearly separates this data we need, for each datapoint
xn, wTxn + b = n where n > 0 for cn = 1 and n < 0 for cn = 0. Comment on the relation between
maximum likelihood training and the algorithm suggested above.

DRAFT March 9, 2010

331

Exercise 183. Given training data D = {(xn, yn), n = 1, . . . , N}, you decide to ﬁt a regression model
y = mx + c to this data. Derive an expression for m and c in terms of D using the minimum sum squared
error criterion.
Exercise 184. Given training data D = {(xn, cn), n = 1, . . . , N}, cn ∈ {0, 1}, where x are vector inputs,
a discriminative model is

Exercises

p(c = 1|x) = σ(b0 + v1g(wT

(17.10.6)
where g(x) = exp(−0.5x2). and σ(x) = ex/(1 + ex) (this is a neural network[41] with a single hidden layer
and two hidden units).

1 x + b1) + v2g(wT

2 x + b2))

1. Write down the log likelihood for the class conditioned on the inputs, based on the usual i.i.d. as-

sumption.

2. Calculate the derivatives of the log likelihood as a function of the network parameters, w1, w2, b1, b2, v, b0

3. Comment on the relationship between this model and logistic regression.

4. Comment on the decision boundary of this model.

332

DRAFT March 9, 2010

CHAPTER 18

Bayesian Linear Models

18.1 Regression With Additive Gaussian Noise

The linear models in chapter(17) were trained under Maximum Likelihood and do not deal with the
issue that, from a probabilistic perspective, parameter estimates are inherently uncertain due to the
limited available training data. Regression refers to inferring a mapping on the basis of observed data
D = {(xn, yn), n = 1, . . . , N}, where (xn, yn) represents an input-output pair. We discuss here the scalar
output case (and vector inputs x) with the extension to the vector output case y is straightforward. We
assume that each (clean) output is generated from a model f (x; w) where the parameters w of the function
f are unknown. An output y is generated by the addition of noise η to the clean model output,

(cid:0)η 0, σ2(cid:1), the model generates an output y for input x with

(18.1.1)

y = f (x; w) + η

If the noise is Gaussian distributed, η ∼ N
probability

(cid:0)y f(x; w), σ2(cid:1) =

(18.1.4)

(18.1.5)

p(y|w, x) = N

1

√2πσ2

e

2σ2 [y−f (x;w)]2
− 1

(18.1.2)

If we assume that each data input-output pair is generated identically and independently, the likelihood
the model generates the data is

p(D|w) =

p(yn|w, xn)p(xn)

(18.1.3)

We may use a prior weight distribution p(w) to quantify our a priori belief in the suitability each parameter
setting. Writing D = {Dx,Dy}, the posterior weight distribution is then given by

p(w|D) ∝ p(D|w)p(w) ∝ p(Dy|w,Dx)p(w)

Using the Gaussian noise assumption, and for convenience deﬁning β = 1/σ2, this gives

log p(w|D) = −

β
2

[yn − f(xn; w)]2 + log p (w) + N
2

log β + const.

Note the similarity between equation (18.1.5) and the regularised training error equation (17.2.16). In
the probabilistic framework, we identify the choice of a sum square error with the assumption of additive
Gaussian noise. Similarly, the regularising term is identiﬁed with log p(w).

333

N(cid:89)

n=1

N(cid:88)

n=1

Regression With Additive Gaussian Noise

α

w

xn

yn

N

Figure 18.1: Belief Network representation of a Bayesian Model for
regression under the i.i.d. data assumption. The hyperparameter α
acts as a form of regulariser, controlling the ﬂexibility of the prior on
the weights w. The hyperparameter β controls the level of noise on
the observations.

β

18.1.1 Bayesian linear parameter models

Linear parameter models, as discussed in chapter(17), have the form

f(x; w) =

wiφi(x) ≡ wTφ(x)

(18.1.6)

where the parameters wi are also called ‘weights’ and dim w = B. Such models have a linear parameter
dependence, but may represent a non-linear input-output mapping if the basis functions φi(x) are non-
linear in x.

Since the output scales linearly with w, we can discourage extreme output values by penalising large
weight values. A natural weight prior is thus

B(cid:88)

i=1

(18.1.7)

(cid:0)w 0, α

p(w|α) = N

−1I(cid:1) =
(cid:104)
N(cid:88)

n=1

(cid:16) α

(cid:17) B

2π

− α

2 wTw

2 e

(cid:105)2
yn − wTφ(xn)

where the precision α is the inverse variance. If α is large, the total squared length of the weight vector
w is encouraged to be small. Under the Gaussian noise assumption, the posterior distribution is

log p(w|Γ,D) = −

β
2

α
2

−

wTw + const.

(18.1.8)

where Γ = {α, β} represents the hyperparameter set. Parameters that determine the functions φ may also
be included in the hyperparameter set.

Using the LPM in equation (18.1.5) with a Gaussian prior, equation (18.1.7), and completing the square,
section(8.6.2), the weight posterior is a Gaussian distribution,

p(w|Γ,D) = N (w m, S)

where the covariance and mean are given by

(cid:33)−1

S =

αI + β

φ (xn) φT (xn)

,

m = βS

(18.1.9)

N(cid:88)

n=1

ynφ (xn)

(18.1.10)

(cid:32)

(cid:90)

N(cid:88)

n=1

(cid:28)(cid:104)

The mean prediction for an input x is then given by

¯f(x) ≡

f(x; w)p(w|D, Γ)dw = mTφ (x) .

Similarly, the variance of the underlying estimated clean function is

(cid:105)2(cid:29)

var(f(x)) =

wTφ(x)

− ¯f(x)2 = φT(x)Sφ(x)

(18.1.11)

(18.1.12)

The output variance var(f(x)) depends only on the input variables and not on the training outputs y.
Since the additive noise η is uncorrelated with the model outputs, the predictive variance is

var(y(x)) = var(f(x)) + σ2

(18.1.13)

and represents the variance of the ‘noisy’ output for an input x.

334

DRAFT March 9, 2010

Regression With Additive Gaussian Noise

(a)

(b)

(c)

Figure 18.2: Along the horizontal axis we plot the input x and along the vertical axis the output t. (a):
(b): Prediction using regularised training and ﬁxed hyperparame-
The raw input-output training data.
(c): Prediction using ML-II optimised hyperparameters. Also plotted are standard error bars on
ters.

the clean underlying function,(cid:112)var(f(x)).

Example 81. In ﬁg(18.2b), we show the mean prediction on the data in ﬁg(18.2a) using 15 Gaussian
basis functions

φi(x) = exp(cid:0)

−0.5(x − ci)2/λ2(cid:1)

(18.1.14)

with width λ = 0.032 and centres ci spread out evenly over the 1-dimensional input space from −2 to 2.
We set the other hyperparameters by hand to β = 100 and α = 1. The prediction severely overﬁts the
data, a result of a poor choice of hyperparameter settings. This is resolved in ﬁg(18.2c) using the ML-II
parameters, as described below.

18.1.2 Determining hyperparameters: ML-II

The hyperparameter posterior distribution is

p(Γ|D) ∝ p(D|Γ)p(Γ)

(18.1.15)

A simple summarisation of the posterior is given by the MAP assignment which takes the single ‘optimal’
setting:

Γ∗ = argmax

Γ

p(Γ|D)

(18.1.16)

If the prior belief about the hyperparameters is weak (p(Γ) ≈ const.), this is equivalent to using the Γ that
maximises the marginal likelihood

p(D|Γ) =

p(D|Γ, w)p(w|Γ)dw

(18.1.17)

This approach to setting hyperparameters is called ‘ML-II’ [33] or the Evidence Procedure[181].

In the case of Bayesian Linear Parameter models under Gaussian additive noise computing the marginal
likelihood equation (18.1.17) involves only Gaussian integration. A direct approach to deriving an expres-
sion for the marginal likelihood is to consider

(cid:90)

(cid:18)

(cid:104)

(cid:105)2
yn − wTφ(xn)

β
2

−

(cid:19)

α
2

−

wTw

p(D|Γ, w)p(w) = exp

DRAFT March 9, 2010

(2πβ)N/2 (2πα)B/2

(18.1.18)

335

Regression With Additive Gaussian Noise

(cid:88)

where

d = β

By collating terms in w (completing the square, section(8.6.2)), the above represents a Gaussian in w
with additional factors. After integrating over this Gaussian we have

2 log p(D|Γ) = −β

(yn)2 + dTS−1d − log det (S) + B log α + N log β − N log (2π)

(18.1.19)

N(cid:88)

n=1

φ(xn)yn

(18.1.20)

n

See exercise(186) for an alternative expression.

Example 82. Using the hyperparameters α, β, λ that optimise expression (18.1.19) gives the results in
ﬁg(18.2c) where we plot both the mean predictions and standard predictive error bars. This demonstrates
that an acceptable setting for the hyperparameters can be obtained by maximising the marginal likelihood.

18.1.3 Learning the hyperparameters using EM

We can set hyperparameters such as α and β by maximising the marginal likelihood equation (18.1.17).
A convenient computational procedure to achieve this is to interpret the w as latent variables and apply
the EM algorithm, section(11.2). In this case the energy term is

According to the general EM procedure we need to maximise the energy term. For a hyperparameter Γ
the derivative of the energy is given by

(18.1.21)

(18.1.22)

(18.1.23)

(18.1.24)

(18.1.25)

(18.1.26)

For the Bayesian LPM with Gaussian weight and noise distributions, we obtain

p(w|D,Γold)

∂Γ

(cid:28) ∂

∂
∂Γ E ≡

E ≡ (cid:104)log p(D|w, Γ)p(w|Γ)(cid:105)p(w|D,Γold)
(cid:29)
log p(D|w, Γ)p(w|Γ)
N(cid:88)
N(cid:88)
(cid:104)

yn − mTφ(xn)
(cid:105)2
yn − mTφ(xn)

1
βnew =

(cid:28)(cid:104)
(cid:104)

(cid:105)2(cid:29)
yn − wTφ(xn)
(cid:105)2

= N

2β −

E = N

2β −

+ trace

1
2

1
2

n=1

n=1

∂
∂β

N(cid:88)

n=1

1
N

1
2

(cid:16)

(cid:17)

SˆS

Solving for the zero derivatives gives the M-step update

p(w|Γold,D)

trace

S

−

(cid:32)

(cid:33)

φ(xn)φT(xn)

N(cid:88)

n=1

where ˆS is the empirical covariance of the basis-function vectors φ(xn), n = 1, . . . , N.

Similarly, for α,
E = B

∂
∂α

2α −

(cid:69)

wTw

(cid:68)

1
2

(cid:16)

= B

2α −

1
2

(cid:17)

trace (S) + mTm

(cid:16)

p(w|Γold,D)

(cid:17)

which, on equating to zero, gives the update

1
αnew =

1
B

trace (S) + mTm

where S and m are given in equation (18.1.10). An alternative ﬁxed point procedure that is often more
rapidly convergent than EM is given in equation (18.1.36). Closed form updates for other hyperparameters,
such as the width of the basis functions, are generally not available, and the corresponding energy term
needs to be optimised numerically.

336

DRAFT March 9, 2010

Regression With Additive Gaussian Noise

Figure 18.3: Predictions for an RBF for diﬀerent widths λ. For each λ the ML-II optimal α, β are
obtained by running the EM procedure to convergence and subsequently used to form the predictions. In
each panel the dots represent the training points, with x along the horizontal axis and y along the vertical
axis. Mean predictions are plotted, along with predictive error bars of one standard deviation. According
to ML-II, the best model corresponds to λ = 0.37, see ﬁg(18.4). The smaller values of λ overﬁt the data,
giving rise to too ‘rough’ functions. The largest values of λ underﬁt, giving too ‘smooth’ functions. See
demoBayesLinReg.m.

18.1.4 Hyperparameter optimisation : using the gradient

Hyperparameters such as α can be set by maximising the marginal likelihood

(cid:90)

p(D|α) =

w

p(D|w, β)p(w|α)

(18.1.27)

To ﬁnd the optimal α, we search for the zero derivative of log p(D|α). From equation (18.2.18) we can use
the general derivative identity to arrive at

(cid:28) ∂

∂α

(cid:29)
log p(w|α)

p(w|α,D)

∂
∂α

log p(D|α) =

Since

log p(w|α) = −

α
2

wTw + B
2

log α + const.

(18.1.28)

(18.1.29)

Figure 18.4: The log marginal likelihood log p(D|λ, α∗(λ), β∗(λ)

having found the optimal values of the hyperparameters α and
β using ML-II. These optimal values are dependent on λ. Ac-
cording to ML-II, the best model corresponds to λ = 0.37.

DRAFT March 9, 2010

337

−202−2−1012lambda=0.01−202−2−1012lambda=0.05−202−2−1012lambda=0.09−202−2−1012lambda=0.13−202−2−1012lambda=0.17−202−2−1012lambda=0.21−202−2−1012lambda=0.25−202−2−1012lambda=0.29−202−2−1012lambda=0.33−202−2−1012lambda=0.37−202−2−1012lambda=0.41−202−2−1012lambda=0.45−202−2−1012lambda=0.49−202−2−1012lambda=0.53−202−2−1012lambda=0.57−202−2−1012lambda=0.61−202−2−1012lambda=0.65−202−2−1012lambda=0.69−202−2−1012lambda=0.73−202−2−1012lambda=0.77−0.100.10.20.30.40.50.60.702468101214x 109log marginal likelihoodlambda(cid:28)

1
2

−wTw + B

α

(cid:29)

p(w|α,D)

we obtain

∂
∂α

log p(D|α) =
(cid:68)
(cid:69)

wTw

0 = −

p(w|α,D)

+ B
α

Setting the derivative to zero, the optimal α satisﬁes

One may now form a ﬁxed point equation

Regression With Additive Gaussian Noise

(18.1.30)

(18.1.31)

(18.1.32)

αnew =

B

(cid:104)wTw(cid:105)p(w|α,D)
(cid:16)(cid:68)

= trace

(cid:68)

which is in fact a re-derivation of the EM procedure for this model. For a Gaussian posterior, p(w|α,D) =
(cid:69)
N (w m, S),
wTw

− (cid:104)w(cid:105)(cid:104)w(cid:105)T + (cid:104)w(cid:105)(cid:104)w(cid:105)T(cid:17)

wwT(cid:69)

= trace (S) + mTm

(18.1.33)

αnew =

B

trace (S) + mTm

Gull-MacKay ﬁxed point iteration

From equation (18.1.31) we have

(cid:68)

(cid:69)

0 = −α

wTw

p(w|α,D)

+ B = −αS − αmTwT + B

so that an alternative ﬁxed point equation[123, 180] is

αnew = B − αS
mTm

In practice this update converges more rapidly than equation (18.1.34).

(18.1.34)

(18.1.35)

(18.1.36)

Example 83 (Learning the basis function widths). In ﬁg(18.3) we plot the training data for a regression
problem using a Bayesian LPM. A set of 10 Radial Basis Functions are used,

φi(x) = exp(cid:0)

−0.5(x − ci)2/λ2(cid:1)

(18.1.37)

with ci, i = 1, . . . , 10 spread out evenly between −2 and 2. The hyperparameters α and β are learned by
ML-II under EM updating. For a ﬁxed width λ we then present the predictions, each time ﬁnding the
optimal α and β for this width. The optimal joint α, β, λ hyperparameter setting is obtained as described
in ﬁg(18.4) which shows the marginal log likelihood for a range of widths.

18.1.5 Validation likelihood

The hyperparameters found by ML-II are those which are best at explaining the training data. In principle,
this is diﬀerent from those that are best for prediction and, in practice therefore, it is reasonable to set
hyperparameters also by validation techniques. One such method is to set hyperparameters by minimal
prediction error on a validation set. Another common technique is to set hyperparameters Γ by their
likelihood on a validation set {Xval,Yval} ≡ {(xm

val, ym

(cid:90)

w

val), m = 1, . . . , M}:
p(Yval|w, Γ)p(w|Γ,Xtrain,Ytrain)

p(Yval|Γ,Xtrain,Ytrain,Xval) =

338

(18.1.38)

DRAFT March 9, 2010

Regression With Additive Gaussian Noise

from which we obtain (see exercise(187))

log p(Yval|Γ,Dtrain,Xval) = −

(cid:3)T

where yval =(cid:2)y1
val =(cid:2)φ(x1

Cval ≡ ΦvalSΦT
and the design matrix

ΦT

val, . . . , yM
val

val + σ2IM

val)(cid:3)

val), . . . , φ(xM

1
2

log det (2πCval) −

1
2

(yval − Φvalm)T C−1

val (yval − Φvalm)

(18.1.39)

(18.1.40)

(18.1.41)

The optimal hyperparameters Γ∗ can then be found by maximising (18.1.39) with respect to Γ.

18.1.6 Prediction

The mean function predictor based on hyperparameters Γ and weights w is given by

(cid:90) (cid:26)(cid:90)

(cid:27)

¯f(x) =

f(x; w)p(w, Γ|D)dwdΓ =

f(x; w)p(w|Γ,D)dw

p(Γ|D)dΓ

(18.1.42)

The term in curly brackets is the mean predictor for ﬁxed hyperparameters. Equation(18.1.42) then weights
each mean predictor by the posterior probability of the hyperparameter p(Γ|D). This is a general recipe
for combining model predictions, where each model is weighted by its posterior probability. However,
computing the integral over the hyperparameter posterior is numerically challenging and approximations
are usually required. Provided the hyperparameters are well determined by the data, we may instead
approximate the above hyperparameter integral by ﬁnding the MAP hyperparameters and use

¯f(x) ≈

f(x; w)p(w|Γ∗

,D)dw

(18.1.43)

(cid:90)

(cid:90)

18.1.7 The relevance vector machine

The Relevance Vector Machine assumes that only a small number of components of the basis function
vector are relevant in determining the solution for w. For a predictor,

f(x; w) =

wiφi(x) ≡ wTφ(x)

(18.1.44)

it is often the case that some basis functions will be redundant in the sense that a linear combination of
the other basis functions can reproduce the training outputs with insigniﬁcant loss in accuracy. To exploit
this eﬀect and seek a parsimonious solution we may use a more reﬁned prior that encourages each wi itself
to be small:

(18.1.45)

(18.1.46)

(18.1.47)

The modiﬁcations required to the description of section(18.1.1) are to replace S with

S =

diag (α) + β

φ (xn) φT (xn)

The marginal likelihood is then given by

2 log p(D|Γ) = −β

(yn)2 + dTS−1d − log det (S) +

B(cid:88)

i=1

log αi + N log β − N log (2π)

(18.1.48)

The EM update for β is unchanged, and the EM update for each αi is

1

αnew

i

= [S]ii + m2
i

DRAFT March 9, 2010

(18.1.49)

339

B(cid:88)

i=1

p(w|α) =(cid:89)

i

p(wi|αi) = N

(cid:32)

where the prior on each individual weight is given by

(cid:1) =

−1
i

(cid:16) αi

(cid:17) 1

2π

− αi

2 w2
i

2 e

(cid:33)−1

p(wi|αi)

(cid:0)wi 0, α
N(cid:88)
N(cid:88)

n=1

n=1

Algorithm 18 Evidence Procedure for Bayesian Logistic Regression

1: Initialise w and α.
2: while Not Converged do
3:
4:
5: end while

Find optimal w∗ by iterating equation (18.2.16), equation (18.2.15) to convergence.
Update α according to equation (18.2.9).

18.2 Classiﬁcation

For the logistic regression model

(cid:33)

(cid:32) B(cid:88)

i

p(c = 1|w, x) = σ

wiφi(x)

Classiﬁcation

(cid:46) E-step
(cid:46) M-Step

(18.2.1)

the Maximum Likelihood method returns only a single optimal w. To deal with the inevitable uncertainty
in estimating w we need to determine the posterior distribution of the weights w. To do so we ﬁrst deﬁne
a prior on the weights p(w) for which a convenient choice is a Gaussian:

−αwTw/2

(18.2.2)

(cid:0)w 0, α

−1I(cid:1) = αB/2

(2π)B/2 e

p(w|α) = N

where α is the inverse variance (also called the precision). Given a dataset of input-class labels, D =
{(xn, cn) , n = 1, . . . , N}, the parameter posterior is
p(D|α) p(w|α)

p(w|α,D) = p(D|w, α)p(w|α)

p(cn|xn, w)

p(D|α)

N(cid:89)

(18.2.3)

n=1

=

1

Unfortunately, this distribution is not of any standard form and exactly inferring statistics such as the
mean, or the most probable value are formally computationally intractable.

18.2.1 Hyperparameter optimisation

Hyperparameters such as α can be set by maximising the marginal likelihood

(cid:90)

(cid:90)

N(cid:89)

(cid:16) α

(cid:17)B/2

p(D|α) =

p(D|w)p(w|α) =

w

p(cn|xn, w)

2π

w

n=1

− α
e

2 wTw

(18.2.4)

There are several approaches one could take to approximate this and below we discuss the Laplace and a
variational technique. Common to all approaches, however, is the form of the gradient, diﬀering only in the
statistics under an approximation to the posterior. For this reason we derive ﬁrst generic hyperparameter
update formulae that apply under all approximations.

To ﬁnd the optimal α, we search for the zero derivative of log p(D|α). This is equivalent to the linear
regression case, and we immediately obtain

Setting the derivative to zero, an exact equation is that the optimal α satisﬁes

(cid:28)

(cid:29)

1
2

−wTw + B

α

p(w|α,D)

∂
∂α

log p(D|α) =
(cid:69)
(cid:68)

wTw

0 = −

p(w|α,D)

+ B
α

One may now form a ﬁxed point equation

αnew =

B

(cid:104)wTw(cid:105)p(w|α,D)

340

(18.2.5)

(18.2.6)

(18.2.7)

DRAFT March 9, 2010

Classiﬁcation

The averages in the above expression cannot be computed exactly and are replaced by averages with
respect an approximation of the posterior q(w|α,D). Note that since we only have an approximation to
the posterior, and therefore the mean and covariance statistics, we cannot guarantee that the likelihood
will always increase.

For a Gaussian approximation of the posterior, q(w|α,D) = N (w m, S)

(cid:68)

(cid:69)

wTw

= trace

(cid:16)(cid:68)

wwT(cid:69)

− (cid:104)w(cid:105)(cid:104)w(cid:105)T + (cid:104)w(cid:105)(cid:104)w(cid:105)T(cid:17)

= trace (S) + mTm

αnew =

B

trace (S) + mTm

In this case the Gull-Mackay alternative ﬁxed point equation[123, 180] is

αnew = B − αS
mTm

(18.2.8)

(18.2.9)

(18.2.10)

The hyperparameter updates (18.2.9) and (18.2.10) have the same form as for the regression model. The
mean m and covariance S of the posterior in the regression and classiﬁcation cases are however diﬀerent.
In the classiﬁcation case we need to approximate the mean and covariance, as discussed below.

18.2.2 Laplace approximation

The weight posterior is given by

−E(w)

p(w|α,D) ∝ e

where

E(w) = α
2

wTw −

(cid:16)

wThn(cid:17)

,

log σ

N(cid:88)

n=1

hn ≡ (2cn − 1)φn

(18.2.11)

(18.2.12)

By approximating E(w) by a quadratic function in w, we obtain a Gaussian approximation q(w|D, α) to
p(w|D, α). To do so we ﬁrst ﬁnd the minimum of E(w). Diﬀerentiating, we obtain

(cid:16)

wThn(cid:17)

It is convenient to use a Newton method to ﬁnd the optimum. The Hessian matrix with elements

N(cid:88)

n=1

∇E = αw −

(1 − σn)hn,

σn ≡ σ

∂2

∂wi∂wj

E(w)

Hij ≡
is given by

H = αI +

N(cid:88)
(cid:124)

n=1

(cid:125)
σn(1 − σn)φn (φn)T

(cid:123)(cid:122)

J

(18.2.13)

(18.2.14)

(18.2.15)

Note that the Hessian is positive semideﬁnite (see exercise(188)) so that the function E(w) is convex (bowl
shaped), and ﬁnding a minimum of E(w) is numerically unproblematic. A Newton update then is

wnew = w − H−1 (∇E)

Given a converged w, the posterior approximation is given by

(18.2.16)

q(w|D, α) = N (w m, S) ,

(18.2.17)
where m = w∗ is the converged estimate of the minimum point of E(w) and H is the Hessian of E(w) at
this point.

S ≡ H−1

DRAFT March 9, 2010

341

Classiﬁcation

(cid:16) α

(cid:17)B/2

2π

− α
2 wTw ∝
e

(cid:90)

e

w

−E(w)

(18.2.18)

Approximating the marginal likelihood

The marginal likelihood is given by

(cid:90)

p(D|α) =

p(D|w)p(w|α) =

w

(cid:90)

N(cid:89)

w

n=1

p(cn|xn, w)
(cid:16)

log σ

n

(w∗)Tw∗ +(cid:88)

(w∗)T hn(cid:17)

For an optimum value m = w∗, we approximate the marginal likelihood using (see section(28.2))

log p(D|α) ≈ L(α) ≡ −

α
2

1
2

−

log det (αI + J) + B
2

log α (18.2.19)

Given this approximation L(α) to the marginal likelihood, an alternative strategy for hyperparameter
optimisation, is to optimises L(α) with respect to α. By diﬀerentiating L(α) directly, the reader may show
that the resulting updates are in fact equivalent to using the general condition equation (18.2.6) under a
Laplace approximation to the posterior statistics.

18.2.3 Making predictions

Ultimately, our interest is to classify in novel situations, averaging over posterior weight uncertainty,

p(c = 1|x, α,D) =

w

p(c = 1|x, w)p(w|α,D)

(18.2.20)

The B dimensional integrals over w cannot be computed analytically and numerical approximation is
required. In this particular case the relative benign nature of the posterior (the log posterior is concave, see
below) suggests that a simple Laplace approximation may suﬃce (see [142] for a variational approximation).
To make a class prediction for a novel input x, we use

p(c = 1|x,D, α

∗) =

p(c = 1|x, w)p(w|D, α

∗)dw =

σ

xTw

p(w|D, α

∗)dw

However, since the term σ(cid:0)xTw(cid:1) depends on w via the scalar product xTw, we only require the integral

To compute the predictions it would appear that we need to carry out an integral in B dimensions.

over the one-dimensional projection (see exercise(189))

(cid:90)

(cid:16)

(cid:17)

(cid:90)

(cid:90)

∗)dh

(18.2.21)

(18.2.22)

h ≡ xTw

so that

p(c = 1|x,D, α

∗) =

(cid:90)

σ (h) p(h|x,D, α
(cid:17)

h xTm, xTΣx

(cid:16)

p(h|x,D, α

∗) ≈ N

Under the Laplace approximation, w is Gaussian, p(w|D, α∗) ≈ N (w m, S). Since h is a projection of
w, h is also Gaussian distributed

(18.2.23)

Predictions may then be made by numerically evaluating the one-dimensional integral over the Gaussian
distribution in h, equation (18.2.22).

Approximating the Gaussian average of a logistic sigmoid

Predictions under a Gaussian posterior approximation require the computation of

I ≡ (cid:104)σ(x)(cid:105)N (x µ,σ2)

342

(18.2.24)

DRAFT March 9, 2010

Classiﬁcation

Figure 18.5: Bayesian Logistic Regression with the
RBF e−λ(x−m)2, placing basis functions centred on
a subset of the training points. The green points
are training data from class 1, and the red points are
training data from class 0. The contours represent the
probability of being in class 1. The optimal value of α
found by the evidence procedure in this case is 0.45 (λ
is set by hand to 2). See demoBayesLogRegression.m

where µ = xTµ, σ2 = xTΣx. Gaussian quadrature is an obvious numerical candidate[227]. An alternative
is to replace the logistic sigmoid by a suitably transformed erf function[181], the reason being that the
Gaussian average of an erf function is another erf function. Using a single erf, an approximation is1

σ(x) ≈

1
2

(1 + erf (νx))

(18.2.25)

These two functions agree at −∞, 0,∞. A reasonable criterion is that the derivatives of these two should
agree at x = 0 since then they have locally the same slope around the origin and have globally similar
shape. Using σ(0) = 0.5 and that the derivative is σ(0)(1 − σ(0)), this requires

A more accurate approximation can be found by considering

√π
4

= ν

1
4

√π ⇒ ν =
(cid:88)

ui
2

i

(1 + erf(νix))

σ(x) ≈

where(cid:80)

(18.2.26)

(18.2.27)

(18.2.28)

(18.2.29)

(18.2.30)

i ui = 1. Suitable values for ui and νi are given in logsigapp.m which uses a linear combination
of 11 erf functions to approximate the logistic sigmoid. To compute the approximate average of σ(x) over
a Gaussian, one may then make use of the result

(cid:90) ∞

∞

1
√2π

(cid:18) 1

√2

− x2

2 erf

e

(cx + d)

dx = erf

(cid:19)

(cid:19)

(cid:18)

d

√2 + 2c2

Since

(cid:90) ∞

e

∞

1

√2πσ2

we have

− (x−µ)2

2σ2 erf (νx) dx =

(cid:90) ∞

e

∞

1
√2π

− x2

2 erf (ν (σx + µ)) dx

(cid:104)σ(x)(cid:105)N (x µ,σ) ≈

1
2

+

1
2



(cid:88)

i

uierf

νiµ(cid:113)

1 + 2ν2

i σ2



Further approximate statistics can be obtained using the results derived in [22].

(cid:82) x
1Note that the deﬁnition of the erf function used here is taken to be consistent with MATLAB, namely that erf(x) ≡
0 e−t2

dt. Other authors deﬁne it to be the cumulative density function of a standard Gaussian,

(cid:82) x
−∞ e− 1
2 t2

2√
π

dt.

2√
π

DRAFT March 9, 2010

343

−6−4−20246−6−4−202460.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.50.50.50.50.50.50.60.60.60.60.70.70.70.70.80.80.80.90.9Classiﬁcation

(a)

(b)

Figure 18.6: Classiﬁcation using the RVM with RBF e−λ(x−m)2, placing a basis function on a subset of
the training data points. The green points are training data from class 1, and the red points are training
data from class 0. The contours represent the probability of being in class 1. (a): Training points. (b):
The training points weighted by their relevance value 1/αn. Nearly all the points have a value so small
that they eﬀectively vanish. See demoBayesLogRegRVM.m

18.2.4 Relevance vector machine for classiﬁcation

In adopting the RVM prior to classiﬁcation, we encourage individual weights to be small

(18.2.31)

(18.2.32)

(18.2.33)

p(w|α) =(cid:89)

i

where

p(wi|αi) = N

p(wi|αi)
(cid:18)

wi 0,

1
αi

(cid:19)

(cid:88)

n

The only alterations in the previous evidence procedure are

[∇E]i = αiwi −

(1 − σn)hn
i ,

H = diag (α) + J

These are used in the Newton update formula as before. The EM update equation for the α’s is given by

αnew
i =

1

m2

i + Sii

where Σ = H−1. Similarly, the Gull-MacKay update is given by

αnew
i =

1 − αiSii

m2
i

(18.2.34)

(18.2.35)

Running this procedure, one typically ﬁnds that many of the α’s tend to inﬁnity and the corresponding
weights are pruned from the system. The remaining weights tend correspond to basis functions (in the
RBF case) in the centres of mass of clusters of datapoints of the same class, see ﬁg(18.6). Contrast this
with the situation in SVMs, where the retained datapoints tend to be on the decision boundaries. The
number of training points retained by the RVM tends to be very small – smaller indeed that the number
retained in the SVM framework. Whilst the RVM does not support large margins, and hence may be a
less robust classiﬁer, it does retain the advantages of a probabilistic framework[276]. A potential critique
of the RVM, coupled with an ML-II procedure for learning the αi is that it is overly aggressive in terms of
pruning. Indeed, as one may verify running demoBayesLogRegRVM.m it is common to ﬁnd an instance of a
problem for which there exists a set of αi such that the training data can be classiﬁed perfectly; however,
after using ML-II, so many of the αi are set to zero that the training data can no longer be classiﬁed
perfectly.

344

DRAFT March 9, 2010

−8−6−4−202468−8−6−4−2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.70.80.80.90.9−8−6−4−202468−8−6−4−2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.70.80.80.90.9Exercises

18.2.5 Multi-class case

We brieﬂy note that the multi-class case can be treated by using the softmax function under a one-of-m
class coding scheme. The class probabilities are

p(c = m|y) = eym
Σm(cid:48)eym(cid:48)

which automatically enforces the constraint (cid:80)
classes, the cost of the Laplace approximation scales as O(cid:0)C3N 3(cid:1). However, one may show by careful
implementation that the cost may be reduced to only O(cid:0)CN 3(cid:1), analogous to the cost savings possible in

m p(c = m) = 1. Naively it would appear that for C

(18.2.36)

the Gaussian Process classiﬁcation model [297, 230].

18.3 Code

demoBayesLinReg.m: Demo of Bayesian Linear Regression
BayesLinReg.m: Bayesian Linear Regression

demoBayesLogRegRVM.m: Demo of Bayesian Logistic Regression (RVM)
BayesLogRegressionRVM.m: Bayesian Logistic Regression (RVM)

avsigmaGauss.m: Approximation of the Gaussian average of a logistic sigmoid
logsigapp.m: Approximation of the Logistic Sigmoid using mixture of erfs

18.4 Exercises

Exercise 185. The exercise concerns Bayesian regression.

1. Show that for

f = wTx

(18.4.1)
and p(w) ∼ N (w 0, Σ), that p(f|x) is Gaussian distributed. Furthermore, ﬁnd the mean and
covariance of this Gaussian.

(cid:0) 0, σ2(cid:1). What is p(f|t, x)?

2. Consider a target point t = f + , where  ∼ N

Exercise 186. A Bayesian Linear Parameter regression model is given by

yn = wTφ(xn) + ηn

In vector notation y =(cid:0)y1, . . . , yN(cid:1)T this can be written
with ΦT = (cid:2)φ(x1), . . . , φ(xN )(cid:3) and η is a zero mean Gaussian distributed vector with covariance β−1I.

y = Φw + η

(18.4.2)

(18.4.3)

An expression for the marginal likelihood of a dataset is given in equation (18.1.19). A more compact
expression can be obtained by considering

p(y1, . . . , yN|x1, . . . , xN , Γ)

Since yn is linearly related to xn through w. Then y is Gaussian distributed with mean

(cid:104)y(cid:105) = Φ(cid:104)w(cid:105) = 0
yyT(cid:69)
(cid:68)
and covariance matrix

(cid:68)
(cid:0)w 0, α−1I(cid:1):

− (cid:104)y(cid:105)(cid:104)y(cid:105)T =

For p(w) = N
DRAFT March 9, 2010

(Φw + η) (Φw + η)T(cid:69)

(18.4.4)

(18.4.5)

(18.4.6)

345

1. Show that the covariance matrix can be expressed as

C =

1
β

I +

1
α

ΦΦT

2. Hence show that the log marginal likelihood can we written as

log p(y1, . . . , yN|x1, . . . , xN , Γ) = −

1
2

log det (2πC) −

yTC−1y

1
2

Exercises

(18.4.7)

(18.4.8)

Exercise 187. Using exercise(186) as a basis, derive expression (18.1.39) for the log likelihood on a
validation set.

Exercise 188. Consider the function E(w) as deﬁned in equation (18.2.12).

1. Compute the Hessian matrix which has elements,

Hij ≡

∂2

∂wi∂wj

E(w) = αδij +

σn (1 − σn) φn (φn)T

N(cid:88)

n=1

2. Show that the Hessian is positive semideﬁnite

Exercise 189. Show that for any function f(·),

f(xTw)p(w)dw =

f(h)p(h)dh

(cid:90)

(cid:90)

(18.4.9)

(18.4.10)

where p(h) is the distribution of the scalar xTw. The signiﬁcance of this result is that any high-dimensional
integral of the above form can be reduced to a one-dimensional integral over the distribution of the ‘ﬁeld’
h[22].

Exercise 190. This exercise concerns Bayesian Logistic Regression. Our interest to derive a formula for
the optimal regularisation parameter α based on the Laplace approximation to the marginal log-likelihood
given by

log p(D|α) ≈ L(α) ≡ −

α
2

(w)Tw +(cid:88)

log σ

n

(cid:16)

(w)T hn(cid:17)

1
2

−

log det (αI + J) + B
2

(cid:80)

log α

(18.4.11)

n log σ(cid:0)wThn(cid:1), as in equation

The Laplace procedure ﬁnds ﬁrst an optimal w∗ that minimises αwTw/2−
(18.2.12) which will depend on the setting of α. Formally, therefore, in ﬁnding the α that optimises L(α)
we should make use of the total derivative formula

dL
dα

= ∂L
∂α

∂L
∂wi

∂wi
∂α

(18.4.12)

However, when evaluated at w = w∗, ∂L
∂w = 0. This means that in order to compute the derivative with
respect to α, we only need consider the terms with an explicit α dependence. Equating the derivative to
zero and using

∂ log det (M) = trace(cid:0)M−1∂M(cid:1)

+(cid:88)

i

show that the optimal α satisﬁes the ﬁxed point equation

αnew =

N
(w∗)Tw∗ + trace

(cid:16)

(αI + J)−1(cid:17)

(18.4.13)

(18.4.14)

346

DRAFT March 9, 2010

CHAPTER 19

Gaussian Processes

19.1 Non-Parametric Prediction

Gaussian Processes are ﬂexible Bayesian models that ﬁt well within the probabilistic modelling framework.
In developing GPs it is useful to ﬁrst step back and see what information we need to form a predictor.
Given a set of training data

D = {(xn, yn), n = 1, . . . , N} = X ∪ Y

(19.1.1)

where xn is the input for datapoint n and yn the corresponding output (a continuous variable in the
regression case and a discrete variable in the classiﬁcation case), our aim is to make a prediction y∗ for a
new input x∗. In the discriminative framework no model of the inputs x is assumed and only the outputs
are modelled, conditioned on the inputs. Given a joint model

p(y1, . . . , yN , y

∗

|x1, . . . , xN , x

∗) = p(Y, y

∗

∗)
|X , x

(19.1.2)

we may subsequently use conditioning to form a predictor p(y∗
much use of the i.i.d. assumption that each datapoint is independently sampled from the same generating
distribution. In this context, this might appear to suggest the assumption

|x∗,D). In previous chapters we’ve made

p(y1, . . . , yN , y

∗

|x1, . . . , xN , x

∗) = p(y

∗

|X , x

∗)
p(yn|X , x

(19.1.3)

∗)(cid:89)

n

However, this is clearly of little use since the predictive conditional is simply p(y∗
meaning the predictions make no use of the training outputs. For a non-trivial predictor we therefore need
to specify a joint non-factorised distribution over outputs.

|D, x∗) = p(y∗

|X , x∗)

19.1.1 From parametric to non-parametric

If we revisit our i.i.d. assumptions for parametric models, we used a parameter θ to make a model of the
input-output distribution p(y|x, θ). For a parametric model predictions are formed using

∗

∗
, x

,D) =

∗

∗
,Y, x

∗

p(y

∗

,Y|θ, x

,X )p(θ|
,X )

x

∗

θ

Under the assumption that, given θ, the data is i.i.d., we obtain

∗

∗

∗

∗

|x

p(y

p(y

,D) ∝ p(y
(cid:90)
p(θ|D) ∝ p(θ)(cid:89)

,D) ∝

|x

θ

n

where

p(y

(cid:90)
, θ)p(θ)(cid:89)

θ

(cid:90)

,X , θ) ∝
(cid:90)

p(y

∗

∗

|x

p(yn|θ, xn) ∝

n

∗

p(y

∗

|x

, θ)p(θ|D)

θ

p(yn|θ, xn)

347

(19.1.4)

(19.1.5)

(19.1.6)

θ

yN

xN

(a)

y1

x1

y∗

x∗

y∗

x∗

y1

x1

yN

xN

(b)

Non-Parametric Prediction

Figure 19.1: (a): A parametric model for predic-
(b): The form of the
tion assuming i.i.d. data.
model after integrating out the parameters θ. Our
non-parametric model will have this structure.

After integrating over the parameters θ, the joint data distribution is given by

∗

p(y

∗

,Y|x

,X ) =

∗

p(y

∗

|x

θ

p(yn|θ, xn)

(19.1.7)

(cid:90)

, θ)p(θ)(cid:89)

n

which does not in general factorise into individual datapoint terms, see ﬁg(19.1). The idea of a non-
parametric approach is to specify the form of these dependencies without reference to an explicit parametric
model. One route towards a non-parametric model is to start with a parametric model and integrate out
the parameters. In order to make this tractable, we use a simple linear parameter predictor with a Gaussian
parameter prior. For regression this leads to closed form expressions, although the classiﬁcation case will
require numerical approximation.

19.1.2 From Bayesian linear models to Gaussian processes

To develop the GP, we brieﬂy revisit the Bayesian linear parameter model of section(18.1.1). For param-
eters w and basis functions φi(x) the output is given by (assuming zero output noise)

y =(cid:88)

wiφi(x)

(19.1.8)

(19.1.9)

(19.1.10)

i

If we stack all the y1, . . . , yN into a vector y, then

y = Φw

where Φ =(cid:2)φ(x1), . . . , φ(xN )(cid:3)T is the design matrix. Assuming a Gaussian weight prior

p(w) = N (w 0, Σw)

and since y is linear in w, the joint output y1, . . . , yN is Gaussian distributed. A Gaussian prior on w
induces a Gaussian on the joint y with mean

and covariance

(cid:104)y(cid:105) = Φ(cid:104)w(cid:105)p(w) = 0
(cid:68)

wwT(cid:69)

yyT(cid:69)

(cid:68)

= Φ

p(w)

(cid:18)

ΦΣ

1
2
w

ΦT = ΦΣwΦT =

(cid:19)(cid:18)

ΦΣ

1
2
w

(cid:19)T

(19.1.11)

(19.1.12)

From this we see that the Σw can be absorbed into Φ using its Cholesky decomposition. In other words,
without loss of generality we may assume Σw = I. After integrating out the weights, the Bayesian linear
regression model induces a Gaussian distribution on any set of outputs y as

p(y|x1, . . . , xN ) = N (y 0, K)

where the covariance matrix K depends on the training inputs alone via

[K]n,n(cid:48) = φ(xn)Tφ(xn(cid:48)

),

(cid:48) = 1, . . . , N

n, n

(19.1.13)

(19.1.14)

Since the matrix K is formed as the scalar product of vectors, it is by construction positive semideﬁnite,
as we saw in section(17.3.2). After integrating out the weights, the only thing the model directly depends

348

DRAFT March 9, 2010

Non-Parametric Prediction

on is the covariance matrix K. In a Gaussian process we directly specify the joint output covariance K as
a function of two inputs. Speciﬁcally we need to deﬁne the n, n(cid:48) element of the covariance matrix for any
two inputs xn and xn(cid:48)
[K]n,n(cid:48) = k(xn, xn(cid:48)

. This is achieved using a covariance function k(xn, xn(cid:48)

)

)

(19.1.15)

The required form of the function k(xn, xn(cid:48)
) is very special – when applied to create the elements of the
matrix K it must produce a positive deﬁnite matrix. We discuss how to create such covariance functions
in section(19.3). One explicit straightforward construction is to form the covariance function from the
scalar product of the basis vector φ(xn) and φ(xn(cid:48)
). For ﬁnite-dimensional φ this is known as a ﬁnite
dimensional Gaussian Process. Given any covariance function we can always ﬁnd a corresponding basis
vector representation – that is, for any GP, we can always relate this back to a parametric Bayesian
LPM. However, for many commonly used covariance functions, the basis functions corresponds to inﬁnite
dimensional vectors. It is in such cases that the advantages of using the GP framework are particularly
evident since we would not be able to compute eﬃciently with the corresponding inﬁnite dimensional
parametric model.

19.1.3 A prior on functions

The nature of many machine learning applications is such that the knowledge about the true underlying
mechanism behind the data generation process is limited. Instead one relies on generic ‘smoothness’ as-
sumptions of the form that for two inputs x and x(cid:48) that are close, the corresponding outputs y and y(cid:48)
should be similar. Many generic techniques in machine learning can be viewed as diﬀerent characterisations
of smoothness. An advantage of the GP framework in this respect is that the mathematical smoothness
properties of the functions are well understood, giving conﬁdence in the procedure.

For a given covariance matrix K, equation (19.1.13) speciﬁes a distribution on functions1 in the following
sense: we specify a set of input points x = (x1, . . . , xN ) and a N × N covariance matrix K. Then we draw
a vector y from the Gaussian deﬁned by equation (19.1.13). We can then plot the sampled ‘function’ at
the ﬁnite set of points (xn, yn), n = 1, . . . , N. What kind of functions does a GP correspond to? Consider
two scalar inputs, xi and xj separated by a distance |xi − xj|. The corresponding sampled outputs yi and
yj ﬂuctuate as diﬀerent functions are drawn. For a covariance function that has a high value for |xi − xj|
small, we expect yi and yj to be very similar since they are highly correlated. Conversely, for a covariance
function that has low value for a given small separation |xi − xj|, we expect yi and yj to be eﬀectively
independent2. In general, we would expect the correlation between yi and yj to decrease the further apart
xi and xj are.

In ﬁg(19.2a) we show three sample functions drawn from a Squared Exponential covariance function de-
ﬁned over 500 points uniformly spaced from −2 to 3. Each sampled function looks reasonably smooth.
Conversely, for the Ornstein Uhlenbeck covariance function, the sampled functions ﬁg(19.2c) look locally
rough. These smoothness properties are related to the form of the covariance function, as discussed in
section(19.4.1).

The zero mean assumption implies that if we were to draw a large number of such ‘functions’, the mean
across these functions at a given point x tends to zero. Similarly, for any two points x and x(cid:48) if we compute
the sample covariance between the corresponding y and y(cid:48) for all such sampled functions, this will tend
to the covariance function value k(x, x(cid:48)). The zero-mean assumption can be easily relaxed by deﬁning a
mean function m(x) to give p(y|x) = N (y m, K). In many practical situations one typically deals with
‘detrended’ data in which such mean trends have been already removed. For this reason much of the
development of GPs in the machine learning literature is for the zero mean case.

1The term ‘function’ is potentially confusing since we do not have an explicit functional form for the input output-mapping.

For any ﬁnite set of inputs x1, . . . , xN the values for the ‘function’ are given by the outputs at those points y1, . . . , yN .

2For periodic functions, however, we would expect high correlation at separating distances corresponding to the period of

the function.

DRAFT March 9, 2010

349

Gaussian Process Prediction

19.2 Gaussian Process Prediction
For a dataset D and novel input x∗, a zero mean GP makes a Gaussian model of the joint outputs
y1, . . . , yN , y∗ given the joint inputs x1, . . . , xN , x∗. For convenience we write this as

(cid:0)y, y

∗ 0N +1, K+(cid:1)

p(y, y

∗

|x, x

∗) = N

(19.2.1)

where 0N +1 is a N +1 dimensional zero-vector. The covariance matrix K+ is a block matrix with elements



K+ ≡

Kx,x

Kx,x∗



Kx∗,x

where Kx,x is the covariance matrix of the training inputs x =(cid:8)x1, . . . , xN(cid:9) – that is

Kx∗,x∗

[Kx,x]n,n(cid:48) ≡ k(xn, xn(cid:48)

),

(cid:48) = 1, . . . , N

n, n

The N × 1 vector Kx,x∗ has elements

∗)
[Kx,x∗]n,∗ ≡ k(xn, x

n = 1, . . . , N

Kx∗,x is the transpose of the above vector. The scalar covariance is given by

(19.2.2)

(19.2.3)

(19.2.4)

(19.2.5)

Kx∗,x∗ ≡ k(x

∗

∗)
, x

The predictive distribution p(y∗
giving a Gaussian distribution

(cid:0)y

|y,D) is obtained by Gaussian conditioning using the results in deﬁnition(78),

x,xKx,x∗(cid:1)

∗

p(y

∗

|x

,D) = N

∗ Kx∗,xK−1

x,xy, Kx∗,x∗ − Kx∗,xK−1

For ﬁxed hyperparameters, GP regression is an exact method and there are no issues with local minima.
Furthermore, GPs are attractive since they automatically model uncertainty in the predictions. However,

the computational complexity for making a prediction is O(cid:0)N 3(cid:1) due to the requirement of performing

the matrix inversion (or solving the corresponding linear system by Gaussian elimination). This can be
prohibitively expensive for large datasets and a large body of research on eﬃcient approximations exists.
A discussion of these techniques is beyond the scope of this book, and the reader is referred to [230].

19.2.1 Regression with noisy training outputs

To prevent overﬁtting to noisy data it is useful to assume that a training output yn is the result of some
clean process f n corrupted by additive Gaussian noise,

yn = f n + n, where

(19.2.6)
In this case our interest is to predict the clean signal f∗ for a novel input x∗. Then the distribution
p(y, f∗

(cid:18) Kx,x + σ2I Kx,x∗
|x, x∗) is a zero mean Gaussian with block covariance matrix

n ∼ N
(cid:19)

(19.2.7)

Kx∗,x

Kx∗,x∗

so that Kx,x is replaced by Kx,x + σ2I in forming the prediction, equation (19.2.5).

350

DRAFT March 9, 2010

(cid:0)n 0, σ2(cid:1)

Covariance Functions

Example 84. Training data from a one-dimensional input x and one dimensional output y are plotted
in ﬁg(19.2b,d), along with the mean regression function ﬁt, based on two diﬀerent covariance functions.
Note how the smoothness of the prior translates into smoothness of the prediction. The smoothness of
the function space prior is a consequence of the choice of covariance function. Naively, we can partially
understand this by the behaviour of the covariance function at the origin, section(19.4.1). See demoGPreg.m

The marginal likelihood and hyperparameter learning
For a set of N one-dimensional training inputs represented by the N × 1 dimensional vector y and a
covariance matrix K deﬁned on the inputs x1, . . . , xN , the log marginal likelihood is

One can learn any free parameters of the covariance function by maximising the marginal likelihood. For
example, a squared exponential covariance function may have parameters λ, v0:

log p(y|x) = −

1
2

log det (2πK)

1
2

yTK−1y −
(cid:26)
2 λ(cid:0)x − x

−

1

(cid:27)

(cid:48)(cid:1)2

k(x, x

(cid:48)) = v0 exp

(19.2.8)

(19.2.9)

The λ parameter in equation (19.2.12) speciﬁes the appropriate length-scale of the inputs, and v0 the
variance of the function. The dependence of the marginal likelihood (19.2.8) on the parameters is typically
complex and no closed form expression for the Maximum Likelihood optimum exists; in this case on resorts
to numerical optimisation techniques such as conjugate gradients.

Vector inputs

For regression with vector inputs and scalar outputs we need to deﬁne a covariance as a function of the
two vectors, k(x, x(cid:48)). Using the multiplicative property of covariance functions, deﬁnition(93), a simple
way to do this is to deﬁne
(cid:48)
i)
k(xi, x

k(x, x(cid:48)) =(cid:89)

(19.2.10)

i

For example, for the squared exponential covariance function this gives

k(x, x(cid:48)) = e

−(x−x(cid:48))2

(19.2.11)

though ‘correlated’ forms are possible as well, see exercise(195). We can generalise the above using
parameters:

k(x, x(cid:48)) = v0 exp

(cid:40)

D(cid:88)

l=1

1
2

−

(cid:41)

(cid:1)2

(cid:0)xl − x

(cid:48)
l

λl

(19.2.12)

where xl is the lth component of x and θ = (v0, λ1, . . . , λD) are parameters. The λl in equation (19.2.12)
allow a diﬀerent length scale on each input dimension and can be learned by numerically maximising the
marginal likelihood. For irrelevant inputs, the corresponding λl will become small, and the model will
ignore the lth input dimension. This is closely related to Automatic Relevance Determination [181].

19.3 Covariance Functions
Covariance functions k(x, x(cid:48)) are special in that they deﬁne elements of a positive deﬁnite matrix. These
functions are also referred to as ‘kernels’, particulary in the machine learning literature.

DRAFT March 9, 2010

351

Covariance Functions

(a)

(c)

(b)

(d)

(a):
Figure 19.2: The input space from -2 to 3 is split evenly into 1000 points x1, . . . , x1000.
Three samples from a GP prior with Squared Exponential (SE) covariance function, λ = 2. The
1000 × 1000 covariance matrix K is deﬁned using the SE kernel, from which the samples are drawn
using mvrandn(zeros(1000,1),K,3). (b): Prediction based on training points. Plotted is the posterior
predicted function based on the SE covariance. The central line is the mean prediction, with standard
(c): Three samples from the Ornstein-
errors bars on either side. The log marginal likelihood is ≈ 70.
(d): Posterior prediction for the OU covariance. The log marginal
Uhlenbeck GP prior with λ = 2.
likelihood is ≈ 3, meaning that the SE covariance is much more heavily supported by the data than the
rougher OU covariance.

Deﬁnition 91 (Covariance function). Given any collection of points x1, . . . , xM , a covariance function
k(xi, xj) deﬁnes the elements of a M × M matrix

[C]i,j = k(xi, xj)

such that C is positive semideﬁnite.

19.3.1 Making new covariance functions from old

The following rules (which can all be proved directly) generate new covariance functions from existing
covariance functions k1, k2 [182],[230].

Deﬁnition 92 (Sum).

k(x, x(cid:48)) = k1(x, x(cid:48)) + k2(x, x(cid:48))

(19.3.1)

352

DRAFT March 9, 2010

−2−1.5−1−0.500.511.522.53−3−2.5−2−1.5−1−0.500.511.52−2−1.5−1−0.500.511.522.53−1.5−1−0.500.511.52−2−1.5−1−0.500.511.522.53−2.5−2−1.5−1−0.500.511.522.5−2−1.5−1−0.500.511.522.53−1.5−1−0.500.511.5Covariance Functions

Deﬁnition 93 (Product).

k(x, x(cid:48)) = k1(x, x(cid:48))k2(x, x(cid:48))

(cid:18) x

y

(cid:19)

,

Deﬁnition 94 (Product Spaces). For z =

k(z, z(cid:48)) = k1(x, x(cid:48)) + k2(y, y(cid:48))

and

k(z, z(cid:48)) = k1(x, x(cid:48))k2(y, y(cid:48))

Deﬁnition 95 (Vertical Rescaling).

k(x, x(cid:48)) = a(x)k1(x, x(cid:48))a(x(cid:48))

for any function a(x).

Deﬁnition 96 (Warping and Embedding).

(cid:0)u(x), u(x(cid:48))(cid:1)

k(x, x(cid:48)) = k1

(19.3.2)

(19.3.3)

(19.3.4)

(19.3.5)

(19.3.6)

for any mapping x → u(x), where the mapping u(x) has arbitrary dimension.

A small collection of covariance functions commonly used in machine learning is given below. We refer
the reader to [230] and [107] for further popular covariance functions.

19.3.2 Stationary covariance functions

Deﬁnition 97 (Stationary Kernel). A kernel k(x, x(cid:48)) is stationary if the kernel depends only on the
separation x − x(cid:48). That is
k(x, x(cid:48)) = k(x − x(cid:48))

(19.3.7)

Following the notation in [230], for a stationary covariance function we may write

k(d)

(19.3.8)

where d = x − x(cid:48). This means that for functions drawn from the GP, on average, the functions depend

only on the distance between inputs and not on the absolute position of an input. In other words, the
functions are on average translation invariant.

For isotropic covariance functions, the covariance is deﬁned as a function of the distance k(|d|).
DRAFT March 9, 2010

353

Deﬁnition 98 (Squared Exponential).

k(d) = e

−|d|2

Covariance Functions

(19.3.9)

The Squared Exponential is one of the most common covariance functions. There are many ways to show
that this is a covariance function. An elementary technique is to consider

2|xn|2
− 1

= e

2|xn(cid:48)|2
− 1
e

e(xn)Txn(cid:48)

(19.3.10)

(cid:16)

xn−xn(cid:48)(cid:17)T(cid:16)

xn−xn(cid:48)(cid:17)

− 1

2

e

The ﬁrst two factors form a kernel of the form φ(xn)φ(xn(cid:48)
is
the linear kernel. Taking the exponential and writing the power series expansion of the exponential, we
have

). In the ﬁnal term k1(xn, xn(cid:48)

) = (xn)T xn(cid:48)

ek1(xn,xn(cid:48)

) =

1
i! ki

1(xn, xn(cid:48)

)

(19.3.11)

∞(cid:88)

i=1

this can be expressed as a series of integer powers of k1, with positive coeﬃcients. By the product (with
itself) and sum rules above, this is therefore a kernel as well. We then use the fact that equation (19.3.10)
is the product of two kernels, and hence also a kernel.

Deﬁnition 99 (γ-Exponential).

k(d) = e

−|d|γ

,

0 < γ ≤ 2

(19.3.12)

When γ = 2 we have the squared exponential covariance function. When γ = 1 this is the Ornstein-
Uhlenbeck covariance function.

Deﬁnition 100 (Mat´ern).

k(d) = |d|νKν (|d|)

where Kν is a modiﬁed Bessel function, ν > 0.

(19.3.13)

Deﬁnition 101 (Rational Quadratic).

k(d) =(cid:0)1 + |d|2(cid:1)−α

,

α > 0

(19.3.14)

Deﬁnition 102 (Periodic). For 1-dimensional x and x(cid:48), a stationary (and isotropic) covariance function
can be obtained by ﬁrst mapping x to the two dimensional vector u(x) = (cos(x), sin(x)) and then using
the SE covariance e−(u(x)−u(x(cid:48)))2
−λ sin2(ω(x−x(cid:48))),

(19.3.15)

(cid:48)) = e

λ > 0

k(x − x

See [182] and [230].

354

DRAFT March 9, 2010

Covariance Functions

(a)

(b)

Figure 19.3: (a): Plots of the Gamma-Exponential covariance e−|x|γ versus x. The case γ = 2 corresponds
to the SE covariance function. The drop in the covariance is much more rapid as a function of the
separation x for small γ, suggesting that the functions corresponding to smaller γ will be locally rough
(b): As for (a) but zoomed in towards the
(though possess relatively higher long range correlation).
origin. For the SE case, γ = 2, the derivative of the covariance function is zero, whereas the OU covariance
γ = 1 has a ﬁrst order contribution to the drop in the covariance, suggesting that locally OU sampled
functions will be much rougher than SE functions.

19.3.3 Non-stationary covariance functions

Deﬁnition 103 (Linear).

k(x, x(cid:48)) = xTx(cid:48)

Deﬁnition 104 (Neural Network).

k(x, x(cid:48)) = arcsin

(cid:32)

2xTx(cid:48)

(cid:112)

√1 + 2xTx

1 + 2x(cid:48)Tx(cid:48)

(cid:33)

(19.3.16)

(19.3.17)

The functions deﬁned by this covariance always go through the origin. To shift this, one may use the
embedding x → (1, x) where the 1 has the eﬀect of a ‘bias’ from the origin. To change the scale of the
bias and and non-bias contributions one may use additional parameters x → (b, λx). The NN covariance
function can be derived as a limiting case of a neural network with inﬁnite hidden units[296], and making
use of exact integral results in [21].

Deﬁnition 105 (Gibbs).

k(x, x(cid:48)) =(cid:89)

i

(cid:18) ri(x)ri(x(cid:48))

i (x) + r2
r2

i (x(cid:48))

for functions ri(x) > 0.

DRAFT March 9, 2010

(cid:19) 1

2

− (xi−x(cid:48)
i)2
(x(cid:48))
e

(x)+r2
i

r2
i

(19.3.18)

355

00.511.522.5300.20.40.60.81  21.51.00.500.020.040.060.080.10.120.140.160.180.20.650.70.750.80.850.90.951  21.51.00.5Analysis of Covariance Functions

(a)

(b)

Figure 19.4: Samples from a GP prior for 500 x points uniformly placed from -20 to 20.

from the periodic covariance function exp(cid:0)

with bias b = 5 and λ = 1.

−2 sin2 0.5(x − x(cid:48))(cid:1). (b): Neural Network covariance function

(a): Sampled

19.4 Analysis of Covariance Functions

19.4.1 Smoothness of the functions

We examine local smoothness for a translation invariant kernel k(x, x(cid:48)) = k(x − x(cid:48)). For two one-
dimensional points x and x(cid:48), separated by a small amount δ (cid:28) 1, x(cid:48) = x + δ, the covariance between
the outputs y and y(cid:48) is, by Taylor expansion,

k(x, x

(cid:48)) ≈ k(0) + δ

dx|x=0 + O(cid:0)δ2(cid:1)

dk

(19.4.1)

so that the change in the covariance at the local level is dominated by the ﬁrst derivative of the covariance
function. For the SE covariance k(x) = e−x2,

dk
dx

= −2xe

−x2

(19.4.2)

is zero at x = 0. This means that for the SE covariance function, the ﬁrst order change in the covariance
is zero, and only higher order δ2 terms contribute.
For the Ornstein-Uhlenbeck covariance, k(x) = e−|x|, the right derivative at the origin is

lim
δ→0

k(δ) − k(0)

δ

= lim
δ→0

e−δ − 1

δ

= −1

(19.4.3)

where this result is obtained using L’Hˆopital’s rule. Hence for the OU covariance function, there is a ﬁrst
order negative change in the covariance; at the local level, this decrease in the covariance is therefore much
more rapid than for the SE covariance, see ﬁg(19.3). Since low covariance implies low dependence (in
Gaussian distributions), locally the functions generated from the OU process are rough, whereas they are
smooth in the SE case. A more formal treatment for the stationary case can be obtained by examining
the eigenvalue-frequency plot of the covariance function (spectral density), section(19.4.3). For rough
functions the density of eigenvalues for high frequency components is higher than for smooth functions.

19.4.2 Mercer kernels

Consider the function

k(x, x

(cid:48)) = φ(x)Tφ(x

(cid:48)) =

B(cid:88)

s=1

(cid:48))
φs(x)φs(x

(19.4.4)

where φ(x) is a vector with component functions φ1(x), φ2(x), . . . , φB(x). Then for a set of points
x1, . . . , xP , we construct the matrix K with elements

[K]ij = k(xi, xj) =

s=1

356

φs(xi)φs(xj)

(19.4.5)

DRAFT March 9, 2010

B(cid:88)

−20−15−10−505101520−2−1.5−1−0.500.511.5−20−15−10−505101520−3−2−101234Analysis of Covariance Functions

We claim that the matrix K so constructed is positive semideﬁnite and hence a valid covariance matrix.
Recalling that a matrix is positive semideﬁnite if for any non zero vector z, zTKz ≥ 0. Using the deﬁnition
of K above we have

P(cid:88)

zTKz =

ziKijzj =

i,j=1

s=1

B(cid:88)

(cid:34) P(cid:88)
(cid:124)

i=1

(cid:35)
(cid:125)

 P(cid:88)
(cid:124)

j=1

ziφs(xi)

(cid:123)(cid:122)

γs


(cid:125)

B(cid:88)

s=1

φs(xj)zj

(cid:123)(cid:122)

γs

=

γ2
s ≥ 0

(19.4.6)

Hence any function of the form equation (19.4.4) is a covariance function. We can generalise the Mercer
kernel to complex functions φ(x) using

(cid:48)) = φ(x)Tφ†(x

(cid:48))

k(x, x

(19.4.7)
where † represents the complex conjugate. Then the matrix K formed from inputs xi, i = 1, . . . , P is
positive semideﬁnite since for any real vector z,

zTKz =

B(cid:88)

s=1

(cid:34) P(cid:88)
(cid:124)

i=1

(cid:35)
(cid:125)

 P(cid:88)
(cid:124)

j=1

ziφs(xi)

(cid:123)(cid:122)

γs


(cid:125)

B(cid:88)

s=1

†
s(xj)zj

φ

(cid:123)(cid:122)

†
γ
s

=

|γs|2 ≥ 0

(19.4.8)

where we made use of the general result for a complex variable xx† = |x|2. A further generalisation is to

write

k(x, x

(cid:48)) =

f(s)φ(x, s)φ

(cid:48)

†(x

, s)ds

(19.4.9)

for f(s) ≥ 0, and scalar complex functions φ(x, s). Then replacing summations with integration (and
assuming we can interchange the sum over the components of z with the integral over s), we obtain

(cid:90)

(cid:90)

zTKz =

f(s)

ziφ(xi, s)

ds =

f(s)|γ(s)|2ds ≥ 0

(19.4.10)

(cid:34) P(cid:88)
(cid:124)

i=1

(cid:123)(cid:122)

γ(s)

(cid:35)
(cid:125)

 P(cid:88)
(cid:124)

j=1


(cid:125)

†(xj, s)zj

φ

(cid:123)(cid:122)

γ†(s)

(cid:90)

Spectral decomposition
Equation(19.4.9) is a generalisation of the spectral decomposition of a kernel k(x, x(cid:48)) since if we write f(s)
as a sum of Dirac delta functions,

(19.4.11)

∞(cid:88)

k=1

f(s) =

λkδ(s − k)
∞(cid:88)

and using φ(x, k) = ψk(x), for an eigenfunction ψk(x) indexed by k with eigenvalue λk, we obtain the
spectral decomposition

k(x, x

(cid:48)) =

λkψk(x)ψk(x

(cid:48))

(19.4.12)

k=1

If all the eigenvalues of a kernel are non-negative, the kernel is a covariance function.

Consider for example the following function

(cid:48)) = e

−(x−x(cid:48))2

k(x, x

(19.4.13)
We claim that this is a covariance function. This is indeed a valid covariance function in the sense that
for any set of points x1, . . . , xd, the d × d matrix formed with elements k(xd, xd(cid:48)
) is positive deﬁnite,
as discussed after deﬁnition(98). The solution given to exercise(193) shows that there do indeed exist
real-valued vectors such that one can represent

(cid:48)) = φ(x)Tφ(x
(cid:48))

k(x, x

(19.4.14)
where the vectors are inﬁnite dimensional. This demonstrates the generalisation of the ﬁnite-dimensional
‘weight space’ viewpoint of a GP to the potentially implicit inﬁnite dimensional representation.

DRAFT March 9, 2010

357

Gaussian Processes for Classiﬁcation

c1

cN

y1

x1

yN

xN

c∗

y∗

x∗

Figure 19.5: GP classiﬁcation. The GP induces a Gaussian distribution on
the latent activations y1, . . . , yN , y∗, given the observed values of c1, . . . , cN .
The classiﬁcation of the new input x∗ is then given via the correlation
induced by the training points on the latent activation y∗.

19.4.3 Fourier analysis for stationary kernels

(cid:90)

g(x) =

1
2π

For a function g(x) with Fourier transform ˜g(s), we may use the inverse Fourier transform to write

˜g(s)e

−ixsds

(19.4.15)

where i ≡ √−1. For a stationary kernel k(x) with Fourier transform ˜k(s), we can therefore write

k(x − x

(cid:48)) =

1
2π

˜k(s)e

−i(x−x(cid:48))sds =

˜k(s)e

−ixseix(cid:48)sds

(19.4.16)

which is of the same form as equation (19.4.9) where the Fourier transform ˜k(s) is identiﬁed with f(s)
and φ(x, s) = e−isx. Hence, provided the Fourier transform ˜k(s) is positive, the translation invariant
kernel k(x− x(cid:48)) is a covariance function. Bochner’s Theorem[230] asserts the converse that any translation

invariant covariance function must have such a Fourier representation.

(cid:90)

1
2π

(cid:90) ∞

−∞

(cid:90)

Application to the squared exponential kernel

For the translation invariant squared exponential kernel, k(x) = e
− s2

− s2

− 1

− 1

2 (x+is)2

dx = √2πe

2

e

2 x2+isxdx = e

2

˜k(s) =

(cid:90) ∞

e

−∞

− 1

2 x2, its Fourier transform is

(19.4.17)

Hence the Fourier transform of the SE kernel is a Gaussian. Since this is positive the SE kernel is a
covariance function.

19.5 Gaussian Processes for Classiﬁcation

Adapting the GP framework to classiﬁcation requires replacing the Gaussian regression term p(y|x) with
a corresponding classiﬁcation term p(c|x) for a discrete label c. To do so we will use the GP to deﬁne a
latent continuous space which will then be mapped to a class probability using

p(c|x) =

Given training data inputs X =(cid:8)x1, . . . , xN(cid:9), corresponding class labels C =(cid:8)c1, . . . , cN(cid:9), and a novel

p(c|y,x)p(y|x)dy =

p(c|y)p(y|x)dy

(19.5.1)

(cid:90)

(cid:90)

(cid:90)

input x∗, then
∗
|x

∗
p(c

where

∗

p(y

∗
p(c

∗)p(y

∗

∗

=

|y

,C|X )
∗
p(y

,C,X ) =
(cid:90)
|X ,C) ∝ p(y
(cid:90)
∗)dY
(cid:41)
(cid:90) (cid:40) N(cid:89)
p(C|Y)p(y
,Y|X , x
(cid:124)
(cid:125)
(cid:123)(cid:122)
p(cn|yn)

,Y,C|X , x

(cid:124)

n=1

=

=

∗

class mapping

∗

(19.5.2)

|X ,C)dy

∗)dY
p(y1, . . . , yN , y

∗

(cid:123)(cid:122)
∗)
|x1, . . . , xN , x

(cid:125)

Gaussian Process

dy1, . . . , dyN

(19.5.3)

358

DRAFT March 9, 2010

Gaussian Processes for Classiﬁcation

The graphical structure of this model is depicted in ﬁg(19.5.) The posterior latent y∗ is then formed
from the standard regression term from the Gaussian Process, multiplied by a set of non-Gaussian maps
from the latent activations to the class probabilities. We can reformulate the prediction problem more
conveniently as follows:

∗

p(y

∗

,Y|x

,X ,C) ∝ p(y

,Y,C|x

,X ) ∝ p(y

∗

∗

∗

∗
|Y, x

,X )p(Y|C,X )

where

(cid:40) N(cid:89)

p(Y|C,X ) ∝

(cid:41)
p(y1, . . . , yN|x1, . . . , xN )
p(cn|yn)
In equation (19.5.4) the term p(y∗
|Y, x∗,X ) does not contain any class label information and is simply a
conditional Gaussian. The advantage of the above description is that we can therefore form an approx-
imation to p(Y|C,X ) and then reuse this approximation in the prediction for many diﬀerent x∗ without

(19.5.5)

n=1

needing to rerun the approximation[297, 230].

(19.5.4)

19.5.1 Binary classiﬁcation
For the binary class case we will use the convention that c ∈ {1, 0}. We therefore need to specify p(c = 1|y)
for a real valued activation y. A convenient choice is the logistic transfer function3

σ(x) =

1

1 + e−x

Then

p(c|y) = σ ((2c − 1) y)

(19.5.6)

(19.5.7)

is a valid distribution since σ(−x) = 1 − σ(x), ensuring that the sum over the class states is 1. A
diﬃculty is that the non-linear class mapping term makes the computation of the posterior distribution
equation (19.5.3) diﬃcult since the integrals over y1, . . . , yN cannot be carried out analytically. There are
many approximate techniques one could apply in this case, including variational methods analogous to
that described in section(??). Below we describe the straightforward Laplace method, leaving the more
sophisticated methods for further reading[230].

19.5.2 Laplace’s approximation
In the Laplace method we approximate the non-Gaussian distribution (19.5.5) by a Gaussian4 q(Y|C,X ),
(19.5.8)

p(Y|C,X ) ≈ q(Y|C,X )

Predictions can be formed from the joint Gaussian

For compactness we deﬁne the class label vector, and outputs

∗

∗
|Y, x

,X )q(Y|C,X )

y =(cid:0)y1, . . . , yN(cid:1)T

∗

p(y

∗

,Y|x

,X ,C) ≈ p(y

c =(cid:0)c1, . . . , cN(cid:1)T
σ =(cid:0)σ(y1), . . . , σ(yN )(cid:1)T

,

and notationally drop the (ever present) conditioning on the inputs x. Also for convenience, we deﬁne

(19.5.9)

(19.5.10)

(19.5.11)

3We will also refer to this as ‘the sigmoid function’. More strictly a sigmoid function refers to any ‘s-shaped’ function

(from the Greek for ‘s’).

4Some authors use the term Laplace approximation solely for approximating an integral. Here we use the term to refer to

a Gaussian approximation of a non-Gaussian distribution.

DRAFT March 9, 2010

359

Finding the mode

The Laplace approximation, section(28.2), corresponds to a second order expansion around the mode of
the distribution. Our task is therefore to ﬁnd the maximum of

Gaussian Processes for Classiﬁcation

p(y|c) ∝ p(y, c) = eΨ(y)

where

Ψ(y) = cTy −

N(cid:88)

n=1

log(1 + eyn) −

1
2

yTK−1

x,xy −

1
2

log det (Kx,x) −

N
2

log 2π

The maximum needs to be found numerically, and it is convenient to use the Newton method [120, 297, 230].

ynew = y − (∇∇Ψ)−1 ∇Ψ

Diﬀerentiating equation 19.5.13 with respect to y we obtain the gradient and Hessian

∇Ψ = (c − σ) − K−1
x,xy
∇∇Ψ = −K−1

x,x − D

where the ‘noise’ matrix is given by

(19.5.12)

(19.5.13)

(19.5.14)

(19.5.15)
(19.5.16)

(19.5.17)

(19.5.21)

(19.5.24)

(19.5.25)
(19.5.26)

D = diag (σ1(1 − σ1), . . . , σN (1 − σN ))

Using these expressions in the Newton update, (19.5.14) gives

To avoid unnecessary inversions, one may rewrite this in the form

ynew = y +(cid:0)K−1

x,x + D(cid:1)−1(cid:0)c − σ − K−1
x,xy(cid:1)
ynew = Kx,x (I + DKx,x)−1 (Dy + c − σ)
x,x + D(cid:1)−1(cid:17)
|y) and q(y|X , x∗,C) in equation (19.5.9). Predictions are then made

y ˜y,(cid:0)K−1

∗
q(y|X , x

(19.5.19)

(19.5.18)

(19.5.20)

(cid:16)

Given a converged solution ˜y we have found a Gaussian approximation

We now have Gaussians for p(y∗
using

Making predictions

,C) = N
(cid:90)

p(y

∗

∗
|x

,X ,C) ≈

p(y

∗

∗
|x

∗
,X , y)q(y|X , x

,C)dy

(cid:0)y

where, by conditioning, section(8.6.1),
∗ Kx∗,xK−1
We can also write this as a linear system

,X ) = N

∗
|Y, x

p(y

∗

y

x,xy + η

∗ = Kx∗,xK−1

(cid:0)η 0, Kx∗,x∗ − Kx∗,xK−1
,X ,C(cid:105) ≈ Kx∗,xK−1

where η ∼ N
aging over y and the noise η, we obtain
∗

(cid:104)y

|x

∗

x,xKx,x∗(cid:1)

x,xy, Kx∗,x∗ − Kx∗,xK−1
x,xKx,x∗(cid:1). Using equation (19.5.23) and equation (19.5.20) and aver-

(19.5.22)

(19.5.23)

Similarly, the variance of the latent prediction is

x,x˜y = Kx∗,x (c − σ (˜y))

(cid:0)K−1
x,x + D(cid:1)−1 K−1
(cid:0)Kx,x + D−1(cid:1)−1 Kx,x∗

,X ,C) ≈ Kx∗,xK−1

x,x

= Kx∗,x∗ − Kx∗,x

var(y

∗

∗

|x

360

x,xKx,x∗ + Kx∗,x∗ − Kx∗,xK−1

x,xKx,x∗

DRAFT March 9, 2010

Gaussian Processes for Classiﬁcation

(a)

(b)

Figure 19.6: Gaussian Process classiﬁcation. The x-axis are the inputs, and the class is the y-axis. Green
points are training points from class 1 and red from class 0. The dots are the predictions p(c = 1|x∗) for
a rand of points x∗.
(b): OU covariance (γ = 1). See
demoGPclass1D.m.

(a): Square exponential covariance (γ = 2).

where the last line is obtained using the Matrix Inversion Lemma, deﬁnition(132).
The class prediction for a new input x∗ is then given by

∗

∗ = 1|x

p(c

(19.5.27)
In order to calculate the Gaussian integral over the logistic sigmoid function, we use an approximation of
the sigmoid function based on the error function erf(x), see section(18.2.3) and avsigmaGauss.m.

,X ,C) ≈ (cid:104)σ(y

∗)(cid:105)N (y∗ (cid:104)y∗(cid:105),var(y∗))

Example 85. An example of binary classiﬁcation is given in ﬁg(19.6) in which one-dimensional input
training data with binary class labels is plotted along with the class probability predictions on a range of
input points. In both cases the covariance function is of the form 2e|xi−xj|γ + 0.001δij. The square expo-
nential covariance produces a smoother class prediction than the Ornstein-Uhlenbeck covariance function.
See demoGPclass1D.m and demoGPclass.m.

Marginal likelihood

The marginal likelihood is given by

(cid:90)
(cid:90)

Under the Laplace approximation, the marginal likelihood is approximated by

p(C|X ) =

p(C|Y)p(Y|X )

Y

p(C|X ) ≈

y

eΨ(˜y)e

− 1
2 (y−˜y)TA(y−˜y)

where A = −∇∇Ψ. Integrating over y gives

log p(C|X ) ≈ log q(C|X )

where

log det (2πA)

log det(cid:0)K−1
log q(C|X ) = Ψ(˜y) −
N(cid:88)
= Ψ(˜y) −
= cT˜y −

log(1 + e˜yn) −

1
2
1
2

x,x + D(cid:1) + N

n=1

log 2π

2
˜yTK−1
x,x˜y −

1
2

1
2

log det (I + Kx,xD)

(19.5.28)

(19.5.29)

(19.5.30)

(19.5.31)

(19.5.32)

(19.5.33)

where ˜y is the converged iterate of equation 19.5.18. One can also simplify the above using that at
convergence K−1

x,x˜y = c − σ(y).

DRAFT March 9, 2010

361

−10−8−6−4−2024681000.20.40.60.81−10−8−6−4−2024681000.20.40.60.8119.5.3 Hyperparameter optimisation

Code

The approximate marginal likelihood can be used to assess hyperparameters θ of the kernel. A little care
is required in computing derivatives of the approximate marginal likelihood since the optimum ˜y depends
on θ. We use the total derivative formula [25]

log q(C|X ) +(cid:88)
(cid:104)

yTK−1

i

d
dθ

log q(C|X ) = ∂

∂θ

∂
∂θ

log q(C|X ) = −

1
2

∂
∂θ

∂
∂ ˜yi

log q(C|X ) d

dθ

˜yi

(cid:105)

x,xy + log det (I + Kx,xD)

(19.5.34)

(19.5.35)

which can be evaluated using the standard results for the derivative of a matrix determinant and inverse.
Since the derivative of Ψ is zero at ˜y, and noting that D depends explicitly on ˜y,

∂
∂ ˜yi

log q(C|X ) = −

1
2

∂
∂ ˜yi

log det (I + Kx,xD)

The implicit derivative is obtained from using the fact that at convergence

˜y = Kx,x (c − σ(y))

to give

d
dθ

˜y = (I + Kx,xD)−1 ∂
∂θ

Kx,x (c − σ)

(19.5.36)

(19.5.37)

(19.5.38)

These results are substituted into equation (19.5.34) to ﬁnd an explicit expression for the derivative.

19.5.4 Multiple classes

The extension of the preceding framework to multiple classes is essentially straightforward and may be
achieved using the softmax function

p(c = m|y) = eym
Σm(cid:48)eym(cid:48)

which automatically enforces the constraint (cid:80)
classes, the cost of implementing the Laplace approximation for the multiclass case scales as O(cid:0)C3N 3(cid:1).
However, one may show by careful implementation that the cost is only O(cid:0)CN 3(cid:1), and we refer the reader

m p(c = m) = 1. Naively it would appear that the for C

(19.5.39)

to [297, 230] for details.

19.6 Further Reading

Gaussian Processes have been heavily developed within the machine learning community over recent years
for which eﬃcient approximations for both regression and classiﬁcation remains an active research topic.
We direct the interested reader to [245] and [230] for further discussion.

19.7 Code

GPreg.m: Gaussian Process Regression
demoGPreg.m: Demo GP regression
covfnGE.m: Gamma-Exponential Covariance function
GPclass.m: Gaussian Process Classiﬁcation
demoGPclass.m: Demo Gaussian Process Classiﬁcation

362

DRAFT March 9, 2010

Exercises

19.8 Exercises

Exercise 191. Show that the sample covariance matrix with elements Sij =(cid:80)N
¯xi =(cid:80)N

i /N, is positive semideﬁnite.

n=1 xn

n=1 xn

i xn

j /N − ¯xi¯xj, where

Exercise 192. Show that

(cid:48)) = e

−| sin(x−x(cid:48))|

k(x − x

is a covariance function.
Exercise 193. Consider the function

f(xi, xj) = e

− 1
2 (xi−xj )2

for one dimensional inputs xi. Show that

f(xi, xj) = e

− 1

2 x2

i exixj e

− 1

2 x2
j

(19.8.3)
− 1
2 (xi−xj )2 is a kernel and ﬁnd an explicit representation

By Taylor expanding the central term, show that e
for the kernel f(xi, xj) as the scalar product of two inﬁnite dimensional vectors.
Exercise 194. Show that for a covariance function k1 (x, x(cid:48)) then

k(x, x(cid:48)) = f(cid:0)k1

(cid:0)x, x(cid:48)(cid:1)(cid:1)

(19.8.4)
is also a covariance function for any polynomial f(x) with positive coeﬃcients. Show therefore that ek1(x,x(cid:48))
and tan (k1 (x, x(cid:48))) are covariance functions.
Exercise 195. For a covariance function

k1(x, x(cid:48)) = f((cid:0)x − x(cid:48)(cid:1)T(cid:0)x − x(cid:48)(cid:1))
k2(x, x(cid:48)) = f((cid:0)x − x(cid:48)(cid:1)T A(cid:0)x − x(cid:48)(cid:1))

show that

(19.8.1)

(19.8.2)

(19.8.5)

(19.8.6)

is also a valid covariance function for a positive deﬁnite symmetric matrix A.
Exercise 196 (String kernel). Let x and x(cid:48) be two strings of characters and φs(x) be the number of times
that substring s appears in string x. Then

wsφs(x)φs(x

(cid:48))

(19.8.7)

(cid:48)) =(cid:88)

k(x, x

s

is a (string kernel) covariance function, provided the weight of each substring ws is positive.

1. Given a collection of strings about politics and another collection about sport, explain how to form a

GP classiﬁer using a string kernel.

2. Explain how the weights ws can be adjusted to improve the ﬁt of the classiﬁer to the data and give an
explicit formula for the derivative with respect to ws of the log marginal likelihood under the Laplace
approximation.

Exercise 197 (Vector regression). Consider predicting a vector output y given training data X ∪ Y =
{xn, yn, n = 1, . . . , n}. To make a GP predictor

(19.8.8)

p(y∗

|x∗

,X ,Y)

we need a Gaussian model

p(y1, . . . , yN , y∗

|x1, . . . , xn, x∗)

A GP requires then a speciﬁcation of the covariance c(ym
two diﬀerent input vectors. Show that under the dimension independence assumption

i , yn

(19.8.9)
j |xn, xm) of the components of the outputs for

c(ym

j |xn, xm) = ci(ym
(19.8.10)
i |xn, xm) is a covariance function for the ith dimension, that separate GP predictors can be

where ci(ym
constructed independently, one for each output dimension i.

i |xn, xm)δij

i , yn
i , yn

i , yn

DRAFT March 9, 2010

363

Exercise 198. Consider the Markov update of a linear dynamical system, section(24.1),

Exercises

xt = Axt−1 + ηt,

where A is a given matrix and ηt is zero mean Gaussian noise with covariance(cid:10)ηi,tηj,t(cid:48)(cid:11) = σ2δijδt,t(cid:48). Also,

(19.8.11)

t ≥ 2

1. Show that x1, . . . , xt is Gaussian distributed.

2. Show that the covariance matrix of x1, . . . , xt has elements

p(x1) = N (x1 0, Σ).
(cid:69)

(cid:68)

xt(cid:48)xT
t

=

(cid:40)

At(cid:48)−tΣ

AtΣ(cid:0)At(cid:1)T

t (cid:54)= t(cid:48)
t = t(cid:48)

(19.8.12)

and explain why a linear dynamical system is a (constrained) Gaussian Process.

3. Consider

yt = Bxt + t

where t is zero mean Gaussian noise with covariance (cid:10)i,tj,t(cid:48)(cid:11) = ν2δijδt,t(cid:48). The vectors  are

(19.8.13)

uncorrelated with the vectors η. Show that the sequence of vectors y1, . . . , yt is a Gaussian Process
with a suitably deﬁned covariance function.

Exercise 199. A form of independent components analysis, section(21.6), of a one-dimensional signal
y1, . . . , yT is obtained from the joint model

(cid:40)(cid:89)
(cid:0)yt w1x1

t

p(y1:T , x1

1:T , x2

1:T|w) =

p(yt|x1

t , x2

t , w)

N

with

p(yt|x1

t , x2

t , w) = N

t , ν2(cid:1)

t + w2x2

(cid:41)

(cid:0)x1
1:T 0, Σ(cid:1)

(cid:0)x2
1:T 0, Σ(cid:1)

N

(19.8.14)

(19.8.15)

where ν2 is a given noise variance. The signal y1:T can be viewed as the linear combination of two
independent Gaussian Processes. The covariance matrices of the two processes have elements from a
stationary kernel,

Σt,t(cid:48) = e

−(t−t(cid:48))2

(19.8.16)

1. Write down an EM algorithm for learning the mixing parameters w1, w2 given an observation se-

quence y1:T .

2. Consider an extension of the above model to the case of two outputs:

p(y1

1:T , y2

1:T , x1

1:T , x2

1:T|W) =

p(yi

t|x1

t , x2

t , wi)

N

(cid:40)(cid:89)

2(cid:89)

i=1

t

(cid:41)

(cid:0)x1
1:T 0, Σ(cid:1)

with

p(y1
p(y2

t , x2
t , x2

t |x1
t |x1

t , W) = N
t , W) = N

(cid:0)y1
(cid:0)y1

t , ν2(cid:1)
t , ν2(cid:1)

t w11x1
t w21x1

t + w12x2
t + w22x2

(cid:0)x2
1:T 0, Σ(cid:1)

N

(19.8.17)

(19.8.18)
(19.8.19)

Show that for T > 1 the likelihood

p(y1

1:T , y2

1:T|W)

(19.8.20)
is not invariant with respect to an orthogonal rotation W(cid:48) = WR, with RRT = I, and explain the
signiﬁcance of this with respect to identifying independent components.

364

DRAFT March 9, 2010

CHAPTER 20

Mixture Models

20.1 Density Estimation Using Mixtures

A mixture model is one in which a set of component models is combined to produce a richer model:

p(v) =

p(v|h)p(h)

(20.1.1)

The variable v is ‘visible’ or ‘observable’ and h = 1, . . . , H indexes each component model p(v|h), along
with its weight p(h). Mixture models have natural application in clustering data, where h indexes the
cluster. This interpretation can be gained from considering how to generate a sample datapoint v from the
model equation (20.1.1). First we sample a cluster h from p(h), and then draw a visible state v from p(v|h).
For a set of i.i.d. data v1, . . . , vN , a mixture model is of the form, ﬁg(20.1),

H(cid:88)

h=1

N(cid:89)

(cid:88)

n=1

hn

p(v1, . . . , vN ) =

p(vn|hn)p(hn)

Clustering is achieved by inference of

argmax
h1,...,hN

p(h1, . . . , hN|v1, . . . , vN )

(20.1.2)

(20.1.3)

which, thanks to the factorised form of the distribution is equivalent to computing arg maxhn p(hn|vn) for
each datapoint. In this way we can cluster many kinds of data for which a ‘distance’ measure in the sense
of the classical K-means algorithm, section(20.3.5), is not directly apparent.

Explicitly writing the dependence on the parameters, the model is

p(v, h|θ) = p(v|h, θv|h)p(h|θh)

(20.1.4)

The optimal parameters θv|h, θh of a mixture model are then most commonly set by Maximum Likelihood,

θopt = argmax

θ

p(v1, . . . , vN|θ)

(20.1.5)

Numerically this can be achieved using an optimisation procedure such as gradient based approaches.
Alternatively, by treating the component indices as latent variables, one may also apply the EM algorithm,
as described in the following section, which in many classical models produces simple update formulae.

365

Expectation Maximisation for Mixture Models

θh

θv|h

hn

vn

N

Figure 20.1: A mixture model has a trivial graphical represen-
tation as a DAG with a single hidden node, which can be in
and one of H states, i = 1 . . . , H. The parameters are assumed
common across all datapoints.

Example 86. The data in ﬁg(20.2) naturally has two clusters and can be modelled with a mixture of
two two-dimensional Gaussians, each Gaussian describing one of the clusters. Here there is a clear visual
interpretation of the meaning of ‘cluster’, with the mixture model placing two datapoints in the same
cluster if they are both likely to be generated by the same model component.

20.2 Expectation Maximisation for Mixture Models

By treating the index h as a missing variable, mixture models can be trained using the EM algorithm,
section(11.2). There are two sets of parameters – θv|h for each component model p(v|h, θv|h) and θh for
the mixture weights p(h|θh). According to the general approach for i.i.d. data of section(11.2), we need
to consider the energy term:

N(cid:88)
N(cid:88)

n=1

n=1

where

E(θ) =

=

(cid:104)log p(vn, h|θ)(cid:105)pold(h|vn)

(cid:10)log p(vn|h, θv|h)(cid:11)

pold(h|vn) +

N(cid:88)

n=1

(cid:104)log p(h|θh)(cid:105)pold(h|vn)

(20.2.1)

(20.2.2)

(20.2.3)

pold(h|vn) ∝ p(vn|h, θold

v|h)p(h|θold
h )

and maximise (20.2.2) with respect to the parameters θv|h, θh, h = 1, . . . , H.

20.2.1 Unconstrained discrete tables

Here we consider training a simple belief networkp(v|h, θv|h)p(h|θh), v = 1, . . . , V , h = 1, . . . , H in
which the tables are unconstrained. This is a special case of the more general framework discussed in
section(11.2).

Figure 20.2: Two dimensional data which displays clusters.
In this case a Gaus-
sian mixture model 1/2N (x m1, C1) + 1/2N (x m2, C2) would ﬁt the data well for
suitable means m1, m2 and covariances C1, C2.

366

DRAFT March 9, 2010

−3−2−10123−3−2−101234Expectation Maximisation for Mixture Models

N(cid:88)

h p(h) = 1. Isolating the dependence of equation (20.2.2) on p(h) we obtain

(cid:104)log p(h)(cid:105)pold(h|vn) =(cid:88)

M-step: p(h)
If no constraint is placed on p(h|θh) we may write the parameters as simply p(h), with the understanding

that 0 ≤ p(h) ≤ 1 and(cid:80)
We now wish to maximise equation (20.2.4) with respect to p(h) under the constraint that(cid:80)

j p(h) = 1.
It is standard to treat this maximisation problem using Lagrange multipliers, see exercise(203). Here we
take an alternative approach based on recognising the similarity of the above to a form of Kullback-Leibler
divergence. First we deﬁne the distribution

pold(h|vn)

N(cid:88)

(20.2.4)

log p(h)

n=1

n=1

h

(cid:80)N
(cid:80)
(cid:80)N
n=1 pold(h|vn)
n=1 pold(h|vn)

h

˜p(h) ≡

Then maximising equation (20.2.4) is equivalent to maximising

(cid:104)log p(h)(cid:105)˜p(h)

since the two expressions are related by the constant factor (cid:80)

(cid:80)N
n=1 pold(h|vn). By subtracting the θ
independent term (cid:104)log ˜p(h)(cid:105)˜p(h) from equation (20.2.6), we obtain the negative Kullback-Leibler divergence
KL(˜p|p). This means that the optimal p(h) is that distribution which minimises the Kullback-Leibler
divergence. Optimally, therefore ˜p(h) = p(h), so that

(20.2.6)

h

H(cid:88)

h=1

λ(h)

(cid:32)
1 −

V(cid:88)

v=1

(cid:33)
p(v|h)

− λ(h = j)p(v = i|h = j) = 0

(cid:88)

N(cid:88)

H(cid:88)

v

n=1

h=1

L ≡

Diﬀerentiating with respect to p(v = i|h = j) and equating to zero,

I [vn = v] pold(h|vn) log p (v|h) +
N(cid:88)

p(v = i|h = j)

I [vn = i] pold(h = j|v = i)
N(cid:88)
I [vn = i] pold(h = j|v = i)
(cid:80)N
(cid:80)V
(cid:80)N
I [vn = i] pold(h = j|v = i)

n=1

n=1

i=1

n=1

I [vn = i] pold(h = j|v = i)

∂L

∂p(v = i|h = j)

Hence

=

n=1

pnew(v = i|h = j) ∝

pnew(v = i|h = j) =

DRAFT March 9, 2010

which, using the normalisation requirement, gives

pnew(h) =

(cid:80)N
(cid:80)N
(cid:80)
n=1 pold(h|vn)
n=1 pold(h|vn)
M-step : p(v|h)
The dependence of equation (20.2.2) on p(v|h) is

N(cid:88)

1
N

n=1

=

h

pold(h|vn)

N(cid:88)

(cid:10)log p(cid:0)vn|h, θv|h
(cid:1)(cid:11)
If the distributions p(cid:0)v|h, θv|h

n=1

pold(h|vn)

(cid:1) are not constrained, we can apply a similar Kullback-Leibler method, as we

(20.2.8)

did in section(11.2). For compatibility with other texts, here we demonstrate the more standard Lagrange
procedure. We need to ensure that p(v|h) is a distribution for each of the mixture states h = 1, . . . , H.
This can be achieved using a set of Lagrange multipliers, giving the Lagrangian:

(20.2.5)

(20.2.7)

(20.2.9)

(20.2.10)

(20.2.11)

(20.2.12)

367

Expectation Maximisation for Mixture Models

θh

θvi|h

hn

vn
i

i = 1, . . . , D

n = 1, . . . , N

Figure 20.3: Mixture of a product of Bernoulli distributions. In
a Bayesian Treatment, a parameter prior is used. In the text we
simply set the parameters using Maximum Likelihood.

E-step
According to the general EM procedure, section(11.2), optimally we set pnew(h|vn) = p(h|vn):

(cid:80)
pnew(h|vn) = p(vn|h)p(h)
h p(vn|h)p(h)

(20.2.13)

Equations (20.2.7,20.2.12,20.2.13) are repeated until convergence. The initialisation of the tables and
mixture probabilities can severely aﬀect the quality of the solution found since the likelihood often has
local optima.
If random initialisations are used, it is recommended to record the converged value of
the likelihood itself, to see which parameters have the higher likelihood. The solution with the highest
likelihood is to be preferred.

20.2.2 Mixture of product of Bernoulli distributions
We describe a simple mixture model that can be used to clustering binary vectors, v = (v1, . . . , vD)T,
vi ∈ {0, 1}. The mixture of Bernoulli products1 model is given by

H(cid:88)

D(cid:89)

p(v) =

p(h)

h=1

i=1

p(vi|h)

(20.2.14)

where each term p(vi|h) is a Bernoulli distribution. The model is depicted in ﬁg(20.3) and has parameters
p(h) and p(vi = 1|h), h = 1 . . . , H.
EM training

To train the model under Maximum Likelihood it is convenient to use the EM algorithm which, as usual,
may be derived by writing down the energy:

n (cid:104)log p(vn, h)(cid:105)pold(h|vn) =(cid:88)
(cid:88)

n

(cid:88)

i

i |h)(cid:105)pold(h|vn) +(cid:88)

n (cid:104)log p(h)(cid:105)pold(h|vn)

(20.2.15)

and then performing the maximisation over the table entries. From our general results, section(11.2), we
may immediately jump to the updates. The M-step is given by

i = 1] pold(h = j|vn) +(cid:80)

I [vn

i = 1] pold(h = j|vn)

I [vn

n

i = 0] pold(h = j|vn)

(cid:104)log p(vn
(cid:80)

n

n

(cid:80)
(cid:80)
I [vn
(cid:80)
h(cid:48)(cid:80)
n pold(h = j|vn)
|vn)
D(cid:89)

n pold(h(cid:48)

p(vn

i |h = j)

i=1

pnew(vi = 1|h = j) =
pnew(h = j) =

and the E-step by

pold(h = j|vn) ∝ p(h = j)

(20.2.16)

(20.2.17)

1This is similar to the Naive Bayes classiﬁer in which the class labels are always hidden.

368

DRAFT March 9, 2010

Expectation Maximisation for Mixture Models

Figure 20.4: Data from questionnaire responses. 150 people were each asked 5 questions, with ‘yes’ (white)
and ‘no’ (gray) answers. Black denotes that the absence of a response (missing data). This training data
was generated by two component Binomial mixture. Missing data was simulated by randomly removing
values from the dataset.

Equations (20.2.16,20.2.17) are iterated until convergence.

If an attribute i is missing for datapoint n, one needs to sum over the states of the corresponding vn
eﬀect of performing the summation for this model is simply to remove the corresponding factor p(vn
from the algorithm, see exercise(200).

i . The
i |h)

Initialisation

The EM algorithm can be very sensitive to initial conditions. Consider the following initialisation: p(vi =
1|h = j) = 0.5, with p(h) set arbitrarily. This means that at the ﬁrst iteration, pold(h = j|vn) = p(h = j).
The subsequent M-step updates are

(cid:48))

(20.2.18)

pnew(h) = p(h),

pnew(vi|h = j) = pnew(vi|h = j

for any j, j(cid:48). This means that the parameters p(v|h) immediately become independent of h and the model

is numerically trapped in a symmetric solution. It makes sense, therefore, to initialise the parameters in
a non-symmetric fashion.

Example 87 (Questionnaire). A company sends out a questionnaire containing a set of D ‘yes/no’ ques-
tions to a set of customers. The binary responses of a customer are stored in a vector v = (v1, . . . , vD)T.
In total N customers send back their questionnaires, v1, . . . , vN , and the company wishes to perform an
analysis to ﬁnd what kinds of customers it has. The company assumes there are H essential types of
customer for which the proﬁle of responses is deﬁned by only the customer type.

Data from a questionnaire containing 5 questions, with 150 respondents is presented in ﬁg(20.4). The data
has a large number of missing values. We assume there are H = 2 kinds of respondents and attempt to
assign each respondent into one of the two clusters. Running the EM algorithm on this data, with random
initial values for the tables, produces the results in ﬁg(20.5). Based on assigning each datapoint vn to the
cluster with maximal posterior probability hn = arg maxh p(h|vn), given a trained model p(v|h)p(h), the
model assigns 86% of the data to the correct cluster (which is known in this simulated case). See ﬁg(20.5)
and MIXprodBern.m.

(a)

(b)

DRAFT March 9, 2010

Figure 20.5: EM learning of a mixture
(a): True p(h)
of Bernoulli products.
(left) and learned p(h) (right) for h = 1, 2.
(b): True p(v|h) (left) and learned p(v|h)
(right) for v = 1, . . . , 5. Each column
pair corresponds to p(vi|h = 1) (red) and
p(vi|h = 2) (blue) with i = 1, . . . , 5. The
learned probabilities are reasonably close
to the true values.

369

20406080100120140123451200.10.20.30.40.50.60.70.80.911200.10.20.30.40.50.60.70.80.911234500.10.20.30.40.50.60.70.80.911234500.10.20.30.40.50.60.70.80.91The Gaussian Mixture Model

Figure 20.6: Top: a selection of 200 of the 5000 handwritten digits in the training set. Bottom: the trained
cluster outputs p(vi = 1|h) for h = 1, . . . , 20 mixtures. See demoMixBernoulliDigits.m.

Example 88 (Handwritten digits). We have a collection of 5000 handwritten digits which we wish to
cluster into 20 groups, ﬁg(20.6). Each digit is a 28× 28 = 784 dimensional binary vector. Using a mixture
of Bernoulli products, trained with 50 iterations of EM (with a random perturbation of the mean of the
data used as initialisation), the clusters are presented in ﬁg(20.6). As we see, the method captures natural
clusters in the data – for example, there are two kinds of 1, one slightly more slanted than the other, two
kinds of 4, etc.

20.3 The Gaussian Mixture Model

Gaussians are particularly convenient continuous mixture components since they constitute ‘bumps’ of
probability mass, aiding an intuitive interpretation of the model. As a reminder, a D dimensional Gaussian
distribution for a continuous variable x is

(cid:26)

1(cid:112)det (2πS)

exp

1
2

−

(x − m)T S−1 (x − m)

(cid:27)

(20.3.1)

where m is the mean and S is the covariance matrix. A mixture of Gaussians is then

p (x|m, S) =
H(cid:88)

p (x) =

where p(i) is the mixture weight for component i. For a set of data X =(cid:8)x1, . . . , xN(cid:9) and under the usual

(20.3.2)

i=1

(cid:26)

exp

1
2

−

(xn − mi)T S−1

i

(cid:27)
(xn − mi)

(20.3.3)

p(x|mi, Si)p(i)
H(cid:88)
N(cid:88)

i.i.d. assumption, the log likelihood is

log p(X|θ) =

1(cid:112)det (2πSi)
matrices, in addition to 0 ≤ p(i) ≤ 1, (cid:80)

p(i)

log

n=1

i=1

where the parameters are θ = {mi, Si, p(i), i = 1, . . . , H}. The optimal parameters θ can be set using
Maximum Likelihood, bearing in mind the constraint that the Si must be symmetric positive deﬁnite
i p(i) = 1. Gradient based optimisation approaches are feasible
under a parameterisation of the Si (e.g. Cholesky decomposition) and p(i) (e.g. softmax) that enforce
the constraints. An alternative is the EM approach which in this case is particularly convenient since it
automatically provides parameter updates that ensure these constraints.

20.3.1 EM algorithm

The energy term is

(cid:104)log p(xn, i)(cid:105)pold(i|xn) =

N(cid:88)

n=1

370

N(cid:88)

n=1

(cid:104)log [p(xn|h)p(i)](cid:105)pold(i|xn)

(20.3.4)

DRAFT March 9, 2010

The Gaussian Mixture Model

(cid:26)

N(cid:88)

n=1

Plugging in the deﬁnition of the Gaussian components, we have

pold(i|xn)

1
2

−

(xn − mi)T S−1

i

(xn − mi) −

1
2

log det (2πSi) + log p(i)

The M-step requires the maximisation of the above with respect to mi, Si, p(i).

M-step : optimal mi
Maximising equation (20.3.5) with respect to mi is equivalent to minimising

(cid:27)

pold(i|xn) (xn − mi)T S−1

i

(xn − mi)

Diﬀerentiating with respect to mi and equating to zero we have

(xn − mi) = 0

Hence, optimally,

−2

mi =

i

n=1

N(cid:88)
pold(i|xn)S−1
(cid:80)N
(cid:80)N
n=1 pold(i|xn)xn
n=1 pold(i|xn)
(cid:80)N
pold(i|xn)
n=1 pold(i|xn)

pold(n|i) ≡

N(cid:88)

n=1

mi =

pold(n|i)xn

By deﬁning the membership distribution

H(cid:88)

i=1

N(cid:88)

H(cid:88)

n=1

i=1

N(cid:88)

(cid:68)

n=1

N(cid:88)

n=1

(20.3.5)

(20.3.6)

(20.3.7)

(20.3.8)

(20.3.9)

(20.3.10)

(20.3.11)

(20.3.12)

(20.3.13)

(20.3.14)

371

which quantiﬁes the membership of datapoints to cluster i, we can write equation (20.3.8) more compactly
as

M-step : optimal Si
Optimising equation (20.3.5) with respect to Si is equivalent to minimising

i − log det(cid:0)S−1

i

pold(i|xn)

(cid:1)(cid:69)
(cid:33)

(∆n

i ∆n

i )TS−1
N(cid:88)

S−1

where ∆n

(cid:32)
i ≡ xn − mi. To aid the matrix calculus, we isolate the dependency on Si to give

− log det(cid:0)S−1

i

(cid:1) N(cid:88)

n=1

pold(i|xn)

i

trace

pold(i|xn)∆n
Diﬀerentiating with respect to S−1

n=1

i

i (∆n

i )T

and equating to zero, we obtain

N(cid:88)

n=1

i (∆n

i )T − Si

pold(i|xn) = 0

pold(i|xn)∆n
N(cid:88)

Using the membership pold(n|i), the optimal Si is given by

Si =

n=1

pold(n|i) (xn − mi) (xn − mi)T

DRAFT March 9, 2010

The Gaussian Mixture Model

Figure 20.7: Training a mixture of 10 isotropic
Gaussians (a): If we start with large variances
for the Gaussians, even after one iteration, the
Gaussians are centred close to the mean of the
(b): The Gaussians begin to separate
data.
(c): One by one, the Gaussians move towards
appropriate parts of the data (d): The ﬁnal
converged solution. The Gaussians are con-
strained to have variances greater than a set
amount. See demoGMMem.m.

(a) 1 iteration

(b) 50 iterations

(c) 125 iterations

(d) 150 iterations

This ensures that Si is symmetric positive semi-deﬁnite. A special case is to constrain the covariances Si
to be diagonal for which the update is, see exercise(201),

N(cid:88)

n=1

Si =

pold(n|i)diag

(cid:16)

(xn − mi) (xn − mi)T(cid:17)

where above diag (M) means forming a new matrix from the matrix M with zero entries except for the
diagonal entries of M. A more extreme case is that of isotropic Gaussians Si = σ2
i I. The reader may show
that the optimal update for σ2
in this case is given by taking the average of the diagonal entries of the
i
diagonally constrained covariance update,

pold(n|i)(xn − mi)2

(20.3.16)

(20.3.15)

(20.3.17)

(20.3.18)

(20.3.19)

If no constraint is placed on the weights, the update follows the general formula given in equation (20.2.7),

N(cid:88)

n=1

σ2
i =

1

dim x

M-step : optimal mixture coeﬃcients

N(cid:88)

n=1

pold(i|xn)

p(i) =

1
N

E-step

Explicitly, this is given by the responsibility

pold(i|xn) ∝ pold(xn|i)p(i)
(cid:110)
(cid:110)
2(xn − mi)T S−1
− 1
2(xn − mi(cid:48))T S−1
− 1

p(i) exp
i(cid:48) p(i(cid:48)) exp

pold(i|xn) =

(cid:80)

i

(cid:111)
(cid:111)
(xn − mi)
i(cid:48) (xn − mi(cid:48))

The above equations (20.3.8,20.3.14,20.3.17,20.3.19) are iterated until convergence.

The performance of EM for Gaussian mixtures can be strongly dependent on the initialisation, which we
discuss below.
In addition, constraints on the covariance matrix are required in order to ﬁnd sensible
solutions.

372

DRAFT March 9, 2010

The Gaussian Mixture Model

20.3.2 Practical issues

Inﬁnite troubles

A diﬃculty arises with using Maximum Likelihood to ﬁt a Gaussian mixture model. Consider placing a
component p(x|mi, Si) with mean mi set to one of the datapoints mi = xn. The contribution from that
Gaussian will be

p(xn|mi, Si) =

1(cid:112)det (2πSi)

2 (xn−xn)TS−1
− 1
e

i

(xn−xn) =

1(cid:112)det (2πSi)

(20.3.20)

In the limit that the ‘width’ of the covariance goes to zero (the eigenvalues of Si tend to zero), this
probability density becomes inﬁnite. This means that one can obtain a Maximum Likelihood solution by
placing zero-width Gaussians on a selection of the datapoints, resulting in an inﬁnite likelihood. This is
clearly undesirable and arises because, in this case, the Maximum Likelihood solution does not constrain
the parameters in a sensible way. Note that this is not related to the EM algorithm, but a property of the
Maximum Likelihood method itself. All computational methods which aim to ﬁt unconstrained mixtures of
Gaussians using Maximum Likelihood therefore succeed in ﬁnding ‘reasonable’ solutions merely by getting
trapped in favourable local maxima. A remedy is to include an additional constraint on the width of the
Gaussians, ensuring that they cannot become too small. One approach is to monitor the eigenvalues of
each covariance matrix and if an update would result in a new eigenvalue smaller than a desired threshold,
the update is rejected. In GMMem.m we use a similar approach in which we constrain the determinant (the
product of the eigenvalues) of the covariances to be greater than a desired speciﬁed minimum value. One
can view the formal failure of Maximum Likelihood in the case of Gaussian mixtures as a result of an
inappropriate prior. Maximum Likelihood is equivalent to MAP in which a ﬂat prior is placed on each
matrix Si. This is unreasonable since the matrices are required to be positive deﬁnite and of non-vanishing
width. A Bayesian solution to this problem is possible, placing a prior on covariance matrices. The natural
prior in this case is the Wishart Distribution, or a Gamma distribution in the case of a diagonal covariance.

Initialisation

A useful intialisation strategy is to set the covariances to be diagonal with large variances. This gives the
components a chance to ‘sense’ where data lies. An illustration of the performance of the algorithm is
given in ﬁg(20.7).

Symmetry breaking

If the covariances are initialised to large values, the EM algorithm appears to make little progress in the ﬁrst
iterations as each component jostles with the others to try to explain the data. Eventually one Gaussian
component breaks away and takes responsibility for explaining the data in its vicinity, see ﬁg(20.7). The
origin of this jostling is an inherent symmetry in the solution – it makes no diﬀerence to the likelihood if we
relabel what the components are called. This permutation symmetry causes initial confusion as to which
model should explain which parts of the data. Eventually, this symmetry is broken, and a local solution
is found. The symmetries can severely handicap EM in ﬁtting a large number of models in the mixture
since the number of permutations increases dramatically with the number of components. A heuristic is
to begin with a small number of components, say two, for which symmetry breaking is less problematic.
Once a local broken solution has been found, more models are included into the mixture, initialised close
to the currently found solutions. In this way, a hierarchical breaking scheme is envisaged. Another popular
method for initialisation is to center the means to those found by the K-means algorithm – however, this
itself requires a heuristic initialisation.

20.3.3 Classiﬁcation using Gaussian mixture models
Consider data drawn from two classes, c ∈ {1, 2}. We can ﬁt a GMM p(x|c = 1,X1) to the data X1
from class 1, and another GMM p(x|c = 2,X2) to the data X2 from class 2. This gives rise to two
class-conditional GMMs,

H(cid:88)

i=1

p(x|c,Xc) =

p(i|c)N (x mc

i , Sc
i)

DRAFT March 9, 2010

(20.3.21)

373

The Gaussian Mixture Model

(a)

(b)

(c)

Figure 20.8: (a): A Gaussian mixture model with H = 4 components. There is a component (purple)
with large variance and small weight that has little eﬀect on the distribution close to where the other three
components have appreciable mass. As we move further away this additional component gains in inﬂuence.
(b): The GMM probability density function from (a).
(c): Plotted on a log scale, the inﬂuence of each
Gaussian far from the origin becomes clearer.

For a novel point x∗, the posterior class probability is

p(c|x∗

,X ) ∝ p(x∗

|c,Xc)p(c)

(20.3.22)

where p(c) is the prior class probability. The Maximum Likelihood setting is that p(c) is proportional to
the number of training points in class c.
Consider a testpoint x∗ a long way from the training data for both classes. For such a point, the proba-
bility that either of the two class models generated the data is very low. However, one will be much lower
than the other (since Gaussians drop exponentially quickly), meaning that the posterior probability will
be conﬁdently close to 1 for that class which has a component closest to x∗. This is an unfortunate prop-
erty since we would end up conﬁdently predicting the class of novel data that is not similar to anything
we’ve seen before. We would prefer the opposite eﬀect that for novel data far from the training data, the
classiﬁcation conﬁdence drops and all classes become equally likely.

A remedy for this situation is to include an additional component in the Gaussian mixture for each class
that is very broad. We ﬁrst collect the input data from all classes into a dataset X , and let m be the mean
of all this data and S the covariance. Then for the model of each class c data we include an additional
Gaussian (dropping the notational dependency on X )

p(x|c) =

iN (x mc
˜pc

i , Sc

i) + ˜pc

H+1N (x m, λS)

H(cid:88)
(cid:26) pc

i=1

i

δ

where

˜pc
i ∝

i ≤ H
i = H + 1

(20.3.23)

(20.3.24)

where δ is a small positive value and λ inﬂates the covariance (we take δ = 0.0001 and λ = 10 in
demoGMMclass.m). The eﬀect of the additional component on the training likelihood is negligible since
it has small weight and large variance compared to the other components, see ﬁg(20.8). However, as we
move away from the region where the ﬁrst H components have appreciable mass, the additional component
gains in inﬂuence since it has a higher variance. If we include the same additional component in the GMM
for each class c then the inﬂuence of this additional component will be the same for each class, dominating
as we move far from the inﬂuence of the other components. For a point far from the training data the
likelihood will be roughly equal for each class since in this region the additional broad component dominates
with equal measure. The posterior distribution will then tend to the prior class probability p(c), mitigating
the deleterious eﬀect of a single GMM dominating when a testpoint is far from the training data.

374

DRAFT March 9, 2010

−10−8−6−4−2024681000.050.10.150.20.25−10−8−6−4−2024681000.050.10.150.20.25−10−8−6−4−20246810−180−160−140−120−100−80−60−40−200The Gaussian Mixture Model

(a)

(b)

Figure 20.9: Class conditional GMM train-
ing and classiﬁcation. (a): Data from two
diﬀerent classes. We ﬁt a GMM with two
components to the data from each class.
The (magenta) diamond is a test point
far from the training data we will clas-
(b): Upper subpanel are the class
sify.
probabilities p(c = 1|n) for the 40 train-
ing points, and the 41st point, being the
test point. The lower subpanel are the
class probabilities but including the addi-
tional large variance Gaussian term. See
demoGMMclass.m.

Example 89. The data in ﬁg(20.9a) has a cluster structure for each class. Based on ﬁtting a GMM
to each of the two classes, a test point (diamond) far from the training data is conﬁdently classiﬁed as
belonging to class 1. This is an undesired eﬀect since we would prefer that points far from the training
data are not classiﬁed with any certainty. By including an additional large variance Gaussian component
for each class this has little eﬀect on the class probabilities of the training data, yet has the desired eﬀect
of making the class probability for the test point maximally uncertain, ﬁg(20.9b).

20.3.4 The Parzen estimator
The Parzen density estimator is formed by placing a ‘bump of mass’, ρ(x|xn), on each datapoint,

A popular choice is (for a D dimensional x)

n=1

1
N

p(x) =

ρ(x|xn)

N(cid:88)
(cid:0)x xn, σ2ID
ρ(x|xn) = N
N(cid:88)

(cid:1)

p(x) =

1
N

giving the mixture of Gaussians

1

2σ2 (x−xn)2
− 1

(2πσ2)D/2 e

n=1

(20.3.25)

(20.3.26)

(20.3.27)

(20.3.28)

There is no training required for a Parzen estimator – only the positions of the N datapoints need storing.
Whilst the Parzen technique is a reasonable and cheap way to form a density estimator, it does not enable
us to form any simpler description of the data. In particular, we cannot perform clustering since there is
no lower number of clusters assumed to underly the data generating process. This is in contrast to GMMs
trained using Maximum Likelihood on a ﬁxed number H ≤ N of components.
20.3.5 K-Means

Consider a mixture of K isotropic Gaussians in which each covariance is constrained to be equal to σ2I,

K(cid:88)

i=1

p(x) =

piN

(cid:0)x mi, σ2I(cid:1)

Whilst the EM algorithm breaks down if a Gaussian component is allowed to set mi equal to a datapoint
with σ2 → 0, by constraining all components to have the same variance σ2, the algorithm has a well
375
DRAFT March 9, 2010

−8−6−4−20246810−6−4−2024681012051015202530354000.20.40.60.81051015202530354000.20.40.60.81Algorithm 19 K-means

The Gaussian Mixture Model

1: Initialise the centres mi, i = 1, . . . , K.
2: while not converged do
3:
4:
5:

For each centre i, ﬁnd all the xn for which i is the nearest (in Euclidean sense) centre.
Call this set of points Ni. Let Ni be the number of datapoints in set Ni.
Update the means

(cid:88)

n∈Ni

xn

mnew

i =

1
Ni

6: end while

Figure 20.10: (a): 550 datapoints
clustered using K-means with 3 com-
ponents. The means are given by the
(b): Evolution of the
red crosses.
mean square distance to nearest cen-
tre with iterations of the algorithm.
The means were initialised to close
to the overall mean of the data. See
demoKmeans.m.

(a)

(b)

(cid:26) 1

0

deﬁned limit as σ2 → 0. The reader may show, exercise(202), that in this case the membership distribution
equation (20.3.9) becomes deterministic

pold(n|i) ∝

if mi is closest to xn
otherwise

(20.3.29)

In this limit the EM update (20.3.10) for the mean mi is given by taking the average of the points closest to
mi. This limiting and constrained GMM then reduces to the so-called K-means algorithm, algorithm(19).
Despite its simplicity the K-means algorithm converges quickly and often gives a reasonable clustering,
provided the centres are initialised reasonably. See ﬁg(20.10).

K-means is often used as a simple form of data compression. Rather than sending the datapoint xn, one
sends instead the index of the centre to which it is associated. This is called vector quantisation and is
a form of lossy compression. To improve the quality, more information can be transmitted such as an
approximation of the diﬀerence between x and the corresponding mean m, which can be used to improve
the reconstruction of the compressed datapoint.

20.3.6 Bayesian mixture models

Bayesian extensions include placing priors on the parameters of each model in the mixture, and also on the
component distribution. In most cases this will give rise to the marginal likelihood being an intractable
integral. Methods that approximate the integral include sampling techniques [98]. See also [109, 67] for
an approximate variational treatment focussed on Bayesian Gaussian mixture models.

20.3.7 Semi-supervised learning

In some cases we may know to which mixture component certain datapoints belong. Given this information
we want to ﬁt a mixture model with a speciﬁed number of components H and parameters θ. We write
(vm∗ , hm∗ ), m = 1, . . . , M for the M known datapoints and corresponding components, and (vn, hn), n =
1, . . . , N for the remaining datapoints whose components hn are unknown. We aim then to maximise the

376

DRAFT March 9, 2010

−8−6−4−202468−4−20246811.522.533.544.550510152025Mixture of Experts

n = 1, . . . , N

xn

yn

hn

W

U

Figure 20.11: Mixture of experts model. The prediction of
the output yn (real or continuous) given the input xn aver-
ages over individual experts p(yn|xn, whn). The expert hn is
selected by the gating mechanism with probability p(hn|xn, U),
so that some experts will be more able to predict the output
for xn in ‘their’ part of the space. The parameters W, U can
be learned by Maximum Likelihood after marginalising over the
hidden expert variables.

likelihood

p(v1:M∗

, v1:N|h1:M∗

, θ) =(cid:89)

m

p(vm∗ |hm∗ , θ)(cid:89)

n

(cid:88)

hn

p(vn|hn, θ)p(hn)

(20.3.30)

If we were to lump all the datapoints together, this is essentially equivalent to the standard unsupervised
case, expect that some of the h are ﬁxed into known states. The only eﬀect on the EM algorithm is therefore
in the terms pold(h|v) which are delta functions in the known state, resulting in a minor modiﬁcation of
the standard algorithm, exercise(205).

20.4 Mixture of Experts

The mixture of experts model[149] is related to discriminative training of an output y distribution con-
ditioned on an input x. This can be used in either the regression of classiﬁcation contexts and has the
general form, see ﬁg(20.11),

p(y|x, W, U) =

p(y|x, wh)p(h|x, U),

(20.4.1)

Here h indexes the mixture component. Each expert has parameters W = [w1, . . . , wH] and corresponding
gating parameters U = [u1, . . . , uH]. Unlike a standard mixture model, the component distribution
p(h|x, U) is dependent on the input x. This so-called gating distribution is conventionally taken to be of
the softmax form

H(cid:88)

h=1

hx(cid:80)
p(h|x, U) = euT
h euT
hx

(20.4.2)

The idea is that we have a set of H predictive models (experts), p(y|x, wh), each with a diﬀerent param-
eter wh, h = 1, . . . , H. How suitable model h is for predicting with the current input x is determined
by the alignment of input x with the weight vector uh. In this way the input x is softly assigned to the
appropriate experts.

Maximum Likelihood training can be achieved using a form of EM. We will not derive the EM algorithm
for the mixture of experts model in full, merely pointing the direction along which the derivation would
continue. For a single datapoint x, the EM energy term is

(cid:104)log p(y|x, wh)p(h|x, U)(cid:105)p(h|x,Wold,Uold)

For regression a simple choice is

(cid:16)

y xTwh, σ2(cid:17)

p(y|x, wh) = N

and for (binary) classiﬁcation

p(y = 1|x, wh) = σ(xTwh)

DRAFT March 9, 2010

(20.4.3)

(20.4.4)

(20.4.5)

377

In both cases computing the derivatives of the energy with respect to the parameters W is straightforward,
so that an EM algorithm is readily available. An alternative to EM is to compute the gradient of the
likelihood directly using the standard approach discussed in section(11.7).

Indicator Models

A Bayesian treatment is to consider

(cid:90)

(cid:88)

p(y|x) =

where it is conventional to assume p(W) = (cid:81)

p(y|x, wh)p(h|x, u)p(W)p(U)

h p(wh), p(U) = (cid:81)

W,U

h

h p(uh). The integrals are generally
intractable and approximations are required. See [290] for a variational treatment for regression and [43]
for a variational treatment of classiﬁcation. An extension to Bayesian model selection in which the number
of experts is estimated is considered in [143].

(20.4.6)

20.5 Indicator Models

In the indicator approach we specify a distribution over the cluster assignments. For consistency with the
literature we use an indicator z, as opposed to a hidden variable h, although they play the same role. A
clustering model with parameters θ on the component models and joint indicator prior p(z1:N ) takes the
form

(20.5.1)

(20.5.2)

(20.5.3)

(20.5.4)

p(v1:N|θ) =(cid:88)
p(v1:N|θ) =(cid:88)

z1:N

z1:N

p(v1:N|z1:N , θ)p(z1:N )

N(cid:89)

n=1

p(z1:N )

p(vn|zn, θ)

Since the zn indicate cluster membership,

Below we discuss the role of diﬀerent indicator priors p(z1:N ) in clustering.

20.5.1 Joint indicator approach: factorised prior

Assuming prior independence of indicators,

N(cid:89)

n=1

p(z1:N ) =

p(zn),

we obtain from equation (20.5.2)

p(v1:N|θ) =(cid:88)

N(cid:89)

z1:N

n=1

zn ∈ {1, . . . , K}
N(cid:89)

(cid:88)

p(vn|zn, θ)p(zn) =

p(vn|zn, θ)p(zn)

n=1

zn

which recovers the standard mixture model equation (20.1.2). As we discuss below, more sophisticated
joint indicator priors can be used to explicitly control the complexity of the indicator assignments and
open the path to essentially ‘inﬁnite dimensional’ models.

20.5.2 Joint indicator approach : Polya prior
For a large number of available clusters (mixture components) K (cid:29) 1, using a factorised joint indicator
distribution could potentially lead to overﬁtting, resulting in little or no meaningful clustering. One way
to control the eﬀective number of components that are used is via a parameter π that regulates the
complexity,

(cid:90)

(cid:40)(cid:89)

π

n

(cid:41)
p(zn|π)

p(z1:N ) =

378

p(π)

(20.5.5)

DRAFT March 9, 2010

Indicator Models

z1

v1

zN

vN

z1

v1

θ

(a)

zN

vN

π

θ

(b)

π

zn

vn

θ

(c)

N

Figure 20.12: (a): A generic mixture model for
data v1:N . Each zn indicates the cluster of each
datapoint. θ is a set of parameters and zn = k
selects parameter θk for datapoint vn. (b): For
a potentially large number of clusters one way
to control complexity is to constrain the joint
(c): Plate notation of
indicator distribution.
(b).

(a)

(b)

(c)

Figure 20.13: The number of unique clusters U when indicators are sampled from a Polya distribution
(a): K = 50, (b): K = 100, (c): K = 1000.
equation (20.5.8), with α = 2, and N = 50 datapoints.
Even though the number of available clusters K is larger than the number of datapoints, the eﬀective
number of used clusters remains constrained. See demoPolya.m.

where p(z|π) is a categorical distribution,

p(zn = k|π) = πk

(20.5.6)

A convenient choice for p(π) is the Dirichlet distribution (since this is conjugate to the categorical distri-
bution),

α/K−1
π
k

(20.5.7)

K(cid:89)

k=1

p(π) = Dirichlet (π|α) ∝
K(cid:89)

Γ(α)

(cid:88)
The number of unique clusters used is then given by U = (cid:80)

Γ(Nk + α/K)

p(z1:N ) =

Γ(N + α)

Γ(α/K)

k=1

,

Nk ≡

n

The integral over π in equation (20.5.5) can be performed analytically to give a Polya distribution:

I [zn = k]

(20.5.8)

I [Nk > 0]. The distribution over likely
cluster numbers is controlled by the parameter α. The scaling α/K in equation (20.5.7) ensures a sensible
limit as K → ∞, see ﬁg(20.13), in which limit the models are known as Dirichlet process mixture models.
This approach means that we do not need to explicitly constrain the number of possible components K
since the number of active components U remains limited even for very large K.

k

Clustering is achieved by considering argmax

z1:N

p(z1:N|v1:N ). In practice it is common to consider

argmax

zn

p(zn|v1:N )

(20.5.9)

Unfortunately, posterior inference of p(zn|v1:N ) for this class of models is formally computationally in-
tractable and approximate inference techniques are required. A detailed discussion of these techniques is
beyond the scope of this book and we refer the reader to [165] for a deterministic (variational) approach
and [206] for a discussion of sampling approaches.

DRAFT March 9, 2010

379

051015200501001502002500510152005010015020025005101520050100150200250Mixed Membership Models

Figure 20.14: Latent Dirichlet Allocation. For document n we ﬁrst sample
a distribution of topics πn. Then for each word position w in the document
we sample a topic zn
w from the topic distribution. Given the topic we then
sample a word from the word distribution of that topic. The parameters of
the model are the word distributions for each topic θ, and the parameters
of the topic distribution α.

α

πn

zn
w

vn
w

Wn

N

θ

20.6 Mixed Membership Models

Unlike standard mixture models in which each object is assumed to have been generated from a single
cluster, in mixed membership models an object may be a member of more than one group. Latent Dirichlet
Allocation discussed below is an example of such a mixed membership model, and is one of a number of
models developed in recent years [4, 91].

20.6.1 Latent Dirichlet allocation

So far we’ve considered clustering in the sense that each observation is assumed to have been generated
from a single cluster.
In contrast, Latent Dirichlet Allocation[44] and related methods are generative
mixed membership models in which each datapoint may belong to more than a single cluster. A typical
application is to identify topic clusters in a collection of documents. A single document contains a sequence
of words, for example

v = (the, cat, sat, on, the, cat, mat)

(20.6.1)

vn =(cid:0)vn

(cid:1) ,

If each word in the available dictionary is assigned to a unique state (say dog = 1, tree = 2, cat = 3, . . .),
we can represent then the nth document as a vector

1 , . . . , vn
Wn

vn
i ∈ {1, . . . , D}

(20.6.2)

where Wn is the number of words in the nth document. The number of words Wn in each document can
vary although the overall dictionary from which they came is ﬁxed.

The aim is to ﬁnd common topics in documents, assuming that any document could potentially contain
more than one topic. It is useful to think ﬁrst of an underlying generative model of words, including latent
topics (which we will later integrate out). We ﬁrst sample a probability distribution (histogram) that
represents the topics likely to occur for this document. Then, for each word-position in the document,
sample a topic and subsequently a word from the distribution of words for that topic. Mathematically, for
document n and the wth word-position in the document, vn
w ∈ {1, . . . , K} to indicate which of
the K possible topics that word belongs. For each topic k, one then has a categorical distribution over all
the words i = 1, . . . , D, in the dictionary:

w, we use zn

p(vn

w = i|zn

w = k, θ) = θi|k

distribution of topics πn with(cid:80)K

The ‘animal’ topic has high probability to emit animal-like words, etc. For each document n we have a
k = 1 which gives a latent description of the document in terms of
its topic membership. For example, document n (which discusses issues related to wildlife conservation)
might have a topic distribution with high mass on the latent ‘animals’ and ‘environment’, topics. Note
that the topics are indeed latent – the name ‘animal’ would be given post-hoc based on the kinds of words

k=1 πn

(20.6.3)

380

DRAFT March 9, 2010

Mixed Membership Models

that the latent topic would generate, θi|k. As in section(20.5.2), to control complexity one may use a
Dirichlet prior to limit the number of topics active in any particular document:

p(πn|α) = Dirichlet (πn|α)

where α is a vector of length the number of topics.

(20.6.4)

A generative model for sampling a document vn with Wn word positions is:

1. Choose πn ∼ Dirichlet (πn|α)
2. For each of word position vn

w, w = 1, . . . , Wn :

(a) Choose a topic zn
(b) Choose a word vn

w ∼ p (zn

w ∼ p(cid:0)vn

(cid:1)

w|πn)
w|θ·|zn

w

Training the LDA model corresponds to learning the parameters α, which relates to the number of topics,
and θ, which describes the distribution of words within each topic. Unfortunately, ﬁnding the requisite
marginals for learning from the posterior is formally computationally intractable. Eﬃcient approximate
inference for this class of models is a topic of research interest and both variational and sampling ap-
proaches have recently been developed[44, 273, 225].

There are close similarities between LDA and PLSA[113], section(15.6.1), both of which describe a doc-
ument in terms of a distribution over latent topics. LDA is a probabilistic model for which issues such
as setting hyperparameters can be addressed using Maximum Likelihood. PLSA on the other hand is
essentially a matrix decomposition technique (such as PCA). Issues such as hyperparameters setting for
PLSA are therefore addressed using validation data. Whilst PLSA is a description only of the training
data, LDA is a generative data model and can in principle be used to synthesise new documents.

Example 90. An illustration of the use of LDA is given in ﬁg(20.15)[44]. The documents are taken from
the TREC Associated Press corpus containing 16,333 newswire articles with 23,075 unique terms. After
removing a standard list of stop words (frequent words such as ‘the’,‘a’ etc. that would otherwise dominate
the statistics), the EM algorithm (with variational approximate inference) was used to ﬁnd the Dirichlet
and conditional categorical parameters for a 100-topic LDA model. The top words from four resulting
categorical distributions θi|k are illustrated ﬁg(20.15a). These distributions capture some of the underlying
topics in the corpus. An example document from the corpus is presented along with the words coloured
by the most probable latent topic they correspond to.

20.6.2 Graph based representations of data

Mixed membership models are used in a variety of contexts and are distinguished also by the form of data
available. Here we focus on analysing a representation of the interactions amongst a collection of objects;
in particular, the data has been processed such that all the information of interest is characterised by an
interaction matrix. For graph based representations of data, two objects are similar if they are neighbours
on a graph representing the data objects. In the ﬁeld of social-networks, for example, each individual is
represented as a node in a graph, with a link between two nodes if the individuals are friends. Given a
graph one might wish to identify communities of closely linked friends. Interpreted as a social network, in
ﬁg(20.16a), individual 3 is a member of his work group (1, 2, 3) and also the poker group (3, 4, 5). These
two groups of individuals are otherwise disjoint. Discovering such groupings contrasts with graph parti-
tioning in which each node is assigned to only one of a set of subgraphs, ﬁg(20.16b), for which a typical
criterion is that each subgraph should be roughly of the same size and that there are few connections
between the subgraphs[156].

DRAFT March 9, 2010

381

Mixed Membership Models

Arts
new
ﬁlm
show
music
movie
play
musical
best
actor
ﬁrst
york
opera
theater
actress
love

Budgets
million
tax
program
budget
billion
federal
year
spending
new
state
plan
money
programs
government
congress

Children Education
children
women
people
child
years
families
work
parents
says
family
welfare
men
percent
care
life

school
students
schools
education
teachers
high
public
teacher
bennett
manigat
namphy
state
president
elementary
haiti

(a)

The William Randolph Hearst Foundation will give $ 1.25 million to Lin-
coln Center, Metropolitan Opera Co., New York Philharmonic and Juil-
liard School. Our board felt that we had a real opportunity to make a mark
on the future of the performing arts with these grants an act every bit as
important as our traditional areas of support in health, medical research,
education and the social services, Hearst Foundation President Randolph
A. Hearst said Monday in announcing the grants. Lincoln Centers share
will be $200,000 for its new building, which will house young artists and
provide new public facilities. The Metropolitan Opera Co. and New York
Philharmonic will receive $400,000 each. The Juilliard School, where music
and the performing arts are taught, will get $250,000. The Hearst Foun-
dation, a leading supporter of the Lincoln Center Consolidated Corporate
Fund, will make its usual annual $100,000 donation, too.

(b)

Figure 20.15: (a): A subset of the latent topics discovered by LDA and the high probability words
associated with each topic. Each column represents a topic, with the topic name such as ‘art’ assigned by
hand after viewing the most likely words corresponding to the topic. (b): A document from the training
data in which the words are coloured according to the most likely latent topic. This demonstrates the
mixed-membership nature of the model, assigning the datapoint (document in this case) to several clusters
(topics). Reproduced from [44].

1

2

3

(a)

4

5

1

2

3

(b)

4

5

Figure 20.16: (a) The social network of a set of 5 individuals, repre-
sented as an undirected graph. Here individual 3 belongs to the group
(1, 2, 3) and also (3, 4, 5). (b) By contrast, in graph partitioning, one
breaks the graph into roughly equally sized disjoint partitions such
that each node is a member of only a single partition, with a minimal
number of edges between partitions.

Another example is that nodes in the graph represent products and a link between nodes i and j indicates
that customers who by product i frequently also buy product j. The aim is to decompose the graph into
groups, each corresponding to products that are commonly co-bought by customers[116]. A growing area
of application of graph based representations is in bioinformatics in which nodes represent genes, and
a link between them representing that the two genes have similar activity proﬁles. The task is then to
identify groups of similarly behaving genes[5].

20.6.3 Dyadic data

Consider two kinds of objects, for example, ﬁlms and customers. Each ﬁlm is indexed by f = 1, . . . , F
and each user by u = 1, . . . , U. The interaction of user u with ﬁlm f can be described by the element of a
matrix Muf representing the rating a user gives to a ﬁlm. A dyadic dataset consists of such a matrix and
the aim is to decompose this matrix to explain the ratings by ﬁnding types of ﬁlms and types of user.

Another example is to consider a collection of documents, summarised by an interaction matrix in which
Mwd is 1 if word w appears in document d and zero otherwise. This matrix can be represented as a
bipartite graph, as in ﬁg(20.17a). The upper nodes represent documents, and the lower nodes words,
with a link between them if that word occurs in that document. One the seeks assignments of documents
to groups or latent ‘topics’ to succinctly explain the link structure of the bipartite graph via a small
number of latent nodes, as schematically depicted in ﬁg(20.17b). One may view this as a form of matrix
factorisation[136, 189]

(cid:88)

t

Mwd ≈

UwtV T
td

(20.6.5)

where t indexes the topics and the feature matrices U and V control the word-to-topic mapping and the
topic-to-document mapping. This diﬀers from latent Dirichlet allocation which has a probabilistic inter-
pretation of ﬁrst generating a topic and then a word, conditional on the chosen topic. Here the interaction
between document-topic matrix V and word-topic matrix U is non-probabilistic. More generally, we can

382

DRAFT March 9, 2010

(a): There are
Figure 20.17: Graphical representation of dyadic data.
6 documents and 13 words. A link represents that a particular word-
document pair occurs in the dataset. (b): A latent decomposition of (a)
using 3 ‘topics’. A topic corresponds to a collection of words, and each
document a collection of topics. The open nodes indicate latent variables.

where U and V are feature matrices. In [189], real-valued data is modelled using

(cid:16)

(cid:17)

p(M|U, W, V) = N

M UWVT, σ2I

where U and V are assumed binary and the real-valued W is a topic-interaction matrix. In this viewpoint
learning then consists of inferring U,W,V, given the dyadic observation matrix M. Assuming factorised
priors, the posterior over the matrices is

p(U, W, V|M) ∝ p(M|U, W, V)p(U)p(W)p(V)

(20.6.8)

A convenient choice is a Gaussian prior distribution for W, with the feature matrices U and V sampled
from Beta-Bernoulli priors. The resulting posterior distribution is formally computationally intractable,
and in [189] this is addressed using a sampling approximation.

20.6.4 Monadic data

In monadic data there is only one type of object and the interaction between the objects is represented
by a square interaction matrix. For example one might have a matrix with elements Aij = 1 if proteins i
and j can bind to each other and 0 otherwise. A depiction of the interaction matrix is given by a graph
in which an edge represents an interaction, for example ﬁg(20.18). In the following section we discuss a
particular mixed membership model and highlight potential applications. The method is based on clique
decompositions of graphs and as such we require a short digression into clique-based graph representations.

20.6.5 Cliques and adjacency matrices for monadic binary data
A symmetric adjacency matrix has elements Aij ∈ {0, 1}, with a 1 indicating a link between nodes i and
j. For the graph in ﬁg(20.18), the adjacency matrix is

(20.6.6)

(20.6.7)

(20.6.9)

Mixed Membership Models

(a)

(b)

write a distribution

p(M|U, V)

 1 1

1 1
1 1
0 1

A =



1
1
1
1

0
1
1
1

where we include self connections on the diagonal. Given A, our aim is to ﬁnd a ‘simpler’ description that
reveals the underlying cluster structure, such as (1, 2, 3) and (2, 3, 4) in ﬁg(20.18). Given the undirected
graph in ﬁg(20.18), the incidence matrix Finc is an alternative description of the adjacency structure[81].

1

2

3

4

DRAFT March 9, 2010

Figure 20.18: The minimal clique cover is (1, 2, 3), (2, 3, 4).

383

Mixed Membership Models

1

2

3

4

1

2

3

4

(a)

(b)

Figure 20.19: Bipartite representations of the decomposi-
tions of ﬁg(20.18). Shaded nodes represent observed vari-
ables, and open nodes latent variables. (a) Incidence ma-
trix representation. (b) Minimal clique decomposition.

Given the V nodes in the graph, we construct Finc as follows: For each link i ∼ j in the graph, form a
column of the matrix Finc with zero entries except for a 1 in the ith and jth row. The column ordering is
arbitrary. For example, for the graph in ﬁg(20.18) an incidence matrix is

The incidence matrix has the property that the adjacency structure of the original graph is given by the
outer product of the incidence matrix with itself. The diagonal entries contain the degree (number of
links) of each node. For our example, this gives

 1 1

1 0
0 1
0 0



0 0 0
1 1 0
1 0 1
0 1 1

Finc =



1
1
3
1

0
1
1
2

 2 1
(cid:17)

1 3
1 1
0 1

FincFT
inc

FincFT

inc =

so that

A = H

(cid:16)

(20.6.10)

(20.6.11)

(20.6.12)

(20.6.13)

(20.6.14)

Here H(·) is the element-wise Heaviside step function, [H(M)]ij = 1 if Mij > 0 and is 0 otherwise. A
useful viewpoint of the incidence matrix is that it identiﬁes two-cliques in the graph (here we are using
the term ‘clique’ in the non-maximal sense). There are ﬁve 2-cliques in ﬁg(20.18), and each column of
Finc speciﬁes which elements are in each 2-clique. Graphically we can depict this incidence decomposition
as a bipartite graph, as in ﬁg(20.19a) where the open nodes represent the ﬁve 2-cliques. The incidence
matrix can be generalised to describe larger cliques. Consider the following matrix as a decomposition for
ﬁg(20.18), and its outer-product:

 1 0

1 1
1 1
0 1

,

F =

FFT =

 1 1

1 2
1 2
0 1



1
2
2
1

0
1
1
1

The interpretation is that F represents a decomposition into two 3-cliques. As in the incidence matrix,
each column represents a clique, and the rows containing a ‘1’ express which elements are in the clique
deﬁned by that column. This decomposition can be represented as the bipartite graph of ﬁg(20.19b). For
the graph of ﬁg(20.18), both Finc and F satisfy

(cid:16)

FFT(cid:17)

(cid:16)

A = H

= H

FincFT
inc

(cid:17)

One can view equation (20.6.14) as a form of binary matrix factoristation of the binary square (symmetric)
matrix A into non-square binary matrices. For our clustering purposes, the decomposition using F is to
be preferred to the incidence decomposition since F decomposes the graph into a smaller number of larger
cliques. A formal speciﬁcation of the problem of ﬁnding a minimum number of maximal fully-connected
subsets is the computational problem MIN CLIQUE COVER[103, 251]. Indeed, F solves MIN CLIQUE COVER
for ﬁg(20.18).

384

DRAFT March 9, 2010

Mixed Membership Models

Figure 20.20: The function σ(x) ≡
β increases, this sigmoid function tends to a step function.

(cid:0)1 + eβ(0.5−x)(cid:1)−1 for β = 1, 10, 100. As

(cid:2)FFT(cid:3)

ii express the number of cliques/columns that node i occurs in. Oﬀ-diagonal elements (cid:2)FFT(cid:3)

Deﬁnition 106 (Clique matrix). Given an adjacency matrix [A]ij , i, j = 1, . . . , V (Aii = 1), a clique
matrix F has elements Fic ∈ {0, 1} , i = 1, . . . , V, c = 1, . . . , C such that A = H(FFT). Diagonal elements
contain the number of cliques/columns that nodes i and j jointly inhabit [18].

ij

Whilst ﬁnding a clique decomposition F is easy (use the incidence matrix for example), ﬁnding a clique
decomposition2 with the minimal number of columns, i.e. solving MIN CLIQUE COVER, is NP-Hard[103, 10].

A generative model of adjacency matrices

Solving MIN CLIQUE COVER is a computationally hard problem and approximations are in general unavoid-
able. Below we relax the strict clique requirement and assume that provided only a small number of links
in an ‘almost clique’ are missing, this may be considered a suﬃciently well-connected group of nodes to
form a cluster.

Given an adjacency matrix A and a prior on clique matrices F, our interest is the posterior

p(F|A) ∝ p(A|F)p(F)

(20.6.15)
We ﬁrst concentrate on the generative term p(A|F). To ﬁnd ‘well-connected’ clusters, we relax the con-
straint that the decomposition is in the form of cliques in the original graph and view the absence of links
as statistical ﬂuctuations away from a perfect clique. Given a V × C matrix F, we desire that the higher
the overlap between rows3 fi and fj is, the greater the probability of a link between i and j. This may be
achieved using, for example,

p(Aij = 1|F) = σ

(cid:16)

(cid:17)
1 + eβ(0.5−x)(cid:17)−1

fif T
j

(cid:16)

with

σ(x) ≡

(20.6.16)

(20.6.17)

where β controls the steepness of the function, see ﬁg(20.20). The 0.5 shift in equation (20.6.17) ensures
that σ approximates the step-function since the argument of σ is an integer. Under equation (20.6.16), if
j − 0.5 > 0 and p(Aij = 1|F) is high. Absent links
fi and fj have at least one ‘1’ in the same position, fif T
contribute p(Aij = 0|F) = 1 − p(Aij = 1|F). The parameter β controls how strictly σ(FFT) matches A;
for large β, very little ﬂexibility is allowed and only cliques will be identiﬁed. For small β, subsets that
would be cliques if it were not for a small number of missing links, are clustered together. The setting of
β is user and problem dependent.

Assuming each element of the adjacency matrix is sampled independently from the generating process,
the joint probability of observing A is (neglecting its diagonal elements),

p(A|F) =(cid:89)

i∼j

(cid:16)

(cid:17)(cid:89)

(cid:16)

i(cid:54)∼j

σ

fif T
j

1 − σ

fif T
j

(cid:16)

(cid:17)(cid:17)

(20.6.18)

The ultimate quantity of interest is the posterior distribution of clique structure, equation (20.6.15), for
which we now specify a prior p(F) over clique matrices.

3We use lower indices fi to denote the ith row of F.

DRAFT March 9, 2010

385

−2−1.5−1−0.500.511.5200.20.40.60.81xMixed Membership Models

Figure 20.21: (a): Adjacency ma-
trix of 105 Political Books (black=1).
(b): Clique matrix: 521 non-zero en-
tries. (c): Adjacency reconstruction
using an approximate clique matrix
with 10 cliques – see also ﬁg(20.22)
and demoCliqueDecomp.m.

(a)

(b)

(c)

Figure 20.22: Political Books. 105 × 10 dimensional clique matrix broken into 3 groups by a politically
astute reader. A black square indicates q(fic) > 0.5. Liberal books (red), Conservative books (green),
Neutral books(yellow). By inspection, cliques 5,6,7,8,9 largely correspond to ‘conservative’ books.

Clique matrix prior p(F)

Since we are interested in clustering, ideally we want to place as many nodes in the graph as possible in
a cluster. This means that we wish to bias the contributions to the adjacency matrix A to occur from a
small number of columns of F. To achieve this we ﬁrst reparameterise F as

F =(cid:0)α1f 1, . . . , αCmaxf Cmax(cid:1)

(20.6.19)

(20.6.21)

where αc ∈ {0, 1} play the role of indicators and f c is column c of F. Cmax is an assumed maximal number
of clusters. Ideally, we would like to ﬁnd an F with a low number of indicators α1, . . . , αCmax in state 1.
To achieve this we deﬁne a prior distribution on the binary hypercube α = (α1, . . . , αCmax),

ναc (1 − ν)1−αc

(20.6.20)

To encourage a small number of the α(cid:48)
ensure that ν is less than 0.5. This gives rise to a Beta-Bernoulli distribution

cs to be 1, we use a Beta prior p(ν) with suitable parameters to

p(α|ν) =(cid:89)
(cid:90)

c

p(α) =

p(α|ν)p(ν) = B(a + N, b + Cmax − N)
where B(a, b) is the beta function and N =(cid:80)Cmax

B(a, b)

ν

c=1 αc is the number of indicators in state 1. To encourage
that only a small number of components should be active, we set a = 1, b = 3 (which corresponds to a
mean ν of 0.25 and variance 0.0375. The distribution (20.6.21) is on the vertices of the binary hypercube
{0, 1}Cmax with a bias towards vertices close to the origin (0, . . . , 0). Through equation (20.6.19), the prior
on α induces a prior on F. The resulting distribution p(F, α|A) ∝ p(F|α)p(α) is formally intractable and
in [18] this is addressed using a variational technique. See cliquedecomp.c and cliquedecomp.m.

Clique matrices also play a natural role in the parameterisation of positive deﬁnite matrices, see exercise(206)[18].

386

DRAFT March 9, 2010

20406080100204060801002040608010012014020406080100204060801002040608010005101000 Years for Revenge  Bush vs. the Beltway  Charlie Wilson’s War  Losing Bin Laden  Sleeping With the Devil  The Man Who Warned America  Why America Slept  Ghost Wars  A National Party No More  Bush Country  Dereliction of Duty  Legacy  Off with Their Heads  Persecution  Rumsfeld’s War  Breakdown  Betrayal  Shut Up and Sing  Meant To Be  The Right Man  Ten Minutes from Normal  Hillary’s Scheme  The French Betrayal of America  Tales from the Left Coast  Hating America  The Third Terrorist  Endgame  Spin Sisters  All the Shah’s Men  Dangerous Dimplomacy  The Price of Loyalty  House of Bush, House of Saud  The Death of Right and Wrong  Useful Idiots  The O’Reilly Factor  Let Freedom Ring  Those Who Trespass  Bias  Slander  The Savage Nation  Deliver Us from Evil  Give Me a Break  The Enemy Within  The Real America  Who’s Looking Out for You?  The Official Handbook Vast Right Wing Conspiracy  Power Plays  Arrogance  The Perfect Wife  The Bushes  Things Worth Fighting For  Surprise, Security, the American Experience  Allies  Why Courage Matters  Hollywood Interrupted  Fighting Back  We Will Prevail  The Faith of George W Bush  Rise of the Vulcans  Downsize This!  Stupid White Men  Rush Limbaugh Is a Big Fat Idiot  The Best Democracy Money Can Buy  The Culture of Fear  America Unbound  The Choice  The Great Unraveling  Rogue Nation  Soft Power  Colossus  The Sorrows of Empire  Against All Enemies  American Dynasty  Big Lies  The Lies of George W. Bush  Worse Than Watergate  Plan of Attack  Bush at War  The New Pearl Harbor  Bushwomen  The Bubble of American Supremacy  Living History  The Politics of Truth  Fanatics and Fools  Bushwhacked  Disarming Iraq  Lies and the Lying Liars Who Tell Them  MoveOn’s 50 Ways to Love Your Country  The Buying of the President 2004  Perfectly Legal  Hegemony or Survival  The Exception to the Rulers  Freethinkers  Had Enough?  It’s Still the Economy, Stupid!  We’re Right They’re Wrong  What Liberal Media?  The Clinton Wars  Weapons of Mass Deception  Dude, Where’s My Country?  Thieves in High Places  Shrub  Buck Up Suck Up  The Future of Freedom  Empire  Exercises

Example 91 (Political Books Clustering). The data consists of 105 books on US politics sold by the
online bookseller Amazon. The adjacency matrix with element Aij = 1 ﬁg(20.21a), represents fre-
quent co-purchasing of books i and j (Valdis Krebs, www.orgnet.com). Additionally, books are la-
belled ‘liberal’,
‘neutral’, or ‘conservative’ according to the judgement of a politically astute reader
(www-personal.umich.edu/∼mejn/netdata/). The interest is to assign books to clusters, using A alone,
and then see if these clusters correspond in some way to the ascribed political leanings of each book. Note
that the information here is minimal – all that is known to the clustering algorithm is which books were
co-bought (matrix A); no other information on the content or title of the books are exploited by the
algorithm. With an initial Cmax = 200 cliques, Beta parameters a = 1, b = 3 and steepness β = 10, the
most probably posterior marginal solution contains 142 cliques ﬁg(20.21b), giving a perfect reconstruction
of the adjacency A. For comparison, the incidence matrix has 441 2-cliques. However, this clique matrix
is too large to provide a compact interpretation of the data – indeed there are more clusters than books.
To cluster the data more aggressively, we ﬁx Cmax = 10 and re-run the algorithm. This results only in
an approximate clique decomposition, A ≈ H(FFT), as plotted in ﬁg(20.21c). The resulting 105 × 10
approximate clique matrix is plotted in ﬁg(20.22) and demonstrates how individual books are present in
more than one cluster. Interestingly, the clusters found only on the basis of the adjacency matrix have
some correspondence with the ascribed political leanings of each book; cliques 5, 6, 7, 8, 9 correspond to
largely ‘conservative’ books. Most books belong to more than a single clique/cluster, suggesting that they
are not single topic books, consistent with the assumption of a mixed membership model.

20.7 Further Reading

The literature on mixture modelling is extensive, and a good overview and entrance to the literature is
contained in [188].

20.8 Code

MIXprodBern.m: EM training of a Mixture of product Bernoulli distributions
demoMixBernoulli.m: Demo of a Mixture of product Bernoulli distributions

GMMem.m: EM training of a mixture of Gaussians
GMMloglik.m: GMM log likelihood
demoGMMem.m: Demo of a EM for mixture of Gaussians
demoGMMclass.m: Demo GMM for classiﬁcation

Kmeans.m: K-means
demoKmeans.m: Demo of K-means

demoPolya.m: Demo of the number of active clusters from a Polya distribution
dirrnd.m: Dirichlet random distribution generator

cliquedecomp.m: Clique Matrix decomposition
cliquedecomp.c: Clique Matrix decomposition (C-code)
DemoCliqueDecomp.m: Demo clique matrix decomposition

20.9 Exercises

p(v) =(cid:88)

p(h)(cid:89)

Exercise 200. Consider a mixture of factorised models

h

i

DRAFT March 9, 2010

p(vi|h)

(20.9.1)

387

N(cid:88)

n=1

N(cid:88)

n=1

show that, optimally

N(cid:88)

n=1

p(h) =

1
N

(cid:33)

(cid:32)
1 −

(cid:88)

For assumed i.i.d. data vn, n = 1, . . . , N, some observation components may be missing so that, for example
the third component of the ﬁfth datapoint, v5
3 is unknown. Show that Maximum Likelihood training on the
observed data corresponds to ignoring components vn

i that are missing.

Exercise 201. Derive the optimal EM update for ﬁtting a mixture of Gaussians under the constraint that
the covariances are diagonal.

Exercises

Exercise 202. Consider a mixture of K isotropic Gaussians, each with the same covariance, Si = σ2I.
In the limit σ2 → 0 show that the EM algorithm tends to the K-means clustering algorithm.
Exercise 203. Consider the term

(cid:104)log p(h)(cid:105)pold(h|vn)

(20.9.2)

We wish to optimise the above with respect to the distribution p(h). This can be achieved by deﬁning the
Lagrangian

L =

(cid:104)log p(h)(cid:105)pold(h|vn) + λ

By diﬀerentiating the Lagrangian with respect to p(h) and using the normalisation constraint(cid:80)

p(h)

h

(20.9.3)

h p(h) = 1,

pold(h|vn)

(20.9.4)

Exercise 204. We showed that ﬁtting an unconstrained mixture of Gaussians using Maximum Likelihood is
problematic since, by placing one of the Gaussians over a datapoints and letting the covariance determinant
go to zero, we obtain an inﬁnite likelihood. In contrast, when ﬁtting a single Gaussian N (x µ, Σ) to i.i.d.
data x1, x2, . . . , xN show that the Maximum Likelihood optimum for Σ has non-zero determinant, and that
the optimal likelihood remains ﬁnite.

Exercise 205. Modify GMMem.m suitably so that it can deal with the semi-supervised scenario in which the
mixture component h of some of the observations v is known.

Exercise 206. You wish to parameterise covariance matrices S under the constraint that speciﬁed elements
are zero. The constraints are speciﬁed using a matrix A with elements Aij = 0 if Sij = 0 and Aij = 1
otherwise. Consider a clique matrix Z, for which

A = H(ZZT)

and matrix

S∗ = Z∗ZT∗

with

[Z∗]ij =

(cid:26) 0

θij

if Zij = 0
if Zij = 1

(20.9.5)

(20.9.6)

(20.9.7)

for parameters θ. Show that for any θ, S∗ is positive semideﬁnite and parameterises covariance matrices
under the zero constraints speciﬁed by A.

388

DRAFT March 9, 2010

CHAPTER 21

Latent Linear Models

21.1 Factor Analysis

In chapter(15) we discussed Principal Components Analysis which forms lower dimensional representa-
tions of data based on assuming that the data lies close to a hyperplane. Here we describe a related
probabilistic model for which extensions to Bayesian methods can be envisaged. Any probabilistic model
may also be used as a component of a larger more complex model, such as a mixture model, enabling
natural generalisations.

We use v to describe a real data vector to emphasise that this is a visible (observable) quantity. The
dataset is then given by a set of vectors,

V =(cid:8)v1, . . . , vN(cid:9)

(21.1.1)

where dim (v) = D. Our interest is to ﬁnd a lower dimensional probabilistic description of this data. If
data lies close to a H-dimensional hyperplane we may accurately approximate each datapoint by a low
H-dimensional coordinate system. In general, datapoints will not lie exactly on the hyperplane and we
model this discrepancy with Gaussian noise. Mathematically, the FA model generates an observation v
according to

v = Fh + c + 

where the noise  is Gaussian distributed,

 ∼ N ( 0, Ψ)

(21.1.2)

(21.1.3)

The constant bias c sets the origin of the coordinate system. The factor loading matrix F plays a similar
role as the basis matrix in PCA, see section(15.2). Similarly, the hidden coordinates h plays the role of
the components we used in section(15.2).

The diﬀerence between PCA and Factor Analysis is in the choice of Ψ:

Probabilistic PCA

Ψ = σ2I

Factor Analysis

Ψ = diag (ψ1, . . . , ψD)

389

(21.1.4)

(21.1.5)

Factor Analysis

h1

h2

h3

v1

v2

v3

v4

v5

Figure 21.1: Factor Analysis. The visible vector variable v is related
to the vector hidden variable h by a linear mapping, with independent
additive Gaussian noise on each visible variable. The prior on the
hidden variable may be taken to be an isotropic Gaussian, thus being
independent across its components.

A probabilistic description

From equation (21.1.2) and equation (21.1.3), given h, the data is Gaussian distributed with mean Fh + c
and covariance Ψ

p (v|h) = N (v Fh + c, Ψ) ∝ e

− 1
2 (v−Fh−c)TΨ−1(v−Fh−c)

(21.1.6)

To complete the model, we need to specify the hidden distribution p(h). A convenient choice is a Gaussian

p (h) = N (h 0, I) ∝ e

−hTh/2

(21.1.7)

Under this prior the coordinates h will be preferentially concentrated around values close to 0.
If we
sample a h from p(h) and then draw a value for v using p(v|h), the sampled v vectors would produce a
saucer or ‘pancake’ of points in the v space. Using a correlated Gaussian prior p(h) = N (h 0, ΣH) has no
eﬀect on the complexity of the model since ΣH can be absorbed into F, exercise(209). Since v is linearly
related to h through equation (21.1.2) and both  and h are Gaussian, then v is Gaussian distributed.
The mean and covariance can be computed using the propagation results in section(8.6.3):

(cid:90)

(cid:16)

(cid:17)

p (v) =

p (v|h) p (h) dh = N

v c, FFT + Ψ

(21.1.8)

Invariance of the likelihood under factor rotation

Since the matrix F only appears in the ﬁnal model p(v) through FFT + Ψ, the likelihood is unchanged if
we rotate F using FR, with RRT = I:

FR(FR)T + Ψ = FRRTFT + Ψ = FFT + Ψ

(21.1.9)

The solution space for F is therefore not unique – we can arbitrarily rotate the matrix F and produce
an equally likely model of the data. Some care is therefore required when interpreting the entries of F.
Varimax provides a more interpretable F by using a suitable rotation matrix R. The aim is to produce
a rotated F for which each column has only a small number of large values. Finding a suitable rotation
results in a non-linear optimisation problem and needs to be solved numerically. See [185] for details.

21.1.1 Finding the optimal bias
For a set of data V and using the usual i.i.d. assumption, the log likelihood is

N(cid:88)

n=1

log p(vn) = −

1
2

N(cid:88)

n=1

(vn − c)T Σ−1

D (vn − c) −

N
2

log det (2πΣD)

(21.1.10)

(21.1.11)

log p(V|F, Ψ) =

where

ΣD ≡ FFT + Ψ

N(cid:88)

n=1

c =

1
N

390

Diﬀerentiating equation (21.1.10) with respect to c and equating to zero, we arrive at the Maximum
Likelihood optimal setting that the bias c is the mean of the data,

vn ≡ ¯v

(21.1.12)

DRAFT March 9, 2010

Factor Analysis : Maximum Likelihood

(a)

(b)

(a): 1000 latent two-dimensional
Figure 21.2: Factor Analysis: 1000 points generated from the model.
points hn sampled from N (h 0, I). These are transformed to a point on the three-dimensional plane by
xn
0 = c + Fhn. The covariance of x0 is degenerate, with covariance matrix FFT. (b): For each point xn
0
on the plane a random noise vector is drawn from N ( 0, Ψ) and added to the in-plane vector to form a
sample xn, plotted in red. The distribution of points forms a ‘pancake’ in space. Points ‘underneath’ the
plane are not shown.

We will use this setting throughout. With this setting the log likelihood equation (21.1.10) can be written

log p(V|F, Ψ) = −

N
2

(cid:0)trace(cid:0)Σ−1

D S(cid:1) + log det (2πΣD)(cid:1)

where S is the sample covariance matrix

N(cid:88)

n=1

S =

1
N

(v − ¯v) (v − ¯v)T

21.2 Factor Analysis : Maximum Likelihood

We now specialise to the assumption that Ψ = diag (ψ1, . . . , ψD). We consider two methods for learning
the factor loadings F: a ‘direct’ approach1, section(21.2.1) and an EM approach, section(21.2.2).

21.2.1 Direct likelihood optimisation

Optimal F for ﬁxed Ψ

To ﬁnd the maximum likelihood setting of F we diﬀerentiate the log likelihood equation (21.1.13) with
respect to F and equate to zero. This gives

0 = trace(cid:0)Σ−1

D (∂FΣD)Σ−1

D S(cid:1)

− trace(cid:0)Σ−1

D ∂FΣ(cid:1)

Using ∂F(FFT) = F(∂FFT) + (∂FF)FT, a stationary point is given when

Σ−1
D F = Σ−1

D SΣ−1
D F

Since ΣD is invertible, the optimal F satisﬁes

F = SΣ−1
D F

Using the deﬁnition of ΣD, equation (21.1.11), one can rewrite Σ−1

D F as (see exercise(210))

(cid:16)

(cid:17)−1

Σ−1
D F = Ψ−1F

I + FTΨ−1F

1The presentation here follows closely that of [302].

DRAFT March 9, 2010

(21.1.13)

(21.1.14)

(21.2.1)

(21.2.2)

(21.2.3)

(21.2.4)

391

−4−20246−4−20246−2−101234−4−20246−4−20246−2−101234Factor Analysis : Maximum Likelihood

Plugging this into the zero derivative condition, equation (21.2.3) becomes

(cid:16)

(cid:17)

F

I + FTΨ−1F

= SΨ−1F

Using the reparameterisations

equation (21.2.5) can be written in the ‘isotropic’ form

˜S = Ψ− 1

2 SΨ− 1

2

˜F ≡ Ψ− 1
2 F,
(cid:16)

I + ˜FT ˜F

˜F

(cid:17)

= ˜S˜F

We assume that the transformed factor matrix ˜F has a thin SVD decomposition

˜F = UHLVT

where dim UH = D × H, dim L = H × H, dim V = H × H and

(21.2.5)

(21.2.6)

(21.2.7)

(21.2.8)

UT

HUH = IH ,

(21.2.9)
and L = diag (l1, . . . , lH) are the singular values of ˜F. Plugging this assumption into equation (21.2.7) we
obtain

VTV = IH

IH + VL2VT(cid:17)
UHLVT(cid:16)
(cid:0)IH + L2(cid:1) = ˜SUH ,

UH

which gives

= ˜SUHLVT

L2 = diag(cid:0)l2

1, . . . , l2
H

(cid:1)

(21.2.10)

(21.2.11)

Equation(21.2.11) is then an eigen-equation for UH. Intuitively, it’s clear that we need to ﬁnd then the
eigendecomposition of ˜S and then set the columns of UH to those eigenvectors corresponding to the largest
eigenvalues. This is derived more formally below.

Determining the appropriate eigenvalues
We can relate the form of the solution to the eigen-decomposition of ˜S

˜S = UΛUT,

U = [UH|Ur]

(21.2.12)

where Ur are arbitrary additional columns chosen to complete UH to form an orthogonal U, UTU =
i = λi, or li = √λi − 1. Given
UUT = I. Using Λ = diag (λ1, . . . , λD), equation (21.2.11) stipulates 1 + l2
the solution for ˜F, the solution for F is found from equation (21.2.6). To determine the optimal λi we
write the log likelihood in terms of the λi as follows. Using the new parameterisation,

(21.2.13)

(21.2.14)

+ log det (2πΨ)

(21.2.15)

(21.2.16)

DRAFT March 9, 2010

ΣD = Ψ 1

2

Ψ 1

2

(cid:16)˜F˜FT + I
(cid:17)
D S(cid:1) = trace

2 , we have

and S = Ψ 1

2 ˜SΨ 1

trace(cid:0)Σ−1

2
N

−

log p(V|F, Ψ) = trace

The log likelihood equation (21.1.10) in this new parameterisation is

(cid:18)(cid:16)˜F˜FT + ID
(cid:18)(cid:16)

(cid:19)
(cid:17)−1 ˜S
ID + ˜F˜FT(cid:17)−1 ˜S

(cid:19)

(cid:16)

ID + ˜F˜FT(cid:17)

+ log det

Using λi = 1 + l2

i , and equation (21.2.8) we can write

ID + ˜F˜FT = ID + UHL2UT

H = Udiag (λ1, . . . , λH , 1, . . . , 1) UT

392

(cid:26) λi

1

(cid:48)
i =

λ

i ≤ H
i > H

=(cid:88)

i

λi
λ(cid:48)

i

,

log λi

(21.2.17)

(21.2.18)

Using this we can write the log likelihood as a function of the eigenvalues (for ﬁxed Ψ) as

To maximise the likelihood we need to minimise the right hand side of the above. Since log λ < λ we

i log λi term. A solution for ﬁxed Ψ is therefore

λi + log det (2πΨ)

(21.2.19)

Factor Analysis : Maximum Likelihood

so that

trace

Similarly

log det

(cid:18)(cid:16)
(cid:19)
ID + ˜F˜FT(cid:17)−1 ˜S
ID + ˜F˜FT(cid:17)
(cid:16)
H(cid:88)
H(cid:88)

=

i=1

D(cid:88)
should place the largest H eigenvalues in the(cid:80)

log p(V|F, Ψ) =

log λi + H +

2
N

−

i=1

i=H+1

F = Ψ 1

2 UH (ΛH − IH) 1

2 R

where

ΛH ≡ diag (λ1, . . . , λH)

are the H largest eigenvalues of Ψ− 1
R is an arbitrary orthogonal matrix.

2 SΨ− 1

2 , with UH being the matrix of the corresponding eigenvectors.

SVD based approach
rather than ﬁnding the eigen-decomposition of Ψ− 1
considering the thin SVD decomposition of

2 SΨ− 1

2 we can avoid forming the covariance matrix by

(21.2.20)

(21.2.21)

(21.2.22)

(21.2.23)

(21.2.25)

(21.2.26)

where the data matrix is

˜X =

1
√N

Ψ− 1

2 X

(cid:2)x1, . . . , xN(cid:3)

X ≡

Given a thin decomposition

˜X = UH ˜ΛVT

we obtain the eigenvalues λi = ˜Λ2
using this SVD method is O
SVD methods are available[49].

Finding the optimal Ψ

(cid:16)

S − FFT(cid:17)
S − FFT(cid:17)
(cid:16)

Ψ = diag

Ψnew = diag

(cid:16)

min (H, N)3(cid:17)

(21.2.24)
ii. For D > N this is convenient since the computational complexity
. When the matrix X is too large to store in memory, online

The zero derivative of the log likelihood occurs when

where F is given by equation (21.2.20). There is no closed form solution to equations(21.2.25, 21.2.20). A
simple iterative scheme is to ﬁrst guess values for the diagonal entries of Ψ and then ﬁnd the optimal F
using equation (21.2.20). Subsequently Ψ is updated using

We update F using equation (21.2.20) and Ψ using equation (21.2.26) until convergence.

Alternative schemes for updating the noise matrix Ψ can improve convergence considerably. For ex-
ample updating only a single component of Ψ with the rest ﬁxed can be achieved using a closed form
expression[302].

DRAFT March 9, 2010

393

21.2.2 Expectation maximisation

A popular way to train Factor Analysis in machine learning is to use EM. We assume that the bias c has
been optimally set to the data mean ¯v.

Factor Analysis : Maximum Likelihood

M-step

As usual, we need to consider the energy which, neglecting constants, is

E (F, Ψ) = −

(dn − Fh)T Ψ−1 (dn − Fh)

q(h|vn) −

N
2

log det (Ψ)

(21.2.27)

N(cid:88)

(cid:28)1

2

n=1

(cid:29)

where dn ≡ vn − ¯v. The optimal variational distribution q(h|vn) is determined by the E-step below.
Maximising E (F, Ψ) with respect to F gives

Fnew = AH−1

where

A ≡

1
N

(cid:88)

n

Finally

Ψnew =

1
N

1
N

H ≡

(cid:68)
hhT(cid:69)
(cid:88)
(dn − Fh) (dn − Fh)T(cid:69)

n

q(h|vn)

q(h|vn) ,

dn (cid:104)h(cid:105)T
(cid:88)

diag

n

(cid:18)(cid:68)

q(h|vn)

(cid:19)

= diag

(cid:32)

(cid:88)

n

1
N

(21.2.28)

(21.2.29)

(cid:33)

dn(dn)T − 2FAT + FHFT

(21.2.30)

E-step

The above recursions depend on the statistics (cid:104)h(cid:105)q(h|vn) and(cid:10)hhT(cid:11)

for the E-step we have

q(h|vn). Using the EM optimal choice

with

q(h|vn) ∝ p(vn|h)p(h) = N (h mn, Σ)
(cid:17)−1

(cid:16)

I + FTΨ−1F

FTΨ−1dn,

(cid:16)

Σ =

I + FTΨ−1F

(cid:17)−1

Using these results we can express the statistics in equation (21.2.29) as

mn = (cid:104)h(cid:105)q(h|vn) =
(cid:88)

H = Σ +

1
N

n

mn(mn)T

(21.2.31)

(21.2.32)

(21.2.33)

Equations (21.2.28,21.2.30,21.2.32) are iterated till convergence. As for any EM algorithm, the likelihood
equation (21.1.10) (under the diagonal constraint on Ψ) increases at each iteration. Convergence using
this EM technique can be slower than that of the direct eigen-approach of section(21.2.1) and commercial
implementations usually avoid EM for this reason. Provided however that a reasonable initialisation is
used, the performance of the two training algorithms can be similar. A useful initialisation is to use PCA
and then set F to the principal directions.

Mixtures of FA

An advantage of probabilistic models is that they may be used as components in more complex models, such
as mixtures of FA[275]. Training can then be achieved using EM or gradient based approaches. Bayesian
extensions are clearly of interest; whilst formally intractable they can be addressed using approximate
methods, for example [98, 178, 109].

394

DRAFT March 9, 2010

Interlude: Modelling Faces

G
¯x12

¯x13

f1

¯x11

G

¯x21

f2

µ

¯x22

origin

θ

F

Figure 21.3: Latent Identity Model. The mean µ rep-
resents the mean of the faces. The subspace F repre-
sents the directions of variation of diﬀerent faces so that
f1 = µ + Fh1 is a mean face for individual 1, and similarly
for f2 = µ+Fh2. The subspace G denotes the directions of
variability for any individual face, caused by pose, lighting
etc. This variability is assumed the same for each person.
A particular mean face is then given by the mean face of
the person plus pose/illumination variation, for example
¯x12 = f1 + Gw12. A sample face is then given by a mean
face ¯xij plus Gaussian noise from N (ij 0, Σ).

wij

hi

xij

J

I

Figure 21.4: The jth image of the ith person, xij, is modelled
using a linear latent model with parameters θ.

21.3 Interlude: Modelling Faces

Factor Analysis has widespread application in statistics and machine learning. As an inventive application
of FA, highlighting the probabilistic nature of the model, we describe a face modelling technique that has as
its heart a latent linear model[228]. Consider a gallery of face images X = {xij, i = 1, . . . , I; j = 1, . . . , J}
so that the vector xij represents the jth image of the ith person. As a latent linear model of faces we
consider

xij = µ + Fhi + Gwij + ij

(21.3.1)
Here F (dim F = D × F ) is used to model variability between people, and G (dim G = D × G) models
variability related to pose, illumination etc. within the diﬀerent images of each person. The contribution

fi ≡ µ + Fhi

accounts for variability between diﬀerent people, being constant for individual i. For ﬁxed i, the contri-
bution

Gwij + ij

(21.3.3)

accounts for the variability over the images of person i, explaining why two images of the same person do
not look identical. See ﬁg(21.3) for a graphical representation. As a probabilistic linear latent variable
model, we have for an image xij:

(21.3.2)

(21.3.4)

(21.3.5)

(21.3.6)

(21.3.7)

395

The parameters are θ = {F, G, µ, Σ}. For the collection of images, assuming i.i.d. data,

p(xij|hi, wij, θ) = N (xij µ + Fhi + Gwij, Σ)
p(hi) = N (hi 0, I) ,
p(wij) = N (wij 0, I)
I(cid:89)

p(xij|hi, wij, θ)p(wij)

 J(cid:89)

j=1

i=1

 p(hi)

for which the graphical model is depicted in ﬁg(21.4). The task of learning is then to maximise the
likelihood

p(X , w, h|θ) =
(cid:90)

p(X|θ) =

p(X , w, h|θ)

w,h

DRAFT March 9, 2010

Interlude: Modelling Faces

(a) Mean

(b) Variance

(c)

(d)

(e)

(f)

(g)

(h)

Figure 21.5: Latent Identity Model of face images. Each image is represented by a 70 × 70 × 3 vector
(the 3 comes from the RGB colour coding). There are I = 195 individuals in the database and J = 4
(b): Per pixel standard deviation – black is low, white is
images per person.
(f,g,h): Three samples from
high.
the model with h ﬁxed and drawing randomly from w in the within-individual subspace G. Reproduced
from [228].

(c,d,e): Three directions from the between-individual subspace F.

(a): Mean of the data.

This model can be seem as a constrained version of Factor Analysis by using stacked vectors (here for only
a single individual, I = 1)



x11
x12
...
x1J

 =

 µ

µ
...
µ

 +



F G 0 . . . 0
F 0 G . . . 0
...
...
...
...
0 . . . G
F 0

...





h1
w11
w12
...
w1J

 +

 11

12
...
1J



(21.3.8)

The generalisation to multiple individuals I > 1 is straightforward. The model can be trained using either
a constrained form of the direct method, or EM as described in [228]. Example images from the trained
model are presented in ﬁg(21.5).

Recognition

In closed set face recognition a new ‘probe’ face x∗ is to be matched to a person n in the gallery of training
faces. In model Mn the nth gallery face is forced to share its latent identity variable hn with the test face,
indicating that these faces belong to the same person2. Assuming a single exemplar per person (J = 1),

I(cid:89)

i=1,i(cid:54)=n

p(x1, . . . , xI , x∗|Mn) = p(xn, x∗)

p(xi)

(21.3.9)

Bayes’ rule then gives the posterior class assignment

p(Mn|x1, . . . , xI , x∗) ∝ p(x1, . . . , xI , x∗|Mn)p(Mn)

(21.3.10)
For a uniform prior, the term p(Mn) is constant and can be neglected. The quantities we require above
are given by

p(xn) =

p(xn, hn, wn),

p(x∗) =

p(x∗, h∗, w∗)

(21.3.11)

(cid:90)

hnwn

(cid:90)

h∗w∗

2This is analogous to Bayesian outcome analysis in section(13.5) in which the hypotheses assume that either the errors

were generated from the same or a diﬀerent model.

396

DRAFT March 9, 2010

Probabilistic Principal Components Analysis

h1

x1

h2

x2

h1

x1

x∗

h2

x2

w1

w2

w∗

w1

w2

(a) M1

(b) M2

(cid:90)

hnwn,w∗

p(xn, x∗) =

x∗

w∗

(a):

Figure 21.6: Face recognition model (de-
picted only for a single examplar per per-
son, J = 1).
In model M1 the
test image (or ‘probe’) x∗ is assumed to
be from person 1, albeit with a diﬀerent
(b): For model M2
pose/illumination.
the test image is assumed to be from per-
son 2. One calculates p(x1, x2, x∗|M1) and
p(x1, x2, x∗|M2) and then uses Bayes’ rule
to infer which person the test image x∗
most likely belongs.

p(xn, x∗, hn, wn, w∗)

(21.3.12)

where p(x, h, w) is obtained from equation (21.3.6) (where we assume the parameters θ are ﬁxed, having
been learned using Maximum Likelihood). Note that in equation (21.3.12) we do not introduce h∗ since
both xn and x∗ are assumed to be generated from the same person with the latent identity hn. These
marginal probabilities are straightforward to derive since they are marginals of Gaussians.

In practice, the best results are obtained using a between-individual subspace dimension F and within-
individual subspace dimension G both equal to 128. This model then has performance competitive with
the state-of-the-art[228]. A beneﬁt of the probabilistic model is that the extension to mixtures of this
model is essentially straightforward, which boosts performance further. Related models can also be used
for the ‘open set’ face recognition problem in which the probe face may or may not belong to one of the
individuals in the database[228].

21.4 Probabilistic Principal Components Analysis

PPCA corresponds to Factor Analysis under the restriction Ψ = σ2ID. Plugging this assumption into the
direct optimisation solution equation (21.2.20) gives

F = σUH (ΛH − IH) 1

2 R

(21.4.1)

where the eigenvalues (diagonal entries of ΛH) and corresponding eigenvectors (columns of UH) are the
largest eigenvalues of σ−2S. Since the eigenvalues of σ−2S are those of S simply scaled by σ−2 (and the
eigenvectors are unchanged), we can equivalently write

(cid:0)ΛH − σ2IH

(cid:1) 1

2 R

F = UH

(21.4.2)

where R is an arbitrary orthogonal matrix with RTR = I and UH, ΛH are the eigenvectors and corre-
sponding eigenvalues of the sample covariance S. Classical PCA, section(15.2), is recovered in the limit
σ2 → 0. Note that for a full correspondence with PCA, one needs to set R = I, which points F along the
principal directions.

Optimal σ2

A particular convenience of PPCA is that the optimal noise σ2 can be found immediately. We order the
eigenvalues of S so that λ1 ≥ λ2, ... ≥ λD. In equation (21.2.19) an expression for the log likelihood is
given in which the eigenvalues are those σ−2S. On replacing λi with λi/σ2 we can therefore write an
explicit expression for the log likelihood in terms of σ2 and the eigenvalues of A,

L(σ2) = −

N
2

D log(2π) +

DRAFT March 9, 2010

λi + (D − H) log σ2 + H

(21.4.3)

397

(cid:32)

H(cid:88)

i=1

log λi +

1
σ2

D(cid:88)

i=H+1

(cid:33)

Canonical Correlation Analysis and Factor Analysis

Figure 21.7: For a 5 hidden unit model, here are plotted the
results of training PPCA and FA on 100 examples of the hand-
written digit seven. The top row contains the 5 Factor Analy-
sis factors and the bottom row the 5 largest eigenvectors from
PPCA are plotted.

By diﬀerentiating L(σ2) and equating to zero, the Maximum Likelihood optimal setting for σ2 is

D(cid:88)

D − H

j=H+1

1

σ2 =

λj

(21.4.4)

In summary PPCA is obtained by taking the principal eigenvalues and corresponding eigenvectors of the
sample covariance matrix S, and setting the variance by equation (21.4.4). The single-shot training nature
of PPCA makes it an attractive algorithm and also gives a useful initialisation for Factor Analysis.

Example 92 (A Comparison of FA and PPCA). We trained both PPCA and FA to model handwritten
digits of the number 7. From a database of 100 such images, we ﬁtted both PPCA and FA (100 iterations
of EM in each case from the same random initialisation) using 5 hidden units. The learned factors for
these models are in ﬁg(21.7). To get a feeling for how well each of these models the data, we drew 25
samples from each model, as given in ﬁg(21.8a). Compared with PPCA, in FA the individual noise on
each observation pixel enables a cleaner representation of the regions of zero sample variance.

21.5 Canonical Correlation Analysis and Factor Analysis

We outline how CCA, as discussed in section(15.8), is related to a constrained form of FA. As a brief
reminder, CCA considers two spaces X and Y where, for example, X might represent an audio sequence
of a person speaking and Y the corresponding video sequence of the face of the person speaking. The two
streams of data are dependent since we would expect the parts around the mouth region to be correlated
with the speech signal. The aim in CCA is to ﬁnd a low dimensional representation that explains the
correlation between the X and Y spaces.
A model that achieves a similar eﬀect to CCA is to use a latent factor h to underlie the data in both the
X and Y spaces. That is

(cid:90)

p(x, y) =

where

p(x|h)p(y|h)p(h)dh

p(x|h) = N (x ha, Ψx) ,

p(y|h) = N (y hb, Ψy) ,

p(h) = N (h 0, 1)

(21.5.1)

(21.5.2)

Figure 21.8: (a): 25 samples
from the learned FA model.
Note how the noise variance de-
pends on the pixel, being zero
for pixels on the boundary of
(b): 25 samples
the image.
from the learned PPCA model.

(a) Factor Analysis

(b) PPCA

398

DRAFT March 9, 2010

FAFAFAFAFAPCAPCAPCAPCAPCAIndependent Components Analysis

h

x

y

Canonical Correlation Analysis. CCA corresponds to the latent vari-
able model in which a common latent variable generates both the
observed x and y variables. This is therefore a formed of constrained
Factor Analysis.

We can express equation (21.5.2) as a form of Factor Analysis by writing

x ∼ N (x 0, Ψx) , y ∼ N (y 0, Ψy)

(cid:18) x

y

z =

=

(cid:19)
(cid:18) x

y

(cid:18) a
(cid:19)

b

,

(cid:19)

h +

(cid:18) x
(cid:18) a

y

b

(cid:19)

,

(cid:19)

,

f =

By using the stacked vectors

and integrating out the latent variable h, we obtain

Σ = ﬀ T + Ψ,

Ψ =

(cid:18) Ψx

0
0 Ψy

(cid:19)

From the FA results equation (21.2.5) the optimal f is given by

p(z) = N (z 0, Σ) ,
(cid:17)
(cid:16)

1 + f TΨ−1f

f

= SΨ−1f ⇒ f ∝ SΨ−1f

(21.5.3)

(21.5.4)

(21.5.5)

(21.5.6)

(21.5.7)

(21.5.8)

(21.5.9)

so that optimally f is given by the principal eigenvector of SΨ−1. By imposing Ψx = σ2
above equation can be expressed as the coupled equations

xI, Ψy = σ2

yI the

1
σ2
x
1
σ2
x

Sxxa +

Syxa +

1
σ2
y
1
σ2
y

Sxyb

Syyb

a ∝
b ∝
(cid:18)
I −

Eliminating b we have, for an arbitrary proportionality constant γ,

(cid:19)

γ
σ2
x

Sxx

a = γ2
σ2
xσ2
y

Sxy

γ
σ2
y

Syy

Syxa

(cid:19)−1

(cid:18)
I −

x, σ2

In the limit σ2
y → 0, this tends to the zero derivative condition equation (15.8.8) so that CCA can be
seen as in fact a form limiting of FA (see [12] for a more thorough correspondence). By viewing CCA in
this manner extensions to using more than a single latent dimension H become clear, see exercise(208), in
addition to the beneﬁts of a probabilistic interpretation.

As we’ve indicated, CCA corresponds to training a form of FA by maximising the joint likelihood p(x, y|w, u).
Alternatively, training based on the maximising the conditional p(y|x, w, u) corresponds to a special case
of a technique called Partial Least Squares, see for example [78]. This correspondence is left as an exercise
for the interested reader.

Extending FA to kernel variants is not straightforward since under replacing x with a non-linear mapping
φ(x), normalising the expression e−(φ(x)−wh)2 is in general intractable.

21.6 Independent Components Analysis

Independent Components Analysis (ICA) is a linear decomposition of the data v in which the components
of underlying latent vector variable h are independent[221, 138]. In other words, we seek a linear coordinate
system in which the coordinates are independent. Such independent coordinate systems arguably form

DRAFT March 9, 2010

399

Independent Components Analysis

exp(−5(cid:112)

Figure 21.9: Latent data is sampled from the prior p(xi) ∝
|xi|) with the mixing matrix A shown in green to cre-
ate the observed two dimensional vectors y = Ax. The red lines
are the mixing matrix estimated by ica.m based on the obser-
vations. For comparison, PCA produces the blue (dashed) com-
ponents. Note that the components have been scaled to improve
visualisation. As expected, PCA ﬁnds the orthogonal directions
of maximal variation. ICA however, correctly estimates the di-
rections in which the components were independently generated.
See demoICA.m.

a natural representation of the data and can give rise to very diﬀerent representations than PCA, see
ﬁg(21.9). From a probabilistic viewpoint, the model is described by

p(hi)

(21.6.1)

p(v, h|A) = p(v|h, A)(cid:89)

i

In ICA it is common to assume that the observations are linearly related to the latent variables h. For
technical reasons, the most convenient practical choice is to use3

where A is a square mixing matrix so that the likelihood of an observation v is

v = Ah

(cid:90)

p(v|h, A)(cid:89)

i

p(v) =

p(hi)dh =

(cid:90)

δ (v − Ah)(cid:89)

i

p(hi)dh =

1

|det (A)|

i

p((cid:2)A−1v(cid:3)

i)

(21.6.2)

(21.6.3)

(cid:89)

The underlying independence assumptions are then the same as for PPCA (in the limit of zero output
noise). Below, however, we will choose a non-Gaussian prior p(hi).

For a given set of data V = (cid:0)v1, . . . , vN(cid:1) and prior p(h), our aim is to ﬁnd A. For i.i.d. data, the log

likelihood is conveniently written in terms of B = A−1,

(21.6.4)

(21.6.5)

(21.6.6)

(21.6.7)

(21.6.8)

L(B) = N log det (B) +(cid:88)

(cid:88)

log p([Bvn]i)

n

i

Note that for a Gaussian prior

−h2

p(h) ∝ e

the log likelihood becomes

L(B) = N log det (B) −

(cid:88)

n

(vn)T BTBvn + const.

L(B) = N Aba +(cid:88)

∂

∂Bab

where

φ(x) ≡

d
dx

log p(x) =

φ([Bv]a)vn
b

n

1

p(x)

d
dx

p(x)

which is invariant with respect to an orthogonal rotation B → RB, with RTR = I. This means that for a
Gaussian prior p(h), we cannot estimate uniquely the mixing matrix. To break this rotational invariance
we therefore need to use a non-Gaussian prior. Assuming we have a non-Gaussian prior p(h), taking the
derivative w.r.t. Bab we obtain

3This treatment follows that presented in [183].

400

DRAFT March 9, 2010

−2−1.5−1−0.500.511.5−1.5−1−0.500.511.5Exercises

A simple gradient ascent learning rule for B is then

(cid:32)

(cid:32)

(cid:88)

n

B−T +

1
N

(cid:88)

n

1
N

(cid:33)

(cid:33)

Bnew = B + η

φ(Bvn) (vn)T

(21.6.9)

An alternative ‘natural gradient’ algorithm[8, 183] that approximates a Newton update is given by multi-
plying the gradient by BTB on the right to give the update

Bnew = B + η

I +

φ(Bvn) (Bvn)T

B

(21.6.10)

Here η is a learning rate which in the code ica.m we nominally set to 0.5.

A natural extension is to consider noise on the outputs, exercise(212), for which an EM algorithm is readily
available. However, in the limit of low output noise, the EM formally fails, an eﬀect which is related to
the general discussion in section(11.4).

A popular alternative estimation method is FastICA4 and can be related to an iterative Maximum Like-
lihood optimisation procedure. ICA can also be motivated from several alternative directions, including
information theory[29]. We refer the reader to [138] for an in-depth discussion of ICA and related exten-
sions.

21.7 Code

FA.m: Factor Analysis
demoFA.m: Demo of Factor Analysis
ica.m: Independent Components Analysis
demoIca.m: Demo ICA

21.8 Exercises

Exercise 207. Factor Analysis and scaling. Assume that a H-factor model holds for x. Now consider the
the transformation y = Cx, where C is a non-singular square diagonal matrix. Show that Factor Analysis
is scale invariant, i.e.
that the H-factor model also holds for y, with the factor loadings appropriately
scaled. How must the speciﬁc factors be scaled?

Exercise 208. For the constrained Factor Analysis model

h + ,

 ∼ N ( 0, diag (ψ1, . . . , ψn)) ,

h ∼ N (h 0, I)

(21.8.1)

derive a Maximum Likelihood EM algorithm for the matrices A and B, assuming the datapoints x1, . . . , xN
are i.i.d.

Exercise 209. An apparent extension of FA analysis is to consider a correlated prior

p(h) = N (h 0, ΣH)

(21.8.2)

Show that, provided no constraints are placed on the factor loading matrix F, using a correlated prior p(h)
is an equivalent model to the original uncorrelated FA model.

Exercise 210. Using the Woodbury identity and the deﬁnition of ΣD in equation (21.2.2), show that one
can rewrite ΣD

(cid:19)

(cid:18) A 0

0 B

x =

−1F as
−1F = Ψ−1F

(cid:16)

ΣD

I + FTΨ−1F

(cid:17)−1

4See www.cis.hut.fi/projects/ica/fastica/

DRAFT March 9, 2010

(21.8.3)

401

Exercise 212. Consider an ICA model

Show L(σ2) is maximal for

λj

j=H+1

1

σ2 =

D − H

D(cid:88)
p(yj|x, W)(cid:89)
p(y, x|W) =(cid:89)
j x, σ2(cid:17)
(cid:16)

p(yj|x, W) = N

yj wT

j

i

with

p(xi),

W = [w1, . . . , wJ]

Exercises

(21.8.4)

(21.8.5)

(21.8.6)

(21.8.7)

Exercise 211. For the log likelihood function

L(σ2) = −

N
2

D log(2π) +

log λi +

1
σ2

(cid:32)

H(cid:88)

i=1

D(cid:88)

i=H+1

λi + (D − H) log σ2 + H

(cid:33)

1. For the above model derive an EM algorithm for a set of i.i.d. data y1, . . . , yN and show that the

required statistics for the M-step are (cid:104)x(cid:105)p(x|yn,W) and(cid:10)xxT(cid:11)

p(x|yn,W).

2. Show that for a non-Gaussian prior p(xi), the posterior

p(x|y, W)

(21.8.8)

is non-factorised, non-Gaussian and generally intractable (its normalisation constant cannot be com-
puted eﬃciently).

3. Show that in the limit σ2 → 0, the EM algorithm fails.

402

DRAFT March 9, 2010

CHAPTER 22

Latent Ability Models

22.1 The Rasch Model

Consider an exam in which student s answers question q either correctly xqs = 1 or incorrectly xqs = 0.
For a set of N students and Q questions, the performance of all students is given in the Q × N binary
matrix X. Based on this data alone we wish to evaluate the ability of each student. One approach is
to deﬁne the ability as as the fraction of questions student s answered correctly. A more subtle analysis
is to accept that some questions are more diﬃcult than others so that a student who answered diﬃcult
questions should be awarded more highly than a student who answered the same number of easy questions.
A priori we do not know which are the diﬃcult questions and this needs to be estimated based on X. To
account for inherent diﬀerences in question diﬃculty we may model the probability that a student s gets
a question q correct based on the student’s latent ability αs and the latent diﬃculty of the question δq. A
simple generative model of the response is

p(xqs = 1|α, δ) = σ (αs − δq)

(22.1.1)
where σ (x) = 1/(1 + e−x). Under this model, the higher the latent ability is above the latent diﬃculty of
the question, the more likely it is that the student will answer the question correctly.

22.1.1 Maximum Likelihood training

Making the i.i.d. assumption, the likelihood of the data X under this model is

S(cid:89)

p(X|α, δ) =

Q(cid:89)
L ≡ log p(X|α, δ) =(cid:88)

s=1
The log likelihood is

q=1

σ (αs − δq)xqs (1 − σ (αs − δq))1−xqs

xqs log σ (αs − δq) + (1 − xqs) log (1 − σ (αs − δq))

q,s

(22.1.2)

(22.1.3)

Q

δq

xqs

αs

S

Figure 22.1: The Rasch model for analysing questions. Each
element of the binary matrix X, with xqs = 1 if student s gets
question q correct, is generated using the latent ability of the
student αs and the latent diﬃculty of the question δq.

403

with derivatives

Competition Models

(xqs − σ (αs − δq)) ,

∂L
∂δq

= −

S(cid:88)

s=1

(xqs − σ (αs − δq))

(22.1.4)

Q(cid:88)

q=1

∂L
∂αs

=

A simple way to learn the parameters is to use gradient ascent, see demoRasch.m, with extensions to
Newton methods being straightforward.

The generalisation to more than two responses xqs ∈ {1, 2, . . .} can be achieved using a softmax-style
function. More generally, the Rasch model is an example of an item response theory, a subject dealing
with the analysis of questionnaires[94].

Missing data

Assuming the data is missing at random, missing data can be treated by computing the likelihood of
only the observed elements of X. In rasch.m missing data is assumed to be coded as nan so that the
likelihood and gradients are straightforward to compute based on summing only over terms containing
non nan entries.

Example 93. We display an example of the use of the Rasch model
in ﬁg(22.2), estimat-
ing the latent abilities of 20 students based on a set of 50 questions.
Based on using the
number of questions each student answered correctly, the best students are (ranked from ﬁrst)
8, 6, 1, 19, 4, 17, 20, 7, 15, 5, 12, 16, 2, 3, 18, 9, 11, 14, 10, 13. Alternatively, ranking students according to the
latent ability gives 8, 6, 19, 1, 20, 4, 17, 7, 15, 12, 5, 16, 2, 3, 18, 9, 11, 14, 10, 13. This diﬀers (only slightly in
this case) from the number-correct ranking since the Rasch model takes into account the fact that some
students answered diﬃcult questions correctly. For example student 20 answered some diﬃcult questions
correctly.

22.1.2 Bayesian Rasch models

The Rasch model will potentially overﬁt the data especially when there is only a small amount of data.
For this case a natural extension is to use a Bayesian technique, placing independent priors on the ability
and question diﬃculty, so that the posterior ability and question diﬃculty is given by

p(α, δ|X) ∝ p(X|α, δ)p(α)p(δ)

Natural priors are

p(α) =(cid:89)

(cid:0)αs 0, σ2(cid:1) ,

s N

p(δ) =(cid:89)

q N

(cid:0)δq 0, τ 2(cid:1)

(22.1.5)

(22.1.6)

where σ2 and τ 2 are hyperparameters that can be learned by maximising p(X|σ2, τ 2). Even using Gaussian
priors, the posterior distribution p(α, δ|X) is not of a standard form and approximations are required.
In this case however, the posterior is log concave so that approximation methods based on variational or
Laplace techniques are potentially adequate, or alternatively one may use sampling approximations.

22.2 Competition Models

22.2.1 Bradly-Terry-Luce model

The Bradly-Terry-Luce model assesses the ability of players based on one-on-one matches. Here we describe
games in which only win/lose outcomes arise, leaving aside the complicating possibility of draws. For this

404

DRAFT March 9, 2010

Competition Models

(a)

(c)

(b)

(d)

Figure 22.2: Rasch model. (a): The data of correct answers (white) and incorrect answers (black). (b):
(c): The fraction of questions each student answered
The estimated latent diﬃculty of each question.
correctly. (d): The estimated latent ability.

win/lose scenario, the BTL model is a straightforward modiﬁcation of the Rasch model so that for latent
ability αi of player i and latent ability αj of player j, the probability that i beats j is given by

where i (cid:66) j stands for player i beats player j. Based on a matrix of games data X with

(22.2.1)

(22.2.2)

(22.2.3)

p(i (cid:66) j|α) = σ (αi − αj)

if i (cid:66) j in game n
otherwise

0

xn
ij =

(cid:26) 1
p(X|α) =(cid:89)
where Mij =(cid:80)

(cid:89)

the likelihood of the model is given by
[σ (αi − αj)]xn

n

ij

ij =(cid:89)

ij

[σ (αi − αj)]Mij

or a Bayesian technique can then proceed as for the Rasch model.

n xn

ij is the number of times player i beat player j. Training using Maximum Likelihood

For the case of only two objects interacting, these models are called pairwise comparison models. Thurstone
in the 1920’s applied such models to a wide range of data, and the Bradley-Terry-Luce model is in fact a
special case of his work[73].

Example 94. An example application of the BTL model is given in ﬁg(22.3) in which a matrix X
containing the number of times that competitor i beat competitor j is given. The matrix entries X were
drawn from a BTL model based on ‘true abilities’. Using X alone the Maximum Likelihood estimate of
these latent abilities is in close agreement with the true abilities.

DRAFT March 9, 2010

405

questionstudent5101520253035404550246810121416182005101520253035404550−8−6−4−20246questionestimated difficulty0246810121416182000.10.20.30.40.50.60.7studentfraction of questions correct02468101214161820−2.5−2−1.5−1−0.500.511.5studentestimated abilityCompetition Models

(a)

(b)

(a): The data X with Xij being the number of times that competitor i beat
Figure 22.3: BTL model.
competitor j. (b): The true versus estimated ability. Even though the data is quite sparse, a reasonable
estimate of the latent ability of each competitor is found.

22.2.2 Elo ranking model

The Elo system [89] used in chess ranking is closely related to the BTL model above, though there is the
added complication of the possibility of draws. In addition, the Elo system takes into account a measure
of the variability in performance. For a given ability αi, the actual performance πi of player i in a game
is given by

πi = αi + i

where i ∼ N
variability in the performance. More formally the Elo model modiﬁes the BTL model to give

(cid:0)i 0, σ2(cid:1). The variance σ2 is ﬁxed across all players and thus takes into account intrinsic
(cid:90)

(22.2.4)

(cid:0)π α, σ2I(cid:1)

p(X|α) =

p(X|π)p(π|α),

π

p(π|α) = N

where p(X|π) is given by equation (22.2.3) on replacing α with π.
22.2.3 Glicko and TrueSkill

Glicko[114] and TrueSkill[130] are essentially Bayesian versions of the Elo model with the reﬁnement that
the latent ability is modelled, not by a single number, but by a Gaussian distribution

This can capture the fact that a player may be consistently reasonable (quite high µi and low σ2
erratic genius (high µi but with large σ2

i ). The parameters of the model are then

i ) or an

(22.2.7)
for a set of S players. The interaction model p(X|α) is as for the win/lose Elo model, equation (22.2.1).
The likelihood for the model given the parameters is

(22.2.5)

(22.2.6)

(22.2.8)

i

p(αi|θi) = N

(cid:0)αi µi, σ2
(cid:1)
θ =(cid:8)µi, σ2
i , i = 1, . . . , S(cid:9)
(cid:90)

p(X|θ) =

p(X|α)p(α|θ)

α

This integral is formally intractable and numerical approximations are required. In this context Expecta-
tion Propagation has proven to be a useful technique[195].

The TrueSkill system is used for example to assess the abilities of players in online gaming, also taking
into account the abilities of teams of individuals in tournaments. A temporal extension has recently been
used to reevaluate the change in ability of chess players with time[72].

406

DRAFT March 9, 2010

competitorcompetitor  2040608010010203040506070809010000.511.522.533.544.55−10−50510−10−50510true abilityestimated abilityExercises

22.3 Code

rasch.m: Rasch model training
demoRasch.m: Demo for the Rasch model

22.4 Exercises

Exercise 213 (Bucking Bronco). bronco.mat contains information about a bucking bronco competition.
There are 500 competitors and 20 bucking broncos. A competitor j attempts to stay on a bucking bronco i
for a minute. If the competitor succeeds, the entry Xij is 1, otherwise 0. Each competitor gets to ride three
bucking broncos only (the missing data is coded as nan). Having viewed all the 500 amateurs, desperate
Dan enters the competition and bribes the organisers into letting him avoid having to ride the diﬃcult
broncos. Based on using a Rasch model, which are the top 10 most diﬃcult broncos, in order of the most
diﬃcult ﬁrst?

Exercise 214 (BTL training).

1. Show that the log likelihood for the Bradly-Terry-Luce model is given by

Xij log σ (αi − αj)

(22.4.1)

L(α) =(cid:88)

ij

where Xij is the number of times that player i beats player j in a set of games.

2. Compute the gradient of L(α).

3. Compute the Hessian of the BTL model and verify that it is negative semideﬁnite.

Exercise 215 (La Reine).

1. Program a simple gradient ascent routine to learn the latent abilities of competitors based on a series

of win/lose outcomes.

2. In a modiﬁed form of Swiss cow ‘ﬁghting’, a set of cows compete by pushing each other until submis-
sion. At the end of the competition one cow is deemed to be ‘la reine’. Based on the data in BTL.mat
(for which Xij contains the number of times cow i beat cow j), ﬁt a BTL model and return a ranked
list of the top ten best ﬁghting cows, ‘la reine’ ﬁrst.

Exercise 216. An extension of the BTL model is to consider additional ‘factors’ that describe the state
of the competitors when they play. For example, we have a set of S football teams, and a set of matrices
ij = 1 if team i beat team j in match n. In addition we have for each match and team
X1, . . . , XN , with X n
i ∈ {0, 1} that describes the team. For example, for the team i = 1 (Madchester
a vector of binary factors f n
United), the factor f1,1 = 1 if Bozo is playing, 0 if not. It is suggested that the ability of team i in game
n is measured by

αn
i = di +

wh,if n
h,i

(22.4.2)

h=1

h,i = 1 if factor h is present in team i in game n. di is a default latent ability of the team which

where f n
is assumed constant across all games. We have such a set of factors for each match, giving f n
h,i.

1. Using the above deﬁnition of the latent ability in the BTL model, our interest is to ﬁnd the weights
W and abilities d that best predict the ability of the team, given that we have a set of historical plays
(Xn, Fn), n = 1, . . . , N. Write down the likelihood for the BTL model as a function of the set of all
team weights W, d.

DRAFT March 9, 2010

407

H(cid:88)

Exercises

2. Compute the gradient of the log likelihood of this model.

3. Explain how this model can be used to assess the importance of Bozo’s contribution to Madchester

United’s ability.

4. Given learned W, d and the knowledge that Madchester United (team 1) will play Chelski (team 2)
tomorrow explain how, given the list of factors f for Chelski (which includes issues such as who will
be playing in the team), one can select the best Madchester United team to maximise the probability
of winning the game.

408

DRAFT March 9, 2010

Part IV

Dynamical Models

409

CHAPTER 23

Discrete-State Markov Models

23.1 Markov Models

Time-series are datasets for which the constituent datapoints can be naturally ordered. This order often
corresponds to an underlying single physical dimension, typically time, though any other single dimension
may be used. The time-series models we consider are probability models over a collection of random
variables v1, . . . , vT with individual variables vt indexed by a time index t. These indices are elements of
the index set T . For nonnegative indices, T = N+, the model is a discrete-time process. Continuous-
time processes, T = R, are natural in particular application domains yet require additional notation
and concepts. We therefore focus exclusively on discrete-time models. A probabilistic time-series model
requires a speciﬁcation of the joint distribution p(v1, . . . , vT ). For the case in which the observed data vt is
discrete, the joint probability table for p(v1, . . . , vT ) has exponentially many entries. We cannot expect to
independently specify all the exponentially many entries and are therefore forced to make simpliﬁed models
under which these entries can be parameterised in a lower dimensional manner. Such simpliﬁcations are
at the heart of time-series modelling and we will discuss some classical models in the following sections.

Deﬁnition 107 (Time-Series Notation).

xa:b ≡ xa, xa+1, . . . , xb,

with xa:b = xa for b ≤ a

(23.1.1)

For timeseries data v1, . . . , vT , we need a model p(v1:T ). For consistency with the causal nature of time,
it is meaningful to consider the decomposition

T(cid:89)

t=1

p(v1:T ) =

p(vt|v1:t−1)

(23.1.2)

with the convention p(vt|v1:t−1) = p(v1) for t = 1. It is often natural to assume that the inﬂuence of the
immediate past is more relevant than the remote past and in Markov models only a limited number of
previous observations are required to predict the future.

Deﬁnition 108 (Markov chain). A Markov chain deﬁned on either discrete or continuous variables v1:T
is one in which the following conditional independence assumption holds:

p(vt|v1, . . . , vt−1) = p(vt|vt−L, . . . , vt−1)

(23.1.3)

411

Markov Models

v1

v2

v3

v4

v1

v2

v3

v4

(a)

(b)

Figure 23.1: (a): First order Markov chain. (b): Second order Markov chain.

where L ≥ 1 is the order of the Markov chain and vt = ∅ for t < 1. For a ﬁrst order Markov chain,

p(v1:T ) = p(v1)p(v2|v1)p(v3|v2) . . . p(vT|vT−1)

For a stationary Markov chain the transitions p(vt = s(cid:48)
wise the chain is non-stationary, p(vt = s(cid:48)

|vt−1 = s) = f(s(cid:48), s, t).

|vt−1 = s) = f(s(cid:48), s) are time-independent. Other-

(23.1.4)

23.1.1 Equilibrium and stationary distribution of a Markov chain

The stationary distribution p∞ of a Markov chain with transition matrix M is deﬁned by the condition

p∞(i) =(cid:88)

(cid:125)
(cid:124)
p(xt = i|xt−1 = j)

(cid:123)(cid:122)

Mij

j

p∞(j)

In matrix notation this can be written as the vector equation

p∞ = Mp∞

so that the stationary distribution is proportional to the eigenvector with unit eigenvalue of the transition
matrix. Note that there may be more than one stationary distribution. See exercise(217) and [122].

Given a state x1, we can iteratively draw samples x2, . . . , xt from the Markov chain drawing a sample from
p(x2|x1 = x1), and then from p(x3|x2) etc. As we repeatedly sample a new state from the chain, the
distribution at time t, for an initial distribution p1(i) = δ (i, x1), is

pt = Mtp1

(23.1.7)
If for t → ∞, p∞ is independent of the initial distribution p1, then p∞ is called the equilibrium distribution
of the chain. See exercise(218) for an example of a Markov chain which does not have an equilibrium
distribution.

(23.1.5)

(23.1.6)

(23.1.8)

(23.1.9)

Example 95 (PageRank). Despite their apparent simplicity, Markov chains have been put to interesting
use in information retrieval and search-engines. Deﬁne the matrix

(cid:26) 1

0

Aij =

if website j has a hyperlink to website i
otherwise

From this we can deﬁne a Markov transition matrix with elements

Mij = Aij(cid:80)

i Aij

The equilibrium distribution of this Markov Chain has the interpretation : If follow links at random,
jumping from website to website, the equilibrium distribution component p∞(i) is the relative number of
times we will visit website i. This has a natural interpretation as the ‘importance’ of website i; if a website

412

DRAFT March 9, 2010

Markov Models

is isolated in the web, it will be visited infrequently by random hopping; if a website is linked by many
others it will be visited more frequently.

A crude search engine works then as follows. For each website i a list of words associated with that website
is collected. After doing this for all websites, one can make an ‘inverse’ list of which websites contain word
w. When a user searches for word w, the list of websites that that word is then returned, ranked according
to the importance of the site (as deﬁned by the equilibrium distribution). This is a crude summary as
how early search engines worked, infolab.stanford.edu/∼backrub/google.html.

1

3

2

Figure 23.2: A state transition diagram for a three state Markov
chain. Note that a state transition diagram is not a graphical
model – it simply displays the non-zero entries of the transition
matrix p(i|j). The absence of link from j to i indicates that
p(i|j) = 0.

23.1.2 Fitting Markov models

Given a sequence v1:T , ﬁtting a stationary Markov chain by Maximum Likelihood corresponds to setting
the transitions by counting the number of observed (ﬁrst-order) transitions in the sequence:

p(vτ = i|vτ−1 = j) ∝

I [vt = i, vt−1 = j]

(23.1.10)

To show this, for convenience we write p(vτ = i|vτ−1 = j) ≡ θi|j, so that the likelihood is (assuming v1 is
known):

T(cid:88)

t=2

I[vt=i,vt−1=j]
i|j

θ

(23.1.11)

T(cid:89)

(cid:89)

T(cid:89)

(cid:89)

θvt|vt−1 =

t=2

ij

t=2

ij

p(v2:T|θ, v1) =
(cid:88)
T(cid:88)

L(θ) =

Taking logs and adding the Lagrange constraint for the normalisation,

I [vt = i, vt−1 = j] log θi|j +(cid:88)

λj

t=2

ij

j

(cid:33)

(cid:32)
1 −

(cid:88)

i

θi|j

(23.1.12)

Diﬀerentiating with respect to θi|j and equating to zero, we immediately arrive at the intuitive setting,
equation (23.1.10). For a set of timeseries, vn
1:Tn, n = 1, . . . , N, the transition is given by counting all
transitions across time and datapoints. The Maximum Likelihood setting for the initial ﬁrst timestep
distribution is p(v1 = i) ∝
Bayesian ﬁtting

(cid:80)

1 = i].

I [vn

n

For simplicity, we assume a factorised prior on the transition

p(θ·|j)

(23.1.13)

A convenient choice for each conditional transition is a Dirichlet distribution with hyperparameters uj,

p(θ) =(cid:89)

j

p(θ·|j) = Dirichlet(cid:0)θ·|j|uj
where ˆuj =(cid:80)T

t=2

p(θ|v1:T ) ∝ p(v1:T|θ)p(θ) ∝

DRAFT March 9, 2010

(cid:1), since this is conjugate to the categorical transition and
(cid:1)
Dirichlet(cid:0)θ·|j|ˆuj

I[vt=i,vt−1=j]
i|j

=(cid:89)

(cid:89)

(cid:89)

uij−1
i|j

θ

θ

t

ij

j

I [vt−1 = i, vt = j], being the number of j → i transitions in the dataset.

(23.1.14)

413

Markov Models

h

v1

v2

v3

v4

(a)

Figure 23.3: Mixture of ﬁrst order Markov chains.
The discrete hidden variable dom(h) = {1, . . . , H}
indexes the Markov chain p(vt|vt−1, h). Such models
can be useful as simple sequence clustering tools.

23.1.3 Mixture of Markov models
Given a set of sequences V = {vn
1:T , n = 1, . . . , N}, how might we cluster them? To keep the notation less
cluttered, we assume that all sequences are of the same length T with the extension to diﬀering lengths
being straightforward. One simple approach is to ﬁt a mixture of Markov models. Assuming the data
1:T ), we deﬁne a mixture model for a single datapoint v1:T . Here we assume each

n p(vn

component model is ﬁrst order Markov

is i.i.d., p(V) =(cid:81)
H(cid:88)

p(v1:T ) =

p(h)p(v1:T|h) =

h=1

p(h)

p(vt|vt−1, h)

(23.1.15)

The graphical model is depicted in ﬁg(23.3). Clustering can then be achieved by ﬁnding the Maximum
Likelihood parameters p(h), p(vt|vt−1, h) and subsequently assigning the clusters according to p(h|vn
1:T ).
Below we discuss the application of the EM algorithm to this model to learn the Maximum Likelihood
parameters.

EM algorithm

H(cid:88)

T(cid:89)

h=1

t=1

Under the i.i.d. data assumption, the log likelihood is

N(cid:88)

H(cid:88)

T(cid:89)

n=1

h=1

t=1

log

p(h)

p(vn

t−1, h)

For the M-step, our task is to maximise the energy

(cid:104)log p(vn

1:T , h)(cid:105)pold(h|vn

1:T ) =

The contribution to the energy from the parameter p(h) is

t |vn
N(cid:88)

n=1

(cid:40)
(cid:104)log p(h)(cid:105)pold(h|vn

1:T ) +

(23.1.16)

(cid:41)

(cid:104)log p(vt|vt−1, h)(cid:105)pold(h|vn

1:T )

T(cid:88)

t=1

log p(V) =
N(cid:88)

E =

n=1

N(cid:88)

n=1

By deﬁning

(cid:104)log p(h)(cid:105)pold(h|vn

1:T )

N(cid:88)

n=1

ˆpold(h) ∝
(cid:16)

KL

1:T )

pold(h|vn
(cid:17)

ˆpold(h)|p(h)
N(cid:88)

pnew(h) ∝

pold(h|vn

1:T )

n=1

(23.1.17)

(23.1.18)

(23.1.19)

(23.1.20)

one can view maximising (23.1.17) as equivalent to minimising

so that the optimal choice from the M-step is to set pnew = ˆpold, namely

For those less comfortable with this argument, a direct maximisation including a Lagrange term to ensure
normalisation of p(h) can be used to derive the same result.

414

DRAFT March 9, 2010

Markov Models

Similarly, the M-step for p(vt|vt−1, h) is
N(cid:88)

pnew(vt = i|vt−1 = j, h = k) ∝

n=1

T(cid:88)

t=2

pold(h = k|vn

1:T )

t = i] I(cid:2)vn

t−1 = j(cid:3)

I [vn

The initial term p(v1|h) is updated using

pnew(v1 = i|h = k) ∝

The E-step sets

1:T )I [vn

1 = i]

N(cid:88)

n=1

pold(h = k|vn
T(cid:89)

pold(h|vn

1:T ) ∝ p(h)p(vn

1:T|h) = p(h)

p(vn

t |vn

t−1, h)

t=1

(23.1.21)

(23.1.22)

(23.1.23)

Given an initialisation, the EM algorithm then iterates (23.1.20),(23.1.21), (23.1.22) and (23.1.23) until
convergence.

For long sequences, explicitly computing the product of many terms may lead to numerical underﬂow
issues. In practice it is therefore best to work with logs,

T(cid:88)

t=1

log pold(h|vn

1:T ) = log p(h) +

log p(vn

t |vn

t−1, h) + const.

(23.1.24)

In this way any large constants common to all h can be removed and the distribution may be computed
accurately. See mixMarkov.m.

Example 96 (Gene Clustering). Consider the 20 ﬁctitious gene sequences below presented in an arbitrarily
chosen order. Each sequence consists of 20 symbols from the set {A, C, G, T}. The task is to try to cluster
these sequences into two groups, based on the (perhaps biologically unrealistic) assumption that gene
sequences in the same cluster follow a stationary Markov chain.

CATAGGCATTCTATGTGCTG
GTGCCTGGACCTGAAAAGCC
GTTGGTCAGCACACGGACTG
TAAGTGTCCTCTGCTCCTAA
GCCAAGCAGGGTCTCAACTT

CCAGTTACGGACGCCGAAAG
CGGCCGCGCCTCCGGGAACG
CCTCCCCTCCCCTTTCCTGC
CACCATCACCCTTGCTAAGG
CATGGACTGCTCCACAAAGG

TGGAACCTTAAAAAAAAAAA
AAAGTGCTCTGAAAACTCAC
CACTACGGCTACCTGGGCAA
AAAGAACTCCCCTCCCTGCC
AAAAAAACGAAAAACCTAAG

GTCTCCTGCCCTCTCTGAAC
ACATGAACTACATAGTATAA
CGGTCCGTCCGAGGCACTC
CAAATGCCTCACGCGTCTCA
GCGTAAAAAAAGTCCTGGGT

(23.1.25)

A simple approach is to assume that the sequences are generated from a two-component H = 2 mixture of
Markov models and train the model using Maximum Likelihood. The likelihood has local optima so that
the procedure needs to be run several times and the solution with the highest likelihood chosen. One can
1:T ). If this posterior probability is greater than
then assign each of the sequences by examining p(h = 1|vn
0.5, we assign it to class 1, otherwise to class 2. Using this procedure, we ﬁnd the following clusters:

CATAGGCATTCTATGTGCTG
CCAGTTACGGACGCCGAAAG
CGGCCGCGCCTCCGGGAACG
ACATGAACTACATAGTATAA
GTTGGTCAGCACACGGACTG
CACTACGGCTACCTGGGCAA
CGGTCCGTCCGAGGCACTCG
CACCATCACCCTTGCTAAGG
CAAATGCCTCACGCGTCTCA
GCCAAGCAGGGTCTCAACTT
CATGGACTGCTCCACAAAGG

TGGAACCTTAAAAAAAAAAA
GTCTCCTGCCCTCTCTGAAC
GTGCCTGGACCTGAAAAGCC
AAAGTGCTCTGAAAACTCAC
CCTCCCCTCCCCTTTCCTGC
TAAGTGTCCTCTGCTCCTAA
AAAGAACTCCCCTCCCTGCC
AAAAAAACGAAAAACCTAAG
GCGTAAAAAAAGTCCTGGGT

(23.1.26)

where sequences in the ﬁrst column are assigned to cluster 1, and sequences in the second column to cluster
2. In this case the data in (23.1.25) was in fact generated by a two-component Markov mixture and the
posterior assignment (23.1.26) is in agreement with the known clusters. See demoMixMarkov.m

DRAFT March 9, 2010

415

Hidden Markov Models

h1

v1

h2

v2

h3

v3

h4

v4

Figure 23.4: A ﬁrst order hidden Markov model with
‘hidden’ variables dom(ht) = {1, . . . , H}, t = 1 : T .
The ‘visible’ variables vt can be either discrete or con-
tinuous.

23.2 Hidden Markov Models

The Hidden Markov Model (HMM) deﬁnes a Markov chain on hidden (or ‘latent’) variables h1:T . The
observed (or ‘visible’) variables are dependent on the hidden variables through an emission p(vt|ht). This
deﬁnes a joint distribution

p(h1:T , v1:T ) = p(v1|h1)p(h1)

p(vt|ht)p(ht|ht−1)

(23.2.1)

for which the graphical model is depicted in ﬁg(23.4). For a stationary HMM the transition p(ht|ht−1)
and emission p(vt|ht) distributions are constant through time. The use of the HMM is widespread and a
subset of the many applications of HMMs is given in section(23.5).

T(cid:89)

t=2

Deﬁnition 109 (Transition Distribution). For a stationary HMM the transition distribution p(ht+1|ht)
is deﬁned by the H × H transition matrix

(cid:48)
Ai(cid:48),i = p(ht+1 = i

|ht = i)

and an initial distribution

ai = p(h1 = i).

(23.2.2)

(23.2.3)

Deﬁnition 110 (Emission Distribution). For a stationary HMM and emission distribution p(vt|ht) with
discrete states vt ∈ {1, . . . , V }, we deﬁne a V × H emission matrix

Bi,j = p(vt = i|ht = j)

(23.2.4)

For continuous outputs, ht selects one of H possible output distributions p(vt|ht), ht ∈ {1, . . . , H}.

In the engineering and machine learning communities, the term HMM typically refers to the case of discrete
variables ht, a convention that we adopt here. In statistics the term HMM often refers to any model with
the independence structure in equation (23.2.1), regardless of the form of the variables ht (see for example
[54]).

23.2.1 The classical inference problems

Filtering
Prediction
Smoothing
Likelihood
Most likely Hidden path (Viterbi alignment)

(Inferring the present)
(Inferring the future)
(Inferring the past)

p(ht|v1:t)
p(ht|v1:s)
p(ht|v1:u)
p(v1:T )
argmax

h1:T

p(h1:T|v1:T )

t > s
t < u

The most likely hidden path problem is termed Viterbi alignment in the engineering literature. All these
classical inference problems are straightforward since the distribution is singly-connected, so that any
standard inference method can be adopted for these problems. The factor graph and junction trees for

416

DRAFT March 9, 2010

Hidden Markov Models

h1

v1

h3

v3

h2

v2

(a)

h4

v4

v1, h1

h1

h1, h2

h2

h2, h3

h3

h3, h4

h2

v2, h2

h3

v3, h3

h4

v4, h4

(b)

Figure 23.5: (a): Factor graph for the ﬁrst order HMM of ﬁg(23.4). (b): Junction tree for ﬁg(23.4).

the ﬁrst order HMM are given in ﬁg(23.5). In both cases, after suitable setting of the factors and clique
potentials, ﬁltering corresponds to passing messages from left to right and upwards; smoothing corresponds
to a valid schedule of message passing/absorption both forwards and backwards along all edges. It is also
straightforward to derive appropriate recursions directly. This is instructive and also useful in constructing
compact and numerically stable algorithms.

23.2.2 Filtering p(ht|v1:t)
We ﬁrst compute the joint marginal p(ht, v1:t) from which the conditional marginal p(ht|v1:t) can subse-
quently be obtained by normalisation. A recursion for p(ht, v1:t) is obtained by considering:

p(ht, v1:t) =(cid:88)
=(cid:88)
=(cid:88)

ht−1

ht−1

ht−1

p(ht, ht−1, v1:t−1, vt)

ht−1)p(ht|v1:t−1, ht−1)p(v1:t−1, ht−1)

p(vt|v1:t−1, ht,
p(vt|ht)p(ht|ht−1)p(ht−1, v1:t−1)

(23.2.5)

(23.2.6)

(23.2.7)

The cancellations follow from the conditional independence assumptions of the model. Hence if we deﬁne

α(ht) = p(ht, v1:t)

(cid:124) (cid:123)(cid:122) (cid:125)
α(ht) = p(vt|ht)

corrector

(cid:88)
(cid:124)

ht−1

equation (23.2.7) above gives the α-recursion

(cid:125)
p(ht|ht−1)α(ht−1)
,

(cid:123)(cid:122)

predictor

with

α(h1) = p(h1, v1) = p(v1|h1)p(h1)

t > 1

(23.2.8)

(23.2.9)

(23.2.10)

This recursion has the interpretation that the ﬁltered distribution α(ht−1) is propagated forwards by the
dynamics for one timestep to reveal a new ‘prior’ distribution at time t. This distribution is then mod-
ulated by the observation vt, incorporating the new evidence into the ﬁltered distribution (this is also
referred to as a predictor-corrector method). Since each α is smaller than 1, and the recursion involves
multiplication by terms less than 1, the α’s can become very small. To avoid numerical problems it is
therefore advisable to work with log α(ht), see HMMforward.m.

Normalisation gives the ﬁltered posterior

p(ht|v1:t) ∝ α(ht)

(23.2.11)

If we only require the ﬁltered posterior we are free to rescale the α’s as we wish. In this case an alternative
to working with log α messages is to work with normalised α messages so that the sum of the components
is always 1.

DRAFT March 9, 2010

417

Hidden Markov Models

We can write equation (23.2.7) above directly as a recursion for the ﬁltered distribution

p(ht|v1:t) ∝

p(vt|ht)p(ht|ht−1)p(ht−1|v1:t−1)

t > 1

(23.2.12)

(cid:88)

ht−1

Intuitively, the term p(ht−1|v1:t−1) has the eﬀect of removing all nodes in the graph before time t − 1 and
replacing their inﬂuence by a modiﬁed ‘prior’ distribution on ht. One may interpret p(vt|ht)p(ht|ht−1) as a
likelihood, giving rise to the joint posterior p(ht, ht−1|v1:t) under Bayesian updating. At the next timestep
the previous posterior becomes the new prior.

23.2.3 Parallel smoothing p(ht|v1:T )
There are two main approaches to computing p(ht|v1:T ). Perhaps the most common in the HMM literature
is the parallel method which is equivalent to message passing on factor graphs. In this one separates the
smoothed posterior into contributions from the past and future:
(cid:124)
(cid:125)
p(vt+1:T|ht, v1:t)

p(ht, v1:T ) = p(ht, v1:t, vt+1:T ) = p(ht, v1:t)

= α(ht)β(ht)

(23.2.13)

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:124)

(cid:125)

past

future

The term α(ht) is obtained from the ‘forward’ α recursion, (23.2.9). The term β(ht) may be obtained
using a ‘backward’ β recursion as we show below. The forward and backward recursions are independent
and may therefore be run in parallel, with their results combined to obtain the smoothed posterior. This
approach is also sometimes termed the two-ﬁlter smoother.

The β recursion

p(vt:T|ht−1) =(cid:88)
=(cid:88)
=(cid:88)

ht

ht

ht

Deﬁning

p(vt, vt+1:T , ht|ht−1)
p(vt|vt+1:T , ht,
p(vt|ht)p(vt+1:T|ht,

ht−1)p(vt+1:T , ht|ht−1)

ht−1)p(ht|ht−1)

β(ht) ≡ p(vt+1:T|ht)

β(ht−1) =(cid:88)

ht

equation (23.2.16) above gives the β-recursion

p(vt|ht)p(ht|ht−1)β(ht),

2 ≤ t ≤ T

(23.2.14)

(23.2.15)

(23.2.16)

(23.2.17)

(23.2.18)

(23.2.19)

with β(hT ) = 1. As for the forward pass, working in log space is recommended to avoid numerical
diﬃculties. If one only desires posterior distributions, one can also perform local normalisation at each
stage since only the relative magnitude of the components of β are of importance. The smoothed posterior
is then given by

p(ht|v1:T ) ≡ γ(ht) = α(ht)β(ht)
ht α(ht)β(ht)

(cid:80)

Together the α − β recursions are called the Forward-Backward algorithm.
23.2.4 Correction smoothing

An alternative to the parallel method is to form a recursion directly for the smoothed posterior. This can
be achieved by recognising that conditioning on the present makes the future redundant:

p(ht|v1:T ) =(cid:88)

p(ht, ht+1|v1:T ) =(cid:88)

ht+1

ht+1

p(ht|ht+1, v1:t,vt+1:T )p(ht+1|v1:T )

(23.2.20)

418

DRAFT March 9, 2010

Hidden Markov Models

γ(ht) =(cid:88)

ht+1

This gives a recursion for γ(ht) ≡ p(ht|v1:T ):

p(ht|ht+1, v1:t)γ(ht+1)

(23.2.21)

with γ(hT ) ∝ α(hT ). The term p(ht|ht+1, v1:t) may be computed using the ﬁltered results p(ht|v1:t):

p(ht|ht+1, v1:t) ∝ p(ht+1, ht|v1:t) ∝ p(ht+1|ht)p(ht|v1:t)

(23.2.22)

where the proportionality constant is found by normalisation. This is a form of dynamics reversal, as if
we are reversing the direction of the hidden to hidden arrow in the HMM. This procedure, also termed
the Rauch-Tung-Striebel smoother1, is sequential since we need to ﬁrst complete the α recursions, after
which the γ recursion may begin. This is a so-called correction smoother since it ‘corrects’ the ﬁltered
result. Interestingly, once ﬁltering has been carried out, the evidential states v1:T are not needed during
the subsequent γ recursion.

The α − β and α − γ recursions are related through

γ(ht) ∝ α(ht)β(ht)

(23.2.23)

Computing the pairwise marginal p(ht, ht−1|v1:T )
To implement the EM algorithm for learning, section(23.3.1), we require terms such as p(ht, ht−1|v1:T ).
These can be obtained by message passing on either a factor graph or junction tree (for which the pairwise
marginals are contained in the cliques, see ﬁg(23.4b)). Alternatively, an explicit recursion is as follows:

p(ht, ht+1|v1:T ) ∝ p(v1:t, vt+1, vt+2:T , ht+1, ht)
= p(vt+2:T|((((((
= p(vt+2:T|ht+1)p(vt+1|
v1:t, ht, ht+1)p(v1:t, ht+1, ht)
= p(vt+2:T|ht+1)p(vt+1|ht+1)p(ht+1|v1:t, ht)p(v1:t, ht)

v1:t, vt+1, ht, ht+1)p(v1:t, vt+1, ht+1, ht)

Rearranging, we therefore have

p(ht, ht+1|v1:T ) ∝ α(ht)p(vt+1|ht+1)p(ht+1|ht)β(ht+1)

See HMMsmooth.m.

The likelihood p(v1:T )
The likelihood of a sequence of observations can be computed from

p(hT , v1:T ) =(cid:88)

α(hT )

hT

An alternative computation can be found by making use of the decomposition

hT

p(v1:T ) =

p(v1:T ) =(cid:88)
T(cid:89)
p(vt|v1:t−1) =(cid:88)
=(cid:88)
=(cid:88)

t=1

p(vt|v1:t−1)

Each factor can be computed using
p(vt, ht|v1:t−1)
p(vt|ht,v1:t−1)p(ht|v1:t−1)

ht

p(vt|ht)(cid:88)

ht−1

ht

ht

p(ht|ht−1,v1:t−1)p(ht−1|v1:t−1)

(23.2.24)

(23.2.25)

(23.2.26)

(23.2.27)

(23.2.28)

(23.2.29)

(23.2.30)

1It is most common to use this terminology for the continuous variable case, though we adopt it here also for the discrete

variable case.

DRAFT March 9, 2010

419

Hidden Markov Models

Figure 23.6: Localising the burglar. The latent variable ht ∈
{1, . . . , 25} denotes the positions, deﬁned over the 5 × 5 grid
of the ground ﬂoor of the house. (a): A representation of the
probability that the ‘ﬂoor will creak’ at each of the 25 positions,
p(vcreak|h). Light squares represent probability 0.9 and dark
square 0.1. (b): A representation of the probability p(vbump|h)
that the burglar will bump into something in each of the 25
positions.

(a) ‘Creaks’

(b) ‘Bumps’

where the ﬁnal term p(ht−1|v1:t−1) is the ﬁltered result.
In both approaches the likelihood of an output sequence requires only a forward computation (ﬁltering).
If required, one can also compute the likelihood using, (23.2.13),

p(v1:T ) =(cid:88)

ht

α(ht)β(ht)

(23.2.31)

(23.2.32)

which is valid for any 1 ≤ t ≤ T .
23.2.5 Most likely joint state
The most likely path h1:T of p(h1:T|v1:T ) is the same as the most likely state (for ﬁxed v1:T ) of

p(h1:T , v1:T ) =(cid:89)

p(vt|ht)p(ht|ht−1)

t

The most likely path can be found using the max-product version of the factor graph or max-absorption
on the junction tree. Alternatively, an explicit derivation can be obtained by considering:

T(cid:89)

t=1

max
hT

p(vt|ht)p(ht|ht−1) =

(cid:40)T−1(cid:89)

t=1

(cid:41)
p(vt|ht)p(ht|ht−1)

max
hT

(cid:124)

(cid:125)
p(vT|hT )p(hT|hT−1)

(cid:123)(cid:122)

µ(hT−1)

(23.2.33)

The message µ(hT−1) conveys information from the end of the chain to the penultimate timestep. We can
continue in this manner, deﬁning the recursion

µ(ht−1) = max
ht

p(vt|ht)p(ht|ht−1)µ(ht),

2 ≤ t ≤ T

(23.2.34)

with µ(hT ) = 1. This means that the eﬀect of maximising over h2, . . . , hT is compressed into a message
µ(h1) so that the most likely state h∗
p(v1|h1)p(h1)µ(h1)

∗
1 = argmax
h

1 is given by

(23.2.35)

h1

Once computed, backtracking gives

∗
t = argmax
h

ht

p(vt|ht)p(ht|h

∗
t−1)µ(ht)

(23.2.36)

This special case of the max-product algorithm is called the Viterbi algorithm. Similarly, one may use the
N-max-product algorithm, section(5.2.1), to obtain the N-most likely hidden paths.

Example 97 (A localisation example). You’re asleep upstairs in your house and awoken by noises from
downstairs. You realise that a burglar is on the ground ﬂoor and attempt to understand where he his
from listening to his movements. You mentally partition the ground ﬂoor into a 5 × 5 grid. For each grid
position you know the probability that if someone is in that position the ﬂoorboard will creak, ﬁg(23.6a).

420

DRAFT March 9, 2010

Hidden Markov Models

(a) Creaks and Bumps

(b) Filtering

(c) Smoothing

(d) Viterbi

(e) True Burglar position

(cid:16)

(cid:17)

t

t

vcreak
t

, vbump

, where vcreak

t

t
= 2 otherwise) and vbump

(a): Each panel represents the
= 1 means that there was a ‘creak in the ﬂoorboard’
= 1 meaning ‘bumped into something’ (and is in state 2 otherwise).
t and the right
t . The lighter colour represents the occurrence of a creak or bump, the darker colour the absence.
(c): The smoothed
(d): The most likely

Figure 23.7: Localising the burglar through time for 10 time steps.
visible information vt =
(vcreak
There are 10 panels, one for each time t = 1, . . . , 10. The left half of the panel represents v1
half v2
(b): The ﬁltered distribution p(ht|v1:t) representing where we think the burglar is.
distribution p(ht|v1:10) so that we can ﬁgure out where we think the burglar went.
(Viterbi) burglar path arg maxh1:10 p(h1:10|v1:10). (e): The actual path of the burglar.

Similarly you know for each position the probability that someone will bump into something in the dark,
ﬁg(23.6b). The ﬂoorboard creaking and bumping into objects can occur independently. In addition you
assume that the burglar will move only one grid square – forwards, backwards, left or right in a single
timestep. Based on a series of bump/no bump and creak/no creak information, ﬁg(23.7a), you try to
ﬁgure out based on your knowledge of the ground ﬂoor, where the burglar might be.

We can represent the scenario using a HMM where h ∈ {1, . . . , 25} denotes the grid square. The visible
variable has a factorised form v = vcreak ⊗ vbump and, to use our standard code, we form a new visible
variable with 4 states using

p(v|h) = p(vcreak|h)p(vbump|h)

(23.2.37)

Based on the past information, our belief as to where the burglar might be is represented by the ﬁltered
distribution p(ht|v1:t), ﬁg(23.7). After the burglar has left at T = 10, the police arrive and try to piece
together where the burglar went, based on the sequence of creaks and bumps you provide. At any time
t, the information as to where the burglar could have been is represented by the smoothed distribution
p(ht|v1:10). The police’s single best-guess for the sequence of burglar positions is provided by the most
likely joint hidden state arg maxh1:10 p(h1:10|v1:10).

23.2.6 Self localisation and kidnapped robots
A robot has an internal grid-based map of its environment and for each location h ∈ {1, . . . , H}, knows the
likely sensor readings he would expect in that location. The robot is ‘kidnapped’ and placed somewhere in
the environment. The robot then starts to move, gathering sensor information. Based on these readings
v1:t and intended movements m1:t, the robot attempts to ﬁgure out his location. Due to wheel slippage
on the ﬂoor an intended action by the robot, such as ‘move forwards’, might not be successful. Given all
the information the robot has, he would like to infer p(ht|v1:t, m1:t). This problem diﬀers from the burglar
scenario in that the robot now has knowledge of the intended movements he makes. This should give more

DRAFT March 9, 2010

421

Learning HMMs

h1

m3

m2

m1

Figure 23.8: A model for robot self-localisation. At
each time the robot makes an intended movement,
mt. As a generative model, knowing the intended
movement mt and the current grid position ht, the
robot has an idea of where he should be at the next
time-step and what sensor reading vt+1 he would ex-
pect there. Based on only the sensor information v1:T
and the intended movements m1:T , the task is to infer
a distribution over robot locations p(h1:T|m1:T , v1:T ).
information as to where he could be. One can view this as extra ‘visible’ information, though it is more
natural to think of this as additional input information. A model of this scenario is, see ﬁg(23.8),

h4

v4

h2

v2

h3

v3

v1

T(cid:89)

t=1

T(cid:89)

t=1

p(v1:T , m1:T , h1:T ) =

p(vt|ht)p(ht|ht−1, mt−1)p(mt)

(23.2.38)

The visible variables v1:T are known, as are the intended movements m1:T . The model expresses that the
movements selected by the robot are random (hence no decision making in terms of where to go next). We
assume that the robot has full knowledge of the conditional distributions deﬁning the model (he knows
the ‘map’ of his environment and all state transition and emission probabilities). If our interest is only in
localising the robot, since the inputs m are known, this model is in fact a form of time-dependent HMM:

p(v1:T , h1:T ) =

p(vt|ht)p(ht|ht−1, t)

(23.2.39)

for a time-dependent transition p(ht|ht−1, t) deﬁned by the intended movement mt−1. Any inference task
required then follows the standard stationary HMM algorithms, albeit on replacing the time-independent
transitions p(ht|ht−1) with the known time-dependent transitions.
In self localisation and mapping (SLAM) the robot does not know the map of his environment. This
corresponds to having to learn the transition and emission distributions on-the-ﬂy as he explores the
environment.

23.2.7 Natural language models

A simple generative model of language can be obtained from the letter-to-letter transitions (a so-called
bigram). In the example below, we use this in a HMM to clean up the mis-typings.

Example 98 (Stubby ﬁngers). A ‘stubby ﬁngers’ typist has the tendency to hit either the correct key
or a neighbouring key. For simplicity we assume that there are 27 keys:
lower case a to lower case
z and the space bar. To model this we use an emission distribution Bij = p(v = i|h = j) where
i = 1, . . . , 27, j = 1, . . . , 27, as depicted in ﬁg(23.9). A database of letter-to-next-letter frequencies
(www.data-compression.com/english.shtml), yields the transition matrix Aij = p(h(cid:48) = i|h = j) in En-
glish. For simplicity we assume that p(h1) is uniform. Also we assume that each intended key press results
in a single press. Given a typed sequence kezrninh what is the most likely word that this corresponds to?
By listing the 200 most likely hidden sequences (using the N-max-product algorithm) and discarding those
that are not in a standard English dictionary (www.curlewcommunications.co.uk/wordlist.html), the
most likely word that was intended is learning. See demoHMMbigram.m.

23.3 Learning HMMs

Given a set of data V =(cid:8)v1, . . . , vN(cid:9) of N sequences, where sequence vn = vn

1:Tn is of length Tn, we seek
the HMM transition matrix A, emission matrix B, and initial vector a most likely to have have generated

422

DRAFT March 9, 2010

Learning HMMs

(a)

(b)

Figure 23.9: (a): The letter-to-letter transition matrix for English p(h(cid:48) = i|h = j).

(b): The letter
emission matrix for a typist with ‘stubby ﬁngers’ in which the key or its neighbours on the keyboard are
likely to be hit.

V. We make the i.i.d. assumption so that each sequence is independently generated and assume that we
know the number of hidden states H. For simplicity we concentrate here on the case of discrete visible
variables, assuming also we know the number of states V .

23.3.1 EM algorithm

The application of EM to the HMM model is called the Baum-Welch algorithm and follows the general
strategy outlined in section(11.2).

M-step

Assuming i.i.d. data, the M-step is given by maximising the ‘energy’:

n=1

T n, hn

1 , vn

2 . . . , vn

N(cid:88)
(cid:104)log p(vn
(cid:40)
(cid:104)log p(h1)(cid:105)pold(h1|vn) +

N(cid:88)

n=1

Tn−1(cid:88)

t=1

1 , hn

2 , . . . , hn

T n)(cid:105)pold(hn|vn)

Tn(cid:88)

t=1

with respect to the parameters A, B, a; hn denotes h1:Tn. Using the form of the HMM, we obtain

(cid:104)log p(ht+1|ht)(cid:105)pold(ht,ht+1|vn) +

(cid:104)log p(vn

t |ht)(cid:105)pold(ht|vn)

(23.3.2)

where for compactness we drop the sequence index from the h variables. To avoid potential confusion, we
write pnew(h1 = i) to denote the (new) table entry for the probability that the initial hidden variable is in
state i. Optimising equation (23.3.2) with respect to p(h1), (and enforcing p(h1) to be a distribution) we
obtain

anew
i ≡ pnew(h1 = i) =

1
N

N(cid:88)

n=1

pold(h1 = i|vn)
Tn−1(cid:88)
N(cid:88)

which is the average number of times that the ﬁrst hidden variable is in state i. Similarly,

which is the number of times that a transition from hidden state i to hidden state i(cid:48) occurs, averaged over
all times (since we assumed stationarity) and training sequences. Normalising, we obtain

pold(ht = i, ht+1 = i

(cid:48)

|vn)

n=1

|ht = i) ∝

(cid:48)
Anew
i(cid:48),i ≡ pnew(ht+1 = i
(cid:80)Tn−1
(cid:80)Tn−1
t=1 pold(ht = i, ht+1 = i(cid:48)
|vn)
t=1 pold(ht = i, ht+1 = i(cid:48)
|vn)

(cid:80)N
(cid:80)
i(cid:48)(cid:80)N

i(cid:48),i =

Anew

n=1

n=1

t=1

DRAFT March 9, 2010

(23.3.1)

(cid:41)

(23.3.3)

(23.3.4)

(23.3.5)

423

  abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz 00.10.20.30.40.50.60.70.80.9  abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz 0.050.10.150.20.250.30.350.40.450.50.55Finally,

Bnew
j,i ≡ pnew(vt = j|ht = i) ∝

N(cid:88)

Tn(cid:88)

n=1

t=1

I [vn

t = j] pold(ht = i|vn)

Learning HMMs

(23.3.6)

which is the expected number of times that, for the observation being in state j, the hidden state is i.
The proportionality constant is determined by the normalisation requirement.

E-step

In computing the M-step above the quantities pold(h1 = i|vn), pold(ht = i, ht+1 = i(cid:48)

are obtained by inference using the techniques described in section(23.2.1).

|vn) and pold(ht = i|vn)

Equations (23.3.3,23.3.5,23.3.6) are repeated until convergence. See HMMem.m and demoHMMlearn.m.

Parameter initialisation

The EM algorithm converges to a local maxima of the likelihood and, in general, there is no guarantee
that the algorithm will ﬁnd the global maximum. How best to initialise the parameters is a thorny issue,
(cid:80)
with a suitable initialisation of the emission distribution often being critical for success[229]. A practical
strategy is to initialise the emission p(v|h) based on ﬁrst ﬁtting a simpler non-temporal mixture model
h p(v|h)p(h) to the data.
Continuous observations
For a continuous vector observation vt, with dim vt = D, we require a model p(vt|ht) mapping the discrete
state ht to a distribution over outputs. Using a continuous output does not change any of the standard
inference message passing equations so that inference can be carried out for essentially arbitrarily complex
emission distributions.
Indeed, ﬁltering, smoothing and Viterbi inference, the normalisation Z of the
emission p(v|h) = φ(v, h)/Z is not required. For learning, however, the emission normalisation constant is
required since this is a dependent on the parameters of the model.

23.3.2 Mixture emission

To make a richer emission model (particularly for continuous observations), one approach is use a mixture

p(vt|ht) =(cid:88)

kt

(cid:88)

T(cid:88)

n

t=1

p(vt|kt, ht)p(kt|ht)

(23.3.7)

where kt is a discrete summation variable. For learning, it is useful to consider the kt as additional latent
variables so that updates for each component of the emission model can be derived. To achieve this,
consider the contribution to the energy from the emission (assuming equal length sequences):

Ev ≡

(cid:104)log p(vn

t |ht)(cid:105)q(ht|vn

1:T )

(23.3.8)

As it stands, the parameters of each component p(vt|kt, ht) are coupled in the above expression. One
approach is to consider

KL(q(kt|ht)|p(kt|ht, vt)) ≥ 0

from which we immediately obtain the bound

log p(vt, ht) ≥ −(cid:104)log q(kt|ht)(cid:105)q(kt|ht) + (cid:104)log p(vt, kt, ht)(cid:105)q(kt|ht)

(23.3.9)

(23.3.10)

and

424

log p(vn

t |hn

t ) ≥ −(cid:104)log q(kt|hn

t )(cid:105)q(kt|hn

t ) + (cid:104)log p(vn

t |kt, hn

t )(cid:105)q(kt|hn

t ) + (cid:104)log p(kt|hn

t )

(23.3.11)

t )(cid:105)q(kt|hn
DRAFT March 9, 2010

Learning HMMs

(cid:88)

T(cid:88)

(cid:68)

n

t=1

(cid:88)

T(cid:88)

n

t=1

(cid:69)

Using this in the energy contribution (23.3.8) we have the bound on the energy contribution

Ev ≥

−(cid:104)log q(kt|hn

t )(cid:105)q(kt|hn

t ) + (cid:104)log p(vn

t |kt, hn

t )(cid:105)q(kt|hn

t ) + (cid:104)log p(kt|hn

t )(cid:105)q(kt|hn

t )

q(hn

t |vn

1:T )

(23.3.12)

We may now maximise this lower bound on the energy (instead of the energy itself). The contribution
from each emission component p(v = v|h = h, k = k) is

q(kt = k|hn

t = h)q(hn

t = h|vn

1:T ) log p(vn

t |h = h, k = k)

(23.3.13)

The above can then be optimised (M-step) for ﬁxed q(kt = k|hn
using

t = h), with these distributions updated

The contribution to the energy bound from the mixture weights is given by

t , kt)p(kt|ht)
T(cid:88)

qnew(kt|hn

t ) ∝ p(vn|hn

log p(k = k|h = h)(cid:88)
T(cid:88)
(cid:88)

n

p(k = k|h = h) ∝

t=1

n

t=1

q(kt = k|hn

t = h)q(hn

t = h|vn

1:T )

q(kt = k|hn

t = h)q(hn

t = h|vn

1:T )

so that the M-step update for the mixture weights is,

(23.3.14)

(23.3.15)

(23.3.16)

1:T ) is ﬁxed, during which the emissions p(v|h, k) are learned, along with updating q(kt = k|hn

t =
In this case the EM algorithm is composed of an ‘emission’ EM loop in which the transitions and q(hn
t = h).
h|vn
The ‘transition’ EM loop ﬁxes the emission distribution p(v|h) and learns the best transition p(ht|ht−1).
An alternative to the above derivation is to consider the k as hidden variables, and then use standard
EM algorithm on the joint latent variables (ht, kt). The reader may show that the two approaches are
equivalent.

23.3.3 The HMM-GMM

(cid:0)vt µkt,ht, Σkt,ht

(cid:1)

A common continuous observation mixture emission model component is a Gaussian

p(vt|kt, ht) = N

(23.3.17)
so that kt, ht indexes the K × H mean vectors and covariance matrices. EM updates for these means
and covariances are straightforward to derive from equation (23.3.12), see exercise(232). These models are
common in tracking applications, in particular in speech recognition (usually under the constraint that
the covariances are diagonal).

23.3.4 Discriminative training

HMMs can be used for supervised learning of sequences. That is, for each sequence vn
1:T , we have a
corresponding class label cn. For example, we might associated a particular composer c with a sequence v1:T
and wish to make a model that will predict the composer for a novel music sequence. A generative approach
to using HMMs for classiﬁcation is to train a separate HMM for each class, p(v1:T|c) and subsequently use
Bayes’ rule to form the classiﬁcation for a novel sequence v∗

1:T using

(23.3.18)

∗
p(c

|v

∗
1:T ) =

(cid:80)C
p(v∗
c(cid:48)=1 p(v∗

1:T|c∗)p(c∗)

1:T|c(cid:48))p(c(cid:48))

If the data is noisy and diﬃcult to model, however, this generative approach may not work well since
much of the expressive power of each model is used to model the complex data, rather than focussing on

DRAFT March 9, 2010

425

c1

h1

v1

c2

h2

v2

c3

h3

v3

c4

h4

v4

Related Models

Figure 23.10: An explicit duration HMM. The
counter variables ct deterministically count down to
zero. When they reach one, a h transition is allowed,
and the new value for ct is sampled.

the decision boundary. In applications such as speech recognition, improvements in performance are often
reported when the models are trained in a discriminative way. In discriminative training, see for example
[150], one deﬁnes a new single discriminative model, formed from the C HMMs using

(23.3.19)

p(c|v1:T ) =

(cid:80)C
p(v1:T|c)p(c)
c(cid:48)=1 p(v1:T|c(cid:48))p(c(cid:48))

log p(cn|vn

1:T ) = log p(vn

+ log p(c) − log

(cid:124)

(cid:123)(cid:122)
(cid:125)
1:T|c)

generative likelihood

C(cid:88)

c(cid:48)=1

and then maximises the likelihood of a set of observed classes and corresponding observations v1:T . For a
single data pair, (cn, vn

1:T ), the log likelihood is

(cid:48))p(c
(cid:48))

p(vn

1:T|c

(23.3.20)

p(v1:T , c1:T ) =(cid:89)

t

The ﬁrst term above represents the generative likelihood term, with the last term accounting for the dis-
crimination. Whilst deriving EM style updates is hampered by the discriminative terms, computing the
gradient is straightforward using the technique described in section(11.7).

In some applications, a class label ct is available at each timestep, together with an observation vt. Given
a training sequence v1:T , c1:T (or more generally a set of sequences) the aim is to ﬁnd the optimal class
sequence c∗

1:T for a novel observation sequence v∗

1:T . One approach is to train a generative model

p(vt|ct)p(ct|ct−1)

(23.3.21)

(cid:82)

and subsequently use Viterbi to form the class c∗
1:T = arg maxc1:T p(c1:T|v1:T ). However, this approach may
not be optimal in terms of class discrimination. A cheap surrogate is to train a discriminative classiﬁcation
model ˜p(ct|vt) separately. With this one can form the emission (here written for continuous vt)

˜p(ct|vt)˜p(vt)
˜p(ct|vt)˜p(vt)

vt

p(vt|ct) =

where ˜p(vt) is user deﬁned. Whilst computing the local normalisation(cid:82)
if the only use of p(vt|ct) is to ﬁnd the optimal class sequence for a novel observation sequence v∗
1:T ,
(23.3.23)

˜p(ct|vt)˜p(vt) may be problematic,

∗
1:T = argmax
c

(23.3.22)

∗
1:T )

vt

p(c1:T|v

c1:T

then the local normalisations play no role since they are independent of c. Hence, during Viterbi decoding
we may replace the term p(vt|ht) with ˜p(ct|vt) without aﬀecting the optimal sequence. Using a model in
this way is a special case of the general hybrid procedure described in section(13.2.4). The approach is
suboptimal since learning the classiﬁer is divorced from learning the transition model. Nevertheless, this
heuristic historically has some support in the speech recognition community.

23.4 Related Models

23.4.1 Explicit duration model
For a HMM with self-transition p(ht = i|ht−1 = i) ≡ γi, the probability that the latent dynamics stays
i , which decays exponentially with time. In practice, however, we would
in state i for τ timesteps is γτ

426

DRAFT March 9, 2010

Related Models

x1

h1

v1

x2

h2

v2

x3

h3

v3

x4

h4

v4

Figure 23.11: A ﬁrst order input-output hidden
Markov model. The input x and output v nodes are
shaded to emphasise that their states are known dur-
ing training. During testing, the inputs are known
and the outputs are predicted.

often like to constrain the dynamics to remain in the same state for a minimum number of timesteps, or
to have a speciﬁed duration distribution. A way to enforce this is to use a latent counter variable ct which
at the beginning is initialised to a duration sampled from the duration distribution pdur(ct) with maximal
duration Dmax. Then at each timestep the counter decrements by 1, until it reaches 1, after which a new
duration is sampled:

p(ct|ct−1) =

The state ht can transition only when ct = 1:

(cid:26) δ (ct, ct−1 − 1)
(cid:26) δ (ht, ht−1)

pdur(ct)

ct−1 > 1
ct−1 = 1

ct > 1
ct = 1

p(ht|ht−1, ct) =

ptran(ht|ht−1)

(23.4.1)

(23.4.2)

Including the counter variable c deﬁnes a joint latent variable (c1:T , h1:T ) distribution that ensures h re-
mains in a desired minimal number of timesteps, see ﬁg(23.10). Since dim ct ⊗ ht = DmaxH, naively the
the forward and backward recursions, the deterministic nature of the transitions means that this can be

computational complexity of inference in this model scales as O(cid:0)T H 2D2
reduced to O(cid:0)T H 2Dmax

(cid:1)[199] – see also exercise(233).

(cid:1). However, when one runs

max

The hidden semi-Markov model generalises the explicit duration model in that once a new duration ct is
sampled, the model emits a distribution p(vt:t+ct−1|ht) deﬁned on a segment of the next ct observations[216].
23.4.2 Input-Output HMM

The IOHMM[31] is a HMM with additional input variables x1:T , see ﬁg(23.11). Each input can be con-
tinuous or discrete and modulates the transitions

p(vt|ht, xt)p(ht|ht−1, xt)

(23.4.3)

p(v1:T , h1:T|x1:T ) =(cid:89)

t

The IOHMM may be used as a conditional predictor, where the outputs vt represent the prediction at
time t. In the case of continuous inputs and discrete outputs, the tables p(vt|ht, xt) and p(ht|ht−1, xt) are
usually parameterised using a non-linear function, for example

p(vt = y|ht = h, xt = x, w) ∝ ewT

h,yx

Inference then follows in a similar manner as for the standard HMM. Deﬁning

α(ht) ≡ p(ht, v1:t|x1:t)
the forward pass is given by

α(ht) =(cid:88)
=(cid:88)
= p(vt|xt, ht)(cid:88)

ht−1

ht−1

ht−1

DRAFT March 9, 2010

p(ht, ht−1, v1:t−1, vt|x1:t)

p(vt|v1:t−1, x1:t, ht, ht−1)p(ht|v1:t−1, x1:t, ht−1)p(v1:t−1, ht−1|x1:t)

p(ht|ht−1, xt)α(ht−1)

(23.4.4)

(23.4.5)

(23.4.6)

(23.4.7)

(23.4.8)

427

x

y2

y1

Figure 23.12: Linear chain CRF. Since the input x is observed, the dis-
tribution is just a linear chain factor graph. The inference of pairwise
marginals p(yt, yt−1|x) is therefore straightforward using message passing.

y3

Related Models

The γ backward pass is

p(ht|x1:T , v1:T ) =(cid:88)

ht+1

p(ht, ht+1|x1:t+1, xt+2:T , v1:T ) =(cid:88)

ht+1

p(ht|ht+1, x1:t+1, v1:t)p(ht+1|x1:T , v1:T )

for which we need

p(ht|ht+1, x1:t+1, v1:t) = p(ht+1, ht|x1:t+1, v1:t)
p(ht+1|x1:t+1, v1:t)

The likelihood can be found from(cid:80)

α(hT ).

hT

(cid:80)
= p(ht+1|ht, xt+1)p(ht|x1:t, v1:t)
ht p(ht+1|ht, xt+1)p(ht|x1:t, v1:t)

(23.4.9)

(23.4.10)

Direction bias
Consider predicting the output distribution p(vt|x1:T ) given both past and future input information x1:T .
Because the hidden states are unobserved we have p(vt|x1:T ) = p(vt|x1:t). Thus the prediction uses
only past information and discards any future contextual information. This ‘direction bias’ is sometimes
considered problematic (particularly in natural language modelling) and motivates the use of undirected
models, such as conditional random ﬁelds.

23.4.3 Linear chain CRFs

Linear chain Conditional Random Fields (CRFs) are an extension of the unstructured CRFs we brieﬂy
discussed in section(9.4.6) and have application to modelling the distribution of a set of outputs y1:T given
an input vector x. For example, x might represent a sentence in English, and y1:T should represent the
translation into French. Note that the vector x does not have to have dimension T . A ﬁrst order linear
chain CRF has the form

p(y1:T|x, λ) =

1

Z(x, λ)

t=2

φt(yt, yt−1, x, λ)

(23.4.11)

where λ are the free parameters of the potentials. In practice it is common to use potentials of the form

T(cid:89)

(cid:33)

exp

λkfk,t(yt, yt−1, x)

(23.4.12)

where fk,t(yt, yt−1, x) are ‘features’, see also section(9.4.6). Given a set of input-output sequence pairs,
xn, yn
1:T , n = 1, . . . , N (assuming all sequenced have equal length T for simplicity), we can learn the
parameters λ by Maximum Likelihood. Under the standard i.i.d. data assumption, the log likelihood is

λkfk(yn

t , yn

t−1, xn) −

log Z(xn, λ)

(23.4.13)

(cid:88)

n

The reader may readily check that the log likelihood is concave so that the objective function has no local
optima. The gradient is given by

fi(yn

t , yn

t−1, xn) − (cid:104)fi(yt, yt−1, xn)(cid:105)p(yt,yt−1|xn,λ)

(23.4.14)

Learning therefore requires inference of the marginal terms p(yt, yt−1|x, λ). Since equation (23.4.11) cor-
responds to a linear chain factor graph, see ﬁg(23.12), inference of pairwise marginals is straightforward

428

DRAFT March 9, 2010

(cid:32) K(cid:88)

k=1

L(λ) =(cid:88)

(cid:88)

t,n

k

L(λ) =(cid:88)

(cid:16)

n,t

∂
∂λi

(cid:17)

Related Models

(a)

(b)

Figure 23.13: Using a linear chain CRF to learn the sequences in table(23.1).
log likelihood under gradient ascent. (b): The learned parameter vector λ at the end of training.

(a): The evolution of the

using message passing. This can be achieved using either the standard factor graph message passing or
by deriving an explicit algorithm, see exercise(227).

Finding the most likely output sequence for a novel input x∗ is straightforward since

(cid:89)

t

∗
1:T = argmax
y

y1:T

φt(yt, yt−1, x∗

, λ)

(23.4.15)

corresponds again to a simple linear chain, for which max-product inference yields the required result, see
also exercise(226).

In some applications, particularly in natural language processing, the dimension K of the vector of features
f1, . . . , fK may be many hundreds of thousands. This means that the storage of the Hessian is not
feasible for Newton based training and either limited memory methods or conjugate gradient techniques
are typically preferred[288].

7
3
10
1
9
2
7
2
7
3

4
1
3
1
8
3
8
3
9
1

7
3
2
1
3
3
8
3
3
2

2
1

9
3
4
1
3
3

3
2

6
3
4
3

4
3

10
1
8
3

5
3

2
1
8
3

7
1

7
3

3
2

7
2

5
1

6
2

Table 23.1: A subset of the 10 training input-output
sequences. Each row contains an input xt (upper en-
try) and output yt (lower entry). There are 10 input
states and 3 output states.

6
3

10
1

potentials φ(yt, yt−1, xt) = exp ((cid:80)

Example 99 (Linear Chain CRF). As a model for the data in table(23.1), a linear CRF model has
i λifi(yt, yt−1, xt)) where we set the binary feature functions by ﬁrst
mapping each of the dim (x) × dim (y)2 states to a unique integer i(a, b, c) from 1 to dim (x) × dim (y)2
(23.4.16)

fi(a,b,c)(yt, yt−1, xt) = I [yt = a] I [yt−1 = b] I [xt = c]

That is, each joint conﬁguration of yt, yt−1, xt is mapped to an index, and in this case the feature vector f
will trivially have only a single non-zero entry. The evolution of the gradient ascent training algorithm is
plotted in ﬁg(23.13). In practice one would use richer feature functions deﬁned to seek features of the input
sequence x and also to produce a feature vector with more than one non-zero entry. See demoLinearCRF.m.

DRAFT March 9, 2010

429

05101520253035404550−14−12−10−8−6−4−200510152025303540−20−15−10−50510152025Applications

Figure 23.14: A Dynamic Bayesian Network. Possi-
ble transitions between variables at the same time-
slice have not been shown.

Figure 23.15: A Coupled HMM. For example the up-
per HMM might model speech, and the lower the cor-
responding video sequence. The upper hidden units
then correspond to phonemes, and the lower to mouth
positions; this model therefore captures the expected
coupling between mouth positions and phonemes.

x3(t)

x2(t)

x1(t)

x3(t + 1)

x2(t + 1)

x1(t + 1)

v1(t)

v1(t + 1)

h1(t)

h1(t + 1)

h2(t)

h2(t + 1)

v2(t)

v2(t + 1)

23.4.4 Dynamic Bayesian networks

A DBN is deﬁned as a belief networkreplicated through time. For a multivariate xt, with dim xt = D, the
DBN deﬁnes a joint model

T(cid:89)

D(cid:89)

t=1

i=1

p(x1, . . . , xT ) =

p(xi(t)|x\i(t), x(t − 1))

(23.4.17)

where x\i(t) denotes the set of variables at time t, except for xi(t). The form of each p(xi(t)|x\i(t), x(t−1))
is chosen such that the overall distribution remains acyclic. At each time-step t there is a set of variables
xi(t), i = 1, . . . , X, some of which may be observed. In a ﬁrst order DBN, each variable xi(t) has parental
variables taken from the set of variables in the previous time-slice, xt−1, or from the present time-slice. In
most applications, the model is temporally homogeneous so that one may fully describe the distribution in
terms of a two-time-slice model, ﬁg(23.14). The generalisation to higher-order models is straightforward.
A coupled HMM is a special DBN that may be used to model coupled ‘streams’ of information, for example
video and audio, see ﬁg(23.15)[209].

23.5 Applications

23.5.1 Object tracking

HMMs are used to track moving objects, based on an understanding of the dynamics of the object (encoded
in the transition distribution) and an understanding of how an object with a known position would be
observed (encoded in the emission distribution). Given an observed sequence, the hidden position can then
be inferred. The burglar, example(10) is a case in point. HMMs have been applied in a many tracking
contexts, including tracking people in videos, musical pitch, and many more[54, 229, 51].

23.5.2 Automatic speech recognition

Many speech recognition systems make use of HMMs[300]. Roughly speaking, a continuous output vector
vt at time t, represents which frequencies are present in the speech signal in a small window around time t.
These acoustic vectors are typically formed from taking a discrete Fourier transform of the speech signal
over a small window around time t, with additional transformations to mimic human auditory processing.
Alternatively, related forms of linear coding of the observed acoustic waveform may be used[131].

The corresponding discrete latent state ht represents a phoneme – a basic unit of human speech (for which
there are 44 in standard English). Training data is painstakingly constructed by a human linguist who

430

DRAFT March 9, 2010

Applications

determines the phoneme ht for each time t and many diﬀerent observed sequences vt. Given then each
acoustic vector vt and an associated phoneme ht, one may use maximum likelihood to ﬁt a mixture of
(usually isotropic) Gaussians p(vt|ht) to vt. This forms the emission distribution for a HMM.
Using the database of labelled phonemes, the phoneme transition p(ht|ht−1) can be learned (by simple
counting) and forms the transition distribution for a HMM. Note that in this case, since the ‘hidden’
variable h and observation v are known during training, training the HMM is straightforward and boils
down to training the emission and transition distributions independently.

For a new sequence of ‘acoustic’ vectors v1:T we can then use the HMM to infer the most likely phoneme
sequence through time, arg maxh1:T p(h1:T|v1:T ), which takes into account both the way that phonemes
appear as acoustic vectors, and also the prior language constraints of likely phoneme to phoneme transi-
tions. The fact that people speak at diﬀerent speeds can be addressed using time-warping in which the
latent phoneme remains in the same state for a number of timesteps.

HMM models are typically trained on the assumption of ‘clean’ underlying speech. In practice noise cor-
rupts the speech signal in a complex way, so that the resulting model is inappropriate, and performance
degrades signiﬁcantly. To account for this, it is traditional to attempt to denoise the signal before sending
this to a standard HMM recogniser.

If the HMM is used to model a single word, it is natural to constrain the hidden state sequence to go
‘forwards’ through time, visiting a set of states in sequence (since the phoneme order for the word is
known).
In this case the structure of the transition matrices is upper triangular (or lower, depending
on your deﬁnition), or even a banded triangular matrix. Such forward constraints describe a so-called
left-to-right transition matrix.

23.5.3 Bioinformatics

In the ﬁeld of Bioinformatics HMMs have been widely applied to modelling genetic sequences. Multiple
sequence alignment using forms of constrained HMMs have been particularly successful. Other applications
involve gene ﬁnding and protein family modelling[163, 84].

23.5.4 Part-of-speech tagging

Consider the sentence below in which each word has been linguistically tagged

hospitality_NN is_BEZ an_AT excellent_JJ virtue_NN ,_,
but_CC not_XNOT when_WRB the_ATI guests_NNS have_HV
to_TO sleep_VB in_IN rows_NNS in_IN the_ATI cellar_NN !_!

The subscripts denote a linguistic tag, for example NN is the singular common noun tag, ATI is the article
tag etc. Given a training set of such tagged sequences, the task is to tag a novel word sequence. One
approach is to use ht to be a tag, and vt to be a word and ﬁt a HMM to this data. For the training
data, both the tags and words are observed so that Maximum Likelihood training of the transition and
emission distribution can be achieved by simple counting. Given a new sequence of words, the most likely
tag sequence can be inferred using the Viterbi algorithm.

More recent part-of-speech taggers tend to use conditional random ﬁelds in which the input sequence x1:T
is the sentence and the output sequence y1:T is the tag sequence. One possible parameterisation of for a
linear chain CRF is to use a potential of the form φ(yt−1, yt)φ(yt, x) in which the ﬁrst factor encodes the
grammatical structure of the language and the second the a priori likely tag yt[166].

DRAFT March 9, 2010

431

Exercises

23.6 Code

demoMixMarkov.m: Demo for Mixture of Markov models
mixMarkov.m: Mixture of Markov models
demoHMMinference.m: Demo of HMM Inference
HMMforward.m: Forward α recursion
HMMbackward.m: Forward β recursion
HMMgamma.m: RTS γ ‘correction’ recursion
HMMsmooth.m: Single and Pairwise α − β smoothing
HMMviterbi.m: Most Likely State (Viterbi) algorithm
demoHMMburglar.m: Demo of Burglar Localisation
demoHMMbigram.m: demo of stubby ﬁngers typing
HMMem.m: EM algorithm for HMM (Baum-Welch)
demoHMMlearn.m: demo of EM algorithm for HMM (Baum-Welch)
demoLinearCRF.m: demo of learning a linear chain CRF

The following linear chain CRF potential is particularly simple and in practice one would use a more
complex one.
linearCRFpotential.m: Linear CRF potential
The following likelihood and gradient routines are valid for any linear CRF potential φ(yt−1, yt, x).
linearCRFgrad.m: Linear CRF gradient
linearCRFloglik.m: Linear CRF log likelihood

23.7 Exercises

Exercise 217. A stochastic matrix Mij as non-negative entries with(cid:80)
λ and eigenvector e such(cid:80)

j Mijej = λei. By summing over i show that, provided(cid:80)

i Mij = 1. Consider an eigenvalue
i ei > 0, then λ must

Exercise 218. Consider the Markov chain with transition matrix

be equal to 1.

(cid:19)

(cid:18) 0 1

1 0

M =

Show that this Markov chain does not have an equilibrium distribution and state a stationary distribution
for this chain.

Exercise 219. Consider a HMM with 3 states (M = 3) and 2 output symbols, with a left-to-right state
transition matrix

(23.7.1)

(23.7.2)

(23.7.3)

 0.5 0.0 0.0
(cid:18) 0.7 0.4 0.8

0.3 0.6 0.0
0.2 0.4 1.0

0.3 0.6 0.2


(cid:19)

A =

B =

where Aij ≡ p(h(t + 1) = i|h(t) = j), emission matrix Bij ≡ p(v(t) = i|h(t) = j)

and initial state probability vector a = (0.9 0.1 0.0)T. Given the observed symbol sequence is v1:3 = (0, 1, 1):

1. Compute p(v1:3).
2. Compute p(h1|v1:3).
3. Find the most probable hidden state sequence arg maxh1:3 p(h1:3|v1:3).

432

DRAFT March 9, 2010

Exercises

Exercise 220. This exercise follows from example(98). Given the 27 long character string
rgenmonleunosbpnntje vrancg typed with ‘stubby ﬁngers’, what is the most likely correct English sentence
intended? In the list of decoded sequences, what value is log p(h1:27|v1:27) for this sequence? You will need
to modify demoHMMbigram.m suitably.
Exercise 221. Show that if a transition probability Aij = p(ht = i|ht−1 = j) in a HMM is initialised to
zero for EM training, then it will remain at zero throughout training.

Exercise 222. Consider the problem : Find the most likely joint output sequence v1:T for a HMM. That
is,

v

∗
1:T ≡ argmax

v1:T

p(v1:T )

where

p(h1:T , v1:T ) =

T(cid:89)

t=1

p(vt|ht)p(ht|ht−1)

(23.7.4)

(23.7.5)

1. Explain why a local message passing algorithm cannot, in general, be found for this problem and

discuss the computational complexity of ﬁnding an exact solution.

2. Explain how to adapt the Expectation-Maximisation algorithm to form a recursive algorithm, for
1:T . Explain which it guarantees an improved solution at each iteration.

ﬁnding an approximate v∗
Additionally, explain how the algorithm can be implemented using local message passing.

Exercise 223. Explain how to train a HMM using EM, but with a constrained transition matrix.
particular, explain how to learn a transition matrix with an upper triangular structure.

In

Exercise 224. Write a program to ﬁt a mixture of Lth order Markov models.

Exercise 225.

1. Using the correspondence A = 1, C = 2, G = 3, T = 4 deﬁne a 4× 4 transition matrix p that produces

sequences of the form

A, C, G, T, A, C, G, T, A, C, G, T, A, C, G, T, . . .

Now deﬁne a new transition matrix

pnew = 0.9*p + 0.1*ones(4)/4

Deﬁne a 4 × 4 transition matrix q that produces sequences of the form

T, G, C, A, T, G, C, A, T, G, C, A, T, G, C, A, . . .

Now deﬁne a new transition matrix

qnew = 0.9*q + 0.1*ones(4)/4

(23.7.6)

(23.7.7)

(23.7.8)

(23.7.9)

Assume that the probability of being in the initial state of the Markov chain p(h1) is constant for all
four states A, C, G, T . What is the probability that the Markov chain pnew generated the sequence S
given by

S ≡ A, A, G, T, A, C, T, T, A, C, C, T, A, C, G, C

(23.7.10)

2. Similarly what is the probability that S was generated by qnew? Does it make sense that S has a

higher likelihood under pnew compared with qnew?

DRAFT March 9, 2010

433

3. Using the function randgen.m, generate 100 sequences of length 16 from the Markov chain deﬁned by
pnew. Similarly, generate 100 sequences each of length 16 from the Markov chain deﬁned by qnew.
Concatenate all these sequences into a cell array v so that v{1} contains the ﬁrst sequence and
v{200} the last sequence. Use MixMarkov.m to learn the optimum Maximum Likelihood parameters
that generated these sequences. Assume that there are H = 2 kinds of Markov chain. The result
returned in phgvn indicate the posterior probability of sequence assignment. Do you agree with the
solution found?

Exercises

4. Take the sequence S as deﬁned in equation (23.7.10). Deﬁne an emission distribution that has 4

output states such that

p(v = i|h = j) =

(cid:26) 0.7 i = j

0.1 i (cid:54)= j

Using this emission distribution and the transition given by pnew deﬁned in equation (23.7.7), adapt
demoHMMinferenceSimple.m suitably to ﬁnd the most likely hidden sequence hp
1:16 that generated the
observed sequence S. Repeat this computation but for the transition qnew to give hq
1:16. Which hidden
sequence – hp

1:16 is to be preferred? Justify your answer.

1:16 or hq

Exercise 226. Derive an algorithm that will ﬁnd the most likely joint state

T(cid:89)

t=2

T(cid:89)

t=2

argmax

h1:T

φt(ht−1, ht)

for arbitrarily deﬁned potentials φt(ht−1, ht).

1. First consider

max
h1:T

φt(ht−1, ht)

γT−1←T (hT−1)

2. Derive the recursion

γt−1←t(ht−1) = max
ht

φt(ht, ht−1)γt←t+1(ht)

3. Explain how the above recursion enables the computation of

T(cid:89)

t=2

argmax

h1

φt(ht, ht−1)

Show that how the maximisation over hT may be pushed inside the product and that the result of the
maximisation can be interpreted as a message

(23.7.11)

(23.7.12)

(23.7.13)

(23.7.14)

(23.7.15)

(23.7.16)

(23.7.17)

(23.7.18)

4. Explain how once the most likely state for h1 is computed, one may eﬃciently compute the remaining

optimal states h2, . . . , hT .

Exercise 227. Derive an algorithm that will compute pairwise marginals

p(ht, ht−1)

from the joint distribution

T(cid:89)

t=2

p(h1:T ) ∝

φt(ht−1, ht)

for arbitrarily deﬁned potentials φt(ht−1, ht).

434

DRAFT March 9, 2010

Exercises

1. First consider(cid:88)
T(cid:89)

h1,...,hT

t=2

φt(ht, ht−1)

(23.7.19)

Show that how the summation over h1 may be pushed inside the product and that the result of the
maximisation can be interpreted as a message

2. Derive the recursion

φt(ht−1, ht)αt−2→t−1(ht−1)

3. Similarly, show that one can push the summation of hT inside the product to deﬁne

φ2(h1, h2)

h1

ht−1

α1→2(h2) =(cid:88)
αt−1→t(ht) =(cid:88)
βT−1←T (hT−1) =(cid:88)
βt←t+1(ht) =(cid:88)
(cid:88)

ht+1

hT

p(ht, ht−1) ∝

ht+1

φT (hT−1, hT )

and that by pushing in hT−1 etc. one can deﬁne messages

φt+1(ht, ht+1)βt+1←t+2(ht+1)

4. Show that

αt−2→t−1(ht−1)φ(ht−1, ht)βt←t+1(ht)

(23.7.20)

(23.7.21)

(23.7.22)

(23.7.23)

(23.7.24)

(23.7.27)

(23.7.28)

435

Exercise 228. A second order HMM is deﬁned as

pHM M 2(h1:T , v1:T ) = p(h1)p(v1|h1)p(h2|h1)p(v2|h2)

T(cid:89)

t=3

p(ht|ht−1, ht−2)p(vt|ht)

(23.7.25)

Following a similar approach to the ﬁrst order HMM, derive explicitly a message passing algorithm to
compute the most likely joint state

argmax

h1:T

pHM M 2(h1:T|v1:T )

(23.7.26)

Exercise 229. Since the likelihood of the HMM can be computed using ﬁltering only, in principle we do
not need smoothing to maximise the likelihood (contrary to the EM approach). Explain how to compute
the likelihood gradient by the use of ﬁltered information alone (i.e. using only a forward pass).
Exercise 230. Derive the EM updates for ﬁtting a HMM with an emission distribution given by a mixture
of multi-variate Gaussians.
Exercise 231. Consider the HMM deﬁned on hidden variables H = {h1, . . . , hT} and observations V =
{v1, . . . , vT}

T(cid:89)

p(V,H) = p(h1)p(v1|h1)

p(ht|ht−1)p(vt|ht)
Show that the posterior p(H|V) is a Markov chain

t=2

T(cid:89)

t=2

p(H|V) = ˜p(h1)

˜p(ht|ht−1)

where ˜p(ht|ht−1) and ˜p(h1) are suitably deﬁned distributions.
DRAFT March 9, 2010

Exercise 232. For training a HMM with a Gaussian mixture emission (the HMM-GMM model) in
section(23.3.3), derive the following EM update formulae for the means and covariances:

N(cid:88)

T(cid:88)

n=1

t=1

N(cid:88)

T(cid:88)

n=1

t=1

µnew

k,h =

µnew

k,h =

ρk,h(t, n) =

(cid:80)

n

and

where

ρk,h(t, n)vn
t

ρk,h(t, n)(cid:0)vn
(cid:80)
q(kt = k|hn
t q(kt = k|hn

(cid:1)(cid:0)vn

t − µk,h

(cid:1)T

t − µk,h

t = h)q(hn

t = h)q(hn

t = h|vn

1:T )
t = h|vn

1:T )

Exercises

(23.7.29)

(23.7.30)

(23.7.31)

Exercise 233. Consider the HMM duration model deﬁned by equation (23.4.2) and equation (23.4.1) with
emission distribution p(vt|ht). Our interest is to derive a recursion for the ﬁltered distribution

αt(ht, ct) ≡ p(ht, ct, v1:t)

1. Show that :

αt(ht, ct) = p(vt|ht) (cid:88)

ht−1,ct−1

p(ht|ht−1, ct)p(ct|ct−1)αt−1(ht−1, ct−1)

2. Using this derive

=(cid:88)

ht−1

αt(ht, ct)
p(vt|ht)

p(ht|ht−1, c)p(ct|ct−1 = 1)αt−1(ht−1, ct−1 = 1)
Dmax(cid:88)

+(cid:88)

p(ht|ht−1, c)

ht−1

ct−1=2

p(c|ct−1)αt−1(ht−1, ct−1)

(23.7.32)

(23.7.33)

(23.7.34)

3. Show that the right hand side of the above can be written as

(cid:88)

ht−1

p(ht|ht−1, ct = c)p(ct = c|ct−1 = 1)αt−1(ht−1, 1)

+ I [c (cid:54)= Dmax](cid:88)

ht−1

p(ht|ht−1, c)αt−1(ht−1, c + 1)

(23.7.35)

4. Show that the recursion for α is then given by

αt(h, 1) = p(vt|ht = h)pdur(1)(cid:88)

ht−1

ptran(h|ht−1)αt−1(ht−1, 1)

+ I [Dmax (cid:54)= 1] p(vt|ht = h)(cid:88)

ht−1

ptran(ht|ht−1)αt−1(ht−1, 2)

(23.7.36)

and for c > 1

αt(h, c) = p(vt|ht = h){pdur(c)αt−1(h, 1) + I [c (cid:54)= Dmax] αt−1(h, c + 1)}

5. Explain why the computational complexity of ﬁltered inference in the duration model is O(cid:0)T H 2Dmax

(23.7.37)

(cid:1).

6. Derive an eﬃcient smoothing algorithm for this duration model.

436

DRAFT March 9, 2010

CHAPTER 24

Continuous-state Markov Models

24.1 Observed Linear Dynamical Systems

In many practical timeseries applications the data is naturally continuous, particularly for models of
the physical environment.
In contrast to discrete-state Markov models, chapter(23), continuous state
distributions are not automatically closed under operations such as products and marginalisation. To
make practical algorithms for which inference and learning can be carried eﬃciently, we therefore are
heavily restricted in the form of the continuous transition p(vt|vt−1). A simple yet powerful class of such
transitions are the linear dynamical systems. A deterministic observed linear dynamical system1 (OLDS)
deﬁnes the temporal evolution of a vector vt according to the discrete-time update equation

vt = Atvt−1

(24.1.1)

where At is the transition matrix at time t. For the case that At is invariant with t, the process is called
time-invariant, which we assume throughout unless explicitly stated otherwise.

A motivation for studying OLDSs is that many equations that describe the physical world can be written
as an OLDS. OLDSs are interesting since they may be used as simple prediction models: if vt describes the
state of the environment at time t, then Avt predicts the environment at time t+1. As such, these models,
have widespread application in many branches of science, from engineering and physics to economics.

The OLDS equation (24.1.1) is deterministic so that if we specify v1, all future values v2, v3, . . . , are
deﬁned. For a dim v = V dimensional vector, its evolution is described by

vt = At−1v1 = PΛt−1P−1v1

(24.1.2)

where Λ = diag (λ1, . . . , λV ), is the diagonal eigenvalue matrix, and P is the corresponding eigenvector
matrix of A. If λi > 1 then for large t, vt will explode. On the other hand, if λi < 1, then λt−1
i will tend
to zero. For stable systems we require therefore no eigenvalues of magnitude greater than 1 and only unit
eigenvalues will contribute in long term. Note that the eigenvalues may be complex which corresponds to
rotational behaviour, see exercise(234). More generally, we may consider additive noise on v and deﬁne a
stochastic OLDS.

Deﬁnition 111 (Observed Linear Dynamical System).

vt = Atvt−1 + ηt

(24.1.3)

1We use the terminology ‘observed’ LDS to diﬀerentiate from the more general LDS state-space model. In some texts,

however, the term LDS is applied to the models under discussion in this chapter.

437

where ηt is a noise vector sampled from a Gaussian distribution,

N (ηt µt, Σt)

This is equivalent to a ﬁrst order Markov model

p(vt|vt−1) = N (vt Atvt−1 + µt, Σt)

Auto-Regressive Models

(24.1.4)

(24.1.5)

At t = 1 we have an initial distribution p(v1) = N (v1 µ1, Σ1). For t > 1 if the parameters are time-
independent, µt ≡ µ, At ≡ A, Σt ≡ Σ, the process is called time-invariant.

24.1.1 Stationary distribution with noise

Consider the one-dimensional linear system

(cid:0)ηt 0, σ2

v

(cid:1)

vt = avt−1 + ηt,

ηt ∼ N

If we start at some state v1, and then for t > 1 recursively sample according to vt = avt−1 + ηt, does
the distribution of the vt, t (cid:29) 1 tend to a steady, ﬁxed distribution? Assuming that we can represent the
distribution of vt−1 as a Gaussian with mean µt−1 and variance σ2
using (cid:104)ηt(cid:105) = 0 we have

t−1, vt−1 ∼ N

(cid:0)vt−1 µt−1, σ2

(cid:1), then

t−1

(cid:11) + 2(cid:104)vt−1(cid:105)(cid:104)ηt(cid:105) +(cid:10)η2

t

(cid:11)

(cid:10)v2

t

⇒ σ2

(cid:104)vt(cid:105) = a(cid:104)vt−1(cid:105) + (cid:104)ηt(cid:105) ⇒ µt = aµt−1

(cid:11) = (cid:104)avt−1 + ηt(cid:105)2 = a2(cid:10)v2
(cid:0)vt aµt−1, a2σ2
(cid:1)

t = a2σ2

t−1 + σ2
v

t−1 + σ2
v

t−1

vt ∼ N

so that

(24.1.6)

(24.1.7)
(24.1.8)
(24.1.9)

(24.1.10)

(24.1.11)

(24.1.12)

(24.2.1)

Assuming there is a ﬁxed variance σ2∞ for the inﬁnite time case, the stationary distribution satisﬁes

σ2∞ = a2σ2∞ + σ2

v ⇒ σ2∞ = σ2
1 − a2

v

Similarly, the mean is given by µ∞ = a∞µ1. If a ≥ 1 the variance (and mean) increases indeﬁnitely with
t. For a < 1, the mean tends to zero yet the variance remains ﬁnite. Even though the magnitude of vt−1
is decreased by a factor of a at each iteration, the additive noise on average boosts the magnitude so that
it remains steady in the long run. More generally for a system updating a vector vt according to

vt = Avt−1 + ηt

for the existence of a steady state we require that all eigenvalues of A must be ≤ 1.

24.2 Auto-Regressive Models

A scalar time-invariant auto-regressive model is deﬁned by

vt =

alvt−l + ηt,

ηt ∼ N

(cid:0)ηt µ, σ2(cid:1)

where a = (a1, . . . , aL)T are called the AR coeﬃcients and σ2 is called the innovation noise. The model
predicts the future based on a linear combination of the previous L observations. As a belief network, the
AR model can be written as an Lth order Markov model:

p(v1:T ) =

t=1

438

p(vt|vt−1, . . . , vt−L),

with vi = ∅ for i ≤ 0

(24.2.2)

DRAFT March 9, 2010

L(cid:88)

l=1

T(cid:89)

Auto-Regressive Models

Figure 24.1: Fitting an order 3 AR model to the training points. The x
axis represents time, and the y axis the value of the timeseries. The solid
line is the mean prediction and the dashed lines ± one standard deviation.
See demoARtrain.m

with

(cid:32)

p(vt|vt−1, . . . , vt−L) = N

vt

(cid:33)

alvt−l, σ2

L(cid:88)

l=1

Introducing the vector of the L previous observations

ˆvt−1 ≡ [vt−1, vt−2, . . . , vt−L]T
(cid:16)

we can write more compactly
p(vt|vt−1, . . . , vt−L) = N

vt aTˆvt−1, σ2(cid:17)

(24.2.3)

(24.2.4)

(24.2.5)

AR models are heavily used in ﬁnancial time-series prediction (see for example [272]), being able to capture
simple trends in the data. Another common application area is in speech processing whereby for a one-
dimensional speech signal partitioned into windows of length T , the AR coeﬃcients best able to describe
the signal in each window are found[215]. These AR coeﬃcients then form a compressed representation of
the signal and subsequently transmitted for each window, rather than the original signal itself. The signal
can then be approximately reconstructed based on the AR coeﬃcients. Such a representation is used for
example in telephones and known as a linear predictive vocoder[257].

24.2.1 Training an AR model

Maximum Likelihood training of the AR coeﬃcients is straightforward based on

log p(v1:T ) =

vt − ˆvT
Diﬀerentiating w.r.t. a and equating to zero we arrive at

log p(vt|ˆvt−1) = −

t=1

t−1a

T(cid:88)

(cid:16)

1
2σ2

(cid:17)2

T
2

−

log(2πσ2)

T(cid:88)
(cid:17)

t=1

(cid:88)

(cid:16)
(cid:32)(cid:88)
T(cid:88)

t

1
T

t=1

(cid:16)

t−1a

ˆvt−1 = 0

vt − ˆvT
so that optimally

t

a =

ˆvt−1ˆvT

t−1

vtˆvt−1

(cid:33)−1(cid:88)
(cid:17)2

t

σ2 =

vt − ˆvT

t−1a

(24.2.6)

(24.2.7)

(24.2.8)

(24.2.9)

These equations can be solved by Gaussian elimination. Similarly, optimally,

Above we assume that ‘negative’ timepoints are available in order to keep the notation simple. If times
before the window over which we learn the coeﬃcients are not available, a minor adjustment is required
to start the summations from t = L + 1.

Given a trained a, future predictions can be made using vt+1 = ˆvT
capturing the trend in the data.

t a. As we see, the model is capable of

DRAFT March 9, 2010

439

020406080100120140−50050100150200Auto-Regressive Models

a1

v1

a2

v2

a3

v3

a4

v4

Figure 24.2: A time-varying AR model as a latent
LDS. Since the observations are known, this model is
a time-varying latent LDS, for which smoothed infer-
ence determines the time-varying AR coeﬃcients.

24.2.2 AR model as an OLDS

We can write equation (24.2.1) as an OLDS using

 vt

vt−1
...

vt−L+1

 =

 a1

1
...
0



 vt−1

vt−2
...
vt−L

 +

 ηt

0
...
0



a2
0
1
. . .

. . . aL
0
. . .
0
0

. . .
1

We can write equation (24.2.1) as the OLDS
ηt ∼ N (ηt 0, Σ)

ˆvt = Aˆvt−1 + ηt,

where we deﬁne the block matrices

(cid:18) a1:L−1 aL

(cid:19)

I

0

A =

(cid:18)

,

Σ =

σ2

01,1:L−1

01:L−1,1 01:L−1,1:L−1

(cid:19)

(24.2.10)

(24.2.11)

(24.2.12)

In this representation, the ﬁrst component of the vector is updated according to the standard AR model,
with the remaining components being copies of the previous values.

24.2.3 Time-varying AR model

An alternative to Maximum Likelihood is to view learning the AR coeﬃcients as a problem in inference
in a latent LDS, a model which is discussed in detail in section(24.3). If at are the latent AR coeﬃcients,
the term

vt = ˆvT

t−1at + ηt,

ηt ∼ N

can be viewed as the emission distribution of a latent LDS in which the hidden variable is at and the
time-dependent emission matrix is given by ˆvT

t−1. By placing a simple latent transition

(cid:0)ηt 0, σ2(cid:1)
aI(cid:1)
(cid:0)ηa

t 0, σ2

(24.2.13)

(24.2.14)

(24.2.15)

we encourage the AR coeﬃcients to change slowly with time. This deﬁnes a model

ηa
t ∼ N

at = at−1 + ηa
t ,

p(v1:T , a1:T ) =(cid:89)

p(vt|at, ˆvt−1)p(at|at−1)

t

Our interest is then in the conditional p(a1:T|v1:T ) from which we can compute the a-posteriori most likely
sequence of AR coeﬃcients. Standard smoothing algorithms can then be applied to yield the time-varying
AR coeﬃcients, see demoARlds.m.

N−1(cid:88)

n=0

Deﬁnition 112 (Discrete Fourier Transform). For a sequence x0:N−1 the DFT f0:N−1 is deﬁned as

fk =

− 2πi

N kn,

xne

k = 0, . . . , N − 1

(24.2.16)

fk is a (complex) representation as to how much frequency k is present in the sequence x0:N−1. The power
of component k is deﬁned as the absolute length of the complex fk.

440

DRAFT March 9, 2010

Auto-Regressive Models

(a)

(b)

(c)

(d)

(e)

(b): Spectrogram of (a) up to 20,000 Hz.

Figure 24.3: (a): The raw recording of 5 seconds of a nightingale song (with additional background
(c): Clustering of the results in panel (b) using
birdsong).
an 8 component Gaussian mixture model. The index (from 1 to 8) of the component most probably
responsible for the observation is indicated vertically in black. (d): The 20 AR coeﬃcients learned using
(e): Clustering the results in panel (d) using a Gaussian mixture
v = 0.001, σ2
σ2
model with 8 components. The AR components group roughly according to the diﬀerent song regimes.

h = 0.001, see ARlds.m.

Deﬁnition 113 (Spectrogram). Given a timeseries x1:T the spectrogram at time t is a representation
of the frequencies present in a window localised around t. For each window one computes the Discrete
Fourier Transform, from which we obtain a vector of log power in each frequency. The window is then
moved (usually) one step forward and the DFT recomputed. Note that by taking the logarithm, small
values in the original signal can translate to visibly appreciable values in the spectrogram.

Example 100 (Nightingale). In ﬁg(24.3a) we plot the raw acoustic recording for a 5 second fragment of
a nightingale song freesound.org/samplesViewSingle.php?id=17185. The spectrogram is also plotted
and gives an indication of which frequencies are present in the signal as a function of time. The nightingale
song is very complicated but at least locally can be very repetitive. A crude way to ﬁnd which segments
repeat is to form a cluster analysis of the spectrogram.
In ﬁg(24.3c) we show the results of ﬁtting a
Gaussian mixture model, section(20.3), with 8 components, from which we see there is some repetition
of components locally in time. An alternative representation of the signal is given by the time-varying
AR coeﬃcients, section(24.2.3), as plotted in ﬁg(24.3d). A GMM clustering with 8 components ﬁg(24.3e)

DRAFT March 9, 2010

441

Latent Linear Dynamical Systems

h1

v1

h2

v2

h3

v3

h4

v4

Figure 24.4: A (latent) LDS. Both hidden and visible
variables are Gaussian distributed.

in this case produces a somewhat clearer depiction of the diﬀerent phases of the nightingale singing than
that aﬀorded by the spectrogram.

24.3 Latent Linear Dynamical Systems

The Latent LDS deﬁnes a stochastic linear dynamical system in a latent (or ‘hidden’) space on a sequence
of vectors h1:T . Each observation vt is as linear function of the latent vector ht. This model is also called a
linear Gaussian state space model 2. The model can also be considered a form of LDS on the joint variables
xt = (vt, ht), with parts of the vector xt missing. For this reason we will also refer to this model as a
Linear Dynamical System (without the ‘latent’ preﬁx).

t are noise vectors. At is called the transition matrix and Bt the emission matrix. The
where ηh
terms ¯ht and ¯vt are the hidden and output bias respectively. The transition and emission models deﬁne
a ﬁrst order Markov model

Deﬁnition 114 (Latent linear dynamical system).

(cid:0)ηh
(cid:0)ηv

¯ht, ΣH
t
t
t ¯vt, ΣV
t

(cid:1) transition model
(cid:1)

emission model

ht = Atht−1 + ηh
vt = Btht + ηv
t
t and ηv

t ηh
t ∼ N
ηv
t ∼ N

T(cid:89)
(cid:0)ht Atht−1 + ¯ht, ΣH
(cid:0)vt Btht + ¯vt, ΣV
(cid:1)

t=2

t

t

(cid:1) ,

p(h1:T , v1:T ) = p(h1)p(v1|h1)

p(ht|ht−1)p(vt|ht)

with the transitions and emissions given by Gaussian distributions

p(ht|ht−1) = N
p(vt|ht) = N

p(h1) = N (h1 µπ, Σπ)

(24.3.1)

(24.3.2)

(24.3.3)
(24.3.4)

(24.3.5)

(24.3.6)

This (latent) LDS can be represented as a belief networkin ﬁg(24.4) with the extension to higher orders
being intuitive. One may also include an external input ot at each time, which will add Cot to the mean
of the hidden variable and Dot to the mean of the observation.

1(cid:112)
|2πΣH|

(cid:18)

(cid:18)

Explicit expressions for the transition and emission distributions are given below for the time-invariant
case with ¯vt = 0, ¯ht = 0. Each hidden variable is a multidimensional Gaussian distributed vector ht, with

(cid:19)

p(ht|ht−1) =

exp

1
2

(ht − Aht−1)T Σ−1

H (ht − Aht−1)

−

which states that ht+1 has a mean equal to Aht with Gaussian ﬂuctuations described by the covariance
matrix ΣH. Similarly,

p(vt|ht) =

1(cid:112)
|2πΣV |

exp

1
2

−

(vt − Bht)T Σ−1

(cid:19)
V (vt − Bht)

describes an output vt with mean Bht and covariance ΣV .

2These models are also often called Kalman Filters. We avoid this terminology here since the word ‘ﬁlter’ refers to a

speciﬁc kind of inference and runs the risk of confusing a ﬁltering algorithm with the model itself.

442

DRAFT March 9, 2010

Inference

Figure 24.5: A single phasor plotted as a damped two
dimensional rotation ht+1 = γRθht with a damping
factor 0 < γ < 1. By taking a projection onto the y
axis, the phasor generates a damped sinusoid.

Example 101. Consider a dynamical system deﬁned on two dimensional vectors ht:

(cid:18) cos θ − sin θ

cos θ

sin θ

(cid:19)

ht+1 = Rθht,

with Rθ =

(24.3.7)

Rθ rotates the vector ht through angle θ in one timestep. Under this LDS h will trace out points on a
circle through time. By taking a scalar projection of ht, for example,

vt = [ht]1 = [1 0]Tht,

(24.3.8)

the elements vt, t = 1, . . . , T describe a sinusoid through time, see ﬁg(24.5). By using a block diagonal
R = blkdiag (Rθ1, . . . , Rθm) and taking a scalar projection of the extended m × 2 dimensional h vector,
one can construct a representation of a signal in terms of m sinusoidal components.

24.4 Inference

Given an observation sequence v1:T we wish to consider ﬁltering and smoothing, as we did for the HMM,
section(23.2.1). For the HMM, in deriving the various message passing recursions, we used only the
independence structure encoded by the belief network. Since the LDS has the same independence structure
as the HMM, we can use the same independence assumptions in deriving the updates for the LDS. However,
in implementing them we need to deal with the issue that we now have continuous hidden variables, rather
than discrete states. The fact that the distributions are Gaussian means that we can deal with continuous
messages exactly. In translating the HMM message passing equations, we ﬁrst replace summation with
integration. For example, the ﬁltering recursion (23.2.7) becomes

p(ht|v1:t) ∝

ht−1

p(vt|ht)p(ht|ht−1)p(ht−1|v1:t−1),

t > 1

(24.4.1)

Since the product of two Gaussians is another Gaussian, and the integral of a Gaussian is another Gaussian,
the resulting p(ht|v1:t) is also Gaussian. This closure property of Gaussians means that we may represent
p(ht−1|v1:t−1) = N (ht−1 ft−1, Ft−1) with mean ft−1 and covariance Ft−1. The eﬀect of equation (24.4.1)
is equivalent to updating the mean ft−1 and covariance Ft−1 into a mean ft and covariance Ft for p(ht|v1:t).
Our task below is to ﬁnd explicit algebraic formulae for these updates.

Numerical stability

Translating the message passing inference techniques we developed for the HMM into the LDS is largely
straightforward. Indeed, one could simply run a standard sum-product algorithm (albeit for continuous
variables), see demoSumprodGaussCanonLDS.m. In long timeseries numerical instabilities can build up and
may result in grossly inaccurate results, depending on the transition and emission distribution parameters
and the method of implementing the message updates. For this reason specialised routines have been
developed that are reasonably numerically stable under certain parameter regimes[283]. For the HMM
in section(23.2.1), we discussed two alternative methods for smoothing, the parallel β approach, and the
sequential γ approach. The β recursion is suitable when the emission and transition covariance entries are
small, and the γ recursion usually preferable in the more standard case of small covariance values.

DRAFT March 9, 2010

443

(cid:90)

(cid:90)

Inference

Analytical shortcuts

In deriving the inference recursions we need to frequently multiply and integrate Gaussians. Whilst in
principle straightforward, this can be algebraically tedious and, wherever possible, it is useful to appeal
to known shortcuts. For example, one can exploit the general result that the linear transform of a Gaus-
sian random variable is another Gaussian random variable. Similarly it is convenient to make use of the
conditioning formulae, as well as the dynamics reversal intuition. These results are stated in section(8.6),
and below we derive the most useful for our purposes here.

Consider a linear transformation of a Gaussian random variable:
x ∼ N (x µx, Σx)

η ∼ N (η µ, Σ) ,

y = Mx + η,

(24.4.2)

where x and η are assumed to be generated from independent processes. To ﬁnd the distribution p(y),
one approach would be to write this formally as

p(y) =

N (y Mx + µ, Σ)N (x µx, Σx) dx

(24.4.3)

and carry out the integral (by completing the square). However, since a Gaussian variable under linear
transformation is another Gaussian, we can take a shortcut and just ﬁnd the mean and covariance of the
transformed variable. Its mean is given by

(cid:104)y(cid:105) = M(cid:104)x(cid:105) + (cid:104)η(cid:105) = Mµx + µ

(24.4.4)

To ﬁnd the covariance, consider the displacement of a variable h from its mean, which we write as

The covariance is, by deﬁnition,(cid:10)∆h∆hT(cid:11). For y, the displacement is

∆h ≡ h − (cid:104)h(cid:105)

∆y = M∆x + ∆η,

(cid:68)

∆y∆yT(cid:69)

So that the covariance is

(cid:68)
(M∆x + ∆η) (M∆x + ∆η)T(cid:69)
∆x∆ηT(cid:69)
∆x∆xT(cid:69)
(cid:68)

(cid:68)
Since the noises η and x are assumed independent,(cid:10)∆η∆xT(cid:11) = 0 we have

∆η∆xT(cid:69)
(cid:68)

MT + M

MT +

=

= M

(cid:68)

+

∆η∆ηT(cid:69)

Σy = MΣxMT + Σ

24.4.1 Filtering

We represent the ﬁltered distribution as a Gaussian with mean ft and covariance Ft,

p(ht|v1:t) ∼ N (ht ft, Ft)

(24.4.5)

(24.4.6)

(24.4.7)

(24.4.8)

This is called the moment representation. Our task is then to ﬁnd a recursion for ft, Ft in terms of ft−1,
Ft−1. A convenient approach is to ﬁrst ﬁnd the joint distribution p(ht, vt|v1:t−1) and then condition on vt
to ﬁnd the distribution p(ht|v1:t). The term p(ht, vt|v1:t−1) is a Gaussian whose statistics can be found
from the relations

vt = Bht + ηv
t ,

ht = Aht−1 + ηh
t

(24.4.9)

Using the above, and assuming time-invariance and zero biases, we readily ﬁnd

(cid:68)

(cid:69)

(cid:68)

(cid:69)

∆ht∆hT

t |v1:t−1

= A

∆ht−1∆hT

t−1|v1:t−1

AT + ΣH = AFt−1AT + ΣH

(24.4.10)

444

DRAFT March 9, 2010

Inference

parameters θt =(cid:8)A, B, Σh, Σv, ¯h, ¯v(cid:9)

Algorithm 20 LDS Forward Pass. Compute the ﬁltered posteriors p(ht|v1:t) ≡ N (ft, Ft) for a LDS with

t. The log-likelihood L = log p(v1:T ) is also returned.

{f1, F1, p1} = LDSFORWARD(0, 0, v1; θt)
F0 ← 0, f0 ← 0,
L ← log p1
for t ← 2, T do

{ft, Ft, pt} = LDSFORWARD(ft−1, Ft−1, vt; θ)
L ← L + log pt

end for
function ldsforward(f , F, v; θ)

(cid:46) Mean of p(ht, vt|v1:t−1)
(cid:46) Covariance of p(ht, vt|v1:t−1)
(cid:46) Find p(ht|v1:t) by conditioning:
(cid:46) Compute p(vt|v1:t−1)

(cid:17)

BT + ΣV

AFt−1AT + ΣH

/

(cid:16)

(cid:17)

vv Σvh

vhΣ−1

(cid:17)
F(cid:48)
vv (v − µv)

µh ← Af + ¯h,
µv ← Bµh + ¯v
Σhh ← AFAT + Σh, Σvv ← BΣhhBT + Σv, Σvh ← BΣhh
vhΣ−1
f(cid:48)
vv (v − µv),
← µh + ΣT
2 (v − µv)T Σ−1
p(cid:48)
← exp
− 1
return f(cid:48), F(cid:48), p(cid:48)
(cid:68)
(cid:69)
end function
(cid:68)
(cid:69)

(cid:112)det (2πΣvv)
← Σhh − ΣT
(cid:69)
(cid:16)
(cid:69)
(cid:104)ht|v1:t−1(cid:105) = A(cid:104)ht−1|v1:t−1(cid:105)
In the above, using our moment representation of the forward messages

(cid:68)
(cid:68)
∆vt∆vT
∆ht∆hT
(cid:104)vt|v1:t−1(cid:105) = BA(cid:104)ht−1|v1:t−1(cid:105) ,
(cid:68)

t |v1:t−1
t |v1:t−1

t |v1:t−1
t |v1:t−1

AFt−1AT + ΣH

BT + ΣV = B

∆ht∆hT

∆vt∆hT

= B

(cid:104)ht−1|v1:t−1(cid:105) ≡ ft−1,
(cid:68)

(cid:69)
Then, using conditioning3 p(ht|vt, v1:t−1) will have mean
t |v1:t−1
(cid:69)(cid:68)

t |v1:t−1
(cid:68)

ft ≡ (cid:104)ht|v1:t−1(cid:105) +

t−1|v1:t−1

and covariance

∆ht−1∆hT

(cid:69)(cid:68)

∆ht∆vT

∆vt∆vT

(cid:68)

(cid:69)

≡ Ft−1
(cid:69)−1

(cid:69)−1(cid:68)

= B

= B

(cid:16)

(vt − (cid:104)vt|v1:t−1(cid:105))

(cid:69)

∆ht∆vT

−

∆vt∆vT

t |v1:t−1

∆vt∆hT

t |v1:t−1

Writing out the above explicitly we have for the mean:

BPBT + ΣV

t |v1:t−1
(cid:17)−1
(vt − BAft−1)
(cid:17)−1

BPBT + ΣV

BP

Ft ≡

∆ht∆hT

t |v1:t−1

ft = Aft−1 + PBT(cid:16)
Ft = P + ΣH − PBT(cid:16)

and covariance

where

P ≡ AFt−1AT + ΣH

(24.4.11)

(24.4.12)

(24.4.13)

(24.4.14)

(24.4.15)

(24.4.16)

(24.4.17)

(24.4.18)

(24.4.19)

The ﬁltering procedure is presented in algorithm(20) with a single update in LDSforwardUpdate.m.

One can write the covariance update as

where we deﬁne the Kalman gain matrix

Ft = (I − KB) P

K = PBT(cid:16)

ΣV + BPBT(cid:17)−1

(24.4.20)

(24.4.21)

We present in algorithm(??) the recursion in standard engineering notation. See also LDSsmooth.m. The
iteration is expected to be numerically stable when the noise covariances are small.

(cid:0)y − µy

(cid:1) and covariance Σxx − ΣxyΣ−1

yy Σyx.

3p(x|y) is a Gaussian with mean µx + ΣxyΣ−1

yy

DRAFT March 9, 2010

445

Symmetrising the updates

Inference

A potential numerical issue with the covariance update (24.4.20) is that it is the diﬀerence of two positive
deﬁnite matrices. If there are numerical errors, the Ft may not be positive deﬁnite, nor symmetric. Using
the Woodbury identity, deﬁnition(132), equation (24.4.18) can be written more compactly as

(cid:16)

Ft =

P−1 + BTΣ−1
V B

(cid:17)−1

Whilst this is positive semideﬁnite, this is numerically expensive since it involves two matrix inversions.
An alternative is to use the deﬁnition of K, from which we can write

KΣV KT = (I − KB) PBTKT

Hence we arrive at Joseph’s symmetrized update[104]

(cid:16)

P (I − KB)T + PBTKT(cid:17)

(I − KB) P (I − KB)T + KΣV KT ≡ (I − KB)

≡ (I − KB) P (24.4.24)
The left hand side is the addition of two positive deﬁnite matrices so that the resulting update for the
covariance is more numerically stable. A similar method can be used in the backward pass below. An
alternative is to avoid using covariance matrices directly and use their square root as the parameter,
deriving updates for these instead[226, 38].

24.4.2 Smoothing : Rauch-Tung-Striebel correction method
The smoothed posterior p(ht|v1:T ) is necessarily Gaussian since it is the conditional marginal of a larger
Gaussian. By representing the posterior as a Gaussian with mean gt and covariance Gt,

(24.4.22)

(24.4.23)

(24.4.25)

(24.4.26)

(24.4.27)

p(ht|v1:T ) ∼ N (ht gt, Gt)

we can form a recursion for gt and Gt as follows:

(cid:90)
(cid:90)

p(ht|v1:T ) =
=

p(ht, ht+1|v1:T )
p(ht|v1:T , ht+1)p(ht+1|v1:T ) =

(cid:90)

ht+1

p(ht|v1:t, ht+1)p(ht+1|v1:T )

ht+1

ht+1

The term p(ht|v1:t, ht+1) can be found by conditioning the joint distribution

p(ht, ht+1|v1:t) = p(ht+1|ht,v1:t)p(ht|v1:t)

(24.4.28)
which is obtained in the usual manner by ﬁnding its mean and covariance: The term p(ht|v1:t) is a known
Gaussian from ﬁltering with mean ft and covariance Ft. Hence the joint distribution p(ht, ht+1|v1:t) has
means

(cid:104)ht+1|v1:t(cid:105) = Aft

and covariance elements

(cid:104)ht|v1:t(cid:105) = ft,
(cid:68)

∆ht∆hT

t |v1:t

(cid:69)

(cid:68)

(cid:69)

= FtAT,

(cid:68)

= Ft,

∆ht∆hT

t+1|v1:t

∆ht+1∆hT

t+1|v1:t

(24.4.29)

(cid:69)

= AFtAT + ΣH

(24.4.30)
To ﬁnd p(ht|v1:t, ht+1) we may use the conditioned Gaussian results, deﬁnition(78). It is useful to use the
system reversal result, section(8.6.1), which interprets p(ht|v1:t, ht+1) as an equivalent linear system going
backwards in time:

ht = ←−Atht+1 + ←−mt + ←−η t
(cid:69)(cid:68)

(cid:68)

where

←−At ≡

∆ht∆hT

t+1|v1:t

∆ht+1∆hT

t+1|v1:t

(cid:69)−1

(24.4.31)

(24.4.32)

446

DRAFT March 9, 2010

Inference

Algorithm 21 LDS Backward Pass. Compute the smoothed posteriors p(ht|v1:T ). This requires the
ﬁltered results from algorithm(20).

{gt, Gt} = LDSBACKWARD(gt+1, Gt+1, ft, Ft; θ)

GT ← FT , gT ← fT
for t ← T − 1, 1 do
end for
function ldsbackward(g, G, f , F; θ)
Σh(cid:48)h(cid:48) ← AFAT + Σh,
h(cid:48)hΣ−1
h(cid:48)h(cid:48)Σh(cid:48)h, ←−A ← ΣT
← ←−AG←−AT + ←−Σ
(cid:69)(cid:68)

µh ← Af + ¯h,
h(cid:48)hΣ−1
←−Σ ← F − ΣT
← ←−Ag + ←−m, G(cid:48)
g(cid:48)
return g(cid:48), G(cid:48)
(cid:68)
(cid:17)
←−mt ≡ (cid:104)ht|v1:t(cid:105) −
←−η t 0,←−Σ t

end function

∆ht∆hT

, with

t+1|v1:t
(cid:68)

(cid:69)

(cid:69)(cid:68)

(cid:16)
(cid:68)
and ←−η t ∼ N
←−Σ t ≡

∆ht+1∆hT

t+1|v1:t

(cid:69)−1

(cid:104)ht+1|v1:t(cid:105)
(cid:69)−1(cid:68)

Σh(cid:48)h ← AF

h(cid:48)h(cid:48), ←−m ← f − ←−Aµh

(cid:46) Statistics of p(ht, ht+1|v1:t)
(cid:46) Dynamics Reversal p(ht|ht+1, v1:t)
(cid:46) Backward propagation

(cid:69)

(24.4.33)

(24.4.34)

∆ht∆hT

t |v1:t

−

∆ht∆hT

t+1|v1:t

∆ht+1∆hT

t+1|v1:t

∆ht+1∆hT

t |v1:t

Using dynamics reversal, equation (24.4.31) and assuming that ht+1 is Gaussian distributed, it is then
straightforward to work out the statistics of p(ht|v1:T ). The mean is given by

and covariance

gt ≡ (cid:104)ht|v1:T(cid:105) = ←−At (cid:104)ht+1|v1:T(cid:105) + ←−mt ≡ ←−Atgt+1 + ←−mt
(cid:69)←−AT
t + ←−Σ t ≡ ←−AtGt+1←−AT

t+1|v1:T

∆ht+1∆hT

∆ht∆hT

t |v1:T

Gt ≡

= ←−At

(cid:68)

(cid:69)

(cid:68)

t + ←−Σ t

(24.4.35)

(24.4.36)

This procedure is the Rauch-Tung-Striebel Kalman smoother[231]. This is called a ‘correction’ method
since it takes the ﬁltered estimate p(ht|v1:t) and ‘corrects’ it to form a smoothed estimate p(ht|v1:T ). The
procedure is outlined in algorithm(21) and is detailed in LDSbackwardUpdate.m. See also LDSsmooth.m.

The cross moment

An advantage of the dynamics reversal interpretation given above is that the cross moment (which is
required for learning) is immediately obtained from

= ←−AtGt+1 + gtgT

t+1

(24.4.37)

(cid:68)

hthT

t+1|v1:T

(cid:69)

(cid:104)∆ht∆ht+1|v1:T(cid:105) = ←−AtGt+1 ⇒

24.4.3 The likelihood

We can compute the likelihood using the decomposition

p(v1:T ) =

p(vt|v1:t−1)

(24.4.38)

in which each conditional p(vt|v1:t−1) is a Gaussian in vt. It is straightforward to show that the term
p(vt|v1:t−1) has mean and covariance

T(cid:89)

t=1

µ1 ≡ Bµ

µt ≡ BAft−1 Σt ≡ B(cid:0)AFt−1AT + ΣH
(vt − µt)T Σ−1

Σ1 ≡ BΣBT + ΣV
T(cid:88)

log p(v1:T ) = −

(cid:104)

1
2

t

t=1

The log likelihood is then given by

(cid:1) BT + ΣV

t = 1
t > 1

(cid:105)
(vt − µt) + log det (2πΣt)

DRAFT March 9, 2010

(24.4.39)

(24.4.40)

447

24.4.4 Most likely state

Since the mode of a Gaussian is equal to its mean, there is no diﬀerence between the most probable joint
posterior state

Inference

argmax

h1:T

p(h1:T|v1:T )

and the set of most probable marginal states

ht = argmax

htT

p(ht|v1:T ),

t = 1, . . . , T

(24.4.41)

(24.4.42)

Hence the most likely hidden state sequence is equivalent to the smoothed mean sequence.

24.4.5 Time independence and Riccati equations

Both the ﬁltered Ft and smoothed Gt covariance recursions are independent of the observations v1:T ,
depending only on the parameters of the model. This is a general characteristic of linear Gaussian systems.
Typically the covariance recursions converge quickly to values that are reasonably constant throughout
the dynamics, with only appreciable diﬀerences at the boundaries t = 1 and t = T . In practice one often
drops the time-dependence of the covariances and approximates them with a single time-independent
covariance. This approximation dramatically reduces storage requirements. The converged ﬁltered F
satisﬁes the recursion

(cid:17)

(cid:17)−1

(cid:16)

(cid:17)

AFAT + ΣH

B

AFAT + ΣH

BT + ΣV

B

AFAT + ΣH

(24.4.43)

F = AFAT + ΣH −

(cid:16)

(cid:17)

BT(cid:16)

(cid:16)

which can be related to a form of algebraic Riccati equation. A technique to solve these equations is to being
with setting the covariance to Σ. With this, a new F is found using the right hand side of (24.4.43), and
subsequently recursively updated. Alternatively, using the Woodbury identity, the converged covariance
satisﬁes

(cid:18)(cid:16)

(cid:17)−1

(cid:19)−1

F =

AFAT + ΣH

+ BTΣ−1
V B

(24.4.44)

although this form is less numerically convenient in forming an iterative solver for F since it requires two
matrix inversions.

Example 102 (Newtonian Trajectory Analysis). A toy rocket with unknown mass and initial velocity
is launched in the air. In addition, the constant accelerations from the rocket’s propulsion system are
unknown. It is known is that Newton’s laws apply and an instrument can measure the vertical height and
horizontal distance of the rocket at each time x(t), y(t) from the origin. Based on noisy measurements of
x(t) and y(t), our task is to infer the position of the rocket at each time.

Although this is perhaps most appropriately considered from the using continuous time dynamics, we will
translate this into a discrete time approximation. Newton’s law states that

d2

dt2 x = fx(t)

m

,

d2

dt2 y = fy(t)

m

(24.4.45)

where m is the mass of the object and fx(t), fy(t) are the horizontal and vertical forces respectively. Hence
As they stand, these equations are not in a form directly usable in the LDS framework. A naive approach
is to reparameterise time to use the variable ˜t such that t ≡ ˜t∆, where ˜t is integer and ∆ is a unit of time.
The dynamics is then

x((˜t + 1)∆) = x(˜t∆) + ∆x

y((˜t + 1)∆) = y(˜t∆) + ∆y

(cid:48)(˜t∆)
(cid:48)(˜t∆)

(24.4.46)

(24.4.47)

448

DRAFT March 9, 2010

Learning Linear Dynamical Systems

Figure 24.6: Estimate of the trajectory of a New-
tonian ballistic object based on noisy observations
(small circles). All time labels are known but omit-
ted in the plot. The ‘x’ points are the true po-
sitions of the object, and the crosses ‘+’ are the
estimated smoothed mean positions (cid:104)xt, yt|v1:T(cid:105) of
the object plotted every several time steps.
See
demoLDStracking.m

where y(cid:48)(t) ≡ dy

(cid:48)((˜t + 1)∆) = x

x

dt . We can write an update equation for the x(cid:48) and y(cid:48) as
(cid:48)(˜t∆) + fy∆/m

(cid:48)(˜t∆) + fx∆/m,

(cid:48)((˜t + 1)∆) = y

y

(24.4.48)

These are discrete time diﬀerence equations indexed by ˜t. The instrument which measures x(t) and y(t)
is not completely accurate. For simplicity, we relabel ax(t) = fx(t)/m(t), ay(t) = fy(t)/m(t) – these
accelerations will be assumed to be roughly constant, but unknown :

ax((˜t + 1)∆) = ax(˜t∆) + ηx, ay((˜t + 1)∆) = ay(˜t∆) + ηy,

(24.4.49)

where ηx and ηy are small noise terms. The initial distributions for the accelerations are assumed vague,
using a zero mean Gaussian with large variance.
We describe the above model by considering x(t), x(cid:48)(t), y(t), y(cid:48)(t), ax(t), ay(t) as hidden variables, giving
rise to a H = 6 dimensional LDS with transition and emission matrices as below:

(cid:18) 0 1 0 0 0 0

0 0 0 1 0 0

(cid:19)

B =

(24.4.50)



A =

 ,

0
0
1

0 ∆ 0
1
0
0
0
0
∆ 1
0 ∆
0
0
0
0
0
0 ∆ 1
0
0
0
0
1
0
1
0
0
0
0

0
0

We place a large variance on their initial values, and attempt to infer the unknown trajectory. A demonstra-
tion is given in ﬁg(24.6). Despite the signiﬁcant observation noise, the object trajectory can be accurately
inferred.

24.5 Learning Linear Dynamical Systems

Whilst in many applications, particularly of underlying known physical processes, the paremeters of the
LDS are known, in many machine learning tasks we need to learn the parameters of the LDS based on
v1:T . For simplicity we assume that we know the dimensionality H of the LDS.

24.5.1 Identiﬁability issues

An interesting question is whether we can uniquely identify (learn) the parameters of an LDS. There are
always trivial redundancies in the solution obtained by permuting the hidden variables arbitrarily and
ﬂipping their signs. To show that there are potentially many more equivalent solutions, consider the
following LDS

vt = Bht + ηv
t ,

ht = Aht−1 + ηh
t

DRAFT March 9, 2010

(24.5.1)

449

−1000100200300400500600700800−150−100−50050100150200250300xyWe now attempt to transform this original system to a new form which will produce exactly the same
outputs v1:T . For an invertible matrix R we consider

Learning Linear Dynamical Systems

Rht = RAR−1Rht−1 + Rηh

t

which is representable as a new latent dynamics

ˆht = ˆAˆht−1 + ˆηh

t

(24.5.2)

(24.5.3)

where ˆA ≡ RAR−1, ˆht ≡ Rht, ˆηh
the transformed h:

t ≡ Rηh

t . In addition, we can reexpress the outputs to be a function of

vt = BR−1Rht + ηv

t = ˆBˆht + ηv

t

(24.5.4)

Hence, provided we place no constraints on A, B and ΣH there exists an inﬁnite space of equivalent
solutions, ˆA = RA R−1, ˆB = BR−1, ˆΣH = RΣHRT, all with the same likelihood value. This means
that directly interpreting the learned parameters needs to be done with some care. This redundancy can
be mitigated by imposing constraints on the parameters.

24.5.2 EM algorithm

For simplicity, we assume we have a single sequence v1:T , to which we wish to ﬁt a LDS using Maximum
Likelihood. Since the LDS contains latent variables one approach is to use the EM algorithm. As usual,
the M-step of the EM algorithm requires us to maximise the energy

with respect to the parameters A, B, a, Σ, ΣV , ΣH. Thanks to the form of the LDS the energy decomposes
as

t=2

(cid:104)log p(h1)(cid:105)pold(h1|v1:T ) +

(cid:104)log p(ht|ht−1)(cid:105)pold(ht,ht−1|v1:T ) +
It is straightforward to derive that the M-step for the parameters is given by (angled brackets (cid:104)·(cid:105) denote
(cid:69)
expectation with respect to the smoothed posterior p(h1:T|v1:T )):
(cid:69)
hthT
t

(cid:104)log p(vt|ht)(cid:105)pold(ht|v1:T )

(cid:16)(cid:68)
(cid:69)
t − vt (cid:104)ht(cid:105)T BT − B(cid:104)ht(cid:105) vT
ht+1hT

AT(cid:17)

BT(cid:17)

(cid:88)

AT + A

(24.5.6)

(24.5.8)

(24.5.7)

Σnew

V =

t + B

hthT

vtvT

(cid:69)

(cid:68)

(cid:68)

(cid:68)

(cid:68)

(cid:69)

t

1

1
T

ht+1hT
t

hthT
t

t+1

− A

t+1

−

T(cid:88)

t=1

(cid:104)log p(v1:T , h1:T )(cid:105)pold(h1:T |v1:T )
T(cid:88)

(cid:16)
(cid:88)
(cid:69)

t

ht+1hT
t

Σnew

H =

h1hT
1

Anew =

T − 1
(cid:68)
µnew = (cid:104)h1(cid:105)
(cid:68)
T−1(cid:88)
Σnew =
T(cid:88)
vt (cid:104)ht(cid:105)T
(cid:16)
(cid:88)

Bnew =

t=1

t=1

Σnew

V =

1
T

t

vtvT

(cid:88)

1

T − 1

t

Σnew

H =

450

− µµT

t=1

(cid:68)

hthT
t

hthT
t

(cid:69)(cid:33)−1
(cid:69)(cid:32)T−1(cid:88)
(cid:32) T(cid:88)
(cid:69)(cid:33)−1
(cid:68)
t − vt (cid:104)ht(cid:105)T BT(cid:17)
(cid:69)
(cid:16)(cid:68)

(cid:68)

t=1

ht+1hT

t+1

(cid:69)(cid:17)

− A

hthT

t+1

(24.5.5)

(24.5.9)

(24.5.10)

(24.5.11)

(24.5.12)

(24.5.13)

If B is updated according to the above, the ﬁrst equation can be simpliﬁed to

Similarly, if A is updated according to EM algorithm, then the second equation can be simpliﬁed to

(24.5.14)

DRAFT March 9, 2010

Learning Linear Dynamical Systems

The statistics required therefore include smoothed means, covariances, and cross moments. The extension
to learning multiple timeseries is straightforward since the energy is simply summed over the individual
sequences.

The performance of the EM algorithm for the LDS often depends heavily on a the initialisation. If we
remove the hidden to hidden links, the model is closely related to Factor Analysis (the LDS can be
considered a temporal extension of Factor Analysis). One initialisation technique is therefore to learn the
B matrix using Factor Analysis by treating the observations as temporally independent.

24.5.3 Subspace Methods

An alternative to EM and Maximum Likelihood training of an LDS is to use a subspace method[281, 249].
The chief beneﬁt of these techniques is that they avoid the convergence diﬃculties of EM. To motivate
subspace techniques, consider a deterministic LDS

vt = Bht

ht = Aht−1,

(24.5.15)
Under this assumption, vt = Bht = BAht−1 and, more generally, vt = BAth1. This means that a low
dimensional system underlies all visible information since all points Ath1 lie in a H-dimensional subspace,
which is then projected to form the observation. This suggests that some form of subspace identiﬁcation
technique will enable us to learn A and B.

Given a set of observation vectors v1, . . . , vt, consider the block Hankel matrix formed from stacking the
vectors. For an order L matrix, this is a V L× T − L + 1 matrix. For example, for T = 6 and L = 3, this is

 =

 B

BA
BA2

 (h1 h2 h3 h4)

(24.5.16)

(24.5.17)

(24.5.18)

M =

M =



v2 v3 v4 v5
v3 v4 v5 v6

 v1 v2 v3 v4
 Bh1
(cid:124)(cid:123)(cid:122)(cid:125)

Bh2

W

We now ﬁnd the SVD of M,

M = ˆU ˆS ˆVT

If the v are generated from a (noise free) LDS, we can write

Bh4
BAh1 BAh2 BAh3 BAh4
BA2h1 BA2h2 BA2h3 BA2h4

Bh3

T(cid:88)

t=2

where W is termed the extended observability matrix. The matrix ˆS will contain the singular values up to
the dimension of the hidden variables H, with the remaining singular values 0. From equation (24.5.17),
this means that the emission matrix B is contained in ˆU1:V,1:H. The estimated hidden variables are then
contained in the submatrix W1:H,1:T−L+1,

(h1 h2 h3 h4) = W1:H,1:T−L+1

(24.5.19)
Based on the relation ht = Aht−1 one can then ﬁnd the best least squares estimate for A by minimising

(ht − Aht−1)2

(24.5.20)

for which the optimal solution is

. . . hT−1)†

A = (h2 h3

. . . ht) (h1 h2

(24.5.21)
where † denotes the pseudo inverse, see LDSsubspace.m. Estimates for the covariance matrices can also
be obtained from the residual errors in ﬁtting the block Hankel matrix (ΣV ) and extended observability
matrix (ΣH). Whilst this derivation formally holds only for the noise free case one can nevertheless apply
this in the case of non-zero noise and hope to gain an estimate for A and B that is correct in the mean.
In addition to forming a solution in its own right, the subspace method forms a potentially useful way to
initialise the EM algorithm.

DRAFT March 9, 2010

451

Switching Auto-Regressive Models

s1

v1

s2

v2

s3

v3

s4

v4

Figure 24.7: A ﬁrst order switching AR model.
In
terms of inference, conditioned on v1:T , this is a
HMM.

24.5.4 Structured LDSs

Many physical equations are local both in time and space. For example in weather models the atmosphere
is partitioned into cells hi(t) each containing the pressure at that location. The equations describing how
the pressure updates only depend on the pressure at the current cell and small number of neighbouring
cells at the previous time t − 1. If we use a linear model and measure some aspects of the cells at each
time, then the weather is describable by a LDS with a highly structured sparse transition matrix A. In
practice, the weather models are non-linear but local linear approximations are often employed[254]. A
similar situation arises in brain imaging in which voxels (local cubes of activity) depend only on their
neighbours from the previous timestep[101].

Another application of structured LDSs is in temporal independent component analysis. This is deﬁned
as the discovery of a set of independent latent dynamical processes, from which the data is a projected
observation.
If each independent dynamical process can itself be described by a LDS, this gives rise
to a structured LDS with a block diagonal transition matrix A. Such models can be used to extract
independent components under prior knowledge of the likely underlying frequencies in each of the temporal
compoments[59]. See also exercise(199).

24.5.5 Bayesian LDSs

p(v1:T ) = (cid:82)

The extension to placing priors on the transition and emission parameters of the LDS leads in general to
computational diﬃculties in computing the likelihood. For example, for a prior on A, the likelihood is
A p(v1:T|A)p(A) which is diﬃcult to evaluate since the dependence of the likelihood on the
matrix A is a complicated function. Approximate treatments of this case are beyond the scope of this
book, although we brieﬂy note that sampling methods[54, 98] are popular in this context, in addition to
deterministic variational approximations[27, 23, 59].

24.6 Switching Auto-Regressive Models

For a time-series of scalar values v1:T an Lth order switching AR model can be written as

vt = ˆvT

t−1a(st) + ηt,

(cid:0)ηt 0, σ2(st)(cid:1)
where we now have a set of AR coeﬃcients θ =(cid:8)a(s), σ2(s), s ∈ {1, . . . , S}
themselves have a Markov transition p(s1:T ) =(cid:81)
p(v1:T , s1:T|θ) =(cid:89)

p(vt|vt−1, . . . , vt−L, st,|θ)p(st|st−1)

ηt ∼ N

t

t p(st|st−1) so that the full model is

(cid:9). The discrete switch variables

(24.6.1)

(24.6.2)

Given an observed sequence v1:T and parameters θ inference is straightforward since this is a form of
HMM. To make this more apparent we may write

24.6.1 Inference

p(v1:T , s1:T ) =(cid:89)

t

where

ˆp(vt|st)p(st|st−1)

ˆp(vt|st) ≡ p(vt|vt−1, . . . , vt−L, st) = N

452

(cid:16)

(cid:17)

vt ˆvT

t−1a(st), σ2(st)

(24.6.3)

(24.6.4)

DRAFT March 9, 2010

Switching Auto-Regressive Models

Figure 24.8: Learning a Switching AR
model. The upper plot shows the train-
ing data. The colour indicates which of
the two AR models is active at that time.
Whilst this information is plotted here,
this is assumed unknown to the learning
algorithm, as are the coeﬃcients a(s). We
assume that the order L = 2 and num-
ber of switches S = 2 however is known.
In the bottom plot we show the time se-
ries again after training in which we colour
the points according to the most likely
smoothed AR model at each timestep. See
demoSARlearn.m.

Note that the emission distribution ˆp(vt|st) is time-dependent. The ﬁltering recursion is then

ˆp(vt|st)p(st|st−1)α(st−1)

(24.6.5)

α(st) =(cid:88)

st−1

Smoothing can be achieved using the standard recursions, modiﬁed to use the time-dependent emissions,
see demoSARinference.m.

24.6.2 Maximum Likelihood Learning using EM

To ﬁt the set of AR coeﬃcients and innovation variances, a(s), σ2(s), s = 1, . . . , S, using Maximum Like-
lihood training for a set of data v1:T , we may make use of the EM algorithm.

which we need to maximise with respect to the parameters θ. Using the deﬁnition of the emission and
isolating the dependency on a, we have

t (cid:104)log p(st|st−1)(cid:105)pold(st,st−1)

(24.6.6)

M-step

Up to negligible constants, the energy is given by

t (cid:104)log p(vt|ˆvt−1, a(st))(cid:105)pold(st|v1:T ) +(cid:88)
(cid:28) 1

(cid:17)2

(cid:16)

σ2(st)

vt − ˆvT

t−1a(st)

E =(cid:88)
−2E =(cid:88)
(cid:88)

t

(cid:34)(cid:88)

(cid:29)

pold(st|v1:T )

(cid:35)

+ log σ2(st)

+ const.

(24.6.7)

On diﬀerentiating with respect to a(s) and equating to zero, the optimal a(s) satisﬁes the linear equation

which may be solved using Gaussian elimination. Similarly one may show that updates that maximise the
energy with respect to σ2 are

pold(st = s|v1:T ) vtˆvt−1

σ2(s)

=

t

σ2(s) =

(cid:80)
1
t(cid:48) pold(s(cid:48)
t = s|v1:T )

t−1

ˆvt−1ˆvT
σ2(s)

t

pold(st = s|v1:T )
(cid:104)
vt − ˆvT

pold(st = s|v1:T )

(cid:88)

t

(cid:105)2

t−1a(st)

a(s)

(24.6.8)

The update for p(st|st−1) follows the standard EM for HMM rule, equation (23.3.5), see SARlearn.m. Here
we don’t include an update for the prior p(s1) since there is insuﬃcient information at the start of the
sequence and assume p(s1) is ﬂat. With high frequency data it is unlikely that a change in the switch
variable is reasonable at each time t. A simple constraint to account for this is to use a modiﬁed transition

(cid:26) p(st|st−1)

ˆp(st|st−1) =

δ (st − st−1) otherwise

mod (t, Tskip) = 0

DRAFT March 9, 2010

(24.6.9)

(24.6.10)

453

050100150200250300350400−100−50050100sample switches050100150200250300350400−100−50050100learned switchesCode

s1

v1

˜v1

s2

v2

˜v2

s3

v3

˜v3

s4

v4

˜v4

Figure 24.9: (a): A latent switching (second order) AR model. Here the st indicates which of a set of 10
available AR models is active at time t. The square nodes emphasise that these are discrete variables. The
‘clean’ AR signal vt, which is not observed, is corrupted by additive noise to form the noisy observations
˜vt. In terms of inference, conditioned on ˜v1:T , this can be expressed as a Switching LDS, chapter(25).
(b): Signal reconstruction using the latent switching AR model in (a). Top: noisy signal ˜v1:T ; bottom:
reconstructed clean signal v1:T . The dashed lines and the numbers show the most-likely state segmentation
arg maxs1:T p(s1:T|˜v1:T ).

E-step

The M-step requires the smoothed statistics pold(st = s|v1:T ) and pold(st = s, st−1 = s(cid:48)

obtained from HMM inference.

|v1:T ) which can be

Example 103 (Learning a switching AR model). In ﬁg(24.8) the training data is generated by an Switch-
ing AR model so that we know the ground truth as to which model generated which parts of the data.
Based on the training data (assuming the labels st are unknown), a Switching AR model is ﬁtted using
EM. In this case the problem is straightforward so that a good estimate is obtained of both the sets of
AR parameters and which switches were used at which time.

Example 104 (Modelling parts of speech). In ﬁg(24.9) a segment of a speech signal is shown described
by a Switching AR model. Each of the 10 available AR models is responsible for modelling the dynamics
of a basic subunit of speech[90][192]. The model was trained on many example sequences using S = 10
states with a left-to-right transition matrix. The interest is to determine when each subunit is most likely
to be active. This corresponds to the computation of the most-likely switch path s1:T given the observed
signal p(s1:T|˜v1:T ).

24.7 Code

In the Linear Dynamical System code below only the simplest form of the recursions is given. No attempt
has been made to ensure numerical stability.
LDSforwardUpdate.m: LDS forward
LDSbackwardUpdate.m: LDS backward
LDSsmooth.m: Linear Dynamical System : ﬁltering and smoothing
LDSforward.m: Alternative LDS forward algorithm (see SLDS chapter)
LDSbackward.m: Alternative LDS backward algorithm (see SLDS chapter)
demoSumprodGaussCanonLDS.m: Sum-product algorithm for smoothed inference

demoLDStracking.m: Demo of tracking in a Newtonian system

454

DRAFT March 9, 2010

1234567891012345678910Exercises

LDSsubspace.m: Subspace Learning (Hankel matrix method)
demoLDSsubspace.m: Demo of Subspace Learning method

24.7.1 Autoregressive models

Note that in the code the autoregressive vector a has as its last entry the ﬁrst AR coeﬃcient (i.e.
reverse order to that presented in the text).
ARtrain.m: Learn AR coeﬃcients (Gaussian Elimination)
demoARtrain.m: Demo of ﬁtting an AR model to data
ARlds.m: Learn AR coeﬃcients using a LDS
demoARlds.m: Demo of learning AR coeﬃcients using an LDS
demoSARinference.m: Demo for inference in a Switching Autoregressive Model

in

In SARlearn.m a slight fudge is used since we do not deal fully with the case at the start where there
is insuﬃcient information to deﬁne the AR model. For long timeseries this will have a negligible eﬀect,
although it might lead to small decreases in the log likelihood under the EM algorithm.
SARlearn.m: Learning of a SAR using EM
demoSARlearn.m: Demo of SAR learning
HMMforwardSAR.m: Switching Autoregressive HMM forward pass
HMMbackwardSAR.m: Switching Autoregressive HMM backward pass

24.8 Exercises

Exercise 234. Consider the two-dimension linear model

ht = Rθht−1

where

Rθ =

Rθ is rotation matrix which rotates the vector ht through angle θ in one timestep.

(cid:19)

sin θ

(cid:18) cos θ − sin θ
(cid:19)

cos θ

=

1. By writing(cid:18) xt

(cid:18) R11 R12

(cid:19)(cid:18) xt−1

(cid:19)

yt

R21 R22

yt−1

(24.8.1)

(24.8.2)

(24.8.3)

eliminate yt to write an equation for xt+1 in terms of xt and xt−1.

2. Explain why the eigenvalues of a rotation matrix are (in general) imaginary.

3. Explain how to model a sinusoid, rotating with angular velocity ω using a two-dimensional LDS.

4. Explain how to model a sinusoid using an AR model.
5. Explain the relationship between the second order diﬀerential equation ¨x = −λx, which describes
a Harmonic Oscillator, and the second order diﬀerence equation which approximates this diﬀeren-
tial equation. Is it possible to ﬁnd a diﬀerence equation which exactly matches the solution of the
diﬀerential equation at chosen points?

Exercise 235. Show that for any anti-symmetric matrix M,

M = −MT

the matrix exponential (in MATLAB this is expm)

A = eM

DRAFT March 9, 2010

(24.8.4)

(24.8.5)

455

is orthogonal, namely

ATA = I

Exercises

(24.8.6)

Explain how one may then construct random orthogonal matrices with some control over the angles of the
complex eigenvalues. Discuss how this relates to the frequencies encountered in a LDS where A is the
transition matrix.

Exercise 236. Run the demo demoLDStracking.m which tracks a ballistic object using a Linear Dynamical
system, see example(102). Modify demoLDStracking.m so that in addition to the x and y positions, the
x speed is also observed. Compare and contrast the accuracy of the tracking with and without this extra
information.

Exercise 237. nightsong.mat contains a small stereo segment nightingale song sampled at 44100 Hertz.

1. Plot the original waveform using plot(x(:,1))

2. Download the program myspecgram.m from

labrosa.ee.columbia.edu/matlab/sgram/myspecgram.m and plot the spectrogram

y=myspecgram(x,1024,44100); imagesc(log(abs(y)))

3. The routine demoGMMem.m demonstrates ﬁtting a mixture of Gaussians to data. The mixture assign-
ment probabilities are contained in phgn. Write a routine to cluster the data v=log(abs(y)) using
8 Gaussian components, and explain how one might segment the series x into diﬀerent regions.

4. Examine the routine demoARlds.m which ﬁts autoregressive coeﬃcients using an interpretation as
a Linear Dynamical System. Adapt the routine demoARlds.m to learn the AR coeﬃcients of the
data x. You will almost certainly need to subsample the data x – for example by taking every 4th
datapoint. With the learned AR coeﬃcients (use the smoothed results) ﬁt a Gaussian mixture with
8 components. Compare and contrast your results with those obtained from the Gaussian mixture
model ﬁt to the spectrogram.

Exercise 238. Consider a supervised learning problem in which we make a linear model of the scaler
output yt based on vector input xt:

t

where ηy

t xt + ηy

yt = wT
t is zero mean Gaussian noise. Training data D = {(xt, yt), t = 1, . . . , T} is available.

(24.8.7)

1. For a time-invariant weight vector wt ≡ w, explain how to ﬁnd the single weight vector w and the

noise variance σ2 by Maximum Likelihood.

2. Extend the above model to include a transition

wt = wt−1 + ηw
t

(24.8.8)

where ηw
is zero mean Gaussian noise with a given covariance Σ; w1 has zero mean. Explain
t
how to cast ﬁnding (cid:104)wt|D(cid:105) as smoothing in a related Linear Dynamical System. Write a routine
W = LinPredAR(X,Y,SigmaW,SigmaY) that takes an input data matrix X = [x1, . . . , xT ] where each
column contains an input, and vector Y = [y1, . . . , yT ]T; SigmaW is the additive weight noise and
SigmaY is an assumed known time-invariant output noise. The returned W contains the smoothed
mean weights.

456

DRAFT March 9, 2010

CHAPTER 25

Switching Linear Dynamical Systems

25.1 Introduction

Complex timeseries which are not well described globally by a single Linear Dynamical System may be
divided into segments, each modelled by a potentially diﬀerent LDS. Such models can handle situations
in which the underlying model ‘jumps’ from one parameter setting to another. For example a single
LDS might well represent the normal ﬂows in a chemical plant. When a break in a pipeline occurs, the
dynamics of the system changes from one set of linear ﬂow equations to another. This scenario can be
modelled suing a sets of two linear systems, each with diﬀerent parameters. The discrete latent variable
at each time st ∈ {normal, pipe broken} indicates which of the LDSs is most appropriate at the current
time. This is called a Switching LDS and used in many disciplines, from econometrics to machine learning
[13, 110, 174, 161, 160, 60, 57, 218, 303, 175].

25.2 The Switching LDS

At each time t, a switch variable st ∈ 1, . . . , S describes which of a set of LDSs is to be used. The
observation (or ‘visible’) variable vt ∈ RV is linearly related to the hidden state ht ∈ RH by

vt = B(st)ht + ηv(st),

ηv(st) ∼ N (ηv(st) ¯v(st), Σv(st))

(25.2.1)

Here st describes which of the set of emission matrices B(1), . . . , B(S) is active at time t. The observation
noise ηv(st) is drawn from a Gaussian with mean ¯v(st) and covariance Σv(st). The transition dynamics
of the continuous hidden state ht is linear,

(cid:16)

(cid:17)

ht = A(st)ht−1 + ηh(st),

ηh(st) ∼ N

ηh(st) ¯h(st), Σh(st)

(25.2.2)

and the switch variable st selects a single transition matrix from the available set A(1), . . . , A(S). The
Gaussian transition noise ηh(st) also depends on the switch variable. The dynamics of st itself is Marko-
vian, with transition p(st|st−1). For the more general ‘augmented’ aSLDS model the switch st is dependent
on both the previous st−1 and ht−1. The model deﬁnes a joint distribution (see ﬁg(25.1))

T(cid:89)

p(v1:T , h1:T , s1:T ) =

p(vt|ht, st)p(ht|ht−1, st)p(st|ht−1, st−1)

with

t=1

p(vt|ht, st) = N (vt ¯v(st) + B(st)ht, Σv(st)) ,

p(ht|ht−1, st) = N

457

(cid:16)

(cid:17)

ht ¯h(st) + A(st)ht, Σh(st)
(25.2.3)

Gaussian Sum Filtering

s1

h1

v1

s2

h2

v2

s3

h3

v3

s4

h4

v4

Figure 25.1: The independence structure of the aSLDS.
Square nodes st denote discrete switch variables; ht are
continuous latent/hidden variables, and vt continuous ob-
served/visible variables. The discrete state st determines
which Linear Dynamical system from a ﬁnite set of Linear
Dynamical systems is operational at time t. In the SLDS
links from h to s are not normally considered.

At time t = 1, p(s1|h0, s0) denotes the prior p(s1), and p(h1|h0, s1) denotes p(h1|s1).
The SLDS can be thought of as a marriage between a hidden Markov model and a Linear Dynamical
System. The SLDS is also called a Jump Markov model/process, switching Kalman Filter, Switching
Linear Gaussian State Space model, Conditional Linear Gaussian Model.

25.2.1 Exact inference is computationally intractable

Both exact ﬁltered and smoothed inference in the SLDS is intractable, scaling exponentially with time. As
an informal explanation, consider ﬁltered posterior inference, for which, by analogy with equation (23.2.9),
the forward pass is

p(st+1, ht+1|st, ht, vt+1)p(st, ht|v1:t)

(25.2.4)

p(st+1, ht+1|v1:t+1) =(cid:88)

(cid:90)

st

ht

At timestep 1, p(s1, h1|v1) = p(h1|s1, v1)p(s1|v1) is an indexed set of Gaussians. At timestep 2, due to the
summation over the states s1, p(s2, h2|v1:2) will be an indexed set of S Gaussians; similarly at timestep
3, it will be S2 and, in general, gives rise to St−1 Gaussians at time t. Even for small t, the number of
components required to exactly represent the ﬁltered distribution is therefore computationally intractable.
Analogously, smoothing is also intractable. The origin of the intractability of the SLDS diﬀers from ‘struc-
tural intractability’ that we’ve previously encountered. In the SLDS, in terms of the cluster variables x1:T ,
with xt ≡ (st, ht) and visible variables v1:T , the graph of the distribution is singly-connected. From a
purely graph theoretic viewpoint, one would therefore envisage little diﬃculty in carrying out inference.
Indeed, as we saw above, the derivation of the ﬁltering algorithm is straightforward since the graph is
singly-connected. However, the numerical implementation of the algorithm is intractable since the de-
scription of the messages requires an exponentially increasing number of terms.

In order to deal with this intractability, several approximation schemes have been introduced [98, 110,
174, 161, 160]. Here we focus on techniques which approximate the switch conditional posteriors using
a limited mixture of Gaussians. Since the exact posterior distributions are mixtures of Gaussians, albeit
with an exponentially large number of components, the aim is to drop low weight components such that
the resulting approximation accurately represents the posterior.

25.3 Gaussian Sum Filtering

Equation(25.2.4) describes the exact ﬁltering recursion with an exponentially increasing number of com-
ponents with time. In general, the inﬂuence of ancient observations will be much less relevant than that
of recent observations. This suggests that the ‘eﬀective time’ is limited and that therefore a corresponding
limited number of components in the Gaussian mixture should suﬃce to accurately represent the ﬁltered
posterior[6].

Our aim is to form a recursion for p(st, ht|v1:t) based on a Gaussian mixture approximation of p(ht|st, v1:t).
Given an approximation of the ﬁltered distribution p(st, ht|v1:t) ≈ q(st, ht|v1:t), the exact recursion equa-
tion (25.2.4) is approximated by

p(st+1, ht+1|st, ht, vt+1)q(st, ht|v1:t)

(25.3.1)

q(st+1, ht+1|v1:t+1) =(cid:88)

(cid:90)

st

ht

458

DRAFT March 9, 2010

Gaussian Sum Filtering

This approximation to the ﬁltered posterior at the next timestep will contain S times more components
than in the previous timestep and, to prevent an exponential explosion in mixture components, we will
need to subsequently collapse the mixture q(st+1, ht+1|v1:t+1) in a suitable way. We will deal with this
issue once q(st+1, ht+1|v1:t+1) has been computed.
To derive the updates it is useful to break the new ﬁltered approximation from equation (25.2.4) into
continuous and discrete parts:

q(ht, st|v1:t) = q(ht|st, v1:t)q(st|v1:t)

and derive separate ﬁltered update formulae, as described below.

(25.3.2)

25.3.1 Continuous ﬁltering

The exact representation of p(ht|st, v1:t) is a mixture with O(cid:0)St−1(cid:1) components. To retain computational

feasibility we therefore approximate this with a limited I-component mixture

q(ht|st, v1:t) =

q(ht|it, st, v1:t)q(it|st, v1:t)

(25.3.3)

where q(ht|it, st, v1:t) is a Gaussian parameterised with mean f(it, st) and covariance F(it, st). Strictly
speaking, we should use the notation ft(it, st) since, for each time t, we have a set of means indexed by
it, st, although we drop these dependencies in the notation used here.

urally, this gives rise to a mixture of Gaussians for p(ht|v1:t) = (cid:80)

An important remark is that many techniques approximate p(ht|st, v1:t) using a single Gaussian. Nat-
st p(ht|st, v1:t)p(st|v1:t). However, in
making a single Gaussian approximation to p(ht|st, v1:t) the representation of the posterior may be poor.
Our aim here is to maintain an accurate approximation to p(ht|st, v1:t) by using a mixture of Gaussians.
To ﬁnd a recursion for the approximating distribution we ﬁrst assume that we know the ﬁltered approx-
imation q(ht, st|v1:t) and then propagate this forwards using the exact dynamics. To do so consider ﬁrst
the relation

I(cid:88)

it=1

q(ht+1|st+1, v1:t+1) =(cid:88)
=(cid:88)

st,it

st,it

q(ht+1, st, it|st+1, v1:t+1)
q(ht+1|st, it, st+1, v1:t+1)q(st, it|st+1, v1:t+1)

(25.3.4)

Wherever possible we now substitute the exact dynamics and evaluate each of the two factors above. The
usefulness of decomposing the update in this way is that the new ﬁltered approximation is of the form of
a Gaussian mixture, where q(ht+1|st, it, st+1, v1:t+1) is Gaussian and q(st, it|st+1, v1:t+1) are the weights
or mixing proportions of the components. We describe below how to compute these terms explicitly.
Equation(25.3.4) produces a new Gaussian mixture with I × S components which we will collapse back to
I components at the end of the computation.

Evaluating q(ht+1|st, it, st+1, v1:t+1)
We aim to ﬁnd a ﬁltering recursion for q(ht+1|st, it, st+1, v1:t+1). Since this is conditional on switch states
and components, this corresponds to a single LDS forward step which can be evaluated by considering
(cid:90)
ﬁrst the joint distribution

q(ht+1, vt+1|st, it, st+1, v1:t) =

ht

p(ht+1, vt+1|ht, st, it,st+1, v1:t)q(ht|st, it, st+1, v1:t)

(25.3.5)

and subsequently conditioning on vt+1. In the above we used the exact dynamics where possible. Equation(25.3.5)
states that we know the ﬁltered information up to time t, in addition to knowing the switch states st, st+1

DRAFT March 9, 2010

459

Gaussian Sum Filtering

and the mixture component index it. To ease the burden on notation we derive this for ¯ht, ¯vt ≡ 0 for all
t. The exact forward dynamics is then given by
(25.3.6)

vt+1 = B(st+1)ht+1 + ηv(st+1),

ht+1 = A(st+1)ht + ηh(st+1),

Given the mixture component index it,

q(ht|v1:t, it, st) = N (ht f(it, st), F(it, st))

(25.3.7)
we propagate this Gaussian with the exact dynamics equation (25.3.6). Then q(ht+1, vt+1|st, it, st+1, v1:t)
is a Gaussian with covariance and mean elements

Σhh = A(st+1)F(it, st)AT(st+1) + Σh(st+1), Σvv = B(st+1)ΣhhBT(st+1) + Σv(st+1)
Σvh = B(st+1)Σhh = ΣT

(25.3.8)
These results are obtained from integrating the forward dynamics, equations (25.2.1,25.2.2) over ht, using
the results in section(8.6.3).

hv, µv = B(st+1)A(st+1)f(it, st), µh = A(st+1)f(it, st)

To ﬁnd q(ht+1|st, it, st+1, v1:t+1) we condition q(ht+1, vt+1|st, it, st+1, v1:t) on vt+1 using the standard
Gaussian conditioning formulae, deﬁnition(78), to obtain

(cid:16)

(cid:17)

q(ht+1|st, it, st+1, v1:t+1) = N

ht+1 µh|v, Σh|v

(25.3.9)

(25.3.10)

with

µh|v = µh + ΣhvΣ−1

vv (vt+1 − µv) ,

Σh|v = Σhh − ΣhvΣ−1

vv Σvh

where the quantities required are deﬁned in equation (25.3.8).

Evaluating the mixture weights q(st, it|st+1, v1:t+1)
Up to a normalisation constant the mixture weight in equation (25.3.4) can be found from

q(st, it|st+1, v1:t+1) ∝ q(vt+1|it, st, st+1, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t)

(25.3.11)
The ﬁrst factor in equation (25.3.11), q(vt+1|it, st, st+1, v1:t) is Gaussian with mean µv and covariance
Σvv, as given in equation (25.3.8). The last two factors q(it|st, v1:t) and q(st|v1:t) are given from the
previous ﬁltered iteration. Finally, q(st+1|it, st, v1:t) is found from

(cid:26)

q(st+1|it, st, v1:t) =

(cid:104)p(st+1|ht, st)(cid:105)q(ht|it,st,v1:t)
p(st+1|st)

augmented SLDS
standard SLDS

where the result above for the standard SLDS follows from the independence assumptions present in
the standard SLDS. In the aSLDS, the term in equation (25.3.12) will generally need to be computed
numerically. A simple approximation is to evaluate equation (25.3.12) at the mean value of the distribution
q(ht|it, st, v1:t). To take covariance information into account an alternative would be to draw samples from
the Gaussian q(ht|it, st, v1:t) and thus approximate the average of p(st+1|ht, st) by sampling. Note that this
does not equate Gaussian Sum ﬁltering with a sequential sampling procedure, such as Particle Filtering,
section(27.6.2). The sampling here is exact, for which no convergence issues arise.

(25.3.12)

Closing the recursion

We are now in a position to calculate equation (25.3.4). For each setting of the variable st+1, we have a
mixture of I × S Gaussians. To prevent the number of components increasing exponentially with time, we
numerically collapse q(ht+1|st+1, v1:t+1) back to I Gaussians to form

q(ht+1|st+1, v1:t+1) →

q(ht+1|it+1, st+1, v1:t+1)q(it+1|st+1, v1:t+1)

(25.3.13)

I(cid:88)

it+1=1

Any method of choice may be supplied to collapse a mixture to a smaller mixture. A straightforward
approach is to repeatedly merge low-weight components, as explained in section(25.3.4). In this way the
new mixture coeﬃcients q(it+1|st+1, v1:t+1), it+1 ∈ 1, . . . , I are deﬁned. This completes the description of
how to form a recursion for the continuous ﬁltered posterior approximation q(ht+1|st+1, v1:t+1) in equation
(25.3.2).

460

DRAFT March 9, 2010

Gaussian Sum Filtering

Figure 25.2: Gaussian Sum Filtering. The leftmost column depicts the
previous Gaussian mixture approximation q(ht, it|v1:t) for two states S = 2
(red and blue) and three mixture components I = 3, with the mixture
weight represented by the area of each oval. There are S = 2 diﬀerent
linear systems which take each of the components of the mixture into a
new ﬁltered state, the colour of the arrow indicating which dynamic system
is used. After one time-step each mixture component branches into a
further S components so that the joint approximation q(ht+1, st+1|v1:t+1)
contains S2I components (middle column). To keep the representation
computationally tractable the mixture of Gaussians for each state st+1
is collapsed back to I components. This means that each coloured state
needs to be approximated by a smaller I component mixture of Gaussians.
There are many ways to achieve this. A naive but computationally eﬃcient
approach is to simply ignore the lowest weight components, as depicted on
the right column, see mix2mix.m.

25.3.2 Discrete ﬁltering

A recursion for the switch variable distribution in equation (25.3.2) is

q(st+1, it, st, vt+1, v1:t)

(cid:88)

it,st

q(st+1|v1:t+1) ∝
(cid:88)

st,it

The r.h.s. of the above equation is proportional to

q(vt+1|st+1, it, st, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t)

(25.3.14)

(25.3.15)

for which all terms have been computed during the recursion for q(ht+1|st+1, v1:t+1). We now have all
the quantities required to compute the Gaussian Sum approximation of the ﬁltering forward pass. A
schematic representation of Gaussian Sum ﬁltering is given in ﬁg(25.2) and the pseudo code is presented
in algorithm(22). See also SLDSforward.m.

25.3.3 The likelihood p(v1:T )
The likelihood p(v1:T ) may be found from

T−1(cid:89)

p(v1:T ) =

t=0

where

p(vt+1|v1:t) ≈

p(vt+1|v1:t)
(cid:88)

it,st,st+1

q(vt+1|it, st, st+1, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t)

(25.3.16)

In the above expression, all terms have been computed in forming the recursion for the ﬁltered posterior
q(ht+1, st+1|v1:t+1).
25.3.4 Collapsing Gaussians

Given a mixture of N Gaussians

N(cid:88)

i=1

p(x) =

piN (x µi, Σi)

(25.3.17)

we wish to collapse this to a smaller K < N mixture of Gaussians. We describe a simple method which
has the advantage of computational eﬃciency, but the disadvantage that no spatial information about

DRAFT March 9, 2010

461

t+1tGaussian Sum Smoothing

(cid:80)
Algorithm 22 aSLDS Forward Pass. Approximate the ﬁltered posterior p(st|v1:t) ≡ αt, p(ht|st, v1:t) ≡
it wt(it, st)N (ht ft(it, st), Ft(it, st)). Also return the approximate log-likelihood L ≡ log p(v1:T ). It are
the number of components in each Gaussian mixture approximation. We require I1 = 1, I2 ≤ S, It ≤
S × It−1. θ(s) = A(s), B(s), Σh(s), Σv(s), ¯h(s), ¯v(s).
for s1 ← 1 to S do

{f1(1, s1), F1(1, s1), ˆp} = LDSFORWARD(0, 0, v1; θ(s1))
α1 ← p(s1)ˆp

end for
for t ← 2 to T do

for st ← 1 to S do

for i ← 1 to It−1, and s ← 1 to S do

{µx|y(i, s), Σx|y(i, s), ˆp} = LDSFORWARD(ft−1(i, s), Ft−1(i, s), vt; θ(st))
p∗(st|i, s) ≡ (cid:104)p(st|ht−1, st−1 = s)(cid:105)p(ht−1|it−1=i,st−1=s,v1:t−1)
p(cid:48)(st, i, s) ← wt−1(i, s)p∗(st|i, s)αt−1(s)ˆp
end for
(cid:80)It
Collapse the It−1 × S mixture of Gaussians deﬁned by µx|y,Σx|y,
and weights
p(i, s|st) ∝ p(cid:48)(st, i, s)
p(ht|st, v1:t) ≈
it=1 p(it|st, v1:t)p(ht|st, it, v1:t). This deﬁnes the new means ft(it, st), covariances
Ft(it, st) and mixture weights wt(it, st) ≡ p(it|st, v1:t).
Compute αt(st) ∝
L ← L + log(cid:80)

(cid:80)
i,s p(cid:48)(st, i, s)

end for
normalise αt

to

a Gaussian with It

st,i,s p(cid:48)(st, i, s)

components,

end for

the mixture is used[277]. First we describe how to collapse a mixture to a single Gaussian. This can be
achieved by ﬁnding the mean and covariance of the mixture distribution (25.3.17). These are

µ =(cid:88)

piµi,

Σ =(cid:88)

(cid:16)

(cid:17)

i

i

pi

Σi + µiµT
i

− µµT

(25.3.18)

To collapse a mixture then to a K-component mixture we may ﬁrst retain the K − 1 Gaussians with the
largest mixture weights. The remaining N − K + 1 Gaussians are simply merged to a single Gaussian
using the above method. Alternative heuristics such as recursively merging the two Gaussians with the
lowest mixture weights are also reasonable.

More sophisticated methods which retain some spatial information would clearly be potentially useful. The
method presented in [174] is a suitable approach which considers removing Gaussians which are spatially
similar (and not just low-weight components), thereby retaining a sense of diversity over the possible
solutions. In applications with many thousands of timesteps, speed can be a factor in determining which
method of collapsing Gaussians is to be preferred.

25.3.5 Relation to other methods

Gaussian Sum Filtering can be considered a form of ‘analytical particle ﬁltering’, section(27.6.2), in which
instead of point distributions (delta functions) being propagated, Gaussians are propagated. The collapse
operation to a smaller number of Gaussians is analogous to resampling in Particle Filtering. Since a
Gaussian is more expressive than a delta-function, the Gaussian Sum ﬁlter is generally an improved
approximation technique over using point particles. See [17] for a numerical comparison.

25.4 Gaussian Sum Smoothing
Approximating the smoothed posterior p(ht, st|v1:T ) is more involved than ﬁltering and requires additional
approximations. For this reason smoothing is more prone to failure since there are more assumptions that
need to be satisﬁed for the approximations to hold. The route we take here is to assume that a Gaussian

462

DRAFT March 9, 2010

Gaussian Sum Smoothing

p(ht, st|v1:T ) =(cid:88)

(cid:90)

st+1

ht+1

p(st, ht|v1:T ) =(cid:88)
p(st, ht|v1:T ) =(cid:88)

st+1

st+1

Sum ﬁltered approximation has been carried out, and then approximate the γ backward pass, analogous
to that of section(23.2.4). By analogy with the RTS smoothing recursion equation (23.2.20), the exact
backward pass for the SLDS reads

p(ht, st|ht+1, st+1, v1:t)p(ht+1, st+1|v1:T )

(25.4.1)

where p(ht+1, st+1|v1:T ) = p(st+1|v1:T )p(ht+1|st+1, v1:T ) is composed of the discrete and continuous com-
ponents of the smoothed posterior at the next time step. The recursion runs backwards in time, beginning
with the initialisation p(hT , sT|v1:T ) set by the ﬁltered result (at time t = T , the ﬁltered and smoothed
posteriors coincide). Apart from the fact that the number of mixture components will increase at each
step, computing the integral over ht+1 in equation (25.4.1) is problematic since the conditional distribu-
tion term is non-Gaussian in ht+1. For this reason it is more useful derive an approximate recursion by
beginning with the exact relation

p(st+1|v1:T )p(ht|st, st+1, v1:T )p(st|st+1, v1:T )

(25.4.2)

which can be expressed more directly in terms of the SLDS dynamics as

p(st+1|v1:T )(cid:104)p(ht|ht+1, st, st+1, v1:t,vt+1:T )(cid:105)p(ht+1|st,st+1,v1:T )

(25.4.3)
In forming the recursion we assume access to the distribution p(st+1, ht+1|v1:T ) from the future timestep.
However, we also require the distribution p(ht+1|st, st+1, v1:T ) which is not directly known and needs to
be inferred, in itself a computationally challenging task. In the Expectation Correction (EC) approach [17]
one assumes the approximation (see ﬁg(25.3))
p(ht+1|st, st+1, v1:T ) ≈ p(ht+1|st+1, v1:T )

× (cid:104)p(st|ht+1, st+1, v1:T )(cid:105)p(ht+1|st+1,v1:T )

(25.4.4)

resulting in an approximate recursion for the smoothed posterior,

p(st, ht|v1:T ) ≈

p(st+1|v1:T )(cid:104)p(ht|ht+1, st, st+1, v1:t)(cid:105)ht+1 (cid:104)p(st|ht+1, st+1, v1:T )(cid:105)ht+1

(25.4.5)

(cid:88)

st+1

where (cid:104)·(cid:105)ht+1 represents averaging with respect to the distribution p(ht+1|st+1, v1:T ). In carrying out the
approximate recursion, (25.4.5) we will end up with a mixture of Gaussians that grows at each timestep.
To avoid the exponential explosion problem, we use a ﬁnite mixture approximation, q(ht+1, st+1|v1:T ):

p(ht+1, st+1|v1:T ) ≈ q(ht+1, st+1|v1:T ) = q(ht+1|st+1, v1:T )q(st+1|v1:T )

(25.4.6)

and plug this into the approximate recursion above. From equation (25.4.5) a recursion for the approxi-
mation is given by

(cid:125)
q(st+1|v1:T )(cid:104)q(ht|ht+1, st, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:T )

(cid:123)(cid:122)

(cid:124)

(cid:124)
(cid:125)
(cid:104)q(st|ht+1, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:T )

(cid:123)(cid:122)

q(ht|st,st+1,v1:T )

q(st|st+1,v1:T )

q(ht, st|v1:T ) =(cid:88)

st+1

As for ﬁltering, wherever possible, we replace approximate terms by their exact counterparts and param-
eterise the posterior using

q(ht+1, st+1|v1:T ) = q(ht+1|st+1, v1:T )q(st+1|v1:T )

(25.4.8)

To reduce the notational burden here we outline the method only for the case of using a single component
approximation in both the forward and backward passes. The extension to using a mixture to approxi-
mate each p(ht+1|st+1, v1:T ) is conceptually straightforward and deferred to section(25.4.4). In the single
Gaussian case we assume we have a Gaussian approximation available for

(25.4.7)

q(ht+1|st+1, v1:T ) = N (ht+1 g(st+1, G(st+1))

DRAFT March 9, 2010

(25.4.9)

463

Gaussian Sum Smoothing

Figure 25.3:
The EC backpass approximates
p(ht+1|st+1, st, v1:T ) by p(ht+1|st+1, v1:T ). The moti-
vation for this is that st inﬂuences ht+1 only indirectly
through ht. However, ht will most likely be heavily
inﬂuenced by v1:t, so that not knowing the state of
st is likely to be of secondary importance. The green
shaded node is the variable we wish to ﬁnd the pos-
terior for. The values of the blue shaded nodes are
known, and the red shaded node indicates a known
variable which is assumed unknown in forming the
approximation.

st−1

ht−1

vt−1

st

ht

vt

st+1

st+2

ht+1

ht+2

vt+1

vt+2

25.4.1 Continuous smoothing

For given st, st+1, an RTS style recursion for the smoothed continuous is obtained from equation (25.4.7),
giving

(cid:90)

q(ht|st, st+1, v1:T ) =

ht+1

p(ht|ht+1, st, st+1, v1:t)q(ht+1|st+1, v1:T )

(25.4.10)

To compute equation (25.4.10) we then perform a single update of the LDS backward recursion, section(24.4.2).

25.4.2 Discrete smoothing

The second average in equation (25.4.7) corresponds to a recursion for the discrete variable and is given
by

(cid:104)q(st|ht+1, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:T ) ≡ q(st|st+1, v1:T ).

(25.4.11)
The average of q(st|ht+1, st+1, v1:t) with respect to q(ht+1|st+1, v1:T ) cannot be achieved in closed form.
A simple approach is to approximate the average by evaluation at the mean1

(cid:104)q(st|ht+1, st+1v1:t)(cid:105)q(ht+1|st+1,v1:T ) ≈ q(st|ht+1, st+1, v1:t)(cid:12)(cid:12)ht+1=(cid:104)ht+1|st+1,v1:T (cid:105)

(25.4.12)

where (cid:104)ht+1|st+1, v1:T(cid:105) is the mean of ht+1 with respect to q(ht+1|st+1, v1:T ).
Replacing ht+1 by its mean gives the approximation

(cid:104)q(st|ht+1, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:T ) ≈
where

e

1
Z

− 1

2 zT

t+1(st,st+1)Σ−1(st,st+1|v1:t)zt+1(st,st+1)

(cid:112)det (Σ(st, st+1|v1:t))

q(st|st+1, v1:t) (25.4.13)

zt+1(st, st+1) ≡ (cid:104)ht+1|st+1, v1:T(cid:105) − (cid:104)ht+1|st, st+1, v1:t(cid:105)

(25.4.14)
and Z ensures normalisation over st. Σ(st, st+1|v1:t) is the ﬁltered covariance of ht+1 given st, st+1 and
the observations v1:t, which may be taken from Σhh in equation (25.3.8). Approximations which take
covariance information into account can also be considered, although the above simple (and fast) method
may suﬃce in practice [17, 192].

25.4.3 Collapsing the mixture

From section(25.4.1) and section(25.4.2) we now have all the terms in equation (25.4.8) to compute the
approximation to equation (25.4.7). Due to the summatino over st+1 in equation (25.4.7), the number
of mixture components is multiplied by S at each iteration. To prevent an exponential explosion of
components, the mixture equation (25.4.7) is then collapsed to a single Gaussian

q(ht, st|v1:T ) → q(ht|st, v1:T )q(st|v1:T ))

The collapse to a mixture is discussed in section(25.4.4).

1In general this approximation has the form (cid:104)f (x)(cid:105) ≈ f ((cid:104)x(cid:105)).

(25.4.15)

464

DRAFT March 9, 2010

Gaussian Sum Smoothing

routine needs the results from algorithm(22).

Algorithm 23 aSLDS: EC Backward Pass.

(cid:80)Jt
Approximates p(st|v1:T ) and p(ht|st, v1:T ) ≡
jt=1 ut(jt, st)N (gt(jt, st), Gt(jt, st)) using a mixture of Gaussians. JT = IT , Jt ≤ S × It × Jt+1. This
GT ← FT , gT ← fT , uT ← wT
for t ← T − 1 to 1 do
for s ← 1 to S, s(cid:48)

(µ, Σ)(i, s, j(cid:48), s(cid:48)) = LDSBACKWARD(gt+1(j(cid:48), s(cid:48)), Gt+1(j(cid:48), s(cid:48)), ft(i, s), Ft(i, s), θ(s(cid:48)))
p(it, st|jt+1, st+1, v1:T ) = (cid:104)p(st = s, it = i|ht+1, st+1 = s(cid:48), jt+1 = j(cid:48), v1:t)(cid:105)p(ht+1|st+1=s(cid:48),jt+1=j(cid:48),v1:T )
p(i, s, j(cid:48), s(cid:48)

← 1 to S, i ← 1 to It, j(cid:48)

← 1 to Jt+1 do

|v1:T ) ← p(st+1 = s(cid:48)

|v1:T )ut+1(j(cid:48), s(cid:48))p(it, st|jt+1, st+1, v1:T )

end for
for st ← 1 to S do
Collapse the mixture deﬁned by weights p(it = i, st+1 = s(cid:48), jt+1 = j(cid:48)
|st, v1T ) ∝
p(i, st, j(cid:48), s(cid:48)
|v1:T ), means µ(it, st, jt+1, st+1) and covariances Σ(it, st, jt+1, st+1) to a mix-
(cid:80)
ture with Jt components. This deﬁnes the new means gt(jt, st), covariances Gt(jt, st) and
mixture weights ut(jt, st).
it,j(cid:48),s(cid:48) p(it, st, j(cid:48), s(cid:48)
p(st|v1:T ) ←

|v1:T )

end for

end for

25.4.4 Using mixtures in smoothing

The extension to the mixture case is straightforward based on the representation

p(it, jt+1, st+1|st, v1:T )p(ht|it, st, jt+1, st+1, v1:T )

This mixture can then be collapsed to a smaller mixture using any method of choice, to give

p(ht|st, v1:T ) = (cid:88)
(cid:88)

p(ht|st, v1:T ) ≈

jt

it,jt+1,st+1

q(jt|st, v1:T )q(ht|jt, v1:T )

(25.4.19)

(25.4.20)

(25.4.21)

The resulting procedure is sketched in algorithm(23), including using mixtures in both the forward and
backward passes.

DRAFT March 9, 2010

465

p(ht|st, v1:T ) ≈

J(cid:88)
q(ht, st|v1:T ) = (cid:88)

jt=1

it,jt+1,st+1

q(jt|st, v1:T )q(ht|jt, v1:T ).

(25.4.16)

Analogously to the case with a single component,

p(st+1|v1:T )p(jt+1|st+1, v1:T )q(ht|jt+1, st+1, it, st, v1:T )

× (cid:104)q(it, st|ht+1, jt+1, st+1, v1:t)(cid:105)q(ht+1|jt+1,st+1,v1:T )

(25.4.17)

The average in the last line of the above equation can be tackled using the same techniques as outlined
in the single Gaussian case. To approximate q(ht|jt+1, st+1, it, st, v1:T ) we consider this as the marginal of
the joint distribution

q(ht, ht+1|it, st, jt+1, st+1, v1:T ) = q(ht|ht+1, it, st, jt+1, st+1, v1:t)q(ht+1|it, st, jt+1, st+1, v1:T ) (25.4.18)
As in the case of a single mixture, the problematic term is q(ht+1|it, st, jt+1, st+1, v1:T ). Analogously to
equation (25.4.4), we make the assumption

q(ht+1|it, st, jt+1, st+1, v1:T ) ≈ q(ht+1|jt+1, st+1, v1:T )

meaning that information about the current switch state st, it is ignored. We can then form

25.4.5 Relation to other methods

Gaussian Sum Smoothing

A classical smoothing approximation for the SLDS is generalised pseudo Bayes
In GPB one starts from the exact recursion

(GPB) [13, 160, 159].

p(st|v1:T ) =(cid:88)

p(st, st+1|v1:T ) =(cid:88)

st+1

p(st|st+1, v1:T )p(st+1|v1:T )

st+1

The quantity p(st|st+1, v1:T ) is diﬃcult to obtain and GPB makes the approximation

p(st|st+1, v1:T ) ≈ p(st|st+1, v1:t)

Plugging this into equation (25.4.22) we have

p(st|v1:T ) ≈

(cid:88)
=(cid:88)

st+1

st+1

p(st|st+1, v1:t)p(st+1|v1:T )
(cid:80)
p(st+1|st)p(st|v1:t)
st p(st+1|st)p(st|v1:t) p(st+1|v1:T )

(25.4.22)

(25.4.23)

(25.4.24)

(25.4.25)

The recursion is initialised with the approximate ﬁltered p(sT|v1:T ). Computing the smoothed recursion
for the switch states in GPB is then equivalent to running the RTS backward pass on a hidden Markov
model, independently of the backward recursion for the continuous variables. The only information the
GPB method uses to form the smoothed distribution p(st|v1:T ) from the ﬁltered distribution p(st|v1:t)
is the Markov switch transition p(st+1|st). This approximation drops information from the future since
information passed via the continuous variables is not taken into account. In contrast to GPB, the EC
Gaussian smoothing technique preserves future information passing through the continuous variables. As
for EC, GPB forms an approximation for p(ht|st, v1:T ) by using the recursion (25.4.8) where q(st|st+1, v1:T )
is given by q(st|st+1, v1:t). In SLDSbackward.m one may choose to use either EC or GBP.

Example 105 (Traﬃc Flow). A illustration of modelling and inference with a SLDS is to consider a
simple network of traﬃc ﬂow, ﬁg(25.4). Here there are 4 junctions a, b, c, d and traﬃc ﬂows along the
roads in the direction indicated. Traﬃc ﬂows into the junction at a and then goes via diﬀerent routes
to d. Flow out of a junction must match the ﬂow in to a junction (up to noise). There are traﬃc
light switches at junctions a and b which, depending on their state, route traﬃc diﬀerently along the roads.

Using φ to denote the clean (noise free) ﬂow, we model the ﬂows using the switching linear system:

φa(t − 1)
φa(t − 1) (0.75 × I [sa(t) = 1] + 1 × I [sa(t) = 2] + 0 × I [sa(t) = 3])
φa(t − 1) (0.25 × I [sa(t) = 1] + 0 × I [sa(t) = 2] + 1 × I [sa(t) = 3])
φa→b(t − 1) (0.5 × I [sb(t) = 1] + 0 × I [sb(t) = 2])
φa→b(t − 1) (0.5 × I [sb(t) = 1] + 1 × I [sb(t) = 2])
φb→c(t − 1)

(25.4.26)

By identifying the ﬂows at time t with a 6 dimensional vector hidden variable ht, we can write the above
ﬂow equations as

 =



φa(t)
φa→d(t)
φa→b(t)
φb→d(t)
φb→c(t)
φc→d(t)

ht = A(s)ht−1 + ηh
t

(25.4.27)

(cid:78) sb, which takes

for a set of suitably deﬁned matrices A(s) indexed by the switch variable s = sa
3 × 2 = 6 states. We additionally include noise terms to model cars parking or de-parking during a single
timestep. The covariance Σh is diagonal with a larger variance at the inﬂow point a to model that the
total volume of traﬃc entering the system can vary.

Noisy measurements of the ﬂow into the network are taken at a

v1,t = φa(t) + ηv

1(t)

466

(25.4.28)

DRAFT March 9, 2010

Gaussian Sum Smoothing

a

d

b

c

Figure 25.4: A representation of the traﬃc ﬂow between junctions at a,b,c,d, with
traﬃc lights at a and b. If sa = 1 a → d and a → b carry 0.75 and 0.25 of the ﬂow
out of a respectively. If sa = 2 all the ﬂow from a goes through a → d; for sa = 3,
all the ﬂow goes through a → b. For sb = 1 the ﬂow out of b is split equally between
b → d and b → c. For sb = 2 all ﬂow out of b goes along b → c.

Figure 25.5: Time evolution of the traﬃc ﬂow measured
at two points in the network. Sensors measure the total
ﬂow into the network φa(t) and the total ﬂow out of the
network, φd(t) = φa→d(t) + φb→d(t) + φc→d(t). The total
inﬂow at a undergoes a random walk. Note that the ﬂow
measured at d can momentarily drop to zero if all traﬃc is
routed through a → b → c in two consecutive time steps.

along with a noisy measurement of the total ﬂow out of the system at d,

v2,t = φd(t) = φa→d(t) + φb→d(t) + φc→d(t) + ηv

2(t)

(25.4.29)

The observation model can be represented by vt = Bht + ηv
t using a constant 2 × 6 projection
matrix B. The switch variables follow a simple Markov transition p(st|st−1) which biases the switches
to remain in the same state in preference to jumping to another state. See demoSLDStraffic.m for details.

Given the above system and a prior which initialises all ﬂow at a, we draw samples from the model using
forward (ancestral) sampling which form the observations v1:100, ﬁg(25.5). Using only the observations
and the known model structure we then attempt to infer the latent switch variables and traﬃc ﬂows
using Gaussian Sum ﬁltering and smoothing (EC method) with 2 mixture components per switch state,
ﬁg(25.6).

We note that a naive HMM approximation based on discretising each continuous ﬂow into 20 bins would
contain 2 × 3 × 206 or 384 million states. Even for modest size problems, a naive approximation based on
discretisation is therefore impractical.

Example 106 (Following the price trend). The following is a simple model of the price trend of a stock,
which assumes that the price tends to continue going up (or down) for a while before it reverses direction:

h1(t) = h1(t − 1) + h2(t − 1) + ηh
h2(t) = I [s(t) = 1] h2(t − 1) + ηh
v(t) = h1(t) + ηv(st)

1 (st)
2 (st)

(25.4.30)
(25.4.31)
(25.4.32)

here h1 represents the price and h2 the direction. There is only a single observation variable at each time,
which is the price plus a small amount of noise. There are two switch states, dom(st) = {1, 2}. When
st = 1, the model functions normally, with the direction being equal to the previous direction plus a small
amount of noise ηh
2 (st = 1). When st = 2 however, the direction is sampled from a Gaussian with a large
variance. The transition p(st|st−1) is set so that normal dynamics is more likely, and when st = 2 it is
likely to go back to normal dynamics the next timestep. Full details are in SLDSpricemodel.mat.
In
ﬁg(25.7) we plot some samples from the model and also smoothed inference of the switch distribution,
showing how we can a posteriori infer the likely changes in the stock price direction. See also exercise(239).

DRAFT March 9, 2010

467

0204060801000204002040608010002040Reset Models

(a)

(b)

(c)

Figure 25.6: Given the observations from ﬁg(25.5) we infer the ﬂows and switch states of all the latent
(a): The correct latent ﬂows through time along with the switch variable state used to
variables.
generate the data. The colours corresponds to the ﬂows at the corresponding coloured edges/nodes in
(b): Filtered ﬂows based on a I = 2 Gaussian Sum forward pass approximation. Plotted are
ﬁg(25.4).
the 6 components of the vector (cid:104)ht|v1:t(cid:105) with the posterior distribution of the sa and sb traﬃc light states
(c): Smoothed ﬂows (cid:104)ht|v1:T(cid:105) and corresponding smoothed switch
p(sa
states p(st|v1:T ) using a Gaussian Sum smoothing approximation (EC with J = 1).

t|v1:t) plotted below.

t |v1:t),p(sb

Figure 25.7: The top panel is a time series of ‘prices’. The prices
tend to keep going up or down with infrequent changes in the
direction. Based on ﬁtting a simple SLDS model to capture
this kind of behaviour, the probability of a signiﬁcant change
in the price direction is given in the panel below based on the
smoothed distribution p(st = 2|v1:T ).

25.5 Reset Models

Reset models are special switching models in which the switch state isolates the present from the past,
resetting the position of the latent dynamics (these are also known as changepoint models). Whilst these
models are rather general, it can be helpful to consider a speciﬁc model, and here we consider the SLDS
changepoint model with two states. We use the state st = 0 to denote that the LDS continues with the
standard dynamics. With st = 1, however, the continuous dynamics is reset to a prior:

(cid:0)ht µ1, Σ1(cid:1)

p1(ht) = N

(25.5.1)

(25.5.2)

(25.5.3)

st = 1

p1(ht)

(cid:26) p0(ht|ht−1) st = 0
(cid:0)ht Aht−1 + µ0, Σ0(cid:1) ,
(cid:26) p0(vt|ht) st = 0

p1(vt|ht) st = 1

p(ht|ht−1, st) =

where

p0(ht|ht−1) = N

similarly we write

p(vt|ht, st) =

For simplicity we assume the switch dynamics are ﬁrst order Markov with transition p(st|st−1). Under
this model the dynamics follows a standard LDS, but when st = 1, ht is reset to a value drawn from

468

DRAFT March 9, 2010

020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100120140160180200−40−30−20−1001002040608010012014016018020000.20.40.60.81Reset Models

c1

h1

v1

c2

h2

v2

c3

h3

v3

c4

h4

v4

Figure 25.8: The independence structure of a reset model.
Square nodes ct denote the binary reset variables and st
the state dynamics. The ht are continuous variables, and
vt continuous observations.
If the dynamics resets, the
dependence of the continuous ht on the past is cut.

a Gaussian distribution, independent of the past. Such models might be of interest in prediction where
the time-series is following a trend but suddenly changes and the past is forgotten. Whilst this may not

seem like a big change to the model, this model is computationally more tractable, scaling with O(cid:0)T 2(cid:1),
compared to O(cid:0)T 2T(cid:1) in the general two-state SLDS. To see this, consider the ﬁltering recursion

(cid:90)

(cid:88)

α(ht, st) ∝

ht−1

st−1

(cid:90)

p(vt|ht, st)p(ht|ht−1, st)p(st|st−1)α(ht−1, st−1)
(cid:88)

p0(vt|ht)p0(ht|ht−1)p(st = 0|st−1)α(ht−1, st−1)

We now consider the two cases

α(ht, st = 0) ∝

(cid:88)

st−1

p(st = 1|st−1)α(ht−1, st−1)

p(st = 1|st−1)α(st−1)

α(ht, st = 1) ∝ p1(vt|ht)p1(ht)

ht−1

ht−1

st−1

(cid:90)
∝ p1(vt|ht)p1(ht)(cid:88)
(cid:90)

st−1

(cid:90)

(25.5.4)

(25.5.5)

(25.5.6)

(25.5.8)

(25.5.9)

469

Equation(25.5.6) shows that p(ht, st = 1|v1:t) is not a mixture model in ht, but contains only a single
component proportional to p1(vt|ht)p1(ht). If we use this information in equation (25.5.5) we have

α(ht, st = 0) ∝

ht−1

p0(vt|ht)p0(ht|ht−1)p(st = 0|st−1 = 0)α(ht−1, st−1 = 0)

+

ht−1

p0(vt|ht)p0(ht|ht−1)p(st = 0|st−1 = 1)α(ht−1, st−1 = 1)

(25.5.7)

Assuming α(ht−1, st−1 = 0) is a mixture distribution with K components, then α(ht, st = 0) will be a
In general, therefore, α(ht, st = 0) will contain T components and
mixture with K + 1 components.
α(ht, st = 1) a single component. As opposed to the full SLDS case, the number of components therefore
grows only linearly with time, as opposed to exponentially. This means that the computational eﬀort to

perform exact ﬁltering scales as O(cid:0)T 2(cid:1).

Run-length formalism

One may also describe reset models using a ‘run-length’ formalism using at each time t a latent variable rt
which describes the length of the current segment[3]. If there is a change, the run-length variable is reset
to zero, otherwise it is increased by 1:

rt = 0
rt = rt−1 + 1

1 − Pcp

p(rt|rt−1) =

(cid:26) Pcp
p(v1:T , r1:T ) =(cid:89)

t

DRAFT March 9, 2010

where Pcp is the probability of a reset (or ‘changepoint’). The joint distribution is given by

p(rt|rt−1)p(vt|v1:t−1, rt),

p(vt|v1:t−1, rt) = p(vt|vt−rt:t−1)

with the understanding that if rt = 0 then p(vt|vt−rt:t−1) = p(vt). The graphical model of this distribution
is awkward to draw since the number of links depends on the run-length rt. Predictions can be made using

p(vt+1|vt−rt:t)p(rt|v1:t)

(25.5.10)

Reset Models

rt

p(vt+1|v1:t) =(cid:88)
p(rt, v1:t) =(cid:88)
=(cid:88)
=(cid:88)

rt−1

rt−1

where the ﬁltered ‘run-length’ p(rt|v1:t) is given by the forward recursion:

p(rt, rt−1, v1:t−1, vt) =(cid:88)
p(vt|rt,rt−1, v1:t−1)p(rt|rt−1,v1:t−1)p(rt−1, v1:t−1)
p(rt|rt−1)p(vt|vt−rt:t−1)p(rt−1, v1:t−1)

rt−1

p(rt, vt|rt−1, v1:t−1)p(rt−1, v1:t−1)

which shows that ﬁltered inference scales with O(cid:0)T 2(cid:1).

rt−1

25.5.1 A Poisson reset model

The changepoint structure is not limited to conditionally Gaussian cases only. To illustrate this, we
consider the following model2: At each time t, we observe a count yt which we assume is Poisson distributed
with an unknown positive intensity h. The intensity is constant, but at certain unknown times t, it jumps to
a new value. The indicator variable ct denotes whether time t is such a changepoint or not. Mathematically,
the model is:

p(h0) = G(h0; a0, b0)
p(ct) = BE(ct; π)

p(vt|ht) = PO(vt; ht)

p(ht|ht−1, ct) = I [ct = 0] δ(ht, ht−1) + I [ct = 1]G(ht; ν, b)

(25.5.11)
(25.5.12)
(25.5.13)
(25.5.14)

The symbols G, BE and PO denote the Gamma, Bernoulli and the Poisson distributions respectively:

G(h; a, b) = exp ((a − 1) log h − bh − log Γ(a) + a log b)
BE(c; π) = exp (c log π + (1 − c) log(1 − π))
PO(v; h) = exp (v log h − h − log Γ(v + 1))

(25.5.15)
(25.5.16)
(25.5.17)

Given observed counts v1:T , the task is to ﬁnd the posterior probability of a change and the associated
intensity levels for each region between two consecutive changepoints. Plugging the above deﬁnitions in the
generic updates equation (25.5.5) and equation (25.5.6), we see that α(ht, ct = 0) is a Gamma potential,
and that α(gt, ct = 1) is a mixture of Gamma potentials, where a Gamma potential is deﬁned as

φ(h) = elG(h; a, b)

(25.5.18)

via the triple (a, b, l). For the corrector update step we need to calculate the product of a Poisson term
with the observation model p(vt|ht) = PO(vt; ht). A useful property of the Poisson distribution is that,
given the observation, the latent variable is Gamma distributed:

PO(v; h) = v log h − h − log Γ(v + 1)

= (v + 1 − 1) log h − h − log Γ(v + 1)
= G(h; v + 1, 1)

(25.5.19)
(25.5.20)
(25.5.21)

Hence, the update equation requires multiplication of two Gamma potentials. A nice property of the
Gamma density is that the product of two Gamma densities is also a Gamma potential:

(a1, b1, l1) × (a2, b2, l2) = (a1 + a2 − 1, b1 + b2, l1 + l2 + g(a1, b1, a2, b2))

(25.5.22)

2This example is due to Taylan Cemgil.

470

DRAFT March 9, 2010

Reset Models

where

g(a1, b1, a2, b2) ≡ log

Γ(a1 + a2 − 1)
Γ(a1)Γ(a2)

+ log(b1 + b2) + a1 log(b1/(b1 + b2)) + a2 log(b2/(b1 + b2)) (25.5.23)

The α recursions for this reset model are therefore closed in the space of a mixture of Gamma potentials,
with an additional Gamma potential in the mixture at each timestep. A similar approach can be used to
form the smoothing recursions.

Example 107 (Coal mining disasters). We illustrate the algorithm on the coal mining disaster dataset
[144]. The data set consists of the number of deadly coal-mining disasters in England per year over a
time span of 112 years from 1851 to 1962. It is widely agreed in the statistical literature that a change
in the intensity (the expected value of the number of disasters) occurs around the year 1890, after new
health and safety regulations were introduced. In ﬁg(25.9) we show the marginals p(ht|y1:T ) along with
the ﬁltering density. Note that we are not constraining the number of changepoints and in principle allow
any number. The smoothed density suggests a sharp decrease around t = 1890.

Figure 25.9: Estimation of change points. (Top) coal
mining disaster dataset. (Middle) Filtered estimate
of the marginal
intensity p(ht|v1:t) and (Bottom)
smoothed estimate p(ht|v1:T ). We evaluate these mix-
ture of Gamma distributions on a ﬁxed grid of h and
show the density as a function of t. Here, darker color
means higher probability.

25.5.2 HMM-reset

The reset model deﬁned by equations (25.5.1,25.5.3) above is useful in many applications, but is limited
since only a single dynamical model is considered. An important extension is to consider a set of available
dynamical models, indexed by st ∈ {1, . . . , S}, with a reset that cuts dependency of the continuous variable
on the past[93, 57]:

(cid:26) p0(ht|ht−1, st)

p1(ht|st)

p(ht|ht−1, st, ct) =

ct = 0
ct = 1

(25.5.24)

with the reset α recursions, equations(25.5.5,25.5.6) on replacing ht by (ht, st). To see this we consider
the ﬁltering recursion for the two cases

The computational complexity of ﬁltering for this model is O(cid:0)S2T 2(cid:1) which can be understood by analogy

The states st follow a Markovian dynamics p(st|st−1, ct−1), see ﬁg(25.10). A reset occurs if the state st
changes, otherwise, no reset occurs:
p(ct = 1|st, st−1) = I [st (cid:54)= st−1]
(cid:88)
(cid:88)

p0(vt|ht, st)p0(ht|ht−1, st)p(st|st−1, ct−1)p(ct = 0|st, st−1)α(ht−1, ct−1)
(25.5.26)

α(ht, st, ct = 0) =

ht−1

st−1,ct−1

(25.5.25)

(cid:90)

α(ht, st, ct = 1) =

p1(vt|ht, st)p1(ht|st)p(st|st−1, ct)p(ct = 1|st, st−1)α(ht−1, st−1, ct−1)

(cid:90)
= p1(vt|ht, st)p1(ht|st) (cid:88)

st−1,ct−1

ht−1

st−1,ct−1

DRAFT March 9, 2010

p(ct = 1|st, st−1)p(st|st−1, ct−1)α(st−1, ct−1)

(25.5.27)

471

186018701880189019001910192019301940195019600246# of accidentsfiltered intensity18601870188018901900191019201930194019501960246smoothed intensityyear18601870188018901900191019201930194019501960246c1

s1

h1

v1

c2

s2

h2

v2

c3

s3

h3

v3

c4

s4

h4

v4

Exercises

Figure 25.10: The independence structure of a HMM-reset
model. Square nodes ct denote discrete switch variables;
ht are continuous latent variables, and vt continuous ob-
servations. The discrete state ct determines which Linear
Dynamical system from a ﬁnite set of Linear Dynamical
systems is operational at time t.

From equation (25.5.27) we see that α(ht, st, ct = 1) contains only a single component proportional to
p1(vt|ht, st)p1(ht|st). This is therefore exactly analogous to the standard reset model, except that we
need now to index a set of messages with st, therefore each message taking O (S) steps to compute. The

computational eﬀort to perform exact ﬁltering scales as O(cid:0)S2T 2(cid:1).

25.6 Code

SLDSforward.m: SLDS forward
SLDSbackward.m: SLDS backward (Expectation Correction)
mix2mix.m: Collapse a mixture of Gaussians to a smaller mixture of Gaussians
SLDSmargGauss.m: Marginalise an SLDS Gaussian mixture
logeps.m: Logarithm with oﬀset to deal with log(0)
demoSLDStraffic.m: Demo of Traﬃc Flow using a switching Linear Dynamical System

25.7 Exercises

Exercise 239. Consider the setup described in example(106), for which the full SLDS model is given
in SLDSpricemodel.m, following the notation used in demoSLDStraffic.m. Given the data in the vec-
tor v your task is to ﬁt a prediction model to the data. To do so, approximate the ﬁltered distribution
p(h(t), s(t)|v1:t) using a mixture of I = 2 components. The prediction of the price at the next day is then
(25.7.1)

vpred(t + 1) = (cid:104)h1(t) + h2(t)(cid:105)p(h(t)|v1:t)

where p(h(t)|v1:t) =(cid:80)

st p(h(t), st|v1:t).
1. Compute the mean prediction error

mean_abs_pred_error=mean(abs(vpred(2:end)-v(2:end)))

2. Compute the mean naive prediction error

mean_abs_pred_error_naive=mean(abs(v(1:end-1)-v(2:end)))
which corresponds to saying that tomorrow’s price will be the same as today’s.

You might ﬁnd SLDSmargGauss.m of interest.

Exercise 240. The data in ﬁg(25.11) are observed prices from an intermittent mean-reverting process,
contained in meanrev.mat. There are two states S = 2. There is a true (latent) price pt and an observed
price vt (which is plotted). When s = 1, the true underlying price reverts back towards the mean m = 10
with rate r = 0.9. Otherwise the true price follows a random walk:

(cid:26) r(pt−1 − m) + m + ηp

t

pt−1 + ηp
t

pt =

472

st = 1
st = 2

(25.7.2)

DRAFT March 9, 2010

Exercises

Figure 25.11: Data from an intermittent mean-reverting process. See exercise(240).

where

(cid:26)

ηp
t ∼

N (ηp
N (ηp

t 0, 0.0001) st = 1
st = 2
t 0, 0.01)

The observed price vt is related to the unknown price pt by

vt ∼ N (vt pt, 0.001)

(25.7.3)

(25.7.4)

It is known that 95% of the time st+1 is in the same state as at time t and that at time t = 1 either state
of s is equally likely. Also at t = 1, p1 ∼ N (p1 m, 0.1). Based on this information, and using Gaussian
Sum ﬁltering with I = 2 components (use SLDSforward.m), what is the probability at time t = 280 that
the dynamics is following a random walk, p(s280 = 2|v1:280)? Repeat this computation for smoothing
p(s280 = 2|v1:400) based on using Expectation Correction with I = J = 2 components.

DRAFT March 9, 2010

473

0501001502002503003504009.51010.511Exercises

474

DRAFT March 9, 2010

CHAPTER 26

Distributed Computation

26.1 Introduction

How natural organisms process information is a fascinating subject and one of the grand challenges of
science. Whilst this subject is still in its early stages, loosely speaking, there are some generic properties
that most such systems are believed to possess: patterns are stored in a set of neurons; recall of patterns
is robust to noise; transmission between neurons is of a binary nature and is stochastic; information
processing is distributed and highly modular. In this chapter we discuss some of the classical toy models
that have been developed as a test bed for analysing such properties[62, 76, 65, 132]. In particular we
discuss some classical models from a probabilistic viewpoint.

26.2 Stochastic Hopﬁeld Networks

Hopﬁeld networks are models of biological memory in which a pattern is represented by the activity of a
set of V interconnected neurons. The term ‘network’ here refers to the set of neurons, see ﬁg(26.1), and
not the Belief Network representation of distribution of neural states unrolled through time, ﬁg(26.2). At
time t neuron i ﬁres vi(t) = +1 or is quiescent vi(t) = −1 (not ﬁring) depending on the states of the
neurons at the preceding time t − 1. Explicitly, neuron i ﬁres depending on the potential

V(cid:88)

j=1

ai(t) ≡ θi +

wijvj(t)

(26.2.1)

where wij characterizes the eﬃcacy with which neuron j transmits a binary signal to neuron i. The
threshold θi relates to the neuron’s predisposition to ﬁring. Writing the state of the network at time t as
v(t) ≡ (v1(t), . . . , vV (t)), the probability that neuron i ﬁres at time t + 1 is modelled as

p(vi(t + 1) = 1|v(t)) = σβ (ai(t))

(26.2.2)
where σβ(x) = 1/(1+ e−βx) and β controls the level of stochastic behaviour of the neuron. The probability
of being in the quiescent state is given by normalization

p(vi(t + 1) = −1|v(t)) = 1 − p(vi(t + 1) = 1|v(t)) = 1 − σβ (ai(t))

These two rules can be compactly written as

p(vi(t + 1)|v(t)) = σβ (vi(t + 1)ai(t))

which follows directly from 1 − σβ(x) = σβ(−x).

475

(26.2.3)

(26.2.4)

Learning Sequences

v4

v4

v1

v3

v2

Figure 26.1: A depiction of a Hopﬁeld network (for 5 neurons).
The connectivity of the neurons is described by a weight matrix
with elements wij. The graph represents a snapshot of the state
of all neurons at time t which simultaneously update as function
of the network at the previous time t − 1.

In the limit β → ∞, the neuron updates deterministically

vi(t + 1) = sgn (ai(t))

(26.2.5)

In a synchronous Hopﬁeld network, all neurons update independently and simultaneously, so that we can
represent the temporal evolution of the neurons as a dynamic Bayes network, ﬁg(26.2)

p(v(t + 1)|v(t)) =

p(vi(t + 1)|v(t)).

(26.2.6)

V(cid:89)

i=1

Given this toy description of how neurons update, how can we use the network to do interesting things,
for example to store a set of patterns and recall them under some cue. The patterns will be stored in
the weights and in the following section we address how to learn suitable parameters wij and θi to learn
temporal sequences based on a simple local learning rule.

26.3 Learning Sequences

26.3.1 A single sequence

Given a sequence of network states, V = {v(1), . . . , v(T )}, we would like the network to ‘store’ this se-
quence such that it can be recalled under some cue. That is, if the network is initialized in the correct
starting state of the training sequence v(t = 1), the remainder of the training sequence for t > 1 should
be reproduced under the deterministic dynamics equation (26.2.5), without error.

Two classical approaches to learning a temporal sequence are the Hebb1 and Pseudo Inverse rules[132].
In both the standard Hebb and PI cases, the thresholds θi are usually set to zero.

Standard Hebb rule

T−1(cid:88)

t=1

wij =

1
V

vi(t + 1)vj(t)

(26.3.1)

1Donald Hebb, a neurobiologist actually stated[127]

Let us assume that the persistence or repetition of a reverberatory activity (or ‘trace’) tends to induce lasting
cellular changes that add to its stability. . . When an axon of cell A is near enough to excite a cell B and repeatedly
or persistently takes part in ﬁring it, some growth process or metabolic change takes place in one or both cells
such that A’s eﬃciency, as one of the cells ﬁring B, is increased.

This statement is sometimes interpreted to mean that weights are exclusively of the correlation form equation (26.3.1)(see [270]
for a discussion). This can severely limit the performance and introduce adverse storage artifacts including local minima[132].

476

DRAFT March 9, 2010

Learning Sequences

v4(t)

v3(t)

v2(t)

v1(t)

v4(t + 1)

v3(t + 1)

v2(t + 1)

v1(t + 1)

Figure 26.2: A Dynamic Bayesian Network representation of
a Hopﬁeld Network. The network operates by simultaneously
generating a new set of neuron states according to equation
(26.2.6). Equation(26.2.6) deﬁnes a Markov transition matrix,
modelling the transition probability v(t) → v(t+1) and further-
more imposes the constraint that the neurons are conditionally
independent given the previous state of the network.

The Hebb rule can be motivated mathematically by considering

(cid:88)

j

wijvj(t) =

=

1
V

1
V

τ =1

vi(τ + 1)(cid:88)
T−1(cid:88)
vi(t + 1)(cid:88)
T−1(cid:88)

j

j

1
V

τ(cid:54)=t

= vi(t + 1) +

v2
j (t) +

vj(τ)vj(t)

T−1(cid:88)
vi(τ + 1)(cid:88)

1
V

τ(cid:54)=t

j

vi(τ + 1)(cid:88)

j

vj(τ)vj(t)

If the patterns are uncorrelated then the ‘interference’ term

T−1(cid:88)

vi(τ + 1)(cid:88)

τ =1

j

Ω ≡

vj(τ)vj(t)/V

vj(τ)vj(t)

(26.3.2)

(26.3.3)

(26.3.4)

(26.3.5)

will be relatively small. To see this, we ﬁrst note that for randomly drawn patterns, the mean of Ω is zero,
since τ (cid:54)= t and the patterns are randomly ±1. The variance is therefore given by

For j (cid:54)= k, all the terms are independent and contribute zero on average. Therefore

(cid:48))vk(t)(cid:11)
j (t)(cid:11)

1
V 2

τ,τ(cid:48)(cid:54)=t

j,k

1
V 2

(cid:88)

T−1(cid:88)

(cid:88)

T−1(cid:88)

(cid:10)Ω2(cid:11) =
(cid:10)Ω2(cid:11) =
(cid:10)Ω2(cid:11) =

(cid:10)vi(τ + 1)vi(τ
(cid:10)vi(τ + 1)vi(τ
(cid:10)v2
meaning that the sign of(cid:80)

i (τ + 1)v2

(cid:88)

(cid:88)

τ,τ(cid:48)=1

j

1
V 2

τ(cid:54)=t

j

(cid:48) + 1)vj(τ)vj(t)vk(τ

(cid:48) + 1)vj(τ)vj(τ

(cid:48))v2

j (t)(cid:11) = T − 1

V

j (τ)v2

When τ (cid:54)= τ(cid:48) all the terms are independent zero mean and contribute zero. Hence

(26.3.6)

(26.3.7)

(26.3.8)

(26.3.9)

477

Provided that the number of neurons V is signiﬁcantly larger than the length of the sequence, T , then the
average size of the interference will be small. In this case the term vi(t+1) in equation (26.3.4) dominates,
j wijvj(t) will be that of vi(t + 1), and the correct pattern sequence recalled.
The Hebb rule is capable of storing a random (uncorrelated) temporal sequence of length 0.269V time
steps[85]. However, the Hebb rule performs poorly for the case of correlated patterns since interference
from the other patterns becomes signiﬁcant[132, 65].

Pseudo inverse rule
The PI rule ﬁnds a matrix [W]ij = wij that solves the linear equations

wijvj(t) = vi(t + 1),

t = 1, . . . , T − 1

(cid:88)

j

DRAFT March 9, 2010

(cid:16)(cid:80)

(cid:17)

Under this condition sgn
j wijvj(t)
recalled. In matrix notation we require

WV = ˆV

where

[V]it = vi(t),

t = 1, . . . , T − 1,
(cid:17)−1

(cid:16)

W = ˆV

VTV

VT

= sgn (vi(t + 1)) = vi(t + 1) so that patterns will be correctly

Learning Sequences

(cid:105)

(cid:104) ˆV

it

= vi(t + 1),

t = 2, . . . , T

(26.3.11)

(26.3.10)

(26.3.12)

For T < V the problem is under-determined. One solution is given by the pseudo inverse:

The Pseudo Inverse (PI) rule can store any sequence of V linearly independent patterns. Whilst attractive
compared to the standard Hebb in terms of its ability to store longer correlated sequences, this rule suﬀers
from very small basins of attraction for temporally correlated patterns, see ﬁg(26.3).

The maximum likelihood Hebb rule

An alternative to the above classical algorithms is to view this as a problem of pattern storage in the DBN,
equation (26.2.6) [19]. First, we need to clarify what we mean by ‘store’. Given that we initialize the
network in a state v(t = 1), we wish that the remaining sequence will be generated with high probability.
That is, we wish to adjust the network parameters such that the probability

p(v(T ), v(T − 1), . . . , v(2)|v(1))

(26.3.13)

is maximal2. Furthermore, we might hope that the sequence will be recalled with high probability not just
when initialized in the correct state but also for states close (in Hamming distance) to the correct initial
state v(1).

Due to the Markov nature of the dynamics, the conditional likelihood is

p(v(T ), v(T − 1), . . . , v(2)|v(1)) =

p(v(t + 1)|v(t))

(26.3.14)

This is a product of transitions from given states to given states. Since these transition probabilities are
known (26.2.6,26.2.2), the conditional likelihood can be easily evaluated. The sequence log (conditional)
likelihood is

T−1(cid:89)

t=1

p(v(t + 1)|v(t)) =(cid:88)

t

L(w, θ) ≡ log

T−1(cid:88)

V(cid:88)

t=1

i=1

log p(v(t + 1)|v(t)) =

log σβ (vi(t + 1)ai(t)) (26.3.15)

Our task is then to ﬁnd weights w and thresholds θ that maximisise L(w, θ). There is no closed form
solution and the weights therefore need to be determined numerically. This corresponds to a straightfor-
ward computational problem since the log likelihood is a convex function. To show this, we compute the
Hessian (neglecting θ for expositional clarity – this does not aﬀect the conclusions):

T−1(cid:89)

t=1

T−1(cid:88)

t=1

d2L

dwijdwkl

= −β2

where we deﬁned

(vi(t + 1)vj(t)) γi(t)(1 − γi(t))vk(t + 1)vl(t)δik

γi(t) ≡ 1 − σβ (vi(t + 1)ai(t)) .

2Static patterns can also be considered in this framework as a set of patterns that map to each other.

478

DRAFT March 9, 2010

(26.3.16)

(26.3.17)

Learning Sequences

Figure 26.3: Leftmost panel: The temporally highly-
correlated training sequence we desire to store. The other
panels show the temporal evolution of the network af-
ter initialization in the correct starting state but cor-
rupted with 30% noise. During recall, deterministic up-
dates β = ∞ were used. The Maximum Likelihood rule
was trained using 10 batch epochs with η = 0.1. See also
demoHopfield.m

It is straightforward to show that the Hessian is negative deﬁnite (see exercise(244)) and hence the likeli-
hood has a single global maximum. To increase the likelihood of the sequence, we can use a simple method
such as gradient ascent3

wnew

ij = wij + η

dL
dwij

,

θnew
i = θi + η

dL
dθi

where

T−1(cid:88)

t=1

dL
dwij

= β

γi(t)vi(t + 1)vj(t),

T−1(cid:88)

t=1

dL
dθi

= β

(26.3.18)

γi(t)vi(t + 1)

(26.3.19)

The learning rate η is chosen empirically to be suﬃciently small to ensure convergence. The learning
rule equation (26.3.19) can be seen as a modiﬁed Hebb learning rule, the basic Hebb rule being given
when γi(t) ≡ 1. As learning progresses, the γi(t) will typically tend to values close to either 1 or 0, and
hence the learning rule can be seen as asymptotically equivalent to making an update only in the case of
disagreement (ai(t) and vi(t + 1) are of diﬀerent signs).

This batch training procedure can be readily converted to an online in which an update occurs immediately
after the presentation of two consecutive patterns.

Storage capacity of the ML Hebb rule

The ML Hebb rule is capable of storing a sequence of V linearly independent patterns. To see this, we
can form an input-output training set for each neuron i, {(v(t), vi(t + 1)), t = 1, . . . , T − 1}. Each neuron
has an associated weight vector wi ≡ wij, j = 1, . . . , V , which forms a logistic regressor or, in the limit
β = ∞, a perceptron[132]. For perfect recall of the patterns, we therefore need only that the patterns on
the sequence be linearly separable. This will be the case if the patterns are linearly independent, regardless
of the outputs vi(t + 1), t = 1, . . . , T − 1.

Relation to the perceptron rule
In the limit that the activation is large, |ai| (cid:29) 1

(cid:26) 1 vi(t + 1)ai < 0

γi ≈

0 vi(t + 1)ai ≥ 0

(26.3.20)

Provided the activation and desired next output are the same sign, no update is made for neuron i. In
this limit, equation (26.3.19) is called the perceptron rule[132, 80]. For an activation a that is close to the

3Naturally, one can use more sophisticated methods such as the Newton method, or conjugate gradients. In theoretical

neurobiology the emphasis is small gradient style updates since these are deemed to be biologically more plausible.

DRAFT March 9, 2010

479

Training Sequencetimeneuron number1020102030405060708090100Max Likelihood1020102030405060708090100Hebb1020102030405060708090100Pseudo Inverse1020102030405060708090100Learning Sequences

Figure 26.4: The fraction of neurons correct for the ﬁnal state of
the network T = 50 for a 100 neuron Hopﬁeld network trained
to store a length 50 sequence patterns. After initialization in
the correct initial state at t = 1, the Hopﬁeld network is up-
dated deterministically, with a randomly chosen percentage of
the neurons ﬂipped post updating. The correlated sequence of
length T = 50 was produced by ﬂipping with probability 0.5,
20% of the previous state of the network. A fraction correct
value of 1 indicates perfect recall of the ﬁnal state, and a value
of 0.5 indicates a performance no better than random guess-
ing of the ﬁnal state. For maximum likelihood 50 epochs of
training were used with η = 0.02. During recall, deterministic
updates β = ∞ were used. The results presented are averages
over 5000 simulations, resulting in standard errors of the order
of the symbol sizes.

(a)

decision boundary, a small change can lead to a diﬀerent sign of the neural ﬁring. To guard against this
it is common to include a stability criterion

(cid:26) 1 vi(t + 1)ai < M

0 vi(t + 1)ai ≥ M

γi =

(26.3.21)

where M is an empirically chosen positive threshold.

Example 108 (Storing a correlated sequence). In ﬁg(26.3) we consider storage of a highly-correlated
temporal sequence of length T = 20 of 100 neurons using the three learning rules: Hebb, Maximum
Likelihood and Pseudo Inverse. The sequence is chosen to be highly correlated, which constitutes a
diﬃcult learning task. The thresholds θi are set to zero throughout to facilitate comparison. The initial
state of the training sequence, corrupted by 30% noise is presented to the trained networks, and we desire
that the training sequence will be generated from this initial noisy state. Whilst the Hebb rule is operating
in a feasible limit for uncorrelated patterns, the strong correlations in this training sequence entails poor
results. The PI rule is capable of storing a sequence of length 100 yet is not robust to perturbations from
the correct initial state. The Maximum Likelihood rule performs well after a small amount of training.

Stochastic interpretation

By straightforward manipulations, the weight update rule in equation (26.3.19) can be written as

(cid:16)

T−1(cid:88)

t=1

1
2

dL
dwij

=

vi(t + 1) − (cid:104)vi(t + 1)(cid:105)p(vi(t+1)|ai(t))

A stochastic, online learning rule is therefore

∆wij(t) = η (vi(t + 1) − ˜vi(t + 1)) vj(t)

(cid:17)

vj(t)

(26.3.22)

(26.3.23)

where ˜vi(t + 1) is 1 with probability σβ(ai(t)), and −1 otherwise. Provided that the learning rate η is
small, this stochastic updating will approximate the learning rule (26.3.18,26.3.19).

Example 109 (Recalling sequences under perpetual noise). We compare the performance of the
Maximum Likelihood learning rule (with zero thresholds θ) with the standard Hebb, Pseudo Inverse, and
Perceptron rule for learning a single temporal sequence. The network is initialized to a noise corrupted

480

DRAFT March 9, 2010

00.050.10.150.20.250.30.350.40.450.50.50.550.60.650.70.750.80.850.90.951flip probabilityfraction correctsequence length=50Max Likelihoodnoise trained Max Likelihoodperceptron (M=10)perceptron (M=0)hebbpseudo inverseLearning Sequences

(a)

(b)

(b): The
Figure 26.5: (a): Original T = 25 binary video sequence on a set of 81 × 111 = 8991 neurons.
reconstructions beginning from a 20% noise perturbed initial state. Every odd time reconstruction is also
randomly perturbed. Despite the high level of noise the basis of attraction of the pattern sequence is very
broad and the patterns immediately fall back close to the pattern sequence even after a single timestep.

version of the correct initial state v(t = 1) from the training sequence. The dynamics is then run (at
β = ∞) for the same number of steps as the length of the training sequence, and the fraction of bits of
the recalled ﬁnal state which are the same as the training sequence ﬁnal state v(T ) is measured, ﬁg(26.4).
At each stage in the dynamics (except the last), the state of the network was corrupted with noise by
ﬂipping each neuron state with the speciﬁed ﬂip probability. The training sequences are produced by
starting from a random initial state, v(1), and then choosing at random 20% percent of the neurons to
ﬂip, each of the chosen neurons being ﬂipped with probability 0.5, giving a random training sequence
with a high degree of temporal correlation.

The standard Hebb rule performs relatively poorly, particularly for small ﬂip rates, whilst the other
methods perform relatively well, being robust at small ﬂip rates. As the ﬂip rate increases, the pseudo
inverse rule becomes unstable, especially for the longer temporal sequence which places more demands
on the network. The perceptron rule can perform as well as the Maximum Likelihood rule, although
its performance is critically dependent on an appropriate choice of the threshold M. The results for
M = 0 Perceptron training are poor for small ﬂip rates. An advantage of the Maximum Likelihood rule
is that it performs well without the need for ﬁne tuning of parameters. In all cases, batch training was used.

An example for a larger network is given in ﬁg(26.5) which consists of highly correlated sequences. For
such short sequences the basin of attraction is very large and the video sequence can be stored robustly.

26.3.2 Multiple sequences

The previous section detailed how to train a Hopﬁeld network for a single temporal sequence. We now
address the learning a set of sequences {V n, n = 1, . . . , N}. If we assume that the sequences are indepen-
dent, the log likelihood of a set of sequences is the sum of the individual sequences. The gradient is given
by

γn
i (t)vn

i (t + 1)vn

j (t),

γn
i (t)vn

i (t + 1)

(26.3.24)

N(cid:88)

T−1(cid:88)

n=1

t=1

dL
dwij

= β

where

T−1(cid:88)

t=1

= β

dL
dθi

N(cid:88)
i (t) = θi +(cid:88)

n=1

an

wijvn

j (t)

j

γn
i (t) ≡ 1 − σβ (vn

i (t + 1)an

i (t)) ,

(26.3.25)

The log likelihood remains convex since it is the sum of convex functions, so that the standard gradient
based learning algorithms can be used here as well.

DRAFT March 9, 2010

481

Tractable Continuous Latent Variable Models

26.3.3 Boolean networks
The Hopﬁeld network is one particular parameterisation of the table p(vi(t + 1) = 1|v(t)). However,
less constrained parameters may be considered – indeed one could consider the fully unconstrained case in
which each neuron i would have an associated 2V parental states. This exponentially large number of states
is impractical and an interesting restriction is to consider that each neuron has only K parents, so that
each table contains 2K entries. Learning the table parameters by Maximum Likelihood is straightforward
since the log likelihood is a convex function of the table entries. Hence, for given any sequence (or set
of sequences) one may readily ﬁnd parameters that maximise the sequence reconstruction probability.
The Maximum Likelihood method also produces large basins of attraction for the associated stochastic
dynamical system. Such models are of potential interest in Artiﬁcial Life and Random Boolean networks
in which emergent macroscopic behaviour appears from local update rules[158].

26.3.4 Sequence disambiguation

A limitation of ﬁrst order networks deﬁned on visible variables alone (such as the Hopﬁeld network) is
that the observation transition p(vt+1|vt = v) is the same every time the joint state v is encountered.
This means that if the sequence contains a subsequence such as a, b, a, c this cannot be recalled with
high probability since a transitions to diﬀerent states, depending on time. Whilst one could attempt
to resolve this sequence disambiguation problem using a higher order Markov model to account for a
longer temporal context, we would lose biological plausibility. Using latent variables is an alternative
way to sequence disambiguation. In the Hopﬁeld model the recall capacity can be increased using latent
variables by make a sequencing in the joint latent-visible space that is linearly independent, even if the
visible variable sequence alone is not. In section(26.4) we discuss a general method that extends dynamic
Bayes networks deﬁned on visible variables alone, such as the Hopﬁeld network, to include continuous
non-linearly updating latent variables, without requiring additional approximations.

26.4 Tractable Continuous Latent Variable Models

A dynamic Bayes network with latent variables takes the form

T−1(cid:89)

t=1

p(v(1 : T ), h(1 : T )) = p(v(1))p(h(1)|v(1))

p(v(t+1)|v(t), h(t))p(h(t+1)|v(t), v(t+1), h(t)) (26.4.1)

As we saw in chapter(23), provided all hidden variables are discrete, inference in these models is straight-
forward. However, in many physical systems it is more natural to assume continuous h(t). In chapter(24)
we saw that one such tractable continuous h(t) model is given by linear Gaussian transitions and emissions
- the LDS. Whilst this is useful, we cannot represent non-linear changes in the latent process using an LDS
alone. The Switching LDS of chapter(25) is able to model non-linear continuous dynamics (via switching)
although we saw that this leads to computational diﬃculties. For computational reasons we therefore seem
limited to either purely discrete h (with no limitation on the discrete transitions) or purely continuous h
(but be forced to use simple linear dynamics). Is there a way to have a continuous state with non-linear
dynamics for which posterior inference remains tractable? The answer is yes, provided that we assume
the hidden transitions are deterministic[14]. When conditioned on the visible variables, this renders the
hidden unit distribution trivial. This allows the consideration of rich non-linear dynamics in the hidden
space if required.

26.4.1 Deterministic latent variables

Consider a Belief Network deﬁned on a sequence of visible variables v1:T . To enrich the model we include
additional continuous latent variables h1:T that will follow a non-linear Markov transition. To retain
tractability of inference, we constrain the latent dynamics to be deterministic, described by

p(h(t + 1)|v(t + 1), v(t), h(t)) = δ (h(t + 1) − f (v(t + 1), v(t), h(t), θh))

(26.4.2)

Here δ(x) represents the Dirac delta function for continuous hidden variables. The (possibly non-linear)
function f parameterises the CPT. Whilst the restriction to deterministic CPTs appears severe, the model

482

DRAFT March 9, 2010

Tractable Continuous Latent Variable Models

h(1)

h(2)

v(1)

v(2)

(a)

h(t)

v(t)

h(1)

h(2)

h(t)

v(1)

(b)

v(t)

v(2)

(c)

Figure 26.6: (a): A ﬁrst order Dynamic Bayesian Network with deterministic hidden CPTs (represented
(b):
by diamonds) that is, the hidden node is certainly in a single state, determined by its parents.
Conditioning on the visible variables forms a directed chain in the hidden space which is deterministic.
Hidden unit inference can be achieved by forward propagation alone. (c): Integrating out hidden variables
gives a cascade style directed visible graph which so that each v(t) depends on all v(1 : t − 1).

retains some attractive features: The marginal p(v(1 : T )) is non-Markovian, coupling all the variables in
the sequence, see ﬁg(26.6c), whilst hidden unit inference p(h(1 : T )|v(1 : T )) is deterministic, as illustrated
in ﬁg(26.6b).

The adjustable parameters of the hidden and visible CPTs are represented by θh and θv respectively. For
learning, the log likelihood of a single training sequence V is

To maximise the log likelihood using gradient techniques we need to the derivatives with respect to the
model parameters. These can be calculated as follows:

f(t) ≡ f(v(t), v(t − 1), h(t − 1), θh)

Hence the derivatives can be calculated by deterministic forward propagation of errors alone. The case of
training multiple independently generated sequences V n, n = 1, . . . , N is a straightforward extension.
26.4.2 An augmented Hopﬁeld network

To make the deterministic latent variable model more explicit, we consider the case of continuous hidden
units and discrete, binary visible units, vi(t) ∈ {−1, 1}. In particular, we restrict attention to the Hopﬁeld
model augmented with latent variables that have a simple linear dynamics (see exercise(245) for a non-
linear extension):

h(t + 1) = 2σ(Ah(t) + Bv(t)) − 1 deterministic latent transition

σ (vi(t + 1)φi(t)) ,

φ(t) ≡ Ch(t) + Dv(t)

V(cid:89)

i=1

p(v(t + 1)|v(t), h(t)) =

DRAFT March 9, 2010

T−1(cid:88)

t=1

T−1(cid:88)

t=1

L(θv, θh|V) = log p(v(1)|θv) +

log p(v(t + 1)|v(t), h(t), θv)

where the hidden unit values are calculated recursively using

h(t + 1) = f (v(t + 1), v(t), h(t), θh)

log p(v(1)|θv) +

∂
∂θv

log p(v(t + 1)|v(t), h(t), θv)

= ∂
∂θv

T−1(cid:88)

=

dL
dθv

dL
dθh

dh(t)
dθh

where

∂

t=1

∂h(t)
= ∂f(t)
∂θh

log p(v(t + 1)|v(t), h(t), θv) dh(t)
+ ∂f(t)
∂h(t − 1)

dh(t − 1)

dθh

dθh

(26.4.3)

(26.4.4)

(26.4.5)

(26.4.6)

(26.4.7)

(26.4.8)

(26.4.9)

(26.4.10)

483

This model generalises a recurrent stochastic heteroassociative Hopﬁeld network[132] to include determin-
istic hidden units dependent on past network states. The parameters of the model are A, B, C, D. For
gradient based training we require the derivatives with respect to each of these parameters. The derivative
of the log likelihood for a generic parameter θ is

This gives (where all indices are summed over the dimensions of the quantities they relate to):

Neural Models

(26.4.11)

(26.4.12)

(26.4.13)

(26.4.14)

(26.4.15)

(26.4.16)

(26.4.17)

(26.4.18)

 vi(t + 1)

φi(t)

d
dθ

where

νi(t) ≡

i

φi(t)

νi(t) d
dθ

L =(cid:88)
1 − σ(vi(t + 1))(cid:88)
φi(t) =(cid:88)
φi(t) =(cid:88)

dAαβ

Cij

Cij

d

d

j

d

d

j

j

hj(t)

hj(t)

dBαβ

dAαβ

dBαβ

d

dCαβ

d

dDαβ

d

dAαβ

d

φi(t) = δiαhβ(t)

φi(t) = δiαvβ(t)

hi(t + 1) = 2σ

(cid:48)

i(t + 1)(cid:88)
i(t + 1)(cid:88)

(cid:48)

j

hi(t + 1) = 2σ

dBαβ
(cid:48)
i(t) ≡ σ(hi(t)) (1 − σ(hi(t)))

j

Aij

d

dAαβ

Aij

d

dBαβ

hj(t) + δiαhβ(t)

hj(t) + δiαvβ(t)

σ

(26.4.19)
If we assume that h(1) is a given ﬁxed value (say 0), we can compute the derivatives recursively by forward
propagation. Gradient based training for this augmented Hopﬁeld network is therefore straightforward
to implement. This model extends the power of the original Hopﬁeld model, being capable of resolving
ambiguous transitions in sequences such as a, b, a, c, see example(110) and demoHopfieldLatent.m. In
terms of a dynamic system, the learned network is an attractor with the training sequence as a stable point
and demonstrates that such models are capable of learning attractor recurrent networks more powerful
than those without hidden units.

Example 110 (Sequence disambiguation). The sequence in ﬁg(26.7a) contains repeated patterns and
therefore cannot be reliably recalled with a ﬁrst order model containing visible variables alone. To deal with
this we consider a Hopﬁeld network with 3 visible units and 7 additional hidden units with deterministic
(linear) latent dynamics. The model was trained with gradient ascent to maximise the likelihood of the
binary sequence in ﬁg(26.7a). As shown in ﬁg(26.7b), the learned network is capable of recalling the
sequence correctly, even when initialised in an incorrect state, having no diﬃculty with the fact that the
sequence transitions are ambiguous.

26.5 Neural Models

The tractable deterministic latent variable model introduced in section(26.4) presents an opportunity
to extend models such as the Hopﬁeld network to include more biologically realistic processes without
losing computational tractability. First we discuss a general framework for learning in a class of neural
models[15, 223], this being a special case of the deterministic latent variable models[14] and a generalisation
of the spike-response model of theoretical neurobiology[108].

484

DRAFT March 9, 2010

Neural Models

(a)

(b)

Figure 26.7: (a): The training sequence consists of a random set of vectors (V = 3)
(b): The reconstruction using H = 7 hidden units. The
over T = 10 time steps.
initial state v(t = 1) for the recalled sequence was set to the correct initial training
value albeit with one of the values ﬂipped. Note that the method is capable of
sequence disambiguation in the sense that the transitions of the form a, b, . . . , a, c
can be recalled.

26.5.1 Stochastically spiking neurons

We assume that neuron i ﬁres depending on the membrane potential ai(t) through

p(vi(t + 1) = 1|v(t), h(t)) = p(vi(t + 1) = 1|ai(t))

To be speciﬁc, we take throughout

p(vi(t + 1) = 1|ai(t)) = σ (ai(t))

Here we to deﬁne the quiescent state as vi(t + 1) = 0, so that

p(vi(t + 1)|ai(t)) = σ ((2vi(t + 1) − 1)ai(t))

(26.5.1)

(26.5.2)

(26.5.3)

The choice of the sigmoid function σ(x) is not fundamental and is chosen merely for analytical convenience.
The log-likelihood of a sequence of visible states V is

L =

log σ ((2vi(t + 1) − 1)ai(t))

and the gradient of the log-likelihood is then

dL
dwij

=

(vi(t + 1) − σ(ai(t))) dai(t)

dwij

(26.5.4)

(26.5.5)

T−1(cid:88)

V(cid:88)

t=1

i=1

T−1(cid:88)

t=1

V(cid:88)

j=1

where we used the fact that vi ∈ {0, 1}. Here wij are parameters of the membrane potential (see below).
We take equation (26.5.5) as common in the following models in which the membrane potential ai(t) is
described with increasing sophistication.

26.5.2 Hopﬁeld membrane potential

As a ﬁrst step, we show how the Hopﬁeld network training, as described in section(26.3.1), can be recovered
as a special case of the above framework. The Hopﬁeld membrane potential is

ai(t) ≡

wijvj(t) − bi

(26.5.6)

where wij characterizes the eﬃcacy of information transmission from neuron j to neuron i, and bi is a
threshold. Applying the Maximum Likelihood framework to this model to learn a temporal sequence V
by adjustment of the parameters wij (the bi are ﬁxed for simplicity), we obtain the (batch) learning rule
(using dai/dwij = vj(t) in equation (26.5.4))

T−1(cid:88)

t=1

wnew

ij = wij + η

dL
dwij

,

dL
dwij

=

(vi(t + 1) − σ(ai(t))) vj(t),

(26.5.7)

where the learning rate η is chosen empirically to be suﬃciently small to ensure convergence. Equation(26.5.7)
matches equation (26.3.19) (which uses the ±1 encoding).
DRAFT March 9, 2010

485

Neural Models

Figure 26.8: Learning with depression : U = 0.5,
τ = 5, δt = 1, η = 0.25. Despite the apparent com-
plexity of the dynamics, learning appropriate neu-
ral connection weights is straightforward using Max-
imum Likelihood. The reconstruction using the stan-
dard Hebb rule by contrast is poor[15].

26.5.3 Dynamic synapses

In more realistic synaptic models, neurotransmitter generation depends on a ﬁnite rate of cell subcom-
ponent production, and the quantity of vesicles released is aﬀected by the history of ﬁring[1]. Loosely
speaking, when a neuron ﬁres it releases a chemical substance from a local reservoir, this reservoir being
reﬁlled at a lower rate than the neuron can ﬁre. If the neuron ﬁres continually, its ability to continue
ﬁring weakens since the reservoir of release chemical is depleted. This can be accounted for by using a
depression mechanism that aﬀects the membrane potential

for depression factors xj(t) ∈ [0, 1]. A simple dynamics for these depression factors is[280]

ai(t) = wijxj(t)vj(t)

xj(t + 1) = xj(t) + δt

(cid:18)1 − xj(t)

τ

(cid:19)
− U xj(t)vj(t)

(26.5.8)

(26.5.9)

where δt, τ, and U represent time scales, recovery times and spiking eﬀect parameters respectively. Note
that these depression factor dynamics are exactly of the form of deterministic hidden variables.

It is straightforward to include these dynamic synapses in a principled way using the Maximum Likelihood
learning framework. For the Hopﬁeld potential, the learning dynamics is simply given by equations
(26.5.5,26.5.9), with

dai(t)
dwij

= xj(t)vj(t)

(26.5.10)

Example 111 (Learning with depression). In ﬁg(26.4) we demonstrate learning a random temporal
sequence of 20 time steps for an assembly of 50 neurons with dynamic depressive synapses. After learning
wij the trained network is initialised in the ﬁrst state of the training sequence. The remaining states of
the sequence were then correctly recalled by iteration of the learned model. The corresponding generated
factors xi(t) are also plotted. For comparison, we plot the results of using the dynamics having set the
wij using the temporal Hebb rule, equation (26.3.1). The poor performance of the correlation based Hebb
rule demonstrates the necessity, in general, to couple a dynamical system with an appropriate learning
mechanism.

26.5.4 Leaky integrate and ﬁre models

Leaky integrate and ﬁre models move a step further towards biological realism in which the membrane
potential increments if it receives an excitatory stimulus (wij > 0), and decrements if it receives an
inhibitory stimulus (wij < 0). After ﬁring, the membrane potential is reset to a low value below the ﬁring
threshold, and thereafter steadily increases to a resting level (see for example [62, 108]). A model that
incorporates such eﬀects is

αai(t − 1) +(cid:88)

j

 (1 − vi(t − 1)) + vi(t − 1)θf ired

wijvj(t) + θrest (1 − α)

ai(t) =

486

(26.5.11)

DRAFT March 9, 2010

neuron numberOriginalt10205101520253035404550Reconstructiont10205101520253035404550x valuest10205101520253035404550Hebb Reconstructiont10205101520253035404550Exercises

Since vi ∈ {0, 1}, if neuron i ﬁres at time t − 1 the potential is reset to θf ired at time t. Similarly, with no
synaptic input, the potential equilibrates to θrest with time constant −1/ log α[15].
Despite the increase in complexity of the membrane potential over the Hopﬁeld case, deriving appropriate
learning dynamics for this new system is straightforward since, as before, the hidden variables (here the
membrane potentials) update in a deterministic fashion. The potential derivatives are

(cid:18)

(cid:19)

dai(t)
dwij

= (1 − vi(t − 1))

α

dai(t − 1)

dwij

+ vj(t)

(26.5.12)

By initialising the derivative dai(t=1)
= 0, equations (26.5.5,26.5.11,26.5.12) deﬁne a ﬁrst order recursion
for the gradient which can be used to adapt wij in the usual manner wij ← wij + ηdL/dwij. We could
also apply synaptic dynamics to this case by replacing the term vj(t) in equation (26.5.12) by xj(t)vj(t).

dwij

Although a detailed discussion of the properties of the neuronal responses for networks trained in this way
is beyond the scope of these notes, an interesting consequence of the learning rule equation (26.5.12) is a
spike time dependent learning window in qualitative agreement with experimental results[223, 186].

In summary, provided one deals with deterministic latent dynamics, essentially arbitrarily complex spatio-
temporal patterns may potentially be learned, and generated under cued retrieval, for very complex neural
dynamics. The spike-response model [108] can be seen as a special case of the deterministic latent variable
model in which the latent variables have been explicitly integrated out.

26.6 Code

demoHopfield.m: Demo of Hopﬁeld sequence learning
HebbML.m: Gradient ascent training of a set of sequences using Max Likelihood
HopfieldHiddenNL.m: Hopﬁeld network with additional non-linear latent variables
demoHopfieldLatent.m: demo of Hopﬁeld net with deterministic latent variables
HopfieldHiddenLikNL.m: Hopﬁeld Net with hidden variables sequence likelihood

T−1(cid:88)

26.7 Exercises
Exercise 241. Consider a very large Hopﬁeld network V (cid:29) 1 used to store a single temporal sequence of
length v(1 : T ), T (cid:28) V . In this case the weight matrix w may be diﬃcult to store. Explain how to justify
the assumption

wij =

ui(t)vi(t + 1)vj(t)

(26.7.1)

t=1

where ui(t) are the dual parameters and derive an update rule for the dual parameters u.

Exercise 242. A Hopﬁeld network is used to store a raw uncompressed binary video sequence. Each image
in the sequence contains 106 binary pixels. At a rate of 10 frames per second, how many hours of video
can 106 neurons store?

Exercise 243. Derive the update equation (26.3.22).

Exercise 244. Show that the Hessian equation (26.3.16) is negative deﬁnite. That is

(cid:88)

i,j,k,l

xijxkl

d2L

dwijdwkl ≤ 0

for any x (cid:54)= 0.
DRAFT March 9, 2010

(26.7.2)

487

Exercise 245. For the augmented Hopﬁeld network of section(26.4.2),with latent dynamics

hi(t + 1) = 2σ

Aijhj(t) + Bijvj(t)

(cid:88)

j

 − 1

Exercises

(26.7.3)

derive the derivative recursions described in section(26.4.2).

488

DRAFT March 9, 2010

Part V

Approximate Inference

489

CHAPTER 27

Sampling

27.1 Introduction

Sampling concerns drawing realisations x1, . . . , xL of a variable x from a distribution p(x). For a discrete
variable x, in the limit of a large number of samples, the fraction of samples in state x tends to p(x = x).
That is,

xl = x

= p(x = x)

(27.1.1)

(cid:82)
In the continuous case, one can consider a small region ∆ such that the probability that the samples
occupy ∆ tends to the integral of p(x) over ∆. In other words, the relative frequency x ∈ ∆ tends to
x∈∆ p(x). Given a ﬁnite set of samples, one can then approximate expectations using

(cid:105)

(cid:69)

L(cid:88)

l=1

L(cid:88)

I(cid:104)

l=1

lim
L→∞

1
L

(cid:104)f(x)(cid:105)p(x) ≈

1
L

=

(cid:68) ˆf
(cid:69)
(cid:68) ˆf 2(cid:69)

−

(cid:68)

1
L

L(cid:88)
(cid:69)2
(cid:68) ˆf

l=1

f(xl) ≡ ˆf

(27.1.2)

This approximation holds for both discrete and continuous variables. Provided the samples are indeed
from p(x), then the average of the approximation is

The variance of the approximation is

f(xl)

p(xl=xl)

= (cid:104)f(x)(cid:105)p(x)

(cid:16)(cid:10)f 2(x)(cid:11)

=

1
L

p(x) − (cid:104)f(x)(cid:105)2

p(x)

(cid:17)

(27.1.3)

(27.1.4)

Hence the mean of the approximation is the exact mean of f and the variance of the approximation scales
inversely with the number of samples.
In principle, therefore, provided the samples are independently
drawn from p(x), only a small number of samples is required to accurately estimate this mean. Impor-
tantly, this result is independent of the dimension of x. However, the critical diﬃculty is in actually
generating independent samples from p(x). Drawing samples from high-dimensional distributions is gen-
erally diﬃcult and few guarantees exist to ensure that in a practical timeframe the samples produced are
representative enough such that expectations can be approximated accurately. There are many diﬀerent
sampling algorithms, all of which ‘work in principle’, but each ‘working in practice’ only when the distri-
bution satisﬁes particular properties[112]. Before we develop schemes for multi-variate distributions, we
consider the univariate case.

491

Introduction

1

×

2

3

Figure 27.1: A representation of the discrete distri-
bution equation (27.1.5). The unit interval from 0 to
1 is partitioned in parts whose lengths are equal to
0.6, 0.1 and 0.3.

27.1.1 Univariate sampling

In the following, we assume that a random number generator exists which is able to produce a value
uniformly at random from the unit interval [0, 1]. We will make use of this uniform random number
generator to draw samples from non-uniform distributions.

Discrete case
Consider the one dimensional discrete distribution p(x) where dom(x) = {1, 2, 3}, with

 0.6 x = 1

0.1 x = 2
0.3 x = 3

p(x) =

(27.1.5)

This represents a partitioning of the unit interval [0, 1] in which the interval [0, 0.6] has been labelled as
state 1, [0.6, 0.7] as state 2, and [0.7, 1.0] as state 3, ﬁg(27.1). If we were to drop a point × anywhere at
random, uniformly in the interval [0, 1], the chance that × would land in interval 1 is 0.6, and the chance
that it would be in interval 2 is 0.1 and similarly, for interval 3, 0.3. This therefore deﬁnes for us a valid
sampling procedure for discrete one-dimensional distributions as described in algorithm(24).

In our example, we have (c0, c1, c2, c3) = (0, 0.6, 0.7, 1). We then draw a sample uniformly from [0, 1], say
u = 0.66. Then the sampled state would be state 2, since this is in the interval (c1, c2].

Sampling from a discrete univariate distribution is straightforward since computing the cumulant takes
only O (K) steps for a K state discrete variable.

Continuous case

In the following we assume that a method exists to generate samples from the uniform distribution
Intuitively, the generalisation of the discrete case to the continuous case is clear. First
U (x| [0, 1]).
we calculate the cumulant density function

C(y) =

p(x)dx

(27.1.6)

(cid:90) y

−∞

Then we sample u uniformly from [0, 1], and obtain the corresponding sample x by solving C(x) = u ⇒
x = C−1(u). Formally, therefore, sampling of a continuous univariate variable is straightforward provided
we can compute the integral of the corresponding probability density function.

Algorithm 24 Sampling from a univariate discrete distribution p with K states.

1: Label the K states as i = 1, . . . , K, with associated probabilities pi.
2: Calculate the cumulant

ci =(cid:88)

j≤i

pj

and set c0 = 0.

3: Draw a value u uniformly at random from the unit interval [0, 1].
4: Find that i for which ci−1 < u ≤ ci.
5: Return state i as a sample from p.

492

DRAFT March 9, 2010

Introduction

(a)

(b)

Figure 27.2: Histograms of the samples from the three
(a): 20
state distribution p(x) = {0.6, 0.1, 0.3}.
samples. (b): 1000 samples. As the number of sam-
ples increases, the relative frequency of the samples
tends to the distribution p(x).

For special distributions, such as Gaussians, numerically eﬃcient alternative procedures exist, usually
based on co-ordinate transformations, see exercise(246).

27.1.2 Multi-variate sampling

One way to generalise the one dimensional discrete case to a higher dimensional distribution p(x1, . . . , xn)
is to translate this into an equivalent one-dimensional distribution. This can be achieved by enumerating
all the possible joint states (x1, . . . , xn), giving each a unique integer i from 1 to the total number of states,
and constructing a univariate distribution with probability p(i) = p(x) for i corresponding to the multivari-
ate state x. This then transforms the multi-dimensional distribution into an equivalent one-dimensional
distribution, and sampling can be achieved as before. In general, of course, this procedure is impractical
since the number of states will grow exponentially with the number of variables x1, . . . , xn.

An alternative exact approach would be to capitalise on the relation

p(x1, x2) = p(x2|x1)p(x1)

(27.1.7)

We can sample from the joint distribution p(x1, x2) by ﬁrst sampling a state for x1 from the one-dimensional
p(x1), and then, with x1 clamped to this state, sampling a state for x2 from the one-dimensional p(x2|x1).
It is clear how to generalise this to more variables by using a cascade decomposition:

p(x1, . . . , xn) = p(xn|xn−1, . . . , x1)p(xn−1|xn−2, . . . , x1) . . . , p(x2|x1)p(x1)

(27.1.8)
However, in order to apply this technique, we need to know the conditionals p(xi|xi−1, . . . , x1). Unless
these are explicitly given we need to compute these from the joint distribution p(x1, . . . , xn). Such con-
ditionals will, in general, require the summation over an exponential number of states and, except for
small n, generally also be impractical. For Belief Networks, however, by construction the conditionals are
speciﬁed so that this technique becomes practical, as we discuss in section(27.2).

Drawing samples from a multi-variate distribution is in general therefore a complex task and one seeks to
exploit any structural properties of the distribution to make this computationally more feasible. A common
approach is to seek to transform the distribution into a product of lower dimensional distributions. A classic
example of this is sampling from a multi-variate Gaussian, which can be reduced to sampling from a set
of univariate Gaussians by a suitable coordinate transformation, as discussed in example(112).

Example 112 (Sampling from a multi-variate Gaussian). Our interest is to draw a sample from the
multi-variate Gaussian p(x) = N (x m, S). For a general covariance matrix S, p(x) does not factorise into
a product of univariate distributions. However, consider the transformation

where C is chosen so that CCT = S. Since this is a linear transformation, y is also Gaussian distributed
with mean

(27.1.9)

y = C−1 (x − m)
(cid:104)y(cid:105) =(cid:10)C−1 (x − m)(cid:11)
= C−1(cid:68)
yyT(cid:69)
(cid:68)

p(x)

(cid:17)
(cid:104)x(cid:105)p(x) − m

p(x) = C−1(cid:16)
(x − m) (x − m)T(cid:69)

Since the mean of y is zero, the covariance is given by

= C−1 (m − m) = 0

(27.1.10)

C−T = C−1SC−T = C−1CCTC−T = I

(27.1.11)

p(x)

DRAFT March 9, 2010

493

123024681230200400600x1

x2

x3

x4

x5

x6

Ancestral Sampling

Figure 27.3: An ancestral Belief Network without any
evidential variables. To sample from this distribu-
tion, we draw a sample from variable 1, and then
variables, 2,. . . ,6 in order.

Hence

p(y) = N (y 0, I) =(cid:89)

i

N (yi 0, 1)

(27.1.12)

Hence a sample from y can be obtained by independently drawing a sample from each of the univariate
zero mean unit variance Gaussians. Given a sample for y, a sample for x is obtained using

x = Cy + m

(27.1.13)

Drawing samples from a univariate Gaussian is a well-studied topic, with a popular method being the
Box-Muller technique, exercise(246).

27.2 Ancestral Sampling

Belief Networks take the general form:

p(x) =(cid:89)

i

p(xi|pa (xi))

(27.2.1)

where each of the conditional distributions p(xi|pa (xi)) is known. Provided that no variables are evidential,
we can sample from this distribution in a straightforward manner. For convenience, we ﬁrst rename the
variable indices so that parent variables always come before their children (ancestral ordering), for example
(see ﬁg(27.3))

p(x1, . . . , x6) = p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

(27.2.2)

One can sample ﬁrst from those nodes that do not have any parents (here, x1 and x2). Given these values,
one can then sample x3, and then x4 and x5 and ﬁnally x6. Despite the presence of loops in the graph, such
a forward sampling procedure is straightforward. This procedure holds for both discrete and continuous
variables.

If one attempted to carry out an exact inference scheme using moralisation and triangulation, in more
complex multiply connected graphs, cliques can become very large. However, regardless of the loop struc-
ture, ancestral sampling is straightforward.

Ancestral or ‘forward’ sampling is a case of perfect sampling (also termed exact sampling) since each
sample is indeed drawn from the required distribution. This is in contrast to Markov Chain Monte Carlo
methods sections(27.3,27.4) for which the samples are from p(x) only in the limit of a large number of
iterations.

27.2.1 Dealing with evidence

How can we sample from a distribution in which certain variables xE are clamped to evidential states?
Formally we need to sample from

p(x\E|xE) =

p(x\E , xE)

p(xE)

494

(27.2.3)

DRAFT March 9, 2010

Gibbs Sampling

x1

x2

x3

x4

x5

x6

x7

Figure 27.4: The Markov blanket of x4. To draw a sample from p(x4|x\4)
we clamp x1, x2, x3, x5, x7 into their evidential states and draw a sam-
ple from p(x4|x1, x2)p(x6|x3, x4)p(x7|x4, x5)/Z where Z is a normalisation
constant.

If an evidential variable xi has no parents, then one can simply set the variable into this state and con-
tinue forward sampling as before. For example, to compute a sample from p(x1, x3, x4, x5, x6|x2) deﬁned
in equation (27.2.2), one simply clamps the x2 into its evidential state and continues forward sampling.
The reason this is straightforward is that conditioning on x2 merely deﬁnes a new distribution on a subset
of the variables, for which the distribution is immediately known.

On the other hand, consider sampling from p(x1, x2, x3, x4, x5|x6). Using Bayes’ rule, we have

(cid:80)

p(x1, x2, x3, x4, x5|x6) =

p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

x1,x2,x3,x4,x5 p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

(27.2.4)

The conditioning on x6 means that the structure of the distribution on the non-evidential variables changes
– for example x4 and x5 become coupled. One could attempt to work out an equivalent new forward sam-
pling structure, (see exercise(247)) although generally this will be as complex as running an exact inference
approach.

probability that a sample from p(x) will be consistent with the evidence is roughly O (1/(cid:81)

An alternative is to proceed with forward sampling from the non-evidential distribution, and then discard
any samples which do not match the evidential states. This is generally not recommended since the
i ) where
i is the number of states of evidential variable i. In principle one can ease this eﬀect by discarding
dim xe
the sample as soon as any variable state is inconsistent with the evidence. Nevertheless, the number of
re-starts required to obtain a valid sample would on average be very large.

i dim xe

27.2.2 Perfect sampling for a Markov network

For a Markov network we can draw exact samples by forming an equivalent directed representation of
the graph, see section(6.8), and subsequently using ancestral sampling on this directed graph. This is
achieved by ﬁrst choosing a root clique and then consistently orienting edges away from this clique. An
exact sample can then be drawn from the Markov network by ﬁrst sampling from the root clique and then
recursively from the children of this clique. See potsample.m, JTsample.m and demoJTreeSample.m.

27.3 Gibbs Sampling

The ineﬃciency of methods such as ancestral sampling under evidence, motivates alternative techniques.
An important and widespread technique is Gibbs sampling which is generally straightforward to implement.

No evidence

Assume we have a joint sample state x1 from the multivariate distribution p(x). We then consider a
particular variable, xi. Using Bayes’ rule we may write

p(x) = p(xi|x1, . . . , xi−1, xi+1, . . . , xn)p(x1, . . . , xi−1, xi+1, . . . , xn)

(27.3.1)

Given a joint initial state x1, from which we can read oﬀ the ‘parental’ state x1
can then draw a sample x2

i from

1, . . . , x1

i−1, x1

i+1, . . . , x1

n, we

p(xi|x1

1, . . . , x1

i−1, x1

i+1, . . . , x1

n) ≡ p(xi|x\i)

DRAFT March 9, 2010

(27.3.2)

495

Gibbs Sampling

x1

x4

x2

x1

x2

x3

x4

(a)

(b)

Figure 27.5: (a): A toy ‘intractable’ distribution.
Gibbs sampling by conditioning on all variables ex-
cept one leads to a simple univariate conditional dis-
(b): Conditioning on x3 yields a new
tribution.
distribution that is singly-connected, for which exact
sampling is straightforward.

(cid:1). One then selects another variable xj

which only xi has been updated) x2 =(cid:0)x1

1, . . . , x1

i−1, x2

i , x1

i+1, . . . , x1
n

We assume this distribution is easy to sample from since it is univariate. We call this new joint sample (in

to sample and, by continuing this procedure, generates a set x1, . . . , xL of samples in which each xl+1 diﬀers
from xl in only a single component. The reason this is valid sampling scheme is outlined in section(27.3.1).

For a Belief Network, the conditional p(xi|x\i) is deﬁned by the Markov blanket of xi:

p(xj|pa (xj))

(27.3.3)

p(xi|x\i) =

1
Z

p(xi|pa (xi)) (cid:89)

j∈ch(i)

Z =(cid:88)

p(xi|pa (xi)) (cid:89)

xi

j∈ch(i)

see for example, ﬁg(27.4). This means that only the parent and parents of children states are required in
forming the sample update. The normalisation constant for this univariate distribution is straightforward
to work out from the requirement:

p(xj|pa (xj))

(27.3.4)

In the case of a continuous variable xi the summation above is replaced with integration.

Evidence

Evidence is readily dealt with by clamping for all samples the evidential variables into their evidential
states. There is also no need to sample for these variables, since their states are known.

27.3.1 Gibbs sampling as a Markov chain

In Gibbs sampling we have a sample of the joint variables xl at stage l. Based on this we produce a new
joint sample xl+1. This means that we can write Gibbs sampling as a procedure that draws from

xl+1 ∼ q(xl+1|xl)

(27.3.5)
for some distribution q(xl+1|xl). If we choose the variable to update, xi, at random from a distribution
q(i), then Gibbs sampling corresponds to drawing samples using the Markov transition

q(xl+1|xl, i)q(i),

q(xl+1|xl, i) = p(xl+1

i

δ

xl+1
j

, xl
j

(27.3.6)

(cid:16)

|xl\i)(cid:89)

j(cid:54)=i

(cid:17)

with q(i) > 0. Our interest is to show that the stationary distribution of q(x(cid:48)
out assuming x is continuous – the discrete case is analogous:

(cid:90)
(cid:90)
(cid:90)

q(i)

q(i)

q(i)

xi

q(i)p(x

(cid:48)

δ(cid:0)x

q(x

(cid:89)

j(cid:54)=i

x

x

|x\i)p(x)

(cid:48)
j, xj

(cid:1) p(x
\i) =(cid:88)

(cid:48)

(cid:48)
(cid:48)
\i)p(xi, x
\i)

p(x

(cid:48)
i|x
(cid:48)
(cid:48)
\i)p(x
i|x

(cid:48)
i|x\i)p(xi, x\i)

q(i)p(x

(cid:48)) = p(x
(cid:48))

|x) is p(x). We carry this

(27.3.7)

(27.3.8)

(27.3.9)

(27.3.10)

i

q(xl+1|xl) =(cid:88)
(cid:90)
|x)p(x) =(cid:88)
=(cid:88)
=(cid:88)
=(cid:88)

(cid:48)
q(x

x

i

i

i

i

496

i

DRAFT March 9, 2010

Gibbs Sampling

Figure 27.6: A two dimensional distribution for which Gibbs sampling fails.
The distribution has mass only in the shaded quadrants. Gibbs sampling
proceeds from the lth sample state (xl
1),
2) and then sampling from p(x2|xl
which we write (xl+1
, xl+1
1. One then continues with a
), etc. If we start in the lower left quadrant
sample from p(x1|x2 = xl+1
and proceed this way, the upper right region is never explored.

1, xl
) where xl+1
1 = xl

2

1

2

Hence, as long as we continue to draw samples according to the distribution q(x(cid:48)
|x), in the limit of a large
number of samples we will ultimately tend to draw samples from p(x). Any distribution q(i) > 0 suﬃces
so visiting all variables equally often is also a valid choice. Technically, we also require that q(x(cid:48)
|x) has
p(x) as its equilibrium distribution, so that no matter in which state we start, we always converge to p(x);
see ﬁg(27.6) for a discussion of this issue.

27.3.2 Structured Gibbs sampling

One can extend Gibbs sampling by using conditioning to reveal a tractable distribution on the remaining
variables. For example, consider the simple distribution, ﬁg(27.5a)

p(x1, x2, x3, x4) = φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)φ(x1, x3)

(27.3.11)

In single-site Gibbs sampling we would condition on three of the four variables, and sample from the
remaining variable. For example

p(x1|x2, x3, x4) ∝ φ(x1, x2)φ(x4, x1)φ(x1, x3)

(27.3.12)

However, we may use more limited conditioning as long as the conditioned distribution is easy to sample
from. In the case of equation (27.3.11) we can condition on x3 alone to give

p(x1, x2, x4|x3) ∝ φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)φ(x1, x3)

This can be written as a modiﬁed distribution, ﬁg(27.5b)

p(x1, x2, x4|x3) ∝ φ

(cid:48)(x1, x2)φ

(cid:48)(x4, x1)

(27.3.13)

(27.3.14)

As a distribution on x1, x2, x4 this is a singly-connected linear chain from which samples can be drawn
exactly. A simple approach is compute the normalisation constant by any of the standard techniques, for
example, using the factor graph method. One may then convert this undirected linear chain to a directed
graph, and use ancestral sampling. These operations are linear in the number of variables in the condi-
tioned distribution. Alternatively, one may form a junction tree from a set of potentials, choose a root and
then form a set chain by reabsorption on the junction tree. Ancestral sampling can then be performed on
the resulting oriented clique tree. This is the approach taken in GibbsSample.m.

In the above example one can also reveal a tractable distribution by conditioning on x1,

p(x3, x2, x4|x1) ∝ φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)φ(x1, x3)

(27.3.15)

and then draw a sample of x2, x3, x4 from this distribution. A valid sampling procedure is then to draw
a sample x1, x2, x4 from equation (27.3.13) and then a sample x3, x2, x4 from equation (27.3.15). These
two steps are then iterated. Note that x2 and x4 are not constrained to be equal to their values in the
previous sample. This procedure is generally to be preferred to the single-site Gibbs updating since the
samples are less correlated from one sample to the next.

See demoGibbsSample.m for a comparison of unstructured and structured sampling from a set of potentials.

DRAFT March 9, 2010

497

1x2xGibbs Sampling

(a)

(b)

Figure 27.7: Two hundred Gibbs samples for a two dimensional Gaussian. At each stage only a single
component is updated. (a): For a Gaussian with low correlation, Gibbs sampling can move through the
(b): For a strongly correlated Gaussian, Gibbs sampling is less eﬀective and
likely regions eﬀectively.
does not rapidly explore the likely regions, see demoGibbsGauss.m.

27.3.3 Remarks

If the initial sample x1 is in a part of the state space that is very unlikely then it may take some time for
the samples to become representative, as only a single component of x is updated at each iteration. This
motivates a so-called burn in stage in which the initial samples are discarded.

In single site Gibbs sampling there will be a high degree of correlation in any two successive samples,
since only one variable (in the single-site updating version) is updated at each stage. An ideal ‘perfect’
sampling scheme would draw each x ‘at random’ from p(x) – clearly, in general, two such perfect samples
will not possess the same degree of correlation as those from Gibbs sampling. This motivates subsampling
in which, say, every 10th, sample xK, xK+10, xK+20, . . ., is taken, and the rest discarded.

Due to its simplicity, Gibbs sampling is one of the most popular sampling methods and is particularly
convenient when applied to Belief Networks, due to the Markov blanket property1. Gibbs sampling is a
special case of the MCMC framework and, as with all MCMC methods, one should bear in mind that
convergence can be a major issue – that is, answering questions such as ‘how many samples are needed to
be reasonably sure that my sample estimate p(x5) is accurate?’ is diﬃcult. Despite mathematical results
for special cases, general rules of thumb and awareness on behalf of the user are required to monitor the
eﬃciency of the sampling.

Gibbs sampling assumes that we can move throughout the space eﬀectively by only single co-ordinate
updates. We also require that every state can be visited inﬁnitely often. In ﬁg(27.6), we show a case in
which the two dimensional continuous distribution has mass only in the lower left and upper right regions.
In that case, if we start in the lower left region, we will always remain there, and never explore the upper
right region. This problem occurs when two regions which are not connected by a ‘probable’ Gibbs path.

Gibbs sampling becomes a perfect sampler when the distribution is factorised – that is the variables are
independent. This suggests that in general Gibbs sampling will be less eﬀective when variables are strongly
correlated. For example, if we consider Gibbs sampling from a strongly correlated two variable Gaussian
distribution, then updates will move very slowly in space, ﬁg(27.7).

1The BUGS package www.mrc-bsu.cam.ac.uk/bugs is general purpose software for sampling from Belief Networks.

498

DRAFT March 9, 2010

−4−3−2−101234−3−2−10123−3−2−10123−2−1.5−1−0.500.511.52Markov Chain Monte Carlo (MCMC)

27.4 Markov Chain Monte Carlo (MCMC)

We assume we have a multivariate distribution in the form

p(x) =

∗(x)

p

1
Z

assume we are able to evaluate p∗(x = x), for any state x, but not Z, since Z =(cid:82)

where Z is the normalisation constant of the distribution and p∗(x) is the unnormalised distribution. We
x p∗(x) is an intractable

high dimensional summation/integration.

(27.4.1)

The idea in MCMC sampling is to sample, not directly from p(x), but from a diﬀerent distribution such
that, in the limit of a large number of samples, eﬀectively the samples will be from p(x). To achieve this
we forward sample from a Markov transition whose stationary distribution is equal to p(x).

27.4.1 Markov chains
Consider the conditional distribution q(xl+1|xl).
If we are given an initial sample x1, then we can re-
cursively generate samples x1, x2, . . . , xL. After a long time L (cid:29) 1, (and provided the Markov chain is
‘irreducible’, meaning that we can eventually get from any state to any other state) the samples are from
the stationary distribution q∞(x) which is deﬁned as (for a continuous variable)

(cid:90)

x

q∞(x

(cid:48)) =

(cid:48)

q(x

|x)q∞(x)

(27.4.2)

The condition for a discrete variable is analogous on replacing integration with summation. The idea
in MCMC is, for a given distribution p(x), to ﬁnd a transition q(x(cid:48)
|x) which has p(x) as its stationary
distribution. If we can do so, then we can draw samples from the Markov Chain by forward sampling and
take these as samples from p(x).
Note that for every distribution p(x) there will be more than one transition q(x(cid:48)
|x) with p(x) as its
stationary distribution. This is why there are very many diﬀerent MCMC sampling methods, each with
diﬀerent characteristics and varying suitability for the particular distribution at hand.

Detailed balance
How do we construct a transition q(x(cid:48)
|x) with given p(x) as its stationary distribution? This problem can
be simpliﬁed if we consider special transitions that satisfy the detailed balance condition. If we are given
the marginal distribution p(x), the detailed balance condition for a transition q is

Under this we see

q(x(cid:48)
|x)
q(x|x(cid:48))
(cid:90)

(cid:48)
q(x

x

= p(x(cid:48))
p(x) ,

|x)p(x) =

(cid:48)
∀x, x

(cid:48))p(x

(cid:48)) = p(x

(cid:48))

q(x|x

(cid:90)

x

(27.4.3)

(27.4.4)

so that p(x) is the stationary distribution of q(x(cid:48)
|x). The detailed balance requirement can make the
process of constructing a suitable transition easier since only the relative value of p(x(cid:48)) to p(x) is required
in equation (27.4.3), and not the absolute value of p(x) or p(x(cid:48)).

27.4.2 Metropolis-Hastings sampling

Consider the following transition

(cid:48)
q(x

|x) = ˜q(x

(cid:48)

(cid:48)

|x)f(x

, x) + δ(x

(cid:48)

, x)

(cid:18)
1 −

(cid:90)

x(cid:48)(cid:48)

(cid:19)

(cid:48)(cid:48)

˜q(x

(cid:48)(cid:48)

, x)

|x)f(x

DRAFT March 9, 2010

(27.4.5)

499

Algorithm 25 Metropolis-Hastings MCMC sampling.

Markov Chain Monte Carlo (MCMC)

1: Choose a starting point x1.
2: for i = 2 to L do
3:

Draw a candidate sample xcand from the proposal ˜q(x(cid:48)
Let a = ˜q(xl−1|xcand)p(xcand)
˜q(xcand|xl−1)p(xl−1)
if a ≥ 1 then xl = xcand
else

|xl−1).

draw a random value u uniformly from the unit interval [0, 1].
if u < a then xl = xcand
else

4:

5:
6:
7:
8:
9:
10:
11:
12:
13: end for

xl = xl−1

end if

end if

(cid:46) Accept the candidate

(cid:46) Accept the candidate

(cid:46) Reject the candidate

where ˜q(x(cid:48)
valid distribution q(x(cid:48)

|x) is a so-called proposal distribution and 0 < f(x(cid:48), x) ≤ 1 a positive function. This deﬁnes a

(cid:90)

|x) since it is non-negative and
(cid:48)(cid:48)
˜q(x

˜q(x

(cid:48)

(cid:48)

|x)f(x

, x) + 1 −

x(cid:48)(cid:48)

|x)f(x

(cid:90)

(cid:48)(cid:48)

, x) = 1

(27.4.6)

Our interest is to set f(x, x(cid:48)) such that the stationary distribution of q(x(cid:48)
proposal ˜q(x(cid:48)

|x) is equal to p(x) for any

(cid:48)

q(x

x(cid:48)

(cid:48)) =

p(x

x(cid:48)

|x) =
(cid:90)
|x). That is
(cid:90)

q(x

x

(cid:48)

|x)p(x)
(cid:48)
|x)f(x

(cid:48)

=

˜q(x

x

(cid:90)

(cid:90)

(cid:18)

(cid:48))

1 −

(cid:90)

x(cid:48)(cid:48)

(cid:48)(cid:48)
˜q(x

|x

(cid:48))f(x

(cid:48)(cid:48)

(cid:19)

(cid:48))
, x

, x)p(x) + p(x

In order that this holds, we require (changing the integral variable from x(cid:48)(cid:48) to x)

Now consider the Metropolis-Hastings acceptance function

(cid:90)

x

(cid:48)

˜q(x

(cid:48)
|x)f(x

x

(cid:48)

f(x

, x) = min

(cid:48)

f(x

, x)˜q(x

(cid:48)

, x)p(x) =

˜q(x|x
(cid:19)

1,

˜q(x|x(cid:48))p(x(cid:48))
˜q(x(cid:48)
|x)p(x)

(cid:18)
|x)p(x) = min(cid:0)˜q(x
= min(cid:0)˜q(x|x

(cid:48)

(cid:48))f(x, x

(cid:48))p(x

(cid:48))

(cid:18)

= min

1,

|x)p(x), ˜q(x|x
(cid:48)
(cid:48))p(x
(cid:48)), ˜q(x

(cid:19)

˜q(x|x(cid:48))p∗(x(cid:48))
|x)p∗(x)
˜q(x(cid:48)
(cid:48))(cid:1)
|x)p(x)(cid:1) = f(x, x

(cid:48))p(x

which is deﬁned for all x, x(cid:48) and has the detailed balance property

(cid:48))˜q(x|x

(cid:48))p(x

(cid:48))

(27.4.7)

(27.4.8)

(27.4.9)

(27.4.10)

(27.4.11)
(27.4.12)

Hence the function f(x(cid:48), x) as deﬁned above ensures equation (27.4.9) holds and that q(x(cid:48)
its stationary distribution.
How do we sample from q(x(cid:48)
|x)? Equation(27.4.5) can be interpreted as a mixture of two distributions,
|x)f(x(cid:48)(cid:48), x).
one proportional to ˜q(x(cid:48)
To draw a sample from this, we draw a sample from ˜q(x(cid:48)
|x) and accept this with probability f(x(cid:48), x). Since
drawing from ˜q(x(cid:48)
|x) and accepting are performed independently, the probability of accepting the drawn
candidate is the product of these probabilities, namely ˜q(x(cid:48)
|x)f(x(cid:48), x). Otherwise the candidate is rejected
and we take the sample x(cid:48) = x. Using the properties of the acceptance function, equation (27.4.10), the
following is equivalent to deciding on accepting/rejecting the candidate. If

|x)f(x(cid:48), x) and the other δ(x(cid:48), x) with mixture coeﬃcient 1 −

(cid:82)
x(cid:48)(cid:48) ˜q(x(cid:48)(cid:48)

|x) has p(x) as

˜q(x|x

(cid:48))p

∗(x

(cid:48)) > ˜q(x
(cid:48)

∗(x)

|x)p

500

(27.4.13)

DRAFT March 9, 2010

Auxiliary Variable Methods

Figure 27.8: Metropolis-Hastings samples from a bi-
variate distribution p(x1, x2) using a proposal ˜q(x(cid:48)
|x) =
N (x(cid:48) x, I). We also plot the iso-probability contours of
p. Although p(x) is multimodal, the dimensionality is low
enough and the modes suﬃciently close such that a simple
Gaussian proposal distribution is able to bridge the two
modes. In higher dimensions, such multi-modality is more
problematic. See demoMetropolis.m

|x)p∗(x). If we reject the candidate we take x(cid:48) = x.

|x). Otherwise we accept the sample x(cid:48) from q(x(cid:48)

we accept the sample from ˜q(x(cid:48)
˜q(x|x(cid:48))p∗(x(cid:48))/˜q(x(cid:48)
Note that if the candidate x(cid:48) is rejected, we take the original x as the new sample. Hence at each iteration
of the algorithm produces a sample – either a copy of the current sample, or the candidate sample. A
rough rule of thumb is to choose a proposal distribution for which the acceptance rate is between 50% and
85%[105].

|x) with probability

Gaussian proposal distribution

A common proposal distribution for multivariate x (writing explicitly as a vector) is

(27.4.14)

(27.4.15)

˜q(x(cid:48)

|x) = N

for which ˜q(x(cid:48)

(cid:0)x(cid:48) x, σ2I(cid:1)
(cid:18)
|x) = ˜q(x|x(cid:48)) and the acceptance criterion becomes

− 1
2σ2 (x(cid:48)−x)2

(cid:19)

∝ e

f(x(cid:48)

, x) = min

1,

p∗(x(cid:48))
p∗(x)

If the unnormalised probability of the candidate state is higher than the current state, we therefore accept
the candidate. Otherwise, if the unnormalised probability of the candidate state is lower than the current
state, we accept the candidate only with probability p∗(x(cid:48))/p∗(x). If the candidate is rejected, the new
sample is taken to be a copy of the previous sample x. See ﬁg(27.8) for a demonstration.

In high dimensions it is unlikely that a random candidate sampled from a Gaussian will result in a candi-
date probability higher than the current value, exercise(249). Because of this, only very small jumps (σ2
small) are likely to be accepted. This limits the speed at which we explore the space x.

This acceptance function above highlights that sampling is diﬀerent from ﬁnding the optimum. Provided
x(cid:48) has a higher probability than x, we accept x(cid:48). However, we also accept (with a speciﬁed acceptance
probability) candidates that have also a lower probability than the current sample.

27.5 Auxiliary Variable Methods

A practical concern in MCMC methods is ensuring that one moves eﬀectively through the signiﬁcant
probability regions of the distribution. For methods such as Metropolis-Hastings with local proposal dis-
tributions (local in the sense they are unlikely to propose a candidate far from the current sample), if the
target distribution has isolated islands of high density, then the likelihood that we would be able to move
from one island to the other is very small. If we attempt to make the proposal less local by using one
with a high variance the chance then of landing at random on a high density island is remote. Auxiliary
variable methods use additional dimensions to exploration and in certain cases to provide a bridge between

DRAFT March 9, 2010

501

Auxiliary Variable Methods

(a)

(b)

(c)

Figure 27.9: Hybrid Monte Carlo. (a): Multi-modal distribution p(x) for which we desire samples. (b):
HMC forms the joint distibution p(x)p(y) where p(y) is Gaussian. (c): Starting from the point x, we ﬁrst
draw a y from the Gaussian p(y), giving a point (x, y), green line. Then we use Hamiltonian dynamics
(white line) to traverse the distribution at roughly constant energy for a ﬁxed number of steps, giving
x(cid:48), y(cid:48). We accept this point if H(x(cid:48), y(cid:48)) > H(x, y(cid:48)) and make the new sample x(cid:48) (red line). Otherwise this
candidate is accepted with probability exp(H(x(cid:48), y(cid:48)) − H(x, y(cid:48))). If rejected the new sample x(cid:48) is taken as

a copy of x.

isolated high density islands.

Consider drawing samples from p(x) where x is a high-dimensional vector. For an auxiliary variable y we
introduce a distribution p(y|x), to form the joint distribution

p(x, y) = p(y|x)p(x)

(27.5.1)

If we draw samples (xl, yl) from this joint distribution then a valid set of samples from p(x) is given by
taking the xl alone. If we sampled x directly from p(x) and then y from p(y|x), introducing y is pointless
since there is no eﬀect on the x sampling procedure. In order for this to be useful, therefore, the auxiliary
variable must inﬂuence how we sample x. Below we discuss some of the common auxiliary variable schemes.

27.5.1 Hybrid Monte Carlo

Hybrid MC is a method for continuous variables that aims to make non-local jumps in the samples and, in
so doing, to jump potentially from one mode to another. We deﬁne the distribution from which we wish
to sample as

p(x) =

1
Zx

eHx(x)

(27.5.2)

for some given ‘Hamiltonian’ Hx(x) (this is just a potential). We then deﬁne another, ‘easy’ distribution
from which we can readily generate samples,

p(y) =

1
Zy

eHy(y)

so that the joint distribution is given by

p(x, y) = p(x)p(y) =

1
Z

eHx(x)+Hy(y) =

1
Z

eH(x,y)

(27.5.3)

(27.5.4)

In the standard form of the algorithm, a multi-dimensional Gaussian is chosen for the auxiliary distribution
with dim y = dim x, so that

Hy(y) = −

1
2

yTy

(27.5.5)

The HMC algorithm ﬁrst draws from p(y) and subsequently from p(x, y). For a Gaussian p(y), sampling
from is straightforward. In the next,‘dynamic’ step, a sample is drawn from p(x, y) using a Metropolis

502

DRAFT March 9, 2010

−5−4−3−2−101234500.511.522.53x 10−3Auxiliary Variable Methods

MCMC sampler. The idea is to go from one point of the space x, y to a new point x(cid:48), y(cid:48) that is a non-trivial
distance from x, y and which will be accepted with a high probability. The candidate (x(cid:48), y(cid:48)) will have a
good chance to be accepted if H(x(cid:48), y(cid:48)) is close to H(x, y) – this can be achieved by following a contour
of equal ‘energy’ H, as described in the next section.

Hamiltonian dynamics
We wish to make an update x(cid:48) = x + ∆x, y(cid:48) = y + ∆y for small ∆x and ∆y such that the Hamiltonian
H(x, y) ≡ Hx(x) + Hy(y) is conserved,

We can satisfy this (up to ﬁrst order) by considering the Taylor expansion

H(x(cid:48)

, y(cid:48)) = H(x, y)

H(x(cid:48)

, y(cid:48)) = H(x + ∆x, y + ∆y)

≈ H(x) + ∆xT∇xH(x, y) + H(y) + ∆yT∇yH(x, y) + O(cid:0)

|∆x|2(cid:1) + O(cid:0)

Conservation, up to ﬁrst order, therefore requires

∆xT∇xH(x, y) + ∆yT∇yH(x, y) = 0

|∆y|2(cid:1)

(27.5.6)

(27.5.7)

(27.5.8)

This is a single scalar requirement, and there are therefore many diﬀerent solutions for ∆x and ∆y that
satisfy this single condition. It is customary to use Hamiltonian dynamics, which correspond to the setting:

∆x = ∇yH(x, y)

∆y = −∇xH(x, y)

where  is a small value to ensure that the Taylor expansion is accurate. Hence

x(t + 1) = x(t) + ∇yHy(y)

y(t + 1) = y(t) − ∇xHx(x)

(27.5.9)

(27.5.10)

For the HMC method, H(x, y) = Hx(x) + Hy(y), so that ∇xH(x, y) = ∇xHx(x) and ∇yH(x, y) =
∇yHy(y) For the Gaussian case, ∇yHy(y) = −y so that
y(t + 1) = y(t) − ∇xH(x)

x(t + 1) = x(t) − y

(27.5.11)

There are speciﬁc ways to implement the Hamiltonian dynamics called Leapfrog discretisation that are
more accurate than the simple time-discretisation used above, and we refer the reader to [205] for details.

In order to make a symmetric proposal distribution, at the start of the dynamic step, we choose  = +0
or  = −0 uniformly. This means that there is the same chance that we go back to the point x, y starting
from x(cid:48), y(cid:48), as vice versa. We then follow the Hamiltonian dynamics for many time steps (usually of the
order of several hundred) to reach a candidate point x(cid:48), y(cid:48). If the Hamiltonian dynamics is numerically
accurate, H(x(cid:48), y(cid:48)) will have roughly the same value as H(x, y). We then do a Metropolis step, and accept
the point x(cid:48), y(cid:48) if H(x(cid:48), y(cid:48)) > H(x, y) and otherwise accept it with probability exp(H(x(cid:48), y(cid:48)) − H(x, y)).
If rejected, we take the initial point x, y as the sample. Combined with the p(y) sample step, we then
have the general procedure as described in algorithm(26).

In HMC we use not just the potential Hx(x) to deﬁne candidate samples, but the gradient of Hx(x) as
well. An intuitive explanation for the success of the algorithm is that it is less myopic than straightforward
Metropolis since the gradient enables the algorithm to feel its way to other regions of high probability
by contouring paths in the augmented space. One can also view the auxiliary variables as momentum
variables – it is as if the sample has now a momentum which can carry it through the low-density x-
regions. Provided this momentum is high enough, we can escape local regions of signiﬁcant probability,
see ﬁg(27.9).

DRAFT March 9, 2010

503

Auxiliary Variable Methods

Algorithm 26 Hybrid Monte Carlo sampling

1: Start from x
2: for i = 1 to L do
3:
4:
5:
6:
7:
8: end for

Draw a new sample y from p(y).
Choose a random (forwards or backwards) trajectory direction.
Starting from x, y, follow Hamiltonian dynamics for a ﬁxed number of steps, giving a candidate x(cid:48), y(cid:48).
Accept x(cid:48), y(cid:48) if H(x(cid:48), y(cid:48)) > H(x, y), otherwise accept it with probability exp(H(x(cid:48), y(cid:48)) − H(x, y)).
If rejected, we take the sample as x, y.

(a)

(c)

(b)

(cid:81)

(a): Current sample of states
Figure 27.10: Swendson-Wang updating for p(x) ∝
(here on a nearest neighbour lattice). (b): Like coloured neighbours are bonded together with probability
1−e−β, forming clusters of variables. (c): Each cluster is given a random colour, forming the new sample.

i∼j exp βI [xi = xj].

27.5.2 Swendson-Wang

Originally, the SW method was introduced to alleviate the problems encountered in sampling from Ising
Models close to their critical temperature[268]. At this point large islands of same-state variables form so
that strong correlations appear in the distribution – the scenario under which, for example, Gibbs sampling
is not well suited. The method has been generalised to other models[88], although here we outline the
procedure for the Ising model only, referring the reader to more specialised text for the extensions [37].
See also [198] for the use of auxiliary variables in perfect sampling.

The Ising model with no external ﬁelds is deﬁned on variables x = (x1, . . . , xn), xi ∈ {0, 1} and takes the
form

eβI[xi=xj ]

(27.5.12)

(cid:89)

i∼j

p(x) =

1
Z

which means that this is a pairwise Markov network with a potential contribution eβ if neighbouring
nodes i and j on a square lattice are in the same state, and a contribution 1 otherwise. We assume that
β > 0 which encourages neighbours to be in the same state. The lattice based neighbourhood structure
makes this diﬃcult to sample from, and especially when β ≈ 0.9 which encourages large scale islands of
same-state variables to form.
The aim is to remove the problematic terms eβI[xi=xj ] by the use of the auxiliary ‘bond’ variables, yij, one
for each edge on the lattice, making the conditional p(x|y) easy to sample from. This is given by

p(x|y) ∝ p(y|x)p(x) ∝ p(y|x)(cid:89)
0 < yij < eβI[xi=xj ](cid:105)
Using p(y|x) we can cancel the terms eβI[xi=xj ] by setting
p(yij|xi, xj) =(cid:89)
p(y|x) =(cid:89)
where I(cid:2)0 < yij < eβI[xi=xj ](cid:3) denotes a uniform distribution between 0 and eβI[xi=xj ]; zij is the normalisa-

eβI[xi=xj ]

(27.5.13)

(27.5.14)

I(cid:104)

i∼j

i∼j

1
zij

i∼j

504

DRAFT March 9, 2010

Auxiliary Variable Methods

, with
Figure 27.11: Ten successive samples from a 25 × 25 Ising model p(x) ∝ exp
β = 0.88, close to the critical temperature. The Swendson-Wang procedure is used. Starting in a random
initial conﬁguration, the samples quickly move away from this initial state, with the characteristic long-
range correlations of the variables seen close to the critical temperature.

i∼j βI [xi = xj]

(cid:16)(cid:80)

(cid:17)

tion constant zij = eβI[xi=xj ]. Hence

I(cid:104)
0 < yij < eβI[xi=xj ](cid:105)

p(x|y) ∝ p(y|x)p(x)
1
eβI[xi=xj ]

∝

I(cid:104)

0 < yij < eβI[xi=xj ](cid:105)

(cid:89)
(cid:89)

i∼j

i∼j

∝

eβI[xi=xj ]

(27.5.15)

(27.5.16)

(27.5.17)

Let’s assume that we have a sample {yij}. If yij > 1, then to draw a sample from p(x|y), we must have
1 < eβI[xi=xj ], which means that xi and xj are constrained to be in the same state. Otherwise, if yij < 1,
then this introduces no extra constraint on xi and xj. Hence, wherever yij > 1, we bond xi and xj to be
in the same state.

same state. Then p(yij|xi = xj) = U(cid:0)yij|

To sample from the bond variables p(yij|xi, xj) consider ﬁrst the situation that xi and xj are in the
p(yij|xi (cid:54)= xj) = U (yij| [0, 1]). A bond will occur if yij > 1, which occurs with probability

(cid:2)0, eβ(cid:3)(cid:1). Similarly when xi and xj are in diﬀerent states,

(cid:90) ∞

1
zij

yij =1

I(cid:104)

0 < yij < eβ(cid:105)

p(yij > 1|xi = xj) =

= eβ − 1

eβ = 1 − e

−β

(27.5.18)

Hence, if xi = xj, we bind xi and xj to be in the same state with probability 1 − e−β. On the other hand
if xi and xj are in diﬀerent states, yij is uniformly distributed between 0 and 1.

After doing this for all the xi and xj pairs, we will end up with a graph in which we have clusters of
like-state bonded variables. The algorithm simply chooses a random state for each cluster – that is, with
probability 0.5 all variables in the cluster are in state 1. The algorithm is described below, see ﬁg(27.10):

Algorithm 27 Swendson-Wang sampling

1, . . . , x1
n.

for i, j in the edge set do

1: Start from a random conﬁguration of all x1
2: for l = 1 to L do
3:
4:
5:
6:
7:
8: end for

If xi = xj, we bond variables xi and xj with probability 1 − e−β.

end for
For each cluster formed from the above, set the state of the cluster uniformly at random.
This gives a new joint conﬁguration xl

1, . . . , xl
n.

This technique has found application in spatial statistics, particularly image restoration[134].

27.5.3 Slice sampling

Slice sampling[207] is an auxiliary variable technique that aims to overcome some of the diﬃculties in
choosing an appropriate ‘length scale’ in methods such as Metropolis sampling. The brief discussion

DRAFT March 9, 2010

505

Importance Sampling

Figure 27.12: The full slice for a given y.
Ideally slice sampling would
draw an x sample from anywhere on the full slice (green). In general this
is intractable for a complex distribution and a local approximate slice is
formed instead, see ﬁg(27.13).

Z p∗(x) where the
here follows that presented in [183] and [42]. We want to draw samples from p(x) = 1
normalisation constant Z is unknown. By introducing the auxiliary variable y and deﬁning the distribution

(cid:26) 1/Z for 0 ≤ y ≤ p∗(x)
(cid:90) p∗(x)

otherwise

0

p(x, y) =

we have(cid:90)

(27.5.19)

(27.5.20)

p(x, y)dy =

0

1
Z

dy =

1
Z

∗(x) = p(x)

p

which shows that the marginal of p(x, y) over y is equal to the distribution we wish to draw samples from.
Hence if we draw samples from p(x, y), we can ignore the y samples and we will have a valid sampling
scheme for p(x).

To draw from p(x, y) we use Gibbs sampling, ﬁrst drawing from p(y|x) and then from p(x|y). Drawing a
sample from p(y|x) means that we draw a value y from the uniform distribution U (y| [0, p∗(x)]).
Given a sample y, one then draws a sample x from p(x|y). Using p(x|y) ∝ p(x, y) we see that p(x|y) is
the distribution over x such that p∗(x) > y:

p(x|y) ∝ I [p

∗(x) > y]

(27.5.21)

For a given y, we call the x that satisfy this a ‘slice’, ﬁg(27.12). Computing the normalisation of this
distribution is in general non-trivial since we would in principle need to search over all x to ﬁnd those
for which p∗(x) > y. Ideally we would like to get as much of the slice as feasible, since this will improve
the mixing of the chain. If we concentrate on the part of the slice only very local to the current x, then
the samples move through the space very slowly. If we attempt to guess at random a point a long way
from x and check if is in the slice, this will be mostly unsuccessful. The happy compromise presented in
algorithm(28)[207] and described in ﬁg(27.13) determines an appropriate local slice by adjusting the left
and right regions. The technique is to start from the current x and attempt to ﬁnd the largest local slice
by incrementally widening the candidate slice. Once we have the largest potential slice we attempt to
sample from this. If the sample point within the local slice is in fact not in the slice, this is rejected and
the slice is shrunk.
This describes a procedure for sampling from a univariate distribution p∗(x). To sample from a multivariate
distribution p(x), single variable Gibbs sampling can be used to sample from p(xj|x\j), repeatedly choosing
a new variable xj to sample.

27.6 Importance Sampling

Z where p∗(x) can be evaluated but Z =(cid:82)

Importance sampling is a technique to approximate averages with respect to an intractable distribution
p(x). The term ‘sampling’ is arguably a misnomer since the method does not attempt to draw samples from
p(x). Rather the method draws samples from a simpler importance distribution q(x) and then reweights
them such that averages with respect to p(x) can be approximated using the samples from q(x). Consider
p(x) = p∗(x)
x p∗(x) is an intractable normalisation constant. The
(cid:82)
average of f(x) with respect to p(x) is given by
(cid:82)
x f(x) p∗(x)
p∗(x)
q(x) q(x)

(cid:82)
(cid:82)
x f(x)p∗(x)
x p∗(x)

f(x)p(x) =

q(x) q(x)

(27.6.1)

(cid:90)

x

=

x

506

DRAFT March 9, 2010

Importance Sampling

(a)

(c)

(b)

(d)

Figure 27.13: (a): For the current sample x, a point y is sampled between 0 and p∗(x), giving a point
(x, y) (black circle). Then an interval of width w is placed around x, the blue bar. The ends of the bar
(b): The interval is increased until it
denote if the point is in the slice (green) or out of the slice (red).
(c): Given an interval a sample x(cid:48) is taken uniformly in the interval. If the
hits a point out of the slice.
candidate x(cid:48) is not in the slice (red), p(x(cid:48)) < y, the candidate is rejected and the interval is shrunk. (d):
The sampling from the interval is repeated until a candidate is in the slice (green), and is subsequently
accepted.

Let x1, . . . , xL be samples from q(x), then we can approximate the average by

(cid:90)

f(x)p(x) ≈

x

(cid:80)L
(cid:80)L
l=1 f(xl) p∗(xl)

q(xl)

p∗(xl)
q(xl)

l=1

L(cid:88)

l=1

=

f(xl)wl

where we deﬁne the normalised importance weights

(cid:80)L

p∗(xl)/q(xl)
l=1 p∗(xl)/q(xl)

wl =

,

with

wl = 1

L(cid:88)

l=1

(27.6.2)

(27.6.3)

In principle, reweighing the samples from q will give the correct result for the average with respect to
p. Since the weight is a measure of how well q matches p, there will typically be only a single dominant
weight. This is particularly evident in high dimensions there will typically only be one dominant weight
with value close to 1, and the rest will be zero, particularly if the sampling distribution q is not well
matched to p. As an indication of this eﬀect, consider a D-dimensional multivariate x with two samples
x1 and x2. For simplicity we assume that both p and q are factorised over their variables. The associated
unnormalised importance weights are then
p∗(x2
d)
q(x2
d)

p∗(x1
d)
d) ,
q(x1

D(cid:89)

D(cid:89)

(27.6.4)

˜w1 =

˜w2 =

d=1

d=1

If we assume the the match between q and p better is worse by a factor α ≤ 1 in each of the dimensions
at x2 than x1 then

(27.6.5)

˜w1 = O(cid:0)αD(cid:1)

˜w2

so that importance weight at w1 will exponentially dominate w2. A method that can help address this
weight dominance is resampling. Given the weight distribution w1, . . . , wL, one draws a set of L sample
indices. This new set of indices will almost certainly contain repeats since any of the original low-weight
samples will most likely not be included. The weight of each of these new samples is set uniformly to
1/L. This procedure helps select only the ‘ﬁttest’ of the samples and is known as Sampling Importance
Resampling[235].

DRAFT March 9, 2010

507

Draw a vertical coordinate y uniformly from the interval(cid:0)0, p∗(xi)(cid:1).

Create a horizontal interval (xlef t, xright) that contains xi as follows:
Draw r ∼ U (r| (0, 1))
xlef t = xi − rw, xright = xi + (1 − r)w
while p∗(xlef t) > y do
xlef t = xlef t − w

Algorithm 28 Slice Sampling (univariate).

end while
while p∗(xright) > y do
xright = xright + w

1: Choose a starting point x1 and step size w.
2: for i = 1 to L do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end for

end if
end while
xi+1 = x(cid:48)

end if

else

end while
accept = false
while accept = false do
draw a random value x(cid:48) uniformly from the unit interval (xlef t, xright).
if p∗(x(cid:48)) > y then
accept = true

modify the interval (xlef t, xright) as follows:
if x(cid:48) > xi then
xright = x(cid:48)
xlef t = x(cid:48)

else

Importance Sampling

(cid:46) Create an initial interval

(cid:46) Step out left

(cid:46) Step out right

(cid:46) Found a valid sample

(cid:46) Shrinking

27.6.1 Sequential importance sampling

One can apply importance sampling to temporal distributions p(x1:t) for which the importance distribution
samples from q(x1:t) are paths. In many applications such as tracking, one wishes to update ones beliefs
as time increases and, as such, is required to resample and then reweight the whole path. For distributions
p(x1:t) with a Markov structure, one would expect that a local update is possible, without needing to deal
with the previous path. To show this, consider the unnormalised importance weights for a sample path
xl
1:t

t = p∗(xl
1:t)
1:t)

q(xl

˜wl

=

p∗(xl
q(xl

1:t−1)
1:t−1)

p∗(xl
1:t)
1:t−1)q(xl
t|xl

p∗(xl

1:t−1)

,

1 = p∗(xl
1)
q(xl
1)

˜wl

We can recursively deﬁne the un-normalised weights using

˜wl

t = ˜wl

t−1αl
t,

t > 1

where

αl
t ≡

p∗(xl
1:t)
1:t−1)q(xl
t|xl

p∗(xl

1:t−1)

(27.6.6)

(27.6.7)

(27.6.8)

This means that in SIS we need only deﬁne the conditional importance distribution q(xt|x1:t−1). The ideal
setting of the sequential importance distribution is q = p and q(xt|x1:t−1) = p(xt|x1:t−1), although this
choice is impractical in most cases.

508

DRAFT March 9, 2010

Importance Sampling

h1

v1

h2

v2

h3

v3

h4

v4

Figure 27.14: A Dynamic Bayesian Network.
In
many applications of interest, the emission distribu-
tion p(vt|ht) is non-Gaussian, leading to the formal
intractability of ﬁltering/smoothing.

For Dynamic Bayes Networks, equation (27.6.8) will simplify considerably. For example consider distribu-
tions with a Hidden Markov independence structure,

p(v1:t, h1:t) = p(v1|h1)p(h1)

p(vt|ht)p(ht|ht−1)

(27.6.9)

t(cid:89)

t=2

where v1:t are observations and h1:t are the random variables. A cancelation of terms in the numerator
and denominator occurs, leaving simply

αl
t ≡

t−1)

p(vt|hl
q(hl

t)p(hl
t|hl

t|hl
1:t)

Sequential importance sampling is also known as particle ﬁltering.

(27.6.10)

Particularly in cases where the transition is easy to sample from, a common sequential importance distri-
bution is

q(ht|h1:t−1) = p(ht|ht−1)

in which case, from equation (27.6.8), αl
by

(27.6.11)
t = p(vt|ht) and the unnormalised weights are recursively deﬁned

˜wl

t = ˜wl

t−1p(vt|hl
t)

(27.6.12)

A drawback of this procedure is that after a small number of iterations only very few particle weights
will be signiﬁcantly non-zero due to the mismatch between the importance distribution q and the target
distribution p. This can be addressed using resampling, as described in section(27.6)[82].

27.6.2 Particle ﬁltering as an approximate forward pass

Particle ﬁltering can be viewed as an approximation to the exact ﬁltering recursion. Using ρ to represent
the ﬁltered distribution,
ρ(ht) ∝ p(ht|v1:t)
(cid:90)

the exact ﬁltering recursion is

(27.6.13)

ρ(ht) ∝ p(vt|ht)

p(ht|ht−1)ρ(ht−1)

ht−1

A PF can be viewed as an approximation of equation (27.6.14) in which the message ρ(ht−1) is approxi-
mated by a sum of δ-peaks:

(27.6.14)

(27.6.15)

L(cid:88)

(cid:17)

ρ(ht−1) ≈

wl

t−1δ

ht−1, hl

t−1

t−1 are the normalised importance weights(cid:80)L

l=1

(cid:16)

L(cid:88)

l=1

where wl
t−1 are the particles . In other
words, the ρ message is represented as a weighted mixture of delta-spikes where the weight and position of
the spikes are the parameters of the distribution. Using equation (27.6.15) in equation (27.6.14), we have

t−1 = 1, and hl

l=1 wl

ρ(ht) =

1
Z

p(vt|ht)

DRAFT March 9, 2010

p(ht|hl

t−1)wl

t−1

(27.6.16)

509

Importance Sampling

The constant Z is used to normalise the distribution ρ(ht). Although ρ(ht−1) was a simple sum of delta
peaks, in general ρ(ht) will not be – the delta-peaks get ‘broadened’ by the hidden-to-hidden and hidden-
to-observation factors. Our task is then to approximate ρ(ht) as a new sum of delta-peaks. Below we
discuss a method to achieve this for which explicit knowledge of the normalisation Z is not required. This
is useful since in many tracking applications the normalisation of the emission p(vt|ht) is unknown.
A Monte-Carlo sampling approximation

A simple approach to forming an approximate mixture-of-delta functions representation of equation
(27.6.16) is to generate a set of sample points using importance sampling. That is we generate a set
from some importance distribution q(ht) which gives the unnormalised importance
of samples h1
weights

t , . . . , hL
t

(27.6.17)

(27.6.18)

(27.6.19)

t)(cid:80)L

p(vt|hl

˜wl

t =

t−1)wl(cid:48)
t|hl(cid:48)
t−1

l(cid:48)=1 p(hl
q(hl
t)

Deﬁning the normalised weights:

we obtain an approximation

wl

t =

t

˜wl
l(cid:48) ˜wl(cid:48)

t(cid:80)
L(cid:88)

l=1

ρ(ht) ≈

wl
tδ

ht, hl
t

(cid:17)

(cid:16)

L(cid:88)

l=1

Ideally one would use the importance distribution that makes the importance weights unity, namely

q(ht) ∝ p(vt|ht)

p(ht|hl

t−1)wl

t−1

(27.6.20)

L(cid:88)

l=1

However, this is often diﬃcult to sample from directly due to the unknown normalisation of the emission
p(vt|ht). A simpler alternative is to sample from the transition mixture:

q(ht) =

p(ht|hl

t−1)wl

t−1

(27.6.21)

To do so, one ﬁrst samples a component l∗ from the histogram with weights from w1
this sample index, say l∗, one then draws a sample from p(ht|hl∗

t−1. Given
t−1). In this case the un-normalised weights

t−1, . . . , wL

become simply

t = p(vt|hl
˜wl
t)

(27.6.22)

This Forward-Sampling-Resampling procedure is used in demoParticleFilter.m and in the following toy
example.

Example 113 (A toy face-tracking example). At time t a binary face template is in a location ht, which
describes the upper-left corner of the template using a two-dimensional vector. At time t = 1 the position
of the face is known, see ﬁg(27.15a). The face template is known. In subsequent times the face moves
randomly according to

ht = ht−1 + σηt

(27.6.23)

where ηt ∼ N (ηt 0, I) is a two dimensional zero mean unit covariance noise vector.
In addition, a
fraction of the binary pixels in the whole image are selected at random and their states ﬂipped. The aim

510

DRAFT March 9, 2010

Importance Sampling

Figure 27.15: Tracking an object
with a particle ﬁlter containing 50
particles. The small circles are the
particles,
scaled by their weights.
The correct corner position of the
face is given by the ‘×’, the ﬁl-
tered average by the large circle
‘o’, and the most likely particle by
‘+’.
Initial position of
the face without noise and corre-
sponding weights of the particles.
(b): Face with noisy background
and the tracked corner position af-
ter 20 timesteps.
The Forward-
Sampling-Resampling PF method is
used to maintain a healthy pro-
portion of non-zero weights.
See
demoParticleFilter.m

(a):

(a)

(b)

is to try to track the upper-left corner of the face through time.

We need to deﬁne the emission distribution p(vt|ht) on the binary pixels with vi ∈ {0, 1}. Consider the
following compatibility function

φ(vt, ht) = vT

t ˜v(ht)

(27.6.24)

where ˜v(ht) is the vector representing the image with a clean face placed at position ht. This measures the
overlap between the face template and the noisy image restricted to the template pixels. The compatibility
function is maximal when the observed image vt has the face placed at position ht. We can therefore
tentatively deﬁne

p(vt|ht) ∝ φ(vt, ht)

(27.6.25)

A subtlety is that ht is continuous, and in the compatibility function we ﬁrst map ht to the nearest
integer pixel representation. We have not speciﬁed the normalisation constant of this distribution, which
fortunately this is not required by the particle ﬁltering algorithm.
In ﬁg(27.15a) 50 particles are used
to track the face. The particles are plotted along with their corresponding weights. For each t > 1, 5%
of the pixels are selected at random in the image and their states ﬂipped. Using the Forward-Sampling-
Resampling method we can successfully track the face despite the presence of the background clutter.

Real tracking applications involve complex issues, including tracking multiple objects, transformations
of the object (scaling, rotation, morphology changes). Nevertheless, the principles are largely the same
and many tracking applications work by seeking simple compatibility functions, often based on the colour
histogram in a template. Indeed, tracking objects in complex environments was one of the original appli-
cations of particle ﬁlters [140].

DRAFT March 9, 2010

511

510152025303540510152025303540102030405000.010.020.030.040.050.060.07particle weights510152025303540510152025303540102030405000.010.020.030.040.050.060.070.080.09particle weights27.7 Code

potsample.m: Exact sample from a set of potentials
ancestralsample.m: Ancestral sample from a Belief Net
JTsample.m: Sampling from a consistent Junction Tree
GibbsSample.m: Gibbs sampling from a set of potentials
demoMetropolis.m: Demo of Metropolis sampling for a bimodal distribution
metropolis.m: Metropolis sample
logp.m: Log of a bimodal distribution
demoParticleFilter.m: Demo Particle Filtering (Forward-Sampling-Resampling method)
placeobject.m: Place an object in a grid
compat.m: Compatibility function
demoSampleHMM.m: Naive Gibbs sampling for a HMM

27.8 Exercises
Exercise 246 (Box-Muller method). Let x1 ∼ U (x1| [0, 1]), x2 ∼ U (x2| [0, 1]) and

y2 =(cid:112)

−2 log x2 sin 2πx2

y1 =(cid:112)

Show that

−2 log x1 cos 2πx2,
(cid:90)

p(y1, y2) =

p(y1|x1, x2)p(y2|x1, x2)p(x1)p(x2)dx1dx2 = N (y1 0, 1)N (y2 0, 1)

Exercises

(27.8.1)

(27.8.2)

and suggest an algorithm to sample from a univariate normal distribution.
Exercise 247. Consider the distribution

p(x1, . . . , x6) = p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

(27.8.3)
For x5 ﬁxed in a given state x5, write down a distribution on the remaining variables p(cid:48)(x1, x2, x3, x4, x6)
and explain how forward (ancestral) sampling can be carried out for this new distribution.
Exercise 248. Consider an Ising model on an M × M square lattice with nearest neighbour interactions:
(27.8.4)

I [xi = xj]

(cid:88)

i∼j

p(x) ∝ exp β

Now consider the M × M grid as a checkerboard, and give each white square a label wi, and each black
square a label bj, so that each square is associated with a particular variable. Show that

p(b1, b2, . . . ,|w1, w2, . . .) = p(b1|w1, w2, . . .)p(b2|w1, w2, . . .) . . .

That is, conditioned on the white variables, the black variables are independent. The converse is also
true, that conditioned on the black variables, the white variables are independent. Explain how this can
be exploited by a Gibbs sampling procedure. This procedure is known as checkerboard or black and white
sampling.
Exercise 249. Consider the symmetric Gaussian proposal distribution

and the target distribution

q I(cid:1)
(cid:0)x(cid:48) x, σ2
(cid:0)x 0, σ2
pI(cid:1)
(cid:29)

= −

N σ2
q
2σ2
p

˜q(x(cid:48)|x)

˜q(x(cid:48)

|x) = N

p(x) = N
(cid:28)
log p(x(cid:48))
p(x)

where dim x = N. Show that

(27.8.5)

(27.8.6)

(27.8.7)

(27.8.8)

Discuss how this result relates to the probability of accepting a Metropolis-Hastings update under a Gaussian
proposal distribution in high-dimensions.

512

DRAFT March 9, 2010

Exercises

Exercise 250. The ﬁle demoSampleHMM.m performs naive Gibbs sampling of the posterior p(h1:T|v1:T ) for
a HMM. At each Gibbs update a single variable ht is chosen, with the remaining h variables clamped. The
procedure starts from t = 1 and sweeps forwards through time. When the end time t = T is reached, the
joint state h1:T is taken as a sample from the posterior. The parameter λ controls how deterministic the
hidden transition matrix p(ht|ht−1) will be. Adjust demoSampleHMM.m to run 100 times, each time for the
same λ, computing a mean absolute error over these 100 runs. Then repeat this for λ = 0.1, 1, 10, 20.
Discuss why the performance of this Gibbs sampling routine deteriorates with increasing λ.

DRAFT March 9, 2010

513

Exercises

514

DRAFT March 9, 2010

CHAPTER 28

Deterministic Approximate Inference

28.1 Introduction

Deterministic approximate inference methods are an alternative to the stochastic techniques discussed in
chapter(27). Whilst stochastic methods are powerful and often generally applicable, they nevertheless
produce sample estimates of a quantity. Even if we are able to perform perfect sampling, we would still
only obtain an approximate result due to the inherent uncertainty introduced by sampling. Furthermore,
in practice, drawing exact samples is typically computationally intractable and assessing the quality of
the sample estimates is diﬃcult. In this chapter we discuss some alternative deterministic approximate
inference schemes. The ﬁrst, Laplace’s method, is a simple perturbation technique. The second class of
methods are those that produce rigorous bounds on quantities of interest. Such methods are interesting
since they provide certain knowledge – it may be suﬃcient, for example, to show that a marginal prob-
ability is greater than 0.1 in order to make an informed decision. A further class of methods are the
consistency methods, such as loopy belief propagation. Such methods have revolutionised certain ﬁelds,
including error correction[183], providing performance unobtainable from sampling based procedures.

It is important to bear in mind that no single approximation technique, deterministic or stochastic, is
going to beat all others on all problems, given the same computational resources. In this sense, insight
as to the properties of the approximation method used is useful in matching an approximation method to
the problem at hand.

28.2 The Laplace approximation

Consider a distribution on a continuous variable of the form

p(x) =

−E(x)

1
Z

e

(28.2.1)

The Laplace method makes a Gaussian approximation of p(x) based on a local perturbation expansion
around a mode x∗. First we ﬁnd the mode numerically, giving

x∗ = argmin

x

E(x)

Then a Taylor expansion up to second order around this mode gives

(28.2.2)

E(x) ≈ E(x∗) + (x − x∗)T ∇E|x∗ +

(28.2.3)
where H ≡ ∇∇E(x)|x∗ is the Hessian evaluated at the mode. At the mode, ∇E|x∗ = 0, and an approxi-
mation of the distribution is given by the Gaussian

1
2

(x − x∗)T H (x − x∗)
(cid:0)x x∗
, H−1(cid:1)

515

∗(x) =

p

1
Z∗ e

2 (x−x∗)TH(x−x∗) = N
− 1

(28.2.4)

Properties of Kullback-Leibler Variational Inference

Figure 28.1: Fitting a mixture of Gaussians p(x) (blue) with a single Gaus-
sian. The green curve minimises KL(q|p) corresponding to ﬁtting a local
model. The red curve minimises KL(p|q) corresponding to moment match-
ing.

which has mean x∗ and covariance H−1, with Z∗ = (cid:112)det (2πH−1). Similarly, we can use the above

−E(x∗)(cid:112)det (2πH−1)

(28.2.5)

(cid:90)

(cid:90)

expansion to estimate the integral

e

−E(x) ≈

x

x

−E(x∗)− 1
e

2 (x−x∗)TH(x−x∗) = e

Although the Laplace approximation ﬁts a Gaussian to a distribution, it is not necessarily the ‘best’
Gaussian approximation. As we’ll see below, other criteria, such as based on minimal KL divergence
between p(x) and a Gaussian approximation may be more appropriate, depending on the context. A
beneﬁt of Laplace’s method is its relative speed and simplicity compared with other approximate inference
techniques.

28.3 Properties of Kullback-Leibler Variational Inference

Variational methods can be used to approximate a complex distribution p(x) by a simpler distribution
q(x). Given a deﬁnition of discrepancy between an approximation q(x) to p(x), any free parameters of
q(x) are then set by minimising the discrepancy.

A particularly popular measure of the discrepancy between an approximation q(x) and the intractable
distribution p(x) is the Kullback-Leibler divergence

KL(q|p) = (cid:104)log q(cid:105)q − (cid:104)log p(cid:105)q

(28.3.1)
It is straightforward to show that KL(q|p) ≥ 0 and is zero if and only if the distributions p and q are
identical, see section(8.8). Note that whilst the KL divergence cannot be negative, there is no upper bound
on the value it can potentially take so that the discrepancy can be ‘inﬁnitely’ bad.

28.3.1 Bounding the normalisation constant

For a distribution of the form

p(x) =

1
Z

eφ(x)

we have

KL(q|p) = (cid:104)log q(x)(cid:105)q(x) − (cid:104)log p(x)(cid:105)q(x) = (cid:104)log q(x)(cid:105)q(x) − (cid:104)φ(x)(cid:105)q(x) + log Z

Since KL(q|p) ≥ 0 this immediately gives the bound

(cid:125)
log Z ≥ −(cid:104)log q(x)(cid:105)q(x)

(cid:123)(cid:122)

(cid:124)

entropy

(cid:125)
(cid:123)(cid:122)
(cid:124)
+(cid:104)φ(x)(cid:105)q(x)

energy

(28.3.2)

(28.3.3)

(28.3.4)

which is called the ‘free energy’ bound in the physics community[236]. Using the notation Hq for the
entropy of q, we can write the bound more compactly as

log Z ≥ Hq + (cid:104)φ(x)(cid:105)q(x)

(28.3.5)
The KL(q|p) method provides therefore a lower bound on the normalisation constant. For some models
it is possible (using alternative methods, see for example [287] and exercise(258)) to also form an upper

516

DRAFT March 9, 2010

−30−20−10010203000.050.10.150.20.250.30.350.4Properties of Kullback-Leibler Variational Inference

bound on the normalisation constant. With both an upper and lower bound on the normalisation terms,
we are able to bracket marginals l ≤ p(xi) ≤ u, see exercise(261). The tightness of the resulting bracket
gives an indication as to how tight the bounding procedures are. Even in cases where the resulting bracket
is weak – for example it might be that the result is that 0.1 < p(cancer = true) < 0.99, this may be
suﬃcient for decision making purposes since the probability of cancer is suﬃciently large to merit action.

28.3.2 Bounding the marginal likelihood
In Bayesian modelling the likelihood of the model M with parameters θ generating data D is given by

(cid:90)

p(D|M) =

(cid:124)
(cid:123)(cid:122)
(cid:125)
p(D|θ,M)

likelihood

θ

(cid:124) (cid:123)(cid:122) (cid:125)
p(θ|M)

prior

(28.3.6)

This quantity is fundamental to model comparison. However, in cases where θ is high-dimensional, the
integral over θ is diﬃcult to perform. Using Bayes’ rule,

p(θ|D,M) = p(D|θ,M)p(θ|M)

p(D|M)

and considering

KL(q(θ)|p(θ|D,M)) = (cid:104)log q(θ)(cid:105)q(θ) − (cid:104)log p(θ|D,M)(cid:105)q(θ)

= (cid:104)log q(θ)(cid:105)q(θ) − (cid:104)log p(D|θ,M)p(θ|M(cid:105)q(θ) + log p(D|M)

the non-negativity of the Kullback-Leibler divergence gives the bound

log p(D|M) ≥ −(cid:104)log q(θ)(cid:105)q(θ) + (cid:104)log p(D|θ,M)p(θ|M(cid:105)q(θ)

(28.3.7)

(28.3.8)
(28.3.9)

(28.3.10)

This bound holds for any distribution q(θ) and saturates when q(θ) = p(θ|D,M). Since using the optimal
setting is assumed computationally intractable, the idea in variational bounding is to choose a distribution
family for q(θ) for which the bound is computationally tractable (for example factorised, or Gaussian),
and then maximise the bound with respect to any free parameters of q(θ). The resulting bound then can
be used as a surrogate for the exact marginal likelihood in model comparison.

28.3.3 Gaussian approximations using KL divergence
Minimising KL(q|p)
Using a simple approximation q(x) of a more complex distribution p(x) by minimising KL(q|p) tends to
give a solution for q(x) that focuses on a local mode of p(x), thereby underestimating the variance of p(x).
To show this, consider approximating a mixture of two Gaussians with equal variance σ2,

(cid:0)x µ, σ2(cid:1)(cid:1)

N

(cid:0)x − µ, σ2(cid:1) + N
(cid:0)
(cid:0)x m, s2(cid:1)

p(x) =

1
2

q(x) = N

see ﬁg(28.1), with a single Gaussian

We wish to ﬁnd the optimal m, s2 that minimise

KL(q|p) = (cid:104)log q(x)(cid:105)q(x) − (cid:104)log p(x)(cid:105)q(x)

(28.3.11)

(28.3.12)

(28.3.13)

If we consider the case that the two Gaussian components of p(x) are well separated, µ (cid:29) σ, then setting
q(x) to be centred on the left mode at −µ the Gaussian q(x) only has appreciable mass close to −µ, so
that the second mode at µ has negligible contribution to the Kullback-Leibler divergence. In this sense
one can approximate p(x) ≈ 1

2 q(x), so that

KL(q|p) ≈ (cid:104)log q(x)(cid:105)q(x) − (cid:104)log p(x)(cid:105)q(x) = log 2

DRAFT March 9, 2010

(28.3.14)

517

Properties of Kullback-Leibler Variational Inference

representing a distribution of the form (cid:81)

Figure 28.2: A planar pairwise Markov random ﬁeld on a set of variables x1, . . . , x25,
In statistical physics such
lattice models include the Ising model on binary ‘spin’ variables xi ∈ {+1,−1} with
φ(xi, xj) = ewij xixj .

i∼j φ(xi, xj).

On the other hand, setting m = 0, which is the correct mean of the distribution p(x), very little of the
mass of the mixture is captured unless s2 is large, giving a poor ﬁt and large KL divergence. Another
way to view this is to consider KL(q|p) = (cid:104)log q(x)/p(x)(cid:105)q(x); provided q is close to p around where q has
signiﬁcant mass, the ratio q(x)/p(x) will be order 1 and the KL divergence small. Setting m = 0 means
that q(x)/p(x) is large where q has signiﬁcant mass, and is therefore a poor ﬁt. The optimal solution in
this case is to place the Gaussian close to a single mode. Note, however, that for two modes that are less
well-separated, the optimal solution will not necessarily be to place the Gaussian around a local mode.
In general, the optimal Gaussian ﬁt needs to be determined numerically – that is, there is no closed form
solution to ﬁnding the optimal mean and (co)variance parameters.

Minimising KL(p|q)
For ﬁtting a Gaussian q to p based on KL(p|q), we have

log det(cid:0)σ2(cid:1) + const.

KL(p|q) = (cid:104)log p(x)(cid:105)p(x) − (cid:104)log q(x)(cid:105)p(x)

1
2σ2

= −

1
2

p(x) −

(cid:68)
(x − m)2(cid:69)
σ2 =(cid:10)(x − m)2(cid:11)

p(x)

Minimising this with respect to m and σ2 we obtain:

m = (cid:104)x(cid:105)p(x) ,

so that the optimal Gaussian ﬁt matches the ﬁrst and second moments of p(x).

In the case of ﬁg(28.1), the mean of p(x) is a zero, and the variance of p(x) is large. This solution is
therefore dramatically diﬀerent from that produced by ﬁtting the Gaussian using KL(q|p). The ﬁt found
using KL(q|p) focusses on making q ﬁt p locally well, whereas KL(p|q) focusses on making q ﬁt p well to
the global statistics of the distribution (possibly at the expense of a good local match).

For simplicity, consider a factorised approximation q(x) =(cid:81)

28.3.4 Moment matching properties of minimising KL(p|q)
i q(xi). Then

(28.3.15)

(28.3.16)

(28.3.17)

(28.3.19)

(cid:104)log q(xi)(cid:105)p(xi)

(28.3.18)

The ﬁrst entropic term is independent of q(x) so that up to a constant independent of q(x), the above is

(cid:88)

i

KL(p|q) = (cid:104)log p(x)(cid:105)p(x) −
(cid:88)

KL(p(xi)|q(xi))

i

so that optimally q(xi) = p(xi). That is, the optimal factorised approximation is to set the factors of q(xi)
to the marginals of p(xi), exercise(265). For any approximating distribution in the exponential family,
minimising KL(p|q) corresponds to moment matching, see exercise(264). In practice, one generally cannot
compute the moments of p(x) (since the distribution p(x) is considered ‘intractable’), so that ﬁtting q to
p based only on KL(p|q) does not itself lead to a practical algorithm for approximate inference. Neverthe-
less, as we will see, it is a useful subroutine for local approximations, in particular Expectation Propagation.

518

DRAFT March 9, 2010

Variational Bounding Using KL(q|p)
28.4 Variational Bounding Using KL(q|p)
In this section we discuss how to ﬁt a distribution q(x) from some assumed family to an ‘intractable’
distribution p(x). As we saw above for the case of ﬁtting Gaussians, the optimal q needs to be found
numerically. This itself can be a complex task (indeed, formally this can be just as diﬃcult as performing
inference directly with the intractable p) and the reader may wonder why we trade a diﬃcult inference
task for a potentially diﬃcult optimisation problem. The general idea is that the optimisation problem has
some local smoothness properties that enable one to rapidly ﬁnd a reasonable optimum based on generic
optimisation methods. To make these ideas more concrete, we discuss a particular case of ﬁtting q to a
formally intractable p in section(28.4.1) below.

28.4.1 Pairwise Markov random ﬁeld

A canonical ‘intractable’ distribution is the pairwise Markov Random Field deﬁned on binary variables
xi ∈ {+1,−1}, i = 1, . . . , D,

p(x) =

1

(cid:80)
i,j wij xixj +(cid:80)
Z(w, b) e
(cid:80)
i,j wij xixj +(cid:80)

Z(w, b) =(cid:88)

e

i bixi

i bixi

Here the ‘partition function’ Z(w, b) ensures normalisation,

(28.4.1)

(28.4.2)

x

i = 1, the terms wiix2

i are constant and without loss of generality we may set wii to zero1. This
Since x2
gives an undirected distribution with connection geometry deﬁned by the weights w.
In practice, the
weights often deﬁne local interactions on a lattice, see ﬁg(28.2). A case for which inference in this model
is required is given in example(114).

p(y|x) =(cid:89)

i

(cid:80)
p(x) ∝ e

Example 114 (Bayesian image denoising). Consider a binary image, where x describes the state of the
clean pixels (±1 encoding). We assume a noisy pixel generating process that takes each clean pixel xi and
ﬂips its binary state:

p(yi|xi),

p(yi|xi) ∝ eγyixi

(28.4.3)

The probability that yi and xi are in the same state is eγ/(eγ + e−γ). Our interest is to the posterior
distribution on clean pixels p(x|y). In order to do this we need to make an assumption as to what clean
images look like. We do this using a MRF

i∼j wij xixj

(28.4.4)
for some settings of wij > 0, with ı ∼ j indicating that i and j are neighbours. This encodes the assumption
that clean images tend to have neighbouring pixels in the same state. An isolated pixel in a diﬀerent state
to its neighbours is unlikely under this prior. We now have the joint distribution

p(x, y) = p(x)(cid:89)

p(yi|xi)

i

see ﬁg(28.3), from which the posterior is given by

p(x|y) = p(y|x)p(x)

(cid:80)
x p(y|x)p(x) ∝ e

(cid:80)

i∼j wij xixj +(cid:80)

i γyixi

(28.4.5)

(28.4.6)

Quantities such as the MAP state (most a posteriori probable image), marginals p(xi|y) and the normali-
sation constant are of interest.

1Whilst inference with a general MRF is formally computationally intractable (no exact polynomial time methods are
known), two celebrated results that we mention in passing are that for the planar MRF model with pure interactions (b = 0),
the partition function is computable in polynomial time[157, 95, 177, 115, 243], as is the MAP state for attractive planar
Ising models w > 0 [121], see section(28.8).

DRAFT March 9, 2010

519

Variational Bounding Using KL(q|p)

Figure 28.3: A distribution on pixels. The ﬁlled nodes indicate observed noisy pixels,
the unshaded nodes a Markov Random Field on latent clean pixels. The task is
to infer the clean pixels given the noisy pixels. The MRF encourages the posterior
distribution on the clean pixels to contain neighbouring pixels in the same state.

Kullback-Leibler based methods

For the MRF we have

KL(q|p) = (cid:104)log q(cid:105)q −

bi (cid:104)xi(cid:105)q + log Z ≥ 0

Rewriting, this gives a bound on the log-partition function

ij

wij (cid:104)xixj(cid:105)q −

(cid:88)
(cid:88)
+(cid:88)
wij (cid:104)xixj(cid:105)q +(cid:88)
(cid:123)(cid:122)
(cid:124)

ij

i

i

energy

bi (cid:104)xi(cid:105)q

(cid:125)

(cid:123)(cid:122)
(cid:125)
log Z ≥ −(cid:104)log q(cid:105)q

(cid:124)

entropy

(28.4.7)

(28.4.8)

(28.4.9)

(28.4.10)

(28.4.11)

(28.4.12)

(28.4.13)

The bound saturates when q = p. However, this is of little help since we cannot compute the averages of
variables with respect to this intractable distribution (cid:104)xixj(cid:105)p ,(cid:104)xi(cid:105)p. The idea of a variational method is
to assume a simpler ‘tractable’ distribution q for which these averages can be computed, along with the
entropy of q. Minimising the KL divergence with respect to any free parameters of q(x) is then equivalent
to maximising the lower bound on the log partition function.

Factorised approximation

A simple ‘naive’ assumption is the fully factorised distribution

q(x) =(cid:89)

qi(xi)

i

The graphical model of this approximation is given in ﬁg(28.4a). In this case

(cid:104)log qi(cid:105)qi +(cid:88)

wij (cid:104)xixj(cid:105)q(xi,xj ) +(cid:88)

ij

bi (cid:104)xi(cid:105)q(xi)

i

log Z ≥ −

For a factorised distribution and bearing in mind that xi ∈ {+1,−1},

(cid:88)
(cid:26) 1

i

(cid:104)xixj(cid:105) =

(cid:104)xi(cid:105)(cid:104)xj(cid:105)

i = j
i (cid:54)= j

For a binary variable, one may use the convenient parametrization

qi(xi = 1) =

eθi

eθi + e−θi

so that

(cid:104)xi(cid:105)qi = +1 × q(xi = 1) − 1 × q(xi = −1) = tanh(θi)

q(x) =(cid:81)

tion.
tion.

Figure 28.4: (a): Naive Mean Field approximation
(b): A spanning tree approxima-
(c): A decomposable (hypertree) approxima-

i qi(xi).

(a)

(b)

(c)

520

DRAFT March 9, 2010

Variational Bounding Using KL(q|p)

This gives the following lower bound on the log partition function:

wij tanh(θi) tanh(θj) +(cid:88)

i

(cid:88)

i

H(θi) +(cid:88)
(cid:17)

i(cid:54)=j

−θi

eθ
i + e

− θi tanh(θi)

log Z ≥ B(θ) ≡
(cid:16)

H(θi) = log

bi tanh(θi)

(28.4.14)

(28.4.15)

where H(θi) is the binary entropy of a distribution parameterised according to equation (28.4.12):

Finding the best factorised approximation in the minimal Kullback-Liebler divergence sense then corre-
sponds to maximising the bound B(θ) with respect to the variational parameters θ.
The bound B, equation (28.4.14), is generally non-convex in θ and riddled with local optima. Finding the
globally optimal θ is therefore typically a computationally hard problem. It seems that we have simply
replaced a computationally hard problem of computing log Z by an equally hard computational problem of
maximising B(θ). Indeed, the ‘graphical structure’ of this optimisation problem matches exactly that of the
original MRF. However, the hope is that by transforming a diﬃcult discrete summation into a continuous
optimisation problem, we will be able to bring to the table techniques of continuous variable numerical
optimisation to ﬁnd a good approximation. A particularly simple optimisation technique is to diﬀerentiate
the bound equation (28.4.14) and equate to zero. Straightforward algebra leads to requirement that the
optimal solution satisﬁes the equations

wij tanh(θj), ∀i

(28.4.16)

θi = bi +(cid:88)

j(cid:54)=i

One may show that updating any θi according to equation (28.4.16) increases B(θ). This is called asy-
chronous updating and is guaranteed to lead to a (local) minimum of the KL divergence, section(28.4.3).

Once a converged solution has been identiﬁed, in addition to a bound on log Z, we can approximate

(cid:104)xi(cid:105)p ≈ (cid:104)xi(cid:105)q = tanh(θi)

Validity of the factorised approximation

(28.4.17)

When might one expect such a naive factorised approximation to work well? Clearly, if wij is very small
for i (cid:54)= j, the distribution p will be eﬀectively factorised. However, a more interesting case is when each
variable xi has many neighbours. It is useful to write the MRF as

p(x) =

ij wij xixj =

1
Z

eD(cid:80)

i xi

1
D

(cid:80)

eD(cid:80)

j wij xj =

1
Z

i xizi

(28.4.18)

(28.4.19)

where the local ‘ﬁelds’ are deﬁned as

1
D

zi ≡

wijxj

j

zi, each of the terms xj in the summation (cid:80)

We now invoke a circular (but self-consistent) argument: Let’s assume that p(x) is factorised. Then for
j wijxj is independent. Provided the wij are not strongly
correlated the conditions of validity of the central limit theorem hold[122], and each zi will be Gaussian
distributed. Assuming that each wij is O (1), the mean of zi is

The variance is

(cid:104)zi(cid:105) =
(cid:11)

(cid:10)z2

i

wij (cid:104)xj(cid:105) = O (1)
(cid:16)

D(cid:88)

w2
ik

k=1

1 − (cid:104)xk(cid:105)2(cid:17)

− (cid:104)zi(cid:105)2 =

1
D2

DRAFT March 9, 2010

= O (1/D)

(28.4.20)

(28.4.21)

521

e

1
Z

(cid:80)
(cid:88)

(cid:88)

j

1
D

Variational Bounding Using KL(q|p)

Hence the variance of the ﬁeld zi is much smaller than its mean value. As D increases the ﬂuctuations
around the mean diminish, and we may write

p(xi)

(28.4.22)

eD(cid:80)

p(x) ≈

1
Z

i xi(cid:104)zi(cid:105)

≈

(cid:89)

i

The assumption that p is approximately factorised is therefore self-consistent in the limit of MRFs with
a large number of neighbours. Hence the factorised approximation would appear to be reasonable in the
extreme limits (i) a very weakly connected system wij ≈ 0, or (ii) a large densely connected system.
The fully factorised approximation is also called the Naive Mean Field theory since for the MRF case it
assumes that we can replace the eﬀect of the neighbours by a mean of the ﬁeld at each site.

28.4.2 General mean ﬁeld equations

For a general intractable distribution p(x) on discrete or continuous x, the KL divergence between a
factorised approximation q(x) and p(x) is

Isolating the dependency of the above on a single factor q(xi) we have

(cid:104)log q(xi)(cid:105)q(xi) − (cid:104)log p(x)(cid:105)(cid:81)

i q(xi)

(cid:32)(cid:89)

i

KL

=(cid:88)

i

(cid:33)
q(xi)|p(x)
(cid:68)

(cid:104)log p(x)(cid:105)(cid:81)
(cid:104)log q(xi)(cid:105)q(xi) −
(cid:16)
(cid:104)log p(x)(cid:105)(cid:81)
(cid:16)
(cid:104)log p(x)(cid:105)(cid:81)

q(xi) ∝ exp

j(cid:54)=i q(xj )

(cid:17)

(cid:17)

j(cid:54)=i q(xj )

(cid:69)

j(cid:54)=i q(xj )

q(xi)

Up to a normalisation constant, this is therefore the KL divergence between q(xi) and a distribution
proportional to exp

so that the optimal setting for q(xi) satisﬁes

These are known as the mean-ﬁeld equations and deﬁne a new approximation factor in terms of the previous
approximation factors. Note that if the normalisation constant of p(x) is unknown, this presents no problem
since this constant is simply absorbed into the normalisation of the factors q(xi). In other words one may
replace p(x) with the unnormalised p∗(x) in equation (28.4.25). Beginning with an initial randomly chosen
set of distributions q(xi), the mean-ﬁeld equations are iterated until convergence. Asynchronous updating
is guaranteed to decrease the KL divergence at each stage, section(28.4.3).

28.4.3 Asynchronous updating guarantees approximation improvement

For a factorised variational approximation equation (28.4.23), we claim that each update equation (28.4.25)
reduces the Kullback-Leibler approximation error. To show this we write a single updated distribution as

(28.4.23)

(28.4.24)

(28.4.25)

(28.4.26)

(28.4.27)

(28.4.28)

(28.4.29)

The joint distribution under this single update is

qnew
i =

1
Zi

exp(cid:104)log p(x)(cid:105)(cid:81)
(cid:89)

j(cid:54)=i qold

j

qnew = qnew

i

qold
j

j(cid:54)=i

(cid:16)
∆ ≡ KL(qnew|p) − KL

qold|p

Using

522

KL(qnew|p) = (cid:104)log qnew

i

(cid:105)qnew

i

(cid:17)
+(cid:88)

j(cid:54)=i

Our interest is the change in the approximation error under this single mean-ﬁeld update:

(cid:68)

log qold

j

(cid:69)

j −

qold

(cid:68)

(cid:104)log p(x)(cid:105)(cid:81)

(cid:69)

j(cid:54)=i qold

j

qnew
i

DRAFT March 9, 2010

Variational Bounding Using KL(q|p)

x1

x4

x2

x1

x3

x4

x2

x3

(a)

(b)

Figure 28.5: (a): A toy ‘intractable’ distribution.
(b): A structured singly-connected approximation.

j(cid:54)=i qold

j

= Ziqnew

(cid:69)

(cid:68)

qold

i −
log qold

i

i

(cid:68)
(cid:104)log p(cid:105)(cid:81)
(cid:69)
i − (cid:104)log q
qold
∗
i (cid:105)qold

i

+ (cid:104)log q

qold
i

(cid:69)

(cid:68)

+

(cid:104)log p(cid:105)(cid:81)

j(cid:54)=i qold

j

qnew
i

∗
i (cid:105)qnew

i

+ (cid:104)log q

∗
i (cid:105)qold

i

and deﬁning the un-normalised distribution

q

then

∗

i (xi) = exp(cid:104)log p(x)(cid:105)(cid:81)
(cid:68)

∆ = (cid:104)log qnew

log qold

i

(cid:105)qnew
i −
(cid:68)
(cid:69)
i − log Zi −

log qold

i

i
∗
= (cid:104)log q
i (cid:105)qnew
(cid:16)
= − log Zi −
qold
= −KL
(cid:17)
(cid:16)
i
qnew|p

i

|qnew
(cid:16)
≤ KL

(cid:17)

≤ 0
(cid:17)

qold|p

Hence

KL

(cid:69)

j(cid:54)=i qold

j

qold
i

(28.4.30)

(28.4.31)

(28.4.32)

(28.4.33)

(28.4.34)

(28.4.35)

so that updating a single component of q at a time is guaranteed to improve the approximation. Note
that this result is quite general, holding for any distribution p(x).
In the case of a Markov network
the guaranteed approximation improvement is equivalent to a guaranteed increase (strictly speaking a
non-decrease) in the lower bound on the partition function.

28.4.4 Intractable energy

Whilst the fully factorised approximation is rather severe, even this may not be enough to render the mean-
j(cid:54)=i q(xj ). For

ﬁeld equations tractably implementable. To do so we need to be able to compute (cid:104)log p∗(x)(cid:105)(cid:81)

some models of interest this is still not possible and additional approximations are required.

(cid:0)w 0, s2I(cid:1)(cid:89)

(cid:16)

cnwTxn(cid:17)

Example 115 (‘Intractable’ mean-ﬁeld approximation). Consider the posterior distribution from a Rel-
evance Vector Machine classiﬁcation problem, section(18.2.4):

p(w|D) ∝ N

The terms σ(cid:0)cnwTxn(cid:1) render p(w|D) non-Gaussian. We can ﬁnd a Gaussian approximation q(w) =

(28.4.36)

σ

n

N (w µ, Σ) by minimising the Kullback-Leibler divergence
KL(q(w)|p(w|D)) = (cid:104)log q(w)(cid:105)q(w) − (cid:104)log p(w|D)(cid:105)q(w)
(cid:68)

(cid:16)

cnwTxn(cid:17)(cid:69)

log σ

N (w µ,Σ)

The entropic term is straightforward since this is the negative entropy of a Gaussian. However, we also
require the ‘energy’ which includes a contribution

(28.4.37)

(28.4.38)

There is no closed form expression for this. One approach is to use additional variational approximations,
[141, 238]. Another approach is to recognise that the multi-variate average can be reduced to a uni-variate
Gaussian average:

N (w µ,Σ)

= (cid:104)log σ (cna)(cid:105)N (a m,τ 2) , m = µTxn, τ 2 = (xn)T Σxn

(28.4.39)

(cid:68)

log σ

(cid:16)

cnwTxn(cid:17)(cid:69)

DRAFT March 9, 2010

523

Mutual Information Maximisation : A KL Variational Approach

and the uni-variate Gaussian average can be carried out using quadrature. This approach was used in [22]
to approximate the posterior distribution of Bayesian Neural Networks.

28.4.5 Structured variational approximation

One can extend the factorised KL variational approximation by using non-factorised q(x)[239, 24]. Those
for which averages of the variables can be computed in linear time include spanning trees, ﬁg(28.4b) and
decomposable graphs ﬁg(28.4c). For example, for the distribution

p(x1, x2, x3, x4) =

1
Z

φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)φ(x1, x3)

(28.4.40)

a tractable q distribution would be, ﬁg(28.5)

q(x1, x2, x3, x4) =

˜φ(x1, x2) ˜φ(x1, x3) ˜φ(x1, x4)

(28.4.41)

1
˜Z

In this case we have

KL(q|p) = Hq(x1, x2) + Hq(x1, x3) + Hq(x1, x4) − 3Hq(x1) +(cid:88)

(cid:104)log φ(xi, xj)(cid:105)q(xi,xj )

(28.4.42)

i∼j

Since q is singly-connected, computing the marginals and entropy is straightforward (since the entropy
requires only pairwise marginals on graph neighbours).

More generally one can exploit any structural approximation with an arbitrary hypertree width w by use of
the junction tree algorithm in combination with the KL divergence. However, the computational expense
increases exponentially with the hypertree width[294].

28.5 Mutual Information Maximisation : A KL Variational Approach

Here we take a short interlude here to demonstrate an application of the Kullback-Leibler variational
approach in information theory. A fundamental goal is to maximise information transfer, measured by the
(see also deﬁnition(87))

I(X, Y ) ≡ H(X) − H(X|Y )

where the and are deﬁned

H(X) ≡ −(cid:104)log p(x)(cid:105)p(x) ,

H(X|Y ) ≡ −(cid:104)log p(x|y)(cid:105)p(x,y)

(28.5.1)

(28.5.2)

Here we are interested in the situation in which p(x) is ﬁxed, but p(y|x, θ) has adjustable parameters θ that
we wish to set in order to maximise I(X, Y ). In this case H(X) is constant and the optimisation problem
is equivalent to minimising the conditional entropy H(X|Y ). Unfortunately, in many cases of practical
interest H(X|Y ) is computationally intractable. See example(116) below for a motivating example. We
discuss in section(28.5.1) a general procedure based on the Kullback-Leibler divergence to approximately
maximise the mutual information.

Example 116. Consider a neural transmission system in which xi ∈ {0, 1} denotes an emitting neuron
in a non-ﬁring state (0) or ﬁring state (1), and yj ∈ {0, 1} a receiving neuron. If each receiving neuron
ﬁres independently, depending only on the emitting neurons, we have

p(yi|x)

(28.5.3)

DRAFT March 9, 2010

p(y|x) =(cid:89)

i

524

Mutual Information Maximisation : A KL Variational Approach

x1

x2

x3

x4

y1

y2

y3

y4

Algorithm 29 IM algorithm

bution p(x) = (cid:81)
σ(cid:0)wT
i x(cid:1), ﬁnd the optimal parameters wi that maximise the mutual

Figure 28.6: An information transfer problem. For a ﬁxed distri-
i p(xi) and parameterised distributions p(yj|x) =
information between the variables x and y. Such considerations are
popular in theoretical neuroscience and aim to understand how the
receptive ﬁelds wi of a neuron relate to the statistics of the environ-
ment p(x).

1: Choose a class of approximating distributions Q (for example factorised)
2: Initialise the parameters θ
3: repeat
4:

For ﬁxed q(x|y), ﬁnd

θnew = argmax

˜I(X, Y )

θ

5:

For ﬁxed θ,

qnew(x|y) = argmax
q(x|y)∈Q

˜I(X, Y )

where Q is a chosen class of distributions.

6: until converged

where for example we could use

(cid:16)

(cid:17)

p(yi = 1|x) = σ

wT
i x

p(x) =(cid:89)

p(xi)

If we make the simple assumption that emitting neurons ﬁre independently,

(28.5.6)

(28.5.7)

(28.5.4)

(28.5.5)

i

then for p(x|y) all components of the x variable are dependent, see ﬁg(28.6). This deﬁnes a complex
high-dimensional p(x, y) for which the conditional entropy is typically intractable.

28.5.1 The information maximisation algorithm

Consider

This immediately gives a bound

KL(p(x|y)|q(x|y)) ≥ 0
(cid:88)
p(x|y) log p(x|y) −
(cid:88)

x

(cid:88)

x

Multiplying both sides by p(y), we obtain

p(x|y) log q(x|y) ≥ 0
(cid:88)

⇒

x,y

p(y)p(x|y) log p(x|y) ≥

p(x, y) log q(x|y)

x,y

From the deﬁnition, the left of the above is −H(X|Y ). Hence

I(X, Y ) ≥ H(X) + (cid:104)log q(x|y)(cid:105)p(x,y) ≡ ˜I(X, Y )

DRAFT March 9, 2010

(28.5.8)

(28.5.9)

(28.5.10)

(28.5.11)

525

Loopy Belief Propagation

c

a

e

d

b

f

Figure 28.7: Classical Belief Propagation can be derived by con-
sidering how to compute the marginal of a variable on a MRF.
In this case the marginal p(d) depends on messages transmitted
via the neighbours of d. By deﬁning local messages on the links
of the graph, a recursive algorithm for computing all marginals
can be derived, see text.

From this lower bound on the mutual information we arrive at the information maximisation (IM)
algorithm[20]. Given a distribution p(x) and a parameterised distribution p(y|x, θ), we seek to max-
imise ˜I(X, Y ) with respect to θ. A co-ordinate wise optimisation procedure is presented in algorithm(29).
The Blahut-Arimoto algorithm in information theory (see for example [184]) is a special case in which the
optimal decoder

q(x|y) ∝ p(y|x, θ)p(x)

(28.5.12)

is used. In applications where the Blahut-Arimoto algorithm is intractable to implement, the IM algorithm
can provide an alternative by restricting q to a tractable family of distributions (tractable in the sense
that the lower bound can be computed). The Blahut-Arimoto algorithm is analogous to the EM algorithm
for Maximum Likelihood and guarantees a non-decrease of the mutual information at each stage of the
update. Similarly, the IM procedure is analogous to a Generalised EM procedure and each step of the
procedure cannot decrease the lower bound on the mutual information.

28.5.2 Linear Gaussian decoder

A special case of the IM framework is to use a linear Gaussian decoder

q(x|y) = N (x Uy, Σ) ⇒ log q(x|y) = (x − Uy)T Σ−1 (x − Uy)

(28.5.13)
Plugging this into the MI bound, equation (28.5.11), and optimising with respect to Σ, and U, we obtain

(cid:68)

(x − Uy) (x − Uy)T(cid:69)

Σ =

I(X, Y ) ≥ H(X) −

1
2

log det

,

(cid:18)(cid:68)

U =

xxT(cid:69)

−

where (cid:104)·(cid:105) ≡ (cid:104)·(cid:105)p(x,y). Using this in the MI bound we obtain

xyT(cid:69)(cid:68)
(cid:68)
xyT(cid:69)(cid:68)
(cid:68)

yyT(cid:69)−1
yyT(cid:69)−1(cid:68)

yxT(cid:69)(cid:19)

(28.5.14)

+ const.

(28.5.15)

Up to irrelevant constants, this is equivalent to Linsker’s as-if-Gaussian approximation to the mutual
information [176]. One can therefore view Linsker’s approach as a special case of the IM algorithm
restricted to linear-Gaussian decoders. In principle, one can therefore improve on Linsker’s method by
considering more powerful non-linear-Gaussian decoders. Applications of this technique to Neural systems
are discussed in [20].

28.6 Loopy Belief Propagation

Belief Propagation is a technique for exact inference of marginals p(xi) for singly-connected distributions
p(x). There are diﬀerent formulations of BP, the most modern treatment being the sum-product algo-
rithm on the corresponding factor graph, as described in section(5.1.2). An important observation is that
the algorithm is purely local – the updates are unaware of the global structure of the graph, depending
only on the local neighbourhood structure. This means that even if the graph is multiply-connected (it
is loopy) one can still apply the algorithm and ‘see what happens’. Provided the loops in the graph are
relatively long, one may hope that running ‘loopy’ BP will converge to a good approximation of the true
marginals. In general, this cannot be guaranteed; however, when the method converges, the results can
be surprisingly accurate.

In the following we will show how loopy BP can also be motivated by a variational objective. To do so, the
most natural connection is with the classical BP algorithm (rather than the factor graph sum-product)
algorithm. For this reason we brieﬂy describe below the classical BP approach.

526

DRAFT March 9, 2010

Loopy Belief Propagation

x2

λ

x

2→

x

i(
x

i)

xi

λxi→xj (xj)

xj

x1

λx1→xi (xi)
(x i)
→ x i

λ x 3

x3

Figure 28.8: Loopy Belief Propagation. Once a node has received
incoming messages from all neighbours (excluding the one it wants to
send a message to), it may send an outgoing message to a neighbour:

φ (xi, xj)λx1→xi (xi) λx2→xi (xi) λx3→xi (xi)

λxi→xj (xj) =(cid:88)

xi

28.6.1 Classical BP on an undirected graph

graph. Consider calculating the marginal p(d) = (cid:80)
work in ﬁg(28.7). We denote both a node and its state by the same symbol, so that (cid:80)

BP can be derived by considering how to calculate a marginal in terms of messages on an undirected
a,b,c,e,f p(a, b, c, d, e, f) for the pairwise Markov net-
b φ (d, b) denotes
summation over the states of the variable b. Carrying out such a summation results in a ‘message’ λb→d (d)
containing information from node b to node d. In order to compute the summation eﬃciently, we distribute
summations as follows:

p(d) =

1
Z

(cid:88)
(cid:124)

b

(cid:123)(cid:122)

(cid:88)
(cid:124)

a

(cid:125)

(cid:123)(cid:122)

(cid:88)
(cid:124)

f

(cid:125)

(cid:123)(cid:122)

φ (b, d)

φ (a, d)

φ (d, f)

λb→d(d)

λa→d(d)

λf→d(d)

(cid:88)
(cid:124)

e

φ (d, e)(cid:88)
(cid:124)
(cid:123)(cid:122)

c

λe→d(d)

(cid:125)

φ (c, e)

(cid:123)(cid:122)

λc→e(e)

(cid:125)
(cid:125)

(28.6.1)

where we deﬁne messages λn1→n2 (n2) sending information from node n1 to node n2 as a function of the
state of node n2. In general, a node xi passes a message to node xj via

λxi→xj (xj) =(cid:88)

φ (xi, xj) (cid:89)

xi

k∈ne(i),k(cid:54)=j

λxk→xi (xi)

(28.6.2)

This algorithm is equivalent to the sum-product algorithm provided the graph is singly-connected.

28.6.2 Loopy BP as a variational procedure

A variational procedure that corresponds to loopy BP can be derived by considering the terms of a standard
variational approximation based on the Kullback-Leibler divergence KL(q|p)[299]. For a pairwise MRF
deﬁned on potentials φ(xi, xj),

and approximating distribution q(x), the Kullback-Leibler bound is

(cid:89)

i∼j

p(x) =

1
Z

φ(xi, xj)

log Z ≥ −(cid:104)log q(x)(cid:105)q(x) +(cid:88)
(cid:124)

i∼j

(cid:125)
(cid:104)log φ(xi, xj)(cid:105)q(x)

(cid:123)(cid:122)

energy

Since

(cid:104)log φ(xi, xj)(cid:105)q(x) = (cid:104)log φ(xi, xj)(cid:105)q(xi,xj )

(28.6.3)

(28.6.4)

(28.6.5)

each contribution to the energy depends on q(x) only via the pairwise marginals q(xi, xj). This suggests
that these marginals should form the natural parameters of any approximation. Can we then ﬁnd an
expression for the entropy −(cid:104)log q(x)(cid:105)q(x) in terms of these pairwise marginals? Consider a case in which
the required marginals are

q(x1, x2), q(x2, x3), q(x3, x4)

DRAFT March 9, 2010

(28.6.6)

527

Loopy Belief Propagation

Given these marginals, the energy term is straightforward to compute, and we are left with requiring only
the entropy of q. Either by appealing to the junction tree representation, or by straightforward algebra,
one can show that we can uniquely express q in terms of the marginals as

q(x) = q(x1, x2)q(x2, x3)q(x3, x4)

q(x2)q(x3)

(28.6.7)

An intuitive way to arrive at this result is by examining the numerator of equation (28.6.7). The variable
x2 appears twice, as does the variable x3 and, since any joint distribution cannot have replicated variables
on the left of any conditioning, we must compensate for the additional x2 and x3 variables by dividing by
these marginals. In this case, the entropy of q(x) can be written as

Hq(x) = −(cid:104)log q(x)(cid:105)q(x) = Hq(x1, x2) + Hq(x2, x3) + Hq(x3, x4) − Hq(x2) − Hq(x3)

More generally, chapter(6), any decomposable graph can be represented as

(cid:81)
(cid:81)
c q(Xc)
s q(Xs)

q(x) =

(28.6.8)

(28.6.9)

where the q(Xc) are the marginals deﬁned on cliques of the graph, with Xc being the variables of the
clique, and the q(Xs) are deﬁned on the separators (intersections of neighbouring cliques). The expression
for the entropy of the distribution is then given by a sum of marginal entropies minus the entropy of the
marginals deﬁned on the separators.

Bethe Free energy

Consider now a MRF corresponding to a non-decomposable graph, for example the 4-cycle

p(x) =

1
Z

φ(x1, x2)φ(x2, x3)φ(x3, x4)φ(x4, x1)

The energy requires therefore that we know

q(x1, x2), q(x2, x3), q(x3, x4), q(x4, x1)

(28.6.10)

(28.6.11)

Assuming that these marginals are given, can we ﬁnd an expression for the entropy of the joint distribution
q(x1, x2, x3, x4) in terms of its pairwise marginals q(xi, xj)? In general this is not possible since the graph
corresponding to the marginals contains loops (so that the junction tree representation would result in
cliques greater than size 2). However, a simple ‘no overcounting’ approximation is to write

q(x1, x2)q(x2, x3)q(x3, x4)q(x4, x1)

q(x1)q(x2)q(x3)q(x4)

q(x) ≈
(cid:88)

subject to the constraints

q(xi, xj) = q(xj)

(28.6.12)

(28.6.13)

xi

An entropy approximation using this representation is therefore

Hq(x) ≈ Hq(x1, x2) + Hq(x2, x3) + Hq(x3, x4) + Hq(x1, x4) −

4(cid:88)

i=1

Hq(xi)

(28.6.14)

With this approximation the log partition function is known in statistical physics as the (negative) Bethe
free energy. Our interest is then to maximise this expression with respect to the parameters q(xi, xj)
xi q(xi, xj) = q(xj). These may be enforced using Lagrange

(cid:104)log φ(xi, xj)(cid:105)q(xi,xj ) +(cid:88)

i∼j

(cid:32)
q(xi) −

(cid:88)

xi

λi,j

(cid:33)

q(xi, xj)

DRAFT March 9, 2010

multipliers. One can write the Bethe free energy as

subject to marginal consistency constraints,(cid:80)
Hq(xi) +(cid:88)

Hq(xi, xj) +(cid:88)

F(q, λ) ≡ −

(cid:88)

i∼j

i

i∼j

528

Loopy Belief Propagation

where i ∼ j denotes the unique neighbouring edges on the graph (each edge is counted only once). This
is no longer a bound on the log partition function since the entropy approximation is not a lower bound on
the true entropy. The task is now to maximise this ‘approximate bound’ with respect to the parameters,
namely all the pairwise marginals q(xi, xj) and the Lagrange multipliers λ.

(28.6.15)

A simple scheme to maximise equation (28.6.15) is to use a ﬁxed point iteration by equating the derivatives
of the Bethe free energy with respect to the parameters q(xi, xj) to zero, and likewise for the Lagrange
multipliers. One may show that the resulting set of ﬁxed point equations, on eliminating q, is equivalent
to (undirected) Belief Propagation[299] for which, in general, a node xi passes a message to node xj using

(28.6.16)

(28.6.17)

λxi→xj (xj) =(cid:88)

φ (xi, xj) (cid:89)

xi

k∈ne(i),k(cid:54)=j

λxk→xi (xi)

At convergence the marginal p(xi) is then approximated by

(cid:89)

q(xi) ∝

i∈ne(j)

λxj→xi (xi)

the prefactor being determined by normalisation. For a singly-connected distribution p, this message pass-
ing scheme converges and the marginal corresponds to the exact result. For multiply connected (loopy)
structures, running this loopy Belief Propagation will generally result in an approximation. Naturally,
we can dispense with the Bethe free energy if desired and run the associated loopy Belief Propagation
algorithm directly on the undirected graph.

The convergence of Loopy Belief Propagation which can be heavily dependent on the topology of the
graph and also the message updating schedule[291, 201]. The potential beneﬁt of the Bethe free energy
viewpoint is that it gives an objective that is required to be optimised, opening up the possibility of more
general optimisation techniques than BP. The so-called double-loop techniques iteratively isolate convex
contributions to the Bethe Free energy, interleaved with concave contributions. At each stage, the resulting
optimisiations can be carried out eﬃciently[301, 133, 299].

Validity of loopy belief propagation

For a MRF which has a loop, computationally this means that a perturbation in a variable on the loop
eventually reverberates to the same variable. However, if there are a large number of variables in the loop,
and the individual neighbouring links are not all extremely strong, the numerical eﬀect of the loop is small
in the sense that inﬂuence of the variable on itself is negligible. In such cases one would expect the loopy
BP approximation to be accurate. An area of particular success for Loopy Belief Propagation inference
is in error correction based on low density parity check codes, which are designed to have this long-
loop property[183]. In many examples of practical interest (for example an MRF with nearest neighbour
interactions on a lattice), however, loops can be very short. In such cases a naive implementation of Loopy
BP will fail. A natural extension is to cluster variables to alleviate some of the issues arising from strong
local dependencies; this technique is called the Kikuchi or Cluster Variation method[154]. More elaborate
ways of clustering variables can be considered using region graphs[299, 292].

Example 117. The ﬁle demoMFBPGibbs.m compares the performance of naive Mean Field theory, Belief
Propagation and unstructured Gibbs sampling on marginal inference in a pairwise Markov network

p(w, x, y, z) = φwx(w, x)φwy(w, y)φwz(w, z)φxy(x, y)φxz(x, z)φyz(y, z)

(28.6.18)

in which all variables take 6 states. In the experiment the tables are selected from a uniform distribution
raised to a power α. For α close to zero, all the tables are essentially ﬂat and therefore the variables become
independent, a situation for which MF, BP and Gibbs sampling are ideally suited. As α is increased to

DRAFT March 9, 2010

529

Expectation Propagation

(a)

(b)

(c)

(a): The Markov network (left)

Figure 28.9:
that we wish to approximate the marginals
p(w), p(x), p(y), p(z) for. All tables are drawn from a uniform distribution raised to a power α. On
(b): There are 64 = 1295
the right is shown the naive mean ﬁeld approximation factorised structure.
states of the distribution. Shown is a randomly sampled distribution for α = 5 which has many isolated
peaks, suggesting the distribution is far from factorised. In this case the MF and Gibbs sampling approx-
(c): As α is increased to 25, typically only one state of the distribution
imations may perform poorly.
dominates. See demoMFBPGibbs.m.

x1

x4

x2

x1

x2

x1

x3

x4

x3

x4

(a)

(b)

(c)

x2

x3

Figure 28.10: (a): Multiply-connected factor
(b): Expectation
graph representing p(x).
propagation approximates (a) in terms of a
tractable factor graph. The open squares indi-
cate that the factors are parameters of the ap-
proximation. The basic EP approximation is to
replace all factors in p(x) by product factors.
(c): Tree structured EP.

5, the dependencies amongst the variables increase and the methods perform worse, especially MF and
Gibbs. As α is increased to 25, the distribution becomes sharply peaked around a single state, such that
the posterior is eﬀectively factorised, see ﬁg(28.9). This suggests that a MF approximation (and also Gibbs
sampling) should work well. However, ﬁnding this state is computationally diﬃcult and both methods
often get stuck in local minima. Belief propagation seems less susceptible to being trapped in local minima
in this regime and tends to outperform both MF and Gibbs sampling.

28.7 Expectation Propagation

The messages in schemes such as belief propagation are not always representable in a compact form. The
Switching Linear Dynamical System, chapter(25) is such an instance, in which the messages require an
exponential amount of storage. This limits BP to cases such as discrete networks, or more generally expo-
nential family messages. Expectation Propagation extends the applicability of BP to cases in which the
messages are not in the exponential family by projecting the messages back to the exponential family at
each stage. This projection is obtained by using a Kullback-Leibler measure[195, 246, 197].

Consider a distribution of the form

(cid:89)

i

p(x) =

1
Z

φi(X )

(28.7.1)

In EP one identiﬁes those factors φi(X ) which, if replaced by simpler factors ˜φi(X ), would render the
distribution ˜p(x) tractable. One then sets any free parameters of ˜φi(X ) by minimising the Kullback-
530

DRAFT March 9, 2010

wxyzoriginal graphwxyzFactorised graph0500100000.050.10.150.2p distribution0500100000.20.40.60.81p distributionExpectation Propagation

Leibler divergence KL(p|˜p). For example, consider a pairwise MRF

p(x) =

1
Z

φ1,2(x1, x2)φ2,3(x2, x3)φ3,4(x3, x4)φ4,1(x4, x1)

(28.7.2)

with Factor Graph as depicted in ﬁg(28.10a). If we replace all terms φij(xi, xj) by approximate factors
˜φij(xi) ˜φij(xj) then the resulting joint distribution ˜p would be factorised and hence tractable. Since the
variable xi appears in more than one term from p(x), we need to index the approximation factors. A
convenient way to do this is

˜p =

1
˜Z

˜φ2→1 (x1) ˜φ1→2 (x2) ˜φ3→2 (x2) ˜φ2→3 (x3) ˜φ4→3 (x3) ˜φ3→4 (x4) ˜φ1→4 (x4) ˜φ4→1 (x1)

(28.7.3)

which is represented in ﬁg(28.10b). The idea in EP is now to determine the optimal approximation term
by the self-consistent requirement that, on replacing it with its exact form, there is no diﬀerence to the
marginal of ˜p. Consider the approximation parameters ˜φ3→2 (x2) and ˜φ2→3 (x3). To set these we ﬁrst
replace the contribution ˜φ3→2 (x2) ˜φ2→3 (x3) by φ(x2, x3). This gives

˜p∗ =

1
˜Z∗

˜φ2→1 (x1) ˜φ1→2 (x2) φ(x2, x3) ˜φ4→3 (x3) ˜φ3→4 (x4) ˜φ1→4 (x4) ˜φ4→1 (x1)

(28.7.4)

Now consider the Kullback-Leibler divergence between this distribution and our approximation,

KL(˜p∗|˜p) = (cid:104)log ˜p∗(cid:105)˜p∗ − (cid:104)log ˜p(cid:105)˜p∗

(28.7.5)

Since our interest is in updating ˜φ3→2 (x2) and ˜φ2→3 (x3), we isolate the contribution from these parameters
in the Kullback-Leibler divergence which gives

(cid:68)

(cid:69)

KL(˜p∗|˜p) = log ˜Z −

log ˜φ3→2 (x2) ˜φ2→3 (x3)

˜p∗(x2,x3)

+ const.

(28.7.6)

Also, since ˜p is factorised, up to a constant proportionality factor, the dependence of ˜Z on ˜φ3→2 (x2) and
˜φ2→3 (x3) is
˜Z ∝

˜φ1→2 (x2) ˜φ3→2 (x2)(cid:88)

˜φ2→3 (x3) ˜φ4→3 (x3)

(28.7.7)

(cid:88)

x2

x3

Diﬀerentiating the Kullback-Leibler divergence equation (28.7.6) with respect to ˜φ3→2 (x2) and equating
to zero, we obtain

(cid:80)

(cid:80)

˜φ1→2 (x2) ˜φ3→2 (x2)

˜φ1→2 (x2) ˜φ3→2 (x2)

x2

= ˜p∗(x2)

Similarly, optimising w.r.t. ˜φ2→3 (x3) gives

˜φ2→3 (x3) ˜φ4→3 (x3)

˜φ2→3 (x3) ˜φ4→3 (x3)

x3

= ˜p∗(x3)

(28.7.8)

(28.7.9)

These equations only determine the approximation factors up to a proportionality constant. We can
therefore write the optimal updates as

˜φ3→2 (x2) = z3→2

˜p∗(x2)
˜φ1→2 (x2)

and

˜φ2→3 (x3) = z2→3

˜p∗(x3)
˜φ4→3 (x3)

DRAFT March 9, 2010

(28.7.10)

(28.7.11)

531

where z3→2 and z2→3 are proportionality terms. We can determine the proportionalities by the requirement
that the term approximation ˜φ3→2 (x2) ˜φ2→3 (x3) has the same eﬀect on the normalisation of ˜p as it has

Expectation Propagation

on ˜p∗. That is(cid:88)

x1,x2,x3,x4

= (cid:88)

˜φ2→1 (x1) ˜φ1→2 (x2) ˜φ3→2 (x2) ˜φ2→3 (x3) ˜φ4→3 (x3) ˜φ3→4 (x4) ˜φ1→4 (x4) ˜φ4→1 (x1)

˜φ2→1 (x1) ˜φ1→2 (x2) φ(x2, x3) ˜φ4→3 (x3) ˜φ3→4 (x4) ˜φ1→4 (x4) ˜φ4→1 (x1)

(28.7.12)

x1,x2,x3,x4

which, on substituting in the updates equation (28.7.10) and equation (28.7.11), reduces to

z2→3z3→2 =

z∗
2,3
˜z2,3

where

and

z

˜z2,3 = (cid:88)
2,3 = (cid:88)

∗

x2,x3

x2,x3

˜φ1→2 (x2)

˜p∗(x2)
˜φ1→2 (x2)

˜p∗(x3)
˜φ4→3 (x3)

˜φ4→3 (x3)

˜φ1→2 (x2) φ(x2, x3) ˜φ4→3 (x3)

(28.7.13)

(28.7.14)

(28.7.15)

Any choice of local normalisations z2→3, z3→2 that satisﬁes equation (28.7.13) suﬃces to ensure that the
scale of the term approximation matches. For example, one may set

z2→3 = z3→2 =

z∗
2,3
˜z2,3

(28.7.16)

(cid:115)

Once set, an approximation for the global normalisation constant of p is

Z ≈ ˜Z

(28.7.17)
The above gives a procedure for updating the terms ˜φ3→2 (x2) and ˜φ2→3 (x3). One then chooses another
term and replaces it with its approximation, until the parameters of the approximation converge. The
generic procedure is outlined in algorithm(30).

Comments on EP

• For the MRF example above, EP corresponds to Belief Propagation (the sum-product form on the
factor graph). This is intuitively clear since in both EP and BP the product of messages incoming
to a variable is proportional to the approximation of the marginal of that variable. A diﬀerence,
however, is the schedule:
in EP all messages corresponding to a term approximation are updated
simultaneously (in the above ˜φ3→2 (x2) and ˜φ2→3 (x3)), whereas in BP they are updated in arbitrary
order.

• EP is a useful extension of BP to cases in which the BP messages cannot be easily represented. In the
case that the approximating distribution ˜p is in the exponential family, the minimal Kullback-Leibler
criterion equates to matching moments of the approximating distribution to p∗. See [246] for a more
detailed discussion.

• In general there is no need to replace all terms in the joint distribution with factorised approximations.
One only needs that the resulting approximating distribution is tractable; this results in a structured
Expectation Propagation algorithm, see ﬁg(28.10c).

• EP and its extensions are closely related to other variational procedures such as tree-reweighting[287]

and fractional EP[295] designed to compensate for message overcounting eﬀects.

532

DRAFT March 9, 2010

Algorithm 30 Expectation Propagation: approximation of p(x) = 1
Z
1: Decide on a set of terms φi(Xi) to replace with ˜φi(Xi) in order to reveal a tractable distribution

i φi(Xi).

(cid:81)

MAP for MRFs

(cid:89)

i

˜p(x) =

1
˜Z

˜φi(Xi)

2: Initialise the all parameters ˜φi(Xi).

3: repeat
4:
5:

Select a term φi(Xi) from p to update.
Replace the term φi(Xi) by the tractable term ˜φi(Xi) to form
˜φ∗ ≡

φi(Xi) = φi(Xi)(cid:89)

(cid:81)
˜φj(Xj)
˜φi(Xi)

˜φj(Xj)

j(cid:54)=i

j

Find the parameters of ˜φi(Xi) by
˜φi(Xi) ∝ argmin
˜φi(Xi)

˜p(x) ∝

KL(˜p∗|˜p)
(cid:89)
˜φj(Xj) =(cid:88)

x

˜p∗ ∝ ˜φ∗,
(cid:88)
Set any proportionality terms of ˜φi(Xi) by requiring

˜φi(Xi)
(cid:89)

i

6:

7:

where

x

j(cid:54)=i

φi(Xi)(cid:89)
(cid:89)

1
˜Z

i

8: until converged
9: return

j

˜φj(Xj)
˜Z =(cid:88)

(cid:89)

x

i

˜p(x) =

˜φi(Xi),

˜φi(Xi)

(28.7.18)

(28.7.19)

(28.7.20)

(28.7.21)

(28.7.22)

(28.7.23)

as an approximation to p(x), where ˜Z approximates the normalisation constant Z.

28.8 MAP for MRFs

Consider a pairwise MRF p(x) ∝ eE(x) with

(cid:88)

i∼j

f (xi, xj) +(cid:88)

i

E(x) ≡

g(xi, x0
i )

(28.8.1)

where i ∼ j denotes neighbouring variables. Here the terms f (xi, xj) represent pairwise interactions. The
i ) represent unary interactions, written for convenience in terms of a pairwise interaction with
terms g(xi, x0
a ﬁxed (non-variable) x0. Typically the term f(xi, xj) is used to ensure that neighbouring variables xi and
xj are in similar states; the term g(xi, x0
i . Such models
have application in areas such as computer vision and image restoration in which an observed noisy image
x0 is to be cleaned, ﬁg(28.3). To do so we seek a clean image x for which each clean pixel value xi is close
to the observed noisy pixel value x0

i ) is used to bias xi to be close to a desired state x0

i , whilst being in a similar state to its clean neighbours.

28.8.1 MAP assignment

The MAP assignment of a set of variables x1, . . . , xD corresponds to that joint x that maximises E(x).
For a general graph connectivity we cannot naively exploit dynamic programming intuitions to ﬁnd an

DRAFT March 9, 2010

533

MAP for MRFs

b

c

f

(a)

a

e

d

a

e

b

c

f

(b)

Figure 28.11: (a): A graph with bidirectional weights
(b): A graph cut partitions the nodes
wij = wji.
into two groups S (blue) and T (red). The weight of
the cut is the sum of the edge weights from S (blue)
to T (red). Intuitively, it is clear that after assign-
ing nodes to state 1 (for blue) and 0 (red) that the
weight of the cut corresponds to the summed weights
of neighbours in diﬀerent states. Here we high-
light those weight contributions. The non-highlighted
edges do not contribute to the cut weight. Note that
only one of the edge directions contributes to the cut.

d

exact solution since the graph is loopy. As we see below, in special cases, even though the graph is loopy,
this is possible. In general, however, approximate algorithms are required.

A simple general approximate solution can be found as follows: ﬁrst initialise all x at random. Then select
a variable xi and ﬁnd the state of xi that maximally improves E(x), keeping all other variables ﬁxed. One
then repeats this selection and local maximal state computation until convergence. This is called Iterated
Conditional Modes[36]. Due to the Markov properties its clear that we can improve on this ICM method
by simultaneously optimising all variables conditioned on their respective Markov blankets (similar to the
approach used in black-white sampling). Another improvement is to update only a subset of the variables,
where the subset has the form of singly-connected structure. By recursively clamping variables to reveal
a singly-connected structure on un-clamped variables, one may ﬁnd an approximate solution by solving a
sequence of tractable problems.

Remarkably, in the special case of binary variables and positive w discussed below, an eﬃcient exact
algorithm exists for ﬁnding the MAP state, regardless of the topology.

28.8.2 Attractive binary MRFs
Consider ﬁnding the MAP of a MRF with binary variables dom(xi) = {0, 1} and positive connections
wij ≥ 0. In this case our task is to ﬁnd the assignment x that maximises

cixi

(28.8.2)

(cid:88)

i∼j

wijI [xi = xj] +(cid:88)

i

E(x) ≡

where i ∼ j denotes neighbouring variables and real ci. Note that for binary variables xi ∈ {0, 1},

(28.8.3)

For this particular case an eﬃcient MAP algorithm exists for arbitrary topology of w[121]. The algorithm
ﬁrst translates the MAP assignment problem into an equivalent min s-t-cut problem[39], for which eﬃcient
algorithms exist. In min s-t-cut, we need a graph with positive weights on the edges. This is clearly satisﬁed

i cixi need to be addressed.

(28.8.4)

(28.8.5)

DRAFT March 9, 2010

To translate the MAP assignment problem to a min-cut problem we need to deal with the additional linear
i cixi. First consider the eﬀect of including a new node x∗ and connecting this to each existing

Dealing with the bias terms

I [xi = xj] = xixj + (1 − xi)(1 − xj)
if wij > 0, although the bias terms(cid:80)
terms (cid:80)
(cid:88)
(cid:88)

ciI [xi = x∗] =(cid:88)

i

i

cixi

node i with weight ci. This adds then a term

If we set x∗ in state 1, this will then add terms

ci (xix∗ + (1 − xi) (1 − x∗))

i

534

MAP for MRFs

s

s

a+

e−

a

e

b−

c+

b

c

f+

(a)

f

(b)

d−

d

t

t

(a): A Graph with bidirectional
Figure 28.12:
weights wij = wji augmented with a source node s
and sink node t. Each node has a corresponding bias
whose sign is indicated. The source node is linked
to the nodes corresponding to positive bias, and the
(b): A graph
nodes with negative bias to the sink.
cut partitions the nodes into two groups S (blue) and
T (red), where S is the union of the source node and
nodes in state 1, T is the union of the sink node and
nodes in state 0. The weight of the cut is the sum of
the edge weights from S (blue) to T (red). The red
lines indicate contributions to the cut, and can be
considered penalties since we wish to ﬁnd the mini-
mal cut. For example a being in state 1 (blue) does
not incur a penalty since ca > 0; on the other hand,
variable f being in state 0 (red) incurs a penalty since
cf > 0.

Otherwise, if we set x∗ in state 0 we obtain

(cid:88)

i

ci (1 − xi) = −

(cid:88)

i

cixi + const.

(28.8.6)

Since our requirement is that we need the weights to be positive we see that we can achieve this by deﬁning
two additional nodes. We deﬁne a source node xs, set to state 1 and connect it to those xi which have
positive ci, deﬁning wsi = ci. In addition we deﬁne a sink node xt = 0 and connect all nodes with negative
ci, to xt, using weight wit = −ci, (which is therefore positive).
For the source node clamped to xs = 1 and the sink node to xt = 0, then including the source and sink,
we have

wijI [xi = xj] + const.

(28.8.7)

E(x) =(cid:88)

i∼j

is equal to the energy function, equation (28.8.2).

Deﬁnition 115 (Graph Cut). For a graph G with vertices v1, . . . , vD, and weights wij > 0 a cut is a
partition of the vertices into two disjoint groups, called S and T . The weight of a cut is then deﬁned as
the sum of the weights that leave S and land in T , see ﬁg(28.11).

The weight of a cut corresponds to the sum of weights between mismatched neighbours, see ﬁg(28.11b).
That is,

(28.8.8)

(28.8.9)

535

wijI [xi (cid:54)= xj]

cut(x) =(cid:88)
cut(x) =(cid:88)

i∼j

i∼j

Since I [xi (cid:54)= xj] = 1 − I [xi = xj], we can deﬁne the weight of the cut equivalently as

wij (1 − I [xi = xj]) = −

wijI [xi = xj] + const.

(cid:88)

i∼j

DRAFT March 9, 2010

MAP for MRFs

(a):

Clean im-
Figure 28.13:
(c):
age.
Restored image using ICM. See
demoMRFclean.m.

(b): Noisy image.

(a)

(b)

(c)

so that the minimal cut assignment will correspond to maximising(cid:80)
which take O(cid:0)D3(cid:1) operations or less. This means that one can ﬁnd the exact MAP assignment of an
attractive binary MRF eﬃciently in O(cid:0)D3(cid:1) operations. In MaxFlow.m we implement the Ford-Fulkerson

i∼j wijI [xi = xj]. In the MRF case,
our translation into a weighted graph with positive interactions then requires that we identify the source
and all other variables assigned to state 1 with S, and the sink and all variables in state 0 with T , see
ﬁg(28.12). A fundamental result is that the min s-t-cut solution corresponds to the max-ﬂow solution
from the source s to the sink t [39]. There are eﬃcient algorithms for max-ﬂow, see for example [47],

(Edmonds-Karp-Dinic breadth ﬁrst search variant)[87], see also exercise(251).

E(x) =(cid:88)

wijI [xi = xj] +(cid:88)

Example 118 (Analysing dirty pictures). In ﬁg(28.13) we present a noisy binary y image that we wish
to clean. To do so we use an objective

I [xi = yi]

(28.8.10)

ij

i

The variables xi, i = 1, . . . , 784 are deﬁned on a 28×28 grid and where wij = 10 if xi and xj are neighbours
on the grid. Using

I [xi = yi] = xi (2yi − 1) + const.

(28.8.11)

we have a standard binary MRF MAP problem with ‘bias’ b = 2y − 1. Once can then ﬁnd the exact
optimal x by the min-cut procedure. However, our implementation of this is slow and instead we use the
simpler ICM algorithm, with results as shown in ﬁg(28.13).

28.8.3 Potts model

An extension of the previous model is to the case when the variables are non-binary, which is termed the
Potts model:

E(x) =(cid:88)

wijI [xi = xj] +(cid:88)

ciI(cid:2)xi = x0

i

(cid:3)

(28.8.12)

i∼j

i

i are known. This model has immediate application in non-binary image restora-
where wij > 0 and the x0
tion, and also in clustering based on a similarity score. Whilst no eﬃcient exact algorithm is known, a
useful approach is to approximate the problem as a sequence of binary problems, as we describe below.

Potts to binary MRF translation

Consider the α-expansion representation

xi = siα + (1 − si)xold

i

(28.8.13)

or α,
where si ∈ {0, 1} and for a given α ∈ {0, 1, 2, . . . , N}. This restricts xi to be either the state xold
depending on the binary variable si. Using a new binary variable s we can therefore restrict x to a subpart

i

536

DRAFT March 9, 2010

MAP for MRFs

Figure 28.14: (a): Noisy im-
age. (b): Restored image. The
α-expansion method was used,
with suitable interactions w and
bias c to ensure reasonable re-
sults. From [47].

(a)

(b)

of the full space and write a new objective function in terms of s alone:

E(s) =(cid:88)

i∼j

I [si = sj] +(cid:88)

(cid:48)
ij

w

i

(cid:48)
isi + const.
c

(28.8.14)

for w(cid:48)
ij > 0. This new problem is of the form of an attractive binary MRF which can be solved exactly
using the graph cuts procedure. The idea is then to choose another α value (at random) and then ﬁnd
the optimal s for the new α. In this way we are guaranteed to iteratively increase E.

For a given α and xold, the transformation of the Potts model objective is given by using si ∈ {0, 1} and
considering

I [xi = xj] = I(cid:104)

i = sjα + (1 − sj)xold
siα + (1 − si)xold
i = xold
xold
= sisjuij + aisi + bjsj + const.

j

= (1 − si)(1 − sj)I(cid:104)
− I(cid:104)

xold
i = α

(cid:105)

(cid:105)

j

(cid:105)
+ (1 − si)sjI(cid:104)
(cid:105)

with

uij ≡ 1 − I(cid:104)

(cid:105)

+ I(cid:104)

xold
j = α

xold
i = xold

j

(cid:105)

+ si(1 − sj)I(cid:104)

(cid:105)

xold
j = α

xold
i = α

+ sisj
(28.8.15)

(28.8.16)

and similarly deﬁned ai, bi. By enumeration it is straightforward to show that uij is either 0, 1 or 2. Using
the mathematical identity

sisj =

1
2

(I [si = sj] + si + sj − 1)

we can write,

I [xi = xj] = uij
2

(I [si = sj] + si + sj) + aisi + bjsj + const.

(28.8.17)

(28.8.18)

Hence terms wijI [xi = xj] translate to positive interaction terms I [si = sj] wijuij/2. All the unary terms
are easily exactly mapped into corresponding unary terms c(cid:48)
i deﬁned as the sum of all unary terms
in si. This shows that the positive interaction wij in terms of the original variables x maps to a positive
interaction in the new variables s. Hence we can ﬁnd the maximal state of s using a graph cut algorithm.
A related (though diﬀerent) procedure is outlined in [48].

isi for c(cid:48)

DRAFT March 9, 2010

537

Exercises

Example 119 (Potts model for image reconstruction). An example image restoration problem for nearest
neighbour interactions on a pixel lattice and suitably chosen w, c is given in ﬁg(28.14). The images are
non-binary and therefore the optimal MAP assignment cannot be computed exactly in an eﬃcient way.
The alpha-expansion technique was used here combined with an eﬃcient min-cut approach, see [47] for
details.

28.9 Further Reading

Approximate inference is a highly active research area and increasingly links to convex optimisation[46] are
being developed. See [287] for a general overview and [247] for recent application of convex optimisation
to approximate inference in a practical machine learning application.

28.10 Code

LoopyBP.m: Loopy Belief Propagation (Factor Graph formalism)
demoLoopyBP.m: Demo of loopy Belief Propagation
demoMFBPGibbs.m: Comparison of Mean Field, Belief Propagation and Gibbs sampling

demoMRFclean.m: Demo of analysing a dirty picture
MaxFlow.m: Max-Flow Min-Cut algorithm (Ford-Fulkerson)
binaryMRFmap.m: Optimising a binary MRF

28.11 Exercises

Exercise 251. For the max-ﬂow-min-cut problem, under the convention that the source node xs is clamped
to state 1, and the sink node xt to state 0, a standard min-cut algorithm returns that joint x which

wijI [xi = 1] I [xj = 0]

minimises(cid:88)
(cid:88)

ij

Explain how this can be written in the form

˜wijI [xi (cid:54)= xj]

ij

(28.11.1)

(28.11.2)

Exercise 252. Using BRMLtoolbox, write a routine KLdiv(q,p) that returns the Kullback-Leibler di-
vergence between two discrete distributions q and p deﬁned as potentials q and p.

Exercise 253. The ﬁle p.mat contains a distribution p(x, y, z) on ternary state variables. Using BRML-
toolbox, ﬁnd the best approximation q(x, y)q(z) that minimises the Kullback-Leibler divergence KL(q|p)
and state the value of the minimal Kullback-Leibler divergence for the optimal q.
Exercise 254. Consider the pairwise MRF deﬁned on a 2 × 2 lattice, as given in pMRF.mat. Using
BRMLtoolbox,

1. Find the optimal fully factorised approximation(cid:81)4
2. Find the optimal fully factorised approximation (cid:81)4

factor graph formalism.

equations.

i=1 qBP

i

by Loopy Belief Propagation, based on the

i=1 qM F

i

by solving the variational Mean Field

3. By pure enumeration, compute the exact marginals pi.

538

DRAFT March 9, 2010

Exercises

4. Averaged over all 4 variables, compute the mean expected deviation in the marginals

4(cid:88)

2(cid:88)

1
2

i=1

j=1

1
4

|qi(x = j) − pi(x = j)|

for both the BP and MF approximations, and comment on your results.

Exercise 255. In LoopyBP.m the message schedule is chosen at random. Modify the routine to choose a
schedule using a forward-reverse elimination sequence on a random spanning tree.
Exercise 256 (Double Integration Bounds). Consider a bound

Then for

f(x) ≥ g(x)
(cid:90) x

˜f(x) ≡
Show that:

a

f(x)dx,

(cid:90) x

a

˜g(x) ≡

g(x)dx

1.

2.

where

˜f(x) ≥ ˜g(x),

for x ≥ a

ˆf(x) ≥ ˆg(x)
(cid:90) x

ˆf(x) ≡

a

for all x

˜f(x)dx,

ˆg(x) ≡

(cid:90) x

a

˜g(x)dx

(28.11.3)

(28.11.4)

(28.11.5)

(28.11.6)

(28.11.7)

The signiﬁcance is that this double integration (or summation in the case of discrete variables) is a general
procedure for generating a new bound from an existing bound [172].
Exercise 257. Starting from

ex ≥ 0

and using the double integration procedure, show that

(28.11.8)

ex ≥ ea(1 + x − a)

Z =(cid:88)

1. By replacing x → sTWs for s ∈ {0, 1}D, and a → hTs derive a bound on the partition function of a

Boltzmann distribution

esTWs

(28.11.9)

s

2. Show that this bound is equivalent to the Mean Field bound on the partition function.

3. Discuss how one can generate tighter bounds on the partition function of a Boltzmann distribution

by further application of the double integration procedure.

Exercise 258. Consider a pairwise MRF

for symmetric W. Consider the decomposition

exTWx+bTx

1
Z

p(x) =

W =(cid:88)

qiWi,

where 0 ≤ qi ≤ 1 and(cid:80)

i

i = 1, . . . , I

(28.11.10)

(28.11.11)

bound on the normalisation Z and discuss a naive method to ﬁnd the tightest upper bound.

i qi = 1, and the graph of each matrix Wi is a tree. Explain how to form an upper

DRAFT March 9, 2010

539

Exercise 259. Derive Linkser’s bound on the Mutual Information, equation (28.5.15).

Exercises

Exercise 260. Consider the average of a positive function f(x) with respect to a distribution p(x)

(cid:90)

x

(cid:90)

J = log

p(x)f(x)

J ≥

x

p(x) log f(x)

where f(x) ≥ 0. The simplest version of Jensen’s inequality states that

(28.11.12)

(28.11.13)

1. By considering a distribution r(x) ∝ p(x)f(x), and KL(q|r), for some variational distribution q(x),

show that

J ≥ −KL(q(x)|p(x)) + (cid:104)log f(x)(cid:105)q(x)

(28.11.14)
The bound saturates when q(x) ∝ p(x)f(x). This shows that if we wish to approximate the average
J, the optimal choice for the approximating distribution depends on both the distribution p(x) and
integrand f(x).

2. Furthermore, show that

J ≥ −KL(q(x)|p(x)) − KL(q(x)|f(x)) − H(q(x))

(28.11.15)

where H(q(x)) is the entropy of q(x). The ﬁrst term encourages q to be close to p. The second
encourages q to be close to f, and the third encourages q to be sharply peaked.

Exercise 261. For a Markov Random ﬁeld over D binary variables xi ∈ {0, 1}, i = 1, . . . , D, we deﬁne
(28.11.16)

exTWx

p(x) =

1
Z

show that

p(xi) =

Z\i
Z

where

Z\i ≡

x1,...,xi−1,xi+1,...,xD

(cid:88)

exTWx

(28.11.17)

(28.11.18)

and explain why a bound on the marginal p(xi) requires both upper and lower bounds on partition functions.
Exercise 262. Consider a directed graph such that the capacity of an edge x → y is c(x, y) ≥ 0. The ﬂow
on an edge f(x, y) ≥ 0 must not exceed the capacity of the edge. The aim is to maximise the ﬂow from a
deﬁned source node s to a deﬁned sink node t. In addition ﬂow must be conserved such that for any node
other than the source or sink (y (cid:54)= s, t),

f(y, x)

(28.11.19)

(cid:88)

f(x, y) =(cid:88)

x

x

A cut is deﬁned as a partition of the nodes into two non-overlapping sets S and T such that s is in S and
t in T . Show that:

1. The net ﬂow from s to t, val(f) is the same as the net ﬂow from S to T :

f(y, x)

(28.11.20)

val(f) = (cid:88)
(cid:80)

x∈S,y∈T

(cid:88)

f(x, y) −

y∈T ,x∈S

2. val(f) ≤

x∈S,y∈T f(x, y) namely that the ﬂow is upper bounded by the capacity of the cut.

540

DRAFT March 9, 2010

Exercises

The max-ﬂow-min-cut theorem further states that the maximal ﬂow is actually equal to the capacity of the
cut.

Exercise 263 (Potts to Ising translation). Consider the function E(x) deﬁned on a set of multistate
variables dom(xi) = {0, 1, 2, . . . , N},

E(x) =(cid:88)

wijI [xi = xj] +(cid:88)

ciI(cid:2)xi = x0

i

(cid:3)

(28.11.21)

i∼j

i

where wij > 0 and observed pixel states x0
maximisation of E(x). Using the restricted parameteristion

i are known, as are ci. Our interest is to ﬁnd an approximate

xi = siα + (1 − si)xold

i

(28.11.22)

where si ∈ {0, 1} and for a given α ∈ {0, 1, 2, . . . , N}, show how to write E(x) as a function of the binary
variables

(cid:48)
isi + const.
c

(28.11.23)

E(s) =(cid:88)

i∼j

I [si = sj] +(cid:88)

(cid:48)
ij

w

i

for w(cid:48)
using the graph cuts procedure.

ij > 0. This new problem is of the form of an attractive binary MRF which can be solved exactly

Exercise 264. Consider an approximating distribution in the exponential family,

q(x) =

1
Z(φ) eφTg(x)

We wish to use q(x) to approximate a distribution p(x) using the KL divergence

KL(p|q)

1. Show that optimally

(cid:104)g(x)(cid:105)p(x) = (cid:104)g(x)(cid:105)q(x)

2. Show that a Gaussian can be written in the exponential form

(cid:0)x µ, σ2(cid:1) =

N

1
Z(φ) eφTg(x)

(28.11.24)

(28.11.25)

(28.11.26)

(28.11.27)

where g1(x) = x, g2(x) = x2 and suitably chosen φ.

3. Hence show that the optimal Gaussian ﬁt N

sense matches the moments:

σ2 =(cid:10)x2(cid:11)

µ = (cid:104)x(cid:105)p(x) ,

p(x) − (cid:104)x(cid:105)2

p(x)

(cid:0)x µ, σ2(cid:1) to any distribution, in the minimal KL(p|q)
(cid:0)x m, s2(cid:1) to a distribution p(x).

(28.11.28)

Exercise 265. We wish to ﬁnd a Gaussian approximation q(x) = N
Show that

KL(p|q) = −(cid:104)log q(x)(cid:105)p(x) + const.

(28.11.29)

Write the KL divergence explicitly as a function of m and s2 and conﬁrm the general result that the optimal
m and s2 that minimise KL(p|q) are given by setting the mean and variance of q to those of p.
DRAFT March 9, 2010

541

Exercise 266. For a pairwise binary Markov Random Field, p with partition function

Z(w, b) =(cid:88)

(cid:80)

e

x

∂
∂bi

log Z(w, b) =

i bixi

i,j wij xixj +(cid:80)
(cid:88)

1

Z(w, b)

x

show that the means can be computed using

(cid:80)

i∼j wij xixj +(cid:80)

xie

i bixi = (cid:104)xi(cid:105)p

and that similarly the covariance is given by

(cid:104)xixj(cid:105)p − (cid:104)xi(cid:105)p (cid:104)xj(cid:105)p = ∂2

∂bi∂bj

log Z(w, b)

Exercises

(28.11.30)

(28.11.31)

(28.11.32)

Exercise 267. The naive mean ﬁeld theory applied to a pairwise MRF

(cid:80)
p(x) ∝ e

i,j wij xixj +(cid:80)

dom(xi) = {0, 1}, gives a factorised approximation q(x) =(cid:81)

i bixi

this we can approximate

(28.11.33)
i q(xi), based on minimising KL(q|p). Using

(cid:104)xixj(cid:105)p ≈ (cid:104)xi(cid:105)q (cid:104)xj(cid:105)q ,

(28.11.34)
To produce a better, non-factorised approximation to (cid:104)xixj(cid:105)p we could ﬁt a non-factorised q. The linear-
response method[153] may also be used, based on a perturbation expansion of the free energy. Alternatively,
consider the relation

i (cid:54)= j

p(xi, xj) = p(xi|xj)p(xj)

Show that

(28.11.35)

(cid:104)xixj(cid:105)p = p(xi = 1, xj = 1) = p(xi = 1|xj = 1)p(xj = 1)

(28.11.36)
Explain how to use a modiﬁed naive mean ﬁeld method to ﬁnd a non-factorised approximation to (cid:104)xixj(cid:105)p.
Exercise 268. Derive the EP updates equation (28.7.8) and equation (28.7.9).
Exercise 269. You are given a set of datapoints labelled 1 to N and a similarity ‘metric’ wij ≥ 0, i, j =
1, . . . , N which denotes the similarity of the points i and j. You want to assign each datapoint to a cluster
index cn ∈ {1, . . . , K}. For a subset of the datapoints you have a preference for the cluster index. Explain
how to use a Potts model to formulate an objective function for this ‘semi-supervised’ clustering problem.

542

DRAFT March 9, 2010

APPENDIX A

Background Mathematics

(A.1.1)

(A.1.2)

A.1 Linear Algebra

A.1.1 Vector algebra

Let x denote the n-dimensional column vector with components

 x1

x2
...
xn


n(cid:88)

i=1

Deﬁnition 116 (scalar product). The scalar product w · x is deﬁned as:

w · x =

wixi = wTx

and has a natural geometric interpretation as:

w · x = |w||x| cos(θ)

where θ is the angle between the two vectors. Thus if the lengths of two vectors are ﬁxed their inner
product is largest when θ = 0, whereupon one vector is a constant multiple of the other. If the scalar
product xTy = 0, then x and y are orthogonal (they are a right angles to each other).

The length of a vector is denoted |x|, the squared length is given by

|x|2 = xTx = x2 = x2
A unit vector x has xTx = 1.

1 + x2

2 + ··· + x2

n

(A.1.3)

Deﬁnition 117 (Linear dependence). A set of vectors x1, . . . , xn is linearly dependent if there exists
a vector xj that can be expressed as a linear combination of the other vectors. Vice-versa, if the only
solution to

αixi = 0

(A.1.4)

n(cid:88)

i=1

543

Linear Algebra

Figure A.1: Resolving a vector a into components along the
orthogonal directions e and e∗. The projection of a onto these
two directions are lengths α and β along the directions e and
e∗.

is for all αi = 0, i = 1, . . . , n, the vectors x1, . . . , xn are linearly independent.

A.1.2 The scalar product as a projection

Suppose that we wish to resolve the vector a into its components along the orthogonal directions speciﬁed
by the unit vectors e and e∗. That is |e| = |e|
∗ = 1 and e · e∗ = 0. This is depicted in ﬁg(A.1). We are

required to ﬁnd the scalar values α and β such that

a = αe + βe∗
From this we obtain

a · e = αe · e + βe∗

· e,

a · e∗ = αe · e∗ + βe∗

· e∗

From the orthogonality and unit lengths of the vectors e and e∗, this becomes simply

a · e = α,

a · e∗ = β

(A.1.5)

(A.1.6)

(A.1.7)

A set of vectors is orthonormal if they are mutually orthogonal and have unit length. This means that we
can write the vector a in terms of the orthonormal components e and e∗ as

a = (a · e) e + (a · e∗) e∗

One can see therefore that the scalar product between a and e projects the vector a onto the (unit)
direction e. The projection of a vector a onto a direction speciﬁed by f is therefore

a · f
|f|2 f

A.1.3 Lines in space

(A.1.9)

A line in 2 (or more) dimensions can be speciﬁed as follows. The vector of any point along the line is
given, for some s, by the equation

(A.1.8)

(A.1.10)

(A.1.11)

where u is parallel to the line, and the line passes through the point a, see ﬁg(A.2). This is called the
parametric representation of the line. An alternative speciﬁcation can be given by realising that all vectors
along the line are orthogonal to the normal of the line, n (u and n are orthonormal). That is

p = a + su,

s ∈ R.

(p − a) · n = 0 ⇔ p · n = a · n

If the vector n is of unit length, the right hand side of the above represents the shortest distance from the
origin to the line, drawn by the dashed line in ﬁg(A.2) (since this is the projection of a onto the normal
direction).

Figure A.2: A line can be speciﬁed by some position vector on
the line, a, and a unit vector along the direction of the line, u.
In 2 dimensions, there is a unique direction, n, perpendicular to
the line. In three dimensions, the vectors perpendicular to the
direction of the line lie in a plane, whose normal vector is in the
direction of the line, u.

544

DRAFT March 9, 2010

e*eβαaee*aapnuLinear Algebra

Figure A.3: A plane can be speciﬁed by a point in
the plane, a and two, non-parallel directions in the
plane, u and v. The normal to the plane is unique,
and in the same direction as the directed line from
the origin to the nearest point on the plane.

A.1.4 Planes and hyperplanes

A line is a one dimensional hyperplane. To deﬁne a two-dimensional plane (in arbitrary dimensional space)
one may specify two vectors u and v that lie in the plane (they need not be mutually orthogonal), and a
position vector a in the plane, see ﬁg(A.3). Any vector p in the plane can then be written as

p = a + su + tv,

(s, t) ∈ R.

(A.1.12)

An alternative deﬁnition is given by considering that any vector within the plane must be orthogonal to
the normal of the plane n.

(p − a) · n = 0 ⇔ p · n = a · n

(A.1.13)

The right hand side of the above represents the shortest distance from the origin to the plane, drawn by
the dashed line in ﬁg(A.3). The advantage of this representation is that it has the same form as a line.
Indeed, this representation of (hyper)planes is independent of the dimension of the space. In addition,
only two vectors need to be deﬁned – a point in the plane, a, and the normal to the plane n.

A.1.5 Matrices
An m× n matrix A is a collection of scalar m× n values arranged in a rectangle of m rows and n columns.
A vector can be considered a n× 1 matrix. If the element of the i-th row and j-th column is Aij, then AT
denotes the matrix that has Aji there instead - the transpose of A. For example A and its transpose are :



 2 3 4
example(cid:2)A−1(cid:3)

4 5 9
6 7 1

A =

ij).

 2 4 6

3 5 7
4 9 1



AT =

The i, j element of matrix A can be written Aij or in cases where more clarity is required, [A]ij (for

(A.1.14)

(cid:104)
BT(cid:105)

Deﬁnition 118 (transpose). The transpose BT of the n by m matrix B is the m by n matrix D with
components

= Bjk ;

B, (cid:0)BT(cid:1)T = B and (AB)T = BTAT. If the shapes of the matrices A,B and C are such that it makes

k = 1, . . . , m j = 1, . . . , n .

(A.1.15)

kj

sense to calculate the product ABC, then

(ABC)T = CTBTAT

(A.1.16)

A square matrix A is symmetric if AT = A. A square matrix is called Hermitian if

A = AT∗

(A.1.17)
where ∗ denotes the complex conjugate operator. For Hermitian matrices, the eigenvectors form an or-
thogonal set, with real eigenvalues.

DRAFT March 9, 2010

545

anuvpDeﬁnition 119 (Matrix addition). For two matrix A and B of the same size,

[A + B]ij = [A]ij + [B]ij

Linear Algebra

(A.1.18)

Deﬁnition 120 (Matrix multiplication). For an l by n matrix A and an n by m matrix B, the product
AB is the l by m matrix with elements

[A]ij [B]jk ;

i = 1, . . . , l k = 1, . . . , m .

[AB]ij =

n(cid:88)
(cid:19)(cid:18) x1
(cid:18) a11 a12

j=1

a21 a22

x2

For example

(cid:19)

(cid:18) a11x1 + a12x2

a21x2 + a22x2

(cid:19)

=

(A.1.19)

(A.1.20)

Note that even if BA is deﬁned as well, that is if l = n, generally BA is not equal to AB (when they do
we say they commute). The matrix I is the identity matrix, necessarily square, with 1’s on the diagonal
and 0’s everywhere else. For clarity we may also write Im for an square m × m identity matrix.
Then
for an m × n matrix A
ImA = AIn = A

(A.1.21)

The identity matrix has elements [I]ij = δij given by the Kronecker delta:

(cid:26) 1 i = j

0 i (cid:54)= j

δij ≡

(A.1.22)

(A.1.23)

λi

Deﬁnition 121 (Trace).

trace (A) =(cid:88)

Aii =(cid:88)

i

i

where λi are the eigenvalues of A.

A.1.6 Linear transformations

Rotations

If we assume that rotation of a two-dimensional vector x = (x, y)T can be accomplished by matrix multi-
plication Rx then, since matrix multiplication is distributive, we only need to work out how the axes unit
vectors i = (1, 0)T and j = (0, 1)T transform since

Rx = xRi + yRj

The unit vectors i and j under rotation by θ degrees transform to vectors

(cid:19)

(cid:18) r11

r21

(cid:19)

(cid:18) cos θ

sin θ

=

Rj =

(cid:18)

(cid:19)

(cid:18) r12

r22

=

− sin θ
cos θ

(cid:19)

Ri =

546

(A.1.24)

(A.1.25)

DRAFT March 9, 2010

Linear Algebra

From this, one can simply read oﬀ the values for the elements

(cid:18) cos θ − sin θ

cos θ

sin θ

(cid:19)

R =

A.1.7 Determinants

(A.1.26)

Deﬁnition 122 (Determinant). For a square matrix A, the determinant is the volume of the transfor-
mation of the matrix A (up to a sign change). That is, we take a hypercube of unit volume and map each
vertex under the transformation, and the volume of the resulting object is deﬁned as the determinant.
Writing [A]ij = aij,

det

det

a21 a22

= a11a22 − a21a12

(cid:18)a11 a12
(cid:19)
a11 a12 a13
 = a11 (a22a33 − a23a32) − a12 (a21a33 − a31a23) + a13 (a21a32 − a31a22)
(cid:19)
(cid:18)a12 a23

(cid:18)a21 a22

(cid:18)a21 a23

a21 a22 a23
a31 a32 a33

(cid:19)

(cid:19)

The determinant in the (3 × 3) case has the form

(A.1.27)

(A.1.28)

(A.1.29)

a11det

a32 a33

− a12det

a31 a33

+ a13det

a31 a32

The determinant of the (3 × 3) matrix A is given by the sum of terms (−1)i+1a1idet (Ai) where Ai is
the (2 × 2) matrix formed from A by removing the ith row and column. This form of the determinant
generalises to any dimension. That is, we can deﬁne the determinant recursively as an expansion along
the top row of determinants of reduced matrices. The absolute value of the determinant is the volume of
the transformation.

(cid:16)

AT(cid:17)

det

= det (A)

(A.1.30)

(A.1.31)

For square matrices A and B of equal dimensions,

det (AB) = det (A) det (B) ,

det (I) = 1 ⇒ det(cid:0)A−1(cid:1) = 1/det (A)

For any matrix A which collapses dimensions, then the volume of the transformation is zero, and so is the
determinant. If the determinant is zero, the matrix cannot be invertible since given any vector x, given a
‘projection’ y = Ax, we cannot uniquely compute which vector x was projected to y– there will in general
be an inﬁnite number of solutions.

Deﬁnition 123 (Orthogonal matrix). A square matrix A is orthogonal if AAT = I = ATA. From the
properties of the determinant, we see therefore that an orthogonal matrix has determinant ±1 and hence
corresponds to a volume preserving transformation – i.e. a rotation.

X =(cid:2)x1, . . . , xn(cid:3)

Deﬁnition 124 (Matrix rank). For an m × n matrix X with n columns, each written as an m-vector:

(A.1.32)
the rank of X is the maximum number of linearly independent columns (or equivalently rows). A n × n
square matrix is full rank if the rank is n and the matrix is non-singular. Otherwise the matrix is reduced
rank and is singular.

DRAFT March 9, 2010

547

A.1.8 Matrix inversion

Linear Algebra

Deﬁnition 125 (Matrix inversion). For a square matrix A, its inverse satisﬁes

A−1A = I = AA−1

(A.1.33)
It is not always possible to ﬁnd a matrix A−1 such that A−1A = I. In that case, we call the matrix A
singular. Geometrically, singular matrices correspond to ‘projections’: if we were to take the transform
of each of the vertices v of a binary hypercube Av, the volume of the transformed hypercube would be
zero. If you are given a vector y and a singular transformation, A, one cannot uniquely identify a vector
x for which y = Ax - typically there will be a whole space of possibilities.

Provided the inverses matrices exist

(AB)−1 = B−1A−1

For a non-square matrix A such that AAT is invertible, then the pseudo inverse, deﬁned as

A† = AT(cid:16)

AAT(cid:17)−1

satisﬁes AA† = I.

(A.1.34)

(A.1.35)

A.1.9 Computing the matrix inverse
For a 2 × 2 matrix, it is straightforward to work out for a general matrix, the explicit form of the inverse.
If the matrix whose inverse we wish to ﬁnd is A =

, then the condition for the inverse is

(cid:18)a b

(cid:19)

c d

=

c d

g h

0 1

(cid:19)
(cid:18)1 0
(cid:19)
(cid:19)(cid:18)e f
(cid:18)a b
(cid:18)1 0
(cid:19)
(cid:18)ae + bg af + bh
(cid:18)e f
(cid:18) d −c
(cid:19)

cf + dh

ce + dg

(cid:19)

0 1

=

ad − bc

−b

a

(cid:19)

1

=

g h

= A−1

(A.1.36)

(A.1.37)

(A.1.38)

Multiplying out the left hand side, we obtain the four conditions

It is readily veriﬁed that the solution to this set of four linear equations is given by

The quantity ad − bc is the determinant of A. There are many ways to compute the inverse of a general
matrix, and we refer the reader to more specialised texts.

Note that, if one wants to solve only a linear system, although the solution can be obtained through
matrix inversion, this should not be use. Often, one needs to solve huge dimensional linear systems of
equations, and speed becomes an issue. These equations can be solved much more accurately and quickly
using elimination techniques such as Gaussian Elimination.

A.1.10 Eigenvalues and eigenvectors

The eigenvectors of a matrix correspond to the natural coordinate system, in which the geometric trans-
formation represented by A can be most easily understood.

548

DRAFT March 9, 2010

Linear Algebra

Deﬁnition 126 (Eigenvalues and Eigenvectors). For a square matrix A, e is an eigenvector of A with
eigenvalue λ if

Ae = λe

det (A) =

n(cid:89)
trace (A) =(cid:88)

λi

i=1

λi

(A.1.39)

(A.1.40)

(A.1.41)

Hence a matrix is singular if it has a zero eigenvalue. The trace of a matrix can be expressed as

i

For an (n × n) dimensional matrix, there are (including repetitions) n eigenvalues, each with a correspond-
ing eigenvector. We can reform equation (A.1.39) as

(A − λI) e = 0

(A.1.42)

This is a linear equation, for which the eigenvector e and eigenvalue λ is a solution. We can write equation
(A.1.42) as Be = 0, where B ≡ A − λI. If B has an inverse, then a solution is e = B−10 = 0, which
trivially satisﬁes the eigen-equation. For any non-trivial solution to the problem Be = 0, we therefore
need B to be non-invertible. This is equivalent to the condition that B has zero determinant. Hence λ is
an eigenvalue of A if

det (A − λI) = 0

(A.1.43)

This is known as the characteristic equation. This determinant equation will be a polynomial of degree n
and the resulting equation is known as the characteristic polynomial. Once we have found an eigenvalue,
the corresponding eigenvector can be found by substituting this value for λ in equation (A.1.39) and solving
the linear equations for e. It may be that the for an eigenvalue λ the eigenvector is not unique and there
is a space of corresponding vectors. Geometrically, the eigenvectors are special directions such that the
eﬀect of the transformation A along a direction e is simply to scale the vector e. For a rotation matric R
in general there will be no direction preserved under the rotation so that the eigenvalues and eigenvectors
are complex valued (which is why the Fourier representation, which corresponds to representation in a
rotated basis, is necessarily complex).

Remark 11 (Orthogonality of eigenvectors of symmetric matrices). For a real symmetric matric
A = AT, and two of its eigenvectors ei and ej of A are orthogonal (ei)Tej = 0 if the eigenvalues λi
and λj are diﬀerent.

The above can be shown by considering:

Aei = λiei ⇒ (ej)TAei = λi(ej)Tei

Since A is symmetric, the left hand side is equivalent to

((ej)TA)ei = (Aej)Tei = λj(ej)Tei ⇒ λi(ej)Tei = λj(ej)Tei

(A.1.44)

(A.1.45)

If λi (cid:54)= λj, this condition can be satisﬁed only if (ej)Tei = 0, namely that the eigenvectors are orthogonal.
549
DRAFT March 9, 2010

A.1.11 Matrix decompositions

The observation that the eigenvectors of a symmetric matrix are orthogonal leads directly to the spectral
decomposition formula below.

Linear Algebra

Deﬁnition 127 (Spectral decomposition). A symmetric matrix A has an eigen-decomposition

n(cid:88)

A =

λieieT
i

i=1

where λi is the eigenvalue of eigenvector ei and the eigenvectors form an orthogonal set,

(cid:0)ei(cid:1)T ej = δij

(cid:0)ei(cid:1)T ei

In matrix notation

A = EΛET

(A.1.46)

(A.1.47)

(A.1.48)

where E is the matrix of eigenvectors and Λ the corresponding diagonal eigenvalue matrix. More generally,
for a square non-symmetric non-singular A we can write

A = EΛE−1

(A.1.49)

Deﬁnition 128 (Singular Value Decomposition). The SVD decomposition of a n × p matrix X is

X = USVT

(A.1.50)

where dim U = n×n with UTU = In. Also dim V = p×p with VTV = Ip. The matrix S has dim S = n×p
with zeros everywhere except on the diagonal entries. The ‘singular values’ are the diagonal entries [S]ii
and are positive. The singular values are ordered so that the upper left diagonal element of S contains the
largest singular value.

Quadratic forms

Deﬁnition 129 (Quadratic form).

xTAx + xTb

(A.1.51)

Deﬁnition 130 (Positive deﬁnite matrix). A symmetric matrix A, with the property that xTAx ≥ 0 for
any vector x is called nonnegative deﬁnite. A symmetric matrix A, with the property that xTAx > 0 for
any vector x (cid:54)= 0 is called positive deﬁnite. A positive deﬁnite matrix has full rank and is thus invertible.
Using the eigen-decomposition of A,

xTAx =(cid:88)

λiyTei(ei)Tx =(cid:88)

λi

(cid:16)

xTei(cid:17)2

i

i

which is greater than zero if and only if all the eigenvalues are positive. Hence A is positive deﬁnite if and
only if all its eigenvalues are positive.

550

DRAFT March 9, 2010

(A.1.52)

Multivariate Calculus

A.2 Matrix Identities

Deﬁnition 131 (Trace-Log formula). For a positive deﬁnite matrix A,

trace (log A) ≡ log det (A)

(A.2.1)

Note that the above logarithm of a matrix is not the element-wise logarithm. In MATLAB the required
function is logm. In general for an analytic function f(x), f(M) is deﬁned via the power-series expansion
of the function. On the right, since det (A) is a scalar, the logarithm is the standard logarithm of a scalar.

Deﬁnition 132 (Matrix Inversion Lemma (Woodbury formula)). Provided the appropriate inverses exist:

(cid:16)

A + UVT(cid:17)−1

(cid:16)

(cid:17)−1

= A−1 − A−1U

I + VTA−1U

VTA−1

(A.2.2)

(A.2.3)

Eigenfunctions

(cid:48)
K(x

, x)φa(x) = λaφa(x

(cid:48))

(cid:90)

(cid:90)

x

x

By an analogous argument that proves the theorem of linear algebra above, the eigenfunctions are orthog-
onal of a real symmetric kernel, K(x(cid:48), x) = K(x, x(cid:48)) are orthogonal:

φa(x)φ

∗
b(x) = δab

(A.2.4)

where φ∗(x) is the complex conjugate of φ(x)1. From the previous results, we know that a symmetric real
matrix K must have a decomposition in terms eigenvectors with positive, real eigenvalues. Since this is to
be true for any dimension of matrix, it suggests that we need the (real symmetric) kernel function itself
to have a decomposition (provided the eigenvalues are countable)

λµφµ(xi)φ

∗
µ(xj)

K(xi, xj) =(cid:88)
yiK(xi, xj)yj =(cid:88)

µ

i,j

i,j,µ

since then(cid:88)

λµyiφµ(xi)φ

µ(xj)yj =(cid:88)

∗

µ

λµ ((cid:88)
(cid:124)

i

((cid:88)
(cid:124)

i

(cid:125)

yiφµ(xi))

(cid:123)(cid:122)

zi

(A.2.5)

(A.2.6)

∗
µ(xi))

(cid:125)

yiφ

(cid:123)(cid:122)

z∗

i

which is greater than zero if the eigenvalues are all positive (since for complex z, zz∗
If the
eigenvalues are uncountable (which happens when the domain of the kernel is unbounded), the appropriate
decomposition is

≥ 0).

K(xi, xj) =

λ(s)φ(xi, s)φ

∗(xj, s)ds

(A.2.7)

(cid:90)

A.3 Multivariate Calculus

1This deﬁnition of the inner product is useful, and particularly natural in the context of translation invariant kernels. We

are free to deﬁne the inner product, but this conjugate form is often the most useful.

DRAFT March 9, 2010

551

Multivariate Calculus

Figure A.4: Interpreting the gradient. The ellipses are contours
of constant function value, f = const. At any point x, the gradi-
ent vector ∇f(x) points along the direction of maximal increase
of the function.

Deﬁnition 133 (Partial derivative). Consider a function of n variables, f(x1, x2, . . . , xn) or f(x). The
partial derivative of f w.r.t. x1 at x∗ is deﬁned as the following limit (when it exists)

(cid:12)(cid:12)(cid:12)(cid:12)x=x∗

∂f
∂x1

f(x∗

1 + h, x∗

2, . . . , x∗

n) − f(x∗)

h

= lim
h→0

The gradient vector of f will be denoted by ∇f or g



 ∂f

∂x1...

∂f
∂xn

∇f(x) ≡ g(x) ≡

(A.3.1)

(A.3.2)

(A.3.3)

(A.3.4)

A.3.1

Interpreting the gradient vector

Consider a function f(x) that depends on a vector x. We are interested in how the function changes when
the vector x changes by a small amount : x → x + δ, where δ is a vector whose length is very small.
According to a Taylor expansion, the function φ will change to

δi

∂f
∂xi

f (x + δ) = f(x) +(cid:88)
+ O(cid:0)δ2(cid:1)
f (x + δ) = f(x) + (∇f)Tδ + O(cid:0)δ2(cid:1)

∂xi

i

We can interpret the summation above as the scalar product between the vector ∇f with components
[∇f]i = ∂f

and δ.

The gradient points along the direction in which the function increases most rapidly. Why? Consider a
direction ˆp (a unit length vector). Then a displacement, δ units along this direction changes the function
value to

f(x + δˆp) ≈ f(x) + δ∇f(x) · ˆp

(A.3.5)

The direction ˆp for which the function has the largest change is that which maximises the overlap

∇f(x) · ˆp = |∇f(x)||ˆp| cos θ = |∇f(x)| cos θ

(A.3.6)
where θ is the angle between ˆp and ∇f(x). The overlap is maximised when θ = 0, giving ˆp =
∇f(x)/|∇f(x)|. Hence, the direction along which the function changes the most rapidly is along ∇f(x).
A.3.2 Higher derivatives

The ‘ﬁrst derivative’ of a function of n variables is an n-vector; the ‘second-derivative’ of an n-variable
function is deﬁned by the n2 partial derivatives of the n ﬁrst partial derivatives w.r.t. the n variables

i = 1, . . . , n; j = 1, . . . , n

(A.3.7)

DRAFT March 9, 2010

(cid:18) ∂f

(cid:19)

∂xj

∂
∂xi

552

x1x2f(x)Multivariate Calculus

which is usually written

∂2f

∂xi∂xj

,

i (cid:54)= j

∂2f
2 ,
∂xi

i = j

(A.3.8)

If the partial derivatives ∂f /∂xi, ∂f /∂xj and ∂2f /∂xi∂xj are continuous, then ∂2f /∂xi∂xj exists and

∂2f /∂xi∂xj = ∂2f /∂xj∂xi .

(A.3.9)

These n2 second partial derivatives are represented by a square, symmetric matrix called the Hessian
matrix of f(x).

 ∂2f

∂x1

...

2

∂2f

∂x1∂xn

Hf (x) =



. . .

. . .

∂2f

∂x1∂xn

...

∂2f
2
∂xn

(A.3.10)

A.3.3 Chain rule
Consider f(x1, . . . , xn). Now let each xj be parameterized by u1, . . . , um, i.e. xj = xj(u1, . . . , um). What
is ∂f /∂uα?

δf = f(x1 + δx1, . . . , xn + δxn) − f(x1, . . . , xn) =

δxj =

δuα + higher order terms

∂f
∂xj

∂xj
∂uα

δuα + higher order terms

∂xj
∂uα

m(cid:88)
n(cid:88)
m(cid:88)

α=1

j=1

α=1

So

δf =

Therefore

Deﬁnition 134 (Chain rule).

n(cid:88)

j=1

∂f
∂uα

=

∂f
∂xj

∂xj
∂uα

or in vector notation

∂

∂uα

f(x(u)) = ∇f T(x(u)) ∂x(u)

∂uα

n(cid:88)

j=1

∂f
∂xj

δxj + higher order terms

(A.3.11)

(A.3.12)

(A.3.13)

(cid:12)(cid:12)(cid:12)(cid:12)h=0

= (cid:88)

j

vj

∂f
∂xj

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x=x∗

Deﬁnition 135 (Directional derivative). Assume f is diﬀerentiable. We deﬁne the scalar directional
derivative (Dvf)(x∗) of f in a direction v at a point x∗. Let x = x∗ + hv, Then

(Dvf)(x∗) = d
dh

f(x∗ + hv)

A.3.4 Matrix calculus

DRAFT March 9, 2010

= ∇f Tv

(A.3.14)

553

Deﬁnition 136 (Derivative of a matrix trace). For matrices A, and B

∂
∂A

trace (AB) ≡ BT

Deﬁnition 137 (Derivative of log det (A)).

∂ log det (A) = ∂trace (log A) = trace(cid:0)A−1∂A(cid:1)

So that

log det (A) = A−T

∂
∂A

Deﬁnition 138 (Derivative of a matrix inverse). For an invertible matrix A,

∂A−1 ≡ −A−T∂AA−1

A.4

Inequalities

A.4.1 Convexity

Inequalities

(A.3.15)

(A.3.16)

(A.3.17)

(A.3.18)

Deﬁnition 139 (Convex function). A function f(x) is deﬁned as convex if for any x, y and 0 ≤ λ ≤ 1

f(λx + (1 − λ)y) ≤ λf(x) + (1 − λ)f(y)

If −f(x) is convex, f(x) is called concave.

(A.4.1)

An intuitive picture of a convex function is to consider ﬁrst the quantity λx + (1 − λ)y. As we vary λ
from 0 to 1, this traces points between x (λ = 0) and y (λ = 1). Hence for λ = 0 we start at the point
x, f(x) and as λ increase trace a straight line towards the point y, f(y) at λ = 1. Convexity states that the
function f always lies below this straight line. Geometrically this means that the function f(x) is always
always increasing (never non-decreasing). Hence if d2f(x)/dx2 > 0 the function is convex.

As an example, the function log x is concave since its second derivative is negative:

d
dx

log x =

1
x

,

d2
dx2 log x = −

1
x2

A.4.2 Jensen’s inequality

For a convex function f(x), it follows directly from the deﬁnition of convexity that

f((cid:104)x(cid:105)p(x)) ≤ (cid:104)f(x)(cid:105)p(x)

for any distribution p(x).

554

(A.4.2)

(A.4.3)

DRAFT March 9, 2010

Gradient Descent

A.5 Optimisation

A.5.1 Critical points
When all ﬁrst-order partial derivatives at a point are zero (i.e. ∇f = 0) then the point is said to be a
stationary or critical point. Can be a minimum, maximum or saddle point.

Necessary ﬁrst-order condition for a minimum

There is a minimum of f at x∗ if f(x∗) ≤ f(x) for all x suﬃciently close to x∗. Let x = x∗ + hv for small
h and some direction v. Then by a Taylor expansion, for small h,

f(x∗ + hv) = f(x∗) + h∇f Tv + O(h2)

and thus for a minimum

h∇f Tv + O(h2) ≥ 0

Choosing v to be −∇f the condition becomes

−h∇f T∇f + O(h2) ≥ 0

(A.5.1)

(A.5.2)

(A.5.3)

and is violated for small positive h unless |∇f|2 = ∇f T∇f = 0. So x∗ can only be a local minimum if
|∇f(x∗)| = 0, i.e. if ∇f(x∗) = 0.

Necessary second-order condition for a minimum
At a stationary point ∇f = 0. Hence the Taylor expansion is given by

f(x∗ + hv) = f(x∗) + h2vTHf v + O(h3)

(A.5.4)

Thus the minimum condition requires that vTHf v ≥ 0, i.e. the Hessian is non-negative deﬁnite.

Deﬁnition 140 (Conditions for a minimum). Suﬃcient conditions for a minimum at x∗ are (i) ∇f(x∗) = 0
and (ii) Hf (x∗) is positive deﬁnite.

For a quadratic function f(x) = 1
reads:

2xTAx−bTx+ c, with symmetric A the necessary condition ∇f(x∗) = 0

Ax∗

− b = 0

If A is invertible this equation has the unique solution x∗ = A−1b.
minimum.

(A.5.5)
If A is positive deﬁnite, x∗ is a

A.6 Gradient Descent

Almost all of the search techniques that we consider are iterative, i.e. we proceed towards the minimum
x∗ by a sequence of steps. On the kth step we take a step of length αk in the direction pk,

xk+1 = xk + αkpk

(A.6.1)

The length of the step can either be chosen using prior knowledge, or by carrying out a line search in the
direction pk. It is the way that pk is chosen that tends to distinguish the diﬀerent methods of multivariate
optimization that we will discuss.

DRAFT March 9, 2010

555

Gradient Descent

We shall assume that we can analytically evaluate the gradient of f and will often use the shorthand
notation

gk = ∇f(xk) .

(A.6.2)

Typically we will want to choose pk using only gradient information; for large problems it can be very
expensive to compute the Hessian, and this can also require a large amount of storage.
Consider the change of variables x = My. Then

g(y) = f(x) = f(My)

and

=(cid:88)

j

∂g
∂yi

∂f
∂xi

∂xi
∂yj

so that

∇yg = M∇xf

(A.6.3)

(A.6.4)

(A.6.5)

Then the change in g is diﬀerent from the change in f, even though the only diﬀerence between the two
functions is the coordinate system. This unfortunate sensitivity to the parameterisation of the function is
partially addressed in ﬁrst order methods such as gradient descent by the natural gradient which uses a
prefactor designed to compensate for some of the lost invariance. We refer the reader to [7] for a description
of this method.

A.6.1 Gradient descent with ﬁxed stepsize
Locally, if we are at point xk, we can decrease f(x) by taking a step in the direction −g(x). If we make
the update equation

xk+1 = xk − ηgk

(A.6.6)

then we are doing gradient descent with ﬁxed stepsize η. If η is non-inﬁnitesimal, it is always possible
that we will step over the true minimum. Making η very small guards against this, but means that the
optimization process will take a very long time to reach a minimum.

To see why gradient descent works, consider the general update

xk+1 = xk + αpk

For small α we can expand f around xk using Taylor’s theorem:

f(xk + αkpk) ≈ f(xk) + αkgT

k pk .

With pk = −gk and for small positive αk, we see a guaranteed reduction:

f(xk + αkpk) ≈ f(xk) − αk||gk||2 .

A.6.2 Gradient descent with momentum

(A.6.7)

(A.6.8)

(A.6.9)

A simple idea that can improve convergence of gradient descent is to include at each iteration a proportion
of the change from the previous iteration. one uses

∆xk+1 = −η

∂E
∂x

+ α∆xk

where α is the momentum coeﬃcient.

556

(A.6.10)

DRAFT March 9, 2010

Multivariate Minimization: Quadratic functions

Figure A.5: Optimisation using line search along
steepest descent directions. Rushing oﬀ following the
steepest way downhill from a point (and continuing
for a ﬁnite time in that direction) doesn’t always re-
sult in the fastest way to get to the bottom!

A.6.3 Gradient descent with line searches

An extension to the idea of gradient descent is to choose the direction of steepest descent, as indicated
by the gradient g, but to calculate the value of the step to take which most reduces the value of E when
moving in that direction. This involves solving the one-dimensional problem of minimizing E(xk − λgk)
with respect to λ, and is known as a line search. That step is then taken and the process repeated again.

Finding the size of the step takes a little work; for example, you might ﬁnd three points along the line
such that the error at the intermediate point is less than at the other two, so that there is some minimum
along the line lies between the ﬁrst and second or between the second and third, and some kind of interval-
halving approach can then be used to ﬁnd it. (The minimum found in this way, just as with any sort of
gradient-descent algorithm, may not be a global minimum of course.) There are several variants of this
theme. Notice that if the step size is chosen to reduce E as much as it can in that direction, then no
further improvement in E can be made by moving in that direction for the moment. Thus the next step
will have no component in that direction; that is, the next step will be at right angles to the one just
taken. This can lead to zig-zag type behaviour in the optimisation, see ﬁg(A.5).

A.6.4 Exact line search condition

At the k-th step, we chose αk to minimize f(xk + αkpk). So setting F (λ) = f(xk + λpk), at this step
we solve the one-dimensional minimization problem for F (λ). Thus our choice of αk = λ∗ will satisfy
F (cid:48)(αk) = 0. Now

F (αk + h)|h=0 = d
dh

f(xk + αkpk + hpk)|h=0

F

(cid:48)(αk) = d
dh
= d
dh

(A.6.11)
So F (cid:48)(αk) = 0 means the directional derivative in the search direction must vanish at the new point and
this gives the Exact Line Search Condition:

f(xk+1 + hpk)|h=0 = (Dpk f)(xk+1) = ∇f T(xk+1)pk

0 = gT

k+1pk .

(A.6.12)

For a quadratic function f(x) = 1
calculate αk. Since ∇f(xk+1) = Axk + αkApk − b = ∇f(xk) + αkApk we ﬁnd

2xTAx−bTx+c, with symmetric , we can use the condition to analytically

αk = −

pT
k gk
pT
k Apk

.

(A.6.13)

A.7 Multivariate Minimization: Quadratic functions

The goal of this section is to derive eﬃcient algorithms for minimizing multivariate quadratic functions.
We shall begin by summarizing some properties of quadratic functions, and as byproduct obtain an eﬃcient
method for checking whether a symmetric matrix is positive deﬁnite.

A.7.1 Minimising quadratic functions using line search

Consider minimising the quadratic function

f(x) =

1
2

xTAx − bTx + c

DRAFT March 9, 2010

(A.7.1)

557

Multivariate Minimization: Quadratic functions

(cid:0)b − Ax0(cid:1)

pTAp

λ =

where A is positive deﬁnite and symmetric. (If A is not symmetric, consider instead the symmetrised
matrix (A + AT)/2, which gives the same function f). Although we know where the minimum of this
function is, just using linear algebra, we wish to use this function as a toy model for more complex functions
which however locally look approximately quadratic. One approach is to search along a particular direction
p, and ﬁnd a minimum along this direction. We can then search for a deeper minima by looking in diﬀerent
directions. That is, we can search along a line x0 + λp such that the function attains a minimum. That
is, the directional derivative is zero along this line, This has solution,

· p

≡ −∇f(x0) · p

pTAp

(A.7.2)

Now we’ve found the minimum along the line through x0 with direction p. But how should we choose the
line search direction p? It would seem sensible to choose successive line search directions p according to
pnew = −∇f(x∗), so that each time we minimise the function along the line of steepest descent. However,
this is far from the optimal choice in the case of minimising quadratic functions. A much better set of
search directions are those deﬁned by the vectors conjugate to A.

If the matrix A were diagonal, then the minimisation is straightforward and can be carried out inde-
pendently for each dimension. If we could ﬁnd an invertible matrix P with the property that PTAP is
diagonal then the solution is easy since for

f(ˆx) =

1
2

ˆxTPTAPˆx − bTPˆx + c

(A.7.3)

with x = Pˆx, we can compute the minimum for each dimension of ˆx separately and then retransform to
ﬁnd x∗ = Pˆx∗.

Deﬁnition 141 (Conjugate vectors). The vectors pi, i = 1, . . . , k are called conjugate to the matrix A, if
and only if for i, j = 1, . . . , k and i (cid:54)= j:
i Api > 0 .

pT
i Apj = 0

and pT

(A.7.4)

The two conditions guarantee that conjugate vectors are linearly independent: Assume that

k(cid:88)

0 =

i−1(cid:88)

k(cid:88)

αjpj =

αjpj + αipi +

αjpj

(A.7.5)

j=1

j=1

j=i+1

Now multiplying from the left with pT
As we can make this argument for any i = 1, . . . , k, all of the αi must be zero.

i A yields 0 = αipT

i Api. So αi is zero since we know that pT

i Api > 0.

A.7.2 Gram-Schmidt construction of conjugate vectors

Let P = (p1, p2, . . . , pk), where the columns are formed from A-conjugate vectors and note that we start
with an n by k matrix, k ≤ n. The reason for this is that we are aiming at an incremental procedure,
where columns are successively added to P. Since (PTAP)ij = pT
i Apj the matrix PTAP will be diagonal
i Apj = 0 for i (cid:54)= j. Assume we already have k conjugate vectors p1, . . . , pk and let v be a vector
if pT
which is linearly independent of p1, . . . , pk. We then set

pk+1 = v −

pT
j Av
j Apj
pT

pj

(A.7.6)

k(cid:88)

j=1

for which it is clear that the vectors p1, . . . , pk+1 are conjugate if A is positive deﬁnite. Using the Gram-
Schmidt procedure we can construct n conjugate vectors for a positive deﬁnite matrix in the following
way. We start with n linearly independent vectors u1, . . . , un, we might chose ui = ei, the unit vector in

558

DRAFT March 9, 2010

Multivariate Minimization: Quadratic functions

the ith direction. We then set p1 = u1 and use (A.7.6) to compute p2 from p1 and v = u2. Next we set
v = u3 and compute p3 from p1, p2 and v. Continuing in this manner we obtain n conjugate vectors.
Note that at each stage of the procedure the vectors u1, . . . , uk span the same subspace as the vectors
p1, . . . , pk. What is going to happen if A is not positive deﬁnite? If we could ﬁnd n conjugate vectors, A
would be positive deﬁnite, and so at some point k the Gram-Schmidt procedure must break down. This
will happen if pT
k Apk ≤ 0. So by trying out the Gram-Schmidt procedure, we can in fact ﬁnd out whether
a matrix is positive deﬁnite.

A.7.3 The conjugate vectors algorithm

Let us assume that when minimising f(x) = 1
conjugate to A which we use as our search directions. So

2xTAx − bTx + c we ﬁrst construct n vectors p1, . . . , pn

xk+1 = xk + αkpk .

At each step we chose αk by an exact line search, thus

αk = −

pT
k gk
k Apk
pT

.

(A.7.7)

(A.7.8)

This conjugate vectors algorithm, has the geometrical interpretation that not only is the directional deriva-
tive zero at the new point along the direction pk, it is zero along all the previous search directions
p1, . . . , pk).

Theorem 1 (Luenberger expanding subspace theorem).

i=1 be a sequence of vectors in Rn conjugate to the (positive deﬁnite) matrix A and
Let {pi}n
f(x) = 1
2xTAx − bTx + c. Then for any x1 the sequence {xk} generated according to (A.7.7) and (A.7.8)
has the property that the directional derivative of f in the direction pi vanishes at the point xk+1 if i ≤ k;
i.e. Dpif(xk+1) = 0.

Proof: For i ≤ k, we can write xk+1 as:

k(cid:88)

xk+1 = xi+1 +

αjpj .

(A.7.9)

j=i+1

Since ∇f(x) = Ax − b we have

∇f(xk+1) = Axk+1 − b = Axi+1 − b + A

k(cid:88)

j=i+1

αjpj = ∇f(xi+1) +

k(cid:88)

j=i+1

αjApj

(A.7.10)

So

Dpif(xk+1) = pT

i ∇f(xk+1) = pT

i ∇f(xi+1)+

k(cid:88)

j=i+1

αjpT

i Apj = (Dpif)(xi+1)+

k(cid:88)

j=i+1

αjpT

i Apj (A.7.11)

Now (Dxi+1f)(pi) = 0 since the point xi+1 was obtained by an exact line search in the direction pi. But all
i Apj = 0 by conjugacy. So (Dpif )(xk+1) = 0.
of the terms in the sum over j also vanish since j > i and pT

The subspace theorem shows, that because we use conjugate vectors, optimizing in the direction pk, does
not spoil the optimality w.r.t. to the previous search directions. In particular after having carried out n
steps of the algorithm we have (Dxn+1f)(pi) = ∇f T(xn+1)pi = 0, for i = 1, . . . , n. The n equations can
be written in a more compact form as:
∇f T(xn+1)(p1, p2, . . . , pn) = 0 .

(A.7.12)

DRAFT March 9, 2010

559

k(cid:88)

i=1

Multivariate Minimization: Quadratic functions

The square matrix P = (p1, p2, . . . pn) is invertible since the pi are conjugate, so ∇f(xn+1) = 0: The
point xn+1 is the minimum x∗ of the quadratic function f. So in contrast to gradient descent, for a
quadratic function the conjugate vectors algorithm converges in a ﬁnite number of steps.

A.7.4 The conjugate gradients algorithm

The conjugate gradients algorithm is a special case of the conjugate vectors algorithm, in which the Gram-
Schmidt procedure becomes very simple. We do not use a predetermined set of conjugate vectors but
construct these ‘on-the-ﬂy’. After k-steps of the conjugate vectors algorithm we need to construct a vector
pk+1 which is conjugate to p1, . . . , pk. This could be done by applying the Gram-Schmidt procedure to
any vector v which is linearly independent of the vectors p1, . . . , pk. In the conjugate gradients algorithm
one makes the special choice v = −∇f(xk+1). By the subspace theorem the gradient at the new point
xk+1 is orthogonal to pi, i = 1, . . . , k. So ∇f(xk+1) is linearly independent of p1, . . . , pk and a valid choice
for v, unless ∇f(xk+1) = 0. In the latter case xk+1 is our minimum and we are done, and from now on we
assume that ∇f(xk+1) (cid:54)= 0. Using the notation gk = ∇f(xk), the equation for the new search direction
given by the Gram-Schmidt procedure is:

pk+1 = −gk+1 +

pT
i Agk+1
pT
i Api

pi .

(A.7.13)

Since gk+1 is orthogonal to pi, i = 1, . . . , k, by the subspace theorem we have pT
αk+1 can be written as

k+1gk+1 = −gT

k+1gk+1. So

αk+1 =

k+1gk+1
gT
k+1Apk+1
pT

,

(A.7.14)

and in particular αk+1 (cid:54)= 0. We now want to show that because we have been using the conjugate gradients
algorithm at the previous steps as well, in equation (A.7.13) all terms but the last in the sum over i vanish.
We shall assume that k > 0 since in the ﬁrst step (k = 0) we just set p1 = −g1. First note that

gi+1 − gi = Axi+1 − b − (Axi − b) = A(xi+1 − xi) = αiApi

and since αi (cid:54)= 0:

Api = (gi+1 − gi)/αi.

So in equation (A.7.13):

i Agk+1 = gT
pT

k+1Api = gT

k+1(gi+1 − gi)/αi = (gT

k+1gi+1 − gT

k+1gi)/αi

(A.7.15)

(A.7.16)

(A.7.17)

Since the pi where obtained by applying the Gram-Schmidt procedure to the gradients gi, the subspace
theorem gT

k+1gi = 0 for i = 1, . . . , k. This shows that

k+1pi = 0, implies, also gT

(cid:26) 0

pT
i Agk+1 = (gT

k+1gi+1 − gT

k+1gi)/αi =

gT
k+1gk+1/αk

if 1 ≤ i < k
if i = k

Hence equation (A.7.13) simpliﬁes to

pk+1 = −gk+1 +

gT
k+1gk+1/αk
pT
k Apk

pk .

This can be brought into an even simpler form by applying equation (A.7.14) to αk:

pk+1 = −gk+1 +

k+1gk+1
gT
pT
k Apk
We shall write this in the form

k Apk
pT
gT
k gk

pk = −gk+1 +

gT
k+1gk+1
gT
k gk

pk

pk+1 = −gk+1 + βkpk where βk =

gT
k+1gk+1
gT
k gk

.

(A.7.18)

(A.7.19)

(A.7.20)

(A.7.21)

560

DRAFT March 9, 2010

Multivariate Minimization: Quadratic functions

Algorithm 31 Conjugate Gradients for minimising a function f(x)

5:

1: k = 1
2: Choose x1.
3: p1 = −g1
4: while gk (cid:54)= 0 do
αk = argmin
xk+1 := xk + αkpk
βk := gT
k+1gk+1/(gT
k gk)
pk+1 := −gk+1 + βkpk
k = k + 1

6:
7:
8:
9:
10: end while

f(xk + αkpk)

αk

(cid:46) Line Search

The formula (A.7.21) for βk is due to Fletcher and Reeves. Since the gradients are orthogonal, βk can also
be written as

βk =

gT
k+1(gk+1 − gk)

k gk
gT

,

(A.7.22)

this is the Polak-Ribiere formula. The choice between the two expression for βk can be of some importance
if f is not quadratic.

A.7.5 Newton’s method

Consider a function f(x) that we wish to ﬁnd the minimum of. A Taylor expansion up to second order
gives

f(x + ∆) = f(x) + ∆T∇f +

1
2

∆TH∆ + O(|∆|3)

(A.7.23)

The matrix H is the Hessian. Diﬀerentiating the right hand side with respect to ∆ (or, equivalently,
completing the square), we ﬁnd that the right hand side has its lowest value when

∇f = H∆ ⇒ ∆ = H−1∇f

Hence, an optimisation routine to minimise E is given by the Newton update

xk+1 = xk − H−1∇f

(A.7.24)

(A.7.25)

A beneﬁt of Newton method over gradient descent is that the decrease in the objective function is invariant
under a linear change of co-ordinates, y = Mx.

A.7.6 Quasi-Newton methods

For large-scale problems the inversion of the Hessian is computationally demanding, especially if the matrix
is close to singular. An alternative is to set up the iteration

xk+1 = xk − αkSkgk.

(A.7.26)
This is a very general form; if Sk = A−1 then we have Newton’s method, while if Sk = I we have steepest
descent. In general it would seem to be a good idea to choose Sk to be an approximation to the inverse
Hessian. Also note that it is important that Sk be positive deﬁnite so that for small αk we obtain a
descent method. The idea behind most quasi-Newton methods is to try to construct an approximate
inverse Hessian ˜Hk using information gathered as the descent progresses, and to set Sk = ˜Hk. As we have
seen, for a quadratic optimization problem we have the relationship

gk+1 − gk = A(xk+1 − xk)

DRAFT March 9, 2010

(A.7.27)

561

Algorithm 32 Quasi-Newton for minimising a function f(x)

Constrained Optimisation using Lagrange Multipliers

1: k = 1
2: Choose x1
3: ˜H1 = I
4: while gk (cid:54)= 0 do
pk = − ˜Hkgk
αk = argmin
xk+1 := xk + αkpk
sk = xk+1 − xk, yk = gk+1 − gk, and update ˜Hk+1
k = k + 1

f(xk + αkpk)

5:
6:

αk

7:
8:
9:
10: end while

Deﬁning

sk = xk+1 − xk

and

yk = gk+1 − gk

we see that equation A.7.27 becomes

yk = Ask

(cid:46) Line Search

(A.7.28)

(A.7.29)

It is reasonable to demand that
1 ≤ i ≤ k

˜Hk+1yi = si

(A.7.30)
After n linearly independent steps we would then have ˜Hn+1 = A−1. For k < n there are an inﬁnity of
solutions for ˜Hk+1 satisfying equation A.7.30. A popular choice is the Broyden-Fletcher-Goldfarb-Shanno
(or BFGS) update, given by

(cid:32)

(cid:33)

˜Hk+1 = ˜Hk +

1 +

˜Hkyk
yT
k
k sk
yT

skyT
k

sksT
k
k yk −
sT

˜Hk + ˜HkyksT

k

k yk
sT

(A.7.31)

(A.7.32)
(A.7.33)

This is a rank-2 correction to ˜Hk constructed from the vectors sk and ˜Hkyk.
The direction vectors p1, p2, . . . , pk, pk = − ˜Hkgk, produced by the algorithm obey

pT
i Apj = 0
˜Hk+1Api = pi

1 ≤ i < j ≤ k
1 ≤ i ≤ k

Equation A.7.33 is called the hereditary property. In our notation sk = αkpk, and as the α’s are non-zero,
equation A.7.32 can also be written as

sT
i Asj = 0

1 ≤ i < j ≤ k

(A.7.34)

Since the pk’s are A-conjugate and since we successively minimize f in these directions, we see that the
BFGS algorithm is a conjugate direction method; with the choice of H1 = I it is in fact the conjugate
gradient method. Note that the storage requirements for Quasi Newton methods scale quadratically with
the number of variables, and hence tends to be used for smaller problems. Limited memory BFGS reduces
the storage by only using the l latest updates in computing the approximate Hessian inverse, equation
(A.7.31). In contrast, the memory requirements for pure Conjugate Gradient methods scale only linearly
with the dimension of x.

A.7 Constrained Optimisation using Lagrange Multipliers

Single constraint

Consider ﬁrst the problem of minimising f(x) subject to a single constraint c(x) = 0. Imagine that we have
already identiﬁed an x that satisﬁes the constraint, that is c(x) = 0. How can we tell if this x minimises

562

DRAFT March 9, 2010

Constrained Optimisation using Lagrange Multipliers

the function f? We are only allowed to search for lower function values around this x in directions which
are consistent with the constraint. For a small change δ, the change in the constraint is,

c(x + δ) ≈ c(x) + δ · ∇c(x)

(A.7.1)
Hence, in order that the constraint remains satisﬁed, we can only search in a direction such that δ·∇c(x) =
0, that is in directions δ that are orthogonal to ∇c(x). So, let us explore the change in f along a direction
δ where δ · ∇c(x) = 0,

f(x + δ) ≈ f(x) + ∇f(x) · δ.

(A.7.2)
Since we are looking for a point x that minimises the function f, we require x to be a stationary point,
∇f(x) · δ = 0. Thus δ must be orthogonal to both ∇f(x) and ∇c(x). Since we wish to constrain δ as
little as possible, the most freedom is given by enforcing ∇f(x) to be parallel to ∇c(x), so that

∇f(x) = λ∇c(x)

(A.7.3)
for some λ ∈ R. To solve the optimisation problem therefore, we look for a point x such that ∇f(x) =
λ∇c(x), for some λ, and for which c(x) = 0. An alternative formulation of this dual requirement is to
look for x and λ that jointly minimise the Lagrangian

L(x, λ) = f(x) − λc(x)

(A.7.4)
Diﬀerentiating with respect to x, we get the requirement ∇f(x) = λ∇c(x), and diﬀerentiating with respect
to λ, we get that c(x) = 0.

Multiple constraints

Consider the problem of optimising f(x) subject to the constraints ci(x) = 0, i = 1, . . . , r < n, where
n is the dimensionality of the space. Denote by S the n − r dimensional subspace of x which obeys the
constraints. Assume that x∗ is such an optimum. As in the unconstrained case, we consider perturbations
v to x∗, but now such that v lies in S

2, . . . , a∗

ci(x∗ + hv) = ci(x∗) + vT∇ci(x∗) + O(h2)
(A.7.5)
i = ∇ci(x∗). Thus for the perturbation to stay within S, we require that vTa∗
Let a∗
i = 0 for all i = 1, . . . , r.
r. Then this condition can be rewritten as A∗v = 0.
1, a∗
Let A∗ be the matrix whose columns are a∗
We also require for a local optimum that vT∇f = 0 for all v in S. We see that ∇f must be orthogonal
to v, and that v must be orthogonal to the a∗
i ’s. This can be achieved by forcing ∇f to be a linear
combination of the a∗
i a∗
∗

(A.7.6)
Geometrically this says that the gradient vector is normal to the tangent plane to S at x∗. These conditions
give rise to the method of Lagrange multipliers for optimisation problems with equality constraints. The
method requires ﬁnding x∗ and λ∗ which solve the equations

r(cid:88)

∇f =

i ’s, i.e.

i=1

λ

i

∇f = (cid:88)

i

ai(x)λi

(A.7.7)

ci(x) = 0

for i = 1, . . . , r

(A.7.8)
There are n + r equations and n + r unknowns, so the system is well-determined. However, the system is
nonlinear (in x) in general, and so may not be easy to solve. We can restate these conditions by introducing
the Lagrangian function
L(x, λ) = f(x) −

(cid:88)

λici(x).

(A.7.9)

i

The partial derivatives of L with respect to x and λ reproduce equations A.7.7 and A.7.8. Hence a
necessary condition for a local minimizer is that x∗, λ∗ is a stationary point of the Lagrangian function.
Note that this stationary point is not a minimum but a saddle point, as L depends linearly on λ. We have
given ﬁrst-order necessary and suﬃcient conditions for a local optimum. To show that this optimum is a
local minimum, we would need to consider second-order conditions, analogous to the positive deﬁniteness
of the Hessian in the unconstrained case; this can be done, but will not be considered here.

DRAFT March 9, 2010

563

Constrained Optimisation using Lagrange Multipliers

564

DRAFT March 9, 2010

Bibliography

[1] L. F. Abbott, J. A. Varela, K. Sen, and S. B. Nelson. Synaptic Depression and Cortical Gain Control. Science,

275:220–223, 1997.

[2] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A Learning Algorithm for Boltzmann Machines. Cognitive

Science, 9:147–169, 1985.

[3] R. P. Adams and D. J. C. MacKay. Bayesian Online Changepoint Detection. Cavendish laboratory, department

of physics, University of Cambridge, Cambridge, UK, 2006. arXiv:0710.3742v1 [stat.ML].

[4] E. Airoldi, D. Blei, E. Xing, and S. Fienberg. A latent mixed membership model for relational data.

In
LinkKDD ’05: Proceedings of the 3rd international workshop on Link discovery, pages 82–89, New York, NY,
USA, 2005. ACM.

[5] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing. Mixed membership stochastic blockmodels. Journal

of Machine Learning Research, 9:1981–2014, 2008.

[6] D. L. Alspach and H. W. Sorenson. Nonlinear Bayesian Estimation Using Gaussian Sum Approximations.

IEEE Transactions on Automatic Control, 17(4):439–448, 1972.

[7] S-i. Amari. Natural Gradient Works Eﬃciently in Learning. Neural Computation, 10(2):251–276, 1998.

[8] S-i. Amari. Natural Gradient Learning for Over and Under-Complete Bases in ICA. Neural Computation,

11:1875–1883, 1999.

[9] I. Androutsopoulos, J. Koutsias, K. V. Chandrinos, and C. D. Spyropoulos. An experimental comparison
In Proceedings of
of naive bayesian and keyword-based anti-spam ﬁltering with personal e-mail messages.
the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,
pages 160–167, New York, NY, USA, 2000. ACM.

[10] S. Arora and C. Lund. Hardness of approximations. In Approximation algorithms for NP-hard problems, pages

399–446. PWS Publishing Co., Boston, MA, USA, 1997.

[11] F. R. Bach and M. I. Jordan. Thin junction trees. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,
Advances in Neural Information Processing Systems (NIPS), number 14, pages 569–576, Cambridge, MA,
2001. MIT Press.

[12] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Computer Science

Division and Department of Statistics 688, University of California Berkeley, Berkeley, USA, 2005.

[13] Y. Bar-Shalom and Xiao-Rong Li. Estimation and Tracking : Principles, Techniques and Software. Artech

House, Norwood, MA, 1998.

[14] D. Barber. Dynamic Bayesian Networks with Deterministic Tables. In S. Becker, S. Thrun, and K. Obermayer,
editors, Advances in Neural Information Processing Systems (NIPS), number 15, pages 713–720, Cambridge,
MA, 2003. MIT Press.

[15] D. Barber. Learning in Spiking Neural Assemblies.

In S. Becker, S. Thrun, and K. Obermayer, editors,
Advances in Neural Information Processing Systems (NIPS), number 15, pages 149–156, Cambridge, MA,
2003. MIT Press.

565

BIBLIOGRAPHY

BIBLIOGRAPHY

[16] D. Barber. Are two Classiﬁers performing equally? A treatment using Bayesian Hypothesis Testing. IDIAP-

RR 57, IDIAP, Rue de Simplon 4, Martigny, CH-1920, Switerland, May 2004. IDIAP-RR 04-57.

[17] D. Barber. Expectation Correction for smoothing in Switching Linear Gaussian State Space models. Journal

of Machine Learning Research, 7:2515–2540, 2006.

[18] D. Barber. Clique Matrices for Statistical Graph Decomposition and Parameterising Restricted Positive
In D. A. McAllester and P. Myllymaki, editors, Uncertainty in Artiﬁcial Intelligence,

Deﬁnite Matrices.
number 24, pages 26–33, Corvallis, Oregon, USA, 2008. AUAI press.

[19] D. Barber and F. V. Agakov. Correlated sequence learning in a network of spiking neurons using maximum

likelihood. Informatics Research Reports EDI-INF-RR-0149, Edinburgh University, 2002.

[20] D. Barber and F.V. Agakov. The IM Algorithm: A variational approach to Information Maximization. In

Advances in Neural Information Processing Systems (NIPS), number 16, 2004.

[21] D. Barber and C. M. Bishop. Bayesian Model Comparison by Monte Carlo Chaining. In M. C. Mozer, M. I.
Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems (NIPS), number 9, pages
333–339, Cambridge, MA, 1997. MIT Press.

[22] D. Barber and C. M. Bishop. Ensemble Learning in Bayesian Neural Networks.

Machine Learning, pages 215–237. Springer, 1998.

In Neural Networks and

[23] D. Barber and S. Chiappa. Uniﬁed Inference for Variational Bayesian Linear Gaussian State-Space Models. In
B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in Neural Information Processing Systems (NIPS),
number 19, pages 81–88, Cambridge, MA, 2007. MIT Press.

[24] D. Barber and W. Wiegerinck. Tractable Variational Structures for Approximating Graphical Models. In M. S.
Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems (NIPS),
number 11, pages 183–189, Cambridge, MA, 1999. MIT Press.

[25] D. Barber and C. K. I. Williams. Gaussian processes for Bayesian classiﬁcation via hybrid Monte Carlo. In
M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems NIPS
9, pages 340–346, Cambridge, MA, 1997. MIT Press.

[26] R. J. Baxter. Exactly solved models in statistical mechanics. Academic Press, 1982.

[27] M. J. Beal, F. Falciani, Z. Ghahramani, C. Rangel, and D. L. Wild. A Bayesian approach to reconstructing

genetic regulatory networks with hidden factors. Bioinformatics, (21):349–356, 2005.

[28] A. Becker and D. Geiger. A suﬃciently fast algorithm for ﬁnding close to optimal clique trees. Artiﬁcial

Intelligence, 125(1-2):3–17, 2001.

[29] A. J. Bell and T. J. Sejnowski. An Information-Maximization Approach to Blind Separation and Blind

Deconvolution. Neural Computation, 7(6):1129–1159, 1995.

[30] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957. Paperback edition

by Dover Publications (2003).

[31] Y. Bengio and P. Frasconi.

(7):1231–1249, 1996.

Input-Output HMMs for sequence processing. IEEE Trans. Neural Networks,

[32] A. L. Berger, S. D. Della Pietra, and V. J. D. Della Pietra. A maximum entropy approach to natural language

processing. Computational Linguistics, 22(1):39–71, 1996.

[33] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, second edition, 1985.

[34] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, second edition, 2000.

[35] J. Besag. Spatial Interactions and the Statistical Analysis of Lattice Systems. Journal of the Royal Statistical

Society, Series B, 36(2):192–236, 1974.

[36] J. Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society, Series B,

48:259–302, 1986.

[37] J. Besag and P. Green. Spatial statistics and Bayesian computation. Journal of the Royal Statistical Society,

Series B, 55:25–37, 1993.

566

DRAFT March 9, 2010

BIBLIOGRAPHY

BIBLIOGRAPHY

[38] G. J. Bierman. Measurement updating using the U-D factorization. Automatica, 12:375–382, 1976.

[39] N. L. Biggs. Discrete Mathematics. Oxford University Press, 1990.

[40] K. Binder and A. P. Young. Spin glasses: Experimental facts, theoretical concepts, and open questions. Rev.

Mod. Phys., 58(4):801–976, Oct 1986.

[41] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.

[42] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.

[43] C. M. Bishop and M. Svens´en. Bayesian hierarchical mixtures of experts. In U. Kjaerulﬀ and C. Meek, editors,
Proceedings Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 57–64. Morgan Kaufmann,
2003.

[44] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of machine Learning Research, (3):993–

1022, 2003.

[45] R. R. Bouckaert. Bayesian belief networks: from construction to inference. PhD thesis, University of Utrecht,

1995.

[46] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

[47] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. IEEE Trans. Pattern Anal. Mach. Intell., 26(9):1124–1137, 2004.

[48] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. IEEE Trans.

Pattern Anal. Mach. Intell., 23:1222–1239, 2001.

[49] M. Brand.

Incremental singular value decomposition of uncertain data with missing values.

Conference on Computer Vision (ECCV), pages 707–720, 2002.

In European

[50] J. Breese and D. Heckerman. Decision-theoretic troubleshooting: A framework for repair and experiment.
In E. Horvitz and F. Jensen, editors, Uncertainty in Artiﬁcial Intelligence, number 12, pages 124–132, San
Francisco, CA, 1996. Morgan Kaufmann.

[51] H. Bunke and T. Caelli. Hidden Markov models: applications in computer vision. Machine Perception and

Artiﬁcial Intelligence. World Scientiﬁc Publishing Co., Inc., River Edge, NJ, USA, 2001.

[52] W. Buntine. Theory reﬁnement on Bayesian networks. In Uncertainty in Artiﬁcial Intelligence, number 7,

pages 52–60, San Francisco, CA, 1991. Morgan Kaufmann.

[53] A. Cano and S. Moral. Advances in Intelligent Computing – IPMU 1994, chapter Heuristic Algorithms for the
Triangulation of Graphs, pages 98–107. Number 945 in Lectures Notes in Computer Sciences. Springer-Verlag,
1995.

[54] O. Capp´e, E. Moulines, and T. Ryden. Inference in Hidden Markov Models. Springer, New York, 2005.

[55] E. Castillo, J. M. Gutierrez, and A. S. Hadi. Expert Systems and Probabilistic Network Models. Springer,

1997.

[56] A. T. Cemgil. Bayesian Inference in Non-negative Matrix Factorisation Models. Technical Report CUED/F-

INFENG/TR.609, University of Cambridge, July 2008.

[57] A. T. Cemgil, B. Kappen, and D. Barber. A Generative Model for Music Transcription. IEEE Transactions

on Audio, Speech and Language Processing, 14(2):679–694, 2006.

[58] H. S. Chang, M. C. Fu, J. Hu, and S. I. Marcus. Simulation-based Algorithms for Markov Decision Processes.

Springer, 2007.

[59] S. Chiappa and D. Barber. Bayesian Linear Gaussian State Space Models for Biosignal Decomposition. Signal

Processing Letters, 14(4):267–270, 2007.

[60] S. Chib and M. Dueker. Non-Markovian Regime Switching with Endogenous States and Time-Varying State
Strengths. Econometric Society 2004 North American Summer Meetings 600, Econometric Society, August
2004.

[61] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE

Transactions on Information Theory, 14(3):462–467, 1968.

DRAFT March 9, 2010

567

BIBLIOGRAPHY

BIBLIOGRAPHY

[62] P. S. Churchland and T. J. Sejnowski. The Computational Brain. MIT Press, Cambridge, MA, USA, 1994.

[63] D. Cohn and H. Chang. Learning to probabilistically identify authoritative documents. In P. Langley, editor,

International Conference on Machine Learning, number 17, pages 167–174. Morgan Kaufmann, 2000.

[64] D. Cohn and T. Hofmann. The Missing Link - A Probabilistic Model of Document Content and Hypertext

Connectivity. Number 13, pages 430–436, Cambridge, MA, 2001. MIT Press.

[65] A. C. C. Coolen, R. K¨uhn, and P. Sollich. Theory of Neural Information Processing Systems. Oxford University

Press, 2005.

[66] G. F. Cooper and E. Herskovits. A Bayesian Method for the Induction of Probabilistic Networks from Data.

Machine Learning, 9(4):309–347, 1992.

[67] A. Corduneanu and C. M. Bishop. Variational Bayesian Model Selection for Mixture Distributions.

In
T. Jaakkola and T. Richardson, editors, Artifcial Intelligence and Statistics, pages 27–34. Morgan Kaufmann,
2001.

[68] M. T. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991.

[69] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. Probabilistic Networks and Expert Systems.

Springer, 1999.

[70] D. R. Cox and N. Wermuth. Multivariate Dependencies. Chapman and Hall, 1996.

[71] N. Cristianini and J. Shawe-Taylor. An Introduction To Support Vector Machines. Cambridge University

Press, 2000.

[72] P. Dangauthier, R. Herbrich, T. Minka, and T. Graepel. Trueskill through time: Revisiting the history of
chess. In B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in Neural Information Processing Systems
(NIPS), number 19, pages 569–576, Cambridge, MA, 2007. MIT Press.

[73] H. A. David. The method of paired comparisons. Oxford University Press, New York, 1988.

[74] A. P. Dawid. Inﬂuence diagrams for causal modelling and inference. International Statistical Review, 70:161–

189, 2002.

[75] A. P. Dawid and S. L. Lauritzen. Hyper Markov Laws in the Statistical Analysis of Decomposable Graphical

Models. Annals of Statistics, 21(3):1272–1317, 1993.

[76] P. Dayan and L.F. Abbott. Theoretical Neuroscience. MIT Press, 2001.

[77] P. Dayan and G. E. Hinton. Using Expectation-Maximization for Reinforcement Learning. Neural Computa-

tion, 9:271–278, 1997.

[78] T. De Bie, N. Cristianini, and R. Rosipal. Handbook of Geometric Computing : Applications in Pattern
Recognition, Computer Vision, Neuralcomputing, and Robotics, chapter Eigenproblems in Pattern Recognition.
Springer-Verlag, 2005.

[79] R. Dechter. Bucket Elimination: A unifying framework for probabilistic inference algorithms. In E. Horvitz
and F. Jensen, editors, Uncertainty in Artiﬁcial Intelligence, pages 211–219, San Francisco, CA, 1996. Morgan
Kaufmann.

[80] S. Diederich and M. Opper. Learning of Correlated Patterns in Spin-Glass Networks by Local Learning Rules.

Physical Review Letters, 58(9):949–952, 1986.

[81] R. Diestel. Graph Theory. Springer, 2005.

[82] A. Doucet and A. M. Johansen. A Tutorial on Particle Filtering and Smoothing: Fifteen years later.

In
D. Crisan and B. Rozovsky, editors, Oxford Handbook of Nonlinear Filtering. Oxford University Press, 2009.

[83] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. Wiley-Interscience Publication, 2000.

[84] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis : Probabilistic Models of

Proteins and Nucleic Acids. Cambridge University Press, 1999.

[85] A. D¨uring, A. C. C. Coolen, and D. Sherrington. Phase diagram and storage capacity of sequence processing

neural networks. Journal of Physics A, 31:8607–8621, 1998.

568

DRAFT March 9, 2010

BIBLIOGRAPHY

BIBLIOGRAPHY

[86] J. M. Gutierrez E. Castillo and A. S. Hadi. Expert Systems and Probabilistic Network Models. Springer Verlag,

1997.

[87] J. Edmonds and R. M. Karp. Theoretical improvements in algorithmic eﬃciency for network ﬂow problems.

Journal of the ACM, 19(2):248–264, 1972.

[88] R. Edwards and A. Sokal. Generalization of the fortium-kasteleyn-swendson-wang representation and monte

carlo algorithm. Physical Review D, 38:2009–2012, 1988.

[89] A. E. Elo. The rating of chess players, past and present. Arco, New York, second edition, 1986.

[90] Y. Ephraim and W. J. J. Roberts. Revisiting autoregressive hidden Markov modeling of speech signals. IEEE

Signal Processing Letters, 12(2):166–169, February 2005.

[91] E. Erosheva, S. Fienberg, and J. Laﬀerty. Mixed membership models of scientiﬁc publications. In Proceedings

of the National Academy of Sciences, volume 101, pages 5220–5227, 2004.

[92] R-E. Fan, P-H. Chen, and C-J. Lin. Working Set Selection Using Second Order Information for Training

Support Vector Machines. Journal of Machine Learning Research, 6:1889–1918, 2005.

[93] P. Fearnhead. Exact and Eﬃcient Bayesian inference for multiple changepoint problems. Technical report,

Deptartment of Mathematics and Statistics, Lancaster University, 2003.

[94] G. H. Fischer and I. W. Molenaar. Rasch Models: Foundations, Recent Developments, and Applications.

Springer, New York, 1995.

[95] M. E. Fisher. Statistical Mechanics of Dimers on a Plane Lattice. Physical Review, 124:1664–1672, 1961.

[96] B. Frey. Extending Factor Graphs as to Unify Directed and Undirected Graphical Models. In C. Meek and
U. Kjærulﬀ, editors, Uncertainty in Artiﬁcial Intelligence, number 19, pages 257–264. Morgan Kaufmann,
2003.

[97] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian Network Classiﬁers. Machine Learning, 29:131–163,

1997.

[98] S. Fr¨uhwirth-Schnatter. Finite Mixture and Markov Switching Models. Springer, 2006.

[99] M. Frydenberg. The chain graph Markov property. Scandanavian Journal of Statistics, 17:333–353, 1990.

[100] T. Furmston and D. Barber. Solving deterministic policy (PO)MDPs using Expectation-Maximisation and
Antifreeze. In E. Suzuki and M. Sebag, editors, European Conference on Machine Learning and Principles
and Practice of Knowledge discovery in Databases, September 2009. Workshop on Learning and Data Mining
for Robotics.

[101] A. Galka, O. Yamashita, T. Ozaki, R. Biscay, and P. Valdes-Sosa. A solution to the dynamical inverse problem

of EEG generation using spatiotemporal Kalman ﬁltering. NeuroImage, (23):435–453, 2004.

[102] P. Gandhi, F. Bromberg, and D. Margaritis. Learning markov network structure using few independence tests.

In Proceedings of the SIAM International Conference on Data Mining, pages 680–691, 2008.

[103] M. R. Garey and D. S. Johnson. Computers and Intractability, A Guide to the Theory of NP-Completeness.

W.H. Freeman and Company, New York, 1979.

[104] A. Gelb. Applied optimal estimation. MIT press, 1974.

[105] A. Gelman, G. O. Roberts, and W. R. Gilks. Eﬃcient Metropolis jumping rules.

In J. O. Bernardo, J.
M. Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics, volume 5, pages 599–607. Oxford
University Press, 1996.

[106] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. In
Readings in uncertain reasoning, pages 452–472, San Francisco, CA, USA, 1990. Morgan Kaufmann Publishers
Inc.

[107] M. G. Genton. Classes of kernels for machine learning: A statistics perspective. Journal of Machine Learning

Research, 2:299–312, 2001.

[108] W. Gerstner and W. M. Kistler. Spiking Neuron Models. Cambridge University Press, 2002.

DRAFT March 9, 2010

569

BIBLIOGRAPHY

BIBLIOGRAPHY

[109] Z. Ghahramani and M. J. Beal. Variational Inference for Bayesian Mixtures of Factor Analysers. In S. A.
Solla, T. K. Leen, and K-R. M¨uller, editors, Advances in Neural Information Processing Systems (NIPS),
number 12, pages 449–455, Cambridge, MA, 2000. MIT Press.

[110] Z. Ghahramani and G. E. Hinton. Variational learning for switching state-space models. Neural Computation,

12(4):963–996, 1998.

[111] A. Gibbons. Algorithmic Graph Theory. Cambridge University Press, 1991.

[112] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov chain Monte Carlo in practice. Chapman & Hall,

1996.

[113] M. Girolami and A. Kaban. On an equivalence between PLSI and LDA. In Proceedings of the 26th annual
international ACM SIGIR conference on Research and development in information retrieval, pages 433–434,
New York, NY, USA, 2003. ACM Press.

[114] M. E. Glickman. Parameter estimation in large dynamic paired comparison experiments. Applied Statistics,

48:377–394, 1999.

[115] A. Globerson and T. Jaakkola. Approximate inference using planar graph decomposition. In B. Sch¨olkopf,
J. Platt, and T. Hoﬀman, editors, Advances in Neural Information Processing Systems (NIPS), number 19,
pages 473–480, Cambridge, MA, 2007. MIT Press.

[116] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry. Using collaborative ﬁltering to weave an information

tapestry. Communications ACM, 35:61–70, 1992.

[117] G. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins University Press, 3rd edition, 1996.

[118] M. C. Golumbic and I. Ben-Arroyo Hartman. Graph Theory, Combinatorics, and Algorithms. Springer-Verlag,

2005.

[119] C. Goutis. A graphical method for solving a decision analysis problem. IEEE Transactions on Systems, Man

and Cybernetics, 25:1181–1193, 1995.

[120] P. J. Green and B. W. Silverman. Nonparametric Regression and Generalized Linear Models, volume 58 of

Monographs on Statistics and Applied Probability. Chapman and Hall, 1994.

[121] D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

Journal of the Royal Statistical Society, Series B, 2:271–279, 1989.

[122] G. Grimmett and D. Stirzaker. Probability and Random Processes. Oxford University Press, second edition,

1992.

[123] S. F. Gull. Bayesian data analysis: straight-line ﬁtting. In J. Skilling, editor, Maximum entropy and Bayesian

methods (Cambridge 1988), pages 511–518. Kluwer, 1989.

[124] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions. Chapman and Hall/CRC, Boca Raton, Florida

USA, 1999.

[125] D. J. Hand and K. Yu. Idiot’s Bayes—Not So Stupid After All? International Statistical Review, 69(3):385–398,

2001.

[126] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical Correlation Analysis: An Overview with Appli-

cation to Learning Methods. Neural Computation, 16(12):2639–2664, 2004.

[127] D. O. Hebb. The organization of behavior. Wiley, New York, 1949.

[128] D. Heckerman. A Tutorial on Learning With Bayesian Networks. Technical Report MSR-TR-95-06, Microsoft

Research, Redmond, WA, March 1996. Revised November 1996.

[129] D. Heckerman, D. Geiger, and D. Chickering. Learning Bayesian Networks: The Combination of Knowledge

and Statistical Data. Machine Learning, 20(3):197–243, 1995.

[130] R. Herbrich, T. Minka, and T. Graepel. TrueSkillTM: A Bayesian Skill Rating System.

In B. Sch¨olkopf,
J. Platt, and T. Hoﬀman, editors, Advances in Neural Information Processing Systems (NIPS), number 19,
pages 569–576, Cambridge, MA, 2007. MIT Press.

[131] H. Hermansky. Should recognizers have ears? Speech Communication, 25:3–27, 1998.

570

DRAFT March 9, 2010

BIBLIOGRAPHY

BIBLIOGRAPHY

[132] J. Hertz, A. Krogh, and R. Palmer. Introduction to the theory of Neural Computation. Addison-Wesley, 1991.

[133] T. Heskes. Convexity arguments for eﬃcient minimization of the Bethe and Kikuchi free energies. Journal of

Artiﬁcial Intelligence Research, 26:153–190, 2006.

[134] D. M. Higdon. Auxiliary variable methods for Markov chain Monte Carlo with applications. Journal of the

American Statistical Association, 93(442):585–595, 1998.

[135] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,

(313):504–507, 2006.

[136] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning from dyadic data. In M. S. Kearns, S. A. Solla, and
D. A. Cohn, editors, Advances in Neural Information Processing Systems (NIPS), pages 466–472, Cambridge,
MA, 1999. MIT Press.

[137] R. A. Howard and J. E. Matheson. Inﬂuence diagrams. Decision Analysis, 2(3), 2005. Republished version of

the original 1981 report.

[138] A. Hyv¨arinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley, 2001.

[139] Aapo Hyv¨arinen. Consistency of Pseudolikelihood Estimation of Fully Visible Boltzmann Machines. Neural

Computation, 18(10):2283–2292, 2006.

[140] M. Isard and A. Blake. CONDENSATION Conditional Density Propagation for Visual Tracking. International

Journal of Computer Vision, 29:5–28, 1998.

[141] T. S. Jaakkola and M. I. Jordan. Variational probabilistic inference and the qmr-dt network. Journal of

Artiﬁcial Intelligence Research, 10:291–322, 1999.

[142] T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods. Statistics and

Computing, 10(1):25–37, 2000.

[143] R. A. Jacobs, F. Peng, and M. A. Tanner. A Bayesian approach to model selection in hierarchical mixtures-

of-experts architectures. Neural Networks, 10(2):231–241, 1997.

[144] R. G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, (66):191–193, 1979.

[145] E. T. Jaynes. Probability Theory : The Logic of Science. Cambridge University Press, 2003.

[146] F. Jensen, F. V. Jensen, and D. Dittmer. From inﬂuence diagrams to junction trees. In Proceedings of the
10th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-94), pages 367–373, San Francisco, CA,
1994. Morgan Kaufmann.

[147] F. V. Jensen and F. Jensen. Optimal Junction Trees.

In R. Lopez de Mantaras and D. Poole, editors,
Uncertainty in Artiﬁcial Intelligence, number 10, pages 360–366, San Francisco, CA, 1994. Morgan Kaufmann.

[148] F. V. Jensen and T. D. Nielson. Bayesian Networks and Decision Graphs. Springer Verlag, second edition,

2007.

[149] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation,

6:181–214, 1994.

[150] B. H. Juang, W. Chou, and C. H. Lee. Minimum classiﬁcation error rate methods for speech recognition.

IEEE Transactions on Speech and Audio Processing, 5:257–265, 1997.

[151] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic

domains. Artiﬁcial Intelligence, 101(1-2):99–134, 1998.

[152] H. J. Kappen. An introduction to stochastic control theory, path integrals and reinforcement learning. In
Proceedings 9th Granada seminar on Computational Physics: Computational and Mathematical Modeling of
Cooperative Behavior in Neural Systems, volume 887, pages 149–181. American Institute of Physics, 2007.

[153] H. J. Kappen and F. B. Rodr´ıguez. Eﬃcient learning in Boltzmann machines using linear response theory.

Neural Compution, 10(5):1137–1156, 1998.

[154] H. J. Kappen and W. Wiegerinck. Novel iteration schemes for the Cluster Variation Method.

In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems (NIPS),
number 14, pages 415–422, Cambridge, MA, 2002. MIT Press.

DRAFT March 9, 2010

571

BIBLIOGRAPHY

BIBLIOGRAPHY

[155] Y. Karklin and M. S. Lewicki. Emergence of complex cell properties by learning to generalize in natural scenes.

Nature, (457):83–86, November 2008.

[156] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. Siam

Journal on Scientiﬁc Computing, 20(1):359–392, 1998.

[157] P. W. Kasteleyn. Dimer Statistics and Phase Transitions. Journal of Mathematical Physics, 4(2):287–293,

1963.

[158] S. A. Kauﬀman. At Home in the Universe: The Search for Laws of Self-Organization and Complexity. Oxford

University Press, Oxford, UK, 1995.

[159] C-J. Kim. Dynamic linear models with Markov-switching. Journal of Econometrics, 60:1–22, 1994.

[160] C-J. Kim and C. R. Nelson. State-Space models with regime switching. MIT Press, 1999.

[161] G. Kitagawa. The Two-Filter Formula for Smoothing and an implementation of the Gaussian-sum smoother.

Annals of the Institute of Statistical Mathematics, 46(4):605–623, 1994.

[162] U. B. Kjaerulﬀ and A. L. Madsen. Bayesian Networks and Inﬂuence Diagrams : A Guide to Construction

and Analysis. Springer, 2008.

[163] A. Krogh, M. Brown, I. Mian, K. Sjolander, and D. Haussler. Hidden Markov models in computational biology:

Applications to protein modeling. Journal of Molecular Biology, 235:1501–1531, 1994.

[164] S. Kullback. Information Theory and Statistics. Dover, 1968.

[165] K. Kurihara, M. Welling, and Y. W. Teh. Collapsed Variational Dirichlet Process Mixture Models.

In
Proceedings of the International Joint Conference on Artiﬁcial Intelligence, volume 20, pages 2796–2801, 2007.

[166] J. Laﬀerty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and
labeling sequence data. In C. E. Brodley and A. P. Danyluk, editors, International Conference on Machine
Learning, number 18, pages 282–289, San Francisco, CA, 2001. Morgan Kaufmann.

[167] H. Lass. Elements of Pure and Applied Mathematics. McGraw-Hill (reprinted by Dover), 1957.

[168] S. L. Lauritzen. Graphical Models. Oxford University Press, 1996.

[169] S. L. Lauritzen, A. P. Dawid, B. N. Larsen, and H-G. Leimer. Independence properties of directed Markov

ﬁelds. Networks, 20:491–505, 1990.

[170] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and

their application to expert systems. Journal of Royal Statistical Society B, 50(2):157 – 224, 1988.

[171] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In T. K. Leen, T. G. Dietterich,
and V. Tresp, editors, Advances in Neural Information Processing Systems (NIPS), number 13, pages 556–562,
Cambridge, MA, 2001. MIT Press.

[172] M. A. R. Leisink and H. J. Kappen. A Tighter Bound for Graphical Models. In Neural Computation, volume 13,

pages 2149–2171. MIT Press, 2001.

[173] V. Lepar and P. P. Shenoy. A Comparison of Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer Architectures
for Computing Marginals of Probability Distributions. In G. Cooper and S. Moral, editors, Uncertainty in
Artiﬁcial Intelligence, number 14, pages 328–333, San Francisco, CA, 1998. Morgan Kaufmann.

[174] U. Lerner, R. Parr, D. Koller, and G. Biswas. Bayesian Fault Detection and Diagnosis in Dynamic Systems. In
Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence (AIII-00), pages 531–537, 2000.

[175] U. N. Lerner. Hybrid Bayesian Networks for Reasoning about Complex Systems. Computer science department,

Stanford University, 2002.

[176] R. Linsker.

Improved local learning rule for information maximization and related applications. Neural

Networks, 18(3):261–265, 2005.

[177] Y. L. Loh, E. W. Carlson, and M. Y. J. Tan. Bond-propagation algorithm for thermodynamic functions in

general two-dimensional Ising models. Physical Review B, 76(1):014404, 2007.

[178] H. Lopes and M. West. Bayesian model assessment in factor analysis. Statistica Sinica, (14):41–67, 2003.

572

DRAFT March 9, 2010

BIBLIOGRAPHY

BIBLIOGRAPHY

[179] T. J. Loredo. From Laplace To Supernova Sn 1987A: Bayesian Inference In Astrophysics. In P.F. Fougere,

editor, Maximum Entropy and Bayesian Methods, pages 81–142. Kluwer, 1990.

[180] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992.

[181] D. J. C. MacKay. Probable Networks and plausisble predictions – a review of practical Bayesian methods for

supervised neural networks. Network: Computation in Neural Systems, 6(3):469–505, 1995.

[182] D. J. C. MacKay. Introduction to Gaussian Processes. In Neural Networks and Machine Learning, volume 168
of NATO advanced study institute on generalization in neural networks and machine learning, pages 133–165.
Springer, August 1998.

[183] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003.

[184] U. Madhow. Fundamentals of Digital Communication. Cambridge University Press, 2008.

[185] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate Analysis. Academic Press, 1997.

[186] H. Markram, J. Lubke, M. Frotscher, and B. Sakmann. Regulation of synaptic eﬃcacy by coincidence of

postsynaptic APs and EPSPs. Science, 275:213–215, 1997.

[187] G. McLachlan and T. Krishnan. The EM Algorithm and Extensions. John Wiley and Sons, 1997.

[188] G. McLachlan and D. Peel. Finite Mixture Models. Wiley Series in Probability and Statistics. Wiley-

Interscience, 2000.

[189] E. Meeds, Z. Ghahramani, R. M. Neal, and S. T. Roweis. Modeling Dyadic Data with Binary Latent Factors. In
B. Sch¨olkopf, J. Platt, and T. Hoﬀman, editors, Advances in Neural Information Processing Systems (NIPS),
volume 19, pages 977–984, Cambridge, MA, 2007. MIT Press.

[190] M. Meila. An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse
Data. In I. Bratko, editor, International Conference on Machine Learning, pages 249–257, San Francisco, CA,
1999. Morgan Kaufmann.

[191] M. Meila and M. I. Jordan. Triangulation by continuous embedding.

In M. C. Mozer, M. I. Jordan, and
T. Petsche, editors, Advances in Neural Information Processing Systems (NIPS), number 9, pages 557–563,
Cambridge, MA, 1997. MIT Press.

[192] B. Mesot and D. Barber. Switching Linear Dynamical Systems for Noise Robust Speech Recognition. IEEE

Transactions of Audio, Speech and Language Processing, 15(6):1850–1858, 2007.

[193] N. Meuleau, M. Hauskrecht, K-E. Kim, L. Peshkin, Kaelbling. L. P., T. Dean, and C. Boutilier. Solving Very
Large Weakly Coupled Markov Decision Processes. In Proceedings of the Fifteenth National Conference on
Artiﬁcial Intelligence, pages 165–172, 1998.

[194] T. Mills. The Econometric Modelling of Financial Time Series. Cambridge University Press, 2000.

[195] T. Minka. Expectation Propagation for approximate Bayesian inference. In J. Breese and D. Koller, editors,
Uncertainty in Artiﬁcial Intelligence, number 17, pages 362–369, San Francisco, CA, 2001. Morgan Kaufmann.

[196] T. Minka. A comparison of numerical optimizers for logistic regression. Technical report, Microsoft Research,

2003. research.microsoft.com/∼minka/papers/logreg.

[197] T. Minka. Divergence measures and message passing. Technical Report MSR-TR-2005-173, Microsoft Research

Ltd., Cambridge, UK, December 2005.

[198] A. Mira, J. Møller, and G. O. Roberts. Perfect slice samplers. Journal of the Royal Statistical Society,

63(3):593–606, 2001. Series B (Statistical Methodology).

[199] C. Mitchell, M. Harper, and L. Jamieson. On the complexity of explicit duration HMM’s. Speech and Audio

Processing, IEEE Transactions on, 3(3):213–217, May 1995.

[200] T. Mitchell. Machine Learning. McGraw-Hill, 1997.

[201] J. Mooij and H. J. Kappen. Suﬃcient conditions for convergence of Loopy Belief Propagation. IEEE Infor-

mation Theory, 53:4422–4437, 2007.

[202] A. Moore.

A tutorial

http://www.cs.cmu.edu/∼awm/papers.html.

on

kd-trees.

Technical

report,

1991.

Available

from

DRAFT March 9, 2010

573

BIBLIOGRAPHY

BIBLIOGRAPHY

[203] J. Moussouris. Gibbs and Markov Random Systems with Constraints. Journal of Statistical Physics, 10:11–33,

1974.

[204] R. M. Neal. Connectionist Learning of Belief Networks. Artiﬁcial Intelligence, 56:71–113, 1992.

[205] R. M. Neal. Probabilistic inference using Markov Chain Monte Carlo methods. CRG-TR-93-1, Dept. of

Computer Science, University of Toronto, 1993.

[206] R. M. Neal. Markov Chain Sampling Methods for Dirichlet Process Mixture Models. Journal of Computational

and Graphical Statistics, 9(2):249–265, 2000.

[207] R. M. Neal. Slice sampling. Annals of Statistics, 31:705–767, 2003.

[208] R. E. Neapolitan. Learning Bayesian Networks. Prentice Hall, 2003.

[209] A. V. Neﬁan, Luhong L., Xiaobo P., Liu X., C. Mao, and K. Murphy. A coupled HMM for audio-visual speech
recognition. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 2, pages
2013–2016, 2002.

[210] D. Nilsson. An eﬃcient algorithm for ﬁnding the m most probable conﬁgurations in a probabilistic expert

system. Statistics and Computing, 8:159–173, 1998.

[211] D. Nilsson and J. Goldberger. Sequentially ﬁnnding the N-best list in Hidden Markov Models. Internation

Joint Conference on Artiﬁcial Intelligence (IJCAI), 17, 2001.

[212] A. B. Novikoﬀ. On convergence proofs on perceptrons. In Symposium on the Mathematical Theory of Automata
(New York, 1962), volume 12, pages 615–622, Brooklyn, N.Y., 1963. Polytechnic Press of Polytechnic Institute
of Brooklyn.

[213] F. J. Och and H. Ney. Discriminative training and maximum entropy models for statistical machine transla-
tion. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 295–302,
Philadelphia, July 2002.

[214] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1?

Vision Research, 37:3311–3325, 1998.

[215] A. V. Oppenheim, R. W. Shafer, M. T. Yoder, and W. T. Padgett. Discrete-Time Signal Processing. Prentice

Hall, third edition, 2009.

[216] M. Ostendorf, V. Digalakis, and O. A. Kimball. From HMMs to Segment Models: A Uniﬁed View of Stochastic

Modeling for Speech Recognition. IEEE Transactions on Speech and Audio Processing, 4:360–378, 1995.

[217] P. Paatero and U. Tapper. Positive matrix factorization: A non-negative factor model with optimal utilization

of error estimates of data values. Environmetrics, 5:111–126, 1994.

[218] V. Pavlovic, J. M. Rehg, and J. MacCormick. Learning switching linear models of human motion. In T. K.
Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems (NIPS),
number 13, pages 981–987, Cambridge, MA, 2001. MIT Press.

[219] J. Pearl. Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference. Morgan Kaufmann,

1988.

[220] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2000.

[221] B. A. Pearlmutter and L. C. Parra. Maximum Likelihood Blind Source Separation: A Context-Sensitive
Generalization of ICA. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information
Processing Systems (NIPS), number 9, pages 613–619, Cambridge, MA, 1997. MIT Press.

[222] K. B. Petersen and O. Winther. The EM algorithm in independent component analysis. In IEEE International

Conference on Acoustics, Speech, and Signal Processing, volume 5, pages 169–172, 2005.

[223] J-P. Pﬁster, T. Toyiozumi, D. Barber, and W. Gerstner. Optimal Spike-Timing Dependent Plasticity for

Precise Action Potential Firing in Supervised learning. Neural Computation, 18:1309–1339, 2006.

[224] J. Platt. Fast Training of Support Vector Machines Using Sequential Minimal Optimization. In B. Sch¨olkopf,
C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages
185–208. MIT Press, 1999.

574

DRAFT March 9, 2010

BIBLIOGRAPHY

BIBLIOGRAPHY

[225] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth, and M. Welling. Fast collapsed Gibbs sampling for
Latent Dirichlet Allocation. In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 569–577, New York, NY, USA, 2008. ACM.

[226] J. E. Potter and R. G. Stern. Statistical ﬁltering of space navigation measurements. In American Institute
of Aeronautics and Astronautics Guidance and Control Conference, volume 13, pages 775–801, Cambridge,
Mass., August 1963.

[227] W. Press, W. Vettering, S. Teukolsky, and B. Flannery. Numerical Recipes in Fortran. Cambridge University

Press, 1992.

[228] S. J. D. Prince and J. H. Elder. Probabilistic Linear Discriminant Analysis for Inferences About Identity. In

IEEE 11th International Conference on Computer Vision ICCV, pages 1–8, 2007.

[229] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. of

the IEEE, 77(2):257–286, 1989.

[230] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.

[231] H. E. Rauch, G. Tung, and C. T. Striebel. Maximum Likelihood estimates of linear dynamic systems. American

Institute of Aeronautics and Astronautics Journal (AIAAJ), 3(8):1445–1450, 1965.

[232] T. Richardson and P. Spirtes. Ancestral Graph Markov Models. Annals of Statistics, 30(4):962–1030, 2002.

[233] D. Rose, R. E. Tarjan, and E. S. Lueker. Algorithmic aspects of vertex elimination of graphs. SIAM Journal

on Computing, (5):266–283, 1976.

[234] F. Rosenblatt. The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.

Psychological Review, 65(6):386–408, 1958.

[235] D. B. Rubin. Using the SIR algorithm to simulate posterior distributions. In M. H. Bernardo, K. M. Degroot,

D. V. Lindley, and A. F. M. Smith, editors, Bayesian Statistics 3. Oxford University Press, 1988.

[236] D. Saad and M. Opper. Advanced Mean Field Methods Theory and Practice. MIT Press, 2001.

[237] R. Salakhutdinov, S. Roweis, and Z. Ghahramani. Optimization with EM and Expectation-Conjugate-
Gradient. In T. Fawcett and N. Mishra, editors, International Conference on Machine Learning, number 20,
pages 672–679, Menlo Park, CA, 2003. AAAI Press.

[238] L. K. Saul, T. S. Jaakkola, and M. J. Jordan. Mean ﬁeld theory for sigmoid belief networks. Journal of

Artiﬁcial Intelligence Research, 4:61–76, 1996.

[239] L. K. Saul and M. I. Jordan. Exploiting tractable substructures in intractable networks. In D. S. Touretzky,
M. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems (NIPS), number 8,
pages 486–492, Cambridge, MA, 1996. MIT Press.

[240] L. Savage. The Foundations of Statistics. Wiley, 1954.

[241] R. D. Schachter. Bayes-ball: The rational pastime (for determining irrelevance and requisite information
In G. Cooper and S. Moral, editors, Uncertainty in Artiﬁcial

in belief networks and inﬂuence diagrams).
Intelligence, number 14, pages 480–487, San Francisco, CA, 1998. Morgan Kaufmann.

[242] B. Sch¨olkopf, A. Smola, and K. R. M¨uller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem.

Neural Computation, 10:1299–1319, 1998.

[243] N. N. Schraudolph and D. Kamenetsky. Eﬃcient Exact Inference in Planar Ising Models. In D. Koller, D. Schu-
urmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems (NIPS),
number 21, pages 1417–1424, Cambridge, MA, 2009. MIT Press.

[244] E. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461–464, 1978.

[245] M. Seeger. Gaussian Processes for Machine Learning. International Journal of Neural Systems, 14(2):69–106,

2004.

[246] M. Seeger. Expectation propagation for exponential families. Technical report, Department of EECS, Berkeley,

2005. www.kyb.tuebingen.mpg.de/bs/people/seeger.

DRAFT March 9, 2010

575

BIBLIOGRAPHY

BIBLIOGRAPHY

[247] M. Seeger and H. Nickisch. Large scale variational inference and experimental design for sparse generalized

linear models. Technical Report 175, Max Planck Institute for Biological Cybernetics, September 2008.

[248] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.

[249] S. Siddiqi, B. Boots, and G. Gordon. A Constraint Generation Approach to Learning Stable Linear Dynamical
Systems. In J. C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing
Systems (NIPS), number 20, pages 1329–1336, Cambridge, MA, 2008. MIT Press.

[250] T. Silander, P. Kontkanen, and P. Myllym¨aki. On Sensitivity of the MAP Bayesian Network Structure to
the Equivalent Sample Size Parameter. In R. Parr and L. van der Gaag, editors, Uncertainty in Artiﬁcial
Intelligence, number 23, pages 360–367, Corvallis, Oregon, USA, 2007. AUAI press.

[251] S. S. Skiena. The algorithm design manual. Springer-Verlag, New York, USA, 1998.

[252] E. Smith and M. S. Lewicki. Eﬃcient auditory coding. Nature, 439(7079):978–982, 2006.

[253] P. Smolensky. Parallel Distributed Processing: Volume 1: Foundations, chapter Information processing in

dynamical systems: Foundations of harmony theory, pages 194–281. MIT Press, Cambridge, MA, 1986.

[254] G. Sneddon. Studies in the atmospheric sciences, chapter A Statistical Perspective on Data Assimilation in

Numerical Models. Number 144 in Lecture Notes in Statistics. Springer-Verlag, 2000.

[255] P. Sollich. Bayesian Methods for Support Vector Machines: Evidence and Predictive Class Probabilities.

Machine Learning, 46(1-3):21–52, 2002.

[256] D. X. Song, D. Wagner, and X. Tian. Timing Analysis of Keystrokes and Timing Attacks on SSH.

Proceedings of the 10th conference on USENIX Security Symposium. USENIX Association, 2001.

In

[257] A. S. Spanias. Speech coding: a tutorial review. Proceedings of the IEEE, 82(10):1541–1582, Oct 1994.

[258] D. J. Spiegelhalter, A. P. Dawid, S. L. Lauritzen, and R. G. Cowell. Bayesian analysis in expert systems.

Statistical Science, 8(3):219–247, 1993.

[259] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2 edition, 2000.

[260] N. Srebro. Maximum Likelihood Bounded Tree-Width Markov Networks. In J. Breese and D. Koller, editors,
Uncertainty in Artiﬁcial Intelligence, number 17, pages 504–511, San Francisco, CA, 2001. Morgan Kaufmann.

[261] H. Steck. Constraint-Based Structural Learning in Bayesian Networks using Finite Data Sets. PhD thesis,

Technical University Munich, 2001.

[262] H. Steck. Learning the Bayesian Network Structure: Dirichlet Prior vs Data.

In D. A. McAllester and
P. Myllymaki, editors, Uncertainty in Artiﬁcial Intelligence, number 24, pages 511–518, Corvallis, Oregon,
USA, 2008. AUAI press.

[263] H. Steck and T. Jaakkola. On the Dirichlet prior and Bayesian regularization. In S. Becker, S. Thrun, and

K. Obermayer, editors, NIPS, pages 697–704. MIT Press, 2002.

[264] M. Studen´y. On mathematical description of probabilistic conditional independence structures. PhD thesis,

Academy of Sciences of the Czech Republic, 2001.

[265] M. Studen´y. On non-graphical description of models of conditional independence structure. In HSSS Workshop

on Stochastic Systems for Individual Behaviours. Louvain la Neueve, Belgium, 22-23 January 2001.

[266] C. Sutton and A. McCallum. An introduction to conditional random ﬁelds for relational learning. In L. Getoor

and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT press, 2006.

[267] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.

[268] R. J. Swendsen and J-S. Wang. Nonuniversal critical dynamics in Monte Carlo simulations. Physical Review

Letters, 58:86–88, 1987.

[269] B. K. Sy. A Recurrence Local Computation Approach Towards Ordering Composite Beliefs in Bayesian Belief

Networks. International Journal of Approximate Reasoning, 8:17–50, 1993.

[270] T. Sejnowski. The Book of Hebb. Neuron, 24:773–776, 1999.

576

DRAFT March 9, 2010

BIBLIOGRAPHY

BIBLIOGRAPHY

[271] R. E. Tarjan and M. Yannakakis. Simple linear-time algorithms to test chordality of graphs, test acyclicity of
hypergraphs, and selectively reduce acyclic hypergraphs. SIAM Journal on Computing, 13(3):566–579, 1984.

[272] S. J. Taylor. Modelling Financial Time Series. World Scientiﬁc, second edition, 2008.

[273] Y. W. Teh, D. Newman, and M. Welling. A Collapsed Variational Bayesian Inference Algorithm for La-
tent Dirichlet Allocation. In J. C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural
Information Processing Systems (NIPS), number 20, pages 1481–1488, Cambridge, MA, 2008. MIT Press.

[274] Y. W. Teh and M. Welling. The uniﬁed propagation and scaling algorithm. In T. G. Dietterich, S. Becker,
and Z. Ghahramani, editors, Advances in Neural Information Processing Systems (NIPS), number 14, pages
953–960, Cambridge, MA, 2002. MIT Press.

[275] M. Tipping and C. M. Bishop. Mixtures of probabilistic principal component analysers. Neural Computation,

11(2):443–482, 1999.

[276] M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning

Research, 1:211–244, 2001.

[277] D. M. Titterington, A. F. M. Smith, and U. E. Makov. Statistical analysis of ﬁnite mixture distributions.

Wiley, 1985.

[278] E. Todorov. Eﬃcient computation of optimal actions. Proceedings of the National Academy of Sciences of the

United States of America (PNAS), 106(28):11478–11483, 2009.

[279] M. Toussaint, S. Harmeling, and A. Storkey. Probabilistic inference for solving (PO)MDPs. Research Report

EDI-INF-RR-0934, University of Edinburgh, School of Informatics, 2006.

[280] M. Tsodyks, K. Pawelzik, and H. Markram. Neural Networks with Dynamic Synapses. Neural Computation,

10:821–835, 1998.

[281] P. Van Overschee and B. De Moor. Subspace Identiﬁcation for Linear Systems; Theory, Implementations,

Applications. Kluwer, 1996.

[282] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.

[283] M. Verhaegen and P. Van Dooren. Numerical Aspects of Diﬀerent Kalman Filter Implementations. IEEE

Transactions of Automatic Control, 31(10):907–917, 1986.

[284] T. Verma and J. Pearl. Causal networks : Semantics and expressiveness. In R. D. Schacter, T. S. Levitt, L. N.
Kanal, and J.F. Lemmer, editors, Uncertainty in Artiﬁcial Intelligence, volume 4, pages 69–76, Amsterdam,
1990. North-Holland.

[285] T. O. Virtanen, A. T. Cemgil, and S. J. Godsill. Bayesian extensions to nonnegative matrix factorisation for
audio signal modelling. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages
1825–1828, 2008.

[286] G. Wahba. Support Vector Machines, Repreducing Kernel Hilbert Spaces, and Randomized GACV, pages

69–88. MIT Press, 1999.

[287] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foun-

dations and Trends in Machine Learning, 1(1-2):1–305, 2008.

[288] H. Wallach. Eﬃcient training of conditional random ﬁelds. Master’s thesis, Division of Informatics, University

of Edinburgh, 2002.

[289] Y. Wang, J. Hodges, and B. Tang. Classiﬁcation of Web Documents Using a Naive Bayes Method. 15th IEEE

International Conference on Tools with Artiﬁcial Intelligence, pages 560–564, 2003.

[290] S. Waterhouse, D. Mackay, and T. Robinson. Bayesian methods for mixtures of experts. In D. S. Touretzky,
M. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems (NIPS), number 8,
pages 351–357, Cambridge, MA, 1996. MIT Press.

[291] Y. Weiss and W. T. Freeman. Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary

Topology. Neural Computation, 13(10):2173–2200, 2001.

DRAFT March 9, 2010

577

BIBLIOGRAPHY

BIBLIOGRAPHY

[292] M. Welling, T. P. Minka, and Y. W. Teh. Structured Region Graphs: Morphing EP into GBP. In F. Bacchus
and T. Jaakkola, editors, Uncertainty in Artiﬁcial Intelligence, number 21, pages 609–614, Corvallis, Oregon,
USA, 2005. AUAI press.

[293] J. Whittaker. Graphical Models in Applied Multivariate Statistics. John Wiley & Sons, 1990.

[294] W. Wiegerinck. Variational approximations between mean ﬁeld theory and the Junction Tree algorithm. In
C. Boutilier and M. Goldszmidt, editors, Uncertainty in Artiﬁcial Intelligence, number 16, pages 626–633, San
Francisco, CA, 2000. Morgan Kaufmann.

[295] W. Wiegerinck and T. Heskes. Fractional Belief Propagation. In S. Becker, S. Thrun, and K. Obermayer,
editors, Advances in Neural Information Processing Systems (NIPS), number 15, pages 438–445, Cambridge,
MA, 2003. MIT Press.

[296] C. K. I. Williams. Computing with inﬁnite networks. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors,
Advances in Neural Information Processing Systems NIPS 9, pages 295–301, Cambridge, MA, 1997. MIT
Press.

[297] C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Trans Pattern Analysis

and Machine Intelligence, 20:1342–1351, 1998.

[298] C. Yanover and Y. Weiss. Finding the M Most Probable Conﬁgurations Using Loopy Belief Propagation. In
S. Thrun, L. Saul, and B. Sch¨olkopf, editors, Advances in Neural Information Processing Systems (NIPS),
number 16, pages 1457–1464, Cambridge, MA, 2004. MIT Press.

[299] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief

propagation algorithms. Information Theory, IEEE Transactions on, 51(7):2282–2312, July 2005.

[300] S. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and P. Woodland. The HTK Book Version 3.0.

Cambridge University Press, 2000.

[301] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915–936, 2003.

[302] J.-H. Zhao, P. L. H. Yu, and Q. Jiang. ML estimation for factor analysis: EM or non-EM? Statistics and

Computing, 18(2):109–123, 2008.

[303] O. Zoeter. Monitoring non-linear and switching dynamical systems. PhD thesis, Radboud University Nijmegen,

2005.

578

DRAFT March 9, 2010

Index

1 − of − M coding, 207
N-max-product, 74
α-expansion, 536
α-recursion, 417
β-recursion, 418
γ-recursion, 419

absorbing state, 75
absorption, 87

inﬂuence diagram, 117

acceptance function, 500
active learning, 253
adjacency matrix, 21, 385
algebraic Riccati equation, 448
ancestor, 19
ancestral ordering, 494
ancestral sampling, 494
antifreeze, 231
approximate inference, 103, 379, 515

belief propagation, 529, 530
Bethe free energy, 528
double integration bound, 539
expectation propagation, 530
graph cut, 535
Laplace approximation, 515
switching linear dynamical system, 458
variational approach, 516
variational inference, 519

AR model, see auto-regressive model
Artiﬁcial Life, 482
asychronous updating, 521
asymmetry, 113
auto-regressive model, 438

switching, 452
time-varying, 440

automatic relevance determination, 351
auxiliary variable sampling, 501
average, 139

backtracking, 71
bag of words, 208, 287
batch update, 322
Baum-Welch, 423

Bayes Information Criterion, 247
Bayes’

factor, 184, 241
model selection, 241
theorem, 8

Bayes’ rule, see Bayes’ theorem
Bayesian

decision theory, 259
hypothesis testing, 241, 263
image denoising, 519
linear model, 333
mixture model, 373
model selection, 241
Occam’s razor, 244
outcome analysis, 263

Bayesian Dirichlet score, 184
Bayesian linear model, 348
BD score, 184

BDeu score, 185

BDeu score, 185
belief network

asbestos-smoking-cancer, 177
cascade, 32
chest clinic, 45
divorcing parents, 43
dynamic, 430
noisy AND gate, 44
noisy logic gate, 44
noisy OR gate, 44
sigmoid, 239
structure learning, 180
training

Bayesian, 174

belief propagation, 66, 529

loopy, 526

belief revision, 73, see max-product
Bellman’s equation, 121
Bessel function, 354
beta

distribution, 145
function, 145, 168
Bethe free energy, 528

579

INDEX

bias, 142

unbiased estimator, 142

bigram, 422
binary entropy, 521
bioinformatics, 431
black and white sampling, 512
black-box, 261
Blahut-Arimoto algorithm, 526
Boltzmann machine, 52, 60, 189, 539

restricted, 60

bond propagation, 81
Bonferroni inequality, 17
Boolean network, 482
Bradly-Terry-Luce model, 405
bucket elimination, 78
burn in, 498

calculus, 551
canonical correlation analysis, 300

constrained factor analysis, 398

canonical variates, 305
causal consistency, 113
causality, 39, 113

do calculus, 42
inﬂuence diagrams, 42
post intervention distribution, 42

CCA

see canonical correlation analysis, 300

centering, 152
chain graph, 55

chain component, 55

chain rule, 553
chain structure, 71
changepoint model, 468
checkerboard, 512
chest clinic, 45

missing data, 237
with decisions, 131

children, see directed acyclic graph
Cholesky, 306
chord, 95
chordal, 95
Chow-Liu tree, 210
classiﬁcation, 252, 324, 358

Bayesian, 340
boundary, 205
error analysis, 263
linear parameter model, 319
multiple classes, 324
performance, 263

random guessing, 270

softmax, 324

clique, 20

decomposition, 383
graph, 86

580

INDEX

matrix, 22, 383

cliquo, 22
Cluster Variation method, 529
clustering, 253
collaborative ﬁltering, 291
collider, see directed acyclic graph
commute, 546
compatibility function, 511
competition model

Bradly-Terry-Luce, 405
Elo, 406
TrueSkill, 406

competition models, 405
concave function, 554
condindep.m, 59
conditional entropy, 524
conditional likelihood, 200
conditional mutual information, 212
conditional probability, 4
conditional random ﬁeld, 193, 428
conditioning, 151

loop cut set, 80

conjugate distribution, 168
exponential family, 156
Gaussian, 154, 155
prior, 156

conjugate gradient, 324
conjugate gradients algorithm, 560
conjugate vector, 558
conjugate vectors algorithm, 559
connected components, 20
connected graph, 20
consistent, 91
consistent estimator, 197
convex function, 554
correction smoother, 419, 446
correlation

matrix, 140

cosine similarity, 289
coupled HMM, 430
covariance, 140
matrix, 140

covariance function, 318, 349, 351

γ-exponential, 354
construction, 352
Gibbs, 355
isotropic, 354
Mat´ern, 354
Mercer kernel, 356
neural network, 355
non-stationary, 355
Ornstein-Uhlenbeck, 354, 356
periodic, 354
rational quadratic, 354
smoothness, 356

DRAFT March 9, 2010

INDEX

INDEX

squared exponential, 354, 357
stationary, 353

CPT, see conditional probability table
CRF, see conditional random ﬁeld
critical point, 555
cross-validation, 256
cumulant, 492
curse of dimensionality, 124, 316
cut set conditioning, 79

D-map, see dependence map
DAG, see directed acyclic graph
data

anomaly detection, 253
catagorical, 263
dyadic, 382
handwritten digits, 323
labelled, 251
monadic, 383
numerical, 263
ordinal, 263
unlabelled, 252

data compression, 376

vector quantisation, 376

decision boundary, 319
decision function, 254
decision theory, 107, 168, 259
decision tree, 108
decomposable, 95
degree, 22
degree of belief, 8
delta function, see Dirac delta function

Kronecker, 142

density estimation, 365

Parzen estimator, 375

dependence
map, 57

descendant, 19
design matrix, 339, 348
detailed balance, 499
determinant, 547
deterministic latent variable model, 482
diﬀerentiation, 552
digamma function, 160
digit data, 283
Dijkstra’s algorithm, 76
dimension reduction
linear, 279, 282
supervised, 303

dimensionality reduction

linear, 285
non-linear, 285

Dirac delta function, 141, 142, 357
directed acyclic graph, 32

ancestor, 19

DRAFT March 9, 2010

ancestral order, 21
cascade, 32
children, 19
collider, 34
descendant, 19
family, 19
immorality, 38
Markov Blanket, 19
moralisation, 54
parents, 19

direction bias, 428
directional derivative, 553
Dirichlet

distribution, 147

Dirichlet process mixture models, 379
discount factor, 122
discriminative

approach, 260
training, 260

discriminative approach, 259
discriminative training, 425
dissimilarity function, 273
distributed computation, 475
distribution

Bernoulli, 143, 368
beta, 145, 148, 160, 176, 243
binomial, 143
Categorical, 143
change of variables, 159
conjugate, 154
continuous, 4
density, 4
Dirichlet, 147, 161, 178, 183, 379, 380
discrete, 3
divergence, 157
double exponential, 146
empirical, 141, 255

average, 139
expectation, 139

exponential, 144
exponential family, 155

canonical, 156

gamma, 373
mode, 161

Gauss-gamma, 155, 162
Gauss-inverse-gamma, 154
Gaussian

canonical exponential form, 156
conditioning, 151
conjugate, 154, 155
entropy, 150
isotropic, 149
mixture, 163
multivariate, 148
normalisation, 158, 159

581

INDEX

INDEX

partitioned, 150
propagation, 152
system reversal, 151
univariate, 146

inverse gamma, 145
inverse Wishart, 155
joint, 4
kurtosis, 141
Laplace, 146
marginal, 4
mode, 139
multinomial, 144
normal, 146
Poisson, 144, 163
Polya, 378
scaled mixture, 147
skewness, 141
Student’s t, 146
uniform, 144, 157
Wishart, 373

domain, 3
double integration bound, 539
dual parameters, 317
dual representation, 316
dyadic data, 382
dynamic Bayesian network, 430
dynamic synapses, 486
dynamical system

linear, 437
non-linear, 482

dynamics reversal, 446

edge list, 21
eﬃcient IPF, 191
eigen

decomposition, 149, 282, 550
equation, 281
function, 551
problem, 300
spectrum, 356
value, 281, 548

Elo model, 405
emission distribution, 416
emission matrix, 442
empirical

independence, 182

empirical distribution, 141, 170, 255
empirical risk, 255
penalised, 256

empirical risk minimisation, 256
energy, 221
entropy, 99, 157, 163, 221, 524

diﬀerential, 157, 164
Gaussian, 150

EP, see expectation propagation

582

error function, 319
estimator

consistent, 197

evidence, see marginal likelihood

hard, 29
likelihood, 30
soft, 29
uncertain, 29
virtual, 30

Evidence Procedure, 335
exact sampling, 494
expectation, see average
expectation correction, 462, 463
expectation maximisation, 126, 135, 220, 293, 336,

423, 450

algorithm, 220, 222
antifreeze, 231
belief networks, 224
E-step, 221
energy, 221
entropy, 221
failure case, 230
generalised, 126
intractable energy, 229
M-step, 221
Markov decision process, 126
mixture model, 366
partial E-step, 229
partial M-step, 229
variational Bayes, 233
Viterbi training, 229

expectation propagation, 530
exponential family, 155
canonical form, 156
conjugate, 156

extended observability matrix, 451

face model, 395
factor analysis, 391

factor rotation, 390
probabilistic PCA, 397
training

EM, 394
SVD, 392
factor graph, 58
factor loading, 389
family, see directed acyclic graph
feature map, 298
ﬁltering, 417
ﬁnite dimensional Gaussian Process, 349
Fisher information, 161
Fisher’s linear discriminant, 303
Floyd-Warshall-Roy algorithm, 77
forward sampling, 494
Forward-Backward, 418

DRAFT March 9, 2010

INDEX

INDEX

Forward-Sampling-Resampling, 510

gamma

digamma, 160
distribution, 145
function, 146, 160

Gaussian

canonical representation, 149
distribution, 146
moment representation, 149, 444
sub, 141
super, 141

Gaussian mixture model, 370

Bayesian, 373
collapsing the mixture, 461
EM algorithm, 370
inﬁnite problems, 373
k-means, 375
Parzen estimator, 375
symmetry breaking, 373

Gaussian process, 347
classiﬁcation, 358
Laplace approximation, 359
multiple classes, 362
regression, 350
smoothness, 356
weight space view, 348
Gaussian sum ﬁltering, 458
Gaussian sum smoothing, 462
generalisation, 251, 255
generalised pseudo Bayes, 466
generative

approach, 259
model, 259
training, 259

generative approach, 259
Gibbs sampling, 495
Glicko, 406
GMM, see Gaussian mixture model
Google, 413
gradient, 552

descent, 555
natural, 556

Gram matrix, 317
Gram-Schmidt procedure, 558
Gramm matrix, 299
graph, 19

adjacency matrix, 21
chain, 55
chain structured, 71
chordal, 95
clique, 20, 22, 86
clique matrix, 22
cliquo, 22
connected, 20

DRAFT March 9, 2010

cut, 535
decomposable, 95
descendant, 22
directed, 19
disconnected, 99
edge list, 21
factor, 58
loopy, 20
multiply-connected, 20, 93
neighbour, 20
path, 19
separation, 54
set chain, 102
singly-connected, 20
skeleton, 38
spanning tree, 21
tree, 20
triangulated, 95
undirected, 19, 20
vertex

degree, 22

graph cut algorithm, 535
graph partitioning, 381
Gull-MacKay iteration, 338

Hamilton-Jacobi equation, 122
Hamiltonian dynamics, 503
Hammersley Cliﬀord theorem, 53
handwritten digits, 323
Hankel matrix, 451
harmonium, see restricted Boltzmann machine
Heaviside step function, 384
Hebb, 476
Hebb rule, 476
hedge fund, 247
Hermitian, 545
Hessian, 324, 553
hidden Markov model, 99, 230, 416

α recursion, 417
β recursion, 418
coupled, 430
direction bias, 428
discriminative training, 425
duration model, 427
entropy, 99
ﬁltering, 417
input-output, 427
likelihood, 419
most likely state (MAP), 420
pairwise marginal, 419
Rauch Tung Striebel smoother, 419
smoothing

parallel, 418
sequential, 418

viterbi, 74

583

INDEX

Viterbi algorithm, 420

hidden variables, 217
HMM, see hidden Markov model
Hopﬁeld network, 475

augmented, 483
capacity, 479
Hebb rule, 477
heteroassociative, 484
maximum likelihood, 478
perceptron, 479
pseudo inverse rule, 477
sequence learning, 476

hybrid Monte Carlo, 502
hyper Markov, 196
hyper tree, 98
hyperparameter, 156, 176, 335
hyperplane, 545
hypothesis testing, 241

Bayesian error analysis, 263

I-map, see independence map
ICA, 399
identically and independently distributed, 165
identiﬁability, 449
identity matrix, 546
IID, see identically and independently distributed
IM algorithm, see information-maximisation algo-

rithm
immorality, 38
importance

distribution, 506
sampling, 506

particle ﬁlter, 509
resampling, 508
sequential, 508

weight, 507

incidence matrix, 22
independence

Bayesian, 183
conditional, 26, 33
empirical, 182
map, 57
Markov equivalent, 38
mutual information, 182
naive Bayes, 203
parameter, 174
perfect map, 57

independent components analysis, 364, 400, 402
indicator function, 11
indicator model, 378
induced representation, 94
inference

bond propagation, 81
bucket elimination, 78
causal, 39

584

INDEX

cut set conditioning, 79
HMM, 417
linear dynamical system, 443
MAP, 81
marginal, 63
Markov decision process, 124, 126
max-product, 71
message passing, 63
mixed, 77
MPM, 81
sum-product algorithm, 68
transfer matrix, 65
variable elimination, 63

inﬂuence diagram, 111

absorption, 117
asymmetry, 113
causal consistency, 113
chest clinic, 131
decision potential, 116
fundamental link, 112
information link, 111
junction tree, 116
no forgetting principle, 112
partial order, 112
probability potential, 116
solving, 115
utility, 111
utility potential, 116

information link, 111
information maximisation, 526
information retrieval, 288, 413
information-maximisation algorithm, 525
innovation noise, 438
input-output HMM, 427
inverse modus ponens, 12
IPF, 188, see iterative proportional ﬁtting

eﬃcient, 191

Ising model, 54, see Markov network, 81

approximate inference, 519

isotropic, 149, 372
isotropic covariance functions, 353
item response theory, 404
Iterated Conditional Modes, 534
iterative proportional ﬁtting, 188
iterative scaling, 192

Jeﬀrey’s rule, 29
Jensen’s inequality, 540, 554
Joseph’s symmetrized update, 446
jump Markov model, see switching linear dynamical

system

junction tree, 88, 92

absorption, 87
algorithm, 85, 97
clique graph, 86

DRAFT March 9, 2010

INDEX

INDEX

computational complexity, 98
conditional marginal, 100
consistent, 91
hyper tree, 98
inﬂuence diagram, 116
marginal likelihood, 99
most likely state, 101
normalisation constant, 99
potential, 86
running intersection property, 89
separator, 86
strong, 117
strong triangulation, 117
tree width, 98
triangulation, 94

k-means, 375
Kalman ﬁlter, 442
Kalman gain, 445
KD-tree, 274
kernel, 316, 317, see covariance function

classiﬁer, 324

kidnapped robot, 421
Kikuchi, 529
KL divergence, see Kullback-Leibler divergence
KNN, see nearest neighbour
Kronecker delta, 142, 546
Kullback-Leibler divergence, 157, 220, 517
kurtosis, 141

labelled data, 251
Lagrange

multiplier, 562

Lagrangian, 563
Laplace approximation, 341, 359, 515
latent ability model, 403
latent Dirichlet allocation, 380
latent linear model, 389
latent semantic analysis, 287
latent topic, 287
latent variable, 217

deterministic, 482
model, 217

lattice model, 54
LDA regularised, 309
LDS, see linear dynamical system
leaky integrate and ﬁre model, 486
Leapfrog discretisation, 503
learning

active, 253
anomaly detection, 253
Bayesian, 166
belief network, 171
belief networks

EM, 224

DRAFT March 9, 2010

Dirichlet prior, 178
inference, 165
nearest neighbour, 273
online, 253
query, 253
reinforcement, 254
semi-supervised, 254, 377
sequences, 423, 449, 476
sequential, 253
structure, 180
supervised, 251
unsupervised, 252, 389

learning rate, 322
likelihood, 10, 99, 419, 447, 461

bound, 220
marginal, 10, 69
model, 10, 180

approximate, 246

pseudo, 196

likelihood decomposable, 180
line search, 557
linear algebra, 543
linear dimension reduction, 279, 282

canonical correlation analysis, 300
latent semantic analysis, 287
non-negative matrix factorisation, 295
probabilistic latent semantic analysis, 292
supervised, 303
unsupervised, 279

Linear Discriminant Analysis, 303
linear discriminant analysis, 303

as regression, 308
penalised, 308
regularised, 308

linear dynamical system, 151, 442

cross moment, 447
dynamics reversal, 446
ﬁltering, 444
identiﬁability, 449
inference, 443
learning, 449
likelihood, 447
most likely state, 448
numerical stability, 443
Riccati equations, 448
smoothing, 446
subspace method, 451
switching, 457
symmetrising updates, 446

linear Gaussian state space model, 442
linear model, 311
Bayesian, 333
classiﬁcation, 319
factor analysis, 391
latent, 389

585

INDEX

INDEX

regression, 312

linear parameter model, 312

Bayesian, 334

linear perceptron, 321
linear separability, 320
linear transformation, 546
linearly independent, 544
linearly separable, 320
Linsker’s as-if-Gaussian approximation, 526
localisation, 420
logic

Aristotle, 12

logistic regression, 319
logistic sigmoid, 319
logit, 319
loop cut set, 80
loopy, 20
loss function, 254, 255

zero-one, 254

loss matrix, 255
Luenberger expanding subspace theorem, 559

Mahalanobis distance, 273, 302
manifold

linear, 279
low dimensional, 279

MAP, see most probable a posteriori
MAR, see missing at random
margin, 325, 326

soft, 327
marginal, 4

generalised, 117

marginal likelihood, 10, 69, 99, 219, 335, 351

approximate, 337, 340, 342, 361

marginalisation, 4
Markov

chain, 238, 411
ﬁrst order, 438
stationary distribution, 412

equivalent, 38
global, 52
hyper, 196
local, 51
model, 411
pairwise, 51
random ﬁeld, 192

approximation, 519

Markov blanket, see directed acyclic graph, 496
Markov chain, 63, 129, 499

absorbing state, 75
detailed balance, 499
PageRank, 412

Markov chain Monte Carlo, 499

auxiliary variable, 501

hybrid Monte Carlo, 502

586

slice sampling, 505
Swendson-Wang, 504

Gibbs sampling, 496
Metropolis-Hastings, 499
proposal distribution, 500
structured Gibbs sampling, 497

Markov decision process, 120, 133

Bellman’s equation, 121
discount factor, 122
non-stationary policy, 127
partially observable, 129
planning, 124
policy iteration, 123
reinforcement learning, 130
stationary, 125
stationary deterministic policy, 128
temporally unbounded, 122
value iteration, 122
variational inference, 126

Markov equivalence, 38
Markov network, 50

Boltzmann machine, 52
continuous-state temporal, 437
discrete-state temporal, 411
Gibbs distribution, 50
Gibbs network, 52
Hammersley Cliﬀord theorem, 53
pairwise, 50
potential, 50

Markov random ﬁeld, 53, 540, 542

alpha-expansion, 536
attractive binary, 534
graph cut, 535
map, 533
Potts model, 536

matrix, 545

adjacency, 22, 385
Cholesky, 306
clique, 22
Gramm, 299
Hankel, 451
incidence, 22
inversion, 548
inversion lemma, 551
orthogonal, 547
positive deﬁnite, 550
pseudo inverse, 548
rank, 547

matrix factorisation, 295
max-product, 71

N most probable states, 74

max-sum, 75
maximum cardinality checking, 96
maximum likelihood, 152, 169, 170, 321

belief network, 171

DRAFT March 9, 2010

INDEX

INDEX

Chow-Liu tree, 212
counting, 171
empirical distribution, 170
factor analysis, 391
Gaussian, 153
gradient optimisation, 236
Markov network, 185
ML-II, 236
naive Bayes, 204
properties, 196

Maximum Likelihood

Hopﬁeld network, 478

MCMC, see Markov chain Monte Carlo
MDP, see Markov decision process
mean ﬁeld theory, 522

asynchronous updating, 522

Mercer kernel, 356
message

passing, 88
schedule, 68, 88
message passing, 63
Metropolis-Hastings acceptance function, 500
Metropolis-Hastings sampling, 499
minimum clique cover, 384
missing at random, 218

completely, 219

missing data, 217
mixed inference, 77
mixed membership model, 380
mixing matrix, 400
mixture

Gaussian, 163
mixture model, 365

Bernoulli product, 368
Dirichlet process mixture, 379
expectation maximisation, 366
factor analysis, 394
Gaussian, 370
indicator approach, 378
Markov chain, 414
PCA, 394

mixture of experts, 377
MN, see Markov network
mode, 139
model

auto-regressive, 438
changepoint, 468
deterministic latent variable, 482
faces, 395
leaky integrate and ﬁre, 486
linear, 311
mixed membership, 380
mixture, 365
Rasch, 403

model selection, 241

DRAFT March 9, 2010

approximate, 246

moment generating function, 159
moment representation, 444
momentum, 556
monadic data, 383
money

ﬁnancial prediction, 247
loadsa, 247

moralisation, 54, 92
most probable a posteriori, 10
most probable path

multiple-source multiple-sink, 76

most probable state

N most probable, 73

MRF, see Markov random ﬁeld
multiply-connected, 20
multiply-connected-distributions, 93
multpots.m, 16
mutual information, 182, 211, 524

approximation, 524
conditional, 182
maximisation, 525

naive Bayes, 203, 368

Bayesian, 208
tree augmented, 210

Naive Mean Field, 522
naive mean ﬁeld theory, 520
natural gradient, 556
nearest neighbour, 273
probabilistic, 275

network ﬂow, 540
network modelling, 297
neural computation, 475
neural network, 332, 484

depression, 486
dynamic synapses, 486
leaky integrate and ﬁre, 486

Newton update, 324
Newton’s method, 561
no forgetting principle, 112
node

extremal, 73
simplical, 73

non-negative matrix factorisation, 295
normal distribution, 146
normal equations, 313
normalised importance weights, 507

observed linear dynamical system, 437
Occam’s razor, 244
One of m encoding, 263
online learning, 253
optimisation, 174, 555

Broyden-Fletcher-Goldfarb-Shanno, 562

587

INDEX

INDEX

conjugate gradients algorithm, 560
conjugate vectors algorithm, 559
constrained optimisation, 562
critical point, 555
gradient descent, 555
Luenberger expanding subspace theorem, 559
Newton’s method, 561
quasi Newton method, 561

ordinary least squares, 311
Ornstein-Uhlenbeck, 318
orthogonal, 543
orthogonal least squares, 311
orthonormal, 544
outcome analysis, 264
outlier, 329
over-complete representation, 292
over-complete representations, 292
overcounting, 85
overﬁtting, 194, 245, 350

PageRank, 412
pairwise comparison models, 405
pairwise Markov network, 50
parents, see directed acyclic graph
part-of-speech tagging, 431
Partial Least Squares, 399
partial order, 112
partially observable MDP, 129
particle ﬁlter, 509
partition function, 50, 539
partitioned matrix
inversion, 159

Parzen estimator, 275, 375
path, 508

blocked, 35

PC algorithm, 181
PCA, see Principal Components Analysis
perceptron, 321

logistic regression, 321

perfect elimination order, 97
perfect map, see independence
perfect sampling, 494
planning, 124
plant monitoring, 253
plate, 166
Poisson distribution, 144, 163
policy, 122

iteration, 123
non-stationary, 127
stationary deterministic, 128

Polya distribution, 378
POMDP, see partially observable MDP
positive deﬁnite
kernel, 318
matrix, 349

588

parameterisation, 386

posterior, 10, 165
Dirichlet, 178

potential, 50
Potts model, 536
precision, 148, 155, 334
prediction

auto-regression, 438
ﬁnancial, 247
non-parametric, 347
parameteric, 347

predictive variance, 334
predictor-corrector, 417
Principal Components Analysis, 279, 285

algorithm, 282
high dimensional data, 285
kernel, 298
latent semantic analysis, 287
missing data, 289
probabilistic, 397

principal directions, 282
printer nightmare, 198
missing data, 237

prior, 10, 165
probabilistic latent semantic analysis, 292

conditional, 295
EM algorithm, 293
probabilistic PCA, 397
probability

conditional, 4

function, 173

density, 4
frequentist, 8
posterior, 10
potential, 116
prior, 10
subjective, 8

probit, 319
probit regression, 319
projection, 544
proposal distribution, 500
Pseudo Inverse, 476
pseudo inverse

Hopﬁeld network, 477

pseudo likelihood, 196

quadratic form, 550
quadratic programming, 326
query learning, 253
questionnaire analysis, 403

radial basis functions, 315
Raleigh quotient, 306
Random Boolean networks, 482
Rasch model, 403

DRAFT March 9, 2010

INDEX

Bayesian, 404

Rauch-Tung-Striebel, 419
reabsorption, 102
region graphs, 529
regresion

linear parameter model, 312

regression, 252, 348

logisitic, 319

regularisation, 245, 256, 314, 324
reinforcement learning, 130, 254
Relevance Vector Machine, 339
relevance vector machine, 339, 344
reparameterisation, 85
representation
dual, 316
over-complete, 292
sparse, 292
under-complete, 292

resampling, 507
reset model, 468
residuals, 313
resolution, 12
responsibility, 372
restricted Boltzmann machine, 60
Riccati equation, 448
risk, 255
robust classiﬁcation, 329
Rose-Tarjan-Lueker elimination, 96
running intersection property, 89, 91

sample

mean, 141
variance, 141

sampling, 379

ancestral, 494
Gibbs, 495
importance, 506
multi-variate, 493
particle ﬁlter, 509
univariate, 492

Sampling Importance Resampling, 507
scalar product, 543
scaled mixture, 147
search engine, 413
self localisation and mapping, 422
semi-supervised learning, 254

lower dimensional representations, 261

separator, 86
sequential importance sampling, 508
sequential minimal optimisation, 329
set chain, 102
shortest path, 75
shortest weighted path, 76
sigmoid

logistic, 319

DRAFT March 9, 2010

sigmoid belief network, 239
sigmoid function

approximate average, 342

simple path, 75
simplical nodes, 96
Simpson’s Paradox, 40
singly-connected, 20
singular, 548
Singular Value Decomposition, 286, 550

thin, 307

skeleton, 38, 181
skewness, 141
slice sampling, 505
smoothing, 418
softmax function, 324, 377
spam ﬁltering, 208
spanning tree, 21
sparse representation, 292
spectrogram, 441
speech recognition, 430
spike response model, 484
squared Euclidean distance, 273
squared exponential, 318
standard deviation, 140
standard normal distribution, 146
stationary, 499

distribution, 66

stationary Markov chain, 412
stationary planner, 125
stop words, 381
strong Junction Tree, 117
strong triangulation, 117
structure learning, 180

Bayesian, 184
network scoring, 184
PC algorithm, 181
undirected, 196

structured Expectation Propagation, 532
subsampling, 498
subspace method, 451
sum-product, 526
sum-product algorithm, 68
supervised learning, 251

-semi, 254
classiﬁcation, 252
regression, 252

support vector machine, 325

chunking, 329
training, 329

support vectors, 327
SVD, see Singular Value Decomposition
SVM, see support vector machine
Swendson-Wang sampling, 504
switching AR model, 452

INDEX

589

INDEX

INDEX

switching Kalman ﬁlter, see switching linear dynam-

ical system

switching linear dynamical system, 457

under-complete representation, 292
undirected graph, 20
undirected model

changepoint model, 468
expectation correction, 462
ﬁltering, 458
Gaussian sum smoothing, 462
generalised Pseudo Bayes, 466
inference

computational complexity, 458

likelihood, 461
smoothing, 464

switching linear dynamical systemcollapsing Gaus-

sians, 461

symmetry breaking, 373
system reversal, 151

tagging, 431
tall matrix, 292
Taylor expansion, 555
term-document matrix, 287
test set, 251
text analysis, 296, 380

latent semantic analysis, 287
latent topic, 287
probabilistic latent semantic analysis, 292

time-invariant LDS, 448
Tower of Hanoi, 124
trace-log formula, 551
train set, 251
training

batch, 322
discriminative, 260
generative, 259
generative-discriminative, 261
HMM, 423
linear dynamical system, 449
online, 323

transfer matrix, 65
transition distribution, 416
transition matrix, 437, 442
tree, 20, 64

Chow-Liu, 210

tree augmented network, 212
tree width, 98
triangulation, 94, 96

check, 97
greedy elimination, 96
maximum cardinality, 96
strong, 117
variable elimination, 96

TrueSkill, 406
two-ﬁlter smoother, 418

uncertainty, 157

590

learning

hidden variable, 237
latent variable, 237
uniform distribution, 144
unit vector, 543
unlabelled data, 253
unsupervised learning, 252, 389
utility, 107, 254
matrix, 255
message, 116
money, 107
potential, 116
zero-one loss, 254

validation, 256
cross, 256

value, 121
value iteration, 122
variable

hidden, 220
missing, 220
visible, 220

variable elimination, 63
variance, 140
variational approximation

factorised, 520
structured, 522, 524

variational Bayes, 231

expectation maximisation, 233

variational inference, 126
varimax, 390
vector algebra, 543
vector quantisation, 376
Viterbi, 74, 229
Viterbi algorithm, 420
Viterbi alignment, 416
Voronoi tessellation, 273

web

modelling, 297

website, 297

analysis, 413

whitening, 152, 163
Woodbury formula, 551

XOR function, 321

zero-one loss, 254, 329

DRAFT March 9, 2010

