Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Information Theory, Inference, and Learning Algorithms

David J.C. MacKay

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Information Theory,

Inference,

and Learning Algorithms

David J.C. MacKay
mackay@mrao.cam.ac.uk

c(cid:13)1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005

c(cid:13)Cambridge University Press 2003

Version 7.2 (fourth printing) March 28, 2005

Please send feedback on this book via

http://www.inference.phy.cam.ac.uk/mackay/itila/

Version 6.0 of this book was published by C.U.P. in September 2003. It will
remain viewable on-screen on the above website, in postscript, djvu, and pdf

formats.

In the second printing (version 6.6) minor typos were corrected, and the book

design was slightly altered to modify the placement of section numbers.

In the third printing (version 7.0) minor typos were corrected, and chapter 8

was renamed ‘Dependent random variables’ (instead of ‘Correlated’).

In the fourth printing (version 7.2) minor typos were corrected.

(C.U.P. replace this page with their own page ii.)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Contents

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Preface
Introduction to Information Theory
. . . . . . . . . . . . .
Probability, Entropy, and Inference . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .

1
2
3 More about Inference

I

Data Compression

. . . . . . . . . . . . . . . . . . . . . .

4
5
6
7

The Source Coding Theorem . . . . . . . . . . . . . . . . .
Symbol Codes
. . . . . . . . . . . . . . . . . . . . . . . . .
Stream Codes . . . . . . . . . . . . . . . . . . . . . . . . . .
Codes for Integers
. . . . . . . . . . . . . . . . . . . . . . .

v
3
22
48

65

67
91
110
132

II

Noisy-Channel Coding

. . . . . . . . . . . . . . . . . . . .

137

Dependent Random Variables . . . . . . . . . . . . . . . . .
8
Communication over a Noisy Channel
. . . . . . . . . . . .
9
10 The Noisy-Channel Coding Theorem . . . . . . . . . . . . .
11 Error-Correcting Codes and Real Channels
. . . . . . . . .

138
146
162
177

III

Further Topics in Information Theory . . . . . . . . . . . . .

191

12 Hash Codes: Codes for E(cid:14)cient Information Retrieval
13 Binary Codes
14 Very Good Linear Codes Exist
15 Further Exercises on Information Theory
16 Message Passing
17 Communication over Constrained Noiseless Channels
18 Crosswords and Codebreaking
19 Why have Sex? Information Acquisition and Evolution

. .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . .
. .

193
206
229
233
241
248
260
269

IV Probabilities and Inference . . . . . . . . . . . . . . . . . .

281

. . . . . . . . . . .
20 An Example Inference Task: Clustering
21 Exact Inference by Complete Enumeration
. . . . . . . . .
22 Maximum Likelihood and Clustering . . . . . . . . . . . . .
23 Useful Probability Distributions
. . . . . . . . . . . . . . .
24 Exact Marginalization . . . . . . . . . . . . . . . . . . . . .
25 Exact Marginalization in Trellises
. . . . . . . . . . . . . .
26 Exact Marginalization in Graphs
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
27 Laplace’s Method

284
293
300
311
319
324
334
341

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Ising Models

. . . . . . . . . . .
28 Model Comparison and Occam’s Razor
. . . . . . . . . . . . . . . . . . . . .
29 Monte Carlo Methods
. . . . . . . . . . . . . . . .
30 E(cid:14)cient Monte Carlo Methods
31
. . . . . . . . . . . . . . . . . . . . . . . . . .
32 Exact Monte Carlo Sampling . . . . . . . . . . . . . . . . .
33 Variational Methods
. . . . . . . . . . . . . . . . . . . . . .
34

Independent Component Analysis and Latent Variable Mod-
elling

343
357
387
400
413
422

437
445
451
457

35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference and Sampling Theory

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .

V

Neural networks . . . . . . . . . . . . . . . . . . . . . . . .

467

Introduction to Neural Networks

38
. . . . . . . . . . . . . . .
39 The Single Neuron as a Classi(cid:12)er . . . . . . . . . . . . . . .
40 Capacity of a Single Neuron . . . . . . . . . . . . . . . . . .
41 Learning as Inference
. . . . . . . . . . . . . . . . . . . . .
42 Hop(cid:12)eld Networks
. . . . . . . . . . . . . . . . . . . . . . .
43 Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . .
44 Supervised Learning in Multilayer Networks . . . . . . . . .
45 Gaussian Processes
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
46 Deconvolution

468
471
483
492
505
522
527
535
549

VI

Sparse Graph Codes

. . . . . . . . . . . . . . . . . . . . .

555

47 Low-Density Parity-Check Codes
. . . . . . . . . . . . . .
48 Convolutional Codes and Turbo Codes . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
49 Repeat{Accumulate Codes
50 Digital Fountain Codes
. . . . . . . . . . . . . . . . . . . .

557
574
582
589

VII Appendices . . . . . . . . . . . . . . . . . . . . . . . . . .

597

. . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Notation
. . . . . . . . . . . . . . . . . . . . . . . . . .
B Some Physics
C Some Mathematics
. . . . . . . . . . . . . . . . . . . . . . .
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

598
601
605
613
620

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Preface

This book is aimed at senior undergraduates and graduate students in Engi-
neering, Science, Mathematics, and Computing. It expects familiarity with
calculus, probability theory, and linear algebra as taught in a (cid:12)rst- or second-
year undergraduate course on mathematics for scientists and engineers.

Conventional courses on information theory cover not only the beauti-
ful theoretical
ideas of Shannon, but also practical solutions to communica-
tion problems. This book goes further, bringing in Bayesian data modelling,
Monte Carlo methods, variational methods, clustering algorithms, and neural
networks.

Why unify information theory and machine learning? Because they are
two sides of the same coin.
In the 1960s, a single (cid:12)eld, cybernetics, was
populated by information theorists, computer scientists, and neuroscientists,
all studying common problems. Information theory and machine learning still
belong together. Brains are the ultimate compression and communication
systems. And the state-of-the-art algorithms for both data compression and
error-correcting codes use the same tools as machine learning.

How to use this book

The essential dependencies between chapters are indicated in the (cid:12)gure on the
next page. An arrow from one chapter to another indicates that the second
chapter requires some of the (cid:12)rst.

Within Parts I, II, IV, and V of this book, chapters on advanced or optional
topics are towards the end. All chapters of Part III are optional on a (cid:12)rst
reading, except perhaps for Chapter 16 (Message Passing).

The same system sometimes applies within a chapter: the (cid:12)nal sections of-
ten deal with advanced topics that can be skipped on a (cid:12)rst reading. For exam-
ple in two key chapters { Chapter 4 (The Source Coding Theorem) and Chap-
ter 10 (The Noisy-Channel Coding Theorem) { the (cid:12)rst-time reader should
detour at section 4.5 and section 10.4 respectively.

Pages vii{x show a few ways to use this book. First, I give the roadmap for
a course that I teach in Cambridge: ‘Information theory, pattern recognition,
and neural networks’. The book is also intended as a textbook for traditional
courses in information theory. The second roadmap shows the chapters for an
introductory information theory course and the third for a course aimed at an
understanding of state-of-the-art error-correcting codes. The fourth roadmap
shows how to use the text in a conventional course on machine learning.

v

vi

1

2

3

Probability, Entropy, and Inference

More about Inference

I Data Compression

4

5

6

7

The Source Coding Theorem

Symbol Codes

Stream Codes

Codes for Integers

8

9

10

11

Dependent Random Variables

Communication over a Noisy Channel

The Noisy-Channel Coding Theorem

Error-Correcting Codes and Real Channels

III Further Topics in Information Theory

Hash Codes

Binary Codes

12

13

14

15

20

21

An Example Inference Task: Clustering

Exact Inference by Complete Enumeration

22 Maximum Likelihood and Clustering

23

24

25

26

27

Useful Probability Distributions

Exact Marginalization

Exact Marginalization in Trellises

Exact Marginalization in Graphs

Laplace’s Method

28 Model Comparison and Occam’s Razor

30

31

32

33

34

35

36

37

E(cid:14)cient Monte Carlo Methods

Ising Models

Exact Monte Carlo Sampling

Variational Methods

Independent Component Analysis

Random Inference Topics

Decision Theory

Bayesian Inference and Sampling Theory

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Introduction to Information Theory

IV Probabilities and Inference

Preface

II Noisy-Channel Coding

29 Monte Carlo Methods

Very Good Linear Codes Exist

Further Exercises on Information Theory

V Neural networks

16 Message Passing

17

18

Constrained Noiseless Channels

Crosswords and Codebreaking

19 Why have Sex?

Dependencies

38

39

40

41

42

43

44

45

46

Introduction to Neural Networks

The Single Neuron as a Classi(cid:12)er

Capacity of a Single Neuron

Learning as Inference

Hop(cid:12)eld Networks

Boltzmann Machines

Supervised Learning in Multilayer Networks

Gaussian Processes

Deconvolution

VI Sparse Graph Codes

47

48

49

50

Low-Density Parity-Check Codes

Convolutional Codes and Turbo Codes

Repeat{Accumulate Codes

Digital Fountain Codes

Preface

1
1

2
2

3
3

Probability, Entropy, and Inference
Probability, Entropy, and Inference

More about Inference
More about Inference

I Data Compression

4
4

5
5

6
6

7

The Source Coding Theorem
The Source Coding Theorem

Symbol Codes
Symbol Codes

Stream Codes
Stream Codes

Codes for Integers

8
8

9
9

10
10

11
11

Dependent Random Variables
Dependent Random Variables

Communication over a Noisy Channel
Communication over a Noisy Channel

The Noisy-Channel Coding Theorem
The Noisy-Channel Coding Theorem

Error-Correcting Codes and Real Channels
Error-Correcting Codes and Real Channels

III Further Topics in Information Theory

Hash Codes

Binary Codes

12

13

14

15

20
20

21
21

An Example Inference Task: Clustering
An Example Inference Task: Clustering

Exact Inference by Complete Enumeration
Exact Inference by Complete Enumeration

22 Maximum Likelihood and Clustering
22 Maximum Likelihood and Clustering

23

24
24

25

26

27
27

Useful Probability Distributions

Exact Marginalization
Exact Marginalization

Exact Marginalization in Trellises

Exact Marginalization in Graphs

Laplace’s Method
Laplace’s Method

28 Model Comparison and Occam’s Razor

30
30

31
31

32
32

33
33

34

35

36

37

E(cid:14)cient Monte Carlo Methods
E(cid:14)cient Monte Carlo Methods

Ising Models
Ising Models

Exact Monte Carlo Sampling
Exact Monte Carlo Sampling

Variational Methods
Variational Methods

Independent Component Analysis

Random Inference Topics

Decision Theory

Bayesian Inference and Sampling Theory

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Introduction to Information Theory
Introduction to Information Theory

IV Probabilities and Inference

vii

II Noisy-Channel Coding

29 Monte Carlo Methods
29 Monte Carlo Methods

Very Good Linear Codes Exist

Further Exercises on Information Theory

V Neural networks

16 Message Passing

17

18

Constrained Noiseless Channels

Crosswords and Codebreaking

19 Why have Sex?

My Cambridge Course on,

Information Theory,
Pattern Recognition,
and Neural Networks

38
38

39
39

40
40

41
41

42
42

43

44

45

46

Introduction to Neural Networks
Introduction to Neural Networks

The Single Neuron as a Classi(cid:12)er
The Single Neuron as a Classi(cid:12)er

Capacity of a Single Neuron
Capacity of a Single Neuron

Learning as Inference
Learning as Inference

Hop(cid:12)eld Networks
Hop(cid:12)eld Networks

Boltzmann Machines

Supervised Learning in Multilayer Networks

Gaussian Processes

Deconvolution

VI Sparse Graph Codes

47
47

48

49

50

Low-Density Parity-Check Codes
Low-Density Parity-Check Codes

Convolutional Codes and Turbo Codes

Repeat{Accumulate Codes

Digital Fountain Codes

viii

1
1

2
2

3

Probability, Entropy, and Inference
Probability, Entropy, and Inference

More about Inference

I Data Compression

4
4

5
5

6
6

7

The Source Coding Theorem
The Source Coding Theorem

Symbol Codes
Symbol Codes

Stream Codes
Stream Codes

Codes for Integers

8
8

9
9

10
10

11

Dependent Random Variables
Dependent Random Variables

Communication over a Noisy Channel
Communication over a Noisy Channel

The Noisy-Channel Coding Theorem
The Noisy-Channel Coding Theorem

Error-Correcting Codes and Real Channels

III Further Topics in Information Theory

Hash Codes

Binary Codes

12

13

14

15

20

21

An Example Inference Task: Clustering

Exact Inference by Complete Enumeration

22 Maximum Likelihood and Clustering

23

24

25

26

27

Useful Probability Distributions

Exact Marginalization

Exact Marginalization in Trellises

Exact Marginalization in Graphs

Laplace’s Method

28 Model Comparison and Occam’s Razor

30

31

32

33

34

35

36

37

E(cid:14)cient Monte Carlo Methods

Ising Models

Exact Monte Carlo Sampling

Variational Methods

Independent Component Analysis

Random Inference Topics

Decision Theory

Bayesian Inference and Sampling Theory

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Introduction to Information Theory
Introduction to Information Theory

IV Probabilities and Inference

Preface

II Noisy-Channel Coding

29 Monte Carlo Methods

Very Good Linear Codes Exist

Further Exercises on Information Theory

V Neural networks

16 Message Passing

17

18

Constrained Noiseless Channels

Crosswords and Codebreaking

19 Why have Sex?

Short Course on

Information Theory

38

39

40

41

42

43

44

45

46

Introduction to Neural Networks

The Single Neuron as a Classi(cid:12)er

Capacity of a Single Neuron

Learning as Inference

Hop(cid:12)eld Networks

Boltzmann Machines

Supervised Learning in Multilayer Networks

Gaussian Processes

Deconvolution

VI Sparse Graph Codes

47

48

49

50

Low-Density Parity-Check Codes

Convolutional Codes and Turbo Codes

Repeat{Accumulate Codes

Digital Fountain Codes

Preface

1

2

3

Probability, Entropy, and Inference

More about Inference

I Data Compression

4

5

6

7

The Source Coding Theorem

Symbol Codes

Stream Codes

Codes for Integers

8

9

10

11
11

Dependent Random Variables

Communication over a Noisy Channel

The Noisy-Channel Coding Theorem

Error-Correcting Codes and Real Channels
Error-Correcting Codes and Real Channels

III Further Topics in Information Theory

Hash Codes
Hash Codes

Binary Codes
Binary Codes

12
12

13
13

14
14

15
15

20

21

An Example Inference Task: Clustering

Exact Inference by Complete Enumeration

22 Maximum Likelihood and Clustering

23

24
24

25
25

26
26

27

Useful Probability Distributions

Exact Marginalization
Exact Marginalization

Exact Marginalization in Trellises
Exact Marginalization in Trellises

Exact Marginalization in Graphs
Exact Marginalization in Graphs

Laplace’s Method

28 Model Comparison and Occam’s Razor

30

31

32

33

34

35

36

37

E(cid:14)cient Monte Carlo Methods

Ising Models

Exact Monte Carlo Sampling

Variational Methods

Independent Component Analysis

Random Inference Topics

Decision Theory

Bayesian Inference and Sampling Theory

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Introduction to Information Theory

IV Probabilities and Inference

ix

II Noisy-Channel Coding

29 Monte Carlo Methods

Very Good Linear Codes Exist
Very Good Linear Codes Exist

Further Exercises on Information Theory
Further Exercises on Information Theory

V Neural networks

16 Message Passing
16 Message Passing

17
17

18

Constrained Noiseless Channels
Constrained Noiseless Channels

Crosswords and Codebreaking

19 Why have Sex?

Advanced Course on

Information Theory and Coding

38

39

40

41

42

43

44

45

46

Introduction to Neural Networks

The Single Neuron as a Classi(cid:12)er

Capacity of a Single Neuron

Learning as Inference

Hop(cid:12)eld Networks

Boltzmann Machines

Supervised Learning in Multilayer Networks

Gaussian Processes

Deconvolution

VI Sparse Graph Codes

47
47

48
48

49
49

50
50

Low-Density Parity-Check Codes
Low-Density Parity-Check Codes

Convolutional Codes and Turbo Codes
Convolutional Codes and Turbo Codes

Repeat{Accumulate Codes
Repeat{Accumulate Codes

Digital Fountain Codes
Digital Fountain Codes

x

1

2
2

3
3

Probability, Entropy, and Inference
Probability, Entropy, and Inference

More about Inference
More about Inference

I Data Compression

4

5

6

7

The Source Coding Theorem

Symbol Codes

Stream Codes

Codes for Integers

8

9

10

11

Dependent Random Variables

Communication over a Noisy Channel

The Noisy-Channel Coding Theorem

Error-Correcting Codes and Real Channels

III Further Topics in Information Theory

Hash Codes

Binary Codes

12

13

14

15

20
20

21
21

An Example Inference Task: Clustering
An Example Inference Task: Clustering

Exact Inference by Complete Enumeration
Exact Inference by Complete Enumeration

22 Maximum Likelihood and Clustering
22 Maximum Likelihood and Clustering

23

24
24

25

26

27
27

Useful Probability Distributions

Exact Marginalization
Exact Marginalization

Exact Marginalization in Trellises

Exact Marginalization in Graphs

Laplace’s Method
Laplace’s Method

28 Model Comparison and Occam’s Razor
28 Model Comparison and Occam’s Razor

30
30

31
31

32
32

33
33

34
34

35

36

37

E(cid:14)cient Monte Carlo Methods
E(cid:14)cient Monte Carlo Methods

Ising Models
Ising Models

Exact Monte Carlo Sampling
Exact Monte Carlo Sampling

Variational Methods
Variational Methods

Independent Component Analysis
Independent Component Analysis

Random Inference Topics

Decision Theory

Bayesian Inference and Sampling Theory

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Introduction to Information Theory

IV Probabilities and Inference

Preface

II Noisy-Channel Coding

29 Monte Carlo Methods
29 Monte Carlo Methods

Very Good Linear Codes Exist

Further Exercises on Information Theory

V Neural networks

16 Message Passing

17

18

Constrained Noiseless Channels

Crosswords and Codebreaking

19 Why have Sex?

A Course on Bayesian Inference

and Machine Learning

38
38

39
39

40
40

41
41

42
42

43
43

44
44

45
45

46

Introduction to Neural Networks
Introduction to Neural Networks

The Single Neuron as a Classi(cid:12)er
The Single Neuron as a Classi(cid:12)er

Capacity of a Single Neuron
Capacity of a Single Neuron

Learning as Inference
Learning as Inference

Hop(cid:12)eld Networks
Hop(cid:12)eld Networks

Boltzmann Machines
Boltzmann Machines

Supervised Learning in Multilayer Networks
Supervised Learning in Multilayer Networks

Gaussian Processes
Gaussian Processes

Deconvolution

VI Sparse Graph Codes

47

48

49

50

Low-Density Parity-Check Codes

Convolutional Codes and Turbo Codes

Repeat{Accumulate Codes

Digital Fountain Codes

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

xi

Preface

About the exercises

You can understand a subject only by creating it for yourself. The exercises
play an essential role in this book. For guidance, each has a rating (similar to
that used by Knuth (1968)) from 1 to 5 to indicate its di(cid:14)culty.

In addition, exercises that are especially recommended are marked by a
marginal encouraging rat. Some exercises that require the use of a computer
are marked with a C.

Answers to many exercises are provided. Use them wisely. Where a solu-
tion is provided, this is indicated by including its page number alongside the
di(cid:14)culty rating.

Solutions to many of the other exercises will be supplied to instructors

using this book in their teaching; please email solutions@cambridge.org.

Summary of codes for exercises

Especially recommended

.
C

Recommended
Parts require a computer

[p. 42] Solution provided on page 42

[1 ] Simple (one minute)
[2 ] Medium (quarter hour)
[3 ] Moderately hard
[4 ] Hard
[5 ] Research project

Internet resources

The website

http://www.inference.phy.cam.ac.uk/mackay/itila

contains several resources:

1. Software. Teaching software that I use in lectures, interactive software,
and research software, written in perl, octave, tcl, C, and gnuplot.
Also some animations.

2. Corrections to the book. Thank you in advance for emailing these!

3. This book. The book is provided in postscript, pdf, and djvu formats
for on-screen viewing. The same copyright restrictions apply as to a
normal book.

About this edition

This is the fourth printing of the (cid:12)rst edition.
In the second printing, the
design of the book was altered slightly. Page-numbering generally remained
unchanged, except in chapters 1, 6, and 28, where a few paragraphs, (cid:12)gures,
and equations moved around. All equation, section, and exercise numbers
were unchanged.
In the third printing, chapter 8 was renamed ‘Dependent
Random Variables’, instead of ‘Correlated’, which was sloppy.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Preface

xii

Acknowledgments

I am most grateful to the organizations who have supported me while this
book gestated: the Royal Society and Darwin College who gave me a fantas-
tic research fellowship in the early years; the University of Cambridge; the
Keck Centre at the University of California in San Francisco, where I spent a
productive sabbatical; and the Gatsby Charitable Foundation, whose support
gave me the freedom to break out of the Escher staircase that book-writing
had become.

My work has depended on the generosity of free software authors. I wrote
the book in LATEX 2". Three cheers for Donald Knuth and Leslie Lamport!
Our computers run the GNU/Linux operating system. I use emacs, perl, and
gnuplot every day. Thank you Richard Stallman, thank you Linus Torvalds,
thank you everyone.

Many readers, too numerous to name here, have given feedback on the
book, and to them all I extend my sincere acknowledgments. I especially wish
to thank all the students and colleagues at Cambridge University who have
attended my lectures on information theory and machine learning over the last
nine years.

The members of the Inference research group have given immense support,
and I thank them all for their generosity and patience over the last ten years:
Mark Gibbs, Michelle Povinelli, Simon Wilson, Coryn Bailer-Jones, Matthew
Davey, Katriona Macphee, James Miskin, David Ward, Edward Ratzer, Seb
Wills, John Barry, John Winn, Phil Cowans, Hanna Wallach, Matthew Gar-
rett, and especially Sanjoy Mahajan. Thank you too to Graeme Mitchison,
Mike Cates, and Davin Yap.

Finally I would like to express my debt to my personal heroes, the mentors
from whom I have learned so much: Yaser Abu-Mostafa, Andrew Blake, John
Bridle, Peter Cheeseman, Steve Gull, Geo(cid:11) Hinton, John Hop(cid:12)eld, Steve Lut-
trell, Robert MacKay, Bob McEliece, Radford Neal, Roger Sewell, and John
Skilling.

Dedication

This book is dedicated to the campaign against the arms trade.

www.caat.org.uk

Peace cannot be kept by force.
It can only be achieved through understanding.
{ Albert Einstein

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 1

In the (cid:12)rst chapter, you will need to be familiar with the binomial distribution.
And to solve the exercises in the text { which I urge you to do { you will need
to know Stirling’s approximation for the factorial function, x! ’ xx e(cid:0)x, and
be able to apply it to (cid:0)N

(N(cid:0)r)! r! . These topics are reviewed below.

r(cid:1) = N !

Unfamiliar notation?
See Appendix A, p.598.

The binomial distribution

Example 1.1. A bent coin has probability f of coming up heads. The coin is
tossed N times. What is the probability distribution of the number of
heads, r? What are the mean and variance of r?

Solution. The number of heads has a binomial distribution.

P (r j f; N ) =(cid:18)N

r(cid:19)f r(1 (cid:0) f )N(cid:0)r:

(1.1)

0.3
0.25
0.2
0.15
0.1
0.05
0

The mean, E[r], and variance, var[r], of this distribution are de(cid:12)ned by

0 1 2 3 4 5 6 7 8 9 10

r

N

P (r j f; N ) r

(1.2)

Figure 1.1. The binomial
distribution P (r j f = 0:3; N = 10).

E[r] (cid:17)

Xr=0
var[r] (cid:17) Eh(r (cid:0) E[r])2i
= E[r2] (cid:0) (E[r])2 =

N

Xr=0

P (r j f; N )r2 (cid:0) (E[r])2 :

(1.3)

(1.4)

Rather than evaluating the sums over r in (1.2) and (1.4) directly, it is easiest
to obtain the mean and variance by noting that r is the sum of N independent
random variables, namely, the number of heads in the (cid:12)rst toss (which is either
zero or one), the number of heads in the second toss, and so forth. In general,

E[x + y] = E[x] + E[y]
var[x + y] = var[x] + var[y]

for any random variables x and y;
if x and y are independent:

(1.5)

So the mean of r is the sum of the means of those random variables, and the
variance of r is the sum of their variances. The mean number of heads in a
single toss is f (cid:2) 1 + (1 (cid:0) f ) (cid:2) 0 = f , and the variance of the number of heads
in a single toss is

(cid:2)f (cid:2) 12 + (1 (cid:0) f ) (cid:2) 02(cid:3) (cid:0) f 2 = f (cid:0) f 2 = f (1 (cid:0) f );

so the mean and variance of r are:

(1.6)

E[r] = N f

and

var[r] = N f (1 (cid:0) f ):

2

(1.7)

1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2

About Chapter 1

Approximating x! and (cid:0)N
r(cid:1)

Let’s derive Stirling’s approximation by an unconventional route. We start
from the Poisson distribution with mean (cid:21),

P (r j (cid:21)) = e(cid:0)(cid:21) (cid:21)r

r!

r 2 f0; 1; 2; : : :g:

(1.8)

For large (cid:21), this distribution is well approximated { at least in the vicinity of
r ’ (cid:21) { by a Gaussian distribution with mean (cid:21) and variance (cid:21):

e(cid:0)(cid:21) (cid:21)r

r! ’

1

p2(cid:25)(cid:21)

(r(cid:0)(cid:21))2

2(cid:21)

:

e(cid:0)

Let’s plug r = (cid:21) into this formula, then rearrange it.

1

(cid:21)! ’

e(cid:0)(cid:21) (cid:21)(cid:21)
) (cid:21)! ’ (cid:21)(cid:21) e(cid:0)(cid:21)p2(cid:25)(cid:21):

p2(cid:25)(cid:21)

This is Stirling’s approximation for the factorial function.

x! ’ xx e(cid:0)xp2(cid:25)x , ln x! ’ x ln x (cid:0) x + 1

2 ln 2(cid:25)x:

(1.9)

(1.10)

(1.11)

(1.12)

0.12
0.1
0.08
0.06
0.04
0.02
0

15

20

25

0

5

10
r

Figure 1.2. The Poisson
distribution P (r j (cid:21) = 15).

We have derived not only the leading order behaviour, x! ’ xx e(cid:0)x, but also,
at no cost, the next-order correction term p2(cid:25)x. We now apply Stirling’s
approximation to ln(cid:0)N
r(cid:1):
r(cid:19) (cid:17) ln
(N (cid:0) r)! r! ’ (N (cid:0) r) ln

ln(cid:18)N

N
N (cid:0) r

(1.13)

+ r ln

N !

N
r

:

Since all the terms in this equation are logarithms, this result can be rewritten
in any base. We will denote natural logarithms (log e) by ‘ln’, and logarithms
to base 2 (log2) by ‘log’.

If we introduce the binary entropy function,

Recall that log2 x =

.

loge x
loge 2
1

loge 2

Note that

@ log2 x

@x

=

1
x

.

H2(x) (cid:17) x log

1
x

+ (1(cid:0)x) log

1

(1(cid:0)x)

;

then we can rewrite the approximation (1.13) as

or, equivalently,

log(cid:18)N
r(cid:19) ’ N H2(r=N );
(cid:18)N
r(cid:19) ’ 2N H2(r=N ):

(1.14)

(1.15)

(1.16)

1

H2(x)

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1 x

If we need a more accurate approximation, we can include terms of the next
order from Stirling’s approximation (1.12):

Figure 1.3. The binary entropy
function.

log(cid:18)N

r(cid:19) ’ N H2(r=N ) (cid:0) 1

2 log(cid:20)2(cid:25)N

N(cid:0)r
N

r

N(cid:21) :

(1.17)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1

Introduction to Information Theory

The fundamental problem of communication is that of reproducing
at one point either exactly or approximately a message selected at
another point.

(Claude Shannon, 1948)

In the (cid:12)rst half of this book we study how to measure information content; we
learn how to compress data; and we learn how to communicate perfectly over
imperfect communication channels.

We start by getting a feeling for this last problem.

1.1 How can we achieve perfect communication over an imperfect,

noisy communication channel?

Some examples of noisy communication channels are:

(cid:15) an analogue telephone line, over which two modems communicate digital

information;

(cid:15) the radio communication link from Galileo, the Jupiter-orbiting space-

craft, to earth;

(cid:15) reproducing cells, in which the daughter cells’ DNA contains information

from the parent cells;

(cid:15) a disk drive.

The last example shows that communication doesn’t have to involve informa-
tion going from one place to another. When we write a (cid:12)le on a disk drive,
we’ll read it o(cid:11) in the same location { but at a later time.

These channels are noisy. A telephone line su(cid:11)ers from cross-talk with
other lines; the hardware in the line distorts and adds noise to the transmitted
signal. The deep space network that listens to Galileo’s puny transmitter
receives background radiation from terrestrial and cosmic sources. DNA is
subject to mutations and damage. A disk drive, which writes a binary digit
(a one or zero, also known as a bit) by aligning a patch of magnetic material
in one of two orientations, may later fail to read out the stored binary digit:
the patch of material might spontaneously (cid:13)ip magnetization, or a glitch of
background noise might cause the reading circuit to report the wrong value
for the binary digit, or the writing head might not induce the magnetization
in the (cid:12)rst place because of interference from neighbouring bits.

In all these cases, if we transmit data, e.g., a string of bits, over the channel,
there is some probability that the received message will not be identical to the

3

modem

-

phone

-

line

modem

Galileo

-

-

radio
waves

Earth

parent

cell

(cid:0)(cid:0)(cid:18)
@@R

daughter

cell

daughter

cell

computer
memory

-

disk
drive

-

computer
memory

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4

1 | Introduction to Information Theory

transmitted message. We would prefer to have a communication channel for
which this probability was zero { or so close to zero that for practical purposes
it is indistinguishable from zero.

Let’s consider a noisy disk drive that transmits each bit correctly with
probability (1(cid:0)f ) and incorrectly with probability f . This model communi-
cation channel is known as the binary symmetric channel ((cid:12)gure 1.4).

0

-
(cid:0)(cid:0)(cid:18)@@R
x -

1

0

1

y P (y = 0j x = 0) = 1 (cid:0) f ;

P (y = 1j x = 0) = f ;

P (y = 0j x = 1) = f ;
P (y = 1j x = 1) = 1 (cid:0) f:

0

1

@
(cid:0)

(1 (cid:0) f )
-
(cid:0)(cid:0)(cid:18)@
f
@@R
(cid:0)
-
(1 (cid:0) f )

0

1

Figure 1.4. The binary symmetric
channel. The transmitted symbol
is x and the received symbol y.
The noise level, the probability
that a bit is (cid:13)ipped, is f .

Figure 1.5. A binary data
sequence of length 10 000
transmitted over a binary
symmetric channel with noise
level f = 0:1. [Dilbert image
Copyright c(cid:13)1997 United Feature
Syndicate, Inc., used with
permission.]

As an example, let’s imagine that f = 0:1, that is, ten per cent of the bits are
(cid:13)ipped ((cid:12)gure 1.5). A useful disk drive would (cid:13)ip no bits at all in its entire
lifetime. If we expect to read and write a gigabyte per day for ten years, we
require a bit error probability of the order of 10(cid:0)15, or smaller. There are two
approaches to this goal.

The physical solution

The physical solution is to improve the physical characteristics of the commu-
nication channel to reduce its error probability. We could improve our disk
drive by

1. using more reliable components in its circuitry;

2. evacuating the air from the disk enclosure so as to eliminate the turbu-

lence that perturbs the reading head from the track;

3. using a larger magnetic patch to represent each bit; or

4. using higher-power signals or cooling the circuitry in order to reduce

thermal noise.

These physical modi(cid:12)cations typically increase the cost of the communication
channel.

The ‘system’ solution

Information theory and coding theory o(cid:11)er an alternative (and much more ex-
citing) approach: we accept the given noisy channel as it is and add communi-
cation systems to it so that we can detect and correct the errors introduced by
the channel. As shown in (cid:12)gure 1.6, we add an encoder before the channel and
a decoder after it. The encoder encodes the source message s into a transmit-
ted message t, adding redundancy to the original message in some way. The
channel adds noise to the transmitted message, yielding a received message r.
The decoder uses the known redundancy introduced by the encoding system
to infer both the original signal s and the added noise.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.2: Error-correcting codes for the binary symmetric channel

5

Source

s

?

Encoder

t

-

6
^s

Decoder

6
r

Noisy
channel

Figure 1.6. The ‘system’ solution
for achieving reliable
communication over a noisy
channel. The encoding system
introduces systematic redundancy
into the transmitted vector t. The
decoding system uses this known
redundancy to deduce from the
received vector r both the original
source vector and the noise
introduced by the channel.

Whereas physical solutions give incremental channel improvements only at
an ever-increasing cost, system solutions can turn noisy channels into reliable
communication channels with the only cost being a computational requirement
at the encoder and decoder.

Information theory is concerned with the theoretical limitations and po-
‘What is the best error-correcting performance we

tentials of such systems.
could achieve?’

Coding theory is concerned with the creation of practical encoding and

decoding systems.

1.2 Error-correcting codes for the binary symmetric channel

We now consider examples of encoding and decoding systems. What is the
simplest way to add useful redundancy to a transmission? [To make the rules
of the game clear: we want to be able to detect and correct errors; and re-
transmission is not an option. We get only one chance to encode, transmit,
and decode.]

Repetition codes

A straightforward idea is to repeat every bit of the message a prearranged
number of times { for example, three times, as shown in table 1.7. We call
this repetition code ‘R3’.

Imagine that we transmit the source message

s = 0 0 1 0 1 1 0

over a binary symmetric channel with noise level f = 0:1 using this repetition
code. We can describe the channel as ‘adding’ a sparse noise vector n to the
transmitted vector { adding in modulo 2 arithmetic, i.e., the binary algebra
in which 1+1=0. A possible noise vector n and received vector r = t + n are
shown in (cid:12)gure 1.8.

Source
sequence

Transmitted

sequence

s

0
1

t

000
111

Table 1.7. The repetition code R3.

s

0

0

1

0

1

1

0

z}|{0 0 0 z}|{0 0 0 z}|{1 1 1 z}|{0 0 0 z}|{1 1 1 z}|{1 1 1 z}|{0 0 0

t
n 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0
r 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0

Figure 1.8. An example
transmission using R3.

How should we decode this received vector? The optimal algorithm looks
at the received bits three at a time and takes a majority vote (algorithm 1.9).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6

1 | Introduction to Information Theory

Received sequence r Likelihood ratio P (rj s = 1)

P (rj s = 0) Decoded sequence ^s

000
001
010
100
101
110
011
111

(cid:13)(cid:0)3
(cid:13)(cid:0)1
(cid:13)(cid:0)1
(cid:13)(cid:0)1
(cid:13)1
(cid:13)1
(cid:13)1
(cid:13)3

0
0
0
0
1
1
1
1

Algorithm 1.9. Majority-vote
decoding algorithm for R3. Also
shown are the likelihood ratios
(1.23), assuming the channel is a
binary symmetric channel;
(cid:13) (cid:17) (1 (cid:0) f )=f .

At the risk of explaining the obvious, let’s prove this result. The optimal
decoding decision (optimal in the sense of having the smallest probability of
being wrong) is to (cid:12)nd which value of s is most probable, given r. Consider
the decoding of a single bit s, which was encoded as t(s) and gave rise to three
received bits r = r1r2r3. By Bayes’ theorem, the posterior probability of s is

P (sj r1r2r3) =

P (r1r2r3 j s)P (s)

P (r1r2r3)

:

We can spell out the posterior probability of the two alternatives thus:

P (s = 1j r1r2r3) =

P (r1r2r3 j s = 1)P (s = 1)

P (r1r2r3)

P (s = 0j r1r2r3) =

P (r1r2r3 j s = 0)P (s = 0)

P (r1r2r3)

;

:

(1.18)

(1.19)

(1.20)

This posterior probability is determined by two factors: the prior probability
P (s), and the data-dependent term P (r1r2r3 j s), which is called the likelihood
of s. The normalizing constant P (r1r2r3) needn’t be computed when (cid:12)nding the
optimal decoding decision, which is to guess ^s = 0 if P (s = 0j r) > P (s = 1j r),
and ^s = 1 otherwise.
To (cid:12)nd P (s = 0j r) and P (s = 1j r), we must make an assumption about the
prior probabilities of the two hypotheses s = 0 and s = 1, and we must make an
assumption about the probability of r given s. We assume that the prior prob-
abilities are equal: P (s = 0) = P (s = 1) = 0:5; then maximizing the posterior
probability P (sj r) is equivalent to maximizing the likelihood P (rj s). And we
assume that the channel is a binary symmetric channel with noise level f < 0:5,
so that the likelihood is

P (rj s) = P (rj t(s)) =

N

Yn=1

P (rn j tn(s));

(1.21)

where N = 3 is the number of transmitted bits in the block we are considering,
and

P (rn j tn) =(cid:26) (1(cid:0)f )

f

if
if

rn = tn
rn 6= tn:

(1.22)

Thus the likelihood ratio for the two hypotheses is

N

Yn=1

=

P (rj s = 1)
P (rj s = 0)
P (rnjtn(0)) equals (1(cid:0)f )
each factor P (rnjtn(1))
(1(cid:0)f ) if rn = 0. The ratio
(cid:13) (cid:17) (1(cid:0)f )
is greater than 1, since f < 0:5, so the winning hypothesis is the
one with the most ‘votes’, each vote counting for a factor of (cid:13) in the likelihood
ratio.

P (rn j tn(1))
P (rn j tn(0))

if rn = 1 and

(1.23)

;

f

f

f

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.2: Error-correcting codes for the binary symmetric channel

7

Thus the majority-vote decoder shown in algorithm 1.9 is the optimal decoder
if we assume that the channel is a binary symmetric channel and that the two
possible source messages 0 and 1 have equal prior probability.

We now apply the majority vote decoder to the received vector of (cid:12)gure 1.8.
The (cid:12)rst three received bits are all 0, so we decode this triplet as a 0. In the
second triplet of (cid:12)gure 1.8, there are two 0s and one 1, so we decode this triplet
as a 0 { which in this case corrects the error. Not all errors are corrected,
however. If we are unlucky and two errors fall in a single block, as in the (cid:12)fth
triplet of (cid:12)gure 1.8, then the decoding rule gets the wrong answer, as shown
in (cid:12)gure 1.10.

s

0

0

1

0

1

1

0

Figure 1.10. Decoding the received
vector from (cid:12)gure 1.8.

t
n 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0
r 0 0 0
0 0 0

z}|{0 0 0 z}|{0 0 0 z}|{1 1 1 z}|{0 0 0 z}|{1 1 1 z}|{1 1 1 z}|{0 0 0
|{z}
|{z}

|{z}

|{z}

|{z}

0
?

1

|{z}

1 1 1

1

|{z}

0 0 0

0

0 0 1

1 1 1

0 1 0

^s
corrected errors
undetected errors

0

0

0

?

Exercise 1.2.[2, p.16] Show that the error probability is reduced by the use of
R3 by computing the error probability of this code for a binary symmetric
channel with noise level f .

The error probability is dominated by the probability that two bits in
a block of three are (cid:13)ipped, which scales as f 2.
In the case of the binary
symmetric channel with f = 0:1, the R3 code has a probability of error, after
decoding, of pb ’ 0:03 per bit. Figure 1.11 shows the result of transmitting a
binary image over a binary symmetric channel using the repetition code.

The exercise’s rating, e.g.‘[2 ]’,
indicates its di(cid:14)culty: ‘1’
exercises are the easiest. Exercises
that are accompanied by a
marginal rat are especially
recommended. If a solution or
partial solution is provided, the
page is indicated after the
di(cid:14)culty rating; for example, this
exercise’s solution is on page 16.

s

encoder

t

-

channel
f = 10%

-

r

decoder

^s

-

Figure 1.11. Transmitting 10 000
source bits over a binary
symmetric channel with f = 10%
using a repetition code and the
majority vote decoding algorithm.
The probability of decoded bit
error has fallen to about 3%; the
rate has fallen to 1/3.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

8

0.1

0.08

0.06

0.04

0.02

0

1 | Introduction to Information Theory

R1

0.1
0.01

R5

R3

R1

more useful codes

1e-05

pb

1e-10

Figure 1.12. Error probability pb
versus rate for repetition codes
over a binary symmetric channel
with f = 0:1. The right-hand
(cid:12)gure shows pb on a logarithmic
scale. We would like the rate to
be large and pb to be small.

R3

R5

R61

more useful codes

R61

1e-15

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Rate

Rate

The repetition code R3 has therefore reduced the probability of error, as
desired. Yet we have lost something: our rate of information transfer has
fallen by a factor of three. So if we use a repetition code to communicate data
over a telephone line, it will reduce the error frequency, but it will also reduce
our communication rate. We will have to pay three times as much for each
phone call. Similarly, we would need three of the original noisy gigabyte disk
drives in order to create a one-gigabyte disk drive with pb = 0:03.

Can we push the error probability lower, to the values required for a sell-
able disk drive { 10(cid:0)15? We could achieve lower error probabilities by using
repetition codes with more repetitions.

Exercise 1.3.[3, p.16]

(a) Show that the probability of error of RN , the repe-

tition code with N repetitions, is

pb =

for odd N .

N

Xn=(N +1)=2(cid:18)N

n(cid:19)f n(1 (cid:0) f )N(cid:0)n;

(1.24)

(b) Assuming f = 0:1, which of the terms in this sum is the biggest?

How much bigger is it than the second-biggest term?

(c) Use Stirling’s approximation (p.2) to approximate the (cid:0)N

largest term, and (cid:12)nd, approximately, the probability of error of
the repetition code with N repetitions.

n(cid:1) in the

(d) Assuming f = 0:1, (cid:12)nd how many repetitions are required to get

the probability of error down to 10(cid:0)15. [Answer: about 60.]

So to build a single gigabyte disk drive with the required reliability from noisy
gigabyte drives with f = 0:1, we would need sixty of the noisy disk drives.
The tradeo(cid:11) between error probability and rate for repetition codes is shown
in (cid:12)gure 1.12.

Block codes { the (7; 4) Hamming code

We would like to communicate with tiny probability of error and at a substan-
tial rate. Can we improve on repetition codes? What if we add redundancy to
blocks of data instead of encoding one bit at a time? We now study a simple
block code.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.2: Error-correcting codes for the binary symmetric channel

9

A block code is a rule for converting a sequence of source bits s, of length
K, say, into a transmitted sequence t of length N bits. To add redundancy,
we make N greater than K. In a linear block code, the extra N (cid:0) K bits are
linear functions of the original K bits; these extra bits are called parity-check
bits. An example of a linear block code is the (7; 4) Hamming code, which
transmits N = 7 bits for every K = 4 source bits.

t
5

s
3
4s

s
2

t
6

s
1

t
7

1

0

0

1

1

0

0

(a)

(b)

Figure 1.13. Pictorial
representation of encoding for the
(7; 4) Hamming code.

The encoding operation for the code is shown pictorially in (cid:12)gure 1.13. We
arrange the seven transmitted bits in three intersecting circles. The (cid:12)rst four
transmitted bits, t1t2t3t4, are set equal to the four source bits, s1s2s3s4. The
parity-check bits t5t6t7 are set so that the parity within each circle is even:
the (cid:12)rst parity-check bit is the parity of the (cid:12)rst three source bits (that is, it
is 0 if the sum of those bits is even, and 1 if the sum is odd); the second is
the parity of the last three; and the third parity bit is the parity of source bits
one, three and four.

As an example, (cid:12)gure 1.13b shows the transmitted codeword for the case
s = 1000. Table 1.14 shows the codewords generated by each of the 24 =
sixteen settings of the four source bits. These codewords have the special
property that any pair di(cid:11)er from each other in at least three bits.

s

0000
0001
0010
0011

t

0000000
0001011
0010111
0011100

s

0100
0101
0110
0111

t

0100110
0101101
0110001
0111010

s

1000
1001
1010
1011

t

1000101
1001110
1010010
1011001

s

1100
1101
1110
1111

t

1100011
1101000
1110100
1111111

Table 1.14. The sixteen codewords
ftg of the (7; 4) Hamming code.
Any pair of codewords di(cid:11)er from
each other in at least three bits.

Because the Hamming code is a linear code, it can be written compactly in
terms of matrices as follows. The transmitted codeword t is obtained from the
source sequence s by a linear operation,

t = GTs;

(1.25)

where G is the generator matrix of the code,

2

666666664

1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
1 1 1 0
0 1 1 1
1 0 1 1

3

777777775

GT =

;

(1.26)

and the encoding operation (1.25) uses modulo-2 arithmetic (1 + 1 = 0, 0 + 1 =
1, etc.).

In the encoding operation (1.25) I have assumed that s and t are column vectors.
If instead they are row vectors, then this equation is replaced by

t = sG;

(1.27)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10

where

1 | Introduction to Information Theory

:

(1.28)

G =2
664

1 0 0 0 1 0 1
0 1 0 0 1 1 0
0 0 1 0 1 1 1
0 0 0 1 0 1 1

3
775

I (cid:12)nd it easier to relate to the right-multiplication (1.25) than the left-multiplica-
tion (1.27). Many coding theory texts use the left-multiplying conventions
(1.27{1.28), however.

The rows of the generator matrix (1.28) can be viewed as de(cid:12)ning four basis
vectors lying in a seven-dimensional binary space. The sixteen codewords are
obtained by making all possible linear combinations of these vectors.

Decoding the (7; 4) Hamming code

When we invent a more complex encoder s ! t, the task of decoding the
received vector r becomes less straightforward. Remember that any of the
bits may have been (cid:13)ipped, including the parity bits.

If we assume that the channel is a binary symmetric channel and that all
source vectors are equiprobable, then the optimal decoder identi(cid:12)es the source
vector s whose encoding t(s) di(cid:11)ers from the received vector r in the fewest
bits. [Refer to the likelihood function (1.23) to see why this is so.] We could
solve the decoding problem by measuring how far r is from each of the sixteen
codewords in table 1.14, then picking the closest. Is there a more e(cid:14)cient way
of (cid:12)nding the most probable source vector?

Syndrome decoding for the Hamming code

For the (7; 4) Hamming code there is a pictorial solution to the decoding
problem, based on the encoding picture, (cid:12)gure 1.13.

As a (cid:12)rst example, let’s assume the transmission was t = 1000101 and the
noise (cid:13)ips the second bit, so the received vector is r = 1000101 (cid:8) 0100000 =
1100101. We write the received vector into the three circles as shown in
(cid:12)gure 1.15a, and look at each of the three circles to see whether its parity
is even. The circles whose parity is not even are shown by dashed lines in
(cid:12)gure 1.15b. The decoding task is to (cid:12)nd the smallest set of (cid:13)ipped bits that
can account for these violations of the parity rules. [The pattern of violations
of the parity checks is called the syndrome, and can be written as a binary
vector { for example, in (cid:12)gure 1.15b, the syndrome is z = (1; 1; 0), because
the (cid:12)rst two circles are ‘unhappy’ (parity 1) and the third circle is ‘happy’
(parity 0).]

To solve the decoding task, we ask the question: can we (cid:12)nd a unique bit
that lies inside all the ‘unhappy’ circles and outside all the ‘happy’ circles? If
so, the (cid:13)ipping of that bit would account for the observed syndrome. In the
case shown in (cid:12)gure 1.15b, the bit r2 lies inside the two unhappy circles and
outside the happy circle; no other single bit has this property, so r2 is the only
single bit capable of explaining the syndrome.

Let’s work through a couple more examples. Figure 1.15c shows what
happens if one of the parity bits, t5, is (cid:13)ipped by the noise. Just one of the
checks is violated. Only r5 lies inside this unhappy circle and outside the other
two happy circles, so r5 is identi(cid:12)ed as the only single bit capable of explaining
the syndrome.

If the central bit r3 is received (cid:13)ipped, (cid:12)gure 1.15d shows that all three
checks are violated; only r3 lies inside all three circles, so r3 is identi(cid:12)ed as
the suspect bit.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.2: Error-correcting codes for the binary symmetric channel

11

Figure 1.15. Pictorial
representation of decoding of the
Hamming (7; 4) code. The
received vector is written into the
diagram as shown in (a). In
(b,c,d,e), the received vector is
shown, assuming that the
transmitted vector was as in
(cid:12)gure 1.13b and the bits labelled
by ? were (cid:13)ipped. The violated
parity checks are highlighted by
dashed circles. One of the seven
bits is the most probable suspect
to account for each ‘syndrome’,
i.e., each pattern of violated and
satis(cid:12)ed parity checks.
In examples (b), (c), and (d), the
most probable suspect is the one
bit that was (cid:13)ipped.
In example (e), two bits have been
(cid:13)ipped, s3 and t7. The most
probable suspect is r2, marked by
a circle in (e0), which shows the
output of the decoding algorithm.

Algorithm 1.16. Actions taken by
the optimal decoder for the (7; 4)
Hamming code, assuming a
binary symmetric channel with
small noise level f . The syndrome
vector z lists whether each parity
check is violated (1) or satis(cid:12)ed
(0), going through the checks in
the order of the bits r5, r6, and r7.

(a)

r
5

r
3
4r

1

0

0

r
2

r
6

1*

0

r
1

r
7

1

1

*0

0

0

0

0

1

1

1

1*

0

0

0

1

1

(b)

(c)

(d)

1

1*

0

0

0

1

0*

(e)

1

1*

0

1

0

-

1

0*

(e0)

Syndrome z

000

001

010

011

100

101

110

111

Un(cid:13)ip this bit

none

r7

r6

r4

r5

r1

r2

r3

If you try (cid:13)ipping any one of the seven bits, you’ll (cid:12)nd that a di(cid:11)erent
syndrome is obtained in each case { seven non-zero syndromes, one for each
bit. There is only one other syndrome, the all-zero syndrome. So if the
channel is a binary symmetric channel with a small noise level f , the optimal
decoder un(cid:13)ips at most one bit, depending on the syndrome, as shown in
algorithm 1.16. Each syndrome could have been caused by other noise patterns
too, but any other noise pattern that has the same syndrome must be less
probable because it involves a larger number of noise events.

What happens if the noise actually (cid:13)ips more than one bit? Figure 1.15e
shows the situation when two bits, r3 and r7, are received (cid:13)ipped. The syn-
drome, 110, makes us suspect the single bit r2; so our optimal decoding al-
gorithm (cid:13)ips this bit, giving a decoded pattern with three errors as shown
in (cid:12)gure 1.15e0. If we use the optimal decoding algorithm, any two-bit error
pattern will lead to a decoded seven-bit vector that contains three errors.

General view of decoding for linear codes: syndrome decoding

We can also describe the decoding problem for a linear code in terms of matrices.
The (cid:12)rst four received bits, r1r2r3r4, purport to be the four source bits; and the
received bits r5r6r7 purport to be the parities of the source bits, as de(cid:12)ned by
the generator matrix G. We evaluate the three parity-check bits for the received
bits, r1r2r3r4, and see whether they match the three received bits, r5r6r7. The
di(cid:11)erences (modulo 2) between these two triplets are called the syndrome of the
received vector. If the syndrome is zero { if all three parity checks are happy
{ then the received vector is a codeword, and the most probable decoding is

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

12

1 | Introduction to Information Theory

s

encoder

t

-

channel
f = 10%

-

r

decoder

^s

-

Figure 1.17. Transmitting 10 000
source bits over a binary
symmetric channel with f = 10%
using a (7; 4) Hamming code. The
probability of decoded bit error is
about 7%.

parity bits 8>><
>>:

given by reading out its (cid:12)rst four bits. If the syndrome is non-zero, then the
noise sequence for this block was non-zero, and the syndrome is our pointer to
the most probable error pattern.

The computation of the syndrome vector is a linear operation. If we de(cid:12)ne the
3 (cid:2) 4 matrix P such that the matrix of equation (1.26) is

P (cid:21) ;
GT =(cid:20) I4

(1.29)

where I4 is the 4 (cid:2) 4 identity matrix, then the syndrome vector is z = Hr,
where the parity-check matrix H is given by H = (cid:2) (cid:0)P I3 (cid:3); in modulo 2
arithmetic, (cid:0)1 (cid:17) 1, so

H =(cid:2) P I3 (cid:3) =2
4

1 1 1 0 1 0 0
0 1 1 1 0 1 0

1 0 1 1 0 0 1 3
5 :

All the codewords t = GTs of the code satisfy

Ht =2
4

0
0

0 3
5 :

(1.30)

(1.31)

. Exercise 1.4.[1 ] Prove that this is so by evaluating the 3 (cid:2) 4 matrix HGT.

Since the received vector r is given by r = GTs + n, the syndrome-decoding
problem is to (cid:12)nd the most probable noise vector n satisfying the equation

Hn = z:

(1.32)

A decoding algorithm that solves this problem is called a maximum-likelihood
decoder. We will discuss decoding problems like this in later chapters.

Summary of the (7; 4) Hamming code’s properties

Every possible received vector of length 7 bits is either a codeword, or it’s one
(cid:13)ip away from a codeword.

Since there are three parity constraints, each of which might or might not
be violated, there are 2 (cid:2) 2 (cid:2) 2 = 8 distinct syndromes. They can be divided
into seven non-zero syndromes { one for each of the one-bit error patterns {
and the all-zero syndrome, corresponding to the zero-noise case.

The optimal decoder takes no action if the syndrome is zero, otherwise it
uses this mapping of non-zero syndromes onto one-bit error patterns to un(cid:13)ip
the suspect bit.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.2: Error-correcting codes for the binary symmetric channel

13

There is a decoding error if the four decoded bits ^s1; ^s2; ^s3; ^s4 do not all
match the source bits s1; s2; s3; s4. The probability of block error pB is the
probability that one or more of the decoded bits in one block fail to match the
corresponding source bits,

The probability of bit error pb is the average probability that a decoded bit
fails to match the corresponding source bit,

pB = P (^s 6= s):

(1.33)

pb =

1
K

K

Xk=1

P (^sk 6= sk):

(1.34)

In the case of the Hamming code, a decoding error will occur whenever
the noise has (cid:13)ipped more than one bit in a block of seven. The probability
of block error is thus the probability that two or more bits are (cid:13)ipped in a
block. This probability scales as O(f 2), as did the probability of error for the
repetition code R3. But notice that the Hamming code communicates at a
greater rate, R = 4=7.

Figure 1.17 shows a binary image transmitted over a binary symmetric
channel using the (7; 4) Hamming code. About 7% of the decoded bits are
in error. Notice that the errors are correlated: often two or three successive
decoded bits are (cid:13)ipped.

Exercise 1.5.[1 ] This exercise and the next three refer to the (7; 4) Hamming

code. Decode the received strings:

(a) r = 1101011

(b) r = 0110110

(c) r = 0100111

(d) r = 1111111.

Exercise 1.6.[2, p.17]

(a) Calculate the probability of block error pB of the
(7; 4) Hamming code as a function of the noise level f and show
that to leading order it goes as 21f 2.

(b) [3 ] Show that to leading order the probability of bit error pb goes

as 9f 2.

Exercise 1.7.[2, p.19] Find some noise vectors that give the all-zero syndrome
(that is, noise vectors that leave all the parity checks unviolated). How
many such noise vectors are there?

. Exercise 1.8.[2 ] I asserted above that a block decoding error will result when-
ever two or more bits are (cid:13)ipped in a single block. Show that this is
indeed so.
[In principle, there might be error patterns that, after de-
coding, led only to the corruption of the parity bits, with no source bits
incorrectly decoded.]

Summary of codes’ performances

Figure 1.18 shows the performance of repetition codes and the Hamming code.
It also shows the performance of a family of linear block codes that are gen-
eralizations of Hamming codes, called BCH codes.

This (cid:12)gure shows that we can, using linear block codes, achieve better
performance than repetition codes; but the asymptotic situation still looks
grim.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

14

0.1

0.08

0.06

0.04

0.02

0

1 | Introduction to Information Theory

R1

0.1
0.01

R5

H(7,4)

R1

H(7,4)

1e-05

pb

more useful codes

BCH(511,76)

Figure 1.18. Error probability pb
versus rate R for repetition codes,
the (7; 4) Hamming code and
BCH codes with blocklengths up
to 1023 over a binary symmetric
channel with f = 0:1. The
righthand (cid:12)gure shows pb on a
logarithmic scale.

BCH(31,16)

1e-10

R3

BCH(15,7)

R5

more useful codes

BCH(1023,101)

1e-15

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Rate

Rate

Exercise 1.9.[4, p.19] Design an error-correcting code and a decoding algorithm
for it, estimate its probability of error, and add it to (cid:12)gure 1.18. [Don’t
worry if you (cid:12)nd it di(cid:14)cult to make a code better than the Hamming
code, or if you (cid:12)nd it di(cid:14)cult to (cid:12)nd a good decoder for your code; that’s
the point of this exercise.]

Exercise 1.10.[3, p.20] A (7; 4) Hamming code can correct any one error; might

there be a (14; 8) code that can correct any two errors?

Optional extra: Does the answer to this question depend on whether the
code is linear or nonlinear?

Exercise 1.11.[4, p.21] Design an error-correcting code, other than a repetition

code, that can correct any two errors in a block of size N .

1.3 What performance can the best codes achieve?

There seems to be a trade-o(cid:11) between the decoded bit-error probability pb
(which we would like to reduce) and the rate R (which we would like to keep
large). How can this trade-o(cid:11) be characterized? What points in the (R; pb)
plane are achievable? This question was addressed by Claude Shannon in his
pioneering paper of 1948, in which he both created the (cid:12)eld of information
theory and solved most of its fundamental problems.

At that time there was a widespread belief that the boundary between
achievable and nonachievable points in the (R; pb) plane was a curve passing
through the origin (R; pb) = (0; 0); if this were so, then, in order to achieve
a vanishingly small error probability pb, one would have to reduce the rate
correspondingly close to zero. ‘No pain, no gain.’

However, Shannon proved the remarkable result that the boundary be- (cid:3)

tween achievable and nonachievable points meets the R axis at a non-zero
value R = C, as shown in (cid:12)gure 1.19. For any channel, there exist codes that
make it possible to communicate with arbitrarily small probability of error pb
at non-zero rates. The (cid:12)rst half of this book (Parts I{III) will be devoted to
understanding this remarkable result, which is called the noisy-channel coding
theorem.

Example: f = 0:1

The maximum rate at which communication is possible with arbitrarily small
pb is called the capacity of the channel. The formula for the capacity of a

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.4: Summary

0.1

0.08

0.06

0.04

0.02

0

R1

0.1
0.01

R5

R1

H(7,4)

1e-05

pb

R3

R5

achievable

not achievable

0

0.2

0.4

C

0.6

Rate

0.8

1

0

0.2

0.4

C

0.6

Rate

0.8

1

achievable

not achievable

1e-10

1e-15

15

Figure 1.19. Shannon’s
noisy-channel coding theorem.
The solid curve shows the
Shannon limit on achievable
values of (R; pb) for the binary
symmetric channel with f = 0:1.
Rates up to R = C are achievable
with arbitrarily small pb. The
points show the performance of
some textbook codes, as in
(cid:12)gure 1.18.

The equation de(cid:12)ning the

Shannon limit (the solid curve) is
R = C=(1 (cid:0) H2(pb)); where C and
H2 are de(cid:12)ned in equation (1.35).

binary symmetric channel with noise level f is

C(f ) = 1 (cid:0) H2(f ) = 1 (cid:0)(cid:20)f log2

1
f

+ (1 (cid:0) f ) log2

1

1 (cid:0) f(cid:21) ;

(1.35)

the channel we were discussing earlier with noise level f = 0:1 has capacity
C ’ 0:53. Let us consider what this means in terms of noisy disk drives. The
repetition code R3 could communicate over this channel with pb = 0:03 at a
rate R = 1=3. Thus we know how to build a single gigabyte disk drive with
pb = 0:03 from three noisy gigabyte disk drives. We also know how to make a
single gigabyte disk drive with pb ’ 10(cid:0)15 from sixty noisy one-gigabyte drives
(exercise 1.3, p.8). And now Shannon passes by, notices us juggling with disk
drives and codes and says:

‘What performance are you trying to achieve? 10(cid:0)15? You don’t
need sixty disk drives { you can get that performance with just
two disk drives (since 1/2 is less than 0:53). And if you want
pb = 10(cid:0)18 or 10(cid:0)24 or anything, you can get there with two disk
drives too!’

[Strictly, the above statements might not be quite right, since, as we shall see,
Shannon proved his noisy-channel coding theorem by studying sequences of
block codes with ever-increasing blocklengths, and the required blocklength
might be bigger than a gigabyte (the size of our disk drive), in which case,
Shannon might say ‘well, you can’t do it with those tiny disk drives, but if you
had two noisy terabyte drives, you could make a single high-quality terabyte
drive from them’.]

1.4 Summary

The (7; 4) Hamming Code

By including three parity-check bits in a block of 7 bits it is possible to detect
and correct any single bit error in each block.

Shannon’s noisy-channel coding theorem

Information can be communicated over a noisy channel at a non-zero rate with
arbitrarily small error probability.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

16

1 | Introduction to Information Theory

Information theory addresses both the limitations and the possibilities of
communication. The noisy-channel coding theorem, which we will prove in
Chapter 10, asserts both that reliable communication at any rate beyond the
capacity is impossible, and that reliable communication at all rates up to
capacity is possible.

The next few chapters lay the foundations for this result by discussing
how to measure information content and the intimately related topic of data
compression.

1.5 Further exercises

. Exercise 1.12.[2, p.21] Consider the repetition code R9. One way of viewing
this code is as a concatenation of R3 with R3. We (cid:12)rst encode the
source stream with R3, then encode the resulting output with R3. We
could call this code ‘R2
3’. This idea motivates an alternative decoding
algorithm, in which we decode the bits three at a time using the decoder
for R3; then decode the decoded bits from that (cid:12)rst decoder using the
decoder for R3.
Evaluate the probability of error for this decoder and compare it with
the probability of error for the optimal decoder for R9.
Do the concatenated encoder and decoder for R2
those for R9?

3 have advantages over

1.6 Solutions

Solution to exercise 1.2 (p.7). An error is made by R3 if two or more bits are
(cid:13)ipped in a block of three. So the error probability of R3 is a sum of two
terms: the probability that all three bits are (cid:13)ipped, f 3; and the probability
that exactly two bits are (cid:13)ipped, 3f 2(1 (cid:0) f ).
[If these expressions are not
obvious, see example 1.1 (p.1): the expressions are P (r = 3j f; N = 3) and
P (r = 2j f; N = 3).]

pb = pB = 3f 2(1 (cid:0) f ) + f 3 = 3f 2 (cid:0) 2f 3:
This probability is dominated for small f by the term 3f 2.

See exercise 2.38 (p.39) for further discussion of this problem.

(1.36)

Solution to exercise 1.3 (p.8). The probability of error for the repetition code
RN is dominated by the probability that dN=2e bits are (cid:13)ipped, which goes
(for odd N ) as

dN=2e(cid:19)f (N +1)=2(1 (cid:0) f )(N(cid:0)1)=2:
(cid:18) N
K(cid:19) (cid:20) 2N H2(K=N ) ) (cid:18)N

K(cid:1) can be approximated using the binary entropy function:
K(cid:19) ’ 2N H2(K=N );
2N H2(K=N ) (cid:20)(cid:18)N

The term (cid:0)N
(1.38)
where this approximation introduces an error of order pN { as shown in
equation (1.17). So

Notation: (cid:6)N=2(cid:7) denotes the

smallest integer greater than or
equal to N=2.

(1.37)

1

N + 1

pb = pB ’ 2N (f (1 (cid:0) f ))N=2 = (4f (1 (cid:0) f ))N=2:

(1.39)

log 4f (1(cid:0)f ) = 68.
This answer is a little out because the approximation we used overestimated

Setting this equal to the required value of 10(cid:0)15 we (cid:12)nd N ’ 2 log 10(cid:0)15
K(cid:1) and we did not distinguish between dN=2e and N=2.
(cid:0)N

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.6: Solutions

17

A slightly more careful answer (short of explicit computation) goes as follows.

K(cid:1) to the next order, we (cid:12)nd:
Taking the approximation for (cid:0)N
N=2(cid:19) ’ 2N
(cid:18) N

1

:

p2(cid:25)N=4

This approximation can be proved from an accurate version of Stirling’s ap-
proximation (1.12), or by considering the binomial distribution with p = 1=2
and noting

(1.40)

N=2(cid:19) N=2
Xr=(cid:0)N=2

K(cid:19)2(cid:0)N ’ 2(cid:0)N(cid:18) N

1 =XK (cid:18)N
N=2(cid:19)p2(cid:25)(cid:27); (1.41)
where (cid:27) =pN=4, from which equation (1.40) follows. The distinction between
dN=2e and N=2 is not important in this term since (cid:0)N
K(cid:1) has a maximum at

’ 2(cid:0)N(cid:18) N

e(cid:0)r2=2(cid:27)2

K = N=2.

Then the probability of error (for odd N ) is to leading order

pb ’ (cid:18) N

(N +1)=2(cid:19)f (N +1)=2(1 (cid:0) f )(N (cid:0)1)=2
f [f (1 (cid:0) f )](N (cid:0)1)=2 ’
p(cid:25)N=2

1

The equation pb = 10(cid:0)15 can be written

’ 2N

1

p(cid:25)N=8

(1.42)

f [4f (1 (cid:0) f )](N (cid:0)1)=2: (1.43)

(N (cid:0) 1)=2 ’

log 10(cid:0)15 + log

p(cid:25)N=8

f

log 4f (1 (cid:0) f )

In equation (1.44), the logarithms
can be taken to any base, as long
as it’s the same base throughout.
In equation (1.45), I use base 10.

(1.44)

which may be solved for N iteratively, the (cid:12)rst iteration starting from ^N1 = 68:

( ^N2 (cid:0) 1)=2 ’ (cid:0)15 + 1:7
(cid:0)0:44

= 29:9 ) ^N2 ’ 60:9:

(1.45)

This answer is found to be stable, so N ’ 61 is the blocklength at which
pb ’ 10(cid:0)15.

Solution to exercise 1.6 (p.13).

(a) The probability of block error of the Hamming code is a sum of six terms

{ the probabilities that 2, 3, 4, 5, 6, or 7 errors occur in one block.

pB =

7

Xr=2(cid:18)7

r(cid:19)f r(1 (cid:0) f )7(cid:0)r:

To leading order, this goes as

pB ’(cid:18)7

2(cid:19)f 2 = 21f 2:

(1.46)

(1.47)

(b) The probability of bit error of the Hamming code is smaller than the
probability of block error because a block error rarely corrupts all bits in
the decoded block. The leading-order behaviour is found by considering
the outcome in the most probable case where the noise vector has weight
two. The decoder will erroneously (cid:13)ip a third bit, so that the modi(cid:12)ed
received vector (of length 7) di(cid:11)ers in three bits from the transmitted
vector. That means, if we average over all seven bits, the probability that
a randomly chosen bit is (cid:13)ipped is 3=7 times the block error probability,
to leading order. Now, what we really care about is the probability that

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

18

1 | Introduction to Information Theory

a source bit is (cid:13)ipped. Are parity bits or source bits more likely to be
among these three (cid:13)ipped bits, or are all seven bits equally likely to be
corrupted when the noise vector has weight two? The Hamming code
is in fact completely symmetric in the protection it a(cid:11)ords to the seven
bits (assuming a binary symmetric channel).
[This symmetry can be
proved by showing that the role of a parity bit can be exchanged with
a source bit and the resulting code is still a (7; 4) Hamming code; see
below.] The probability that any one bit ends up corrupted is the same
for all seven bits. So the probability of bit error (for the source bits) is
simply three sevenths of the probability of block error.

pb ’

3
7

pB ’ 9f 2:

(1.48)

Symmetry of the Hamming (7; 4) code

To prove that the (7; 4) code protects all bits equally, we start from the parity-
check matrix

H =2
4

H =2
4

1 1 1 0 1 0 0
0 1 1 1 0 1 0
1 0 1 1 0 0 1

1 1 1 0 1 0 0
0 1 1 1 0 1 0
0 0 1 1 1 0 1

(1.49)

(1.50)

The symmetry among the seven transmitted bits will be easiest to see if we
reorder the seven bits using the permutation (t1t2t3t4t5t6t7) ! (t5t2t3t4t1t6t7).
Then we can rewrite H thus:

Now,
if we take any two parity constraints that t satis(cid:12)es and add them
together, we get another parity constraint. For example, row 1 asserts t5 +
t2 + t3 + t1 = even, and row 2 asserts t2 + t3 + t4 + t6 = even, and the sum of
these two constraints is

t5 + 2t2 + 2t3 + t1 + t4 + t6 = even;

(1.51)

we can drop the terms 2t2 and 2t3, since they are even whatever t2 and t3 are;
thus we have derived the parity constraint t5 + t1 + t4 + t6 = even, which we
can if we wish add into the parity-check matrix as a fourth row.
[The set of
vectors satisfying Ht = 0 will not be changed.] We thus de(cid:12)ne

H0 =2
664

1 1 1 0 1 0 0
0 1 1 1 0 1 0
0 0 1 1 1 0 1
1 0 0 1 1 1 0

:

(1.52)

The fourth row is the sum (modulo two) of the top two rows. Notice that the
second, third, and fourth rows are all cyclic shifts of the top row. If, having
added the fourth redundant constraint, we drop the (cid:12)rst constraint, we obtain
a new parity-check matrix H00,

H00 =2
4

0 1 1 1 0 1 0
0 0 1 1 1 0 1
1 0 0 1 1 1 0

(1.53)

which still satis(cid:12)es H00t = 0 for all codewords, and which looks just like
the starting H in (1.50), except that all the columns have shifted along one

3
5 :

3
5 :

3
775

3
5 ;

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.6: Solutions

19

to the right, and the rightmost column has reappeared at the left (a cyclic
permutation of the columns).

This establishes the symmetry among the seven bits. Iterating the above
procedure (cid:12)ve more times, we can make a total of seven di(cid:11)erent H matrices
for the same original code, each of which assigns each bit to a di(cid:11)erent role.

We may also construct the super-redundant seven-row parity-check matrix

for the code,

H000 =

:

(1.54)

1 1 1 0 1 0 0
0 1 1 1 0 1 0
0 0 1 1 1 0 1
1 0 0 1 1 1 0
0 1 0 0 1 1 1
1 0 1 0 0 1 1
1 1 0 1 0 0 1

2

666666664

3

777777775

This matrix is ‘redundant’ in the sense that the space spanned by its rows is
only three-dimensional, not seven.

This matrix is also a cyclic matrix. Every row is a cyclic permutation of

the top row.

Cyclic codes: if there is an ordering of the bits t1 : : : tN such that a linear
code has a cyclic parity-check matrix, then the code is called a cyclic
code.

The codewords of such a code also have cyclic properties: any cyclic
permutation of a codeword is a codeword.

For example, the Hamming (7; 4) code, with its bits ordered as above,
consists of all seven cyclic shifts of the codewords 1110100 and 1011000,
and the codewords 0000000 and 1111111.

Cyclic codes are a cornerstone of the algebraic approach to error-correcting
codes. We won’t use them again in this book, however, as they have been
superceded by sparse-graph codes (Part VI).

Solution to exercise 1.7 (p.13). There are (cid:12)fteen non-zero noise vectors which
give the all-zero syndrome; these are precisely the (cid:12)fteen non-zero codewords
of the Hamming code. Notice that because the Hamming code is linear , the
sum of any two codewords is a codeword.

Graphs corresponding to codes

Solution to exercise 1.9 (p.14). When answering this question, you will prob-
ably (cid:12)nd that it is easier to invent new codes than to (cid:12)nd optimal decoders
for them. There are many ways to design codes, and what follows is just one
possible train of thought. We make a linear block code that is similar to the
(7; 4) Hamming code, but bigger.

Many codes can be conveniently expressed in terms of graphs.

In (cid:12)g-
ure 1.13, we introduced a pictorial representation of the (7; 4) Hamming code.
If we replace that (cid:12)gure’s big circles, each of which shows that the parity of
four particular bits is even, by a ‘parity-check node’ that is connected to the
four bits, then we obtain the representation of the (7; 4) Hamming code by a
bipartite graph as shown in (cid:12)gure 1.20. The 7 circles are the 7 transmitted
bits. The 3 squares are the parity-check nodes (not to be confused with the
3 parity-check bits, which are the three most peripheral circles). The graph
is a ‘bipartite’ graph because its nodes fall into two classes { bits and checks

Figure 1.20. The graph of the
(7; 4) Hamming code. The 7
circles are the bit nodes and the 3
squares are the parity-check
nodes.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

20

1 | Introduction to Information Theory

{ and there are edges only between nodes in di(cid:11)erent classes. The graph and
the code’s parity-check matrix (1.30) are simply related to each other: each
parity-check node corresponds to a row of H and each bit node corresponds to
a column of H; for every 1 in H, there is an edge between the corresponding
pair of nodes.

Having noticed this connection between linear codes and graphs, one way
to invent linear codes is simply to think of a bipartite graph. For example,
a pretty bipartite graph can be obtained from a dodecahedron by calling the
vertices of the dodecahedron the parity-check nodes, and putting a transmitted
bit on each edge in the dodecahedron. This construction de(cid:12)nes a parity-
check matrix in which every column has weight 2 and every row has weight 3.
[The weight of a binary vector is the number of 1s it contains.]

This code has N = 30 bits, and it appears to have Mapparent = 20 parity-
check constraints. Actually, there are only M = 19 independent constraints;
the 20th constraint is redundant (that is, if 19 constraints are satis(cid:12)ed, then
the 20th is automatically satis(cid:12)ed); so the number of source bits is K =
N (cid:0) M = 11. The code is a (30; 11) code.
It is hard to (cid:12)nd a decoding algorithm for this code, but we can estimate
its probability of error by (cid:12)nding its lowest-weight codewords. If we (cid:13)ip all
the bits surrounding one face of the original dodecahedron, then all the parity
checks will be satis(cid:12)ed; so the code has 12 codewords of weight 5, one for each
face. Since the lowest-weight codewords have weight 5, we say that the code
has distance d = 5; the (7; 4) Hamming code had distance 3 and could correct
all single bit-(cid:13)ip errors. A code with distance 5 can correct all double bit-(cid:13)ip
errors, but there are some triple bit-(cid:13)ip errors that it cannot correct. So the
error probability of this code, assuming a binary symmetric channel, will be
dominated, at least for low noise levels f , by a term of order f 3, perhaps
something like

12(cid:18)5

3(cid:19)f 3(1 (cid:0) f )27:

(1.55)

Of course, there is no obligation to make codes whose graphs can be rep-
resented on a plane, as this one can; the best linear codes, which have simple
graphical descriptions, have graphs that are more tangled, as illustrated by
the tiny (16; 4) code of (cid:12)gure 1.22.

Furthermore, there is no reason for sticking to linear codes; indeed some
nonlinear codes { codes whose codewords cannot be de(cid:12)ned by a linear equa-
tion like Ht = 0 { have very good properties. But the encoding and decoding
of a nonlinear code are even trickier tasks.

Solution to exercise 1.10 (p.14).
code and decoding it with syndrome decoding.
bits, then the number of possible error patterns of weight up to two is

First let’s assume we are making a linear
If there are N transmitted

2(cid:19) +(cid:18)N
(cid:18)N

0(cid:19):
1(cid:19) +(cid:18)N

(1.56)

For N = 14, that’s 91 + 14 + 1 = 106 patterns. Now, every distinguishable
error pattern must give rise to a distinct syndrome; and the syndrome is a
list of M bits, so the maximum possible number of syndromes is 2M . For a
(14; 8) code, M = 6, so there are at most 26 = 64 syndromes. The number of
possible error patterns of weight up to two, 106, is bigger than the number of
syndromes, 64, so we can immediately rule out the possibility that there is a
(14; 8) code that is 2-error-correcting.

Figure 1.21. The graph de(cid:12)ning
the (30; 11) dodecahedron code.
The circles are the 30 transmitted
bits and the triangles are the 20
parity checks. One parity check is
redundant.

Figure 1.22. Graph of a rate-1/4
low-density parity-check code
(Gallager code) with blocklength
N = 16, and M = 12 parity-check
constraints. Each white circle
represents a transmitted bit. Each
bit participates in j = 3
constraints, represented by
squares. The edges between nodes
were placed at random. (See
Chapter 47 for more.)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

1.6: Solutions

21

The same counting argument works (cid:12)ne for nonlinear codes too. When
the decoder receives r = t + n, his aim is to deduce both t and n from r. If
it is the case that the sender can select any transmission t from a code of size
St, and the channel can select any noise vector from a set of size Sn, and those
two selections can be recovered from the received bit string r, which is one of
at most 2N possible strings, then it must be the case that

So, for a (N; K) two-error-correcting code, whether linear or nonlinear,

StSn (cid:20) 2N :

2K(cid:20)(cid:18)N

2(cid:19) +(cid:18)N

1(cid:19) +(cid:18)N

0(cid:19)(cid:21) (cid:20) 2N :

(1.57)

(1.58)

Solution to exercise 1.11 (p.14). There are various strategies for making codes
that can correct multiple errors, and I strongly recommend you think out one
or two of them for yourself.

If your approach uses a linear code, e.g., one with a collection of M parity
checks, it is helpful to bear in mind the counting argument given in the previous
exercise, in order to anticipate how many parity checks, M , you might need.
Examples of codes that can correct any two errors are the (30; 11) dodeca-
hedron code on page 20, and the (15; 6) pentagonful code to be introduced on
p.221. Further simple ideas for making codes that can correct multiple errors
from codes that can correct only one error are discussed in section 13.7.

Solution to exercise 1.12 (p.16). The probability of error of R2
order,

3 is, to leading

pb(R2

3) ’ 3 [pb(R3)]2 = 3(3f 2)2 + (cid:1)(cid:1)(cid:1) = 27f 4 + (cid:1)(cid:1)(cid:1) ;

(1.59)

whereas the probability of error of R9 is dominated by the probability of (cid:12)ve
(cid:13)ips,

pb(R9) ’(cid:18)9

5(cid:19)f 5(1 (cid:0) f )4 ’ 126f 5 + (cid:1)(cid:1)(cid:1) :

(1.60)

The R2
tors of weight four that cause it to make a decoding error.

3 decoding procedure is therefore suboptimal, since there are noise vec-

It has the advantage, however, of requiring smaller computational re-
sources: only memorization of three bits, and counting up to three, rather
than counting up to nine.

This simple code illustrates an important concept. Concatenated codes
are widely used in practice because concatenation allows large codes to be
implemented using simple encoding and decoding hardware. Some of the best
known practical codes are concatenated codes.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2

Probability, Entropy, and Inference

This chapter, and its sibling, Chapter 8, devote some time to notation. Just
as the White Knight distinguished between the song, the name of the song,
and what the name of the song was called (Carroll, 1998), we will sometimes
need to be careful to distinguish between a random variable, the value of the
random variable, and the proposition that asserts that the random variable
has a particular value. In any particular chapter, however, I will use the most
simple and friendly notation possible, at the risk of upsetting pure-minded
readers. For example, if something is ‘true with probability 1’, I will usually
simply say that it is ‘true’.

2.1 Probabilities and ensembles

An ensemble X is a triple (x;AX ;PX ), where the outcome x is the value
of a random variable, which takes on one of a set of possible values,
AX = fa1; a2; : : : ; ai; : : : ; aIg, having probabilities PX = fp1; p2; : : : ; pIg,
with P (x = ai) = pi, pi (cid:21) 0 and Pai2AX
The name A is mnemonic for ‘alphabet’. One example of an ensemble is a
letter that is randomly selected from an English document. This ensemble is
shown in (cid:12)gure 2.1. There are twenty-seven possible letters: a{z, and a space
character ‘-’.

P (x = ai) = 1.

Abbreviations. Briefer notation will sometimes be used. For example,

P (x = ai) may be written as P (ai) or P (x).

Probability of a subset. If T is a subset of AX then:

P (T ) = P (x2 T ) = Xai2T

P (x = ai):

(2.1)

For example,
fa; e; i; o; ug, then

if we de(cid:12)ne V to be vowels from (cid:12)gure 2.1, V =

P (V ) = 0:06 + 0:09 + 0:06 + 0:07 + 0:03 = 0:31:

(2.2)

A joint ensemble XY is an ensemble in which each outcome is an ordered

pair x; y with x 2 AX = fa1; : : : ; aIg and y 2 AY = fb1; : : : ; bJg.
We call P (x; y) the joint probability of x and y.
Commas are optional when writing ordered pairs, so xy , x; y.
N.B. In a joint ensemble XY the two variables are not necessarily inde-
pendent.

22

i

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

ai

pi

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{

0.0575
0.0128
0.0263
0.0285
0.0913
0.0173
0.0133
0.0313
0.0599
0.0006
0.0084
0.0335
0.0235
0.0596
0.0689
0.0192
0.0008
0.0508
0.0567
0.0706
0.0334
0.0069
0.0119
0.0073
0.0164
0.0007
0.1928

a
b
c
d
e
f
g

h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{

Figure 2.1. Probability
distribution over the 27 outcomes
for a randomly selected letter in
an English language document
(estimated from The Frequently
Asked Questions Manual for
Linux ). The picture shows the
probabilities by the areas of white
squares.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.1: Probabilities and ensembles

23

Figure 2.2. The probability
distribution over the 27(cid:2)27
possible bigrams xy in an English
language document, The
Frequently Asked Questions
Manual for Linux.

x

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{

a b c d e f g h i j k l m n o p q r s t u v w x y z { y

Marginal probability. We can obtain the marginal probability P (x) from

the joint probability P (x; y) by summation:

P (x = ai) (cid:17) Xy2AY

P (x = ai; y):

(2.3)

Similarly, using briefer notation, the marginal probability of y is:

P (y) (cid:17) Xx2AX

P (x; y):

(2.4)

Conditional probability

P (x = ai j y = bj) (cid:17)

P (x = ai; y = bj)

P (y = bj)

if P (y = bj) 6= 0.

(2.5)

[If P (y = bj) = 0 then P (x = ai j y = bj) is unde(cid:12)ned.]
We pronounce P (x = ai j y = bj) ‘the probability that x equals ai, given
y equals bj’.

Example 2.1. An example of a joint ensemble is the ordered pair XY consisting
of two successive letters in an English document. The possible outcomes
are ordered pairs such as aa, ab, ac, and zz; of these, we might expect
ab and ac to be more probable than aa and zz. An estimate of the
joint probability distribution for two neighbouring characters is shown
graphically in (cid:12)gure 2.2.

This joint ensemble has the special property that its two marginal dis-
tributions, P (x) and P (y), are identical. They are both equal to the
monogram distribution shown in (cid:12)gure 2.1.

From this joint ensemble P (x; y) we can obtain conditional distributions,
P (y j x) and P (xj y), by normalizing the rows and columns, respectively
((cid:12)gure 2.3). The probability P (y j x = q) is the probability distribution
of the second letter given that the (cid:12)rst letter is a q. As you can see in
(cid:12)gure 2.3a, the two most probable values for the second letter y given

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

24

x
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{

2 | Probability, Entropy, and Inference

Figure 2.3. Conditional
probability distributions. (a)
P (y j x): Each row shows the
conditional distribution of the
second letter, y, given the (cid:12)rst
letter, x, in a bigram xy. (b)
P (xj y): Each column shows the
conditional distribution of the
(cid:12)rst letter, x, given the second
letter, y.

x
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{

a b c d e f g h i j k l m n o p q r s t u v w x y z { y

a b c d e f g h i j k l m n o p q r s t u v w x y z { y

(a) P (y j x)

(b) P (xj y)

that the (cid:12)rst letter x is q are u and -. (The space is common after q
because the source document makes heavy use of the word FAQ.)
The probability P (xj y = u) is the probability distribution of the (cid:12)rst
letter x given that the second letter y is a u. As you can see in (cid:12)gure 2.3b
the two most probable values for x given y = u are n and o.

Rather than writing down the joint probability directly, we often de(cid:12)ne an
ensemble in terms of a collection of conditional probabilities. The following
rules of probability theory will be useful. (H denotes assumptions on which
the probabilities are based.)

Product rule { obtained from the de(cid:12)nition of conditional probability:

P (x; y jH) = P (xj y;H)P (y jH) = P (y j x;H)P (xjH):

(2.6)

This rule is also known as the chain rule.

Sum rule { a rewriting of the marginal probability de(cid:12)nition:

(2.7)

(2.8)

(2.9)

:

(2.10)

P (xj y;H)P (y jH)

P (xjH)

P (xj y;H)P (y jH)
Py0 P (xj y0;H)P (y0 jH)

P (xjH) = Xy
= Xy

P (x; y jH)

P (xj y;H)P (y jH):

Bayes’ theorem { obtained from the product rule:

P (y j x;H) =

=

Independence. Two random variables X and Y are independent (sometimes

written X?Y ) if and only if

P (x; y) = P (x)P (y):

(2.11)

Exercise 2.2.[1, p.40] Are the random variables X and Y in the joint ensemble

of (cid:12)gure 2.2 independent?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.2: The meaning of probability

25

I said that we often de(cid:12)ne an ensemble in terms of a collection of condi-

tional probabilities. The following example illustrates this idea.

Example 2.3. Jo has a test for a nasty disease. We denote Jo’s state of health

by the variable a and the test result by b.

a = 1
a = 0

Jo has the disease
Jo does not have the disease.

(2.12)

The result of the test is either ‘positive’ (b = 1) or ‘negative’ (b = 0);
the test is 95% reliable:
in 95% of cases of people who really have the
disease, a positive result is returned, and in 95% of cases of people who
do not have the disease, a negative result is obtained. The (cid:12)nal piece of
background information is that 1% of people of Jo’s age and background
have the disease.

OK { Jo has the test, and the result is positive. What is the probability
that Jo has the disease?

Solution. We write down all the provided probabilities. The test reliability
speci(cid:12)es the conditional probability of b given a:

P (b = 1j a = 1) = 0:95
P (b = 0j a = 1) = 0:05

P (b = 1j a = 0) = 0:05
P (b = 0j a = 0) = 0:95;

(2.13)

and the disease prevalence tells us about the marginal probability of a:

P (a = 1) = 0:01

P (a = 0) = 0:99:

(2.14)

From the marginal P (a) and the conditional probability P (bj a) we can deduce
the joint probability P (a; b) = P (a)P (bj a) and any other probabilities we are
interested in. For example, by the sum rule, the marginal probability of b = 1
{ the probability of getting a positive result { is

P (b = 1) = P (b = 1j a = 1)P (a = 1) + P (b = 1j a = 0)P (a = 0):

(2.15)

Jo has received a positive result b = 1 and is interested in how plausible it is
that she has the disease (i.e., that a = 1). The man in the street might be
duped by the statement ‘the test is 95% reliable, so Jo’s positive result implies
that there is a 95% chance that Jo has the disease’, but this is incorrect. The
correct solution to an inference problem is found using Bayes’ theorem.

P (a = 1j b = 1) =

=

P (b = 1j a = 1)P (a = 1)

P (b = 1j a = 1)P (a = 1) + P (b = 1j a = 0)P (a = 0)

0:95 (cid:2) 0:01

0:95 (cid:2) 0:01 + 0:05 (cid:2) 0:99

= 0:16:

(2.16)

(2.17)

(2.18)

So in spite of the positive result, the probability that Jo has the disease is only
16%.
2

2.2 The meaning of probability

Probabilities can be used in two ways.

Probabilities can describe frequencies of outcomes in random experiments,
but giving noncircular de(cid:12)nitions of the terms ‘frequency’ and ‘random’ is a
challenge { what does it mean to say that the frequency of a tossed coin’s

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

26

2 | Probability, Entropy, and Inference

Box 2.4. The Cox axioms.
If a set of beliefs satisfy these
axioms then they can be mapped
onto probabilities satisfying
P (false) = 0, P (true) = 1,
0 (cid:20) P (x) (cid:20) 1, and the rules of
probability:

and

P (x) = 1 (cid:0) P (x),

P (x; y) = P (xj y)P (y):

Notation. Let ‘the degree of belief in proposition x’ be denoted by B(x). The
negation of x (not-x) is written x. The degree of belief in a condi-
tional proposition, ‘x, assuming proposition y to be true’, is represented
by B(xj y).

Axiom 1. Degrees of belief can be ordered; if B(x) is ‘greater’ than B(y), and

B(y) is ‘greater’ than B(z), then B(x) is ‘greater’ than B(z).

[Consequence: beliefs can be mapped onto real numbers.]

Axiom 2. The degree of belief in a proposition x and its negation x are related.

There is a function f such that

B(x) = f [B(x)]:

Axiom 3. The degree of belief in a conjunction of propositions x; y (x and y) is
related to the degree of belief in the conditional proposition xj y and the
degree of belief in the proposition y. There is a function g such that

B(x; y) = g [B(xj y); B(y)] :

coming up heads is 1/2? If we say that this frequency is the average fraction of
heads in long sequences, we have to de(cid:12)ne ‘average’; and it is hard to de(cid:12)ne
‘average’ without using a word synonymous to probability! I will not attempt
to cut this philosophical knot.

Probabilities can also be used, more generally, to describe degrees of be-
lief in propositions that do not involve random variables { for example ‘the
probability that Mr. S. was the murderer of Mrs. S., given the evidence’ (he
either was or wasn’t, and it’s the jury’s job to assess how probable it is that he
was); ‘the probability that Thomas Je(cid:11)erson had a child by one of his slaves’;
‘the probability that Shakespeare’s plays were written by Francis Bacon’; or,
to pick a modern-day example, ‘the probability that a particular signature on
a particular cheque is genuine’.

The man in the street is happy to use probabilities in both these ways, but
some books on probability restrict probabilities to refer only to frequencies of
outcomes in repeatable random experiments.

Nevertheless, degrees of belief can be mapped onto probabilities if they sat-
isfy simple consistency rules known as the Cox axioms (Cox, 1946) ((cid:12)gure 2.4).
Thus probabilities can be used to describe assumptions, and to describe in-
ferences given those assumptions. The rules of probability ensure that if two
people make the same assumptions and receive the same data then they will
draw identical conclusions. This more general use of probability to quantify
beliefs is known as the Bayesian viewpoint. It is also known as the subjective
interpretation of probability, since the probabilities depend on assumptions.
Advocates of a Bayesian approach to data modelling and pattern recognition
do not view this subjectivity as a defect, since in their view,

you cannot do inference without making assumptions.

In this book it will from time to time be taken for granted that a Bayesian
approach makes sense, but the reader is warned that this is not yet a globally
held view { the (cid:12)eld of statistics was dominated for most of the 20th century
by non-Bayesian methods in which probabilities are allowed to describe only
random variables. The big di(cid:11)erence between the two approaches is that

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.3: Forward probabilities and inverse probabilities

27

Bayesians also use probabilities to describe inferences.

2.3 Forward probabilities and inverse probabilities

Probability calculations often fall into one of two categories:
forward prob-
ability and inverse probability. Here is an example of a forward probability
problem:
Exercise 2.4.[2, p.40] An urn contains K balls, of which B are black and W =
K (cid:0) B are white. Fred draws a ball at random from the urn and replaces
it, N times.

(a) What is the probability distribution of the number of times a black

ball is drawn, nB?

(b) What is the expectation of nB? What is the variance of nB? What
is the standard deviation of nB? Give numerical answers for the
cases N = 5 and N = 400, when B = 2 and K = 10.

Forward probability problems involve a generative model that describes a pro-
cess that is assumed to give rise to some data; the task is to compute the
probability distribution or expectation of some quantity that depends on the
data. Here is another example of a forward probability problem:
Exercise 2.5.[2, p.40] An urn contains K balls, of which B are black and W =
K (cid:0) B are white. We de(cid:12)ne the fraction fB (cid:17) B=K. Fred draws N
times from the urn, exactly as in exercise 2.4, obtaining nB blacks, and
computes the quantity

z =

(nB (cid:0) fBN )2
N fB(1 (cid:0) fB)

:

(2.19)

What is the expectation of z? In the case N = 5 and fB = 1=5, what
is the probability distribution of z? What is the probability that z < 1?
[Hint: compare z with the quantities computed in the previous exercise.]

Like forward probability problems, inverse probability problems involve a
generative model of a process, but instead of computing the probability distri-
bution of some quantity produced by the process, we compute the conditional
probability of one or more of the unobserved variables in the process, given
the observed variables. This invariably requires the use of Bayes’ theorem.
Example 2.6. There are eleven urns labelled by u 2 f0; 1; 2; : : : ; 10g, each con-
taining ten balls. Urn u contains u black balls and 10 (cid:0) u white balls.
Fred selects an urn u at random and draws N times with replacement
from that urn, obtaining nB blacks and N (cid:0) nB whites. Fred’s friend,
Bill, looks on. If after N = 10 draws nB = 3 blacks have been drawn,
what is the probability that the urn Fred is using is urn u, from Bill’s
point of view? (Bill doesn’t know the value of u.)

Solution. The joint probability distribution of the random variables u and nB
can be written

P (u; nB j N ) = P (nB j u; N )P (u):

(2.20)

From the joint probability of u and nB, we can obtain the conditional

distribution of u given nB:

P (uj nB; N ) =

=

P (u; nB j N )
P (nB j N )
P (nB j u; N )P (u)

P (nB j N )

:

(2.21)

(2.22)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28

2 | Probability, Entropy, and Inference

Figure 2.5. Joint probability of u
and nB for Bill and Fred’s urn
problem, after N = 10 draws.

0.3
0.25
0.2
0.15
0.1
0.05
0

0

1

2

3

4

5
u

6

7

8

9 10

u

0
1
2
3
4
5
6
7
8
9
10

P (uj nB = 3; N )
0
0.063
0.22
0.29
0.24
0.13
0.047
0.0099
0.00086
0.0000096
0

Figure 2.6. Conditional
probability of u given nB = 3 and
N = 10.

u

0
1
2
3
4
5
6
7
8
9
10

0 1 2 3 4 5 6 7 8 9 10 nB

The marginal probability of u is P (u) = 1
11 for all u. You wrote down the
probability of nB given u and N , P (nB j u; N ), when you solved exercise 2.4
(p.27). [You are doing the highly recommended exercises, aren’t you?] If we
de(cid:12)ne fu (cid:17) u=10 then

P (nB j u; N ) =(cid:18) N

nB(cid:19)f nB

u (1 (cid:0) fu)N(cid:0)nB :

(2.23)

What about the denominator, P (nB j N )? This is the marginal probability of
nB, which we can obtain using the sum rule:

P (nB j N ) =Xu

P (u; nB j N ) =Xu

P (u)P (nB j u; N ):

(2.24)

So the conditional probability of u given nB is

P (uj nB; N ) =

=

P (u)P (nB j u; N )
P (nB j N )
11(cid:18) N
1
1

P (nB j N )

nB(cid:19)f nB

u (1 (cid:0) fu)N(cid:0)nB :

(2.25)

(2.26)

This conditional distribution can be found by normalizing column 3 of
(cid:12)gure 2.5 and is shown in (cid:12)gure 2.6. The normalizing constant, the marginal
probability of nB, is P (nB = 3j N = 10) = 0:083. The posterior probability
(2.26) is correct for all u, including the end-points u = 0 and u = 10, where
fu = 0 and fu = 1 respectively. The posterior probability that u = 0 given
nB = 3 is equal to zero, because if Fred were drawing from urn 0 it would be
impossible for any black balls to be drawn. The posterior probability that
u = 10 is also zero, because there are no white balls in that urn. The other
hypotheses u = 1; u = 2, : : : u = 9 all have non-zero posterior probability.
2

Terminology of inverse probability

In inverse probability problems it is convenient to give names to the proba-
bilities appearing in Bayes’ theorem. In equation (2.25), we call the marginal
probability P (u) the prior probability of u, and P (nB j u; N ) is called the like-
lihood of u. It is important to note that the terms likelihood and probability
are not synonyms. The quantity P (nB j u; N ) is a function of both nB and
u. For (cid:12)xed u, P (nB j u; N ) de(cid:12)nes a probability over nB. For (cid:12)xed nB,
P (nB j u; N ) de(cid:12)nes the likelihood of u.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.3: Forward probabilities and inverse probabilities

29

Never say ‘the likelihood of the data’. Always say ‘the likelihood
of the parameters’. The likelihood function is not a probability
distribution.

(If you want to mention the data that a likelihood function is associated with,
you may say ‘the likelihood of the parameters given the data’.)

The conditional probability P (uj nB; N ) is called the posterior probability
of u given nB. The normalizing constant P (nB j N ) has no u-dependence so its
value is not important if we simply wish to evaluate the relative probabilities
of the alternative hypotheses u. However, in most data-modelling problems
of any complexity, this quantity becomes important, and it is given various
names: P (nB j N ) is known as the evidence or the marginal likelihood.

If (cid:18) denotes the unknown parameters, D denotes the data, and H denotes

the overall hypothesis space, the general equation:

is written:

P ((cid:18) j D;H) =

P (D j (cid:18);H)P ((cid:18) jH)

P (D jH)

posterior =

likelihood (cid:2) prior

evidence

:

(2.27)

(2.28)

Inverse probability and prediction

Example 2.6 (continued). Assuming again that Bill has observed nB = 3 blacks
in N = 10 draws, let Fred draw another ball from the same urn. What
is the probability that the next drawn ball is a black? [You should make
use of the posterior probabilities in (cid:12)gure 2.6.]

Solution. By the sum rule,

P (ballN+1 is black j nB; N ) =Xu

P (ballN+1 is black j u; nB; N )P (uj nB; N ):
(2.29)
Since the balls are drawn with replacement from the chosen urn, the proba-
bility P (ballN+1 is black j u; nB; N ) is just fu = u=10, whatever nB and N are.
So

P (ballN+1 is black j nB; N ) =Xu

fuP (uj nB; N ):

(2.30)

Using the values of P (uj nB; N ) given in (cid:12)gure 2.6 we obtain
P (ballN+1 is black j nB = 3; N = 10) = 0:333:

2

(2.31)

Comment. Notice the di(cid:11)erence between this prediction obtained using prob-
ability theory, and the widespread practice in statistics of making predictions
by (cid:12)rst selecting the most plausible hypothesis (which here would be that the
urn is urn u = 3) and then making the predictions assuming that hypothesis
to be true (which would give a probability of 0.3 that the next ball is black).
The correct prediction is the one that takes into account the uncertainty by
marginalizing over the possible values of the hypothesis u. Marginalization
here leads to slightly more moderate, less extreme predictions.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

30

2 | Probability, Entropy, and Inference

Inference as inverse probability

Now consider the following exercise, which has the character of a simple sci-
enti(cid:12)c investigation.

Example 2.7. Bill tosses a bent coin N times, obtaining a sequence of heads
and tails. We assume that the coin has a probability fH of coming up
heads; we do not know fH. If nH heads have occurred in N tosses, what
is the probability distribution of fH? (For example, N might be 10, and
nH might be 3; or, after a lot more tossing, we might have N = 300 and
nH = 29.) What is the probability that the N +1th outcome will be a
head, given nH heads in N tosses?

Unlike example 2.6 (p.27), this problem has a subjective element. Given a
restricted de(cid:12)nition of probability that says ‘probabilities are the frequencies
of random variables’, this example is di(cid:11)erent from the eleven-urns example.
Whereas the urn u was a random variable, the bias fH of the coin would not
normally be called a random variable. It is just a (cid:12)xed but unknown parameter
that we are interested in. Yet don’t the two examples 2.6 and 2.7 seem to have
an essential similarity? [Especially when N = 10 and nH = 3!]

To solve example 2.7, we have to make an assumption about what the bias
of the coin fH might be. This prior probability distribution over fH, P (fH),
corresponds to the prior over u in the eleven-urns problem. In that example,
the helpful problem de(cid:12)nition speci(cid:12)ed P (u). In real life, we have to make
assumptions in order to assign priors; these assumptions will be subjective,
and our answers will depend on them. Exactly the same can be said for the
other probabilities in our generative model too. We are assuming, for example,
that the balls are drawn from an urn independently; but could there not be
correlations in the sequence because Fred’s ball-drawing action is not perfectly
random? Indeed there could be, so the likelihood function that we use depends
on assumptions too. In real data modelling problems, priors are subjective and
so are likelihoods.

Here P (f ) denotes a probability
density, rather than a probability
distribution.

We are now using P () to denote probability densities over continuous vari-
ables as well as probabilities over discrete variables and probabilities of logical
propositions. The probability that a continuous variable v lies between values
a dv P (v). P (v)dv is dimensionless.
The density P (v) is a dimensional quantity, having dimensions inverse to the
dimensions of v { in contrast to discrete probabilities, which are dimensionless.
Don’t be surprised to see probability densities greater than 1. This is normal,

a and b (where b > a) is de(cid:12)ned to be R b
and nothing is wrong, as long as R b

Conditional and joint probability densities are de(cid:12)ned in just the same way as
conditional and joint probabilities.

a dv P (v) (cid:20) 1 for any interval (a; b).

. Exercise 2.8.[2 ] Assuming a uniform prior on fH, P (fH) = 1, solve the problem
posed in example 2.7 (p.30). Sketch the posterior distribution of fH and
compute the probability that the N +1th outcome will be a head, for

(a) N = 3 and nH = 0;
(b) N = 3 and nH = 2;
(c) N = 10 and nH = 3;
(d) N = 300 and nH = 29.

You will (cid:12)nd the beta integral useful:

Z 1

0

dpa pFa

a (1 (cid:0) pa)Fb =

(cid:0)(Fa + 1)(cid:0)(Fb + 1)

(cid:0)(Fa + Fb + 2)

=

Fa!Fb!

(Fa + Fb + 1)!

:

(2.32)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.3: Forward probabilities and inverse probabilities

31

You may also (cid:12)nd it instructive to look back at example 2.6 (p.27) and
equation (2.31).

People sometimes confuse assigning a prior distribution to an unknown pa-
rameter such as fH with making an initial guess of the value of the parameter.
But the prior over fH, P (fH ), is not a simple statement like ‘initially, I would
guess fH = 1/2’. The prior is a probability density over fH which speci(cid:12)es the
prior degree of belief that fH lies in any interval (f; f + (cid:14)f ). It may well be
the case that our prior for fH is symmetric about 1/2, so that the mean of fH
under the prior is 1/2. In this case, the predictive distribution for the (cid:12)rst toss
x1 would indeed be

P (x1 = head) =Z dfH P (fH)P (x1 = headj fH) =Z dfH P (fH)fH = 1/2:

(2.33)
But the prediction for subsequent tosses will depend on the whole prior dis-
tribution, not just its mean.

Data compression and inverse probability

Consider the following task.

Example 2.9. Write a computer program capable of compressing binary (cid:12)les

like this one:

0000000000000000000010010001000000100000010000000000000000000000000000000000001010000000000000110000
1000000000010000100000000010000000000000000000000100000000000000000100000000011000001000000011000100
0000000001001000000000010001000000000000000011000000000000000000000000000010000000000000000100000000

The string shown contains n1 = 29 1s and n0 = 271 0s.

Intuitively, compression works by taking advantage of the predictability of a
(cid:12)le. In this case, the source of the (cid:12)le appears more likely to emit 0s than
1s. A data compression program that compresses this (cid:12)le must, implicitly or
explicitly, be addressing the question ‘What is the probability that the next
character in this (cid:12)le is a 1?’

Do you think this problem is similar in character to example 2.7 (p.30)?
I do. One of the themes of this book is that data compression and data
modelling are one and the same, and that they should both be addressed, like
the urn of example 2.6, using inverse probability. Example 2.9 is solved in
Chapter 6.

The likelihood principle

Please solve the following two exercises.

A

B

Example 2.10. Urn A contains three balls: one black, and two white; urn B
contains three balls:
two black, and one white. One of the urns is
selected at random and one ball is drawn. The ball is black. What is
the probability that the selected urn is urn A?

Figure 2.7. Urns for example 2.10.

Example 2.11. Urn A contains (cid:12)ve balls: one black, two white, one green and
one pink; urn B contains (cid:12)ve hundred balls: two hundred black, one
hundred white, 50 yellow, 40 cyan, 30 sienna, 25 green, 25 silver, 20
gold, and 10 purple.
[One (cid:12)fth of A’s balls are black; two-(cid:12)fths of B’s
are black.] One of the urns is selected at random and one ball is drawn.
The ball is black. What is the probability that the urn is urn A?

p

g

s

...
...
...

c

g p

y

Figure 2.8. Urns for example 2.11.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

32

2 | Probability, Entropy, and Inference

ai

pi

h(pi)

i

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
-

.0575
.0128
.0263
.0285
.0913
.0173
.0133
.0313
.0599
.0006
.0084
.0335
.0235
.0596
.0689
.0192
.0008
.0508
.0567
.0706
.0334
.0069
.0119
.0073
.0164
.0007
.1928

4.1
6.3
5.2
5.1
3.5
5.9
6.2
5.0
4.1
10.7
6.9
4.9
5.4
4.1
3.9
5.7
10.3
4.3
4.1
3.8
4.9
7.2
6.4
7.1
5.9
10.4
2.4

4.1

What do you notice about your solutions? Does each answer depend on the
detailed contents of each urn?

The details of the other possible outcomes and their probabilities are ir-
relevant. All that matters is the probability of the outcome that actually
happened (here, that the ball drawn was black) given the di(cid:11)erent hypothe-
ses. We need only to know the likelihood, i.e., how the probability of the data
that happened varies with the hypothesis. This simple rule about inference is
known as the likelihood principle.

The likelihood principle: given a generative model for data d given
parameters (cid:18), P (dj (cid:18)), and having observed a particular outcome
d1, all inferences and predictions should depend only on the function
P (d1 j (cid:18)).

In spite of the simplicity of this principle, many classical statistical methods
violate it.

2.4 De(cid:12)nition of entropy and related functions

The Shannon information content of an outcome x is de(cid:12)ned to be

h(x) = log2

1

P (x)

:

(2.34)

It is measured in bits. [The word ‘bit’ is also used to denote a variable
whose value is 0 or 1; I hope context will always make clear which of the
two meanings is intended.]

In the next few chapters, we will establish that the Shannon information
content h(ai) is indeed a natural measure of the information content
of the event x = ai. At that point, we will shorten the name of this
quantity to ‘the information content’.

The fourth column in table 2.9 shows the Shannon information content
of the 27 possible outcomes when a random character is picked from
an English document. The outcome x = z has a Shannon information
content of 10.4 bits, and x = e has an information content of 3.5 bits.

The entropy of an ensemble X is de(cid:12)ned to be the average Shannon in-

formation content of an outcome:

H(X) (cid:17) Xx2AX

P (x) log

1

P (x)

;

convention for P (x) = 0

with the
lim(cid:18)!0+ (cid:18) log 1=(cid:18) = 0.
Like the information content, entropy is measured in bits.

that

0 (cid:2) log 1=0 (cid:17) 0,

(2.35)

since

When it is convenient, we may also write H(X) as H(p), where p is
the vector (p1; p2; : : : ; pI). Another name for the entropy of X is the
uncertainty of X.

Example 2.12. The entropy of a randomly selected letter in an English docu-
ment is about 4.11 bits, assuming its probability is as given in table 2.9.
We obtain this number by averaging log 1=pi (shown in the fourth col-
umn) under the probability distribution pi (shown in the third column).

pi log2

1
pi

Xi

Table 2.9. Shannon information
contents of the outcomes a{z.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.5: Decomposability of the entropy

33

We now note some properties of the entropy function.

(cid:15) H(X) (cid:21) 0 with equality i(cid:11) pi = 1 for one i. [‘i(cid:11)’ means ‘if and only if’.]
(cid:15) Entropy is maximized if p is uniform:

H(X) (cid:20) log(jAXj) with equality i(cid:11) pi = 1=jAXj for all i.

(2.36)

Notation: the vertical bars ‘j (cid:1) j’ have two meanings. If AX is a set, jAXj
denotes the number of elements in AX; if x is a number, then jxj is the
absolute value of x.

The redundancy measures the fractional di(cid:11)erence between H(X) and its max-
imum possible value, log(jAXj).
The redundancy of X is:

1 (cid:0)

H(X)
log jAXj

:

(2.37)

We won’t make use of ‘redundancy’ in this book, so I have not assigned
a symbol to it.

The joint entropy of X; Y is:

H(X; Y ) = Xxy2AXAY

P (x; y) log

1

P (x; y)

:

(2.38)

Entropy is additive for independent random variables:

H(X; Y ) = H(X) + H(Y ) i(cid:11) P (x; y) = P (x)P (y):

(2.39)

Our de(cid:12)nitions for information content so far apply only to discrete probability
distributions over (cid:12)nite sets AX. The de(cid:12)nitions can be extended to in(cid:12)nite
sets, though the entropy may then be in(cid:12)nite. The case of a probability
density over a continuous set is addressed in section 11.3. Further important
de(cid:12)nitions and exercises to do with entropy will come along in section 8.1.

2.5 Decomposability of the entropy

The entropy function satis(cid:12)es a recursive property that can be very useful
when computing entropies. For convenience, we’ll stretch our notation so that
we can write H(X) as H(p), where p is the probability vector associated with
the ensemble X.

Let’s illustrate the property by an example (cid:12)rst. Imagine that a random
variable x 2 f0; 1; 2g is created by (cid:12)rst (cid:13)ipping a fair coin to determine whether
x = 0; then, if x is not 0, (cid:13)ipping a fair coin a second time to determine whether
x is 1 or 2. The probability distribution of x is

P (x = 0) =

1
2

; P (x = 1) =

1
4

; P (x = 2) =

1
4

:

(2.40)

What is the entropy of X? We can either compute it by brute force:

H(X) = 1/2 log 2 + 1/4 log 4 + 1/4 log 4 = 1:5;

(2.41)

or we can use the following decomposition, in which the value of x is revealed
gradually. Imagine (cid:12)rst learning whether x = 0, and then, if x is not 0, learning
which non-zero value is the case. The revelation of whether x = 0 or not entails

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

34

2 | Probability, Entropy, and Inference

revealing a binary variable whose probability distribution is f1/2; 1/2g. This
revelation has an entropy H(1/2; 1/2) = 1
2 log 2 = 1 bit. If x is not 0,
we learn the value of the second coin (cid:13)ip. This too is a binary variable whose
probability distribution is f1/2; 1/2g, and whose entropy is 1 bit. We only get
to experience the second revelation half the time, however, so the entropy can
be written:

2 log 2 + 1

H(X) = H(1/2; 1/2) + 1/2 H(1/2; 1/2):

(2.42)

Generalizing, the observation we are making about the entropy of any

probability distribution p = fp1; p2; : : : ; pIg is that
p3
1(cid:0)p1

H(p) = H(p1; 1(cid:0)p1) + (1(cid:0)p1)H(cid:18) p2
1(cid:0)p1

;

; : : : ;

pI

1(cid:0)p1(cid:19) :

(2.43)

When it’s written as a formula, this property looks regrettably ugly; nev-

ertheless it is a simple property and one that you should make use of.
Generalizing further, the entropy has the property for any m that

H(p) = H [(p1 + p2 + (cid:1)(cid:1)(cid:1) + pm); (pm+1 + pm+2 + (cid:1)(cid:1)(cid:1) + pI)]
pm

p1

; : : : ;

(p1 + (cid:1)(cid:1)(cid:1) + pm)(cid:19)

; : : : ;

pI

(pm+1 + (cid:1)(cid:1)(cid:1) + pI)(cid:19) :

(2.44)

+(p1 + (cid:1)(cid:1)(cid:1) + pm)H(cid:18)
+(pm+1 + (cid:1)(cid:1)(cid:1) + pI)H(cid:18)

(p1 + (cid:1)(cid:1)(cid:1) + pm)
pm+1

(pm+1 + (cid:1)(cid:1)(cid:1) + pI)

Example 2.13. A source produces a character x from the alphabet A =
f0; 1; : : : ; 9; a; b; : : : ; zg; with probability 1/3, x is a numeral (0; : : : ; 9);
with probability 1/3, x is a vowel (a; e; i; o; u); and with probability 1/3
it’s one of the 21 consonants. All numerals are equiprobable, and the
same goes for vowels and consonants. Estimate the entropy of X.

Solution.

log 3 + 1

3 (log 10 + log 5 + log 21) = log 3 + 1

3 log 1050 ’ log 30 bits. 2

The ‘ei’ in Leibler is pronounced
the same as in heist.

2.6 Gibbs’ inequality

The relative entropy or Kullback{Leibler divergence between

two
probability distributions P (x) and Q(x) that are de(cid:12)ned over the same
alphabet AX is

DKL(PjjQ) =Xx

P (x) log

P (x)
Q(x)

:

The relative entropy satis(cid:12)es Gibbs’ inequality

DKL(PjjQ) (cid:21) 0

(2.45)

(2.46)

with equality only if P = Q. Note that in general the relative entropy
is not symmetric under interchange of the distributions P and Q:
in
general DKL(PjjQ) 6= DKL(QjjP ), so DKL, although it is sometimes
called the ‘KL distance’, is not strictly a distance. The relative entropy
is important in pattern recognition and neural networks, as well as in
information theory.

Gibbs’ inequality is probably the most important inequality in this book. It,
and many other inequalities, can be proved using the concept of convexity.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.7: Jensen’s inequality for convex functions

35

2.7 Jensen’s inequality for convex functions

The words ‘convex ^’ and ‘concave _’ may be pronounced ‘convex-smile’ and
‘concave-frown’. This terminology has useful redundancy: while one may forget
which way up ‘convex’ and ‘concave’ are, it is harder to confuse a smile with a
frown.

Convex ^ functions. A function f (x) is convex ^ over (a; b) if every chord
of the function lies above the function, as shown in (cid:12)gure 2.10; that is,
for all x1; x2 2 (a; b) and 0 (cid:20) (cid:21) (cid:20) 1,

f ((cid:21)x1 + (1 (cid:0) (cid:21))x2) (cid:20) (cid:21)f (x1) + (1 (cid:0) (cid:21))f (x2):

(2.47)

A function f is strictly convex ^ if, for all x1; x2 2 (a; b), the equality
holds only for (cid:21) = 0 and (cid:21) = 1.

Similar de(cid:12)nitions apply to concave _ and strictly concave _ functions.

(cid:21)f (x1) + (1 (cid:0) (cid:21))f (x2)

f (x(cid:3))

x1

x2

x(cid:3) = (cid:21)x1 + (1 (cid:0) (cid:21))x2

Figure 2.10. De(cid:12)nition of
convexity.

Some strictly convex ^ functions are

(cid:15) x2, ex and e(cid:0)x for all x;
(cid:15) log(1=x) and x log x for x > 0.

x2

e(cid:0)x

log 1
x

x log x

Figure 2.11. Convex ^ functions.

Centre of gravity

-1

0

1

2

3

-1

0

1

2

3

0

1

2

3

0

1

2

3

Jensen’s inequality. If f is a convex ^ function and x is a random variable

then:

E [f (x)] (cid:21) f (E[x]) ;

(2.48)
where E denotes expectation. If f is strictly convex ^ and E [f (x)] =
f (E[x]), then the random variable x is a constant.
Jensen’s inequality can also be rewritten for a concave _ function, with
the direction of the inequality reversed.

A physical version of Jensen’s inequality runs as follows.

If a collection of masses pi are placed on a convex ^ curve f (x)
at locations (xi; f (xi)), then the centre of gravity of those masses,
which is at (E[x];E [f (x)]), lies above the curve.

If this fails to convince you, then feel free to do the following exercise.

Exercise 2.14.[2, p.41] Prove Jensen’s inequality.

Example 2.15. Three squares have average area (cid:22)A = 100 m2. The average of
the lengths of their sides is (cid:22)l = 10 m. What can be said about the size
of the largest of the three squares? [Use Jensen’s inequality.]

Solution. Let x be the length of the side of a square, and let the probability
of x be 1/3; 1/3; 1/3 over the three lengths l1; l2; l3. Then the information that
we have is that E [x] = 10 and E [f (x)] = 100, where f (x) = x2 is the function
mapping lengths to areas. This is a strictly convex ^ function. We notice
that the equality E [f (x)] = f (E[x]) holds, therefore x is a constant, and the
three lengths must all be equal. The area of the largest square is 100 m2. 2

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

36

2 | Probability, Entropy, and Inference

Convexity and concavity also relate to maximization

If f (x) is concave _ and there exists a point at which

@f
@xk

= 0 for all k;

(2.49)

then f (x) has its maximum value at that point.

The converse does not hold: if a concave _ f (x) is maximized at some x
it is not necessarily true that the gradient rf (x) is equal to zero there. For
example, f (x) = (cid:0)jxj is maximized at x = 0 where its derivative is unde(cid:12)ned;
and f (p) = log(p); for a probability p 2 (0; 1), is maximized on the boundary
of the range, at p = 1, where the gradient df (p)=dp = 1.

2.8 Exercises

Sums of random variables

Exercise 2.16.[3, p.41]

(a) Two ordinary dice with faces labelled 1; : : : ; 6 are
thrown. What is the probability distribution of the sum of the val-
ues? What is the probability distribution of the absolute di(cid:11)erence
between the values?

(b) One hundred ordinary dice are thrown. What, roughly, is the prob-
ability distribution of the sum of the values? Sketch the probability
distribution and estimate its mean and standard deviation.

(c) How can two cubical dice be labelled using the numbers
f0; 1; 2; 3; 4; 5; 6g so that when the two dice are thrown the sum
has a uniform probability distribution over the integers 1{12?

(d) Is there any way that one hundred dice could be labelled with inte-
gers such that the probability distribution of the sum is uniform?

Inference problems
Exercise 2.17.[2, p.41] If q = 1 (cid:0) p and a = ln p=q, show that

p =

1

1 + exp((cid:0)a)

:

(2.50)

Sketch this function and (cid:12)nd its relationship to the hyperbolic tangent
function tanh(u) = eu(cid:0)e(cid:0)u
eu+e(cid:0)u .
It will be useful to be (cid:13)uent in base-2 logarithms also. If b = log 2 p=q,
what is p as a function of b?

. Exercise 2.18.[2, p.42] Let x and y be dependent random variables with x a
binary variable taking values in AX = f0; 1g. Use Bayes’ theorem to
show that the log posterior probability ratio for x given y is

log

P (x = 1j y)
P (x = 0j y)

= log

P (y j x = 1)
P (y j x = 0)

+ log

P (x = 1)
P (x = 0)

:

(2.51)

. Exercise 2.19.[2, p.42] Let x, d1 and d2 be random variables such that d1 and
d2 are conditionally independent given a binary variable x. Use Bayes’
theorem to show that the posterior probability ratio for x given fdig is

P (x = 1jfdig)
P (x = 0jfdig)

=

P (d1 j x = 1)
P (d1 j x = 0)

P (d2 j x = 1)
P (d2 j x = 0)

P (x = 1)
P (x = 0)

:

(2.52)

This exercise is intended to help
you think about the central-limit
theorem, which says that if
independent random variables
x1; x2; : : : ; xN have means (cid:22)n and
(cid:12)nite variances (cid:27)2

n, then, in the

has a distribution that tends to a
normal (Gaussian) distribution

limit of large N , the sum Pn xn
with meanPn (cid:22)n and variance
Pn (cid:27)2

n.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.8: Exercises

Life in high-dimensional spaces

Probability distributions and volumes have some unexpected properties in
high-dimensional spaces.

37

Exercise 2.20.[2, p.42] Consider a sphere of radius r in an N -dimensional real
space. Show that the fraction of the volume of the sphere that is in the
surface shell lying at values of the radius between r (cid:0) (cid:15) and r, where
0 < (cid:15) < r, is:

f = 1 (cid:0)(cid:16)1 (cid:0)

(cid:15)

r(cid:17)N

:

(2.53)

Evaluate f for the cases N = 2, N = 10 and N = 1000, with (a) (cid:15)=r = 0:01;
(b) (cid:15)=r = 0:5.

Implication: points that are uniformly distributed in a sphere in N di-
mensions, where N is large, are very likely to be in a thin shell near the
surface.

Expectations and entropies

You are probably familiar with the idea of computing the expectation of a
function of x,

E [f (x)] = hf (x)i =Xx

P (x)f (x):

(2.54)

Maybe you are not so comfortable with computing this expectation in cases
where the function f (x) depends on the probability P (x). The next few ex-
amples address this concern.

Exercise 2.21.[1, p.43] Let pa = 0:1, pb = 0:2, and pc = 0:7.

Let f (a) = 10,

f (b) = 5, and f (c) = 10=7. What is E [f (x)]? What is E [1=P (x)]?
Exercise 2.22.[2, p.43] For an arbitrary ensemble, what is E [1=P (x)]?
. Exercise 2.23.[1, p.43] Let pa = 0:1, pb = 0:2, and pc = 0:7. Let g(a) = 0, g(b) = 1,

and g(c) = 0. What is E [g(x)]?

. Exercise 2.24.[1, p.43] Let pa = 0:1, pb = 0:2, and pc = 0:7. What is the proba-

bility that P (x) 2 [0:15; 0:5]? What is
P (x)

log

P(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)

0:2 (cid:12)(cid:12)(cid:12)(cid:12)

> 0:05(cid:19)?

Exercise 2.25.[3, p.43] Prove the assertion that H(X) (cid:20) log(jAXj) with equal-
ity i(cid:11) pi = 1=jAXj for all i. (jAXj denotes the number of elements in
the set AX.) [Hint: use Jensen’s inequality (2.48); if your (cid:12)rst attempt
to use Jensen does not succeed, remember that Jensen involves both a
random variable and a function, and you have quite a lot of freedom in
choosing these; think about whether your chosen function f should be
convex or concave.]

. Exercise 2.26.[3, p.44] Prove that the relative entropy (equation (2.45)) satis(cid:12)es

DKL(PjjQ) (cid:21) 0 (Gibbs’ inequality) with equality only if P = Q.

. Exercise 2.27.[2 ] Prove that the entropy is indeed decomposable as described

in equations (2.43{2.44).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

38

2 | Probability, Entropy, and Inference

g
(cid:0)(cid:18)
(cid:0)
@
(cid:0)(cid:18)
@R
1(cid:0)g

(cid:0)

@

@R

h
(cid:0)(cid:18)
(cid:0)
@
@R
1(cid:0)h

0

1

2

3

f
(cid:0)
@
1(cid:0)f

. Exercise 2.28.[2, p.45] A random variable x 2 f0; 1; 2; 3g is selected by (cid:13)ipping
a bent coin with bias f to determine whether the outcome is in f0; 1g or
f2; 3g; then either (cid:13)ipping a second bent coin with bias g or a third bent
coin with bias h respectively. Write down the probability distribution
of x. Use the decomposability of the entropy (2.44) to (cid:12)nd the entropy
of X.
[Notice how compact an expression is obtained if you make use
of the binary entropy function H2(x), compared with writing out the
four-term entropy explicitly.] Find the derivative of H(X) with respect
to f . [Hint: dH2(x)=dx = log((1 (cid:0) x)=x).]

. Exercise 2.29.[2, p.45] An unbiased coin is (cid:13)ipped until one head is thrown.
What is the entropy of the random variable x 2 f1; 2; 3; : : :g, the num-
ber of (cid:13)ips? Repeat the calculation for the case of a biased coin with
probability f of coming up heads. [Hint: solve the problem both directly
and by using the decomposability of the entropy (2.43).]

2.9 Further exercises

Forward probability

. Exercise 2.30.[1 ] An urn contains w white balls and b black balls. Two balls
are drawn, one after the other, without replacement. Prove that the
probability that the (cid:12)rst ball is white is equal to the probability that the
second is white.

. Exercise 2.31.[2 ] A circular coin of diameter a is thrown onto a square grid
whose squares are b (cid:2) b. (a < b) What is the probability that the coin
will lie entirely within one square? [Ans: (1 (cid:0) a=b)2]

. Exercise 2.32.[3 ] Bu(cid:11)on’s needle. A needle of length a is thrown onto a plane
covered with equally spaced parallel lines with separation b. What is
the probability that the needle will cross a line? [Ans, if a < b: 2a/(cid:25)b]
[Generalization { Bu(cid:11)on’s noodle: on average, a random curve of length
A is expected to intersect the lines 2A/(cid:25)b times.]

Exercise 2.33.[2 ] Two points are selected at random on a straight line segment
of length 1. What is the probability that a triangle can be constructed
out of the three resulting segments?

Exercise 2.34.[2, p.45] An unbiased coin is (cid:13)ipped until one head is thrown.
What is the expected number of tails and the expected number of heads?

Fred, who doesn’t know that the coin is unbiased, estimates the bias
using ^f (cid:17) h=(h + t), where h and t are the numbers of heads and tails
tossed. Compute and sketch the probability distribution of ^f.

N.B., this is a forward probability problem, a sampling theory problem,
not an inference problem. Don’t use Bayes’ theorem.

Exercise 2.35.[2, p.45] Fred rolls an unbiased six-sided die once per second, not-

ing the occasions when the outcome is a six.

(a) What is the mean number of rolls from one six to the next six?

(b) Between two rolls, the clock strikes one. What is the mean number

of rolls until the next six?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.9: Further exercises

39

(c) Now think back before the clock struck. What is the mean number

of rolls, going back in time, until the most recent six?

(d) What is the mean number of rolls from the six before the clock

struck to the next six?

(e) Is your answer to (d) di(cid:11)erent from your answer to (a)? Explain.

Another version of this exercise refers to Fred waiting for a bus at a
bus-stop in Poissonville where buses arrive independently at random (a
Poisson process), with, on average, one bus every six minutes. What is
the average wait for a bus, after Fred arrives at the stop? [6 minutes.] So
what is the time between the two buses, the one that Fred just missed,
and the one that he catches? [12 minutes.] Explain the apparent para-
dox. Note the contrast with the situation in Clockville, where the buses
are spaced exactly 6 minutes apart. There, as you can con(cid:12)rm, the mean
wait at a bus-stop is 3 minutes, and the time between the missed bus
and the next one is 6 minutes.

Conditional probability

. Exercise 2.36.[2 ] You meet Fred. Fred tells you he has two brothers, Alf and

Bob.

What is the probability that Fred is older than Bob?

Fred tells you that he is older than Alf. Now, what is the probability
that Fred is older than Bob? (That is, what is the conditional probability
that F > B given that F > A?)

. Exercise 2.37.[2 ] The inhabitants of an island tell the truth one third of the

time. They lie with probability 2/3.

On an occasion, after one of them made a statement, you ask another
‘was that statement true?’ and he says ‘yes’.

What is the probability that the statement was indeed true?

. Exercise 2.38.[2, p.46] Compare two ways of computing the probability of error
of the repetition code R3, assuming a binary symmetric channel (you
did this once for exercise 1.2 (p.7)) and con(cid:12)rm that they give the same
answer.

Binomial distribution method. Add the probability that all three
bits are (cid:13)ipped to the probability that exactly two bits are (cid:13)ipped.

Sum rule method. Using the sum rule, compute the marginal prob-
ability that r takes on each of the eight possible values, P (r).

[P (r) = Ps P (s)P (rj s).] Then compute the posterior probabil-

ity of s for each of the eight values of r.
[In fact, by symmetry,
only two example cases r = (000) and r = (001) need be consid-
ered.] Notice that some of the inferred bits are better determined
than others. From the posterior probability P (sj r) you can read
out the case-by-case error probability, the probability that the more
probable hypothesis is not correct, P (errorj r). Find the average
error probability using the sum rule,

P (error) =Xr

P (r)P (error j r):

(2.55)

Equation (1.18) gives the
posterior probability of the input
s, given the received vector r.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

40

2 | Probability, Entropy, and Inference

. Exercise 2.39.[3C, p.46] The frequency pn of the nth most frequent word in

English is roughly approximated by

pn ’(cid:26) 0:1

n
0

for n 2 1; : : : ; 12 367
n > 12 367:

(2.56)

[This remarkable 1=n law is known as Zipf’s law, and applies to the word
frequencies of many languages (Zipf, 1949).] If we assume that English
is generated by picking words at random according to this distribution,
what is the entropy of English (per word)? [This calculation can be found
in ‘Prediction and entropy of printed English’, C.E. Shannon, Bell Syst.
Tech. J. 30, pp.50{64 (1950), but, inexplicably, the great man made
numerical errors in it.]

2.10 Solutions

Solution to exercise 2.2 (p.24). No, they are not independent. If they were
then all the conditional distributions P (y j x) would be identical functions of
y, regardless of x (cf. (cid:12)gure 2.3).

Solution to exercise 2.4 (p.27). We de(cid:12)ne the fraction fB (cid:17) B=K.
(a) The number of black balls has a binomial distribution.

P (nB j fB; N ) =(cid:18) N

nB(cid:19)f nB

B (1 (cid:0) fB)N(cid:0)nB :

(b) The mean and variance of this distribution are:

E[nB] = N fB

var[nB] = N fB(1 (cid:0) fB):

(2.57)

(2.58)

(2.59)

These results were derived in example 1.1 (p.1). The standard deviation

of nB is pvar[nB] =pN fB(1 (cid:0) fB).

When B=K = 1=5 and N = 5, the expectation and variance of nB are 1
and 4/5. The standard deviation is 0.89.

When B=K = 1=5 and N = 400, the expectation and variance of nB are
80 and 64. The standard deviation is 8.

Solution to exercise 2.5 (p.27). The numerator of the quantity

z =

(nB (cid:0) fBN )2
N fB(1 (cid:0) fB)

can be recognized as (nB (cid:0) E[nB])2; the denominator is equal to the variance
of nB (2.59), which is by de(cid:12)nition the expectation of the numerator. So the
expectation of z is 1. [A random variable like z, which measures the deviation
of data from the expected value, is sometimes called (cid:31)2 (chi-squared).]

In the case N = 5 and fB = 1=5, N fB is 1, and var[nB] is 4/5. The
numerator has (cid:12)ve possible values, only one of which is smaller than 1: (nB (cid:0)
fBN )2 = 0 has probability P (nB = 1) = 0:4096; so the probability that z < 1
is 0.4096.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.10: Solutions

41

Solution to exercise 2.14 (p.35). We wish to prove, given the property

f ((cid:21)x1 + (1 (cid:0) (cid:21))x2) (cid:20) (cid:21)f (x1) + (1 (cid:0) (cid:21))f (x2);

(2.60)

that, if P pi = 1 and pi (cid:21) 0,
Xi=1

I

pif (xi) (cid:21) f  I
Xi=1

pixi! :

(2.61)

We proceed by recursion, working from the right-hand side. (This proof does
not handle cases where some pi = 0; such details are left to the pedantic
reader.) At the (cid:12)rst line we use the de(cid:12)nition of convexity (2.60) with (cid:21) =

p1
I
i=1 pi

= p1; at the second line, (cid:21) = p2

.

I
i=2 pi

I

pixi! = f p1x1 +

f  I
Xi=1
(cid:20) p1f (x1) +" I
Xi=2
(cid:20) p1f (x1) +" I
Xi=2

pixi!
Xi=2
pi!#
pixi, I
pi#"f  I
Xi=2
Xi=2
pi#"
f (x2) +PI
PI

p2
i=2 pi

PI

i=3 pi
i=2 pi

and so forth.

Solution to exercise 2.16 (p.36).

(2.62)

f  I
Xi=3

pixi, I
Xi=3

pi!# ;

2

(a) For the outcomes f2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12g, the probabilities are P =

36 ; 2
f 1

36 ; 3

36 ; 4

36 ; 5

36 ; 6

36 ; 5

36 ; 4

36 ; 3

36 ; 2

36 ; 1

36g.

(b) The value of one die has mean 3:5 and variance 35=12. So the sum of
one hundred has mean 350 and variance 3500=12 ’ 292, and by the
central-limit theorem the probability distribution is roughly Gaussian
(but con(cid:12)ned to the integers), with this mean and variance.

(c) In order to obtain a sum that has a uniform distribution we have to start
from random variables some of which have a spiky distribution with the
probability mass concentrated at the extremes. The unique solution is
to have one ordinary die and one with faces 6, 6, 6, 0, 0, 0.

(d) Yes, a uniform distribution can be created in several ways, for example

by labelling the rth die with the numbers f0; 1; 2; 3; 4; 5g (cid:2) 6r.

To think about: does this uniform
distribution contradict the
central-limit theorem?

Solution to exercise 2.17 (p.36).

a = ln

p
q

)

p
q

= ea

and q = 1 (cid:0) p gives

p
1 (cid:0) p

)

p =

The hyperbolic tangent is

= ea

ea

ea + 1

=

1

1 + exp((cid:0)a)

:

tanh(a) =

ea (cid:0) e(cid:0)a
ea + e(cid:0)a

(2.63)

(2.64)

(2.65)

(2.66)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42

so

f (a) (cid:17)

=

1

1 + exp((cid:0)a)
2  ea=2 (cid:0) e(cid:0)a=2
1

ea=2 + e(cid:0)a=2

1

=

1 + e(cid:0)a + 1(cid:19)
2(cid:18) 1 (cid:0) e(cid:0)a
+ 1! =

1
2

(tanh(a=2) + 1):

2 | Probability, Entropy, and Inference

(2.67)

In the case b = log2 p=q, we can repeat steps (2.63{2.65), replacing e by 2,

to obtain

p =

1

1 + 2(cid:0)b :

Solution to exercise 2.18 (p.36).

P (xj y) =

P (y j x)P (x)

P (y)

)

) log

P (x = 1j y)
P (x = 0j y)
P (x = 1j y)
P (x = 0j y)

=

P (y j x = 1)
P (y j x = 0)

P (x = 1)
P (x = 0)

= log

P (y j x = 1)
P (y j x = 0)

+ log

P (x = 1)
P (x = 0)

:

(2.68)

(2.69)

(2.70)

(2.71)

Solution to exercise 2.19 (p.36). The conditional independence of d1 and d2
given x means

P (x; d1; d2) = P (x)P (d1 j x)P (d2 j x):

(2.72)

This gives a separation of the posterior probability ratio into a series of factors,
one for each data point, times the prior probability ratio.

P (x = 1jfdig)
P (x = 0jfdig)

=

=

P (fdigj x = 1)
P (fdigj x = 0)
P (d1 j x = 1)
P (d1 j x = 0)

P (x = 1)
P (x = 0)
P (d2 j x = 1)
P (d2 j x = 0)

P (x = 1)
P (x = 0)

:

(2.73)

(2.74)

Life in high-dimensional spaces

Solution to exercise 2.20 (p.37). The volume of a hypersphere of radius r in
N dimensions is in fact

V (r; N ) =

(cid:25)N=2
(N=2)!

rN ;

(2.75)

but you don’t need to know this. For this question all that we need is the
r-dependence, V (r; N ) / rN : So the fractional volume in (r (cid:0) (cid:15); r) is

rN (cid:0) (r (cid:0) (cid:15))N

rN

= 1 (cid:0)(cid:16)1 (cid:0)

(cid:15)

r(cid:17)N

:

(2.76)

The fractional volumes in the shells for the required cases are:

N

2

10

1000

(cid:15)=r = 0:01
(cid:15)=r = 0:5

0.02
0.75

0.096
0.999

0.99996
1 (cid:0) 2(cid:0)1000

Notice that no matter how small (cid:15) is, for large enough N essentially all the
probability mass is in the surface shell of thickness (cid:15).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

2.10: Solutions

43

Solution to exercise 2.21 (p.37).
f (b) = 5, and f (c) = 10=7.

pa = 0:1, pb = 0:2, pc = 0:7.

f (a) = 10,

E [f (x)] = 0:1 (cid:2) 10 + 0:2 (cid:2) 5 + 0:7 (cid:2) 10=7 = 3:

For each x, f (x) = 1=P (x), so

Solution to exercise 2.22 (p.37). For general X,

E [1=P (x)] = E [f (x)] = 3:
P (x)1=P (x) = Xx2AX

E [1=P (x)] = Xx2AX

1 = jAXj:

(2.77)

(2.78)

(2.79)

Solution to exercise 2.23 (p.37). pa = 0:1, pb = 0:2, pc = 0:7. g(a) = 0, g(b) = 1,
and g(c) = 0.

Solution to exercise 2.24 (p.37).

E [g(x)] = pb = 0:2:

P (P (x)2 [0:15; 0:5]) = pb = 0:2:
log

> 0:05(cid:19) = pa + pc = 0:8:

P (x)

P(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)

0:2 (cid:12)(cid:12)(cid:12)(cid:12)

Solution to exercise 2.25 (p.37). This type of question can be approached in
two ways: either by di(cid:11)erentiating the function to be maximized, (cid:12)nding the
maximum, and proving it is a global maximum; this strategy is somewhat
risky since it is possible for the maximum of a function to be at the boundary
of the space, at a place where the derivative is not zero. Alternatively, a
carefully chosen inequality can establish the answer. The second method is
much neater.

Proof by di(cid:11)erentiation (not the recommended method).
Since it is slightly
easier to di(cid:11)erentiate ln 1=p than log2 1=p, we temporarily de(cid:12)ne H(X) to be
measured using natural logarithms, thus scaling it down by a factor of log 2 e.

H(X) = Xi

pi ln

1
pi

@H(X)

@pi

= ln

1
pi (cid:0) 1

(2.83)

(2.84)

we maximize subject to the constraint Pi pi = 1 which can be enforced with

a Lagrange multiplier:

(2.80)

(2.81)

(2.82)

(2.85)

(2.86)

(2.87)

(2.88)

At a maximum,

@pi

G(p) (cid:17) H(X) + (cid:21) Xi

pi (cid:0) 1!

@G(p)

= ln

1
pi (cid:0) 1 + (cid:21):

ln

1
pi (cid:0) 1 + (cid:21) = 0

) ln

1
pi

= 1 (cid:0) (cid:21);

so all the pi are equal. That this extremum is indeed a maximum is established
by (cid:12)nding the curvature:

which is negative de(cid:12)nite.

@2G(p)
@pi@pj

= (cid:0)

1
pi

(cid:14)ij;

(2.89)

2

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

44

2 | Probability, Entropy, and Inference

Proof using Jensen’s inequality (recommended method).
the inequality.

First a reminder of

If f is a convex ^ function and x is a random variable then:

E [f (x)] (cid:21) f (E[x]) :

If f is strictly convex ^ and E [f (x)] = f (E[x]), then the random
variable x is a constant (with probability 1).

The secret of a proof using Jensen’s inequality is to choose the right func-

tion and the right random variable. We could de(cid:12)ne

f (u) = log

1
u

= (cid:0) log u

(2.90)

(which is a convex function) and think of H(X) =P pi log 1

as the mean of
f (u) where u = P (x), but this would not get us there { it would give us an
inequality in the wrong direction. If instead we de(cid:12)ne

pi

then we (cid:12)nd:

u = 1=P (x)

H(X) = (cid:0)E [f (1=P (x))] (cid:20) (cid:0)f (E[1=P (x)]) ;

now we know from exercise 2.22 (p.37) that E[1=P (x)] = jAXj, so

H(X) (cid:20) (cid:0)f (jAXj) = log jAXj:

(2.91)

(2.92)

(2.93)

Equality holds only if the random variable u = 1=P (x) is a constant, which
means P (x) is a constant for all x.
2

Solution to exercise 2.26 (p.37).

DKL(PjjQ) =Xx

P (x) log

P (x)
Q(x)

:

(2.94)

We prove Gibbs’ inequality using Jensen’s inequality. Let f (u) = log 1=u and
u = Q(x)
P (x)

. Then

DKL(PjjQ) = E[f (Q(x)=P (x))]
Q(x)

(cid:21) f Xx

P (x)

P (x)! = log(cid:18)

with equality only if u = Q(x)

P (x) is a constant, that is, if Q(x) = P (x).

1

Px Q(x)(cid:19) = 0;

(2.95)

(2.96)

2

In the above proof the expectations were with respect to
Second solution.
the probability distribution P (x). A second solution method uses Jensen’s
inequality with Q(x) instead. We de(cid:12)ne f (u) = u log u and let u = P (x)
Q(x) .
Then

Q(x)

DKL(PjjQ) = Xx
(cid:21) f Xx

P (x)
Q(x)

log

P (x)
Q(x)

=Xx
Q(x)! = f (1) = 0;

P (x)

Q(x)

Q(x)f(cid:18) P (x)

Q(x)(cid:19) (2.97)

(2.98)

with equality only if u = P (x)

Q(x) is a constant, that is, if Q(x) = P (x).

2

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45

2.10: Solutions

Solution to exercise 2.28 (p.38).

H(X) = H2(f ) + f H2(g) + (1 (cid:0) f )H2(h):

(2.99)

Solution to exercise 2.29 (p.38). The probability that there are x(cid:0) 1 tails and
then one head (so we get the (cid:12)rst head on the xth toss) is

P (x) = (1 (cid:0) f )x(cid:0)1f:

(2.100)

If the (cid:12)rst toss is a tail, the probability distribution for the future looks just
like it did before we made the (cid:12)rst toss. Thus we have a recursive expression
for the entropy:

Rearranging,

H(X) = H2(f ) + (1 (cid:0) f )H(X):

H(X) = H2(f )=f:

(2.101)

(2.102)

Solution to exercise 2.34 (p.38). The probability of the number of tails t is

P (t) =(cid:18) 1

2(cid:19)t 1

2

for t (cid:21) 0:

(2.103)

The expected number of heads is 1, by de(cid:12)nition of the problem. The expected
number of tails is

which may be shown to be 1 in a variety of ways. For example, since the
situation after one tail is thrown is equivalent to the opening situation, we can
write down the recurrence relation

E[t] =

1
2

(1 + E[t]) +

1
2

0 ) E[t] = 1:

(2.105)

0

0.2

0.4

0.6

0.8

1
^f

The probability distribution of the ‘estimator’ ^f = 1=(1 + t), given that
f = 1=2, is plotted in (cid:12)gure 2.12. The probability of ^f is simply the probability
of the corresponding value of t.

Figure 2.12. The probability
distribution of the estimator
^f = 1=(1 + t), given that f = 1=2.

Solution to exercise 2.35 (p.38).

(a) The mean number of rolls from one six to the next six is six (assuming
we start counting rolls after the (cid:12)rst of the two sixes). The probability
that the next six occurs on the rth roll is the probability of not getting
a six for r (cid:0) 1 rolls multiplied by the probability of then getting a six:

P (r1 = r) =(cid:18) 5

6(cid:19)r(cid:0)1 1

6

;

for r 2 f1; 2; 3; : : :g.

(2.106)

This probability distribution of the number of rolls, r, may be called an
exponential distribution, since

P (r1 = r) = e(cid:0)(cid:11)r=Z;

(2.107)

where (cid:11) = ln(6=5), and Z is a normalizing constant.

(b) The mean number of rolls from the clock until the next six is six.

(c) The mean number of rolls, going back in time, until the most recent six

is six.

E[t] =

1Xt=0

t(cid:18) 1
2(cid:19)t 1

2

;

(2.104)

P ( ^f)

0.5

0.4

0.3

0.2

0.1

0

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

46

2 | Probability, Entropy, and Inference

(d) The mean number of rolls from the six before the clock struck to the six
after the clock struck is the sum of the answers to (b) and (c), less one,
that is, eleven.

(e) Rather than explaining the di(cid:11)erence between (a) and (d), let me give
another hint.
Imagine that the buses in Poissonville arrive indepen-
dently at random (a Poisson process), with, on average, one bus every
six minutes. Imagine that passengers turn up at bus-stops at a uniform
rate, and are scooped up by the bus without delay, so the interval be-
tween two buses remains constant. Buses that follow gaps bigger than
six minutes become overcrowded. The passengers’ representative com-
plains that two-thirds of all passengers found themselves on overcrowded
buses. The bus operator claims, ‘no, no { only one third of our buses
are overcrowded’. Can both these claims be true?

Solution to exercise 2.38 (p.39).

Binomial distribution method. From the solution to exercise 1.2, pB =

3f 2(1 (cid:0) f ) + f 3.

Sum rule method. The marginal probabilities of the eight values of r are

illustrated by:

P (r = 000) = 1/2(1 (cid:0) f )3 + 1/2f 3;

P (r = 001) = 1/2f (1 (cid:0) f )2 + 1/2f 2(1 (cid:0) f ) = 1/2f (1 (cid:0) f ):

The posterior probabilities are represented by

P (s = 1j r = 000) =

f 3

(1 (cid:0) f )3 + f 3

(2.108)

(2.109)

(2.110)

0.15

0.1

0.05

0

0

5

10

15

20

Figure 2.13. The probability
distribution of the number of rolls
r1 from one 6 to the next (falling
solid line),

P (r1 = r) =(cid:18) 5

6(cid:19)r(cid:0)1 1

6

;

and the probability distribution
(dashed line) of the number of
rolls from the 6 before 1pm to the
next 6, rtot,

P (rtot = r) = r (cid:18) 5

6(cid:19)r(cid:0)1(cid:18) 1
6(cid:19)2

:

The probability P (r1 > 6) is
about 1/3; the probability
P (rtot > 6) is about 2/3. The
mean of r1 is 6, and the mean of
rtot is 11.

and

P (s = 1j r = 001) =

= f:

(2.111)

The probabilities of error in these representative cases are thus

(1 (cid:0) f )f 2

f (1 (cid:0) f )2 + f 2(1 (cid:0) f )

P (errorj r = 000) =

f 3

(1 (cid:0) f )3 + f 3

(2.112)

and

(2.113)
Notice that while the average probability of error of R3 is about 3f 2, the
probability (given r) that any particular bit is wrong is either about f 3
or f .

P (errorj r = 001) = f:

The average error probability, using the sum rule, is

P (error) = Xr

P (r)P (error j r)

= 2[1/2(1 (cid:0) f )3 + 1/2f 3]

So

f 3

(1 (cid:0) f )3 + f 3 + 6[1/2f (1 (cid:0) f )]f:

P (error) = f 3 + 3f 2(1 (cid:0) f ):

Solution to exercise 2.39 (p.40). The entropy is 9.7 bits per word.

The (cid:12)rst two terms are for the
cases r = 000 and 111; the
remaining 6 are for the other
outcomes, which share the same
probability of occurring and
identical error probability, f .

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 3

If you are eager to get on to information theory, data compression, and noisy
channels, you can skip to Chapter 4. Data compression and data modelling
are intimately connected, however, so you’ll probably want to come back to
this chapter by the time you get to Chapter 6. Before reading Chapter 3, it
might be good to look at the following exercises.

. Exercise 3.1.[2, p.59] A die is selected at random from two twenty-faced dice
on which the symbols 1{10 are written with nonuniform frequency as
follows.

Symbol

1

Number of faces of die A 6
Number of faces of die B 3

2

4
3

3

3
2

4

2
2

5

1
2

6

1
2

7

1
2

8

1
2

9

1
1

10

0
1

The randomly chosen die is rolled 7 times, with the following outcomes:

What is the probability that the die is die A?

5, 3, 9, 3, 8, 4, 7.

. Exercise 3.2.[2, p.59] Assume that there is a third twenty-faced die, die C, on
which the symbols 1{20 are written once each. As above, one of the
three dice is selected at random and rolled 7 times, giving the outcomes:
3, 5, 4, 8, 3, 9, 7.
What is the probability that the die is (a) die A, (b) die B, (c) die C?

Exercise 3.3.[3, p.48] Inferring a decay constant

Unstable particles are emitted from a source and decay at a distance
x, a real number that has an exponential probability distribution with
characteristic length (cid:21). Decay events can be observed only if they occur
in a window extending from x = 1 cm to x = 20 cm. N decays are
observed at locations fx1; : : : ; xNg. What is (cid:21)?

*

* * * *

*

*

* *

x

. Exercise 3.4.[3, p.55] Forensic evidence

Two people have left traces of their own blood at the scene of a crime. A
suspect, Oliver, is tested and found to have type ‘O’ blood. The blood
groups of the two traces are found to be of type ‘O’ (a common type
in the local population, having frequency 60%) and of type ‘AB’ (a rare
type, with frequency 1%). Do these data (type ‘O’ and ‘AB’ blood were
found at scene) give evidence in favour of the proposition that Oliver
was one of the two people present at the crime?

47

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3

More about Inference

It is not a controversial statement that Bayes’ theorem provides the correct
language for describing the inference of a message communicated over a noisy
channel, as we used it in Chapter 1 (p.6). But strangely, when it comes to
other inference problems, the use of Bayes’ theorem is not so widespread.

3.1 A (cid:12)rst inference problem

When I was an undergraduate in Cambridge, I was privileged to receive su-
pervisions from Steve Gull. Sitting at his desk in a dishevelled o(cid:14)ce in St.
John’s College, I asked him how one ought to answer an old Tripos question
(exercise 3.3):

Unstable particles are emitted from a source and decay at a
distance x, a real number that has an exponential probability dis-
tribution with characteristic length (cid:21). Decay events can be ob-
served only if they occur in a window extending from x = 1 cm
to x = 20 cm. N decays are observed at locations fx1; : : : ; xNg.
What is (cid:21)?

*

* * * *

*

*

* *

x

I had scratched my head over this for some time. My education had provided
me with a couple of approaches to solving such inference problems: construct-
ing ‘estimators’ of the unknown parameters; or ‘(cid:12)tting’ the model to the data,
or to a processed version of the data.

Since the mean of an unconstrained exponential distribution is (cid:21), it seemed

reasonable to examine the sample mean (cid:22)x =Pn xn=N and see if an estimator
^(cid:21) could be obtained from it. It was evident that the estimator ^(cid:21) = (cid:22)x(cid:0)1 would
be appropriate for (cid:21) (cid:28) 20 cm, but not for cases where the truncation of the
distribution at the right-hand side is signi(cid:12)cant; with a little ingenuity and
the introduction of ad hoc bins, promising estimators for (cid:21) (cid:29) 20 cm could be
constructed. But there was no obvious estimator that would work under all
conditions.

to a histogram derived from the data. I was stuck.

Nor could I (cid:12)nd a satisfactory approach based on (cid:12)tting the density P (xj (cid:21))
What is the general solution to this problem and others like it? Is it
always necessary, when confronted by a new inference problem, to grope in the
dark for appropriate ‘estimators’ and worry about (cid:12)nding the ‘best’ estimator
(whatever that means)?

48

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.1: A (cid:12)rst inference problem

49

0.25

0.2

0.15

0.1

0.05

0

0.2

0.15

0.1

0.05

0

P(x|lambda=2)
P(x|lambda=5)
P(x|lambda=10)

Figure 3.1. The probability
density P (xj (cid:21)) as a function of x.

2

4

6

8

10 12 14 16 18 20

x

P(x=3|lambda)
P(x=5|lambda)
P(x=12|lambda)

1

10

100

(cid:21)

Figure 3.2. The probability
density P (xj (cid:21)) as a function of (cid:21),
for three di(cid:11)erent values of x.
When plotted this way round, the
function is known as the likelihood
of (cid:21). The marks indicate the
three values of (cid:21), (cid:21) = 2; 5; 10, that
were used in the preceding (cid:12)gure.

Steve wrote down the probability of one data point, given (cid:21):

where

P (xj (cid:21)) =(cid:26) 1
Z((cid:21)) =Z 20

dx 1

1

(cid:21) e(cid:0)x=(cid:21)=Z((cid:21)) 1 < x < 20
otherwise
0

(cid:21) e(cid:0)x=(cid:21) =(cid:16)e(cid:0)1=(cid:21) (cid:0) e(cid:0)20=(cid:21)(cid:17) :

This seemed obvious enough. Then he wrote Bayes’ theorem:

(3.1)

(3.2)

(3.3)

(3.4)

3

2

1

P ((cid:21)jfx1; : : : ; xNg) =

/

P (fxgj (cid:21))P ((cid:21))

P (fxg)
1

((cid:21)Z((cid:21)))N exp(cid:16)(cid:0)PN

1 xn=(cid:21)(cid:17) P ((cid:21)):

Suddenly, the straightforward distribution P (fx1; : : : ; xNgj (cid:21)), de(cid:12)ning the
probability of the data given the hypothesis (cid:21), was being turned on its head
so as to de(cid:12)ne the probability of a hypothesis given the data. A simple (cid:12)gure
showed the probability of a single data point P (xj (cid:21)) as a familiar function of x,
for di(cid:11)erent values of (cid:21) ((cid:12)gure 3.1). Each curve was an innocent exponential,
normalized to have area 1. Plotting the same function as a function of (cid:21) for a
(cid:12)xed value of x, something remarkable happens: a peak emerges ((cid:12)gure 3.2).
To help understand these two points of view of the one function, (cid:12)gure 3.3
shows a surface plot of P (xj (cid:21)) as a function of x and (cid:21).
n=1 =
f1:5; 2; 3; 4; 5; 12g, the likelihood function P (fxgj (cid:21)) is the product of the N
functions of (cid:21), P (xn j (cid:21)) ((cid:12)gure 3.4).

For a dataset consisting of several points, e.g., the six points fxgN

1.4e-06

1.2e-06

1e-06

8e-07

6e-07

4e-07

2e-07

0

1

10

100

1

1.5

x

2

2.5

100

10

(cid:21)

1

Figure 3.3. The probability
density P (xj (cid:21)) as a function of x
and (cid:21). Figures 3.1 and 3.2 are
vertical sections through this
surface.

Figure 3.4. The likelihood function
in the case of a six-point dataset,
P (fxg = f1:5; 2; 3; 4; 5; 12gj (cid:21)), as
a function of (cid:21).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

50

3 | More about Inference

If you have any di(cid:14)culty
understanding this chapter I
recommend ensuring you are
happy with exercises 3.1 and 3.2
(p.47) then noting their similarity
to exercise 3.3.

Steve summarized Bayes’ theorem as embodying the fact that

what you know about (cid:21) after the data arrive is what you knew
before [P ((cid:21))], and what the data told you [P (fxgj (cid:21))].

Probabilities are used here to quantify degrees of belief. To nip possible
confusion in the bud, it must be emphasized that the hypothesis (cid:21) that cor-
rectly describes the situation is not a stochastic variable, and the fact that the
Bayesian uses a probability distribution P does not mean that he thinks of
the world as stochastically changing its nature between the states described
by the di(cid:11)erent hypotheses. He uses the notation of probabilities to represent
his beliefs about the mutually exclusive micro-hypotheses (here, values of (cid:21)),
of which only one is actually true. That probabilities can denote degrees of
belief, given assumptions, seemed reasonable to me.

The posterior probability distribution (3.4) represents the unique and com-
plete solution to the problem. There is no need to invent ‘estimators’; nor do
we need to invent criteria for comparing alternative estimators with each other.
Whereas orthodox statisticians o(cid:11)er twenty ways of solving a problem, and an-
other twenty di(cid:11)erent criteria for deciding which of these solutions is the best,
Bayesian statistics only o(cid:11)ers one answer to a well-posed problem.

Assumptions in inference

Our inference is conditional on our assumptions [for example, the prior P ((cid:21))].
Critics view such priors as a di(cid:14)culty because they are ‘subjective’, but I don’t
see how it could be otherwise. How can one perform inference without making
assumptions? I believe that it is of great value that Bayesian methods force
one to make these tacit assumptions explicit.

First, once assumptions are made, the inferences are objective and unique,
reproducible with complete agreement by anyone who has the same informa-
tion and makes the same assumptions. For example, given the assumptions
listed above, H, and the data D, everyone will agree about the posterior prob-
ability of the decay length (cid:21):

P ((cid:21)j D;H) =

P (D j (cid:21);H)P ((cid:21)jH)

P (D jH)

:

(3.5)

Second, when the assumptions are explicit, they are easier to criticize, and
easier to modify { indeed, we can quantify the sensitivity of our inferences to
the details of the assumptions. For example, we can note from the likelihood
curves in (cid:12)gure 3.2 that in the case of a single data point at x = 5, the
likelihood function is less strongly peaked than in the case x = 3; the details
of the prior P ((cid:21)) become increasingly important as the sample mean (cid:22)x gets
closer to the middle of the window, 10.5. In the case x = 12, the likelihood
function doesn’t have a peak at all { such data merely rule out small values
of (cid:21), and don’t give any information about the relative probabilities of large
values of (cid:21). So in this case, the details of the prior at the small{(cid:21) end of things
are not important, but at the large{(cid:21) end, the prior is important.

Third, when we are not sure which of various alternative assumptions is
the most appropriate for a problem, we can treat this question as another
inference task. Thus, given data D, we can compare alternative assumptions
H using Bayes’ theorem:

P (H j D; I) =

P (D jH; I)P (H j I)

P (D j I)

;

(3.6)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.2: The bent coin

51

where I denotes the highest assumptions, which we are not questioning.

Fourth, we can take into account our uncertainty regarding such assump-
tions when we make subsequent predictions. Rather than choosing one partic-
ular assumption H(cid:3), and working out our predictions about some quantity t,
P (tj D;H(cid:3); I), we obtain predictions that take into account our uncertainty
about H by using the sum rule:
P (tj D; I) =XH

P (tj D;H; I)P (H j D; I):

(3.7)

This is another contrast with orthodox statistics, in which it is conventional
to ‘test’ a default model, and then, if the test ‘accepts the model’ at some
‘signi(cid:12)cance level’, to use exclusively that model to make predictions.

Steve thus persuaded me that

probability theory reaches parts that ad hoc methods cannot reach.

Let’s look at a few more examples of simple inference problems.

3.2 The bent coin

A bent coin is tossed F times; we observe a sequence s of heads and tails
(which we’ll denote by the symbols a and b). We wish to know the bias of
the coin, and predict the probability that the next toss will result in a head.
We (cid:12)rst encountered this task in example 2.7 (p.30), and we will encounter it
again in Chapter 6, when we discuss adaptive data compression. It is also the
original inference problem studied by Thomas Bayes in his essay published in
1763.

As in exercise 2.8 (p.30), we will assume a uniform prior distribution and
obtain a posterior distribution by multiplying by the likelihood. A critic might
object, ‘where did this prior come from?’ I will not claim that the uniform
prior is in any way fundamental; indeed we’ll give examples of nonuniform
priors later. The prior is a subjective assumption. One of the themes of this
book is:

you can’t do inference { or data compression { without making
assumptions.

We give the name H1 to our assumptions.

[We’ll be introducing an al-
ternative set of assumptions in a moment.] The probability, given p a, that F
tosses result in a sequence s that contains fFa; Fbg counts of the two outcomes
is
(3.8)
[For example, P (s = aabaj pa; F = 4;H1) = papa(1 (cid:0) pa)pa:] Our (cid:12)rst model
assumes a uniform prior distribution for pa,

P (sj pa; F;H1) = pFa

a (1 (cid:0) pa)Fb:

P (pa jH1) = 1;

pa 2 [0; 1]

(3.9)

and pb (cid:17) 1 (cid:0) pa.

Inferring unknown parameters

Given a string of length F of which Fa are as and Fb are bs, we are interested
in (a) inferring what pa might be; (b) predicting whether the next character is

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

52

3 | More about Inference

an a or a b. [Predictions are always expressed as probabilities. So ‘predicting
whether the next character is an a’ is the same as computing the probability
that the next character is an a.]

Assuming H1 to be true, the posterior probability of pa, given a string s

of length F that has counts fFa; Fbg, is, by Bayes’ theorem,
P (sj pa; F;H1)P (pa jH1)

P (pa j s; F;H1) =

P (sj F;H1)

:

(3.10)

The factor P (sj pa; F;H1), which, as a function of pa, is known as the likeli-
hood function, was given in equation (3.8); the prior P (p a jH1) was given in
equation (3.9). Our inference of pa is thus:

pFa
a (1 (cid:0) pa)Fb
P (sj F;H1)
The normalizing constant is given by the beta integral

P (pa j s; F;H1) =

:

(3.11)

0

dpa pFa

P (sj F;H1) =Z 1
Exercise 3.5.[2, p.59] Sketch the posterior probability P (pa j s = aba; F = 3).

a (1 (cid:0) pa)Fb =

(cid:0)(Fa + 1)(cid:0)(Fb + 1)

What is the most probable value of pa (i.e., the value that maximizes
the posterior probability density)? What is the mean value of p a under
this distribution?

(cid:0)(Fa + Fb + 2)

(Fa + Fb + 1)!

:

(3.12)

Fa!Fb!

=

the

Answer
same
P (pa j s = bbb; F = 3).

From inferences to predictions

questions

for

the

posterior

probability

Our prediction about the next toss, the probability that the next toss is an a,
is obtained by integrating over pa. This has the e(cid:11)ect of taking into account
our uncertainty about pa when making predictions. By the sum rule,

P (aj s; F ) = Z dpa P (aj pa)P (pa j s; F ):

The probability of an a given pa is simply pa, so

a (1 (cid:0) pa)Fb
pFa
P (sj F )

P (aj s; F ) =Z dpa pa
= Z dpa
(1 (cid:0) pa)Fb
P (sj F )
(Fa + Fb + 2)!(cid:21)(cid:30)(cid:20)
= (cid:20) (Fa + 1)! Fb!

pFa+1

a

which is known as Laplace’s rule.

(3.13)

(3.14)

(3.15)

Fa! Fb!

(Fa + Fb + 1)!(cid:21) =

Fa + 1

Fa + Fb + 2

;

(3.16)

3.3 The bent coin and model comparison

Imagine that a scientist introduces another theory for our data. He asserts
that the source is not really a bent coin but is really a perfectly formed die with
one face painted heads (‘a’) and the other (cid:12)ve painted tails (‘b’). Thus the
parameter pa, which in the original model, H1, could take any value between
0 and 1, is according to the new hypothesis, H0, not a free parameter at all;
rather, it is equal to 1=6. [This hypothesis is termed H0 so that the su(cid:14)x of
each model indicates its number of free parameters.]
How can we compare these two models in the light of data? We wish to

infer how probable H1 is relative to H0.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.3: The bent coin and model comparison

53

Model comparison as inference

In order to perform model comparison, we write down Bayes’ theorem again,
but this time with a di(cid:11)erent argument on the left-hand side. We wish to
know how probable H1 is given the data. By Bayes’ theorem,

P (H1 j s; F ) =

P (sj F;H1)P (H1)

P (sj F )

Similarly, the posterior probability of H0 is

P (H0 j s; F ) =

P (sj F;H0)P (H0)

P (sj F )

:

:

(3.17)

(3.18)

The normalizing constant in both cases is P (sj F ), which is the total proba-
bility of getting the observed data. If H1 and H0 are the only models under
consideration, this probability is given by the sum rule:

P (sj F ) = P (sj F;H1)P (H1) + P (sj F;H0)P (H0):

(3.19)

To evaluate the posterior probabilities of the hypotheses we need to assign
values to the prior probabilities P (H1) and P (H0); in this case, we might
set these to 1/2 each. And we need to evaluate the data-dependent terms
P (sj F;H1) and P (sj F;H0). We can give names to these quantities. The
quantity P (sj F;H1) is a measure of how much the data favour H1, and we
call it the evidence for model H1. We already encountered this quantity in
equation (3.10) where it appeared as the normalizing constant of the (cid:12)rst
inference we made { the inference of pa given the data.

How model comparison works: The evidence for a model is
usually the normalizing constant of an earlier Bayesian inference.

We evaluated the normalizing constant for model H1 in (3.12). The evi-
dence for model H0 is very simple because this model has no parameters to
infer. De(cid:12)ning p0 to be 1=6, we have

P (sj F;H0) = pFa

0 (1 (cid:0) p0)Fb:

Thus the posterior probability ratio of model H1 to model H0 is

P (H1 j s; F )
P (H0 j s; F )

=

=

P (sj F;H1)P (H1)
P (sj F;H0)P (H0)
(Fa + Fb + 1)!(cid:30) pFa

Fa!Fb!

0 (1 (cid:0) p0)Fb:

(3.20)

(3.21)

(3.22)

Some values of this posterior probability ratio are illustrated in table 3.5. The
(cid:12)rst (cid:12)ve lines illustrate that some outcomes favour one model, and some favour
the other. No outcome is completely incompatible with either model. With
small amounts of data (six tosses, say) it is typically not the case that one of
the two models is overwhelmingly more probable than the other. But with
more data, the evidence against H0 given by any data set with the ratio Fa: Fb
di(cid:11)ering from 1: 5 mounts up. You can’t predict in advance how much data
are needed to be pretty sure which theory is true. It depends what p a is.

The simpler model, H0, since it has no adjustable parameters, is able to
lose out by the biggest margin. The odds may be hundreds to one against it.
The more complex model can never lose out by a large margin; there’s no data
set that is actually unlikely given model H1.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3 | More about Inference

Table 3.5. Outcome of model
comparison between models H1
and H0 for the ‘bent coin’. Model
H0 states that pa = 1=6, pb = 5=6.

Figure 3.6. Typical behaviour of
the evidence in favour of H1 as
bent coin tosses accumulate under
three di(cid:11)erent conditions
(columns 1, 2, 3). Horizontal axis
is the number of tosses, F . The
vertical axis on the left is
ln P (s j F; H1)
P (s j F; H0)
vertical axis shows the values of
P (s j F; H1)
P (s j F; H0)
The three rows show independent
simulated experiments.
(See also (cid:12)gure 3.8, p.60.)

; the right-hand

.

54

F Data (Fa; Fb)

6
6
6
6
6

20
20
20

(5; 1)
(3; 3)
(2; 4)
(1; 5)
(0; 6)

(10; 10)
(3; 17)
(0; 20)

P (H1 j s; F )
P (H0 j s; F )

222.2
2.67
0.71
0.356
0.427

96.5
0.2
1.83

= 1/1.4
= 1/2.8
= 1/2.3

= 1/5

H0 is true
pa = 1=6

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

8
6
4
2
0
-2
-4

8
6
4
2
0
-2
-4

8
6
4
2
0
-2
-4

0

50

0

50

0

50

H1 is true

8
6
4
2
0
-2
-4

8
6
4
2
0
-2
-4

8
6
4
2
0
-2
-4

0

50

0

50

0

50

pa = 0:25

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

8
6
4
2
0
-2
-4

8
6
4
2
0
-2
-4

8
6
4
2
0
-2
-4

0

50

0

50

0

50

pa = 0:5

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

. Exercise 3.6.[2 ] Show that after F tosses have taken place, the biggest value

that the log evidence ratio

log

P (sj F;H1)
P (sj F;H0)

(3.23)

can have scales linearly with F if H1 is more probable, but the log
evidence in favour of H0 can grow at most as log F .

. Exercise 3.7.[3, p.60] Putting your sampling theory hat on, assuming Fa has
not yet been measured, compute a plausible range that the log evidence
ratio might lie in, as a function of F and the true value of p a, and sketch
it as a function of F for pa = p0 = 1=6, pa = 0:25, and pa = 1=2. [Hint:
sketch the log evidence as a function of the random variable F a and work
out the mean and standard deviation of Fa.]

Typical behaviour of the evidence

Figure 3.6 shows the log evidence ratio as a function of the number of tosses,
F , in a number of simulated experiments. In the left-hand experiments, H0
was true. In the right-hand ones, H1 was true, and the value of pa was either
0.25 or 0.5.

We will discuss model comparison more in a later chapter.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

55

3.4: An example of legal evidence

3.4 An example of legal evidence

The following example illustrates that there is more to Bayesian inference than
the priors.

Two people have left traces of their own blood at the scene of a
crime. A suspect, Oliver, is tested and found to have type ‘O’
blood. The blood groups of the two traces are found to be of type
‘O’ (a common type in the local population, having frequency 60%)
and of type ‘AB’ (a rare type, with frequency 1%). Do these data
(type ‘O’ and ‘AB’ blood were found at scene) give evidence in
favour of the proposition that Oliver was one of the two people
present at the crime?

A careless lawyer might claim that the fact that the suspect’s blood type was
found at the scene is positive evidence for the theory that he was present. But
this is not so.

Denote the proposition ‘the suspect and one unknown person were present’
by S. The alternative, (cid:22)S, states ‘two unknown people from the population were
present’. The prior in this problem is the prior probability ratio between the
propositions S and (cid:22)S. This quantity is important to the (cid:12)nal verdict and
would be based on all other available information in the case. Our task here is
just to evaluate the contribution made by the data D, that is, the likelihood
ratio, P (D j S;H)=P (D j (cid:22)S;H). In my view, a jury’s task should generally be to
multiply together carefully evaluated likelihood ratios from each independent
piece of admissible evidence with an equally carefully reasoned prior proba-
bility.
[This view is shared by many statisticians but learned British appeal
judges recently disagreed and actually overturned the verdict of a trial because
the jurors had been taught to use Bayes’ theorem to handle complicated DNA
evidence.]

The probability of the data given S is the probability that one unknown

person drawn from the population has blood type AB:

P (D j S;H) = pAB

(3.24)

(since given S, we already know that one trace will be of type O). The prob-
ability of the data given (cid:22)S is the probability that two unknown people drawn
from the population have types O and AB:

P (D j (cid:22)S;H) = 2 pO pAB:

(3.25)

In these equations H denotes the assumptions that two people were present
and left blood there, and that the probability distribution of the blood groups
of unknown people in an explanation is the same as the population frequencies.

Dividing, we obtain the likelihood ratio:

P (D j S;H)
P (D j (cid:22)S;H)

=

1
2pO

=

1

2 (cid:2) 0:6

= 0:83:

(3.26)

Thus the data in fact provide weak evidence against the supposition that
Oliver was present.

This result may be found surprising, so let us examine it from various
points of view. First consider the case of another suspect, Alberto, who has
type AB. Intuitively, the data do provide evidence in favour of the theory S0

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

56

3 | More about Inference

that this suspect was present, relative to the null hypothesis (cid:22)S. And indeed
the likelihood ratio in this case is:

P (D j S0;H)
P (D j (cid:22)S;H)

=

1

2 pAB

= 50:

(3.27)

Now let us change the situation slightly; imagine that 99% of people are of
blood type O, and the rest are of type AB. Only these two blood types exist
in the population. The data at the scene are the same as before. Consider
again how these data in(cid:13)uence our beliefs about Oliver, a suspect of type
O, and Alberto, a suspect of type AB. Intuitively, we still believe that the
presence of the rare AB blood provides positive evidence that Alberto was
there. But does the fact that type O blood was detected at the scene favour
the hypothesis that Oliver was present? If this were the case, that would mean
that regardless of who the suspect is, the data make it more probable they were
present; everyone in the population would be under greater suspicion, which
would be absurd. The data may be compatible with any suspect of either
blood type being present, but if they provide evidence for some theories, they
must also provide evidence against other theories.

Here is another way of thinking about this:

imagine that instead of two
people’s blood stains there are ten, and that in the entire local population
of one hundred, there are ninety type O suspects and ten type AB suspects.
Consider a particular type O suspect, Oliver: without any other information,
and before the blood test results come in, there is a one in 10 chance that he
was at the scene, since we know that 10 out of the 100 suspects were present.
We now get the results of blood tests, and (cid:12)nd that nine of the ten stains are
of type AB, and one of the stains is of type O. Does this make it more likely
that Oliver was there? No, there is now only a one in ninety chance that he
was there, since we know that only one person present was of type O.

Maybe the intuition is aided (cid:12)nally by writing down the formulae for the
general case where nO blood stains of individuals of type O are found, and
nAB of type AB, a total of N individuals in all, and unknown people come
from a large population with fractions pO; pAB. (There may be other blood
types too.) The task is to evaluate the likelihood ratio for the two hypotheses:
S, ‘the type O suspect (Oliver) and N(cid:0)1 unknown others left N stains’; and
(cid:22)S, ‘N unknowns left N stains’. The probability of the data under hypothesis
(cid:22)S is just the probability of getting nO; nAB individuals of the two types when
N individuals are drawn at random from the population:

P (nO; nAB j (cid:22)S) =

N !

nO! nAB!

O pnAB
pnO
AB :

(3.28)

In the case of hypothesis S, we need the distribution of the N (cid:0)1 other indi-
viduals:

pnO(cid:0)1
O

pnAB
AB :

(3.29)

P (nO; nAB j S) =

The likelihood ratio is:

(N (cid:0) 1)!

(nO (cid:0) 1)! nAB!

P (nO; nAB j S)
P (nO; nAB j (cid:22)S)

=

nO=N

pO

:

(3.30)

This is an instructive result. The likelihood ratio, i.e. the contribution of
these data to the question of whether Oliver was present, depends simply on
a comparison of the frequency of his blood type in the observed data with the
background frequency in the population. There is no dependence on the counts
of the other types found at the scene, or their frequencies in the population.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.5: Exercises

57

If there are more type O stains than the average number expected under
hypothesis (cid:22)S, then the data give evidence in favour of the presence of Oliver.
Conversely, if there are fewer type O stains than the expected number under
(cid:22)S, then the data reduce the probability of the hypothesis that he was there.
In the special case nO=N = pO, the data contribute no evidence either way,
regardless of the fact that the data are compatible with the hypothesis S.

3.5 Exercises

Exercise 3.8.[2, p.60] The three doors, normal rules.

On a game show, a contestant is told the rules as follows:

There are three doors, labelled 1, 2, 3. A single prize has
been hidden behind one of them. You get to select one door.
Initially your chosen door will not be opened.
Instead, the
gameshow host will open one of the other two doors, and he
will do so in such a way as not to reveal the prize. For example,
if you (cid:12)rst choose door 1, he will then open one of doors 2 and
3, and it is guaranteed that he will choose which one to open
so that the prize will not be revealed.
At this point, you will be given a fresh choice of door: you
can either stick with your (cid:12)rst choice, or you can switch to the
other closed door. All the doors will then be opened and you
will receive whatever is behind your (cid:12)nal choice of door.

Imagine that the contestant chooses door 1 (cid:12)rst; then the gameshow host
opens door 3, revealing nothing behind the door, as promised. Should
the contestant (a) stick with door 1, or (b) switch to door 2, or (c) does
it make no di(cid:11)erence?

Exercise 3.9.[2, p.61] The three doors, earthquake scenario.

Imagine that the game happens again and just as the gameshow host is
about to open one of the doors a violent earthquake rattles the building
and one of the three doors (cid:13)ies open. It happens to be door 3, and it
happens not to have the prize behind it. The contestant had initially
chosen door 1.

Repositioning his toup(cid:19)ee, the host suggests, ‘OK, since you chose door
1 initially, door 3 is a valid door for me to open, according to the rules
of the game; I’ll let door 3 stay open. Let’s carry on as if nothing
happened.’

Should the contestant stick with door 1, or switch to door 2, or does it
make no di(cid:11)erence? Assume that the prize was placed randomly, that
the gameshow host does not know where it is, and that the door (cid:13)ew
open because its latch was broken by the earthquake.

[A similar alternative scenario is a gameshow whose confused host for-
gets the rules, and where the prize is, and opens one of the unchosen
doors at random. He opens door 3, and the prize is not revealed. Should
the contestant choose what’s behind door 1 or door 2? Does the opti-
mal decision for the contestant depend on the contestant’s beliefs about
whether the gameshow host is confused or not?]

. Exercise 3.10.[2 ] Another example in which the emphasis is not on priors. You
visit a family whose three children are all at the local school. You don’t

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

58

3 | More about Inference

know anything about the sexes of the children. While walking clum-
sily round the home, you stumble through one of the three unlabelled
bedroom doors that you know belong, one each, to the three children,
and (cid:12)nd that the bedroom contains girlie stu(cid:11) in su(cid:14)cient quantities to
convince you that the child who lives in that bedroom is a girl. Later,
you sneak a look at a letter addressed to the parents, which reads ‘From
the Headmaster: we are sending this letter to all parents who have male
children at the school to inform them about the following boyish mat-
ters. . . ’.

These two sources of evidence establish that at least one of the three
children is a girl, and that at least one of the children is a boy. What
are the probabilities that there are (a) two girls and one boy; (b) two
boys and one girl?

. Exercise 3.11.[2, p.61] Mrs S is found stabbed in her family garden. Mr S
behaves strangely after her death and is considered as a suspect. On
investigation of police and social records it is found that Mr S had beaten
up his wife on at least nine previous occasions. The prosecution advances
this data as evidence in favour of the hypothesis that Mr S is guilty of the
murder. ‘Ah no,’ says Mr S’s highly paid lawyer, ‘statistically, only one
in a thousand wife-beaters actually goes on to murder his wife.1 So the
wife-beating is not strong evidence at all. In fact, given the wife-beating
evidence alone, it’s extremely unlikely that he would be the murderer of
his wife { only a 1=1000 chance. You should therefore (cid:12)nd him innocent.’

Is the lawyer right to imply that the history of wife-beating does not
point to Mr S’s being the murderer? Or is the lawyer a slimy trickster?
If the latter, what is wrong with his argument?

[Having received an indignant letter from a lawyer about the preceding
paragraph, I’d like to add an extra inference exercise at this point: Does
my suggestion that Mr. S.’s lawyer may have been a slimy trickster imply
that I believe all lawyers are slimy tricksters? (Answer: No.)]

. Exercise 3.12.[2 ] A bag contains one counter, known to be either white or
black. A white counter is put in, the bag is shaken, and a counter
is drawn out, which proves to be white. What is now the chance of
drawing a white counter? [Notice that the state of the bag, after the
operations, is exactly identical to its state before.]

. Exercise 3.13.[2, p.62] You move into a new house; the phone is connected, and
you’re pretty sure that the phone number is 740511, but not as sure as
you would like to be. As an experiment, you pick up the phone and
dial 740511; you obtain a ‘busy’ signal. Are you now more sure of your
phone number? If so, how much?

. Exercise 3.14.[1 ] In a game, two coins are tossed. If either of the coins comes
up heads, you have won a prize. To claim the prize, you must point to
one of your coins that is a head and say ‘look, that coin’s a head, I’ve
won’. You watch Fred play the game. He tosses the two coins, and he

1In the U.S.A., it is estimated that 2 million women are abused each year by their partners.
In 1994, 4739 women were victims of homicide; of those, 1326 women (28%) were slain by
husbands and boyfriends.
(Sources: http://www.umn.edu/mincava/papers/factoid.htm,
http://www.gunfree.inter.net/vpc/womenfs.htm)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.6: Solutions

59

points to a coin and says ‘look, that coin’s a head, I’ve won’. What is
the probability that the other coin is a head?

. Exercise 3.15.[2, p.63] A statistical statement appeared in The Guardian on

Friday January 4, 2002:

When spun on edge 250 times, a Belgian one-euro coin came
up heads 140 times and tails 110.
‘It looks very suspicious
to me’, said Barry Blight, a statistics lecturer at the London
School of Economics. ‘If the coin were unbiased the chance of
getting a result as extreme as that would be less than 7%’.

But do these data give evidence that the coin is biased rather than fair?
[Hint: see equation (3.22).]

3.6 Solutions

Solution to exercise 3.1 (p.47).
probabilities,

Let the data be D. Assuming equal prior

P (Aj D)
P (B j D)

=

1
2

3
2

1
1

3
2

1
2

2
2

1
2

=

9
32

(3.31)

and P (Aj D) = 9=41:
Solution to exercise 3.2 (p.47). The probability of the data given each hy-
pothesis is:

P (D j A) =

P (D j B) =

P (D j C) =

=

=

=

18
207 ;
64
207 ;
1
207 :

3
20
2
20
1
20

1
20
2
20
1
20

2
20
2
20
1
20

1
20
2
20
1
20

3
20
2
20
1
20

1
20
1
20
1
20

1
20
2
20
1
20

(3.32)

(3.33)

(3.34)

:

1
83
(3.35)

Figure 3.7. Posterior probability
for the bias pa of a bent coin
given two di(cid:11)erent data sets.

So

P (Aj D) =

18

18 + 64 + 1

=

18
83

;

P (B j D) =

64
83

;

P (C j D) =

(a)

0

0.2

0.4

0.6

P (pa j s = aba; F = 3) / p2

0.8

1
a (1 (cid:0) pa)

(b)

0

0.2

0.4

0.6

0.8

1

P (pa j s = bbb; F = 3) / (1 (cid:0) pa)3

Solution to exercise 3.5 (p.52).
(a) P (pa j s = aba; F = 3) / p2
a(1 (cid:0) pa). The most probable value of pa (i.e.,
the value that maximizes the posterior probability density) is 2=3. The
mean value of pa is 3=5.

See (cid:12)gure 3.7a.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

60

3 | More about Inference

.

; the right-hand

Figure 3.8. Range of plausible
values of the log evidence in
favour of H1 as a function of F .
The vertical axis on the left is
log P (s j F;H1)
P (s j F;H0)
vertical axis shows the values of
P (s j F;H1)
P (s j F;H0)
The solid line shows the log
evidence if the random variable
Fa takes on its mean value,
Fa = paF . The dotted lines show
(approximately) the log evidence
if Fa is at its 2.5th or 97.5th
percentile.
(See also (cid:12)gure 3.6, p.54.)

(b) P (pa j s = bbb; F = 3) / (1 (cid:0) pa)3. The most probable value of pa (i.e.,
the value that maximizes the posterior probability density) is 0. The
mean value of pa is 1=5.

See (cid:12)gure 3.7b.

H0 is true
pa = 1=6

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

8
6
4
2
0
-2
-4

0

50

H1 is true

8
6
4
2
0
-2
-4

0

50

pa = 0:25

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

8
6
4
2
0
-2
-4

0

50

pa = 0:5

1000/1
100/1
10/1
1/1
1/10
1/100

100 150 200

Solution to exercise 3.7 (p.54). The curves in (cid:12)gure 3.8 were found by (cid:12)nding
the mean and standard deviation of Fa, then setting Fa to the mean (cid:6) two
standard deviations to get a 95% plausible range for Fa, and computing the
three corresponding values of the log evidence ratio.

Solution to exercise 3.8 (p.57). Let Hi denote the hypothesis that the prize is
behind door i. We make the following assumptions: the three hypotheses H1,
H2 and H3 are equiprobable a priori, i.e.,

P (H1) = P (H2) = P (H3) =

1
3

:

(3.36)

The datum we receive, after choosing door 1, is one of D = 3 and D = 2 (mean-
ing door 3 or 2 is opened, respectively). We assume that these two possible
outcomes have the following probabilities. If the prize is behind door 1 then
the host has a free choice; in this case we assume that the host selects at
random between D = 2 and D = 3. Otherwise the choice of the host is forced
and the probabilities are 0 and 1.

P (D = 2jH1) = 1/2 P (D = 2jH2) = 0 P (D = 2jH3) = 1
P (D = 3jH1) = 1/2 P (D = 3jH2) = 1 P (D = 3jH3) = 0

(3.37)

Now, using Bayes’ theorem, we evaluate the posterior probabilities of the
hypotheses:

P (Hi j D = 3) =

P (D = 3jHi)P (Hi)

P (D = 3)

(3.38)

P (H1 j D = 3) = (1=2)(1=3)

P (D=3)

(3.39)
The denominator P (D = 3) is (1=2) because it is the normalizing constant for
this posterior distribution. So

P (H2 j D = 3) = (1)(1=3)

P (D=3) P (H3 j D = 3) = (0)(1=3)

P (D=3)

P (H1 j D = 3) = 1/3 P (H2 j D = 3) = 2/3 P (H3 j D = 3) = 0:

(3.40)
So the contestant should switch to door 2 in order to have the biggest chance
of getting the prize.

Many people (cid:12)nd this outcome surprising. There are two ways to make it
more intuitive. One is to play the game thirty times with a friend and keep
track of the frequency with which switching gets the prize. Alternatively, you
can perform a thought experiment in which the game is played with a million
doors. The rules are now that the contestant chooses one door, then the game

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.6: Solutions

61

show host opens 999,998 doors in such a way as not to reveal the prize, leaving
the contestant’s selected door and one other door closed. The contestant may
now stick or switch. Imagine the contestant confronted by a million doors,
of which doors 1 and 234,598 have not been opened, door 1 having been the
contestant’s initial guess. Where do you think the prize is?

Solution to exercise 3.9 (p.57).
If door 3 is opened by an earthquake, the
inference comes out di(cid:11)erently { even though visually the scene looks the
same. The nature of the data, and the probability of the data, are both
now di(cid:11)erent. The possible data outcomes are, (cid:12)rstly, that any number of
the doors might have opened. We could label the eight possible outcomes
d = (0; 0; 0); (0; 0; 1); (0; 1; 0); (1; 0; 0); (0; 1; 1); : : : ; (1; 1; 1). Secondly, it might
be that the prize is visible after the earthquake has opened one or more doors.
So the data D consists of the value of d, and a statement of whether the prize
was revealed. It is hard to say what the probabilities of these outcomes are,
since they depend on our beliefs about the reliability of the door latches and
the properties of earthquakes, but it is possible to extract the desired posterior
probability without naming the values of P (djHi) for each d. All that matters
are the relative values of the quantities P (D jH1), P (D jH2), P (D jH3), for
the value of D that actually occurred. [This is the likelihood principle, which
we met in section 2.3.] The value of D that actually occurred is ‘d = (0; 0; 1),
and no prize visible’. First, it is clear that P (D jH3) = 0, since the datum
that no prize is visible is incompatible with H3. Now, assuming that the
contestant selected door 1, how does the probability P (D jH1) compare with
P (D jH2)? Assuming that earthquakes are not sensitive to decisions of game
show contestants, these two quantities have to be equal, by symmetry. We
don’t know how likely it is that door 3 falls o(cid:11) its hinges, but however likely
it is, it’s just as likely to do so whether the prize is behind door 1 or door 2.
So, if P (D jH1) and P (D jH2) are equal, we obtain:
P (H2jD) = P (DjH2)(1/3)
P (H1jD) = P (DjH1)(1/3)

P (H3jD) = P (DjH3)(1/3)

P (D)

P (D)

P (D)

= 0:

= 1/2

= 1/2

(3.41)

The two possible hypotheses are now equally likely.

If we assume that the host knows where the prize is and might be acting
deceptively, then the answer might be further modi(cid:12)ed, because we have to
view the host’s words as part of the data.

Confused? It’s well worth making sure you understand these two gameshow
problems. Don’t worry, I slipped up on the second problem, the (cid:12)rst time I
met it.

There is a general rule which helps immensely when you have a confusing

probability problem:

Always write down the probability of everything.
(Steve Gull)

From this joint probability, any desired inference can be mechanically ob-

tained ((cid:12)gure 3.9).

Solution to exercise 3.11 (p.58). The statistic quoted by the lawyer indicates
the probability that a randomly selected wife-beater will also murder his wife.
The probability that the husband was the murderer, given that the wife has
been murdered, is a completely di(cid:11)erent quantity.

Where the prize is

door door door

1

2

3

none

pnone

pnone

pnone

3

3

3

p3
3

p3
3

p3
3

e
k
a
u
q
h
t
r
a
e

y
b

d
e
n
e
p
o

s
r
o
o
d

h
c
i
h
W

1

2

3

1,2

1,3

2,3

1,2,3

p1;2;3

p1;2;3

p1;2;3

3

3

3

Figure 3.9. The probability of
everything, for the second
three-door problem, assuming an
earthquake has just occurred.
Here, p3 is the probability that
door 3 alone is opened by an
earthquake.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

62

3 | More about Inference

To deduce the latter, we need to make further assumptions about the
probability that the wife is murdered by someone else. If she lives in a neigh-
bourhood with frequent random murders, then this probability is large and
the posterior probability that the husband did it (in the absence of other ev-
idence) may not be very large. But in more peaceful regions, it may well be
that the most likely person to have murdered you, if you are found murdered,
is one of your closest relatives.

Let’s work out some illustrative numbers with the help of the statistics
on page 58. Let m = 1 denote the proposition that a woman has been mur-
dered; h = 1, the proposition that the husband did it; and b = 1, the propo-
sition that he beat her in the year preceding the murder. The statement
‘someone else did it’ is denoted by h = 0. We need to de(cid:12)ne P (hj m = 1),
P (bj h = 1; m = 1), and P (b = 1j h = 0; m = 1) in order to compute the pos-
terior probability P (h = 1j b = 1; m = 1). From the statistics, we can read
out P (h = 1j m = 1) = 0:28. And if two million women out of 100 million
are beaten, then P (b = 1j h = 0; m = 1) = 0:02. Finally, we need a value for
P (bj h = 1; m = 1): if a man murders his wife, how likely is it that this is the
(cid:12)rst time he laid a (cid:12)nger on her? I expect it’s pretty unlikely; so maybe
P (b = 1j h = 1; m = 1) is 0.9 or larger.

By Bayes’ theorem, then,

P (h = 1j b = 1; m = 1) =

:9 (cid:2) :28

:9 (cid:2) :28 + :02 (cid:2) :72 ’ 95%:

(3.42)

One way to make obvious the sliminess of the lawyer on p.58 is to construct
arguments, with the same logical structure as his, that are clearly wrong.
For example, the lawyer could say ‘Not only was Mrs. S murdered, she was
murdered between 4.02pm and 4.03pm. Statistically, only one in a million
wife-beaters actually goes on to murder his wife between 4.02pm and 4.03pm.
So the wife-beating is not strong evidence at all. In fact, given the wife-beating
evidence alone, it’s extremely unlikely that he would murder his wife in this
way { only a 1/1,000,000 chance.’

Solution to exercise 3.13 (p.58). There are two hypotheses. H0: your number
is 740511; H1: it is another number. The data, D, are ‘when I dialed 740511,
I got a busy signal’. What is the probability of D, given each hypothesis? If
your number is 740511, then we expect a busy signal with certainty:

P (D jH0) = 1:

On the other hand, if H1 is true, then the probability that the number dialled
returns a busy signal is smaller than 1, since various other outcomes were also
possible (a ringing tone, or a number-unobtainable signal, for example). The
value of this probability P (D jH1) will depend on the probability (cid:11) that a
random phone number similar to your own phone number would be a valid
phone number, and on the probability (cid:12) that you get a busy signal when you
dial a valid phone number.

I estimate from the size of my phone book that Cambridge has about
75 000 valid phone numbers, all of length six digits. The probability that a
random six-digit number is valid is therefore about 75 000=106 = 0:075.
If
we exclude numbers beginning with 0, 1, and 9 from the random choice, the
probability (cid:11) is about 75 000=700 000 ’ 0:1.
If we assume that telephone
numbers are clustered then a misremembered number might be more likely
to be valid than a randomly chosen number; so the probability, (cid:11), that our
guessed number would be valid, assuming H1 is true, might be bigger than

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

3.6: Solutions

63

0.1. Anyway, (cid:11) must be somewhere between 0.1 and 1. We can carry forward
this uncertainty in the probability and see how much it matters at the end.

The probability (cid:12) that you get a busy signal when you dial a valid phone
number is equal to the fraction of phones you think are in use or o(cid:11)-the-hook
when you make your tentative call. This fraction varies from town to town
and with the time of day. In Cambridge, during the day, I would guess that
about 1% of phones are in use. At 4am, maybe 0.1%, or fewer.

The probability P (D jH1) is the product of (cid:11) and (cid:12), that is, about 0:1 (cid:2)
0:01 = 10(cid:0)3. According to our estimates, there’s about a one-in-a-thousand
chance of getting a busy signal when you dial a random number; or one-in-a-
hundred, if valid numbers are strongly clustered; or one-in-104, if you dial in
the wee hours.

How do the data a(cid:11)ect your beliefs about your phone number? The pos-
terior probability ratio is the likelihood ratio times the prior probability ratio:

P (H0 j D)
P (H1 j D)

=

P (D jH0)
P (D jH1)

P (H0)
P (H1)

:

(3.43)

The likelihood ratio is about 100-to-1 or 1000-to-1, so the posterior probability
ratio is swung by a factor of 100 or 1000 in favour of H0. If the prior probability
of H0 was 0.5 then the posterior probability is

P (H0 j D) =

1

1 + P (H1 j D)
P (H0 j D)

’ 0:99 or 0:999:

(3.44)

Solution to exercise 3.15 (p.59). We compare the models H0 { the coin is fair
{ and H1 { the coin is biased, with the prior on its bias set to the uniform
distribution P (pjH1) = 1.
[The use of a uniform prior seems reasonable
to me, since I know that some coins, such as American pennies, have severe
biases when spun on edge; so the situations p = 0:01 or p = 0:1 or p = 0:95
would not surprise me.]

When I mention H0 { the coin is fair { a pedant would say, ‘how absurd to even
consider that the coin is fair { any coin is surely biased to some extent’. And
of course I would agree. So will pedants kindly understand H0 as meaning ‘the
coin is fair to within one part in a thousand, i.e., p 2 0:5 (cid:6) 0:001’.

The likelihood ratio is:

P (DjH1)
P (DjH0)

140!110!

=

251!

1=2250 = 0:48:

(3.45)

Thus the data give scarcely any evidence either way; in fact they give weak
evidence (two to one) in favour of H0!
‘No, no’, objects the believer in bias, ‘your silly uniform prior doesn’t
represent my prior beliefs about the bias of biased coins { I was expecting only
a small bias’. To be as generous as possible to the H1, let’s see how well it
could fare if the prior were presciently set. Let us allow a prior of the form

P (pjH1; (cid:11)) =

1

Z((cid:11))

p(cid:11)(cid:0)1(1 (cid:0) p)(cid:11)(cid:0)1; where Z((cid:11)) = (cid:0)((cid:11))2=(cid:0)(2(cid:11))

(3.46)

0.05

0.04

0.03

0.02

0.01

0

0

H0
H1

140

50

100

150

200

250

Figure 3.10. The probability
distribution of the number of
heads given the two hypotheses,
that the coin is fair, and that it is
biased, with the prior distribution
of the bias being uniform. The
outcome (D = 140 heads) gives
weak evidence in favour of H0, the
hypothesis that the coin is fair.

(a Beta distribution, with the original uniform prior reproduced by setting
(cid:11) = 1). By tweaking (cid:11), the likelihood ratio for H1 over H0,
(cid:0)(140+(cid:11)) (cid:0)(110+(cid:11)) (cid:0)(2(cid:11))2250

;

(3.47)

P (DjH1; (cid:11))
P (DjH0)

=

(cid:0)(250+2(cid:11)) (cid:0)((cid:11))2

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

64

3 | More about Inference

can be increased a little.
It is shown for several values of (cid:11) in (cid:12)gure 3.11.
Even the most favourable choice of (cid:11) ((cid:11) ’ 50) can yield a likelihood ratio of
only two to one in favour of H1.
In conclusion, the data are not ‘very suspicious’. They can be construed
as giving at most two-to-one evidence in favour of one or other of the two
hypotheses.

Are these wimpy likelihood ratios the fault of over-restrictive priors? Is there
any way of producing a ‘very suspicious’ conclusion? The prior that is best-
matched to the data, in terms of likelihood, is the prior that sets p to f (cid:17)
140=250 with probability one. Let’s call this model H(cid:3). The likelihood ratio is
P (DjH(cid:3))=P (DjH0) = 2250f 140(1 (cid:0) f )110 = 6:1. So the strongest evidence that
these data can possibly muster against the hypothesis that there is no bias is
six-to-one.

While we are noticing the absurdly misleading answers that ‘sampling the-
ory’ statistics produces, such as the p-value of 7% in the exercise we just solved,
let’s stick the boot in. If we make a tiny change to the data set, increasing
the number of heads in 250 tosses from 140 to 141, we (cid:12)nd that the p-value
goes below the mystical value of 0.05 (the p-value is 0.0497). The sampling
theory statistician would happily squeak ‘the probability of getting a result as
extreme as 141 heads is smaller than 0.05 { we thus reject the null hypothesis
at a signi(cid:12)cance level of 5%’. The correct answer is shown for several values
of (cid:11) in (cid:12)gure 3.12. The values worth highlighting from this table are, (cid:12)rst,
the likelihood ratio when H1 uses the standard uniform prior, which is 1:0.61
in favour of the null hypothesis H0. Second, the most favourable choice of (cid:11),
from the point of view of H1, can only yield a likelihood ratio of about 2.3:1
in favour of H1.
Be warned! A p-value of 0.05 is often interpreted as implying that the odds
are stacked about twenty-to-one against the null hypothesis. But the truth
in this case is that the evidence either slightly favours the null hypothesis, or
disfavours it by at most 2.3 to one, depending on the choice of prior.

The p-values and ‘signi(cid:12)cance levels’ of classical statistics should be treated

with extreme caution. Shun them! Here ends the sermon.

(cid:11)

.37

1.0
2.7
7.4

20
55
148
403
1096

P (DjH1; (cid:11))
P (DjH0)

.25
.48
.82

1.3
1.8
1.9
1.7
1.3
1.1

Figure 3.11. Likelihood ratio for
various choices of the prior
distribution’s hyperparameter (cid:11).

(cid:11)

.37

1.0
2.7
7.4

20
55
148
403
1096

P (D0jH1; (cid:11))
P (D0jH0)

.32
.61

1.0
1.6
2.2
2.3
1.9
1.4
1.2

Figure 3.12. Likelihood ratio for
various choices of the prior
distribution’s hyperparameter (cid:11),
when the data are D0 = 141 heads
in 250 trials.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part I

Data Compression

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Notation

x is a member of the
x 2 A
set A
S is a subset of the
S (cid:26) A
set A
S (cid:18) A
S is a subset of, or
equal to, the set A
V = B [ A V is the union of the
sets B and A
V = B \ A V is the intersection
of the sets B and A
number of elements
jAj
in set A

About Chapter 4

In this chapter we discuss how to measure the information content of the
outcome of a random experiment.

This chapter has some tough bits. If you (cid:12)nd the mathematical details
hard, skim through them and keep going { you’ll be able to enjoy Chapters 5
and 6 without this chapter’s tools.

Before reading Chapter 4, you should have read Chapter 2 and worked on

exercises 2.21{2.25 and 2.16 (pp.36{37), and exercise 4.1 below.

The following exercise is intended to help you think about how to measure

information content.

Exercise 4.1.[2, p.69] { Please work on this problem before reading Chapter 4.

You are given 12 balls, all equal in weight except for one that is either
heavier or lighter. You are also given a two-pan balance to use. In each
use of the balance you may put any number of the 12 balls on the left
pan, and the same number on the right pan, and push a button to initiate
the weighing; there are three possible outcomes: either the weights are
equal, or the balls on the left are heavier, or the balls on the left are
lighter. Your task is to design a strategy to determine which is the odd
ball and whether it is heavier or lighter than the others in as few uses
of the balance as possible.

While thinking about this problem, you may (cid:12)nd it helpful to consider
the following questions:

(a) How can one measure information?

(b) When you have identi(cid:12)ed the odd ball and whether it is heavy or

light, how much information have you gained?

(c) Once you have designed a strategy, draw a tree showing, for each
of the possible outcomes of a weighing, what weighing you perform
next. At each node in the tree, how much information have the
outcomes so far given you, and how much information remains to
be gained?

(d) How much information is gained when you learn (i) the state of a
(cid:13)ipped coin; (ii) the states of two (cid:13)ipped coins; (iii) the outcome
when a four-sided die is rolled?

(e) How much information is gained on the (cid:12)rst step of the weighing
problem if 6 balls are weighed against the other 6? How much is
gained if 4 are weighed against 4 on the (cid:12)rst step, leaving out 4
balls?

66

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4

The Source Coding Theorem

4.1 How to measure the information content of a random variable?

In the next few chapters, we’ll be talking about probability distributions and
random variables. Most of the time we can get by with sloppy notation,
but occasionally, we will need precise notation. Here is the notation that we
established in Chapter 2.
An ensemble X is a triple (x;AX ;PX ), where the outcome x is the value
of a random variable, which takes on one of a set of possible values,
AX = fa1; a2; : : : ; ai; : : : ; aIg, having probabilities PX = fp1; p2; : : : ; pIg,
with P (x = ai) = pi, pi (cid:21) 0 and Pai2AX

How can we measure the information content of an outcome x = ai from such
an ensemble? In this chapter we examine the assertions

P (x = ai) = 1.

1. that the Shannon information content,

h(x = ai) (cid:17) log2

1
pi

;

(4.1)

is a sensible measure of the information content of the outcome x = ai,
and

2. that the entropy of the ensemble,

H(X) =Xi

pi log2

1
pi

;

(4.2)

is a sensible measure of the ensemble’s average information content.

10

h(p) = log2

1
p

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1 p

p

h(p) H2(p)

0.001
0.01
0.1
0.2
0.5

10.0
6.6
3.3
2.3
1.0

0.011
0.081
0.47
0.72
1.0

H2(p)

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1 p

Figure 4.1. The Shannon
information content h(p) = log2
and the binary entropy function
H2(p) = H(p; 1(cid:0)p) =
p + (1 (cid:0) p) log2
p log2
function of p.

(1(cid:0)p) as a

1

1

1
p

Figure 4.1 shows the Shannon information content of an outcome with prob-
ability p, as a function of p. The less probable an outcome is, the greater
its Shannon information content. Figure 4.1 also shows the binary entropy
function,

H2(p) = H(p; 1(cid:0)p) = p log2

1
p

+ (1 (cid:0) p) log2

1

(1 (cid:0) p)

;

(4.3)

which is the entropy of the ensemble X whose alphabet and probability dis-
tribution are AX = fa; bg;PX = fp; (1 (cid:0) p)g.

67

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

68

4 | The Source Coding Theorem

Information content of independent random variables

Why should log 1=pi have anything to do with the information content? Why
not some other function of pi? We’ll explore this question in detail shortly,
but (cid:12)rst, notice a nice property of this particular function h(x) = log 1=p(x).
Imagine learning the value of two independent random variables, x and y.
The de(cid:12)nition of independence is that the probability distribution is separable
into a product:

P (x; y) = P (x)P (y):

(4.4)

Intuitively, we might want any measure of the ‘amount of information gained’
to have the property of additivity { that is, for independent random variables
x and y, the information gained when we learn x and y should equal the sum
of the information gained if x alone were learned and the information gained
if y alone were learned.

The Shannon information content of the outcome x; y is

h(x; y) = log

1

P (x; y)

= log

1

P (x)P (y)

= log

1

P (x)

+ log

1

P (y)

(4.5)

so it does indeed satisfy

h(x; y) = h(x) + h(y);

if x and y are independent.

(4.6)

Exercise 4.2.[1, p.86] Show that, if x and y are independent, the entropy of the

outcome x; y satis(cid:12)es

H(X; Y ) = H(X) + H(Y ):

(4.7)

In words, entropy is additive for independent variables.

We now explore these ideas with some examples; then, in section 4.4 and
in Chapters 5 and 6, we prove that the Shannon information content and the
entropy are related to the number of bits needed to describe the outcome of
an experiment.

The weighing problem: designing informative experiments

Have you solved the weighing problem (exercise 4.1, p.66) yet? Are you sure?
Notice that in three uses of the balance { which reads either ‘left heavier’,
‘right heavier’, or ‘balanced’ { the number of conceivable outcomes is 33 = 27,
whereas the number of possible states of the world is 24: the odd ball could
be any of twelve balls, and it could be heavy or light. So in principle, the
problem might be solvable in three weighings { but not in two, since 32 < 24.
If you know how you can determine the odd weight and whether it is
heavy or light in three weighings, then you may read on. If you haven’t found
a strategy that always gets there in three weighings, I encourage you to think
about exercise 4.1 some more.

Why is your strategy optimal? What is it about your series of weighings
that allows useful information to be gained as quickly as possible? The answer
is that at each step of an optimal procedure, the three outcomes (‘left heavier’,
‘right heavier’, and ‘balance’) are as close as possible to equiprobable. An
optimal solution is shown in (cid:12)gure 4.2.

Suboptimal strategies, such as weighing balls 1{6 against 7{12 on the (cid:12)rst
step, do not achieve all outcomes with equal probability: these two sets of balls
can never balance, so the only possible outcomes are ‘left heavy’ and ‘right
heavy’. Such a binary outcome rules out only half of the possible hypotheses,

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.1: How to measure the information content of a random variable?

69

1+
2+
3+
4+
5+
6+
7+
8+
9+
10+
11+
12+
1(cid:0)
2(cid:0)
3(cid:0)
4(cid:0)
5(cid:0)
6(cid:0)
7(cid:0)
8(cid:0)
9(cid:0)
10(cid:0)
11(cid:0)
12(cid:0)

weigh

1 2 3 4
5 6 7 8

(cid:2)
B

(cid:2)
(cid:2)

B
B

(cid:2)(cid:2)(cid:14)

(cid:2)

(cid:2)
(cid:2)

-

(cid:2)

(cid:2)
(cid:2)

(cid:2)

(cid:2)
(cid:2)

(cid:2)

B

B
B

B

B
B

B

B
B

B

BBN

1+
2+
3+
4+
5(cid:0)
6(cid:0)
7(cid:0)
8(cid:0)

1(cid:0)
2(cid:0)
3(cid:0)
4(cid:0)
5+
6+
7+
8+

9+
10+
11+
12+
9(cid:0)
10(cid:0)
11(cid:0)
12(cid:0)

weigh

1 2 6
3 4 5

weigh

1 2 6
3 4 5

weigh

9 10 11

1 2 3

(cid:1)
(cid:1)
A
A

(cid:1)(cid:21)
(cid:1)
(cid:1)

-

A
A
AU

(cid:1)
(cid:1)
A
A

(cid:1)(cid:21)
(cid:1)
(cid:1)

-

A
A
AU

(cid:1)
(cid:1)
A
A

(cid:1)(cid:21)
(cid:1)
(cid:1)

-

A
A
AU

1+2+5(cid:0)

3+4+6(cid:0)

7(cid:0)8(cid:0)

6+3(cid:0)4(cid:0)

1(cid:0)2(cid:0)5+

7+8+

9+10+11+

9(cid:0)10(cid:0)11(cid:0)

12+12(cid:0)

1
2

3
4

1
7

3
4

1
2

7
1

9
10

9
10

12
1

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

(cid:0)
@

(cid:0)(cid:18)
-
@R

1+
2+
5(cid:0)
3+
4+
6(cid:0)
7(cid:0)
8(cid:0)

?
4(cid:0)
3(cid:0)
6+
2(cid:0)
1(cid:0)
5+
7+
8+

?
9+
10+
11+
10(cid:0)
9(cid:0)
11(cid:0)
12+
12(cid:0)

?

Figure 4.2. An optimal solution to the weighing problem. At each step there are two boxes: the left
box shows which hypotheses are still possible; the right box shows the balls involved in the
next weighing. The 24 hypotheses are written 1+; : : : ; 12(cid:0), with, e.g., 1+ denoting that
1 is the odd ball and it is heavy. Weighings are written by listing the names of the balls
on the two pans, separated by a line; for example, in the (cid:12)rst weighing, balls 1, 2, 3, and
4 are put on the left-hand side and 5, 6, 7, and 8 on the right. In each triplet of arrows
the upper arrow leads to the situation when the left side is heavier, the middle arrow to
the situation when the right side is heavier, and the lower arrow to the situation when the
outcome is balanced. The three points labelled ? correspond to impossible outcomes.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

70

4 | The Source Coding Theorem

so a strategy that uses such outcomes must sometimes take longer to (cid:12)nd the
right answer.

The insight that the outcomes should be as near as possible to equiprobable
makes it easier to search for an optimal strategy. The (cid:12)rst weighing must
divide the 24 possible hypotheses into three groups of eight. Then the second
weighing must be chosen so that there is a 3:3:2 split of the hypotheses.

Thus we might conclude:

the outcome of a random experiment is guaranteed to be most in-
formative if the probability distribution over outcomes is uniform.

This conclusion agrees with the property of the entropy that you proved
when you solved exercise 2.25 (p.37): the entropy of an ensemble X is biggest
if all the outcomes have equal probability pi = 1=jAXj.

Guessing games

In the game of twenty questions, one player thinks of an object, and the
other player attempts to guess what the object is by asking questions that
have yes/no answers, for example, ‘is it alive?’, or ‘is it human?’ The aim
is to identify the object with as few questions as possible. What is the best
strategy for playing this game? For simplicity, imagine that we are playing
the rather dull version of twenty questions called ‘sixty-three’.

Example 4.3. The game ‘sixty-three’. What’s the smallest number of yes/no

questions needed to identify an integer x between 0 and 63?

Intuitively, the best questions successively divide the 64 possibilities into equal
sized sets. Six questions su(cid:14)ce. One reasonable strategy asks the following
questions:

1: is x (cid:21) 32?
2: is x mod 32 (cid:21) 16?
3: is x mod 16 (cid:21) 8?
4: is x mod 8 (cid:21) 4?
5: is x mod 4 (cid:21) 2?
6: is x mod 2 = 1?

[The notation x mod 32, pronounced ‘x modulo 32’, denotes the remainder
when x is divided by 32; for example, 35 mod 32 = 3 and 32 mod 32 = 0.]

The answers to these questions, if translated from fyes; nog to f1; 0g, give

the binary expansion of x, for example 35 ) 100011.

2

What are the Shannon information contents of the outcomes in this ex-
ample? If we assume that all values of x are equally likely, then the answers
to the questions are independent and each has Shannon information content
log2(1=0:5) = 1 bit; the total Shannon information gained is always six bits.
Furthermore, the number x that we learn from these questions is a six-bit bi-
nary number. Our questioning strategy de(cid:12)nes a way of encoding the random
variable x as a binary (cid:12)le.

So far, the Shannon information content makes sense:

it measures the
length of a binary (cid:12)le that encodes x. However, we have not yet studied
ensembles where the outcomes have unequal probabilities. Does the Shannon
information content make sense there too?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.1: How to measure the information content of a random variable?

71

32
E5

x = n

32
33

0.0443

1.0

48
F3

x = n

16
17

0.0874

2.0

x = y

1
16

4.0

6.0

Figure 4.3. A game of submarine.
The submarine is hit on the 49th
attempt.

A

B

C

D

E

F

G

H

(cid:2)j

87654321

1
G3

x = n

63
64

0.0227

0.0227

(cid:2)j

(cid:2)

2
B1

x = n

62
63

0.0230

0.0458

move #
question
outcome

P (x)

h(x)

Total info.

(cid:2)
(cid:2)

(cid:2)

(cid:2)
(cid:2)

(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2) j
(cid:2)(cid:2)(cid:2)(cid:2)

(cid:2)
(cid:2)
(cid:2)
(cid:2)

(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)j
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)

(cid:2)

(cid:2)
(cid:2)

(cid:2)
(cid:2)
(cid:2)
(cid:2)

(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)jS
(cid:2)(cid:2)(cid:2)(cid:2)

(cid:2)

49
H3

The game of submarine: how many bits can one bit convey?

In the game of battleships, each player hides a (cid:13)eet of ships in a sea represented
by a square grid. On each turn, one player attempts to hit the other’s ships by
(cid:12)ring at one square in the opponent’s sea. The response to a selected square
such as ‘G3’ is either ‘miss’, ‘hit’, or ‘hit and destroyed’.

In a boring version of battleships called submarine, each player hides just
one submarine in one square of an eight-by-eight grid. Figure 4.3 shows a few
pictures of this game in progress: the circle represents the square that is being
(cid:12)red at, and the (cid:2)s show squares in which the outcome was a miss, x = n; the
submarine is hit (outcome x = y shown by the symbol s) on the 49th attempt.
Each shot made by a player de(cid:12)nes an ensemble. The two possible out-
comes are fy; ng, corresponding to a hit and a miss, and their probabili-
ties depend on the state of the board. At the beginning, P (y) = 1=64 and
P (n) = 63=64. At the second shot, if the (cid:12)rst shot missed, P (y) = 1=63 and
P (n) = 62=63. At the third shot, if the (cid:12)rst two shots missed, P (y) = 1=62
and P (n) = 61=62.

The Shannon information gained from an outcome x is h(x) = log(1=P (x)).

If we are lucky, and hit the submarine on the (cid:12)rst shot, then

h(x) = h(1)(y) = log2 64 = 6 bits:

(4.8)

Now, it might seem a little strange that one binary outcome can convey six
bits. But we have learnt the hiding place, which could have been any of 64
squares; so we have, by one lucky binary question, indeed learnt six bits.

What if the (cid:12)rst shot misses? The Shannon information that we gain from

this outcome is

h(x) = h(1)(n) = log2

64
63

= 0:0227 bits:

(4.9)

Does this make sense? It is not so obvious. Let’s keep going. If our second
shot also misses, the Shannon information content of the second outcome is

h(2)(n) = log2

63
62

= 0:0230 bits:

(4.10)

If we miss thirty-two times ((cid:12)ring at a new square each time), the total Shan-
non information gained is

64
63

63
62

+ log2

log2
= 0:0227 + 0:0230 + (cid:1)(cid:1)(cid:1) + 0:0430 = 1:0 bits:

+ (cid:1)(cid:1)(cid:1) + log2

33
32

(4.11)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

72

4 | The Source Coding Theorem

Why this round number? Well, what have we learnt? We now know that the
submarine is not in any of the 32 squares we (cid:12)red at; learning that fact is just
like playing a game of sixty-three (p.70), asking as our (cid:12)rst question ‘is x
one of the thirty-two numbers corresponding to these squares I (cid:12)red at?’, and
receiving the answer ‘no’. This answer rules out half of the hypotheses, so it
gives us one bit.

After 48 unsuccessful shots, the information gained is 2 bits: the unknown
location has been narrowed down to one quarter of the original hypothesis
space.

What if we hit the submarine on the 49th shot, when there were 16 squares

left? The Shannon information content of this outcome is

h(49)(y) = log2 16 = 4:0 bits:

(4.12)

The total Shannon information content of all the outcomes is

log2

64
63

+ log2

63
62

+ (cid:1)(cid:1)(cid:1) + log2

17
16

+ log2

16
1

= 0:0227 + 0:0230 + (cid:1)(cid:1)(cid:1) + 0:0874 + 4:0 = 6:0 bits:

(4.13)

So once we know where the submarine is, the total Shannon information con-
tent gained is 6 bits.

This result holds regardless of when we hit the submarine.

If we hit it
when there are n squares left to choose from { n was 16 in equation (4.13) {
then the total information gained is:

log2

64
63

+ log2

63
62
= log2(cid:20) 64
63 (cid:2)

n + 1

+ (cid:1)(cid:1)(cid:1) + log2
63
62 (cid:2) (cid:1)(cid:1)(cid:1) (cid:2)

n
n + 1
n (cid:2)

+ log2

n
1

n

1(cid:21) = log2

64
1

= 6 bits: (4.14)

What have we learned from the examples so far? I think the submarine
example makes quite a convincing case for the claim that the Shannon infor-
mation content is a sensible measure of information content. And the game of
sixty-three shows that the Shannon information content can be intimately
connected to the size of a (cid:12)le that encodes the outcomes of a random experi-
ment, thus suggesting a possible connection to data compression.
In case you’re not convinced, let’s look at one more example.

The Wenglish language

Wenglish is a language similar to English. Wenglish sentences consist of words
drawn at random from the Wenglish dictionary, which contains 215 = 32,768
words, all of length 5 characters. Each word in the Wenglish dictionary was
constructed at random by picking (cid:12)ve letters from the probability distribution
over a: : :z depicted in (cid:12)gure 2.1.

Some entries from the dictionary are shown in alphabetical order in (cid:12)g-
ure 4.4. Notice that the number of words in the dictionary (32,768) is
much smaller than the total number of possible words of length 5 letters,
265 ’ 12,000,000.
Because the probability of the letter z is about 1=1000, only 32 of the
words in the dictionary begin with the letter z. In contrast, the probability
of the letter a is about 0:0625, and 2048 of the words begin with the letter a.
Of those 2048 words, two start az, and 128 start aa.

Let’s imagine that we are reading a Wenglish document, and let’s discuss
the Shannon information content of the characters as we acquire them. If we

1
2
3

aaail
aaaiu
aaald

...

129

abati

...

2047
2048

azpan
aztdn

...
...

16 384

odrcr

...
...

32 737

zatnt

...

32 768

zxast

Figure 4.4. The Wenglish
dictionary.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.2: Data compression

73

are given the text one word at a time, the Shannon information content of
each (cid:12)ve-character word is log 32,768 = 15 bits, since Wenglish uses all its
words with equal probability. The average information content per character
is therefore 3 bits.

Now let’s look at the information content if we read the document one
character at a time.
If, say, the (cid:12)rst letter of a word is a, the Shannon
information content is log 1=0:0625 ’ 4 bits. If the (cid:12)rst letter is z, the Shannon
information content is log 1=0:001 ’ 10 bits. The information content is thus
highly variable at the (cid:12)rst character. The total information content of the 5
characters in a word, however, is exactly 15 bits; so the letters that follow an
initial z have lower average information content per character than the letters
that follow an initial a. A rare initial letter such as z indeed conveys more
information about what the word is than a common initial letter.

Similarly, in English, if rare characters occur at the start of the word (e.g.
xyl...), then often we can identify the whole word immediately; whereas
words that start with common characters (e.g. pro...) require more charac-
ters before we can identify them.

4.2 Data compression

The preceding examples justify the idea that the Shannon information content
of an outcome is a natural measure of its information content. Improbable out-
comes do convey more information than probable outcomes. We now discuss
the information content of a source by considering how many bits are needed
to describe the outcome of an experiment.

If we can show that we can compress data from a particular source into
a (cid:12)le of L bits per source symbol and recover the data reliably, then we will
say that the average information content of that source is at most L bits per
symbol.

Example: compression of text (cid:12)les

A (cid:12)le is composed of a sequence of bytes. A byte is composed of 8 bits and
can have a decimal value between 0 and 255. A typical text (cid:12)le is composed
of the ASCII character set (decimal values 0 to 127). This character set uses
only seven of the eight bits in a byte.

Here we use the word ‘bit’ with its
meaning, ‘a symbol with two
values’, not to be confused with
the unit of information content.

. Exercise 4.4.[1, p.86] By how much could the size of a (cid:12)le be reduced given

that it is an ASCII (cid:12)le? How would you achieve this reduction?

Intuitively, it seems reasonable to assert that an ASCII (cid:12)le contains 7=8 as
much information as an arbitrary (cid:12)le of the same size, since we already know
one out of every eight bits before we even look at the (cid:12)le. This is a simple ex-
ample of redundancy. Most sources of data have further redundancy: English
text (cid:12)les use the ASCII characters with non-equal frequency; certain pairs of
letters are more probable than others; and entire words can be predicted given
the context and a semantic understanding of the text.

Some simple data compression methods that de(cid:12)ne measures of informa-
tion content

One way of measuring the information content of a random variable is simply
to count the number of possible outcomes, jAXj. (The number of elements in
a set A is denoted by jAj.) If we gave a binary name to each outcome, the

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

74

4 | The Source Coding Theorem

length of each name would be log2 jAXj bits, if jAXj happened to be a power
of 2. We thus make the following de(cid:12)nition.

The raw bit content of X is

H0(X) = log2 jAXj:

(4.15)

H0(X) is a lower bound for the number of binary questions that are always
guaranteed to identify an outcome from the ensemble X.
It is an additive
quantity: the raw bit content of an ordered pair x; y, having jAXjjAY j possible
outcomes, satis(cid:12)es
(4.16)

H0(X; Y ) = H0(X) + H0(Y ):

This measure of information content does not include any probabilistic
element, and the encoding rule it corresponds to does not ‘compress’ the source
data, it simply maps each outcome to a constant-length binary string.

Exercise 4.5.[2, p.86] Could there be a compressor that maps an outcome x to
a binary code c(x), and a decompressor that maps c back to x, such
that every possible outcome is compressed into a binary code of length
shorter than H0(X) bits?

Even though a simple counting argument shows that it is impossible to make
a reversible compression program that reduces the size of all (cid:12)les, ama-
teur compression enthusiasts frequently announce that they have invented
a program that can do this { indeed that they can further compress com-
pressed (cid:12)les by putting them through their compressor several times. Stranger
yet, patents have been granted to these modern-day alchemists. See the
comp.compression frequently asked questions for further reading.1

There are only two ways in which a ‘compressor’ can actually compress

(cid:12)les:

1. A lossy compressor compresses some (cid:12)les, but maps some (cid:12)les to the
same encoding. We’ll assume that the user requires perfect recovery of
the source (cid:12)le, so the occurrence of one of these confusable (cid:12)les leads
to a failure (though in applications such as image compression, lossy
compression is viewed as satisfactory). We’ll denote by (cid:14) the probability
that the source string is one of the confusable (cid:12)les, so a lossy compressor
has a probability (cid:14) of failure. If (cid:14) can be made very small then a lossy
compressor may be practically useful.

2. A lossless compressor maps all (cid:12)les to di(cid:11)erent encodings; if it shortens
some (cid:12)les, it necessarily makes others longer. We try to design the
compressor so that the probability that a (cid:12)le is lengthened is very small,
and the probability that it is shortened is large.

In this chapter we discuss a simple lossy compressor. In subsequent chapters
we discuss lossless compression methods.

4.3 Information content de(cid:12)ned in terms of lossy compression

Whichever type of compressor we construct, we need somehow to take into
account the probabilities of the di(cid:11)erent outcomes.
Imagine comparing the
information contents of two text (cid:12)les { one in which all 128 ASCII characters

1http://sunsite.org.uk/public/usenet/news-faqs/comp.compression/

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.3: Information content de(cid:12)ned in terms of lossy compression

75

are used with equal probability, and one in which the characters are used with
their frequencies in English text. Can we de(cid:12)ne a measure of information
content that distinguishes between these two (cid:12)les? Intuitively, the latter (cid:12)le
contains less information per character because it is more predictable.

One simple way to use our knowledge that some symbols have a smaller
probability is to imagine recoding the observations into a smaller alphabet
{ thus losing the ability to encode some of the more improbable symbols {
and then measuring the raw bit content of the new alphabet. For example,
we might take a risk when compressing English text, guessing that the most
infrequent characters won’t occur, and make a reduced ASCII code that omits
the characters f !, @, #, %, ^, *, ~, <, >, /, \, _, {, }, [, ], | g, thereby reducing
the size of the alphabet by seventeen. The larger the risk we are willing to
take, the smaller our (cid:12)nal alphabet becomes.

We introduce a parameter (cid:14) that describes the risk we are taking when
using this compression method: (cid:14) is the probability that there will be no
name for an outcome x.

Example 4.6. Let

4 ; 1

4 ; 1

(4.17)

AX =f a; b; c; d; e; f; g; h g;
64 g:

and PX =f 1

64 ; 1

64 ; 1

64 ; 1

4 ; 3

16 ; 1

The raw bit content of this ensemble is 3 bits, corresponding to 8 binary
names. But notice that P (x 2 fa; b; c; dg) = 15=16. So if we are willing
to run a risk of (cid:14) = 1=16 of not having a name for x, then we can get
by with four names { half as many names as are needed if every x 2 AX
has a name.

(cid:14) = 0

(cid:14) = 1=16

x c(x)

a
b
c
d
e
f
g
h

000
001
010
011
100
101
110
111

x

a
b
c
d
e
f
g
h

c(x)

00
01
10
11
(cid:0)
(cid:0)
(cid:0)
(cid:0)

Table 4.5 shows binary names that could be given to the di(cid:11)erent out-
comes in the cases (cid:14) = 0 and (cid:14) = 1=16. When (cid:14) = 0 we need 3 bits to
encode the outcome; when (cid:14) = 1=16 we need only 2 bits.

Table 4.5. Binary names for the
outcomes, for two failure
probabilities (cid:14).

Let us now formalize this idea. To make a compression strategy with risk
(cid:14), we make the smallest possible subset S(cid:14) such that the probability that x is
not in S(cid:14) is less than or equal to (cid:14), i.e., P (x 62 S(cid:14)) (cid:20) (cid:14). For each value of (cid:14)
we can then de(cid:12)ne a new measure of information content { the log of the size
of this smallest subset S(cid:14).
[In ensembles in which several elements have the
same probability, there may be several smallest subsets that contain di(cid:11)erent
elements, but all that matters is their sizes (which are equal), so we will not
dwell on this ambiguity.]
The smallest (cid:14)-su(cid:14)cient subset S(cid:14) is the smallest subset of AX satisfying
(4.18)

P (x 2 S(cid:14)) (cid:21) 1 (cid:0) (cid:14):

The subset S(cid:14) can be constructed by ranking the elements of AX in order of
decreasing probability and adding successive elements starting from the most
probable elements until the total probability is (cid:21) (1(cid:0)(cid:14)).
We can make a data compression code by assigning a binary name to each
element of the smallest su(cid:14)cient subset. This compression scheme motivates
the following measure of information content:

The essential bit content of X is:

H(cid:14)(X) = log2 jS(cid:14)j:

(4.19)

Note that H0(X) is the special case of H(cid:14)(X) with (cid:14) = 0 (if P (x) > 0 for all
x 2 AX). [Caution: do not confuse H0(X) and H(cid:14)(X) with the function H2(p)
displayed in (cid:12)gure 4.1.]
Figure 4.6 shows H(cid:14)(X) for the ensemble of example 4.6 as a function of

(cid:14).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

S0

(cid:0)6

6

e,f,g,h

76

(a)

(b)

(cid:0)4

(cid:0)2:4

(cid:0)2

log2 P (x)
-

S 1

16

66

d

a,b,c

4 | The Source Coding Theorem

Figure 4.6. (a) The outcomes of X
(from example 4.6 (p.75)), ranked
by their probability. (b) The
essential bit content H(cid:14)(X). The
labels on the graph show the
smallest su(cid:14)cient set as a
function of (cid:14). Note H0(X) = 3
bits and H1=16(X) = 2 bits.

3

2.5

H(cid:14)(X)

2

1.5

1

0.5

0

0

{a,b,c,d,e,f,g,h}
{a,b,c,d,e,f,g}
{a,b,c,d,e,f}
{a,b,c,d,e}

{a,b,c,d}

{a,b,c}

{a,b}

{a}

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

(cid:14)

Extended ensembles

Is this compression method any more useful if we compress blocks of symbols
from a source?

We now turn to examples where the outcome x = (x1; x2; : : : ; xN ) is a
string of N independent identically distributed random variables from a single
ensemble X. We will denote by X N the ensemble (X1; X2; : : : ; XN ). Remem-
ber that entropy is additive for independent variables (exercise 4.2 (p.68)), so
H(X N ) = N H(X).

Example 4.7. Consider a string of N (cid:13)ips of a bent coin, x = (x1; x2; : : : ; xN ),
where xn 2 f0; 1g, with probabilities p0 = 0:9; p1 = 0:1. The most prob-
able strings x are those with most 0s. If r(x) is the number of 1s in x
then

pr(x)
1

:

P (x) = pN(cid:0)r(x)

0

(4.20)
To evaluate H(cid:14)(X N ) we must (cid:12)nd the smallest su(cid:14)cient subset S(cid:14). This
subset will contain all x with r(x) = 0; 1; 2; : : : ; up to some rmax((cid:14)) (cid:0) 1,
and some of the x with r(x) = rmax((cid:14)). Figures 4.7 and 4.8 show graphs
of H(cid:14)(X N ) against (cid:14) for the cases N = 4 and N = 10. The steps are the
values of (cid:14) at which jS(cid:14)j changes by 1, and the cusps where the slope of
the staircase changes are the points where rmax changes by 1.

Exercise 4.8.[2, p.86] What are the mathematical shapes of the curves between

the cusps?

For the examples shown in (cid:12)gures 4.6{4.8, H(cid:14)(X N ) depends strongly on
the value of (cid:14), so it might not seem a fundamental or useful de(cid:12)nition of
information content. But we will consider what happens as N , the number
of independent variables in X N , increases. We will (cid:12)nd the remarkable result
that H(cid:14)(X N ) becomes almost independent of (cid:14) { and for all (cid:14) it is very close
to N H(X), where H(X) is the entropy of one of the random variables.

Figure 4.9 illustrates this asymptotic tendency for the binary ensemble of
N H(cid:14)(X N ) becomes an increasingly (cid:13)at function,

example 4.7. As N increases, 1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.3: Information content de(cid:12)ned in terms of lossy compression

77

(cid:0)14

(cid:0)12

(cid:0)10

(cid:0)8

S0:01

(cid:0)6

S0:1

(cid:0)4

(cid:0)2

log2 P (x)

0

-

6
1111

(a)

6

6

6

1101; 1011; : : :

0110; 1010; : : :

0010; 0001; : : :

6
0000

Figure 4.7. (a) The sixteen
outcomes of the ensemble X 4 with
p1 = 0:1, ranked by probability.
(b) The essential bit content
H(cid:14)(X 4). The upper schematic
diagram indicates the strings’
probabilities by the vertical lines’
lengths (not to scale).

(b)

4

3.5

H(cid:14)(X 4)

3

2.5

2

1.5

1

0.5

0

0

H(cid:14)(X 10)

10

8

6

4

2

0

N=4

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

(cid:14)

N=10

Figure 4.8. H(cid:14)(X N ) for N = 10
binary variables with p1 = 0:1.

0

0.2

0.4

0.6

0.8

1

(cid:14)

1
N H(cid:14)(X N )

1

0.8

0.6

0.4

0.2

0

0

N=10
N=210
N=410
N=610
N=810
N=1010

N H(cid:14)(X N ) for

Figure 4.9. 1
N = 10; 210; : : : ; 1010 binary
variables with p1 = 0:1.

0.2

0.4

0.6

0.8

1

(cid:14)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

78

4 | The Source Coding Theorem

Figure 4.10. The top 15 strings
are samples from X 100, where
p1 = 0:1 and p0 = 0:9. The
bottom two are the most and
least probable strings in this
ensemble. The (cid:12)nal column shows
the log-probabilities of the
random strings, which may be
compared with the entropy
H(X 100) = 46:9 bits.

x

...1...................1.....1....1.1.......1........1...........1.....................1.......11...

.....1..1..1...............111...................1...............1.........1.1...1...1.............1

.........1..........1.....1......1..........1....1..............................................1...

...11...........1...1.....1.1......1..........1....1...1.....1............1.........................

..............1......1.........1.1.......1..........1............1...1......................1.......

.....1........1.......1...1............1............1...........1......1..11........................

1.1...1................1.......................11.1..1............................1.....1..1.11.....

........1....1..1...1....11..1.1.........11.........................1...1.1..1...1................1.

......................1.....1.....1.......1....1.........1.....................................1....

log2(P (x))
(cid:0)50.1
(cid:0)37.3
(cid:0)65.9
(cid:0)56.4
(cid:0)53.2
(cid:0)43.7
(cid:0)46.8
(cid:0)56.4
(cid:0)37.3
(cid:0)43.7
(cid:0)56.4
(cid:0)37.3
(cid:0)56.4
(cid:0)59.5
(cid:0)46.8
(cid:0)15.2
1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 (cid:0)332.1

......1........................1..............1.....1..1.1.1..1...................................1.

1.......................1..........1...1...................1....1....1........1..11..1.1...1........

...........11.1.........1................1......1.....................1.............................

.1..........1...1.1.............1.......11...........1.1...1..............1.............11..........

......1...1..1.....1..11.1.1.1...1.....................1............1.............1..1..............

............11.1......1....1..1............................1.......1..............1.......1.........

....................................................................................................

except for tails close to (cid:14) = 0 and 1. As long as we are allowed a tiny
probability of error (cid:14), compression down to N H bits is possible. Even if we
are allowed a large probability of error, we still can compress only down to
N H bits. This is the source coding theorem.

Theorem 4.1 Shannon’s source coding theorem. Let X be an ensemble with
entropy H(X) = H bits. Given (cid:15) > 0 and 0 < (cid:14) < 1, there exists a positive
integer N0 such that for N > N0,

1
N

(cid:12)(cid:12)(cid:12)(cid:12)

H(cid:14)(X N ) (cid:0) H(cid:12)(cid:12)(cid:12)(cid:12)

< (cid:15):

(4.21)

4.4 Typicality

Why does increasing N help? Let’s examine long strings from X N . Table 4.10
shows (cid:12)fteen samples from X N for N = 100 and p1 = 0:1. The probability
of a string x that contains r 1s and N(cid:0)r 0s is

P (x) = pr

1(1 (cid:0) p1)N(cid:0)r:

The number of strings that contain r 1s is

n(r) =(cid:18)N
r(cid:19):

So the number of 1s, r, has a binomial distribution:

P (r) =(cid:18)N

r(cid:19)pr

1(1 (cid:0) p1)N(cid:0)r:

(4.22)

(4.23)

(4.24)

These functions are shown in (cid:12)gure 4.11. The mean of r is N p1, and its

standard deviation is pN p1(1 (cid:0) p1) (p.1). If N is 100 then
r (cid:24) N p1 (cid:6)pN p1(1 (cid:0) p1) ’ 10 (cid:6) 3:

(4.25)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.4: Typicality

79

r(cid:1)
n(r) =(cid:0)N

P (x) = pr

1(1 (cid:0) p1)N (cid:0)r

log2 P (x)

r(cid:1)pr
n(r)P (x) =(cid:0)N
1(1 (cid:0) p1)N (cid:0)r

N = 100

N = 1000

3e+299

2.5e+299

2e+299

1.5e+299

0 10 20 30 40 50 60 70 80 90 100

2e-05

1e-05

0

0

1

2

3

4

5

0 10 20 30 40 50 60 70 80 90 100

T

0 10 20 30 40 50 60 70 80 90 100

0 10 20 30 40 50 60 70 80 90 100

1e+299

5e+298

0

0

-500

-1000

-1500

-2000

-2500

-3000

-3500

0.045
0.04
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0

0 100 200 300 400 500 600 700 800 9001000

T

0 100 200 300 400 500 600 700 800 9001000

0 100 200 300 400 500 600 700 800 9001000

1.2e+29

1e+29

8e+28

6e+28

4e+28

2e+28

0

2e-05

1e-05

0

0

-50

-100

-150

-200

-250

-300

-350

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

r

r

Figure 4.11. Anatomy of the typical set T . For p1 = 0:1 and N = 100 and N = 1000, these graphs
show n(r), the number of strings containing r 1s; the probability P (x) of a single string
that contains r 1s; the same probability on a log scale; and the total probability n(r)P (x) of
all strings that contain r 1s. The number r is on the horizontal axis. The plot of log2 P (x)
also shows by a dotted line the mean value of log2 P (x) = (cid:0)N H2(p1), which equals (cid:0)46:9
when N = 100 and (cid:0)469 when N = 1000. The typical set includes only the strings that
have log2 P (x) close to this value. The range marked T shows the set TN (cid:12) (as de(cid:12)ned in
section 4.4) for N = 100 and (cid:12) = 0:29 (left) and N = 1000, (cid:12) = 0:09 (right).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

80

If N = 1000 then

4 | The Source Coding Theorem

(4.26)

r (cid:24) 100 (cid:6) 10:

Notice that as N gets bigger, the probability distribution of r becomes more
concentrated, in the sense that while the range of possible values of r grows
as N , the standard deviation of r grows only as pN . That r is most likely to
fall in a small range of values implies that the outcome x is also most likely to
fall in a corresponding small subset of outcomes that we will call the typical
set.

De(cid:12)nition of the typical set
Let us de(cid:12)ne typicality for an arbitrary ensemble X with alphabet AX. Our
de(cid:12)nition of a typical string will involve the string’s probability. A long string
of N symbols will usually contain about p1N occurrences of the (cid:12)rst symbol,
p2N occurrences of the second, etc. Hence the probability of this string is
roughly

P (x)typ = P (x1)P (x2)P (x3) : : : P (xN ) ’ p(p1N )

1

p(p2N )
2

: : : p(pI N )

I

(4.27)

so that the information content of a typical string is

log2

1

P (x) ’ NXi

pi log2

1
pi

= N H:

(4.28)

1/P (x), which is the information content of x, is
So the random variable log2
very likely to be close in value to N H. We build our de(cid:12)nition of typicality
on this observation.

We de(cid:12)ne the typical elements of AN

X to be those elements that have prob-
ability close to 2(cid:0)N H . (Note that the typical set, unlike the smallest su(cid:14)cient
subset, does not include the most probable elements of AN
X, but we will show
that these most probable elements contribute negligible probability.)

We introduce a parameter (cid:12) that de(cid:12)nes how close the probability has to
be to 2(cid:0)N H for an element to be ‘typical’. We call the set of typical elements
the typical set, TN (cid:12):

X :(cid:12)(cid:12)(cid:12)(cid:12)
TN (cid:12) (cid:17)(cid:26)x 2 AN

1
N

log2

1

P (x) (cid:0) H(cid:12)(cid:12)(cid:12)(cid:12)

< (cid:12)(cid:27) :

(4.29)

We will show that whatever value of (cid:12) we choose, the typical set contains

almost all the probability as N increases.

This important result is sometimes called the ‘asymptotic equipartition’

principle.

‘Asymptotic equipartition’ principle. For an ensemble of N independent
identically distributed (i.i.d.) random variables X N (cid:17) (X1; X2; : : : ; XN ),
with N su(cid:14)ciently large, the outcome x = (x1; x2; : : : ; xN ) is almost
certain to belong to a subset of AN
X having only 2N H(X) members, each
having probability ‘close to’ 2(cid:0)N H(X).

Notice that if H(X) < H0(X) then 2N H(X) is a tiny fraction of the number
of possible outcomes jAN

Xj = jAXjN = 2N H0(X):

The term equipartition is chosen to describe the idea that the members of
the typical set have roughly equal probability.
[This should not be taken too
literally, hence my use of quotes around ‘asymptotic equipartition’; see page
83.]

A second meaning for equipartition, in thermal physics, is the idea that each
degree of freedom of a classical system has equal average energy, 1
2 kT . This
second meaning is not intended here.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.5: Proofs

81

log2 P (x)

-

(cid:0)N H(X)

TN (cid:12)

6
1111111111110. . . 11111110111

66

6

6

0000100000010. . . 00001000010

0100000001000. . . 00010000000

0001000000000. . . 00000000000

0000000000000. . . 00000000000

Figure 4.12. Schematic diagram
showing all strings in the ensemble
X N ranked by their probability,
and the typical set TN (cid:12).

The ‘asymptotic equipartition’ principle is equivalent to:

Shannon’s source coding theorem (verbal statement). N i.i.d.

ran-
dom variables each with entropy H(X) can be compressed into more
than N H(X) bits with negligible risk of information loss, as N ! 1;
conversely if they are compressed into fewer than N H(X) bits it is vir-
tually certain that information will be lost.

These two theorems are equivalent because we can de(cid:12)ne a compression algo-
rithm that gives a distinct name of length N H(X) bits to each x in the typical
set.

4.5 Proofs

This section may be skipped if found tough going.

The law of large numbers

Our proof of the source coding theorem uses the law of large numbers.

Mean and variance of a real random variable are E[u] = (cid:22)u = Pu P (u)u

u = E[(u (cid:0) (cid:22)u)2] =Pu P (u)(u (cid:0) (cid:22)u)2:

and var(u) = (cid:27)2

Technical note: strictly I am assuming here that u is a function u(x)
of a sample x from a (cid:12)nite discrete ensemble X. Then the summations

Pu P (u)f (u) should be written Px P (x)f (u(x)). This means that P (u)

is a (cid:12)nite sum of delta functions. This restriction guarantees that the
mean and variance of u do exist, which is not necessarily the case for
general P (u).

Chebyshev’s inequality 1. Let t be a non-negative real random variable,

and let (cid:11) be a positive real number. Then

P (t (cid:21) (cid:11)) (cid:20)

(cid:22)t
(cid:11)

:

(4.30)

Proof: P (t (cid:21) (cid:11)) = Pt(cid:21)(cid:11) P (t). We multiply each term by t=(cid:11) (cid:21) 1 and
obtain: P (t (cid:21) (cid:11)) (cid:20) Pt(cid:21)(cid:11) P (t)t=(cid:11): We add the (non-negative) missing
terms and obtain: P (t (cid:21) (cid:11)) (cid:20)Pt P (t)t=(cid:11) = (cid:22)t=(cid:11).

2

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

82

4 | The Source Coding Theorem

Chebyshev’s inequality 2. Let x be a random variable, and let (cid:11) be a

positive real number. Then

Proof: Take t = (x (cid:0) (cid:22)x)2 and apply the previous proposition.

P(cid:0)(x (cid:0) (cid:22)x)2 (cid:21) (cid:11)(cid:1) (cid:20) (cid:27)2

x=(cid:11):

(4.31)

2

Weak law of large numbers. Take x to be the average of N independent
random variables h1; : : : ; hN , having common mean (cid:22)h and common vari-
ance (cid:27)2

n=1 hn. Then

h: x = 1

N PN

P ((x (cid:0) (cid:22)h)2 (cid:21) (cid:11)) (cid:20) (cid:27)2

h=(cid:11)N:

(4.32)

2

Proof: obtained by showing that (cid:22)x = (cid:22)h and that (cid:27)2

x = (cid:27)2

h=N .

We are interested in x being very close to the mean ((cid:11) very small). No matter
how large (cid:27)2
h is, and no matter how small the required (cid:11) is, and no matter
how small the desired probability that (x (cid:0) (cid:22)h)2 (cid:21) (cid:11), we can always achieve it
by taking N large enough.

Proof of theorem 4.1 (p.78)

We apply the law of large numbers to the random variable 1
1
P (x) de(cid:12)ned
for x drawn from the ensemble X N . This random variable can be written as
the average of N information contents hn = log2(1=P (xn)), each of which is a
random variable with mean H = H(X) and variance (cid:27)2 (cid:17) var[log2(1=P (xn))].
(Each term hn is the Shannon information content of the nth outcome.)

N log2

We again de(cid:12)ne the typical set with parameters N and (cid:12) thus:

TN (cid:12) =(x 2 AN

X :(cid:20) 1

N

log2

1

P (x) (cid:0) H(cid:21)2

< (cid:12)2) :

For all x 2 TN (cid:12), the probability of x satis(cid:12)es

2(cid:0)N (H+(cid:12)) < P (x) < 2(cid:0)N (H(cid:0)(cid:12)):

And by the law of large numbers,

P (x 2 TN (cid:12)) (cid:21) 1 (cid:0)

(cid:27)2
(cid:12)2N

:

(4.33)

(4.34)

(4.35)

We have thus proved the ‘asymptotic equipartition’ principle. As N increases,
the probability that x falls in TN (cid:12) approaches 1, for any (cid:12). How does this
result relate to source coding?

We must relate TN (cid:12) to H(cid:14)(X N ). We will show that for any given (cid:14) there

is a su(cid:14)ciently big N such that H(cid:14)(X N ) ’ N H.
Part 1: 1

N H(cid:14)(X N ) < H + (cid:15).

The set TN (cid:12) is not the best subset for compression. So the size of TN (cid:12) gives
an upper bound on H(cid:14). We show how small H(cid:14)(X N ) must be by calculating
how big TN (cid:12) could possibly be. We are free to set (cid:12) to any convenient value.
The smallest possible probability that a member of TN (cid:12) can have is 2(cid:0)N (H+(cid:12)),
and the total probability contained by TN (cid:12) can’t be any bigger than 1. So

jTN (cid:12)j 2(cid:0)N (H+(cid:12)) < 1;
that is, the size of the typical set is bounded by
jTN (cid:12)j < 2N (H+(cid:12)):

If we set (cid:12) = (cid:15) and N0 such that
TN (cid:12) becomes a witness to the fact that H(cid:14)(X N ) (cid:20) log2 jTN (cid:12)j < N (H + (cid:15)).

(4.37)
(cid:15)2N0 (cid:20) (cid:14), then P (TN (cid:12)) (cid:21) 1 (cid:0) (cid:14), and the set

(cid:27)2

(4.36)

1
N

H(cid:14)(X N )

H0(X)

H + (cid:15)

H
H (cid:0) (cid:15)

0

1

(cid:14)

Figure 4.13. Schematic illustration
of the two parts of the theorem.
Given any (cid:14) and (cid:15), we show that
for large enough N , 1
N H(cid:14)(X N )
lies (1) below the line H + (cid:15) and
(2) above the line H (cid:0) (cid:15).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

83

’$
&%

’$
&%

CCO
C
S0 \ TN (cid:12)

S0 \ TN (cid:12)

S0

@@I

4.6: Comments

Part 2: 1

N H(cid:14)(X N ) > H (cid:0) (cid:15).

Imagine that someone claims this second part is not so { that, for any N ,
the smallest (cid:14)-su(cid:14)cient subset S(cid:14) is smaller than the above inequality would
allow. We can make use of our typical set to show that they must be mistaken.
Remember that we are free to set (cid:12) to any value we choose. We will set
(cid:12) = (cid:15)=2, so that our task is to prove that a subset S0 having jS0j (cid:20) 2N (H(cid:0)2(cid:12))
and achieving P (x 2 S0) (cid:21) 1 (cid:0) (cid:14) cannot exist (for N greater than an N0 that
we will specify).
So, let us consider the probability of falling in this rival smaller subset S0.

The probability of the subset S0 is

P (x 2 S0) = P (x 2 S0\TN (cid:12)) + P (x 2 S0\TN (cid:12));

(4.38)

where TN (cid:12) denotes the complement fx 62 TN (cid:12)g. The maximum value of
the (cid:12)rst term is found if S0 \ TN (cid:12) contains 2N (H(cid:0)2(cid:12)) outcomes all with the
maximum probability, 2(cid:0)N (H(cid:0)(cid:12)). The maximum value the second term can
have is P (x 62 TN (cid:12)). So:

P (x 2 S0) (cid:20) 2N (H(cid:0)2(cid:12)) 2(cid:0)N (H(cid:0)(cid:12)) +

(cid:27)2
(cid:12)2N

= 2(cid:0)N (cid:12) +

(cid:27)2
(cid:12)2N

:

(4.39)

TN (cid:12)

We can now set (cid:12) = (cid:15)=2 and N0 such that P (x 2 S0) < 1 (cid:0) (cid:14), which shows
that S0 cannot satisfy the de(cid:12)nition of a su(cid:14)cient subset S(cid:14). Thus any subset
S0 with size jS0j (cid:20) 2N (H(cid:0)(cid:15)) has probability less than 1(cid:0) (cid:14), so by the de(cid:12)nition
of H(cid:14), H(cid:14)(X N ) > N (H (cid:0) (cid:15)).
N H(cid:14)(X N ) is essentially a constant
function of (cid:14), for 0 < (cid:14) < 1, as illustrated in (cid:12)gures 4.9 and 4.13.

Thus for large enough N , the function 1

2

4.6 Comments

The source coding theorem (p.78) has two parts, 1
1
N H(cid:14)(X N ) > H (cid:0) (cid:15). Both results are interesting.
The (cid:12)rst part tells us that even if the probability of error (cid:14) is extremely
small, the number of bits per symbol 1
N H(cid:14)(X N ) needed to specify a long
N -symbol string x with vanishingly small error probability does not have to
exceed H + (cid:15) bits. We need to have only a tiny tolerance for error, and the
number of bits required drops signi(cid:12)cantly from H0(X) to (H + (cid:15)).

N H(cid:14)(X N ) < H + (cid:15), and

What happens if we are yet more tolerant to compression errors? Part 2
tells us that even if (cid:14) is very close to 1, so that errors are made most of the
time, the average number of bits per symbol needed to specify x must still be
at least H (cid:0) (cid:15) bits. These two extremes tell us that regardless of our speci(cid:12)c
allowance for error, the number of bits per symbol needed to specify x is H
bits; no more and no less.

Caveat regarding ‘asymptotic equipartition’

I put the words ‘asymptotic equipartition’ in quotes because it is important
not to think that the elements of the typical set TN (cid:12) really do have roughly
the same probability as each other. They are similar in probability only in
1
P (x) are within 2N (cid:12) of each other. Now, as
the sense that their values of log2
(cid:12) is decreased, how does N have to increase, if we are to keep our bound on
the mass of the typical set, P (x 2 TN (cid:12)) (cid:21) 1 (cid:0) (cid:27)2
(cid:12)2N , constant? N must grow
as 1=(cid:12)2, so, if we write (cid:12) in terms of N as (cid:11)=pN , for some constant (cid:11), then

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

84

4 | The Source Coding Theorem

the most probable string in the typical set will be of order 2(cid:11)pN times greater
than the least probable string in the typical set. As (cid:12) decreases, N increases,
and this ratio 2(cid:11)pN grows exponentially. Thus we have ‘equipartition’ only in
a weak sense!

Why did we introduce the typical set?

The best choice of subset for block compression is (by de(cid:12)nition) S(cid:14), not a
typical set. So why did we bother introducing the typical set? The answer is,
we can count the typical set. We know that all its elements have ‘almost iden-
tical’ probability (2(cid:0)N H ), and we know the whole set has probability almost
1, so the typical set must have roughly 2N H elements. Without the help of
the typical set (which is very similar to S(cid:14)) it would have been hard to count
how many elements there are in S(cid:14).

4.7 Exercises

Weighing problems

. Exercise 4.9.[1 ] While some people, when they (cid:12)rst encounter the weighing
problem with 12 balls and the three-outcome balance (exercise 4.1
(p.66)), think that weighing six balls against six balls is a good (cid:12)rst
weighing, others say ‘no, weighing six against six conveys no informa-
tion at all’. Explain to the second group why they are both right and
wrong. Compute the information gained about which is the odd ball,
and the information gained about which is the odd ball and whether it is
heavy or light.

. Exercise 4.10.[2 ] Solve the weighing problem for the case where there are 39

balls of which one is known to be odd.

. Exercise 4.11.[2 ] You are given 16 balls, all of which are equal in weight except
for one that is either heavier or lighter. You are also given a bizarre two-
pan balance that can report only two outcomes: ‘the two sides balance’
or ‘the two sides do not balance’. Design a strategy to determine which
is the odd ball in as few uses of the balance as possible.

. Exercise 4.12.[2 ] You have a two-pan balance; your job is to weigh out bags of
(cid:13)our with integer weights 1 to 40 pounds inclusive. How many weights
do you need? [You are allowed to put weights on either pan. You’re only
allowed to put one (cid:13)our bag on the balance at a time.]

Exercise 4.13.[4, p.86]

(a) Is it possible to solve exercise 4.1 (p.66) (the weigh-
ing problem with 12 balls and the three-outcome balance) using a
sequence of three (cid:12)xed weighings, such that the balls chosen for the
second weighing do not depend on the outcome of the (cid:12)rst, and the
third weighing does not depend on the (cid:12)rst or second?

(b) Find a solution to the general N -ball weighing problem in which
exactly one of N balls is odd. Show that in W weighings, an odd
ball can be identi(cid:12)ed from among N = (3W (cid:0) 3)=2 balls.

Exercise 4.14.[3 ] You are given 12 balls and the three-outcome balance of exer-
cise 4.1; this time, two of the balls are odd; each odd ball may be heavy
or light, and we don’t know which. We want to identify the odd balls
and in which direction they are odd.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.7: Exercises

85

(a) Estimate how many weighings are required by the optimal strategy.

And what if there are three odd balls?

(b) How do your answers change if it is known that all the regular balls
weigh 100 g, that light balls weigh 99 g, and heavy ones weigh 110 g?

Source coding with a lossy compressor, with loss (cid:14)
. Exercise 4.15.[2, p.87] Let PX = f0:2; 0:8g. Sketch 1

(cid:14) for N = 1; 2 and 1000.

N H(cid:14)(X N ) as a function of

. Exercise 4.16.[2 ] Let PY = f0:5; 0:5g. Sketch 1

N = 1; 2; 3 and 100.

N H(cid:14)(Y N ) as a function of (cid:14) for

. Exercise 4.17.[2, p.87] (For physics students.) Discuss the relationship between
the proof of the ‘asymptotic equipartition’ principle and the equivalence
(for large systems) of the Boltzmann entropy and the Gibbs entropy.

Distributions that don’t obey the law of large numbers

The law of large numbers, which we used in this chapter, shows that the mean
of a set of N i.i.d. random variables has a probability distribution that becomes

narrower, with width / 1=pN , as N increases. However, we have proved

this property only for discrete random variables, that is, for real numbers
taking on a (cid:12)nite set of possible values. While many random variables with
continuous probability distributions also satisfy the law of large numbers, there
are important distributions that do not. Some continuous distributions do not
have a mean or variance.

. Exercise 4.18.[3, p.88] Sketch the Cauchy distribution

P (x) =

1
Z

1

x2 + 1

; x 2 ((cid:0)1;1):

(4.40)

What is its normalizing constant Z? Can you evaluate its mean or
variance?

Consider the sum z = x1 + x2, where x1 and x2 are independent random
variables from a Cauchy distribution. What is P (z)? What is the prob-
ability distribution of the mean of x1 and x2, (cid:22)x = (x1 + x2)=2? What is
the probability distribution of the mean of N samples from this Cauchy
distribution?

Other asymptotic properties

Exercise 4.19.[3 ] Cherno(cid:11) bound. We derived the weak law of large numbers
from Chebyshev’s inequality (4.30) by letting the random variable t in
the inequality P (t (cid:21) (cid:11)) (cid:20) (cid:22)t=(cid:11) be a function, t = (x(cid:0) (cid:22)x)2, of the random
variable x we were interested in.

Other useful inequalities can be obtained by using other functions. The
Cherno(cid:11) bound, which is useful for bounding the tails of a distribution,
is obtained by letting t = exp(sx).

Show that

and

P (x (cid:21) a) (cid:20) e(cid:0)sag(s);

for any s > 0

P (x (cid:20) a) (cid:20) e(cid:0)sag(s);

for any s < 0

(4.41)

(4.42)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

86

4 | The Source Coding Theorem

where g(s) is the moment-generating function of x,

g(s) =Xx

P (x) esx:

(4.43)

Curious functions related to p log 1=p

Exercise 4.20.[4, p.89] This exercise has no purpose at all; it’s included for the

enjoyment of those who like mathematical curiosities.

Sketch the function

f (x) = xxxxx(cid:1)(cid:1)(cid:1)

(4.44)
for x (cid:21) 0. Hint: Work out the inverse function to f { that is, the function
g(y) such that if x = g(y) then y = f (x) { it’s closely related to p log 1=p.

4.8 Solutions

Solution to exercise 4.2 (p.68). Let P (x; y) = P (x)P (y). Then

H(X; Y ) = Xxy
= Xxy
= Xx

= H(X) + H(Y ):

P (x)P (y) log

1

P (x)P (y)

P (x)P (y) log

1

P (x)

P (x)P (y) log

1

P (y)

+Xxy

P (x) log

1

P (x)

P (y) log

1

P (y)

+Xy

(4.45)

(4.46)

(4.47)

(4.48)

Solution to exercise 4.4 (p.73). An ASCII (cid:12)le can be reduced in size by a
factor of 7/8. This reduction could be achieved by a block code that maps
8-byte blocks into 7-byte blocks by copying the 56 information-carrying bits
into 7 bytes, and ignoring the last bit of every character.

Solution to exercise 4.5 (p.74). The pigeon-hole principle states: you can’t
put 16 pigeons into 15 holes without using one of the holes twice.

Similarly, you can’t give AX outcomes unique binary names of some length
l shorter than log2 jAXj bits, because there are only 2l such binary names,
and l < log2 jAXj implies 2l < jAXj, so at least two di(cid:11)erent inputs to the
compressor would compress to the same output (cid:12)le.

Solution to exercise 4.8 (p.76). Between the cusps, all the changes in proba-
bility are equal, and the number of elements in T changes by one at each step.
So H(cid:14) varies logarithmically with ((cid:0)(cid:14)).
Solution to exercise 4.13 (p.84). This solution was found by Dyson and Lyness
in 1946 and presented in the following elegant form by John Conway in 1999.
Be warned: the symbols A, B, and C are used to name the balls, to name the
pans of the balance, to name the outcomes, and to name the possible states
of the odd ball!

(a) Label the 12 balls by the sequences

AAB ABA ABB ABC BBC BCA BCB BCC CAA CAB CAC CCA

and in the

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.8: Solutions

87

1st
2nd weighings put AAB CAA CAB CAC in pan A, ABA ABB ABC BBC in pan B.
3rd

AAB ABA ABB ABC

BBC BCA BCB BCC

ABA BCA CAA CCA

AAB ABB BCB CAB

Now in a given weighing, a pan will either end up in the

(cid:15) Canonical position (C) that it assumes when the pans are balanced,

or

(cid:15) Above that position (A), or
(cid:15) Below it (B),

so the three weighings determine for each pan a sequence of three of
these letters.

If both sequences are CCC, then there’s no odd ball. Otherwise, for just
one of the two pans, the sequence is among the 12 above, and names
the odd ball, whose weight is Above or Below the proper one according
as the pan is A or B.

(b) In W weighings the odd ball can be identi(cid:12)ed from among

N = (3W (cid:0) 3)=2

(4.49)

balls in the same way, by labelling them with all the non-constant se-
quences of W letters from A, B, C whose (cid:12)rst change is A-to-B or B-to-C
or C-to-A, and at the wth weighing putting those whose wth letter is A
in pan A and those whose wth letter is B in pan B.

Solution to exercise 4.15 (p.85). The curves 1
N = 1; 2 and 1000 are shown in (cid:12)gure 4.14. Note that H2(0:2) = 0:72 bits.

N H(cid:14)(X N ) as a function of (cid:14) for

N=1
N=2
N=1000

N = 1

N = 2

(cid:14)

1
N H(cid:14)(X)

2H(cid:14)(X)

(cid:14)

1
N H(cid:14)(X)

2H(cid:14) (X)

0{0.2

0.2{1

1
0

2
1

0{0.04
0.04{0.2
0.2{0.36
0.36{1

1

0.79
0.5
0

4
3
2
1

Figure 4.14. 1
N H(cid:14)(X) (vertical
axis) against (cid:14) (horizontal), for
N = 1; 2; 100 binary variables
with p1 = 0:4.

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

Solution to exercise 4.17 (p.85). The Gibbs entropy is kBPi pi ln 1

, where i
runs over all states of the system. This entropy is equivalent (apart from the
factor of kB) to the Shannon entropy of the ensemble.

pi

Whereas the Gibbs entropy can be de(cid:12)ned for any ensemble, the Boltz-
mann entropy is only de(cid:12)ned for microcanonical ensembles, which have a
probability distribution that is uniform over a set of accessible states. The
Boltzmann entropy is de(cid:12)ned to be SB = kB ln (cid:10) where (cid:10) is the number of ac-
cessible states of the microcanonical ensemble. This is equivalent (apart from
the factor of kB) to the perfect information content H0 of that constrained
ensemble. The Gibbs entropy of a microcanonical ensemble is trivially equal
to the Boltzmann entropy.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

88

4 | The Source Coding Theorem

We now consider a thermal distribution (the canonical ensemble), where

the probability of a state x is

P (x) =

1
Z

exp(cid:18)(cid:0)

E(x)

kBT (cid:19) :

(4.50)

With this canonical ensemble we can associate a corresponding microcanonical
ensemble, an ensemble with total energy (cid:12)xed to the mean energy of the
canonical ensemble ((cid:12)xed to within some precision (cid:15)). Now, (cid:12)xing the total
energy to a precision (cid:15) is equivalent to (cid:12)xing the value of ln 1/P (x) to within
(cid:15)kBT . Our de(cid:12)nition of the typical set TN (cid:12) was precisely that it consisted
of all elements that have a value of log P (x) very close to the mean value of
log P (x) under the canonical ensemble, (cid:0)N H(X). Thus the microcanonical
ensemble is equivalent to a uniform distribution over the typical set of the
canonical ensemble.

Our proof of the ‘asymptotic equipartition’ principle thus proves { for the
case of a system whose energy is separable into a sum of independent terms
{ that the Boltzmann entropy of the microcanonical ensemble is very close
(for large N ) to the Gibbs entropy of the canonical ensemble, if the energy of
the microcanonical ensemble is constrained to equal the mean energy of the
canonical ensemble.

Solution to exercise 4.18 (p.85). The normalizing constant of the Cauchy dis-
tribution

P (x) =

1
Z

1

x2 + 1

is

Z =Z 1

(cid:0)1

dx

1

x2 + 1

=(cid:2)tan(cid:0)1x(cid:3)1

(cid:0)1

=

(cid:25)

2 (cid:0) (cid:0)(cid:25)

2

= (cid:25):

(4.51)

The mean and variance of this distribution are both unde(cid:12)ned. (The distribu-
tion is symmetrical about zero, but this does not imply that its mean is zero.
The mean is the value of a divergent integral.) The sum z = x1 + x2, where
x1 and x2 both have Cauchy distributions, has probability density given by
the convolution

P (z) =

1

(cid:25)2 Z 1

(cid:0)1

dx1

1
x2
1 + 1

1

(z (cid:0) x1)2 + 1

;

(4.52)

which after a considerable labour using standard methods gives

P (z) =

1
(cid:25)2 2

(cid:25)

z2 + 4

=

2
(cid:25)

1

z2 + 22 ;

(4.53)

which we recognize as a Cauchy distribution with width parameter 2 (where
the original distribution has width parameter 1). This implies that the mean
of the two points, (cid:22)x = (x1 + x2)=2 = z=2, has a Cauchy distribution with
width parameter 1. Generalizing, the mean of N samples from a Cauchy
distribution is Cauchy-distributed with the same parameters as the individual
samples. The probability distribution of the mean does not become narrower
as 1=pN .

The central-limit theorem does not apply to the Cauchy distribution, be-

cause it does not have a (cid:12)nite variance.

An alternative neat method for getting to equation (4.53) makes use of the
Fourier transform of the Cauchy distribution, which is a biexponential e(cid:0)j!j.
Convolution in real space corresponds to multiplication in Fourier space, so
the Fourier transform of z is simply e(cid:0)j2!j. Reversing the transform, we obtain
equation (4.53).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

4.8: Solutions

89

Solution to exercise 4.20 (p.86). The function f (x) has inverse function

Note

g(y) = y1=y:

log g(y) = 1=y log y:

(4.54)

(4.55)

I obtained a tentative graph of f (x) by plotting g(y) with y along the vertical
axis and g(y) along the horizontal axis. The resulting graph suggests that
f (x) is single valued for x 2 (0; 1), and looks surprisingly well-behaved and
ordinary; for x 2 (1; e1=e), f (x) is two-valued. f (p2) is equal both to 2 and
4. For x > e1=e (which is about 1.44), f (x) is in(cid:12)nite. However, it might be
argued that this approach to sketching f (x) is only partly valid, if we de(cid:12)ne f
as the limit of the sequence of functions x, xx, xxx
; : : :; this sequence does not
have a limit for 0 (cid:20) x (cid:20) (1=e)e ’ 0:07 on account of a pitchfork bifurcation
at x = (1=e)e; and for x 2 (1; e1=e), the sequence’s limit is single-valued { the
lower of the two values sketched in the (cid:12)gure.

50

40

30

20

10

0

5

4

3

2

1

0

0

0.2

0.4

0.6

0.8

1

1.2

1.4

0

0.2

0.4

0.6

0.8

1

1.2

1.4

0.5

0.4

0.3

0.2

0.1

0

0

Figure 4.15. f (x) = xxx
;
at three di(cid:11)erent scales.

x

0.2

(cid:1)

(cid:1)

(cid:1)

x

shown

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 5

In the last chapter, we saw a proof of the fundamental status of the entropy
as a measure of average information content. We de(cid:12)ned a data compression
scheme using (cid:12)xed length block codes, and proved that as N increases, it is
possible to encode N i.i.d. variables x = (x1; : : : ; xN ) into a block of N (H(X)+
(cid:15)) bits with vanishing probability of error, whereas if we attempt to encode
X N into N (H(X) (cid:0) (cid:15)) bits, the probability of error is virtually 1.
We thus veri(cid:12)ed the possibility of data compression, but the block coding
de(cid:12)ned in the proof did not give a practical algorithm. In this chapter and
the next, we study practical data compression algorithms. Whereas the last
chapter’s compression scheme used large blocks of (cid:12)xed size and was lossy,
in the next chapter we discuss variable-length compression schemes that are
practical for small block sizes and that are not lossy.

Imagine a rubber glove (cid:12)lled with water. If we compress two (cid:12)ngers of the
glove, some other part of the glove has to expand, because the total volume
of water is constant. (Water is essentially incompressible.) Similarly, when
we shorten the codewords for some outcomes, there must be other codewords
that get longer, if the scheme is not lossy. In this chapter we will discover the
information-theoretic equivalent of water volume.

Before reading Chapter 5, you should have worked on exercise 2.26 (p.37).

We will use the following notation for intervals:

x 2 [1; 2) means that x (cid:21) 1 and x < 2;
x 2 (1; 2] means that x > 1 and x (cid:20) 2.

90

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5

Symbol Codes

In this chapter, we discuss variable-length symbol codes, which encode one
source symbol at a time, instead of encoding huge strings of N source sym-
bols. These codes are lossless: unlike the last chapter’s block codes, they are
guaranteed to compress and decompress without any errors; but there is a
chance that the codes may sometimes produce encoded strings longer than
the original source string.

The idea is that we can achieve compression, on average, by assigning
shorter encodings to the more probable outcomes and longer encodings to the
less probable.

The key issues are:

What are the implications if a symbol code is lossless? If some code-
words are shortened, by how much do other codewords have to be length-
ened?

Making compression practical. How can we ensure that a symbol code is

easy to decode?

Optimal symbol codes. How should we assign codelengths to achieve the

best compression, and what is the best achievable compression?

We again verify the fundamental status of the Shannon information content

and the entropy, proving:

Source coding theorem (symbol codes). There exists a variable-length
encoding C of an ensemble X such that the average length of an en-
coded symbol, L(C; X), satis(cid:12)es L(C; X) 2 [H(X); H(X) + 1).
The average length is equal to the entropy H(X) only if the codelength
for each outcome is equal to its Shannon information content.

We will also de(cid:12)ne a constructive procedure, the Hu(cid:11)man coding algorithm,
that produces optimal symbol codes.

Notation for alphabets. AN denotes the set of ordered N -tuples of ele-
ments from the set A, i.e., all strings of length N . The symbol A+ will
denote the set of all strings of (cid:12)nite length composed of elements from
the set A.

Example 5.1. f0; 1g3 = f000; 001; 010; 011; 100; 101; 110; 111g.
Example 5.2. f0; 1g+ = f0; 1; 00; 01; 10; 11; 000; 001; : : :g.

91

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5 | Symbol Codes

92

5.1 Symbol codes

A (binary) symbol code C for an ensemble X is a mapping from the range
of x, AX =fa1; : : : ; aIg, to f0; 1g+. c(x) will denote the codeword cor-
responding to x, and l(x) will denote its length, with li = l(ai).
The extended code C + is a mapping from A+
concatenation, without punctuation, of the corresponding codewords:

X to f0; 1g+ obtained by

c+(x1x2 : : : xN ) = c(x1)c(x2) : : : c(xN ):

(5.1)

[The term ‘mapping’ here is a synonym for ‘function’.]

Example 5.3. A symbol code for the ensemble X de(cid:12)ned by

AX = f a; b; c; d g;
PX = f 1/2; 1/4; 1/8; 1/8 g;

(5.2)

is C0, shown in the margin.

Using the extended code, we may encode acdbac as

c+(acdbac) = 100000100001010010000010:

(5.3)

C0:

ai

c(ai)

a
b
c
d

1000
0100
0010
0001

li

4
4
4
4

There are basic requirements for a useful symbol code. First, any encoded
string must have a unique decoding. Second, the symbol code must be easy to
decode. And third, the code should achieve as much compression as possible.

Any encoded string must have a unique decoding

A code C(X) is uniquely decodeable if, under the extended code C +, no

two distinct strings have the same encoding, i.e.,

8 x; y 2 A+

X; x 6= y ) c+(x) 6= c+(y):

(5.4)

The code C0 de(cid:12)ned above is an example of a uniquely decodeable code.

The symbol code must be easy to decode

A symbol code is easiest to decode if it is possible to identify the end of a
codeword as soon as it arrives, which means that no codeword can be a pre(cid:12)x
of another codeword. [A word c is a pre(cid:12)x of another word d if there exists a
tail string t such that the concatenation ct is identical to d. For example, 1 is
a pre(cid:12)x of 101, and so is 10.]

We will show later that we don’t lose any performance if we constrain our

symbol code to be a pre(cid:12)x code.

A symbol code is called a pre(cid:12)x code if no codeword is a pre(cid:12)x of any

other codeword.

A pre(cid:12)x code is also known as an instantaneous or self-punctuating code,
because an encoded string can be decoded from left to right without
looking ahead to subsequent codewords. The end of a codeword is im-
mediately recognizable. A pre(cid:12)x code is uniquely decodeable.

Pre(cid:12)x codes are also known as ‘pre(cid:12)x-free codes’ or ‘pre(cid:12)x condition codes’.

Pre(cid:12)x codes correspond to trees, as illustrated in the margin of the next page.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.1: Symbol codes

93

Example 5.4. The code C1 = f0; 101g is a pre(cid:12)x code because 0 is not a pre(cid:12)x

of 101, nor is 101 a pre(cid:12)x of 0.

Example 5.5. Let C2 = f1; 101g. This code is not a pre(cid:12)x code because 1 is a

pre(cid:12)x of 101.

Example 5.6. The code C3 = f0; 10; 110; 111g is a pre(cid:12)x code.
Example 5.7. The code C4 = f00; 01; 10; 11g is a pre(cid:12)x code.
Exercise 5.8.[1, p.104] Is C2 uniquely decodeable?

Example 5.9. Consider exercise 4.1 (p.66) and (cid:12)gure 4.2 (p.69). Any weighing
strategy that identi(cid:12)es the odd ball and whether it is heavy or light can
be viewed as assigning a ternary code to each of the 24 possible states.
This code is a pre(cid:12)x code.

The code should achieve as much compression as possible

The expected length L(C; X) of a symbol code C for ensemble X is

L(C; X) = Xx2AX

P (x) l(x):

(5.5)

We may also write this quantity as

where I = jAXj.

Example 5.10. Let

L(C; X) =

pili

I

Xi=1

and

AX = f a; b; c; d g;
PX = f 1/2; 1/4; 1/8; 1/8g;

(5.6)

(5.7)

and consider the code C3. The entropy of X is 1.75 bits, and the expected
length L(C3; X) of this code is also 1.75 bits. The sequence of symbols
x = (acdbac) is encoded as c+(x) = 0110111100110. C3 is a pre(cid:12)x code
and is therefore uniquely decodeable. Notice that the codeword lengths
satisfy li = log2(1=pi), or equivalently, pi = 2(cid:0)li.

Example 5.11. Consider the (cid:12)xed length code for the same ensemble X, C4.

The expected length L(C4; X) is 2 bits.

Example 5.12. Consider C5. The expected length L(C5; X) is 1.25 bits, which
is less than H(X). But the code is not uniquely decodeable. The se-
quence x = (acdbac) encodes as 000111000, which can also be decoded
as (cabdca).

Example 5.13. Consider the code C6. The expected length L(C6; X) of this
code is 1.75 bits. The sequence of symbols x = (acdbac) is encoded as
c+(x) = 0011111010011.

Is C6 a pre(cid:12)x code? It is not, because c(a) = 0 is a pre(cid:12)x of both c(b)
and c(c).

0

0

0

1

C1

1 101

0

1

0

1

C3

C4

0

0

1

0

1
0

1

10
0 110

1 111

00

01
10

11

Pre(cid:12)x codes can be represented
on binary trees. Complete pre(cid:12)x
codes correspond to binary trees
with no unused branches. C1 is an
incomplete code.

C3:

pi
1/2
1/4
1/8
1/8

ai

c(ai)

a
b
c
d

0
10
110
111

h(pi)

1.0
2.0
3.0
3.0

li

1
2
3
3

C4 C5

a
b
c
d

00
01
10
11

0
1
00
11

C6:

pi
1/2
1/4
1/8
1/8

ai

c(ai)

a
b
c
d

0
01
011
111

h(pi)

1.0
2.0
3.0
3.0

li

1
2
3
3

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

94

5 | Symbol Codes

Is C6 uniquely decodeable? This is not so obvious. If you think that it
might not be uniquely decodeable, try to prove it so by (cid:12)nding a pair of
strings x and y that have the same encoding. [The de(cid:12)nition of unique
decodeability is given in equation (5.4).]

C6 certainly isn’t easy to decode. When we receive ‘00’, it is possible
that x could start ‘aa’, ‘ab’ or ‘ac’. Once we have received ‘001111’,
the second symbol is still ambiguous, as x could be ‘abd. . . ’ or ‘acd. . . ’.
But eventually a unique decoding crystallizes, once the next 0 appears
in the encoded stream.

C6 is in fact uniquely decodeable. Comparing with the pre(cid:12)x code C3,
we see that the codewords of C6 are the reverse of C3’s. That C3 is
uniquely decodeable proves that C6 is too, since any string from C6 is
identical to a string from C3 read backwards.

5.2 What limit is imposed by unique decodeability?

We now ask, given a list of positive integers flig, does there exist a uniquely
decodeable code with those integers as its codeword lengths? At this stage, we
ignore the probabilities of the di(cid:11)erent symbols; once we understand unique
decodeability better, we’ll reintroduce the probabilities and discuss how to
make an optimal uniquely decodeable symbol code.

In the examples above, we have observed that if we take a code such as
f00; 01; 10; 11g, and shorten one of its codewords, for example 00 ! 0, then
we can retain unique decodeability only if we lengthen other codewords. Thus
there seems to be a constrained budget that we can spend on codewords, with
shorter codewords being more expensive.

Let us explore the nature of this budget. If we build a code purely from
codewords of length l equal to three, how many codewords can we have and
retain unique decodeability? The answer is 2l = 8. Once we have chosen all
eight of these codewords, is there any way we could add to the code another
codeword of some other length and retain unique decodeability? It would
seem not.

What if we make a code that includes a length-one codeword, ‘0’, with the
other codewords being of length three? How many length-three codewords can
we have? If we restrict attention to pre(cid:12)x codes, then we can have only four
codewords of length three, namely f100; 101; 110; 111g. What about other
codes? Is there any other way of choosing codewords of length 3 that can give
more codewords? Intuitively, we think this unlikely. A codeword of length 3
appears to have a cost that is 22 times smaller than a codeword of length 1.
Let’s de(cid:12)ne a total budget of size 1, which we can spend on codewords. If
we set the cost of a codeword whose length is l to 2(cid:0)l, then we have a pricing
system that (cid:12)ts the examples discussed above. Codewords of length 3 cost
1/8 each; codewords of length 1 cost 1=2 each. We can spend our budget on
any codewords. If we go over our budget then the code will certainly not be
uniquely decodeable. If, on the other hand,

2(cid:0)li (cid:20) 1;

Xi

(5.8)

then the code may be uniquely decodeable. This inequality is the Kraft in-
equality.

Kraft inequality. For any uniquely decodeable code C(X) over the binary

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.2: What limit is imposed by unique decodeability?

95

alphabet f0; 1g, the codeword lengths must satisfy:

I

Xi=1

2(cid:0)li (cid:20) 1;

(5.9)

where I = jAXj.

Completeness. If a uniquely decodeable code satis(cid:12)es the Kraft inequality

with equality then it is called a complete code.

We want codes that are uniquely decodeable; pre(cid:12)x codes are uniquely de-
codeable, and are easy to decode. So life would be simpler for us if we could
restrict attention to pre(cid:12)x codes. Fortunately, for any source there is an op-
timal symbol code that is also a pre(cid:12)x code.

Kraft inequality and pre(cid:12)x codes. Given a set of codeword lengths that
satisfy the Kraft inequality, there exists a uniquely decodeable pre(cid:12)x
code with these codeword lengths.

The Kraft inequality might be more accurately referred to as the Kraft{
McMillan inequality: Kraft proved that if the inequality is satis(cid:12)ed, then a
pre(cid:12)x code exists with the given lengths. McMillan (1956) proved the con-
verse, that unique decodeability implies that the inequality holds.

Proof of the Kraft inequality. De(cid:12)ne S =Pi 2(cid:0)li. Consider the quantity

2(cid:0) (li1 + li2 + (cid:1)(cid:1)(cid:1) liN ):

(5.10)

SN ="Xi

2(cid:0)li#N

=

I

I

Xi1=1

Xi2=1

(cid:1)(cid:1)(cid:1)

I

XiN =1

The quantity in the exponent, (li1 + li2 + (cid:1)(cid:1)(cid:1) + liN ), is the length of the
encoding of the string x = ai1ai2 : : : aiN . For every string x of length N ,
there is one term in the above sum. Introduce an array Al that counts
how many strings x have encoded length l. Then, de(cid:12)ning lmin = mini li
and lmax = maxi li:

SN =

N lmax

Xl=N lmin

2(cid:0)lAl:

(5.11)

Now assume C is uniquely decodeable, so that for all x 6= y, c+(x) 6=
c+(y). Concentrate on the x that have encoded length l. There are a
total of 2l distinct bit strings of length l, so it must be the case that
Al (cid:20) 2l. So

SN =

N lmax

Xl=N lmin

2(cid:0)lAl (cid:20)

N lmax

Xl=N lmin

1 (cid:20) N lmax:

(5.12)

Thus SN (cid:20) lmaxN for all N . Now if S were greater than 1, then as N
increases, SN would be an exponentially growing function, and for large
enough N , an exponential always exceeds a polynomial such as lmaxN .
But our result (SN (cid:20) lmaxN ) is true for any N . Therefore S (cid:20) 1.
2

. Exercise 5.14.[3, p.104] Prove the result stated above, that for any set of code-
word lengths flig satisfying the Kraft inequality, there is a pre(cid:12)x code
having those lengths.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

96

5 | Symbol Codes

0

1

00

01

10

11

000

001

010

011

100

101

110

111

0000
0001
0010

0011

0100
0101
0110

0111
1000
1001

1010
1011
1100

1101
1110

1111

t
e
g
d
u
b
 
e
d
o
c
 
l
o
b
m
y
s
 
l
a
t
o
t
 
e
h
T

Figure 5.1. The symbol coding
budget. The ‘cost’ 2(cid:0)l of each
codeword (with length l) is
indicated by the size of the box it
is written in. The total budget
available when making a uniquely
decodeable code is 1.
You can think of this diagram as
showing a codeword supermarket,
with the codewords arranged in
aisles by their length, and the cost
of each codeword indicated by the
size of its box on the shelf. If the
cost of the codewords that you
take exceeds the budget then your
code will not be uniquely
decodeable.

C0

00

01

10

11

000

001

010

011

100

101

110

111

0

1

0000
0001
0010



















0011










0100
0101
0110

          

          

          

          


0111
1000
1001

1010
1011
1100

1101
1110

1111

										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										











										












0

1

C3

00

01










































10

000

001

010

011

100

101

11

110










































111

0000
0001
0010

0011

0100
0101
0110

0111
1000
1001

1010
1011
1100

1101
1110

1111

0

1

C4

C6

00

01


































































































































































11

10

000

001

010

011

100

101

110

111

0000
0001
0010

0011

0100
0101
0110

0111
1000
1001

1010
1011
1100

1101
1110

1111


















































































0

00










































01

1

10

11

000

001

010






















011

100

101

110






















111

0000
0001
0010

0011

0100
0101
0110

0111
1000
1001

1010
1011
1100

1101
1110

1111

Figure 5.2. Selections of
codewords made by codes
C0; C3; C4 and C6 from section
5.1.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.3: What’s the most compression that we can hope for?

97

A pictorial view of the Kraft inequality may help you solve this exercise.
Imagine that we are choosing the codewords to make a symbol code. We can
draw the set of all candidate codewords in a supermarket that displays the
‘cost’ of the codeword by the area of a box ((cid:12)gure 5.1). The total budget
available { the ‘1’ on the right-hand side of the Kraft inequality { is shown at
one side. Some of the codes discussed in section 5.1 are illustrated in (cid:12)gure
5.2. Notice that the codes that are pre(cid:12)x codes, C0, C3, and C4, have the
property that to the right of any selected codeword, there are no other selected
codewords { because pre(cid:12)x codes correspond to trees. Notice that a complete
pre(cid:12)x code corresponds to a complete tree having no unused branches.

We are now ready to put back the symbols’ probabilities fpig. Given a
set of symbol probabilities (the English language probabilities of (cid:12)gure 2.1,
for example), how do we make the best symbol code { one with the smallest
possible expected length L(C; X)? And what is that smallest possible expected
length? It’s not obvious how to assign the codeword lengths. If we give short
codewords to the more probable symbols then the expected length might be
reduced; on the other hand, shortening some codewords necessarily causes
others to lengthen, by the Kraft inequality.

5.3 What’s the most compression that we can hope for?

We wish to minimize the expected length of a code,

L(C; X) = Xi
(cid:21) Xi
(cid:21) H(X):

pi log 1=qi (cid:0) log z

pili =Xi
pi log 1=pi (cid:0) log z

(5.15)

(5.16)

L(C; X) = Xi

pili:

(5.13)

As you might have guessed, the entropy appears as the lower bound on the

expected length of a code.

Lower bound on expected length. The expected length L(C; X) of a

uniquely decodeable code is bounded below by H(X).

Proof. We de(cid:12)ne the implicit probabilities qi (cid:17) 2(cid:0)li=z, where z =Pi0 2(cid:0)li0 , so
that li = log 1=qi (cid:0) log z. We then use Gibbs’ inequality, Pi pi log 1=qi (cid:21)
Pi pi log 1=pi, with equality if qi = pi, and the Kraft inequality z (cid:20) 1:

(5.14)

The equality L(C; X) = H(X) is achieved only if the Kraft equality z = 1
is satis(cid:12)ed, and if the codelengths satisfy li = log(1=pi).
2

This is an important result so let’s say it again:

Optimal source codelengths. The expected length is minimized and is
equal to H(X) only if the codelengths are equal to the Shannon in-
formation contents:

li = log2(1=pi):

(5.17)

Implicit probabilities de(cid:12)ned by codelengths. Conversely, any choice

of codelengths flig implicitly de(cid:12)nes a probability distribution fqig,

qi (cid:17) 2(cid:0)li=z;

(5.18)

for which those codelengths would be the optimal codelengths. If the
code is complete then z = 1 and the implicit probabilities are given by
qi = 2(cid:0)li.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

98

5 | Symbol Codes

5.4 How much can we compress?

So, we can’t compress below the entropy. How close can we expect to get to
the entropy?

Theorem 5.1 Source coding theorem for symbol codes. For an ensemble X
there exists a pre(cid:12)x code C with expected length satisfying

H(X) (cid:20) L(C; X) < H(X) + 1:

(5.19)

Proof. We set the codelengths to integers slightly larger than the optimum

lengths:

li = dlog2(1=pi)e

(5.20)
where dl(cid:3)e denotes the smallest integer greater than or equal to l(cid:3). [We
are not asserting that the optimal code necessarily uses these lengths,
we are simply choosing these lengths because we can use them to prove
the theorem.]

We check that there is a pre(cid:12)x code with these lengths by con(cid:12)rming
that the Kraft inequality is satis(cid:12)ed.

Xi

2(cid:0)li =Xi

2(cid:0)dlog2(1=pi)e (cid:20)Xi

2(cid:0) log2(1=pi) =Xi

pi = 1:

(5.21)

Then we con(cid:12)rm

L(C; X) =Xi

pidlog(1=pi)e <Xi

pi(log(1=pi) + 1) = H(X) + 1: (5.22)

2

The cost of using the wrong codelengths

If we use a code whose lengths are not equal to the optimal codelengths, the
average message length will be larger than the entropy.

If the true probabilities are fpig and we use a complete code with lengths
li, we can view those lengths as de(cid:12)ning implicit probabilities qi = 2(cid:0)li. Con-
tinuing from equation (5.14), the average length is

L(C; X) = H(X) +Xi

pi log pi=qi;

(5.23)

i.e., it exceeds the entropy by the relative entropy DKL(pjjq) (as de(cid:12)ned on
p.34).

5.5 Optimal source coding with symbol codes: Hu(cid:11)man coding

Given a set of probabilities P, how can we design an optimal pre(cid:12)x code?
For example, what is the best symbol code for the English language ensemble
shown in (cid:12)gure 5.3? When we say ‘optimal’, let’s assume our aim is to
minimize the expected length L(C; X).

How not to do it
One might try to roughly split the set AX in two, and continue bisecting the
subsets so as to de(cid:12)ne a binary tree from the root. This construction has the
right spirit, as in the weighing problem, but it is not necessarily optimal; it
achieves L(C; X) (cid:20) H(X) + 2.

x

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
(cid:0)

P (x)

0.0575
0.0128
0.0263
0.0285
0.0913
0.0173
0.0133
0.0313
0.0599
0.0006
0.0084
0.0335
0.0235
0.0596
0.0689
0.0192
0.0008
0.0508
0.0567
0.0706
0.0334
0.0069
0.0119
0.0073
0.0164
0.0007
0.1928

Figure 5.3. An ensemble in need of
a symbol code.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.5: Optimal source coding with symbol codes: Hu(cid:11)man coding

99

Algorithm 5.4. Hu(cid:11)man coding
algorithm.

ai

pi

h(pi)

a
b
c
d
e

0.25
0.25
0.2
0.15
0.15

2.0
2.0
2.3
2.7
2.7

li

2
2
2
3
3

c(ai)

00
10
11
010
011

Table 5.5. Code created by the
Hu(cid:11)man algorithm.

The Hu(cid:11)man coding algorithm

We now present a beautifully simple algorithm for (cid:12)nding an optimal pre(cid:12)x
code. The trick is to construct the code backwards starting from the tails of
the codewords; we build the binary tree from its leaves.

1. Take the two least probable symbols in the alphabet. These two
symbols will be given the longest codewords, which will have equal
length, and di(cid:11)er only in the last digit.

2. Combine these two symbols into a single symbol, and repeat.

Since each step reduces the size of the alphabet by one, this algorithm will

have assigned strings to all the symbols after jAXj (cid:0) 1 steps.
Example 5.15. Let AX =f a,

g
and PX =f 0.25, 0.25, 0.2, 0.15, 0.15 g.

c, d,

b,

e

x

a

b

c

d

e

step 1

step 2

step 3

step 4

0.55
0.45

0
(cid:0)
(cid:0)
1

1.0

0.25
0.25
0.2
0.3

0
(cid:0)
(cid:0)
1

0.25
0.45

0.3

0.25
0.25
0.2
0.15
0.15

0
(cid:0)
(cid:0)
1

0

(cid:2)
(cid:2)

(cid:2)

(cid:2)
(cid:2)
1

The codewords are then obtained by concatenating the binary digits in
reverse order: C = f00; 10; 11; 010; 011g.
The codelengths selected
by the Hu(cid:11)man algorithm (column 4 of table 5.5) are in some cases
longer and in some cases shorter than the ideal codelengths, the Shannon
1/pi (column 3). The expected length of the
information contents log2
code is L = 2:30 bits, whereas the entropy is H = 2:2855 bits.
2

If at any point there is more than one way of selecting the two least probable
symbols then the choice may be made in any manner { the expected length of
the code will not depend on the choice.

Exercise 5.16.[3, p.105] Prove that there is no better symbol code for a source

than the Hu(cid:11)man code.

Example 5.17. We can make a Hu(cid:11)man code for the probability distribution
over the alphabet introduced in (cid:12)gure 2.1. The result is shown in (cid:12)g-
ure 5.6. This code has an expected length of 4.15 bits; the entropy of
the ensemble is 4.11 bits. Observe the disparities between the assigned
codelengths and the ideal codelengths log2

1/pi.

Constructing a binary tree top-down is suboptimal

In previous chapters we studied weighing problems in which we built ternary
or binary trees. We noticed that balanced trees { ones in which, at every step,
the two possible outcomes were as close as possible to equiprobable { appeared
to describe the most e(cid:14)cient experiments. This gave an intuitive motivation
for entropy as a measure of information content.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5 | Symbol Codes

Figure 5.6. Hu(cid:11)man code for the
English language ensemble
(monogram statistics).

ai

pi

Greedy Hu(cid:11)man

a
b
c
d
e
f
g

.01
.24
.05
.20
.47
.01
.02

000
001
010
011
10
110
111

000000
01
0001
001
1
000001
00001

Table 5.7. A greedily-constructed
code compared with the Hu(cid:11)man
code.

100

ai

pi

log2

1
pi

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{

0.0575
0.0128
0.0263
0.0285
0.0913
0.0173
0.0133
0.0313
0.0599
0.0006
0.0084
0.0335
0.0235
0.0596
0.0689
0.0192
0.0008
0.0508
0.0567
0.0706
0.0334
0.0069
0.0119
0.0073
0.0164
0.0007
0.1928

4.1
6.3
5.2
5.1
3.5
5.9
6.2
5.0
4.1
10.7
6.9
4.9
5.4
4.1
3.9
5.7
10.3
4.3
4.1
3.8
4.9
7.2
6.4
7.1
5.9
10.4
2.4

li

4
6
5
5
4
6
6
5
4
10
7
5
6
4
4
6
9
5
4
4
5
8
7
7
6
10
2

c(ai)

0000
001000
00101
10000
1100
111000
001001
10001
1001
1101000000
1010000
11101
110101
0001
1011
111001
110100001
11011
0011
1111
10101
11010001
1101001
1010001
101001
1101000001
01

−

a
n

s

i

o
e

t

b
g

c

d
h

k
x

y

u

j
z

q

v

w

m

f
p

r

l

It is not the case, however, that optimal codes can always be constructed
by a greedy top-down method in which the alphabet is successively divided
into subsets that are as near as possible to equiprobable.

Example 5.18. Find the optimal binary symbol code for the ensemble:

g g
AX = f a;
PX = f 0:01; 0:24; 0:05; 0:20; 0:47; 0:01; 0:02 g

d;

b;

c;

e;

f;

:

(5.24)

Notice that a greedy top-down method can split this set into two sub-
sets fa; b; c; dg and fe; f; gg which both have probability 1=2, and that
fa; b; c; dg can be divided into subsets fa; bg and fc; dg, which have prob-
ability 1=4; so a greedy top-down method gives the code shown in the
third column of table 5.7, which has expected length 2.53. The Hu(cid:11)man
coding algorithm yields the code shown in the fourth column, which has
expected length 1.97.
2

5.6 Disadvantages of the Hu(cid:11)man code

The Hu(cid:11)man algorithm produces an optimal symbol code for an ensemble,
but this is not the end of the story. Both the word ‘ensemble’ and the phrase
‘symbol code’ need careful attention.

Changing ensemble

If we wish to communicate a sequence of outcomes from one unchanging en-
semble, then a Hu(cid:11)man code may be convenient. But often the appropriate

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.6: Disadvantages of the Hu(cid:11)man code

101

ensemble changes. If for example we are compressing text, then the symbol
frequencies will vary with context: in English the letter u is much more prob-
able after a q than after an e ((cid:12)gure 2.3). And furthermore, our knowledge of
these context-dependent symbol frequencies will also change as we learn the
statistical properties of the text source.

Hu(cid:11)man codes do not handle changing ensemble probabilities with any
elegance. One brute-force approach would be to recompute the Hu(cid:11)man code
every time the probability over symbols changes. Another attitude is to deny
the option of adaptation, and instead run through the entire (cid:12)le in advance
and compute a good probability distribution, which will then remain (cid:12)xed
throughout transmission. The code itself must also be communicated in this
scenario. Such a technique is not only cumbersome and restrictive, it is also
suboptimal, since the initial message specifying the code and the document
itself are partially redundant. This technique therefore wastes bits.

The extra bit

An equally serious problem with Hu(cid:11)man codes is the innocuous-looking ‘ex-
tra bit’ relative to the ideal average length of H(X) { a Hu(cid:11)man code achieves
a length that satis(cid:12)es H(X) (cid:20) L(C; X) < H(X)+1; as proved in theorem 5.1.
A Hu(cid:11)man code thus incurs an overhead of between 0 and 1 bits per symbol.
If H(X) were large, then this overhead would be an unimportant fractional
increase. But for many applications, the entropy may be as low as one bit
per symbol, or even smaller, so the overhead L(C; X) (cid:0) H(X) may domi-
in some contexts, long
nate the encoded (cid:12)le length. Consider English text:
strings of characters may be highly predictable. For example, in the context
‘strings_of_ch’, one might predict the next nine symbols to be ‘aracters_’
with a probability of 0.99 each. A traditional Hu(cid:11)man code would be obliged
to use at least one bit per character, making a total cost of nine bits where
virtually no information is being conveyed (0.13 bits in total, to be precise).
The entropy of English, given a good model, is about one bit per character
(Shannon, 1948), so a Hu(cid:11)man code is likely to be highly ine(cid:14)cient.

A traditional patch-up of Hu(cid:11)man codes uses them to compress blocks of
symbols, for example the ‘extended sources’ X N we discussed in Chapter 4.
The overhead per block is at most 1 bit so the overhead per symbol is at most
1=N bits. For su(cid:14)ciently large blocks, the problem of the extra bit may be
removed { but only at the expenses of (a) losing the elegant instantaneous
decodeability of simple Hu(cid:11)man coding; and (b) having to compute the prob-
abilities of all relevant strings and build the associated Hu(cid:11)man tree. One will
end up explicitly computing the probabilities and codes for a huge number of
strings, most of which will never actually occur. (See exercise 5.29 (p.103).)

Beyond symbol codes

Hu(cid:11)man codes, therefore, although widely trumpeted as ‘optimal’, have many
defects for practical purposes. They are optimal symbol codes, but for practi-
cal purposes we don’t want a symbol code.

The defects of Hu(cid:11)man codes are recti(cid:12)ed by arithmetic coding, which
dispenses with the restriction that each symbol must translate into an integer
number of bits. Arithmetic coding is the main topic of the next chapter.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5 | Symbol Codes

102

5.7 Summary

Kraft inequality. If a code is uniquely decodeable its lengths must satisfy

2(cid:0)li (cid:20) 1:

Xi

(5.25)

For any lengths satisfying the Kraft inequality, there exists a pre(cid:12)x code
with those lengths.

Optimal source codelengths for an ensemble are equal to the Shannon

information contents

li = log2

1
pi

;

(5.26)

and conversely, any choice of codelengths de(cid:12)nes implicit probabilities

qi =

2(cid:0)li
z

:

(5.27)

The relative entropy DKL(pjjq) measures how many bits per symbol are
wasted by using a code whose implicit probabilities are q, when the
ensemble’s true probability distribution is p.

Source coding theorem for symbol codes. For an ensemble X, there ex-

ists a pre(cid:12)x code whose expected length satis(cid:12)es

H(X) (cid:20) L(C; X) < H(X) + 1:

(5.28)

The Hu(cid:11)man coding algorithm generates an optimal symbol code itera-
tively. At each iteration, the two least probable symbols are combined.

5.8 Exercises
. Exercise 5.19.[2 ] Is the code f00; 11; 0101; 111; 1010; 100100; 0110g uniquely

decodeable?

. Exercise 5.20.[2 ] Is the ternary code f00; 012; 0110; 0112; 100; 201; 212; 22g

uniquely decodeable?

Exercise 5.21.[3, p.106] Make Hu(cid:11)man codes for X 2, X 3 and X 4 where AX =
f0; 1g and PX = f0:9; 0:1g. Compute their expected lengths and com-
pare them with the entropies H(X 2), H(X 3) and H(X 4).
Repeat this exercise for X 2 and X 4 where PX = f0:6; 0:4g.

Exercise 5.22.[2, p.106] Find a probability distribution fp1; p2; p3; p4g such that
there are two optimal codes that assign di(cid:11)erent lengths flig to the four
symbols.

Exercise 5.23.[3 ] (Continuation of exercise 5.22.) Assume that the four proba-
bilities fp1; p2; p3; p4g are ordered such that p1 (cid:21) p2 (cid:21) p3 (cid:21) p4 (cid:21) 0. Let
Q be the set of all probability vectors p such that there are two optimal
codes with di(cid:11)erent lengths. Give a complete description of Q. Find
three probability vectors q(1), q(2), q(3), which are the convex hull of Q,
i.e., such that any p 2 Q can be written as

p = (cid:22)1q(1) + (cid:22)2q(2) + (cid:22)3q(3);

(5.29)

where f(cid:22)ig are positive.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.8: Exercises

103

. Exercise 5.24.[1 ] Write a short essay discussing how to play the game of twenty
questions optimally. [In twenty questions, one player thinks of an object,
and the other player has to guess the object using as few binary questions
as possible, preferably fewer than twenty.]

. Exercise 5.25.[2 ] Show that, if each probability pi is equal to an integer power
of 2 then there exists a source code whose expected length equals the
entropy.

. Exercise 5.26.[2, p.106] Make ensembles for which the di(cid:11)erence between the
entropy and the expected length of the Hu(cid:11)man code is as big as possible.

. Exercise 5.27.[2, p.106] A source X has an alphabet of eleven characters

fa; b; c; d; e; f; g; h; i; j; kg;

all of which have equal probability, 1=11.

Find an optimal uniquely decodeable symbol code for this source. How
much greater is the expected length of this optimal code than the entropy
of X?

. Exercise 5.28.[2 ] Consider the optimal symbol code for an ensemble X with
alphabet size I from which all symbols have identical probability p =
1=I. I is not a power of 2.
Show that the fraction f + of the I symbols that are assigned codelengths
equal to

satis(cid:12)es

l+ (cid:17) dlog2 Ie
2l+
I

f + = 2 (cid:0)

and that the expected length of the optimal symbol code is

L = l+ (cid:0) 1 + f +:

(5.30)

(5.31)

(5.32)

By di(cid:11)erentiating the excess length (cid:1)L (cid:17) L (cid:0) H(X) with respect to I,
show that the excess length is bounded by

(cid:1)L (cid:20) 1 (cid:0)

ln(ln 2)
ln 2 (cid:0)

1
ln 2

= 0:086:

(5.33)

Exercise 5.29.[2 ] Consider a sparse binary source with PX = f0:99; 0:01g. Dis-

cuss how Hu(cid:11)man codes could be used to compress this source e(cid:14)ciently.
Estimate how many codewords your proposed solutions require.

. Exercise 5.30.[2 ] Scienti(cid:12)c American carried the following puzzle in 1975.

The poisoned glass. ‘Mathematicians are curious birds’, the police
commissioner said to his wife.
‘You see, we had all those partly
(cid:12)lled glasses lined up in rows on a table in the hotel kitchen. Only
one contained poison, and we wanted to know which one before
searching that glass for (cid:12)ngerprints. Our lab could test the liquid
in each glass, but the tests take time and money, so we wanted to
make as few of them as possible by simultaneously testing mixtures
of small samples from groups of glasses. The university sent over a

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

104

5 | Symbol Codes

mathematics professor to help us. He counted the glasses, smiled
and said:
‘ \Pick any glass you want, Commissioner. We’ll test it (cid:12)rst."
‘ \But won’t that waste a test?" I asked.
‘ \No," he said, \it’s part of the best procedure. We can test one
glass (cid:12)rst. It doesn’t matter which one." ’
‘How many glasses were there to start with?’ the commissioner’s
wife asked.
‘I don’t remember. Somewhere between 100 and 200.’
What was the exact number of glasses?

Solve this puzzle and then explain why the professor was in fact wrong
and the commissioner was right. What is in fact the optimal procedure
for identifying the one poisoned glass? What is the expected waste
relative to this optimum if one followed the professor’s strategy? Explain
the relationship to symbol coding.

Exercise 5.31.[2, p.106] Assume that a sequence of symbols from the ensemble
X introduced at the beginning of this chapter is compressed using the
code C3.
Imagine picking one bit at random from the binary encoded
sequence c = c(x1)c(x2)c(x3) : : : . What is the probability that this bit
is a 1?

. Exercise 5.32.[2, p.107] How should the binary Hu(cid:11)man encoding scheme be
modi(cid:12)ed to make optimal symbol codes in an encoding alphabet with q
symbols? (Also known as ‘radix q’.)

Mixture codes

It is a tempting idea to construct a ‘metacode’ from several symbol codes that
assign di(cid:11)erent-length codewords to the alternative symbols, then switch from
one code to another, choosing whichever assigns the shortest codeword to the
current symbol. Clearly we cannot do this for free. If one wishes to choose
between two codes, then it is necessary to lengthen the message in a way that
indicates which of the two codes is being used. If we indicate this choice by
a single leading bit, it will be found that the resulting code is suboptimal
because it is incomplete (that is, it fails the Kraft equality).

Exercise 5.33.[3, p.108] Prove that this metacode is incomplete, and explain

why this combined code is suboptimal.

5.9 Solutions

Solution to exercise 5.8 (p.93). Yes, C2 = f1; 101g is uniquely decodeable,
even though it is not a pre(cid:12)x code, because no two di(cid:11)erent strings can map
onto the same string; only the codeword c(a2) = 101 contains the symbol 0.

Solution to exercise 5.14 (p.95). We wish to prove that for any set of codeword
lengths flig satisfying the Kraft inequality, there is a pre(cid:12)x code having those
lengths. This is readily proved by thinking of the codewords illustrated in
(cid:12)gure 5.8 as being in a ‘codeword supermarket’, with size indicating cost.
We imagine purchasing codewords one at a time, starting from the shortest
codewords (i.e., the biggest purchases), using the budget shown at the right
of (cid:12)gure 5.8. We start at one side of the codeword supermarket, say the

C3:

pi
1/2
1/4
1/8
1/8

ai

c(ai)

a
b
c
d

0
10
110
111

h(pi)

1.0
2.0
3.0
3.0

li

1
2
3
3

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

105

Figure 5.8. The codeword
supermarket and the symbol
coding budget. The ‘cost’ 2(cid:0)l of
each codeword (with length l) is
indicated by the size of the box it
is written in. The total budget
available when making a uniquely
decodeable code is 1.

Figure 5.9. Proof that Hu(cid:11)man
coding makes an optimal symbol
code. We assume that the rival
code, which is said to be optimal,
assigns unequal length codewords
to the two symbols with smallest
probability, a and b. By
interchanging codewords a and c
of the rival code, where c is a
symbol with rival codelength as
long as b’s, we can make a code
better than the rival code. This
shows that the rival code was not
optimal.

5.9: Solutions

0

1

00

01

10

11

000

001

010

011

100

101

110

111

0000
0001
0010

0011

0100
0101
0110

0111
1000
1001

1010
1011
1100

1101
1110

1111

t
e
g
d
u
b
 
e
d
o
c
 
l
o
b
m
y
s
 
l
a
t
o
t
 
e
h
T

symbol probability Hu(cid:11)man

a

b

c

pa

pb

pc

codewords

cH(a)

cH(b)

cH(c)

Rival code’s Modi(cid:12)ed rival
codewords

code

cR(a)

cR(b)

cR(c)

cR(c)

cR(b)

cR(a)

top, and purchase the (cid:12)rst codeword of the required length. We advance
down the supermarket a distance 2(cid:0)l, and purchase the next codeword of the
next required length, and so forth. Because the codeword lengths are getting
longer, and the corresponding intervals are getting shorter, we can always
buy an adjacent codeword to the latest purchase, so there is no wasting of
i=1 2(cid:0)li

the budget. Thus at the Ith codeword we have advanced a distance PI
down the supermarket; ifP 2(cid:0)li (cid:20) 1, we will have purchased all the codewords

without running out of budget.

Solution to exercise 5.16 (p.99). The proof that Hu(cid:11)man coding is optimal
depends on proving that the key step in the algorithm { the decision to give
the two symbols with smallest probability equal encoded lengths { cannot
lead to a larger expected length than any other code. We can prove this by
contradiction.

Assume that the two symbols with smallest probability, called a and b,
to which the Hu(cid:11)man algorithm would assign equal length codewords, do not
have equal lengths in any optimal symbol code. The optimal symbol code
is some other rival code in which these two codewords have unequal lengths
la and lb with la < lb. Without loss of generality we can assume that this
other code is a complete pre(cid:12)x code, because any codelengths of a uniquely
decodeable code can be realized by a pre(cid:12)x code.

In this rival code, there must be some other symbol c whose probability
pc is greater than pa and whose length in the rival code is greater than or
equal to lb, because the code for b must have an adjacent codeword of equal
or greater length { a complete pre(cid:12)x code never has a solo codeword of the
maximum length.

Consider exchanging the codewords of a and c ((cid:12)gure 5.9), so that a is

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

106

5 | Symbol Codes

encoded with the longer codeword that was c’s, and c, which is more probable
than a, gets the shorter codeword. Clearly this reduces the expected length
of the code. The change in expected length is (pa (cid:0) pc)(lc (cid:0) la). Thus we have
contradicted the assumption that the rival code is optimal. Therefore it is
valid to give the two symbols with smallest probability equal encoded lengths.
Hu(cid:11)man coding produces optimal symbol codes.
2
Solution to exercise 5.21 (p.102). A Hu(cid:11)man code for X 2 where AX = f0; 1g
and PX = f0:9; 0:1g is f00; 01; 10; 11g ! f1; 01; 000; 001g. This code has
L(C; X 2) = 1:29, whereas the entropy H(X 2) is 0.938.

A Hu(cid:11)man code for X 3 is

f000; 100; 010; 001; 101; 011; 110; 111g !

f1; 011; 010; 001; 00000; 00001; 00010; 00011g:

This has expected length L(C; X 3) = 1:598 whereas the entropy H(X 3) is
1.4069.

A Hu(cid:11)man code for X 4 maps the sixteen source strings to the following

codelengths:
f0000; 1000; 0100; 0010; 0001; 1100; 0110; 0011; 0101; 1010; 1001; 1110; 1101;

1011; 0111; 1111g ! f1; 3; 3; 3; 4; 6; 7; 7; 7; 7; 7; 9; 9; 9; 10; 10g:

This has expected length L(C; X 4) = 1:9702 whereas the entropy H(X 4) is
1.876.

When PX = f0:6; 0:4g, the Hu(cid:11)man code for X 2 has lengths f2; 2; 2; 2g;
the expected length is 2 bits, and the entropy is 1.94 bits. A Hu(cid:11)man code for
X 4 is shown in table 5.10. The expected length is 3.92 bits, and the entropy
is 3.88 bits.

Solution to exercise 5.22 (p.102). The set of probabilities fp1; p2; p3; p4g =
f1/6; 1/6; 1/3; 1/3g gives rise to two di(cid:11)erent optimal sets of codelengths, because
at the second step of the Hu(cid:11)man coding algorithm we can choose any of the
three possible pairings. We may either put them in a constant length code
f00; 01; 10; 11g or the code f000; 001; 01; 1g. Both codes have expected length
2.

Another solution is fp1; p2; p3; p4g = f1/5; 1/5; 1/5; 2/5g.
And a third is fp1; p2; p3; p4g = f1/3; 1/3; 1/3; 0g.

Solution to exercise 5.26 (p.103).
Let pmax be the largest probability in
p1; p2; : : : ; pI. The di(cid:11)erence between the expected length L and the entropy
H can be no bigger than max(pmax; 0:086) (Gallager, 1978).

See exercises 5.27{5.28 to understand where the curious 0.086 comes from.

Solution to exercise 5.27 (p.103). Length (cid:0) entropy = 0.086.
Solution to exercise 5.31 (p.104). There are two ways to answer this problem
correctly, and one popular way to answer it incorrectly. Let’s give the incorrect
answer (cid:12)rst:

Erroneous answer. \We can pick a random bit by (cid:12)rst picking a random
source symbol xi with probability pi, then picking a random bit from
c(xi). If we de(cid:12)ne fi to be the fraction of the bits of c(xi) that are 1s,
we (cid:12)nd

P (bit is 1) = Xi

pifi

(5.34)

= 1/2 (cid:2) 0 + 1/4 (cid:2) 1/2 + 1/8 (cid:2) 2/3 + 1/8 (cid:2) 1 = 1/3." (5.35)

ai

pi

li

c(ai)

0000
0001
0010
0100
1000
1100
1010
1001
0110
0101
0011
1110
1101
1011
0111
1111

0.1296
0.0864
0.0864
0.0864
0.0864
0.0576
0.0576
0.0576
0.0576
0.0576
0.0576
0.0384
0.0384
0.0384
0.0384
0.0256

3
4
4
4
3
4
4
4
4
4
4
5
5
5
4
5

000
0100
0110
0111
100
1010
1100
1101
1110
1111
0010
00110
01010
01011
1011
00111

Table 5.10. Hu(cid:11)man code for X 4
when p0 = 0:6. Column 3 shows
the assigned codelengths and
column 4 the codewords. Some
strings whose probabilities are
identical, e.g., the fourth and
(cid:12)fth, receive di(cid:11)erent codelengths.

C3:

ai

c(ai)

a
b
c
d

0
10
110
111

pi
1/2
1/4
1/8
1/8

li

1
2
3
3

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

5.9: Solutions

107

This answer is wrong because it falls for the bus-stop fallacy, which was intro-
duced in exercise 2.35 (p.38): if buses arrive at random, and we are interested
in ‘the average time from one bus until the next’, we must distinguish two
possible averages: (a) the average time from a randomly chosen bus until the
next; (b) the average time between the bus you just missed and the next bus.
The second ‘average’ is twice as big as the (cid:12)rst because, by waiting for a bus
at a random time, you bias your selection of a bus in favour of buses that
follow a large gap. You’re unlikely to catch a bus that comes 10 seconds after
a preceding bus! Similarly, the symbols c and d get encoded into longer-length
binary strings than a, so when we pick a bit from the compressed string at
random, we are more likely to land in a bit belonging to a c or a d than would
be given by the probabilities pi in the expectation (5.34). All the probabilities
need to be scaled up by li, and renormalized.

Correct answer in the same style. Every time symbol xi is encoded, li
bits are added to the binary string, of which fili are 1s. The expected
number of 1s added per symbol is

pifili;

Xi

and the expected total number of bits added per symbol is

pili:

Xi

So the fraction of 1s in the transmitted string is

P (bit is 1) = Pi pifili
Pi pili
1/2 (cid:2) 0 + 1/4 (cid:2) 1 + 1/8 (cid:2) 2 + 1/8 (cid:2) 3

=

7/4

(5.36)

(5.37)

(5.38)

=

7/8
7/4

= 1=2:

For a general symbol code and a general ensemble, the expectation (5.38) is
the correct answer. But in this case, we can use a more powerful argument.

Information-theoretic answer. The encoded string c is the output of an
optimal compressor that compresses samples from X down to an ex-
pected length of H(X) bits. We can’t expect to compress this data any
further. But if the probability P (bit is 1) were not equal to 1/2 then it
would be possible to compress the binary string further (using a block
compression code, say). Therefore P (bit is 1) must be equal to 1/2; in-
deed the probability of any sequence of l bits in the compressed stream
taking on any particular value must be 2(cid:0)l. The output of a perfect
compressor is always perfectly random bits.

To put it another way, if the probability P (bit is 1) were not equal to
1/2, then the information content per bit of the compressed string would
be at most H2(P (1)), which would be less than 1; but this contradicts
the fact that we can recover the original data from c, so the information
content per bit of the compressed string must be H(X)=L(C; X) = 1.

Solution to exercise 5.32 (p.104). The general Hu(cid:11)man coding algorithm for
an encoding alphabet with q symbols has one di(cid:11)erence from the binary case.
The process of combining q symbols into 1 symbol reduces the number of
symbols by q(cid:0) 1. So if we start with A symbols, we’ll only end up with a

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

108

5 | Symbol Codes

complete q-ary tree if A mod (q(cid:0)1) is equal to 1. Otherwise, we know that
whatever pre(cid:12)x code we make, it must be an incomplete tree with a number
of missing leaves equal, modulo (q(cid:0)1), to A mod (q(cid:0)1) (cid:0) 1. For example, if
a ternary tree is built for eight symbols, then there will unavoidably be one
missing leaf in the tree.

The optimal q-ary code is made by putting these extra leaves in the longest
branch of the tree. This can be achieved by adding the appropriate number
of symbols to the original source symbol set, all of these extra symbols having
probability zero. The total number of leaves is then equal to r(q(cid:0)1) + 1, for
some integer r. The symbols are then repeatedly combined by taking the q
symbols with smallest probability and replacing them by a single symbol, as
in the binary Hu(cid:11)man coding algorithm.

Solution to exercise 5.33 (p.104). We wish to show that a greedy metacode,
which picks the code which gives the shortest encoding, is actually suboptimal,
because it violates the Kraft inequality.

We’ll assume that each symbol x is assigned lengths lk(x) by each of the
candidate codes Ck. Let us assume there are K alternative codes and that we
can encode which code is being used with a header of length log K bits. Then
the metacode assigns lengths l0(x) that are given by

l0(x) = log2 K + min
k

lk(x):

We compute the Kraft sum:

2(cid:0)l0(x) =

2(cid:0) mink lk(x):

S =Xx

1

K Xx

(5.39)

(5.40)

Let’s divide the set AX into non-overlapping subsets fAkgK
Ak contains all the symbols x that the metacode sends via code k. Then

k=1 such that subset

2(cid:0)lk(x):

(5.41)

Now if one sub-code k satis(cid:12)es the Kraft equality Px2AX

must be the case that

2(cid:0)lk(x) = 1, then it

(5.42)

with equality only if all the symbols x are in Ak, which would mean that we
are only using one of the K codes. So

S =

1

K Xk Xx2Ak

2(cid:0)lk(x) (cid:20) 1;

Xx2Ak

S (cid:20)

1
K

K

Xk=1

1 = 1;

(5.43)

with equality only if equation (5.42) is an equality for all codes k. But it’s
impossible for all the symbols to be in all the non-overlapping subsets fAkgK
k=1,
so we can’t have equality (5.42) holding for all k. So S < 1.

Another way of seeing that a mixture code is suboptimal is to consider
the binary tree that it de(cid:12)nes. Think of the special case of two codes. The
(cid:12)rst bit we send identi(cid:12)es which code we are using. Now, in a complete code,
any subsequent binary string is a valid string. But once we know that we
are using, say, code A, we know that what follows can only be a codeword
corresponding to a symbol x whose encoding is shorter under code A than
code B. So some strings are invalid continuations, and the mixture code is
incomplete and suboptimal.

For further discussion of this issue and its relationship to probabilistic

modelling read about ‘bits back coding’ in section 28.3 and in Frey (1998).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 6

Before reading Chapter 6, you should have read the previous chapter and
worked on most of the exercises in it.

We’ll also make use of some Bayesian modelling ideas that arrived in the

vicinity of exercise 2.8 (p.30).

109

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6

Stream Codes

In this chapter we discuss two data compression schemes.

Arithmetic coding is a beautiful method that goes hand in hand with the
philosophy that compression of data from a source entails probabilistic mod-
elling of that source. As of 1999, the best compression methods for text (cid:12)les
use arithmetic coding, and several state-of-the-art image compression systems
use it too.

Lempel{Ziv coding is a ‘universal’ method, designed under the philosophy
that we would like a single compression algorithm that will do a reasonable job
for any source. In fact, for many real life sources, this algorithm’s universal
properties hold only in the limit of unfeasibly large amounts of data, but, all
the same, Lempel{Ziv compression is widely used and often e(cid:11)ective.

6.1 The guessing game

As a motivation for these two compression methods, consider the redundancy
in a typical English text (cid:12)le. Such (cid:12)les have redundancy at several levels: for
example, they contain the ASCII characters with non-equal frequency; certain
consecutive pairs of letters are more probable than others; and entire words
can be predicted given the context and a semantic understanding of the text.
To illustrate the redundancy of English, and a curious way in which it
could be compressed, we can imagine a guessing game in which an English
speaker repeatedly attempts to predict the next character in a text (cid:12)le.

For simplicity, let us assume that the allowed alphabet consists of the 26
upper case letters A,B,C,..., Z and a space ‘-’. The game involves asking
the subject to guess the next character repeatedly, the only feedback being
whether the guess is correct or not, until the character is correctly guessed.
After a correct guess, we note the number of guesses that were made when
the character was identi(cid:12)ed, and ask the subject to guess the next character
in the same way.

One sentence gave the following result when a human was asked to guess

a sentence. The numbers of guesses are listed below each character.

T H E R E - I S - N O - R E V E R S E - O N - A - M O T O R C Y C L E -
1 1 1 5 1 1 2 1 1 2 1 1 15 1 17 1 1 1 2 1 3 2 1 2 2 7 1 1 1 1 4 1 1 1 1 1

Notice that in many cases, the next letter is guessed immediately, in one
guess. In other cases, particularly at the start of syllables, more guesses are
needed.

What do this game and these results o(cid:11)er us? First, they demonstrate the
redundancy of English from the point of view of an English speaker. Second,
this game might be used in a data compression scheme, as follows.

110

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.2: Arithmetic codes

111

The string of numbers ‘1, 1, 1, 5, 1, . . . ’, listed above, was obtained by
presenting the text to the subject. The maximum number of guesses that the
subject will make for a given letter is twenty-seven, so what the subject is
doing for us is performing a time-varying mapping of the twenty-seven letters
fA; B; C; : : : ; Z;(cid:0)g onto the twenty-seven numbers f1; 2; 3; : : : ; 27g, which we
can view as symbols in a new alphabet. The total number of symbols has not
been reduced, but since he uses some of these symbols much more frequently
than others { for example, 1 and 2 { it should be easy to compress this new
string of symbols.

How would the uncompression of the sequence of numbers ‘1, 1, 1, 5, 1, . . . ’
work? At uncompression time, we do not have the original string ‘THERE. . . ’,
we have only the encoded sequence. Imagine that our subject has an absolutely
identical twin who also plays the guessing game with us, as if we knew the
source text. If we stop him whenever he has made a number of guesses equal to
the given number, then he will have just guessed the correct letter, and we can
then say ‘yes, that’s right’, and move to the next character. Alternatively, if
the identical twin is not available, we could design a compression system with
the help of just one human as follows. We choose a window length L, that is,
a number of characters of context to show the human. For every one of the
27L possible strings of length L, we ask them, ‘What would you predict is the
next character?’, and ‘If that prediction were wrong, what would your next
guesses be?’. After tabulating their answers to these 26 (cid:2) 27L questions, we
could use two copies of these enormous tables at the encoder and the decoder
in place of the two human twins. Such a language model is called an Lth order
Markov model.

These systems are clearly unrealistic for practical compression, but they

illustrate several principles that we will make use of now.

6.2 Arithmetic codes

When we discussed variable-length symbol codes, and the optimal Hu(cid:11)man
algorithm for constructing them, we concluded by pointing out two practical
and theoretical problems with Hu(cid:11)man codes (section 5.6).

These defects are recti(cid:12)ed by arithmetic codes, which were invented by
Elias, by Rissanen and by Pasco, and subsequently made practical by Witten
et al. (1987).
In an arithmetic code, the probabilistic modelling is clearly
separated from the encoding operation. The system is rather similar to the
guessing game. The human predictor is replaced by a probabilistic model of
the source. As each symbol is produced by the source, the probabilistic model
supplies a predictive distribution over all possible values of the next symbol,
that is, a list of positive numbers fpig that sum to one. If we choose to model
the source as producing i.i.d. symbols with some known distribution, then the
predictive distribution is the same every time; but arithmetic coding can with
equal ease handle complex adaptive models that produce context-dependent
predictive distributions. The predictive model is usually implemented in a
computer program.

The encoder makes use of the model’s predictions to create a binary string.
The decoder makes use of an identical twin of the model (just as in the guessing
game) to interpret the binary string.

Let the source alphabet be AX = fa1; : : : ; aIg, and let the Ith symbol aI
have the special meaning ‘end of transmission’. The source spits out a sequence
x1; x2; : : : ; xn; : : : : The source does not necessarily produce i.i.d. symbols. We
will assume that a computer program is provided to the encoder that assigns a

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

112

6 | Stream Codes

predictive probability distribution over ai given the sequence that has occurred
thus far, P (xn = ai j x1; : : : ; xn(cid:0)1). The receiver has an identical program that
produces the same predictive probability distribution P (xn = ai j x1; : : : ; xn(cid:0)1).

0.00

0.25

0.50

0.75

1.00

(cid:27)

6
01
?

6

0

?
6

1

?

01101

Figure 6.1. Binary strings de(cid:12)ne
real intervals within the real line
[0,1). We (cid:12)rst encountered a
picture like this when we
discussed the symbol-code
supermarket in Chapter 5.

Concepts for understanding arithmetic coding

Notation for intervals. The interval [0:01; 0:10) is all numbers between 0:01 and
0:10, including 0:01 _0 (cid:17) 0:01000 : : : but not 0:10 _0 (cid:17) 0:10000 : : : :

A binary transmission de(cid:12)nes an interval within the real line from 0 to 1.
For example, the string 01 is interpreted as a binary real number 0.01. . . , which
corresponds to the interval [0:01; 0:10) in binary, i.e., the interval [0:25; 0:50)
in base ten.

The longer string 01101 corresponds to a smaller interval

[0:01101;
0:01110). Because 01101 has the (cid:12)rst string, 01, as a pre(cid:12)x, the new in-
terval is a sub-interval of the interval [0:01; 0:10). A one-megabyte binary (cid:12)le
(223 bits) is thus viewed as specifying a number between 0 and 1 to a precision
of about two million decimal places { two million decimal digits, because each
byte translates into a little more than two decimal digits.

Now, we can also divide the real line [0,1) into I intervals of lengths equal

to the probabilities P (x1 = ai), as shown in (cid:12)gure 6.2.

0.00
P (x1 = a1)

P (x1 = a1) + P (x1 = a2)

P (x1 = a1) + : : : + P (x1 = aI(cid:0)1)
1.0

(cid:27)

(cid:27)

...

6?a1
6
a2
?

...
6?aI

Figure 6.2. A probabilistic model
de(cid:12)nes real intervals within the
real line [0,1).

a2a1

a2a5

We may then take each interval ai and subdivide it into intervals de-
is proportional to
Indeed the length of the interval aiaj will be precisely

noted aia1; aia2; : : : ; aiaI, such that the length of aiaj
P (x2 = aj j x1 = ai).
the joint probability

P (x1 = ai; x2 = aj) = P (x1 = ai)P (x2 = aj j x1 = ai):

(6.1)

Iterating this procedure, the interval [0; 1) can be divided into a sequence
of intervals corresponding to all possible (cid:12)nite length strings x1x2 : : : xN , such
that the length of an interval is equal to the probability of the string given
our model.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

113

Algorithm 6.3. Arithmetic coding.
Iterative procedure to (cid:12)nd the
interval [u; v) for the string
x1x2 : : : xN .

6.2: Arithmetic codes

u := 0.0
v := 1.0
p := v (cid:0) u
for n = 1 to N {

Compute the cumulative probabilities Qn and Rn (6.2, 6.3)
v := u + pRn(xn j x1; : : : ; xn(cid:0)1)
u := u + pQn(xn j x1; : : : ; xn(cid:0)1)
p := v (cid:0) u

}

Formulae describing arithmetic coding

The process depicted in (cid:12)gure 6.2 can be written explicitly as follows. The
intervals are de(cid:12)ned in terms of the lower and upper cumulative probabilities

Qn(ai j x1; : : : ; xn(cid:0)1) (cid:17)

Rn(ai j x1; : : : ; xn(cid:0)1) (cid:17)

i(cid:0)1

Xi0 = 1
Xi0 = 1

i

P (xn = ai0 j x1; : : : ; xn(cid:0)1);

P (xn = ai0 j x1; : : : ; xn(cid:0)1):

(6.2)

(6.3)

As the nth symbol arrives, we subdivide the n(cid:0)1th interval at the points de(cid:12)ned
by Qn and Rn. For example, starting with the (cid:12)rst symbol, the intervals ‘a1’,
‘a2’, and ‘aI ’ are

a1 $ [Q1(a1); R1(a1)) = [0; P (x1 = a1));

a2 $ [Q1(a2); R1(a2)) = [P (x = a1); P (x = a1) + P (x = a2)) ;

(6.4)

(6.5)

and

aI $ [Q1(aI ); R1(aI )) = [P (x1 = a1) + : : : + P (x1 = aI(cid:0)1); 1:0) :

(6.6)

Algorithm 6.3 describes the general procedure.

To encode a string x1x2 : : : xN , we locate the interval corresponding to
x1x2 : : : xN , and send a binary string whose interval lies within that interval.
This encoding can be performed on the (cid:13)y, as we now illustrate.

Example: compressing the tosses of a bent coin

Imagine that we watch as a bent coin is tossed some number of times (cf.
example 2.7 (p.30) and section 3.2 (p.51)). The two outcomes when the coin
is tossed are denoted a and b. A third possibility is that the experiment is
halted, an event denoted by the ‘end of (cid:12)le’ symbol, ‘2’. Because the coin is
bent, we expect that the probabilities of the outcomes a and b are not equal,
though beforehand we don’t know which is the more probable outcome.

Encoding

Let the source string be ‘bbba2’. We pass along the string one symbol at a
time and use our model to compute the probability distribution of the next

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

114

6 | Stream Codes

symbol given the string thus far. Let these probabilities be:

Context
(sequence thus far)

Probability of next symbol

b

bb

bbb

bbba

P (a) = 0:425
P (aj b) = 0:28
P (aj bb) = 0:21
P (aj bbb) = 0:17
P (aj bbba) = 0:28

P (b) = 0:425
P (bj b) = 0:57
P (bj bb) = 0:64
P (bj bbb) = 0:68
P (bj bbba) = 0:57

P (2) = 0:15
P (2j b) = 0:15
P (2j bb) = 0:15
P (2j bbb) = 0:15
P (2j bbba) = 0:15

Figure 6.4 shows the corresponding intervals. The interval b is the middle
0.425 of [0; 1). The interval bb is the middle 0.567 of b, and so forth.

00000
00001
00010
00011
00100
00101
00110
00111
01000
01001
01010
01011
01100
01101
01110
01111
10000
10001
10010
10011
10100
10101
10110
10111
11000
11001
11010
11011
11100
11101
11110
11111

0000

000

0001

0010

001

0011

0100

010

0101

0110

011

0111

1000

100

1001

1010

101

1011

1100

110

1101

1110

111

1111

00

01

10

11

0

(cid:2)
(cid:2)

B
B

(cid:2)
(cid:2)

B
B

1

a

b

2

ba

bba

bbba

bb

bbb

bbbb

bbb2

bb2

b2

Figure 6.4. Illustration of the
arithmetic coding process as the
sequence bbba2 is transmitted.

bbbaa

bbba

bbbab

bbba2

10010111
10011000
10011001
10011010
10011011
10011100
10011101
10011110
10011111
C
10100000
C
100111101

CCO

10011

When the (cid:12)rst symbol ‘b’ is observed, the encoder knows that the encoded
string will start ‘01’, ‘10’, or ‘11’, but does not know which. The encoder
writes nothing for the time being, and examines the next symbol, which is ‘b’.
The interval ‘bb’ lies wholly within interval ‘1’, so the encoder can write the
(cid:12)rst bit: ‘1’. The third symbol ‘b’ narrows down the interval a little, but not
quite enough for it to lie wholly within interval ‘10’. Only when the next ‘a’
is read from the source can we transmit some more bits. Interval ‘bbba’ lies
wholly within the interval ‘1001’, so the encoder adds ‘001’ to the ‘1’ it has
written. Finally when the ‘2’ arrives, we need a procedure for terminating the
encoding. Magnifying the interval ‘bbba2’ ((cid:12)gure 6.4, right) we note that the
marked interval ‘100111101’ is wholly contained by bbba2, so the encoding
can be completed by appending ‘11101’.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.2: Arithmetic codes

115

Exercise 6.1.[2, p.127] Show that the overhead required to terminate a message
is never more than 2 bits, relative to the ideal message length given the
probabilistic model H, h(xjH) = log[1=P (xjH)].

This is an important result. Arithmetic coding is very nearly optimal. The
message length is always within two bits of the Shannon information content
of the entire source string, so the expected message length is within two bits
of the entropy of the entire message.

Decoding

The decoder receives the string ‘100111101’ and passes along it one symbol
at a time. First, the probabilities P (a); P (b); P (2) are computed using the
identical program that the encoder used and the intervals ‘a’, ‘b’ and ‘2’ are
deduced. Once the (cid:12)rst two bits ‘10’ have been examined, it is certain that
the original string must have been started with a ‘b’, since the interval ‘10’ lies
wholly within interval ‘b’. The decoder can then use the model to compute
P (aj b); P (bj b); P (2j b) and deduce the boundaries of the intervals ‘ba’, ‘bb’
and ‘b2’. Continuing, we decode the second b once we reach ‘1001’, the third
b once we reach ‘100111’, and so forth, with the unambiguous identi(cid:12)cation
of ‘bbba2’ once the whole binary string has been read. With the convention
that ‘2’ denotes the end of the message, the decoder knows to stop decoding.

Transmission of multiple (cid:12)les

How might one use arithmetic coding to communicate several distinct (cid:12)les over
the binary channel? Once the 2 character has been transmitted, we imagine
that the decoder is reset into its initial state. There is no transfer of the learnt
statistics of the (cid:12)rst (cid:12)le to the second (cid:12)le. If, however, we did believe that
there is a relationship among the (cid:12)les that we are going to compress, we could
de(cid:12)ne our alphabet di(cid:11)erently, introducing a second end-of-(cid:12)le character that
marks the end of the (cid:12)le but instructs the encoder and decoder to continue
using the same probabilistic model.

The big picture

Notice that to communicate a string of N letters both the encoder and the
decoder needed to compute only NjAj conditional probabilities { the proba-
bilities of each possible letter in each context actually encountered { just as in
the guessing game. This cost can be contrasted with the alternative of using
a Hu(cid:11)man code with a large block size (in order to reduce the possible one-
bit-per-symbol overhead discussed in section 5.6), where all block sequences
that could occur must be considered and their probabilities evaluated.

Notice how (cid:13)exible arithmetic coding is:

it can be used with any source
alphabet and any encoded alphabet. The size of the source alphabet and the
encoded alphabet can change with time. Arithmetic coding can be used with
any probability distribution, which can change utterly from context to context.
Furthermore, if we would like the symbols of the encoding alphabet (say,
0 and 1) to be used with unequal frequency, that can easily be arranged by
subdividing the right-hand interval in proportion to the required frequencies.

How the probabilistic model might make its predictions

The technique of arithmetic coding does not force one to produce the predic-
tive probability in any particular way, but the predictive distributions might

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6 | Stream Codes

Figure 6.5. Illustration of the
intervals de(cid:12)ned by a simple
Bayesian probabilistic model. The
size of an intervals is proportional
to the probability of the string.
This model anticipates that the
source is likely to be biased
towards one of a and b, so
sequences having lots of as or lots
of bs have larger intervals than
sequences of the same length that
are 50:50 as and bs.

116

aa

ab

a2

ba

aaa

aab

aa2

aba

abb
ab2

baa

bab
ba2
bba

aaaa

aaab

aaba
aabb

abaa
abab
abba
abbb

baaa
baab
baba
babb

bbaa
bbab
bbba

bb

bbb

bbbb

bb2

b2

a

b

2

00000
00001
00010
00011
00100
00101
00110
00111
01000
01001
01010
01011
01100
01101
01110
01111
10000
10001
10010
10011
10100
10101
10110
10111
11000
11001
11010
11011
11100
11101
11110
11111

0000

0001

0010

0011

0100

0101

0110

0111

1000

1001

1010

1011

1100

1101

1110

1111

000

001

010

011

100

101

110

111

00

01

10

11

0

1

naturally be produced by a Bayesian model.

Figure 6.4 was generated using a simple model that always assigns a prob-
ability of 0.15 to 2, and assigns the remaining 0.85 to a and b, divided in
proportion to probabilities given by Laplace’s rule,

PL(aj x1; : : : ; xn(cid:0)1) =

Fa + 1

Fa + Fb + 2

;

(6.7)

where Fa(x1; : : : ; xn(cid:0)1) is the number of times that a has occurred so far, and
Fb is the count of bs. These predictions correspond to a simple Bayesian model
that expects and adapts to a non-equal frequency of use of the source symbols
a and b within a (cid:12)le.

Figure 6.5 displays the intervals corresponding to a number of strings of
length up to (cid:12)ve. Note that if the string so far has contained a large number of
bs then the probability of b relative to a is increased, and conversely if many
as occur then as are made more probable. Larger intervals, remember, require
fewer bits to encode.

Details of the Bayesian model

Having emphasized that any model could be used { arithmetic coding is not
wedded to any particular set of probabilities { let me explain the simple adaptive

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.2: Arithmetic codes

117

probabilistic model used in the preceding example; we (cid:12)rst encountered this
model in exercise 2.8 (p.30).

Assumptions

The model will be described using parameters p2, pa and pb, de(cid:12)ned below,
which should not be confused with the predictive probabilities in a particular
context, for example, P (aj s = baa). A bent coin labelled a and b is tossed some
number of times l, which we don’t know beforehand. The coin’s probability of
coming up a when tossed is pa, and pb = 1 (cid:0) pa; the parameters pa; pb are not
known beforehand. The source string s = baaba2 indicates that l was 5 and
the sequence of outcomes was baaba.

1. It is assumed that the length of the string l has an exponential probability

distribution

P (l) = (1 (cid:0) p2)lp2:

(6.8)

This distribution corresponds to assuming a constant probability p2 for
the termination symbol ‘2’ at each character.

2. It is assumed that the non-terminal characters in the string are selected in-
dependently at random from an ensemble with probabilities P = fpa; pbg;
the probability pa is (cid:12)xed throughout the string to some unknown value
that could be anywhere between 0 and 1. The probability of an a occur-
ring as the next symbol, given pa (if only we knew it), is (1 (cid:0) p2)pa. The
probability, given pa, that an unterminated string of length F is a given
string s that contains fFa; Fbg counts of the two outcomes is the Bernoulli
distribution
(6.9)

P (sj pa; F ) = pFa

a (1 (cid:0) pa)Fb :

3. We assume a uniform prior distribution for pa,

P (pa) = 1;

pa 2 [0; 1];

(6.10)

and de(cid:12)ne pb (cid:17) 1 (cid:0) pa. It would be easy to assume other priors on pa,
with beta distributions being the most convenient to handle.

This model was studied in section 3.2. The key result we require is the predictive
distribution for the next symbol, given the string so far, s. This probability
that the next character is a or b (assuming that it is not ‘2’) was derived in
equation (3.16) and is precisely Laplace’s rule (6.7).

. Exercise 6.2.[3 ] Compare the expected message length when an ASCII (cid:12)le is

compressed by the following three methods.

Hu(cid:11)man-with-header. Read the whole (cid:12)le, (cid:12)nd the empirical fre-
quency of each symbol, construct a Hu(cid:11)man code for those frequen-
cies, transmit the code by transmitting the lengths of the Hu(cid:11)man
codewords, then transmit the (cid:12)le using the Hu(cid:11)man code. (The
actual codewords don’t need to be transmitted, since we can use a
deterministic method for building the tree given the codelengths.)

Arithmetic code using the Laplace model.

PL(aj x1; : : : ; xn(cid:0)1) =

:

(6.11)

Fa + 1

Pa0(Fa0 + 1)

Arithmetic code using a Dirichlet model. This model’s predic-

tions are:

PD(aj x1; : : : ; xn(cid:0)1) =

;

Fa + (cid:11)

Pa0(Fa0 + (cid:11))

(6.12)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

118

6 | Stream Codes

where (cid:11) is (cid:12)xed to a number such as 0.01. A small value of (cid:11)
corresponds to a more responsive version of the Laplace model;
the probability over characters is expected to be more nonuniform;
(cid:11) = 1 reproduces the Laplace model.

Take care that the header of your Hu(cid:11)man message is self-delimiting.
Special cases worth considering are (a) short (cid:12)les with just a few hundred
characters; (b) large (cid:12)les in which some characters are never used.

6.3 Further applications of arithmetic coding

E(cid:14)cient generation of random samples

Arithmetic coding not only o(cid:11)ers a way to compress strings believed to come
from a given model; it also o(cid:11)ers a way to generate random strings from a
model.
Imagine sticking a pin into the unit interval at random, that line
having been divided into subintervals in proportion to probabilities pi; the
probability that your pin will lie in interval i is pi.

So to generate a sample from a model, all we need to do is feed ordinary
random bits into an arithmetic decoder for that model. An in(cid:12)nite random
bit sequence corresponds to the selection of a point at random from the line
[0; 1), so the decoder will then select a string at random from the assumed
distribution. This arithmetic method is guaranteed to use very nearly the
smallest number of random bits possible to make the selection { an important
point in communities where random numbers are expensive! [This is not a joke.
Large amounts of money are spent on generating random bits in software and
hardware. Random numbers are valuable.]

A simple example of the use of this technique is in the generation of random

bits with a nonuniform distribution fp0; p1g.
Exercise 6.3.[2, p.128] Compare the following two techniques for generating
random symbols from a nonuniform distribution fp0; p1g = f0:99; 0:01g:
(a) The standard method: use a standard random number generator
to generate an integer between 1 and 232. Rescale the integer to
(0; 1). Test whether this uniformly distributed random variable is
less than 0:99, and emit a 0 or 1 accordingly.

(b) Arithmetic coding using the correct model, fed with standard ran-

dom bits.

Roughly how many random bits will each method use to generate a
thousand samples from this sparse distribution?

E(cid:14)cient data-entry devices

When we enter text into a computer, we make gestures of some sort { maybe
we tap a keyboard, or scribble with a pointer, or click with a mouse; an
e(cid:14)cient text entry system is one where the number of gestures required to
enter a given text string is small.

Writing can be viewed as an inverse process to data compression. In data
compression, the aim is to map a given text string into a small number of bits.
In text entry, we want a small sequence of gestures to produce our intended
text.

By inverting an arithmetic coder, we can obtain an information-e(cid:14)cient
text entry device that is driven by continuous pointing gestures (Ward et al.,

Compression:
text ! bits

Writing:
text   gestures

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.4: Lempel{Ziv coding

119

2000). In this system, called Dasher, the user zooms in on the unit interval to
locate the interval corresponding to their intended string, in the same style as
(cid:12)gure 6.4. A language model (exactly as used in text compression) controls
the sizes of the intervals such that probable strings are quick and easy to
identify. After an hour’s practice, a novice user can write with one (cid:12)nger
driving Dasher at about 25 words per minute { that’s about half their normal
ten-(cid:12)nger typing speed on a regular keyboard. It’s even possible to write at 25
words per minute, hands-free, using gaze direction to drive Dasher (Ward and
MacKay, 2002). Dasher is available as free software for various platforms.1

6.4 Lempel{Ziv coding

The Lempel{Ziv algorithms, which are widely used for data compression (e.g.,
the compress and gzip commands), are di(cid:11)erent in philosophy to arithmetic
coding. There is no separation between modelling and coding, and no oppor-
tunity for explicit modelling.

Basic Lempel{Ziv algorithm

The method of compression is to replace a substring with a pointer to
an earlier occurrence of the same substring. For example if the string is
1011010100010. . . , we parse it into an ordered dictionary of substrings that
have not appeared before as follows: (cid:21), 1, 0, 11, 01, 010, 00, 10, . . . . We in-
clude the empty substring (cid:21) as the (cid:12)rst substring in the dictionary and order
the substrings in the dictionary by the order in which they emerged from the
source. After every comma, we look along the next part of the input sequence
until we have read a substring that has not been marked o(cid:11) before. A mo-
ment’s re(cid:13)ection will con(cid:12)rm that this substring is longer by one bit than a
substring that has occurred earlier in the dictionary. This means that we can
encode each substring by giving a pointer to the earlier occurrence of that pre-
(cid:12)x and then sending the extra bit by which the new substring in the dictionary
di(cid:11)ers from the earlier substring. If, at the nth bit, we have enumerated s(n)
substrings, then we can give the value of the pointer in dlog 2 s(n)e bits. The
code for the above sequence is then as shown in the fourth line of the following
table (with punctuation included for clarity), the upper lines indicating the
source string and the value of s(n):

source substrings (cid:21)
s(n)
0
s(n)binary
000 001
(; 1)
(pointer; bit)

1
1

0
2
010
(0; 0)

11
3
011
(01; 1)

01
4
100
(10; 1)

010
5
101
(100; 0)

00
6
110
(010; 0)

10
7
111
(001; 0)

Notice that the (cid:12)rst pointer we send is empty, because, given that there is
only one substring in the dictionary { the string (cid:21) { no bits are needed to
convey the ‘choice’ of that substring as the pre(cid:12)x. The encoded string is
100011101100001000010. The encoding, in this simple case, is actually a
longer string than the source string, because there was no obvious redundancy
in the source string.

. Exercise 6.4.[2 ] Prove that any uniquely decodeable code from f0; 1g+ to
f0; 1g+ necessarily makes some strings longer if it makes some strings
shorter.

1http://www.inference.phy.cam.ac.uk/dasher/

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

120

6 | Stream Codes

One reason why the algorithm described above lengthens a lot of strings is
because it is ine(cid:14)cient { it transmits unnecessary bits; to put it another way,
its code is not complete. Once a substring in the dictionary has been joined
there by both of its children, then we can be sure that it will not be needed
(except possibly as part of our protocol for terminating a message); so at that
point we could drop it from our dictionary of substrings and shu(cid:15)e them
all along one, thereby reducing the length of subsequent pointer messages.
Equivalently, we could write the second pre(cid:12)x into the dictionary at the point
previously occupied by the parent. A second unnecessary overhead is the
transmission of the new bit in these cases { the second time a pre(cid:12)x is used,
we can be sure of the identity of the next bit.

Decoding

The decoder again involves an identical twin at the decoding end who con-
structs the dictionary of substrings as the data are decoded.

. Exercise 6.5.[2, p.128] Encode the string 000000000000100000000000 using

the basic Lempel{Ziv algorithm described above.

. Exercise 6.6.[2, p.128] Decode the string

00101011101100100100011010101000011

that was encoded using the basic Lempel{Ziv algorithm.

Practicalities

In this description I have not discussed the method for terminating a string.
There are many variations on the Lempel{Ziv algorithm, all exploiting the
same idea but using di(cid:11)erent procedures for dictionary management, etc. The
resulting programs are fast, but their performance on compression of English
text, although useful, does not match the standards set in the arithmetic
coding literature.

Theoretical properties

In contrast to the block code, Hu(cid:11)man code, and arithmetic coding methods
we discussed in the last three chapters, the Lempel{Ziv algorithm is de(cid:12)ned
without making any mention of a probabilistic model for the source. Yet, given
any ergodic source (i.e., one that is memoryless on su(cid:14)ciently long timescales),
the Lempel{Ziv algorithm can be proven asymptotically to compress down to
the entropy of the source. This is why it is called a ‘universal’ compression
algorithm. For a proof of this property, see Cover and Thomas (1991).

It achieves its compression, however, only by memorizing substrings that
have happened so that it has a short name for them the next time they occur.
The asymptotic timescale on which this universal performance is achieved may,
for many sources, be unfeasibly long, because the number of typical substrings
that need memorizing may be enormous. The useful performance of the al-
gorithm in practice is a re(cid:13)ection of the fact that many (cid:12)les contain multiple
repetitions of particular short sequences of characters, a form of redundancy
to which the algorithm is well suited.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

121

6.5: Demonstration

Common ground

I have emphasized the di(cid:11)erence in philosophy behind arithmetic coding and
Lempel{Ziv coding. There is common ground between them, though: in prin-
ciple, one can design adaptive probabilistic models, and thence arithmetic
codes, that are ‘universal’, that is, models that will asymptotically compress
any source in some class to within some factor (preferably 1) of its entropy.
However, for practical purposes, I think such universal models can only be
constructed if the class of sources is severely restricted. A general purpose
compressor that can discover the probability distribution of any source would
be a general purpose arti(cid:12)cial intelligence! A general purpose arti(cid:12)cial intelli-
gence does not yet exist.

6.5 Demonstration

An interactive aid for exploring arithmetic coding, dasher.tcl, is available.2
A demonstration arithmetic-coding software package written by Radford
Neal3 consists of encoding and decoding modules to which the user adds a
module de(cid:12)ning the probabilistic model. It should be emphasized that there
is no single general-purpose arithmetic-coding compressor; a new model has to
be written for each type of source. Radford Neal’s package includes a simple
adaptive model similar to the Bayesian model demonstrated in section 6.2.
The results using this Laplace model should be viewed as a basic benchmark
since it is the simplest possible probabilistic model { it simply assumes the
characters in the (cid:12)le come independently from a (cid:12)xed ensemble. The counts
fFig of the symbols faig are rescaled and rounded as the (cid:12)le is read such that
all the counts lie between 1 and 256.
A state-of-the-art compressor for documents containing text and images,
DjVu, uses arithmetic coding.4 It uses a carefully designed approximate arith-
metic coder for binary alphabets called the Z-coder (Bottou et al., 1998), which
is much faster than the arithmetic coding software described above. One of
the neat tricks the Z-coder uses is this: the adaptive model adapts only occa-
sionally (to save on computer time), with the decision about when to adapt
being pseudo-randomly controlled by whether the arithmetic encoder emitted
a bit.

The JBIG image compression standard for binary images uses arithmetic
coding with a context-dependent model, which adapts using a rule similar to
Laplace’s rule. PPM (Teahan, 1995) is a leading method for text compression,
and it uses arithmetic coding.

There are many Lempel{Ziv-based programs. gzip is based on a version
of Lempel{Ziv called ‘LZ77’ (Ziv and Lempel, 1977). compress is based on
‘LZW’ (Welch, 1984). In my experience the best is gzip, with compress being
inferior on most (cid:12)les.

bzip is a block-sorting (cid:12)le compressor, which makes use of a neat hack
called the Burrows{Wheeler transform (Burrows and Wheeler, 1994). This
method is not based on an explicit probabilistic model, and it only works well
for (cid:12)les larger than several thousand characters; but in practice it is a very
e(cid:11)ective compressor for (cid:12)les in which the context of a character is a good
predictor for that character.5

2http://www.inference.phy.cam.ac.uk/mackay/itprnn/softwareI.html
3ftp://ftp.cs.toronto.edu/pub/radford/www/ac.software.html
4http://www.djvuzone.org/
5There is a lot of

information about the Burrows{Wheeler transform on the net.

http://dogma.net/DataCompression/BWT.shtml

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

122

Compression of a text (cid:12)le

Table 6.6 gives the computer time in seconds taken and the compression
achieved when these programs are applied to the LATEX (cid:12)le containing the
text of this chapter, of size 20,942 bytes.

6 | Stream Codes

Method

Compression Compressed size Uncompression

time = sec

(%age of 20,942)

time = sec

Table 6.6. Comparison of
compression algorithms applied to
a text (cid:12)le.

Laplace model
gzip
compress

0.28
0.10
0.05

bzip
bzip2
ppmz

12 974 (61%)
8 177 (39%)
10 816 (51%)

7 495 (36%)
7 640 (36%)
6 800 (32%)

0.32
0.01
0.05

Compression of a sparse (cid:12)le

Interestingly, gzip does not always do so well. Table 6.7 gives the compres-
sion achieved when these programs are applied to a text (cid:12)le containing 106
characters, each of which is either 0 and 1 with probabilities 0.99 and 0.01.
The Laplace model is quite well matched to this source, and the benchmark
arithmetic coder gives good performance, followed closely by compress; gzip
is worst. An ideal model for this source would compress the (cid:12)le into about
106H2(0:01)=8 ’ 10 100 bytes. The Laplace-model compressor falls short of
this performance because it is implemented using only eight-bit precision. The
ppmz compressor compresses the best of all, but takes much more computer
time.

Method

Compression Compressed size Uncompression

time = sec

= bytes

time = sec

Laplace model
gzip
gzip --best+
compress

bzip
bzip2
ppmz

0.45
0.22
1.63
0.13

0.30
0.19
533

14 143 (1.4%)
20 646 (2.1%)
15 553 (1.6%)
14 785 (1.5%)

10 903 (1.09%)
11 260 (1.12%)
10 447 (1.04%)

0.57
0.04
0.05
0.03

0.17
0.05
535

6.6 Summary

In the last three chapters we have studied three classes of data compression
codes.

Fixed-length block codes (Chapter 4). These are mappings from a (cid:12)xed
number of source symbols to a (cid:12)xed-length binary message. Only a tiny
fraction of the source strings are given an encoding. These codes were
fun for identifying the entropy as the measure of compressibility but they
are of little practical use.

Table 6.7. Comparison of
compression algorithms applied to
a random (cid:12)le of 106 characters,
99% 0s and 1% 1s.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.7: Exercises on stream codes

123

Symbol codes (Chapter 5). Symbol codes employ a variable-length code for
each symbol in the source alphabet, the codelengths being integer lengths
determined by the probabilities of the symbols. Hu(cid:11)man’s algorithm
constructs an optimal symbol code for a given set of symbol probabilities.

Every source string has a uniquely decodeable encoding, and if the source
symbols come from the assumed distribution then the symbol code will
compress to an expected length per character L lying in the interval
[H; H + 1). Statistical (cid:13)uctuations in the source may make the actual
length longer or shorter than this mean length.

If the source is not well matched to the assumed distribution then the
mean length is increased by the relative entropy DKL between the source
distribution and the code’s implicit distribution. For sources with small
entropy, the symbol has to emit at least one bit per source symbol;
compression below one bit per source symbol can be achieved only by
the cumbersome procedure of putting the source data into blocks.

Stream codes. The distinctive property of stream codes, compared with
symbol codes, is that they are not constrained to emit at least one bit for
every symbol read from the source stream. So large numbers of source
symbols may be coded into a smaller number of bits. This property
could be obtained using a symbol code only if the source stream were
somehow chopped into blocks.

(cid:15) Arithmetic codes combine a probabilistic model with an encoding
algorithm that identi(cid:12)es each string with a sub-interval of [0; 1) of
size equal to the probability of that string under the model. This
code is almost optimal in the sense that the compressed length of a
string x closely matches the Shannon information content of x given
the probabilistic model. Arithmetic codes (cid:12)t with the philosophy
that good compression requires data modelling, in the form of an
adaptive Bayesian model.

(cid:15) Lempel{Ziv codes are adaptive in the sense that they memorize
strings that have already occurred. They are built on the philoso-
phy that we don’t know anything at all about what the probability
distribution of the source will be, and we want a compression algo-
rithm that will perform reasonably well whatever that distribution
is.

Both arithmetic codes and Lempel{Ziv codes will fail to decode correctly
if any of the bits of the compressed (cid:12)le are altered. So if compressed (cid:12)les are
to be stored or transmitted over noisy media, error-correcting codes will be
essential. Reliable communication over unreliable channels is the topic of Part
II.

6.7 Exercises on stream codes

Exercise 6.7.[2 ] Describe an arithmetic coding algorithm to encode random bit
strings of length N and weight K (i.e., K ones and N (cid:0) K zeroes) where
N and K are given.

For the case N = 5, K = 2, show in detail the intervals corresponding to
all source substrings of lengths 1{5.

. Exercise 6.8.[2, p.128] How many bits are needed to specify a selection of K
objects from N objects? (N and K are assumed to be known and the

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

124

6 | Stream Codes

selection of K objects is unordered.) How might such a selection be
made at random without being wasteful of random bits?

. Exercise 6.9.[2 ] A binary source X emits independent identically distributed
symbols with probability distribution ff0; f1g, where f1 = 0:01. Find
an optimal uniquely-decodeable symbol code for a string x = x1x2x3 of
three successive samples from this source.

Estimate (to one decimal place) the factor by which the expected length
of this optimal code is greater than the entropy of the three-bit string x.
[H2(0:01) ’ 0:08, where H2(x) = x log2(1=x) + (1 (cid:0) x) log2(1=(1 (cid:0) x)).]
An arithmetic code is used to compress a string of 1000 samples from
the source X. Estimate the mean and standard deviation of the length
of the compressed (cid:12)le.

. Exercise 6.10.[2 ] Describe an arithmetic coding algorithm to generate random
bit strings of length N with density f (i.e., each bit has probability f of
being a one) where N is given.

Exercise 6.11.[2 ] Use a modi(cid:12)ed Lempel{Ziv algorithm in which, as discussed
on p.120, the dictionary of pre(cid:12)xes is pruned by writing new pre(cid:12)xes
into the space occupied by pre(cid:12)xes that will not be needed again.
Such pre(cid:12)xes can be identi(cid:12)ed when both their children have been
added to the dictionary of pre(cid:12)xes.
(You may neglect the issue of
termination of encoding.) Use this algorithm to encode the string
0100001000100010101000001. Highlight the bits that follow a pre(cid:12)x
on the second occasion that that pre(cid:12)x is used. (As discussed earlier,
these bits could be omitted.)

Exercise 6.12.[2, p.128] Show that this modi(cid:12)ed Lempel{Ziv code is still not
‘complete’, that is, there are binary strings that are not encodings of
any string.

. Exercise 6.13.[3, p.128] Give examples of simple sources that have low entropy

but would not be compressed well by the Lempel{Ziv algorithm.

6.8 Further exercises on data compression

The following exercises may be skipped by the reader who is eager to learn
about noisy channels.

Exercise 6.14.[3, p.130] Consider a Gaussian distribution in N dimensions,

n

(6.13)

P (x) =

1

(2(cid:25)(cid:27)2)N=2

exp(cid:18)(cid:0)Pn x2
2(cid:27)2 (cid:19) :
n(cid:1)1=2. Estimate the mean
De(cid:12)ne the radius of a point x to be r =(cid:0)Pn x2
and variance of the square of the radius, r2 =(cid:0)Pn x2
n(cid:1).
2(cid:27)2(cid:19) = 3(cid:27)4;

(2(cid:25)(cid:27)2)1=2 x4 exp(cid:18)(cid:0)

You may (cid:12)nd helpful the integral

Z dx

1

x2

though you should be able to estimate the required quantities without it.

(6.14)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.8: Further exercises on data compression

Assuming that N is large, show that nearly all the probability of a
Gaussian is contained in a thin shell of radius pN (cid:27). Find the thickness
of the shell.

Evaluate the probability density (6.13) at a point in that thin shell and
at the origin x = 0 and compare. Use the case N = 1000 as an example.

Notice that nearly all the probability mass is located in a di(cid:11)erent part
of the space from the region of highest probability density.

Exercise 6.15.[2 ] Explain what is meant by an optimal binary symbol code.

Find an optimal binary symbol code for the ensemble:

125

probability density
is maximized here

pN(cid:27)

almost all

probability mass is here

Figure 6.8. Schematic
representation of the typical set of
an N -dimensional Gaussian
distribution.

A = fa; b; c; d; e; f; g; h; i; jg;
10
100

9
100

5
100

4
100

;

8
100

;

;

2
100

;

;

6
100

;

P =(cid:26) 1

100

;

;

25
100

;

30

100(cid:27) ;

and compute the expected length of the code.

Exercise 6.16.[2 ] A string y = x1x2 consists of two independent samples from

an ensemble

X : AX = fa; b; cg;PX =(cid:26) 1

10

;

3
10

;

6

10(cid:27) :

What is the entropy of y? Construct an optimal binary symbol code for
the string y, and (cid:12)nd its expected length.

Exercise 6.17.[2 ] Strings of N independent samples from an ensemble with
P = f0:1; 0:9g are compressed using an arithmetic code that is matched
to that ensemble. Estimate the mean and standard deviation of the
compressed strings’ lengths for the case N = 1000. [H2(0:1) ’ 0:47]

Exercise 6.18.[3 ] Source coding with variable-length symbols.

In the chapters on source coding, we assumed that we were
encoding into a binary alphabet f0; 1g in which both symbols
should be used with equal frequency. In this question we ex-
plore how the encoding alphabet should be used if the symbols
take di(cid:11)erent times to transmit.

A poverty-stricken student communicates for free with a friend using a
telephone by selecting an integer n 2 f1; 2; 3 : : :g, making the friend’s
phone ring n times, then hanging up in the middle of the nth ring. This
process is repeated so that a string of symbols n1n2n3 : : : is received.
What is the optimal way to communicate? If large integers n are selected
then the message takes longer to communicate. If only small integers n
are used then the information content per symbol is small. We aim to
maximize the rate of information transfer, per unit time.

Assume that the time taken to transmit a number of rings n and to
redial is ln seconds. Consider a probability distribution over n, fpng.
De(cid:12)ning the average duration per symbol to be

L(p) =Xn

pnln

(6.15)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

126

6 | Stream Codes

and the entropy per symbol to be

H(p) =Xn

pn log2

1
pn

;

(6.16)

show that for the average information rate per second to be maximized,
the symbols must be used with probabilities of the form

pn =

1
Z

2(cid:0)(cid:12)ln

where Z =Pn 2(cid:0)(cid:12)ln and (cid:12) satis(cid:12)es the implicit equation

(cid:12) =

H(p)
L(p)

;

(6.17)

(6.18)

that is, (cid:12) is the rate of communication. Show that these two equations
(6.17, 6.18) imply that (cid:12) must be set such that

log Z = 0:

Assuming that the channel has the property

ln = n seconds;

(6.19)

(6.20)

(cid:12)nd the optimal distribution p and show that the maximal information
rate is 1 bit per second.

How does this compare with the information rate per second achieved if
p is set to (1=2; 1=2; 0; 0; 0; 0; : : :) | that is, only the symbols n = 1 and
n = 2 are selected, and they have equal probability?

Discuss the relationship between the results (6.17, 6.19) derived above,
and the Kraft inequality from source coding theory.

How might a random binary source be e(cid:14)ciently encoded into a se-
quence of symbols n1n2n3 : : : for transmission over the channel de(cid:12)ned
in equation (6.20)?

. Exercise 6.19.[1 ] How many bits does it take to shu(cid:15)e a pack of cards?

. Exercise 6.20.[2 ] In the card game Bridge, the four players receive 13 cards
each from the deck of 52 and start each game by looking at their own
hand and bidding. The legal bids are, in ascending order 1|; 1}; 1~; 1(cid:127);
1N T; 2|; 2}; : : : 7~; 7(cid:127); 7N T , and successive bids must follow this
order; a bid of, say, 2~ may only be followed by higher bids such as 2(cid:127)
or 3| or 7N T . (Let us neglect the ‘double’ bid.)
The players have several aims when bidding. One of the aims is for two
partners to communicate to each other as much as possible about what
cards are in their hands.

Let us concentrate on this task.

(a) After the cards have been dealt, how many bits are needed for North

to convey to South what her hand is?

(b) Assuming that E and W do not bid at all, what is the maximum
total information that N and S can convey to each other while
bidding? Assume that N starts the bidding, and that once either
N or S stops bidding, the bidding stops.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.9: Solutions

127

. Exercise 6.21.[2 ] My old ‘arabic’ microwave oven had 11 buttons for entering
cooking times, and my new ‘roman’ microwave has just (cid:12)ve. The but-
tons of the roman microwave are labelled ‘10 minutes’, ‘1 minute’, ‘10
seconds’, ‘1 second’, and ‘Start’; I’ll abbreviate these (cid:12)ve strings to the
symbols M, C, X, I, 2.
To enter one minute and twenty-three seconds
(1:23), the arabic sequence is

Arabic

Roman

M
C

X
I 2

1
4
7

3
2
6
5
8
9
0 2

1232;

(6.21)

Figure 6.9. Alternative keypads
for microwave ovens.

and the roman sequence is

CXXIII2:

(6.22)

Each of these keypads de(cid:12)nes a code mapping the 3599 cooking times
from 0:01 to 59:59 into a string of symbols.

(a) Which times can be produced with two or three symbols? (For
example, 0:20 can be produced by three symbols in either code:
XX2 and 202.)

(b) Are the two codes complete? Give a detailed answer.

(c) For each code, name a cooking time that it can produce in four

symbols that the other code cannot.

(d) Discuss the implicit probability distributions over times to which

each of these codes is best matched.

(e) Concoct a plausible probability distribution over times that a real
user might use, and evaluate roughly the expected number of sym-
bols, and maximum number of symbols, that each code requires.
Discuss the ways in which each code is ine(cid:14)cient or e(cid:14)cient.

(f) Invent a more e(cid:14)cient cooking-time-encoding system for a mi-

crowave oven.

Exercise 6.22.[2, p.132] Is the standard binary representation for positive inte-

gers (e.g. cb(5) = 101) a uniquely decodeable code?

Design a binary code for the positive integers,
i.e., a mapping from
n 2 f1; 2; 3; : : :g to c(n) 2 f0; 1g+, that is uniquely decodeable. Try
to design codes that are pre(cid:12)x codes and that satisfy the Kraft equality
Pn 2(cid:0)ln = 1.

Motivations: any data (cid:12)le terminated by a special end of (cid:12)le character
can be mapped onto an integer, so a pre(cid:12)x code for integers can be used
as a self-delimiting encoding of (cid:12)les too. Large (cid:12)les correspond to large
integers. Also, one of the building blocks of a ‘universal’ coding scheme {
that is, a coding scheme that will work OK for a large variety of sources
{ is the ability to encode integers. Finally, in microwave ovens, cooking
times are positive integers!

Discuss criteria by which one might compare alternative codes for inte-
gers (or, equivalently, alternative self-delimiting codes for (cid:12)les).

6.9 Solutions

Solution to exercise 6.1 (p.115). The worst-case situation is when the interval
to be represented lies just inside a binary interval. In this case, we may choose
either of two binary intervals as shown in (cid:12)gure 6.10. These binary intervals

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

128

6 | Stream Codes

Source string’s interval

Binary intervals

6

P (xjH)

?

6

?
6

?

Figure 6.10. Termination of
arithmetic coding in the worst
case, where there is a two bit
overhead. Either of the two
binary intervals marked on the
right-hand side may be chosen.
These binary intervals are no
smaller than P (xjH)=4.

are no smaller than P (xjH)=4, so the binary encoding has a length no greater
than log2 1=P (xjH) + log2 4, which is two bits more than the ideal message
length.

Solution to exercise 6.3 (p.118). The standard method uses 32 random bits
per generated symbol and so requires 32 000 bits to generate one thousand
samples.

Arithmetic coding uses on average about H2(0:01) = 0:081 bits per gener-
ated symbol, and so requires about 83 bits to generate one thousand samples
(assuming an overhead of roughly two bits associated with termination).

Fluctuations in the number of 1s would produce variations around this

mean with standard deviation 21.

Solution to exercise 6.5 (p.120). The encoding is 010100110010110001100,
which comes from the parsing

0; 00; 000; 0000; 001; 00000; 000000

(6.23)

which is encoded thus:

(; 0); (1; 0); (10; 0); (11; 0); (010; 1); (100; 0); (110; 0):

(6.24)

Solution to exercise 6.6 (p.120). The decoding is

0100001000100010101000001.

Solution to exercise 6.8 (p.123). This problem is equivalent to exercise 6.7
(p.123).

K(cid:1)e bits ’

The selection of K objects from N objects requires dlog 2(cid:0)N

N H2(K=N ) bits. This selection could be made using arithmetic coding. The
selection corresponds to a binary string of length N in which the 1 bits rep-
resent which objects are selected. Initially the probability of a 1 is K=N and
the probability of a 0 is (N(cid:0)K)=N . Thereafter, given that the emitted string
thus far, of length n, contains k 1s, the probability of a 1 is (K(cid:0)k)=(N (cid:0)n)
and the probability of a 0 is 1 (cid:0) (K(cid:0)k)=(N(cid:0)n).
Solution to exercise 6.12 (p.124). This modi(cid:12)ed Lempel{Ziv code is still not
‘complete’, because, for example, after (cid:12)ve pre(cid:12)xes have been collected, the
pointer could be any of the strings 000, 001, 010, 011, 100, but it cannot be
101, 110 or 111. Thus there are some binary strings that cannot be produced
as encodings.

Solution to exercise 6.13 (p.124). Sources with low entropy that are not well
compressed by Lempel{Ziv include:

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.9: Solutions

129

(a) Sources with some symbols that have long range correlations and inter-
vening random junk. An ideal model should capture what’s correlated
and compress it. Lempel{Ziv can compress the correlated features only
by memorizing all cases of the intervening junk. As a simple example,
consider a telephone book in which every line contains an (old number,
new number) pair:

285-3820:572-58922
258-8302:593-20102

The number of characters per line is 18, drawn from the 13-character
alphabet f0; 1; : : : ; 9;(cid:0); :; 2g. The characters ‘-’, ‘:’ and ‘2’ occur in a
predictable sequence, so the true information content per line, assuming
all the phone numbers are seven digits long, and assuming that they are
random sequences, is about 14 bans. (A ban is the information content of
a random integer between 0 and 9.) A (cid:12)nite state language model could
easily capture the regularities in these data. A Lempel{Ziv algorithm
will take a long time before it compresses such a (cid:12)le down to 14 bans
per line, however, because in order for it to ‘learn’ that the string :ddd
is always followed by -, for any three digits ddd, it will have to see all
those strings. So near-optimal compression will only be achieved after
thousands of lines of the (cid:12)le have been read.

Figure 6.11. A source with low entropy that is not well compressed by Lempel{Ziv. The bit sequence
is read from left to right. Each line di(cid:11)ers from the line above in f = 5% of its bits. The
image width is 400 pixels.

(b) Sources with long range correlations, for example two-dimensional im-
ages that are represented by a sequence of pixels, row by row, so that
vertically adjacent pixels are a distance w apart in the source stream,
where w is the image width. Consider, for example, a fax transmission in
which each line is very similar to the previous line ((cid:12)gure 6.11). The true
entropy is only H2(f ) per pixel, where f is the probability that a pixel
di(cid:11)ers from its parent. Lempel{Ziv algorithms will only compress down
to the entropy once all strings of length 2w = 2400 have occurred and
their successors have been memorized. There are only about 2300 par-
ticles in the universe, so we can con(cid:12)dently say that Lempel{Ziv codes
will never capture the redundancy of such an image.

Another highly redundant texture is shown in (cid:12)gure 6.12. The image was
made by dropping horizontal and vertical pins randomly on the plane. It
contains both long-range vertical correlations and long-range horizontal
correlations. There is no practical way that Lempel{Ziv, fed with a
pixel-by-pixel scan of this image, could capture both these correlations.

Biological computational systems can readily identify the redundancy in
these images and in images that are much more complex; thus we might
anticipate that the best data compression algorithms will result from the
development of arti(cid:12)cial intelligence methods.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

130

6 | Stream Codes

Figure 6.12. A texture consisting of horizontal and vertical pins dropped at random on the plane.

(c) Sources with intricate redundancy, such as (cid:12)les generated by computers.
For example, a LATEX (cid:12)le followed by its encoding into a PostScript
(cid:12)le. The information content of this pair of (cid:12)les is roughly equal to the
information content of the LATEX (cid:12)le alone.

(d) A picture of the Mandelbrot set. The picture has an information content
equal to the number of bits required to specify the range of the complex
plane studied, the pixel sizes, and the colouring rule used.

(e) A picture of a ground state of a frustrated antiferromagnetic Ising model
((cid:12)gure 6.13), which we will discuss in Chapter 31. Like (cid:12)gure 6.12, this
binary image has interesting correlations in two directions.

Figure 6.13. Frustrated triangular
Ising model in one of its ground
states.

(f) Cellular automata { (cid:12)gure 6.14 shows the state history of 100 steps of
a cellular automaton with 400 cells. The update rule, in which each
cell’s new state depends on the state of (cid:12)ve preceding cells, was selected
at random. The information content is equal to the information in the
boundary (400 bits), and the propagation rule, which here can be de-
scribed in 32 bits. An optimal compressor will thus give a compressed (cid:12)le
length which is essentially constant, independent of the vertical height of
the image. Lempel{Ziv would only give this zero-cost compression once
the cellular automaton has entered a periodic limit cycle, which could
easily take about 2100 iterations.

In contrast, the JBIG compression method, which models the probability
of a pixel given its local context and uses arithmetic coding, would do a
good job on these images.

Solution to exercise 6.14 (p.124). For a one-dimensional Gaussian, the vari-
ance of x, E[x2], is (cid:27)2. So the mean value of r2 in N dimensions, since the
components of x are independent random variables, is

E[r2] = N (cid:27)2:

(6.25)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

6.9: Solutions

131

Figure 6.14. The 100-step time-history of a cellular automaton with 400 cells.

The variance of r2, similarly, is N times the variance of x2, where x is a
one-dimensional Gaussian variable.

var(x2) =Z dx

1

(2(cid:25)(cid:27)2)1=2

x4 exp(cid:18)(cid:0)

x2

2(cid:27)2(cid:19) (cid:0) (cid:27)4:

(6.26)

The integral is found to be 3(cid:27)4 (equation (6.14)), so var(x2) = 2(cid:27)4. Thus the
variance of r2 is 2N (cid:27)4.
For large N , the central-limit theorem indicates that r 2 has a Gaussian
distribution with mean N (cid:27)2 and standard deviation p2N (cid:27)2, so the probability
density of r must similarly be concentrated about r ’ pN (cid:27).
The thickness of this shell is given by turning the standard deviation
of r2 into a standard deviation on r:
for small (cid:14)r=r, (cid:14) log r = (cid:14)r=r =
(1/2)(cid:14) log r2 = (1/2)(cid:14)(r2)=r2, so setting (cid:14)(r2) = p2N (cid:27)2, r has standard de-
viation (cid:14)r = (1/2)r(cid:14)(r2)=r2 = (cid:27)=p2.
The probability density of the Gaussian at a point xshell where r = pN (cid:27)

is

P (xshell) =

1

(2(cid:25)(cid:27)2)N=2

exp(cid:18)(cid:0)

N (cid:27)2

2(cid:27)2 (cid:19) =

1

(2(cid:25)(cid:27)2)N=2

exp(cid:18)(cid:0)

N

2 (cid:19) :

Whereas the probability density at the origin is

P (x = 0) =

1

(2(cid:25)(cid:27)2)N=2

:

(6.27)

(6.28)

Thus P (xshell)=P (x = 0) = exp ((cid:0)N=2) : The probability density at the typical
radius is e(cid:0)N=2 times smaller than the density at the origin. If N = 1000, then
the probability density at the origin is e500 times greater.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

7

Codes for Integers

This chapter is an aside, which may safely be skipped.

Solution to exercise 6.22 (p.127)

To discuss the coding of integers we need some de(cid:12)nitions.

The standard binary representation of a positive integer n will be

denoted by cb(n), e.g., cb(5) = 101, cb(45) = 101101.

The standard binary length of a positive integer n,

lb(n),
length of the string cb(n). For example, lb(5) = 3, lb(45) = 6.

is

the

The standard binary representation cb(n) is not a uniquely decodeable code
for integers since there is no way of knowing when an integer has ended. For
example, cb(5)cb(5) is identical to cb(45). It would be uniquely decodeable if
we knew the standard binary length of each integer before it was received.

Noticing that all positive integers have a standard binary representation

that starts with a 1, we might de(cid:12)ne another representation:

The headless binary representation of a positive integer n will be de-
noted by cB(n), e.g., cB(5) = 01, cB(45) = 01101 and cB(1) = (cid:21) (where
(cid:21) denotes the null string).

This representation would be uniquely decodeable if we knew the length lb(n)
of the integer.

So, how can we make a uniquely decodeable code for integers? Two strate-

gies can be distinguished.

1. Self-delimiting codes. We (cid:12)rst communicate somehow the length of
the integer, lb(n), which is also a positive integer; then communicate the
original integer n itself using cB(n).

2. Codes with ‘end of (cid:12)le’ characters. We code the integer into blocks
of length b bits, and reserve one of the 2b symbols to have the special
meaning ‘end of (cid:12)le’. The coding of integers into blocks is arranged so
that this reserved symbol is not needed for any other purpose.

The simplest uniquely decodeable code for integers is the unary code, which

can be viewed as a code with an end of (cid:12)le character.

132

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

7 | Codes for Integers

133

Unary code. An integer n is encoded by sending a string of n(cid:0)1 0s followed

by a 1.

n cU(n)

1
01
001
0001
00001

1
2
3
4
5
...
45 000000000000000000000000000000000000000000001

The unary code has length lU(n) = n.
The unary code is the optimal code for integers if the probability distri-
bution over n is pU(n) = 2(cid:0)n.

Self-delimiting codes

We can use the unary code to encode the length of the binary encoding of n
and make a self-delimiting code:

Code C(cid:11). We send the unary code for lb(n), followed by the headless binary

representation of n.

c(cid:11)(n) = cU[lb(n)]cB(n):

(7.1)

Table 7.1 shows the codes for some integers. The overlining indicates
the division of each string into the parts cU[lb(n)] and cB(n). We might
equivalently view c(cid:11)(n) as consisting of a string of (lb(n) (cid:0) 1) zeroes
followed by the standard binary representation of n, cb(n).
The codeword c(cid:11)(n) has length l(cid:11)(n) = 2lb(n) (cid:0) 1.
The implicit probability distribution over n for the code C(cid:11) is separable
into the product of a probability distribution over the length l,

and a uniform distribution over integers having that length,

P (l) = 2(cid:0)l;

P (nj l) =(cid:26) 2(cid:0)l+1

0

lb(n) = l
otherwise:

(7.2)

(7.3)

Now, for the above code, the header that communicates the length always
occupies the same number of bits as the standard binary representation of
the integer (give or take one). If we are expecting to encounter large integers
(large (cid:12)les) then this representation seems suboptimal, since it leads to all (cid:12)les
occupying a size that is double their original uncoded size. Instead of using
the unary code to encode the length lb(n), we could use C(cid:11).

Code C(cid:12). We send the length lb(n) using C(cid:11), followed by the headless binary

representation of n.

c(cid:12)(n) = c(cid:11)[lb(n)]cB(n):

Iterating this procedure, we can de(cid:12)ne a sequence of codes.

Code C(cid:13).

Code C(cid:14).

c(cid:13)(n) = c(cid:12)[lb(n)]cB(n):

c(cid:14)(n) = c(cid:13)[lb(n)]cB(n):

(7.4)

(7.5)

(7.6)

n

1
2
3
4
5
6
...
45

cb(n) lb(n) c(cid:11)(n)

1
10
11
100
101
110

101101

1
2
2
3
3
3

6

1
010
011
00100
00101
00110

00000101101

Table 7.1. C(cid:11).

n

1
2
3
4
5
6
...
45

c(cid:12)(n)

1
0100
0101
01100
01101
01110

c(cid:13) (n)

1
01000
01001
010100
010101
010110

0011001101

0111001101

Table 7.2. C(cid:12) and C(cid:13).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

7 | Codes for Integers

n

c3(n)

c7(n)

001 111
010 111
011 111

01 11
10 11
01 00 11

1
2
3
...
45 01 10 00 00 11 110 011 111

Table 7.3. Two codes with
end-of-(cid:12)le symbols, C3 and C7.
Spaces have been included to
show the byte boundaries.

134

Codes with end-of-(cid:12)le symbols

We can also make byte-based representations. (Let’s use the term byte (cid:13)exibly
here, to denote any (cid:12)xed-length string of bits, not just a string of length 8
bits.) If we encode the number in some base, for example decimal, then we
can represent each digit in a byte. In order to represent a digit from 0 to 9 in a
byte we need four bits. Because 24 = 16, this leaves 6 extra four-bit symbols,
f1010, 1011, 1100, 1101, 1110, 1111g, that correspond to no decimal digit.
We can use these as end-of-(cid:12)le symbols to indicate the end of our positive
integer.

Clearly it is redundant to have more than one end-of-(cid:12)le symbol, so a more
e(cid:14)cient code would encode the integer into base 15, and use just the sixteenth
symbol, 1111, as the punctuation character. Generalizing this idea, we can
make similar byte-based codes for integers in bases 3 and 7, and in any base
of the form 2n (cid:0) 1.
These codes are almost complete. (Recall that a code is ‘complete’ if it
satis(cid:12)es the Kraft inequality with equality.) The codes’ remaining ine(cid:14)ciency
is that they provide the ability to encode the integer zero and the empty string,
neither of which was required.

. Exercise 7.1.[2, p.136] Consider the implicit probability distribution over inte-

gers corresponding to the code with an end-of-(cid:12)le character.

(a) If the code has eight-bit blocks (i.e., the integer is coded in base
255), what is the mean length in bits of the integer, under the
implicit distribution?

(b) If one wishes to encode binary (cid:12)les of expected size about one hun-
dred kilobytes using a code with an end-of-(cid:12)le character, what is
the optimal block size?

Encoding a tiny (cid:12)le

To illustrate the codes we have discussed, we now use each code to encode a
small (cid:12)le consisting of just 14 characters,

Claude Shannon :

(cid:15) If we map the ASCII characters onto seven-bit symbols (e.g., in decimal,
C = 67, l = 108, etc.), this 14 character (cid:12)le corresponds to the integer

n = 167 987 786 364 950 891 085 602 469 870 (decimal):

(cid:15) The unary code for n consists of this many (less one) zeroes, followed by
a one. If all the oceans were turned into ink, and if we wrote a hundred
bits with every cubic millimeter, there might be enough ink to write
cU(n).

(cid:15) The standard binary representation of n is this length-98 sequence of

bits:

cb(n) = 1000011110110011000011110101110010011001010100000
1010011110100011000011101110110111011011111101110:

. Exercise 7.2.[2 ] Write down or describe the following self-delimiting represen-
tations of the above number n: c(cid:11)(n), c(cid:12)(n), c(cid:13)(n), c(cid:14)(n), c3(n), c7(n),
and c15(n). Which of these encodings is the shortest? [Answer: c15.]

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

135

Algorithm 7.4. Elias’s encoder for
an integer n.

7 | Codes for Integers

Comparing the codes

One could answer the question ‘which of two codes is superior?’ by a sentence
of the form ‘For n > k, code 1 is superior, for n < k, code 2 is superior’ but I
contend that such an answer misses the point: any complete code corresponds
to a prior for which it is optimal; you should not say that any other code is
superior to it. Other codes are optimal for other priors. These implicit priors
should be thought about so as to achieve the best code for one’s application.

Notice that one cannot, for free, switch from one code to another, choosing
If one were to do this, then it would be necessary to
whichever is shorter.
lengthen the message in some way that indicates which of the two codes is
being used. If this is done by a single leading bit, it will be found that the
resulting code is suboptimal because it fails the Kraft equality, as was discussed
in exercise 5.33 (p.104).

Another way to compare codes for integers is to consider a sequence of
probability distributions, such as monotonic probability distributions over n (cid:21)
1, and rank the codes as to how well they encode any of these distributions.
A code is called a ‘universal’ code if for any distribution in a given class, it
encodes into an average length that is within some factor of the ideal average
length.

Let me say this again. We are meeting an alternative world view { rather
than (cid:12)guring out a good prior over integers, as advocated above, many the-
orists have studied the problem of creating codes that are reasonably good
codes for any priors in a broad class. Here the class of priors convention-
ally considered is the set of priors that (a) assign a monotonically decreasing
probability over integers and (b) have (cid:12)nite entropy.

Several of the codes we have discussed above are universal. Another code
which elegantly transcends the sequence of self-delimiting codes is Elias’s ‘uni-
versal code for integers’ (Elias, 1975), which e(cid:11)ectively chooses from all the
codes C(cid:11); C(cid:12); : : : : It works by sending a sequence of messages each of which
encodes the length of the next message, and indicates by a single bit whether
or not that message is the (cid:12)nal integer (in its standard binary representation).
Because a length is a positive integer and all positive integers begin with ‘1’,
all the leading 1s can be omitted.

Write ‘0’
Loop f

g

If blog nc = 0 halt
Prepend cb(n) to the written string
n:=blog nc

The encoder of C! is shown in algorithm 7.4. The encoding is generated

from right to left. Table 7.5 shows the resulting codewords.

. Exercise 7.3.[2 ] Show that the Elias code is not actually the best code for a
prior distribution that expects very large integers. (Do this by construct-
ing another code and specifying how large n must be for your code to
give a shorter length than Elias’s.)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

136

n

c!(n)

1
2
3
4
5
6
7
8

0
100
110
101000
101010
101100
101110
1110000

Solutions

7 | Codes for Integers

n

9
10
11
12
13
14
15
16

c!(n)

1110010
1110100
1110110
1111000
1111010
1111100
1111110
10100100000

n

31
32
45
63
64
127
128
255

c!(n)

n

c!(n)

10100111110
101011000000
101011011010
101011111110
1011010000000
1011011111110
10111100000000
10111111111110

256
365
511
512
719
1023
1024
1025

1110001000000000
1110001011011010
1110001111111110
11100110000000000
11100110110011110
11100111111111110
111010100000000000
111010100000000010

Table 7.5. Elias’s ‘universal’ code
for integers. Examples from 1 to
1025.

Solution to exercise 7.1 (p.134). The use of the end-of-(cid:12)le symbol in a code
that represents the integer in some base q corresponds to a belief that there is
a probability of (1=(q + 1)) that the current character is the last character of
the number. Thus the prior to which this code is matched puts an exponential
prior distribution over the length of the integer.

(a) The expected number of characters is q +1 = 256, so the expected length

of the integer is 256 (cid:2) 8 ’ 2000 bits.

(b) We wish to (cid:12)nd q such that q log q ’ 800 000 bits. A value of q between
215 and 216 satis(cid:12)es this constraint, so 16-bit blocks are roughly the
optimal size, assuming there is one end-of-(cid:12)le character.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part II

Noisy-Channel Coding

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

8

Dependent Random Variables

In the last three chapters on data compression we concentrated on random
vectors x coming from an extremely simple probability distribution, namely
the separable distribution in which each component xn is independent of the
others.

In this chapter, we consider joint ensembles in which the random variables
are dependent. This material has two motivations. First, data from the real
world have interesting correlations, so to do data compression well, we need
to know how to work with models that include dependences. Second, a noisy
channel with input x and output y de(cid:12)nes a joint ensemble in which x and y are
dependent { if they were independent, it would be impossible to communicate
over the channel { so communication over noisy channels (the topic of chapters
9{11) is described in terms of the entropy of joint ensembles.

8.1 More about entropy

This section gives de(cid:12)nitions and exercises to do with entropy, carrying on
from section 2.4.

The joint entropy of X; Y is:

H(X; Y ) = Xxy2AXAY

P (x; y) log

1

P (x; y)

:

(8.1)

Entropy is additive for independent random variables:

H(X; Y ) = H(X) + H(Y ) i(cid:11) P (x; y) = P (x)P (y):

(8.2)

The conditional entropy of X given y = bk is the entropy of the proba-

bility distribution P (xj y = bk).
H(X j y = bk) (cid:17) Xx2AX

P (xj y = bk) log

1

P (xj y = bk)

:

(8.3)

The conditional entropy of X given Y is the average, over y, of the con-

ditional entropy of X given y.

H(X j Y ) (cid:17) Xy2AY

P (y)2
= Xxy2AXAY

4 Xx2AX

P (x; y) log

1

P (xj y)3
5

(8.4)

P (xj y) log

1

P (xj y)

:

This measures the average uncertainty that remains about x when y is
known.

138

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

8.1: More about entropy

139

The marginal entropy of X is another name for the entropy of X, H(X),

used to contrast it with the conditional entropies listed above.

Chain rule for information content. From the product rule for probabil-

ities, equation (2.6), we obtain:

so

log

1

P (x; y)

= log

1

P (x)

+ log

1

P (y j x)

h(x; y) = h(x) + h(y j x):

(8.5)

(8.6)

In words, this says that the information content of x and y is the infor-
mation content of x plus the information content of y given x.

Chain rule for entropy. The

joint
marginal entropy are related by:

entropy,

conditional

entropy and

H(X; Y ) = H(X) + H(Y j X) = H(Y ) + H(X j Y ):

(8.7)

In words, this says that the uncertainty of X and Y is the uncertainty
of X plus the uncertainty of Y given X.

The mutual information between X and Y is

I(X; Y ) (cid:17) H(X) (cid:0) H(X j Y );
and satis(cid:12)es I(X; Y ) = I(Y ; X), and I(X; Y ) (cid:21) 0.
It measures the
average reduction in uncertainty about x that results from learning the
value of y; or vice versa, the average amount of information that x
conveys about y.

(8.8)

The conditional mutual information between X and Y given z = ck

is the mutual information between the random variables X and Y in
the joint ensemble P (x; y j z = ck),

I(X; Y j z = ck) = H(X j z = ck) (cid:0) H(X j Y; z = ck):

(8.9)

The conditional mutual information between X and Y given Z is

the average over z of the above conditional mutual information.

I(X; Y j Z) = H(X j Z) (cid:0) H(X j Y; Z):

(8.10)

No other ‘three-term entropies’ will be de(cid:12)ned. For example, expres-
sions such as I(X; Y ; Z) and I(X j Y ; Z) are illegal. But you may put
conjunctions of arbitrary numbers of variables in each of the three spots
in the expression I(X; Y j Z) { for example, I(A; B; C; D j E; F ) is (cid:12)ne:
it measures how much information on average c and d convey about a
and b, assuming e and f are known.

Figure 8.1 shows how the total entropy H(X; Y ) of a joint ensemble can be
broken down. This (cid:12)gure is important.

(cid:3)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

140

8 | Dependent Random Variables

H(X; Y )

H(X)

H(Y )

H(X j Y )

I(X; Y )

H(Y jX)

Figure 8.1. The relationship
between joint information,
marginal entropy, conditional
entropy and mutual entropy.

8.2 Exercises

. Exercise 8.1.[1 ] Consider three independent random variables u; v; w with en-
tropies Hu; Hv; Hw. Let X (cid:17) (U; V ) and Y (cid:17) (V; W ). What is H(X; Y )?
What is H(X j Y )? What is I(X; Y )?

. Exercise 8.2.[3, p.142] Referring to the de(cid:12)nitions of conditional entropy (8.3{
8.4), con(cid:12)rm (with an example) that it is possible for H(X j y = bk) to
exceed H(X), but that the average, H(X j Y ), is less than H(X). So
data are helpful { they do not increase uncertainty, on average.

. Exercise 8.3.[2, p.143] Prove the chain rule for entropy,

equation (8.7).

[H(X; Y ) = H(X) + H(Y j X)].

Exercise 8.4.[2, p.143] Prove that the mutual information I(X; Y ) (cid:17) H(X) (cid:0)

H(X j Y ) satis(cid:12)es I(X; Y ) = I(Y ; X) and I(X; Y ) (cid:21) 0.
[Hint: see exercise 2.26 (p.37) and note that

I(X; Y ) = DKL(P (x; y)jjP (x)P (y)):]

(8.11)

Exercise 8.5.[4 ] The ‘entropy distance’ between two random variables can be
de(cid:12)ned to be the di(cid:11)erence between their joint entropy and their mutual
information:

DH (X; Y ) (cid:17) H(X; Y ) (cid:0) I(X; Y ):

(8.12)

Prove that the entropy distance satis(cid:12)es the axioms for a distance {
DH (X; Y ) (cid:21) 0, DH (X; X) = 0, DH (X; Y ) = DH (Y; X), and DH (X; Z) (cid:20)
DH (X; Y ) + DH (Y; Z).
[Incidentally, we are unlikely to see DH (X; Y )
again but it is a good function on which to practise inequality-proving.]

Exercise 8.6.[2, p.147] A joint ensemble XY has the following joint distribution.

1 2 3 4

P (x; y)

x

1

2

3

4

1/8
1/16
1/16
1/4

1/16
1/8
1/16

1/32
1/32
1/16

1/32
1/32
1/16

0

0

0

y

1

2

3

4

1
2
3
4

What is the joint entropy H(X; Y )? What are the marginal entropies
H(X) and H(Y )? For each value of y, what is the conditional entropy
H(X j y)? What is the conditional entropy H(X j Y )? What is the
conditional entropy of Y given X? What is the mutual information
between X and Y ?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

8.3: Further exercises

141

Exercise 8.7.[2, p.143] Consider the ensemble XY Z in which AX = AY =
AZ = f0; 1g, x and y are independent with PX = fp; 1 (cid:0) pg and
PY = fq; 1(cid:0)qg and
(8.13)

z = (x + y) mod 2:

(a) If q = 1/2, what is PZ? What is I(Z; X)?
(b) For general p and q, what is PZ? What is I(Z; X)? Notice that
this ensemble is related to the binary symmetric channel, with x =
input, y = noise, and z = output.

H(Y)

Figure 8.2. A misleading
representation of entropies
(contrast with (cid:12)gure 8.1).

H(X|Y)

I(X;Y)

H(Y|X)

H(X,Y)

H(X)

Three term entropies

Exercise 8.8.[3, p.143] Many texts draw (cid:12)gure 8.1 in the form of a Venn diagram
((cid:12)gure 8.2). Discuss why this diagram is a misleading representation
of entropies. Hint: consider the three-variable ensemble XY Z in which
x 2 f0; 1g and y 2 f0; 1g are independent binary variables and z 2 f0; 1g
is de(cid:12)ned to be z = x + y mod 2.

8.3 Further exercises

The data-processing theorem

The data processing theorem states that data processing can only destroy
information.

Exercise 8.9.[3, p.144] Prove this theorem by considering an ensemble W DR
in which w is the state of the world, d is data gathered, and r is the
processed data, so that these three variables form a Markov chain

that is, the probability P (w; d; r) can be written as

w ! d ! r;

P (w; d; r) = P (w)P (dj w)P (r j d):

(8.14)

(8.15)

Show that the average information that R conveys about W, I(W ; R), is
less than or equal to the average information that D conveys about W ,
I(W ; D).

This theorem is as much a caution about our de(cid:12)nition of ‘information’ as it
is a caution about data processing!

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

142

8 | Dependent Random Variables

Inference and information measures

Exercise 8.10.[2 ] The three cards.

(a) One card is white on both faces; one is black on both faces; and one
is white on one side and black on the other. The three cards are
shu(cid:15)ed and their orientations randomized. One card is drawn and
placed on the table. The upper face is black. What is the colour of
its lower face? (Solve the inference problem.)

(b) Does seeing the top face convey information about the colour of
the bottom face? Discuss the information contents and entropies
in this situation. Let the value of the upper face’s colour be u and
the value of the lower face’s colour be l.
Imagine that we draw
a random card and learn both u and l. What is the entropy of
u, H(U )? What is the entropy of l, H(L)? What is the mutual
information between U and L, I(U ; L)?

Entropies of Markov processes

. Exercise 8.11.[3 ] In the guessing game, we imagined predicting the next letter
in a document starting from the beginning and working towards the end.
Consider the task of predicting the reversed text, that is, predicting the
letter that precedes those already known. Most people (cid:12)nd this a harder
task. Assuming that we model the language using an N -gram model
(which says the probability of the next character depends only on the
N (cid:0) 1 preceding characters), is there any di(cid:11)erence between the average
information contents of the reversed language and the forward language?

8.4 Solutions

Solution to exercise 8.2 (p.140). See exercise 8.6 (p.140) for an example where
H(X j y) exceeds H(X) (set y = 3).
We can prove the inequality H(X j Y ) (cid:20) H(X) by turning the expression
into a relative entropy (using Bayes’ theorem) and invoking Gibbs’ inequality
(exercise 2.26 (p.37)):

1

P (xj y)3
5

4 Xx2AX

P (x; y) log

P (xj y) log

1

P (xj y)
P (y)

H(X j Y ) (cid:17) Xy2AY

P (y)2
= Xxy2AXAY
= Xxy
= Xx

P (x) log

(8.16)

(8.17)

P (x)P (y j x) log

1

P (x)

P (y j x)P (x)
+Xx
P (x)Xy

P (y j x) log

P (y)
P (y j x)

: (8.18)

The last expression is a sum of relative entropies between the distributions
P (y j x) and P (y). So

H(X j Y ) (cid:20) H(X) + 0;

(8.19)

with equality only if P (y j x) = P (y) for all x and y (that is, only if X and Y
are independent).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

8.4: Solutions

143

Solution to exercise 8.3 (p.140). The chain rule for entropy follows from the
decomposition of a joint probability:

1

P (x; y)

P (x; y) log

H(X; Y ) = Xxy
= Xxy
= Xx
P (x)
= H(X) + H(Y j X):

P (x)P (y j x)(cid:20)log
+Xx

P (x) log

1

1

P (x)

+ log

P (x)Xy

1

P (y j x)(cid:21)
P (y j x) log

1

P (y j x)

Solution to exercise 8.4 (p.140). Symmetry of mutual information:

I(X; Y ) = H(X) (cid:0) H(X j Y )

P (x) log

P (x; y) log

1

P (x) (cid:0)Xxy
P (xj y)
P (x)

P (x; y) log

P (x; y)

P (x)P (y)

:

= Xx
= Xxy
= Xxy

P (x; y) log

1

P (xj y)

(8.20)

(8.21)

(8.22)

(8.23)

(8.24)

(8.25)

(8.26)

(8.27)

(8.29)

This expression is symmetric in x and y so

I(X; Y ) = H(X) (cid:0) H(X j Y ) = H(Y ) (cid:0) H(Y j X):

(8.28)

We can prove that mutual information is positive in two ways. One is to
continue from

I(X; Y ) =Xx;y

P (x; y) log

P (x; y)

P (x)P (y)

which is a relative entropy and use Gibbs’ inequality (proved on p.44), which
asserts that this relative entropy is (cid:21) 0, with equality only if P (x; y) =
P (x)P (y), that is, if X and Y are independent.

The other is to use Jensen’s inequality on

P (x; y) log

(cid:0)Xx;y

P (x)P (y)

P (x; y) (cid:21) (cid:0) logXx;y

P (x; y)
P (x; y)

P (x)P (y) = log 1 = 0: (8.30)

Solution to exercise 8.7 (p.141).

z = x + y mod 2:

(a) If q = 1/2, PZ = f1/2; 1/2g and I(Z; X) = H(Z) (cid:0) H(Z j X) = 1 (cid:0) 1 = 0.
(b) For general q and p, PZ = fpq+(1(cid:0)p)(1(cid:0)q); p(1(cid:0)q)+q(1(cid:0)p)g. The mutual
information is I(Z; X) = H(Z)(cid:0)H(Z j X) = H2(pq+(1(cid:0)p)(1(cid:0)q))(cid:0)H2(q).

Three term entropies

Solution to exercise 8.8 (p.141). The depiction of entropies in terms of Venn
diagrams is misleading for at least two reasons.

First, one is used to thinking of Venn diagrams as depicting sets; but what
are the ‘sets’ H(X) and H(Y ) depicted in (cid:12)gure 8.2, and what are the objects
that are members of those sets? I think this diagram encourages the novice
student to make inappropriate analogies. For example, some students imagine

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

8 | Dependent Random Variables

Figure 8.3. A misleading
representation of entropies,
continued.

144

I(X;Y)

H(X|Y,Z)

H(X)

     

     

     

     

     

     

     

     

     

     

     

     

     

     

     

     

     


H(Z|X)

H(Y)

H(Y|X,Z)

H(Z)

I(X;Y|Z)

A




























H(Z|Y)

H(X,Y|Z)

H(Z|X,Y)

that the random outcome (x; y) might correspond to a point in the diagram,
and thus confuse entropies with probabilities.

Secondly, the depiction in terms of Venn diagrams encourages one to be-
lieve that all the areas correspond to positive quantities. In the special case of
two random variables it is indeed true that H(X j Y ), I(X; Y ) and H(Y j X)
are positive quantities. But as soon as we progress to three-variable ensembles,
we obtain a diagram with positive-looking areas that may actually correspond
to negative quantities. Figure 8.3 correctly shows relationships such as

H(X) + H(Z j X) + H(Y j X; Z) = H(X; Y; Z):

(8.31)

But it gives the misleading impression that the conditional mutual information
I(X; Y j Z) is less than the mutual information I(X; Y ).
In fact the area
labelled A can correspond to a negative quantity. Consider the joint ensemble
(X; Y; Z) in which x 2 f0; 1g and y 2 f0; 1g are independent binary variables
and z 2 f0; 1g is de(cid:12)ned to be z = x + y mod 2. Then clearly H(X) =
H(Y ) = 1 bit. Also H(Z) = 1 bit. And H(Y j X) = H(Y ) = 1 since the two
variables are independent. So the mutual information between X and Y is
zero. I(X; Y ) = 0. However, if z is observed, X and Y become dependent |
knowing x, given z, tells you what y is: y = z (cid:0) x mod 2. So I(X; Y j Z) = 1
bit. Thus the area labelled A must correspond to (cid:0)1 bits for the (cid:12)gure to give
the correct answers.
The above example is not at all a capricious or exceptional illustration. The
binary symmetric channel with input X, noise Y , and output Z is a situation
in which I(X; Y ) = 0 (input and noise are independent) but I(X; Y j Z) > 0
(once you see the output, the unknown input and the unknown noise are
intimately related!).

The Venn diagram representation is therefore valid only if one is aware
that positive areas may represent negative quantities. With this proviso kept
in mind, the interpretation of entropies in terms of sets can be helpful (Yeung,
1991).

Solution to exercise 8.9 (p.141). For any joint ensemble XY Z, the following
chain rule for mutual information holds.

(8.32)
in the case w ! d ! r, w and r are independent given d, so

I(X; Y; Z) = I(X; Y ) + I(X; Z j Y ):

Now,
I(W ; R j D) = 0. Using the chain rule twice, we have:

and

so

I(W ; D; R) = I(W ; D)

I(W ; D; R) = I(W ; R) + I(W ; D j R);

I(W ; R) (cid:0) I(W ; D) (cid:20) 0:

(8.33)

(8.34)

(8.35)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 9

Before reading Chapter 9, you should have read Chapter 1 and worked on
exercise 2.26 (p.37), and exercises 8.2{8.7 (pp.140{141).

145

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9

Communication over a Noisy Channel

9.1 The big picture

Source
coding

Channel
coding

Source

?

6

Compressor

Decompressor

?

Encoder

-

Noisy
channel

6

Decoder

6

In Chapters 4{6, we discussed source coding with block codes, symbol codes
and stream codes. We implicitly assumed that the channel from the compres-
sor to the decompressor was noise-free. Real channels are noisy. We will now
spend two chapters on the subject of noisy-channel coding { the fundamen-
tal possibilities and limitations of error-free communication through a noisy
channel. The aim of channel coding is to make the noisy channel behave like
a noiseless channel. We will assume that the data to be transmitted has been
through a good compressor, so the bit stream has no obvious redundancy. The
channel code, which makes the transmission, will put back redundancy of a
special sort, designed to make the noisy received signal decodeable.

Suppose we transmit 1000 bits per second with p0 = p1 = 1/2 over a
noisy channel that (cid:13)ips bits with probability f = 0:1. What is the rate of
transmission of information? We might guess that the rate is 900 bits per
second by subtracting the expected number of errors per second. But this is
not correct, because the recipient does not know where the errors occurred.
Consider the case where the noise is so great that the received symbols are
independent of the transmitted symbols. This corresponds to a noise level of
f = 0:5, since half of the received symbols are correct due to chance alone.
But when f = 0:5, no information is transmitted at all.

Given what we have learnt about entropy, it seems reasonable that a mea-
sure of the information transmitted is given by the mutual information between
the source and the received signal, that is, the entropy of the source minus the
conditional entropy of the source given the received signal.

We will now review the de(cid:12)nition of conditional entropy and mutual in-
formation. Then we will examine whether it is possible to use such a noisy
channel to communicate reliably. We will show that for any channel Q there
is a non-zero rate, the capacity C(Q), up to which information can be sent

146

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.2: Review of probability and information

147

with arbitrarily small probability of error.

9.2 Review of probability and information

As an example, we take the joint distribution XY from exercise 8.6 (p.140).
The marginal distributions P (x) and P (y) are shown in the margins.

P (x; y)

x

P (y)

1
1/8
1/16
1/16
1/4
1/2

2
1/16
1/8
1/16
0
1/4

3
1/32
1/32
1/16
0
1/8

4
1/32
1/32
1/16
0
1/8

1/4
1/4
1/4
1/4

y

1
2
3
4

P (x)

The joint entropy is H(X; Y ) = 27=8 bits. The marginal entropies are H(X) =
7=4 bits and H(Y ) = 2 bits.

We can compute the conditional distribution of x for each value of y, and

the entropy of each of those conditional distributions:

P (xj y)

y

1
2
3
4

x

1
1/2
1/4
1/4
1

2
1/4
1/2
1/4
0

3
1/8
1/8
1/4
0

4
1/8
1/8
1/4
0

H(X j y)=bits

7/4
7/4
2
0

H(X j Y ) = 11/8

Note that whereas H(X j y = 4) = 0 is less than H(X), H(X j y = 3) is greater
than H(X). So in some cases, learning y can increase our uncertainty about
x. Note also that although P (xj y = 2) is a di(cid:11)erent distribution from P (x),
the conditional entropy H(X j y = 2) is equal to H(X). So learning that y
is 2 changes our knowledge about x but does not reduce the uncertainty of
x, as measured by the entropy. On average though, learning y does convey
information about x, since H(X j Y ) < H(X).
I(X; Y ) = H(X) (cid:0) H(X j Y ) = 3=8 bits.

One may also evaluate H(Y jX) = 13=8 bits. The mutual information is

9.3 Noisy channels

A discrete memoryless channel Q is characterized by an input alphabet
AX, an output alphabet AY , and a set of conditional probability distri-
butions P (y j x), one for each x 2 AX.
These transition probabilities may be written in a matrix

Qjji = P (y = bj j x = ai):

(9.1)

I usually orient this matrix with the output variable j indexing the rows
and the input variable i indexing the columns, so that each column of Q is
a probability vector. With this convention, we can obtain the probability
of the output, pY , from a probability distribution over the input, pX , by
right-multiplication:

pY = QpX :

(9.2)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

148

9 | Communication over a Noisy Channel

Some useful model channels are:
Binary symmetric channel. AX =f0; 1g. AY =f0; 1g.
P (y = 0j x = 0) = 1 (cid:0) f ;
P (y = 1j x = 0) = f ;
Binary erasure channel. AX =f0; 1g. AY =f0; ?; 1g.

-
(cid:0)(cid:0)(cid:18)@@R
x -

y

1

0

1

0

P (y = 0j x = 1) = f ;
P (y = 1j x = 1) = 1 (cid:0) f:

-
@@R
(cid:0)(cid:0)(cid:18)
-

x

0

1

0
? y
1

P (y = 0j x = 0) = 1 (cid:0) f ;
P (y = ?j x = 0) = f ;
P (y = 1j x = 0) = 0;

P (y = 0j x = 1) = 0;
P (y = ?j x = 1) = f ;
P (y = 1j x = 1) = 1 (cid:0) f:

0 1

0 1

0
1

0
?
1

Noisy typewriter. AX = AY = the 27 letters fA, B, . . . , Z, -g. The letters
are arranged in a circle, and when the typist attempts to type B, what
comes out is either A, B or C, with probability 1/3 each; when the input is
C, the output is B, C or D; and so forth, with the (cid:12)nal letter ‘-’ adjacent
to the (cid:12)rst letter A.
-PPPq
-(cid:16)(cid:16)(cid:16)1
C
(cid:3)(cid:23)
-(cid:16)(cid:16)(cid:16)1
PPPq
C
(cid:3)
PPPq
-(cid:16)(cid:16)(cid:16)1
C
(cid:3)
-(cid:16)(cid:16)(cid:16)1
PPPq
C
(cid:3)
-(cid:16)(cid:16)(cid:16)1
PPPq
(cid:3)
C
-(cid:16)(cid:16)(cid:16)1
PPPq
PPPq
(cid:16)(cid:16)(cid:16)1-(cid:16)(cid:16)(cid:16)1
PPPq
C
(cid:3)
...PPPq
C
(cid:3)
-(cid:16)(cid:16)(cid:16)1
-
(cid:3)
C
PPPq
(cid:16)(cid:16)(cid:16)1
(cid:3)
-
C
(cid:16)(cid:16)(cid:16)1
PPPq
-
(cid:3)
CW

P (y = Fj x = G) = 1=3;
P (y = Gj x = G) = 1=3;
P (y = Hj x = G) = 1=3;

A
B
C
D
E
F
G
H

A
B
C
D
E
F
G
H

...

...

C
(cid:3)

(cid:3)
C

Y
Z
-

Y
Z
-

A B C D E F G H I J K L M N O P Q R S T U V W X Y Z -

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z
-

Z channel. AX =f0; 1g. AY =f0; 1g.

0

-
(cid:0)(cid:0)(cid:18)
x -

1

y

0

1

P (y = 0j x = 0) = 1;
P (y = 1j x = 0) = 0;

P (y = 0j x = 1) = f ;
P (y = 1j x = 1) = 1 (cid:0) f:

0 1

0
1

9.4 Inferring the input given the output

If we assume that the input x to a channel comes from an ensemble X, then
we obtain a joint ensemble XY in which the random variables x and y have
the joint distribution:

P (x; y) = P (y j x)P (x):

(9.3)

Now if we receive a particular symbol y, what was the input symbol x? We
typically won’t know for certain. We can write down the posterior distribution
of the input using Bayes’ theorem:

P (xj y) =

P (y j x)P (x)

P (y)

=

P (y j x)P (x)
Px0 P (y j x0)P (x0)

Example 9.1. Consider a binary symmetric channel with probability of error
f = 0:15. Let the input ensemble be PX : fp0 = 0:9; p1 = 0:1g. Assume
we observe y = 1.

:

(9.4)

P (x = 1j y = 1) =

=

=

P (y = 1j x = 1)P (x = 1)
Px0 P (y j x0)P (x0)
0:85 (cid:2) 0:1
0:85 (cid:2) 0:1 + 0:15 (cid:2) 0:9
0:085
0:22

= 0:39:

(9.5)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.5: Information conveyed by a channel

149

Thus ‘x = 1’ is still less probable than ‘x = 0’, although it is not as im-
probable as it was before.

Exercise 9.2.[1, p.157] Now assume we observe y = 0. Compute the probability

of x = 1 given y = 0.

Example 9.3. Consider a Z channel with probability of error f = 0:15. Let the
input ensemble be PX : fp0 = 0:9; p1 = 0:1g. Assume we observe y = 1.

P (x = 1j y = 1) =
=

0:85 (cid:2) 0:1

0:85 (cid:2) 0:1 + 0 (cid:2) 0:9
0:085
0:085

= 1:0:

(9.6)

So given the output y = 1 we become certain of the input.

Exercise 9.4.[1, p.157] Alternatively, assume we observe y = 0.

Compute

P (x = 1j y = 0).

9.5 Information conveyed by a channel

We now consider how much information can be communicated through a chan-
nel. In operational terms, we are interested in (cid:12)nding ways of using the chan-
nel such that all the bits that are communicated are recovered with negligible
probability of error. In mathematical terms, assuming a particular input en-
semble X, we can measure how much information the output conveys about
the input by the mutual information:

I(X; Y ) (cid:17) H(X) (cid:0) H(X j Y ) = H(Y ) (cid:0) H(Y jX):

(9.7)

Our aim is to establish the connection between these two ideas. Let us evaluate
I(X; Y ) for some of the channels above.

Hint for computing mutual information

We will tend to think of I(X; Y ) as H(X) (cid:0) H(X j Y ), i.e., how much the
uncertainty of the input X is reduced when we look at the output Y . But for
computational purposes it is often handy to evaluate H(Y )(cid:0)H(Y jX) instead.

H(X; Y )

H(X)

H(Y )

H(X j Y )

I(X; Y )

H(Y jX)

Figure 9.1. The relationship
between joint information,
marginal entropy, conditional
entropy and mutual entropy.
This (cid:12)gure is important, so I’m
showing it twice.

Example 9.5. Consider the binary symmetric channel again, with f = 0:15 and
PX : fp0 = 0:9; p1 = 0:1g. We already evaluated the marginal probabil-
ities P (y) implicitly above: P (y = 0) = 0:78; P (y = 1) = 0:22. The
mutual information is:

I(X; Y ) = H(Y ) (cid:0) H(Y jX):

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

150

9 | Communication over a Noisy Channel

What is H(Y jX)? It is de(cid:12)ned to be the weighted sum over x of H(Y j x);
but H(Y j x) is the same for each value of x: H(Y j x = 0) is H2(0:15),
and H(Y j x = 1) is H2(0:15). So

I(X; Y ) = H(Y ) (cid:0) H(Y jX)

= H2(0:22) (cid:0) H2(0:15)
= 0:76 (cid:0) 0:61 = 0:15 bits:

(9.8)

Throughout this book, log means
log2.

This may be contrasted with the entropy of the source H(X) =
H2(0:1) = 0:47 bits.
Note: here we have used the binary entropy function H2(p) (cid:17) H(p; 1(cid:0)
p) = p log 1

1

p + (1 (cid:0) p) log

(1(cid:0)p) .

Example 9.6. And now the Z channel, with PX as above. P (y = 1) = 0:085.

I(X; Y ) = H(Y ) (cid:0) H(Y jX)

= H2(0:085) (cid:0) [0:9H2(0) + 0:1H2(0:15)]
= 0:42 (cid:0) (0:1 (cid:2) 0:61) = 0:36 bits:

(9.9)

The entropy of the source, as above, is H(X) = 0:47 bits. Notice that
the mutual information I(X; Y ) for the Z channel is bigger than the
mutual information for the binary symmetric channel with the same f .
The Z channel is a more reliable channel.

Exercise 9.7.[1, p.157] Compute the mutual information between X and Y for
the binary symmetric channel with f = 0:15 when the input distribution
is PX = fp0 = 0:5; p1 = 0:5g.

Exercise 9.8.[2, p.157] Compute the mutual information between X and Y for
the Z channel with f = 0:15 when the input distribution is PX :
fp0 = 0:5; p1 = 0:5g.

Maximizing the mutual information

We have observed in the above examples that the mutual information between
the input and the output depends on the chosen input ensemble.

Let us assume that we wish to maximize the mutual information conveyed
by the channel by choosing the best possible input ensemble. We de(cid:12)ne the
capacity of the channel to be its maximum mutual information.

The capacity of a channel Q is:

C(Q) = max
PX

I(X; Y ):

(9.10)

The distribution PX that achieves the maximum is called the optimal
input distribution, denoted by P(cid:3)X.
[There may be multiple optimal
input distributions achieving the same value of I(X; Y ).]

In Chapter 10 we will show that the capacity does indeed measure the maxi-
mum amount of error-free information that can be transmitted over the chan-
nel per unit time.

Example 9.9. Consider the binary symmetric channel with f = 0:15. Above,
we considered PX = fp0 = 0:9; p1 = 0:1g, and found I(X; Y ) = 0:15 bits.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.6: The noisy-channel coding theorem

151

How much better can we do? By symmetry, the optimal input distribu-
tion is f0:5; 0:5g and the capacity is

I(X; Y )

C(QBSC) = H2(0:5) (cid:0) H2(0:15) = 1:0 (cid:0) 0:61 = 0:39 bits:

(9.11)

We’ll justify the symmetry argument later. If there’s any doubt about
the symmetry argument, we can always resort to explicit maximization
of the mutual information I(X; Y ),

I(X; Y ) = H2((1(cid:0)f )p1 + (1(cid:0)p1)f ) (cid:0) H2(f )

((cid:12)gure 9.2).

(9.12)

Example 9.10. The noisy typewriter. The optimal input distribution is a uni-

form distribution over x, and gives C = log2 9 bits.

Example 9.11. Consider the Z channel with f = 0:15. Identifying the optimal
input distribution is not so straightforward. We evaluate I(X; Y ) explic-
itly for PX = fp0; p1g. First, we need to compute P (y). The probability
of y = 1 is easiest to write down:

0.4

0.3

0.2

0.1

0

0

0.25

0.5

0.75

1

p1

Figure 9.2. The mutual
information I(X; Y ) for a binary
symmetric channel with f = 0:15
as a function of the input
distribution.

P (y = 1) = p1(1 (cid:0) f ):

(9.13)

Then the mutual information is:

I(X; Y ) = H(Y ) (cid:0) H(Y jX)

= H2(p1(1 (cid:0) f )) (cid:0) (p0H2(0) + p1H2(f ))
= H2(p1(1 (cid:0) f )) (cid:0) p1H2(f ):

(9.14)

This is a non-trivial function of p1, shown in (cid:12)gure 9.3. It is maximized
for f = 0:15 by p(cid:3)1 = 0:445. We (cid:12)nd C(QZ) = 0:685. Notice the optimal
input distribution is not f0:5; 0:5g. We can communicate slightly more
information by using input symbol 0 more frequently than 1.

I(X; Y )

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.25

0.5

0.75

1

p1

Figure 9.3. The mutual
information I(X; Y ) for a Z
channel with f = 0:15 as a
function of the input distribution.

Exercise 9.12.[1, p.158] What is the capacity of the binary symmetric channel

for general f ?

Exercise 9.13.[2, p.158] Show that the capacity of the binary erasure channel
with f = 0:15 is CBEC = 0:85. What is its capacity for general f ?
Comment.

9.6 The noisy-channel coding theorem

It seems plausible that the ‘capacity’ we have de(cid:12)ned may be a measure of
information conveyed by a channel; what is not obvious, and what we will
prove in the next chapter, is that the capacity indeed measures the rate at
which blocks of data can be communicated over the channel with arbitrarily
small probability of error.

We make the following de(cid:12)nitions.

An (N; K) block code for a channel Q is a list of S = 2K codewords

fx(1); x(2); : : : ; x(2K )g;

x(s) 2 AN
X;

length N . Using this code we can encode a signal s 2
each of
f1; 2; 3; : : : ; 2Kg as x(s).
[The number of codewords S is an integer,
but the number of bits speci(cid:12)ed by choosing a codeword, K (cid:17) log 2 S, is
not necessarily an integer.]

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

152

9 | Communication over a Noisy Channel

The rate of the code is R = K=N bits per channel use.

[We will use this de(cid:12)nition of the rate for any channel, not only chan-
nels with binary inputs; note however that it is sometimes conventional
to de(cid:12)ne the rate of a code for a channel with q input symbols to be
K=(N log q).]

A decoder for an (N; K) block code is a mapping from the set of length-N
Y , to a codeword label ^s 2 f0; 1; 2; : : : ; 2Kg.

strings of channel outputs, AN
The extra symbol ^s = 0 can be used to indicate a ‘failure’.

The probability of block error of a code and decoder, for a given channel,
and for a given probability distribution over the encoded signal P (sin),
is:

pB =Xsin

P (sin)P (sout6= sin j sin):

The maximal probability of block error is

pBM = max
sin

P (sout6= sin j sin):

(9.15)

(9.16)

The optimal decoder for a channel code is the one that minimizes the prob-
ability of block error. It decodes an output y as the input s that has
maximum posterior probability P (sj y).

P (sj y) =

P (y j s)P (s)
Ps0 P (y j s0)P (s0)
^soptimal = argmax P (sj y):

(9.17)

(9.18)

A uniform prior distribution on s is usually assumed, in which case the
optimal decoder is also the maximum likelihood decoder, i.e., the decoder
that maps an output y to the input s that has maximum likelihood
P (y j s).

The probability of bit error pb is de(cid:12)ned assuming that the codeword
number s is represented by a binary vector s of length K bits; it is the
average probability that a bit of sout is not equal to the corresponding
bit of sin (averaging over all K bits).

Shannon’s noisy-channel coding theorem (part one). Associated with
each discrete memoryless channel, there is a non-negative number C
(called the channel capacity) with the following property. For any (cid:15) > 0
and R < C, for large enough N , there exists a block code of length N and
rate (cid:21) R and a decoding algorithm, such that the maximal probability
of block error is < (cid:15).

Con(cid:12)rmation of the theorem for the noisy typewriter channel

In the case of the noisy typewriter, we can easily con(cid:12)rm the theorem, because
we can create a completely error-free communication strategy using a block
code of length N = 1: we use only the letters B, E, H, . . . , Z, i.e., every third
letter. These letters form a non-confusable subset of the input alphabet (see
(cid:12)gure 9.5). Any output can be uniquely decoded. The number of inputs
in the non-confusable subset is 9, so the error-free information rate of this
system is log2 9 bits, which is equal to the capacity C, which we evaluated in
example 9.10 (p.151).

pBM

6

achievable

-
R

C

Figure 9.4. Portion of the R; pBM
plane asserted to be achievable by
the (cid:12)rst part of Shannon’s noisy
channel coding theorem.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.7: Intuitive preview of proof

153

-(cid:16)(cid:16)(cid:16)1
PPPq
-(cid:16)(cid:16)(cid:16)1
PPPq
-(cid:16)(cid:16)(cid:16)1
PPPq

(cid:16)(cid:16)(cid:16)1
-
PPPq

...

B

E

H

Z

A B C D E F G H I J K L M N O P Q R S T U V W X Y Z -

Figure 9.5. A non-confusable
subset of inputs for the noisy
typewriter.

A
B
C
D
E
F
G
H
I

Y
Z
-

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z
-

0
0
0
0

0
0
0
1

0
0
1
0

0
0
1
1

0
1
0
0

0
1
0
1

0
1
1
0

0
1
1
1

1
0
0
0

1
0
0
1

1
0
1
0

1
0
1
1

1
1
0
0

1
1
0
1

1
1
1
0

1
1
1
1

Figure 9.6. Extended channels
obtained from a binary symmetric
channel with transition
probability 0.15.

0000
1000
0100
1100
0010
1010
0110
1110
0001
1001
0101
1101
0011
1011
0111
1111

0
0

0
1

1
0

1
1

0 1

00
10
01
11

0
1

N = 1

N = 2

N = 4

How does this translate into the terms of the theorem? The following table

explains.

The theorem

Associated with each discrete
memoryless channel, there is a
non-negative number C.
For any (cid:15) > 0 and R < C, for large
enough N ,
there exists a block code of length N and
rate (cid:21) R

and a decoding algorithm,

How it applies to the noisy typewriter

The capacity C is log2 9.

No matter what (cid:15) and R are, we set the blocklength N to 1.

The block code is fB; E; : : : ; Zg. The value of K is given by
2K = 9, so K = log2 9, and this code has rate log2 9, which is
greater than the requested value of R.

The decoding algorithm maps the received letter to the nearest
letter in the code;

such that the maximal probability of
block error is < (cid:15).

the maximal probability of block error is zero, which is less
than the given (cid:15).

9.7 Intuitive preview of proof

Extended channels

To prove the theorem for any given channel, we consider the extended channel
corresponding to N uses of the channel. The extended channel has jAXjN
possible inputs x and jAY jN possible outputs. Extended channels obtained
from a binary symmetric channel and from a Z channel are shown in (cid:12)gures
9.6 and 9.7, with N = 2 and N = 4.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

154

9 | Communication over a Noisy Channel

Figure 9.7. Extended channels
obtained from a Z channel with
transition probability 0.15. Each
column corresponds to an input,
and each row is a di(cid:11)erent output.

Figure 9.8. (a) Some typical
outputs in AN
Y corresponding to
typical inputs x. (b) A subset of
the typical sets shown in (a) that
do not overlap each other. This
picture can be compared with the
solution to the noisy typewriter in
(cid:12)gure 9.5.

0
0
0
0

0
0
0
1

0
0
1
0

0
0
1
1

0
1
0
0

0
1
0
1

0
1
1
0

0
1
1
1

1
0
0
0

1
0
0
1

1
0
1
0

1
0
1
1

1
1
0
0

1
1
0
1

1
1
1
0

1
1
1
1

0000
1000
0100
1100
0010
1010
0110
1110
0001
1001
0101
1101
0011
1011
0111
1111

0
0

0
1

1
0

1
1

0 1

00
10
01
11

0
1

N = 1

N = 2

N = 4

AN

Typical y

Y ’
$
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20) (cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
&
%

Typical y for a given typical x

6

AN

$

Typical y

Y ’
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:23)(cid:20)(cid:22)(cid:21)
(cid:23)(cid:20)
(cid:23)(cid:20)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
(cid:22)(cid:21)
&

%

(a)

(b)

Exercise 9.14.[2, p.159] Find the transition probability matrices Q for the ex-
tended channel, with N = 2, derived from the binary erasure channel
having erasure probability 0.15.

By selecting two columns of this transition probability matrix, we can
de(cid:12)ne a rate-1/2 code for this channel with blocklength N = 2. What is
the best choice of two columns? What is the decoding algorithm?

To prove the noisy-channel coding theorem, we make use of large block-
lengths N . The intuitive idea is that, if N is large, an extended channel looks
a lot like the noisy typewriter. Any particular input x is very likely to produce
an output in a small subspace of the output alphabet { the typical output set,
given that input. So we can (cid:12)nd a non-confusable subset of the inputs that
produce essentially disjoint output sequences. For a given N , let us consider
a way of generating such a non-confusable subset of the inputs, and count up
how many distinct inputs it contains.

Imagine making an input sequence x for the extended channel by drawing
it from an ensemble X N , where X is an arbitrary ensemble over the input
alphabet. Recall the source coding theorem of Chapter 4, and consider the
number of probable output sequences y. The total number of typical output
sequences y is 2N H(Y ), all having similar probability. For any particular typical
input sequence x, there are about 2N H(Y jX) probable sequences. Some of these
subsets of AN
We now imagine restricting ourselves to a subset of the typical inputs
x such that the corresponding typical output sets do not overlap, as shown
in (cid:12)gure 9.8b. We can then bound the number of non-confusable inputs by
dividing the size of the typical y set, 2N H(Y ), by the size of each typical-y-

Y are depicted by circles in (cid:12)gure 9.8a.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.8: Further exercises

155

given-typical-x set, 2N H(Y jX). So the number of non-confusable inputs, if they
are selected from the set of typical inputs x (cid:24) X N , is (cid:20) 2N H(Y )(cid:0)N H(Y jX) =
2N I(X;Y ).
The maximum value of this bound is achieved if X is the ensemble that
maximizes I(X; Y ), in which case the number of non-confusable inputs is
(cid:20) 2N C . Thus asymptotically up to C bits per cycle, and no more, can be
communicated with vanishing error probability.
2
This sketch has not rigorously proved that reliable communication really

is possible { that’s our task for the next chapter.

9.8 Further exercises

Exercise 9.15.[3, p.159] Refer back to the computation of the capacity of the Z

channel with f = 0:15.

(a) Why is p(cid:3)1 less than 0.5? One could argue that it is good to favour
the 0 input, since it is transmitted without error { and also argue
that it is good to favour the 1 input, since it often gives rise to the
highly prized 1 output, which allows certain identi(cid:12)cation of the
input! Try to make a convincing argument.

(b) In the case of general f , show that the optimal input distribution

is

p(cid:3)1 =

1=(1 (cid:0) f )

1 + 2(H2(f )=(1(cid:0)f )) :

(9.19)

(c) What happens to p(cid:3)1 if the noise level f is very close to 1?

Exercise 9.16.[2, p.159] Sketch graphs of the capacity of the Z channel, the
binary symmetric channel and the binary erasure channel as a function
of f .

. Exercise 9.17.[2 ] What is the capacity of the (cid:12)ve-input, ten-output channel

whose transition probability matrix is

0
0

0:25
0:25
0:25 0:25
0:25 0:25

0
0
0
0

0:25 0:25
0:25 0:25

0
0
0
0
0
0

0
0
0
0

0:25 0:25
0:25 0:25

0
0

0:25 0:25
0:25 0:25

2

666666666666664

0
0
0
0
0
0

0:25
0:25

0
0
0
0
0
0

0 1 2 3 4

0
1
2
3
4
5
6
7
8
9

?

(9.20)

3

777777777777775

Exercise 9.18.[2, p.159] Consider a Gaussian channel with binary input x 2
f(cid:0)1; +1g and real output alphabet AY , with transition probability den-
sity

Q(y j x; (cid:11); (cid:27)) =
where (cid:11) is the signal amplitude.

1

p2(cid:25)(cid:27)2

e(cid:0)

(y(cid:0)x(cid:11))2

2(cid:27)2

;

(9.21)

(a) Compute the posterior probability of x given y, assuming that the

two inputs are equiprobable. Put your answer in the form

P (x = 1j y; (cid:11); (cid:27)) =

1

1 + e(cid:0)a(y)

:

(9.22)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

156

9 | Communication over a Noisy Channel

Sketch the value of P (x = 1j y; (cid:11); (cid:27)) as a function of y.

(b) Assume that a single bit is to be transmitted. What is the optimal
decoder, and what is its probability of error? Express your answer
in terms of the signal-to-noise ratio (cid:11)2=(cid:27)2 and the error function
(the cumulative probability function of the Gaussian distribution),

(cid:8)(z) (cid:17)Z z

(cid:0)1

1
p2(cid:25)

z2
2 dz:

e(cid:0)

(9.23)

[Note that this de(cid:12)nition of the error function (cid:8)(z) may not corre-
spond to other people’s.]

Pattern recognition as a noisy channel

We may think of many pattern recognition problems in terms of communi-
cation channels. Consider the case of recognizing handwritten digits (such
as postcodes on envelopes). The author of the digit wishes to communicate
a message from the set AX = f0; 1; 2; 3; : : : ; 9g; this selected message is the
input to the channel. What comes out of the channel is a pattern of ink on
paper. If the ink pattern is represented using 256 binary pixels, the channel
Q has as its output a random variable y 2 AY = f0; 1g256. An example of an
element from this alphabet is shown in the margin.

Exercise 9.19.[2 ] Estimate how many patterns in AY are recognizable as the

[The aim of this problem is to try to demonstrate the

character ‘2’.
existence of as many patterns as possible that are recognizable as 2s.]
Discuss how one might model the channel P (y j x = 2). Estimate the
entropy of the probability distribution P (y j x = 2).
One strategy for doing pattern recognition is to create a model for
P (y j x) for each value of the input x = f0; 1; 2; 3; : : : ; 9g, then use Bayes’
theorem to infer x given y.

P (xj y) =

P (y j x)P (x)
Px0 P (y j x0)P (x0)

:

(9.24)

This strategy is known as full probabilistic modelling or generative
modelling. This is essentially how current speech recognition systems
work. In addition to the channel model, P (y j x), one uses a prior proba-
bility distribution P (x), which in the case of both character recognition
and speech recognition is a language model that speci(cid:12)es the probability
of the next character/word given the context and the known grammar
and statistics of the language.

Random coding

Exercise 9.20.[2, p.160] Given twenty-four people in a room, what is the prob-
ability that there are at least two people present who have the same
birthday (i.e., day and month of birth)? What is the expected number
of pairs of people with the same birthday? Which of these two questions
is easiest to solve? Which answer gives most insight? You may (cid:12)nd it
helpful to solve these problems and those that follow using notation such
as A = number of days in year = 365 and S = number of people = 24.

. Exercise 9.21.[2 ] The birthday problem may be related to a coding scheme.
Assume we wish to convey a message to an outsider identifying one of

Figure 9.9. Some more 2s.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.9: Solutions

157

the twenty-four people. We could simply communicate a number s from
AS = f1; 2; : : : ; 24g, having agreed a mapping of people onto numbers;
alternatively, we could convey a number from AX = f1; 2; : : : ; 365g,
identifying the day of the year that is the selected person’s birthday
(with apologies to leapyearians).
[The receiver is assumed to know all
the people’s birthdays.] What, roughly, is the probability of error of this
communication scheme, assuming it is used for a single transmission?
What is the capacity of the communication channel, and what is the
rate of communication attempted by this scheme?

. Exercise 9.22.[2 ] Now imagine that there are K rooms in a building, each
containing q people.
(You might think of K = 2 and q = 24 as an
example.) The aim is to communicate a selection of one person from each
room by transmitting an ordered list of K days (from AX). Compare
the probability of error of the following two schemes.

(a) As before, where each room transmits the birthday of the selected

person.

(b) To each K-tuple of people, one drawn from each room, an ordered
K-tuple of randomly selected days from AX is assigned (this K-
tuple has nothing to do with their birthdays). This enormous list
of S = qK strings is known to the receiver. When the building has
selected a particular person from each room, the ordered string of
days corresponding to that K-tuple of people is transmitted.

What is the probability of error when q = 364 and K = 1? What is the
probability of error when q = 364 and K is large, e.g. K = 6000?

9.9 Solutions

Solution to exercise 9.2 (p.149).

If we assume we observe y = 0,

P (x = 1j y = 0) =

=

=

P (y = 0j x = 1)P (x = 1)
Px0 P (y j x0)P (x0)
0:15 (cid:2) 0:1
0:15 (cid:2) 0:1 + 0:85 (cid:2) 0:9
0:015
0:78

= 0:019:

Solution to exercise 9.4 (p.149).

If we observe y = 0,

P (x = 1j y = 0) =
=

0:15 (cid:2) 0:1

0:15 (cid:2) 0:1 + 1:0 (cid:2) 0:9
0:015
0:915

= 0:016:

(9.25)

(9.26)

(9.27)

(9.28)

(9.29)

Solution to exercise 9.7 (p.150). The probability that y = 1 is 0:5, so the
mutual information is:

I(X; Y ) = H(Y ) (cid:0) H(Y j X)
= H2(0:5) (cid:0) H2(0:15)
= 1 (cid:0) 0:61 = 0:39 bits:

(9.30)

(9.31)

(9.32)

Solution to exercise 9.8 (p.150). We again compute the mutual information
using I(X; Y ) = H(Y ) (cid:0) H(Y j X). The probability that y = 0 is 0:575, and

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

158

9 | Communication over a Noisy Channel

H(Y j X) =Px P (x)H(Y j x) = P (x = 1)H(Y j x = 1) + P (x = 0)H(Y j x = 0)

so the mutual information is:

I(X; Y ) = H(Y ) (cid:0) H(Y j X)

= H2(0:575) (cid:0) [0:5 (cid:2) H2(0:15) + 0:5 (cid:2) 0]
= 0:98 (cid:0) 0:30 = 0:679 bits:

(9.33)

(9.34)

(9.35)

Solution to exercise 9.12 (p.151). By symmetry, the optimal input distribution
is f0:5; 0:5g. Then the capacity is

C = I(X; Y ) = H(Y ) (cid:0) H(Y j X)
= H2(0:5) (cid:0) H2(f )
= 1 (cid:0) H2(f ):

(9.36)

(9.37)

(9.38)

Would you like to (cid:12)nd the optimal input distribution without invoking sym-
metry? We can do this by computing the mutual information in the general
case where the input ensemble is fp0; p1g:

I(X; Y ) = H(Y ) (cid:0) H(Y j X)

= H2(p0f + p1(1 (cid:0) f )) (cid:0) H2(f ):

(9.39)

(9.40)

The only p-dependence is in the (cid:12)rst term H2(p0f + p1(1 (cid:0) f )), which is
maximized by setting the argument to 0.5. This value is given by setting
p0 = 1=2.

Solution to exercise 9.13 (p.151). Answer 1. By symmetry, the optimal input
distribution is f0:5; 0:5g. The capacity is most easily evaluated by writing the
mutual information as I(X; Y ) = H(X) (cid:0) H(X j Y ). The conditional entropy
H(X j Y ) is Py P (y)H(X j y); when y is known, x is uncertain only if y = ?,
which occurs with probability f =2 + f =2, so the conditional entropy H(X j Y )
is f H2(0:5).

C = I(X; Y ) = H(X) (cid:0) H(X j Y )
= H2(0:5) (cid:0) f H2(0:5)
= 1 (cid:0) f:

(9.41)

(9.42)

(9.43)

The binary erasure channel fails a fraction f of the time.
Its capacity is
precisely 1 (cid:0) f , which is the fraction of the time that the channel is reliable.
This result seems very reasonable, but it is far from obvious how to encode
information so as to communicate reliably over this channel.

Answer 2. Alternatively, without invoking the symmetry assumed above, we
can start from the input ensemble fp0; p1g. The probability that y = ? is
p0f + p1f = f , and when we receive y = ?, the posterior probability of x is
the same as the prior probability, so:

I(X; Y ) = H(X) (cid:0) H(X j Y )
= H2(p1) (cid:0) f H2(p1)
= (1 (cid:0) f )H2(p1):

(9.44)

(9.45)

(9.46)

This mutual information achieves its maximum value of (1(cid:0)f ) when p1 = 1=2.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

9.9: Solutions

0
0

0
1

1
0

1
1

x(1) x(2)

0
0

0
1

1
0

1
1

x(1) x(2)

0
0

0
1

1
0

1
1

00
?0
10
0?
??
1?
01
?1
11

(a)

0 1

0
?
Q 1

N = 1

N = 2

00
?0
10
0?
??
1?
01
?1
11

(b)

00
?0
10
0?
??
1?
01
?1
11

(c)

-
-
-
-
-
-
-

^m = 1
^m = 1

^m = 1
^m = 0
^m = 2

^m = 2
^m = 2

The extended channel

Solution to exercise 9.14 (p.153).
is shown in (cid:12)g-
ure 9.10. The best code for this channel with N = 2 is obtained by choosing
two columns that have minimal overlap, for example, columns 00 and 11. The
decoding algorithm returns ‘00’ if the extended channel output is among the
top four and ‘11’ if it’s among the bottom four, and gives up if the output is
‘??’.

Solution to exercise 9.15 (p.155).
mutual information between input and output of the Z channel is

In example 9.11 (p.151) we showed that the

I(X; Y ) = H(Y ) (cid:0) H(Y j X)

= H2(p1(1 (cid:0) f )) (cid:0) p1H2(f ):

(9.47)

We di(cid:11)erentiate this expression with respect to p1, taking care not to confuse
log2 with loge:

d
dp1

I(X; Y ) = (1 (cid:0) f ) log2

1 (cid:0) p1(1 (cid:0) f )
p1(1 (cid:0) f ) (cid:0) H2(f ):

(9.48)

Setting this derivative to zero and rearranging using skills developed in exer-
cise 2.17 (p.36), we obtain:

p(cid:3)1(1 (cid:0) f ) =

1

1 + 2H2(f )=(1(cid:0)f ) ;

so the optimal input distribution is

p(cid:3)1 =

1=(1 (cid:0) f )

1 + 2(H2(f )=(1(cid:0)f ))

:

(9.49)

(9.50)

As the noise level f tends to 1, this expression tends to 1=e (as you can prove
using L’H^opital’s rule).

For all values of f, p(cid:3)1 is smaller than 1=2. A rough intuition for why input
1 is used less than input 0 is that when input 1 is used, the noisy channel
injects entropy into the received string; whereas when input 0 is used, the
noise has zero entropy.

Solution to exercise 9.16 (p.155).
The capacities of the three channels are
shown in (cid:12)gure 9.11. For any f < 0:5, the BEC is the channel with highest
capacity and the BSC the lowest.

Solution to exercise 9.18 (p.155). The logarithm of the posterior probability
ratio, given y, is

a(y) = ln

P (x = 1j y; (cid:11); (cid:27))
P (x = (cid:0) 1j y; (cid:11); (cid:27))

= ln

Q(y j x = 1; (cid:11); (cid:27))
Q(y j x = (cid:0) 1; (cid:11); (cid:27))

= 2

(cid:11)y
(cid:27)2 :

(9.51)

159

Figure 9.10. (a) The extended
channel (N = 2) obtained from a
binary erasure channel with
erasure probability 0.15. (b) A
block code consisting of the two
codewords 00 and 11. (c) The
optimal decoder for this code.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Z
BSC
BEC

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 9.11. Capacities of the Z
channel, binary symmetric
channel, and binary erasure
channel.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

160

9 | Communication over a Noisy Channel

Using our skills picked up from exercise 2.17 (p.36), we rewrite this in the
form

P (x = 1j y; (cid:11); (cid:27)) =

1 + e(cid:0)a(y)

1

:

(9.52)

The optimal decoder selects the most probable hypothesis; this can be done
simply by looking at the sign of a(y). If a(y) > 0 then decode as ^x = 1.

The probability of error is

pb =Z 0

(cid:0)1

dy Q(y j x = 1; (cid:11); (cid:27)) =Z (cid:0)x(cid:11)

(cid:0)1

dy

1

p2(cid:25)(cid:27)2

e(cid:0)

y2

2(cid:27)2 = (cid:8)(cid:16)(cid:0)

x(cid:11)

(cid:27) (cid:17) :

(9.53)

Random coding

Solution to exercise 9.20 (p.156). The probability that S = 24 people whose
birthdays are drawn at random from A = 365 days all have distinct birthdays
is

A(A (cid:0) 1)(A (cid:0) 2) : : : (A (cid:0) S + 1)

:

(9.54)

AS

The probability that two (or more) people share a birthday is one minus this
quantity, which, for S = 24 and A = 365, is about 0.5. This exact way of
answering the question is not very informative since it is not clear for what
value of S the probability changes from being close to 0 to being close to 1.
The number of pairs is S(S (cid:0) 1)=2, and the probability that a particular

pair shares a birthday is 1=A, so the expected number of collisions is

S(S (cid:0) 1)

2

1
A

:

(9.55)

This answer is more instructive. The expected number of collisions is tiny if

S (cid:28) pA and big if S (cid:29) pA.

We can also approximate the probability that all birthdays are distinct,

for small S, thus:

A(A (cid:0) 1)(A (cid:0) 2) : : : (A (cid:0) S + 1)

AS

’ exp(0) exp((cid:0)1=A) exp((cid:0)2=A) : : : exp((cid:0)(S(cid:0)1)=A)
’ exp (cid:0)

= (1)(1 (cid:0) 1/A)(1 (cid:0) 2/A) : : : (1 (cid:0) (S(cid:0)1)/A)
(9.56)
i! = exp(cid:18)(cid:0)

S(S (cid:0) 1)=2

(cid:19) :

(9.57)

A

1
A

S(cid:0)1

Xi=1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 10

Before reading Chapter 10, you should have read Chapters 4 and 9. Exer-
cise 9.14 (p.153) is especially recommended.

Cast of characters

Q
C
X N
C
N
x(s)
s

the noisy channel
the capacity of the channel
an ensemble used to create a random code
a random code
the length of the codewords
a codeword, the sth in the code
the number of a chosen codeword (mnemonic:
selects s)
the total number of codewords in the code

the source

S = 2K
K = log2 S the number of bits conveyed by the choice of one codeword

s
R = K=N

^s

from S, assuming it is chosen with uniform probability
a binary representation of the number s
the rate of the code, in bits per channel use (sometimes called
R0 instead)
the decoder’s guess of s

161

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

R(pb)

3

-
R

(cid:1)

2
(cid:1)
(cid:1)

(cid:1)
C

Figure 10.1. Portion of the R; pb
plane to be proved achievable
(1, 2) and not achievable (3).

10

The Noisy-Channel Coding Theorem

10.1 The theorem

The theorem has three parts, two positive and one negative. The main positive
result is the (cid:12)rst.

1. For every discrete memoryless channel, the channel capacity

6

pb

C = max
PX

I(X; Y )

(10.1)

1

has the following property. For any (cid:15) > 0 and R < C, for large enough N ,
there exists a code of length N and rate (cid:21) R and a decoding algorithm,
such that the maximal probability of block error is < (cid:15).

2. If a probability of bit error pb is acceptable, rates up to R(pb) are achiev-

able, where

R(pb) =

C

1 (cid:0) H2(pb)

:

(10.2)

3. For any pb, rates greater than R(pb) are not achievable.

10.2 Jointly-typical sequences

We formalize the intuitive preview of the last chapter.

We will de(cid:12)ne codewords x(s) as coming from an ensemble X N , and con-
sider the random selection of one codeword and a corresponding channel out-
put y, thus de(cid:12)ning a joint ensemble (XY )N . We will use a typical-set decoder,
which decodes a received signal y as s if x(s) and y are jointly typical, a term
to be de(cid:12)ned shortly.

The proof will then centre on determining the probabilities (a) that the
true input codeword is not jointly typical with the output sequence; and (b)
that a false input codeword is jointly typical with the output. We will show
that, for large N , both probabilities go to zero as long as there are fewer than
2N C codewords, and the ensemble X is the optimal input distribution.

Joint typicality. A pair of sequences x; y of length N are de(cid:12)ned to be
jointly typical (to tolerance (cid:12)) with respect to the distribution P (x; y)
if

x is typical of P (x),

i.e.,

y is typical of P (y),

i.e.,

and x; y is typical of P (x; y),

i.e.,

162

log

log

log

1
N
1
N
1
N

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

1

1

1

< (cid:12);

P (x) (cid:0) H(X)(cid:12)(cid:12)(cid:12)(cid:12)
P (y) (cid:0) H(Y )(cid:12)(cid:12)(cid:12)(cid:12)
P (x; y) (cid:0) H(X; Y )(cid:12)(cid:12)(cid:12)(cid:12)

< (cid:12);

< (cid:12):

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.2: Jointly-typical sequences

163

The jointly-typical set JN (cid:12) is the set of all jointly-typical sequence pairs

of length N .

Example. Here is a jointly-typical pair of length N = 100 for the ensemble
P (x; y) in which P (x) has (p0; p1) = (0:9; 0:1) and P (y j x) corresponds to a
binary symmetric channel with noise level 0:2.

x 1111111111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
y 0011111111000000000000000000000000000000000000000000000000000000000000000000000000111111111111111111

Notice that x has 10 1s, and so is typical of the probability P (x) (at any
tolerance (cid:12)); and y has 26 1s, so it is typical of P (y) (because P (y = 1) = 0:26);
and x and y di(cid:11)er in 20 bits, which is the typical number of (cid:13)ips for this
channel.

Joint typicality theorem. Let x; y be drawn from the ensemble (XY )N

de(cid:12)ned by

Then

P (x; y) =

P (xn; yn):

N

Yn=1

1. the probability that x; y are jointly typical (to tolerance (cid:12)) tends

to 1 as N ! 1;

To be precise,

2. the number of jointly-typical sequences jJN (cid:12)j is close to 2N H(X;Y ).
(10.3)
3. if x0 (cid:24) X N and y0 (cid:24) Y N , i.e., x0 and y0 are independent samples
with the same marginal distribution as P (x; y), then the probability
that (x0; y0) lands in the jointly-typical set is about 2(cid:0)N I(X;Y ). To
be precise,

jJN (cid:12)j (cid:20) 2N (H(X;Y )+(cid:12));

P ((x0; y0) 2 JN (cid:12)) (cid:20) 2(cid:0)N (I(X;Y )(cid:0)3(cid:12)):

(10.4)

Proof. The proof of parts 1 and 2 by the law of large numbers follows that
of the source coding theorem in Chapter 4. For part 2, let the pair x; y
play the role of x in the source coding theorem, replacing P (x) there by
the probability distribution P (x; y).

For the third part,

P ((x0; y0) 2 JN (cid:12)) = X(x;y)2JN (cid:12)

P (x)P (y)

(cid:20) jJN (cid:12)j 2(cid:0)N (H(X)(cid:0)(cid:12)) 2(cid:0)N (H(Y )(cid:0)(cid:12))
(cid:20) 2N (H(X;Y )+(cid:12))(cid:0)N (H(X)+H(Y )(cid:0)2(cid:12))
= 2(cid:0)N (I(X;Y )(cid:0)3(cid:12)):

(10.5)

(10.6)

(10.7)

2 (10.8)

A cartoon of the jointly-typical set is shown in (cid:12)gure 10.2. Two independent
typical vectors are jointly typical with probability

P ((x0; y0) 2 JN (cid:12)) ’ 2(cid:0)N (I(X;Y ))

(10.9)

because the total number of independent typical pairs is the area of the dashed
rectangle, 2N H(X)2N H(Y ), and the number of jointly-typical pairs is roughly
2N H(X;Y ), so the probability of hitting a jointly-typical pair is roughly

2N H(X;Y )=2N H(X)+N H(Y ) = 2(cid:0)N I(X;Y ):

(10.10)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

164

10 | The Noisy-Channel Coding Theorem

-

Y , the set of

X , the set of all input

Figure 10.2. The jointly-typical
set. The horizontal direction
represents AN
strings of length N . The vertical
direction represents AN
all output strings of length N .
The outer box contains all
conceivable input{output pairs.
Each dot represents a
jointly-typical pair of sequences
(x; y). The total number of
jointly-typical sequences is about
2N H(X;Y ).

AN

X

(cid:27)
6

-

2N H(X)

2N H(X;Y ) dots

qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq

6
2N H(Y jX)
?

2N H(XjY )

2N H(Y )

6

?

-(cid:27)

-(cid:27)

AN

Y

(cid:27)
6

?

?

10.3 Proof of the noisy-channel coding theorem

Analogy

Imagine that we wish to prove that there is a baby in a class of one hundred
babies who weighs less than 10 kg. Individual babies are di(cid:14)cult to catch and
weigh. Shannon’s method of solving the task is to scoop up all the babies
and weigh them all at once on a big weighing machine. If we (cid:12)nd that their
average weight is smaller than 10 kg, there must exist at least one baby who
weighs less than 10 kg { indeed there must be many! Shannon’s method isn’t
guaranteed to reveal the existence of an underweight child, since it relies on
there being a tiny number of elephants in the class. But if we use his method
and get a total weight smaller than 1000 kg then our task is solved.

From skinny children to fantastic codes

We wish to show that there exists a code and a decoder having small prob-
ability of error. Evaluating the probability of error of any particular coding
and decoding system is not easy. Shannon’s innovation was this:
instead of
constructing a good coding and decoding system and evaluating its error prob-
ability, Shannon calculated the average probability of block error of all codes,
and proved that this average is small. There must then exist individual codes
that have small probability of block error.

Random coding and typical-set decoding

Consider the following encoding{decoding system, whose rate is R0.

1. We (cid:12)x P (x) and generate the S = 2N R0

codewords of a (N; N R0) =

Figure 10.3. Shannon’s method for
proving one baby weighs less than
10 kg.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.3: Proof of the noisy-channel coding theorem

165

x(2) x(4)

x(3) x(1)

qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq

ya

yb

yd

yc

x(2) x(4)

x(3) x(1)

qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq
qqqqqqqqqqqqqqqqqqqqq

(a)

(b)

(N; K) code C at random according to
Yn=1

P (x) =

N

P (xn):

Figure 10.4. (a) A random code.
(b) Example decodings by the
typical set decoder. A sequence
that is not jointly typical with any
of the codewords, such as ya, is
decoded as ^s = 0. A sequence that
is jointly typical with codeword
x(3) alone, yb, is decoded as ^s = 3.
Similarly, yc is decoded as ^s = 4.
A sequence that is jointly typical
with more than one codeword,
such as yd, is decoded as ^s = 0.

-

-

^s(ya) = 0

^s(yb) = 3

-

-

^s(yd) = 0

^s(yc) = 4

(10.11)

A random code is shown schematically in (cid:12)gure 10.4a.

2. The code is known to both sender and receiver.
3. A message s is chosen from f1; 2; : : : ; 2N R0g, and x(s) is transmitted. The

received signal is y, with

P (y j x(s)) =

N

Yn=1

P (yn j x(s)
n ):

(10.12)

4. The signal is decoded by typical-set decoding.

Typical-set decoding. Decode y as ^s if (x(^s); y) are jointly typical and

there is no other s0 such that (x(s0); y) are jointly typical;
otherwise declare a failure (^s = 0).

This is not the optimal decoding algorithm, but it will be good enough,
and easier to analyze. The typical-set decoder is illustrated in (cid:12)g-
ure 10.4b.

5. A decoding error occurs if ^s 6= s.
There are three probabilities of error that we can distinguish. First, there

is the probability of block error for a particular code C, that is,

pB(C) (cid:17) P (^s 6= sjC):

(10.13)

This is a di(cid:14)cult quantity to evaluate for any given code.

Second, there is the average over all codes of this block error probability,

hpBi (cid:17)XC

P (^s 6= sjC)P (C):

(10.14)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

166

10 | The Noisy-Channel Coding Theorem

hpBi is just the probability that
there is a decoding error at step 5
of the (cid:12)ve-step process on the
previous page.

Fortunately, this quantity is much easier to evaluate than the (cid:12)rst quantity
P (^s 6= sjC).

Third, the maximal block error probability of a code C,

pBM(C) (cid:17) max

s

P (^s 6= sj s;C);

(10.15)

is the quantity we are most interested in: we wish to show that there exists a
code C with the required rate whose maximal block error probability is small.
We will get to this result by (cid:12)rst (cid:12)nding the average block error probability,
hpBi. Once we have shown that this can be made smaller than a desired small
number, we immediately deduce that there must exist at least one code C
whose block error probability is also less than this small number. Finally,
we show that this code, whose block error probability is satisfactorily small
but whose maximal block error probability is unknown (and could conceivably
be enormous), can be modi(cid:12)ed to make a code of slightly smaller rate whose
maximal block error probability is also guaranteed to be small. We modify
the code by throwing away the worst 50% of its codewords.

We therefore now embark on (cid:12)nding the average probability of block error.

Probability of error of typical-set decoder

There are two sources of error when we use typical-set decoding. Either (a)
the output y is not jointly typical with the transmitted codeword x(s), or (b)
there is some other codeword in C that is jointly typical with y.
By the symmetry of the code construction, the average probability of error
averaged over all codes does not depend on the selected value of s; we can
assume without loss of generality that s = 1.

(a) The probability that the input x(1) and the output y are not jointly
typical vanishes, by the joint typicality theorem’s (cid:12)rst part (p.163). We give a
name, (cid:14), to the upper bound on this probability, satisfying (cid:14) ! 0 as N ! 1;
for any desired (cid:14), we can (cid:12)nd a blocklength N ((cid:14)) such that the P ((x(1); y) 62
JN (cid:12)) (cid:20) (cid:14).
(b) The probability that x(s0) and y are jointly typical, for a given s0 6= 1
is (cid:20) 2(cid:0)N (I(X;Y )(cid:0)3(cid:12)), by part 3. And there are (2N R0 (cid:0) 1) rival values of s0 to
worry about.

Thus the average probability of error hpBi satis(cid:12)es:

hpBi (cid:20) (cid:14) +

2(cid:0)N (I(X;Y )(cid:0)3(cid:12))

2N R0

Xs0=2

(cid:20) (cid:14) + 2(cid:0)N (I(X;Y )(cid:0)R0(cid:0)3(cid:12)):

(10.16)

(10.17)

The inequality (10.16) that bounds a total probability of error PTOT by the
sum of the probabilities Ps0 of all sorts of events s0 each of which is su(cid:14)cient
to cause error,

is called a union bound. It is only an equality if the di(cid:11)erent events that cause
error never occur at the same time as each other.

PTOT (cid:20) P1 + P2 + (cid:1)(cid:1)(cid:1) ;

The average probability of error (10.17) can be made < 2(cid:14) by increasing N if

R0 < I(X; Y ) (cid:0) 3(cid:12):

(10.18)

We are almost there. We make three modi(cid:12)cations:

1. We choose P (x) in the proof to be the optimal input distribution of the
channel. Then the condition R0 < I(X; Y ) (cid:0) 3(cid:12) becomes R0 < C (cid:0) 3(cid:12).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.4: Communication (with errors) above capacity

167

Figure 10.5. How expurgation
works. (a) In a typical random
code, a small fraction of the
codewords are involved in
collisions { pairs of codewords are
su(cid:14)ciently close to each other
that the probability of error when
either codeword is transmitted is
not tiny. We obtain a new code
from a random code by deleting
all these confusable codewords.
(b) The resulting code has slightly
fewer codewords, so has a slightly
lower rate, and its maximal
probability of error is greatly
reduced.

6

pb

achievable

-
R

C

Figure 10.6. Portion of the R; pb
plane proved achievable in the
(cid:12)rst part of the theorem. [We’ve
proved that the maximal
probability of block error pBM can
be made arbitrarily small, so the
same goes for the bit error
probability pb, which must be
smaller than pBM.]

(a) A random code : : :

)

(b) expurgated

2. Since the average probability of error over all codes is < 2(cid:14), there must

exist a code with mean probability of block error pB(C) < 2(cid:14).

3. To show that not only the average but also the maximal probability of
error, pBM, can be made small, we modify this code by throwing away
the worst half of the codewords { the ones most likely to produce errors.
Those that remain must all have conditional probability of error less
than 4(cid:14). We use these remaining codewords to de(cid:12)ne a new code. This
new code has 2N R0(cid:0)1 codewords, i.e., we have reduced the rate from R0
to R0(cid:0) 1/N (a negligible reduction, if N is large), and achieved pBM < 4(cid:14).
This trick is called expurgation ((cid:12)gure 10.5). The resulting code may
not be the best code of its rate and length, but it is still good enough to
prove the noisy-channel coding theorem, which is what we are trying to
do here.

In conclusion, we can ‘construct’ a code of rate R0 (cid:0) 1/N, where R0 < C (cid:0) 3(cid:12),
with maximal probability of error < 4(cid:14). We obtain the theorem as stated by
setting R0 = (R + C)=2, (cid:14) = (cid:15)=4, (cid:12) < (C (cid:0) R0)=3, and N su(cid:14)ciently large for
the remaining conditions to hold. The theorem’s (cid:12)rst part is thus proved. 2

10.4 Communication (with errors) above capacity

We have proved, for any discrete memoryless channel, the achievability of a
portion of the R; pb plane shown in (cid:12)gure 10.6. We have shown that we can
turn any noisy channel into an essentially noiseless binary channel with rate
up to C bits per cycle. We now extend the right-hand boundary of the region
of achievability at non-zero error probabilities.
[This is called rate-distortion
theory.]

We do this with a new trick. Since we know we can make the noisy channel
into a perfect channel with a smaller rate, it is su(cid:14)cient to consider commu-
nication with errors over a noiseless channel. How fast can we communicate
over a noiseless channel, if we are allowed to make errors?

Consider a noiseless binary channel, and assume that we force communi-
cation at a rate greater than its capacity of 1 bit. For example, if we require
the sender to attempt to communicate at R = 2 bits per cycle then he must
e(cid:11)ectively throw away half of the information. What is the best way to do
this if the aim is to achieve the smallest possible probability of bit error? One
simple strategy is to communicate a fraction 1=R of the source bits, and ignore
the rest. The receiver guesses the missing fraction 1 (cid:0) 1=R at random, and

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

168

10 | The Noisy-Channel Coding Theorem

Optimum
Simple

0.3

0.25

0.2

pb

0.15

0.1

0.05

0

0

0.5

1

1.5

2

2.5

R

Figure 10.7. A simple bound on
achievable points (R; pb), and
Shannon’s bound.

the average probability of bit error is

pb =

1
2

(1 (cid:0) 1=R):

(10.19)

The curve corresponding to this strategy is shown by the dashed line in (cid:12)g-
ure 10.7.

the risk of corruption evenly among all the bits.
pb = H(cid:0)1
can this optimum be achieved?

We can do better than this (in terms of minimizing pb) by spreading out
In fact, we can achieve
2 (1 (cid:0) 1=R), which is shown by the solid curve in (cid:12)gure 10.7. So, how
We reuse a tool that we just developed, namely the (N; K) code for a
noisy channel, and we turn it on its head, using the decoder to de(cid:12)ne a lossy
compressor. Speci(cid:12)cally, we take an excellent (N; K) code for the binary
symmetric channel. Assume that such a code has a rate R0 = K=N , and that
it is capable of correcting errors introduced by a binary symmetric channel
whose transition probability is q. Asymptotically, rate-R0 codes exist that
have R0 ’ 1 (cid:0) H2(q). Recall that, if we attach one of these capacity-achieving
codes of length N to a binary symmetric channel then (a) the probability
distribution over the outputs is close to uniform, since the entropy of the
output is equal to the entropy of the source (N R0) plus the entropy of the
noise (N H2(q)), and (b) the optimal decoder of the code, in this situation,
typically maps a received vector of length N to a transmitted vector di(cid:11)ering
in qN bits from the received vector.

We take the signal that we wish to send, and chop it into blocks of length N
(yes, N , not K). We pass each block through the decoder, and obtain a shorter
signal of length K bits, which we communicate over the noiseless channel. To
decode the transmission, we pass the K bit message to the encoder of the
original code. The reconstituted message will now di(cid:11)er from the original
message in some of its bits { typically qN of them. So the probability of bit
error will be pb = q. The rate of this lossy compressor is R = N=K = 1=R0 =
1=(1 (cid:0) H2(pb)).
Now, attaching this lossy compressor to our capacity-C error-free commu-
nicator, we have proved the achievability of communication up to the curve
(pb; R) de(cid:12)ned by:

R =

:

2

(10.20)

C

1 (cid:0) H2(pb)

For further reading about rate-distortion theory, see Gallager (1968), p. 451,
or McEliece (2002), p. 75.

10.5 The non-achievable region (part 3 of the theorem)

The source, encoder, noisy channel and decoder de(cid:12)ne a Markov chain:

s ! x ! y ! ^s

P (s; x; y; ^s) = P (s)P (xj s)P (y j x)P (^s j y):

(10.21)

The data processing inequality (exercise 8.9, p.141) must apply to this chain:
I(s; ^s) (cid:20) I(x; y): Furthermore, by the de(cid:12)nition of channel capacity, I(x; y) (cid:20)
N C, so I(s; ^s) (cid:20) N C.
Assume that a system achieves a rate R and a bit error probability pb;
then the mutual information I(s; ^s) is (cid:21) N R(1 (cid:0) H2(pb)). But I(s; ^s) > N C
is not achievable, so R >

is not achievable.

2

C

1(cid:0)H2(pb)

Exercise 10.1.[3 ] Fill in the details in the preceding argument. If the bit errors
between ^s and s are independent then we have I(s; ^s) = N R(1(cid:0)H2(pb)).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.6: Computing capacity

169

Sections 10.6{10.8 contain
advanced material. The (cid:12)rst-time
reader is encouraged to skip to
section 10.9 (p.172).

What if we have complex correlations among those bit errors? Why does
the inequality I(s; ^s) (cid:21) N R(1 (cid:0) H2(pb)) hold?

10.6 Computing capacity

We have proved that the capacity of a channel is the maximum rate at which
reliable communication can be achieved. How can we compute the capacity of
a given discrete memoryless channel? We need to (cid:12)nd its optimal input distri-
bution. In general we can (cid:12)nd the optimal input distribution by a computer
search, making use of the derivative of the mutual information with respect
to the input probabilities.

. Exercise 10.2.[2 ] Find the derivative of I(X; Y ) with respect to the input prob-
ability pi, @I(X; Y )=@pi, for a channel with conditional probabilities Qjji.

Exercise 10.3.[2 ] Show that I(X; Y ) is a concave _ function of the input prob-

ability vector p.

Since I(X; Y ) is concave _ in the input distribution p, any probability distri-
bution p at which I(X; Y ) is stationary must be a global maximum of I(X; Y ).
So it is tempting to put the derivative of I(X; Y ) into a routine that (cid:12)nds a
local maximum of I(X; Y ), that is, an input distribution P (x) such that

@I(X; Y )

@pi

= (cid:21) for all i;

(10.22)

where (cid:21) is a Lagrange multiplier associated with the constraint Pi pi = 1.

However, this approach may fail to (cid:12)nd the right answer, because I(X; Y )
might be maximized by a distribution that has pi = 0 for some inputs. A
simple example is given by the ternary confusion channel.

Ternary confusion channel. AX =f0; ?; 1g. AY =f0; 1g.

-
0
(cid:0)(cid:0)(cid:18)
?
@@R1
-

0

1

P (y = 0j x = 0) = 1 ;
P (y = 1j x = 0) = 0 ;

P (y = 0j x = ?) = 1=2 ;
P (y = 1j x = ?) = 1=2 ;

P (y = 0j x = 1) = 0 ;
P (y = 1j x = 1) = 1:

Whenever the input ? is used, the output is random; the other inputs
are reliable inputs. The maximum information rate of 1 bit is achieved
by making no use of the input ?.

. Exercise 10.4.[2, p.173] Sketch the mutual information for this channel as a
function of the input distribution p. Pick a convenient two-dimensional
representation of p.

The optimization routine must therefore take account of the possibility that,
as we go up hill on I(X; Y ), we may run into the inequality constraints pi (cid:21) 0.
. Exercise 10.5.[2, p.174] Describe the condition, similar to equation (10.22), that
is satis(cid:12)ed at a point where I(X; Y ) is maximized, and describe a com-
puter program for (cid:12)nding the capacity of a channel.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

170

10 | The Noisy-Channel Coding Theorem

Results that may help in (cid:12)nding the optimal input distribution

1. All outputs must be used.

2. I(X; Y ) is a convex ^ function of the channel parameters.

3. There may be several optimal input distributions, but they all look the

same at the output.

. Exercise 10.6.[2 ] Prove that no output y is unused by an optimal input distri-

Reminder: The term ‘convex ^’
means ‘convex’, and the term
‘concave _’ means ‘concave’; the
little smile and frown symbols are
included simply to remind you
what convex and concave mean.

bution, unless it is unreachable, that is, has Q(y j x) = 0 for all x.
Exercise 10.7.[2 ] Prove that I(X; Y ) is a convex ^ function of Q(y j x).
Exercise 10.8.[2 ] Prove that all optimal input distributions of a channel have

the same output probability distribution P (y) =Px P (x)Q(y j x).

These results, along with the fact that I(X; Y ) is a concave _ function of
the input probability vector p, prove the validity of the symmetry argument
that we have used when (cid:12)nding the capacity of symmetric channels.
If a
channel is invariant under a group of symmetry operations { for example,
interchanging the input symbols and interchanging the output symbols { then,
given any optimal input distribution that is not symmetric, i.e., is not invariant
under these operations, we can create another input distribution by averaging
together this optimal input distribution and all its permuted forms that we
can make by applying the symmetry operations to the original optimal input
distribution. The permuted distributions must have the same I(X; Y ) as the
original, by symmetry, so the new input distribution created by averaging
must have I(X; Y ) bigger than or equal to that of the original distribution,
because of the concavity of I.

Symmetric channels

In order to use symmetry arguments, it will help to have a de(cid:12)nition of a
symmetric channel. I like Gallager’s (1968) de(cid:12)nition.

A discrete memoryless channel is a symmetric channel if the set of
outputs can be partitioned into subsets in such a way that for each
subset the matrix of transition probabilities has the property that each
row (if more than 1) is a permutation of each other row and each column
is a permutation of each other column.

Example 10.9. This channel

P (y = 0j x = 0) = 0:7 ;
P (y = ?j x = 0) = 0:2 ;
P (y = 1j x = 0) = 0:1 ;

P (y = 0j x = 1) = 0:1 ;
P (y = ?j x = 1) = 0:2 ;
P (y = 1j x = 1) = 0:7:

(10.23)

is a symmetric channel because its outputs can be partitioned into (0; 1)
and ?, so that the matrix can be rewritten:

P (y = 0j x = 0) = 0:7 ;
P (y = 1j x = 0) = 0:1 ;
P (y = ?j x = 0) = 0:2 ;

P (y = 0j x = 1) = 0:1 ;
P (y = 1j x = 1) = 0:7 ;
P (y = ?j x = 1) = 0:2:

(10.24)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.7: Other coding theorems

171

Symmetry is a useful property because, as we will see in a later chapter,
communication at capacity can be achieved over symmetric channels by linear
codes.

Exercise 10.10.[2 ] Prove that for a symmetric channel with any number of
inputs, the uniform distribution over the inputs is an optimal input
distribution.

. Exercise 10.11.[2, p.174] Are there channels that are not symmetric whose op-
timal input distributions are uniform? Find one, or prove there are
none.

10.7 Other coding theorems

The noisy-channel coding theorem that we proved in this chapter is quite gen-
eral, applying to any discrete memoryless channel; but it is not very speci(cid:12)c.
The theorem only says that reliable communication with error probability (cid:15)
and rate R can be achieved by using codes with su(cid:14)ciently large blocklength
N . The theorem does not say how large N needs to be to achieve given values
of R and (cid:15).

Presumably, the smaller (cid:15) is and the closer R is to C, the larger N has to

be.

Er(R)

C

R

Figure 10.8. A typical
random-coding exponent.

Noisy-channel coding theorem { version with explicit N -dependence

For a discrete memoryless channel, a blocklength N and a rate R,
there exist block codes of length N whose average probability of
error satis(cid:12)es:

pB (cid:20) exp [(cid:0)N Er(R)]

(10.25)

where Er(R) is the random-coding exponent of the channel, a
convex ^, decreasing, positive function of R for 0 (cid:20) R < C. The
random-coding exponent is also known as the reliability function.

[By an expurgation argument it can also be shown that there exist
block codes for which the maximal probability of error pBM is also
exponentially small in N .]

The de(cid:12)nition of Er(R) is given in Gallager (1968), p. 139. Er(R) approaches
zero as R ! C; the typical behaviour of this function is illustrated in (cid:12)g-
ure 10.8. The computation of the random-coding exponent for interesting
channels is a challenging task on which much e(cid:11)ort has been expended. Even
for simple channels like the binary symmetric channel, there is no simple ex-
pression for Er(R).

Lower bounds on the error probability as a function of blocklength

The theorem stated above asserts that there are codes with pB smaller than
exp [(cid:0)N Er(R)]. But how small can the error probability be? Could it be
much smaller?

For any code with blocklength N on a discrete memoryless channel,
the probability of error assuming all source messages are used with
equal probability satis(cid:12)es

pB  

exp[(cid:0)N Esp(R)];

(10.26)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

172

10 | The Noisy-Channel Coding Theorem

where the function Esp(R), the sphere-packing exponent of the
channel, is a convex ^, decreasing, positive function of R for 0 (cid:20)
R < C.

For a precise statement of this result and further references, see Gallager
(1968), p. 157.

10.8 Noisy-channel coding theorems and coding practice

Imagine a customer who wants to buy an error-correcting code and decoder
for a noisy channel. The results described above allow us to o(cid:11)er the following
service: if he tells us the properties of his channel, the desired rate R and the
desired error probability pB, we can, after working out the relevant functions
C, Er(R), and Esp(R), advise him that there exists a solution to his problem
using a particular blocklength N ; indeed that almost any randomly chosen
code with that blocklength should do the job. Unfortunately we have not
found out how to implement these encoders and decoders in practice; the cost
of implementing the encoder and decoder for a random code with large N
would be exponentially large in N .

Furthermore, for practical purposes, the customer is unlikely to know ex-
actly what channel he is dealing with. So Berlekamp (1980) suggests that
the sensible way to approach error-correction is to design encoding-decoding
systems and plot their performance on a variety of idealized channels as a
function of the channel’s noise level. These charts (one of which is illustrated
on page 568) can then be shown to the customer, who can choose among the
systems on o(cid:11)er without having to specify what he really thinks his channel
is like. With this attitude to the practical problem, the importance of the
functions Er(R) and Esp(R) is diminished.

10.9 Further exercises

Exercise 10.12.[2 ] A binary erasure channel with input x and output y has

transition probability matrix:

Q =2
4

1 (cid:0) q
q
0

0
q
1 (cid:0) q

3
5

-
@@R
(cid:0)(cid:0)(cid:18)
-

0

1

0
?
1

Find the mutual information I(X; Y ) between the input and output for
general input distribution fp0; p1g, and show that the capacity of this
channel is C = 1 (cid:0) q bits.

A Z channel has transition probability matrix:

Q =(cid:20) 1

0 1 (cid:0) q (cid:21)

q

-
(cid:1)(cid:1)(cid:21)
-
(cid:1)

0

1

0

1

Show that, using a (2; 1) code, two uses of a Z channel can be made to
emulate one use of an erasure channel, and state the erasure probability
of that erasure channel. Hence show that the capacity of the Z channel,
CZ, satis(cid:12)es CZ (cid:21) 1
2 (1 (cid:0) q) bits.
Explain why the result CZ (cid:21) 1
equality.

2 (1 (cid:0) q) is an inequality rather than an

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.10: Solutions

173

Exercise 10.13.[3, p.174] A transatlantic cable contains N = 20 indistinguish-
able electrical wires. You have the job of (cid:12)guring out which wire is
which, that is, to create a consistent labelling of the wires at each end.
Your only tools are the ability to connect wires to each other in groups
of two or more, and to test for connectedness with a continuity tester.
What is the smallest number of transatlantic trips you need to make,
and how do you do it?

How would you solve the problem for larger N such as N = 1000?

As an illustration, if N were 3 then the task can be solved in two steps
by labelling one wire at one end a, connecting the other two together,
crossing the Atlantic, measuring which two wires are connected, labelling
them b and c and the unconnected one a, then connecting b to a and
returning across the Atlantic, whereupon on disconnecting b from c, the
identities of b and c can be deduced.

This problem can be solved by persistent search, but the reason it is
posed in this chapter is that it can also be solved by a greedy approach
based on maximizing the acquired information. Let the unknown per-
mutation of wires be x. Having chosen a set of connections of wires C at
one end, you can then make measurements at the other end, and these
measurements y convey information about x. How much? And for what
set of connections is the information that y conveys about x maximized?

10.10 Solutions

Solution to exercise 10.4 (p.169).
the mutual information is

If the input distribution is p = (p0; p?; p1),

I(X; Y ) = H(Y ) (cid:0) H(Y jX) = H2(p0 + p?=2) (cid:0) p?:

(10.27)

We can build a good sketch of this function in two ways: by careful inspection
of the function, or by looking at special cases.

For the plots, the two-dimensional representation of p I will use has p0 and
p1 as the independent variables, so that p = (p0; p?; p1) = (p0; (1(cid:0)p0(cid:0)p1); p1).

If we use the quantities p(cid:3) (cid:17) p0 + p?=2 and p? as our two
By inspection.
degrees of freedom, the mutual information becomes very simple: I(X; Y ) =
H2(p(cid:3)) (cid:0) p?. Converting back to p0 = p(cid:3) (cid:0) p?=2 and p1 = 1 (cid:0) p(cid:3) (cid:0) p?=2,
we obtain the sketch shown at the left below. This function is like a tunnel
rising up the direction of increasing p0 and p1. To obtain the required plot of
I(X; Y ) we have to strip away the parts of this tunnel that live outside the
feasible simplex of probabilities; we do this by redrawing the surface, showing
only the parts where p0 > 0 and p1 > 0. A full plot of the function is shown
at the right.

-
(cid:0)(cid:0)(cid:18)
1/2
1/2
@@R
-

(cid:0)
@

0

?

1

0

1

1

0.5

0

-0.5

-1

-0.5

0

0.5

p0

1

1
1

0.5
0.5

0
0

0
0

p1

1
0.5

0
-0.5

1
1
0.5
0.5

p1

0
0

0.5
0.5

1
1

p0

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

174

10 | The Noisy-Channel Coding Theorem

1

0.5

0

0

p1
1
0.5

0

0.5

p0

1

Figure 10.9. Skeleton of the
mutual information for the
ternary confusion channel.

Special cases.
channel, and I(X; Y ) = H2(p0).

In the special case p? = 0, the channel is a noiseless binary

In the special case p0 = p1, the term H2(p0 + p?=2) is equal to 1, so

I(X; Y ) = 1 (cid:0) p?.
0.5. We know how to sketch that, from the previous chapter ((cid:12)gure 9.3).

In the special case p0 = 0, the channel is a Z channel with error probability

These special cases allow us to construct the skeleton shown in (cid:12)gure 10.9.

Solution to exercise 10.5 (p.169). Necessary and su(cid:14)cient conditions for p to
maximize I(X; Y ) are

@I(X;Y )

@pi

@I(X;Y )

@pi

= (cid:21) and pi > 0

(cid:20) (cid:21) and pi = 0 ) for all i;

(10.28)

where (cid:21) is a constant related to the capacity by C = (cid:21) + log2 e.

This result can be used in a computer program that evaluates the deriva-
tives, and increments and decrements the probabilities pi in proportion to the
di(cid:11)erences between those derivatives.

This result is also useful for lazy human capacity-(cid:12)nders who are good
guessers. Having guessed the optimal input distribution, one can simply con-
(cid:12)rm that equation (10.28) holds.

Solution to exercise 10.11 (p.171). We certainly expect nonsymmetric chan-
nels with uniform optimal input distributions to exist, since when inventing a
channel we have I(J (cid:0) 1) degrees of freedom whereas the optimal input dis-
tribution is just (I (cid:0) 1)-dimensional; so in the I(J (cid:0)1)-dimensional space of
perturbations around a symmetric channel, we expect there to be a subspace
of perturbations of dimension I(J (cid:0) 1) (cid:0) (I (cid:0) 1) = I(J (cid:0) 2) + 1 that leave the
optimal input distribution unchanged.

Here is an explicit example, a bit like a Z channel.

Q =2
664

0:9585 0:0415 0:35
0:0415 0:9585
0:0
0:65

0:0
0:35

0

0
0

0
0

0

0:65

3
775

(10.29)

Solution to exercise 10.13 (p.173). The labelling problem can be solved for
any N > 2 with just two trips, one each way across the Atlantic.

The key step in the information-theoretic approach to this problem is to
write down the information content of one partition, the combinatorial object
that is the connecting together of subsets of wires. If N wires are grouped
together into g1 subsets of size 1, g2 subsets of size 2, : : : ; then the number of
such partitions is

(cid:10) =

;

(10.30)

N !
(r!)gr gr!

Yr

and the information content of one such partition is the log of this quantity.
In a greedy strategy we choose the (cid:12)rst partition to maximize this information
content.

One game we can play is to maximize this information content with re-
spect to the quantities gr, treated as real numbers, subject to the constraint
Introducing a Lagrange multiplier (cid:21) for the constraint, the

Pr grr = N .

derivative is

@

@gr  log (cid:10) + (cid:21)Xr

grr! = (cid:0) log r! (cid:0) log gr + (cid:21)r;

(10.31)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

10.10: Solutions

175

which, when set to zero, leads to the rather nice expression

gr =

e(cid:21)r
r!

;

(10.32)

the optimal gr is proportional to a Poisson distribution! We can solve for the

N = (cid:22) e(cid:22);

gives the implicit equation

Lagrange multiplier by plugging gr into the constraint Pr grr = N , which
(10.33)
where (cid:22) (cid:17) e(cid:21) is a convenient reparameterization of the Lagrange multiplier.
Figure 10.10a shows a graph of (cid:22)(N ); (cid:12)gure 10.10b shows the deduced non-
integer assignments gr when (cid:22) = 2:2, and nearby integers gr = f1; 2; 2; 1; 1g
that motivate setting the (cid:12)rst partition to (a)(bc)(de)(fgh)(ijk)(lmno)(pqrst).
This partition produces a random partition at the other end, which has an
information content of log (cid:10) = 40:4 bits, which is a lot more than half the total
information content we need to acquire to infer the transatlantic permutation,
log 20! ’ 61 bits.
[In contrast, if all the wires are joined together in pairs,
the information content generated is only about 29 bits.] How to choose the
second partition is left to the reader. A Shannonesque approach is appropriate,
picking a random partition at the other end, using the same fgrg; you need
to ensure the two partitions are as unlike each other as possible.
If N 6= 2, 5 or 9, then the labelling problem has solutions that are
particularly simple to implement, called Knowlton{Graham partitions: par-
tition f1; : : : ; Ng into disjoint sets in two ways A and B, subject to the
condition that at most one element appears both in an A set of cardinal-
ity j and in a B set of cardinality k, for each j and k (Graham, 1966;
Graham and Knowlton, 1968).

5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
(a) 0.5

2.5

2

1.5

1

0.5

0

(b)

1

10

100

1000

1

2

3

4

5

6

7

8

9

10

Figure 10.10. Approximate
solution of the cable-labelling
problem using Lagrange
multipliers. (a) The parameter (cid:22)
as a function of N ; the value
(cid:22)(20) = 2:2 is highlighted. (b)
Non-integer values of the function
gr = (cid:22)r/r! are shown by lines and
integer values of gr motivated by
those non-integer values are
shown by crosses.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 11

Before reading Chapter 11, you should have read Chapters 9 and 10.
You will also need to be familiar with the Gaussian distribution.

One-dimensional Gaussian distribution. If a random variable y is Gaus-

sian and has mean (cid:22) and variance (cid:27)2, which we write:

y (cid:24) Normal((cid:22); (cid:27)2); or P (y) = Normal(y; (cid:22); (cid:27)2);

(11.1)

then the distribution of y is:

P (y j (cid:22); (cid:27)2) =

1

p2(cid:25)(cid:27)2

exp(cid:2)(cid:0)(y (cid:0) (cid:22))2=2(cid:27)2(cid:3) :

(11.2)

[I use the symbol P for both probability densities and probabilities.]
The inverse-variance (cid:28) (cid:17) 1/(cid:27)2 is sometimes called the precision of the
Gaussian distribution.

Multi-dimensional Gaussian distribution. If y = (y1; y2; : : : ; yN ) has a

multivariate Gaussian distribution, then

P (y j x; A) =

1

Z(A)

exp(cid:18)(cid:0)

1
2

(y (cid:0) x)TA(y (cid:0) x)(cid:19) ;

(11.3)

where x is the mean of the distribution, A is the inverse of the
variance{covariance matrix, and the normalizing constant is Z(A) =
(det(A=2(cid:25)))(cid:0)1=2.

This distribution has the property that the variance (cid:6)ii of yi, and the
covariance (cid:6)ij of yi and yj are given by

(cid:6)ij (cid:17) E [(yi (cid:0) (cid:22)yi)(yj (cid:0) (cid:22)yj)] = A(cid:0)1
ij ;

(11.4)

where A(cid:0)1 is the inverse of the matrix A.
The marginal distribution P (yi) of one component yi
is Gaussian;
the joint marginal distribution of any subset of the components is
multivariate-Gaussian; and the conditional density of any subset, given
the values of another subset, for example, P (yi j yj), is also Gaussian.

176

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11

Error-Correcting Codes & Real Channels

The noisy-channel coding theorem that we have proved shows that there exist
reliable error-correcting codes for any noisy channel. In this chapter we address
two questions.

First, many practical channels have real, rather than discrete, inputs and
outputs. What can Shannon tell us about these continuous channels? And
how should digital signals be mapped into analogue waveforms, and vice versa?
Second, how are practical error-correcting codes made, and what is

achieved in practice, relative to the possibilities proved by Shannon?

11.1 The Gaussian channel

The most popular model of a real-input, real-output channel is the Gaussian
channel.

The Gaussian channel has a real input x and a real output y. The condi-

tional distribution of y given x is a Gaussian distribution:

P (y j x) =

1

p2(cid:25)(cid:27)2

exp(cid:2)(cid:0)(y (cid:0) x)2=2(cid:27)2(cid:3) :

(11.5)

This channel has a continuous input and output but is discrete in time.
We will show below that certain continuous-time channels are equivalent
to the discrete-time Gaussian channel.

This channel
(AWGN) channel.

is sometimes called the additive white Gaussian noise

As with discrete channels, we will discuss what rate of error-free information
communication can be achieved over this channel.

Motivation in terms of a continuous-time channel

Consider a physical (electrical, say) channel with inputs and outputs that are
continuous in time. We put in x(t), and out comes y(t) = x(t) + n(t).

Our transmission has a power cost. The average power of a transmission

of length T may be constrained thus:

Z T

0

dt [x(t)]2=T (cid:20) P:

(11.6)

The received signal is assumed to di(cid:11)er from x(t) by additive noise n(t) (for
example Johnson noise), which we will model as white Gaussian noise. The
magnitude of this noise is quanti(cid:12)ed by the noise spectral density, N0.

177

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

178

11 | Error-Correcting Codes and Real Channels

How could such a channel be used to communicate information? Consider
n=1 in a signal of duration T made

transmitting a set of N real numbers fxngN
up of a weighted combination of orthonormal basis functions (cid:30)n(t),

(cid:30)1(t)

x(t) =

N

Xn=1

xn(cid:30)n(t);

(11.7)

(cid:30)2(t)

where R T

0 dt (cid:30)n(t)(cid:30)m(t) = (cid:14)nm. The receiver can then compute the scalars:

yn (cid:17) Z T

0

dt (cid:30)n(t)y(t) = xn +Z T
(cid:17) xn + nn

0

dt (cid:30)n(t)n(t)

(11.8)

(11.9)

for n = 1 : : : N . If there were no noise, then yn would equal xn. The white
Gaussian noise n(t) adds scalar noise nn to the estimate yn. This noise is
Gaussian:

nn (cid:24) Normal(0; N0=2);

(11.10)

where N0 is the spectral density introduced above. Thus a continuous chan-
nel used in this way is equivalent to the Gaussian channel de(cid:12)ned at equa-
0 dt [x(t)]2 (cid:20) P T de(cid:12)nes a constraint on

tion (11.5). The power constraint R T

the signal amplitudes xn,

(cid:30)3(t)

x(t)

Figure 11.1. Three basis functions,
and a weighted combination of

them, x(t) =PN
n=1 xn(cid:30)n(t); with
x1 = 0:4, x2 = (cid:0) 0:2, and x3 = 0:1.

x2
n (cid:20) P T

)

Xn

x2
n (cid:20)

P T
N

:

(11.11)

Before returning to the Gaussian channel, we de(cid:12)ne the bandwidth (mea-

sured in Hertz) of the continuous channel to be:

W =

N max

2T

;

(11.12)

where N max is the maximum number of orthonormal functions that can be
produced in an interval of length T . This de(cid:12)nition can be motivated by
imagining creating a band-limited signal of duration T from orthonormal co-
sine and sine curves of maximum frequency W . The number of orthonormal
functions is N max = 2W T . This de(cid:12)nition relates to the Nyquist sampling
theorem:
if the highest frequency present in a signal is W , then the signal
can be fully determined from its values at a series of discrete sample points
separated by the Nyquist interval (cid:1)t = 1/2W seconds.

So the use of a real continuous channel with bandwidth W , noise spectral
density N0, and power P is equivalent to N=T = 2W uses per second of a
Gaussian channel with noise level (cid:27)2 = N0=2 and subject to the signal power
constraint x2

n (cid:20) P/2W .
De(cid:12)nition of Eb=N0

Imagine that the Gaussian channel yn = xn + nn is used with an encoding
system to transmit binary source bits at a rate of R bits per channel use. How
can we compare two encoding systems that have di(cid:11)erent rates of communi-
cation R and that use di(cid:11)erent powers x2
n? Transmitting at a large rate R is
good; using small power is good too.

It is conventional to measure the rate-compensated signal-to-noise ratio by
n=R to the noise spectral density

the ratio of the power per source bit Eb = x2
N0:

Eb=N0 =

x2
n

2(cid:27)2R

:

(11.13)

Eb=N0 is one of the measures used to compare coding schemes for Gaussian

channels.

Eb=N0 is dimensionless, but it is
usually reported in the units of
decibels; the value given is
10 log10 Eb=N0.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11.2: Inferring the input to a real channel

179

11.2 Inferring the input to a real channel

‘The best detection of pulses’

In 1944 Shannon wrote a memorandum (Shannon, 1993) on the problem of
best di(cid:11)erentiating between two types of pulses of known shape, represented
by vectors x0 and x1, given that one of them has been transmitted over a
noisy channel. This is a pattern recognition problem. It is assumed that the
noise is Gaussian with probability density

P (n) =(cid:20)det(cid:18) A

2(cid:25)(cid:19)(cid:21)1=2

exp(cid:18)(cid:0)

1
2

nTAn(cid:19) ;

(11.14)

where A is the inverse of the variance{covariance matrix of the noise, a sym-
metric and positive-de(cid:12)nite matrix. (If A is a multiple of the identity matrix,
I=(cid:27)2, then the noise is ‘white’. For more general A, the noise is ‘coloured’.)
The probability of the received vector y given that the source signal was s
(either zero or one) is then

P (y j s) =(cid:20)det(cid:18) A

2(cid:25)(cid:19)(cid:21)1=2

exp(cid:18)(cid:0)

1
2

(y (cid:0) xs)TA(y (cid:0) xs)(cid:19) :

The optimal detector is based on the posterior probability ratio:

P (s = 1)
P (s = 0)

P (s = 1j y)
P (s = 0j y)
= exp(cid:18)(cid:0)
= exp (yTA(x1 (cid:0) x0) + (cid:18)) ;

P (y j s = 1)
P (y j s = 0)
(y (cid:0) x1)TA(y (cid:0) x1) +

=

1
2

1
2

(y (cid:0) x0)TA(y (cid:0) x0) + ln

(11.15)

(11.16)

P (s = 1)

P (s = 0)(cid:19)

(11.17)

where (cid:18) is a constant independent of the received vector y,

(cid:18) = (cid:0)

1
2

xT

1Ax1 +

1
2

xT

0Ax0 + ln

P (s = 1)
P (s = 0)

:

(11.18)

If the detector is forced to make a decision (i.e., guess either s = 1 or s = 0) then
the decision that minimizes the probability of error is to guess the most prob-
able hypothesis. We can write the optimal decision in terms of a discriminant
function:

with the decisions

a(y) (cid:17) yTA(x1 (cid:0) x0) + (cid:18)

a(y) > 0 ! guess s = 1
a(y) < 0 ! guess s = 0
a(y) = 0 ! guess either.

(11.19)

(11.20)

Notice that a(y) is a linear function of the received vector,

a(y) = wTy + (cid:18);

(11.21)

where w (cid:17) A(x1 (cid:0) x0).

11.3 Capacity of Gaussian channel

Until now we have measured the joint, marginal, and conditional entropy
of discrete variables only.
In order to de(cid:12)ne the information conveyed by
continuous variables, there are two issues we must address { the in(cid:12)nite length
of the real line, and the in(cid:12)nite precision of real numbers.

x0

x1

y

Figure 11.2. Two pulses x0 and
x1, represented as 31-dimensional
vectors, and a noisy version of one
of them, y.

w

Figure 11.3. The weight vector
w / x1 (cid:0) x0 that is used to
discriminate between x0 and x1.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

180

In(cid:12)nite inputs

11 | Error-Correcting Codes and Real Channels

(a)

(b)

-(cid:27)g

...

Figure 11.4. (a) A probability
density P (x). Question: can we
de(cid:12)ne the ‘entropy’ of this
density? (b) We could evaluate
the entropies of a sequence of
probability distributions with
decreasing grain-size g, but these
entropies tend to

Z P (x) log

1

P (x)g

independent of g: the entropy
goes up by one bit for every
halving of g.

dx, which is not

Z P (x) log

integral.

1

P (x)

dx is an illegal

How much information can we convey in one use of a Gaussian channel? If
we are allowed to put any real number x into the Gaussian channel, we could
communicate an enormous string of N digits d1d2d3 : : : dN by setting x =
d1d2d3 : : : dN 000 : : : 000. The amount of error-free information conveyed in
just a single transmission could be made arbitrarily large by increasing N ,
and the communication could be made arbitrarily reliable by increasing the
number of zeroes at the end of x. There is usually some power cost associated
with large inputs, however, not to mention practical limits in the dynamic
range acceptable to a receiver.
It is therefore conventional to introduce a
cost function v(x) for every input x, and constrain codes to have an average
cost (cid:22)v less than or equal to some maximum value. A generalized channel
coding theorem, including a cost function for the inputs, can be proved { see
McEliece (1977). The result is a channel capacity C((cid:22)v) that is a function of
the permitted cost. For the Gaussian channel we will assume a cost

v(x) = x2

(11.22)

such that the ‘average power’ x2 of the input is constrained. We motivated this
cost function above in the case of real electrical channels in which the physical
power consumption is indeed quadratic in x. The constraint x2 = (cid:22)v makes
it impossible to communicate in(cid:12)nite information in one use of the Gaussian
channel.

In(cid:12)nite precision

It is tempting to de(cid:12)ne joint, marginal, and conditional entropies for real
variables simply by replacing summations by integrals, but this is not a well
de(cid:12)ned operation. As we discretize an interval into smaller and smaller divi-
sions, the entropy of the discrete distribution diverges (as the logarithm of the
granularity) ((cid:12)gure 11.4). Also, it is not permissible to take the logarithm of
a dimensional quantity such as a probability density P (x) (whose dimensions
are [x](cid:0)1).

There is one information measure, however, that has a well-behaved limit,
namely the mutual information { and this is the one that really matters, since
it measures how much information one variable conveys about another. In the
discrete case,

I(X; Y ) =Xx;y

P (x; y) log

P (x; y)

P (x)P (y)

:

(11.23)

Now because the argument of the log is a ratio of two probabilities over the
same space, it is OK to have P (x; y), P (x) and P (y) be probability densities
and replace the sum by an integral:

I(X; Y ) = Z dx dy P (x; y) log

P (x; y)

P (x)P (y)

= Z dx dy P (x)P (y j x) log

P (y j x)
P (y)

:

(11.24)

(11.25)

We can now ask these questions for the Gaussian channel: (a) what probability
distribution P (x) maximizes the mutual information (subject to the constraint
x2 = v)? and (b) does the maximal mutual information still measure the
maximum error-free communication rate of this real channel, as it did for the
discrete channel?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11.3: Capacity of Gaussian channel

181

Exercise 11.1.[3, p.189] Prove that the probability distribution P (x) that max-
imizes the mutual information (subject to the constraint x2 = v) is a
Gaussian distribution of mean zero and variance v.

. Exercise 11.2.[2, p.189] Show that the mutual information I(X; Y ), in the case

of this optimized distribution, is

C =

1
2

log(cid:16)1 +

v

(cid:27)2(cid:17) :

(11.26)

This is an important result. We see that the capacity of the Gaussian channel
is a function of the signal-to-noise ratio v=(cid:27)2.

Inferences given a Gaussian input distribution
If P (x) = Normal(x; 0; v) and P (y j x) = Normal(y; x; (cid:27) 2) then the marginal
distribution of y is P (y) = Normal(y; 0; v+(cid:27)2) and the posterior distribution
of the input, given that the output is y, is:

P (xj y) / P (y j x)P (x)
/ exp((cid:0)(y (cid:0) x)2=2(cid:27)2) exp((cid:0)x2=2v)
= Normal x;

v + (cid:27)2 y ; (cid:18) 1

+

v

1

(cid:27)2(cid:19)(cid:0)1! :

v

(11.27)

(11.28)

(11.29)

[The step from (11.28) to (11.29) is made by completing the square in the
exponent.] This formula deserves careful study. The mean of the posterior
distribution,
v+(cid:27)2 y, can be viewed as a weighted combination of the value
that best (cid:12)ts the output, x = y, and the value that best (cid:12)ts the prior, x = 0:

v

v

v + (cid:27)2 y =

1=(cid:27)2

1=v + 1=(cid:27)2 y +

1=v

1=v + 1=(cid:27)2 0:

(11.30)

The weights 1=(cid:27)2 and 1=v are the precisions of the two Gaussians that we
multiplied together in equation (11.28): the prior and the likelihood.

The precision of the posterior distribution is the sum of these two pre-
cisions. This is a general property: whenever two independent sources con-
tribute information, via Gaussian distributions, about an unknown variable,
the precisions add.
[This is the dual to the better-known relationship ‘when
independent variables are added, their variances add’.]

Noisy-channel coding theorem for the Gaussian channel

We have evaluated a maximal mutual information. Does it correspond to a
maximum possible rate of error-free information transmission? One way of
proving that this is so is to de(cid:12)ne a sequence of discrete channels, all derived
from the Gaussian channel, with increasing numbers of inputs and outputs,
and prove that the maximum mutual information of these channels tends to the
asserted C. The noisy-channel coding theorem for discrete channels applies
to each of these derived channels, thus we obtain a coding theorem for the
continuous channel. Alternatively, we can make an intuitive argument for the
coding theorem speci(cid:12)c for the Gaussian channel.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

182

11 | Error-Correcting Codes and Real Channels

Geometrical view of the noisy-channel coding theorem: sphere packing

Consider a sequence x = (x1; : : : ; xN ) of inputs, and the corresponding output
y, as de(cid:12)ning two points in an N dimensional space. For large N , the noise
power is very likely to be close (fractionally) to N (cid:27) 2. The output y is therefore
very likely to be close to the surface of a sphere of radius pN (cid:27)2 centred on x.
Similarly, if the original signal x is generated at random subject to an average
power constraint x2 = v, then x is likely to lie close to a sphere, centred
on the origin, of radius pN v; and because the total average power of y is
v + (cid:27)2, the received signal y is likely to lie on the surface of a sphere of radius
pN (v + (cid:27)2), centred on the origin.

The volume of an N -dimensional sphere of radius r is

V (r; N ) = (cid:25)N=2

(cid:0)(N=2+1)

rN :

(11.31)

Now consider making a communication system based on non-confusable
inputs x, that is, inputs whose spheres do not overlap signi(cid:12)cantly. The max-
imum number S of non-confusable inputs is given by dividing the volume of
the sphere of probable ys by the volume of the sphere for y given x:

S (cid:20) pN (v + (cid:27)2)
pN (cid:27)2 !N

Thus the capacity is bounded by:

C =

1
N

log M (cid:20)

1
2

log(cid:16)1 +

(11.32)

(11.33)

v

(cid:27)2(cid:17) :

A more detailed argument like the one used in the previous chapter can es-
tablish equality.

Back to the continuous channel

Recall that the use of a real continuous channel with bandwidth W , noise
spectral density N0 and power P is equivalent to N=T = 2W uses per second of
a Gaussian channel with (cid:27)2 = N0=2 and subject to the constraint x2
n (cid:20) P=2W .
Substituting the result for the capacity of the Gaussian channel, we (cid:12)nd the
capacity of the continuous channel to be:

C = W log(cid:18)1 +

P

N0W(cid:19) bits per second.

(11.34)

This formula gives insight into the tradeo(cid:11)s of practical communication. Imag-
ine that we have a (cid:12)xed power constraint. What is the best bandwidth to make
use of that power? Introducing W0 = P=N0, i.e., the bandwidth for which the
signal-to-noise ratio is 1, (cid:12)gure 11.5 shows C=W0 = W=W0 log(1 + W0=W ) as
a function of W=W0. The capacity increases to an asymptote of W0 log e. It
is dramatically better (in terms of capacity for (cid:12)xed power) to transmit at a
low signal-to-noise ratio over a large bandwidth, than with high signal-to-noise
in a narrow bandwidth; this is one motivation for wideband communication
methods such as the ‘direct sequence spread-spectrum’ approach used in 3G
mobile phones. Of course, you are not alone, and your electromagnetic neigh-
bours may not be pleased if you use a large bandwidth, so for social reasons,
engineers often have to make do with higher-power, narrow-bandwidth trans-
mitters.

y
t
i
c
a
p
a
c

1.4
1.2
1
0.8
0.6
0.4
0.2
0

0

1

3

2
4
bandwidth

5

6

Figure 11.5. Capacity versus
bandwidth for a real channel:
C=W0 = W=W0 log (1 + W0=W )
as a function of W=W0.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11.4: What are the capabilities of practical error-correcting codes?

183

11.4 What are the capabilities of practical error-correcting codes?

Nearly all codes are good, but nearly all codes require exponential look-up
tables for practical implementation of the encoder and decoder { exponential
in the blocklength N . And the coding theorem required N to be large.

By a practical error-correcting code, we mean one that can be encoded
and decoded in a reasonable amount of time, for example, a time that scales
as a polynomial function of the blocklength N { preferably linearly.

The Shannon limit is not achieved in practice

The non-constructive proof of the noisy-channel coding theorem showed that
good block codes exist for any noisy channel, and indeed that nearly all block
codes are good. But writing down an explicit and practical encoder and de-
coder that are as good as promised by Shannon is still an unsolved problem.

Very good codes. Given a channel, a family of block codes that achieve
arbitrarily small probability of error at any communication rate up to
the capacity of the channel are called ‘very good’ codes for that channel.

Good codes are code families that achieve arbitrarily small probability of
error at non-zero communication rates up to some maximum rate that
may be less than the capacity of the given channel.

Bad codes are code families that cannot achieve arbitrarily small probability
of error, or that can achieve arbitrarily small probability of error only by
decreasing the information rate to zero. Repetition codes are an example
of a bad code family. (Bad codes are not necessarily useless for practical
purposes.)

Practical codes are code families that can be encoded and decoded in time

and space polynomial in the blocklength.

Most established codes are linear codes

Let us review the de(cid:12)nition of a block code, and then add the de(cid:12)nition of a
linear block code.

An (N; K) block code for a channel Q is a list of S = 2K codewords
fx(1); x(2); : : : ; x(2K )g, each of length N : x(s) 2 AN
X. The signal to be
encoded, s, which comes from an alphabet of size 2K, is encoded as x(s).
A linear (N; K) block code is a block code in which the codewords fx(s)g
make up a K-dimensional subspace of AN
X. The encoding operation can
be represented by an N (cid:2) K binary matrix GT such that if the signal to
be encoded, in binary notation, is s (a vector of length K bits), then the
encoded signal is t = GTs modulo 2.
The codewords ftg can be de(cid:12)ned as the set of vectors satisfying Ht =
0 mod 2, where H is the parity-check matrix of the code.

For example the (7; 4) Hamming code of section 1.2 takes K = 4 signal
bits, s, and transmits them followed by three parity-check bits. The N = 7
transmitted symbols are given by GTs mod 2.

Coding theory was born with the work of Hamming, who invented a fam-
ily of practical error-correcting codes, each able to correct one error in a
block of length N , of which the repetition code R3 and the (7; 4) code are

GT =2
6664

1 (cid:1) (cid:1) (cid:1)
(cid:1) 1 (cid:1) (cid:1)
(cid:1) (cid:1) 1 (cid:1)
(cid:1) (cid:1) (cid:1) 1
1 1 1 (cid:1)
(cid:1) 1 1 1
1 (cid:1) 1 1

3
7775

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

184

11 | Error-Correcting Codes and Real Channels

the simplest. Since then most established codes have been generalizations of
Hamming’s codes: Bose{Chaudhury{Hocquenhem codes, Reed{M(cid:127)uller codes,
Reed{Solomon codes, and Goppa codes, to name a few.

Convolutional codes

Another family of linear codes are convolutional codes, which do not divide
the source stream into blocks, but instead read and transmit bits continuously.
The transmitted bits are a linear function of the past source bits. Usually the
rule for generating the transmitted bits involves feeding the present source
bit into a linear-feedback shift-register of length k, and transmitting one or
more linear functions of the state of the shift register at each iteration. The
resulting transmitted bit stream is the convolution of the source stream with
a linear (cid:12)lter. The impulse-response function of this (cid:12)lter may have (cid:12)nite or
in(cid:12)nite duration, depending on the choice of feedback shift-register.

We will discuss convolutional codes in Chapter 48.

Are linear codes ‘good’ ?

One might ask, is the reason that the Shannon limit is not achieved in practice
because linear codes are inherently not as good as random codes? The answer
is no, the noisy-channel coding theorem can still be proved for linear codes,
at least for some channels (see Chapter 14), though the proofs, like Shannon’s
proof for random codes, are non-constructive.

Linear codes are easy to implement at the encoding end. Is decoding a
linear code also easy? Not necessarily. The general decoding problem ((cid:12)nd
the maximum likelihood s in the equation GTs + n = r) is in fact NP-complete
(Berlekamp et al., 1978). [NP-complete problems are computational problems
that are all equally di(cid:14)cult and which are widely believed to require expo-
nential computer time to solve in general.] So attention focuses on families of
codes for which there is a fast decoding algorithm.

Concatenation

One trick for building codes with practical decoders is the idea of concatena-
tion.

An encoder{channel{decoder system C ! Q ! D can be viewed as de(cid:12)ning
a super-channel Q0 with a smaller probability of error, and with complex
correlations among its errors. We can create an encoder C0 and decoder D0 for
this super-channel Q0. The code consisting of the outer code C0 followed by
the inner code C is known as a concatenated code.
Some concatenated codes make use of the idea of interleaving. We read
the data in blocks, the size of each block being larger than the blocklengths
of the constituent codes C and C0. After encoding the data of one block using
code C0, the bits are reordered within the block in such a way that nearby
bits are separated from each other once the block is fed to the second code
C. A simple example of an interleaver is a rectangular code or product code
in which the data are arranged in a K2 (cid:2) K1 block, and encoded horizontally
using an (N1; K1) linear code, then vertically using a (N2; K2) linear code.

. Exercise 11.3.[3 ] Show that either of the two codes can be viewed as the inner

code or the outer code.

As an example, (cid:12)gure 11.6 shows a product code in which we encode
(cid:12)rst with the repetition code R3 (also known as the Hamming code H(3; 1))

C0 ! C ! Q ! D
}

{z

Q0

|

! D0

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11.4: What are the capabilities of practical error-correcting codes?

185

1
0
1
1
0
0
1

1
0
1
1
0
0
1

1
0
1
1
0
0
1

(a)

?

?

?

?

?

(b)

(c)

1
1
1
1
0
1
1

1
1
1
0
0
0
1

1
0
1
1
1
0
1

1
1
1
1
0
0
1
1
1
1
1
1
1
1

1
1
1
1
0
0
1
0
1
1
0
0
0
1

1
1
1
1
0
0
1
1
0
1
1
0
0
1

(d)

(d0)

(e)

(e0)

1
0
1
1
0
0
1
1
(1)
1
1
0
0
1

1
0
1
1
0
0
1
1
(1)
1
1
0
0
1

1
0
1
1
0
0
1
1
(1)
1
1
0
0
1

Figure 11.6. A product code. (a)
A string 1011 encoded using a
concatenated code consisting of
two Hamming codes, H(3; 1) and
H(7; 4). (b) a noise pattern that
(cid:13)ips 5 bits. (c) The received
vector. (d) After decoding using
the horizontal (3; 1) decoder, and
(e) after subsequently using the
vertical (7; 4) decoder. The
decoded vector matches the
original.
(d0, e0) After decoding in the other
order, three errors still remain.

horizontally then with H(7; 4) vertically. The blocklength of the concatenated
code is 27. The number of source bits per codeword is four, shown by the
small rectangle.

We can decode conveniently (though not optimally) by using the individual
decoders for each of the subcodes in some sequence. It makes most sense to
(cid:12)rst decode the code which has the lowest rate and hence the greatest error-
correcting ability.

Figure 11.6(c{e) shows what happens if we receive the codeword of (cid:12)g-
ure 11.6a with some errors ((cid:12)ve bits (cid:13)ipped, as shown) and apply the decoder
for H(3; 1) (cid:12)rst, and then the decoder for H(7; 4). The (cid:12)rst decoder corrects
three of the errors, but erroneously modi(cid:12)es the third bit in the second row
where there are two bit errors. The (7; 4) decoder can then correct all three
of these errors.

Figure 11.6(d0{ e0) shows what happens if we decode the two codes in the
other order. In columns one and two there are two errors, so the (7; 4) decoder
introduces two extra errors. It corrects the one error in column 3. The (3; 1)
decoder then cleans up four of the errors, but erroneously infers the second
bit.

Interleaving

The motivation for interleaving is that by spreading out bits that are nearby
in one code, we make it possible to ignore the complex correlations among the
errors that are produced by the inner code. Maybe the inner code will mess
up an entire codeword; but that codeword is spread out one bit at a time over
several codewords of the outer code. So we can treat the errors introduced by
the inner code as if they are independent.

Other channel models

In addition to the binary symmetric channel and the Gaussian channel, coding
theorists keep more complex channels in mind also.

Burst-error channels are important models in practice. Reed{Solomon
codes use Galois (cid:12)elds (see Appendix C.1) with large numbers of elements
(e.g. 216) as their input alphabets, and thereby automatically achieve a degree
of burst-error tolerance in that even if 17 successive bits are corrupted, only 2
successive symbols in the Galois (cid:12)eld representation are corrupted. Concate-
nation and interleaving can give further protection against burst errors. The
concatenated Reed{Solomon codes used on digital compact discs are able to
correct bursts of errors of length 4000 bits.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

186

11 | Error-Correcting Codes and Real Channels

. Exercise 11.4.[2, p.189] The technique of interleaving, which allows bursts of
errors to be treated as independent, is widely used, but is theoretically
a poor way to protect data against burst errors, in terms of the amount
of redundancy required. Explain why interleaving is a poor method,
using the following burst-error channel as an example. Time is divided
into chunks of length N = 100 clock cycles; during each chunk, there
is a burst with probability b = 0:2; during a burst, the channel is a bi-
nary symmetric channel with f = 0:5. If there is no burst, the channel
is an error-free binary channel. Compute the capacity of this channel
and compare it with the maximum communication rate that could con-
ceivably be achieved if one used interleaving and treated the errors as
independent.

Fading channels are real channels like Gaussian channels except that the
received power is assumed to vary with time. A moving mobile phone is an
important example. The incoming radio signal is re(cid:13)ected o(cid:11) nearby objects
so that there are interference patterns and the intensity of the signal received
by the phone varies with its location. The received power can easily vary by
10 decibels (a factor of ten) as the phone’s antenna moves through a distance
similar to the wavelength of the radio signal (a few centimetres).

11.5 The state of the art

What are the best known codes for communicating over Gaussian channels?
All the practical codes are linear codes, and are either based on convolutional
codes or block codes.

Convolutional codes, and codes based on them

Textbook convolutional codes. The ‘de facto standard’ error-correcting
code for satellite communications is a convolutional code with constraint
length 7. Convolutional codes are discussed in Chapter 48.

Concatenated convolutional codes. The above convolutional code can be
used as the inner code of a concatenated code whose outer code is a Reed{
Solomon code with eight-bit symbols. This code was used in deep space
communication systems such as the Voyager spacecraft. For further
reading about Reed{Solomon codes, see Lin and Costello (1983).

The code for Galileo. A code using the same format but using a longer
constraint length { 15 { for its convolutional code and a larger Reed{
Solomon code was developed by the Jet Propulsion Laboratory (Swan-
son, 1988). The details of this code are unpublished outside JPL, and the
decoding is only possible using a room full of special-purpose hardware.
In 1992, this was the best code known of rate 1/4.

Turbo codes. In 1993, Berrou, Glavieux and Thitimajshima reported work
on turbo codes. The encoder of a turbo code is based on the encoders
of two convolutional codes. The source bits are fed into each encoder,
the order of the source bits being permuted in a random way, and the
resulting parity bits from each constituent code are transmitted.

The decoding algorithm involves iteratively decoding each constituent
code using its standard decoding algorithm, then using the output of
the decoder as the input to the other decoder. This decoding algorithm

-

C1

-

-

-

-

C2(cid:14)(cid:13)(cid:15)(cid:12)(cid:25)

Figure 11.7. The encoder of a
turbo code. Each box C1, C2,
contains a convolutional code.
The source bits are reordered
using a permutation (cid:25) before they
are fed to C2. The transmitted
codeword is obtained by
concatenating or interleaving the
outputs of the two convolutional
codes. The random permutation
is chosen when the code is
designed, and (cid:12)xed thereafter.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11.6: Summary

187

is an instance of a message-passing algorithm called the sum{product
algorithm.

Turbo codes are discussed in Chapter 48, and message passing in Chap-
ters 16, 17, 25, and 26.

H =

Block codes

Gallager’s low-density parity-check codes. The best block codes known
for Gaussian channels were invented by Gallager in 1962 but were
promptly forgotten by most of the coding theory community. They were
rediscovered in 1995 and shown to have outstanding theoretical and prac-
tical properties. Like turbo codes, they are decoded by message-passing
algorithms.

We will discuss these beautifully simple codes in Chapter 47.

The performances of the above codes are compared for Gaussian channels

in (cid:12)gure 47.17, p.568.

11.6 Summary

Random codes are good, but they require exponential resources to encode

and decode them.

Non-random codes tend for the most part not to be as good as random
codes. For a non-random code, encoding may be easy, but even for
simply-de(cid:12)ned linear codes, the decoding problem remains very di(cid:14)cult.

The best practical codes (a) employ very large block sizes; (b) are based
on semi-random code constructions; and (c) make use of probability-
based decoding algorithms.

11.7 Nonlinear codes

Figure 11.8. A low-density
parity-check matrix and the
corresponding graph of a rate-1/4
low-density parity-check code
with blocklength N = 16, and
M = 12 constraints. Each white
circle represents a transmitted bit.
Each bit participates in j = 3
constraints, represented by
squares. Each constraint forces
the sum of the k = 4 bits to which
it is connected to be even. This
code is a (16; 4) code.
Outstanding performance is
obtained when the blocklength is
increased to N ’ 10 000.

Most practically used codes are linear, but not all. Digital soundtracks are
encoded onto cinema (cid:12)lm as a binary pattern. The likely errors a(cid:11)ecting the
(cid:12)lm involve dirt and scratches, which produce large numbers of 1s and 0s
respectively. We want none of the codewords to look like all-1s or all-0s, so
that it will be easy to detect errors caused by dirt and scratches. One of the
codes used in digital cinema sound systems is a nonlinear (8; 6) code consisting

of 64 of the (cid:0)8

4(cid:1) binary patterns of weight 4.

11.8 Errors other than noise

Another source of uncertainty for the receiver is uncertainty about the tim-
ing of the transmitted signal x(t).
In ordinary coding theory and infor-
mation theory, the transmitter’s time t and the receiver’s time u are as-
sumed to be perfectly synchronized. But if the receiver receives a signal
y(u), where the receiver’s time, u,
is an imperfectly known function u(t)
of the transmitter’s time t, then the capacity of this channel for commu-
nication is reduced. The theory of such channels is incomplete, compared
with the synchronized channels we have discussed thus far. Not even the ca-
pacity of channels with synchronization errors is known (Levenshtein, 1966;
Ferreira et al., 1997); codes for reliable communication over channels with
synchronization errors remain an active research area (Davey and MacKay,
2001).

 
 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

188

Further reading

11 | Error-Correcting Codes and Real Channels

For a review of the history of spread-spectrum methods, see Scholtz (1982).

11.9 Exercises

The Gaussian channel

. Exercise 11.5.[2, p.190] Consider a Gaussian channel with a real input x, and

signal to noise ratio v=(cid:27)2.

(a) What is its capacity C?

(b) If the input is constrained to be binary, x 2 f(cid:6)pvg, what is the

capacity C0 of this constrained channel?

(c) If in addition the output of the channel is thresholded using the

mapping

y ! y0 =(cid:26) 1

y > 0
0 y (cid:20) 0;

(11.35)

what is the capacity C00 of the resulting channel?

(d) Plot the three capacities above as a function of v=(cid:27) 2 from 0.1 to 2.

[You’ll need to do a numerical integral to evaluate C0.]

. Exercise 11.6.[3 ] For large integers K and N , what fraction of all binary error-
correcting codes of length N and rate R = K=N are linear codes? [The
answer will depend on whether you choose to de(cid:12)ne the code to be an
ordered list of 2K codewords, that is, a mapping from s 2 f1; 2; : : : ; 2Kg
to x(s), or to de(cid:12)ne the code to be an unordered list, so that two codes
consisting of the same codewords are identical. Use the latter de(cid:12)nition:
a code is a set of codewords; how the encoder operates is not part of the
de(cid:12)nition of the code.]

Erasure channels

. Exercise 11.7.[4 ] Design a code for the binary erasure channel, and a decoding
algorithm, and evaluate their probability of error.
[The design of good
codes for erasure channels is an active research area (Spielman, 1996;
Byers et al., 1998); see also Chapter 50.]

. Exercise 11.8.[5 ] Design a code for the q-ary erasure channel, whose input x is
drawn from 0; 1; 2; 3; : : : ; (q (cid:0) 1), and whose output y is equal to x with
probability (1 (cid:0) f ) and equal to ? otherwise. [This erasure channel is a
good model for packets transmitted over the internet, which are either
received reliably or are lost.]

Exercise 11.9.[3, p.190] How do redundant arrays of independent disks (RAID)
work? These are information storage systems consisting of about ten
disk drives, of which any two or three can be disabled and the others are
able to still able to reconstruct any requested (cid:12)le. What codes are used,
and how far are these systems from the Shannon limit for the problem
they are solving? How would you design a better RAID system? Some
information is provided in the solution section. See http://www.acnc.
com/raid2.html; see also Chapter 50.

[Some people say RAID stands for
‘redundant array of inexpensive
disks’, but I think that’s silly {
RAID would still be a good idea
even if the disks were expensive!]

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

11.10: Solutions

11.10 Solutions

Solution to exercise 11.1 (p.181).
Introduce a Lagrange multiplier (cid:21) for the
power constraint and another, (cid:22), for the constraint of normalization of P (x).

189

F = I(X; Y ) (cid:0) (cid:21)R dx P (x)x2 (cid:0) (cid:22)R dx P (x)

= Z dx P (x)(cid:20)Z dy P (y j x) ln

P (y) (cid:0) (cid:21)x2 (cid:0) (cid:22)(cid:21) :
P (y j x)

(11.36)

(11.37)

Make the functional derivative with respect to P (x(cid:3)).

(cid:14)F

(cid:14)P (x(cid:3))

= Z dy P (y j x(cid:3)) ln

P (y j x(cid:3))
P (y) (cid:0) (cid:21)x(cid:3)2 (cid:0) (cid:22)
(cid:0)Z dx P (x)Z dy P (y j x)
(cid:14)P (y)
(cid:14)P (x(cid:3))

P (y)

1

:

(11.38)

The (cid:12)nal factor (cid:14)P (y)=(cid:14)P (x(cid:3)) is found, using P (y) =R dx P (x)P (y j x), to be
P (y j x(cid:3)), and the whole of the last term collapses in a pu(cid:11) of smoke to 1,
which can be absorbed into the (cid:22) term.
Substitute P (y j x) = exp((cid:0)(y (cid:0) x)2=2(cid:27)2)=p2(cid:25)(cid:27)2 and set the derivative to

zero:

P (y j x)
P (y) (cid:0) (cid:21)x2 (cid:0) (cid:22)0 = 0

(11.39)

ln [P (y)(cid:27)] = (cid:0)(cid:21)x2 (cid:0) (cid:22)0 (cid:0)

This condition must be satis(cid:12)ed by ln[P (y)(cid:27)] for all x.

)Z dy
Writing a Taylor expansion of ln[P (y)(cid:27)] = a+by+cy 2+(cid:1)(cid:1)(cid:1), only a quadratic
function ln[P (y)(cid:27)] = a + cy2 would satisfy the constraint (11.40). (Any higher
order terms yp, p > 2, would produce terms in xp that are not present on
the right-hand side.) Therefore P (y) is Gaussian. We can obtain this optimal
output distribution by using a Gaussian input distribution P (x).

(11.40)

1
2

:

Z dy P (y j x) ln
exp((cid:0)(y (cid:0) x)2=2(cid:27)2)
p2(cid:25)(cid:27)2

Solution to exercise 11.2 (p.181). Given a Gaussian input distribution of vari-
ance v, the output distribution is Normal(0; v + (cid:27) 2), since x and the noise
are independent random variables, and variances add for independent random
variables. The mutual information is:

I(X; Y ) = Z dx dy P (x)P (y j x) log P (y j x) (cid:0)Z dy P (y) log P (y) (11.41)

=

=

1
2
1
2

log

1
(cid:27)2 (cid:0)
log(cid:16)1 +

1

v + (cid:27)2

1
2
v

log

(cid:27)2(cid:17) :

(11.42)

(11.43)

Solution to exercise 11.4 (p.186). The capacity of the channel is one minus
the information content of the noise that it adds. That information content is,
per chunk, the entropy of the selection of whether the chunk is bursty, H2(b),
plus, with probability b, the entropy of the (cid:13)ipped bits, N , which adds up
to H2(b) + N b per chunk (roughly; accurate if N is large). So, per bit, the
capacity is, for N = 100,

C = 1 (cid:0)(cid:18) 1

N

H2(b) + b(cid:19) = 1 (cid:0) 0:207 = 0:793:

(11.44)

In contrast, interleaving, which treats bursts of errors as independent, causes
the channel to be treated as a binary symmetric channel with f = 0:2 (cid:2) 0:5 =
0:1, whose capacity is about 0.53.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

190

11 | Error-Correcting Codes and Real Channels

Interleaving throws away the useful information about the correlated-
ness of the errors. Theoretically, we should be able to communicate about
(0:79=0:53) ’ 1:6 times faster using a code and decoder that explicitly treat
bursts as bursts.

Solution to exercise 11.5 (p.188).

1.2
1
0.8
0.6
0.4
0.2
0

0

0.5

1

1.5

2

2.5

1

0.1

0.01

0.1

1

Figure 11.9. Capacities (from top
to bottom in each graph) C, C 0,
and C 00, versus the signal-to-noise
ratio (pv=(cid:27)). The lower graph is
a log{log plot.

C =

1
2

log(cid:16)1 +

v

(cid:27)2(cid:17) :

(11.45)

(a) Putting together the results of exercises 11.1 and 11.2, we deduce that
a Gaussian channel with real input x, and signal to noise ratio v=(cid:27) 2 has
capacity

(b) If the input is constrained to be binary, x 2 f(cid:6)pvg, the capacity is

achieved by using these two inputs with equal probability. The capacity
is reduced to a somewhat messy integral,

C00 =Z 1

(cid:0)1

dy N (y; 0) log N (y; 0) (cid:0)Z 1

(cid:0)1

dy P (y) log P (y);

(11.46)

where N (y; x) (cid:17) (1=p2(cid:25)) exp[(y (cid:0) x)2=2], x (cid:17) pv=(cid:27), and P (y) (cid:17)
[N (y; x) + N (y;(cid:0)x)]=2. This capacity is smaller than the unconstrained
capacity (11.45), but for small signal-to-noise ratio, the two capacities
are close in value.

(c) If the output is thresholded, then the Gaussian channel is turned into
a binary symmetric channel whose transition probability is given by the
error function (cid:8) de(cid:12)ned on page 156. The capacity is

C00 = 1 (cid:0) H2(f ); where f = (cid:8)(pv=(cid:27)):

(11.47)

Solution to exercise 11.9 (p.188). There are several RAID systems. One of
the easiest to understand consists of 7 disk drives which store data at rate
4=7 using a (7; 4) Hamming code: each successive four bits are encoded with
the code and the seven codeword bits are written one to each disk. Two or
perhaps three disk drives can go down and the others can recover the data.
The e(cid:11)ective channel model here is a binary erasure channel, because it is
assumed that we can tell when a disk is dead.

It is not possible to recover the data for some choices of the three dead

disk drives; can you see why?

. Exercise 11.10.[2, p.190] Give an example of three disk drives that, if lost, lead
to failure of the above RAID system, and three that can be lost without
failure.

Solution to exercise 11.10 (p.190). The (7; 4) Hamming code has codewords
of weight 3. If any set of three disk drives corresponding to one of those code-
words is lost, then the other four disks can recover only 3 bits of information
about the four source bits; a fourth bit is lost. [cf. exercise 13.13 (p.220) with
q = 2: there are no binary MDS codes. This de(cid:12)cit is discussed further in
section 13.11.]

Any other set of three disk drives can be lost without problems because
the corresponding four by four submatrix of the generator matrix is invertible.
A better code would be a digital fountain { see Chapter 50.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part III

Further Topics in Information Theory

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 12

In Chapters 1{11, we concentrated on two aspects of information theory and
coding theory: source coding { the compression of information so as to make
e(cid:14)cient use of data transmission and storage channels; and channel coding {
the redundant encoding of information so as to be able to detect and correct
communication errors.

In both these areas we started by ignoring practical considerations, concen-
trating on the question of the theoretical limitations and possibilities of coding.
We then discussed practical source-coding and channel-coding schemes, shift-
ing the emphasis towards computational feasibility. But the prime criterion
for comparing encoding schemes remained the e(cid:14)ciency of the code in terms
of the channel resources it required: the best source codes were those that
achieved the greatest compression; the best channel codes were those that
communicated at the highest rate with a given probability of error.

In this chapter we now shift our viewpoint a little, thinking of ease of
information retrieval as a primary goal. It turns out that the random codes
which were theoretically useful in our study of channel coding are also useful
for rapid information retrieval.

E(cid:14)cient information retrieval is one of the problems that brains seem to
solve e(cid:11)ortlessly, and content-addressable memory is one of the topics we will
study when we look at neural networks.

192

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

string length
number of strings
number of possible

strings

N ’ 200
S ’ 223
2N ’ 2200

Figure 12.1. Cast of characters.

12

Hash Codes: Codes for E(cid:14)cient

Information Retrieval

12.1 The information-retrieval problem

A simple example of an information-retrieval problem is the task of imple-
menting a phone directory service, which, in response to a person’s name,
returns (a) a con(cid:12)rmation that that person is listed in the directory; and (b)
the person’s phone number and other details. We could formalize this prob-
lem as follows, with S being the number of names that must be stored in the
directory.

You are given a list of S binary strings of length N bits, fx(1); : : : ; x(S)g,
where S is considerably smaller than the total number of possible strings, 2N .
We will call the superscript ‘s’ in x(s) the record number of the string. The
idea is that s runs over customers in the order in which they are added to the
directory and x(s) is the name of customer s. We assume for simplicity that
all people have names of the same length. The name length might be, say,
N = 200 bits, and we might want to store the details of ten million customers,
so S ’ 107 ’ 223. We will ignore the possibility that two customers have
identical names.
The task is to construct the inverse of the mapping from s to x(s), i.e., to
make a system that, given a string x, returns the value of s such that x = x(s)
if one exists, and otherwise reports that no such s exists. (Once we have the
record number, we can go and look in memory location s in a separate memory
full of phone numbers to (cid:12)nd the required number.) The aim, when solving
this task, is to use minimal computational resources in terms of the amount
of memory used to store the inverse mapping from x to s and the amount of
time to compute the inverse mapping. And, preferably, the inverse mapping
should be implemented in such a way that further new strings can be added
to the directory in a small amount of computer time too.

Some standard solutions

The simplest and dumbest solutions to the information-retrieval problem are
a look-up table and a raw list.

The look-up table is a piece of memory of size 2N log2 S, log2 S being the
amount of memory required to store an integer between 1 and S.
In
each of the 2N locations, we put a zero, except for the locations x that
correspond to strings x(s), into which we write the value of s.

The look-up table is a simple and quick solution, but only if there is
su(cid:14)cient memory for the table, and if the cost of looking up entries in

193

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

194

12 | Hash Codes: Codes for E(cid:14)cient Information Retrieval

memory is independent of the memory size. But in our de(cid:12)nition of the
task, we assumed that N is about 200 bits or more, so the amount of
memory required would be of size 2200; this solution is completely out
of the question. Bear in mind that the number of particles in the solar
system is only about 2190.

The raw list is a simple list of ordered pairs (s; x(s)) ordered by the value
of s. The mapping from x to s is achieved by searching through the list
of strings, starting from the top, and comparing the incoming string x
with each record x(s) until a match is found. This system is very easy
to maintain, and uses a small amount of memory, about SN bits, but
is rather slow to use, since on average (cid:12)ve million pairwise comparisons
will be made.

. Exercise 12.1.[2, p.202] Show that the average time taken to (cid:12)nd the required
string in a raw list, assuming that the original names were chosen at
random, is about S + N binary comparisons.
(Note that you don’t
have to compare the whole string of length N , since a comparison can
be terminated as soon as a mismatch occurs; show that you need on
average two binary comparisons per incorrect string match.) Compare
this with the worst-case search time { assuming that the devil chooses
the set of strings and the search key.

The standard way in which phone directories are made improves on the look-up
table and the raw list by using an alphabetically-ordered list.
Alphabetical list. The strings fx(s)g are sorted into alphabetical order.
Searching for an entry now usually takes less time than was needed
for the raw list because we can take advantage of the sortedness; for
example, we can open the phonebook at its middle page, and compare
the name we (cid:12)nd there with the target string; if the target is ‘greater’
than the middle string then we know that the required string, if it exists,
will be found in the second half of the alphabetical directory. Otherwise,
we look in the (cid:12)rst half. By iterating this splitting-in-the-middle proce-
dure, we can identify the target string, or establish that the string is not
listed, in dlog2 Se string comparisons. The expected number of binary
comparisons per string comparison will tend to increase as the search
progresses, but the total number of binary comparisons required will be
no greater than dlog2 SeN .
The amount of memory required is the same as that required for the raw
list.

Adding new strings to the database requires that we insert them in the
correct location in the list. To (cid:12)nd that location takes about dlog 2 Se
binary comparisons.

Can we improve on the well-established alphabetized list? Let us consider

our task from some new viewpoints.

The task is to construct a mapping x ! s from N bits to log 2 S bits. This
is a pseudo-invertible mapping, since for any x that maps to a non-zero s, the
customer database contains the pair (s; x(s)) that takes us back. Where have
we come across the idea of mapping from N bits to M bits before?

We encountered this idea twice: (cid:12)rst, in source coding, we studied block
codes which were mappings from strings of N symbols to a selection of one
label in a list. The task of information retrieval is similar to the task (which

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

12.2: Hash codes

195

string length
number of strings
size of hash function M ’ 30 bits
size of hash table

N ’ 200
S ’ 223
T = 2M
’ 230

Figure 12.2. Revised cast of
characters.

we never actually solved) of making an encoder for a typical-set compression
code.

The second time that we mapped bit strings to bit strings of another
dimensionality was when we studied channel codes. There, we considered
codes that mapped from K bits to N bits, with N greater than K, and we
made theoretical progress using random codes.

In hash codes, we put together these two notions. We will study random

codes that map from N bits to M bits where M is smaller than N .

The idea is that we will map the original high-dimensional space down into
a lower-dimensional space, one in which it is feasible to implement the dumb
look-up table method which we rejected a moment ago.

12.2 Hash codes

First we will describe how a hash code works, then we will study the properties
of idealized hash codes. A hash code implements a solution to the information-
retrieval problem, that is, a mapping from x to s, with the help of a pseudo-
random function called a hash function, which maps the N -bit string x to an
M -bit string h(x), where M is smaller than N . M is typically chosen such that
the ‘table size’ T ’ 2M is a little bigger than S { say, ten times bigger. For
example, if we were expecting S to be about a million, we might map x into
a 30-bit hash h (regardless of the size N of each item x). The hash function
is some (cid:12)xed deterministic function which should ideally be indistinguishable
from a (cid:12)xed random code. For practical purposes, the hash function must be
quick to compute.

Two simple examples of hash functions are:

Division method. The table size T is a prime number, preferably one that
is not close to a power of 2. The hash value is the remainder when the
integer x is divided by T .

Variable string addition method. This method assumes that x is a string
of bytes and that the table size T is 256. The characters of x are added,
modulo 256. This hash function has the defect that it maps strings that
are anagrams of each other onto the same hash.

It may be improved by putting the running total through a (cid:12)xed pseu-
dorandom permutation after each character is added.
In the variable
string exclusive-or method with table size (cid:20) 65 536, the string is hashed
twice in this way, with the initial running total being set to 0 and 1
respectively (algorithm 12.3). The result is a 16-bit hash.

Having picked a hash function h(x), we implement an information retriever

as follows. (See (cid:12)gure 12.4.)

Encoding. A piece of memory called the hash table is created of size 2M b
memory units, where b is the amount of memory needed to represent an
integer between 0 and S. This table is initially set to zero throughout.
Each memory x(s) is put through the hash function, and at the location
in the hash table corresponding to the resulting vector h(s) = h(x(s)),
the integer s is written { unless that entry in the hash table is already
occupied, in which case we have a collision between x(s) and some earlier
x(s0) which both happen to have the same hash code. Collisions can be
handled in various ways { we will discuss some in a moment { but (cid:12)rst
let us complete the basic picture.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

196

12 | Hash Codes: Codes for E(cid:14)cient Information Retrieval

Algorithm 12.3. C code
implementing the variable string
exclusive-or method to create a
hash h in the range 0 : : : 65 535
from a string x. Author: Thomas
Niemann.

unsigned char Rand8[256];

// This array contains a random

int Hash(char *x) {

int h;
unsigned char h1, h2;

permutation from 0..255 to 0..255
// x is a pointer to the first char;
//

*x is the first character

if (*x == 0) return 0;
h1 = *x; h2 = *x + 1;
x++;
while (*x) {

// Special handling of empty string
// Initialize two hashes
// Proceed to the next character

h1 = Rand8[h1 ^ *x]; // Exclusive-or with the two hashes
h2 = Rand8[h2 ^ *x]; //
x++;

and put through the randomizer

}
h = ((int)(h1)<<8) |

// End of string is reached when *x=0
// Shift h1 left 8 bits and add h2

(int) h2 ;

return h ;

}

// Hash is concatenation of h1 and h2

Figure 12.4. Use of hash functions
for information retrieval. For each
string x(s), the hash h = h(x(s))
is computed, and the value of s is
written into the hth row of the
hash table. Blank rows in the
hash table contain the value zero.
The table size is T = 2M .

Strings

Hash
function-

hashes

Hash table

(cid:27) -M bits

(cid:27)

N bits

-

x(1)

x(2)

x(3)

...

x(s)

...

S

6

?

(cid:1)
@
(cid:1)

@

A
A

(cid:1)(cid:21)
(cid:1)

(cid:1)
(cid:1)

@

@R

@

@R

A
A

A
AU

h(x(2)) !

h(x(1)) !

h(x(3)) !

h(x(s)) !

2

1

3

s

6

2M

?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

12.3: Collision resolution

197

Decoding. To retrieve a piece of information corresponding to a target vector
x, we compute the hash h of x and look at the corresponding location
in the hash table. If there is a zero, then we know immediately that the
string x is not in the database. The cost of this answer is the cost of one
hash-function evaluation and one look-up in the table of size 2M . If, on
the other hand, there is a non-zero entry s in the table, there are two
possibilities: either the vector x is indeed equal to x(s); or the vector x(s)
is another vector that happens to have the same hash code as the target
x. (A third possibility is that this non-zero entry might have something
to do with our yet-to-be-discussed collision-resolution system.)
To check whether x is indeed equal to x(s), we take the tentative answer
s, look up x(s) in the original forward database, and compare it bit by
bit with x; if it matches then we report s as the desired answer. This
successful retrieval has an overall cost of one hash-function evaluation,
one look-up in the table of size 2M , another look-up in a table of size
S, and N binary comparisons { which may be much cheaper than the
simple solutions presented in section 12.1.

. Exercise 12.2.[2, p.202] If we have checked the (cid:12)rst few bits of x(s) with x and
found them to be equal, what is the probability that the correct entry
has been retrieved, if the alternative hypothesis is that x is actually not
in the database? Assume that the original source strings are random,
and the hash function is a random hash function. How many binary
evaluations are needed to be sure with odds of a billion to one that the
correct entry has been retrieved?

The hashing method of information retrieval can be used for strings x of
arbitrary length, if the hash function h(x) can be applied to strings of any
length.

12.3 Collision resolution

We will study two ways of resolving collisions: appending in the table, and
storing elsewhere.

Appending in table

When encoding, if a collision occurs, we continue down the hash table and
write the value of s into the next available location in memory that currently
contains a zero.
If we reach the bottom of the table before encountering a
zero, we continue from the top.

When decoding, if we compute the hash code for x and (cid:12)nd that the s
contained in the table doesn’t point to an x(s) that matches the cue x, we
continue down the hash table until we either (cid:12)nd an s whose x(s) does match
the cue x, in which case we are done, or else encounter a zero, in which case
we know that the cue x is not in the database.

For this method, it is essential that the table be substantially bigger in size
than S. If 2M < S then the encoding rule will become stuck with nowhere to
put the last strings.

Storing elsewhere

A more robust and (cid:13)exible method is to use pointers to additional pieces of
memory in which collided strings are stored. There are many ways of doing

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

198

12 | Hash Codes: Codes for E(cid:14)cient Information Retrieval

this. As an example, we could store in location h in the hash table a pointer
(which must be distinguishable from a valid record number s) to a ‘bucket’
where all the strings that have hash code h are stored in a sorted list. The
encoder sorts the strings in each bucket alphabetically as the hash table and
buckets are created.

The decoder simply has to go and look in the relevant bucket and then

check the short list of strings that are there by a brief alphabetical search.

This method of storing the strings in buckets allows the option of making
the hash table quite small, which may have practical bene(cid:12)ts. We may make it
so small that almost all strings are involved in collisions, so all buckets contain
a small number of strings. It only takes a small number of binary comparisons
to identify which of the strings in the bucket matches the cue x.

12.4 Planning for collisions: a birthday problem

Exercise 12.3.[2, p.202] If we wish to store S entries using a hash function whose
output has M bits, how many collisions should we expect to happen,
assuming that our hash function is an ideal random function? What
size M of hash table is needed if we would like the expected number of
collisions to be smaller than 1?

What size M of hash table is needed if we would like the expected number
of collisions to be a small fraction, say 1%, of S?

[Notice the similarity of this problem to exercise 9.20 (p.156).]

12.5 Other roles for hash codes

Checking arithmetic

If you wish to check an addition that was done by hand, you may (cid:12)nd useful
the method of casting out nines.
In casting out nines, one (cid:12)nds the sum,
modulo nine, of all the digits of the numbers to be summed and compares
it with the sum, modulo nine, of the digits of the putative answer.
[With a
little practice, these sums can be computed much more rapidly than the full
original addition.]

Example 12.4. In the calculation shown in the margin the sum, modulo nine, of
the digits in 189+1254+238 is 7, and the sum, modulo nine, of 1+6+8+1
is 7. The calculation thus passes the casting-out-nines test.

Casting out nines gives a simple example of a hash function. For any
addition expression of the form a + b + c + (cid:1)(cid:1)(cid:1), where a; b; c; : : : are decimal
numbers we de(cid:12)ne h 2 f0; 1; 2; 3; 4; 5; 6; 7; 8g by

h(a + b + c + (cid:1)(cid:1)(cid:1)) = sum modulo nine of all digits in a; b; c ;

(12.1)

then it is nice property of decimal arithmetic that if

a + b + c + (cid:1)(cid:1)(cid:1) = m + n + o + (cid:1)(cid:1)(cid:1)

(12.2)

then the hashes h(a + b + c + (cid:1)(cid:1)(cid:1)) and h(m + n + o + (cid:1)(cid:1)(cid:1)) are equal.

. Exercise 12.5.[1, p.203] What evidence does a correct casting-out-nines match
give in favour of the hypothesis that the addition has been done cor-
rectly?

189
+1254
+ 238
1681

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

199

12.5: Other roles for hash codes

Error detection among friends

Are two (cid:12)les the same? If the (cid:12)les are on the same computer, we could just
compare them bit by bit. But if the two (cid:12)les are on separate machines, it
would be nice to have a way of con(cid:12)rming that two (cid:12)les are identical without
having to transfer one of the (cid:12)les from A to B. [And even if we did transfer one
of the (cid:12)les, we would still like a way to con(cid:12)rm whether it has been received
without modi(cid:12)cations!]

This problem can be solved using hash codes. Let Alice and Bob be the
holders of the two (cid:12)les; Alice sent the (cid:12)le to Bob, and they wish to con(cid:12)rm
it has been received without error. If Alice computes the hash of her (cid:12)le and
sends it to Bob, and Bob computes the hash of his (cid:12)le, using the same M -bit
hash function, and the two hashes match, then Bob can deduce that the two
(cid:12)les are almost surely the same.

Example 12.6. What is the probability of a false negative, i.e., the probability,
given that the two (cid:12)les do di(cid:11)er, that the two hashes are nevertheless
identical?

If we assume that the hash function is random and that the process that causes
the (cid:12)les to di(cid:11)er knows nothing about the hash function, then the probability
of a false negative is 2(cid:0)M .
2

A 32-bit hash gives a probability of false negative of about 10(cid:0)10. It is
common practice to use a linear hash function called a 32-bit cyclic redundancy
check to detect errors in (cid:12)les. (A cyclic redundancy check is a set of 32 parity-
check bits similar to the 3 parity-check bits of the (7; 4) Hamming code.)

To have a false-negative rate smaller than one in a billion, M = 32
bits is plenty, if the errors are produced by noise.

. Exercise 12.7.[2, p.203] Such a simple parity-check code only detects errors; it
doesn’t help correct them. Since error-correcting codes exist, why not
use one of them to get some error-correcting capability too?

Tamper detection

What if the di(cid:11)erences between the two (cid:12)les are not simply ‘noise’, but are
introduced by an adversary, a clever forger called Fiona, who modi(cid:12)es the
original (cid:12)le to make a forgery that purports to be Alice’s (cid:12)le? How can Alice
make a digital signature for the (cid:12)le so that Bob can con(cid:12)rm that no-one has
tampered with the (cid:12)le? And how can we prevent Fiona from listening in on
Alice’s signature and attaching it to other (cid:12)les?

Let’s assume that Alice computes a hash function for the (cid:12)le and sends it
securely to Bob. If Alice computes a simple hash function for the (cid:12)le like the
linear cyclic redundancy check, and Fiona knows that this is the method of
verifying the (cid:12)le’s integrity, Fiona can make her chosen modi(cid:12)cations to the
(cid:12)le and then easily identify (by linear algebra) a further 32-or-so single bits
that, when (cid:13)ipped, restore the hash function of the (cid:12)le to its original value.
Linear hash functions give no security against forgers.

We must therefore require that the hash function be hard to invert so that
no-one can construct a tampering that leaves the hash function una(cid:11)ected.
We would still like the hash function to be easy to compute, however, so that
Bob doesn’t have to do hours of work to verify every (cid:12)le he received. Such
a hash function { easy to compute, but hard to invert { is called a one-way

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

200

12 | Hash Codes: Codes for E(cid:14)cient Information Retrieval

hash function. Finding such functions is one of the active research areas of
cryptography.

A hash function that is widely used in the free software community to
con(cid:12)rm that two (cid:12)les do not di(cid:11)er is MD5, which produces a 128-bit hash. The
details of how it works are quite complicated, involving convoluted exclusive-
or-ing and if-ing and and-ing.1

Even with a good one-way hash function, the digital signatures described
above are still vulnerable to attack, if Fiona has access to the hash function.
Fiona could take the tampered (cid:12)le and hunt for a further tiny modi(cid:12)cation to
it such that its hash matches the original hash of Alice’s (cid:12)le. This would take
some time { on average, about 232 attempts, if the hash function has 32 bits {
but eventually Fiona would (cid:12)nd a tampered (cid:12)le that matches the given hash.
To be secure against forgery, digital signatures must either have enough bits
for such a random search to take too long, or the hash function itself must be
kept secret.

Fiona has to hash 2M (cid:12)les to cheat. 232 (cid:12)le modi(cid:12)cations is not
very many, so a 32-bit hash function is not large enough for forgery
prevention.

Another person who might have a motivation for forgery is Alice herself.
For example, she might be making a bet on the outcome of a race, without
wishing to broadcast her prediction publicly; a method for placing bets would
be for her to send to Bob the bookie the hash of her bet. Later on, she could
send Bob the details of her bet. Everyone can con(cid:12)rm that her bet is consis-
tent with the previously publicized hash. [This method of secret publication
was used by Isaac Newton and Robert Hooke when they wished to establish
priority for scienti(cid:12)c ideas without revealing them. Hooke’s hash function
was alphabetization as illustrated by the conversion of UT TENSIO, SIC VIS
into the anagram CEIIINOSSSTTUV.] Such a protocol relies on the assumption
that Alice cannot change her bet after the event without the hash coming
out wrong. How big a hash function do we need to use to ensure that Alice
cannot cheat? The answer is di(cid:11)erent from the size of the hash we needed in
order to defeat Fiona above, because Alice is the author of both (cid:12)les. Alice
could cheat by searching for two (cid:12)les that have identical hashes to each other.
For example, if she’d like to cheat by placing two bets for the price of one,
she could make a large number N1 of versions of bet one (di(cid:11)ering from each
other in minor details only), and a large number N2 of versions of bet two, and
hash them all. If there’s a collision between the hashes of two bets of di(cid:11)erent
types, then she can submit the common hash and thus buy herself the option
of placing either bet.

Example 12.8. If the hash has M bits, how big do N1 and N2 need to be for
Alice to have a good chance of (cid:12)nding two di(cid:11)erent bets with the same
hash?

This is a birthday problem like exercise 9.20 (p.156). If there are N1 Montagues
and N2 Capulets at a party, and each is assigned a ‘birthday’ of M bits, the
expected number of collisions between a Montague and a Capulet is

N1N22(cid:0)M ;

(12.3)

1http://www.freesoft.org/CIE/RFC/1321/3.htm

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

12.6: Further exercises

201

so to minimize the number of (cid:12)les hashed, N1 + N2, Alice should make N1
and N2 equal, and will need to hash about 2M=2 (cid:12)les until she (cid:12)nds two that
match.
2

Alice has to hash 2M=2 (cid:12)les to cheat. [This is the square root of the
number of hashes Fiona had to make.]

If Alice has the use of C = 106 computers for T = 10 years, each computer
taking t = 1 ns to evaluate a hash, the bet-communication system is secure
against Alice’s dishonesty only if M (cid:29) 2 log2 CT =t ’ 160 bits.

Further reading

The Bible for hash codes is volume 3 of Knuth (1968). I highly recommend the
story of Doug McIlroy’s spell program, as told in section 13.8 of Programming
Pearls (Bentley, 2000). This astonishing piece of software makes use of a 64-
kilobyte data structure to store the spellings of all the words of 75 000-word
dictionary.

12.6 Further exercises

Exercise 12.9.[1 ] What is the shortest the address on a typical international
letter could be, if it is to get to a unique human recipient? (Assume
the permitted characters are [A-Z,0-9].) How long are typical email
addresses?

Exercise 12.10.[2, p.203] How long does a piece of text need to be for you to be
pretty sure that no human has written that string of characters before?
How many notes are there in a new melody that has not been composed
before?

. Exercise 12.11.[3, p.204] Pattern recognition by molecules.

Some proteins produced in a cell have a regulatory role. A regulatory
protein controls the transcription of speci(cid:12)c genes in the genome. This
control often involves the protein’s binding to a particular DNA sequence
in the vicinity of the regulated gene. The presence of the bound protein
either promotes or inhibits transcription of the gene.

(a) Use information-theoretic arguments to obtain a lower bound on
the size of a typical protein that acts as a regulator speci(cid:12)c to one
gene in the whole human genome. Assume that the genome is a
sequence of 3 (cid:2) 109 nucleotides drawn from a four letter alphabet
fA; C; G; Tg; a protein is a sequence of amino acids drawn from a
twenty letter alphabet.
[Hint: establish how long the recognized
DNA sequence has to be in order for that sequence to be unique
to the vicinity of one gene, treating the rest of the genome as a
random sequence. Then discuss how big the protein must be to
recognize a sequence of that length uniquely.]

(b) Some of the sequences recognized by DNA-binding regulatory pro-
teins consist of a subsequence that is repeated twice or more, for
example the sequence

GCCCCCCACCCCTGCCCCC

(12.4)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

202

12 | Hash Codes: Codes for E(cid:14)cient Information Retrieval

is a binding site found upstream of the alpha-actin gene in humans.
Does the fact that some binding sites consist of a repeated subse-
quence in(cid:13)uence your answer to part (a)?

12.7 Solutions

Solution to exercise 12.1 (p.194). First imagine comparing the string x with
another random string x(s). The probability that the (cid:12)rst bits of the two
strings match is 1=2. The probability that the second bits match is 1=2. As-
suming we stop comparing once we hit the (cid:12)rst mismatch, the expected number
of matches is 1, so the expected number of comparisons is 2 (exercise 2.34,
p.38).

Assuming the correct string is located at random in the raw list, we will
have to compare with an average of S=2 strings before we (cid:12)nd it, which costs
2S=2 binary comparisons; and comparing the correct strings takes N binary
comparisons, giving a total expectation of S + N binary comparisons, if the
strings are chosen at random.

In the worst case (which may indeed happen in practice), the other strings
are very similar to the search key, so that a lengthy sequence of comparisons
is needed to (cid:12)nd each mismatch. The worst case is when the correct string
is last in the list, and all the other strings di(cid:11)er in the last bit only, giving a
requirement of SN binary comparisons.

Solution to exercise 12.2 (p.197). The likelihood ratio for the two hypotheses,
H0: x(s) = x, and H1: x(s) 6= x, contributed by the datum ‘the (cid:12)rst bits of
x(s) and x are equal’ is

P (Datum jH0)
P (Datum jH1)

=

1
1=2

= 2:

(12.5)

If the (cid:12)rst r bits all match, the likelihood ratio is 2r to one. On (cid:12)nding that
30 bits match, the odds are a billion to one in favour of H0, assuming we start
from even odds.
[For a complete answer, we should compute the evidence
given by the prior information that the hash entry s has been found in the
table at h(x). This fact gives further evidence in favour of H0.]
Solution to exercise 12.3 (p.198).
Let the hash function have an output al-
phabet of size T = 2M . If M were equal to log2 S then we would have exactly
enough bits for each entry to have its own unique hash. The probability that
one particular pair of entries collide under a random hash function is 1=T . The
number of pairs is S(S (cid:0) 1)=2. So the expected number of collisions between
pairs is exactly
(12.6)

S(S (cid:0) 1)=(2T ):

If we would like this to be smaller than 1, then we need T > S(S (cid:0) 1)=2 so

M > 2 log2 S:

(12.7)

We need twice as many bits as the number of bits, log 2 S, that would be
su(cid:14)cient to give each entry a unique name.

If we are happy to have occasional collisions, involving a fraction f of the
names S, then we need T > S=f (since the probability that one particular
name is collided-with is f ’ S=T ) so

M > log2 S + log2[1=f ];

(12.8)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

12.7: Solutions

203

which means for f ’ 0:01 that we need an extra 7 bits above log2 S.
The important point to note is the scaling of T with S in the two cases
(12.7, 12.8). If we want the hash function to be collision-free, then we must
have T greater than (cid:24) S2.
If we are happy to have a small frequency of
collisions, then T needs to be of order S only.

Solution to exercise 12.5 (p.198). The posterior probability ratio for the two
hypotheses, H+ = ‘calculation correct’ and H(cid:0) = ‘calculation incorrect’ is
the product of the prior probability ratio P (H+)=P (H(cid:0)) and the likelihood
ratio, P (match jH+)=P (match jH(cid:0)). This second factor is the answer to the
question. The numerator P (match jH+) is equal to 1. The denominator’s
value depends on our model of errors. If we know that the human calculator is
prone to errors involving multiplication of the answer by 10, or to transposition
of adjacent digits, neither of which a(cid:11)ects the hash value, then P (match jH(cid:0))
could be equal to 1 also, so that the correct match gives no evidence in favour
of H+. But if we assume that errors are ‘random from the point of view of the
hash function’ then the probability of a false positive is P (match jH(cid:0)) = 1=9,
and the correct match gives evidence 9:1 in favour of H+.
Solution to exercise 12.7 (p.199).
If you add a tiny M = 32 extra bits of hash
to a huge N -bit (cid:12)le you get pretty good error detection { the probability that
an error is undetected is 2(cid:0)M , less than one in a billion. To do error correction
requires far more check bits, the number depending on the expected types of
corruption, and on the (cid:12)le size. For example, if just eight random bits in a

8 (cid:1) ’ 23 (cid:2) 8 ’ 180

megabyte (cid:12)le are corrupted, it would take about log2(cid:0)223

bits to specify which are the corrupted bits, and the number of parity-check
bits used by a successful error-correcting code would have to be at least this
number, by the counting argument of exercise 1.10 (solution, p.20).

Solution to exercise 12.10 (p.201). We want to know the length L of a string
such that it is very improbable that that string matches any part of the entire
writings of humanity. Let’s estimate that these writings total about one book
for each person living, and that each book contains two million characters (200
pages with 10 000 characters per page) { that’s 1016 characters, drawn from
an alphabet of, say, 37 characters.

The probability that a randomly chosen string of length L matches at one
point in the collected works of humanity is 1=37L. So the expected number
of matches is 1016=37L, which is vanishingly small if L (cid:21) 16= log 10 37 ’ 10.
Because of the redundancy and repetition of humanity’s writings, it is possible
that L ’ 10 is an overestimate.
So, if you want to write something unique, sit down and compose a string
of ten characters. But don’t write gidnebinzz, because I already thought of
that string.

As for a new melody, if we focus on the sequence of notes, ignoring duration
and stress, and allow leaps of up to an octave at each note, then the number
of choices per note is 23. The pitch of the (cid:12)rst note is arbitrary. The number
of melodies of length r notes in this rather ugly ensemble of Sch(cid:127)onbergian
tunes is 23r(cid:0)1; for example, there are 250 000 of length r = 5. Restricting
the permitted intervals will reduce this (cid:12)gure; including duration and stress
will increase it again. [If we restrict the permitted intervals to repetitions and
tones or semitones, the reduction is particularly severe; is this why the melody
of ‘Ode to Joy’ sounds so boring?] The number of recorded compositions is
probably less than a million. If you learn 100 new melodies per week for every
week of your life then you will have learned 250 000 melodies at age 50. Based

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

204

12 | Hash Codes: Codes for E(cid:14)cient Information Retrieval

In guess that tune, one player
chooses a melody, and sings a
gradually-increasing number of its
notes, while the other participants
try to guess the whole melody.

The Parsons code is a related hash
function for melodies: each pair of
consecutive notes is coded as U
(‘up’) if the second note is higher
than the (cid:12)rst, R (‘repeat’) if the
pitches are equal, and D (‘down’)
otherwise. You can (cid:12)nd out how
well this hash function works at
http://musipedia.org/.

on empirical experience of playing the game ‘guess that tune’, it seems to
me that whereas many four-note sequences are shared in common between
melodies, the number of collisions between (cid:12)ve-note sequences is rather smaller
{ most famous (cid:12)ve-note sequences are unique.

Solution to exercise 12.11 (p.201).
(a) Let the DNA-binding protein recognize
a sequence of length L nucleotides. That is, it binds preferentially to that
DNA sequence, and not to any other pieces of DNA in the whole genome. (In
reality, the recognized sequence may contain some wildcard characters, e.g.,
the * in TATAA*A, which denotes ‘any of A, C, G and T’; so, to be precise, we are
assuming that the recognized sequence contains L non-wildcard characters.)
Assuming the rest of the genome is ‘random’, i.e., that the sequence con-
sists of random nucleotides A, C, G and T with equal probability { which is
obviously untrue, but it shouldn’t make too much di(cid:11)erence to our calculation
{ the chance that there is no other occurrence of the target sequence in the
whole genome, of length N nucleotides, is roughly

(1 (cid:0) (1=4)L)N ’ exp((cid:0)N (1=4)L);

which is close to one only if

that is,

N 4(cid:0)L (cid:28) 1;

(12.9)

(12.10)

(12.11)
Using N = 3 (cid:2) 109, we require the recognized sequence to be longer than
Lmin = 16 nucleotides.

L > log N= log 4:

What size of protein does this imply?
(cid:15) A weak lower bound can be obtained by assuming that the information
content of the protein sequence itself is greater than the information
content of the nucleotide sequence the protein prefers to bind to (which
we have argued above must be at least 32 bits). This gives a minimum
protein length of 32= log2(20) ’ 7 amino acids.

(cid:15) Thinking realistically, the recognition of the DNA sequence by the pro-
tein presumably involves the protein coming into contact with all sixteen
nucleotides in the target sequence. If the protein is a monomer, it must
be big enough that it can simultaneously make contact with sixteen nu-
cleotides of DNA. One helical turn of DNA containing ten nucleotides
has a length of 3.4 nm, so a contiguous sequence of sixteen nucleotides
has a length of 5.4 nm. The diameter of the protein must therefore be
about 5.4 nm or greater. Egg-white lysozyme is a small globular protein
with a length of 129 amino acids and a diameter of about 4 nm. As-
suming that volume is proportional to sequence length and that volume
scales as the cube of the diameter, a protein of diameter 5.4 nm must
have a sequence of length 2:5 (cid:2) 129 ’ 324 amino acids.

(b) If, however, a target sequence consists of a twice-repeated sub-sequence, we
can get by with a much smaller protein that recognizes only the sub-sequence,
and that binds to the DNA strongly only if it can form a dimer, both halves
of which are bound to the recognized sequence. Halving the diameter of the
protein, we now only need a protein whose length is greater than 324/8 = 40
amino acids. A protein of length smaller than this cannot by itself serve as
a regulatory protein speci(cid:12)c to one gene, because it’s simply too small to be
able to make a su(cid:14)ciently speci(cid:12)c match { its available surface does not have
enough information content.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 13

In Chapters 8{11, we established Shannon’s noisy-channel coding theorem
for a general channel with any input and output alphabets. A great deal of
attention in coding theory focuses on the special case of channels with binary
inputs. Constraining ourselves to these channels simpli(cid:12)es matters, and leads
us into an exceptionally rich world, which we will only taste in this book.

One of the aims of this chapter is to point out a contrast between Shannon’s
aim of achieving reliable communication over a noisy channel and the apparent
aim of many in the world of coding theory. Many coding theorists take as
their fundamental problem the task of packing as many spheres as possible,
with radius as large as possible, into an N -dimensional space, with no spheres
overlapping. Prizes are awarded to people who (cid:12)nd packings that squeeze in an
extra few spheres. While this is a fascinating mathematical topic, we shall see
that the aim of maximizing the distance between codewords in a code has only
a tenuous relationship to Shannon’s aim of reliable communication.

205

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13

Binary Codes

We’ve established Shannon’s noisy-channel coding theorem for a general chan-
nel with any input and output alphabets. A great deal of attention in coding
theory focuses on the special case of channels with binary inputs, the (cid:12)rst
implicit choice being the binary symmetric channel.

The optimal decoder for a code, given a binary symmetric channel, (cid:12)nds
the codeword that is closest to the received vector, closest in Hamming dis-
tance. The Hamming distance between two binary vectors is the number of
coordinates in which the two vectors di(cid:11)er. Decoding errors will occur if the
noise takes us from the transmitted codeword t to a received vector r that
is closer to some other codeword. The distances between codewords are thus
relevant to the probability of a decoding error.

13.1 Distance properties of a code

The distance of a code is the smallest separation between two of its codewords.

Example 13.1. The (7; 4) Hamming code (p.8) has distance d = 3. All pairs of
its codewords di(cid:11)er in at least 3 bits. The maximum number of errors
it can correct is t = 1; in general a code with distance d is b(d(cid:0)1)=2c-
error-correcting.

A more precise term for distance is the minimum distance of the code. The

distance of a code is often denoted by d or dmin.

We’ll now constrain our attention to linear codes.

In a linear code, all
codewords have identical distance properties, so we can summarize all the
distances between the code’s codewords by counting the distances from the
all-zero codeword.

The weight enumerator function of a code, A(w),

is de(cid:12)ned to be the
number of codewords in the code that have weight w. The weight enumerator
function is also known as the distance distribution of the code.

Example 13.2. The weight enumerator functions of the (7; 4) Hamming code

and the dodecahedron code are shown in (cid:12)gures 13.1 and 13.2.

13.2 Obsession with distance

Since the maximum number of errors that a code can guarantee to correct,
t, is related to its distance d by t = b(d(cid:0)1)=2c, many coding theorists focus
on the distance of a code, searching for codes of a given size that have the
biggest possible distance. Much of practical coding theory has focused on
decoders that give the optimal decoding for all error patterns of weight up to
the half-distance t of their codes.

206

Example:

The Hamming distance
between
and

00001111
11001101

is 3.

w A(w)

0
3
4
7

1
7
7
1

Total

16

8

7

6

5

4

3

2

1

0

0

1

2

3

4

5

6

7

Figure 13.1. The graph of the
(7; 4) Hamming code, and its
weight enumerator function.

d = 2t + 1 if d is odd, and
d = 2t + 2 if d is even.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.2: Obsession with distance

207

w A(w)

0
5
8
9
10
11
12
13
14
15
16
17
18
19
20

1
12
30
20
72
120
100
180
240
272
345
300
200
120
36

Total

2048

350

300

250

200

150

100

50

0

100

10

1

0

5

8 10

15

20

25

30

0

5

8 10

15

20

25

30

Figure 13.2. The graph de(cid:12)ning
the (30; 11) dodecahedron code
(the circles are the 30 transmitted
bits and the triangles are the 20
parity checks, one of which is
redundant) and the weight
enumerator function (solid lines).
The dotted lines show the average
weight enumerator function of all
random linear codes with the
same size of generator matrix,
which will be computed shortly.
The lower (cid:12)gure shows the same
functions on a log scale.

A bounded-distance decoder is a decoder that returns the closest code-
word to a received binary vector r if the distance from r to that codeword
is less than or equal to t; otherwise it returns a failure message.

The rationale for not trying to decode when more than t errors have occurred
might be ‘we can’t guarantee that we can correct more than t errors, so we
won’t bother trying { who would be interested in a decoder that corrects some
error patterns of weight greater than t, but not others?’ This defeatist attitude
is an example of worst-case-ism, a widespread mental ailment which this book
is intended to cure.

The fact is that bounded-distance decoders cannot reach the Shannon limit (cid:3)

of the binary symmetric channel; only a decoder that often corrects more than
t errors can do this. The state of the art in error-correcting codes have decoders
that work way beyond the minimum distance of the code.

De(cid:12)nitions of good and bad distance properties

Given a family of codes of increasing blocklength N , and with rates approach-
ing a limit R > 0, we may be able to put that family in one of the following
categories, which have some similarities to the categories of ‘good’ and ‘bad’
codes de(cid:12)ned earlier (p.183):

A sequence of codes has ‘good’ distance if d=N tends to a constant

greater than zero.

A sequence of codes has ‘bad’ distance if d=N tends to zero.

A sequence of codes has ‘very bad’ distance if d tends to a constant.

Example 13.3. A low-density generator-matrix code is a linear code whose K(cid:2)
N generator matrix G has a small number d0 of 1s per row, regardless
of how big N is. The minimum distance of such a code is at most d0, so
low-density generator-matrix codes have ‘very bad’ distance.

While having large distance is no bad thing, we’ll see, later on, why an

emphasis on distance can be unhealthy.

Figure 13.3. The graph of a
rate-1/2 low-density
generator-matrix code. The
rightmost M of the transmitted
bits are each connected to a single
distinct parity constraint. The
leftmost K transmitted bits are
each connected to a small number
of parity constraints.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

208

13 | Binary Codes

Figure 13.4. Schematic picture of
part of Hamming space perfectly
(cid:12)lled by t-spheres centred on the
codewords of a perfect code.

t

t

t

. . .

1

2

13.3 Perfect codes

A t-sphere (or a sphere of radius t) in Hamming space, centred on a point x,
is the set of points whose Hamming distance from x is less than or equal to t.
The (7; 4) Hamming code has the beautiful property that if we place 1-
spheres about each of its 16 codewords, those spheres perfectly (cid:12)ll Hamming
space without overlapping. As we saw in Chapter 1, every binary vector of
length 7 is within a distance of t = 1 of exactly one codeword of the Hamming
code.

A code is a perfect t-error-correcting code if the set of t-spheres cen-
tred on the codewords of the code (cid:12)ll the Hamming space without over-
lapping. (See (cid:12)gure 13.4.)

Let’s recap our cast of characters. The number of codewords is S = 2K.
The number of points in the entire Hamming space is 2N . The number of
points in a Hamming sphere of radius t is

t

Xw=0(cid:18)N
w(cid:19):

(13.1)

For a code to be perfect with these parameters, we require S times the number
of points in the t-sphere to equal 2N :

for a perfect code, 2K

or, equivalently,

t

t

Xw=0(cid:18)N
Xw=0(cid:18)N

w(cid:19) = 2N
w(cid:19) = 2N(cid:0)K:

(13.2)

(13.3)

For a perfect code, the number of noise vectors in one sphere must equal
the number of possible syndromes. The (7; 4) Hamming code satis(cid:12)es this
numerological condition because

1 +(cid:18)7

1(cid:19) = 23:

(13.4)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.3: Perfect codes

t

. . .

1

2

t

. . .

1

2

t

. . .

1

2

t

. . .

1

2

209

Figure 13.5. Schematic picture of
Hamming space not perfectly
(cid:12)lled by t-spheres centred on the
codewords of a code. The grey
regions show points that are at a
Hamming distance of more than t
from any codeword. This is a
misleading picture, as, for any
code with large t in high
dimensions, the grey space
between the spheres takes up
almost all of Hamming space.

How happy we would be to use perfect codes

If there were large numbers of perfect codes to choose from, with a wide
range of blocklengths and rates, then these would be the perfect solution to
Shannon’s problem. We could communicate over a binary symmetric channel
with noise level f , for example, by picking a perfect t-error-correcting code
with blocklength N and t = f (cid:3)N , where f(cid:3) = f + (cid:14) and N and (cid:14) are chosen
such that the probability that the noise (cid:13)ips more than t bits is satisfactorily
small.

However, there are almost no perfect codes. The only nontrivial perfect (cid:3)

binary codes are

1. the Hamming codes, which are perfect codes with t = 1 and blocklength
N = 2M (cid:0) 1, de(cid:12)ned below; the rate of a Hamming code approaches 1
as its blocklength N increases;

2. the repetition codes of odd blocklength N , which are perfect codes with

t = (N (cid:0) 1)=2; the rate of repetition codes goes to zero as 1=N ; and

3. one remarkable 3-error-correcting code with 212 codewords of block-
length N = 23 known as the binary Golay code.
[A second 2-error-
correcting Golay code of length N = 11 over a ternary alphabet was dis-
covered by a Finnish football-pool enthusiast called Juhani Virtakallio
in 1947.]

There are no other binary perfect codes. Why this shortage of perfect codes?
Is it because precise numerological coincidences like those satis(cid:12)ed by the
parameters of the Hamming code (13.4) and the Golay code,

1 +(cid:18)23

1(cid:19) +(cid:18)23

2(cid:19) +(cid:18)23

3(cid:19) = 211;

(13.5)

are rare? Are there plenty of ‘almost-perfect’ codes for which the t-spheres (cid:12)ll
almost the whole space?

No.

In fact, the picture of Hamming spheres centred on the codewords
almost (cid:12)lling Hamming space ((cid:12)gure 13.5) is a misleading one: for most codes,
whether they are good codes or bad codes, almost all the Hamming space is
taken up by the space between t-spheres (which is shown in grey in (cid:12)gure 13.5).

Having established this gloomy picture, we spend a moment (cid:12)lling in the

properties of the perfect codes mentioned above.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

210

13 | Binary Codes

Figure 13.6. Three codewords.

00000
0
00000
100000
0000
0
1 1 1 1
0000
1 1 1

wN

xN

0

00000
11
00000 1

0

1 1 1 1

1 1 1

1

00000

00000
1 1 1 1

0
0
1 1 1 1
11

1

1 1 1 1

1 1 1 1

uN

vN

N

The Hamming codes

The (7; 4) Hamming code can be de(cid:12)ned as the linear code whose 3(cid:2) 7 parity-
check matrix contains, as its columns, all the 7 (= 23 (cid:0) 1) non-zero vectors of
length 3. Since these 7 vectors are all di(cid:11)erent, any single bit-(cid:13)ip produces a
distinct syndrome, so all single-bit errors can be detected and corrected.

We can generalize this code, with M = 3 parity constraints, as follows. The
Hamming codes are single-error-correcting codes de(cid:12)ned by picking a number
of parity-check constraints, M ; the blocklength N is N = 2M (cid:0) 1; the parity-
check matrix contains, as its columns, all the N non-zero vectors of length M
bits.

The (cid:12)rst few Hamming codes have the following rates:

Checks, M (N; K) R = K=N

2
3
4
5
6

(3, 1)
(7, 4)
(15, 11)
(31, 26)
(63, 57)

1/3
4/7
11/15
26/31
57/63

repetition code R3
(7; 4) Hamming code

Exercise 13.4.[2, p.223] What is the probability of block error of the (N; K)
Hamming code to leading order, when the code is used for a binary
symmetric channel with noise density f ?

13.4 Perfectness is unattainable { (cid:12)rst proof

We will show in several ways that useful perfect codes do not exist (here,
‘useful’ means ‘having large blocklength N , and rate close neither to 0 nor 1’).
Shannon proved that, given a binary symmetric channel with any noise
level f , there exist codes with large blocklength N and rate as close as you
like to C(f ) = 1 (cid:0) H2(f ) that enable communication with arbitrarily small
error probability. For large N , the number of errors per block will typically be
about fN , so these codes of Shannon are ‘almost-certainly-fN -error-correcting’
codes.

Let’s pick the special case of a noisy channel with f 2 (1=3; 1=2). Can
we (cid:12)nd a large perfect code that is fN -error-correcting? Well, let’s suppose
that such a code has been found, and examine just three of its codewords.
(Remember that the code ought to have rate R ’ 1(cid:0) H2(f ), so it should have
an enormous number (2N R) of codewords.) Without loss of generality, we
choose one of the codewords to be the all-zero codeword and de(cid:12)ne the other
two to have overlaps with it as shown in (cid:12)gure 13.6. The second codeword
di(cid:11)ers from the (cid:12)rst in a fraction u + v of its coordinates. The third codeword
di(cid:11)ers from the (cid:12)rst in a fraction v + w, and from the second in a fraction
u + w. A fraction x of the coordinates have value zero in all three codewords.
Now, if the code is fN -error-correcting, its minimum distance must be greater

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.5: Weight enumerator function of random linear codes

211

than 2fN , so

u + v > 2f; v + w > 2f; and u + w > 2f:

(13.6)

Summing these three inequalities and dividing by two, we have

u + v + w > 3f:

(13.7)

So if f > 1=3, we can deduce u + v + w > 1, so that x < 0, which is impossible.
Such a code cannot exist. So the code cannot have three codewords, let alone
2N R.

We conclude that, whereas Shannon proved there are plenty of codes for
communicating over a binary symmetric channel with f > 1=3, there are no
perfect codes that can do this.

We now study a more general argument that indicates that there are no
large perfect linear codes for general rates (other than 0 and 1). We do this
by (cid:12)nding the typical distance of a random linear code.

13.5 Weight enumerator function of random linear codes

Imagine making a code by picking the binary entries in the M(cid:2)N parity-check
matrix H at random. What weight enumerator function should we expect?
The weight enumerator of one particular code with parity-check matrix H,

A(w)H, is the number of codewords of weight w, which can be written

A(w)H = Xx:jxj=w

 [Hx = 0] ;

(13.8)

where the sum is over all vectors x whose weight is w and the truth function
 [Hx = 0] equals one if Hx = 0 and zero otherwise.

We can (cid:12)nd the expected value of A(w),

N

1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0
0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0
1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0
0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0
0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0
1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0
1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0
1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0
1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1
0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0
0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0
1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1

M

Figure 13.7. A random binary
parity-check matrix.

hA(w)i = XH

P (H)A(w)H

= Xx:jxj=wXH

P (H)  [Hx = 0] ;

(13.9)

(13.10)

by evaluating the probability that a particular word of weight w > 0 is a
codeword of the code (averaging over all binary linear codes in our ensemble).
By symmetry, this probability depends only on the weight w of the word, not
on the details of the word. The probability that the entire syndrome Hx is
zero can be found by multiplying together the probabilities that each of the
M bits in the syndrome is zero. Each bit zm of the syndrome is a sum (mod
2) of w random bits, so the probability that zm = 0 is 1/2. The probability that
Hx = 0 is thus

P (H)  [Hx = 0] = (1/2)M = 2(cid:0)M ;

(13.11)

XH

independent of w.

The expected number of words of weight w (13.10) is given by summing,
over all words of weight w, the probability that each word is a codeword. The

number of words of weight w is (cid:0)N
hA(w)i =(cid:18)N

w(cid:1), so
w(cid:19)2(cid:0)M for any w > 0:

(13.12)

 
 






































	




































Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

212

13 | Binary Codes

6e+52

5e+52

4e+52

3e+52

2e+52

1e+52

0

0

100

200

300

400

500

1e+60
1e+40
1e+20
1
1e-20
1e-40
1e-60
1e-80
1e-100
1e-120

0

100

200

300

400

500

Figure 13.8. The expected weight
enumerator function hA(w)i of a
random linear code with N = 540
and M = 360. Lower (cid:12)gure shows
hA(w)i on a logarithmic scale.

1

0.5

0

0

Capacity
R_GV

0.25

0.5

f

Figure 13.9. Contrast between
Shannon’s channel capacity C and
the Gilbert rate RGV { the
maximum communication rate
achievable using a
bounded-distance decoder, as a
function of noise level f . For any
given rate, R, the maximum
tolerable noise level for Shannon
is twice as big as the maximum
tolerable noise level for a
‘worst-case-ist’ who uses a
bounded-distance decoder.

For large N , we can use log(cid:0)N

w(cid:1) ’ N H2(w=N ) and R ’ 1 (cid:0) M=N to write

log2hA(w)i ’ N H2(w=N ) (cid:0) M

’ N [H2(w=N ) (cid:0) (1 (cid:0) R)] for any w > 0:

(13.13)

(13.14)

As a concrete example, (cid:12)gure 13.8 shows the expected weight enumerator
function of a rate-1=3 random linear code with N = 540 and M = 360.

Gilbert{Varshamov distance
For weights w such that H2(w=N ) < (1 (cid:0) R), the expectation of A(w) is
smaller than 1; for weights such that H2(w=N ) > (1 (cid:0) R), the expectation is
greater than 1. We thus expect, for large N , that the minimum distance of a
random linear code will be close to the distance dGV de(cid:12)ned by

H2(dGV=N ) = (1 (cid:0) R):

(13.15)

De(cid:12)nition. This distance, dGV (cid:17) N H(cid:0)1
distance for rate R and blocklength N .

2 (1 (cid:0) R), is the Gilbert{Varshamov
The Gilbert{Varshamov conjecture, widely believed, asserts that (for large
N ) it is not possible to create binary codes with minimum distance signi(cid:12)cantly
greater than dGV.

De(cid:12)nition. The Gilbert{Varshamov rate RGV is the maximum rate at which
you can reliably communicate with a bounded-distance decoder (as de(cid:12)ned on
p.207), assuming that the Gilbert{Varshamov conjecture is true.

Why sphere-packing is a bad perspective, and an obsession with distance
is inappropriate

If one uses a bounded-distance decoder, the maximum tolerable noise level
will (cid:13)ip a fraction fbd = 1
2 dmin=N of the bits. So, assuming dmin is equal to
the Gilbert distance dGV (13.15), we have:

H2(2fbd) = (1 (cid:0) RGV):
RGV = 1 (cid:0) H2(2fbd):

(13.16)

(13.17)

Now, here’s the crunch: what did Shannon say is achievable? He said the
maximum possible rate of communication is the capacity,

C = 1 (cid:0) H2(f ):

(13.18)

So for a given rate R, the maximum tolerable noise level, according to Shannon,
is given by

H2(f ) = (1 (cid:0) R):

(13.19)

Our conclusion:
imagine a good code of rate R has been chosen; equations
(13.16) and (13.19) respectively de(cid:12)ne the maximum noise levels tolerable by
a bounded-distance decoder, fbd, and by Shannon’s decoder, f .

fbd = f =2:

(13.20)

Bounded-distance decoders can only ever cope with half the noise-level that
Shannon proved is tolerable!

How does this relate to perfect codes? A code is perfect if there are t-
spheres around its codewords that (cid:12)ll Hamming space without overlapping.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.6: Berlekamp’s bats

213

Figure 13.10. Two overlapping
spheres whose radius is almost as
big as the distance between their
centres.

But when a typical random linear code is used to communicate over a bi-
nary symmetric channel near to the Shannon limit, the typical number of bits
(cid:13)ipped is fN , and the minimum distance between codewords is also fN , or
a little bigger, if we are a little below the Shannon limit. So the fN -spheres
around the codewords overlap with each other su(cid:14)ciently that each sphere
almost contains the centre of its nearest neighbour!
The reason why this
overlap is not disastrous is because, in high dimensions, the volume associated
with the overlap, shown shaded in (cid:12)gure 13.10, is a tiny fraction of either
sphere, so the probability of landing in it is extremely small.

The moral of the story is that worst-case-ism can be bad for you, halving
your ability to tolerate noise. You have to be able to decode way beyond the
minimum distance of a code to get to the Shannon limit!

Nevertheless, the minimum distance of a code is of interest in practice,
because, under some conditions, the minimum distance dominates the errors
made by a code.

13.6 Berlekamp’s bats

A blind bat lives in a cave. It (cid:13)ies about the centre of the cave, which corre-
sponds to one codeword, with its typical distance from the centre controlled
by a friskiness parameter f . (The displacement of the bat from the centre
corresponds to the noise vector.) The boundaries of the cave are made up of
stalactites that point in towards the centre of the cave ((cid:12)gure 13.11). Each
stalactite is analogous to the boundary between the home codeword and an-
other codeword. The stalactite is like the shaded region in (cid:12)gure 13.10, but
reshaped to convey the idea that it is a region of very small volume.

Decoding errors correspond to the bat’s intended trajectory passing inside
a stalactite. Collisions with stalactites at various distances from the centre
are possible.

If the friskiness is very small, the bat is usually very close to the centre
of the cave; collisions will be rare, and when they do occur, they will usually
involve the stalactites whose tips are closest to the centre point. Similarly,
under low-noise conditions, decoding errors will be rare, and they will typi-
cally involve low-weight codewords. Under low-noise conditions, the minimum
distance of a code is relevant to the (very small) probability of error.

Figure 13.11. Berlekamp’s
schematic picture of Hamming
space in the vicinity of a
codeword. The jagged solid line
encloses all points to which this
codeword is the closest. The
t-sphere around the codeword
takes up a small fraction of this
space.

t

. . .

1

2

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

214

13 | Binary Codes

If the friskiness is higher, the bat may often make excursions beyond the
safe distance t where the longest stalactites start, but it will collide most fre-
quently with more distant stalactites, owing to their greater number. There’s
only a tiny number of stalactites at the minimum distance, so they are rela-
tively unlikely to cause the errors. Similarly, errors in a real error-correcting
code depend on the properties of the weight enumerator function.

At very high friskiness, the bat is always a long way from the centre of
the cave, and almost all its collisions involve contact with distant stalactites.
Under these conditions, the bat’s collision frequency has nothing to do with
the distance from the centre to the closest stalactite.

13.7 Concatenation of Hamming codes

It is instructive to play some more with the concatenation of Hamming codes,
a concept we (cid:12)rst visited in (cid:12)gure 11.6, because we will get insights into the
notion of good codes and the relevance or otherwise of the minimum distance
of a code.

We can create a concatenated code for a binary symmetric channel with

noise density f by encoding with several Hamming codes in succession.

The table recaps the key properties of the Hamming codes, indexed by
number of constraints, M . All the Hamming codes have minimum distance
d = 3 and can correct one error in N .

blocklength

N = 2M (cid:0) 1
K = N (cid:0) M number of source bits
2(cid:1)f 2
N(cid:0)N
pB = 3

probability of block error to leading order

If we make a product code by concatenating a sequence of C Hamming
c=1 in such a

codes with increasing M , we can choose those parameters fMcgC
way that the rate of the product code

RC =

Nc (cid:0) Mc

Nc

C

Yc=1

(13.21)

tends to a non-zero limit as C increases. For example, if we set M1 = 2,
M2 = 3, M3 = 4, etc., then the asymptotic rate is 0.093 ((cid:12)gure 13.12).

The blocklength N is a rapidly-growing function of C, so these codes are
somewhat impractical. A further weakness of these codes is that their min-
imum distance is not very good ((cid:12)gure 13.13). Every one of the constituent
Hamming codes has minimum distance 3, so the minimum distance of the
Cth product is 3C. The blocklength N grows faster than 3C, so the ratio d=N
tends to zero as C increases. In contrast, for typical random codes, the ratio
d=N tends to a constant such that H2(d=N ) = 1(cid:0) R. Concatenated Hamming
codes thus have ‘bad’ distance.
Nevertheless, it turns out that this simple sequence of codes yields good
codes for some channels { but not very good codes (see section 11.4 to recall
the de(cid:12)nitions of the terms ‘good’ and ‘very good’). Rather than prove this
result, we will simply explore it numerically.

Figure 13.14 shows the bit error probability pb of the concatenated codes
assuming that the constituent codes are decoded in sequence, as described
in section 11.4.
[This one-code-at-a-time decoding is suboptimal, as we saw
there.] The horizontal axis shows the rates of the codes. As the number
of concatenations increases, the rate drops to 0.093 and the error probability
drops towards zero. The channel assumed in the (cid:12)gure is the binary symmetric

R

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8

10

12

C

Figure 13.12. The rate R of the
concatenated Hamming code as a
function of the number of
concatenations, C.

1e+25

1e+20

1e+15

1e+10

100000

1

0

2

4

6

8

10

12

C

Figure 13.13. The blocklength NC
(upper curve) and minimum
distance dC (lower curve) of the
concatenated Hamming code as a
function of the number of
concatenations C.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.8: Distance isn’t everything

215

channel with f = 0:0588. This is the highest noise level that can be tolerated
using this concatenated code.

The take-home message from this story is distance isn’t everything. The
minimum distance of a code, although widely worshipped by coding theorists,
is not of fundamental importance to Shannon’s mission of achieving reliable
communication over noisy channels.

pb

1
0.01
0.0001
1e-06
1e-08
1e-10
1e-12
1e-14

N=3

21

315

61525

10^13

0

0.2

0.4

0.6

R

0.8

1

Figure 13.14. The bit error
probabilities versus the rates R of
the concatenated Hamming codes,
for the binary symmetric channel
with f = 0:0588. Labels alongside
the points show the blocklengths,
N . The solid line shows the
Shannon limit for this channel.
The bit error probability drops to
zero while the rate tends to 0.093,
so the concatenated Hamming
codes are a ‘good’ code family.

d=10
d=20
d=30
d=40
d=50
d=60

1

1e-05

1e-10

1e-15

1e-20

0.0001

0.001

0.01

0.1

Figure 13.15. The error
probability associated with a
single codeword of weight d,

(cid:0) d
d=2(cid:1)f d=2(1 (cid:0) f )d=2, as a function

of f .

. Exercise 13.5.[3 ] Prove that there exist families of codes with ‘bad’ distance

that are ‘very good’ codes.

13.8 Distance isn’t everything

Let’s get a quantitative feeling for the e(cid:11)ect of the minimum distance of a
code, for the special case of a binary symmetric channel.

The error probability associated with one low-weight codeword

Let a binary code have blocklength N and just two codewords, which di(cid:11)er in
d places. For simplicity, let’s assume d is even. What is the error probability
if this code is used on a binary symmetric channel with noise level f ?

Bit (cid:13)ips matter only in places where the two codewords di(cid:11)er. The error
probability is dominated by the probability that d=2 of these bits are (cid:13)ipped.
What happens to the other bits is irrelevant, since the optimal decoder ignores
them.

P (block error) ’ (cid:18) d

d=2(cid:19)f d=2(1 (cid:0) f )d=2:

(13.22)

This error probability associated with a single codeword of weight d is plotted
in (cid:12)gure 13.15. Using the approximation for the binomial coe(cid:14)cient (1.16),
we can further approximate

P (block error) ’ h2f 1=2(1 (cid:0) f )1=2id

(cid:17) [(cid:12)(f )]d;

(13.23)

(13.24)

where (cid:12)(f ) = 2f 1=2(1 (cid:0) f )1=2 is called the Bhattacharyya parameter of the
channel.
Now, consider a general linear code with distance d. Its block error prob-
ability must be at least (cid:0) d
d=2(cid:1)f d=2(1 (cid:0) f )d=2, independent of the blocklength

N of the code. For this reason, a sequence of codes of increasing blocklength
N and constant distance d (i.e., ‘very bad’ distance) cannot have a block er-
ror probability that tends to zero, on any binary symmetric channel. If we
are interested in making superb error-correcting codes with tiny, tiny error
probability, we might therefore shun codes with bad distance. However, being
pragmatic, we should look more carefully at (cid:12)gure 13.15.
In Chapter 1 we
argued that codes for disk drives need an error probability smaller than about
10(cid:0)18. If the raw error probability in the disk drive is about 0:001, the error
probability associated with one codeword at distance d = 20 is smaller than
10(cid:0)24. If the raw error probability in the disk drive is about 0:01, the error
probability associated with one codeword at distance d = 30 is smaller than
10(cid:0)20. For practical purposes, therefore, it is not essential for a code to have
good distance. For example, codes of blocklength 10 000, known to have many
codewords of weight 32, can nevertheless correct errors of weight 320 with tiny
error probability.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

216

13 | Binary Codes

I wouldn’t want you to think I am recommending the use of codes with
bad distance; in Chapter 47 we will discuss low-density parity-check codes, my
favourite codes, which have both excellent performance and good distance.

13.9 The union bound

The error probability of a code on the binary symmetric channel can be
bounded in terms of its weight enumerator function by adding up appropriate
multiples of the error probability associated with a single codeword (13.24):

P (block error) (cid:20) Xw>0

A(w)[(cid:12)(f )]w:

(13.25)

This inequality, which is an example of a union bound, is accurate for low
noise levels f , but inaccurate for high noise levels, because it overcounts the
contribution of errors that cause confusion with more than one codeword at a
time.

. Exercise 13.6.[3 ] Poor man’s noisy-channel coding theorem.

Pretending that the union bound (13.25) is accurate, and using the aver-
age weight enumerator function of a random linear code (13.14) (section
13.5) as A(w), estimate the maximum rate RUB(f ) at which one can
communicate over a binary symmetric channel.

Or, to look at it more positively, using the union bound (13.25) as an
inequality, show that communication at rates up to RUB(f ) is possible
over the binary symmetric channel.

In the following chapter, by analysing the probability of error of syndrome
decoding for a binary linear code, and using a union bound, we will prove
Shannon’s noisy-channel coding theorem (for symmetric binary channels), and
thus show that very good linear codes exist.

13.10 Dual codes

A concept that has some importance in coding theory, though we will have
no immediate use for it in this book, is the idea of the dual of a linear error-
correcting code.

An (N; K) linear error-correcting code can be thought of as a set of 2K
codewords generated by adding together all combinations of K independent
basis codewords. The generator matrix of the code consists of those K basis
codewords, conventionally written as row vectors. For example, the (7; 4)
Hamming code’s generator matrix (from p.10) is

3
775

(13.26)

G =2
664

1 0 0 0 1 0 1
0 1 0 0 1 1 0
0 0 1 0 1 1 1
0 0 0 1 0 1 1

and its sixteen codewords were displayed in table 1.14 (p.9). The code-
words of this code are linear combinations of the four vectors [1 0 0 0 1 0 1],
[0 1 0 0 1 1 0], [0 0 1 0 1 1 1], and [0 0 0 1 0 1 1].

An (N; K) code may also be described in terms of an M (cid:2) N parity-check

matrix (where M = N (cid:0) K) as the set of vectors ftg that satisfy

Ht = 0:

(13.27)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.10: Dual codes

217

One way of thinking of this equation is that each row of H speci(cid:12)es a vector
to which t must be orthogonal if it is a codeword.

The generator matrix speci(cid:12)es K vectors from which all codewords
can be built, and the parity-check matrix speci(cid:12)es a set of M vectors
to which all codewords are orthogonal.

The dual of a code is obtained by exchanging the generator matrix
and the parity-check matrix.

De(cid:12)nition. The set of all vectors of length N that are orthogonal to all code-
words in a code, C, is called the dual of the code, C?.

If t is orthogonal to h1 and h2, then it is also orthogonal to h3 (cid:17) h1 + h2;
so all codewords are orthogonal to any linear combination of the M rows of
H. So the set of all linear combinations of the rows of the parity-check matrix
is the dual code.

For our Hamming (7; 4) code, the parity-check matrix is (from p.12):

H =(cid:2) P I3 (cid:3) =2
4

1 1 1 0 1 0 0
0 1 1 1 0 1 0
1 0 1 1 0 0 1

3
5 :

(13.28)

The dual of the (7; 4) Hamming code H(7;4) is the code shown in table 13.16.

0000000
0010111

0101101
0111010

1001110
1011001

1100011
1110100

A possibly unexpected property of this pair of codes is that the dual,
H?(7;4), is contained within the code H(7;4) itself: every word in the dual code
is a codeword of the original (7; 4) Hamming code. This relationship can be
written using set notation:

H?(7;4) (cid:26) H(7;4):

(13.29)

The possibility that the set of dual vectors can overlap the set of codeword
vectors is counterintuitive if we think of the vectors as real vectors { how can
a vector be orthogonal to itself? But when we work in modulo-two arithmetic,
many non-zero vectors are indeed orthogonal to themselves!

. Exercise 13.7.[1, p.223] Give a simple rule that distinguishes whether a binary
vector is orthogonal to itself, as is each of the three vectors [1 1 1 0 1 0 0],
[0 1 1 1 0 1 0], and [1 0 1 1 0 0 1].

Some more duals

In general, if a code has a systematic generator matrix,

G = [IKjPT] ;

where P is a K (cid:2) M matrix, then its parity-check matrix is

H = [PjIM ] :

(13.30)

(13.31)

Table 13.16. The eight codewords
of the dual of the (7; 4) Hamming
code. [Compare with table 1.14,
p.9.]

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

218

13 | Binary Codes

Example 13.8. The repetition code R3 has generator matrix

its parity-check matrix is

G =(cid:2) 1 1 1 (cid:3) ;
1 0 1 (cid:21) :
H =(cid:20) 1 1 0

The two codewords are [1 1 1] and [0 0 0].

The dual code has generator matrix

G? = H =(cid:20) 1 1 0
1 0 1 (cid:21)

(13.32)

(13.33)

(13.34)

or equivalently, modifying G? into systematic form by row additions,

G? =(cid:20) 1 0 1
0 1 1 (cid:21) :

(13.35)

We call this dual code the simple parity code P3; it is the code with one
parity-check bit, which is equal to the sum of the two source bits. The
dual code’s four codewords are [1 1 0], [1 0 1], [0 0 0], and [0 1 1].

In this case, the only vector common to the code and the dual is the
all-zero codeword.

Goodness of duals

If a sequence of codes is ‘good’, are their duals good too? Examples can be
constructed of all cases: good codes with good duals (random linear codes);
bad codes with bad duals; and good codes with bad duals. The last category
is especially important: many state-of-the-art codes have the property that
their duals are bad. The classic example is the low-density parity-check code,
whose dual is a low-density generator-matrix code.

. Exercise 13.9.[3 ] Show that low-density generator-matrix codes are bad. A
family of low-density generator-matrix codes is de(cid:12)ned by two param-
eters j; k, which are the column weight and row weight of all rows and
columns respectively of G. These weights are (cid:12)xed, independent of N ;
for example, (j; k) = (3; 6).
[Hint: show that the code has low-weight
codewords, then use the argument from p.215.]

Exercise 13.10.[5 ] Show that low-density parity-check codes are good, and have
good distance. (For solutions, see Gallager (1963) and MacKay (1999b).)

Self-dual codes

The (7; 4) Hamming code had the property that the dual was contained in the
code itself. A code is self-orthogonal if it is contained in its dual. For example,
the dual of the (7; 4) Hamming code is a self-orthogonal code. One way of
seeing this is that the overlap between any pair of rows of H is even. Codes that
contain their duals are important in quantum error-correction (Calderbank
and Shor, 1996).

It is intriguing, though not necessarily useful, to look at codes that are
self-dual. A code C is self-dual if the dual of the code is identical to the code.
(13.36)

C? = C:

Some properties of self-dual codes can be deduced:

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.11: Generalizing perfectness to other channels

219

1. If a code is self-dual, then its generator matrix is also a parity-check

matrix for the code.

2. Self-dual codes have rate 1=2, i.e., M = K = N=2.

3. All codewords have even weight.

. Exercise 13.11.[2, p.223] What property must the matrix P satisfy, if the code

with generator matrix G = [IKjPT] is self-dual?

Examples of self-dual codes

1. The repetition code R2 is a simple example of a self-dual code.

2. The smallest non-trivial self-dual code is the following (8; 4) code.

G = H =(cid:2) 1 1 (cid:3) :

(13.37)

G =(cid:2) I4 PT (cid:3) =2
664

1 0 0 0 0 1 1 1
0 1 0 0 1 0 1 1
0 0 1 0 1 1 0 1
0 0 0 1 1 1 1 0

:

3
775

(13.38)

. Exercise 13.12.[2, p.223] Find the relationship of the above (8; 4) code to the

(7; 4) Hamming code.

Duals and graphs

Let a code be represented by a graph in which there are nodes of two types,
parity-check constraints and equality constraints, joined by edges which rep-
resent the bits of the code (not all of which need be transmitted).

The dual code’s graph is obtained by replacing all parity-check nodes by
equality nodes and vice versa. This type of graph is called a normal graph by
Forney (2001).

Further reading

Duals are important in coding theory because functions involving a code (such
as the posterior distribution over codewords) can be transformed by a Fourier
transform into functions over the dual code. For an accessible introduction
to Fourier analysis on (cid:12)nite groups, see Terras (1999). See also MacWilliams
and Sloane (1977).

13.11 Generalizing perfectness to other channels

Having given up on the search for perfect codes for the binary symmetric
channel, we could console ourselves by changing channel. We could call a
code ‘a perfect u-error-correcting code for the binary erasure channel’ if it
can restore any u erased bits, and never more than u. Rather than using the
word perfect, however, the conventional term for such a code is a ‘maximum
distance separable code’, or MDS code.

As we already noted in exercise 11.10 (p.190), the (7; 4) Hamming code is
not an MDS code. It can recover some sets of 3 erased bits, but not all. If
any 3 bits corresponding to a codeword of weight 3 are erased, then one bit of
information is unrecoverable. This is why the (7; 4) code is a poor choice for
a RAID system.

In a perfect u-error-correcting
code for the binary erasure
channel, the number of redundant
bits must be N (cid:0) K = u.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

220

13 | Binary Codes

A tiny example of a maximum distance separable code is the simple parity-
check code P3 whose parity-check matrix is H = [1 1 1]. This code has 4
codewords, all of which have even parity. All codewords are separated by
a distance of 2. Any single erased bit can be restored by setting it to the
parity of the other two bits. The repetition codes are also maximum distance
separable codes.

. Exercise 13.13.[5, p.224] Can you make an (N; K) code, with M = N (cid:0) K

parity symbols, for a q-ary erasure channel, such that the decoder can
recover the codeword when any M symbols are erased in a block of N ?
[Example: for the channel with q = 4 symbols there is an (N; K) = (5; 2)
code which can correct any M = 3 erasures.]

For the q-ary erasure channel with q > 2, there are large numbers of MDS
codes, of which the Reed{Solomon codes are the most famous and most widely
used. As long as the (cid:12)eld size q is bigger than the blocklength N , MDS block
codes of any rate can be found. (For further reading, see Lin and Costello
(1983).)

13.12 Summary

Shannon’s codes for the binary symmetric channel can almost always correct
fN errors, but they are not fN -error-correcting codes.

Reasons why the distance of a code has little relevance

1. The Shannon limit shows that the best codes must be able to cope with
a noise level twice as big as the maximum noise level for a bounded-
distance decoder.

2. When the binary symmetric channel has f > 1=4, no code with a
bounded-distance decoder can communicate at all; but Shannon says
good codes exist for such channels.

3. Concatenation shows that we can get good performance even if the dis-

tance is bad.

The whole weight enumerator function is relevant to the question of

whether a code is a good code.

The relationship between good codes and distance properties is discussed

further in exercise 13.14 (p.220).

13.13 Further exercises

Exercise 13.14.[3, p.224] A codeword t is selected from a linear (N; K) code
C, and it is transmitted over a noisy channel; the received signal is y.
We assume that the channel is a memoryless channel such as a Gaus-
sian channel. Given an assumed channel model P (y j t), there are two
decoding problems.

The codeword decoding problem is the task of

inferring which

codeword t was transmitted given the received signal.

The bitwise decoding problem is the task of

inferring for each
transmitted bit tn how likely it is that that bit was a one rather
than a zero.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.13: Further exercises

221

Consider optimal decoders for these two decoding problems. Prove that
the probability of error of the optimal bitwise-decoder is closely related
to the probability of error of the optimal codeword-decoder, by proving
the following theorem.

Theorem 13.1 If a binary linear code has minimum distance dmin,
then, for any given channel, the codeword bit error probability of the
optimal bitwise decoder, pb, and the block error probability of the maxi-
mum likelihood decoder, pB, are related by:

pB (cid:21) pb (cid:21)

1
2

dmin
N

pB:

(13.39)

Exercise 13.15.[1 ] What are the minimum distances of the (15; 11) Hamming

code and the (31; 26) Hamming code?

. Exercise 13.16.[2 ] Let A(w) be the average weight enumerator function of a
rate-1=3 random linear code with N = 540 and M = 360. Estimate,
from (cid:12)rst principles, the value of A(w) at w = 1.

Exercise 13.17.[3C ] A code with minimum distance greater than dGV. A rather
nice (15; 5) code is generated by this generator matrix, which is based

on measuring the parities of all the (cid:0)5
G =2
66664

(cid:1)
1 1 1
(cid:1)
(cid:1)
(cid:1)
1
1 1

(cid:1)
1
(cid:1)
(cid:1)
(cid:1)

3(cid:1) = 10 triplets of source bits:
(cid:1)
1 1
(cid:1)
(cid:1)

(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
1 1 1 1
(cid:1)
(cid:1)
(cid:1)
1
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
1 1
1 1 1 1

(cid:1)
1
(cid:1)
1 1
(cid:1)
1 1
(cid:1)
1
1
(cid:1)
(cid:1)
1

3
77775

(cid:1)
(cid:1)
1
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
1
(cid:1)

1
(cid:1)
(cid:1)
(cid:1)
(cid:1)

Find the minimum distance and weight enumerator function of this code.

:

(13.40)

Exercise 13.18.[3C ] Find the minimum distance of the ‘pentagonful’

low-

density parity-check code whose parity-check matrix is

H =

:

(13.41)

2

666666666666664

(cid:1)
(cid:1)
(cid:1)
1 1
1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1
(cid:1)
(cid:1)

(cid:1)
(cid:1)
1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1

(cid:1)
(cid:1)
(cid:1)
1
(cid:1)
(cid:1)
1
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
1
(cid:1)
(cid:1)
(cid:1)
1
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1
1
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
1 1
(cid:1)
(cid:1)
(cid:1)
1 1

3

777777777777775

Show that nine of the ten rows are independent, so the code has param-
eters N = 15, K = 6. Using a computer, (cid:12)nd its weight enumerator
function.

. Exercise 13.19.[3C ] Replicate the calculations used to produce (cid:12)gure 13.12.
Check the assertion that the highest noise level that’s correctable is
0.0588. Explore alternative concatenated sequences of codes. Can you
(cid:12)nd a better sequence of concatenated codes { better in the sense that it
has either higher asymptotic rate R or can tolerate a higher noise level
f ?

Figure 13.17. The graph of the
pentagonful low-density
parity-check code with 15 bit
nodes (circles) and 10 parity-check
nodes (triangles). [This graph is
known as the Petersen graph.]

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

222

13 | Binary Codes

Exercise 13.20.[3, p.226] Investigate the possibility of achieving the Shannon
limit with linear block codes, using the following counting argument.
Assume a linear code of large blocklength N and rate R = K=N . The
code’s parity-check matrix H has M = N (cid:0) K rows. Assume that the
code’s optimal decoder, which solves the syndrome decoding problem
Hn = z, allows reliable communication over a binary symmetric channel
with (cid:13)ip probability f .

How many ‘typical’ noise vectors n are there?

Roughly how many distinct syndromes z are there?

Since n is reliably deduced from z by the optimal decoder, the number
of syndromes must be greater than or equal to the number of typical
noise vectors. What does this tell you about the largest possible value
of rate R for a given f ?

. Exercise 13.21.[2 ] Linear binary codes use the input symbols 0 and 1 with
equal probability, implicitly treating the channel as a symmetric chan-
nel. Investigate how much loss in communication rate is caused by this
assumption, if in fact the channel is a highly asymmetric channel. Take
as an example a Z-channel. How much smaller is the maximum possible
rate of communication using symmetric inputs than the capacity of the
channel? [Answer: about 6%.]

Exercise 13.22.[2 ] Show that codes with ‘very bad’ distance are ‘bad’ codes, as

de(cid:12)ned in section 11.4 (p.183).

Exercise 13.23.[3 ] One linear code can be obtained from another by punctur-
ing. Puncturing means taking each codeword and deleting a de(cid:12)ned set
of bits. Puncturing turns an (N; K) code into an (N 0; K) code, where
N0 < N .

Another way to make new linear codes from old is shortening. Shortening
means constraining a de(cid:12)ned set of bits to be zero, and then deleting
them from the codewords. Typically if we shorten by one bit, half of the
code’s codewords are lost. Shortening typically turns an (N; K) code
into an (N0; K0) code, where N (cid:0) N0 = K (cid:0) K0.
Another way to make a new linear code from two old ones is to make
the intersection of the two codes: a codeword is only retained in the new
code if it is present in both of the two old codes.

Discuss the e(cid:11)ect on a code’s distance-properties of puncturing, short-
ening, and intersection.
Is it possible to turn a code family with bad
distance into a code family with good distance, or vice versa, by each of
these three manipulations?

Exercise 13.24.[3, p.226] Todd Ebert’s ‘hat puzzle’.

Three players enter a room and a red or blue hat is placed on each
person’s head. The colour of each hat is determined by a coin toss, with
the outcome of one coin toss having no e(cid:11)ect on the others. Each person
can see the other players’ hats but not his own.

No communication of any sort is allowed, except for an initial strategy
session before the group enters the room. Once they have had a chance
to look at the other hats, the players must simultaneously guess their

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.14: Solutions

223

If you already know the hat
puzzle, you could try the ‘Scottish
version’ of the rules in which the
prize is only awarded to the group
if they all guess correctly.

In the ‘Reformed Scottish
version’, all the players must
guess correctly, and there are two
rounds of guessing. Those players
who guess during round one leave
the room. The remaining players
must guess in round two. What
strategy should the team adopt to
maximize their chance of winning?

own hat’s colour or pass. The group shares a $3 million prize if at least
one player guesses correctly and no players guess incorrectly.

The same game can be played with any number of players. The general
problem is to (cid:12)nd a strategy for the group that maximizes its chances of
winning the prize. Find the best strategies for groups of size three and
seven.

[Hint: when you’ve done three and seven, you might be able to solve
(cid:12)fteen.]

Exercise 13.25.[5 ] Estimate how many binary low-density parity-check codes
have self-orthogonal duals. [Note that we don’t expect a huge number,
since almost all low-density parity-check codes are ‘good’, but a low-
density parity-check code that contains its dual must be ‘bad’.]

Exercise 13.26.[2C ] In (cid:12)gure 13.15 we plotted the error probability associated
with a single codeword of weight d as a function of the noise level f of a
binary symmetric channel. Make an equivalent plot for the case of the
Gaussian channel, showing the error probability associated with a single
codeword of weight d as a function of the rate-compensated signal-to-
noise ratio Eb=N0. Because Eb=N0 depends on the rate, you have to
choose a code rate. Choose R = 1=2, 2=3, 3=4, or 5=6.

13.14 Solutions

Solution to exercise 13.4 (p.210). The probability of block error to leading
order is pB = 3

Solution to exercise 13.7 (p.217). A binary vector is perpendicular to itself if
it has even weight, i.e., an even number of 1s.

N(cid:0)N
2(cid:1)f 2.

Solution to exercise 13.11 (p.219).
The self-dual code has two equivalent
parity-check matrices, H1 = G = [IKjPT] and H2 = [PjIK]; these must be
equivalent to each other through row additions, that is, there is a matrix U
such that UH2 = H1, so

[UPjUIK ] = [IKjPT] :

(13.42)

From the right-hand sides of this equation, we have U = PT, so the left-hand
sides become:

(13.43)
Thus if a code with generator matrix G = [IKjPT] is self-dual then P is an
orthogonal matrix, modulo 2, and vice versa.

PTP = IK:

Solution to exercise 13.12 (p.219). The (8; 4) and (7; 4) codes are intimately
related. The (8; 4) code, whose parity-check matrix is

H =(cid:2) P I4 (cid:3) =2
664

0 1 1 1 1 0 0 0
1 0 1 1 0 1 0 0
1 1 0 1 0 0 1 0
1 1 1 0 0 0 0 1

;

(13.44)

3
775

is obtained by (a) appending an extra parity-check bit which can be thought
of as the parity of all seven bits of the (7; 4) Hamming code; and (b) reordering
the (cid:12)rst four bits.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

224

13 | Binary Codes

If an (N; K) code, with M = N (cid:0) K parity
Solution to exercise 13.13 (p.220).
symbols, has the property that the decoder can recover the codeword when any
M symbols are erased in a block of N , then the code is said to be maximum
distance separable (MDS).

No MDS binary codes exist, apart from the repetition codes and simple

parity codes. For q > 2, some MDS codes can be found.

As a simple example, here is a (9; 2) code for the 8-ary erasure channel.
The code is de(cid:12)ned in terms of the multiplication and addition rules of GF (8),
which are given in Appendix C.1. The elements of the input alphabet are
f0; 1; A; B; C; D; E; Fg and the generator matrix of the code is
1 (cid:21) :
G =(cid:20) 1 0 1 A B C D E F

0 1 1

(13.45)

1

1

1

1

1

The resulting 64 codewords are:

000000000
101ABCDEF
A0ACEB1FD
B0BEDFC1A
C0CBFEAD1
D0D1CAFBE
E0EF1DBAC
F0FDA1ECB

011111111
110BADCFE
A1BDFA0EC
B1AFCED0B
C1DAEFBC0
D1C0DBEAF
E1FE0CABD
F1ECB0FDA

0AAAAAAAA
1AB01EFCD
AA0EC1BDF
BA1CFDEB0
CAE1DC0FB
DAFBE0D1C
EACDBF10E
FADF0BCE1

0BBBBBBBB
1BA10FEDC
AB1FD0ACE
BB0DECFA1
CBF0CD1EA
DBEAF1C0D
EBDCAE01F
FBCE1ADF0

0CCCCCCCC
1CDEF01AB
ACE0AFDB1
BCFA1B0DE
CC0FBAE1D
DC1D0EBFA
ECABD1FE0
FCB1EDA0F

0DDDDDDDD
1DCFE10BA
ADF1BECA0
BDEB0A1CF
CD1EABF0C
DD0C1FAEB
EDBAC0EF1
FDA0FCB1E

0EEEEEEEE
1EFCDAB01
AECA0DF1B
BED0B1AFC
CEAD10CBF
DEBFAC1D0
EE01FBDCA
FE1BCF0AD

0FFFFFFFF
1FEDCBA10
AFDB1CE0A
BFC1A0BED
CFBC01DAE
DFAEBD0C1
EF10EACDB
FF0ADE1BC

Solution to exercise 13.14 (p.220). Quick, rough proof of the theorem. Let x
denote the di(cid:11)erence between the reconstructed codeword and the transmitted
codeword. For any given channel output r, there is a posterior distribution
over x. This posterior distribution is positive only on vectors x belonging
to the code; the sums that follow are over codewords x. The block error
probability is:

pB =Xx6=0

P (xj r):

(13.46)

The average bit error probability, averaging over all bits in the codeword, is:

pb =Xx6=0

P (xj r)

w(x)

N

;

(13.47)

where w(x) is the weight of codeword x. Now the weights of the non-zero
codewords satisfy

w(x)
N (cid:21)

dmin
N

:

1 (cid:21)

(13.48)

Substituting the inequalities (13.48) into the de(cid:12)nitions (13.46, 13.47), we ob-
tain:

pB (cid:21) pb (cid:21)

dmin
N

pB;

(13.49)

which is a factor of two stronger, on the right, than the stated result (13.39).
In making the proof watertight, I have weakened the result a little.

Careful proof. The theorem relates the performance of the optimal block de-
coding algorithm and the optimal bitwise decoding algorithm.

We introduce another pair of decoding algorithms, called the block-
guessing decoder and the bit-guessing decoder. The idea is that these two
algorithms are similar to the optimal block decoder and the optimal bitwise
decoder, but lend themselves more easily to analysis.

We now de(cid:12)ne these decoders. Let x denote the inferred codeword. For

any given code:

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.14: Solutions

225

The optimal block decoder returns the codeword x that maximizes the
posterior probability P (xj r), which is proportional to the likelihood
P (rj x).
The probability of error of this decoder is called pB.

The optimal bit decoder returns

the
value of a that maximizes the posterior probability P (xn = aj r) =

the N bits, xn,

for each of

Px P (xj r)  [xn = a].

The probability of error of this decoder is called pb.

The block-guessing decoder returns a random codeword x with probabil-

ity distribution given by the posterior probability P (xj r).
The probability of error of this decoder is called pG
B.

The bit-guessing decoder returns for each of the N bits, xn, a random bit

from the probability distribution P (xn = aj r).
The probability of error of this decoder is called pG
b .

The theorem states that the optimal bit error probability pb is bounded above
by pB and below by a given multiple of pB (13.39).

The left-hand inequality in (13.39) is trivially true { if a block is correct, all
its constituent bits are correct; so if the optimal block decoder outperformed
the optimal bit decoder, we could make a better bit decoder from the block
decoder.

We prove the right-hand inequality by establishing that:

(a) the bit-guessing decoder is nearly as good as the optimal bit decoder:

pG
b (cid:20) 2pb:

(13.50)

(b) the bit-guessing decoder’s error probability is related to the block-

guessing decoder’s by

Then since pG

B (cid:21) pB, we have
1
2

pb >

pG
b (cid:21)

pG
b (cid:21)

dmin
N

pG
B:

1
2

dmin
N

pG
B (cid:21)

1
2

dmin
N

pB:

(13.51)

(13.52)

We now prove the two lemmas.

Near-optimality of guessing: Consider (cid:12)rst the case of a single bit, with posterior
probability fp0; p1g. The optimal bit decoder has probability of error

P optimal = min(p0; p1):

(13.53)

The guessing decoder picks from 0 and 1. The truth is also distributed with
the same probability. The probability that the guesser and the truth match is
p2
0 + p2
1; the probability that they mismatch is the guessing error probability,

P guess = 2p0p1 (cid:20) 2 min(p0; p1) = 2P optimal:

(13.54)

Since pG
b is the average of many such error probabilities, P guess, and pb is the
average of the corresponding optimal error probabilities, P optimal, we obtain
the desired relationship (13.50) between pG
2

b and pb.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

226

13 | Binary Codes

Relationship between bit error probability and block error probability: The bit-
guessing and block-guessing decoders can be combined in a single system: we
can draw a sample xn from the marginal distribution P (xn j r) by drawing
a sample (xn; x) from the joint distribution P (xn; xj r), then discarding the
value of x.
We can distinguish between two cases: the discarded value of x is the
correct codeword, or not. The probability of bit error for the bit-guessing
decoder can then be written as a sum of two terms:

pG
b = P (x correct)P (bit errorj x correct)

+ P (x incorrect)P (bit errorj x incorrect)

= 0 + pG

BP (bit errorj x incorrect):

Now, whenever the guessed x is incorrect, the true x must di(cid:11)er from it in at
least d bits, so the probability of bit error in these cases is at least d=N . So

pG
b (cid:21)

d
N

pG
B:

QED.

2

Solution to exercise 13.20 (p.222). The number of ‘typical’ noise vectors n
is roughly 2N H2(f ). The number of distinct syndromes z is 2M . So reliable
communication implies

or, in terms of the rate R = 1 (cid:0) M=N ,

M (cid:21) N H2(f );

R (cid:20) 1 (cid:0) H2(f );

(13.55)

(13.56)

a bound which agrees precisely with the capacity of the channel.
This argument is turned into a proof in the following chapter.

Solution to exercise 13.24 (p.222).
the group to win three-quarters of the time.

In the three-player case, it is possible for

Three-quarters of the time, two of the players will have hats of the same
colour and the third player’s hat will be the opposite colour. The group can
win every time this happens by using the following strategy. Each player looks
at the other two players’ hats. If the two hats are di(cid:11)erent colours, he passes.
If they are the same colour, the player guesses his own hat is the opposite
colour.

This way, every time the hat colours are distributed two and one, one
player will guess correctly and the others will pass, and the group will win the
game. When all the hats are the same colour, however, all three players will
guess incorrectly and the group will lose.

When any particular player guesses a colour, it is true that there is only a
50:50 chance that their guess is right. The reason that the group wins 75% of
the time is that their strategy ensures that when players are guessing wrong,
a great many are guessing wrong.

For larger numbers of players, the aim is to ensure that most of the time
no one is wrong and occasionally everyone is wrong at once. In the game with
7 players, there is a strategy for which the group wins 7 out of every 8 times
they play. In the game with 15 players, the group can win 15 out of 16 times.
If you have not (cid:12)gured out these winning strategies for teams of 7 and 15,
I recommend thinking about the solution to the three-player game in terms

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

13.14: Solutions

227

of the locations of the winning and losing states on the three-dimensional
hypercube, then thinking laterally.

If the number of players, N , is 2r (cid:0) 1, the optimal strategy can be de(cid:12)ned
using a Hamming code of length N , and the probability of winning the prize
is N=(N + 1). Each player is identi(cid:12)ed with a number n 2 1 : : : N . The two
colours are mapped onto 0 and 1. Any state of their hats can be viewed as a
received vector out of a binary channel. A random binary vector of length N
is either a codeword of the Hamming code, with probability 1=(N + 1), or it
di(cid:11)ers in exactly one bit from a codeword. Each player looks at all the other
bits and considers whether his bit can be set to a colour such that the state is
a codeword (which can be deduced using the decoder of the Hamming code).
If it can, then the player guesses that his hat is the other colour. If the state is
actually a codeword, all players will guess and will guess wrong. If the state is
a non-codeword, only one player will guess, and his guess will be correct. It’s
quite easy to train seven players to follow the optimal strategy if the cyclic
representation of the (7; 4) Hamming code is used (p.19).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 14

In this chapter we will draw together several ideas that we’ve encountered
so far in one nice short proof. We will simultaneously prove both Shannon’s
noisy-channel coding theorem (for symmetric binary channels) and his source
coding theorem (for binary sources). While this proof has connections to many
preceding chapters in the book, it’s not essential to have read them all.

On the noisy-channel coding side, our proof will be more constructive than
the proof given in Chapter 10; there, we proved that almost any random code
is ‘very good’. Here we will show that almost any linear code is very good. We
will make use of the idea of typical sets (Chapters 4 and 10), and we’ll borrow
from the previous chapter’s calculation of the weight enumerator function of
random linear codes (section 13.5).

On the source coding side, our proof will show that random linear hash
functions can be used for compression of compressible binary sources, thus
giving a link to Chapter 12.

228

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

14

Very Good Linear Codes Exist

In this chapter we’ll use a single calculation to prove simultaneously the source
coding theorem and the noisy-channel coding theorem for the binary symmet-
ric channel.

Incidentally, this proof works for much more general channel models, not
only the binary symmetric channel. For example, the proof can be reworked
for channels with non-binary outputs, for time-varying channels and for chan-
nels with memory, as long as they have binary inputs satisfying a symmetry
property, cf. section 10.6.

14.1 A simultaneous proof of the source coding and noisy-channel

coding theorems

We consider a linear error-correcting code with binary parity-check matrix H.
The matrix has M rows and N columns. Later in the proof we will increase
N and M , keeping M / N . The rate of the code satis(cid:12)es

R (cid:21) 1 (cid:0)

M
N

:

(14.1)

If all the rows of H are independent then this is an equality, R = 1 (cid:0) M=N .
In what follows, we’ll assume the equality holds. Eager readers may work out
the expected rank of a random binary matrix H (it’s very close to M ) and
pursue the e(cid:11)ect that the di(cid:11)erence (M (cid:0) rank) has on the rest of this proof
(it’s negligible).

A codeword t is selected, satisfying

Ht = 0 mod 2;

(14.2)

and a binary symmetric channel adds noise x, giving the received signal

r = t + x mod 2:

(14.3)

The receiver aims to infer both t and x from r using a syndrome-decoding
approach. Syndrome decoding was (cid:12)rst introduced in section 1.2 (p.10 and
11). The receiver computes the syndrome

z = Hr mod 2 = Ht + Hx mod 2 = Hx mod 2:

(14.4)

The syndrome only depends on the noise x, and the decoding problem is to
(cid:12)nd the most probable x that satis(cid:12)es

Hx = z mod 2:

(14.5)

229

In this chapter x denotes the
noise added by the channel, not
the input to the channel.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

230

14 | Very Good Linear Codes Exist

This best estimate for the noise vector, ^x, is then subtracted from r to give the
best guess for t. Our aim is to show that, as long as R < 1(cid:0)H(X) = 1(cid:0)H2(f ),
where f is the (cid:13)ip probability of the binary symmetric channel, the optimal
decoder for this syndrome-decoding problem has vanishing probability of error,
as N increases, for random H.

We prove this result by studying a sub-optimal strategy for solving the
decoding problem. Neither the optimal decoder nor this typical-set decoder
would be easy to implement, but the typical-set decoder is easier to analyze.
The typical-set decoder examines the typical set T of noise vectors, the set
of noise vectors x0 that satisfy log 1/P (x0) ’ N H(X), checking to see if any of We’ll leave out the (cid:15)s and (cid:12)s that
those typical vectors x0 satis(cid:12)es the observed syndrome,

make a typical-set de(cid:12)nition
rigorous. Enthusiasts are
encouraged to revisit section 4.4
and put these details into this
proof.

Hx0 = z:

(14.6)

If exactly one typical vector x0 does so, the typical set decoder reports that
vector as the hypothesized noise vector.
If no typical vector matches the
observed syndrome, or more than one does, then the typical set decoder reports
an error.

The probability of error of the typical-set decoder, for a given matrix H,

can be written as a sum of two terms,

PTSjH = P (I) + P (II)
TSjH;

(14.7)

where P (I) is the probability that the true noise vector x is itself not typical,
and P (II)
TSjH is the probability that the true x is typical and at least one other
typical vector clashes with it. The (cid:12)rst probability vanishes as N increases, as
we proved when we (cid:12)rst studied typical sets (Chapter 4). We concentrate on
the second probability. To recap, we’re imagining a true noise vector, x; and
if any of the typical noise vectors x0, di(cid:11)erent from x, satis(cid:12)es H(x0 (cid:0) x) = 0,
then we have an error. We use the truth function

 (cid:2)H(x0 (cid:0) x) = 0(cid:3) ;

(14.8)

whose value is one if the statement H(x0 (cid:0) x) = 0 is true and zero otherwise.
We can bound the number of type II errors made when the noise is x thus:

[Number of errors given x and H] (cid:20) Xx0: x0 2 T

x0 6= x

 (cid:2)H(x0 (cid:0) x) = 0(cid:3) :

(14.9)

The number of errors is either zero or one; the sum on the right-hand side
may exceed one, in cases where several typical noise vectors have the same
syndrome.

We can now write down the probability of a type-II error by averaging over

Equation (14.9) is a union bound.

x:

P (II)

TSjH (cid:20) Xx2T

P (x) Xx0:

x0 2 T
x0 6= x

 (cid:2)H(x0 (cid:0) x) = 0(cid:3) :

(14.10)

Now, we will (cid:12)nd the average of this probability of type-II error over all linear
codes by averaging over H. By showing that the average probability of type-II
error vanishes, we will thus show that there exist linear codes with vanishing
error probability, indeed, that almost all linear codes are very good.

We denote averaging over all binary matrices H by h: : :iH. The average

probability of type-II error is

(cid:22)P (II)

TS = XH

P (H)P (II)

TSjH = DP (II)

TSjHEH

(14.11)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

14.2: Data compression by linear hash codes

231

= *Xx2T
= Xx2T

P (x) Xx0: x0 2 T
P (x) Xx0: x0 2 T

 (cid:2)H(x0 (cid:0) x) = 0(cid:3)+
x0 6= x(cid:10) (cid:2)H(x0 (cid:0) x) = 0(cid:3)(cid:11)H :

x0 6= x

H

(14.12)

(14.13)

Now, the quantity h [H(x0 (cid:0) x) = 0]iH already cropped up when we were

calculating the expected weight enumerator function of random linear codes
(section 13.5): for any non-zero binary vector v, the probability that Hv = 0,
averaging over all matrices H, is 2(cid:0)M . So

(cid:22)P (II)

TS =  Xx2T

P (x)! (jTj (cid:0) 1) 2(cid:0)M

(14.14)

(14.15)
where jTj denotes the size of the typical set. As you will recall from Chapter
4, there are roughly 2N H(X) noise vectors in the typical set. So

(cid:20) jTj 2(cid:0)M ;

(cid:22)P (II)
TS (cid:20) 2N H(X)2(cid:0)M :

(14.16)

This bound on the probability of error either vanishes or grows exponentially
as N increases (remembering that we are keeping M proportional to N as N
increases). It vanishes if

(14.17)
Substituting R = 1(cid:0) M=N , we have thus established the noisy-channel coding
theorem for the binary symmetric channel: very good linear codes exist for
any rate R satisfying

H(X) < M=N:

where H(X) is the entropy of the channel noise, per bit.

R < 1 (cid:0) H(X);

(14.18)

2

Exercise 14.1.[3 ] Redo the proof for a more general channel.

14.2 Data compression by linear hash codes

The decoding game we have just played can also be viewed as an uncompres-
sion game. The world produces a binary noise vector x from a source P (x).
The noise has redundancy (if the (cid:13)ip probability is not 0.5). We compress it
with a linear compressor that maps the N -bit input x (the noise) to the M -bit
output z (the syndrome). Our uncompression task is to recover the input x
from the output z. The rate of the compressor is
Rcompressor (cid:17) M=N:

(14.19)

[We don’t care about the possibility of linear redundancies in our de(cid:12)nition
of the rate, here.] The result that we just found, that the decoding problem
can be solved, for almost any H, with vanishing error probability, as long as
H(X) < M=N , thus instantly proves a source coding theorem:

Given a binary source X of entropy H(X), and a required com-
pressed rate R > H(X), there exists a linear compressor x ! z =
Hx mod 2 having rate M=N equal to that required rate R, and an
associated uncompressor, that is virtually lossless.

This theorem is true not only for a source of independent identically dis-
tributed symbols but also for any source for which a typical set can be de-
(cid:12)ned: sources with memory, and time-varying sources, for example; all that’s
required is that the source be ergodic.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

232

Notes

14 | Very Good Linear Codes Exist

This method for proving that codes are good can be applied to other linear
codes, such as low-density parity-check codes (MacKay, 1999b; Aji et al., 2000).
For each code we need an approximation of its expected weight enumerator
function.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

15

Further Exercises on Information Theory

The most exciting exercises, which will introduce you to further ideas in in-
formation theory, are towards the end of this chapter.

Refresher exercises on source coding and noisy channels
. Exercise 15.1.[2 ] Let X be an ensemble with AX = f0; 1g and PX =
f0:995; 0:005g. Consider source coding using the block coding of X 100
where every x 2 X 100 containing 3 or fewer 1s is assigned a distinct
codeword, while the other xs are ignored.

(a) If the assigned codewords are all of the same length, (cid:12)nd the min-
imum length required to provide the above set with distinct code-
words.

(b) Calculate the probability of getting an x that will be ignored.

. Exercise 15.2.[2 ] Let X be an ensemble with PX = f0:1; 0:2; 0:3; 0:4g. The en-
semble is encoded using the symbol code C = f0001; 001; 01; 1g. Consider
the codeword corresponding to x 2 X N , where N is large.
(a) Compute the entropy of the fourth bit of transmission.
(b) Compute the conditional entropy of the fourth bit given the third

bit.

(c) Estimate the entropy of the hundredth bit.
(d) Estimate the conditional entropy of the hundredth bit given the

ninety-ninth bit.

Exercise 15.3.[2 ] Two fair dice are rolled by Alice and the sum is recorded.
Bob’s task is to ask a sequence of questions with yes/no answers to (cid:12)nd
out this number. Devise in detail a strategy that achieves the minimum
possible average number of questions.

. Exercise 15.4.[2 ] How can you use a coin to draw straws among 3 people?

. Exercise 15.5.[2 ] In a magic trick, there are three participants: the magician,
an assistant, and a volunteer. The assistant, who claims to have paranor-
mal abilities, is in a soundproof room. The magician gives the volunteer
six blank cards, (cid:12)ve white and one blue. The volunteer writes a dif-
ferent integer from 1 to 100 on each card, as the magician is watching.
The volunteer keeps the blue card. The magician arranges the (cid:12)ve white
cards in some order and passes them to the assistant. The assistant then
announces the number on the blue card.

How does the trick work?

233

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

234

15 | Further Exercises on Information Theory

. Exercise 15.6.[3 ] How does this trick work?

‘Here’s an ordinary pack of cards, shu(cid:15)ed into random order.
Please choose (cid:12)ve cards from the pack, any that you wish.
Don’t let me see their faces. No, don’t give them to me: pass
them to my assistant Esmerelda. She can look at them.
‘Now, Esmerelda, show me four of the cards. Hmm: : : nine
of spades, six of clubs, four of hearts, ten of diamonds. The
hidden card, then, must be the queen of spades!’

The trick can be performed as described above for a pack of 52 cards.
Use information theory to give an upper bound on the number of cards
for which the trick can be performed.

. Exercise 15.7.[2 ] Find a probability sequence p = (p1; p2; : : :) such that

H(p) = 1.

. Exercise 15.8.[2 ] Consider a discrete memoryless source with AX = fa; b; c; dg
and PX = f1=2; 1=4; 1=8; 1=8g. There are 48 = 65 536 eight-letter words
that can be formed from the four letters. Find the total number of such
words that are in the typical set TN (cid:12) (equation 4.29) where N = 8 and
(cid:12) = 0:1.

. Exercise 15.9.[2 ] Consider

=
f1/3; 1/3; 1/9; 1/9; 1/9g and the channel whose transition probability matrix
is

the

source AS
Q =2
664

0
1 0
0 0 2/3 0
0 1
1
0 0 1/3 0

= fa; b; c; d; eg, PS
3
775

:

0

0

(15.1)

Note that the source alphabet has (cid:12)ve symbols, but the channel alphabet
AX = AY = f0; 1; 2; 3g has only four. Assume that the source produces
symbols at exactly 3/4 the rate that the channel accepts channel sym-
bols. For a given (tiny) (cid:15) > 0, explain how you would design a system
for communicating the source’s output over the channel with an aver-
age error probability per source symbol less than (cid:15). Be as explicit as
possible. In particular, do not invoke Shannon’s noisy-channel coding
theorem.

. Exercise 15.10.[2 ] Consider a binary symmetric channel and a code C =
f0000; 0011; 1100; 1111g; assume that the four codewords are used with
probabilities f1=2; 1=8; 1=8; 1=4g.
What is the decoding rule that minimizes the probability of decoding
error? [The optimal decoding rule depends on the noise level f of the
binary symmetric channel. Give the decoding rule for each range of
values of f , for f between 0 and 1=2.]

Exercise 15.11.[2 ] Find the capacity and optimal input distribution for the

three-input, three-output channel whose transition probabilities are:

Q =2
4

0
1
0 2/3
0 1/3

0
1/3
2/3

3
5 :

(15.2)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

15 | Further Exercises on Information Theory

235

Exercise 15.12.[3, p.239] The input to a channel Q is a word of 8 bits. The
output is also a word of 8 bits. Each time it is used, the channel (cid:13)ips
exactly one of the transmitted bits, but the receiver does not know which
one. The other seven bits are received without error. All 8 bits are
equally likely to be the one that is (cid:13)ipped. Derive the capacity of this
channel.

Show, by describing an explicit encoder and decoder that it is possible
reliably (that is, with zero error probability) to communicate 5 bits per
cycle over this channel.

. Exercise 15.13.[2 ] A channel with input x 2 fa; b; cg and output y 2 fr; s; t; ug

has conditional probability matrix:

Q =2
664

What is its capacity?

1/2
1/2
0
0

0
1/2
1/2
0

0
0
1/2
1/2

:

3
775

(cid:8)(cid:8)*
HHj
(cid:8)(cid:8)*
HHj
(cid:8)(cid:8)*
HHj

a

b
c

r
s

t
u

0-521-64298-1
1-010-00000-4

Table 15.1. Some valid ISBNs.
[The hyphens are included for
legibility.]

. Exercise 15.14.[3 ] The ten-digit number on the cover of a book known as the
ISBN incorporates an error-detecting code. The number consists of nine
source digits x1; x2; : : : ; x9, satisfying xn 2 f0; 1; : : : ; 9g, and a tenth
check digit whose value is given by

x10 =  9
Xn=1

nxn! mod 11:

Here x10 2 f0; 1; : : : ; 9; 10g: If x10 = 10 then the tenth digit is shown
using the roman numeral X.

Show that a valid ISBN satis(cid:12)es:

  10
Xn=1

nxn! mod 11 = 0:

Imagine that an ISBN is communicated over an unreliable human chan-
nel which sometimes modi(cid:12)es digits and sometimes reorders digits.

Show that this code can be used to detect (but not correct) all errors in
which any one of the ten digits is modi(cid:12)ed (for example, 1-010-00000-4
! 1-010-00080-4).
Show that this code can be used to detect all errors in which any two ad-
jacent digits are transposed (for example, 1-010-00000-4 ! 1-100-00000-
4).

What other transpositions of pairs of non-adjacent digits can be de-
tected?

If the tenth digit were de(cid:12)ned to be

x10 =  9
Xn=1

nxn! mod 10;

why would the code not work so well? (Discuss the detection of both
modi(cid:12)cations of single digits and transpositions of digits.)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

236

15 | Further Exercises on Information Theory

-
(cid:0)(cid:18)
@R
-

@
(cid:0)

-
(cid:0)(cid:18)@
@R
-
(cid:0)

a

b

c

d

a

b

c

d

Remember d

dp H2(p) = log2

1(cid:0)p
p .

Exercise 15.15.[3 ] A channel with input x and output y has transition proba-

bility matrix:

f
1 (cid:0) f
0
0

0
0
1 (cid:0) g
g

0
0
g
1 (cid:0) g

:

3
775

1 (cid:0) f
f
0
0

Q =2
664
PX =(cid:26) p

2

Assuming an input distribution of the form

;

p
2

;

1 (cid:0) p
2

;

2 (cid:27) ;
1 (cid:0) p

write down the entropy of the output, H(Y ), and the conditional entropy
of the output given the input, H(Y jX).
Show that the optimal input distribution is given by

p =

1

1 + 2(cid:0)H2(g)+H2(f ) ;

1

where H2(f ) = f log2
Write down the optimal input distribution and the capacity of the chan-
nel in the case f = 1=2, g = 0, and comment on your answer.

f + (1 (cid:0) f ) log2

(1(cid:0)f ) .

1

. Exercise 15.16.[2 ] What are the di(cid:11)erences in the redundancies needed in an
error-detecting code (which can reliably detect that a block of data has
been corrupted) and an error-correcting code (which can detect and cor-
rect errors)?

Further tales from information theory

The following exercises give you the chance to discover for yourself the answers
to some more surprising results of information theory.

Exercise 15.17.[3 ] Communication of information from correlated sources. Imag-
ine that we want to communicate data from two data sources X (A) and X (B)
to a central location C via noise-free one-way communication channels ((cid:12)g-
ure 15.2a). The signals x(A) and x(B) are strongly dependent, so their joint
information content is only a little greater than the marginal information con-
tent of either of them. For example, C is a weather collator who wishes to
receive a string of reports saying whether it is raining in Allerton (x(A)) and
whether it is raining in Bognor (x(B)). The joint probability of x(A) and x(B)
might be

P (x(A); x(B)):

x(A)
0
1

x(B) 0
1

0.49 0.01
0.01 0.49

(15.3)

The weather collator would like to know N successive values of x(A) and x(B)
exactly, but, since he has to pay for every bit of information he receives, he
is interested in the possibility of avoiding buying N bits from source A and
N bits from source B. Assuming that variables x(A) and x(B) are generated
repeatedly from this distribution, can they be encoded at rates RA and RB in
such a way that C can reconstruct all the variables, with the sum of information
transmission rates on the two lines being less than two bits per cycle?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

15 | Further Exercises on Information Theory

237

x(A)

encode

-

RA

t(A)

(a)

x(B)

encode

-

RB

t(B)

HHHj
(cid:8)(cid:8)(cid:8)* C

RB
H(X (A); X (B))

H(X (B))

H(X (B) j X (A))

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    

                    


Achievable

(b)

H(X (A) j X (B)) H(X (A))

RA

The answer, which you should demonstrate, is indicated in (cid:12)gure 15.2.
In the general case of two dependent sources X (A) and X (B), there exist
codes for the two transmitters that can achieve reliable communication of
both X (A) and X (B) to C, as long as:
the information rate from X (A),
RA, exceeds H(X (A) j X (B)); the information rate from X (B), RB, exceeds
H(X (B) j X (A)); and the total information rate RA + RB exceeds the joint
entropy H(X (A); X (B)) (Slepian and Wolf, 1973).
So in the case of x(A) and x(B) above, each transmitter must transmit at
a rate greater than H2(0:02) = 0:14 bits, and the total rate RA + RB must
be greater than 1.14 bits, for example RA = 0:6, RB = 0:6. There exist codes
that can achieve these rates. Your task is to (cid:12)gure out why this is so.

Try to (cid:12)nd an explicit solution in which one of the sources is sent as plain

text, t(B) = x(B), and the other is encoded.

Exercise 15.18.[3 ] Multiple access channels. Consider a channel with two sets
of inputs and one output { for example, a shared telephone line ((cid:12)gure 15.3a).
A simple model system has two binary inputs x(A) and x(B) and a ternary
output y equal to the arithmetic sum of the two inputs, that’s 0, 1 or 2. There
is no noise. Users A and B cannot communicate with each other, and they
cannot hear the output of the channel. If the output is a 0, the receiver can
be certain that both inputs were set to 0; and if the output is a 2, the receiver
can be certain that both inputs were set to 1. But if the output is 1, then
it could be that the input state was (0; 1) or (1; 0). How should users A and
B use this channel so that their messages can be deduced from the received
signals? How fast can A and B communicate?

Clearly the total information rate from A and B to the receiver cannot
be two bits. On the other hand it is easy to achieve a total information rate
RA+RB of one bit. Can reliable communication be achieved at rates (RA; RB)
such that RA + RB > 1?

The answer is indicated in (cid:12)gure 15.3.
Some practical codes for multi-user channels are presented in Ratzer and

MacKay (2003).

Exercise 15.19.[3 ] Broadcast channels. A broadcast channel consists of a single
transmitter and two or more receivers. The properties of the channel are de-
(cid:12)ned by a conditional distribution Q(y(A); y(B) j x). (We’ll assume the channel
is memoryless.) The task is to add an encoder and two decoders to enable
reliable communication of a common message at rate R0 to both receivers, an
individual message at rate RA to receiver A, and an individual message at rate
RB to receiver B. The capacity region of the broadcast channel is the convex
hull of the set of achievable rate triplets (R0; RA; RB).

A simple benchmark for such a channel is given by time-sharing (time-
division signaling). If the capacities of the two channels, considered separately,

Figure 15.2. Communication of
information from dependent
sources. (a) x(A) and x(B) are
dependent sources (the
dependence is represented by the
dotted arrow). Strings of values of
each variable are encoded using
codes of rate RA and RB into
transmissions t(A) and t(B), which
are communicated over noise-free
channels to a receiver C. (b) The
achievable rate region. Both
strings can be conveyed without
error even though RA < H(X (A))
and RB < H(X (B)).

x

(cid:8)(cid:8)*
HHj

y(A)

y(B)

Figure 15.4. The broadcast
channel. x is the channel input;
y(A) and y(B) are the outputs.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

238

15 | Further Exercises on Information Theory

x(A)

x(B)

-

-

(a)

P (yjx(A); x(B))

y-

RB

1

y:

x(B) 0
1

(b)

x(A)
0 1

0 1
1 2

1=2

Achievable

(c)

1=2

1

RA

are C (A) and C (B), then by devoting a fraction (cid:30)A of the transmission time
to channel A and (cid:30)B = 1(cid:0)(cid:30)A to channel B, we can achieve (R0; RA; RB) =
(0; (cid:30)AC (A); (cid:30)BC (B)).
We can do better than this, however. As an analogy, imagine speaking
simultaneously to an American and a Belarusian; you are (cid:13)uent in American
and in Belarusian, but neither of your two receivers understands the other’s
language.
If each receiver can distinguish whether a word is in their own
language or not, then an extra binary (cid:12)le can be conveyed to both recipients by
using its bits to decide whether the next transmitted word should be from the
American source text or from the Belarusian source text. Each recipient can
concatenate the words that they understand in order to receive their personal
message, and can also recover the binary string.

An example of a broadcast channel consists of two binary symmetric chan-
nels with a common input. The two halves of the channel have (cid:13)ip prob-
abilities fA and fB. We’ll assume that A has the better half-channel, i.e.,
fA < fB < 1/2.
[A closely related channel is a ‘degraded’ broadcast channel,
in which the conditional probabilities are such that the random variables have
the structure of a Markov chain,

x ! y(A) ! y(B);

(15.4)

i.e., y(B) is a further degraded version of y(A).] In this special case, it turns
out that whatever information is getting through to receiver B can also be
recovered by receiver A. So there is no point distinguishing between R0 and
RB: the task is to (cid:12)nd the capacity region for the rate pair (R0; RA), where
R0 is the rate of information reaching both A and B, and RA is the rate of
the extra information reaching A.

The following exercise is equivalent to this one, and a solution to it is

illustrated in (cid:12)gure 15.8.

Exercise 15.20.[3 ] Variable-rate error-correcting codes for channels with unknown
noise level.
In real life, channels may sometimes not be well characterized
before the encoder is installed. As a model of this situation, imagine that a
channel is known to be a binary symmetric channel with noise level either fA
or fB. Let fB > fA, and let the two capacities be CA and CB.

Those who like to live dangerously might install a system designed for noise
level fA with rate RA ’ CA; in the event that the noise level turns out to be
fB, our experience of Shannon’s theories would lead us to expect that there

Figure 15.3. Multiple access
channels. (a) A general multiple
access channel with two
transmitters and one receiver. (b)
A binary multiple access channel
with output equal to the sum of
two inputs. (c) The achievable
region.

RB
C (B)

6
@

@

@

-

@@
C (A)

RA

Figure 15.5. Rates achievable by
simple timesharing.

R

C
A

BC

f
A

f
B

f

Figure 15.6. Rate of reliable
communication R, as a function of
noise level f , for Shannonesque
codes designed to operate at noise
levels fA (solid line) and fB
(dashed line).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

15 | Further Exercises on Information Theory

239

R

C
A

BC

f
A

f
B

f

Figure 15.7. Rate of reliable
communication R, as a function of
noise level f , for a desired
variable-rate code.

0.6

0.4

0.2

0

0

0.2 0.4 0.6 0.8

1

Figure 15.8. An achievable region
for the channel with unknown
noise level. Assuming the two
possible noise levels are fA = 0:01
and fB = 0:1, the dashed lines
show the rates RA; RB that are
achievable using a simple
time-sharing approach, and the
solid line shows rates achievable
using a more cunning approach.

would be a catastrophic failure to communicate information reliably (solid line
in (cid:12)gure 15.6).

A conservative approach would design the encoding system for the worst-
case scenario, installing a code with rate RB ’ CB (dashed line in (cid:12)gure 15.6).
In the event that the lower noise level, fA, holds true, the managers would
have a feeling of regret because of the wasted capacity di(cid:11)erence CA (cid:0) RB.
Is it possible to create a system that not only transmits reliably at some
rate R0 whatever the noise level, but also communicates some extra, ‘lower-
priority’ bits if the noise level is low, as shown in (cid:12)gure 15.7? This code
communicates the high-priority bits reliably at all noise levels between fA and
fB, and communicates the low-priority bits also if the noise level is fA or
below.

This problem is mathematically equivalent to the previous problem, the
degraded broadcast channel. The lower rate of communication was there called
R0, and the rate at which the low-priority bits are communicated if the noise
level is low was called RA.

An illustrative answer is shown in (cid:12)gure 15.8, for the case fA = 0:01 and
fB = 0:1. (This (cid:12)gure also shows the achievable region for a broadcast channel
whose two half-channels have noise levels fA = 0:01 and fB = 0:1.) I admit I
(cid:12)nd the gap between the simple time-sharing solution and the cunning solution
disappointingly small.

In Chapter 50 we will discuss codes for a special class of broadcast channels,
namely erasure channels, where every symbol is either received without error
or erased. These codes have the nice property that they are rateless { the
number of symbols transmitted is determined on the (cid:13)y such that reliable
comunication is achieved, whatever the erasure statistics of the channel.

Exercise 15.21.[3 ] Multiterminal information networks are both important practi-
cally and intriguing theoretically. Consider the following example of a two-way
binary channel ((cid:12)gure 15.9a,b): two people both wish to talk over the channel,
and they both want to hear what the other person is saying; but you can hear
the signal transmitted by the other person only if you are transmitting a zero.
What simultaneous information rates from A to B and from B to A can be
achieved, and how? Everyday examples of such networks include the VHF
channels used by ships, and computer ethernet networks (in which all the
devices are unable to hear anything if two or more devices are broadcasting
simultaneously).

Obviously, we can achieve rates of 1/2 in both directions by simple time-
sharing. But can the two information rates be made larger? Finding the
capacity of a general two-way channel is still an open problem. However,
we can obtain interesting results concerning achievable points for the simple
binary channel discussed above, as indicated in (cid:12)gure 15.9c. There exist codes
that can achieve rates up to the boundary shown. There may exist better
codes too.

Solutions

Solution to exercise 15.12 (p.235). C(Q) = 5 bits.
Hint for the last part: a solution exists that involves a simple (8; 5) code.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

240

15 | Further Exercises on Information Theory

x(B)

x(A)
0 1

0 1
0 0

Figure 15.9. (a) A general
two-way channel. (b) The rules
for a binary two-way channel.
The two tables show the outputs
y(A) and y(B) that result for each
state of the inputs. (c) Achievable
region for the two-way binary
channel. Rates below the solid
line are achievable. The dotted
line shows the ‘obviously
achievable’ region which can be
attained by simple time-sharing.

-

x(A)

y(A)

(cid:27)

(a)

P (y(A); y(B)jx(A); x(B))

(cid:27)

-

y(B)

y(A):

x(B) 0
1

(b)

x(A)
0 1

0 0
1 0

y(B):

x(B) 0
1

1

0.8

0.6

0.4

0.2

)

(

B
R

Achievable

(c)

0

0

0.2

0.4

R(A)

0.6

0.8

1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

16

Message Passing

One of the themes of this book is the idea of doing complicated calculations
using simple distributed hardware. It turns out that quite a few interesting
problems can be solved by message-passing algorithms, in which simple mes-
sages are passed locally among simple processors whose operations lead, after
some time, to the solution of a global problem.

16.1 Counting

As an example, consider a line of soldiers walking in the mist. The commander
wishes to perform the complex calculation of counting the number of soldiers
in the line. This problem could be solved in two ways.

First there is a solution that uses expensive hardware: the loud booming
voices of the commander and his men. The commander could shout ‘all soldiers
report back to me within one minute!’, then he could listen carefully as the
men respond ‘Molesworth here sir!’, ‘Fotherington{Thomas here sir!’, and so
on. This solution relies on several expensive pieces of hardware: there must be
a reliable communication channel to and from every soldier; the commander
must be able to listen to all the incoming messages { even when there are
hundreds of soldiers { and must be able to count; and all the soldiers must be
well-fed if they are to be able to shout back across the possibly-large distance
separating them from the commander.

The second way of (cid:12)nding this global function, the number of soldiers,
does not require global communication hardware, high IQ, or good food; we
simply require that each soldier can communicate single integers with the two
adjacent soldiers in the line, and that the soldiers are capable of adding one
to a number. Each soldier follows these rules:

1. If you are the front soldier in the line, say the number ‘one’ to the

soldier behind you.

Algorithm 16.1. Message-passing
rule-set A.

2. If you are the rearmost soldier in the line, say the number ‘one’ to

the soldier in front of you.

3. If a soldier ahead of or behind you says a number to you, add one

to it, and say the new number to the soldier on the other side.

If the clever commander can not only add one to a number, but also add
two numbers together, then he can (cid:12)nd the global number of soldiers by simply
adding together:

241

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

242

16 | Message Passing

Figure 16.2. A line of soldiers
counting themselves using
message-passing rule-set A. The
commander can add ‘3’ from the
soldier in front, ‘1’ from the
soldier behind, and ‘1’ for himself,
and deduce that there are 5
soldiers in total.

Figure 16.3. A swarm of guerillas.

the number said to him by the
soldier in front of him,

+ the number said to the com-
mander by the soldier behind
him,
+ one

(which equals the total number of
soldiers in front)
(which is the number behind)

(to count the commander himself).

This solution requires only local communication hardware and simple compu-
tations (storage and addition of integers).

1

4

2

3

3

2

4

1

Commander

Separation

This clever trick makes use of a profound property of the total number of
soldiers: that it can be written as the sum of the number of soldiers in front
of a point and the number behind that point, two quantities which can be
computed separately, because the two groups are separated by the commander.
If the soldiers were not arranged in a line but were travelling in a swarm,
then it would not be easy to separate them into two groups in this way. The

Commander

Jim

guerillas in (cid:12)gure 16.3 could not be counted using the above message-passing
rule-set A, because, while the guerillas do have neighbours (shown by lines),
it is not clear who is ‘in front’ and who is ‘behind’; furthermore, since the
graph of connections between the guerillas contains cycles, it is not possible
for a guerilla in a cycle (such as ‘Jim’) to separate the group into two groups,
‘those in front’, and ‘those behind’.

A swarm of guerillas can be counted by a modi(cid:12)ed message-passing algo-

rithm if they are arranged in a graph that contains no cycles.

Rule-set B is a message-passing algorithm for counting a swarm of guerillas
whose connections form a cycle-free graph, also known as a tree, as illustrated
in (cid:12)gure 16.4. Any guerilla can deduce the total in the tree from the messages
that they receive.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

16.1: Counting

243

Figure 16.4. A swarm of guerillas
whose connections form a tree.

Commander

Jim

Algorithm 16.5. Message-passing
rule-set B.

1. Count your number of neighbours, N .

2. Keep count of the number of messages you have received from your
neighbours, m, and of the values v1, v2, : : : ; vN of each of those
messages. Let V be the running total of the messages you have
received.

3. If the number of messages you have received, m, is equal to N (cid:0) 1,
then identify the neighbour who has not sent you a message and tell
them the number V + 1.

4. If the number of messages you have received is equal to N , then:

(a) the number V + 1 is the required total.
(b) for each neighbour n f

say to neighbour n the number V + 1 (cid:0) vn.

g

A

Figure 16.6. A triangular 41 (cid:2) 41
grid. How many paths are there
from A to B? One path is shown.

B

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

244

16.2 Path-counting

A more profound task than counting squaddies is the task of counting the
number of paths through a grid, and (cid:12)nding how many paths pass through
any given point in the grid.

Figure 16.6 shows a rectangular grid, and a path through the grid, con-
necting points A and B. A valid path is one that starts from A and proceeds
to B by rightward and downward moves. Our questions are:

1. How many such paths are there from A to B?

2. If a random path from A to B is selected, what is the probability that it
passes through a particular node in the grid? [When we say ‘random’, we
mean that all paths have exactly the same probability of being selected.]

3. How can a random path from A to B be selected?

Counting all the paths from A to B doesn’t seem straightforward. The number
of paths is expected to be pretty big { even if the permitted grid were a diagonal
strip only three nodes wide, there would still be about 2N=2 possible paths.

The computational breakthrough is to realize that to (cid:12)nd the number of
paths, we do not have to enumerate all the paths explicitly. Pick a point P in
the grid and consider the number of paths from A to P. Every path from A
to P must come in to P through one of its upstream neighbours (‘upstream’
meaning above or to the left). So the number of paths from A to P can be
found by adding up the number of paths from A to each of those neighbours.
This message-passing algorithm is illustrated in (cid:12)gure 16.8 for a simple
grid with ten vertices connected by twelve directed edges. We start by send-
ing the ‘1’ message from A. When any node has received messages from all its
upstream neighbours, it sends the sum of them on to its downstream neigh-
bours. At B, the number 5 emerges: we have counted the number of paths
from A to B without enumerating them all. As a sanity-check, (cid:12)gure 16.9
shows the (cid:12)ve distinct paths from A to B.

Having counted all paths, we can now move on to more challenging prob-
lems: computing the probability that a random path goes through a given
vertex, and creating a random path.

Probability of passing through a node

By making a backward pass as well as the forward pass, we can deduce how
many of the paths go through each node; and if we divide that by the total
number of paths, we obtain the probability that a randomly selected path
passes through that node. Figure 16.10 shows the backward-passing messages
in the lower-right corners of the tables, and the original forward-passing mes-
sages in the upper-left corners. By multiplying these two numbers at a given
vertex, we (cid:12)nd the total number of paths passing through that vertex. For
example, four paths pass through the central vertex.

Figure 16.11 shows the result of this computation for the triangular 41 (cid:2)
41 grid. The area of each blob is proportional to the probability of passing
through the corresponding node.

Random path sampling

Exercise 16.1.[1, p.247] If one creates a ‘random’ path from A to B by (cid:13)ipping
a fair coin at every junction where there is a choice of two directions, is

16 | Message Passing

A

N
P

M

B

Figure 16.7. Every path from A to
P enters P through an upstream
neighbour of P, either M or N; so
we can (cid:12)nd the number of paths
from A to P by adding the
number of paths from A to M to
the number from A to N.

1
A

1

1

1

2

2

1

3

5

5

B

Figure 16.8. Messages sent in the
forward pass.

A

B

Figure 16.9. The (cid:12)ve paths.

1

5
A

1

1

5

2

1

2

2

3

2

1

B

1

1

3

5

5

1

1

1

Figure 16.10. Messages sent in the
forward and backward passes.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

16.3: Finding the lowest-cost path

245

the resulting path a uniform random sample from the set of all paths?
[Hint: imagine trying it for the grid of (cid:12)gure 16.8.]

There is a neat insight to be had here, and I’d like you to have the satisfaction
of (cid:12)guring it out.

Exercise 16.2.[2, p.247] Having run the forward and backward algorithms be-
tween points A and B on a grid, how can one draw one path from A to
B uniformly at random? (Figure 16.11.)

A

Figure 16.11. (a) The probability
of passing through each node, and
(b) a randomly chosen path.

(a)

(b)

B

The message-passing algorithm we used to count the paths to B is an
example of the sum{product algorithm. The ‘sum’ takes place at each node
when it adds together the messages coming from its predecessors; the ‘product’
was not mentioned, but you can think of the sum as a weighted sum in which
all the summed terms happened to have weight 1.

16.3 Finding the lowest-cost path

Imagine you wish to travel as quickly as possible from Ambridge (A) to Bognor
(B). The various possible routes are shown in (cid:12)gure 16.12, along with the cost
in hours of traversing each edge in the graph. For example, the route A{I{L{
N{B has a cost of 8 hours. We would like to (cid:12)nd the lowest-cost path without
explicitly evaluating the cost of all paths. We can do this e(cid:14)ciently by (cid:12)nding
for each node what the cost of the lowest-cost path to that node from A is.
These quantities can be computed by message-passing, starting from node A.
The message-passing algorithm is called the min{sum algorithm or Viterbi
algorithm.

For brevity, we’ll call the cost of the lowest-cost path from node A to
node x ‘the cost of x’. Each node can broadcast its cost to its descendants
once it knows the costs of all its possible predecessors. Let’s step through the
algorithm by hand. The cost of A is zero. We pass this news on to H and I.
As the message passes along each edge in the graph, the cost of that edge is
added. We (cid:12)nd the costs of H and I are 4 and 1 respectively ((cid:12)gure 16.13a).
Similarly then, the costs of J and L are found to be 6 and 2 respectively, but
what about K? Out of the edge H{K comes the message that a path of cost 5
exists from A to K via H; and from edge I{K we learn of an alternative path
of cost 3 ((cid:12)gure 16.13b). The min{sum algorithm sets the cost of K equal
to the minimum of these (the ‘min’), and records which was the smallest-cost
route into K by retaining only the edge I{K and pruning away the other edges
leading to K ((cid:12)gure 16.13c). Figures 16.13d and e show the remaining two
iterations of the algorithm which reveal that there is a path from A to B with
cost 6. [If the min{sum algorithm encounters a tie, where the minimum-cost

A

4

(cid:8)(cid:8)(cid:8)*
HHHj

1

H

I

2

1

(cid:8)(cid:8)(cid:8)*
HHHj
(cid:8)(cid:8)(cid:8)*
HHHj

2

1

J

K

L

2

2

HHHj
(cid:8)(cid:8)(cid:8)*
HHHj
(cid:8)(cid:8)(cid:8)*

1

3

M

N

1

HHHj
(cid:8)(cid:8)(cid:8)*

3

B

Figure 16.12. Route diagram from
Ambridge to Bognor, showing the
costs associated with the edges.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

246

16 | Message Passing

path to a node is achieved by more than one route to it, then the algorithm
can pick any of those routes at random.]

We can recover this lowest-cost path by backtracking from B, following
the trail of surviving edges back to A. We deduce that the lowest-cost path is
A{I{K{M{B.

Other applications of the min{sum algorithm

Imagine that you manage the production of a product from raw materials
via a large set of operations. You wish to identify the critical path in your
process, that is, the subset of operations that are holding up production. If
any operations on the critical path were carried out a little faster then the
time to get from raw materials to product would be reduced.

The critical path of a set of operations can be found using the min{sum

algorithm.

In Chapter 25 the min{sum algorithm will be used in the decoding of

error-correcting codes.

16.4 Summary and related ideas

Some global functions have a separability property. For example, the number
of paths from A to P separates into the sum of the number of paths from A to M
(the point to P’s left) and the number of paths from A to N (the point above
P). Such functions can be computed e(cid:14)ciently by message-passing. Other
functions do not have such separability properties, for example

1. the number of pairs of soldiers in a troop who share the same birthday;

2. the size of the largest group of soldiers who share a common height

(rounded to the nearest centimetre);

3. the length of the shortest tour that a travelling salesman could take that

visits every soldier in a troop.

One of the challenges of machine learning is to (cid:12)nd low-cost solutions to prob-
lems like these. The problem of (cid:12)nding a large subset of variables that are
approximately equal can be solved with a neural network approach (Hop(cid:12)eld
and Brody, 2000; Hop(cid:12)eld and Brody, 2001). A neural approach to the trav-
elling salesman problem will be discussed in section 42.9.

(a)

0
A

(b)

0
A

(c)

0
A

(d)

0
A

(e)

0
A

4

(cid:8)(cid:8)(cid:8)*
HHHj

1

4
H

1
I

2
(cid:8)(cid:8)(cid:8)*
HHHj
1
2
(cid:8)(cid:8)(cid:8)*
HHHj
1

4

(cid:8)(cid:8)(cid:8)*
HHHj

1

4

(cid:8)(cid:8)(cid:8)*
HHHj

1

4

(cid:8)(cid:8)(cid:8)*
HHHj

1

4

(cid:8)(cid:8)(cid:8)*
HHHj

1

4
H

1
I

4
H

1
I

4
H

1
I

4
H

1
I

2
(cid:8)(cid:8)(cid:8)*
HHj
1
2
(cid:8)(cid:8)*
HHHj
1

(cid:8)(cid:8)(cid:8)*

2
1

2

(cid:8)(cid:8)(cid:8)*
HHHj

1

(cid:8)(cid:8)(cid:8)*

2
1

2

(cid:8)(cid:8)(cid:8)*
HHHj

1

(cid:8)(cid:8)(cid:8)*

2
1

2

(cid:8)(cid:8)(cid:8)*
HHHj

1

J

K

L

6
J

5
K
3

2
L

6
J

3
K

2
L

6
J

3
K

2
L

6
J

3
K

2
L

M

N

HHHj
1
(cid:8)(cid:8)(cid:8)*
3

B

M

N

HHHj
1
(cid:8)(cid:8)(cid:8)*
3

B

M

N

HHHj
1
(cid:8)(cid:8)(cid:8)*
3

B

HHHj
2
2
(cid:8)(cid:8)(cid:8)*
HHHj
1
(cid:8)(cid:8)(cid:8)*
3

HHHj
2
2
(cid:8)(cid:8)(cid:8)*
HHHj
1
(cid:8)(cid:8)(cid:8)*
3

HHHj
2
2
(cid:8)(cid:8)(cid:8)*
HHHj
1
(cid:8)(cid:8)(cid:8)*
3

2

5
M

2

HHHj
1
(cid:8)(cid:8)(cid:8)*
HHHj (cid:8)(cid:8)(cid:8)*

1

3

4
N

B

3

2

2

(cid:8)(cid:8)(cid:8)*
HHHj

1

3

5
M

1

HHHj

4
N

3

6
B

16.5 Further exercises

. Exercise 16.3.[2 ] Describe the asymptotic properties of the probabilities de-

picted in (cid:12)gure 16.11a, for a grid in a triangle of width and height N .

. Exercise 16.4.[2 ] In image processing, the integral image I(x; y) obtained from

an image f (x; y) (where x and y are pixel coordinates) is de(cid:12)ned by

Figure 16.13. Min{sum
message-passing algorithm to (cid:12)nd
the cost of getting to each node,
and thence the lowest cost route
from A to B.

I(x; y) (cid:17)

x

Xu=0

y

Xv=0

f (u; v):

(16.1)

Show that the integral image I(x; y) can be e(cid:14)ciently computed by mes-
sage passing.

Show that, from the integral image, some simple functions of the image
can be obtained. For example, give an expression for the sum of the
image intensities f (x; y) for all (x; y) in a rectangular region extending
from (x1; y1) to (x2; y2).

y2

y1

x1

x2

(0; 0)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

247

16.6: Solutions

16.6 Solutions

Solution to exercise 16.1 (p.244). Since there are (cid:12)ve paths through the grid
of (cid:12)gure 16.8, they must all have probability 1=5. But a strategy based on fair
coin-(cid:13)ips will produce paths whose probabilities are powers of 1=2.

Solution to exercise 16.2 (p.245). To make a uniform random walk, each for-
ward step of the walk should be chosen using a di(cid:11)erent biased coin at each
junction, with the biases chosen in proportion to the backward messages ema-
nating from the two options. For example, at the (cid:12)rst choice after leaving A,
there is a ‘3’ message coming from the East, and a ‘2’ coming from South, so
one should go East with probability 3=5 and South with probability 2=5. This
is how the path in (cid:12)gure 16.11b was generated.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17

Communication over Constrained

Noiseless Channels

In this chapter we study the task of communicating e(cid:14)ciently over a con-
strained noiseless channel { a constrained channel over which not all strings
from the input alphabet may be transmitted.

We make use of the idea introduced in Chapter 16, that global properties

of graphs can be computed by a local message-passing algorithm.

17.1 Three examples of constrained binary channels

A constrained channel can be de(cid:12)ned by rules that de(cid:12)ne which strings are
permitted.

Example 17.1. In Channel A every 1 must be followed by at least one 0.

A valid string for this channel is

Channel A:

the substring 11 is forbidden.

00100101001010100010:

(17.1)

As a motivation for this model, consider a channel in which 1s are repre-
sented by pulses of electromagnetic energy, and the device that produces
those pulses requires a recovery time of one clock cycle after generating
a pulse before it can generate another.

Example 17.2. Channel B has the rule that all 1s must come in groups of two

or more, and all 0s must come in groups of two or more.

A valid string for this channel is

Channel B:

101 and 010 are forbidden.

00111001110011000011:

(17.2)

As a motivation for this model, consider a disk drive in which succes-
sive bits are written onto neighbouring points in a track along the disk
surface; the values 0 and 1 are represented by two opposite magnetic
orientations. The strings 101 and 010 are forbidden because a single
isolated magnetic domain surrounded by domains having the opposite
orientation is unstable, so that 101 might turn into 111, for example.

Example 17.3. Channel C has the rule that the largest permitted runlength is

two, that is, each symbol can be repeated at most once.

A valid string for this channel is

Channel C:

111 and 000 are forbidden.

10010011011001101001:

(17.3)

248

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17.1: Three examples of constrained binary channels

249

A physical motivation for this model is a disk drive in which the rate of
rotation of the disk is not known accurately, so it is di(cid:14)cult to distinguish
between a string of two 1s and a string of three 1s, which are represented
by oriented magnetizations of duration 2(cid:28) and 3(cid:28) respectively, where
(cid:28) is the (poorly known) time taken for one bit to pass by; to avoid
the possibility of confusion, and the resulting loss of synchronization of
sender and receiver, we forbid the string of three 1s and the string of
three 0s.

All three of these channels are examples of runlength-limited channels.
The rules constrain the minimum and maximum numbers of successive 1s and
0s.

Channel

Runlength of 1s

Runlength of 0s

minimum maximum minimum maximum

unconstrained

A
B
C

1
1
2
1

1
1
1
2

1
1
2
1

1
1
1
2

In channel A, runs of 0s may be of any length but runs of 1s are restricted to
length one. In channel B all runs must be of length two or more. In channel
C, all runs must be of length one or two.

The capacity of the unconstrained binary channel is one bit per channel
use. What are the capacities of the three constrained channels? [To be fair,
we haven’t de(cid:12)ned the ‘capacity’ of such channels yet; please understand ‘ca-
pacity’ as meaning how many bits can be conveyed reliably per channel-use.]

Some codes for a constrained channel

Let us concentrate for a moment on channel A, in which runs of 0s may be
of any length but runs of 1s are restricted to length one. We would like to
communicate a random binary (cid:12)le over this channel as e(cid:14)ciently as possible.
A simple starting point is a (2; 1) code that maps each source bit into two
transmitted bits, C1. This is a rate-1/2 code, and it respects the constraints of
channel A, so the capacity of channel A is at least 0.5. Can we do better?

C1 is redundant because if the (cid:12)rst of two received bits is a zero, we know
that the second bit will also be a zero. We can achieve a smaller average
transmitted length using a code that omits the redundant zeroes in C1.

C2 is such a variable-length code.

If the source symbols are used with

equal frequency then the average transmitted length per source bit is

L =

1
2

1 +

1
2

2 =

3
2

;

so the average communication rate is

R = 2/3;

(17.4)

(17.5)

and the capacity of channel A must be at least 2/3.

Can we do better than C2? There are two ways to argue that the infor-

mation rate could be increased above R = 2/3.

The (cid:12)rst argument assumes we are comfortable with the entropy as a
measure of information content. The idea is that, starting from code C2, we
can reduce the average message length, without greatly reducing the entropy

Code C1

s

t

0 00
1 10

Code C2

s

t

0
0
1 10

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

250

17 | Communication over Constrained Noiseless Channels

2

1

0

0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1+f
H_2(f)

0.25

0.5

0.75

1

R(f) = H_2(f)/(1+f)

0.25

0.5

0.75

1

Figure 17.1. Top: The information
content per source symbol and
mean transmitted length per
source symbol as a function of the
source density. Bottom: The
information content per
transmitted symbol, in bits, as a
function of f .

of the message we send, by decreasing the fraction of 1s that we transmit.
Imagine feeding into C2 a stream of bits in which the frequency of 1s is f . [Such
a stream could be obtained from an arbitrary binary (cid:12)le by passing the source
(cid:12)le into the decoder of an arithmetic code that is optimal for compressing
binary strings of density f .] The information rate R achieved is the entropy
of the source, H2(f ), divided by the mean transmitted length,

Thus

L(f ) = (1 (cid:0) f ) + 2f = 1 + f:

R(f ) =

H2(f )
L(f )

=

H2(f )
1 + f

:

(17.6)

(17.7)

The original code C2, without preprocessor, corresponds to f = 1/2. What
happens if we perturb f a little towards smaller f , setting

f =

1
2

+ (cid:14);

(17.8)

for small negative (cid:14)? In the vicinity of f = 1/2, the denominator L(f ) varies
linearly with (cid:14).
In contrast, the numerator H2(f ) only has a second-order
dependence on (cid:14).

. Exercise 17.4.[1 ] Find, to order (cid:14)2, the Taylor expansion of H2(f ) as a function

of (cid:14).

To (cid:12)rst order, R(f ) increases linearly with decreasing (cid:14). It must be possible
to increase R by decreasing f . Figure 17.1 shows these functions; R(f ) does
indeed increase as f decreases and has a maximum of about 0.69 bits per
channel use at f ’ 0:38.
maxf R(f ) = 0:69.

By this argument we have shown that the capacity of channel A is at least

. Exercise 17.5.[2, p.257] If a (cid:12)le containing a fraction f = 0:5 1s is transmitted

by C2, what fraction of the transmitted stream is 1s?

What fraction of the transmitted bits is 1s if we drive code C2 with a
sparse source of density f = 0:38?

A second, more fundamental approach counts how many valid sequences
of length N there are, SN . We can communicate log SN bits in N channel
cycles by giving one name to each of these valid sequences.

17.2 The capacity of a constrained noiseless channel

We de(cid:12)ned the capacity of a noisy channel in terms of the mutual information
between its input and its output, then we proved that this number, the capac-
ity, was related to the number of distinguishable messages S(N ) that could be
reliably conveyed over the channel in N uses of the channel by

C = lim
N!1

1
N

log S(N ):

(17.9)

In the case of the constrained noiseless channel, we can adopt this identity as
our de(cid:12)nition of the channel’s capacity. However, the name s, which, when
we were making codes for noisy channels (section 9.6), ran over messages
s = 1; : : : ; S, is about to take on a new role: labelling the states of our channel;

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17.3: Counting the number of possible messages

251

1

0

0

1

0

(a)

(cid:0)(cid:0)(cid:18)1

(c) f0 -

(cid:0)
0

s1
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s2
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s3
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s4
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s5
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s6
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s7
1

f
f

0

(cid:0)(cid:0)(cid:18)1
0
@
@@R
(cid:0)
-
0

s8
1

f
f

0

(b)

sn

1

j
j

0

@
0
@
(cid:0)

(cid:0)

(cid:0)(cid:18)1
(cid:0)
@

@R
-

0

1

11

1

0

1

0

1

0

00

0

B

1

1

0

11

sn

sn+1

j
j
m
m
m
m
A =2
664

00

0

sn+1

11

-1
(cid:0)(cid:18)1
(cid:0)

A
0
A
(cid:0)
A
(cid:0)
A

1
(cid:1)
@
0
(cid:1)
@
(cid:1)
(cid:1)

A
A
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:21)
AAU

@

@R
-

0

00

1 1 0 0
0 0 0 1
1 0 0 0
0 0 1 1

1

0

m
m
m
m
3
775

Figure 17.2. (a) State diagram for
channel A. (b) Trellis section. (c)
Trellis. (d) Connection matrix.

Figure 17.3. State diagrams, trellis
sections and connection matrices
for channels B and C.

(d) A =

(to)

1
0

(from)
1
0
1

(cid:20) 0

1

1 (cid:21)

11

1

1

0

0

1
1 0
0

00

C

(cid:0)(cid:18)1
(cid:0)

0
A
(cid:0)
A
(cid:0)
A
A
(cid:0)(cid:18)1
@
0
(cid:1)(cid:1)(cid:21)
(cid:0)
A
@
(cid:0)
(cid:1)
AAU
@
1(cid:0)
@R
(cid:1)
(cid:1)
@
0
(cid:1)
@
(cid:1)

@

@R

sn

1

0

A

11

n
n
n
n
A =2
664

00

(cid:1)

sn+1

11

1

0

n
n
n
n
3
775

00

0 1 0 0
0 0 1 1
1 1 0 0
0 0 1 0

so in this chapter we will denote the number of distinguishable messages of
length N by MN , and de(cid:12)ne the capacity to be:

C = lim
N!1

1
N

log MN :

(17.10)

Once we have (cid:12)gured out the capacity of a channel we will return to the

task of making a practical code for that channel.

17.3 Counting the number of possible messages

First let us introduce some representations of constrained channels. In a state
diagram, states of the transmitter are represented by circles labelled with the
name of the state. Directed edges from one state to another indicate that
the transmitter is permitted to move from the (cid:12)rst state to the second, and a
label on that edge indicates the symbol emitted when that transition is made.
Figure 17.2a shows the state diagram for channel A. It has two states, 0 and
1. When transitions to state 0 are made, a 0 is transmitted; when transitions
to state 1 are made, a 1 is transmitted; transitions from state 1 to state 1 are
not possible.

We can also represent the state diagram by a trellis section, which shows
two successive states in time at two successive horizontal
locations ((cid:12)g-
ure 17.2b). The state of the transmitter at time n is called sn. The set of
possible state sequences can be represented by a trellis as shown in (cid:12)gure 17.2c.
A valid sequence corresponds to a path through the trellis, and the number of

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

252

17 | Communication over Constrained Noiseless Channels

M1 = 2

M1 = 2

M2 = 3

M1 = 2

M2 = 3

M3 = 5

1
1

h
h

0
1

(cid:0)(cid:0)(cid:18)

(cid:0)

-(cid:0)

h0

1
1

h
h

0
1

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

(cid:0)

@
(cid:0)

-(cid:0)

@@R(cid:0)
-

h0

1
1

h
h

0
2

h0

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@
(cid:0)

@
(cid:0)

(cid:0)

-(cid:0)

@@R(cid:0)
-

@@R(cid:0)
-

1
1

h
h

0
2

2
1

h
h

0
3

1
1

h
h

0
1

Figure 17.4. Counting the number
of paths in the trellis of channel
A. The counts next to the nodes
are accumulated by passing from
left to right across the trellises.

1
1

h
h

0
1

11

1
1

h
h
h
h

0

00
1

1
1

h
h

0
2

1
11

1
1

h
h
h
h

0

00
1

(a) Channel A

(b) Channel B

(c) Channel C

M1 = 2

M2 = 3

M3 = 5

M4 = 8

M5 = 13

M6 = 21

M7 = 34

M8 = 55

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

@

(cid:0)(cid:0)(cid:18)

(cid:0)

@
(cid:0)

@
(cid:0)

@
(cid:0)

@
(cid:0)

@
(cid:0)

@
(cid:0)

@
(cid:0)

-(cid:0)

@@R(cid:0)
-

@@R(cid:0)
-

@@R(cid:0)
-

@@R(cid:0)
-

@@R(cid:0)
-

@@R(cid:0)
-

@@R(cid:0)
-

3
1

h
h

0
5

5
1

h
h

0
8

8
1

h
h

0
13

13
1

h
h

0
21

21
1

h
h

0
34

2
1

h
h

0
3

h0

M1 = 2

M2 = 3

M3 = 5

M4 = 8

M5 = 13

M6 = 21

M7 = 34

M8 = 55

A

(cid:0)
A
(cid:0)
A

-
(cid:0)(cid:0)(cid:18)

A
(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU
(cid:1)

A

(cid:0)
A
(cid:0)
A

-
(cid:0)(cid:0)(cid:18)

A
(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU
(cid:1)

(cid:1)
@
(cid:1)
@

(cid:1)

@@R
-

(cid:1)
@
(cid:1)
@

(cid:1)

@@R
-

-
(cid:0)(cid:0)(cid:18)

(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU

@@R
-

(cid:0)
A
A

(cid:1)
(cid:1)
@

2
11

1
1

1
0

h
h
h
h

00
1

A
A
(cid:0)

@
(cid:1)
(cid:1)

3
11

1
1

2
0

h
h
h
h

00
2

A

(cid:0)
A
(cid:0)
A

-
(cid:0)(cid:0)(cid:18)

A
(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU
(cid:1)

(cid:1)
@
(cid:1)
@

(cid:1)

@@R
-

4
11

2
1

3
0

h
h
h
h

00
4

A
A
(cid:0)

@
(cid:1)
(cid:1)

-
(cid:0)(cid:0)(cid:18)

(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU

@@R
-

(cid:0)
A
A

(cid:1)
(cid:1)
@

-
(cid:0)(cid:0)(cid:18)

(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU

@@R
-

(cid:0)
A
A

(cid:1)
(cid:1)
@

6
11

4
1

4
0

h
h
h
h

00
7

A
A
(cid:0)

@
(cid:1)
(cid:1)

10
11

7
1

6
0

h
h
h
h

00
11

A

(cid:0)
A
(cid:0)
A

-
(cid:0)(cid:0)(cid:18)

A
(cid:1)(cid:1)(cid:21)
A
(cid:1)
AAU
(cid:1)

(cid:1)
@
(cid:1)
@

(cid:1)

@@R
-

17
11

11
1

10
0

h
h
h
h

00
17

(cid:1)(cid:1)(cid:21)
(cid:1)

(cid:1)
(cid:1)

(cid:1)
-(cid:1)

h00

M1 = 1

M2 = 2

M3 = 3

M4 = 5

M5 = 8

M6 = 13

M7 = 21

M8 = 34

11

1
1

h
h
h
h

0

00

(cid:0)(cid:0)(cid:18)

A

(cid:0)
A
(cid:0)
A
@

(cid:0)
(cid:1)
@
(cid:1)
@

(cid:1)

A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)

@@R

(cid:1)(cid:1)(cid:21)
(cid:1)

(cid:1)
(cid:1)

(cid:1)
(cid:1)

h00

1
11

h
h
h
h

1

1
0

00

(cid:0)(cid:0)(cid:18)

A

(cid:0)
A
(cid:0)
A
@

(cid:0)
(cid:1)
@
(cid:1)
@

(cid:1)

A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)

@@R

11

1
1

1
0

h
h
h
h

A
A
(cid:0)
@

(cid:0)
@
(cid:1)
(cid:1)

00
1

(cid:0)(cid:0)(cid:18)

(cid:0)
A
A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)
(cid:1)
@

@@R

1
11

2
1

1
0

h
h
h
h

00
1

(cid:0)(cid:0)(cid:18)

A

(cid:0)
A
(cid:0)
A
@

(cid:0)
(cid:1)
@
(cid:1)
@

(cid:1)

A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)

@@R

2
11

2
1

3
0

h
h
h
h

A
A
(cid:0)
@

(cid:0)
@
(cid:1)
(cid:1)

(cid:0)(cid:0)(cid:18)

(cid:0)
A
A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)
(cid:1)
@

@@R

2
11

4
1

4
0

h
h
h
h

A
A
(cid:0)
@

(cid:0)
@
(cid:1)
(cid:1)

00
3

(cid:0)(cid:0)(cid:18)

(cid:0)
A
A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)
(cid:1)
@

@@R

00
1

4
11

7
1

6
0

h
h
h
h

00
4

(cid:0)(cid:0)(cid:18)

A

(cid:0)
A
(cid:0)
A
@

(cid:0)
(cid:1)
@
(cid:1)
@

(cid:1)

A
(cid:0)(cid:0)(cid:18)
(cid:1)(cid:1)(cid:21)
A
(cid:0)
@
(cid:1)
AAU
@@R
(cid:1)

@@R

7
11

10
1

11
0

h
h
h
h

00
6

Figure 17.5. Counting the number of paths in the trellises of channels A, B, and C. We assume that at
the start the (cid:12)rst bit is preceded by 00, so that for channels A and B, any initial character
is permitted, but for channel C, the (cid:12)rst character must be a 1.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17.3: Counting the number of possible messages

253

n

1
2
3
4
5
6
7
8
9
10
11
12
100
200
300
400

Mn Mn=Mn(cid:0)1

log2 Mn

1
n log2 Mn

Figure 17.6. Counting the number
of paths in the trellis of channel A.

2
3
5
8
13
21
34
55
89
144
233
377
9(cid:2)1020
7(cid:2)1041
6(cid:2)1062
5(cid:2)1083

1.500
1.667
1.600
1.625
1.615
1.619
1.618
1.618
1.618
1.618
1.618
1.618
1.618
1.618
1.618

1.0
1.6
2.3
3.0
3.7
4.4
5.1
5.8
6.5
7.2
7.9
8.6
69.7
139.1
208.5
277.9

1.00
0.79
0.77
0.75
0.74
0.73
0.73
0.72
0.72
0.72
0.71
0.71
0.70
0.70
0.70
0.69

valid sequences is the number of paths. For the purpose of counting how many
paths there are through the trellis, we can ignore the labels on the edges and
summarize the trellis section by the connection matrix A, in which Ass0 = 1
if there is an edge from state s to s0, and Ass0 = 0 otherwise ((cid:12)gure 17.2d).
Figure 17.3 shows the state diagrams, trellis sections and connection matrices
for channels B and C.

Let’s count the number of paths for channel A by message-passing in its
trellis. Figure 17.4 shows the (cid:12)rst few steps of this counting process, and
(cid:12)gure 17.5a shows the number of paths ending in each state after n steps for
n = 1; : : : ; 8. The total number of paths of length n, Mn, is shown along the
top. We recognize Mn as the Fibonacci series.

. Exercise 17.6.[1 ] Show that the ratio of successive terms in the Fibonacci series

tends to the golden ratio,

1 + p5

2

(cid:13) (cid:17)

= 1:618:

(17.11)

Thus, to within a constant factor, MN scales as MN (cid:24) (cid:13)N as N ! 1, so the
capacity of channel A is

C = lim

1
N

log2(cid:2)constant (cid:1) (cid:13)N(cid:3) = log2 (cid:13) = log2 1:618 = 0:694:

(17.12)

How can we describe what we just did? The count of the number of paths

is a vector c(n); we can obtain c(n+1) from c(n) using:

c(n+1) = Ac(n):

(17.13)

So

c(N ) = AN c(0);

(17.14)
where c(0) is the state count before any symbols are transmitted. In (cid:12)gure 17.5
we assumed c(0) = [0; 1]T, i.e., that either of the two symbols is permitted at
s = c(n) (cid:1) n. In the limit,

the outset. The total number of paths is Mn =Ps c(n)

c(N ) becomes dominated by the principal right-eigenvector of A.

c(N ) ! constant (cid:1) (cid:21)N

1 e(0)
R :

(17.15)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

254

17 | Communication over Constrained Noiseless Channels

Here, (cid:21)1 is the principal eigenvalue of A.

So to (cid:12)nd the capacity of any constrained channel, all we need to do is (cid:12)nd

the principal eigenvalue, (cid:21)1, of its connection matrix. Then

C = log2 (cid:21)1:

(17.16)

17.4 Back to our model channels

Comparing (cid:12)gure 17.5a and (cid:12)gures 17.5b and c it looks as if channels B and
C have the same capacity as channel A. The principal eigenvalues of the three
trellises are the same (the eigenvectors for channels A and B are given at the
bottom of table C.4, p.608). And indeed the channels are intimately related.

- t

z1

hd
-

z0
6
(cid:27)
(cid:8)

s

z1

hd (cid:27)
z0
t
? - s
(cid:8)

-

Figure 17.7. An accumulator and
a di(cid:11)erentiator.

Equivalence of channels A and B

If we take any valid string s for channel A and pass it through an accumulator,
obtaining t de(cid:12)ned by:

t1 = s1
tn = tn(cid:0)1 + sn mod 2 for n (cid:21) 2,

(17.17)

then the resulting string is a valid string for channel B, because there are no
11s in s, so there are no isolated digits in t. The accumulator is an invertible
operator, so, similarly, any valid string t for channel B can be mapped onto a
valid string s for channel A through the binary di(cid:11)erentiator,

s1 = t1
sn = tn (cid:0) tn(cid:0)1 mod 2 for n (cid:21) 2.

(17.18)

Because + and (cid:0) are equivalent in modulo 2 arithmetic, the di(cid:11)erentiator is
also a blurrer, convolving the source stream with the (cid:12)lter (1; 1).

Channel C is also intimately related to channels A and B.

. Exercise 17.7.[1, p.257] What is the relationship of channel C to channels A

and B?

17.5 Practical communication over constrained channels

OK, how to do it in practice? Since all three channels are equivalent, we can
concentrate on channel A.

Fixed-length solutions

We start with explicitly-enumerated codes. The code in the table 17.8 achieves
a rate of 3/5 = 0:6.

. Exercise 17.8.[1, p.257] Similarly, enumerate all strings of length 8 that end in
the zero state. (There are 34 of them.) Hence show that we can map 5
bits (32 source strings) to 8 transmitted bits and achieve rate 5/8 = 0:625.

What rate can be achieved by mapping an integer number of source bits
to N = 16 transmitted bits?

s

1
2
3
4
5
6
7
8

c(s)

00000
10000
01000
00100
00010
10100
01010
10010

Table 17.8. A runlength-limited
code for channel A.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17.5: Practical communication over constrained channels

255

Optimal variable-length solution

The optimal way to convey information over the constrained channel is to (cid:12)nd
the optimal transition probabilities for all points in the trellis, Qs0js, and make
transitions with these probabilities.

When discussing channel A, we showed that a sparse source with density
f = 0:38, driving code C2, would achieve capacity. And we know how to
make sparsi(cid:12)ers (Chapter 6): we design an arithmetic code that is optimal
for compressing a sparse source; then its associated decoder gives an optimal
mapping from dense (i.e., random binary) strings to sparse strings.

The task of (cid:12)nding the optimal probabilities is given as an exercise.

Exercise 17.9.[3 ] Show that the optimal transition probabilities Q can be found

as follows.

Find the principal right- and left-eigenvectors of A, that is the solutions
of Ae(R) = (cid:21)e(R) and e(L)T
with largest eigenvalue (cid:21). Then
construct a matrix Q whose invariant distribution is proportional to
e(R)
i

A = (cid:21)e(L)T

, namely

e(L)
i

Qs0js =

e(L)
s0 As0s
(cid:21)e(L)

s

:

(17.19)

[Hint: exercise 16.2 (p.245) might give helpful cross-fertilization here.]

. Exercise 17.10.[3, p.258] Show that when sequences are generated using the op-
timal transition probability matrix (17.19), the entropy of the resulting
sequence is asymptotically log2 (cid:21) per symbol. [Hint: consider the condi-
tional entropy of just one symbol given the previous one, assuming the
previous one’s distribution is the invariant distribution.]

In practice, we would probably use (cid:12)nite-precision approximations to the
optimal variable-length solution. One might dislike variable-length solutions
because of the resulting unpredictability of the actual encoded length in any
particular case. Perhaps in some applications we would like a guarantee that
the encoded length of a source (cid:12)le of size N bits will be less than a given
length such as N=(C + (cid:15)). For example, a disk drive is easier to control if
all blocks of 512 bytes are known to take exactly the same amount of disk
real-estate. For some constrained channels we can make a simple modi(cid:12)cation
to our variable-length encoding and o(cid:11)er such a guarantee, as follows. We
(cid:12)nd two codes, two mappings of binary strings to variable-length encodings,
having the property that for any source string x, if the encoding of x under
the (cid:12)rst code is shorter than average, then the encoding of x under the second
code is longer than average, and vice versa. Then to transmit a string x we
encode the whole string with both codes and send whichever encoding has the
shortest length, prepended by a suitably encoded single bit to convey which
of the two codes is being used.

. Exercise 17.11.[3C, p.258] How many valid sequences of length 8 starting with
a 0 are there for the run-length-limited channels shown in (cid:12)gure 17.9?

What are the capacities of these channels?

Using a computer, (cid:12)nd the matrices Q for generating a random path
through the trellises of the channel A, and the two run-length-limited
channels shown in (cid:12)gure 17.9.

0 1 0
0 0 1

1 1 1 3
5

1

1

0

0 1 0 0
0 0 1 0
0 0 0 1
1 1 1 1

2
4
2
664

2

0

1
0
0

0

0

3

2

1
0
0

1

1

3
775 1

0

Figure 17.9. State diagrams and
connection matrices for channels
with maximum runlengths for 1s
equal to 2 and 3.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

256

17 | Communication over Constrained Noiseless Channels

. Exercise 17.12.[3, p.258] Consider the run-length-limited channel in which any
length of run of 0s is permitted, and the maximum run length of 1s is a
large number L such as nine or ninety.

Estimate the capacity of this channel. (Give the (cid:12)rst two terms in a
series expansion involving L.)

What, roughly, is the form of the optimal matrix Q for generating a
random path through the trellis of this channel? Focus on the values of
the elements Q1j0, the probability of generating a 1 given a preceding 0,
and QLjL(cid:0)1, the probability of generating a 1 given a preceding run of
L(cid:0)1 1s. Check your answer by explicit computation for the channel in
which the maximum runlength of 1s is nine.

17.6 Variable symbol durations

We can add a further frill to the task of communicating over constrained
channels by assuming that the symbols we send have di(cid:11)erent durations, and
that our aim is to communicate at the maximum possible rate per unit time.
Such channels can come in two (cid:13)avours: unconstrained, and constrained.

Unconstrained channels with variable symbol durations

We encountered an unconstrained noiseless channel with variable symbol du-
rations in exercise 6.18 (p.125). Solve that problem, and you’ve done this
topic. The task is to determine the optimal frequencies with which the sym-
bols should be used, given their durations.

There is a nice analogy between this task and the task of designing an
optimal symbol code (Chapter 4). When we make an binary symbol code
for a source with unequal probabilities pi, the optimal message lengths are
l(cid:3)i = log2

1/pi, so

pi = 2(cid:0)l(cid:3)
i :

(17.20)

Similarly, when we have a channel whose symbols have durations li (in some
units of time), the optimal probability with which those symbols should be
used is

p(cid:3)i = 2(cid:0)(cid:12)li;

(17.21)

where (cid:12) is the capacity of the channel in bits per unit time.

Constrained channels with variable symbol durations

Once you have grasped the preceding topics in this chapter, you should be
able to (cid:12)gure out how to de(cid:12)ne and (cid:12)nd the capacity of these, the trickiest
constrained channels.

Exercise 17.13.[3 ] A classic example of a constrained channel with variable

symbol durations is the ‘Morse’ channel, whose symbols are

the dot
the dash
the short space (used between letters in morse code)
the long space (used between words)

d,
D,
s, and
S;

the constraints are that spaces may only be followed by dots and dashes.

Find the capacity of this channel in bits per unit time assuming (a) that
all four symbols have equal durations; or (b) that the symbol durations
are 2, 4, 3 and 6 time units respectively.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17.7: Solutions

257

Exercise 17.14.[4 ] How well-designed is Morse code for English (with, say, the

probability distribution of (cid:12)gure 2.1)?

Exercise 17.15.[3C ] How di(cid:14)cult is it to get DNA into a narrow tube?

To an information theorist, the entropy associated with a constrained
channel reveals how much information can be conveyed over it. In sta-
tistical physics, the same calculations are done for a di(cid:11)erent reason: to
predict the thermodynamics of polymers, for example.

As a toy example, consider a polymer of length N that can either sit
in a constraining tube, of width L, or in the open where there are no
constraints. In the open, the polymer adopts a state drawn at random
from the set of one dimensional random walks, with, say, 3 possible
directions per step. The entropy of this walk is log 3 per step, i.e., a
[The free energy of the polymer is de(cid:12)ned to be (cid:0)kT
total of N log 3.
times this, where T is the temperature.] In the tube, the polymer’s one-
dimensional walk can go in 3 directions unless the wall is in the way, so
the connection matrix is, for example (if L = 10),

1 1 0 0 0 0 0 0 0 0
1 1 1 0 0 0 0 0 0 0
0 1 1 1 0 0 0 0 0 0
0 0 1 1 1 0 0 0 0 0
0 0 0 1 1 1 0 0 0 0

. . .

. . .

. . .

0 0 0 0 0 0 0 1 1 1
0 0 0 0 0 0 0 0 1 1

:

3

777777777775

2

666666666664

Now, what is the entropy of the polymer? What is the change in entropy
associated with the polymer entering the tube? If possible, obtain an
expression as a function of L. Use a computer to (cid:12)nd the entropy of the
walk for a particular value of L, e.g. 20, and plot the probability density
of the polymer’s transverse location in the tube.

Notice the di(cid:11)erence in capacity between two channels, one constrained
and one unconstrained, is directly proportional to the force required to
pull the DNA into the tube.

17.7 Solutions

Solution to exercise 17.5 (p.250). A (cid:12)le transmitted by C2 contains, on aver-
age, one-third 1s and two-thirds 0s.

If f = 0:38, the fraction of 1s is f =(1 + f ) = ((cid:13) (cid:0) 1:0)=(2(cid:13) (cid:0) 1:0) = 0:2764.
Solution to exercise 17.7 (p.254). A valid string for channel C can be obtained
from a valid string for channel A by (cid:12)rst inverting it [1 ! 0; 0 ! 1], then
passing it through an accumulator. These operations are invertible, so any
valid string for C can also be mapped onto a valid string for A. The only
proviso here comes from the edge e(cid:11)ects. If we assume that the (cid:12)rst character
transmitted over channel C is preceded by a string of zeroes, so that the (cid:12)rst
character is forced to be a 1 ((cid:12)gure 17.5c) then the two channels are exactly
equivalent only if we assume that channel A’s (cid:12)rst character must be a zero.

Solution to exercise 17.8 (p.254). With N = 16 transmitted bits, the largest
integer number of source bits that can be encoded is 10, so the maximum rate
of a (cid:12)xed length code with N = 16 is 0.625.

Figure 17.10. Model of DNA
squashed in a narrow tube. The
DNA will have a tendency to pop
out of the tube, because, outside
the tube, its random walk has
greater entropy.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

258

17 | Communication over Constrained Noiseless Channels

Solution to exercise 17.10 (p.255). Let the invariant distribution be

P (s) = (cid:11)e(L)

s e(R)

s

;

(17.22)

where (cid:11) is a normalization constant. The entropy of St given St(cid:0)1, assuming
St(cid:0)1 comes from the invariant distribution, is

H(StjSt(cid:0)1) = (cid:0)Xs;s0
= (cid:0)Xs;s0
hlog e(L)

e(L)
s0 As0s
(cid:21)

(cid:11) e(R)

s

= (cid:0)Xs;s0

P (s)P (s0js) log P (s0js)

(cid:11)e(L)

s e(R)

s

e(L)
s0 As0s
(cid:21)e(L)

s

log

e(L)
s0 As0s
(cid:21)e(L)

s

s i :
s0 + log As0s (cid:0) log (cid:21) (cid:0) log e(L)

(17.23)

(17.24)

(17.25)

Now, As0s is either 0 or 1, so the contributions from the terms proportional to
As0s log As0s are all zero. So

Here, as in Chapter 4, St denotes
the ensemble whose random
variable is the state st.

H(StjSt(cid:0)1) = log (cid:21) + (cid:0)

log e(L)

s0 +

As0se(R)

s ! e(L)

s0

(cid:11)

(cid:21)Xs0  Xs
s0 As0s! e(R)

e(L)

s

log e(L)

s

(17.26)

(cid:11)

(cid:21)Xs  Xs0
(cid:21)Xs0

(cid:21)e(R)

s0 e(L)

s0

(cid:11)

= log (cid:21) (cid:0)
= log (cid:21):

log e(L)

s0 +

(cid:11)

(cid:21)Xs

(cid:21)e(L)

s e(R)

s

log e(L)

s

(17.27)

(17.28)

Solution to exercise 17.11 (p.255). The principal eigenvalues of the connection
matrices of the two channels are 1.839 and 1.928. The capacities (log (cid:21)) are
0.879 and 0.947 bits.

Solution to exercise 17.12 (p.256). The channel is similar to the unconstrained
binary channel; runs of length greater than L are rare if L is large, so we only
expect weak di(cid:11)erences from this channel; these di(cid:11)erences will show up in
contexts where the run length is close to L. The capacity of the channel is
very close to one bit.

A lower bound on the capacity is obtained by considering the simple
variable-length code for this channel which replaces occurrences of the maxi-
mum runlength string 111: : :1 by 111: : :10, and otherwise leaves the source (cid:12)le
unchanged. The average rate of this code is 1=(1 + 2(cid:0)L) because the invariant
distribution will hit the ‘add an extra zero’ state a fraction 2(cid:0)L of the time.
We can reuse the solution for the variable-length channel in exercise 6.18

(p.125). The capacity is the value of (cid:12) such that the equation

Z((cid:12)) =

2(cid:0)(cid:12)l = 1

L+1

Xl=1

(17.29)

is satis(cid:12)ed. The L+1 terms in the sum correspond to the L+1 possible strings
that can be emitted, 0, 10, 110, : : : , 11: : :10. The sum is exactly given by:

Z((cid:12)) = 2(cid:0)(cid:12)(cid:0)2(cid:0)(cid:12)(cid:1)L+1 (cid:0) 1
2(cid:0)(cid:12) (cid:0) 1

:

(17.30)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

17.7: Solutions

259

L

2
3
4
5
6
9

(cid:12)

True capacity

0.910
0.955
0.977
0.9887
0.9944
0.9993

0.879
0.947
0.975
0.9881
0.9942
0.9993

N

arn =

"Here we used
We anticipate that (cid:12) should be a little less than 1 in order for Z((cid:12)) to
equal 1. Rearranging and solving approximately for (cid:12), using ln(1 + x) ’ x,

a(rN +1 (cid:0) 1)

Xn=0

r (cid:0) 1

:#

Z((cid:12)) = 1
) (cid:12) ’ 1 (cid:0) 2(cid:0)(L+2)= ln 2:

(17.31)

(17.32)

We evaluated the true capacities for L = 2 and L = 3 in an earlier exercise.
The table compares the approximate capacity (cid:12) with the true capacity for a
selection of values of L.

The element Q1j0 will be close to 1=2 (just a tiny bit larger), since in the
unconstrained binary channel Q1j0 = 1=2. When a run of length L (cid:0) 1 has
occurred, we e(cid:11)ectively have a choice of printing 10 or 0. Let the probability of
selecting 10 be f . Let us estimate the entropy of the remaining N characters
in the stream as a function of f , assuming the rest of the matrix Q to have
been set to its optimal value. The entropy of the next N characters in the
stream is the entropy of the (cid:12)rst bit, H2(f ), plus the entropy of the remaining
characters, which is roughly (N (cid:0) 1) bits if we select 0 as the (cid:12)rst bit and
(N(cid:0)2) bits if 1 is selected. More precisely, if C is the capacity of the channel
(which is roughly 1),

H(the next N chars) ’ H2(f ) + [(N (cid:0) 1)(1 (cid:0) f ) + (N (cid:0) 2)f ] C

= H2(f ) + N C (cid:0) f C ’ H2(f ) + N (cid:0) f: (17.33)

Di(cid:11)erentiating and setting to zero to (cid:12)nd the optimal f , we obtain:

log2

1 (cid:0) f
f ’ 1 )

1 (cid:0) f
f ’ 2 ) f ’ 1=3:

(17.34)

The probability of emitting a 1 thus decreases from about 0.5 to about 1=3 as
the number of emitted 1s increases.

Here is the optimal matrix:

0
0

:4669

0
0
0

0

:4287

0 :3334
0
0
0
0
0
0
0
0
:4998
1 :6666 :5713 :5331 :5159 :5077 :5037 :5017 :5007 :5002

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0

0
0
0
0
0
0
0

0
0
0
0
0
0

0
0
0
0
0

0
0
0
0
0
0

0
0

0
0
0
0
0

0
0
0

0
0
0
0

0
0
0
0

:4993

0

:4923

:4841

:4963

:4983

2

666666666666664

:

(17.35)

3

777777777777775

Our rough theory works.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

18

Crosswords and Codebreaking

In this chapter we make a random walk through a few topics related to lan-
guage modelling.

18.1 Crosswords

The rules of crossword-making may be thought of as de(cid:12)ning a constrained
channel. The fact that many valid crosswords can be made demonstrates that
this constrained channel has a capacity greater than zero.

There are two archetypal crossword formats. In a ‘type A’ (or American)
crossword, every row and column consists of a succession of words of length 2
or more separated by one or more spaces. In a ‘type B’ (or British) crossword,
each row and column consists of a mixture of words and single characters,
separated by one or more spaces, and every character lies in at least one word
(horizontal or vertical). Whereas in a type A crossword every letter lies in a
horizontal word and a vertical word, in a typical type B crossword only about
half of the letters do so; the other half lie in one word only.

Type A crosswords are harder to create than type B because of the con-
straint that no single characters are permitted. Type B crosswords are gener-
ally harder to solve because there are fewer constraints per character.

Why are crosswords possible?

If a language has no redundancy, then any letters written on a grid form a
valid crossword. In a language with high redundancy, on the other hand, it
is hard to make crosswords (except perhaps a small number of trivial ones).
The possibility of making crosswords in a language thus demonstrates a bound
on the redundancy of that language. Crosswords are not normally written in
genuine English. They are written in ‘word-English’, the language consisting
of strings of words from a dictionary, separated by spaces.

D
U
F
F

S
T
U
D

G
I
L
D
S

B

P

V

J

D

P

B

A
F
A
R

T
I
T
O

A
D
I
E
U

A
V
A
L
A
N
C
H
E

U
S
H
E
R

T
O
T
O

O
L
A
V

R
I
D
E
R

N

R

L

A

N

E

I

I

A
S
H

M
O
T
H
E
R
G
O
O
S
E

G
A
L
L
E
O
N

N
E
T
T
L
E
S

E
V
I
L
S

C
U
L
T

E

I

N

O

I

W

T

S
T
R
E
S
S

S
O
L
E

B
A
S

R
O
A
S
T
B
E
E
F

N
O
B
E
L

C
I
T
E
S

U
T
T
E
R

R
O
T

M

I

E

U

A

E

H
E
I
R

S
N
E
E
R

C
O
R
E

B
R
E
M
N
E
R

R
O
T
A
T
E
S

M
U
M

A
T
L
A
S

M
A
T
T
E

A

N

E

H

C

T

O
P
E

P
A
U
L

M
I
S
H
A
P

K
I
T
E
S

A
U
S
T
R
A
L
I
A

E
P
I
C

C
A
R
T
E

E

L

P

T

A

E

U

S
I
S
T
E
R
K
E
N
N
Y

R
A
H

R
O
C
K
E
T
S

E
X
C
U
S
E
S

A
L
O
H
A

I
R
O
N

T
R
E
E

I

A

T

O

P

K

T

T

S
I
R
E
S

L
A
T
E

E
A
R
L

E
L
T
O
N

D
E
S
P
E
R
A
T
E

S
A
B
R
E

Y
S
E
R

A
T
O
M

S

S

A

Y

R

R

N

. Exercise 18.1.[2 ] Estimate the capacity of word-English, in bits per character.
[Hint: think of word-English as de(cid:12)ning a constrained channel (Chapter
17) and see exercise 6.18 (p.125).]

Figure 18.1. Crosswords of types
A (American) and B (British).

The fact that many crosswords can be made leads to a lower bound on the
entropy of word-English.

For simplicity, we now model word-English by Wenglish, the language in-
troduced in section 4.1 which consists of W words all of length L. The entropy
of such a language, per character, including inter-word spaces, is:

HW (cid:17)

log2 W
L + 1

:

260

(18.1)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

18.1: Crosswords

261

We’ll (cid:12)nd that the conclusions we come to depend on the value of HW and
are not terribly sensitive to the value of L. Consider a large crossword of size
S squares in area. Let the number of words be fwS and let the number of
letter-occupied squares be f1S. For typical crosswords of types A and B made
of words of length L, the two fractions fw and f1 have roughly the values in
table 18.2.

We now estimate how many crosswords there are of size S using our simple
model of Wenglish. We assume that Wenglish is created at random by gener-
ating W strings from a monogram (i.e., memoryless) source with entropy H0.
If, for example, the source used all A = 26 characters with equal probability
then H0 = log2 A = 4:7 bits. If instead we use Chapter 2’s distribution then
the entropy is 4.2. The redundancy of Wenglish stems from two sources:
it
tends to use some letters more than others; and there are only W words in
the dictionary.

Let’s now count how many crosswords there are by imagining (cid:12)lling in
the squares of a crossword at random using the same distribution that pro-
duced the Wenglish dictionary and evaluating the probability that this random
scribbling produces valid words in all rows and columns. The total number of
typical (cid:12)llings-in of the f1S squares in the crossword that can be made is

The probability that one word of length L is validly (cid:12)lled-in is

jTj = 2f1SH0:

(cid:12) =

W
2LH0

;

(18.2)

(18.3)

and the probability that the whole crossword, made of fwS words, is validly
(cid:12)lled-in by a single typical in-(cid:12)lling is approximately

(cid:12)fwS:

(18.4)

So the log of the number of valid crosswords of size S is estimated to be

log (cid:12)fwSjTj = S [(f1 (cid:0) fwL)H0 + fw log W ]

= S [(f1 (cid:0) fwL)H0 + fw(L + 1)HW ] ;

which is an increasing function of S only if

(f1 (cid:0) fwL)H0 + fw(L + 1)HW > 0:

(18.5)

(18.6)

(18.7)

So arbitrarily many crosswords can be made only if there’s enough words in
the Wenglish dictionary that

HW >

(fwL (cid:0) f1)
fw(L + 1)

H0:

(18.8)

Plugging in the values of f1 and fw from table 18.2, we (cid:12)nd the following.

A

2

B

1

L + 1

L

L + 1

L + 1
3
L
4

L + 1

fw

f1

Table 18.2. Factors fw and f1 by
which the number of words and
number of letter-squares
respectively are smaller than the
total number of squares.

This calculation underestimates
the number of valid Wenglish
crosswords by counting only
crosswords (cid:12)lled with ‘typical’
strings. If the monogram
distribution is non-uniform then
the true count is dominated by
‘atypical’ (cid:12)llings-in, in which
crossword-friendly words appear
more often.

Crossword type
A
Condition for crosswords HW > 1
2

B
L+1 H0 HW > 1

L

4

L
L+1 H0

If we set H0 = 4:2 bits and assume there are W = 4000 words in a normal
English-speaker’s dictionary, all with length L = 5, then we (cid:12)nd that the
condition for crosswords of type B is satis(cid:12)ed, but the condition for crosswords
of type A is only just satis(cid:12)ed. This (cid:12)ts with my experience that crosswords
of type A usually contain more obscure words.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

262

Further reading

18 | Crosswords and Codebreaking

Figure 18.3. A binary pattern in
which every pixel is adjacent to
four black and four white pixels.

These observations about crosswords were (cid:12)rst made by Shannon (1948); I
learned about them from Wolf and Siegel (1998). The topic is closely related
to the capacity of two-dimensional constrained channels. An example of a
two-dimensional constrained channel is a two-dimensional bar-code, as seen
on parcels.

Exercise 18.2.[3 ] A two-dimensional channel is de(cid:12)ned by the constraint that,
of the eight neighbours of every interior pixel in an N (cid:2) N rectangular
grid, four must be black and four white. (The counts of black and white
pixels around boundary pixels are not constrained.) A binary pattern
satisfying this constraint is shown in (cid:12)gure 18.3. What is the capacity
of this channel, in bits per pixel, for large N ?

18.2 Simple language models

The Zipf{Mandelbrot distribution

The crudest model for a language is the monogram model, which asserts that
each successive word is drawn independently from a distribution over words.
What is the nature of this distribution over words?

Zipf’s law (Zipf, 1949) asserts that the probability of the rth most probable

word in a language is approximately

P (r) =

(cid:20)
r(cid:11) ;

(18.9)

where the exponent (cid:11) has a value close to 1, and (cid:20) is a constant. According
to Zipf, a log{log plot of frequency versus word-rank should show a straight
line with slope (cid:0)(cid:11).
eter v, asserting that the probabilities are given by

Mandelbrot’s (1982) modi(cid:12)cation of Zipf’s law introduces a third param-

P (r) =

(cid:20)

(r + v)(cid:11) :

(18.10)

For some documents, such as Jane Austen’s Emma, the Zipf{Mandelbrot dis-
tribution (cid:12)ts well { (cid:12)gure 18.4.

Other documents give distributions that are not so well (cid:12)tted by a Zipf{
Mandelbrot distribution. Figure 18.5 shows a plot of frequency versus rank for
the LATEX source of this book. Qualitatively, the graph is similar to a straight
line, but a curve is noticeable. To be fair, this source (cid:12)le is not written in
pure English { it is a mix of English, maths symbols such as ‘x’, and LATEX
commands.

0.1

0.01

to theand
of
I

is

Harriet

0.001

0.0001

1e-05

1

information

probability

10

100

1000

10000

Figure 18.4. Fit of the
Zipf{Mandelbrot distribution
(18.10) (curve) to the empirical
frequencies of words in Jane
Austen’s Emma (dots). The (cid:12)tted
parameters are (cid:20) = 0:56; v = 8:0;
(cid:11) = 1:26.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

18.2: Simple language models

263

0.1

0.01

the

of a is

x

0.001

0.0001

0.00001

0.1

0.01

0.001

0.0001

0.00001

probability

information

Shannon
Bayes

1

10

100

1000

alpha=1

alpha=10

alpha=100

alpha=1000

book

1

10

100

1000

10000

Figure 18.5. Log{log plot of
frequency versus rank for the
words in the LATEX (cid:12)le of this
book.

Figure 18.6. Zipf plots for four
‘languages’ randomly generated
from Dirichlet processes with
parameter (cid:11) ranging from 1 to
1000. Also shown is the Zipf plot
for this book.

The Dirichlet process

Assuming we are interested in monogram models for languages, what model
should we use? One di(cid:14)culty in modelling a language is the unboundedness
of vocabulary. The greater the sample of language, the greater the number
of words encountered. A generative model for a language should emulate
this property.
If asked ‘what is the next word in a newly-discovered work
of Shakespeare?’ our probability distribution over words must surely include
some non-zero probability for words that Shakespeare never used before. Our
generative monogram model for language should also satisfy a consistency
rule called exchangeability.
If we imagine generating a new language from
our generative model, producing an ever-growing corpus of text, all statistical
properties of the text should be homogeneous: the probability of (cid:12)nding a
particular word at a given location in the stream of text should be the same
everywhere in the stream.

The Dirichlet process model is a model for a stream of symbols (which we
think of as ‘words’) that satis(cid:12)es the exchangeability rule and that allows the
vocabulary of symbols to grow without limit. The model has one parameter
(cid:11). As the stream of symbols is produced, we identify each new symbol by a
unique integer w. When we have seen a stream of length F symbols, we de(cid:12)ne
the probability of the next symbol in terms of the counts fFwg of the symbols
seen so far thus: the probability that the next symbol is a new symbol, never
seen before, is

(cid:11)

(18.11)

(18.12)

:

F + (cid:11)

The probability that the next symbol is symbol w is

Fw

F + (cid:11)

:

Figure 18.6 shows Zipf plots (i.e., plots of symbol frequency versus rank) for
million-symbol ‘documents’ generated by Dirichlet process priors with values
of (cid:11) ranging from 1 to 1000.

It is evident that a Dirichlet process is not an adequate model for observed

distributions that roughly obey Zipf’s law.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

264

18 | Crosswords and Codebreaking

0.1

0.01

0.001

0.0001

0.00001

1

10

100

1000

10000

Figure 18.7. Zipf plots for the
words of two ‘languages’
generated by creating successive
characters from a Dirichlet
process with (cid:11) = 2, and declaring
one character to be the space
character. The two curves result
from two di(cid:11)erent choices of the
space character.

With a small tweak, however, Dirichlet processes can produce rather nice
Zipf plots. Imagine generating a language composed of elementary symbols
using a Dirichlet process with a rather small value of the parameter (cid:11), so that
the number of reasonably frequent symbols is about 27. If we then declare
one of those symbols (now called ‘characters’ rather than words) to be a space
character, then we can identify the strings between the space characters as
‘words’. If we generate a language in this way then the frequencies of words
often come out as very nice Zipf plots, as shown in (cid:12)gure 18.7. Which character
is selected as the space character determines the slope of the Zipf plot { a less
probable space character gives rise to a richer language with a shallower slope.

18.3 Units of information content

The information content of an outcome, x, whose probability is P (x), is de(cid:12)ned
to be

h(x) = log

:

(18.13)

1

P (x)

The entropy of an ensemble is an average information content,

H(X) =Xx

P (x) log

1

P (x)

:

(18.14)

When we compare hypotheses with each other in the light of data, it is of-
ten convenient to compare the log of the probability of the data under the
alternative hypotheses,

‘log evidence for Hi’ = log P (D jHi);

(18.15)

or, in the case where just two hypotheses are being compared, we evaluate the
‘log odds’,

log

P (D jH1)
P (D jH2)

;

(18.16)

which has also been called the ‘weight of evidence in favour of H1’. The
log evidence for a hypothesis, log P (D jHi) is the negative of the information
content of the data D: if the data have large information content, given a hy-
pothesis, then they are surprising to that hypothesis; if some other hypothesis
is not so surprised by the data, then that hypothesis becomes more probable.
‘Information content’, ‘surprise value’, and log likelihood or log evidence are
the same thing.

All these quantities are logarithms of probabilities, or weighted sums of
logarithms of probabilities, so they can all be measured in the same units.
The units depend on the choice of the base of the logarithm.

The names that have been given to these units are shown in table 18.8.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

18.4: A taste of Banburismus

265

Unit

bit
nat
ban

deciban (db)

Expression that has those units

Table 18.8. Units of measurement
of information content.

log2 p
loge p
log10 p
10 log10 p

The bit is the unit that we use most in this book. Because the word ‘bit’
has other meanings, a backup name for this unit is the shannon. A byte is
8 bits. A megabyte is 220 ’ 106 bytes. If one works in natural logarithms,
information contents and weights of evidence are measured in nats. The most
interesting units are the ban and the deciban.

The history of the ban

Let me tell you why a factor of ten in probability is called a ban. When Alan
Turing and the other codebreakers at Bletchley Park were breaking each new
day’s Enigma code, their task was a huge inference problem: to infer, given
the day’s cyphertext, which three wheels were in the Enigma machines that
day; what their starting positions were; what further letter substitutions were
in use on the steckerboard; and, not least, what the original German messages
were. These inferences were conducted using Bayesian methods (of course!),
and the chosen units were decibans or half-decibans, the deciban being judged
the smallest weight of evidence discernible to a human. The evidence in favour
of particular hypotheses was tallied using sheets of paper that were specially
printed in Banbury, a town about 30 miles from Bletchley. The inference task
was known as Banburismus, and the units in which Banburismus was played
were called bans, after that town.

18.4 A taste of Banburismus

The details of the code-breaking methods of Bletchley Park were kept secret
for a long time, but some aspects of Banburismus can be pieced together.
I hope the following description of a small part of Banburismus is not too
inaccurate.1

How much information was needed? The number of possible settings of
the Enigma machine was about 8 (cid:2) 1012. To deduce the state of the machine,
‘it was therefore necessary to (cid:12)nd about 129 decibans from somewhere’, as
Good puts it. Banburismus was aimed not at deducing the entire state of the
machine, but only at (cid:12)guring out which wheels were in use; the logic-based
bombes, fed with guesses of the plaintext (cribs), were then used to crack what
the settings of the wheels were.

The Enigma machine, once its wheels and plugs were put in place, im-
plemented a continually-changing permutation cypher that wandered deter-
ministically through a state space of 263 permutations. Because an enormous
number of messages were sent each day, there was a good chance that what-
ever state one machine was in when sending one character of a message, there
would be another machine in the same state while sending a particular char-
acter in another message. Because the evolution of the machine’s state was
deterministic, the two machines would remain in the same state as each other

1I’ve been most helped by descriptions

(http://www.
codesandciphers.org.uk/lectures/) and by Jack Good (1979), who worked with Turing
at Bletchley.

given by Tony Sale

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

266

18 | Crosswords and Codebreaking

for the rest of the transmission. The resulting correlations between the out-
puts of such pairs of machines provided a dribble of information-content from
which Turing and his co-workers extracted their daily 129 decibans.

How to detect that two messages came from machines with a common
state sequence
The hypotheses are the null hypothesis, H0, which states that the machines
are in di(cid:11)erent states, and that the two plain messages are unrelated; and the
‘match’ hypothesis, H1, which says that the machines are in the same state,
and that the two plain messages are unrelated. No attempt is being made
here to infer what the state of either machine is. The data provided are the
two cyphertexts x and y; let’s assume they both have length T and that the
alphabet size is A (26 in Enigma). What is the probability of the data, given
the two hypotheses?

First, the null hypothesis. This hypothesis asserts that the two cyphertexts

are given by

and

x = x1x2x3 : : : = c1(u1)c2(u2)c3(u3) : : :

(18.17)

y = y1y2y3 : : : = c01(v1)c02(v2)c03(v3) : : : ;

(18.18)
where the codes ct and c0t are two unrelated time-varying permutations of the
alphabet, and u1u2u3 : : : and v1v2v3 : : : are the plaintext messages. An exact
computation of the probability of the data (x; y) would depend on a language
model of the plain text, and a model of the Enigma machine’s guts, but if we
assume that each Enigma machine is an ideal random time-varying permuta-
tion, then the probability distribution of the two cyphertexts is uniform. All
cyphertexts are equally likely.

P (x; y jH0) =(cid:18) 1

A(cid:19)2T

for all x; y of length T :

(18.19)

What about H1? This hypothesis asserts that a single time-varying permuta-
tion ct underlies both

x = x1x2x3 : : : = c1(u1)c2(u2)c3(u3) : : :

(18.20)

and

y = y1y2y3 : : : = c1(v1)c2(v2)c3(v3) : : : :

(18.21)

What is the probability of the data (x; y)? We have to make some assumptions
about the plaintext language. If it were the case that the plaintext language
was completely random, then the probability of u1u2u3 : : : and v1v2v3 : : : would
be uniform, and so would that of x and y, so the probability P (x; y jH1)
would be equal to P (x; y jH0), and the two hypotheses H0 and H1 would be
indistinguishable.
We make progress by assuming that the plaintext is not completely ran-
dom. Both plaintexts are written in a language, and that language has redun-
dancies. Assume for example that particular plaintext letters are used more
often than others. So, even though the two plaintext messages are unrelated,
they are slightly more likely to use the same letters as each other; if H1 is true,
two synchronized letters from the two cyphertexts are slightly more likely to
be identical. Similarly, if a language uses particular bigrams and trigrams
frequently, then the two plaintext messages will occasionally contain the same
bigrams and trigrams at the same time as each other, giving rise, if H1 is true,

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

18.4: A taste of Banburismus

267

u LITTLE-JACK-HORNER-SAT-IN-THE-CORNER-EATING-A-CHRISTMAS-PIE--HE-PUT-IN-H
v
RIDE-A-COCK-HORSE-TO-BANBURY-CROSS-TO-SEE-A-FINE-LADY-UPON-A-WHITE-HORSE
matches:
.*....*..******.*..............*...........*................*...........

Table 18.9. Two aligned pieces of
English plaintext, u and v, with
matches marked by *. Notice that
there are twelve matches,
including a run of six, whereas the
expected number of matches in
two completely random strings of
length T = 74 would be about 3.
The two corresponding
cyphertexts from two machines in
identical states would also have
twelve matches.

to a little burst of 2 or 3 identical letters. Table 18.9 shows such a coinci-
dence in two plaintext messages that are unrelated, except that they are both
written in English.

The codebreakers hunted among pairs of messages for pairs that were sus-
piciously similar to each other, counting up the numbers of matching mono-
grams, bigrams, trigrams, etc. This method was (cid:12)rst used by the Polish
codebreaker Rejewski.

Let’s look at the simple case of a monogram language model and estimate
how long a message is needed to be able to decide whether two machines
are in the same state. I’ll assume the source language is monogram-English,
the language in which successive letters are drawn i.i.d. from the probability
distribution fpig of (cid:12)gure 2.1. The probability of x and y is nonuniform:
consider two single characters, xt = ct(ut) and yt = ct(vt); the probability
that they are identical is

Xut;vt

P (ut)P (vt)  [ut = vt] = Xi

p2
i (cid:17) m:

(18.22)

We give this quantity the name m, for ‘match probability’; for both English
and German, m is about 2=26 rather than 1=26 (the value that would hold
for a completely random language). Assuming that ct is an ideal random
permutation, the probability of xt and yt is, by symmetry,

P (xt; yt jH1) = (

m
A

(1(cid:0)m)
A(A(cid:0)1)

if xt = yt
for xt 6= yt.

(18.23)

Given a pair of cyphertexts x and y of length T that match in M places and
do not match in N places, the log evidence in favour of H1 is then

log

P (x; y jH1)
P (x; y jH0)

= M log

m=A
1=A2 + N log

(1(cid:0)m)
A(A(cid:0)1)
1=A2

= M log mA + N log

(1 (cid:0) m)A
A (cid:0) 1

:

(18.24)

(18.25)

Every match contributes log mA in favour of H1; every non-match contributes
log A(cid:0)1

0.076
0.037
3.1 db
10 log10 mA
(1(cid:0)m)A
(A(cid:0)1) (cid:0)0:18 db

(1(cid:0)m)A in favour of H0.
Match probability for monogram-English
Coincidental match probability
log-evidence for H1 per match
log-evidence for H1 per non-match
If there were M = 4 matches and N = 47 non-matches in a pair of length
T = 51, for example, the weight of evidence in favour of H1 would be +4
decibans, or a likelihood ratio of 2.5 to 1 in favour.
The expected weight of evidence from a line of text of length T = 20
characters is the expectation of (18.25), which depends on whether H1 or H0
is true. If H1 is true then matches are expected to turn up at rate m, and the
expected weight of evidence is 1.4 decibans per 20 characters. If H0 is true

m
1=A

10 log10

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

268

18 | Crosswords and Codebreaking

then spurious matches are expected to turn up at rate 1=A, and the expected
weight of evidence is (cid:0)1:1 decibans per 20 characters. Typically, roughly 400
characters need to be inspected in order to have a weight of evidence greater
than a hundred to one (20 decibans) in favour of one hypothesis or the other.
So, two English plaintexts have more matches than two random strings.
Furthermore, because consecutive characters in English are not independent,
the bigram and trigram statistics of English are nonuniform and the matches
tend to occur in bursts of consecutive matches.
[The same observations also
apply to German.] Using better language models, the evidence contributed
by runs of matches was more accurately computed. Such a scoring system
was worked out by Turing and re(cid:12)ned by Good. Positive results were passed
on to automated and human-powered codebreakers. According to Good, the
longest false-positive that arose in this work was a string of 8 consecutive
matches between two machines that were actually in unrelated states.

Further reading

For further reading about Turing and Bletchley Park, see Hodges (1983) and
Good (1979). For an in-depth read about cryptography, Schneier’s (1996)
book is highly recommended. It is readable, clear, and entertaining.

18.5 Exercises

. Exercise 18.3.[2 ] Another weakness in the design of the Enigma machine,
which was intended to emulate a perfectly random time-varying permu-
tation, is that it never mapped a letter to itself. When you press Q, what
comes out is always a di(cid:11)erent letter from Q. How much information per
character is leaked by this design (cid:13)aw? How long a crib would be needed
to be con(cid:12)dent that the crib is correctly aligned with the cyphertext?
And how long a crib would be needed to be able con(cid:12)dently to identify
the correct key?

[A crib is a guess for what the plaintext was. Imagine that the Brits
know that a very important German is travelling from Berlin to Aachen,
and they intercept Enigma-encoded messages sent to Aachen.
It is a
good bet that one or more of the original plaintext messages contains
the string OBERSTURMBANNFUEHRERXGRAFXHEINRICHXVONXWEIZSAECKER,
the name of the important chap. A crib could be used in a brute-force
approach to (cid:12)nd the correct Enigma key (feed the received messages
through all possible Engima machines and see if any of the putative
decoded texts match the above plaintext). This question centres on the
idea that the crib can also be used in a much less expensive manner:
slide the plaintext crib along all the encoded messages until a perfect
mismatch of the crib and the encoded message is found; if correct, this
alignment then tells you a lot about the key.]

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

19

Why have Sex? Information Acquisition

and Evolution

Evolution has been happening on earth for about the last 109 years. Un-
deniably, information has been acquired during this process. Thanks to the
tireless work of the Blind Watchmaker, some cells now carry within them all
the information required to be outstanding spiders; other cells carry all the
information required to make excellent octopuses. Where did this information
come from?

The entire blueprint of all organisms on the planet has emerged in a teach-
ing process in which the teacher is natural selection: (cid:12)tter individuals have
more progeny, the (cid:12)tness being de(cid:12)ned by the local environment (including
the other organisms). The teaching signal is only a few bits per individual: an
individual simply has a smaller or larger number of grandchildren, depending
on the individual’s (cid:12)tness. ‘Fitness’ is a broad term that could cover

(cid:15) the ability of an antelope to run faster than other antelopes and hence

avoid being eaten by a lion;

(cid:15) the ability of a lion to be well-enough camou(cid:13)aged and run fast enough

to catch one antelope per day;

(cid:15) the ability of a peacock to attract a peahen to mate with it;
(cid:15) the ability of a peahen to rear many young simultaneously.

The (cid:12)tness of an organism is largely determined by its DNA { both the coding
regions, or genes, and the non-coding regions (which play an important role
in regulating the transcription of genes). We’ll think of (cid:12)tness as a function
of the DNA sequence and the environment.

How does the DNA determine (cid:12)tness, and how does information get from
natural selection into the genome? Well, if the gene that codes for one of an
antelope’s proteins is defective, that antelope might get eaten by a lion early
in life and have only two grandchildren rather than forty. The information
content of natural selection is fully contained in a speci(cid:12)cation of which o(cid:11)-
spring survived to have children { an information content of at most one bit
per o(cid:11)spring. The teaching signal does not communicate to the ecosystem
any description of the imperfections in the organism that caused it to have
fewer children. The bits of the teaching signal are highly redundant, because,
throughout a species, un(cid:12)t individuals who are similar to each other will be
failing to have o(cid:11)spring for similar reasons.

So, how many bits per generation are acquired by the species as a whole
by natural selection? How many bits has natural selection succeeded in con-
veying to the human branch of the tree of life, since the divergence between

269

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

270

19 | Why have Sex? Information Acquisition and Evolution

Australopithecines and apes 4 000 000 years ago? Assuming a generation time
of 10 years for reproduction, there have been about 400 000 generations of
human precursors since the divergence from apes. Assuming a population of
109 individuals, each receiving a couple of bits of information from natural
selection, the total number of bits of information responsible for modifying
the genomes of 4 million B.C. into today’s human genome is about 8 (cid:2) 1014
bits. However, as we noted, natural selection is not smart at collating the
information that it dishes out to the population, and there is a great deal of
redundancy in that information. If the population size were twice as great,
would it evolve twice as fast? No, because natural selection will simply be
correcting the same defects twice as often.

John Maynard Smith has suggested that the rate of information acquisition
by a species is independent of the population size, and is of order 1 bit per
generation. This (cid:12)gure would allow for only 400 000 bits of di(cid:11)erence between
apes and humans, a number that is much smaller than the total size of the
human genome { 6 (cid:2) 109 bits.
[One human genome contains about 3 (cid:2) 109
nucleotides.] It is certainly the case that the genomic overlap between apes
and humans is huge, but is the di(cid:11)erence that small?

In this chapter, we’ll develop a crude model of the process of information
acquisition through evolution, based on the assumption that a gene with two
defects is typically likely to be more defective than a gene with one defect, and
an organism with two defective genes is likely to be less (cid:12)t than an organism
with one defective gene. Undeniably, this is a crude model, since real biological
systems are baroque constructions with complex interactions. Nevertheless,
we persist with a simple model because it readily yields striking results.

What we (cid:12)nd from this simple model is that

1. John Maynard Smith’s (cid:12)gure of 1 bit per generation is correct for an

asexually-reproducing population;

2. in contrast, if the species reproduces sexually, the rate of information
acquisition can be as large as pG bits per generation, where G is the
size of the genome.

We’ll also (cid:12)nd interesting results concerning the maximum mutation rate

that a species can withstand.

19.1 The model

We study a simple model of a reproducing population of N individuals with
a genome of size G bits: variation is produced by mutation or by recombina-
tion (i.e., sex) and truncation selection selects the N (cid:12)ttest children at each
generation to be the parents of the next. We (cid:12)nd striking di(cid:11)erences between
populations that have recombination and populations that do not.

The genotype of each individual is a vector x of G bits, each having a good
state xg = 1 and a bad state xg = 0. The (cid:12)tness F (x) of an individual is simply
the sum of her bits:

The bits in the genome could be considered to correspond either to genes
that have good alleles (xg = 1) and bad alleles (xg = 0), or to the nucleotides
of a genome. We will concentrate on the latter interpretation. The essential
property of (cid:12)tness that we are assuming is that it is locally a roughly linear
function of the genome, that is, that there are many possible changes one

F (x) =

xg:

(19.1)

G

Xg=1

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

19.2: Rate of increase of (cid:12)tness

271

could make to the genome, each of which has a small e(cid:11)ect on (cid:12)tness, and
that these e(cid:11)ects combine approximately linearly.

We de(cid:12)ne the normalized (cid:12)tness f (x) (cid:17) F (x)=G.
We consider evolution by natural selection under two models of variation.

Variation by mutation. The model assumes discrete generations. At each
generation, t, every individual produces two children. The children’s
genotypes di(cid:11)er from the parent’s by random mutations. Natural selec-
tion selects the (cid:12)ttest N progeny in the child population to reproduce,
and a new generation starts.

[The selection of the (cid:12)ttest N individuals at each generation is known
as truncation selection.]
The simplest model of mutations is that the child’s bits fxgg are in-
dependent. Each bit has a small probability of being (cid:13)ipped, which,
thinking of the bits as corresponding roughly to nucleotides, is taken to
be a constant m, independent of xg. [If alternatively we thought of the
bits as corresponding to genes, then we would model the probability of
the discovery of a good gene, P (xg = 0 ! xg = 1), as being a smaller
number than the probability of a deleterious mutation in a good gene,
P (xg = 1 ! xg = 0).]

Variation by recombination (or crossover, or sex). Our organisms are
haploid, not diploid. They enjoy sex by recombination. The N individ-
uals in the population are married into M = N=2 couples, at random,
and each couple has C children { with C = 4 children being our stan-
dard assumption, so as to have the population double and halve every
generation, as before. The C children’s genotypes are independent given
the parents’. Each child obtains its genotype z by random crossover of
its parents’ genotypes, x and y. The simplest model of recombination
has no linkage, so that:

zg = (cid:26) xg with probability 1=2

yg with probability 1=2.

(19.2)

Once the M C progeny have been born, the parents pass away, the (cid:12)ttest
N progeny are selected by natural selection, and a new generation starts.

We now study these two models of variation in detail.

19.2 Rate of increase of (cid:12)tness

Theory of mutations

We assume that the genotype of an individual with normalized (cid:12)tness f = F=G
is subjected to mutations that (cid:13)ip bits with probability m. We (cid:12)rst show that
if the average normalized (cid:12)tness f of the population is greater than 1=2, then
the optimal mutation rate is small, and the rate of acquisition of information
is at most of order one bit per generation.

Since it is easy to achieve a normalized (cid:12)tness of f = 1=2 by simple muta-
tion, we’ll assume f > 1=2 and work in terms of the excess normalized (cid:12)tness
(cid:14)f (cid:17) f (cid:0) 1=2. If an individual with excess normalized (cid:12)tness (cid:14)f has a child
and the mutation rate m is small, the probability distribution of the excess
normalized (cid:12)tness of the child has mean

(cid:14)f child = (1 (cid:0) 2m)(cid:14)f

(19.3)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

272

and variance

19 | Why have Sex? Information Acquisition and Evolution

m(1 (cid:0) m)

G

m
G

:

’

(19.4)

If the population of parents has mean (cid:14)f (t) and variance (cid:27) 2(t) (cid:17) (cid:12)m=G, then
the child population, before selection, will have mean (1 (cid:0) 2m)(cid:14)f (t) and vari-
ance (1+(cid:12))m=G. Natural selection chooses the upper half of this distribution,
so the mean (cid:12)tness and variance of (cid:12)tness at the next generation are given by

(cid:14)f (t+1) = (1 (cid:0) 2m)(cid:14)f (t) + (cid:11)p(1 + (cid:12))r m

G

;

(19.5)

(cid:27)2(t+1) = (cid:13)(1 + (cid:12))

;

(19.6)

m
G

where (cid:11) is the mean deviation from the mean, measured in standard devia-
tions, and (cid:13) is the factor by which the child distribution’s variance is reduced
by selection. The numbers (cid:11) and (cid:13) are of order 1. For the case of a Gaussian

distribution, (cid:11) = p2=(cid:25) ’ 0:8 and (cid:13) = (1 (cid:0) 2=(cid:25)) ’ 0:36. If we assume that
the variance is in dynamic equilibrium, i.e., (cid:27)2(t+1) ’ (cid:27)2(t), then

(cid:13)(1 + (cid:12)) = (cid:12); so (1 + (cid:12)) =

1
1 (cid:0) (cid:13)

;

(19.7)

and the factor (cid:11)p(1 + (cid:12)) in equation (19.5) is equal to 1, if we take the results

for the Gaussian distribution, an approximation that becomes poorest when
the discreteness of (cid:12)tness becomes important, i.e., for small m. The rate of
increase of normalized (cid:12)tness is thus:

dt ’ (cid:0)2m (cid:14)f +r m
which, assuming G((cid:14)f )2 (cid:29) 1, is maximized for

df

G

;

(19.8)

at which point,

mopt =

1

16G((cid:14)f )2 ;

dt(cid:19)opt
(cid:18) df

=

1

8G((cid:14)f )

:

(19.9)

(19.10)

So the rate of increase of (cid:12)tness F = f G is at most

dF
dt

=

1

8((cid:14)f )

per generation:

(19.11)

For a population with low (cid:12)tness ((cid:14)f < 0:125), the rate of increase of (cid:12)tness
1=pG, the rate of increase, if
may exceed 1 unit per generation. Indeed, if (cid:14)f  
m = 1/2, is of order pG; this initial spurt can last only of order pG generations.
For (cid:14)f > 0:125, the rate of increase of (cid:12)tness is smaller than one per generation.
As the (cid:12)tness approaches G, the optimal mutation rate tends to m = 1=(4G), so
that an average of 1=4 bits are (cid:13)ipped per genotype, and the rate of increase of
(cid:12)tness is also equal to 1=4; information is gained at a rate of about 0:5 bits per
generation. It takes about 2G generations for the genotypes of all individuals
in the population to attain perfection.
For (cid:12)xed m, the (cid:12)tness is given by

(cid:14)f (t) =

1

2pmG

(1 (cid:0) c e(cid:0)2mt);

(19.12)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

19.2: Rate of increase of (cid:12)tness

273

No sex

Sex

Histogram of parents’ (cid:12)tness

Histogram of children’s (cid:12)tness

Selected children’s (cid:12)tness

Figure 19.1. Why sex is better
than sex-free reproduction. If
mutations are used to create
variation among children, then it
is unavoidable that the average
(cid:12)tness of the children is lower
than the parents’ (cid:12)tness; the
greater the variation, the greater
the average de(cid:12)cit. Selection
bumps up the mean (cid:12)tness again.
In contrast, recombination
produces variation without a
decrease in average (cid:12)tness. The
typical amount of variation scales
as pG, where G is the genome
size, so after selection, the average
(cid:12)tness rises by O(pG).

subject to the constraint (cid:14)f (t) (cid:20) 1=2, where c is a constant of integration,
equal to 1 if f (0) = 1=2. If the mean number of bits (cid:13)ipped per genotype,
mG, exceeds 1, then the (cid:12)tness F approaches an equilibrium value Feqm =
(1=2 + 1=(2pmG))G.

This theory is somewhat inaccurate in that the true probability distribu-
tion of (cid:12)tness is non-Gaussian, asymmetrical, and quantized to integer values.
All the same, the predictions of the theory are not grossly at variance with
the results of simulations described below.

Theory of sex

The analysis of the sexual population becomes tractable with two approxi-
mations: (cid:12)rst, we assume that the gene-pool mixes su(cid:14)ciently rapidly that
correlations between genes can be neglected; second, we assume homogeneity,
i.e., that the fraction fg of bits g that are in the good state is the same, f (t),
for all g.

Given these assumptions, if two parents of (cid:12)tness F = f G mate, the prob-
ability distribution of their children’s (cid:12)tness has mean equal to the parents’
(cid:12)tness, F ; the variation produced by sex does not reduce the average (cid:12)tness.

The standard deviation of the (cid:12)tness of the children scales as pGf (1 (cid:0) f ).
Since, after selection, the increase in (cid:12)tness is proportional to this standard
deviation, the (cid:12)tness increase per generation scales as the square root of the
size of the genome, pG. As shown in box 19.2, the mean (cid:12)tness (cid:22)F = f G
evolves in accordance with the di(cid:11)erential equation:

d (cid:22)F

dt ’ (cid:17)pf (t)(1 (cid:0) f (t))G;
where (cid:17) (cid:17)p2=((cid:25) + 2). The solution of this equation is
(t + c)(cid:19)(cid:21) ;
for t + c 2(cid:16)(cid:0) (cid:25)

pG=(cid:17)(cid:17), (19.14)
where c is a constant of integration, c = sin(cid:0)1(2f (0) (cid:0) 1). So this idealized
system reaches a state of eugenic perfection (f = 1) within a (cid:12)nite time:
((cid:25)=(cid:17))pG generations.

pG=(cid:17); (cid:25)

1

2(cid:20)1 + sin(cid:18) (cid:17)
pG

(19.13)

f (t) =

2

2

Simulations

Figure 19.3a shows the (cid:12)tness of a sexual population of N = 1000 individ-
uals with a genome size of G = 1000 starting from a random initial state
with normalized (cid:12)tness 0:5. It also shows the theoretical curve f (t)G from
equation (19.14), which (cid:12)ts remarkably well.

In contrast, (cid:12)gures 19.3(b) and (c) show the evolving (cid:12)tness when variation
is produced by mutation at rates m = 0:25=G and m = 6=G respectively. Note
the di(cid:11)erence in the horizontal scales from panel (a).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

274

19 | Why have Sex? Information Acquisition and Evolution

How does f (t+1) depend on f (t)? Let’s (cid:12)rst assume the two parents of a child both
have exactly f (t)G good bits, and, by our homogeneity assumption, that those bits are
independent random subsets of the G bits. The number of bits that are good in both
parents is roughly f (t)2G, and the number that are good in one parent only is roughly
2f (t)(1(cid:0)f (t))G, so the (cid:12)tness of the child will be f (t)2G plus the sum of 2f (t)(1(cid:0)f (t))G
fair coin (cid:13)ips, which has a binomial distribution of mean f (t)(1 (cid:0) f (t))G and variance
2 f (t)(1 (cid:0) f (t))G. The (cid:12)tness of a child is thus roughly distributed as

1

Fchild (cid:24) Normal

mean = f (t)G; variance =

1
2

f (t)(1 (cid:0) f (t))G

:

Box 19.2. Details of the theory of
sex.

The important property of this distribution, contrasted with the distribution under
mutation, is that the mean (cid:12)tness is equal to the parents’ (cid:12)tness; the variation produced
by sex does not reduce the average (cid:12)tness.
If we include the parental population’s variance, which we will write as (cid:27)2(t) =
(cid:12)(t) 1

2 f (t)(1 (cid:0) f (t))G, the children’s (cid:12)tnesses are distributed as
1
2

Fchild (cid:24) Normal

mean = f (t)G; variance =

1 +

f (t)(1 (cid:0) f (t))G

:

Natural selection selects the children on the upper side of this distribution. The mean
increase in (cid:12)tness will be

(cid:12)

2

(cid:22)F (t+1) (cid:0) (cid:22)F (t) = [(cid:11)(1 + (cid:12)=2)1=2=p2]

and the variance of the surviving children will be

f (t)(1 (cid:0) f (t))G;

(cid:27)2(t + 1) = (cid:13)(1 + (cid:12)=2)

1
2

f (t)(1 (cid:0) f (t))G;

where (cid:11) =
then the factor in (19.2) is

2=(cid:25) and (cid:13) = (1(cid:0) 2=(cid:25)). If there is dynamic equilibrium [(cid:27)2(t + 1) = (cid:27)2(t)]

(cid:11)(1 + (cid:12)=2)1=2=p2 =

2

((cid:25) + 2) ’ 0:62:

2=((cid:25) + 2), we conclude that, under sex and natural
selection, the mean (cid:12)tness of the population increases at a rate proportional to the
square root of the size of the genome,

De(cid:12)ning this constant to be (cid:17) (cid:17)

d (cid:22)F
dt ’ (cid:17)

f (t)(1 (cid:0) f (t))G bits per generation:

 

 
 





Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

19.3: The maximal tolerable mutation rate

275

Figure 19.3. Fitness as a function
of time. The genome size is
G = 1000. The dots show the
(cid:12)tness of six randomly selected
individuals from the birth
population at each generation.
The initial population of
N = 1000 had randomly
generated genomes with
f (0) = 0:5 (exactly). (a) Variation
produced by sex alone. Line
shows theoretical curve (19.14) for
in(cid:12)nite homogeneous population.
(b,c) Variation produced by
mutation, with and without sex,
when the mutation rate is
mG = 0:25 (b) or 6 (c) bits per
genome. The dashed line shows
the curve (19.12).

Figure 19.4. Maximal tolerable
mutation rate, shown as number
of errors per genome (mG), versus
normalized (cid:12)tness f = F=G. Left
panel: genome size G = 1000;
right: G = 100 000.
Independent of genome size, a
parthenogenetic species (no sex)
can tolerate only of order 1 error
per genome per generation; a
species that uses recombination
(sex) can tolerate far greater
mutation rates.

1000

900

800

700

600

500

(a)

0

10

20

30

40

50

60

70

80

1000

900

800

700

600

500

(b)

sex

no sex

0

200

400

600

800

1000 1200 1400 1600

1000

900

800

700

600

500

0

(c)

sex

no sex

50

100

150

200

250

300

350

G = 1000

G = 100 000

mG

20

15

10

5

0

with sex

without sex

50

45

40

35

30

25

20

15

10

5

0

with sex

without sex

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

f

f

Exercise 19.1.[3, p.280] Dependence on population size. How do the results for
a sexual population depend on the population size? We anticipate that
there is a minimum population size above which the theory of sex is
accurate. How is that minimum population size related to G?

Exercise 19.2.[3 ] Dependence on crossover mechanism. In the simple model of
sex, each bit is taken at random from one of the two parents, that is, we
allow crossovers to occur with probability 50% between any two adjacent
nucleotides. How is the model a(cid:11)ected (a) if the crossover probability is
smaller? (b) if crossovers occur exclusively at hot-spots located every d
bits along the genome?

19.3 The maximal tolerable mutation rate

What if we combine the two models of variation? What is the maximum
mutation rate that can be tolerated by a species that has sex?

The rate of increase of (cid:12)tness is given by

df

dt ’ (cid:0)2m (cid:14)f + (cid:17)p2r m + f (1 (cid:0) f )=2

G

;

(19.15)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

276

19 | Why have Sex? Information Acquisition and Evolution

which is positive if the mutation rate satis(cid:12)es

m < (cid:17)r f (1 (cid:0) f )

G

:

(19.16)

Let us compare this rate with the result in the absence of sex, which, from
equation (19.8), is that the maximum tolerable mutation rate is

(19.17)
The tolerable mutation rate with sex is of order pG times greater than that
without sex!

(2 (cid:14)f )2 :

m <

1

1
G

A parthenogenetic (non-sexual) species could try to wriggle out of this
bound on its mutation rate by increasing its litter sizes. But if mutation (cid:13)ips
on average mG bits, the probability that no bits are (cid:13)ipped in one genome
is roughly e(cid:0)mG, so a mother needs to have roughly emG o(cid:11)spring in order
to have a good chance of having one child with the same (cid:12)tness as her. The
litter size of a non-sexual species thus has to be exponential in mG (if mG is
bigger than 1), if the species is to persist.
So the maximum tolerable mutation rate is pinned close to 1=G, for a non-
sexual species, whereas it is a larger number of order 1=pG, for a species with
recombination.

Turning these results around, we can predict the largest possible genome
size for a given (cid:12)xed mutation rate, m. For a parthenogenetic species, the
largest genome size is of order 1=m, and for a sexual species, 1=m2. Taking
the (cid:12)gure m = 10(cid:0)8 as the mutation rate per nucleotide per generation (Eyre-
Walker and Keightley, 1999), and allowing for a maximum brood size of 20 000
(that is, mG ’ 10), we predict that all species with more than G = 109 coding
nucleotides make at least occasional use of recombination. If the brood size is
12, then this number falls to G = 2:5 (cid:2) 108.
19.4 Fitness increase and information acquisition

For this simple model it is possible to relate increasing (cid:12)tness to information
acquisition.

If the bits are set at random, the (cid:12)tness is roughly F = G=2. If evolution
leads to a population in which all individuals have the maximum (cid:12)tness F = G,
then G bits of information have been acquired by the species, namely for each
bit xg, the species has (cid:12)gured out which of the two states is the better.

We de(cid:12)ne the information acquired at an intermediate (cid:12)tness to be the
amount of selection (measured in bits) required to select the perfect state
from the gene pool. Let a fraction fg of the population have xg = 1. Because
log2(1=f ) is the information required to (cid:12)nd a black ball in an urn containing
black and white balls in the ratio f : 1(cid:0)f , we de(cid:12)ne the information acquired
to be

bits:

(19.18)

log2

fg
1=2

I =Xg

If all the fractions fg are equal to F=G, then
2F
G

I = G log2

;

which is well approximated by

~I (cid:17) 2(F (cid:0) G=2):

(19.19)

(19.20)

The rate of information acquisition is thus roughly two times the rate of in-
crease of (cid:12)tness in the population.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

277

19.5: Discussion

19.5 Discussion

These results quantify the well known argument for why species reproduce
by sex with recombination, namely that recombination allows useful muta-
tions to spread more rapidly through the species and allows deleterious muta-
tions to be more rapidly cleared from the population (Maynard Smith, 1978;
Felsenstein, 1985; Maynard Smith, 1988; Maynard Smith and Sz(cid:19)athmary,
1995). A population that reproduces by recombination can acquire informa-
tion from natural selection at a rate of order pG times faster than a partheno-
genetic population, and it can tolerate a mutation rate that is of order pG
times greater. For genomes of size G ’ 108 coding nucleotides, this factor of
pG is substantial.
This enormous advantage conferred by sex has been noted before by Kon-
drashov (1988), but this meme, which Kondrashov calls ‘the deterministic
mutation hypothesis’, does not seem to have di(cid:11)used throughout the evolu-
tionary research community, as there are still numerous papers in which the
prevalence of sex is viewed as a mystery to be explained by elaborate mecha-
nisms.

‘The cost of males’ { stability of a gene for sex or parthenogenesis

Why do people declare sex to be a mystery? The main motivation for being
mysti(cid:12)ed is an idea called the ‘cost of males’. Sexual reproduction is disad-
vantageous compared with asexual reproduction, it’s argued, because of every
two o(cid:11)spring produced by sex, one (on average) is a useless male, incapable
of child-bearing, and only one is a productive female.
In the same time, a
parthenogenetic mother could give birth to two female clones. To put it an-
other way, the big advantage of parthenogenesis, from the point of view of
the individual, is that one is able to pass on 100% of one’s genome to one’s
children, instead of only 50%. Thus if there were two versions of a species, one
reproducing with and one without sex, the single mothers would be expected
to outstrip their sexual cousins. The simple model presented thus far did not
include either genders or the ability to convert from sexual reproduction to
asexual, but we can easily modify the model.

We modify the model so that one of the G bits in the genome determines
whether an individual prefers to reproduce parthenogenetically (x = 1) or sex-
ually (x = 0). The results depend on the number of children had by a single
parthenogenetic mother, Kp and the number of children born by a sexual
couple, Ks. Both (Kp = 2, Ks = 4) and (Kp = 4, Ks = 4) are reasonable mod-
els. The former (Kp = 2, Ks = 4) would seem most appropriate in the case
of unicellular organisms, where the cytoplasm of both parents goes into the
children. The latter (Kp = 4, Ks = 4) is appropriate if the children are solely
nurtured by one of the parents, so single mothers have just as many o(cid:11)spring
as a sexual pair. I concentrate on the latter model, since it gives the greatest
advantage to the parthenogens, who are supposedly expected to outbreed the
sexual community. Because parthenogens have four children per generation,
the maximum tolerable mutation rate for them is twice the expression (19.17)
derived before for Kp = 2. If the (cid:12)tness is large, the maximum tolerable rate
is mG ’ 2.
Initially the genomes are set randomly with F = G=2, with half of the pop-
ulation having the gene for parthenogenesis. Figure 19.5 shows the outcome.
During the ‘learning’ phase of evolution, in which the (cid:12)tness is increasing
rapidly, pockets of parthenogens appear brie(cid:13)y, but then disappear within
a couple of generations as their sexual cousins overtake them in (cid:12)tness and

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

278

19 | Why have Sex? Information Acquisition and Evolution

Figure 19.5. Results when there is
a gene for parthenogenesis, and no
interbreeding, and single mothers
produce as many children as
sexual couples. G = 1000,
N = 1000. (a) mG = 4; (b)
mG = 1. Vertical axes show the
(cid:12)tnesses of the two
sub-populations, and the
percentage of the population that
is parthenogenetic.

(a) mG = 4

(b) mG = 1

s
e
s
s
e
n
t
i
F

e
g
a
t
n
e
c
r
e
P

1000

900

800

700

600

500

0

100

80

60

40

20

0

0

1000

900

800

700

600

500

0

100

80

60

40

20

0

0

sexual fitness
parthen fitness

50

100

150

200

250

50

100

150

200

250

sexual fitness
parthen fitness

50

100

150

200

250

50

100

150

200

250

In the presence of a higher mutation rate (mG = 4), however,

leave them behind. Once the population reaches its top (cid:12)tness, however, the
parthenogens can take over, if the mutation rate is su(cid:14)ciently low (mG = 1).
the
parthenogens never take over. The breadth of the sexual population’s (cid:12)t-
ness is of order pG, so a mutant parthenogenetic colony arising with slightly
above-average (cid:12)tness will last for about pG=(mG) = 1=(mpG) generations
before its (cid:12)tness falls below that of its sexual cousins. As long as the popu-
lation size is su(cid:14)ciently large for some sexual individuals to survive for this
time, sex will not die out.

In a su(cid:14)ciently unstable environment, where the (cid:12)tness function is con-
tinually changing, the parthenogens will always lag behind the sexual commu-
nity. These results are consistent with the argument of Haldane and Hamilton
(2002) that sex is helpful in an arms race with parasites. The parasites de(cid:12)ne
an e(cid:11)ective (cid:12)tness function which changes with time, and a sexual population
will always ascend the current (cid:12)tness function more rapidly.

Additive (cid:12)tness function

Of course, our results depend on the (cid:12)tness function that we assume, and on
our model of selection. Is it reasonable to model (cid:12)tness, to (cid:12)rst order, as a sum
of independent terms? Maynard Smith (1968) argues that it is: the more good
genes you have, the higher you come in the pecking order, for example. The
directional selection model has been used extensively in theoretical popula-
tion genetic studies (Bulmer, 1985). We might expect real (cid:12)tness functions to
involve interactions, in which case crossover might reduce the average (cid:12)tness.
However, since recombination gives the biggest advantage to species whose (cid:12)t-
ness functions are additive, we might predict that evolution will have favoured
species that used a representation of the genome that corresponds to a (cid:12)tness
function that has only weak interactions. And even if there are interactions,
it seems plausible that the (cid:12)tness would still involve a sum of such interacting
terms, with the number of terms being some fraction of the genome size G.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

19.6: Further exercises

279

Exercise 19.3.[3C ] Investigate how fast sexual and asexual species evolve if
they have a (cid:12)tness function with interactions. For example, let the (cid:12)tness
be a sum of exclusive-ors of pairs of bits; compare the evolving (cid:12)tnesses
with those of the sexual and asexual species with a simple additive (cid:12)tness
function.

Furthermore, if the (cid:12)tness function were a highly nonlinear function of the
genotype, it could be made more smooth and locally linear by the Baldwin
e(cid:11)ect. The Baldwin e(cid:11)ect (Baldwin, 1896; Hinton and Nowlan, 1987) has been
widely studied as a mechanism whereby learning guides evolution, and it could
also act at the level of transcription and translation. Consider the evolution of
a peptide sequence for a new purpose. Assume the e(cid:11)ectiveness of the peptide
is a highly nonlinear function of the sequence, perhaps having a small island
of good sequences surrounded by an ocean of equally bad sequences. In an
organism whose transcription and translation machinery is (cid:13)awless, the (cid:12)tness
will be an equally nonlinear function of the DNA sequence, and evolution
will wander around the ocean making progress towards the island only by a
random walk. In contrast, an organism having the same DNA sequence, but
whose DNA-to-RNA transcription or RNA-to-protein translation is ‘faulty’,
will occasionally, by mistranslation or mistranscription, accidentally produce a
working enzyme; and it will do so with greater probability if its DNA sequence
is close to a good sequence. One cell might produce 1000 proteins from the
one mRNA sequence, of which 999 have no enzymatic e(cid:11)ect, and one does.
The one working catalyst will be enough for that cell to have an increased
(cid:12)tness relative to rivals whose DNA sequence is further from the island of
good sequences. For this reason I conjecture that, at least early in evolution,
and perhaps still now, the genetic code was not implemented perfectly but was
implemented noisily, with some codons coding for a distribution of possible
amino acids. This noisy code could even be switched on and o(cid:11) from cell
to cell in an organism by having multiple aminoacyl-tRNA synthetases, some
more reliable than others.

Whilst our model assumed that the bits of the genome do not interact,
ignored the fact that the information is represented redundantly, assumed
that there is a direct relationship between phenotypic (cid:12)tness and the genotype,
and assumed that the crossover probability in recombination is high, I believe
these qualitative results would still hold if more complex models of (cid:12)tness and
crossover were used: the relative bene(cid:12)t of sex will still scale as pG. Only in
small, in-bred populations are the bene(cid:12)ts of sex expected to be diminished.

In summary: Why have sex? Because sex is good for your bits!

Further reading

How did a high-information-content self-replicating system ever emerge in the
(cid:12)rst place? In the general area of the origins of life and other tricky ques-
tions about evolution, I highly recommend Maynard Smith and Sz(cid:19)athmary
(1995), Maynard Smith and Sz(cid:19)athmary (1999), Kondrashov (1988), May-
nard Smith (1988), Ridley (2000), Dyson (1985), Cairns-Smith (1985), and
Hop(cid:12)eld (1978).

19.6 Further exercises

Exercise 19.4.[3 ] How good must the error-correcting machinery in DNA repli-
cation be, given that mammals have not all died out long ago? Estimate the
probability of nucleotide substitution, per cell division. [See Appendix C.4.]

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

280

19 | Why have Sex? Information Acquisition and Evolution

Exercise 19.5.[4 ] Given that DNA replication is achieved by bumbling Brow-
nian motion and ordinary thermodynamics in a biochemical porridge at a
temperature of 35 C, it’s astonishing that the error-rate of DNA replication
is about 10(cid:0)9 per replicated nucleotide. How can this reliability be achieved,
given that the energetic di(cid:11)erence between a correct base-pairing and an incor-
rect one is only one or two hydrogen bonds and the thermal energy kT is only
about a factor of four smaller than the free energy associated with a hydro-
gen bond? If ordinary thermodynamics is what favours correct base-pairing,
surely the frequency of incorrect base-pairing should be about

f = exp((cid:0)(cid:1)E/kT );

(19.21)

where (cid:1)E is the free energy di(cid:11)erence, i.e., an error frequency of f ’ 10(cid:0)4?
How has DNA replication cheated thermodynamics?
The situation is equally perplexing in the case of protein synthesis, which
translates an mRNA sequence into a polypeptide in accordance with the ge-
netic code. Two speci(cid:12)c chemical reactions are protected against errors: the
binding of tRNA molecules to amino acids, and the production of the polypep-
tide in the ribosome, which,
involves base-pairing.
Again, the (cid:12)delity is high (an error rate of about 10(cid:0)4), and this (cid:12)delity
can’t be caused by the energy of the ‘correct’ (cid:12)nal state being especially low
{ the correct polypeptide sequence is not expected to be signi(cid:12)cantly lower in
energy than any other sequence. How do cells perform error correction? (See
Hop(cid:12)eld (1974), Hop(cid:12)eld (1980)).

like DNA replication,

Exercise 19.6.[2 ] While the genome acquires information through natural se-
lection at a rate of a few bits per generation, your brain acquires information
at a greater rate.

Estimate at what rate new information can be stored in long term memory

by your brain. Think of learning the words of a new language, for example.

19.7 Solutions

Solution to exercise 19.1 (p.275). For small enough N , whilst the average (cid:12)t-
ness of the population increases, some unlucky bits become frozen into the
bad state. (These bad genes are sometimes known as hitchhikers.) The ho-
mogeneity assumption breaks down. Eventually, all individuals have identical
genotypes that are mainly 1-bits, but contain some 0-bits too. The smaller
the population, the greater the number of frozen 0-bits is expected to be. How
small can the population size N be if the theory of sex is accurate?

We (cid:12)nd experimentally that the theory based on assuming homogeneity (cid:12)ts
poorly only if the population size N is smaller than (cid:24)pG. If N is signi(cid:12)cantly
smaller than pG, information cannot possibly be acquired at a rate as big as
pG, since the information content of the Blind Watchmaker’s decisions cannot
be any greater than 2N bits per generation, this being the number of bits
required to specify which of the 2N children get to reproduce. Baum et al.
(1995), analyzing a similar model, show that the population size N should be
about pG(log G)2 to make hitchhikers unlikely to arise.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part IV

Probabilities and Inference

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Part IV

The number of inference problems that can (and perhaps should) be tackled
by Bayesian inference methods is enormous.
In this book, for example, we
discuss the decoding problem for error-correcting codes, the task of inferring
clusters from data, the task of interpolation through noisy data, and the task
of classifying patterns given labelled examples. Most techniques for solving
these problems can be categorized as follows.

Exact methods compute the required quantities directly. Only a few inter-
esting problems have a direct solution, but exact methods are important
as tools for solving subtasks within larger problems. Methods for the
exact solution of inference problems are the subject of Chapters 21, 24,
25, and 26.

Approximate methods can be subdivided into

1. deterministic approximations, which include maximum likeli-
hood (Chapter 22), Laplace’s method (Chapters 27 and 28) and
variational methods (Chapter 33); and

2. Monte Carlo methods { techniques in which random numbers
play an integral part { which will be discussed in Chapters 29, 30,
and 32.

This part of the book does not form a one-dimensional story. Rather, the
ideas make up a web of interrelated threads which will recombine in subsequent
chapters.

Chapter 3, which is an honorary member of this part, discussed a range of

simple examples of inference problems and their Bayesian solutions.

To give further motivation for the toolbox of inference methods discussed in
this part, Chapter 20 discusses the problem of clustering; subsequent chapters
discuss the probabilistic interpretation of clustering as mixture modelling.

Chapter 21 discusses the option of dealing with probability distributions
by completely enumerating all hypotheses. Chapter 22 introduces the idea
of maximization methods as a way of avoiding the large cost associated with
complete enumeration, and points out reasons why maximum likelihood is
not good enough. Chapter 23 reviews the probability distributions that arise
most often in Bayesian inference. Chapters 24, 25, and 26 discuss another
way of avoiding the cost of complete enumeration: marginalization. Chapter
25 discusses message-passing methods appropriate for graphical models, using
the decoding of error-correcting codes as an example. Chapter 26 combines
these ideas with message-passing concepts from Chapters 16 and 17. These
chapters are a prerequisite for the understanding of advanced error-correcting
codes.

Chapter 27 discusses deterministic approximations including Laplace’s
method. This chapter is a prerequisite for understanding the topic of complex-
ity control in learning algorithms, an idea that is discussed in general terms
in Chapter 28.

282

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Part IV

283

Chapter 29 discusses Monte Carlo methods. Chapter 30 gives details of

state-of-the-art Monte Carlo techniques.

Chapter 31 introduces the Ising model as a test-bed for probabilistic meth-
ods. An exact message-passing method and a Monte Carlo method are demon-
strated. A motivation for studying the Ising model is that it is intimately
related to several neural network models. Chapter 32 describes ‘exact’ Monte
Carlo methods and demonstrates their application to the Ising model.

Chapter 33 discusses variational methods and their application to Ising
models and to simple statistical inference problems including clustering. This
chapter will help the reader understand the Hop(cid:12)eld network (Chapter 42)
and the EM algorithm, which is an important method in latent-variable mod-
elling. Chapter 34 discusses a particularly simple latent variable model called
independent component analysis.

Chapter 35 discusses a ragbag of assorted inference topics. Chapter 36
discusses a simple example of decision theory. Chapter 37 discusses di(cid:11)erences
between sampling theory and Bayesian methods.

A theme: what inference is about

A widespread misconception is that the aim of inference is to (cid:12)nd the most
probable explanation for some data. While this most probable hypothesis may
be of interest, and some inference methods do locate it, this hypothesis is just
the peak of a probability distribution, and it is the whole distribution that is
of interest. As we saw in Chapter 4, the most probable outcome from a source
is often not a typical outcome from that source. Similarly, the most probable
hypothesis given some data may be atypical of the whole set of reasonably-
plausible hypotheses.

About Chapter 20

Before reading the next chapter, exercise 2.17 (p.36) and section 11.2 (inferring
the input to a Gaussian channel) are recommended reading.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

20

An Example Inference Task: Clustering

Human brains are good at (cid:12)nding regularities in data. One way of expressing
regularity is to put a set of objects into groups that are similar to each other.
For example, biologists have found that most objects in the natural world
fall into one of two categories:
things that are brown and run away, and
things that are green and don’t run away. The (cid:12)rst group they call animals,
and the second, plants. We’ll call this operation of grouping things together
clustering. If the biologist further sub-divides the cluster of plants into sub-
clusters, we would call this ‘hierarchical clustering’; but we won’t be talking
about hierarchical clustering yet.
In this chapter we’ll just discuss ways to
take a set of N objects and group them into K clusters.

There are several motivations for clustering. First, a good clustering has
predictive power. When an early biologist encounters a new green thing he has
not seen before, his internal model of plants and animals (cid:12)lls in predictions for
attributes of the green thing: it’s unlikely to jump on him and eat him; if he
touches it, he might get grazed or stung; if he eats it, he might feel sick. All of
these predictions, while uncertain, are useful, because they help the biologist
invest his resources (for example, the time spent watching for predators) well.
Thus, we perform clustering because we believe the underlying cluster labels
are meaningful, will lead to a more e(cid:14)cient description of our data, and will
help us choose better actions. This type of clustering is sometimes called
‘mixture density modelling’, and the objective function that measures how
well the predictive model is working is the information content of the data,
log 1=P (fxg).

Second, clusters can be a useful aid to communication because they allow
lossy compression. The biologist can give directions to a friend such as ‘go to
the third tree on the right then take a right turn’ (rather than ‘go past the
large green thing with red berries, then past the large green thing with thorns,
then : : :’). The brief category name ‘tree’ is helpful because it is su(cid:14)cient to
identify an object. Similarly, in lossy image compression, the aim is to convey
in as few bits as possible a reasonable reproduction of a picture; one way to do
this is to divide the image into N small patches, and (cid:12)nd a close match to each
patch in an alphabet of K image-templates; then we send a close (cid:12)t to the
image by sending the list of labels k1; k2; : : : ; kN of the matching templates.
The task of creating a good library of image-templates is equivalent to (cid:12)nding
a set of cluster centres. This type of clustering is sometimes called ‘vector
quantization’.

We can formalize a vector quantizer in terms of an assignment rule x !
k(x) for assigning datapoints x to one of K codenames, and a reconstruction
rule k ! m(k), the aim being to choose the functions k(x) and m(k) so as to

284

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

20.1: K-means clustering

285

minimize the expected distortion, which might be de(cid:12)ned to be

D =Xx

P (x)

1

2hm(k(x)) (cid:0) xi2

:

(20.1)

[The ideal objective function would be to minimize the psychologically per-
ceived distortion of the image. Since it is hard to quantify the distortion
perceived by a human, vector quantization and lossy compression are not so
crisply de(cid:12)ned problems as data modelling and lossless compression.] In vec-
tor quantization, we don’t necessarily believe that the templates fm(k)g have
any natural meaning; they are simply tools to do a job. We note in passing
the similarity of the assignment rule (i.e., the encoder) of vector quantization
to the decoding problem when decoding an error-correcting code.

A third reason for making a cluster model is that failures of the cluster
model may highlight interesting objects that deserve special attention.
If
we have trained a vector quantizer to do a good job of compressing satellite
pictures of ocean surfaces, then maybe patches of image that are not well
compressed by the vector quantizer are the patches that contain ships! If the
biologist encounters a green thing and sees it run (or slither) away, this mis(cid:12)t
with his cluster model (which says green things don’t run away) cues him
to pay special attention. One can’t spend all one’s time being fascinated by
things; the cluster model can help sift out from the multitude of objects in
one’s world the ones that really deserve attention.

A fourth reason for liking clustering algorithms is that they may serve
as models of learning processes in neural systems. The clustering algorithm
that we now discuss, the K-means algorithm, is an example of a competitive
learning algorithm. The algorithm works by having the K clusters compete
with each other for the right to own the data points.

20.1 K-means clustering

Figure 20.1. N = 40 data points.

The K-means algorithm is an algorithm for putting N data points in an I-
dimensional space into K clusters. Each cluster is parameterized by a vector
m(k) called its mean.

The data points will be denoted by fx(n)g where the superscript n runs
from 1 to the number of data points N . Each vector x has I components xi.
We will assume that the space that x lives in is a real space and that we have
a metric that de(cid:12)nes distances between points, for example,

About the name... As far as I
know, the ‘K’ in K-means
clustering simply refers to the
chosen number of clusters. If
Newton had followed the same
naming policy, maybe we would
learn at school about ‘calculus for
the variable x’. It’s a silly name,
but we are stuck with it.

d(x; y) =

1

2Xi

(xi (cid:0) yi)2:

(20.2)

To start the K-means algorithm (algorithm 20.2), the K means fm(k)g
are initialized in some way, for example to random values. K-means is then
an iterative two-step algorithm. In the assignment step, each data point n is
assigned to the nearest mean. In the update step, the means are adjusted to
match the sample means of the data points that they are responsible for.

The K-means algorithm is demonstrated for a toy two-dimensional data set
in (cid:12)gure 20.3, where 2 means are used. The assignments of the points to the
two clusters are indicated by two point styles, and the two means are shown
by the circles. The algorithm converges after three iterations, at which point
the assignments are unchanged so the means remain unmoved when updated.
The K-means algorithm always converges to a (cid:12)xed point.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

286

20 | An Example Inference Task: Clustering

Algorithm 20.2. The K-means
clustering algorithm.

Initialization. Set K means fm(k)g to random values.
Assignment step. Each data point n is assigned to the nearest mean.
We denote our guess for the cluster k(n) that the point x(n) belongs
to by ^k(n).

fd(m(k); x(n))g:

(20.3)

^k(n) = argmin

k

An alternative, equivalent representation of this assignment of
points to clusters is given by ‘responsibilities’, which are indicator
variables r(n)
to one if mean k
is the closest mean to datapoint x(n); otherwise r(n)

k . In the assignment step, we set r(n)

is zero.

k

k

r(n)

k =(cid:26) 1

0

if
if

^k(n) = k
^k(n) 6= k:

(20.4)

What about ties? { We don’t expect two means to be exactly the
same distance from a data point, but if a tie does happen, ^k(n) is
set to the smallest of the winning fkg.

Update step. The model parameters, the means, are adjusted to match
the sample means of the data points that they are responsible for.

m(k) = Xn

r(n)
k x(n)

R(k)

where R(k) is the total responsibility of mean k,

R(k) =Xn

r(n)
k :

(20.5)

(20.6)

What about means with no responsibilities? { If R(k) = 0, then we
leave the mean m(k) where it is.

Repeat the assignment step and update step until

the

assign-

ments do not change.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

20.1: K-means clustering

Data:

287

Figure 20.3. K-means algorithm
applied to a data set of 40 points.
K = 2 means evolve to stable
locations after three iterations.

Assignment

Update

Assignment

Update

Assignment

Update

Run 1

Run 2

Figure 20.4. K-means algorithm
applied to a data set of 40 points.
Two separate runs, both with
K = 4 means, reach di(cid:11)erent
solutions. Each frame shows a
successive assignment step.

Exercise 20.1.[4, p.291] See if you can prove that K-means always converges.

[Hint: (cid:12)nd a physical analogy and an associated Lyapunov function.]

[A Lyapunov function is a function of the state of the algorithm that
decreases whenever the state changes and that is bounded below. If a
system has a Lyapunov function then its dynamics converge.]

The K-means algorithm with a larger number of means, 4, is demonstrated in
(cid:12)gure 20.4. The outcome of the algorithm depends on the initial condition.
In the (cid:12)rst case, after (cid:12)ve iterations, a steady state is found in which the data
points are fairly evenly split between the four clusters. In the second case,
after six iterations, half the data points are in one cluster, and the others are
shared among the other three clusters.

Questions about this algorithm

The K-means algorithm has several ad hoc features. Why does the update step
set the ‘mean’ to the mean of the assigned points? Where did the distance d
come from? What if we used a di(cid:11)erent measure of distance between x and m?
How can we choose the ‘best’ distance? [In vector quantization, the distance

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

288

(a)

10

8

6

4

2

0

0

2

4

6

8

10

20 | An Example Inference Task: Clustering

(b)

10

8

6

4

2

0

0

2

4

6

8

10

Figure 20.5. K-means algorithm
for a case with two dissimilar
clusters. (a) The \little ’n’ large"
data. (b) A stable set of
assignments and means. Note that
four points belonging to the broad
cluster have been incorrectly
assigned to the narrower cluster.
(Points assigned to the right-hand
cluster are shown by plus signs.)

Figure 20.6. Two elongated
clusters, and the stable solution
found by the K-means algorithm.

(a)

(b)

function is provided as part of the problem de(cid:12)nition; but I’m assuming we
are interested in data-modelling rather than vector quantization.] How do we
choose K? Having found multiple alternative clusterings for a given K, how
can we choose among them?

Cases where K-means might be viewed as failing.

Further questions arise when we look for cases where the algorithm behaves
badly (compared with what the man in the street would call ‘clustering’).
Figure 20.5a shows a set of 75 data points generated from a mixture of two
Gaussians. The right-hand Gaussian has less weight (only one (cid:12)fth of the data
points), and it is a less broad cluster. Figure 20.5b shows the outcome of using
K-means clustering with K = 2 means. Four of the big cluster’s data points
have been assigned to the small cluster, and both means end up displaced
to the left of the true centres of the clusters. The K-means algorithm takes
account only of the distance between the means and the data points; it has
no representation of the weight or breadth of each cluster. Consequently, data
points that actually belong to the broad cluster are incorrectly assigned to the
narrow cluster.

Figure 20.6 shows another case of K-means behaving badly. The data
evidently fall into two elongated clusters. But the only stable state of the
K-means algorithm is that shown in (cid:12)gure 20.6b: the two clusters have been
sliced in half! These two examples show that there is something wrong with
the distance d in the K-means algorithm. The K-means algorithm has no way
of representing the size or shape of a cluster.

A (cid:12)nal criticism of K-means is that it is a ‘hard’ rather than a ‘soft’
algorithm: points are assigned to exactly one cluster and all points assigned
to a cluster are equals in that cluster. Points located near the border between
two or more clusters should, arguably, play a partial role in determining the
locations of all the clusters that they could plausibly be assigned to. But in
the K-means algorithm, each borderline point is dumped in one cluster, and

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

20.2: Soft K-means clustering

289

has an equal vote with all the other points in that cluster, and no vote in any
other clusters.

20.2 Soft K-means clustering

These criticisms of K-means motivate the ‘soft K-means algorithm’, algo-
rithm 20.7. The algorithm has one parameter, (cid:12), which we could term the
sti(cid:11)ness.

Assignment step. Each data point x(n) is given a soft ‘degree of as-
signment’ to each of the means. We call the degree to which x(n)
is assigned to cluster k the responsibility r(n)
(the responsibility of
cluster k for point n).

k

Algorithm 20.7. Soft K-means
algorithm, version 1.

r(n)
k =

exp(cid:0)(cid:0)(cid:12) d(m(k); x(n))(cid:1)
Pk0 exp(cid:0)(cid:0)(cid:12) d(m(k0); x(n))(cid:1) :

The sum of the K responsibilities for the nth point is 1.

(20.7)

Update step. The model parameters, the means, are adjusted to match
the sample means of the data points that they are responsible for.

m(k) = Xn

r(n)
k x(n)

R(k)

where R(k) is the total responsibility of mean k,

R(k) =Xn

r(n)
k :

(20.8)

(20.9)

Notice the similarity of this soft K-means algorithm to the hard K-means
algorithm 20.2. The update step is identical; the only di(cid:11)erence is that the
responsibilities r(n)
can take on values between 0 and 1. Whereas the assign-
ment ^k(n) in the K-means algorithm involved a ‘min’ over the distances, the
rule for assigning the responsibilities is a ‘soft-min’ (20.7).

k

. Exercise 20.2.[2 ] Show that as the sti(cid:11)ness (cid:12) goes to 1, the soft K-means algo-

rithm becomes identical to the original hard K-means algorithm, except
for the way in which means with no assigned points behave. Describe
what those means do instead of sitting still.

Dimensionally, the sti(cid:11)ness (cid:12) is an inverse-length-squared, so we can as-

sociate a lengthscale, (cid:27) (cid:17) 1=p(cid:12), with it. The soft K-means algorithm is

demonstrated in (cid:12)gure 20.8. The lengthscale is shown by the radius of the
circles surrounding the four means. Each panel shows the (cid:12)nal (cid:12)xed point
reached for a di(cid:11)erent value of the lengthscale (cid:27).

20.3 Conclusion

At this point, we may have (cid:12)xed some of the problems with the original K-
means algorithm by introducing an extra complexity-control parameter (cid:12). But
how should we set (cid:12)? And what about the problem of the elongated clusters,

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

290

Large (cid:27) : : :

: : :

20 | An Example Inference Task: Clustering

Figure 20.8. Soft K-means
algorithm, version 1, applied to a
data set of 40 points. K = 4.
Implicit lengthscale parameter
(cid:27) = 1=(cid:12)1=2 varied from a large to
a small value. Each picture shows
the state of all four means, with
the implicit lengthscale shown by
the radius of the four circles, after
running the algorithm for several
tens of iterations. At the largest
lengthscale, all four means
converge exactly to the data
mean. Then the four means
separate into two groups of two.
At shorter lengthscales, each of
these pairs itself bifurcates into
subgroups.

: : : small (cid:27)

and the clusters of unequal weight and width? Adding one sti(cid:11)ness parameter
(cid:12) is not going to make all these problems go away.

We’ll come back to these questions in a later chapter, as we develop the

mixture-density-modelling view of clustering.

Further reading

For a vector-quantization approach to clustering see (Luttrell, 1989; Luttrell,
1990).

20.4 Exercises

. Exercise 20.3.[3, p.291] Explore the properties of the soft K-means algorithm,
version 1, assuming that the datapoints fxg come from a single separable
two-dimensional Gaussian distribution with mean zero and variances
(var(x1); var(x2)) = ((cid:27)2
2. Set K = 2, assume N is
large, and investigate the (cid:12)xed points of the algorithm as (cid:12) is varied.
[Hint: assume that m(1) = (m; 0) and m(2) = ((cid:0)m; 0).]

2), with (cid:27)2

1; (cid:27)2

1 > (cid:27)2

. Exercise 20.4.[3 ] Consider the soft K-means algorithm applied to a large
amount of one-dimensional data that comes from a mixture of two equal-
weight Gaussians with true means (cid:22) = (cid:6)1 and standard deviation (cid:27)P ,
for example (cid:27)P = 1. Show that the hard K-means algorithm with K = 2
leads to a solution in which the two means are further apart than the
two true means. Discuss what happens for other values of (cid:12), and (cid:12)nd
the value of (cid:12) such that the soft algorithm puts the two means in the
correct places.

-1

1

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

20.5: Solutions

20.5 Solutions

Solution to exercise 20.1 (p.287). We can associate an ‘energy’ with the state
of the K-means algorithm by connecting a spring between each point x(n) and
the mean that is responsible for it. The energy of one spring is proportional to
its squared length, namely (cid:12)d(x(n); m(k)) where (cid:12) is the sti(cid:11)ness of the spring.
The total energy of all the springs is a Lyapunov function for the algorithm,
because (a) the assignment step can only decrease the energy { a point only
changes its allegiance if the length of its spring would be reduced; (b) the
update step can only decrease the energy { moving m(k) to the mean is the
way to minimize the energy of its springs; and (c) the energy is bounded below
{ which is the second condition for a Lyapunov function. Since the algorithm
has a Lyapunov function, it converges.

291

m1
m2

If the means are initialized to m(1) = (m; 0)
Solution to exercise 20.3 (p.290).
and m(1) = ((cid:0)m; 0), the assignment step for a point at location x1; x2 gives

m2

m1

r1(x) =

=

and the updated m is

exp((cid:0)(cid:12)(x1 (cid:0) m)2=2)

exp((cid:0)(cid:12)(x1 (cid:0) m)2=2) + exp((cid:0)(cid:12)(x1 + m)2=2)
1 + exp((cid:0)2(cid:12)mx1)

1

;

m0 = R dx1 P (x1) x1 r1(x)
R dx1 P (x1) r1(x)
= 2Z dx1 P (x1) x1

1

1 + exp((cid:0)2(cid:12)mx1)

(20.10)

(20.11)

(20.12)

Figure 20.9. Schematic diagram of
the bifurcation as the largest data
variance (cid:27)1 increases from below
1=(cid:12)1=2 to above 1=(cid:12)1=2. The data
variance is indicated by the
ellipse.

:

(20.13)

Now, m = 0 is a (cid:12)xed point, but the question is, is it stable or unstable? For
tiny m (that is, (cid:12)(cid:27)1m (cid:28) 1), we can Taylor-expand

1

1 + exp((cid:0)2(cid:12)mx1) ’

1
2

(1 + (cid:12)mx1) + (cid:1)(cid:1)(cid:1)

so

m0 ’ Z dx1 P (x1) x1 (1 + (cid:12)mx1)

= (cid:27)2

1(cid:12)m:

(20.14)

(20.15)

(20.16)

For small m, m either grows or decays exponentially under this mapping,
depending on whether (cid:27)2
1(cid:12) is greater than or less than 1. The (cid:12)xed point
m = 0 is stable if

(cid:27)2
1 (cid:20) 1=(cid:12)

(20.17)

If (cid:27)2

and unstable otherwise. [Incidentally, this derivation shows that this result is
general, holding for any true probability distribution P (x1) having variance
(cid:27)2
1, not just the Gaussian.]

1 > 1=(cid:12) then there is a bifurcation and there are two stable (cid:12)xed points
surrounding the unstable (cid:12)xed point at m = 0. To illustrate this bifurcation,
(cid:12)gure 20.10 shows the outcome of running the soft K-means algorithm with
(cid:12) = 1 on one-dimensional data with standard deviation (cid:27)1 for various values of
(cid:27)1. Figure 20.11 shows this pitchfork bifurcation from the other point of view,
where the data’s standard deviation (cid:27)1 is (cid:12)xed and the algorithm’s lengthscale
(cid:27) = 1=(cid:12)1=2 is varied on the horizontal axis.

4
3
2
1
0
-1
-2
-3
-4

Data density

Mean locations

-2-1 0 1 2

-2-1 0 1 2

0

0.5

1

1.5

2

2.5

3

3.5

4

Figure 20.10. The stable mean
locations as a function of (cid:27)1, for
constant (cid:12), found numerically
(thick lines), and the
approximation (20.22) (thin lines).

0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8

Data density

Mean locns.

-2 -1 0 1 2

-2 -1 0 1 2

0

0.5

1

1.5

2

Figure 20.11. The stable mean
locations as a function of 1=(cid:12)1=2,
for constant (cid:27)1.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

292

20 | An Example Inference Task: Clustering

Here is a cheap theory to model how the (cid:12)tted parameters (cid:6)m behave beyond
the bifurcation, based on continuing the series expansion. This continuation
of the series is rather suspect, since the series isn’t necessarily expected to
converge beyond the bifurcation point, but the theory (cid:12)ts well anyway.

We take our analytic approach one term further in the expansion

1

1 + exp((cid:0)2(cid:12)mx1) ’

1
2

(1 + (cid:12)mx1 (cid:0)

1
3

((cid:12)mx1)3) + (cid:1)(cid:1)(cid:1)

(20.18)

then we can solve for the shape of the bifurcation to leading order, which
depends on the fourth moment of the distribution:

m0 ’ Z dx1 P (x1)x1(1 + (cid:12)mx1 (cid:0)

((cid:12)m)33(cid:27)4
1:

= (cid:27)2

1(cid:12)m (cid:0)

1
3

1
3

((cid:12)mx1)3)

(20.19)

(20.20)

[At (20.20) we use the fact that P (x1) is Gaussian to (cid:12)nd the fourth moment.]
This map has a (cid:12)xed point at m such that

(cid:27)2
1(cid:12)(1 (cid:0) ((cid:12)m)2(cid:27)2

1) = 1;

(20.21)

i.e.,

m = (cid:6)(cid:12)(cid:0)1=2 ((cid:27)2

1(cid:12) (cid:0) 1)1=2

(cid:27)2
1(cid:12)

:

(20.22)

The thin line in (cid:12)gure 20.10 shows this theoretical approximation. Figure 20.10
shows the bifurcation as a function of (cid:27)1 for (cid:12)xed (cid:12); (cid:12)gure 20.11 shows the
bifurcation as a function of 1=(cid:12)1=2 for (cid:12)xed (cid:27)1.

. Exercise 20.5.[2, p.292] Why does the pitchfork in (cid:12)gure 20.11 tend to the val-
ues (cid:24)(cid:6)0:8 as 1=(cid:12)1=2 ! 0? Give an analytic expression for this asymp-
tote.

Solution to exercise 20.5 (p.292). The asymptote is the mean of the recti(cid:12)ed
Gaussian,

R 10 Normal(x; 1)x dx

1=2

=p2=(cid:25) ’ 0:798:

(20.23)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

21

Exact Inference by Complete

Enumeration

We open our toolbox of methods for handling probabilities by discussing a
brute-force inference method: complete enumeration of all hypotheses, and
evaluation of their probabilities. This approach is an exact method, and the
di(cid:14)culty of carrying it out will motivate the smarter exact and approximate
methods introduced in the following chapters.

21.1 The burglar alarm

Bayesian probability theory is sometimes called ‘common sense, ampli(cid:12)ed’.
When thinking about the following questions, please ask your common sense
what it thinks the answers are; we will then see how Bayesian methods con(cid:12)rm
your everyday intuition.

Example 21.1. Fred lives in Los Angeles and commutes 60 miles to work.
Whilst at work, he receives a phone-call from his neighbour saying that
Fred’s burglar alarm is ringing. What is the probability that there was
a burglar in his house today? While driving home to investigate, Fred
hears on the radio that there was a small earthquake that day near his
home.
‘Oh’, he says, feeling relieved, ‘it was probably the earthquake
that set o(cid:11) the alarm’. What is the probability that there was a burglar
in his house? (After Pearl, 1988).

Let’s introduce variables b (a burglar was present in Fred’s house today),
a (the alarm is ringing), p (Fred receives a phonecall from the neighbour re-
porting the alarm), e (a small earthquake took place today near Fred’s house),
and r (the radio report of earthquake is heard by Fred). The probability of
all these variables might factorize as follows:

P (b; e; a; p; r) = P (b)P (e)P (aj b; e)P (pj a)P (r j e);

(21.1)

and plausible values for the probabilities are:

1. Burglar probability:

P (b = 1) = (cid:12); P (b = 0) = 1 (cid:0) (cid:12);

(21.2)

e.g., (cid:12) = 0:001 gives a mean burglary rate of once every three years.

2. Earthquake probability:

P (e = 1) = (cid:15); P (e = 0) = 1 (cid:0) (cid:15);

(21.3)

293

jEarthquake
(cid:0)(cid:0)(cid:9)jRadio

@@R

jBurglar
(cid:0)(cid:0)(cid:9)jAlarm
@@R jPhonecall

Figure 21.1. Belief network for the
burglar alarm problem.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

294

21 | Exact Inference by Complete Enumeration

with, e.g., (cid:15) = 0:001; our assertion that the earthquakes are independent
of burglars, i.e., the prior probability of b and e is P (b; e) = P (b)P (e),
seems reasonable unless we take into account opportunistic burglars who
strike immediately after earthquakes.

3. Alarm ringing probability: we assume the alarm will ring if any of the
following three events happens: (a) a burglar enters the house, and trig-
gers the alarm (let’s assume the alarm has a reliability of (cid:11)b = 0:99, i.e.,
99% of burglars trigger the alarm); (b) an earthquake takes place, and
triggers the alarm (perhaps (cid:11)e = 1% of alarms are triggered by earth-
quakes?); or (c) some other event causes a false alarm; let’s assume the
false alarm rate f is 0.001, so Fred has false alarms from non-earthquake
causes once every three years. [This type of dependence of a on b and e
is known as a ‘noisy-or’.] The probabilities of a given b and e are then:

P (a = 1j b = 0; e = 0) = f
P (a = 1j b = 1; e = 0) = 1 (cid:0) (1 (cid:0) f )(1 (cid:0) (cid:11)b)
P (a = 1j b = 0; e = 1) = 1 (cid:0) (1 (cid:0) f )(1 (cid:0) (cid:11)e)

P (a = 0j b = 0; e = 0) = (1 (cid:0) f );
P (a = 0j b = 1; e = 0) = (1 (cid:0) f )(1 (cid:0) (cid:11)b);
P (a = 0j b = 0; e = 1) = (1 (cid:0) f )(1 (cid:0) (cid:11)e);
P (a = 0j b = 1; e = 1) = (1 (cid:0) f )(1 (cid:0) (cid:11)b)(1 (cid:0) (cid:11)e); P (a = 1j b = 1; e = 1) = 1 (cid:0) (1 (cid:0) f )(1 (cid:0) (cid:11)b)(1 (cid:0) (cid:11)e)
or, in numbers,
P (a = 0j b = 0; e = 0) = 0:999;
P (a = 0j b = 1; e = 0) = 0:009 99;
P (a = 0j b = 0; e = 1) = 0:989 01;
P (a = 0j b = 1; e = 1) = 0:009 890 1; P (a = 1j b = 1; e = 1) = 0:990 109 9:

P (a = 1j b = 0; e = 0) = 0:001
P (a = 1j b = 1; e = 0) = 0:990 01
P (a = 1j b = 0; e = 1) = 0:010 99

We assume the neighbour would never phone if the alarm is not ringing
[P (p = 1j a = 0) = 0]; and that the radio is a trustworthy reporter too
[P (r = 1j e = 0) = 0]; we won’t need to specify the probabilities P (p = 1j a = 1)
or P (r = 1j e = 1) in order to answer the questions above, since the outcomes
p = 1 and r = 1 give us certainty respectively that a = 1 and e = 1.
We can answer the two questions about the burglar by computing the
posterior probabilities of all hypotheses given the available information. Let’s
start by reminding ourselves that the probability that there is a burglar, before
either p or r is observed, is P (b = 1) = (cid:12) = 0:001, and the probability that an
earthquake took place is P (e = 1) = (cid:15) = 0:001, and these two propositions are
independent.

First, when p = 1, we know that the alarm is ringing: a = 1. The posterior

probability of b and e becomes:

P (b; ej a = 1) =

P (a = 1j b; e)P (b)P (e)

P (a = 1)

:

(21.4)

The numerator’s four possible values are
P (a = 1j b = 0; e = 0) (cid:2) P (b = 0) (cid:2) P (e = 0) = 0:001
(cid:2) 0:999(cid:2) 0:999 = 0:000 998
P (a = 1j b = 1; e = 0) (cid:2) P (b = 1) (cid:2) P (e = 0) = 0:990 01 (cid:2) 0:001(cid:2) 0:999 = 0:000 989
P (a = 1j b = 0; e = 1) (cid:2) P (b = 0) (cid:2) P (e = 1) = 0:010 99 (cid:2) 0:999(cid:2) 0:001 = 0:000 010 979
P (a = 1j b = 1; e = 1) (cid:2) P (b = 1) (cid:2) P (e = 1) = 0:990 109 9(cid:2) 0:001(cid:2) 0:001 = 9:9 (cid:2) 10(cid:0)7:
The normalizing constant is the sum of these four numbers, P (a = 1) = 0:002,
and the posterior probabilities are

P (b = 0; e = 0j a = 1) = 0:4993
P (b = 1; e = 0j a = 1) = 0:4947
P (b = 0; e = 1j a = 1) = 0:0055
P (b = 1; e = 1j a = 1) = 0:0005:

(21.5)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

21.2: Exact inference for continuous hypothesis spaces

295

To answer the question, ‘what’s the probability a burglar was there?’ we
marginalize over the earthquake variable e:

P (b = 0j a = 1) = P (b = 0; e = 0j a = 1) + P (b = 0; e = 1j a = 1) = 0:505
P (b = 1j a = 1) = P (b = 1; e = 0j a = 1) + P (b = 1; e = 1j a = 1) = 0:495:
(21.6)
So there is nearly a 50% chance that there was a burglar present. It is impor-
tant to note that the variables b and e, which were independent a priori, are
now dependent. The posterior distribution (21.5) is not a separable function of
b and e. This fact is illustrated most simply by studying the e(cid:11)ect of learning
that e = 1.

When we learn e = 1,

the posterior probability of b is given by
P (bj e = 1; a = 1) = P (b; e = 1j a = 1)=P (e = 1j a = 1), i.e., by dividing the bot-
tom two rows of (21.5), by their sum P (e = 1j a = 1) = 0:0060. The posterior
probability of b is:

P (b = 0j e = 1; a = 1) = 0:92
P (b = 1j e = 1; a = 1) = 0:08:

(21.7)

There is thus now an 8% chance that a burglar was in Fred’s house.
It is
in accordance with everyday intuition that the probability that b = 1 (a pos-
sible cause of the alarm) reduces when Fred learns that an earthquake, an
alternative explanation of the alarm, has happened.

Explaining away

This phenomenon, that one of the possible causes (b = 1) of some data (the
data in this case being a = 1) becomes less probable when another of the causes
(e = 1) becomes more probable, even though those two causes were indepen-
dent variables a priori, is known as explaining away. Explaining away is an
important feature of correct inferences, and one that any arti(cid:12)cial intelligence
should replicate.

If we believe that the neighbour and the radio service are unreliable or
capricious, so that we are not certain that the alarm really is ringing or that
an earthquake really has happened, the calculations become more complex,
but the explaining-away e(cid:11)ect persists; the arrival of the earthquake report r
simultaneously makes it more probable that the alarm truly is ringing, and
less probable that the burglar was present.

In summary, we solved the inference questions about the burglar by enu-
merating all four hypotheses about the variables (b; e), (cid:12)nding their posterior
probabilities, and marginalizing to obtain the required inferences about b.

. Exercise 21.2.[2 ] After Fred receives the phone-call about the burglar alarm,
but before he hears the radio report, what, from his point of view, is the
probability that there was a small earthquake today?

21.2 Exact inference for continuous hypothesis spaces

Many of the hypothesis spaces we will consider are naturally thought of as
continuous. For example, the unknown decay length (cid:21) of section 3.1 (p.48)
lives in a continuous one-dimensional space; and the unknown mean and stan-
dard deviation of a Gaussian (cid:22); (cid:27) live in a continuous two-dimensional space.
In any practical computer implementation, such continuous spaces will neces-
sarily be discretized, however, and so can, in principle, be enumerated { at a
grid of parameter values, for example. In (cid:12)gure 3.2 we plotted the likelihood

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

296

21 | Exact Inference by Complete Enumeration

Figure 21.2. Enumeration of an
entire (discretized) hypothesis
space for one Gaussian with
parameters (cid:22) (horizontal axis)
and (cid:27) (vertical).

function for the decay length as a function of (cid:21) by evaluating the likelihood
at a (cid:12)nely-spaced series of points.

A two-parameter model

Let’s look at the Gaussian distribution as an example of a model with a two-
dimensional hypothesis space. The one-dimensional Gaussian distribution is
parameterized by a mean (cid:22) and a standard deviation (cid:27):

P (xj (cid:22); (cid:27)) =

1

p2(cid:25)(cid:27)

exp(cid:18)(cid:0)

2(cid:27)2 (cid:19) (cid:17) Normal(x; (cid:22); (cid:27)2):
(x (cid:0) (cid:22))2

(21.8)

Figure 21.2 shows an enumeration of one hundred hypotheses about the mean
and standard deviation of a one-dimensional Gaussian distribution. These
hypotheses are evenly spaced in a ten by ten square grid covering ten values
of (cid:22) and ten values of (cid:27). Each hypothesis is represented by a picture showing
the probability density that it puts on x. We now examine the inference of (cid:22)
and (cid:27) given data points xn, n = 1; : : : ; N , assumed to be drawn independently
from this density.

Imagine that we acquire data, for example the (cid:12)ve points shown in (cid:12)g-
ure 21.3. We can now evaluate the posterior probability of each of the one
hundred subhypotheses by evaluating the likelihood of each, that is, the value
of P (fxng5
n=1 j (cid:22); (cid:27)). The likelihood values are shown diagrammatically in
(cid:12)gure 21.4 using the line thickness to encode the value of the likelihood. Sub-
hypotheses with likelihood smaller than e(cid:0)8 times the maximum likelihood
have been deleted.

Using a (cid:12)ner grid, we can represent the same information by plotting the
likelihood as a surface plot or contour plot as a function of (cid:22) and (cid:27) ((cid:12)gure 21.5).

A (cid:12)ve-parameter mixture model

Eyeballing the data ((cid:12)gure 21.3), you might agree that it seems more plau-
sible that they come not from a single Gaussian but from a mixture of two
Gaussians, de(cid:12)ned by two means, two standard deviations, and two mixing

-0.5

0

0.5

1

1.5

2

2.5

n=1. The horizontal

Figure 21.3. Five datapoints
fxng5
coordinate is the value of the
datum, xn; the vertical coordinate
has no meaning.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

21.2: Exact inference for continuous hypothesis spaces

297

Figure 21.4. Likelihood function,
given the data of (cid:12)gure 21.3,
represented by line thickness.
Subhypotheses having likelihood
smaller than e(cid:0)8 times the
maximum likelihood are not
shown.

0.06

0.05

0.04

0.03

0.02

0.01

1 0
0.8

0.6

sigma

0.4

0.2

0.5

1
mean

0

1.5

2

0

0.5

1

1.5

2

mean

1

0.9

0.8

0.7

0.6
0.5

0.4

0.3

0.2

0.1

sigma

Figure 21.5. The likelihood
function for the parameters of a
Gaussian distribution.
Surface plot and contour plot of
the log likelihood as a function of
(cid:22) and (cid:27). The data set of N = 5
points had mean (cid:22)x = 1:0 and

S =P(x (cid:0) (cid:22)x)2 = 1:0.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

298

21 | Exact Inference by Complete Enumeration

Figure 21.6. Enumeration of the
entire (discretized) hypothesis
space for a mixture of two
Gaussians. Weight of the mixture
components is (cid:25)1; (cid:25)2 = 0:6; 0:4 in
the top half and 0:8; 0:2 in the
bottom half. Means (cid:22)1 and (cid:22)2
vary horizontally, and standard
deviations (cid:27)1 and (cid:27)2 vary
vertically.

coe(cid:14)cients (cid:25)1 and (cid:25)2, satisfying (cid:25)1 + (cid:25)2 = 1, (cid:25)i (cid:21) 0.
1 (cid:17) +
exp(cid:16)(cid:0) (x(cid:0)(cid:22)1)2
P (xj (cid:22)1; (cid:27)1; (cid:25)1; (cid:22)2; (cid:27)2; (cid:25)2) =

(cid:25)1p2(cid:25)(cid:27)1

2(cid:27)2

(cid:25)2p2(cid:25)(cid:27)2

2 (cid:17)
exp(cid:16)(cid:0) (x(cid:0)(cid:22)2)2

2(cid:27)2

Let’s enumerate the subhypotheses for this alternative model. The parameter
space is (cid:12)ve-dimensional, so it becomes challenging to represent it on a single
page. Figure 21.6 enumerates 800 subhypotheses with di(cid:11)erent values of the
(cid:12)ve parameters (cid:22)1; (cid:22)2; (cid:27)1; (cid:27)2; (cid:25)1. The means are varied between (cid:12)ve values
each in the horizontal directions. The standard deviations take on four values
each vertically. And (cid:25)1 takes on two values vertically. We can represent the
inference about these (cid:12)ve parameters in the light of the (cid:12)ve datapoints as
shown in (cid:12)gure 21.7.

If we wish to compare the one-Gaussian model with the mixture-of-two
model, we can (cid:12)nd the models’ posterior probabilities by evaluating the
marginal likelihood or evidence for each model H, P (fxgjH). The evidence
is given by integrating over the parameters, (cid:18); the integration can be imple-
mented numerically by summing over the alternative enumerated values of
(cid:18),

P (fxgjH) =X

P ((cid:18))P (fxgj (cid:18);H);

(21.9)

where P ((cid:18)) is the prior distribution over the grid of parameter values, which
I take to be uniform.

For the mixture of two Gaussians this integral is a (cid:12)ve-dimensional integral;
if it is to be performed at all accurately, the grid of points will need to be
much (cid:12)ner than the grids shown in the (cid:12)gures. If the uncertainty about each
of K parameters has been reduced by, say, a factor of ten by observing the
data, then brute-force integration requires a grid of at least 10K points. This

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

21.2: Exact inference for continuous hypothesis spaces

299

Figure 21.7. Inferring a mixture of
two Gaussians. Likelihood
function, given the data of
(cid:12)gure 21.3, represented by line
thickness. The hypothesis space is
identical to that shown in
(cid:12)gure 21.6. Subhypotheses having
likelihood smaller than e(cid:0)8 times
the maximum likelihood are not
shown, hence the blank regions,
which correspond to hypotheses
that the data have ruled out.

-0.5

0

0.5

1

1.5

2

2.5

exponential growth of computation with model size is the reason why complete
enumeration is rarely a feasible computational strategy.

Exercise 21.3.[1 ] Imagine (cid:12)tting a mixture of ten Gaussians to data in a
twenty-dimensional space. Estimate the computational cost of imple-
menting inferences for this model by enumeration of a grid of parameter
values.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

22

Maximum Likelihood and Clustering

Rather than enumerate all hypotheses { which may be exponential in number
{ we can save a lot of time by homing in on one good hypothesis that (cid:12)ts
the data well. This is the philosophy behind the maximum likelihood method,
which identi(cid:12)es the setting of the parameter vector (cid:18) that maximizes the
likelihood, P (Data j (cid:18);H).
For some models the maximum likelihood parameters can be identi(cid:12)ed
instantly from the data; for more complex models, (cid:12)nding the maximum like-
lihood parameters may require an iterative algorithm.

For any model, it is usually easiest to work with the logarithm of the
likelihood rather than the likelihood, since likelihoods, being products of the
probabilities of many data points, tend to be very small. Likelihoods multiply;
log likelihoods add.

22.1 Maximum likelihood for one Gaussian

We return to the Gaussian for our (cid:12)rst examples. Assume we have data
fxngN

n=1. The log likelihood is:

ln P (fxngN

n=1 j (cid:22); (cid:27)) = (cid:0)N ln(p2(cid:25)(cid:27)) (cid:0)Xn

(xn (cid:0) (cid:22))2=(2(cid:27)2):

(22.1)

The likelihood can be expressed in terms of two functions of the data, the
sample mean

and the sum of square deviations

N

(cid:22)x (cid:17)

xn=N;

Xn=1
S (cid:17)Xn
(xn (cid:0) (cid:22)x)2 :

(22.2)

(22.3)

(22.4)

ln P (fxngN

n=1 j (cid:22); (cid:27)) = (cid:0)N ln(p2(cid:25)(cid:27)) (cid:0) [N ((cid:22) (cid:0) (cid:22)x)2 + S]=(2(cid:27)2):

Because the likelihood depends on the data only through (cid:22)x and S, these two
quantities are known as su(cid:14)cient statistics.

Example 22.1. Di(cid:11)erentiate the log likelihood with respect to (cid:22) and show that,
if the standard deviation is known to be (cid:27), the maximum likelihood mean
(cid:22) of a Gaussian is equal to the sample mean (cid:22)x, for any value of (cid:27).

Solution.

@
@(cid:22)

ln P = (cid:0)

N ((cid:22) (cid:0) (cid:22)x)

(cid:27)2

= 0 when (cid:22) = (cid:22)x.

300

(22.5)

2 (22.6)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

22.1: Maximum likelihood for one Gaussian

301

Figure 22.1. The likelihood
function for the parameters of a
Gaussian distribution.
(a1, a2) Surface plot and contour
plot of the log likelihood as a
function of (cid:22) and (cid:27). The data set
of N = 5 points had mean (cid:22)x = 1:0

and S =P(x (cid:0) (cid:22)x)2 = 1:0.

(b) The posterior probability of (cid:22)
for various values of (cid:27).
(c) The posterior probability of (cid:27)
for various (cid:12)xed values of (cid:22)
(shown as a density over ln (cid:27)).

0.06

0.05

0.04

0.03

0.02

0.01

1 0
0.8

0.6

sigma

(a1)

0.4

0.2

1

0.9

0.8

0.7

0.6
0.5

0.4

0.3

0.2

0.1

sigma

0.5

1
mean

0

1.5

2

0
(a2)

0.5

1

1.5

2

mean

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

r
o
i
r
e

t
s
o
P

(b)

mu=1   
mu=1.25
mu=1.5 

sigma=0.2
sigma=0.4
sigma=0.6

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

(c)

0.2

0.4

0.6

0.8

1

mean

1.2

1.4

1.6

1.8

2

0
0.2

0.4

0.6

0.8

1

1.2 1.4 1.6 1.8 2

If we Taylor-expand the log likelihood about the maximum, we can de-
(cid:12)ne approximate error bars on the maximum likelihood parameter: we use
a quadratic approximation to estimate how far from the maximum-likelihood
parameter setting we can go before the likelihood falls by some standard fac-
tor, for example e1=2, or e4=2.
In the special case of a likelihood that is a
Gaussian function of the parameters, the quadratic approximation is exact.

Example 22.2. Find the second derivative of the log likelihood with respect to

(cid:22), and (cid:12)nd the error bars on (cid:22), given the data and (cid:27).

Solution.

@2
@(cid:22)2 ln P = (cid:0)

N
(cid:27)2 :

2 (22.7)

Comparing this curvature with the curvature of the log of a Gaussian distri-
bution over (cid:22) of standard deviation (cid:27)(cid:22), exp((cid:0)(cid:22)2=(2(cid:27)2
(cid:22), we
can deduce that the error bars on (cid:22) (derived from the likelihood function) are

(cid:22))), which is (cid:0)1=(cid:27)2

(cid:27)(cid:22) =

(cid:27)
pN

:

(22.8)

The error bars have this property: at the two points (cid:22) = (cid:22)x(cid:6) (cid:27)(cid:22), the likelihood
is smaller than its maximum value by a factor of e1=2.

Example 22.3. Find the maximum likelihood standard deviation (cid:27) of a Gaus-
sian, whose mean is known to be (cid:22), in the light of data fxngN
n=1. Find
the second derivative of the log likelihood with respect to ln (cid:27), and error
bars on ln (cid:27).

Solution. The likelihood’s dependence on (cid:27) is

ln P (fxngN

n=1 j (cid:22); (cid:27)) = (cid:0)N ln(p2(cid:25)(cid:27)) (cid:0)

Stot
(2(cid:27)2)

;

(22.9)

where Stot = Pn(xn (cid:0) (cid:22))2. To (cid:12)nd the maximum of the likelihood, we can

di(cid:11)erentiate with respect to ln (cid:27). [It’s often most hygienic to di(cid:11)erentiate with

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

302

22 | Maximum Likelihood and Clustering

respect to ln u rather than u, when u is a scale variable; we use dun=d(ln u) =
nun.]

@ ln P (fxngN
@ ln (cid:27)

n=1 j (cid:22); (cid:27))

= (cid:0)N +

Stot
(cid:27)2

This derivative is zero when

(cid:27)2 =

Stot
N

;

(22.10)

(22.11)

i.e.,

The second derivative is

(cid:27) =sPN

n=1(xn (cid:0) (cid:22))2

N

:

(22.12)

@2 ln P (fxngN
@(ln (cid:27))2

n=1 j (cid:22); (cid:27))

= (cid:0)2

Stot
(cid:27)2 ;

(22.13)

and at the maximum-likelihood value of (cid:27)2, this equals (cid:0)2N . So error bars
on ln (cid:27) are

(cid:27)ln (cid:27) =

:

2

(22.14)

1
p2N

. Exercise 22.4.[1 ] Show that the values of (cid:22) and ln (cid:27) that jointly maximize the

likelihood are: f(cid:22); (cid:27)gML =n(cid:22)x; (cid:27)N =pS=No ; where

(cid:27)N (cid:17)sPN

n=1(xn (cid:0) (cid:22)x)2

N

:

(22.15)

22.2 Maximum likelihood for a mixture of Gaussians

We now derive an algorithm for (cid:12)tting a mixture of Gaussians to one-
dimensional data. In fact, this algorithm is so important to understand that,
you, gentle reader, get to derive the algorithm. Please work through the fol-
lowing exercise.

Exercise 22.5.[2, p.310] A random variable x is assumed to have a probability
distribution that is a mixture of two Gaussians,

P (xj (cid:22)1; (cid:22)2; (cid:27)) =" 2
Xk=1

pk

1

p2(cid:25)(cid:27)2

exp(cid:18)(cid:0)

2(cid:27)2 (cid:19)# ;
(x (cid:0) (cid:22)k)2

(22.16)

where the two Gaussians are given the labels k = 1 and k = 2; the prior
probability of the class label k is fp1 = 1=2; p2 = 1=2g; f(cid:22)kg are the means
of the two Gaussians; and both have standard deviation (cid:27). For brevity, we
denote these parameters by (cid:18) (cid:17) ff(cid:22)kg; (cid:27)g.
A data set consists of N points fxngN
n=1 which are assumed to be indepen-
dent samples from this distribution. Let kn denote the unknown class label of
the nth point.

Assuming that f(cid:22)kg and (cid:27) are known, show that the posterior probability

of the class label kn of the nth point can be written as

P (kn = 1j xn; (cid:18)) =

P (kn = 2j xn; (cid:18)) =

1

1 + exp[(cid:0)(w1xn + w0)]

1

1 + exp[+(w1xn + w0)]

;

(22.17)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

22.3: Enhancements to soft K-means

and give expressions for w1 and w0.

303

Assume now that the means f(cid:22)kg are not known, and that we wish to
infer them from the data fxngN
n=1. (The standard deviation (cid:27) is known.) In
the remainder of this question we will derive an iterative algorithm for (cid:12)nding
values for f(cid:22)kg that maximize the likelihood,
n=1 jf(cid:22)kg; (cid:27)) =Yn

P (xn jf(cid:22)kg; (cid:27)):

P (fxngN

(22.18)

Let L denote the natural log of the likelihood. Show that the derivative of the
log likelihood with respect to (cid:22)k is given by

@
@(cid:22)k

L =Xn

(xn (cid:0) (cid:22)k)

(cid:27)2

;

pkjn

(22.19)

where pkjn (cid:17) P (kn = k j xn; (cid:18)) appeared above at equation (22.17).

Show, neglecting terms in @
@(cid:22)k

is approximately given by

P (kn = k j xn; (cid:18)), that the second derivative

@2
@(cid:22)2
k

L = (cid:0)Xn

1
(cid:27)2 :

pkjn

(22.20)

Hence show that from an initial state (cid:22)1; (cid:22)2, an approximate Newton{Raphson
step updates these parameters to (cid:22)01; (cid:22)02, where

(cid:22)0k = Pn pkjnxn
Pn pkjn

:

(22.21)

[The Newton{Raphson method for maximizing L((cid:22)) updates (cid:22) to (cid:22)0 = (cid:22) (cid:0)
h @L
@(cid:22). @2L
@(cid:22)2i.]

0

1

2

3

4

5

6

Assuming that (cid:27) = 1, sketch a contour plot of the likelihood function as a
function of (cid:22)1 and (cid:22)2 for the data set shown above. The data set consists of
32 points. Describe the peaks in your sketch and indicate their widths.

Notice that the algorithm you have derived for maximizing the likelihood
is identical to the soft K-means algorithm of section 20.4. Now that it is clear
that clustering can be viewed as mixture-density-modelling, we are able to
derive enhancements to the K-means algorithm, which rectify the problems
we noted earlier.

22.3 Enhancements to soft K-means

Algorithm 22.2 shows a version of the soft-K-means algorithm corresponding
to a modelling assumption that each cluster is a spherical Gaussian having its
own width (each cluster has its own (cid:12) (k) = 1/ (cid:27)2
k). The algorithm updates the
lengthscales (cid:27)k for itself. The algorithm also includes cluster weight parame-
ters (cid:25)1; (cid:25)2; : : : ; (cid:25)K which also update themselves, allowing accurate modelling
of data from clusters of unequal weights. This algorithm is demonstrated in
(cid:12)gure 22.3 for two data sets that we’ve seen before. The second example shows

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

304

22 | Maximum Likelihood and Clustering

Assignment step. The responsibilities are

Algorithm 22.2. The soft K-means
algorithm, version 2.

r(n)
k =

(cid:25)k

1

1
(cid:27)2
k

(p2(cid:25)(cid:27)k)I exp(cid:18)(cid:0)
(p2(cid:25)(cid:27)k0 )I exp(cid:18)(cid:0)

d(m(k); x(n))(cid:19)
d(m(k0); x(n))(cid:19)

1
(cid:27)2
k0

1

Pk0 (cid:25)k

where I is the dimensionality of x.

(22.22)

Update step. Each cluster’s parameters, m(k), (cid:25)k, and (cid:27)2

k, are adjusted

to match the data points that it is responsible for.

r(n)
k x(n)

R(k)

m(k) = Xn
k = Xn

(cid:27)2

r(n)
k (x(n) (cid:0) m(k))2

where R(k) is the total responsibility of mean k,

IR(k)
R(k)

(cid:25)k =

Pk R(k)
R(k) =Xn

r(n)
k :

(22.23)

(22.24)

(22.25)

(22.26)

t = 0

t = 1

t = 2

t = 3

t = 9

Figure 22.3. Soft K-means
algorithm, with K = 2, applied
(a) to the 40-point data set of
(cid:12)gure 20.3; (b) to the little ’n’
large data set of (cid:12)gure 20.5.

t = 0

t = 1

t = 10

t = 20

t = 30

t = 35

(cid:25)k

r(n)
k =

I

exp (cid:0)

1

(m(k)

i (cid:0) x(n)

i=1 p2(cid:25)(cid:27)(k)
QI
Pk0 (numerator, with k0 in place of k)

Xi=1

i

i

)2. 2((cid:27)(k)

i

= Xn

r(n)
k (x(n)

i

i (cid:0) m(k)
R(k)

)2

(k)

(cid:27)2
i

Algorithm 22.4. The soft K-means
algorithm, version 3, which
corresponds to a model of
axis-aligned Gaussians.

)2!

(22.27)

(22.28)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

22.4: A fatal (cid:13)aw of maximum likelihood

305

t = 0

t = 10

t = 20

t = 30

Figure 22.5. Soft K-means
algorithm, version 3, applied to
the data consisting of two
cigar-shaped clusters. K = 2 (cf.
(cid:12)gure 20.6).

t = 0

t = 10

t = 20

t = 26

t = 32

Figure 22.6. Soft K-means
algorithm, version 3, applied to
the little ’n’ large data set. K = 2.

that convergence can take a long time, but eventually the algorithm identi(cid:12)es
the small cluster and the large cluster.

Soft K-means, version 2, is a maximum-likelihood algorithm for (cid:12)tting a
mixture of spherical Gaussians to data { ‘spherical’ meaning that the variance
of the Gaussian is the same in all directions. This algorithm is still no good
at modelling the cigar-shaped clusters of (cid:12)gure 20.6. If we wish to model the
clusters by axis-aligned Gaussians with possibly-unequal variances, we replace
the assignment rule (22.22) and the variance update rule (22.24) by the rules
(22.27) and (22.28) displayed in algorithm 22.4.

This third version of soft K-means is demonstrated in (cid:12)gure 22.5 on the
‘two cigars’ data set of (cid:12)gure 20.6. After 30 iterations, the algorithm correctly
locates the two clusters. Figure 22.6 shows the same algorithm applied to the
little ’n’ large data set; again, the correct cluster locations are found.

22.4 A fatal (cid:13)aw of maximum likelihood

Finally, (cid:12)gure 22.7 sounds a cautionary note: when we (cid:12)t K = 4 means to our
(cid:12)rst toy data set, we sometimes (cid:12)nd that very small clusters form, covering
just one or two data points. This is a pathological property of soft K-means
clustering, versions 2 and 3.

. Exercise 22.6.[2 ] Investigate what happens if one mean m(k) sits exactly on
k is su(cid:14)ciently small,

top of one data point; show that if the variance (cid:27)2
then no return is possible: (cid:27)2
k becomes ever smaller.

t = 0

t = 5

t = 10

t = 20

A proof that the algorithm does
indeed maximize the likelihood is
deferred to section 33.7.

Figure 22.7. Soft K-means
algorithm applied to a data set of
40 points. K = 4. Notice that at
convergence, one very small
cluster has formed between two
data points.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

306

KABOOM!

22 | Maximum Likelihood and Clustering

Soft K-means can blow up. Put one cluster exactly on one data point and let its
variance go to zero { you can obtain an arbitrarily large likelihood! Maximum
likelihood methods can break down by (cid:12)nding highly tuned models that (cid:12)t part
of the data perfectly. This phenomenon is known as over(cid:12)tting. The reason
we are not interested in these solutions with enormous likelihood is this: sure,
these parameter-settings may have enormous posterior probability density,
but the density is large over only a very small volume of parameter space. So
the probability mass associated with these likelihood spikes is usually tiny.

We conclude that maximum likelihood methods are not a satisfactory gen-
eral solution to data-modelling problems: the likelihood may be in(cid:12)nitely large
at certain parameter settings. Even if the likelihood does not have in(cid:12)nitely-
large spikes, the maximum of the likelihood is often unrepresentative, in high-
dimensional problems.

Even in low-dimensional problems, maximum likelihood solutions can be
unrepresentative. As you may know from basic statistics, the maximum like-
lihood estimator (22.15) for a Gaussian’s standard deviation, (cid:27) N, is a biased
estimator, a topic that we’ll take up in Chapter 24.

The maximum a posteriori (MAP) method

A popular replacement for maximizing the likelihood is maximizing the
Bayesian posterior probability density of the parameters instead. However,
multiplying the likelihood by a prior and maximizing the posterior does
not make the above problems go away; the posterior density often also has
in(cid:12)nitely-large spikes, and the maximum of the posterior probability density
is often unrepresentative of the whole posterior distribution. Think back to
the concept of typicality, which we encountered in Chapter 4: in high dimen-
sions, most of the probability mass is in a typical set whose properties are
quite di(cid:11)erent from the points that have the maximum probability density.
Maxima are atypical.

A further reason for disliking the maximum a posteriori is that it is basis-
dependent. If we make a nonlinear change of basis from the parameter (cid:18) to
the parameter u = f ((cid:18)) then the probability density of (cid:18) is transformed to

P (u) = P ((cid:18))(cid:12)(cid:12)(cid:12)(cid:12)

@(cid:18)

@u(cid:12)(cid:12)(cid:12)(cid:12)

:

(22.29)

The maximum of the density P (u) will usually not coincide with the maximum
of the density P ((cid:18)). (For (cid:12)gures illustrating such nonlinear changes of basis,
see the next chapter.) It seems undesirable to use a method whose answers
change when we change representation.

Further reading

The soft K-means algorithm is at the heart of the automatic classi(cid:12)cation
package, AutoClass (Hanson et al., 1991b; Hanson et al., 1991a).

22.5 Further exercises

Exercises where maximum likelihood may be useful

Exercise 22.7.[3 ] Make a version of the K-means algorithm that models the
data as a mixture of K arbitrary Gaussians, i.e., Gaussians that are not
constrained to be axis-aligned.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

22.5: Further exercises

307

. Exercise 22.8.[2 ]

(a) A photon counter is pointed at a remote star for one
minute, in order to infer the brightness, i.e., the rate of photons
arriving at the counter per minute, (cid:21). Assuming the number of
photons collected r has a Poisson distribution with mean (cid:21),

P (r j (cid:21)) = exp((cid:0)(cid:21))

(cid:21)r
r!

;

(22.30)

what is the maximum likelihood estimate for (cid:21), given r = 9? Find
error bars on ln (cid:21).

(b) Same situation, but now we assume that the counter detects not
only photons from the star but also ‘background’ photons. The
background rate of photons is known to be b = 13 photons per
minute. We assume the number of photons collected, r, has a Pois-
son distribution with mean (cid:21)+b. Now, given r = 9 detected photons,
what is the maximum likelihood estimate for (cid:21)? Comment on this
answer, discussing also the Bayesian posterior distribution, and the
‘unbiased estimator’ of sampling theory, ^(cid:21) (cid:17) r (cid:0) b.

Exercise 22.9.[2 ] A bent coin is tossed N times, giving Na heads and Nb tails.
Assume a beta distribution prior for the probability of heads, p, for
example the uniform distribution. Find the maximum likelihood and
maximum a posteriori values of p, then (cid:12)nd the maximum likelihood
and maximum a posteriori values of the logit a (cid:17) ln[p=(1(cid:0)p)]. Compare
with the predictive distribution, i.e., the probability that the next toss
will come up heads.

. Exercise 22.10.[2 ] Two men looked through prison bars; one saw stars, the

other tried to infer where the window frame was.

From the other side of a room, you look through a window and see stars
at locations f(xn; yn)g. You can’t see the window edges because it is to-
tally dark apart from the stars. Assuming the window is rectangular and
that the visible stars’ locations are independently randomly distributed,
what are the inferred values of (xmin; ymin, xmax, ymax), according to
maximum likelihood? Sketch the likelihood as a function of xmax, for
(cid:12)xed xmin, ymin, and ymax.

. Exercise 22.11.[3 ] A sailor infers his location (x; y) by measuring the bearings
of three buoys whose locations (xn; yn) are given on his chart. Let the
true bearings of the buoys be (cid:18)n. Assuming that his measurement ~(cid:18)n of
each bearing is subject to Gaussian noise of small standard deviation (cid:27),
what is his inferred location, by maximum likelihood?

The sailor’s rule of thumb says that the boat’s position can be taken to
be the centre of the cocked hat, the triangle produced by the intersection
of the three measured bearings ((cid:12)gure 22.8). Can you persuade him that
the maximum likelihood answer is better?

. Exercise 22.12.[3, p.310] Maximum likelihood (cid:12)tting of an exponential-family

model.

Assume that a variable x comes from a probability distribution of the
form

P (xj w) =

1

Z(w)

exp Xk

wkfk(x)! ;

(22.31)

(xmax; ymax)

?

?

?

?
?

?

(xmin; ymin)

(x3; y3)

b

Q

Q

(cid:0)

(cid:0)

(cid:0)

b

(x1; y1)

(cid:0)(cid:0)
A
Q
A
(cid:0)
Q
A
QQ
(cid:0)
A
A

(cid:0)

A
A

A
A
(x2; y2)

b

Figure 22.8. The standard way of
drawing three slightly inconsistent
bearings on a chart produces a
triangle called a cocked hat.
Where is the sailor?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

308

22 | Maximum Likelihood and Clustering

where the functions fk(x) are given, and the parameters w = fwkg are
not known. A data set fx(n)g of N points is supplied.
Show by di(cid:11)erentiating the log likelihood that the maximum-likelihood
parameters wML satisfy

P (xj wML)fk(x) =

Xx

1

N Xn

fk(x(n));

(22.32)

where the left-hand sum is over all x, and the right-hand sum is over the
data points. A shorthand for this result is that each function-average
under the (cid:12)tted model must equal the function-average found in the
data:

hfkiP (x j wML) = hfkiData :

(22.33)

. Exercise 22.13.[3 ] ‘Maximum entropy’ (cid:12)tting of models to constraints.

When confronted by a probability distribution P (x) about which only a
few facts are known, the maximum entropy principle (maxent) o(cid:11)ers a
rule for choosing a distribution that satis(cid:12)es those constraints. Accord-
ing to maxent, you should select the P (x) that maximizes the entropy

H =Xx

P (x) log 1=P (x);

(22.34)

subject to the constraints. Assuming the constraints assert that the
averages of certain functions fk(x) are known, i.e.,

hfkiP (x) = Fk;

(22.35)

show, by introducing Lagrange multipliers (one for each constraint, in-
cluding normalization), that the maximum-entropy distribution has the
form

exp Xk

wkfk(x)! ;

P (x)Maxent =

1
Z

(22.36)

where the parameters Z and fwkg are set such that the constraints
(22.35) are satis(cid:12)ed.

And hence the maximum entropy method gives identical results to max-
imum likelihood (cid:12)tting of an exponential-family model (previous exer-
cise).

The maximum entropy method has sometimes been recommended as a
method for assigning prior distributions in Bayesian modelling. While
the outcomes of the maximum entropy method are sometimes interesting
and thought-provoking, I do not advocate maxent as the approach to
assigning priors.

Maximum entropy is also sometimes proposed as a method for solv-
ing inference problems { for example,
‘given that the mean score of
this unfair six-sided die is 2.5, what is its probability distribution
(p1; p2; p3; p4; p5; p6)?’ I think it is a bad idea to use maximum entropy
in this way; it can give silly answers. The correct way to solve inference
problems is to use Bayes’ theorem.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

22.5: Further exercises

309

Exercises where maximum likelihood and MAP have di(cid:14)culties

A

B C D-G

-30

-20

-10

0

10

20

Scientist

xn

A
B
C
D
E
F
G

(cid:0)27.020
3.570
8.191
9.898
9.603
9.945
10.056

Figure 22.9. Seven measurements
fxng of a parameter (cid:22) by seven
scientists each having his own
noise-level (cid:27)n.

. Exercise 22.14.[2 ] This exercise explores the idea that maximizing a proba-
bility density is a poor way to (cid:12)nd a point that is representative of
the density. Consider a Gaussian distribution in a k-dimensional space,
P (w) = (1=p2(cid:25) (cid:27)W )k exp((cid:0)Pk
W ). Show that nearly all of the
probability mass of a Gaussian is in a thin shell of radius r = pk(cid:27)W
and of thickness proportional to r=pk. For example, in 1000 dimen-
sions, 90% of the mass of a Gaussian with (cid:27)W = 1 is in a shell of radius
31.6 and thickness 2.8. However, the probability density at the origin is
ek=2 ’ 10217 times bigger than the density at this shell where most of
the probability mass is.

i =2(cid:27)2

1 w2

Now consider two Gaussian densities in 1000 dimensions that di(cid:11)er in
radius (cid:27)W by just 1%, and that contain equal total probability mass.
Show that the maximum probability density is greater at the centre of
the Gaussian with smaller (cid:27)W by a factor of (cid:24) exp(0:01k) ’ 20 000.
In ill-posed problems, a typical posterior distribution is often a weighted
superposition of Gaussians with varying means and standard deviations,
so the true posterior has a skew peak, with the maximum of the prob-
ability density located near the mean of the Gaussian distribution that
has the smallest standard deviation, not the Gaussian with the greatest
weight.

. Exercise 22.15.[3 ] The seven scientists. N datapoints fxng are drawn from
N distributions, all of which are Gaussian with a common mean (cid:22) but
with di(cid:11)erent unknown standard deviations (cid:27)n. What are the maximum
likelihood parameters (cid:22);f(cid:27)ng given the data?
For example, seven
scientists (A, B, C, D, E, F, G) with wildly-di(cid:11)ering experimental skills
measure (cid:22). You expect some of them to do accurate work (i.e., to have
small (cid:27)n), and some of them to turn in wildly inaccurate answers (i.e.,
to have enormous (cid:27)n). Figure 22.9 shows their seven results. What is
(cid:22), and how reliable is each scientist?

I hope you agree that, intuitively, it looks pretty certain that A and B
are both inept measurers, that D{G are better, and that the true value
of (cid:22) is somewhere close to 10. But what does maximizing the likelihood
tell you?

Exercise 22.16.[3 ] Problems with MAP method. A collection of widgets i =
1; : : : ; k have a property called ‘wodge’, wi, which we measure, wid-
get by widget, in noisy experiments with a known noise level (cid:27)(cid:23) = 1:0.
Our model for these quantities is that they come from a Gaussian prior
P (wi j (cid:11)) = Normal(0; 1/(cid:11)), where (cid:11) = 1=(cid:27)2
W is not known. Our prior for
this variance is (cid:13)at over log (cid:27)W from (cid:27)W = 0:1 to (cid:27)W = 10.

Scenario 1. Suppose four widgets have been measured and give the fol-
lowing data: fd1; d2; d3; d4g = f2.2, (cid:0)2:2, 2.8, (cid:0)2:8g. We are interested
in inferring the wodges of these four widgets.

(a) Find the values of w and (cid:11) that maximize the posterior probability

P (w; log (cid:11)j d).

(b) Marginalize over (cid:11) and (cid:12)nd the posterior probability density of w
given the data. [Integration skills required. See MacKay (1999a) for
solution.] Find maxima of P (w j d). [Answer: two maxima { one at
wMP = f1:8;(cid:0)1:8; 2:2;(cid:0)2:2g; with error bars on all four parameters

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

310

22 | Maximum Likelihood and Clustering

(obtained from Gaussian approximation to the posterior) (cid:6)0:9; and
one at w0MP = f0:03;(cid:0)0:03; 0:04;(cid:0)0:04g with error bars (cid:6)0:1.]

Scenario 2. Suppose in addition to the four measurements above we are
now informed that there are four more widgets that have been measured
with a much less accurate instrument, having (cid:27)0(cid:23) = 100:0. Thus we now
have both well-determined and ill-determined parameters, as in a typical
ill-posed problem. The data from these measurements were a string of
uninformative values, fd5; d6; d7; d8g = f100, (cid:0)100; 100, (cid:0)100g.
We are again asked to infer the wodges of the widgets. Intuitively, our
inferences about the well-measured widgets should be negligibly a(cid:11)ected
by this vacuous information about the poorly-measured widgets. But
what happens to the MAP method?

(a) Find the values of w and (cid:11) that maximize the posterior probability

P (w; log (cid:11)j d).

(b) Find maxima of P (w j d).

[Answer: only one maximum, wMP =
f0:03, (cid:0)0:03, 0:03, (cid:0)0:03, 0:0001, (cid:0)0:0001, 0:0001, (cid:0)0:0001g, with
error bars on all eight parameters (cid:6)0:11.]

22.6 Solutions

Solution to exercise 22.5 (p.302).
Figure 22.10 shows a contour plot of the
likelihood function for the 32 data points. The peaks are pretty-near centred
on the points (1; 5) and (5; 1), and are pretty-near circular in their contours.
The width of each of the peaks is a standard deviation of (cid:27)=p16 = 1/4. The
peaks are roughly Gaussian in shape.

Solution to exercise 22.12 (p.307). The log likelihood is:

ln P (fx(n)gj w) = (cid:0)N ln Z(w) +Xn Xk

wkfk(x(n)):

(22.37)

5

4

3

2

1

0

0

1

2

3

4

5

Figure 22.10. The likelihood as a
function of (cid:22)1 and (cid:22)2.

@

@wk

ln P (fx(n)gj w) = (cid:0)N

@

@wk

ln Z(w) +Xn

fk(x):

(22.38)

Now, the fun part is what happens when we di(cid:11)erentiate the log of the nor-
malizing constant:

wk0fk0(x)!

1

@

@wk

exp Xk0
Z(w)Xx
wk0fk0(x)! fk(x) = Xx

P (xj w)fk(x);

(22.39)

@

@wk

ln Z(w) =

=

so

1

exp Xk0

Z(w)Xx
ln P (fx(n)gj w) = (cid:0)NXx

@

@wk

P (xj w)fk(x) +Xn

fk(x);

(22.40)

and at the maximum of the likelihood,

Xx

P (xj wML)fk(x) =

1

N Xn

fk(x(n)):

(22.41)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

23

Useful Probability Distributions

In Bayesian data modelling, there’s a small collection of probability distribu-
tions that come up again and again. The purpose of this chapter is to intro-
duce these distributions so that they won’t be intimidating when encountered
in combat situations.

There is no need to memorize any of them, except perhaps the Gaussian;
if a distribution is important enough, it will memorize itself, and otherwise, it
can easily be looked up.

23.1 Distributions over integers

Binomial, Poisson, exponential

We already encountered the binomial distribution and the Poisson distribution
on page 2.

The binomial distribution for an integer r with parameters f (the bias,

f 2 [0; 1]) and N (the number of trials) is:
r(cid:19)f r(1 (cid:0) f )N(cid:0)r

P (r j f; N ) =(cid:18)N

r 2 f0; 1; 2; : : : ; Ng:

(23.1)

The binomial distribution arises, for example, when we (cid:13)ip a bent coin,

with bias f , N times, and observe the number of heads, r.

The Poisson distribution with parameter (cid:21) > 0 is:

P (r j (cid:21)) = e(cid:0)(cid:21) (cid:21)r

r!

r 2 f0; 1; 2; : : :g:

(23.2)

The Poisson distribution arises, for example, when we count the number of
photons r that arrive in a pixel during a (cid:12)xed interval, given that the mean
intensity on the pixel corresponds to an average number of photons (cid:21).

The exponential distribution on integers,,

P (r j f ) = f r(1 (cid:0) f )

r 2 (0; 1; 2; : : : ;1);

(23.3)

arises in waiting problems. How long will you have to wait until a six is rolled,
if a fair six-sided dice is rolled? Answer: the probability distribution of the
number of rolls, r, is exponential over integers with parameter f = 5=6. The
distribution may also be written

P (r j f ) = (1 (cid:0) f ) e(cid:0)(cid:21)r

r 2 (0; 1; 2; : : : ;1);

(23.4)

where (cid:21) = ln(1=f ).

311

0 1 2 3 4 5 6 7 8 9 10

0.3
0.25
0.2
0.15
0.1
0.05
0

1
0.1
0.01
0.001
0.0001
1e-05
1e-06

0 1 2 3 4 5 6 7 8 9 10

r

Figure 23.1. The binomial
distribution P (r j f = 0:3; N = 10),
on a linear scale (top) and a
logarithmic scale (bottom).

0

5

10

15

0.25

0.2

0.15

0.1

0.05

0

1
0.1
0.01
0.001
0.0001
1e-05
1e-06
1e-07

0

5
r

10

15

Figure 23.2. The Poisson
distribution P (r j (cid:21) = 2:7), on a
linear scale (top) and a
logarithmic scale (bottom).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

312

23 | Useful Probability Distributions

23.2 Distributions over unbounded real numbers

Gaussian, Student, Cauchy, biexponential, inverse-cosh.

The Gaussian distribution or normal distribution with mean (cid:22) and standard
deviation (cid:27) is

P (xj (cid:22); (cid:27)) =

1
Z

where

2(cid:27)2 (cid:19) x 2 ((cid:0)1;1);
(x (cid:0) (cid:22))2

exp(cid:18)(cid:0)
Z = p2(cid:25)(cid:27)2:

(23.6)
It is sometimes useful to work with the quantity (cid:28) (cid:17) 1=(cid:27) 2, which is called the
precision parameter of the Gaussian.
A sample z from a standard univariate Gaussian can be generated by

computing

(23.5)

(23.7)

z = cos(2(cid:25)u1)p2 ln(1=u2);

where u1 and u2 are uniformly distributed in (0; 1). A second sample z2 =

sin(2(cid:25)u1)p2 ln(1=u2), independent of the (cid:12)rst, can then be obtained for free.

The Gaussian distribution is widely used and often asserted to be a very
common distribution in the real world, but I am sceptical about this asser-
tion. Yes, unimodal distributions may be common; but a Gaussian is a spe-
cial, rather extreme, unimodal distribution. It has very light tails: the log-
probability-density decreases quadratically. The typical deviation of x from (cid:22)
is (cid:27), but the respective probabilities that x deviates from (cid:22) by more than 2(cid:27),
3(cid:27), 4(cid:27), and 5(cid:27), are 0:046, 0.003, 6 (cid:2) 10(cid:0)5, and 6 (cid:2) 10(cid:0)7. In my experience,
deviations from a mean four or (cid:12)ve times greater than the typical deviation
may be rare, but not as rare as 6(cid:2) 10(cid:0)5! I therefore urge caution in the use of
Gaussian distributions: if a variable that is modelled with a Gaussian actually
has a heavier-tailed distribution, the rest of the model will contort itself to
reduce the deviations of the outliers, like a sheet of paper being crushed by a
rubber band.

. Exercise 23.1.[1 ] Pick a variable that is supposedly bell-shaped in probability
distribution, gather data, and make a plot of the variable’s empirical
distribution. Show the distribution as a histogram on a log scale and
investigate whether the tails are well-modelled by a Gaussian distribu-
tion.
[One example of a variable to study is the amplitude of an audio
signal.]

One distribution with heavier tails than a Gaussian is a mixture of Gaus-
sians. A mixture of two Gaussians, for example, is de(cid:12)ned by two means,
two standard deviations, and two mixing coe(cid:14)cients (cid:25)1 and (cid:25)2, satisfying
(cid:25)1 + (cid:25)2 = 1, (cid:25)i (cid:21) 0.
P (xj (cid:22)1; (cid:27)1; (cid:25)1; (cid:22)2; (cid:27)2; (cid:25)2) =

(cid:25)1p2(cid:25)(cid:27)1

(cid:25)2p2(cid:25)(cid:27)2

2 (cid:17) :
exp(cid:16)(cid:0) (x(cid:0)(cid:22)2)2

2(cid:27)2

1 (cid:17)+
exp(cid:16)(cid:0) (x(cid:0)(cid:22)1)2

2(cid:27)2

If we take an appropriately weighted mixture of an in(cid:12)nite number of

Gaussians, all having mean (cid:22), we obtain a Student-t distribution,

where

P (xj (cid:22); s; n) =

1
Z

1

(1 + (x (cid:0) (cid:22))2=(ns2))(n+1)=2

;

(23.8)

Z = p(cid:25)ns2

(cid:0)(n=2)

(cid:0)((n + 1)=2)

(23.9)

-2

0

2

4

6

8

0.5

0.4

0.3

0.2

0.1

0

0.1

0.01

0.001

0.0001

-2

0

2

4

6

8

Figure 23.3. Three unimodal
distributions. Two Student
distributions, with parameters
(m; s) = (1; 1) (heavy line) (a
Cauchy distribution) and (2; 4)
(light line), and a Gaussian
distribution with mean (cid:22) = 3 and
standard deviation (cid:27) = 3 (dashed
line), shown on linear vertical
scales (top) and logarithmic
vertical scales (bottom). Notice
that the heavy tails of the Cauchy
distribution are scarcely evident
in the upper ‘bell-shaped curve’.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

23.3: Distributions over positive real numbers

313

and n is called the number of degrees of freedom and (cid:0) is the gamma function.
If n > 1 then the Student distribution (23.8) has a mean and that mean is
If n > 2 the distribution also has a (cid:12)nite variance, (cid:27) 2 = ns2=(n (cid:0) 2).
(cid:22).
As n ! 1, the Student distribution approaches the normal distribution with
mean (cid:22) and standard deviation s. The Student distribution arises both in
classical statistics (as the sampling-theoretic distribution of certain statistics)
and in Bayesian inference (as the probability distribution of a variable coming
from a Gaussian distribution whose standard deviation we aren’t sure of).

In the special case n = 1, the Student distribution is called the Cauchy

distribution.

A distribution whose tails are intermediate in heaviness between Student

and Gaussian is the biexponential distribution,

P (xj (cid:22); s) =

1
Z

exp(cid:18)(cid:0)jx (cid:0) (cid:22)j

s (cid:19) x 2 ((cid:0)1;1)

where

Z = 2s:

The inverse-cosh distribution

P (xj (cid:12)) /

1

[cosh((cid:12)x)]1=(cid:12)

(23.10)

(23.11)

(23.12)

is a popular model in independent component analysis. In the limit of large (cid:12),
the probability distribution P (xj (cid:12)) becomes a biexponential distribution. In
the limit (cid:12) ! 0 P (xj (cid:12)) approaches a Gaussian with mean zero and variance
1=(cid:12).

23.3 Distributions over positive real numbers

Exponential, gamma, inverse-gamma, and log-normal.

The exponential distribution,

where

P (xj s) =

1
Z

exp(cid:16)(cid:0)

x

s(cid:17)

x 2 (0;1);

Z = s;

(23.13)

(23.14)

arises in waiting problems. How long will you have to wait for a bus in Pois-
sonville, given that buses arrive independently at random with one every s
minutes on average? Answer: the probability distribution of your wait, x, is
exponential with mean s.

The gamma distribution is like a Gaussian distribution, except whereas the
Gaussian goes from (cid:0)1 to 1, gamma distributions go from 0 to 1. Just as
the Gaussian distribution has two parameters (cid:22) and (cid:27) which control the mean
and width of the distribution, the gamma distribution has two parameters. It
is the product of the one-parameter exponential distribution (23.13) with a
polynomial, xc(cid:0)1. The exponent c in the polynomial is the second parameter.

P (xj s; c) = (cid:0)(x; s; c) =

where

1

Z (cid:16) x

s(cid:17)c(cid:0)1

exp(cid:16)(cid:0)

x

s(cid:17) ; 0 (cid:20) x < 1 (23.15)

Z = (cid:0)(c)s:

(23.16)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

314

23 | Useful Probability Distributions

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

0.1

0.01

0.001

0.0001

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

2

4

6

8

10

-4

-2

0

2

4

0.1

0.01

0.001

0.0001

0

2

4

6

8

10

x

-4

-2

0

2

4

l = ln x

Figure 23.4. Two gamma
distributions, with parameters
(s; c) = (1; 3) (heavy lines) and
10; 0:3 (light lines), shown on
linear vertical scales (top) and
logarithmic vertical scales
(bottom); and shown as a
function of x on the left (23.15)
and l = ln x on the right (23.18).

This is a simple peaked distribution with mean sc and variance s2c.

It is often natural to represent a positive real variable x in terms of its

logarithm l = ln x. The probability density of l is

@x

@l(cid:12)(cid:12)(cid:12)(cid:12)
P (l) = P (x(l)) (cid:12)(cid:12)(cid:12)(cid:12)
Zl (cid:18) x(l)
exp(cid:18)(cid:0)
s (cid:19)c

x(l)

s (cid:19) ;

= P (x(l))x(l)

1

=

where

Zl = (cid:0)(c):

(23.17)

(23.18)

(23.19)

[The gamma distribution is named after its normalizing constant { an odd
convention, it seems to me!]

Figure 23.4 shows a couple of gamma distributions as a function of x and
of l. Notice that where the original gamma distribution (23.15) may have a
‘spike’ at x = 0, the distribution over l never has such a spike. The spike is
an artefact of a bad choice of basis.

In the limit sc = 1; c ! 0, we obtain the noninformative prior for a scale
parameter, the 1=x prior. This improper prior is called noninformative because
it has no associated length scale, no characteristic value of x, so it prefers all
values of x equally. It is invariant under the reparameterization x = mx. If
we transform the 1=x probability density into a density over l = ln x we (cid:12)nd
the latter density is uniform.

. Exercise 23.2.[1 ] Imagine that we reparameterize a positive variable x in terms
of its cube root, u = x1=3. If the probability density of x is the improper
distribution 1=x, what is the probability density of u?

The gamma distribution is always a unimodal density over l = ln x, and,
as can be seen in the (cid:12)gures, it is asymmetric. If x has a gamma distribution,
and we decide to work in terms of the inverse of x, v = 1=x, we obtain a new
distribution, in which the density over l is (cid:13)ipped left-for-right: the probability
density of v is called an inverse-gamma distribution,

P (v j s; c) =

where

1

Zv (cid:18) 1

sv(cid:19)c+1

exp(cid:18)(cid:0)

1

sv(cid:19) ;

0 (cid:20) v < 1

(23.20)

Zv = (cid:0)(c)=s:

(23.21)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

23.4: Distributions over periodic variables

315

2.5

2

1.5

1

0.5

0

1

0.1

0.01

0.001

0.0001

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

1

2

3

-4

-2

0

2

4

0.1

0.01

0.001

0.0001

0

1

2

3

v

-4

-2

0

2

4

ln v

Figure 23.5. Two inverse gamma
distributions, with parameters
(s; c) = (1; 3) (heavy lines) and
10; 0:3 (light lines), shown on
linear vertical scales (top) and
logarithmic vertical scales
(bottom); and shown as a
function of x on the left and
l = ln x on the right.

Gamma and inverse gamma distributions crop up in many inference prob-
lems in which a positive quantity is inferred from data. Examples include
inferring the variance of Gaussian noise from some noise samples, and infer-
ring the rate parameter of a Poisson distribution from the count.

Gamma distributions also arise naturally in the distributions of waiting
times between Poisson-distributed events. Given a Poisson process with rate
(cid:21), the probability density of the arrival time x of the mth event is

(cid:21)((cid:21)x)m(cid:0)1
(m(cid:0)1)!

e(cid:0)(cid:21)x:

(23.22)

Log-normal distribution

Another distribution over a positive real number x is the log-normal distribu-
tion, which is the distribution that results when l = ln x has a normal distri-
bution. We de(cid:12)ne m to be the median value of x, and s to be the standard
deviation of ln x.

0

1

2

3

4

5

0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

0.1

0.01

0.001

0.0001

P (l j m; s) =

1
Z

exp(cid:18)(cid:0)

(l (cid:0) ln m)2

2s2

(cid:19) l 2 ((cid:0)1;1);

Z = p2(cid:25)s2;

where

implies

P (xj m; s) =

1
x

1
Z

exp(cid:18)(cid:0)

(ln x (cid:0) ln m)2

2s2

(cid:19) x 2 (0;1):

(23.23)

0

1

2

3

4

5

(23.24)

(23.25)

Figure 23.6. Two log-normal
distributions, with parameters
(m; s) = (3; 1:8) (heavy line) and
(3; 0:7) (light line), shown on
linear vertical scales (top) and
logarithmic vertical scales
(bottom). [Yes, they really do
have the same value of the
median, m = 3.]

23.4 Distributions over periodic variables

A periodic variable (cid:18) is a real number 2 [0; 2(cid:25)] having the property that (cid:18) = 0
and (cid:18) = 2(cid:25) are equivalent.
A distribution that plays for periodic variables the role played by the Gaus-

sian distribution for real variables is the Von Mises distribution:

P ((cid:18) j (cid:22); (cid:12)) =

1
Z

exp ((cid:12) cos((cid:18) (cid:0) (cid:22)))

(cid:18) 2 (0; 2(cid:25)):

(23.26)

The normalizing constant is Z = 2(cid:25)I0((cid:12)), where I0(x) is a modi(cid:12)ed Bessel
function.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

316

23 | Useful Probability Distributions

A distribution that arises from Brownian di(cid:11)usion around the circle is the

wrapped Gaussian distribution,

P ((cid:18) j (cid:22); (cid:27)) =

1Xn=(cid:0)1

Normal((cid:18); ((cid:22) + 2(cid:25)n); (cid:27)2) (cid:18) 2 (0; 2(cid:25)):

(23.27)

23.5 Distributions over probabilities

Beta distribution, Dirichlet distribution, entropic distribution

The beta distribution is a probability density over a variable p that is a prob-
ability, p 2 (0; 1):

P (pj u1; u2) =

1

Z(u1; u2)

pu1(cid:0)1(1 (cid:0) p)u2(cid:0)1:

(23.28)

The parameters u1; u2 may take any positive value. The normalizing constant
is the beta function,

Z(u1; u2) =

(cid:0)(u1)(cid:0)(u2)
(cid:0)(u1 + u2)

:

(23.29)

Special cases include the uniform distribution { u1 = 1; u2 = 1; the Je(cid:11)reys
prior { u1 = 0:5; u2 = 0:5; and the improper Laplace prior { u1 = 0; u2 = 0. If
we transform the beta distribution to the corresponding density over the logit
l (cid:17) ln p/ (1 (cid:0) p), we (cid:12)nd it is always a pleasant bell-shaped density over l, while
the density over p may have singularities at p = 0 and p = 1 ((cid:12)gure 23.7).

More dimensions

5

4

3

2

1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.25

0.5

0.75

1

-6

-4

-2

0

2

4

6

Figure 23.7. Three beta
distributions, with
(u1; u2) = (0:3; 1), (1:3; 1), and
(12; 2). The upper (cid:12)gure shows
P (pj u1; u2) as a function of p; the
lower shows the corresponding
density over the logit,

ln

p
1 (cid:0) p

:

Notice how well-behaved the
densities are as a function of the
logit.

The Dirichlet distribution is a density over an I-dimensional vector p whose
I components are positive and sum to 1. The beta distribution is a special
case of a Dirichlet distribution with I = 2. The Dirichlet distribution is
parameterized by a measure u (a vector with all coe(cid:14)cients ui > 0) which
I will write here as u = (cid:11)m, where m is a normalized measure over the I

components (P mi = 1), and (cid:11) is positive:
P (pj (cid:11)m) =

p(cid:11)mi(cid:0)1
i

Z((cid:11)m)

1

(cid:14) (Pi pi (cid:0) 1) (cid:17) Dirichlet(I)(pj (cid:11)m): (23.30)
to the simplex such that p is normalized, i.e., Pi pi = 1. The normalizing

The function (cid:14)(x) is the Dirac delta function, which restricts the distribution

constant of the Dirichlet distribution is:

I

Yi=1

(cid:0)((cid:11)mi) /(cid:0)((cid:11)) :

(23.31)

Z((cid:11)m) =Yi

The vector m is the mean of the probability distribution:

Z Dirichlet(I)(pj (cid:11)m) p dIp = m:

(23.32)

When working with a probability vector p, it is often helpful to work in the
‘softmax basis’, in which, for example, a three-dimensional probability p =
(p1; p2; p3) is represented by three numbers a1; a2; a3 satisfying a1 +a2 +a3 = 0
and

pi =

1
Z

eai; where Z =Pi eai.

(23.33)

This nonlinear transformation is analogous to the (cid:27) ! ln (cid:27) transformation
for a scale variable and the logit transformation for a single probability, p !

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

23.5: Distributions over probabilities

317

u = (20; 10; 7)

u = (0:2; 1; 2)

u = (0:2; 0:3; 0:15)

8

4

0

-4

8

4

0

-4

8

4

0

-4

-8

-8

-4

0

4

-8

-8

8

-4

0

4

-8

-8

8

-4

0

4

8

ln p
1(cid:0)p .
Dirichlet distribution (23.30) disappear, and the density is given by:

In the softmax basis, the ugly minus-ones in the exponents in the

P (aj (cid:11)m) /

1

Z((cid:11)m)

p(cid:11)mi
i

I

Yi=1

(cid:14) (Pi ai) :

(23.34)

The role of the parameter (cid:11) can be characterized in two ways. First, (cid:11) mea-
sures the sharpness of the distribution ((cid:12)gure 23.8); it measures how di(cid:11)erent
we expect typical samples p from the distribution to be from the mean m, just
as the precision (cid:28) = 1/(cid:27)2 of a Gaussian measures how far samples stray from its
mean. A large value of (cid:11) produces a distribution over p that is sharply peaked
around m. The e(cid:11)ect of (cid:11) in higher-dimensional situations can be visualized
by drawing a typical sample from the distribution Dirichlet(I)(pj (cid:11)m), with
m set to the uniform vector mi = 1/I, and making a Zipf plot, that is, a ranked
plot of the values of the components pi. It is traditional to plot both pi (ver-
tical axis) and the rank (horizontal axis) on logarithmic scales so that power
law relationships appear as straight lines. Figure 23.9 shows these plots for a
single sample from ensembles with I = 100 and I = 1000 and with (cid:11) from 0.1
to 1000. For large (cid:11), the plot is shallow with many components having simi-
lar values. For small (cid:11), typically one component pi receives an overwhelming
share of the probability, and of the small probability that remains to be shared
among the other components, another component pi0 receives a similarly large
share. In the limit as (cid:11) goes to zero, the plot tends to an increasingly steep
power law.

Second, we can characterize the role of (cid:11) in terms of the predictive dis-
tribution that results when we observe samples from p and obtain counts
F = (F1; F2; : : : ; FI ) of the possible outcomes. The value of (cid:11) de(cid:12)nes the
number of samples from p that are required in order that the data dominate
over the prior in predictions.

Exercise 23.3.[3 ] The Dirichlet distribution satis(cid:12)es a nice additivity property.
Imagine that a biased six-sided die has two red faces and four blue faces.
The die is rolled N times and two Bayesians examine the outcomes in
order to infer the bias of the die and make predictions. One Bayesian
has access to the red/blue colour outcomes only, and he infers a two-
component probability vector (pR; pB). The other Bayesian has access
to each full outcome: he can see which of the six faces came up, and
he infers a six-component probability vector (p1; p2; p3; p4; p5; p6), where

Figure 23.8. Three Dirichlet
distributions over a
three-dimensional probability
vector (p1; p2; p3). The upper
(cid:12)gures show 1000 random draws
from each distribution, showing
the values of p1 and p2 on the two
axes. p3 = 1 (cid:0) (p1 + p2). The
triangle in the (cid:12)rst (cid:12)gure is the
simplex of legal probability
distributions.
The lower (cid:12)gures show the same
points in the ‘softmax’ basis
(equation (23.33)). The two axes
show a1 and a2. a3 = (cid:0)a1 (cid:0) a2.

1

0.1

0.01

0.001

0.0001

1

1

0.1

0.01

0.001

0.0001

1e-05

1

I = 100

0.1
1
10
100
1000

10

100

I = 1000

0.1
1
10
100
1000

10

100

1000

Figure 23.9. Zipf plots for random
samples from Dirichlet
distributions with various values
of (cid:11) = 0:1 : : : 1000. For each value
of I = 100 or 1000 and each (cid:11),
one sample p from the Dirichlet
distribution was generated. The
Zipf plot shows the probabilities
pi, ranked by magnitude, versus
their rank.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

318

23 | Useful Probability Distributions

pR = p1 + p2 and pB = p3 + p4 + p5 + p6. Assuming that the sec-
ond Bayesian assigns a Dirichlet distribution to (p1; p2; p3; p4; p5; p6) with
hyperparameters (u1; u2; u3; u4; u5; u6), show that, in order for the (cid:12)rst
Bayesian’s inferences to be consistent with those of the second Bayesian,
the (cid:12)rst Bayesian’s prior should be a Dirichlet distribution with hyper-
parameters ((u1 + u2); (u3 + u4 + u5 + u6)).

Hint: a brute-force approach is to compute the integral P (pR; pB) =

R d6p P (pj u) (cid:14)(pR (cid:0) (p1 + p2)) (cid:14)(pB (cid:0) (p3 + p4 + p5 + p6)). A cheaper

approach is to compute the predictive distributions, given arbitrary data
(F1; F2; F3; F4; F5; F6), and (cid:12)nd the condition for the two predictive dis-
tributions to match for all data.

The entropic distribution for a probability vector p is sometimes used in

the ‘maximum entropy’ image reconstruction community.

P (pj (cid:11); m) =

1

Z((cid:11); m)

exp[(cid:0)(cid:11)DKL(pjjm)] (cid:14)(Pi pi (cid:0) 1) ;

(23.35)

where m, the measure, is a positive vector, and DKL(pjjm) =Pi pi log pi=mi.

Further reading

See (MacKay and Peto, 1995) for fun with Dirichlets.

23.6 Further exercises
Exercise 23.4.[2 ] N datapoints fxng are drawn from a gamma distribution
P (xj s; c) = (cid:0)(x; s; c) with unknown parameters s and c. What are the
maximum likelihood parameters s and c?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

24

Exact Marginalization

How can we avoid the exponentially large cost of complete enumeration of
all hypotheses? Before we stoop to approximate methods, we explore two
approaches to exact marginalization: (cid:12)rst, marginalization over continuous
variables (sometimes known as nuisance parameters) by doing integrals; and
second, summation over discrete variables by message-passing.

Exact marginalization over continuous parameters is a macho activity en-
joyed by those who are (cid:13)uent in de(cid:12)nite integration. This chapter uses gamma
distributions; as was explained in the previous chapter, gamma distributions
are a lot like Gaussian distributions, except that whereas the Gaussian goes
from (cid:0)1 to 1, gamma distributions go from 0 to 1.

24.1 Inferring the mean and variance of a Gaussian distribution

We discuss again the one-dimensional Gaussian distribution, parameterized
by a mean (cid:22) and a standard deviation (cid:27):

P (xj (cid:22); (cid:27)) =

1

p2(cid:25)(cid:27)

exp(cid:18)(cid:0)

2(cid:27)2 (cid:19) (cid:17) Normal(x; (cid:22); (cid:27)2):
(x (cid:0) (cid:22))2

(24.1)

When inferring these parameters, we must specify their prior distribution.
The prior gives us the opportunity to include speci(cid:12)c knowledge that we have
about (cid:22) and (cid:27) (from independent experiments, or on theoretical grounds, for
example). If we have no such knowledge, then we can construct an appropriate
prior that embodies our supposed ignorance. In section 21.2, we assumed a
uniform prior over the range of parameters plotted. If we wish to be able to
perform exact marginalizations, it may be useful to consider conjugate priors;
these are priors whose functional form combines naturally with the likelihood
such that the inferences have a convenient form.

Conjugate priors for (cid:22) and (cid:27)

The conjugate prior for a mean (cid:22) is a Gaussian: we introduce two ‘hy-
perparameters’, (cid:22)0 and (cid:27)(cid:22), which parameterize the prior on (cid:22), and write
P ((cid:22)j (cid:22)0; (cid:27)(cid:22)) = Normal((cid:22); (cid:22)0; (cid:27)2
In the limit (cid:22)0 = 0, (cid:27)(cid:22) ! 1, we obtain
(cid:22)).
the noninformative prior for a location parameter, the (cid:13)at prior. This is
noninformative because it is invariant under the natural reparameterization
(cid:22)0 = (cid:22) + c. The prior P ((cid:22)) = const: is also an improper prior, that is, it is not
normalizable.

The conjugate prior for a standard deviation (cid:27) is a gamma distribution,
which has two parameters b(cid:12) and c(cid:12). It is most convenient to de(cid:12)ne the prior

319

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

320

24 | Exact Marginalization

density of the inverse variance (the precision parameter) (cid:12) = 1=(cid:27) 2:

P ((cid:12)) = (cid:0)((cid:12); b(cid:12); c(cid:12)) =

1

(cid:0)(c(cid:12))

(cid:12)c(cid:12)(cid:0)1
b

c(cid:12)
(cid:12)

exp(cid:18)(cid:0)

(cid:12)

b(cid:12)(cid:19) ;

0 (cid:20) (cid:12) < 1:

(24.2)

This is a simple peaked distribution with mean b(cid:12)c(cid:12) and variance b2
(cid:12)c(cid:12). In
the limit b(cid:12)c(cid:12) = 1; c(cid:12) ! 0, we obtain the noninformative prior for a scale
parameter, the 1=(cid:27) prior. This is ‘noninformative’ because it is invariant
under the reparameterization (cid:27)0 = c(cid:27). The 1=(cid:27) prior is less strange-looking if
we examine the resulting density over ln (cid:27), or ln (cid:12), which is (cid:13)at. This is the
prior that expresses ignorance about (cid:27) by saying ‘well, it could be 10, or it
could be 1, or it could be 0.1, . . . ’ Scale variables such as (cid:27) are usually best
represented in terms of their logarithm. Again, this noninformative 1=(cid:27) prior
is improper.

In the following examples, I will use the improper noninformative priors
for (cid:22) and (cid:27). Using improper priors is viewed as distasteful in some circles,
so let me excuse myself by saying it’s for the sake of readability; if I included
proper priors, the calculations could still be done but the key points would be
obscured by the (cid:13)ood of extra parameters.

Maximum likelihood and marginalization: (cid:27)N and (cid:27)N(cid:0)1

The task of inferring the mean and standard deviation of a Gaussian distribu-
tion from N samples is a familiar one, though maybe not everyone understands
the di(cid:11)erence between the (cid:27)N and (cid:27)N(cid:0)1 buttons on their calculator. Let us
recap the formulae, then derive them.

Given data D = fxngN

n=1, an ‘estimator’ of (cid:22) is

Reminder: when we change
variables from (cid:27) to l((cid:27)), a
one-to-one function of (cid:27), the
probability density transforms
from P(cid:27)((cid:27)) to

Here, the Jacobian is

:

@(cid:27)

@l(cid:12)(cid:12)(cid:12)(cid:12)

Pl(l) = P(cid:27)((cid:27))(cid:12)(cid:12)(cid:12)(cid:12)
@ ln (cid:27)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

@(cid:27)

= (cid:27):

and two estimators of (cid:27) are:

(cid:27)N (cid:17)sPN

n=1(xn (cid:0) (cid:22)x)2

N

n=1 xn=N;

(cid:22)x (cid:17)PN
and (cid:27)N(cid:0)1 (cid:17)sPN

n=1(xn (cid:0) (cid:22)x)2

N (cid:0) 1

(24.3)

:

(24.4)

There are two principal paradigms for statistics: sampling theory and Bayesian
inference. In sampling theory (also known as ‘frequentist’ or orthodox statis-
tics), one invents estimators of quantities of interest and then chooses between
those estimators using some criterion measuring their sampling properties;
there is no clear principle for deciding which criterion to use to measure the
performance of an estimator; nor, for most criteria, is there any systematic
procedure for the construction of optimal estimators. In Bayesian inference,
in contrast, once we have made explicit all our assumptions about the model
and the data, our inferences are mechanical. Whatever question we wish to
pose, the rules of probability theory give a unique answer which consistently
takes into account all the given information. Human-designed estimators and
con(cid:12)dence intervals have no role in Bayesian inference; human input only en-
ters into the important tasks of designing the hypothesis space (that is, the
speci(cid:12)cation of the model and all its probability distributions), and (cid:12)guring
out how to do the computations that implement inference in that space. The
answers to our questions are probability distributions over the quantities of
interest. We often (cid:12)nd that the estimators of sampling theory emerge auto-
matically as modes or means of these posterior distributions when we choose
a simple hypothesis space and turn the handle of Bayesian inference.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

24.1: Inferring the mean and variance of a Gaussian distribution

321

Figure 24.1. The likelihood
function for the parameters of a
Gaussian distribution, repeated
from (cid:12)gure 21.5.
(a1, a2) Surface plot and contour
plot of the log likelihood as a
function of (cid:22) and (cid:27). The data set
of N = 5 points had mean (cid:22)x = 1:0

and S =P(x (cid:0) (cid:22)x)2 = 1:0. Notice

that the maximum is skew in (cid:27).
The two estimators of standard
deviation have values (cid:27)N = 0:45
and (cid:27)N(cid:0)1 = 0:50.
(c) The posterior probability of (cid:27)
for various (cid:12)xed values of (cid:22)
(shown as a density over ln (cid:27)).
(d) The posterior probability of (cid:27),
P ((cid:27) j D), assuming a (cid:13)at prior on
(cid:22), obtained by projecting the
probability mass in (a) onto the (cid:27)
axis. The maximum of P ((cid:27) j D) is
at (cid:27)N(cid:0)1. By contrast, the
maximum of P ((cid:27) j D; (cid:22) = (cid:22)x) is at
(cid:27)N . (Both probabilities are shows
as densities over ln (cid:27).)

0.06

0.05

0.04

0.03

0.02

0.01

1 0
0.8

0.6

sigma

(a1)

0.4

0.2

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0
0.2

(c)

1

0.9

0.8

0.7

0.6
0.5

0.4

0.3

0.2

0.1

sigma

0.5

1
mean

0

1.5

2

0
(a2)

0.5

1

1.5

2

mean

mu=1   
mu=1.25
mu=1.5 

P(sigma|D,mu=1)

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

(d)

P(sigma|D)

0.4

0.6

0.8

1

1.2 1.4 1.6 1.8 2

0
0.2

0.4

0.6

0.8

1

1.2 1.4 1.6 1.8 2

In sampling theory, the estimators above can be motivated as follows. (cid:22)x is
an unbiased estimator of (cid:22) which, out of all the possible unbiased estimators
of (cid:22), has smallest variance (where this variance is computed by averaging over
an ensemble of imaginary experiments in which the data samples are assumed
to come from an unknown Gaussian distribution). The estimator ((cid:22)x; (cid:27) N ) is the
maximum likelihood estimator for ((cid:22); (cid:27)). The estimator (cid:27)N is biased, however:
the expectation of (cid:27)N , given (cid:27), averaging over many imagined experiments, is
not (cid:27).
Exercise 24.1.[2, p.323] Give an intuitive explanation why the estimator (cid:27)N is

biased.

This bias motivates the invention, in sampling theory, of (cid:27)N(cid:0)1, which can be
shown to be an unbiased estimator. Or to be precise, it is (cid:27) 2
N(cid:0)1 that is an
unbiased estimator of (cid:27)2.

We now look at some Bayesian inferences for this problem, assuming non-
informative priors for (cid:22) and (cid:27). The emphasis is thus not on the priors, but
rather on (a) the likelihood function, and (b) the concept of marginalization.
The joint posterior probability of (cid:22) and (cid:27) is proportional to the likelihood
function illustrated by a contour plot in (cid:12)gure 24.1a. The log likelihood is:

ln P (fxngN

n=1 j (cid:22); (cid:27)) = (cid:0)N ln(p2(cid:25)(cid:27)) (cid:0)Xn

(xn (cid:0) (cid:22))2=(2(cid:27)2);
= (cid:0)N ln(p2(cid:25)(cid:27)) (cid:0) [N ((cid:22) (cid:0) (cid:22)x)2 + S]=(2(cid:27)2);

(24.5)

(24.6)

where S (cid:17) Pn(xn (cid:0) (cid:22)x)2. Given the Gaussian model, the likelihood can be

expressed in terms of the two functions of the data (cid:22)x and S, so these two
quantities are known as ‘su(cid:14)cient statistics’. The posterior probability of (cid:22)
and (cid:27) is, using the improper priors:

P ((cid:22); (cid:27) jfxngN

n=1) =

=

P (fxngN
n=1 j (cid:22); (cid:27))P ((cid:22); (cid:27))
P (fxngN
(2(cid:25)(cid:27)2)N=2 exp(cid:16)(cid:0) N ((cid:22)(cid:0)(cid:22)x)2+S

n=1)

1

2(cid:27)2
n=1)

P (fxngN

(24.7)

(24.8)

(cid:17) 1

(cid:27)(cid:22)

1
(cid:27)

:

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

322

24 | Exact Marginalization

This function describes the answer to the question, ‘given the data, and the
noninformative priors, what might (cid:22) and (cid:27) be?’ It may be of interest to (cid:12)nd
the parameter values that maximize the posterior probability, though it should
be emphasized that posterior probability maxima have no fundamental status
in Bayesian inference, since their location depends on the choice of basis. Here
we choose the basis ((cid:22); ln (cid:27)), in which our prior is (cid:13)at, so that the posterior
probability maximum coincides with the maximum of the likelihood. As we
saw in exercise 22.4 (p.302), the maximum likelihood solution for (cid:22) and ln (cid:27)

is f(cid:22); (cid:27)gML =n(cid:22)x; (cid:27)N =pS=No :

There is more to the posterior distribution than just its mode. As can
be seen in (cid:12)gure 24.1a, the likelihood has a skew peak. As we increase (cid:27),
the width of the conditional distribution of (cid:22) increases ((cid:12)gure 22.1b). And
if we (cid:12)x (cid:22) to a sequence of values moving away from the sample mean (cid:22)x, we
obtain a sequence of conditional distributions over (cid:27) whose maxima move to
increasing values of (cid:27) ((cid:12)gure 24.1c).

The posterior probability of (cid:22) given (cid:27) is

P ((cid:22)jfxngN

n=1; (cid:27)) =

P (fxngN

n=1 j (cid:22); (cid:27))P ((cid:22))

P (fxngN

n=1 j (cid:27))

/ exp((cid:0)N ((cid:22) (cid:0) (cid:22)x)2=(2(cid:27)2))
= Normal((cid:22); (cid:22)x; (cid:27)2=N ):
We note the familiar (cid:27)=pN scaling of the error bars on (cid:22).

(24.9)

(24.10)

(24.11)

Let us now ask the question ‘given the data, and the noninformative priors,
what might (cid:27) be?’ This question di(cid:11)ers from the (cid:12)rst one we asked in that we
are now not interested in (cid:22). This parameter must therefore be marginalized
over. The posterior probability of (cid:27) is:

P ((cid:27) jfxngN

n=1) =

P (fxngN

n=1 j (cid:27))P ((cid:27))

P (fxngN

n=1)

:

(24.12)

The data-dependent term P (fxngN
n=1 j (cid:27)) appeared earlier as the normalizing
constant in equation (24.9); one name for this quantity is the ‘evidence’, or
marginal likelihood, for (cid:27). We obtain the evidence for (cid:27) by integrating out
(cid:22); a noninformative prior P ((cid:22)) = constant is assumed; we call this constant
1=(cid:27)(cid:22), so that we can think of the prior as a top-hat prior of width (cid:27)(cid:22). The
Gaussian integral, P (fxngN

n=1 j (cid:22); (cid:27))P ((cid:22)) d(cid:22); yields:

n=1 j (cid:27)) =R P (fxngN

ln P (fxngN

n=1 j (cid:27)) = (cid:0)N ln(p2(cid:25)(cid:27)) (cid:0)

S
2(cid:27)2 + ln

p2(cid:25)(cid:27)=pN

(cid:27)(cid:22)

:

(24.13)

The (cid:12)rst two terms are the best-(cid:12)t log likelihood (i.e., the log likelihood with
(cid:22) = (cid:22)x). The last term is the log of the Occam factor which penalizes smaller
values of (cid:27). (We will discuss Occam factors more in Chapter 28.) When we
di(cid:11)erentiate the log evidence with respect to ln (cid:27), to (cid:12)nd the most probable
(cid:27), the additional volume factor ((cid:27)=pN ) shifts the maximum from (cid:27)N to

(cid:27)N(cid:0)1 =pS=(N (cid:0) 1):

(24.14)

Intuitively, the denominator (N(cid:0)1) counts the number of noise measurements
contained in the quantity S = Pn(xn(cid:0) (cid:22)x)2. The sum contains N residuals
squared, but there are only (N(cid:0)1) e(cid:11)ective noise measurements because the
determination of one parameter (cid:22) from the data causes one dimension of noise
to be gobbled up in unavoidable over(cid:12)tting. In the terminology of classical

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

24.2: Exercises

323

statistics, the Bayesian’s best guess for (cid:27) sets (cid:31)2 (the measure of deviance

de(cid:12)ned by (cid:31)2 (cid:17)Pn(xn (cid:0) ^(cid:22))2=^(cid:27)2) equal to the number of degrees of freedom,
N (cid:0) 1.
Figure 24.1d shows the posterior probability of (cid:27), which is proportional
to the marginal likelihood. This may be contrasted with the posterior prob-
ability of (cid:27) with (cid:22) (cid:12)xed to its most probable value, (cid:22)x = 1, which is shown in
(cid:12)gure 24.1c and d.

The (cid:12)nal inference we might wish to make is ‘given the data, what is (cid:22)?’

. Exercise 24.2.[3 ] Marginalize over (cid:27) and obtain the posterior marginal distri-

bution of (cid:22), which is a Student-t distribution:

P ((cid:22)j D) / 1=(cid:0)N ((cid:22) (cid:0) (cid:22)x)2 + S(cid:1)N=2

:

(24.15)

Further reading

A bible of exact marginalization is Bretthorst’s (1988) book on Bayesian spec-
trum analysis and parameter estimation.

24.2 Exercises

. Exercise 24.3.[3 ] [This exercise requires macho integration capabilities.] Give
a Bayesian solution to exercise 22.15 (p.309), where seven scientists of
varying capabilities have measured (cid:22) with personal noise levels (cid:27)n,
and we are interested in inferring (cid:22). Let the prior on each (cid:27)n be a
broad prior, for example a gamma distribution with parameters (s; c) =
(10; 0:1). Find the posterior distribution of (cid:22). Plot it, and explore its
properties for a variety of data sets such as the one given, and the data
set fxng = f13:01; 7:39g.
[Hint: (cid:12)rst (cid:12)nd the posterior distribution of (cid:27)n given (cid:22) and xn,
P ((cid:27)n j xn; (cid:22)). Note that the normalizing constant for this inference is
P (xn j (cid:22)). Marginalize over (cid:27)n to (cid:12)nd this normalizing constant, then
use Bayes’ theorem a second time to (cid:12)nd P ((cid:22)jfxng).]

24.3 Solutions

Solution to exercise 24.1 (p.321). 1. The data points are distributed with mean
squared deviation (cid:27)2 about the true mean. 2. The sample mean is unlikely
to exactly equal the true mean. 3. The sample mean is the value of (cid:22) that
minimizes the sum squared deviation of the data points from (cid:22). Any other
value of (cid:22) (in particular, the true value of (cid:22)) will have a larger value of the
sum-squared deviation that (cid:22) = (cid:22)x.

So the expected mean squared deviation from the sample mean is neces-

sarily smaller than the mean squared deviation (cid:27) 2 about the true mean.

A

B C D-G

-30

-20

-10

0

10

20

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

25

Exact Marginalization in Trellises

In this chapter we will discuss a few exact methods that are used in proba-
bilistic modelling. As an example we will discuss the task of decoding a linear
error-correcting code. We will see that inferences can be conducted most e(cid:14)-
ciently by message-passing algorithms, which take advantage of the graphical
structure of the problem to avoid unnecessary duplication of computations
(see Chapter 16).

25.1 Decoding problems

A codeword t is selected from a linear (N; K) code C, and it is transmitted
over a noisy channel; the received signal is y. In this chapter we will assume
that the channel is a memoryless channel such as a Gaussian channel. Given
an assumed channel model P (y j t), there are two decoding problems.
The codeword decoding problem is the task of inferring which codeword

t was transmitted given the received signal.

The bitwise decoding problem is the task of inferring for each transmit-

ted bit tn how likely it is that that bit was a one rather than a zero.

As a concrete example, take the (7; 4) Hamming code. In Chapter 1, we
discussed the codeword decoding problem for that code, assuming a binary
symmetric channel. We didn’t discuss the bitwise decoding problem and we
didn’t discuss how to handle more general channel models such as a Gaussian
channel.

Solving the codeword decoding problem

By Bayes’ theorem, the posterior probability of the codeword t is

P (tj y) =

P (y j t)P (t)

P (y)

:

(25.1)

Likelihood function. The (cid:12)rst factor in the numerator, P (y j t), is the likeli-
hood of the codeword, which, for any memoryless channel, is a separable
function,

P (y j t) =

N

Yn=1

P (yn j tn):

(25.2)

For example, if the channel is a Gaussian channel with transmissions (cid:6)x
and additive noise of standard deviation (cid:27), then the probability density

324

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

25.1: Decoding problems

325

of the received signal yn in the two cases tn = 0; 1 is

P (yn j tn = 1) =

P (yn j tn = 0) =

1

p2(cid:25)(cid:27)2
p2(cid:25)(cid:27)2

1

exp(cid:18)(cid:0)
exp(cid:18)(cid:0)

2(cid:27)2 (cid:19)
(yn (cid:0) x)2
2(cid:27)2 (cid:19) :

(yn + x)2

(25.3)

(25.4)

From the point of view of decoding, all that matters is the likelihood
ratio, which for the case of the Gaussian channel is

P (yn j tn = 1)
P (yn j tn = 0)

= exp(cid:18) 2xyn
(cid:27)2 (cid:19) :

(25.5)

Exercise 25.1.[2 ] Show that from the point of view of decoding, a Gaussian
channel is equivalent to a time-varying binary symmetric channel with
a known noise level fn which depends on n.

Prior. The second factor in the numerator is the prior probability of the
codeword, P (t), which is usually assumed to be uniform over all valid
codewords.

The denominator in (25.1) is the normalizing constant

P (y) =Xt

P (y j t)P (t):

(25.6)

The complete solution to the codeword decoding problem is a list of all
codewords and their probabilities as given by equation (25.1). Since the num-
ber of codewords in a linear code, 2K, is often very large, and since we are not
interested in knowing the detailed probabilities of all the codewords, we often
restrict attention to a simpli(cid:12)ed version of the codeword decoding problem.

The MAP codeword decoding problem is the task of

identifying the

most probable codeword t given the received signal.

If the prior probability over codewords is uniform then this task is iden-
tical to the problem of maximum likelihood decoding, that is, identifying
the codeword that maximizes P (y j t).

Example: In Chapter 1, for the (7; 4) Hamming code and a binary symmetric
channel we discussed a method for deducing the most probable codeword from
the syndrome of the received signal, thus solving the MAP codeword decoding
problem for that case. We would like a more general solution.

The MAP codeword decoding problem can be solved in exponential time
(of order 2K) by searching through all codewords for the one that maximizes
P (y j t)P (t). But we are interested in methods that are more e(cid:14)cient than
this. In section 25.3, we will discuss an exact method known as the min{sum
algorithm which may be able to solve the codeword decoding problem more
e(cid:14)ciently; how much more e(cid:14)ciently depends on the properties of the code.
It is worth emphasizing that MAP codeword decoding for a general lin-
ear code is known to be NP-complete (which means in layman’s terms that
MAP codeword decoding has a complexity that scales exponentially with the
blocklength, unless there is a revolution in computer science). So restrict-
ing attention to the MAP decoding problem hasn’t necessarily made the task
much less challenging; it simply makes the answer briefer to report.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

326

25 | Exact Marginalization in Trellises

Solving the bitwise decoding problem

Formally, the exact solution of the bitwise decoding problem is obtained from
equation (25.1) by marginalizing over the other bits.

P (tn j y) = Xftn0 : n06=ng

P (tj y):

(25.7)

We can also write this marginal with the aid of a truth function  [S] that is
one if the proposition S is true and zero otherwise.

P (tn = 1j y) = Xt
P (tn = 0j y) = Xt

P (tj y)  [tn = 1]
P (tj y)  [tn = 0]:

(25.8)

(25.9)

Computing these marginal probabilities by an explicit sum over all codewords
t takes exponential time. But, for certain codes, the bitwise decoding problem
can be solved much more e(cid:14)ciently using the forward{backward algorithm.
We will describe this algorithm, which is an example of the sum{product
algorithm, in a moment. Both the min{sum algorithm and the sum{product
algorithm have widespread importance, and have been invented many times
in many (cid:12)elds.

25.2 Codes and trellises

In Chapters 1 and 11, we represented linear (N; K) codes in terms of their
generator matrices and their parity-check matrices. In the case of a systematic
block code, the (cid:12)rst K transmitted bits in each block of size N are the source
bits, and the remaining M = N (cid:0)K bits are the parity-check bits. This means
that the generator matrix of the code can be written

(a)

(b)

Repetition code R3

Simple parity code P3

P (cid:21) ;
GT =(cid:20) IK

and the parity-check matrix can be written

H =(cid:2) P IM (cid:3) ;

(25.10)

(c)

(25.11)

where P is an M (cid:2) K matrix.
In this section we will study another representation of a linear code called a
trellis. The codes that these trellises represent will not in general be systematic
codes, but they can be mapped onto systematic codes if desired by a reordering
of the bits in a block.

(7; 4) Hamming code

Figure 25.1. Examples of trellises.
Each edge in a trellis is labelled
by a zero (shown by a square) or
a one (shown by a cross).

De(cid:12)nition of a trellis

Our de(cid:12)nition will be quite narrow. For a more comprehensive view of trellises,
the reader should consult Kschischang and Sorokine (1995).

A trellis is a graph consisting of nodes (also known as states or vertices) and
edges. The nodes are grouped into vertical slices called times, and the
times are ordered such that each edge connects a node in one time to
a node in a neighbouring time. Every edge is labelled with a symbol.
The leftmost and rightmost states contain only one node. Apart from
these two extreme nodes, all nodes in the trellis have at least one edge
connecting leftwards and at least one connecting rightwards.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

25.3: Solving the decoding problems on a trellis

327

A trellis with N + 1 times de(cid:12)nes a code of blocklength N as follows: a
codeword is obtained by taking a path that crosses the trellis from left to right
and reading out the symbols on the edges that are traversed. Each valid path
through the trellis de(cid:12)nes a codeword. We will number the leftmost time ‘time
0’ and the rightmost ‘time N ’. We will number the leftmost state ‘state 0’
and the rightmost ‘state I’, where I is the total number of states (vertices) in
the trellis. The nth bit of the codeword is emitted as we move from time n(cid:0)1
to time n.
The width of the trellis at a given time is the number of nodes in that

time. The maximal width of a trellis is what it sounds like.

A trellis is called a linear trellis if the code it de(cid:12)nes is a linear code. We will
solely be concerned with linear trellises from now on, as nonlinear trellises are
much more complex beasts. For brevity, we will only discuss binary trellises,
that is, trellises whose edges are labelled with zeroes and ones. It is not hard
to generalize the methods that follow to q-ary trellises.

Figures 25.1(a{c) show the trellises corresponding to the repetition code
R3 which has (N; K) = (3; 1); the parity code P3 with (N; K) = (3; 2); and
the (7; 4) Hamming code.

. Exercise 25.2.[2 ] Con(cid:12)rm that the sixteen codewords listed in table 1.14 are

generated by the trellis shown in (cid:12)gure 25.1c.

Observations about linear trellises

For any linear code the minimal trellis is the one that has the smallest number
of nodes. In a minimal trellis, each node has at most two edges entering it and
at most two edges leaving it. All nodes in a time have the same left degree as
each other and they have the same right degree as each other. The width is
always a power of two.

A minimal trellis for a linear (N; K) code cannot have a width greater than
2K since every node has at least one valid codeword through it, and there are
only 2K codewords. Furthermore, if we de(cid:12)ne M = N (cid:0) K, the minimal
trellis’s width is everywhere less than 2M . This will be proved in section 25.4.
Notice that for the linear trellises in (cid:12)gure 25.1, all of which are minimal
trellises, K is the number of times a binary branch point is encountered as the
trellis is traversed from left to right or from right to left.

We will discuss the construction of trellises more in section 25.4. But we

now know enough to discuss the decoding problem.

25.3 Solving the decoding problems on a trellis

We can view the trellis of a linear code as giving a causal description of the
probabilistic process that gives rise to a codeword, with time (cid:13)owing from left
to right. Each time a divergence is encountered, a random source (the source
of information bits for communication) determines which way we go.

At the receiving end, we receive a noisy version of the sequence of edge-
labels, and wish to infer which path was taken, or to be precise, (a) we want
to identify the most probable path in order to solve the codeword decoding
problem; and (b) we want to (cid:12)nd the probability that the transmitted symbol
at time n was a zero or a one, to solve the bitwise decoding problem.

Example 25.3. Consider the case of a single transmission from the Hamming
(7; 4) trellis shown in (cid:12)gure 25.1c.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

328

25 | Exact Marginalization in Trellises

Figure 25.2. Posterior probabilities
over the sixteen codewords when
the received vector y has
normalized likelihoods
(0:1; 0:4; 0:9; 0:1; 0:1; 0:1; 0:3).

t

Likelihood Posterior probability

0000000
0001011
0010111
0011100
0100110
0101101
0110001
0111010
1000101
1001110
1010010
1011001
1100011
1101000
1110100
1111111

0.0275562
0.0001458
0.0013122
0.0030618
0.0002268
0.0000972
0.0708588
0.0020412
0.0001458
0.0000042
0.0030618
0.0013122
0.0000972
0.0002268
0.0020412
0.0000108

0.25
0.0013
0.012
0.027
0.0020
0.0009
0.63
0.018
0.0013
0.0000
0.027
0.012
0.0009
0.0020
0.018
0.0001

Let the normalized likelihoods be: (0:1; 0:4; 0:9; 0:1; 0:1; 0:1; 0:3). That is,

the ratios of the likelihoods are

P (y1 j x1 = 1)
P (y1 j x1 = 0)

=

0:1
0:9

;

P (y2 j x2 = 1)
P (y2 j x2 = 0)

=

0:4
0:6

; etc.

(25.12)

How should this received signal be decoded?

1. If we threshold the likelihoods at 0.5 to turn the signal

into a bi-
nary received vector, we have r = (0; 0; 1; 0; 0; 0; 0), which decodes,
using the decoder for the binary symmetric channel (Chapter 1), into
^t = (0; 0; 0; 0; 0; 0; 0).

This is not the optimal decoding procedure. Optimal inferences are
always obtained by using Bayes’ theorem.

2. We can (cid:12)nd the posterior probability over codewords by explicit enu-
meration of all sixteen codewords. This posterior distribution is shown
in (cid:12)gure 25.2. Of course, we aren’t really interested in such brute-force
solutions, and the aim of this chapter is to understand algorithms for
getting the same information out in less than 2K computer time.

Examining the posterior probabilities, we notice that the most probable
codeword is actually the string t = 0110001. This is more than twice as
probable as the answer found by thresholding, 0000000.

Using the posterior probabilities shown in (cid:12)gure 25.2, we can also com-
pute the posterior marginal distributions of each of the bits. The result
is shown in (cid:12)gure 25.3. Notice that bits 1, 4, 5 and 6 are all quite con-
(cid:12)dently inferred to be zero. The strengths of the posterior probabilities
for bits 2, 3, and 7 are not so great.
2

In the above example, the MAP codeword is in agreement with the bitwise
decoding that is obtained by selecting the most probable state for each bit
using the posterior marginal distributions. But this is not always the case, as
the following exercise shows.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

25.3: Solving the decoding problems on a trellis

329

Likelihood

Posterior marginals

P (yn j tn = 1) P (yn j tn = 0)

P (tn = 1j y)

P (tn = 0j y)

Figure 25.3. Marginal posterior
probabilities for the 7 bits under
the posterior distribution of
(cid:12)gure 25.2.

0:1
0:4
0:9
0:1
0:1
0:1
0:3

0:9
0:6
0:1
0:9
0:9
0:9
0:7

0:061
0:674
0:746
0:061
0:061
0:061
0:659

0:939
0:326
0:254
0:939
0:939
0:939
0:341

n

1
2
3
4
5
6
7

Exercise 25.4.[2, p.333] Find the most probable codeword in the case where
the normalized likelihood is (0:2; 0:2; 0:9; 0:2; 0:2; 0:2; 0:2). Also (cid:12)nd or
estimate the marginal posterior probability for each of the seven bits,
and give the bit-by-bit decoding.

[Hint: concentrate on the few codewords that have the largest probabil-
ity.]

We now discuss how to use message passing on a code’s trellis to solve the

decoding problems.

The min{sum algorithm

The MAP codeword decoding problem can be solved using the min{sum al-
gorithm that was introduced in section 16.3. Each codeword of the code
corresponds to a path across the trellis. Just as the cost of a journey is the
sum of the costs of its constituent steps, the log likelihood of a codeword is
the sum of the bitwise log likelihoods. By convention, we (cid:13)ip the sign of the
log likelihood (which we would like to maximize) and talk in terms of a cost,
which we would like to minimize.

We associate with each edge a cost (cid:0)log P (yn j tn), where tn is the trans-
mitted bit associated with that edge, and yn is the received symbol. The
min{sum algorithm presented in section 16.3 can then identify the most prob-
able codeword in a number of computer operations equal to the number of
edges in the trellis. This algorithm is also known as the Viterbi algorithm
(Viterbi, 1967).

The sum{product algorithm

To solve the bitwise decoding problem, we can make a small modi(cid:12)cation to
the min{sum algorithm, so that the messages passed through the trellis de(cid:12)ne
‘the probability of the data up to the current point’ instead of ‘the cost of the
best route to this point’. We replace the costs on the edges, (cid:0)log P (yn j tn), by
the likelihoods themselves, P (yn j tn). We replace the min and sum operations
of the min{sum algorithm by a sum and product respectively.
Let i run over nodes/states, i = 0 be the label for the start state, P(i)
denote the set of states that are parents of state i, and wij be the likelihood
associated with the edge from node j to node i. We de(cid:12)ne the forward-pass
messages (cid:11)i by

(cid:11)0 = 1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

330

25 | Exact Marginalization in Trellises

(cid:11)i = Xj2P(i)

wij(cid:11)j:

(25.13)

These messages can be computed sequentially from left to right.

. Exercise 25.5.[2 ] Show that for a node i whose time-coordinate is n, (cid:11)i is
proportional to the joint probability that the codeword’s path passed
through node i and that the (cid:12)rst n received symbols were y1; : : : ; yn.

The message (cid:11)I computed at the end node of the trellis is proportional to the
marginal probability of the data.

. Exercise 25.6.[2 ] What is the constant of proportionality? [Answer: 2K]

We de(cid:12)ne a second set of backward-pass messages (cid:12)i in a similar manner.

Let node I be the end node.

(cid:12)I = 1

(cid:12)j = Xi:j2P(i)

wij(cid:12)i:

(25.14)

These messages can be computed sequentially in a backward pass from right
to left.

. Exercise 25.7.[2 ] Show that for a node i whose time-coordinate is n, (cid:12)i

is
proportional to the conditional probability, given that the codeword’s
path passed through node i, that the subsequent received symbols were
yn+1 : : : yN .

Finally, to (cid:12)nd the probability that the nth bit was a 1 or 0, we do two
summations of products of the forward and backward messages. Let i run over
nodes at time n and j run over nodes at time n (cid:0) 1, and let tij be the value
of tn associated with the trellis edge from node j to node i. For each value of
t = 0=1, we compute

r(t)

n = Xi;j: j2P(i); tij =t

(cid:11)jwij(cid:12)i:

(25.15)

Then the posterior probability that tn was t = 0=1 is

P (tn = tj y) =

1
Z
n + r(1)
where the normalizing constant Z = r(0)
forward message (cid:11)I that was computed earlier.
Exercise 25.8.[2 ] Con(cid:12)rm that the above sum{product algorithm does com-

n should be identical to the (cid:12)nal

r(t)
n ;

(25.16)

pute P (tn = tj y).

Other names for the sum{product algorithm presented here are ‘the forward{
backward algorithm’, ‘the BCJR algorithm’, and ‘belief propagation’.

. Exercise 25.9.[2, p.333] A codeword of the simple parity code P3 is transmitted,
and the received signal y has associated likelihoods shown in table 25.4.
Use the min{sum algorithm and the sum{product algorithm in the trellis
((cid:12)gure 25.1) to solve the MAP codeword decoding problem and the
bitwise decoding problem. Con(cid:12)rm your answers by enumeration of
all codewords (000, 011, 110, 101).
[Hint: use logs to base 2 and do
the min{sum computations by hand. When working the sum{product
algorithm by hand, you may (cid:12)nd it helpful to use three colours of pen,
one for the (cid:11)s, one for the ws, and one for the (cid:12)s.]

n

1
2
3

P (yn j tn)

tn = 0

tn = 1

1/4
1/2
1/8

1/2
1/4
1/2

Table 25.4. Bitwise likelihoods for
a codeword of P3.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

25.4: More on trellises

25.4 More on trellises

We now discuss various ways of making the trellis of a code. You may safely
jump over this section.

The span of a codeword is the set of bits contained between the (cid:12)rst bit in
the codeword that is non-zero, and the last bit that is non-zero, inclusive. We
can indicate the span of a codeword by a binary vector as shown in table 25.5.

331

Codeword 0000000
Span 0000000

0001011
0001111

0100110
0111110

1100011
1111111

0101101
0111111

Table 25.5. Some codewords and
their spans.

A generator matrix is in trellis-oriented form if the spans of the rows of the
generator matrix all start in di(cid:11)erent columns and the spans all end in di(cid:11)erent
columns.

How to make a trellis from a generator matrix

First, put the generator matrix into trellis-oriented form by row-manipulations
similar to Gaussian elimination. For example, our (7; 4) Hamming code can
be generated by

(25.17)

G =2
664

1 0 0 0 1 0 1
0 1 0 0 1 1 0
0 0 1 0 1 1 1
0 0 0 1 0 1 1

3
775

but this matrix is not in trellis-oriented form { for example, rows 1, 3 and 4
all have spans that end in the same column. By subtracting lower rows from
upper rows, we can obtain an equivalent generator matrix (that is, one that
generates the same set of codewords) as follows:

G =2
664

1 1 0 1 0 0 0
0 1 0 0 1 1 0
0 0 1 1 1 0 0
0 0 0 1 0 1 1

:

3
775

(25.18)

Now, each row of the generator matrix can be thought of as de(cid:12)ning an
(N; 1) subcode of the (N; K) code, that is, in this case, a code with two
codewords of length N = 7. For the (cid:12)rst row, the code consists of the two
codewords 1101000 and 0000000. The subcode de(cid:12)ned by the second row
consists of 0100110 and 0000000. It is easy to construct the minimal trellises
of these subcodes; they are shown in the left column of (cid:12)gure 25.6.

We build the trellis incrementally as shown in (cid:12)gure 25.6. We start with
the trellis corresponding to the subcode given by the (cid:12)rst row of the generator
matrix. Then we add in one subcode at a time. The vertices within the span
of the new subcode are all duplicated. The edge symbols in the original trellis
are left unchanged and the edge symbols in the second part of the trellis are
(cid:13)ipped wherever the new subcode has a 1 and otherwise left alone.

Another (7; 4) Hamming code can be generated by

G =2
664

1 1 1 0 0 0 0
0 1 1 1 1 0 0
0 0 1 0 1 1 0
0 0 0 1 1 1 1

:

3
775

(25.19)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

332

25 | Exact Marginalization in Trellises

Figure 25.6. Trellises for four
subcodes of the (7; 4) Hamming
code (left column), and the
sequence of trellises that are made
when constructing the trellis for
the (7; 4) Hamming code (right
column).
Each edge in a trellis is labelled
by a zero (shown by a square) or
a one (shown by a cross).

+

+

+

=

=

=

The (7; 4) Hamming code generated by this matrix di(cid:11)ers by a permutation
of its bits from the code generated by the systematic matrix used in Chapter
1 and above. The parity-check matrix corresponding to this permutation is:

H =2
4

1 0 1 0 1 0 1
0 1 1 0 0 1 1
0 0 0 1 1 1 1

3
5 :

(25.20)

(a)

The trellis obtained from the permuted matrix G given in equation (25.19) is
shown in (cid:12)gure 25.7a. Notice that the number of nodes in this trellis is smaller
than the number of nodes in the previous trellis for the Hamming (7; 4) code
in (cid:12)gure 25.1c. We thus observe that rearranging the order of the codeword
bits can sometimes lead to smaller, simpler trellises.

Trellises from parity-check matrices

Another way of viewing the trellis is in terms of the syndrome. The syndrome
of a vector r is de(cid:12)ned to be Hr, where H is the parity-check matrix. A vector
is only a codeword if its syndrome is zero. As we generate a codeword we can
describe the current state by the partial syndrome, that is, the product of
H with the codeword bits thus far generated. Each state in the trellis is a
partial syndrome at one time coordinate. The starting and ending states are
both constrained to be the zero syndrome. Each node in a state represents a
di(cid:11)erent possible value for the partial syndrome. Since H is an M (cid:2) N matrix,
where M = N (cid:0) K, the syndrome is at most an M -bit vector. So we need at
most 2M nodes in each state. We can construct the trellis of a code from its
parity-check matrix by walking from each end, generating two trees of possible
syndrome sequences. The intersection of these two trees de(cid:12)nes the trellis of
the code.

In the pictures we obtain from this construction, we can let the vertical
coordinate represent the syndrome. Then any horizontal edge is necessarily
associated with a zero bit (since only a non-zero bit changes the syndrome)

(b)

Figure 25.7. Trellises for the
permuted (7; 4) Hamming code
generated from (a) the generator
matrix by the method of
(cid:12)gure 25.6; (b) the parity-check
matrix by the method on page
332.
Each edge in a trellis is labelled
by a zero (shown by a square) or
a one (shown by a cross).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

25.5: Solutions

333

and any non-horizontal edge is associated with a one bit. (Thus in this rep-
resentation we no longer need to label the edges in the trellis.) Figure 25.7b
shows the trellis corresponding to the parity-check matrix of equation (25.20).

25.5 Solutions

t

Likelihood Posterior probability

Table 25.8. The posterior
probability over codewords for
exercise 25.4.

0000000
0001011
0010111
0011100
0100110
0101101
0110001
0111010
1000101
1001110
1010010
1011001
1100011
1101000
1110100
1111111

0.026
0.00041
0.0037
0.015
0.00041
0.00010
0.015
0.0037
0.00041
0.00010
0.015
0.0037
0.00010
0.00041
0.0037
0.000058

0.3006
0.0047
0.0423
0.1691
0.0047
0.0012
0.1691
0.0423
0.0047
0.0012
0.1691
0.0423
0.0012
0.0047
0.0423
0.0007

Solution to exercise 25.4 (p.329). The posterior probability over codewords is
shown in table 25.8. The most probable codeword is 0000000. The marginal
posterior probabilities of all seven bits are:

n

1
2
3
4
5
6
7

Likelihood

Posterior marginals

P (yn j tn = 1) P (yn j tn = 0)

P (tn = 1j y)

P (tn = 0j y)

0:2
0:2
0:9
0:2
0:2
0:2
0:2

0:8
0:8
0:1
0:8
0:8
0:8
0:8

0:266
0:266
0:677
0:266
0:266
0:266
0:266

0:734
0:734
0:323
0:734
0:734
0:734
0:734

So the bitwise decoding is 0010000, which is not actually a codeword.

Solution to exercise 25.9 (p.330). The MAP codeword is 101, and its like-
lihood is 1=8. The normalizing constant of the sum{product algorithm is
Z = (cid:11)I = 3/16. The intermediate (cid:11)i are (from left to right) 1/2, 1/4, 5/16, 4/16;
the intermediate (cid:12)i are (from right to left), 1/2, 1/8, 9/32, 3/16. The bitwise
decoding is: P (t1 = 1j y) = 3=4; P (t1 = 1j y) = 1=4; P (t1 = 1j y) = 5=6. The
codewords’ probabilities are 1/12, 2/12, 1/12, 8/12 for 000, 011, 110, 101.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

26

Exact Marginalization in Graphs

We now take a more general view of the tasks of inference and marginalization.
Before reading this chapter, you should read about message passing in Chapter
16.

26.1 The general problem
Assume that a function P (cid:3) of a set of N variables x (cid:17) fxngN
a product of M factors as follows:

n=1 is de(cid:12)ned as

P (cid:3)(x) =

M

Ym=1

fm(xm):

(26.1)

Each of the factors fm(xm) is a function of a subset xm of the variables that
make up x. If P (cid:3) is a positive function then we may be interested in a second
normalized function,

P (x) (cid:17) 1

Z

P (cid:3)(x) = 1
Z

M

Ym=1

fm(xm);

(26.2)

where the normalizing constant Z is de(cid:12)ned by

Z =Xx

M

Ym=1

fm(xm):

(26.3)

As an example of the notation we’ve just introduced, here’s a function of

three binary variables x1, x2, x3 de(cid:12)ned by the (cid:12)ve factors:

0:9 x1 = 1

f1(x1) = (cid:26) 0:1 x1 = 0
f2(x2) = (cid:26) 0:1 x2 = 0
f3(x3) = (cid:26) 0:9 x3 = 0
f4(x1; x2) = (cid:26) 1 (x1; x2) = (0; 0) or (1; 1)
f5(x2; x3) = (cid:26) 1 (x2; x3) = (0; 0) or (1; 1)

0 (x1; x2) = (1; 0) or (0; 1)

0 (x2; x3) = (1; 0) or (0; 1)

0:9 x2 = 1

0:1 x3 = 1

(26.4)

P (cid:3)(x) = f1(x1)f2(x2)f3(x3)f4(x1; x2)f5(x2; x3)
P (x) = 1

Z f1(x1)f2(x2)f3(x3)f4(x1; x2)f5(x2; x3):

The (cid:12)ve subsets of fx1; x2; x3g denoted by xm in the general function (26.1)
are here x1 = fx1g, x2 = fx2g, x3 = fx3g, x4 = fx1; x2g, and x5 = fx2; x3g.

334

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

26.1: The general problem

335

g
g
x2
x1
x3
       
       
       
@@ (cid:0)(cid:0) @@ (cid:0)(cid:0)
f5
f4
f3
f2
f1

g

Figure 26.1. The factor graph
associated with the function
P (cid:3)(x) (26.4).

The function P (x), by the way, may be recognized as the posterior prob-
ability distribution of the three transmitted bits in a repetition code (section
1.2) when the received signal is r = (1; 1; 0) and the channel is a binary sym-
metric channel with (cid:13)ip probability 0.1. The factors f4 and f5 respectively
enforce the constraints that x1 and x2 must be identical and that x2 and x3
must be identical. The factors f1, f2, f3 are the likelihood functions con-
tributed by each component of r.

A function of the factored form (26.1) can be depicted by a factor graph, in
which the variables are depicted by circular nodes and the factors are depicted
by square nodes. An edge is put between variable node n and factor node m
if the function fm(xm) has any dependence on variable xn. The factor graph
for the example function (26.4) is shown in (cid:12)gure 26.1.

The normalization problem

The (cid:12)rst task to be solved is to compute the normalizing constant Z.

The marginalization problems

The second task to be solved is to compute the marginal function of any
variable xn, de(cid:12)ned by

Zn(xn) = Xfxn0g; n06=n

P (cid:3)(x):

(26.5)

For example, if f is a function of three variables then the marginal for

n = 1 is de(cid:12)ned by

Z1(x1) = Xx2;x3

f (x1; x2; x3):

(26.6)

This type of summation, over ‘all the xn0 except for xn’ is so important that it
can be useful to have a special notation for it { the ‘not-sum’ or ‘summary’.
The third task to be solved is to compute the normalized marginal of any

variable xn, de(cid:12)ned by

Pn(xn) (cid:17) Xfxn0g; n06=n

P (x):

(26.7)

[We include the su(cid:14)x ‘n’ in Pn(xn), departing from our normal practice in the
rest of the book, where we would omit it.]

. Exercise 26.1.[1 ] Show that the normalized marginal is related to the marginal

Zn(xn) by

Pn(xn) =

Zn(xn)

Z

:

(26.8)

We might also be interested in marginals over a subset of the variables,

such as

Z12(x1; x2) (cid:17)Xx3

P (cid:3)(x1; x2; x3):

(26.9)

All these tasks are intractable in general. Even if every factor is a function
of only three variables, the cost of computing exact solutions for Z and for
the marginals is believed in general to grow exponentially with the number of
variables N .

For certain functions P (cid:3), however, the marginals can be computed e(cid:14)-
ciently by exploiting the factorization of P (cid:3). The idea of how this e(cid:14)ciency

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

336

26 | Exact Marginalization in Graphs

arises is well illustrated by the message-passing examples of Chapter 16. The
sum{product algorithm that we now review is a generalization of message-
passing rule-set B (p.242). As was the case there, the sum{product algorithm
is only valid if the graph is tree-like.

26.2 The sum{product algorithm

Notation

We identify the set of variables that the mth factor depends on, xm, by the set
of their indices N (m). For our example function (26.4), the sets are N (1) =
f1g (since f1 is a function of x1 alone), N (2) = f2g, N (3) = f3g, N (4) =
f1; 2g, and N (5) = f2; 3g. Similarly we de(cid:12)ne the set of factors in which
variable n participates, by M(n). We denote a set N (m) with variable n
excluded by N (m)nn. We introduce the shorthand xmnn or xmnn to denote
the set of variables in xm with xn excluded, i.e.,

xmnn (cid:17) fxn0 : n0 2 N (m)nng:

(26.10)

The sum{product algorithm will involve messages of two types passing
along the edges in the factor graph: messages qn!m from variable nodes to
factor nodes, and messages rm!n from factor nodes to variable nodes. A
message (of either type, q or r) that is sent along an edge connecting factor
fm to variable xn is always a function of the variable xn.

Here are the two rules for the updating of the two sets of messages.

From variable to factor:

qn!m(xn) = Ym02M(n)nm

rm0!n(xn):

(26.11)

From factor to variable:

rm!n(xn) = Xxmnn

0
@fm(xm) Yn02N (m)nn

qn0!m(xn0)1
A :

xn

rm!n(xn) = fm(xn)

(26.12)

fm

How these rules apply to leaves in the factor graph

A node that has only one edge connecting it to another node is called a leaf
node.

Some factor nodes in the graph may be connected to only one vari-
able node, in which case the set N (m)nn of variables appearing in the fac-
tor message update (26.12) is an empty set, and the product of functions
Qn02N (m)nn qn0!m(xn0) is the empty product, whose value is 1. Such a fac-

tor node therefore always broadcasts to its one neighbour xn the message
rm!n(xn) = fm(xn).
Similarly, there may be variable nodes that are connected to only one
factor node, so the set M(n)nm in (26.11) is empty. These nodes perpetually
broadcast the message qn!m(xn) = 1.

Starting and (cid:12)nishing, method 1

The algorithm can be initialized in two ways. If the graph is tree-like then
it must have nodes that are leaves. These leaf nodes can broadcast their

Figure 26.2. A factor node that is
a leaf node perpetually sends the
message rm!n(xn) = fm(xn) to
its one neighbour xn.

xn

qn!m(xn) = 1

fm

Figure 26.3. A variable node that
is a leaf node perpetually sends
the message qn!m(xn) = 1.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

26.2: The sum{product algorithm

337

messages to their respective neighbours from the start.

For all leaf variable nodes n:

For all leaf factor nodes m:

qn!m(xn) = 1
rm!n(xn) = fm(xn):

(26.13)

(26.14)

We can then adopt the procedure used in Chapter 16’s message-passing rule-
set B (p.242): a message is created in accordance with the rules (26.11, 26.12)
only if all the messages on which it depends are present. For example, in
(cid:12)gure 26.4, the message from x1 to f1 will be sent only when the message
from f4 to x1 has been received; and the message from x2 to f2, q2!2, can be
sent only when the messages r4!2 and r5!2 have both been received.

Messages will thus (cid:13)ow through the tree, one in each direction along every
edge, and after a number of steps equal to the diameter of the graph, every
message will have been created.

The answers we require can then be read out. The marginal function of

xn is obtained by multiplying all the incoming messages at that node.

Zn(xn) = Ym2M(n)

rm!n(xn):

(26.15)

g
g
x1
x2
x3
       
       
       
@@ (cid:0)(cid:0) @@ (cid:0)(cid:0)
f5
f4
f3
f2
f1

g

Figure 26.4. Our model factor
graph for the function P (cid:3)(x)
(26.4).

The normalizing constant Z can be obtained by summing any marginal

function, Z =Pxn Zn(xn), and the normalized marginals obtained from

Zn(xn)

:

Z

Pn(xn) =

(26.16)

. Exercise 26.2.[2 ] Apply the sum{product algorithm to the function de(cid:12)ned in
equation (26.4) and (cid:12)gure 26.1. Check that the normalized marginals
are consistent with what you know about the repetition code R3.

Exercise 26.3.[3 ] Prove that the sum{product algorithm correctly computes

the marginal functions Zn(xn) if the graph is tree-like.

Exercise 26.4.[3 ] Describe how to use the messages computed by the sum{
product algorithm to obtain more complicated marginal functions in a
tree-like graph, for example Z1;2(x1; x2), for two variables x1 and x2 that
are connected to one common factor node.

Starting and (cid:12)nishing, method 2

Alternatively, the algorithm can be initialized by setting all the initial mes-
sages from variables to 1:

for all n, m: qn!m(xn) = 1;

(26.17)

then proceeding with the factor message update rule (26.12), alternating with
the variable message update rule (26.11). Compared with method 1, this lazy
initialization method leads to a load of wasted computations, whose results
are gradually (cid:13)ushed out by the correct answers computed by method 1.

After a number of iterations equal to the diameter of the factor graph,
the algorithm will converge to a set of messages satisfying the sum{product
relationships (26.11, 26.12).

Exercise 26.5.[2 ] Apply this second version of the sum{product algorithm to

the function de(cid:12)ned in equation (26.4) and (cid:12)gure 26.1.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

338

26 | Exact Marginalization in Graphs

The reason for introducing this lazy method is that (unlike method 1) it can
be applied to graphs that are not tree-like. When the sum{product algorithm
is run on a graph with cycles, the algorithm does not necessarily converge,
and certainly does not in general compute the correct marginal functions; but
it is nevertheless an algorithm of great practical importance, especially in the
decoding of sparse-graph codes.

Sum{product algorithm with on-the-(cid:13)y normalization

If we are interested in only the normalized marginals, then another version
of the sum{product algorithm may be useful. The factor-to-variable messages
rm!n are computed in just the same way (26.12), but the variable-to-factor
messages are normalized thus:

qn!m(xn) = (cid:11)nm Ym02M(n)nm

rm0!n(xn)

where (cid:11)nm is a scalar chosen such that

qn!m(xn) = 1:

Xxn

(26.18)

(26.19)

Exercise 26.6.[2 ] Apply this normalized version of the sum{product algorithm

to the function de(cid:12)ned in equation (26.4) and (cid:12)gure 26.1.

A factorization view of the sum{product algorithm

One way to view the sum{product algorithm is that it reexpresses the original
m=1 fm(xm), as another

factored function, the product of M factors P (cid:3)(x) =QM

factored function which is the product of M + N factors,

P (cid:3)(x) =

M

Ym=1

(cid:30)m(xm)

N

Yn=1

 n(xn):

(26.20)

Each factor (cid:30)m is associated with a factor node m, and each factor  n(xn) is
associated with a variable node. Initially (cid:30)m(xm) = fm(xm) and  n(xn) = 1.
Each time a factor-to-variable message rm!n(xn) is sent, the factorization

is updated thus:

 n(xn) = Ym2M(n)

rm!n(xn)

(26.21)

(cid:30)m(xm) =

:

(26.22)

And each message can be computed in terms of (cid:30) and   using

f (xm)

Qn2N (m) rm!n(xn)
0
@(cid:30)m(xm) Yn02N (m)

 n0(xn0)1
A

rm!n(xn) = Xxmnn

(26.23)

which di(cid:11)ers from the assignment (26.12) in that the product is over all n0 2
N (m).
Exercise 26.7.[2 ] Con(cid:12)rm that the update rules (26.21{26.23) are equivalent
to the sum{product rules (26.11{26.12). So  n(xn) eventually becomes
the marginal Zn(xn).

This factorization viewpoint applies whether or not the graph is tree-like.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

339

26.3: The min{sum algorithm

Computational tricks

On-the-(cid:13)y normalization is a good idea from a computational point of view
because if P (cid:3) is a product of many factors, its values are likely to be very large
or very small.

Another useful computational trick involves passing the logarithms of the
messages q and r instead of q and r themselves; the computations of the
products in the algorithm (26.11, 26.12) are then replaced by simpler additions.
The summations in (26.12) of course become more di(cid:14)cult: to carry them out
and return the logarithm, we need to compute softmax functions like

l = ln(el1 + el2 + el3):

(26.24)

But this computation can be done e(cid:14)ciently using look-up tables along with
the observation that the value of the answer l is typically just a little larger
than maxi li. If we store in look-up tables values of the function

ln(1 + e(cid:14))

(26.25)

(for negative (cid:14)) then l can be computed exactly in a number of look-ups and
additions scaling as the number of terms in the sum. If look-ups and sorting
operations are cheaper than exp() then this approach costs less than the
direct evaluation (26.24). The number of operations can be further reduced
by omitting negligible contributions from the smallest of the flig.
A third computational trick applicable to certain error-correcting codes is
to pass not the messages but the Fourier transforms of the messages. This
again makes the computations of the factor-to-variable messages quicker. A
simple example of this Fourier transform trick is given in Chapter 47 at equa-
tion (47.9).

26.3 The min{sum algorithm

The sum{product algorithm solves the problem of (cid:12)nding the marginal func-
tion of a given product P (cid:3)(x). This is analogous to solving the bitwise decod-
ing problem of section 25.1. And just as there were other decoding problems
(for example, the codeword decoding problem), we can de(cid:12)ne other tasks
involving P (cid:3)(x) that can be solved by modi(cid:12)cations of the sum{product algo-
rithm. For example, consider this task, analogous to the codeword decoding
problem:

The maximization problem. Find the setting of x that maximizes the

product P (cid:3)(x).

This problem can be solved by replacing the two operations add and mul-
tiply everywhere they appear in the sum{product algorithm by another pair
of operations that satisfy the distributive law, namely max and multiply. If

we replace summation (+, P) by maximization, we notice that the quantity

formerly known as the normalizing constant,

P (cid:3)(x);

(26.26)

Z =Xx

becomes maxx P (cid:3)(x).

Thus the sum{product algorithm can be turned into a max{product algo-
rithm that computes maxx P (cid:3)(x), and from which the solution of the max-
imization problem can be deduced. Each ‘marginal’ Zn(xn) then lists the
maximum value that P (cid:3)(x) can attain for each value of xn.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

340

26 | Exact Marginalization in Graphs

In practice, the max{product algorithm is most often carried out in the
negative log likelihood domain, where max and product become min and sum.
The min{sum algorithm is also known as the Viterbi algorithm.

26.4 The junction tree algorithm

What should one do when the factor graph one is interested in is not a tree?
There are several options, and they divide into exact methods and approx-
imate methods. The most widely used exact method for handling marginaliza-
tion on graphs with cycles is called the junction tree algorithm. This algorithm
works by agglomerating variables together until the agglomerated graph has
no cycles. You can probably (cid:12)gure out the details for yourself; the complexity
of the marginalization grows exponentially with the number of agglomerated
variables. Read more about the junction tree algorithm in (Lauritzen, 1996;
Jordan, 1998).

There are many approximate methods, and we’ll visit some of them over
the next few chapters { Monte Carlo methods and variational methods, to
name a couple. However, the most amusing way of handling factor graphs
to which the sum{product algorithm may not be applied is, as we already
mentioned, to apply the sum{product algorithm! We simply compute the
messages for each node in the graph, as if the graph were a tree, iterate, and
cross our (cid:12)ngers. This so-called ‘loopy’ message passing has great importance
in the decoding of error-correcting codes, and we’ll come back to it in section
33.8 and Part VI.

Further reading

For further reading about factor graphs and the sum{product algorithm, see
Kschischang et al. (2001), Yedidia et al. (2000), Yedidia et al. (2001a), Yedidia
et al. (2002), Wainwright et al. (2003), and Forney (2001).

See also Pearl (1988). A good reference for the fundamental theory of
graphical models is Lauritzen (1996). A readable introduction to Bayesian
networks is given by Jensen (1996).

Interesting message-passing algorithms that have di(cid:11)erent capabilities from
the sum{product algorithm include expectation propagation (Minka, 2001)
and survey propagation (Braunstein et al., 2003). See also section 33.8.

26.5 Exercises

. Exercise 26.8.[2 ] Express the joint probability distribution from the burglar
alarm and earthquake problem (example 21.1 (p.293)) as a factor graph,
and (cid:12)nd the marginal probabilities of all the variables as each piece of
information comes to Fred’s attention, using the sum{product algorithm
with on-the-(cid:13)y normalization.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

27

Laplace’s Method

The idea behind the Laplace approximation is simple. We assume that an
unnormalized probability density P (cid:3)(x), whose normalizing constant

ZP (cid:17)Z P (cid:3)(x) dx

(27.1)

is of interest, has a peak at a point x0. We Taylor-expand the logarithm of
P (cid:3)(x) around this peak:

ln P (cid:3)(x) ’ ln P (cid:3)(x0) (cid:0)

c
2

(x (cid:0) x0)2 + (cid:1)(cid:1)(cid:1) ;

where

c = (cid:0)

@2

@x2 ln P (cid:3)(x)(cid:12)(cid:12)(cid:12)(cid:12)x=x0

:

We then approximate P (cid:3)(x) by an unnormalized Gaussian,

Q(cid:3)(x) (cid:17) P (cid:3)(x0) exph(cid:0)

c
2

(x (cid:0) x0)2i ;

(27.2)

(27.3)

(27.4)

P (cid:3)(x)

ln P (cid:3)(x)

ln P (cid:3)(x)
& ln Q(cid:3)(x)

P (cid:3)(x)
& Q(cid:3)(x)

and we approximate the normalizing constant ZP by the normalizing constant
of this Gaussian,

ZQ = P (cid:3)(x0)r 2(cid:25)

c

:

(27.5)

We can generalize this integral to approximate ZP for a density P (cid:3)(x) over
a K-dimensional space x. If the matrix of second derivatives of (cid:0) ln P (cid:3)(x) at
the maximum x0 is A, de(cid:12)ned by:

Aij = (cid:0)

@2

@xi@xj

ln P (cid:3)(x)(cid:12)(cid:12)(cid:12)(cid:12)x=x0

so that the expansion (27.2) is generalized to

;

(27.6)

ln P (cid:3)(x) ’ ln P (cid:3)(x0) (cid:0)

1
2

(x (cid:0) x0)TA(x (cid:0) x0) + (cid:1)(cid:1)(cid:1) ;

(27.7)

then the normalizing constant can be approximated by:

ZP ’ ZQ = P (cid:3)(x0)

= P (cid:3)(x0)r (2(cid:25))K

det A

:

(27.8)

Predictions can be made using the approximation Q. Physicists also call this
widely-used approximation the saddle-point approximation.

1

qdet 1

2(cid:25) A

341

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

342

27 | Laplace’s Method

The fact that the normalizing constant of a Gaussian is given by

Z dKx exp(cid:20)(cid:0)

1
2

xTAx(cid:21) =r (2(cid:25))K

det A

(27.9)

can be proved by making an orthogonal transformation into the basis u in which
A is transformed into a diagonal matrix. The integral then separates into a
product of one-dimensional integrals, each of the form

Z dui exp(cid:20)(cid:0)

1
2

(cid:21)iu2

i(cid:21) =r 2(cid:25)

(cid:21)i

:

(27.10)

The product of the eigenvalues (cid:21)i is the determinant of A.

The Laplace approximation is basis-dependent:

if x is transformed to a
nonlinear function u(x) and the density is transformed to P (u) = P (x)jdx=duj
then in general the approximate normalizing constants ZQ will be di(cid:11)erent.
This can be viewed as a defect { since the true value ZP is basis-independent
{ or an opportunity { because we can hunt for a choice of basis in which the
Laplace approximation is most accurate.

27.1 Exercises

Exercise 27.1.[2 ] (See also exercise 22.8 (p.307).) A photon counter is pointed
at a remote star for one minute, in order to infer the rate of photons
arriving at the counter per minute, (cid:21). Assuming the number of photons
collected r has a Poisson distribution with mean (cid:21),

P (r j (cid:21)) = exp((cid:0)(cid:21))

(cid:21)r
r!

;

(27.11)

and assuming the improper prior P ((cid:21)) = 1=(cid:21), make Laplace approxima-
tions to the posterior distribution

(a) over (cid:21)
(b) over log (cid:21).
constant.]

[Note the improper prior transforms to P (log (cid:21)) =

. Exercise 27.2.[2 ] Use Laplace’s method to approximate the integral

Z(u1; u2) =Z 1

(cid:0)1

da f (a)u1(1 (cid:0) f (a))u2;

(27.12)

where f (a) = 1=(1 + e(cid:0)a) and u1; u2 are positive. Check the accuracy of
the approximation against the exact answer (23.29, p.316) for (u1; u2) =
(1/2; 1/2) and (u1; u2) = (1; 1). Measure the error (log ZP (cid:0) log ZQ) in
bits.

. Exercise 27.3.[3 ] Linear regression. N datapoints f(x(n); t(n))g are generated by
the experimenter choosing each x(n), then the world delivering a noisy
version of the linear function

y(x) = w0 + w1x;

t(n) (cid:24) Normal(y(x(n)); (cid:27)2
(cid:23)):

(27.13)

(27.14)

Assuming Gaussian priors on w0 and w1, make the Laplace approxima-
tion to the posterior distribution of w0 and w1 (which is exact, in fact)
and obtain the predictive distribution for the next datapoint t(N+1), given
x(N+1).

(See MacKay (1992a) for further reading.)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28

Model Comparison and Occam’s Razor

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          

                                          


28.1 Occam’s razor

How many boxes are in the picture ((cid:12)gure 28.1)? In particular, how many
boxes are in the vicinity of the tree? If we looked with x-ray spectacles,
would we see one or two boxes behind the trunk ((cid:12)gure 28.2)? (Or even
more?) Occam’s razor is the principle that states a preference for simple
theories. ‘Accept the simplest explanation that (cid:12)ts the data’. Thus according
to Occam’s razor, we should deduce that there is only one box behind the tree.
Is this an ad hoc rule of thumb? Or is there a convincing reason for believing
there is most likely one box? Perhaps your intuition likes the argument ‘well,
it would be a remarkable coincidence for the two boxes to be just the same
height and colour as each other’.
If we wish to make arti(cid:12)cial intelligences
that interpret data correctly, we must translate this intuitive feeling into a
concrete theory.

Motivations for Occam’s razor

If several explanations are compatible with a set of observations, Occam’s
razor advises us to buy the simplest. This principle is often advocated for one
of two reasons: the (cid:12)rst is aesthetic (‘A theory with mathematical beauty is
more likely to be correct than an ugly one that (cid:12)ts some experimental data’

343

Figure 28.1. A picture to be
interpreted. It contains a tree and
some boxes.

1?

or 2?

Figure 28.2. How many boxes are
behind the tree?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

344

28 | Model Comparison and Occam’s Razor

Figure 28.3. Why Bayesian
inference embodies Occam’s razor.
This (cid:12)gure gives the basic
intuition for why complex models
can turn out to be less probable.
The horizontal axis represents the
space of possible data sets D.
Bayes’ theorem rewards models in
proportion to how much they
predicted the data that occurred.
These predictions are quanti(cid:12)ed
by a normalized probability
distribution on D. This
probability of the data given
model Hi, P (D jHi), is called the
evidence for Hi.
A simple model H1 makes only a
limited range of predictions,
shown by P (D jH1); a more
powerful model H2, that has, for
example, more free parameters
than H1, is able to predict a
greater variety of data sets. This
means, however, that H2 does not
predict the data sets in region C1
as strongly as H1. Suppose that
equal prior probabilities have been
assigned to the two models. Then,
if the data set falls in region C1,
the less powerful model H1 will be
the more probable model.

Evidence

P(D|H )1

P(D|H )2

C
1

D

(Paul Dirac)); the second reason is the past empirical success of Occam’s razor.
However there is a di(cid:11)erent justi(cid:12)cation for Occam’s razor, namely:

Coherent inference (as embodied by Bayesian probability) auto-
matically embodies Occam’s razor, quantitatively.

It is indeed more probable that there’s one box behind the tree, and we can
compute how much more probable one is than two.

Model comparison and Occam’s razor
We evaluate the plausibility of two alternative theories H1 and H2 in the light
of data D as follows: using Bayes’ theorem, we relate the plausibility of model
H1 given the data, P (H1 j D), to the predictions made by the model about
the data, P (D jH1), and the prior plausibility of H1, P (H1). This gives the
following probability ratio between theory H1 and theory H2:

:

(28.1)

P (H1 j D)
P (H2 j D)

=

P (H1)
P (H2)

P (D jH1)
P (D jH2)

The (cid:12)rst ratio (P (H1)=P (H2)) on the right-hand side measures how much our
initial beliefs favoured H1 over H2. The second ratio expresses how well the
observed data were predicted by H1, compared to H2.
How does this relate to Occam’s razor, when H1 is a simpler model than
H2? The (cid:12)rst ratio (P (H1)=P (H2)) gives us the opportunity, if we wish, to
insert a prior bias in favour of H1 on aesthetic grounds, or on the basis of
experience. This would correspond to the aesthetic and empirical motivations
for Occam’s razor mentioned earlier. But such a prior bias is not necessary:
the second ratio, the data-dependent factor, embodies Occam’s razor auto-
matically. Simple models tend to make precise predictions. Complex models,
by their nature, are capable of making a greater variety of predictions ((cid:12)gure
28.3). So if H2 is a more complex model, it must spread its predictive proba-
bility P (D jH2) more thinly over the data space than H1. Thus, in the case
where the data are compatible with both theories, the simpler H1 will turn out
more probable than H2, without our having to express any subjective dislike
for complex models. Our subjective prior just needs to assign equal prior prob-
abilities to the possibilities of simplicity and complexity. Probability theory
then allows the observed data to express their opinion.

Let us turn to a simple example. Here is a sequence of numbers:

(cid:0)1; 3; 7; 11:

The task is to predict the next two numbers, and infer the underlying process
that gave rise to this sequence. A popular answer to this question is the
prediction ‘15, 19’, with the explanation ‘add 4 to the previous number’.

What about the alternative answer ‘(cid:0)19:9; 1043:8’ with the underlying
rule being: ‘get the next number from the previous number, x, by evaluating

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28.1: Occam’s razor

345

(cid:0)x3=11 + 9=11x2 + 23=11’ ? I assume that this prediction seems rather less
plausible. But the second rule (cid:12)ts the data ((cid:0)1, 3, 7, 11) just as well as the
rule ‘add 4’. So why should we (cid:12)nd it less plausible? Let us give labels to the
two general theories:

Ha { the sequence is an arithmetic progression, ‘add n’, where n is an integer.
Hc { the sequence is generated by a cubic function of the form x ! cx3 +

dx2 + e, where c, d and e are fractions.

One reason for (cid:12)nding the second explanation, Hc, less plausible, might be
that arithmetic progressions are more frequently encountered than cubic func-
tions. This would put a bias in the prior probability ratio P (Ha)=P (Hc) in
equation (28.1). But let us give the two theories equal prior probabilities, and
concentrate on what the data have to say. How well did each theory predict
the data?

To obtain P (D jHa) we must specify the probability distribution that each
model assigns to its parameters. First, Ha depends on the added integer n,
and the (cid:12)rst number in the sequence. Let us say that these numbers could
each have been anywhere between (cid:0)50 and 50. Then since only the pair of
values fn = 4, (cid:12)rst number = (cid:0) 1g give rise to the observed data D = ((cid:0)1, 3,
7, 11), the probability of the data, given Ha, is:

P (D jHa) =

1
101

1
101

= 0:00010:

(28.2)

To evaluate P (D jHc), we must similarly say what values the fractions c; d
and e might take on. [I choose to represent these numbers as fractions rather
than real numbers because if we used real numbers, the model would assign,
relative to Ha, an in(cid:12)nitesimal probability to D. Real parameters are the
norm however, and are assumed in the rest of this chapter.] A reasonable
prior might state that for each fraction the numerator could be any number
between (cid:0)50 and 50, and the denominator is any number between 1 and 50.
As for the initial value in the sequence, let us leave its probability distribution
the same as in Ha. There are four ways of expressing the fraction c = (cid:0)1=11 =
(cid:0)2=22 = (cid:0)3=33 = (cid:0)4=44 under this prior, and similarly there are four and two
possible solutions for d and e, respectively. So the probability of the observed
data, given Hc, is found to be:
P (D jHc) = (cid:18) 1

50(cid:19)(cid:18) 2
= 0:0000000000025 = 2:5 (cid:2) 10(cid:0)12:

101(cid:19)(cid:18) 4

1

50(cid:19)

(28.3)

101

1

1

50(cid:19)(cid:18) 4

101

101

Thus comparing P (D jHc) with P (D jHa) = 0:00010, even if our prior prob-
abilities for Ha and Hc are equal, the odds, P (D jHa) : P (D jHc), in favour
of Ha over Hc, given the sequence D = ((cid:0)1, 3, 7, 11), are about forty million
to one.
This answer depends on several subjective assumptions; in particular, the
probability assigned to the free parameters n, c, d, e of the theories. Bayesians
make no apologies for this: there is no such thing as inference or prediction
without assumptions. However, the quantitative details of the prior proba-
bilities have no e(cid:11)ect on the qualitative Occam’s razor e(cid:11)ect; the complex
theory Hc always su(cid:11)ers an ‘Occam factor’ because it has more parameters,
and so can predict a greater variety of data sets ((cid:12)gure 28.3). This was only
a small example, and there were only four data points; as we move to larger

2

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

346

28 | Model Comparison and Occam’s Razor

Figure 28.4. Where Bayesian
inference (cid:12)ts into the data
modelling process.
This (cid:12)gure illustrates an
abstraction of the part of the
scienti(cid:12)c process in which data
are collected and modelled. In
particular, this (cid:12)gure applies to
pattern classi(cid:12)cation, learning,
interpolation, etc. The two
double-framed boxes denote the
two steps which involve inference.
It is only in those two steps that
Bayes’ theorem can be used.
Bayes does not tell you how to
invent models, for example.
The (cid:12)rst box, ‘(cid:12)tting each model
to the data’, is the task of
inferring what the model
parameters might be given the
model and the data. Bayesian
methods may be used to (cid:12)nd the
most probable parameter values,
and error bars on those
parameters. The result of
applying Bayesian methods to this
problem is often little di(cid:11)erent
from the answers given by
orthodox statistics.
The second inference task, model
comparison in the light of the
data, is where Bayesian methods
are in a class of their own. This
second inference problem requires
a quantitative Occam’s razor to
penalize over-complex models.
Bayesian methods can assign
objective preferences to the
alternative models in a way that
automatically embodies Occam’s
razor.

Gather
DATA

Create

alternative
MODELS

(cid:19) -

(cid:0)(cid:0)@@

Fit each MODEL

to the DATA

(cid:0)(cid:0)@@

(cid:16)(cid:27)

Gather

more data

6

Choose what

data to

gather next

Assign preferences to the

alternative MODELS

(cid:0)

(cid:0)(cid:0)(cid:9)

@

@@R

?

Choose future

actions

Create new

models

6

Decide whether
to create new

models

and more sophisticated problems the magnitude of the Occam factors typi-
cally increases, and the degree to which our inferences are in(cid:13)uenced by the
quantitative details of our subjective assumptions becomes smaller.

Bayesian methods and data analysis

Let us now relate the discussion above to real problems in data analysis.

There are countless problems in science, statistics and technology which
require that, given a limited data set, preferences be assigned to alternative
models of di(cid:11)ering complexities. For example, two alternative hypotheses
accounting for planetary motion are Mr. Inquisition’s geocentric model based
on ‘epicycles’, and Mr. Copernicus’s simpler model of the solar system with
the sun at the centre. The epicyclic model (cid:12)ts data on planetary motion at
least as well as the Copernican model, but does so using more parameters.
Coincidentally for Mr. Inquisition, two of the extra epicyclic parameters for
every planet are found to be identical to the period and radius of the sun’s
‘cycle around the earth’.
Intuitively we (cid:12)nd Mr. Copernicus’s theory more
probable.

The mechanism of the Bayesian razor: the evidence and the Occam factor

Two levels of inference can often be distinguished in the process of data mod-
elling. At the (cid:12)rst level of inference, we assume that a particular model is true,
and we (cid:12)t that model to the data, i.e., we infer what values its free param-
eters should plausibly take, given the data. The results of this inference are
often summarized by the most probable parameter values, and error bars on
those parameters. This analysis is repeated for each model. The second level
of inference is the task of model comparison. Here we wish to compare the
models in the light of the data, and assign some sort of preference or ranking
to the alternatives.

Note that both levels of inference are distinct from decision theory. The goal
of inference is, given a de(cid:12)ned hypothesis space and a particular data set, to
assign probabilities to hypotheses. Decision theory typically chooses between
alternative actions on the basis of these probabilities so as to minimize the

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28.1: Occam’s razor

347

expectation of a ‘loss function’. This chapter concerns inference alone and no
loss functions are involved. When we discuss model comparison, this should
not be construed as implying model choice. Ideal Bayesian predictions do not
involve choice between models; rather, predictions are made by summing over
all the alternative models, weighted by their probabilities.

Bayesian methods are able consistently and quantitatively to solve both
the inference tasks. There is a popular myth that states that Bayesian meth-
ods di(cid:11)er from orthodox statistical methods only by the inclusion of subjective
priors, which are di(cid:14)cult to assign, and which usually don’t make much dif-
ference to the conclusions.
It is true that, at the (cid:12)rst level of inference, a
Bayesian’s results will often di(cid:11)er little from the outcome of an orthodox at-
tack. What is not widely appreciated is how a Bayesian performs the second
level of inference; this chapter will therefore focus on Bayesian model compar-
ison.

Model comparison is a di(cid:14)cult task because it is not possible simply to
choose the model that (cid:12)ts the data best: more complex models can always
(cid:12)t the data better, so the maximum likelihood model choice would lead us
inevitably to implausible, over-parameterized models, which generalize poorly.
Occam’s razor is needed.

Let us write down Bayes’ theorem for the two levels of inference described
above, so as to see explicitly how Bayesian model comparison works. Each
model Hi is assumed to have a vector of parameters w. A model is de(cid:12)ned
by a collection of probability distributions: a ‘prior’ distribution P (w jHi),
which states what values the model’s parameters might be expected to take;
and a set of conditional distributions, one for each value of w, de(cid:12)ning the
predictions P (D j w;Hi) that the model makes about the data D.

1. Model (cid:12)tting. At the (cid:12)rst level of inference, we assume that one model,
the ith, say, is true, and we infer what the model’s parameters w might
be, given the data D. Using Bayes’ theorem, the posterior probability
of the parameters w is:

P (w j D;Hi) =

P (D j w;Hi)P (w jHi)

P (D jHi)

;

(28.4)

that is,

Posterior =

Likelihood (cid:2) Prior

Evidence

:

The normalizing constant P (D jHi) is commonly ignored since it is irrel-
evant to the (cid:12)rst level of inference, i.e., the inference of w; but it becomes
important in the second level of inference, and we name it the evidence
for Hi. It is common practice to use gradient-based methods to (cid:12)nd the
maximum of the posterior, which de(cid:12)nes the most probable value for the
parameters, wMP; it is then usual to summarize the posterior distribution
by the value of wMP, and error bars or con(cid:12)dence intervals on these best-
(cid:12)t parameters. Error bars can be obtained from the curvature of the pos-
terior; evaluating the Hessian at wMP, A = (cid:0)rr ln P (w j D;Hi)jwMP
,
and Taylor-expanding the log posterior probability with (cid:1)w = w(cid:0)wMP:
(28.5)

P (w j D;Hi) ’ P (wMP j D;Hi) exp(cid:0)(cid:0)1/2(cid:1)wTA(cid:1)w(cid:1) ;

we see that the posterior can be locally approximated as a Gaussian
with covariance matrix (equivalent to error bars) A(cid:0)1.
[Whether this
approximation is good or not will depend on the problem we are solv-
ing. Indeed, the maximum and mean of the posterior distribution have

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

348

28 | Model Comparison and Occam’s Razor

Figure 28.5. The Occam factor.
This (cid:12)gure shows the quantities
that determine the Occam factor
for a hypothesis Hi having a
single parameter w. The prior
distribution (solid line) for the
parameter has width (cid:27)w. The
posterior distribution (dashed
line) has a single peak at wMP
with characteristic width (cid:27)wjD.
The Occam factor is

(cid:27)wjDP (wMP jHi) =

(cid:27)wjD
(cid:27)w

:

P (w j D;Hi)

P (w jHi)

(cid:27)wjD

wMP

(cid:27)w

w

no fundamental status in Bayesian inference { they both change under
nonlinear reparameterizations. Maximization of a posterior probabil-
ity is useful only if an approximation like equation (28.5) gives a good
summary of the distribution.]

2. Model comparison. At the second level of inference, we wish to infer
which model is most plausible given the data. The posterior probability
of each model is:

P (Hi j D) / P (D jHi)P (Hi):

(28.6)

Notice that the data-dependent term P (D jHi) is the evidence for Hi,
which appeared as the normalizing constant in (28.4). The second term,
P (Hi), is the subjective prior over our hypothesis space, which expresses
how plausible we thought the alternative models were before the data
arrived. Assuming that we choose to assign equal priors P (Hi) to the
alternative models, models Hi are ranked by evaluating the evidence. The
normalizing constant P (D) =Pi P (D jHi)P (Hi) has been omitted from
equation (28.6) because in the data-modelling process we may develop
new models after the data have arrived, when an inadequacy of the (cid:12)rst
models is detected, for example. Inference is open ended: we continually
seek more probable models to account for the data we gather.
To repeat the key idea: to rank alternative models Hi, a Bayesian eval-
uates the evidence P (D jHi). This concept is very general:
the ev-
idence can be evaluated for parametric and ‘non-parametric’ models
alike; whatever our data-modelling task, a regression problem, a clas-
si(cid:12)cation problem, or a density estimation problem, the evidence is a
transportable quantity for comparing alternative models.
In all these
cases the evidence naturally embodies Occam’s razor.

Evaluating the evidence

Let us now study the evidence more closely to gain insight into how the
Bayesian Occam’s razor works. The evidence is the normalizing constant for
equation (28.4):

P (D jHi) =Z P (D j w;Hi)P (w jHi) dw:

(28.7)

For many problems the posterior P (w j D;Hi) / P (D j w;Hi)P (w jHi) has
a strong peak at the most probable parameters wMP ((cid:12)gure 28.5). Then,
taking for simplicity the one-dimensional case, the evidence can be approx-
imated, using Laplace’s method, by the height of the peak of the integrand
P (D j w;Hi)P (w jHi) times its width, (cid:27)wjD:

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28.1: Occam’s razor

349

P (D jHi) ’
{z
Evidence ’ Best (cid:12)t likelihood (cid:2) Occam factor

(cid:2) P (wMP jHi) (cid:27)wjD
|
}

P (D j wMP;Hi)
}
|

{z

Thus the evidence is found by taking the best-(cid:12)t likelihood that the model
can achieve and multiplying it by an ‘Occam factor’, which is a term with
magnitude less than one that penalizes Hi for having the parameter w.
Interpretation of the Occam factor

:

(28.8)

The quantity (cid:27)wjD is the posterior uncertainty in w. Suppose for simplicity
that the prior P (w jHi) is uniform on some large interval (cid:27)w, representing the
range of values of w that were possible a priori, according to Hi ((cid:12)gure 28.5).
Then P (wMP jHi) = 1=(cid:27)w, and

Occam factor =

;

(28.9)

(cid:27)wjD
(cid:27)w

i.e., the Occam factor is equal to the ratio of the posterior accessible volume
of Hi’s parameter space to the prior accessible volume, or the factor by which
Hi’s hypothesis space collapses when the data arrive. The model Hi can be
viewed as consisting of a certain number of exclusive submodels, of which only
one survives when the data arrive. The Occam factor is the inverse of that
number. The logarithm of the Occam factor is a measure of the amount of
information we gain about the model’s parameters when the data arrive.

A complex model having many parameters, each of which is free to vary
over a large range (cid:27)w, will typically be penalized by a stronger Occam factor
than a simpler model. The Occam factor also penalizes models that have to
be (cid:12)nely tuned to (cid:12)t the data, favouring models for which the required pre-
cision of the parameters (cid:27)wjD is coarse. The magnitude of the Occam factor
is thus a measure of complexity of the model; it relates to the complexity of
the predictions that the model makes in data space. This depends not only
on the number of parameters in the model, but also on the prior probability
that the model assigns to them. Which model achieves the greatest evidence
is determined by a trade-o(cid:11) between minimizing this natural complexity mea-
sure and minimizing the data mis(cid:12)t. In contrast to alternative measures of
model complexity, the Occam factor for a model is straightforward to evalu-
ate: it simply depends on the error bars on the parameters, which we already
evaluated when (cid:12)tting the model to the data.

Figure 28.6 displays an entire hypothesis space so as to illustrate the var-
ious probabilities in the analysis. There are three models, H1;H2;H3, which
have equal prior probabilities. Each model has one parameter w (each shown
on a horizontal axis), but assigns a di(cid:11)erent prior range (cid:27)W to that parame-
ter. H3 is the most ‘(cid:13)exible’ or ‘complex’ model, assigning the broadest prior
range. A one-dimensional data space is shown by the vertical axis. Each
model assigns a joint probability distribution P (D; w jHi) to the data and
the parameters, illustrated by a cloud of dots. These dots represent random
samples from the full probability distribution. The total number of dots in
each of the three model subspaces is the same, because we assigned equal prior
probabilities to the models.

When a particular data set D is received (horizontal line), we infer the pos-
terior distribution of w for a model (H3, say) by reading out the density along
that horizontal line, and normalizing. The posterior probability P (w j D;H3)
is shown by the dotted curve at the bottom. Also shown is the prior distribu-
tion P (w jH3) (cf. (cid:12)gure 28.5). [In the case of model H1 which is very poorly

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

350

28 | Model Comparison and Occam’s Razor

Figure 28.6. A hypothesis space
consisting of three exclusive
models, each having one
parameter w, and a
one-dimensional data set D. The
‘data set’ is a single measured
value which di(cid:11)ers from the
parameter w by a small amount
of additive noise. Typical samples
from the joint distribution
P (D; w;H) are shown by dots.
(N.B., these are not data points.)
The observed ‘data set’ is a single
particular value for D shown by
the dashed horizontal line. The
dashed curves below show the
posterior probability of w for each
model given this data set (cf.
(cid:12)gure 28.3). The evidence for the
di(cid:11)erent models is obtained by
marginalizing onto the D axis at
the left-hand side (cf. (cid:12)gure 28.5).

D

D

P (D jH3)

P (D jH2)

P (D jH1)

P (w j D;H1)

P (w j D;H2)

P (w jH1)

P (w jH2)

P (w j D;H3)
P (w jH3)

(cid:27)wjD

w

(cid:27)w

w

w

matched to the data, the shape of the posterior distribution will depend on
the details of the tails of the prior P (w jH1) and the likelihood P (D j w;H1);
the curve shown is for the case where the prior falls o(cid:11) more strongly.]
We obtain (cid:12)gure 28.3 by marginalizing the joint distributions P (D; w jHi)
onto the D axis at the left-hand side. For the data set D shown by the dotted
horizontal line, the evidence P (D jH3) for the more (cid:13)exible model H3 has
a smaller value than the evidence for H2. This is because H3 placed less
predictive probability (fewer dots) on that line. In terms of the distributions
over w, model H3 has smaller evidence because the Occam factor (cid:27)wjD=(cid:27)w is
smaller for H3 than for H2. The simplest model H1 has the smallest evidence
of all, because the best (cid:12)t that it can achieve to the data D is very poor.
Given this data set, the most probable model is H2.
Occam factor for several parameters

If the posterior is well approximated by a Gaussian, then the Occam factor
is obtained from the determinant of the corresponding covariance matrix (cf.
equation (28.8) and Chapter 27):

2 (A=2(cid:25))

;

(28.10)

}

P (D jHi) ’
P (D j wMP; Hi)
}
|
Evidence ’ Best (cid:12)t likelihood (cid:2)

{z

(cid:2) P (wMP jHi) det(cid:0) 1
|

{z

Occam factor

where A = (cid:0)rr ln P (w j D;Hi), the Hessian which we evaluated when we
calculated the error bars on wMP (equation 28.5 and Chapter 27). As the
amount of data collected increases, this Gaussian approximation is expected
to become increasingly accurate.

In summary, Bayesian model comparison is a simple extension of maximum
likelihood model selection: the evidence is obtained by multiplying the best-(cid:12)t
likelihood by the Occam factor.

To evaluate the Occam factor we need only the Hessian A, if the Gaussian
approximation is good. Thus the Bayesian method of model comparison by
evaluating the evidence is no more computationally demanding than the task
of (cid:12)nding for each model the best-(cid:12)t parameters and their error bars.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

351

1?

or 2?

Figure 28.7. How many boxes are
behind the tree?

28.2: Example

28.2 Example

Let’s return to the example that opened this chapter. Are there one or two
boxes behind the tree in (cid:12)gure 28.1? Why do coincidences make us suspicious?
Let’s assume the image of the area round the trunk and box has a size
of 50 pixels, that the trunk is 10 pixels wide, and that 16 di(cid:11)erent colours of
boxes can be distinguished. The theory H1 that says there is one box near
the trunk has four free parameters: three coordinates de(cid:12)ning the top three
edges of the box, and one parameter giving the box’s colour. (If boxes could
levitate, there would be (cid:12)ve free parameters.)

The theory H2 that says there are two boxes near the trunk has eight free
parameters (twice four), plus a ninth, a binary variable that indicates which
of the two boxes is the closest to the viewer.

What is the evidence for each model? We’ll do H1 (cid:12)rst. We need a prior on
the parameters to evaluate the evidence. For convenience, let’s work in pixels.
Let’s assign a separable prior to the horizontal location of the box, its width,
its height, and its colour. The height could have any of, say, 20 distinguishable
values, so could the width, and so could the location. The colour could have
any of 16 values. We’ll put uniform priors over these variables. We’ll ignore
all the parameters associated with other objects in the image, since they don’t
come into the model comparison between H1 and H2. The evidence is

P (D jH1) =

1
20

1
20

1
20

1
16

(28.11)

since only one setting of the parameters (cid:12)ts the data, and it predicts the data
perfectly.

As for model H2, six of its nine parameters are well-determined, and three
of them are partly-constrained by the data. If the left-hand box is furthest
away, for example, then its width is at least 8 pixels and at most 30; if it’s
the closer of the two boxes, then its width is between 8 and 18 pixels. (I’m
assuming here that the visible portion of the left-hand box is about 8 pixels
wide.) To get the evidence we need to sum up the prior probabilities of all
viable hypotheses. To do an exact calculation, we need to be more speci(cid:12)c
about the data and the priors, but let’s just get the ballpark answer, assuming
that the two unconstrained real variables have half their values available, and
that the binary variable is completely undetermined. (As an exercise, you can
make an explicit model and work out the exact answer.)

P (D jH2) ’

1
20

1
20

10
20

1
16

1
20

1
20

10
20

1
16

2
2

:

(28.12)

Thus the posterior probability ratio is (assuming equal prior probability):

P (D jH1)P (H1)
P (D jH2)P (H2)

=

1
10
10
20
20

1
20

1
16

= 20 (cid:2) 2 (cid:2) 2 (cid:2) 16 ’ 1000=1:

(28.13)

(28.14)

So the data are roughly 1000 to 1 in favour of the simpler hypothesis. The
four factors in (28.13) can be interpreted in terms of Occam factors. The more
complex model has four extra parameters for sizes and colours { three for sizes,
and one for colour. It has to pay two big Occam factors (1/20 and 1/16) for the
highly suspicious coincidences that the two box heights match exactly and the
two colours match exactly; and it also pays two lesser Occam factors for the
two lesser coincidences that both boxes happened to have one of their edges
conveniently hidden behind a tree or behind each other.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

352

28 | Model Comparison and Occam’s Razor

Figure 28.8. A popular view of
model comparison by minimum
description length. Each model
Hi communicates the data D by
sending the identity of the model,
sending the best-(cid:12)t parameters of
the model w(cid:3), then sending the
data relative to those parameters.
As we proceed to more complex
models the length of the
parameter message increases. On
the other hand, the length of the
data message decreases, because a
complex model is able to (cid:12)t the
data better, making the residuals
smaller. In this example the
intermediate model H2 achieves
the optimum trade-o(cid:11) between
these two trends.

H1: L(H1) L(w(cid:3)
H2: L(H2)
H3: L(H3)

(1) jH1)
L(w(cid:3)

(2) jH2)
L(w(cid:3)

(3) jH3)

L(D j w(cid:3)
L(D j w(cid:3)

(1);H1)
(2);H2)
L(D j w(cid:3)

(3);H3)

28.3 Minimum description length (MDL)

A complementary view of Bayesian model comparison is obtained by replacing
probabilities of events by the lengths in bits of messages that communicate
the events without loss to a receiver. Message lengths L(x) correspond to a
probabilistic model over events x via the relations:

P (x) = 2(cid:0)L(x); L(x) = (cid:0) log2 P (x):

(28.15)

The MDL principle (Wallace and Boulton, 1968) states that one should
prefer models that can communicate the data in the smallest number of bits.
Consider a two-part message that states which model, H, is to be used, and
then communicates the data D within that model, to some pre-arranged pre-
cision (cid:14)D. This produces a message of length L(D;H) = L(H) + L(D jH).
The lengths L(H) for di(cid:11)erent H de(cid:12)ne an implicit prior P (H) over the alter-
native models. Similarly L(D jH) corresponds to a density P (D jH). Thus, a
procedure for assigning message lengths can be mapped onto posterior prob-
abilities:

L(D;H) = (cid:0) log P (H) (cid:0) log (P (D jH)(cid:14)D)

= (cid:0) log P (H j D) + const:

(28.16)

(28.17)

In principle, then, MDL can always be interpreted as Bayesian model compar-
ison and vice versa. However, this simple discussion has not addressed how
one would actually evaluate the key data-dependent term L(D jH), which
corresponds to the evidence for H. Often, this message is imagined as being
subdivided into a parameter block and a data block ((cid:12)gure 28.8). Models with
a small number of parameters have only a short parameter block but do not
(cid:12)t the data well, and so the data message (a list of large residuals) is long. As
the number of parameters increases, the parameter block lengthens, and the
data message becomes shorter. There is an optimum model complexity (H2
in the (cid:12)gure) for which the sum is minimized.
This picture glosses over some subtle issues. We have not speci(cid:12)ed the
precision to which the parameters w should be sent. This precision has an
important e(cid:11)ect (unlike the precision (cid:14)D to which real-valued data D are
sent, which, assuming (cid:14)D is small relative to the noise level, just introduces
an additive constant). As we decrease the precision to which w is sent, the
parameter message shortens, but the data message typically lengthens because
the truncated parameters do not match the data so well. There is a non-trivial
optimal precision.
In simple Gaussian cases it is possible to solve for this
optimal precision (Wallace and Freeman, 1987), and it is closely related to the
posterior error bars on the parameters, A(cid:0)1, where A = (cid:0)rr ln P (w j D;H).
It turns out that the optimal parameter message length is virtually identical to
the log of the Occam factor in equation (28.10). (The random element involved
in parameter truncation means that the encoding is slightly sub-optimal.)

With care, therefore, one can replicate Bayesian results in MDL terms.
Although some of the earliest work on complex model comparison involved
the MDL framework (Patrick and Wallace, 1982), MDL has no apparent ad-
vantages over the direct probabilistic approach.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28.3: Minimum description length (MDL)

353

MDL does have its uses as a pedagogical tool. The description length
concept is useful for motivating prior probability distributions. Also, di(cid:11)erent
ways of breaking down the task of communicating data using a model can give
helpful insights into the modelling process, as will now be illustrated.

On-line learning and cross-validation.

In cases where the data consist of a sequence of points D = t(1); t(2); : : : ; t(N ),
the log evidence can be decomposed as a sum of ‘on-line’ predictive perfor-
mances:

log P (D jH) = log P (t(1) jH) + log P (t(2) j t(1);H)

+ log P (t(3) j t(1); t(2);H) + (cid:1)(cid:1)(cid:1) + log P (t(N ) j t(1) : : : t(N(cid:0)1);H): (28.18)
This decomposition can be used to explain the di(cid:11)erence between the ev-
idence and ‘leave-one-out cross-validation’ as measures of predictive abil-
ity. Cross-validation examines the average value of just the last term,
log P (t(N ) j t(1) : : : t(N(cid:0)1);H), under random re-orderings of the data. The evi-
dence, on the other hand, sums up how well the model predicted all the data,
starting from scratch.

The ‘bits-back’ encoding method.

Another MDL thought experiment (Hinton and van Camp, 1993) involves in-
corporating random bits into our message. The data are communicated using a
parameter block and a data block. The parameter vector sent is a random sam-
ple from the posterior, P (w j D;H) = P (D j w;H)P (w jH)=P (D jH). This
sample w is sent to an arbitrary small granularity (cid:14)w using a message length
L(w jH) = (cid:0) log[P (w jH)(cid:14)w]. The data are encoded relative to w with a
message of length L(D j w;H) = (cid:0) log[P (D j w;H)(cid:14)D]. Once the data mes-
sage has been received, the random bits used to generate the sample w from
the posterior can be deduced by the receiver. The number of bits so recov-
ered is (cid:0)log[P (w j D;H)(cid:14)w]. These recovered bits need not count towards the
message length, since we might use some other optimally-encoded message as
a random bit string, thereby communicating that message at the same time.
The net description cost is therefore:

L(w jH) + L(D j w;H) (cid:0) ‘Bits back’ = (cid:0) log

= (cid:0) log P (D jH) (cid:0) log (cid:14)D: (28.19)
Thus this thought experiment has yielded the optimal description length. Bits-
back encoding has been turned into a practical compression method for data
modelled with latent variable models by Frey (1998).

P (w jH) P (D j w;H) (cid:14)D

P (w j D;H)

Further reading

Bayesian methods are introduced and contrasted with sampling-theory statis-
tics in (Jaynes, 1983; Gull, 1988; Loredo, 1990). The Bayesian Occam’s razor
is demonstrated on model problems in (Gull, 1988; MacKay, 1992a). Useful
textbooks are (Box and Tiao, 1973; Berger, 1985).

One debate worth understanding is the question of whether it’s permis-
sible to use improper priors in Bayesian inference (Dawid et al., 1996).
If
we want to do model comparison (as discussed in this chapter), it is essen-
tial to use proper priors { otherwise the evidences and the Occam factors are

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

354

28 | Model Comparison and Occam’s Razor

meaningless. Only when one has no intention to do model comparison may
it be safe to use improper priors, and even in such cases there are pitfalls, as
Dawid et al. explain.
I would agree with their advice to always use proper
priors, tempered by an encouragement to be smart when making calculations,
recognizing opportunities for approximation.

28.4 Exercises

P (xjH0)

(cid:0)1

x

1

P (xj m =(cid:0)0:4;H1)

(cid:0)1

1 x

y = w0 + w1x

x

Exercise 28.1.[3 ] Random variables x come independently from a probability
distribution P (x). According to model H0, P (x) is a uniform distribu-
tion

P (xjH0) =

1
2

x 2 ((cid:0)1; 1):

(28.20)

According to model H1, P (x) is a nonuniform distribution with an un-
known parameter m 2 ((cid:0)1; 1):
1
P (xj m;H1) =
2

x 2 ((cid:0)1; 1):

(1 + mx)

(28.21)

Given the data D = f0:3; 0:5; 0:7; 0:8; 0:9g, what is the evidence for H0
and H1?

Exercise 28.2.[3 ] Datapoints (x; t) are believed to come from a straight line.

The experimenter chooses x, and t is Gaussian-distributed about

y = w0 + w1x

(28.22)

with variance (cid:27)2
(cid:23). According to model H1, the straight line is horizontal,
so w1 = 0. According to model H2, w1 is a parameter with prior distribu-
tion Normal(0; 1). Both models assign a prior distribution Normal(0; 1)
to w0. Given the data set D = f((cid:0)8; 8); ((cid:0)2; 10); (6; 11)g, and assuming
the noise level is (cid:27)(cid:23) = 1, what is the evidence for each model?

Exercise 28.3.[3 ] A six-sided die is rolled 30 times and the numbers of times
each face came up were F = f3; 3; 2; 2; 9; 11g. What is the probability
that the die is a perfectly fair die (‘H0’), assuming the alternative hy-
pothesis H1 says that the die has a biased distribution p, and the prior
density for p is uniform over the simplex pi (cid:21) 0, Pi pi = 1?

Solve this problem two ways: exactly, using the helpful Dirichlet formu-
lae (23.30, 23.31), and approximately, using Laplace’s method. Notice
that your choice of basis for the Laplace approximation is important.
See MacKay (1998a) for discussion of this exercise.

Exercise 28.4.[3 ] The in(cid:13)uence of race on the imposition of the death penalty
for murder in America has been much studied. The following three-way
table classi(cid:12)es 326 cases in which the defendant was convicted of mur-
der. The three variables are the defendant’s race, the victim’s race, and
whether the defendant was sentenced to death. (Data from M. Radelet,
‘Racial characteristics and imposition of the death penalty,’ American
Sociological Review, 46 (1981), pp. 918-927.)

White defendant

Black defendant

Death penalty
Yes

No

Death penalty
Yes

No

White victim 19
Black victim 0

132
9

White victim 11
Black victim 6

52
97

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

28.4: Exercises

355

It seems that the death penalty was applied much more often when the
victim was white then when the victim was black. When the victim was
white 14% of defendants got the death penalty, but when the victim was
black 6% of defendants got the death penalty. [Incidentally, these data
provide an example of a phenomenon known as Simpson’s paradox: a
higher fraction of white defendants are sentenced to death overall, but
in cases involving black victims a higher fraction of black defendants are
sentenced to death and in cases involving white victims a higher fraction
of black defendants are sentenced to death.]

Quantify the evidence for the four alternative hypotheses shown in (cid:12)g-
ure 28.9. I should mention that I don’t believe any of these models is
adequate: several additional variables are important in murder cases,
such as whether the victim and murderer knew each other, whether the
murder was premeditated, and whether the defendant had a prior crim-
inal record; none of these variables is included in the table. So this is
an academic exercise in model comparison rather than a serious study
of racial bias in the state of Florida.

The hypotheses are shown as graphical models, with arrows showing
dependencies between the variables v (victim race), m (murderer race),
and d (whether death penalty given). Model H00 has only one free
parameter, the probability of receiving the death penalty; model H11 has
four such parameters, one for each state of the variables v and m. Assign
uniform priors to these variables. How sensitive are the conclusions to
the choice of prior?

H11
H11

H10
H10

v
v

m
m

v
v

m
m

d
d

H01
H01

d
d

H00
H00

v
v

m
m

v
v

m
m

d
d

d
d

Figure 28.9. Four hypotheses
concerning the dependence of the
imposition of the death penalty d
on the race of the victim v and
the race of the convicted murderer
m. H01, for example, asserts that
the probability of receiving the
death penalty does depend on the
murderer’s race, but not on the
victim’s.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 29

The last couple of chapters have assumed that a Gaussian approximation to
the probability distribution we are interested in is adequate. What if it is not?
We have already seen an example { clustering { where the likelihood function
is multimodal, and has nasty unboundedly-high spikes in certain locations in
the parameter space; so maximizing the posterior probability and (cid:12)tting a
Gaussian is not always going to work. This di(cid:14)culty with Laplace’s method is
one motivation for being interested in Monte Carlo methods. In fact, Monte
Carlo methods provide a general-purpose set of tools with applications in
Bayesian data modelling and many other (cid:12)elds.

This chapter describes a sequence of methods:

importance sampling, re-
jection sampling, the Metropolis method, Gibbs sampling and slice sampling.
For each method, we discuss whether the method is expected to be useful for
high-dimensional problems such as arise in inference with graphical models.
[A graphical model is a probabilistic model in which dependencies and inde-
pendencies of variables are represented by edges in a graph whose nodes are
the variables.] Along the way, the terminology of Markov chain Monte Carlo
methods is presented. The subsequent chapter discusses advanced methods
for reducing random walk behaviour.

For details of Monte Carlo methods, theorems and proofs and a full list
of references, the reader is directed to Neal (1993b), Gilks et al. (1996), and
Tanner (1996).

In this chapter I will use the word ‘sample’ in the following sense: a sample
from a distribution P (x) is a single realization x whose probability distribution
is P (x). This contrasts with the alternative usage in statistics, where ‘sample’
refers to a collection of realizations fxg.
cation convention: I like my matrices to act to the right, preferring

When we discuss transition probability matrices, I will use a right-multipli-

u = Mv

(29.1)

to

uT = vTMT:

(29.2)
A transition probability matrix Tij or Tijj speci(cid:12)es the probability, given the
current state is j, of making the transition from j to i. The columns of T are
probability vectors. If we write down a transition probability density, we use
the same convention for the order of its arguments: T (x0; x) is a transition
probability density from x to x0. This unfortunately means that you have
to get used to reading from right to left { the sequence xyz has probability
T (z; y)T (y; x)(cid:25)(x).

356

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29

Monte Carlo Methods

29.1 The problems to be solved

Monte Carlo methods are computational techniques that make use of random
numbers. The aims of Monte Carlo methods are to solve one or both of the
following problems.
Problem 1: to generate samples fx(r)gR

r=1 from a given probability distribu-

tion P (x).

Problem 2: to estimate expectations of functions under this distribution, for

example

(cid:8) = h(cid:30)(x)i (cid:17)Z dN x P (x)(cid:30)(x):

(29.3)

The probability distribution P (x), which we call the target density, might
be a distribution from statistical physics or a conditional distribution arising
in data modelling { for example, the posterior probability of a model’s pa-
rameters given some observed data. We will generally assume that x is an
N -dimensional vector with real components xn, but we will sometimes con-
sider discrete spaces also.

Simple examples of functions (cid:30)(x) whose expectations we might be inter-
ested in include the (cid:12)rst and second moments of quantities that we wish to
predict, from which we can compute means and variances; for example if some
quantity t depends on x, we can (cid:12)nd the mean and variance of t under P (x)
by (cid:12)nding the expectations of the functions (cid:30)1(x) = t(x) and (cid:30)2(x) = (t(x))2,

then using

(cid:8)1 (cid:17) E[(cid:30)1(x)] and (cid:8)2 (cid:17) E[(cid:30)2(x)];

(cid:22)t = (cid:8)1 and var(t) = (cid:8)2 (cid:0) (cid:8)2
1:

(29.4)

(29.5)

It is assumed that P (x) is su(cid:14)ciently complex that we cannot evaluate these
expectations by exact methods; so we are interested in Monte Carlo methods.
We will concentrate on the (cid:12)rst problem (sampling), because if we have
solved it, then we can solve the second problem by using the random samples
fx(r)gR

r=1 to give the estimator

^(cid:8) (cid:17)

(cid:30)(x(r)):

(29.6)

1

RXr

r=1 are generated from P (x) then the expectation of ^(cid:8) is
If the vectors fx(r)gR
(cid:8). Also, as the number of samples R increases, the variance of ^(cid:8) will decrease
as (cid:27)2/R, where (cid:27)2 is the variance of (cid:30),

(cid:27)2 =Z dN x P (x)((cid:30)(x) (cid:0) (cid:8))2:

357

(29.7)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29 | Monte Carlo Methods

Figure 29.1. (a) The function
P (cid:3)(x) =

exp(cid:2)0:4(x (cid:0) 0:4)2 (cid:0) 0:08x4(cid:3). How

to draw samples from this
density? (b) The function P (cid:3)(x)
evaluated at a discrete set of
uniformly spaced points fxig.
How to draw samples from this
discrete distribution?

358

3

2.5

2

1.5

1

0.5

0

(a)

P*(x)

-4

-2

0

2

4

3

2.5

2

1.5

1

0.5

0

(b)

P*(x)

-4

-2

0

2

4

This is one of the important properties of Monte Carlo methods.

The accuracy of the Monte Carlo estimate (29.6) depends only on
the variance of (cid:30), not on the dimensionality of the space sampled.
To be precise, the variance of ^(cid:8) goes as (cid:27)2=R. So regardless of the
dimensionality of x, it may be that as few as a dozen independent
samples fx(r)g su(cid:14)ce to estimate (cid:8) satisfactorily.

We will (cid:12)nd later, however, that high dimensionality can cause other di(cid:14)-
culties for Monte Carlo methods. Obtaining independent samples from a given
distribution P (x) is often not easy.

Why is sampling from P (x) hard?

We will assume that the density from which we wish to draw samples, P (x),
can be evaluated, at least to within a multiplicative constant; that is, we can
evaluate a function P (cid:3)(x) such that

P (x) = P (cid:3)(x)=Z:

(29.8)

If we can evaluate P (cid:3)(x), why can we not easily solve problem 1? Why is it in
general di(cid:14)cult to obtain samples from P (x)? There are two di(cid:14)culties. The
(cid:12)rst is that we typically do not know the normalizing constant

Z =Z dN x P (cid:3)(x):

(29.9)

The second is that, even if we did know Z, the problem of drawing samples
from P (x) is still a challenging one, especially in high-dimensional spaces,
because there is no obvious way to sample from P without enumerating most
or all of the possible states. Correct samples from P will by de(cid:12)nition tend
to come from places in x-space where P (x) is big; how can we identify those
places where P (x) is big, without evaluating P (x) everywhere? There are only
a few high-dimensional densities from which it is easy to draw samples, for
example the Gaussian distribution.

Let us start with a simple one-dimensional example. Imagine that we wish

to draw samples from the density P (x) = P (cid:3)(x)=Z where

P (cid:3)(x) = exp(cid:2)0:4(x (cid:0) 0:4)2 (cid:0) 0:08x4(cid:3) ; x 2 ((cid:0)1;1):

We can plot this function ((cid:12)gure 29.1a). But that does not mean we can draw
samples from it. To start with, we don’t know the normalizing constant Z.

(29.10)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.1: The problems to be solved

359

To give ourselves a simpler problem, we could discretize the variable x and
ask for samples from the discrete probability distribution over a (cid:12)nite set of
uniformly spaced points fxig ((cid:12)gure 29.1b). How could we solve this problem?
If we evaluate p(cid:3)i = P (cid:3)(xi) at each point xi, we can compute

p(cid:3)i

Z =Xi

(29.11)

and

pi = p(cid:3)i =Z

(29.12)
and we can then sample from the probability distribution fpig using various
methods based on a source of random bits (see section 6.3). But what is the
cost of this procedure, and how does it scale with the dimensionality of the
space, N ? Let us concentrate on the initial cost of evaluating Z (29.11). To
compute Z we have to visit every point in the space. In (cid:12)gure 29.1b there are
50 uniformly spaced points in one dimension. If our system had N dimensions,
N = 1000 say, then the corresponding number of points would be 501000, an
unimaginable number of evaluations of P (cid:3). Even if each component xn took
only two discrete values, the number of evaluations of P (cid:3) would be 21000, a
number that is still horribly huge. If every electron in the universe (there are
about 2266 of them) were a 1000 gigahertz computer that could evaluate P (cid:3)
for a trillion (240) states every second, and if we ran those 2266 computers for
a time equal to the age of the universe (258 seconds), they would still only
visit 2364 states. We’d have to wait for more than 2636 ’ 10190 universe ages
to elapse before all 21000 states had been visited.
Systems with 21000 states are two a penny.? One example is a collection
of 1000 spins such as a 30 (cid:2) 30 fragment of an Ising model whose probability
distribution is proportional to

P (cid:3)(x) = exp[(cid:0)(cid:12)E(x)]

where xn 2 f(cid:6)1g and

E(x) = (cid:0)" 1

2Xm;n

Jmnxmxn +Xn

Hnxn# :

(29.13)

(29.14)

The energy function E(x) is readily evaluated for any x. But if we wish to
evaluate this function at all states x, the computer time required would be
21000 function evaluations.

The Ising model is a simple model which has been around for a long time,
but the task of generating samples from the distribution P (x) = P (cid:3)(x)=Z is
still an active research area; the (cid:12)rst ‘exact’ samples from this distribution
were created in the pioneering work of Propp and Wilson (1996), as we’ll
describe in Chapter 32.

A useful analogy

Imagine the tasks of drawing random water samples from a lake and (cid:12)nding
the average plankton concentration ((cid:12)gure 29.2). The depth of the lake at
x = (x; y) is P (cid:3)(x), and we assert (in order to make the analogy work) that
the plankton concentration is a function of x, (cid:30)(x). The required average
concentration is an integral like (29.3), namely

(cid:8) = h(cid:30)(x)i (cid:17)

1

Z Z dN x P (cid:3)(x)(cid:30)(x);

(29.15)

? Translation for American
readers: ‘such systems are a dime
a dozen’; incidentally, this
equivalence (10c = 6p) shows that
the correct exchange rate between
our currencies is $1.00 = $1.67.

P (cid:3)(x)

Figure 29.2. A lake whose depth
at x = (x; y) is P (cid:3)(x).

 
 
 
 
 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

360

29 | Monte Carlo Methods

where Z = R dx dy P (cid:3)(x) is the volume of the lake. You are provided with a

boat, a satellite navigation system, and a plumbline. Using the navigator, you
can take your boat to any desired location x on the map; using the plumbline
you can measure P (cid:3)(x) at that point. You can also measure the plankton
concentration there.

Problem 1 is to draw 1 cm3 water samples at random from the lake, in
such a way that each sample is equally likely to come from any point within
the lake. Problem 2 is to (cid:12)nd the average plankton concentration.

These are di(cid:14)cult problems to solve because at the outset we know nothing
about the depth P (cid:3)(x). Perhaps much of the volume of the lake is contained
in narrow, deep underwater canyons ((cid:12)gure 29.3), in which case, to correctly
sample from the lake and correctly estimate (cid:8) our method must implicitly
discover the canyons and (cid:12)nd their volume relative to the rest of the lake.
Di(cid:14)cult problems, yes; nevertheless, we’ll see that clever Monte Carlo methods
can solve them.

Uniform sampling

Having accepted that we cannot exhaustively visit every location x in the
state space, we might consider trying to solve the second problem (estimating
the expectation of a function (cid:30)(x)) by drawing random samples fx(r)gR
r=1
uniformly from the state space and evaluating P (cid:3)(x) at those points. Then
we could introduce a normalizing constant ZR, de(cid:12)ned by

Figure 29.3. A slice through a lake
that includes some canyons.

P (cid:3)(x(r));

(29.16)

R

ZR =

Xr=1
and estimate (cid:8) =R dN x (cid:30)(x)P (x) by
Xr=1

R

^(cid:8) =

(cid:30)(x(r))

P (cid:3)(x(r))

ZR

:

(29.17)

Is anything wrong with this strategy? Well, it depends on the functions (cid:30)(x)
and P (cid:3)(x). Let us assume that (cid:30)(x) is a benign, smoothly varying function
and concentrate on the nature of P (cid:3)(x). As we learnt in Chapter 4, a high-
dimensional distribution is often concentrated in a small region of the state
space known as its typical set T , whose volume is given by jTj ’ 2H(X), where
H(X) is the entropy of the probability distribution P (x). If almost all the
probability mass is located in the typical set and (cid:30)(x) is a benign function,
the value of (cid:8) =R dN x (cid:30)(x)P (x) will be principally determined by the values

that (cid:30)(x) takes on in the typical set. So uniform sampling will only stand
a chance of giving a good estimate of (cid:8) if we make the number of samples
R su(cid:14)ciently large that we are likely to hit the typical set at least once or
twice. So, how many samples are required? Let us take the case of the Ising
model again. (Strictly, the Ising model may not be a good example, since it
doesn’t necessarily have a typical set, as de(cid:12)ned in Chapter 4; the de(cid:12)nition
of a typical set was that all states had log probability close to the entropy,
which for an Ising model would mean that the energy is very close to the
mean energy; but in the vicinity of phase transitions, the variance of energy,
also known as the heat capacity, may diverge, which means that the energy
of a random state is not necessarily expected to be very close to the mean
energy.) The total size of the state space is 2N states, and the typical set has
size 2H. So each sample has a chance of 2H =2N of falling in the typical set.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.2: Importance sampling

361

64 log(2)

y
p
o
r
t
n
E

Figure 29.4. (a) Entropy of a
64-spin Ising model as a function
of temperature. (b) One state of a
1024-spin Ising model.

0

0

(a)

1

2

3

Temperature

4

5

6

(b)

The number of samples required to hit the typical set once is thus of order

Rmin ’ 2N(cid:0)H:

(29.18)

So, what is H? At high temperatures, the probability distribution of an Ising
model tends to a uniform distribution and the entropy tends to Hmax = N
bits, which means Rmin is of order 1. Under these conditions, uniform sampling
may well be a satisfactory technique for estimating (cid:8). But high temperatures
are not of great interest. Considerably more interesting are intermediate tem-
peratures such as the critical temperature at which the Ising model melts from
an ordered phase to a disordered phase. The critical temperature of an in(cid:12)nite
Ising model, at which it melts, is (cid:18)c = 2:27. At this temperature the entropy
of an Ising model is roughly N=2 bits ((cid:12)gure 29.4). For this probability dis-
tribution the number of samples required simply to hit the typical set once is
of order

Rmin ’ 2N(cid:0)N=2 = 2N=2;

(29.19)

which for N = 1000 is about 10150. This is roughly the square of the number
of particles in the universe. Thus uniform sampling is utterly useless for the
study of Ising models of modest size. And in most high-dimensional problems,
if the distribution P (x) is not actually uniform, uniform sampling is unlikely
to be useful.

Overview

Having established that drawing samples from a high-dimensional distribution
P (x) = P (cid:3)(x)=Z is di(cid:14)cult even if P (cid:3)(x) is easy to evaluate, we will now
study a sequence of more sophisticated Monte Carlo methods:
importance
sampling, rejection sampling, the Metropolis method, Gibbs sampling, and
slice sampling.

29.2 Importance sampling

Importance sampling is not a method for generating samples from P (x) (prob-
lem 1); it is just a method for estimating the expectation of a function (cid:30)(x)
(problem 2).
It can be viewed as a generalization of the uniform sampling
method.

For illustrative purposes, let us imagine that the target distribution is a
one-dimensional density P (x). Let us assume that we are able to evaluate this
density at any chosen point x, at least to within a multiplicative constant;
thus we can evaluate a function P (cid:3)(x) such that

P (x) = P (cid:3)(x)=Z:

(29.20)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

362

29 | Monte Carlo Methods

But P (x) is too complicated a function for us to be able to sample from it
directly. We now assume that we have a simpler density Q(x) from which we
can generate samples and which we can evaluate to within a multiplicative
constant (that is, we can evaluate Q(cid:3)(x), where Q(x) = Q(cid:3)(x)=ZQ). An
example of the functions P (cid:3), Q(cid:3) and (cid:30) is shown in (cid:12)gure 29.5. We call Q the
sampler density.

In importance sampling, we generate R samples fx(r)gR

r=1 from Q(x). If
these points were samples from P (x) then we could estimate (cid:8) by equa-
tion (29.6). But when we generate samples from Q, values of x where Q(x) is
greater than P (x) will be over-represented in this estimator, and points where
Q(x) is less than P (x) will be under-represented. To take into account the
fact that we have sampled from the wrong distribution, we introduce weights

wr (cid:17)

P (cid:3)(x(r))
Q(cid:3)(x(r))

(29.21)

which we use to adjust the ‘importance’ of each point in our estimator thus:

^(cid:8) (cid:17) Pr wr(cid:30)(x(r))
Pr wr

:

(29.22)

. Exercise 29.1.[2, p.384] Prove that, if Q(x) is non-zero for all x where P (x) is
non-zero, the estimator ^(cid:8) converges to (cid:8), the mean value of (cid:30)(x), as R
increases. What is the variance of this estimator, asymptotically? Hint:
consider the statistics of the numerator and the denominator separately.
Is the estimator ^(cid:8) an unbiased estimator for small R?

A practical di(cid:14)culty with importance sampling is that it is hard to estimate
how reliable the estimator ^(cid:8) is. The variance of the estimator is unknown
beforehand, because it depends on an integral over x of a function involving
P (cid:3)(x). And the variance of ^(cid:8) is hard to estimate, because the empirical
variances of the quantities wr and wr(cid:30)(x(r)) are not necessarily a good guide
to the true variances of the numerator and denominator in equation (29.22).
If the proposal density Q(x) is small in a region where j(cid:30)(x)P (cid:3)(x)j is large
then it is quite possible, even after many points x(r) have been generated, that
none of them will have fallen in that region. In this case the estimate of (cid:8)
would be drastically wrong, and there would be no indication in the empirical
variance that the true variance of the estimator ^(cid:8) is large.

-6.2

-6.4

-6.6

-6.8

-7

-7.2

(a)

-6.2

-6.4

-6.6

-6.8

-7

(b)

-7.2

10

100

1000

10000

100000 1000000

10

100

1000

10000

100000 1000000

Cautionary illustration of importance sampling

In a toy problem related to the modelling of amino acid probability distribu-
tions with a one-dimensional variable x, I evaluated a quantity of interest us-
ing importance sampling. The results using a Gaussian sampler and a Cauchy
sampler are shown in (cid:12)gure 29.6. The horizontal axis shows the number of

P (cid:3)(x)

Q(cid:3)(x)

(cid:30)(x)

x

Figure 29.5. Functions involved in
importance sampling. We wish to
estimate the expectation of (cid:30)(x)
under P (x) / P (cid:3)(x). We can
generate samples from the simpler
distribution Q(x) / Q(cid:3)(x). We
can evaluate Q(cid:3) and P (cid:3) at any
point.

Figure 29.6. Importance sampling
in action: (a) using a Gaussian
sampler density; (b) using a
Cauchy sampler density. Vertical
axis shows the estimate ^(cid:8). The
horizontal line indicates the true
value of (cid:8). Horizontal axis shows
number of samples on a log scale.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

363

P(x)
Q(x)
phi(x)

-5

0

5

10

15

Figure 29.7. A multimodal
distribution P (cid:3)(x) and a unimodal
sampler Q(x).

29.2: Importance sampling

samples on a log scale. In the case of the Gaussian sampler, after about 500
samples had been evaluated one might be tempted to call a halt; but evidently
there are infrequent samples that make a huge contribution to ^(cid:8), and the value
of the estimate at 500 samples is wrong. Even after a million samples have
been taken, the estimate has still not settled down close to the true value. In
contrast, the Cauchy sampler does not su(cid:11)er from glitches; it converges (on
the scale shown here) after about 5000 samples.

This example illustrates the fact that an importance sampler should have

heavy tails.

Exercise 29.2.[2, p.385] Consider the situation where P (cid:3)(x) is multimodal, con-
sisting of several widely-separated peaks. (Probability distributions like
this arise frequently in statistical data modelling.) Discuss whether it is
a wise strategy to do importance sampling using a sampler Q(x) that
is a unimodal distribution (cid:12)tted to one of these peaks. Assume that
the function (cid:30)(x) whose mean (cid:8) is to be estimated is a smoothly vary-
ing function of x such as mx + c. Describe the typical evolution of the
estimator ^(cid:8) as a function of the number of samples R.

Importance sampling in many dimensions

We have already observed that care is needed in one-dimensional importance
sampling problems. Is importance sampling a useful technique in spaces of
higher dimensionality, say N = 1000?

Consider a simple case-study where the target density P (x) is a uniform

distribution inside a sphere,

where (cid:26)(x) (cid:17) (Pi x2

the origin,

P (cid:3)(x) =(cid:26) 1 0 (cid:20) (cid:26)(x) (cid:20) RP

0 (cid:26)(x) > RP ;

(29.23)

i )1=2, and the proposal density is a Gaussian centred on

Q(x) =Yi

Normal(xi; 0; (cid:27)2):

(29.24)

An importance-sampling method will be in trouble if the estimator ^(cid:8) is dom-
inated by a few large weights wr. What will be the typical range of values of
the weights wr? We know from our discussions of typical sequences in Part I {
see exercise 6.14 (p.124), for example { that if (cid:26) is the distance from the origin
of a sample from Q, the quantity (cid:26)2 has a roughly Gaussian distribution with
mean and standard deviation:

(cid:26)2 (cid:24) N (cid:27)2 (cid:6)

p2N (cid:27)2:

(29.25)

Thus almost all samples from Q lie in a typical set with distance from the origin
very close to pN (cid:27). Let us assume that (cid:27) is chosen such that the typical set
of Q lies inside the sphere of radius RP . [If it does not, then the law of large
numbers implies that almost all the samples generated from Q will fall outside
RP and will have weight zero.] Then we know that most samples from Q will
have a value of Q that lies in the range

1

(2(cid:25)(cid:27)2)N=2

exp (cid:0)

N
2 (cid:6)

p2N
2 ! :

Thus the weights wr = P (cid:3)=Q will typically have values in the range

(2(cid:25)(cid:27)2)N=2 exp  N
2 (cid:6)

p2N
2 ! :

(29.26)

(29.27)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

364

(a)

P (cid:3)(x)

cQ(cid:3)(x)

(b)

cQ(cid:3)(x)

P (cid:3)(x)

u

x

x

x

So if we draw a hundred samples, what will the typical range of weights be?
We can roughly estimate the ratio of the largest weight to the median weight
by doubling the standard deviation in equation (29.27). The largest weight
and the median weight will typically be in the ratio:

29 | Monte Carlo Methods

Figure 29.8. Rejection sampling.
(a) The functions involved in
rejection sampling. We desire
samples from P (x) / P (cid:3)(x). We
are able to draw samples from
Q(x) / Q(cid:3)(x), and we know a
value c such that c Q(cid:3)(x) > P (cid:3)(x)
for all x. (b) A point (x; u) is
generated at random in the lightly
shaded area under the curve
c Q(cid:3)(x). If this point also lies
below P (cid:3)(x) then it is accepted.

r

wmax
wmed

r

= exp(cid:16)p2N(cid:17) :

(29.28)

In N = 1000 dimensions therefore, the largest weight after one hundred sam-
ples is likely to be roughly 1019 times greater than the median weight. Thus an
importance sampling estimate for a high-dimensional problem will very likely
be utterly dominated by a few samples with huge weights.

In conclusion, importance sampling in high dimensions often su(cid:11)ers from
two di(cid:14)culties. First, we need to obtain samples that lie in the typical set of P ,
and this may take a long time unless Q is a good approximation to P . Second,
even if we obtain samples in the typical set, the weights associated with those
samples are likely to vary by large factors, because the probabilities of points
in a typical set, although similar to each other, still di(cid:11)er by factors of order
exp(pN ), so the weights will too, unless Q is a near-perfect approximation to
P .

29.3 Rejection sampling

We assume again a one-dimensional density P (x) = P (cid:3)(x)=Z that is too com-
plicated a function for us to be able to sample from it directly. We assume
that we have a simpler proposal density Q(x) which we can evaluate (within a
multiplicative factor ZQ, as before), and from which we can generate samples.
We further assume that we know the value of a constant c such that

c Q(cid:3)(x) > P (cid:3)(x);

for all x:

(29.29)

A schematic picture of the two functions is shown in (cid:12)gure 29.8a.

We generate two random numbers. The (cid:12)rst, x, is generated from the
proposal density Q(x). We then evaluate c Q(cid:3)(x) and generate a uniformly
distributed random variable u from the interval [0; c Q(cid:3)(x)]. These two random
numbers can be viewed as selecting a point in the two-dimensional plane as
shown in (cid:12)gure 29.8b.

We now evaluate P (cid:3)(x) and accept or reject the sample x by comparing the
value of u with the value of P (cid:3)(x). If u > P (cid:3)(x) then x is rejected; otherwise
it is accepted, which means that we add x to our set of samples fx(r)g. The
value of u is discarded.
Why does this procedure generate samples from P (x)? The proposed point
(x; u) comes with uniform probability from the lightly shaded area underneath
the curve c Q(cid:3)(x) as shown in (cid:12)gure 29.8b. The rejection rule rejects all the
points that lie above the curve P (cid:3)(x). So the points (x; u) that are accepted
are uniformly distributed in the heavily shaded area under P (cid:3)(x). This implies

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.4: The Metropolis{Hastings method

that the probability density of the x-coordinates of the accepted points must
be proportional to P (cid:3)(x), so the samples must be independent samples from
P (x).

Rejection sampling will work best if Q is a good approximation to P . If Q
is very di(cid:11)erent from P then, for c Q to exceed P everywhere, c will necessarily
have to be large and the frequency of rejection will be large.

Rejection sampling in many dimensions

In a high-dimensional problem it is very likely that the requirement that c Q(cid:3)
be an upper bound for P (cid:3) will force c to be so huge that acceptances will be
very rare indeed. Finding such a value of c may be di(cid:14)cult too, since in many
problems we know neither where the modes of P (cid:3) are located nor how high
they are.

As a case study, consider a pair of N -dimensional Gaussian distributions
with mean zero ((cid:12)gure 29.9). Imagine generating samples from one with stan-
dard deviation (cid:27)Q and using rejection sampling to obtain samples from the
other whose standard deviation is (cid:27)P . Let us assume that these two standard
deviations are close in value { say, (cid:27)Q is 1% larger than (cid:27)P . [(cid:27)Q must be larger
than (cid:27)P because if this is not the case, there is no c such that c Q exceeds P
for all x.] So, what value of c is required if the dimensionality is N = 1000?
The density of Q(x) at the origin is 1=(2(cid:25)(cid:27)2
Q)N=2, so for c Q to exceed P we
need to set

c =

(2(cid:25)(cid:27)2
(2(cid:25)(cid:27)2

Q)N=2
P )N=2

= exp(cid:18)N ln

(cid:27)Q

(cid:27)P(cid:19) :

(29.30)

With N = 1000 and (cid:27)Q
= 1:01, we (cid:12)nd c = exp(10) ’ 20,000. What will the
(cid:27)P
acceptance rate be for this value of c? The answer is immediate: since the
acceptance rate is the ratio of the volume under the curve P (x) to the volume
under c Q(x), the fact that P and Q are both normalized here implies that
the acceptance rate will be 1=c, for example, 1/20,000.
In general, c grows
exponentially with the dimensionality N , so the acceptance rate is expected
to be exponentially small in N .

Rejection sampling, therefore, whilst a useful method for one-dimensional
problems, is not expected to be a practical technique for generating samples
from high-dimensional distributions P (x).

29.4 The Metropolis{Hastings method

Importance sampling and rejection sampling work well only if the proposal
density Q(x) is similar to P (x). In large and complex problems it is di(cid:14)cult
to create a single density Q(x) that has this property.

The Metropolis{Hastings algorithm instead makes use of a proposal den-
sity Q which depends on the current state x(t). The density Q(x0; x(t)) might
be a simple distribution such as a Gaussian centred on the current x(t). The
proposal density Q(x0; x) can be any (cid:12)xed density from which we can draw
samples. In contrast to importance sampling and rejection sampling, it is not
necessary that Q(x0; x(t)) look at all similar to P (x) in order for the algorithm
to be practically useful. An example of a proposal density is shown in (cid:12)g-
ure 29.10; this (cid:12)gure shows the density Q(x0; x(t)) for two di(cid:11)erent states x(1)
and x(2).

As before, we assume that we can evaluate P (cid:3)(x) for any x. A tentative
new state x0 is generated from the proposal density Q(x0; x(t)). To decide

365

P(x)
cQ(x)

-4

-3

-2

-1

0

1

2

3

4

Figure 29.9. A Gaussian P (x) and
a slightly broader Gaussian Q(x)
scaled up by a factor c such that
c Q(x) (cid:21) P (x).

Q(x; x(1))

x(1)

P (cid:3)(x)

P (cid:3)(x)

x

Q(x; x(2))

x(2)

x

Figure 29.10. Metropolis{Hastings
method in one dimension. The
proposal distribution Q(x0; x) is
here shown as having a shape that
changes as x changes, though this
is not typical of the proposal
densities used in practice.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

366

29 | Monte Carlo Methods

whether to accept the new state, we compute the quantity

a =

:

P (cid:3)(x0)
P (cid:3)(x(t))

Q(x(t); x0)
Q(x0; x(t))
If a (cid:21) 1 then the new state is accepted.
Otherwise, the new state is accepted with probability a.

(29.31)

If the step is accepted, we set x(t+1) = x0.
If the step is rejected, then we set x(t+1) = x(t).

Note the di(cid:11)erence from rejection sampling:
in rejection sampling, rejected
points are discarded and have no in(cid:13)uence on the list of samples fx(r)g that
we collected. Here, a rejection causes the current state to be written again
onto the list.
Notation.

I have used the superscript r = 1; : : : ; R to label points that
are independent samples from a distribution, and the superscript t = 1; : : : ; T
to label the sequence of states in a Markov chain. It is important to note that
a Metropolis{Hastings simulation of T iterations does not produce T indepen-
dent samples from the target distribution P . The samples are dependent.

To compute the acceptance probability (29.31) we need to be able to com-
pute the probability ratios P (x0)=P (x(t)) and Q(x(t); x0)=Q(x0; x(t)).
If the
proposal density is a simple symmetrical density such as a Gaussian centred on
the current point, then the latter factor is unity, and the Metropolis{Hastings
method simply involves comparing the value of the target density at the two
points. This special case is sometimes called the Metropolis method. How-
ever, with apologies to Hastings, I will call the general Metropolis{Hastings
algorithm for asymmetric Q ‘the Metropolis method’ since I believe important
ideas deserve short names.

Convergence of the Metropolis method to the target density

It can be shown that for any positive Q (that is, any Q such that Q(x0; x) > 0
for all x; x0), as t ! 1, the probability distribution of x(t) tends to P (x) =
P (cid:3)(x)=Z. [This statement should not be seen as implying that Q has to assign
positive probability to every point x0 { we will discuss examples later where
Q(x0; x) = 0 for some x; x0; notice also that we have said nothing about how
rapidly the convergence to P (x) takes place.]

The Metropolis method is an example of a Markov chain Monte Carlo
method (abbreviated MCMC). In contrast to rejection sampling, where the
accepted points fx(r)g are independent samples from the desired distribution,
Markov chain Monte Carlo methods involve a Markov process in which a se-
quence of states fx(t)g is generated, each sample x(t) having a probability
distribution that depends on the previous value, x(t(cid:0)1). Since successive sam-
ples are dependent, the Markov chain may have to be run for a considerable
time in order to generate samples that are e(cid:11)ectively independent samples
from P .

Just as it was di(cid:14)cult to estimate the variance of an importance sampling
estimator, so it is di(cid:14)cult to assess whether a Markov chain Monte Carlo
method has ‘converged’, and to quantify how long one has to wait to obtain
samples that are e(cid:11)ectively independent samples from P .

Demonstration of the Metropolis method

The Metropolis method is widely used for high-dimensional problems. Many
implementations of the Metropolis method employ a proposal distribution

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.4: The Metropolis{Hastings method

367

(cid:15)

x(1)

Q(x; x(1))

P (cid:3)(x)

L

Figure 29.11. Metropolis method
in two dimensions, showing a
traditional proposal density that
has a su(cid:14)ciently small step size (cid:15)
that the acceptance frequency will
be about 0.5.

with a length scale (cid:15) that is short relative to the longest length scale L of the
probable region ((cid:12)gure 29.11). A reason for choosing a small length scale is
that for most high-dimensional problems, a large random step from a typical
point (that is, a sample from P (x)) is very likely to end in a state that has
very low probability; such steps are unlikely to be accepted.
If (cid:15) is large,
movement around the state space will only occur when such a transition to a
low-probability state is actually accepted, or when a large random step chances
to land in another probable state. So the rate of progress will be slow if large
steps are used.

The disadvantage of small steps, on the other hand, is that the Metropolis
method will explore the probability distribution by a random walk, and a
random walk takes a long time to get anywhere, especially if the walk is made
of small steps.
Exercise 29.3.[1 ] Consider a one-dimensional random walk, on each step of
which the state moves randomly to the left or to the right with equal
probability. Show that after T steps of size (cid:15), the state is likely to have
moved only a distance about pT (cid:15).
(Compute the root mean square
distance travelled.)

Recall that the (cid:12)rst aim of Monte Carlo sampling is to generate a number of
independent samples from the given distribution (a dozen, say). If the largest
length scale of the state space is L, then we have to simulate a random-walk
Metropolis method for a time T ’ (L=(cid:15))2 before we can expect to get a sample
that is roughly independent of the initial condition { and that’s assuming that
every step is accepted: if only a fraction f of the steps are accepted on average,
then this time is increased by a factor 1=f .

Rule of thumb: lower bound on number of iterations of a
Metropolis method. If the largest length scale of the space of
probable states is L, a Metropolis method whose proposal distribu-
tion generates a random walk with step size (cid:15) must be run for at
least

T ’ (L=(cid:15))2

(29.32)

iterations to obtain an independent sample.

This rule of thumb gives only a lower bound; the situation may be much
worse, if, for example, the probability distribution consists of several islands
of high probability separated by regions of low probability.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

368

29 | Monte Carlo Methods

Figure 29.12. Metropolis method
for a toy problem. (a) The state
sequence for t = 1; : : : ; 600.
Horizontal direction = states from
0 to 20; vertical direction = time
from 1 to 600; the cross bars mark
time intervals of duration 50. (b)
Histogram of occupancy of the
states after 100, 400, and 1200
iterations. (c) For comparison,
histograms resulting when
successive points are drawn
independently from the target
distribution.

(a)

(b) Metropolis

(c) Independent sampling

100 iterations

100 iterations

12

10

8

6

4

2

0

40

35

30

25

20

15

10

5

0

90
80
70
60
50
40
30
20
10
0

0

5

10

15

20

400 iterations

0

5

10

15

20

1200 iterations

0

5

10

15

20

12

10

8

6

4

2

0

40

35

30

25

20

15

10

5

0

90
80
70
60
50
40
30
20
10
0

0

5

10

15

20

400 iterations

0

5

10

15

20

1200 iterations

0

5

10

15

20

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.4: The Metropolis{Hastings method

369

To illustrate how slowly a random walk explores a state space, (cid:12)gure 29.12
shows a simulation of a Metropolis algorithm for generating samples from the
distribution:

(29.33)

P (x) =(cid:26) 1/21 x 2 f0; 1; 2; : : : ; 20g

otherwise.

0

The proposal distribution is

0

(29.34)

otherwise.

Q(x0; x) =(cid:26) 1/2 x0 = x (cid:6) 1
Because the target distribution P (x) is uniform, rejections occur only when
the proposal takes the state to x0 = (cid:0)1 or x0 = 21.
The simulation was started in the state x0 = 10 and its evolution is shown
in (cid:12)gure 29.12a. How long does it take to reach one of the end states x = 0
and x = 20? Since the distance is 10 steps, the rule of thumb (29.32) predicts
that it will typically take a time T ’ 100 iterations to reach an end state. This
is con(cid:12)rmed in the present example: the (cid:12)rst step into an end state occurs on
the 178th iteration. How long does it take to visit both end states? The rule
of thumb predicts about 400 iterations are required to traverse the whole state
space; and indeed the (cid:12)rst encounter with the other end state takes place on
the 540th iteration. Thus e(cid:11)ectively-independent samples are generated only
by simulating for about four hundred iterations per independent sample.

This simple example shows that it is important to try to abolish random
walk behaviour in Monte Carlo methods. A systematic exploration of the toy
state space f0; 1; 2; : : : ; 20g could get around it, using the same step sizes, in
about twenty steps instead of four hundred. Methods for reducing random
walk behaviour are discussed in the next chapter.

Metropolis method in high dimensions

The rule of thumb (29.32), which gives a lower bound on the number of itera-
tions of a random walk Metropolis method, also applies to higher-dimensional
problems. Consider the simple case of a target distribution that is an N -
dimensional Gaussian, and a proposal distribution that is a spherical Gaussian
of standard deviation (cid:15) in each direction. Without loss of generality, we can
assume that the target distribution is a separable distribution aligned with the
axes fxng, and that it has standard deviation (cid:27)n in direction n. Let (cid:27)max and
(cid:27)min be the largest and smallest of these standard deviations. Let us assume
that (cid:15) is adjusted such that the acceptance frequency is close to 1. Under this
assumption, each variable xn evolves independently of all the others, executing
a random walk with step size about (cid:15). The time taken to generate e(cid:11)ectively
independent samples from the target distribution will be controlled by the
largest lengthscale (cid:27)max. Just as in the previous section, where we needed at
least T ’ (L=(cid:15))2 iterations to obtain an independent sample, here we need
T ’ ((cid:27)max=(cid:15))2.
Now, how big can (cid:15) be? The bigger it is, the smaller this number T be-
comes, but if (cid:15) is too big { bigger than (cid:27)min { then the acceptance rate will
fall sharply. It seems plausible that the optimal (cid:15) must be similar to (cid:27) min.
Strictly, this may not be true; in special cases where the second smallest (cid:27)n
is signi(cid:12)cantly greater than (cid:27)min, the optimal (cid:15) may be closer to that second
smallest (cid:27)n. But our rough conclusion is this: where simple spherical pro-
posal distributions are used, we will need at least T ’ ((cid:27) max=(cid:27)min)2 iterations
to obtain an independent sample, where (cid:27)max and (cid:27)min are the longest and
shortest lengthscales of the target distribution.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29 | Monte Carlo Methods

Figure 29.13. Gibbs sampling.
(a) The joint density P (x) from
which samples are required. (b)
Starting from a state x(t), x1 is
sampled from the conditional

density P (x1 j x(t)
2 ). (c) A sample
is then made from the conditional
density P (x2 j x1). (d) A couple of
iterations of Gibbs sampling.

370

x2

(a)

x2

(c)

P (x)

x2

(b)

x1

x2

P (x1 j x(t)
2 )

x(t)

x1

x(t+2)

x(t+1)

P (x2 j x1)

(d)

x1

x(t)

x1

This is good news and bad news.

It is good news because, unlike the
cases of rejection sampling and importance sampling, there is no catastrophic
dependence on the dimensionality N . Our computer will give useful answers
in a time shorter than the age of the universe. But it is bad news all the same,
because this quadratic dependence on the lengthscale-ratio may still force us
to make very lengthy simulations.

Fortunately, there are methods for suppressing random walks in Monte

Carlo simulations, which we will discuss in the next chapter.

29.5 Gibbs sampling

We introduced importance sampling, rejection sampling and the Metropolis
method using one-dimensional examples. Gibbs sampling, also known as the
heat bath method or ‘Glauber dynamics’, is a method for sampling from dis-
tributions over at least two dimensions. Gibbs sampling can be viewed as a
Metropolis method in which a sequence of proposal distributions Q are de(cid:12)ned
in terms of the conditional distributions of the joint distribution P (x). It is
assumed that, whilst P (x) is too complex to draw samples from directly, its
conditional distributions P (xi jfxjgj6=i) are tractable to work with. For many
graphical models (but not all) these one-dimensional conditional distributions
are straightforward to sample from. For example, if a Gaussian distribution
for some variables d has an unknown mean m, and the prior distribution of m
is Gaussian, then the conditional distribution of m given d is also Gaussian.
Conditional distributions that are not of standard form may still be sampled
from by adaptive rejection sampling if the conditional distribution satis(cid:12)es
certain convexity properties (Gilks and Wild, 1992).

Gibbs sampling is illustrated for a case with two variables (x1; x2) = x
in (cid:12)gure 29.13. On each iteration, we start from the current state x(t), and
x1 is sampled from the conditional density P (x1 j x2), with x2 (cid:12)xed to x(t)
2 .
A sample x2 is then made from the conditional density P (x2 j x1), using the

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.5: Gibbs sampling

371

new value of x1. This brings us to the new state x(t+1), and completes the
iteration.

In the general case of a system with K variables, a single iteration involves

sampling one parameter at a time:

x(t+1)
1
x(t+1)
2
x(t+1)
3

(cid:24) P (x1 j x(t)
2 ; x(t)
(cid:24) P (x2 j x(t+1)
(cid:24) P (x3 j x(t+1)

1

1

3 ; : : : ; x(t)
K )
; x(t)
3 ; : : : ; x(t)
K )
; x(t+1)
; : : : ; x(t)

2

K ); etc.

(29.35)

(29.36)

(29.37)

Convergence of Gibbs sampling to the target density

. Exercise 29.4.[2 ] Show that a single variable-update of Gibbs sampling can
be viewed as a Metropolis method with target density P (x), and that
this Metropolis method has the property that every proposal is always
accepted.

Because Gibbs sampling is a Metropolis method, the probability distribution
of x(t) tends to P (x) as t ! 1, as long as P (x) does not have pathological
properties.

. Exercise 29.5.[2, p.385] Discuss whether the syndrome decoding problem for a
(7; 4) Hamming code can be solved using Gibbs sampling. The syndrome
decoding problem, if we are to solve it with a Monte Carlo approach,
is to draw samples from the posterior distribution of the noise vector
n = (n1; : : : ; nn; : : : ; nN ),

P (nj f ; z) =

1
Z

N

Yn=1

f nn

n (1 (cid:0) fn)(1(cid:0)nn)  [Hn = z];

(29.38)

where fn is the normalized likelihood for the nth transmitted bit and z
is the observed syndrome. The factor  [Hn = z] is 1 if n has the correct
syndrome z and 0 otherwise.

What about the syndrome decoding problem for any linear error-correcting
code?

Gibbs sampling in high dimensions

Gibbs sampling su(cid:11)ers from the same defect as simple Metropolis algorithms
{ the state space is explored by a slow random walk, unless a fortuitous pa-
rameterization has been chosen that makes the probability distribution P (x)
separable.
If, say, two variables x1 and x2 are strongly correlated, having
marginal densities of width L and conditional densities of width (cid:15), then it will
take at least about (L=(cid:15))2 iterations to generate an independent sample from
the target density. Figure 30.3, p.390, illustrates the slow progress made by
Gibbs sampling when L (cid:29) (cid:15).
However Gibbs sampling involves no adjustable parameters, so it is an at-
tractive strategy when one wants to get a model running quickly. An excellent
software package, BUGS, makes it easy to set up almost arbitrary probabilistic
models and simulate them by Gibbs sampling (Thomas et al., 1992).1

1http://www.mrc-bsu.cam.ac.uk/bugs/

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

372

29 | Monte Carlo Methods

29.6 Terminology for Markov chain Monte Carlo methods

We now spend a few moments sketching the theory on which the Metropolis
method and Gibbs sampling are based. We denote by p(t)(x) the probabil-
ity distribution of the state of a Markov chain simulator. (To visualize this
distribution, imagine running an in(cid:12)nite collection of identical simulators in
parallel.) Our aim is to (cid:12)nd a Markov chain such that as t ! 1, p(t)(x) tends
to the desired distribution P (x).
A Markov chain can be speci(cid:12)ed by an initial probability distribution

p(0)(x) and a transition probability T (x0; x).

The probability distribution of the state at the (t + 1)th iteration of the

Markov chain, p(t+1)(x), is given by

p(t+1)(x0) =Z dN x T (x0; x)p(t)(x):

(29.39)

Example 29.6. An example of a Markov chain is given by the Metropolis
demonstration of section 29.4 ((cid:12)gure 29.12), for which the transition proba-
bility is

T =

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
and the initial distribution was

1/2 1/2 (cid:1)
(cid:1)
(cid:1)
1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1) 1/2 (cid:1) 1/2 (cid:1)
(cid:1) 1/2 (cid:1) 1/2
(cid:1)
(cid:1) 1/2 1/2
(cid:1)
(cid:1)

p(0)(x) =(cid:2) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) 1 (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:3) :

The probability distribution p(t)(x) of the state at the tth iteration is shown
for t = 0; 1, 2, 3, 5, 10, 100, 200, 400 in (cid:12)gure 29.14; an equivalent sequence of
distributions is shown in (cid:12)gure 29.15 for the chain that begins in initial state
x0 = 17. Both chains converge to the target density, the uniform density, as
t ! 1.
Required properties

(29.40)

When designing a Markov chain Monte Carlo method, we construct a chain
with the following properties:

1. The desired distribution P (x) is an invariant distribution of the chain.

p(0)(x)

p(1)(x)

0

0

p(2)(x)

5

10

15

20

5

10

15

20

0

5

10

15

20

p(3)(x)

p(10)(x)

p(100)(x)

p(200)(x)

p(400)(x)

0

0

0

0

0

5

10

15

20

5

10

15

20

5

10

15

20

5

10

15

20

5

10

15

20

Figure 29.14. The probability
distribution of the state of the
Markov chain of example 29.6.

A distribution (cid:25)(x) is an invariant distribution of the transition proba-
bility T (x0; x) if

An invariant distribution is an eigenvector of the transition probability
matrix that has eigenvalue 1.

(cid:25)(x0) =Z dN x T (x0; x)(cid:25)(x):

(29.41)

 
 































































Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.6: Terminology for Markov chain Monte Carlo methods

373

2. The chain must also be ergodic, that is,

p(t)(x) ! (cid:25)(x) as t ! 1, for any p(0)(x).
A couple of reasons why a chain might not be ergodic are:

(29.42)

p(0)(x)

5

10

15

20

5

10

15

20

0

0

p(1)(x)

p(2)(x)

0

5

10

15

20

p(3)(x)

p(10)(x)

p(100)(x)

p(200)(x)

p(400)(x)

0

0

0

0

0

5

10

15

20

5

10

15

20

5

10

15

20

5

10

15

20

5

10

15

20

Figure 29.15. The probability
distribution of the state of the
Markov chain for initial condition
x0 = 17 (example 29.6 (p.372)).

(a) Its matrix might be reducible, which means that the state space
contains two or more subsets of states that can never be reached
from each other. Such a chain has many invariant distributions;
which one p(t)(x) would tend to as t ! 1 would depend on the
initial condition p(0)(x).
The transition probability matrix of such a chain has more than
one eigenvalue equal to 1.

(b) The chain might have a periodic set, which means that, for some
initial conditions, p(t)(x) doesn’t tend to an invariant distribution,
but instead tends to a periodic limit-cycle.
A simple Markov chain with this property is the random walk on the
N -dimensional hypercube. The chain T takes the state from one
corner to a randomly chosen adjacent corner. The unique invariant
distribution of this chain is the uniform distribution over all 2N
states, but the chain is not ergodic; it is periodic with period two:
if we divide the states into states with odd parity and states with
even parity, we notice that every odd state is surrounded by even
states and vice versa. So if the initial condition at time t = 0 is a
state with even parity, then at time t = 1 { and at all odd times
{ the state must have odd parity, and at all even times, the state
will be of even parity.
The transition probability matrix of such a chain has more than
one eigenvalue with magnitude equal to 1. The random walk on
the hypercube, for example, has eigenvalues equal to +1 and (cid:0)1.

Methods of construction of Markov chains

It is often convenient to construct T by mixing or concatenating simple base
transitions B all of which satisfy

P (x0) =Z dN x B(x0; x)P (x);

(29.43)

for the desired density P (x),
i.e., they all have the desired density as an
invariant distribution. These base transitions need not individually be ergodic.
T is a mixture of several base transitions Bb(x0; x) if we make the transition
by picking one of the base transitions at random, and allowing it to determine
the transition, i.e.,

pbBb(x0; x);

(29.44)

T (x0; x) =Xb

where fpbg is a probability distribution over the base transitions.
T is a concatenation of two base transitions B1(x0; x) and B2(x0; x) if we
(cid:12)rst make a transition to an intermediate state x00 using B1, and then make a
transition from state x00 to x0 using B2.

T (x0; x) =Z dN x00 B2(x0; x00)B1(x00; x):

(29.45)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29 | Monte Carlo Methods

374

Detailed balance

Many useful transition probabilities satisfy the detailed balance property:

T (xa; xb)P (xb) = T (xb; xa)P (xa); for all xb and xa:

(29.46)

This equation says that if we pick (by magic) a state from the target density
P and make a transition under T to another state, it is just as likely that we
will pick xb and go from xb to xa as it is that we will pick xa and go from xa
to xb. Markov chains that satisfy detailed balance are also called reversible
Markov chains. The reason why the detailed-balance property is of interest
is that detailed balance implies invariance of the distribution P (x) under the
Markov chain T , which is a necessary condition for the key property that we
want from our MCMC simulation { that the probability distribution of the
chain should converge to P (x).

. Exercise 29.7.[2 ] Prove that detailed balance implies invariance of the distri-

bution P (x) under the Markov chain T .

Proving that detailed balance holds is often a key step when proving that a
Markov chain Monte Carlo simulation will converge to the desired distribu-
tion. The Metropolis method satis(cid:12)es detailed balance, for example. Detailed
balance is not an essential condition, however, and we will see later that ir-
reversible Markov chains can be useful in practice, because they may have
di(cid:11)erent random walk properties.

. Exercise 29.8.[2 ] Show that, if we concatenate two base transitions B1 and B2
that satisfy detailed balance, it is not necessarily the case that the T
thus de(cid:12)ned (29.45) satis(cid:12)es detailed balance.

Exercise 29.9.[2 ] Does Gibbs sampling, with several variables all updated in a

deterministic sequence, satisfy detailed balance?

29.7 Slice sampling

Slice sampling (Neal, 1997a; Neal, 2003) is a Markov chain Monte Carlo
method that has similarities to rejection sampling, Gibbs sampling and the
Metropolis method. It can be applied wherever the Metropolis method can
be applied, that is, to any system for which the target density P (cid:3)(x) can be
evaluated at any point x; it has the advantage over simple Metropolis methods
that it is more robust to the choice of parameters like step sizes. The sim-
plest version of slice sampling is similar to Gibbs sampling in that it consists of
one-dimensional transitions in the state space; however there is no requirement
that the one-dimensional conditional distributions be easy to sample from, nor
that they have any convexity properties such as are required for adaptive re-
jection sampling. And slice sampling is similar to rejection sampling in that
it is a method that asymptotically draws samples from the volume under the
curve described by P (cid:3)(x); but there is no requirement for an upper-bounding
function.

I will describe slice sampling by giving a sketch of a one-dimensional sam-
pling algorithm, then giving a pictorial description that includes the details
that make the method valid.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.7: Slice sampling

375

The skeleton of slice sampling
Let us assume that we want to draw samples from P (x) / P (cid:3)(x) where x
is a real number. A one-dimensional slice sampling algorithm is a method
for making transitions from a two-dimensional point (x; u) lying under the
curve P (cid:3)(x) to another point (x0; u0) lying under the same curve, such that
the probability distribution of (x; u) tends to a uniform distribution over the
area under the curve P (cid:3)(x), whatever initial point we start from { like the
uniform distribution under the curve P (cid:3)(x) produced by rejection sampling
(section 29.3).

A single transition (x; u) ! (x0; u0) of a one-dimensional slice sampling
algorithm has the following steps, of which steps 3 and 8 will require further
elaboration.

1: evaluate P (cid:3)(x)
2: draw a vertical coordinate u0 (cid:24) Uniform(0; P (cid:3)(x))
3: create a horizontal interval (xl; xr) enclosing x
4: loop f
5:
6:
7:
8:
9: g

draw x0 (cid:24) Uniform(xl; xr)
evaluate P (cid:3)(x0)
if P (cid:3)(x0) > u0 break out of loop 4-9
else modify the interval (xl; xr)

There are several methods for creating the interval (xl; xr) in step 3, and
several methods for modifying it at step 8. The important point is that the
overall method must satisfy detailed balance, so that the uniform distribution
for (x; u) under the curve P (cid:3)(x) is invariant.

The ‘stepping out’ method for step 3

In the ‘stepping out’ method for creating an interval (xl; xr) enclosing x, we
step out in steps of length w until we (cid:12)nd endpoints xl and xr at which P (cid:3) is
smaller than u. The algorithm is shown in (cid:12)gure 29.16.

3a: draw r (cid:24) Uniform(0; 1)
3b: xl := x (cid:0) rw
3c: xr := x + (1 (cid:0) r)w
3d: while (P (cid:3)(xl) > u0) f xl := xl (cid:0) w g
3e: while (P (cid:3)(xr) > u0) f xr := xr + w g

The ‘shrinking’ method for step 8

Whenever a point x0 is drawn such that (x0; u0) lies above the curve P (cid:3)(x),
we shrink the interval so that one of the end points is x0, and such that the
original point x is still enclosed in the interval.

8a: if (x0 > x) f xr := x0 g
8b: else f xl := x0 g

Properties of slice sampling

Like a standard Metropolis method, slice sampling gets around by a random
walk, but whereas in the Metropolis method, the choice of the step size is

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

376

29 | Monte Carlo Methods

1

2

3a,3b,3c

3d,3e

5,6

8

5,6,7

Figure 29.16. Slice sampling. Each
panel is labelled by the steps of
the algorithm that are executed in
it. At step 1, P (cid:3)(x) is evaluated
at the current point x. At step 2,
a vertical coordinate is selected
giving the point (x; u0) shown by
the box; At steps 3a-c, an
interval of size w containing
(x; u0) is created at random. At
step 3d, P (cid:3) is evaluated at the left
end of the interval and is found to
be larger than u0, so a step to the
left of size w is made. At step 3e,
P (cid:3) is evaluated at the right end of
the interval and is found to be
smaller than u0, so no stepping
out to the right is needed. When
step 3d is repeated, P (cid:3) is found to
be smaller than u0, so the
stepping out halts. At step 5 a
point is drawn from the interval,
shown by a (cid:14). Step 6 establishes
that this point is above P (cid:3) and
step 8 shrinks the interval to the
rejected point in such a way that
the original point x is still in the
interval. When step 5 is repeated,
the new coordinate x0 (which is to
the right-hand side of the
interval) gives a value of P (cid:3)
greater than u0, so this point x0 is
the outcome at step 7.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.7: Slice sampling

377

10

1

0

1 2 3 4 5 6 7 8 9 10 11

Figure 29.17. P (cid:3)(x).

critical to the rate of progress, in slice sampling the step size is self-tuning. If
the initial interval size w is too small by a factor f compared with the width of
the probable region then the stepping-out procedure expands the interval size.
The cost of this stepping-out is only linear in f , whereas in the Metropolis
method the computer-time scales as the square of f if the step size is too
small.

If the chosen value of w is too large by a factor F then the algorithm
spends a time proportional to the logarithm of F shrinking the interval down
to the right size, since the interval typically shrinks by a factor in the ballpark
of 0:6 each time a point is rejected.
In contrast, the Metropolis algorithm
responds to a too-large step size by rejecting almost all proposals, so the rate
of progress is exponentially bad in F . There are no rejections in slice sampling.
The probability of staying in exactly the same place is very small.

. Exercise 29.10.[2 ] Investigate the properties of slice sampling applied to the
density shown in (cid:12)gure 29.17. x is a real variable between 0.0 and 11.0.
How long does it take typically for slice sampling to get from an x in
the peak region x 2 (0; 1) to an x in the tail region x 2 (1; 11), and vice
versa? Con(cid:12)rm that the probabilities of these transitions do yield an
asymptotic probability density that is correct.

How slice sampling is used in real problems
An N -dimensional density P (x) / P (cid:3)(x) may be sampled with the help of the
one-dimensional slice sampling method presented above by picking a sequence
of directions y(1); y(2); : : : and de(cid:12)ning x = x(t) + xy(t). The function P (cid:3)(x)
above is replaced by P (cid:3)(x) = P (cid:3)(x(t) + xy(t)). The directions may be chosen
in various ways; for example, as in Gibbs sampling, the directions could be the
coordinate axes; alternatively, the directions y(t) may be selected at random
in any manner such that the overall procedure satis(cid:12)es detailed balance.

Computer-friendly slice sampling

The real variables of a probabilistic model will always be represented in a
computer using a (cid:12)nite number of bits. In the following implementation of
slice sampling due to Skilling, the stepping-out, randomization, and shrinking
operations, described above in terms of (cid:13)oating-point operations, are replaced
by binary and integer operations.

We assume that the variable x that is being slice-sampled is represented by
a b-bit integer X taking on one of B = 2b values, 0; 1; 2; : : : ; B(cid:0)1, many or all
of which correspond to valid values of x. Using an integer grid eliminates any
errors in detailed balance that might ensue from variable-precision rounding of
(cid:13)oating-point numbers. The mapping from X to x need not be linear; if it is
nonlinear, we assume that the function P (cid:3)(x) is replaced by an appropriately
transformed function { for example, P (cid:3)(cid:3)(X) / P (cid:3)(x)jdx=dXj.

We assume the following operators on b-bit integers are available:

X + N
X (cid:0) N
X (cid:8) N

N := randbits(l)

arithmetic sum, modulo B, of X and N .

di(cid:11)erence, modulo B, of X and N .
bitwise exclusive-or of X and N .
sets N to a random l-bit integer.

A slice-sampling procedure for integers is then as follows:

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

378

29 | Monte Carlo Methods

Given: a current point X and a height Y = P (cid:3)(X) (cid:2) Uniform(0; 1) (cid:20) P (cid:3)(X)

1: U := randbits(b)

set l to a value l (cid:20) b
do f
N := randbits(l)

2:
3:
4:

5:

6:

X0 := ((X (cid:0) U ) (cid:8) N ) + U
l := l (cid:0) 1

7: g until (X0 = X) or (P (cid:3)(X0) (cid:21) Y )

De(cid:12)ne a random translation U of the binary coor-
dinate system.
Set initial l-bit sampling range.

De(cid:12)ne a random move within the current interval of
width 2l.
Randomize the lowest l bits of X (in the translated
coordinate system).
If X0 is not acceptable, decrease l and try again
with a smaller perturbation of X; termination at or
before l = 0 is assured.

0

X

B−1

Figure 29.18. The sequence of
intervals from which the new
candidate points are drawn.

The translation U is introduced to avoid permanent sharp edges, where
for example the adjacent binary integers 0111111111 and 1000000000 would
otherwise be permanently in di(cid:11)erent sectors, making it di(cid:14)cult for X to move
from one to the other.

The sequence of intervals from which the new candidate points are drawn
is illustrated in (cid:12)gure 29.18. First, a point is drawn from the entire interval,
shown by the top horizontal line. At each subsequent draw, the interval is
halved in such a way as to contain the previous point X.

If preliminary stepping-out from the initial range is required, step 2 above

can be replaced by the following similar procedure:

l sets the initial width

2a: set l to a value l < b
2b: do f
2c:
2d:
2e:

N := randbits(l)
X0 := ((X (cid:0) U ) (cid:8) N ) + U
l := l + 1

2f: g until (l = b) or (P (cid:3)(X0) < Y )

These shrinking and stepping out methods shrink and expand by a factor
of two per evaluation. A variant is to shrink or expand by more than one bit
each time, setting l := l (cid:6) (cid:1)l with (cid:1)l > 1. Taking (cid:1)l at each step from any
pre-assigned distribution (which may include (cid:1)l = 0) allows extra (cid:13)exibility.

Exercise 29.11.[4 ] In the shrinking phase, after an unacceptable X0 has been
produced, the choice of (cid:1)l is allowed to depend on the di(cid:11)erence between
the slice’s height Y and the value of P (cid:3)(X0), without spoiling the algo-
rithm’s validity. (Prove this.) It might be a good idea to choose a larger
value of (cid:1)l when Y (cid:0) P (cid:3)(X0) is large. Investigate this idea theoretically
or empirically.

A feature of using the integer representation is that, with a suitably ex-
tended number of bits, the single integer X can represent two or more real
parameters { for example, by mapping X to (x1; x2; x3) through a space-(cid:12)lling
curve such as a Peano curve. Thus multi-dimensional slice sampling can be
performed using the same software as for one dimension.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

379

29.8: Practicalities

29.8 Practicalities

Can we predict how long a Markov chain Monte Carlo simulation
will take to equilibrate? By considering the random walks involved in a
Markov chain Monte Carlo simulation we can obtain simple lower bounds on
the time required for convergence. But predicting this time more precisely is a
di(cid:14)cult problem, and most of the theoretical results giving upper bounds on
the convergence time are of little practical use. The exact sampling methods
of Chapter 32 o(cid:11)er a solution to this problem for certain Markov chains.

Can we diagnose or detect convergence in a running simulation?
This is also a di(cid:14)cult problem. There are a few practical tools available, but
none of them is perfect (Cowles and Carlin, 1996).

Can we speed up the convergence time and time between indepen-
dent samples of a Markov chain Monte Carlo method? Here, there is
good news, as described in the next chapter, which describes the Hamiltonian
Monte Carlo method, overrelaxation, and simulated annealing.

29.9 Further practical issues

Can the normalizing constant be evaluated?

If the target density P (x) is given in the form of an unnormalized density
P (cid:3)(x) with P (x) = 1
Z P (cid:3)(x), the value of Z may well be of interest. Monte
Carlo methods do not readily yield an estimate of this quantity, and it is an
area of active research to (cid:12)nd ways of evaluating it. Techniques for evaluating
Z include:

1. Importance sampling (reviewed by Neal (1993b)) and annealed impor-

tance sampling (Neal, 1998).

2. ‘Thermodynamic integration’ during simulated annealing, the ‘accep-
tance ratio’ method, and ‘umbrella sampling’ (reviewed by Neal (1993b)).

3. ‘Reversible jump Markov chain Monte Carlo’ (Green, 1995).

One way of dealing with Z, however, may be to (cid:12)nd a solution to one’s
task that does not require that Z be evaluated. In Bayesian data modelling
one might be able to avoid the need to evaluate Z { which would be important
for model comparison { by not having more than one model. Instead of using
several models (di(cid:11)ering in complexity, for example) and evaluating their rel-
ative posterior probabilities, one can make a single hierarchical model having,
for example, various continuous hyperparameters which play a role similar to
that played by the distinct models (Neal, 1996). In noting the possibility of
not computing Z, I am not endorsing this approach. The normalizing constant
Z is often the single most important number in the problem, and I think every
e(cid:11)ort should be devoted to calculating it.

The Metropolis method for big models

Our original description of the Metropolis method involved a joint updating
of all the variables using a proposal density Q(x0; x). For big problems it
may be more e(cid:14)cient to use several proposal distributions Q(b)(x0; x), each of
which updates only some of the components of x. Each proposal is individually
accepted or rejected, and the proposal distributions are repeatedly run through
in sequence.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

380

29 | Monte Carlo Methods

. Exercise 29.12.[2, p.385] Explain why the rate of movement through the state
space will be greater when B proposals Q(1); : : : ; Q(B) are considered
individually in sequence, compared with the case of a single proposal
Q(cid:3) de(cid:12)ned by the concatenation of Q(1); : : : ; Q(B). Assume that each
proposal distribution Q(b)(x0; x) has an acceptance rate f < 1=2.

In the Metropolis method, the proposal density Q(x0; x) typically has a
number of parameters that control, for example, its ‘width’. These parameters
are usually set by trial and error with the rule of thumb being to aim for a
rejection frequency of about 0.5. It is not valid to have the width parameters
be dynamically updated during the simulation in a way that depends on the
history of the simulation. Such a modi(cid:12)cation of the proposal density would
violate the detailed-balance condition that guarantees that the Markov chain
has the correct invariant distribution.

Gibbs sampling in big models

Our description of Gibbs sampling involved sampling one parameter at a time,
as described in equations (29.35{29.37). For big problems it may be more
e(cid:14)cient to sample groups of variables jointly, that is to use several proposal
distributions:

x(t+1)
; : : : ; x(t+1)
1
a+1 ; : : : ; x(t+1)
x(t+1)

a

b

(cid:24) P (x1; : : : ; xa j x(t)
(cid:24) P (xa+1; : : : ; xb j x(t+1)

1

a+1; : : : ; x(t)
K )

; : : : ; x(t+1)

a

(29.47)

; x(t)

b+1; : : : ; x(t)
K );

etc.

How many samples are needed?
At the start of this chapter, we observed that the variance of an estimator ^(cid:8)
depends only on the number of independent samples R and the value of

(cid:27)2 =Z dN x P (x)((cid:30)(x) (cid:0) (cid:8))2:

(29.48)

We have now discussed a variety of methods for generating samples from P (x).
How many independent samples R should we aim for?

In many problems, we really only need about twelve independent samples
from P (x).
Imagine that x is an unknown vector such as the amount of
corrosion present in each of 10 000 underground pipelines around Cambridge,
and (cid:30)(x) is the total cost of repairing those pipelines. The distribution P (x)
describes the probability of a state x given the tests that have been carried out
on some pipelines and the assumptions about the physics of corrosion. The
quantity (cid:8) is the expected cost of the repairs. The quantity (cid:27) 2 is the variance
of the cost { (cid:27) measures by how much we should expect the actual cost to
di(cid:11)er from the expectation (cid:8).

Now, how accurately would a manager like to know (cid:8)? I would suggest
there is little point in knowing (cid:8) to a precision (cid:12)ner than about (cid:27)=3. After
all, the true cost is likely to di(cid:11)er by (cid:6)(cid:27) from (cid:8).
If we obtain R = 12
independent samples from P (x), we can estimate (cid:8) to a precision of (cid:27)=p12 {
which is smaller than (cid:27)=3. So twelve samples su(cid:14)ce.

Allocation of resources

Assuming we have decided how many independent samples R are required,
an important question is how one should make use of one’s limited computer
resources to obtain these samples.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

381

Figure 29.19. Three possible
Markov chain Monte Carlo
strategies for obtaining twelve
samples in a (cid:12)xed amount of
computer time. Time is
represented by horizontal lines;
samples by white circles. (1) A
single run consisting of one long
‘burn in’ period followed by a
sampling period. (2) Four
medium-length runs with di(cid:11)erent
initial conditions and a
medium-length burn in period.
(3) Twelve short runs.

29.10: Summary

(1)

(2)

(3)

A typical Markov chain Monte Carlo experiment involves an initial pe-
riod in which control parameters of the simulation such as step sizes may be
adjusted. This is followed by a ‘burn in’ period during which we hope the
simulation ‘converges’ to the desired distribution. Finally, as the simulation
continues, we record the state vector occasionally so as to create a list of states
fx(r)gR

r=1 that we hope are roughly independent samples from P (x).

There are several possible strategies ((cid:12)gure 29.19):

1. Make one long run, obtaining all R samples from it.

2. Make a few medium-length runs with di(cid:11)erent initial conditions, obtain-

ing some samples from each.

3. Make R short runs, each starting from a di(cid:11)erent random initial condi-
tion, with the only state that is recorded being the (cid:12)nal state of each
simulation.

The (cid:12)rst strategy has the best chance of attaining ‘convergence’. The last
strategy may have the advantage that the correlations between the recorded
samples are smaller. The middle path is popular with Markov chain Monte
Carlo experts (Gilks et al., 1996) because it avoids the ine(cid:14)ciency of discarding
burn-in iterations in many runs, while still allowing one to detect problems
with lack of convergence that would not be apparent from a single run.

Finally, I should emphasize that there is no need to make the points in
the estimate nearly-independent. Averaging over dependent points is (cid:12)ne { it
won’t lead to any bias in the estimates. For example, when you use strategy
1 or 2, you may, if you wish, include all the points between the (cid:12)rst and last
sample in each run. Of course, estimating the accuracy of the estimate is
harder when the points are dependent.

29.10 Summary

(cid:15) Monte Carlo methods are a powerful tool that allow one to sample from
any probability distribution that can be expressed in the form P (x) =
1
Z P (cid:3)(x).

(cid:15) Monte Carlo methods can answer virtually any query related to P (x) by

putting the query in the form

Z (cid:30)(x)P (x) ’

1

RXr

(cid:30)(x(r)):

(29.49)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

382

29 | Monte Carlo Methods

(cid:15) In high-dimensional problems the only satisfactory methods are those
based on Markov chains, such as the Metropolis method, Gibbs sam-
pling and slice sampling. Gibbs sampling is an attractive method be-
cause it has no adjustable parameters but its use is restricted to cases
where samples can be generated from the conditional distributions. Slice
sampling is attractive because, whilst it has step-length parameters, its
performance is not very sensitive to their values.

(cid:15) Simple Metropolis algorithms and Gibbs sampling algorithms, although
widely used, perform poorly because they explore the space by a slow
random walk. The next chapter will discuss methods for speeding up
Markov chain Monte Carlo simulations.

(cid:15) Slice sampling does not avoid random walk behaviour, but it automat-
ically chooses the largest appropriate step size, thus reducing the bad
e(cid:11)ects of the random walk compared with, say, a Metropolis method
with a tiny step size.

29.11 Exercises

Exercise 29.13.[2C, p.386] A study of importance sampling. We already estab-
lished in section 29.2 that importance sampling is likely to be useless in
high-dimensional problems. This exercise explores a further cautionary
tale, showing that importance sampling can fail even in one dimension,
even with friendly Gaussian distributions.

Imagine that we want to know the expectation of a function (cid:30)(x) under
a distribution P (x),

(cid:8) =Z dx P (x)(cid:30)(x);

(29.50)

and that this expectation is estimated by importance sampling with
a distribution Q(x). Alternatively, perhaps we wish to estimate the
normalizing constant Z in P (x) = P (cid:3)(x)=Z using

Z =Z dx P (cid:3)(x) =Z dx Q(x)

P (cid:3)(x)
Q(x)

=(cid:28) P (cid:3)(x)

Q(x)(cid:29)x(cid:24)Q

:

(29.51)

Now, let P (x) and Q(x) be Gaussian distributions with mean zero and
standard deviations (cid:27)p and (cid:27)q. Each point x drawn from Q will have
an associated weight P (cid:3)(x)=Q(x). What is the variance of the weights?
[Assume that P (cid:3) = P , so P is actually normalized, and Z = 1, though
we can pretend that we didn’t know that.] What happens to the variance
of the weights as (cid:27)2

Check your theory by simulating this importance-sampling problem on
a computer.

q ! (cid:27)2

p=2?

Exercise 29.14.[2 ] Consider the Metropolis algorithm for the one-dimensional
toy problem of section 29.4, sampling from f0; 1; : : : ; 20g. Whenever
the current state is one of the end states, the proposal density given in
equation (29.34) will propose with probability 50% a state that will be
rejected.

To reduce this ‘waste’, Fred modi(cid:12)es the software responsible for gen-
erating samples from Q so that when x = 0, the proposal density is
100% on x0 = 1, and similarly when x = 20, x0 = 19 is always proposed.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.11: Exercises

383

Fred sets the software that implements the acceptance rule so that the
software accepts all proposed moves. What probability P 0(x) will Fred’s
modi(cid:12)ed software generate samples from?

What is the correct acceptance rule for Fred’s proposal density, in order
to obtain samples from P (x)?

. Exercise 29.15.[3C ] Implement Gibbs sampling for the inference of a single
one-dimensional Gaussian, which we studied using maximum likelihood
in section 22.1. Assign a broad Gaussian prior to (cid:22) and a broad gamma
prior (24.2) to the precision parameter (cid:12) = 1=(cid:27)2. Each update of (cid:22) will
involve a sample from a Gaussian distribution, and each update of (cid:27)
requires a sample from a gamma distribution.

Exercise 29.16.[3C ] Gibbs sampling for clustering. Implement Gibbs sampling
for the inference of a mixture of K one-dimensional Gaussians, which we
studied using maximum likelihood in section 22.2. Allow the clusters to
have di(cid:11)erent standard deviations (cid:27)k. Assign priors to the means and
standard deviations in the same way as the previous exercise. Either (cid:12)x
the prior probabilities of the classes f(cid:25)kg to be equal or put a uniform
prior over the parameters (cid:25) and include them in the Gibbs sampling.

Notice the similarity of Gibbs sampling to the soft K-means clustering
algorithm (algorithm 22.2). We can alternately assign the class labels
fkng given the parameters f(cid:22)k; (cid:27)kg, then update the parameters given
the class labels. The assignment step involves sampling from the proba-
bility distributions de(cid:12)ned by the responsibilities (22.22), and the update
step updates the means and variances using probability distributions
centred on the K-means algorithm’s values (22.23, 22.24).

Do your experiments con(cid:12)rm that Monte Carlo methods bypass the over-
(cid:12)tting di(cid:14)culties of maximum likelihood discussed in section 22.4?

A solution to this exercise and the previous one, written in octave, is
available.2

. Exercise 29.17.[3C ] Implement Gibbs sampling for the seven scientists inference
problem, which we encountered in exercise 22.15 (p.309), and which you
may have solved by exact marginalization (exercise 24.3 (p.323)) [it’s
not essential to have done the latter].

. Exercise 29.18.[2 ] A Metropolis method is used to explore a distribution P (x)
that is actually a 1000-dimensional spherical Gaussian distribution of
standard deviation 1 in all dimensions. The proposal density Q is a
1000-dimensional spherical Gaussian distribution of standard deviation
(cid:15). Roughly what is the step size (cid:15) if the acceptance rate is 0.5? Assuming
this value of (cid:15),

(a) roughly how long would the method take to traverse the distribution

and generate a sample independent of the initial condition?

(b) By how much does ln P (x) change in a typical step? By how much

should ln P (x) vary when x is drawn from P (x)?

(c) What happens if, rather than using a Metropolis method that tries
to change all components at once, one instead uses a concatenation
of Metropolis updates changing one component at a time?

2http://www.inference.phy.cam.ac.uk/mackay/itila/

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

384

29 | Monte Carlo Methods

. Exercise 29.19.[2 ] When discussing the time taken by the Metropolis algo-
rithm to generate independent samples we considered a distribution with
longest spatial length scale L being explored using a proposal distribu-
tion with step size (cid:15). Another dimension that a MCMC method must
explore is the range of possible values of the log probability ln P (cid:3)(x).
Assuming that the state x contains a number of independent random
variables proportional to N , when samples are drawn from P (x), the
‘asymptotic equipartition’ principle tell us that the value of (cid:0) ln P (x) is
likely to be close to the entropy of x, varying either side with a standard
deviation that scales as pN . Consider a Metropolis method with a sym-
metrical proposal density, that is, one that satis(cid:12)es Q(x; x0) = Q(x0; x).
Assuming that accepted jumps either increase ln P (cid:3)(x) by some amount
or decrease it by a small amount, e.g. ln e = 1 (is this a reasonable
assumption?), discuss how long it must take to generate roughly inde-
pendent samples from P (x). Discuss whether Gibbs sampling has similar
properties.

Exercise 29.20.[3 ] Markov chain Monte Carlo methods do not compute parti-
tion functions Z, yet they allow ratios of quantities like Z to be esti-
mated. For example, consider a random-walk Metropolis algorithm in a
state space where the energy is zero in a connected accessible region, and
in(cid:12)nitely large everywhere else; and imagine that the accessible space can
be chopped into two regions connected by one or more corridor states.
The fraction of times spent in each region at equilibrium is proportional
to the volume of the region. How does the Monte Carlo method manage
to do this without measuring the volumes?

Exercise 29.21.[5 ] Philosophy.

One curious defect of these Monte Carlo methods { which are widely used
by Bayesian statisticians { is that they are all non-Bayesian (O’Hagan,
1987). They involve computer experiments from which estimators of
quantities of interest are derived. These estimators depend on the pro-
posal distributions that were used to generate the samples and on the
random numbers that happened to come out of our random number
generator. In contrast, an alternative Bayesian approach to the problem
would use the results of our computer experiments to infer the proper-
ties of the target function P (x) and generate predictive distributions for
quantities of interest such as (cid:8). This approach would give answers that
would depend only on the computed values of P (cid:3)(x(r)) at the points
fx(r)g; the answers would not depend on how those points were chosen.
Can you make a Bayesian Monte Carlo method? (See Rasmussen and
Ghahramani (2003) for a practical attempt.)

29.12 Solutions

Solution to exercise 29.1 (p.362). We wish to show that

^(cid:8) (cid:17) Pr wr(cid:30)(x(r))
Pr wr

(29.52)

converges to the expectation of (cid:8) under P . We consider the numerator and the
denominator separately. First, the denominator. Consider a single importance
weight

wr (cid:17)

P (cid:3)(x(r))
Q(cid:3)(x(r))

:

(29.53)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

29.12: Solutions

385

What is its expectation, averaged under the distribution Q = Q(cid:3)=ZQ of the
point x(r)?

hwri =Z dx Q(x)

P (cid:3)(x)
Q(cid:3)(x)

=Z dx

1
ZQ

P (cid:3)(x) =

ZP
ZQ

:

So the expectation of the denominator is

*Xr

wr+ = R

ZP
ZQ

:

(29.54)

(29.55)

As long as the variance of wr is (cid:12)nite, the denominator, divided by R, will
converge to ZP =ZQ as R increases.
[In fact, the estimate converges to the
right answer even if this variance is in(cid:12)nite, as long as the expectation is
well-de(cid:12)ned.] Similarly, the expectation of one term in the numerator is

hwr(cid:30)(x)i =Z dx Q(x)

P (cid:3)(x)
Q(cid:3)(x)

(cid:30)(x) =Z dx

1
ZQ

P (cid:3)(x)(cid:30)(x) =

ZP
ZQ

(cid:8);

(29.56)

where (cid:8) is the expectation of (cid:30) under P . So the numerator, divided by R,
converges to ZP
ZQ

(cid:8) with increasing R. Thus ^(cid:8) converges to (cid:8).

The numerator and the denominator are unbiased estimators of RZP =ZQ
and RZP =ZQ(cid:8) respectively, but their ratio ^(cid:8) is not necessarily an unbiased
estimator for (cid:12)nite R.

Solution to exercise 29.2 (p.363). When the true density P is multimodal, it is
unwise to use importance sampling with a sampler density (cid:12)tted to one mode,
because on the rare occasions that a point is produced that lands in one of
the other modes, the weight associated with that point will be enormous. The
estimates will have enormous variance, but this enormous variance may not
be evident to the user if no points in the other modes have been seen.

Solution to exercise 29.5 (p.371). The posterior distribution for the syndrome
decoding problem is a pathological distribution from the point of view of Gibbs
sampling. The factor  [Hn = z] is 1 only on a small fraction of the space of
possible vectors n, namely the 2K points that correspond to the valid code-
words. No two codewords are adjacent, so similarly, any single bit (cid:13)ip from
a viable state n will take us to a state with zero probability and so the state
will never move in Gibbs sampling.

A general code has exactly the same problem. The points corresponding
to valid codewords are relatively few in number and they are not adjacent (at
least for any useful code). So Gibbs sampling is no use for syndrome decoding
for two reasons. First, (cid:12)nding any reasonably good hypothesis is di(cid:14)cult, and
as long as the state is not near a valid codeword, Gibbs sampling cannot help
since none of the conditional distributions is de(cid:12)ned; and second, once we are
in a valid hypothesis, Gibbs sampling will never take us out of it.

One could attempt to perform Gibbs sampling using the bits of the original
message s as the variables. This approach would not get locked up in the way
just described, but, for a good code, any single bit (cid:13)ip would substantially
alter the reconstructed codeword, so if one had found a state with reasonably
large likelihood, Gibbs sampling would take an impractically large time to
escape from it.

Solution to exercise 29.12 (p.380).
Each Metropolis proposal will take the
energy of the state up or down by some amount. The total change in energy

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

386

29 | Monte Carlo Methods

when B proposals are concatenated will be the end-point of a random walk
with B steps in it. This walk might have mean zero, or it might have a
tendency to drift upwards (if most moves increase the energy and only a few
decrease it). In general the latter will hold, if the acceptance rate f is small:
the mean change in energy from any one move will be some (cid:1)E > 0 and so
the acceptance probability for the concatenation of B moves will be of order
1=(1 + exp((cid:0)B(cid:1)E)), which scales roughly as f B. The mean-square-distance
moved will be of order f BB(cid:15)2, where (cid:15) is the typical step size. In contrast,
the mean-square-distance moved when the moves are considered individually
will be of order f B(cid:15)2.

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

10

1000
10000
100000
theory

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

8

6

4

2

0

1000
10000
100000
theory

3.5

3

2.5

2

1.5

1

0.5

0

0.2

0.4

0.6

0.8

1

1.2

1.4

0

0

1.6

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

Figure 29.20. Importance
sampling in one dimension. For
R = 1000; 104, and 105, the
normalizing constant of a
Gaussian distribution (known in
fact to be 1) was estimated using
importance sampling with a
sampler density of standard
deviation (cid:27)q (horizontal axis).
The same random number seed
was used for all runs. The three
plots show (a) the estimated
normalizing constant; (b) the
empirical standard deviation of
the R weights; (c) 30 of the
weights.

(29.57)

(29.58)

(29.59)

(29.60)

(29.61)

(29.62)

Solution to exercise 29.13 (p.382). The weights are w = P (x)=Q(x) and x is
drawn from Q. The mean weight is

Z dx Q(x) [P (x)=Q(x)] =Z dx P (x) = 1;

assuming the integral converges. The variance is

var(w) = Z dx Q(x)(cid:20) P (x)

Q(x) (cid:0) 1(cid:21)2

= Z dx
= (cid:20)Z dx
P = (cid:27)q=(p2(cid:25)(cid:27)2

P (x)2
Q(x) (cid:0) 2P (x) + Q(x)
2 (cid:18) 2
ZQ
p (cid:0)
Z 2
(cid:27)2
P

exp(cid:18)(cid:0)

x2

1
(cid:27)2

q(cid:19)(cid:19)(cid:21) (cid:0) 1;

where ZQ=Z 2
coe(cid:14)cient of x2 in the exponent is positive, i.e., if

p). The integral in (29.60) is (cid:12)nite only if the

If this condition is satis(cid:12)ed, the variance is

(cid:27)2
q >

1
2

(cid:27)2
p:

var(w) =

(cid:27)qp2(cid:25)(cid:27)2

p

p2(cid:25)(cid:18) 2
p (cid:0)
(cid:27)2

1
(cid:27)2

q(cid:19)(cid:0) 1

2

(cid:0) 1 =

(cid:27)2
q
q (cid:0) (cid:27)2

p(cid:1)1=2 (cid:0) 1:

(cid:27)p(cid:0)2(cid:27)2

As (cid:27)q approaches the critical value { about 0:7(cid:27)p { the variance becomes
in(cid:12)nite. Figure 29.20 illustrates these phenomena for (cid:27)p = 1 with (cid:27)q varying
from 0.1 to 1.5. The same random number seed was used for all runs, so
the weights and estimates follow smooth curves. Notice that the empirical
standard deviation of the R weights can look quite small and well-behaved
(say, at (cid:27)q ’ 0:3) when the true standard deviation is nevertheless in(cid:12)nite.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

30

E(cid:14)cient Monte Carlo Methods

This chapter discusses several methods for reducing random walk behaviour
in Metropolis methods. The aim is to reduce the time required to obtain
e(cid:11)ectively independent samples. For brevity, we will say ‘independent samples’
when we mean ‘e(cid:11)ectively independent samples’.

30.1 Hamiltonian Monte Carlo

The Hamiltonian Monte Carlo method is a Metropolis method, applicable
to continuous state spaces, that makes use of gradient information to reduce
random walk behaviour. [The Hamiltonian Monte Carlo method was originally
called hybrid Monte Carlo, for historical reasons.]

For many systems whose probability P (x) can be written in the form

P (x) =

e(cid:0)E(x)

Z

;

(30.1)

not only E(x) but also its gradient with respect to x can be readily evaluated.
It seems wasteful to use a simple random-walk Metropolis method when this
gradient is available { the gradient indicates which direction one should go in
to (cid:12)nd states that have higher probability!

Overview of Hamiltonian Monte Carlo

In the Hamiltonian Monte Carlo method, the state space x is augmented by
momentum variables p, and there is an alternation of two types of proposal.
The (cid:12)rst proposal randomizes the momentum variable, leaving the state x un-
changed. The second proposal changes both x and p using simulated Hamil-
tonian dynamics as de(cid:12)ned by the Hamiltonian

H(x; p) = E(x) + K(p);

(30.2)

where K(p) is a ‘kinetic energy’ such as K(p) = pTp=2. These two proposals
are used to create (asymptotically) samples from the joint density

PH (x; p) =

1
ZH

exp[(cid:0)H(x; p)] =

1
ZH

exp[(cid:0)E(x)] exp[(cid:0)K(p)]:

(30.3)

This density is separable, so the marginal distribution of x is the desired
distribution exp[(cid:0)E(x)]=Z. So, simply discarding the momentum variables,
we obtain a sequence of samples fx(t)g that asymptotically come from P (x).

387

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

388

30 | E(cid:14)cient Monte Carlo Methods

Algorithm 30.1. Octave source
code for the Hamiltonian Monte
Carlo method.

g = gradE ( x ) ;
E = findE ( x ) ;

# set gradient using initial x
# set objective function too

for l = 1:L

p = randn ( size(x) ) ;
H = p’ * p / 2 + E ;

# loop L times
# initial momentum is Normal(0,1)
# evaluate H(x,p)

xnew = x ;
for tau = 1:Tau

gnew = g ;

# make Tau ‘leapfrog’ steps

p = p - epsilon * gnew / 2 ; # make half-step in p
xnew = xnew + epsilon * p ;
gnew = gradE ( xnew ) ;
p = p - epsilon * gnew / 2 ; # make half-step in p

# make step in x
# find new gradient

endfor

Enew = findE ( xnew ) ;
Hnew = p’ * p / 2 + Enew ;
dH = Hnew - H ;

# find new value of H

# Decide whether to accept

if ( dH < 0 )
accept = 1 ;
elseif ( rand() < exp(-dH) ) accept = 1 ;
else
accept = 0 ;
endif

if ( accept )
g = gnew ;

endif

endfor

x = xnew ;

E = Enew ;

Hamiltonian Monte Carlo
1

(a)

(c)

0.5

0

-0.5

-1

1

(b)

0.5

0

-0.5

-1

-1

-0.5

0

0.5

1

(d)

Simple Metropolis

Figure 30.2. (a,b) Hamiltonian
Monte Carlo used to generate
samples from a bivariate Gaussian
with correlation (cid:26) = 0:998. (c,d)
For comparison, a simple
random-walk Metropolis method,
given equal computer time.

-1

-0.5

0

0.5

1

1

0.5

0

-0.5

-1

1

0.5

0

-0.5

-1

-1.5

-1.5 -1 -0.5 0

0.5

1

-1

-0.5

0

0.5

1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

389

30.1: Hamiltonian Monte Carlo

Details of Hamiltonian Monte Carlo

The (cid:12)rst proposal, which can be viewed as a Gibbs sampling update, draws a
new momentum from the Gaussian density exp[(cid:0)K(p)]=ZK . This proposal is
always accepted. During the second, dynamical proposal, the momentum vari-
able determines where the state x goes, and the gradient of E(x) determines
how the momentum p changes, in accordance with the equations

_x = p

_p = (cid:0)

@E(x)

@x

:

(30.4)

(30.5)

Because of the persistent motion of x in the direction of the momentum p
during each dynamical proposal, the state of the system tends to move a
distance that goes linearly with the computer time, rather than as the square
root.

The second proposal is accepted in accordance with the Metropolis rule.
If the simulation of the Hamiltonian dynamics is numerically perfect then
the proposals are accepted every time, because the total energy H(x; p) is a
constant of the motion and so a in equation (29.31) is equal to one. If the
simulation is imperfect, because of (cid:12)nite step sizes for example, then some of
the dynamical proposals will be rejected. The rejection rule makes use of the
change in H(x; p), which is zero if the simulation is perfect. The occasional
rejections ensure that, asymptotically, we obtain samples (x(t); p(t)) from the
required joint density PH (x; p).

The source code in (cid:12)gure 30.1 describes a Hamiltonian Monte Carlo method
that uses the ‘leapfrog’ algorithm to simulate the dynamics on the function
findE(x), whose gradient is found by the function gradE(x). Figure 30.2
shows this algorithm generating samples from a bivariate Gaussian whose en-
ergy function is E(x) = 1

2 xTAx with

A =(cid:20)

250:25 (cid:21) ;
250:25 (cid:0)249:75
(cid:0)249:75

corresponding to a variance{covariance matrix of

(cid:20) 1

0:998

0:998 1

(cid:21) :

(30.6)

(30.7)

In (cid:12)gure 30.2a, starting from the state marked by the arrow, the solid line
represents two successive trajectories generated by the Hamiltonian dynamics.
The squares show the endpoints of these two trajectories. Each trajectory
consists of Tau = 19 ‘leapfrog’ steps with epsilon = 0:055. These steps are
indicated by the crosses on the trajectory in the magni(cid:12)ed inset. After each
trajectory, the momentum is randomized. Here, both trajectories are accepted;
the errors in the Hamiltonian were only +0:016 and (cid:0)0:06 respectively.
Figure 30.2b shows how a sequence of four trajectories converges from an
initial condition, indicated by the arrow, that is not close to the typical set
of the target distribution. The trajectory parameters Tau and epsilon were
randomized for each trajectory using uniform distributions with means 19 and
0.055 respectively. The (cid:12)rst trajectory takes us to a new state, ((cid:0)1:5;(cid:0)0:5),
similar in energy to the (cid:12)rst state. The second trajectory happens to end in
a state nearer the bottom of the energy landscape. Here, since the potential
energy E is smaller, the kinetic energy K = p2=2 is necessarily larger than it
was at the start of the trajectory. When the momentum is randomized before
the third trajectory, its kinetic energy becomes much smaller. After the fourth

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

390

(a)

(b)

1

0.5

0

-0.5

-1

Gibbs sampling

Overrelaxation

-1

-0.5

0

0.5

1

1

0.5

0

-0.5

-1

0

-0.2

-0.4

-0.6

-0.8

-1

-1

-0.5

0

0.5

1

30 | E(cid:14)cient Monte Carlo Methods

Figure 30.3. Overrelaxation
contrasted with Gibbs sampling
for a bivariate Gaussian with
correlation (cid:26) = 0:998. (a) The
state sequence for 40 iterations,
each iteration involving one
update of both variables. The
overrelaxation method had
(cid:11) = (cid:0)0:98. (This excessively large
value is chosen to make it easy to
see how the overrelaxation method
reduces random walk behaviour.)
The dotted line shows the contour
xT(cid:6)(cid:0)1x = 1. (b) Detail of (a),
showing the two steps making up
each iteration. (c) Time-course of
the variable x1 during 2000
iterations of the two methods.
The overrelaxation method had
(cid:11) = (cid:0)0:89. (After Neal (1995).)

(c)

Gibbs sampling

-1 -0.8-0.6-0.4-0.2 0

3
2
1
0
-1
-2
-3

3
2
1
0
-1
-2
-3

200

400
0
Overrelaxation

600

800

1000

1200

1400

1600

1800

2000

0

200

400

600

800

1000

1200

1400

1600

1800

2000

trajectory has been simulated, the state appears to have become typical of the
target density.

Figures 30.2(c) and (d) show a random-walk Metropolis method using a
Gaussian proposal density to sample from the same Gaussian distribution,
starting from the initial conditions of (a) and (b) respectively. In (c) the step
size was adjusted such that the acceptance rate was 58%. The number of
proposals was 38 so the total amount of computer time used was similar to
that in (a). The distance moved is small because of random walk behaviour.
In (d) the random-walk Metropolis method was used and started from the
same initial condition as (b) and given a similar amount of computer time.

30.2 Overrelaxation

The method of overrelaxation is a method for reducing random walk behaviour
in Gibbs sampling. Overrelaxation was originally introduced for systems in
which all the conditional distributions are Gaussian.

An example of a joint distribution that is not Gaussian but whose conditional
distributions are all Gaussian is P (x; y) = exp((cid:0)x2y2 (cid:0) x2 (cid:0) y2)=Z.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

30.2: Overrelaxation

391

Overrelaxation for Gaussian conditional distributions

In ordinary Gibbs sampling, one draws the new value x(t+1)
of the current
variable xi from its conditional distribution, ignoring the old value x(t)
. The
i
state makes lengthy random walks in cases where the variables are strongly
correlated, as illustrated in the left-hand panel of (cid:12)gure 30.3. This (cid:12)gure uses
a correlated Gaussian distribution as the target density.

i

In Adler’s (1981) overrelaxation method, one instead samples x(t+1)

from
a Gaussian that is biased to the opposite side of the conditional distribution.
If the conditional distribution of xi is Normal((cid:22); (cid:27)2) and the current value of
xi is x(t)
i

, then Adler’s method sets xi to

i

x(t+1)
i

= (cid:22) + (cid:11)(x(t)

(30.8)
where (cid:23) (cid:24) Normal(0; 1) and (cid:11) is a parameter between (cid:0)1 and 1, usually set to
a negative value. (If (cid:11) is positive, then the method is called under-relaxation.)
Exercise 30.1.[2 ] Show that this individual transition leaves invariant the con-

i (cid:0) (cid:22)) + (1 (cid:0) (cid:11)2)1=2(cid:27)(cid:23);

ditional distribution xi (cid:24) Normal((cid:22); (cid:27)2).

A single iteration of Adler’s overrelaxation, like one of Gibbs sampling, updates
each variable in turn as indicated in equation (30.8). The transition matrix
T (x0; x) de(cid:12)ned by a complete update of all variables in some (cid:12)xed order does
not satisfy detailed balance. Each individual transition for one coordinate
just described does satisfy detailed balance { so the overall chain gives a valid
sampling strategy which converges to the target density P (x) { but when we
form a chain by applying the individual transitions in a (cid:12)xed sequence, the
overall chain is not reversible. This temporal asymmetry is the key to why
overrelaxation can be bene(cid:12)cial. If, say, two variables are positively correlated,
then they will (on a short timescale) evolve in a directed manner instead of by
random walk, as shown in (cid:12)gure 30.3. This may signi(cid:12)cantly reduce the time
required to obtain independent samples.

Exercise 30.2.[3 ] The transition matrix T (x0; x) de(cid:12)ned by a complete update
of all variables in some (cid:12)xed order does not satisfy detailed balance. If
the updates were in a random order, then T would be symmetric. Inves-
tigate, for the toy two-dimensional Gaussian distribution, the assertion
that the advantages of overrelaxation are lost if the overrelaxed updates
are made in a random order.

Ordered Overrelaxation

The overrelaxation method has been generalized by Neal (1995) whose ordered
overrelaxation method is applicable to any system where Gibbs sampling is
used. In ordered overrelaxation, instead of taking one sample from the condi-
tional distribution P (xi jfxjgj6=i), we create K such samples x(1)
; : : : ; x(K)
where K might be set to twenty or so. Often, generating K (cid:0) 1 extra samples
adds a negligible computational cost to the initial computations required for
making the (cid:12)rst sample. The points fx(k)
i g are then sorted numerically, and
the current value of xi is inserted into the sorted list, giving a list of K + 1
points. We give them ranks 0; 1; 2; : : : ; K. Let (cid:20) be the rank of the current
value of xi in the list. We set x0i to the value that is an equal distance from
the other end of the list, that is, the value with rank K (cid:0) (cid:20). The role played
by Adler’s (cid:11) parameter is here played by the parameter K. When K = 1, we
obtain ordinary Gibbs sampling. For practical purposes Neal estimates that
ordered overrelaxation may speed up a simulation by a factor of ten or twenty.

; x(2)

i

i

i

,

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

392

30 | E(cid:14)cient Monte Carlo Methods

30.3 Simulated annealing

A third technique for speeding convergence is simulated annealing. In simu-
lated annealing, a ‘temperature’ parameter is introduced which, when large,
allows the system to make transitions that would be improbable at temper-
ature 1. The temperature is set to a large value and gradually reduced to
1. This procedure is supposed to reduce the chance that the simulation gets
stuck in an unrepresentative probability island.

We asssume that we wish to sample from a distribution of the form

P (x) =

e(cid:0)E(x)

Z

(30.9)

where E(x) can be evaluated. In the simplest simulated annealing method,
we instead sample from the distribution

PT (x) = 1

Z(T ) e(cid:0)

E(x)

T

(30.10)

and decrease T gradually to 1.

Often the energy function can be separated into two terms,

E(x) = E0(x) + E1(x);

(30.11)

of which the (cid:12)rst term is ‘nice’ (for example, a separable function of x) and the
second is ‘nasty’. In these cases, a better simulated annealing method might
make use of the distribution

P 0T (x) = 1

Z 0(T ) e(cid:0)E0(x)(cid:0) E1(x)/T

(30.12)

with T gradually decreasing to 1. In this way, the distribution at high tem-
peratures reverts to a well-behaved distribution de(cid:12)ned by E0.

Simulated annealing is often used as an optimization method, where the
aim is to (cid:12)nd an x that minimizes E(x), in which case the temperature is
decreased to zero rather than to 1.

As a Monte Carlo method, simulated annealing as described above doesn’t
sample exactly from the right distribution, because there is no guarantee that
the probability of falling into one basin of the energy is equal to the total prob-
ability of all the states in that basin. The closely related ‘simulated tempering’
method (Marinari and Parisi, 1992) corrects the biases introduced by the an-
nealing process by making the temperature itself a random variable that is
updated in Metropolis fashion during the simulation. Neal’s (1998) ‘annealed
importance sampling’ method removes the biases introduced by annealing by
computing importance weights for each generated point.

30.4 Skilling’s multi-state leapfrog method

A fourth method for speeding up Monte Carlo simulations, due to John
Skilling, has a similar spirit to overrelaxation, but works in more dimensions.
This method is applicable to sampling from a distribution over a continuous
state space, and the sole requirement is that the energy E(x) should be easy
to evaluate. The gradient is not used. This leapfrog method is not intended to
be used on its own but rather in sequence with other Monte Carlo operators.
Instead of moving just one state vector x around the state space, as was
the case for all the Monte Carlo methods discussed thus far, Skilling’s leapfrog
method simultaneously maintains a set of S state vectors fx(s)g, where S

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

30.4: Skilling’s multi-state leapfrog method

393

might be six or twelve. The aim is that all S of these vectors will represent
independent samples from the same distribution P (x).

Skilling’s leapfrog makes a proposal for the new state x(s)0, which is ac-
cepted or rejected in accordance with the Metropolis method, by leapfrogging
the current state x(s) over another state vector x(t):

x(s)0 = x(t) + (x(t) (cid:0) x(s)) = 2x(t) (cid:0) x(s):

(30.13)

All the other state vectors are left where they are, so the acceptance probability
depends only on the change in energy of x(s).

Which vector, t, is the partner for the leapfrog event can be chosen in
various ways. The simplest method is to select the partner at random from
the other vectors.
It might be better to choose t by selecting one of the
nearest neighbours x(s) { nearest by any chosen distance function { as long
as one then uses an acceptance rule that ensures detailed balance by checking
whether point t is still among the nearest neighbours of the new point, x(s)0.

x(s)

x(s)0

x(t)

Why the leapfrog is a good idea

Imagine that the target density P (x) has strong correlations { for example,
the density might be a needle-like Gaussian with width (cid:15) and length L(cid:15), where
L (cid:29) 1. As we have emphasized, motion around such a density by standard
methods proceeds by a slow random walk.

Imagine now that our set of S points is lurking initially in a location that
is probable under the density, but in an inappropriately small ball of size (cid:15).
Now, under Skilling’s leapfrog method, a typical (cid:12)rst move will take the point
a little outside the current ball, perhaps doubling its distance from the centre
of the ball. After all the points have had a chance to move, the ball will have
increased in size; if all the moves are accepted, the ball will be bigger by a
factor of two or so in all dimensions. The rejection of some moves will mean
that the ball containing the points will probably have elongated in the needle’s
long direction by a factor of, say, two. After another cycle through the points,
the ball will have grown in the long direction by another factor of two. So the
typical distance travelled in the long dimension grows exponentially with the
number of iterations.

Now, maybe a factor of two growth per iteration is on the optimistic side;
but even if the ball only grows by a factor of, let’s say, 1.1 per iteration, the
growth is nevertheless exponential. It will only take a number of iterations
proportional to log L= log(1:1) for the long dimension to be explored.

. Exercise 30.3.[2, p.398] Discuss how the e(cid:11)ectiveness of Skilling’s method scales
with dimensionality, using a correlated N -dimensional Gaussian distri-
bution as an example. Find an expression for the rejection probability,
assuming the Markov chain is at equilibrium. Also discuss how it scales
with the strength of correlation among the Gaussian variables.
[Hint:
Skilling’s method is invariant under a(cid:14)ne transformations, so the rejec-
tion probability at equilibrium can be found by looking at the case of a
separable Gaussian.]

This method has some similarity to the ‘adaptive direction sampling’ method

of Gilks et al. (1994) but the leapfrog method is simpler and can be applied
to a greater variety of distributions.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

394

30 | E(cid:14)cient Monte Carlo Methods

30.5 Monte Carlo algorithms as communication channels

It may be a helpful perspective, when thinking about speeding up Monte Carlo
methods, to think about the information that is being communicated. Two
communications take place when a sample from P (x) is being generated.

First, the selection of a particular x from P (x) necessarily requires that
at least log 1=P (x) random bits be consumed. [Recall the use of inverse arith-
metic coding as a method for generating samples from given distributions
(section 6.3).]

Second, the generation of a sample conveys information about P (x) from
the subroutine that is able to evaluate P (cid:3)(x) (and from any other subroutines
that have access to properties of P (cid:3)(x)).

Consider a dumb Metropolis method, for example. In a dumb Metropolis
method, the proposals Q(x0; x) have nothing to do with P (x). Properties
of P (x) are only involved in the algorithm at the acceptance step, when the
ratio P (cid:3)(x0)=P (cid:3)(x) is computed. The channel from the true distribution P (x)
to the user who is interested in computing properties of P (x) thus passes
through a bottleneck: all the information about P is conveyed by the string of
acceptances and rejections. If P (x) were replaced by a di(cid:11)erent distribution
P2(x), the only way in which this change would have an in(cid:13)uence is that the
string of acceptances and rejections would be changed. I am not aware of much
use being made of this information-theoretic view of Monte Carlo algorithms,
but I think it is an instructive viewpoint: if the aim is to obtain information
about properties of P (x) then presumably it is helpful to identify the channel
through which this information (cid:13)ows, and maximize the rate of information
transfer.

Example 30.4. The information-theoretic viewpoint o(cid:11)ers a simple justi(cid:12)cation
for the widely-adopted rule of thumb, which states that the parameters of
a dumb Metropolis method should be adjusted such that the acceptance
rate is about one half. Let’s call the acceptance history, that is, the
binary string of accept or reject decisions, a. The information learned
about P (x) after the algorithm has run for T steps is less than or equal to
the information content of a, since all information about P is mediated
by a. And the information content of a is upper-bounded by T H2(f ),
where f is the acceptance rate. This bound on information acquired
about P is maximized by setting f = 1=2.

Another helpful analogy for a dumb Metropolis method is an evolutionary
one. Each proposal generates a progeny x0 from the current state x. These two
individuals then compete with each other, and the Metropolis method uses a
noisy survival-of-the-(cid:12)ttest rule. If the progeny x0 is (cid:12)tter than the parent (i.e.,
P (cid:3)(x0) > P (cid:3)(x), assuming the Q=Q factor is unity) then the progeny replaces
the parent. The survival rule also allows less-(cid:12)t progeny to replace the parent,
sometimes. Insights about the rate of evolution can thus be applied to Monte
Carlo methods.

Exercise 30.5.[3 ] Let x 2 f0; 1gG and let P (x) be a separable distribution,

P (x) =Yg

p(xg);

(30.14)

with p(0) = p0 and p(1) = p1, for example p1 = 0:1. Let the proposal
density of a dumb Metropolis algorithm Q involve (cid:13)ipping a fraction m
of the G bits in the state x. Analyze how long it takes for the chain to

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

30.6: Multi-state methods

395

converge to the target density as a function of m. Find the optimal m
and deduce how long the Metropolis method must run for.

Compare the result with the results for an evolving population under
natural selection found in Chapter 19.

The insight that the fastest progress that a standard Metropolis method
can make, in information terms, is about one bit per iteration, gives a strong
motivation for speeding up the algorithm. This chapter has already reviewed
several methods for reducing random-walk behaviour. Do these methods also
speed up the rate at which information is acquired?

Exercise 30.6.[4 ] Does Gibbs sampling, which is a smart Metropolis method
whose proposal distributions do depend on P (x), allow information about
P (x) to leak out at a rate faster than one bit per iteration? Find toy
examples in which this question can be precisely investigated.

Exercise 30.7.[4 ] Hamiltonian Monte Carlo is another smart Metropolis method
in which the proposal distributions depend on P (x). Can Hamiltonian
Monte Carlo extract information about P (x) at a rate faster than one
bit per iteration?

Exercise 30.8.[5 ] In importance sampling, the weight wr = P (cid:3)(x(r))=Q(cid:3)(x(r)),
a (cid:13)oating-point number, is computed and retained until the end of the
computation.
In contrast, in the dumb Metropolis method, the ratio
a = P (cid:3)(x0)=P (cid:3)(x) is reduced to a single bit (‘is a bigger than or smaller
than the random number u?’). Thus in principle importance sampling
preserves more information about P (cid:3) than does dumb Metropolis. Can
you (cid:12)nd a toy example in which this extra information does indeed lead
to faster convergence of importance sampling than Metropolis? Can
you design a Markov chain Monte Carlo algorithm that moves around
adaptively, like a Metropolis method, and that retains more useful in-
formation about the value of P (cid:3), like importance sampling?

In Chapter 19 we noticed that an evolving population of N individuals can
make faster evolutionary progress if the individuals engage in sexual reproduc-
tion. This observation motivates looking at Monte Carlo algorithms in which
multiple parameter vectors x are evolved and interact.

30.6 Multi-state methods

In a multi-state method, multiple parameter vectors x are maintained; they
evolve individually under moves such as Metropolis and Gibbs; there are also
interactions among the vectors. The intention is either that eventually all the
vectors x should be samples from P (x) (as illustrated by Skilling’s leapfrog
method), or that information associated with the (cid:12)nal vectors x should allow
us to approximate expectations under P (x), as in importance sampling.

Genetic methods

Genetic algorithms are not often described by their proponents as Monte Carlo
algorithms, but I think this is the correct categorization, and an ideal genetic
algorithm would be one that can be proved to be a valid Monte Carlo algorithm
that converges to a speci(cid:12)ed density.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

396

30 | E(cid:14)cient Monte Carlo Methods

I’ll use R to denote the number of vectors in the population. We aim to

have P (cid:3)(fx(r)gR
three types.

1 ) =Q P (cid:3)(x(r)). A genetic algorithm involves moves of two or
First, individual moves in which one state vector is perturbed, x(r) ! x(r)0,

which could be performed using any of the Monte Carlo methods we have
mentioned so far.

Second, we allow crossover moves of the form x; y ! x0; y0; in a typical
crossover move, the progeny x0 receives half his state vector from one parent,
x, and half from the other, y; the secret of success in a genetic algorithm is
that the parameter x must be encoded in such a way that the crossover of
two independent states x and y, both of which have good (cid:12)tness P (cid:3), should
have a reasonably good chance of producing progeny who are equally (cid:12)t. This
constraint is a hard one to satisfy in many problems, which is why genetic
algorithms are mainly talked about and hyped up, and rarely used by serious
experts. Having introduced a crossover move x; y ! x0; y0, we need to choose
an acceptance rule. One easy way to obtain a valid algorithm is to accept or
reject the crossover proposal using the Metropolis rule with P (cid:3)(fx(r)gR
1 ) as
the target density { this involves comparing the (cid:12)tnesses before and after the
crossover using the ratio

P (cid:3)(x0)P (cid:3)(y0)
P (cid:3)(x)P (cid:3)(y)

:

(30.15)

If the crossover operator is reversible then we have an easy proof that this
procedure satis(cid:12)es detailed balance and so is a valid component in a chain
converging to P (cid:3)(fx(r)gR
1 ).
. Exercise 30.9.[3 ] Discuss whether the above two operators, individual varia-
tion and crossover with the Metropolis acceptance rule, will give a more
e(cid:14)cient Monte Carlo method than a standard method with only one
state vector and no crossover.

The reason why the sexual community could acquire information faster than
the asexual community in Chapter 19 was because the crossover operation
produced diversity with standard deviation pG, then the Blind Watchmaker
was able to convey lots of information about the (cid:12)tness function by killing
o(cid:11) the less (cid:12)t o(cid:11)spring. The above two operators do not o(cid:11)er a speed-up of
pG compared with standard Monte Carlo methods because there is no killing.
What’s required, in order to obtain a speed-up, is two things: multiplication
and death; and at least one of these must operate selectively. Either we must
kill o(cid:11) the less-(cid:12)t state vectors, or we must allow the more-(cid:12)t state vectors to
give rise to more o(cid:11)spring. While it’s easy to sketch these ideas, it is hard to
de(cid:12)ne a valid method for doing it.

Exercise 30.10.[5 ] Design a birth rule and a death rule such that the chain

converges to P (cid:3)(fx(r)gR
1 ).

I believe this is still an open research problem.

Particle (cid:12)lters

Particle (cid:12)lters, which are particularly popular in inference problems involving
temporal tracking, are multistate methods that mix the ideas of importance
sampling and Markov chain Monte Carlo. See Isard and Blake (1996), Isard
and Blake (1998), Berzuini et al. (1997), Berzuini and Gilks (2001), Doucet
et al. (2001).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

30.7: Methods that do not necessarily help

397

30.7 Methods that do not necessarily help

It is common practice to use many initial conditions for a particular Markov
chain ((cid:12)gure 29.19). If you are worried about sampling well from a complicated
density P (x), can you ensure the states produced by the simulations are well
distributed about the typical set of P (x) by ensuring that the initial points
are ‘well distributed about the whole state space’ ?

The answer is, unfortunately, no.

In hierarchical Bayesian models, for
example, a large number of parameters fxng may be coupled together via an-
other parameter (cid:12) (known as a hyperparameter). For example, the quantities
fxng might be independent noise signals, and (cid:12) might be the inverse-variance
of the noise source. The joint distribution of (cid:12) and fxng might be

P ((cid:12);fxng) = P ((cid:12))

= P ((cid:12))

P (xn j (cid:12))

1

Z((cid:12))

e(cid:0)(cid:12)x2

n=2;

N

Yn=1
Yn=1

N

where Z((cid:12)) = p2(cid:25)=(cid:12) and P ((cid:12)) is a broad distribution describing our igno-
rance about the noise level. For simplicity, let’s leave out all the other variables
{ data and such { that might be involved in a realistic problem. Let’s imagine
that we want to sample e(cid:11)ectively from P ((cid:12);fxng) by Gibbs sampling { alter-
nately sampling (cid:12) from the conditional distribution P ((cid:12) j xn) then sampling all
the xn from their conditional distributions P (xn j (cid:12)). [The resulting marginal
distribution of (cid:12) should asymptotically be the broad distribution P ((cid:12)).]
If N is large then the conditional distribution of (cid:12) given any particular
setting of fxng will be tightly concentrated on a particular most-probable value
of (cid:12), with width proportional to 1=pN . Progress up and down the (cid:12)-axis will
therefore take place by a slow random walk with steps of size / 1=pN .

So, to the initialization strategy. Can we (cid:12)nesse our slow convergence
problem by using initial conditions located ‘all over the state space’ ? Sadly,
If we distribute the points fxng widely, what we are actually doing is
no.
favouring an initial value of the noise level 1=(cid:12) that is large. The random
walk of the parameter (cid:12) will thus tend, after the (cid:12)rst drawing of (cid:12) from
P ((cid:12) j xn), always to start o(cid:11) from one end of the (cid:12)-axis.

Further reading

The Hamiltonian Monte Carlo method (Duane et al., 1987) is reviewed in Neal
(1993b). This excellent tome also reviews a huge range of other Monte Carlo
methods, including the related topics of simulated annealing and free energy
estimation.

30.8 Further exercises

Exercise 30.11.[4 ] An important detail of the Hamiltonian Monte Carlo method
is that the simulation of the Hamiltonian dynamics, while it may be in-
accurate, must be perfectly reversible, in the sense that if the initial con-
dition (x; p) goes to (x0; p0), then the same simulator must take (x0;(cid:0)p0)
to (x;(cid:0)p), and the inaccurate dynamics must conserve state-space vol-
ume. [The leapfrog method in algorithm 30.1 satis(cid:12)es these rules.]

Explain why these rules must be satis(cid:12)ed and create an example illus-
trating the problems that arise if they are not.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

398

30 | E(cid:14)cient Monte Carlo Methods

Exercise 30.12.[4 ] A multi-state idea for slice sampling. Investigate the follow-
ing multi-state method for slice sampling. As in Skilling’s multi-state
leapfrog method (section 30.4), maintain a set of S state vectors fx(s)g.
Update one state vector x(s) by one-dimensional slice sampling in a di-
rection y determined by picking two other state vectors x(v) and x(w)
at random and setting y = x(v) (cid:0) x(w).
Investigate this method on toy
problems such as a highly-correlated multivariate Gaussian distribution.
Bear in mind that if S (cid:0) 1 is smaller than the number of dimensions
N then this method will not be ergodic by itself, so it may need to be
mixed with other methods. Are there classes of problems that are better
solved by this slice-sampling method than by the standard methods for
picking y such as cycling through the coordinate axes or picking u at
random from a Gaussian distribution?

30.9 Solutions

x(v)

x(s)

x(w)

Solution to exercise 30.3 (p.393). Consider the spherical Gaussian distribution
where all components have mean zero and variance 1. In one dimension, the
nth, if x(1)

n , we obtain the proposed coordinate

n leapfrogs over x(2)

(x(1)

n )0 = 2x(2)

n (cid:0) x(1)
n :

(30.16)

n and x(2)

Assuming that x(1)
(x(1)
in energy contributed by this one dimension will be

n are Gaussian random variables from Normal(0; 1),
n )0 is Gaussian from Normal(0; (cid:27)2), where (cid:27)2 = 22+((cid:0)1)2 = 5. The change

n )2i = 2(x(2)

n )2 (cid:0) 2x(2)

n x(1)
n

(30.17)

1

2h(2x(2)

n (cid:0) x(1)

n )2 (cid:0) (x(1)

so the typical change in energy is 2h(x(2)

n )2i = 2. This positive change is bad
news. In N dimensions, the typical change in energy when a leapfrog move is
made, at equilibrium, is thus +2N . The probability of acceptance of the move
scales as

e(cid:0)2N :

(30.18)

This implies that Skilling’s method, as described, is not e(cid:11)ective in very high-
dimensional problems { at least, not once convergence has occurred. Nev-
ertheless it has the impressive advantage that its convergence properties are
independent of the strength of correlations between the variables { a property
that not even the Hamiltonian Monte Carlo and overrelaxation methods o(cid:11)er.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 31

Some of the neural network models that we will encounter are related to Ising
models, which are idealized magnetic systems. It is not essential to understand
the statistical physics of Ising models to understand these neural networks, but
I hope you’ll (cid:12)nd them helpful.

Ising models are also related to several other topics in this book. We will
use exact tree-based computation methods like those introduced in Chapter
25 to evaluate properties of interest in Ising models. Ising models o(cid:11)er crude
models for binary images. And Ising models relate to two-dimensional con-
strained channels (cf. Chapter 17): a two-dimensional bar-code in which a
black dot may not be completely surrounded by black dots, and a white dot
may not be completely surrounded by white dots, is similar to an antiferro-
magnetic Ising model at low temperature. Evaluating the entropy of this Ising
model is equivalent to evaluating the capacity of the constrained channel for
conveying bits.

If you would like to jog your memory on statistical physics and thermody-
namics, you might (cid:12)nd Appendix B helpful. I also recommend the book by
Reif (1965).

399

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31

Ising Models

An Ising model is an array of spins (e.g., atoms that can take states (cid:6)1) that
are magnetically coupled to each other. If one spin is, say, in the +1 state
then it is energetically favourable for its immediate neighbours to be in the
same state, in the case of a ferromagnetic model, and in the opposite state, in
the case of an antiferromagnet. In this chapter we discuss two computational
techniques for studying Ising models.

Let the state x of an Ising model with N spins be a vector in which each
component xn takes values (cid:0)1 or +1. If two spins m and n are neighbours we
write (m; n) 2 N . The coupling between neighbouring spins is J. We de(cid:12)ne
Jmn = J if m and n are neighbours and Jmn = 0 otherwise. The energy of a
state x is

2Xm;n

Jmnxmxn +Xn

Hxn# ;

E(x; J; H) = (cid:0)" 1

(31.1)

where H is the applied (cid:12)eld. If J > 0 then the model is ferromagnetic, and
if J < 0 it is antiferromagnetic. We’ve included the factor of 1/2 because each
pair is counted twice in the (cid:12)rst sum, once as (m; n) and once as (n; m). At
equilibrium at temperature T , the probability that the state is x is

P (xj (cid:12); J; H) =

1

Z((cid:12); J; H)

exp[(cid:0)(cid:12)E(x; J; H)] ;

where (cid:12) = 1=kBT , kB is Boltzmann’s constant, and

Z((cid:12); J; H) (cid:17)Xx

exp[(cid:0)(cid:12)E(x; J; H)] :

Relevance of Ising models

(31.2)

(31.3)

Ising models are relevant for three reasons.

Ising models are important (cid:12)rst as models of magnetic systems that have
a phase transition. The theory of universality in statistical physics shows that
all systems with the same dimension (here, two), and the same symmetries,
have equivalent critical properties, i.e., the scaling laws shown by their phase
transitions are identical. So by studying Ising models we can (cid:12)nd out not only
about magnetic phase transitions but also about phase transitions in many
other systems.

Second, if we generalize the energy function to

E(x; J; h) = (cid:0)" 1

2Xm;n

Jmnxmxn +Xn

hnxn# ;

(31.4)

where the couplings Jmn and applied (cid:12)elds hn are not constant, we obtain
a family of models known as ‘spin glasses’ to physicists, and as ‘Hop(cid:12)eld

400

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31 | Ising Models

401

networks’ or ‘Boltzmann machines’ to the neural network community. In some
of these models, all spins are declared to be neighbours of each other, in which
case physicists call the system an ‘in(cid:12)nite-range’ spin glass, and networkers
call it a ‘fully connected’ network.

Third, the Ising model is also useful as a statistical model in its own right.
In this chapter we will study Ising models using two di(cid:11)erent computational

techniques.

Some remarkable relationships in statistical physics

We would like to get as much information as possible out of our computations.
Consider for example the heat capacity of a system, which is de(cid:12)ned to be

@
@T

(cid:22)E;

C (cid:17)
Z Xx

1

(31.5)

(31.6)

where

(cid:22)E =

exp((cid:0)(cid:12)E(x)) E(x):

To work out the heat capacity of a system, we might naively guess that we have
to increase the temperature and measure the energy change. Heat capacity,
however, is intimately related to energy (cid:13)uctuations at constant temperature.
Let’s start from the partition function,

Z =Xx

exp((cid:0)(cid:12)E(x)):

The mean energy is obtained by di(cid:11)erentiation with respect to (cid:12):

@ ln Z

@(cid:12)

=

1

Z Xx

(cid:0)E(x) exp((cid:0)(cid:12)E(x)) = (cid:0) (cid:22)E:

A further di(cid:11)erentiation spits out the variance of the energy:

(31.7)

(31.8)

@2 ln Z
@(cid:12)2 =

1

Z Xx

E(x)2 exp((cid:0)(cid:12)E(x)) (cid:0) (cid:22)E2 = hE2i (cid:0) (cid:22)E2 = var(E):

(31.9)

But the heat capacity is also the derivative of (cid:22)E with respect to temperature:

@ (cid:22)E
@T

@
@T

= (cid:0)

@ ln Z

@(cid:12)

@2 ln Z

@(cid:12)2

@(cid:12)
@T

= (cid:0)

= (cid:0)var(E)((cid:0)1=kBT 2):

(31.10)

So for any system at temperature T ,

C =

var(E)
kBT 2 = kB(cid:12)2 var(E):

(31.11)

Thus if we can observe the variance of the energy of a system at equilibrium,
we can estimate its heat capacity.

I (cid:12)nd this an almost paradoxical relationship. Consider a system with
a (cid:12)nite set of states, and imagine heating it up. At high temperature, all
states will be equiprobable, so the mean energy will be essentially constant
and the heat capacity will be essentially zero. But on the other hand, with
all states being equiprobable, there will certainly be (cid:13)uctuations in energy.
So how can the heat capacity be related to the (cid:13)uctuations? The answer is
in the words ‘essentially zero’ above. The heat capacity is not quite zero at
high temperature, it just tends to zero. And it tends to zero as var(E)
kBT 2 , with

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

402

31 | Ising Models

the quantity var(E) tending to a constant at high temperatures. This 1=T 2
behaviour of the heat capacity of (cid:12)nite systems at high temperatures is thus
very general.

The 1=T 2 factor can be viewed as an accident of history.

perature scales had been de(cid:12)ned using (cid:12) = 1
capacity would be

If only tem-
kBT , then the de(cid:12)nition of heat

C ((cid:12)) (cid:17)

@ (cid:22)E
@(cid:12)

= var(E);

(31.12)

and heat capacity and (cid:13)uctuations would be identical quantities.

. Exercise 31.1.[2 ] [We will call the entropy of a physical system S rather than

H, while we are in a statistical physics chapter; we set kB = 1.]
The entropy of a system whose states are x, at temperature T = 1=(cid:12), is

where

(a) Show that

S =X p(x)[ln 1=p(x)]
exp[(cid:0)(cid:12)E(x)] :

Z((cid:12))

1

p(x) =

S = ln Z((cid:12)) + (cid:12) (cid:22)E((cid:12))

where (cid:22)E((cid:12)) is the mean energy of the system.

(b) Show that

S = (cid:0)

@F
@T

;

where the free energy F = (cid:0)kT ln Z and kT = 1=(cid:12).

31.1 Ising models { Monte Carlo simulation

(31.13)

(31.14)

(31.15)

(31.16)

In this section we study two-dimensional planar Ising models using a simple
Gibbs-sampling method. Starting from some initial state, a spin n is selected
at random, and the probability that it should be +1 given the state of the
other spins and the temperature is computed,

P (+1j bn) =

1

1 + exp((cid:0)2(cid:12)bn)

;

(31.17)

where (cid:12) = 1=kBT and bn is the local (cid:12)eld

bn = Xm:(m;n)2N

Jxm + H:

(31.18)

[The factor of 2 appears in equation (31.17) because the two spin states are
f+1;(cid:0)1g rather than f+1; 0g.] Spin n is set to +1 with that probability,
and otherwise to (cid:0)1; then the next spin to update is selected at random.
After su(cid:14)ciently many iterations, this procedure converges to the equilibrium
distribution (31.2). An alternative to the Gibbs sampling formula (31.17) is
the Metropolis algorithm, in which we consider the change in energy that
results from (cid:13)ipping the chosen spin from its current state xn,

(cid:1)E = 2xnbn;

and adopt this change in con(cid:12)guration with probability

P (accept; (cid:1)E; (cid:12)) =(cid:26)

1

(cid:1)E (cid:20) 0
exp((cid:0)(cid:12)(cid:1)E) (cid:1)E > 0:

(31.19)

(31.20)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

403

bbbb

bbbb

bbbb

bbbb

bbbb

bbbb

Figure 31.1. Rectangular Ising
model.

T

5

2.5

2.4

2.3

2

Figure 31.2. Sample states of
rectangular Ising models with
J = 1 at a sequence of
temperatures T .

31.1: Ising models { Monte Carlo simulation

This procedure has roughly double the probability of accepting energetically
unfavourable moves, so may be a more e(cid:14)cient sampler { but at very low tem-
peratures the relative merits of Gibbs sampling and the Metropolis algorithm
may be subtle.

Rectangular geometry

I (cid:12)rst simulated an Ising model with the rectangular geometry shown in (cid:12)g-
ure 31.1, and with periodic boundary conditions. A line between two spins
indicates that they are neighbours. I set the external (cid:12)eld H = 0 and con-
sidered the two cases J = (cid:6)1, which are a ferromagnet and antiferromagnet
respectively.
I started at a large temperature (T = 33; (cid:12) = 0:03) and changed the temper-
ature every I iterations, (cid:12)rst decreasing it gradually to T = 0:1; (cid:12) = 10, then
increasing it gradually back to a large temperature again. This procedure
gives a crude check on whether ‘equilibrium has been reached’ at each tem-
perature; if not, we’d expect to see some hysteresis in the graphs we plot. It
also gives an idea of the reproducibility of the results, if we assume that the two
runs, with decreasing and increasing temperature, are e(cid:11)ectively independent
of each other.

At each temperature I recorded the mean energy per spin and the standard

deviation of the energy, and the mean square value of the magnetization m,

m = 1

N Xn

xn:

(31.21)

One tricky decision that has to be made is how soon to start taking these
measurements after a new temperature has been established; it is di(cid:14)cult to
detect ‘equilibrium’ { or even to give a clear de(cid:12)nition of a system’s being ‘at
equilibrium’ ! [But in Chapter 32 we will see a solution to this problem.] My
crude strategy was to let the number of iterations at each temperature, I, be
a few hundred times the number of spins N , and to discard the (cid:12)rst 1/3 of
those iterations. With N = 100, I found I needed more than 100 000 iterations
to reach equilibrium at any given temperature.

Results for small N with J = 1.
I simulated an l (cid:2) l grid for l = 4; 5; : : : ; 10; 40; 64. Let’s have a quick think
about what results we expect. At low temperatures the system is expected
to be in a ground state. The rectangular Ising model with J = 1 has two
ground states, the all +1 state and the all (cid:0)1 state. The energy per spin of
either ground state is (cid:0)2. At high temperatures, the spins are independent,
all states are equally probable, and the energy is expected to (cid:13)uctuate around
a mean of 0 with a standard deviation proportional to 1=pN .

Let’s look at some results.

In all (cid:12)gures temperature T is shown with
kB = 1. The basic picture emerges with as few as 16 spins ((cid:12)gure 31.3,
top): the energy rises monotonically. As we increase the number of spins to
100 ((cid:12)gure 31.3, bottom) some new details emerge. First, as expected, the
(cid:13)uctuations at large temperature decrease as 1=pN . Second, the (cid:13)uctuations
at intermediate temperature become relatively bigger. This is the signature
of a ‘collective phenomenon’, in this case, a phase transition. Only systems
with in(cid:12)nite N show true phase transitions, but with N = 100 we are getting
a hint of the critical (cid:13)uctuations. Figure 31.5 shows details of the graphs for
N = 100 and N = 4096. Figure 31.2 shows a sequence of typical states from
the simulation of N = 4096 spins at a sequence of decreasing temperatures.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31 | Ising Models

Figure 31.3. Monte Carlo
simulations of rectangular Ising
models with J = 1. Mean energy
and (cid:13)uctuations in energy as a
function of temperature (left).
Mean square magnetization as a
function of temperature (right).
In the top row, N = 16, and the
bottom, N = 100. For even larger
N , see later (cid:12)gures.

T

Figure 31.4. Schematic diagram to
explain the meaning of a Schottky
anomaly. The curve shows the
heat capacity of two gases as a
function of temperature. The
lower curve shows a normal gas
whose heat capacity is an
increasing function of
temperature. The upper curve has
a small peak in the heat capacity,
which is known as a Schottky
anomaly (at least in Cambridge).
The peak is produced by the gas
having magnetic degrees of
freedom with a (cid:12)nite number of
accessible states.

404

N

Mean energy and (cid:13)uctuations

Mean square magnetization

0.5

0

-0.5

-1

-1.5

-2

0.5

0

-0.5

-1

-1.5

-2

y
g
r
e
n
E

y
g
r
e
n
E

16

100

1

1

Temperature

10

Temperature

10

n
o
i
t
a
z
i
t
e
n
g
a
M
 
e
r
a
u
q
S
 
n
a
e
M

n
o
i
t
a
z
i
t
e
n
g
a
M
 
e
r
a
u
q
S
 
n
a
e
M

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

1

Temperature

10

Temperature

10

Contrast with Schottky anomaly

A peak in the heat capacity, as a function of temperature, occurs in any system
that has a (cid:12)nite number of energy levels; a peak is not in itself evidence of a
phase transition. Such peaks were viewed as anomalies in classical thermody-
namics, since ‘normal’ systems with in(cid:12)nite numbers of energy levels (such as
a particle in a box) have heat capacities that are either constant or increasing
functions of temperature. In contrast, systems with a (cid:12)nite number of levels
produced small blips in the heat capacity graph ((cid:12)gure 31.4).

Let us refresh our memory of the simplest such system, a two-level system

with states x = 0 (energy 0) and x = 1 (energy (cid:15)). The mean energy is

E((cid:12)) = (cid:15)

exp((cid:0)(cid:12)(cid:15))
1 + exp((cid:0)(cid:12)(cid:15))

= (cid:15)

1

1 + exp((cid:12)(cid:15))

and the derivative with respect to (cid:12) is

dE=d(cid:12) = (cid:0)(cid:15)2

exp((cid:12)(cid:15))

[1 + exp((cid:12)(cid:15))]2 :

So the heat capacity is

C = dE=dT = (cid:0)

dE
d(cid:12)

1
kBT 2 =

(cid:15)2

exp((cid:12)(cid:15))

kBT 2

[1 + exp((cid:12)(cid:15))]2

(31.22)

(31.23)

(31.24)

and the (cid:13)uctuations in energy are given by var(E) = CkBT 2 = (cid:0)dE=d(cid:12),
which was evaluated in (31.23). The heat capacity and (cid:13)uctuations are plotted
in (cid:12)gure 31.6. The take-home message at this point is that whilst Schottky
anomalies do have a peak in the heat capacity, there is no peak in their
(cid:13)uctuations; the variance of the energy simply increases monotonically with
temperature to a value proportional to the number of independent spins. Thus
it is a peak in the (cid:13)uctuations that is interesting, rather than a peak in the
heat capacity. The Ising model has such a peak in its (cid:13)uctuations, as can be
seen in the second row of (cid:12)gure 31.5.

Rectangular Ising model with J = (cid:0)1
What do we expect to happen in the case J = (cid:0)1? The ground states of an
in(cid:12)nite system are the two checkerboard patterns ((cid:12)gure 31.7), and they have

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31.1: Ising models { Monte Carlo simulation

405

y
g
r
e
n
E

(a)

y
g
r
e
n
E

 
f

o

 

d
s

(b)

n
o

i
t

a
z
i
t

 

e
n
g
a
M
e
r
a
u
q
S
n
a
e
M

 

(c)

y
t
i
c
a
p
a
C

 
t

a
e
H

(d)

Figure 31.5. Detail of Monte Carlo
simulations of rectangular Ising
models with J = 1. (a) Mean
energy and (cid:13)uctuations in energy
as a function of temperature. (b)
Fluctuations in energy (standard
deviation). (c) Mean square
magnetization. (d) Heat capacity.

0

-0.5

-1

-1.5

-2

0.28

0.26

0.24

0.22

0.2

0.18

0.16

0.14

0.12

0.1

0.08

1

0.8

0.6

0.4

0.2

0

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

N = 100

N = 4096

0

-0.5

-1

-1.5

-2

2

2.5

3

3.5

4

4.5

5

2

2.5

3

3.5

4

4.5

5

0.05

0.045

0.04

0.035

0.03

0.025

0.02

0.015

1

0.8

0.6

0.4

0.2

0

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

2

2.5

3

3.5

4

4.5

5

2

2.5

3

3.5

4

4.5

5

2

2.5

3

3.5

4

4.5

5

2

2.5

3

3.5

4

4.5

5

2

2.5

3

3.5

4

4.5

5

2

2.5

3

3.5

4

4.5

5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
0.1

Heat Capacity
Var(E)

Figure 31.6. Schottky anomaly {
Heat capacity and (cid:13)uctuations in
energy as a function of
temperature for a two-level system
with separation (cid:15) = 1 and kB = 1.

1

Temperature

10

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

406

31 | Ising Models

energy per spin (cid:0)2, like the ground states of the J = 1 model. Can this analogy
be pressed further? A moment’s re(cid:13)ection will con(cid:12)rm that the two systems
are equivalent to each other under a checkerboard symmetry operation. If you
take an in(cid:12)nite J = 1 system in some state and (cid:13)ip all the spins that lie on
the black squares of an in(cid:12)nite checkerboard, and set J = (cid:0)1 ((cid:12)gure 31.8),
then the energy is unchanged. (The magnetization changes, of course.) So all
thermodynamic properties of the two systems are expected to be identical in
the case of zero applied (cid:12)eld.

But there is a subtlety lurking here. Have you spotted it? We are simu-
lating (cid:12)nite grids with periodic boundary conditions. If the size of the grid in
any direction is odd, then the checkerboard operation is no longer a symme-
try operation relating J = +1 to J = (cid:0)1, because the checkerboard doesn’t
match up at the boundaries. This means that for systems of odd size, the
ground state of a system with J = (cid:0)1 will have degeneracy greater than 2,
and the energy of those ground states will not be as low as (cid:0)2 per spin. So we
expect qualitative di(cid:11)erences between the cases J = (cid:6)1 in odd-sized systems.
These di(cid:11)erences are expected to be most prominent for small systems. The
frustrations are introduced by the boundaries, and the length of the boundary
grows as the square root of the system size, so the fractional in(cid:13)uence of this
boundary-related frustration on the energy and entropy of the system will de-
crease as 1=pN . Figure 31.9 compares the energies of the ferromagnetic and
antiferromagnetic models with N = 25. Here, the di(cid:11)erence is striking.

Figure 31.7. The two ground
states of a rectangular Ising model
with J = (cid:0)1.
J = (cid:0)1

J = +1

Figure 31.8. Two states of
rectangular Ising models with
J = (cid:6)1 that have identical energy.

J = +1

y
g
r
e
n
E

0.5

0

-0.5

-1

-1.5

-2

0.5

0

-0.5

-1

-1.5

-2

J = (cid:0)1

Figure 31.9. Monte Carlo
simulations of rectangular Ising
models with J = (cid:6)1 and N = 25.
Mean energy and (cid:13)uctuations in
energy as a function of
temperature.

1

Temperature

10

1

Temperature

10

Triangular Ising model

We can repeat these computations for a triangular Ising model. Do we expect
the triangular Ising model with J = (cid:6)1 to show di(cid:11)erent physical properties
from the rectangular Ising model? Presumably the J = 1 model will have
broadly similar properties to its rectangular counterpart. But the case J = (cid:0)1
is radically di(cid:11)erent from what’s gone before. Think about it:
there is no
unfrustrated ground state; in any state, there must be frustrations { pairs of
neighbours who have the same sign as each other. Unlike the case of the
rectangular model with odd size, the frustrations are not introduced by the
periodic boundary conditions. Every set of three mutually neighbouring spins
must be in a state of frustration, as shown in (cid:12)gure 31.10. (Solid lines show
‘happy’ couplings which contribute (cid:0)jJj to the energy; dashed lines show
‘unhappy’ couplings which contribute jJj.) Thus we certainly expect di(cid:11)erent
behaviour at low temperatures. In fact we might expect this system to have
a non-zero entropy at absolute zero. (‘Triangular model violates third law of
thermodynamics!’)

Let’s look at some results. Sample states are shown in (cid:12)gure 31.12, and
(cid:12)gure 31.11 shows the energy, (cid:13)uctuations, and heat capacity for N = 4096.

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

-1

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

+1

+1

+1

+1

+1

(a)

(b)

Figure 31.10. In an
antiferromagnetic triangular Ising
model, any three neighbouring
spins are frustrated. Of the eight
possible con(cid:12)gurations of three
spins, six have energy (cid:0)jJj (a),
and two have energy 3jJj (b).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31.2: Direct computation of partition function of Ising models

407

Note how di(cid:11)erent the results for J = (cid:6)1 are. There is no peak at all in
the standard deviation of the energy in the case J = (cid:0)1. This indicates that
the antiferromagnetic system does not have a phase transition to a state with
long-range order.

0.5

0

-0.5

-1

-1.5

-2

-2.5

-3

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

y
g
r
e
n
E

(a)

y
g
r
e
n
E

 
f

o

 

d
s

(b)

y
t
i
c
a
p
a
C

 
t

a
e
H

(c)

1

1

1

J = +1

10

Temperature

10

Temperature

10

Temperature

0.5

0

-0.5

-1

-1.5

-2

-2.5

-3

0.03

0.025

0.02

0.015

0.01

0.005

0

0.25

0.2

0.15

0.1

0.05

0

y
g
r
e
n
E

(d)

y
g
r
e
n
E

 
f

o

 

d
s

(e)

y
t
i
c
a
p
a
C

 
t

a
e
H

(f)

1

1

1

J = (cid:0)1

10

Temperature

10

Temperature

10

Temperature

31.2 Direct computation of partition function of Ising models

We now examine a completely di(cid:11)erent approach to Ising models. The trans-
fer matrix method is an exact and abstract approach that obtains physical
properties of the model from the partition function

Z((cid:12); J; b) (cid:17)Xx

exp[(cid:0)(cid:12)E(x; J; b)] ;

(31.25)

[As usual, Let kB = 1.] The free energy is given by F = (cid:0) 1

where the summation is over all states x, and the inverse temperature is
(cid:12) ln Z.
(cid:12) = 1=T .
The number of states is 2N , so direct computation of the partition function
is not possible for large N . To avoid enumerating all global states explicitly,
we can use a trick similar to the sum{product algorithm discussed in Chapter
25. We concentrate on models that have the form of a long thin strip of width
W with periodic boundary conditions in both directions, and we iterate along
the length of our model, working out a set of partial partition functions at one
location l in terms of partial partition functions at the previous location l (cid:0) 1.
Each iteration involves a summation over all the states at the boundary. This
operation is exponential in the width of the strip, W . The (cid:12)nal clever trick

Figure 31.11. Monte Carlo
simulations of triangular Ising
models with J = (cid:6)1 and
N = 4096. (a{c) J = 1. (d{f)
J = (cid:0)1. (a, d) Mean energy and
(cid:13)uctuations in energy as a
function of temperature. (b, e)
Fluctuations in energy (standard
deviation). (c, f) Heat capacity.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

408

31 | Ising Models

T

J = +1

T

J = (cid:0)1

50

5

2

0.5

20

6

4

3

2

Figure 31.12. Sample states of
triangular Ising models with J = 1
and J = (cid:0)1. High temperatures
at the top; low at the bottom.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31.2: Direct computation of partition function of Ising models

409

is to note that if the system is translation-invariant along its length then we
need to do only one iteration in order to (cid:12)nd the properties of a system of any
length.

The computational task becomes the evaluation of an S (cid:2) S matrix, where
S is the number of microstates that need to be considered at the boundary,
and the computation of its eigenvalues. The eigenvalue of largest magnitude
gives the partition function for an in(cid:12)nite-length thin strip.

Here is a more detailed explanation. Label the states of the C columns of
the thin strip s1; s2; : : : ; sC, with each s an integer from 0 to 2W (cid:0)1. The rth
bit of sc indicates whether the spin in row r, column c is up or down. The
partition function is

Z = Xx
= Xs1 Xs2

exp((cid:0)(cid:12)E(x))
(cid:1)(cid:1)(cid:1)XsC

exp (cid:0)(cid:12)

C

Xc=1

(31.26)

(31.27)

E(sc; sc+1)! ;

where E(sc; sc+1) is an appropriately de(cid:12)ned energy, and, if we want periodic
boundary conditions, sC+1 is de(cid:12)ned to be s1. One de(cid:12)nition for E is:

J xmxn + 1

J xmxn + 1

J xmxn:

(31.28)

E(sc; sc+1) = X(m;n)2N :

m2c;n2c+1

4 X(m;n)2N :

m2c;n2c

4 X(m;n)2N :

m2c+1;n2c+1

This de(cid:12)nition of the energy has the nice property that (for the rectangular
Ising model) it de(cid:12)nes a matrix that is symmetric in its two indices sc; sc+1.
The factors of 1=4 are needed because vertical links are counted four times.
Let us de(cid:12)ne

+

(cid:0)
+

+

(cid:0)
(cid:0)
s2

+

+

(cid:0)
s3

+

(cid:0)
(cid:0)

+

+

+

Figure 31.13. Illustration to help
explain the de(cid:12)nition (31.28).
E(s2; s3) counts all the
contributions to the energy in the
rectangle. The total energy is
given by stepping the rectangle
along. Each horizontal bond
inside the rectangle is counted
once; each vertical bond is
half-inside the rectangle (and will
be half-inside an adjacent
rectangle) so half its energy is
included in E(s2; s3); the factor of
1=4 appears in the second term
because m and n both run over all
nodes in column c, so each bond is
visited twice.

For the state shown here,
s2 = (100)2, s3 = (110)2, the
horizontal bonds contribute +J to
E(s2; s3), and the vertical bonds
contribute (cid:0)J=2 on the left and
(cid:0)J=2 on the right, assuming
periodic boundary conditions
between top and bottom. So
E(s2; s3) = 0.

Then continuing from equation (31.27),

Mss0 = exp(cid:0)(cid:0)(cid:12)E(s; s0)(cid:1) :

(cid:1)(cid:1)(cid:1)XsC " C
Yc=1

Msc;sc+1#

Z = Xs1 Xs2
= Trace (cid:2)MC(cid:3)
= Xa

(cid:22)C
a ;

(31.29)

(31.30)

(31.31)

(31.32)

where f(cid:22)ag2W
Z becomes dominated by the largest eigenvalue (cid:22)max:

a=1 are the eigenvalues of M. As the length of the strip C increases,

Z ! (cid:22)C

max:

(31.33)

So the free energy per spin in the limit of an in(cid:12)nite thin strip is given by:

f = (cid:0)kT ln Z=(W C) = (cid:0)kT C ln (cid:22)max=(W C) = (cid:0)kT ln (cid:22)max=W:

(31.34)

It’s really neat that all the thermodynamic properties of a long thin strip can
be obtained from just the largest eigenvalue of this matrix M!

Computations

I computed the partition functions of long-thin-strip Ising models with the
geometries shown in (cid:12)gure 31.14.

As in the last section, I set the applied (cid:12)eld H to zero and considered the
two cases J = (cid:6)1 which are a ferromagnet and antiferromagnet respectively. I
computed the free energy per spin, f ((cid:12); J; H) = F=N for widths from W = 2
to 8 as a function of (cid:12) for H = 0.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

410

6

W

?

bbbb

bbbb

bbbb

bbbb

bbbb

Computational ideas:

Triangular:

bbbb

bbbb

bbbb

6

W

?

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

31 | Ising Models

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH
(cid:8)(cid:8)
HH

bbbb

bbbb

Rectangular:

bbbb

bbbb

bbbb

Figure 31.14. Two long-thin-strip
Ising models. A line between two
spins indicates that they are
neighbours. The strips have width
W and in(cid:12)nite length.

Only the largest eigenvalue is needed. There are several ways of getting this
quantity, for example, iterative multiplication of the matrix by an initial vec-
tor. Because the matrix is all positive we know that the principal eigenvector
is all positive too (Frobenius{Perron theorem), so a reasonable initial vector is
(1; 1; : : : ; 1). This iterative procedure may be faster than explicit computation
of all eigenvalues. I computed them all anyway, which has the advantage that
we can (cid:12)nd the free energy of (cid:12)nite length strips { using equation (31.32) { as
well as in(cid:12)nite ones.

Ferromagnets of width 8

Triangular
Rectangular

y
g
r
e
n
E
e
e
r
F

 

-1

-2

-3

-4

-5

-6

-7

0

2

6
4
Temperature

8

10

-1

-2

-3

-4

-5

-6

-7

-8

Antiferromagnets of width 8

Triangular
Rectangular

Figure 31.15. Free energy per spin
of long-thin-strip Ising models.
Note the non-zero gradient at
T = 0 in the case of the triangular
antiferromagnet.

0

2

4
6
Temperature

8

10

Comments on graphs:

For large temperatures all Ising models should show the same behaviour: the
free energy is entropy-dominated, and the entropy per spin is ln(2). The mean
energy per spin goes to zero. The free energy per spin should tend to (cid:0)ln(2)=(cid:12).
The free energies are shown in (cid:12)gure 31.15.
One of the interesting properties we can obtain from the free energy is
the degeneracy of the ground state. As the temperature goes to zero, the
Boltzmann distribution becomes concentrated in the ground state.
If the
ground state is degenerate (i.e., there are multiple ground states with identical

y
p
o
r
t
n
E

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Triangular(-)
Rectangular
Triangular(+)

2

4
6
Temperature

8

10

Figure 31.16. Entropies (in nats)
of width 8 Ising systems as a
function of temperature, obtained
by di(cid:11)erentiating the free energy
curves in (cid:12)gure 31.15. The
rectangular ferromagnet and
antiferromagnet have identical
thermal properties. For the
triangular systems, the upper
curve ((cid:0)) denotes the
antiferromagnet and the lower
curve (+) the ferromagnet.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

31.2: Direct computation of partition function of Ising models

411

0

-0.5

-1

-1.5

-2

-2.5

-3

-3.5

Rectangular Ferromagnet

width 4
width 8

Triangular(-)
Rectangular(+/-)
Triangular(+)

1

10

Triangular Ising Models

width 4 (-)
width 8 (-)
width 4 (+)
width 8 (+)

1.2

1

0.8

0.6

0.4

0.2

0

1

Temperature

-0.2

10

1

Temperature

10

y
t
i
c
a
p
a
C

 
t
a
e
H

1.2

1

0.8

0.6

0.4

0.2

0

-0.2

energy) then the entropy as T ! 0 is non-zero. We can (cid:12)nd the entropy from
the free energy using S = (cid:0)@F=@T .
The entropy of the triangular antiferromagnet at absolute zero appears to
be about 0.3, that is, about half its high temperature value ((cid:12)gure 31.16).
The mean energy as a function of temperature is plotted in (cid:12)gure 31.17. It is
evaluated using the identity hEi = (cid:0)@ ln Z=@(cid:12).
Figure 31.18 shows the estimated heat capacity (taking raw derivatives of
the mean energy) as a function of temperature for the triangular models with
widths 4 and 8. Figure 31.19 shows the (cid:13)uctuations in energy as a function of
temperature. All of these (cid:12)gures should show smooth graphs; the roughness of
the curves is due to inaccurate numerics. The nature of any phase transition
is not obvious, but the graphs seem compatible with the assertion that the
ferromagnet shows, and the antiferromagnet does not show a phase transition.
The pictures of the free energy in (cid:12)gure 31.15 give some insight into how
we could predict the transition temperature. We can see how the two phases
of the ferromagnetic systems each have simple free energies: a straight sloping
line through F = 0, T = 0 for the high temperature phase, and a horizontal
line for the low temperature phase. (The slope of each line shows what the
entropy per spin of that phase is.) The phase transition occurs roughly at
the intersection of these lines. So we predict the transition temperature to be
linearly related to the ground state energy.

)

E

(
r
a
v

7

6

5

4

3

2

1

0

-1

Rectangular Ferromagnet

width 4
width 8

1

Temperature

10

16

14

12

10

8

6

4

2

0

-2

Triangular Ising Models

width 4 (-)
width 8 (-)
width 4 (+)
width 8 (+)

1

Temperature

10

Figure 31.17. Mean energy versus
temperature of long thin strip
Ising models with width 8.
Compare with (cid:12)gure 31.3.

Figure 31.18. Heat capacities of
(a) rectangular model; (b)
triangular models with di(cid:11)erent
widths, (+) and ((cid:0)) denoting
ferromagnet and antiferromagnet.
Compare with (cid:12)gure 31.11.

Figure 31.19. Energy variances,
per spin, of (a) rectangular model;
(b) triangular models with
di(cid:11)erent widths, (+) and ((cid:0))
denoting ferromagnet and
antiferromagnet. Compare with
(cid:12)gure 31.11.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

412

31 | Ising Models

Comparison with the Monte Carlo results

The agreement between the results of the two experiments seems very good.
The two systems simulated (the long thin strip and the periodic square) are
not quite identical. One could a more accurate comparison by (cid:12)nding all

eigenvalues for the strip of width W and computingP (cid:21)W to get the partition
function of a W (cid:2) W patch.

31.3 Exercises

. Exercise 31.2.[4 ] What would be the best way to extract the entropy from the
Monte Carlo simulations? What would be the best way to obtain the
entropy and the heat capacity from the partition function computation?

Exercise 31.3.[3 ] An Ising model may be generalized to have a coupling Jmn
between any spins m and n, and the value of Jmn could be di(cid:11)erent for each
m and n. In the special case where all the couplings are positive we know
that the system has two ground states, the all-up and all-down states. For a
more general setting of Jmn it is conceivable that there could be many ground
states.

Imagine that it is required to make a spin system whose local minima are
a given list of states x(1); x(2); : : : ; x(S). Can you think of a way of setting J
such that the chosen states are low energy states? You are allowed to adjust
all the fJmng to whatever values you wish.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

32

Exact Monte Carlo Sampling

32.1 The problem with Monte Carlo methods

For high-dimensional problems, the most widely used random sampling meth-
ods are Markov chain Monte Carlo methods like the Metropolis method, Gibbs
sampling, and slice sampling.

The problem with all these methods is this: yes, a given algorithm can be
guaranteed to produce samples from the target density P (x) asymptotically,
‘once the chain has converged to the equilibrium distribution’. But if one runs
the chain for too short a time T , then the samples will come from some other
distribution P (T )(x). For how long must the Markov chain be run before it has
‘converged’ ? As was mentioned in Chapter 29, this question is usually very
hard to answer. However, the pioneering work of Propp and Wilson (1996)
allows one, for certain chains, to answer this very question; furthermore Propp
and Wilson show how to obtain ‘exact’ samples from the target density.

32.2 Exact sampling concepts

Propp and Wilson’s exact sampling method (also known as ‘perfect simulation’
or ‘coupling from the past’) depends on three ideas.

Coalescence of coupled Markov chains

First, if several Markov chains starting from di(cid:11)erent initial conditions share
a single random-number generator, then their trajectories in state space may
coalesce; and, having, coalesced, will not separate again. If all initial condi-
tions lead to trajectories that coalesce into a single trajectory, then we can be
sure that the Markov chain has ‘forgotten’ its initial condition. Figure 32.1a-i
shows twenty-one Markov chains identical to the one described in section 29.4,
which samples from f0; 1; : : : ; 20g using the Metropolis algorithm ((cid:12)gure 29.12,
p.368); each of the chains has a di(cid:11)erent initial condition but they are all driven
by a single random number generator; the chains coalesce after about 80 steps.
Figure 32.1a-ii shows the same Markov chains with a di(cid:11)erent random number
seed; in this case, coalescence does not occur until 400 steps have elapsed (not
shown). Figure 32.1b shows similar Markov chains, each of which has identical
proposal density to those in section 29.4 and (cid:12)gure 32.1a; but in (cid:12)gure 32.1b,
the proposed move at each step, ‘left’ or ‘right’, is obtained in the same way by
all the chains at any timestep, independent of the current state. This coupling
of the chains changes the statistics of coalescence. Because two neighbouring
paths merge only when a rejection occurs, and rejections occur only at the
walls (for this particular Markov chain), coalescence will occur only when the
chains are all in the leftmost state or all in the rightmost state.

413

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

414

32 | Exact Monte Carlo Sampling

Figure 32.1. Coalescence, the (cid:12)rst
idea behind the exact sampling
method. Time runs from bottom
to top. In the leftmost panel,
coalescence occurred within 100
steps. Di(cid:11)erent coalescence
properties are obtained depending
on the way each state uses the
random numbers it is supplied
with. (a) Two runs of a
Metropolis simulator in which the
random bits that determine the
proposed step depend on the
current state; a di(cid:11)erent random
number seed was used in each
case. (b) In this simulator the
random proposal (‘left’ or ‘right’)
is the same for all states. In each
panel, one of the paths, the one
starting at location x = 8, has
been highlighted.

250

250

250

250

200

200

200

200

150

150

150

150

100

100

100

100

50

50

50

50

0

0

0

0

0 5 10 15 20

0 5 10 15 20

0 5 10 15 20

0 5 10 15 20

(i)

(ii)

(a)

(i)

(ii)

(b)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

415

32.2: Exact sampling concepts

Coupling from the past

How can we use the coalescence property to (cid:12)nd an exact sample from the
equilibrium distribution of the chain? The state of the system at the moment
when complete coalescence occurs is not a valid sample from the equilibrium
distribution; for example in (cid:12)gure 32.1b, (cid:12)nal coalescence always occurs when
the state is against one of the two walls, because trajectories merge only at
the walls. So sampling forward in time until coalescence occurs is not a valid
method.

The second key idea of exact sampling is that we can obtain exact samples
by sampling from a time T0 in the past, up to the present.
If coalescence
has occurred, the present sample is an unbiased sample from the equilibrium
distribution;
if not, we restart the simulation from a time T0 further into
the past, reusing the same random numbers. The simulation is repeated at a
sequence of ever more distant times T0, with a doubling of T0 from one run to
the next being a convenient choice. When coalescence occurs at a time before
‘the present’, we can record x(0) as an exact sample from the equilibrium
distribution of the Markov chain.

Figure 32.2 shows two exact samples produced in this way. In the leftmost
panel of (cid:12)gure 32.2a, we start twenty-one chains in all possible initial condi-
tions at T0 = (cid:0)50 and run them forward in time. Coalescence does not occur.
We restart the simulation from all possible initial conditions at T0 = (cid:0)100,
and reset the random number generator in such a way that the random num-
bers generated at each time t (in particular, from t = (cid:0)50 to t = 0) will be
identical to what they were in the (cid:12)rst run. Notice that the trajectories pro-
duced from t = (cid:0)50 to t = 0 by these runs that started from T0 = (cid:0)100 are
identical to a subset of the trajectories in the (cid:12)rst simulation with T0 = (cid:0)50.
Coalescence still does not occur, so we double T0 again to T0 = (cid:0)200. This
time, all the trajectories coalesce and we obtain an exact sample, shown by
the arrow. If we pick an earlier time such as T0 = (cid:0)500, all the trajectories
must still end in the same point at t = 0, since every trajectory must pass
through some state at t = (cid:0)200, and all those states lead to the same (cid:12)nal
point. So if we ran the Markov chain for an in(cid:12)nite time in the past, from any
initial condition, it would end in the same state. Figure 32.2b shows an exact
sample produced in the same way with the Markov chains of (cid:12)gure 32.1b.

This method, called coupling from the past, is important because it allows
us to obtain exact samples from the equilibrium distribution; but, as described
here, it is of little practical use, since we are obliged to simulate chains starting
in all initial states. In the examples shown, there are only twenty-one states,
but in any realistic sampling problem there will be an utterly enormous number
of states { think of the 21000 states of a system of 1000 binary spins, for
example. The whole point of introducing Monte Carlo methods was to try to
avoid having to visit all the states of such a system!

Monotonicity

Having established that we can obtain valid samples by simulating forward
from times in the past, starting in all possible states at those times, the third
trick of Propp and Wilson, which makes the exact sampling method useful
in practice, is the idea that, for some Markov chains, it may be possible to
detect coalescence of all trajectories without simulating all those trajectories.
This property holds, for example, in the chain of (cid:12)gure 32.1b, which has the
property that two trajectories never cross. So if we simply track the two tra-
jectories starting from the leftmost and rightmost states, we will know that

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

416

32 | Exact Monte Carlo Sampling

0

0

0

0

0

0

-50

-50

-50

-50

-50

-50

-100

-100

-100

-100

-100

-100

-150

-150

-150

-150

-150

-150

-200

-200

-200

-200

-200

-200

-250

-250

-250

-250

-250

-250

0
10 20
T0 = (cid:0)50

0

10 20
T0 = (cid:0)100

(a)

0

10 20
T0 = (cid:0)200

0
10 20
T0 = (cid:0)50

0

10 20
T0 = (cid:0)100

(b)

0

10 20
T0 = (cid:0)200

Figure 32.2. ‘Coupling from the past’, the second idea behind the exact sampling method.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

32.2: Exact sampling concepts

417

0

0

0

0

0

-50

-50

-50

-50

-50

-100

-100

-100

-100

-100

-150

-150

-150

-150

-150

-200

-200

-200

-200

-200

-250

-250

-250

-250

-250

0
10 20
T0 = (cid:0)50

0

10 20
T0 = (cid:0)100

(a)

0

10 20
T0 = (cid:0)200

0
10 20
T0 = (cid:0)50

(b)

0

10 20
T0 = (cid:0)1000

(c)

Figure 32.3. (a) Ordering of states, the third idea behind the exact sampling method. The trajectories
shown here are the left-most and right-most trajectories of (cid:12)gure 32.2b. In order to establish
what the state at time zero is, we only need to run simulations from T0 = (cid:0)50, T0 = (cid:0)100,
and T0 = (cid:0)200, after which point coalescence occurs.
(b,c) Two more exact samples from the target density, generated by this method, and
di(cid:11)erent random number seeds. The initial times required were T0 = (cid:0)50 and T0 = (cid:0)1000,
respectively.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

418

32 | Exact Monte Carlo Sampling

Compute ai :=Pj Jijxj

Draw u from Uniform(0; 1)
If u < 1=(1 + e(cid:0)2ai)

xi := +1

Else

xi := (cid:0)1

Algorithm 32.4. Gibbs sampling
coupling method. The Markov
chains are coupled together by
having all chains update the same
spin i at each time step and
having all chains share a common
sequence of random numbers u.

Figure 32.5. An exact sample from
the Ising model at its critical
temperature, produced by
D.B. Wilson. Such samples can be
produced within seconds on an
ordinary computer by exact
sampling.

coalescence of all trajectories has occurred when those two trajectories co-
alesce. Figure 32.3a illustrates this idea by showing only the left-most and
right-most trajectories of (cid:12)gure 32.2b. Figure 32.3(b,c) shows two more ex-
act samples from the same equilibrium distribution generated by running the
‘coupling from the past’ method starting from the two end-states alone. In
(b), two runs coalesced starting from T0 = (cid:0)50; in (c), it was necessary to try
times up to T0 = (cid:0)1000 to achieve coalescence.

32.3 Exact sampling from interesting distributions

In the toy problem we studied, the states could be put in a one-dimensional
order such that no two trajectories crossed. The states of many interesting
state spaces can also be put into a partial order and coupled Markov chains
can be found that respect this partial order.
[An example of a partial order
on the four possible states of two spins is this: (+; +) > (+;(cid:0)) > ((cid:0);(cid:0));
and (+; +) > ((cid:0); +) > ((cid:0);(cid:0)); and the states (+;(cid:0)) and ((cid:0); +) are not
ordered.] For such systems, we can show that coalescence has occurred merely
by verifying that coalescence has occurred for all the histories whose initial
states were ‘maximal’ and ‘minimal’ states of the state space.

As an example, consider the Gibbs sampling method applied to a ferro-
magnetic Ising spin system, with the partial ordering of states being de(cid:12)ned
thus: state x is ‘greater than or equal to’ state y if xi (cid:21) yi for all spins i.
The maximal and minimal states are the the all-up and all-down states. The
Markov chains are coupled together as shown in algorithm 32.4. Propp and
Wilson (1996) show that exact samples can be generated for this system, al-
though the time to (cid:12)nd exact samples is large if the Ising model is below its
critical temperature, since the Gibbs sampling method itself is slowly-mixing
under these conditions. Propp and Wilson have improved on this method for
the Ising model by using a Markov chain called the single-bond heat bath
algorithm to sample from a related model called the random cluster model;
they show that exact samples from the random cluster model can be obtained
rapidly and can be converted into exact samples from the Ising model. Their
ground-breaking paper includes an exact sample from a 16-million-spin Ising
model at its critical temperature. A sample for a smaller Ising model is shown
in (cid:12)gure 32.5.

A generalization of the exact sampling method for ‘non-attractive’ distri-
butions

The method of Propp and Wilson for the Ising model, sketched above, can
be applied only to probability distributions that are, as they call them, ‘at-
tractive’. Rather than de(cid:12)ne this term, let’s say what it means, for practical
purposes: the method can be applied to spin systems in which all the cou-
plings are positive (e.g., the ferromagnet), and to a few special spin systems
with negative couplings (e.g., as we already observed in Chapter 31, the rect-
angular ferromagnet and antiferromagnet are equivalent); but it cannot be
applied to general spin systems in which some couplings are negative, because
in such systems the trajectories followed by the all-up and all-down states
are not guaranteed to be upper and lower bounds for the set of all trajecto-
ries. Fortunately, however, we do not need to be so strict. It is possible to
re-express the Propp and Wilson algorithm in a way that generalizes to the
case of spin systems with negative couplings. The idea of the summary state
version of exact sampling is still that we keep track of bounds on the set of

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

32.3: Exact sampling from interesting distributions

419

all trajectories, and detect when these bounds are equal, so as to (cid:12)nd exact
samples. But the bounds will not themselves be actual trajectories, and they
will not necessarily be tight bounds.

Instead of simulating two trajectories, each of which moves in a state space
f(cid:0)1; +1gN , we simulate one trajectory envelope in an augmented state space
f(cid:0)1; +1; ?gN , where the symbol ? denotes ‘either (cid:0)1 or +1’. We call the state
of this augmented system the ‘summary state’. An example summary state of
a six-spin system is ++-?+?. This summary state is shorthand for the set of
states

++-+++, ++-++-, ++--++, ++--+- .

The update rule at each step of the Markov chain takes a single spin, enu-
merates all possible states of the neighbouring spins that are compatible with
the current summary state, and, for each of these local scenarios, computes
the new value (+ or -) of the spin using Gibbs sampling (coupled to a random
number u as in algorithm 32.4). If all these new values agree, then the new
value of the updated spin in the summary state is set to the unanimous value
(+ or -). Otherwise, the new value of the spin in the summary state is ‘?’. The
initial condition, at time T0, is given by setting all the spins in the summary
state to ‘?’, which corresponds to considering all possible start con(cid:12)gurations.
In the case of a spin system with positive couplings, this summary state
simulation will be identical to the simulation of the uppermost state and low-
ermost states, in the style of Propp and Wilson, with coalescence occuring
when all the ‘?’ symbols have disappeared. The summary state method can
be applied to general spin systems with any couplings. The only shortcoming
of this method is that the envelope may describe an unnecessarily large set of
states, so there is no guarantee that the summary state algorithm will con-
verge; the time for coalescence to be detected may be considerably larger than
the actual time taken for the underlying Markov chain to coalesce.

The summary state scheme has been applied to exact sampling in belief
networks by Harvey and Neal (2000), and to the triangular antiferromagnetic
Ising model by Childs et al. (2001). Summary state methods were (cid:12)rst intro-
duced by Huber (1998); they also go by the names sandwiching methods and
bounding chains.

Further reading

For further reading, impressive pictures of exact samples from other distribu-
tions, and generalizations of the exact sampling method, browse the perfectly-
random sampling website.1

For beautiful exact-sampling demonstrations running live in your web-

browser, see Jim Propp’s website.2

Other uses for coupling

The idea of coupling together Markov chains by having them share a random
number generator has other applications beyond exact sampling. Pinto and
Neal (2001) have shown that the accuracy of estimates obtained from a Markov
chain Monte Carlo simulation (the second problem discussed in section 29.1,
p.357), using the estimator

^(cid:8)P (cid:17)

1

T Xt

(cid:30)(x(t));

(32.1)

1http://www.dbwilson.com/exact/
2http://www.math.wisc.edu/(cid:24)propp/tiling/www/applets/

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

420

32 | Exact Monte Carlo Sampling

Figure 32.6. A perfectly random
tiling of a hexagon by lozenges,
provided by J.G. Propp and
D.B. Wilson.

can be improved by coupling the chain of interest, which converges to P , to a
second chain, which generates samples from a second, simpler distribution, Q.
The coupling must be set up in such a way that the states of the two chains
are strongly correlated. The idea is that we (cid:12)rst estimate the expectations of
a function of interest, (cid:30), under P and under Q in the normal way (32.1) and
compare the estimate under Q, ^(cid:8)Q, with the true value of the expectation
under Q, (cid:8)Q which we assume can be evaluated exactly. If ^(cid:8)Q is an overes-
timate then it is likely that ^(cid:8)P will be an overestimate too. The di(cid:11)erence
( ^(cid:8)Q (cid:0) (cid:8)Q) can thus be used to correct ^(cid:8)P .
32.4 Exercises

. Exercise 32.1.[2, p.421] Is there any relationship between the probability dis-
tribution of the time taken for all trajectories to coalesce, and the equi-
libration time of a Markov chain? Prove that there is a relationship, or
(cid:12)nd a single chain that can be realized in two di(cid:11)erent ways that have
di(cid:11)erent coalescence times.

. Exercise 32.2.[2 ] Imagine that Fred ignores the requirement that the random
bits used at some time t, in every run from increasingly distant times
T0, must be identical, and makes a coupled-Markov-chain simulator that
uses fresh random numbers every time T0 is changed. Describe what
happens if Fred applies his method to the Markov chain that is intended
to sample from the uniform distribution over the states 0, 1, and 2, using
the Metropolis method, driven by a random bit source as in (cid:12)gure 32.1b.

Exercise 32.3.[5 ] Investigate the application of perfect sampling to linear re-
gression in Holmes and Mallick (1998) or Holmes and Denison (2002)
and try to generalize it.

Exercise 32.4.[3 ] The concept of coalescence has many applications. Some sur-
names are more frequent than others, and some die out altogether. Make

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

32.5: Solutions

421

a model of this process; how long will it take until everyone has the same
surname?

Similarly, variability in any particular portion of the human genome
(which forms the basis of forensic DNA (cid:12)ngerprinting) is inherited like a
surname. A DNA (cid:12)ngerprint is like a string of surnames. Should the fact
that these surnames are subject to coalescences, so that some surnames
are by chance more prevalent than others, a(cid:11)ect the way in which DNA
(cid:12)ngerprint evidence is used in court?

. Exercise 32.5.[2 ] How can you use a coin to create a random ranking of 3
people? Construct a solution that uses exact sampling. For example,
you could apply exact sampling to a Markov chain in which the coin is
repeatedly used alternately to decide whether to switch (cid:12)rst and second,
then whether to switch second and third.

Exercise 32.6.[5 ] Finding the partition function Z of a probability distribution
is a di(cid:14)cult problem. Many Markov chain Monte Carlo methods produce
valid samples from a distribution without ever (cid:12)nding out what Z is.

Is there any probability distribution and Markov chain such that either
the time taken to produce a perfect sample or the number of random bits
used to create a perfect sample are related to the value of Z? Are there
some situations in which the time to coalescence conveys information
about Z?

32.5 Solutions

Solution to exercise 32.1 (p.420).
It is perhaps surprising that there is no di-
rect relationship between the equilibration time and the time to coalescence.
We can prove this using the example of the uniform distribution over the inte-
gers A = f0; 1; 2; : : : ; 20g. A Markov chain that converges to this distribution
in exactly one iteration is the chain for which the probability of state st+1
given st is the uniform distribution, for all st. Such a chain can be coupled
to a random number generator in two ways: (a) we could draw a random
integer u 2 A, and set st+1 equal to u regardless of st; or (b) we could draw
a random integer u 2 A, and set st+1 equal to (st + u) mod 21. Method (b)
would produce a cohort of trajectories locked together, similar to the trajec-
tories in (cid:12)gure 32.1, except that no coalescence ever occurs. Thus, while the
equilibration times of methods (a) and (b) are both one, the coalescence times
are respectively one and in(cid:12)nity.

It seems plausible on the other hand that coalescence time provides some

sort of upper bound on equilibration time.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Gibbs’ inequality (cid:12)rst appeared in
equation (1.24); see also
exercise 2.26 (p.37).

33

Variational Methods

Variational methods are an important technique for the approximation of com-
plicated probability distributions, having applications in statistical physics,
data modelling and neural networks.

33.1 Variational free energy minimization

One method for approximating a complex distribution in a physical system is
mean (cid:12)eld theory. Mean (cid:12)eld theory is a special case of a general variational
free energy approach of Feynman and Bogoliubov which we will now study.
The key piece of mathematics needed to understand this method is Gibbs’
inequality, which we repeat here.

The relative entropy between two probability distributions Q(x) and P (x)

that are de(cid:12)ned over the same alphabet AX is

DKL(QjjP ) =Xx

Q(x) log

Q(x)
P (x)

:

(33.1)

The relative entropy satis(cid:12)es DKL(QjjP ) (cid:21) 0 (Gibbs’ inequality) with
equality only if Q = P . In general DKL(QjjP ) 6= DKL(PjjQ).
In this chapter we will replace the log by ln, and measure the divergence
in nats.

Probability distributions in statistical physics

In statistical physics one often encounters probability distributions of the form

P (xj (cid:12); J) =

1

Z((cid:12); J)

exp[(cid:0)(cid:12)E(x; J)] ;

(33.2)

where for example the state vector is x 2 f(cid:0)1; +1gN , and E(x; J) is some
energy function such as

E(x; J) = (cid:0)

1

2Xm;n

Jmnxmxn (cid:0)Xn

hnxn:

The partition function (normalizing constant) is

Z((cid:12); J) (cid:17)Xx

exp[(cid:0)(cid:12)E(x; J)] :

(33.3)

(33.4)

The probability distribution of equation (33.2) is complex. Not unbearably
complex { we can, after all, evaluate E(x; J) for any particular x in a time

422

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.1: Variational free energy minimization

423

polynomial in the number of spins. But evaluating the normalizing constant
Z((cid:12); J) is di(cid:14)cult, as we saw in Chapter 29, and describing the properties of
the probability distribution is also hard. Knowing the value of E(x; J) at a
few arbitrary points x, for example, gives no useful information about what
the average properties of the system are.

An evaluation of Z((cid:12); J) would be particularly desirable because from Z

we can derive all the thermodynamic properties of the system.

Variational free energy minimization is a method for approximating the
complex distribution P (x) by a simpler ensemble Q(x; (cid:18)) that is parameterized
by adjustable parameters (cid:18). We adjust these parameters so as to get Q to
best approximate P , in some sense. A by-product of this approximation is a
lower bound on Z((cid:12); J).

The variational free energy

The objective function chosen to measure the quality of the approximation is
the variational free energy

(cid:12) ~F ((cid:18)) =Xx

Q(x; (cid:18)) ln

Q(x; (cid:18))

exp[(cid:0)(cid:12)E(x; J)]

:

(33.5)

This expression can be manipulated into a couple of interesting forms: (cid:12)rst,

(cid:12) ~F ((cid:18)) = (cid:12)Xx

Q(x; (cid:18))E(x; J) (cid:0)Xx

(cid:17) (cid:12) hE(x; J)iQ (cid:0) SQ;

Q(x; (cid:18)) ln

1

Q(x; (cid:18))

(33.6)

(33.7)

where hE(x; J)iQ is the average of the energy function under the distribution
Q(x; (cid:18)), and SQ is the entropy of the distribution Q(x; (cid:18)) (we set kB to one
in the de(cid:12)nition of S so that it is identical to the de(cid:12)nition of the entropy H
in Part I).

Second, we can use the de(cid:12)nition of P (xj (cid:12); J) to write:

(cid:12) ~F ((cid:18)) = Xx

Q(x; (cid:18)) ln

Q(x; (cid:18))

P (xj (cid:12); J) (cid:0) ln Z((cid:12); J)

= DKL(QjjP ) + (cid:12)F;
where F is the true free energy, de(cid:12)ned by

(cid:12)F (cid:17) (cid:0) ln Z((cid:12); J);

(33.8)

(33.9)

(33.10)

and DKL(QjjP ) is the relative entropy between the approximating distribution
Q(x; (cid:18)) and the true distribution P (xj (cid:12); J). Thus by Gibbs’ inequality, the
variational free energy ~F ((cid:18)) is bounded below by F and attains this value only
for Q(x; (cid:18)) = P (xj (cid:12); J).
Our strategy is thus to vary (cid:18) in such a way that (cid:12) ~F ((cid:18)) is minimized.
The approximating distribution then gives a simpli(cid:12)ed approximation to the
true distribution that may be useful, and the value of (cid:12) ~F ((cid:18)) will be an upper
bound for (cid:12)F . Equivalently, ~Z (cid:17) e(cid:0)(cid:12) ~F (
Can the objective function (cid:12) ~F be evaluated?

) is a lower bound for Z.

We have already agreed that the evaluation of various interesting sums over x
is intractable. For example, the partition function

Z =Xx

exp((cid:0)(cid:12)E(x; J)) ;

(33.11)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33 | Variational Methods

(33.12)

(33.13)

424

the energy

and the entropy

hEiP =

E(x; J) exp((cid:0)(cid:12)E(x; J)) ;

1

Z Xx
S (cid:17)Xx

P (xj (cid:12); J) ln

1

P (xj (cid:12); J)

are all presumed to be impossible to evaluate. So why should we suppose
that this objective function (cid:12) ~F ((cid:18)), which is also de(cid:12)ned in terms of a sum
over all x (33.5), should be a convenient quantity to deal with? Well, for a
range of interesting energy functions, and for su(cid:14)ciently simple approximating
distributions, the variational free energy can be e(cid:14)ciently evaluated.

33.2 Variational free energy minimization for spin systems

An example of a tractable variational free energy is given by the spin system
whose energy function was given in equation (33.3), which we can approximate
with a separable approximating distribution,

Q(x; a) =

1
ZQ

exp Xn

anxn! :

(33.14)

The variational parameters (cid:18) of the variational free energy (33.5) are the
components of the vector a. To evaluate the variational free energy we need
the entropy of this distribution,

SQ =Xx

Q(x; a) ln

1

Q(x; a)

;

(33.15)

and the mean of the energy,

hE(x; J)iQ =Xx

Q(x; a)E(x; J):

(33.16)

The entropy of the separable approximating distribution is simply the sum of
the entropies of the individual spins (exercise 4.2, p.68),

SQ =Xn

H (e)

2 (qn);

(33.17)

where qn is the probability that spin n is +1,

qn =

ean

ean + e(cid:0)an

=

1

1 + exp((cid:0)2an)

;

(33.18)

and

H (e)

2 (q) = q ln

1
q

+ (1 (cid:0) q) ln

1

(1 (cid:0) q)

:

(33.19)

The mean energy under Q is easy to obtain because Pm;n Jmnxmxn is a sum

of terms each involving the product of two independent random variables.
(There are no self-couplings, so Jmn = 0 when m = n.) If we de(cid:12)ne the mean
value of xn to be (cid:22)xn, which is given by

(cid:22)xn =

ean (cid:0) e(cid:0)an
ean + e(cid:0)an

= tanh(an) = 2qn (cid:0) 1;

(33.20)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.2: Variational free energy minimization for spin systems

425

we obtain

hE(x; J)iQ = Xx
= (cid:0)

1

1

Q(x; a)"(cid:0)
2Xm;n
2Xm;n
Jmn (cid:22)xm (cid:22)xn (cid:0)Xn

Jmnxmxn (cid:0)Xn

hnxn# (33.21)

hn (cid:22)xn:

(33.22)

So the variational free energy is given by

(cid:12) ~F (a) = (cid:12) hE(x; J)iQ(cid:0)SQ = (cid:12) (cid:0)

1

2Xm;n

Jmn (cid:22)xm (cid:22)xn (cid:0)Xn

hn (cid:22)xn!(cid:0)Xn

H (e)

2 (qn):

(33.23)

We now consider minimizing this function with respect to the variational

parameters a. If q = 1=(1 + e(cid:0)2a), the derivative of the entropy is

@
@q

H e

2(q) = ln

1 (cid:0) q
q

= (cid:0)2a:

(33.24)

So we obtain

@

@am

(cid:12) ~F (a) = (cid:12)"(cid:0)Xn
= 2(cid:18) @qm

Jmn (cid:22)xn (cid:0) hm#(cid:18)2

qm (cid:19)(cid:18) @qm
@am(cid:19)

@qm

@am(cid:19) (cid:0) ln(cid:18) 1 (cid:0) qm
Jmn (cid:22)xn + hm! + am# :

@am(cid:19)"(cid:0)(cid:12) Xn

This derivative is equal to zero when

am = (cid:12) Xn

Jmn (cid:22)xn + hm! :

So ~F (a) is extremized at any point that satis(cid:12)es equation (33.26) and

(cid:22)xn = tanh(an):

(33.27)

The variational free energy ~F (a) may be a multimodal function, in which
case each stationary point (maximum, minimum or saddle) will satisfy equa-
tions (33.26) and (33.27). One way of using these equations, in the case of a
system with an arbitrary coupling matrix J, is to update each parameter am
and the corresponding value of (cid:22)xm using equation (33.26), one at a time. This
asynchronous updating of the parameters is guaranteed to decrease (cid:12) ~F (a).

Equations (33.26) and (33.27) may be recognized as the mean (cid:12)eld equa-
tions for a spin system. The variational parameter an may be thought of as
the strength of a (cid:12)ctitious (cid:12)eld applied to an isolated spin n. Equation (33.27)
describes the mean response of spin n, and equation (33.26) describes how the
(cid:12)eld am is set in response to the mean state of all the other spins.

The variational free energy derivation is a helpful viewpoint for mean (cid:12)eld

theory for two reasons.

1. This approach associates an objective function (cid:12) ~F with the mean (cid:12)eld
equations; such an objective function is useful because it can help identify
alternative dynamical systems that minimize the same function.

1
1

0.5
0.5

1
1

0.5
0.5

0
0

0
0

Figure 33.1. The variational free
energy of the two-spin system
whose energy is E(x) = (cid:0)x1x2, as
a function of the two variational
parameters q1 and q2. The
inverse-temperature is (cid:12) = 1:44.
The function plotted is

2 (q2);

(cid:12) ~F = (cid:0)(cid:12) (cid:22)x1 (cid:22)x2(cid:0)H (e)
2 (q1)(cid:0)H (e)
where (cid:22)xn = 2qn (cid:0) 1. Notice that
for (cid:12)xed q2 the function is
convex ^ with respect to q1, and
for (cid:12)xed q1 it is convex ^ with
respect to q2.

(33.25)

(33.26)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

426

1

0.5

0

-0.5

-1

h = 0.00
h = 0.40
h = 0.80

33 | Variational Methods

Figure 33.2. Solutions of the
variational free energy
extremization problem for the
Ising model, for three di(cid:11)erent
applied (cid:12)elds h. Horizontal axis:
temperature T = 1=(cid:12). Vertical
axis: magnetization (cid:22)x. The
critical temperature found by
mean (cid:12)eld theory is T mft
c = 4.

0

1

2

3

4

5

6

7

8

2. The theory is readily generalized to other approximating distributions.
We can imagine introducing a more complex approximation Q(x; (cid:18)) that
might for example capture correlations among the spins instead of mod-
elling the spins as independent. One could then evaluate the variational
free energy and optimize the parameters (cid:18) of this more complex approx-
imation. The more degrees of freedom the approximating distribution
has, the tighter the bound on the free energy becomes. However, if the
complexity of an approximation is increased, the evaluation of either the
mean energy or the entropy typically becomes more challenging.

33.3 Example: mean (cid:12)eld theory for the ferromagnetic Ising model

In the simple Ising model studied in Chapter 31, every coupling Jmn is equal
to J if m and n are neighbours and zero otherwise. There is an applied
(cid:12)eld hn = h that is the same for all spins. A very simple approximating
distribution is one with just a single variational parameter a, which de(cid:12)nes a
separable distribution

Q(x; a) =

1
ZQ

exp Xn

axn!

in which all spins are independent and have the same probability

qn =

1

1 + exp((cid:0)2a)

of being up. The mean magnetization is

(33.28)

(33.29)

(cid:22)x = tanh(a)

(33.30)

and the equation (33.26) which de(cid:12)nes the minimum of the variational free
energy becomes

a = (cid:12) (CJ (cid:22)x + h) ;

(33.31)

where C is the number of couplings that a spin is involved in { C = 4 in the
case of a rectangular two-dimensional Ising model. We can solve equations
(33.30) and (33.31) for (cid:22)x numerically { in fact, it is easiest to vary (cid:22)x and solve
for (cid:12) { and obtain graphs of the free energy minima and maxima as a function
of temperature as shown in (cid:12)gure 33.2. The solid line shows (cid:22)x versus T = 1=(cid:12)
for the case C = 4; J = 1.

When h = 0, there is a pitchfork bifurcation at a critical temperature T mft
.
[A pitchfork bifurcation is a transition like the one shown by the solid lines in

c

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.4: Variational methods in inference and data modelling

427

(cid:12)gure 33.2, from a system with one minimum as a function of a (on the right)
to a system (on the left) with two minima and one maximum; the maximum
is the middle one of the three lines. The solid lines look like a pitchfork.]
Above this temperature, there is only one minimum in the variational free
energy, at a = 0 and (cid:22)x = 0; this minimum corresponds to an approximating
distribution that is uniform over all states. Below the critical temperature,
there are two minima corresponding to approximating distributions that are
symmetry-broken, with all spins more likely to be up, or all spins more likely
to be down. The state (cid:22)x = 0 persists as a stationary point of the variational
free energy, but now it is a local maximum of the variational free energy.

When h > 0, there is a global variational free energy minimum at any
temperature for a positive value of (cid:22)x, shown by the upper dotted curves in
(cid:12)gure 33.2. As long as h < JC, there is also a second local minimum in the
free energy, if the temperature is su(cid:14)ciently small. This second minimum cor-
responds to a self-preserving state of magnetization in the opposite direction
to the applied (cid:12)eld. The temperature at which the second minimum appears
is smaller than T mft
, and when it appears, it is accompanied by a saddle point
located between the two minima. A name given to this type of bifurcation is
a saddle-node bifurcation.

c

The variational free energy per spin is given by

(cid:12) ~F = (cid:12)(cid:18)(cid:0)

C
2

J (cid:22)x2 (cid:0) h(cid:22)x(cid:19) (cid:0) H (e)

2 (cid:18) (cid:22)x + 1
2 (cid:19) :

(33.32)

Exercise 33.1.[2 ] Sketch the variational free energy as a function of its one
parameter (cid:22)x for a variety of values of the temperature T and the applied
(cid:12)eld h.

Figure 33.2 reproduces the key properties of the real Ising system { that,
for h = 0, there is a critical temperature below which the system has long-
range order, and that it can adopt one of two macroscopic states. However,
by probing a little more we can reveal some inadequacies of the variational
approximation. To start with, the critical temperature T mft
is 4, which is
nearly a factor of 2 greater than the true critical temperature Tc = 2:27. Also,
the variational model has equivalent properties in any number of dimensions,
including d = 1, where the true system does not have a phase transition. So
the bifurcation at T mft

should not be described as a phase transition.

c

c

For the case h = 0 we can follow the trajectory of the global minimum as
a function of (cid:12) and (cid:12)nd the entropy, heat capacity and (cid:13)uctuations of the ap-
proximating distribution and compare them with those of a real 8(cid:2)8 fragment
using the matrix method of Chapter 31. As shown in (cid:12)gure 33.3, one of the
biggest di(cid:11)erences is in the (cid:13)uctuations in energy. The real system has large
(cid:13)uctuations near the critical temperature, whereas the approximating distri-
bution has no correlations among its spins and thus has an energy-variance
which scales simply linearly with the number of spins.

33.4 Variational methods in inference and data modelling

In statistical data modelling we are interested in the posterior probability
distribution of a parameter vector w given data D and model assumptions H,
P (w j D;H).

:

(33.33)

P (w j D;H) =

P (D j w;H)P (w jH)

P (D jH)

In traditional approaches to model (cid:12)tting, a single parameter vector w is op-
timized to (cid:12)nd the mode of this distribution. What is really of interest is

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

428

33 | Variational Methods

Free Energy

Energy

Figure 33.3. Comparison of
approximating distribution’s
properties with those of a real
8 (cid:2) 8 fragment. Notice that the
variational free energy of the
approximating distribution is
indeed an upper bound on the
free energy of the real system. All
quantities are shown ‘per spin’.

mean field theory
real 8x8 system

0

-0.5

-1

-1.5

-2

0

-2

-2.5

-3

-3.5

-4

-4.5

-5

-5.5

-6

0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

6

5

4

3

2

1

0

-1

0

mean field theory
real 8x8 system

1

2

3

4

5

6

7

8

Entropy

mean field theory
real 8x8 system

3

2

1
6
Fluctuations, var(E)

4

5

7

8

mean field theory
real 8x8 system

1

2

3

4

5

6

7

8

3

2

1
6
Heat Capacity, dE=dT

4

5

7

8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

-0.2

mean field theory
real 8x8 system

0

1

2

3

4

5

6

7

8

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.5: The case of an unknown Gaussian

429

the whole distribution. We may also be interested in its normalizing constant
P (D jH) if we wish to do model comparison. The probability distribution
P (w j D;H) is often a complex distribution. In a variational approach to in-
ference, we introduce an approximating probability distribution over the pa-
rameters, Q(w; (cid:18)), and optimize this distribution (by varying its own param-
eters (cid:18)) so that it approximates the posterior distribution of the parameters
P (w j D;H) well.
proximation is the variational free energy

One objective function we may choose to measure the quality of the ap-

~F ((cid:18)) =Z dkw Q(w; (cid:18)) ln

Q(w; (cid:18))

P (D j w;H)P (w jH)

:

(33.34)

The denominator P (D j w;H)P (w jH) is, within a multiplicative constant, the
posterior probability P (w j D;H) = P (D j w;H)P (w jH)=P (D jH): So the
variational free energy ~F ((cid:18)) can be viewed as the sum of (cid:0) ln P (D jH) and
the relative entropy between Q(w; (cid:18)) and P (w j D;H). ~F ((cid:18)) is bounded below
by (cid:0) ln P (D jH) and only attains this value for Q(w; (cid:18)) = P (w j D;H). For
certain models and certain approximating distributions, this free energy, and
its derivatives with respect to the approximating distribution’s parameters,
can be evaluated.

The approximation of posterior probability distributions using variational
free energy minimization provides a useful approach to approximating Bayesian
inference in a number of (cid:12)elds ranging from neural networks to the decoding of
error-correcting codes (Hinton and van Camp, 1993; Hinton and Zemel, 1994;
Dayan et al., 1995; Neal and Hinton, 1998; MacKay, 1995a). The method
is sometimes called ensemble learning to contrast it with traditional learning
processes in which a single parameter vector is optimized. Another name for
it is variational Bayes. Let us examine how ensemble learning works in the
simple case of a Gaussian distribution.

33.5 The case of an unknown Gaussian: approximating the posterior

distribution of (cid:22) and (cid:27)

We will (cid:12)t an approximating ensemble Q((cid:22); (cid:27)) to the posterior distribution
that we studied in Chapter 24,

P ((cid:22); (cid:27) jfxngN

n=1) =

=

P (fxngN
n=1 j (cid:22); (cid:27))P ((cid:22); (cid:27))
P (fxngN
(2(cid:25)(cid:27)2)N=2 exp(cid:16)(cid:0) N ((cid:22)(cid:0)(cid:22)x)2+S

n=1)

1

2(cid:27)2
n=1)

P (fxngN

(33.35)

(33.36)

(cid:17) 1

(cid:27)(cid:22)

1
(cid:27)

:

We make the single assumption that the approximating ensemble is separable
in the form Q((cid:22); (cid:27)) = Q(cid:22)((cid:22))Q(cid:27)((cid:27)). No restrictions on the functional form of
Q(cid:22)((cid:22)) and Q(cid:27)((cid:27)) are made.

We write down a variational free energy,

~F (Q) =Z d(cid:22) d(cid:27) Q(cid:22)((cid:22))Q(cid:27)((cid:27)) ln

Q(cid:22)((cid:22))Q(cid:27)((cid:27))

P (D j (cid:22); (cid:27))P ((cid:22); (cid:27))

:

(33.37)

We can (cid:12)nd the optimal separable distribution Q by considering separately
the optimization of ~F over Q(cid:22)((cid:22)) for (cid:12)xed Q(cid:27)((cid:27)), and then the optimization
of Q(cid:27)((cid:27)) for (cid:12)xed Q(cid:22)((cid:22)).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

(c)

(e)

(cid:27)
1
0.9
0.8
0.7
0.6
0.5
0.4

0.3

0.2

(cid:27)
1
0.9
0.8
0.7
0.6
0.5
0.4

0.3

0.2

0

0.5

1

1.5

2

(cid:22)

0

0.5

1

1.5

2

(cid:22)

33 | Variational Methods

Figure 33.4. Optimization of an
approximating distribution. The
posterior distribution
P ((cid:22); (cid:27) jfxng), which is the same
as that in (cid:12)gure 24.1, is shown by
solid contours. (a) Initial
condition. The approximating
distribution Q((cid:22); (cid:27)) (dotted
contours) is an arbitrary separable
distribution. (b) Q(cid:22) has been
updated, using equation (33.41).
(c) Q(cid:27) has been updated, using
equation (33.44). (d) Q(cid:22) updated
again. (e) Q(cid:27) updated again. (f)
Converged approximation (after
15 iterations). The arrows point
to the peaks of the two
distributions, which are at
(cid:27)N = 0:45 (for P ) and (cid:27)N (cid:0)1 = 0:5
(for Q).

430

(a)

(cid:27)
1
0.9
0.8
0.7
0.6
0.5
0.4

0.3

0.2

0

0.5

1

1.5

2

(cid:22)

: : :

(b)

(d)

(f)

(cid:27)
1
0.9
0.8
0.7
0.6
0.5
0.4

0.3

0.2

(cid:27)
1
0.9
0.8
0.7
0.6
0.5
0.4

0.3

0.2

(cid:27)
1
0.9
0.8
0.7
0.6
0.5
0.4

0.3

0.2

0

0.5

1

1.5

2

(cid:22)

0

0.5

1

1.5

2

(cid:22)

0

0.5

1

1.5

2

(cid:22)

Optimization of Q(cid:22)((cid:22))
As a functional of Q(cid:22)((cid:22)), ~F is:

~F = (cid:0)Z d(cid:22) Q(cid:22)((cid:22))(cid:20)Z d(cid:27) Q(cid:27)((cid:27)) ln P (D j (cid:22); (cid:27)) + ln[P ((cid:22))=Q(cid:22)((cid:22))](cid:21) + (cid:20) (33.38)
= Z d(cid:22) Q(cid:22)((cid:22))(cid:20)Z d(cid:27) Q(cid:27)((cid:27))N (cid:12)

((cid:22) (cid:0) (cid:22)x)2 + ln Q(cid:22)((cid:22))(cid:21) + (cid:20)0;

(33.39)

1
2

where (cid:12) (cid:17) 1=(cid:27)2 and (cid:20) denote constants that do not depend on Q(cid:22)((cid:22)). The
dependence on Q(cid:27) thus collapses down to a simple dependence on the mean

(cid:22)(cid:12) (cid:17)Z d(cid:27) Q(cid:27)((cid:27))1=(cid:27)2:
Now we can recognize the function (cid:0)N (cid:22)(cid:12) 1
2 ((cid:22) (cid:0) (cid:22)x)2 as the logarithm of a
Gaussian identical to the posterior distribution for a particular value of (cid:12) = (cid:22)(cid:12).
Since a relative entropy R Q ln(Q=P ) is minimized by setting Q = P , we can

immediately write down the distribution Qopt
Q(cid:27):

(cid:22) ((cid:22)) that minimizes ~F for (cid:12)xed

(33.40)

(33.41)

(cid:22) ((cid:22)) = P ((cid:22)j D; (cid:22)(cid:12);H) = Normal((cid:22); (cid:22)x; (cid:27)2
Qopt

(cid:22)jD):

where (cid:27)2

(cid:22)jD = 1=(N (cid:22)(cid:12)).
Optimization of Q(cid:27)((cid:27))
We represent Q(cid:27)((cid:27)) using the density over (cid:12), Q(cid:27)((cid:12)) (cid:17) Q(cid:27)((cid:27))jd(cid:27)=d(cid:12)j. As a
functional of Q(cid:27)((cid:12)), ~F is (neglecting additive constants):
~F = (cid:0)Z d(cid:12) Q(cid:27)((cid:12))(cid:20)Z d(cid:22) Q(cid:22)((cid:22)) ln P (D j (cid:22); (cid:27)) + ln[P ((cid:12))=Q(cid:27)((cid:12))](cid:21)(33.42)
= Z d(cid:12) Q(cid:27)((cid:12))h(N (cid:27)2
2 (cid:0) 1(cid:1) ln (cid:12) + ln Q(cid:27)((cid:12))i ; (33.43)

(cid:22)jD + S)(cid:12)=2 (cid:0)(cid:0) N

The prior P ((cid:27)) / 1=(cid:27) transforms
to P ((cid:12)) / 1=(cid:12).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.6: Interlude

431

where the integral over (cid:22) is performed assuming Q(cid:22)((cid:22)) = Qopt
(cid:22) ((cid:22)): Here, the (cid:12)-
dependent expression in square brackets can be recognized as the logarithm of
a gamma distribution over (cid:12) { see equation (23.15) { giving as the distribution
that minimizes ~F for (cid:12)xed Q(cid:22):

with

Qopt

(cid:27) ((cid:12)) = (cid:0)((cid:12); b0; c0);

1
b0

=

1
2

(N (cid:27)2

(cid:22)jD + S) and c0 =

N
2

:

(33.44)

(33.45)

In (cid:12)gure 33.4, these two update rules (33.41, 33.44) are applied alternately,
starting from an arbitrary initial condition. The algorithm converges to the
optimal approximating ensemble in a few iterations.

Direct solution for the joint optimum Q(cid:22)((cid:22))Q(cid:27)((cid:27))

In this problem, we do not need to resort to iterative computation to (cid:12)nd
the optimal approximating ensemble. Equations (33.41) and (33.44) de(cid:12)ne
the optimum implicitly. We must simultaneously have (cid:27) 2
(cid:22)(cid:12) = b0c0. The solution is:

(cid:22)jD = 1=(N (cid:22)(cid:12)), and

1= (cid:22)(cid:12) = S=(N (cid:0) 1):

(33.46)

This is similar to the true posterior distribution of (cid:27), which is a gamma distri-
bution with c0 = N(cid:0)1
and 1=b0 = S=2 (see equation 24.13). This true posterior
2
also has a mean value of (cid:12) satisfying 1= (cid:22)(cid:12) = S=(N (cid:0) 1); the only di(cid:11)erence is
that the approximating distribution’s parameter c0 is too large by 1=2.

The approximations given by variational free energy minimization
always tend to be more compact than the true distribution.

In conclusion, ensemble learning gives an approximation to the posterior
that agrees nicely with the conventional estimators. The approximate poste-
rior distribution over (cid:12) is a gamma distribution with mean (cid:22)(cid:12) corresponding
to a variance of (cid:27)2 = S=(N (cid:0) 1) = (cid:27)2
N(cid:0)1. And the approximate posterior dis-
tribution over (cid:22) is a Gaussian with mean (cid:22)x and standard deviation (cid:27) N(cid:0)1=pN .
The variational free energy minimization approach has the nice prop-
erty that it is parameterization-independent; it avoids the problem of basis-
dependence from which MAP methods and Laplace’s method su(cid:11)er.

A convenient software package for automatic implementation of variational
inference in graphical models is VIBES (Bishop et al., 2002). It plays the same
role for variational inference as BUGS plays for Monte Carlo inference.

33.6 Interlude

One of my students asked:

How do you ever come up with a useful approximating distribution,
given that the true distribution is so complex you can’t compute
it directly?

Let’s answer this question in the context of Bayesian data modelling. Let the
‘true’ distribution of interest be the posterior probability distribution over a
set of parameters x, P (xj D). A standard data modelling practice is to (cid:12)nd
a single, ‘best-(cid:12)t’ setting of the parameters, x(cid:3), for example, by (cid:12)nding the
maximum of the likelihood function P (D j x), or of the posterior distribution.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

432

33 | Variational Methods

One interpretation of this standard practice is that the full description of
our knowledge about x, P (xj D), is being approximated by a delta-function,
a probability distribution concentrated on x(cid:3). From this perspective, any
approximating distribution Q(x; (cid:18)), no matter how crummy it is, has to be
an improvement on the spike produced by the standard method! So even if
we use only a simple Gaussian approximation, we are doing well.

We now study an application of the variational approach to a realistic

example { data clustering.

33.7 K-means clustering and the expectation{maximization algo-

rithm as a variational method

In Chapter 20, we introduced the soft K-means clustering algorithm, version 1.
In Chapter 22, we introduced versions 2 and 3 of this algorithm, and motivated
the algorithm as a maximum likelihood algorithm.

K-means clustering is an example of an ‘expectation{maximization’ (EM)
algorithm, with the two steps, which we called ‘assignment’ and ‘update’,
being known as the ‘E-step’ and the ‘M-step’ respectively.

We now give a more general view of K-means clustering, due to Neal
and Hinton (1998), in which the algorithm is shown to optimize a variational
objective function. Neal and Hinton’s derivation applies to any EM algorithm.

The probability of everything

Let the parameters of the mixture model { the means, standard deviations, and
weights { be denoted by (cid:18). For each data point, there is a missing variable (also
known as a latent variable), the class label kn for that point. The probability
of everything, given our assumed model H, is

P (fx(n); kngN

n=1; (cid:18) jH) = P ((cid:18) jH)

N

Yn=1hP (x(n) j kn; (cid:18))P (kn j (cid:18))i :

(33.47)

The posterior probability of everything, given the data, is proportional to the
probability of everything:

P (fkngN

n=1; (cid:18) jfx(n)gN

n=1;H) =

P (fx(n); kngN
P (fx(n)gN

n=1; (cid:18) jH)
n=1 jH)

:

(33.48)

We now approximate this posterior distribution by a separable distribution

and de(cid:12)ne a variational free energy in the usual way:

Qk(fkngN

n=1) Q

((cid:18));

(33.49)

n=1) Q

((cid:18)) ln

n=1) Q

Qk(fkngN
P (fx(n); kngN

((cid:18))
n=1; (cid:18) jH)

:

~F (Qk; Q

) = XfkngZ dD(cid:18) Qk(fkngN

(33.50)
~F is bounded below by minus the evidence, ln P (fx(n)gN
n=1 jH). We can now
make an iterative algorithm with an ‘assignment’ step and an ‘update’ step.
In the assignment step, Qk(fkngN
; in
the update step, Q

n=1) is adjusted to reduce ~F , for (cid:12)xed Q

is adjusted to reduce ~F , for (cid:12)xed Qk.

If we wish to obtain exactly the soft K-means algorithm, we impose a
is constrained to be

further constraint on our approximating distribution: Q
a delta function centred on a point estimate of (cid:18), (cid:18) = (cid:18)(cid:3):

Q

((cid:18)) = (cid:14)((cid:18) (cid:0) (cid:18)(cid:3)):

(33.51)

 
 
 
 
 
 
 
 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.8: Variational methods other than free energy minimization

433

2

1

0

-5

0

5

Upper bound

Lower bound

1

1 + e(cid:0)a (cid:20) exp((cid:22)a (cid:0) H e

2 ((cid:22)))

(cid:22) 2 [0; 1]

1

1 + e(cid:0)a (cid:21) g((cid:23)) exp(cid:2)(a (cid:0) (cid:23))=2 (cid:0) (cid:21)((cid:23))(a2 (cid:0) (cid:23)2)(cid:3)

where (cid:21)((cid:23)) = [g((cid:23)) (cid:0) 1=2] =2(cid:23).

Figure 33.5. Illustration of the
Jaakkola{Jordan variational
method. Upper and lower bounds
on the logistic function (solid line)

g(a) (cid:17)

1

1 + e(cid:0)a :

These upper and lower bounds are
exponential or Gaussian functions
of a, and so easier to integrate
over. The graph shows the
sigmoid function and upper and
lower bounds with (cid:22) = 0:505 and
(cid:23) = (cid:0)2:015.

in(cid:12)nitely large integral R dD(cid:18) Q

Unfortunately, this distribution contributes to the variational free energy an
((cid:18)), so we’d better leave that term
out of ~F , treating it as an additive constant. [Using a delta function Q
is not
a good idea if our aim is to minimize ~F !] Moving on, our aim is to derive the
soft K-means algorithm.

((cid:18)) ln Q

. Exercise 33.2.[2 ] Show that, given Q

((cid:18)) = (cid:14)((cid:18) (cid:0) (cid:18)(cid:3)), the optimal Qk, in the
sense of minimizing ~F , is a separable distribution in which the probabil-
ity that kn = k is given by the responsibility r(n)
k .

. Exercise 33.3.[3 ] Show that, given a separable Qk as described above, the op-
timal (cid:18)(cid:3), in the sense of minimizing ~F , is obtained by the update step
of the soft K-means algorithm. (Assume a uniform prior on (cid:18).)

Exercise 33.4.[4 ] We can instantly improve on the in(cid:12)nitely large value of ~F
achieved by soft K-means clustering by allowing Q
to be a more general
distribution than a delta-function. Derive an update step in which Q
is
allowed to be a separable distribution, a product of Q(cid:22)(f(cid:22)g), Q(cid:27)(f(cid:27)g),
and Q(cid:25)((cid:25)). Discuss whether this generalized algorithm still su(cid:11)ers from
soft K-means’s ‘kaboom’ problem, where the algorithm glues an ever-
shrinking Gaussian to one data point.

Sadly, while it sounds like a promising generalization of the algorithm
to be a non-delta-function, and the ‘kaboom’ problem goes
to allow Q
away, other artefacts can arise in this approximate inference method,
involving local minima of ~F . For further reading, see (MacKay, 1997a;
MacKay, 2001).

33.8 Variational methods other than free energy minimization

There are other strategies for approximating a complicated distribution P (x),
in addition to those based on minimizing the relative entropy between an
approximating distribution, Q, and P . One approach pioneered by Jaakkola
and Jordan is to create adjustable upper and lower bounds QU and QL to P ,
as illustrated in (cid:12)gure 33.5. These bounds (which are unnormalized densities)
are parameterized by variational parameters which are adjusted in order to
obtain the tightest possible (cid:12)t. The lower bound can be adjusted to maximize

QL(x);

Xx

and the upper bound can be adjusted to minimize

QU (x):

Xx

(33.52)

(33.53)

 
 
 
 
 
 
 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

434

33 | Variational Methods

Using the normalized versions of the optimized bounds we then compute ap-
proximations to the predictive distributions. Further reading on such methods
can be found in the references (Jaakkola and Jordan, 2000a; Jaakkola and Jor-
dan, 2000b; Jaakkola and Jordan, 1996; Gibbs and MacKay, 2000).

Further reading

The Bethe and Kikuchi free energies

In Chapter 26 we discussed the sum{product algorithm for functions of the
factor-graph form (26.1). If the factor graph is tree-like, the sum{product algo-
rithm converges and correctly computes the marginal function of any variable
xn and can also yield the joint marginal function of subsets of variables that
appear in a common factor, such as xm.

The sum{product algorithm may also be applied to factor graphs that are
not tree-like. If the algorithm converges to a (cid:12)xed point, it has been shown
that that (cid:12)xed point is a stationary point (usually a minimum) of a function
of the messages called the Kikuchi free energy. In the special case where all
factors in factor graph are functions of one or two variables, the Kikuchi free
energy is called the Bethe free energy.

For articles on this idea, and new approximate inference algorithms mo-
tivated by it, see Yedidia (2000); Yedidia et al. (2000); Welling and Teh
(2001); Yuille (2001); Yedidia et al. (2001b); Yedidia et al. (2001a).

33.9 Further exercises

Exercise 33.5.[2, p.435] This exercise explores the assertion, made above, that
the approximations given by variational free energy minimization al-
ways tend to be more compact than the true distribution. Consider a
two dimensional Gaussian distribution P (x) with axes aligned with the
directions e(1) = (1; 1) and e(2) = (1;(cid:0)1). Let the variances in these two
directions be (cid:27)2
2. What is the optimal variance if this distribution
is approximated by a spherical Gaussian with variance (cid:27) 2
Q, optimized by
variational free energy minimization? If we instead optimized the objec-
tive function

1 and (cid:27)2

P (x)

Q(x; (cid:27)2)

G =Z dx P (x) ln

;

(33.54)

what would be the optimal value of (cid:27)2? Sketch a contour of the true
distribution P (x) and the two approximating distributions in the case
(cid:27)1=(cid:27)2 = 10.

[Note that in general it is not possible to evaluate the objective func-
tion G, because integrals under the true distribution P (x) are usually
intractable.]

Exercise 33.6.[2, p.436] What do you think of the idea of using a variational
method to optimize an approximating distribution Q which we then use
as a proposal density for importance sampling?

Exercise 33.7.[2 ] De(cid:12)ne the relative entropy or Kullback{Leibler divergence be-
tween two probability distributions P and Q, and state Gibbs’ inequality.

Consider the problem of approximating a joint distribution P (x; y) by a
separable distribution Q(x; y) = QX(x)QY (y). Show that if the objec-

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

33.10: Solutions

435

tive function for this approximation is

G(QX ; QY ) =Xx;y

P (x; y) log2

P (x; y)

QX (x)QY (y)

that the minimal value of G is achieved when QX and QY are equal to
the marginal distributions over x and y.

Now consider the alternative objective function

F (QX ; QY ) =Xx;y

QX(x)QY (y) log2

QX(x)QY (y)

P (x; y)

;

the probability distribution P (x; y) shown in the margin is to be ap-
proximated by a separable distribution Q(x; y) = QX(x)QY (y). State
the value of F (QX; QY ) if QX and QY are set to the marginal distribu-
tions over x and y.

Show that F (QX; QY ) has three distinct minima, identify those minima,
and evaluate F at each of them.

33.10 Solutions

Solution to exercise 33.5 (p.434). We need to know the relative entropy be-
tween two one-dimensional Gaussian distributions:

P (x; y)

x

1

1/8
1/8
0
0

2

1/8
1/8
0
0

3

4

0
0
1/4
0

0
0
0
1/4

y

1
2
3
4

Z dx Normal(x; 0; (cid:27)Q) ln
= Z dx Normal(x; 0; (cid:27)Q)"ln
P! :
2 ln

(cid:27)2
P
Q (cid:0) 1 +
(cid:27)2

(cid:27)2
Q
(cid:27)2

=

1

Normal(x; 0; (cid:27)Q)
Normal(x; 0; (cid:27)P )

(cid:27)P
(cid:27)Q (cid:0)

1
2

x2  1
Q (cid:0)
(cid:27)2

P!#

1
(cid:27)2

(33.55)

(33.56)

So, if we approximate P , whose variances are (cid:27)2
are both (cid:27)2

Q, we (cid:12)nd

1 and (cid:27)2

2, by Q, whose variances

F ((cid:27)2

Q) =

1

2 ln

(cid:27)2
1
Q (cid:0) 1 +
(cid:27)2

(cid:27)2
Q
(cid:27)2
1

+ ln

(cid:27)2
2
Q (cid:0) 1 +
(cid:27)2

(cid:27)2
Q
(cid:27)2

2! ;

di(cid:11)erentiating,

which is zero when

(cid:27)2
Q
(cid:27)2

2!# ;

d

d ln((cid:27)2

Q)

F =

+

1

Q
(cid:27)2
1

2"(cid:0)2 +  (cid:27)2
2(cid:18) 1
2(cid:19) :

1
(cid:27)2

(cid:27)2
1

+

1

1
(cid:27)2
Q

=

(33.57)

(33.58)

(33.59)

Thus we set the approximating distribution’s inverse variance to the mean
inverse variance of the target distribution P .
In the case (cid:27)1 = 10 and (cid:27)2 = 1, we obtain (cid:27)Q ’ p2, which is just a factor
of p2 larger than (cid:27)2, pretty much independent of the value of the larger
standard deviation (cid:27)1. Variational free energy minimization typically leads to
approximating distributions whose length scales match the shortest length scale
of the target distribution. The approximating distribution might be viewed as
too compact.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

436

(a)

(b)

In contrast, if we use the objective function G then we (cid:12)nd:

G((cid:27)2

Q) =

1

2 ln (cid:27)2

Q +

(cid:27)2
1
(cid:27)2
Q

+ ln (cid:27)2

Q +

(cid:27)2
2
(cid:27)2

Q! + constant;

where the constant depends on (cid:27)1 and (cid:27)2 only. Di(cid:11)erentiating,

d

d ln (cid:27)2
Q

G =

1

2"2 (cid:0)  (cid:27)2

1
(cid:27)2
Q

+

(cid:27)2
2
(cid:27)2

Q!# ;

33 | Variational Methods

Figure 33.6. Two separable
Gaussian approximations (dotted
lines) to a bivariate Gaussian
distribution (solid line). (a) The
approximation that minimizes the
variational free energy. (b) The
approximation that minimizes the
objective function G. In each
(cid:12)gure, the lines show the contours
at which xTAx = 1, where A is
the inverse covariance matrix of
the Gaussian.

(33.60)

(33.61)

(33.62)

which is zero when

(cid:27)2
Q =

1

2(cid:0)(cid:27)2

1 + (cid:27)2

2(cid:1) :

Thus we set the approximating distribution’s variance to the mean variance
of the target distribution P .
factor of p2 smaller than (cid:27)1, independent of the value of (cid:27)2.
The two approximations are shown to scale in (cid:12)gure 33.6.

In the case (cid:27)1 = 10 and (cid:27)2 = 1, we obtain (cid:27)Q ’ 10=p2, which is just a

Solution to exercise 33.6 (p.434). The best possible variational approximation
is of course the target distribution P . Assuming that this is not possible, a
good variational approximation is more compact than the true distribution.
In contrast, a good sampler is more heavy tailed than the true distribution.
An over-compact distribution would be a lousy sampler with a large variance.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

s1

sK

G

y1

yN

Figure 34.1. Error-correcting
codes as latent variable models.
The K latent variables are the
independent source bits
s1; : : : ; sK; these give rise to the
observables via the generator
matrix G.

34

Independent Component Analysis and

Latent Variable Modelling

34.1 Latent variable models

Many statistical models are generative models (that is, models that specify
a full probability density over all variables in the situation) that make use of
latent variables to describe a probability distribution over observables.

Examples of latent variable models include Chapter 22’s mixture models,
which model the observables as coming from a superposed mixture of simple
probability distributions (the latent variables are the unknown class labels
of the examples); hidden Markov models (Rabiner and Juang, 1986; Durbin
et al., 1998); and factor analysis.

The decoding problem for error-correcting codes can also be viewed in
In that case, the encoding
terms of a latent variable model { (cid:12)gure 34.1.
matrix G is normally known in advance.
In latent variable modelling, the
parameters equivalent to G are usually not known, and must be inferred from
the data along with the latent variables s.

Usually, the latent variables have a simple distribution, often a separable
distribution. Thus when we (cid:12)t a latent variable model, we are (cid:12)nding a de-
scription of the data in terms of ‘independent components’. The ‘independent
component analysis’ algorithm corresponds to perhaps the simplest possible
latent variable model with continuous latent variables.

34.2 The generative model for independent component analysis

A set of N observations D = fx(n)gN
n=1 are assumed to be generated as follows.
Each J-dimensional vector x is a linear mixture of I underlying source signals,
s:

x = Gs;

(34.1)

where the matrix of mixing coe(cid:14)cients G is not known.

The simplest algorithm results if we assume that the number of sources
is equal to the number of observations, i.e., I = J. Our aim is to recover
the source variables s (within some multiplicative factors, and possibly per-
muted). To put it another way, we aim to create the inverse of G (within a
post-multiplicative factor) given only a set of examples fxg. We assume that
the latent variables are independently distributed, with marginal distributions
P (si jH) (cid:17) pi(si). Here H denotes the assumed form of this model and the
assumed probability distributions pi of the latent variables.
The probability of the observables and the hidden variables, given G and

437

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

34 | Independent Component Analysis and Latent Variable Modelling

438

H, is:

P (fx(n); s(n)gN

n=1 j G;H) =

N

Yn=1hP (x(n) j s(n); G;H)P (s(n) jH)i
)!3
5 :

i (cid:17)1
A Yi
j (cid:0)Pi Gjis(n)

pi(s(n)

i

(34.2)

(34.3)

=

N

Yn=1

2
4

0
@Yj

(cid:14)(cid:16)x(n)

We assume that the vector x is generated without noise. This assumption is
not usually made in latent variable modelling, since noise-free data are rare;
but it makes the inference problem far simpler to solve.

The likelihood function

For learning about G from the data D, the relevant quantity is the likelihood
function

P (D j G;H) =Yn

P (x(n) j G;H)

(34.4)

which is a product of factors each of which is obtained by marginalizing over
the latent variables. When we marginalize over delta functions, remember
v f (x=v): We adopt summation convention at this
. A single factor in the

that R ds (cid:14)(x (cid:0) vs)f (s) = 1

point, such that, for example, Gjis(n)
likelihood is given by

i (cid:17) Pi Gjis(n)

i

P (x(n) j G;H) = Z dIs(n) P (x(n) j s(n); G;H)P (s(n) jH)

= Z dIs(n) Yj
jdet GjYi

=

1

(cid:14)(cid:16)x(n)

j (cid:0) Gjis(n)

i (cid:17)Yi

pi(s(n)

i

pi(G(cid:0)1

ij xj)

(34.5)

) (34.6)

(34.7)

(34.8)

) ln P (x(n) j G;H) = (cid:0) lnjdet Gj +Xi

ln pi(G(cid:0)1

ij xj):

To obtain a maximum likelihood algorithm we (cid:12)nd the gradient of the log
If we introduce W (cid:17) G(cid:0)1, the log likelihood contributed by a
likelihood.
single example may be written:
ln P (x(n) j G;H) = lnjdet Wj +Xi

ln pi(Wijxj):

(34.9)

We’ll assume from now on that det W is positive, so that we can omit the
absolute value sign. We will need the following identities:

@

@Gji

ln det G = G(cid:0)1

ij = Wij

@

@Gji
@

lj G(cid:0)1

lm = (cid:0)G(cid:0)1
G(cid:0)1
f = (cid:0)Gjm(cid:18) @

im = (cid:0)WljWim
f(cid:19) Gli:

@Glm

@Wij

Let us de(cid:12)ne ai (cid:17) Wijxj,

(cid:30)i(ai) (cid:17) d ln pi(ai)=dai;

(34.10)

(34.11)

(34.12)

(34.13)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

34.2: The generative model for independent component analysis

439

Algorithm 34.2. Independent
component analysis { online
steepest ascents version.
See also algorithm 34.4, which is
to be preferred.

Repeat for each datapoint x:

1. Put x through a linear mapping:

a = Wx:

2. Put a through a nonlinear map:

zi = (cid:30)i(ai);

where a popular choice for (cid:30) is (cid:30) = (cid:0) tanh(ai).

3. Adjust the weights in accordance with

(cid:1)W / [WT](cid:0)1 + zxT:

and zi = (cid:30)i(ai), which indicates in which direction ai needs to change to make
the probability of the data greater. We may then obtain the gradient with
respect to Gji using equations (34.10) and (34.11):

@

@Gji

ln P (x(n) j G;H) = (cid:0)Wij (cid:0) aizi0Wi0j:

Or alternatively, the derivative with respect to Wij:

@

@Wij

ln P (x(n) j G;H) = Gji + xjzi:

(34.14)

(34.15)

If we choose to change W so as to ascend this gradient, we obtain the learning
rule

(cid:1)W / [WT](cid:0)1 + zxT:

(34.16)

The algorithm so far is summarized in algorithm 34.2.

Choices of (cid:30)

The choice of the function (cid:30) de(cid:12)nes the assumed prior distribution of the
latent variable s.

Let’s (cid:12)rst consider the linear choice (cid:30)i(ai) = (cid:0)(cid:20)ai, which implicitly (via
equation 34.13) assumes a Gaussian distribution on the latent variables. The
Gaussian distribution on the latent variables is invariant under rotation of the
latent variables, so there can be no evidence favouring any particular alignment
of the latent variable space. The linear algorithm is thus uninteresting in that
it will never recover the matrix G or the original sources. Our only hope is
thus that the sources are non-Gaussian. Thankfully, most real sources have
non-Gaussian distributions; often they have heavier tails than Gaussians.

We thus move on to the popular tanh nonlinearity. If

then implicitly we are assuming

(cid:30)i(ai) = (cid:0) tanh(ai)

pi(si) / 1= cosh(si) /

1

esi + e(cid:0)si

:

(34.17)

(34.18)

This is a heavier-tailed distribution for the latent variables than the Gaussian
distribution.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

440

34 | Independent Component Analysis and Latent Variable Modelling

Figure 34.3. Illustration of the
generative models implicit in the
learning algorithm.
(a) Distributions over two
observables generated by 1= cosh
distributions on the latent

(compact distribution) and

1 (cid:21)
variables, for G =(cid:20) 3=4 1=2
G =(cid:20) 2 (cid:0)1
(cid:0)1 3=2 (cid:21) (broader

1=2

distribution). (b) Contours of the
generative distributions when the
latent variables have Cauchy
distributions. The learning
algorithm (cid:12)ts this amoeboid
object to the empirical data in
such a way as to maximize the
likelihood. The contour plot in
(b) does not adequately represent
this heavy-tailed distribution.
(c) Part of the tails of the Cauchy
distribution, giving the contours
0:01 : : : 0:1 times the density at
the origin. (d) Some data from
one of the generative distributions
illustrated in (b) and (c). Can you
tell which? 200 samples were
created, of which 196 fell in the
plotted region.

-4

-2

x1
0
0

2

4

-4

-2

x1
0
0

2

4

4

2

0
0

x2

-2

-4

4

2

0
0

x2

-2

-4

(a)

-8

-6

-4

-2

x1
0
0

2

4

6

8

8

6

4

2

0
0

-2

-4

-6

-8

(b)

30

20

10

x2

0
x2 0

-10

-20

(c)

(d)

-30

-30

-20

-10

0
0
x1

10

20

30

We could also use a tanh nonlinearity with gain (cid:12), that is, (cid:30)i(ai) =
(cid:0) tanh((cid:12)ai), whose implicit probabilistic model is pi(si) / 1=[cosh((cid:12)si)]1=(cid:12). In
the limit of large (cid:12), the nonlinearity becomes a step function and the probabil-
ity distribution pi(si) becomes a biexponential distribution, pi(si) / exp((cid:0)jsj).
In the limit (cid:12) ! 0, pi(si) approaches a Gaussian with mean zero and variance
1=(cid:12). Heavier-tailed distributions than these may also be used. The Student
and Cauchy distributions spring to mind.

Example distributions

Figures 34.3(a{c) illustrate typical distributions generated by the independent
components model when the components have 1= cosh and Cauchy distribu-
tions. Figure 34.3d shows some samples from the Cauchy model. The Cauchy
distribution, being the more heavy-tailed, gives the clearest picture of how the
predictive distribution depends on the assumed generative parameters G.

34.3 A covariant, simpler, and faster learning algorithm

We have thus derived a learning algorithm that performs steepest descents
on the likelihood function. The algorithm does not work very quickly, even
on toy data; the algorithm is ill-conditioned and illustrates nicely the general
advice that, while (cid:12)nding the gradient of an objective function is a splendid
idea, ascending the gradient directly may not be. The fact that the algorithm is
ill-conditioned can be seen in the fact that it involves a matrix inverse, which
can be arbitrarily large or even unde(cid:12)ned.

Covariant optimization in general

The principle of covariance says that a consistent algorithm should give the
same results independent of the units in which quantities are measured (Knuth,

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

34.3: A covariant, simpler, and faster learning algorithm

441

Here n is the number of iterations.

1968). A prime example of a non-covariant algorithm is the popular steepest
descents rule. A dimensionless objective function L(w) is de(cid:12)ned, its deriva-
tive with respect to some parameters w is computed, and then w is changed
by the rule

(cid:1)wi = (cid:17)

@L
@wi

:

(34.19)

This popular equation is dimensionally inconsistent: the left-hand side of this
equation has dimensions of [wi] and the right-hand side has dimensions 1=[wi].
The behaviour of the learning algorithm (34.19) is not covariant with respect
to linear rescaling of the vector w. Dimensional inconsistency is not the end of
the world, as the success of numerous gradient descent algorithms has demon-
strated, and indeed if (cid:17) decreases with n (during on-line learning) as 1=n then
the Munro{Robbins theorem (Bishop, 1992, p. 41) shows that the parameters
will asymptotically converge to the maximum likelihood parameters. But the
non-covariant algorithm may take a very large number of iterations to achieve
this convergence; indeed many former users of steepest descents algorithms
prefer to use algorithms such as conjugate gradients that adaptively (cid:12)gure
out the curvature of the objective function. The defense of equation (34.19)
that points out (cid:17) could be a dimensional constant is untenable if not all the
parameters wi have the same dimensions.

The algorithm would be covariant if it had the form

(cid:1)wi = (cid:17)Xi0

Mii0

@L
@wi

;

(34.20)

where M is a positive-de(cid:12)nite matrix whose i; i0 element has dimensions [wiwi0].
From where can we obtain such a matrix? Two sources of such matrices are
metrics and curvatures.

Metrics and curvatures

If there is a natural metric that de(cid:12)nes distances in our parameter space w,
then a matrix M can be obtained from the metric. There is often a natural
choice. In the special case where there is a known quadratic metric de(cid:12)ning
the length of a vector w, then the matrix can be obtained from the quadratic
form. For example if the length is w2 then the natural matrix is M = I, and
steepest descents is appropriate.

Another way of (cid:12)nding a metric is to look at the curvature of the objective
function, de(cid:12)ning A (cid:17) (cid:0)rrL (where r (cid:17) @=@w). Then the matrix M =
A(cid:0)1 will give a covariant algorithm; what is more, this algorithm is the Newton
algorithm, so we recognize that it will alleviate one of the principal di(cid:14)culties
with steepest descents, namely its slow convergence to a minimum when the
objective function is at all ill-conditioned. The Newton algorithm converges
to the minimum in a single step if L is quadratic.

In some problems it may be that the curvature A consists of both data-
dependent terms and data-independent terms; in this case, one might choose
to de(cid:12)ne the metric using the data-independent terms only (Gull, 1989). The
resulting algorithm will still be covariant but it will not implement an exact
Newton step. Obviously there are many covariant algorithms; there is no
unique choice. But covariant algorithms are a small subset of the set of all
algorithms!

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

442

34 | Independent Component Analysis and Latent Variable Modelling

Back to independent component analysis

For the present maximum likelihood problem we have evaluated the gradient
with respect to G and the gradient with respect to W = G(cid:0)1. Steepest
ascents in W is not covariant. Let us construct an alternative, covariant
algorithm with the help of the curvature of the log likelihood. Taking the
second derivative of the log likelihood with respect to W we obtain two terms,
the (cid:12)rst of which is data-independent:

@Gji
@Wkl

= (cid:0)GjkGli;

(34.21)

and the second of which is data-dependent:

@(zixj)
@Wkl

= xjxl(cid:14)ikz0i; (no sum over i)

(34.22)

where z0 is the derivative of z. It is tempting to drop the data-dependent term
and de(cid:12)ne the matrix M by [M (cid:0)1](ij)(kl) = [GjkGli]. However, this matrix
is not positive de(cid:12)nite (it has at least one non-positive eigenvalue), so it is
a poor approximation to the curvature of the log likelihood, which must be
positive de(cid:12)nite in the neighbourhood of a maximum likelihood solution. We
must therefore consult the data-dependent term for inspiration. The aim is
to (cid:12)nd a convenient approximation to the curvature and to obtain a covariant
algorithm, not necessarily to implement an exact Newton step. What is the
average value of xjxl(cid:14)ikz0i? If the true value of G is G(cid:3), then

(cid:10)xjxl(cid:14)ikz0i(cid:11) =(cid:10)G(cid:3)jmsmsnG(cid:3)ln(cid:14)ikz0i(cid:11) :

(34.23)

We now make several severe approximations: we replace G(cid:3) by the present
value of G, and replace the correlated average hsmsnz0ii by hsmsnihz0ii (cid:17)
(cid:6)mnDi. Here (cid:6) is the variance{covariance matrix of the latent variables
(which is assumed to exist), and Di
is the typical value of the curvature
d2 ln pi(a)=da2. Given that the sources are assumed to be independent, (cid:6)
and D are both diagonal matrices. These approximations motivate the ma-
trix M given by:

that is,

[M(cid:0)1](ij)(kl) = Gjm(cid:6)mnGln(cid:14)ikDi;

M(ij)(kl) = Wmj(cid:6)(cid:0)1

mnWnl(cid:14)ikD(cid:0)1

i

:

(34.24)

(34.25)

For simplicity, we further assume that the sources are similar to each other so
that (cid:6) and D are both homogeneous, and that (cid:6)D = 1. This will lead us to
an algorithm that is covariant with respect to linear rescaling of the data x,
but not with respect to linear rescaling of the latent variables. We thus use:

M(ij)(kl) = WmjWml(cid:14)ik:

(34.26)

Multiplying this matrix by the gradient in equation (34.15) we obtain the
following covariant learning algorithm:

(cid:1)Wij = (cid:17)(cid:0)Wij + Wi0jai0zi(cid:1) :

(34.27)

Notice that this expression does not require any inversion of the matrix W.
The only additional computation once z has been computed is a single back-
ward pass through the weights to compute the quantity

x0j = Wi0jai0

(34.28)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

34.3: A covariant, simpler, and faster learning algorithm

443

Repeat for each datapoint x:

1. Put x through a linear mapping:

a = Wx:

2. Put a through a nonlinear map:

zi = (cid:30)i(ai);

where a popular choice for (cid:30) is (cid:30) = (cid:0) tanh(ai).

3. Put a back through W:

x0 = WTa:

4. Adjust the weights in accordance with

(cid:1)W / W + zx0T:

in terms of which the covariant algorithm reads:

(cid:1)Wij = (cid:17)(cid:0)Wij + x0jzi(cid:1) :

Algorithm 34.4. Independent
component analysis { covariant
version.

(34.29)

The quantity (cid:16)Wij + x0jzi(cid:17) on the right-hand side is sometimes called the

natural gradient. The covariant independent component analysis algorithm is
summarized in algorithm 34.4.

Further reading

ICA was originally derived using an information maximization approach (Bell
and Sejnowski, 1995). Another view of ICA, in terms of energy functions,
which motivates more general models, is given by Hinton et al. (2001). Another
generalization of ICA can be found in Pearlmutter and Parra (1996, 1997).
There is now an enormous literature on applications of ICA. A variational free
energy minimization approach to ICA-like models is given in (Miskin, 2001;
Miskin and MacKay, 2000; Miskin and MacKay, 2001). Further reading on
blind separation, including non-ICA algorithms, can be found in (Jutten and
Herault, 1991; Comon et al., 1991; Hendin et al., 1994; Amari et al., 1996;
Hojen-Sorensen et al., 2002).

In(cid:12)nite models

While latent variable models with a (cid:12)nite number of latent variables are widely
used, it is often the case that our beliefs about the situation would be most
accurately captured by a very large number of latent variables.

Consider clustering, for example. If we attack speech recognition by mod-
elling words using a cluster model, how many clusters should we use? The
number of possible words is unbounded (section 18.2), so we would really like
to use a model in which it’s always possible for new clusters to arise.

Furthermore, if we do a careful job of modelling the cluster corresponding
to just one English word, we will probably (cid:12)nd that the cluster for one word
should itself be modelled as composed of clusters { indeed, a hierarchy of

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

444

34 | Independent Component Analysis and Latent Variable Modelling

clusters within clusters. The (cid:12)rst levels of the hierarchy would divide male
speakers from female, and would separate speakers from di(cid:11)erent regions {
India, Britain, Europe, and so forth. Within each of those clusters would be
subclusters for the di(cid:11)erent accents within each region. The subclusters could
have subsubclusters right down to the level of villages, streets, or families.

Thus we would often like to have in(cid:12)nite numbers of clusters; in some
cases the clusters would have a hierarchical structure, and in other cases the
hierarchy would be (cid:13)at. So, how should such in(cid:12)nite models be implemented
in (cid:12)nite computers? And how should we set up our Bayesian models so as to
avoid getting silly answers?

In(cid:12)nite mixture models for categorical data are presented in Neal (1991),
along with a Monte Carlo method for simulating inferences and predictions.
In(cid:12)nite Gaussian mixture models with a (cid:13)at hierarchical structure are pre-
sented in Rasmussen (2000). Neal (2001) shows how to use Dirichlet di(cid:11)usion
trees to de(cid:12)ne models of hierarchical clusters. Most of these ideas build on
the Dirichlet process (section 18.2). This remains an active research area
(Rasmussen and Ghahramani, 2002; Beal et al., 2002).

34.4 Exercises

Exercise 34.1.[3 ] Repeat the derivation of the algorithm, but assume a small

amount of noise in x: x = Gs + n; so the term (cid:14)(cid:16)x(n)

in the joint probability (34.3) is replaced by a probability distribution
over x(n)
. Show that, if this noise distribution has
su(cid:14)ciently small standard deviation, the identical algorithm results.

i (cid:17)
j (cid:0)Pi Gjis(n)

j with meanPi Gjis(n)

i

Exercise 34.2.[3 ] Implement the covariant ICA algorithm and apply it to toy

data.

Exercise 34.3.[4-5 ] Create algorithms appropriate for the situations: (a) x in-
cludes substantial Gaussian noise; (b) more measurements than latent
variables (J > I); (c) fewer measurements than latent variables (J < I).

Factor analysis assumes that the observations x can be described in terms of
independent latent variables fskg and independent additive noise. Thus the
observable x is given by
(34.30)

x = Gs + n;

where n is a noise vector whose components have a separable probability distri-
bution. In factor analysis it is often assumed that the probability distributions
of fskg and fnig are zero-mean Gaussians; the noise terms may have di(cid:11)erent
variances (cid:27)2
i .

Exercise 34.4.[4 ] Make a maximum likelihood algorithm for inferring G from
data, assuming the generative model x = Gs + n is correct and that s
and n have independent Gaussian distributions. Include parameters (cid:27) 2
j
to describe the variance of each nj, and maximize the likelihood with
respect to them too. Let the variance of each si be 1.

Exercise 34.5.[4C ] Implement the in(cid:12)nite Gaussian mixture model of Rasmussen

(2000).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

35

Random Inference Topics

35.1 What do you know if you are ignorant?

6

6

6

Example 35.1. A real variable x is measured in an accurate experiment. For
example, x might be the half-life of the neutron, the wavelength of light
emitted by a (cid:12)re(cid:13)y, the depth of Lake Vostok, or the mass of Jupiter’s
moon Io.

What is the probability that the value of x starts with a ‘1’, like the
charge of the electron (in S.I. units),

e = 1:602 : : : (cid:2) 10(cid:0)19 C;

and the Boltzmann constant,

k = 1:380 66 : : : (cid:2) 10(cid:0)23 J K(cid:0)1?

And what is the probability that it starts with a ‘9’, like the Faraday
constant,

F = 9:648 : : : (cid:2) 104 C mol(cid:0)1?

What about the second digit? What is the probability that the mantissa
of x starts ‘1.1...’, and what is the probability that x starts ‘9.9...’ ?

Solution. An expert on neutrons, (cid:12)re(cid:13)ies, Antarctica, or Jove might be able to
predict the value of x, and thus predict the (cid:12)rst digit with some con(cid:12)dence, but
what about someone with no knowledge of the topic? What is the probability
distribution corresponding to ‘knowing nothing’ ?

One way to attack this question is to notice that the units of x have not
been speci(cid:12)ed.
If the half-life of the neutron were measured in fortnights
instead of seconds, the number x would be divided by 1 209 600; if it were
measured in years, it would be divided by 3 (cid:2) 107. Now, is our knowledge
about x, and, in particular, our knowledge of its (cid:12)rst digit, a(cid:11)ected by the
change in units? For the expert, the answer is yes; but let us take someone
truly ignorant, for whom the answer is no; their predictions about the (cid:12)rst digit
of x are independent of the units. The arbitrariness of the units corresponds to
invariance of the probability distribution when x is multiplied by any number.

80
70
60

50

40

30

20

10
9
8
7
6

5

4

3

2

1

metres

200

100
90
80
70
60

50

40

30

20

10
9
8
7
6

5

4

3000

2000

1000
900
800
700
600

500

400

300

200

100
90
80
70
60

50

40

3
feet

inches

Figure 35.1. When viewed on a
logarithmic scale, scales using
di(cid:11)erent units are translated
relative to each other.

If you don’t know the units that a quantity is measured in, the probability
of the (cid:12)rst digit must be proportional to the length of the corresponding piece
of logarithmic scale. The probability that the (cid:12)rst digit of a number is 1 is
thus

=

log 2
log 10

:

(35.1)

p1 =

log 2 (cid:0) log 1
log 10 (cid:0) log 1

445

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

446

35 | Random Inference Topics

6?P (9)

P (3)

6
?

10
9
8
7
6

5

4

3

P (1)

2

6

?

1

Now, 210 = 1024 ’ 103 = 1000, so without needing a calculator, we have
10 log 2 ’ 3 log 10 and

:

(35.2)

p1 ’

3
10

More generally, the probability that the (cid:12)rst digit is d is

(log(d + 1) (cid:0) log(d))=(log 10 (cid:0) log 1) = log10(1 + 1=d):

(35.3)

This observation about initial digits is known as Benford’s law.
does not correspond to a uniform probability distribution over d.

Ignorance
2

. Exercise 35.2.[2 ] A pin is thrown tumbling in the air. What is the probability
distribution of the angle (cid:18)1 between the pin and the vertical at a moment
while it is in the air? The tumbling pin is photographed. What is the
probability distribution of the angle (cid:18)3 between the pin and the vertical
as imaged in the photograph?

. Exercise 35.3.[2 ] Record breaking. Consider keeping track of the world record
for some quantity x, say earthquake magnitude, or longjump distances
jumped at world championships. If we assume that attempts to break
the record take place at a steady rate, and if we assume that the under-
lying probability distribution of the outcome x, P (x), is not changing {
an assumption that I think is unlikely to be true in the case of sports
endeavours, but an interesting assumption to consider nonetheless { and
assuming no knowledge at all about P (x), what can be predicted about
successive intervals between the dates when records are broken?

35.2 The Luria{Delbr(cid:127)uck distribution

Exercise 35.4.[3C, p.449] In their landmark paper demonstrating that bacteria
could mutate from virus sensitivity to virus resistance, Luria and Delbr(cid:127)uck
(1943) wanted to estimate the mutation rate in an exponentially-growing pop-
ulation from the total number of mutants found at the end of the experi-
ment. This problem is di(cid:14)cult because the quantity measured (the number
of mutated bacteria) has a heavy-tailed probability distribution: a mutation
occuring early in the experiment can give rise to a huge number of mutants.
Unfortunately, Luria and Delbr(cid:127)uck didn’t know Bayes’ theorem, and their way
of coping with the heavy-tailed distribution involves arbitrary hacks leading to
two di(cid:11)erent estimators of the mutation rate. One of these estimators (based
on the mean number of mutated bacteria, averaging over several experiments)
has appallingly large variance, yet sampling theorists continue to use it and
base con(cid:12)dence intervals around it (Kepler and Oprea, 2001). In this exercise
you’ll do the inference right.

In each culture, a single bacterium that is not resistant gives rise, after g
generations, to N = 2g descendants, all clones except for di(cid:11)erences arising
from mutations. The (cid:12)nal culture is then exposed to a virus, and the number
of resistant bacteria n is measured. According to the now accepted mutation
hypothesis, these resistant bacteria got their resistance from random mutations
that took place during the growth of the colony. The mutation rate (per cell
per generation), a, is about one in a hundred million. The total number of
i=0 2i ’ 2g = N . If a bacterium mutates
at the ith generation, its descendants all inherit the mutation, and the (cid:12)nal
number of resistant bacteria contributed by that one ancestor is 2g(cid:0)i.

opportunities to mutate is N , sincePg(cid:0)1

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

35.3: Inferring causation

447

Given M separate experiments, in each of which a colony of size N is
m=1,

created, and where the measured numbers of resistant bacteria are fnmgM
what can we infer about the mutation rate, a?

Make the inference given the following dataset from Luria and Delbr(cid:127)uck,
for N = 2:4 (cid:2) 108: fnmg = f1; 0; 3; 0; 0; 5; 0; 5; 0; 6; 107; 0; 0; 0; 1; 0; 0; 64; 0; 35g.
[A small amount of computation is required to solve this problem.]

35.3 Inferring causation

Exercise 35.5.[2, p.450]
In the Bayesian graphical model community, the task
of inferring which way the arrows point { that is, which nodes are parents,
and which children { is one on which much has been written.

Inferring causation is tricky because of ‘likelihood equivalence’. Two graph-
ical models are likelihood-equivalent if for any setting of the parameters of
either, there exists a setting of the parameters of the other such that the two
joint probability distributions of all observables are identical. An example of
a pair of likelihood-equivalent models are A ! B and B ! A. The model
A ! B asserts that A is the parent of B, or, in very sloppy terminology, ‘A
causes B’. An example of a situation where ‘B ! A’ is true is the case where
B is the variable ‘burglar in house’ and A is the variable ‘alarm is ringing’.
Here it is literally true that B causes A. But this choice of words is confusing if
applied to another example, R ! D, where R denotes ‘it rained this morning’
and D denotes ‘the pavement is dry’. ‘R causes D’ is confusing. I’ll therefore
use the words ‘B is a parent of A’ to denote causation. Some statistical meth-
ods that use the likelihood alone are unable to use data to distinguish between
likelihood-equivalent models. In a Bayesian approach, on the other hand, two
likelihood-equivalent models may nevertheless be somewhat distinguished, in
the light of data, since likelihood-equivalence does not force a Bayesian to use
priors that assign equivalent densities over the two parameter spaces of the
models.

However, many Bayesian graphical modelling folks, perhaps out of sym-
pathy for their non-Bayesian colleagues, or from a latent urge not to appear
di(cid:11)erent from them, deliberately discard this potential advantage of Bayesian
methods { the ability to infer causation from data { by skewing their models
so that the ability goes away; a widespread orthodoxy holds that one should
identify the choices of prior for which ‘prior equivalence’ holds, i.e., the priors
such that models that are likelihood-equivalent also have identical posterior
probabilities; and then one should use one of those priors in inference and
prediction. This argument motivates the use, as the prior over all probability
vectors, of specially-constructed Dirichlet distributions.

In my view it is a philosophical error to use only those priors such that
causation cannot be inferred. Priors should be set to describe one’s assump-
tions; when this is done, it’s likely that interesting inferences about causation
can be made from data.

In this exercise, you’ll make an example of such an inference.
Consider the toy problem where A and B are binary variables. The two
models are HA!B and HB!A. HA!B asserts that the marginal probabil-
ity of A comes from a beta distribution with parameters (1; 1), i.e., the uni-
form distribution; and that the two conditional distributions P (bj a = 0) and
P (bj a = 1) also come independently from beta distributions with parameters
(1; 1). The other model assigns similar priors to the marginal probability of
B and the conditional distributions of A given B. Data are gathered, and the

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

448

35 | Random Inference Topics

counts, given F = 1000 outcomes, are

b = 0
b = 1

a = 0 a = 1
760
190
950

5
45
50

765
235

(35.4)

What are the posterior probabilities of the two hypotheses?

it’s a good idea to work this exercise out symbolically in order to spot

Hint:
all the simpli(cid:12)cations that emerge.

(cid:9) (x) =

d
dx

ln (cid:0)(x) ’ ln(x) (cid:0)

1
2x

+ O(1=x2):

(35.5)

The topic of inferring causation is a complex one. The fact that Bayesian
inference can sensibly be used to infer the directions of arrows in graphs seems
to be a neglected view, but it is certainly not the whole story. See Pearl (2000)
for discussion of many other aspects of causality.

35.4 Further exercises

Exercise 35.6.[3 ] Photons arriving at a photon detector are believed to be emit-

ted as a Poisson process with a time-varying rate,

(cid:21)(t) = exp(a + b sin(!t + (cid:30)));

(35.6)

where the parameters a, b, !, and (cid:30) are known. Data are collected during
the time t = 0 : : : T . Given that N photons arrived at times ftngN
n=1,
[Further reading: Gregory and
discuss the inference of a, b, !, and (cid:30).
Loredo (1992).]

. Exercise 35.7.[2 ] A data (cid:12)le consisting of two columns of numbers has been
printed in such a way that the boundaries between the columns are
unclear. Here are the resulting strings.

891.10.0
903.10.0
924.20.0
849.20.0
898.20.0
966.20.0
950.20.0
923.50.0

912.20.0
937.10.0
861.10.0
891.10.0
924.10.0
908.10.0
911.10.0

874.10.0
850.20.0
899.20.0
916.20.0
950.20.0
924.20.0
913.20.0

870.20.0
916.20.0
849.10.0
891.10.0
958.10.0
983.10.0
921.25.0

836.10.0
899.10.0
887.20.0
912.20.0
971.20.0
924.20.0
912.20.0

861.20.0
907.10.0
840.10.0
875.10.0
933.10.0
908.10.0
917.30.0

Discuss how probable it is, given these data, that the correct parsing of
each item is:

(a) 891:10:0 ! 891: 10:0, etc.
(b) 891:10:0 ! 891:1 0:0, etc.
[A parsing of a string is a grammatical interpretation of the string. For
example, ‘Punch bores’ could be parsed as ‘Punch (noun) bores (verb)’,
or ‘Punch (imperative verb) bores (plural noun)’.]

. Exercise 35.8.[2 ] In an experiment, the measured quantities fxng come inde-

pendently from a biexponential distribution with mean (cid:22),

P (xj (cid:22)) =

1
Z

exp((cid:0)jx (cid:0) (cid:22)j) ;

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

35.5: Solutions

449

where Z is the normalizing constant, Z = 2. The mean (cid:22) is not known.
An example of this distribution, with (cid:22) = 1, is shown in (cid:12)gure 35.2.

Assuming the four datapoints are

-3

-2

-1

0

1

2

3

Figure 35.2. The biexponential
distribution P (xj (cid:22) = 1).

fxng = f0; 0:9; 2; 6g;
what do these data tell us about (cid:22)? Include detailed sketches in your
answer. Give a range of plausible values of (cid:22).

0

3

1

2

6

7

4

5

8

35.5 Solutions

Solution to exercise 35.4 (p.446). A population of size N has N opportunities
to mutate. The probability of the number of mutations that occurred, r, is
roughly Poisson

P (r j a; N ) = e(cid:0)aN (aN )r

r!

:

(35.7)

(This is slightly inaccurate because the descendants of a mutant cannot them-
selves undergo the same mutation.) Each mutation gives rise to a number of
(cid:12)nal mutant cells ni that depends on the generation time of the mutation. If
multiplication went like clockwork then the probability of ni being 1 would
be 1=2, the probability of 2 would be 1=4, the probability of 4 would be 1=8,
and P (ni) = 1=(2n) for all ni that are powers of two. But we don’t expect
the mutant progeny to divide in exact synchrony, and we don’t know the pre-
cise timing of the end of the experiment compared to the division times. A
smoothed version of this distribution that permits all integers to occur is

P (ni) =

1
Z

1
n2
i

;

(35.8)

where Z = (cid:25)2=6 = 1:645.
[This distribution’s moments are all wrong, since
ni can never exceed N , but who cares about moments? { only sampling
theory statisticians who are barking up the wrong tree, constructing ‘unbiased
estimators’ such as ^a = ((cid:22)n=N )= log N . The error that we introduce in the
likelihood function by using the approximation to P (ni) is negligible.]

The observed number of mutants n is the sum

n =

ni:

r

Xi=1

(35.9)

The probability distribution of n given r is the convolution of r identical
distributions of the form (35.8). For example,

P (nj r = 2) =

n(cid:0)1

Xn1=1

1
Z 2

1
n2
1

1

(n (cid:0) n1)2 for n (cid:21) 2:

(35.10)

The probability distribution of n given a, which is what we need for the
Bayesian inference, is given by summing over r.

P (nj a) =

N

Xr=0

P (nj r)P (r j a; N ):

(35.11)

This quantity can’t be evaluated analytically, but for small a, it’s easy to
evaluate to any desired numerical precision by explicitly summing over r from

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

450

35 | Random Inference Topics

r = 0 to some rmax, with P (nj r) also being found for each r by rmax explicit
convolutions for all required values of n; if rmax = nmax, the largest value
of n encountered in the data, then P (nj a) is computed exactly; but for this
question’s data, rmax = 9 is plenty for an accurate result; I used rmax =
74 to make the graphs in (cid:12)gure 35.3. Octave source code is available.1
Incidentally, for data sets like the one in this exercise, which have a substantial
number of zero counts, very little is lost by making Luria and Delbruck’s second
approximation, which is to retain only the count of how many n were equal to
zero, and how many were non-zero. The likelihood function found using this
weakened data set,

L(a) = (e(cid:0)aN )11(1 (cid:0) e(cid:0)aN )9;

(35.12)

is scarcely distinguishable from the likelihood computed using full information.

Solution to exercise 35.5 (p.447). From the six terms of the form

P (Fj (cid:11)m) = Qi (cid:0)(Fi + (cid:11)mi)
(cid:0)(Pi Fi + (cid:11))

most factors cancel and all that remains is

;

(cid:0)((cid:11))

Qi (cid:0)((cid:11)mi)

P (HA!B j Data)
P (HB!A j Data)

=

(765 + 1)(235 + 1)
(950 + 1)(50 + 1)

=

3:8
1

:

(35.13)

(35.14)

There is modest evidence in favour of HA!B because the three probabilities
inferred for that hypothesis (roughly 0.95, 0.8, and 0.1) are more typical of
the prior than are the three probabilities inferred for the other (0.24, 0.008,
and 0.19). This statement sounds absurd if we think of the priors as ‘uniform’
over the three probabilities { surely, under a uniform prior, any settings of the
probabilities are equally probable? But in the natural basis, the logit basis,
the prior is proportional to p(1 (cid:0) p), and the posterior probability ratio can
be estimated by

0:95 (cid:2) 0:05 (cid:2) 0:8 (cid:2) 0:2 (cid:2) 0:1 (cid:2) 0:9

0:24 (cid:2) 0:76 (cid:2) 0:008 (cid:2) 0:992 (cid:2) 0:19 (cid:2) 0:81 ’

3
1

;

(35.15)

which is not exactly right, but it does illustrate where the preference for A ! B
is coming from.

1.2

1

0.8

0.6

0.4

0.2

0
1e-10

1

0.01

0.0001

1e-06

1e-08

1e-09

1e-08

1e-07

1e-10

1e-10

1e-09

1e-08

1e-07

Figure 35.3. Likelihood of the
mutation rate a on a linear scale
and log scale, given Luria and
Delbruck’s data. Vertical axis:
likelihood/10(cid:0)23; horizontal axis:
a.

1www.inference.phy.cam.ac.uk/itprnn/code/octave/luria0.m

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

36

Decision Theory

Decision theory is trivial, apart from computational details (just like playing
chess!).

You have a choice of various actions, a. The world may be in one of many
states x; which one occurs may be in(cid:13)uenced by your action. The world’s
state has a probability distribution P (xj a). Finally, there is a utility function
U (x; a) which speci(cid:12)es the payo(cid:11) you receive when the world is in state x and
you chose action a.

The task of decision theory is to select the action that maximizes the

expected utility,

E[U j a] =Z dKx U (x; a)P (xj a):

(36.1)

That’s all. The computational problem is to maximize E[U j a] over a.
[Pes-
simists may prefer to de(cid:12)ne a loss function L instead of a utility function U
and minimize the expected loss.]

Is there anything more to be said about decision theory?
Well, in a real problem, the choice of an appropriate utility function may
be quite di(cid:14)cult. Furthermore, when a sequence of actions is to be taken,
with each action providing information about x, we have to take into account
the e(cid:11)ect that this anticipated information may have on our subsequent ac-
tions. The resulting mixture of forward probability and inverse probability
computations in a decision problem is distinctive. In a realistic problem such
as playing a board game, the tree of possible cogitations and actions that must
be considered becomes enormous, and ‘doing the right thing’ is not simple,
because the expected utility of an action cannot be computed exactly (Russell
and Wefald, 1991; Baum and Smith, 1993; Baum and Smith, 1997).

Let’s explore an example.

36.1 Rational prospecting

Suppose you have the task of choosing the site for a Tanzanite mine. Your
(cid:12)nal action will be to select the site from a list of N sites. The nth site has
a net value called the return xn which is initially unknown, and will be found
out exactly only after site n has been chosen.
[xn equals the revenue earned
from selling the Tanzanite from that site, minus the costs of buying the site,
paying the sta(cid:11), and so forth.] At the outset, the return xn has a probability
distribution P (xn), based on the information already available.

Before you take your (cid:12)nal action you have the opportunity to do some
prospecting. Prospecting at the nth site has a cost cn and yields data dn
which reduce the uncertainty about xn.
[We’ll assume that the returns of

451

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

452

36 | Decision Theory

the N sites are unrelated to each other, and that prospecting at one site only
yields information about that site and doesn’t a(cid:11)ect the return from that site.]

Your decision problem is:

given the initial probability distributions P (x1), P (x2), . . . , P (xN ),
(cid:12)rst, decide whether to prospect, and at which sites; then, in the
light of your prospecting results, choose which site to mine.

For simplicity, let’s make everything in the problem Gaussian and focus
on the question of whether to prospect once or not. We’ll assume our utility
function is linear in xn; we wish to maximize our expected return. The utility
function is

The notation
P (y) = Normal(y; (cid:22); (cid:27)2) indicates
that y has Gaussian distribution
with mean (cid:22) and variance (cid:27)2.

if no prospecting is done, where na is the chosen ‘action’ site; and, if prospect-
ing is done, the utility is

U = xna;

(36.2)

where np is the site at which prospecting took place.

U = (cid:0)cnp + xna;

The prior distribution of the return of site n is

P (xn) = Normal(xn; (cid:22)n; (cid:27)2

n):

If you prospect at site n, the datum dn is a noisy version of xn:

P (dn j xn) = Normal(dn; xn; (cid:27)2):

(36.3)

(36.4)

(36.5)

. Exercise 36.1.[2 ] Given these assumptions, show that the prior probability dis-

tribution of dn is

P (dn) = Normal(dn; (cid:22)n; (cid:27)2 +(cid:27)2
n)

(36.6)

(mnemonic: when independent variables add, variances add), and that
the posterior distribution of xn given dn is

where

P (xn j dn) = Normal(cid:16)xn; (cid:22)0n; (cid:27)2
n0(cid:17)
1
1
n0 =
(cid:27)2 +
(cid:27)2
(mnemonic: when Gaussians multiply, precisions add).

dn=(cid:27)2 + (cid:22)n=(cid:27)2
n
1=(cid:27)2 + 1=(cid:27)2
n

(cid:22)0n =

and

(36.7)

(36.8)

1
(cid:27)2
n

To start with, let’s evaluate the expected utility if we do no prospecting (i.e.,
choose the site immediately); then we’ll evaluate the expected utility if we (cid:12)rst
prospect at one site and then make our choice. From these two results we will
be able to decide whether to prospect once or zero times, and, if we prospect
once, at which site.

So, (cid:12)rst we consider the expected utility without any prospecting.

Exercise 36.2.[2 ] Show that the optimal action, assuming no prospecting, is to

select the site with biggest mean

and the expected utility of this action is

na = argmax

n

(cid:22)n;

E[U j optimal n] = max

n

(cid:22)n:

(36.9)

(36.10)

[If your intuition says ‘surely the optimal decision should take into ac-
count the di(cid:11)erent uncertainties (cid:27)n too?’, the answer to this question is
‘reasonable { if so, then the utility function should be nonlinear in x’.]

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

36.2: Further reading

453

Now the exciting bit. Should we prospect? Once we have prospected at
site np, we will choose the site using the decision rule (36.9) with the value of
mean (cid:22)np replaced by the updated value (cid:22)0n given by (36.8). What makes the
problem exciting is that we don’t yet know the value of dn, so we don’t know
what our action na will be; indeed the whole value of doing the prospecting
comes from the fact that the outcome dn may alter the action from the one
that we would have taken in the absence of the experimental information.

From the expression for the new mean in terms of dn (36.8), and the known
variance of dn (36.6), we can compute the probability distribution of the key
quantity, (cid:22)0n, and can work out the expected utility by integrating over all
possible outcomes and their associated actions.

Exercise 36.3.[2 ] Show that the probability distribution of the new mean (cid:22)0n

(36.8) is Gaussian with mean (cid:22)n and variance

s2 (cid:17) (cid:27)2

n

(cid:27)2
n

(cid:27)2 + (cid:27)2
n

:

(36.11)

Consider prospecting at site n. Let the biggest mean of the other sites be
(cid:22)1. When we obtain the new value of the mean, (cid:22)0n, we will choose site n and
get an expected return of (cid:22)0n if (cid:22)0n > (cid:22)1, and we will choose site 1 and get an
expected return of (cid:22)1 if (cid:22)0n < (cid:22)1.

So the expected utility of prospecting at site n, then picking the best site,

is

E[U j prospect at n] = (cid:0)cn + P ((cid:22)0n < (cid:22)1) (cid:22)1 +Z 1

(cid:22)1

d(cid:22)0n (cid:22)0n Normal((cid:22)0n; (cid:22)n; s2):

(36.12)
The di(cid:11)erence in utility between prospecting and not prospecting is the
quantity of interest, and it depends on what we would have done without
prospecting; and that depends on whether (cid:22)1 is bigger than (cid:22)n.

E[U j no prospecting] = (cid:26) (cid:0)(cid:22)1
(cid:0)(cid:22)n

if (cid:22)1 (cid:21) (cid:22)n
if (cid:22)1 (cid:20) (cid:22)n:

(36.13)

So

-6

-4

-2

0

2

4

((cid:22)n (cid:0) (cid:22)1)

(cid:27)n
3.5

3

2.5

2

1.5

1

0.5

0

6

E[U j prospect at n] (cid:0) E[U j no prospecting]

d(cid:22)0n ((cid:22)0n (cid:0) (cid:22)1) Normal((cid:22)0n; (cid:22)n; s2)
d(cid:22)0n ((cid:22)1 (cid:0) (cid:22)0n) Normal((cid:22)0n; (cid:22)n; s2)

if (cid:22)1 (cid:21) (cid:22)n
if (cid:22)1 (cid:20) (cid:22)n:

(36.14)

(cid:0)cn +Z 1
(cid:0)cn +Z (cid:22)1

(cid:0)1

(cid:22)1

= 8>><
>>:

We can plot the change in expected utility due to prospecting (omitting
cn) as a function of the di(cid:11)erence ((cid:22)n (cid:0) (cid:22)1) (horizontal axis) and the initial
standard deviation (cid:27)n (vertical axis). In the (cid:12)gure the noise variance is (cid:27) 2 = 1.

36.2 Further reading

If the world in which we act is a little more complicated than the prospecting
problem { for example, if multiple iterations of prospecting are possible, and
the cost of prospecting is uncertain { then (cid:12)nding the optimal balance between
exploration and exploitation becomes a much harder computational problem.
Reinforcement learning addresses approximate methods for this problem (Sut-
ton and Barto, 1998).

Figure 36.1. Contour plot of the
gain in expected utility due to
prospecting. The contours are
equally spaced from 0.1 to 1.2 in
steps of 0.1. To decide whether it
is worth prospecting at site n, (cid:12)nd
the contour equal to cn (the cost
of prospecting); all points
[((cid:22)n(cid:0)(cid:22)1); (cid:27)n] above that contour
are worthwhile.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

454

36 | Decision Theory

36.3 Further exercises

. Exercise 36.4.[2 ] The four doors problem.

A new game show uses rules similar to those of the three doors (exer-
cise 3.8 (p.57)), but there are four doors, and the host explains: ‘First
you will point to one of the doors, and then I will open one of the other
doors, guaranteeing to choose a non-winner. Then you decide whether
to stick with your original pick or switch to one of the remaining doors.
Then I will open another non-winner (but never the current pick). You
will then make your (cid:12)nal decision by sticking with the door picked on
the previous decision or by switching to the only other remaining door.’

What is the optimal strategy? Should you switch on the (cid:12)rst opportu-
nity? Should you switch on the second opportunity?

. Exercise 36.5.[3 ] One of the challenges of decision theory is (cid:12)guring out ex-
actly what the utility function is. The utility of money, for example, is
notoriously nonlinear for most people.

In fact, the behaviour of many people cannot be captured by a coher-
ent utility function, as illustrated by the Allais paradox, which runs as
follows.

Which of these choices do you (cid:12)nd most attractive?

A. $1 million guaranteed.
B.

89% chance of $1 million;
10% chance of $2.5 million;
1% chance of nothing.

Now consider these choices:

C.

D.

89% chance of nothing;
11% chance of $1 million.
90% chance of nothing;
10% chance of $2.5 million.

Many people prefer A to B, and, at the same time, D to C. Prove
that these preferences are inconsistent with any utility function U (x)
for money.

Exercise 36.6.[4 ] Optimal stopping.

A large queue of N potential partners is waiting at your door, all asking
to marry you. They have arrived in random order. As you meet each
partner, you have to decide on the spot, based on the information so
far, whether to marry them or say no. Each potential partner has a
desirability dn, which you (cid:12)nd out if and when you meet them. You
must marry one of them, but you are not allowed to go back to anyone
you have said no to.

There are several ways to de(cid:12)ne the precise problem.

(a) Assuming your aim is to maximize the desirability dn, i.e., your
utility function is d^n, where ^n is the partner selected, what strategy
should you use?

(b) Assuming you wish very much to marry the most desirable person
(i.e., your utility function is 1 if you achieve that, and zero other-
wise); what strategy should you use?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

36.3: Further exercises

455

Action

Buy Don’t
buy

Outcome
No win (cid:0)1
Wins +9

0
0

Table 36.2. Utility in the lottery
ticket problem.

Action

Buy Don’t
buy

Outcome

No win
Wins

1
0

0
9

Table 36.3. Regret in the lottery
ticket problem.

(c) Assuming you wish very much to marry the most desirable person,

and that your strategy will be ‘strategy M ’:

Strategy M { Meet the (cid:12)rst M partners and say no to all
of them. Memorize the maximum desirability dmax among
them. Then meet the others in sequence, waiting until a
partner with dn > dmax comes along, and marry them.
If none more desirable comes along, marry the (cid:12)nal N th
partner (and feel miserable).

{ what is the optimal value of M ?

Exercise 36.7.[3 ] Regret as an objective function?

The preceding exercise (parts b and c) involved a utility function based
on regret. If one married the tenth most desirable candidate, the utility
function asserts that one would feel regret for having not chosen the
most desirable.

Many people working in learning theory and decision theory use ‘mini-
mizing the maximal possible regret’ as an objective function, but does
this make sense?

Imagine that Fred has bought a lottery ticket, and o(cid:11)ers to sell it to you
before it’s known whether the ticket is a winner. For simplicity say the
probability that the ticket is a winner is 1=100, and if it is a winner, it
is worth $10. Fred o(cid:11)ers to sell you the ticket for $1. Do you buy it?

The possible actions are ‘buy’ and ‘don’t buy’. The utilities of the four
possible action{outcome pairs are shown in table 36.2. I have assumed
that the utility of small amounts of money for you is linear. If you don’t
buy the ticket then the utility is zero regardless of whether the ticket
proves to be a winner. If you do buy the ticket you end up either losing
one pound (with probability 99=100) or gaining nine (with probability
1=100). In the minimax regret community, actions are chosen to mini-
mize the maximum possible regret. The four possible regret outcomes
are shown in table 36.3. If you buy the ticket and it doesn’t win, you
have a regret of $1, because if you had not bought it you would have
been $1 better o(cid:11). If you do not buy the ticket and it wins, you have
a regret of $9, because if you had bought it you would have been $9
better o(cid:11). The action that minimizes the maximum possible regret is
thus to buy the ticket.

Discuss whether this use of regret to choose actions can be philosophi-
cally justi(cid:12)ed.

The above problem can be turned into an investment portfolio decision
problem by imagining that you have been given one pound to invest in
two possible funds for one day: Fred’s lottery fund, and the cash fund. If
you put $f1 into Fred’s lottery fund, Fred promises to return $9f1 to you
if the lottery ticket is a winner, and otherwise nothing. The remaining
$f0 (with f0 = 1 (cid:0) f1) is kept as cash. What is the best investment?
Show that the minimax regret community will invest f1 = 9=10 of their
money in the high risk, high return lottery fund, and only f0 = 1=10 in
cash. Can this investment method be justi(cid:12)ed?

Exercise 36.8.[3 ] Gambling oddities (from Cover and Thomas (1991)). A horse
race involving I horses occurs repeatedly, and you are obliged to bet
all your money each time. Your bet at time t can be represented by

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

456

36 | Decision Theory

a normalized probability vector b multiplied by your money m(t). The
odds o(cid:11)ered by the bookies are such that if horse i wins then your return
is m(t+1) = bioim(t). Assuming the bookies’ odds are ‘fair’, that is,

= 1;

1
oi

Xi

(36.15)

and assuming that the probability that horse i wins is pi, work out the
optimal betting strategy if your aim is Cover’s aim, namely, to maximize
the expected value of log m(T ). Show that the optimal strategy sets b
equal to p, independent of the bookies’ odds o. Show that when this
strategy is used, the money is expected to grow exponentially as:

2nW (b;p)

(36.16)

where W =Pi pi log bioi.

If you only bet once, is the optimal strategy any di(cid:11)erent?

Do you think this optimal strategy makes sense? Do you think that it’s
‘optimal’, in common language, to ignore the bookies’ odds? What can
you conclude about ‘Cover’s aim’ ?

Exercise 36.9.[3 ] Two ordinary dice are thrown repeatedly; the outcome of
each throw is the sum of the two numbers. Joe Shark, who says that 6
and 8 are his lucky numbers, bets even money that a 6 will be thrown
before the (cid:12)rst 7 is thrown. If you were a gambler, would you take the
bet? What is your probability of winning? Joe then bets even money
that an 8 will be thrown before the (cid:12)rst 7 is thrown. Would you take
the bet?

Having gained your con(cid:12)dence, Joe suggests combining the two bets into
a single bet: he bets a larger sum, still at even odds, that an 8 and a
6 will be thrown before two 7s have been thrown. Would you take the
bet? What is your probability of winning?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

37

Bayesian Inference and Sampling Theory

There are two schools of statistics. Sampling theorists concentrate on having
methods guaranteed to work most of the time, given minimal assumptions.
Bayesians try to make inferences that take into account all available informa-
tion and answer the question of interest given the particular data set. As you
have probably gathered, I strongly recommend the use of Bayesian methods.

Sampling theory is the widely used approach to statistics, and most pa-
pers in most journals report their experiments using quantities like con(cid:12)dence
intervals, signi(cid:12)cance levels, and p-values. A p-value (e.g. p = 0:05) is the prob-
ability, given a null hypothesis for the probability distribution of the data, that
the outcome would be as extreme as, or more extreme than, the observed out-
come. Untrained readers { and perhaps, more worryingly, the authors of many
papers { usually interpret such a p-value as if it is a Bayesian probability (for
example, the posterior probability of the null hypothesis), an interpretation
that both sampling theorists and Bayesians would agree is incorrect.

In this chapter we study a couple of simple inference problems in order to

compare these two approaches to statistics.

While in some cases, the answers from a Bayesian approach and from sam-
pling theory are very similar, we can also (cid:12)nd cases where there are signi(cid:12)cant
di(cid:11)erences. We have already seen such an example in exercise 3.15 (p.59),
where a sampling theorist got a p-value smaller than 7%, and viewed this as
strong evidence against the null hypothesis, whereas the data actually favoured
the null hypothesis over the simplest alternative. On p.64, another example
was given where the p-value was smaller than the mystical value of 5%, yet the
data again favoured the null hypothesis. Thus in some cases, sampling theory
can be trigger-happy, declaring results to be ‘su(cid:14)ciently improbable that the
null hypothesis should be rejected’, when those results actually weakly sup-
port the null hypothesis. As we will now see, there are also inference problems
where sampling theory fails to detect ‘signi(cid:12)cant’ evidence where a Bayesian
approach and everyday intuition agree that the evidence is strong. Most telling
of all are the inference problems where the ‘signi(cid:12)cance’ assigned by sampling
theory changes depending on irrelevant factors concerned with the design of
the experiment.

This chapter is only provided for those readers who are curious about the
sampling theory / Bayesian methods debate. If you (cid:12)nd any of this chapter
tough to understand, please skip it. There is no point trying to understand
the debate. Just use Bayesian methods { they are much easier to understand
than the debate itself!

457

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

458

37 | Bayesian Inference and Sampling Theory

37.1 A medical example

We are trying to reduce the incidence of an unpleasant disease
called microsoftus. Two vaccinations, A and B, are tested on
a group of volunteers. Vaccination B is a control treatment, a
placebo treatment with no active ingredients. Of the 40 subjects,
30 are randomly assigned to have treatment A and the other 10
are given the control treatment B. We observe the subjects for one
year after their vaccinations. Of the 30 in group A, one contracts
microsoftus. Of the 10 in group B, three contract microsoftus.

Is treatment A better than treatment B?

Sampling theory has a go

The standard sampling theory approach to the question ‘is A better than B?’
is to construct a statistical test. The test usually compares a hypothesis such
as

H1: ‘A and B have di(cid:11)erent e(cid:11)ectivenesses’

with a null hypothesis such as

H0: ‘A and B have exactly the same e(cid:11)ectivenesses as each other’.

A novice might object ‘no, no, I want to compare the hypothesis \A is better
than B" with the alternative \B is better than A"!’ but such objections are
not welcome in sampling theory.

Once the two hypotheses have been de(cid:12)ned, the (cid:12)rst hypothesis is scarcely
mentioned again { attention focuses solely on the null hypothesis. It makes me
laugh to write this, but it’s true! The null hypothesis is accepted or rejected
purely on the basis of how unexpected the data were to H0, not on how much
better H1 predicted the data. One chooses a statistic which measures how
much a data set deviates from the null hypothesis. In the example here, the
standard statistic to use would be one called (cid:31)2 (chi-squared). To compute
(cid:31)2, we take the di(cid:11)erence between each data measurement and its expected
value assuming the null hypothesis to be true, and divide the square of that
di(cid:11)erence by the variance of the measurement, assuming the null hypothesis to
be true. In the present problem, the four data measurements are the integers
FA+, FA(cid:0), FB+, and FB(cid:0), that is, the number of subjects given treatment A
who contracted microsoftus (FA+), the number of subjects given treatment A
who didn’t (FA(cid:0)), and so forth. The de(cid:12)nition of (cid:31)2 is:

(cid:31)2 =Xi

(Fi (cid:0) hFii)2

hFii

:

(37.1)

Actually, in my elementary statistics book (Spiegel, 1988) I (cid:12)nd Yates’s cor-
rection is recommended:

(cid:31)2 =Xi

(jFi (cid:0) hFiij (cid:0) 0:5)2

hFii

:

(37.2)

In this case, given the null hypothesis that treatments A and B are equally
e(cid:11)ective, and have rates f+ and f(cid:0) for the two outcomes, the expected counts
are:

hFA+i=f+NA
hFB+i=f+NB

hFA(cid:0)i= f(cid:0)NA
hFB(cid:0)i=f(cid:0)NB:

(37.3)

If you want to know about Yates’s
correction, read a sampling theory
textbook. The point of this
chapter is not to teach sampling
theory; I merely mention Yates’s
correction because it is what a
professional sampling theorist
might use.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

37.1: A medical example

459

The sampling distribution of a
statistic is the probability
distribution of its value under
repetitions of the experiment,
assuming that the null hypothesis
is true.

The test accepts or rejects the null hypothesis on the basis of how big (cid:31)2 is.
To make this test precise, and give it a ‘signi(cid:12)cance level’, we have to work
out what the sampling distribution of (cid:31)2 is, taking into account the fact that
the four data points are not independent (they satisfy the two constraints
FA+ + FA(cid:0) = NA and FB+ + FB(cid:0) = NB) and the fact that the parameters
f(cid:6) are not known. These three constraints reduce the number of degrees
of freedom in the data from four to one.
[If you want to learn more about
computing the ‘number of degrees of freedom’, read a sampling theory book; in
Bayesian methods we don’t need to know all that, and quantities equivalent to
the number of degrees of freedom pop straight out of a Bayesian analysis when
they are appropriate.] These sampling distributions are tabulated by sampling
theory gnomes and come accompanied by warnings about the conditions under
which they are accurate. For example, standard tabulated distributions for (cid:31)2
are only accurate if the expected numbers Fi are about 5 or more.

Once the data arrive, sampling theorists estimate the unknown parameters

f(cid:6) of the null hypothesis from the data:

^f+ =

FA+ + FB+
NA + NB

;

^f(cid:0) =

FA(cid:0) + FB(cid:0)
NA + NB

;

(37.4)

and evaluate (cid:31)2. At this point, the sampling theory school divides itself into
two camps. One camp uses the following protocol: (cid:12)rst, before looking at the
data, pick the signi(cid:12)cance level of the test (e.g. 5%), and determine the critical
value of (cid:31)2 above which the null hypothesis will be rejected. (The signi(cid:12)cance
level is the fraction of times that the statistic (cid:31)2 would exceed the critical
value, if the null hypothesis were true.) Then evaluate (cid:31)2, compare with the
critical value, and declare the outcome of the test, and its signi(cid:12)cance level
(which was (cid:12)xed beforehand).

The second camp looks at the data, (cid:12)nds (cid:31)2, then looks in the table of
(cid:31)2-distributions for the signi(cid:12)cance level, p, for which the observed value of (cid:31)2
would be the critical value. The result of the test is then reported by giving
this value of p, which is the fraction of times that a result as extreme as the one
observed, or more extreme, would be expected to arise if the null hypothesis
were true.

Let’s apply these two methods. First camp:

cance level. The critical value for (cid:31)2 with one degree of freedom is (cid:31)2
The estimated values of f(cid:6) are

let’s pick 5% as our signi(cid:12)-
0:05 = 3:84.

f(cid:0) = 9=10:
The expected values of the four measurements are

f+ = 1=10;

hFA+i = 3
hFA(cid:0)i = 27
hFB+i = 1
hFB(cid:0)i = 9

(37.5)

(37.6)

(37.7)

(37.8)

(37.9)

and (cid:31)2 (as de(cid:12)ned in equation (37.1)) is

(cid:31)2 = 5:93:

(37.10)

Since this value exceeds 3.84, we reject the null hypothesis that the two treat-
ments are equivalent at the 0.05 signi(cid:12)cance level. However, if we use Yates’s
correction, we (cid:12)nd (cid:31)2 = 3:33, and therefore accept the null hypothesis.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

460

37 | Bayesian Inference and Sampling Theory

Camp two runs a (cid:12)nger across the (cid:31)2 table found at the back of any good
:10 and

:10 = 2:71. Interpolating between (cid:31)2

sampling theory book and (cid:12)nds (cid:31)2
(cid:31)2

:05, camp two reports ‘the p-value is p = 0:07’.

Notice that this answer does not say how much more e(cid:11)ective A is than B,
it simply says that A is ‘signi(cid:12)cantly’ di(cid:11)erent from B. And here, ‘signi(cid:12)cant’
means only ‘statistically signi(cid:12)cant’, not practically signi(cid:12)cant.

The man in the street, reading the statement that ‘the treatment was sig-
ni(cid:12)cantly di(cid:11)erent from the control (p = 0:07)’, might come to the conclusion
that ‘there is a 93% chance that the treatments di(cid:11)er in e(cid:11)ectiveness’. But
what ‘p = 0:07’ actually means is ‘if you did this experiment many times, and
the two treatments had equal e(cid:11)ectiveness, then 7% of the time you would
(cid:12)nd a value of (cid:31)2 more extreme than the one that happened here’. This has
almost nothing to do with what we want to know, which is how likely it is
that treatment A is better than B.

Let me through, I’m a Bayesian

OK, now let’s infer what we really want to know. We scrap the hypothesis
that the two treatments have exactly equal e(cid:11)ectivenesses, since we do not
believe it. There are two unknown parameters, pA+ and pB+, which are the
probabilities that people given treatments A and B, respectively, contract the
disease.

Given the data, we can infer these two probabilities, and we can answer

questions of interest by examining the posterior distribution.

The posterior distribution is

P (pA+; pB+ jfFig) =

P (fFigj pA+; pB+)P (pA+; pB+)

P (fFig)

:

(37.11)

The likelihood function is

FA+(cid:19)pFA+
P (fFigj pA+; pB+) = (cid:18) NA
= (cid:18)30
1(cid:19)p1

FB+(cid:19)pFB+
A(cid:0) (cid:18) NB
A(cid:0)(cid:18)10
3(cid:19)p3
B(cid:0):

A+ pFA(cid:0)

A+p29

B+p7

B+ pFB(cid:0)
B(cid:0)

(37.12)

(37.13)

What prior distribution should we use? The prior distribution gives us the
opportunity to include knowledge from other experiments, or a prior belief
that the two parameters pA+ and pB+, while di(cid:11)erent from each other, are
expected to have similar values.

Here we will use the simplest vanilla prior distribution, a uniform distri-

bution over each parameter.

P (pA+; pB+) = 1:

(37.14)

We can now plot the posterior distribution. Given the assumption of a sepa-
rable prior on pA+ and pB+, the posterior distribution is also separable:

P (pA+; pB+ jfFig) = P (pA+ j FA+; FA(cid:0))P (pB+ j FB+; FB(cid:0)):

(37.15)

The two posterior distributions are shown in (cid:12)gure 37.1 (except the graphs
are not normalized) and the joint posterior probability is shown in (cid:12)gure 37.2.
If we want to know the answer to the question ‘how probable is it that pA+
is smaller than pB+?’, we can answer exactly that question by computing the
posterior probability

P (pA+ < pB+ j Data);

(37.16)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

37.1: A medical example

461

Figure 37.1. Posterior
probabilities of the two
e(cid:11)ectivenesses. Treatment A {
solid line; B { dotted line.

Figure 37.2. Joint posterior
probability of the two
e(cid:11)ectivenesses { contour plot and
surface plot.

0

0.2

0.4

0.6

0.8

1

pB+

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

pA+

0

0.2

0.4

0.6

0.8

1

0

1
0.8

0.2

0.6

0.4

which is the integral of the joint posterior probability P (pA+; pB+ j Data)
shown in (cid:12)gure 37.2 over the region in which pA+ < pB+, i.e., the shaded
triangle in (cid:12)gure 37.3. The value of this integral (obtained by a straightfor-
ward numerical integration of the likelihood function (37.13) over the relevant
region) is P (pA+ < pB+ j Data) = 0:990.
Thus there is a 99% chance, given the data and our prior assumptions,
that treatment A is superior to treatment B. In conclusion, according to our
Bayesian model, the data (1 out of 30 contracted the disease after vaccination
A, and 3 out of 10 contracted the disease after vaccination B) give very strong
evidence { about 99 to one { that treatment A is superior to treatment B.

In the Bayesian approach, it is also easy to answer other relevant questions.
For example, if we want to know ‘how likely is it that treatment A is ten times
more e(cid:11)ective than treatment B?’, we can integrate the joint posterior proba-
bility P (pA+; pB+ j Data) over the region in which pA+ < 10 pB+ ((cid:12)gure 37.4).

Model comparison

1
pB+

0

0

pA+

1

Figure 37.3. The proposition
pA+ < pB+ is true for all points in
the shaded triangle. To (cid:12)nd the
probability of this proposition we
integrate the joint posterior
probability P (pA+; pB+ j Data)
((cid:12)gure 37.2) over this region.

1
pB+

If there were a situation in which we really did want to compare the two
hypotheses H0: pA+ = pB+ and H1: pA+ 6= pB+, we can of course do this
directly with Bayesian methods also.

As an example, consider the data set:

D: One subject, given treatment A, subsequently contracted microsoftus.

One subject, given treatment B, did not.

0

0

pA+

1

Figure 37.4. The proposition
pA+ < 10 pB+ is true for all points
in the shaded triangle.

Treatment A B

Got disease
Did not

Total treated

1
0

1

0
1

1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

462

37 | Bayesian Inference and Sampling Theory

How strongly does this data set favour H1 over H0?
We answer this question by computing the evidence for each hypothesis.
Let’s assume uniform priors over the unknown parameters of the models. The
(cid:12)rst hypothesis H0: pA+ = pB+ has just one unknown parameter, let’s call it
p.
(37.17)
We’ll use the uniform prior over the two parameters of model H1 that we used
before:

P (pjH0) = 1

p 2 (0; 1):

P (pA+; pB+ jH1) = 1

pA+ 2 (0; 1); pB+ 2 (0; 1):

(37.18)

Now, the probability of the data D under model H0 is the normalizing constant
from the inference of p given D:

P (D jH0) = Z dp P (D j p)P (pjH0)

= Z dp p(1 (cid:0) p) (cid:2) 1

= 1=6:

(37.19)

(37.20)

(37.21)

The probability of the data D under model H1 is given by a simple two-
dimensional integral:
P (D jH1) = Z Z dpA+ dpB+ P (D j pA+; pB+)P (pA+; pB+ jH1) (37.22)

= Z dpA+ pA+ Z dpB+ (1 (cid:0) pB+)
= 1=2 (cid:2) 1=2
= 1=4:

(37.23)

(37.24)

(37.25)

Thus the evidence ratio in favour of model H1, which asserts that the two
e(cid:11)ectivenesses are unequal, is

P (D jH1)
P (D jH0)

=

1=4
1=6

=

0:6
0:4

:

(37.26)

So if the prior probability over the two hypotheses was 50:50, the posterior
probability is 60:40 in favour of H1.
2
Is it not easy to get sensible answers to well-posed questions using Bayesian
methods?

[The sampling theory answer to this question would involve the identical
signi(cid:12)cance test that was used in the preceding problem; that test would yield
a ‘not signi(cid:12)cant’ result. I think it is greatly preferable to acknowledge what
is obvious to the intuition, namely that the data D do give weak evidence in
favour of H1. Bayesian methods quantify how weak the evidence is.]

37.2 Dependence of p-values on irrelevant information

In an expensive laboratory, Dr. Bloggs tosses a coin labelled a and b twelve
times and the outcome is the string

aaabaaaabaab;

which contains three bs and nine as.

What evidence do these data give that the coin is biased in favour of a?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

37.2: Dependence of p-values on irrelevant information

463

Dr. Bloggs consults his sampling theory friend who says ‘let r be the num-
ber of bs and n = 12 be the total number of tosses; I view r as the random
variable and (cid:12)nd the probability of r taking on the value r = 3 or a more
extreme value, assuming the null hypothesis pa = 0:5 to be true’. He thus
computes

P (r (cid:20) 3j n = 12;H0) =

n

3

r(cid:19)1/2
Xr=0(cid:18)n

= 0:07;

=(cid:0)(cid:0)12

0(cid:1) +(cid:0)12

1(cid:1) +(cid:0)12

2(cid:1) +(cid:0)12

3(cid:1)(cid:1) 1/2

12

(37.27)

and reports ‘at the signi(cid:12)cance level of 5%, there is not signi(cid:12)cant evidence
of bias in favour of a’. Or, if the friend prefers to report p-values rather than
simply compare p with 5%, he would report ‘the p-value is 7%, which is not
conventionally viewed as signi(cid:12)cantly small’. If a two-tailed test seemed more
appropriate, he might compute the two-tailed area, which is twice the above
probability, and report ‘the p-value is 15%, which is not signi(cid:12)cantly small’.
We won’t focus on the issue of the choice between the one-tailed and two-tailed
tests, as we have bigger (cid:12)sh to catch.

Dr. Bloggs pays careful attention to the calculation (37.27), and responds
‘no, no, the random variable in the experiment was not r: I decided before
running the experiment that I would keep tossing the coin until I saw three
bs; the random variable is thus n’.

Such experimental designs are not unusual.
In my experiments on error-
correcting codes I often simulate the decoding of a code until a chosen number
r of block errors (bs) has occurred, since the error on the inferred value of log pb
goes roughly as pr, independent of n.

Exercise 37.1.[2 ] Find the Bayesian inference about the bias pa of the coin
given the data, and determine whether a Bayesian’s inferences depend
on what stopping rule was in force.

According to sampling theory, a di(cid:11)erent calculation is required in order
to assess the ‘signi(cid:12)cance’ of the result n = 12. The probability distribution
of n given H0 is the probability that the (cid:12)rst n(cid:0)1 tosses contain exactly r(cid:0)1
bs and then the nth toss is a b.

P (njH0; r) =(cid:18)n(cid:0)1

r(cid:0)1(cid:19)1/2

n

:

The sampling theorist thus computes

P (n (cid:21) 12j r = 3;H0) = 0:03:

(37.28)

(37.29)

He reports back to Dr. Bloggs, ‘the p-value is 3% { there is signi(cid:12)cant evidence
of bias after all!’

What do you think Dr. Bloggs should do? Should he publish the result,
with this marvellous p-value, in one of the journals that insists that all exper-
imental results have their ‘signi(cid:12)cance’ assessed using sampling theory? Or
should he boot the sampling theorist out of the door and seek a coherent
method of assessing signi(cid:12)cance, one that does not depend on the stopping
rule?

At this point the audience divides in two. Half the audience intuitively
feel that the stopping rule is irrelevant, and don’t need any convincing that
the answer to exercise 37.1 (p.463) is ‘the inferences about pa do not depend
on the stopping rule’. The other half, perhaps on account of a thorough

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

464

37 | Bayesian Inference and Sampling Theory

training in sampling theory, intuitively feel that Dr. Bloggs’s stopping rule,
which stopped tossing the moment the third b appeared, may have biased the
experiment somehow. If you are in the second group, I encourage you to re(cid:13)ect
on the situation, and hope you’ll eventually come round to the view that is
consistent with the likelihood principle, which is that the stopping rule is not
relevant to what we have learned about pa.

As a thought experiment, consider some onlookers who (in order to save
money) are spying on Dr. Bloggs’s experiments: each time he tosses the coin,
the spies update the values of r and n. The spies are eager to make inferences
from the data as soon as each new result occurs. Should the spies’ beliefs
about the bias of the coin depend on Dr. Bloggs’s intentions regarding the
continuation of the experiment?

The fact that the p-values of sampling theory do depend on the stopping
rule (indeed, whole volumes of the sampling theory literature are concerned
with the task of assessing ‘signi(cid:12)cance’ when a complicated stopping rule is
required { ‘sequential probability ratio tests’, for example) seems to me a com-
pelling argument for having nothing to do with p-values at all. A Bayesian
solution to this inference problem was given in sections 3.2 and 3.3 and exer-
cise 3.15 (p.59).

Would it help clarify this issue if I added one more scene to the story?
The janitor, who’s been eavesdropping on Dr. Bloggs’s conversation, comes in
and says ‘I happened to notice that just after you stopped doing the experi-
ments on the coin, the O(cid:14)cer for Whimsical Departmental Rules ordered the
immediate destruction of all such coins. Your coin was therefore destroyed by
the departmental safety o(cid:14)cer. There is no way you could have continued the
experiment much beyond n = 12 tosses. Seems to me, you need to recompute
your p-value?’

37.3 Con(cid:12)dence intervals

In an experiment in which data D are obtained from a system with an unknown
parameter (cid:18), a standard concept in sampling theory is the idea of a con(cid:12)dence
interval for (cid:18). Such an interval ((cid:18)min(D); (cid:18)max(D)) has associated with it a
con(cid:12)dence level such as 95% which is informally interpreted as ‘the probability
that (cid:18) lies in the con(cid:12)dence interval’.

Let’s make precise what the con(cid:12)dence level really means, then give an
example. A con(cid:12)dence interval is a function ((cid:18)min(D); (cid:18)max(D)) of the data
set D. The con(cid:12)dence level of the con(cid:12)dence interval is a property that we can
compute before the data arrive. We imagine generating many data sets from a
particular true value of (cid:18), and calculating the interval ((cid:18)min(D); (cid:18)max(D)), and
then checking whether the true value of (cid:18) lies in that interval. If, averaging
over all these imagined repetitions of the experiment, the true value of (cid:18) lies
in the con(cid:12)dence interval a fraction f of the time, and this property holds for
all true values of (cid:18), then the con(cid:12)dence level of the con(cid:12)dence interval is f .

For example, if (cid:18) is the mean of a Gaussian distribution which is known
to have standard deviation 1, and D is a sample from that Gaussian, then
((cid:18)min(D); (cid:18)max(D)) = (D(cid:0)2; D+2) is a 95% con(cid:12)dence interval for (cid:18).
Let us now look at a simple example where the meaning of the con(cid:12)dence
level becomes clearer. Let the parameter (cid:18) be an integer, and let the data be
a pair of points x1; x2, drawn independently from the following distribution:

P (xj (cid:18)) =8<
:

1/2 x = (cid:18)
1/2 x = (cid:18) + 1
0

for other values of x.

(37.30)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

37.4: Some compromise positions

465

For example, if (cid:18) were 39, then we could expect the following data sets:

D = (x1; x2) = (39; 39) with probability 1/4;
(x1; x2) = (39; 40) with probability 1/4;
(x1; x2) = (40; 39) with probability 1/4;
(x1; x2) = (40; 40) with probability 1/4.

(37.31)

We now consider the following con(cid:12)dence interval:

[(cid:18)min(D); (cid:18)max(D)] = [min(x1; x2); min(x1; x2)]:

(37.32)

For example, if (x1; x2) = (40; 39), then the con(cid:12)dence interval for (cid:18) would be
[(cid:18)min(D); (cid:18)max(D)] = [39; 39].

Let’s think about this con(cid:12)dence interval. What is its con(cid:12)dence level?
By considering the four possibilities shown in (37.31), we can see that there
is a 75% chance that the con(cid:12)dence interval will contain the true value. The
con(cid:12)dence interval therefore has a con(cid:12)dence level of 75%, by de(cid:12)nition.

Now, what if the data we acquire are (x1; x2) = (29; 29)? Well, we can
compute the con(cid:12)dence interval, and it is [29; 29]. So shall we report this
interval, and its associated con(cid:12)dence level, 75%? This would be correct
by the rules of sampling theory. But does this make sense? What do we
actually know in this case? Intuitively, or by Bayes’ theorem, it is clear that (cid:18)
could either be 29 or 28, and both possibilities are equally likely (if the prior
probabilities of 28 and 29 were equal). The posterior probability of (cid:18) is 50%
on 29 and 50% on 28.

What if the data are (x1; x2) = (29; 30)? In this case, the con(cid:12)dence
interval is still [29; 29], and its associated con(cid:12)dence level is 75%. But in this
case, by Bayes’ theorem, or common sense, we are 100% sure that (cid:18) is 29.

In neither case is the probability that (cid:18) lies in the ‘75% con(cid:12)dence interval’

equal to 75%!

Thus

1. the way in which many people interpret the con(cid:12)dence levels of sampling

theory is incorrect;

2. given some data, what people usually want to know (whether they know

it or not) is a Bayesian posterior probability distribution.

Are all these examples contrived? Am I making a fuss about nothing?
If you are sceptical about the dogmatic views I have expressed, I encourage
you to look at a case study:
look in depth at exercise 35.4 (p.446) and the
reference (Kepler and Oprea, 2001), in which sampling theory estimates and
con(cid:12)dence intervals for a mutation rate are constructed. Try both methods
on simulated data { the Bayesian approach based on simply computing the
likelihood function, and the con(cid:12)dence interval from sampling theory; and let
me know if you don’t (cid:12)nd that the Bayesian answer is always better than the
sampling theory answer; and often much, much better. This suboptimality
of sampling theory, achieved with great e(cid:11)ort, is why I am passionate about
Bayesian methods. Bayesian methods are straightforward, and they optimally
use all the information in the data.

37.4 Some compromise positions

Let’s end on a conciliatory note. Many sampling theorists are pragmatic {
they are happy to choose from a selection of statistical methods, choosing
whichever has the ‘best’ long-run properties. In contrast, I have no problem

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

466

37 | Bayesian Inference and Sampling Theory

with the idea that there is only one answer to a well-posed problem; but it’s
not essential to convert sampling theorists to this viewpoint: instead, we can
o(cid:11)er them Bayesian estimators and Bayesian con(cid:12)dence intervals, and request
that the sampling theoretical properties of these methods be evaluated. We
don’t need to mention that the methods are derived from a Bayesian per-
spective.
If the sampling properties are good then the pragmatic sampling
theorist will choose to use the Bayesian methods. It is indeed the case that
many Bayesian methods have good sampling-theoretical properties. Perhaps
it’s not surprising that a method that gives the optimal answer for each indi-
vidual case should also be good in the long run!

Another piece of common ground can be conceded: while I believe that
most well-posed inference problems have a unique correct answer, which can
be found by Bayesian methods, not all problems are well-posed. A common
question arising in data modelling is ‘am I using an appropriate model?’ Model
criticism, that is, hunting for defects in a current model, is a task that may
be aided by sampling theory tests, in which the null hypothesis (‘the current
model is correct’) is well de(cid:12)ned, but the alternative model is not speci(cid:12)ed.
One could use sampling theory measures such as p-values to guide one’s search
for the aspects of the model most in need of scrutiny.

Further reading

My favourite reading on this topic includes (Jaynes, 1983; Gull, 1988; Loredo,
1990; Berger, 1985; Jaynes, 2003). Treatises on Bayesian statistics from the
statistics community include (Box and Tiao, 1973; O’Hagan, 1994).

37.5 Further exercises

. Exercise 37.2.[3C ] A tra(cid:14)c survey records tra(cid:14)c on two successive days. On
Friday morning, there are 12 vehicles in one hour. On Saturday morn-
ing, there are 9 vehicles in half an hour. Assuming that the vehicles are
Poisson distributed with rates (cid:21)F and (cid:21)S (in vehicles per hour) respec-
tively,

(a) is (cid:21)S greater than (cid:21)F ?
(b) by what factor is (cid:21)S bigger or smaller than (cid:21)F ?

. Exercise 37.3.[3C ] Write a program to compare treatments A and B given
data FA+, FA(cid:0), FB+, FB(cid:0) as described in section 37.1. The outputs
of the program should be (a) the probability that treatment A is more
e(cid:11)ective than treatment B; (b) the probability that pA+ < 10 pB+; (c)
the probability that pB+ < 10 pA+.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part V

Neural networks

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

38

Introduction to Neural Networks

In the (cid:12)eld of neural networks, we study the properties of networks of idealized
‘neurons’.

Three motivations underlie work in this broad and interdisciplinary (cid:12)eld.

Biology. The task of understanding how the brain works is one of the out-
standing unsolved problems in science. Some neural network models are
intended to shed light on the way in which computation and memory
are performed by brains.

Engineering. Many researchers would like to create machines that can ‘learn’,

perform ‘pattern recognition’ or ‘discover patterns in data’.

Complex systems. A third motivation for being interested in neural net-
works is that they are complex adaptive systems whose properties are
interesting in their own right.

I should emphasize several points at the outset.

(cid:15) This book gives only a taste of this (cid:12)eld. There are many interesting

neural network models which we will not have time to touch on.

(cid:15) The models that we discuss are not intended to be faithful models of
biological systems. If they are at all relevant to biology, their relevance
is on an abstract level.

(cid:15) I will describe some neural network methods that are widely used in
nonlinear data modelling, but I will not be able to give a full description
of the state of the art. If you wish to solve real problems with neural
networks, please read the relevant papers.

38.1 Memories

In the next few chapters we will meet several neural network models which
come with simple learning algorithms which make them function as memories.
Perhaps we should dwell for a moment on the conventional idea of memory
in digital computation. A memory (a string of 5000 bits describing the name
of a person and an image of their face, say) is stored in a digital computer
at an address. To retrieve the memory you need to know the address. The
address has nothing to do with the memory itself. Notice the properties that
this scheme does not have:

1. Address-based memory is not associative. Imagine you know half of a
memory, say someone’s face, and you would like to recall the rest of the

468

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

38.1: Memories

469

memory { their name. If your memory is address-based then you can’t
get at a memory without knowing the address. [Computer scientists have
devoted e(cid:11)ort to wrapping traditional address-based memories inside
cunning software to produce content-addressable memories, but content-
addressability does not come naturally. It has to be added on.]

2. Address-based memory is not robust or fault-tolerant. If a one-bit mis-
take is made in specifying the address then a completely di(cid:11)erent mem-
ory will be retrieved. If one bit of a memory is (cid:13)ipped then whenever
that memory is retrieved the error will be present. Of course, in all mod-
ern computers, error-correcting codes are used in the memory, so that
small numbers of errors can be detected and corrected. But this error-
tolerance is not an intrinsic property of the memory system. If minor
damage occurs to certain hardware that implements memory retrieval,
it is likely that all functionality will be catastrophically lost.

3. Address-based memory is not distributed.

In a serial computer that
is accessing a particular memory, only a tiny fraction of the devices
participate in the memory recall: the CPU and the circuits that are
storing the required byte. All the other millions of devices in the machine
are sitting idle.

Are there models of truly parallel computation, in which multiple de-
vices participate in all computations? [Present-day parallel computers
scarcely di(cid:11)er from serial computers from this point of view. Memory
retrieval works in just the same way, and control of the computation
process resides in CPUs. There are simply a few more CPUs. Most of
the devices sit idle most of the time.]

Biological memory systems are completely di(cid:11)erent.

1. Biological memory is associative. Memory recall is content-addressable.
Given a person’s name, we can often recall their face; and vice versa.
Memories are apparently recalled spontaneously, not just at the request
of some CPU.

2. Biological memory recall is error-tolerant and robust.

(cid:15) Errors in the cues for memory recall can be corrected. An example
asks you to recall ‘An American politician who was very intelligent
and whose politician father did not like broccoli’. Many people
think of president Bush { even though one of the cues contains an
error.

(cid:15) Hardware faults can also be tolerated. Our brains are noisy lumps
of meat that are in a continual state of change, with cells being
damaged by natural processes, alcohol, and boxing. While the cells
in our brains and the proteins in our cells are continually changing,
many of our memories persist una(cid:11)ected.

3. Biological memory is parallel and distributed { not completely distributed
throughout the whole brain: there does appear to be some functional
specialization { but in the parts of the brain where memories are stored,
it seems that many neurons participate in the storage of multiple mem-
ories.

These properties of biological memory systems motivate the study of ‘arti-
(cid:12)cial neural networks’ { parallel distributed computational systems consisting

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

470

38 | Introduction to Neural Networks

of many interacting simple elements. The hope is that these model systems
might give some hints as to how neural computation is achieved in real bio-
logical neural networks.

38.2 Terminology

Each time we describe a neural network algorithm we will typically specify
three things. [If any of this terminology is hard to understand, it’s probably
best to dive straight into the next chapter.]

Architecture. The architecture speci(cid:12)es what variables are involved in the
network and their topological relationships { for example, the variables
involved in a neural net might be the weights of the connections between
the neurons, along with the activities of the neurons.

Activity rule. Most neural network models have short time-scale dynamics:
local rules de(cid:12)ne how the activities of the neurons change in response
to each other. Typically the activity rule depends on the weights (the
parameters) in the network.

Learning rule. The learning rule speci(cid:12)es the way in which the neural net-
work’s weights change with time. This learning is usually viewed as
taking place on a longer time scale than the time scale of the dynamics
under the activity rule. Usually the learning rule will depend on the
activities of the neurons.
It may also depend on the values of target
values supplied by a teacher and on the current value of the weights.

Where do these rules come from? Often, activity rules and learning rules are
invented by imaginative researchers. Alternatively, activity rules and learning
rules may be derived from carefully chosen objective functions.

Neural network algorithms can be roughly divided into two classes.

Supervised neural networks are given data in the form of inputs and tar-
gets, the targets being a teacher’s speci(cid:12)cation of what the neural net-
work’s response to the input should be.

Unsupervised neural networks are given data in an undivided form { sim-
ply a set of examples fxg. Some learning algorithms are intended simply
to memorize these data in such a way that the examples can be recalled
in the future. Other algorithms are intended to ‘generalize’, to discover
‘patterns’ in the data, or extract the underlying ‘features’ from them.

Some unsupervised algorithms are able to make predictions { for exam-
ple, some algorithms can ‘(cid:12)ll in’ missing variables in an example x { and
so can also be viewed as supervised networks.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

39

The Single Neuron as a Classi(cid:12)er

39.1 The single neuron

We will study a single neuron for two reasons. First, many neural network
models are built out of single neurons, so it is good to understand them in
detail. And second, a single neuron is itself capable of ‘learning’ { indeed,
various standard statistical methods can be viewed in terms of single neurons
{ so this model will serve as a (cid:12)rst example of a supervised neural network.

De(cid:12)nition of a single neuron

We will start by de(cid:12)ning the architecture and the activity rule of a single
neuron, and we will then derive a learning rule.

Architecture. A single neuron has a number I of inputs xi and one output
which we will here call y. (See (cid:12)gure 39.1.) Associated with each input
is a weight wi (i = 1; : : : ; I). There may be an additional parameter
w0 of the neuron called a bias which we may view as being the weight
associated with an input x0 that is permanently set to 1. The single
neuron is a feedforward device { the connections are directed from the
inputs to the output of the neuron.

Activity rule. The activity rule has two steps.

1. First, in response to the imposed inputs x, we compute the activa-

tion of the neuron,

a =Xi

wixi;

(39.1)

where the sum is over i = 0; : : : ; I if there is a bias and i = 1; : : : ; I
otherwise.

2. Second, the output y is set as a function f (a) of the activation.
The output is also called the activity of the neuron, not to be
confused with the activation a. There are several possible activation
functions; here are the most popular.

(a) Deterministic activation functions:

i. Linear.

ii. Sigmoid (logistic function).

y(a) = a:

(39.2)

y(a) =

1

1 + e(cid:0)a

(y 2 (0; 1)):

(39.3)

471

b(cid:8)(cid:8)(cid:8)w0 (cid:18)(cid:17)(cid:19)(cid:16)6
b
b
b
b

(cid:1)
w1
(cid:1)
x1

A
wI
A
xI

(cid:1)(cid:1)
(cid:5)(cid:5)
(cid:5)

AA
EE
E
A

: : :

E

E

y

(cid:1)

(cid:5)

(cid:5)

Figure 39.1. A single neuron

activation

activity

a ! y(a)

1

0

-5

0

5

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

472

39 | The Single Neuron as a Classi(cid:12)er

iii. Sigmoid (tanh).

y(a) = tanh(a)

(y 2 ((cid:0)1; 1)):

(39.4)

iv. Threshold function.

y(a) = (cid:2)(a) (cid:17)(cid:26) 1 a > 0
(cid:0)1 a (cid:20) 0:

(39.5)

(b) Stochastic activation functions: y is stochastically selected from

(cid:6)1.
i. Heat bath.

1
0
-1

1
0
-1

-5

-5

0

0

5

5

y(a) =8<
:

with probability

1
(cid:0)1 otherwise.

1

1 + e(cid:0)a

(39.6)

ii. The Metropolis rule produces the output in a way that

depends on the previous output state y:

Compute (cid:1) = ay
If (cid:1) (cid:20) 0; (cid:13)ip y to the other state
Else (cid:13)ip y to the other state with probability e(cid:0)(cid:1).

39.2 Basic neural network concepts

A neural network implements a function y(x; w); the ‘output’ of the network,
y, is a nonlinear function of the ‘inputs’ x; this function is parameterized by
‘weights’ w.

We will study a single neuron which produces an output between 0 and 1

as the following function of x:

y(x; w) =

1

1 + e(cid:0)w(cid:1)x :

(39.7)

Exercise 39.1.[1 ] In what contexts have we encountered the function y(x; w) =

1=(1 + e(cid:0)w(cid:1)x) already?

Motivations for the linear logistic function

In section 11.2 we studied ‘the best detection of pulses’, assuming that one
of two signals x0 and x1 had been transmitted over a Gaussian channel with
variance{covariance matrix A(cid:0)1. We found that the probability that the source
signal was s = 1 rather than s = 0, given the received signal y, was

P (s = 1j y) =

1

1 + exp((cid:0)a(y))

;

(39.8)

where a(y) was a linear function of the received vector,

a(y) = wTy + (cid:18);

(39.9)

with w (cid:17) A(x1 (cid:0) x0).
the exercises.

The linear logistic function can be motivated in several other ways { see

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

39.2: Basic neural network concepts

473

Figure 39.2. Output of a simple
neural network as a function of its
input.

1

0.5

-10

-5

10

5
x2

0

0
x1

5

10

-5

-10

w = (0; 2)

Input space and weight space

For convenience let us study the case where the input vector x and the param-
eter vector w are both two-dimensional: x = (x1; x2), w = (w1; w2). Then we
can spell out the function performed by the neuron thus:

y(x; w) =

1

1 + e(cid:0)(w1x1+w2x2)

:

(39.10)

Figure 39.2 shows the output of the neuron as a function of the input vector,
for w = (0; 2). The two horizontal axes of this (cid:12)gure are the inputs x1 and x2,
with the output y on the vertical axis. Notice that on any line perpendicular
to w, the output is constant; and along a line in the direction of w, the output
is a sigmoid function.

We now introduce the idea of weight space, that is, the parameter space of
the network. In this case, there are two parameters w1 and w2, so the weight
space is two dimensional. This weight space is shown in (cid:12)gure 39.3. For a
selection of values of the parameter vector w, smaller inset (cid:12)gures show the
function of x performed by the network when w is set to those values. Each of
these smaller (cid:12)gures is equivalent to (cid:12)gure 39.2. Thus each point in w space
corresponds to a function of x. Notice that the gain of the sigmoid function
(the gradient of the ramp) increases as the magnitude of w increases.

Now, the central idea of supervised neural networks is this. Given examples
of a relationship between an input vector x, and a target t, we hope to make
the neural network ‘learn’ a model of the relationship between x and t. A
successfully trained network will, for any given x, give an output y that is
close (in some sense) to the target value t. Training the network involves
searching in the weight space of the network for a value of w that produces a
function that (cid:12)ts the provided training data well.

Typically an objective function or error function is de(cid:12)ned, as a function
of w, to measure how well the network with weights set to w solves the task.
The objective function is a sum of terms, one for each input/target pair fx; tg,
measuring how close the output y(x; w) is to the target t. The training process
is an exercise in function minimization { i.e., adjusting w in such a way as to
(cid:12)nd a w that minimizes the objective function. Many function-minimization
algorithms make use not only of the objective function, but also its gradient
with respect to the parameters w. For general feedforward neural networks
the backpropagation algorithm e(cid:14)ciently evaluates the gradient of the output
y with respect to the parameters w, and thence the gradient of the objective
function with respect to w.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

474

39 | The Single Neuron as a Classi(cid:12)er

5

4

3

2

1

0

(cid:0)1

(cid:0)2

1

0.5

0

-10

-5

10

5
x2

0

0
x1

5

10

-5

-10

w = ((cid:0)2; 3)

1

0.5

-10

-5

0
x1

5

10

-5

-10

10

5
x2

0

w = ((cid:0)2;(cid:0)1)

w2

1

0.5
6
-10

0

10

5
x2

0

-5

0
x1

5

10

-5

-10

w = (1; 4)

1

0.5

10

5
x2

0

0

-10

-5

1

0.5

-10

-5

0
x1

5

10

-5

-10

w = (0; 2)

10

5
x2

0

0
x1

5

10

-5

-10

w = (2; 2)

1

0.5

10

5
x2

0

-10

-5

1

0.5

-10

-5

0
x1

5

10

-5

-10

w = (1; 0)

10

5
x2

0

0
x1

5

10

-5

-10

w = (3; 0)

1

0.5

0

-10

-5

10

5
x2

0

0
x1

5

10

-5

-10

w = (2;(cid:0)2)

1

0.5

-10

-5

1

0.5

-10

-5

10

5
x2

0

0
x1

5

10

-5

-10

w = (5; 4)

10

5
x2

0

0
x1

5

10

-5

-10

w = (5; 1)

-

w1

(cid:0)3

(cid:0)2

(cid:0)1

0

1

2

3

4

5

6

Figure 39.3. Weight space.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

39.3: Training the single neuron as a binary classi(cid:12)er

475

39.3 Training the single neuron as a binary classi(cid:12)er

We assume we have a data set of inputs fx(n)gN
n=1,
and a neuron whose output y(x; w) is bounded between 0 and 1. We can then
write down the following error function:

n=1 with binary labels ft(n)gN

G(w) = (cid:0)Xn ht(n) ln y(x(n); w) + (1 (cid:0) t(n)) ln(1 (cid:0) y(x(n); w))i :

(39.11)

Each term in this objective function may be recognized as the information
content of one outcome. It may also be described as the relative entropy be-
tween the empirical probability distribution (t(n); 1(cid:0) t(n)) and the probability
distribution implied by the output of the neuron (y; 1(cid:0)y). The objective func-
tion is bounded below by zero and only attains this value if y(x(n); w) = t(n)
for all n.

We now di(cid:11)erentiate this objective function with respect to w.

Exercise 39.2.[2 ] The backpropagation algorithm. Show that the derivative g =

@G=@w is given by:

gj =

@G
@wj

=

N

Xn=1

(cid:0)(t(n) (cid:0) y(n))x(n)

j

:

(39.12)

Notice that the quantity e(n) (cid:17) t(n) (cid:0) y(n) is the error on example n { the
di(cid:11)erence between the target and the output. The simplest thing to do with
a gradient of an error function is to descend it (even though this is often di-
mensionally incorrect, since a gradient has dimensions [1/parameter], whereas
a change in a parameter has dimensions [parameter]). Since the derivative
@G=@w is a sum of terms g(n) de(cid:12)ned by

g(n)
j (cid:17) (cid:0)(t(n) (cid:0) y(n))x(n)

j

(39.13)

for n = 1; : : : ; N , we can obtain a simple on-line algorithm by putting each
input through the network one at a time, and adjusting w a little in a direction
opposite to g(n).

We summarize the whole learning algorithm.

The on-line gradient-descent learning algorithm

Architecture. A single neuron has a number I of inputs xi and one output

y. Associated with each input is a weight wi (i = 1; : : : ; I).

Activity rule.

1. First, in response to the received inputs x (which may be
arbitrary real numbers), we compute the activation of the neuron,

a =Xi

wixi;

(39.14)

where the sum is over i = 0; : : : ; I if there is a bias and i = 1; : : : ; I
otherwise.

2. Second, the output y is set as a sigmoid function of the activation.

y(a) =

1

1 + e(cid:0)a :

(39.15)

This output might be viewed as stating the probability, according to the
neuron, that the given input is in class 1 rather than class 0.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

476

39 | The Single Neuron as a Classi(cid:12)er

Learning rule. The teacher supplies a target value t 2 f0; 1g which says
what the correct answer is for the given input. We compute the error
signal

e = t (cid:0) y

(39.16)

then adjust the weights w in a direction that would reduce the magnitude
of this error:

(cid:1)wi = (cid:17)exi;

(39.17)

where (cid:17) is the ‘learning rate’. Commonly (cid:17) is set by trial and error to a
constant value or to a decreasing function of simulation time (cid:28) such as
(cid:17)0=(cid:28) .

The activity rule and learning rule are repeated for each input/target pair
(x; t) that is presented. If there is a (cid:12)xed data set of size N , we can cycle
through the data multiple times.

Batch learning versus on-line learning

Here we have described the on-line learning algorithm, in which a change in
the weights is made after every example is presented. An alternative paradigm
is to go through a batch of examples, computing the outputs and errors and
accumulating the changes speci(cid:12)ed in equation (39.17) which are then made
at the end of the batch.

Batch learning for the single neuron classi(cid:12)er

For each input/target pair (x(n); t(n))

(n = 1; : : : ; N ),

compute

y(n) = y(x(n); w), where

y(x; w) =

;

(39.18)

de(cid:12)ne e(n) = t(n) (cid:0) y(n), and compute for each weight wi

1 + exp((cid:0)Pi wixi)
g(n)
i = (cid:0)e(n)x(n)

:

1

i

(39.19)

(39.20)

Then let

(cid:1)wi = (cid:0)(cid:17)Xn

g(n)
i

:

This batch learning algorithm is a gradient descent algorithm, whereas the
on-line algorithm is a stochastic gradient descent algorithm. Source code
implementing batch learning is given in algorithm 39.5. This algorithm is
demonstrated in (cid:12)gure 39.4 for a neuron with two inputs with weights w1 and
w2 and a bias w0, performing the function

y(x; w) =

1

1 + e(cid:0)(w0+w1x1+w2x2) :

(39.21)

The bias w0 is included, in contrast to (cid:12)gure 39.3, where it was omitted. The
neuron is trained on a data set of ten labelled examples.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

39.3: Training the single neuron as a binary classi(cid:12)er

477

x2

(a)

(f)

10

8

6

4

2

0

10

8

6

4

2

0

10

8

6

4

2

0

(h)

10

8

6

4

2

0

(j)

w2

(c)

3
2.5
2
1.5
1
0.5
0
-0.5

-0.5 0 0.5 1 1.5 2 2.5 3

2

0

-2

-4

-6

-8

(b)

-10

-12

w1

10

8

6

4

2

0

10

8

6

4

2

0

10

0

2

4

6

8

10

0

2

4

6

8

10

(d)

7

6

5

4

3

2

1

0

400

350

300

250

200

150

100

50

0

2

4

6

8

10

x1

0

2

4

6

8

10

0

2

4

6

8

10

(g)

(i)

8

6

4

2

0

(k)

0

2

4

6

8

10

w0
w1
w2

1

10

100

1000 10000 100000

G(w)

1

10

100

1000 10000 100000

E_W(w)

0

2

4

6

8

10

(e)

0

1

10

100

1000 10000 100000

Figure 39.4. A single neuron learning to classify by gradient descent. The neuron has two weights w1
and w2 and a bias w0. The learning rate was set to (cid:17) = 0:01 and batch-mode gradient
descent was performed using the code displayed in algorithm 39.5. (a) The training data.
(b) Evolution of weights w0, w1 and w2 as a function of number of iterations (on log scale).
(c) Evolution of weights w1 and w2 in weight space. (d) The objective function G(w) as a
function of number of iterations. (e) The magnitude of the weights EW (w) as a function of
time. (f{k) The function performed by the neuron (shown by three of its contours) after 30,
80, 500, 3000, 10 000 and 40 000 iterations. The contours shown are those corresponding to
a = 0;(cid:6)1, namely y = 0:5; 0:27 and 0:73. Also shown is a vector proportional to (w1; w2).
The larger the weights are, the bigger this vector becomes, and the closer together are the
contours.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

478

39 | The Single Neuron as a Classi(cid:12)er

Algorithm 39.5. Octave source
code for a gradient descent
optimizer of a single neuron,
batch learning, with optional
weight decay (rate alpha).
Octave notation: the instruction
a = x * w causes the (N (cid:2) I)
matrix x consisting of all the
input vectors to be multiplied by
the weight vector w, giving the
vector a listing the activations for
all N input vectors; x’ means
x-transpose; the single command
y = sigmoid(a) computes the
sigmoid function of all elements of
the vector a.

Figure 39.6. The in(cid:13)uence of
weight decay on a single neuron’s
learning. The objective function is
M (w) = G(w) + (cid:11)EW (w). The
learning method was as in
(cid:12)gure 39.4. (a) Evolution of
weights w0, w1 and w2. (b)
Evolution of weights w1 and w2 in
weight space shown by points,
contrasted with the trajectory
followed in the case of zero weight
decay, shown by a thin line (from
(cid:12)gure 39.4). Notice that for this
problem weight decay has an
e(cid:11)ect very similar to ‘early
stopping’. (c) The objective
function M (w) and the error
function G(w) as a function of
number of iterations. (d) The
function performed by the neuron
after 40 000 iterations.

global x ;
global t ;

# x is an N * I matrix containing all the input vectors
# t is a vector of length N containing all the targets

for l = 1:L

# loop L times

;

a = x * w
y = sigmoid(a) ;
e = t - y
g = - x’ * e ;
w = w - eta * ( g + alpha * w )

;

# compute all activations
# compute outputs
# compute errors
# compute the gradient vector
# make step, using learning rate eta
#

and weight decay alpha

;

endfor

function f = sigmoid ( v )

f = 1.0 ./ ( 1.0 .+ exp ( - v ) ) ;

endfunction

(cid:11) = 0:01

(cid:11) = 0:1

(cid:11) = 1

2

0

-2

-4

-6

-8

-10

-12

w0
w1
w2

1

10

100

1000 10000 100000

2

1

0

-1

-2

-3

-4

-5

w0
w1
w2

1

10

100

1000 10000 100000

2

1

0

-1

-2

-3

-4

-5

w0
w1
w2

1

10

100

1000 10000 100000

3
2.5
2
1.5
1
0.5
0
-0.5

-0.5 0 0.5 1 1.5 2 2.5 3

3
2.5
2
1.5
1
0.5
0
-0.5

-0.5 0 0.5 1 1.5 2 2.5 3

3
2.5
2
1.5
1
0.5
0
-0.5

-0.5 0 0.5 1 1.5 2 2.5 3

7

6

5

4

3

2

1

0

1
10

8

6

4

2

0

G(w)
M(w)

10

100

1000 10000 100000

0

2

4

6

8

10

7

6

5

4

3

2

1

0

1
10

8

6

4

2

0

G(w)
M(w)

10

100

1000 10000 100000

0

2

4

6

8

10

7

6

5

4

3

2

1

0

1
10

8

6

4

2

0

G(w)
M(w)

10

100

1000 10000 100000

0

2

4

6

8

10

(a)

(b)

(c)

(d)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

39.4: Beyond descent on the error function: regularization

479

39.4 Beyond descent on the error function: regularization

If the parameter (cid:17) is set to an appropriate value, this algorithm works: the
algorithm (cid:12)nds a setting of w that correctly classi(cid:12)es as many of the examples
as possible.

If the examples are in fact linearly separable then the neuron (cid:12)nds this lin-
ear separation and its weights diverge to ever-larger values as the simulation
continues. This can be seen happening in (cid:12)gure 39.4(f{k). This is an exam-
ple of over(cid:12)tting, where a model (cid:12)ts the data so well that its generalization
performance is likely to be adversely a(cid:11)ected.

This behaviour may be viewed as undesirable. How can it be recti(cid:12)ed?
An ad hoc solution to over(cid:12)tting is to use early stopping, that is, use
an algorithm originally intended to minimize the error function G(w), then
prevent it from doing so by halting the algorithm at some point.

A more principled solution to over(cid:12)tting makes use of regularization. Reg-
ularization involves modifying the objective function in such a way as to in-
corporate a bias against the sorts of solution w which we dislike. In the above
example, what we dislike is the development of a very sharp decision bound-
ary in (cid:12)gure 39.4k; this sharp boundary is associated with large weight values,
so we use a regularizer that penalizes large weight values. We modify the
objective function to:

M (w) = G(w) + (cid:11)EW (w)

(39.22)

where the simplest choice of regularizer is the weight decay regularizer

EW (w) =

w2
i :

1

2Xi

(39.23)

The regularization constant (cid:11) is called the weight decay rate. This additional
term favours small values of w and decreases the tendency of a model to over(cid:12)t
(cid:12)ne details of the training data. The quantity (cid:11) is known as a hyperparameter.
Hyperparameters play a role in the learning algorithm but play no role in the
activity rule of the network.

Exercise 39.3.[1 ] Compute the derivative of M (w) with respect to wi. Why is

the above regularizer known as the ‘weight decay’ regularizer?

The gradient descent source code of algorithm 39.5 implements weight decay.
This gradient descent algorithm is demonstrated in (cid:12)gure 39.6 using weight
decay rates (cid:11) = 0:01, 0:1, and 1. As the weight decay rate is increased
the solution becomes biased towards broader sigmoid functions with decision
boundaries that are closer to the origin.

Note

Gradient descent with a step size (cid:17) is in general not the most e(cid:14)cient way to
minimize a function. A modi(cid:12)cation of gradient descent known as momentum,
while improving convergence, is also not recommended. Most neural network
experts use more advanced optimizers such as conjugate gradient algorithms.
[Please do not confuse momentum, which is sometimes given the symbol (cid:11),
with weight decay.]

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

480

39 | The Single Neuron as a Classi(cid:12)er

39.5 Further exercises

More motivations for the linear neuron

. Exercise 39.4.[2 ] Consider the task of recognizing which of two Gaussian distri-
butions a vector z comes from. Unlike the case studied in section 11.2,
where the distributions had di(cid:11)erent means but a common variance{
covariance matrix, we will assume that the two distributions have ex-
actly the same mean but di(cid:11)erent variances. Let the probability of z
given s (s 2 f0; 1g) be

P (zj s) =

I

Yi=1

Normal(zi; 0; (cid:27)2

si);

(39.24)

si is the variance of zi when the source symbol is s. Show that

where (cid:27)2
P (s = 1j z) can be written in the form

P (s = 1j z) =

1 + exp((cid:0)wTx + (cid:18))
where xi is an appropriate function of zi, xi = g(zi).

1

;

(39.25)

Exercise 39.5.[2 ] The noisy LED.

2

5

1

4

7

3

6

c(2) =

c(3) =

c(8) =

Consider an LED display with 7 elements numbered as shown above. The
state of the display is a vector x. When the controller wants the display
to show character number s, e.g. s = 2, each element xj (j = 1; : : : ; 7)
either adopts its intended state cj(s), with probability 1(cid:0)f , or is (cid:13)ipped,
with probability f . Let’s call the two states of x ‘+1’ and ‘(cid:0)1’.
(a) Assuming that the intended character s is actually a 2 or a 3, what
is the probability of s, given the state x? Show that P (s = 2j x)
can be written in the form

P (s = 2j x) =

1

1 + exp((cid:0)wTx + (cid:18))

;

(39.26)

and compute the values of the weights w in the case f = 0:1.

(b) Assuming that s is one of f0; 1; 2; : : : ; 9g, with prior probabilities
ps, what is the probability of s, given the state x? Put your answer
in the form

P (sj x) =

eas

eas0

Xs0

;

(39.27)

where fasg are functions of fcj(s)g and x.

Could you make a better alphabet of 10 characters for a noisy LED, i.e.,
an alphabet less susceptible to confusion?

. Exercise 39.6.[2 ] A (3; 1) error-correcting code consists of the two codewords
x(1) = (1; 0; 0) and x(2) = (0; 0; 1). A source bit s 2 f1; 2g having proba-
bility distribution fp1; p2g is used to select one of the two codewords for
transmission over a binary symmetric channel with noise level f . The

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Table 39.7. An alternative
15-character alphabet for the
7-element LED display.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

39.5: Further exercises

481

received vector is r. Show that the posterior probability of s given r can
be written in the form

P (s = 1j r) =

1 + exp(cid:16)(cid:0)w0 (cid:0)P3
and give expressions for the coe(cid:14)cients fwng3
Describe, with a diagram, how this optimal decoder can be expressed in
terms of a ‘neuron’.

n=1 and the bias, w0.

1

n=1 wnrn(cid:17) ;

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Problems to look at before Chapter 40

. Exercise 40.1.[2 ] What is PN

K=0(cid:0)N
K(cid:1)?

[The symbol (cid:0)N

K(cid:1) means the combination

N !

K!(N(cid:0)K)!.]

. Exercise 40.2.[2 ] If the top row of Pascal’s triangle (which contains the single
number ‘1’) is denoted row zero, what is the sum of all the numbers in
the triangle above row N ?

. Exercise 40.3.[2 ] 3 points are selected at random on the surface of a sphere.

What is the probability that all of them lie on a single hemisphere?

This chapter’s material is originally due to Polya (1954) and Cover (1965) and
the exposition that follows is Yaser Abu-Mostafa’s.

482

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

40

Capacity of a Single Neuron

ftngN

n=1

-

Learning
algorithm

- w

6

fxngN

n=1

- w
6

- f^tngN

n=1

Figure 40.1. Neural network
learning viewed as
communication.

fxngN

n=1

40.1 Neural network learning as communication

n=1 at given locations fxngN

Many neural network models involve the adaptation of a set of weights w in
response to a set of data points, for example a set of N target values DN =
ftngN
n=1. The adapted weights are then used to
process subsequent input data. This process can be viewed as a communication
process, in which the sender examines the data DN and creates a message w
that depends on those data. The receiver then uses w; for example, the
receiver might use the weights to try to reconstruct what the data DN was.
[In neural network parlance, this is using the neuron for ‘memory’ rather than
for ‘generalization’; ‘generalizing’ means extrapolating from the observed data
to the value of tN +1 at some new location xN +1.] Just as a disk drive is a
communication channel, the adapted network weights w therefore play the
role of a communication channel, conveying information about the training
data to a future user of that neural net. The question we now address is,
‘what is the capacity of this channel?’ { that is, ‘how much information can
be stored by training a neural network?’

If we had a learning algorithm that either produces a network whose re-
sponse to all inputs is +1 or a network whose response to all inputs is 0,
depending on the training data, then the weights allow us to distinguish be-
tween just two sorts of data set. The maximum information such a learning
algorithm could convey about the data is therefore 1 bit, this information con-
tent being achieved if the two sorts of data set are equiprobable. How much
more information can be conveyed if we make full use of a neural network’s
ability to represent other functions?

40.2 The capacity of a single neuron

We will look at the simplest case, that of a single binary threshold neuron. We
will (cid:12)nd that the capacity of such a neuron is two bits per weight. A neuron
with K inputs can store 2K bits of information.

To obtain this interesting result we lay down some rules to exclude less
interesting answers, such as: ‘the capacity of a neuron is in(cid:12)nite, because each

483

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

484

40 | Capacity of a Single Neuron

of its weights is a real number and so can convey an in(cid:12)nite number of bits’.
We exclude this answer by saying that the receiver is not able to examine the
weights directly, nor is the receiver allowed to probe the weights by observing
the output of the neuron for arbitrarily chosen inputs. We constrain the
receiver to observe the output of the neuron at the same (cid:12)xed set of N points
fxng that were in the training set. What matters now is how many di(cid:11)erent
distinguishable functions our neuron can produce, given that we can observe
the function only at these N points. How many di(cid:11)erent binary labellings of
N points can a linear threshold function produce? And how does this number
compare with the maximum possible number of binary labellings, 2N ? If
nearly all of the 2N labellings can be realized by our neuron, then it is a
communication channel that can convey all N bits (the target values ftng)
with small probability of error. We will identify the capacity of the neuron as
the maximum value that N can have such that the probability of error is very
small. [We are departing a little from the de(cid:12)nition of capacity in Chapter 9.]
We thus examine the following scenario. The sender is given a neuron
with K inputs and a data set DN which is a labelling of N points. The
sender uses an adaptive algorithm to try to (cid:12)nd a w that can reproduce this
labelling exactly. We will assume the algorithm (cid:12)nds such a w if it exists. The
receiver then evaluates the threshold function on the N input values. What
is the probability that all N bits are correctly reproduced? How large can N
become, for a given K, without this probability becoming substantially less
than one?

General position

One technical detail needs to be pinned down: what set of inputs fxng are we
considering? Our answer might depend on this choice. We will assume that
the points are in general position.

De(cid:12)nition 40.1 A set of points fxng in K-dimensional space are in general
position if any subset of size (cid:20) K is linearly independent, and no K + 1 of
them lie in a (K (cid:0) 1)-dimensional plane.
In K = 3 dimensions, for example, a set of points are in general position if no
three points are colinear and no four points are coplanar. The intuitive idea is
that points in general position are like random points in the space, in terms of
the linear dependences between points. You don’t expect three random points
in three dimensions to lie on a straight line.

The linear threshold function

The neuron we will consider performs the function

where

y = f  K
Xk=1
f (a) =(cid:26) 1

wkxk!

a > 0
0 a (cid:20) 0:

(40.1)

(40.2)

We will not have a bias w0; the capacity for a neuron with a bias can be
obtained by replacing K by K + 1 in the (cid:12)nal result below, i.e., considering
one of the inputs to be (cid:12)xed to 1. (These input points would not then be in
general position; the derivation still works.)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

40.3: Counting threshold functions

485

w2

x2

(0)

Figure 40.2. One data point in a
two-dimensional input space, and
the two regions of weight space
that give the two alternative
labellings of that point.

x1

x(1)

(a)

(b)

w1

(1)

40.3 Counting threshold functions

Let us denote by T (N; K) the number of distinct threshold functions on N
points in general position in K dimensions. We will derive a formula for
T (N; K).

To start with, let us work out a few cases by hand.

In K = 1 dimension, for any N

The N points lie on a line. By changing the sign of the one weight w1 we can
label all points on the right side of the origin 1 and the others 0, or vice versa.
Thus there are two distinct threshold functions. T (N; 1) = 2.

With N = 1 point, for any K

If there is just one point x(1) then we can realize both possible labellings by
setting w = (cid:6)x(1). Thus T (1; K) = 2.

In K = 2 dimensions

In two dimensions with N points, we are free to spin the separating line around
the origin. Each time the line passes over a point we obtain a new function.
Once we have spun the line through 360 degrees we reproduce the function
we started from. Because the points are in general position, the separating
plane (line) crosses only one point at a time. In one revolution, every point
is passed over twice. There are therefore 2N distinct threshold functions.
T (N; 2) = 2N .

Comparing with the total number of binary functions, 2N , we may note
that for N (cid:21) 3, not all binary functions can be realized by a linear threshold
function. One famous example of an unrealizable function with N = 4 and
K = 2 is the exclusive-or function on the points x = ((cid:6)1;(cid:6)1). [These points
are not in general position, but you may con(cid:12)rm that the function remains
unrealizable even if the points are perturbed into general position.]

In K = 2 dimensions, from the point of view of weight space

There is another way of visualizing this problem.
Instead of visualizing a
plane separating points in the two-dimensional input space, we can consider
the two-dimensional weight space, colouring regions in weight space di(cid:11)erent
colours if they label the given datapoints di(cid:11)erently. We can then count the
number of threshold functions by counting how many distinguishable regions
there are in weight space. Consider (cid:12)rst the set of weight vectors in weight

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

486

40 | Capacity of a Single Neuron

x2

(0,1)

x1

x(2)

x(1)

(a)

(b)

w2

(0,0)

(1,1)

w2

(0,0,1)

x2

x(3)

x1

x(2)

x(1)

(a)

(0,1,1)

       
       
       

       

       

       

       

       

       

       

       

       

       

       

       

       

       




(0,1,0)

(b)

(1,1,0)

Figure 40.3. Two data points in a
two-dimensional input space, and
the four regions of weight space
that give the four alternative
labellings.

Figure 40.4. Three data points in
a two-dimensional input space,
and the six regions of weight
space that give alternative
labellings of those points. In this
case, the labellings (0; 0; 0) and
(1; 1; 1) cannot be realized. For
any three points in general
position there are always two
labellings that cannot be realized.

(1,0)

w1

(1,0,1)

w1

(1,0,0)

space that classify a particular example x(n) as a 1. For example, (cid:12)gure 40.2a
shows a single point in our two-dimensional x-space, and (cid:12)gure 40.2b shows
the two corresponding sets of points in w-space. One set of weight vectors
occupy the half space

x(n)(cid:1)w > 0;

(40.3)

and the others occupy x(n)(cid:1)w < 0. In (cid:12)gure 40.3a we have added a second
point in the input space. There are now 4 possible labellings: (1; 1), (1; 0),
(0; 1), and (0; 0). Figure 40.3b shows the two hyperplanes x(1)(cid:1)w = 0 and
x(2)(cid:1)w = 0 which separate the sets of weight vectors that produce each of
these labellings. When N = 3 ((cid:12)gure 40.4), weight space is divided by three
hyperplanes into six regions. Not all of the eight conceivable labellings can be
realized. Thus T (3; 2) = 6.

In K = 3 dimensions

We now use this weight space visualization to study the three dimensional
case.

Let us imagine adding one point at a time and count the number of thresh-
old functions as we do so. When N = 2, weight space is divided by two hy-
perplanes x(1)(cid:1)w = 0 and x(2)(cid:1)w = 0 into four regions; in any one region all
vectors w produce the same function on the 2 input vectors. Thus T (2; 3) = 4.
Adding a third point in general position produces a third plane in w space,
so that there are 8 distinguishable regions. T (3; 3) = 8. The three bisecting
planes are shown in (cid:12)gure 40.5a.

At this point matters become slightly more tricky. As (cid:12)gure 40.5b illus-
trates, the fourth plane in the three-dimensional w space cannot transect all
eight of the sets created by the (cid:12)rst three planes. Six of the existing regions
are cut in two and the remaining two are una(cid:11)ected. So T (4; 3) = 14. Two

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

40.3: Counting threshold functions

487

(a)





































    

    

    

    













(b)





























































































K
4

2

5

2

6

2

7

2

8

2

N

1
2
3
4
5
6

1

2
2
2
2
2
2

3

2
4
8
14

2

2
4
6
8
10
12

         
         

         

         

         

         

         

         

         

         

         

         



(b)























































(a)

(c)

!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""
!!!!!!!!!
"""""""""

							








							








							








							








							








							








							









Figure 40.5. Weight space
illustrations for T (3; 3) and
T (4; 3). (a) T (3; 3) = 8. Three
hyperplanes (corresponding to
three points in general position)
divide 3-space into 8 regions,
shown here by colouring the
relevant part of the surface of a
hollow, semi-transparent cube
centred on the origin. (b)
T (4; 3) = 14. Four hyperplanes
divide 3-space into 14 regions, of
which this (cid:12)gure shows 13 (the
14th region is out of view on the
right-hand face. Compare with
(cid:12)gure 40.5a: all of the regions
that are not coloured white have
been cut into two.

Table 40.6. Values of T (N; K)
deduced by hand.

Figure 40.7. Illustration of the
cutting process going from T (3; 3)
to T (4; 3). (a) The eight regions
of (cid:12)gure 40.5a with one added
hyperplane. All of the regions
that are not coloured white have
been cut into two. (b) Here, the
hollow cube has been made solid,
so we can see which regions are
cut by the fourth plane. The front
half of the cube has been cut
away. (c) This (cid:12)gure shows the
new two dimensional hyperplane,
which is divided into six regions
by the three one-dimensional
hyperplanes (lines) which cross it.
Each of these regions corresponds
to one of the three-dimensional
regions in (cid:12)gure 40.7a which is
cut into two by this new
hyperplane. This shows that
T (4; 3) (cid:0) T (3; 3) = 6. Figure 40.7c
should be compared with (cid:12)gure
40.4b.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

488

40 | Capacity of a Single Neuron

of the binary functions on 4 points in 3 dimensions cannot be realized by a
linear threshold function.

We have now (cid:12)lled in the values of T (N; K) shown in table 40.6. Can we
obtain any insights into our derivation of T (4; 3) in order to (cid:12)ll in the rest of
the table for T (N; K)? Why was T (4; 3) greater than T (3; 3) by six?

Six is the number of regions that the new hyperplane bisected in w-space
((cid:12)gure 40.7a b). Equivalently, if we look in the K (cid:0) 1 dimensional subspace
that is the N th hyperplane, that subspace is divided into six regions by the
N(cid:0)1 previous hyperplanes ((cid:12)gure 40.7c). Now this is a concept we have met
before. Compare (cid:12)gure 40.7c with (cid:12)gure 40.4b. How many regions are created
by N (cid:0) 1 hyperplanes in a K(cid:0)1 dimensional space? Why, T (N(cid:0)1; K(cid:0)1), of
course! In the present case N = 4, K = 3, we can look up T (3; 2) = 6 in the
previous section. So

T (4; 3) = T (3; 3) + T (3; 2):

(40.4)

Recurrence relation for any N; K

Generalizing this picture, we see that when we add an N th hyperplane in K
dimensions, it will bisect T (N(cid:0)1; K(cid:0)1) of the T (N(cid:0)1; K) regions that were
created by the previous N (cid:0) 1 hyperplanes. Therefore, the total number of
regions obtained after adding the N th hyperplane is 2T (N (cid:0)1; K(cid:0)1) (since
T (N(cid:0)1; K(cid:0)1) out of T (N(cid:0)1; K) regions are split in two) plus the remaining
T (N(cid:0)1; K) (cid:0) T (N(cid:0)1; K(cid:0)1) regions not split by the N th hyperplane, which
gives the following equation for T (N; K):

T (N; K) = T (N(cid:0)1; K) + T (N(cid:0)1; K(cid:0)1):

(40.5)

Now all that remains is to solve this recurrence relation given the boundary
conditions T (N; 1) = 2 and T (1; K) = 2.

Does the recurrence relation (40.5) look familiar? Maybe you remember
building Pascal’s triangle by adding together two adjacent numbers in one row
to get the number below. The N; K element of Pascal’s triangle is equal to

C(N; K) (cid:17)(cid:18)N

K(cid:19) (cid:17)

N !

(N (cid:0) K)!K!

:

(40.6)

Table 40.8. Pascal’s triangle.

N

0
1
2
3
4
5

K

0 1 2 3 4 5 6 7

1
1 1
1 2 1
1 3 3 1
1 4 6 4 1
1 5 10 10 5 1

K(cid:1) satisfy the equation

Combinations (cid:0)N
C(N; K) = C(N(cid:0)1; K(cid:0)1) + C(N(cid:0)1; K);
[Here we are adopting the convention that (cid:0)N
K(cid:1) (cid:17) 0 if K > N or K < 0.]
K(cid:1) satis(cid:12)es the required recurrence relation (40.5). This doesn’t mean
So (cid:0)N
K(cid:1), since many functions can satisfy one recurrence relation.
T (N; K) = (cid:0)N

for all N > 0:

(40.7)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

40.3: Counting threshold functions

489

1

0.75

0.5

0.25

(a)

N

0
10
20
30
40
50
60
70

1

0.75

0.5

0.25

(c) 0

0

1

0.75

0.5

0.25

0

70
60
50
40
30
20
10

K

(b)

N=K

K=N/2

10

20

30

K

40

50

60

70

N=K

N=2K

50

N

100

150

log T(N,K)
log 2^N

2400

2300

2200

2100

2000

1900

N=K

N=2K

0.5

1

1.5
N/K

2

2.5

3

(d) 1800

1800

1900

2000

2100

2200

2300

2400

Figure 40.9. The fraction of functions on N points in K dimensions that are linear threshold functions,
T (N; K)=2N, shown from various viewpoints. In (a) we see the dependence on K, which
is approximately an error function passing through 0.5 at K = N=2; the fraction reaches 1
at K = N . In (b) we see the dependence on N , which is 1 up to N = K and drops sharply
at N = 2K. Panel (c) shows the dependence on N=K for K = 1000. There is a sudden
drop in the fraction of realizable labellings when N = 2K. Panel (d) shows the values of
log2 T (N; K) and log2 2N as a function of N for K = 1000. These (cid:12)gures were plotted
using the approximation of T =2N by the error function.

But perhaps we can express T (N; K) as a linear superposition of combination

functions of the form C(cid:11);(cid:12)(N; K) (cid:17) (cid:0)N +(cid:11)

40.6 we can see how to satisfy the boundary conditions: we simply need to
translate Pascal’s triangle to the right by 1, 2, 3, : : :; superpose; add; multiply
by two, and drop the whole table by one line. Thus:

K+(cid:12)(cid:1). By comparing tables 40.8 and

T (N; K) = 2

K(cid:0)1

k (cid:19):
Xk=0(cid:18)N(cid:0)1

(40.8)

Using the fact that the N th row of Pascal’s triangle sums to 2N , that is,

k=0(cid:0)N(cid:0)1
PN(cid:0)1

k (cid:1) = 2N(cid:0)1, we can simplify the cases where K(cid:0)1 (cid:21) N(cid:0)1.

2N

T (N; K) =(cid:26)

2PK(cid:0)1

k=0 (cid:0)N(cid:0)1

K (cid:21) N
k (cid:1) K < N:

(40.9)

Interpretation

It is natural to compare T (N; K) with the total number of binary functions on
N points, 2N . The ratio T (N; K)=2N tells us the probability that an arbitrary
labelling ftngN
n=1 can be memorized by our neuron. The two functions are
equal for all N (cid:20) K. The line N = K is thus a special line, de(cid:12)ning the
maximum number of points on which any arbitrary labelling can be realized.
This number of points is referred to as the Vapnik{Chervonenkis dimension
(VC dimension) of the class of functions. The VC dimension of a binary
threshold function on K dimensions is thus K.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

490

40 | Capacity of a Single Neuron

What is interesting is (for large K) the number of points N such that
almost any labelling can be realized. The ratio T (N; K)=2N is, for N < 2K,
still greater than 1/2, and for large K the ratio is very close to 1.

For our purposes the sum in equation (40.9) is well approximated by the

error function,

K

pN =2 (cid:19) ;
k(cid:19) ’ 2N (cid:8)(cid:18) K (cid:0) (N=2)

X0 (cid:18)N
exp((cid:0)z2=2)=p2(cid:25). Figure 40.9 shows the realizable fraction

(40.10)

where (cid:8)(z) (cid:17)R z

(cid:0)1

T (N; K)=2N as a function of N and K. The take-home message is shown in
(cid:12)gure 40.9c: although the fraction T (N; K)=2N is less than 1 for N > K, it is
only negligibly less than 1 up to N = 2K; there, there is a catastrophic drop
to zero, so that for N > 2K, only a tiny fraction of the binary labellings can
be realized by the threshold function.

Conclusion

The capacity of a linear threshold neuron, for large K, is 2 bits per weight.

A single neuron can almost certainly memorize up to N = 2K random

binary labels perfectly, but will almost certainly fail to memorize more.

40.4 Further exercises

. Exercise 40.4.[2 ] Can a (cid:12)nite set of 2N distinct points in a two-dimensional

space be split in half by a straight line

(cid:15) if the points are in general position?
(cid:15) if the points are not in general position?

Can 2N points in a K dimensional space be split in half by a K (cid:0) 1
dimensional hyperplane?

Exercise 40.5.[2, p.491] Four points are selected at random on the surface of a
sphere. What is the probability that all of them lie on a single hemi-
sphere? How does this question relate to T (N; K)?

Exercise 40.6.[2 ] Consider the binary threshold neuron in K = 3 dimensions,
and the set of points fxg = f(1; 0; 0); (0; 1; 0); (0; 0; 1); (1; 1; 1)g. Find
a parameter vector w such that the neuron memorizes the labels: (a)
ftg = f1; 1; 1; 1g; (b) ftg = f1; 1; 0; 0g.
Find an unrealizable labelling ftg.

. Exercise 40.7.[3 ] In this chapter we constrained all our hyperplanes to go

through the origin. In this exercise, we remove this constraint.

How many regions in a plane are created by N lines in general position?

Exercise 40.8.[2 ] Estimate in bits the total sensory experience that you have
had in your life { visual information, auditory information, etc. Estimate
how much information you have memorized. Estimate the information
content of the works of Shakespeare. Compare these with the capacity of
your brain assuming you have 1011 neurons each making 1000 synaptic
connections, and that the capacity result for one neuron (two bits per
connection) applies. Is your brain full yet?

Figure 40.10. Three lines in a
plane create seven regions.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

40.5: Solutions

491

. Exercise 40.9.[3 ] What is the capacity of the axon of a spiking neuron, viewed
in bits per second? [See MacKay and
as a communication channel,
McCulloch (1952) for an early publication on this topic.] Multiply by
the number of axons in the optic nerve (about 106) or cochlear nerve
(about 50 000 per ear) to estimate again the rate of acquisition sensory
experience.

40.5 Solutions

Solution to exercise 40.5 (p.490). The probability that all four points lie on a
single hemisphere is

T (4; 3)=24 = 14=16 = 7=8:

(40.11)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41

Learning as Inference

41.1 Neural network learning as inference

In Chapter 39 we trained a simple neural network as a classi(cid:12)er by minimizing
an objective function

M (w) = G(w) + (cid:11)EW (w)

made up of an error function

G(w) = (cid:0)Xn ht(n) ln y(x(n); w) + (1 (cid:0) t(n)) ln(1 (cid:0) y(x(n); w))i

and a regularizer

EW (w) =

(41.1)

(41.2)

(41.3)

w2
i :

1

2Xi

This neural network learning process can be given the following probabilistic
interpretation.

We interpret the output y(x; w) of the neuron literally as de(cid:12)ning (when
its parameters w are speci(cid:12)ed) the probability that an input x belongs to class
t = 1, rather than the alternative t = 0. Thus y(x; w) (cid:17) P (t = 1j x; w). Then
each value of w de(cid:12)nes a di(cid:11)erent hypothesis about the probability of class 1
relative to class 0 as a function of x.

We de(cid:12)ne the observed data D to be the targets ftg { the inputs fxg are
assumed to be given, and not to be modelled. To infer w given the data, we
require a likelihood function and a prior probability over w. The likelihood
function measures how well the parameters w predict the observed data; it is
the probability assigned to the observed t values by the model with parameters
set to w. Now the two equations

P (t = 1j w; x) = y
P (t = 0j w; x) = 1 (cid:0) y

can be rewritten as the single equation

P (tj w; x) = yt(1 (cid:0) y)1(cid:0)t = exp[t ln y + (1 (cid:0) t) ln(1 (cid:0) y)] :

So the error function G can be interpreted as minus the log likelihood:

P (D j w) = exp[(cid:0)G(w)]:

(41.4)

(41.5)

(41.6)

Similarly the regularizer can be interpreted in terms of a log prior proba-

bility distribution over the parameters:

P (w j (cid:11)) =

1

ZW ((cid:11))

exp((cid:0)(cid:11)EW ):

(41.7)

492

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41.2: Illustration for a neuron with two weights

493

If EW is quadratic as de(cid:12)ned above, then the corresponding prior distribution
W = 1=(cid:11), and 1=ZW ((cid:11)) is equal to ((cid:11)=2(cid:25))K=2,
is a Gaussian with variance (cid:27)2
where K is the number of parameters in the vector w.

The objective function M (w) then corresponds to the inference of the

parameters w, given the data:

P (w j D; (cid:11)) =

=

=

P (D j w)P (w j (cid:11))

P (D j (cid:11))

e(cid:0)G(w) e(cid:0)(cid:11)EW (w)=ZW ((cid:11))

P (D j (cid:11))
exp((cid:0)M (w)):

1
ZM

(41.8)

(41.9)

(41.10)

So the w found by (locally) minimizing M (w) can be interpreted as the (locally)
most probable parameter vector, w(cid:3). From now on we will refer to w(cid:3) as wMP.
Why is it natural to interpret the error functions as log probabilities? Error
functions are usually additive. For example, G is a sum of information con-
tents, and EW is a sum of squared weights. Probabilities, on the other hand,
are multiplicative: for independent events X and Y , the joint probability is
P (x; y) = P (x)P (y). The logarithmic mapping maintains this correspondence.
The interpretation of M (w) as a log probability has numerous bene(cid:12)ts,

some of which we will discuss in a moment.

41.2 Illustration for a neuron with two weights

In the case of a neuron with just two inputs and no bias,

y(x; w) =

1

1 + e(cid:0)(w1x1+w2x2) ;

(41.11)

we can plot the posterior probability of w, P (w j D; (cid:11)) / exp((cid:0)M (w)). Imag-
ine that we receive some data as shown in the left column of (cid:12)gure 41.1. Each
data point consists of a two-dimensional input vector x and a t value indicated
by (cid:2) (t = 1) or 2 (t = 0). The likelihood function exp((cid:0)G(w)) is shown as a
function of w in the second column. It is a product of functions of the form
(41.11).

The product of traditional learning is a point in w-space, the estimator w(cid:3),
which maximizes the posterior probability density. In contrast, in the Bayesian
view, the product of learning is an ensemble of plausible parameter values
(bottom right of (cid:12)gure 41.1). We do not choose one particular hypothesis w;
rather we evaluate their posterior probabilities. The posterior distribution is
obtained by multiplying the likelihood by a prior distribution over w space
(shown as a broad Gaussian at the upper right of (cid:12)gure 41.1). The posterior
ensemble (within a multiplicative constant) is shown in the third column of
(cid:12)gure 41.1, and as a contour plot in the fourth column. As the amount of data
increases (from top to bottom), the posterior ensemble becomes increasingly
concentrated around the most probable value w(cid:3).

41.3 Beyond optimization: making predictions

Let us consider the task of making predictions with the neuron which we
trained as a classi(cid:12)er in section 39.3. This was a neuron with two inputs and
a bias.

y(x; w) =

:

(41.12)

1

1 + e(cid:0)(w0+w1x1+w2x2)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Data set

Likelihood

Probability of parameters

41 | Learning as Inference

494

10

2
x

5

0

-5

N = 0

(constant)

N = 2

-5

0

w1

5

5

0

w2

-5

-5

0

w1

5

-5

0

w1

5

5

0

w2

-5

-5

0

w1

5

-5

0

w1

5

0.5

0.1

0.05

0.05

5

0

w2

-5

-10

-10

-5

0
x1

5

10

N = 4

10

2
x

5

0

-5

-10

-10

-5

0
x1

5

10

N = 6

10

2
x

5

0

-5

-5

0
w1

-5

0
w1

-5

0
w1

5

0

w2

-5

5

0

w2

-5

5

0

w2

-5

5

0

w2

-5

5

0

-5

5

0

-5

5

0

-5

5

0

-5

w2

w2

w2

w2

5

5

5

5

-10

-10

-5

0
x1

5

10

-5

0

w1

5

-5

0

w1

5

-5

0
w1

Figure 41.1. The Bayesian interpretation and generalization of traditional neural network learning.

Evolution of the probability distribution over parameters as data arrive.

A

B

(a)

(b)

0

2w

2

Samples from 

P(w|D,H)

10

5

0

10

(c)

wMP

1

w
1

A

B

5

Figure 41.2. Making predictions. (a) The function performed by an optimized neuron wMP (shown by
three of its contours) trained with weight decay, (cid:11) = 0:01 (from (cid:12)gure 39.6). The contours
shown are those corresponding to a = 0;(cid:6)1, namely y = 0:5; 0:27 and 0:73. (b) Are these
predictions more reasonable? (Contours shown are for y = 0:5; 0:27, 0:73, 0:12 and 0:88.)
(c) The posterior probability of w (schematic); the Bayesian predictions shown in (b) were
obtained by averaging together the predictions made by each possible value of the weights
w, with each value of w receiving a vote proportional to its probability under the posterior
ensemble. The method used to create (b) is described in section 41.4.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41.3: Beyond optimization: making predictions

495

When we last played with it, we trained it by minimizing the objective function

M (w) = G(w) + (cid:11)E(w):

(41.13)

The resulting optimized function for the case (cid:11) = 0:01 is reproduced in (cid:12)g-
ure 41.2a.

We now consider the task of predicting the class t(N+1) corresponding to a
new input x(N+1). It is common practice, when making predictions, simply to
use a neural network with its weights (cid:12)xed to their optimized value wMP, but
this is not optimal, as can be seen intuitively by considering the predictions
shown in (cid:12)gure 41.2a. Are these reasonable predictions? Consider new data
arriving at points A and B. The best-(cid:12)t model assigns both of these examples
probability 0.2 of being in class 1, because they have the same value of w MP(cid:1)x.
If we really knew that w was equal to wMP, then these predictions would be
correct. But we do not know w. The parameters are uncertain. Intuitively we
might be inclined to assign a less con(cid:12)dent probability (closer to 0.5) at B than
at A, as shown in (cid:12)gure 41.2b, since point B is far from the training data. The
best-(cid:12)t parameters wMP often give over-con(cid:12)dent predictions. A non-Bayesian
approach to this problem is to downweight all predictions uniformly, by an
empirically determined factor (Copas, 1983). This is not ideal, since intuition
suggests the strength of the predictions at B should be downweighted more
than those at A. A Bayesian viewpoint helps us to understand the cause of
the problem, and provides a straightforward solution. In a nutshell, we obtain
Bayesian predictions by taking into account the whole posterior ensemble,
shown schematically in (cid:12)gure 41.2c.

The Bayesian prediction of a new datum t(N+1) involves marginalizing over
the parameters (and over anything else about which we are uncertain). For
simplicity, let us assume that the weights w are the only uncertain quantities
{ the weight decay rate (cid:11) and the model H itself are assumed to be (cid:12)xed.
Then by the sum rule, the predictive probability of a new target t(N+1) at a
location x(N+1) is:

P (t(N+1) j x(N+1); D; (cid:11)) =Z dKw P (t(N+1) j x(N+1); w; (cid:11))P (w j D; (cid:11));

(41.14)

where K is the dimensionality of w, three in the toy problem. Thus the
predictions are obtained by weighting the prediction for each possible w,

P (t(N+1) = 1j x(N+1); w; (cid:11)) = y(x(N+1); w)
P (t(N+1) = 0j x(N+1); w; (cid:11)) = 1 (cid:0) y(x(N+1); w);

(41.15)

with a weight given by the posterior probability of w, P (w j D; (cid:11)), which we
most recently wrote down in equation (41.10). This posterior probability is

1
ZM

P (w j D; (cid:11)) =
exp((cid:0)M (w));
ZM =Z dKw exp((cid:0)M (w)):

(41.16)

(41.17)

where

In summary, we can get the Bayesian predictions if we can (cid:12)nd a way of
computing the integral

P (t(N+1) = 1j x(N+1); D; (cid:11)) =Z dKw y(x(N+1); w)

1
ZM

exp((cid:0)M (w));

(41.18)

which is the average of the output of the neuron at x(N+1) under the posterior
distribution of w.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

496

41 | Learning as Inference

Figure 41.3. One step of the
Langevin method in two
dimensions (c), contrasted with a
traditional ‘dumb’ Metropolis
method (a) and with gradient
descent (b). The proposal density
of the Langevin method is given
by ‘gradient descent with noise’.

P (cid:3)(x)

(cid:15)

x(1)

Q(x; x(1))

(cid:0)(cid:17)g

(a)

(b)

(c)

Dumb Metropolis

Gradient descent

Langevin

Implementation

How shall we compute the integral (41.18)? For our toy problem, the weight
space is three dimensional; for a realistic neural network the dimensionality
K might be in the thousands.

Bayesian inference for general data modelling problems may be imple-
mented by exact methods (Chapter 25), by Monte Carlo sampling (Chapter
29), or by deterministic approximate methods, for example, methods that
make Gaussian approximations to P (w j D; (cid:11)) using Laplace’s method (Chap-
ter 27) or variational methods (Chapter 33). For neural networks there are few
exact methods. The two main approaches to implementing Bayesian inference
for neural networks are the Monte Carlo methods developed by Neal (1996)
and the Gaussian approximation methods developed by MacKay (1991).

41.4 Monte Carlo implementation of a single neuron

First we will use a Monte Carlo approach in which the task of evaluating the
integral (41.18) is solved by treating y(x(N+1); w) as a function f of w whose
mean we compute using

hf (w)i ’

f (w(r))

1

RXr

(41.19)

ZM

where fw(r)g are samples from the posterior distribution 1
exp((cid:0)M (w)) (cf.
equation (29.6)). We obtain the samples using a Metropolis method (section
29.4). As an aside, a possible disadvantage of this Monte Carlo approach is
that it is a poor way of estimating the probability of an improbable event, i.e.,
a P (tj D;H) that is very close to zero, if the improbable event is most likely
to occur in conjunction with improbable parameter values.
How to generate the samples fw(r)g? Radford Neal introduced the Hamil-
tonian Monte Carlo method to neural networks. We met this sophisticated
Metropolis method, which makes use of gradient information, in Chapter 30.
The method we now demonstrate is a simple version of Hamiltonian Monte
Carlo called the Langevin Monte Carlo method.

The Langevin Monte Carlo method

The Langevin method (algorithm 41.4) may be summarized as ‘gradient de-
scent with added noise’, as shown pictorially in (cid:12)gure 41.3. A noise vector p
is generated from a Gaussian with unit variance. The gradient g is computed,

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41.4: Monte Carlo implementation of a single neuron

497

g = gradM ( w ) ;
M = findM ( w ) ;

# set gradient using initial w
# set objective function too

for l = 1:L

p = randn ( size(w) ) ;
H = p’ * p / 2 + M ;

# loop L times
# initial momentum is Normal(0,1)
# evaluate H(w,p)

*
*
*
*

p = p - epsilon * g / 2 ;
wnew = w + epsilon * p ;
gnew = gradM ( wnew ) ;
p = p - epsilon * gnew / 2 ;

# make half-step in p
# make step in w
# find new gradient
# make half-step in p

Algorithm 41.4. Octave source
code for the Langevin Monte
Carlo method. To obtain the
Hamiltonian Monte Carlo method,
we repeat the four lines marked *
multiple times (algorithm 41.8).

Mnew = findM ( wnew ) ;
Hnew = p’ * p / 2 + Mnew ;
dH = Hnew - H ;
if ( dH < 0 )
accept = 1 ;
elseif ( rand() < exp(-dH) ) accept = 1 ;
else
accept = 0 ;
endif
if ( accept )

g = gnew ;

w = wnew ;

# find new objective function
# evaluate new value of H
# decide whether to accept

# compare with a uniform
#
variate

M = Mnew ;

endif

endfor

function gM = gradM ( w )

;

a = x * w
y = sigmoid(a) ;
e = t - y
g = - x’ * e ;
gM = alpha * w + g ;

;

# gradient of objective function
# compute activations
# compute outputs
# compute errors
# compute the gradient of G(w)

endfunction

function M = findM ( w )

# objective function

G = - (t’ * log(y) + (1-t’) * log( 1-y )) ;
EW = w’ * w / 2 ;
M = G + alpha * EW ;

endfunction

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

498

10

5

0

-5

-10

-15

-20

-25

-30

0
(a)

10000

20000

30000

40000

14

12

10

8

6

4

2

0

0
(c)

G(w) - Langevin
G(w) - optimizer

10000

20000

30000

40000

41 | Learning as Inference

12

10

8

6

4

2

0

0
(d)

M(w) - Langevin
M(w) - optimizer

10000

20000

30000

40000

5
4
3
2
1
0
-1
-2
-3

-1 0 1 2 3 4 5 6 7

(b)

Figure 41.5. A single neuron learning under the Langevin Monte Carlo method. (a) Evolution of
weights w0, w1 and w2 as a function of number of iterations. (b) Evolution of weights w1
and w2 in weight space. Also shown by a line is the evolution of the weights using the
optimizer of (cid:12)gure 39.6. (c) The error function G(w) as a function of number of iterations.
Also shown is the error function during the optimization of (cid:12)gure 39.6. (d) The objective
function M (x) as a function of number of iterations. See also (cid:12)gures 41.6 and 41.7.

and a step in w is made, given by

(cid:1)w = (cid:0) 1

2 (cid:15)2g + (cid:15)p:

(41.20)

Notice that if the (cid:15)p term were omitted this would simply be gradient descent
with learning rate (cid:17) = 1
2 (cid:15)2. This step in w is accepted or rejected depending
on the change in the value of the objective function M (w) and on the change
in gradient, with a probability of acceptance such that detailed balance holds.
The Langevin method has one free parameter, (cid:15), which controls the typical
step size. If (cid:15) is set to too large a value, moves may be rejected. If it is set to
a very small value, progress around the state space will be slow.

Demonstration of Langevin method

The Langevin method is demonstrated in (cid:12)gures 41.5, 41.6 and 41.7. Here, the
objective function is M (w) = G(w) + (cid:11)EW (w), with (cid:11) = 0:01. These (cid:12)gures
include, for comparison, the results of the previous optimization method using
gradient descent on the same objective function ((cid:12)gure 39.6). It can be seen
that the mean evolution of w is similar to the evolution of the parameters
under gradient descent. The Monte Carlo method appears to have converged
to the posterior distribution after about 10 000 iterations.

The average acceptance rate during this simulation was 93%; only 7% of
the proposed moves were rejected. Probably, faster progress around the state
space would have been made if a larger step size (cid:15) had been used, but the
value was chosen so that the ‘descent rate’ (cid:17) = 1
2 (cid:15)2 matched the step size of
the earlier simulations.

Making Bayesian predictions

From iteration 10,000 to 40,000, the weights were sampled every 1000 itera-
tions and the corresponding functions of x are plotted in (cid:12)gure 41.6. There
is a considerable variety of plausible functions. We obtain a Monte Carlo ap-
proximation to the Bayesian predictions by averaging these thirty functions of
x together. The result is shown in (cid:12)gure 41.7 and contrasted with the predic-
tions given by the optimized parameters. The Bayesian predictions become
satisfyingly moderate as we move away from the region of highest data density.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41.4: Monte Carlo implementation of a single neuron

499

Figure 41.6. Samples obtained by
the Langevin Monte Carlo
method. The learning rate was set
to (cid:17) = 0:01 and the weight decay
rate to (cid:11) = 0:01. The step size is
given by (cid:15) = p2(cid:17). The function
performed by the neuron is shown
by three of its contours every 1000
iterations from iteration 10 000 to
40 000. The contours shown are
those corresponding to a = 0;(cid:6)1,
namely y = 0:5; 0:27 and 0:73.
Also shown is a vector
proportional to (w1; w2).

(a)

10

5

0

(b)

10
10

5
5

0
0

0

5

10

0
0

5
5

10
10

Figure 41.7. Bayesian predictions found by the Langevin Monte Carlo method compared with the
predictions using the optimized parameters. (a) The predictive function obtained by av-
eraging the predictions for 30 samples uniformly spaced between iterations 10 000 and
40 000, shown in (cid:12)gure 41.6. The contours shown are those corresponding to a = 0;(cid:6)1;(cid:6)2,
namely y = 0:5; 0:27, 0:73, 0:12 and 0:88. (b) For contrast, the predictions given by the
‘most probable’ setting of the neuron’s parameters, as given by optimization of M (w).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41 | Learning as Inference

Algorithm 41.8. Octave source
code for the Hamiltonian Monte
Carlo method. The algorithm is
identical to the Langevin method
in algorithm 41.4, except for the
replacement of the four lines
marked * in that algorithm by the
fragment shown here.

Figure 41.9. Comparison of
sampling properties of the
Langevin Monte Carlo method
and the Hamiltonian Monte Carlo
(HMC) method. The horizontal
axis is the number of gradient
evaluations made. Each (cid:12)gure
shows the weights during the (cid:12)rst
10,000 iterations. The rejection
rate during this Hamiltonian
Monte Carlo simulation was 8%.

500

wnew = w ;
gnew = g ;
for tau = 1:Tau

p = p - epsilon * gnew / 2 ;
wnew = wnew + epsilon * p ;

# make half-step in p
# make step in w

gnew = gradM ( wnew ) ;
p = p - epsilon * gnew / 2 ;

# find new gradient
# make half-step in p

endfor

10
5
0
-5
-10
-15
-20
-25
Langevin -30

5
0
-5
-10
-15
-20
-25
-30
-35
-40

HMC

0

0

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

The Bayesian classi(cid:12)er is better able to identify the points where the classi-
(cid:12)cation is uncertain. This pleasing behaviour results simply from a mechanical
application of the rules of probability.

Optimization and typicality

A (cid:12)nal observation concerns the behaviour of the functions G(w) and M (w)
during the Monte Carlo sampling process, compared with the values of G and
M at the optimum wMP ((cid:12)gure 41.5). The function G(w) (cid:13)uctuates around
the value of G(wMP), though not in a symmetrical way. The function M (w)
also (cid:13)uctuates, but it does not (cid:13)uctuate around M (wMP) { obviously it cannot,
because M is minimized at wMP, so M could not go any smaller { furthermore,
M only rarely drops close to M (wMP). In the language of information theory,
the typical set of w has di(cid:11)erent properties from the most probable state w MP.
A general message therefore emerges { applicable to all data models, not
just neural networks: one should be cautious about making use of optimized
parameters, as the properties of optimized parameters may be unrepresen-
tative of the properties of typical, plausible parameters; and the predictions
obtained using optimized parameters alone will often be unreasonably over-
con(cid:12)dent.

Reducing random walk behaviour using Hamiltonian Monte Carlo

As a (cid:12)nal study of Monte Carlo methods, we now compare the Langevin Monte
Carlo method with its big brother, the Hamiltonian Monte Carlo method. The
change to Hamiltonian Monte Carlo is simple to implement, as shown in algo-
rithm 41.8. Each single proposal makes use of multiple gradient evaluations

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41.5: Implementing inference with Gaussian approximations

501

along a dynamical trajectory in w; p space, where p are the extra ‘momentum’
variables of the Langevin and Hamiltonian Monte Carlo methods. The num-
ber of steps ‘Tau’ was set at random to a number between 100 and 200 for each
trajectory. The step size (cid:15) was kept (cid:12)xed so as to retain comparability with
the simulations that have gone before; it is recommended that one randomize
the step size in practical applications, however.

Figure 41.9 compares the sampling properties of the Langevin and Hamil-
tonian Monte Carlo methods. The autocorrelation of the state of the Hamil-
tonian Monte Carlo simulation falls much more rapidly with simulation time
than that of the Langevin method. For this toy problem, Hamiltonian Monte
Carlo is at least ten times more e(cid:14)cient in its use of computer time.

41.5 Implementing inference with Gaussian approximations

Physicists love to take nonlinearities and locally linearize them, and they love
to approximate probability distributions by Gaussians. Such approximations
o(cid:11)er an alternative strategy for dealing with the integral

P (t(N+1) = 1j x(N+1); D; (cid:11)) =Z dKw y(x(N+1); w)

1
ZM

which we just evaluated using Monte Carlo methods.

exp((cid:0)M (w));

(41.21)

We start by making a Gaussian approximation to the posterior probability.
We go to the minimum of M (w) (using a gradient-based optimizer) and Taylor-
expand M there:

M (w) ’ M (wMP) +

1
2

(w (cid:0) wMP)TA(w (cid:0) wMP) + (cid:1)(cid:1)(cid:1) ;

(41.22)

where A is the matrix of second derivatives, also known as the Hessian, de(cid:12)ned
by

Aij (cid:17)

@wi@wj

@2

:

(41.23)

M (w)(cid:12)(cid:12)(cid:12)(cid:12)w=wMP
(w (cid:0) wMP)TA(w (cid:0) wMP)(cid:21) : (41.24)

1
2

We thus de(cid:12)ne our Gaussian approximation:

Q(w; wMP; A) = [det(A=2(cid:25))]1=2 exp(cid:20)(cid:0)

We can think of the matrix A as de(cid:12)ning error bars on w. To be precise, Q
is a normal distribution whose variance{covariance matrix is A(cid:0)1.
Exercise 41.1.[2 ] Show that the second derivative of M (w) with respect to w

is given by

@2

@wi@wj

M (w) =

N

Xn=1

f0(a(n))x(n)

i x(n)

j + (cid:11)(cid:14)ij;

(41.25)

where f0(a) is the (cid:12)rst derivative of f (a) (cid:17) 1=(1 + e(cid:0)a), which is

f0(a) =

and

d
da

f (a) = f (a)(1 (cid:0) f (a));
a(n) =Xj

wjx(n)

:

j

(41.26)

(41.27)

Having computed the Hessian, our task is then to perform the integral (41.21)
using our Gaussian approximation.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41 | Learning as Inference

Figure 41.10. The marginalized
probability, and an approximation
to it. (a) The function  (a; s2),
evaluated numerically. In (b) the
functions  (a; s2) and (cid:30)(a; s2)
de(cid:12)ned in the text are shown as a
function of a for s2 = 4. From
MacKay (1992b).

Figure 41.11. The Gaussian
approximation in weight space
and its approximate predictions in
input space. (a) A projection of
the Gaussian approximation onto
the (w1; w2) plane of weight
space. The one- and
two-standard-deviation contours
are shown. Also shown are the
trajectory of the optimizer, and
the Monte Carlo method’s
samples. (b) The predictive
function obtained from the
Gaussian approximation and
equation (41.30). (cf. (cid:12)gure 41.2.)

502

 (a; s2)








(a)

 
 

5
4
3
2
1
0
-1
-2
-3

(a)

(b)














 



A

(b)

0
0

B

5
5

10
10

0 1 2 3 4 5 6

10
10

5
5

0
0

Calculating the marginalized probability

The output y(x; w) depends on w only through the scalar a(x; w), so we can
reduce the dimensionality of the integral by (cid:12)nding the probability density of
a. We are assuming a locally Gaussian posterior probability distribution over
w = wMP + (cid:1)w, P (w j D; (cid:11)) ’ (1=ZQ) exp((cid:0) 1
2 (cid:1)wTA(cid:1)w). For our single
neuron, the activation a(x; w) is a linear function of w with @a=@w = x, so
for any x, the activation a is Gaussian-distributed.

. Exercise 41.2.[2 ] Assuming w is Gaussian-distributed with mean wMP and
variance{covariance matrix A(cid:0)1, show that the probability distribution
of a(x) is

P (aj x; D; (cid:11)) = Normal(aMP; s2) =

1

p2(cid:25)s2

exp(cid:18)(cid:0)

(a (cid:0) aMP)2

2s2

(cid:19) ;

(41.28)

where aMP = a(x; wMP) and s2 = xTA(cid:0)1x.

This means that the marginalized output is:

P (t = 1j x; D; (cid:11)) =  (aMP; s2) (cid:17)Z da f (a) Normal(aMP; s2):

(41.29)

This is to be contrasted with y(x; wMP) = f (aMP), the output of the most prob-
able network. The integral of a sigmoid times a Gaussian can be approximated
by:

 (aMP; s2) ’ (cid:30)(aMP; s2) (cid:17) f ((cid:20)(s)aMP)

with (cid:20) = 1=p1 + (cid:25)s2=8 ((cid:12)gure 41.10).

Demonstration

(41.30)

Figure 41.11 shows the result of (cid:12)tting a Gaussian approximation at the op-
timum wMP, and the results of using that Gaussian approximation and equa-

 





	















	









 




	



Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

41.5: Implementing inference with Gaussian approximations

503

tion (41.30) to make predictions. Comparing these predictions with those of
the Langevin Monte Carlo method ((cid:12)gure 41.7) we observe that, whilst quali-
tatively the same, the two are clearly numerically di(cid:11)erent. So at least one of
the two methods is not completely accurate.

. Exercise 41.3.[2 ] Is the Gaussian approximation to P (w j D; (cid:11)) too heavy-tailed
or too light-tailed, or both? It may help to consider P (w j D; (cid:11)) as a
function of one parameter wi and to think of the two distributions on
a logarithmic scale. Discuss the conditions under which the Gaussian
approximation is most accurate.

Why marginalize?

If the output is immediately used to make a (0/1) decision and the costs asso-
ciated with error are symmetrical, then the use of marginalized outputs under
this Gaussian approximation will make no di(cid:11)erence to the performance of the
classi(cid:12)er, compared with using the outputs given by the most probable param-
eters, since both functions pass through 0:5 at aMP = 0. But these Bayesian
outputs will make a di(cid:11)erence if, for example, there is an option of saying ‘I
don’t know’, in addition to saying ‘I guess 0’ and ‘I guess 1’. And even if
there are just the two choices ‘0’ and ‘1’, if the costs associated with error are
unequal, then the decision boundary will be some contour other than the 0.5
contour, and the boundary will be a(cid:11)ected by marginalization.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Postscript on Supervised Neural

Networks

One of my students, Robert, asked:

Maybe I’m missing something fundamental, but supervised neural
networks seem equivalent to (cid:12)tting a pre-de(cid:12)ned function to some
given data, then extrapolating { what’s the di(cid:11)erence?

I agree with Robert. The supervised neural networks we have studied so far
are simply parameterized nonlinear functions which can be (cid:12)tted to data.
Hopefully you will agree with another comment that Robert made:

Unsupervised networks seem much more interesting than their su-
pervised counterparts. I’m amazed that it works!

504

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42

Hop(cid:12)eld Networks

We have now spent three chapters studying the single neuron. The time has
come to connect multiple neurons together, making the output of one neuron
be the input to another, so as to make neural networks.

Neural networks can be divided into two classes on the basis of their con-

nectivity.

Figure 42.1. (a) A feedforward
network. (b) A feedback network.

(a)

(b)

Feedforward networks. In a feedforward network, all the connections are

directed such that the network forms a directed acyclic graph.

Feedback networks. Any network that is not a feedforward network will be

called a feedback network.

In this chapter we will discuss a fully connected feedback network called
the Hop(cid:12)eld network. The weights in the Hop(cid:12)eld network are constrained to
be symmetric, i.e., the weight from neuron i to neuron j is equal to the weight
from neuron j to neuron i.

Hop(cid:12)eld networks have two applications. First, they can act as associative
memories. Second, they can be used to solve optimization problems. We will
(cid:12)rst discuss the idea of associative memory, also known as content-addressable
memory.

42.1 Hebbian learning

In Chapter 38, we discussed the contrast between traditional digital memories
and biological memories. Perhaps the most striking di(cid:11)erence is the associative
nature of biological memory.

A simple model due to Donald Hebb (1949) captures the idea of associa-
tive memory. Imagine that the weights between neurons whose activities are
positively correlated are increased:

dwij
dt (cid:24) Correlation(xi; xj):

(42.1)

Now imagine that when stimulus m is present (for example, the smell of a
banana), the activity of neuron m increases; and that neuron n is associated

505

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

506

42 | Hop(cid:12)eld Networks

with another stimulus, n (for example, the sight of a yellow object). If these
two stimuli { a yellow sight and a banana smell { co-occur in the environment,
then the Hebbian learning rule (42.1) will increase the weights wnm and wmn.
This means that when, on a later occasion, stimulus n occurs in isolation, mak-
ing the activity xn large, the positive weight from n to m will cause neuron m
also to be activated. Thus the response to the sight of a yellow object is an
automatic association with the smell of a banana. We could call this ‘pattern
completion’. No teacher is required for this associative memory to work. No
signal is needed to indicate that a correlation has been detected or that an as-
sociation should be made. The unsupervised, local learning algorithm and the
unsupervised, local activity rule spontaneously produce associative memory.

This idea seems so simple and so e(cid:11)ective that it must be relevant to how

memories work in the brain.

42.2 De(cid:12)nition of the binary Hop(cid:12)eld network

Convention for weights. Our convention in general will be that wij denotes

the connection from neuron j to neuron i.

Architecture. A Hop(cid:12)eld network consists of I neurons. They are fully
connected through symmetric, bidirectional connections with weights
wij = wji. There are no self-connections, so wii = 0 for all i. Biases
wi0 may be included (these may be viewed as weights from a neuron ‘0’
whose activity is permanently x0 = 1). We will denote the activity of
neuron i (its output) by xi.

Activity rule. Roughly, a Hop(cid:12)eld network’s activity rule is for each neu-
ron to update its state as if it were a single neuron with the threshold
activation function

x(a) = (cid:2)(a) (cid:17)(cid:26) 1

a (cid:21) 0
(cid:0)1 a < 0:

(42.2)

Since there is feedback in a Hop(cid:12)eld network (every neuron’s output is
an input to all the other neurons) we will have to specify an order for the
updates to occur. The updates may be synchronous or asynchronous.

Synchronous updates { all neurons compute their activations

ai =Xj

wijxj

(42.3)

then update their states simultaneously to

xi = (cid:2)(ai):

(42.4)

Asynchronous updates { one neuron at a time computes its activa-
tion and updates its state. The sequence of selected neurons may
be a (cid:12)xed sequence or a random sequence.

The properties of a Hop(cid:12)eld network may be sensitive to the above
choices.

Learning rule. The learning rule is intended to make a set of desired memo-
ries fx(n)g be stable states of the Hop(cid:12)eld network’s activity rule. Each
memory is a binary pattern, with xi 2 f(cid:0)1; 1g.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.3: De(cid:12)nition of the continuous Hop(cid:12)eld network

507

(a)

moscow------russia
lima----------peru
london-----england
tokyo--------japan
edinburgh-scotland
ottawa------canada
oslo--------norway
stockholm---sweden
paris-------france

(b)

moscow---::::::::: =) moscow------russia
::::::::::--canada =) ottawa------canada

(c)

otowa-------canada =) ottawa------canada
egindurrh-sxotland =) edinburgh-scotland

The weights are set using the sum of outer products or Hebb rule,

wij = (cid:17)Xn

x(n)
i x(n)

j

;

(42.5)

where (cid:17) is an unimportant constant. To prevent the largest possible
weight from growing with N we might choose to set (cid:17) = 1=N .

Exercise 42.1.[1 ] Explain why the value of (cid:17) is not important for the Hop(cid:12)eld

network de(cid:12)ned above.

42.3 De(cid:12)nition of the continuous Hop(cid:12)eld network

Figure 42.2. Associative memory
(schematic). (a) A list of desired
memories. (b) The (cid:12)rst purpose of
an associative memory is pattern
completion, given a partial
pattern. (c) The second purpose
of a memory is error correction.

ai =Xj

wijxj

(42.6)

(42.7)

Using the identical architecture and learning rule we can de(cid:12)ne a Hop(cid:12)eld
network whose activities are real numbers between (cid:0)1 and 1.
Activity rule. A Hop(cid:12)eld network’s activity rule is for each neuron to up-
date its state as if it were a single neuron with a sigmoid activation
function. The updates may be synchronous or asynchronous, and in-
volve the equations

and

xi = tanh(ai):

The learning rule is the same as in the binary Hop(cid:12)eld network, but the
value of (cid:17) becomes relevant. Alternatively, we may (cid:12)x (cid:17) and introduce a gain
(cid:12) 2 (0;1) into the activation function:

xi = tanh((cid:12)ai):

(42.8)

Exercise 42.2.[1 ] Where have we encountered equations 42.6, 42.7, and 42.8

before?

42.4 Convergence of the Hop(cid:12)eld network

The hope is that the Hop(cid:12)eld networks we have de(cid:12)ned will perform associa-
tive memory recall, as shown schematically in (cid:12)gure 42.2. We hope that the
activity rule of a Hop(cid:12)eld network will take a partial memory or a corrupted
memory, and perform pattern completion or error correction to restore the
original memory.

But why should we expect any pattern to be stable under the activity rule,

let alone the desired memories?

We address the continuous Hop(cid:12)eld network, since the binary network is
a special case of it. We have already encountered the activity rule (42.6, 42.8)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

508

42 | Hop(cid:12)eld Networks

when we discussed variational methods (section 33.2): when we approximated
the spin system whose energy function was

(42.9)

(42.10)

with a separable distribution

E(x; J) = (cid:0)

hnxn

1
ZQ

1

2Xm;n

Q(x; a) =

Jmnxmxn (cid:0)Xn
anxn!
exp Xn
Q(x; a)E(x; J) (cid:0)Xx
am = (cid:12) Xn

Jmn (cid:22)xn + hm!

(cid:12) ~F (a) = (cid:12)Xx

we found that the pair of iterative equations

and optimized the latter so as to minimize the variational free energy

Q(x; a) ln

1

Q(x; a)

;

(42.11)

and

were guaranteed to decrease the variational free energy

(cid:22)xn = tanh(an)

(cid:12) ~F (a) = (cid:12) (cid:0)

1

2Xm;n

Jmn (cid:22)xm (cid:22)xn (cid:0)Xn

hn (cid:22)xn! (cid:0)Xn

(42.12)

(42.13)

H (e)

2 (qn):

(42.14)

If we simply replace J by w, (cid:22)x by x, and hn by wi0, we see that the
equations of the Hop(cid:12)eld network are identical to a set of mean-(cid:12)eld equations
that minimize

(cid:12) ~F (x) = (cid:0)(cid:12)

1
2

xTWx (cid:0)Xi

H (e)

2 [(1 + xi)=2]:

(42.15)

There is a general name for a function that decreases under the dynamical
evolution of a system and that is bounded below: such a function is a Lyapunov
function for the system.
It is useful to be able to prove the existence of
Lyapunov functions: if a system has a Lyapunov function then its dynamics
are bound to settle down to a (cid:12)xed point, which is a local minimum of the
Lyapunov function, or a limit cycle, along which the Lyapunov function is a
constant. Chaotic behaviour is not possible for a system with a Lyapunov
function.
If a system has a Lyapunov function then its state space can be
divided into basins of attraction, one basin associated with each attractor.

So, the continuous Hop(cid:12)eld network’s activity rules (if implemented asyn-
chronously) have a Lyapunov function. This Lyapunov function is a convex
function of each parameter ai so a Hop(cid:12)eld network’s dynamics will always
converge to a stable (cid:12)xed point.

This convergence proof depends crucially on the fact that the Hop(cid:12)eld
network’s connections are symmetric. It also depends on the updates being
made asynchronously.
Exercise 42.3.[2, p.520] Show by constructing an example that if a feedback
network does not have symmetric connections then its dynamics may
fail to converge to a (cid:12)xed point.

Exercise 42.4.[2, p.521] Show by constructing an example that if a Hop(cid:12)eld
network is updated synchronously that, from some initial conditions, it
may fail to converge to a (cid:12)xed point.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.4: Convergence of the Hop(cid:12)eld network

509

(a)

. 0 0 0 0 -2 2 -2 2 2 -2 0 0 0 2 0 0 -2 0 2 2 0 0 -2 -2
0 . 4 4 0 -2 -2 -2 -2 -2 -2 0 -4 0 -2 0 0 -2 0 -2 -2 4 4 2 -2
0 4 . 4 0 -2 -2 -2 -2 -2 -2 0 -4 0 -2 0 0 -2 0 -2 -2 4 4 2 -2
0 4 4 . 0 -2 -2 -2 -2 -2 -2 0 -4 0 -2 0 0 -2 0 -2 -2 4 4 2 -2
0 0 0 0 . 2 -2 -2 2 -2 2 -4 0 0 -2 4 -4 -2 0 -2 2 0 0 -2 2
-2 -2 -2 -2 2 . 0 0 0 0 4 -2 2 -2 0 2 -2 0 -2 0 0 -2 -2 0 4
2 -2 -2 -2 -2 0 . 0 0 4 0 2 2 -2 4 -2 2 0 -2 4 0 -2 -2 0 0
-2 -2 -2 -2 -2 0 0 . 0 0 0 2 2 2 0 -2 2 4 2 0 0 -2 -2 0 0
2 -2 -2 -2 2 0 0 0 . 0 0 -2 2 2 0 2 -2 0 2 0 4 -2 -2 -4 0
2 -2 -2 -2 -2 0 4 0 0 . 0 2 2 -2 4 -2 2 0 -2 4 0 -2 -2 0 0
-2 -2 -2 -2 2 4 0 0 0 0 . -2 2 -2 0 2 -2 0 -2 0 0 -2 -2 0 4
0 0 0 0 -4 -2 2 2 -2 2 -2 . 0 0 2 -4 4 2 0 2 -2 0 0 2 -2
0 -4 -4 -4 0 2 2 2 2 2 2 0 . 0 2 0 0 2 0 2 2 -4 -4 -2 2
0 0 0 0 0 -2 -2 2 2 -2 -2 0 0 . -2 0 0 2 4 -2 2 0 0 -2 -2
2 -2 -2 -2 -2 0 4 0 0 4 0 2 2 -2 . -2 2 0 -2 4 0 -2 -2 0 0
0 0 0 0 4 2 -2 -2 2 -2 2 -4 0 0 -2 . -4 -2 0 -2 2 0 0 -2 2
0 0 0 0 -4 -2 2 2 -2 2 -2 4 0 0 2 -4 . 2 0 2 -2 0 0 2 -2
-2 -2 -2 -2 -2 0 0 4 0 0 0 2 2 2 0 -2 2 . 2 0 0 -2 -2 0 0
0 0 0 0 0 -2 -2 2 2 -2 -2 0 0 4 -2 0 0 2 . -2 2 0 0 -2 -2
2 -2 -2 -2 -2 0 4 0 0 4 0 2 2 -2 4 -2 2 0 -2 . 0 -2 -2 0 0
2 -2 -2 -2 2 0 0 0 4 0 0 -2 2 2 0 2 -2 0 2 0 . -2 -2 -4 0
0 4 4 4 0 -2 -2 -2 -2 -2 -2 0 -4 0 -2 0 0 -2 0 -2 -2 . 4 2 -2
0 4 4 4 0 -2 -2 -2 -2 -2 -2 0 -4 0 -2 0 0 -2 0 -2 -2 4 . 2 -2
-2 2 2 2 -2 0 0 0 -4 0 0 2 -2 -2 0 -2 2 0 -2 0 -4 2 2 . 0
-2 -2 -2 -2 2 4 0 0 0 0 4 -2 2 -2 0 2 -2 0 -2 0 0 -2 -2 0 .

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(l)

!

!

!

(j)

(m)

!

!

!

(k)

!

!

!

!

!

!

!

!

!

!

Figure 42.3. Binary Hop(cid:12)eld
network storing four memories.
(a) The four memories, and the
weight matrix. (b{h) Initial states
that di(cid:11)er by one, two, three, four,
or even (cid:12)ve bits from a desired
memory are restored to that
memory in one or two iterations.
(i{m) Some initial conditions that
are far from the memories lead to
stable states other than the four
memories; in (i), the stable state
looks like a mixture of two
memories, ‘D’ and ‘J’; stable state
(j) is like a mixture of ‘J’ and ‘C’;
in (k), we (cid:12)nd a corrupted version
of the ‘M’ memory (two bits
distant); in (l) a corrupted version
of ‘J’ (four bits distant) and in
(m), a state which looks spurious
until we recognize that it is the
inverse of the stable state (l).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

510

42 | Hop(cid:12)eld Networks

42.5 The associative memory in action

Figure 42.3 shows the dynamics of a 25-unit binary Hop(cid:12)eld network that
has learnt four patterns by Hebbian learning. The four patterns are displayed
as (cid:12)ve by (cid:12)ve binary images in (cid:12)gure 42.3a. For twelve initial conditions,
panels (b{m) show the state of the network, iteration by iteration, all 25
units being updated asynchronously in each iteration. For an initial condition
randomly perturbed from a memory, it often only takes one iteration for all
the errors to be corrected. The network has more stable states in addition
to the four desired memories: the inverse of any stable state is also a stable
state; and there are several stable states that can be interpreted as mixtures
of the memories.

Brain damage

The network can be severely damaged and still work (cid:12)ne as an associative
memory. If we take the 300 weights of the network shown in (cid:12)gure 42.3 and
randomly set 50 or 100 of them to zero, we still (cid:12)nd that the desired memories
are attracting stable states. Imagine a digital computer that still works (cid:12)ne
even when 20% of its components are destroyed!

. Exercise 42.5.[3C ] Implement a Hop(cid:12)eld network and con(cid:12)rm this amazing

robust error-correcting capability.

More memories

We can squash more memories into the network too. Figure 42.4a shows a set
of (cid:12)ve memories. When we train the network with Hebbian learning, all (cid:12)ve
memories are stable states, even when 26 of the weights are randomly deleted
(as shown by the ‘x’s in the weight matrix). However, the basins of attraction
are smaller than before: (cid:12)gures 42.4(b{f) show the dynamics resulting from
randomly chosen starting states close to each of the memories (3 bits (cid:13)ipped).
Only three of the memories are recovered correctly.

If we try to store too many patterns, the associative memory fails catas-
trophically. When we add a sixth pattern, as shown in (cid:12)gure 42.5, only one
of the patterns is stable; the others all (cid:13)ow into one of two spurious stable
states.

42.6 The continuous-time continuous Hop(cid:12)eld network

The fact that the Hop(cid:12)eld network’s properties are not robust to the minor
change from asynchronous to synchronous updates might be a cause for con-
cern; can this model be a useful model of biological networks? It turns out
that once we move to a continuous-time version of the Hop(cid:12)eld networks, this
issue melts away.

We assume that each neuron’s activity xi is a continuous function of time
xi(t) and that the activations ai(t) are computed instantaneously in accordance
with

wijxj(t):

(42.16)

ai(t) =Xj

The neuron’s response to its activation is assumed to be mediated by the
di(cid:11)erential equation:

d
dt

xi(t) = (cid:0)

1
(cid:28)

(xi(t) (cid:0) f (ai));

(42.17)

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.6: The continuous-time continuous Hop(cid:12)eld network

511

(a)

. -1 1 -1 1 x x -3 3 x x -1 1 -1 x -1 1 -3 x 1 3 -1 1 x -1
-1 . 3 5 -1 -1 -3 -1 -3 -1 -3 1 x 1 -3 1 -1 -1 -1 -1 -3 5 3 3 -3
1 3 . 3 1 -3 -1 x -1 -3 -1 -1 x -1 -1 -1 1 -3 1 -3 -1 3 5 1 -1
-1 5 3 . -1 -1 -3 -1 -3 -1 -3 1 -5 1 -3 1 -1 -1 -1 -1 -3 5 x 3 -3
1 -1 1 -1 . 1 -1 -3 x x 3 -5 1 -1 -1 3 x -3 1 -3 3 -1 1 -3 3
x -1 -3 -1 1 . -1 1 -1 1 3 -1 1 -1 -1 3 -3 1 x 1 x -1 -3 1 3
x -3 -1 -3 -1 -1 . -1 1 3 1 1 3 -3 5 -3 3 -1 -1 x 1 -3 -1 -1 1
-3 -1 x -1 -3 1 -1 . -1 1 -1 3 1 x -1 -1 1 5 1 1 -1 x -3 1 -1
3 -3 -1 -3 x -1 1 -1 . -1 1 -3 3 1 1 1 -1 -1 3 -1 5 -3 -1 x 1
x -1 -3 -1 x 1 3 1 -1 . -1 3 1 -1 3 -1 x 1 -3 5 -1 -1 -3 1 -1
x -3 -1 -3 3 3 1 -1 1 -1 . -3 3 -3 1 1 -1 -1 -1 -1 1 -3 -1 -1 5
-1 1 -1 1 -5 -1 1 3 -3 3 -3 . -1 1 1 -3 3 x -1 3 -3 1 -1 3 -3
1 x x -5 1 1 3 1 3 1 3 -1 . -1 3 -1 1 1 1 1 3 -5 -3 -3 3
-1 1 -1 1 -1 -1 -3 x 1 -1 -3 1 -1 . x 1 -1 3 3 -1 1 1 -1 -1 -3
x -3 -1 -3 -1 -1 5 -1 1 3 1 1 3 x . x 3 -1 -1 3 1 -3 -1 -1 1
-1 1 -1 1 3 3 -3 -1 1 -1 1 -3 -1 1 x . -5 -1 -1 -1 1 1 -1 -1 1
1 -1 1 -1 x -3 3 1 -1 x -1 3 1 -1 3 -5 . 1 1 1 -1 -1 1 1 -1
-3 -1 -3 -1 -3 1 -1 5 -1 1 -1 x 1 3 -1 -1 1 . 1 1 -1 -1 -3 1 -1
x -1 1 -1 1 x -1 1 3 -3 -1 -1 1 3 -1 -1 1 1 . -3 3 -1 1 -3 -1
1 -1 -3 -1 -3 1 x 1 -1 5 -1 3 1 -1 3 -1 1 1 -3 . x -1 -3 1 -1
3 -3 -1 -3 3 x 1 -1 5 -1 1 -3 3 1 1 1 -1 -1 3 x . -3 -1 -5 1
-1 5 3 5 -1 -1 -3 x -3 -1 -3 1 -5 1 -3 1 -1 -1 -1 -1 -3 . 3 x -3
1 3 5 x 1 -3 -1 -3 -1 -3 -1 -1 -3 -1 -1 -1 1 -3 1 -3 -1 3 . 1 -1
x 3 1 3 -3 1 -1 1 x 1 -1 3 -3 -1 -1 -1 1 1 -3 1 -5 x 1 . -1
-1 -3 -1 -3 3 3 1 -1 1 -1 5 -3 3 -3 1 1 -1 -1 -1 -1 1 -3 -1 -1 .

(b)

(c)

(d)

(e)

(f)

!

!

!

!

!

!

!

!

Figure 42.4. Hop(cid:12)eld network
storing (cid:12)ve memories, and
su(cid:11)ering deletion of 26 of its 300
weights. (a) The (cid:12)ve memories,
and the weights of the network,
with deleted weights shown by ‘x’.
(b{f) Initial states that di(cid:11)er by
three random bits from a
memory: some are restored, but
some converge to other states.

Desired memories:

Figure 42.5. An overloaded
Hop(cid:12)eld network trained on six
memories, most of which are not
stable.

!

!

!

!

!

!

!

!

!

!

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

512

Desired memories

moscow------russia
lima----------peru
london-----england
tokyo--------japan
edinburgh-scotland
ottawa------canada
oslo--------norway
stockholm---sweden
paris-------france

! W !

Attracting stable states
moscow------russia
lima----------peru
londog-----englard
tonco--------japan
edinburgh-scotland

oslo--------norway
stockholm---sweden
paris-------france
wzkmhewn--xqwqwpoq
paris-------sweden
ecnarf-------sirap

(1)
(1)

(2)

(3)
(4)
(4)

where f (a) is the activation function, for example f (a) = tanh(a). For a
steady activation ai, the activity xi(t) relaxes exponentially to f (ai) with
time-constant (cid:28) .

Now, here is the nice result: as long as the weight matrix is symmetric,
this system has the variational free energy (42.15) as its Lyapunov function.

. Exercise 42.6.[1 ] By computing d
dt

~F , prove that the variational free energy
~F (x) is a Lyapunov function for the continuous-time Hop(cid:12)eld network.

42 | Hop(cid:12)eld Networks

Figure 42.6. Failure modes of a
Hop(cid:12)eld network (highly
schematic). A list of desired
memories, and the resulting list of
attracting stable states. Notice
(1) some memories that are
retained with a small number of
errors; (2) desired memories that
are completely lost (there is no
attracting stable state at the
desired memory or near it); (3)
spurious stable states unrelated to
the original list; (4) spurious
stable states that are
confabulations of desired
memories.

It is particularly easy to prove that a function L is a Lyapunov function if the
system’s dynamics perform steepest descent on L, with d
L. In
the case of the continuous-time continuous Hop(cid:12)eld network, it is not quite
~F ,
so simple, but every component of d
which means that with an appropriately de(cid:12)ned metric, the Hop(cid:12)eld network
dynamics do perform steepest descents on ~F (x).

dt xi(t) / @
dt xi(t) does have the same sign as

@xi

@
@xi

42.7 The capacity of the Hop(cid:12)eld network

One way in which we viewed learning in the single neuron was as communica-
tion { communication of the labels of the training data set from one point in
time to a later point in time. We found that the capacity of a linear threshold
neuron was 2 bits per weight.

Similarly, we might view the Hop(cid:12)eld associative memory as a commu-
nication channel ((cid:12)gure 42.6). A list of desired memories is encoded into a
set of weights W using the Hebb rule of equation (42.5), or perhaps some
other learning rule. The receiver, receiving the weights W only, (cid:12)nds the
stable states of the Hop(cid:12)eld network, which he interprets as the original mem-
ories. This communication system can fail in various ways, as illustrated in
the (cid:12)gure.

1. Individual bits in some memories might be corrupted, that is, a sta-
ble state of the Hop(cid:12)eld network is displaced a little from the desired
memory.

2. Entire memories might be absent from the list of attractors of the net-
work; or a stable state might be present but have such a small basin of
attraction that it is of no use for pattern completion and error correction.

3. Spurious additional memories unrelated to the desired memories might

be present.

4. Spurious additional memories derived from the desired memories by op-

erations such as mixing and inversion may also be present.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.7: The capacity of the Hop(cid:12)eld network

513

Of these failure modes, modes 1 and 2 are clearly undesirable, mode 2 espe-
cially so. Mode 3 might not matter so much as long as each of the desired
memories has a large basin of attraction. The fourth failure mode might in
some contexts actually be viewed as bene(cid:12)cial. For example, if a network is
required to memorize examples of valid sentences such as ‘John loves Mary’
and ‘John gets cake’, we might be happy to (cid:12)nd that ‘John loves cake’ was also
a stable state of the network. We might call this behaviour ‘generalization’.

The capacity of a Hop(cid:12)eld network with I neurons might be de(cid:12)ned to be
the number of random patterns N that can be stored without failure-mode 2
having substantial probability. If we also require failure-mode 1 to have tiny
probability then the resulting capacity is much smaller. We now study these
alternative de(cid:12)nitions of the capacity.

The capacity of the Hop(cid:12)eld network { stringent de(cid:12)nition

We will (cid:12)rst explore the information storage capabilities of a binary Hop(cid:12)eld
network that learns using the Hebb rule by considering the stability of just
one bit of one of the desired patterns, assuming that the state of the network
is set to that desired pattern x(n). We will assume that the patterns to be
stored are randomly selected binary patterns.
The activation of a particular neuron i is

wijx(n)

j

;

ai =Xj

where the weights are, for i 6= j,
wij = x(n)

i x(n)

j + Xm6=n

x(m)
i x(m)

j

:

(42.18)

(42.19)

Here we have split W into two terms, the (cid:12)rst of which will contribute ‘signal’,
reinforcing the desired memory, and the second ‘noise’. Substituting for wij,
the activation is

ai = Xj6=i

x(n)
i x(n)

j x(n)

j +Xj6=i Xm6=n

x(m)
i x(m)

j x(n)

j

(42.20)

x(m)
i x(m)

j x(n)

j

:

(42.21)

= (I (cid:0) 1)x(n)

i +Xj6=i Xm6=n

The (cid:12)rst term is (I (cid:0) 1) times the desired state x(n)
second term is a sum of (I (cid:0) 1)(N (cid:0) 1) random quantities x(m)

. If this were the only
term, it would keep the neuron (cid:12)rmly clamped in the desired state. The
. A
moment’s re(cid:13)ection con(cid:12)rms that these quantities are independent random
binary variables with mean 0 and variance 1.

i x(m)

j x(n)

j

i

Thus, considering the statistics of ai under the ensemble of random pat-
terns, we conclude that ai has mean (I (cid:0) 1)x(n)
and variance (I (cid:0) 1)(N (cid:0) 1).
For brevity, we will now assume I and N are large enough that we can
neglect the distinction between I and I (cid:0) 1, and between N and N (cid:0) 1. Then
we can restate our conclusion: ai is Gaussian-distributed with mean Ix(n)
and
variance IN .

i

i

What then is the probability that the selected bit is stable, if we put the
network into the state x(n)? The probability that bit i will (cid:13)ip on the (cid:12)rst
iteration of the Hop(cid:12)eld network’s dynamics is

P (i unstable) = (cid:8)(cid:18)(cid:0)

I

pIN(cid:19) = (cid:8) (cid:0)

1

pN=I! ;

(42.22)

pIN

I

ai

Figure 42.7. The probability
density of the activation ai in the
case x(n)
i = 1; the probability that
bit i becomes (cid:13)ipped is the area
of the tail.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42 | Hop(cid:12)eld Networks

Figure 42.8. Overlap between a
desired memory and the stable
state nearest to it as a function of
the loading fraction N=I. The
overlap is de(cid:12)ned to be the scaled
i =I, which
is 1 when recall is perfect and zero
when the stable state has 50% of
the bits (cid:13)ipped. There is an
abrupt transition at N=I = 0:138,
where the overlap drops from 0.97
to zero.

inner productPi xix(n)

514

1

0.8

0.6

0.4

0.2

0

where

1

0.99

0.98

0.97

0.96

0

0.02 0.04 0.06 0.08

0.1

0.12 0.14 0.16

0.95

0.09

0.1

0.11

0.12

0.13

0.14

0.15

(cid:8)(z) (cid:17)Z z

(cid:0)1

dz

1p2(cid:25)

e(cid:0)z2=2:

(42.23)

The important quantity N=I is the ratio of the number of patterns stored to
the number of neurons. If, for example, we try to store N ’ 0:18I patterns
in the Hop(cid:12)eld network then there is a chance of 1% that a speci(cid:12)ed bit in a
speci(cid:12)ed pattern will be unstable on the (cid:12)rst iteration.

We are now in a position to derive our (cid:12)rst capacity result, for the case

where no corruption of the desired memories is permitted.

. Exercise 42.7.[2 ] Assume that we wish all the desired patterns to be completely
stable { we don’t want any of the bits to (cid:13)ip when the network is put
into any desired pattern state { and the total probability of any error at
all is required to be less than a small number (cid:15). Using the approximation
to the error function for large z,

(cid:8)((cid:0)z) ’

1
p2(cid:25)

e(cid:0)z2=2

z

;

(42.24)

show that the maximum number of patterns that can be stored, Nmax,
is

Nmax ’

4 ln I + 2 ln(1=(cid:15))

I

:

(42.25)

If, however, we allow a small amount of corruption of memories to occur, the
number of patterns that can be stored increases.

The statistical physicists’ capacity
The analysis that led to equation (42.22) tells us that if we try to store N ’
0:18I patterns in the Hop(cid:12)eld network then, starting from a desired memory,
about 1% of the bits will be unstable on the (cid:12)rst iteration. Our analysis does
not shed light on what is expected to happen on subsequent iterations. The
(cid:13)ipping of these bits might make some of the other bits unstable too, causing
an increasing number of bits to be (cid:13)ipped. This process might lead to an
avalanche in which the network’s state ends up a long way from the desired
memory.

In fact, when N=I is large, such avalanches do happen. When N=I is small,
they tend not to { there is a stable state near to each desired memory. For the
limit of large I, Amit et al. (1985) have used methods from statistical physics
to (cid:12)nd numerically the transition between these two behaviours. There is a
sharp discontinuity at

Ncrit = 0:138I:

(42.26)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.8: Improving on the capacity of the Hebb rule

515

Below this critical value, there is likely to be a stable state near every desired
memory, in which a small fraction of the bits are (cid:13)ipped. When N=I exceeds
0.138, the system has only spurious stable states, known as spin glass states,
none of which is correlated with any of the desired memories. Just below the
critical value, the fraction of bits that are (cid:13)ipped when a desired memory has
evolved to its associated stable state is 1.6%. Figure 42.8 shows the overlap
between the desired memory and the nearest stable state as a function of N=I.
Some other transitions in properties of the model occur at some additional

values of N=I, as summarized below.

For all N=I, stable spin glass states exist, uncorrelated with the desired

memories.

For N=I > 0:138, these spin glass states are the only stable states.

For N=I 2 (0; 0:138), there are stable states close to the desired memories.
For N=I 2 (0; 0:05), the stable states associated with the desired memories

have lower energy than the spurious spin glass states.

For N=I 2 (0:05; 0:138), the spin glass states dominate { there are spin glass
states that have lower energy than the stable states associated with the
desired memories.

For N=I 2 (0; 0:03), there are additional mixture states, which are combina-
tions of several desired memories. These stable states do not have as low
energy as the stable states associated with the desired memories.

In conclusion, the capacity of the Hop(cid:12)eld network with I neurons, if we
de(cid:12)ne the capacity in terms of the abrupt discontinuity discussed above, is
0:138I random binary patterns, each of length I, each of which is received
with 1.6% of its bits (cid:13)ipped. In bits, this capacity is

0:138I 2 (cid:2) (1 (cid:0) H2(0:016)) = 0:122 I 2 bits:

(42.27)

Since there are I 2=2 weights in the network, we can also express the capacity
as 0.24 bits per weight.

42.8 Improving on the capacity of the Hebb rule

The capacities discussed in the previous section are the capacities of the Hop-
(cid:12)eld network whose weights are set using the Hebbian learning rule. We can
do better than the Hebb rule by de(cid:12)ning an objective function that measures
how well the network stores all the memories, and minimizing it.

For an associative memory to be useful, it must be able to correct at
least one (cid:13)ipped bit. Let’s make an objective function that measures whether
(cid:13)ipped bits tend to be restored correctly. Our intention is that, for every
neuron i in the network, the weights to that neuron should satisfy this rule:

This expression for the capacity
omits a smaller negative term of
order N log2 N bits, associated
with the arbitrary order of the
memories.

for every pattern x(n), if the neurons other than i are set correctly
to xj = x(n)
, then the activation of neuron i should be such that
its preferred output is xi = x(n)

.

j

i

Is this rule a familiar idea? Yes, it is precisely what we wanted the single
neuron of Chapter 39 to do. Each pattern x(n) de(cid:12)nes an input, target pair
for the single neuron i. And it de(cid:12)nes an input, target pair for all the other
neurons too.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42 | Hop(cid:12)eld Networks

Algorithm 42.9. Octave source
code for optimizing the weights of
a Hop(cid:12)eld network, so that it
works as an associative memory.
cf. algorithm 39.5. The data
matrix x has I columns and N
rows. The matrix t is identical to
x except that (cid:0)1s are replaced by
0s.

516

w = x’ * x ;

# initialize the weights using Hebb rule

for l = 1:L

# loop L times

for i=1:I

w(i,i) = 0 ;

end

#
#
#

ensure the self-weights are zero.

a
y
e
gw = x’ * e
gw = gw + gw’

= x * w
;
= sigmoid(a) ;
;
= t - y
;
;

# compute all activations
# compute all outputs
# compute all errors
# compute the gradients
# symmetrize gradients

w

= w + eta * ( gw - alpha * w ) ;

# make step

endfor

So, just as we de(cid:12)ned an objective function (39.11) for the training of a

single neuron as a classi(cid:12)er, we can de(cid:12)ne

G(W) = (cid:0)Xi Xn

t(n)
i

ln y(n)

i + (1 (cid:0) t(n)

i

) ln(1 (cid:0) y(n)

i

)

(42.28)

where

and

t(n)

i =( 1

x(n)
i = 1
0 x(n)
i = (cid:0)1

y(n)
i =

1

1 + exp((cid:0)a(n)

i

)

(42.29)

:

(42.30)

; where a(n)

i =P wijx(n)

j

We can then steal the algorithm (algorithm 39.5, p.478) which we wrote for
the single neuron, to write an algorithm for optimizing a Hop(cid:12)eld network,
algorithm 42.9. The convenient syntax of Octave requires very few changes;
the extra lines enforce the constraints that the self-weights wii should all be
zero and that the weight matrix should be symmetrical (wij = wji).

As expected, this learning algorithm does a better job than the one-shot
Hebbian learning rule. When the six patterns of (cid:12)gure 42.5, which cannot be
memorized by the Hebb rule, are learned using algorithm 42.9, all six patterns
become stable states.

Exercise 42.8.[4C ] Implement this learning rule and investigate empirically its
capacity for memorizing random patterns; also compare its avalanche
properties with those of the Hebb rule.

42.9 Hop(cid:12)eld networks for optimization problems

Since a Hop(cid:12)eld network’s dynamics minimize an energy function, it is natural
to ask whether we can map interesting optimization problems onto Hop(cid:12)eld
networks. Biological data processing problems often involve an element of
constraint satisfaction { in scene interpretation, for example, one might wish
to infer the spatial location, orientation, brightness and texture of each visible
element, and which visible elements are connected together in objects. These
inferences are constrained by the given data and by prior knowledge about
continuity of objects.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.9: Hop(cid:12)eld networks for optimization problems

517

Place in tour
1

2

3

4

Place in tour
1

2

3

4

City

A

B

C

D

A

C

City

A

B

C

D

A

C

B

D

B

D

(a1)

(a2)

Figure 42.10. Hop(cid:12)eld network for
solving a travelling salesman
problem with K = 4 cities. (a1,2)
Two solution states of the
16-neuron network, with activites
represented by black = 1, white =
0; and the tours corresponding to
these network states. (b) The
negative weights between node B2
and other nodes; these weights
enforce validity of a tour. (c) The
negative weights that embody the
distance objective function.

1

2

3

4

1

2

3

4

(cid:0)dBD

A

B

C

D

A

B

C

D

(b)

(c)

Hop(cid:12)eld and Tank (1985) suggested that one might take an interesting
constraint satisfaction problem and design the weights of a binary or contin-
uous Hop(cid:12)eld network such that the settling process of the network would
minimize the objective function of the problem.

The travelling salesman problem

A classic constraint satisfaction problem to which Hop(cid:12)eld networks have been
applied is the travelling salesman problem.

A set of K cities is given, and a matrix of the K(K(cid:0)1)=2 distances between
those cities. The task is to (cid:12)nd a closed tour of the cities, visiting each city
once, that has the smallest total distance. The travelling salesman problem is
equivalent in di(cid:14)culty to an NP-complete problem.

The method suggested by Hop(cid:12)eld and Tank is to represent a tentative so-
lution to the problem by the state of a network with I = K 2 neurons arranged
in a square, with each neuron representing the hypothesis that a particular
city comes at a particular point in the tour. It will be convenient to consider
the states of the neurons as being between 0 and 1 rather than (cid:0)1 and 1.
Two solution states for a four-city travelling salesman problem are shown in
(cid:12)gure 42.10a.

The weights in the Hop(cid:12)eld network play two roles. First, they must de(cid:12)ne
an energy function which is minimized only when the state of the network
represents a valid tour. A valid state is one that looks like a permutation
matrix, having exactly one ‘1’ in every row and one ‘1’ in every column. This
rule can be enforced by putting large negative weights between any pair of
neurons that are in the same row or the same column, and setting a positive
bias for all neurons to ensure that K neurons do turn on. Figure 42.10b shows
the negative weights that are connected to one neuron, ‘B2’, which represents
the statement ‘city B comes second in the tour’.

Second, the weights must encode the objective function that we want
to minimize { the total distance. This can be done by putting negative
weights proportional to the appropriate distances between the nodes in adja-
cent columns. For example, between the B and D nodes in adjacent columns,
the weight would be (cid:0)dBD. The negative weights that are connected to neu-
ron B2 are shown in (cid:12)gure 42.10c. The result is that when the network is in
a valid state, its total energy will be the total distance of the corresponding

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

518

42 | Hop(cid:12)eld Networks

Figure 42.11. (a) Evolution of the
state of a continuous Hop(cid:12)eld
network solving a travelling
salesman problem using Aiyer’s
(1991) graduated non-convexity
method; the state of the network
is projected into the
two-dimensional space in which
the cities are located by (cid:12)nding
the centre of mass for each point
in the tour, using the neuron
activities as the mass function.
(b) The travelling scholar
problem. The shortest tour
linking the 27 Cambridge
Colleges, the Engineering
Department, the University
Library, and Sree Aiyer’s house.
From Aiyer (1991).

(a)

(b)

tour, plus a constant given by the energy associated with the biases.

Now, since a Hop(cid:12)eld network minimizes its energy, it is hoped that the
binary or continuous Hop(cid:12)eld network’s dynamics will take the state to a
minimum that is a valid tour and which might be an optimal tour. This hope
is not ful(cid:12)lled for large travelling salesman problems, however, without some
careful modi(cid:12)cations. We have not speci(cid:12)ed the size of the weights that enforce
the tour’s validity, relative to the size of the distance weights, and setting this
scale factor poses di(cid:14)culties.
If ‘large’ validity-enforcing weights are used,
the network’s dynamics will rattle into a valid state with little regard for the
distances. If ‘small’ validity-enforcing weights are used, it is possible that the
distance weights will cause the network to adopt an invalid state that has lower
energy than any valid state. Our original formulation of the energy function
puts the objective function and the solution’s validity in potential con(cid:13)ict
with each other. This di(cid:14)culty has been resolved by the work of Sree Aiyer
(1991), who showed how to modify the distance weights so that they would not
interfere with the solution’s validity, and how to de(cid:12)ne a continuous Hop(cid:12)eld
network whose dynamics are at all times con(cid:12)ned to a ‘valid subspace’. Aiyer
used a graduated non-convexity or deterministic annealing approach to (cid:12)nd
good solutions using these Hop(cid:12)eld networks. The deterministic annealing
approach involves gradually increasing the gain (cid:12) of the neurons in the network
from 0 to 1, at which point the state of the network corresponds to a valid
tour. A sequence of trajectories generated by applying this method to a thirty-
city travelling salesman problem is shown in (cid:12)gure 42.11a.

A solution to the ‘travelling scholar problem’ found by Aiyer using a con-

tinuous Hop(cid:12)eld network is shown in (cid:12)gure 42.11b.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

519

42.10: Further exercises

42.10 Further exercises

. Exercise 42.9.[3 ] Storing two memories.

Two binary memories m and n (mi; ni 2 f(cid:0)1; +1g) are stored by Heb-
bian learning in a Hop(cid:12)eld network using

wij =(cid:26) mimj + ninj

0

for i 6= j
for i = j.

(42.31)

The biases bi are set to zero.

The network is put in the state x = m. Evaluate the activation ai of
neuron i and show that in can be written in the form

ai = (cid:22)mi + (cid:23)ni:

(42.32)

By comparing the signal strength, (cid:22), with the magnitude of the noise
strength, j(cid:23)j, show that x = m is a stable state of the dynamics of the
network.

The network is put in a state x di(cid:11)ering in D places from m,

x = m + 2d;

(42.33)

where the perturbation d satis(cid:12)es di 2 f(cid:0)1; 0; +1g. D is the number
of components of d that are non-zero, and for each di that is non-zero,
di = (cid:0)mi. De(cid:12)ning the overlap between m and n to be

omn =

mini;

I

Xi=1

(42.34)

evaluate the activation ai of neuron i again and show that the dynamics
of the network will restore x to m if the number of (cid:13)ipped bits satis(cid:12)es

D <

1
4

(I (cid:0) jomnj (cid:0) 2):

(42.35)

How does this number compare with the maximum number of (cid:13)ipped
bits that can be corrected by the optimal decoder, assuming the vector
x is either a noisy version of m or of n?

Exercise 42.10.[3 ] Hop(cid:12)eld network as a collection of binary classi(cid:12)ers. This ex-
ercise explores the link between unsupervised networks and supervised
networks.
If a Hop(cid:12)eld network’s desired memories are all attracting
stable states, then every neuron in the network has weights going to it
that solve a classi(cid:12)cation problem personal to that neuron. Take the set
of memories and write them in the form x0(n); x(n)
, where x0 denotes all
the components xi0 for all i0 6= i, and let w0 denote the vector of weights
wii0, for i0 6= i.
Using what we know about the capacity of the single neuron, show that
it is almost certainly impossible to store more than 2I random memories
in a Hop(cid:12)eld network of I neurons.

i

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

520

Lyapunov functions

Exercise 42.11.[3 ] Erik’s puzzle. In a stripped-down version of Conway’s game
of life, cells are arranged on a square grid. Each cell is either alive or
dead. Live cells do not die. Dead cells become alive if two or more of
their immediate neighbours are alive. (Neighbours to north, south, east
and west.) What is the smallest number of live cells needed in order
that these rules lead to an entire N (cid:2) N square being alive?
In a d-dimensional version of the same game, the rule is that if d neigh-
bours are alive then you come to life. What is the smallest number of
live cells needed in order that an entire N (cid:2) N (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) N hypercube
becomes alive? (And how should those live cells be arranged?)

42 | Hop(cid:12)eld Networks

!

!

Figure 42.12. Erik’s dynamics.

The southeast puzzle

u

(a)

-

u?

-(b)

-u
u?

-(c)

-u
u?

u

-(d)

u uu

u

- : : : -(z)

eee ee e

eeee

The southeast puzzle is played on a semi-in(cid:12)nite chess board, starting at

its northwest (top left) corner. There are three rules:

Figure 42.13. The southeast
puzzle.

1. In the starting position, one piece is placed in the northwest-most square

((cid:12)gure 42.13a).

2. It is not permitted for more than one piece to be on any given square.

3. At each step, you remove one piece from the board, and replace it with
two pieces, one in the square immediately to the east, and one in the the
square immediately to the south, as illustrated in (cid:12)gure 42.13b. Every
such step increases the number of pieces on the board by one.

After move (b) has been made, either piece may be selected for the next move.
Figure 42.13c shows the outcome of moving the lower piece. At the next move,
either the lowest piece or the middle piece of the three may be selected; the
uppermost piece may not be selected, since that would violate rule 2. At move
(d) we have selected the middle piece. Now any of the pieces may be moved,
except for the leftmost piece.

Now, here is the puzzle:

. Exercise 42.12.[4, p.521] Is it possible to obtain a position in which all the ten
squares closest to the northwest corner, marked in (cid:12)gure 42.13z, are
empty?

[Hint: this puzzle has a connection to data compression.]

42.11 Solutions

Solution to exercise 42.3 (p.508). Take a binary feedback network with 2 neu-
rons and let w12 = 1 and w21 = (cid:0)1. Then whenever neuron 1 is updated,
it will match neuron 2, and whenever neuron 2 is updated, it will (cid:13)ip to the
opposite state from neuron 1. There is no stable state.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

42.11: Solutions

521

Solution to exercise 42.4 (p.508). Take a binary Hop(cid:12)eld network with 2 neu-
rons and let w12 = w21 = 1, and let the initial condition be x1 = 1, x2 = (cid:0)1.
Then if the dynamics are synchronous, on every iteration both neurons will
(cid:13)ip their state. The dynamics do not converge to a (cid:12)xed point.

Solution to exercise 42.12 (p.520). The key to this problem is to notice its
similarity to the construction of a binary symbol code. Starting from the
empty string, we can build a binary tree by repeatedly splitting a codeword
into two. Every codeword has an implicit probability 2(cid:0)l, where l is the
depth of the codeword in the binary tree. Whenever we split a codeword in
two and create two new codewords whose length is increased by one, the two
new codewords each have implicit probability equal to half that of the old
codeword. For a complete binary code, the Kraft equality a(cid:14)rms that the
sum of these implicit probabilities is 1.

Similarly, in southeast, we can associate a ‘weight’ with each piece on the
board. If we assign a weight of 1 to any piece sitting on the top left square;
a weight of 1/2 to any piece on a square whose distance from the top left is
one; a weight of 1/4 to any piece whose distance from the top left is two; and
so forth, with ‘distance’ being the city-block distance; then every legal move
in southeast leaves unchanged the total weight of all pieces on the board.
Lyapunov functions come in two (cid:13)avours: the function may be a function of
state whose value is known to stay constant; or it may be a function of state
that is bounded below, and whose value always decreases or stays constant.
The total weight is a Lyapunov function of the second type.

The starting weight is 1, so now we have a powerful tool: a conserved
function of the state. Is it possible to (cid:12)nd a position in which the ten highest-
weight squares are vacant, and the total weight is 1? What is the total weight
if all the other squares on the board are occupied ((cid:12)gure 42.14)? The total

weight would be P1l=4(l + 1)2(cid:0)l, which is equal to 3=4. So it is impossible to

empty all ten of those squares.

uuuuu

: : :

: : :

. . .

uu
uu

uuuuuu

...

...

Figure 42.14. A possible position
for the southeast puzzle?

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

43

Boltzmann Machines

43.1 From Hop(cid:12)eld networks to Boltzmann machines

We have noticed that the binary Hop(cid:12)eld network minimizes an energy func-
tion

xTWx

(43.1)

E(x) = (cid:0)

1
2

and that the continuous Hop(cid:12)eld network with activation function xn =
tanh(an) can be viewed as approximating the probability distribution asso-
ciated with that energy function,

P (xj W) =

1

Z(W)

exp[(cid:0)E(x)] =

1

Z(W)

exp(cid:20) 1

2

xTWx(cid:21) :

(43.2)

These observations motivate the idea of working with a neural network model
that actually implements the above probability distribution.

The stochastic Hop(cid:12)eld network or Boltzmann machine (Hinton and Se-

jnowski, 1986) has the following activity rule:

Activity rule of Boltzmann machine: after computing the activa-

tion ai (42.3),

set xi = +1 with probability
else set xi = (cid:0)1:

1

1 + e(cid:0)2ai

(43.3)

This rule implements Gibbs sampling for the probability distribution (43.2).

Boltzmann machine learning
Given a set of examples fx(n)gN
in adjusting the weights W such that the generative model

1 from the real world, we might be interested

P (xj W) =

1

Z(W)

exp(cid:20) 1

2

xTWx(cid:21)

(43.4)

is well matched to those examples. We can derive a learning algorithm by
writing down Bayes’ theorem to obtain the posterior probability of the weights
given the data:

P (W jfx(n)gN

1 g) =

" N
Yn=1

P (x(n) j W)# P (W)
P (fx(n)gN

1 g)

:

522

(43.5)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

43.1: From Hop(cid:12)eld networks to Boltzmann machines

523

We concentrate on the (cid:12)rst term in the numerator, the likelihood, and derive a
maximum likelihood algorithm (though there might be advantages in pursuing
a full Bayesian approach as we did in the case of the single neuron). We
di(cid:11)erentiate the logarithm of the likelihood,

ln" N
Yn=1

P (x(n) j W)# =

N

Xn=1(cid:20) 1

2

x(n)T

Wx(n) (cid:0) ln Z(W)(cid:21) ;

(43.6)

with respect to wij, bearing in mind that W is de(cid:12)ned to be symmetric with
wji = wij.

Exercise 43.1.[2 ] Show that the derivative of ln Z(W) with respect to wij is

@

@wij

ln Z(W) =Xx

xixjP (xj W) = hxixjiP (xj W) :

(43.7)

[This exercise is similar to exercise 22.12 (p.307).]

The derivative of the log likelihood is therefore:

@

@wij

ln P (fx(n)gN

1 gj W) =

N

i x(n)

Xn=1hx(n)
j (cid:0) hxixjiP (x j W)i
= NhhxixjiData (cid:0) hxixjiP (xj W)i :

(43.8)

(43.9)

This gradient is proportional to the di(cid:11)erence of two terms. The (cid:12)rst term is
the empirical correlation between xi and xj,

hxixjiData (cid:17)

1
N

N

Xn=1hx(n)

i x(n)

j i ;

(43.10)

and the second term is the correlation between xi and xj under the current
model,

hxixjiP (x j W) (cid:17)Xx

xixjP (xj W):

(43.11)

The (cid:12)rst correlation hxixjiData is readily evaluated { it is just the empirical
correlation between the activities in the real world. The second correlation,
hxixjiP (x j W), is not so easy to evaluate, but it can be estimated by Monte
Carlo methods, that is, by observing the average value of xixj while the ac-
tivity rule of the Boltzmann machine, equation (43.3), is iterated.

In the special case W = 0, we can evaluate the gradient exactly because,
by symmetry, the correlation hxixjiP (x j W) must be zero. If the weights are
adjusted by gradient descent with learning rate (cid:17), then, after one iteration,
the weights will be

wij = (cid:17)

N

Xn=1hx(n)

i x(n)

j i ;

(43.12)

precisely the value of the weights given by the Hebb rule, equation (16.5), with
which we trained the Hop(cid:12)eld network.

Interpretation of Boltzmann machine learning

One way of viewing the two terms in the gradient (43.9) is as ‘waking’ and
‘sleeping’ rules. While the network is ‘awake’,
it measures the correlation
between xi and xj in the real world, and weights are increased in proportion.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

524

43 | Boltzmann Machines

(a)

(b)

Figure 43.1. The ‘shifter’
ensembles. (a) Four samples from
the plain shifter ensemble. (b)
Four corresponding samples from
the labelled shifter ensemble.

While the network is ‘asleep’, it ‘dreams’ about the world using the generative
model (43.4), and measures the correlations between xi and xj in the model
world; these correlations determine a proportional decrease in the weights. If
the second-order correlations in the dream world match the correlations in the
real world, then the two terms balance and the weights do not change.

Criticism of Hop(cid:12)eld networks and simple Boltzmann machines

Up to this point we have discussed Hop(cid:12)eld networks and Boltzmann machines
in which all of the neurons correspond to visible variables xi. The result
is a probabilistic model that, when optimized, can capture the second-order
statistics of the environment.
[The second-order statistics of an ensemble
P (x) are the expected values hxixji of all the pairwise products xixj.] The
real world, however, often has higher-order correlations that must be included
if our description of it is to be e(cid:11)ective. Often the second-order correlations
in themselves may carry little or no useful information.

Consider, for example, the ensemble of binary images of chairs. We can
imagine images of chairs with various designs { four-legged chairs, comfy
chairs, chairs with (cid:12)ve legs and wheels, wooden chairs, cushioned chairs, chairs
with rockers instead of legs. A child can easily learn to distinguish these images
from images of carrots and parrots. But I expect the second-order statistics of
the raw data are useless for describing the ensemble. Second-order statistics
only capture whether two pixels are likely to be in the same state as each
other. Higher-order concepts are needed to make a good generative model of
images of chairs.

A simpler ensemble of images in which high-order statistics are important
is the ‘shifter ensemble’, which comes in two (cid:13)avours. Figure 43.1a shows a
few samples from the ‘plain shifter ensemble’. In each image, the bottom eight
pixels are a copy of the top eight pixels, either shifted one pixel to the left,
or unshifted, or shifted one pixel to the right. (The top eight pixels are set
at random.) This ensemble is a simple model of the visual signals from the
two eyes arriving at early levels of the brain. The signals from the two eyes
are similar to each other but may di(cid:11)er by small translations because of the
varying depth of the visual world. This ensemble is simple to describe, but its
second-order statistics convey no useful information. The correlation between
one pixel and any of the three pixels above it is 1=3. The correlation between
any other two pixels is zero.

Figure 43.1b shows a few samples from the ‘labelled shifter ensemble’.
Here, the problem has been made easier by including an extra three neu-
rons that label the visual image as being an instance of either the ‘shift left’,
‘no shift’, or ‘shift right’ sub-ensemble. But with this extra information, the
ensemble is still not learnable using second-order statistics alone. The second-
order correlation between any label neuron and any image neuron is zero. We
need models that can capture higher-order statistics of an environment.

So, how can we develop such models? One idea might be to create models

that directly capture higher-order correlations, such as:

P 0(xj W; V; : : :) =

1
Z0

exp0
@

1

2Xij

wijxixj +

1

6Xij

vijkxixjxk + (cid:1)(cid:1)(cid:1)1
A :

(43.13)
Such higher-order Boltzmann machines are equally easy to simulate using
stochastic updates, and the learning rule for the higher-order parameters vijk
is equivalent to the learning rule for wij.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

43.2: Boltzmann machine with hidden units

525

. Exercise 43.2.[2 ] Derive the gradient of the log likelihood with respect to vijk.
It is possible that the spines found on biological neurons are responsible for
detecting correlations between small numbers of incoming signals. However,
to capture statistics of high enough order to describe the ensemble of images
of chairs well would require an unimaginable number of terms. To capture
merely the fourth-order statistics in a 128 (cid:2) 128 pixel image, we need more
than 107 parameters.
So measuring moments of images is not a good way to describe their un-
derlying structure. Perhaps what we need instead or in addition are hidden
variables, also known to statisticians as latent variables. This is the important
innovation introduced by Hinton and Sejnowski (1986). The idea is that the
high-order correlations among the visible variables are described by includ-
ing extra hidden variables and sticking to a model that has only second-order
interactions between its variables; the hidden variables induce higher-order
correlations between the visible variables.

43.2 Boltzmann machine with hidden units

We now add hidden neurons to our stochastic model. These are neurons that
do not correspond to observed variables; they are free to play any role in the
probabilistic model de(cid:12)ned by equation (43.4). They might actually take on
interpretable roles, e(cid:11)ectively performing ‘feature extraction’.

Learning in Boltzmann machines with hidden units

The activity rule of a Boltzmann machine with hidden units is identical to that
of the original Boltzmann machine. The learning rule can again be derived
by maximum likelihood, but now we need to take into account the fact that
the states of the hidden units are unknown. We will denote the states of the
visible units by x, the states of the hidden units by h, and the generic state
of a neuron (either visible or hidden) by yi, with y (cid:17) (x; h). The state of the
network when the visible neurons are clamped in state x(n) is y(n) (cid:17) (x(n); h).
The likelihood of W given a single data example x(n) is

P (x(n) j W) =Xh

where

P (x(n); hj W) =Xh
exp(cid:20) 1

Z(W) =Xx;h

2

yTWy(cid:21) :

Equation (43.14) may also be written

1

Z(W)

exp(cid:20) 1

2

[y(n)]TWy(n)(cid:21) ;

(43.14)

(43.15)

(43.16)

(43.17)

where

P (x(n) j W) =
exp(cid:20) 1

2

Zx(n)(W) =Xh

Zx(n) (W)

Z(W)

[y(n)]TWy(n)(cid:21) :

Di(cid:11)erentiating the likelihood as before, we (cid:12)nd that the derivative with re-
spect to any weight wij is again the di(cid:11)erence between a ‘waking’ term and a
‘sleeping’ term,

@

@wij

ln P (fx(n)gN

1 j W) =Xn nhyiyjiP (h j x(n);W) (cid:0) hyiyjiP (x;hj W)o :

(43.18)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

526

43 | Boltzmann Machines

The (cid:12)rst term hyiyjiP (h j x(n);W) is the correlation between yi and yj if the
Boltzmann machine is simulated with the visible variables clamped to x(n)
and the hidden variables freely sampling from their conditional distribution.
The second term hyiyjiP (x;hj W) is the correlation between yi and yj when
Hinton and Sejnowski demonstrated that non-trivial ensembles such as
the labelled shifter ensemble can be learned using a Boltzmann machine with
hidden units. The hidden units take on the role of feature detectors that spot
patterns likely to be associated with one of the three shifts.

the Boltzmann machine generates samples from its model distribution.

The Boltzmann machine is time-consuming to simulate because the compu-
tation of the gradient of the log likelihood depends on taking the di(cid:11)erence of
two gradients, both found by Monte Carlo methods. So Boltzmann machines
are not in widespread use. It is an area of active research to create models
that embody the same capabilities using more e(cid:14)cient computations (Hinton
et al., 1995; Dayan et al., 1995; Hinton and Ghahramani, 1997; Hinton, 2001;
Hinton and Teh, 2001).

43.3 Exercise

. Exercise 43.3.[3 ] Can the ‘bars and stripes’ ensemble ((cid:12)gure 43.2) be learned
by a Boltzmann machine with no hidden units? [You may be surprised!]

Figure 43.2. Four samples from
the ‘bars and stripes’ ensemble.
Each sample is generated by (cid:12)rst
picking an orientation, horizontal
or vertical; then, for each row of
spins in that orientation (each bar
or stripe respectively), switching
all spins on with probability 1/2.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

44

Supervised Learning in Multilayer

Networks

44.1 Multilayer perceptrons

No course on neural networks could be complete without a discussion of su-
pervised multilayer networks, also known as backpropagation networks.

The multilayer perceptron is a feedforward network. It has input neurons,
hidden neurons and output neurons. The hidden neurons may be arranged
in a sequence of layers. The most common multilayer perceptrons have a
single hidden layer, and are known as ‘two-layer’ networks, the number ‘two’
counting the number of layers of neurons not including the inputs.

Such a feedforward network de(cid:12)nes a nonlinear parameterized mapping
from an input x to an output y = y(x; w;A). The output is a continuous
function of the input and of the parameters w; the architecture of the net, i.e.,
the functional form of the mapping, is denoted by A. Feedforward networks
can be ‘trained’ to perform regression and classi(cid:12)cation tasks.

Regression networks

In the case of a regression problem, the mapping for a network with one hidden
layer may have the form:

Hidden layer:

Output layer:

a(1)

j =Xl
i =Xj

a(2)

jl xl + (cid:18)(1)
w(1)

j

;

hj = f (1)(a(1)
j )

(44.1)

w(2)
ij hj + (cid:18)(2)

i

;

yi = f (2)(a(2)

i

)

(44.2)

where, for example, f (1)(a) = tanh(a), and f (2)(a) = a. Here l runs over
the inputs x1; : : : ; xL, j runs over the hidden units, and i runs over the out-
puts. The ‘weights’ w and ‘biases’ (cid:18) together make up the parameter vector
w. The nonlinear sigmoid function f (1) at the hidden layer gives the neu-
ral network greater computational (cid:13)exibility than a standard linear regression
model. Graphically, we can represent the neural network as a set of layers of
connected neurons ((cid:12)gure 44.1).

What sorts of functions can these networks implement?

Just as we explored the weight space of the single neuron in Chapter 39,
examining the functions it could produce, let us explore the weight space of
a multilayer network.
In (cid:12)gures 44.2 and 44.3 I take a network with one
input and one output and a large number H of hidden units, set the biases

527

Outputs

Hiddens

Inputs

Figure 44.1. A typical two-layer
network, with six inputs, seven
hidden units, and three outputs.
Each line represents one weight.

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-1.2

-1.4

-2

-1

0

1

2

3

4

5

Figure 44.2. Samples from the
prior over functions of a one-input
network. For each of a sequence of
values of (cid:27)bias = 8, 6, 4, 3, 2, 1.6,
1.2, 0.8, 0.4, 0.3, 0.2, and
(cid:27)in = 5(cid:27)w
is shown. The other
hyperparameters of the network
were H = 400, (cid:27)w

bias, one random function

out = 0:05.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

528

(cid:27)bias

Output

(cid:27) (cid:27)out

(cid:0)

@

(cid:0)(cid:0)@

ty
ttttt
t
(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)
tx
t1

@@
(cid:0)(cid:0)
(cid:27) (cid:27)in

@@(cid:0)

(cid:0)
@
-

(cid:0)

@

Hidden layer

Input

44 | Supervised Learning in Multilayer Networks

10

5

0

-5

-10

t

u
p

t

u
O

pH(cid:27)out

(cid:24) 1=(cid:27)in

(cid:24) (cid:27)bias=(cid:27)in

-2

-1

0

1

Input

2

3

4

Figure 44.3. Properties of a
function produced by a random
network. The vertical scale of a
typical function produced by the
network with random weights is of
order pH(cid:27)out; the horizontal
range in which the function varies
signi(cid:12)cantly is of order (cid:27)bias=(cid:27)in;
and the shortest horizontal length
scale is of order 1=(cid:27)in. The
function shown was produced by
making a random network with
H = 400 hidden units, and
Gaussian weights with (cid:27)bias = 4,
(cid:27)in = 8, and (cid:27)out = 0:5.

i

j

jl , (cid:18)(2)

and w(2)
ij

to random values, and plot the resulting

, w(1)
I set the hidden units’ biases (cid:18)(1)

and weights (cid:18)(1)
j
function y(x).
to random values from a
Gaussian with zero mean and standard deviation (cid:27)bias; the input-to-hidden
weights w(1)
to random values with standard deviation (cid:27)in; and the bias and
jl
output weights (cid:18)(2)

to random values with standard deviation (cid:27)out.
The sort of functions that we obtain depend on the values of (cid:27)bias, (cid:27)in
and (cid:27)out. As the weights and biases are made bigger we obtain more complex
functions with more features and a greater sensitivity to the input variable.
The vertical scale of a typical function produced by the network with random
weights is of order pH(cid:27)out; the horizontal range in which the function varies
signi(cid:12)cantly is of order (cid:27)bias=(cid:27)in; and the shortest horizontal length scale is of
order 1=(cid:27)in.

and w(2)
ij

i

Radford Neal (1996) has also shown that in the limit as H ! 1 the
statistical properties of the functions generated by randomizing the weights are
independent of the number of hidden units; so, interestingly, the complexity of
the functions becomes independent of the number of parameters in the model.
What determines the complexity of the typical functions is the characteristic
magnitude of the weights. Thus we anticipate that when we (cid:12)t these models to
real data, an important way of controlling the complexity of the (cid:12)tted function
will be to control the characteristic magnitude of the weights.

Figure 44.4 shows one typical function produced by a network with two
inputs and one output. This should be contrasted with the function produced
by a traditional linear regression model, which is a (cid:13)at plane. Neural networks
can create functions with more complexity than a linear regression.

44.2 How a regression network is traditionally trained

This network is trained using a data set D = fx(n); t(n)g by adjusting w so as
to minimize an error function, e.g.,

ED(w) =

1

2Xn Xi (cid:16)t(n)

i (cid:0) yi(x(n); w)(cid:17)2

:

(44.3)

This objective function is a sum of terms, one for each input/target pair fx; tg,
measuring how close the output y(x; w) is to the target t.
This minimization is based on repeated evaluation of the gradient of ED.
This gradient can be e(cid:14)ciently computed using the backpropagation algorithm
(Rumelhart et al., 1986), which uses the chain rule to (cid:12)nd the derivatives.

1

0

-1

-2

-1

-0.5

0

0.5

1

-1

1

0.5

0
-0.5

Figure 44.4. One sample from the
prior of a two-input network with
fH; (cid:27)w
outg =
f400; 8:0; 8:0; 0:05g.

bias; (cid:27)w

in; (cid:27)w

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

44.3: Neural network learning as inference

529

Often, regularization (also known as weight decay) is included, modifying

the objective function to:

M (w) = (cid:12)ED + (cid:11)EW

(44.4)

where, for example, EW = 1
i . This additional term favours small values
of w and decreases the tendency of a model to over(cid:12)t noise in the training
data.

2Pi w2

Rumelhart et al. (1986) showed that multilayer perceptrons can be trained,
by gradient descent on M (w), to discover solutions to non-trivial problems
such as deciding whether an image is symmetric or not. These networks have
been successfully applied to real-world tasks as varied as pronouncing English
text (Sejnowski and Rosenberg, 1987) and focussing multiple-mirror telescopes
(Angel et al., 1990).

44.3 Neural network learning as inference

The neural network learning process above can be given the following proba-
bilistic interpretation. [Here we repeat and generalize the discussion of Chap-
ter 41.]

The error function is interpreted as de(cid:12)ning a noise model. (cid:12)ED is the

negative log likelihood:

P (D j w; (cid:12);H) =

1

ZD((cid:12))

exp((cid:0)(cid:12)ED):

(44.5)

Thus, the use of the sum-squared error ED (44.3) corresponds to an assump-
tion of Gaussian noise on the target variables, and the parameter (cid:12) de(cid:12)nes a
noise level (cid:27)2

(cid:23) = 1=(cid:12).

Similarly the regularizer is interpreted in terms of a log prior probability

distribution over the parameters:

P (w j (cid:11);H) =

1

ZW ((cid:11))

exp((cid:0)(cid:11)EW ):

(44.6)

If EW is quadratic as de(cid:12)ned above, then the corresponding prior distribution
is a Gaussian with variance (cid:27)2
W = 1=(cid:11). The probabilistic model H speci(cid:12)es
the architecture A of the network, the likelihood (44.5), and the prior (44.6).
The objective function M (w) then corresponds to the inference of the

parameters w, given the data:

P (w j D; (cid:11); (cid:12);H) =

P (D j w; (cid:12);H)P (w j (cid:11);H)

P (D j (cid:11); (cid:12);H)
exp((cid:0)M (w)):

=

1
ZM

(44.7)

(44.8)

The w found by (locally) minimizing M (w) is then interpreted as the (locally)
most probable parameter vector, wMP.

The interpretation of M (w) as a log probability adds little new at this
stage. But new tools will emerge when we proceed to other inferences. First,
though, let us establish the probabilistic interpretation of classi(cid:12)cation net-
works, to which the same tools apply.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

530

44 | Supervised Learning in Multilayer Networks

Binary classi(cid:12)cation networks

If the targets t in a data set are binary classi(cid:12)cation labels (0; 1), it is natural
to use a neural network whose output y(x; w;A) is bounded between 0 and 1,
and is interpreted as a probability P (t = 1 j x; w;A). For example, a network
with one hidden layer could be described by the feedforward equations (44.1)
and (44.2), with f (2)(a) = 1=(1 + e(cid:0)a). The error function (cid:12)ED is replaced by
the negative log likelihood:

G(w) = (cid:0)"Xn

t(n) ln y(x(n); w) + (1 (cid:0) t(n)) ln(1 (cid:0) y(x(n); w))# :

(44.9)

The total objective function is then M = G + (cid:11)EW . Note that this includes
no parameter (cid:12) (because there is no Gaussian noise).

Multi-class classi(cid:12)cation networks

For a multi-class classi(cid:12)cation problem, we can represent the targets by a
vector, t, in which a single element is set to 1, indicating the correct class, and
all other elements are set to 0. In this case it is appropriate to use a ‘softmax’
network having coupled outputs which sum to one and are interpreted as
class probabilities yi = P (ti = 1 j x; w;A). The last part of equation (44.2) is
replaced by:

eai

eai0

Xi0

yi =

:

(44.10)

The negative log likelihood in this case is

G(w) = (cid:0)Xn Xi

t(n)
i

ln yi(x(n); w):

(44.11)

As in the case of the regression network, the minimization of the objective
function M (w) = G + (cid:11)EW corresponds to an inference of the form (44.8). A
variety of useful results can be built on this interpretation.

44.4 Bene(cid:12)ts of the Bayesian approach to supervised feedforward

neural networks

From the statistical perspective, supervised neural networks are nothing more
than nonlinear curve-(cid:12)tting devices. Curve (cid:12)tting is not a trivial task however.
The e(cid:11)ective complexity of an interpolating model is of crucial importance,
as illustrated in (cid:12)gure 44.5. Consider a control parameter that in(cid:13)uences the
complexity of a model, for example a regularization constant (cid:11) (weight decay
parameter). As the control parameter is varied to increase the complexity of
the model (descending from (cid:12)gure 44.5a{c and going from left to right across
(cid:12)gure 44.5d), the best (cid:12)t to the training data that the model can achieve
becomes increasingly good. However, the empirical performance of the model,
the test error, (cid:12)rst decreases then increases again. An over-complex model
over(cid:12)ts the data and generalizes poorly. This problem may also complicate
the choice of architecture in a multilayer perceptron, the radius of the basis
functions in a radial basis function network, and the choice of the input vari-
ables themselves in any multidimensional regression problem. Finding values
for model control parameters that are appropriate for the data is therefore an
important and non-trivial problem.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

44.4: Bene(cid:12)ts of the Bayesian approach to supervised feedforward neural networks

531

Figure 44.5. Optimization of
model complexity. Panels (a{c)
show a radial basis function model
interpolating a simple data set
with one input variable and one
output variable. As the
regularization constant is varied
to increase the complexity of the
model (from (a) to (c)), the
interpolant is able to (cid:12)t the
training data increasingly well,
but beyond a certain point the
generalization ability (test error)
of the model deteriorates.
Probability theory allows us to
optimize the control parameters
without needing a test set.

Test Error

Training Error

(d)

Model Control Parameters

Log Probability(Training Data | Control Parameters)

(e)

Model Control Parameters

(a)

(b)

(c)

The over(cid:12)tting problem can be solved by using a Bayesian approach to

control model complexity.

If we give a probabilistic interpretation to the model, then we can evaluate
the evidence for alternative values of the control parameters. As was explained
in Chapter 28, over-complex models turn out to be less probable, and the
evidence P (Data j Control Parameters) can be used as an objective function
for optimization of model control parameters ((cid:12)gure 44.5e). The setting of (cid:11)
that maximizes the evidence is displayed in (cid:12)gure 44.5b.

Bayesian optimization of model control parameters has four important ad-
vantages. (1) No ‘test set’ or ‘validation set’ is involved, so all available training
data can be devoted to both model (cid:12)tting and model comparison. (2) Reg-
ularization constants can be optimized on-line, i.e., simultaneously with the
optimization of ordinary model parameters. (3) The Bayesian objective func-
tion is not noisy, in contrast to a cross-validation measure. (4) The gradient of
the evidence with respect to the control parameters can be evaluated, making
it possible to simultaneously optimize a large number of control parameters.
Probabilistic modelling also handles uncertainty in a natural manner. It
o(cid:11)ers a unique prescription, marginalization, for incorporating uncertainty
about parameters into predictions; this procedure yields better predictions, as
we saw in Chapter 41. Figure 44.6 shows error bars on the predictions of a
trained neural network.

Implementation of Bayesian inference

As was mentioned in Chapter 41, Bayesian inference for multilayer networks
may be implemented by Monte Carlo sampling, or by deterministic methods
employing Gaussian approximations (Neal, 1996; MacKay, 1992c).

Figure 44.6. Error bars on the
predictions of a trained regression
network. The solid line gives the
predictions of the best-(cid:12)t
parameters of a multilayer
perceptron trained on the data
points. The error bars (dotted
lines) are those produced by the
uncertainty of the parameters w.
Notice that the error bars become
larger where the data are sparse.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

532

44 | Supervised Learning in Multilayer Networks

Within the Bayesian framework for data modelling, it is easy to improve
our probabilistic models. For example, if we believe that some input variables
in a problem may be irrelevant to the predicted quantity, but we don’t know
which, we can de(cid:12)ne a new model with multiple hyperparameters that captures
the idea of uncertain input variable relevance (MacKay, 1994b; Neal, 1996;
MacKay, 1995b); these models then infer automatically from the data which
are the relevant input variables for a problem.

44.5 Exercises

Exercise 44.1.[4 ] How to measure a classi(cid:12)er’s quality. You’ve just written a new
classi(cid:12)cation algorithm and want to measure how well it performs on a test
set, and compare it with other classi(cid:12)ers. What performance measure should
you use? There are several standard answers. Let’s assume the classi(cid:12)er gives
an output y(x), where x is the input, which we won’t discuss further, and that
the true target value is t. In the simplest discussions of classi(cid:12)ers, both y and
t are binary variables, but you might care to consider cases where y and t are
more general objects also.

The most widely used measure of performance on a test set is the error
rate { the fraction of misclassi(cid:12)cations made by the classi(cid:12)er. This measure
forces the classi(cid:12)er to give a 0/1 output and ignores any additional information
that the classi(cid:12)er might be able to o(cid:11)er { for example, an indication of the
(cid:12)rmness of a prediction. Unfortunately, the error rate does not necessarily
measure how informative a classi(cid:12)er’s output is. Consider frequency tables
showing the joint frequency of the 0/1 output of a classi(cid:12)er (horizontal axis),
and the true 0/1 variable (vertical axis). The numbers that we’ll show are
percentages. The error rate e is the sum of the two o(cid:11)-diagonal numbers,
which we could call the false positive rate e+ and the false negative rate e(cid:0).
Of the following three classi(cid:12)ers, A and B have the same error rate of 10%

and C has a greater error rate of 12%.

Classi(cid:12)er A

y

0

t
0
1

90
10

1

0
0

Classi(cid:12)er B

y

0

1

Classi(cid:12)er C

y

0

1

t
0
1

80
0

10
10

t
0
1

78
0

12
10

But clearly classi(cid:12)er A, which simply guesses that the outcome is 0 for all
cases, is conveying no information at all about t; whereas classi(cid:12)er B has an
informative output: if y = 0 then we are sure that t really is zero; and if y = 1
then there is a 50% chance that t = 1, as compared to the prior probability
P (t = 1) = 0:1. Classi(cid:12)er C is slightly less informative than B, but it is still
much more useful than the information-free classi(cid:12)er A.

One way to improve on the error rate as a performance measure is to report
the pair (e+; e(cid:0)), the false positive error rate and the false negative error rate,
which are (0; 0:1) and (0:1; 0) for classi(cid:12)ers A and B. It is especially important
to distinguish between these two error probabilities in applications where the
two sorts of error have di(cid:11)erent associated costs. However, there are a couple
of problems with the ‘error rate pair’:

(cid:15) First, if I simply told you that classi(cid:12)er A has error rates (0; 0:1) and B
has error rates (0:1; 0), it would not be immediately evident that classi(cid:12)er
A is actually utterly worthless. Surely we should have a performance
measure that gives the worst possible score to A!

How common sense ranks the

classi(cid:12)ers:

(best) B > C > A (worst).

How error rate ranks the

classi(cid:12)ers:

(best) A = B > C (worst).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

44.5: Exercises

533

(cid:15) Second, if we turn to a multiple-class classi(cid:12)cation problem such as digit
recognition, then the number of types of error increases from two to
10 (cid:2) 9 = 90 { one for each possible confusion of class t with t0. It would
be nice to have some sensible way of collapsing these 90 numbers into a
single rankable number that makes more sense than the error rate.

Another reason for not liking the error rate is that it doesn’t give a classi(cid:12)er
credit for accurately specifying its uncertainty. Consider classi(cid:12)ers that have
three outputs available, ‘0’, ‘1’ and a rejection class, ‘?’, which indicates that
the classi(cid:12)er is not sure. Consider classi(cid:12)ers D and E with the following
frequency tables, in percentages:

Classi(cid:12)er D

y

0

?

t
0
1

74
0

10
1

1

6
9

Classi(cid:12)er E

y

0

t
0
1

78
0

?

6
5

1

6
5

Both of these classi(cid:12)ers have (e+; e(cid:0); r) = (6%; 0%; 11%). But are they equally
good classi(cid:12)ers? Compare classi(cid:12)er E with C. The two classi(cid:12)ers are equiva-
lent. E is just C in disguise { we could make E by taking the output of C and
tossing a coin when C says ‘1’ in order to decide whether to give output ‘1’ or
‘?’. So E is equal to C and thus inferior to B. Now compare D with B. Can
you justify the suggestion that D is a more informative classi(cid:12)er than B, and
thus is superior to E? Yet D and E have the same (e+; e(cid:0); r) scores.

People often plot error-reject curves (also known as ROC curves; ROC
stands for ‘receiver operating characteristic’) which show the total e = (e+ +
e(cid:0)) versus r as r is allowed to vary from 0 to 1, and use these curves to
compare classi(cid:12)ers ((cid:12)gure 44.7).
[In the special case of binary classi(cid:12)cation
problems, e+ may be plotted versus e(cid:0) instead.] But as we have seen, error
rates can be undiscerning performance measures. Does plotting one error rate
as a function of another make this weakness of error rates go away?

For this exercise, either construct an explicit example demonstrating that
the error-reject curve, and the area under it, are not necessarily good ways to
compare classi(cid:12)ers; or prove that they are.

As a suggested alternative method for comparing classi(cid:12)ers, consider the

mutual information between the output and the target,

I(T ; Y ) (cid:17) H(T ) (cid:0) H(T j Y ) =Xy;t

P (y)P (tj y) log

P (t)
P (tj y)

;

(44.12)

which measures how many bits the classi(cid:12)er’s output conveys about the target.

Evaluate the mutual information for classi(cid:12)ers A{E above.
Investigate this performance measure and discuss whether it is a useful

one. Does it have practical drawbacks?

Error rate

Rejection rate

Figure 44.7. An error-reject curve.
Some people use the area under
this curve as a measure of
classi(cid:12)er quality.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 45

Feedforward neural networks such as multilayer perceptrons are popular tools
for nonlinear regression and classi(cid:12)cation problems. From a Bayesian per-
spective, a choice of a neural network model can be viewed as de(cid:12)ning a prior
probability distribution over nonlinear functions, and the neural network’s
learning process can be interpreted in terms of the posterior probability dis-
tribution over the unknown function. (Some learning algorithms search for the
function with maximum posterior probability and other Monte Carlo methods
draw samples from this posterior probability.)

In the limit of large but otherwise standard networks, Neal (1996) has
shown that the prior distribution over nonlinear functions implied by the
Bayesian neural network falls in a class of probability distributions known
as Gaussian processes. The hyperparameters of the neural network model
determine the characteristic lengthscales of the Gaussian process. Neal’s ob-
servation motivates the idea of discarding parameterized networks and working
directly with Gaussian processes. Computations in which the parameters of
the network are optimized are then replaced by simple matrix operations using
the covariance matrix of the Gaussian process.

In this chapter I will review work on this idea by Williams and Rasmussen
(1996), Neal (1997b), Barber and Williams (1997) and Gibbs and MacKay
(2000), and will assess whether, for supervised regression and classi(cid:12)cation
tasks, the feedforward network has been superceded.

. Exercise 45.1.[3 ] I regret that this chapter is rather dry. There’s no simple
explanatory examples in it, and few pictures. This exercise asks you to
create interesting pictures to explain to yourself this chapter’s ideas.

Source code for computer demonstrations written in the free language

octave is available at:
http://www.inference.phy.cam.ac.uk/mackay/itprnn/software.html.

Radford Neal’s software for Gaussian processes is available at:

http://www.cs.toronto.edu/~radford/.

534

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45

Gaussian Processes

After the publication of Rumelhart, Hinton and Williams’s (1986) paper on
supervised learning in neural networks there was a surge of interest in the
empirical modelling of relationships in high-dimensional data using nonlinear
parametric models such as multilayer perceptrons and radial basis functions.
In the Bayesian interpretation of these modelling methods, a nonlinear func-
tion y(x) parameterized by parameters w is assumed to underlie the data
fx(n); tngN
n=1, and the adaptation of the model to the data corresponds to an
inference of the function given the data. We will denote the set of input vectors
by XN (cid:17) fx(n)gN
n=1 and the set of corresponding target values by the vector
tN (cid:17) ftngN
n=1. The inference of y(x) is described by the posterior probability
distribution

P (y(x)j tN ; XN ) =

P (tN j y(x); XN )P (y(x))

P (tN j XN )

:

(45.1)

Of the two terms on the right-hand side, the (cid:12)rst, P (tN j y(x); XN ), is the
probability of the target values given the function y(x), which in the case of
regression problems is often assumed to be a separable Gaussian distribution;
and the second term, P (y(x)), is the prior distribution on functions assumed
by the model. This prior is implicit in the choice of parametric model and
the choice of regularizers used during the model (cid:12)tting. The prior typically
speci(cid:12)es that the function y(x) is expected to be continuous and smooth,
and has less high frequency power than low frequency power, but the precise
meaning of the prior is somewhat obscured by the use of the parametric model.
Now, for the prediction of future values of t, all that matters is the as-
sumed prior P (y(x)) and the assumed noise model P (tN j y(x); XN ) { the
parameterization of the function y(x; w) is irrelevant.
The idea of Gaussian process modelling is to place a prior P (y(x)) directly
on the space of functions, without parameterizing y(x). The simplest type of
prior over functions is called a Gaussian process. It can be thought of as the
generalization of a Gaussian distribution over a (cid:12)nite vector space to a function
space of in(cid:12)nite dimension. Just as a Gaussian distribution is fully speci(cid:12)ed
by its mean and covariance matrix, a Gaussian process is speci(cid:12)ed by a mean
and a covariance function. Here, the mean is a function of x (which we will
often take to be the zero function), and the covariance is a function C(x; x0)
that expresses the expected covariance between the values of the function y
at the points x and x0. The function y(x) in any one data modelling problem
is assumed to be a single sample from this Gaussian distribution. Gaussian
processes are already well established models for various spatial and temporal
problems { for example, Brownian motion, Langevin processes and Wiener
processes are all examples of Gaussian processes; Kalman (cid:12)lters, widely used

535

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

536

45 | Gaussian Processes

to model speech waveforms, also correspond to Gaussian process models; the
method of ‘kriging’ in geostatistics is a Gaussian process regression method.

Reservations about Gaussian processes

It might be thought that it is not possible to reproduce the interesting prop-
erties of neural network interpolation methods with something so simple as a
Gaussian distribution, but as we shall now see, many popular nonlinear inter-
polation methods are equivalent to particular Gaussian processes. (I use the
term ‘interpolation’ to cover both the problem of ‘regression’ { (cid:12)tting a curve
through noisy data { and the task of (cid:12)tting an interpolant that passes exactly
through the given data points.)

It might also be thought that the computational complexity of inference
when we work with priors over in(cid:12)nite-dimensional function spaces might be
in(cid:12)nitely large. But by concentrating on the joint probability distribution of
the observed data and the quantities we wish to predict, it is possible to make
predictions with resources that scale as polynomial functions of N , the number
of data points.

45.1 Standard methods for nonlinear regression

The problem
We are given N data points XN ; tN = fx(n); tngN
n=1. The inputs x are vec-
tors of some (cid:12)xed input dimension I. The targets t are either real numbers,
in which case the task will be a regression or interpolation task, or they are
categorical variables, for example t 2 f0; 1g, in which case the task is a clas-
si(cid:12)cation task. We will concentrate on the case of regression for the time
being.

Assuming that a function y(x) underlies the observed data, the task is to
infer the function from the given data, and predict the function’s value { or
the value of the observation tN +1 { at a new point x(N +1).

Parametric approaches to the problem

In a parametric approach to regression we express the unknown function y(x)
in terms of a nonlinear function y(x; w) parameterized by parameters w.
Example 45.2. Fixed basis functions. Using a set of basis functions f(cid:30)h(x)gH

h=1,

we can write

H

y(x; w) =

wh(cid:30)h(x):

Xh=1
If the basis functions are nonlinear functions of x such as radial basis
functions centred at (cid:12)xed points fchgH
(cid:30)h(x) = exp(cid:20)(cid:0)

h=1,
(x (cid:0) ch)2

(cid:21) ;

(45.2)

2r2

(45.3)

then y(x; w) is a nonlinear function of x; however, since the dependence
of y on the parameters w is linear, we might sometimes refer to this as
a ‘linear’ model. In neural network terms, this model is like a multilayer
network whose connections from the input layer to the nonlinear hidden
layer are (cid:12)xed; only the output weights w are adaptive.

Other possible sets of (cid:12)xed basis functions include polynomials such as
(cid:30)h(x) = xp

j where p and q are integer powers that depend on h.

i xq

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45.1: Standard methods for nonlinear regression

537

Example 45.3. Adaptive basis functions. Alternatively, we might make a func-
tion y(x) from basis functions that depend on additional parameters
included in the vector w.
In a two-layer feedforward neural network
with nonlinear hidden units and a linear output, the function can be
written

y(x; w) =

H

Xh=1

w(2)

h tanh  I
Xi=1

w(1)
hi xi + w(1)

h0! + w(2)

0

(45.4)

where I is the dimensionality of the input space and the weight vector
hi g, the hidden unit biases fw(1)
h0 g,
0 . In this model, the

w consists of the input weights fw(1)
the output weights fw(2)

h g and the output bias w(2)

dependence of y on w is nonlinear.

Having chosen the parameterization, we then infer the function y(x; w) by

inferring the parameters w. The posterior probability of the parameters is

P (w j tN ; XN ) =

P (tN j w; XN )P (w)

P (tN j XN )

:

(45.5)

The factor P (tN j w; XN ) states the probability of the observed data points
when the parameters w (and hence, the function y) are known. This proba-
bility distribution is often taken to be a separable Gaussian, each data point
tn di(cid:11)ering from the underlying value y(x(n); w) by additive noise. The factor
P (w) speci(cid:12)es the prior probability distribution of the parameters. This too
is often taken to be a separable Gaussian distribution. If the dependence of y
on w is nonlinear the posterior distribution P (w j tN ; XN ) is in general not a
Gaussian distribution.

The inference can be implemented in various ways. In the Laplace method,

we minimize an objective function

M (w) = (cid:0) ln [P (tN j w; XN )P (w)]

(45.6)

with respect to w, locating the locally most probable parameters, then use the
curvature of M , @2M (w)=@wi@wj, to de(cid:12)ne error bars on w. Alternatively we
can use more general Markov chain Monte Carlo techniques to create samples
from the posterior distribution P (w j tN ; XN ).

Having obtained one of these representations of the inference of w given

the data, predictions are then made by marginalizing over the parameters:

P (tN +1 j tN ; XN +1) =Z dHw P (tN +1 j w; x(N +1))P (w j tN ; XN ):
If we have a Gaussian representation of the posterior P (w j tN ; XN ), then this
integral can typically be evaluated directly. In the alternative Monte Carlo
approach, which generates R samples w(r) that are intended to be samples
from the posterior distribution P (w j tN ; XN ), we approximate the predictive
distribution by

(45.7)

P (tN +1 j tN ; XN +1) ’

1
R

R

Xr=1

P (tN +1 j w(r); x(N +1)):

(45.8)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45 | Gaussian Processes

538

Nonparametric approaches.

In nonparametric methods, predictions are obtained without explicitly pa-
rameterizing the unknown function y(x); y(x) lives in the in(cid:12)nite-dimensional
space of all continuous functions of x. One well known nonparametric ap-
proach to the regression problem is the spline smoothing method (Kimeldorf
and Wahba, 1970). A spline solution to a one-dimensional regression problem
can be described as follows: we de(cid:12)ne the estimator of y(x) to be the function
^y(x) that minimizes the functional

M (y(x)) =

1
2

(cid:12)

N

Xn=1

(y(x(n)) (cid:0) tn)2 +

1
2

(cid:11)Z dx [y(p)(x)]2;

(45.9)

where y(p) is the pth derivative of y and p is a positive number. If p is set to
2 then the resulting function ^y(x) is a cubic spline, that is, a piecewise cubic
function that has ‘knots’ { discontinuities in its second derivative { at the data
points fx(n)g.
tifying the prior for the function y(x) as:

This estimation method can be interpreted as a Bayesian method by iden-

ln P (y(x)j (cid:11)) = (cid:0)

(cid:11)Z dx [y(p)(x)]2 + const;
and the probability of the data measurements tN = ftngN
pendent Gaussian noise as:

1
2

(45.10)

n=1 assuming inde-

ln P ( tN j y(x); (cid:12)) = (cid:0)

1
2

(cid:12)

N

Xn=1

(y(x(n)) (cid:0) tn)2 + const:

(45.11)

[The constants in equations (45.10) and (45.11) are functions of (cid:11) and (cid:12) re-
spectively. Strictly the prior (45.10) is improper since addition of an arbitrary
polynomial of degree (p (cid:0) 1) to y(x) is not constrained. This impropriety is
easily recti(cid:12)ed by the addition of (p(cid:0) 1) appropriate terms to (45.10).] Given
this interpretation of the functions in equation (45.9), M (y(x)) is equal to mi-
nus the log of the posterior probability P (y(x)j tN ; (cid:11); (cid:12)), within an additive
constant, and the splines estimation procedure can be interpreted as yielding
a Bayesian MAP estimate. The Bayesian perspective allows us additionally
to put error bars on the splines estimate and to draw typical samples from
the posterior distribution, and it gives an automatic method for inferring the
hyperparameters (cid:11) and (cid:12).

Comments

Splines priors are Gaussian processes

The prior distribution de(cid:12)ned in equation (45.10) is our (cid:12)rst example of a
Gaussian process. Throwing mathematical precision to the winds, a Gaussian
process can be de(cid:12)ned as a probability distribution on a space of functions
y(x) that can be written in the form

P (y(x)j (cid:22)(x); A) =

1
Z

exp(cid:20)(cid:0)

1
2

(y(x) (cid:0) (cid:22)(x))TA(y(x) (cid:0) (cid:22)(x))(cid:21) ;

(45.12)

where (cid:22)(x) is the mean function and A is a linear operator, and where the inner

product of two functions y(x)Tz(x) is de(cid:12)ned by, for example, R dx y(x)z(x).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45.1: Standard methods for nonlinear regression

539

Here, if we denote by D the linear operator that maps y(x) to the derivative
of y(x), we can write equation (45.10) as

ln P (y(x)j (cid:11)) = (cid:0)

1
2

(cid:11)Z dx [Dpy(x)]2 + const = (cid:0)

1
2

y(x)TAy(x) + const;

(45.13)
which has the same form as equation (45.12) with (cid:22)(x) = 0, and A (cid:17) [D p]TDp.
In order for the prior in equation (45.12) to be a proper prior, A must be a
positive de(cid:12)nite operator, i.e., one satisfying y(x)TAy(x) > 0 for all functions
y(x) other than y(x) = 0.

Splines can be written as parametric models

Splines may be written in terms of an in(cid:12)nite set of (cid:12)xed basis functions, as in
equation (45.2), as follows. First rescale the x axis so that the interval (0; 2(cid:25))
is much wider than the range of x values of interest. Let the basis functions
be a Fourier set fcos hx; sin hx, h = 0; 1; 2; : : :g, so the function is

wh(cos) cos(hx) +

wh(sin) sin(hx):

y(x) =

Use the regularizer

1Xh=0

EW (w) =

1
2

h

1Xh=0

to de(cid:12)ne a Gaussian prior on w,

1Xh=1
1Xh=1

p

2 w2

h(cos) +

1
2

h

p

2 w2

h(sin)

(45.14)

(45.15)

(45.16)

P (w j (cid:11)) =

1

ZW ((cid:11))

exp((cid:0)(cid:11)EW ):

If p = 2 then we have the cubic splines regularizer EW (w) =R y(2)(x)2 dx, as
if p = 1 we have the regularizer EW (w) =R y(1)(x)2 dx,

in equation (45.9);
(To make the prior proper we must add an extra regularizer on the
etc.
term w0(cos).) Thus in terms of the prior P (y(x)) there is no fundamental
di(cid:11)erence between the ‘nonparametric’ splines approach and other parametric
approaches.

Representation is irrelevant for prediction

From the point of view of prediction at least, there are two objects of inter-
est. The (cid:12)rst is the conditional distribution P (tN +1 j tN ; XN +1) de(cid:12)ned in
equation (45.7). The other object of interest, should we wish to compare one
model with others, is the joint probability of all the observed data given the
model, the evidence P (tN j XN ), which appeared as the normalizing constant
in equation (45.5). Neither of these quantities makes any reference to the rep-
resentation of the unknown function y(x). So at the end of the day, our choice
of representation is irrelevant.

The question we now address is, in the case of popular parametric models,
what form do these two quantities take? We will see that for standard models
with (cid:12)xed basis functions and Gaussian distributions on the unknown parame-
ters, the joint probability of all the observed data given the model, P (tN j XN ),
is a multivariate Gaussian distribution with mean zero and with a covariance
matrix determined by the basis functions; this implies that the conditional
distribution P (tN +1 j tN ; XN +1) is also a Gaussian distribution, whose mean
depends linearly on the values of the targets tN . Standard parametric models
are simple examples of Gaussian processes.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

540

45 | Gaussian Processes

45.2 From parametric models to Gaussian processes

Linear models

Let us consider a regression problem using H (cid:12)xed basis functions, for example
one-dimensional radial basis functions as de(cid:12)ned in equation (45.3).

Let us assume that a list of N input points fx(n)g has been speci(cid:12)ed and
de(cid:12)ne the N (cid:2) H matrix R to be the matrix of values of the basis functions
f(cid:30)h(x)gH

h=1 at the points fxng,

Rnh (cid:17) (cid:30)h(x(n)):

(45.17)

We de(cid:12)ne the vector yN to be the vector of values of y(x) at the N points,

yn (cid:17)Xh

Rnhwh:

(45.18)

If the prior distribution of w is Gaussian with zero mean,

P (w) = Normal(w; 0; (cid:27)2

wI);

(45.19)

then y, being a linear function of w, is also Gaussian distributed, with mean
zero. The covariance matrix of y is

Q = hyyTi = hRwwTRTi = RhwwTi RT

= (cid:27)2

wRRT:

(45.20)

(45.21)

So the prior distribution of y is:

P (y) = Normal(y; 0; Q) = Normal(y; 0; (cid:27)2

wRRT):

(45.22)

This result, that the vector of N function values y has a Gaussian distribu-
tion, is true for any selected points XN . This is the de(cid:12)ning property of a
Gaussian process. The probability distribution of a function y(x) is a Gaus-
sian process if for any (cid:12)nite selection of points x(1); x(2); : : : ; x(N ), the density
P (y(x(1)); y(x(2)); : : : ; y(x(N ))) is a Gaussian.

Now, if the number of basis functions H is smaller than the number of
data points N , then the matrix Q will not have full rank. In this case the
probability distribution of y might be thought of as a (cid:13)at elliptical pancake
con(cid:12)ned to an H-dimensional subspace in the N -dimensional space in which
y lives.

What about the target values? If each target tn is assumed to di(cid:11)er by
(cid:23) from the corresponding function value

additive Gaussian noise of variance (cid:27)2
yn then t also has a Gaussian prior distribution,

P (t) = Normal(t; 0; Q + (cid:27)2

(cid:23)I):

We will denote the covariance matrix of t by C:

C = Q + (cid:27)2

(cid:23)I = (cid:27)2

wRRT + (cid:27)2

(cid:23)I:

(45.23)

(45.24)

Whether or not Q has full rank, the covariance matrix C has full rank since
(cid:27)2
(cid:23)I is full rank.

What does the covariance matrix Q look like? In general, the (n; n0) entry

of Q is

Qnn0 = [(cid:27)2

wRRT]nn0 = (cid:27)2

wXh

(cid:30)h(x(n))(cid:30)h(x(n0))

(45.25)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45.2: From parametric models to Gaussian processes

541

and the (n; n0) entry of C is

Cnn0 = (cid:27)2

wXh

(cid:30)h(x(n))(cid:30)h(x(n0)) + (cid:14)nn0(cid:27)2
(cid:23);

(45.26)

where (cid:14)nn0 = 1 if n = n0 and 0 otherwise.

Example 45.4. Let’s take as an example a one-dimensional case, with radial
basis functions. The expression for Qnn0 becomes simplest if we assume we
have uniformly-spaced basis functions with the basis function labelled h cen-
tred on the point x = h, and take the limit H ! 1, so that the sum over
h becomes an integral; to avoid having a covariance that diverges with H,
we had better make (cid:27)2
w scale as S=((cid:1)H), where (cid:1)H is the number of basis
functions per unit length of the x-axis, and S is a constant; then

Qnn0 = SZ hmax
= SZ hmax

hmin

hmin

# :
If we let the limits of integration be (cid:6)1, we can solve this integral:

dh exp"(cid:0)

# exp"(cid:0)

(x(n0) (cid:0) h)2

2r2

dh (cid:30)h(x(n))(cid:30)h(x(n0))

(x(n) (cid:0) h)2

2r2

Qnn0 = p(cid:25)r2 S exp"(cid:0)

(x(n0) (cid:0) x(n))2

4r2

# :

(45.27)

(45.28)

(45.29)

We are arriving at a new perspective on the interpolation problem. Instead of
specifying the prior distribution on functions in terms of basis functions and
priors on parameters, the prior can be summarized simply by a covariance
function,

C(x(n); x(n0)) (cid:17) (cid:18)1 exp"(cid:0)

(x(n0) (cid:0) x(n))2

4r2

# ;

(45.30)

where we have given a new name, (cid:18)1, to the constant out front.

Generalizing from this particular case, a vista of interpolation methods
opens up. Given any valid covariance function C(x; x0) { we’ll discuss in
a moment what ‘valid’ means { we can de(cid:12)ne the covariance matrix for N
function values at locations XN to be the matrix Q given by

Qnn0 = C(x(n); x(n0))

(45.31)

and the covariance matrix for N corresponding target values, assuming Gaus-
sian noise, to be the matrix C given by

Cnn0 = C(x(n); x(n0)) + (cid:27)2

(cid:23)(cid:14)nn0:

(45.32)

In conclusion, the prior probability of the N target values t in the data set is:

P (t) = Normal(t; 0; C) = 1
Z

e(cid:0) 1

2

tTC(cid:0)1t:

(45.33)

Samples from this Gaussian process and a few other simple Gaussian processes
are displayed in (cid:12)gure 45.1.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

542

3.0

2.0

1.0

0.0

t

−1.0

−2.0

−3.0

4.0

2.0

t

0.0

−2.0

−4.0

−3.0

−1.0

1.0

3.0

5.0

x

2(1:5)2 (cid:17)
(a) 2 exp(cid:16)(cid:0) (x(cid:0)x0)2

−1.0

1.0

3.0

5.0

4.0

2.0

t

0.0

−2.0

−4.0

−3.0

6.0

4.0

2.0

0.0

t

−2.0

−4.0

−3.0

−1.0

1.0

3.0

5.0

x

(b) 2 exp(cid:16)(cid:0) (x(cid:0)x0)2
2(0:35)2(cid:17)

−1.0

1.0

3.0

5.0

45 | Gaussian Processes

Figure 45.1. Samples drawn from
Gaussian process priors. Each
panel shows two functions drawn
from a Gaussian process prior.
The four corresponding covariance
functions are given below each
plot. The decrease in lengthscale
from (a) to (b) produces more
rapidly (cid:13)uctuating functions. The
periodic properties of the
covariance function in (c) can be
seen. The covariance function in
(d) contains the non-stationary
term xx0 corresponding to the
covariance of a straight line, so
that typical functions include
linear trends. From Gibbs (1997).

x

(c) 2 exp(cid:16)(cid:0) sin2((cid:25)(x(cid:0)x0)=3:0)

2(0:5)2

(cid:17)

x

(d) 2 exp(cid:16)(cid:0) (x(cid:0)x0)2

2(1:5)2 (cid:17) + xx0

Multilayer neural networks and Gaussian processes

Figures 44.2 and 44.3 show some random samples from the prior distribution
over functions de(cid:12)ned by a selection of standard multilayer perceptrons with
large numbers of hidden units. Those samples don’t seem a million miles away
from the Gaussian process samples of (cid:12)gure 45.1. And indeed Neal (1996)
showed that the properties of a neural network with one hidden layer (as
in equation (45.4)) converge to those of a Gaussian process as the number of
hidden neurons tends to in(cid:12)nity, if standard ‘weight decay’ priors are assumed.
The covariance function of this Gaussian process depends on the details of the
priors assumed for the weights in the network and the activation functions of
the hidden units.

45.3 Using a given Gaussian process model in regression

We have spent some time talking about priors. We now return to our data
and the problem of prediction. How do we make predictions with a Gaussian
process?

Having formed the covariance matrix C de(cid:12)ned in equation (45.32) our task
is to infer tN +1 given the observed vector tN . The joint density P (tN +1; tN )
is a Gaussian; so the conditional distribution

P (tN +1 j tN ) =

P (tN +1; tN )

P (tN )

(45.34)

is also a Gaussian. We now distinguish between di(cid:11)erent sizes of covariance
matrix C with a subscript, such that CN +1 is the (N + 1)(cid:2) (N + 1) covariance

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45.4: Examples of covariance functions

543

matrix for the vector tN +1 (cid:17) (t1; : : : ; tN +1)T. We de(cid:12)ne submatrices of CN +1
as follows:

CN +1 (cid:17)

:

3
2
4 k3
2
4 CN 3
2
7775
6664
5
5
(cid:3) (cid:2) (cid:20) (cid:3)
(cid:2) kT
tN +1(cid:21)(cid:21) :
N +1(cid:20) tN
2(cid:2)tN tN +1(cid:3) C(cid:0)1

1

P (tN +1 j tN ) / exp(cid:20)(cid:0)

The posterior distribution (45.34) is given by

(45.35)

(45.36)

We can evaluate the mean and standard deviation of the posterior distribution
of tN +1 by brute-force inversion of CN +1. There is a more elegant expression
for the predictive distribution, however, which is useful whenever predictions
are to be made at a number of new points on the basis of the data set of size
N . We can write C(cid:0)1
N using the partitioned inverse
equations (Barnett, 1979):

N +1 in terms of CN and C(cid:0)1

C(cid:0)1

N +1 =(cid:20) M m
mT m (cid:21)

where

m = (cid:0)(cid:20) (cid:0) kT C(cid:0)1
m = (cid:0)m C(cid:0)1
N k
1
M = C(cid:0)1
N +
m

N k(cid:1)(cid:0)1

mmT :

When we substitute this matrix into equation (45.36) we (cid:12)nd

P (tN +1 j tN ) =

1
Z

exp"(cid:0)

where

(tN +1 (cid:0) ^tN +1)2

2(cid:27)2

^tN +1

#

^tN +1 = kT C(cid:0)1
(cid:27)2
^tN +1

N tN
= (cid:20) (cid:0) kT C(cid:0)1
N k:

(45.37)

(45.38)

(45.39)

(45.40)

(45.41)

(45.42)

(45.43)

The predictive mean at the new point is given by ^tN +1 and (cid:27)^tN +1
de(cid:12)nes the
error bars on this prediction. Notice that we do not need to invert CN +1 in
order to make predictions at x(N +1). Only CN needs to be inverted. Thus
Gaussian processes allow one to implement a model with a number of basis
functions H much larger than the number of data points N , with the com-
putational requirement being of order N 3, independent of H.
[We’ll discuss
ways of reducing this cost later.]

The predictions produced by a Gaussian process depend entirely on the
covariance matrix C. We now discuss the sorts of covariance functions one
might choose to de(cid:12)ne C, and how we can automate the selection of the
covariance function in response to data.

45.4 Examples of covariance functions

The only constraint on our choice of covariance function is that it must gen-
erate a non-negative-de(cid:12)nite covariance matrix for any set of points fxngN
n=1.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

544

45 | Gaussian Processes

We will denote the parameters of a covariance function by (cid:18). The covariance
matrix of t has entries given by

Cmn = C(x(m); x(n); (cid:18)) + (cid:14)mnN (x(n); (cid:18))

(45.44)

where C is the covariance function and N is a noise model which might be
stationary or spatially varying, for example,
N (x; (cid:18)) =( (cid:18)3

j=1 (cid:12)j(cid:30)j(x)(cid:17) for input-dependent noise.

The continuity properties of C determine the continuity properties of typical
samples from the Gaussian process prior. An encyclopaedic paper on Gaus-
sian processes giving many valid covariance functions has been written by
Abrahamsen (1997).

for input-independent noise

(45.45)

exp(cid:16)PJ

Stationary covariance functions

A stationary covariance function is one that is translation invariant in that it
satis(cid:12)es

C(x; x0; (cid:18)) = D(x (cid:0) x0; (cid:18))

(45.46)

for some function D, i.e., the covariance is a function of separation only, also
known as the autocovariance function. If additionally C depends only on the
magnitude of the distance between x and x0 then the covariance function is said
to be homogeneous. Stationary covariance functions may also be described in
terms of the Fourier transform of the function D, which is known as the power
spectrum of the Gaussian process. This Fourier transform is necessarily a
positive function of frequency. One way of constructing a valid stationary
covariance function is to invent a positive function of frequency and de(cid:12)ne D
to be its inverse Fourier transform.

Example 45.5. Let the power spectrum be a Gaussian function of frequency.
Since the Fourier transform of a Gaussian is a Gaussian, the autoco-
variance function corresponding to this power spectrum is a Gaussian
function of separation. This argument rederives the covariance function
we derived at equation (45.30).

Generalizing slightly, a popular form for C with hyperparameters (cid:18) =

((cid:18)1; (cid:18)2;frig) is

C(x; x0; (cid:18)) = (cid:18)1 exp"(cid:0)

1
2

(xi (cid:0) x0i)2

r2
i

# + (cid:18)2:

I

Xi=1

(45.47)

x is an I-dimensional vector and ri is a lengthscale associated with input xi, the
lengthscale in direction i on which y is expected to vary signi(cid:12)cantly. A very
large lengthscale means that y is expected to be essentially a constant function
of that input. Such an input could be said to be irrelevant, as in the automatic
relevance determination method for neural networks (MacKay, 1994a; Neal,
1996). The (cid:18)1 hyperparameter de(cid:12)nes the vertical scale of variations of a
typical function. The (cid:18)2 hyperparameter allows the whole function to be
o(cid:11)set away from zero by some unknown constant { to understand this term,
examine equation (45.25) and consider the basis function (cid:30)(x) = 1.

Another stationary covariance function is
C(x; x0) = exp((cid:0)jx (cid:0) x0j(cid:23))

0 < (cid:23) (cid:20) 2:

(45.48)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

45.5: Adaptation of Gaussian process models

545

Figure 45.2. Multimodal
likelihood functions for Gaussian
processes. A data set of (cid:12)ve
points is modelled with the simple
covariance function (45.47), with
one hyperparameter (cid:18)3 controlling
the noise variance. Panels a and b
show the most probable
interpolant and its 1(cid:27) error bars
when the hyperparameters (cid:18) are
set to two di(cid:11)erent values that
(locally) maximize the likelihood
P (tN j XN ; (cid:18)): (a) r1 = 0:95,
(cid:18)3 = 0:0; (b) r1 = 3:5, (cid:18)3 = 3:0.
Panel c shows a contour plot of
the likelihood as a function of r1
and (cid:18)3, with the two maxima
shown by crosses. From Gibbs
(1997).

7.0

5.0

3.0

1.0

−1.0

7.0

5.0

3.0

1.0

−1.0

(a)

−3.0

0.0

2.0

4.0

6.0

(b)

−3.0

0.0

2.0

4.0

6.0

(cid:2)

4

3.5

3

2.5

2

1.5

1

0.5

(cid:18)3

(cid:2)

1

0.5

1.5

(c)

2.5

3

3.5

4

2
r1

For (cid:23) = 2, this is a special case of the previous covariance function. For
(cid:23) 2 (1; 2), the typical functions from this prior are smooth but not analytic
functions. For (cid:23) (cid:20) 1 typical functions are continuous but not smooth.
A covariance function that models a function that is periodic with known
period (cid:21)i in the ith input direction is

C(x; x0; (cid:18)) = (cid:18)1 exp2
64(cid:0)

1

2Xi

0
@

:

(45.49)

sin(cid:16) (cid:25)

(cid:21)i

(xi (cid:0) x0i)(cid:17)

ri

23
1
75
A

Figure 45.1 shows some random samples drawn from Gaussian processes

with a variety of di(cid:11)erent covariance functions.

Nonstationary covariance functions

The simplest nonstationary covariance function is the one corresponding to a

linear trend. Consider the plane y(x) = Pi wixi + c. If the fwig and c have

Gaussian distributions with zero mean and variances (cid:27) 2
then the plane has a covariance function

w and (cid:27)2

c respectively

Clin(x; x0;f(cid:27)w; (cid:27)cg) =

I

Xi=1

wxix0i + (cid:27)2
(cid:27)2
c :

(45.50)

An example of random sample functions incorporating the linear term can be
seen in (cid:12)gure 45.1d.

45.5 Adaptation of Gaussian process models

Let us assume that a form of covariance function has been chosen, but that it
depends on undetermined hyperparameters (cid:18). We would like to ‘learn’ these

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

546

45 | Gaussian Processes

hyperparameters from the data. This learning process is equivalent to the
inference of the hyperparameters of a neural network, for example, weight
decay hyperparameters. It is a complexity-control problem, one that is solved
nicely by the Bayesian Occam’s razor.

Ideally we would like to de(cid:12)ne a prior distribution on the hyperparameters
and integrate over them in order to make our predictions, i.e., we would like
to (cid:12)nd

P (tN +1 j xN +1;D) =Z P (tN +1 j xN +1; (cid:18);D)P ((cid:18) jD) d(cid:18):

(45.51)

But this integral is usually intractable. There are two approaches we can take.

1. We can approximate the integral by using the most probable values of

hyperparameters.

P (tN +1 j xN +1;D) ’ P (tN +1 j xN +1;D; (cid:18)

MP)

(45.52)

2. Or we can perform the integration over (cid:18) numerically using Monte Carlo

methods (Williams and Rasmussen, 1996; Neal, 1997b).

Either of these approaches is implemented most e(cid:14)ciently if the gradient

of the posterior probability of (cid:18) can be evaluated.

Gradient

The posterior probability of (cid:18) is

P ((cid:18) jD) / P (tN j XN ; (cid:18))P ((cid:18)):

(45.53)

The log of the (cid:12)rst term (the evidence for the hyperparameters) is

ln P (tN j XN ; (cid:18)) = (cid:0)

N tN (cid:0)
and its derivative with respect to a hyperparameter (cid:18) is

ln det CN (cid:0)

1
2

1
2

N C(cid:0)1
tT

N
2

ln 2(cid:25);

(45.54)

@
@(cid:18)

ln P (tN j XN ; (cid:18)) = (cid:0)

1
2

Trace (cid:18)C(cid:0)1

N

@CN

@(cid:18) (cid:19) +

1
2

N C(cid:0)1
tT
N

@CN
@(cid:18)

C(cid:0)1

N tN :

(45.55)

Comments

Assuming that (cid:12)nding the derivatives of the priors is straightforward, we can
now search for (cid:18)
MP. However there are two problems that we need to be aware
of. Firstly, as illustrated in (cid:12)gure 45.2, the evidence may be multimodal.
Suitable priors and sensible optimization strategies often eliminate poor op-
tima. Secondly and perhaps most importantly the evaluation of the gradi-
ent of the log likelihood requires the evaluation of C(cid:0)1
N . Any exact inversion
method (such as Cholesky decomposition, LU decomposition or Gauss{Jordan
elimination) has an associated computational cost that is of order N 3 and so
calculating gradients becomes time consuming for large training data sets. Ap-
proximate methods for implementing the predictions (equations (45.42) and
(45.43)) and gradient computation (equation (45.55)) are an active research
area. One approach based on the ideas of Skilling (1993) makes approxima-
tions to C(cid:0)1t and Trace C(cid:0)1 using iterative methods with cost O(N 2) (Gibbs
and MacKay, 1996; Gibbs, 1997). Further references on this topic are given
at the end of the chapter.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

547

45.6: Classi(cid:12)cation

45.6 Classi(cid:12)cation

Gaussian processes can be integrated into classi(cid:12)cation modelling once we
identify a variable that can sensibly be given a Gaussian process prior.

In a binary classi(cid:12)cation problem, we can de(cid:12)ne a quantity an (cid:17) a(x(n))

such that the probability that the class is 1 rather than 0 is

P (tn = 1j an) =

1

1 + e(cid:0)an

:

(45.56)

Large positive values of a correspond to probabilities close to one; large neg-
ative values of a de(cid:12)ne probabilities that are close to zero.
In a classi(cid:12)ca-
tion problem, we typically intend that the probability P (tn = 1) should be a
smoothly varying function of x. We can embody this prior belief by de(cid:12)ning
a(x) to have a Gaussian process prior.

Implementation

It is not so easy to perform inferences and adapt the Gaussian process model
to data in a classi(cid:12)cation model as in regression problems because the like-
lihood function (45.56) is not a Gaussian function of an. So the posterior
distribution of a given some observations t is not Gaussian and the normal-
ization constant P (tN j XN ) cannot be written down analytically. Barber and
Williams (1997) have implemented classi(cid:12)ers based on Gaussian process priors
using Laplace approximations (Chapter 27). Neal (1997b) has implemented a
Monte Carlo approach to implementing a Gaussian process classi(cid:12)er. Gibbs
and MacKay (2000) have implemented another cheap and cheerful approach
based on the methods of Jaakkola and Jordan (section 33.8). In this varia-
tional Gaussian process classi(cid:12)er, we obtain tractable upper and lower bounds
for the unnormalized posterior density over a, P (tN j a)P (a). These bounds
are parameterized by variational parameters which are adjusted in order to
obtain the tightest possible (cid:12)t. Using normalized versions of the optimized
bounds we then compute approximations to the predictive distributions.

Multi-class classi(cid:12)cation problems can also be solved with Monte Carlo

methods (Neal, 1997b) and variational methods (Gibbs, 1997).

45.7 Discussion

Gaussian processes are moderately simple to implement and use. Because very
few parameters of the model need to be determined by hand (generally only
the priors on the hyperparameters), Gaussian processes are useful tools for
automated tasks where (cid:12)ne tuning for each problem is not possible. We do
not appear to sacri(cid:12)ce any performance for this simplicity.

It is easy to construct Gaussian processes that have particular desired
properties; for example we can make a straightforward automatic relevance
determination model.

One obvious problem with Gaussian processes is the computational cost
associated with inverting an N (cid:2) N matrix. The cost of direct methods of
inversion becomes prohibitive when the number of data points N is greater
than about 1000.

Have we thrown the baby out with the bath water?

According to the hype of 1987, neural networks were meant to be intelligent
models that discovered features and patterns in data. Gaussian processes in

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

548

45 | Gaussian Processes

contrast are simply smoothing devices. How can Gaussian processes possi-
bly replace neural networks? Were neural networks over-hyped, or have we
underestimated the power of smoothing methods?

I think both these propositions are true. The success of Gaussian processes
shows that many real-world data modelling problems are perfectly well solved
by sensible smoothing methods. The most interesting problems, the task of
feature discovery for example, are not ones that Gaussian processes will solve.
But maybe multilayer perceptrons can’t solve them either. Perhaps a fresh
start is needed, approaching the problem of machine learning from a paradigm
di(cid:11)erent from the supervised feedforward mapping.

Further reading

The study of Gaussian processes for regression is far from new. Time series
analysis was being performed by the astronomer T.N. Thiele using Gaussian
processes in 1880 (Lauritzen, 1981). In the 1940s, Wiener{Kolmogorov pre-
diction theory was introduced for prediction of trajectories of military targets
(Wiener, 1948). Within the geostatistics (cid:12)eld, Matheron (1963) proposed a
framework for regression using optimal linear estimators which he called ‘krig-
ing’ after D.G. Krige, a South African mining engineer. This framework is
identical to the Gaussian process approach to regression. Kriging has been
developed considerably in the last thirty years (see Cressie (1993) for a re-
view) including several Bayesian treatments (Omre, 1987; Kitanidis, 1986).
However the geostatistics approach to the Gaussian process model has con-
centrated mainly on low-dimensional problems and has largely ignored any
probabilistic interpretation of the model. Kalman (cid:12)lters are widely used to
implement inferences for stationary one-dimensional Gaussian processes, and
are popular models for speech and music modelling (Bar-Shalom and Fort-
mann, 1988). Generalized radial basis functions (Poggio and Girosi, 1989),
ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe,
1995) are all closely related to Gaussian processes. See also O’Hagan (1978).
The idea of replacing supervised neural networks by Gaussian processes
was (cid:12)rst explored by Williams and Rasmussen (1996) and Neal (1997b). A
thorough comparison of Gaussian processes with other methods such as neural
networks and MARS was made by Rasmussen (1996). Methods for reducing
the complexity of data modelling with Gaussian processes remain an active
research area (Poggio and Girosi, 1990; Luo and Wahba, 1997; Tresp, 2000;
Williams and Seeger, 2001; Smola and Bartlett, 2001; Rasmussen, 2002; Seeger
et al., 2003; Opper and Winther, 2000).

A longer review of Gaussian processes is in (MacKay, 1998b). A review
paper on regression with complexity control using hierarchical Bayesian models
is (MacKay, 1992a).

Gaussian processes and support vector learning machines (Scholkopf et al.,
1995; Vapnik, 1995) have a lot in common. Both are kernel-based predictors,
the kernel being another name for the covariance function. A Bayesian version
of support vectors, exploiting this connection, can be found in (Chu et al.,
2001; Chu et al., 2002; Chu et al., 2003b; Chu et al., 2003a).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

46

Deconvolution

46.1 Traditional image reconstruction methods

Optimal linear (cid:12)lters
In many imaging problems, the data measurements fdng are linearly related
to the underlying image f :

dn =Xk

Rnkfk + nn:

(46.1)

The vector n denotes the inevitable noise that corrupts real data. In the case
of a camera which produces a blurred picture, the vector f denotes the true
image, d denotes the blurred and noisy picture, and the linear operator R
is a convolution de(cid:12)ned by the point spread function of the camera. In this
special case, the true image and the data vector reside in the same space;
but it is important to maintain a distinction between them. We will use the
subscript n = 1; : : : ; N to run over data measurements, and the subscripts
k; k0 = 1; : : : ; K to run over image pixels.

One might speculate that since the blur was created by a linear operation,
then perhaps it might be deblurred by another linear operation. We can derive
the optimal linear (cid:12)lter in two ways.

Bayesian derivation

We assume that the linear operator R is known, and that the noise n is
Gaussian and independent, with a known standard deviation (cid:27)(cid:23).

P (dj f ; (cid:27)(cid:23);H) =

1
(cid:23))N=2
(2(cid:25)(cid:27)2

exp (cid:0)Xn

(cid:23))! :
(dn (cid:0)Pk Rnkfk)2. (2(cid:27)2

(46.2)

We assume that the prior probability of the image is also Gaussian, with a
scale parameter (cid:27)f .

P (f j (cid:27)f ;H) =

det(cid:0) 1
(2(cid:25)(cid:27)2

2 C
f )K=2

(46.3)

exp0
@(cid:0)Xk;k0

f )1
fkCkk0f0k(cid:14) (2(cid:27)2
A :

If we assume no correlations among the pixels then the symmetric, full rank
matrix C is equal to the identity matrix I. The more sophisticated ‘intrinsic
correlation function’ model uses C = [GGT](cid:0)1, where G is a convolution that
takes us from an imaginary ‘hidden’ image, which is uncorrelated, to the real
correlated image. The intrinsic correlation function should not be confused
with the point spread function R which de(cid:12)nes the image-to-data mapping.

549

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

550

46 | Deconvolution

A zero-mean Gaussian prior is clearly a poor assumption if it is known that
all elements of the image f are positive, but let us proceed. We can now write
down the posterior probability of an image f given the data d.

In words,

P (f j d; (cid:27)(cid:23); (cid:27)f ;H) =

P (dj f ; (cid:27)(cid:23);H)P (f j (cid:27)f ;H))

P (dj (cid:27)(cid:23); (cid:27)f ;H)

:

(46.4)

Posterior =

Likelihood (cid:2) Prior

Evidence

:

(46.5)

The ‘evidence’ P (dj (cid:27)(cid:23); (cid:27)f ;H) is the normalizing constant for this posterior
distribution. Here it is unimportant, but it is used in a more sophisticated
analysis to compare, for example, di(cid:11)erent values of (cid:27)(cid:23) and (cid:27)f , or di(cid:11)erent
point spread functions R.

Since the posterior distribution is the product of two Gaussian functions of
f , it is also a Gaussian, and can therefore be summarized by its mean, which
is also the most probable image, fMP, and its covariance matrix:

(cid:6)fjd (cid:17) [(cid:0)rr log P (f j d; (cid:27)(cid:23) ; (cid:27)f ;H)](cid:0)1 ;

(46.6)

C#(cid:0)1

(cid:27)2
(cid:23)
(cid:27)2
f

fMP ="RTR +
C(cid:21)(cid:0)1

RT is called the optimal linear (cid:12)lter. When

which de(cid:12)nes the joint error bars on f . In this equation, the symbol r denotes
di(cid:11)erentiation with respect to the image parameters f . We can (cid:12)nd fMP by
di(cid:11)erentiating the log of the posterior, and solving for the derivative being
zero. We obtain:

RTd:

(46.7)

The operator (cid:20)RTR + (cid:27)2

the term (cid:27)2
(cid:23)
(cid:27)2
f
[RTR](cid:0)1 RT. The term (cid:27)2
(cid:23)
(cid:27)2
f

(cid:23)
(cid:27)2
f

C can be neglected, the optimal linear (cid:12)lter is the pseudoinverse

C regularizes this ill-conditioned inverse.

The optimal linear (cid:12)lter can also be manipulated into the form:

Optimal linear (cid:12)lter = C(cid:0)1RT"RC(cid:0)1RT +

I#(cid:0)1

(cid:27)2
(cid:23)
(cid:27)2
f

:

(46.8)

Minimum square error derivation

The non-Bayesian derivation of the optimal linear (cid:12)lter starts by assuming
that we will ‘estimate’ the true image f by a linear function of the data:

^f = Wd:

(46.9)

The linear operator W is then ‘optimized’ by minimizing the expected sum-
squared error between ^f and the unknown true image . In the following equa-
tions, summations over repeated indices k, k0, n are implicit. The expectation
h(cid:1)i is over both the statistics of the random variables fnng, and the ensemble
of images f which we expect to bump into. We assume that the noise is zero
mean and uncorrelated to second order with itself and everything else, with
hnnnn0i = (cid:27)2

(cid:23)(cid:14)nn0.

hEi =
=

1

2D(Wkndn (cid:0) fk)2E
2D(WknRnjfj (cid:0) fk)2E +

1

1
2

WknWkn(cid:27)2
(cid:23):

(46.10)

(46.11)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

46.1: Traditional image reconstruction methods

551

Di(cid:11)erentiating with respect to W, and introducing F (cid:17)(cid:10)fj0fj(cid:11) (cf. (cid:27)2

the Bayesian derivation above), we (cid:12)nd that the optimal linear (cid:12)lter is:

f C(cid:0)1 in

Wopt = FRT(cid:2)RFRT + (cid:27)2

(cid:23)I(cid:3)(cid:0)1

:

(46.12)

f C(cid:0)1, we obtain the optimal linear (cid:12)lter (46.8) of the
If we identify F = (cid:27)2
Bayesian derivation. The ad hoc assumptions made in this derivation were the
choice of a quadratic error measure, and the decision to use a linear estimator.
It is interesting that without explicit assumptions of Gaussian distributions,
this derivation has reproduced the same estimator as the Bayesian posterior
mode, fMP.

The advantage of a Bayesian approach is that we can criticize these as-

sumptions and modify them in order to make better reconstructions.

Other image models
The better matched our model of images P (f jH) is to the real world, the bet-
ter our image reconstructions will be, and the less data we will need to answer
any given question. The Gaussian models which lead to the optimal linear
(cid:12)lter are spectacularly poorly matched to the real world. For example, the
Gaussian prior (46.3) fails to specify that all pixel intensities in an image are
positive. This omission leads to the most pronounced artefacts where the im-
age under observation has high contrast or large black patches. Optimal linear
(cid:12)lters applied to astronomical data give reconstructions with negative areas in
them, corresponding to patches of sky that suck energy out of telescopes! The
maximum entropy model for image deconvolution (Gull and Daniell, 1978)
was a great success principally because this model forced the reconstructed
image to be positive. The spurious negative areas and complementary spu-
rious positive areas are eliminated, and the quality of the reconstruction is
greatly enhanced.

The ‘classic maximum entropy’ model assigns an entropic prior

P (f j (cid:11); m;HClassic) = exp((cid:11)S(f ; m))=Z;
S(f ; m) =Xi

(fi ln(mi=fi) + fi (cid:0) mi)

(46.13)

(46.14)

where

(Skilling, 1989). This model enforces positivity; the parameter (cid:11) de(cid:12)nes a
characteristic dynamic range by which the pixel values are expected to di(cid:11)er
from the default image m.

The ‘intrinsic-correlation-function maximum-entropy’ model (Gull, 1989)
introduces an expectation of spatial correlations into the prior on f by writing
f = Gh, where G is a convolution with an intrinsic correlation function, and
putting a classic maxent prior on the underlying hidden image h.

Probabilistic movies

Having found not only the most probable image fMP but also error bars on
it, (cid:6)fjd, one task is to visualize those error bars. Whether or not we use
Monte Carlo methods to infer f , a correlated random walk around the posterior
distribution can be used to visualize the uncertainties and correlations. For
a Gaussian posterior distribution, we can create a correlated sequence of unit
normal random vectors n using

n(t+1) = cn(t) + sz;

(46.15)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

552

46 | Deconvolution

where z is a unit normal random vector and c2 + s2 = 1 (c controls how
persistent the memory of the sequence is). We then render the image sequence
de(cid:12)ned by

f (t) = fMP + (cid:6)1=2

fjd n(t)
fjd is the Cholesky decomposition of (cid:6)fjd.

where (cid:6)1=2

(46.16)

46.2 Supervised neural networks for image deconvolution

Neural network researchers often exploit the following strategy. Given a prob-
lem currently solved with a standard algorithm:
interpret the computations
performed by the algorithm as a parameterized mapping from an input to an
output, and call this mapping a neural network; then adapt the parameters
to data so as to produce another mapping that solves the task better. By
construction, the neural network can reproduce the standard algorithm, so
this data-driven adaptation can only make the performance better.

There are several reasons why standard algorithms can be bettered in this

way.

1. Algorithms are often not designed to optimize the real objective func-
tion. For example, in speech recognition, a hidden Markov model is
designed to model the speech signal, and is (cid:12)tted so as to to maximize
the generative probability given the known string of words in the training
data; but the real objective is to discriminate between di(cid:11)erent words.
If an inadequate model is being used, the neural-net-style training of
the model will focus the limited resources of the model on the aspects
relevant to the discrimination task. Discriminative training of hidden
Markov models for speech recognition does improve their performance.

2. The neural network can be more (cid:13)exible than the standard model; some
of the adaptive parameters might have been viewed as (cid:12)xed features by
the original designers. A (cid:13)exible network can (cid:12)nd properties in the data
that were not included in the original model.

46.3 Deconvolution in humans

A huge fraction of our brain is devoted to vision. One of the neglected features
of our visual system is that the raw image falling on the retina is severely
blurred: while most people can see with a resolution of about 1 arcminute
(one sixtieth of a degree) under any daylight conditions, bright or dim, the
image on our retina is blurred through a point spread function of width as
large as 5 arcminutes (Wald and Gri(cid:14)n, 1947; Howarth and Bradley, 1986).
It is amazing that we are able to resolve pixels that are twenty-(cid:12)ve times
smaller in area than the blob produced on our retina by any point source.

Isaac Newton was aware of this conundrum. It’s hard to make a lens that
does not have chromatic aberration, and our cornea and lens, like a lens made
of ordinary glass, refract blue light more strongly than red. Typically our eyes
focus correctly for the middle of the visible spectrum (green), so if we look
at a single white dot made of red, green, and blue light, the image on our
retina consists of a sharply focussed green dot surrounded by a broader red
blob superposed on an even broader blue blob. The width of the red and blue
blobs is proportional to the diameter of the pupil, which is largest under dim
lighting conditions.
[The blobs are roughly concentric, though most people
have a slight bias, such that in one eye the red blob is centred a tiny distance

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

46.3: Deconvolution in humans

553

to the left and the blue is centred a tiny distance to the right, and in the other
eye it’s the other way round. This slight bias explains why when we look
at blue and red writing on a dark background most people perceive the blue
writing to be at a slightly greater depth than the red. In a minority of people,
this small bias is the other way round and the red/blue depth perception is
reversed. But this e(cid:11)ect (which many people are aware of, having noticed it
in cinemas, for example) is tiny compared with the chromatic aberration we
are discussing.]

You can vividly demonstrate to yourself how enormous the chromatic aber-
ration in your eye is with the help of a sheet of card and a colour computer
screen.

For the most impressive results { I guarantee you will be amazed { use
a dim room with no light apart from the computer screen; a pretty strong
e(cid:11)ect will still be seen even if the room has daylight coming into it, as long as
it is not bright sunshine. Cut a slit about 1.5 mm wide in the card. On the
screen, display a few small coloured objects on a black background. I especially
recommend thin vertical objects coloured pure red, pure blue, magenta (i.e.,
red plus blue), and white (red plus blue plus green).1 Include a little black-
and-white text on the screen too. Stand or sit su(cid:14)ciently far away that you
can only just read the text { perhaps a distance of four metres or so, if you have
normal vision. Now, hold the slit vertically in front of one of your eyes, and
close the other eye. Hold the slit near to your eye { brushing your eyelashes {
and look through it. Waggle the slit slowly to the left and to the right, so that
the slit is alternately in front of the left and right sides of your pupil. What
do you see? I see the red objects waggling to and fro, and the blue objects
waggling to and fro, through huge distances and in opposite directions, while
white objects appear to stay still and are negligibly distorted. Thin magenta
objects can be seen splitting into their constituent red and blue parts. Measure
how large the motion of the red and blue objects is { it’s more than 5 minutes
of arc for me, in a dim room. Then check how sharply you can see under these
conditions { look at the text on the screen, for example: is it not the case that
you can see (through your whole pupil) features far smaller than the distance
through which the red and blue components were waggling? Yet when you are
using the whole pupil, what is falling on your retina must be an image blurred
with a blurring diameter equal to the waggling amplitude.

One of the main functions of early visual processing must be to deconvolve
this chromatic aberration. Neuroscientists sometimes conjecture that the rea-
son why retinal ganglion cells and cells in the lateral geniculate nucleus (the
main brain area to which retinal ganglion cells project) have centre-surround
receptive (cid:12)elds with colour opponency (long wavelength in the centre and
medium wavelength in the surround, for example) is in order to perform ‘fea-
ture extraction’ or ‘edge detection’, but I think this view is mistaken. The
reason we have centre-surround (cid:12)lters at the (cid:12)rst stage of visual processing
(in the fovea at least) is for the huge task of deconvolution of chromatic aber-
ration.

I speculate that the McCollough e(cid:11)ect, an extremely long-lasting associ-
ation of colours with orientation (McCollough, 1965; MacKay and MacKay,
1974), is produced by the adaptation mechanism that tunes our chromatic-
aberration-deconvolution circuits. Our deconvolution circuits need to be rapidly
tuneable, because the point spread function of our eye changes with our pupil
diameter, which can change within seconds; and indeed the McCollough e(cid:11)ect
can be induced within 30 seconds. At the same time, the e(cid:11)ect is long-lasting

1http://www.inference.phy.cam.ac.uk/mackay/itila/Files.html

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

554

46 | Deconvolution

when an eye is covered, because it’s in our interests that our deconvolution
circuits should stay well-tuned while we sleep, so that we can see sharply the
instant we wake up.

I also wonder whether the main reason that we evolved colour vision was
not ‘in order to see fruit better’ but ‘so as to be able to see black and white
sharper’ { deconvolving chromatic aberration is easier, even in an entirely black
and white world, if one has access to chromatic information in the image.

And a (cid:12)nal speculation: why do our eyes make micro-saccades when we
look at things? These miniature eye-movements are of an angular size big-
ger than the spacing between the cones in the fovea (which are spaced at
roughly 1 minute of arc, the perceived resolution of the eye). The typical
size of a microsaccade is 5{10 minutes of arc (Ratli(cid:11) and Riggs, 1950). Is it a
coincidence that this is the same as the size of chromatic aberration? Surely
micro-saccades must play an essential role in the deconvolution mechanism
that delivers our high-resolution vision.

46.4 Exercises

Exercise 46.1.[3C ] Blur an image with a circular (top hat) point spread func-
tion and add noise. Then deconvolve the blurry noisy image using the
optimal linear (cid:12)lter. Find error bars and visualize them by making a
probabilistic movie.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part VI

Sparse Graph Codes

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Part VI

The central problem of communication theory is to construct an encoding
and a decoding system that make it possible to communicate reliably over
a noisy channel. During the 1990s, remarkable progress was made towards
the Shannon limit, using codes that are de(cid:12)ned in terms of sparse random
graphs, and which are decoded by a simple probability-based message-passing
algorithm.

In a sparse-graph code, the nodes in the graph represent the transmitted
bits and the constraints they satisfy. For a linear code with a codeword length
N and rate R = K=N , the number of constraints is of order M = N (cid:0) K.
Any linear code can be described by a graph, but what makes a sparse-graph
code special is that each constraint involves only a small number of variables
in the graph: so the number of edges in the graph scales roughly linearly with
N , rather than quadratically.

In the following four chapters we will look at four families of sparse-graph
codes: three families that are excellent for error-correction: low-density parity-
check codes, turbo codes, and repeat{accumulate codes; and the family of
digital fountain codes, which are outstanding for erasure-correction.

All these codes can be decoded by a local message-passing algorithm on the
graph, the sum{product algorithm, and, while this algorithm is not a perfect
maximum likelihood decoder, the empirical results are record-breaking.

556

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47

Low-Density Parity-Check Codes

H =

Figure 47.1. A low-density
parity-check matrix and the
corresponding graph of a rate-1/4
low-density parity-check code
with blocklength N = 16, and
M = 12 constraints. Each white
circle represents a transmitted bit.
Each bit participates in j = 3
constraints, represented by
squares. Each constraint forces
the sum of the k = 4 bits to which
it is connected to be even.

A low-density parity-check code (or Gallager code) is a block code that has a
parity-check matrix, H, every row and column of which is ‘sparse’.

A regular Gallager code is a low-density parity-check code in which every
column of H has the same weight j and every row has the same weight k; reg-
ular Gallager codes are constructed at random subject to these constraints. A
low-density parity-check code with j = 3 and k = 4 is illustrated in (cid:12)gure 47.1.

47.1 Theoretical properties

Low-density parity-check codes lend themselves to theoretical study. The fol-
lowing results are proved in Gallager (1963) and MacKay (1999b).

Low-density parity-check codes, in spite of their simple construction, are
good codes, given an optimal decoder (good codes in the sense of section 11.4).
Furthermore, they have good distance (in the sense of section 13.2). These two
results hold for any column weight j (cid:21) 3. Furthermore, there are sequences of
low-density parity-check codes in which j increases gradually with N , in such
a way that the ratio j=N still goes to zero, that are very good, and that have
very good distance.

However, we don’t have an optimal decoder, and decoding low-density
parity-check codes is an NP-complete problem. So what can we do in practice?

47.2 Practical decoding

Given a channel output r, we wish to (cid:12)nd the codeword t whose likelihood
P (rj t) is biggest. All the e(cid:11)ective decoding strategies for low-density parity-
check codes are message-passing algorithms. The best algorithm known is
the sum{product algorithm, also known as iterative probabilistic decoding or
belief propagation.

We’ll assume that the channel is a memoryless channel (though more com-
plex channels can easily be handled by running the sum{product algorithm
on a more complex graph that represents the expected correlations among the
errors (Worthen and Stark, 1998)). For any memoryless channel, there are
two approaches to the decoding problem, both of which lead to the generic
problem ‘(cid:12)nd the x that maximizes

P (cid:3)(x) = P (x)  [Hx = z]’;

(47.1)

where P (x) is a separable distribution on a binary vector x, and z is another
binary vector. Each of these two approaches represents the decoding problem
in terms of a factor graph (Chapter 26).

557

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

558

47 | Low-Density Parity-Check Codes

tn

(a) The prior distribution over codewords

P (t) /  

[Ht = 0]:

P (rn j tn)
tn

P (nn)
nn

zm

The variable nodes are the transmitted bits ftng.
Each

node represents the factor

[Pn2N (m) tn = 0 mod 2].

(b) The posterior distribution over codewords,

P (tj r) / P (t)P (rj t):

Each upper function node represents a likelihood factor P (rn j tn).

(c) The joint probability of the noise n and syndrome z,

P (n; z) = P (n)

[z = Hn]:

The top variable nodes are now the noise bits fnng.
The added variable nodes at the base are the syndrome values
fzmg.
Each de(cid:12)nition zm =Pn Hmnnn mod 2 is enforced by a

factor.

Figure 47.2. Factor graphs
associated with a low-density
parity-check code.

The codeword decoding viewpoint

First, we note that the prior distribution over codewords,

P (t) /  [Ht = 0 mod 2];

(47.2)

can be represented by a factor graph ((cid:12)gure 47.2a), with the factorization
being

tn = 0 mod 2]:

(47.3)

P (t) / Ym

 [ Xn2N (m)

(We’ll omit the ‘mod 2’s from now on.) The posterior distribution over code-
words is given by multiplying this prior by the likelihood, which introduces
another N factors, one for each received bit.

P (tj r) / P (t)P (rj t)

/ Ym

 [ Xn2N (m)

tn = 0 ] Yn

P (rn j tn)

(47.4)

The factor graph corresponding to this function is shown in (cid:12)gure 47.2b. It
is the same as the graph for the prior, except for the addition of likelihood
‘dongles’ to the transmitted bits.

In this viewpoint, the received signal rn can live in any alphabet; all that

matters are the values of P (rn j tn).

The syndrome decoding viewpoint

Alternatively, we can view the channel output in terms of a binary received
vector r and a noise vector n, with a probability distribution P (n) that can
be derived from the channel properties and whatever additional information
is available at the channel outputs.

For example, with a binary symmetric channel, we de(cid:12)ne the noise by
r = t + n, the syndrome z = Hr, and noise model P (nn = 1) = f . For other
channels such as the Gaussian channel with output y, we may de(cid:12)ne a received

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.3: Decoding with the sum{product algorithm

559

binary vector r however we wish and obtain an e(cid:11)ective binary noise model
P (n) from y (exercises 9.18 (p.155) and 25.1 (p.325)).

The joint probability of the noise n and syndrome z = Hn can be factored

as

P (n; z) = P (n)  [z = Hn]
P (nn) Ym

= Yn

 [zm = Xn2N (m)

nn ]:

(47.5)

The factor graph of this function is shown in (cid:12)gure 47.2c. The variables n
and z can also be drawn in a ‘belief network’ (also known as a ‘Bayesian
network’, ‘causal network’, or ‘in(cid:13)uence diagram’) similar to (cid:12)gure 47.2a, but
with arrows on the edges from the upper circular nodes (which represent the
variables n) to the lower square nodes (which now represent the variables z).
We can say that every bit xn is the parent of j checks zm, and each check zm
is the child of k bits.

Both decoding viewpoints involve essentially the same graph. Either ver-
sion of the decoding problem can be expressed as the generic decoding problem
‘(cid:12)nd the x that maximizes

P (cid:3)(x) = P (x)  [Hx = z]’;

(47.6)

in the codeword decoding viewpoint, x is the codeword t, and z is 0; in the
syndrome decoding viewpoint, x is the noise n, and z is the syndrome.

It doesn’t matter which viewpoint we take when we apply the sum{product
algorithm. The two decoding algorithms are isomorphic and will give equiva-
lent outcomes (unless numerical errors intervene).

I tend to use the syndrome decoding viewpoint because it has one advantage:
one does not need to implement an encoder for a code in order to be able to
simulate a decoding problem realistically.

We’ll now talk in terms of the generic decoding problem.

47.3 Decoding with the sum{product algorithm

We aim, given the observed checks, to compute the marginal posterior proba-
bilities P (xn = 1j z; H) for each n. It is hard to compute these exactly because
the graph contains many cycles. However, it is interesting to implement the
decoding algorithm that would be appropriate if there were no cycles, on the
assumption that the errors introduced might be relatively small. This ap-
proach of ignoring cycles has been used in the arti(cid:12)cial intelligence literature
but is now frowned upon because it produces inaccurate probabilities. How-
ever, if we are decoding a good error-correcting code, we don’t care about
accurate marginal probabilities { we just want the correct codeword. Also,
the posterior probability, in the case of a good code communicating at an
achievable rate, is expected typically to be hugely concentrated on the most
probable decoding; so we are dealing with a distinctive probability distribution
to which experience gained in other (cid:12)elds may not apply.

The sum{product algorithm was presented in Chapter 26. We now write

out explicitly how it works for solving the decoding problem

Hx = z (mod 2):

For brevity, we reabsorb the dongles hanging o(cid:11) the x and z nodes in (cid:12)g-
ure 47.2c and modify the sum{product algorithm accordingly. The graph in

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

560

47 | Low-Density Parity-Check Codes

which x and z live is then the original graph ((cid:12)gure 47.2a) whose edges are
de(cid:12)ned by the 1s in H. The graph contains nodes of two types, which we’ll
call checks and bits. The graph connecting the checks and bits is a bipartite
graph: bits connect only to checks, and vice versa. On each iteration, a prob-
ability ratio is propagated along each edge in the graph, and each bit node xn
updates its probability that it should be in state 1.

We denote the set of bits n that participate in check m by N (m) (cid:17) fn :
Hmn = 1g. Similarly we de(cid:12)ne the set of checks in which bit n participates,
M(n) (cid:17) fm : Hmn = 1g. We denote a set N (m) with bit n excluded by
N (m)nn. The algorithm has two alternating parts, in which quantities qmn
and rmn associated with each edge in the graph are iteratively updated. The
quantity qx
mn is meant to be the probability that bit n of x has the value x,
given the information obtained via checks other than check m. The quantity
rx
mn is meant to be the probability of check m being satis(cid:12)ed if bit n of x is
considered (cid:12)xed at x and the other bits have a separable distribution given
by the probabilities fqmn0 : n0 2 N (m)nng. The algorithm would produce the
exact posterior probabilities of all the bits after a (cid:12)xed number of iterations
if the bipartite graph de(cid:12)ned by the matrix H contained no cycles.

n = P (xn = 1) = 1 (cid:0) p0

Initialization. Let p0
n = P (xn = 0) (the prior probability that bit xn is 0),
and let p1
n. If we are taking the syndrome decoding
viewpoint and the channel is a binary symmetric channel then p1
n will equal
f . If the noise level varies in a known way (for example if the channel is a
binary-input Gaussian channel with a real output) then p1
n is initialized to the
appropriate normalized likelihood. For every (n; m) such that Hmn = 1 the
variables q0

mn are initialized to the values p0

n respectively.

mn and q1

n and p1

Horizontal step.
In the horizontal step of the algorithm (horizontal from
the point of view of the matrix H), we run through the checks m and compute
for each n 2 N (m) two probabilities: (cid:12)rst, r0
mn, the probability of the observed
value of zm arising when xn = 0, given that the other bits fxn0 : n0 6= ng have
a separable distribution given by the probabilities fq 0

mn0; q1

r0

mn = Xfxn0: n02N (m)nng

P(cid:0)zm j xn = 0; (cid:8)xn0 : n0 2 N (m)nn(cid:9)(cid:1) Yn02N (m)nn

(47.7)
mn, the probability of the observed value of zm arising when

mn0g, de(cid:12)ned by:
qxn0
mn0

and second, r1
xn = 1, de(cid:12)ned by:

r1

qxn0
mn0:

mn = Xfxn0: n02N (m)nng

P(cid:0)zm j xn = 1; (cid:8)xn0 : n0 2 N (m)nn(cid:9)(cid:1) Yn02N (m)nn

(47.8)
The conditional probabilities in these summations are either zero or one, de-
pending on whether the observed zm matches the hypothesized values for xn
and the fxn0g.
These probabilities can be computed in various obvious ways based on
equation (47.7) and (47.8). The computations may be done most e(cid:14)ciently (if
jN (m)j is large) by regarding zm +xn as the (cid:12)nal state of a Markov chain with
states 0 and 1, this chain being started in state 0, and undergoing transitions
corresponding to additions of the various xn0, with transition probabilities
given by the corresponding q0
mn0. The probabilities for zm having its
observed value given either xn = 0 or xn = 1 can then be found e(cid:14)ciently by
use of the forward{backward algorithm (section 25.3).

mn0 and q1

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.3: Decoding with the sum{product algorithm

561

A particularly convenient implementation of this method uses forward and
mn are

backward passes in which products of the di(cid:11)erences (cid:14)qmn (cid:17) q0
computed. We obtain (cid:14)rmn (cid:17) r0

mn from the identity:

mn (cid:0) q1

mn (cid:0) r1

(cid:14)rmn = ((cid:0)1)zm Yn02N (m)nn

(cid:14)qmn0:

(47.9)

This identity is derived by iterating the following observation:
x(cid:23) mod 2, and x(cid:22) and x(cid:23) have probabilities q0
then P ((cid:16) = 1) = q1
(cid:22) (cid:0) q1
P ((cid:16) = 1) = (q0
We recover r0

(cid:22); q0
(cid:23) and P ((cid:16) = 0) = q0
(cid:22)q1
(cid:23) + q0
(cid:23) (cid:0) q1
(cid:23)).
mn and r1

(cid:23) and q1
(cid:23) + q1
(cid:22)q0

(cid:22)q0
(cid:22))(q0

mn using

if (cid:16) = x(cid:22) +
(cid:22); q1
(cid:23) of being 0 and 1,
(cid:22)q1
(cid:23). Thus P ((cid:16) = 0) (cid:0)

r0
mn = 1/2(1 + (cid:14)rmn); r1

mn = 1/2(1 (cid:0) (cid:14)rmn):

(47.10)

The transformations into di(cid:11)erences (cid:14)q and back from (cid:14)r to frg may be viewed
as a Fourier transform and an inverse Fourier transformation.

Vertical step. The vertical step takes the computed values of r 0
and updates the values of the probabilities q0
compute:

mn and q1

mn
mn. For each n we

mn and r1

q0
mn = (cid:11)mn p0

q1
mn = (cid:11)mn p1

n Ym02M(n)nm
n Ym02M(n)nm

r0
m0n

r1
m0n

(47.11)

(47.12)

where (cid:11)mn is chosen such that q0
computed in a downward pass and an upward pass.

mn+q1

mn = 1. These products can be e(cid:14)ciently

We can also compute the ‘pseudoposterior probabilities’ q 0

n and q1

n at this

iteration, given by:

q0
n = (cid:11)n p0

n = (cid:11)n p1
q1

n Ym2M(n)
n Ym2M(n)

r0
mn;

r1
mn:

(47.13)

(47.14)

These quantities are used to create a tentative decoding ^x, the consistency
of which is used to decide whether the decoding algorithm can halt. (Halt if
H^x = z.)

At this point, the algorithm repeats from the horizontal step.

The stop-when-it’s-done decoding method. The recommended decod-
ing procedure is to set ^xn to 1 if q1
n > 0:5 and see if the checks H^x = z mod 2 are
all satis(cid:12)ed, halting when they are, and declaring a failure if some maximum
number of iterations (e.g. 200 or 1000) occurs without successful decoding. In
the event of a failure, we may still report ^x, but we (cid:13)ag the whole block as a
failure.

We note in passing the di(cid:11)erence between this decoding procedure and
the widespread practice in the turbo code community, where the decoding
algorithm is run for a (cid:12)xed number of iterations (irrespective of whether the
decoder (cid:12)nds a consistent state at some earlier time). This practice is wasteful
of computer time, and it blurs the distinction between undetected and detected
errors. In our procedure, ‘undetected’ errors occur if the decoder (cid:12)nds an ^x

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

562

47 | Low-Density Parity-Check Codes

(a)

!

parity bits

8>>>>>>>><
>>>>>>>>:

(b)

(c)

Figure 47.3. Demonstration of encoding with a rate-1=2 Gallager code. The encoder is derived from
a very sparse 10 000 (cid:2) 20 000 parity-check matrix with three 1s per column ((cid:12)gure 47.4).
(a) The code creates transmitted vectors consisting of 10 000 source bits and 10 000 parity-
check bits. (b) Here, the source sequence has been altered by changing the (cid:12)rst bit. Notice
that many of the parity-check bits are changed. Each parity bit depends on about half of
the source bits. (c) The transmission for the case s = (1; 0; 0; : : : ; 0). This vector is the
di(cid:11)erence (modulo 2) between transmissions (a) and (b). [Dilbert image Copyright c(cid:13)1997
United Feature Syndicate, Inc., used with permission.]

satisfying H^x = z mod 2 that is not equal to the true x.
‘Detected’ errors
occur if the algorithm runs for the maximum number of iterations without
(cid:12)nding a valid decoding. Undetected errors are of scienti(cid:12)c interest because
they reveal distance properties of a code. And in engineering practice, it would
seem preferable for the blocks that are known to contain detected errors to be
so labelled if practically possible.

Cost.
In a brute-force approach, the time to create the generator matrix
scales as N 3, where N is the block size. The encoding time scales as N 2, but
encoding involves only binary arithmetic, so for the block lengths studied here
it takes considerably less time than the simulation of the Gaussian channel.
Decoding involves approximately 6N j (cid:13)oating-point multiplies per iteration,
so the total number of operations per decoded bit (assuming 20 iterations)
is about 120t=R, independent of blocklength. For the codes presented in the
next section, this is about 800 operations.

The encoding complexity can be reduced by clever encoding tricks invented
by Richardson and Urbanke (2001b) or by specially constructing the parity-
check matrix (MacKay et al., 1999).

The decoding complexity can be reduced, with only a small loss in perfor-
mance, by passing low-precision messages in place of real numbers (Richardson
and Urbanke, 2001a).

47.4 Pictorial demonstration of Gallager codes

Figures 47.3{47.7 illustrate visually the conditions under which low-density
parity-check codes can give reliable communication over binary symmetric
channels and Gaussian channels. These demonstrations may be viewed as
animations on the world wide web.1

1http://www.inference.phy.cam.ac.uk/mackay/codes/gifs/

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.4: Pictorial demonstration of Gallager codes

563

H =

Figure 47.4. A low-density parity-check matrix with N = 20 000 columns of weight j = 3 and M =

10 000 rows of weight k = 6.

Encoding

Figure 47.3 illustrates the encoding operation for the case of a Gallager code
whose parity-check matrix is a 10 000 (cid:2) 20 000 matrix with three 1s per col-
umn ((cid:12)gure 47.4). The high density of the generator matrix is illustrated in
(cid:12)gure 47.3b and c by showing the change in the transmitted vector when one
of the 10 000 source bits is altered. Of course, the source images shown here
are highly redundant, and such images should really be compressed before
encoding. Redundant images are chosen in these demonstrations to make it
easier to see the correction process during the iterative decoding. The decod-
ing algorithm does not take advantage of the redundancy of the source vector,
and it would work in exactly the same way irrespective of the choice of source
vector.

Iterative decoding

The transmission is sent over a channel with noise level f = 7:5% and the
received vector is shown in the upper left of (cid:12)gure 47.5. The subsequent
pictures in (cid:12)gure 47.5 show the iterative probabilistic decoding process. The
sequence of (cid:12)gures shows the best guess, bit by bit, given by the iterative
decoder, after 0, 1, 2, 3, 10, 11, 12, and 13 iterations. The decoder halts after
the 13th iteration when the best guess violates no parity checks. This (cid:12)nal
decoding is error free.

In the case of an unusually noisy transmission, the decoding algorithm fails
to (cid:12)nd a valid decoding. For this code and a channel with f = 7:5%, such
failures happen about once in every 100 000 transmissions. Figure 47.6 shows
this error rate compared with the block error rates of classical error-correcting
codes.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

564

47 | Low-Density Parity-Check Codes

received:

0

1

2

3

10

11

12

13

!

decoded:

Figure 47.5.

Iterative probabilistic decoding of a low-density parity-check code for a transmission
received over a channel with noise level f = 7:5%. The sequence of (cid:12)gures shows the best
guess, bit by bit, given by the iterative decoder, after 0, 1, 2, 3, 10, 11, 12, and 13 iterations.
The decoder halts after the 13th iteration when the best guess violates no parity checks.
This (cid:12)nal decoding is error free.

0.1

0.01

0.001

0.0001

1e-05

1e-06

r
o
r
r
e
 
r
e
d
o
c
e
d
 
f
o
 
y
t
i
l
i

b
a
b
o
r
P

low-density
parity-check code

Shannon limit

GV

0

0.2

0.4

C
0.6

Rate

0.8

1

Figure 47.6. Error probability of
the low-density parity-check code
(with error bars) for binary
symmetric channel with f = 7:5%,
compared with algebraic codes.
Squares: repetition codes and
Hamming (7; 4) code; other
points: Reed{Muller and BCH
codes.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.4: Pictorial demonstration of Gallager codes

565

Figure 47.7. Demonstration of a
Gallager code for a Gaussian
channel. (a1) The received vector
after transmission over a Gaussian
channel with x=(cid:27) = 1:185
(Eb=N0 = 1:47 dB). The greyscale
represents the value of the
normalized likelihood. This
transmission can be perfectly
decoded by the sum{product
decoder. The empirical
probability of decoding failure is
about 10(cid:0)5. (a2) The probability
distribution of the output y of the
channel with x=(cid:27) = 1:185 for each
of the two possible inputs. (b1)
The received transmission over a
Gaussian channel with x=(cid:27) = 1:0,
which corresponds to the Shannon
limit. (b2) The probability
distribution of the output y of the
channel with x=(cid:27) = 1:0 for each of
the two possible inputs.

Figure 47.8. Performance of
rate-1/2 Gallager codes on the
Gaussian channel. Vertical axis:
block error probability. Horizontal
axis: signal-to-noise ratio Eb=N0.
(a) Dependence on blocklength N
for (j; k) = (3; 6) codes. From left
to right: N = 816, N = 408,
N = 204, N = 96. The dashed
lines show the frequency of
undetected errors, which is
measurable only when the
blocklength is as small as N = 96
or N = 204. (b) Dependence on
column weight j for codes of
blocklength N = 816.

(a1)

(b1)

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

(a2)

P(y|‘0’)

P(y|‘1’)

-4

-2

0

2

4

1

0.1

0.01

0.001

0.0001

1e-05

1e-06

N=816

N=408

(N=204)

N=96
(N=96)

N=204

1

1.5

2

2.5

3

3.5

4

4.5

5

5.5

(a)

Gaussian channel

(b2)

1

0.1

0.01

0.001

0.0001

1e-05

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

P(y|‘0’)

P(y|‘1’)

-4

-2

0

2

4

j=4

j=3

3

j=5

3.5

j=6
4

1

1.5

2

2.5

(b)

In (cid:12)gure 47.7 the left picture shows the received vector after transmission over
a Gaussian channel with x=(cid:27) = 1:185. The greyscale represents the value
. This signal-to-noise ratio
of the normalized likelihood,
x=(cid:27) = 1:185 is a noise level at which this rate-1/2 Gallager code communicates
reliably (the probability of error is ’ 10(cid:0)5). To show how close we are to the
Shannon limit, the right panel shows the received vector when the signal-to-
noise ratio is reduced to x=(cid:27) = 1:0, which corresponds to the Shannon limit
for codes of rate 1/2.

P (y j t = 1)+P (y j t = 0)

P (y j t = 1)

Variation of performance with code parameters

Figure 47.8 shows how the parameters N and j a(cid:11)ect the performance of
low-density parity-check codes. As Shannon would predict, increasing the
blocklength leads to improved performance. The dependence on j follows a
di(cid:11)erent pattern. Given an optimal decoder, the best performance would be
obtained for the codes closest to random codes, that is, the codes with largest
j. However, the sum{product decoder makes poor progress in dense graphs,
so the best performance is obtained for a small value of j. Among the values

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

566

47 | Low-Density Parity-Check Codes

3

3

3

(a)

(b)

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

f = 0:080

f = 0:075

5

10

15

20

25

30

Figure 47.9. Schematic illustration
of constructions (a) of a
completely regular Gallager code
with j = 3, k = 6 and R = 1=2;
(b) of a nearly-regular Gallager
code with rate 1=3. Notation: an
integer represents a number of
permutation matrices superposed
on the surrounding square. A
diagonal line represents an
identity matrix.

Figure 47.10. Monte Carlo simulation of density evolution, following the decoding process for j = 4; k =
8. Each curve shows the average entropy of a bit as a function of number of iterations,
as estimated by a Monte Carlo algorithm using 10 000 samples per iteration. The noise
level of the binary symmetric channel f increases by steps of 0:005 from bottom graph
(f = 0:010) to top graph (f = 0:100). There is evidently a threshold at about f = 0:075,
above which the algorithm cannot determine x. From MacKay (1999b).

of j shown in the (cid:12)gure, j = 3 is the best, for a blocklength of 816, down to a
block error probability of 10(cid:0)5.

This observation motivates construction of Gallager codes with some col-
umns of weight 2. A construction with M=2 columns of weight 2 is shown in
(cid:12)gure 47.9b. Too many columns of weight 2, and the code becomes a much
poorer code.

As we’ll discuss later, we can do even better by making the code even more

irregular.

47.5 Density evolution

One way to study the decoding algorithm is to imagine it running on an in(cid:12)nite
tree-like graph with the same local topology as the Gallager code’s graph.
The larger the matrix H, the closer its decoding properties should approach
those of the in(cid:12)nite graph.

Imagine an in(cid:12)nite belief network with no loops, in which every bit xn
connects to j checks and every check zm connects to k bits ((cid:12)gure 47.11).
We consider the iterative (cid:13)ow of information in this network, and examine
the average entropy of one bit as a function of number of iterations. At each
iteration, a bit has accumulated information from its local network out to a
radius equal to the number of iterations. Successful decoding will occur only
if the average entropy of a bit decreases to zero as the number of iterations
increases.

The iterations of an in(cid:12)nite belief network can be simulated by Monte
Carlo methods { a technique (cid:12)rst used by Gallager (1963). Imagine a network
of radius I (the total number of iterations) centred on one bit. Our aim is
to compute the conditional entropy of the central bit x given the state z of
all checks out to radius I. To evaluate the probability that the central bit
is 1 given a particular syndrome z involves an I-step propagation from the
outside of the network into the centre. At the ith iteration, probabilities r at

Figure 47.11. Local topology of
the graph of a Gallager code with
column weight j = 3 and row
weight k = 4. White nodes
represent bits, xl; black nodes
represent checks, zm; each edge
corresponds to a 1 in H.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.6: Improving Gallager codes

radius I (cid:0) i + 1 are transformed into qs and then into rs at radius I (cid:0) i in
a way that depends on the states x of the unknown bits at radius I (cid:0) i. In
the Monte Carlo method, rather than simulating this network exactly, which
would take a time that grows exponentially with I, we create for each iteration
a representative sample (of size 100, say) of the values of fr; xg. In the case
of a regular network with parameters j; k, each new pair fr; xg in the list at
the ith iteration is created by drawing the new x from its distribution and
drawing at random with replacement (j (cid:0) 1)(k(cid:0) 1) pairs fr; xg from the list at
the (i(cid:0)1)th iteration; these are assembled into a tree fragment ((cid:12)gure 47.12)
and the sum{product algorithm is run from top to bottom to (cid:12)nd the new r
value associated with the new node.

As an example, the results of runs with j = 4, k = 8 and noise densities f
between 0.01 and 0.10, using 10 000 samples at each iteration, are shown in
(cid:12)gure 47.10. Runs with low enough noise level show a collapse to zero entropy
after a small number of iterations, and those with high noise level decrease to
a non-zero entropy corresponding to a failure to decode.

The boundary between these two behaviours is called the threshold of the
decoding algorithm for the binary symmetric channel. Figure 47.10 shows by
Monte Carlo simulation that the threshold for regular (j; k) = (4; 8) codes
is about 0.075. Richardson and Urbanke (2001a) have derived thresholds for
regular codes by a tour de force of direct analytic methods. Some of these
thresholds are shown in table 47.13.

Approximate density evolution

For practical purposes, the computational cost of density evolution can be
reduced by making Gaussian approximations to the probability distributions
over the messages in density evolution, and updating only the parameters of
these approximations. For further information about these techniques, which
produce diagrams known as EXIT charts, see (ten Brink, 1999; Chung et al.,
2001; ten Brink et al., 2002).

47.6 Improving Gallager codes

Since the rediscovery of Gallager codes, two methods have been found for
enhancing their performance.

Clump bits and checks together

First, we can make Gallager codes in which the variable nodes are grouped
together into metavariables consisting of say 3 binary variables, and the check
nodes are similarly grouped together into metachecks. As before, a sparse
graph can be constructed connecting metavariables to metachecks, with a lot
of freedom about the details of how the variables and checks within are wired
up. One way to set the wiring is to work in a (cid:12)nite (cid:12)eld GF (q) such as GF (4)
or GF (8), de(cid:12)ne low-density parity-check matrices using elements of GF (q),
and translate our binary messages into GF (q) using a mapping such as the
one for GF (4) given in table 47.14. Now, when messages are passed during
decoding, those messages are probabilities and likelihoods over conjunctions
of binary variables. For example if each clump contains three binary variables
then the likelihoods will describe the likelihoods of the eight alternative states
of those bits.

With carefully optimized constructions, the resulting codes over GF (4),

rf f f

x
@@R

(cid:0)(cid:0)(cid:9)
?
@

@@R

567

(cid:21) iteration
i(cid:0)1

(cid:0)(cid:0)(cid:9)
?

@@R
(cid:0)

f f f
(cid:0)(cid:0)(cid:9)f?

x
r

(cid:21) iteration

i

Figure 47.12. A tree-fragment
constructed during Monte Carlo
simulation of density evolution.
This fragment is appropriate for a
regular j = 3, k = 4 Gallager code.

(j; k)

(3,6)
(4,8)
(5,10)

fmax

0.084
0.076
0.068

Table 47.13. Thresholds fmax for
regular low-density parity-check
codes, assuming sum{product
decoding algorithm, from
Richardson and Urbanke (2001a).
The Shannon limit for rate-1/2
codes is fmax = 0:11.

GF (4) $ binary

0 $ 00
1 $ 01
A $ 10
B $ 11

Table 47.14. Translation between
GF (4) and binary for message
symbols.

00

GF (4) ! binary
0 ! 00
1 ! 10
A ! 11
B ! 01

01

10

11

Table 47.15. Translation between
GF (4) and binary for matrix
entries. An M (cid:2) N parity-check
matrix over GF (4) can be turned
into a 2M (cid:2) 2N binary
parity-check matrix in this way.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

568

47 | Low-Density Parity-Check Codes

F 0 = [f 0 + f 1] + [f A + f B]
F 1 = [f 0 (cid:0) f 1] + [f A (cid:0) f B]
F A = [f 0 + f 1] (cid:0) [f A + f B]
F B = [f 0 (cid:0) f 1] (cid:0) [f A (cid:0) f B]

Luby

Reg GF(2)

Irreg GF(2)

Irreg GF(8)

Reg GF(16)

Gallileo

Turbo

0.1

0.01

0.001

0.0001

1e-05

1e-06

y
t
i
l
i

b
a
b
o
r
P

 
r
o
r
r

E

-
t
i

B

 
l

a
c
i
r
i
p
m
E

Algorithm 47.16. The Fourier
transform over GF (4).
The Fourier transform F of a
function f over GF (2) is given by
F 0 = f 0 + f 1, F 1 = f 0 (cid:0) f 1.
Transforms over GF (2k) can be
viewed as a sequence of binary
transforms in each of k
dimensions. The inverse
transform is identical to the
Fourier transform, except that we
also divide by 2k.

-0.4

-0.2

0

0.2

0.4

0.6

0.8

Signal to Noise ratio (dB)

Figure 47.17. Comparison of regular binary Gallager codes with irregular codes, codes over GF (q),
and other outstanding codes of rate 1/4. From left (best performance) to right: Irregular
low-density parity-check code over GF (8), blocklength 48 000 bits (Davey, 1999); JPL
turbo code (JPL, 1996) blocklength 65 536; Regular low-density parity-check over GF (16),
blocklength 24 448 bits (Davey and MacKay, 1998); Irregular binary low-density parity-
check code, blocklength 16 000 bits (Davey, 1999); Luby et al. (1998) irregular binary low-
density parity-check code, blocklength 64 000 bits; JPL code for Galileo (in 1992, this was
the best known code of rate 1/4); Regular binary low-density parity-check code: blocklength
40 000 bits (MacKay, 1999b). The Shannon limit is at about (cid:0)0:79 dB. As of 2003, even
better sparse-graph codes have been constructed.

GF (8), and GF (16) perform nearly one decibel better than comparable binary
Gallager codes.

The computational cost for decoding in GF (q) scales as q log q, if the ap-
propriate Fourier transform is used in the check nodes: the update rule for
the check-to-variable message,

ra

mn = Xx:xn=a

Hmn0xn0 = zm3

 2
4 Xn02N (m)

qxj
mj;

(47.15)

5 Yj2N (m)nn

is a convolution of the quantities qa
mj, so the summation can be replaced by
a product of the Fourier transforms of qa
mj for j 2 N (m)nn, followed by
an inverse Fourier transform. The Fourier transform for GF (4) is shown in
algorithm 47.16.

Make the graph irregular

The second way of improving Gallager codes, introduced by Luby et al. (2001b),
is to make their graphs irregular. Instead of giving all variable nodes the same
degree j, we can have some variable nodes with degree 2, some 3, some 4, and
a few with degree 20. Check nodes can also be given unequal degrees { this
helps improve performance on erasure channels, but it turns out that for the
Gaussian channel, the best graphs have regular check degrees.

Figure 47.17 illustrates the bene(cid:12)ts o(cid:11)ered by these two methods for im-
proving Gallager codes, focussing on codes of rate 1/4. Making the binary code
irregular gives a win of about 0.4 dB; switching from GF (2) to GF (16) gives

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.7: Fast encoding of low-density parity-check codes

569

Figure 47.18. An algebraically
constructed low-density
parity-check code satisfying many
redundant constraints
outperforms an equivalent random
Gallager code. The table shows
the N , M , K, distance d, and row
weight k of some di(cid:11)erence-set
cyclic codes, highlighting the
codes that have large d=N , small
k, and large N=M . In the
comparison the Gallager code had
(j; k) = (4; 13), and rate identical
to the N = 273 di(cid:11)erence-set
cyclic code. Vertical axis: block
error probability. Horizontal axis:
signal-to-noise ratio Eb=N0 (dB).

difference set cyclic codes

21

N 7
73
M 4 10 28
11
K 3
45
d
4
6 10
9
5
3
k

273
82
191
18
17

1057
244
813
34
33

4161
730
3431
66
65

1

0.1

0.01

0.001

Gallager(273,82)
DSC(273,82)

0.0001

1.5

2

2.5

3

3.5

4

about 0.6 dB; and Matthew Davey’s code that combines both these features {
it’s irregular over GF (8) { gives a win of about 0.9 dB over the regular binary
Gallager code.

Methods for optimizing the pro(cid:12)le of a Gallager code (that is, its number of
rows and columns of each degree), have been developed by Richardson et al.
(2001) and have led to low-density parity-check codes whose performance,
when decoded by the sum{product algorithm, is within a hair’s breadth of the
Shannon limit.

Algebraic constructions of Gallager codes

The performance of regular Gallager codes can be enhanced in a third man-
ner: by designing the code to have redundant sparse constraints. There is a
di(cid:11)erence-set cyclic code, for example, that has N = 273 and K = 191, but
the code satis(cid:12)es not M = 82 but N , i.e., 273 low-weight constraints ((cid:12)gure
47.18). It is impossible to make random Gallager codes that have anywhere
near this much redundancy among their checks. The di(cid:11)erence-set cyclic code
performs about 0.7 dB better than an equivalent random Gallager code.

An open problem is to discover codes sharing the remarkable properties of
the di(cid:11)erence-set cyclic codes but with di(cid:11)erent blocklengths and rates. I call
this task the Tanner challenge.

47.7 Fast encoding of low-density parity-check codes

We now discuss methods for fast encoding of low-density parity-check codes {
faster than the standard method, in which a generator matrix G is found by
Gaussian elimination (at a cost of order M 3) and then each block is encoded
by multiplying it by G (at a cost of order M 2).

Staircase codes

Certain low-density parity-check matrices with M columns of weight 2 or less
can be encoded easily in linear time. For example, if the matrix has a staircase
structure as illustrated by the right-hand side of

H =2
6664

;

3
7775

(47.16)

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

570

47 | Low-Density Parity-Check Codes

and if the data s are loaded into the (cid:12)rst K bits, then the M parity bits p
can be computed from left to right in linear time.

p1 =

PK
p2 = p1 + PK
p3 = p2 + PK
pM = pM(cid:0)1+ PK

...

n=1 H1nsn
n=1 H2nsn
n=1 H3nsn

n=1 HM nsn:

(47.17)

If we call two parts of the H matrix [HsjHp], we can describe the encoding
operation in two steps: (cid:12)rst compute an intermediate parity vector v = Hss;
then pass v through an accumulator to create p.

Figure 47.19. The parity-check
matrix in approximate
lower-triangular form.

The cost of this encoding method is linear if the sparsity of H is exploited

when computing the sums in (47.17).

Fast encoding of general low-density parity-check codes

Richardson and Urbanke (2001b) demonstrated an elegant method by which
the encoding cost of any low-density parity-check code can be reduced from
the straightforward method’s M 2 to a cost of N + g2, where g, the gap, is
hopefully a small constant, and in the worst cases scales as a small fraction of
N .

(cid:27)
-(cid:27)g

M

-

A

C

(cid:27)

B

D

N

@

@

@
T

0
@

@

@

E

-

6

M

6
g
?

?

In the (cid:12)rst step, the parity-check matrix is rearranged, by row-interchange
and column-interchange, into the approximate lower-triangular form shown in
(cid:12)gure 47.19. The original matrix H was very sparse, so the six matrices A,
B, T, C, D; and E are also very sparse. The matrix T is lower triangular and
has 1s everywhere on the diagonal.

H =(cid:20) A B T
C D E (cid:21) :

(47.18)

The source vector s of length K = N (cid:0) M is encoded into a transmission
t = [s; p1; p2] as follows.

1. Compute the upper syndrome of the source vector,

zA = As:

(47.19)

This can be done in linear time.

2. Find a setting of the second parity bits, pA

2 , such that the upper syn-

drome is zero.

2 = (cid:0)T(cid:0)1zA:
pA

(47.20)

This vector can be found in linear time by back-substitution, i.e., com-
puting the (cid:12)rst bit of pA
2 , then the second, then the third, and so forth.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.8: Further reading

571

3. Compute the lower syndrome of the vector [s; 0; pA

2 ]:

zB = Cs (cid:0) EpA
2 :

This can be done in linear time.

4. Now we get to the clever bit. De(cid:12)ne the matrix
F (cid:17) (cid:0)ET(cid:0)1B + D;

(47.21)

(47.22)

and (cid:12)nd its inverse, F(cid:0)1. This computation needs to be done once only,
and its cost is of order g3. This inverse F(cid:0)1 is a dense g(cid:2) g matrix. [If F
is not invertible then either H is not of full rank, or else further column
permutations of H can produce an F that is invertible.]

Set the (cid:12)rst parity bits, p1, to

p1 = (cid:0)F(cid:0)1zB:

(47.23)

This operation has a cost of order g2.

Claim: At this point, we have found the correct setting of the (cid:12)rst parity
bits, p1.

5. Discard the tentative parity bits pA

2 and (cid:12)nd the new upper syndrome,

zC = zA + Bp1:

(47.24)

This can be done in linear time.

6. Find a setting of the second parity bits, p2, such that the upper syndrome

is zero,

This vector can be found in linear time by back-substitution.

p2 = (cid:0)T(cid:0)1zC

(47.25)

47.8 Further reading

Low-density parity-check codes codes were (cid:12)rst studied in 1962 by Gallager,
then were generally forgotten by the coding theory community. Tanner (1981)
generalized Gallager’s work by introducing more general constraint nodes; the
codes that are now called turbo product codes should in fact be called Tanner
product codes, since Tanner proposed them, and his colleagues (Karplus and
Krit, 1991) implemented them in hardware. Publications on Gallager codes
contributing to their 1990s rebirth include (Wiberg et al., 1995; MacKay and
Neal, 1995; MacKay and Neal, 1996; Wiberg, 1996; MacKay, 1999b; Spielman,
1996; Sipser and Spielman, 1996). Low-precision decoding algorithms and fast
encoding algorithms for Gallager codes are discussed in (Richardson and Ur-
banke, 2001a; Richardson and Urbanke, 2001b). MacKay and Davey (2000)
showed that low-density parity-check codes can outperform Reed{Solomon
codes, even on the Reed{Solomon codes’ home turf: high rate and short block-
lengths. Other important papers include (Luby et al., 2001a; Luby et al.,
2001b; Luby et al., 1997; Davey and MacKay, 1998; Richardson et al., 2001;
Chung et al., 2001). Useful tools for the design of irregular low-density parity-
check codes include (Chung et al., 1999; Urbanke, 2001).

See (Wiberg, 1996; Frey, 1998; McEliece et al., 1998) for further discussion

of the sum{product algorithm.

For a view of low-density parity-check code decoding in terms of group
theory and coding theory, see (Forney, 2001; O(cid:11)er and Soljanin, 2000; O(cid:11)er

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

572

47 | Low-Density Parity-Check Codes

and Soljanin, 2001); and for background reading on this topic see (Hartmann
and Rudolph, 1976; Terras, 1999). There is a growing literature on the prac-
tical design of low-density parity-check codes (Mao and Banihashemi, 2000;
Mao and Banihashemi, 2001; ten Brink et al., 2002); they are now being
adopted for applications from hard drives to satellite communications.

For low-density parity-check codes applicable to quantum error-correction,

see MacKay et al. (2004).

47.9 Exercises

Exercise 47.1.[2 ] The ‘hyperbolic tangent’ version of the decoding algorithm. In
section 47.3, the sum{product decoding algorithm for low-density parity-
check codes was presented (cid:12)rst in terms of quantities q 0=1
mn, then
in terms of quantities (cid:14)q and (cid:14)r. There is a third description, in which
the fqg are replaced by log probability-ratios,

mn and r0=1

lmn (cid:17) ln

q0
mn
q1
mn

:

Show that

(cid:14)qmn (cid:17) q0

mn (cid:0) q1

mn = tanh(lmn=2):

(47.26)

(47.27)

Derive the update rules for frg and flg.

Exercise 47.2.[2, p.572] I am sometimes asked ‘why not decode other linear
codes, for example algebraic codes, by transforming their parity-check
matrices so that they are low-density, and applying the sum{product
algorithm?’ [Recall that any linear combination of rows of H, H0 = PH,
is a valid parity-check matrix for a code, as long as the matrix P is
invertible; so there are many parity check matrices for any one code.]

Explain why a random linear code does not have a low-density parity-
check matrix. [Here, low-density means ‘having row-weight at most k’,
where k is some small constant (cid:28) N .]

Exercise 47.3.[3 ] Show that if a low-density parity-check code has more than
M columns of weight 2 { say (cid:11)M columns, where (cid:11) > 1 { then the code
will have words with weight of order log M .

Exercise 47.4.[5 ] In section 13.5 we found the expected value of the weight
enumerator function A(w), averaging over the ensemble of all random
linear codes. This calculation can also be carried out for the ensemble of
low-density parity-check codes (Gallager, 1963; MacKay, 1999b; Litsyn
and Shevelev, 2002).
It is plausible, however, that the mean value of
A(w) is not always a good indicator of the typical value of A(w) in the
ensemble. For example, if, at a particular value of w, 99% of codes have
A(w) = 0, and 1% have A(w) = 100 000, then while we might say the
typical value of A(w) is zero, the mean is found to be 1000. Find the
typical weight enumerator function of low-density parity-check codes.

47.10 Solutions

Solution to exercise 47.2 (p.572). Consider codes of rate R and blocklength
N , having K = RN source bits and M = (1(cid:0)R)N parity-check bits. Let all

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

47.10: Solutions

573

the codes have their bits ordered so that the (cid:12)rst K bits are independent, so
that we could if we wish put the code in systematic form,

G = [1KjPT]; H = [Pj1M ]:

(47.28)

The number of distinct linear codes is the number of matrices P, which is
N1 = 2M K = 2N 2R(1(cid:0)R). Can these all be expressed as distinct low-density
parity-check codes?

The number of low-density parity-check matrices with row-weight k is

log N1 ’ N 2R(1 (cid:0) R)

k(cid:19)M
(cid:18)N

and the number of distinct codes that they de(cid:12)ne is at most

N2 = (cid:18)N

k(cid:19)M, M !;

(47.29)

(47.30)

which is much smaller than N1, so, by the pigeon-hole principle, it is not
possible for every random linear code to map on to a low-density H.

log N2 < N k log N

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

48

Convolutional Codes and Turbo Codes

This chapter follows tightly on from Chapter 25. It makes use of the ideas of
codes and trellises and the forward{backward algorithm.

48.1 Introduction to convolutional codes

When we studied linear block codes, we described them in three ways:

1. The generator matrix describes how to turn a string of K arbitrary

source bits into a transmission of N bits.

2. The parity-check matrix speci(cid:12)es the M = N (cid:0) K parity-check con-

straints that a valid codeword satis(cid:12)es.

3. The trellis of the code describes its valid codewords in terms of paths

through a trellis with labelled edges.

A fourth way of describing some block codes, the algebraic approach, is not
covered in this book (a) because it has been well covered by numerous other
books in coding theory; (b) because, as this part of the book discusses, the
state-of-the-art in error-correcting codes makes little use of algebraic coding
theory; and (c) because I am not competent to teach this subject.

We will now describe convolutional codes in two ways: (cid:12)rst, in terms of
mechanisms for generating transmissions t from source bits s; and second, in
terms of trellises that describe the constraints satis(cid:12)ed by valid transmissions.

48.2 Linear-feedback shift-registers

We generate a transmission with a convolutional code by putting a source
stream through a linear (cid:12)lter. This (cid:12)lter makes use of a shift register, linear
output functions, and, possibly, linear feedback.

I will draw the shift-register in a right-to-left orientation: bits roll from

right to left as time goes on.

Figure 48.1 shows three linear-feedback shift-registers which could be used
to de(cid:12)ne convolutional codes. The rectangular box surrounding the bits
z1 : : : z7 indicates the memory of the (cid:12)lter, also known as its state. All three
(cid:12)lters have one input and two outputs. On each clock cycle, the source sup-
plies one bit, and the (cid:12)lter outputs two bits t(a) and t(b). By concatenating
together these bits we can obtain from our source stream s1s2s3 : : : a trans-
mission stream t(a)
3 : : : : Because there are two transmitted bits
for every source bit, the codes shown in (cid:12)gure 48.1 have rate 1/2. Because

2 t(b)

1 t(a)

1 t(b)

2 t(a)

3 t(b)

574

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

48.2: Linear-feedback shift-registers

575

Figure 48.1. Linear-feedback
shift-registers for generating
convolutional codes with rate 1=2.
The symbol hd indicates a
copying with a delay of one clock
cycle. The symbol (cid:8) denotes
linear addition modulo 2 with no
delay.
The (cid:12)lters are (a) systematic and
nonrecursive; (b) nonsystematic
and nonrecursive; (c) systematic
and recursive.

11 101 011
#
# #
3
3 5

Table 48.2. How taps in the delay
line are converted to octal.

hdz7

hdz6
? --

(cid:8)

-

hdz7

hdz6
? --

(cid:8)

-

hdz7

hdz6
? --

(cid:8)

hdz5
?
(cid:8)

hdz4
-

hdz3
?
(cid:8)

hdz2
-

-

(cid:8)6
hdz2

-

(cid:8)6
hdz2

(cid:8)6
hdz5
? -
(cid:8)

(cid:8)6
hdz5
? -
(cid:8)

-

hdz4
? -
(cid:8)

hdz3
?
(cid:8)

-

hdz4
? -
(cid:8)

hdz3
?
(cid:8)

(a)

(b)

(c)

‘ -

hd (cid:27)
z1
? -
(cid:8)

z0
? -
(cid:8)

Octal name

t(a)
s

t(b)

(1; 353)8

-

t(b)

-

z1

(cid:8)6
(cid:8)6
hd (cid:27)
-

z0
? -
(cid:8)

s

t(a)

(247; 371)8

-

(cid:8)6
hd
-

z1

(cid:8)6

z0
6
(cid:27)
(cid:8)

-

t(b)

t(a)
s

‘ -

(cid:0)1; 247
371(cid:1)8

these (cid:12)lters require k = 7 bits of memory, the codes they de(cid:12)ne are known as
a constraint-length 7 codes.

Convolutional codes come in three (cid:13)avours, corresponding to the three

types of (cid:12)lter in (cid:12)gure 48.1.

Systematic nonrecursive

The (cid:12)lter shown in (cid:12)gure 48.1a has no feedback. It also has the property that
one of the output bits, t(a), is identical to the source bit s. This encoder is
thus called systematic, because the source bits are reproduced transparently
in the transmitted stream, and nonrecursive, because it has no feedback. The
other transmitted bit t(b) is a linear function of the state of the (cid:12)lter. One
way of describing that function is as a dot product (modulo 2) between two
binary vectors of length k + 1: a binary vector g(b) = (1; 1; 1; 0; 1; 0; 1; 1) and
the state vector z = (zk; zk(cid:0)1; : : : ; z1; z0). We include in the state vector the
bit z0 that will be put into the (cid:12)rst bit of the memory on the next cycle. The
vector g(b) has g(b)
(cid:20) = 1 for every (cid:20) where there is a tap (a downward pointing
arrow) from state bit z(cid:20) into the transmitted bit t(b).

A convenient way to describe these binary tap vectors is in octal. Thus,
I have drawn the delay lines from

this (cid:12)lter makes use of the tap vector 3538.
right to left to make it easy to relate the diagrams to these octal numbers.

Nonsystematic nonrecursive

The (cid:12)lter shown in (cid:12)gure 48.1b also has no feedback, but it is not systematic.
It makes use of two tap vectors g(a) and g(b) to create its two transmitted bits.
This encoder is thus nonsystematic and nonrecursive. Because of their added
complexity, nonsystematic codes can have error-correcting abilities superior to
those of systematic nonrecursive codes with the same constraint length.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

576

Systematic recursive

48 | Convolutional Codes and Turbo Codes

The (cid:12)lter shown in (cid:12)gure 48.1c is similar to the nonsystematic nonrecursive
(cid:12)lter shown in (cid:12)gure 48.1b, but it uses the taps that formerly made up g(a)
to make a linear signal that is fed back into the shift register along with the
source bit. The output t(b) is a linear function of the state vector as before.
The other output is t(a) = s, so this (cid:12)lter is systematic.

A recursive code is conventionally identi(cid:12)ed by an octal ratio, e.g., (cid:12)g-

ure 48.1c’s code is denoted by (247=371)8.

Equivalence of systematic recursive and nonsystematic nonrecursive codes

The two (cid:12)lters in (cid:12)gure 48.1b,c are code-equivalent in that the sets of code-
words that they de(cid:12)ne are identical. For every codeword of the nonsystematic
nonrecursive code we can choose a source stream for the other encoder such
that its output is identical (and vice versa).

(cid:20)=1 g(a)

To prove this, we denote by p the quantity Pk

(cid:20) z(cid:20), as shown in (cid:12)g-
ure 48.3a and b, which shows a pair of smaller but otherwise equivalent (cid:12)lters.
If the two transmissions are to be equivalent { that is, the t(a)s are equal in
both (cid:12)gures and so are the t(b)s { then on every cycle the source bit in the
systematic code must be s = t(a). So now we must simply con(cid:12)rm that for
this choice of s, the systematic code’s shift register will follow the same state
sequence as that of the nonsystematic code, assuming that the states match
initially. In (cid:12)gure 48.3a we have

-

t(b)

-

(cid:8)6
hd (cid:27)

hdz2

z1

z0
? -
(cid:8)

? --
p
(a) (5; 7)8

(cid:8)

s

t(a)

-

(cid:8)6

-

t(b)

hdz2

z1

hd
? --
p

z0
‘ -
6
(cid:27)
(cid:8)
7(cid:1)8
(b) (cid:0)1; 5

(cid:8)

t(a)
s

Figure 48.3. Two rate-1/2
convolutional codes with
constraint length k = 2:
(a) non-recursive; (b) recursive.
The two codes are equivalent.

t(a) = p (cid:8) znonrecursive

0

whereas in (cid:12)gure 48.3b we have

zrecursive
0

= t(a) (cid:8) p:

Substituting for t(a), and using p (cid:8) p = 0 we immediately (cid:12)nd

zrecursive
0

= znonrecursive

0

:

(48.1)

(48.2)

(48.3)

Thus, any codeword of a nonsystematic nonrecursive code is a codeword of
a systematic recursive code with the same taps { the same taps in the sense
that there are vertical arrows in all the same places in (cid:12)gures 48.3(a) and (b),
though one of the arrows points up instead of down in (b).

Now, while these two codes are equivalent, the two encoders behave dif-
ferently. The nonrecursive encoder has a (cid:12)nite impulse response, that is, if
one puts in a string that is all zeroes except for a single one, the resulting
output stream contains a (cid:12)nite number of ones. Once the one bit has passed
through all the states of the memory, the delay line returns to the all-zero
state. Figure 48.4a shows the state sequence resulting from the source string
s =(0, 0, 1, 0, 0, 0, 0, 0).

Figure 48.4b shows the trellis of the recursive code of (cid:12)gure 48.3b and the
response of this (cid:12)lter to the same source string s =(0, 0, 1, 0, 0, 0, 0, 0). The
(cid:12)lter has an in(cid:12)nite impulse response. The response settles into a periodic
state with period equal to three clock cycles.

. Exercise 48.1.[1 ] What is the input to the recursive (cid:12)lter such that its state
sequence and the transmission are the same as those of the nonrecursive
(cid:12)lter? (Hint: see (cid:12)gure 48.5.)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

48.2: Linear-feedback shift-registers

577

(a)

(b)

11

10

01

00

transmit

0

0

source 0

0

0

0

1

1

1

0

1

0

1

0

1

0

0

0

0

0

0

0

0

0

11

10

01

00

transmit

0

0

source 0

0

0

0

1

1

1

1

0

0

0

0

1

0

0

0

0

0

1

0

0

1

11

10

01

00

transmit

0

0

source 0

0

0

0

1

1

1

0

1

1

1

1

1

0

0

0

0

0

0

0

0

0

Figure 48.4. Trellises of the
rate-1/2 convolutional codes of
(cid:12)gure 48.3. It is assumed that the
initial state of the (cid:12)lter is
(z2; z1) = (0; 0). Time is on the
horizontal axis and the state of
the (cid:12)lter at each time step is the
vertical coordinate. On the line
segments are shown the emitted
symbols t(a) and t(b), with stars
for ‘1’ and boxes for ‘0’. The
paths taken through the trellises
when the source sequence is
00100000 are highlighted with a
solid line. The light dotted lines
show the state trajectories that
are possible for other source
sequences.

Figure 48.5. The source sequence
for the systematic recursive code
00111000 produces the same path
through the trellis as 00100000
does in the nonsystematic
nonrecursive case.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

578

48 | Convolutional Codes and Turbo Codes

-

hdz4

hdz3
? --
(cid:8)

hdz2
? -
(cid:8)

z1
hd
? -
(cid:8)

(cid:8)6
z0
6
(cid:27)
(cid:8)

-

t(b)

t(a)
s

‘ -

(cid:0)1; 21
37(cid:1)8

Figure 48.6. The trellis for a k = 4
code painted with the likelihood
function when the received vector
is equal to a codeword with just
one bit (cid:13)ipped. There are three
line styles, depending on the value
of the likelihood: thick solid lines
show the edges in the trellis that
match the corresponding two bits
of the received string exactly;
thick dotted lines show edges that
match one bit but mismatch the
other; and thin dotted lines show
the edges that mismatch both
bits.

0

1

1

0

1

0

1

0

0

0

0

1

1

1

1

0

1111
1110
1101
1100
1011
1010
1001
1000
0111
0110
0101
0100
0011
0010
0001
0000
received

In general a linear-feedback shift-register with k bits of memory has an impulse
response that is periodic with a period that is at most 2k (cid:0) 1, corresponding
to the (cid:12)lter visiting every non-zero state in its state space.

Incidentally, cheap pseudorandom number generators and cheap crypto-
graphic products make use of exactly these periodic sequences, though with
larger values of k than 7; the random number seed or cryptographic key se-
lects the initial state of the memory. There is thus a close connection between
certain cryptanalysis problems and the decoding of convolutional codes.

48.3 Decoding convolutional codes

The receiver receives a bit stream, and wishes to infer the state sequence
and thence the source stream. The posterior probability of each bit can be
found by the sum{product algorithm (also known as the forward{backward or
BCJR algorithm), which was introduced in section 25.3. The most probable
state sequence can be found using the min{sum algorithm of section 25.3
(also known as the Viterbi algorithm). The nature of this task is illustrated
in (cid:12)gure 48.6, which shows the cost associated with each edge in the trellis
for the case of a sixteen-state code; the channel is assumed to be a binary
symmetric channel and the received vector is equal to a codeword except that
one bit has been (cid:13)ipped. There are three line styles, depending on the value
of the likelihood: thick solid lines show the edges in the trellis that match the
corresponding two bits of the received string exactly; thick dotted lines show
edges that match one bit but mismatch the other; and thin dotted lines show
the edges that mismatch both bits. The min{sum algorithm seeks the path
through the trellis that uses as many solid lines as possible; more precisely, it
minimizes the cost of the path, where the cost is zero for a solid line, one for
a thick dotted line, and two for a thin dotted line.

. Exercise 48.2.[1, p.581] Can you spot the most probable path and the (cid:13)ipped

bit?

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

579

Figure 48.7. Two paths that di(cid:11)er
in two transmitted bits only.

1

1
1

0

0

1
1

1
1

0

0

0
0

0
0

1

1
1

1

1
1

0

1

1
1

0

0

1
1

1
1

0

0

0
0

0
0

1

1
1

1

0
0

1

Figure 48.8. A terminated trellis.
When any codeword is completed,
the (cid:12)lter state is 0000.

48.4: Turbo codes

1111
1110
1101
1100
1011
1010
1001
1000
0111
0110
0101
0100
0011
0010
0001
0000
transmit
1
source 1

1111
1110
1101
1100
1011
1010
1001
1000
0111
0110
0101
0100
0011
0010
0001
0000
transmit
1
source 1

1111
1110
1101
1100
1011
1010
1001
1000
0111
0110
0101
0100
0011
0010
0001
0000

Unequal protection

A defect of the convolutional codes presented thus far is that they o(cid:11)er un-
equal protection to the source bits. Figure 48.7 shows two paths through the
trellis that di(cid:11)er in only two transmitted bits. The last source bit is less well
protected than the other source bits. This unequal protection of bits motivates
the termination of the trellis.

A terminated trellis is shown in (cid:12)gure 48.8. Termination slightly reduces
the number of source bits used per codeword. Here, four source bits are turned
into parity bits because the k = 4 memory bits must be returned to zero.

48.4 Turbo codes

An (N; K) turbo code is de(cid:12)ned by a number of constituent convolutional
encoders (often, two) and an equal number of interleavers which are K (cid:2) K
permutation matrices. Without loss of generality, we take the (cid:12)rst interleaver
to be the identity matrix. A string of K source bits is encoded by feeding them
into each constituent encoder in the order de(cid:12)ned by the associated interleaver,
and transmitting the bits that come out of each constituent encoder. Often
the (cid:12)rst constituent encoder is chosen to be a systematic encoder, just like the
recursive (cid:12)lter shown in (cid:12)gure 48.6, and the second is a non-systematic one of
rate 1 that emits parity bits only. The transmitted codeword then consists of

-

C1

-

-

-

-

C2(cid:14)(cid:13)(cid:15)(cid:12)(cid:25)

Figure 48.10. The encoder of a
turbo code. Each box C1, C2,
contains a convolutional code.
The source bits are reordered
using a permutation (cid:25) before they
are fed to C2. The transmitted
codeword is obtained by
concatenating or interleaving the
outputs of the two convolutional
codes.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

580

48 | Convolutional Codes and Turbo Codes

(a)

(b)

Figure 48.9. Rate-1/3 (a) and rate-1/2 (b) turbo codes represented as factor graphs. The circles
represent the codeword bits. The two rectangles represent trellises of rate-1/2 convolutional
codes, with the systematic bits occupying the left half of the rectangle and the parity bits
occupying the right half. The puncturing of these constituent codes in the rate-1/2 turbo
code is represented by the lack of connections to half of the parity bits in each trellis.

K source bits followed by M1 parity bits generated by the (cid:12)rst convolutional
code and M2 parity bits from the second. The resulting turbo code has rate
1/3.

The turbo code can be represented by a factor graph in which the two
trellises are represented by two large rectangular nodes ((cid:12)gure 48.9a); the K
source bits and the (cid:12)rst M1 parity bits participate in the (cid:12)rst trellis and the K
source bits and the last M2 parity bits participate in the second trellis. Each
codeword bit participates in either one or two trellises, depending on whether
it is a parity bit or a source bit. Each trellis node contains a trellis exactly like
the terminated trellis shown in (cid:12)gure 48.8, except one thousand times as long.
[There are other factor graph representations for turbo codes that make use
of more elementary nodes, but the factor graph given here yields the standard
version of the sum{product algorithm used for turbo codes.]

If a turbo code of smaller rate such as 1/2 is required, a standard modi(cid:12)ca-
tion to the rate-1/3 code is to puncture some of the parity bits ((cid:12)gure 48.9b).
Turbo codes are decoded using the sum{product algorithm described in
Chapter 26. On the (cid:12)rst iteration, each trellis receives the channel likelihoods,
and runs the forward{backward algorithm to compute, for each bit, the relative
likelihood of its being 1 or 0, given the information about the other bits.
These likelihoods are then passed across from each trellis to the other, and
multiplied by the channel likelihoods on the way. We are then ready for the
second iteration: the forward{backward algorithm is run again in each trellis
using the updated probabilities. After about ten or twenty such iterations, it’s
hoped that the correct decoding will be found. It is common practice to stop
after some (cid:12)xed number of iterations, but we can do better.

As a stopping criterion, the following procedure can be used at every iter-
ation. For each time-step in each trellis, we identify the most probable edge,
according to the local messages. If these most probable edges join up into two
valid paths, one in each trellis, and if these two paths are consistent with each
other, it is reasonable to stop, as subsequent iterations are unlikely to take
the decoder away from this codeword. If a maximum number of iterations is
reached without this stopping criterion being satis(cid:12)ed, a decoding error can
be reported. This stopping procedure is recommended for several reasons: it
allows a big saving in decoding time with no loss in error probability; it allows
decoding failures that are detected by the decoder to be so identi(cid:12)ed { knowing
that a particular block is de(cid:12)nitely corrupted is surely useful information for
the receiver! And when we distinguish between detected and undetected er-
rors, the undetected errors give helpful insights into the low-weight codewords

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

48.5: Parity-check matrices of convolutional codes and turbo codes

581

of the code, which may improve the process of code design.

Turbo codes as described here have excellent performance down to decoded
error probabilities of about 10(cid:0)5, but randomly-constructed turbo codes tend
to have an error (cid:13)oor starting at that level. This error (cid:13)oor is caused by low-
weight codewords. To reduce the height of the error (cid:13)oor, one can attempt
to modify the random construction to increase the weight of these low-weight
codewords. The tweaking of turbo codes is a black art, and it never succeeds
in totalling eliminating low-weight codewords; more precisely, the low-weight
codewords can be eliminated only by sacri(cid:12)cing the turbo code’s excellent per-
formance. In contrast, low-density parity-check codes rarely have error (cid:13)oors,
as long as their number of weight{2 columns is not too large (cf. exercise 47.3,
p.572).

(a)

(b)

Figure 48.11. Schematic pictures
of the parity-check matrices of (a)
a convolutional code, rate 1/2,
and (b) a turbo code, rate 1/3.
Notation: A diagonal line
represents an identity matrix. A
band of diagonal lines represent a
band of diagonal 1s. A circle
inside a square represents the
random permutation of all the
columns in that square. A number
inside a square represents the
number of random permutation
matrices superposed in that
square. Horizontal and vertical
lines indicate the boundaries of
the blocks within the matrix.

48.5 Parity-check matrices of convolutional codes and turbo codes

We close by discussing the parity-check matrix of a rate-1/2 convolutional code
viewed as a linear block code. We adopt the convention that the N bits of one
block are made up of the N=2 bits t(a) followed by the N=2 bits t(b).

. Exercise 48.3.[2 ] Prove that a convolutional code has a low-density parity-

check matrix as shown schematically in (cid:12)gure 48.11a.

Hint: It’s easiest to (cid:12)gure out the parity constraints satis(cid:12)ed by a convo-
lutional code by thinking about the nonsystematic nonrecursive encoder
((cid:12)gure 48.1b). Consider putting through (cid:12)lter a a stream that’s been
through convolutional (cid:12)lter b, and vice versa; compare the two resulting
streams. Ignore termination of the trellises.

The parity-check matrix of a turbo code can be written down by listing the
constraints satis(cid:12)ed by the two constituent trellises ((cid:12)gure 48.11b). So turbo
codes are also special cases of low-density parity-check codes. If a turbo code
is punctured, it no longer necessarily has a low-density parity-check matrix,
but it always has a generalized parity-check matrix that is sparse, as explained
in the next chapter.

Further reading

For further reading about convolutional codes, Johannesson and Zigangirov
(1999) is highly recommended. One topic I would have liked to include is
sequential decoding. Sequential decoding explores only the most promising
paths in the trellis, and backtracks when evidence accumulates that a wrong
turning has been taken. Sequential decoding is used when the trellis is too
big for us to be able to apply the maximum likelihood algorithm, the min{
sum algorithm. You can read about sequential decoding in Johannesson and
Zigangirov (1999).

For further information about the use of the sum{product algorithm in
turbo codes, and the rarely-used but highly recommended stopping criteria
for halting their decoding, Frey (1998) is essential reading. (And there’s lots
more good stu(cid:11) in the same book!)

48.6 Solutions

Solution to exercise 48.2 (p.578). The (cid:12)rst bit was (cid:13)ipped. The most probable
path is the upper one in (cid:12)gure 48.7.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

49

Repeat{Accumulate Codes

In Chapter 1 we discussed a very simple and not very e(cid:11)ective method for
communicating over a noisy channel: the repetition code. We now discuss a
code that is almost as simple, and whose performance is outstandingly good.
Repeat{accumulate codes were studied by Divsalar et al. (1998) for theo-
retical purposes, as simple turbo-like codes that might be more amenable to
analysis than messy turbo codes. Their practical performance turned out to
be just as good as other sparse-graph codes.

49.1 The encoder

1. Take K source bits.

s1s2s3 : : : sK

2. Repeat each bit three times, giving N = 3K bits.

s1s1s1s2s2s2s3s3s3 : : : sKsKsK

3. Permute these N bits using a random permutation (a (cid:12)xed random
permutation { the same one for every codeword). Call the permuted
string u.

u1u2u3u4u5u6u7u8u9 : : : uN

4. Transmit the accumulated sum.

t1 = u1
t2 = t1 + u2 (mod 2)
tn = tn(cid:0)1 + un (mod 2)
tN = tN(cid:0)1 + uN (mod 2):

: : :

: : :

(49.1)

5. That’s it!

49.2 Graph

Figure 49.1a shows the graph of a repeat{accumulate code, using four types
of node: equality constraints
, intermediate binary variables (black circles),
parity constraints

, and the transmitted bits (white circles).

The source sets the values of the black bits at the bottom, three at a time,

and the accumulator computes the transmitted bits along the top.

582

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

49.3: Decoding

(a)

(b)

1

1

0

0

1

1

0

0

1

1

0

0

1

1

0

0

1

1

0

0

1

0

1

0.1

0.01

0.001

0.0001

1e-05

total
undetected

N=204

408

816

9999

3000

3

4

5

N=30000
1
2

583

Figure 49.1. Factor graphs for a
repeat{accumulate code with rate
1/3. (a) Using elementary nodes.
Each white circle represents a
transmitted bit. Each
constraint forces the sum of the 3
bits to which it is connected to be
even. Each black circle represents
an intermediate binary variable.
Each
variables to which it is connected
to be equal.
(b) Factor graph normally used
for decoding. The top rectangle
represents the trellis of the
accumulator, shown in the inset.

constraint forces the three

Figure 49.2. Performance of six
rate-1/3 repeat{accumulate codes
on the Gaussian channel. The
blocklengths range from N = 204
to N = 30 000. Vertical axis:
block error probability; horizontal
axis: Eb=N0. The dotted lines
show the frequency of undetected
errors.

This graph is a factor graph for the prior probability over codewords,
with the circles being binary variable nodes, and the squares representing
contributes a factor of the form
two types of factor nodes. As usual, each

contributes a factor of the form  [x1 = x2 = x3].

 [P x = 0 mod 2]; each

49.3 Decoding

The repeat{accumulate code is normally decoded using the sum{product algo-
rithm on the factor graph depicted in (cid:12)gure 49.1b. The top box represents the
trellis of the accumulator, including the channel likelihoods. In the (cid:12)rst half
of each iteration, the top trellis receives likelihoods for every transition in the
trellis, and runs the forward{backward algorithm so as to produce likelihoods
for each variable node. In the second half of the iteration, these likelihoods
nodes to produce new likelihood messages to
are multiplied together at the
send back to the trellis.

As with Gallager codes and turbo codes, the stop-when-it’s-done decoding
method can be applied, so it is possible to distinguish between undetected
errors (which are caused by low-weight codewords in the code) and detected
errors (where the decoder gets stuck and knows that it has failed to (cid:12)nd a
valid answer).

Figure 49.2 shows the performance of six randomly-constructed repeat{
accumulate codes on the Gaussian channel. If one does not mind the error
(cid:13)oor which kicks in at about a block error probability of 10(cid:0)4, the performance
is staggeringly good for such a simple code (cf. (cid:12)gure 47.17).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

584

1

0.1

0.01

0.001

0.0001

total
detected
undetected

1e-05

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1.1

(a)

2000

1800

1600

1400

1200

1000

800

600

400

200

0

0

1000

100

10

1

10

49 | Repeat{Accumulate Codes

3000

2500

2000

1500

1000

500

0

0

1000

100

10

1

10

60

40

20
(ii.b) Eb=N0 = 0:749 dB

80

100 120 140 160 180

20

30

40

50 60 70 80 90100

(iii.b)

60

40

20
(ii.c) Eb=N0 = 0:846 dB

80

100 120 140 160 180

20

30

40

50

60

70 80 90 100

(iii.c)

49.4 Empirical distribution of decoding times

It is interesting to study the number of iterations (cid:28) of the sum{product algo-
rithm required to decode a sparse-graph code. Given one code and a set of
channel conditions, the decoding time varies randomly from trial to trial. We
(cid:12)nd that the histogram of decoding times follows a power law, P ((cid:28) ) / (cid:28) (cid:0)p,
for large (cid:28) . The power p depends on the signal-to-noise ratio and becomes
smaller (so that the distribution is more heavy-tailed) as the signal-to-noise
ratio decreases. We have observed power laws in repeat{accumulate codes
and in irregular and regular Gallager codes. Figures 49.3(ii) and (iii) show the
distribution of decoding times of a repeat{accumulate code at two di(cid:11)erent
signal-to-noise ratios. The power laws extend over several orders of magnitude.

Exercise 49.1.[5 ] Investigate these power laws. Does density evolution predict
them? Can the design of a code be used to manipulate the power law in
a useful way?

49.5 Generalized parity-check matrices

Figure 49.3. Histograms of
number of iterations to (cid:12)nd a
valid decoding for a
repeat{accumulate code with
source block length K = 10 000
and transmitted blocklength
N = 30 000. (a) Block error
probability versus signal-to-noise
ratio for the RA code. (ii.b)
Histogram for x=(cid:27) = 0:89,
Eb=N0 = 0:749 dB. (ii.c)
x=(cid:27) = 0:90, Eb=N0 = 0:846 dB.
(iii.b, iii.c) Fits of power laws to
(ii.b) (1=(cid:28) 6) and (ii.c) (1=(cid:28) 9).

and

node to a

I (cid:12)nd that it is helpful when relating sparse-graph codes to each other to use
a common representation for them all. Forney (2001) introduced the idea of
and all variable nodes
a normal graph in which the only nodes are
have degree one or two; variable nodes with degree two can be represented on
node. The generalized parity-check matrix
edges that connect a
is a graphical way of representing normal graphs. In a parity-check matrix,
the columns are transmitted bits, and the rows are linear constraints. In a
generalized parity-check matrix, additional columns may be included, which
represent state variables that are not transmitted. One way of thinking of these
state variables is that they are punctured from the code before transmission.
State variables are indicated by a horizontal line above the corresponding
columns. The other pieces of diagrammatic notation for generalized parity-

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

49.5: Generalized parity-check matrices

585

GT =

H =

Figure 49.4. The generator
matrix, parity-check matrix, and a
generalized parity-check matrix of
a repetition code with rate 1/3.

fA; pg =

check matrices are, as in (MacKay, 1999b; MacKay et al., 1998):

(cid:15) A diagonal line in a square indicates that that part of the matrix contains

an identity matrix.

(cid:15) Two or more parallel diagonal lines indicate a band-diagonal matrix with

a corresponding number of 1s per row.

(cid:15) A horizontal ellipse with an arrow on it indicates that the corresponding

columns in a block are randomly permuted.

(cid:15) A vertical ellipse with an arrow on it indicates that the corresponding

rows in a block are randomly permuted.

(cid:15) An integer surrounded by a circle represents that number of superposed

random permutation matrices.

De(cid:12)nition. A generalized parity-check matrix is a pair fA; pg, where A is a
binary matrix and p is a list of the punctured bits. The matrix de(cid:12)nes a set
of valid vectors x, satisfying

Ax = 0;

(49.2)

for each valid vector there is a codeword t(x) that is obtained by puncturing
from x the bits indicated by p. For any one code there are many generalized
parity-check matrices.

The rate of a code with generalized parity-check matrix fA; pg can be
estimated as follows. If A is L (cid:2) M 0, and p punctures S bits and selects N
bits for transmission (L = N + S), then the e(cid:11)ective number of constraints on
the codeword, M , is

the number of source bits is

M = M0 (cid:0) S;

K = N (cid:0) M = L (cid:0) M0;

and the rate is greater than or equal to

R = 1 (cid:0)

M
N

= 1 (cid:0)

M0 (cid:0) S
L (cid:0) S

:

(49.3)

(49.4)

(49.5)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

586

GT =

GT =

3

3

3

3

H =

A; p =

3

3

3

3

49 | Repeat{Accumulate Codes

Figure 49.5. The generator matrix
and parity-check matrix of a
systematic low-density
generator-matrix code. The code
has rate 1/3.

Figure 49.6. The generator matrix
and generalized parity-check
matrix of a non-systematic
low-density generator-matrix
code. The code has rate 1/2.

Examples

Repetition code. The generator matrix, parity-check matrix, and generalized
parity-check matrix of a simple rate-1/3 repetition code are shown in (cid:12)gure 49.4.

In an (N; K) systematic low-
Systematic low-density generator-matrix code.
density generator-matrix code, there are no state variables. A transmitted
codeword t of length N is given by

where

t = GTs;

GT =(cid:20) IK
P (cid:21) ;

(49.6)

(49.7)

with IK denoting the K(cid:2)K identity matrix, and P being a very sparse M (cid:2)K
matrix, where M = N (cid:0) K. The parity-check matrix of this code is

H = [PjIM ]:

(49.8)

In the case of a rate-1/3 code, this parity-check matrix might be represented
as shown in (cid:12)gure 49.5.

Non-systematic low-density generator-matrix code. In an (N; K) non-systematic
low-density generator-matrix code, a transmitted codeword t of length N is
given by

(49.9)
where GT is a very sparse N (cid:2) K matrix. The generalized parity-check matrix
of this code is
(49.10)

t = GTs;

and the corresponding generalized parity-check equation is

A =(cid:2)GTjIN(cid:3) ;

Ax = 0; where x =(cid:20) s
t (cid:21).

(49.11)

Whereas the parity-check matrix of this simple code is typically a com-
plex, dense matrix, the generalized parity-check matrix retains the underlying
simplicity of the code.

In the case of a rate-1/2 code, this generalized parity-check matrix might

be represented as shown in (cid:12)gure 49.6.

Low-density parity-check codes and linear MN codes. The parity-check matrix
of a rate-1/3 low-density parity-check code is shown in (cid:12)gure 49.7a.

3

3

(a)

(b)

Figure 49.7. The generalized
parity-check matrices of (a) a
rate-1/3 Gallager code with M=2
columns of weight 2; (b) a rate-1/2
linear MN code.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

49.5: Generalized parity-check matrices

587

Figure 49.8. The generalized
parity-check matrices of (a) a
convolutional code with rate 1/2.
(b) a rate-1/3 turbo code built by
parallel concatenation of two
convolutional codes.

(a)

(b)

A linear MN code is a non-systematic low-density parity-check code. The
K state bits of an MN code are the source bits. Figure 49.7b shows the
generalized parity-check matrix of a rate-1=2 linear MN code.

In a non-systematic, non-recursive convolutional code,
Convolutional codes.
the source bits, which play the role of state bits, are fed into a delay-line and
two linear functions of the delay-line are transmitted. In (cid:12)gure 49.8a, these
two parity streams are shown as two successive vectors of length K.
[It is
common to interleave these two parity streams, a bit-reordering that is not
relevant here, and is not illustrated.]

Concatenation.
‘Parallel concatenation’ of two codes is represented in one of
these diagrams by aligning the matrices of two codes in such a way that the
‘source bits’ line up, and by adding blocks of zero-entries to the matrix such
that the state bits and parity bits of the two codes occupy separate columns.
An example is given by the turbo code that follows. In ‘serial concatenation’,
the columns corresponding to the transmitted bits of the (cid:12)rst code are aligned
with the columns corresponding to the source bits of the second code.

Turbo codes. A turbo code is the parallel concatenation of two convolutional
codes. The generalized parity-check matrix of a rate-1/3 turbo code is shown
in (cid:12)gure 49.8b.

Repeat{accumulate codes. The generalized parity-check matrices of a rate-1/3
repeat{accumulate code is shown in (cid:12)gure 49.9. Repeat-accumulate codes are
equivalent to staircase codes (section 47.7, p.569).

Intersection. The generalized parity-check matrix of the intersection of two
codes is made by stacking their generalized parity-check matrices on top of
each other in such a way that all the transmitted bits’ columns are correctly
aligned, and any punctured bits associated with the two component codes
occupy separate columns.

Figure 49.9. The generalized
parity-check matrix of a
repeat{accumulate code with rate
1/3.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

About Chapter 50

The following exercise provides a helpful background for digital fountain codes.

. Exercise 50.1.[3 ] An author proofreads his K = 700-page book by inspecting
random pages. He makes N page-inspections, and does not take any
precautions to avoid inspecting the same page twice.

(a) After N = K page-inspections, what fraction of pages do you expect

have never been inspected?

(b) After N > K page-inspections, what is the probability that one or

more pages have never been inspected?

(c) Show that in order for the probability that all K pages have been
inspected to be 1 (cid:0) (cid:14), we require N ’ K ln(K=(cid:14)) page-inspections.
[This problem is commonly presented in terms of throwing N balls at
random into K bins; what’s the probability that every bin gets at least
one ball?]

588

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

50

Digital Fountain Codes

Digital fountain codes are record-breaking sparse-graph codes for channels
with erasures.

Channels with erasures are of great importance. For example, (cid:12)les sent
over the internet are chopped into packets, and each packet is either received
without error or not received. A simple channel model describing this situation
is a q-ary erasure channel, which has (for all inputs in the input alphabet
f0; 1; 2; : : : ; q(cid:0)1g) a probability 1(cid:0) f of transmitting the input without error,
and probability f of delivering the output ‘?’. The alphabet size q is 2l, where
l is the number of bits in a packet.

Common methods for communicating over such channels employ a feed-
back channel from receiver to sender that is used to control the retransmission
of erased packets. For example, the receiver might send back messages that
identify the missing packets, which are then retransmitted. Alternatively, the
receiver might send back messages that acknowledge each received packet; the
sender keeps track of which packets have been acknowledged and retransmits
the others until all packets have been acknowledged.

These simple retransmission protocols have the advantage that they will
work regardless of the erasure probability f , but purists who have learned their
Shannon theory will feel that these retransmission protocols are wasteful. If
the erasure probability f is large, the number of feedback messages sent by
the (cid:12)rst protocol will be large. Under the second protocol, it’s likely that the
receiver will end up receiving multiple redundant copies of some packets, and
heavy use is made of the feedback channel. According to Shannon, there is no
need for the feedback channel: the capacity of the forward channel is (1 (cid:0) f )l
bits, whether or not we have feedback.
The wastefulness of the simple retransmission protocols is especially evi-
dent in the case of a broadcast channel with erasures { channels where one
sender broadcasts to many receivers, and each receiver receives a random
fraction (1 (cid:0) f ) of the packets. If every packet that is missed by one or more
receivers has to be retransmitted, those retransmissions will be terribly re-
dundant. Every receiver will have already received most of the retransmitted
packets.

So, we would like to make erasure-correcting codes that require no feed-
back or almost no feedback. The classic block codes for erasure correction are
called Reed{Solomon codes. An (N; K) Reed{Solomon code (over an alpha-
bet of size q = 2l) has the ideal property that if any K of the N transmitted
symbols are received then the original K source symbols can be recovered.
[See Berlekamp (1968) or Lin and Costello (1983) for further information;
Reed{Solomon codes exist for N < q.] But Reed{Solomon codes have the
disadvantage that they are practical only for small K, N , and q: standard im-

589

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

590

50 | Digital Fountain Codes

LT stands for ‘Luby transform’.

plementations of encoding and decoding have a cost of order K(N(cid:0)K) log 2 N
packet operations. Furthermore, with a Reed{Solomon code, as with any block
code, one must estimate the erasure probability f and choose the code rate
R = K=N before transmission. If we are unlucky and f is larger than expected
and the receiver receives fewer than K symbols, what are we to do? We’d like
a simple way to extend the code on the (cid:13)y to create a lower-rate (N 0; K) code.
For Reed{Solomon codes, no such on-the-(cid:13)y method exists.

There is a better way, pioneered by Michael Luby (2002) at his company
Digital Fountain, the (cid:12)rst company whose business is based on sparse-graph
codes.

The digital fountain codes I describe here, LT codes, were invented by
Luby in 1998. The idea of a digital fountain code is as follows. The encoder is
a fountain that produces an endless supply of water drops (encoded packets);
let’s say the original source (cid:12)le has a size of Kl bits, and each drop contains
l encoded bits. Now, anyone who wishes to receive the encoded (cid:12)le holds a
bucket under the fountain and collects drops until the number of drops in the
bucket is a little larger than K. They can then recover the original (cid:12)le.

Digital fountain codes are rateless in the sense that the number of encoded
packets that can be generated from the source message is potentially limitless;
and the number of encoded packets generated can be determined on the (cid:13)y.
Regardless of the statistics of the erasure events on the channel, we can send
as many encoded packets as are needed in order for the decoder to recover
the source data. The source data can be decoded from any set of K 0 encoded
packets, for K0 slightly larger than K (in practice, about 5% larger).

Digital fountain codes also have fantastically small encoding and decod-
ing complexities. With probability 1 (cid:0) (cid:14), K packets can be communicated
with average encoding and decoding costs both of order K ln(K=(cid:14)) packet
operations.

Luby calls these codes universal because they are simultaneously near-
optimal for every erasure channel, and they are very e(cid:14)cient as the (cid:12)le length

K grows. The overhead K0 (cid:0) K is of order pK(ln(K=(cid:14)))2.

50.1 A digital fountain’s encoder

Each encoded packet tn is produced from the source (cid:12)le s1s2s3 : : : sK as
follows:

1. Randomly choose the degree dn of the packet from a degree distri-
bution (cid:26)(d); the appropriate choice of (cid:26) depends on the source (cid:12)le
size K, as we’ll discuss later.

2. Choose, uniformly at random, dn distinct input packets, and set tn
equal to the bitwise sum, modulo 2 of those dn packets. This sum
can be done by successively exclusive-or-ing the packets together.

This encoding operation de(cid:12)nes a graph connecting encoded packets to
source packets. If the mean degree (cid:22)d is signi(cid:12)cantly smaller than K then the
graph is sparse. We can think of the resulting code as an irregular low-density
generator-matrix code.

The decoder needs to know the degree of each packet that is received, and
which source packets it is connected to in the graph. This information can
be communicated to the decoder in various ways. For example, if the sender
and receiver have synchronized clocks, they could use identical pseudo-random

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

50.2: The decoder

591

s1

s2 s3

a)

1

0

11

1

b)

0

11

1

c)

1

01

01

d)

1 1

01

1 1

01

1

e)

f)

Figure 50.1. Example decoding for
a digital fountain code with
K = 3 source bits and N = 4
encoded bits.

number generators, seeded by the clock, to choose each random degree and
each set of connections. Alternatively, the sender could pick a random key,
(cid:20)n, given which the degree and the connections are determined by a pseudo-
random process, and send that key in the header of the packet. As long as the
packet size l is much bigger than the key size (which need only be 32 bits or
so), this key introduces only a small overhead cost.

50.2 The decoder

Decoding a sparse-graph code is especially easy in the case of an erasure chan-
nel. The decoder’s task is to recover s from t = Gs, where G is the matrix
associated with the graph. The simple way to attempt to solve this prob-
lem is by message-passing. We can think of the decoding algorithm as the
sum{product algorithm if we wish, but all messages are either completely un-
certain messages or completely certain messages. Uncertain messages assert
that a message packet sk could have any value, with equal probability; certain
messages assert that sk has a particular value, with probability one.

This simplicity of the messages allows a simple description of the decoding

process. We’ll call the encoded packets ftng check nodes.

1. Find a check node tn that is connected to only one source packet
sk. (If there is no such check node, this decoding algorithm halts at
this point, and fails to recover all the source packets.)

(a) Set sk = tn.
(b) Add sk to all checks tn0 that are connected to sk:

tn0 := tn0 + sk

for all n0 such that Gn0k = 1.

(50.1)

(c) Remove all the edges connected to the source packet sk.

2. Repeat (1) until all fskg are determined.

This decoding process is illustrated in (cid:12)gure 50.1 for a toy case where each
packet is just one bit. There are three source packets (shown by the upper
circles) and four received packets (shown by the lower check symbols), which
have the values t1t2t3t4 = 1011 at the start of the algorithm.

At the (cid:12)rst iteration, the only check node that is connected to a sole source
bit is the (cid:12)rst check node (panel a). We set that source bit s1 accordingly
(panel b), discard the check node, then add the value of s1 (1) to the checks to
which it is connected (panel c), disconnecting s1 from the graph. At the start
of the second iteration (panel c), the fourth check node is connected to a sole
source bit, s2. We set s2 to t4 (0, in panel d), and add s2 to the two checks
it is connected to (panel e). Finally, we (cid:12)nd that two check nodes are both
connected to s3, and they agree about the value of s3 (as we would hope!),
which is restored in panel f.

50.3 Designing the degree distribution

The probability distribution (cid:26)(d) of the degree is a critical part of the design:
occasional encoded packets must have high degree (i.e., d similar to K) in
order to ensure that there are not some source packets that are connected to
no-one. Many packets must have low degree, so that the decoding process

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

592

50 | Digital Fountain Codes

can get started, and keep going, and so that the total number of addition
operations involved in the encoding and decoding is kept small. For a given
degree distribution (cid:26)(d), the statistics of the decoding process can be predicted
by an appropriate version of density evolution.

Ideally, to avoid redundancy, we’d like the received graph to have the prop-
erty that just one check node has degree one at each iteration. At each itera-
tion, when this check node is processed, the degrees in the graph are reduced
in such a way that one new degree-one check node appears. In expectation,
this ideal behaviour is achieved by the ideal soliton distribution,

(cid:26)(1) = 1=K
1
(cid:26)(d) =

d(d(cid:0)1)

for d = 2; 3; : : : ; K.

(50.2)

The expected degree under this distribution is roughly ln K.

. Exercise 50.2.[2 ] Derive the ideal soliton distribution. At the (cid:12)rst iteration
(t = 0) let the number of packets of degree d be h0(d); show that (for
d > 1) the expected number of packets of degree d that have their degree
reduced to d (cid:0) 1 is h0(d)d=K; and at the tth iteration, when t of the
K packets have been recovered and the number of packets of degree d
is ht(d), the expected number of packets of degree d that have their
degree reduced to d (cid:0) 1 is ht(d)d=(K (cid:0) t). Hence show that in order
to have the expected number of packets of degree 1 satisfy ht(1) = 1
for all t 2 f0; : : : K (cid:0) 1g, we must to start with have h0(1) = 1 and
h0(2) = K=2; and more generally, ht(2) = (K (cid:0) t)=2; then by recursion
solve for h0(d) for d = 3 upwards.

This degree distribution works poorly in practice, because (cid:13)uctuations
around the expected behaviour make it very likely that at some point in the
decoding process there will be no degree-one check nodes; and, furthermore, a
few source nodes will receive no connections at all. A small modi(cid:12)cation (cid:12)xes
these problems.

The robust soliton distribution has two extra parameters, c and (cid:14); it is

designed to ensure that the expected number of degree-one checks is about

S (cid:17) c ln(K=(cid:14))pK;

(50.3)

rather than 1, throughout the decoding process. The parameter (cid:14) is a bound
on the probability that the decoding fails to run to completion after a certain
number K0 of packets have been received. The parameter c is a constant of
order 1, if our aim is to prove Luby’s main theorem about LT codes; in practice
however it can be viewed as a free parameter, with a value somewhat smaller
than 1 giving good results. We de(cid:12)ne a positive function

1
d
ln(S=(cid:14))

S
K
S
K
0

for d = 1; 2; : : : (K=S)(cid:0)1
for d = K=S
for d > K=S

(50.4)

(cid:28) (d) =8><
>:

(see (cid:12)gure 50.2 and exercise 50.4 (p.594)) then add the ideal soliton distribu-
tion (cid:26) to (cid:28) and normalize to obtain the robust soliton distribution, (cid:22):

(cid:22)(d) =

(cid:26)(d) + (cid:28) (d)

Z

;

(50.5)

where Z = Pd (cid:26)(d) + (cid:28) (d): The number of encoded packets required at the
receiving end to ensure that the decoding can run to completion, with proba-
bility at least 1 (cid:0) (cid:14), is K0 = KZ.

rho
tau

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50

Figure 50.2. The distributions
(cid:26)(d) and (cid:28) (d) for the case
K = 10 000, c = 0:2, (cid:14) = 0:05,
which gives S = 244, K=S = 41,
and Z ’ 1:3. The distribution (cid:28) is
largest at d = 1 and d = K=S.

140
120
100
80
60
40
20
0

11000

10800

10600

10400

10200

10000

delta=0.01
delta=0.1
delta=0.9

0.01

0.1

delta=0.01
delta=0.1
delta=0.9

0.01

0.1
c

Figure 50.3. The number of
degree-one checks S (upper (cid:12)gure)
and the quantity K 0 (lower (cid:12)gure)
as a function of the two
parameters c and (cid:14), for
K = 10 000. Luby’s main theorem
proves that there exists a value of
c such that, given K 0 received
packets, the decoding algorithm
will recover the K source packets
with probability 1 (cid:0) (cid:14).

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

50.4: Applications

593

10000

10500

11000

11500

12000

10000

10500

11000

11500

12000

10000

10500

11000

11500

12000

Figure 50.4. Histograms of the
actual number of packets N
required in order to recover a (cid:12)le
of size K = 10 000 packets. The
parameters were as follows:
top histogram: c = 0:01, (cid:14) = 0:5
(S = 10, K=S = 1010, and
Z ’ 1:01);
middle: c = 0:03, (cid:14) = 0:5 (S = 30,
K=S = 337, and Z ’ 1:03);
bottom: c = 0:1, (cid:14) = 0:5 (S = 99,
K=S = 101, and Z ’ 1:1).

Luby’s (2002) analysis explains how the small-d end of (cid:28) has the role of
ensuring that the decoding process gets started, and the spike in (cid:28) at d = K=S
is included to ensure that every source packet is likely to be connected to a
check at least once. Luby’s key result is that (for an appropriate value of the
constant c) receiving K0 = K + 2 ln(S=(cid:14))S checks ensures that all packets can
be recovered with probability at least 1 (cid:0) (cid:14). In the illustrative (cid:12)gures I have
set the allowable decoder failure probability (cid:14) quite large, because the actual
failure probability is much smaller than is suggested by Luby’s conservative
analysis.

In practice, LT codes can be tuned so that a (cid:12)le of original size K ’ 10 000
packets is recovered with an overhead of about 5%. Figure 50.4 shows his-
tograms of the actual number of packets required for a couple of settings of
the parameters, achieving mean overheads smaller than 5% and 10% respec-
tively.

50.4 Applications

Digital fountain codes are an excellent solution in a wide variety of situations.
Let’s mention two.

Storage

You wish to make a backup of a large (cid:12)le, but you are aware that your magnetic
tapes and hard drives are all unreliable in the sense that catastrophic failures,
in which some stored packets are permanently lost within one device, occur at
a rate of something like 10(cid:0)3 per day. How should you store your (cid:12)le?

A digital fountain can be used to spray encoded packets all over the place,
on every storage device available. Then to recover the backup (cid:12)le, whose size
was K packets, one simply needs to (cid:12)nd K0 ’ K packets from anywhere.
Corrupted packets do not matter; we simply skip over them and (cid:12)nd more
packets elsewhere.

This method of storage also has advantages in terms of speed of (cid:12)le re-
covery.
In a hard drive, it is standard practice to store a (cid:12)le in successive
sectors of a hard drive, to allow rapid reading of the (cid:12)le; but if, as occasion-
ally happens, a packet is lost (owing to the reading head being o(cid:11) track for
a moment, giving a burst of errors that cannot be corrected by the packet’s
error-correcting code), a whole revolution of the drive must be performed to
bring back the packet to the head for a second read. The time taken for one
revolution produces an undesirable delay in the (cid:12)le system.

If (cid:12)les were instead stored using the digital fountain principle, with the
digital drops stored in one or more consecutive sectors on the drive, then one
would never need to endure the delay of re-reading a packet; packet loss would
become less important, and the hard drive could consequently be operated
faster, with higher noise level, and with fewer resources devoted to noisy-
channel coding.

. Exercise 50.3.[2 ] Compare the digital fountain method of robust storage on
multiple hard drives with RAID (the redundant array of independent
disks).

Broadcast

Imagine that ten thousand subscribers in an area wish to receive a digital
movie from a broadcaster. The broadcaster can send the movie in packets

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

594

50 | Digital Fountain Codes

over a broadcast network { for example, by a wide-bandwidth phone line, or
by satellite.

Imagine that not all packets are received at all the houses. Let’s say
f = 0:1% of them are lost at each house. In a standard approach in which the
(cid:12)le is transmitted as a plain sequence of packets with no encoding, each house
would have to notify the broadcaster of the f K missing packets, and request
that they be retransmitted. And with ten thousand subscribers all requesting
such retransmissions, there would be a retransmission request for almost every
packet. Thus the broadcaster would have to repeat the entire broadcast twice
in order to ensure that most subscribers have received the whole movie, and
most users would have to wait roughly twice as long as the ideal time before
the download was complete.

If the broadcaster uses a digital fountain to encode the movie, each sub-
scriber can recover the movie from any K0 ’ K packets. So the broadcast
needs to last for only, say, 1.1K packets, and every house is very likely to have
successfully recovered the whole (cid:12)le.

Another application is broadcasting data to cars. Imagine that we want to
send updates to in-car navigation databases by satellite. There are hundreds
of thousands of vehicles, and they can receive data only when they are out
on the open road; there are no feedback channels. A standard method for
sending the data is to put it in a carousel, broadcasting the packets in a (cid:12)xed
periodic sequence.
‘Yes, a car may go through a tunnel, and miss out on a
few hundred packets, but it will be able to collect those missed packets an
hour later when the carousel has gone through a full revolution (we hope); or
maybe the following day: : :’

If instead the satellite uses a digital fountain, each car needs to receive

only an amount of data equal to the original (cid:12)le size (plus 5%).

Further reading

The encoders and decoders sold by Digital Fountain have even higher e(cid:14)ciency
than the LT codes described here, and they work well for all blocklengths, not
only large lengths such as K  
10 000. Shokrollahi (2003) presents raptor
codes, which are an extension of LT codes with linear-time encoding and de-
coding.

50.5 Further exercises

. Exercise 50.4.[2 ] Understanding the robust soliton distribution.

Repeat the analysis of exercise 50.2 (p.592) but now aim to have the
expected number of packets of degree 1 be ht(1) = 1 + S for all t, instead
of 1. Show that the initial required number of packets is

h0(d) =

K

d(d (cid:0) 1)

+

S
d

for d > 1.

(50.6)

The reason for truncating the second term beyond d = K=S and replac-
ing it by the spike at d = K=S (see equation (50.4)) is to ensure that
the decoding complexity does not grow larger than O(K ln K).

Estimate the expected number of packets Pd h0(d) and the expected
number of edges in the sparse graph Pd h0(d)d (which determines the

decoding complexity) if the histogram of packets is as given in (50.6).
Compare with the expected numbers of packets and edges when the
robust soliton distribution (50.4) is used.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

50.5: Further exercises

595

Exercise 50.5.[4 ] Show that the spike at d = K=S (equation (50.4)) is an ade-

quate replacement for the tail of high-weight packets in (50.6).

Exercise 50.6.[3C ] Investigate experimentally how necessary the spike at d =
K=S (equation (50.4)) is for successful decoding. Investigate also whether
the tail of (cid:26)(d) beyond d = K=S is necessary. What happens if all high-
weight degrees are removed, both the spike at d = K=S and the tail of
(cid:26)(d) beyond d = K=S?

Exercise 50.7.[4 ] Fill in the details in the proof of Luby’s main theorem, that
receiving K0 = K + 2 ln(S=(cid:14))S checks ensures that all the source packets
can be recovered with probability at least 1 (cid:0) (cid:14).

Exercise 50.8.[4C ] Optimize the degree distribution of a digital fountain code
for a (cid:12)le of K = 10 000 packets. Pick a sensible objective function for
your optimization, such as minimizing the mean of N , the number of
packets required for complete decoding, or the 95th percentile of the
histogram of N ((cid:12)gure 50.4).

. Exercise 50.9.[3 ] Make a model of the situation where a data stream is broad-
cast to cars, and quantify the advantage that the digital fountain has
over the carousel method.

Exercise 50.10.[2 ] Construct a simple example to illustrate the fact that the
digital fountain decoder of section 50.2 is suboptimal { it sometimes
gives up even though the information available is su(cid:14)cient to decode
the whole (cid:12)le. How does the cost of the optimal decoder compare?

. Exercise 50.11.[2 ] If every transmitted packet were created by adding together
source packets at random with probability 1/2 of each source packet’s
being included, show that the probability that K0 = K received packets
su(cid:14)ce for the optimal decoder to be able to recover the K source packets
is just a little below 1=2. [To put it another way, what is the probability
that a random K (cid:2) K matrix has full rank?]
Show that if K0 = K + (cid:1) packets are received, the probability that they
will not su(cid:14)ce for the optimal decoder is roughly 2(cid:0)(cid:1).

. Exercise 50.12.[4C ] Implement an optimal digital fountain decoder that uses
the method of Richardson and Urbanke (2001b) derived for fast encod-
ing of sparse-graph codes (section 47.7) to handle the matrix inversion
required for optimal decoding. Now that you have changed the decoder,
you can reoptimize the degree distribution, using higher-weight packets.
By how much can you reduce the overhead? Con(cid:12)rm the assertion that
this approach makes digital fountain codes viable as erasure-correcting
codes for all blocklengths, not just the large blocklengths for which LT
codes are excellent.

. Exercise 50.13.[5 ] Digital fountain codes are excellent rateless codes for erasure
channels. Make a rateless code for a channel that has both erasures and
noise.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

596

50 | Digital Fountain Codes

50.6 Summary of sparse-graph codes

A simple method for designing error-correcting codes for noisy channels, (cid:12)rst
pioneered by Gallager (1962), has recently been rediscovered and generalized,
and communication theory has been transformed. The practical performance
of Gallager’s low-density parity-check codes and their modern cousins is vastly
better than the performance of the codes with which textbooks have been (cid:12)lled
in the intervening years.

Which sparse-graph code is ‘best’ for a noisy channel depends on the cho-
sen rate and blocklength, the permitted encoding and decoding complexity,
and the question of whether occasional undetected errors are acceptable. Low-
density parity-check codes are the most versatile; it’s easy to make a compet-
itive low-density parity-check code with almost any rate and blocklength, and
low-density parity-check codes virtually never make undetected errors.

For the special case of the erasure channel, the sparse-graph codes that are

best are digital fountain codes.

50.7 Conclusion

The best solution to the communication problem is:

Combine a simple, pseudo-random code

with a message-passing decoder.

 
 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Part VII

Appendices

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

A

Notation

What does P (Aj B; C) mean? P (Aj B; C) is pronounced ‘the probability
that A is true given that B is true and C is true’. Or, more brie(cid:13)y, ‘the
probability of A given B and C’. (See Chapter 2, p.22.)

What do log and ln mean? In this book, log x means the base-two loga-

rithm, log2 x; ln x means the natural logarithm, loge x.

What does ^s mean? Usually, a ‘hat’ over a variable denotes a guess or es-

timator. So ^s is a guess at the value of s.

Integrals. There is no di(cid:11)erence between R f (u) du andR du f (u). The inte-

grand is f (u) in both cases.

What does

n=1 but it denotes a

product. It’s pronounced ‘product over n from 1 to N’. So, for example,

N

Yn=1

N

Yn=1

mean? This is like the summation PN
n = 1 (cid:2) 2 (cid:2) 3 (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) N = N ! = exp" N
Xn=1

ln n# :

(A.1)

I like to choose the name of the free variable in a sum or a product {
here, n { to be the lower case version of the range of the sum. So n
usually runs from 1 to N , and m usually runs from 1 to M . This is a
habit I learnt from Yaser Abu-Mostafa, and I think it makes formulae
easier to understand.

What does (cid:18)N

n(cid:19) mean? This is pronounced ‘N choose n’, and it is the

number of ways of selecting an unordered set of n objects from a set of
size N .

n(cid:19) =
(cid:18)N

N !

(N (cid:0) n)! n!

:

(A.2)

This function is known as the combination function.

What is (cid:0)(x)? The gamma function is de(cid:12)ned by (cid:0)(x) (cid:17) R 10 du ux(cid:0)1e(cid:0)u,
for x > 0. The gamma function is an extension of the factorial function
to real number arguments. In general, (cid:0)(x + 1) = x(cid:0)(x), and for integer
arguments, (cid:0)(x + 1) = x!. The digamma function is de(cid:12)ned by (cid:9) (x) (cid:17)
d
dx ln (cid:0)(x).
For large x (for practical purposes, 0:1 (cid:20) x (cid:20) 1),

ln (cid:0)(x) ’(cid:0)x (cid:0) 1

2(cid:1) ln(x) (cid:0) x + 1

598

2 ln 2(cid:25) + O(1=x);

(A.3)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

A | Notation

599

and for small x (for practical purposes, 0 (cid:20) x (cid:20) 0:5):

ln (cid:0)(x) ’ ln

where (cid:13)e is Euler’s constant.

1
x (cid:0) (cid:13)ex + O(x2)

(A.4)

What does H(cid:0)1

2 (1 (cid:0) R=C) mean? Just as sin(cid:0)1(s) denotes the inverse func-
tion to s = sin(x), so H(cid:0)1
There is potential confusion when people use sin2 x to denote (sin x)2,
since then we might expect sin(cid:0)1 s to denote 1= sin(s); I therefore like
to avoid using the notation sin2 x.

2 (h) is the inverse function to h = H2(x).

What does f0(x) mean? The answer depends on the context. Often, a

‘prime’ is used to denote di(cid:11)erentiation:

f0(x) (cid:17)

d
dx

f (x);

similarly, a dot denotes di(cid:11)erentiation with respect to time, t:

d
dt

x:

_x (cid:17)

(A.5)

(A.6)

However, the prime is also a useful indicator for ‘another variable’, for
example ‘a new value for a variable’. So, for example, x0 might denote
‘the new value of x’. Also, if there are two integers that both range from
1 to N , I will often name those integers n and n0.
So my rule is: if a prime occurs in an expression that could be a func-
tion, such as f0(x) or h0(y), then it denotes di(cid:11)erentiation; otherwise it
indicates ‘another variable’.

What is the error function? De(cid:12)nitions of this function vary. I de(cid:12)ne it
to be the cumulative probability of a standard (variance = 1) normal
distribution,

(cid:8)(z) (cid:17)Z z

(cid:0)1

exp((cid:0)z2=2)=p2(cid:25) dz:

(A.7)

What does E(r) mean? E[r] is pronounced ‘the expected value of r’ or ‘the
expectation of r’, and it is the mean value of r. Another symbol for
‘expected value’ is the pair of angle-brackets, hri:

What does jxj mean? The vertical bars ‘j (cid:1) j’ have two meanings. If A is a
set, then jAj denotes the number of elements in the set; if x is a number,
then jxj is the absolute value of x.

What does [AjP] mean? Here, A and P are matrices with the same num-
ber of rows. [AjP] denotes the double-width matrix obtained by putting
A alongside P. The vertical bar is used to avoid confusion with the
product AP.

What does xT mean? The superscript T is pronounced ‘transpose’. Trans-

posing a row-vector turns it into a column vector:

(1; 2; 3)T =0
@

1
2
3

1
A ;

(A.8)

and vice versa.
are column vectors.]

[Normally my vectors, indicated by bold face type (x),

Similarly, matrices can be transposed. If Mij is the entry in row i and
column j of matrix M, and N = MT, then Nji = Mij.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

600

A | Notation

What are Trace M and det M? The trace of a matrix is the sum of its di-

agonal elements,

Trace M =Xi

Mii:

(A.9)

The determinant of M is denoted det M.

What does (cid:14)mn mean? The (cid:14) matrix is the identity matrix.

(cid:14)mn =(cid:26) 1

0

if m = n
if m 6= n.

Another name for the identity matrix is I or 1. Sometimes I include a
subscript on this symbol { 1K { which indicates the size of the matrix
(K (cid:2) K).

What does (cid:14)(x) mean? The delta function has the property

Z dx f (x)(cid:14)(x) = f (0):

(A.10)

Another possible meaning for (cid:14)(S) is the truth function, which is 1 if the
proposition S is true but I have adopted another notation for that. After
all, the symbol (cid:14) is quite busy already, with the two roles mentioned above
in addition to its role as a small real number (cid:14) and an increment operator
(as in (cid:14)x)!

What does  [S] mean?  [S] is the truth function, which is 1 if the propo-
sition S is true and 0 otherwise. For example, the number of positive
numbers in the set T = f(cid:0)2; 1; 3g can be written

 [x > 0]:

Xx2T

(A.11)

What is the di(cid:11)erence between ‘:=’ and ‘=’ ? In an algorithm, x := y

means that the variable x is updated by assigning it the value of y.

In contrast, x = y is a proposition, a statement that x is equal to y.

See Chapters 23 and 29 for further de(cid:12)nitions and notation relating to

probability distributions.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

B

Some Physics

B.1 About phase transitions

A system with states x in contact with a heat bath at temperature T = 1=(cid:12)
has probability distribution

P (xj (cid:12)) =

1

Z((cid:12))

exp((cid:0)(cid:12)E(x)):

The partition function is

Z((cid:12)) =Xx

exp((cid:0)(cid:12)E(x)):

(B.1)

(B.2)

The inverse temperature (cid:12) can be interpreted as de(cid:12)ning an exchange rate
between entropy and energy.
(1=(cid:12)) is the amount of energy that must be
given to a heat bath to increase its entropy by one nat.

Often, the system will be a(cid:11)ected by some other parameters such as the
volume of the box it is in, V , in which case Z is a function of V too, Z((cid:12); V ).
For any system with a (cid:12)nite number of states, the function Z((cid:12)) is evi-
dently a continuous function of (cid:12), since it is simply a sum of exponentials.
Moreover, all the derivatives of Z((cid:12)) with respect to (cid:12) are continuous too.

What phase transitions are all about, however, is this: phase transitions
correspond to values of (cid:12) and V (called critical points) at which the derivatives
of Z have discontinuities or divergences.

Immediately we can deduce:

Only systems with an in(cid:12)nite number of states can show phase
transitions.

Often, we include a parameter N describing the size of the system. Phase
transitions may appear in the limit N ! 1. Real systems may have a value
of N like 1023.
If we make the system large by simply grouping together N independent
systems whose partition function is Z(1)((cid:12)), then nothing interesting happens.
The partition function for N independent identical systems is simply

Z(N )((cid:12)) = [Z(1)((cid:12))]N :

(B.3)

Now, while this function Z(N )((cid:12)) may be a very rapidly varying function of (cid:12),
that doesn’t mean it is showing phase transitions. The natural way to look at
the partition function is in the logarithm

ln Z(N )((cid:12)) = N ln Z(1)((cid:12)):

(B.4)

601

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

602

B | Some Physics

Duplicating the original system N times simply scales up all properties like
the energy and heat capacity of the system by a factor of N . So if the original
system showed no phase transitions then the scaled up system won’t have any
either.

Only systems with long-range correlations show phase transitions.

Long-range correlations do not require long-range energetic couplings; for
example, a magnet has only short-range couplings (between adjacent spins)
but these are su(cid:14)cient to create long-range order.

Why are points at which derivatives diverge interesting?

The derivatives of ln Z describe properties like the heat capacity of the sys-
tem (that’s the second derivative) or its (cid:13)uctuations in energy. If the second
derivative of ln Z diverges at a temperature 1=(cid:12), then the heat capacity of the
system diverges there, which means it can absorb or release energy without
changing temperature (think of ice melting in ice water); when the system is
at equilibrium at that temperature, its energy (cid:13)uctuates a lot, in contrast to
the normal law-of-large-numbers behaviour, where the energy only varies by
one part in pN .

A toy system that shows a phase transition

Imagine a collection of N coupled spins that have the following energy as a
function of their state x 2 f0; 1gN .
E(x) =(cid:26) (cid:0)N (cid:15)

x = (0; 0; 0; : : : ; 0)
otherwise.

(B.5)

0

This energy function describes a ground state in which all the spins are aligned
in the zero direction; the energy per spin in this state is (cid:0)(cid:15).
if any spin
changes state then the energy is zero. This model is like an extreme version
of a magnetic interaction, which encourages pairs of spins to be aligned.

We can contrast it with an ordinary system of N independent spins whose

energy is:

E0(x) = (cid:15)Xn

(2xn (cid:0) 1):

(B.6)

Like the (cid:12)rst system, the system of independent spins has a single ground
state (0; 0; 0; : : : ; 0) with energy (cid:0)N (cid:15), and it has roughly 2N states with energy
very close to 0, so the low-temperature and high-temperature properties of the
independent-spin system and the coupled-spin system are virtually identical.

The partition function of the coupled-spin system is

The function

is sketched in (cid:12)gure B.1a along with its low temperature behaviour,

Z((cid:12)) = e(cid:12)N (cid:15) + 2N (cid:0) 1:
ln Z((cid:12)) = ln(cid:16)e(cid:12)N (cid:15) + 2N (cid:0) 1(cid:17)
ln Z((cid:12)) ’ N (cid:12)(cid:15); (cid:12) ! 1;

and its high temperature behaviour,

ln Z((cid:12)) ’ N ln 2; (cid:12) ! 0:

(B.7)

(B.8)

(B.9)

(B.10)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

B.1: About phase transitions

603

log Z
N beta epsilon
N log (2)

var(E) N=24
var(E)  N=8

(a)

beta

(c)

beta

Figure B.1. (a) Partition function
of toy system which shows a phase
transition for large N . The arrow
marks the point (cid:12)c = log 2=(cid:15). (b)
The same, for larger N .
(c) The variance of the energy of
the system as a function of (cid:12) for
two system sizes. As N increases
the variance has an increasingly
sharp peak at the critical point (cid:12)c.
Contrast with (cid:12)gure B.2.

Figure B.2. The partition function
(a) and energy-variance (b) of a
system consisting of N
independent spins. The partition
function changes gradually from
one asymptote to the other,
regardless of how large N is; the
variance of the energy does not
have a peak. The (cid:13)uctuations are
largest at high temperature (small
(cid:12)) and scale linearly with system
size N .

log Z
N beta epsilon
N log (2)

(b)

beta

log Z
N beta epsilon
N log (2)

var(E) N=24
var(E)  N=8

(a)

beta

(b)

beta

The arrow marks the point

(cid:12) =

ln 2

(cid:15)

(B.11)

at which these two asymptotes intersect. In the limit N ! 1, the graph of
ln Z((cid:12)) becomes more and more sharply bent at this point ((cid:12)gure B.1b).
The second derivative of ln Z, which describes the variance of the energy

of the system, has a peak value, at (cid:12) = ln 2=(cid:15), roughly equal to

N 2(cid:15)2

4

;

(B.12)

which corresponds to the system spending half of its time in the ground state
and half its time in the other states.

At this critical point, the heat capacity of this system is thus proportional
to N 2; the heat capacity per spin is proportional to N , which, for in(cid:12)nite N , is
in(cid:12)nite, in contrast to the behaviour of systems away from phase transitions,
whose capacity per atom is a (cid:12)nite number.

For comparison, (cid:12)gure B.2 shows the partition function and energy-variance

of the ordinary independent-spin system.

More generally

Phase transitions can be categorized into ‘(cid:12)rst-order’ and ‘continuous’ transi-
tions. In a (cid:12)rst-order phase transition, there is a discontinuous change of one

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

604

B | Some Physics

or more order-parameters;
in a continuous transition, all order-parameters
change continuously. [What’s an order-parameter? { a scalar function of the
state of the system; or, to be precise, the expectation of such a function.]

In the vicinity of a critical point, the concept of ‘typicality’ de(cid:12)ned in
Chapter 4 does not hold. For example, our toy system, at its critical point,
has a 50% chance of being in a state with energy (cid:0)N (cid:15), and roughly a 1=2N +1
chance of being in each of the other states that have energy zero. It is thus not
the case that ln 1=P (x) is very likely to be close to the entropy of the system
at this point, unlike a system with N i.i.d. components.

Remember that information content (ln 1=P (x)) and energy are very closely
related. If typicality holds, then the system’s energy has negligible (cid:13)uctua-
tions, and vice versa.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

C

Some Mathematics

C.1 Finite (cid:12)eld theory

Most linear codes are expressed in the language of Galois theory

Why are Galois (cid:12)elds an appropriate language for linear codes? First, a de(cid:12)-
nition and some examples.

A (cid:12)eld F is a set F = f0; F 0g such that

1. F forms an Abelian group under an addition operation ‘+’, with
0 being the identity; [Abelian means all elements commute, i.e.,
satisfy a + b = b + a.]

2. F 0 forms an Abelian group under a multiplication operation ‘(cid:1)’;

multiplication of any element by 0 yields 0;

3. these operations satisfy the distributive rule (a + b)(cid:1) c = a(cid:1) c + b(cid:1) c.
For example, the real numbers form a (cid:12)eld, with ‘+’ and ‘(cid:1)’ denoting
ordinary addition and multiplication.

A Galois (cid:12)eld GF (q) is a (cid:12)eld with a (cid:12)nite number of elements q.

A unique Galois (cid:12)eld exists for any q = pm, where p is a prime number
and m is a positive integer; there are no other (cid:12)nite (cid:12)elds.

GF (2): The addition and multiplication tables for GF (2) are shown in ta-

ble C.1. These are the rules of addition and multiplication modulo 2.

GF (p): For any prime number p, the addition and multiplication rules are

those for ordinary addition and multiplication, modulo p.

GF (4): The rules for GF (pm), with m > 1, are not those of ordinary addition
and multiplication. For example the tables for GF (4) (table C.2) are not
the rules of addition and multiplication modulo 4. Notice that 1 + 1 = 0,
for example. So how can GF (4) be described? It turns out that the
elements can be related to polynomials. Consider polynomial functions
of x of degree 1 and with coe(cid:14)cients that are elements of GF (2). The
polynomials shown in table C.3 obey the addition and multiplication
rules of GF (4) if addition and multiplication are modulo the polynomial
x2 + x + 1, and the coe(cid:14)cients of the polynomials are from GF (2). For
example, B (cid:1) B = x2 + (1 + 1)x + 1 = x = A. Each element may also be
represented as a bit pattern as shown in table C.3, with addition being
bitwise modulo 2, and multiplication de(cid:12)ned with an appropriate carry
operation.

605

+ 0 1
0
0 1
1 0
1

(cid:1)
0 1
0 0 0
1 0 1

Table C.1. Addition and
multiplication tables for GF (2).

1 A B
+ 0
1 A B
0
0
0 B A
1
1
1
A A B 0
0
B B A 1
(cid:1)
1 A B
0
0
0
1
1 A B
A 0 A B 1
B 0 B 1 A

0
0
0

0

Table C.2. Addition and
multiplication tables for GF (4).

Element Polynomial Bit pattern

0
1
A
B

0
1
x

x + 1

00
01
10
11

Table C.3. Representations of the
elements of GF (4).

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

606

C | Some Mathematics

GF (8). We can denote the elements of GF (8) by f0; 1; A; B; C; D; E; Fg. Each
element can be mapped onto a polynomial over GF (2). The multiplica-
tion and addition operations are given by multiplication and addition of
the polynomials, modulo x3 + x + 1. The multiplication table is given
below.

element polynomial binary representation

0
1
A
B
C
D
E
F

0
1
x

x + 1

x2

x2 + 1
x2 + x

x2 + x + 1

000
001
010
011
100
101
110
111

0

0

0

0

0

0
0
0

(cid:1)
1 A B C D E F
0
0
0
1
1 A B C D E F
A 0 A C E B 1 F D
B 0 B E D F C 1 A
C 0 C B F E A D 1
D 0 D 1 C A F B E
E 0 E F
1 D B A C
F 0 F D A 1 E C B

Why are Galois (cid:12)elds relevant to linear codes? Imagine generalizing a binary
generator matrix G and binary vector s to a matrix and vector with elements
from a larger set, and generalizing the addition and multiplication operations
that de(cid:12)ne the product Gs.
In order to produce an appropriate input for
a symmetric channel, it would be convenient if, for random s, the product
Gs produced all elements in the enlarged set with equal probability. This
uniform distribution is easiest to guarantee if these elements form a group
under both addition and multiplication, because then these operations do not
break the symmetry among the elements. When two random elements of a
multiplicative group are multiplied together, all elements are produced with
equal probability. This is not true of other sets such as the integers, for which
the multiplication operation is more likely to give rise to some elements (the
composite numbers) than others. Galois (cid:12)elds, by their de(cid:12)nition, avoid such
symmetry-breaking e(cid:11)ects.

C.2 Eigenvectors and eigenvalues

A right-eigenvector of a square matrix A is a non-zero vector eR that satis(cid:12)es

AeR = (cid:21)eR;

(C.1)

where (cid:21) is the eigenvalue associated with that eigenvector. The eigenvalue
may be a real number or complex number and it may be zero. Eigenvectors
may be real or complex.

A left-eigenvector of a matrix A is a vector eL that satis(cid:12)es

LA = (cid:21)eT
eT
L:

(C.2)

The following statements for right-eigenvectors also apply to left-eigenvectors.
(cid:15) If a matrix has two or more linearly independent right-eigenvectors with
the same eigenvalue then that eigenvalue is called a degenerate eigenvalue
of the matrix, or a repeated eigenvalue. Any linear combination of those
eigenvectors is another right-eigenvector with the same eigenvalue.

(cid:15) The principal right-eigenvector of a matrix is, by de(cid:12)nition, the right-

eigenvector with the largest associated eigenvalue.

(cid:15) If a real matrix has a right-eigenvector with complex eigenvalue (cid:21) =
x + yi then it also has a right-eigenvector with the conjugate eigenvalue
(cid:21)(cid:3) = x (cid:0) yi.

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

C.2: Eigenvectors and eigenvalues

607

Symmetric matrices
If A is a real symmetric N (cid:2) N matrix then

1. all the eigenvalues and eigenvectors of A are real;

2. every left-eigenvector of A is also a right-eigenvector of A with the same

eigenvalue, and vice versa;

3. a set of N eigenvectors and eigenvalues fe(a); (cid:21)agN

are orthonormal, that is,

a=1 can be found that

e(a)(cid:1)e(b) = (cid:14)ab;

(C.3)

the matrix can be expressed as a weighted sum of outer products of the
eigenvectors:

A =

N

Xa=1

(cid:21)a[e(a)][e(a)]T:

(C.4)

(Whereas I often use i and n as indices for sets of size I and N , I will use the
indices a and b to run over eigenvectors, even if there are N of them. This is
to avoid confusion with the components of the eigenvectors, which are indexed
by n, e.g. e(a)

n .)

General square matrices
An N (cid:2) N matrix can have up to N distinct eigenvalues. Generically, there
are N eigenvalues, all distinct, and each has one left-eigenvector and one right-
eigenvector. In cases where two or more eigenvalues coincide, for each distinct
eigenvalue that is non-zero there is at least one left-eigenvector and one right-
eigenvector.

Left- and right-eigenvectors that have di(cid:11)erent eigenvalue are orthogonal,

that is,

if (cid:21)a 6= (cid:21)b then e(a)

L (cid:1)e(b)

R = 0:

(C.5)

Non-negative matrices
De(cid:12)nition. If all the elements of a non-zero matrix C satisfy Cmn (cid:21) 0 then C
is a non-negative matrix. Similarly, if all the elements of a non-zero vector c
satisfy cn (cid:21) 0 then c is a non-negative vector.
Properties. A non-negative matrix has a principal eigenvector that is non-
negative. It may also have other eigenvectors with the same eigenvalue that
are not non-negative. But if the principal eigenvalue of a non-negative matrix
is not degenerate, then the matrix has only one principal eigenvector e(1), and
it is non-negative.

Generically, all the other eigenvalues are smaller in absolute magnitude.

[There can be several eigenvalues of identical magnitude in special cases.]

Transition probability matrices

An important example of a non-negative matrix is a transition probability
matrix Q.
De(cid:12)nition. A transition probability matrix Q has columns that are probability
vectors, that is, it satis(cid:12)es Q (cid:21) 0 and

Qij = 1 for all j:

(C.6)

Xi

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

608

Matrix

2
4

1 2 0
1 1 0

0 0 13
5
1 1(cid:21)
(cid:20) 0 1

2
664

1 1 0 0
0 0 0 1
1 0 0 0
0 0 1 1

3
775

2
664

:60
:37
:37
:60

1:62

3
775

2
664

:60
:37
:37
:60

3
775

Eigenvalues and eigenvectors eL; eR

C | Some Mathematics

2:41

2
4

:58
:82

03
5

0:5+0:9i

2
664

3
:1(cid:0):5i
(cid:0):3(cid:0):4i
775
:3+:4i
(cid:0):1+:5i

03
(cid:0):82
:58
5

1

0
0

0
0

1:62

:82
:58

2
4

(cid:0)0:41
03
2
2
13
13
03
2
2
(cid:0):58
:82
5
4
4
5
5
5
4
4
(cid:0)0:62
(cid:0):53(cid:21)
(cid:0):53(cid:21)(cid:20) :85
:85(cid:21) (cid:20) :85
:85(cid:21)(cid:20):53
(cid:20):53
0:5(cid:0)0:9i
2
3
2
3
2
:1(cid:0):5i
:1+:5i
(cid:0):3+:4i
:3+:4i
664
775
664
775
664
:3(cid:0):4i
(cid:0):3(cid:0):4i
(cid:0):1(cid:0):5i
(cid:0):1+:5i

3
:1+:5i
:3(cid:0):4i
775
(cid:0):3+:4i
(cid:0):1(cid:0):5i

(cid:0)0:62
2
3
:37
(cid:0):60
664
775
(cid:0):60
:37

3
:37
(cid:0):60
775
(cid:0):60
:37

2
664

Table C.4. Some matrices and their eigenvectors.

Matrix

(cid:20) 0 :38
1 :62(cid:21)
1 :65 :543
2
5
4

0 :35 0
0 0 :46

2
4

Eigenvalues and eigenvectors eL; eR

1

(cid:0)0:38
(cid:20):71
:71(cid:21)(cid:20):36
:93(cid:21) (cid:20)(cid:0):93
:36(cid:21)(cid:20)(cid:0):71
:71(cid:21)
(cid:0)0:2+0:3i
(cid:0)0:2(cid:0)0:3i
2
:2(cid:0):2i3
2
:4+:3i3
2
:2+:2i3
(cid:0):8(cid:0):1i
:2(cid:0):5i
(cid:0):8+:1i
(cid:0):2+:5i
(cid:0):6+:2i
(cid:0):2(cid:0):5i
4
5
5
4
5
4

:4(cid:0):3i3
:2+:5i
(cid:0):6(cid:0):2i
5

:14
:41

:903
5

2
4

1

:58
:58

:583
5

2
4

Table C.5. Transition probability matrices for generating random paths through trellises.

This property can be rewritten in terms of the all-ones vector n = (1; 1; : : : ; 1)T:

So n is the principal left-eigenvector of Q with eigenvalue (cid:21)1 = 1.

nTQ = nT:

e(1)
L = n:

(C.7)

(C.8)

Because it is a non-negative matrix, Q has a principal right-eigenvector that
is non-negative, e(1)
R . Generically, for Markov processes that are ergodic, this
eigenvector is the only right-eigenvector with eigenvalue of magnitude 1 (see
table C.6 for illustrative exceptions). This vector, if we normalize it such that
e(1)
R (cid:1)n = 1, is called the invariant distribution of the transition probability
matrix. It is the probability density that is left unchanged under Q. Unlike
the principal left-eigenvector, which we explicitly identi(cid:12)ed above, we can’t
usually identify the principal right-eigenvector without computation.

The matrix may have up to N (cid:0) 1 other right-eigenvectors all of which are

orthogonal to the left-eigenvector n, that is, they are zero-sum vectors.

C.3 Perturbation theory

Perturbation theory is not used in this book, but it is useful in this book’s
(cid:12)elds. In this section we derive (cid:12)rst-order perturbation theory for the eigen-
vectors and eigenvalues of square, not necessarily symmetric, matrices. Most
presentations of perturbation theory focus on symmetric matrices, but non-
symmetric matrices (such as transition matrices) also deserve to be perturbed!

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

C.3: Perturbation theory

609

Matrix

Eigenvalues and eigenvectors eL; eR

(a)

2
664
(a0) 2
664
(b) 2
664

:90 :20 0
:10 :80 0
0
0

0
0
0 :90 :20
0 :10 :80

:90 :20 0
0
:10 :79 :02 0
0 :01 :88 :20
0
0 :10 :80

0
0
:90 :20 0
:10 :80 0

0 :90 :20
0 :10 :80
0
0

3
775
3
775
3
775

0
0
:71
:71

2
664

1

:50
:50
:50
:50

:50
:50
:50
:50

2
664
2
664

3
775
3
775

1

3
775
2
664
2
664

1

2
664

0
0
:89
:45

3
775

:87
:43
:22
:11

:63
:32
:63
:32

3
775
3
775

2
664
2
664

3
:71
(cid:0):71
775
0
0

2
664

2
664

1

0:98

2
664

:71
:71
0
0

3
775
3
(cid:0):18
(cid:0):15
775
:66
:72
3
(cid:0):32
:63
775
(cid:0):32
:63

2
664
2
664
2
664

0:70

:89
:45
0
0

3
775

3
(cid:0):66
(cid:0):28
775
:61
:33
3
:50
(cid:0):50
775
:50
(cid:0):50

0:70

0:70

3
:45
(cid:0):89
775
0
0

2
664
3
2
:20
(cid:0):40
775
664
(cid:0):40
:80
(cid:0)0:70
3
2
:32
(cid:0):63
775
664
(cid:0):32
:63

2
664
2
664

3
:63
(cid:0):63
775
(cid:0):32
:32
3
(cid:0):50
:50
775
:50
(cid:0):50

0:69

0:70
3
0
0
775
(cid:0):45
:89

2
664
3
2
(cid:0):19
:41
775
664
(cid:0):44
:77
(cid:0)1
3
2
:50
:50
775
664
(cid:0):50
(cid:0):50

2
664
2
664

3
0
0
775
(cid:0):71
:71

3
(cid:0):61
:65
775
(cid:0):35
:30
3
:63
:32
775
(cid:0):63
(cid:0):32

Table C.6. Illustrative transition probability matrices and their eigenvectors showing the two ways of
being non-ergodic. (a) More than one principal eigenvector with eigenvalue 1 because the
state space falls into two unconnected pieces. (a0) A small perturbation breaks the degen-
eracy of the principal eigenvectors. (b) Under this chain, the density may oscillate between
two parts of the state space. In addition to the invariant distribution, there is another
right-eigenvector with eigenvalue (cid:0)1. In general such circulating densities correspond to
complex eigenvalues with magnitude 1.

We assume that we have an N (cid:2) N matrix H that is a function H((cid:15)) of
a real parameter (cid:15), with (cid:15) = 0 being our starting point. We assume that a
Taylor expansion of H((cid:15)) is appropriate:

where

H((cid:15)) = H(0) + (cid:15)V + (cid:1)(cid:1)(cid:1)

V (cid:17)

@H
@(cid:15)

:

(C.9)

(C.10)

We assume that for all (cid:15) of interest, H((cid:15)) has a complete set of N right-
eigenvectors and left-eigenvectors, and that these eigenvectors and their eigen-
values are continuous functions of (cid:15). This last assumption is not necessarily a
good one: if H(0) has degenerate eigenvalues then it is possible for the eigen-
vectors to be discontinuous in (cid:15); in such cases, degenerate perturbation theory
is needed. That’s a fun topic, but let’s stick with the non-degenerate case
here.

We write the eigenvectors and eigenvalues as follows:

H((cid:15))e(a)

R ((cid:15)) = (cid:21)(a)((cid:15))e(a)

R ((cid:15));

and we Taylor-expand

with

and

(cid:21)(a)((cid:15)) = (cid:21)(a)(0) + (cid:15)(cid:22)(a) + (cid:1)(cid:1)(cid:1)

(cid:22)(a) (cid:17)

@(cid:21)(a)((cid:15))

@(cid:15)

e(a)
R ((cid:15)) = e(a)

R (0) + (cid:15)f (a)

R + (cid:1)(cid:1)(cid:1)

(C.11)

(C.12)

(C.13)

(C.14)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

610

with

C | Some Mathematics

@e(a)
R
@(cid:15)

;

(C.15)

f (a)
R (cid:17)
L and f (a)

and similar de(cid:12)nitions for e(a)
L . We de(cid:12)ne these left-vectors to be row
vectors, so that the ‘transpose’ operation is not needed and can be banished.
We are free to constrain the magnitudes of the eigenvectors in whatever way
we please. Each left-eigenvector and each right-eigenvector has an arbitrary
magnitude. The natural constraints to use are as follows. First, we constrain
the inner products with:

e(a)
L ((cid:15))e(a)

R ((cid:15)) = 1;

for all a:

Expanding the eigenvectors in (cid:15), equation (C.19) implies

(e(a)

L (0) + (cid:15)f (a)

L + (cid:1)(cid:1)(cid:1))(e(a)

R (0) + (cid:15)f (a)

R + (cid:1)(cid:1)(cid:1)) = 1;

from which we can extract the terms in (cid:15), which say:

e(a)
L (0)f (a)

R + f (a)

L e(a)

R (0) = 0

We are now free to choose the two constraints:

L (0)f (a)
e(a)

R = 0;

L e(a)
f (a)

R (0) = 0;

(C.16)

(C.17)

(C.18)

(C.19)

which in the special case of a symmetric matrix correspond to constraining
the eigenvectors to be of constant length, as de(cid:12)ned by the Euclidean norm.
OK, now that we have de(cid:12)ned our cast of characters, what do the de(cid:12)ning
equations (C.11) and (C.9) tell us about our Taylor expansions (C.13) and
(C.15)? We expand equation (C.11) in (cid:15).

(H(0)+(cid:15)V+(cid:1)(cid:1)(cid:1))(e(a)

R (0)+(cid:15)f (a)

R +(cid:1)(cid:1)(cid:1)) = ((cid:21)(a)(0)+(cid:15)(cid:22)(a)+(cid:1)(cid:1)(cid:1))(e(a)

R (0)+(cid:15)f (a)

R +(cid:1)(cid:1)(cid:1)):
(C.20)

Identifying the terms of order (cid:15), we have:

H(0)f (a)

R + Ve(a)

R (0) = (cid:21)(a)(0)f (a)

R + (cid:22)(a)e(a)

R (0):

(C.21)

We can extract interesting results from this equation by hitting it with e(b)

L (0):

L (0)H(0)f (a)
e(b)
) (cid:21)(b)e(b)

R + e(b)

L (0)Ve(a)

R (0) = e(b)

L (0)(cid:21)(a)(0)f (a)

R + (cid:22)(a)e(b)

L (0)e(a)

R (0):

L (0)f (a)

R + e(b)

L (0)Ve(a)

R (0) = (cid:21)(a)(0)e(b)

L (0)f (a)

R + (cid:22)(a)(cid:14)ab:

Setting b = a we obtain

e(a)
L (0)Ve(a)

R (0) = (cid:22)(a):

Alternatively, choosing b 6= a, we obtain:

e(b)
L (0)Ve(a)

R (0) =h(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)i e(b)

L (0)f (a)

R

1

e(b)
L (0)Ve(a)

R (0):

L (0)f (a)

R =

) e(b)

(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)
Now, assuming that the right-eigenvectors fe(b)

we must be able to write

R (0)gN

(C.22)

(C.23)

(C.24)

(C.25)

b=1 form a complete basis,

f (a)

R =Xb

wbe(b)

R (0);

(C.26)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

611

C.3: Perturbation theory

where

wb = e(b)

L (0)f (a)
R ;

so, comparing (C.25) and (C.27), we have:

f (a)

R =Xb6=a

e(b)
L (0)Ve(a)
R (0)
(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)

e(b)
R (0):

(C.27)

(C.28)

Equations (C.23) and (C.28) are the solution to the (cid:12)rst-order perturbation
theory problem, giving respectively the (cid:12)rst derivative of the eigenvalue and
the eigenvectors.

Second-order perturbation theory

If we expand the eigenvector equation (C.11) to second order in (cid:15), and assume
that the equation

H((cid:15)) = H(0) + (cid:15)V

(C.29)

is exact, that is, H is a purely linear function of (cid:15), then we have:

(H(0) + (cid:15)V)(e(a)

R (0) + (cid:15)f (a)

1
2

R +

(cid:15)2g(a)
2 (cid:15)2(cid:23)(a) + (cid:1)(cid:1)(cid:1))(e(a)

R + (cid:1)(cid:1)(cid:1))
R (0) + (cid:15)f (a)

= ((cid:21)(a)(0) + (cid:15)(cid:22)(a) + 1

R + (cid:1)(cid:1)(cid:1)) (C.30)
R and (cid:23)(a) are the second derivatives of the eigenvector and eigenvalue.

where g(a)
Equating the second-order terms in (cid:15) in equation (C.30),

R + 1

2 (cid:15)2g(a)

Vf (a)

R +

1
2

H(0)g(a)

R =

1
2

(cid:21)(a)(0)g(a)

R +

1
2

(cid:23)(a)e(a)

R (0) + (cid:22)(a)f (a)
R :

(C.31)

Hitting this equation on the left with e(a)

L (0), we obtain:

L (0)Vf (a)
e(a)
R +
2 (cid:21)(a)(0)e(a)
L (0)f (a)

R

= 1
The term e(a)

(cid:21)(a)e(a)

1
2

R

L (0)g(a)
2 (cid:23)(a)e(a)

L (0)g(a)

R + 1

L (0)e(a)

R (0) + (cid:22)(a)e(a)

L (0)f (a)
R :

(C.32)

is equal to zero because of our constraints (C.19), so

so the second derivative of the eigenvalue with respect to (cid:15) is given by

e(a)
L (0)Vf (a)

R =

(cid:23)(a);

1
2

(C.33)

(cid:23)(a) = e(a)

1
2

L (0)VXb6=a

= Xb6=a

e(b)
L (0)Ve(a)
R (0)
(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)
R (0)][e(a)
(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)

e(b)
R (0)

(C.34)

[e(b)

L (0)Ve(a)

L (0)Ve(b)

R (0)]

:

(C.35)

This is as far as we will take the perturbation expansion.

Summary
If we introduce the abbreviation Vba for e(b)
eigenvectors of H((cid:15)) = H(0) + (cid:15)V to (cid:12)rst order as

L (0)Ve(a)

R (0), we can write the

e(a)
R ((cid:15)) = e(a)

R (0) + (cid:15)Xb6=a

Vba

(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)

e(b)
R (0) + (cid:1)(cid:1)(cid:1)

and the eigenvalues to second order as

(cid:21)(a)((cid:15)) = (cid:21)(a)(0) + (cid:15)Vaa + (cid:15)2Xb6=a

VbaVab

(cid:21)(a)(0) (cid:0) (cid:21)(b)(0)

+ (cid:1)(cid:1)(cid:1) :

(C.36)

(C.37)

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

612

C.4 Some numbers

C | Some Mathematics

28192
21024

2469
2266

2190
2171

298
258

232

225

e7

2(cid:0)2

21000
2500

2200

2100

250

240

230

220

210
20

2(cid:0)10

2(cid:0)20

2(cid:0)30
2(cid:0)60

102466
10308
10301
3(cid:2)10150
10141
1080

1:6(cid:2)1060
1057
3(cid:2)1051
1030
3(cid:2)1029
3(cid:2)1017
1015

1012
1011
1011
3(cid:2)1010
6(cid:2)109
6(cid:2)109
109

Number of distinct 1-kilobyte (cid:12)les
Number of states of a 2D Ising model with 32(cid:2)32 spins
Number of binary strings of length 1000

Number of binary strings of length 1000 having 100 1s and 900 0s
Number of electrons in universe

Number of electrons in solar system
Number of electrons in the earth

Age of universe/picoseconds

Age of universe/seconds

Number of neurons in human brain
Number of bits stored on a DVD
Number of bits in the wheat genome
Number of bits in the human genome
Population of earth

Number of bits in C. Elegans (a worm) genome
Number of bits in Arabidopsis thaliana (a (cid:13)owering plant related to broccoli) genome
One year/seconds
Number of bits in the compressed PostScript (cid:12)le that is this book
Number of bits in unix kernel
Number of bits in the E. Coli genome, or in a (cid:13)oppy disk
Number of years since human/chimpanzee divergence
1 048 576

2:5 (cid:2) 108 Number of (cid:12)bres in the corpus callosum
2(cid:2)108
2(cid:2)108
3(cid:2)107
2(cid:2)107
2(cid:2)107
107
4(cid:2)106
106
2(cid:2)105
3 (cid:2) 104
3 (cid:2) 104
1:5(cid:2)103 Number of base pairs in a gene
103
100

210 = 1024; e7 = 1096

1

Number of generations since human/chimpanzee divergence
Number of genes in human genome
Number of genes in Arabidopsis thaliana genome

2:5(cid:2)10(cid:0)1 Lifetime probability of dying from smoking one pack of cigarettes per day.

Lifetime probability of dying in a motor vehicle accident

Lifetime probability of developing cancer because of drinking 2 litres per day of
water containing 12 p.p.b. benzene

Probability of error in transmission of coding DNA, per nucleotide, per generation

Probability of undetected error in a hard disk drive, after error correction

10(cid:0)2
10(cid:0)3
10(cid:0)5

10(cid:0)6
3(cid:2)10(cid:0)8
10(cid:0)9
10(cid:0)18

 
Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Bibliography

Abrahamsen, P. (1997) A review of Gaussian random (cid:12)elds and
correlation functions. Technical Report 917, Norwegian Com-
puting Center, Blindern, N-0314 Oslo, Norway. 2nd edition.

Abramson, N. (1963) Information Theory and Coding. McGraw-

Hill.

Adler, S. L. (1981) Over-relaxation method for the Monte-Carlo
evaluation of the partition function for multiquadratic actions.
Physical Review D { Particles and Fields 23 (12): 2901{2904.
Aiyer, S. V. B. (1991) Solving Combinatorial Optimization Prob-
lems Using Neural Networks. Cambridge Univ. Engineering
Dept. PhD dissertation. CUED/F-INFENG/TR 89.

Aji, S., Jin, H., Khandekar, A., McEliece, R. J., and
MacKay, D. J. C. (2000) BSC thresholds for code ensembles
based on ‘typical pairs’ decoding. In Codes, Systems and Graph-
ical Models, ed. by B. Marcus and J. Rosenthal, volume 123 of
IMA Volumes in Mathematics and its Applications, pp. 195{
210. Springer.

Amari, S., Cichocki, A., and Yang, H. H. (1996) A new learn-
ing algorithm for blind signal separation. In Advances in Neural
Information Processing Systems, ed. by D. S. Touretzky, M. C.
Mozer, and M. E. Hasselmo, volume 8, pp. 757{763. MIT Press.
(1985)
Storing in(cid:12)nite numbers of patterns in a spin glass model of
neural networks. Phys. Rev. Lett. 55: 1530{1533.

Amit, D. J., Gutfreund, H., and Sompolinsky, H.

Angel, J. R. P., Wizinowich, P., Lloyd-Hart, M., and San-
dler, D. (1990) Adaptive optics for array telescopes using
neural-network techniques. Nature 348: 221{224.

Bahl, L. R., Cocke, J., Jelinek, F., and Raviv, J.

(1974)
Optimal decoding of linear codes for minimizing symbol error
rate. IEEE Trans. Info. Theory IT-20: 284{287.

Baldwin, J. (1896) A new factor in evolution. American Natu-

ralist 30: 441{451.

Bar-Shalom, Y., and Fortmann, T. (1988) Tracking and Data

Association. Academic Press.

Barber, D., and Williams, C. K. I. (1997) Gaussian processes
for Bayesian classi(cid:12)cation via hybrid Monte Carlo. In Neural
Information Processing Systems 9 , ed. by M. C. Mozer, M. I.
Jordan, and T. Petsche, pp. 340{346. MIT Press.

Barnett, S. (1979) Matrix Methods for Engineers and Scientists.

McGraw-Hill.

Battail, G. (1993) We can think of good codes, and even de-
code them. In Eurocode ’92. Udine, Italy, 26-30 October , ed.
by P. Camion, P. Charpin, and S. Harari, number 339 in CISM
Courses and Lectures, pp. 353{368. Springer.

Baum, E., Boneh, D., and Garrett, C.

(1995) On genetic
In Proc. Eighth Annual Conf. on Computational

algorithms.
Learning Theory, pp. 230{239. ACM.

Baum, E. B., and Smith, W. D. (1993) Best play for imperfect
players and game tree search. Technical report, NEC, Prince-
ton, NJ.

Baum, E. B., and Smith, W. D. (1997) A Bayesian approach to
relevance in game playing. Arti(cid:12)cial Intelligence 97 (1-2): 195{
242.

Baum, L. E., and Petrie, T.

inference
for probabilistic functions of (cid:12)nite-state Markov chains. Ann.
Math. Stat. 37: 1559{1563.

Statistical

(1966)

Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. (2002)
The in(cid:12)nite hidden Markov model. In Advances in Neural In-
formation Processing Systems 14 . MIT Press.

Bell, A. J., and Sejnowski, T. J. (1995) An information maxi-
mization approach to blind separation and blind deconvolution.
Neural Computation 7 (6): 1129{1159.

Bentley, J. (2000) Programming Pearls. Addison-Wesley, sec-

ond edition.

Berger, J. (1985) Statistical Decision theory and Bayesian Anal-

ysis. Springer.

Berlekamp, E. R. (1968) Algebraic Coding Theory. McGraw-

Hill.

Berlekamp, E. R.

(1980) The technology of error-correcting

codes. IEEE Trans. Info. Theory 68: 564{593.

Berlekamp, E. R., McEliece, R. J., and van Tilborg, H.
C. A. (1978) On the intractability of certain coding problems.
IEEE Trans. Info. Theory 24 (3): 384{386.

Berrou, C., and Glavieux, A. (1996) Near optimum error cor-
IEEE Trans. on

recting coding and decoding: Turbo-codes.
Communications 44: 1261{1271.

Berrou, C., Glavieux, A., and Thitimajshima, P. (1993) Near
Shannon limit error-correcting coding and decoding: Turbo-
codes. In Proc. 1993 IEEE International Conf. on Communi-
cations, Geneva, Switzerland , pp. 1064{1070.

Berzuini, C., Best, N. G., Gilks, W. R., and Larizza, C.
(1997) Dynamic conditional independence models and Markov
chain Monte Carlo methods. J. American Statistical Assoc. 92
(440): 1403{1412.

Berzuini, C., and Gilks, W. R. (2001) Following a moving tar-
get { Monte Carlo inference for dynamic Bayesian models. J.
Royal Statistical Society Series B { Statistical Methodology 63
(1): 127{146.

Bhattacharyya, A. (1943) On a measure of divergence between
two statistical populations de(cid:12)ned by their probability distri-
butions. Bull. Calcutta Math. Soc. 35: 99{110.

Bishop, C. M. (1992) Exact calculation of the Hessian matrix for
the multilayer perceptron. Neural Computation 4 (4): 494{501.
Bishop, C. M. (1995) Neural Networks for Pattern Recognition.

Oxford Univ. Press.

Bishop, C. M., Winn, J. M., and Spiegelhalter, D. (2002)
VIBES: A variational inference engine for Bayesian networks.
In Advances in Neural Information Processing Systems XV , ed.
by S. Becker, S. Thrun, and K. Obermayer.

Blahut, R. E. (1987) Principles and Practice of Information

Theory. Addison-Wesley.

Bottou, L., Howard, P. G., and Bengio, Y. (1998) The Z-
coder adaptive binary coder. In Proc. Data Compression Conf.,
Snowbird, Utah, March 1998 , pp. 13{22.

Box, G. E. P., and Tiao, G. C. (1973) Bayesian Inference in

Statistical Analysis. Addison{Wesley.

Braunstein, A., M(cid:19)ezard, M., and Zecchina, R., (2003) Survey

propagation: an algorithm for satis(cid:12)ability. cs.CC/0212002.

613

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

614

Bibliography

Bretthorst, G. (1988) Bayesian Spectrum Analysis and Param-
eter Estimation. Springer. Also available at bayes.wustl.edu.
Bridle, J. S. (1989) Probabilistic interpretation of feedforward
classi(cid:12)cation network outputs, with relationships to statisti-
cal pattern recognition. In Neuro-computing: Algorithms, Ar-
chitectures and Applications, ed. by F. Fougelman-Soulie and
J. H(cid:19)erault. Springer{Verlag.

Bulmer, M. (1985) The Mathematical Theory of Quantitative

Genetics. Oxford Univ. Press.

Burrows, M., and Wheeler, D. J. (1994) A block-sorting loss-
less data compression algorithm. Technical Report 124, Digital
SRC.

Byers, J., Luby, M., Mitzenmacher, M., and Rege, A. (1998)
A digital fountain approach to reliable distribution of bulk data.
In Proc. ACM SIGCOMM ’98, September 2{4, 1998 .

Cairns-Smith, A. G. (1985) Seven Clues to the Origin of Life.

Cambridge Univ. Press.

Calderbank, A. R., and Shor, P. W. (1996) Good quantum
error-correcting codes exist. Phys. Rev. A 54: 1098. quant-ph/
9512032.

Carroll, L.

(1998) Alice’s Adventures in Wonderland; and,
and what Alice Found There.

Through the Looking-glass:
Macmillan Children’s Books.

Childs, A. M., Patterson, R. B., and MacKay, D. J. C.
(2001) Exact sampling from non-attractive distributions using
summary states. Physical Review E 63: 036113.

Chu, W., Keerthi, S. S., and Ong, C. J.

(2001) A uni(cid:12)ed
loss function in Bayesian framework for support vector regres-
sion. In Proc. 18th International Conf. on Machine Learning,
pp. 51{58.

Chu, W., Keerthi, S. S., and Ong, C. J. (2002) A new Bayesian
design method for support vector classi(cid:12)cation. In Special Sec-
tion on Support Vector Machines of the 9th International Conf.
on Neural Information Processing.

Chu, W., Keerthi, S. S., and Ong, C. J. (2003a) Bayesian
support vector regression using a uni(cid:12)ed loss function. IEEE
Trans. on Neural Networks. Submitted.

Chu, W., Keerthi, S. S., and Ong, C. J. (2003b) Bayesian

trigonometric support vector classi(cid:12)er. Neural Computation.

Chung, S.-Y., Richardson, T. J., and Urbanke, R. L. (2001)
Analysis of sum-product decoding of low-density parity-check
codes using a Gaussian approximation. IEEE Trans. Info. The-
ory 47 (2): 657{670.

Chung, S.-Y., Urbanke, R. L., and Richardson, T. J.,
(1999) LDPC code design applet. lids.mit.edu/~sychung/
gaopt.html.

Comon, P., Jutten, C., and Herault, J. (1991) Blind sepa-
ration of sources. 2. Problems statement. Signal Processing 24
(1): 11{20.

Copas, J. B. (1983) Regression, prediction and shrinkage (with

discussion). J. R. Statist. Soc. B 45 (3): 311{354.

Cover, T. M. (1965) Geometrical and statistical properties of
systems of linear inequalities with applications in pattern recog-
nition. IEEE Trans. on Electronic Computers 14: 326{334.

Cover, T. M., and Thomas, J. A. (1991) Elements of Informa-

tion Theory. Wiley.

Cowles, M. K., and Carlin, B. P. (1996) Markov-chain Monte-
Carlo convergence diagnostics { a comparative review. J. Amer-
ican Statistical Assoc. 91 (434): 883{904.

Cox, R. (1946) Probability, frequency, and reasonable expecta-

tion. Am. J. Physics 14: 1{13.

Cressie, N. (1993) Statistics for Spatial Data. Wiley.
Davey, M. C. (1999) Error-correction using Low-Density Parity-

Check Codes. Univ. of Cambridge PhD dissertation.

Davey, M. C., and MacKay, D. J. C. (1998) Low density par-
ity check codes over GF(q). IEEE Communications Letters 2
(6): 165{167.

Davey, M. C., and MacKay, D. J. C. (2000) Watermark codes:
Reliable communication over insertion/deletion channels.
In
Proc. 2000 IEEE International Symposium on Info. Theory,
p. 477.

Davey, M. C., and MacKay, D. J. C. (2001) Reliable commu-
nication over channels with insertions, deletions and substitu-
tions. IEEE Trans. Info. Theory 47 (2): 687{698.

Dawid, A., Stone, M., and Zidek, J.

(1996) Critique of
E.T Jaynes’s ‘paradoxes of probability theory’. Technical Re-
port 172, Dept. of Statistical Science, Univ. College London.

Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S.
(1995) The Helmholtz machine. Neural Computation 7
(5): 889{904.

Divsalar, D., Jin, H., and McEliece, R. J. (1998) Coding the-
orems for ‘turbo-like’ codes. In Proc. 36th Allerton Conf. on
Communication, Control, and Computing, Sept. 1998 , pp. 201{
210. Allerton House.

Doucet, A., de Freitas, J., and Gordon, N. eds. (2001) Se-

quential Monte Carlo Methods in Practice. Springer.

Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth,
D. (1987) Hybrid Monte Carlo. Physics Letters B 195: 216{
222.

Durbin, R., Eddy, S. R., Krogh, A., and Mitchison, G. (1998)
Biological Sequence Analysis. Probabilistic Models of Proteins
and Nucleic Acids. Cambridge Univ. Press.

Dyson, F. J. (1985) Origins of Life. Cambridge Univ. Press.
Elias, P. (1975) Universal codeword sets and representations of

the integers. IEEE Trans. Info. Theory 21 (2): 194{203.

Felsenstein, J.

Eyre-Walker, A., and Keightley, P. (1999) High genomic
deleterious mutation rates in hominids. Nature 397: 344{347.
is Maynard
Smith necessary?
In Evolution. Essays in Honour of John
Maynard Smith, ed. by P. J. Greenwood, P. H. Harvey, and
M. Slatkin, pp. 209{220. Cambridge Univ. Press.

(1985) Recombination and sex:

Ferreira, H., Clarke, W., Helberg, A., Abdel-Ghaffar,
K. S., and Vinck, A. H. (1997) Insertion/deletion correction
with spectral nulls. IEEE Trans. Info. Theory 43 (2): 722{732.
Feynman, R. P. (1972) Statistical Mechanics. Addison{Wesley.
Forney, Jr., G. D. (1966) Concatenated Codes. MIT Press.
Forney, Jr., G. D. (2001) Codes on graphs: Normal realiza-

tions. IEEE Trans. Info. Theory 47 (2): 520{548.

Frey, B. J. (1998) Graphical Models for Machine Learning and

Digital Communication. MIT Press.

Gallager, R. G. (1962) Low density parity check codes. IRE

Trans. Info. Theory IT-8: 21{28.

Gallager, R. G. (1963) Low Density Parity Check Codes. Num-
ber 21 in MIT Research monograph series. MIT Press. Avail-
able from www.inference.phy.cam.ac.uk/mackay/gallager/
papers/.

Gallager, R. G. (1968) Information Theory and Reliable Com-

munication. Wiley.

Gallager, R. G. (1978) Variations on a theme by Hu(cid:11)man.

IEEE Trans. Info. Theory IT-24 (6): 668{674.

Gibbs, M. N. (1997) Bayesian Gaussian Processes for Regres-
sion and Classi(cid:12)cation. Cambridge Univ. PhD dissertation.
www.inference.phy.cam.ac.uk/mng10/.

Gibbs, M. N., and MacKay, D. J. C.,

Ef-
for interpo-
www.inference.phy.cam.ac.uk/mackay/abstracts/

(cid:12)cient implementation of Gaussian processes
lation.
gpros.html.

(1996)

Gibbs, M. N., and MacKay, D. J. C. (2000) Variational Gaus-
sian process classi(cid:12)ers. IEEE Trans. on Neural Networks 11
(6): 1458{1464.

Gilks, W., Roberts, G., and George, E.

(1994) Adaptive

direction sampling. Statistician 43: 179{189.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Bibliography

615

Gilks, W., and Wild, P. (1992) Adaptive rejection sampling for

Gibbs sampling. Applied Statistics 41: 337{348.

Gilks, W. R., Richardson, S., and Spiegelhalter, D. J.
(1996) Markov Chain Monte Carlo in Practice. Chapman and
Hall.

Goldie, C. M., and Pinch, R. G. E. (1991) Communication

theory. Cambridge Univ. Press.

Golomb, S. W., Peile, R. E., and Scholtz, R. A. (1994) Basic
Concepts in Information Theory and Coding: The Adventures
of Secret Agent 00111 . Plenum Press.

Good, I. J. (1979) Studies in the history of probability and statis-
tics. XXXVII. A.M. Turing’s statistical work in World War II.
Biometrika 66 (2): 393{396.

Graham, R. L. (1966) On partitions of a (cid:12)nite set. Journal of

Combinatorial Theory 1: 215{223.

Graham, R. L., and Knowlton, K. C., (1968) Method of iden-
tifying conductors in a cable by establishing conductor connec-
tion groupings at both ends of the cable. U.S. Patent 3,369,177.
Green, P. J. (1995) Reversible jump Markov chain Monte Carlo
computation and Bayesian model determination. Biometrika
82: 711{732.

Gregory, P. C., and Loredo, T. J. (1992) A new method for
the detection of a periodic signal of unknown shape and period.
In Maximum Entropy and Bayesian Methods,, ed. by G. Erick-
son and C. Smith. Kluwer. Also in Astrophysical Journal, 398,
pp. 146{168, Oct 10, 1992.

Gull, S. F. (1988) Bayesian inductive inference and maximum
entropy. In Maximum Entropy and Bayesian Methods in Sci-
ence and Engineering, vol. 1: Foundations, ed. by G. Erickson
and C. Smith, pp. 53{74. Kluwer.

Gull, S. F. (1989) Developments in maximum entropy data anal-
ysis. In Maximum Entropy and Bayesian Methods, Cambridge
1988 , ed. by J. Skilling, pp. 53{71. Kluwer.

Gull, S. F., and Daniell, G. (1978) Image reconstruction from

incomplete and noisy data. Nature 272: 686{690.

Hamilton, W. D. (2002) Narrow Roads of Gene Land, Volume

2: Evolution of Sex . Oxford Univ. Press.

Hanson, R., Stutz, J., and Cheeseman, P. (1991a) Bayesian
classi(cid:12)cation theory. Technical Report FIA{90-12-7-01, NASA
Ames.

Hanson, R., Stutz, J., and Cheeseman, P. (1991b) Bayesian
classi(cid:12)cation with correlation and inheritance. In Proc. 12th In-
tern. Joint Conf. on Arti(cid:12)cial Intelligence, Sydney, Australia,
volume 2, pp. 692{698. Morgan Kaufmann.

Hartmann, C. R. P., and Rudolph, L. D. (1976) An optimum
symbol by symbol decoding rule for linear codes. IEEE Trans.
Info. Theory IT-22: 514{517.

Harvey, M., and Neal, R. M. (2000) Inference for belief net-
works using coupling from the past. In Uncertainty in Arti(cid:12)cial
Intelligence: Proc. Sixteenth Conf., pp. 256{263.

Hebb, D. O. (1949) The Organization of Behavior . Wiley.
Hendin, O., Horn, D., and Hopfield, J. J. (1994) Decompo-
sition of a mixture of signals in a model of the olfactory bulb.
Proc. Natl. Acad. Sci. USA 91 (13): 5942{5946.

Hertz, J., Krogh, A., and Palmer, R. G. (1991) Introduction

to the Theory of Neural Computation. Addison-Wesley.

Hinton, G.

(2001) Training products of experts by minimiz-
ing contrastive divergence. Technical Report 2000-004, Gatsby
Computational Neuroscience Unit, Univ. College London.

Hinton, G., and Nowlan, S. (1987) How learning can guide

evolution. Complex Systems 1: 495{502.

Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M.
(1995) The wake-sleep algorithm for unsupervised neural net-
works. Science 268 (5214): 1158{1161.

Hinton, G. E., and Sejnowski, T. J.

(1986) Learning and
relearning in Boltzmann machines. In Parallel Distributed Pro-
cessing, ed. by D. E. Rumelhart and J. E. McClelland, pp. 282{
317. MIT Press.

Hinton, G. E., and Teh, Y. W.

(2001) Discovering multi-
ple constraints that are frequently approximately satis(cid:12)ed. In
Uncertainty in Arti(cid:12)cial Intelligence: Proc. Seventeenth Conf.
(UAI-2001), pp. 227{234. Morgan Kaufmann.

Hinton, G. E., and van Camp, D.

(1993) Keeping neural
networks simple by minimizing the description length of the
weights. In Proc. 6th Annual Workshop on Comput. Learning
Theory, pp. 5{13. ACM Press, New York, NY.

Hinton, G. E., Welling, M., Teh, Y. W., and Osindero, S.
(2001) A new view of ICA. In Proc. International Conf. on
Independent Component Analysis and Blind Signal Separation,
volume 3.

Hinton, G. E., and Zemel, R. S. (1994) Autoencoders, min-
imum description length and Helmholtz free energy.
In Ad-
vances in Neural Information Processing Systems 6 , ed. by J. D.
Cowan, G. Tesauro, and J. Alspector. Morgan Kaufmann.

Hodges, A. (1983) Alan Turing: The Enigma. Simon and Schus-

ter.

Hojen-Sorensen, P. A., Winther, O., and Hansen, L. K.
(2002) Mean (cid:12)eld approaches to independent component anal-
ysis. Neural Computation 14: 889{918.

Holmes, C., and Denison, D.

(2002) Perfect sampling for
wavelet reconstruction of signals. IEEE Trans. Signal Process-
ing 50: 237{244.

Holmes, C., and Mallick, B.

(1998) Perfect simulation for
orthogonal model mixing. Technical report, Imperial College,
London.

Hopfield, J. J.

(1974) Kinetic proofreading: A new mecha-
nism for reducing errors in biosynthetic processes requiring high
speci(cid:12)city. Proc. Natl. Acad. Sci. USA 71 (10): 4135{4139.

Hopfield, J. J. (1978) Origin of the genetic code: A testable hy-
pothesis based on tRNA structure, sequence, and kinetic proof-
reading. Proc. Natl. Acad. Sci. USA 75 (9): 4334{4338.

Hopfield, J. J. (1980) The energy relay: A proofreading scheme
based on dynamic cooperativity and lacking all characteristic
symptoms of kinetic proofreading in DNA replication and pro-
tein synthesis. Proc. Natl. Acad. Sci. USA 77 (9): 5248{5252.
Hopfield, J. J. (1982) Neural networks and physical systems
with emergent collective computational abilities. Proc. Natl.
Acad. Sci. USA 79: 2554{8.

Hopfield, J. J. (1984) Neurons with graded response properties
have collective computational properties like those of two-state
neurons. Proc. Natl. Acad. Sci. USA 81: 3088{92.

Hopfield, J. J. (1987) Learning algorithms and probability dis-
tributions in feed-forward and feed-back networks. Proc. Natl.
Acad. Sci. USA 84: 8429{33.

Hopfield, J. J., and Brody, C. D. (2000) What is a moment?
\Cortical" sensory integration over a brief interval. Proc. Natl.
Acad. Sci 97: 13919{13924.

Hopfield, J. J., and Brody, C. D. (2001) What is a moment?
Transient synchrony as a collective mechanism for spatiotem-
poral integration. Proc. Natl. Acad. Sci 98: 1282{1287.

Hopfield, J. J., and Tank, D. W. (1985) Neural computation of
decisions in optimization problems. Biol. Cybernetics 52: 1{25.
Howarth, P., and Bradley, A. (1986) The longitudinal aberra-
tion of the human eye and its correction. Vision Res. 26: 361{
366.

Hinton, G. E., and Ghahramani, Z. (1997) Generative models
for discovering sparse distributed representations. Philosophical
Trans. Royal Society B .

Huber, M. (1998) Exact sampling and approximate counting
techniques. In Proc. 30th ACM Symposium on the Theory of
Computing, pp. 31{40.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

616

Bibliography

Huffman, D. (1952) A method for construction of minimum-

redundancy codes. Proc. of IRE 40 (9): 1098{1101.

Ichikawa, K., Bhadeshia, H. K. D. H., and MacKay, D. J. C.
(1996) Model for hot cracking in low-alloy steel weld metals.
Science and Technology of Welding and Joining 1: 43{50.

Levenshtein, V. I. (1966) Binary codes capable of correcting
deletions, insertions, and reversals. Soviet Physics { Doklady
10 (8): 707{710.

Lin, S., and Costello, Jr., D. J. (1983) Error Control Coding:

Fundamentals and Applications. Prentice-Hall.

Isard, M., and Blake, A. (1996) Visual tracking by stochastic
propagation of conditional density. In Proc. Fourth European
Conf. Computer Vision, pp. 343{356.

Litsyn, S., and Shevelev, V.

(2002) On ensembles of low-
density parity-check codes: asymptotic distance distributions.
IEEE Trans. Info. Theory 48 (4): 887{908.

Isard, M., and Blake, A. (1998) Condensation { conditional
density propagation for visual tracking. International Journal
of Computer Vision 29 (1): 5{28.

Jaakkola, T. S., and Jordan, M. I. (1996) Computing upper
and lower bounds on likelihoods in intractable networks.
In
Proc. Twelfth Conf. on Uncertainty in AI . Morgan Kaufman.
Jaakkola, T. S., and Jordan, M. I. (2000a) Bayesian logistic
regression: a variational approach. Statistics and Computing
10: 25{37.

Jaakkola, T. S., and Jordan, M. I. (2000b) Bayesian parame-
ter estimation via variational methods. Statistics and Comput-
ing 10 (1): 25{37.

Jaynes, E. T. (1983) Bayesian intervals versus con(cid:12)dence in-
tervals. In E.T. Jaynes. Papers on Probability, Statistics and
Statistical Physics, ed. by R. D. Rosenkrantz, p. 151. Kluwer.
Jaynes, E. T. (2003) Probability Theory: The Logic of Science.

Cambridge Univ. Press. Edited by G. Larry Bretthorst.

Jensen, F. V. (1996) An Introduction to Bayesian Networks.

UCL press.

Johannesson, R., and Zigangirov, K. S. (1999) Fundamentals

of Convolutional Coding. IEEE Press.

Jordan, M. I. ed. (1998) Learning in Graphical Models. NATO

Science Series. Kluwer Academic Publishers.

JPL,

(1996)

Turbo codes performance.

Available from

www331.jpl.nasa.gov/public/TurboPerf.html.

Jutten, C., and Herault, J. (1991) Blind separation of sources.
1. An adaptive algorithm based on neuromimetic architecture.
Signal Processing 24 (1): 1{10.

Karplus, K., and Krit, H. (1991) A semi-systolic decoder for
the PDSC{73 error-correcting code. Discrete Applied Mathe-
matics 33: 109{128.

Kepler, T., and Oprea, M. (2001) Improved inference of muta-
tion rates: I. An integral representation of the Luria-Delbr(cid:127)uck
distribution. Theoretical Population Biology 59: 41{48.

Kimeldorf, G. S., and Wahba, G. (1970) A correspondence be-
tween Bayesian estimation of stochastic processes and smooth-
ing by splines. Annals of Math. Statistics 41 (2): 495{502.

Kitanidis, P. K. (1986) Parameter uncertainty in estimation of
spatial functions: Bayesian analysis. Water Resources Research
22: 499{507.

Loredo, T. J. (1990) From Laplace to supernova SN 1987A:
Bayesian inference in astrophysics.
In Maximum Entropy
and Bayesian Methods, Dartmouth, U.S.A., 1989 , ed. by
P. Fougere, pp. 81{142. Kluwer.

Lowe, D. G.

(1995) Similarity metric learning for a variable

kernel classi(cid:12)er. Neural Computation 7: 72{85.

Luby, M. (2002) LT codes. In Proc. The 43rd Annual IEEE Sym-
posium on Foundations of Computer Science, November 16{19
2002 , pp. 271{282.

Luby, M. G., Mitzenmacher, M., Shokrollahi, M. A., and
Spielman, D. A. (1998) Improved low-density parity-check
codes using irregular graphs and belief propagation. In Proc.
IEEE International Symposium on Info. Theory, p. 117.

Luby, M. G., Mitzenmacher, M., Shokrollahi, M. A., and
Spielman, D. A. (2001a) E(cid:14)cient erasure correcting codes.
IEEE Trans. Info. Theory 47 (2): 569{584.

Luby, M. G., Mitzenmacher, M., Shokrollahi, M. A., and
Spielman, D. A. (2001b) Improved low-density parity-check
codes using irregular graphs and belief propagation.
IEEE
Trans. Info. Theory 47 (2): 585{598.

Luby, M. G., Mitzenmacher, M., Shokrollahi, M. A., Spiel-
man, D. A., and Stemann, V. (1997) Practical loss-resilient
codes.
In Proc. Twenty-Ninth Annual ACM Symposium on
Theory of Computing (STOC).

Luo, Z., and Wahba, G. (1997) Hybrid adaptive splines. J.

Amer. Statist. Assoc. 92: 107{116.

Luria, S. E., and Delbr(cid:127)uck, M. (1943) Mutations of bacteria
from virus sensitivity to virus resistance. Genetics 28: 491{
511. Reprinted in Microbiology: A Centenary Perspective,
Wolfgang K. Joklik, ed., 1999, ASM Press, and available from
www.esp.org/.

Luttrell, S. P. (1989) Hierarchical vector quantisation. Proc.

IEE Part I 136: 405{413.

Luttrell, S. P. (1990) Derivation of a class of training algo-

rithms. IEEE Trans. on Neural Networks 1 (2): 229{232.

MacKay, D. J. C. (1991) Bayesian Methods for Adaptive Models.

California Institute of Technology PhD dissertation.

MacKay, D. J. C. (1992a) Bayesian interpolation. Neural Com-

putation 4 (3): 415{447.

MacKay, D. J. C. (1992b) The evidence framework applied to

Knuth, D. E. (1968) The Art of Computer Programming. Ad-

classi(cid:12)cation networks. Neural Computation 4 (5): 698{714.

dison Wesley.

Kondrashov, A. S. (1988) Deleterious mutations and the evo-

lution of sexual reproduction. Nature 336 (6198): 435{440.

Kschischang, F. R., Frey, B. J., and Loeliger, H.-A. (2001)
IEEE Trans.

Factor graphs and the sum-product algorithm.
Info. Theory 47 (2): 498{519.

Kschischang, F. R., and Sorokine, V. (1995) On the trel-
IEEE Trans. Info. Theory 41

lis structure of block codes.
(6): 1924{1937.

Lauritzen, S. L. (1981) Time series analysis in 1880, a discussion
of contributions made by T. N. Thiele. ISI Review 49: 319{333.
Lauritzen, S. L. (1996) Graphical Models. Number 17 in Oxford

Statistical Science Series. Clarendon Press.

MacKay, D. J. C. (1992c) A practical Bayesian framework for
backpropagation networks. Neural Computation 4 (3): 448{472.
(1994a) Bayesian methods for backprop-
MacKay, D. J. C.
In Models of Neural Networks III , ed. by
agation networks.
E. Domany, J. L. van Hemmen, and K. Schulten, chapter 6,
pp. 211{254. Springer.

MacKay, D. J. C. (1994b) Bayesian non-linear modelling for
the prediction competition. In ASHRAE Trans., V.100, Pt.2 ,
pp. 1053{1062. American Society of Heating, Refrigeration, and
Air-conditioning Engineers.

MacKay, D. J. C. (1995a) Free energy minimization algorithm
for decoding and cryptanalysis. Electronics Letters 31 (6): 446{
447.

Lauritzen, S. L., and Spiegelhalter, D. J. (1988) Local com-
putations with probabilities on graphical structures and their
application to expert systems. J. Royal Statistical Society B
50: 157{224.

MacKay, D. J. C.

(1995b) Probable networks and plausible
predictions { a review of practical Bayesian methods for su-
pervised neural networks. Network: Computation in Neural
Systems 6: 469{505.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Bibliography

617

MacKay, D. J. C., (1997a) Ensemble learning for hidden Markov
www.inference.phy.cam.ac.uk/mackay/abstracts/

models.
ensemblePaper.html.

MacKay, D. J. C., (1997b) Iterative probabilistic decoding of low
density parity check codes. Animations available on world wide
web. www.inference.phy.cam.ac.uk/mackay/codes/gifs/.

MacKay, D. J. C. (1998a) Choice of basis for Laplace approxi-

Matheron, G. (1963) Principles of geostatistics. Economic Ge-

ology 58: 1246{1266.

Maynard Smith, J. (1968) ‘Haldane’s dilemma’ and the rate of

evolution. Nature 219 (5159): 1114{1116.

Maynard Smith, J. (1978) The Evolution of Sex . Cambridge

Univ. Press.

Maynard Smith, J.

(1988) Games, Sex and Evolution.

mation. Machine Learning 33 (1): 77{86.

Harvester{Wheatsheaf.

MacKay, D. J. C. (1998b) Introduction to Gaussian processes. In
Neural Networks and Machine Learning, ed. by C. M. Bishop,
NATO ASI Series, pp. 133{166. Kluwer.

MacKay, D. J. C. (1999a) Comparison of approximate meth-
ods for handling hyperparameters. Neural Computation 11
(5): 1035{1068.

MacKay, D. J. C. (1999b) Good error correcting codes based on
very sparse matrices. IEEE Trans. Info. Theory 45 (2): 399{
431.

MacKay, D. J. C., (2000) An alternative to runlength-limiting
codes: Turn timing errors into substitution errors. Available
from www.inference.phy.cam.ac.uk/mackay/.

MacKay, D. J. C., (2001) A problem with variational free
energy minimization. www.inference.phy.cam.ac.uk/mackay/
abstracts/minima.html.

MacKay, D. J. C., and Davey, M. C. (2000) Evaluation of Gal-
lager codes for short block length and high rate applications.
In Codes, Systems and Graphical Models, ed. by B. Marcus and
J. Rosenthal, volume 123 of IMA Volumes in Mathematics and
its Applications, pp. 113{130. Springer.

MacKay, D. J. C., Mitchison, G. J., and McFadden, P. L.
(2004) Sparse-graph codes for quantum error-correction. IEEE
Trans. Info. Theory 50 (10): 2315{2330.

MacKay, D. J. C., and Neal, R. M. (1995) Good codes based
on very sparse matrices. In Cryptography and Coding. 5th IMA
Conf., LNCS 1025 , ed. by C. Boyd, pp. 100{111. Springer.

MacKay, D. J. C., and Neal, R. M.

(1996) Near Shannon
limit performance of low density parity check codes. Electron-
ics Letters 32 (18): 1645{1646. Reprinted Electronics Letters,
33(6):457{458, March 1997.

MacKay, D. J. C., and Peto, L. (1995) A hierarchical Dirichlet

language model. Natural Language Engineering 1 (3): 1{19.

MacKay, D. J. C., Wilson, S. T., and Davey, M. C. (1998)
Comparison of constructions of irregular Gallager codes.
In
Proc. 36th Allerton Conf. on Communication, Control, and
Computing, Sept. 1998 , pp. 220{229. Allerton House.

MacKay, D. J. C., Wilson, S. T., and Davey, M. C. (1999)
Comparison of constructions of irregular Gallager codes. IEEE
Trans. on Communications 47 (10): 1449{1454.

MacKay, D. M., and MacKay, V.

(1974) The time course
of the McCollough e(cid:11)ect and its physiological implications. J.
Physiol. 237: 38{39.

MacKay, D. M., and McCulloch, W. S. (1952) The limiting
information capacity of a neuronal link. Bull. Math. Biophys.
14: 127{135.

MacWilliams, F. J., and Sloane, N. J. A. (1977) The Theory

of Error-correcting Codes. North-Holland.

Mandelbrot, B. (1982) The Fractal Geometry of Nature. W.H.

Freeman.

Mao, Y., and Banihashemi, A. (2000) Design of good LDPC
codes using girth distribution. In IEEE International Sympo-
sium on Info. Theory, Italy, June, 2000 .

Mao, Y., and Banihashemi, A. (2001) A heuristic search for
In IEEE Interna-

good LDPC codes at short block lengths.
tional Conf. on Communications.

Marinari, E., and Parisi, G. (1992) Simulated tempering { a
new Monte-Carlo scheme. Europhysics Letters 19 (6): 451{458.

Maynard Smith, J., and Sz(cid:19)athmary, E.

(1995) The Major

Transitions in Evolution. Freeman.

Maynard Smith, J., and Sz(cid:19)athmary, E. (1999) The Origins of

Life. Oxford Univ. Press.

McCollough, C. (1965) Color adaptation of edge-detectors in

the human visual system. Science 149: 1115{1116.

McEliece, R. J. (2002) The Theory of Information and Coding.

Cambridge Univ. Press, second edition.

McEliece, R. J., MacKay, D. J. C., and Cheng, J.-F. (1998)
Turbo decoding as an instance of Pearl’s ‘belief propagation’ al-
gorithm. IEEE Journal on Selected Areas in Communications
16 (2): 140{152.

McMillan, B. (1956) Two inequalities implied by unique deci-

pherability. IRE Trans. Inform. Theory 2: 115{116.

Minka, T.

(2001) A family of algorithms for approximate

Bayesian inference. MIT PhD dissertation.

Miskin, J. W. (2001) Ensemble Learning for Independent Com-
ponent Analysis. Dept. of Physics, Univ. of Cambridge PhD
dissertation.

Miskin, J. W., and MacKay, D. J. C.

(2000) Ensemble
learning for blind image separation and deconvolution. In Ad-
vances in Independent Component Analysis, ed. by M. Giro-
lami. Springer.

Miskin, J. W., and MacKay, D. J. C. (2001) Ensemble learning
for blind source separation. In ICA: Principles and Practice,
ed. by S. Roberts and R. Everson. Cambridge Univ. Press.

Mosteller, F., and Wallace, D. L. (1984) Applied Bayesian
and Classical Inference. The case of The Federalist papers.
Springer.

Neal, R. M. (1991) Bayesian mixture modelling by Monte Carlo
simulation. Technical Report CRG{TR{91{2, Computer Sci-
ence, Univ. of Toronto.

Neal, R. M. (1993a) Bayesian learning via stochastic dynamics.
In Advances in Neural Information Processing Systems 5 , ed.
by C. L. Giles, S. J. Hanson, and J. D. Cowan, pp. 475{482.
Morgan Kaufmann.

Neal, R. M. (1993b) Probabilistic inference using Markov chain
Monte Carlo methods. Technical Report CRG{TR{93{1, Dept.
of Computer Science, Univ. of Toronto.

Neal, R. M. (1995) Suppressing random walks in Markov chain
Monte Carlo using ordered overrelaxation. Technical Report
9508, Dept. of Statistics, Univ. of Toronto.

Neal, R. M.

(1996) Bayesian Learning for Neural Networks.

Springer.

Neal, R. M. (1997a) Markov chain Monte Carlo methods based
on ‘slicing’ the density function. Technical Report 9722, Dept.
of Statistics, Univ. of Toronto.

Neal, R. M. (1997b) Monte Carlo implementation of Gaussian
process models for Bayesian regression and classi(cid:12)cation. Tech-
nical Report CRG{TR{97{2, Dept. of Computer Science, Univ.
of Toronto.

Neal, R. M. (1998) Annealed importance sampling. Technical

Report 9805, Dept. of Statistics, Univ. of Toronto.

Neal, R. M. (2001) De(cid:12)ning priors for distributions using Dirich-
let di(cid:11)usion trees. Technical Report 0104, Dept. of Statistics,
Univ. of Toronto.

Neal, R. M.

(2003) Slice sampling. Annals of Statistics 31

(3): 705{767.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

618

Bibliography

Neal, R. M., and Hinton, G. E. (1998) A new view of the EM
algorithm that justi(cid:12)es incremental, sparse, and other variants.
In Learning in Graphical Models, ed. by M. I. Jordan, NATO
Science Series, pp. 355{368. Kluwer.

Nielsen, M., and Chuang, I. (2000) Quantum Computation and

Quantum Information. Cambridge Univ. Press.

Offer, E., and Soljanin, E. (2000) An algebraic description
of iterative decoding schemes. In Codes, Systems and Graphi-
cal Models, ed. by B. Marcus and J. Rosenthal, volume 123 of
IMA Volumes in Mathematics and its Applications, pp. 283{
298. Springer.

Offer, E., and Soljanin, E. (2001) LDPC codes: a group al-
gebra formulation. In Proc. Internat. Workshop on Coding and
Cryptography WCC 2001, 8-12 Jan. 2001, Paris.

O’Hagan, A.

(1978) On curve (cid:12)tting and optimal design for

regression. J. Royal Statistical Society, B 40: 1{42.

O’Hagan, A. (1987) Monte Carlo is fundamentally unsound. The

Statistician 36: 247{249.

O’Hagan, A. (1994) Bayesian Inference, volume 2B of Kendall’s

Advanced Theory of Statistics. Edward Arnold.

Omre, H. (1987) Bayesian kriging { merging observations and
quali(cid:12)ed guesses in kriging. Mathematical Geology 19: 25{39.
(2000) Gaussian processes for
classi(cid:12)cation: Mean-(cid:12)eld algorithms. Neural Computation 12
(11): 2655{2684.

Opper, M., and Winther, O.

Rasmussen, C. E., and Ghahramani, Z. (2003) Bayesian Monte
Carlo. In Advances in Neural Information Processing Systems
XV , ed. by S. Becker, S. Thrun, and K. Obermayer.

Ratliff, F., and Riggs, L. A. (1950) Involuntary motions of the
eye during monocular (cid:12)xation. J. Exptl. Psychol. 40: 687{701.
Ratzer, E. A., and MacKay, D. J. C. (2003) Sparse low-density
parity-check codes for channels with cross-talk. In Proc. 2003
IEEE Info. Theory Workshop, Paris.

Reif, F. (1965) Fundamentals of Statistical and Thermal Physics.

McGraw{Hill.

Richardson, T., Shokrollahi, M. A., and Urbanke, R.
(2001) Design of capacity-approaching irregular low-density
parity check codes. IEEE Trans. Info. Theory 47 (2): 619{637.
(2001a) The capacity of
low-density parity check codes under message-passing decod-
ing. IEEE Trans. Info. Theory 47 (2): 599{618.

Richardson, T., and Urbanke, R.

Richardson, T., and Urbanke, R. (2001b) E(cid:14)cient encoding
of low-density parity-check codes. IEEE Trans. Info. Theory
47 (2): 638{656.

Ridley, M. (2000) Mendel’s Demon: gene justice and the com-

plexity of life. Phoenix.

Ripley, B. D. (1991) Statistical Inference for Spatial Processes.

Cambridge Univ. Press.

Ripley, B. D. (1996) Pattern Recognition and Neural Networks.

Cambridge Univ. Press.

Patrick, J. D., and Wallace, C. S. (1982) Stone circle geome-
tries: an information theory approach. In Archaeoastronomy in
the Old World , ed. by D. C. Heggie, pp. 231{264. Cambridge
Univ. Press.

Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986)
Learning representations by back-propagating errors. Nature
323: 533{536.

Russell, S., and Wefald, E. (1991) Do the Right Thing: Studies

Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems:

in Limited Rationality. MIT Press.

Networks of Plausible Inference. Morgan Kaufmann.

Pearl, J. (2000) Causality. Cambridge Univ. Press.
Pearlmutter, B. A., and Parra, L. C.

(1996) A context-
sensitive generalization of ICA. In International Conf. on Neu-
ral Information Processing, Hong Kong, pp. 151{157.

Pearlmutter, B. A., and Parra, L. C.

(1997) Maximum
likelihood blind source separation: A context-sensitive general-
ization of ICA. In Advances in Neural Information Processing
Systems, ed. by M. C. Mozer, M. I. Jordan, and T. Petsche,
volume 9, p. 613. MIT Press.

Pinto, R. L., and Neal, R. M. (2001) Improving Markov chain
Monte Carlo estimators by coupling to an approximating chain.
Technical Report 0101, Dept. of Statistics, Univ. of Toronto.

Poggio, T., and Girosi, F. (1989) A theory of networks for
approximation and learning. Technical Report A.I. 1140, MIT.
Poggio, T., and Girosi, F. (1990) Networks for approximation

and learning. Proc. IEEE 78: 1481{1497.

Polya, G. (1954) Induction and Analogy in Mathematics. Prince-

ton Univ. Press.

Schneier, B. (1996) Applied Cryptography. Wiley.
Scholkopf, B., Burges, C., and Vapnik, V. (1995) Extract-
ing support data for a given task. In Proc. First International
Conf. on Knowledge Discovery and Data Mining, ed. by U. M.
Fayyad and R. Uthurusamy. AAAI Press.

Scholtz, R. A. (1982) The origins of spread-spectrum commu-

nications. IEEE Trans. on Communications 30 (5): 822{854.

Seeger, M., Williams, C. K. I., and Lawrence, N. (2003)
Fast forward selection to speed up sparse Gaussian process re-
gression. In Proc. Ninth International Workshop on Arti(cid:12)cial
Intelligence and Statistics, ed. by C. Bishop and B. J. Frey.
Society for Arti(cid:12)cial Intelligence and Statistics.

Sejnowski, T. J. (1986) Higher order Boltzmann machines. In
Neural networks for computing, ed. by J. Denker, pp. 398{403.
American Institute of Physics.

Sejnowski, T. J., and Rosenberg, C. R. (1987) Parallel net-
works that learn to pronounce English text. Journal of Complex
Systems 1 (1): 145{168.

Shannon, C. E. (1948) A mathematical theory of communica-

Propp, J. G., and Wilson, D. B. (1996) Exact sampling with
coupled Markov chains and applications to statistical mechan-
ics. Random Structures and Algorithms 9 (1-2): 223{252.

Rabiner, L. R., and Juang, B. H. (1986) An introduction to

tion. Bell Sys. Tech. J. 27: 379{423, 623{656.

Shannon, C. E. (1993) The best detection of pulses. In Collected
Papers of Claude Shannon, ed. by N. J. A. Sloane and A. D.
Wyner, pp. 148{150. IEEE Press.

hidden Markov models. IEEE ASSP Magazine pp. 4{16.

Shannon, C. E., and Weaver, W. (1949) The Mathematical

Rasmussen, C. E. (1996) Evaluation of Gaussian Processes and
Other Methods for Non-Linear Regression. Univ. of Toronto
PhD dissertation.

Rasmussen, C. E. (2000) The in(cid:12)nite Gaussian mixture model.
In Advances in Neural Information Processing Systems 12 , ed.
by S. Solla, T. Leen, and K.-R. M(cid:127)uller, pp. 554{560. MIT Press.
Rasmussen, C. E., (2002) Reduced rank Gaussian process learn-

ing. Unpublished manuscript.

Rasmussen, C. E., and Ghahramani, Z. (2002) In(cid:12)nite mixtures
of Gaussian process experts. In Advances in Neural Informa-
tion Processing Systems 14 , ed. by T. G. Diettrich, S. Becker,
and Z. Ghahramani. MIT Press.

Theory of Communication. Univ. of Illinois Press.

Shokrollahi, A. (2003) Raptor codes. Technical report, Labo-
ratoire d’algorithmique, (cid:19)Ecole Polytechnique F(cid:19)ed(cid:19)erale de Lau-
sanne, Lausanne, Switzerland. Available from algo.epfl.ch/.
Sipser, M., and Spielman, D. A. (1996) Expander codes. IEEE

Trans. Info. Theory 42 (6.1): 1710{1722.

Skilling, J.

(1989) Classic maximum entropy.

In Maxi-
mum Entropy and Bayesian Methods, Cambridge 1988 , ed. by
J. Skilling. Kluwer.

Skilling, J. (1993) Bayesian numerical analysis. In Physics and
Probability, ed. by W. T. Grandy, Jr. and P. Milonni. Cam-
bridge Univ. Press.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Bibliography

619

Skilling, J., and MacKay, D. J. C. (2003) Slice sampling { a
binary implementation. Annals of Statistics 31 (3): 753{755.
Discussion of Slice Sampling by Radford M. Neal.

Slepian, D., and Wolf, J. (1973) Noiseless coding of correlated

information sources. IEEE Trans. Info. Theory 19: 471{480.

Smola, A. J., and Bartlett, P. (2001) Sparse Greedy Gaus-
sian Process Regression.
In Advances in Neural Information
Processing Systems 13 , ed. by T. K. Leen, T. G. Diettrich, and
V. Tresp, pp. 619{625. MIT Press.

Spiegel, M. R.

(1988) Statistics. Schaum’s outline series.

McGraw-Hill, 2nd edition.

Spielman, D. A.

able error-correcting codes.
(6.1): 1723{1731.

(1996) Linear-time encodable and decod-
IEEE Trans. Info. Theory 42

Sutton, R. S., and Barto, A. G. (1998) Reinforcement Learn-

ing: An Introduction. MIT Press.

Swanson, L. (1988) A new code for Galileo. In Proc. 1988 IEEE

International Symposium Info. Theory, pp. 94{95.

Tanner, M. A. (1996) Tools for Statistical Inference: Methods
for the Exploration of Posterior Distributions and Likelihood
Functions. Springer Series in Statistics. Springer, 3rd edition.
Tanner, R. M. (1981) A recursive approach to low complexity

codes. IEEE Trans. Info. Theory 27 (5): 533{547.

Teahan, W. J.

(1995) Probability estimation for PPM.

In
Proc. NZCSRSC’95 . Available from citeseer.nj.nec.com/
teahan95probability.html.

ten Brink, S. (1999) Convergence of iterative decoding. Elec-

tronics Letters 35 (10): 806{808.

ten Brink, S., Kramer, G., and Ashikhmin, A., (2002) Design
of low-density parity-check codes for multi-antenna modulation
and detection. Submitted to IEEE Trans. on Communications.
Terras, A. (1999) Fourier Analysis on Finite Groups and Ap-

plications. Cambridge Univ. Press.

Thomas, A., Spiegelhalter, D. J., and Gilks, W. R. (1992)
BUGS: A program to perform Bayesian inference using Gibbs
sampling.
In Bayesian Statistics 4 , ed. by J. M. Bernardo,
J. O. Berger, A. P. Dawid, and A. F. M. Smith, pp. 837{842.
Clarendon Press.

Tresp, V. (2000) A Bayesian committee machine. Neural Com-

putation 12 (11): 2719{2741.

Urbanke, R., (2001) LdpcOpt { a fast and accurate degree distri-
bution optimizer for LDPC code ensembles. lthcwww.epfl.ch/
research/ldpcopt/.

Vapnik, V. (1995) The Nature of Statistical Learning Theory.

Springer.

Viterbi, A. J. (1967) Error bounds for convolutional codes and
an asymptotically optimum decoding algorithm. IEEE Trans.
Info. Theory IT-13: 260{269.

Wahba, G. (1990) Spline Models for Observational Data. Society
for Industrial and Applied Mathematics. CBMS-NSF Regional
Conf. series in applied mathematics.

Wainwright, M. J., Jaakkola, T., and Willsky, A. S. (2003)
Tree-based reparameterization framework for analysis of sum-
product and related algorithms. IEEE Trans. Info. Theory 45
(9): 1120{1146.

Wald, G., and Griffin, D.

(1947) The change in refractive
power of the eye in bright and dim light. J. Opt. Soc. Am.
37: 321{336.

Wallace, C., and Boulton, D. (1968) An information measure

for classi(cid:12)cation. Comput. J. 11 (2): 185{194.

Wallace, C. S., and Freeman, P. R. (1987) Estimation and
inference by compact coding. J. R. Statist. Soc. B 49 (3): 240{
265.

Ward, D. J., Blackwell, A. F., and MacKay, D. J. C. (2000)
Dasher { A data entry interface using continuous gestures and
language models. In Proc. User Interface Software and Tech-
nology 2000 , pp. 129{137.

Ward, D. J., and MacKay, D. J. C. (2002) Fast hands-free

writing by gaze direction. Nature 418 (6900): 838.

Welch, T. A. (1984) A technique for high-performance data

compression. IEEE Computer 17 (6): 8{19.

Welling, M., and Teh, Y. W. (2001) Belief optimization for
binary networks: A stable alternative to loopy belief propaga-
tion. In Uncertainty in Arti(cid:12)cial Intelligence: Proc. Seventeenth
Conf. (UAI-2001), pp. 554{561. Morgan Kaufmann.

Wiberg, N.

(1996) Codes and Decoding on General Graphs.
Dept. of Elec. Eng., Link(cid:127)oping, Sweden PhD dissertation.
Link(cid:127)oping Studies in Science and Technology No. 440.

Wiberg, N., Loeliger, H.-A., and K(cid:127)otter, R. (1995) Codes
and iterative decoding on general graphs. European Trans. on
Telecommunications 6: 513{525.

Wiener, N. (1948) Cybernetics. Wiley.
Williams, C. K. I., and Rasmussen, C. E. (1996) Gaussian
processes for regression.
In Advances in Neural Information
Processing Systems 8 , ed. by D. S. Touretzky, M. C. Mozer,
and M. E. Hasselmo. MIT Press.

Williams, C. K. I., and Seeger, M. (2001) Using the Nystr(cid:127)om
Method to Speed Up Kernel Machines. In Advances in Neural
Information Processing Systems 13 , ed. by T. K. Leen, T. G.
Diettrich, and V. Tresp, pp. 682{688. MIT Press.

Witten, I. H., Neal, R. M., and Cleary, J. G. (1987) Arith-
metic coding for data compression. Communications of the
ACM 30 (6): 520{540.

Wolf, J. K., and Siegel, P. (1998) On two-dimensional arrays
and crossword puzzles. In Proc. 36th Allerton Conf. on Com-
munication, Control, and Computing, Sept. 1998 , pp. 366{371.
Allerton House.

Worthen, A. P., and Stark, W. E. (1998) Low-density parity
check codes for fading channels with memory.
In Proc. 36th
Allerton Conf. on Communication, Control, and Computing,
Sept. 1998 , pp. 117{125.

Yedidia, J. S.

(2000) An idiosyncratic journey beyond mean
(cid:12)eld theory. Technical report, Mitsubishi Electric Res. Labs.
TR-2000-27.

Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2000) Gener-
alized belief propagation. Technical report, Mitsubishi Electric
Res. Labs. TR-2000-26.

Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2001a) Bethe
free energy, Kikuchi approximations and belief propagation al-
gorithms. Technical report, Mitsubishi Electric Res. Labs. TR-
2001-16.

Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2001b) Char-
acterization of belief propagation and its generalizations. Tech-
nical report, Mitsubishi Electric Res. Labs. TR-2001-15.

Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2002) Con-
structing free energy approximations and generalized belief
propagation algorithms. Technical report, Mitsubishi Electric
Res. Labs. TR-2002-35.

Yeung, R. W. (1991) A new outlook on Shannon-information

measures. IEEE Trans. Info. Theory 37 (3.1): 466{474.

Yuille, A. L.

(2001) A double-loop algorithm to minimize
the Bethe and Kikuchi free energies. In Energy Minimization
Methods in Computer Vision and Pattern Recognition, ed. by
M. Figueiredo, J. Zerubia, and A. Jain, number 2134 in LNCS,
pp. 3{18. Springer.

Zipf, G. K. (1949) Human Behavior and the Principle of Least

E(cid:11)ort. Addison-Wesley.

Ziv, J., and Lempel, A. (1977) A universal algorithm for sequen-
tial data compression. IEEE Trans. Info. Theory 23 (3): 337{
343.

Ziv, J., and Lempel, A. (1978) Compression of individual se-
quences via variable-rate coding. IEEE Trans. Info. Theory 24
(5): 530{536.

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

(cid:0), 598
(cid:8)(z), 514
(cid:31)2, 40, 323, 458, 459
(cid:21), 119
(cid:27)N and (cid:27)N(cid:0)1, 320
:=, 600
?, 419
2s, 156

Abu-Mostafa, Yaser, 482
acceptance rate, 365, 369, 394
acceptance ratio method, 379
accumulator, 254, 570, 582
activation function, 471
activity rule, 470, 471
adaptive direction sampling, 393
adaptive models, 101
adaptive rejection sampling, 370
address, 201, 468
Aiyer, Sree, 518
Alberto, 56
alchemists, 74
algebraic coding theory, 19, 574
algorithm, see learning algorithms

BCJR, 330
belief propagation, 330, 336
covariant, 442
EM, 432
exact sampling, 413
expectation{maximization, 432
function minimization, 473
genetic, 395, 396
Hamiltonian Monte Carlo, 387,

496

independent component analysis,

443

Langevin Monte Carlo, 496
leapfrog, 389
max{product, 339
message-passing, 330
min{sum, 339
Monte Carlo, see Monte Carlo

methods

Newton{Raphson, 303, 441
perfect simulation, 413
sum{product, 336
Viterbi, 340

Alice, 199
Allais paradox, 454
alphabetical ordering, 194
America, 354
American, 238, 260
amino acid, 201, 204, 279, 362
anagram, 200

Index

annealing, 379, 392, 397

deterministic, 518
importance sampling, 379

antiferromagnetic, 400
ape, 269
approximation

by Gaussian, 2, 301, 341, 350,

496

Laplace, 341, 547
of complex distribution, 185, 282,

364, 422, 433

of density evolution, 567
saddle-point, 341
Stirling, 1
variational, 422

arabic, 127
architecture, 470, 529
arithmetic coding, 101, 110, 111

decoder, 118
software, 121
uses beyond compression, 118,

250, 255

arithmetic progression, 344
arms race, 278
arti(cid:12)cial intelligence, 121, 129
associative memory, 468, 505, 507
assumptions, 26, 50
astronomy, 551
asymptotic equipartition, 80, 384

why it is a misleading term, 83

Atlantic, 173
AutoClass, 306
automatic relevance determination,

532, 544

automobile data reception, 594
average, 26, see expectation
AWGN, 177

background rate, 307
backpropagation, 473, 475, 528, 535
backward pass, 244
bad, see error-correcting code
Balakrishnan, Sree, 518
balance, 66
Baldwin e(cid:11)ect, 279
ban (unit), 264
Banburismus, 265
band-limited signal, 178
bandwidth, 178, 182
bar-code, 262, 399
base transitions, 373
base-pairing, 280
basis dependence, 306, 342
bat, 213, 214

620

battleships, 71
Bayes’ theorem, 6, 24, 25, 27, 28,

48{50, 53, 148, 324, 344,
347, 446, 493, 522

Bayes, Rev. Thomas, 51
Bayesian belief networks, 293
Bayesian inference, 26, 346, 457
BCH codes, 13
BCJR algorithm, 330, 578
bearing, 307
Belarusian, 238
belief, 57
belief propagation, 330, 557, see

message passing and
sum{product algorithm

Benford’s law, 446
bent coin, 1, 30, 38, 51, 76, 113, 307
Berlekamp, Elwyn, 172, 213
Bernoulli distribution, 117
Berrou, C., 186
bet, 200, 209, 455
beta distribution, 316
beta function, 316
beta integral, 30
Bethe free energy, 434
Bhattacharyya parameter, 215
bias, 345, 506

in neural net, 471
in statistics, 306, 307, 321

biexponential, 88, 313, 448
bifurcation, 89, 291, 426
binary entropy function, 2, 15
binary erasure channel, 148, 151
binary images, 399
binary representations, 132
binary symmetric channel, 4, 148,

151, 211, 215, 229

binding DNA, 201
binomial distribution, 1, 311
bipartite graph, 19
birthday, 156, 157, 160, 198, 200
bit, 3, 73
bit (unit), 264
bits back, 104, 108, 353
bivariate Gaussian, 388
black, 355
Bletchley Park, 265
Blind Watchmaker, 269, 396
block code, 9, see source code or

error-correcting code

block-sorting, 121
blood group, 55
blow up, 306
blur, 549

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Index

Bob, 199
Boltzmann entropy, 85
Boltzmann machine, 522
bombes, 265
book ISBN, 235
bookies, 456
Bottou, Leon, 121
bound, 85

union, 166, 216, 230

bounded-distance decoder, 207, 212
bounding chain, 419
box, 343, 351
boyish matters, 58
brain, 468
Bridge, 126
British, 260
broadcast channel, 237, 239, 594
Brody, Carlos, 246
Brownian motion, 280, 316, 535
BSC, see channel, binary symmetric
budget, 94, 96
Bu(cid:11)on’s needle, 38
BUGS, 371, 431
buoy, 307
burglar alarm and earthquake, 293
Burrows{Wheeler transform, 121
burst errors, 185, 186
bus-stop paradox, 39, 46, 107
byte, 134, 265

cable labelling, 175
calculator, 320
camera, 549
canonical, 88
capacity, 14, 146, 150, 151, 183, 484

channel with synchronization

errors, 187

constrained channel, 251
Gaussian channel, 182
Hop(cid:12)eld network, 514
neural network, 483
neuron, 483
symmetry argument, 151

car data reception, 594
card, 233
casting out nines, 198
Cauchy distribution, 85, 88, 313, 362
caution, see sermon

equipartition, 83
Gaussian distribution, 312
importance sampling, 362, 382
sampling theory, 64

cave, 214
caveat, see caution and sermon
cellphone, see mobile phone
cellular automaton, 130
central-limit theorem, 36, 41, 88, 131,

see law of large numbers

centre of gravity, 35
chain rule, 528
challenges, 246

Tanner, 569

channel

AWGN, 177
binary erasure, 148, 151

binary symmetric, 4, 146, 148,

151, 206, 211, 215, 229

broadcast, 237, 239, 594
bursty, 185, 557
capacity, 14, 146, 150, 250

connection with physics, 257

coding theorem, see

noisy-channel coding
theorem

complex, 184, 557
constrained, 248, 255, 256
continuous, 178
discrete memoryless, 147
erasure, 188, 219, 589
extended, 153
fading, 186
Gaussian, 155, 177, 186
input ensemble, 150
multiple access, 237
multiterminal, 239
noiseless, 248
noisy, 3, 146
noisy typewriter, 148, 152
symmetric, 171
two-dimensional, 262
unknown noise level, 238
variable symbol durations, 256
with dependent sources, 236
with memory, 557
Z channel, 148, 149, 150, 172

cheat, 200
Chebyshev inequality, 81, 85
checkerboard, 404, 520
Cherno(cid:11) bound, 85
chess, 451
chess board, 406, 520
chi-squared, 27, 40, 323, 458
Cholesky decomposition, 552
chromatic aberration, 552
cinema, 187
circle, 316
classical statistics, 64

criticisms, 32, 50, 457

classi(cid:12)er, 532
Clockville, 39
clustering, 284, 303
coalescence, 413
cocked hat, 307
code, see error-correcting code, source
code (for data compression),
symbol code, arithmetic
coding, linear code, random
code or hash code

dual, see error-correcting code,

dual

for constrained channel, 249

variable-length, 255

code-equivalent, 576
codebreakers, 265
codeword, see source code, symbol
code, or error-correcting
code

coding theory, 4, 19, 205, 215, 574
coin, 1, 30, 38, 63, 76, 307, 464
coincidence, 267, 343, 351

621

collective, 403
collision, 200
coloured noise, 179
combination, 2, 490, 598
commander, 241
communication, v, 3, 16, 138, 146,

156, 162, 167, 178, 182, 186,
192, 205, 210, 215, 394, 556,
562, 596

broadcast, 237
of dependent information, 236
over noiseless channels, 248
perspective on learning, 483, 512

competitive learning, 285
complexity, 531, 548
complexity control, 289, 346, 347, 349
compress, 119
compression, see source code

future methods, 129
lossless, 74
lossy, 74, 284, 285
of already-compressed (cid:12)les, 74
of any (cid:12)le, 74
universal, 121

computer, 370
concatenation, 185, 214, 220

error-correcting codes, 16, 21,

184, 185, 579
in compression, 92
in Markov chains, 373

concave _, 35
conditional entropy, 138, 146
cones, 554
con(cid:12)dence interval, 457, 464
con(cid:12)dence level, 464
confused gameshow host, 57
conjugate gradient, 479
conjugate prior, 319
conjuror, 233
connection between

channel capacity and physics, 257
error correcting code and latent

variable model, 437
pattern recognition and

error-correction, 481

supervised and unsupervised

learning, 515

vector quantization and

error-correction, 285

connection matrix, 253, 257
constrained channel, 248, 257, 260,

399

variable-length code, 249

constraint satisfaction, 516
content-addressable memory, 192, 193,

469, 505

continuous channel, 178
control treatment, 458
conventions, see notation
convex hull, 102
convex ^, 35
convexity, 370
convolution, 568
convolutional code, 184, 186, 574, 587

equivalence, 576

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

622

Conway, John H., 86, 520
Copernicus, 346
correlated sources, 138, 237
correlations, 505

among errors, 557
and phase transitions, 602
high-order, 524
in images, 549

cost function, 180, 451
cost of males, 277
counting, 241
counting argument, 21, 222
coupling from the past, 413
covariance, 440
covariance function, 535
covariance matrix, 176
covariant algorithm, 442
Cover, Thomas, 456, 482
Cox axioms, 26
crib, 265, 268
critical (cid:13)uctuations, 403
critical path, 246
cross-validation, 353, 531
crossover, 396
crossword, 260
cryptanalysis, 265, 578
cryptography, 200, 578

digital signatures, 199
tamper detection, 199

cumulative probability function, 156
cycles in graphs, 242
cyclic, 19

Dasher, 119
data compression, 73, see source code

and compression

data entry, 118
data modelling, see modelling
data set, 288
Davey, Matthew C., 569
death penalty, 354, 355
deciban (unit), 264
decibel, 178, 186
decision theory, 346, 451
decoder, 4, 146, 152

bitwise, 220, 324
bounded-distance, 207
codeword, 220, 324
maximum a posteriori, 325
probability of error, 221

deconvolution, 551
degree, 568
degree sequence, see pro(cid:12)le
degrees of belief, 26
degrees of freedom, 322, 459
d(cid:19)ej(cid:18)a vu, 121
delay line, 575
Delbr(cid:127)uck, Max, 446
deletions, 187
delta function, 438, 600
density evolution, 566, 567, 592
density modelling, 284, 303
dependent sources, 138, 237
depth of lake, 359
design theory, 209
detailed balance, 374, 391

detection of forgery, 199
deterministic annealing, 518
dictionary, 72, 119
die, rolling, 38
di(cid:11)erence-set cyclic code, 569
di(cid:11)erentiator, 254
di(cid:11)usion, 316
digamma function, 598
digital cinema, 187
digital fountain, 590
digital signature, 199, 200
digital video broadcast, 593
dimensions, 180
dimer, 204
directory, 193
Dirichlet distribution, 316
Dirichlet model, 117
discriminant function, 179
discriminative training, 552
disease, 25, 458
disk drive, 3, 188, 215, 248, 255
distance, 205
DKL, 34
bad, 207, 214
distance distribution, 206
entropy distance, 140
Gilbert{Varshamov, 212, 221
good, 207
Hamming, 206
isn’t everything, 215
of code, 206, 214, 220

good/bad, 207

of concatenated code, 214
of product code, 214
relative entropy, 34
very bad, 207

distribution, 311

beta, 316
biexponential, 313
binomial, 311
Cauchy, 85, 312
Dirichlet, 316
exponential, 311, 313
gamma, 313
Gaussian, 312

sample from, 312

inverse-cosh, 313
log-normal, 315
Luria{Delbr(cid:127)uck, 446
normal, 312
over periodic variables, 315
Poisson, 175, 311, 315
Student-t, 312
Von Mises, 315

divergence, 34
DjVu, 121
DNA, 3, 55, 201, 204, 257, 421

replication, 279, 280

do the right thing, 451
dodecahedron code, 20, 206, 207
dongle, 558
doors, on game show, 57
Dr. Bloggs, 462
draw straws, 233
dream, 524

Index

DSC, see di(cid:11)erence-set cyclic code
dual, 216
dumb Metropolis, 394, 496

Eb=N0, 177, 178, 223
earthquake and burglar alarm, 293
earthquake, during game show, 57
Ebert, Todd, 222
edge, 251
eigenvalue, 254, 342, 372, 409, 606
Elias, Peter, 111, 135
EM algorithm, 283, 432
email, 201
empty string, 119
encoder, 4
energy, 291, 401, 601
English, 72, 110, 260
Enigma, 265, 268
ensemble, 67

extended, 76

ensemble learning, 429
entropic distribution, 318, 551
entropy, 2, 32, 67, 601

Boltzmann, 85
conditional, 138
Gibbs, 85
joint, 138
marginal, 139
mutual information, 139
of continuous variable, 180
relative, 34

entropy distance, 140
epicycles, 346
equipartition, 80
erasure channel, 219, 589
erasure correction, 188, 190, 220
erf, 156, see error function
ergodic, 120, 373
error bars, 301, 501
error correction, see error-correcting

code

in DNA replication, 280
in protein synthesis, 280
error detection, 198, 199, 203
error (cid:13)oor, 581
error function, 156, 473, 490, 514, 529,

599

error probability

and distance, 215, 221
block, 152
in compression, 74

error-correcting code, 188, 203

bad, 183, 207
block code, 9, 151, 183
concatenated, 184{186, 214, 579
convolutional, 184, 574, 587
cyclic, 19
decoding, 184
density evolution, 566
di(cid:11)erence-set cyclic, 569
distance, see distance
dodecahedron, 20, 206, 207
dual, 216, 218
equivalence, 576
erasure channel, 589
error probability, 171, 215, 221

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Index

fountain code, 589
Gallager, 557
Golay, 209
good, 183, 184, 207, 214, 218
Hamming, 19, 214
in DNA replication, 280
in protein synthesis, 280
interleaving, 186
linear, 9, 171, 183, 184, 229

coding theorem, 229

low-density generator-matrix,

218, 590

low-density parity-check, 20, 187,

218, 557, 596

fast encoding, 569
pro(cid:12)le, 569
staircase, 569

LT code, 590
maximum distance separable, 220
nonlinear, 187
P3, 218
parity-check code, 220
pentagonful, 221
perfect, 208, 211, 212, 219, 589
practical, 183, 187
product code, 184, 214
quantum, 572
random, 184
random linear, 211, 212
raptor code, 594
rate, 152, 229
rateless, 590
rectangular, 184
Reed{Solomon code, 571, 589
repeat{accumulate, 582
repetition, 183
simple parity, 218
sparse graph, 556

density evolution, 566

syndrome decoding, 11, 371
variable rate, 238, 590
very bad, 207
very good, 183
weight enumerator, 206
with varying level of protection,

239

error-reject curves, 533
errors, see channel
estimator, 48, 307, 320, 446, 459
eugenics, 273
euro, 63
evidence, 29, 53, 298, 322, 347, 531

typical behaviour of, 54, 60

evolution, 269, 279

as learning, 277
Baldwin e(cid:11)ect, 279
colour vision, 554
of the genetic code, 279

evolutionary computing, 394, 395
exact sampling, 413
exchange rate, 601
exchangeability, 263
exclusive or, 590
EXIT chart, 567
expectation, 27, 35, 37

expectation propagation, 340
expectation{maximization algorithm,

283, 432

experimental design, 463
experimental skill, 309
explaining away, 293, 295
exploit, 453
explore, 453
exponential distribution, 45, 313

on integers, 311

exponential-family, 307, 308
expurgation, 167, 171
extended channel, 153, 159
extended code, 92
extended ensemble, 76
extra bit, 98, 101
extreme value, 446
eye movements, 554

factor analysis, 437, 444
factor graph, 334{336, 434, 556, 557,

580, 583

factorial, 2
fading channel, 186
feedback, 506, 589
female, 277
ferromagnetic, 400
Feynman, Richard, 422
Fibonacci, 253
(cid:12)eld, 605, see Galois (cid:12)eld
(cid:12)le storage, 188
(cid:12)nger, 119
(cid:12)nite (cid:12)eld theory, see Galois (cid:12)eld
(cid:12)tness, 269, 279
(cid:12)xed point, 508
Florida, 355
(cid:13)uctuation analysis, 446
(cid:13)uctuations, 401, 404, 427, 602
focus, 529
football pools, 209
forensic, 47, 421
forgery, 199, 200
forward pass, 244
forward probability, 27
forward{backward algorithm, 326, 330
Fotherington{Thomas, 241
fountain code, 589
Fourier transform, 88, 219, 339, 544,

568

fovea, 554
free energy, 257, 407, 409, 410, see

partition function

minimization, 423
variational, 423

frequency, 26
frequentist, 320, see sampling theory
Frey, Brendan J., 353
Frobenius{Perron theorem, 410
frustration, 406
full probabilistic model, 156
function minimization, 473
functions, 246

gain, 507
Galileo code, 186
Gallager code, 557

623

Gallager, Robert G., 170, 187, 557
Galois (cid:12)eld, 185, 224, 567, 568, 605
gambling, 455
game, see puzzle
Bridge, 126
chess, 451
guess that tune, 204
guessing, 110
life, 520
sixty-three, 70
submarine, 71
three doors, 57, 60, 454
twenty questions, 70

game show, 57, 454
game-playing, 451
gamma distribution, 313, 319
gamma function, 598
ganglion cells, 491
Gaussian channel, 155, 177
Gaussian distribution, 2, 36, 176, 312,

321, 398, 549

N {dimensional, 124
approximation, 501
parameters, 319
sample from, 312

Gaussian processes, 535

variational classi(cid:12)er, 547

general position, 484
generalization, 483
generalized parity-check matrix, 581
generating function, 88
generative model, 27, 156
generator matrix, 9, 183
genes, 201
genetic algorithm, 269, 395, 396
genetic code, 279
genome, 201, 280
geometric progression, 258
geostatistics, 536, 548
GF(q), see Galois (cid:12)eld
Gibbs entropy, 85
Gibbs sampling, 370, 391, 418, see

Monte Carlo methods

Gibbs’ inequality, 34, 37, 44
Gilbert{Varshamov conjecture, 212
Gilbert{Varshamov distance, 212, 221
Gilbert{Varshamov rate, 212
Gilks, Wally R., 393
girlie stu(cid:11), 58
Glauber dynamics, 370
Glavieux, A., 186
Golay code, 209
golden ratio, 253
good, see error-correcting code
Good, Jack, 265
gradient descent, 476, 479, 498, 529

natural, 443

graduated non-convexity, 518
Graham, Ronald L., 175
grain size, 180
graph, 251

factor graph, 334
of code, 19, 20, 556

graphs and cycles, 242
guerilla, 242

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

624

guessing decoder, 224
guessing game, 110, 111, 115
Gull, Steve, 48, 61, 551
gzip, 119

Haldane, J.B.S., 278
Hamilton, William D., 278
Hamiltonian Monte Carlo, 387, 397,

496, 497

Hamming code, 8, 17, 183, 184, 190,

208, 209, 214, 219

graph, 19

Hamming distance, 206
handwritten digits, 156
hard drive, 593
hash code, 193, 231
hash function, 195, 200, 228

linear, 231
one-way, 200

hat puzzle, 222
heat bath, 370, 601
heat capacity, 401, 404
Hebb, Donald, 505
Hebbian learning, 505, 507
Hertz, 178
Hessian, 501
hidden Markov model, 437
hidden neurons, 525
hierarchical clustering, 284
hierarchical model, 379, 548
high dimensions, life in, 37, 124
hint for computing mutual
information, 149

Hinton, Geo(cid:11)rey E., 353, 429, 432, 522
hitchhiker, 280
homogeneous, 544
Hooke, Robert, 200
Hop(cid:12)eld network, 283, 505, 506, 517

capacity, 514

Hop(cid:12)eld, John J., 246, 280, 517
horse race, 455
hot-spot, 275
Hu(cid:11)man code, 91, 99, 103

‘optimality’, 99, 101
disadvantages, 100, 115
general alphabet, 104, 107

human, 269
human{machine interfaces, 119, 127
hybrid Monte Carlo, 387, see

Hamiltonian Monte Carlo

hydrogen bond, 280
hyperparameter, 64, 309, 318, 319,

379, 479

hypersphere, 42
hypothesis testing, see model

comparison, sampling theory

i.i.d., 80
ICA, see independent component

analysis

ICF (intrinsic correlation function),

551

identical twin, 111
identity matrix, 600
ignorance, 446
ill-posed problem, 309, 310

image, 549

integral, 246

image analysis, 343, 351
image compression, 74, 284
image models, 399
image processing, 246
image reconstruction, 551
implicit assumptions, 186
implicit probabilities, 97, 98, 102
importance sampling, 361, 379

weakness of, 382

improper, 314, 316, 319, 320, 342, 353
in-car navigation, 594
independence, 138
independent component analysis, 313,

437, 443

indicator function, 600
inequality, 35, 81
inference, 27, 529

and learning, 493

information content, 32, 72, 73, 91,

97, 115, 349

how to measure, 67
Shannon, 67

information maximization, 443
information retrieval, 193
information theory, 4
inner code, 184
Inquisition, 346
insertions, 187
instantaneous, 92
integral image, 246
interleaving, 184, 186, 579
internet, 188, 589
intersection, 66, 222
intrinsic correlation function, 549, 551
invariance, 445
invariant distribution, 372
inverse probability, 27
inverse-arithmetic-coder, 118
inverse-cosh distribution, 313
inverse-gamma distribution, 314
inversion of hash function, 199
investment portfolio, 455
irregular, 568
ISBN, 235
Ising model, 130, 283, 399, 400
iterative probabilistic decoding, 557

Jaakkola, Tommi S., 433, 547
Jacobian, 320
janitor, 464
Je(cid:11)reys prior, 316
Jensen’s inequality, 35, 44
Jet Propulsion Laboratory, 186
Johnson noise, 177
joint ensemble, 138
joint entropy, 138
joint typicality, 162
joint typicality theorem, 163
Jordan, Michael I., 433, 547
journal publication policy, 463
judge, 55
juggling, 15
junction tree algorithm, 340
jury, 26, 55

Index

K-means clustering, 285, 303

derivation, 303
soft, 289

kaboom, 306, 433
Kalman (cid:12)lter, 535
kernel, 548
key points

communication, 596
how much data needed, 53
likelihood principle, 32
model comparison, 53
Monte Carlo, 358, 367
solving probability problems, 61

keyboard, 119
Kikuchi free energy, 434
KL distance, 34
Knowlton{Graham partitions, 175
Knuth, Donald, xii
Kolmogorov, Andrei Nikolaevich, 548
Kraft inequality, 94, 521
Kraft, L.G., 95
kriging, 536
Kullback{Leibler divergence, 34, see

relative entropy

Lagrange multiplier, 174
lake, 359
Langevin method, 496, 498
Langevin process, 535
language model, 119
Laplace approximation, see Laplace’s

method

Laplace model, 117
Laplace prior, 316
Laplace’s method, 341, 354, 496, 501,

537, 547
Laplace’s rule, 52
latent variable, 432, 437
latent variable model, 283

compression, 353

law of large numbers, 36, 81, 82, 85
lawyer, 55, 58, 61
Le Cun, Yann, 121
leaf, 336
leapfrog algorithm, 389
learning, 471

as communication, 483
as inference, 492, 493
Hebbian, 505, 507
in evolution, 277

learning algorithms, 468, see

algorithm

backpropagation, 528
Boltzmann machine, 522
classi(cid:12)cation, 475
competitive learning, 285
Hop(cid:12)eld network, 505
K-means clustering, 286, 289,

303

multilayer perceptron, 528
single neuron, 475

learning rule, 470
Lempel{Ziv coding, 110, 119{122

criticisms, 128

life, 520

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Index

life in high dimensions, 37, 124
likelihood, 6, 28, 49, 152, 324, 529, 558

contrasted with probability, 28
subjectivity, 30

likelihood equivalence, 447
likelihood principle, 32, 61, 464
limit cycle, 508
linear block code, 9, 11, 19, 171, 183,

186, 206, 229

coding theorem, 229
decoding, 184

linear regression, 342, 527
linear-feedback shift-register, 184, 574
Litsyn, Simon, 572
little ’n’ large data set, 288
log-normal, 315
logarithms, 2
logit, 307, 316
long thin strip, 409
loopy belief propagation, 434
loopy message-passing, 338, 340, 556
loss function, 451
lossy compression, 168, 284, 285
low-density generator-matrix code,

207, 590

low-density parity-check code, 557,
see error-correcting code

LT code, 590
Luby, Michael G., 568, 590
Luria, Salvador, 446
Lyapunov function, 287, 291, 508,

520, 521

machine learning, 246
macho, 319
MacKay, David J.C., 187, 496, 557
magician, 233
magnet, 602
magnetic recording, 593
majority vote, 5
male, 277
Mandelbrot, Benoit, 262
MAP, see maximum a posteriori
mapping, 92
marginal entropy, 139, 140
marginal likelihood, 29, 298, 322, see

evidence

marginal probability, 23, 147
marginalization, 29, 295, 319
Markov chain, 141, 168

construction, 373

Markov chain Monte Carlo, see Monte

Carlo methods

Markov model, 111, 437, see Markov

chain

marriage, 454
matrix, 409
matrix identities, 438
max{product, 339
maxent, 308, see maximum entropy
maximum distance separable, 219
maximum entropy, 308, 551
maximum likelihood, 6, 152, 300, 347
maximum a posteriori, 6, 307, 325,

538

McCollough e(cid:11)ect, 553

MCMC (Markov chain Monte Carlo),

see Monte Carlo methods

McMillan, B., 95
MD5, 200
MDL, see minimum description length
MDS, 220
mean, 1
mean (cid:12)eld theory, 422, 425
melody, 201, 203
memory, 468

address-based, 468
associative, 468, 505
content-addressable, 192, 469

MemSys, 551
message passing, 187, 241, 248, 283,

324, 407, 556, 591

BCJR, 330
belief propagation, 330
forward{backward, 330
in graphs with cycles, 338
loopy, 338, 340, 434
sum{product algorithm, 336
Viterbi, 329

metacode, 104, 108
metric, 512
Metropolis method, 496, see Monte

Carlo methods

M(cid:19)ezard, Marc, 340
micro-saccades, 554
microcanonical, 87
microsoftus, 458
microwave oven, 127
min{sum algorithm, 245, 325, 329,

339, 578, 581

mine (hole in ground), 451
minimax, 455
minimization, 473, see optimization
minimum description length, 352
minimum distance, 206, 214, see

distance

Minka, Thomas, 340
mirror, 529
Mitzenmacher, Michael, 568
mixing coe(cid:14)cients, 298, 312
mixture modelling, 282, 284, 303, 437
mixture of Gaussians, 312
mixtures in Markov chains, 373
ML, see maximum likelihood
MLP, see multilayer perceptron
MML, see minimum description

length

mobile phone, 182, 186
model, 111, 120
model comparison, 198, 346, 347, 349

typical evidence, 54, 60

modelling, 285

density modelling, 284, 303
images, 524
latent variable models, 353, 432,

437

nonparametric, 538
moderation, 29, 498, see

marginalization

molecules, 201
Molesworth, 241

625

momentum, 387, 479
Monte Carlo methods, 357, 498

acceptance rate, 394
acceptance ratio method, 379
and communication, 394
annealed importance sampling,

379

coalescence, 413
dependence on dimension, 358
exact sampling, 413
for visualization, 551
Gibbs sampling, 370, 391, 418
Hamiltonian Monte Carlo, 387,

496

hybrid Monte Carlo, see

Hamiltonian Monte Carlo
importance sampling, 361, 379

weakness of, 382

Langevin method, 498
Markov chain Monte Carlo, 365
Metropolis method, 365

dumb Metropolis, 394, 496

Metropolis{Hastings, 365
multi-state, 392, 395, 398
overrelaxation, 390, 391
perfect simulation, 413
random walk suppression, 370,

387

random-walk Metropolis, 388
rejection sampling, 364

adaptive, 370

reversible jump, 379
simulated annealing, 379, 392
slice sampling, 374
thermodynamic integration, 379
umbrella sampling, 379

Monty Hall problem, 57
Morse, 256
motorcycle, 110
movie, 551
multilayer perceptron, 529, 535
multiple access channel, 237
multiterminal networks, 239
multivariate Gaussian, 176
Munro{Robbins theorem, 441
murder, 26, 58, 61, 354
music, 201, 203
mutation rate, 446
mutual information, 139, 146, 150, 151

how to compute, 149

myth, 347

compression, 74

nat (unit), 264, 601
natural gradient, 443
natural selection, 269
navigation, 594
Neal, Radford M., 111, 121, 187, 374,
379, 391, 392, 397, 419, 420,
429, 432, 496

needle, Bu(cid:11)on’s, 38
network, 529
neural network, 468, 470

capacity, 483
learning as communication, 483
learning as inference, 492

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

626

neuron, 471

capacity, 483

Newton, Isaac, 200, 552
Newton{Raphson method, 303, 441
nines, 198
noise, 3, see channel

coloured, 179
spectral density, 177
white, 177, 179

noisy channel, see channel
noisy typewriter, 148, 152, 154
noisy-channel coding theorem, 15,

152, 162, 171, 229
Gaussian channel, 181
linear codes, 229
poor man’s version, 216

noisy-or, 294
non-confusable inputs, 152
noninformative, 319
nonlinear, 535
nonlinear code, 20, 187
nonparametric data modelling, 538
nonrecursive, 575
noodle, Bu(cid:11)on’s, 38
normal, 312, see Gaussian
normal graph, 219, 584
normalizing constant, see partition

function

not-sum, 335
notation, 598

absolute value, 33, 599
conventions of this book, 147
convex/concave, 35
entropy, 33
error function, 156
expectation, 37
intervals, 90
logarithms, 2
matrices, 147
probability, 22, 30
set size, 33, 599
transition probability, 147
vectors, 147

NP-complete, 184, 325, 517
nucleotide, 201, 204
nuisance parameters, 319
numerology, 208
Nyquist sampling theorem, 178

objective function, 473
Occam factor, 322, 345, 348, 350, 352
Occam’s razor, 343
octal, 575
octave, 478
odds, 456
Ode to Joy, 203
o(cid:14)cer for whimsical departmental

rules, 464

Oliver, 56
one-way hash function, 200
optic nerve, 491
optimal decoder, 152
optimal input distribution, 150, 162
optimal linear (cid:12)lter, 549
optimal stopping, 454

optimization, 169, 392, 429, 479, 505,

516, 531

gradient descent, 476
Newton algorithm, 441
of model complexity, 531

order parameter, 604
ordered overrelaxation, 391
orthodox statistics, 320, see sampling

theory
outer code, 184
over(cid:12)tting, 306, 322, 529, 531
overrelaxation, 390

p-value, 64, 457, 462
packet, 188, 589
paradox, 107

Allais, 454
bus-stop, 39
heat capacity, 401
Simpson’s, 355
waiting for a six, 38

paranormal, 233
parasite, 278
parent, 559
parity, 9
parity-check bits, 9, 199, 203
parity-check code, 220
parity-check constraints, 20
parity-check matrix, 12, 183, 229, 332

generalized, 581

parity-check nodes, 19, 219, 567, 568,

583

parse, 119, 448
Parsons code, 204
parthenogenesis, 273
partial order, 418
partial partition functions, 407
particle (cid:12)lter, 396
partition, 174
partition function, 401, 407, 409, 422,

423, 601, 603

analogy with lake, 360
partial, 407

partitioned inverse, 543
path-counting, 244
pattern recognition, 156, 179, 201
pentagonful code, 21, 221
perfect code, 208, 210, 211, 219, 589
perfect simulation, 413
periodic variable, 315
permutation, 19, 268
Petersen graph, 221
phase transition, 361, 403, 601
philosophy, 26, 119, 384
phone, 125, 594

cellular, see mobile phone

phone directory, 193
phone number, 58, 129
photon counter, 307, 342, 448
physics, 80, 85, 257, 357, 401, 422,

514, 601

pigeon-hole principle, 86, 573
pitchfork bifurcation, 291, 426
plaintext, 265
plankton, 359
point estimate, 432

Index

point spread function, 549
pointer, 119
poisoned glass, 103
Poisson distribution, 2, 175, 307, 311,

342

Poisson process, 39, 46, 448
Poissonville, 39, 313
polymer, 257
poor man’s coding theorem, 216
porridge, 280
portfolio, 455
positive de(cid:12)nite, 539
positivity, 551
posterior probability, 6, 152
power cost, 180
power law, 584
practical, 183, see error-correcting

code

precision, 176, 181, 312, 320, 383
precisions add, 181
prediction, 29, 52
predictive distribution, 111
pre(cid:12)x code, 92, 95
prior, 6, 308, 529

assigning, 308
improper, 353
Je(cid:11)reys, 316
subjectivity, 30

prior equivalence, 447
priority of bits in a message, 239
prize, on game show, 57
probabilistic model, 111, 120
probabilistic movie, 551
probability, 26, 38
Bayesian, 50
contrasted with likelihood, 28
density, 30, 33

probability distributions, 311, see

distribution

probability of block error, 152
probability propagation, see

sum{product algorithm

product code, 184, 214
pro(cid:12)le, of random graph, 568
pronunciation, 34
proper, 539
proposal density, 364, 365
Propp, Jim G., 413, 418
prosecutor’s fallacy, 25
prospecting, 451
protein, 204, 269

regulatory, 201, 204
synthesis, 280

protocol, 589
pseudoinverse, 550
Punch, 448
puncturing, 222, 580
pupil, 553
puzzle, see game

cable labelling, 173
chessboard, 520
(cid:12)delity of DNA replication, 280
hat, 222, 223
life, 520
magic trick, 233, 234

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

Index

poisoned glass, 103
secretary, 454
southeast, 520
transatlantic cable, 173
weighing 12 balls, 68

quantum error-correction, 572
queue, 454
QWERTY, 119

R3, see repetition code
race, 354
radial basis function, 535, 536
radio, 186
radix, 104
RAID, 188, 190, 219, 593
random, 26, 357
random cluster model, 418
random code, 156, 161, 164, 165, 184,

192, 195, 214, 565

for compression, 231

random number generator, 578
random variable, 26, 463
random walk, 367

suppression, 370, 387, 390, 395

random-coding exponent, 171
random-walk Metropolis method, 388
rant, see sermon
raptor codes, 594
rate, 152
rate-distortion theory, 167
rateless code, 590
reading aloud, 529
receiver operating characteristic, 533
recognition, 204
record breaking, 446
rectangular code, 184
reducible, 373
redundancy, 4, 33

in channel code, 146

redundant array of independent disks,

188, 190, 219, 593

redundant constraints in code, 20
Reed{Solomon code, 185, 571, 589
regression, 342, 536
regret, 455
regular, 557
regularization, 529, 550
regularization constant, 309, 479
reinforcement learning, 453
rejection, 364, 366, 533
rejection sampling, 364

adaptive, 370

relative entropy, 34, 98, 102, 142, 422,

429, 435, 475
reliability function, 171
repeat{accumulate code, 582
repetition code, 5, 13, 15, 16, 46, 183
responsibility, 289
retransmission, 589
reverse, 110
reversible, 374
reversible jump, 379
Richardson, Thomas J., 570, 595
Rissanen, Jorma, 111
ROC, 533

rolling die, 38
roman, 127
rule of thumb, 380
runlength, 256
runlength-limited channel, 249

saccades, 554
saddle-point approximation, 341
sailor, 307
sample, 312, 356

from Gaussian, 312

sampler density, 362
sampling distribution, 459
sampling theory, 38, 320

criticisms, 32, 64

sandwiching method, 419
satellite communications, 186, 594
scaling, 203
Sch(cid:127)onberg, 203
Schottky anomaly, 404
scientists, 309
secret, 200
secretary problem, 454
security, 199, 201
seek time, 593
Sejnowski, Terry J., 522
self-delimiting, 132
self-dual, 218
self-orthogonal, 218
self-punctuating, 92
separation, 242, 246
sequence, 344
sequential decoding, 581
sequential probability ratio test, 464
sermon, see caution

classical statistics, 64
con(cid:12)dence level, 465
dimensions, 180
gradient descent, 441
illegal integral, 180
importance sampling, 382
interleaving, 189
MAP method, 283, 306
maximum entropy, 308
maximum likelihood, 306
most probable is atypical, 283
p-value, 463
sampling theory, 64
sphere-packing, 209, 212
stopping rule, 463
turbo codes, 581
unbiased estimator, 307
worst-case-ism, 207

set, 66
shannon (unit), 265
Shannon, Claude, 3, 14, 15, 152, 164,

212, 215, 262, see
noisy-channel coding
theorem, source coding
theorem, information
content

shattering, 485
shifter ensemble, 524
Shokrollahi, M. Amin, 568
shortening, 222
Siegel, Paul, 262

627

sigmoid, 473, 527
signal-to-noise ratio, 177, 178, 223
signi(cid:12)cance level, 51, 64, 457, 463
simplex, 173, 316
Simpson’s paradox, 355
Simpson, O.J., see wife-beaters
simulated annealing, 379, 392, see

annealing
six, waiting for, 38
Skilling, John, 392
sleep, 524, 554
Slepian{Wolf, see dependent sources
slice sampling, 374

multi-dimensional, 378
soft K-means clustering, 289
softmax, softmin, 289, 316, 339
software, xi

arithmetic coding, 121
BUGS, 371
Dasher, 119
free, xii
Gaussian processes, 534
hash function, 200
VIBES, 431

solar system, 346
soldier, 241
soliton distribution, 592
sound, 187
source code, 73, see compression,
symbol code, arithmetic
coding, Lempel{Ziv

algorithms, 119, 121
block code, 76
block-sorting compression, 121
Burrows{Wheeler transform, 121
for complex sources, 353
for constrained channel, 249, 255
for integers, 132
Hu(cid:11)man, see Hu(cid:11)man code
implicit probabilities, 102
optimal lengths, 97, 102
pre(cid:12)x code, 95
software, 121
stream codes, 110{130
supermarket, 96, 104, 112
symbol code, 91
uniquely decodeable, 94
variable symbol durations, 125,

256

source coding theorem, 78, 91, 229,

231

southeast puzzle, 520
span, 331
sparse-graph code, 338, 556
density evolution, 566
pro(cid:12)le, 569

sparsi(cid:12)er, 255
species, 269
spell, 201
sphere packing, 182, 205
sphere-packing exponent, 172
Spielman, Daniel A., 568
spin system, 400
spines, 525
spline, 538

Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981
You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.

628

spread spectrum, 182, 188
spring, 291
spy, 464
square, 38
staircase, 569, 587
stalactite, 214
standard deviation, 320
stars, 307
state diagram, 251
statistic, 458

su(cid:14)cient, 300

statistical physics, see physics
statistical test, 51, 458
steepest descents, 441
stereoscopic vision, 524
sti(cid:11)ness, 289
Stirling’s approximation, 1, 8
stochastic, 472
stochastic dynamics, see Hamiltonian

Monte Carlo

stochastic gradient, 476
stop-when-it’s-done, 561, 583
stopping rule, 463
straws, drawing, 233
stream codes, 110{130
student, 125
Student-t distribution, 312, 323
subjective probability, 26, 30
submarine, 71
subscriber, 593
subset, 66
substring, 119
su(cid:14)cient statistics, 300
sum rule, 39, 46
sum{product algorithm, 187, 245, 326,
336, 407, 434, 556, 572, 578

summary, 335
summary state, 418
summation convention, 438
super-channel, 184
supermarket (codewords), 96, 104, 112
support vector, 548
surprise value, 264
survey propagation, 340
suspicious coincidences, 351
symbol code, 91

budget, 94, 96
codeword, 92
disadvantages, 100
optimal, 91
self-delimiting, 132
supermarket, 112

symmetric channel, 171
symmetry argument, 151
synchronization, 249
synchronization errors, 187
syndrome, 10, 11, 20
syndrome decoding, 11, 216, 229, 371
systematic, 575

t-distribution, see Student-t
tail, 85, 312, 313, 440, 446, 503, 584
tamper detection, 199
Tank, David W., 517
Tanner challenge, 569
Tanner product code, 571
Tanner, Michael, 569

Tanzanite, 451
tap, 575
telephone, see phone
telescope, 529
temperature, 392, 601
termination, 579
terminology, 598, see notation

Monte Carlo methods, 372

test

(cid:13)uctuation, 446
statistical, 51, 458

text entry, 118
thermal distribution, 88
thermodynamic integration, 379
thermodynamics, 404, 601

third law, 406

Thiele, T.N., 548
thin shell, 37, 125
third law of thermodynamics, 406
Thitimajshima, P., 186
three cards, 142
three doors, 57
threshold, 567
tiling, 420
time-division, 237
timing, 187
training data, 529
transatlantic, 173
transfer matrix method, 407
transition, 251
transition probability, 147, 356, 607
translation-invariant, 409
travelling salesman problem, 246, 517
tree, 242, 336, 343, 351
trellis, 251, 326, 574, 577, 580, 583,

608

section, 251, 257
termination, 579

triangle, 307
truth function, 211, 600
tube, 257
turbo code, 186, 556
turbo product code, 571
Turing, Alan, 265
twenty questions, 70, 103
twin, 111
twos, 156
typical evidence, 54, 60
typical set, 80, 154, 363
for compression, 80
for noisy channel, 154

typical-set decoder, 165, 230
typicality, 78, 80, 162

umbrella sampling, 379
unbiased estimator, 307, 321, 449
uncompression, 231
union, 66
union bound, 166, 216, 230
uniquely decodeable, 93, 94
units, 264
universal, 110, 120, 121, 135, 590
universality, in physics, 400
Urbanke, R(cid:127)udiger, 570, 595
urn, 31
user interfaces, 118
utility, 451

Index

vaccination, 458
Vapnik{Chervonenkis dimension, 489
variable-length code, 249, 255
variable-rate error-correcting codes,

238, 590

variance, 1, 27, 88, 321
variance{covariance matrix, 176
variances add, 1, 181
variational Bayes, 429
variational free energy, 422, 423
variational methods, 422, 433, 496,

508

typical properties, 435
variational Gaussian process, 547

VC dimension, 489
vector quantization, 284, 290
very good, see error-correcting code
VIBES, 431
Virtakallio, Juhani, 209
vision, 554
visualization, 551
Viterbi algorithm, 245, 329, 340, 578
volume, 42, 90
Von Mises distribution, 315

Wainwright, Martin, 340
waiting for a bus, 39, 46
warning, see caution and sermon
Watson{Crick base pairing, 280
weather collator, 236
weighing babies, 164
weighing problem, 66, 68
weight

importance sampling, 362
in neural net, 471
of binary vector, 20

weight decay, 479, 529
weight enumerator, 206, 211, 214, 216

typical, 572

weight space, 473, 474, 487
Wenglish, 72, 260
what number comes next?, 344
white, 355
white noise, 177, 179
Wiberg, Niclas, 187
widget, 309
Wiener process, 535
Wiener, Norbert, 548
wife-beater, 58, 61
Wilson, David B., 413, 418
window, 307
Winfree, Erik, 520
wodge, 309
Wolf, Jack, 262
word-English, 260
world record, 446
worst-case-ism, 207, 213
writing, 118

Yedidia, Jonathan, 340

Z channel, 148, 149{151, 155
Zipf plot, 262, 263, 317
Zipf’s law, 40, 262, 263
Zipf, George K., 262

