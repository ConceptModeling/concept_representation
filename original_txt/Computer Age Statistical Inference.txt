The Work, Computer Age Statistical Inference, was ﬁrst published by Cambridge University Press.
c(cid:13) in the Work, Bradley Efron and Trevor Hastie, 2016.
Cambridge University Press’s catalogue entry for the Work can be found at http: // www. cambridge. org/
9781107149892
NB: The copy of the Work, as displayed on this website, can be purchased through Cambridge University
Press and other standard distribution channels. This copy is made available for personal use only and must
not be adapted, sold or re-distributed.
Corrected November 10, 2017.

The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. “Big data,” “data science,” and “machine learning” have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going?This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories – Bayesian, frequentist, Fisherian – individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.Efron & hastiEComputEr agE  statistiCal infErEnCE“How and why is computational statistics taking over the world? In this serious work of synthesis that is also fun to read, Efron and Hastie give their take on the unreasonable effectiveness of statistics and machine learning in the context of a series of clear, historically informed examples.”— Andrew Gelman, Columbia University “Computer Age Statistical Inference is written especially for those who want to hear the big ideas, and see them instantiated through the essential mathematics that defines statistical analysis. It makes a great supplement to the traditional curricula for beginning graduate students.”— Rob Kass, Carnegie Mellon University “This is a terrific book. It gives a clear, accessible, and entertaining account of the interplay between theory and methodological development that has driven statistics in the computer age. The authors succeed brilliantly in locating contemporary algorithmic methodologies for analysis of ‘big data’ within the framework of established statistical theory.”— Alastair Young, Imperial College London “This is a guided tour of modern statistics that emphasizes the conceptual and computational advances of the last century. Authored by two masters of the field, it offers just the right mix of mathematical analysis and insightful commentary.”— Hal Varian, Google “Efron and Hastie guide us through the maze of breakthrough statistical methodologies following the computing evolution: why they were developed, their properties, and how they are used. Highlighting their origins, the book helps us understand each method’s roles in inference and/or prediction.”— Galit Shmueli, National Tsing Hua University “A masterful guide to how the inferential bases of classical statistics can provide a principled disciplinary frame for the data science of the twenty-first century.” — Stephen Stigler, University of Chicago, author of Seven Pillars of Statistical Wisdom “A refreshing view of modern statistics. Algorithmics are put on equal footing with intuition, properties, and the abstract arguments behind them. The methods covered are indispensable to practicing statistical analysts in today’s big data and big computing landscape.”— Robert Gramacy, The University of Chicago Booth School of BusinessBradley Efron is Max H. Stein Professor, Professor of Statistics, and Professor of Biomedical Data Science at Stanford University. He has held visiting faculty appointments at Harvard, UC Berkeley, and Imperial College London. Efron has worked extensively on theories of statistical inference, and is the inventor of the bootstrap sampling technique. He received the National Medal of Science in 2005 and the Guy Medal in Gold of the Royal Statistical Society in 2014. Trevor Hastie is John A. Overdeck Professor, Professor of Statistics, and Professor of Biomedical Data Science at Stanford University. He is coauthor of Elements of Statistical Learning, a key text in the field of modern data analysis. He is also known for his work on generalized additive models and principal curves, and for his contributions to the R computing environment. Hastie was awarded the Emmanuel and Carol Parzen prize for Statistical Innovation in 2014. Institute of Mathematical Statistics MonographsEditorial Board:D. R. Cox (University of Oxford)B. Hambly (University of Oxford)S. Holmes (Stanford University)J. Wellner (University of Washington)Cover illustration: Pacific Ocean wave, North Shore, Oahu, Hawaii. © Brian Sytnyk / Getty Images.Cover designed by Zoe Naylor.PRINTED IN THE UNITED KINGDOMComputEr agE statistiCal  infErEnCEalgorithms, EvidEnCE, and data sCiEnCEBradlEy Efron trEvor hastiE9781107149892 Efron & Hastie JKT C M Y KComputer Age Statistical Inference

Algorithms, Evidence, and Data Science

Bradley Efron

Trevor Hastie

Stanford University

To Donna and Lynda

viii

Contents

Preface
Acknowledgments
Notation

1
1.1
1.2
1.3

2
2.1
2.2
2.3

3
3.1
3.2
3.3
3.4
3.5

4
4.1
4.2
4.3
4.4
4.5

5

Part I Classic Statistical Inference

Algorithms and Inference
A Regression Example
Hypothesis Testing
Notes

Frequentist Inference
Frequentism in Practice
Frequentist Optimality
Notes and Details

Bayesian Inference
Two Examples
Uninformative Prior Distributions
Flaws in Frequentist Inference
A Bayesian/Frequentist Comparison List
Notes and Details

Fisherian Inference and Maximum Likelihood Estimation
Likelihood and Maximum Likelihood
Fisher Information and the MLE
Conditional Inference
Permutation and Randomization
Notes and Details

Parametric Models and Exponential Families

ix

xv
xviii
xix

1

3
4
8
11

12
14
18
20

22
24
28
30
33
36

38
38
41
45
49
51

53

x

5.1
5.2
5.3
5.4
5.5
5.6

6
6.1
6.2
6.3
6.4
6.5

7
7.1
7.2
7.3
7.4
7.5

8
8.1
8.2
8.3
8.4
8.5

Contents

Univariate Families
The Multivariate Normal Distribution
Fisher’s Information Bound for Multiparameter Families
The Multinomial Distribution
Exponential Families
Notes and Details

Part II Early Computer-Age Methods

Empirical Bayes
Robbins’ Formula
The Missing-Species Problem
A Medical Example
Indirect Evidence 1
Notes and Details

James–Stein Estimation and Ridge Regression
The James–Stein Estimator
The Baseball Players
Ridge Regression
Indirect Evidence 2
Notes and Details

Generalized Linear Models and Regression Trees
Logistic Regression
Generalized Linear Models
Poisson Regression
Regression Trees
Notes and Details

Survival Analysis and the EM Algorithm
Life Tables and Hazard Rates
Censored Data and the Kaplan–Meier Estimate
The Log-Rank Test
The Proportional Hazards Model

9
9.1
9.2
9.3
9.4
9.5 Missing Data and the EM Algorithm
9.6

Notes and Details

The Jackknife and the Bootstrap

10
10.1 The Jackknife Estimate of Standard Error
10.2 The Nonparametric Bootstrap
10.3 Resampling Plans

54
55
59
61
64
69

73

75
75
78
84
88
88

91
91
94
97
102
104

108
109
116
120
124
128

131
131
134
139
143
146
150

155
156
159
162

Contents

10.4 The Parametric Bootstrap
10.5
10.6 Notes and Details

Inﬂuence Functions and Robust Estimation

Bootstrap Conﬁdence Intervals

11
11.1 Neyman’s Construction for One-Parameter Problems
11.2 The Percentile Method
11.3 Bias-Corrected Conﬁdence Intervals
11.4 Second-Order Accuracy
11.5 Bootstrap-t Intervals
11.6 Objective Bayes Intervals and the Conﬁdence Distribution
11.7 Notes and Details

Cross-Validation and Cp Estimates of Prediction Error

12
12.1 Prediction Rules
12.2 Cross-Validation
12.3 Covariance Penalties
12.4 Training, Validation, and Ephemeral Predictors
12.5 Notes and Details

Objective Bayes Inference and MCMC

13
13.1 Objective Prior Distributions
13.2 Conjugate Prior Distributions
13.3 Model Selection and the Bayesian Information Criterion
13.4 Gibbs Sampling and MCMC
13.5 Example: Modeling Population Admixture
13.6 Notes and Details

14

Postwar Statistical Inference and Methodology

Part III Twenty-First-Century Topics

Large-Scale Hypothesis Testing and FDRs

15
15.1 Large-Scale Testing
15.2 False-Discovery Rates
15.3 Empirical Bayes Large-Scale Testing
15.4 Local False-Discovery Rates
15.5 Choice of the Null Distribution
15.6 Relevance
15.7 Notes and Details

16

Sparse Modeling and the Lasso

xi

169
174
177

181
181
185
190
192
195
198
204

208
208
213
218
227
230

233
234
237
243
251
256
261

264

269

271
272
275
278
282
286
290
294

298

xii

Contents

16.1 Forward Stepwise Regression
16.2 The Lasso
16.3 Fitting Lasso Models
16.4 Least-Angle Regression
16.5 Fitting Generalized Lasso Models
16.6 Post-Selection Inference for the Lasso
16.7 Connections and Extensions
16.8 Notes and Details

Random Forests and Boosting

17
17.1 Random Forests
17.2 Boosting with Squared-Error Loss
17.3 Gradient Boosting
17.4 Adaboost: the Original Boosting Algorithm
17.5 Connections and Extensions
17.6 Notes and Details

Neural Networks and Deep Learning

18
18.1 Neural Networks and the Handwritten Digit Problem
18.2 Fitting a Neural Network
18.3 Autoencoders
18.4 Deep Learning
18.5 Learning a Deep Network
18.6 Notes and Details

Support-Vector Machines and Kernel Methods

19
19.1 Optimal Separating Hyperplane
19.2 Soft-Margin Classiﬁer
19.3 SVM Criterion as Loss Plus Penalty
19.4 Computations and the Kernel Trick
19.5 Function Fitting Using Kernels
19.6 Example: String Kernels for Protein Classiﬁcation
19.7 SVMs: Concluding Remarks
19.8 Kernel Smoothing and Local Regression
19.9 Notes and Details

Inference After Model Selection
20
20.1 Simultaneous Conﬁdence Intervals
20.2 Accuracy After Model Selection
20.3 Selection Bias
20.4 Combined Bayes–Frequentist Estimation
20.5 Notes and Details

299
303
308
309
313
317
319
321

324
325
333
338
341
345
347

351
353
356
362
364
368
371

375
376
378
379
381
384
385
387
387
390

394
395
402
408
412
417

Contents

Empirical Bayes Estimation Strategies

21
21.1 Bayes Deconvolution
21.2 g-Modeling and Estimation
21.3 Likelihood, Regularization, and Accuracy
21.4 Two Examples
21.5 Generalized Linear Mixed Models
21.6 Deconvolution and f -Modeling
21.7 Notes and Details

Epilogue
References
Author Index
Subject Index

xiii

421
421
424
427
432
437
440
444

446
453
463
467

xiv

Preface

Statistical inference is an unusually wide-ranging discipline, located as it is
at the triple-point of mathematics, empirical science, and philosophy. The
discipline can be said to date from 1763, with the publication of Bayes’
rule (representing the philosophical side of the subject; the rule’s early ad-
vocates considered it an argument for the existence of God). The most re-
cent quarter of this 250-year history—from the 1950s to the present—is
the “computer age” of our book’s title, the time when computation, the tra-
ditional bottleneck of statistical applications, became faster and easier by a
factor of a million.

The book is an examination of how statistics has evolved over the past
sixty years—an aerial view of a vast subject, but seen from the height of a
small plane, not a jetliner or satellite. The individual chapters take up a se-
ries of inﬂuential topics—generalized linear models, survival analysis, the
jackknife and bootstrap, false-discovery rates, empirical Bayes, MCMC,
neural nets, and a dozen more—describing for each the key methodologi-
cal developments and their inferential justiﬁcation.

Needless to say, the role of electronic computation is central to our
story. This doesn’t mean that every advance was computer-related. A land
bridge had opened to a new continent but not all were eager to cross.
Topics such as empirical Bayes and James–Stein estimation could have
emerged just as well under the constraints of mechanical computation. Oth-
ers, like the bootstrap and proportional hazards, were pureborn children of
the computer age. Almost all topics in twenty-ﬁrst-century statistics are
now computer-dependent, but it will take our small plane a while to reach
the new millennium.

Dictionary deﬁnitions of statistical inference tend to equate it with the
entire discipline. This has become less satisfactory in the “big data” era of
immense computer-based processing algorithms. Here we will attempt, not
always consistently, to separate the two aspects of the statistical enterprise:
algorithmic developments aimed at speciﬁc problem areas, for instance

xv

xvi

Preface

random forests for prediction, as distinct from the inferential arguments
offered in their support.

Very broadly speaking, algorithms are what statisticians do while infer-
ence says why they do them. A particularly energetic brand of the statisti-
cal enterprise has ﬂourished in the new century, data science, emphasizing
algorithmic thinking rather than its inferential justiﬁcation. The later chap-
ters of our book, where large-scale prediction algorithms such as boosting
and deep learning are examined, illustrate the data-science point of view.
(See the epilogue for a little more on the sometimes fraught statistics/data
science marriage.)

There are no such subjects as Biological Inference or Astronomical In-
ference or Geological Inference. Why do we need “Statistical Inference”?
The answer is simple: the natural sciences have nature to judge the ac-
curacy of their ideas. Statistics operates one step back from Nature, most
often interpreting the observations of natural scientists. Without Nature to
serve as a disinterested referee, we need a system of mathematical logic for
guidance and correction. Statistical inference is that system, distilled from
two and a half centuries of data-analytic experience.

The book proceeds historically, in three parts. The great themes of clas-
sical inference, Bayesian, frequentist, and Fisherian, reviewed in Part I,
were set in place before the age of electronic computation. Modern practice
has vastly extended their reach without changing the basic outlines. (An
analogy with classical and modern literature might be made.) Part II con-
cerns early computer-age developments, from the 1950s through the 1990s.
As a transitional period, this is the time when it is easiest to see the ef-
fects, or noneffects, of fast computation on the progress of statistical meth-
odology, both in its theory and practice. Part III, “Twenty-First-Century
topics,” brings the story up to the present. Ours is a time of enormously
ambitious algorithms (“machine learning” being the somewhat disquieting
catchphrase). Their justiﬁcation is the ongoing task of modern statistical
inference.

Neither a catalog nor an encyclopedia, the book’s topics were chosen as
apt illustrations of the interplay between computational methodology and
inferential theory. Some missing topics that might have served just as well
include time series, general estimating equations, causal inference, graph-
ical models, and experimental design. In any case, there is no implication
that the topics presented here are the only ones worthy of discussion.

Also underrepresented are asymptotics and decision theory, the “math
stat” side of the ﬁeld. Our intention was to maintain a technical level of
discussion appropriate to Masters’-level statisticians or ﬁrst-year PhD stu-

Preface

xvii

dents. Inevitably, some of the presentation drifts into more difﬁcult waters,
more from the nature of the statistical ideas than the mathematics. Readers
who ﬁnd our aerial view circling too long over some topic shouldn’t hesi-
tate to move ahead in the book. For the most part, the chapters can be read
independently of each other (though there is a connecting overall theme).
This comment applies especially to nonstatisticians who have picked up
the book because of interest in some particular topic, say survival analysis
or boosting.

Useful disciplines that serve a wide variety of demanding clients run
the risk of losing their center. Statistics has managed, for the most part,
to maintain its philosophical cohesion despite a rising curve of outside de-
mand. The center of the ﬁeld has in fact moved in the past sixty years, from
its traditional home in mathematics and logic toward a more computational
focus. Our book traces that movement on a topic-by-topic basis. An answer
to the intriguing question “What happens next?” won’t be attempted here,
except for a few words in the epilogue, where the rise of data science is
discussed.

Acknowledgments

We are indebted to Cindy Kirby for her skillful work in the preparation of
this book, and Galit Shmueli for her helpful comments on an earlier draft.
At Cambridge University Press, a huge thank you to Steven Holt for his ex-
cellent copy editing, Clare Dennison for guiding us through the production
phase, and to Diana Gillooly, our editor, for her unfailing support.

Bradley Efron
Trevor Hastie
Department of Statistics
Stanford University
May 2016

xviii

Notation

xix

Throughout the book the numbered  sign indicates a technical note or
reference element which is elaborated on at the end of the chapter. There,
next to the number, the page number of the referenced location is given in
parenthesis. For example, lowess in the notes on page 11 was referenced
via a 1 on page 6. Matrices such as † are represented in bold font, as
are certain vectors such as y, a data vector with n elements. Most other
vectors, such as coefﬁcient vectors, are typically not bold. We use a dark
green typewriter font to indicate data set names such as prostate,
variable names such as prog from data sets, and R commands such as
glmnet or locfdr. No bibliographic references are given in the body of
the text; important references are given in the endnotes of each chapter.

Part I

Classic Statistical Inference

1

Algorithms and Inference

Statistics is the science of learning from experience, particularly experi-
ence that arrives a little bit at a time: the successes and failures of a new
experimental drug, the uncertain measurements of an asteroid’s path to-
ward Earth. It may seem surprising that any one theory can cover such an
amorphous target as “learning from experience.” In fact, there are two main
statistical theories, Bayesianism and frequentism, whose connections and
disagreements animate many of the succeeding chapters.

First, however, we want to discuss a less philosophical, more operational
division of labor that applies to both theories: between the algorithmic and
inferential aspects of statistical analysis. The distinction begins with the
most basic, and most popular, statistical method, averaging. Suppose we
have observed numbers x1; x2; : : : ; xn applying to some phenomenon of
interest, perhaps the automobile accident rates in the n D 50 states. The
mean

xi =n

(1.1)

Nx D nX

iD1

summarizes the results in a single number.

How accurate is that number? The textbook answer is given in terms of

the standard error,bse D

.xi (cid:0) Nx/2ı .n.n (cid:0) 1//

#1=2

" nX

iD1

:

(1.2)

Here averaging (1.1) is the algorithm, while the standard error provides an
inference of the algorithm’s accuracy. It is a surprising, and crucial, aspect
of statistical theory that the same data that supplies an estimate can also
assess its accuracy.1

1 “Inference” concerns more than accuracy: speaking broadly, algorithms say what the

statistician does while inference says why he or she does it.

3

4

Of course,bse (1.2) is itself an algorithm, which could be (and is) subject

Algorithms and Inference

to further inferential analysis concerning its accuracy. The point is that
the algorithm comes ﬁrst and the inference follows at a second level of
statistical consideration. In practice this means that algorithmic invention
is a more free-wheeling and adventurous enterprise, with inference playing
catch-up as it strives to assess the accuracy, good or bad, of some hot new
algorithmic methodology.

If the inference/algorithm race is a tortoise-and-hare affair, then modern
electronic computation has bred a bionic hare. There are two effects at work
here: computer-based technology allows scientists to collect enormous data
sets, orders of magnitude larger than those that classic statistical theory
was designed to deal with; huge data demands new methodology, and the
demand is being met by a burst of innovative computer-based statistical
algorithms. When one reads of “big data” in the news, it is usually these
algorithms playing the starring roles.

Our book’s title, Computer Age Statistical Inference, emphasizes the tor-
toise’s side of the story. The past few decades have been a golden age of
statistical methodology. It hasn’t been, quite, a golden age for statistical
inference, but it has not been a dark age either. The efﬂorescence of am-
bitious new algorithms has forced an evolution (though not a revolution)
in inference, the theories by which statisticians choose among competing
methods. The book traces the interplay between methodology and infer-
ence as it has developed since the 1950s, the beginning of our discipline’s
computer age. As a preview, we end this chapter with two examples illus-
trating the transition from classic to computer-age practice.

1.1 A Regression Example

Figure 1.1 concerns a study of kidney function. Data points .xi ; yi / have
been observed for n D 157 healthy volunteers, with xi the ith volunteer’s
age in years, and yi a composite measure “tot” of overall function. Kid-
ney function generally declines with age, as evident in the downward scat-
ter of the points. The rate of decline is an important question in kidney
transplantation: in the past, potential donors past age 60 were prohibited,
though, given a shortage of donors, this is no longer enforced.

The solid line in Figure 1.1 is a linear regression

y D O

ˇ0 C O

ˇ1x

(1.3)

ﬁt to the data by least squares, that is by minimizing the sum of squared

1.1 A Regression Example

5

Figure 1.1 Kidney ﬁtness tot vs age for 157 volunteers. The
line is a linear regression ﬁt, showing ˙2 standard errors at
selected values of age.

deviations

nX
.yi (cid:0) ˇ0 (cid:0) ˇ1xi /2

iD1

(1.4)

over all choices of .ˇ0; ˇ1/. The least squares algorithm, which dates back
to Gauss and Legendre in the early 1800s, gives O
ˇ1 D
(cid:0)0:079 as the least squares estimates. We can read off of the ﬁtted line
an estimated value of kidney ﬁtness for any chosen age. The top line of
Table 1.1 shows estimate 1.29 at age 20, down to (cid:0)3:43 at age 80.

ˇ0 D 2:86 and O

How accurate are these estimates? This is where inference comes in:
an extended version of formula (1.2), also going back to the 1800s, pro-
vides the standard errors, shown in line 2 of the table. The vertical bars in
Figure 1.1 are ˙ two standard errors, giving them about 95% chance of
containing the true expected value of tot at each age.
That 95% coverage depends on the validity of the linear regression model
(1.3). We might instead try a quadratic regression y D O
ˇ2x2,
or a cubic, etc., all of this being well within the reach of pre-computer
statistical theory.

ˇ1x C O

ˇ0 C O

*************************************************************************************************************************************************************2030405060708090−6−4−2024agetot6

Algorithms and Inference

Table 1.1 Regression analysis of the kidney data; (1) linear regression
estimates; (2) their standard errors; (3) lowess estimates; (4) their
bootstrap standard errors.

age

1. linear regression
2. std error

3. lowess
4. bootstrap std error

20

1.29
.21

1.66
.71

30
80
.50 (cid:0).28 (cid:0)1.07 (cid:0)1.86 (cid:0)2.64 (cid:0)3.43
.15
.42
.65 (cid:0).59 (cid:0)1.27 (cid:0)1.91 (cid:0)2.68 (cid:0)3.50
.70
.23

.15

.31

60

.26

70

.34

.37

.47

40

50

.19

.32

Figure 1.2 Local polynomial lowess(x,y,1/3) ﬁt to the
kidney-ﬁtness data, with ˙2 bootstrap standard deviations.

1

A modern computer-based algorithm lowess produced the somewhat
bumpy regression curve in Figure 1.2. The lowess  2 algorithm moves
its attention along the x-axis, ﬁtting local polynomial curves of differing
degrees to nearby .x; y/ points. (The 1/3 in the call3 lowess(x,y,1/3)

2 Here and throughout the book, the numbered  sign indicates a technical note or

reference element which is elaborated on at the end of the chapter.

3 Here and in all our examples we are employing the language R, itself one of the key

developments in computer-based statistical methodology.

*************************************************************************************************************************************************************2030405060708090−6−4−2024agetot1.1 A Regression Example

7

determines the deﬁnition of local.) Repeated passes over the x-axis reﬁne
the ﬁt, reducing the effects of occasional anomalous points. The ﬁtted curve
in Figure 1.2 is nearly linear at the right, but more complicated at the left
where points are more densely packed. It is ﬂat between ages 25 and 35,
a potentially important difference from the uniform decline portrayed in
Figure 1.1.

There is no formula such as (1.2) to infer the accuracy of the lowess
curve. Instead, a computer-intensive inferential engine, the bootstrap, was
used to calculate the error bars in Figure 1.2. A bootstrap data set is pro-
duced by resampling 157 pairs .xi ; yi / from the original 157 with replace-
ment, so perhaps .x1; y1/ might show up twice in the bootstrap sample,
.x2; y2/ might be missing, .x3; y3/ present once, etc. Applying lowess
to the bootstrap sample generates a bootstrap replication of the original
calculation.

Figure 1.3 25 bootstrap replications of lowess(x,y,1/3).

Figure 1.3 shows the ﬁrst 25 (of 250) bootstrap lowess replications
bouncing around the original curve from Figure 1.2. The variability of the
replications at any one age, the bootstrap standard deviation, determined
the original curve’s accuracy. How and why the bootstrap works is dis-
cussed in Chapter 10. It has the great virtue of assessing estimation accu-

2030405060708090−4−2024agetot8

Algorithms and Inference

racy for any algorithm, no matter how complicated. The price is a hundred-
or thousand-fold increase in computation, unthinkable in 1930, but routine
now.

The bottom two lines of Table 1.1 show the lowess estimates and
their standard errors. We have paid a price for the increased ﬂexibility of
lowess, its standard errors roughly doubling those for linear regression.

1.2 Hypothesis Testing

Our second example concerns the march of methodology and inference
for hypothesis testing rather than estimation: 72 leukemia patients, 47 with
ALL (acute lymphoblastic leukemia) and 25 with AML (acute myeloid leuk-
emia, a worse prognosis) have each had genetic activity measured for a
panel of 7,128 genes. The histograms in Figure 1.4 compare the genetic
activities in the two groups for gene 136.

Figure 1.4 Scores for gene 136, leukemia data. Top ALL
(n D 47), bottom AML (n D 25). A two-sample t-statistic D 3:01
with p-value D :0036.

The AML group appears to show greater activity, the mean values being

ALL D 0:752 and AML D 0:950:

(1.5)

 ALL scores − mean .752  0.20.40.60.81.01.21.41.60246810AML scores − mean .950 0.20.40.60.81.01.21.41.602468101.2 Hypothesis Testing

9

Is the perceived difference genuine, or perhaps, as people like to say, “a
statistical ﬂuke”? The classic answer to this question is via a two-sample
t-statistic,

t D AML (cid:0) ALLbsd

(1.6)

;

wherebsd is an estimate of the numerator’s standard deviation.4
Dividing by bsd allows us (under Gaussian assumptions discussed in
Chapter 5) to compare the observed value of t with a standard “null” dis-
tribution, in this case a Student’s t distribution with 70 degrees of freedom.
We obtain t D 3:01 from (1.6), which would classically be considered very
strong evidence that the apparent difference (1.5) is genuine; in standard
terminology, “with two-sided signiﬁcance level 0.0036.”

A small signiﬁcance level (or “p-value”) is a statement of statistical sur-
prise: something very unusual has happened if in fact there is no difference
in gene 136 expression levels between ALL and AML patients. We are less
surprised by t D 3:01 if gene 136 is just one candidate out of thousands
that might have produced “interesting” results.
That is the case here. Figure 1.5 shows the histogram of the two-sample
t-statistics for the panel of 7128 genes. Now t D 3:01 looks less unusual;
400 other genes have t exceeding 3.01, about 5.6% of them.

This doesn’t mean that gene 136 is “signiﬁcant at the 0.056 level.” There

are two powerful complicating factors:

1 Large numbers of candidates, 7128 here, will produce some large t-
values even if there is really no difference in genetic expression between
ALL and AML patients.

2 The histogram implies that in this study there is something wrong with
the theoretical null distribution (“Student’s t with 70 degrees of free-
dom”), the smooth curve in Figure 1.5. It is much too narrow at the cen-
ter, where presumably most of the genes are reporting non-signiﬁcant
results.

We will see in Chapter 15 that a low false-discovery rate, i.e., a low
chance of crying wolf over an innocuous gene, requires t exceeding 6.16
in the ALL/AML study. Only 47 of the 7128 genes make the cut. False-
discovery-rate theory is an impressive advance in statistical inference, in-
corporating Bayesian, frequentist, and empirical Bayesian (Chapter 6) el-

4 Formally, a standard error is the standard deviation of a summary statistic, andbsd might
better be calledbse, but we will follow the distinction less than punctiliously here.

10

Algorithms and Inference

Figure 1.5 Two-sample t-statistics for 7128 genes, leukemia
data. The smooth curve is the theoretical null density for the
t-statistic.

ements. It was a necessary advance in a scientiﬁc world where computer-
based technology routinely presents thousands of comparisons to be eval-
uated at once.

There is one more thing to say about the algorithm/inference statistical
cycle. Important new algorithms often arise outside the world of profes-
sional statisticians: neural nets, support vector machines, and boosting are
three famous examples. None of this is surprising. New sources of data,
satellite imagery for example, or medical microarrays, inspire novel meth-
odology from the observing scientists. The early literature tends toward the
enthusiastic, with claims of enormous applicability and power.

In the second phase, statisticians try to locate the new metholodogy
within the framework of statistical theory. In other words, they carry out
the statistical inference part of the cycle, placing the new methodology
within the known Bayesian and frequentist limits of performance. (Boost-
ing offers a nice example, Chapter 17.) This is a healthy chain of events,
good both for the hybrid vigor of the statistics profession and for the further
progress of algorithmic technology.

 t statisticsFrequency−10−5051001002003004005006007003.011.3 Notes

1.3 Notes

11

Legendre published the least squares algorithm in 1805, causing Gauss
to state that he had been using the method in astronomical orbit-ﬁtting
since 1795. Given Gauss’ astonishing production of major mathematical
advances, this says something about the importance attached to the least
squares idea. Chapter 8 includes its usual algebraic formulation, as well as
Gauss’ formula for the standard errors, line 2 of Table 1.1.

Our division between algorithms and inference brings to mind Tukey’s
exploratory/conﬁrmatory system. However the current algorithmic world
is often bolder in its claims than the word “exploratory” implies, while to
our minds “inference” conveys something richer than mere conﬁrmation.
1 [p. 6] lowess was devised by William Cleveland (Cleveland, 1981) and
is available in the R statistical computing language. It is applied to the
kidney data in Efron (2004). The kidney data originated in the nephrology
laboratory of Dr. Brian Myers, Stanford University, and is available from
this book’s web site.

2

Frequentist Inference

Before the computer age there was the calculator age, and before “big data”
there were small data sets, often a few hundred numbers or fewer, labori-
ously collected by individual scientists working under restrictive experi-
mental constraints. Precious data calls for maximally efﬁcient statistical
analysis. A remarkably effective theory, feasible for execution on mechan-
ical desk calculators, was developed beginning in 1900 by Pearson, Fisher,
Neyman, Hotelling, and others, and grew to dominate twentieth-century
statistical practice. The theory, now referred to as classical, relied almost
entirely on frequentist inferential ideas. This chapter sketches a quick and
simpliﬁed picture of frequentist inference, particularly as employed in clas-
sical applications.

We begin with another example from Dr. Myers’ nephrology laboratory:
211 kidney patients have had their glomerular ﬁltration rates measured,
with the results shown in Figure 2.1; gfr is an important indicator of kid-
ney function, with low values suggesting trouble. (It is a key component of
andbse D 0:95, typically reported as
tot in Figure 1.1.) The mean and standard error (1.1)–(1.2) are Nx D 54:25
(2.1)
˙0:95 denotes a frequentist inference for the accuracy of the estimate Nx D
54:25, and suggests that we shouldn’t take the “.25” very seriously, even
the “4” being open to doubt. Where the inference comes from and what
exactly it means remains to be said.

54:25 ˙ 0:95I

Statistical inference usually begins with the assumption that some prob-
ability model has produced the observed data x, in our case the vector of
n D 211 gfr measurements x D .x1; x2; : : : ; xn/. Let X D .X1; X2; : : : ;
Xn/ indicate n independent draws from a probability distribution F , writ-
ten

F ! X ;

12

(2.2)

Frequentist Inference

13

Figure 2.1 Glomerular ﬁltration rates for 211 kidney patients;
mean 54.25, standard error .95.

F being the underlying distribution of possible gfr scores here. A realiza-
tion X D x of (2.2) has been observed, and the statistician wishes to infer
some property of the unknown distribution F .

Suppose the desired property is the expectation of a single random draw

X from F , denoted

(which also equals the expectation of the average NX DP Xi =n of random
 D EF fXg
vector (2.2)1). The obvious estimate of  is O
 D Nx, the sample average. If
n were enormous, say 1010, we would expect O
 to nearly equal , but oth-
erwise there is room for error. How much error is the inferential question.
The estimate O
 is calculated from x according to some known algorithm,
say
t .x/ in our example being the averaging function Nx D P xi =n; O

 is a
1 The fact that EFf NXg equals EFfXg is a crucial, though easily proved, probabilistic

O
 D t .x/;

(2.3)

(2.4)

result.

 gfrFrequency2040608010005101520253014

realization of

Frequentist Inference

(cid:22) D EF f O‚g:

(2.6)

O‚ D t .X /;

(2.5)
the output of t .(cid:1)/ applied to a theoretical sample X from F (2.2). We have
chosen t .X /, we hope, to make O‚ a good estimator of , the desired prop-
erty of F .
We can now give a ﬁrst deﬁnition of frequentist inference: the accu-
racy of an observed estimate O
 D t .x/ is the probabilistic accuracy of
O‚ D t .X / as an estimator of . This may seem more a tautology than a
deﬁnition, but it contains a powerful idea: O
 is just a single number but O‚
takes on a range of values whose spread can deﬁne measures of accuracy.
Bias and variance are familiar examples of frequentist inference. Deﬁne
(cid:22) to be the expectation of O‚ D t .X / under model (2.2),

n
. O‚ (cid:0) (cid:22)/2o
Then the bias and variance attributed to estimate O

bias D (cid:22) (cid:0) 

var D EF

 of parameter  are

and

(2.7)
Again, what keeps this from tautology is the attribution to the single num-
ber O
 of the probabilistic properties of O‚ following from model (2.2). If
all of this seems too obvious to worry about, the Bayesian criticisms of
Chapter 3 may come as a shock.

:

Frequentism is often deﬁned with respect to “an inﬁnite sequence of
future trials.” We imagine hypothetical data sets X .1/; X .2/; X .3/; : : : gen-
erated by the same mechanism as x providing corresponding values O‚.1/;
O‚.2/, O‚.3/; : : : as in (2.5). The frequentist principle is then to attribute for
O
 the accuracy properties of the ensemble of O‚ values.2 If the O‚s have
empirical variance of, say, 0.04, then O
0:2 D p
 is claimed to have standard error
0:04, etc. This amounts to a more picturesque restatement of the

previous deﬁnition.

2.1 Frequentism in Practice

Our working deﬁnition of frequentism is that the probabilistic properties
of a procedure of interest are derived and then applied verbatim to the
procedure’s output for the observed data. This has an obvious defect: it
requires calculating the properties of estimators O‚ D t .X / obtained from

2 In essence, frequentists ask themselves “What would I see if I reran the same situation

again (and again and again. . . )?”

2.1 Frequentism in Practice

15

Statistics O

(2.9)

(2.10)

the true distribution F , even though F is unknown. Practical frequentism
uses a collection of more or less ingenious devices to circumvent the defect.
1. The plug-in principle. A simple formula relates the standard error of

NX DP Xi =n to varF .X /, the variance of a single X drawn from F ,

se(cid:0) NX(cid:1) D ŒvarF .X /=n1=2:
cvarF DX

.xi (cid:0) Nx/2 =.n (cid:0) 1/:

(2.8)
But having observed x D .x1; x2; : : : ; xn/ we can estimate varF .X / with-
out bias by
Plugging formula (2.9) into (2.8) givesbse (1.2), the usual estimate for the
standard error of an average Nx. In other words, the frequentist accuracy
estimate for Nx is itself estimated from the observed data.3
 D t .x/ more complicated
2. Taylor-series approximations.
than Nx can often be related back to the plug-in formula by local linear
approximations, sometimes known as the “delta method.”  For example, 1
O
 D Nx2 has d
with bse as in (1.2). Large sample calculations, as sample size n goes to

O
 =d Nx D 2Nx. Thinking of 2Nx as a constant gives

se(cid:0)Nx2(cid:1) :D 2jNxjbse;

inﬁnity, validate the delta method which, fortunately, often performs well
in small samples.
3. Parametric families and maximum likelihood theory.
Theoretical ex-
pressions for the standard error of a maximum likelihood estimate (MLE)
are discussed in Chapters 4 and 5, in the context of parametric families
of distributions. These combine Fisherian theory, Taylor-series approxima-
tions, and the plug-in principle in an easy-to-apply package.
4. Simulation and the bootstrap. Modern computation has opened up the
possibility of numerically implementing the “inﬁnite sequence of future
trials” deﬁnition, except for the inﬁnite part. An estimate OF of F , perhaps
the MLE, is found, and values O‚.k/ D t .X .k// simulated from OF for k D
1; 2; : : : ; B, say B D 1000. The empirical standard deviation of the O‚s is
then the frequentist estimate of standard error for O
 D t .x/, and similarly
with other measures of accuracy.

This is a good description of the bootstrap, Chapter 10. (Notice that

3 The most familiar example is the observed proportion p of heads in n ﬂips of a coin
having true probability (cid:25): the actual standard error is Œ(cid:25).1 (cid:0) (cid:25)/=n1=2 but we can
only report the plug-in estimate Œp.1 (cid:0) p/=n1=2.

16

Frequentist Inference

Table 2.1 Three estimates of location for the gfr data, and their
estimated standard errors; last two standard errors using the bootstrap,
B D 1000.

Estimate

Standard error

mean
25% Winsorized mean
median

54.25
52.61
52.24

.95
.78
.87

OF for F , comes ﬁrst rather than at the end of
here the plugging-in, of
the process.) The classical methods 1–3 above are restricted to estimates
O
 D t .x/ that are smoothly deﬁned functions of various sample means.
Simulation calculations remove this restriction. Table 2.1 shows three “lo-
cation” estimates for the gfr data, the mean, the 25% Winsorized mean,4
and the median, along with their standard errors, the last two computed
by the bootstrap. A happy feature of computer-age statistical inference is
the tremendous expansion of useful and usable statistics t .x/ in the statis-
tician’s working toolbox, the lowess algorithm in Figures 1.2 and 1.3
providing a nice example.
5. Pivotal statistics. A pivotal statistic O
 D t .x/ is one whose distri-
bution does not depend upon the underlying probability distribution F . In
such a case the theoretical distribution of O‚ D t .X / applies exactly to O
,
removing the need for devices 1–4 above. The classic example concerns
Student’s two-sample t-test.

In a two-sample problem the statistician observes two sets of numbers,

x1 D .x11; x12; : : : ; x1n1 / x2 D .x21; x22; : : : ; x2n2 /;

(2.11)

and wishes to test the null hypothesis that they come from the same dis-
tribution (as opposed to, say, the second set tending toward larger values
than the ﬁrst). It is assumed that the distribution F1 for x1 is normal, or
Gaussian,

ind(cid:24)

X1i

N .(cid:22)1; (cid:27) 2/;

i D 1; 2; : : : ; n1;

(2.12)

the notation indicating n1 independent draws from a normal distribution5

4 All observations below the 25th percentile of the 211 observations are moved up to that
point, similarly those above the 75th percentile are moved down, and ﬁnally the mean is
taken.

(cid:0)1=2 expf(cid:0)0:5 (cid:1) .x (cid:0) (cid:22)1/2=(cid:27) 2g.

5 Each draw having probability density .2(cid:25)(cid:27) 2/

2.1 Frequentism in Practice

with expectation (cid:22)1 and variance (cid:27) 2. Likewise

ind(cid:24)

N .(cid:22)2; (cid:27) 2/
We wish to test the null hypothesis

X2i

i D 1; 2; : : : ; n2:

17

(2.13)

(2.14)
The obvious test statistic O
 D Nx2 (cid:0) Nx1, the difference of the means, has
distribution
O
 (cid:24)

C 1

(2.15)

H0 W (cid:22)1 D (cid:22)2:
(cid:16)
0; (cid:27) 2(cid:16) 1
" n1X
.x1i (cid:0) Nx1/2 C n2X

.x2i (cid:0) Nx2/2

N

n1


#,

n2

under H0. We could plug in the unbiased estimate of (cid:27) 2,

O(cid:27) 2 D

.n1 C n2 (cid:0) 2/;

(2.16)

1

1

but Student provided a more elegant solution: instead of O
1=2
using the two-sample t-statistic

wherebsd D O(cid:27)

(cid:16) 1

C 1

;

n1

n2

t D Nx2 (cid:0) Nx1bsd

, we test H0

:

(2.17)

Under H0, t is pivotal, having the same distribution (Student’s t distribu-
tion with n1 C n2 (cid:0) 2 degrees of freedom), no matter what the value of the
“nuisance parameter” (cid:27).
For n1 C n2 (cid:0) 2 D 70, as in the leukemia example (1.5)–(1.6), Student’s
distribution gives

PrH0

(2.18)
The hypothesis test that rejects H0 if jtj exceeds 1.99 has probability ex-
actly 0.05 of mistaken rejection. Similarly,

f(cid:0)1:99  t  1:99g D 0:95:
Nx2 (cid:0) Nx1 ˙ 1:99 (cid:1)bsd

(2.19)
is an exact 0.95 conﬁdence interval for the difference (cid:22)2 (cid:0) (cid:22)1, covering
the true value in 95% of repetitions of probability model (2.12)–(2.13).6

6 Occasionally, one sees frequentism deﬁned in careerist terms, e.g., “A statistician who
always rejects null hypotheses at the 95% level will over time make only 5% errors of
the ﬁrst kind.” This is not a comforting criterion for the statistician’s clients, who are
interested in their own situations, not everyone else’s. Here we are only assuming
hypothetical repetitions of the speciﬁc problem at hand.

18

Frequentist Inference

What might be called the strong deﬁnition of frequentism insists on exact
frequentist correctness under experimental repetitions. Pivotality, unfortu-
nately, is unavailable in most statistical situations. Our looser deﬁnition
of frequentism, supplemented by devices such as those above,7 presents a
more realistic picture of actual frequentist practice.

2.2 Frequentist Optimality

The popularity of frequentist methods reﬂects their relatively modest math-
ematical modeling assumptions: only a probability model F (more exactly
a family of probabilities, Chapter 3) and an algorithm of choice t .x/. This
ﬂexibility is also a defect in that the principle of frequentist correctness
doesn’t help with the choice of algorithm. Should we use the sample mean
to estimate the location of the gfr distribution? Maybe the 25% Win-
sorized mean would be better, as Table 2.1 suggests.

The years 1920–1935 saw the development of two key results on fre-
quentist optimality, that is, ﬁnding the best choice of t .x/ given model F .
The ﬁrst of these was Fisher’s theory of maximum likelihood estimation
and the Fisher information bound: in parametric probability models of the
type discussed in Chapter 4, the MLE is the optimum estimate in terms of
minimum (asymptotic) standard error.

In the same spirit, the Neyman–Pearson lemma provides an optimum
hypothesis-testing algorithm. This is perhaps the most elegant of frequen-
tist constructions. In its simplest formulation, the NP lemma assumes we
are trying to decide between two possible probability density functions for
the observed data x, a null hypothesis density f0.x/ and an alternative
density f1.x/. A testing rule t .x/ says which choice, 0 or 1, we will make
having observed data x. Any such rule has two associated frequentist error
probabilities: choosing f1 when actually f0 generated x, and vice versa,

˛ D Prf0
ˇ D Prf1
Let L.x/ be the likelihood ratio,

ft .x/ D 1g ;
ft .x/ D 0g :

L.x/ D f1.x/=f0.x/

(2.20)

(2.21)

7 The list of devices is not complete. Asymptotic calculations play a major role, as do

more elaborate combinations of pivotality and the plug-in principle; see the discussion
of approximate bootstrap conﬁdence intervals in Chapter 11.

and deﬁne the testing rule tc.x/ by

tc.x/ D

2.2 Frequentist Optimality

(
1 if log L.x/ (cid:21) c
0 if log L.x/ < c:

19

(2.22)

There is one such rule for each choice of the cutoff c. The Neyman–Pearson
lemma says that only rules of form (2.22) can be optimum; for any other
rule t .x/ there will be a rule tc.x/ having smaller errors of both kinds,8

˛c < ˛ and ˇc < ˇ:

(2.23)

Figure 2.2 Neyman–Pearson alpha–beta curve for f0 (cid:24)
f1 (cid:24)
cutoffs c D :8; :6; :4; : : : ;(cid:0):4.

N .0; 1/,
N .:5; 1/, and sample size n D 10. Red dots correspond to

Figure 2.2 graphs .˛c; ˇc/ as a function of the cutoff c, for the case
where x D .x1; x2; : : : ; x10/ is obtained by independent sampling from a
normal distribution, N .0; 1/ for f0 versus N .0:5; 1/ for f1. The NP lemma
says that any rule not of form (2.22) must have its .˛; ˇ/ point lying above
the curve.

8 Here we are ignoring some minor deﬁnitional difﬁculties that can occur if f0 and f1 are

discrete.

0.00.20.40.60.81.00.00.20.40.60.81.0ablllllllc = .4a  = .10,  b = .3820

Frequentist Inference

Frequentist optimality theory, both for estimation and for testing, an-
chored statistical practice in the twentieth century. The larger data sets and
more complicated inferential questions of the current era have strained the
capabilities of that theory. Computer-age statistical inference, as we will
see, often displays an unsettling ad hoc character. Perhaps some contem-
porary Fishers and Neymans will provide us with a more capacious opti-
mality theory equal to the challenges of current practice, but for now that
is only a hope.

Frequentism cannot claim to be a seamless philosophy of statistical in-
ference. Paradoxes and contradictions abound within its borders, as will
be shown in the next chapter. That being said, frequentist methods have
a natural appeal to working scientists, an impressive history of success-
ful application, and, as our list of ﬁve “devices” suggests, the capacity to
encourage clever methodology. The story that follows is not one of aban-
donment of frequentist thinking, but rather a broadening of connections
with other methods.

2.3 Notes and Details

The name “frequentism” seems to have been suggested by Neyman as a
statistical analogue of Richard von Mises’ frequentist theory of probability,
the connection being made explicit in his 1977 paper, “Frequentist prob-
ability and frequentist statistics.” “Behaviorism” might have been a more
descriptive name9 since the theory revolves around the long-run behavior
of statistics t .x/, but in any case “frequentism” has stuck, replacing the
older (sometimes disparaging) term “objectivism.” Neyman’s attempt at a
complete frequentist theory of statistical inference, “inductive behavior,”
is not much quoted today, but can claim to be an important inﬂuence on
Wald’s development of decision theory.

R. A. Fisher’s work on maximum likelihood estimation is featured in
Chapter 4. Fisher, arguably the founder of frequentist optimality theory,
was not a pure frequentist himself, as discussed in Chapter 4 and Efron
(1998), “R. A. Fisher in the 21st Century.” (Now that we are well into the
twenty-ﬁrst century, the author’s talents as a prognosticator can be frequen-
tistically evaluated.)

1 [p. 15] Delta method. The delta method uses a ﬁrst-order Taylor series to
. Suppose O
O

 / (cid:25) s. / C

O
 / of a statistic O
approximate the variance of a function s.
has mean/variance .; (cid:27) 2/, and consider the approximation s.

9 That name is already spoken for in the psychology literature.

O
 (cid:0)  /. Hence varfs.
0
s
and use an estimate for (cid:27) 2.

. /.

2.3 Notes and Details
O
 /g (cid:25) js

. /j2(cid:27) 2. We typically plug-in O

0

21

 for ,

3

Bayesian Inference

The human mind is an inference machine: “It’s getting windy, the sky is
darkening, I’d better bring my umbrella with me.” Unfortunately, it’s not a
very dependable machine, especially when weighing complicated choices
against past experience. Bayes’ theorem is a surprisingly simple mathemat-
ical guide to accurate inference. The theorem (or “rule”), now 250 years
old, marked the beginning of statistical inference as a serious scientiﬁc sub-
ject. It has waxed and waned in inﬂuence over the centuries, now waxing
again in the service of computer-age applications.

Bayesian inference, if not directly opposed to frequentism, is at least or-
thogonal. It reveals some worrisome ﬂaws in the frequentist point of view,
while at the same time exposing itself to the criticism of dangerous overuse.
The struggle to combine the virtues of the two philosophies has become
more acute in an era of massively complicated data sets. Much of what
follows in succeeding chapters concerns this struggle. Here we will review
some basic Bayesian ideas and the ways they impinge on frequentism.

The fundamental unit of statistical inference both for frequentists and

for Bayesians is a family of probability densities

D˚f(cid:22).x/I x 2

X ; (cid:22) 2 (cid:127)(cid:9)I

F

(3.1)

x, the observed data, is a point1 in the sample space X , while the unob-
served parameter (cid:22) is a point in the parameter space (cid:127). The statistician
observes x from f(cid:22).x/, and infers the value of (cid:22).

Perhaps the most familiar case is the normal family

f(cid:22).x/ D 1p

2(cid:25)

(cid:0) 1
2 .x(cid:0)(cid:22)/2

e

(3.2)

1 Both x and (cid:22) may be scalars, vectors, or more complicated objects. Other names for the

generic “x” and “(cid:22)” occur in speciﬁc situations, for instance x for x in Chapter 2. We
will also call F a “family of probability distributions.”

22

Bayesian Inference

23

(more exactly, the one-dimensional normal translation family2 with vari-

ance 1), with both X and (cid:127) equaling R1, the entire real line .(cid:0)1;1/.

Another central example is the Poisson family
(cid:0)(cid:22)(cid:22)x=xŠ;

f(cid:22).x/ D e

where X is the nonnegative integers f0; 1; 2; : : :g and (cid:127) is the nonnegative
real line .0;1/. (Here the “density” (3.3) speciﬁes the atoms of probability
on the discrete points of X .)
probability family F, the knowledge of a prior density

Bayesian inference requires one crucial assumption in addition to the

g.(cid:22)/;

(cid:22) 2 (cid:127)I

g.(cid:22)/ represents prior information concerning the parameter (cid:22), available to
the statistician before the observation of x. For instance, in an application
of the normal model (3.2), it could be known that (cid:22) is positive, while past
experience shows it never exceeding 10, in which case we might take g.(cid:22)/
to be the uniform density g.(cid:22)/ D 1=10 on the interval Œ0; 10. Exactly
what constitutes “prior knowledge” is a crucial question we will consider
in ongoing discussions of Bayes’ theorem.
Bayes’ theorem is a rule for combining the prior knowledge in g.(cid:22)/ with
the current evidence in x. Let g.(cid:22)jx/ denote the posterior density of (cid:22), that
is, our update of the prior density g.(cid:22)/ after taking account of observation
x. Bayes’ rule provides a simple expression for g.(cid:22)jx/ in terms of g.(cid:22)/
and F.

Bayes’ Rule: g.(cid:22)jx/ D g.(cid:22)/f(cid:22).x/=f .x/;

(cid:22) 2 (cid:127);

(3.3)

(3.4)

(3.5)

(3.6)

where f .x/ is the marginal density of x,

f .x/ DZ

f(cid:22).x/g.(cid:22)/ d(cid:22):

(cid:127)

(The integral in (3.6) would be a sum if (cid:127) were discrete.) The Rule is a
straightforward exercise in conditional probability,3 and yet has far-reaching
and sometimes surprising consequences.

In Bayes’ formula (3.5), x is ﬁxed at its observed value while (cid:22) varies
over (cid:127), just the opposite of frequentist calculations. We can emphasize this
2 Standard notation is x (cid:24) N .(cid:22); (cid:27) 2/ for a normal distribution with expectation (cid:22) and
variance (cid:27) 2, so (3.2) has x (cid:24) N .(cid:22); 1/.
3 g.(cid:22)jx/ is the ratio of g.(cid:22)/f(cid:22).x/, the joint probability of the pair .(cid:22); x/, and f .x/,
the marginal probability of x.

24

Bayesian Inference

by rewriting (3.5) as

g.(cid:22)jx/ D cxLx.(cid:22)/g.(cid:22)/;

(3.7)

where Lx.(cid:22)/ is the likelihood function, that is, f(cid:22).x/ with x ﬁxed and (cid:22)
varying. Having computed Lx.(cid:22)/g.(cid:22)/, the constant cx can be determined
numerically from the requirement that g.(cid:22)jx/ integrate to 1, obviating the
calculation of f .x/ (3.6).

Note Multiplying the likelihood function by any ﬁxed constant c0 has no
effect on (3.7) since c0 can be absorbed into cx. So for the Poisson family
(3.3) we can take Lx.(cid:22)/ D e
(cid:0)(cid:22)(cid:22)x, ignoring the xŠ factor, which acts as a
constant in Bayes’ rule. The luxury of ignoring factors depending only on
x often simpliﬁes Bayesian calculations.

For any two points (cid:22)1 and (cid:22)2 in (cid:127), the ratio of posterior densities is, by

division in (3.5),

g.(cid:22)1jx/
g.(cid:22)2jx/

D g.(cid:22)1/
g.(cid:22)2/

f(cid:22)1 .x/
f(cid:22)2 .x/

(3.8)

(no longer involving the marginal density f .x/), that is, “the posterior odds
ratio is the prior odds ratio times the likelihood ratio,” a memorable restate-
ment of Bayes’ rule.

3.1 Two Examples

A simple but genuine example of Bayes’ rule in action is provided by the
story of the Physicist’s Twins: thanks to sonograms, a physicist found out
she was going to have twin boys. “What is the probability my twins will
be Identical, rather than Fraternal?” she asked. The doctor answered that
one-third of twin births were Identicals, and two-thirds Fraternals.

In this situation (cid:22), the unknown parameter (or “state of nature”) is either
Identical or Fraternal with prior probability 1/3 or 2/3; X, the possible
sonogram results for twin births, is either Same Sex or Different Sexes, and
x D Same Sex was observed. (We can ignore sex since that does not affect
the calculation.) A crucial fact is that identical twins are always same-sex
while fraternals have probability 0.5 of same or different, so Same Sex in
the sonogram is twice as likely if the twins are Identical. Applying Bayes’

3.1 Two Examples

rule in ratio form (3.8) answers the physicist’s question:

g.Identicalj Same/
g.Fraternalj Same/

D g.Identical/
g.Fraternal/

(cid:1) fIdentical.Same/
fFraternal.Same/
D 1:

D 1=3
2=3

(cid:1) 1
1=2

25

(3.9)

That is, the posterior odds are even, and the physicist’s twins have equal
probabilities 0.5 of being Identical or Fraternal.4 Here the doctor’s prior
odds ratio, 2 to 1 in favor of Fraternal, is balanced out by the sonogram’s
likelihood ratio of 2 to 1 in favor of Identical.

Figure 3.1 Analyzing the twins problem.

There are only four possible combinations of parameter (cid:22) and outcome
x in the twins problem, labeled a, b, c, and d in Figure 3.1. Cell b has
probability 0 since Identicals cannot be of Different Sexes. Cells c and d
have equal probabilities because of the random sexes of Fraternals. Finally,
a C b must have total probability 1/3, and c C d total probability 2/3,
according to the doctor’s prior distribution. Putting all this together, we
can ﬁll in the probabilities for all four cells, as shown. The physicist knows
she is in the ﬁrst column of the table, where the conditional probabilities
of Identical or Fraternal are equal, just as provided by Bayes’ rule in (3.9).
Presumably the doctor’s prior distribution came from some enormous
state or national database, say three million previous twin births, one mil-
lion Identical pairs and two million Fraternals. We deduce that cells a, c,
and d must have had one million entries each in the database, while cell
b was empty. Bayes’ rule can be thought of as a big book with one page

4 They turned out to be Fraternal.

1	  Identical Twins are: Fraternal Same sex Different Physicist Sonogram shows: Doctor 2/3 1/3 1/3 1/3 0 1/3 b a c d 26

Bayesian Inference

for each possible outcome x. (The book has only two pages in Figure 3.1.)
The physicist turns to the page “Same Sex” and sees two million previous
twin births, half Identical and half Fraternal, correctly concluding that the
odds are equal in her situation.

Given any prior distribution g.(cid:22)/ and any family of densities f(cid:22).x/,
Bayes’ rule will always provide a version of the big book. That doesn’t
mean that the book’s contents will always be equally convincing. The prior
for the twins problems was based on a large amount of relevant previous
experience. Such experience is most often unavailable. Modern Bayesian
practice uses various strategies to construct an appropriate “prior” g.(cid:22)/
in the absence of prior experience, leaving many statisticians unconvinced
by the resulting Bayesian inferences. Our second example illustrates the
difﬁculty.

Table 3.1 Scores from two tests taken by 22 students, mechanics and
vectors.

mechanics
vectors

mechanics
vectors

1

7
51

12

36
59

2

44
69

13

42
60

3

49
41

14

5
30

4

59
70

15

22
58

5

34
42

16

18
51

6

46
40

17

41
63

7

0
40

18

48
38

8

32
45

19

31
42

9

49
57

20

42
69

10

52
64

21

46
49

11

44
61

22

63
63

Table 3.1 shows the scores on two tests, mechanics and vectors,
achieved by n D 22 students. The sample correlation coefﬁcient between
the two scores is O
O

," 22X

 D 0:498,

#1=2

.mi (cid:0) Nm/.vi (cid:0) Nv/

; (3.10)

 D 22X

iD1

22X
.vi (cid:0) Nv/2

iD1

.mi (cid:0) Nm/2

iD1

with m and v short for mechanics and vectors, Nm and Nv their aver-
ages. We wish to assign a Bayesian measure of posterior accuracy to the
true correlation coefﬁcient , “true” meaning the correlation for the hypo-
thetical population of all students, of which we observed only 22.
If we assume that the joint .m; v/ distribution is bivariate normal (as
discussed in Chapter 5), then the density of O
 as a function of  has a
known form,

1

3.1 Two Examples

 2.n(cid:0)4/=2

1 (cid:0) O

Z 1

27

n(cid:0)1 :

(cid:16)
cosh w (cid:0) 

dw

 D .n (cid:0) 2/.1 (cid:0)  2/.n(cid:0)1/=2(cid:16)

(cid:16) O



(cid:25)

f

O

(3.11)
In terms of our general Bayes notation, parameter (cid:22) is , observation x is
O
, and family F is given by (3.11), with both (cid:127) and X equaling the interval
Œ(cid:0)1; 1. Formula (3.11) looks formidable to the human eye but not to the
computer eye, which makes quick work of it.

0

Figure 3.2 Student scores data; posterior density of correlation 
for three possible priors.

In this case, as in the majority of scientiﬁc situations, we don’t have a
trove of relevant past experience ready to provide a prior g. /. One expe-
dient, going back to Laplace, is the “principle of insufﬁcient reason,” that
is, we take  to be uniformly distributed over (cid:127),

g. / D 1

2

for (cid:0) 1    1;

(3.12)

a “ﬂat prior.” The solid black curve in Figure 3.2 shows the resulting poste-
rior density (3.5), which is just the likelihood f .0:498/ plotted as a func-
tion of  (and scaled to have integral 1).

−0.20.00.20.40.60.81.00.00.51.01.52.02.5qg(q|q^)MLE .498flat priorJeffreysTriangularl.093.75028

Bayesian Inference

Jeffreys’ prior,

gJeff. / D 1=.1 (cid:0)  2/;

(3.13)

yields posterior density g.j O
 / shown by the dashed red curve. It suggests
somewhat bigger values for the unknown parameter . Formula (3.13)
arises from a theory of “uninformative priors” discussed in the next sec-
tion, an improvement on the principle of insufﬁcient reason; (3.13) is an

improper density in thatR 1(cid:0)1 g. / d D 1, but it still provides proper pos-

terior densities when deployed in Bayes’ rule (3.5).
The dotted blue curve in Figure 3.2 is posterior density g.j O
from the triangular-shaped prior

 / obtained

g. / D 1 (cid:0) jj:

(3.14)

This is a primitive example of a shrinkage prior, one designed to favor
smaller values of . Its effect is seen in the leftward shift of the posterior
density. Shrinkage priors will play a major role in our discussion of large-
scale estimation and testing problems, where we are hoping to ﬁnd a few
large effects hidden among thousands of negligible ones.

3.2 Uninformative Prior Distributions

Given a convincing prior distribution, Bayes’ rule is easier to use and pro-
duces more satisfactory inferences than frequentist methods. The domi-
nance of frequentist practice reﬂects the scarcity of useful prior information
in day-to-day scientiﬁc applications. But the Bayesian impulse is strong,
and almost from its inception 250 years ago there have been proposals for
the construction of “priors” that permit the use of Bayes’ rule in the ab-
sence of relevant experience.

One approach, perhaps the most inﬂuential in current practice, is the
employment of uninformative priors. “Uninformative” has a positive con-
notation here, implying that the use of such a prior in Bayes’ rule does not
tacitly bias the resulting inference. Laplace’s principle of insufﬁcient rea-
son, i.e., assigning uniform prior distributions to unknown parameters, is
an obvious attempt at this goal. Its use went unchallenged for more than a
century, perhaps because of Laplace’s inﬂuence more than its own virtues.
Venn (of the Venn diagram) in the 1860s, and Fisher in the 1920s, attack-
ing the routine use of Bayes’ theorem, pointed out that Laplace’s principle
could not be applied consistently. In the student correlation example, for
instance, a uniform prior distribution for  would not be uniform if we

changed parameters to (cid:13) D e; posterior probabilities such as

3.2 Uninformative Prior Distributions

n
 > 0j O



o D Pr

n
(cid:13) > 1j O



o

Pr

29

(3.15)

would depend on whether  or (cid:13) was taken to be uniform a priori. Neither
choice then could be considered uninformative.

A more sophisticated version of Laplace’s principle was put forward by
Jeffreys beginning in the 1930s. It depends, interestingly enough, on the
frequentist notion of Fisher information (Chapter 4). For a one-parameter
family f(cid:22).x/, where the parameter space (cid:127) is an interval of the real line
R1, the Fisher information is deﬁned to be

2)

( @

@(cid:22)

I(cid:22) D E(cid:22)

log f(cid:22).x/

:

(3.16)

(For the Poisson family (3.3), @=@(cid:22).log f(cid:22).x// D x=(cid:22)(cid:0)1 and I(cid:22) D 1=(cid:22).)

The Jeffreys’ prior gJeff.(cid:22)/ is by deﬁnition
I 1=2
(cid:22) :

gJeff.(cid:22)/ D

Because 1=I(cid:22) equals, approximately, the variance (cid:27) 2
equivalent deﬁnition is

(3.17)
(cid:22) of the MLE O(cid:22), an

Formula (3.17) does in fact transform correctly under parameter changes,
avoiding the Venn–Fisher criticism.It is known that O
approximate standard deviation

 in family (3.11) has 2

gJeff.(cid:22)/ D 1=(cid:27)(cid:22):

(cid:27) D c.1 (cid:0)  2/;

(3.18)

(3.19)

yielding Jeffreys’ prior (3.13) from (3.18), the constant factor c having no
effect on Bayes’ rule (3.5)–(3.6).

The red triangles in Figure 3.2 indicate the “95% credible interval” [0.093,
Z 0:750
0.750] for , based on Jeffreys’ prior. That is, the posterior probability
0:093    0:750 equals 0.95,

gJeff(cid:16)

j O





d D 0:95;

(3.20)

0:093

with probability 0.025 for  < 0:093 or  > 0:750. It is not an accident that
this nearly equals the standard Neyman 95% conﬁdence interval based on
O
 / (3.11). Jeffreys’ prior tends to induce this nice connection between
f .
the Bayesian and frequentist worlds, at least in one-parameter families.

Multiparameter probability families, Chapter 4, make everything more

30

Bayesian Inference

difﬁcult. Suppose, for instance, the statistician observes 10 independent
versions of the normal model (3.2), with possibly different values of (cid:22),

ind(cid:24)

xi

N .(cid:22)i ; 1/

for i D 1; 2; : : : ; 10;

(3.21)

in standard notation. Jeffreys’ prior is ﬂat for any one of the 10 problems,
which is reasonable for dealing with them separately, but the joint Jeffreys’
prior

g.(cid:22)1; (cid:22)2; : : : ; (cid:22)10/ D constant;

(3.22)

also ﬂat, can produce disastrous overall results, as discussed in Chapter 13.
Computer-age applications are often more like (3.21) than (3.11), except
with hundreds or thousands of cases rather than 10 to consider simultane-
ously. Uninformative priors of many sorts, including Jeffreys’, are highly
popular in current applications, as we will discuss. This leads to an inter-
play between Bayesian and frequentist methodology, the latter intended to
control possible biases in the former, exemplifying our general theme of
computer-age statistical inference.

3.3 Flaws in Frequentist Inference

Bayesian statistics provides an internally consistent (“coherent”) program
of inference. The same cannot be said of frequentism. The apocryphal story
of the meter reader makes the point: an engineer measures the voltages on
a batch of 12 tubes, using a voltmeter that is normally calibrated,

x (cid:24)

N .(cid:22); 1/;

(3.23)

x being any one measurement and (cid:22) the true batch voltage. The measure-
ments range from 82 to 99, with an average of Nx D 92, which he reports
back as an unbiased estimate of (cid:22).
The next day he discovers a glitch in his voltmeter such that any volt-
age exceeding 100 would have been reported as x D 100. His frequentist
statistician tells him that Nx D 92 is no longer unbiased for the true expecta-
tion (cid:22) since (3.23) no longer completely describes the probability family.
(The statistician says that 92 is a little too small.) The fact that the glitch
didn’t affect any of the actual measurements doesn’t let him off the hook;
NX from the actual
Nx would not be unbiased for (cid:22) in future realizations of
probability model.
A Bayesian statistician comes to the meter reader’s rescue. For any prior
density g.(cid:22)/, the posterior density g.(cid:22)jx/ D g.(cid:22)/f(cid:22).x/=f .x/, where
x is the vector of 12 measurements, depends only on the data x actually

3

3.3 Flaws in Frequentist Inference

31

observed, and not on other potential data sets X that might have been
seen. The ﬂat Jeffreys’ prior g.(cid:22)/ D constant yields posterior expectation
Nx D 92 for (cid:22), irrespective of whether or not the glitch would have affected
readings above 100.

Figure 3.3 Z-values against null hypothesis (cid:22) D 0 for months 1
through 30.

A less contrived version of the same phenomenon is illustrated in Fig-
ure 3.3. An ongoing experiment is being run. Each month i an independent
normal variate is observed,

(3.24)
with the intention of testing the null hypothesis H0 W (cid:22) D 0 versus the
alternative (cid:22) > 0. The plotted points are test statistics

N .(cid:22); 1/;

(3.25)

xi (cid:24)

Zi D iX
(cid:16)p

jD1

xj

Zi (cid:24)

.p

i ;



a “z-value” based on all the data up to month i,

(3.26)
At month 30, the scheduled end of the experiment, Z30 D 1:66, just ex-
ceeding 1.645, the upper 95% point for a N .0; 1/ distribution. Victory!
The investigators get to claim “signiﬁcant” rejection of H0 at level 0.05.

i (cid:22); 1

N

:

llllllllllllllllllllllllllllll051015202530−1.5−1.0−0.50.00.51.01.52.0month iz value1.64532

Bayesian Inference

Unfortunately, it turns out that the investigators broke protocol and peek-
ed at the data at month 20, in the hope of being able to stop an expensive
experiment early. This proved a vain hope, Z20 D 0:79 not being anywhere
near signiﬁcance, so they continued on to month 30 as originally planned.
This means they effectively used the stopping rule “stop and declare signif-
icance if either Z20 or Z30 exceeds 1.645.” Some computation shows that
this rule had probability 0.074, not 0.05, of rejecting H0 if it were true.
Victory has turned into defeat according to the honored frequentist 0.05
criterion.
Once again, the Bayesian statistician is more lenient. The likelihood
function for the full data set x D .x1; x2; : : : ; x30/,
2 .xi(cid:0)(cid:22)/2
(cid:0) 1

Lx.(cid:22)/ D 30Y

(3.27)

e

iD1

;

is the same irrespective of whether or not the experiment might have stopped
early. The stopping rule doesn’t affect the posterior distribution g.(cid:22)jx/,
which depends on x only through the likelihood (3.7).

Figure 3.4 Unbiased effect-size estimates for 6033 genes,
prostate cancer study. The estimate for gene 610 is x610 D 5:29.
What is its effect size?

The lenient nature of Bayesian inference can look less benign in multi-

 effect−size estimatesFrequency−4−20240100200300400gene 6105.293.4 A Bayesian/Frequentist Comparison List

33

parameter settings. Figure 3.4 concerns a prostate cancer study comparing
52 patients with 50 healthy controls. Each man had his genetic activity
measured for a panel of N D 6033 genes. A statistic x was computed for
each gene,5 comparing the patients with controls, say
i D 1; 2; : : : ; N;

(3.28)

4

xi (cid:24)

N .(cid:22)i ; 1/

where (cid:22)i represents the true effect size for gene i. Most of the genes, prob-
ably not being involved in prostate cancer, would be expected to have effect
sizes near 0, but the investigators hoped to spot a few large (cid:22)i values, either
positive or negative.
The histogram of the 6033 xi values does in fact reveal some large val-
ues, x610 D 5:29 being the winner. Question: what estimate should we
give for (cid:22)610? Even though x610 was individually unbiased for (cid:22)610, a fre-
quentist would (correctly) worry that focusing attention on the largest of
6033 values would produce an upward bias, and that our estimate should
downwardly correct 5.29. “Selection bias,” “regression to the mean,” and
“the winner’s curse” are three names for this phenomenon.

Bayesian inference, surprisingly, is immune to selection bias.  Irrespec- 5

tive of whether gene 610 was prespeciﬁed for particular attention or only
came to attention as the “winner,” the Bayes’ estimate for (cid:22)610 given all
the data stays the same. This isn’t obvious, but follows from the fact that
any data-based selection process does not affect the likelihood function in
(3.7).

What does affect Bayesian inference is the prior g.(cid:22)/ for the full vector
(cid:22) of 6033 effect sizes. The ﬂat prior, g.(cid:22)/ constant, results in the danger-
ous overestimate O(cid:22)610 D x610 D 5:29. A more appropriate uninformative
prior appears as part of the empirical Bayes calculations of Chapter 15
(and gives O(cid:22)610 D 4:11). The operative point here is that there is a price to
be paid for the desirable properties of Bayesian inference. Attention shifts
from choosing a good frequentist procedure to choosing an appropriate
prior distribution. This can be a formidable task in high-dimensional prob-
lems, the very kinds featured in computer-age inference.

3.4 A Bayesian/Frequentist Comparison List

Bayesians and frequentists start out on the same playing ﬁeld, a family
of probability distributions f(cid:22).x/ (3.1), but play the game in orthogonal

5 The statistic was the two-sample t-statistic (2.17) transformed to normality (3.28); see

the endnotes.

34

Bayesian Inference

directions, as indicated schematically in Figure 3.5: Bayesian inference
proceeds vertically, with x ﬁxed, according to the posterior distribution
g.(cid:22)jx/, while frequentists reason horizontally, with (cid:22) ﬁxed and x varying.
Advantages and disadvantages accrue to both strategies, some of which are
compared next.

Figure 3.5 Bayesian inference proceeds vertically, given x;
frequentist inference proceeds horizontally, given (cid:22).

(cid:15) Bayesian inference requires a prior distribution g.(cid:22)/. When past experi-
ence provides g.(cid:22)/, as in the twins example, there is every good reason to
employ Bayes’ theorem. If not, techniques such as those of Jeffreys still
permit the use of Bayes’ rule, but the results lack the full logical force
of the theorem; the Bayesian’s right to ignore selection bias, for instance,
must then be treated with caution.
(cid:15) Frequentism replaces the choice of a prior with the choice of a method,
or algorithm, t .x/, designed to answer the speciﬁc question at hand. This
adds an arbitrary element to the inferential process, and can lead to meter-
reader kinds of contradictions. Optimal choice of t .x/ reduces arbitrary
behavior, but computer-age applications typically move outside the safe
waters of classical optimality theory, lending an ad-hoc character to fre-
quentist analyses.
(cid:15) Modern data-analysis problems are often approached via a favored meth-

3.4 A Bayesian/Frequentist Comparison List

35

odology, such as logistic regression or regression trees in the examples of
Chapter 8. This plays into the methodological orientation of frequentism,
which is more ﬂexible than Bayes’ rule in dealing with speciﬁc algorithms
(though one always hopes for a reasonable Bayesian justiﬁcation for the
method at hand).
(cid:15) Having chosen g.(cid:22)/, only a single probability distribution g.(cid:22)jx/ is in
play for Bayesians. Frequentists, by contrast, must struggle to balance
the behavior of t .x/ over a family of possible distributions, since (cid:22) in
Figure 3.5 is unknown. The growing popularity of Bayesian applications
(usually begun with uninformative priors) reﬂects their simplicity of ap-
plication and interpretation.
(cid:15) The simplicity argument cuts both ways. The Bayesian essentially bets it
all on the choice of his or her prior being correct, or at least not harmful.
Frequentism takes a more defensive posture, hoping to do well, or at least
not poorly, whatever (cid:22) might be.
(cid:15) A Bayesian analysis answers all possible questions at once, for example,
estimating Efgfrg or Prfgfr < 40g or anything else relating to Figure 2.1.
Frequentism focuses on the problem at hand, requiring different estima-
tors for different questions. This is more work, but allows for more intense
inspection of particular problems. In situation (2.9) for example, estima-
tors of the form

X
.xi (cid:0) Nx/2=.n (cid:0) c/

(3.29)

might be investigated for different choices of the constant c, hoping to
reduce expected mean-squared error.
(cid:15) The simplicity of the Bayesian approach is especially appealing in dy-
namic contexts, where data arrives sequentially and updating one’s beliefs
is a natural practice. Bayes’ rule was used to devastating effect before the
2012 US presidential election, updating sequential polling results to cor-
rectly predict the outcome in all 50 states. Bayes’ theorem is an excellent
tool in general for combining statistical evidence from disparate sources,
the closest frequentist analog being maximum likelihood estimation.
(cid:15) In the absence of genuine prior information, a whiff of subjectivity6 hangs
over Bayesian results, even those based on uninformative priors. Classical
frequentism claimed for itself the high ground of scientiﬁc objectivity,
especially in contentious areas such as drug testing and approval, where
skeptics as well as friends hang on the statistical details.

Figure 3.5 is soothingly misleading in its schematics: (cid:22) and x will

6 Here we are not discussing the important subjectivist school of Bayesian inference, of

Savage, de Finetti, and others, covered in Chapter 13.

36

Bayesian Inference

typically be high-dimensional in the chapters that follow, sometimes very
high-dimensional, straining to the breaking point both the frequentist and
the Bayesian paradigms. Computer-age statistical inference at its most
successful combines elements of the two philosophies, as for instance in
the empirical Bayes methods of Chapter 6, and the lasso in Chapter 16.
There are two potent arrows in the statistician’s philosophical quiver, and
faced, say, with 1000 parameters and 1,000,000 data points, there’s no
need to go hunting armed with just one of them.

3.5 Notes and Details

Thomas Bayes, if transferred to modern times, might well be employed as
a successful professor of mathematics. Actually, he was a mid-eighteenth-
century nonconformist English minister with substantial mathematical in-
terests. Richard Price, a leading ﬁgure of letters, science, and politics, had
Bayes’ theorem published in the 1763 Transactions of the Royal Society
(two years after Bayes’ death), his interest being partly theological, with
the rule somehow proving the existence of God. Bellhouse’s (2004) biog-
raphy includes some of Bayes’ other mathematical accomplishments.

Harold Jeffreys was another part-time statistician, working from his day
job as the world’s premier geophysicist of the inter-war period (and ﬁerce
opponent of the theory of continental drift). What we called uninformative
priors are also called noninformative or objective. Jeffreys’ brand of Bayes-
ianism had a dubious reputation among Bayesians in the period 1950–
1990, with preference going to subjective analysis of the type advocated
by Savage and de Finetti. The introduction of Markov chain Monte Carlo
methodology was the kind of technological innovation that changes philoso-
phies. MCMC (Chapter 13), being very well suited to Jeffreys-style anal-
ysis of Big Data problems, moved Bayesian statistics out of the textbooks
and into the world of computer-age applications. Berger (2006) makes a
spirited case for the objective Bayes approach.

1 [p. 26] Correlation coefﬁcient density. Formula (3.11) for the correlation
coefﬁcient density was R. A. Fisher’s debut contribution to the statistics
literature. Chapter 32 of Johnson and Kotz (1970b) gives several equivalent
forms. The constant c in (3.19) is often taken to be .n (cid:0) 3/
(cid:0)1=2, with n the
sample size.
ters from (cid:22) to Q(cid:22) in a smoothly differentiable way. The new family Q

2 [p. 29] Jeffreys’ prior and transformations. Suppose we change parame-
fQ(cid:22).x/

3.5 Notes and Details

37

satisﬁes

@Q(cid:22)

log Q

@

@ Q(cid:22)

fQ(cid:22).x/ D @(cid:22)
@ Q(cid:22)

I(cid:22) (3.16) and QgJeff. Q(cid:22)/ D ˇˇˇ @(cid:22)

Then Q
@Q(cid:22)
says that gJeff.(cid:22)/ transforms correctly to QgJeff. Q(cid:22)/.

IQ(cid:22) D (cid:16) @(cid:22)

2

@

@(cid:22)

log f(cid:22).x/:

(3.30)

ˇˇˇ gJeff.(cid:22)/. But this just

3 [p. 30] The meter-reader fable is taken from Edwards’ (1992) book Likeli-
hood, where he credits John Pratt. It nicely makes the point that frequentist
inferences, which are calibrated in terms of possible observed data sets X,
may be inappropriate for the actual observation x. This is the difference
between working in the horizontal and vertical directions of Figure 3.5.

4 [p. 33] Two-sample t-statistic. Applied to gene i’s data in the prostate
study, the two-sample t-statistic ti (2.17) has theoretical null hypothesis
distribution t100, a Student’s t distribution with 100 degrees of freedom; xi
(cid:0)1.F100.ti //, where ˆ and F100 are the cumulative distribu-
in (3.28) is ˆ
tion functions of standard normal and t100 variables. Section 7.4 of Efron
(2010) motivates approximation (3.28).

5 [p. 33] Selection bias. Senn (2008) discusses the immunity of Bayesian
inferences to selection bias and other “paradoxes,” crediting Phil Dawid for
the original idea. The article catches the possible uneasiness of following
Bayes’ theorem too literally in applications.
The 22 students in Table 3.1 were randomly selected from a larger data
set of 88 in Mardia et al. (1979) (which gave O
 D 0:553). Welch and
Peers (1963) initiated the study of priors whose credible intervals, such
as Œ0:093; 0:750 in Figure 3.2, match frequentist conﬁdence intervals. In
one-parameter problems, Jeffreys’ priors provide good matches, but not
ususally in multiparameter situations. In fact, no single multiparameter
prior can give good matches for all one-parameter subproblems, a source of
tension between Bayesian and frequentist methods revisited in Chapter 11.

4

Fisherian Inference and Maximum

Likelihood Estimation

Sir Ronald Fisher was arguably the most inﬂuential anti-Bayesian of all
time, but that did not make him a conventional frequentist. His key data-
analytic methods—analysis of variance, signiﬁcance testing, and maxi-
mum likelihood estimation—were almost always applied frequentistically.
Their Fisherian rationale, however, often drew on ideas neither Bayesian
nor frequentist in nature, or sometimes the two in combination. Fisher’s
work held a central place in twentieth-century applied statistics, and some
of it, particularly maximum likelihood estimation, has moved forcefully
into computer-age practice. This chapter’s brief review of Fisherian meth-
odology sketches parts of its unique philosophical structure, while concen-
trating on those topics of greatest current importance.

4.1 Likelihood and Maximum Likelihood

Fisher’s seminal work on estimation focused on the likelihood function,
or more exactly its logarithm. For a family of probability densities f(cid:22).x/
(3.1), the log likelihood function is

lx.(cid:22)/ D logff(cid:22).x/g;

(4.1)

the notation lx.(cid:22)/ emphasizing that the parameter vector (cid:22) is varying
while the observed data vector x is ﬁxed. The maximum likelihood esti-
mate (MLE) is the value of (cid:22) in parameter space (cid:127) that maximizes lx.(cid:22)/,

MLE W

O(cid:22) D arg max
(cid:22)2(cid:127)

flx.(cid:22)/g:

(4.2)
It can happen that O(cid:22) doesn’t exist or that there are multiple maximizers, but
here we will assume the usual case where O(cid:22) exists uniquely. More careful
references are provided in the endnotes.

Deﬁnition (4.2) is extended to provide maximum likelihood estimates

38

4.1 Likelihood and Maximum Likelihood

for a function  D T .(cid:22)/ of (cid:22) according to the simple plug-in rule

O
 D T . O(cid:22)/;

39

(4.3)

most often with  being a scalar parameter of particular interest, such as
the regression coefﬁcient of an important covariate in a linear model.

Maximum likelihood estimation came to dominate classical applied es-
timation practice. Less dominant now, for reasons we will be investigating
in subsequent chapters, the MLE algorithm still has iconic status, being of-
ten the method of ﬁrst choice in any novel situation. There are several good
reasons for its ubiquity.
1 The MLE algorithm is automatic: in theory, and almost in practice, a
single numerical algorithm produces O(cid:22) without further statistical input.
This contrasts with unbiased estimation, for instance, where each new
situation requires clever theoretical calculations.

2 The MLE enjoys excellent frequentist properties. In large-sample situa-
tions, maximum likelihood estimates tend to be nearly unbiased, with the
least possible variance. Even in small samples, MLEs are usually quite
efﬁcient, within say a few percent of the best possible performance.

3 The MLE also has reasonable Bayesian justiﬁcation. Looking at Bayes’

rule (3.7),

g.(cid:22)jx/ D cxg.(cid:22)/elx .(cid:22)/;

(4.4)
we see that O(cid:22) is the maximizer of the posterior density g.(cid:22)jx/ if the prior
g.(cid:22)/ is ﬂat, that is, constant. Because the MLE depends on the family
F only through the likelihood function, anomalies of the meter-reader
type are averted.
Figure 4.1 displays two maximum likelihood estimates for the gfr data
of Figure 2.1. Here the data1 is the vector x D .x1; x2; : : : ; xn/, n D 211.
We assume that x was obtained as a random sample of size n from a density
f(cid:22).x/,

iid(cid:24) f(cid:22).x/

for i D 1; 2; : : : ; n;

xi

(4.5)
“iid” abbreviating “independent and identically distributed.” Two families
are considered for the component density f(cid:22).x/, the normal, with (cid:22) D
.; (cid:27) /,

f(cid:22).x/ D

1p
2(cid:25)(cid:27) 2

(cid:0) 1
2 . x(cid:0)

(cid:27) /2

;

e

(4.6)

1 Now x is what we have been calling “x” before, while we will henceforth use x as a

symbol for the individual components of x.

40

Fisherian Inference and MLE

Figure 4.1 Glomerular ﬁltration data of Figure 2.1 and two
maximum-likelihood density estimates, normal (solid black), and
gamma (dashed blue).

and the gamma,2 with (cid:22) D .(cid:21); (cid:27); (cid:23)/,

f(cid:22).x/ D .x (cid:0) (cid:21)/(cid:23)(cid:0)1

(cid:0) x(cid:0)(cid:21)

(cid:27)

e

(for x (cid:21) (cid:21), 0 otherwise):

(4.7)

(4.8)

(4.9)

under iid sampling, we have

Since

iD1

(cid:27) (cid:23).(cid:23)/

f(cid:22).xi /

f(cid:22).x/ D nY
log f(cid:22).xi / D nX
hX

lx.(cid:22)/ D nX
 D .54:3; 13:7/ DNx;

iD1

(cid:16) O

 ; O(cid:27)

lxi .(cid:22)/:

iD1

i1=2

Maximum likelihood estimates were found by maximizing lx.(cid:22)/. For the
normal model (4.6),

(4.10)
2 The gamma distribution is usually deﬁned with (cid:21) D 0 as the lower limit of x. Here we
are allowing the lower limit (cid:21) to vary as a free parameter.

:

.xi (cid:0) Nx/2 =n

 gfrFrequency20406080100051015202530NormalGamma4.2 Fisher Information and the MLE

41

There is no closed-form solution for gamma model (4.7), where numerical
maximization gave

 D .21:4; 5:47; 6:0/:

(cid:16)O

(cid:21); O(cid:27) ; O(cid:23)

(4.11)

The plotted curves in Figure 4.1 are the two MLE densities f O(cid:22).x/. The
gamma model gives a better ﬁt than the normal, but neither is really satis-
factory. (A more ambitious maximum likelihood ﬁt appears in Figure 5.7.)
Most MLEs require numerical minimization, as for the gamma model.
When introduced in the 1920s, maximum likelihood was criticized as com-
putationally difﬁcult, invidious comparisons being made with the older
method of moments, which relied only on sample moments of various
kinds.

There is a downside to maximum likelihood estimation that remained
nearly invisible in classical applications: it is dangerous to rely upon in
problems involving large numbers of parameters. If the parameter vector
(cid:22) has 1000 components, each component individually may be well esti-
mated by maximum likelihood, while the MLE O
 D T . O(cid:22)/ for a quantity of
particular interest can be grossly misleading.
For the prostate data of Figure 3.4, model (4.6) gives MLE O(cid:22)i D xi for
each of the 6033 genes. This seems reasonable, but if we are interested in
the maximum coordinate value

 D T .(cid:22)/ D max

f(cid:22)ig;

(4.12)
the MLE is O
 D 5:29, almost certainly a ﬂagrant overestimate. “Regular-
ized” versions of maximum likelihood estimation more suitable for high-
dimensional applications play an important role in succeeding chapters.

i

4.2 Fisher Information and the MLE

Fisher was not the ﬁrst to suggest the maximum likelihood algorithm for
parameter estimation. His paradigm-shifting work concerned the favorable
inferential properties of the MLE, and in particular its achievement of the
Fisher information bound. Only a brief heuristic review will be provided
here, with more careful derivations referenced in the endnotes.

We begin3 with a one-parameter family of densities
g ;

D ff .x/;  2 (cid:127); x 2

X
3 The multiparameter case is considered in the next chapter.

F

(4.13)

42

Fisherian Inference and MLE

where (cid:127) is an interval of the real line, possibly inﬁnite, while the sam-
the continuous case, with the probability of set A equalingR
ple space X may be multidimensional. (As in the Poisson example (3.3),
f .x/ can represent a discrete density, but for convenience we assume here
A f .x/ dx,
etc.) The log likelihood function is lx. / D log f .x/ and the MLE O
 D
arg maxflx. /g, with  replacing (cid:22) in (4.1)–(4.2) in the one-dimensional
case.

Dots will indicate differentiation with respect to , e.g., for the score

function

P
lx. / D @

log f .x/ D P

f .x/=f .x/:

The score function has expectation 0,

Z

@

lx. /f .x/ dx DZ

P

X

X

Z
X
1 D 0;

f .x/ dx

P
f .x/ dx D @
@
D @
@

(4.14)

(4.15)

where we are assuming the regularity conditions necessary for differenti-
ating under the integral sign at the third step.

The Fisher information I is deﬁned to be the variance of the score

function,

P
lx. /2f .x/ dx;

(4.16)

I DZ

X

the notation

P
lx. / (cid:24) .0;I /

(4.17)
indicating that P
lx. / has mean 0 and variance I. The term “information” is
well chosen. The main result for maximum likelihood estimation, sketched
next, is that the MLE O
 has an approximately normal distribution with
mean  and variance 1=I,

O
 P(cid:24)

N .; 1=I /;

(4.18)

and that no “nearly unbiased” estimator of  can do better. In other words,
bigger Fisher information implies smaller variance for the MLE.

The second derivative of the log likelihood function

R
lx. / D @2

@ 2

log f .x/ D

R
f .x/
f .x/

(cid:0)

f .x/
f .x/

(4.19)

  P

!2

4.2 Fisher Information and the MLE

has expectation

E

(the R

nR

lx. /

o D (cid:0)

I

43

(4.20)

(4.21)

f .x/=f .x/ term having expectation 0 as in (4.15)). We can write

(cid:0)R
lx. / (cid:24) .I ;J /;
where J is the variance of R
lx. /.
Now suppose that x D .x1; x2; : : : ; xn/ is an iid sample from f .x/, as
in (4.5), so that the total score function P
lx. / D nX
lx. /, as in (4.9), is
P
lxi . /;
lx. / D nX
 :D P
(cid:16) O
lx. / C R

O
 / D 0. A ﬁrst-order Taylor series gives the approximation

 based on the full sample x satisﬁes the maximizing condition

The MLE O
P
lx.

(cid:0)R
lxi . /:

 (cid:0) 

;

0 D P

lx



iD1

iD1

P

(cid:0)R

and similarly

(cid:16) O

(4.22)

(4.23)



lx. /

(4.24)

or

O


:D  C

P
lx. /=n
(cid:0)R

lx. /=n

:

(4.25)

Under reasonable regularity conditions, (4.17) and the central limit theo-
rem imply that

P
lx. /=n P(cid:24)
while the law of large numbers has (cid:0)R
(4.21).

(4.26)
N .0;I =n/;
lx. /=n approaching the constant I
Putting all of this together, (4.25) produces Fisher’s fundamental theo-

rem for the MLE, that in large samples

O
 P(cid:24)

N .; 1=.nI // :

(4.27)

This is the same as result (4.18) since the total Fisher information in an iid
sample (4.5) is nI, as can be seen by taking expectations in (4.23).

In the case of normal sampling,

iid(cid:24)

xi

N .; (cid:27) 2/

for i D 1; 2; : : : ; n;

(4.28)

44

Fisherian Inference and MLE

(cid:27) 2

2

iD1

This gives

log.2(cid:25)(cid:27) 2/:

.xi (cid:0)  /2

with (cid:27) 2 known, we compute the log likelihood
(cid:0) n
2

nX
lx. / D (cid:0) 1
nX
.xi (cid:0)  /
lx. / D n
iD1
yielding the familiar result O
 D Nx and, since I D 1=(cid:27) 2,
O
 (cid:24)

P
lx. / D 1

and (cid:0) R

(cid:27) 2

(cid:27) 2

N .; (cid:27) 2=n/

(4.29)

;

(4.30)

(4.31)

from (4.27).

This brings us to an aspect of Fisherian inference neither Bayesian nor
frequentist. Fisher believed there was a “logic of inductive inference” that
would produce the correct answer to any statistical question, in the same
way ordinary logic solves deductive problems. His principal tactic was to
logically reduce a complicated inferential question to a simple form where
the solution should be obvious to all.
Fisher’s favorite target for the obvious was (4.31), where a single scalar
observation O
 is normally distributed around the unknown parameter of
interest , with known variance (cid:27) 2=n. Then everyone should agree in the
absence of prior information that O
 is the best estimate of , that  has
about 95% chance of lying in the interval O
Fisher was astoundingly resourceful at reducing statistical problems to
the form (4.31). Sufﬁciency, efﬁciency, conditionality, and ancillarity were
all brought to bear, with the maximum likelihood approximation (4.27)
being the most inﬂuential example. Fisher’s logical system is not in favor
these days, but its conclusions remain as staples of conventional statistical
practice.
Suppose that Q
 D t .x/ is any unbiased estimate of  based on an iid
sample x D .x1; x2; : : : ; xn/ from f .x/. That is,

 ˙ 1:96O(cid:27) =

p
n, etc.

Then the Cram´er–Rao lower bound, described in the endnotes, says that
the variance of Q

 exceeds the Fisher information bound (4.27),

1

 D Eft .x/g:

n Q



o (cid:21) 1=.nI /:

var

(4.32)

(4.33)

A loose interpretation is that the MLE has variance at least as small as
the best unbiased estimate of . The MLE is generally not unbiased, but

4.3 Conditional Inference

45

p
its bias is small (of order 1=n, compared with standard deviation of order
n), making the comparison with unbiased estimates and the Cram´er–
1=
Rao bound appropriate.

4.3 Conditional Inference

A simple example gets across the idea of conditional inference: an i.i.d.
sample

xi

iid(cid:24)
(4.34)
N .; 1/;
has produced estimate O
 D Nx. The investigators originally disagreed on an
(
affordable sample size n and ﬂipped a fair coin to decide,

i D 1; 2; : : : ; n;

25

100

probability 1/2
probability 1/2I

n D

(4.35)

p

If you answered 1=

n D 25 won. Question: What is the standard deviation of Nx?
25 D 0:2 then you, like Fisher, are an advocate
of conditional inference. The unconditional frequentist answer says that Nx
could have been N .; 1=100/ or N .; 1=25/ with equal probability, yield-
ing standard deviation Œ.0:01 C 0:04/=21=2 D 0:158. Some less obvious
(and less trivial) examples follow in this section, and in Chapter 9, where
conditional inference plays a central role.
The data for a typical regression problem consists of pairs .xi ; yi /, i D
1; 2; : : : ; n, where xi is a p-dimensional vector of covariates for the ith
subject and yi is a scalar response. In Figure 1.1, xi is age and yi the
kidney ﬁtness measure tot. Let x be the n (cid:2) p matrix having xi as its ith
row, and y the vector of responses. A regression algorithm uses x and y
to construct a function rx;y .x/ predicting y for any value of x, as in (1.3),
where O
How accurate is rx;y .x/? This question is usually answered under the
assumption that x is ﬁxed, not random: in other words, by conditioning
on the observed value of x. The standard errors in the second line of Ta-
ble 1.1 are conditional in this sense; they are frequentist standard deviations
ˇ0 C O
of O
ˇ1x, assuming that the 157 values for age are ﬁxed as observed.
(A correlation analysis between age and tot would not make this as-
sumption.)

ˇ1 were obtained using least squares.

ˇ0 and O

Fisher argued for conditional inference on two grounds.

46

Fisherian Inference and MLE

1 More relevant inferences. The conditional standard deviation in situ-
ation (4.35) seems obviously more relevant to the accuracy of the ob-
served O
 for estimating . It is less obvious in the regression example,
though arguably still the case.

2 Simpler inferences. Conditional inferences are often simpler to exe-
cute and interpret. This is the case with regression, where the statistician
doesn’t have to worry about correlation relationships among the covari-
ates, and also with our next example, a Fisherian classic.

Table 4.1 shows the results of a randomized trial on 45 ulcer patients,
comparing new and old surgical treatments. Was the new surgery signiﬁ-
cantly better? Fisher argued for carrying out the hypothesis test conditional
on the marginals of the table .16; 29; 21; 24/. With the marginals ﬁxed, the
number y in the upper left cell determines the other three cells by subtrac-
tion. We need only test whether the number y D 9 is too big under the null
hypothesis of no treatment difference, instead of trying to test the numbers
in all four cells.4

Table 4.1 Forty-ﬁve ulcer patients randomly assigned to either new or
old surgery, with results evaluated as either success or failure.
Was the new surgery signiﬁcantly better?

success

failure

new

old

9

7

16

12

17

29

21

24

45

An ancillary statistic (again, Fisher’s terminology) is one that contains
no direct information by itself, but does determine the conditioning frame-
work for frequentist calculations. Our three examples of ancillaries were
the sample size n, the covariate matrix x, and the table’s marginals. “Con-
tains no information” is a contentious claim. More realistically, the two ad-
vantages of conditioning, relevance and simplicity, are thought to outweigh
the loss of information that comes from treating the ancillary statistic as
nonrandom. Chapter 9 makes this case speciﬁcally for standard survival
analysis methods.

4 Section 9.3 gives the details of such tests; in the surgery example, the difference was not

signiﬁcant.

4.3 Conditional Inference

47

Our ﬁnal example concerns the accuracy of a maximum likelihood esti-
mate O

. Rather than

(4.36)

(4.37)

:

(4.38)

ˇˇˇˇO

(cid:0); 1ı(cid:0)nIO

(cid:1)(cid:1) ;

O
 P(cid:24)

N

O
 P(cid:24)

the plug-in version of (4.27), Fisher suggested using

N .; 1=I.x// ;
where I.x/ is the observed Fisher information

(cid:16) O



 D (cid:0) @2

lx. /

@ 2

I.x/ D (cid:0)R

lx

The expectation of I.x/ is nI, so in large samples the distribution (4.37)
converges to (4.36). Before convergence, however, Fisher suggested that
(4.37) gives a better idea of O
As a check, a simulation was run involving i.i.d. samples x of size n D

’s accuracy.

20 drawn from a Cauchy density
f .x/ D 1

1

:

(cid:25)

1 C .x (cid:0)  /2

 within each group was then calculated.

(4.39)
10,000 samples x of size n D 20 were drawn (with  D 0) and the ob-
served information bound 1=I.x/ computed for each. The 10,000 O
 values
were grouped according to deciles of 1=I.x/, and the observed empirical
variance of O
This amounts to calculating a somewhat crude estimate of the condi-
tional variance of the MLE O
, given the observed information bound 1=I.x/.
Figure 4.2 shows the results. We see that the conditional variance is close
to 1=I.x/, as Fisher predicted. The conditioning effect is quite substan-
tial; the unconditional variance 1=nI is 0.10 here, while the conditional
variance ranges from 0.05 to 0.20.
The observed Fisher information I.x/ acts as an approximate ancillary,
enjoying both of the virtues claimed by Fisher: it is more relevant than the
unconditional information nIO, and it is usually easier to calculate. Once
O
 has been found, I.x/ is obtained by numerical second differentiation.
Unlike I, no probability calculations are required.
There is a strong Bayesian current ﬂowing here. A narrow peak for the
log likelihood function, i.e., a large value of I.x/, also implies a narrow
posterior distribution for  given x. Conditional inference, of which Fig-
ure 4.2 is an evocative example, helps counter the central Bayesian criti-
cism of frequentist inference: that the frequentist properties relate to data
sets possibly much different than the one actually observed. The maximum

48

Fisherian Inference and MLE

Figure 4.2 Conditional variance of MLE for Cauchy samples of
size 20, plotted versus the observed information bound 1=I.x/.
Observed information bounds are grouped by quantile intervals
for variance calculations (in percentages): (0–5), (5–15), : : : ,
(85–95), (95–100). The broken red horizontal line is the
unconditional variance 1=nI.

likelihood algorithm can be interpreted both vertically and horizontally in
Figure 3.5, acting as a connection between the Bayesian and frequentist
worlds.

The equivalent of result (4.37) for multiparameter families, Section 5.3,

(cid:0)(cid:22); I.x/

(cid:0)1(cid:1) ;

O(cid:22) P(cid:24)

(4.40)
plays an important role in succeeding chapters, with (cid:0)I.x/ the p(cid:2)p matrix
of second derivatives

Np

lx.(cid:22)/ D (cid:0)

I.x/ D (cid:0)R

(cid:21)

@2

@(cid:22)i @(cid:22)j

log f(cid:22).x/

:

O(cid:22)

(4.41)

lllllllllll0.050.100.150.200.250.000.050.100.150.200.25Observed Information BoundMLE variance4.4 Permutation and Randomization

49

4.4 Permutation and Randomization

Fisherian methodology faced criticism for its overdependence on normal
sampling assumptions. Consider the comparison between the 47 ALL and
25 AML patients in the gene 136 leukemia example of Figure 1.4. The two-
sample t-statistic (1.6) had value 3.13, with two-sided signiﬁcance level
0.0025 according to a Student-t null distribution with 70 degrees of free-
dom. All of this depended on the Gaussian, or normal, assumptions (2.12)–
(2.13).

As an alternative signiﬁcance-level calculation, Fisher suggested using
permutations of the 72 data points. The 72 values are randomly divided
into disjoint sets of size 47 and 25, and the two-sample t-statistic (2.17) is
recomputed. This is done some large number B times, yielding permuta-
(cid:3)
B. The two-sided permutation signiﬁcance level
tion t-values t
(cid:3)
for the original value t is then the proportion of the t
i values exceeding t
in absolute value,

(cid:3)
2 ; : : : ; t

(cid:3)
1 ; t

(4.42)

#fjt

(cid:3)

j (cid:21) jtjg =B:

i

Figure 4.3 10,000 permutation t
for gene 136 in the leukemia data of Figure 1.3. Of these, 26
(cid:3)-values (red ticks) exceeded in absolute value the observed
t
t-statistic 3.01, giving permutation signiﬁcance level 0.0026.

(cid:3)-values for testing ALL vs AML,

−4−20240200400600800t* valuesfrequency||||||||||||||||||||||||||−3.013.01originalt−statisticFisherian Inference and MLE
50
Figure 4.3 shows the histogram of B D 10,000 t

(cid:3)
i values for the gene
136 data in Figure 1.3: 26 of these exceeded t D 3:01 in absolute value,
yielding signiﬁcance level 0.0026 against the null hypothesis of no ALL/AML
difference, remarkably close to the normal-theory signiﬁcance level 0.0025.
(We were a little lucky here.)

Why should we believe the permutation signiﬁcance level (4.42)? Fisher
provided two arguments.
(cid:15) Suppose we assume as a null hypothesis that the n D 72 observed mea-
surements x are an iid sample obtained from the same distribution f(cid:22).x/,

iid(cid:24) f(cid:22).x/

xi

for i D 1; 2; : : : ; n:

(4.43)

(There is no normal assumption here, say that f(cid:22).x/ is N .; (cid:27) 2/.)
Let o indicate the order statistic of x, i.e., the 72 numbers ordered
from smallest to largest, with their AML or ALL labels removed. Then it
can be shown that all 72Š=.47Š25Š/ ways of obtaining x by dividing o
into disjoint subsets of sizes 47 and 25 are equally likely under null hy-
pothesis (4.43). A small value of the permutation signiﬁcance level (4.42)
indicates that the actual division of AML/ALL measurements was not ran-
dom, but rather resulted from negation of the null hypothesis (4.43). This
might be considered an example of Fisher’s logic of inductive inference,
where the conclusion “should be obvious to all.” It is certainly an exam-
ple of conditional inference, now with conditioning used to avoid speciﬁc
assumptions about the sampling density f(cid:22).x/.
(cid:15) In experimental situations, Fisher forcefully argued for randomization,
that is for randomly assigning the experimental units to the possible treat-
ment groups. Most famously, in a clinical trial comparing drug A with
drug B, each patient should be randomly assigned to A or B.

Randomization greatly strengthens the conclusions of a permutation
test. In the AML/ALL gene-136 situation, where randomization wasn’t fea-
sible, we wind up almost certain that the AML group has systematically
larger numbers, but cannot be certain that it is the different disease states
causing the difference. Perhaps the AML patients are older, or heavier, or
have more of some other characteristic affecting gene 136. Experimen-
tal randomization almost guarantees that age, weight, etc., will be well-
balanced between the treatment groups. Fisher’s RCT (randomized clini-
cal trial) was and is the gold standard for statistical inference in medical
trials.
Permutation testing is frequentistic: a statistician following the proce-
dure has 5% chance of rejecting a valid null hypothesis at level 0.05, etc.

4.5 Notes and Details

51

Randomization inference is somewhat different, amounting to a kind of
forced frequentism, with the statistician imposing his or her preferred prob-
ability mechanism upon the data. Permutation methods are enjoying a healthy
computer-age revival, in contexts far beyond Fisher’s original justiﬁcation
for the t-test, as we will see in Chapter 15.

4.5 Notes and Details

On a linear scale that puts Bayesian on the left and frequentist on the right,
Fisherian inference winds up somewhere in the middle. Fisher rejected
Bayesianism early on, but later criticized as “wooden” the hard-line fre-
quentism of the Neyman–Wald decision-theoretic school. Efron (1998) lo-
cates Fisher along the Bayes–frequentist scale for several different criteria;
see in particular Figure 1 of that paper.

Bayesians, of course, believe there is only one true logic of inductive in-
ference. Fisher disagreed. His most ambitious attempt to “enjoy the Bayes-
ian omelette without breaking the Bayesian eggs”5 was ﬁducial inference.
The simplest example concerns the normal translation model x (cid:24)
N .; 1/,
where  (cid:0) x has a standard N .0; 1/ distribution, the ﬁducial distribution of
 given x then being N .x; 1/. Among Fisher’s many contributions, ﬁdu-
cial inference was the only outright popular bust. Nevertheless the idea has
popped up again in the current literature under the name “conﬁdence dis-
tribution;” see Efron (1993) and Xie and Singh (2013). A brief discussion
appears in Chapter 11.

1 [p. 44] For an unbiased estimator Q

lx. /f .x/ d x DZ

P

Z

t .x/

X

 D t .x/ (4.32), we have
Z
X
 D 1:

P
f .x/ d x D @
@
D @
@

t .x/

X

t .x/f .x/ d x

(4.44)

Here X is X n, the sample space of x D .x1; x2; : : : ; xn/, and we are as-
(4.44) givesR .t .x/ (cid:0)  /

suming the conditions necessary for differentiating under the integral sign;
lx. / has expectation

P
lx. /f .x/ d x D 1 (since P

5 Attributed to the important Bayesian theorist L. J. Savage.

Fisherian Inference and MLE

0), and then, applying the Cauchy–Schwarz inequality,

52

Z

X

or

(cid:21)2
.t .x/ (cid:0)  /2 f .x/d x

P
lx. /f .x/ d x

.t .x/ (cid:0)  /

Z

X

(cid:21)Z
o
n Q



I :

X

P
lx. /2f .x/ d x

(cid:21)

;

(4.45)

(4.46)

1  var

This veriﬁes the Cram´er–Rao lower bound (4.33): the optimal variance for
an unbiased estimator is one over the Fisher information.

Optimality results are a sign of scientiﬁc maturity. Fisher information
and its estimation bound mark the transition of statistics from a collection
of ad-hoc techniques to a coherent discipline. (We have lost some ground
recently, where, as discussed in Chapter 1, ad-hoc algorithmic coinages
have outrun their inferential justiﬁcation.) Fisher’s information bound was
a major mathematical innovation, closely related to and predating, Heisen-
berg’s uncertainty principle and Shannon’s information bound; see Dembo
et al. (1991).

Unbiased estimation has strong appeal in statistical applications, where
“biased,” its opposite, carries a hint of self-interested data manipulation.
In large-scale settings, such as the prostate study of Figure 3.4, one can,
however, strongly argue for biased estimates. We saw this for gene 610,
where the usual unbiased estimate O(cid:22)610 D 5:29 is almost certainly too
large. Biased estimation will play a major role in our subsequent chapters.
Maximum likelihood estimation is effectively unbiased in most situa-

tions. Under repeated sampling, the expected mean squared error

MSE D E

 (cid:0) 

(4.47)
has order-of-magnitude variance D O.1=n/ and bias2 D O.1=n2/, the
latter usually becoming negligible as sample size n increases. (Important
exceptions, where bias is substantial, can occur if O
 D T . O(cid:22)/ when O(cid:22) is
high-dimensional, as in the James–Stein situation of Chapter 7.) Section
10 of Efron (1975) provides a detailed analysis.

Section 9.2 of Cox and Hinkley (1974) gives a careful and wide-ranging
account of the MLE and Fisher information. Lehmann (1983) covers the
same ground, somewhat more technically, in his Chapter 6.

(cid:26)(cid:16) O

2(cid:27) D variance C bias2

5

Parametric Models and Exponential Families

We have been reviewing classic approaches to statistical inference—fre-
quentist, Bayesian, and Fisherian—with an eye toward examining their
strengths and limitations in modern applications. Putting philosophical dif-
ferences aside, there is a common methodological theme in classical statis-
tics: a strong preference for low-dimensional parametric models; that is, for
modeling data-analysis problems using parametric families of probability
densities (3.1),

D˚f(cid:22).x/I x 2

X ; (cid:22) 2 (cid:127)(cid:9) ;

F

(5.1)

where the dimension of parameter (cid:22) is small, perhaps no greater than 5
or 10 or 20. The inverted nomenclature “nonparametric” suggests the pre-
dominance of classical parametric methods.

Two words explain the classic preference for parametric models: math-
ematical tractability. In a world of sliderules and slow mechanical arith-
metic, mathematical formulation, by necessity, becomes the computational
tool of choice. Our new computation-rich environment has unplugged the
mathematical bottleneck, giving us a more realistic, ﬂexible, and far-reach-
ing body of statistical techniques. But the classic parametric families still
play an important role in computer-age statistics, often assembled as small
parts of larger methodologies (as with the generalized linear models of
Chapter 8). This chapter1 presents a brief review of the most widely used
parametric models, ending with an overview of exponential families, the
great connecting thread of classical theory and a player of continuing im-
portance in computer-age applications.

1 This chapter covers a large amount of technical material for use later, and may be

reviewed lightly at ﬁrst reading.

53

54

Parametric Models

5.1 Univariate Families

Univariate parametric families, in which the sample space X of observation
x is a subset of the real line R1, are the building blocks of most statistical
analyses. Table 5.1 names and describes the ﬁve most familiar univariate
families: normal, Poisson, binomial, gamma, and beta. (The chi-squared
distribution with n degrees of freedom (cid:31)2
n is also included since it is dis-

tributed as 2(cid:1) Gam.n=2; 1/.) The normal distribution N .(cid:22); (cid:27) 2/ is a shifted
and scaled version of the N .0; 1/ distribution2 used in (3.27),

N .(cid:22); (cid:27) 2/ (cid:24) (cid:22) C (cid:27)N .0; 1/:

(5.2)

Table 5.1 Five familiar univariate densities, and their sample spaces X ,
parameter spaces (cid:127), and expectations and variances; chi-squared
distribution with n degrees of freedom is 2 Gam.n=2; 1/.
Name,
Notation

Expectation,

Variance

Density

X

Normal
N .(cid:22); (cid:27) 2/

Poisson
Poi.(cid:22)/

Binomial
Bi.n; (cid:25)/

Gamma
Gam.(cid:23); (cid:27) /

Beta
Be.(cid:23)1; (cid:23)2/

p

1

(cid:27)

2(cid:25)

2 . x(cid:0)(cid:22)
(cid:0) 1
(cid:27) /2

e

R1

(cid:0)(cid:22)(cid:22)x

e

xŠ

f0; 1; : : :g

(cid:22) > 0

xŠ.n(cid:0)x/Š (cid:25) x.1 (cid:0) (cid:25)/n(cid:0)x

nŠ

f0; 1; : : : ; ng 0 < (cid:25) < 1

x(cid:23)(cid:0)1e

(cid:0)x=(cid:27)
(cid:27) (cid:23) .(cid:23)/

x (cid:21) 0

.(cid:23)1/.(cid:23)2/ x(cid:23)1(cid:0)1.1 (cid:0) x/(cid:23)2(cid:0)1 0  x  1
.(cid:23)1C(cid:23)2/

(cid:23) > 0
(cid:27) > 0

(cid:23)1 > 0
(cid:23)2 > 0

(cid:127)

(cid:22) 2 R1
(cid:27) 2 > 0

(cid:22)
(cid:27) 2

(cid:22)
(cid:22)

n(cid:25)

n(cid:25).1 (cid:0) (cid:25)/

(cid:27) (cid:23)
(cid:27) 2(cid:23)

(cid:23)1=.(cid:23)1 C (cid:23)2/

.(cid:23)1C(cid:23)2/2.(cid:23)1C(cid:23)2C1/

(cid:23)1(cid:23)2

Relationships abound among the table’s families. For instance, indepen-
dent gamma variables Gam.(cid:23)1; (cid:27) / and Gam.(cid:23)2; (cid:27) / yield a beta variate ac-
cording to

Be.(cid:23)1; (cid:23)2/ (cid:24)

Gam.(cid:23)1; (cid:27) /

Gam.(cid:23)1; (cid:27) / C Gam.(cid:23)2; (cid:27) /

:

(5.3)

The binomial and Poisson are particularly close cousins. A Bi.n; (cid:25)/ distri-
bution (the number of heads in n independent ﬂips of a coin with probabil-
2 The notation in (5.2) indicates that if X (cid:24) N .(cid:22); (cid:27) 2/ and Y (cid:24) N .0; 1/ then X and
(cid:22) C (cid:27)Y have the same distribution.

5.2 The Multivariate Normal Distribution

55

Figure 5.1 Comparison of the binomial distribution Bi.30; 0:2/
(black lines) with the Poisson Poi.6/ (red dots). In the legend we
show the mean and standard deviation for each distribution.

ity of heads (cid:25)) approaches a Poi.n(cid:25)/ distribution,

Bi.n; (cid:25)/ P(cid:24) Poi.n(cid:25)/

(5.4)
as n grows large and (cid:25) small, the notation P(cid:24) indicating approximate equal-
ity of the two distributions. Figure 5.1 shows the approximation already
working quite effectively for n D 30 and (cid:25) D 0:2.

The ﬁve families in Table 5.1 have ﬁve different sample spaces, making
them appropriate in different situations. Beta distributions, for example,
are natural candidates for modeling continuous data on the unit interval
Œ0; 1. Choices of the two parameters .(cid:23)1; (cid:23)2/ provide a variety of possible
shapes, as illustrated in Figure 5.2. Later we will discuss general exponen-
tial families, unavailable in classical theory, that greatly expand the catalog
of possible shapes.

5.2 The Multivariate Normal Distribution

Classical statistics produced a less rich catalog of multivariate distribu-
tions, ones where the sample space X exists in Rp, p-dimensional Eu-

0.000.050.100.150.20xf(x)llllllllllllllllll01234567891011121314151617lBinomial: (6, 2.19)Poisson:  (6, 2.45)56

Parametric Models

Figure 5.2 Three beta densities, with .(cid:23)1; (cid:23)2/ indicated.

clidean space, p > 1. By far the greatest amount of attention focused on
the multivariate normal distribution.
A random vector x D .x1; x2; : : : ; xp/
mean vector

; normally distributed or not, has

0

(cid:22) D Efxg D(cid:0)Efx1g; Efx2g; : : : ; Efxpg(cid:1)0
0(cid:9) D(cid:0)E˚.xi (cid:0) (cid:22)i /.xj (cid:0) (cid:22)j /(cid:9)(cid:1) :

and p (cid:2) p covariance matrix3

† D E˚.x (cid:0) (cid:22)/.x (cid:0) (cid:22)/

(5.6)
0 of vectors u and v is the matrix having elements

(5.5)

(The outer product uv
ui vj .) We will use the convenient notation
x (cid:24) .(cid:22); †/

(5.7)
for (5.5) and (5.6), reducing to the familiar form x (cid:24) .(cid:22); (cid:27) 2/ in the uni-
variate case.

Denoting the entries of † by (cid:27)ij , for i and j equaling 1; 2; : : : ; p, the

diagonal elements are variances,

(cid:27)i i D var.xi /:

(5.8)

3 The notation † D .(cid:27)ij / deﬁnes the ij th element of a matrix.

0.00.20.40.60.81.00.00.51.01.52.02.53.0xf(x)( n1,n2 )( 8, 4)( 2, 4)(.5,.5)5.2 The Multivariate Normal Distribution

57

The off-diagonal elements relate to the correlations between the coordi-
nates of x,

cor.xi ; xj / D (cid:27)ijp

(cid:27)i i (cid:27)jj

:

(5.9)

The multivariate normal distribution extends the univariate deﬁnition
0 be a vector

N .(cid:22); (cid:27) 2/ in Table 5.1. To begin with, let z D .z1; z2; : : : ; zp/
of p independent N .0; 1/ variates, with probability density function

f .z/ D .2(cid:25)/

(cid:0) p
2 e

(cid:0) 1

2

i D .2(cid:25)/

(cid:0) p
2 e

(cid:0) 1
2 z

0

z

(5.10)

Pp

1 z2

according to line 1 of Table 5.1.
The multivariate normal family is obtained by linear transformations of
z: let (cid:22) be a p-dimensional vector and T a p (cid:2) p nonsingular matrix, and
deﬁne the random vector

Following the usual rules of probability transformations yields the density
of x,

f(cid:22);†.x/ D .2(cid:25)/

(cid:0)1.x(cid:0)(cid:22)/;
where † is the p (cid:2) p symmetric positive deﬁnite matrix

(cid:0) 1
2 .x(cid:0)(cid:22)/
0

e

†

(cid:0)p=2
j†j1=2

and j†j its determinant;  f(cid:22);†.x/, the p-dimensional multivariate normal 1
distribution with mean (cid:22) and covariance †, is denoted

x (cid:24)

Np.(cid:22); †/:

(5.14)
Figure 5.3 illustrates the bivariate normal distribution with (cid:22) D .0; 0/
0
and † having (cid:27)11 D (cid:27)22 D 1 and (cid:27)12 D 0:5 (so cor.x1; x2/ D 0:5). The
bell-shaped mountain on the left is a plot of density (5.12). The right panel
shows a scatterplot of 2000 points drawn from this distribution. Concentric
ellipses illustrate curves of constant density,

.x (cid:0) (cid:22)/
0

(cid:0)1.x (cid:0) (cid:22)/ D constant:

†

(5.15)

Classical multivariate analysis was the study of the multivariate normal
distribution, both of its probabilistic and statistical properties. The notes
reference some important (and lengthy) multivariate texts. Here we will
just recall a couple of results useful in the chapters to follow.

x D (cid:22) C T z:

† D T T

0

(5.11)

(5.12)

(5.13)

58

Parametric Models

Figure 5.3 Left: bivariate normal density, with var.x1/ D
var.x2/ D 1 and cor.x1; x2/ D 0:5. Right: sample of 2000
.x1; x2/ pairs from this bivariate normal density.

0

;

(5.16)

0 is partitioned into

Suppose that x D .x1; x2; : : : ; xp/
x.1/ D .x1; x2; : : : ; xp1 /
and x.2/ D .xp1C1; xp1C2; : : : ; xp1Cp2 /
  
!
p1 C p2 D p, with (cid:22) and † similarly partitioned,

 

!

†11 †12

!

0

Np

†21 †22

(5.17)
(so †11 is p1 (cid:2) p1, †12 is p1 (cid:2) p2, etc.). Then the conditional distribution
of x.2/ given x.1/ is itself normal,
x.2/jx.1/ (cid:24)
11 .x.1/ (cid:0) (cid:22).1//; †22 (cid:0) †21†
(cid:0)1
If p1 D p2 D 1, then (5.18) reduces to

(cid:0)(cid:22).2/ C †21†

(cid:0)1
11 †12

(cid:1) :

Np2

(5.18)

x.1/
x.2/

(cid:22).1/
(cid:22).2/

;

(cid:24)

2

x2jx1 (cid:24)

N

.x1 (cid:0) (cid:22)1/; (cid:27)22 (cid:0) (cid:27) 2

12
(cid:27)11

(5.19)

here (cid:27)12=(cid:27)11 is familiar as the linear regression coefﬁcient of x2 as a func-
12=(cid:27)11(cid:27)22 equals cor.x1; x2/2, the squared proportion
tion of x1, while (cid:27) 2
R2 of the variance of x2 explained by x1. Hence we can write the (unex-
plained) variance term in (5.19) as (cid:27)22.1 (cid:0) R2/.
Bayesian statistics also makes good use of the normal family. It helps to
begin with the univariate case x (cid:24)
N .(cid:22); (cid:27) 2/, where now we assume that


(cid:22)2 C (cid:27)12

(cid:27)11

I

****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************−2−1012−2−1012x1x1x2x25.3 Fisher’s Information Bound

59

the expectation vector itself has a normal prior distribution N .M; A/:

(cid:22) (cid:24)

N .M; A/

and xj(cid:22) (cid:24)

N .(cid:22); (cid:27) 2/:

(5.20)

Bayes’ theorem and some algebra show that the posterior distribution of (cid:22)
having observed x is normal,

3


M C A



(cid:22)jx (cid:24)

.x (cid:0) M /;

A(cid:27) 2
A C (cid:27) 2

N

A C (cid:27) 2

(5.21)
The posterior expectation O(cid:22)Bayes D M C .A=.AC (cid:27) 2//.x(cid:0) M / is a shrink-
age estimator of (cid:22): if, say, A equals (cid:27) 2, then O(cid:22)Bayes D M C .x (cid:0) M /=2
is shrunk half the way back from the unbiased estimate O(cid:22) D x toward the
prior mean M , while the posterior variance (cid:27) 2=2 of O(cid:22)Bayes is only one-half
that of O(cid:22).

:

The multivariate version of the Bayesian setup (5.20) is

(cid:22) (cid:24)

and xj(cid:22) (cid:24)

(5.22)
now with M and (cid:22) p-vectors, and A and † positive deﬁnite p(cid:2)p matrices.
As indicated in the notes, the posterior distribution of (cid:22) given x is then

Np.(cid:22); †/;

Np.M; A/

(cid:0)M C A.A C †/

(cid:22)jx (cid:24)

Np

which reduces to (5.21) when p D 1.

(cid:0)1.x (cid:0) M /; A.A C †/

(5.23)

(cid:0)1†(cid:1) ;

5.3 Fisher’s Information Bound for Multiparameter Families

The multivariate normal distribution plays its biggest role in applications
as a large-sample approximation for maximum likelihood estimates. We
suppose that the parametric family of densities ff(cid:22).x/g, normal or not, is
smoothly deﬁned in terms of its p-dimensional parameter vector (cid:22). (In
terms of (5.1), (cid:127) is a subset of Rp.)
The MLE deﬁnitions and results are direct analogues of the single-param-
eter calculations beginning at (4.14) in Chapter 4. The score function P
lx.(cid:22)/
is now deﬁned as the gradient of logff(cid:22).x/g,

0

˚log f(cid:22).x/(cid:9) D
nP

E(cid:22)

lx.(cid:22)/

: : : ;

@ log f(cid:22).x/

@(cid:22)i

o D 0 D .0; 0; 0; : : : ; 0/

0

; : : :

;

(5.24)

:

(5.25)

the p-vector of partial derivatives of log f(cid:22).x/ with respect to the coordi-
nates of (cid:22). It has mean zero,

P
lx.(cid:22)/ D r(cid:22)

Parametric Models

lx.(cid:22)/; using outer product notation,

60
By deﬁnition, the Fisher information matrix I (cid:22) for (cid:22) is the p (cid:2) p covari-
ance matrix of P
nP
I (cid:22) D E(cid:22)
(5.26)
The key result is that the MLE O(cid:22) D arg max(cid:22)ff(cid:22).x/g has an approxi-
mately normal distribution with covariance matrix I(cid:0)1
(cid:22) ,

(cid:26) @ log f(cid:22).x/

0o D

@ log f(cid:22).x/

P
lx.(cid:22)/

(cid:27)

lx.(cid:22)/

@(cid:22)j

@(cid:22)i

E(cid:22)

:

O(cid:22) P(cid:24)

Np.(cid:22);I(cid:0)1

(cid:22) /:

(5.27)

Approximation (5.27) is justiﬁed by large-sample arguments, say with x
an iid sample in Rp, .x1; x2; : : : ; xn/, n going to inﬁnity.
Suppose the statistician is particularly interested in (cid:22)1, the ﬁrst coordi-
nate of (cid:22). Let (cid:22).2/ D .(cid:22)2; (cid:22)3; : : : ; (cid:22)p/ denote the other p (cid:0) 1 coordinates
of (cid:22), which are now “nuisance parameters” as far as the estimation of (cid:22)1
goes. According to (5.27), the MLE O(cid:22)1, which is the ﬁrst coordinate of O(cid:22),
has

(cid:0)(cid:22)1; .I(cid:0)1

(cid:22) /11

O(cid:22)1 P(cid:24)

N

(5.28)

(cid:1) ;



where the notation indicates the upper leftmost entry of I(cid:0)1
(cid:22) .

We can partition the information matrix I (cid:22) into the two parts corre-

sponding to (cid:22)1 and (cid:22).2/,

I (cid:22) D
(cid:22) /11 D(cid:0)

I(cid:22)11
I(cid:22)1.2/
I(cid:22).2/1 I (cid:22).22/

(5.29)
(cid:22).2/1 of dimension 1(cid:2).p(cid:0)1/ and I (cid:22).22/ .p(cid:0)1/(cid:2).p(cid:0)1/).
0
I
.I(cid:0)1

(cid:1)(cid:0)1

(5.30)

:

I(cid:22)11 (cid:0)

I(cid:22)1.2/I(cid:0)1

(cid:22).22/I(cid:22).2/1

(with I(cid:22)1.2/ D

4

The endnotes show that

The subtracted term on the right side of (5.30) is nonnegative, implying
that

.I(cid:0)1

(cid:22) /11 (cid:21)

(cid:0)1
I
(cid:22)11:

(5.31)

If (cid:22).2/ were known to the statistician, rather than requiring estimation,
then f(cid:22)1(cid:22).2/ .x/ would be a one-parameter family, with Fisher information
I(cid:22)11 for estimating (cid:22)1, giving
O(cid:22)1 P(cid:24)

(5.32)

(cid:0)1
(cid:22)11/:

N .(cid:22)1;I

5.4 The Multinomial Distribution

61

Comparing (5.28) with (5.32), (5.31) shows that the variance of the MLE
O(cid:22)1 must always increase4 in the presence of nuisance parameters.
Maximum likelihood, and in fact any form of unbiased or nearly unbi-
ased estimation, pays a nuisance tax for the presence of “other” parameters.
Modern applications often involve thousands of others; think of regression
ﬁts with too many predictors. In some circumstances, biased estimation
methods can reverse the situation, using the others to actually improve esti-
mation of a target parameter; see Chapter 6 on empirical Bayes techniques,
and Chapter 16 on `1 regularized regression models.

5

5.4 The Multinomial Distribution

Second in the small catalog of well-known classic multivariate distribu-
tions is the multinomial. The multinomial applies to situations in which
the observations take on only a ﬁnite number of discrete values, say L of
them. The 2(cid:2)2 ulcer surgery of Table 4.1 is repeated in Table 5.2, now with
the cells labeled 1; 2; 3; and 4. Here there are L D 4 possible outcomes
for each patient: (new, success), (new, failure), (old, success),
(old, failure).

Table 5.2 The ulcer study of Table 4.1, now with the cells numbered 1
through 4 as shown.

success

failure

new

old

1

3

9

7

2

4

12

17

A number n of cases has been observed, n D 45 in Table 5.2. Let x D

.x1; x2; : : : ; xL/ be the vector of counts for the L possible outcomes,

xl D #fcases having outcome lg;

(5.33)
0 for the ulcer data. It is convenient to code the outcomes

x D .9; 12; 7; 17/
in terms of the coordinate vectors el of length L,
el D .0; 0; : : : ; 0; 1; 0; : : : ; 0/
0

with a 1 in the lth place.
4 Unless I(cid:22)1.2/ is a vector of zeros, a condition that amounts to approximate
independence of O(cid:22)1 and O(cid:22).2/.

;

(5.34)

62

Parametric Models

Figure 5.4 The simplex S3 is an equilateral triangle set at an
angle to the coordinate axes in R3.

The multinomial probability model assumes that the n cases are inde-
pendent of each other, with each case having probability (cid:25)l for outcome
el,

(cid:25)l D Prfelg;

l D 1; 2; : : : ; L:

indicate the vector of probabilities. The count vector x then follows the
multinomial distribution,

Let

denoted

0

(cid:25) D .(cid:25)1; (cid:25)2; : : : ; (cid:25)L/
LY

nŠ

x1Šx2Š : : : xLŠ

lD1

f(cid:25) .x/ D

(5.35)

(5.36)

(cid:25) xl
l

;

(5.37)

(5.38)

:

(5.39)

(for n observations, L outcomes, probability vector (cid:25)).

The parameter space (cid:127) for (cid:25) is the simplex SL,

x (cid:24) MultL.n; (cid:25)/
LX

(
(cid:25) W (cid:25)l (cid:21) 0 and

lD1

)
(cid:25)l D 1

SL D

Figure 5.4 shows S3, an equilateral triangle sitting at an angle to the coordi-
nate axes e1; e2; and e3. The midpoint of the triangle (cid:25) D .1=3; 1=3; 1=3/

5.4 The Multinomial Distribution

63

corresponds to a multinomial distribution putting equal probability on the
three possible outcomes.

Figure 5.5 Sample space X for x (cid:24) Mult3.4; (cid:25)/; numbers

indicate .x1; x2; x3/.

The sample space X for x is the subset of nSL (the set of nonnegative
vectors summing to n) having integer components. Figure 5.5 illustrates
the case n D 4 and L D 3, now with the triangle of Figure 5.4 multiplied
by 4 and set ﬂat on the page. The point 121 indicates x D .1; 2; 1/, with
probability 12 (cid:1) (cid:25)1(cid:25) 2
In the dichotomous case, L D 2, the multinomial distribution reduces
to the binomial, with .(cid:25)1; (cid:25)2/ equaling .(cid:25); 1 (cid:0) (cid:25)/ in line 3 of Table 5.1,
and .x1; x2/ equaling .x; n (cid:0) x/. The mean vector and covariance matrix
of MultL.n; (cid:25)/, for any value of L, are

2 (cid:25)3 according to (5.37), etc.

x (cid:24)(cid:0)n(cid:25); n(cid:2)diag.(cid:25)/ (cid:0) (cid:25)(cid:25)

0(cid:3)(cid:1)

(5.40)
(diag.(cid:25)/ is the diagonal matrix with diagonal elements (cid:25)l), so var.xl / D
n(cid:25)l .1 (cid:0) (cid:25)l / and covariance .xl ; xj / D (cid:0)n(cid:25)l (cid:25)j ; (5.40) generalizes the
binomial mean and variance .n(cid:25); n(cid:25).1 (cid:0) (cid:25)//.

There is a useful relationship between the multinomial distribution and
the Poisson. Suppose S1; S2; : : : ; SL are independent Poissons having pos-
sibly different parameters,

ind(cid:24) Poi.(cid:22)l /;

Sl

l D 1; 2; : : : ; L;

(5.41)

6

or, more concisely,

S (cid:24) Poi.(cid:22)/

with S D .S1; S2; : : : ; SL/

0 and (cid:22) D .(cid:22)1; (cid:22)2; : : : ; (cid:22)L/

(5.42)
0, the independence

103	  202	  301	  004	  400	  211	  121	  112	  310	  220	  130	  040	  031	  022	  013	  being assumed in notation (5.42). Then the conditional distribution of S

7

Parametric Models

64

given the sum SC DP Sl is multinomial,
(cid:22)C DP (cid:22)l.

SjSC (cid:24) MultL.SC; (cid:22)=(cid:22)C/;

(5.43)

Going in the other direction, suppose N (cid:24) Poi.n/. Then the uncondi-

tional or marginal distribution of MultL.N; (cid:25)/ is Poisson,
if N (cid:24) Poi.n/:

MultL.N; (cid:25)/ (cid:24) Poi.n(cid:25)/

(5.44)
Calculations involving x (cid:24) MultL.n; (cid:25)/ are sometimes complicated by
the multinomial’s correlations. The approximation x P(cid:24) Poi.n(cid:25)/ removes
the correlations and is usually quite accurate if n is large.

There is one more important thing to say about the multinomial family: it
contains all distributions on a sample space X composed of L discrete cat-
egories. In this sense it is a model for nonparametric inference on X . The
nonparametric bootstrap calculations of Chapter 10 use the multinomial in
this way. Nonparametrics, and the multinomial, have played a larger role
in the modern environment of large, difﬁcult to model, data sets.

5.5 Exponential Families

Classic parametric families dominated statistical theory and practice for a
century and more, with an enormous catalog of their individual properties—
means, variances, tail areas, etc.—being compiled. A surprise, though a
slowly emerging one beginning in the 1930s, was that all of them were
examples of a powerful general construction: exponential families. What
follows here is a brief introduction to the basic theory, with further devel-
opment to come in subsequent chapters.

To begin with, consider the Poisson family, line 2 of Table 5.1. The ratio

of Poisson densities at two parameter values (cid:22) and (cid:22)0 is

f(cid:22).x/
f(cid:22)0 .x/
which can be re-expressed as

 (cid:22)

x

(cid:22)0

D e

(cid:0).(cid:22)(cid:0)(cid:22)0/

;

f(cid:22).x/ D e˛x(cid:0) .˛/f(cid:22)0 .x/;

where we have deﬁned

˛ D logf(cid:22)=(cid:22)0g

and  .˛/ D (cid:22)0.e˛ (cid:0) 1/:

Looking at (5.46), we can describe the Poisson family in three steps.

(5.45)

(5.46)

(5.47)

5.5 Exponential Families

65

1 Start with any one Poisson distribution f(cid:22)0 .x/.
2 For any value of (cid:22) > 0 let ˛ D logf(cid:22)=(cid:22)0g and calculate
for x D 0; 1; 2; : : : :

Q
f(cid:22).x/ D e˛xf(cid:22)0 .x/
3 Finally, divide Q
f(cid:22).x/ by exp. .˛// to get the Poisson density f(cid:22).x/.
In other words, we “tilt” f(cid:22)0 .x/ with the exponential factor e˛x to get
Q
f(cid:22).x/, and then renormalize Q
f(cid:22).x/ to sum to 1. Notice that (5.46) gives
exp.(cid:0) .˛// as the renormalizing constant since

(5.48)

1X

e .˛/ D

e˛xf(cid:22)0 .x/:

(5.49)

0

Figure 5.6 Poisson densities for (cid:22) D 3; 6; 9; 12; 15; 18; heavy
green curve with dots for (cid:22) D 12.

Figure 5.6 graphs the Poisson density f(cid:22).x/ for (cid:22) D 3; 6; 9; 12; 15; 18.
Each Poisson density is a renormalized exponential tilt of any other Poisson
density. So for instance f6.x/ is obtained from f12.x/ via the tilt e˛x with
˛ D logf6=12g D (cid:0)0:693.5

5 Alternate expressions for f(cid:22).x/ as an exponential family are available, for example
exp.˛x (cid:0)  .˛//f0.x/, where ˛ D log (cid:22),  .˛/ D exp.˛/, and f0.x/ D 1=xŠ. (It
isn’t necessary for f0.x/ to be a member of the family.)

0510152025300.000.050.100.150.20xf(x)lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll66

Parametric Models

The Poisson is a one-parameter exponential family, in that ˛ and x in
expression (5.46) are one-dimensional. A p-parameter exponential family
has the form

f˛.x/ D e˛

0

y(cid:0) .˛/f0.x/

(5.50)
where ˛ and y are p-vectors and A is contained in Rp. Here ˛ is the
“canonical” or “natural” parameter vector and y D t .x/ is the “sufﬁcient
statistic” vector. The normalizing function  .˛/, which makes f˛.x/ inte-
grate (or sum) to one, satisﬁes

for ˛ 2 A;

0

e˛

yf0.x/ dx;

(5.51)

8

and it can be shown that the parameter space A for which the integral is
ﬁnite is a convex set  in Rp. As an example, the gamma family on line 4
of Table 5.1 is a two-parameter exponential family, with ˛ and y D t .x/
given by

e .˛/ DZ

X



.˛1; ˛2/ D(cid:0) 1

and

.y1; y2/ D .x; log x/ ;

; (cid:23)

;

(cid:27)

 .˛/ D (cid:23) log (cid:27) C log .(cid:23)/

D (cid:0)˛2 logf(cid:0)˛1g C logf.˛2/g :

The parameter space A is f˛1 < 0 and ˛2 > 0g.
Why are we interested in exponential tilting rather than some other trans-
formational form? The answer has to do with repeated sampling. Suppose
x D .x1; x2; : : : ; xn/ is an iid sample from a p-parameter exponential
family (5.50). Then, letting yi D t .xi / denote the sufﬁcient vector corre-
sponding to xi,

(5.52)

(5.53)

(5.54)

f˛.x/ D nY

0

yi(cid:0) .˛/f0.xi /
e˛
iD1
D en.˛
0 Ny(cid:0) .˛//f0.x/;

where Ny DPn

1 yi =n. This is still a p-parameter exponential family, now
with natural parameter n˛, sufﬁcient statistic Ny, and normalizer n .˛/.
No matter how large n may be, the statistician can still compress all the
inferential information into a p-dimensional statistic Ny. Only exponential
families enjoy this property.

Even though they were discovered and developed in quite different con-
texts, and at quite different times, all of the distributions discussed in this

5.5 Exponential Families

67

chapter exist in exponential families. This isn’t quite the coincidence it
seems. Mathematical tractability was the prized property of classic para-
metric distributions, and tractability was greatly facilitated by exponential
structure, even if that structure went unrecognized.

In one-parameter exponential families, the normalizer  .˛/ is also known
as the cumulant generating function. Derivatives of  .˛/ yield the cumu-
lants of y,6 the ﬁrst two giving the mean and variance

9

P .˛/ D E˛fyg

R .˛/ D var˛fyg:

and

(5.55)

(5.56)

(5.57)

(5.58)

Similarly, in p-parametric families

and

P .˛/ D .: : : @ =@˛j : : : /

0 D E˛fyg

R .˛/ D @2 .˛/

 D cov˛fyg:

@˛j @˛k

The p-dimensional expectation parameter, denoted

ˇ D E˛fyg;

is a one-to-one function of the natural parameter ˛. Let V˛ indicate the
p (cid:2) p covariance matrix,

V˛ D cov˛.y/:

Then the p (cid:2) p derivative matrix of ˇ with respect to ˛ is

(5.59)

dˇ

D .@ˇj =@˛k/ D V˛;

V

d˛

(5.60)
this following from (5.56)–(5.57), the inverse mapping being d˛=dˇ D
˛ . As a one-parameter example, the Poisson in Table 5.1 has ˛ D log (cid:22),
(cid:0)1
ˇ D (cid:22), y D x, and dˇ=d˛ D 1=.d˛=dˇ/ D (cid:22) D V˛.
The maximum likelihood estimate for the expectation parameter ˇ is
simply y (or Ny under repeated sampling (5.54)), which makes it immediate
to calculate in most situations. Less immediate is the MLE for the natural 10
parameter ˛: the one-to-one mapping ˇ D P .˛/ (5.56) has inverse ˛ D
P 
(cid:0)1.ˇ/, so

O˛ D P 

(cid:0)1.y/;

(5.61)
6 The simpliﬁed dot notation leads to more compact expressions: P .˛/ D d .˛/=d˛
and R .˛/ D d 2 .˛/=d˛2.

Parametric Models

68
e.g., O˛ D log y for the Poisson. The trouble is that P 
(cid:0)1.(cid:1)/ is usually un-
available in closed form. Numerical approximation algorithms are neces-
sary to calculate O˛ in most cases.

All of the classic exponential families have closed-form expressions for
 .˛/ (and f˛.x/), yielding pleasant formulas for the mean ˇ and covar-
iance V˛, (5.56)–(5.57). Modern computational technology allows us to
work with general exponential families, designed for speciﬁc tasks, with-
out concern for mathematical tractability.

Figure 5.7 A seven-parameter exponential family ﬁt to the gfr
data of Figure 2.1 (solid) compared with gamma ﬁt of Figure 4.1
(dashed).

As an example we again consider ﬁtting the gfr data of Figure 2.1.
For our exponential family of possible densities we take f0.x/  1, and
sufﬁcient statistic vector

y.x/ D .x; x2; : : : ; x7/;

0

(5.62)
y in (5.50) can represent all 7th-order polynomials in x, the gfr
so ˛
measurement.7 (Stopping at power 2 gives the N .(cid:22); (cid:27) 2/ family, which we
already know ﬁts poorly from Figure 4.1.) The heavy curve in Figure 5.7
shows the MLE ﬁt fO˛.x/ now following the gfr histogram quite closely.
Chapter 10 discusses “Lindsey’s method,” a simpliﬁed algorithm for cal-
culating the MLE O˛.

7 Any intercept in the polynomial is absorbed into the  .˛/ term in (5.57).

 gfrFrequency20406080100051015202530GammaExponential Family5.6 Notes and Details

69

A more exotic example concerns the generation of random graphs on a
ﬁxed set of N nodes. Each possible graph has a certain total number E of
edges, and T of triangles. A popular choice for generating such graphs is
the two-parameter exponential family having y D .E; T /, so that larger
values of ˛1 and ˛2 yield more connections.

5.6 Notes and Details

The notion of sufﬁcient statistics, ones that contain all available inferen-
tial information, was perhaps Fisher’s happiest contribution to the classic
corpus. He noticed that in the exponential family form (5.50), the fact that
the parameter ˛ interacts with the data x only through the factor exp.˛
y/
makes y.x/ sufﬁcient for estimating ˛. In 1935–36, a trio of authors, work-
ing independently in different countries, Pitman, Darmois, and Koopmans,
showed that exponential families are the only ones that enjoy ﬁxed-dimen-
sional sufﬁcient statistics under repeated independent sampling. Until the
late 1950s such distributions were called Pitman–Darmois–Koopmans fam-
ilies, the long name suggesting infrequent usage.

0

Generalized linear models, Chapter 8, show the continuing impact of
sufﬁciency on statistical practice. Peter Bickel has pointed out that data
compression, a lively topic in areas such as image transmission, is a mod-
ern, less stringent, version of sufﬁciency.

Our only nonexponential family so far was (4.39), the Cauchy transla-
tional model. Efron and Hinkley (1978) analyze the Cauchy family in terms
of curved exponential families, a generalization of model (5.50).

Properties of classical distributions (lots of properties and lots of distri-
butions) are covered in Johnson and Kotz’s invaluable series of reference
books, 1969–1972. Two classic multivariate analysis texts are Anderson
(2003) and Mardia et al. (1979).
(cid:0)1

(cid:0)1.x (cid:0) (cid:22)/ we have dz=dx D T

1 [p. 57] Formula (5.12). From z D T
2 jT
(cid:0) p

and
f(cid:22);†.x/ D f .z/jT
so (5.12) follows from T T

(cid:0)1j D .2(cid:25)/

2 .x(cid:0)(cid:22)/
(cid:0) 1
0
0 D † and jT j D j†j1=2.

0

(cid:0)1

T

(cid:0)1.x(cid:0)(cid:22)/;

(5.63)

T

(cid:0)1je
(cid:1)(cid:0)1

 D

 (cid:0)†11 (cid:0) †12†

ƒ11 ƒ12

2 [p. 58] Formula (5.18). Let ƒ D †
(cid:0)1 be partitioned as in (5.17). Then
(cid:0)1
22 †21

(cid:1)(cid:0)1
(5.64)
direct multiplication showing that ƒ† D I, the identity matrix. If † is

(cid:0)†22 (cid:0) †21†

(cid:0)1
22 †21ƒ11

(cid:0)†

(cid:0)1
11 †12ƒ22

(cid:0)1
11 †12

ƒ21 ƒ22

!

;

(cid:0)†

Parametric Models
70
symmetric then ƒ21 D ƒ
12. By redeﬁning x to be x (cid:0) (cid:22) we can set (cid:22).1/
0
and (cid:22).2/ equal to zero in (5.18). The quadratic form in the exponent of
(5.12) is

0

.2//ƒ(cid:0)x.1/; x.2/
(cid:1) D x
(cid:0)x.2/ (cid:0) †21†

0
.1/; x

.1/ƒ12x.2/ C x
0
(cid:1)
But, using (5.64), this matches the quadratic form from (5.18),

.2/ƒ22x.2/ C 2x
0
(cid:1)0

(cid:0)x.2/ (cid:0) †21†

0
.1/ƒ11x.1/:

(5.65)

(5.66)

(cid:0)1
11 x.1/

(cid:0)1
11 x.1/

ƒ22

.x

except for an added term that does not involve x.2/. For a multivariate nor-
mal distribution, this is sufﬁcient to show that the conditional distribution
of x.2/ given x.1/ is indeed (5.18) (see 3).

3 [p. 59] Formulas (5.21) and (5.23). Suppose that the continuous univariate

random variable z has density of the form

f .z/ D c0e

(cid:0) 1
2 Q.z/;

where Q.z/ D az2 C 2bz C c1;

(5.67)

a; b; c0 and c1 constants, a > 0. Then, by “completing the square,”

f .z/ D c2e

2 a.z(cid:0) b
(cid:0) 1

a /2

;

(5.68)
and we see that z (cid:24)
N .b=a; 1=a/. The key point is that form (5.67) spec-
iﬁes z as normal, with mean and variance uniquely determined by a and
b. The multivariate version of this fact was used in the derivation of for-
mula (5.18).
By redeﬁning (cid:22) and x as (cid:22) (cid:0) M and x (cid:0) M , we can take M D 0 in
(5.21). Setting B D A=.A C (cid:27) 2/, density (5.21) for (cid:22)jx is of form (5.67),
with

Q.(cid:22)/ D (cid:22)2

(cid:0) 2x(cid:22)
(cid:27) 2

C Bx2
(cid:27) 2

(5.69)
But Bayes’ rule says that the density of (cid:22)jx is proportional to g.(cid:22)/f(cid:22).x/,
also of form (5.67), now with

B(cid:27) 2

:

Q.(cid:22)/ D 1



C 1
(cid:27) 2

A

(cid:22)2 (cid:0) 2x(cid:22)

(cid:27) 2

C x2
(cid:27) 2

:

(5.70)

A little algebra shows that the quadratic and linear coefﬁcients of (cid:22) match
in (5.69)–(5.70), verifying (5.21).

We verify the multivariate result (5.23) using a different argument. The

2p vector .(cid:22); x/

0 has joint distribution

M



A

N

;

A

A A C †

M



:

(5.71)

5.6 Notes and Details

71

Now we employ (5.18) and a little manipulation to get (5.23).

4 [p. 60] Formula (5.30). This is the matrix identity (5.64), now with †

equaling I(cid:22).
5 [p. 61] Multivariate Gaussian and nuisance parameters. The cautionary
message here—that increasing the number of unknown nuisance parame-
ters decreases the accuracy of the estimate of interest—can be stated more
positively: if some nuisance parameters are actually known, then the MLE
of the parameter of interest becomes more accurate. Suppose, for example,
we wish to estimate (cid:22)1 from a sample of size n in a bivariate normal model
x (cid:24)
N2.(cid:22); †/ (5.14). The MLE Nx1 has variance (cid:27)11=n in notation (5.19).
But if (cid:22)2 is known then the MLE of (cid:22)1 becomes Nx1 (cid:0) .(cid:27)12=(cid:27)22/.Nx2 (cid:0) (cid:22)2/
p
with variance .(cid:27)11=n/ (cid:1) .1 (cid:0) (cid:26)2/, (cid:26) being the correlation (cid:27)12=
iD1 xi, where the xi are iid observations
having Prfxi D eig D (cid:25)l, as in (5.35). The mean and covariance of each
xi are

6 [p. 63] Formula (5.40). x D Pn
Efxig D LX

(cid:25)l el D (cid:25)

(cid:27)11(cid:27)22.

(5.72)

1

g DX

and

0

(cid:0) (cid:25)(cid:25)

0

covfxig D Efxi x

0

g (cid:0) EfxigEfx

0

i

i

0

(cid:25)l el e
l

D diag.(cid:25)/ (cid:0) (cid:25)(cid:25)

Formula (5.40) follows from Efxg DP Efxig and cov.x/ DP cov.xi /.
7 [p. 64] Formula (5.43). The densities of S (5.42) and SC DP Sl are
f(cid:22).S / D LY

SCC =SCŠ:
(cid:22)

(cid:0)(cid:22)l (cid:22)Sl

(5.74)

(5.73)

(cid:0)(cid:22)C

e

:

l =Sl Š

lD1

The conditional density of S given SC is the ratio

and f(cid:22)C .SC/ D e
 
SCŠQL

! LY

 (cid:22)l

(cid:22)C

1 Sl Š

lD1

Sl

;

(5.75)

f(cid:22).SjSC/ D

which is (5.43).

8 [p. 66] Formula (5.51) and the convexity of A. Suppose ˛1 and ˛2 are any
two points in A, i.e., values of ˛ having the integral in (5.51) ﬁnite. For any
value of c in the interval Œ0; 1, and any value of y, we have

0

1y C .1 (cid:0) c/e˛

0

2y (cid:21) eŒc˛1C.1(cid:0)c/˛2

ce˛

(5.76)
because of the convexity in c of the function on the right (veriﬁed by show-
ing that its second derivative is positive). Integrating both sides of (5.76)

y

0

Parametric Models

72
over X with respect to f0.x/ shows that the integral on the right must be
ﬁnite: that is, c˛1 C .1 (cid:0) c/˛2 is in A, verifying A’s convexity.
9 [p. 67] Formula (5.55). In the univariate case, differentiating both sides of

(5.51) with respect to ˛ gives

(5.77)
dividing by e .˛/ shows that P .˛/ D E˛fyg. Differentiating (5.77) again
gives

ye˛yf0.x/ dxI

P .˛/e .˛/ DZ
(cid:0) R .˛/ C P .˛/2(cid:1) e .˛/ DZ

X

y2e˛yf0.x/ dx;

(5.78)

X

or

Successive derivatives of  .˛/ yield the higher cumulants of y, its skew-
ness, kurtosis, etc.

10 [p. 67] MLE for ˇ. The gradient with respect to ˛ of log f˛.y/ (5.50) is

R .˛/ D E˛fy2g (cid:0) E˛fyg2 D var˛fyg:
y (cid:0)  .˛/(cid:1) D y (cid:0) P .˛/ D y (cid:0) E˛fy
(cid:0)˛

0

(5.79)

(cid:3)g;
r˛
(cid:3)
(cid:3) represents a hypothetical realization y.x
(5.56), where y
f˛.(cid:1)/. We achieve the MLE O˛ at rO˛ D 0, or
(cid:3)g D y:

EO˛fy

(5.81)
In other words the MLE O˛ is the value of ˛ that makes the expectation
(cid:3)g match the observed y. Thus (5.58) implies that the MLE of pa-
E˛fy
rameter ˇ is y.

/ drawn from

(5.80)

Part II

Early Computer-Age Methods

6

Empirical Bayes

The constraints of slow mechanical computation molded classical statistics
into a mathematically ingenious theory of sharply delimited scope. Emerg-
ing after the Second World War, electronic computation loosened the com-
putational stranglehold, allowing a more expansive and useful statistical
methodology.

Some revolutions start slowly. The journals of the 1950s continued to
emphasize classical themes: pure mathematical development typically cen-
tered around the normal distribution. Change came gradually, but by the
1990s a new statistical technology, computer enabled, was ﬁrmly in place.
Key developments from this period are described in the next several chap-
ters. The ideas, for the most part, would not startle a pre-war statistician,
but their computational demands, factors of 100 or 1000 times those of
classical methods, would. More factors of a thousand lay ahead, as will be
told in Part III, the story of statistics in the twenty-ﬁrst century.

Empirical Bayes methodology, this chapter’s topic, has been a particu-
larly slow developer despite an early start in the 1940s. The roadblock here
was not so much the computational demands of the theory as a lack of ap-
propriate data sets. Modern scientiﬁc equipment now provides ample grist
for the empirical Bayes mill, as will be illustrated later in the chapter, and
more dramatically in Chapters 15–21.

6.1 Robbins’ Formula

Table 6.1 shows one year of claims data for a European automobile insur-
ance company; 7840 of the 9461 policy holders made no claims during the
year, 1317 made a single claim, 239 made two claims each, etc., with Ta-
ble 6.1 continuing to the one person who made seven claims. Of course the
insurance company is concerned about the claims each policy holder will
make in the next year.

Bayes’ formula seems promising here. We suppose that xk, the number

75

76

Empirical Bayes

Table 6.1 Counts yx of number of claims x made in a single year by
9461 automobile insurance policy holders. Robbins’ formula (6.7)
estimates the number of claims expected in a succeeding year, for instance
0:168 for a customer in the x D 0 category. Parametric maximum
likelihood analysis based on a gamma prior gives less noisy estimates.

Claims x
Counts yx
Formula (6.7)
Gamma MLE

0

7840
.168
.164

1

1317
.363
.398

2

239
.527
.633

3

42
1.33
.87

4

14
1.43
1.10

5

4
6.00
1.34

6

4
1.75
1.57

7

1

of claims to be made in a single year by policy holder k, follows a Poisson
distribution with parameter k,

Prfxk D xg D pk .x/ D e

(cid:0)k  x

(6.1)
for x D 0; 1; 2; 3; : : : ; k is the expected value of xk. A good customer,
from the company’s point of view, has a small value of k, though in any
one year his or her actual number of accidents xk will vary randomly ac-
cording to probability density (6.1).

k =xŠ;

Suppose we knew the prior density g. / for the customers’  values.

Then Bayes’ rule (3.5) would yield

Efjxg D

0 p .x/g. / d
0 p .x/g. / d

(6.2)

for the expected value of  of a customer observed to make x claims in a
single year. This would answer the insurance company’s question of what
number of claims X to expect the next year from the same customer, since
Efjxg is also EfXjxg ( being the expectation of X).

Formula (6.2) is just the ticket if the prior g. / is known to the company,
but what if it is not? A clever rewriting of (6.2) provides a way forward.
Using (6.1), (6.2) becomes

R 1
R 1

Efjxg D

0

(cid:2)e
(cid:0)  xC1=xŠ(cid:3) g. / d
R 1
(cid:0)  x=xŠ(cid:3) g. / d
(cid:2)e
R 1
(cid:2)e
(cid:0)  xC1=.x C 1/Š(cid:3) g. / d
D .x C 1/R 1
(cid:0)  x=xŠ(cid:3) g. / d
(cid:2)e
R 1

0

0

0

(6.3)

:

O

f .x/ D yx=N; with N DP
f .x C 1/ı O

O
f .0/ D 7840=9461,
version of Robbins’ formula,
O

OEfjxg D .x C 1/

(6.6)
O
f .1/ D 1317=9461, etc. This yields an empirical

x yx; the total count;

The marginal density of x, integrating p .x/ over the prior g. /, is

6.1 Robbins’ Formula

f .x/ DZ 1

p .x/g. / d DZ 1

h

i

(cid:0)  x=xŠ

e

g. / d:

(6.4)

77

0

0

Comparing (6.3) with (6.4) gives Robbins’ formula,

Efjxg D .x C 1/f .x C 1/=f .x/:

(6.5)

The surprising and gratifying fact is that, even with no knowledge of the
prior density g. /, the insurance company can estimate Efjxg (6.2) from
formula (6.5). The obvious estimate of the marginal density f .x/ is the
proportion of total counts in category x,

f .x/ D .x C 1/yxC1=yx;

(6.7)
the ﬁnal expression not requiring N . Table 6.1 gives OEfj0g D 0:168:
customers who made zero claims in one year had expectation 0.168 of a
claim the next year; those with one claim had expectation 0.363, and so on.
Robbins’ formula came as a surprise1 to the statistical world of the
1950s: the expectation Efkjxkg for a single customer, unavailable without
the prior g. /, somehow becomes available in the context of a large study.
The terminology empirical Bayes is apt here: Bayesian formula (6.5) for a
single subject is estimated empirically (i.e., frequentistically) from a col-
lection of similar cases. The crucial point, and the surprise, is that large
data sets of parallel situations carry within them their own Bayesian in-
formation. Large parallel data sets are a hallmark of twenty-ﬁrst-century
scientiﬁc investigation, promoting the popularity of empirical Bayes meth-
ods.

Formula (6.7) goes awry at the right end of Table 6.1, where it is destabi-
lized by small count numbers. A parametric approach gives more depend-
able results: now we assume that the prior density g. / for the customers’
k values has a gamma form (Table 5.1)

g. / D  (cid:23)(cid:0)1e

(cid:0)=(cid:27)

for  (cid:21) 0;

(6.8)
but with parameters (cid:23) and (cid:27) unknown. Estimates .O(cid:23); O(cid:27) / are obtained by

(cid:27) (cid:23).(cid:23)/

;

1 Perhaps it shouldn’t have; estimation methods similar to (6.7) were familiar in the

actuarial literature.

78

Empirical Bayes

maximum likelihood ﬁtting to the counts yx, yielding a parametrically es-
timated marginal density

1

O
f .x/ D fO(cid:23);O(cid:27) .x/;
or equivalently Oyx D NfO(cid:23);O(cid:27) .x/.

(6.9)

Figure 6.1 Auto accident data; log(counts) vs claims for 9461
auto insurance policies. The dashed line is a gamma MLE ﬁt.

The bottom row of Table 6.1 gives parametric estimates EO(cid:23);O(cid:27)fjxg D
.x C 1/ OyxC1= Oyx, which are seen to be less eccentric for large x. Figure 6.1
compares (on the log scale) the raw counts yx with their parametric cousins
Oyx.

6.2 The Missing-Species Problem

The very ﬁrst empirical Bayes success story related to the butterﬂy data of
Table 6.2. Even in the midst of World War II Alexander Corbet, a leading
naturalist, had been trapping butterﬂies for two years in Malaysia (then
Malaya): 118 species were so rare that he had trapped only one specimen
each, 74 species had been trapped twice each, Table 6.2 going on to show
that 44 species were trapped three times each, and so on. Some of the more

012345670246810claimslog(counts)llllllll6.2 The Missing-Species Problem

79

common species had appeared hundreds of times each, but of course Corbet
was interested in the rarer specimens.

Table 6.2 Butterﬂy data; number y of species seen x times each in two
years of trapping; 118 species trapped just once, 74 trapped twice each,
etc.

x

y

x

y

1

118

13

6

2

74

14

12

3

44

15

6

4

24

16

9

5

29

17

9

6

22

18

6

7

20

19

10

8

19

20

10

9

20

21

11

10

15

22

5

11

12

23

3

12

14

24

3

Corbet then asked a seemingly impossible question: if he trapped for one
additional year, how many new species would he expect to capture? The
question relates to the absent entry in Table 6.2, x D 0, the species that
haven’t been seen yet. Do we really have any evidence at all for answering
Corbet? Fortunately he asked the right man: R. A. Fisher, who produced a
surprisingly satisfying solution for the “missing-species problem.”

Suppose there are S species in all, seen or unseen, and that xk, the num-
ber of times species k is trapped in one time unit,2 follows a Poisson dis-
tribution with parameter k as in (6.1),

xk (cid:24) Poi.k/;

for k D 1; 2; : : : ; S:

The entries in Table 6.2 are

yx D #fxk D xg;

for x D 1; 2; : : : ; 24;

(6.10)

(6.11)

the number of species trapped exactly x times each.

Now consider a further trapping period of t time units, t D 1=2 in Cor-
bet’s question, and let xk.t / be the number of times species k is trapped in
the new period. Fisher’s key assumption is that
xk.t / (cid:24) Poi.kt /

(6.12)

independently of xk. That is, any one species is trapped independently over
time3 at a rate proportional to its parameter k.

The probability that species k is not seen in the initial trapping period

2 One time unit equals two years in Corbet’s situation.
3 This is the deﬁnition of a Poisson process.

80
but is seen in the new period, that is xk D 0 and xk.t / > 0, is

Empirical Bayes

;

(cid:0)k t
1 (cid:0) e
(cid:16)
1 (cid:0) e

(cid:0)k

e

(cid:0)k

(cid:16)
E.t / D SX
Z 1
(cid:0)(cid:16)

kD1

e

(cid:0)k t
(cid:0) t

so that E.t /, the expected number of new species seen in the new trapping
period, is

(6.13)

:

(6.14)

It is convenient to write (6.14) as an integral,
1 (cid:0) e

E.t / D S

e

0

g. / d;

(6.15)

e

(cid:0) t gives

Z 1
Expanding 1 (cid:0) e
E.t / D S

where g. / is the “empirical density” putting probability 1=S on each of
the k values. (Later we will think of g. / as a continuous prior density on
the possible k values.)

(cid:0)(cid:2) t (cid:0) . t /2=2Š C . t /3=3Š (cid:0) (cid:1)(cid:1)(cid:1)(cid:3) g. / d:
ex D Efyxg D SX
h

Notice that the expected value ex of yx is the sum of the probabilities of
being seen exactly x times in the initial period,
(cid:0)k  x

Z 1

(6.16)

k =xŠ

i

e

0

kD1
(cid:0)  x=xŠ

e

D S

g. / d:

0

Comparing (6.16) with (6.17) provides a surprising result,

E.t / D e1t (cid:0) e2t 2 C e3t 3 (cid:0) (cid:1)(cid:1)(cid:1) :

We don’t know the ex values but, as in Robbins’ formula, we can esti-

mate them by the yx values, yielding an answer to Corbet’s question,

OE.t / D y1t (cid:0) y2t 2 C y3t 3 (cid:0) (cid:1)(cid:1)(cid:1) :

Corbet speciﬁed t D 1=2, so4

OE.1=2/ D 118.1=2/ (cid:0) 74.1=2/2 C 44.1=2/3 (cid:0) (cid:1)(cid:1)(cid:1)

D 45:2:

(6.17)

(6.18)

(6.19)

(6.20)

4 This may have been discouraging; there were no new trapping results reported.

6.2 The Missing-Species Problem

81

Table 6.3 Expectation (6.19) and its standard error (6.21) for the number
of new species captured in t additional fractional units of trapping time.

t

E.t /

bsd.t /

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0 11.10 20.96 29.79 37.79 45.2 52.1 58.9 65.6 71.6 75.0
8.95 11.2 13.4 15.7 17.9 20.1 22.4
0

2.24

4.48

6.71

2

3

Formulas (6.18) and (6.19) do not require the butterﬂies to arrive inde-
pendently. If we are willing to add the assumption that the xk’s are mutually
independent, we can calculate

  24X

bsd.t / D

!1=2

as an approximate standard error for OE.t /. Table 6.3 shows OE.t / andbsd.t /
for t D 0; 0:1; 0:2; : : : ; 1; in particular,

xD1

yxt 2x

(6.21)

OE.0:5/ D 45:2 ˙ 11:2:

(6.22)

Formula (6.19) becomes unstable for t > 1. This is our price for sub-
stituting the nonparametric estimates yx for ex in (6.18). Fisher actually
answered Corbet using a parametric empirical Bayes model in which the
prior g. / for the Poisson parameters k (6.12) was assumed to be of the
gamma form (6.8). It can be shown that then E.t / (6.15) is given by

E.t / D e1 f1 (cid:0) .1 C (cid:13) t /

(6.23)
where (cid:13) D (cid:27)=.1 C (cid:27) /. Taking Oe1 D y1, maximum likelihood estimation
gave

(cid:0)(cid:23)gı.(cid:13) (cid:23)/;

O(cid:23) D 0:104 and

O(cid:27) D 89:79:

(6.24)
Figure 6.2 shows that the parametric estimate of E.t / (6.23) using Oe1,
O(cid:23), and O(cid:27) is just slightly greater than the nonparametric estimate (6.19) over
the range 0  t  1. Fisher’s parametric estimate, however, gives reason-
able results for t > 1, OE.2/ D 123 for instance, for a future trapping period
of 2 units (4 years). “Reasonable” does not necessarily mean dependable.
The gamma prior is a mathematical convenience, not a fact of nature; pro-
jections into the far future fall into the category of educated guessing.

The missing-species problem encompasses more than butterﬂies. There
are 884,647 words in total in the recognized Shakespearean canon, of which
14,376 are so rare they appear just once each, 4343 appear twice each, etc.,

82

Empirical Bayes

Figure 6.2 Butterﬂy data; expected number of new species in t
units of additional trapping time. Nonparametric ﬁt (solid) ˙ 1
standard deviation; gamma model (dashed).

Table 6.4 Shakespeare’s word counts; 14,376 distinct words appeared
once each in the canon, 4343 distinct words twice each, etc. The canon
has 884,647 words in total, counting repeats.

1
0C 14376
10C
305
20C
104
30C
73
40C
49
50C
25
60C
30
70C
13
80C
13
90C
4

2

4343
259
105
47
41
19
19
12
12
7

3

2292
242
99
56
30
28
21
10
11
6

4

1463
223
112
59
35
27
18
16
8
7

5

1043
187
93
53
37
31
15
18
10
10

6

837
181
74
45
21
19
10
11
11
10

7

638
179
83
34
41
19
15
8
7
15

8

519
130
76
49
30
22
14
15
12
7

9

430
127
72
45
28
23
11
12
9
7

10

364
128
63
52
19
14
16
7
8
5

as in Table 6.4, which goes on to the ﬁve words appearing 100 times each.
All told, 31,534 distinct words appear (including those that appear more
than 100 times each), this being the observed size of Shakespeare’s vocab-
ulary. But what of the words Shakespeare knew but didn’t use? These are
the “missing species” in Table 6.4.

0.00.20.40.60.81.0020406080time tE^(t) Gamma model  E^(2) = 123E^(4) = 176E^(8) = 2336.2 The Missing-Species Problem

83

Suppose another quantity of previously unknown Shakespeare manu-
scripts was discovered, comprising 884647 (cid:1) t words (so t D 1 would rep-
resent a new canon just as large as the old one). How many previously
unseen distinct words would we expect to discover?

Employing formulas (6.19) and (6.21) gives

11430 ˙ 178

(6.25)
for the expected number of distinct new words if t D 1. This is a very con-
servative lower bound on how many words Shakespeare knew but didn’t
use. We can imagine t rising toward inﬁnity, revealing ever more unseen
vocabulary. Formula (6.19) fails for t > 1, and Fisher’s gamma assump-
tion is just that, but more elaborate empirical Bayes calculations give a ﬁrm
lower bound of 35; 000C on Shakespeare’s unseen vocabulary, exceeding
the visible portion!

Missing mass is an easier version of the missing-species problem, in
which we only ask for the proportion of the total sum of k values corre-
sponding to the species that went unseen in the original trapping period,

The numerator has expectation

(cid:30)X
M D X
Z 1

unseen

k

all

(cid:0)k D S

0

k:

(cid:0) g. / D e1

e

(X

)

ke

X
k DX

all

X

(6.26)

(6.27)

(6.29)

(6.30)

as in (6.17), while the expectation of the denominator is

Efxsg D E

D EfNg;

(6.28)

xs

all

all

all

where N is the total number of butterﬂies trapped. The obvious missing-
mass estimate is then

OM D y1=N:

For the Shakespeare data,

OM D 14376=884647 D 0:016:

We have seen most of Shakespeare’s vocabulary, as weighted by his usage,
though not by his vocabulary count.

All of this seems to live in the rareﬁed world of mathematical abstrac-
tion, but in fact some previously unknown Shakespearean work might have

84

Empirical Bayes

been discovered in 1985. A short poem, “Shall I die?,” was found in the
archives of the Bodleian Library and, controversially, attributed to Shake-
speare by some but not all experts.

The poem of 429 words provided a new “trapping period” of length only

t D 429=884647 D 4:85 (cid:1) 10

(cid:0)4;

(6.31)

(6.32)

and a prediction from (6.19) of

Eftg D 6:97

new “species,” i.e., distinct words not appearing in the canon. In fact there
were nine such words in the poem. Similar empirical Bayes predictions
for the number of words appearing once each in the canon, twice each,
etc., showed reasonable agreement with the poem’s counts, but not enough
to stiﬂe doubters. “Shall I die?” is currently grouped with other canonical
apocrypha by a majority of experts.

6.3 A Medical Example

The reader may have noticed that our examples so far have not been par-
ticularly computer intensive; all of the calculations could have been (and
originally were) done by hand.5 This section discusses a medical study
where the empirical Bayes analysis is more elaborate.
Cancer surgery sometimes involves the removal of surrounding lymph
nodes as well as the primary target at the site. Figure 6.3 concerns N D 844
surgeries, each reporting
n D # nodes removed

and x D # nodes found positive;

(6.33)

“positive” meaning malignant. The ratios

pk D xk=nk;

k D 1; 2; : : : ; N;

(6.34)

are described in the histogram. A large proportion of them, 340=844 or
40%, were zero, the remainder spreading unevenly between zero and one.
The denominators nk ranged from 1 to 69, with a mean of 19 and standard
deviation of 11.

We suppose that each patient has some true probability of a node being

5 Not so collecting the data. Corbet’s work was pre-computer but Shakespeare’s word

counts were done electronically. Twenty-ﬁrst-century scientiﬁc technology excels at the
production of the large parallel-structured data sets conducive to empirical Bayes
analysis.

6.3 A Medical Example

85

Figure 6.3 Nodes study; ratio p D x=n for 844 patients; n D
number of nodes removed, x D number positive.

positive, say probability k for patient k, and that his or her nodal results
occur independently of each other, making xk binomial,

xk (cid:24) Bi.nk; k/:
This gives pk D xk=nk with mean and variance

pk (cid:24) .k; k.1 (cid:0) k/=nk/ ;

(6.35)

(6.36)

so that k is estimated more accurately when nk is large.

A Bayesian analysis would begin with the assumption of a prior density

g. / for the k values,

k (cid:24) g. /;

for k D 1; 2; : : : ; N D 844:

(6.37)

We don’t know g. /, but the parallel nature of the nodes data set—844
similar cases—suggests an empirical Bayes approach. As a ﬁrst try for the
nodes study, we assume that logfg. /g is a fourth-degree polynomial in ,

logfg˛. /g D a0 C 4X

jD1

˛j  jI

(6.38)

 p = x/nFrequency0.00.20.40.60.81.0020406080100*34086
g˛. / is determined by the parameter vector ˛ D .˛1; ˛2; ˛3; ˛4/ since,
given ˛, a0 can be calculated from the requirement that

Empirical Bayes

Z 1

0

)

(
a0 C 4X

g˛. / d D 1 DZ 1
 
!
f˛.xk/ DZ 1
 xk .1 (cid:0)  /nk(cid:0)xk g˛. / d:

˛j  j

exp

d:

nk
xk

0

0

1

For a given choice of ˛, let f˛.xk/ be the marginal probability of the

observed value xk for patient k,

(6.39)

(6.40)

The maximum likelihood estimate of ˛ is the maximizer

( NX

kD1

O˛ D arg max

˛

)

log f˛.xk/

:

(6.41)

Figure 6.4 Estimated prior density g. / for the nodes study;
59% of patients have   0:2, 7% have  (cid:21) 0:8.

Figure 6.4 graphs gO˛. /, the empirical Bayes estimate for the prior dis-
tribution of the k values. The huge spike at zero in Figure 6.3 is now
reduced: Prfk  0:01g D 0:12 compared with the 38% of the pk values

0.00.20.40.60.81.00.000.020.040.060.080.100.12qg^(q)– sd6.3 A Medical Example

87

Z 1:00

gO˛. / d D 0:59 compared with

less than 0.01. Small  values are still the rule though, for instance

Z 0:20
gO˛. / d D 0:07: (6.42)
The vertical bars in Figure 6.4 indicate ˙ one standard error for the es-
timation of g. /. The curve seems to have been estimated very accurately,
at least if we assume the adequacy of model (6.37). Chapter 21 describes
the computations involved in Figure 6.4.

0:80

0

The posterior distribution of k given xk and nk is estimated according

to Bayes’ rule (3.5) to be

 
Og.jxk; nk/ D gO˛. /

!
 xk .1 (cid:0)  /nk(cid:0)xk

(cid:30)

fO˛.xk/;

(6.43)

nk
xk

with fO˛.xk/ from (6.40).

Figure 6.5 Empirical Bayes posterior densities of  for three
patients, given x D number of positive nodes, n D number of
nodes.

Figure 6.5 graphs Og.jxk; nk/ for three choices of .xk; nk/: .7; 32/, .3; 6/,
and .17; 18/. If we take  (cid:21) 0:50 as indicating poor prognosis (and sug-
gesting more aggressive follow-up therapy), then the ﬁrst patient is almost
surely on safe ground, the third patient almost surely needs more follow-up
therapy and the situation of the second is uncertain.

0.00.20.40.60.81.00246qg(q | x, n)x=7 n=32x=17 n=18x=3 n=60.588

Empirical Bayes

6.4 Indirect Evidence 1

A good deﬁnition of a statistical argument is one in which many small
pieces of evidence, often contradictory, are combined to produce an overall
conclusion. In the clinical trial of a new drug, for instance, we don’t expect
the drug to cure every patient, or the placebo to always fail, but eventually
perhaps we will obtain convincing evidence of the new drug’s efﬁcacy.

The clinical trial is collecting direct statistical evidence, in which each
subject’s success or failure bears directly upon the question of interest. Di-
rect evidence, interpreted by frequentist methods, was the dominant mode
of statistical application in the twentieth century, being strongly connected
to the idea of scientiﬁc objectivity.

Bayesian inference provides a theoretical basis for incorporating indi-
rect evidence, for example the doctor’s prior experience with twin sexes in
Section 3.1. The assertion of a prior density g. / amounts to a claim for
the relevance of past data to the case at hand.

Empirical Bayes removes the Bayes scaffolding. In place of a reassuring
prior g. /, the statistician must put his or her faith in the relevance of the
“other” cases in a large data set to the case of direct interest. For the second
patient in Figure 6.5, the direct estimate of his  value is O
 D 3=6 D 0:50.
The empirical Bayes estimate is a little less,

 EB DZ 1

O

 Og.jxk D 3; nk D 6/ D 0:446:

(6.44)

0

A small difference, but we will see bigger ones in succeeding chapters.

The changes in twenty-ﬁrst-century statistics have largely been demand
driven, responding to the massive data sets enabled by modern scientiﬁc
equipment. Philosophically, as opposed to methodologically, the biggest
change has been the increased acceptance of indirect evidence, especially
as seen in empirical Bayes and objective (“uninformative”) Bayes appli-
cations. False-discovery rates, Chapter 15, provide a particularly striking
shift from direct to indirect evidence in hypothesis testing. Indirect evi-
dence in estimation is the subject of our next chapter.

6.5 Notes and Details

Robbins (1956) introduced the term “empirical Bayes” as well as rule (6.7)
as part of a general theory of empirical Bayes estimation. 1956 was also the
publication year for Good and Toulmin’s solution (6.19) to the missing-
species problem. Good went out of his way to credit his famous Bletchley

6.5 Notes and Details

89

colleague Alan Turing for some of the ideas. The auto accident data is taken
from Table 3.1 of Carlin and Louis (1996), who provide a more complete
discussion. Empirical Bayes estimates such as 11430 in (6.25) do not de-
pend on independence among the “species,” but accuracies such as ˙178
do; and similarly for the error bars in Figures 6.2 and 6.4.

Corbet’s enormous efforts illustrate the difﬁculties of amassing large
data sets in pre-computer times. Dependable data is still hard to come by,
but these days it is often the statistician’s job to pry it out of enormous
databases. Efron and Thisted (1976) apply formula (6.19) to the Shake-
speare word counts, and then use linear programming methods to bound
Shakespeare’s unseen vocabulary from below at 35,000 words. (Shake-
speare was actually less “wordy” than his contemporaries, Marlow and
Donne.) “Shall I die,” the possibly Shakespearean poem recovered in 1985,
is analyzed by a variety of empirical Bayes techniques in Thisted and Efron
(1987). Comparisons are made with other Elizabethan authors, none of
whom seem likely candidates for authorship.

The Shakespeare word counts are from Spevack’s (1968) concordance.
(The ﬁrst concordance was compiled by hand in the mid 1800s, listing
every word Shakespeare wrote and where it appeared, a full life’s labor.)

The nodes example, Figure 6.3, is taken from Gholami et al. (2015).

1 [p. 78] Formula (6.9). For any positive numbers c and d we have

so combining gamma prior (6.8) with Poisson density (6.1) gives marginal
density

Z 1

0

 c(cid:0)1e

f(cid:23);(cid:27) .x/ D

(cid:0)=d d D d c.c/;
R 1
0  (cid:23)Cx(cid:0)1e
D (cid:13) (cid:23)Cx.(cid:23) C x/

(cid:0)=(cid:13) d

(cid:27) (cid:23).(cid:23)/xŠ

;

(cid:27) (cid:23).(cid:23)/xŠ

(6.45)

(6.46)

where (cid:13) D (cid:27)=.1 C (cid:27) /. Assuming independence among the counts yx
(which is exactly true if the customers act independently of each other and
N , the total number of them, is itself Poisson), the log likelihood function
for the accident data is

yx logff(cid:23);(cid:27) .x/g :

(6.47)

xmaxX

xD0

Here xmax is some notional upper bound on the maximum possible number

Empirical Bayes

90
of accidents for a single customer; since yx D 0 for x > 7 the choice of
xmax is irrelevant. The values .O(cid:23); O(cid:27) / in (6.8) maximize (6.47).

2 [p. 81] Formula (6.21). If N DP yx, the total number trapped, is assumed

to be Poisson, and if the N observed values xk are mutually independent,
then a useful property of the Poisson distribution implies that the counts yx
are themselves approximately independent Poisson variates
for x D 0; 1; 2; : : : ;

ind(cid:24) Poi.ex/;

(6.48)

yx

in notation (6.17). Formula (6.19) and varfyxg D ex then give

ext 2x:

(6.49)

n OE.t /

o DX

x(cid:21)1

var

Substituting yx for ex produces (6.21). Section 11.5 of Efron (2010) shows
that (6.49) is an upper bound on varf OE.t /g if N is considered ﬁxed rather
than Poisson.
3 [p. 81] Formula (6.23). Combining the case x D 1 in (6.17) with (6.15)

(cid:2)R 1

0 e

(cid:0) g. / d (cid:0)R 1
R 1

0 e

0 e

(cid:0) g. / d

(cid:0).1Ct /g. / d(cid:3)

:

(6.50)

yields

E.t / D e1

Substituting the gamma prior (6.8) for g. /, and using (6.45) three times,
gives formula (6.23).

7

James–Stein Estimation and Ridge

Regression

If Fisher had lived in the era of “apps,” maximum likelihood estimation
might have made him a billionaire. Arguably the twentieth century’s most
inﬂuential piece of applied mathematics, maximum likelihood continues to
be a prime method of choice in the statistician’s toolkit. Roughly speaking,
maximum likelihood provides nearly unbiased estimates of nearly mini-
mum variance, and does so in an automatic way.

That being said, maximum likelihood estimation has shown itself to be
an inadequate and dangerous tool in many twenty-ﬁrst-century applica-
tions. Again speaking roughly, unbiasedness can be an unaffordable luxury
when there are hundreds or thousands of parameters to estimate at the same
time.

The James–Stein estimator made this point dramatically in 1961, and
made it in the context of just a few unknown parameters, not hundreds or
thousands. It begins the story of shrinkage estimation, in which deliberate
biases are introduced to improve overall performance, at a possible danger
to individual estimates. Chapters 7 and 21 will carry on the story in its
modern implementations.

7.1 The James–Stein Estimator

Suppose we wish to estimate a single parameter (cid:22) from observation x in
the Bayesian situation
(cid:22) (cid:24)

and xj(cid:22) (cid:24)

(7.1)

N .M; A/

N .(cid:22); 1/;

in which case (cid:22) has posterior distribution

(cid:22)jx (cid:24)

N .M C B.x (cid:0) M /; B/

(7.2)
as given in (5.21) (where we take (cid:27) 2 D 1 for convenience). The Bayes
estimator of (cid:22),

ŒB D A=.A C 1/

O(cid:22)Bayes D M C B.x (cid:0) M /;

(7.3)

91

(7.4)

92

James–Stein Estimation and Ridge Regression

has expected squared error

E

compared with 1 for the MLE O(cid:22)MLE D x,

n(cid:0) O(cid:22)Bayes (cid:0) (cid:22)(cid:1)2o D B;
n(cid:0) O(cid:22)MLE (cid:0) (cid:22)(cid:1)2o D 1:

(7.5)
If, say, A D 1 in (7.1) then B D 1=2 and O(cid:22)Bayes has only half the risk of
the MLE.

E

The same calculation applies to a situation where we have N indepen-

dent versions of (7.1), say

(cid:22) D .(cid:22)1; (cid:22)2; : : : ; (cid:22)N /

0

and x D .x1; x2; : : : ; xN /

0

;

(7.6)

with

(cid:22)i (cid:24)

and xij(cid:22)i (cid:24)

N .M; A/

(7.7)
independently for i D 1; 2; : : : ; N . (Notice that the (cid:22)i differ from each
other, and that this situation is not the same as (5.22)–(5.23).) Let O(cid:22)Bayes
indicate the vector of individual Bayes estimates O(cid:22)
D M CB.xi(cid:0)M /,
(7.8)

(cid:2)M D .M; M; : : : ; M /
0(cid:3) ;

O(cid:22)Bayes D M C B.x (cid:0) M /;

N .(cid:22)i ; 1/;

Bayes
i

and similarly

O(cid:22)MLE D x:

2
Using (7.4) the total squared error risk of O(cid:22)Bayes is
(cid:0) (cid:22)i

n(cid:13)(cid:13) O(cid:22)Bayes (cid:0) (cid:22)

( NX
(cid:16) O(cid:22)
(cid:13)(cid:13)2o D E
(cid:13)(cid:13)2o D N:
n(cid:13)(cid:13) O(cid:22)MLE (cid:0) (cid:22)

compared with

Bayes
i

iD1

E

E

)

D N (cid:1) B

(7.9)

(7.10)

Again, O(cid:22)Bayes has only B times the risk of O(cid:22)MLE.
This is ﬁne if we know M and A (or equivalently M and B) in (7.1). If
not, we might try to estimate them from x D .x1; x2; : : : ; xN /. Marginally,
(7.7) gives

Then OM D Nx is an unbiased estimate of M . Moreover,

ind(cid:24)

xi

N .M; A C 1/:
"
S D NX

iD1

#

(7.11)

(7.12)

.xi (cid:0) Nx/2

OB D 1 (cid:0) .N (cid:0) 3/=S

7.1 The James–Stein Estimator

93

unbiasedly estimates B, as long as N > 3.  The James–Stein estimator is 1
the plug-in version of (7.3),

(cid:16)
xi (cid:0) OM



2

O(cid:22)JS

D OM C OB

for i D 1; 2; : : : ; N;

i

(7.13)
or equivalently O(cid:22)JS D OM C OB.x (cid:0) OM /, with OM D . OM ; OM ; : : : ; OM /
0.
At this point the terminology “empirical Bayes” seems especially apt:
Bayesian model (7.7) leads to the Bayes estimator (7.8), which itself is
estimated empirically (i.e., frequentistically) from all the data x, and then
applied to the individual cases. Of course O(cid:22)JS cannot perform as well as
the actual Bayes’ rule O(cid:22)Bayes, but the increased risk is surprisingly modest.
The expected squared risk of O(cid:22)JS under model (7.7) is

n(cid:13)(cid:13) O(cid:22)JS (cid:0) (cid:22)

(cid:13)(cid:13)2o D NB C 3.1 (cid:0) B/:

E

(7.14)
If, say, N D 20 and A D 1, then (7.14) equals 11.5, compared with true
Bayes risk 10 from (7.9), much less than risk 20 for O(cid:22)MLE.

A defender of maximum likelihood might respond that none of this is
surprising: Bayesian model (7.7) speciﬁes the parameters (cid:22)i to be clustered
more or less closely around a central point M , while O(cid:22)MLE makes no such
assumption, and cannot be expected to perform as well. Wrong! Removing
the Bayesian assumptions does not rescue O(cid:22)MLE, as James and Stein proved
in 1961:
James–Stein Theorem Suppose that

xij(cid:22)i (cid:24)

(7.15)

independently for i D 1; 2; : : : ; N , with N (cid:21) 4. Then

(cid:13)(cid:13)2o
n(cid:13)(cid:13) O(cid:22)JS (cid:0) (cid:22)

N .(cid:22)i ; 1/

n(cid:13)(cid:13) O(cid:22)MLE (cid:0) (cid:22)

(cid:13)(cid:13)2o

< N D E

E

(7.16)
for all choices of (cid:22) 2
RN . (The expectations in (7.16) are with (cid:22) ﬁxed
and x varying according to (7.15).)
In the language of decision theory, equation (7.16) says that O(cid:22)MLE is
inadmissible:  its total squared error risk exceeds that of O(cid:22)JS no matter 3
what (cid:22) may be. This is a strong frequentist form of defeat for O(cid:22)MLE, not
depending on Bayesian assumptions.

The James–Stein theorem came as a rude shock to the statistical world
of 1961. First of all, the defeat came on MLE’s home ﬁeld: normal observa-
tions with squared error loss. Fisher’s “logic of inductive inference,” Chap-
ter 4, claimed that O(cid:22)MLE D x was the obviously correct estimator in the uni-
variate case, an assumption tacitly carried forward to multiparameter linear

James–Stein Estimation and Ridge Regression

94
regression problems, where versions of O(cid:22)MLE were predominant. There are
still some good reasons for sticking with O(cid:22)MLE in low-dimensional prob-
lems, as discussed in Section 7.4. But shrinkage estimation, as exempliﬁed
by the James–Stein rule, has become a necessity in the high-dimensional
situations of modern practice.

7.2 The Baseball Players

The James–Stein theorem doesn’t say by how much O(cid:22)JS beats O(cid:22)MLE. If the
improvement were inﬁnitesimal nobody except theorists would be inter-
ested. In favorable situations the gains can in fact be substantial, as sug-
gested by (7.14). One such situation appears in Table 7.1. The batting av-
erages1 of 18 Major League players have been observed over the 1970 sea-
son. The column labeled MLE reports the player’s observed average over
his ﬁrst 90 at bats; TRUTH is the average over the remainder of the 1970
season (370 further at bats on average). We would like to predict TRUTH
from the early-season observations.

The column labeled JS in Table 7.1 is from a version of the James–
Stein estimator applied to the 18 MLE numbers. We suppose that each
player’s MLE value pi (his batting average in the ﬁrst 90 tries) is a binomial
proportion,

pi (cid:24) Bi.90; Pi /=90:

(7.17)
Here Pi is his true average, how he would perform over an inﬁnite number
of tries; TRUTHi is itself a binomial proportion, taken over an average of
370 more tries per player.

At this point there are two ways to proceed. The simplest uses a normal

approximation to (7.17),

pi P(cid:24)
0 is the binomial variance

where (cid:27) 2

N .Pi ; (cid:27) 2
0 /;

(7.18)

D Np C

OpJS

D Np.1 (cid:0) Np/=90;
(cid:21)
D (cid:27)0 O(cid:22)JS
P.pi (cid:0) Np/2
1 (cid:0) .N (cid:0) 3/(cid:27) 2

0

i

(7.19)
with Np D 0:254 the average of the pi values. Letting xi D pi =(cid:27)0, applying
(7.13), and transforming back to OpJS
i , gives James–Stein estimates

(cid:27) 2
0

i

(7.20)
1 Batting average D # hits =# at bats, that is, the success rate. For example, Player 1 hits
successfully 31 times in his ﬁrst 90 tries, for batting average 31=90 D 0:345. This data
is based on 1970 Major League performances, but is partly artiﬁcial; see the endnotes.

.pi (cid:0) Np/:

7.2 The Baseball Players

95

Table 7.1 Eighteen baseball players; MLE is batting average in ﬁrst 90 at
bats; TRUTH is average in remainder of 1970 season; James–Stein
estimator JS is based on arcsin transformation of MLEs. Sum of squared
errors for predicting TRUTH: MLE .0425, JS .0218.

Player

MLE

JS

TRUTH

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

.345
.333
.322
.311
.289
.289
.278
.255
.244
.233
.233
.222
.222
.222
.211
.211
.200
.145

.283
.279
.276
.272
.265
.264
.261
.253
.249
.245
.245
.242
.241
.241
.238
.238
.234
.212

.298
.346
.222
.276
.263
.273
.303
.270
.230
.264
.264
.210
.256
.269
.316
.226
.285
.200

x

11.96
11.74
11.51
11.29
10.83
10.83
10.60
10.13
9.88
9.64
9.64
9.40
9.39
9.39
9.14
9.14
8.88
7.50

" npi C 0:375

1=2#

A second approach begins with the arcsin transformation

xi D 2.n C 0:5/1=2 sin

(cid:0)1

(7.21)
n D 90 (column labeled x in Table 7.1), a classical device that produces
approximate normal deviates of variance 1,

n C 0:75

;

xi P(cid:24)

N .(cid:22)i ; 1/;

(7.22)

where (cid:22)i is transformation (7.21) applied to TRUTHi. Using (7.13) gives
O(cid:22)JS
i , which is ﬁnally inverted back to the binomial scale,

"

OpJS

i

D 1
n

n C 0:75
n C 0:5

sin O(cid:22)JS

i

#
2 (cid:0) 0:375

2

:

(7.23)

Formulas (7.20) and (7.23) yielded nearly the same estimates for the
baseball players; the JS column in Table 7.1 is from (7.23). James and
Stein’s theorem requires normality, but the James–Stein estimator often

96

James–Stein Estimation and Ridge Regression

18X
.MLEi(cid:0)TRUTHi /2 D 0:0425 while

18X
works perfectly well in less ideal situations. That is the case in Table 7.1:
.JSi(cid:0)TRUTHi /2 D 0:0218:
(7.24)
In other words, the James–Stein estimator reduced total predictive squared
error by about 50%.

iD1

iD1

Figure 7.1 Eighteen baseball players; top line MLE, middle
James–Stein, bottom true values. Only 13 points are visible, since
there are ties.

The James–Stein rule describes a shrinkage estimator, each MLE value
xi being shrunk by factor OB toward the grand mean OM D Nx (7.13). ( OB D
0:34 in (7.20).) Figure 7.1 illustrates the shrinking process for the baseball
players.
To see why shrinking might make sense, let us return to the original
Bayes model (7.8) and take M D 0 for simplicity, so that the xi are
marginally N .0; A C 1/ (7.11). Even though each xi is unbiased for its
( NX
D NA: (7.25)

parameter (cid:22)i, as a group they are “overdispersed,”

D N.A C 1/

compared with E

)

)

E

x2
i

iD1

( NX

iD1

(cid:22)2
i

The sum of squares of the MLEs exceeds that of the true values by expected
amount N ; shrinkage improves group estimation by removing the excess.

0.150.200.250.300.35 llllllllllllllllllMLEllllllllllllllllllJAMES−STEINllllllllllllllllllTRUEBatting averages7.3 Ridge Regression

97

Bayes
i

Bayes
i

( NX

iD1

E

A

A C 1

;

D Bxi have

D NB 2.A C 1/ D NA

In fact the James–Stein rule overshrinks the data, as seen in the bottom
two lines of Figure 7.1, a property it inherits from the underlying Bayes
)
model: the Bayes estimates O(cid:22)
2
(7.26)
i / D NA by factor A=.A C 1/. We could use the
Bxi, which gives the correct expected
sum of squares NA, but a larger expected sum of squared estimation errors

(cid:16) O(cid:22)
overshrinking E.P (cid:22)2
less extreme shrinking rule Q(cid:22)i D p
EfP. Q(cid:22)i (cid:0) (cid:22)i /2jxg.
pothesis of no differences among the (cid:22)i values. (This gaveP.Pi (cid:0) Np/2 D

NULL indicating that in a classical sense we have accepted the null hy-

The most extreme shrinkage rule would be “all the way,” that is, to

for i D 1; 2; : : : ; N;

O(cid:22)NULL

i

D Nx

(7.27)

0:0266 for the baseball data (7.24).) The James–Stein estimator is a data-
based rule for compromising between the null hypothesis of no differences
and the MLE’s tacit assumption of no relationship at all among the (cid:22)i
values. In this sense it blurs the classical distinction between hypothesis
testing and estimation.

7.3 Ridge Regression

Linear regression, perhaps the most widely used estimation technique, is
based on a version of O(cid:22)MLE. In the usual notation, we observe an n-dimen-
sional vector y D .y1; y2; : : : ; yn/

0 from the linear model

y D X ˇ C (cid:15):

(7.28)
Here X is a known n(cid:2) p structure matrix, ˇ is an unknown p-dimensional
parameter vector, while the noise vector (cid:15) D .(cid:15)1; (cid:15)2; : : : ; (cid:15)n/
0 has its com-
ponents uncorrelated and with constant variance (cid:27) 2,

(cid:15) (cid:24) .0; (cid:27) 2I/;

(7.29)
where I is the n (cid:2) n identity matrix. Often (cid:15) is assumed to be multivariate
normal,

(cid:15) (cid:24)

Nn.0; (cid:27) 2I/;

(7.30)

but that is not required for most of what follows.

James–Stein Estimation and Ridge Regression

98
The least squares estimate O
early 1800s, is the minimizer of the total sum of squared errors,

ˇ, going back to Gauss and Legendre in the

˚ky (cid:0) X ˇk2(cid:9) :

O
ˇ D arg min

ˇ

It is given by

O
ˇ D S

(cid:0)1X
0
where S is the p (cid:2) p inner product matrix
XI

S D X

0

y;

O
ˇ is unbiased for ˇ and has covariance matrix (cid:27) 2S

(cid:0)1,

ˇ (cid:24)(cid:0)ˇ; (cid:27) 2S

O

(cid:0)1(cid:1) :

(7.31)

(7.32)

(7.33)

(7.34)

4

ˇ is the MLE of ˇ. Before 1950 a great deal
(cid:0)1 could be feasibly

In the normal case (7.30) O
of effort went into designing matrices X such that S
calculated, which is now no longer a concern.
A great advantage of the linear model is that it reduces the number of
unknown parameters to p (or p C 1 including (cid:27) 2), no matter how large n
may be. In the kidney data example of Section 1.1, n D 157 while p D 2.
In modern applications, however, p has grown larger and larger, sometimes
into the thousands or more, as we will see in Part III, causing statisticians
again to confront the limitations of high-dimensional unbiased estimation.
Ridge regression is a shrinkage method designed to improve the estima-
tion of ˇ in linear models. By transformations  we can standardize (7.28)
so that the columns of X each have mean 0 and sum of squares 1, that is,

Si i D 1

for i D 1; 2; : : : ; p:

(7.35)

(This puts the regression coefﬁcients ˇ1; ˇ2; : : : ; ˇp on comparable scales.)
For convenience, we also assume Ny D 0. A ridge regression estimate O
ˇ.(cid:21)/
is deﬁned, for (cid:21) (cid:21) 0, to be

y D .S C (cid:21)I/

(cid:0)1X
ˇ while O

O
ˇ.(cid:21)/ D .S C (cid:21)I/
(using (7.32)); O
ˇ.(cid:21)/ is a shrunken version of O
ˇ, the bigger (cid:21) the more
ˇ.0/ D O
extreme the shrinkage: O
ˇ.1/ equals the vector of zeros.
Ridge regression effects can be quite dramatic. As an example, con-
sider the diabetes data, partially shown in Table 7.2, in which 10 prediction
variables measured at baseline—age, sex, bmi (body mass index), map
(mean arterial blood pressure), and six blood serum measurements—have

(cid:0)1S

(7.36)

O
ˇ

0

7.3 Ridge Regression

99

Table 7.2 First 7 of n D 442 patients in the diabetes study; we wish to
predict disease progression at one year “prog” from the 10 baseline
measurements age, sex, . . . , glu.

age

sex

bmi

map

59
48
72
24
50
23
36
:::

1
0
1
0
0
0
1
:::

32.1
21.6
30.5
25.3
23.0
22.6
22.0
:::

101
87
93
84
101
89
90
:::

tc

157
183
156
198
192
139
160
:::

ldl

hdl

tch

ltg

glu

prog

93.2
103.2
93.6
131.4
125.4
64.8
99.6
:::

38
70
41
40
52
61
50
:::

4
3
4
5
4
2
3
:::

2.11
1.69
2.03
2.12
1.86
1.82
1.72
:::

87
69
85
89
80
68
82
:::

151
75
141
206
135
97
138
:::

been obtained for n D 442 patients. We wish to use the 10 variables to pre-
dict prog, a quantitative assessment of disease progression one year after
baseline. In this case X is the 442 (cid:2) 10 matrix of standardized predictor
variables, and y is prog with its mean subtracted off.

Figure 7.2 Ridge coefﬁcient trace for the standardized diabetes
data.

−5000500lb^(l)0.000.050.150.200.250.1llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllagesexbmimaptcldlhdltchltggluJames–Stein Estimation and Ridge Regression

100
Table 7.3 Ordinary least squares estimate O
ˇ.0/ compared with ridge
regression estimate O
ˇ.0:1/ with (cid:21) D 0:1. The columns sd(0) and sd(0.1)
are their estimated standard errors. (Here (cid:27) was taken to be 54.1, the
usual OLS estimate based on model (7.28).)

O
ˇ.0:1/

sd(0)

sd(0.1)

O
ˇ.0/
(cid:0)10.0

age
1.3
sex (cid:0)239.8 (cid:0)207.2
bmi
489.7
519.8
map
301.8
324.4
(cid:0)792.2
(cid:0)83.5
tc
(cid:0)70.8
ldl
476.7
101.0 (cid:0)188.7
hdl
tch
115.7
177.1
ltg
443.8
751.3
glu
67.6
86.7

59.7
61.2
66.5
65.3
416.2
338.6
212.3
161.3
171.7
65.9

52.7
53.2
56.3
55.7
43.6
52.4
58.4
70.8
58.4
56.6

Figure 7.2 vertically plots the 10 coordinates of O
ˇ.(cid:21)/ as the ridge pa-
rameter (cid:21) increases from 0 to 0.25. Four of the coefﬁcients change rapidly
at ﬁrst. Table 7.3 compares O
ˇ.0:1/.
Positive coefﬁcients predict increased disease progression. Notice that ldl,
the “bad cholesterol” measurement, goes from being a strongly positive
predictor in O
There is a Bayesian rationale for ridge regression. Assume that the noise

ˇ.0/, that is the usual estimate O

ˇ, with O

ˇ.0:1/.

vector (cid:15) is normal as in (7.30), so that

(7.37)

(7.38)

rather than just (7.34). Then the Bayesian prior

ˇ to a mildly negative one in O
(cid:0)ˇ; (cid:27) 2S


O
ˇ (cid:24)

Np

(cid:0)1(cid:1)


ˇ (cid:24)

(cid:27) 2
(cid:21)

I

Np

0;

n
ˇj O

o D .S C (cid:21)I/

(cid:0)1S

O
ˇ;

makes

E

ˇ

(7.39)
ˇ.(cid:21)/ (using (5.23) with M D 0,
(cid:0)1). Ridge regression amounts to an

the same as the ridge regression estimate O
A D .(cid:27) 2=(cid:21)/I, and † D .S =(cid:27) 2/
increased prior belief that ˇ lies near 0.
The last two columns of Table 7.3 compare the standard deviations  of
ˇ and O
O
ˇ.0:1/. Ridging has greatly reduced the variability of the estimated

5

7.3 Ridge Regression

101

O(cid:22).(cid:21)/ D X

O
ˇ.(cid:21)/;

regression coefﬁcients. This does not guarantee that the corresponding es-
timate of (cid:22) D X ˇ,

(7.40)
O
will be more accurate than the ordinary least squares estimate O(cid:22) D X
ˇ.
We have (deliberately) introduced bias, and the squared bias term coun-
teracts some of the advantage of reduced variability. The Cp calculations
of Chapter 12 suggest that the two effects nearly offset each other for the
diabetes data. However, if interest centers on the coefﬁcients of ˇ, then
ridging can be crucial, as Table 7.3 emphasizes.
By current standards, p D 10 is a small number of predictors. Data sets
with p in the thousands, and more, will show up in Part III. In such situa-
tions the scientist is often looking for a few interesting predictor variables
hidden in a sea of uninteresting ones: the prior belief is that most of the ˇi
values lie near zero. Biasing the maximum likelihood estimates O
ˇi toward
zero then becomes a necessity.
O
ˇ.(cid:21)/:

There is still another way to motivate the ridge regression estimator

O
ˇ.(cid:21)/ D arg min

fky (cid:0) X ˇk2 C (cid:21)kˇk2g:

(cid:0)1X

0

ˇ

(7.41)
Differentiating the term in brackets with respect to ˇ shows that O
ˇ.(cid:21)/ D
.S C (cid:21)I/
y as in (7.36). If (cid:21) D 0 then (7.41) describes the ordinary
least squares algorithm; (cid:21) > 0 penalizes choices of ˇ having kˇk large,
biasing O
Various terminologies are used to describe algorithms such as (7.41): pe-
nalized least squares; penalized likelihood; maximized a-posteriori proba-
bility (MAP);and, generically, regularization describes almost any method 6
that tamps down statistical variability in high-dimensional estimation or
prediction problems.

ˇ.(cid:21)/ toward the origin.

A wide variety of penalty terms are in current use, the most inﬂuential

one involving the “`1 norm” kˇk1 DPp

jˇjj,

1

Q
ˇ.(cid:21)/ D arg min

fky (cid:0) X ˇk2 C (cid:21)kˇk1g;

(7.42)

ˇ

the so-called lasso estimator, Chapter 16. Despite the Bayesian provenance,
most regularization research is carried out frequentistically, with various
penalty terms investigated for their probabilistic behavior regarding esti-
mation, prediction, and variable selection.
If we apply the James–Stein rule to the normal model (7.37), we get a
different shrinkage rule for O

ˇ, say Q
ˇJS,

7

102

James–Stein Estimation and Ridge Regression

# O

"

Q
ˇJS D

S

O
ˇ

1 (cid:0) .p (cid:0) 2/(cid:27) 2
O
ˇ
(cid:13)(cid:13)2o
n(cid:13)(cid:13) Q(cid:22)JS (cid:0) (cid:22)

(7.43)
Q
ˇJS be the corresponding estimator of (cid:22) D Efyg in

ˇ:

0

Letting Q(cid:22)JS D X
(7.28), the James–Stein Theorem guarantees that

E

< p(cid:27) 2

(7.44)
no matter what ˇ is, as long as p (cid:21) 3.2 There is no such guarantee for
ridge regression, and no foolproof way to choose the ridge parameter (cid:21).
On the other hand, Q
ˇJS does not stabilize the coordinate standard devia-
tions, as in the sd(0.1) column of Table 7.3. The main point here is that at
present there is no optimality theory for shrinkage estimation. Fisher pro-
vided an elegant theory for optimal unbiased estimation. It remains to be
seen whether biased estimation can be neatly codiﬁed.

241000X

jD1

351=2

241000X

jD1

(7.45)
D

351=2

:

7.4 Indirect Evidence 2

There is a downside to shrinkage estimation, which we can examine by
returning to the baseball data of Table 7.1. One thousand simulations were
run, each one generating simulated batting averages

(cid:3)

p
i

(cid:24) Bi.90; TRUTHi /=90

i D 1; 2; : : : ; 18:

(cid:3)

These gave corresponding James–Stein (JS) estimates (7.20), with (cid:27) 2
Np
0

.1 (cid:0) Np
(cid:3)
Table 7.4 shows the root mean square error for the MLE and JS estimates

/=90.

over 1000 simulations for each of the 18 players,

(cid:3)

ij

(cid:0) TRUTHi /2

.p

and

. Op

(cid:3) JS

ij

(cid:0) TRUTHi /2

loses to Op

(7.46)
As foretold by the James–Stein Theorem, the JS estimates are easy victors
in terms of total squared error (summing over all 18 players). However,
Op
(cid:3) JS
(cid:3)
i for 4 of the 18 players, losing badly in the case
of player 2.
(cid:3) JS
for player 2 appear in Figure 7.3. Strikingly, all 1000 of the Op
(cid:3) JS
2j values lie

Histograms comparing the 1000 simulations of p

i with those of Op
(cid:3)

D p

(cid:3) MLE

i

i

i

2 Of course we are assumimg (cid:27) 2 is known in (7.43); if it is estimated, some of the

improvement erodes away.

7.4 Indirect Evidence 2

103

Table 7.4 Simulation study comparing root mean square errors for MLE
and JS estimators (7.20) as estimates of TRUTH. Total mean square errors
.0384 (MLE) and .0235 (JS). Asterisks indicate four players for whom
rmsJS exceeded rmsMLE; these have two largest and two smallest
TRUTH values (player 2 is Clemente). Column rmsJS1 is for the limited
translation version of JS that bounds shrinkage to within one standard
deviation of the MLE.

Player

TRUTH

rmsMLE

rmsJS

rmsJS1

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

.298
.346*
.222
.276
.263
.273
.303
.270
.230
.264
.264
.210*
.256
.269
.316*
.226
.285
.200*

.046
.049
.044
.048
.047
.046
.047
.049
.044
.047
.047
.043
.045
.048
.048
.045
.046
.043

.033
.077
.042
.015
.011
.014
.037
.012
.034
.011
.012
.053
.014
.012
.049
.038
.022
.062

.032
.056
.038
.023
.020
.021
.035
.022
.033
.021
.020
.044
.020
.021
.043
.036
.026
.048

(cid:3) JS

i

below TRUTH2 D 0:346. Player 2 could have had a legitimate complaint if
the James–Stein estimate were used to set his next year’s salary.
The four losing cases for Op

are the players with the two largest and
two smallest values of the TRUTH. Shrinkage estimators work against cases
that are genuinely outstanding (in a positive or negative sense). Player 2
was Roberto Clemente. A better informed Bayesian, that is, a baseball fan,
would know that Clemente had led the league in batting over the previ-
ous several years, and shouldn’t be thrown into a shrinkage pool with 17
ordinary hitters.

Of course the James–Stein estimates were more accurate for 14 of the
18 players. Shrinkage estimation tends to produce better results in general,
at the possible expense of extreme cases. Nobody cares much about Cold
War batting averages, but if the context were the efﬁcacies of 18 new anti-
cancer drugs the stakes would be higher.

104

James–Stein Estimation and Ridge Regression

Figure 7.3 Comparing MLE estimates (solid) with JS estimates
(line) for Clemente; 1000 simulations, 90 at bats each.

Compromise methods are available. The rmsJS1 column of Table 7.4
refers to a limited translation version of OpJS
in which shrinkage is not al-
lowed to diverge more than one (cid:27)0 unit from Opi; in formulaic terms,

i

D min˚max(cid:0) OpJS

OpJS 1

i

(cid:1) ; Opi C (cid:27)0

(cid:9) :

i ; Opi (cid:0) (cid:27)0

(7.47)

This mitigates the Clemente problem while still gaining most of the shrink-
age advantages.

The use of indirect evidence amounts to learning from the experience
of others, each batter learning from the 17 others in the baseball exam-
ples. “Which others?” is a key question in applying computer-age methods.
Chapter 15 returns to the question in the context of false-discovery rates.

7.5 Notes and Details

The Bayesian motivation emphasized in Chapters 6 and 7 is anachronistic:
originally the work emerged mainly from frequentist considerations and
was justiﬁed frequentistically, as in Robbins (1956). Stein (1956) proved
the inadmissibility of O(cid:22)MLE, the neat version of O(cid:22)JS appearing in James
and Stein (1961) (Willard James was Stein’s graduate student); O(cid:22)JS is it-
self inadmissable, being everywhere improvable by changing OB in (7.13)

 p^Frequency0.200.250.300.350.400.450.500.55050100150200250300350Truth 0.346p^ MLEp^ James−Stein105
to max. OB; 0/. This in turn is inadmissable, but further gains tend to the
minuscule.

7.5 Notes and Details

In a series of papers in the early 1970s, Efron and Morris emphasized
the empirical Bayes motivation of the James–Stein rule, Efron and Morris
(1972) giving the limited translation version (7.47). The baseball data in its
original form appears in Table 1.1 of Efron (2010). Here the original 45 at
bats recorded for each player have been artiﬁcially augmented by adding
45 binomial draws, Bi.45; TRUTHi / for player i. This gives a somewhat
less optimistic view of the James–Stein rule’s performance.

“Stein’s paradox in statistics,” Efron and Morris’ title for their 1977 Sci-
entiﬁc American article, catches the statistics world’s sense of discomfort
with the James–Stein theorem. Why should our estimate for Player A go
up or down depending on the other players’ performances? This is the
question of direct versus indirect evidence, raised again in the context of
hypothesis testing in Chapter 15. Unbiased estimation has great scientiﬁc
appeal, so the argument is by no means settled.

Ridge regression was introduced into the statistics literature by Hoerl
and Kennard (1970). It appeared previously in the numerical analysis liter-
ature as Tikhonov regularization.
of freedom, Z (cid:24) (cid:31)2

1 [p. 93] Formula (7.12). If Z has a chi-squared distribution with (cid:23) degrees
(cid:23) (that is, Z (cid:24) Gam.(cid:23)=2; 2/ in Table 5.1), it has density
f .z/ D z(cid:23)=2(cid:0)1e

for z (cid:21) 0;

(7.48)

(cid:0)z=2

2(cid:23)=2.(cid:23)=2/

2(cid:23)=2

dz D 2(cid:23)=2(cid:0)1
(cid:26) N (cid:0) 3
(cid:27) D 1

S

A C 1

.(cid:23)=2 (cid:0) 1/

.(cid:23)=2/

D 1
(cid:23) (cid:0) 2

:

(7.49)

;

(7.50)

yielding

(cid:26) 1

(cid:27) DZ 1

E

z

(cid:0)z=2

z(cid:23)=2(cid:0)2e
2(cid:23)=2.(cid:23)=2/

0

E

verifying (7.12).

But standard results, starting from (7.11), show that S (cid:24) .A C 1/(cid:31)2
With (cid:23) D N (cid:0) 1 in (7.49),

N(cid:0)1.

2 [p. 93] Formula (7.14). First consider the simpler situation where M in
(7.11) is known to equal zero, in which case the James–Stein estimator is
(7.51)

with OB D 1 (cid:0) .N (cid:0) 2/=S;

where S DPN
OC D 1 (cid:0) OB D .N (cid:0) 2/=S and C D 1 (cid:0) B D 1=.A C 1/:

D OBxi
O(cid:22)JS
i . For convenient notation let
1 x2

(7.52)

i

James–Stein Estimation and Ridge Regression

106
The conditional distribution (cid:22)ijx (cid:24)

E

E

i

(cid:0) (cid:22)i

N .Bxi ; B/ gives
i ;

(cid:1)2ˇˇˇx
n(cid:0) O(cid:22)JS
o D B C . OC (cid:0) C /2x2
(cid:13)(cid:13)2ˇˇˇx
o D NB C . OC (cid:0) C /2S:
n(cid:13)(cid:13) O(cid:22)JS (cid:0) (cid:22)
o D 2.1 (cid:0) B/;
n
. OC (cid:0) C /2S
n(cid:13)(cid:13) O(cid:22)JS (cid:0) (cid:22)
(cid:13)(cid:13)2o D NB C 2.1 (cid:0) B/:

E

E

and so

and, adding over the N coordinates,

The marginal distribution S (cid:24) .A C 1/(cid:31)2
calculation,

N and (7.49) yields, after a little

(7.53)

(7.54)

(7.55)

(7.56)

By orthogonal transformations, in situation (7.7), where M is not as-
sumed to be zero, O(cid:22)JS can be represented as the sum of two parts: a JS
estimate in N (cid:0) 1 dimensions but with M D 0 as in (7.51), and a MLE
estimate of the remaining one coordinate. Using (7.56) this gives

n(cid:13)(cid:13) O(cid:22)JS (cid:0) (cid:22)

(cid:13)(cid:13)2o D .N (cid:0) 1/B C 2.1 (cid:0) B/ C 1

(7.57)

D NB C 3.1 (cid:0) B/;

E

which is (7.14).

3 [p. 93] The James–Stein Theorem. Stein (1981) derived a simpler proof of

the JS Theorem that appears in Section 1.2 of Efron (2010).

4 [p. 98] Transformations to form (7.35). The linear regression model (7.28)
is equivariant under scale changes of the variables xj . What this means
is that the space of ﬁts using linear combinations of the xj is the same as
the space of linear combinations using scaled versions Qxj D xj =sj , with
sj > 0. Furthermore, the least squares ﬁts are the same, and the coefﬁcient
estimates map in the obvious way:

OQ
ˇj D sj

ridge regression, we see that the penalty term kˇk2 DP

Not so for ridge regression. Changing the scales of the columns of X
will generally lead to different ﬁts. Using the penalty version (7.41) of
j treats all the
coefﬁcients as equals. This penalty is most natural if all the variables are
measured on the same scale. Hence we typically use for sj the standard
deviation of variable xj , which leads to (7.35). Furthermore, with ridge
regression we typically do not penalize the intercept. This can be achieved

O
ˇj .

j ˇ2

107
by centering and scaling each of the variables, Qxj D .xj (cid:0) 1Nxj /=sj , where

7.5 Notes and Details

Nxj D nX

xij =n and sj DhX

.xij (cid:0) Nxj /2i1=2

iD1

(7.58)
with 1 the n-vector of 1s. We now work with QX D . Qx1; Qx2; : : : ; Qxp/ rather
than X, and the intercept is estimated separately as Ny.
we calculate the covariance matrix of O
Cov(cid:21) D (cid:27) 2.S C (cid:21)I/

5 [p. 100] Standard deviations in Table 7.3. From the ﬁrst equality in (7.36)

ˇ.(cid:21)/ to be
(cid:0)1S .S C (cid:21)I/

(7.59)

(cid:0)1:

;

The entries sd(0.1) in Table 7.3 are square roots of the diagonal elements
of Cov(cid:21), substituting the ordinary least squares estimate O(cid:27) D 54:1 for (cid:27) 2.
6 [p. 101] Penalized likelihood and MAP. With (cid:27) 2 ﬁxed and known in the
Nn.X ˇ; (cid:27) 2I/, minimizing ky (cid:0) X ˇk2 is the
normal linear model y (cid:24)
same as maximizing the log density function

log fˇ .y/ D (cid:0) 1

ky (cid:0) X ˇk2 C constant:

2

(7.60)
In this sense, the term (cid:21)kˇk2 in (7.41) penalizes the likelihood log fˇ .y/
connected with ˇ in proportion to the magnitude kˇk2. Under the prior
distribution (7.38), the log posterior density of ˇ given y (the log of (3.5))
is

˚ky (cid:0) X ˇk2 C (cid:21)kˇk2(cid:9) ;

(7.61)

(cid:0) 1
2(cid:27) 2

plus a term that doesn’t depend on ˇ. That makes the maximizer of (7.41)
also the maximizer of the posterior density of ˇ given y, or the MAP.
7 [p. 101] Formula (7.43). Let (cid:13) D .S 1=2=(cid:27) /ˇ and O(cid:13) D .S 1=2=(cid:27) /
(7.37), where S 1=2 is a matrix square root of S , .S 1=2/2 D S . Then

O
ˇ in

O(cid:13) (cid:24)

and the M D 0 form of the James–Stein rule (7.51) is

Np.(cid:13); I/;
1 (cid:0) p (cid:0) 2
kO(cid:13)k2
Transforming back to the ˇ scale gives (7.43).

O(cid:13) JS D

(cid:21) O(cid:13) :

(7.62)

(7.63)

8

Generalized Linear Models and Regression

Trees

Indirect evidence is not the sole property of Bayesians. Regression models
are the frequentist method of choice for incorporating the experience of
“others.” As an example, Figure 8.1 returns to the kidney ﬁtness data of
Section 1.1. A potential new donor, aged 55, has appeared, and we wish
to assess his kidney ﬁtness without subjecting him to an arduous series of
medical tests. Only one of the 157 previously tested volunteers was age
55, his tot score being (cid:0)0:01 (the upper large dot in Figure 8.1). Most
dtot D (cid:0)1:46. The former is the only direct evidence we have, while the
applied statisticians, though, would prefer to read off the height of the least
squares regression line at age D 55 (the green dot on the regression line),

Figure 8.1 Kidney data; a new volunteer donor is aged 55.
Which prediction is preferred for his kidney function?

108

*************************************************************************************************************************************************************2030405060708090−6−4−2024Agetotll558.1 Logistic Regression

109

regression line lets us incorporate indirect evidence for age 55 from all 157
previous cases.

Increasingly aggressive use of regression techniques is a hallmark of
modern statistical practice, “aggressive” applying to the number and type
of predictor variables, the coinage of new methodology, and the sheer size
of the target data sets. Generalized linear models, this chapter’s main topic,
have been the most pervasively inﬂuential of the new methods. The chapter
ends with a brief review of regression trees, a completely different regres-
sion methodology that will play an important role in the prediction algo-
rithms of Chapter 17.

8.1 Logistic Regression

An experimental new anti-cancer drug called Xilathon is under devel-
opment. Before human testing can begin, animal studies are needed to de-
termine safe dosages. To this end, a bioassay or dose–response experiment
was carried out: 11 groups of n D 10 mice each were injected with in-
creasing amounts of Xilathon, dosages coded1 1; 2; : : : ; 11.

Let

yi D # mice dying in ith group:
The points in Figure 8.2 show the proportion of deaths

pi D yi =10;

(8.1)

(8.2)

lethality generally increasing with dose. The counts yi are modeled as in-
dependent binomials,

ind(cid:24) Bi.ni ; (cid:25)i /

for i D 1; 2; : : : ; N;

yi

(8.3)
N D 11 and all ni equaling 10 here; (cid:25)i is the true death rate in group
i, estimated unbiasedly by pi, the direct evidence for (cid:25)i. The regression
curve in Figure 8.2 uses all the doses to give a better picture of the true
dose–response relation.

Logistic regression is a specialized technique for regression analysis of

count or proportion data. The logit parameter (cid:21) is deﬁned as

o

n (cid:25)

1 (cid:0) (cid:25)

(cid:21) D log

;

(8.4)

1 Dose would usually be labeled on a log scale, each one, say, 50% larger than its

predecessor.

110

GLMs and Regression Trees

Figure 8.2 Dose–response study; groups of 10 mice exposed to
increasing doses of experimental drug. The points are the
observed proportions that died in each group. The ﬁtted curve is
the maximum-likelihoood estimate of the linear logistic
regression model. The open circle on the curve is the LD50, the
estimated dose for 50% mortality.

with (cid:21) increasing from (cid:0)1 to 1 as (cid:25) increases from 0 to 1. A linear lo-
gistic regression dose–response analysis begins with binomial model (8.3),
and assumes that the logit is a linear function of dose,

(cid:26) (cid:25)i

1 (cid:0) (cid:25)i

(cid:27) D ˛0 C ˛1xi :

(cid:21)i D log

Maximum likelihood gives estimates .O˛0; O˛1/, and ﬁtted curve

(8.5)

(8.6)

(8.7)

(8.8)

Since the inverse transformation of (8.4) is

we obtain from (8.6) the linear logistic regression curve

O
(cid:21).x/ D O˛0 C O˛1x:

(cid:25) D(cid:16)
O(cid:25).x/ D(cid:16)

1 C e

(cid:0)(cid:21)(cid:0)1
(cid:0).O˛0CO˛1x/(cid:0)1

1 C e

pictured in Figure 8.2.

Table 8.1 compares the standard deviation of the estimated regression

lllllllllllDoseProportion of deaths12345678910110.000.250.500.751.00llLD50 = 5.698.1 Logistic Regression

111
Table 8.1 Standard deviation estimates for O(cid:25).x/ in Figure 8.1. The ﬁrst
row is for the linear logistic regression ﬁt (8.8); the second row is based
on the individual binomial estimates pi.

x

sd O(cid:25).x/
sd pi

1

2

3

4

5

6

7

8

9

10

11

.015
.045

.027
.066

.043
.094

.061
.126

.071
.152

.072
.157

.065
.138

.050
.106

.032
.076

.019
.052

.010
.035

curve (8.8) at x D 1; 2; : : : ; 11 (as discussed in the next section) with
the usual binomial standard deviation estimate Œpi .1(cid:0) pi /=101=2 obtained
by considering the 11 doses separately.2 Regression has reduced error by
better than 50%, the price being possible bias if model (8.5) goes seriously
wrong.

One advantage of the logit transformation is that (cid:21) isn’t restricted to the
range Œ0; 1, so model (8.5) never verges on forbidden territory. A better
reason has to do with the exploitation of exponential family properties. We
can rewrite the density function for Bi.n; y/ as

 

!

n

y

(cid:25) y.1 (cid:0) (cid:25)/n(cid:0)y D e(cid:21)y(cid:0)n .(cid:21)/

 

!

n

y

(8.9)

(8.10)

with (cid:21) the logit parameter (8.4) and

 .(cid:21)/ D logf1 C e(cid:21)gI

(8.9) is a one-parameter exponential family3 as described in Section 5.5,
with (cid:21) the natural parameter, called ˛ there.
Let y D .y1; y2; : : : ; yN / denote the full data set, N D 11 in Figure 8.2.
 
!
Using (8.5), (8.9), and the independence of the yi gives the probability
density of y as a function of .˛0; ˛1/,
e(cid:21)i yi(cid:0)ni  .(cid:21)i /
1 ni  .˛0C˛1xi / (cid:1) NY
(cid:0)PN

f˛0;˛1 .y/ D NY

D e˛0S0C˛1S1 (cid:1) e

(8.11)

 

ni
yi

!

;

iD1

ni
yi

iD1

2 For the separate-dose standard error, pi was taken equal to the ﬁtted value from the

curve in Figure 8.2.

3 It is not necessary for f(cid:22)0 .x/ in (5.46) on page 64 to be a probability density function,

only that it not depend on the parameter (cid:22).

112

where

GLMs and Regression Trees

S0 D NX

yi

iD1

and S1 D NX

iD1

xi yi :

(8.12)

Formula (8.11) expresses f˛0;˛1 .y/ as the product of three factors,

f˛0;˛1 .y/ D g˛0;˛1 .S0; S1/h.˛0; ˛1/j.y/;

(8.13)

1

only the ﬁrst of which involves both the parameters and the data. This im-
plies that .S0; S1/ is a sufﬁcient statistic: no matter how large N might be
(later we will have N in the thousands), just the two numbers .S0; S1/ con-
tain all of the experiment’s information. Only the logistic parameterization
(8.4) makes this happen.4
A more intuitive picture of logistic regression depends on D.pi ; O(cid:25)i /, the
deviance between an observed proportion pi (8.2) and an estimate O(cid:25)i,



 piO(cid:25)i

 C .1 (cid:0) pi / log

(cid:21)

 1 (cid:0) pi

1 (cid:0) O(cid:25)i

D .pi ; O(cid:25)i / D 2ni

pi log

(8.14)
The deviance5 is zero if O(cid:25)i D pi, otherwise it increases as O(cid:25)i departs
further from pi.
The logistic regression MLE value .O˛0; O˛1/ also turns out to be the
choice of .˛0; ˛1/ minimizing the total deviance between the N points pi
and their corresponding estimates O(cid:25)i D (cid:25)O˛0;O˛1 .xi / (8.8):

:

.O˛0; O˛1/ D arg min

.˛0;˛1/

NX

iD1

D .pi ; (cid:25)˛0;˛1 .xi // :

(8.15)

The solid line in Figure 8.2 is the linear logistic curve coming closest to
the 11 points, when distance is measured by total deviance. In this way the
200-year-old notion of least squares is generalized to binomial regression,
as discussed in the next section. A more sophisticated notion of distance
between data and models is one of the accomplishments of modern statis-
tics.

Table 8.2 reports on the data for a more structured logistic regression
analysis. Human muscle cell colonies were infused with mouse nuclei in
ﬁve different ratios, cultured over time periods ranging from one to ﬁve

4 Where the name “logistic regression” comes from is explained in the endnotes, along

with a description of its nonexponential family predecessor probit analysis.

5 Deviance is analogous to squared error in ordinary regression theory, as discussed in
what follows. It is twice the “Kullback–Leibler distance,” the preferred name in the
information-theory literature.

8.1 Logistic Regression

113

Table 8.2 Cell infusion data; human cell colonies infused with mouse
nuclei in ﬁve ratios over 1 to 5 days and observed to see whether they did
or did not thrive. Green numbers are estimates O(cid:25)ij from the logistic
regression model. For example, 5 of 31 colonies in the lowest ratio/days
category thrived, with observed proportion 5=31 D 0:16, and logistic
regression estimate O(cid:25)11 D 0:11:

1

5/31
.11
15/77
.24

48/126

.38
29/92
.32
11/53
.18

2

3/28
.25
36/78
.45

68/116

.62
35/52
.56
20/52
.37

1

2

Ratio 3

4

5

Time
3

20/45
.42
43/71
.64

145/171

.77
57/85
.73
20/48
.55

4

24/47
.54
56/71
.74

98/119

.85
38/50
.81
40/55
.67

5

29/35
.75
66/74
.88

114/129

.93
72/77
.92
52/61
.84

days, and observed to see whether they thrived. For example, of the 126
colonies having the third ratio and shortest time period, 48 thrived.
Let (cid:25)ij denote the true probability of thriving for ratio i during time
period j , and (cid:21)ij its logit logf(cid:25)ij =.1 (cid:0) (cid:25)ij /g. A two-way additive logistic
regression was ﬁt to the data,6

(cid:21)ij D (cid:22) C ˛i C ˇj ;

i D 1; 2; : : : ; 5; j D 1; 2; : : : ; 5:

(8.16)

The green numbers in Table 8.2 show the maximum likelihood estimates

.

(cid:0)(cid:16) O(cid:22)CO˛iC Oˇj

(cid:21)

:

O(cid:25)ij D 1

1 C e

(8.17)

straintsP ˛i D P ˇj D 0 necessary to avoid deﬁnitional difﬁculties)

Model (8.16) has nine free parameters (taking into account the con-

compared with just two in the dose–response experiment. The count can
easily go much higher these days.
Table 8.3 reports on a 57-variable logistic regression applied to the spam
data. A researcher (named George) labeled N D 4601 of his email mes-

6 Using the statistical computing language R; see the endnotes.

114

GLMs and Regression Trees

Table 8.3 Logistic regression analysis of the spam data, model (8.17);
estimated regression coefﬁcients, standard errors, and z D estimate=se,
for 57 keyword predictors. The notation char$ means the relative
number of times $ appears, etc. The last three entries measure
characteristics such as length of capital-letter strings. The word george
is special, since the recipient of the email is named George, and the goal
here is to build a customized spam ﬁlter.

intercept
make
address
all
3d
our
over
remove
internet
order
mail
receive
will
people
report
addresses
free
business
email
you
credit
your
font
000
money
hp
hpl
george
650

Estimate
(cid:0)12.27
(cid:0).12
(cid:0).19
.06
3.14
.38
.24
.89
.23
.20
.08
(cid:0).05
(cid:0).12
(cid:0).02
.05
.32
.86
.43
.06
.14
.53
.29
.21
.79
.19
(cid:0)3.21
(cid:0).92
(cid:0)39.62
.24

se

1.99
.07
.09
.06
2.10
.07
.07
.13
.07
.08
.05
.06
.06
.07
.05
.19
.12
.10
.06
.06
.27
.06
.17
.16
.07
.52
.39
7.12
.11

z-value
(cid:0)6.16 lab
(cid:0)1.68 labs
(cid:0)2.10 telnet
1.03 857
1.49 data
5.52 415
3.53 85
6.85 technology
3.39 1999
2.58 parts
1.75 pm
(cid:0) .86 direct
(cid:0)1.87 cs
(cid:0) .35 meeting
1.06 original
1.70 project
7.13 re
4.26 edu
1.03 table
2.32 conference
1.95 char;
4.62 char(
1.24 char
4.76 char!
2.63 char$
(cid:0)6.14 char#
(cid:0)2.37 cap.ave
(cid:0)5.57 cap.long
2.24 cap.tot

Estimate
(cid:0)1.48
(cid:0).15
(cid:0).07
.84
(cid:0).41
.22
(cid:0)1.09
.37
.02
(cid:0).13
(cid:0).38
(cid:0).11
(cid:0)16.27
(cid:0)2.06
(cid:0).28
(cid:0).98
(cid:0).80
(cid:0)1.33
(cid:0).18
(cid:0)1.15
(cid:0).31
(cid:0).05
(cid:0).07
.28
1.31
1.03
.38
1.78
.51

se

.89
.14
.19
1.08
.17
.53
.42
.12
.07
.09
.17
.13
9.61
.64
.18
.33
.16
.24
.13
.46
.11
.07
.09
.07
.17
.48
.60
.49
.14

sages as either spam or ham (nonspam7), say

(

yi D

1 if email i is spam
0 if email i is ham

z-value
(cid:0)1.66
(cid:0)1.05
(cid:0) .35
.78
(cid:0)2.37
.42
(cid:0)2.61
2.99
.26
(cid:0)1.41
(cid:0)2.26
(cid:0) .84
(cid:0)1.69
(cid:0)3.21
(cid:0)1.55
(cid:0)2.97
(cid:0)5.09
(cid:0)5.43
(cid:0)1.40
(cid:0)2.49
(cid:0)2.92
(cid:0) .75
(cid:0) .78
3.89
7.55
2.16
.64
3.62
3.75

(8.18)

7 “Ham” refers to “nonspam” or good email; this is a playful connection to the processed

8.1 Logistic Regression

115
(40% of the messages were spam). The p D 57 predictor variables repre-
sent the most frequently used words and tokens in George’s corpus of email
(excluding trivial words such as articles), and are in fact the relative fre-
quencies of these chosen words in each email (standardized by the length
of the email). The goal of the study was to predict whether future emails
are spam or ham using these keywords; that is, to build a customized spam
ﬁlter.

Let xij denote the relative frequency of keyword j in email i, and (cid:25)i
represent the probability that email i is spam. Letting (cid:21)i be the logit trans-
form logf(cid:25)i =.1 (cid:0) (cid:25)i /g, we ﬁt the additive logistic model

(8.19)
Table 8.3 shows O˛i for each word—for example, (cid:0)0:12 for make—as well
as the estimated standard error and the z-value: estimate=se.

˛j xij :

jD1

(cid:21)i D ˛0 C 57X

It looks like certain words, such as free and your, are good spam
predictors. However, the table as a whole has an unstable appearance, with
occasional very large estimates O˛i accompanied by very large standard de-
viations.8 The dangers of high-dimensional maximum likelihood estima-
tion are apparent here. Some sort of shrinkage estimation is called for, as
discussed in Chapter 16.

(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)

Regression analysis, either in its classical form or in modern formula-
tions, requires covariate information x to put the various cases into some
sort of geometrical relationship. Given such information, regression is the
statistician’s most powerful tool for bringing “other” results to bear on a
case of primary interest: for instance, the age-55 volunteer in Figure 8.1.

Empirical Bayes methods do not require covariate information but may
be improvable if it exists. If, for example, the player’s age were an impor-
tant covariate in the baseball example of Table 7.1, we might ﬁrst regress
the MLE values on age, and then shrink them toward the regression line
rather than toward the grand mean Np as in (7.20). In this way, two different
sorts of indirect evidence would be brought to bear on the estimation of
each player’s ability.

spam that was fake ham during WWII, and has been adopted by the machine-learning
community.
8 The 4601 (cid:2) 57 X matrix .xij / was standardized, so disparate scalings are not the
cause of these discrepancies. Some of the features have mostly “zero” observations,
which may account for their unstable estimation.

116

GLMs and Regression Trees

8.2 Generalized Linear Models9

Logistic regression is a special case of generalized linear models (GLMs),
a key 1970s methodology having both algorithmic and inferential inﬂu-
ence. GLMs extend ordinary linear regression, that is least squares curve-
ﬁtting, to situations where the response variables are binomial, Poisson,
gamma, beta, or in fact any exponential family form.
n
We begin with a one-parameter exponential family,
f(cid:21).y/ D e(cid:21)y(cid:0)(cid:13).(cid:21)/f0.y/; (cid:21) 2 ƒ

(8.20)

o

;

as in (5.46) (now with ˛ and x replaced by (cid:21) and y, and  .˛/ replaced by
(cid:13).(cid:21)/, for clearer notation in what follows). Here (cid:21) is the natural parameter
and y the sufﬁcient statistic, both being one-dimensional in usual applica-
tions; (cid:21) takes its values in an interval of the real line. Each coordinate yi
of an observed data set y D .y1; y2; : : : ; yi ; : : : ; yN /
0 is assumed to come
from a member of family (8.20),

yi (cid:24) f(cid:21)i .(cid:1)/ independently for i D 1; 2; : : : ; N:

(8.21)

Table 8.4 lists (cid:21) and y for the ﬁrst four families in Table 5.1, as well as
their deviance and normalizing functions.

By itself, model (8.21) requires N parameters (cid:21)1; (cid:21)2; : : : ; (cid:21)N , usually
too many for effective individual estimation. A key GLM tactic is to specify
the (cid:21)s in terms of a linear regression equation. Let X be an N(cid:2)p “structure
0
matrix,” with ith row say x
i, and ˛ an unknown vector of p parameters;
the N -vector (cid:21) D .(cid:21)1; (cid:21)2; : : : ; (cid:21)N /

0 is then speciﬁed by

(cid:21) D X ˛:

(8.22)
In the dose–response experiment of Figure 8.2 and model (8.5), X is N (cid:2) 2
with ith row .1; xi / and parameter vector ˛ D .˛0; ˛1/.
NY
The probability density function f˛.y/ of the data vector y is

PN
1 .(cid:21)i yi(cid:0)(cid:13).(cid:21)i //

f˛.y/ D NY

f(cid:21)i .yi / D e

(8.23)

iD1

f0.yi /;

iD1

which can be written as

f˛.y/ D e˛

0

z(cid:0) .˛/f0.y/;

(8.24)

9 Some of the more technical points raised in this section are referred to in later chapters,

and can be scanned or omitted at ﬁrst reading.

8.2 Generalized Linear Models

117

Table 8.4 Exponential family form for ﬁrst four cases in Table 5.1;
natural parameter (cid:21), sufﬁcient statistic y, deviance (8.31) between family
members f1 and f2, D.f1; f2/, and normalizing function (cid:13).(cid:21)/.

(cid:21)

(cid:22)=(cid:27) 2

y

x

log (cid:22)

x

log (cid:25)

1(cid:0)(cid:25) x 2n

h

(cid:27)

i

D.f1; f2/

2
 (cid:0) log (cid:22)2

(cid:16) (cid:22)1(cid:0)(cid:22)2
h(cid:16) (cid:22)2
C .1 (cid:0) (cid:25)1/ log 1(cid:0)(cid:25)1
1(cid:0)(cid:25)2
i

 (cid:0) log (cid:27)1

h(cid:16) (cid:27)1

(cid:0) 1

(cid:22)1

(cid:13).(cid:21)/

(cid:27) 2(cid:21)2=2

i

e(cid:21)

n log.1 C e(cid:21)/

(cid:0)(cid:23) log.(cid:0)(cid:21)/

2(cid:22)1

(cid:22)1

(cid:25)1 log (cid:25)1
(cid:25)2

(cid:0)1=(cid:27)

x

2(cid:23)

(cid:0) 1

(cid:27)2

(cid:27)2

1. Normal

N .(cid:22); (cid:27) 2/,
(cid:27) 2 known

2. Poisson
Poi.(cid:22)/

3. binomial
Bi.n; (cid:25)/

4. Gamma

Gam.(cid:23); (cid:27) /,
(cid:23) known

where

y and  .˛/ D NX

iD1

z D X

0

0
i ˛/;

(cid:13).x

(8.25)

a p-parameter exponential family (5.50), with natural parameter vector ˛
and sufﬁcient statistic vector z. The main point is that all the information
from a p-parameter GLM is summarized in the p-dimensional vector z,
no matter how large N may be, making it easier both to understand and to
analyze.

We have now reduced the N -parameter model (8.20)–(8.21) to the p-
parameter exponential family (8.24), with p usually much smaller than N ,
in this way avoiding the difﬁculties of high-dimensional estimation. The
moments of the one-parameter constituents (8.20) determine the estimation
properties in model (8.22)–(8.24). Let .(cid:22)(cid:21); (cid:27) 2
(cid:21)/ denote the expectation and
variance of univariate density f(cid:21).y/ (8.20),
y (cid:24) .(cid:22)(cid:21); (cid:27) 2
(cid:21)/;

(8.26)
(cid:21)/ D .e(cid:21); e(cid:21)/ for the Poisson. The N -vector y obtained

for instance .(cid:22)(cid:21); (cid:27) 2
from GLM (8.22) then has mean vector and covariance matrix

y (cid:24) .(cid:22).˛/; †.˛// ;

(8.27)

GLMs and Regression Trees

118
where (cid:22).˛/ is the vector with ith component (cid:22)(cid:21)i with (cid:21)i D x
0
i ˛, and †.˛/
is the N (cid:2) N diagonal matrix having diagonal elements (cid:27) 2
.
The maximum likelihood estimate O˛ of the parameter vector ˛ can be

(cid:21)i

shown to satisfy the simple equation

0

Œy (cid:0) (cid:22) .O˛/ D 0:

X

For the normal case where yi (cid:24)
linear regression, (cid:22).O˛/ D X O˛ and (8.28) becomes X
the familiar solution

(8.28)
N .(cid:22)i ; (cid:27) 2/ in (8.21), that is, for ordinary
.y (cid:0) X O˛/ D 0, with

0

O˛ D .X

0

(cid:0)1X

0

yI

X /

otherwise, (cid:22).˛/ is a nonlinear function of ˛, and (8.28) must be solved
by numerical iteration. This is made easier by the fact that, for GLMs,
log f˛.y/, the likelihood function we wish to maximize, is a concave func-
tion of ˛. The MLE O˛ has approximate expectation and covariance

(8.29)

(8.30)

similar to the exact OLS result O˛ (cid:24) .˛; (cid:27)

Generalizing the binomial deﬁnition (8.14), the deviance between den-

sities f1.y/ and f2.y/ is deﬁned to be

D.f1; f2/ D 2

dy;

(8.31)

O˛ P(cid:24) .˛;(cid:0)X
Z

0

/;

†.˛/X(cid:1)(cid:0)1
(cid:26) f1.y/

(cid:0)2.X

X /

0

f1.y/ log

(cid:0)1/.

(cid:27)

Y

f2.y/

the integral (or sum for discrete distributions) being over their common
sample space Y. D.f1; f2/ is always nonnegative, equaling zero only if
f1 and f2 are the same; in general D.f1; f2/ does not equal D.f2; f1/.
Deviance does not depend on how the two densities are named, for example
(8.14) having the same expression as the Binomial entry in Table 8.4.
In what follows it will sometimes be useful to label the family (8.20) by
its expectation parameter (cid:22) D E(cid:21)fyg rather than by the natural parameter
(cid:21):

f(cid:22).y/ D e(cid:21)y(cid:0)(cid:13).(cid:21)/f0.y/;

(8.32)

meaning the same thing as (8.20), only the names attached to the individ-
ual family members being changed. In this notation it is easy to show a
fundamental result sometimes known as
Hoeffding’s Lemma 
The maximum likelihood estimate of (cid:22) given y
is y itself, and the log likelihood log f(cid:22).y/ decreases from its maximum
log fy.y/ by an amount that depends on the deviance D.y; (cid:22)/,

f(cid:22).y/ D fy.y/e

(cid:0)D.y;(cid:22)/=2:

(8.33)

2

3

4

5

8.2 Generalized Linear Models

119

Returning to the GLM framework (8.21)–(8.22), parameter vector ˛
gives (cid:21).˛/ D X ˛, which in turn gives the vector of expectation param-
eters

(cid:22).˛/ D .: : : (cid:22)i .˛/ : : : /
0
#

(8.34)
for instance (cid:22)i .˛/ D expf(cid:21)i .˛/g for the Poisson family. Multiplying Hoeff-
ding’s lemma (8.33) over the N cases y D .y1; y2; : : : ; yN /

0 yields

;

f(cid:22)i .˛/.yi / D

" NY
f˛.y/ D NY
minimizes the total deviancePN

(8.35)
This has an important consequence: the MLE O˛ is the choice of ˛ that
1 D.yi ; (cid:22)i .˛//. As in Figure 8.2, GLM
maximum likelihood ﬁtting is “least total deviance” in the same way that
ordinary linear regression is least sum of squares.
(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)

1 D.yi ;(cid:22)i .˛//:

iD1

fyi .yi /

e

iD1

(cid:0)PN

The inner circle of Figure 8.3 represents normal theory, the preferred
venue of classical applied statistics. Exact inferences—t-tests, F distribu-
tions, most of multivariate analysis—were feasible within the circle. Out-
side the circle was a general theory based mainly on asymptotic (large-
sample) approximations involving Taylor expansions and the central limit
theorem.

Figure 8.3 Three levels of statistical modeling.

A few useful exact results lay outside the normal theory circle, relating

 NORMAL THEORY(exact calculations)EXPONENTIAL FAMILIES(partly exact)GENERAL THEORY(asymptotics)Figure 8.3. Three levels of statistical modeling120

GLMs and Regression Trees

to a few special families: the binomial, Poisson, gamma, beta, and others
less well known. Exponential family theory, the second circle in Figure 8.3,
uniﬁed the special cases into a coherent whole. It has a “partly exact” ﬂa-
vor, with some ideal counterparts to normal theory—convex likelihood sur-
faces, least deviance regression—but with some approximations necessary,
as in (8.30). Even the approximations, though, are often more convincing
than those of general theory, exponential families’ ﬁxed-dimension sufﬁ-
cient statistics making the asymptotics more transparent.

Logistic regression has banished its predecessors (such as probit anal-
ysis) almost entirely from the ﬁeld, and not only because of estimating
efﬁciencies and computational advantages (which are actually rather mod-
est), but also because it is seen as a clearer analogue to ordinary least
squares, our 200-year-old dependable standby. GLM research development
has been mostly frequentist, but with a substantial admixture of likelihood-
based reasoning, and a hint of Fisher’s “logic of inductive inference.”

Helping the statistician choose between competing methodologies is the
job of statistical inference. In the case of generalized linear models the
choice has been made, at least partly, in terms of aesthetics as well as phi-
losophy.

8.3 Poisson Regression

The third most-used member of the GLM family, after normal theory least
squares and logistic regression, is Poisson regression. N independent Pois-
son variates are observed,

i D 1; 2; : : : ; N;
where (cid:21)i D log (cid:22)i is assumed to follow a linear model,

ind(cid:24) Poi.(cid:22)i /;

yi

(8.36)

(cid:21).˛/ D X ˛;

(8.37)
where X is a known N (cid:2) p structure matrix and ˛ an unknown p-vector
i ˛ for i D 1; 2; : : : ; N , where x
of regression coefﬁcients. That is, (cid:21)i D x
0
0
is the ith row of X.
In the chapters that follow we will see Poisson regression come to the
rescue in what at ﬁrst appear to be awkward data-analytic situations. Here
we will settle for an example involving density estimation from a spatially
truncated sample.

i

6

Table 8.5 shows galaxy counts  from a small portion of the sky: 487
galaxies have had their redshifts r and apparent magnitudes m measured.

8.3 Poisson Regression

121

Table 8.5 Counts for a truncated sample of 487 galaxies, binned by
redshift and magnitude.

redshift (farther) (cid:0)!

1

1
3
3
1
1
3
2
4
1
1
1
0
0
0
0
0
0
0

2

6
2
2
1
3
2
0
1
0
1
0
1
0
3
0
1
1
1

3

6
3
3
4
2
4
2
1
0
0
0
0
3
1
1
0
0
0

4

3
4
3
3
3
5
4
4
2
2
0
1
1
1
1
0
0
0

5

1
0
3
4
3
3
5
7
2
2
1
1
1
0
1
0
0
0

6

4
5
2
3
4
6
4
3
2
2
1
0
0
0
0
0
0
0

7

6
7
9
2
5
4
2
3
1
0
0
0
0
0
0
0
0
0

8

8
6
9
3
7
3
3
1
2
0
0
0
0
0
0
0
0
0

9

8
6
6
8
6
2
3
2
0
0
0
0
0
0
0
0
0
0

10

20
7
3
9
7
2
0
0
0
0
0
0
0
0
0
0
0
0

11

10
5
5
4
3
5
1
1
0
1
1
0
0
0
0
0
0
0

12

7
7
4
3
4
1
2
1
1
0
1
0
0
0
0
0
0
0

13

16
6
5
4
0
0
0
0
2
0
0
0
0
0
0
0
0
0

14

15

9
8
2
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0

4
5
1
1
1
0
1
0
0
0
0
0
0
0
0
0
0
0

"

18
17
16
15
14
13
12
11
magnitude 10
9
8
7
6
5
4
3
2
1

(dimmer)

Distance from earth is an increasing function of r, while apparent bright-
ness is a decreasing function10 of m. In this survey, counts were limited to
galaxies having

(8.38)

1:22  r  3:32

and 17:2  m  21:5;

the upper limit reﬂecting the difﬁculty of measuring very dim galaxies.

The range of log r has been divided into 15 equal intervals and likewise
18 equal intervals for m. Table 8.5 gives the counts of the 487 galaxies in
the 18 (cid:2) 15 D 270 bins. (The lower right corner of the table is empty be-
cause distant galaxies always appear dim.) The multinomial/Poisson con-
nection (5.44) helps motivate model (8.36), picturing the table as a multi-
nomial observation on 270 categories, in which the sample size N was
itself Poisson.

We can imagine Table 8.5 as a small portion of a much more extensive
table, hypothetically available if the data were not truncated. Experience
suggests that we might then ﬁt an appropriate bivariate normal density to
the data, as in Figure 5.3. It seems like it might be awkward to ﬁt part of a
bivariate normal density to truncated data, but Poisson regression offers an
easy solution.

10 An object of the second magnitude is less bright than one of the ﬁrst, and so on, a

classiﬁcation system owing to the Greeks.

122

GLMs and Regression Trees

Let r be the 270-vector listing the values of r in each bin of the table
(in column order), and likewise m for the 270 m values—for instance m D
.18; 17; : : : ; 1/ repeated 15 times—and deﬁne the 270 (cid:2) 5 matrix X as

X D Œr; m; r 2; rm; m2;

(8.39)

where r 2 is the vector whose components are the square of r’s, etc. The
log density of a bivariate normal distribution in .r; m/ is of the form ˛1r C
˛2m C ˛3r 2 C ˛4rm C ˛5m2, agreeing with log (cid:22)i D x
0
i ˛ as speciﬁed
by (8.39). We can use a Poisson GLM, with yi the ith bin’s count, to es-
timate the portion of our hypothesized bivariate normal distribution in the
truncation region (8.38).

Figure 8.4 Left galaxy data; binned counts. Right Poisson GLM
density estimate.

The left panel of Figure 8.4 is a perspective picture of the raw counts
in Table 8.5. On the right is the ﬁtted density from the Poisson regression.
Irrespective of density estimation, Poisson regression has done a useful job
of smoothing the raw bin counts.

Contours of equal value of the ﬁtted log density

O˛0 C O˛1r C O˛2m C O˛3r 2 C O˛4rm C O˛5m2

(8.40)

are shown in Figure 8.5. One can imagine the contours as truncated por-
tions of ellipsoids, of the type shown in Figure 5.3. The right panel of
Figure 8.4 makes it clear that we are nowhere near the center of the hypo-
thetical bivariate normal density, which must lie well beyond our dimness
limit.

DimmerFartherCountDimmerFartherDensity8.3 Poisson Regression

123

Figure 8.5 Contour curves for Poisson GLM density estimate for
the galaxy data. The red dot shows the point of maximum density.

The Poisson deviance residual Z between an observed count y and a

ﬁtted value O(cid:22) is

Z D sign.y (cid:0) O(cid:22)/D.y; O(cid:22)/1=2;

(8.41)

tist GLM theory says that S DP

with D the Poisson deviance from Table 8.4. Zj k, the deviance residual
between the count yij in the ij th bin of Table 8.5 and the ﬁtted value O(cid:22)j k
from the Poisson GLM, was calculated for all 270 bins. Standard frequen-
j k should be about 270 if the bivari-
ate normal model (8.39) is correct.11 Actually the ﬁt was poor: S D 610.
In practice we might try adding columns to X in (8.39), e.g., rm2 or
r 2m2, improving the ﬁt where it was worst, near the boundaries of the ta-
ble. Chapter 12 demonstrates some other examples of Poisson density esti-
mation. In general, Poisson GLMs reduce density estimation to regression
model ﬁtting, a familiar and ﬂexible inferential technology.

j k Z2

11 This is a modern version of the classic chi-squared goodness-of-ﬁt test.

−1.5−1.0−0.50.0−21−20−19−18−17FartherDimmer 0.5  1  1.5  2  2.5  3  3.5  4  4.5  5  5.5  6  6.5  7  7.5  8  8.5  9 l124

GLMs and Regression Trees

8.4 Regression Trees

The data set d for a regression problem typically consists of N pairs .xi ; yi /,

d D f.xi ; yi /; i D 1; 2; : : : ; Ng ;

(8.42)

where xi is a vector of predictors, or “covariates,” taking its value in some
space X , and yi is the response, assumed to be univariate in what follows.
The regression algorithm, perhaps a Poisson GLM, inputs d and outputs
a rule rd .x/: for any value of x in X , rd .x/ produces an estimate Oy for a

possible future value of y,

Oy D rd .x/:

(8.43)

In the logistic regression example (8.8), rd .x/ is O(cid:25).x/.
There are three principal uses for the rule rd .x/.

1 For prediction: Given a new observation of x, but not of its correspond-
ing y, we use Oy D rd .x/ to predict y. In the spam example, the 57
keywords of an incoming message could be used to predict whether or
not it is spam.12 (See Chapter 12.)
2 For estimation: The rule rd .x/ describes a “regression surface” OS over
X ,
(8.44)
The right panel of Figure 8.4 shows OS for the galaxy example. OS can be
thought of as estimating S, the true regression surface, often deﬁned in
the form of conditional expectation,

OS D frd .x/; x 2

g :

X

S D fEfyjxg; x 2

g :

X

X

g.)

(8.45)
(In a dichotomous situation where y is coded as 0 or 1, S D fPrfy D
1jxg; x 2
For estimation, but not necessarily for prediction, we want OS to accu-
rately portray S. The right panel of Figure 8.4 shows the estimated galaxy
density still increasing monotonically in dimmer at the top end of the
truncation region, but not so in farther, perhaps an important clue for
directing future search counts.13 The ﬂat region in the kidney function re-
gression curve of Figure 1.2 makes almost no difference to prediction, but
is of scientiﬁc interest if accurate.

12 Prediction of dichotomous outcomes is often called “classiﬁcation.”
13 Physicists call a regression-based search for new objects “bump hunting.”

8.4 Regression Trees

125

3 For explanation: The 10 predictors for the diabetes data of Section 7.3,
age, sex, bmi,. . . , were selected by the researcher in the hope of ex-
plaining the etiology of diabetes progression. The relative contribution
of the different predictors to rd .x/ is then of interest. How the regression
surface is composed is of prime concern in this use, but not in use 1 or 2
above.

The three different uses of rd .x/ raise different inferential questions.
Use 1 calls for estimates of prediction error. In a dichotomous situation
such as the spam study, we would want to know both error probabilities

Prf Oy D spamjy D hamg

and

Prf Oy D hamjy D spamg :

(8.46)

For estimation, the accuracy of rd .x/ as a function of x, perhaps in stan-
dard deviation terms,

sd.x/ D sd. Oyjx/;

(8.47)
would tell how closely OS approximates S. Use 3, explanation, requires
more elaborate inferential tools, saying for example which of the regression
coefﬁcients ˛i in (8.19) can safely be set to zero.

Figure 8.6 Left a hypothetical regression tree based on two
predictors X1 and X2. Right corresponding regression surface.

Regression trees use a simple but intuitively appealing technique to form
a regression surface: recursive partitioning. The left panel of Figure 8.6
illustrates the method for a hypothetical situation involving two predictor
variables, X1 and X2 (e.g., r and m in the galaxy example). At the top of

|t1t2t3t4R1R1R2R2R3R3R4R4R5R5X1X1X1X2X2X2X1≤t1X2≤t2X1≤t3X2≤t4126

GLMs and Regression Trees

the tree, the sample population of N cases has been split into two groups:
those with X1 equal to or less than value t1 go to the left, those with X1 >
t1 to the right. The leftward group is itself then divided into two groups
depending on whether or not X2  t2. The division stops there, leaving
two terminal nodes R1 and R2. On the tree’s right side, two other splits
give terminal nodes R3, R4, and R5.
A prediction value OyRj is attached to each terminal node Rj . The predic-
tion Oy applying to a new observation x D .x1; x2/ is calculated by starting
x at the top of the tree and following the splits downward until a terminal
node, and its attached prediction OyRj , is reached. The corresponding re-
gression surface OS is shown in the right panel of Figure 8.6 (here the OyRj
happen to be in ascending order).

Various algorithmic rules are used to decide which variable to split and
which splitting value t to take at each step of the tree’s construction. Here
is the most common method: suppose at step k of the algorithm, groupk of
Nk cases remains to be split, those cases having mean and sum of squares

yi =Nk

and s2
k

.yi (cid:0) mk/2:

(8.48)

mk D X

i2groupk

D X

i2groupk

Dividing groupk into groupk;left and groupk;right produces means mk;left and
mk;right, and corresponding sums of squares s2
k;right. The algorithm
proceeds by choosing the splitting variable Xk and the threshold tk to min-
imize

k;left and s2

C s2

k;right:

s2
k;left

(8.49)

7

In other words, it splits groupk into two groups that are as different from
each other as possible.

Cross-validation estimates of prediction error, Chapter 12, are used to
decide when the splitting process should stop. If groupk is not to be further
divided, it becomes terminal node Rk, with prediction value OyRk
D mk.
None of this would be feasible without electronic computation, but even
quite large prediction problems can be short work for modern computers.
Figure 8.7 shows a regression tree analysis14 of the spam data, Ta-
ble 8.3. There are seven terminal nodes, labeled 0 or 1 for decision ham
or spam. The leftmost node, say R1, is a 0, and contains 2462 ham cases
and 275 spam (compared with 2788 and 1813 in the full data set). Starting
at the top of the tree, R1 is reached if it has a low proportion of $ symbols

14 Using the R program rpart, in classiﬁcation mode, employing a different splitting rule

than the version based on (8.49).

8.4 Regression Trees

127

Figure 8.7 Regression tree on the spam data; 0 D ham, 1 D
spam. Error rates: ham 5.2%, spam 17.4%. Captions indicate
leftward (ham) moves.

char$, a low proportion of the word remove, and a low proportion of
exclamation marks char!.

Regression trees are easy to interpret (“Too many dollar signs means
spam!”) seemingly suiting them for use 3, explanation. Unfortunately, they
are also easy to overinterpret, with a reputation for being unstable in prac-
tice. Discontinuous regression surfaces OS, as in Figure 8.6, disqualify them
for use 2, estimation. Their principal use in what follows will be as key
parts of prediction algorithms, use 1. The tree in Figure 8.6 has apparent
error rates (8.46) of 5.2% and 17.4%. This can be much improved upon
by “bagging” (bootstrap aggregation), Chapters 17 and 20, and by other
computer-intensive techniques.

Compared with generalized linear models, regression trees represent a
break from classical methodology that is more stark. First of all, they are
totally nonparametric; bigger but less structured data sets have promoted
nonparametrics in twenty-ﬁrst-century statistics. Regression trees are more
computer-intensive and less efﬁcient than GLMs but, as will be seen in Part
III, the availability of massive data sets and modern computational equip-

|char$< −0.0826remove< −0.1513char!< 0.1335capruntot< −0.3757free< 0.7219hp>=−0.0894502462/2750129/3211/20133/189130/300063/7170/990Figure 8.7 . Regression Tree, Spam Data: 0=nonspam,  1=spam,Error Rates: nonspam 5.2%,  spam 17.4%Captions indicate leftward (nonspam) moves128

GLMs and Regression Trees

ment has diminished the appeal of efﬁciency in favor of easy assumption-
free application.

8.5 Notes and Details

Computer-age algorithms depend for their utility on statistical computing
languages. After a period of evolution, the language S (Becker et al., 1988)
and its open-source successor R (R Core Team, 2015), have come to dom-
inate applied practice.15 Generalized linear models are available from a
single R command, e.g.,

glm(y(cid:24)X,family=binomial)

for logistic regression (Chambers and Hastie, 1993), and similarly for re-
gression trees and hundreds of other applications.

The classic version of bioassay, probit analysis, assumes that each test
animal has its own lethal dose level X, and that the population distribution
of X is normal,

PrfX  xg D ˆ.˛0 C ˛1x/

(8.50)

for unknown parameters .˛0; ˛1/ and standard normal cdf ˆ. Then the
number of animals dying at dose x is binomial Bi.nx; (cid:25)x/ as in (8.3), with
(cid:25)x D ˆ.˛0 C ˛1x/, or

(cid:0)1.(cid:25)x/ D ˛0 C ˛1x:

ˆ

(8.51)
Replacing the standard normal cdf ˆ.z/ with the logistic cdf 1=.1 C e
(cid:0)z/
(which resembles ˆ), changes (8.51) into logistic regression (8.5). The
usual goal of bioassay was to estimate “LD50,” the dose lethal to 50%
of the test population; it is indicated by the open circle in Figure 8.2.

Cox (1970), the classic text on logistic regression, lists Berkson (1944)
as an early practitioner. Wedderburn (1974) is credited with generalized
linear models in McCullagh and Nelder’s inﬂuential text of that name, ﬁrst
edition 1983; Birch (1964) developed an important and suggestive special
case of GLM theory.

The twenty-ﬁrst century has seen an efﬂorescence of computer-based re-
gression techniques, as described extensively in Hastie et al. (2009). The
discussion of regression trees here is taken from their Section 9.2, including
our Figure 8.6. They use the spam data as a central example; it is publicly

15 Previous computer packages such as SAS and SPSS continue to play a major role in

application areas such as the social sciences, biomedical statistics, and the
pharmaceutical industry.

8.5 Notes and Details

129

available at ftp.ics.uci.edu. Breiman et al. (1984) propelled regres-
sion trees into wide use with their CART algorithm.
1 [p. 112] Sufﬁciency as in (8.13). The Fisher–Neyman criterion says that if
f˛.x/ D h˛.S.x//g.x/, when g.(cid:1)/ does not depend on ˛, then S.x/ is
sufﬁcient for ˛.

2 [p. 118] Equation (8.28). From (8.24)–(8.25) we have the log likelihood

function

with sufﬁcient statistic z D X
ing with respect to ˛,

0

l˛.y/ D ˛

z (cid:0)  .˛/

y and  .˛/ DPN

0

(8.52)

iD1 (cid:13).x

0
i ˛/. Differentiat-

0

y (cid:0) X

P
l˛.y/ D z (cid:0) P .˛/ D X

0
where we have used d(cid:13)=d (cid:21) D (cid:22)(cid:21) (5.55), so P(cid:13) .x
(8.53) says P
matrix R

l˛.y/ with respect to ˛ is

l˛.y/ D X

0

.y (cid:0) (cid:22).˛//, verifying the MLE equation (8.28).

i ˛/ D x
0

0
i (cid:22)i .˛/. But

3 [p. 118] Concavity of the log likelihood. From (8.53), the second derivative

(cid:22).˛/;

(8.53)

(8.54)

(5.57)–(5.59). But z D X

(cid:0) R .˛/ D (cid:0) cov˛.z/;
0
y has
cov˛.z/ D X

0

(8.55)
a positive deﬁnite p (cid:2) p matrix, verifying the concavity of l˛.y/ (which in
fact applies to any exponential family, not only GLMs).

†.˛/X ;

4 [p. 118] Formula (8.30). The sufﬁcient statistic z has mean vector and co-

variance matrix

z (cid:24) .ˇ; V˛/;

(8.56)

†.˛/X (8.55). Using (5.60), the

with ˇ D E˛fzg (5.58) and V˛ D X
ﬁrst-order Taylor series for O˛ as a function of z is
˛ .z (cid:0) ˇ/:
(cid:0)1

:D ˛ C V

O˛

0

(8.57)
(cid:0)2 rather
Taken literally, (8.57) gives (8.30). In the OLS formula, we have (cid:27)
than (cid:27) 2 since the natural parameter ˛ for the Normal entry in Table 8.4 is
(cid:22)=(cid:27) 2.

5 [p. 118] Formula (8.33). This formula, attributed to Hoeffding (1965), is a
key result in the interpretation of GLM ﬁtting. Applying deﬁnition (8.31)

130

GLMs and Regression Trees

to family (8.32) gives

1

2

D.(cid:21)1; (cid:21)2/ D E(cid:21)1

f.(cid:21)1 (cid:0) (cid:21)2/y (cid:0) Œ(cid:13).(cid:21)1/ (cid:0) (cid:13).(cid:21)2/g

D .(cid:21)1 (cid:0) (cid:21)2/(cid:22)1 (cid:0) Œ(cid:13).(cid:21)1/ (cid:0) (cid:13).(cid:21)2/ :

(8.58)

If (cid:21)1 is the MLE O
0 D d Œlog f(cid:21).y/=d (cid:21) D y (cid:0) P(cid:13) .(cid:21)/ D y (cid:0) (cid:22)(cid:21)), giving16

(cid:21) then (cid:22)1 D y (from the maximum likelihood equation

(cid:16)O

 D(cid:16)O

(cid:21) (cid:0) (cid:21)



y (cid:0)h

(cid:16)O

(cid:21)

 (cid:0) (cid:13).(cid:21)/
i

D

(cid:21); (cid:21)

(8.59)
for any choice of (cid:21). But the right-hand side of (8.59) is (cid:0) logŒf(cid:21).y/=fy.y/,
verifying (8.33).

(cid:13)

6 [p. 120] Table 8.5. The galaxy counts are from Loh and Spillar’s 1988

redshift survey, as discussed in Efron and Petrosian (1992).

7 [p. 126] Criteria (8.49). Abbreviating “left” and “right” by l and r, we

1

2

have

D s2

kl

C s2

kr

s2
k

C Nkl Nkr

Nk

.mkl (cid:0) mkr /2;

(8.60)

with Nkl and Nkr the subgroup sizes, showing that minimizing (8.49) is
the same as maximizing the last term in (8.60). Intuitively, a good split is
one that makes the left and right groups as different as possible, the ideal
being all 0s on the left and all 1s on the right, making the terminal nodes
“pure.”

16 In some cases O(cid:21) is undeﬁned; for example, when y D 0 for a Poisson response,

O(cid:21) D log.y/ which is undeﬁned. But, in (8.59), we assume that O(cid:21)y D 0. Similarly for

binary y and the binomial family.

9

Survival Analysis and the EM Algorithm

Survival analysis had its roots in governmental and actuarial statistics,
spanning centuries of use in assessing life expectancies, insurance rates,
and annuities. In the 20 years between 1955 and 1975, survival analysis
was adapted by statisticians for application to biomedical studies. Three
of the most popular post-war statistical methodologies emerged during
this period: the Kaplan–Meier estimate, the log-rank test,1 and Cox’s pro-
portional hazards model, the succession showing increased computational
demands along with increasingly sophisticated inferential justiﬁcation. A
connection with one of Fisher’s ideas on maximum likelihood estimation
leads in the last section of this chapter to another statistical method that has
“gone platinum,” the EM algorithm.

9.1 Life Tables and Hazard Rates

An insurance company’s life table appears in Table 9.1, showing its number
of clients (that is, life insurance policy holders) by age, and the number of
deaths during the past year in each age group,2 for example ﬁve deaths
among the 312 clients aged 59. The column labeled OS is of great interest
to the company’s actuaries, who have to set rates for new policy holders.
It is an estimate of survival probability: probability 0.893 of a person aged
30 (the beginning of the table) surviving past age 59, etc. OS is calculated
according to an ancient but ingenious algorithm.

Let X represent a typical lifetime, so

fi D PrfX D ig

(9.1)

1 Also known as the Mantel–Haenszel or Cochran–Mantel–Haenszel test.
2 The insurance company is ﬁctitious but the deaths y are based on the true 2010 rates for

US men, per Social Security Administration data.

131

Survival Analysis and the EM Algorithm

h D hazard rate y=n, OS D

Age

n

Age

Oh

Oh

132
Table 9.1 Insurance company life table; at each age, n D number of
policy holders, y D number of deaths, O
survival probability estimate (9.6).
OS
1.000
1.000
1.000
1.000
1.000
.986
.986
.986
.986
.986
.986
.986
.986
.986
.986
.976
.969
.964
.964
.956
.948
.945
.940
.933
.927
.925
.916
.910
.908
.893

116
44
95
97
120
71
125
122
82
113
79
90
154
103
144
192
153
179
210
259
225
346
370
568
1081
1042
1094
597
359
312

OS
.889
.871
.849
.830
.820
.820
.798
.785
.755
.745
.723
.704
.696
.689
.676
.648
.611
.611
.578
.528
.506
.481
.431
.385
.358
.321
.277
.224
.192
.168

.000
.000
.000
.000
.000
.014
.000
.000
.000
.000
.000
.000
.000
.000
.000
.010
.007
.006
.000
.008
.009
.003
.005
.007
.007
.002
.009
.007
.003
.016

.004
.020
.026
.022
.012
.000
.027
.016
.039
.013
.030
.026
.011
.011
.018
.041
.058
.000
.053
.087
.042
.048
.104
.107
.071
.103
.137
.191
.143
.127

n

231
245
196
180
170
114
185
127
127
158
100
155
92
90
110
122
138
46
75
69
95
124
67
112
113
116
124
110
63
79

y

0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
2
1
1
0
2
2
1
2
4
8
2
10
4
1
5

y

1
5
5
4
2
0
5
2
5
2
3
4
1
1
2
5
8
0
4
6
4
6
7
12
8
12
17
21
9
10

30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59

60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89

is the probability of dying at age i, and

Si DX

j(cid:21)i

fj D PrfX (cid:21) ig

(9.2)

is the probability of surviving past age i (cid:0) 1. The hazard rate at age i is by

9.1 Life Tables and Hazard Rates

133

deﬁnition

Sij D jY

hi D fi =Si D PrfX D ijX (cid:21) ig;

(9.3)

the probability of dying at age i given survival past age i (cid:0) 1.
A crucial observation is that the probability Sij of surviving past age j
given survival past age i (cid:0) 1 is the product of surviving each intermediate
year,

.1 (cid:0) hk/ D PrfX > jjX (cid:21) igI

kDi

(9.4)
ﬁrst you have to survive year i, probability 1 (cid:0) hi; then year i C 1, proba-
bility 1 (cid:0) hiC1, etc., up to year j , probability 1 (cid:0) hj . Notice that Si (9.2)
equals S1;i(cid:0)1.
OS in Table 9.1 is an estimate of Sij for i D 30. First, each hi was
estimated as the binomial proportion of the number of deaths yi among the
ni clients,

and then we set

O
hi D yi =ni ;
(cid:16)
1 (cid:0) O

OS30;j D jY

hk

kD30



:

(9.5)

(9.6)

The insurance company doesn’t have to wait 50 years to learn the proba-
bility of a 30-year-old living past 80 (estimated to be 0.506 in the table).
One year’s data sufﬁces.3

Hazard rates are more often described in terms of a continuous positive
random variable T (often called “time”), having density function f .t / and
“reverse cdf,” or survival function,

S.t / DZ 1

f .x/ dx D PrfT (cid:21) tg:

The hazard rate

t

h.t / D f .t /=S.t /

satisﬁes

:D PrfT 2 .t; t C dt /jT (cid:21) tg
for dt ! 0, in analogy with (9.3). The analog of (9.4) is

h.t /dt

3 Of course the estimates can go badly wrong if the hazard rates change over time.

(9.7)

(9.8)

(9.9)

1

(cid:26)(cid:0)Z t

0

(cid:0)t =c

(cid:27)

(9.10)

(9.11)

(9.12)

(9.13)

134

Survival Analysis and the EM Algorithm
PrfT (cid:21) t1jT (cid:21) t0g D exp

h.x/ dx

(cid:26)(cid:0)Z t1

t0
so in particular the reverse cdf (9.7) is given by

(cid:27)

S.t / D exp

h.x/ dx

:

A one-sided exponential density
f .t / D .1=c/e

for t (cid:21) 0

has S.t / D expf(cid:0)t =cg and constant hazard rate

h.t / D 1=c:

The name “memoryless” is quite appropriate for density (9.12): having
survived to any time t, the probability of surviving dt units more is always
the same, about 1 (cid:0) dt =c, no matter what t is. If human lifetimes were
exponential there wouldn’t be old or young people, only lucky or unlucky
ones.

9.2 Censored Data and the Kaplan–Meier Estimate

Table 9.2 reports the survival data from a randomized clinical trial run by
NCOG (the Northern California Oncology Group) comparing two treat-
ments for head and neck cancer: Arm A, chemotherapy, versus Arm B,
chemotherapy plus radiation. The response for each patient is survival time
in days. The C sign following some entries indicates censored data, that is,
survival times known only to exceed the reported value. These are patients
“lost to followup,” mostly because the NCOG experiment ended with some
of the patients still alive.

This is what the experimenters hoped to see of course, but it compli-
cates the comparison. Notice that there is more censoring in Arm B. In
the absence of censoring we could run a simple two-sample test, maybe
Wilcoxon’s test, to see whether the more aggressive treatment of Arm B
was increasing the survival times. Kaplan–Meier curves provide a graph-
ical comparison that takes proper account of censoring. (The next section
describes an appropriate censored data two-sample test.) Kaplan–Meier
curves have become familiar friends to medical researchers, a lingua franca
for reporting clinical trial results.

Life table methods are appropriate for censored data. Table 9.3 puts the
Arm A results into the same form as the insurance study of Table 9.1, now

9.2 Censored data and Kaplan–Meier

135

Table 9.2 Censored survival times in days, from two arms of the NCOG
study of head/neck cancer.

7
108
149
218
405
1116+

37
133
195
528+
1331+

34
112
154
225
417
1146

84
140
209
547+
1557

Arm A: Chemotherapy

42
129
157
241
420
1226+

63
133
160
248
440
1349+

64
133
160
273
523
1412+

74+
139
165
277
523+
1417

83
140
173
279+
583

84
140
176
297
594

91
146
185+
319+
1101

Arm B: ChemotherapyCRadiation

92
146
249
613+
1642+

94
155
281
633
1771+

110
159
319
725
1776

112
169+
339
759+
1897+

119
173
432
817
2023+

127
179
469
1092+
2146+

130
194
519
1245+
2297+

with the time unit being months. Of the 51 patients enrolled4 in Arm A,
y1 D 1 was observed to die in the ﬁrst month after treatment; this left 50 at
risk, y2 D 2 of whom died in the second month; y3 D 5 of the remaining
48 died in their third month after treatment, and one was lost to followup,
this being noted in the l column of the table, leaving n4 D 40 patients “at
risk” at the beginning of month 5, etc.
OS here is calculated as in (9.6) except starting at time 1 instead of 30.
There is nothing wrong with this estimate, but binning the NCOG survival
data by months is arbitrary. Why not go down to days, as the data was
originally presented in Table 9.2? A Kaplan–Meier survival curve is the
limit of life table survival estimates as the time unit goes to zero.

Observations zi for censored data problems are of the form

zi D .ti ; di /;

(9.14)

where ti equals the observed survival time while di indicates whether or
not there was censoring,
di D

1 if death observed
0 if death not observed

(

(9.15)

4 The patients were enrolled at different calendar times, as they entered the study, but for

each patient “time zero” in the table is set at the beginning of his or her treatment.

136

Survival Analysis and the EM Algorithm

Table 9.3 Arm A of the NCOG head/neck cancer study, binned by month;
n D number at risk, y D number of deaths, l D lost to followup, h D
hazard rate y=n; OS D life table survival estimate.

Month

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

n

51
50
48
42
40
32
25
24
21
19
16
15
15
15
12
11
11
11
9
9
7
7
7
7

y

1
2
5
2
8
7
0
3
2
2
0
0
0
3
1
0
0
1
0
2
0
0
0
0

l

0
0
1
0
0
0
1
0
0
1
1
0
0
0
0
0
0
1
0
0
0
0
0
0

h

.020
.040
.104
.048
.200
.219
.000
.125
.095
.105
.000
.000
.000
.200
.083
.000
.000
.091
.000
.222
.000
.000
.000
.000

OS
.980
.941
.843
.803
.642
.502
.502
.439
.397
.355
.355
.355
.355
.284
.261
.261
.261
.237
.237
.184
.184
.184
.184
.184

Month

n

y

25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47

7
7
7
7
7
7
7
7
7
7
7
7
7
5
4
4
4
3
3
3
3
2
2

0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1

l

0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
1

h

.000
.000
.000
.000
.000
.000
.000
.000
.000
.000
.000
.000
.143
.200
.000
.000
.000
.000
.000
.000
.000
.000
.500

OS
.184
.184
.184
.184
.184
.184
.184
.184
.184
.184
.184
.184
.158
.126
.126
.126
.126
.126
.126
.126
.126
.126
.063

(so di D 0 corresponds to a C in Table 9.2). Let

t.1/ < t.2/ < t.3/ < : : : < t.n/

(9.16)

2

denote the ordered survival times,5 censored or not, with corresponding
indicator d.k/ for t.k/. The Kaplan–Meier estimate for survival probability
d.k/
S.j / D PrfX > t.j /g is then the life table estimate

 n (cid:0) k

OS.j / DY

kj

n (cid:0) k C 1

:

(9.17)

5 Assuming no ties among the survival times, which is convenient but not crucial for what

follows.

9.2 Censored data and Kaplan–Meier

137

OS jumps downward at death times tj , and is constant between observed
deaths.

Figure 9.1 NCOG Kaplan–Meier survival curves; lower Arm A
(chemotherapy only); upper Arm B (chemotherapyCradiation).
Vertical lines indicate approximate 95% conﬁdence intervals.

The Kaplan–Meier curves for both arms of the NCOG study are shown
in Figure 9.1. Arm B, the more aggressive treatment, looks better: its 50%
survival estimate occurs at 324 days, compared with 182 days for Arm A.
The answer to the inferential question—is B really better than A or is this
just random variability?—is less clear-cut.
The accuracy of OS.j / can be estimated from Greenwood’s formula  for 3
its standard deviation (now back in life table notation),

(cid:16) OS.j /

 D OS.j /

sd

24X

kj

351=2

yk

nk.nk (cid:0) yk/

:

(9.18)

The vertical bars in Figure 9.1 are approximate 95% conﬁdence limits for
the two curves based on Greenwood’s formula. They overlap enough to cast
doubt on the superiority of Arm B at any one choice of “days,” but the two-
sample test of the next section, which compares survival at all timepoints,
will provide more deﬁnitive evidence.

Life tables and the Kaplan–Meier estimate seem like a textbook example
of frequentist inference as described in Chapter 2: a useful probabilistic

02004006008001000120014000.00.20.40.60.81.0DaysSurvivalArm A: chemotherapy onlyArm B: chemotherapy + radiation4

138

Survival Analysis and the EM Algorithm

result is derived (9.4), and then implemented by the plug-in principle (9.6).
There is more to the story though, as discussed below.

Life table curves are nonparametric, in the sense that no particular re-
lationship is assumed between the hazard rates hi. A parametric approach
can greatly improve the curves’ accuracy.  Reverting to the life table form
of Table 9.3, we assume that the death counts yk are independent binomi-
als,

(9.19)
and that the logits (cid:21)k D logfhk=.1 (cid:0) hk/g satisfy some sort of regression
equation

yk

(9.20)
as in (8.22). A cubic regression for instance would set xk D .1; k; k2; k3/
0
for the kth row of X, with X 47 (cid:2) 4 for Table 9.3.

ind(cid:24) Bi.nk; hk/;

(cid:21) D X ˛;

Figure 9.2 Parametric hazard rate estimates for the NCOG study.
Arm A, black curve, has about 2.5 times higher hazard than
Arm B for all times more than a year after treatment. Standard
errors shown at 15 and 30 months.

The parametric hazard-rate estimates in Figure 9.2 were instead based

on a “cubic-linear spline,”

xk D(cid:0)1; k; .k (cid:0) 11/2(cid:0); .k (cid:0) 11/3(cid:0)(cid:1)0

(9.21)
where .k (cid:0) 11/(cid:0) equals k (cid:0) 11 for k  11, and 0 for k (cid:21) 11. The vector

;

0102030400.000.050.100.15MonthsDeaths per MonthArm A: chemotherapy onlyArm B: chemotherapy + radiation9.3 The Log-Rank Test

139
(cid:21) D X ˛ describes a curve that is cubic for k  11, linear for k (cid:21) 11,
.(cid:16)
and joined smoothly at 11. The logistic regression maximum likelihood
estimate O˛ produced hazard rate curves
O
1 C e
hk D 1

O˛
as in (8.8). The black curve in Figure 9.2 traces O
red curve is that for Arm B, ﬁt separately.

hk for Arm A, while the

(cid:0)x

0

k

(9.22)

Comparison in terms of hazard rates is more informative than the sur-
vival curves of Figure 9.1. Both arms show high initial hazards, peaking at
ﬁve months, and then a long slow decline.6 Arm B hazard is always below
Arm A, in a ratio of about 2.5 to 1 after the ﬁrst year. Approximate 95%
conﬁdence limits, obtained as in (8.30), don’t overlap, indicating superior-
ity of Arm B at 15 and 30 months after treatment.

In addition to its frequentist justiﬁcation, survival analysis takes us into
the Fisherian realm of conditional inference, Section 4.3. The yk’s in model
(9.19) are considered conditionally on the nk’s, effectively treating the nk
values in Table 9.3 as ancillaries, that is as ﬁxed constants, by themselves
containing no statistical information about the unknown hazard rates. We
will examine this tactic more carefully in the next two sections.

9.3 The Log-Rank Test

A randomized clinical trial, interpreted by a two-sample test, remains the
gold standard of medical experimentation. Interpretation usually involves
Student’s two-sample t-test or its nonparametric cousin Wilcoxon’s test,
but neither of these is suitable for censored data. The log-rank test 
employs an ingenious extension of life tables for the nonparametric two-
sample comparison of censored survival data.

Table 9.4 compares the results of the NCOG study for the ﬁrst six months7
after treatment. At the beginning8 of month 1 there were 45 patients “at
risk” in Arm B, none of whom died, compared with 51 at risk and 1 death
in Arm A. This left 45 at risk in Arm B at the beginning of month 2, and
50 in Arm A, with 1 and 2 deaths during the month respectively. (Losses

5

6 The cubic–linear spline (9.21) is designed to show more detail in the early months,

where there is more available patient data and where hazard rates usually change more
quickly.

7 A month is deﬁned here as 365/12=30.4 days.
8 The “beginning of month 1” is each patient’s initial treatment time, at which all 45

patients ever enrolled in Arm B were at risk, that is, available for observation.

140

Survival Analysis and the EM Algorithm

Table 9.4 Life table comparison for the ﬁrst six months of the NCOG
study. For example, at the beginning of the sixth month after treatment,
there were 33 remaining Arm B patients, of whom 4 died during the
month, compared with 32 at risk and 7 dying in Arm A. The conditional
expected number of deaths in Arm A, assuming the null hypothesis of
equal hazard rates in both arms, was 5.42, using expression (9.24).

Month

Arm B

Arm A

At risk Died

At risk Died

Expected number
Arm A deaths

1
2
3
4
5
6

45
45
44
43
38
33

0
1
1
5
5
4

51
50
48
42
40
32

1
2
5
2
8
7

.53
1.56
3.13
3.46
6.67
5.42

to followup were assumed to occur at the end of each month; there was 1
such at the end of month 3, reducing the number at risk in Arm A to 42 for
month 4.)

The month 6 data is displayed in two-by-two tabular form in Table 9.5,
showing the notation used in what follows: nA for the number at risk in
Arm A, nd for the number of deaths, etc.; y indicates the number of Arm A
deaths. If the marginal totals nA; nB ; nd ; and ns are given, then y deter-
mines the other three table entries by subtraction, so we are not losing any
information by focusing on y.

Table 9.5 Two-by-two display of month-6 data for the NCOG study. E is
the expected number of Arm A deaths assuming the null hypothesis of
equal hazard rates (last column of Table 9.4).

Arm A

Arm B

Died
y D 7
E D 5:42

4
D 11

nd

Survived

25

29

ns D 54

nA D 32
nB D 33
n D 65

Consider the null hypothesis that the hazard rates (9.3) for month 6 are

9.3 The Log-Rank Test

the same in Arm A and Arm B,

H0.6/ W hA6 D hB6:
Under H0.6/, y has mean E and variance V ,

E D nAnd =n
V D nAnB nd ns

ı(cid:2)n2.n (cid:0) 1/(cid:3) ;

141

(9.23)

(9.24)

as calculated according to the hypergeometric distribution. E D 5:42 and 6
V D 2:28 in Table 9.5.

We can form a two-by-two table for each of the N D 47 months of the
NCOG study, calculating yi ; Ei, and Vi for month i. The log-rank statistic
Z is then deﬁned to be

7

Z D NX

iD1

.yi (cid:0) Ei /

!1=2

,  NX

iD1

Vi

:

(9.25)

The idea here is simple but clever. Each month we test the null hypothesis
of equal hazard rates

H0.i / W hAi D hBi :

(9.26)
The numerator yi (cid:0) Ei has expectation 0 under H0.i /, but, if hAi is greater
than hBi, that is, if treatment B is superior, then the numerator has a pos-
itive expectation. Adding up the numerators gives us power to detect a
general superiority of treatment B over A, against the null hypothesis of
equal hazard rates, hAi D hBi for all i.
For the NCOG study, binned by months,
Ei D 32:9;

Vi D 16:0;

yi D 42;

NX

(9.27)

NX

iD1

NX

iD1

iD1

giving log-rank test statistic

Z D 2:27:

Asymptotic calculations based on the central limit theorem suggest

Z P(cid:24)

N .0; 1/

(9.28)

(9.29)

under the null hypothesis that the two treatments are equally effective, i.e.,
that hAi D hBi for i D 1; 2; : : : ; N . In the usual interpretation, Z D
2:27 is signiﬁcant at the one-sided 0.012 level, providing moderately strong
evidence in favor of treatment B.

An impressive amount of inferential guile goes into the log-rank test.

142

Survival Analysis and the EM Algorithm

1 Working with hazard rates instead of densities or cdfs is essential for

survival data.

2 Conditioning at each period on the numbers at risk, nA and nB in Ta-
ble 9.5, ﬁnesses the difﬁculties of censored data; censoring only changes
the at-risk numbers in future periods.

3 Also conditioning on the number of deaths and survivals, nd and ns
in Table 9.5, leaves only the univariate statistic y to interpret at each
period, which is easily done through the null hypothesis of equal hazard
rates (9.26).
4 Adding the discrepancies yi (cid:0) Ei in the numerator of (9.25) (rather than
say, adding the individual Z values Zi D .yi (cid:0) Ei /=V 1=2
, or adding the
i values) accrues power for the natural alternative hypothesis “hAi >
Z2
hBi for all i,” while avoiding destabilization from small values of Vi.

i

Each of the four tactics had been used separately in classical applica-
tions. Putting them together into the log-rank test was a major inferential
accomplishment, foreshadowing a still bigger step forward, the propor-
tional hazards model, our subject in the next section.

Conditional inference takes on an aggressive form in the log-rank test.
Let Di indicate all the data except yi available at the end of the ith period.
For month 6 in the NCOG study, D6 includes all data for months 1–5 in
Table 9.4, and the marginals nA; nB ; nd ; and ns in Table 9.5, but not the y
value for month 6. The key assumption is that, under the null hypothesis of
equal hazard rates (9.26),

yijDi

ind(cid:24) .Ei ; Vi /;

(9.30)

“ind” here meaning that the yi’s can be treated as independent quantities
with means and variances (9.24). In particular, we can add the variances
Vi to get the denominator of (9.25). (A “partial likelihood” argument, de-
scribed in the endnotes, justiﬁes adding the variances.)
The purpose of all this Fisherian conditioning is to simplify the infer-
ence: the conditional distribution yijDi depends only on the hazard rates
hAi and hBi; “nuisance parameters,” relating to the survival times and cen-
soring mechanism of the data in Table 9.2, are hidden away. There is a price
to pay in testing power, though usually a small one. The lost-to-followup
values l in Table 9.3 have been ignored, even though they might contain
useful information, say if all the early losses occurred in one arm.

9.4 The Proportional Hazards Model

143

9.4 The Proportional Hazards Model

The Kaplan–Meier estimator is a one-sample device, dealing with data
coming from a single distribution. The log-rank test makes two-sample
comparisons. Proportional hazards ups the ante to allow for a full regres-
sion analysis of censored data. Now the individual data points zi are of the
form

zi D .ci ; ti ; di /;

(9.31)

where ti and di are observed survival time and censoring indicator, as in
(9.14)–(9.15), and ci is a known 1 (cid:2) p vector of covariates whose effect
on survival we wish to assess. Both of the previous methods are included
here: for the log-rank test, ci indicates treatment, say ci equals 0 or 1 for
Arm A or Arm B, while ci is absent for Kaplan–Meier.
Table 9.6 Pediatric cancer data, ﬁrst 20 of 1620 children. Sex 1 D male,
2 D female; race 1 D white, 2 D nonwhite; age in years; entry D
calendar date of entry in days since July 1, 2001; far D home distance
from treatment center in miles; t D survival time in days; d D 1 if death
observed, 0 if not.

sex

race

age

entry

far

t

d

1
2
2
2
1
2
2
2
1
2
1
1
1
1
2
1
2
1
1
2

1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2

2.50
10.00
18.17
3.92
11.83
11.17
5.17
10.58
1.17
6.83
13.92
5.17
2.50
.83
15.50
17.83
3.25
10.75
18.08
5.83

710
1866
2531
2210
875
1419
1264
670
1518
2101
1239
518
1849
2758
2004
986
1443
2807
1229
2727

108
38
100
100
78
0
28
120
73
104
0
117
99
38
12
65
58
42
23
23

325
1451
221
2158
760
168
2976
1833
131
2405
969
1894
193
1756
682
1835
2993
1616
1302
174

0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1

144

Survival Analysis and the EM Algorithm

Medical studies regularly produce data of form (9.31). An example, the
pediatric cancer data, is partially listed in Table 9.6. The ﬁrst 20 of n D
1620 cases are shown. There are ﬁve explanatory covariates (deﬁned in the
table’s caption): sex, race, age at entry, calendar date of entry into
the study, and far, the distance of the child’s home from the treatment
center. The response variable t is survival in days from time of treatment
until death. Happily, only 160 of the children were observed to die (d D
1). Some left the study for various reasons, but most of the d D 0 cases
were those children still alive at the end of the study period. Of particular
interest was the effect of far on survival. We wish to carry out a regression
analysis of this heavily censored data set.

The proportional hazards model assumes that the hazard rate hi .t / for

the ith individual (9.8) is

hi .t / D h0.t /ec

0
i ˇ :

(9.32)

Here h0.t / is a baseline hazard (which we need not specify) and ˇ is an
unknown p-parameter vector we want to estimate. For concise notation,
let

model (9.32) says that individual i’s hazard is a constant nonnegative factor
i times the baseline hazard. Equivalently, from (9.11), the ith survival
function Si .t / is a power of the baseline survival function S0.t /,

i D ec

i ˇI
0

Si .t / D S0.t /i :

(9.33)

(9.34)

Larger values of i lead to more quickly declining survival curves, i.e., to
worse survival (as in (9.11)).
Let J be the number of observed deaths, J D 160 here, occurring at

times

T.1/ < T.2/ < : : : < T.J /;

(9.35)

again for convenience assuming no ties.9 Just before time T.j / there is a
risk set of individuals still under observation, whose indices we denote by
Rj ,

(9.36)

Rj D fi W ti (cid:21) T.j /g:

Let ij be the index of the individual observed to die at time T.j /. The

key to proportional hazards regression is the following result.

9 More precisely, assuming only one event, a death, occurred at T.j /, with none of the

other individuals being lost to followup at exact time T.j /.

9.4 The Proportional Hazards Model

145

Lemma  Under the proportional hazards model (9.32), the conditional 8
probability, given the risk set Rj , that individual i in Rj is the one ob-
served to die at time T.j / is
Prfij D ij

(cid:30) X

(9.37)

0
k ˇ :

0
i ˇ

ec

Rjg D ec

k2

Rj

To put it in words, given that one person dies at time T.j /, the probability
0
i ˇ/, among the set of individuals
it is individual i is proportional to exp.c
at risk.

For the purpose of estimating the parameter vector ˇ in model (9.32),

we multiply factors (9.37) to form the partial likelihood

L.ˇ/ D JY

jD1

0@e

0
ij

c

ˇ

(cid:30) X

k2

Rj

1A :

0
k ˇ

ec

L.ˇ/ is then treated as an ordinary likelihood function, yielding an approx-
imately unbiased MLE-like estimate

(9.38)

(9.39)

O
ˇ D arg max

fL.ˇ/g ;

ˇ

ˇ P(cid:24) 

O

ˇ;

i(cid:0)1

h(cid:0)R

l

(cid:16) O

ˇ

with an approximate covariance obtained from the second-derivative ma-
trix of l.ˇ/ D log L.ˇ/, as in Section 4.3,

9

:

(9.40)

Table 9.7 shows the proportional hazards analysis of the pediatric can-
cer data, with the covariates age, entry, and far standardized to have
mean 0 and standard deviation 1 for the 1620 cases.10 Neither sex nor
race seems to make much difference. We see that age is a mildly signif-
icant factor, with older children doing better (i.e., the estimated regression
coefﬁcient is negative). However, the dramatic effects are date of entry
and far. Individuals who entered the study later survived longer—perhaps
the treatment protocol was being improved—while children living farther
away from the treatment center did worse.

Justiﬁcation of the partial likelihood calculations is similar to that for
the log-rank test, but there are some important differences, too: the pro-
portional hazards model is semiparametric (“semi” because we don’t have
to specify h0.t / in (9.32)), rather than nonparametric as before; and the

10 Table 9.7 was obtained using the R program coxph.

146

Survival Analysis and the EM Algorithm

Table 9.7 Proportional hazards analysis of pediatric cancer data (age,
entry and far standardized). Age signiﬁcantly negative, older children
doing better; entry very signiﬁcantly negative, showing hazard rate
declining with calendar date of entry; far very signiﬁcantly positive,
indicating worse results for children living farther away from the
treatment center. Last two columns show limits of approximate 95%
conﬁdence intervals for exp.ˇ/.

ˇ

(cid:0).023
sex
race
.282
(cid:0).235
age
entry (cid:0).460
far
.296

sd

z-value
(cid:0).142
.160
.169
1.669
.088 (cid:0)2.664
.079 (cid:0)5.855
4.117
.072

p-value

exp.ˇ/

Lower Upper

.887
.095
.008
.000
.000

.98
1.33
.79
.63
1.34

.71
.95
.67
.54
1.17

1.34
1.85
.94
.74
1.55

emphasis on likelihood has increased the Fisherian nature of the inference,
moving it further away from pure frequentism. Still more Fisherian is the
emphasis on likelihood inference in (9.38)–(9.40), rather than the direct
frequentist calculations of (9.24)–(9.25).

The conditioning argument here is less obvious than that for the Kaplan–
Meier estimate or the log-rank test. Has its convenience possibly come at
too high a price? In fact it can be shown that inference based on the partial
likelihood is highly efﬁcient, assuming of course the correctness of the
proportional hazards model (9.32).

9.5 Missing Data and the EM Algorithm

Censored data, the motivating factor for survival analysis, can be thought
of as a special case of a more general statistical topic, missing data. What’s
missing, in Table 9.2 for example, are the actual survival times for the
C cases, which are known only to exceed the tabled values. If the data
were not missing, we could use standard statistical methods, for instance
Wilcoxon’s test, to compare the two arms of the NCOG study. The EM algo-
rithm is an iterative technique for solving missing-data inferential problems
using only standard methods.
A missing-data situation is shown in Figure 9.3: n D 40 points have
been independently sampled from a bivariate normal distribution (5.12),

9.5 Missing Data and the EM Algorithm

147

Figure 9.3 Forty points from a bivariate normal distribution, the
last 20 with x2 missing (circled).

means .(cid:22)1; (cid:22)2/, variances .(cid:27) 2

1 ; (cid:27) 2

2 /, and correlation (cid:26),

 

!

x1i
x2i

ind(cid:24)

N2

  

!

;

 (cid:27) 2

1

(cid:27)1(cid:27)2(cid:26)

(cid:22)1
(cid:22)2

(cid:27)1(cid:27)2(cid:26)

(cid:27) 2
2

!

:

(9.41)

However, the second coordinates of the last 20 points have been lost. These
are represented by the circled points in Figure 9.3, with their x2 values
arbitrarily set to 0.
We wish to ﬁnd the maximum likelihood estimate of the parameter vec-
tor  D .(cid:22)1; (cid:22)2, (cid:27)1; (cid:27)2; (cid:26)/. The standard maximum likelihood estimates
O(cid:22)2 D 40X
" 40X
#1=2
.x2i (cid:0) O(cid:22)2/2 =40
#,

iD1
.x1i (cid:0) O(cid:22)1/2 =40

O(cid:22)1 D 40X
" 40X

iD1
.x1i (cid:0) O(cid:22)1/ .x2i (cid:0) O(cid:22)2/ =40

" 40X

iD1
O(cid:26) D

.O(cid:27)1O(cid:27)2/ ;

x1i =40;

#1=2

O(cid:27)1 D

O(cid:27)2 D

;

x2i =40;

iD1

;

iD1

(9.42)

llllllllllllllllllllllllllllllllllllllll012345−0.50.00.51.01.52.0x1x2llllllllllllllllllll148

Survival Analysis and the EM Algorithm

are unavailable for (cid:22)2, (cid:27)2, and (cid:26) because of the missing data.
The EM algorithm begins by ﬁlling in the missing data in some way, say
by setting x2i D 0 for the 20 missing values, giving an artiﬁcially complete
data set data.0/. Then it proceeds as follows.
(cid:15) The standard method (9.42) is applied to the ﬁlled-in data.0/ to produce
O
1 ; O(cid:22).0/
 .0/ D . O(cid:22).0/
2 ; O(cid:26).0//; this is the M (“maximizing”) step.11
(cid:15) Each of the missing values is replaced by its conditional expectation
(assuming  D O
 .0/) given the nonmissing data; this is the E (“expecta-
tion”) step. In our case the missing values x2i are replaced by

1 ; O(cid:27) .0/

2 ; O(cid:27) .0/

(cid:16)
x1i (cid:0) O(cid:22).0/



O(cid:22).0/

C O(cid:26).0/

O(cid:27) .0/
2O(cid:27) .0/

1

1

2

:

 .j /C1 (cid:0) O

 .j /k is suitably small.

(9.43)
(cid:15) The E and M steps are repeated, at the j th stage giving a new artiﬁcially
complete data set data.j / and an updated estimate O
 .j /. The iteration
stops when k O
Table 9.8 shows the EM algorithm at work on the bivariate normal ex-
ample of Figure 9.3. In exponential families the algorithm is guaranteed to
converge to the MLE O
 based on just the observed data o; moreover, the
likelihood fO .j / .o/ increases with every step j . (The convergence can be
sluggish, as it is here for O(cid:27)2 and O(cid:26).)
The EM algorithm ultimately derives from the fake-data principle, a
property of maximum likelihood estimation going back to Fisher that can
only brieﬂy be summarized here.  Let x D .o; u/ represent the “complete
data,” of which o is observed while u is unobserved or missing. Write the
density for x as

f .x/ D f .o/f .ujo/;

(9.44)

 .o/ be the MLE of  based just on o.

and let O
Suppose we now generate simulations of u by sampling from the condi-
tional distribution fO .o/.ujo/,

(cid:3)k (cid:24) fO .o/.ujo/

u

for k D 1; 2; : : : ; K

(9.45)

(the stars indicating creation by the statistician and not by observation),
giving fake complete-data values x
(cid:3) D fx

(cid:3)k D .o; u
(cid:3)2; : : : ; x
(cid:3)1; x

(cid:3)k/. Let
(cid:3)Kg;

(9.46)

11 In this example, O(cid:22).0/

1

data
and O(cid:27) .0/

1

and, as in Table 9.8, stay the same in subsequent steps of the algorithm.

are available as the complete-data estimates in (9.42),

10

9.5 Missing Data and the EM Algorithm

149

Table 9.8 EM algorithm for estimating means, standard deviations, and
the correlation of the bivariate normal distribution that gave the data in
Figure 9.3.

Step

(cid:22)1

(cid:22)2

(cid:27)1

(cid:27)2

(cid:26)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86
1.86

.463
.707
.843
.923
.971
1.002
1.023
1.036
1.045
1.051
1.055
1.058
1.060
1.061
1.062
1.063
1.064
1.064
1.064
1.064

1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08
1.08

.738
.622
.611
.636
.667
.694
.716
.731
.743
.751
.756
.760
.763
.765
.766
.767
.768
.768
.769
.769

.162
.394
.574
.679
.736
.769
.789
.801
.808
.813
.816
.819
.820
.821
.822
.822
.823
.823
.823
.823





1 f .x

(cid:3) goes to O

(cid:3)k/ yields MLE O

whose notional likelihoodQK
(cid:3). It then turns out
that O
 .o/ as K goes to inﬁnity. In other words, maximum likeli-
hood estimation is self-consistent: generating artiﬁcial data from the MLE
density fO .o/.ujo/ doesn’t change the MLE. Moreover, any value O
 .0/ not
equal to the MLE O
 .o/ cannot be self-consistent: carrying through (9.45)–
(9.46) using fO .0/ .ujo/ leads to hypothetical MLE O
 .1/ having fO .1/ .o/ >
fO .0/ .o/, etc., a more general version of the EM algorithm.12

Modern technology allows social scientists to collect huge data sets,
perhaps hundreds of responses for each of thousands or even millions of
individuals. Inevitably, some entries of the individual responses will be
missing. Imputation amounts to employing some version of the fake-data
principle to ﬁll in the missing values. Imputation’s goal goes beyond ﬁnd-
be replaced by .o; E .j /.ujo//, with E .j / indicating expectation with respect to O .j /,

(cid:3)
12 Simulation (9.45) is unnecessary in exponential families, where at each stage data

can

as in (9.43).

150

Survival Analysis and the EM Algorithm

11

ing the MLE, to the creation of graphs, conﬁdence intervals, histograms,
and more, using only convenient, standard complete-data methods.
Finally, returning to survival analysis, the Kaplan–Meier estimate (9.17)
is itself self-consistent.  Consider the Arm A censored observation 74C
in Table 9.2. We know that that patient’s survival time exceeded 74. Sup-
pose we distribute his probability mass (1=51 of the Arm A sample) to the
right, in accordance with the conditional distribution for x > 74 deﬁned
by the Arm A Kaplan–Meier survival curve. It turns out that redistributing
all the censored cases does not change the original Kaplan–Meier survival
curve; Kaplan–Meier is self-consistent, leading to its identiﬁcation as the
“nonparametric MLE” of a survival function.

9.6 Notes and Details

The progression from life tables, Kaplan–Meier curves, and the log-rank
test to proportional hazards regression was modest in its computational
demands, until the ﬁnal step. Kaplan–Meier curves lie within the capabil-
ities of mechanical calculators. Not so for proportional hazards, which is
emphatically a child of the computer age. As the algorithms grew more in-
tricate, their inferential justiﬁcation deepened in scope and sophistication.
This is a pattern we also saw in Chapter 8, in the progression from bioassay
to logistic regression to generalized linear models, and will reappear as we
move from the jackknife to the bootstrap in Chapter 10.

Censoring is not the same as truncation. For the truncated galaxy data
of Section 8.3, we learn of the existence of a galaxy only if it falls into
the observation region (8.38). The censored individuals in Table 9.2 are
known to exist, but with imperfect knowledge of their lifetimes. There is a
version of the Kaplan–Meier curve applying to truncated data, which was
developed in the astronomy literature by Lynden-Bell (1971).

The methods of this chapter apply to data that is left-truncated as well
as right-censored. In a survival time study of a new HIV drug, for instance,
subject i might not enter the study until some time (cid:28)i after his or her initial
diagnosis, in which case ti would be left-truncated at (cid:28)i, as well as possibly
later right-censored. This only modiﬁes the composition of the various risk
sets. However, other missing-data situations, e.g., left- and right-censoring,
require more elaborate, less elegant, treatments.

1 [p. 133] Formula (9.10). Let the interval Œt0; t1 be partitioned into a large
number of subintervals of length dt, with tk the midpoint of subinterval k.

9.6 Notes and Details

As in (9.4), using (9.9),

PrfT (cid:21) t1jT (cid:21) t0g :DY

.1 (cid:0) h.ti / dt /

o
nX log.1 (cid:0) h.ti / dt /
o
n(cid:0)X

h.ti / dt

;

D exp
:D exp

151

(9.47)

which, as dt ! 0, goes to (9.10).

2 [p. 136] Kaplan–Meier estimate. In the life table formula (9.6) (with k D
1), let the time unit be small enough to make each bin contain at most one
value t.k/ (9.16). Then at t.k/,
O
h.k/ D d.k/

(9.48)

;

n (cid:0) k C 1

giving expression (9.17).

3 [p. 137] Greenwood’s formula (9.18). In the life table formulation of Sec-

tion 9.1, (9.6) gives

From nk

O
hk

ind(cid:24) Bi.nk; hk/ we get
n
n
log OSj

var

var

log

log OSj D jX
(cid:16)
1 (cid:0) O

log

1

:

hk

(cid:16)

1 (cid:0) O
o :D jX

hk

1

hk

1 (cid:0) hk

1
nk

;

o D jX
D jX

1

1

(9.49)

(9.50)

var O
.1 (cid:0) hk/2

hk

n
log OSj

o :D jX

where we have used the delta-method approximation varflog Xg :D varfXg=
EfXg2. Plugging in hk D yk=nk yields

yk

1

:

var

nk.nk (cid:0) yk/

(9.51)
Then the inverse approximation varfXg D EfXg2 varflog Xg gives Green-
wood’s formula (9.18).
The censored data situation of Section 9.2 does not enjoy independence
between the O
hk values. However, successive conditional independence, given
the nk values, is enough to verify the result, as in the partial likelihood cal-
culations below. Note: the conﬁdence intervals in Figure 9.1 were obtained

152

Survival Analysis and the EM Algorithm

by exponentiating the intervals,

log OSj ˙ 1:96

h

n

log OSj

var

oi1=2

:

(9.52)

4 [p. 138] Parametric life tables analysis. Figure 9.2 and the analysis behind
it is developed in Efron (1988), where it is called “partial logistic regres-
sion” in analogy with partial likelihood.

5 [p. 139] The log-rank test. This chapter featured an all-star cast, includ-
ing four of the most referenced papers of the post-war era: Kaplan and
Meier (1958), Cox (1972) on proportional hazards, Dempster et al. (1977)
codifying and naming the EM algorithm, and Mantel and Haenszel (1959)
on the log-rank test. (Cox (1958) gives a careful, and early, analysis of
the Mantel–Haenszel idea.) The not very helpful name “log-rank” does
at least remind us that the test depends only on the ranks of the survival
times, and will give the same result if all the observed survival times ti are
monotonically transformed, say to exp.ti / or t 1=2
. It is often referred to as
the Mantel–Haenszel or Cochran–Mantel–Haenszel test in older literature.
Kaplan–Meier and proportional hazards are also rank-based procedures.

i

6 [p. 141] Hypergeometric distribution. Hypergeometric calculations, as for
Table 9.5, are often stated as follows: n marbles are placed in an urn, nA
labeled A and nB labeled B; nd marbles are drawn out at random; y is
the number of these labeled A. Elementary (but not simple) calculations
then produce the conditional distribution of y given the table’s marginals
nA; nB ; n; nd ; and ns,

 

! 

!(cid:30) 

!

n
nd

Prfyjmarginalsg D

nA
y

nB

nd (cid:0) y

(9.53)

for

max.nA (cid:0) ns; 0/  y  min.nd ; nA/;

and expressions (9.24) for the mean and variance. If nA and nB go to inﬁn-
ity such that nA=n ! pA and nB =n ! 1(cid:0) pA, then V ! nd pA.1(cid:0) pA/,
the variance of y (cid:24) Bi.nd ; pA/.

7 [p. 141] Log-rank statistic Z (9.25). Why is .PN
nominator for Z? Let ui D yi (cid:0) Ei in (9.30), so Z’s numerator isPN

1 Vi /1=2 the correct de-
1 ui,

with

uijDi (cid:24) .0; Vi /

(9.54)

under the null hypothesis of equal hazard rates. This implies that, uncon-
ditionally, Efuig D 0. For j < i, uj is a function of Di (since yj and

!

D NX

u2
i

varfuig

  NX

1

E

ui

!2 D E
  NX
:D NX

1

Vi :

1

1

(9.55)

153
Ej are), so Efuj uijDig D 0, and, again unconditionally, Efuj uig D 0.
Therefore, assuming equal hazard rates,

9.6 Notes and Details

The last approximation, replacing unconditional variances varfuig with
conditional variances Vi, is justiﬁed in Crowley (1974), as is the asymp-
totic normality (9.29).
the inﬁnitesimal interval .T.j /; T.j / C d T / is hi .T.j // d T , so

Rj , the probability pi that death occurs in

8 [p. 145] Lemma (9.37). For i 2

0
i ˇ d T;

(9.56)

pi D h0.T.j //ec
Y

Pi D pi

.1 (cid:0) pk/:

and the probability of event Ai that individual i dies while the others don’t
is

(9.57)
But the Ai are disjoint events, so, given that [Ai has occurred, the proba-
bility that it is individual i who died is
:D eci ˇ

(cid:30) X

(cid:30)X

k2
Rj(cid:0)i

(9.58)

eck ˇ ;

Pj

Pi

this becoming exactly (9.37) as d T ! 0.

Rj

k2

Rj

9 [p. 145] Partial likelihood (9.40). Cox (1975) introduced partial likelihood
as inferential justiﬁcation for the proportional hazards model, which had
been questioned in the literature. Let Dj indicate all the observable infor-
mation available just before time T.j /(9.35), including all the death or loss
times for individuals having ti < T.j /. (Notice that Dj determines the risk
set Rj .) By successive conditioning we write the full likelihood f .data/
as

f .data/ D f .D1/f .i1j

D JY

jD1

R1/f .D2jD1/f .i2j
JY
Rj /:

f .ijj

jD1

f .DjjDj(cid:0)1/

R2/ : : :

(9.59)

Letting  D .˛; ˇ/, where ˛ is a nuisance parameter vector having to do

154

Survival Analysis and the EM Algorithm

with the occurrence and timing of events between observed deaths,

24 JY

jD1

35 L.ˇ/;

f˛;ˇ .data/ D

f˛;ˇ .DjjDj(cid:0)1/

(9.60)

where L.ˇ/ is the partial likelihood (9.38).
The proportional hazards model simply ignores the bracketed factor in
(9.60); l.ˇ/ D log L.ˇ/ is treated as a genuine likelihood, maximized to
give O
ˇ, and assigned covariance matrix .(cid:0)R
(cid:0)1 as in Section 4.3. Efron
(1977) shows this tactic is highly efﬁcient for the estimation of ˇ.

O
ˇ//

l.

10 [p. 148] Fake-data principle. For any two values of the parameters 1 and

2 deﬁne

l1 .2/ DZ

Œlog f2 .o; u/ f1 .ujo/ d u;

this being the limit as K ! 1 of
l1 .2/ D lim
K!1

log f2 .o; u

(cid:3)k/;

(9.61)

(9.62)

1

K

kD1

KX
CZ
 (cid:0) 1

2

f1 .o/

the fake-data log likelihood (9.46) under 2, if 1 were the true value of .
Using f .o; u/ D f .o/f .ujo/, deﬁnition (9.61) gives
l1 .2/ (cid:0) l1 .1/ D log
f1 .ujo/
D log

f1 .ujo/
log
D .f1 .ujo/; f2 .ujo// ;

 f2 .o/
 f2 .o/

 f2 .ujo/

(9.63)



f1 .o/

with D the deviance (8.31), which is always positive unless ujo has the
same distribution under 1 and 2, which we will assume doesn’t happen.
Suppose we begin the EM algorithm at  D 1 and ﬁnd the value 2
maximizing l1 . /. Then l1 .2/ > l1 .1/ and D > 0 implies f2 .o/ >
f1 .o/ in (9.63); that is, we have increased the likelihood of the observed
data. Now take 1 D O
 D arg max f .o/. Then the right side of (9.63) is
 / > lO .2/ for any 2 not equaling 1 D O
O
negative, implying lO .
. Putting
this together,13 successively computing 1; 2; 3; : : : by fake-data MLE
calculations increases f .o/ at every step, and the only stable point of the
algorithm is at  D O

 .o/.

11 [p. 150] Kaplan–Meier self-consistency. This property was veriﬁed in Efron

(1967), where the name was coined.
13 Generating the fake data is equivalent to the E step of the algorithm, the M step being

the maximization of lj . /.

10

The Jackknife and the Bootstrap

A central element of frequentist inference is the standard error. An algo-
rithm has produced an estimate of a parameter of interest, for instance the
mean Nx D 0:752 for the 47 ALL scores in the top panel of Figure 1.4.
How accurate is the estimate? In this case, formula (1.2) for the standard
deviation1 of a sample mean gives estimated standard error

(10.1)
so one can’t take the third digit of Nx D 0:752 very seriously, and even the
5 is dubious.

bse D 0:040;

Direct standard error formulas like (1.2) exist for various forms of aver-
aging, such as linear regression (7.34), and for hardly anything else. Tay-
lor series approximations (“device 2” of Section 2.1) extend the formulas
to smooth functions of averages, as in (8.30). Before computers, applied
statisticians needed to be Taylor series experts in laboriously pursuing the
accuracy of even moderately complicated statistics.

The jackknife (1957) was a ﬁrst step toward a computation-based, non-
formulaic approach to standard errors. The bootstrap (1979) went further
toward automating a wide variety of inferential calculations, including stan-
dard errors. Besides sparing statisticians the exhaustion of tedious routine
calculations the jackknife and bootstrap opened the door for more com-
plicated estimation algorithms, which could be pursued with the assurance
that their accuracy would be easily assessed. This chapter focuses on stan-
dard errors, with more adventurous bootstrap ideas deferred to Chapter 11.
We end with a brief discussion of accuracy estimation for robust statistics.

1 We will use the terms “standard error” and “standard deviation” interchangeably.

155

156

The Jackknife and the Bootstrap

10.1 The Jackknife Estimate of Standard Error

The basic applications of the jackknife apply to one-sample problems, where
the statistician has observed an independent and identically distributed (iid)
sample x D .x1; x2; : : : ; xn/
0 from an unknown probability distribution F
on some space X ,

(10.2)
X can be anything: the real line, the plane, a function space.2 A real-valued
statistic O

 has been computed by applying some algorithm s.(cid:1)/ to x,

xi

for i D 1; 2; : : : ; n:

iid(cid:24) F

O
 D s.x/;
and we wish to assign a standard error to O
the standard deviation of O
Let x.i / be the sample with xi removed,

 D s.x/ under sampling model (10.2).

. That is, we wish to estimate

x.i / D .x1; x2; : : : ; xi(cid:0)1; xiC1; : : : ; xn/
0

;

(10.4)

(10.3)

(10.5)

and denote the corresponding value of the statistic of interest as

O
.i / D s.x.i //:
#1=2
2

"

n

1

n (cid:0) 1

.(cid:1)/ D nX

Then the jackknife estimate of standard error for O
(cid:16) O
nX
 is
bsejack D
; with O
.i / (cid:0) O
.(cid:1)/
In the case where O
 is the mean Nx of real values x1; x2; : : : ; xn (i.e., X
is an interval of the real line), O
be expressed as
O
.i / D .nNx (cid:0) xi /=.n (cid:0) 1/:
.(cid:1)/ D Nx, O

.(cid:1)/ D .Nx (cid:0) xi /=.n (cid:0) 1/, and

.i / is their average excluding xi, which can

O
.i /=n:

(10.7)

(10.6)

1

Equation (10.7) gives O
bsejack D

" nX

iD1

.i / (cid:0) O
.xi (cid:0) Nx/2= .n.n (cid:0) 1//

#1=2

fudge factor .n(cid:0) 1/=n in deﬁnition (10.6) was inserted to makebsejack agree
exactly the same as the classic formula (1.2). This is no coincidence. The
with (1.2) when O
2 If X is an interval of the real line we might take F to be the usual cumulative

 is Nx.

distribution function, but here we will just think of F as any full description of the
probability distribution for an xi on X .

;

(10.8)

157

10.1 The Jackknife Estimate of Standard Error

The advantage ofbsejack is that deﬁnition (10.6) can be applied in an au-
tomatic way to any statistic O
 D s.x/. All that is needed is an algorithm
that computes s.(cid:1)/ for the deleted data sets x.i /. Computer power is being
substituted for theoretical Taylor series calculations. Later we will see that
the underlying inferential ideas—plug-in estimation of frequentist standard
errors—haven’t changed, only their implementation.
As an example, consider the kidney function data set of Section 1.1. Here
the data consists of n D 157 points .xi ; yi /, with x D age and y D tot in
Figure 1.1. (So the generic xi in (10.2) now represents the pair .xi ; yi /, and
F describes a distribution in the plane.) Suppose we are interested in the
correlation between age and tot, estimated by the usual sample correlation
O
 D s.x/,

," nX

#1=2
s.x/ D nX
nX
.yi (cid:0) Ny/2
.xi (cid:0) Nx/2
Applying (10.6) gavebsejack D 0:058 for the accuracy of O
computed to be O
 D (cid:0)0:572 for the kidney data.
case,bsetaylor D

. Nonpara-
metric bootstrap computations, Section 10.2, also gave estimated standard
error 0.058. The classic Taylor series formula looks quite formidable in this

.xi (cid:0) Nx/.yi (cid:0) Ny/

( O

; (10.9)

C 2 O(cid:22)22
O(cid:22)20 O(cid:22)02

 O(cid:22)40O(cid:22)2

20

 2
4n

C 4 O(cid:22)22
O(cid:22)2

11

(cid:0) 4 O(cid:22)31
O(cid:22)11 O(cid:22)20

(cid:0) 4 O(cid:22)13
O(cid:22)11 O(cid:22)02

1

1

iD1

02

C O(cid:22)04O(cid:22)2
O(cid:22)hk D nX

(cid:21))1=2

(10.10)

(10.11)

where

.xi (cid:0) Nx/h.yi (cid:0) Ny/k=n:

iD1

need be assumed.

It gavebse D 0:057.
It is worth emphasizing some features of the jackknife formula (10.6).
(cid:15) It is nonparametric; no special form of the underlying distribution F
inputs the data set x and the function s.x/, and outputsbsejack.
(cid:15) It is completely automatic: a single master algorithm can be written that
(cid:15) The algorithm works with data sets of size n(cid:0)1, not n. There is a hidden
assumption of smooth behavior across sample sizes. This can be worri-
some for statistics like the sample median that have a different deﬁnition
for odd and even sample size.

The Jackknife and the Bootstrap

158
(cid:15) The jackknife standard error is upwardly biased as an estimate of the
(cid:15) The connection of the jackknife formula (10.6) with Taylor series meth-

true standard error.

1

ods is closer than it appears. We can write

bsejack DPn

i

1 D2
n2

(cid:21)1=2

where Di D

;

p
.i / (cid:0) O
O
.(cid:1)/
n.n (cid:0) 1/

1=

:

(10.12)

As discussed in Section 10.3, the Di are approximate directional deriva-
tives, measures of how fast the statistic s.x/ is changing as we decrease
the weight on data point xi. So se2
jack is proportional to the sum of
squared derivatives of s.x/ in the n component directions. Taylor series
expressions such as (10.10) amount to doing the derivatives by formula
rather than numerically.

Figure 10.1 The lowess curve for the kidney data of
Figure 1.2. Vertical bars indicate ˙2 standard errors: jackknife
(10.6) blue dashed; bootstrap (10.16) red solid. The jackknife
greatly overestimates variability at age 25.

The principal weakness of the jackknife is its dependence on local deriva-
Figure 1.2, can result in erratic behavior forbsejack. Figure 10.1 illustrates
tives. Unsmooth statistics s.x/, such as the kidney data lowess curve in
the point. The dashed blue vertical bars indicate ˙2 jackknife standard er-

−4−202Agetot20253035404550556065707580852510.2 The Nonparametric Bootstrap

159

rors for the lowess curve evaluated at ages 20; 25; : : : ; 85. For the most
part these agree with the dependable bootstrap standard errors, solid red
bars, described in Section 10.2. But things go awry at age 25, where the
local derivatives greatly overstate the sensitivity of the lowess curve to
global changes in the sample x.

10.2 The Nonparametric Bootstrap

From the point of view of the bootstrap, the jackknife was a halfway house
between classical methodology and a full-throated use of electronic com-
putation. (The term “computer-intensive statistics” was coined to describe
the bootstrap.) The frequentist standard error of an estimate O
 D s.x/ is,
ideally, the standard deviation we would observe by repeatedly sampling
new versions of x from F . This is impossible since F is unknown. Instead,
the bootstrap (“ingenious device” number 4 in Section 2.1) substitutes an
estimate OF for F and then estimates the frequentist standard by direct sim-
ulation, a feasible tactic only since the advent of electronic computation.
The bootstrap estimate of standard error for a statistic O
 D s.x/ com-
puted from a random sample x D .x1; x2; : : : ; xn/ (10.2) begins with the
notion of a bootstrap sample

(cid:3) D .x

(cid:3)
1 ; x

(cid:3)
2 ; : : : ; x

(cid:3)
n /;

x

(10.13)
(cid:3)
where each x
i is drawn randomly with equal probability and with replace-
ment from fx1; x2; : : : ; xng. Each bootstrap sample provides a bootstrap
replication of the statistic of interest,3

(10.14)
Some large number B of bootstrap samples are independently drawn
(B D 500 in Figure 10.1). The corresponding bootstrap replications are
calculated, say

/:

O


(cid:3)b/

for b D 1; 2; : : : ; B:
(cid:3)b D s.x
The resulting bootstrap estimate of standard error for O
#1=2
standard deviation of the O
bseboot D
(cid:3)b (cid:0) O

(cid:3)(cid:1)2.

" BX

; with O

(cid:3)b values,

.B (cid:0) 1/

(cid:16) O









(10.15)
 is the empirical

(cid:3)(cid:1) D BX

(cid:3)bıB:

O


(10.16)
(cid:3) is intended to avoid confusion with the original data x, which stays

3 The star notation x

ﬁxed in bootstrap computations, and likewise O

(cid:3) vis-a-vis O.

O


(cid:3) D s.x

(cid:3)

bD1

bD1

F

 :

160

iid(cid:0)! x

The Jackknife and the Bootstrap

Motivation forbseboot begins by noting that O
 is obtained in two steps:
ﬁrst x is generated by iid sampling from probability distribution F , and
then O
 is calculated from x according to algorithm s.(cid:1)/,
s(cid:0)! O

(10.17)
We don’t know F , but we can estimate it by the empirical probability dis-
tribution OF that puts probability 1=n on each point xi (e.g., weight 1=157
(cid:3)
on each point .xi ; yi / in Figure 1.2). Notice that a bootstrap sample x
(10.13) is an iid sample drawn from OF , since then each x
(cid:3) independently
has equal probability of being any member of fx1; x2; : : : ; xng. It can be
shown that OF maximizes the probability of obtaining the observed sample
x under all possible choices of F in (10.2), i.e., it is the nonparametric
MLE of F .
Bootstrap replications O
(cid:3) are obtained by a process analogous to (10.17),
OF

In the real world (10.17) we only get to see the single value O
, but the boot-
strap world (10.18) is more generous: we can generate as many bootstrap
replications O
(cid:3)b as we want, or have time for, and directly estimate their
suggests, correctly in most cases, thatbseboot approaches the true standard
OF approaches F as n grows large
variability as in (10.16). The fact that
error of O
.
The true standard deviation of O
, i.e., its standard error, can be thought
of as a function of the probability distribution F that generates the data,
say Sd.F /. Hypothetically, Sd.F / inputs F and outputs the standard devi-
ation of O
, which we can imagine being evaluated by independently run-
ning (10.17) some enormous number of times N , and then computing the
empirical standard deviation of the resulting O

iid(cid:0)! x

(10.18)

(cid:3)

s(cid:0)! O



(cid:3)

:





 values,
; with O

 .(cid:1)/ D NX

 .j /ıN:

O

1

(10.19)

24 NX

jD1

(cid:16) O
 .j / (cid:0) O

Sd.F / D

351=2
 .(cid:1)/2.
.N (cid:0) 1/
bseboot D Sd. OF /:

The bootstrap standard error of O

 is the plug-in estimate

(10.20)
More exactly, Sd. OF / is the ideal bootstrap estimate of standard error, what
we would get by letting the number of bootstrap replications B go to in-
ﬁnity. In practice we have to stop at some ﬁnite value of B, as discussed in
what follows.

10.2 The Nonparametric Bootstrap

161

and multisample versions will be taken up later.

As with the jackknife, there are several important points worth empha-

sizing aboutbseboot.
ten that inputs the data x and the function s.(cid:1)/, and outputsbseboot.
(cid:15) It is completely automatic. Once again, a master algorithm can be writ-
(cid:15) We have described the one-sample nonparametric bootstrap. Parametric
(cid:15) Bootstrapping “shakes” the original data more violently than jackknif-
(cid:3) from x. The bootstrap is more
ing, producing nonlocal deviations of x
dependable than the jackknife for unsmooth statistics since it doesn’t
(cid:15) B D 200 is usually sufﬁcient  for evaluatingbseboot. Larger values, 1000 2
depend on local derivatives.
or 2000, will be required for the bootstrap conﬁdence intervals of Chap-
ter 11.
(cid:15) There is nothing special about standard errors. We could just as well
use the bootstrap replications to estimate the expected absolute error
Efj O
(cid:15) Fisher’s MLE formula (4.27) is applied in practice via

 (cid:0) jg, or any other accuracy measure.

bseﬁsher D .nIO /

(cid:0)1=2;

that is, by plugging in O
 for  after a theoretical calculation of se. The
bootstrap operates in the same way at (10.20), though the plugging in is
done before rather than after the calculation. The connection with Fishe-
rian theory is more obvious for the parametric bootstrap of Section 10.4.

(10.21)

The jackknife is a completely frequentist device, both in its assumptions
and in its applications (standard errors and biases). The bootstrap is also
basically frequentist, but with a touch of the Fisherian as in the relation
with (10.21). Its versatility has led to applications in a variety of estima-
tion and prediction problems, with even some Bayesian connections. 
Unusual applications can also pop up for the jackknife; see the jackknife-
after-bootstrap comment in the chapter endnotes.

From a classical point of view, the bootstrap is an incredible computa-
tional spendthrift. Classical statistics was fashioned to minimize the hard
labor of mechanical computation. The bootstrap seems to go out of its way
to multiply it, by factors of B D 200 or 2000 or more. It is nice to re-
port that all this computational largesse can have surprising data analytic
payoffs.

The 22 students of Table 3.1 actually each took ﬁve tests, mechanics,
vectors, algebra, analytics, and statistics. Table 10.1 shows

3

4

162

The Jackknife and the Bootstrap

Table 10.1 Correlation matrix for the student score data. The eigenvalues
are 3.463, 0.660, 0.447, 0.234, and 0.197. The eigenratio statistic
O
 D 0:693, and its bootstrap standard error estimate is 0.075
(B D 2000).

mechanics vectors algebra analytics statistics

mechanics
vectors
algebra
analysis
statistics

1.00
.50
.76
.65
.54

.50
1.00
.59
.51
.38

.76
.59
1.00
.76
.67

.65
.51
.76
1.00
.74

.54
.38
.67
.74
1.00

the sample correlation matrix and also its eigenvalues. The “eigenratio”
statistic,

(10.22)

ﬁdence interval calculations.) The jackknife (10.6) gave a bigger estimate,

O
 D largest eigenvalue=sum eigenvalues;
measures how closely the ﬁve scores can be predicted by a single linear
combination, essentially an IQ score for each student: O
 D 0:693 here,
indicating strong predictive power for the IQ score. How accurate is 0.693?
ror estimate (10.16)bseboot D 0:075. (This was 10 times more bootstraps
B D 2000 bootstrap replications (10.15) yielded bootstrap standard er-
than necessary forbseboot, but will be needed for Chapter 11’s bootstrap con-
bsejack D 0:083.
 ˙ 1:96bse for 95% coverage. These are based on an assump-
Standard errors are usually used to suggest approximate conﬁdence in-
tervals, often O
tion of normality for O
. The histogram of the 2000 bootstrap replications of
O
, as seen in Figure 10.2, disabuses belief in even approximate normality.
Compared with classical methods, a massive amount of computation has
gone into the histogram, but this will pay off in Chapter 11 with more ac-
curate conﬁdence limits. We can claim a double reward here for bootstrap
methods: much wider applicability and improved inferences. The bootstrap
histogram—invisible to classical statisticians—nicely illustrates the advan-
tages of computer-age statistical inference.

10.3 Resampling Plans

There is a second way to think about the jackknife and the bootstrap:
as algorithms that reweight, or resample, the original data vector x D

10.3 Resampling Plans

163

Figure 10.2 Histogram of B D 2000 bootstrap replications O
(cid:3)
for the eigenratio statistic (10.22) for the student score data. The
vertical black line is at O
 D :693. The long left tail shows that
normality is a dangerous assumption in this case.



0. At the price of a little more abstraction, resampling con-
0 is by deﬁnition a vector of

.x1; x2; : : : ; xn/
nects the two algorithms and suggests a class of other possibilities.
A resampling vector P D .P1; P2; : : : ; Pn/
nonnegative weights summing to 1,

nX

iD1

P D .P1; P2; : : : ; Pn/

0

with Pi (cid:21) 0 and

Pi D 1:

(10.23)

That is, P is a member of the simplex Sn (5.39). Resampling plans operate
by holding the original data set x ﬁxed, and seeing how the statistic of
interest O
We denote the value of O

 changes as the weight vector P varies across Sn.

 for a vector putting weight Pi on xi as

O


(cid:3) D S.P/;

(10.24)

the star notation now indicating any reweighting, not necessarily from boot-
strapping; O
 D s.x/ describes the behavior of O
mean s.x/ D Nx, we have S.P/ D Pn
 in the real world (10.17),
while O
(cid:3) D S.P/ describes it in the resampling world. For the sample
1 Pi xi. The unbiased estimate of



 q^*Frequency0.40.50.60.70.80.9050100150Standard ErrorBootstrap .075Jackknife .083164

variance s.x/ DPn

The Jackknife and the Bootstrap
i .xi (cid:0) Nx/2=.n (cid:0) 1/ can be seen to have

S.P/ D n
n (cid:0) 1

(cid:0)

Pi x2
i

Pi xi

(10.25)

24 nX

iD1

  nX

iD1

!235 :

Figure 10.3 Resampling simplex for sample size n D 3. The
center point is P0 (10.26); the green circles are the jackknife
points P.i / (10.28); triples indicate bootstrap resampling numbers
.N1; N2; N3/ (10.29). The bootstrap probabilities are 6=27 for
P0, 1=27 for each corner point, and 3=27 for each of the six
starred points.

Letting

P0 D .1; 1; : : : ; 1/
0

=n;

(10.26)

the resampling vector putting equal weight on each value xi, we require in
the deﬁnition of S.(cid:1)/ that

S.P0/ D s.x/ D O
the original estimate. The ith jackknife value O

 ;

(10.27)

.i / (10.5) corresponds to

P0 P(1) P(2) P(3) (3,0,0) (2,1,0) (1,2,0) (0,3,0) (1,1,1) (2,0,1) (0,2,1) (1,0,2) (0,1,2) (0,0,3) 0.5 1.0 1.5 0.0 – 0.5 – 1.0 – 1.5 0.5 1.0 1.5 0.0 – 0.5 2.0 10.3 Resampling Plans

165

resampling vector

=.n (cid:0) 1/;

P.i / D .1; 1; : : : ; 1; 0; 1; : : : ; 1/
0

(10.28)
with 0 in the ith place. Figure 10.3 illustrates the resampling simplex S3
applying to sample size n D 3, with the center point being P0 and the open
circles the three possible jackknife vectors P.i /.
With n D 3 sample points fx1; x2; x3g there are only 10 distinct boot-
strap vectors (10.13), also shown in Figure 10.3. Let

Ni D #fx
the number of bootstrap draws in x
ure are .N1; N2; N3/, for example .1; 0; 2/ for x
twice.4 The bootstrap resampling vectors are of the form

(cid:3)
(10.29)
(cid:3) equaling xi. The triples in the ﬁg-
(cid:3) having x1 once and x3

D xig;

j

(cid:3) D .N1; N2; : : : ; Nn/
0

P

=n;

(10.30)

where the Ni are nonnegative integers summing to n. According to deﬁ-
nition (10.13) of bootstrap sampling, the vector N D .N1; N2; : : : ; Nn/
0
follows a multinomial distribution (5.38) with n draws on n equally likely
categories,

N (cid:24) Multn.n; P0/:

This gives bootstrap probability (5.37)

nŠ

N1ŠN2Š : : : NnŠ

1
nn

on P

(cid:3) (10.30).

Figure 10.3 is misleading in that the jackknife vectors P.i / appear only
(cid:3). As n grows large
slightly closer to P0 than are the bootstrap vectors P
they are, in fact, an order of magnitude closer. Subtracting (10.26) from
(10.28) gives Euclidean distance

(10.31)

(10.32)

(10.33)

kP.i / (cid:0) P0k D 1


Ni (cid:24) Bi

n.n (cid:0) 1/:


.p

1

n

For the bootstrap, notice that Ni in (10.29) has a binomial distribution,

(10.34)
4 A hidden assumption of deﬁnition (10.24) is that O D s.x/ has the same value for any
permutation of x, so for instance s.x1; x3; x3/ D s.x3; x1; x3/ D S.1=3; 0; 2=3/.

n;

;

The Jackknife and the Bootstrap

166
with mean 1 and variance .n(cid:0) 1/=n. Then P
D Ni =n has mean and vari-
ance .1=n; .n (cid:0) 1/=n3/. Adding over the n coordinates gives the expected
(cid:3),
root mean square distance for bootstrap vector P

(cid:3)

i

(cid:0)EkP

(cid:3) (cid:0) P0k2(cid:1)1=2 Dp

.n (cid:0) 1/=n2;

p
n times further than (10.33).

(10.35)

(10.36)

an order of magnitude

The function S.P/ has approximate directional derivative

Di D S.P.i // (cid:0) S.P0/
kP.i / (cid:0) P0k

" 10X

(cid:16) O



ideal bootstrap standard error estimate

in the direction from P0 toward P.i / (measured along the dashed lines
in Figure 10.3). Di measures the slope of function S.P/ at P0, in the

direction of P.i /. Formula (10.12) showsbsejack as proportional to the root
out thatbsejack equalsbseboot (except for the fudge factor .n(cid:0) 1/=n in (10.6)).

If S.P/ is a linear function of P, as it is for the sample mean, it turns

mean square of the slopes.

Most statistics are not linear, and then the local jackknife resamples may
provide a poor approximation to the full resampling behavior of S.P/. This
was the case at one point in Figure 10.1.
(cid:3), we can easily evaluate the

With only 10 possible resampling points P

#1=2

(cid:3)(cid:1)2



;

(cid:3)(cid:1) D 10X

O




pk

pk

O


kD1

kD1

(cid:3)k;

(10.37)

(cid:3)k (cid:0) O

bseboot D
with O
(cid:3)k D S.P k/ and pk the probability from (10.32) (listed in Fig-
!
 
ure 10.3). This rapidly becomes impractical. The number of distinct boot-
strap samples for n points turns out to be
2n (cid:0) 1

(10.38)
For n D 10 this is already 92,378, while n D 20 gives 6:9 (cid:2) 1010 distinct
(cid:3) at random, which is what al-
possible resamples. Choosing B vectors P
gorithm (10.13)–(10.15) effectively is doing, makes the un-ideal bootstrap
standard error estimate (10.16) almost as accurate as (10.37) for B as small
as 200 or even less.

n

:

The luxury of examining the resampling surface provides a major advan-
tage to modern statisticians, both in inference and methodology. A variety
of other resampling schemes have been proposed, a few of which follow.

10.3 Resampling Plans

167

The Inﬁnitesimal Jackknife

Looking at Figure 10.3 again, the vector

Pi .(cid:15)/ D .1 (cid:0) (cid:15)/P0 C (cid:15)P.i / D P0 C (cid:15).P.i / (cid:0) P0/

(10.39)

lies proportion (cid:15) of the way from P0 to P.i /. Then
S .Pi .(cid:15)// (cid:0) S.P0/

QDi D lim
(cid:15)!0

(10.40)

(cid:15)kP.i / (cid:0) P0k
!1=2

ın2

QD2

i

  nX

iD1

bseIJ D

exactly deﬁnes the direction derivative at P0 in the direction of P.i /. The
inﬁnitesimal jackknife estimate of standard error is

;

(10.41)

usually evaluated numerically by setting (cid:15) to some small value in (10.40)–
(10.41) (rather than (cid:15) D 1 in (10.12)). We will meet the inﬁnitesimal jack-
knife again in Chapters 17 and 20.

Multisample Bootstrap

The median difference between the AML and the ALL scores in Figure 1.4
is

mediff D 0:968 (cid:0) 0:733 D 0:235:

(10.42)

How accurate is 0.235? An appropriate form of bootstrapping draws 25
times with replacement from the 25 AML patients, 47 times with replace-
ment from the 47 ALL patients, and computes mediff(cid:3) as the difference
between the medians of the two bootstrap samples. (Drawing one boot-
strap sample of size 72 from all the patients would result in random sample
sizes for the AML(cid:3)
=ALL(cid:3) groups, adding inappropriate variability to the
frequentist standard error estimate.)
givebseboot D 0:074. The estimate (10.42) is 3.18bse units above zero, agree-
A histogram of B D 500 mediff(cid:3) values appears in Figure 10.4. They

ing surprisingly well with the usual two-sample t-statistic 3.01 (based on
mean differences), and its permutation histogram Figure 4.3. Permutation
testing can be considered another form of resampling.

168

The Jackknife and the Bootstrap

Figure 10.4 B D 500 bootstrap replications for the median
bseboot D 0:074. The observed value mediff D 0:235 (vertical
difference between the AML and ALL scores in Figure 1.4, giving

black line) is more than 3 standard errors above zero.

Moving Blocks Bootstrap

Suppose x D .x1; x2; : : : ; xn/, instead of being an iid sample (10.2), is a
time series. That is, the x values occur in a meaningful order, perhaps with
nearby observations highly correlated with each other. Let Bm be the set of
contiguous blocks of length m, for example

B3 D f.x1; x2; x3/; .x2; x3; x4/; : : : ; .xn(cid:0)2; xn(cid:0)1; xn/g :

(10.43)

Presumably, m is chosen large enough that correlations between xi and xj ,
jj (cid:0) ij > m, are neglible. The moving block bootstrap ﬁrst selects n=m
(cid:3). Having constructed B such samples,bseboot is calculated
blocks from Bm, and assembles them in random order to construct a boot-
strap sample x
as in (10.15)–(10.16).

The Bayesian Bootstrap

Let G1; G2; : : : ; Gn be independent one-sided exponential variates (de-
noted Gam(1,1) in Table 5.1), each having density exp.(cid:0)x/ for x > 0.

 mediff*Frequency0.00.10.20.30.40.501020304050607010.4 The Parametric Bootstrap

169

, nX

The Bayesian bootstrap uses resampling vectors

P

It can be shown that P

(cid:3) D .G1; G2; : : : ; Gn/
(10.44)
(cid:3) is then uniformly distributed over the resampling
simplex Sn; for n D 3, uniformly distributed over the triangle in Fig-

ure 10.3. Prescription (10.44) is motivated by assuming a Jeffreys-style
uninformative prior distribution (Section 3.2) on the unknown distribution
F (10.2).

Gi :

1

(cid:3) has mean vector and covariance matrix

Distribution (10.44) for P

P

P0;

1

n C 1

:

(10.45)

This is almost identical to the mean and covariance of bootstrap resamples
(cid:3) (cid:24) Multn.n, P0/=n,
P

(cid:3) (cid:24)
(cid:3) (cid:24)

P

P0;

1

n

0

(cid:1)(cid:21)
(cid:0)diag.P0/ (cid:0) P0P
(cid:1)(cid:21)
(cid:0)diag.P0/ (cid:0) P0P

0

;

0

0

(10.46)

(5.40). The Bayesian bootstrap and the ordinary bootstrap tend to agree, at
least for smoothly deﬁned statistics O
There was some Bayesian disparagement of the bootstrap when it ﬁrst
appeared because of its blatantly frequentist take on estimation accuracy.
And yet connections like (10.45)–(10.46) have continued to pop up, as we
will see in Chapter 13.

(cid:3) D S.P

/.

(cid:3)



10.4 The Parametric Bootstrap

In our description (10.18) of bootstrap resampling,

OF

iid(cid:0)! x

(cid:3) (cid:0)! O

(cid:3)

;



(10.47)
there is no need to insist that OF be the nonparametric MLE of F . Suppose
D˚f(cid:22).x/; (cid:22) 2 (cid:127)(cid:9) :
we are willing to assume that the observed data vector x comes from a
parametric family F as in (5.1),
(10.48)
Let O(cid:22) be the MLE of (cid:22). The bootstrap parametric resamples from f O(cid:22).(cid:1)/,
and proceeds as in (10.14)–(10.16) to calculatebseboot.
(10.49)

f O(cid:22) (cid:0)! x

(cid:3) (cid:0)! O

F

(cid:3)



;

The Jackknife and the Bootstrap

170
As an example, suppose that x D .x1; x2; : : : ; xn/ is an iid sample of
size n from a normal distribution,

iid(cid:24)

xi

N .(cid:22); 1/;

i D 1; 2; : : : ; n:

Then O(cid:22) D Nx, and a parametric bootstrap sample is x
where

(cid:3) D .x

(10.50)

(cid:3)
1 ; x

(cid:3)
2 ; : : : ; x

(cid:3)
n /,

i D 1; 2; : : : ; n:

(cid:3)

iid(cid:24)

N .Nx; 1/;

x
i

(10.51)
More adventurously, if F were a family of time series models for x,
(cid:3)
algorithm (10.49) would still apply (now without any iid structure): x
would be a time series sampled from model f O(cid:22).(cid:1)/, and O
(cid:3) D s.x
/ the
(cid:3)b, b D 1; 2; : : : ; B, andbseboot from (10.16).

(cid:3)b would give
resampled statistic of interest. B independent realizations x
O


(cid:3)

Figure 10.5 The gfr data of Figure 5.7 (histogram). Curves
show the MLE ﬁts from polynomial Poisson models, for degrees
of freedom df D 2; 3; : : : ; 7. The points on the curves show the
ﬁts computed at the centers x.j / of the bins, with the responses
being the counts in the bins. The dashes at the base of the plot
show the nine gfr values appearing in Table 10.2.

As an example of parametric bootstrapping, Figure 10.5 expands the
gfr investigation of Figure 5.7. In addition to the seventh-degree polyno-
mial ﬁt (5.62), we now show lower-degree polynomial ﬁts for 2, 3, 4, 5,

 gfrCounts20406080100051015202530lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllldf = 2df = 3,4,5,6df = 710.4 The Parametric Bootstrap

171
and 6 degrees of freedom; df D 2 obviously gives a poor ﬁt; df D 3; 4; 5; 6
give nearly identical curves; df D 7 gives only a slightly better ﬁt to the
raw data.

The plotted curves were obtained from the Poisson regression method

used in Section 8.3, which we refer to as “Lindsey’s method”.
(cid:15) The x-axis was partitioned into K D 32 bins, with endpoints 13; 16; 19,

: : : ; 109, and centerpoints, say,

x. / D .x.1/; x.2/; : : : ; x.K//;
x.1/ D 14:5, x.2/ D 17:5, etc.
(cid:15) Count vector y D .y1; y2; : : : ; yK/ was computed

yk D #fxi in binkg

(so y gives the heights of the bars in Figure 10.5).

(cid:15) An independent Poisson model was assumed for the counts,

(10.52)

(10.53)

ind(cid:24) Poi.(cid:22)k/

for k D 1; 2; : : : ; K:

yk

(10.54)
(cid:15) The parametric model of degree “df” assumed that the (cid:22)k values were
described by an exponential polynomial of degree df in the x.k/ values,

ˇj xj

.k/:

(10.55)

(cid:15) The MLE Oˇ D .
(cid:15) The plotted curves in Figure 10.5 trace the MLE values O(cid:22)k,

O
ˇdf/ in model (10.54)–(10.55) was found.5

O
ˇ1; : : : ;

O
ˇ0;

log.(cid:22)k/ D dfX

jD0

log. O(cid:22)k/ D dfX

jD0

O
ˇj xj

.k/:

(10.56)

y

k

(cid:3)

How accurate are the curves? Parametric bootstraps were used to assess
their standard errors. That is, Poisson resamples were generated according
to

for k D 1; 2; : : : ; K;

ind(cid:24) Poi. O(cid:22)k/
(10.57)
and bootstrap MLE values O(cid:22)
(cid:3)
k calculated as above, but now based on count
(cid:3) rather than y. All of this was done B D 200 times, yielding
vector y
The results appear in Table 10.2, showing bseboot for df D 2; 3; : : : ; 7
bootstrap standard errors (10.16).

5 A single R command, glm(y(cid:24)poly(x,df),family=poisson) accomplishes
this.

172

The Jackknife and the Bootstrap

Table 10.2 Bootstrap estimates of standard error for the gfr density.
Poisson regression models (10.54)–(10.55), df D 2; 3; : : : ; 7, as in
Figure 10.5; each B D 200 bootstrap replications; nonparametric
standard errors based on binomial bin counts.

Degrees of freedom

gfr

20.5
29.5
38.5
47.5
56.5
65.5
74.5
83.5
92.5

2

.28
.65
1.05
1.47
1.57
1.15
.76
.40
.13

3

.07
.57
1.39
1.91
1.60
1.10
.61
.30
.20

4

.13
.57
1.33
2.12
1.79
1.07
.62
.40
.29

5

.13
.66
1.52
1.93
1.93
1.31
.68
.38
.29

6

.12
.74
1.72
2.15
1.87
1.34
.81
.49
.34

7

.05
1.11
1.73
2.39
2.28
1.27
.71
.68
.46

Nonparametric
standard error

.00
1.72
2.77
4.25
4.35
1.72
1.72
1.72
.00

degrees of freedom evaluated at nine values of gfr. Variability generally
increases with increasing df, as expected. Choosing a “best” model is a
compromise between standard error and possible deﬁnitional bias as sug-
gested by Figure 10.5, with perhaps df D 3 or 4, the winner.
If we kept increasing the degrees of freedom, eventually (at df D 32)
we would exactly match the bar heights yk in the histogram. At this point
the parametric bootstrap would merge into the nonparametric bootstrap.
“Nonparametric” is another name for “very highly parameterized.” The
huge sample sizes associated with modern applications have encouraged
nonparametric methods, on the sometimes mistaken ground that estimation
efﬁciency is no longer of concern. It is costly here, as the “nonparametric”
column of Table 10.2 shows.6

Figure 10.6 returns to the student score eigenratio calculations of Fig-
ure 10.2. The solid histogram shows 2000 parametric bootstrap replica-
tions (10.49), with f O(cid:22) the ﬁve-dimensional bivariate normal distribution

N5.Nx; O†/. Here Nx and O† are the usual MLE estimates for the expectation
togram, with bseboot D 0:070 compared with the nonparametric estimate

vector and covariance matrix based on the 22 ﬁve-component student score
vectors. It is narrower than the corresponding nonparametric bootstrap his-

6 These are the binomial standard errors Œyk.n (cid:0) yk/=n1=2, n D 211. The
nonparametric results look much more competitive when estimating cdf’s rather than
densities.

10.4 The Parametric Bootstrap

173

Figure 10.6 Eigenratio example, student score data. Solid
histogram B D 2000 parametric bootstrap replications O
the ﬁve-dimensional normal MLE; line histogram the 2000
nonparametric replications of Figure 10.2. MLE O
 D :693 is
vertical red line.



(cid:3) from

0.075. (Note the different histogram bin limits from Figure 10.2, changing
the details of the nonparametric histogram.)

Parametric families act as regularizers, smoothing out the raw data and
de-emphasizing outliers. In fact the student score data is not a good can-
didate for normal modeling, having at least one notable outlier,7 casting
doubt on the smaller estimate of standard error.
The classical statistician could only imagine a mathematical device that
given any statistic O
 D s.x/ would produce a formula for its standard er-
ror, as formula (1.2) does for Nx. The electronic computer is such a device.
As harnessed by the bootstrap, it automatically produces a numerical esti-
mate of standard error (though not a formula), with no further cleverness
required. Chapter 11 discusses a more ambitious substitution of computer
power for mathematical analysis: the bootstrap computation of conﬁdence
intervals.

7 As revealed by examining scatterplots of the ﬁve variates taken two at a time. Fast and

painless plotting is another advantage for twenty-ﬁrst-century data analysts.

 eigenratio*Frequency0.40.50.60.70.80.9020406080100120Bootstrap Standard ErrorsNonparametric .075Parametric .070174

The Jackknife and the Bootstrap

10.5 Inﬂuence Functions and Robust Estimation

The sample mean played a dominant role in classical statistics for reasons
heavily weighted toward mathematical tractibility. Beginning in the 1960s,
an important counter-movement, robust estimation, aimed to improve upon
the statistical properties of the mean. A central element of that theory, the
inﬂuence function, is closely related to the jackknife and inﬁnitesimal jack-
knife estimates of standard error.

We will only consider the case where X , the sample space, is an interval
of the real line. The unknown probability distribution F yielding the iid
sample x D .x1; x2; : : : ; xn/ in (10.2) is now the cdf of a density function
f .x/ on X . A parameter of interest, i.e., a function of F , is to be estimated
by the plug-in principle, O
OF is the
empirical probability distribution putting probability 1=n on each sample
point xi. For the mean,

 D T . OF /, where, as in Section 10.2,
 D T .F / DZ
(cid:16) OF
 D 1
nX
 DR xd OF .x/.)
(In Riemann–Stieltjes notation,  DR xdF .x/ and O

xf .x/ dx and

O
 D T

(10.58)

iD1

xi :

X

n

The inﬂuence function of T .F /, evaluated at point x in X , is deﬁned to

be

IF.x/ D lim
(cid:15)!0

T ..1 (cid:0) (cid:15)/F C (cid:15)ıx/ (cid:0) T .F /

(cid:15)

;

(10.59)

where ıx is the “one-point probability distribution” putting probability 1
on x. In words, IF.x/ measures the differential effect of modifying F by

putting additional probability on x. For the mean  DR xf .x/dx we cal-

culate that

5

IF.x/ D x (cid:0) :
A fundamental theorem says that O
 D T . OF / is approximately
nX
:D  C 1

IF.xi /;

O


(10.60)

(10.61)

n

iD1

with the approximation becoming exact as n goes to inﬁnity. This implies
that O
 (cid:0)  is, approximately, the mean of the n iid variates IF.xi /, and that
the variance of O

 is approximately

varfIF.x/g ;

(10.62)

n O



o :D 1

n

var

10.5 Inﬂuence Functions and Robust Estimation

175
varfIF.x/g being the variance of IF.x/ for any one draw of x from F . For
the sample mean, using (10.60) in (10.62) gives the familiar equality

varfNxg D 1

n

varfxg:

(10.63)

The sample mean suffers from an unbounded inﬂuence function (10.60),
which grows ever larger as x moves farther from . This makes Nx unstable
against heavy-tailed densities such as the Cauchy (4.39). Robust estimation
theory seeks estimators O
 of bounded inﬂuence, that do well against heavy-
tailed densities without giving up too much efﬁciency against light-tailed
densities such as the normal. Of particular interest have been the trimmed
mean and its close cousin the winsorized mean.
D ˛ or equivalently

Let x.˛/ denote the 100˛th percentile of distribution F , satisfying F .x.˛//

˛ DZ x.˛/

(cid:0)1 f .x/ dx:

Z x.1(cid:0)˛/

(10.64)

The ˛th trimmed mean of F , trim.˛/, is deﬁned as

trim.˛/ D 1

(10.65)
the mean of the central 1 (cid:0) 2˛ portion of F , trimming off the lower and
upper ˛ portions. This is not the same as the ˛th winsorized mean wins.˛/,

xf .x/ dx;

1 (cid:0) 2˛

x.˛/

where

wins.˛/ DZ
8ˆ<ˆ:x.˛/

x.1(cid:0)˛/

x

W .x/ D

X

W .x/f .x/ dx;

(10.66)

if x  x.˛/
if x.˛/  x  x.1(cid:0)˛/
if x (cid:21) x.1(cid:0)˛/I

(10.67)

trim.˛/ removes the outer portions of F , while wins.˛/ moves them into
x.˛/ or x.1(cid:0)˛/. In practice, empirical versions O
wins.˛/ are used,
substituting the empirical density O
f , with probability 1=n at each xi, for
f .

trim.˛/ and O

There turns out to be an interesting relationship between the two: the

inﬂuence function of trim.˛/ is a function of wins.˛/,

IF˛.x/ D W .x/ (cid:0) wins.˛/

1 (cid:0) 2˛

:

(10.68)

This is pictured in Figure 10.7, where we have plotted empirical inﬂuence

176

The Jackknife and the Bootstrap

Figure 10.7 Empirical inﬂuence functions for the 47 leukemia
ALL scores of Figure 1.4. The two dashed curves are IF˛.x/ for
the trimmed means (10.68), for ˛ D 0:2 and ˛ D 0:4. The solid
curve is IF.x/ for the sample mean Nx (10.60).

functions (plugging in OF for F in deﬁnition (10.59)) relating to the 47
leukemia ALL scores of Figure 1.4: IF0:2.x/ and IF0:4.x/ are plotted, along
with IF0.x/ (10.60), that is, for the mean.

Table 10.3 Trimmed means and their bootstrap standard deviations for
the 47 leukemia ALL scores of Figure 1.4; B D 1000 bootstrap
replications for each trim value. The last column gives empirical inﬂuence
function estimates of the standard error, which are also the inﬁnitesimal
jackknife estimates (10.41). These fail for the median.

Trim

.0
.1
.2
.3
.4
.5

Mean

Median

Trimmed Bootstrap

mean

sd

.752
.729
.720
.725
.734
.733

.040
.038
.035
.044
.047
.053

(IFse)

(.040)
(.034)
(.034)
(.044)
(.054)

0.20.40.60.81.01.21.41.6−0.50.00.51.0ALLInfluence Functionmeantrimmed mean a = 0. 2trimmed mean a = 0. 410.6 Notes and Details

177

The upper panel of Figure 1.4 shows a moderately heavy right tail for
the ALL distribution. Would it be more efﬁcient to estimate the center of
vides an answer: bseboot (10.16) was calculated for Nx and O
the distribution with a trimmed mean rather than Nx? The bootstrap pro-
trim.˛/, ˛ D
0:1; 0:2; 0:3; 0:4, and 0.5, the last being the sample median. It appears that
O
trim.0:2/ is moderately better than Nx. This brings up an important question
discussed in Chapter 20: if we use something like Table 10.3 to select an
estimator, how does the selection process affect the accuracy of the result-
ing estimate?

We might also use the square root of formula (10.62) to estimate the
standard errors of the various estimators, plugging in the empirical inﬂu-
ence function for IF.x/. This turns out to be the same as using the inﬁnites-
imal jackknife (10.41). These appear in the last column of Table 10.3. Pre-
dictably, this approach fails for the sample median, whose inﬂuence func-
tion is a square wave, sharply discontinuous at the median ,

IF.x/ D ˙1ı .2f . // :

(10.69)

Robust estimation offers a nice illustration of statistical progress in the
computer age. Trimmed means go far back into the classical era. Inﬂuence
functions are an insightful inferential tool for understanding the tradeoffs in
trimmed mean estimation. And ﬁnally the bootstrap allows easy assessment
of the accuracy of robust estimation, including some more elaborate ones
not discussed here.

10.6 Notes and Details

Quenouille (1956) introduced what is now called the jackknife estimate
of bias. Tukey (1958) realized that Quenouille-type calculations could be
repurposed for nonparametric standard-error estimation, inventing formula
(10.6) and naming it “the jackknife,” as a rough and ready tool. Miller’s im-
portant 1964 paper, “A trustworthy jackknife,” asked when formula (10.6)
could be trusted. (Not for the median.)

The bootstrap (Efron, 1979) began as an attempt to better understand
the jackknife’s successes and failures. Its name celebrates Baron Mun-
chausen’s success in pulling himself up by his own bootstraps from the
bottom of a lake. Burgeoning computer power soon overcame the boot-
strap’s main drawback, prodigous amounts of calculation, propelling it into
general use. Meanwhile, 1000C theoretical papers were published asking
when the bootstrap itself could be trusted. (Most but not all of the time in
common practice).

178

The Jackknife and the Bootstrap

A main reference for the chapter is Efron’s 1982 monograph The Jack-
knife, the Bootstrap and Other Resampling Plans. Its Chapter 6 shows the
equality of three nonparametric standard error estimates: Jaeckel’s (1972)
inﬁnitesimal jackknife (10.41); the empirical inﬂuence function estimate,
based on (10.62); and what is known as the nonparametric delta method.

Bootstrap Packages

Various bootstrap packages in R are available on the CRAN contributed-
packages web site, bootstrap being an ambitious one. Algorithm 10.1
shows a simple R program for nonparametric bootstrapping. Aside from
bookkeeping, it’s only a few lines long.

Algorithm 10.1 An R program for the nonparametric bootstrap.

Boot <- function (x, B, func, ...){

# x is data vector or matrix (with each row a case)
# B is number of bootstrap replications
# func is R function that inputs a data vector or
# matrix and returns a numeric number or vector
# ... other arguments for func

x <- as.matrix(x)
n <- nrow(x)
f0=func(x,...) # get size of output
fmat <- matrix(0,length(f0),B)
for (b in 1:B) {

i=sample(1:n, n, replace = TRUE)
fmat[,b] <- func(x[i, ],...)

}

drop(fmat)

}

1 [p. 158] The jackknife standard error. The 1982 monograph also contains
Efron and Stein’s (1981) result on the bias of the jackknife variance esti-
mate, the square of formula (10.6): modulo certain sample size considera-
tions, the expectation of the jackknife variance estimate is biased upward
for the true variance.
For the sample mean Nx, the jackknife yields exactly the usual variance
i .xi (cid:0) Nx/2=.n.n (cid:0) 1//, while the ideal bootstrap estimate

estimate (1.2),P

(B ! 1) gives

nX
.xi (cid:0) Nx/2=n2:

iD1

(10.70)

10.6 Notes and Details

179

As with the jackknife, we could append a fudge factor to get perfect agree-
ment with (1.2), but there is no real gain in doing so.

2 [p. 161] Bootstrap sample sizes. LetbseB indicate the bootstrap standard er-
ror estimate (10.16) based on B replications, andbse1 the “ideal bootstrap,”
creasing B past a certain point, becausebse1 is itself a statistic whose value
B ! 1. In any actual application, there are diminishing returns from in-
varies with the observed sample x (as in (10.70)), leaving an irreducible re-
mainder of randomness in any standard error estimate. Section 6.4 of Efron
and Tibshirani (1993) shows that B D 200 will almost always be plenty
(for standard errors, but not for bootstrap conﬁdence intervals, Chapter 11).
Smaller numbers, 25 or even less, can still be quite useful in complicated
situations where resampling is expensive. An early complaint, “Bootstrap
estimates are random,” is less often heard in an era of frequent and massive
simulations.

3 [p. 161] The Bayesian bootstrap. Rubin (1981) suggested the Bayesian
bootstrap (10.44). Section 10.6 of Efron (1982) used (10.45)–(10.46) as
an objective Bayes justiﬁcation for what we will call the percentile-method
bootstrap conﬁdence intervals in Chapter 12.

4 [p. 161] Jackknife-after-bootstrap. For the eigenratio example displayed in

just the original 2000 replications.

0:075. How accurate is this value? Bootstrapping the bootstrap seems like
too much work, perhaps 200 times 2000 resamples. It turns out, though,

Figure 10.2, B D 2000 nonparametric bootstrap replications gavebseboot D
that we can use the jackknife to estimate the variability ofbseboot based on
Now the deleted sample estimate in (10.6) isbseboot.i /. The key idea is
applying deﬁnition (10.16) to this subset givesbseboot.i /. For the estimate of
Figure 10.2, the jackknife-after-bootstrap calculations gavebsejack D 0:022
for bseboot D 0:075. In other words, 0.075 isn’t very accurate, which is
to be expected for the standard error of a complicated statistic estimated
from only n D 22 observations. An inﬁnitesimal jackknife version of this
technique will play a major role in Chapter 20.

(cid:3) (10.13), among the original 2000,
to consider those bootstrap samples x
that do not include the point xi. About 37% of the original B samples will
be in this subset. Section 19.4 of Efron and Tibshirani (1993) shows that

5 [p. 174] A fundamental theorem. Tukey can justly be considered the found-
ing father of robust statistics, his 1960 paper being especially inﬂuential.
Huber’s celebrated 1964 paper brought the subject into the realm of high-
concept mathematical statistics. Robust Statistics: The Approach Based on
Inﬂuence Functions, the 1986 book by Hampel et al., conveys the breadth
of a subject only lightly scratched in our Section 10.5. Hampel (1974)

180

The Jackknife and the Bootstrap

introduced the inﬂuence function as a statistical tool. Boos and Serﬂing
(1980) veriﬁed expression (10.62). Qualitative notions of robustness, more
than speciﬁc theoretical results, have had a continuing inﬂuence on modern
data analysis.

11

Bootstrap Conﬁdence Intervals

The jackknife and the bootstrap represent a different use of modern com-
puter power: rather than extending classical methodology—from ordinary
least squares to generalized linear models, for example—they extend the
reach of classical inference.

Chapter 10 focused on standard errors. Here we will take up a more am-
bitious inferential goal, the bootstrap automation of conﬁdence intervals.
The familiar standard intervals

O

 ˙ 1:96bse;
 D 10 from a Poisson model O

(11.1)

for approximate 95% coverage, are immensely useful in practice but often
the standard 95% interval .3:8; 16:2/ (usingbse D O
not very accurate. If we observe O
 (cid:24) Poi. /,
 1=2) is a mediocre ap-

proximation to the exact interval1

.5:1; 17:8/:

(11.2)

Standard intervals (11.1) are symmetric around O
, this being their main
weakness. Poisson distributions grow more variable as  increases, which
is why interval (11.2) extends farther to the right of O
 D 10 than to the
left. Correctly capturing such effects in an automatic way is the goal of
bootstrap conﬁdence interval theory.

11.1 Neyman’s Construction for One-Parameter

The student score data of Table 3.1 comprised n D 22 pairs,

Problems

xi D .mi ; vi /;

i D 1; 2; : : : ; 22;

(11.3)

1 Using the Neyman construction of Section 11.1, as explained there; see also Table 11.2

in Section 11.4.

181

Bootstrap Conﬁdence Intervals

182
where mi and vi were student i’s scores on the “mechanics” and “vectors”
tests. The sample correlation coefﬁcient O
 between mi and vi was com-
puted to be

O
 D 0:498:

(11.4)

Question: What can we infer about the true correlation  between m and v?
Figure 3.2 displayed three possible Bayesian answers. Conﬁdence intervals
provide the frequentist solution, by far the most popular in applied practice.

Figure 11.1 The solid curve is the normal correlation coefﬁcient
density fO .r/ (3.11) for O
student score data; O
endpoints of the 95% conﬁdence interval for , with
corresponding densities shown by dashed curves. These yield tail
areas 0.025 at O

 D 0:498, the MLE estimate for the
 .up/ D 0:751 are the

 .lo/ D 0:093 and O

 (11.6).

Suppose, ﬁrst, that we assume a bivariate normal model (5.12) for the
O
 / for sample corre-
pairs .mi ; vi /. In that case the probability density f .
lation O
 given true correlation  has known form (3.11). The solid curve in
Figure 11.1 graphs f for  D 0:498, that is, for  set equal to the observed
value O
. In more careful notation, the curve graphs fO .r/ as a function of
the dummy variable2 r taking values in Œ(cid:0)1; 1.
2 This is an example of a parametric bootstrap distribution (10.49), here with O(cid:22) being O.

−0.4−0.20.00.20.40.60.81.001234CorrelationDensity0.0930.7510.498l0.0250.02511.1 Neyman’s construction

Two other curves f .r/ appear in Figure 11.1: for  equaling

These were numerically calculated as the solutions to

O
O
 .lo/ D 0:093 and
 .up/ D 0:751:
Z O

183

(11.5)

(11.6)

(11.7)

Z 1
O
In words, O
above O
0.025 below O
;

fO .lo/.r/ dr D 0:025 and

fO .up/.r/ dr D 0:025:

(cid:0)1

 D 0:498, while O

 .lo/ is the smallest value of  putting probability at least 0.025
 .up/ is the largest value with probability at least

 2h O

O
 .up/

 .lo/;

i

is a 95% conﬁdence interval for the true correlation, statement (11.7) hold-
ing true with probability 0.95, for every possible value of .

We have just described Neyman’s construction of conﬁdence intervals
O
for one-parameter problems f .
 /. (Later we will consider the more difﬁ-
cult situation where there are “nuisance parameters” in addition to the pa-
rameter of interest .) One of the jewels of classical frequentist inference,
it depends on a pivotal argument—“ingenious device” number 5 of Sec-
tion 2.1—to show that it produces genuine conﬁdence intervals, i.e., ones
that contain the true parameter value  at the claimed probability level,
0.95 in Figure 11.1. The argument appears in the chapter endnotes.
For the Poisson calculation (11.2) it was necessary to deﬁne exactly what
“the smallest value of  putting probability at least 0.025 above O
” meant.
O
 / at
This was done assuming that, for any , half of the probability f .
O
 D 10 counted as “above,” and similarly for calculating the upper limit.

1

Transformation Invariance

Conﬁdence intervals enjoy the important and useful property of transfor-
mation invariance. In the Poisson example (11.2), suppose our interest
shifts from parameter  to parameter (cid:30) D log . The 95% exact inter-
val (11.2) for  then transforms to the exact 95% interval for (cid:30) simply by
taking logs of the endpoints,

.log.5:1/; log.17:8// D .1:63; 2:88/:

(11.8)

To state things generally, suppose we observe O
 from a family of densi-
O
O
 / for  of coverage level
 / and construct a conﬁdence interval C.
ties f .

Bootstrap Conﬁdence Intervals

184
˛ (˛ D 0:95 in our examples). Now let parameter (cid:30) be a monotonic in-
creasing function of , say

(m. / D log  in (11.8)), and likewise O(cid:30) D m.
Then C.
for (cid:30),

O
 / maps point by point into C(cid:30). O(cid:30)/, a level-˛ conﬁdence interval

O
 / for the point estimate.

(11.9)

(cid:30) D m. /

(cid:30) D m. / for  2

(11.10)
O
This just says that the event f 2
 /g is the same as the event f(cid:30) 2
C(cid:30). O(cid:30)/g, so if the former always occurs with probability ˛ then so must the

C.

C

:

latter.

C(cid:30). O(cid:30)/ Dn

o

(cid:16) O



Figure 11.2 The situation in Figure 11.1 after transformation to
(cid:30) D m. / according to (11.11). The curves are nearly N.(cid:30); (cid:27) 2/
with standard deviation (cid:27) D 1=

p
19 D 0:229.

Transformation invariance has an historical resonance with the normal
O
correlation coefﬁcient. Fisher’s derivation of f .
 / (3.11) in 1915 was a
mathematical triumph, but a difﬁcult one to exploit in an era of mechanical
computation. Most ingeniously, Fisher suggested instead working with the
transformed parameter (cid:30) D m. / where



 1 C 

1 (cid:0) 

(cid:30) D m. / D 1

2

log

;

(11.11)

−0.50.00.51.01.50.00.51.01.5Fisher¢s f transformDensity0.0930.9750.546l0.0250.025and likewise with statistic O(cid:30) D m.
imation,

11.2 The Percentile Method

185

O
 /. Then, to a surprisingly good approx-

:

(11.12)

See Figure 11.2, which shows Neyman’s construction on the (cid:30) scale.

In other words, we are back in Fisher’s favored situation (4.31), the sim-

ple normal translation problem, where





O(cid:30) P(cid:24)

N

(cid:30);

1

n (cid:0) 3

C(cid:30)(cid:16) O(cid:30)

 D O(cid:30) ˙ 1:96

1p
n (cid:0) 3

(11.13)

is the “obviously correct” 95% conﬁdence interval3 for (cid:30), closely approx-
imating Neyman’s construction. The endpoints of (11.13) are then trans-
formed back to the  scale according to the inverse transformation

 D e2(cid:30) (cid:0) 1
e2(cid:30) C 1
O
 / seen in Figure 11.1, but without the in-

(11.14)

;

giving (almost) the interval C.
volved computations.

Bayesian conﬁdence statements are inherently transformation invariant.
The fact that the Neyman intervals are also invariant, unlike the standard
intervals (11.1), has made them more palatable to Bayesian statisticians.
Transformation invariance will play a major role in justifying the bootstrap
conﬁdence intervals introduced next.

11.2 The Percentile Method

Our goal is to automate the calculation of conﬁdence intervals: given the
bootstrap distribution of a statistical estimator O
, we want to automatically
produce an appropriate conﬁdence interval for the unseen parameter . To
this end, a series of four increasingly accurate bootstrap conﬁdence interval
algorithms will be described.
 ˙ 1:96bse for 95% coverage, withbse taken to be the bootstrap standard
error bseboot (10.16). The limitations of this approach become obvious in
O
Figure 11.3, where the histogram shows B D 2000 nonparametric boot-
strap replications O
(cid:3) of the sample correlation coefﬁcient for the student

The ﬁrst and simplest method is to use the standard interval (11.1),



3 This is an anachronism. Fisher hated the term “conﬁdence interval” after it was later

coined by Neyman for his comprehensive theory. He thought of (11.13) as an example
of the logic of inductive inference.

186

Bootstrap Conﬁdence Intervals

Figure 11.3 Histogram of B D 2000 nonparametric bootstrap
replications O
(cid:3) for the student score sample correlation; the solid
curve is the ideal parametric bootstrap distribution fO .r/ as in
Figure 11.1. Observed correlation O
 D 0:498. Small triangles
show histogram’s 0.025 and 0.975 quantiles.



score data, obtained as in Section 10.2. The standard intervals are justiﬁed
by taking literally the asymptotic normality of O
,

O
 P(cid:24)

N .; (cid:27) 2/;

(11.15)

(cid:27) the true standard error.

Relation (11.15) will generally hold for large enough sample size n, but
we can see that for the student score data asymptotic normality has not
yet set in, with the histogram being notably long-tailed to the left. We can’t
expect good performance from the standard method in this case. (The para-
metric bootstrap distribution is just as nonnormal, as shown by the smooth
curve.)

(cid:3)1;



O


The percentile method uses the shape of the bootstrap distribution to
improve upon the standard intervals (11.1). Having generated B bootstrap
replications O
(cid:3)B, either nonparametrically as in Section 10.2
or parametrically as in Section 10.4, we use the obvious percentiles of their
distribution to deﬁne the percentile conﬁdence limits. The histogram in
Figure 11.3 has its 0.025 and 0.975 percentiles equal to 0.118 and 0.758,

(cid:3)2; : : : ;

O


 Bootstrap CorrelationsFrequency−0.4−0.20.00.20.40.60.81.00204060801000.4980.1180.758l11.2 The Percentile Method

187

and these are the endpoints of the central 95% nonparametric percentile
interval.

Figure 11.4 A 95% central conﬁdence interval via the percentile
method, based on the 2000 nonparametric replications O
Figure 11.3.

(cid:3) of



We can state things more precisely in terms of the bootstrap cdf

the proportion of bootstrap samples less than t,

n O

o.

(cid:3)b  t

OG.t / D #
(11.16)
(cid:3).˛/ of the bootstrap distribution is given by the

B:



The ˛th percentile point O
inverse function of OG,



OG.t /,

O


(cid:3).˛/ D OG

(cid:0)1.˛/I

(11.17)
O
(cid:3).˛/ is the value putting proportion ˛ of the bootstrap sample to its left.
The level-˛ upper endpoint of the percentile interval, say O

%ileŒ˛, is by
deﬁnition

(cid:0)1.˛/:
In this notation, the 95% central percentile interval is

(cid:3).˛/ D OG



%ileŒ˛ D O
O
(cid:16) O

%ileŒ:025;



O
%ileŒ:975

:

(11.19)

(11.18)

0.00.20.40.60.81.00.00.20.40.60.81.0q^*allllll.118.758.025.975G^188

Bootstrap Conﬁdence Intervals

The construction is illustrated in Figure 11.4.
The percentile intervals are transformation invariant. Let (cid:30) D m. / as
O
in (11.9), and likewise O(cid:30) D m.
 / (m.(cid:1)/ monotonically increasing), with
O
bootstrap replications O(cid:30)
(cid:3)b/ for b D 1; 2; : : : ; B. The bootstrap
(cid:3)b D m.

percentiles transform in the same way,
(cid:3).˛/ D m

(cid:3).˛/

(11.20)

O(cid:30)

;

so that, as in (11.18),

O(cid:30)%ileŒ˛ D m
verifying transformation invariance.

%ileŒ˛

;

(11.21)

(cid:16) O
(cid:16) O





In what sense does the percentile method improve upon the standard
intervals? One answer involves transformation invariance. Suppose there
exists a monotone transformation (cid:30) D m. / and O(cid:30) D m.

O
 / such that

(11.22)

O(cid:30) (cid:24)

N .(cid:30); (cid:27) 2/

(cid:16) O(cid:30); (cid:27) 2

O(cid:30)

(cid:3) (cid:24)

for every , with (cid:27) 2 constant. Fisher’s transformation (11.11)–(11.12) al-
most accomplishes this for the normal correlation coefﬁcient.

It would then be true that parametric bootstrap replications would also

follow (11.22),

(11.23)
That is, the bootstrap cdf OG(cid:30) would be normal with mean O(cid:30) and variance
(cid:27) 2. The ˛th percentile of OG(cid:30) would equal

N

:

O(cid:30)%ileŒ˛ D O(cid:30)

(cid:3).˛/ D O(cid:30) C z.˛/(cid:27);

(11.24)

where z.˛/ denotes the ˛th percentile of a standard normal distribution,

z.˛/ D ˆ
(z.:975/ D 1:96, z.:025/ D (cid:0)1:96, etc.).

(cid:0)1.˛/

(11.25)

In other words, the percentile method would provide Fisher’s “obviously

correct” intervals for (cid:30),

O(cid:30) ˙ 1:96(cid:27)

(11.26)

for 95% coverage for example. But, because of transformation invariance,
the percentile intervals for our original parameter  would also be exactly
correct.

Some comments concerning the percentile method are pertinent.

2

O
 / P(cid:24)

11.2 The Percentile Method

O
 /, it only assumes its existence.

189
(cid:15) The method does not require actually knowing the transformation to nor-
mality O(cid:30) D m.
(cid:15) If a transformation to form (11.22) exists, then the percentile intervals
are not only accurate, but also correct in the Fisherian sense of giving
the logically appropriate inference.
(cid:15) The justifying assumption for the standard intervals (11.15), O
 P(cid:24)
N .,
(cid:27) 2/, becomes more accurate as the sample size n increases (usually
with (cid:27) decreasing as 1=
n), but the convergence can be slow in cases
like that of the normal correlation coefﬁcient. The broader assumption
(11.22), that m.
up convergence, irrespective of whether or not it holds exactly. Sec-
tion 11.4 makes this point explicit, in terms of asymptotic rates of con-
vergence.
(cid:15) The standard method works ﬁne once it is applied on an appropriate
scale, as in Figure 11.2. The trouble is that the method is not transforma-
tion invariant, leaving the statistician the job of ﬁnding the correct scale.
The percentile method can be thought of as a transformation-invariant
version of the standard intervals, an “automatic Fisher” that substitutes
massive computations for mathematical ingenuity.

p
N .m. /; (cid:27) 2/ for some transformation m.(cid:1)/, speeds

(cid:15) The method requires bootstrap sample sizes on the order of B D 2000. 3
(cid:15) The percentile method is not the last word in bootstrap conﬁdence in-
tervals. Two improvements, the “BC” and “BCa” methods, will be dis-
cussed in the next section. Table 11.1 compares the various intervals as
applied to the student score correlation, O

 D 0:498.

Table 11.1 Bootstrap conﬁdence limits for student score correlation,
O
 D 0:498, n D 22. Parametric exact limits from Neyman’s construction
as in Figure 11.1. The BC and BCa methods are discussed in the next two
sections; .z0; a/, two constants required for BCa, are .(cid:0)0:055; 0:005/
parametric, and .0:000; 0:006/ nonparametric.

Parametric
.975
.025

Nonparametric
.975
.025

1. Standard
2. Percentile
3. BC
4. BCa

Exact

.17
.11
.08
.08
.09

.83
.77
.75
.75
.75

.18
.13
.13
.12

.82
.76
.76
.76

The label “computer-intensive inference” seems especially apt as ap-

190

Bootstrap Conﬁdence Intervals

plied to bootstrap conﬁdence intervals. Neyman and Fisher’s constructions
are expanded from a few special theoretically tractable cases to almost any
situation where the statistician has a repeatable algorithm. Automation, the
replacement of mathematical formulas with wide-ranging computer algo-
rithms, will be a major theme of succeeding chapters.



(11.28)

 for .



implies

(cid:3)  O(cid:30)

o D 0:50
o D 0:50:



Pr(cid:3)n O(cid:30)
Pr(cid:3)n O
Z O
(cid:0)1 fO
Z :498

11.3 Bias-Corrected Conﬁdence Intervals

The ideal form (11.23) for the percentile method, O(cid:30)
O
that the transformation O(cid:30) D m.
 / yields an unbiased estimator of con-
stant variance. The improved methods of this section and the next take into
account the possibility of bias and changing variance. We begin with bias.
If O(cid:30) (cid:24)
O(cid:30)
N . O(cid:30); (cid:27) 2/ and
(cid:3) (cid:24)

N .(cid:30); (cid:27) 2/ for all (cid:30) D m. /, as hypothesized in (11.22), then

N . O(cid:30); (cid:27) 2/, says

(cid:3) (cid:24)

O
 /, (11.28)

(11.27)
(Pr(cid:3) indicating bootstrap probability), in which case the monotonicity of
m.(cid:1)/ gives

That is, O
We can check that. For a parametric family of densities f .

(11.29)
For the normal correlation coefﬁcient density (3.11), n D 22, numerical
integration gives

(cid:3)  O
(cid:3) is median unbiased4 for O
, and likewise O
(cid:3)
(cid:16) O
(cid:3)
(cid:16) O
which is not far removed from 0.50, but far enough to have a small impact
on proper inference. It suggests that O
(cid:3) is biased upward relative to O
—
that’s why less than half of the bootstrap probability lies below O
—and
by implication that O
 is upwardly biased for estimating . Accordingly,
conﬁdence intervals should be adjusted a little bit downward. The bias-
corrected percentile method (BC for short) is a data-based algorithm for
making such adjustments.

(cid:3) D 0:478;

(cid:3) D 0:50:

O


d



f:498



(cid:0)1

O


d



(11.30)

4 Median unbiasedness, unlike the usual mean unbiasedness deﬁnition, has the advantage

of being transformation invariant.

11.3 Bias-Corrected Conﬁdence Intervals
(cid:3)2; : : : ;

191
Having simulated B bootstrap replications O
(cid:3)B, paramet-
o.
ric or nonparametric, let p0 be the proportion of replications less than O
,
(11.31)

(cid:3)b  O

p0 D #

n O

(cid:3)1;

O


O






B



(an estimate of (11.29)), and deﬁne the bias-correction value

z0 D ˆ

(cid:0)1.p0/;

(11.32)
(cid:0)1 is the inverse function of the standard normal cdf. The BC

where ˆ
level-˛ conﬁdence interval endpoint is deﬁned to be

2z0 C z.˛/i

ˆ

(cid:0)1h
(cid:16)
O
BCŒ˛ D OG
z.˛/i D OG
(cid:16)
(cid:0)1h

ˆ

;

(11.33)

where OG is the bootstrap cdf (11.16) and z.˛/ D ˆ
(cid:0)1.˛/ (11.25).
If p0 D 0:50, the median unbiased situation, then z0 D 0 and

O
BCŒ˛ D OG

(cid:0)1.˛/ D O

%ileŒ˛;

(11.34)

the percentile limit (11.18). Otherwise, a bias correction is made. Taking
p0 D 0:478 for the normal correlation example (the value we would get
from an inﬁnite number of parametric bootstrap replications) gives bias
correction value (cid:0)0:055. Notice that the BC limits are indeed shifted down-
ward from the parametric percentile limits in Table 11.1. Nonparametric
bootstrapping gave p0 about 0.50 in this case, making the BC limits nearly
the same as the percentile limits.
A more general transformation argument motivates the BC deﬁnition
(11.33). Suppose there exists a monotone transformation (cid:30) D m. / and
O(cid:30) D m.

O
 / such that for any 
O(cid:30) (cid:24)

N .(cid:30) (cid:0) z0(cid:27); (cid:27) 2/;

(11.35)

with z0 and (cid:27) ﬁxed constants. Then the BC endpoints are accurate, i.e.,
have the claimed coverage probabilities, and are also “obviously correct”
in the Fisherian sense. See the chapter endnotes for proof and discussion. 4
As before, the statistican does not need to know the transformation m.(cid:1)/
that leads to O(cid:30) (cid:24)
N .(cid:30) (cid:0) z0(cid:27); (cid:27) 2/, only that it exists. It is a broader target
than O(cid:30) (cid:24)
N .(cid:30); (cid:27) 2/ (11.22), making the BC method better justiﬁed than
the percentile method, irrespective of whether or not such a transformation
exists. There is no extra computational burden: the bootstrap replications
f O
(cid:3)b; b D 1; 2; : : : ; Bg, parametric or nonparametric, provide OG (11.16)
and z0 (11.31)–(11.32), giving O


BCŒ˛ from (11.33).

192

Bootstrap Conﬁdence Intervals

11.4 Second-Order Accuracy
p
Coverage errors of the standard conﬁdence intervals typically decrease at
n/ in the sample size n: having calculated O
Cz.˛/O(cid:27)
order O.1=
for an iid sample x D .x1; x2; : : : ; xn/, we can expect the actual coverage
ıp
probability to be

stanŒ˛ D O

o :D ˛ C c1

  O

(11.36)

stanŒ˛

n

Pr

n;

where c1 depends on the problem at hand; (11.36) deﬁnes “ﬁrst-order accu-
racy.” It can connote painfully slow convergence to the nominal coverage
level ˛, requiring sample size 4n to cut the error in half.
A second-order accurate method, say O
O.1=n/,

2ndŒ˛, makes errors of order only

n
  O

Pr

2ndŒ˛

o :D ˛ C c2=n:

(11.37)

The improvement is more than theoretical. In practical problems like that
of Table 11.1, second-order accurate methods—BCa, deﬁned in the follow-
ing, is one such—often provide nearly the claimed coverage probabilities,
even in small-size samples.

Neither the percentile method nor the BC method is second-order ac-
curate (although, as in Table 11.1, they tend to be more accurate than the
standard intervals). The difﬁculty for O
BCŒ˛ lies in the ideal form (11.35),
O
O(cid:30) (cid:24)
 / has constant standard
error (cid:27). Instead, we now postulate the existence of a monotone transforma-
tion (cid:30) D m. / and O(cid:30) D m.

N .(cid:30) (cid:0) z0(cid:27); (cid:27) 2/, where it is assumed O(cid:30) D m.

O
 / less restrictive than (11.35),
(cid:27)(cid:30) D 1 C a(cid:30):

O(cid:30) (cid:24)

N .(cid:30) (cid:0) z0(cid:27)(cid:30); (cid:27) 2

(11.38)
Here the “acceleration”  a is a small constant describing how the standard
deviation of O(cid:30) varies with (cid:30). If a D 0 we are back in situation (11.34)5,
but if not, an amendment to the BC formula (11.33) is required.

(cid:30) /;

The BCa method (“bias-corrected and accelerated”) takes its level-˛

conﬁdence limit to be
O
BCaŒ˛ D OG

(cid:0)1



ˆ


z0 C

(cid:21)

z0 C z.˛/

1 (cid:0) a.z0 C z.˛//

:

(11.39)

A still more elaborate transformation argument shows that, if there exists
a monotone transformation (cid:30) D m. / and constants z0 and a yielding
5 This assumes (cid:27)0 D 1 on the right side of (11.38), which can always be achieved by
further transforming (cid:30) to (cid:30)=(cid:27).

5

11.4 Second-Order Accuracy

193

(11.38), then the BCa limits have their claimed coverage probabilities and,
moreover, are correct in the Fisherian sense.
BCa makes three corrections to the standard intervals (11.1): for non-
normality of O
 (through using the bootstrap percentiles rather than just the
bootstrap standard error); for bias (through the bias correction value z0);
and for nonconstant standard error (through a). Notice that if a D 0 then
BCa (11.39) reduces to BC (11.33). If z0 D 0 then BC reduces to the
percentile method (11.18); and if OG, the bootstrap histogram, is normal,
then (11.18) reduces to the standard interval (11.1). All three of the correc-
tions, for nonnormality, bias, and acceleration, can have substantial effects
in practice and are necessary to achieve second-order accuracy. A great
deal of theoretical effort was devoted to verifying the second-order accu-
racy and BCa intervals under reasonably general assumptions.6

 D 10; actual tail areas above and below

Table 11.2 Nominal 95% central conﬁdence intervals for Poisson
parameter  having observed O
O
 D 10 deﬁned as in Figure 11.1 (atom of probability split at 10). For
instance, lower standard limit 3.80 actually puts probability 0.004 above
10, rather than nominal value 0.025. Bias correction value z0 (11.32) and
acceleration a (11.38) both equal 0.050.

Nominal limits
.025
.975

Tail areas

Above Below

1. Standard
2. %ile
3. BC
4. BCa

Exact

3.80
4.18
4.41
5.02
5.08

16.20
16.73
17.10
17.96
17.82

.004
.007
.010
.023
.025

.055
.042
.036
.023
.025

The advantages of increased accuracy are not limited to large sample
sizes. Table 11.2 returns to our original example of observing O
 D 10
from Poisson model O
 (cid:24) Poi. /. According to Neyman’s construction,
the 0.95 exact limits give tail areas 0.025 in both the above and below
directions, as in Figure 11.1, and this is nearly matched by the BCa limits.
However the standard limits are much too conservative at the left end and
anti-conservative at the right.

6 The mathematical side of statistics has also been affected by electronic computation,

where it is called upon to establish the properties of general-purpose computer
algorithms such as the bootstrap. Asymptotic analysis in particular has been aggressively
developed, the veriﬁcation of second-order accuracy being a nice success story.

194

Bootstrap Conﬁdence Intervals

Table 11.3 95% nominal conﬁdence intervals for the parametric and
nonparametric eigenratio examples of Figures 10.2 and 10.6.

1. Standard
2. %ile
3. BC
4. BCa

Parametric
.975

.025

.829
.815
.828
.820

.556
.542
.523
.555
.z0 D (cid:0):029; a D :058/

Nonparametric
.975

.025

.840
.818
.813
.828

.545
.517
.507
.523
.z0 D (cid:0):049; a D :051/

Bootstrap conﬁdence limits continue to provide better inferences in the
vast majority of situations too complicated for exact analysis. One such
situation is examined in Table 11.3. It relates to the eigenratio example
illustrated in Figures 10.2–10.6. In this case the nonnormality and bias cor-
rections stretch the bootstrap intervals to the left, but the acceleration effect
pulls right, partially canceling out the net change from the standard inter-
vals.

The percentile and BC methods are completely automatic, and can be
applied whenever a sufﬁciently large number of bootstrap replications are
available. The same cannot be said of BCa. A drawback of the BCa method
is that the acceleration a is not a function of the bootstrap distribution and
must be computed separately. Often this is straightforward:
(cid:15) For one-parameter exponential families such as the Poisson, a equals z0.
(cid:15) In one-sample nonparametric problems, a can be estimated from the
jackknife resamples O

.i / (10.5),

Pn
Pn

(cid:16) O
.i / (cid:0) O
(cid:16) O
.(cid:1)/
i (cid:0) O
.(cid:1)/

3
2(cid:21)1:5 :

(11.40)

Oa D 1

6

iD1

iD1

(cid:15) The abc method computes a in multiparameter exponential families (5.54),

as does the resampling-based R algorithm accel.

Conﬁdence intervals require the number of bootstrap replications B to
be on the order of 2000, rather than the 200 or fewer needed for standard
errors; the corrections made to the standard intervals are more delicate than
standard errors and require greater accuracy.

There is one more cautionary note to sound concerning nuisance param-
eters: biases can easily get out of hand when the parameter vector (cid:22) is

11.5 Bootstrap-t Intervals

195

high-dimensional. Suppose we observe

ind(cid:24)

xi

and wish to set a conﬁdence interval for  DPn
1 x2
will be sharply biased upward if n is at all large. To be speciﬁc, if n D 10
and O

 D 20, we compute

for i D 1; 2; : : : ; n;

 DPn

i . The MLE O

N .(cid:22)i ; 1/

(11.41)

1 (cid:22)2

i

6

This makes7 O
centile,

z0 D ˆ

(cid:0)1.0:156/ D (cid:0)1:01:

(11.42)
BCŒ:025 (11.33) equal a ludicrously small bootstrap per-

OG

(cid:0)1.0:000034/;

(11.43)

a warning sign against the BC or BCa intervals, which work most depend-
ably for jz0j and jaj small, say  0:2.
A more general warning would be against blind trust in maximum likeli-
hood estimates in high dimensions. Computing z0 is a wise precaution even
if it is not used for BC or BCa purposes, in case it alerts one to dangerous
biases.

the standard method (11.1) (withbse estimated by the delta method) except

Conﬁdence intervals for classical applications were most often based on

in a few especially simple situations such as the Poisson. Second-order ac-
curate intervals are very much a computer-age development, with both the
algorithms and the inferential theory presupposing high-speed electronic
computation.

11.5 Bootstrap-t Intervals

The initial breakthrough on exact conﬁdence intervals came in the form of
Student’s t distribution in 1908. Suppose we independently observe data
from two possibly different normal distributions, x D .x1; x2; : : : ; xnx /
and y D .y1; y2; : : : ; yny /,

iid(cid:24)

xi

N .(cid:22)x; (cid:27) 2/

and yi

iid(cid:24)

N .(cid:22)y; (cid:27) 2/;

and wish to form a 0.95 central conﬁdence interval for

(11.44)

(11.45)

(11.46)

 D (cid:22)y (cid:0) (cid:22)x:
O
 D Ny (cid:0) Nx;

The obvious estimate is

7 Also OBCaŒ:025, a is zero in this model.

196

Bootstrap Conﬁdence Intervals

but its distribution depends on the nuisance parameter (cid:27) 2.

Student’s masterstroke was to base inference about  on the pivotal

quantity

O

 (cid:0) bse
t D
wherebse2 is an unbiased estimate of (cid:27) 2,
bse2 D 1

Pnx
1 .xi (cid:0) Nx/2 CPny

C 1
ny

1 .yi (cid:0) Ny/2

I

(11.47)

nx

represent the 100˛th percentile of a tdf distribution yields

nx C ny (cid:0) 2
(11.48)
t then has the “Student’s t distribution” with df D nx C ny (cid:0) 2 degrees of
freedom if (cid:22)x D (cid:22)y, no matter what (cid:27) 2 may be.
 (cid:0)bse (cid:1) t .1(cid:0)˛/
Letting t .˛/
df
 D .:062; :314/:

as the upper level-˛ interval of a Student’s t conﬁdence limit. Applied to
the difference between the AML and ALL scores in Figure 1.4, the central
0.95 Student’s t interval for  D EfAMLg (cid:0) EfALLg was calculated to be
(11.50)

O
t Œ˛ D O

O
t Œ:975

(cid:16) O

(11.49)

t Œ:025;

df

Here nx D 47, ny D 25, and df D 70.
Student’s theory depends on the normality assumptions of (11.44). The
bootstrap-t approach is to accept (or pretend) that t in (11.47) is pivotal,
but to estimate its distribution via bootstrap resampling. Nonparametric
bootstrap samples are drawn separately from x and y,

x

(cid:3)
1 ; x

(cid:3) D .x

(cid:3)
nx
from which we calculate O

(cid:3)
2 ; : : : ; x



and y

/

(cid:3) andbse

(cid:3)

/;

(cid:3)
ny

(cid:3)
1 ; y

(cid:3)
2 ; : : : ; y

(cid:3) D .y
, (11.46) and (11.48), giving
(cid:3) (cid:0) O
O
bse

(cid:3)

;

(11.51)

(11.52)

(cid:3) D

t

 playing the role of , as appropriate in the bootstrap world. Repli-
(cid:3).˛/ and cor-

(cid:3)b; b D 1; 2; : : : ; Bg provide estimated percentiles t

with O
cations ft
responding conﬁdence limits
O
t Œ˛ D O
(cid:3)


 (cid:0)bse (cid:1) t

(cid:3).1(cid:0)˛/:

(11.53)

For the AML–ALL example, the t

(cid:3) distribution differed only slightly
from a t70 distribution; the resulting 0.95 interval was .0:072; 0:323/, nearly

11.5 Bootstrap-t Intervals

197

the same as (11.50), lending credence to the original normality assump-
tions.

Figure 11.5 B D 2000 nonparametric replications of bootstrap-t
statistic for the student score correlation; small triangles show
0.025 and 0.975 percentile points. The histogram is sharply
skewed to the right; the solid curve is Student’s t density for 21
degrees of freedom.

Returning to the student score correlation example of Table 11.1, we can
apply bootstrap-t methods by still taking t D .
bse the approximate standard error .1 (cid:0) O
pivotal, but now with  the true correlation, O
p
 the sample correlation, and
19. Figure 11.5 shows the
 /=bse
histogram of B D 2000 nonparametric bootstrap replications t
(cid:3) (cid:0)
O

 (cid:0)  /=bse to be notionally
(cid:3) D .

. These gave bootstrap percentiles

 2/=

O


O

(cid:3)

(cid:16)

(cid:3).:975/ D .(cid:0)1:64; 2:59/

(cid:3).:025/; t

t

(11.54)
(which might be compared with .(cid:0)2:08; 2:08/ for a standard t21 distribu-
tion), and 0.95 interval .0:051; 0:781/ from (11.53), somewhat out of place
compared with the other entries in the right panel of Table 11.1.

Bootstrap-t intervals are not transformation invariant. This means they
can perform poorly or well depending on the scale of application. If per-
formed on Fisher’s scale (11.11) they agree well with exact intervals for

 t* valuesFrequency050100150200t distributiondf = 21−3−2−101234−1.642.59Bootstrap Conﬁdence Intervals

198

mula forbse.

the correlation coefﬁcient. A practical difﬁculty is the requirement of a for-

Nevertheless, the idea of estimating the actual distribution of a proposed
pivotal quantity has great appeal to the modern statistical spirit. Calculat-
ing the percentiles of the original Student t distribution was a multi-year
project in the early twentieth century. Now we can afford to calculate our
own special “t table” for each new application. Spending such computa-
tional wealth wisely, while not losing one’s inferential footing, is the cen-
tral task and goal of twenty-ﬁrst-century statisticians.

11.6 Objective Bayes Intervals and the Conﬁdence

Distribution

Interval estimates are ubiquitous. They play a major role in the scientiﬁc
discourse of a hundred disciplines, from physics, astronomy, and biology
to medicine and the social sciences. Neyman-style frequentist conﬁdence
intervals dominate the literature, but there have been inﬂuential Bayesian
and Fisherian developments as well, as discussed next.

g. /, Bayes’ rule (3.5) produces the posterior density of ,

Given a one-parameter family of densities f .
O
 /;

 / is the marginal densityR f .

O
 / D g. /f .
 /=f .
O
 /g. /d. The Bayes 0.95 cred-

g.j O

(11.55)

O

O
 / and a prior density

 / spans the central 0.95 region of g.j O

 /, say

where f .

ible interval C.j O

 / D .a.

O
 /; b.

O
 //;

with

C.j O
Z b.O /

a.O /

g.j O

 / d D 0:95;

(11.56)

(11.57)

and with posterior probability 0.025 in each tail region.

Conﬁdence intervals, of course, require no prior information, making
them eminently useful in day-to-day applied practice. The Bayesian equiv-
alents are credible intervals based on uninformative priors, Section 3.2.
“Matching priors,” those whose credible intervals nearly match Neyman
conﬁdence intervals, have been of particular interest. Jeffreys’ prior (3.17),

g. / D

I 1=2

I DZ  @

;



@

(cid:21)2

O
 /

log f .

O
 / d

O
 ;

f .

(11.58)

11.6 Objective Bayes intervals

199

provides a generally accurate matching prior for one-parameter problems.
Figure 3.2 illustrates this for the student score correlation, where the credi-
ble interval .0:093; 0:750/ is a near-exact match to the Neyman 0.95 inter-
val of Figure 11.1.
Difﬁculties begin with multiparameter families f(cid:22).x/ (5.1): we wish to
construct an interval estimate for a one-dimensional function  D t .(cid:22)/ of
the p-dimensional parameter vector (cid:22), and must somehow remove the ef-
fects of the p(cid:0) 1 “nuisance parameters.” In a few rare situations, including
the normal theory correlation coefﬁcient, this can be done exactly. Pivotal
methods do the job for Student’s t construction. Bootstrap conﬁdence inter-
vals greatly extend the reach of such methods, at a cost of greatly increased
computation.
Bayesians get rid of nuisance parameters by integrating them out of the
posterior density g.(cid:22)jx/ D g.(cid:22)/f(cid:22).x/=f .x/ (3.6) (x now representing
all the data, “x” equaling .x; y/ for the Student t setup (11.44)). That is,
we calculate8 the marginal density of  D t .(cid:22)/ given x, and call it h.jx/.
A credible interval for , C.jx/, is then constructed as in (11.56)–(11.57),
with h.jx/ playing the role of g.j O
 /. This leaves us the knotty problem
of choosing an uninformative multidimensional prior g.(cid:22)/. We will return
to the question after ﬁrst discussing ﬁducial methods, a uniquely Fisherian
device.
interpretation of pivotality. We rewrite the Student t pivotal t D .
(11.47) as

 (cid:0)bse (cid:1) t;
 D O
(11.59)
where t has a Student’s t distribution with df degrees of freedom, t (cid:24)
 andbse were ﬁxed at their calculated
tdf. Having observed the data .x; y/ (11.44), ﬁducial theory assigns  the
distribution implied by (11.59), as if O
values while t was distributed as tdf. Then O
˛ conﬁdence limit, is the 100˛th percentile of ’s ﬁducial distribution.

Fiducial constructions begin with what seems like an obviously incorrect

 (cid:0)  /=bse

O

t Œ˛ (11.49), the Student t level-

We seem to have achieved a Bayesian posterior conclusion without any
prior assumptions.9 The historical development here is confused by Fisher’s
refusal to accept Neyman’s conﬁdence interval theory, as well as his dispar-
agement of Bayesian ideas. As events worked out, all of Fisher’s immense
prestige was not enough to save ﬁducial theory from the scrapheap of failed
statistical methods.

8 Often a difﬁcult calculation, as discussed in Chapter 13.
9 “Enjoying the Bayesian omelette without breaking the Bayesian eggs,” in L. J. Savage’s

words.

200

Bootstrap Conﬁdence Intervals

 andbse exhaust the information about  available from the

And yet, in Arthur Koestler’s words, “The history of ideas is ﬁlled with
barren truths and fertile errors.” Fisher’s underlying rationale went some-
thing like this: O
data, after which there remains an irreducible component of randomness
described by t. This is an idea of substantial inferential appeal, and one
that can be rephrased in more general terms discussed next that bear on the
question of uninformative priors.
By deﬁnition, an upper conﬁdence limit O

xŒ˛ satisﬁes

n
  O

Pr

xŒ˛

o D ˛

n O
xŒ˛    O

Pr

xŒ˛ C (cid:15)

o D (cid:15):

(where now we have indicated the observed data x in the notation), and so

(11.60)

(11.61)

(11.62)

We can consider O
xŒ˛ as a one-to-one function between ˛ in .0; 1/ and  a
point in its parameter space ‚ (assuming that O
xŒ˛ is smoothly increasing
in ˛). Letting (cid:15) go to zero in (11.61) determines the conﬁdence density of
, say Qgx. /,

Qgx. / D d˛=d;

the local derivative of probability at location  for the unknown parameter,
the derivative being taken at  D O
Integrating Qgx. / recovers ˛ as a function of . Let 1 D O
2 D O

xŒ˛1 and

xŒ˛2 for any two values ˛1 < ˛2 in .0; 1/. Then
d D ˛2 (cid:0) ˛1

Qgx. / d DZ 2

Z 2

xŒ˛.

d˛

d

(11.63)

1

1

D Prf1    2g;

as in (11.60). There is nothing controversial about (11.63) as long as we
remember that the random quantity in Prf1    2g is not  but rather
the interval .1; 2/, which varies as a function of x. Forgetting this leads to
the textbook error of attributing Bayesian properties to frequentist results:
“There is 0.95 probability that  is in its 0.95 conﬁdence interval,” etc.

This is exactly what the ﬁducial argument does.10 Whether or not one
accepts (11.63), there is an immediate connection with matching priors.

10 Fiducial and conﬁdence densities agree, as can be seen in the Student t situation (11.59),

at least in the somewhat limited catalog of cases Fisher thought appropriate for ﬁducial
calculations.

11.6 Objective Bayes intervals

201

Suppose prior g.(cid:22)/ gives a perfect match to the conﬁdence interval system
O
xŒ˛. Then, by deﬁnition, its posterior density h.jx/ must satisfy

(cid:0)1 h.jx/ d D ˛ DZ Ox Œ˛
Z Ox Œ˛

(cid:0)1

(11.64)
for 0 < ˛ < 1. But this implies h.jx/ equals Qgx. / for all . That is,
the conﬁdence density Qgx. / is the posterior density of  given x for any
matching prior.

Qgx. / d

Figure 11.6 Conﬁdence density (11.62) for Poisson parameter 
having observed O
between 5.08 and 17.82, as in Table 11.2, and areas 0.025 in each
tail.

 D 10. There is area 0.95 under the curve

Figure 11.6 graphs the conﬁdence density for O
served O
function of  (11.62),

 (cid:24) Poi. / having ob-
 D 10. This was obtained by numerically differentiating ˛ as a

˛ D Prf10  Poi. /g ;

(11.65)
“” including splitting the atom of probability at 10. According to Ta-
ble 11.2, Qg10. / has area 0.95 between 5.08 and 17.82, and area 0.025 in
each tail. Whatever its provenance, the graph delivers a striking picture of
the uncertainty in the unknown value of .

05101520250.000.020.040.060.080.100.120.14qConfidence Density5.0817.82l10202

Bootstrap Conﬁdence Intervals

Bootstrap conﬁdence intervals provide easily computable conﬁdence den-
OG. / be the bootstrap cdf and Og. / its density function (ob-
sities. Let
tained by differentiating a smoothed version of OG. / when OG is based on
B bootstrap replications). The percentile conﬁdence limits O
(cid:0)1.˛/
(11.17) have ˛ D OG. /, giving

 Œ˛ D OG

Qgx. / D Og. /:

(11.66)
(It is helpful to picture this in Figure 11.4.) For the percentile method, the
bootstrap density is the conﬁdence density.
reweighting Og. /,

For the BCa intervals (11.39), the conﬁdence density is obtained by

Qgx. / D cw. /Og. /;

(11.67)

7

where

w. / D ' Œz =.1 C az / (cid:0) z0
.1 C az /2'.z C z0/

; with z D ˆ

(cid:0)1 OG. / (cid:0) z0:

(11.68)

Here ' is the standard normal density, ˆ its cdf, and c the constant that
makes Qgx. / integrate to 1. In the usual case where the bootstrap cdf is es-
timated from replications O
(cid:3)b, b D 1; 2; : : : ; B (either parametric or non-
parametric), the BCa conﬁdence density is a reweighted version of Og. /.
Deﬁne



(cid:3)b, BX

(cid:16) O



(cid:3)i

(cid:16) O



w

iD1

Wb D w

:

(11.69)

Then the BCa conﬁdence density is the discrete density putting weight Wb
on O
(cid:3)b.
Figure 11.7 returns to the student score data, n D 22 students, ﬁve scores



each, modeled normally as in Figure 10.6,

iid(cid:24)

for i D 1; 2; : : : ; 22:

xi

(11.70)
This is a p D 20-dimensional parametric family: 5 expectations, 5 vari-
ances, 10 covariances. The parameter of interest was taken to be

N5.(cid:21); †/

 D maximum eigenvalue of †:

(11.71)
It had MLE O
 D 683, this being the maximum eigenvalue of the MLE
sample covariance matrix O† (dividing each sum of squares by 22 rather
than 21).
B D 8000 parametric bootstrap replications11 O
(cid:3)b gave percentile and
11 B D 2000 would have been enough for most purposes, but B D 8000 gave a sharper



picture of the different curves.

11.6 Objective Bayes intervals

203

Figure 11.7 Conﬁdence densities for the maximum eigenvalue
parameter (11.71), using a multivariate normal model (11.70) for
the student score data. The dashed red curve is the percentile
method, solid black the BCa (with .z0; a/ D .0:178; 0:093/). The
dotted blue curve is the Bayes posterior density for , using
Jeffreys’ prior (11.72).

BCa conﬁdence densities as shown. In this case the weights Wb (11.69)
increased with O
(cid:3)b, pushing the BCa density to the right. Also shown is the
Bayes posterior density  for  starting from Jeffreys’ multiparameter prior 8
density



gJeff.(cid:22)/ D jI (cid:22)j1=2;

(11.72)
where I (cid:22) is the Fisher information matrix (5.26). It isn’t truly uninforma-
tive here, moving its credible limits upward from the second-order accurate
BCa conﬁdence limits. Formula (11.72) is discussed further in Chapter 13.
Bayesian data analysis has the attractive property that, after examin-
ing the data, we can express our remaining uncertainty in the language
of probability. Fiducial and conﬁdence densities provide something similar
for conﬁdence intervals, at least partially freeing the frequentist from the
interpretive limitations of Neyman’s intervals.

500100015000200400600800q: maximum eigenvaluePosterior DensityBCaJeffreysPercentile683l204

Bootstrap Conﬁdence Intervals

11.7 Notes and Details

Fisher’s theory of ﬁducial inference (1930) preceded Neyman’s approach,
formalized in (1937), which was presented as an attempt to put interval es-
timation on a ﬁrm probabilistic basis, as opposed to the mysteries of ﬁdu-
cialism. The result was an elegant theory of exact and optimal intervals,
phrased in hard-edged frequentistic terms. Readers familiar with the the-
ory will know that Neyman’s construction—a favorite name in the physics
literature—as pictured in Figure 11.1, requires some conditions on the fam-
O
ily of densities f .
 / to yield optimal intervals, a sufﬁcient condition being
monotone likelihood ratios.

Bootstrap conﬁdence intervals, Efron (1979, 1987), are neither exact nor
optimal, but aim instead for wide applicability combined with near-exact
accuracy. Second-order acuracy of BCa intervals was established by Hall
(1988). BCa is emphatically a child of the computer age, routinely requir-
ing B D 2000 or more bootstrap replications per use. Shortcut methods are
available. The “abc method” (DiCiccio and Efron, 1992) needs only 1% as
much computation, at the expense of requiring smoothness properties for
 D t .(cid:22)/, and a less automatic coding of the exponential family setting for
individual situations. In other words, it is less convenient.
Z  .:025/

denote the central 95% interval of density f .

1 [p. 183] Neyman’s construction. For any given value of , let . .:025/;  .:975//



d



O
 D 0:025 and

(cid:16) O
(cid:0)1 f
O
 / be the indicator function for O
1 if  .:025/ <
0 otherwise.

(cid:16) O

(



(cid:16) O

O
 /, satisfying

Z  .:975/
(cid:0)1 f
 2 . .:025/;  .:975//,

d



O
 <  .:975/

O
 D 0:975I

and let I .



I
O
 / has a two-point probability distribution,

By deﬁnition, I .

 D
(cid:16) O
 D



(

I

(11.73)

(11.74)

(11.75)

1 probability 0:95
0 probability 0:05:

O
 / a pivotal statistic, one whose distribution does not de-

This makes I .
pend upon .
Neyman’s construction takes the conﬁdence interval C.
to observed value O

(cid:16) O



 Dn

 W I

(cid:16) O



 D 1
o

 to be
C

O
 / corresponding

:

(11.76)

205

(11.77)

11.7 Notes and Details

O
 / has the desired coverage property

n

I

(cid:16) O



 D 1

o D 0:95

Then C.

Pr

o D Pr

(cid:16) O
n
 2
C
 .:025/ and O
 /, O
O



for any choice of the true parameter . (For the normal theory correlation
density of f .
 .:975/ are increasing functions of . This
makes our previous construction (11.6) agree with (11.76).) The construc-
tion applies quite generally, as long as we are able to deﬁne acceptance
regions of the sample space having the desired target probability content
for every choice of . This can be challenging in multiparameter families.
2 [p. 189] Fisherian correctness. Fisher, arguing against the Neyman para-
digm, pointed out that conﬁdence intervals could be accurate without being
correct: having observed xi
0.95 interval based on just the ﬁrst 10 observations would provide exact
0.95 coverage while giving obviously incorrect inferences for . If we can
reduce the situation to form (11.22), the percentile method intervals satisfy
Fisher’s “logic of inductive inference” for correctness, as at (4.31).
3 [p. 189] Bootstrap sample sizes. Why we need bootstrap sample sizes on
the order of B D 2000 for conﬁdence interval construction can be seen
in the estimation of the bias correction value z0 (11.32). The delta-method
standard error of z0 D ˆ

N .; 1/ for i D 1; 2; : : : ; 20, the standard

(cid:0)1.p0/ is calculated to be

iid(cid:24)

 p0.1 (cid:0) p0/

(cid:21)1=2

1

'.z0/

(11.78)
:D 0 this is
with '.z/ the standard normal density. With p0
about 1:25=B 1=2, equaling 0.028 at B D 2000, a none-too-small error for
use in the BC formula (11.33) or the BCa formula (11.39).

4 [p. 191] BCa accuracy and correctness. The BCa conﬁdence limit O

:D 0:5 and z0

BCaŒ˛

B

;

(11.39) is transformation invariant. Deﬁne

z0 C z.˛/

zŒ˛ D z0 C

;

so O
O
 /, and O(cid:30)
(cid:30) D m. /, O(cid:30) D m.
(cid:0)1.˛/ D mŒ OG
(cid:0)1.˛/ since O(cid:30)
isﬁes
percentiles. Therefore

BCaŒ˛ D OG
OH
O(cid:30)BCaŒ˛ D OH

(cid:16) OG

1 (cid:0) a.z0 C z.˛//

(cid:3) D m.r/, the bootstrap cdf

(11.79)
(cid:0)1fˆŒzŒ˛g. For a monotone increasing transformation
OH of O(cid:30)
(cid:3) sat-
(cid:3).˛// for the bootstrap

(cid:16) O
(cid:0)1Œ OG.

(cid:0)1 fˆ .zŒ˛/g D m

(cid:0)1 fˆ .zŒ˛/g D m

(11.80)
O
 / equals

(cid:3).˛/ D m.

verifying transformation invariance. (Notice that z0 D ˆ

BCaŒ˛

O


;

Bootstrap Conﬁdence Intervals

206
(cid:0)1Œ OH . O(cid:30)/ and is also transformation invariant, as is a, as discussed pre-
ˆ
viously.)

Exact conﬁdence intervals are transformation invariant, adding consider-
ably to their inferential appeal. For approximate intervals, transformation
invariance means that if we can demonstrate good behavior on any one
n
scale then it remains good on all scales. The model (11.38) to the (cid:30) scale
can be re-expressed as
1 C a O(cid:30)

o D f1 C a(cid:30)gf1 C a.Z (cid:0) z0/g ;

(11.81)

where Z is a standard normal variate, Z (cid:24)
O(cid:13) D (cid:13) C U;

Taking logarithms,

N .0; 1/.

(11.82)
where O(cid:13) D logf1 C a O(cid:30)g, (cid:13) D logf1 C a(cid:30)g, and U is the random variable
logf1 C a.Z (cid:0) z0/g; (11.82) represents the simplest kind of translation
model, where the unknown value of (cid:13) rigidly shifts the distribution of U .
The obvious conﬁdence limit for (cid:13),

O(cid:13) Œ˛ D O(cid:13) (cid:0) U .1(cid:0)˛/;

BCŒ˛ (11.33).

(11.83)
where U .1(cid:0)˛/ is the 100.1 (cid:0) ˛/th percentile of U , is then accurate, and
also “correct,” according to Fisher’s (admittedly vague) logic of inductive
inference. It is an algebraic exercise, given in Section 3 of Efron (1987),
to reverse the transformations  ! (cid:30) ! (cid:13) and recover O
BCaŒ˛ (11.39).
Setting a D 0 shows the accuracy and correctness of O
5 [p. 192] The acceleration a. This a appears in (11.38) as d(cid:27)(cid:30)=d(cid:30), the
rate of change of O(cid:30)’s standard deviation as a function of its expectation.
In one-parameter exponential families it turns out that this is one-third of
d(cid:27) =d; that is, the transformation to normality (cid:30) D m. / also decreases
the instability of the standard deviation, though not to zero.
The variance of the score function P
lx. / determines the standard de-
viation of the MLE O
 (4.17)–(4.18). In one-parameter exponential fami-
lies, one-sixth the skewness of P
lx. / gives a. The skewness connection can
be seen at work in estimate (11.40). In multivariate exponential families
(5.50), the skewness must be evaluated in the “least favorable” direction,
discussed further in Chapter 13. The R algorithm accel (book web site)
(cid:3)b/ to estimate a. The per-
uses B parametric bootstrap replications .
centile and BC intervals require only the replications O
(cid:3)b, while BCa also
requires knowledge of the underlying exponential family. See Sections 4,
6, and 7 of Efron (1987).

O


O
ˇ

(cid:3)b;



207

11.7 Notes and Details

 D P x2
tral chi-square variable with noncentrality parameter  D P (cid:22)2
6 [p. 195] Equation (11.42). Model (11.41) makes O
i a noncen-
i and n
degrees of freedom, written as O
;n. With O
 D 20 and n D 10, the
parametric bootstrap distribution is r (cid:24) (cid:31)2
20;10. Numerical evaluation gives
Prf(cid:31)2
Efron (1985) concerns conﬁdence intervals for parameters  D t .(cid:22)/ in
model (11.41), where third-order accurate conﬁdence intervals can be cal-
culated. The acceleration a equals zero for such problems, making the BC
intervals second-order accurate. In practice, the BC intervals usually per-
form well, and are a reasonable choice if the accleration a is unavailable.

 20g D 0:156, leading to (11.42).

 (cid:24) (cid:31)2

20;10

7 [p. 202] BCa conﬁdence density (11.68). Deﬁne

(cid:0)1h OG. /

i (cid:0) z0 D

z D ˆ

so that

z.˛/ D z

1 C az

(cid:0) z0

z0 C z.˛/

1 (cid:0) a(cid:0)z0 C z.˛/(cid:1) ;
 z

1 C az

(cid:0) z0



:

(11.84)

(11.85)

Here we are thinking of ˛ and  as functionally related by  D O
Differentiation yields

BCaŒ˛.

and ˛ D ˆ


(cid:16) z
(cid:16) z

(cid:0) z0
1Caz
.1 C az /2
1Caz


.1 C az /2'.z C z0/

(cid:0) z0

;

D '

D '

d˛
dz

dz
d

(11.86)

Og. /;

which together give d˛=d, verifying (11.68).

The name “conﬁdence density” seems to appear ﬁrst in Efron (1993),
though the idea is familiar in the ﬁducial literature. An ambitious frequen-
tist theory of conﬁdence distributions is developed in Xie and Singh (2013).
8 [p. 203] Jeffreys’ prior. Formula (11.72) is discussed further in Chapter 13,
in the more general context of uninformative prior distributions. The the-
ory of matching priors was initiated by Welch and Peers (1963), another
important reference being Tibshirani (1989).

12

Cross-Validation and Cp Estimates of

Prediction Error

Prediction has become a major branch of twenty-ﬁrst-century commerce.
Questions of prediction arise naturally: how credit-worthy is a loan appli-
cant? Is a new email message spam? How healthy is the kidney of a poten-
tial donor? Two problems present themselves: how to construct an effective
prediction rule, and how to estimate the accuracy of its predictions. In the
language of Chapter 1, the ﬁrst problem is more algorithmic, the second
more inferential. Chapters 16–19, on machine learning, concern predic-
tion rule construction. Here we will focus on the second question: having
chosen a particular rule, how do we estimate its predictive accuracy?

Two quite distinct approaches to prediction error assessment developed
in the 1970s. The ﬁrst, depending on the classical technique of cross-
validation, was fully general and nonparametric. A narrower (but more
efﬁcient) model-based approach was the second, emerging in the form of
Mallows’ Cp estimate and the Akaike information criterion (AIC). Both
theories will be discussed here, beginning with cross-validation, after a
brief overview of prediction rules.

12.1 Prediction Rules

Prediction problems typically begin with a training set d consisting of N
pairs .xi ; yi /,

d D f.xi ; yi /; i D 1; 2; : : : ; Ng ;

(12.1)

where xi is a vector of p predictors and yi a real-valued response. On the
basis of the training set, a prediction rule rd .x/ is constructed such that a

prediction Oy is produced for any point x in the predictor’s sample space X ,

Oy D rd .x/

for x 2

X :

(12.2)

208

(

Oy0 D

if O(cid:25)0 (cid:21) 0:5
if O(cid:25)0 < 0:5:

1

0

12.1 Prediction Rules

209

The inferential task is to assess the accuracy of the rule’s predictions. (In
practice there are usually several competing rules under consideration and
the main question is determining which is best.)
In the spam data of Section 8.1, xi comprised p D 57 keyword counts,
while yi (8.18) indicated whether or not message i was spam. The rule
rd .x/ in Table 8.3 was an MLE logistic regression ﬁt. Given a new mes-
sage’s count vector, say x0, rd .x0/ provided an estimated probability O(cid:25)0 of
it being spam, which could be converted into a prediction Oy0 according to

(12.3)
The diabetes data of Table 7.2, Section 7.3, involved the p D 10 pre-
dictors x D (age, sex, . . . , glu), obtained at baseline, and a response y
measuring disease progression one year later. Given a new patient’s base-
line measurements x0, we would like to predict his or her progression y0.
Table 7.3 suggests two possible prediction rules, ordinary least squares and
ridge regression using ridge parameter (cid:21) D 0:1, either of which will pro-
duce a prediction Oy0. In this case we might assess prediction error in terms
of squared error, .y0 (cid:0) Oy0/2.
In both of these examples, rd .x/ was a regression estimator suggested
by a probability model. One of the charms of prediction is that the rule
rd .x/ need not be based on an explicit model. Regression trees, as pictured
in Figure 8.7, are widely used1 prediction algorithms that do not require
model speciﬁcations. Prediction, perhaps because of its model-free nature,
is an area where algorithmic developments have run far ahead of their in-
ferential justiﬁcation.
Quantifying the prediction error of a rule rd .x/ requires speciﬁcation of
the discrepancy D.y; Oy/ between a prediction Oy and the actual response y.
The two most common choices are squared error
D.y; Oy/ D .y (cid:0) Oy/2;

(12.4)

and classiﬁcation error

D.y; Oy/ D

(

1 if y ¤ Oy
0 if y D Oy;

(12.5)

when, as with the spam data, the response y is dichotomous. (Prediction
of a dichotomous response is often called “classiﬁcation.”)

1 Random forests, one of the most popular machine learning prediction algorithms, is an

elaboration of regression trees. See Chapter 17.

210

Cross-Validation and Cp Estimates

For the purpose of error estimation, we suppose that the pairs .xi ; yi / in
the training set d of (12.1) have been obtained by random sampling from

some probability distribution F on .p C 1/-dimensional space RpC1,

iid(cid:24) F

for i D 1; 2; : : : ; N:

.xi ; yi /

(12.6)
The true error rate Errd of rule rd .x/ is the expected discrepancy of Oy0 D
rd .x0/ from y0 given a new pair .x0; y0/ drawn from F independently of
d,

Errd D EF fD.y0; Oy0/gI

(12.7)

d (and rd .(cid:1)/) is held ﬁxed in expectation (12.7), only .x0; y0/ varying.
Figure 12.1 concerns the supernova data, an example we will return to in
the next section.Absolute magnitudes yi have been measured for N D 39
relatively nearby Type Ia supernovas, with the data scaled such that

1

ind(cid:24)

i D 1; 2; : : : ; 39;

yi

(12.8)
is a reasonable model. For each supernova, a vector xi of p D 10 spectral
energies has been observed,

N .(cid:22)i ; 1/;

i D 1; 2; : : : ; 39:

xi D .xi1; xi 2; : : : ; xi10/;

(12.9)
Table 12.1 shows .xi ; yi / for i D 1; 2; : : : ; 5. (The frequency measure-
ments have been standardized to have mean 0 and variance 1, while y has
been adjusted to have mean 0.)
On the basis of the training set d D f.xi ; yi /; i D 1; 2; : : : ; 39g, we
wish to construct a rule rd .x/ that, given the frequency vector x0 for a
newly observed Type Ia supernova, accurately predicts2 its absolute mag-
nitude y0. To this end, a lasso estimate Q
ˇ.(cid:21)/ was ﬁt, with y in (7.42) the
vector .y1; y2; : : : ; y39/ and x the 39 (cid:2) 10 matrix having ith row xi; (cid:21)
was selected to minimize a Cp estimate of prediction error, Section 12.3,
yielding prediction rule

Oy0 D x

0

Q
ˇ.(cid:21)/:

0

(12.10)

(So in this case constructing rd .x/ itself involves error rate estimation.)

2 Type Ia supernovas were used as “standard candles” in the discovery of dark energy and

the cosmological expansion of the Universe, on the grounds that they have constant
absolute magnitude. This isn’t exactly true. Our training set is unusual in that the 39
supernovas are close enough to Earth to have y ascertained directly. This allows the
construction of a prediction rule based on the frequency vector x, which is observable
for distant supernovas, leading to improved calibration of the cosmological expansion.

12.1 Prediction Rules

211

Figure 12.1 The supernova data; observed absolute magnitudes
yi (on log scale) plotted versus predictions Oyi obtained from lasso
rule (12.10), for N D 39 nearby Type Ia supernovas. Predictions
based on 10 spectral power measurements, 7 of which had
nonzero coefﬁcients in Q

ˇ.(cid:21)/.

The plotted points in Figure 12.1 are . Oyi ; yi / for i D 1; 2; : : : ; N D 39.
These gave apparent error

NX
.yi (cid:0) Oyi /2 D 0:720:

err D 1

Comparing this withP.yi (cid:0) Ny/2=N D 3:91 yields an impressive-looking

(12.11)

N

iD1

“R squared” value

R2 D 1 (cid:0) 0:720=3:91 D 0:816:

(12.12)

Things aren’t really that good (see (12.23)). Cross-validation and Cp meth-
ods allow us to correct apparent errors for the fact that rd .x/ was chosen
to make the predictions Oyi ﬁt the data yi.
Prediction and estimation are close cousins but they are not twins. As
discussed earlier, prediction is less model-dependent, which partly accounts
for the distinctions made in Section 8.4. The prediction criterion Err (12.7)

lllllllllllllllllllllllllllllllllllllll−4−20246−4−20246Predicted magnitude y^iAbsolute magnitude yiapparent mean squarederror = 0.72212

Cross-Validation and Cp Estimates

Table 12.1 Supernova data; 10 frequency measurements and response
variable “absolute magnitude” for the ﬁrst 5 of N D 39 Type Ia
supernovas. In terms of notation (12.1), frequency measurements are x
and magnitude y.

SN5
SN3
SN4
SN2
SN1
(cid:0).08
(cid:0).84 (cid:0)1.89
.41
.26
(cid:0).81
(cid:0).80
(cid:0).93
(cid:0).46
1.02
(cid:0).13
(cid:0).21
1.14
2.41
.32
(cid:0).86 (cid:0)1.12
1.31
.77
.18
(cid:0).65
(cid:0).94
(cid:0).86
(cid:0).68
.68
(cid:0)1.27 (cid:0)1.53
(cid:0).35
.30
.72
(cid:0).82
.09 (cid:0)1.04
.62
.34
.56 (cid:0)1.53
(cid:0).43
.26 (cid:0)1.10
.62 (cid:0)1.49
.18 (cid:0)1.32
(cid:0).02
(cid:0).49 (cid:0)1.09
(cid:0).3
(cid:0).54 (cid:0)1.70
(cid:0):54
(cid:0):22
:95 (cid:0)3:75
2:12

x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
mag

is an expectation over the .x; y/ space. This emphasizes good overall per-
formance, without much concern for behavior at individual points x in X .
Shrinkage usually improves prediction. Consider a Bayesian model like
that of Section 7.1,

(cid:22)i (cid:24)

N .0; A/

and xij(cid:22)i (cid:24)

N .(cid:22)i ; 1/

for i D 1; 2; : : : ; N:

The Bayes shrinkage estimator, which is ideal for estimation,

O(cid:22)i D Bxi ;

B D A=.A C 1/;

(12.13)

(12.14)

(12.15)

(12.16)

(12.17)

is also ideal for prediction. Suppose that in addition to the observations
xi there are independent unobserved replicates, one for each of the N xi
values,

yi (cid:24)

N .(cid:22)i ; 1/

for i D 1; 2; : : : ; N;

that we wish to predict. The Bayes predictor
Oyi D Bxi
)

has overall Bayes prediction error

(

NX
.yi (cid:0) Oyi /2

E

1

N

iD1

D B C 1;

213
which cannot be improved upon. The MLE rule Oyi D xi has Bayes predic-
tion error 2, which is always worse than (12.17).

12.2 Cross-Validation

As far as prediction is concerned it pays to overshrink, as illustrated in
Figure 7.1 for the James–Stein version of situation (12.13). This is ﬁne for
prediction, but less ﬁne for estimation if we are concerned about extreme
cases; see Table 7.4. Prediction rules sacriﬁce the extremes for the sake of
the middle, a particularly effective tactic in dichotomous situations (12.5),
where the cost of individual errors is bounded. The most successful ma-
chine learning prediction algorithms, discussed in Chapters 16–19, carry
out a version of local Bayesian shrinkage in selected regions of X .

12.2 Cross-Validation

Having constructed a prediction rule rd .x/ on the basis of training set d,
we wish to know its prediction error Err D EF fD.y0; Oy0/g (12.7) for a
new case obtained independently of d. A ﬁrst guess is the apparent error

NX

iD1

err D 1

N

D.yi ; Oyi /;

(12.18)

the average discrepancy in the training set between yi and its prediction
Oyi D rd .xi /; err usually underestimates Err since rd .x/ has been adjusted3
to ﬁt the observed responses yi.

The ideal remedy, discussed in Section 12.4, would be to have an inde-

pendent validation set (or test set) dval of Nval additional cases,

Nval

jD1

Cross-validation attempts to mimiccErrval without the need for a valida-
tion set. Deﬁne d .i / to be the reduced training set in which pair .xi ; yi /
has been omitted, and let rd.i /.(cid:1)/ indicate the rule constructed on the basis
err DP

3 Linear regression using ordinary least squares ﬁtting provides a classical illustration:
i .yi (cid:0) Oyi /2=.N (cid:0) p/, where p is

i .yi (cid:0) Oyi /2=N must be increased toP

the degrees of freedom, to obtain an unbiased estimate of the noise variance (cid:27) 2.

(cid:9) :

dval D˚.x0j ; y0j /; j D 1; 2; : : : ; Nval
NvalX

D.y0j ; Oy0j /;

Oy0j D rd .x0j /:

cErrval D 1

This would provide an unbiased estimate of Err,

(12.19)

(12.20)

214

Cross-Validation and Cp Estimates

of d .i /. The cross-validation estimate of prediction error is
Oy.i / D rd.i /.xi /:

cErrcv D 1

D.yi ; Oy.i //;

NX

iD1

N

(12.21)

Now .xi ; yi / is not involved in the construction of the prediction rule for

yi.cErrcv (12.21) is the “leave one out” version of cross-validation. A more
tions for the yi in group j . ThencErrcv is evaluated as in (12.21). Besides

common tactic is to leave out several pairs at a time: d is randomly parti-
tioned into J groups of size about N=J each; d .j /, the training set with
group j omitted, provides rule rd.j /.x/, which is used to provide predic-

reducing the number of rule constructions necessary, from N to J , group-
ing induces larger changes among the J training sets, improving the predic-
tive performance on rules rd .x/ that include discontinuities. (The argument
here is similar to that for the jackknife, Section 10.1.)
Cross-validation was applied to the supernova data pictured in Figure 12.1.
The 39 cases were split, randomly, into J D 13 groups of three cases each.
This gave

cErrcv D 1:17;

(12.22)
(12.21), 62% larger than err D 0:72 (12.11). The R2 calculation (12.12)
now yields the smaller value

R2 D 1 (cid:0) 1:17=3:91 D 0:701:

(12.23)

We can apply cross-validation to the spam data of Section 8.1, having
N D 4061 cases, p D 57 predictors, and dichotomous response y. For
this example, each of the 57 predictors was itself dichotomized to be either
0 or 1 depending on whether the original value xij equaled zero or not.
A logistic regression, Section 8.1, regressing yi on the 57 dichotomized
predictors, gave apparent classiﬁcation error (12.5)

i.e., 295 wrong predictions among the 4061 cases. Cross-validation, with
J D 10 groups of size 460 or 461 each, increased this to

err D 0:064;
cErrcv D 0:069;

(12.24)

(12.25)

an increase of 8%.

Glmnet is an automatic model building program that, among other
things, constructs a lasso sequence of logistic regression models, adding

12.2 Cross-Validation

215

Figure 12.2 Spam data. Apparent error rate err (blue) and
cross-validated estimate (red) for a sequence of prediction rules
generated by glmnet. The degrees of freedom are the number of
nonzero regression coefﬁcients: df D 57 corresponds to ordinary
logistic regression, which gave apparent err 0.064, cross-validated
rate 0.069. The minimum cross-validated error rate is 0.067.

variables one at a time in their order of apparent predictive power; see
Chapter 16. The blue curve in Figure 12.2 tracks the apparent error err
(12.18) as a function of the number of predictors employed. Aside from nu-
merical artifacts, err is monotonically decreasing, declining to err D 0:064
Glmnet produced prediction error estimatescErrcv for each of the suc-
for the full model that employs all 57 predictors, i.e., for the usual logistic
regression model, as in (12.24).

cessive models, shown by the red curve. These are a little noisy themselves,
but settle down between 4% and 8% above the corresponding err estimates.
The minimum value

cErrcv D 0:067

(12.26)

occurred for the model using 47 predictors.

The difference between (12.26) and (12.25) is too small to take seriously

given the noise in the cErrcv estimates. There is a more subtle objection:
the choice of “best” prediction rule based on comparative cErrcv estimates

is not itself cross-validated. Each case .xi ; yi / is involved in choosing its

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.000.050.100.150.200.25Degrees of freedomMisclassification error ratelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllogisticregression216

own best prediction, socErrcv at the apparently optimum choice cannot be

Cross-Validation and Cp Estimates

taken entirely at face value.

Nevertheless, perhaps the principal use of cross-validation lies in choos-
ing among competing prediction rules. Whether or not this is fully justiﬁed,
it is often the only game in town. That being said, minimum predictive er-
ror, no matter how effectuated, is a notably weaker selection principle than
minimum variance of estimation.

As an example, consider an iid normal sample

iid(cid:24)

xi

(12.27)
having mean Nx and median Mx. Both are unbiased for estimating (cid:22), but Nx is
much more efﬁcient,

N .(cid:22); 1/;

i D 1; 2; : : : ; 25;

var.Mx/= var.Nx/

:D 1:57:

(12.28)

E˚.x0 (cid:0) Mx/2(cid:9)ıE˚.x0 (cid:0) Nx/2(cid:9) D 1:02:

Suppose we wish to predict a future observation x0 independently selected
from the same N .(cid:22); 1/ distribution. In this case there is very little advan-
tage to Nx,
(12.29)
The noise in x0 (cid:24)
N .(cid:22); 1/ dominates its prediction error. Perhaps the
proliferation of prediction algorithms to be seen in Part III reﬂects how
weakly changes in strategy affect prediction error.
Table 12.2 Ratio of predictive errors Ef.Nx0 (cid:0) Mx/2g=Ef.Nx0 (cid:0) Nx/2g for Nx0
the mean of an independent sample of size N0 from N .(cid:22); 1/; Nx and Mx are
the mean and median from xi (cid:24)

N .(cid:22); 1/ for i D 1; 2; : : : ; 25.

N0
Ratio

1

1.02

10
1.16

100
1.46

1000 1
1.57
1.56

In this last example, suppose that our task was to predict the average

Nx0 of N0 further draws from the N .(cid:22); 1/ distribution. Table 12.2 shows

the ratio of predictive errors as a function of N0. The superiority of the
mean compared to the median reveals itself as N0 gets larger. In this super-
simpliﬁed example, the difference between prediction and estimation lies
in predicting the average of one versus an inﬁnite number of future obser-
vations.

DoescErrcv actually estimate Errd as deﬁned in (12.7)? It seems like the

answer must be yes, but there is some doubt expressed in the literature, for

12.2 Cross-Validation

217
reasons demonstrated in the following simulation: we take the true distri-
bution F in (12.6) to be the discrete distribution OF that puts weight 1=39
on each of the 39 .xi ; yi / pairs of the supernova data.4 A random sample
with replacement of size 39 from OF gives simulated data set d
(cid:3) and pre-
(cid:3), gives cErr
(cid:3) .(cid:1)/ based on the lasso/Cp recipe used originally. The same
diction rule rd
(cid:3)
cross-validation procedure as before, applied to d
cv. Because
this is a simulation, we can also compute the actual mean-squared error
(cid:3) .(cid:1)/ applied to the true distribution OF ,
rate of rule rd

D .yi ; rd

(cid:3) .xi // :

(12.30)

39X

iD1

Err

(cid:3) D 1
39

with cross-validation estimatecErr
supernova data.cErr

Figure 12.3 Simulation experiment comparing true error Err

(cid:3)
cv; 500 simulations based on the

;cErr

(cid:3)
cv and Err are negatively correlated.
(cid:3)
Figure 12.3 plots .Err

(cid:3)
ble 12.3. cErr
cv/ for 500 simulations, using squared er-
ror discrepancy D.y; Oy/ D .y (cid:0) Oy/2. Summary statistics are given in Ta-
(cid:3)
cv has performed well overall, averaging 1.07, quite near the
true Err 1.02, both estimates being 80% greater than the average appar-
ent error 0.57. However, the ﬁgure shows something unsettling: there is a
4 Simulation based on OF is the same as nonparametric bootstrap analysis, Chapter 10.

********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************1.01.52.02.50.51.01.52.02.53.0Err*Errcv*l218

Cross-Validation and Cp Estimates
(cid:3)
cv, and apparent

(cid:3)
Table 12.3 True error Err
error err(cid:3); 500 simulations based on supernova data. Correlation (cid:0)0:175
(cid:3)
between Err

, cross-validated errorcErr
Err(cid:3) cErr

andcErr

(cid:3)
cv.

(cid:3)
cv
1.07
.34

err(cid:3)
.57
.16

Mean
St dev

1.02
.27

negative correlation betweencErr

(cid:3)
cv and Err

(cid:3)

. Large values ofcErr

(cid:3)
cv go with

smaller values of the true prediction error, and vice versa.

Our original deﬁnition of Err,

Errd D EF fD.y0; rd .x0//g ;

for rd

(cid:3) .(cid:1)/. IfcErr

(12.31)
took rd .(cid:1)/ ﬁxed as constructed from d, only .x0; y0/ (cid:24) F random. In other
words, Errd was the expected prediction error for the speciﬁc rule rd .(cid:1)/, as
(cid:3)
As it is, all we can say is thatcErr
we would expect to see a positive
is Err
correlation in Figure 12.3.
(cid:3)
makes cross-validation a strongly frequentist device:cErrcv is estimating the
cv is estimating the expected predictive
error, where d as well as .x0; y0/ is random in deﬁnition (12.31). This
average prediction error of the algorithm producing rd .(cid:1)/, not of rd .(cid:1)/ itself.

(cid:3)
cv is tracking Err

(cid:3)

12.3 Covariance Penalties

Cross-validation does its work nonparametrically and without the need for
probabilistic modeling. Covariance penalty procedures require probability
models, but within their ambit they provide less noisy estimates of predic-
tion error. Some of the most prominent covariance penalty techniques will
be examined here, including Mallows’ Cp, Akaike’s information criterion
(AIC), and Stein’s unbiased risk estimate (SURE).
The covariance penalty approach treats prediction error estimation in
a regression framework: the predictor vectors xi in the training set d D
f.xi ; yi /; i D 1; 2; : : : ; Ng (12.1) are considered ﬁxed at their observed
values, not random as in (12.6). An unknown vector (cid:22) of expectations
(cid:22)i D Efyig has yielded the observed vector of responses y according to
some given probability model, which to begin with we assume to have the

simple form

12.3 Covariance Penalties

y (cid:24) .(cid:22); (cid:27) 2I/I

219

(12.32)

that is, the yi are uncorrelated, with yi having unknown mean (cid:22)i and vari-
ance (cid:27) 2. We take (cid:27) 2 as known, though in practice it must usually be esti-
mated.
A regression rule r.(cid:1)/ has been used to produce an estimate of vector (cid:22),
(12.33)

O(cid:22) D r.y/:

(Only y is included in the notation since the predictors xi are considered
ﬁxed and known.) For instance we might take

O(cid:22) D r.y/ D X .X

0

(cid:0)1X

0

y;

X /

(12.34)
where X is the N (cid:2) p matrix having xi as the ith row, as suggested by the
linear regression model (cid:22) D X ˇ.
In covariance penalty calculations, the estimator O(cid:22) also functions as a
predictor. We wonder how accurate O(cid:22) D r.y/ will be in predicting a new
vector of observations y0 from model (12.32),

y0 (cid:24) .(cid:22); (cid:27) 2I/;

independent of y:

(12.35)

To begin with, prediction error will be assessed in terms of squared dis-
crepancy,

(12.36)
for component i, where E0 indicates expectation with y0i random but O(cid:22)i
held ﬁxed. Overall prediction error is the average5

Erri D E0

˚.y0i (cid:0) O(cid:22)i /2(cid:9)
NX

Erri :

Err(cid:1) D 1

iD1
The apparent error for component i is

N

erri D .yi (cid:0) O(cid:22)i /2:

(12.37)

(12.38)

A simple but powerful lemma underlies the theory of covariance penalties.
Lemma Let E indicate expectation over both y in (12.32) and y0 in
(12.35). Then

EfErrig D Eferrig C 2 cov. O(cid:22)i ; yi /;

(12.39)

5 Err(cid:1) is sometimes called “insample error,” as opposed to “outsample error” Err (12.7),

though in practice the two tend to behave similarly.

Cross-Validation and Cp Estimates

220
where the last term is the covariance between the ith components of O(cid:22) and
y,

cov. O(cid:22)i ; yi / D E f. O(cid:22)i (cid:0) (cid:22)i /.yi (cid:0) (cid:22)i /g :

(12.40)

i

(cid:0) 2(cid:15)i ıi C ı2

(Note: (12.40) does not require Ef O(cid:22)ig D (cid:22)i.)
Proof Letting (cid:15)i D yi (cid:0) (cid:22)i and ıi D . O(cid:22)i (cid:0) (cid:22)i /, the elementary equality
.(cid:15)i (cid:0) ıi /2 D (cid:15)2
.yi (cid:0) O(cid:22)i /2 D .yi (cid:0) (cid:22)i /2 (cid:0) 2. O(cid:22)i (cid:0) (cid:22)i /.yi (cid:0) (cid:22)i / C . O(cid:22)i (cid:0) (cid:22)i /2;
and likewise
.y0i (cid:0) O(cid:22)i /2 D .y0i (cid:0) (cid:22)i /2 (cid:0) 2. O(cid:22)i (cid:0) (cid:22)i /.y0i (cid:0) (cid:22)i /C . O(cid:22)i (cid:0) (cid:22)i /2: (12.42)
Taking expectations, (12.41) gives

i becomes

(12.41)

Eferrig D (cid:27) 2 (cid:0) 2 cov. O(cid:22)i ; yi / C E. O(cid:22)i (cid:0) (cid:22)i /2;

(12.43)

(12.44)

while (12.42) gives

EfErrig D (cid:27) 2 C E. O(cid:22)i (cid:0) (cid:22)i /2;

the middle term on the right side of (12.42) equaling zero because of the
independence of y0i and O(cid:22)i. Taking the difference between (12.44) and
(cid:4)
(12.43) veriﬁes the lemma.

Note: The lemma remains valid if (cid:27) 2 varies with i.
The lemma says that, on average, the apparent error erri understimates
the true prediction error Erri by the covariance penalty 2 cov. O(cid:22)i ; yi /. (This
makes intuitive sense since cov.(cid:22)i ; yi / measures the amount by which yi
inﬂuences its own prediction O(cid:22)i.) Covariance penalty estimates of predic-
tion error take the formcErri D erri C2dcov. O(cid:22)i ; yi /;
wheredcov. O(cid:22)i ; yi / approximates cov.(cid:22)i ; yi /; overall prediction error (12.37)
cErr(cid:1) D errC 2
dcov. O(cid:22)i ; yi /;
where err DP erri =N as before.
The form ofdcov. O(cid:22)i ; yi / in (12.45) depends on the context assumed for

is estimated by

NX

(12.45)

(12.46)

N

iD1

the prediction problem.

12.3 Covariance Penalties

221

(1)

Suppose that O(cid:22) D r.y/ in (12.32)–(12.33) is linear,

(12.47)
where c is a known N -vector and M a known N (cid:2) N matrix. Then the
covariance matrix between O(cid:22) and y is

O(cid:22) D c C My;

giving cov. O(cid:22)i ; yi / D (cid:27) 2Mi i, Mi i the ith diagonal element of M,

and, since err DP

cov. O(cid:22); y/ D (cid:27) 2M ;
cErri D erri C2(cid:27) 2Mi i ;
i .yi (cid:0) O(cid:22)i /2=N ,
NX
cErr(cid:1) D 1
.yi (cid:0) O(cid:22)i /2 C 2(cid:27) 2

(12.48)

(12.49)

tr.M /:

(12.50)

Formula (12.50) is Mallows’ Cp estimate of prediction error. For OLS
0 has tr.M / D p, the number of

X /

0

N

N

iD1
estimation (12.34), M D X .X
NX
cErr(cid:1) D 1
predictors, so
.yi (cid:0) O(cid:22)i /2 C 2

(cid:0)1X

N

iD1

y yielded err DP.yi (cid:0) O(cid:22)i /2=39 D 0:719. The covariance penalty, with

For the supernova data (12.8)–(12.9), the OLS predictor O(cid:22) D X .X
(cid:0)1
0
N D 39, (cid:27) 2 D 1, and6 p D 10, was 0.513, giving Cp estimate of predic-
tion error

X /

X

N

0

cErr(cid:1) D 0:719 C 0:513 D 1:23:

(12.52)

(cid:27) 2p:

(12.51)

For OLS regression, the degrees of freedom p, the rank of matrix X in
(12.34), determines the covariance penalty .2=N /(cid:27) 2p in (12.51). Compar-
ing this with (12.46) leads to a general deﬁnition of degrees of freedom df
for a regression rule O(cid:22) D r.y/,

df D .1=(cid:27) 2/

NX

iD1

dcov. O(cid:22)i ; yi /:

(12.53)

This deﬁnition provides common ground for comparing different types of
regression rules. Rules with larger df are more ﬂexible and tend toward
better apparent ﬁts to the data, but require bigger covariance penalties for
fair comparison.
6 We are not counting the intercept as an 11th predictor since y and all the xi were

standardized to have mean 0, all our models assuming zero intercept.

2

3

222

Cross-Validation and Cp Estimates

(2)
For lasso estimation (7.42) and (12.10), it can be shown that for-
mula (12.51), with p equaling the number of nonzero regression coefﬁ-
cients, holds to a good approximation.  The lasso rule used in Figure 12.1
for the supernova data had p D 7; err was 0.720 for this rule, almost the
same as for the OLS rule above, but the Cp penalty is less, 2(cid:1)7=39 D 0:359,
giving

cErr(cid:1) D 0:720 C 0:359 D 1:08;
compared with 1.23 for OLS. This estimate does not account for the data-
based selection of the choice p D 7, see item (4) below.
(3)

If we are willing to add multivariate normality to model (12.32),

(12.54)

y (cid:24)

Np.(cid:22); (cid:27) 2I/;

(12.55)

we can drop the assumption of linearity (12.47). In this case it can be
shown that, for any differentiable estimator O(cid:22) D r.y/, the covariance in
formula (12.51) is given by

cov. O(cid:22)i ; yi / D (cid:27) 2Ef@ O(cid:22)i =@yig;

(12.56)
(cid:27) 2 times the partial derivative of O(cid:22)i with respect to yi. (Another measure of
yi’s inﬂuence on its own prediction.) The SURE formula (Stein’s unbiased
risk estimator) is

;

(12.57)

with corresponding estimate for overall prediction error

cErri D erri C2(cid:27) 2 @ O(cid:22)i
NX
cErr(cid:1) D errC 2(cid:27) 2

@yi

@ O(cid:22)i

(12.58)
SURE was applied to the rule O(cid:22) D lowess(x,y,1/3) for the kid-
ney ﬁtness data of Figure 1.2. The open circles in Figure 12.4 plot the
component-wise degrees of freedom estimates7

iD1

@yi

N

:

(obtained by numerical differentiation) versus agei. Their sum

@ O(cid:22)i

@yi

;

i D 1; 2; : : : ; N D 157;
NX

@ O(cid:22)i

D 6:67

iD1

@yi

(12.59)

(12.60)

7 Notice that the factor (cid:27) 2 in (12.56) cancels out in (12.53).

12.3 Covariance Penalties

223

estimates the total degrees of freedom, as in (12.53), implying that
lowess(x,y,1/3) is about as ﬂexible as a sixth-degree polynomial ﬁt,
with df = 7.

Figure 12.4 Analysis of the lowess(x,y,1/3) ﬁt to kidney
data of Figure 1.2. Open circles are SURE coordinate-wise df
estimates @ O(cid:22)i =@yi, plotted versus agei, giving total degrees of
freedom 6.67. The solid curve tracks bootstrap coordinate-wise
estimates (12.65), with their sum giving total df D 6:81.

(4)
The parametric bootstrap8 of Section 10.4 can be used to estimate
the covariances cov. O(cid:22)i ; yi / in the lemma (12.39). The data vector y is as-
sumed to be generated from a member f(cid:22).y/ of a given parametric family

D˚f(cid:22).y/; (cid:22) 2 (cid:127)(cid:9) ;

F

yielding O(cid:22) D r.y/,

(12.62)
Parametric bootstrap replications of y and O(cid:22) are obtained by analogy with

f(cid:22) ! y ! O(cid:22) D r.y/:

(12.61)

8 There is also a nonparametric bootstrap competitor to cross-validation, the “.632

estimate;” see the chapter endnote 4.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll20304050607080900.00.10.20.30.4agedf estimateDegrees of FreedomSURE = 6.67Bootstrap = 6.81224

(12.62),9

Cross-Validation and Cp Estimates

A large number B of replications then yield bootstrap estimates

f O(cid:22) ! y
BX
dcov. O(cid:22)i ; yi / D 1
. O(cid:22)
(cid:3)b

i

B

bD1

(cid:3) ! O(cid:22)

(cid:3) D r.y

(cid:3)

/:

(12.63)

(cid:0) O(cid:22)

(cid:3)(cid:1)
i /.y

(cid:3)b

i

(cid:0) y

(cid:3)(cid:1)
i /;

(12.64)

B D 1000 parametric bootstrap replications . O(cid:22)
(cid:3)

the dot notation indicating averages over the B replications.
/ were obtained
from the normal model (12.55), taking O(cid:22) in (12.63) to be the estimate from
lowess(x,y,1/3) as in Figure 1.2. A standard linear regression, of y
as a 12th-degree polynomial function of age, gave O(cid:27) 2 D 3:28. Covariances
were computed as in (12.64), yielding coordinate-wise degrees of freedom
estimates (12.53),

; y

(cid:3)

dfi Ddcov. O(cid:22)i ; yi /=O(cid:27) 2:

(12.65)

The solid curve in Figure 12.4 plots dfi as a function of agei. These are
seen to be similar to but less noisy than the SURE estimates. They totaled
6.81, nearly the same as (12.60). The overall covariance penalty term in

(12.46) equaled 0.284, increasingcErr(cid:1) by about 9% over err D 3:15.
The advantage of parametric bootstrap estimates (12.64) of covariance
penalties is their applicability to any prediction rule O(cid:22) D r.y/ no matter
how exotic. Applied to the lasso estimates for the supernova data, B D
1000 replications yielded total df D 6:85 for the rule that always used
p D 7 predictors, compared with the theoretical approximation df D 7.
Another 1000 replications, now letting O(cid:22)
/ choose the apparently
(cid:3) each time, increased the df estimate to 7.48, so the adaptive choice
best p
of p cost about 0.6 extra degrees of freedom. These calculations exem-
plify modern computer-intensive inference, carrying through error estima-
tion for complicated adaptive prediction rules on a totally automatic basis.
(5) Covariance penalties can apply to measures of prediction error other
than squared error D.yi ; O(cid:22)i / D .yi (cid:0) O(cid:22)i /2. We will discuss two examples
of a general theory. First consider classiﬁcation, where yi equals 0 or 1 and
9 It isn’t necessary for the O(cid:22) in (12.63) to equal O(cid:22) D r.y/. The calculation (12.64) was
rerun taking O(cid:22) in (12.63) from lowess(x,y,1/6) (but with r.y/ still from
lowess(x,y,1/3)) with almost identical results. In general, one might take O(cid:22) in
(12.63) to be from a more ﬂexible, less biased, estimator than r.y/.

(cid:3) D r.y

(cid:3)

12.3 Covariance Penalties
similarly the predictor O(cid:22)i, with dichotomous error
if yi ¤ O(cid:22)i
if yi D O(cid:22)i ;

D.yi ; O(cid:22)i / D

(

1

0

225

(12.66)

as in (12.5).10 In this situation, the apparent error is the observed proportion
of prediction mistakes in the training set (12.1),
err D #fyi ¤ O(cid:22)ig=N:

(12.67)

Now the true prediction error for case i is

Erri D Pr0fy0i ¤ O(cid:22)ig;

(12.68)
the conditional probability given O(cid:22)i that an independent replicate y0i of yi
will be incorrectly predicted. The lemma holds as stated in (12.39), leading
to the prediction error estimate

cErr(cid:1) D #fyi ¤ O(cid:22)ig

N

C 2
N

NX

iD1

cov. O(cid:22)i ; yi /:

(12.69)

Some algebra yields
cov. O(cid:22)i ; yi / D (cid:22)i .1 (cid:0) (cid:22)i / .Prf O(cid:22)i D 1jyi D 1g (cid:0) Prf O(cid:22)1 D 1jyi D 0g/ ;
(12.70)
with (cid:22)i D Prfyi D 1g, showing again the covariance penalty measuring
the self-inﬂuence of yi on its own prediction.
As a second example, suppose that the observations yi are obtained
from different members of a one-parameter exponential family f(cid:22).y/ D
expf(cid:21)y (cid:0) (cid:13).(cid:21)/gf0.y/ (8.32),
yi (cid:24) f(cid:22)i .yi /
Z
 fyi .yi /

 fy.Y /
˚log(cid:0)fy .y/(cid:1) (cid:0) log(cid:0)f O(cid:22).y/(cid:1)(cid:9) :

According to (8.33), the apparent errorP D.yi ; O(cid:22)i / is then

for i D 1; 2; : : : ; N;


D.y; O(cid:22)/ D 2
NX

and that error is measured by the deviance (8.31),

Y

 D 2

err D 2

fy.Y / log

(12.71)

f O(cid:22).Y /

d Y:

(12.72)

log

N

iD1

f O(cid:22)i .yi /

N

(12.73)

10 More generally, O(cid:25)i is some predictor of Prfyi D 1g, and O(cid:22)i is the indicator function

I. O(cid:25)i (cid:21) 0:5/.

226

Cross-Validation and Cp Estimates

In this case the general theory gives overall covariance penalty

(cid:16)O



NX

iD1

penalty D 2

N

(cid:21)i ; O(cid:22)i

cov

;

(12.74)

where O
(cid:21)i is the natural parameter in family (8.32) corresponding to O(cid:22)(cid:21)
(e.g., O
(cid:21)i D log O(cid:22)i for Poisson observations). Moreover, if O(cid:22) is obtained
as the MLE of (cid:22) in a generalized linear model with p degrees of freedom
(8.22),

penalty :D 2p

N

(12.75)

˚log(cid:0)f O(cid:22).y/(cid:1) (cid:0) p(cid:9) C constant;

to a good approximation. The corresponding version of cErr(cid:1) (12.46) can
then be written ascErr(cid:1)
the constant .2=N / log.fy .y// not depending on O(cid:22).
The term in brackets is the Akaike information criterion (AIC): if the
statistician is comparing possible prediction rules r .j /.y/ for a given data
set y, the AIC says to select the rule maximizing the penalized maximum
likelihood

:D (cid:0) 2
N

(12.76)

log(cid:0)f O(cid:22).j / .y/(cid:1) (cid:0) p.j /;

.

(12.77)
where O(cid:22).j / is rule j ’s MLE and p.j / its degrees of freedom. Comparison
with the smallest value ofcErr.j /(cid:1)
with (12.76) shows that for GLMs, the AIC amounts to selecting the rule
is available then the error estimate cErrcv can be improved by bootstrap
smoothing.11 With the predictor vectors xi considered ﬁxed as observed, a
parametric model generates the data set d D f.xi ; yi /; i D 1; : : : ; Ng as
estimatecErrcv (12.21),
in (12.62), from which we calculate the prediction rule rd .(cid:1)/ and the error

Cross-validation does not require a probability model, but if such a model

(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)

f(cid:22) ! d ! rd .(cid:1)/ !cErrcv:

(12.78)
Substituting the estimated density f O(cid:22) for f(cid:22), as in (12.63), provides

11 Perhaps better known as “bagging;” see Chapter 17.

12.4 Training, Validation, and Ephemeral Predictors

227

(12.79)

parametric bootstrap replicates ofcErrcv,
(cid:3) .(cid:1)/ !cErr
(cid:3) ! rd
BX
cErr

f O(cid:22) ! d

Err D 1

(cid:3)b
cv :

(cid:3)
cv:

Some large number B of replications can then be averaged to give the
smoothed estimate

B

bD1

Err averages out the considerable noise incErrcv, often signiﬁcantly reduc-
left after excess randomness is squeezed out ofcErrcv (an example of “Rao–

A surprising result, referenced in the endnotes, shows that Err approxi-
mates the covariance penalty estimate Err(cid:1). Speaking broadly, Err(cid:1) is what’s

ing its variability.12

(12.80)

Blackwellization,” to use classical terminology). Improvements can be quite
substantial.  Covariance penalty estimates, when believable parametric 4
models are available, should be preferred to cross-validation.

12.4 Training, Validation, and Ephemeral Predictors

Good Practice suggests splitting the full set of observed predictor–response
pairs .x; y/ into a training set d of size N (12.1), and a validation set
dval, of size Nval (12.19). The validation set is put into a vault while the
training set is used to develop an effective prediction rule rd .x/. Finally,

dval is removed from the vault and used to calculatecErrval (12.20), an honest

estimate of the predictive error rate of rd.

This is a good idea, and seems foolproof, at least if one has enough data
to afford setting aside a substantial portion for a validation set during the
training process. Nevertheless, there remains some peril of underestimating
the true error rate, arising from ephemeral predictors, those whose predic-
tive powers fade away over time. A contrived, but not completely fanciful,
example illustrates the danger.

The example takes the form of an imaginary microarray study involving

360 subjects, 180 patients and 180 healthy controls, coded
i D 1; 2; : : : ; 360:

yi D

1 patient
0 control;

(12.81)

12 A related tactic pertaining to grouped cross-validation is to repeat calculation (12.21) for

several different randomly selected splits into J groups, and then average the resulting

cErrcv estimates.

(

228

Cross-Validation and Cp Estimates

Each subject is assessed on a microarray measuring the genetic activity of
p D 100 genes, these being the predictors

xi D .xi1; xi 2; xi 3; : : : ; xi100/

0

:

(12.82)

One subject per day is assessed, alternating patients and controls.

Figure 12.5 Orange bars indicate transient episodes, (12.84) and
the reverse, for imaginary medical study (12.81)–(12.82).

xij

ind(cid:24)

The measurements xij are independent of each other and of the yi’s,
for i D 1; 2; : : : ; 360 and j D 1; 2; : : : ; 100:
(12.83)
Most of the (cid:22)ij equal zero, but each gene’s measurements can experience
“transient episodes” of two possible types: in type 1,

N .(cid:22)ij ; 1/

(
if yi D 1
(cid:0)2 if yi D 0;

2

(cid:22)ij D

(12.84)

while type 2 reverses signs. The episodes are about 30 days long, randomly
and independently located between days 1 and 360, with an average of two
episodes per gene. The orange bars in Figure 12.5 indicate the episodes.
For the purpose of future diagnoses we wish to construct a prediction
rule Oy D rd .x/. To this end we randomly divide the 360 subjects into a

050100150200250300350020406080100 ***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************subjectsgenes12.4 Ephemeral Predictors

229
training set d of size N D 300 and a validation set dval of size Nval D
60. The popular “machine learning” prediction program Random Forests,
Chapter 17, is applied. Random Forests forms rd .x/ by averaging the pre-
dictions of a large number of randomly subsampled regression trees (Sec-
tion 8.4).

Figure 12.6 Test error (blue) and cross-validated training error
(black), for Random Forest prediction rules using the imaginary
medical study (12.81)–(12.82). Top panel: training set randomly
selected 300 days, test set the remaining 60 days. Bottom panel:
training set the ﬁrst 300 days, test set the last 60 days.

The top panel of Figure 12.6 shows the results, with blue points indi-
cating test-set error and black the (cross-validated) training-set error. Both
converge to 15% as the number of Random Forest trees grows large. This
seems to conﬁrm an 85% success rate for prediction rule rd .x/.

One change has been made for the bottom panel: now the training set
is the data for days 1 through 300, and the test set days 301 through 360.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.10.20.30.40.5Training set random 300 days, test set the remainder# treesPrediction errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll15%llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.10.20.30.40.5Training days 1−300, test days 301−360# treesPrediction errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll27%15%Cross-Validation and Cp Estimates

230

cErrval is now 27%, nearly double.

The cross-validated training-set prediction error still converges to 15%, but

The reason isn’t hard to see. Any predictive power must come from the
transient episodes, which lose efﬁcacy outside of their limited span. In the
ﬁrst example the test days are located among the training days, and inherit
their predictive accuracy from them. This mostly fails in the second setup,
where the test days are farther removed from the training days. (Only the

orange bars crossing the 300-day line can help lower cErrval in this situa-
An obvious, but often ignored, dictum is thatcErrval is more believable if

tion.)

the test set is further separated from the training set. “Further” has a clear
meaning in studies with a time or location factor, but not necessarily in
general. For J -fold cross-validation, separation is improved by removing
contiguous blocks of N=J cases for each group, rather than by random

selection, but the amount of separation is still limited, making cErrcv less
believable than a suitably constructedcErrval.

The distinction between transient, ephemeral predictors and dependable
ones is sometimes phrased as the difference between correlation and cau-
sation. For prediction purposes, if not for scientiﬁc exegesis, we may be
happy to settle for correlations as long as they are persistent enough for
our purposes. We return to this question in Chapter 15 in the discussion of
large-scale hypothesis testing.

5

A notorious cautionary tale of fading correlations concerns Google Flu
Trends,  a machine-learning algorithm for predicting inﬂuenza outbreaks.
Introduced in 2008, the algorithm, based on counts of internet search terms,
outperformed traditional medical surveys in terms of speed and predictive
accuracy. Four years later, however, the algorithm failed, badly overesti-
mating what turned out to be a nonexistent ﬂu epidemic. Perhaps one les-
son here is that the Google algorithmists needed a validation set years—not
weeks or months—removed from the training data.

Error rate estimation is mainly frequentist in nature, but the very large
data sets available from the internet have encouraged a disregard for infer-
ential justiﬁcation of any type. This can be dangerous. The heterogeneous
nature of “found” data makes statistical principles of analysis more, not
less, relevant.

12.5 Notes and Details

The evolution of prediction algorithms and their error estimates nicely il-
lustrates the inﬂuence of electronic computation on statistical theory and

12.5 Notes and Details

231

practice. The classical recipe for cross-validation recommended splitting
the full data set in two, doing variable selection, model choice, and data ﬁt-
ting on the ﬁrst half, and then testing the resulting procedure on the second
half. Interest revived in 1974 with the independent publication of papers
by Geisser and by Stone, featuring leave-one-out cross-validation of pre-
dictive error rates.

A question of bias versus variance arises here. A rule based on only N=2
cases is less accurate than the actual rule based on all N . Leave-one-out
cross-validation minimizes this type of bias, at the expense of increased
variability of error rate estimates for “jumpy” rules of a discontinuous
nature. Current best practice is described in Section 7.10 of Hastie et al.
(2009), where J -fold cross-validation with J perhaps 10 is recommended,
possibly averaged over several random data splits.

Nineteen seventy-three was another good year for error estimation, fea-
turing Mallows’ Cp estimator and Akaike’s information criterion. Efron
(1986) extended Cp methods to a general class of situations (see below),
established the connection with AIC, and suggested bootstrapping methods
for covariance penalties. The connection between cross-validation and co-
variance penalties was examined in Efron (2004), where the Rao–Blackwell-
type relationship mentioned at the end of Section 12.3 was demonstrated.
The SURE criterion appeared in Charles Stein’s 1981 paper. Ye (1998)
suggested the general degrees of freedom deﬁnition (12.53).

1 [p. 210] Standard candles and dark energy. Adam Riess, Saul Perlmutter,
and Brian Schmidt won the 2011 Nobel Prize in physics for discovering
increasing rates of expansion of the Universe, attributed to an Einsteinian
concept of dark energy. They measured cosmic distances using Type Ia
supernovas as “standard candles.” The type of analysis suggested by Fig-
ure 12.1 is intended to improve the cosmological distance scale.

2 [p. 222] Data-based choice of a lasso estimate. The regularization param-
eter (cid:21) for a lasso estimator (7.42) controls the number of nonzero coefﬁ-
cients of Q
ˇ.(cid:21)/, with larger (cid:21) yielding fewer nonzeros. Efron et al. (2004)
cients. Substituting this for p in (12.51) provides a quick version of cErr(cid:1).
and Zou et al. (2007) showed that a good approximation for the degrees of
freedom df (12.53) of a lasso estimate is the number of its nonzero coefﬁ-
This was minimized at df D 7 for the supernova example in Figure 12.1
(12.54).

3 [p. 222] Stein’s unbiased risk estimate. The covariance formula (12.56) is
obtained directly from integration by parts. The computation is clear from

232
the one-dimensional version of (12.55), N D 1:

Cross-Validation and Cp Estimates


cov. O(cid:22); y/ DZ 1

Z 1
(cid:0)1
(cid:26) @ O(cid:22).y/
D (cid:27) 2
D (cid:27) 2E

1p
2(cid:25)(cid:27) 2
1p
2(cid:25)(cid:27) 2

(cid:27)

(cid:0)1

e

:

@y

(cid:21) O(cid:22).y/ dy
(cid:21) @ O(cid:22).y/

.y (cid:0) (cid:22)/
.y(cid:0)(cid:22)/2

dy

@y

(cid:0) 1

2

.y(cid:0)(cid:22)/2

(cid:27) 2

(cid:0) 1

2

e

(cid:27) 2

(12.85)

(cid:3)b

D r

(cid:3)b.xi /:

Broad regularity conditions for SURE are given in Stein (1981).

4 [p. 227] The .632 rule. Bootstrap competitors to cross-validation are dis-
cussed in Efron (1983) and Efron and Tibshirani (1997). The most success-
ful of these, the “.632 rule” is generally less variable than leave-one-out
(cid:3)b,
cross-validation. We suppose that nonparametric bootstrap data sets d
b D 1; 2; : : : ; B, have been formed, each by sampling with replacement
(cid:3)b produces
N times from the original N members of d (12.1). Data set d
rule

(cid:3)b.x/ D rd

(cid:3)b .x/;

(12.86)

giving predictions

r

y

Let I b
i

D 1 if pair .xi ; yi / is not in d

(12.87)
(cid:0)1 D
i will equal 1, the remaining 0.632 equaling 0.) The

(cid:3)b, and 0 if it is. (About e

0:368 of the N (cid:1) B I b
“out of bootstrap” estimate of prediction error is

i

cErrout D NX

BX

iD1

jD1

I b
i D

, NX

BX

(cid:16)
yi ; Oy

(cid:3)b

i

I b
i ;

iD1

jD1

(12.88)

the average discrepancy in the omitted cases.

37% of the cases each time. The .632 rule compensates for the upward bias

cErrout is similar to a grouped cross-validation estimate that omits about
incErrout by incorporating the downwardly biased apparent error (12.18),
cErrout has resurfaced in the popular Random Forests prediction algorithm,

cErr:632 D 0:632cErrout C 0:368 err :

Chapter 17, where a closely related procedure gives the “out of bag” esti-
mate of Err.

(12.89)

5 [p. 230] Google Flu Trends. Harford’s 2014 article, “Big data: A big mis-
take?,” concerns the enormous “found” data sets available in the internet
age, and the dangers of forgetting the principles of statistical inference in
their analysis. Google Flu Trends is his primary cautionary example.

13

Objective Bayes Inference and Markov Chain

Monte Carlo

From its very beginnings, Bayesian inference exerted a powerful inﬂuence
on statistical thinking. The notion of a single coherent methodology em-
ploying only the rules of probability to go from assumption to conclusion
was and is immensely attractive. For 200 years, however, two impediments
stood between Bayesian theory’s philosophical attraction and its practical
application.
1 In the absence of relevant past experience, the choice of a prior distribu-
tion introduces an unwanted subjective element into scientiﬁc inference.
2 Bayes’ rule (3.5) looks simple enough, but carrying out the numerical
calculation of a posterior distribution often involves intricate higher-
dimensional integrals.

The two impediments ﬁt neatly into the dichotomy of Chapter 1, the ﬁrst
being inferential and the second algorithmic.1

A renewed cycle of Bayesian enthusiasm took hold in the 1960s, at ﬁrst
concerned mainly with coherent inference. Building on work by Bruno de
Finetti and L. J. Savage, a principled theory of subjective probability was
constructed: the Bayesian statistician, by the careful elicitation of prior
knowledge, utility, and belief, arrives at the correct subjective prior dis-
tribution for the problem at hand. Subjective Bayesianism is particularly
appropriate for individual decision making, say for the business executive
trying to choose the best investment in the face of uncertain information.

It is less appropriate for scientiﬁc inference, where the sometimes skep-
tical world of science puts a premium on objectivity. An answer came from
the school of objective Bayes inference. Following the approach of Laplace
and Jeffreys, as discussed in Section 3.2, their goal was to fashion objec-
tive, or “uninformative,” prior distributions that in some sense were unbi-
ased in their effects upon the data analysis.

1 The exponential family material in this chapter provides technical support, but is not

required in detail for a general understanding of the main ideas.

233

234

Objective Bayes Inference and MCMC

In what came as a surprise to the Bayes community, the objective school
has been the most successful in bringing Bayesian ideas to bear on scien-
tiﬁc data analysis. Of the 24 articles in the December 2014 issue of the
Annals of Applied Statistics, 8 employed Bayesian analysis, predominantly
based on objective priors.

This is where electronic computation enters the story. Commencing in
the 1980s, dramatic steps forward were made in the numerical calculation
of high-dimensional Bayes posterior distributions. Markov chain Monte
Carlo (MCMC) is the generic name for modern posterior computation al-
gorithms. These proved particularly well suited for certain forms of objec-
tive Bayes prior distributions.

Taken together, objective priors and MCMC computations provide an
attractive package for the statistician faced with a complicated data analy-
sis situation. Statistical inference becomes almost automatic, at least com-
pared with the rigors of frequentist analysis. This chapter discusses both
parts of the package, the choice of prior and the subsequent computational
methods. Criticisms arise, both from the frequentist viewpoint and that of
informative Bayesian analysis, which are brought up here and also in Chap-
ter 21.

13.1 Objective Prior Distributions

A ﬂat, or uniform, distribution over the space of possible parameter values
seems like the obvious choice for an uninformative prior distribution, and
has been so ever since Laplace’s advocacy in the late eighteenth century.
For a ﬁnite parameter space (cid:127), say

(cid:127) D f(cid:22).1/; (cid:22).2/; : : : ; (cid:22).K/g;

“ﬂat” has the obvious meaning

gﬂat.(cid:22)/ D 1

K

for all (cid:22) 2 (cid:127):

If K is inﬁnite, or if (cid:127) is continuous, we can still take

gﬂat.(cid:22)/ D constant:

(13.1)

(13.2)

(13.3)

Bayes’ rule (3.5) gives the same posterior distribution for any choice of the
constant,

gﬂat.(cid:22)jx/ D gﬂat.(cid:22)/f(cid:22).x/=f .x/; with

f(cid:22).x/gﬂat.(cid:22)/ d(cid:22):

(13.4)

f .x/ DZ

(cid:127)

13.1 Objective Prior Distributions

235
Notice that gﬂat.(cid:22)/ cancels out of gﬂat.(cid:22)jx/. The fact that gﬂat.(cid:22)/ is “im-
proper,” that is, it integrates to inﬁnity, doesn’t affect the formal use of
Bayes’ rule in (13.4) as long as f .x/ is ﬁnite.
Notice also that gﬂat.(cid:22)jx/ amounts to taking the posterior density of
(cid:22) to be proportional to the likelihood function Lx.(cid:22)/ D f(cid:22).x/ (with x
ﬁxed and (cid:22) varying over (cid:127)). This brings us close to Fisherian inference,
with its emphasis on the direct interpretation of likelihoods, but Fisher was
adamant in his insistance that likelihood was not probability.

Figure 13.1 The solid curve is ﬂat-prior posterior density (13.4)
having observed x D 10 from Poisson model x (cid:24) Poi.(cid:22)/; it is
shifted about 0.5 units right from the conﬁdence density (dashed)
of Figure 11.6. Jeffreys’ prior gives a posterior density (dotted)
nearly the same as the conﬁdence density.

The solid curve in Figure 13.1 shows gﬂat.(cid:22)jx/ for the Poisson situation

of Table 11.2,

x (cid:24) Poi.(cid:22)/;

(13.5)
with x D 10 observed; gﬂat.(cid:22)jx/ is shifted almost exactly 0.5 units right
of the conﬁdence density from Figure 11.6. (“” is (cid:22) itself in this case.)2
Fisher’s withering criticism of ﬂat-prior Bayes inference focused on its

2 The reader may wish to review Chapter 11, particularly Section 11.6, for these

constructions.

0510152025300.000.020.040.060.080.100.12qDensitiesconfidencedensityposterior density,flat prior10JeffreysObjective Bayes Inference and MCMC

236
lack of transformation invariance. If we were interested in  D log.(cid:22)/
rather than (cid:22), gﬂat.jx/ would not be the transformation to the log scale of
gﬂat.(cid:22)jx/. Jeffreys’ prior, (3.17) or (11.72), which does transform correctly,
is

(13.6)
for x (cid:24) Poi.(cid:22)/; gJeff.(cid:22)jx D 10/ is then a close match to the conﬁdence
density in Figure 13.1.

(cid:22)

gJeff.(cid:22)/ D 1ıp

Coverage Matching Priors

1

2

A variety of improvements and variations on Jeffreys’ prior have been
suggested for use as general-purpose uninformative prior distributions, as
brieﬂy discussed in the chapter endnotes.  All share the drawback seen in
Figure 11.7: the posterior distribution g.(cid:22)jx/ can have unintended effects
on the resulting inferences for a real-valued parameter of interest  D t .(cid:22)/.
This is unavoidable; it is mathematically impossible for any single prior to
be uninformative for every choice of  D t .(cid:22)/.

The label “uninformative” for a prior sometimes means “gives Bayes
posterior intervals that closely match conﬁdence intervals.” Perhaps sur-
prisingly, this deﬁnition has considerable resonance in the Bayes commu-
nity. Such priors can be constructed for any given scalar parameter of in-
terest  D t .(cid:22)/, for instance the maximum eigenvalue parameter of Fig-
ure 11.7. In brief, the construction proceeds as follows.
(cid:15) The p-dimensional parameter vector (cid:22) is transformed to a form that

makes  the ﬁrst coordinate, say

where (cid:23) is a .p (cid:0) 1/-dimensioned nuisance parameter.
(cid:15) The transformation is chosen so that the Fisher information matrix (11.72)
for .; (cid:23)/ has the “diagonal” form

(13.7)

(13.8)

(cid:22) ! .; (cid:23)/;


I 
0
0

0

I (cid:23)(cid:23)

:

(This is always possible.)

(cid:15) Finally, the prior for .; (cid:23)/ is taken proportional to

(13.9)
where h.(cid:23)/ is an arbitrary .p (cid:0) 1/-dimensional density. In other words,

I 1=2
  h.(cid:23)/;

g.; (cid:23)/ D

13.2 Conjugate Prior Distributions

237

g.; (cid:23)/ combines the one-dimensional Jeffreys’ prior (3.16) for  with
an arbitrary independent prior for the orthogonal nuisance parameter
vector (cid:23).

The main thing to notice about (13.9) is that g.; (cid:23)/ represents different
priors on the original parameter vector (cid:22) for different functions  D t .(cid:22)/.
No single prior g.(cid:22)/ can be uninformative for all choices of the parameter
of interest .

Calculating g.; (cid:23)/ can be difﬁcult. One alternative is to go directly to
the BCa conﬁdence density (11.68)–(11.69), which can be interpreted as
the posterior distribution from an uninformative prior (because its integrals
agree closely with conﬁdence interval endpoints).

Coverage matching priors are not much used in practice, and in fact none
of the eight Annals of Applied Statistics objective Bayes papers mentioned
earlier were of type (13.9). A form of “almost uninformative” priors, the
conjugates, is more popular, mainly because of the simpler computation of
their posterior distributions.

13.2 Conjugate Prior Distributions

A mathematically convenient class of prior distributions, the conjugate pri-
ors, applies to samples from an exponential family,3 Section 5.5,

f(cid:22).x/ D e˛x(cid:0) .˛/f0.x/:

(13.10)

(13.11)

Here we have indexed the family with the expectation parameter

(cid:22) D Ef fxg;

rather than the canonical parameter ˛. On the right-hand side of (13.10), ˛
can be thought of as a one-to-one function of (cid:22) (the so-called “link func-
tion”), e.g., ˛ D log.(cid:22)/ for the Poisson family. The observed data is a
random sample x D .x1; x2; : : : ; xn/ from f(cid:22),
iid(cid:24) f(cid:22);

x1; x2; : : : ; xn

(13.12)

having density function

the average Nx DP xi =n being sufﬁcient.

f(cid:22).x/ D enŒ˛ Nx(cid:0) .˛/f0.x/;

(13.13)

3 We will concentrate on one-parameter families, though the theory extends to the

multiparameter case. Figure 13.2 relates to a two-parameter situation.

238

Objective Bayes Inference and MCMC

The family of conjugate priors for (cid:22), gn0;x0 .(cid:22)/, allows the statistician

to choose two parameters, n0 and x0,

gn0;x0 .(cid:22)/ D cen0Œx0˛(cid:0) .˛/ıV .(cid:22)/;

V .(cid:22)/ the variance of an x from f(cid:22),

V .(cid:22)/ D varf fxgI

(13.14)

(13.15)

c is the constant that makes gn0;x0 .(cid:22)/ integrate to 1 with respect to Lebesgue
measure on the interval of possible (cid:22) values. The interpretation is that x0
represents the average of n0 hypothetical prior observations from f(cid:22).

The utility of conjugate priors is seen in the following theorem.

3

Theorem 13.1  Deﬁne

nC D n0 C n and

Nx:
Then the posterior density of (cid:22) given x D .x1; x2; : : : ; xn/ is

nC x0 C n
nC

NxC D n0

g.(cid:22)jx/ D gnC;NxC .(cid:22)/I

moreover, the posterior expectation of (cid:22) given x is
Nx:

Ef(cid:22)jxg D n0

nC x0 C n
nC

(13.16)

(13.17)

(13.18)

The intuitive interpretation is quite satisfying: we begin with a hypo-
thetical prior sample of size n0, sufﬁcient statistic x0; observe x, a sam-
ple of size n; and update our prior distribution gn0;x0 .(cid:22)/ to a distribution
gnC;NxC .(cid:22)/ of the same form. Moreover, Ef(cid:22)jxg equals the average of a
hypothetical sample with n0 copies of x0,

.x0; x0; : : : ; x0; x1; x2; : : : ; xn/:

(13.19)
iid(cid:24) Poi.(cid:22)/, that is we have n i.i.d. observa-
tions from a Poisson distribution, Table 5.1. Formula (13.14) gives conju-
gate prior 

As an example, suppose xi

4

gn0;x0 .(cid:22)/ D c(cid:22)n0x0(cid:0)1e

(cid:0)n0(cid:22);

(13.20)

c not depending on (cid:22). So in the notation of Table 5.1, gn0;x0 .(cid:22)/ is a gamma
distribution, Gam.n0x0; 1=n0/. The posterior distribution is
g.(cid:22)jx/ D gnC;NxC .(cid:22)/ (cid:24) Gam.nC NxC; 1=nC/

(cid:24) 1

nC GnC NxC ;

(13.21)

13.2 Conjugate Prior Distributions

239

5

where G(cid:23) indicates a standard gamma distribution,

G(cid:23) D Gam.(cid:23); 1/:

(13.22)

Table 13.1 Conjugate priors (13.14)–(13.16) for four familiar
one-parameter exponential families, using notation in Table 5.1; the last
column shows the posterior distribution of (cid:22) given n observations xi,
starting from prior gn0;x0 .(cid:22)/. In line 4, G(cid:23) is the standard gamma
distribution Gam.(cid:23); 1/, with (cid:22) the same as gamma parameter (cid:27) in
Table 5.1. The chapter endnotes give the density of the inverse gamma
distribution 1=G(cid:23), and corresponding results for chi-squared variates.

Name

1. Normal

xi distribution
N .(cid:22); (cid:27) 2
1 /
((cid:27) 2
1 known)

2. Poisson

Poi.(cid:22)/

3. Binomial

Bi.1; (cid:22)/

4. Gamma

(cid:22)G(cid:23) =(cid:23)
((cid:23) known)

gn0;x0 .(cid:22)/

1 =n0/

N .x0; (cid:27) 2

1 =nC/
Gam.nC NxC; 1=nC/
Gam.n0x0; 1=n0/
Be.n0x0; n0.1 (cid:0) x0// Be.nC NxC; nC.1 (cid:0) NxC//

g.(cid:22)jx/
N .NxC; (cid:27) 2

n0x0(cid:23)=Gn0(cid:23)C1

nC NxC(cid:23)=GnC(cid:23)C1

Table 13.1 describes the conjugate prior and posterior distributions for
four familiar one-parameter families. The binomial case, where (cid:22) is the
“success probability” (cid:25) in Table 5.1, is particularly evocative: indepen-
i xi D nNx successes. Prior
gn0;x0 .(cid:25)/ amounts to assuming proportion x0 D s0=n0 prior successes in
n0 ﬂips. Formula (13.18) becomes

dent coin ﬂips x1; x2; : : : ; xn give, say, s DP
Ef(cid:25)jxg D s0 C s
n0 C n

(13.23)
for the posterior expectation of (cid:25). The choice .n0; x0/ D .2; 1=2/ for in-
stance gives Bayesian estimate .sC 1/=.nC 2/ for (cid:25), pulling the MLE s=n
a little bit toward 1/2.

The size of n0, the number of hypothetical prior observations, deter-
mines how informative or uninformative the prior gn0;x0 .(cid:22)/ is. Recent
objective Bayes literature has favored choosing n0 small, n0 D 1 being
popular. The hope here is to employ a proper prior (one that has a ﬁnite
integral), while still not injecting much unwarranted information into the
analysis. The choice of x0 is also by convention. One possibility is to set

Objective Bayes Inference and MCMC

240
x0 D Nx, in which case the posterior expectation Ef(cid:22)jxg (13.18) equals
the MLE Nx. Another possibility is choosing x0 equal to a “null” value, for
instance x0 D 0 for effect size estimation in (3.28).

Table 13.2 Vasoconstriction data; volume of air inspired in 39 cases, 19
without vasoconstriction (y D 0) and 20 with vasoconstriction (y D 1).

y D 0
98
60
98
74
104
78
104
78
113
78
88
118
120
90
123
95
95
137
98

y D 1
115
85
120
88
126
88
126
90
128
90
93
136
143
104
151
108
154
110
111
157

6

As a miniature example of objective Bayes inference, we consider the
vasoconstriction dataof Table 13.2: n D 39 measurements of lung volume
have been obtained, 19 without vasoconstriction .y D 0/ and 20 with .y D
1/. Here we will think of the yi as binomial variates,

following logistic regression model (8.5),

ind(cid:24) Bi.1; (cid:25)i /;

yi

i D 1; 2; : : : ; 39;

 (cid:25)i

1 (cid:0) (cid:25)i

 D ˛0 C ˛1xi ;

log

(13.24)

(13.25)

with the xi as ﬁxed covariates (the values in Table 13.2).
Letting Xi D .1; xi /
nential family (8.24),

0, (13.24)–(13.25) results in a two-parameter expo-

f0.y/;

(13.26)

The MLE O˛ has approximate 2(cid:2) 2 covariance matrix OV as given in (8.30).

i
0 Oˇ(cid:0) .˛/

˛

h
f˛.y/ D en
!0
nX

yi ;

xi yi

iD1

having
O
ˇ D 1

n

  nX

iD1

and  .˛/ D 1

log.1 C e˛

0

Xi /:

nX

iD1

n

13.2 Conjugate Prior Distributions

241

In Figure 13.2, the posterior distributions are graphed in terms of

(13.27)
rather than ˛ or (cid:22), making the contours of equal density roughly circular
and centered at zero.

(cid:13) D OV

(cid:0)1=2.˛ (cid:0) O˛/

Figure 13.2 Vasoconstriction data; contours of equal posterior
density of (cid:13) (13.27) from four uninformative priors, as described
in the text. Numbers indicate probability content within contours;
light dashed contours from Panel A, ﬂat prior.

Panel A of Figure 13.2 illustrates the ﬂat prior posterior density of (cid:13)
given the data y in model (13.24)–(13.25). The heavy lines are contours
of equal density, with the one labeled “0.9” containing 90% of the pos-
terior probability, etc. Panel B shows the corresponding posterior density

A. Flat Priorg1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.999  0.999 −4−2024−4−2024lB. Jeffreys' Priorg1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.999 −4−2024−4−2024 0.1  0.5  0.9  0.99  0.999  0.999 lC. Conjugate Priorg1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.999 −4−2024−4−2024 0.1  0.5  0.9  0.99  0.999  0.999 lD. Bootstrap Distributiong1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.997  0.999 −4−2024−4−2024l 0.1  0.5  0.9  0.99  0.999  0.999 242

Objective Bayes Inference and MCMC

contours obtained from Jeffreys’ multiparameter prior (11.72), in this case

gJeff.˛/ D jV˛j1=2;

(13.28)
V˛ the covariance matrix of O˛, as calculated from (8.30). For comparison
purposes the light dashed curves show some of the ﬂat prior contours from
panel A. The effect of gJeff.˛/ is to reduce the ﬂat prior bulge toward the
upper left corner.

Panel C relates to the conjugate prior4 g1;0.˛/. Besides reducing the ﬂat

prior bulge, g1;0.˛/ pulls the contours slightly downward.
(13.25), with O˛ replacing ˛, gave resamples y
The contours of O(cid:13)
toward the left.

Panel D shows the parametric bootstrap distribution: model (13.24)–
(cid:3) and MLE replications O˛
(cid:3).
(cid:3) (cid:0) O˛/ considerably accentuate the bulge

(cid:3) D OV

(cid:0)1=2.O˛

Figure 13.3 Posterior densities for (cid:13)1, ﬁrst coordinate of (cid:13) in
(13.27), for the vasoconstriction data. Dashed red curve: raw
(unweighted) distribution of B D 8000 parametric replications
from model (13.24)–(13.25); solid black curve: BCa density
(11.68) (z0 D 0:123, a D 0:053); dotted blue curve: posterior
density using Jeffreys multiparameter prior (11.72).

4 The role of Nx in (13.13) is taken by Oˇ in (13.26), so g1;0 has Oˇ D 0, n0 D 1. This
makes g1;0.˛/ D expf(cid:0) .˛/g. The factor V .(cid:22)/ in (13.14) is absent in the conjugate
prior for ˛ (as opposed to (cid:22)).

−4−20240.000.010.020.030.04g1Density*********************************************************************************BCaraw bootstrapJeffreys13.3 Model Selection and the Bayesian Information Criterion 243

This doesn’t necessarily imply that a bootstrap analysis would give much
different answers than the three (quite similar) objective Bayes results. For
any particular real-valued parameter of interest , the raw bootstrap distri-
bution (equal weight on each replication) would be reweighted according to
the BCa formula (11.68) in order to produce accurate conﬁdence intervals.
Figure 13.3 compares the raw bootstrap distribution, the BCa conﬁdence
density, and the posterior density obtained from Jeffreys’ prior, for  equal
to (cid:13)1, the ﬁrst coordinate of (cid:13) in (13.27). The BCa density is shifted to the
right of Jeffreys’.

Critique of Objective Bayes Inference

Despite its simplicity, or perhaps because of it, objective Bayes procedures
are vulnerable to criticism from both ends of the statistical spectrum. From
the subjectivist point of view, objective Bayes is only partially Bayesian: it
employs Bayes’ theorem but without doing the hard work of determining a
convincing prior distribution. This introduces frequentist elements into its
practice—clearly so in the case of Jeffreys’ prior—along with frequentist
incoherencies.

For the frequentist, objective Bayes analysis can seem dangerously un-
tethered from the usual standards of accuracy, having only tenuous large-
sample claims to legitimacy. This is more than a theoretical objection. The
practical advantages claimed for Bayesian methods depend crucially on the
ﬁne structure of the prior. Can we safely ignore stopping rules or selective
inference (e.g., choosing the largest of many estimated parameters for spe-
cial attention) for a prior not based on some form of genuine experience?
In an era of large, complicated, and difﬁcult data-analytic problems, ob-
jective Bayes methods are answering a felt need for relatively straightfor-
ward paths to solution. Granting their usefulness, it is still reasonable to
hope for better justiﬁcation,5 or at least for more careful comparisons with
competing methods as in Figure 13.3.

13.3 Model Selection and the Bayesian Information Criterion

Data-based model selection has become a major theme of modern statisti-
cal inference. In the problem’s simplest form, the statistician observes data
x and wishes to choose between a smaller model M0 and a larger model

5 Chapter 20 discusses the frequentist assessment of Bayes and objective Bayes estimates.

244

Objective Bayes Inference and MCMC

M1. The classic textbook example takes x D .x1; x2; : : : ; xn/

0 as an inde-

pendent normal sample,
iid(cid:24)

xi

N .(cid:22); 1/

for i D 1; 2; : : : ; n;

(13.29)

(13.30)

with M0 the null hypothesis (cid:22) D 0 and M1 the general two-sided alter-

native,

M0 W (cid:22) D 0; M1 W (cid:22) ¤ 0:

(We can include (cid:22) D 0 in M1 with no effect on what follows.) From a
frequentist viewpoint, choosing between M0 and M1 in (13.29)–(13.30)
amounts to running a hypothesis test of H0 W (cid:22) D 0, perhaps augmented
with a conﬁdence interval for (cid:22).

Bayesian model selection aims for more: an evaluation of the posterior
probabilities of M0 and M1 given x. A full Bayesian speciﬁcation re-
quires prior probabilities for the two models,

and conditional prior densities for (cid:22) within each model,
M1/:

and g1.(cid:22)/ D g.(cid:22)j

M0/

density for x, say

f0.x/ DZ

f(cid:22).x/g0.(cid:22)/ d(cid:22) and f1.x/ DZ

Let f(cid:22).x/ be the density of x given (cid:22). Each model induces a marginal

f(cid:22).x/g1.(cid:22)/ d(cid:22):

M0

M1

(13.33)

Bayes’ theorem, in its ratio form (3.8), then gives posterior probabilities

(cid:25)0.x/ D Prf

M0jxg

and (cid:25)1.x/ D Prf

M1jxg

and (cid:25)1 D 1 (cid:0) (cid:25)0 D Prf

M1g;

(cid:25)0 D Prf

M0g
g0.(cid:22)/ D g.(cid:22)j

(13.31)

(13.32)

(13.34)

(13.35)

(13.36)

satisfying

(cid:25)1.x/
(cid:25)0.x/

D (cid:25)1
(cid:25)0

B.x/;

where B.x/ is the Bayes factor

B.x/ D f1.x/

f0.x/

;

leading to the elegant statement that the posterior odds ratio is the prior
odds ratio times the Bayes factor.

All of this is of more theoretical than applied use. Prior speciﬁcations
(13.31)–(13.32) are usually unavailable in practical settings (which is why

13.3 Model Selection and the BIC

245

standard hypothesis testing is so popular). The objective Bayes school has
concentrated on estimating the Bayes factor B.x/, with the understanding
that the prior odds ratio (cid:25)1=(cid:25)0 in (13.35) would be roughly evaluated de-
pending on the speciﬁc circumstances—perhaps set to the Laplace choice
(cid:25)1=(cid:25)0 D 1.

Table 13.3 Jeffreys’ scale of evidence for the interpretation of Bayes
factors.

Bayes factor

< 1
1–3
3–20
20–150
> 150

Evidence for M1
negative
barely worthwhile
positive
strong
very strong

Jeffreys suggested a scale of evidence for interpreting Bayes factors, re-

produced in Table 13.3;  B.x/ D 10 for instance constitutes positive but 7
not strong evidence in favor of the bigger model. Jeffreys’ scale is a Bayes-
ian version of Fisher’s interpretive scale for the outcome of a hypothetic
test, with coverage value (one minus the signiﬁcance level) 0.95 famously
constituting “signiﬁcant” evidence against the null hypothesis. Table 13.4
shows Fisher’s scale, as commonly interpreted in the biomedical and social
sciences.

Table 13.4 Fisher’s scale of evidence against null hypothesis M0 and in
favor of M1, as a function of coverage level (1 minus the p-value).

Coverage

(p-value)

.80
.90
.95
.975
.99
.995
.999

(.20)
(.10)
(.05)
(.025)
(.01)
(.005)
(.001)

Evidence for M1
null
borderline
moderate
substantial
strong
very strong
overwhelming

Even if we accept the reduction of model selection to assessing the
Bayes factor B.x/ in (13.35), and even if we accept Jeffreys’ scale of in-
terpretation, this still leaves a crucial question: how to compute B.x/ in

8

246

Objective Bayes Inference and MCMC

practice, without requiring informative choices of the priors g0 and g1 in
(13.32).

A popular objective Bayes answer is provided by the Bayesian informa-

tion criterion (BIC). For a given model M we deﬁne
log.n/;

BIC.M/ D log˚f O(cid:22).x/(cid:9) (cid:0) p

(13.37)
where O(cid:22) is the MLE, p the degrees of freedom (number of free parameters)
in M, and n the sample size. Then the BIC approximation to Bayes factor
B.x/ (13.36) is

2

log BBIC.x/ D BIC.M1/ (cid:0) BIC.M0/

D log˚f O(cid:22)1 .x/=f O(cid:22)0 .x/(cid:9) (cid:0) p1 (cid:0) p0
W .x/ D 2 log˚f O(cid:22)1 .x/=f O(cid:22)0 .x/(cid:9) ;

2

the subscripts indexing the MLEs and degrees of freedom in M1 and M0.
This can be restated in somewhat more familiar terms. Letting W .x/ be
Wilks’ likelihood ratio statistic,

log.n/;

(13.38)

(13.39)

we have

2

log BBIC.x/ D 1

fW .x/ (cid:0) d log.n/g ;
with d D p1 (cid:0) p0: W .x/ approximately follows a (cid:31)2
d distribution under
model M0, E0fW .x/g :D d, implying BBIC.x/ will tend to be less than
one, favoring M0 if it is true, ever more strongly as n increases.
We can apply BIC selection to the vasoconstriction data of Table 13.2,
taking M1 to be model (13.24)–(13.25), and M0 to be the submodel
having ˛1 D 0. In this case d D 1 in (13.40). Direct calculation gives
W D 7:07 and

(13.40)

(13.41)
positive but not strong evidence against M0 according to Jeffreys’ scale.
By comparison, the usual frequentist z-value for testing ˛1 D 0 is 2.36,
coverage level 0.982, between substantial and strong evidence against M0
on Fisher’s scale.
The BIC was named in reference to Akaike’s information criterion (AIC),

BBIC D 5:49;

AIC.M/ D log˚f O(cid:22).x/(cid:9) (cid:0) p;

(13.42)

which suggests, as in (12.73), basing model selection on the sign of

AIC.M1/ (cid:0) AIC.M0/ D 1

2

fW .x/ (cid:0) 2dg :

(13.43)

13.3 Model Selection and the BIC

247

The BIC penalty d log.n/ in (13.40) grows more severe than the AIC
penalty 2d as n gets larger, increasingly favoring selection of M0 rather
than M1. The distinction is rooted in Bayesian notions of coherent behav-
ior, as discussed in what follows.
Where does the BIC penalty term d log.n/ in (13.40) come from? A ﬁrst
answer uses the simple normal model xi (cid:24)
N .(cid:22); 1/, (13.29)–(13.30). M0
has prior g0.(cid:22)/ D g.(cid:22)j
M0/ equal a delta function at zero. Suppose we
take g1.(cid:22)/ D g.(cid:22)j
g1.(cid:22)/ (cid:24)

M1/ in (13.32) to be the Gaussian conjugate prior

(13.44)
The discussion following (13.23) in Section 13.2 suggests setting M D 0
and A D 1, corresponding to prior information equivalent to one of the n
actual observations. In this case we can calculate the actual Bayes factor
B.x/,

N .M; A/:

(cid:26) n

(cid:27)
W .x/ (cid:0) log.n C 1/

;

log B.x/ D 1

n C 1

(13.45)
nearly equaling log BBIC.x/ (d D 1), for large n. Justiﬁcations of the BIC
formula as an approximate Bayes factor follow generalizations of this kind
of argument, as discussed in the chapter endnotes.

2

z P(cid:24)

N .0; 1/ under M0:

The difference between BIC and frequentist hypothesis testing grows
more drastic for large n. Suppose M0 is a regression model and M1 is M0
augmented with one additional covariate (so d D 1). Let z be a standard
z-value for testing the hypothesis that M1 is no improvement over M0,
(13.46)
Table 13.5 shows BBIC.x/ as a function of z and n. At n D 15 Fisher’s
and Jeffreys’ scales give roughly similar assessments of the evidence against
M0 (though Jeffreys’ nomenclature is more conservative). At the other end
of the table, at n D 10; 000, the inferences are contradictory: z D 3:29,
with p-value 0.001 and coverage level 0.999, is overwhelming evidence
for M1 on Fisher’s scale, but barely worthwhile for Jeffreys’. Bayesian
coherency, the axiom that inferences should be consistent over related sit-
uations, lies behind the contradiction.
Suppose n D 1 in the simple normal model (13.29)–(13.30). That is, we

observe only the single variable

and wish to decide between M0 W (cid:22) D 0 and M1 W (cid:22) ¤ 0. Let g.1/
denote our M1 prior density (13.32) for this situation.

1 .(cid:22)/

(13.47)

x (cid:24)

N .(cid:22); 1/;

248

Objective Bayes Inference and MCMC

Table 13.5 BIC Bayes factors corresponding to z-values for testing one
additional covariate; coverage value (1 minus the signiﬁcance level) of a
two-sided hypothesis test as interpreted by Fisher’s scale of evidence,
right. Jeffreys’ scale of evidence, Table 13.3, is in rough agreement with
Fisher for n D 15, but favors the null much more strongly for larger
sample sizes.

n

Cover z-value

15

50

250 1000 2500 5000 10000 Fisher

.80
.90
.95
.975
.99
.995
.999

1.28
1.64
1.96
2.24
2.58
2.81
3.29

.32
.55
.97
1.74
3.90
7.27

.14
.59
.24
1.00
.43
1.76
.78
3.18
1.74
7.12
13.27
3.25
57.96 31.75 14.20

.07
.12
.22
.39
.87
1.63
7.10

.05
.08
.14
.25
.55
1.03
4.49

.03
.05
.10
.17
.39
.73
3.17

.02 null
.04 borderline
.07 moderate
.12 substantial
.28 strong
.51 very strong
2.24 overwhelming

n.P xi =n/ and (cid:22).n/ D p

p

The case n > 1 in (13.29) is logically identical to (13.47). Letting x.n/ D

(cid:16)

n(cid:22) gives
x.n/ (cid:24)

N



(cid:22).n/; 1

;

(13.48)

(cid:0)(cid:22)ıp

n(cid:1)ıp

with (13.30) becoming M0 W (cid:22).n/ D 0 and M1 W (cid:22).n/ ¤ 0. Coherency
p
requires that (cid:22).n/ in (13.48) have the same M1 prior as (cid:22) in (13.47). Since
(cid:22) D (cid:22).n/=
1 .(cid:22)/, the M1 prior for sample size n,
satisﬁes
(13.49)

n, this implies that g.n/
1 .(cid:22)/ D g.1/
g.n/
1
this being “sample size coherency.”
1 .(cid:22)/ farther
away from the null value (cid:22) D 0 at rate
0 .(cid:22)/
stays ﬁxed. For any ﬁxed value of the sufﬁcient statistic x.n/ (x.n/ being
p
“z” in Table 13.5), this results in the Bayes factor B.x.n// decreasing at
rate 1=
n; the frequentist/Bayesian contradiction seen in Table 13.5 goes
beyond the speciﬁcs of the BIC algorithm.

The effect of (13.49) is to spread the M1 prior density g.n/

p
n, while the M0 prior g.n/

n;

(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)
A general information criterion takes the form

GIC.M/ D log f O(cid:22).x/ (cid:0) p cn;

(13.50)

13.3 Model Selection and the BIC

249
where cn is any sequence of positive numbers; cn D log.n/=2 for BIC
(13.37) and cn D 1 for AIC (13.42). The difference

  GIC.M1/ (cid:0) GIC.M0/ D 1

(13.51)
d D p1(cid:0)p0, will be positive if W .x/ > 2cnd. For d D 1, as in Table 13.5,
 will favor M1 if W .x/ (cid:21) 2cn, with approximate probability, if M0 is

.W .x/ (cid:0) 2cnd / ;

2

actually true,

(13.52)
This equals 0.157 for the AIC choice cn D 1; for BIC, n D 10; 000, it
equals 0.0024. The choice

1

Prf(cid:31)2

(cid:21) 2cng:

cn D 1:92

(13.53)

M0g :D 0:05, agreeing with the usual frequentist 0.05
makes Prf > 0j
rejection level.
The BIC is consistent: Prf > 0g goes to zero as n ! 1 if M0 is true.
This isn’t true of (13.53) for instance, where we will have Prf > 0g :D
0:05 no matter how large n may be, but consistency is seldom compelling
as a practical argument.
Conﬁdence intervals help compensate for possible frequentist overﬁt-
ting. With z D 3:29 and n D 10; 000, the 95% conﬁdence interval for (cid:22)
in model M1 (13.30) is .0:013; 0:053/. Whether or not such a small effect
is interesting depends on the scientiﬁc context. The fact that BIC says “not
interesting” speaks to its inherent small-model bias.
The prostate cancer study data of Section 3.3 provides a more challeng-
ing model selection problem. Figure 3.4 shows the histogram of N D 6033
observations xi, each measuring the effects of one gene. The histogram has
49 bins, each of width 0.2, with centers cj ranging from (cid:0)4:4 to 5.2; yj ,
the height of the histogram at cj , is the number of xi in bin j ,

yj D #fxi 2 bin jg

for j D 1; 2; : : : ; 49:

(13.54)

We assume that the yj follow a Poisson regression model as in Sec-

tion 8.3,

ind(cid:24) Poi.(cid:23)j /;

yj

j D 1; 2; : : : ; 49;

(13.55)

and wish to ﬁt a log polynomial GLM model to the (cid:23)j . The model selection
question is “What degree polynomial?” Degree 2 corresponds to normal
densities, but the long tails seen in Figure 3.4 suggest otherwise.
Models of degree 2 through 8 are assessed in Figure 13.4. Four model
selection measures are compared: AIC (13.42); BIC (13.37) with n D 49,

250

Objective Bayes Inference and MCMC

Figure 13.4 Log polynomial models of degree 2 through 8
applied to the prostate study histogram of Figure 3.4. Model
selection criteria: AIC (13.42); BIC (13.37) with n D 49, number
of bins, or 6033, number of genes; GIC (13.50) using classic
Fisher hypothesis choice cn D 1:92. All four selected the
fourth-degree model as best.

the number of yj values (bins), and also n D 6033, the number of genes;
and GIC (13.50), with cn D 1:92 (13.53), the choice based on classic Fish-
erian hypothesis testing. (This is almost the same as BIC n D 49, since
log.49/=2 D 1:95.) A fourth-degree polynomial model was the winner
under all four criteria.

The “untethered” criticism made against objective Bayes methods in
general is particularly applicable to BIC. The concept of “sample size”
is not well deﬁned, as the prostate study example shows. Sample size co-
herency (13.49), the rationale for BIC’s strong bias toward smaller models,
is less convincing in the absence of priors based on genuine experience (es-
pecially if there is no prospect of the sample size changing). Whatever its
vulnerabilities, BIC model selection has nevertheless become a mainstay
of objective Bayes model selection, not least because of its freedom from
the choice of Bayesian priors.

2345678−120−100−80−60−40Model polynomial degreeAIC and BICBIC 6033BIC 49 and GICAIC13.4 Gibbs Sampling and MCMC

251

13.4 Gibbs Sampling and MCMC

Miraculously blessed with visions of the future, a Bayesian statistician of
the 1970s would certainly be pleased with the prevalence of Bayes method-
ology in twenty-ﬁrst-century applications. But his pleasure might be tinged
with surprise that the applications were mostly of the objective, “uninfor-
mative” type, rather than taken from the elegant de Finetti–Savage school
of subjective inference.

The increase in Bayesian applications, and the change in emphasis from
subjective to objective, had more to do with computation than philoso-
phy. Better computers and algorithms facilitated the calculation of formerly
intractable Bayes posterior distributions. Technology determines practice,
and the powerful new algorithms encouraged Bayesian analyses of large
and complicated models where subjective priors (or those based on actual
past experience) were hard to come by. Add in the fact that the algorithms
worked most easily with simple “convenience” priors like the conjugates
of Section 13.2, and the stage was set for an objective Bayes renaissance.
At ﬁrst glance it’s hard to see why Bayesian computations should be
daunting. From parameter vector , data x, density function f .x/, and
prior density g./, Bayes’ rule (3.5)–(3.6) directly produces the posterior
density

g.jx/ D g./f .x/=f .x/;

where f .x/ is the marginal density

f .x/ DZ

The posterior probability of any set A in the parameter space (cid:127) is then

PfAjxg DZ

g./f .x/ d :

(cid:127)

(cid:30)Z

(13.56)

(13.57)

(13.58)

(13.59)

(13.60)

g./f .x/ d 

g./f .x/ d :

A

(cid:127)

This is easy to write down but usually difﬁcult to evaluate if  is multidi-
mensional.
Modern Bayes methods attack the problem through the application of
computer power. Even if we can’t integrate g.jx/, perhaps we can sample
from it. If so, a sufﬁciently large sample, say

would provide estimates

 .1/;  .2/; : : : ;  .B/ (cid:24) g.jx/
n
o.
 .j / 2 A

OPfAjxg D #

B;

252

Objective Bayes Inference and MCMC

and similarly for posterior moments, correlations, etc. We would in this
way be employing the same general tactic as the bootstrap, applied now
for Bayesian rather than frequentist purposes—toward the same goal as the
bootstrap, of freeing practical applications from the constraints of mathe-
matical tractability.

The two most popular computational methods,6 Gibbs sampling and
Markov chain Monte Carlo (MCMC), are based on Markov chain algo-
rithms; that is, the posterior samples  .b/ are produced in sequence, each
one depending only on  .b(cid:0)1/ and not on its more distant predecessors. We
begin with Gibbs sampling.
The central idea of Gibbs sampling is to reduce the generation of mul-
tidimensional vectors  D .1; 2; : : : ; K/ to a series of univariate calcu-
lations. Let .k/ denote  with component k removed, and g.k/ the condi-
tional density of k given .k/ and the data x,

kj.k/; x (cid:24) g.k/

kj.k/; x

(13.61)

(cid:16)



:

ˇˇ .b(cid:0)1/

.k/



The algorithm begins at some arbitrary initial value  .0/. Having computed
 .1/,  .2/, : : : ,  .b(cid:0)1/, the components of  .b/ are generated according to
conditional distributions (13.61),

(cid:24) g.k/

for k D 1; 2; : : : ; K:

k

; x

 .b/
k

(13.62)
As an example, we take x to be the n D 20 observations for y D 1 in
the vasoconstriction data of Table 13.2, and assume that these are a normal
sample,

xi

(13.63)
The sufﬁcient statistics for estimating the bivariate parameter  D .(cid:22); (cid:28) /
are the sample mean and variance

N .(cid:22); (cid:28) /;

i D 1; 2; : : : ; n D 20:

(cid:16)

iid(cid:24)

(13.64)

(13.65)

Nx D nX

xi =n and T D nX

1

1

.xi (cid:0) Nx/2=.n (cid:0) 1/;

having independent normal and gamma distributions,

Nx (cid:24)

N .(cid:22); (cid:28)=n/

and T (cid:24) (cid:28) G(cid:23)=(cid:23);

with (cid:23) D n(cid:0)1

2 , the latter being Gam.(cid:23); (cid:28)=(cid:23)/ in the notation of Table 5.1.
6 The two methods are often referred to collectively as MCMC because of mathematical

connections, with “Metropolis-Hasting algorithm” referring to the second type of
procedure.

13.4 Gibbs Sampling and MCMC

253

(cid:28) (cid:24) k1(cid:28)1=Gk1C1

For our Bayes prior distribution we take the conjugates
N .(cid:22)0; (cid:28)=n0/:

(13.66)
1 / D
In terms of Table 13.1, .x0; n0(cid:23)/ D .(cid:28)1; k1/ for the gamma, while .x0; (cid:27) 2
.(cid:22)0; (cid:28) / for the normal. (A simple speciﬁcation would take (cid:22) (cid:24)
N .(cid:22)0; (cid:28)1=n0/.)
Multiplying the normal and gamma functional forms in Table 5.1 yields

and (cid:22)j(cid:28) (cid:24)

density function

f(cid:22);(cid:28) .Nx; T / D c(cid:28)

(cid:0).(cid:23)C 1

2 / exp

and prior density
g.(cid:22); (cid:28) / D c(cid:28)

(cid:0).k1C2:5/ exp

h
(cid:23)T C n

(cid:26)(cid:0) 1
(cid:26)(cid:0) 1
h
k1(cid:28)1 C n0

.Nx (cid:0) (cid:22)/2i(cid:27)
.(cid:22) (cid:0) (cid:22)0/2i(cid:27)

2

(cid:28)

(cid:28)

2

(13.67)

;

(13.68)

c indicating positive constants that do not affect the posterior computations.
The posterior density cg.(cid:22); (cid:28) /f(cid:22);(cid:28) .Nx; T / is then calculated to be

g.(cid:22); (cid:28)jNx; T / D c(cid:28)
where Q D .k1(cid:28)1 C T / C nC

(cid:0).(cid:23)Ck1C3/ expf(cid:0)Q=(cid:28)g;
.(cid:22) (cid:0) N(cid:22)C/2 C n0n

2

2nC .(cid:22)0 (cid:0) Nx/2:

(13.69)

 N(cid:22)C;



Here nC D n0 C n and N(cid:22)C D .n0(cid:22)0 C nNx/=nC.
In order to make use of Gibbs sampling we need to know the full con-
ditional distributions g.(cid:22)j(cid:28); Nx; T / and g.(cid:28)j(cid:22); Nx; T /, as in (13.62). (In this
case, k D 2, 1 D (cid:22), and 2 D (cid:28).) This is where the conjugate expressions
in Table 13.1 come into play. Inspection of density (13.69) shows that

N

(cid:28)
nC

and (cid:28)j(cid:22); Nx; T (cid:24) Q

(cid:22)j(cid:28); Nx; T (cid:24)
(13.70)
B D 10; 000 Gibbs samples  .b/ D .(cid:22).b/; (cid:28) .b// were generated starting
from  .0/ D .Nx; T / D .116; 554/. The prior speciﬁcations were chosen to
be (presumably) uninformative or mildly informative,

G(cid:23)Ck1C2

:

n0 D 1; (cid:22)0 D Nx;

k1 D 1 or 9:5;

and (cid:28)1 D T:

(13.71)
(In which case N(cid:22)C D Nx and Q D .(cid:23) C k1/T C nC.(cid:22) (cid:0) Nx/2. From
(cid:23) D .n (cid:0) 1/=2, we see that k1 corresponds to about 2k1 hypothetical prior
observations.) The resulting posterior distributions for (cid:28) are shown by the
histograms in Figure 13.5.
As a point of frequentist comparison, B D 10; 000 parametric bootstrap

replications (which involve no prior assumptions),
O(cid:28) D T;

(cid:3) (cid:24) O(cid:28) G(cid:23)=(cid:23);

O(cid:28)

(13.72)

254

Objective Bayes Inference and MCMC

Figure 13.5 Posterior distributions for variance parameter (cid:28),
model (13.63)–(13.65), volume of air inspired for
vasoconstriction group y D 1 from Table 13.2. Solid teal
histogram: B D 10; 000 Gibbs samples with k1 D 1; black line
histogram: B D 10; 000 samples with k1 D 9:5; red line
histogram: 10,000 parametric bootstrap samples (13.72) suggests
even the k1 D 1 prior has substantial posterior effect.

are seen to be noticeably more dispersed than even the k1 D 1 Bayes
posterior distribution, the likely choice for an objective Bayes analysis.
Bayes techniques, even objective ones, have regularization effects that may
or may not be appropriate.
A similar, independent Gibbs sample of size 10,000 was obtained for the
19 y D 0 vasoconstriction measurements in Table 13.2, with speciﬁcations
as in (13.71), k D 1. Let

(cid:16)

1=2
(cid:0) (cid:22).b/
C (cid:28) .b/

0

ı.b/ D (cid:22).b/
(cid:28) .b/
1

1

0

;

(13.73)

where .(cid:22).b/
y D 1 and y D 0 runs.

1 ; (cid:28) .b/

1 / and .(cid:22).b/

0 ; (cid:28) .b/

0 / denote the bth Gibbs samples from the

Figure 13.6 shows the posterior distribution of ı. Twenty-eight of the

 tFrequency200400600800100012000200400600800100012001400Tk1=1k1=9.5parametricbootstrap13.4 Gibbs Sampling and MCMC

255

Figure 13.6 B D10,000 Gibbs samples for “Bayes t-statistic”
(13.73) comparing y D 1 with y D 0 values for vasoconstriction
data.

B D10,000 values ı.b/ were less than 0, giving a “Bayesian t-test” estimate
(13.74)

Pfı < 0jNx1; Nx0; T1; T0g D 0:0028:

(The usual t-test yielded one-sided p-value 0.0047 against the null hy-
pothesis (cid:22)0 D (cid:22)1.) An appealing feature of Gibbs sampling is that having
obtained  .1/;  .2/; : : : ;  .B/ (13.59) the posterior distribution of any pa-
rameter (cid:13) D t ./ is obtained directly from the B values (cid:13) .b/ D t . .b//.
Gibbs sampling requires the ability to sample from the full conditional
distributions (13.61). A more general Markov chain Monte Carlo method,
commonly referred to as MCMC, makes clearer the basic idea. Suppose the
space of possible  values is ﬁnite, say f.1/; .2/; : : : , .M /g, and we
wish to simulate samples from a posterior distribution putting probability
p.i / on .i /,

p D .p.1/; p.2/; : : : ; p.M // :

(13.75)

The MCMC algorithm begins with the choice of a “candidate” proba-
bility distribution q.i; j / for moving from .i / to .j /; in theory q.i; j /
can be almost anything, for instance q.i; j / D 1=.M (cid:0) 1/ for j ¤ i. The
simulated samples  .b/ are obtained by a random walk: if  .b/ equals .i /,

 dFrequency0.00.51.01.50100200300400500600Pr(d<0)=0.0028Objective Bayes Inference and MCMC

256
then  .bC1/ equals .j / with probability7
Q.i; j / D q.i; j / (cid:1) min

(cid:26) p.j /q.j; i /

p.i /q.i; j /

(cid:27)

; 1

for j ¤ i, while with probability

Q.i; i / D 1 (cid:0)X

j¤i

Q.i; j /

(13.77)

 .bC1/ D  .b/ D .i /. Markov chain theory then says that, under quite
general conditions, the empirical distribution of the random walk values
 .b/ will approach the desired distribution p as b gets large.
A heuristic argument for why this happens begins by supposing that  .1/
was in fact generated by sampling from the target distribution p, Prf .1/ D
ig D p.i /, and then  .2/ was obtained according to transition probabilities
(13.76)–(13.77). A little algebra shows that (13.76) implies

(13.76)

(13.78)

(13.79)

the so-called balance equations. This results in

n

 .2/ D i

Pr

p.i /Q.i; j / D p.j /Q.j; i /;

o D p.i /Q.i; i / CX

p.j /Q.j; i /

j¤i

D p.i /

Q.i; j / D p.i /:

MX

jD1

In other words, if  .1/ has distribution p then so will  .2/, and like-
wise  .3/;  .4/; : : : ; p is the equilibrium distribution of the Markov chain
random walk deﬁned by transition probabilities Q. Under reasonable con-
ditions,   .b/ must asymptotically attain distribution p no matter how  .1/
is initially selected.

9

13.5 Example: Modeling Population Admixture

MCMC has had a big impact in statistical genetics, where Bayesian mod-
eling is popular and useful for representing the complex evolutionary pro-
cesses. Here we illustrate its use in demography and modeling admixture—
estimating the contributions from ancestral populations in an individual
7 In Bayes applications, p.i / D g..i /jx/ D g..i //f.i /.x/=f .x/ (13.56).
However, f .x/ is not needed since it cancels out of (13.76), a considerable advantage
in complicated situations when f .x/ is often unavailable, and a prime reason for the
popularity of MCMC.

13.5 Example: Modeling Population Admixture

257

genome. For example, we might consider human ancestry, and for each
individual wish to estimate the proportion of their genome coming from
European, African, and Asian origins. The procedure we describe
here is unsupervised—a type of soft clustering—but we will see it can be
very informative with regard to such questions. We have a sample of n in-
dividuals, and we assume each arose from possible admixture among J
parent populations, each with their own characteristic vector of allele fre-
quencies. For us J D 3, and let Qi 2
S3 denote a probability vector for in-
dividual i representing the proportions of their heritage coming from pop-
ulations j 2 f1; 2; 3g (see Section 5.4). We have genomic measurements
for each individual, in our case SNPs (single-nucleotide polymorphisms)
at each of M well-spaced loci, and hence can assume they are in linkage
equilibrium. At each SNP we have a measurement that identiﬁes the two
alleles (one per chromosome), where each can be either the wild-type A or
the mutation a. That is, we have the genotype Gi m at SNP m for individual
i: a three-level factor with levels fAA; Aa; aag which we code as 0; 1; 2.
Table 13.6 shows some examples.

Table 13.6 A subset of the genotype data on 197 individuals, each with
genotype measurements at 100 SNPs. In this case the ethnicity is
known for each individual, one of Japanese, African, European,
or African American. For example, individual NA12239 has
genotype Aa for SNP1, NA19247 has AA, and NA20126 has aa.

Subject

SNP1

SNP2

SNP3

NA10852
NA12239
NA19072
NA19247
NA20126
NA18868
NA19257
NA19079
NA19067
NA19904

1
1
0
0
2
0
0
0
0
0

1
1
0
0
0
0
0
1
0
0

0
0
0
2
0
1
0
0
0
1

(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)

SNP97

SNP98

SNP99

SNP100

1
1
0
0
2
0
0
0
0
0

1
1
0
0
0
0
0
1
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
2
0
1
0
0
0
1

Let Pj be the (unknown) M -vector of minor allele frequencies (propor-
tions actually) in population j . We have available a sample of n individ-
uals, and for each sample we have their genomic information measured at
each of the M loci. Some of the individuals might appear to have pure an-
cestral origins, but many do not. Our goal is to estimate Qi ; i D 1; : : : ; n;
and Pj ; j 2 f1; 2; 3g.

258

Objective Bayes Inference and MCMC

i m ; X .2/

For this purpose it is useful to pose a generative model. We ﬁrst create a
pair of variables Xi m D .X .1/
i m / corresponding to each Gi m, to which
we allocate the two alleles (in arbitrary order). For example, if Gi m D 1
D 1 (or
(corresponding to Aa), then we might set X .1/
vice versa). If Gi m D 0 they are both 0, and if Gi m D 2, they are both 1.
i m
Let Zi m 2 f1; 2; 3g2 represent the ancestral origin for individual i of each
of these allele copies Xi m at locus m, again a two-vector with elements
Zi m D .Z.1/

D 0 and X .2/

i m

i m /. Then our generative model goes as follows.

i m ; Z.2/
(cid:24) Mult.1; Qi /, independently at each m, for each copy c D 1; 2.
That is, we select the ancestral origin of each chromosome at locus m
according to the individual’s mixture proportions Qi.
D j , for each copy c D 1; 2. What this
means is that, for each of the two ancestral picks at locus m (one for
each arm of the chromosome), we draw a binomial with the appropriate
allele frequency.

(cid:24) Bi.1; Pj m/ if Z.c/

i m

1 Z.c/
i m

2 X .c/
i m

To complete the Bayesian speciﬁcation, we need to supply priors for the
Qi and also for Pj m. Although one can get fancy here, we resort to the
recommended ﬂat priors, which are
(cid:15) Qi (cid:24) D.(cid:21); (cid:21); (cid:21)/, a ﬂat three-component Dirichlet, independently for
each subject i  and
10 (cid:15) Pj m (cid:24) D.(cid:13); (cid:13) / independently for each population j , and each locus m

(the beta distribution; see 10 in the end notes).

We use the least-informative values (cid:21) D (cid:13) D 1. In practice, these could
get updated as well, but for the purposes of this demonstration we leave
them ﬁxed at these values.
Let X be the n (cid:2) M (cid:2) 2 array of observed alleles for all n samples.
We wish to estimate the posterior distribution Pr.P; QjX /, referring col-
lectively to all the elements of P and Q.

For this purpose we use Gibbs sampling, which amounts to the following

sequence.

0 Initialize Z.0/; P .0/; Q.0/.
1 Sample Z.b/ from the conditional distribution Pr.ZjX ; P .b(cid:0)1/; Q.b(cid:0)1//.
2 Sample P .b/; Q.b/ from the conditional distribution Pr.P; QjX ; Z.b//.
Gibbs is effective when one can sample efﬁciently from these conditional
distributions, which is the case here.

13.5 Example: Modeling Population Admixture

259

In step 2, we can sample P and Q separately. It can be seen that for each

.j; m/ we should sample Pj m from

Pj mjX ; Z (cid:24) D.(cid:21) C n.0/

j m; (cid:21) C n.1/
j m/;

D #f.i; c/ W X .c/
D #f.i; c/ W X .c/

i m

i m

D 0 and Z.c/
D 1 and Z.c/

i m

i m

D jg;
D jg:

where Z D Z.b/ and

n.0/
j m
n.1/
j m

(13.80)

(13.81)

This follows from the conjugacy of the two-component Dirichlet (beta)
with the binomial distribution, Table 13.1.
Updating Qi involves simulating from

QijX ; Z (cid:24) D.(cid:13) C mi1; (cid:13) C mi 2; (cid:13) C mi 3/;

(13.82)

where mij is the number of allele copies in individual i that originated
(according to Z D Z.b/) in population j :

mij D #f.c; m/ W Z.c/

i m

D jg:

(13.83)

Figure 13.7 Barycentric coordinate plot for the estimated
posterior means of the Qi based on MCMC sampling.

Step 1 can be performed by simulating Z.c/

i m independently, for each i; m;

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllEuropeanJapaneseAfricanAfrican American260

Objective Bayes Inference and MCMC

and c from

Pr.Z.c/
i m

D jjX ; P; Q/ D Qij Pr.X .c/

i m

`D1 Qi ` Pr.X .c/

i m

P3

jP; Z.c/

D j /

i m

jP; Z.c/

i m

:

(13.84)

D `/

The probabilities on the right refer back to our generative distribution de-
scribed earlier.

Figure 13.7 shows a triangle plot that summarizes the result of running
the MCMC algorithm on our 197 subjects. We used a burn in of 1000
complete iterations, and then a further 2000 to estimate the distribution
of the parameters of interest, in this case the Qi. Each dot in the ﬁgure
represents a three-component probability vector, and is the posterior mean
of the sampled Qi for each subject. The points are colored according to
the known ethnicity. Although this algorithm is unsupervised, we see that
the ethnic groups cluster nicely in the corners of the simplex, and allow
us to identify these clusters. The African American group is spread
between the African and European clusters (with a little movement
toward the Japanese).

(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)

Markov chain methods are versatile tools that have proved their value in

requiring some individual ingenuity with each application.

Bayesian applications. There are some drawbacks.
(cid:15) The algorithms are not universal in the sense of maximum likelihood,
(cid:15) As a result, applications, especially of Gibbs sampling, have favored
a small set of convenient priors, mainly Jeffreys and conjugates, that
simplify the calculations. This can cast doubt on the relevance of the
ing the convergence of estimates such as N DP  .b/=B slow.
resulting Bayes inferences.
(cid:15) Successive realizations  .b/ are highly correlated with each other, mak-
(cid:15) The correlation makes it difﬁcult to assign a standard error to N. Actual
applications ignore an initial B0 of the  .b/ values (as a “burn-in” period)
and go on to large enough B such that estimates like N appear to settle
down. However, neither the choice of B0 nor that of B may be clear.

Objective Bayes offers a paradigm of our book’s theme, the effect of
electronic computation on statistical inference: ingenious new algorithms
facilitated Bayesian applications over a wide class of applied problems and,
in doing so, inﬂuenced the dominant philosophy of the whole area.

13.6 Notes and Details

13.6 Notes and Details

261

The books by Savage (1954) and de Finetti (1972), summarizing his ear-
lier work, served as foundational texts for the subjective Bayesian school
of inference. Highly inﬂuential, they championed a framework for Bayes-
ian applications based on coherent behavior and the careful elucidation of
personal probabilities. A current leading text on Bayesian methods, Carlin
and Louis (2000), does not reference either Savage or de Finetti. Now Jef-
freys (1961), again following earlier works, claims foundational status. The
change of direction has not gone without protest from the subjectivists—
see Adrian Smith’s discussion of O’Hagan (1995)—but is nonetheless al-
most a complete rout.

Metropolis et al. (1953), as part of nuclear weapons research, devel-
oped the ﬁrst MCMC algorithm. A vigorous line of work on Markov chain
methods for solving difﬁcult probability problems has continued to ﬂour-
ish under such names as particle ﬁltering and sequential Monte Carlo; see
Gerber and Chopin (2015) and its enthusiastic discussion.

Modeling population admixture (Pritchard et al., 2000) is one of sev-
eral applications of hierarchical Bayesian models and MCMC in genetics.
Other applications include haplotype estimation and motif ﬁnding, as well
as estimation of phylogenetic trees. The examples in this section were de-
veloped with the kind help of Hua Tang and David Golan, both from the
Stanford Genetics department. Hua suggested the example and provided
helpful guidance; David provided the data, and ran the MCMC algorithm
using the STRUCTURE program in the Pritchard lab.

1 [p. 236] Uninformative priors. A large catalog of possible uninformative
priors has been proposed, thoroughly surveyed by Kass and Wasserman
(1996). One approach is to use the likelihood from a small part of the data,
say just one or two data points out of n, as the prior, as with the “intrin-
sic priors” of Berger and Pericchi (1996), or O’Hagan’s (1995) “fractional
Bayes factors.” Another approach is to minimize some mathematical mea-
sure of prior information, as with Bernardo’s (1979) “reference priors” or
Jaynes’ (1968) “maximum entropy” criterion. Kass and Wasserman list a
dozen more possibilities.

2 [p. 236] Coverage matching priors. Welch and Peers (1963) showed that,
for a multiparameter family f(cid:22).x/ and real-valued parameter of interest
 D t .(cid:22)/, there exist priors g.(cid:22)/ such that the Bayes credible interval of
coverage ˛ has frequentist coverage ˛CO.1=n/, with n the sample size. In
other words, the credible intervals are “second-order accurate” conﬁdence
intervals. Tibshirani (1989), building on Stein’s (1985) work, produced the

262

Objective Bayes Inference and MCMC

nice formulation (13.9). Stein’s paper developed the least-favorable fam-
ily, the one-parameter subfamily of f(cid:22).x/ that does not inappropriately in-
crease the amount of Fisher information for estimating . Cox and Reid’s
(1987) orthogonal parameters form (13.8) is formally equivalent to the
least favorable family construction.

Least favorable family versions of reference priors and intrinsic priors
have been proposed to avoid the difﬁculty with general-purpose uninfor-
mative priors seen in Figure 11.7. They do so, but at the price of requiring
a different prior for each choice of  D t .(cid:22)/—which begins to sound more
frequentistic than Bayesian.

3 [p. 238] Conjugate families theorem. Theorem 13.1, (13.16)–(13.18), is
rigorously derived in Diaconis and Ylvisaker (1979). Families other than
(13.14) have conjugate-like properties, but not the neat posterior expecta-
tion result (13.18).
using ˛ D log.(cid:22)/,  .˛/ D (cid:22), and V .(cid:22)/ D (cid:22) for the Poisson.
has density (cid:22)(cid:23)(cid:0)1e
(cid:0).(cid:23)C1/ e

5 [p. 239] Inverse gamma and chi-square distributions. A G(cid:23) variate (13.22)
(cid:0)(cid:22) = .(cid:23)/. An inverse gamma variate 1=G(cid:23) has density

4 [p. 238] Poisson formula (13.20). This follows immediately from (13.14),

(cid:0)1=(cid:22)= .(cid:23)/, so

(cid:22)

gn0;x0 .(cid:22)/ D c(cid:22)

(cid:0).n0x0C2/e

(cid:0)n0x0(cid:23)=(cid:22)

(13.85)

is the gamma conjugate density in Table 13.1. The gamma results can be
restated in terms of chi-squared variates:
D (cid:22)

xi (cid:24) (cid:22)

(13.86)

(cid:31)2
m
m

Gm=2
m=2

has conjugate prior

gn0;x0 .(cid:22)/ (cid:24) n0x0m=(cid:31)2

n0mC2;

(13.87)

an inverse chi-squared distribution.

6 [p. 240] Vasoconstriction data. Efron and Gous (2001) use this data to illus-
trate a theory connecting Bayes factors with Fisherian hypothesis testing.
It is part of a larger data set appearing in Finney (1947), also discussed in
Kass and Raftery (1995).

7 [p. 245] Jeffreys’ and Fisher’s scales of evidence. Jeffreys’ scale as it ap-
pears in Table 13.3 is taken from the slightly amended form in Kass and
Raftery (1995). Efron and Gous (2001) compare it with Fisher’s scale for
the contradictory results of Table 13.5. Fisher and Jeffreys worked in dif-
ferent scientiﬁc contexts—small-sample agricultural experiments versus

13.6 Notes and Details

263

hard-science geostatistics—which might explain Jeffreys’ more stringent
conception of what constitutes signiﬁcant evidence.

8 [p. 246] The Bayesian information criterion. The BIC was proposed by
Schwarz (1978). Kass and Wasserman (1996) provide an extended discus-
sion of the BIC and model selection. “Proofs” of (13.37) ultimately depend
on sample size coherency (13.49), as in Efron and Gous (2001). Quotation
marks are used here to indicate the basically qualitative nature of BIC: if
we think of the data points as being collected in pairs then n becomes n=2
in (13.38), etc., so it doesn’t pay to put too ﬁne a point on the criterion.

Moreover, the convergence is geometric in the L1 normPjp.b/

9 [p. 256] MCMC convergence. Suppose we begin the MCMC random walk
(13.76)–(13.77) by choosing  .1/ according to some arbitrary starting dis-
tribution p.1/. Let p.b/ be the distribution of  .b/, obtained after b steps of
the random walk. Markov chain theory says that, under certain broad con-
ditions on Q.i; j /, p.b/ will converge to the target distribution p (13.75).
(cid:0)pkj, suc-
cessive discrepancies eventually decreasing by a multiplicative factor. A
proof appears in Tanner and Wong (1987). Unfortunately, the factor won’t
be known in most applications, and the actual convergence may be quite
slow.

k

10 [p. 258] Dirichlet distribution. The Dirichlet is a multivariate generaliza-
tion of the beta distribution (Section 5.1), typically used to represent prior
distributions for the multinomial distribution. For x D .x1; x2; : : : ; xk/
0,

j xj D 1, the D.(cid:23)/ density is deﬁned as

kY

jD1

(cid:23)j(cid:0)1

x
j

;

(13.88)

with xj 2 .0; 1/,P
where B.(cid:23)/ DQ

f(cid:23).x/ D 1

j .(cid:23)j /= .P

B.(cid:23)/

j (cid:23)j /.

14

Statistical Inference and Methodology in the

Postwar Era

The fundamentals of statistical inference—frequentist, Bayesian, Fisherian
—were set in place by the end of the ﬁrst half of the twentieth century, as
discussed in Part I of this book. The postwar era witnessed a massive ex-
pansion of statistical methodology, responding to the data-driven demands
of modern scientiﬁc technology. We are now at the end of Part II, “Early
Computer-Age Methods,” having surveyed the march of new statistical
algorithms and their inferential justiﬁcation from the 1950s through the
1990s.

This was a time of opportunity for the discipline of statistics, when the
speed of computation increased by a factor of a thousand, and then another
thousand. As we said before, a land bridge had opened to a new continent,
but not everyone was eager to cross. We saw a mixed picture: the computer
played a minor or negligible role in the development of some inﬂuential
topics such as empirical Bayes, but was fundamental to others such as the
bootstrap.

Fifteen major topics were examined in Chapters 6 through 13. What fol-
lows is a short scorecard of their inferential afﬁnities, Bayesian, frequentist,
or Fisherian, as well as an assessment of the computer’s role in their devel-
opment. None of this is very precise, but the overall picture, illustrated in
Figure 14.1, is evocative.

Empirical Bayes

Robbins’ original development of formula (6.5) was frequentistic, but most
statistical researchers were frequentists in the postwar era so that could be
expected. The obvious Bayesian component of empirical Bayes arguments
is balanced by their frequentist emphasis on (nearly) unbiased estimation
of Bayesian estimators, as well as the restriction to using only current data
for inference. Electronic computation played hardly any role in the theory’s
development (as indicated by blue coloring in the ﬁgure). Of course mod-

264

Postwar Inference and Methodology

265

Figure 14.1 Bayesian, frequentist, and Fisherian inﬂuences, as
described in the text, on 15 major topics, 1950s through 1990s.
Colors indicate the importance of electronic computation in their
development: red, crucial; violet, very important; green,
important; light blue, less important; blue, negligible.

ern empirical Bayes applications are heavily computational, but that is the
case for most methods now.

James–Stein and Ridge Regression

The frequentist roots of James–Stein estimation are more deﬁnitive, es-
pecially given the force of the James–Stein theorem (7.16). Nevertheless,
the empirical Bayes interpretation (7.13) lends James–Stein some Bayes-
ian credibility. Electronic computation played no role in its development.
This was less true for ridge regression, colored light blue in the ﬁgure,
where the matrix calculation (7.36) would have been daunting in the pre-
electronic age. The Bayesian justiﬁcation (7.37)–(7.39) of ridge regression

 lllBayesianFrequentistFisherianlKaplan−Meierllog−ranklglmlproportional hazards(partial likelihood)lbootstraplempiricalBayeslobjectiveBayes(mcmc)ljackknifelCVlJames−Steinlregression treeslridgeregressionlBIClmissing data(EM)lAIC−Cp266

Postwar Inference and Methodology

carries more weight than for James–Stein, given the absence of a strong
frequentist theorem.

Generalized Linear Models

GLM development began with a pronounced Fisherian emphasis on like-
lihood1 modeling, but settled down to more or less standard frequentist
regression theory. A key operational feature, low-dimensional sufﬁcient
statistics, limited its computational demands, but GLM theory could not
have developed before the age of electronic computers (as indicated by
green coloring).

Regression Trees

Model building by means of regression trees is a computationally intensive
enterprise, indicated by its red color in Figure 14.1. Its justiﬁcation has
been mainly in terms of asymptotic frequentist properties.

Survival Analysis

The Kaplan–Meier estimate, log-rank test, and proportional hazards model
move from the frequentist pole of the diagram toward the Fisherian pole
as the conditioning arguments in Sections 9.2 through 9.4 become more
elaborate. The role of computation in their development increases in the
same order. Kaplan–Meier estimates can be done by hand (and were),
while it is impossible to contemplate proportional hazards analysis with-
out the computer. Partial likelihood, the enabling argument for the theory,
is a quintessential Fisherian device.

Missing Data and the EM Algorithm

The imputation of missing data has a Bayesian ﬂavor of indirect evidence,
but the “fake data” principle (9.44)–(9.46) has Fisherian roots. Fast compu-
tation was important to the method’s development, particularly so for the
EM algorithm.

Jackknife and Bootstrap

The purpose of the jackknife was to calculate frequentist standard errors
and biases. Electronic computation was of only minor importance in its

1 More explicitly, quasilikelihoods, an extension to a wider class of exponential family

models.

Postwar Inference and Methodology

267

development. By contrast, the bootstrap is the archetype for computer-
intensive statistical inference. It combines frequentism with Fisherian de-
vices: plug-in estimation of accuracy estimates, as in (10.18)–(10.19), and
correctness arguments for bootstrap conﬁdence intervals, (11.79)–(11.83).

Cross-Validation

The renaissance of interest in cross-validation required fast computation,
especially for assessing modern computer-intensive prediction algorithms.
As pointed out in the text following Figure (12.3), cross-validation is a
strongly frequentist procedure.

BIC, AIC, and Cp

These three algorithms were designed to avoid computation, BIC for Bayes-
ian model selection, Section (13.3), AIC and Cp for unbiased estimation
of frequentist prediction error, (12.76) and (12.50).

Objective Bayes and MCMC

In addition to their Bayesian provenance, objective Bayes methods have
some connection with ﬁducial ideas and the bootstrap, as discussed in Sec-
tion 11.5. (An argument can be made that they are at least as frequentist as
they are Bayesian—see the notes below—though that has not been acted
upon in coloring the ﬁgure.) Gibbs sampling and MCMC, the enabling al-
gorithms, epitomize modern computer-intensive inference.

Notes

Figure 14.1 is an updated version of Figure 8 in Efron (1998), “R. A. Fisher
in the 21st Century.” There the difﬁculty of properly placing objective
Bayes is confessed, with Erich Lehmann arguing for a more frequentist
(decision-theoretic) location: “In fact, the concept of uninformative prior
is philosophically close to Wald’s least favorable distribution, and the two
often coincide.”

Figure 14.1 shows a healthy mixture of philosophical and computational
tactics at work, with all three edges (but not the center) of the triangle in
play. All new points will be red (computer-intensive) as we move into the
twenty-ﬁrst century in Part III. Our triangle will have to struggle to accom-
modate some major developments based on machine learning, a philosoph-
ically atheistic approach to statistical inference.

Part III

Twenty-First-Century Topics

15

Large-Scale Hypothesis Testing and

False-Discovery Rates

By the ﬁnal decade of the twentieth century, electronic computation fully
dominated statistical practice. Almost all applications, classical or other-
wise, were now performed on a suite of computer platforms: SAS, SPSS,
Minitab, Matlab, S (later R), and others.

The trend accelerates when we enter the twenty-ﬁrst century, as statis-
tical methodology struggles, most often successfully, to keep up with the
vastly expanding pace of scientiﬁc data production. This has been a two-
way game of pursuit, with statistical algorithms chasing ever larger data
sets, while inferential analysis labors to rationalize the algorithms.

Part III of our book concerns topics in twenty-ﬁrst-century1 statistics.
The word “topics” is intended to signal selections made from a wide cat-
alog of possibilities. Part II was able to review a large portion (though
certainly not all) of the important developments during the postwar period.
Now, deprived of the advantage of hindsight, our survey will be more illus-
trative than deﬁnitive.

For many statisticians, microarrays provided an introduction to large-
scale data analysis. These were revolutionary biomedical devices that en-
abled the assessment of individual activity for thousands of genes at once—
and, in doing so, raised the need to carry out thousands of simultaneous
hypothesis tests, done with the prospect of ﬁnding only a few interesting
genes among a haystack of null cases. This chapter concerns large-scale
hypothesis testing and the false-discovery rate, the breakthrough in statis-
tical inference it elicited.

1 Actually what historians might call “the long twenty-ﬁrst century” since we will begin

in 1995.

271

272

Large-scale Hypothesis Testing and FDRs

15.1 Large-Scale Testing

The prostate cancer data, Figure 3.4, came from a microarray study of
n D 102 men, 52 prostate cancer patients and 50 normal controls. Each
man’s gene expression levels were measured on a panel of N D 6033
genes, yielding a 6033 (cid:2) 102 matrix of measurements xij ,
xij D activity of ith gene for j th man:

(15.1)

For each gene, a two-sample t statistic (2.17) ti was computed com-
paring gene i’s expression levels for the 52 patients with those for the 50
controls. Under the null hypothesis H0i that the patients’ and the controls’
responses come from the same normal distribution of gene i expression
levels, ti will follow a standard Student t distribution with 100 degrees of
freedom, t100. The transformation
zi D ˆ

(cid:0)1 .F100.ti // ;
(15.2)
(cid:0)1 the inverse function of
where F100 is the cdf of a t100 distribution and ˆ
a standard normal cdf, makes zi standard normal under the null hypothesis:

H0i W zi (cid:24)

N .0; 1/:

(15.3)

Of course the investigators were hoping to spot some non-null genes,
ones for which the patients and controls respond differently. It can be
shown that a reasonable model for both null and non-null genes is2 

1

zi (cid:24)

N .(cid:22)i ; 1/;

(15.4)
(cid:22)i being the effect size for gene i. Null genes have (cid:22)i D 0, while the
investigators hoped to ﬁnd genes with large positive or negative (cid:22)i effects.
Figure 15.1 shows the histogram of the 6033 zi values. The red curve is
the scaled N .0; 1/ density that would apply if in fact all of the genes were
null, that is if all of the (cid:22)i equaled zero.3 We can see that the curve is a
little too high near the center and too low in the tails. Good! Even though
most of the genes appear null, the discrepancies from the curve suggest that
there are some non-null cases, the kind the investigators hoped to ﬁnd.

Large-scale testing refers exactly to this situation: having observed a
large number N of test statistics, how should we decide which if any of the
null hypotheses to reject? Classical testing theory involved only a single
case, N D 1. A theory of multiple testing arose in the 1960s, “multiple”

2 This is model (3.28), with zi now replacing the notation xi .
3 It is ce

2(cid:25) with c chosen to make the area under the curve equal the area of

p

(cid:0)z2=2=
the histogram.

15.1 Large-Scale Testing

273

Figure 15.1 Histogram of N D 6033 z-values, one for each gene
in the prostate cancer study. If all genes were null (15.3) the
histogram would track the red curve. For which genes can we
reject the null hypothesis?

meaning N between 2 and perhaps 20. The microarray era produced data
sets with N in the hundreds, thousands, and now even millions. This sounds
like piling difﬁculty upon difﬁculty, but in fact there are some inferential
advantages to the large-N framework, as we will see.

The most troubling fact about large-scale testing is how easy it is to be
fooled. Running 100 separate hypothesis tests at signiﬁcance level 0.05
will produce about ﬁve “signiﬁcant” results even if each case is actually
null. The classical Bonferroni bound avoids this fallacy by strengthening
the threshold of evidence required to declare an individual case signiﬁcant
(i.e., non-null). For an overall signiﬁcance level ˛, perhaps ˛ D 0:05, with
N simultaneous tests, the Bonferroni bound rejects the ith null hypothesis
H0i only if it attains individual signiﬁcance level ˛=N . For ˛ D 0:05,
N D 6033, and H0i W zi (cid:24)
N .0; 1/, the one-sided Bonferroni threshold
for signiﬁcance is (cid:0)ˆ
(cid:0)1.0:05=N / D 4:31 (compared with 1.645 for N D
1). Only four of the prostate study genes surpass this threshold.

Classic hypothesis testing is usually phrased in terms of signiﬁcance lev-
els and p-values. If test statistic z has cdf F0.z/ under the null hypothesis

−4−20240100200300400500z−valuesCounts274

then4

Large-scale Hypothesis Testing and FDRs

p D 1 (cid:0) F0.z/

(15.5)

is the right-sided p-value, larger z giving smaller p-value. “Signiﬁcance
level” refers to a prechosen threshold value, e.g., ˛ D 0:05. The null
hypothesis is “rejected at level ˛” if we observe p  ˛. Table 13.4 on
page 245 (where “coverage level” means one minus the signiﬁcance level)
shows Fisher’s scale for interpreting p-values.

A level-˛ test for a single null hypothesis H0 satisﬁes, by deﬁnition,

˛ D Prfreject true H0g:

For a collection of N null hypotheses H0i, the family-wise error rate is the
probability of making even one false rejection,

(15.6)

(15.7)

(15.8)

Bonferroni’s procedure controls FWER at level ˛: let I0 be the indices of
the true H0i, having say N0 members. Then

FWER D Prfreject any true H0ig:
n
pi  ˛

9=; X

8<:[

(cid:16)
pi  ˛
 ˛;

N

Pr

I0

N

o

FWER D Pr

I0

D N0

˛

N

2

the top line following from Boole’s inequality (which doesn’t require even
independence among the pi).
The Bonferroni bound is quite conservative: for N D 6033 and ˛ D
0:05 we reject only those cases having pi  8:3 (cid:1) 10
(cid:0)6. One can do only a
little better under the FWER constraint. “Holm’s procedure,”which offers
modest improvement over Bonferroni, goes as follows.
(cid:15) Order the observed p-values from smallest to largest,

p.1/  p.2/  p.3/  : : :  p.i /  : : :  p.N /;

(15.9)

with H0.i / denoting the corresponding null hypotheses.
(cid:15) Let i0 be the smallest index i such that

p.i / > ˛=.N (cid:0) i C 1/:

(15.10)

(cid:15) Reject all null hypotheses H0.i / for i < i0 and accept all with i (cid:21) i0.
4 The left-sided p-value is p D F0.z/. We will avoid two-sided p-values in this
discussion.

15.2 False-Discovery Rates

275

It can be shown that Holm’s procedure controls FWER at level ˛, while
being slightly more generous than Bonferroni in declaring rejections.

15.2 False-Discovery Rates

The FWER criterion aims to control the probability of making even one
false rejection among N simultaneous hypothesis tests. Originally devel-
oped for small-scale testing, say N  20, FWER usually proved too con-
servative for scientists working with N in the thousands. A quite different
and more liberal criterion, false-discovery rate (FDR) control, has become
standard.

Figure 15.2 A decision rule D has rejected R out of N null
hypotheses; a of these decisions were incorrect, i.e., they were
“false discoveries,” while b of them were “true discoveries.” The
false-discovery proportion Fdp equals a=R.

Figure 15.2 diagrams the outcome of a hypothetical decision rule D ap-
plied to the data for N simultaneous hypothesis-testing problems, N0 null
and N1 D N (cid:0) N0 non-null. An omniscient oracle has reported the rule’s
results: R null hypotheses have been rejected; a of these were cases of
false discovery, i.e., valid null hypotheses, for a “false-discovery propor-
tion” (Fdp) of

Fdp.D/ D a=R:

(15.11)
(We deﬁne Fdp D 0 if R D 0.) Fdp is unobservable—without the oracle
we cannot see a—but under certain assumptions we can control its expec-
tation.

4	  Null Actual Non-Null Null N0 - a Non-Null a b N1 - b N - R R N N1 N0 Decision 276

Large-scale Hypothesis Testing and FDRs

Deﬁne

FDR.D/ D E fFdp.D/g :

(15.12)
A decision rule D controls FDR at level q, with q a prechosen value be-
tween 0 and 1, if
(15.13)

FDR.D/  q:

It might seem difﬁcult to ﬁnd such a rule, but in fact a quite simple but in-
genious recipe does the job. Ordering the observed p-values from smallest
to largest as in (15.9), deﬁne imax to be the largest index for which

p.i /  i

N

q;

(15.14)

3

and let Dq be the rule5 that rejects H0.i / for i  imax, accepting otherwise.

A proof of the following theorem is referenced in the chapter endnotes.
Theorem (Benjamini–Hochberg FDR Control)
ing to valid null hypotheses are independent of each other, then

If the p-values correspond-

FDR.Dq/ D (cid:25)0q  q;

where (cid:25)0 D N0=N:

(15.15)

In other words Dq controls FDR at level (cid:25)0q. The null proportion (cid:25)0 is
unknown (though estimable), so the usual claim is that Dq controls FDR at
level q. Not much is sacriﬁced: large-scale testing problems are most often
ﬁshing expeditions in which most of the cases are null, putting (cid:25)0 near 1,
identiﬁcation of a few non-null cases being the goal. The choice q D 0:1
is typical practice.

The popularity of FDR control hinges on the fact that it is more generous
than FWER in declaring signiﬁcance.6 Holm’s procedure (15.10) rejects
null hypothesis H0.i / if

p.i /  Threshold(Holm’s) D

˛

N (cid:0) i C 1

;

(15.16)

while Dq (15.13) has threshold

p.i /  Threshold(Dq) D q

N

i:

(15.17)

5 Sometimes denoted “BHq” after its inventors Benjamini and Hochberg; see the chapter

endnotes.

6 The classic term “signiﬁcant” for a non-null identiﬁcation doesn’t seem quite right for
FDR control, especially given the Bayesian connections of Section 15.3, and we will
sometimes use “interesting” instead.

15.2 False-Discovery Rates

277

In the usual range of interest, large N and small i, the ratio

Threshold(Dq)
Threshold(Holm’s)

D q
˛

increases almost linearly with i.


1 (cid:0) i (cid:0) 1



i

(15.18)

N

Figure 15.3 Ordered p-values p.i / D 1 (cid:0) ˆ.z.i // plotted versus
i for the 50 largest z-values from the prostate data in
Figure 15.1. The FDR control boundary (algorithm Dq, q D 0:1)
rejects H0.i / for the 28 smallest values p.i /, while Holm’s FWER
procedure (˛ D 0:1) rejects for only the 7 smallest values. (The
upward slope of Holm’s boundary (15.16) is too small to see
here.)

Figure 15.3 illustrates the comparison for the right tail of the prostate
data of Figure 15.1, with pi D 1 (cid:0) ˆ.zi / (15.3), (15.5), and ˛ D q D
0:1. The FDR procedure rejects H0.i / for the 28 largest z-values (z.i / (cid:21)
3:33), while FWER control rejects only the 7 most extreme z-values (z.i / (cid:21)
4:14).

Hypothesis testing has been a traditional stronghold of frequentist deci-
sion theory, with “Type 1” error control being strictly enforced, very often
at the 0.05 level. It is surprising that a new control criterion, FDR, has
taken hold in large-scale testing situations. A critic, noting FDR’s relaxed
rejection standards in Figure 15.3, might raise some pointed questions.

**************************************************010203040500e+005e−041e−03index ip−valueHolm'sFDRi = 7i = 28278

Large-scale Hypothesis Testing and FDRs

1 Is controlling a rate (i.e., FDR) as meaningful as controlling a probabil-

ity (of Type 1 error)?

4 The FDR signiﬁcance for gene i0, say one with zi0

Isn’t this unlikely in situations such as the prostate study?

2 How should q be chosen?
3 The control theorem depends on independence among the p-values.
D 3, depends on the
results of all the other genes: the more “other” zi values exceed 3, the
more interesting gene i0 becomes (since that increases i0’s index i in
the ordered list (15.9), making it more likely that pi0 lies below the Dq
threshold (15.14)). Does this make inferential sense?

A Bayes/empirical Bayes restatement of the Dq algorithm helps answer
these questions, as discussed next.

15.3 Empirical Bayes Large-Scale Testing

In practice, single-case hypothesis testing has been a frequentist preserve.
Its methods demand little from the scientist—only the choice of a test
statistic and the calculation of its null distribution—while usually deliver-
ing a clear verdict. By contrast, Bayesian model selection, whatever its in-
ferential virtues, raises the kinds of difﬁcult modeling questions discussed
in Section 13.3.

It then comes as a pleasant surprise that things are different for large-
scale testing: Bayesian methods, at least in their empirical Bayes manifes-
tation, no longer demand heroic modeling efforts, and can help untangle
the interpretation of simultaneous test results. This is particularly true for
the FDR control algorithm Dq of the previous section.
A simple Bayesian framework for simultaneous testing is provided by
the two-groups model: each of the N cases (the genes for the prostate
study) is either null with prior probability (cid:25)0 or non-null with probabil-
ity (cid:25)1 D 1 (cid:0) (cid:25)0; the resulting observation z then has density either f0.z/
or f1.z/,

(cid:25)0 D Prfnullg
(cid:25)1 D Prfnon-nullg
p
For the prostate study, (cid:25)0 is nearly 1, and f0.z/ is the standard normal den-
sity (cid:30).z/ D exp.(cid:0)z2=2/=
2(cid:25) (15.3), while the non-null density remains
to be estimated.

f0.z/ density if null;
f1.z/ density if non-null:

(15.19)

Let F0.z/ and F1.z/ be the cdf values corresponding to f0.z/ and f1.z/,

15.3 Empirical Bayes Large-Scale Testing

279

with “survival curves”

S0.z/ D 1 (cid:0) F0.z/

and S1.z/ D 1 (cid:0) F1.z/;

(15.20)
S0.z0/ being the probability that a null z-value exceeds z0, and similarly
for S1.z/. Finally, deﬁne S.z/ to be the mixture survival curve

(15.21)

(15.22)

(15.23)

The mixture density

determines S.z/,

S.z/ D (cid:25)0S0.z/ C (cid:25)1S1.z/:

f .z/ D (cid:25)0f0.z/ C (cid:25)1f1.z/

S.z0/ DZ 1

z0

f .z/ dz:

Suppose now that observation zi for case i is seen to exceed some thresh-
old value z0, perhaps z0 D 3. Bayes’ rule gives

Fdr.z0/  Prfcase i is nulljzi (cid:21) z0g

D (cid:25)0S0.z0/=S.z0/;

(15.24)
the correspondence with (3.5) on page 23 being (cid:25)0 D g.(cid:22)/, S0.z0/ D
f(cid:22).x/, and S.z0/ D f .x/. Fdr is the “Bayes false-discovery rate,” as con-
trasted with the frequentist quantity FDR (15.12).
In typical applications, S0.z0/ is assumed known7 (equaling 1 (cid:0) ˆ.z0/
in the prostate study), and (cid:25)0 is assumed to be near 1. The denominator
S.z0/ in (15.24) is unknown, but—and this is the crucial point—it has an
obvious estimate in large-scale testing situations, namely

OS .z0/ D N.z0/=N;

where N.z0/ D #fzi (cid:21) z0g:

(15.25)
(By the deﬁnition of the two-group model, each zi has marginal density
f .z/, making OS .z0/ the usual empirical estimate of S.z0/ (15.23).) Plug-
ging into (15.24) yields an empirical Bayes estimate of the Bayes false-
discovery rate

cFdr.z0/ D (cid:25)0S0.z0/ı OS .z0/:

(15.26)
The connection with FDR control is almost immediate. First of all, from
deﬁnitions (15.5) and (15.20) we have pi D S0.zi /; also for the ith from
the largest z-value we have OS .z.i // D i=N (15.25). Putting these together,
condition (15.14), p.i /  .i=N /q, becomes

S0.z.i //  OS .z.i // (cid:1) q;

(15.27)

7 But see Section 15.5.

280

Large-scale Hypothesis Testing and FDRs

or S0.z.i //= OS .z.i //  q, which can be written as
cFdr.z.i //  (cid:25)0q

(15.28)
(15.26). In other words, the Dq algorithm, which rejects those null hy-
potheses having8 p.i /  .i=N /q, is in fact rejecting those cases for which
the empirical Bayes posterior probability of nullness is too small, as de-
ﬁned by (15.28). The Bayesian nature of FDR control offers a clear advan-
tage to the investigating scientist, who gets a numerical assessment of the
probability that he or she will be wasting time following up any one of the
selected cases.

We can now respond to the four questions at the end of the previous

section:

1 FDR control does relate to a probability—the Bayes posterior probabil-

ity of nullness.
2 The choice of q for Dq amounts to setting the maximum tolerable amount
of Bayes risk of nullness9 (usually after taking (cid:25)0 D 1 in (15.28)).
S.z0/, makingcFdr.z0/ (15.26) nearly unbiased for Fdr.z0/ (15.24). There
3 Most often the zi, and hence the pi, will be correlated with each other.
Even under correlation, however, OS .z0/ in (15.25) is still unbiased for
S0.z0/ and cFdr.z0/.

is a price to be paid for correlation, which increases the variance of

4 In the Bayes two-groups model (15.19), all of the non-null zi are i.i.d.
observations from the non-null density f1.z/, with survival curve S1.z/.
The number of null cases zi exceeding some threshold z0 has ﬁxed ex-
pectation N (cid:25)0S0.z0/. Therefore an increase in the number of observed
values zi exceeding z0 must come from a heavier right tail for f1.z/,
implying a greater posterior probability of non-nullness Fdr.z0/ (15.24).
This point is made more clearly in the local false-discovery framework
of the next section. It emphasizes the “learning from the experience of
others” aspect of empirical Bayes inference, Section 7.4. The question
of “Which others?” is returned to in Section 15.6.

Figure 15.4 illustrates the two-group model (15.19). The N cases are

8 The algorithm, as stated just before the FDR control theorem (15.15), is actually a little

more liberal in allowing rejections.
9 For a case of particular interest, the calculation can be reversed: if the case has ordered
index i then, according to (15.14), the value q D Npi = i puts it exactly on the boundary
of rejection, making this its q-value. The 50th largest z-value for the prostate data has
zi D 2:99, pi D 0:00139, and q-value 0.168, that being both the frequentist
boundary for rejection and the empirical Bayes probability of nullness.

15.3 Empirical Bayes Large-Scale Testing

281

Figure 15.4 A diagram of the two-groups model (15.19). Here
the statistician observes values zi from a mixture density
f .z/ D (cid:25)0f0.z/ C (cid:25)1f1.z/ and decides to reject or accept the
null hypothesis H0i depending on whether zi exceeds or is less
than the threshold value z0.

(

W

D

randomly dispatched to the two arms in proportions (cid:25)0 and (cid:25)1, at which
point they produce z-values according to either f0.z/ or f1.z/. Suppose
we are using a simple decision rule D that rejects the ith null hypothesis if
zi exceeds some threshold z0, and accepts otherwise,

Reject H0i
Accept H0i

if zi > z0
if zi  z0:

(15.29)

The oracle of Figure 15.2 knows that N0.z0/ D a of the null case z-
values exceeded z0, and similarly N1.z0/ D b of the non-null cases, lead-
ing to

N.z0/ D N0.z0/ C N1.z0/ D R

(15.30)

(15.31)

total rejections. The false-discovery proportion (15.11) is

Fdp D N0.z0/

N.z0/

but this is unobservable since we see only N.z0/.

The clever inferential strategy of false-discovery rate theory substitutes

the expectation of N0.z0/,

E fN0.z0/g D N (cid:25)0S0.z0/;

(15.32)

282

Large-scale Hypothesis Testing and FDRs

D cFdr.z0/;

for N0.z0/ in (15.31), giving

dFdp D N (cid:25)0S0.z0/

D (cid:25)0S0.z0/
OS .z0/

(15.33)

N.z0/

using (15.25) and (15.26). Starting from the two-groups model, cFdr.z0/ is

an obvious empirical (i.e., frequentist) estimate of the Bayesian probability
Fdr.z0/, as well as of Fdp.

If placed in the Bayes–Fisher–frequentist triangle of Figure 14.1, false-
discovery rates would begin life near the frequentist corner but then mi-
grate at least part of the way toward the Bayes corner. There are remark-
able parallels with the James–Stein estimator of Chapter 7. Both theories
began with a striking frequentist theorem, which was then inferentially
rationalized in empirical Bayes terms. Both rely on the use of indirect
evidence—learning from the experience of others. The difference is that
James–Stein estimation always aroused controversy, while FDR control
has been quickly welcomed into the pantheon of widely used methods.
This could reﬂect a change in twenty-ﬁrst-century attitudes or, perhaps,
only that the Dq rule better conceals its Bayesian aspects.

15.4 Local False-Discovery Rates

Tail-area statistics (p-values) were synonymous with classic one-at-a-time
hypothesis testing, and the Dq algorithm carried over p-value interpreta-
tion to large-scale testing theory. But tail-area calculations are neither nec-
essary nor desirable from a Bayesian viewpoint, where, having observed
test statistic zi equal to some value z0, we should be more interested in the
probability of nullness given zi D z0 than given zi (cid:21) z0.
To this end we deﬁne the local false-discovery rate
fdr.z0/ D Prfcase i is nulljzi D z0g

(15.34)

as opposed to the tail-area false-discovery rate Fdr.z0/ (15.24). The main
point of what follows is that reasonably accurate empirical Bayes estimates
of fdr are available in large-scale testing problems.

As a ﬁrst try, suppose that Z0, a proposed region for rejecting null hy-

potheses, is a small interval centered at z0,

Z0 D

(cid:21)

z0 (cid:0) d

2

; z0 C d

2

;

(15.35)

with d perhaps 0.1. We can redraw Figure 15.4, now with N0.Z0/, N1.Z0/,

15.4 Local False-Discovery Rates

283
and N.Z0/ the null, non-null, and total number of z-values in Z0. The local
false-discovery proportion,

(15.36)
is unobservable, but we can replace N0.Z0/ with N (cid:25)0f0.z0/d, its approx-
imate expectation as in (15.31)–(15.33), yielding the estimate10

fdp.z0/ D N0.Z0/=N.Z0/
cfdr.z0/ D N (cid:25)0f0.z0/d=N.Z0/:

Estimate (15.37) would be needlessly noisy in practice; z-value distri-
butions tend to be smooth, allowing the use of regression estimates for
fdr.z0/. Bayes’ theorem gives

fdr.z/ D (cid:25)0f0.z/=f .z/

(15.37)

(15.38)

in the two-groups model (15.19) (with (cid:22) in (3.5) now the indicator of null
or non-null states, and x now z). Drawing a smooth curve O
f .z/ through
the histogram of the z-values yields the more efﬁcient estimate

cfdr.z0/ D (cid:25)0f0.z0/=

O
f .z0/I

(15.39)

cfdr.z/  0:2

Figure 15.5 shows cfdr.z/ for the prostate study data of Figure 15.1,
the null proportion (cid:25)0 can be estimated—see Section 15.5—or set equal to
1.
where O
f .z/ in (15.39) has been estimated as described below. The curve
hovers near 1 for the 93% of the cases having jzij  2, sensibly suggesting
that there is no involvement with prostate cancer for most genes. It declines
quickly for jzij (cid:21) 3, reaching the conventionally “interesting” threshold
(15.40)
for zi (cid:21) 3:34 and zi  (cid:0)3:40. This was attained for 27 genes in the right
tail and 25 in the left, these being reasonable candidates to ﬂag for follow-
up investigation.
The curve O
f .z/ used in (15.39) was obtained from a fourth-degree log
polynomial Poisson regression ﬁt to the histogram in Figure 15.1, as in
Figure 10.5 (10.52)–(10.56). Log polynomials of degree 2 through 6 were
ﬁt by maximum likelihood, giving total residual deviances (8.35) shown in
Table 15.1. An enormous improvement in ﬁt is seen in going from degree
3 to 4, but nothing signiﬁcant after that, with decreases less than the null
value 2 suggested by (12.75).

10 Equation (15.37) makes argument (4) of the previous section clearer: having more

“other” z-values fall into Z0 increases N.Z0/, decreasingcfdr.z0/ and making it more
likely that zi D z0 represents a non-null case.

284

Large-scale Hypothesis Testing and FDRs

Figure 15.5 Local false-discovery rate estimatecfdr.z/ (15.39)
the left, indicated by dashes, havecfdr.zi /  0:2; light dashed
curves are the left and right tail-area estimates cFdr.z/ (15.26).

for prostate study of Figure 15.1; 27 genes on the right and 25 on

Table 15.1 Total residual deviances from log polynomial Poisson
regressions of the prostate data, for polynomial degrees 2 through 6;
degree 4 is preferred.

Degree
Deviance

2

138.6

3

137.0

4

65.1

5

64.1

6

63.7

The points in Figure 15.6 represent the log bin counts from the histogram
in Figure 15.1 (excluding zero counts), with the solid curve showing the
4th-degree MLE polynomial ﬁt. Also shown is the standard normal log
density

log f0.z/ D (cid:0) 1

z2 C constant:

(15.41)
It ﬁts reasonably well for jzj < 2, emphasizing the null status of the gene
The cutoffcfdr.z/  0:2 for declaring a case interesting is not completely
majority.

arbitrary. Deﬁnitions (15.38) and (15.22), and a little algebra, show that it

2

−4−20240.00.20.40.60.81.0z−valuefdr and Fdrlocal fdr−3.403.3415.4 Local False-Discovery Rates

285

Figure 15.6 Points are log bin counts for Figure 15.1’s
histogram. The solid black curve is a fourth-degree

log-polynomial ﬁt used to calculatecfdr.z/ in Figure 15.5. The
dashed red curve, the log null density (15.41), provides a
reasonable ﬁt for jzj  2.

is equivalent to

(15.42)
If we assume (cid:25)0 (cid:21) 0:90, as is reasonable in most large-scale testing situa-
tions, this makes the Bayes factor f1.z/=f0.z/ quite large,

:

f1.z/
f0.z/

(cid:21) 4

(cid:25)0
(cid:25)1

(cid:21) 36;

f1.z/
f0.z/

(15.43)

“strong evidence” against the null hypothesis in Jeffreys’ scale, Table 13.3.
There is a simple relation between the local and tail-area false-discovery

rates:

Fdr.z0/ D E ffdr.z/jz (cid:21) z0gI

4

(15.44)

so Fdr.z0/ is the average value of fdr.z/ for z greater than z0. In interesting
situations, fdr.z/ will be a decreasing function for large values of z, as on
the right side of Figure 15.5, making Fdr.z0/ < fdr.z0/. This accounts

−4−202460123456z−valuelog densityllllllllllllllllllllllllllllllllllllllllllllll4th degree logpolynomialN(0,1)5

286

Large-scale Hypothesis Testing and FDRs

for the conventional signiﬁcance cutoff cFdr.z/  0:1 being smaller than
cfdr.z/  0:2 (15.40).
as with left-sided and right-sided tail-area cFdr estimates, sincecfdr.z/ ap-

The Bayesian interpretation of local false-discovery rates carries with it
the advantages of Bayesian coherency. We don’t have to change deﬁnitions

plies without change to both tails.11 Also, we don’t need a separate theory
for “true-discovery rates,” since

tdr.z0/  1 (cid:0) fdr.z0/ D (cid:25)1f1.z0/=f .z0/

(15.45)

is the conditional probability that case i is non-null given zi D z0.

15.5 Choice of the Null Distribution

The null distribution, f0.z/ in the two-groups model (15.19), plays a cru-
cial role in large-scale testing, just as it does in the classic single-case the-
ory. Something different however happens in large-scale problems: with
thousands of z-values to examine at once, it can become clear that the con-
ventional theoretical null is inappropriate for the situation at hand. Put more
positively, large-scale applications may allow us to empirically determine
a more realistic null distribution.
The police data of Figure 15.7 illustrates what can happen. Possi-
ble racial bias in pedestrian stops was assessed for N D 2749 New York
City police ofﬁcers in 2006. Each ofﬁcer was assigned a score zi, large
positive scores suggesting racial bias. The zi values were summary scores
from a complicated logistic regression model intended to compensate for
differences in the time of day, location, and context of the stops. Logistic
regression theory suggested the theoretical null distribution

H0i W zi (cid:24)

N .0; 1/

(15.46)

for the absence of racial bias.

The trouble is that the center of the z-value histogram in Figure 15.7,
which should track the N .0; 1/ curve applying to the presumably large
fraction of null-case ofﬁcers, is much too wide. (Unlike the situation for the
prostate data in Figure 15.1.) An MLE ﬁtting algorithm discussed below
produced the empirical null

H0i W zi (cid:24)

N .0:10; 1:402/

(15.47)

11 Going further, z in the two-groups model could be multidimensional. Then tail-area
false-discovery rates would be unavailable, but (15.38) would still legitimately deﬁne
fdr.z/.

15.5 Choice of the Null Distribution

287

Figure 15.7 Police data; histogram of z scores for N D 2749
New York City police ofﬁcers, with large zi suggesting racial
bias. The center of the histogram is too wide compared with the
theoretical null distribution zi (cid:24)
N .0; 1/. An MLE ﬁt to central
data gave N .0:10; 1:402/ as empirical null.

There is a lot at stake here. Based on the empirical null (15.47) only

the four circled points at the far right of Figure 15.8; the ﬁfth point had

as appropriate here. This is reinforced by a QQ plot of the zi values shown
in Figure 15.8, where we see most of the cases falling nicely along a
N .0:09; 1:422/ line, with just a few outliers at both extremes.

four ofﬁcers reached the “probably racially biased” cutoffcfdr.zi /  0:2,
cfdr D 0:38 while all the others exceeded 0.80. The theoretical N .0; 1/
null was much more severe, assigningcfdr  0:2 to the 125 ofﬁcers having
zi (cid:21) 2:50. One can imagine the difference in newspaper headlines.
From a classical point of view it seems heretical to question the theo-
retical null distribution, especially since there is no substitute available in
single-case testing. Once alerted by data sets like the police study, however,
it is easy to list reasons for doubt:
(cid:15) Asymptotics Taylor series approximations go into theoretical null calcu-
lations such as (15.46), which can lead to inaccuracies, particularly in the
crucial tails of the null distribution.
(cid:15) Correlations False-discovery rate methods are correct on the average,

z−valuesFrequency−6−4−20246050100150200N(0,1)N(0.1,1.402)288

Large-scale Hypothesis Testing and FDRs

Figure 15.8 QQ plot of police data z scores; most scores closely
follow the N .0:09; 1:422/ line with a few outliers at either end.
The circled points are cases having local false-discovery estimate

cfdr.zi /  0:2, based on the empirical null. Using the theoretical
N .0; 1/ null gives 216 cases withcfdr.zi /  0:2, 91 on the left and

125 on the right.

even with correlations among the N z-values. However, severe correlation
destabilizes the z-value histogram, which can become randomly wider or
narrower than theoretically predicted, undermining theoretical null results
for the data set at hand.
6 (cid:15) Unobserved covariates The police study was observational: individual
encounters were not assigned at random to the various ofﬁcers but simply
observed as they happened. Observed covariates such as the time of day
and the neighborhood were included in the logistic regression model, but
one can never rule out the possibility of inﬂuential unobserved covariates.
(cid:15) Effect size considerations The hypothesis-testing setup, where a large
fraction of the cases are truly null, may not be appropriate. An effect
size model, with (cid:22)i (cid:24) g.(cid:1)/ and zi (cid:24)
N .(cid:22)i ; 1/, might apply, with the
prior g.(cid:22)/ not having an atom at (cid:22) D 0. The nonatomic choice g.(cid:22)/ (cid:24)
N .0:10; 0:632/ provides a good ﬁt to the QQ plot in Figure 15.8.

*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************−3−2−10123−10−505Normal QuantilesSample Quantileslllllllllintercept = 0.089slope = 1.42415.5 Choice of the Null Distribution

289

Empirical Null Estimation

f0.z/ (cid:24)

Our point of view here is that the theoretical null (15.46), zi (cid:24)
N .0; 1/, is
not completely wrong but needs adjustment for the data set at hand. To this
end we assume the two-groups model (15.19), with f0.z/ normal but not
necessarily N .0; 1/, say

N .ı0; (cid:27) 2
0 /:

(15.48)
In order to compute the local false-discovery rate fdr.z/ D (cid:25)0f0.z/=f .z/
we want to estimate the three numerator parameters .ı0; (cid:27)0; (cid:25)0/, the mean
and standard deviation of the null density and the proportion of null cases.
(The denominator f .z/ is estimated as in Section 15.4.)
Our key assumptions (besides (15.48)) are that (cid:25)0 is large, say (cid:25)0 (cid:21)
0:90, and that most of the zi near 0 are null cases. The algorithm locfdr
begins by selecting a set A0 near z D 0 in which it is assumed that all the 7
zi in A0 are null; in terms of the two-groups model, the assumption can be
stated as
(15.49)

f1.z/ D 0 for z 2

A0:

Modest violations of (15.49), which are to be expected, produce small bi-
ases in the empirical null estimates. Maximum likelihood based on the
number and values of the zi observed in A0 yield the empirical null es-
timates .
Applied to the police data, locfdr chose A0 D Œ(cid:0)1:8; 2:0 and pro-

O
ı0; O(cid:27)0; O(cid:25)0/.

duced estimates

(cid:16)O

ı0; O(cid:27)0; O(cid:25)0

 D .0:10; 1:40; 0:989/:

(15.50)

8

Two small simulation studies described in Table 15.2 give some idea of the
variabilities and biases inherent in the locfdr estimation process.

The third method, somewhere between the theoretical and empirical null
estimates but closer to the former, relies on permutations. The vector z of
6033 z-values for the prostate data of Figure 15.1 was obtained from a
study of 102 men, 52 cancer patients and 50 controls. Randomly permuting
the men’s data, that is randomly choosing 50 of the 102 to be “controls” and
the remaining 52 to be “patients,” and then carrying through steps (15.1)–
(cid:3) in which any actual cancer/control differences have
(15.2) gives a vector z
(cid:3)
been suppressed. A histogram of the z
i values (perhaps combining sev-
eral permutations) provides the “permutation null.” Here we are extending
Fisher’s original permutation idea, Section 4.4, to large-scale testing.

Ten permutations of the prostate study data produced an almost perfect

290

Large-scale Hypothesis Testing and FDRs

O
ı0; O(cid:27)0; O(cid:25)0/ for two

Table 15.2 Means and standard deviations of .
simulation studies of empirical null estimation using locfdr. N D 5000
cases each trial with .ı0; (cid:27)0; (cid:25)0/ as shown; 250 trials; two-groups model
(15.19) with non-null density f1.z/ equal to N .3; 1/ (left side) or
N .4:2; 1/ (right side).

ı0
0
.015
.019

(cid:27)0
1.0
1.017
.017

(cid:25)0
.95
.962
.005

ı0
.10
.114
.025

(cid:27)0
1.40
1.418
.029

(cid:25)0
.95
.958
.006

true
mean
st dev

N .0; 1/ permutation null. (This is as expected from the classic theory of
permutation t-tests.) Permutation methods reliably overcome objection 1
to the theoretical null distribution, over-reliance on asymptotic approxima-
tions, but cannot cure objections 2, 3, and 4.

9

Whatever the cause of disparity, the operational difference between the
theoretical and empirical null distribution is clear: with the latter, the sig-
niﬁcance of an outlying case is judged relative to the dispersion of the
majority, not by a theoretical yardstick as with the former. This was per-
suasive for the police data, but the story isn’t one-sided. Estimating the null

distribution adds substantially to the variability ofcfdr orcFdr. For situations

such as the prostate data, when the theoretical null looks nearly correct,12
it is reasonable to stick with it.

The very large data sets of twenty-ﬁrst-century applications encourage
self-contained methodology that proceeds from just the data at hand using
a minimum of theoretical constructs. False-discovery rate empirical Bayes
analysis of large-scale testing problems, with data-based estimation of O(cid:25)0,
O
f0, and O

f , comes close to the ideal in this sense.

15.6 Relevance

False-discovery rates return us to the purview of indirect evidence, Sec-
tions 6.4 and 7.4. Our interest in any one gene in the prostate cancer study
depends on its own z score of course, but also on the other genes’ scores—
“learning from the experience of others,” in the language used before.

The crucial question we have been avoiding is “Which others?” Our tacit
answer has been “All the cases that arrive in the same data set,” all the genes
12 The locfdr algorithm gave .Oı0; O(cid:27)0; O(cid:25)0/ D .0:00; 1:06; 0:984/ for the prostate data.

15.6 Relevance

291

in the prostate study, all the ofﬁcers in the police study. Why this can be a
dangerous tactic is shown in our ﬁnal example.
A DTI (diffusion tensor imaging) study compared six dyslexic children
with six normal controls. Each DTI scan recorded ﬂuid ﬂows at N D15,443
“voxels,” i.e., at 15,443 three-dimensional brain coordinates. A score zi
comparing dyslexics with normal controls was calculated for each voxel i,
calibrated such that the theoretical null distribution of “no difference” was

H0i W zi (cid:24)

N .0; 1/

(15.51)

as at (15.3).

Figure 15.9 Histogram of z scores for the DTI study, comparing
dyslexic versus normal control children at 15,443 brain locations.
A FDR analysis based on the empirical null distribution gave 149

voxels withcfdr.zi /  0:20, those having zi (cid:21) 3:17 (indicated by

red dashes).

Figure 15.9 shows the histogram of all 15,443 zi values, normal-looking
near the center and with a heavy right tail; locfdr gave empirical null
parameters

 D .(cid:0)0:12; 1:06; 0:984/;

(cid:16)O

ı0; O(cid:27)0; O(cid:25)0

the 149 voxels with zi (cid:21) 3:17 havingcfdr values  0:20. Using the the-

(15.52)

 z−scoreFrequency−4−2024020040060080010003.17292

Large-scale Hypothesis Testing and FDRs

voxels with zi (cid:21) 3:07 havingcfdri  0:20.

oretical null (15.51) yielded only modestly different results, now the 177

Figure 15.10 A plot of 15,443 zi scores from a DTI study
(vertical axis) and voxel distances xi from the back of the brain
(horizontal axis). The starred points are the 149 voxels with

cfdr.zi /  0:20, which occur mostly for xi in the interval Œ50; 70.

In Figure 15.10 the voxel scores zi, graphed vertically, are plotted ver-
sus xi, the voxel’s distance from the back of the brain. Waves of differing
response are apparent. Larger values occur in the interval 50  x  70,
up. Most of the 149 voxels havingcfdri  0:20 occur at the top of this wave.
where the entire z-value distribution—low, medium, and high—is pushed

(15.53)

Figure 15.10 raises the problem of fair comparison. Perhaps the 4,653
voxels with xi between 50 and 70 should be compared only with each other,
and not with all 15,443 cases. Doing so gave

 D .0:23; 1:18; 0:970/;
only 66 voxels havingcfdri  0:20, those with zi (cid:21) 3:57.

ı0; O(cid:27)0; O(cid:25)0

(cid:16)O

All of this is a question of relevance: which other voxels i are relevant
to the assessment of signiﬁcance for voxel i0? One might argue that this is
a question for the scientist who gathers the data and not for the statistical
analyst, but that is unlikely to be a fruitful avenue, at least not without

20406080−2024Distance xZ scores16%ile84%ilemedian*****************************************************************************************************************************************************15.6 Relevance

293

a lot of back-and-forth collaboration. Standard Bayesian analysis solves
the problem by dictate: the assertion of a prior is also an assertion of its
relevance. Empirical Bayes situations expose the dangers lurking in such
assertions.

Relevance was touched upon in Section 7.4, where the limited transla-
tion rule (7.47) was designed to protect extreme cases from being shrunk
too far toward the bulk of ordinary ones. One could imagine having a “rel-
evance function” (cid:26).xi ; zi / that, given the covariate information xi and re-
sponse zi for casei, somehow adjusts an ensemble false-discovery rate es-
timate to correctly apply to the case of interest—but such a theory barely
exists.

10

Summary

combine frequentist and Bayesian thinking.

Large-scale testing, particularly in its false-discovery rate implementation,
is not at all the same thing as the classic Fisher–Neyman–Pearson theory:
(cid:15) Frequentist single-case hypothesis testing depends on the theoretical
long-run behavior of samples from the theoretical null distribution. With
data available from say N D 5000 simultaneous tests, the statistician
has his or her own “long run” in hand, diminishing the importance of
theoretical modeling. In particular, the data may cast doubt on the the-
oretical null, providing a more appropriate empirical null distribution in
its place.
(cid:15) Classic testing theory is purely frequentist, whereas false-discovery rates
on its own score zi, while cfdr.zi / or cFdr.zi / also depends on the ob-
(cid:15) In classic testing, the attained signiﬁcance level for case i depends only
(cid:15) Applications of single-test theory usually hope for rejection of the null
hypothesis, a familiar prescription being 0.80 power at size 0.05. The
opposite is true for large-scale testing, where the usual goal is to ac-
cept most of the null hypotheses, leaving just a few interesting cases for
further study.
(cid:15) Sharp null hypotheses such as (cid:22) D 0 are less important in large-scale
applications, where the statistician is happy to accept a hefty proportion
of uninterestingly small, but nonzero, effect sizes (cid:22)i.
(cid:15) False-discovery rate hypothesis testing involves a substantial amount of
estimation, blurring the line beteen the two main branches of statistical
inference.

served z-values for other cases.

294

Large-scale Hypothesis Testing and FDRs

15.7 Notes and Details

The story of false-discovery rates illustrates how developments in scien-
tiﬁc technology (microarrays in this case) can inﬂuence the progress of
statistical inference. A substantial theory of simultaneous inference was
developed between 1955 and 1995, mainly aimed at the frequentist control
of family-wise error rates in situations involving a small number of hypoth-
esis tests, maybe up to 20. Good references are Miller (1981) and Westfall
and Young (1993).

Benjamini and Hochberg’s seminal 1995 paper introduced false-discov-
ery rates at just the right time to catch the wave of large-scale data sets, now
involving thousands of simultaneous tests, generated by microarray appli-
cations. Most of the material in this chapter is taken from Efron (2010),
where the empirical Bayes nature of Fdr theory is emphasized. The po-
lice data is discussed and analyzed at length in Ridgeway and MacDonald
(2009).

1 [p. 272] Model (15.4). Section 7.4 of Efron (2010) discusses the following
result for the non-null distribution of z-values: a transformation such as
(15.2) that produces a z-value (i.e., a standard normal random variable z (cid:24)
N .0; 1/) under the null hypothesis gives, to a good approximation, z (cid:24)
N .(cid:22); (cid:27) 2
Student’s t with 100 degrees of freedom, (cid:27) 2
(cid:22)

(cid:22)/ under reasonable alternatives. For the speciﬁc situation in (15.2),

:D 1 as in (15.4).

2 [p. 274] Holm’s procedure. Methods of FWER control, including Holm’s
procedure, are surveyed in Chapter 3 of Efron (2010). They display a large
amount of mathematical ingenuity, and provided the background against
which FDR theory developed.

3 [p. 276] FDR control theorem. Benjamini and Hochberg’s striking control
theorem (15.15) was rederived by Storey et al. (2004) using martingale
theory. The basic idea of false discoveries, as displayed in Figure 15.2,
goes back to Soric (1989).

4 [p. 285] Formula (15.44). Integrating fdr.z/ D (cid:25)0f0.z/=f .z/ gives

E ffdr.z/jz (cid:21) z0g DZ 1

,Z 1

(cid:25)0f0.z/ dz

f .z/ dz

z0

D (cid:25)0S0.z0/=S.z0/ D Fdr.z0/:

z0

(15.54)

5 [p. 286] Thresholds for Fdr and fdr. Suppose the survival curves S0.z/ and

S1.z/ (15.20) satisfy the “Lehmann alternative” relationship

log S1.z/ D (cid:13) log S0.z/

(15.55)

15.7 Notes and Details

295

for large values of z, where (cid:13) is a positive constant less than 1. (This is a
reasonable condition for the non-null density f1.z/ to produce larger pos-
itive values of z than does the null density f0.z/.) Differentiating (15.55)
gives

(cid:25)0
(cid:25)1

f0.z/
f1.z/

D 1
(cid:13)

(cid:25)0
(cid:25)1

S0.z/
S1.z/

(15.56)
after some rearrangement. But fdr.z/ D (cid:25)0f0.z/=.(cid:25)0f0.z/ C (cid:25)1f1.z// is
algebraically equivalent to

;

fdr.z/
1 (cid:0) fdr.z/

D (cid:25)0
(cid:25)1

f0.z/
f1.z/

;

(15.57)

and similarly for Fdr.z/=.1 (cid:0) Fdr.z//, yielding
Fdr.z/
1 (cid:0) Fdr.z/

fdr.z/
1 (cid:0) fdr.z/

D 1
(cid:13)

:

(15.58)

:D Fdr.z/=(cid:13):

For large z, both fdr.z/ and Fdr.z/ go to zero, giving the asymptotic rela-
tionship

fdr.z/

(15.59)
This motivates the suggested relative thresholdscfdr.zi /  0:20 compared
If (cid:13) D 1=2 for instance, fdr.z/ will be about twice Fdr.z/ where z is large.
with cFdr.zi /  0:10.
6 [p. 288] Correlation effects. The Poisson regression method used to esti-
mate O
f .z/ in Figure 15.5 proceeds as if the components of the N -vector of
zi values z are independent. Approximation (10.54), that the kth bin count
yk P(cid:24) Poi.(cid:22)k/, requires independence. If not, it can be shown that var.yk/
increases above the Poisson value (cid:22)k as

var.yk/

:D (cid:22)k C ˛2ck:

(15.60)

24 NX

X

˛2 D

35,

Here ck is a ﬁxed constant depending on f .z/, while ˛2 is the root mean
square correlation between all pairs zi and zj ,

cov.zi ; zj /2

N.N (cid:0) 1/:

(15.61)

j¤i

iD1

Estimates likecfdr.z/ in Figure 15.5 remain nearly unbiased under correla-
tion, but their sampling variability increases as a function of ˛. Chapters 7
and 8 of Efron (2010) discuss correlation effects in detail.
Often, ˛ can be estimated. Let X be the 6033 (cid:2) 50 matrix of gene ex-
pression levels measured for the control subject in the prostate study. Rows

296

Large-scale Hypothesis Testing and FDRs

i and j provide an unbiased estimate of cor.zi ; zj /2. Modern computation
is sufﬁciently fast to evaluate all N.N (cid:0) 1/=2 pairs (though that isn’t nec-
essary, sampling is faster) from which estimate O˛ is obtained. It equaled
0:016˙ 0:001 for the control subjects, and 0:015˙ 0:001 for the 6033(cid:2) 52
matrix of the cancer patients. Correlation is not much of a worry for the
prostate study, but other microarray studies show much larger O˛ values.
Sections 6.4 and 8.3 of Efron (2010) discuss how correlations can under-
cut inferences based on the theoretical null even when it is correct for all
the null cases.

7 [p. 289] The program locfdr. Available from CRAN, this is an R pro-
gram that provides fdr and Fdr estimates, using both the theoretical and
empirical null distributions.

8 [p. 289] ML estimation of the empirical null. Let A0 be the “zero set”
(15.49), z0 the set of zi observed to be in A0, I0 their indices, and N0 the
number of zi in A0. Also deﬁne
(cid:0) 1
(cid:30)ı0;(cid:27)0 .z/ D e

2(cid:30)q

(cid:16) z(cid:0)ı0

2(cid:25)(cid:27) 2
0 ;

(cid:27)0

2

P .ı0; (cid:27)0/ DZ
A0g according to (15.48)–(15.49).) Then z0 has density
" 

and  D (cid:25)0P .ı0; (cid:27)0/:
#

(cid:30)ı0;(cid:27)0 .z/ dz

(15.62)

!

A0

(So  D Prfzi 2
and likelihood

#"Y

fı0;(cid:27)0;(cid:25)0 .z0/ D

 N0 .1 (cid:0)  /N(cid:0)N0

N
N0

(cid:30)ı0;(cid:27)0 .zi /

Pı0;(cid:27)0

I0

;

(15.63)

the ﬁrst factor being the binomial probability of seeing N0 of the zi in
A0, and the second the conditional probability of those zi falling within
O
ı0; O(cid:27)0/, while
A0. The second factor is numerically maximized to give .
O
 D N0=N is obtained from the ﬁrst, and then O(cid:25)0 D O
O
ı0; O(cid:27)0/. This is
a partial likelihood argument, as in Section 9.4; locfdr centers A0 at the
median of the N zi values, with width about twice the interquartile range
estimate of (cid:27)0.

 =P .

9 [p. 290] The permutation null. An impressive amount of theoretical effort
concerned the “permutation t-test”: in a single-test two-sample situation,
permuting the data and computing the t statistic gives, after a great many
repetitions, a histogram dependably close to that of the standard t distri-
bution; see Hoeffding (1952). This was Fisher’s justiﬁcation for using the
standard t-test on nonnormal data.

The argument cuts both ways. Permutation methods tend to recreate the

15.7 Notes and Details

297

theoretical null, even in situations like that of Figure 15.7 where it isn’t
appropriate. The difﬁculties are discussed in Section 6.5 of Efron (2010).
10 [p. 293] Relevance theory. Suppose that in the DTI example shown in Fig-
ure 15.10 we want to consider only voxels with x D 60 as relevant to an
observed zi with xi D 60. Now there may not be enough relevant cases to
how the complete-data estimatescfdr.zi / orcFdr.zi / can be efﬁciently mod-
adequately estimate fdr.zi / or Fdr.zi /. Section 10.1 of Efron (2010) shows

iﬁed to conform to this situation.

16

Sparse Modeling and the Lasso

The amount of data we are faced with keeps growing. From around the
late 1990s we started to see wide data sets, where the number of variables
far exceeds the number of observations. This was largely due to our in-
creasing ability to measure a large amount of information automatically. In
genomics, for example, we can use a high-throughput experiment to auto-
matically measure the expression of tens of thousands of genes in a sam-
ple in a short amount of time. Similarly, sequencing equipment allows us
to genotype millions of SNPs (single-nucleotide polymorphisms) cheaply
and quickly. In document retrieval and modeling, we represent a document
by the presence or count of each word in the dictionary. This easily leads to
a feature vector with 20,000 components, one for each distinct vocabulary
word, although most would be zero for a small document. If we move to
bi-grams or higher, the feature space gets really large.

In even more modest situations, we can be faced with hundreds of vari-
ables. If these variables are to be predictors in a regression or logistic re-
gression model, we probably do not want to use them all. It is likely that a
subset will do the job well, and including all the redundant variables will
degrade our ﬁt. Hence we are often interested in identifying a good subset
of variables. Note also that in these wide-data situations, even linear mod-
els are over-parametrized, so some form of reduction or regularization is
essential.

In this chapter we will discuss some of the popular methods for model
selection, starting with the time-tested and worthy forward-stepwise ap-
proach. We then look at the lasso, a popular modern method that does se-
lection and shrinkage via convex optimization. The LARs algorithm ties
these two approaches together, and leads to methods that can deliver paths
of solutions.

Finally, we discuss some connections with other modern big- and wide-

data approaches, and mention some extensions.

298

16.1 Forward Stepwise Regression

299

16.1 Forward Stepwise Regression

Stepwise procedures have been around for a very long time. They were
originally devised in times when data sets were quite modest in size, in
particular in terms of the number of variables. Originally thought of as the
poor cousins of “best-subset” selection, they had the advantage of being
much cheaper to compute (and in fact possible to compute for large p). We
will review best-subset regression ﬁrst.

i

0

Suppose we have a set of n observations on a response yi and a vec-
D .xi1; xi 2; : : : ; xip/, and we plan to ﬁt a linear
tor of p predictors x
regression model. The response could be quantitative, so we can think of
ﬁtting a linear model by least squares. It could also be binary, leading to a
linear logistic regression model ﬁt by maximum likelihood. Although we
will focus on these two cases, the same ideas transfer exactly to other gen-
eralized linear models, the Cox model, and so on. The idea is to build a
model using a subset of the variables; in fact the smallest subset that ade-
quately explains the variation in the response is what we are after, both for
inference and for prediction purposes. Suppose our loss function for ﬁtting
the linear model is L (e.g. sum of squares, negative log-likelihood). The
method of best-subset regression is simple to describe, and is given in Al-
gorithm 16.1. Step 3 is easy to state, but requires a lot of computation. For

Algorithm 16.1 BEST-SUBSET REGRESSION.
1 Start with m D 0 and the null model O0.x/ D O
ˇ0, estimated by the mean
of the yi.
2 At step m D 1, pick the single variable j that ﬁts the response best,
in terms of the loss L evaluated on the training data, in a univariate
regression O1.x/ D O
3 For each subset size m 2 f2; 3; : : : ; Mg (with M  min.n (cid:0) 1; p/)
identify the best subset Am of size m when ﬁtting a linear model
Om.x/ D O
O
ˇAm with m of the p variables, in terms of the
loss L.
4 Use some external data or other means to select the “best” amongst these

O
ˇj . Set A1 D fjg.

ˇ0 C x

ˇ0 C x

0

j

0
Am

M models.

p much larger than about 40 it becomes prohibitively expensive to perform
exactly—a so-called “N-P complete” problem because of its combinatorial
complexity (there are 2p subsets). Note that the subsets need not be nested:

Sparse Modeling and the Lasso

300
the best subset of size m D 3, say, need not include both or any of the
variables in the best subset of size m D 2.

In step 4 there are a number of methods for selecting m. Originally the
Cp criterion of Chapter 12 was proposed for this purpose. Here we will
favor K-fold cross-validation, since it is applicable to all the methods dis-
cussed in this chapter.

It is interesting to digress for a moment on how cross-validation works
here. We are using it to select the subset size m on the basis of prediction
performance (on future data). With K D 10, we divide the n training obser-
vations randomly into 10 equal size groups. Leaving out say group k D 1,
we perform steps 1–3 on the 9=10ths, and for each of the chosen models,
we summarize the prediction performance on the group-1 data. We do this
K D 10 times, each time with group k left out. We then average the 10
performance measures for each m, and select the value of m correspond-
ing to the best performance. Notice that for each m, the 10 models Om.x/
might involve different subsets of variables! This is not a concern, since we
are trying to ﬁnd a good value of m for the method. Having identiﬁed Om,
we rerun steps 1–3 on the entire training set, and deliver the chosen model
O Om.x/.
As hinted above, there are problems with best-subset regression. A pri-
mary issue is that it works exactly only for relatively small p. For example,
we cannot run it on the spam data with 57 variables (at least not in 2015 on
a Macbook Pro!). We may also think that even if we could do the compu-
tations, with such a large search space the variance of the procedure might
be too high.

Am(cid:0)1 (cid:26)

As a result, more manageable stepwise procedures were invented. For-
ward stepwise regression, Algorithm 16.2, is a simple modiﬁcation of best-
subset, with the modiﬁcation occurring in step 3. Forward stepwise re-
gression produces a nested sequence of models ; : : : (cid:26)
Am (cid:26)
AmC1 : : :. It starts with the null model, here an intercept, and adds vari-
ables one at a time. Even with large p, identifying the best variable to add
at each step is manageable, and can be distributed if clusters of machines
are available. Most importantly, it is feasible for large p. Figure 16.1 shows
the coefﬁcient proﬁles for forward-stepwise linear regression on the spam
training data. Here there are 57 input variables (relative prevalence of par-
ticular words in the document), and an “ofﬁcial” (train, test) split of (3065,
1536) observations. The response is coded as +1 if the email was spam,
else -1. The ﬁgure caption gives the details. We saw the spam data earlier,
in Table 8.3, Figure 8.7 and Figure 12.2.

Fitting the entire forward-stepwise linear regression path as in the ﬁgure

16.1 Forward Stepwise Regression

301

Algorithm 16.2 FORWARD STEPWISE REGRESSION.
1 Start with m D 0 and the null model O0.x/ D O
ˇ0, estimated by the mean
of the yi.
2 At step m D 1, pick the single variable j that ﬁts the response best,
in terms of the loss L evaluated on the training data, in a univariate
regression O1.x/ D O
3 For each subset size m 2 f2; 3; : : : ; Mg (with M  min.n (cid:0) 1; p/)
identify the variable k that when augmented with Am(cid:0)1 to form Am,
leads to the model Om.x/ D O
O
ˇAm that performs best in terms
of the loss L.
4 Use some external data or other means to select the “best” amongst these

O
ˇj . Set A1 D fjg.
ˇ0 C x

ˇ0 C x

0

j

0
Am

M models.

(when n > p) has essentially the same cost as a single least squares ﬁt on
all the variables. This is because the sequence of models can be updated
each time a variable is added.However, this is a consequence of the linear 1
model and squared-error loss.

Suppose instead we run a forward stepwise logistic regression. Here up-
dating does not work, and the entire ﬁt has to be recomputed by maximum
likelihood each time a variable is added. Identifying which variable to add
in step 3 in principle requires ﬁtting an .m C 1/-variable model p (cid:0) m
times, and seeing which one reduces the deviance the most. In practice, we
can use score tests which are much cheaper to evaluate.  These amount 2
to using the quadratic approximation to the log-likelihood from the ﬁnal
iteratively reweighted least-squares (IRLS) iteration for ﬁtting the model
with m terms. The score test for a variable not in the model is equivalent
to testing for the inclusion of this variable in the weighted least-squares ﬁt.
Hence identifying the next variable is almost back to the previous cases,
requiring p (cid:0) m simple regression updates.  Figure 16.2 shows the test 3
misclassiﬁcation error for forward-stepwise linear regression and logistic
regression on the spam data, as a function of the number of steps. They
both level off at around 25 steps, and have a similar shape. However, the
logistic regression gives more accurate classiﬁcations.1

Although forward-stepwise methods are possible for large p, they get
tedious for very large p (in the thousands), especially if the data could sup-

1 For this example we can halve the gap between the curves by optimizing the prediction

threshold for linear regression.

302

Sparse Modeling and the Lasso

Figure 16.1 Forward stepwise linear regression on the spam
data. Each curve corresponds to a particular variable, and shows
the progression of its coefﬁcient as the model grows. These are
plotted against the training R2, and the vertical gray bars
correspond to each step. Starting at the left at step 1, the ﬁrst
selected variable explains R2 D 0:16; adding the second increases
R2 to 0:25, etc. What we see is that early steps have a big impact
on the R2, while later steps hardly have any at all. The vertical
black line corresponds to step 25 (see Figure 16.2), and we see
that after that the step-wise improvements in R2 are negligible.

port a model with many variables. However, if the ideal active set is fairly
small, even with many thousands of variables forward-stepwise selection
is a viable option.

Forward-stepwise selection delivers a sequence of models, as seen in
the previous ﬁgures. One would generally want to select a single model,
and as discussed earlier, we often use cross-validation for this purpose.
Figure 16.3 illustrates using stepwise linear regression on the spam data.
Here the sequence of models are ﬁt using squared-error loss on the bi-
nary response variable. However, cross-validation scores each model for
misclassiﬁcation error, the ultimate goal of this modeling exercise. This
highlights one of the advantages of cross-validation in this context. A con-

0.00.10.20.30.40.50.6−0.20.00.20.40.60.8Forward−Stepwise RegressionR2 on Training DataCoefficients16.2 The Lasso

303

Figure 16.2 Forward-stepwise regression on the spam data.
Shown is the misclassiﬁcation error on the test data, as a function
of the number of steps. The brown dots correspond to linear
regression, with the response coded as -1 and +1; a prediction
greater than zero is classiﬁed as +1, one less than zero as -1. The
blue dots correspond to logistic regression, which performs better.
We see that both curves essentially reach their minima after 25
steps.

venient (differentiable and smooth) loss function is used to ﬁt the sequence
of models. However, we can use any performance measure to evaluate the
sequence of models; here misclassiﬁcation error is used. In terms of the
parameters of the linear model, misclassiﬁcation error would be a difﬁcult
and discontinuous loss function to use for parameter estimation. All we
need to use it for here is pick the best model size. There appears to be little
beneﬁt in going beyond 25–30 terms.

16.2 The Lasso

The stepwise model-selection methods of the previous section are useful
if we anticipate a model using a relatively small number of variables, even
if the pool of available variables is very large. If we expect a moderate
number of variables to play a role, these methods become cumbersome.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllll01020304050600.00.10.20.30.4Spam DataStepTest Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward−Stepwise Linear RegressionForward−Stepwise Logistic Regression304

Sparse Modeling and the Lasso

Figure 16.3 Ten-fold cross-validated misclassiﬁcation errors
(green) for forward-stepwise regression on the spam data, as a
function of the step number. Since each error is an average of 10
numbers, we can compute a (crude) standard error; included in
the plot are pointwise standard-error bands. The brown curve is
the misclassiﬁcation error on the test data.

Another black mark against forward-stepwise methods is that the sequence
of models is derived in a greedy fashion, without any claimed optimality.
The methods we describe here are derived from a more principled proce-
dure; indeed they solve a convex optimization, as deﬁned below.

We will ﬁrst present the lasso for squared-error loss, and then the more

i ˇ/2 subject to kˇk1  t;
0

general case later. Consider the constrained linear regression problem

where kˇk1 DPp

nX
.yi (cid:0) ˇ0 (cid:0) x
(16.1)
iD1
jˇjj, the `1 norm of the coefﬁcient vector. Since both
jD1
the loss and the constraint are convex in ˇ, this is a convex optimization
problem, and it is known as the lasso. The constraint kˇk1  t restricts the
coefﬁcients of the model by pulling them toward zero; this has the effect
of reducing their variance, and prevents overﬁtting. Ridge regression is an
earlier great uncle of the lasso, and solves a similar problem to (16.1), ex-

minimize
ˇ02R; ˇ2Rp

1

n

llllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.00.10.20.30.4Spam DataStepTest and CV Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllTest Error10−fold CV Error16.2 The Lasso

305

Figure 16.4 An example with ˇ 2 R2 to illustrate the difference
between ridge regression and the lasso. In both plots, the red
contours correspond to the squared-error loss function, with the
unrestricted least-squares estimate O
regions show the constraints, with the lasso on the left and ridge
on the right. The solution to the constrained problem corresponds
to the value of ˇ where the expanding loss contours ﬁrst touch the
constraint region. Due to the shape of the lasso constraint, this
will often be at a corner (or an edge more generally), as here,
which means in this case that the minimizing ˇ has ˇ1 D 0. For
the ridge constraint, this is unlikely to happen.

ˇ in the center. The blue

cept the constraint is kˇk2  t; ridge regression bounds the quadratic `2
norm of the coefﬁcient vector. It also has the effect of pulling the coefﬁ-
cients toward zero, in an apparently very similar way. Ridge regression is
discussed in Section 7.3.2 Both the lasso and ridge regression are shrinkage
methods, in the spirit of the James–Stein estimator of Chapter 7.

A big difference, however, is that for the lasso, the solution typically has
many of the ˇj equal to zero, while for ridge they are all nonzero. Hence
the lasso does variable selection and shrinkage, while ridge only shrinks.
Figure 16.4 illustrates this for ˇ 2 R2. In higher dimensions, the `1 norm
has sharp edges and corners, which correspond to coefﬁcient estimates zero
in ˇ.

Since the constraint in the lasso treats all the coefﬁcients equally, it usu-
ally makes sense for all the elements of x to be in the same units. If not, we

2 Here we use the “bound” form of ridge regression, while in Section 7.3 we use the

“Lagrange” form. They are equivalent, in that for every “Lagrange” solution, there is a
corresponding bound solution.

β^β^2..β1β2β1β306

Sparse Modeling and the Lasso

typically standardize the predictors beforehand so that each has variance
one.
Two natural boundary values for t in (16.1) are t D 0 and t D 1.
The former corresponds to the constant model (the ﬁt is the mean of the
yi,)3 and the latter corresponds to the unrestricted least-squares ﬁt. In fact,
if n > p, and O
ˇ is the least-squares estimate, then we can replace 1 by
k O
ˇk1, and any value of t (cid:21) k O
ˇk1 is a non-binding constraint. Figure 16.5

4

Figure 16.5 The lasso linear regression regularization path on the
spam data. Each curve corresponds to a particular variable, and
shows the progression of its coefﬁcient as the regularization
bound t grows. These curves are plotted against the training R2
rather than t, to make the curves comparable with the
forward-stepwise curves in Figure 16.1. Some values of t are
indicated at the top. The vertical gray bars indicate changes in the
active set of nonzero coefﬁcients, typically an inclusion. Here we
see clearly the role of the `1 penalty; as t is relaxed, coefﬁcients
become nonzero, but in a smoother fashion than in forward
stepwise.

shows the regularization path4 for the lasso linear regression problem on

3 We typically do not restrict the intercept in the model.
4 Also known as the homotopy path.

0.00.10.20.30.40.50.6−0.20.00.20.4Lasso RegressionR2 on Training DataCoefficients0.000.060.201.052.453.4716.2 The Lasso

307

the spam data; that is, the solution path for all values of t. This can be com-
puted exactly, as we will see in Section 16.4, because the coefﬁcient pro-
ﬁles are piecewise linear in t. It is natural to compare this coefﬁcient proﬁle
with the analogous one in Figure 16.1 for forward-stepwise regression. Be-
cause of the control of k O
ˇ.t /k1, we don’t see the same range as in forward
stepwise, and observe somewhat smoother behavior. Figure 16.6 contrasts

Figure 16.6 Lasso versus forward-stepwise regression on the
spam data. Shown is the misclassiﬁcation error on the test data,
as a function of the number of variables in the model. Linear
regression is coded brown, logistic regression blue; hollow dots
forward stepwise, solid dots lasso. In this case it appears stepwise
and lasso achieve the same performance, but lasso takes longer to
get there, because of the shrinkage.

the prediction performance on the spam data for lasso regularized models
(linear regression and logistic regression) versus forward-stepwise models.
The results are rather similar at the end of the path; here forward stepwise
can achieve classiﬁcation performance similar to that of lasso regularized
logistic regression with about half the terms. Lasso logistic regression (and
indeed any likelihood-based linear model) is ﬁt by penalized maximum

llllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.00.10.20.30.4Spam DataStepTest Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward−Stepwise Linear RegressionForward−Stepwise Logistic RegressionLasso Linear RegressionLasso Logistic RegressionSparse Modeling and the Lasso

308

likelihood:

minimize
ˇ02R; ˇ2Rp

1

n

nX

iD1

L.yi ; ˇ0 C ˇ

0

xi / subject to kˇk1  t:

(16.2)

Here L is the negative of the log-likelihood function for the response dis-
tribution.

16.3 Fitting Lasso Models

The lasso objectives (16.1) or (16.2) are differentiable and convex in ˇ and
ˇ0, and the constraint is convex in ˇ. Hence solving these problems is a
convex optimization problem, for which standard packages are available.
It turns out these problems have special structure that can be exploited
to yield efﬁcient algorithms for ﬁtting the entire path of solutions as in
Figures 16.1 and 16.5. We will start with problem (16.1), which we rewrite
in the more convenient Lagrange form:

minimize

ˇ2Rp

1

2n

ky (cid:0) X ˇk2 C (cid:21)kˇk1:

(16.3)

Here we have centered y and the columns of X beforehand, and hence
the intercept has been omitted. The Lagrange and constraint versions are
equivalent, in the sense that any solution O
ˇ.(cid:21)/ to (16.3) with (cid:21) (cid:21) 0 corre-
sponds to a solution to (16.1) with t D k O
ˇ.(cid:21)/k1. Here large values of (cid:21) will
encourage solutions with small `1 norm coefﬁcient vectors, and vice-versa;
(cid:21) D 0 corresponds to the ordinary least squares ﬁt.

0

The solution to (16.3) satisﬁes the subgradient condition
j D 1; : : : ; p;

O
ˇi C (cid:21)sj D 0;

hxj ; y (cid:0) X
O
ˇj /; j D 1; : : : ; p. This notation means sj D sign.

(cid:0) 1
(16.4)
n
O
where sj 2 sign.
ˇj /
if O
ˇj ¤ 0, and sj 2 Œ(cid:0)1; 1 if O
ˇj D 0.) We use the inner-product notation
ha; bi D a
b in (16.4), which leads to more evocative expressions. These
subgradient conditions are the modern way of characterizing solutions to
problems of this kind, and are equivalent to the Karush–Kuhn–Tucker op-
timality conditions. From these conditions we can immediately learn some
properties of a lasso solution.
(cid:15) 1

O
ˇij D (cid:21) for all members of the active set; i.e., each of the
jhxj ; y (cid:0) X
n
variables in the model (with nonzero coefﬁcient) has the same covari-
ance with the residuals (in absolute value).

(cid:15) 1

16.4 Least-Angle Regression

309
O
jhxk; y (cid:0) X
ˇij  (cid:21) for all variables not in the active set (i.e. with
n
coefﬁcients zero).
These conditions are interesting and have a big impact on computation.
Suppose we have the solution O
ˇ.(cid:21)1/ at (cid:21)1, and we decrease (cid:21) by a small
amount to (cid:21)2 < (cid:21)1. The coefﬁcients and hence the residuals change, in
such a way that the covariances all remain tied at the smaller value (cid:21)2. If
in the process the active set has not changed, and nor have the signs of
their coefﬁcients, then we get an important consequence: O
ˇ.(cid:21)/ is linear for
(cid:21) 2 Œ(cid:21)2; (cid:21)1. To see this, suppose A indexes the active set, which is the
same at (cid:21)1 and (cid:21)2, and let sA be the constant sign vector. Then we have

0
0

A.y (cid:0) X
A.y (cid:0) X

X

X

O
ˇ.(cid:21)1// D nsA(cid:21)1;
O
ˇ.(cid:21)2// D nsA(cid:21)2:

By subtracting and solving we get

(cid:0)1sA;

O
ˇA.(cid:21)2/ (cid:0) O

0
AXA/

ˇA.(cid:21)1/ D n.(cid:21)1 (cid:0) (cid:21)2/.X

(16.5)
and the remaining coefﬁcients (with indices not in A) are all zero. This
shows that the full coefﬁcient vector O
ˇ.(cid:21)/ is linear for (cid:21) 2 Œ(cid:21)2; (cid:21)1. In fact,
the coefﬁcient proﬁles for the lasso are continuous and piecewise linear
over the entire range of (cid:21), with knots occurring whenever the active set
changes, or the signs of the coefﬁcients change.
Another consequence is that we can easily determine (cid:21)max, the smallest
value for (cid:21) such that the solution O
ˇ.(cid:21)max/ D 0. From (16.4) this can be
jhxj ; yij.
seen to be (cid:21)max D maxj
These two facts plus a few more details enable us to compute the exact
solution path for the squared-error-loss lasso; that is the topic of the next
section.

1
n

16.4 Least-Angle Regression
We have just seen that the lasso coefﬁcient proﬁle O
ˇ.(cid:21)/ is piecewise lin-
ear in (cid:21), and that the elements of the active set are tied in their absolute
O
covariance with the residuals. With r.(cid:21)/ D y (cid:0) X
ˇ.(cid:21)/, the covariance be-
tween xj and the evolving residual is cj .(cid:21)/ D 1
jhxj ; r.(cid:21)/ij. Hence these
also change in a piecewise linear fashion, with cj .(cid:21)/ D (cid:21) for j 2
A, and
cj .(cid:21)/  (cid:21) for j 62
A. This inspires the Least-Angle Regression algorithm,
given in Algorithm 16.3, which exploits this linearity to ﬁt the entire lasso
regularization path.

n

310

Sparse Modeling and the Lasso

Algorithm 16.3 LEAST-ANGLE REGRESSION.

0
A

0
A

.X

XA/

D fjg, and XA,

1 Standardize the predictors to have mean zero and unit `2 norm. Start
with the residual r0 D y (cid:0) Ny, ˇ0 D .ˇ1; ˇ2; : : : ; ˇp/ D 0.
2 Find the predictor xj most correlated with r0; i.e., with largest value for
jhxj ; r0ij. Call this value (cid:21)0, deﬁne the active set A
1
n
the matrix consisting of this single variable.
3 For k D 1; 2; : : : ; K D min.n (cid:0) 1; p/ do:
(a) Deﬁne the least-squares direction ı D 1
n(cid:21)k(cid:0)1

rk(cid:0)1, and
D ı, and the remaining elements
deﬁne the p-vector  such that A
are zero.
(b) Move the coefﬁcients ˇ from ˇk(cid:0)1 in the direction  toward their
least-squares solution on XA: ˇ.(cid:21)/ D ˇk(cid:0)1 C .(cid:21)k(cid:0)1 (cid:0) (cid:21)/ for
0 < (cid:21)  (cid:21)k(cid:0)1, keeping track of the evolving residuals r.(cid:21)/ D
y (cid:0) X ˇ.(cid:21)/ D rk(cid:0)1 (cid:0) .(cid:21)k(cid:0)1 (cid:0) (cid:21)/XAı.
jhx`; r.(cid:21)/ij for ` …

(cid:0)1X

(c) Keeping track of 1
n

A, identify the largest value of
(cid:21) at which a variable “catches up” with the active set; if the variable
jhx`; r.(cid:21)/ij D (cid:21). This deﬁnes the next
has index `, that means 1
n
“knot” (cid:21)k.
[ `, ˇk D ˇ.(cid:21)k/ D ˇk(cid:0)1 C .(cid:21)k(cid:0)1 (cid:0) (cid:21)k/, and rk D
D
A
y (cid:0) X ˇk.

(d) Set A
4 Return the sequence f(cid:21)k; ˇkgK
0 .

In step 3(a) ı D .X

0
A

(cid:0)1sA as in (16.5). We can think of the LAR al-

XA/

gorithm as a democratic version of forward-stepwise regression. In forward-
stepwise regression, we identify the variable that will improve the ﬁt the
most, and then move all the coefﬁcients toward the new least-squares ﬁt.
As described in endnotes 1 and 3, this is sometimes done by computing
the inner products of each (unadjusted) variable with the residual, and pick-
ing the largest in absolute value. In step 3 of Algorithm 16.3, we move the
coefﬁcients for the variables in the active set A toward their least-squares
ﬁt (keeping their inner products tied), but stop when a variable not in A
catches up in inner product. At that point, it is invited into the club, and the
process continues.

Step 3(c) can be performed efﬁciently because of the linearity of the
evolving inner products; for each variable not in A, we can determine ex-
actly when (in (cid:21) time) it would catch up, and hence which catches up ﬁrst
and when. Since the path is piecewise linear, and we know the slopes, this

16.4 Least-Angle Regression

311

Figure 16.7 Covariance evolution on the spam data. As
variables tie for maximal covariance, they become part of the
active set. These occasions are indicated by the vertical gray bars,
again plotted against the training R2 as in Figure 16.5.

means we know the path exactly without further computation between (cid:21)k(cid:0)1
and the newly found (cid:21)k.

The name “least-angle regression” derives from the fact that in step 3(b)

the ﬁtted vector evolves in the direction X  D XAı, and its inner product
XAı D sA. Since all the columns

with each active vector is given by X
of X have unit norm, this means the angles between each active vector and
the evolving ﬁtted vector are equal and hence minimal.

0
A

The main computational burden in Algorithm 16.3 is in step 3(a), com-
puting the new direction, each time the active set is updated. However, this
is easily performed using standard updating of a QR decomposition, and
hence the computations for the entire path are of the same order as that of
a single least-squares ﬁt using all the variables.

The vertical gray lines in Figure 16.5 show when the active set changes.
We see the slopes change at each of these transitions. Compare with the
corresponding Figure 16.1 for forward-stepwise regression.

Figure 16.7 shows the the decreasing covariance during the steps of the

0.00.10.20.30.40.50.60.00.10.20.30.4R2 on Training DataCovariance with Residuals312

Sparse Modeling and the Lasso

LAR algorithm. As each variable joins the active set, the covariances be-
come tied. At the end of the path, the covariances are all zero, because this
is the unregularized ordinary least-squares solution.

It turns out that the LAR algorithm is not quite the lasso path; variables
can drop out of the active set as the path evolves. This happens when a coef-
ﬁcient curve passes through zero. The subgradient equations (16.4) imply
that the sign of each active coefﬁcient matches the sign of the gradient.
However, a simple addition to step 3(c) in Algorithm 16.3 takes care of the
issue:

3(c)+ lasso modiﬁcation: If a nonzero coefﬁcient crosses zero before the
next variable enters, drop it from A and recompute the joint least-squares
direction  using the reduced set.

Figure 16.5 was computed using the lars package in R, with the lasso
option set to accommodate step 3(c)+; in this instance there was no need for
dropping. Dropping tends to occur when some of the variables are highly
correlated.

Lasso and Degrees of Freedom

We see in Figure 16.6 (left panel) that forward-stepwise regression is more
aggressive than the lasso, in that it brings down the training MSE faster.
We can use the covariance formula for df from Chapter 12 to quantify the
amount of ﬁtting at each step.

In the right panel we show the results of a simulation for estimating the
df of forward-stepwise regression and the lasso for the spam data. Recall
the covariance formula

cov.yi ; Oyi /:

(16.6)

nX

iD1

df D 1

(cid:27) 2

These covariances are of course with respect to the sampling distribution of
the yi, which we do not have access to since these are real data. So instead
we simulate from ﬁtted values from the full least-squares ﬁt, by adding
Gaussian errors with the appropriate (estimated) standard deviation. (This
is the parametric bootstrap calculation (12.64).)

It turns out that each step of the LAR algorithm spends one df, as is
evidenced by the brown curve in the right plot of Figure 16.8. Forward
stepwise spends more df in the earlier stages, and can be erratic.

Under some technical conditions on the X matrix (that guarantee that

16.5 Fitting Generalized Lasso Models

313

Figure 16.8 Left: Training mean-squared error (MSE) on the
spam data, for forward-stepwise regression and the lasso, as a
function of the size of the active set. Forward stepwise is more
aggressive than the lasso, in that it (over-)ﬁts the training data
more quickly. Right: Simulation showing the degrees of freedom
or df of forward-stepwise regression versus lasso. The lasso uses
one df per step, while forward stepwise is greedier and uses more,
especially in the early steps. Since these df were computed using
5000 random simulated data sets, we include standard-error bands
on the estimates.

LAR delivers the lasso path), one can show that the df is exactly one per

step. More generally, for the lasso, if we deﬁnecdf .(cid:21)/ D j
of the active set at (cid:21)), we have that EŒcdf .(cid:21)/ D df .(cid:21)/. In other words, the
A.(cid:21)/j (the size

size of the active set is an unbiased estimate of df.

Ordinary least squares with a predetermined sequence of variables spends
one df per variable. Intuitively forward stepwise spends more, because it
pays a price (in some extra df) for searching.  Although the lasso does 5
search for the next variable, it does not ﬁt the new model all the way, but
just until the next variable enters. At this point, one new df has been spent.

16.5 Fitting Generalized Lasso Models

So far we have focused on the lasso for squared-error loss, and exploited
the piecewise-linearity of its coefﬁcient proﬁle to efﬁciently compute the
entire path. Unfortunately this is not the case for most other loss functions,

llllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.40.50.60.70.80.9Spam Training DataStepTraining MSEllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward−StepwiseLasso010203040500102030405060StepDfllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward StepwiseLasso314

Sparse Modeling and the Lasso

so obtaining the coefﬁcient path is potentially more costly. As a case in
point, we will use logistic regression as an example; in this case in (16.2) L
represents the negative binomial log-likelihood. Writing the loss explicitly
and using the Lagrange form for the penalty, we wish to solve

nX

#
yi log (cid:22)i C .1 (cid:0) yi / log.1 (cid:0) (cid:22)i /

minimize
ˇ02R; ˇ2Rp
Here we assume the yi 2 f0; 1g and (cid:22)i are the ﬁtted probabilities

iD1

n

C (cid:21)kˇk1: (16.7)

"

(cid:0)

1

(cid:22)i D eˇ0Cx
1 C eˇ0Cx

0
i ˇ

:

0
i ˇ

(16.8)

Similar to (16.4), the solution satisﬁes the subgradient condition

1

hxj ; y (cid:0) (cid:22)i (cid:0) (cid:21)sj D 0;

j D 1; : : : ; p;

n

(16.9)
where sj 2 sign.ˇj /; j D 1; : : : ; p, and (cid:22)
0 D .(cid:22)1; : : : ; (cid:22)n/.5 How-
ever, the nonlinearity of (cid:22)i in ˇj results in piecewise nonlinear coefﬁcient
proﬁles. Instead we settle for a solution path on a sufﬁciently ﬁne grid of
values for (cid:21). It is once again easy to see that the largest value of (cid:21) we need
consider is

(cid:21)max D max

jhxj ; y (cid:0) Ny1ij;

j

(16.10)
since this is the smallest value of (cid:21) for which O
ˇ0 D logit. Ny/. A
reasonable sequence is 100 values (cid:21)1 > (cid:21)2 > : : : > (cid:21)100 equally spaced
on the log-scale from (cid:21)max down to (cid:15)(cid:21)max, where (cid:15) is some small fraction
such as 0:001.

ˇ D 0, and O

An approach that has proven to be surprisingly efﬁcient is path-wise

coordinate descent.
(cid:15) For each value (cid:21)k, solve the lasso problem for one ˇj only, holding all
the others ﬁxed. Cycle around until the estimates stabilize.
(cid:15) By starting at (cid:21)1, where all the parameters are zero, we use warm starts
in computing the solutions at the decreasing sequence of (cid:21) values. The
warm starts provide excellent initializations for the sequence of solutions
O
ˇ.(cid:21)k/.
(cid:15) The active set grows slowly as (cid:21) decreases. Computational hedges that
guess the active set prove to be particularly efﬁcient. If the guess is good
(and correct), one iterates coordinate descent using only those variables,

Pn
iD1 yi D 1

n

Pn

iD1 (cid:22)i .

5 The equation for the intercept is 1
n

16.5 Fitting Generalized Lasso Models

315

until convergence. One more sweep through all the variables conﬁrms
the hunch.

The R package glmnet employs a proximal-Newton strategy at each

value (cid:21)k.
1 Compute a weighted least squares (quadratic) approximation to the log-
likelihood L at the current estimate for the solution vector O
ˇ.(cid:21)k/; This
produces a working response and observation weights, as in a regular
GLM.

2 Solve the weighted least-squares lasso at (cid:21)k by coordinate descent, using

warm starts and active-set iterations.

We now give some details, which illustrate why these particular strate-

gies are effective. Consider the weighted least-squares problem

1

wi .zi (cid:0) ˇ0 (cid:0) x

i ˇ/2 C (cid:21)kˇk1;
0

ˇj

minimize

(16.11)
P
with all but ˇj ﬁxed at their current values. Writing ri D zi (cid:0) ˇ0 (cid:0)

`¤j xi `ˇ`, we can recast (16.11) as

iD1

2n

wi .ri (cid:0) xij ˇj /2 C (cid:21)jˇjj;

(16.12)

nX

nX

1

2n

iD1

minimize

ˇj

nX

iD1

1

n

a one-dimensional problem. The subgradient equation is

wi xij .ri (cid:0) xij ˇj / (cid:0) (cid:21) (cid:1) sign.ˇj / D 0:

(16.13)

The simplest form of the solution occurs if each variable is standardized to
have weighted mean zero and variance one, and the weights sum to one; in
that case we have a two-step solution.

1 Compute the weighted simple least-squares coefﬁcient

Q

ˇj D hxj ; riw D nX
(
ˇj to produce O
2 Soft-threshold Q
ˇj :
O
ˇj D

iD1

0
sign.

ˇj /.j Q
Q

if j Q

ˇj < (cid:21)I
ˇjj (cid:0) (cid:21)/ otherwise:

(16.15)

wi xij ri :

(16.14)

316

Sparse Modeling and the Lasso

Without the standardization, the solution is almost as simple but less
intuitive.

Hence each coordinate-descent update essentially requires an inner prod-
uct, followed by the soft thresholding operation. This is especially conve-
nient for xij that are stored in sparse-matrix format, since then the inner
products need only visit the nonzero values. If the coefﬁcient is zero be-
fore the step, and remains zero, one just moves on, otherwise the model is
updated.
Moving from the solution at (cid:21)k (for which jhxj ; riwj D (cid:21)k for all the
nonzero coefﬁcients O
ˇj ), down to the smaller (cid:21)kC1, one might expect all
variables for which jhxj ; riwj (cid:21) (cid:21)kC1 would be natural candidates for the
new active set. The strong rules lower the bar somewhat, and include any
variables for which jhxj ; riwj (cid:21) (cid:21)kC1 (cid:0) .(cid:21)k (cid:0) (cid:21)kC1/; this tends to rarely
make mistakes, and still leads to considerable computational savings.

Apart from variations in the loss function, other penalties are of interest
as well. In particular, the elastic net penalty bridges the gap between the
lasso and ridge regression. That penalty is deﬁned as
C ˛kˇk1;

.1 (cid:0) ˛/kˇk2

(16.16)

P˛.ˇ/ D 1

2

2

where the factor 1=2 in the ﬁrst term is for mathematical convenience.
When the predictors are excessively correlated, the lasso performs some-
what poorly, since it has difﬁculty in choosing among the correlated cousins.
Like ridge regression, the elastic net shrinks the coefﬁcients of correlated
variables toward each other, and tends to select correlated variables in
groups. In this case the co-ordinate descent update is almost as simple as
in (16.15)

(

O
ˇj D

0
sign. Qˇj /.j Qˇjj(cid:0)˛(cid:21)/

1C.1(cid:0)˛/(cid:21)

if j Q
ˇj < ˛(cid:21)I
otherwise;

(16.17)

again assuming the observations have weighted variance equal to one. When
˛ D 0, the update corresponds to a coordinate update for ridge regression.
Figure 16.9 compares lasso with forward-stepwise logistic regression on
the spam data, here using all binarized variables and their pairwise interac-
tions. This amounts to 3061 variables in all, once degenerate variables have
been excised. Forward stepwise takes a long time to run, since it enters one
variable at a time, and after each one has been selected, a new GLM must
be ﬁt. The lasso path, as ﬁt by glmnet, includes many new variables at
each step ((cid:21)k), and is extremely fast (6 s for the entire path). For very large

16.6 Post-Selection Inference for the Lasso

317

Figure 16.9 Test misclassiﬁcation error for lasso versus
forward-stepwise logistic regression on the spam data, where we
consider pairwise interactions as well as main effects (3061
predictors in all). Here the minimum error for lasso is 0:057
versus 0:064 for stepwise logistic regression, and 0:071 for the
main-effects-only lasso logistic regression model. The stepwise
models went up to 134 variables before encountering convergence
issues, while the lasso had a largest active set of size 682.

and wide modern data sets (millions of examples and millions of variables),
the lasso path algorithm is feasible and attractive.

16.6 Post-Selection Inference for the Lasso

This chapter is mostly about building interpretable models for prediction,
with little attention paid to inference; indeed, inference is generally difﬁcult
for adaptively selected models.

(cid:21), which ends up selecting a subset A of size j
A

Suppose we have ﬁt a lasso regression model with a particular value for
j D k of the p avail-
able variables. The question arises as to whether we can assign p-values
to these selected variables, and produce conﬁdence intervals for their co-
efﬁcients. A recent burst of research activity has made progress on these
important problems. We give a very brief survey here, with references ap-

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.10.20.30.4Spam Data with InteractionsPercentage NULL Deviance Explained on Training DataTest Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward−Stepwise Logistic RegressionLasso Logistic Regression318

Sparse Modeling and the Lasso

6

pearing in the notes.  We discuss post-selection inference more generally
in Chapter 20.

One question that arises is whether we are interested in making infer-
ences about the population regression parameters using the full set of p
predictors, or whether interest is restricted to the population regression pa-
rameters using only the subset A.
For the ﬁrst case, it has been proposed that one can view the coefﬁcients
of the selected model as an efﬁcient but biased estimate of the full popu-
lation coefﬁcient vector. The idea is to then debias this estimate, allowing
inference for the full vector of coefﬁcients. Of course, sharper inference
will be available for the stronger variables that were selected in the ﬁrst
place.

Figure 16.10 HIV data. Linear regression of drug resistance in
HIV-positive patients on seven sites, indicators of mutations at
particular genomic locations. These seven sites were selected
from a total of 30 candidates, using the lasso. The naive 95%
conﬁdence intervals (dark) use standard linear-regression
inference, ignoring the selection event. The light intervals are
95% conﬁdence intervals, using linear regression, but conditioned
on the selection event.

For the second case, the idea is to condition on the selection event(s)
and hence the set A itself, and then perform conditional inference on the

−2−1012PredictorCoefficients5s8s9s16s25s26s28Naive intervalSelection−adjusted interval16.7 Connections and Extensions

319

unrestricted (i.e. not lasso-shrunk) regression coefﬁcients of the response
on only the variables in A. For the case of a lasso with squared-error loss,
it turns out that the set of response vectors y 2 RN that would lead to a
particular subset A of variables in the active set form a convex polytope
in RN (if we condition on the signs of the coefﬁcients as well; ignoring
the signs leads to a ﬁnite union of such polytopes). This, along with del-
icate Gaussian conditioning arguments, leads to truncated Gaussian and
t-distrubtions for parameters of interest.

Figure 16.10 shows the results of using the lasso to select variables in
an HIV study. The outcome Y is a measure of the resistence to an HIV-1
treatment (nucleoside reverse transcriptase inhibitor), and the 30 predictors
are indicators of whether mutations had occurred at particular genomic
sites. Lasso regression with 10-fold cross-validation selected a value of
(cid:21) D 0:003 and the seven sites indicated in the ﬁgure had nonzero coefﬁ-
cients. The dark bars in the ﬁgure indicate standard 95% conﬁdence inter-
vals for the coefﬁcients of the selected variables, using linear regression,
and ignoring the fact that the lasso was used to select the variables. Three
variables are signiﬁcant, and two more nearly so. The lighter bars are con-
ﬁdence intervals in a similar regression, but conditioned on the selection
event. We see that they are generally wider, and only variable s25 remains 7
signiﬁcant.

16.7 Connections and Extensions

There are interesting connections between lasso models and other popular
approaches to the prediction problem. We will brieﬂy cover two of these
here, namely support-vector machines and boosting.

Lasso Logistic Regression and the SVM

We show in Section 19.3 that ridged logistic regression has a lot in com-
mon with the linear support-vector machine. For separable data the limit
as (cid:21) # 0 in ridged logistic regression coincides with the SVM. In addition
their loss functions are somewhat similar. The same holds true for `1 regu-
larized logistic regression versus the `1 SVM—their end-path limits are the
same. In fact, due to the similarity of the loss functions, their solutions are
not too different elsewhere along the path. However, the end-path behavior
is a little more complex. They both converge to the `1 maximizing margin
separator—that is, the margin is measured with respect to the `1 distance
of points to the decision boundary, or maximum absolute coordinate.

8

320

Sparse Modeling and the Lasso

Lasso and Boosting

In Chapter 17 we discuss boosting, a general method for building a com-
plex prediction model using simple building components. In its simplest
form (regression) boosting amounts to the following simple iteration:
1 Inititialize b D 0 and F 0.x/ WD 0.
2 For b D 1; 2; : : : ; B:
(a) compute the residuals ri D yi (cid:0) F .xi /; i D 1; : : : ; n;
(b) ﬁt a small regression tree to the observations .xi ; ri /n
(c) update F b.x/ D F b(cid:0)1.x/ C (cid:15) (cid:1) gb.x/.
The “smallness” of the tree limits the interaction order of the model (e.g.
a tree with only two splits involves at most two variables). The number
of terms B and the shrinkage parameter (cid:15) are both tuning parameters that
control the rate of learning (and hence overﬁtting), and need to be set, for
example by cross-validation.

think of as estimating a function gb.x/; and

1, which we can

In words this algorithm performs a search in the space of trees for the one
most correlated with the residual, and then moves the ﬁtted function F b
a small amount in that direction—a process known as forward-stagewise
ﬁtting. One can paraphrase this simple algorithm in the context of linear
regression, where in step 2(b) the space of small trees is replaced by linear
functions.
1 Inititialize ˇ0 D 0, and standardize all the variables xj ; j D 1; : : : ; p.
2 For b D 1; 2; : : : ; B:
(a) compute the residuals r D y (cid:0) X ˇb;
(b) ﬁnd the predictor xj most correlated with the residual vector r; and
C (cid:15) (cid:1) sj (sj being the sign of
D ˇb
(c) update ˇb to ˇbC1, where ˇbC1

the correlation), leaving all the other components alone.

j

j

For small (cid:15) the solution paths for this least-squares boosting and the lasso
are very similar. It is natural to consider the limiting case or inﬁnitesimal
forward stagewise ﬁtting, which we will abbreviate iFS. One can imagine
a scenario where a number of variables are vying to win the competition
in step 2(b), and once they are tied their coefﬁcients move in concert as
they each get incremented. This was in fact the inspiration for the LAR
algorithm 16.3, where A represents the set of tied variables, and ı is the
relative number of turns they each have in getting their coefﬁcients up-
dated. It turns out that iFS is often but not always exactly the lasso; it can
instead be characterized as a type of monotone lasso.

9

16.8 Notes and Details

321

Not only do these connections inspire new insights and algorithms for
the lasso, they also offer insights into boosting. We can think of boosting
as ﬁtting a monotone lasso path in the high-dimensional space of variables
deﬁned by all possible trees of a certain size.

Extensions of the Lasso

(cid:15) The group lasso penaltyPK

kD1

The idea of using `1 regularization to induce sparsity has taken hold, and
variations of these ideas have spread like wildﬁre in applied statistical mod-
eling. Along with advances in convex optimization, hardly any branch of
applied statistics has been left untouched. We don’t go into detail here, but
refer the reader to the references in the endnotes. Instead we will end this
section with a (non-exhaustive) list of such applications, which may entice
the reader to venture into this domain.

kkk2 applies to vectors k of parame-
ters, and selects whole groups at a time. Armed with these penalties, one
can derive lasso-like schemes for including multilevel factors in linear
models, as well as hierarchical schemes for including low-order interac-
tions.
(cid:15) The graphical lasso applies `1 penalties in the problem of edge selection
in dependence graphs.
(cid:15) Sparse principal components employ `1 penalties to produce compo-
nents with many loadings zero. The same ideas are applied to discrimi-
nant analysis and canonical correlation analysis.
(cid:15) The nuclear norm of a matrix is the sum of its singular values—a lasso
penalty on matrices. Nuclear-norm regularization is popular in matrix
completion for estimating missing entries in a matrix.

16.8 Notes and Details

Classical regression theory aimed for an unbiased estimate of each predic-
tor variable’s effect. Modern wide data sets, often with enormous numbers
of predictors p, make that an untenable goal. The methods described here,
by necessity, use shrinkage methods, biased estimation, and sparsity.

The lasso was introduced by Tibshirani (1996), and has spawned a great
deal of research. The recent monograph by Hastie et al. (2015) gives a
compact summary of some of the areas where the lasso and sparsity have
been applied. The regression version of boosting was given in Hastie et al.
(2009, Chapter 16), and inspired the least-angle regression algorithm (Efron

322

Sparse Modeling and the Lasso

et al., 2004)—a new and more democratic version of forward-stepwise re-
gression, as well as a fast algorithm for ﬁtting the lasso. These authors
showed under some conditions that each step of the LAR algorithm corre-
sponds to one df; Zou et al. (2007) show that, with a ﬁxed (cid:21), the size of
the active set is unbiased for the df for the lasso. Hastie et al. (2009) also
view boosting as ﬁtting a lasso regularization path in the high-dimensional
space of trees.

Friedman et al. (2010) developed the pathwise coordinate-descent algo-
rithm for generalized lasso problems, and provide the glmnet package
for R (Friedman et al., 2009). Strong rules for lasso screening are due to
Tibshirani et al. (2012). Hastie et al. (2015, Chapter 3) show the similarity
between the `1 SVM and lasso logistic regression.

We now give some particular technical details on topics covered in the

chapter.

1 [p. 301] Forward-stepwise computations. Building up the forward-stepwise
model can be seen as a guided Gram–Schmidt orthogonalization (QR de-
composition). After step r, all p (cid:0) r variables not in the model are orthog-
onal to the r in the model, and the latter are in QR form. Then the next
variable to enter is the one most correlated with the residuals. This is the
one that will reduce the residual sum-of-squares the most, and one requires
p (cid:0) r n-vector inner products to identify it. The regression is then updated
trivially to accommodate the chosen one, which is then regressed out of the
p (cid:0) r (cid:0) 1 remaining variables.

2 [p. 301] Iteratively reweighted least squares (IRLS). Generalized linear
models (Chapter 8) are ﬁt by maximum-likelihood, and since the log-likeli-
hood is differentiable and concave, typically a Newton algorithm is used.
The Newton algorithm can be recast as an iteratively reweighted linear re-
gression algorithm (McCullagh and Nelder, 1989). At each iteration one
computes a working response variable zi, and a weight per observation wi
(both of which depend on the current parameter vector O
ˇ). Then the New-
ton update for O
ˇ is obtained by a weighted least-squares ﬁt of the zi on the
xi with weights wi (Hastie et al., 2009, Section 4.4.1).
3 [p. 301] Forward-stepwise logistic regression computations. Although the
current model is in the form of a weighted least-squares ﬁt, the p (cid:0) r vari-
ables not in the model cannot be kept orthogonal to those in the model (the
weights keep changing!). However, since our current model will have per-
formed a weighted QR decomposition (say), this orthogonalization can be
obtained without too much cost. We will need p (cid:0) r multiplications of an
r (cid:2) n matrix with an n vector—O..p (cid:0) r/ (cid:1) r (cid:1) n computations. An even
simpler alternative for the selection is to use the size of the gradient of the

16.8 Notes and Details

323
log-likelihood, which simply requires an inner product jhy (cid:0) O(cid:22)r ; xjij for
each omitted variable xj (assuming all the variables are standardized to
unit variance).

4 [p. 306] Best `1 interpolant. If p > n, then another boundary solution
becomes interesting for the lasso. For t sufﬁciently large, we will be able
to achieve a perfect ﬁt to the data, and hence a zero residual. There will
be many such solutions, so it becomes interesting to ﬁnd the perfect-ﬁt so-
lution with smallest value of t: the minimum-`1-norm perfect-ﬁt solution.
This requires solving a separate convex-optimization problem.

5 [p. 313] More on df. When the search is easy in that a variable stands out as
far superior, LAR takes a big step, and forward stepwise spends close to a
unit df. On the other hand, when there is close competition, the LAR steps
are small, and a unit df is spent for little progress, while forward stepwise
can spend a fair bit more than a unit df (the price paid for searching). In
fact, the dfj curve for forward stepwise can exceed p for j < p (Jansen
et al., 2015).

6 [p. 318] Post-selection inference. There has been a lot of activity around
post-selection inference for lasso and related methods, all of it since 2012.
To a large extent this was inspired by the work of Berk et al. (2013), but
more tailored to the particular selection process employed by the lasso. For
the debiasing approach we look to the work of Zhang and Zhang (2014),
van de Geer et al. (2014) and Javanmard and Montanari (2014). The condi-
tional inference approach began with Lockhart et al. (2014), and then was
developed further in a series of papers (Lee et al., 2016; Taylor et al., 2015;
Fithian et al., 2014), with many more in the pipeline.

7 [p. 319] Selective inference software. The example in Figure 16.10 was pro-
duced using the R package selectiveInference (Tibshirani et al.,
2016). Thanks to Rob Tibshirani for providing this example.

8 [p. 319] End-path behavior of ridge and lasso logistic regression for sep-
arable data. The details here are somewhat technical, and rely on dual
norms. Details are given in Hastie et al. (2015, Section 3.6.1).

9 [p. 320] LAR and boosting. Least-squares boosting moves the “winning”
coefﬁcient in the direction of the correlation of its variable with the resid-
ual. The direction ı computed in step 3(a) of the LAR algorithm may have
some components whose signs do not agree with their correlations, espe-
cially if the variables are very correlated. This can be ﬁxed by a particular
nonnegative least-squares ﬁt to yield an exact path algorithm for iFS; de-
tails can be found in Efron et al. (2004).

17

Random Forests and Boosting

In the modern world we are often faced with enormous data sets, both
in terms of the number of observations n and in terms of the number of
variables p. This is of course good news—we have always said the more
data we have, the better predictive models we can build. Well, we are there
now—we have tons of data, and must ﬁgure out how to use it.

Although we can scale up our software to ﬁt the collection of linear and
generalized linear models to these behemoths, they are often too modest
and can fall way short in terms of predictive power. A need arose for some
general purpose tools that could scale well to these bigger problems, and
exploit the large amount of data by ﬁtting a much richer class of functions,
almost automatically. Random forests and boosting are two relatively re-
cent innovations that ﬁt the bill, and have become very popular as “out-the-
box” learning algorithms that enjoy good predictive performance. Random
forests are somewhat more automatic than boosting, but can also suffer a
small performance hit as a consequence.

These two methods have something in common: they both represent the
ﬁtted model by a sum of regression trees. We discuss trees in some detail
in Chapter 8. A single regression tree is typically a rather weak prediction
model; it is rather amazing that an ensemble of trees leads to the state of
the art in black-box predictors!

We can broadly describe both these methods very simply.

Random forest Grow many deep regression trees to randomized versions
of the training data, and average them. Here “randomized” is a wide-
ranging term, and includes bootstrap sampling and/or subsampling of
the observations, as well as subsampling of the variables.

Boosting Repeatedly grow shallow trees to the residuals, and hence build

up an additive model consisting of a sum of trees.

The basic mechanism in random forests is variance reduction by averag-
ing. Each deep tree has a high variance, and the averaging brings the vari-

324

17.1 Random Forests

325

ance down. In boosting the basic mechanism is bias reduction, although
different ﬂavors include some variance reduction as well. Both methods
inherit all the good attributes of trees, most notable of which is variable
selection.

17.1 Random Forests

Suppose we have the usual setup for a regression problem, with a training
set consisting of an n(cid:2) p data matrix X and an n-vector of responses y. A
tree (Section 8.4) ﬁts a piecewise constant surface Or.x/ over the domain X

by recursive partitioning. The model is built in a greedy fashion, each time
creating two daughter nodes from a terminal node by deﬁning a binary split
using one of the available variables. The model can hence be represented
by a binary tree. Part of the art in using regression trees is to know how
deep to grow the tree, or alternatively how much to prune it back. Typi-
cally that is achieved using left-out data or cross-validation. Figure 17.1
shows a tree ﬁt to the spam training data. The splitting variables and split
points are indicated. Each node is labeled as spam or ham (not spam;
see footnote 7 on page 115). The numbers beneath each node show mis-
classiﬁed/total. The overall misclassiﬁcation error on the test data is 9:3%,
which compares poorly with the performance of the lasso (Figure 16.9:
7:1% for linear lasso, 5:7% for lasso with interactions). The surface Or.x/
here is clearly complex, and by its nature represents a rather high-order
interaction (the deepest branch is eight levels, and involves splits on eight
different variables). Despite the promise to deliver interpretable models,
this bushy tree is not easy to interpret. Nevertheless, trees have some desir-
able properties. The following lists some of the good and bad properties of
trees.

L Trees automatically select variables; only variables used in deﬁning splits

are in the model.

L Tree-growing algorithms scale well to large n; growing a tree is a divide-

and-conquer operation.

L Trees handle mixed features (quantitative/qualitative) seamlessly, and

can deal with missing data.

L Small trees are easy to interpret.
M Large trees are not easy to interpret.
M Trees do not generally have good prediction performance.

Trees are inherently high-variance function estimators, and the bushier
they are, the higher the variance. The early splits dictate the architecture of

326

Random Forests and Boosting

Figure 17.1 Regression tree ﬁt to the binary spam data, a bigger
version of Figure 8.7. The initial trained tree was far bushier than
the one displayed; it was then optimally pruned using 10-fold
cross-validation.

the tree. On the other hand, deep bushy trees localize the training data (us-
ing the variables that matter) to a relatively small region around the target
point. This suggests low bias. The idea of random forests (and its predeces-
sor bagging) is to grow many very bushy trees, and get rid of the variance
by averaging. In order to beneﬁt from averaging, the individual trees should
not be too correlated. This is achieved by injecting some randomness into
the tree-growing process. Random forests achieve this in two ways.

600/1536280/1177180/1065 80/861 80/652 77/423 20/238 19/236  1/2 57/185 48/113 37/101  1/12  9/72  3/229  0/209100/204 36/123 16/94 14/89  3/5  9/29 16/81  9/112  6/109  0/3 48/359 26/337 19/110 18/109  0/1  7/227  0/22spamspamspamspamspamspamspamspamspamspamspamspamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamch$<0.0555remove<0.06ch!<0.191george<0.005hp<0.03CAPMAX<10.5receive<0.125edu<0.045our<1.2CAPAVE<2.7505free<0.065business<0.145george<0.15hp<0.405CAPAVE<2.9071999<0.58ch$>0.0555remove>0.06ch!>0.191george>0.005hp>0.03CAPMAX>10.5receive>0.125edu>0.045our>1.2CAPAVE>2.7505free>0.065business>0.145george>0.15hp>0.405CAPAVE>2.9071999>0.5817.1 Random Forests

327

1 Bootstrap: each tree is grown to a bootstrap resampled training data set,

which makes them different and somewhat decorrelates them.

2 Split-variable randomization: each time a split is to be performed, the
search for the split variable is limited to a random subset of m of the p
variables. Typical values of m are

p or p=3.

p

When m D p, the randomization amounts to using only step 1, and was
an earlier ancestor of random forests called bagging. In most examples the
second level of randomization pays dividends.

B.

Algorithm 17.1 RANDOM FOREST.
1 Given training data set d D .X ; y/. Fix m  p and the number of trees
2 For b D 1; 2; : : : ; B, do the following.
(a) Create a bootstrap version of the training data d

(cid:3)
b , by randomly sam-
pling the n rows with replacement n times. The sample can be repre-
sented by the bootstrap frequency vector w

(b) Grow a maximal-depth tree Orb.x/ using the data in d
of the p features at random prior to making each split.

(cid:3)
b , sampling m

(cid:3)
b.

(c) Save the tree, as well as the bootstrap sampling frequencies for each

of the training observations.

3 Compute the random-forest ﬁt at any prediction point x0 as the average

BX

bD1

Orrf.x0/ D 1

B

Orb.x0/:

4 Compute the OOBi error for each response observation yi in the training
data, by using the ﬁt Or .i /
, obtained by averaging only those Orb.xi / for
which observation i was not in the bootstrap sample. The overall OOB
error is the average of these OOBi.

rf

Algorithm 17.1 gives some of the details; some more are given in the

technical notes.

The package randomForest in R sets as a default m D p
Random forests are easy to use, since there is not much tuning needed.
p for clas-
siﬁcation trees, and m D p=3 for regression trees, but one can use other
values. With m D 1 the split variable is completely random, so all vari-
ables get a chance. This will decorrelate the trees the most, but can create
bias, somewhat similar to that in ridge regression. Figure 17.2 shows the

1

328

Random Forests and Boosting

Figure 17.2 Test misclassiﬁcation error of random forests on the
spam data, as a function of the number of trees. The red curve
selects m D 7 of the p D 57 features at random as candidates for
the split variable, each time a split is made. The blue curve uses
m D 57, and hence amounts to bagging. Both bagging and
random forests outperform the lasso methods, and a single tree.

misclassiﬁcation performance of a random forest on the spam test data, as
a function of the number of trees averaged. We see that in this case, after
a relatively small number of trees (500), the error levels off. The number
B of trees averaged is not a real tuning parameter; as with the bootstrap
(Chapters 10 and 11), we need a sufﬁcient number for the estimate to sta-
bilize, but cannot overﬁt by having too many.

Random forests have been described as adaptive nearest-neighbor esti-
mators—adaptive in that they select predictors. A k-nearest-neighbor esti-
mate ﬁnds the k training observations closest in feature space to the target
point x0, and averages their responses. Each tree in the random forest drills
down by recursive partitioning to pure terminal nodes, often consisting of a
single observation. Hence, when evaluating the prediction from each tree,
Orb.x0/ D y` for some `, and for many of the trees this could be the same

0.000.020.040.060.08Random Forest on the Spam DataNumber of TreesTest Error15001000150020002500BaggingRandom ForestSingle TreeLassoLasso (interaction)17.1 Random Forests

329

`. From the whole collection of B trees, the number of distinct `s can be
fairly small. Since the partitioning that reaches the terminal nodes involves
only a subset of the predictors, the neighborhoods so deﬁned are adaptive.

Out-of-Bag Error Estimates

Random forests deliver cross-validated error estimates at virtually no ex-
tra cost. The idea is similar to the bootstrap error estimates discussed in
Chapter 10. The computation is described in step 4 of Algorithm 17.1.
In making the prediction for observation pair .xi ; yi /, we average all the
random-forest trees Orb.xi / for which that pair is not in the corresponding
bootstrap sample:

Figure 17.3 Out-of-bag misclassiﬁcation error estimate for the
spam data (blue) versus the test error (red), as a function of the
number of trees.

X

b W w

(cid:3)

b i

D0

Or .i /
rf .xi / D 1

Bi

Orb.xi /;

(17.1)

where Bi is the number of times observation i was not in the bootstrap
(cid:0)1B (cid:25) 0:37B). We then compute the OOB
sample (with expected value e
nX
error estimate

(17.2)

errOOB D 1

n

iD1

LŒyi ; Or .i /

rf .xi /;

0.000.020.040.060.08Number of TreesMisclassification Error15001000150020002500OOB ErrorTest Error330

Random Forests and Boosting

where L is the loss function of interest, such as misclassiﬁcation or squared-
error loss. If B is sufﬁciently large (about three times the number needed
for the random forest to stabilize), we can see that the OOB error estimate
is equivalent to leave-one-out cross-validation error.

Standard Errors

 is given by

bVjack.

We can use very similar ideas to estimate the variance of a random-forest
prediction, using the jackknife variance estimator (see (10.6) in Chapter 10).
If O
 is a statistic estimated using all n training observations, then the jack-
knife estimate of the variance of O
 / D n (cid:0) 1
O

2
.i / is the estimate using all but observation i, and O
.(cid:1)/ D 1
2

at x0 is obtained by simply plugging into this formula:

(cid:16) O
.i / (cid:0) O
.(cid:1)/

O
where O
.i /.
The natural jackknife variance estimate for a random-forest prediction

n

i

bVjack.Orrf.x0// D n (cid:0) 1

rf .x0/ (cid:0) Orrf.x0/

:

nX

iD1

;

(17.3)

P

n

(cid:16)Or .i /

nX

iD1

n

(17.4)

This formula is derived under the B D 1 setting, in which case Orrf.x0/ is
an expectation under bootstrap sampling, and hence is free of Monte Carlo
variability. This also makes the distinction clear: we are estimating the sam-
pling variability of a random-forest prediction Orrf.x0/, as distinct from any
Monte Carlo variation. In practice B is ﬁnite, and expression (17.4) will
have Monte Carlo bias and variance. All of the Or .i /
rf .x0/ are based on B
bootstrap samples, and they are hence noisy versions of their expectations.
Since the n quantities summed in (17.4) are squared, by Jensen’s inequal-
ity we will have positive bias (and it turns out that this bias dominates the
Monte Carlo variance). Hence one would want to use a much larger value
of B when estimating variances, than was used in the original random-
forest ﬁt. Alternatively, one can use the same B bootstrap samples as were
used to ﬁt the random forest, along with a bias-corrected version of the
jackknife variance estimate:

bVu
jack.Orrf.x0// DbVjack.Orrf.x0// (cid:0) .e (cid:0) 1/

Ov.x0/;

n

B

(17.5)

2

17.1 Random Forests

331

Figure 17.4 Jackknife Standard Error estimates (with bias
correction) for the probability estimates in the spam test data.
The points labeled red were misclassiﬁcations, and tend to
concentrate near the decision boundary (0.5).

where e D 2:718 : : :, and

Ov.x0/ D 1

B

BX

bD1

.Orb.x0/ (cid:0) Orrf.x0//2 ;

(17.6)

the bootstrap estimate of the variance of a single random-forest tree. All
these quantities are easily computed from the output of a random forest, so
they are immediately available. Figure 17.4 shows the predicted probabil-
ities and their jackknife estimated standard errors for the spam test data.
The estimates near the decision boundary tend to have higher standard er-
rors.

Variable-Importance Plots

A random forest is something of a black box, giving good predictions
but usually not much insight into the underlying surface it has ﬁt. Each
random-forest tree Orb will have used a subset of the predictors as split-
ting variables, and each tree is likely to use overlapping but not necessarily

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.000.050.100.150.200.250.30Probability PredictionStandard Error Estimate332

Random Forests and Boosting

Figure 17.5 Variable-importance plots for random forests ﬁt to
the spam data. On the left we have the m D 7 random forest; due
to the split-variable randomization, it spreads the importance
among the variables. On the right is the m D 57 random forest or
bagging, which focuses on a smaller subset of the variables.

identical subsets. One might conclude that any variable never used in any
of the trees is unlikely to be important, but we would like a method of
assessing the relative importance of variables that are included in the en-
semble. Variable-importance plots ﬁt this bill. Whenever a variable is used
in a tree, the algorithm logs the decrease in the split-criterion due to this
split. These are accumulated over all the trees, for each variable, and sum-
marized as relative importance measures. Figure 17.5 demonstrates this on
the spam data. We see that the m D 7 random forest, by virtue of the
split-variable randomization, spreads the importance out much more than
bagging, which always gets to pick the best variable for splitting. In this
sense small m has some similarity to ridge regression, which also tends to
share the coefﬁcients evenly among correlated variables.

!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyyouourgeorge000edubusinesshpl1999internet(willallrereceiveemailmailover;meeting650addressorderlabspmpeoplecreditmake#technologydatafont85[labtelnetreportoriginalprojectconferenceaddressesdirect4158573dcspartstableRandom Forest m = 7020406080100Variable Importance$!removehpCAPAVEfreeCAPTOTCAPMAXgeorgeyoueduyourourbusinessmoneywill(reemail000internetreceivemailmeeting1999overpm;dataorderfontpeopleallmakeconference650reportaddresshploriginaltechnologylab3dlabs[#creditcs85telnetpartsproject857addressesdirect415tableBagging m = 57020406080100Variable Importance17.2 Boosting with Squared-Error Loss

333

17.2 Boosting with Squared-Error Loss

Boosting was originally proposed as a means for improving the perfor-
mance of “weak learners” in binary classiﬁcation problems. This was achiev-
ed through resampling training points—giving more weight to those which
had been misclassiﬁed—to produce a new classiﬁer that would boost the
performance in previously problematic areas of feature space. This pro-
cess is repeated, generating a stream of classiﬁers, which are ultimately
combined through voting1 to produce the ﬁnal classiﬁer. The prototypical
weak learner was a decision tree.

Boosting has evolved since this earliest invention, and different ﬂavors
are popular in statistics, computer science, and other areas of pattern recog-
nition and prediction. We focus on the version popular in statistics—gradient
boosting—and return to this early version later in the chapter. Algorithm 17.2

Algorithm 17.2 GRADIENT BOOSTING WITH SQUARED-ERROR LOSS.
shrinkage factor (cid:15) and the tree depth d. Set the initial ﬁt bG0  0, and
1 Given a training sample d D .X ; y/. Fix the number of steps B, the
the residual vector r D y.
2 For b D 1; 2; : : : ; B repeat:
(a) Fit a regression tree Qgb to the data .X ; r/, grown best-ﬁrst to depth
d: this means the total number of splits are d, and each successive
(b) Update the ﬁtted model with a shrunken version of Qgb:bGb DbGb(cid:0)1 C
split is made to that terminal node that yields the biggest reduction in
residual sum of squares.
Ogb; with Ogb D (cid:15) (cid:1) Qgb:
3 Return the sequence of ﬁtted functionsbGb; b D 1; : : : ; B.
(c) Update the residuals accordingly: ri D ri (cid:0) Ogb.xi /; i D 1; : : : ; n:

gives the most basic version of gradient boosting, for squared-error loss.
This amounts to building a model by repeatedly ﬁtting a regression tree
to the residuals. Importantly, the tree is typically quite small, involving
a small number d of splits—it is indeed a weak learner. After each tree
has been grown to the residuals, it is shrunk down by a factor (cid:15) before
it is added to the current model; this is a means of slowing the learning
process. Despite the obvious similarities with a random forest, boosting is
different in a fundamental way. The trees in a random forest are identically
1 Each classiﬁer Ocb.x0/ predicts a class label, and the class with the most “votes” wins.

334

Random Forests and Boosting

Figure 17.6 Test performance of a boosted regression-tree model
ﬁt to the ALS training data, with n D 1197 and p D 369. Shown
is the mean-squared error on the 625 designated test observations,
as a function of the number of trees. Here the depth d D 4 and
(cid:15) D 0:02. Boosting achieves a lower test MSE than a random
forest. We see that as the number of trees B gets large, the test
error for boosting starts to increase—a consequence of
overﬁtting. The random forest does not overﬁt. The dotted blue
horizontal line shows the best performance of a linear model, ﬁt
by the lasso. The differences are less dramatic than they appear,
since the vertical scale does not extend to zero.

distributed—the same (random) treatment is repeatedly applied to the same
data. With boosting, on the other hand, each tree is trying to amend errors
made by the ensemble of previously grown trees. The number of terms B
is important as well, because unlike random forests, a boosted regression
model can overﬁt if B is too large. Hence there are three tuning parame-
ters, B, d and (cid:15), and each can change the performance of a boosted model,
sometimes considerably.

Figure 17.6 shows the test performance of boosting on the ALS data. 
These data represent measurements on patients with amyotrophic lateral
sclerosis (Lou Gehrig’s disease). The goal is to predict the rate of pro-
gression of an ALS functional rating score (FRS). There are 1197 training

3

0.250.260.270.280.290.30Number of TreesMean−squared Error1100200300400500BoostRandom ForestLasso17.2 Boosting with Squared-Error Loss

335

measurements on 369 predictors and the response, with a corresponding
test set of size 625 observations.

As is often the case, boosting slightly outperforms a random forest here,
but at a price. Careful tuning of boosting requires considerable extra work,
with time-costly rounds of cross-validation, whereas random forests are al-
most automatic. In the following sections we explore in more detail some
of the tuning parameters. The R package gbm implements gradient boost-
ing, with some added bells and whistles. By default it grows each new tree
on a 50% random sub-sample of the training data. Apart from speeding up
the computations, this has a similar effect to bagging, and results in some
variance reduction in the ensemble.

We can also compute a variable-importance plot, as we did for random
forests; this is displayed in Figure 17.7 for the ALS data. Only 267 of the
369 variables were ever used, with one variable Onset.Delta standing
out ahead of the others. This measures the amount of time that has elapsed
since the patient was ﬁrst diagnosed with ALS, and hence a larger value
will indicate a slower progression rate.

Tree Depth and Interaction Order

Tree depth d is an important parameter for gradient boosted models, and
the right choice will depend on the data at hand. Here depth d D 4 appears
to be a good choice on the test data. Without test data, we could use cross-
validation to make the selection. Apart from a general complexity measure,
Suppose we have a ﬁtted boosted modelbGB .x/, using B trees. Denote by
tree depth also controls the interaction order of the model.2 The easiest
case is with d D 1, where each tree consists of a single split (a stump).
D f1; 2; : : : ; Bg the indices of the trees that made the single split
Bj 
using variable j , for j D 1; : : : ; p. These Bj are disjoint (some B` can be

B

2 A .k (cid:0) 1/th-order interaction is also known as a k-way interaction. Hence an order-one
interaction model has two-way interactions, and an order-zero model is additive.

336

Random Forests and Boosting

Figure 17.7 Variable importance plot for the ALS data. Here 267
of the 369 variables were used in the ensemble. There are too
many variables for the labels to be visible, so this plot serves as a
visual guide. Variable Onset.Delta has relative importance
100 (the lowest red bar), more than double the next two at around
40 (last.slope.weight and alsfrs.score.slope).
However, the importances drop off slowly, suggesting that the
model requires a signiﬁcant fraction of the variables.

empty), andSp

jD1 Bj D

bD1

B. Then we can write
Ogb.x/
X

bGB .x/ D BX
D pX
D pX

jD1

Ogb.x/

Bj

b2
O
fj .xj /:

jD1

(17.7)

Onset.Deltalast.slope.weightalsfrs.score.slopelast.slope.bp.systolicmean.slope.svc.literssum.slope.alsfrs.scoremean.slope.weightlast.slope.fvc.litersmeansquares.alsfrs.scoresum.slope.fvc.literslast.alsfrs.scorefvc.liters.slopesum.bp.diastolicweight.slopemean.speechmax.slope.fvc.litersmean.slope.handwritingslope.bp.systolic.slopemean.slope.fvc.litersmin.slope.alsfrs.scoresd.fvc.litersmeansquares.climbing.stairssd.salivationlast.speechmin.slope.bp.systolicmean.alsfrs.scoremax.slope.salivationsum.slope.climbing.stairslast.fvc.literssum.fvc.litersmean.slope.alsfrs.scorebp.diastolic.slopelast.slope.handwritingmin.slope.fvc.litersspeech.slopesd.slope.alsfrs.scoremeansquares.slope.weightslope.handwriting.slopeslope.weight.slopesum.alsfrs.scoreslope.turning.slopelast.slope.swallowingsum.slope.speechsum.weightmax.fvc.literslast.svc.liters.datemax.bp.diastolicmin.alsfrs.scoremean.climbing.stairsmax.dressingmeansquares.bp.diastoliclast.weightsum.slope.weightmeansquares.resp.rateslope.fvc.liters.slopemean.slope.climbing.stairsswallowing.slopesum.slope.handwritinglast.slope.alsfrs.score.datemeansquares.dressingsd.bp.diastolicclimbing.stairs.slopeslope.alsfrs.score.slopelast.slope.svc.liters.datesum.handwritingmean.slope.speechsum.slope.dressingsd.bp.systolicmin.slope.cuttingmax.weightlast.slope.alsfrs.scoremean.fvc.litersslope.bp.diastolic.slopeslope.salivation.slopemeansquares.slope.dressingmin.slope.speechsd.slope.fvc.litersmax.alsfrs.scoresum.slope.bp.systolicmeansquares.slope.bp.diastolicsd.slope.bp.systolicslope.dressing.slopemin.slope.dressinglast.slope.climbing.stairsmax.slope.alsfrs.scoremax.slope.bp.systolicsd.dressingsum.climbing.stairssd.slope.weightslope.resp.rate.slopesd.resp.ratemean.slope.bp.systoliclast.slope.weight.datemax.heightlast.slope.svc.litersmin.bp.systolicsd.slope.turningsum.slope.salivationfirst.slope.fvc.liters.datelast.bp.diastolicsd.slope.svc.literssd.slope.dressingmeansquares.slope.fvc.litersmeansquares.speechmeansquares.fvc.literslast.alsfrs.score.datesd.slope.resp.ratemeansquares.slope.cuttingmin.slope.turningmin.fvc.litersslope.swallowing.slopelast.slope.bp.diastolicmax.slope.weightsd.slope.swallowinglast.slope.resp.rate.datemean.turningmean.bp.diastolicsum.dressingmeansquares.cuttingsum.resp.ratemeansquares.slope.turningbp.systolic.slopemean.slope.dressingsd.cuttingmean.slope.bp.diastolicsum.heightsalivation.slopemin.slope.weightfirst.slope.weight.datesum.slope.resp.ratemax.slope.swallowinglast.swallowingmean.slope.cuttingmax.slope.svc.litersmean.slope.turningmax.cuttinglast.turningmean.slope.resp.ratesum.bp.systolicmeansquares.slope.climbing.stairsfirst.slope.resp.rate.datesd.slope.salivationslope.cutting.slopelast.resp.rate.datesd.alsfrs.scoresd.slope.cuttingSite.of.Onset.Onset..Bulbarfirst.slope.svc.liters.datemin.turningsd.slope.bp.diastolicmin.cuttingmax.slope.resp.rateAgesum.slope.swallowingsum.swallowingsd.speechlast.dressingfirst.slope.alsfrs.score.datemean.slope.salivationmeansquares.handwritingmin.weightlast.bp.systolicmean.salivationmeansquares.slope.alsfrs.scoremin.slope.climbing.stairsmean.slope.swallowinglast.slope.fvc.liters.datesum.salivationmin.slope.salivationmean.dressingslope.climbing.stairs.slopemeansquares.slope.walkingmeansquares.slope.bp.systolicresp.rate.slopelast.slope.dressinglast.slope.resp.ratemax.slope.dressingSex.Femalemin.swallowingmax.bp.systolicmin.bp.diastolicmeansquares.slope.handwritingmin.slope.bp.diastolicfirst.slope.bp.diastolic.datemean.handwritinglast.height.datesd.svc.literssd.slope.handwritingmean.slope.walkingmean.cuttingsum.slope.bp.diastoliclast.slope.salivationmin.slope.swallowinglast.fvc.liters.datewalking.slopesum.cuttingmean.swallowingsd.swallowingsum.speechmax.slope.climbing.stairshandwriting.slopemean.svc.litersmax.speechmeansquares.salivationmeansquares.slope.svc.literslast.slope.bp.diastolic.datefirst.resp.rate.datemeansquares.slope.salivationmean.resp.ratemax.slope.bp.diastolicmin.speechslope.speech.slopemean.weightSex.Malemax.svc.literssum.turningmax.slope.turninglast.svc.litersmax.swallowingmeansquares.turningmax.handwritingmeansquares.swallowingsd.slope.climbing.stairsmin.climbing.stairssvc.liters.slopelast.cuttingmeansquares.bp.systoliclast.slope.turningmax.salivationlast.weight.datemeansquares.slope.resp.ratemeansquares.weightmeansquares.slope.speechmeansquares.slope.swallowingmax.walkinglast.handwritingnum.slope.weight.visitssum.slope.cuttinglast.resp.ratesd.weightsd.climbing.stairssum.slope.turningsum.walkingslope.walking.slopeno.height.datafirst.alsfrs.score.datemin.salivationmin.slope.resp.ratefirst.slope.bp.systolic.datelast.slope.walkingStudy.Arm.PLACEBOfirst.slope.height.datemin.slope.handwritinglast.climbing.stairssd.handwritingSymptom.WEAKNESSmax.slope.handwritinglast.slope.cuttingmax.turningmin.slope.svc.litersdressing.slopesd.slope.speechMothernum.slope.resp.rate.visitsSymptom.AtrophySymptom.Swallowingcutting.slopenum.slope.bp.systolic.visitslessthan2.slope.bp.diastolicnum.slope.bp.diastolic.visitsno.slope.resp.rate.datalessthan2.slope.resp.rateno.slope.weight.datalessthan2.slope.weightslope.svc.liters.slopeno.slope.fvc.liters.datalessthan2.slope.fvc.litersnum.slope.fvc.liters.visitssd.slope.walkingsum.slope.walkingmin.slope.walkingmax.slope.walkingmax.slope.cuttinglast.slope.speechmax.slope.speechlessthan2.slope.alsfrs.scorenum.slope.alsfrs.score.visitsnum.bp.systolic.visitsmean.bp.systolicno.bp.diastolic.datalessthan2.bp.diastoliclast.bp.diastolic.datefirst.bp.diastolic.datenum.bp.diastolic.visitsno.resp.rate.datalessthan2.resp.ratenum.resp.rate.visitsmin.resp.ratemax.resp.ratesd.heightmeansquares.heightfirst.height.datenum.height.visitsmean.heightmin.heightlessthan2.weightfirst.weight.datenum.weight.visitsmeansquares.svc.literssum.svc.litersnum.svc.liters.visitsmin.svc.litersno.fvc.liters.datalessthan2.fvc.litersfirst.fvc.liters.datenum.fvc.liters.visitsmax.climbing.stairssd.walkingmeansquares.walkingmean.walkinglast.walkingmin.walkingturning.slopesd.turningmin.dressingmin.handwritinglast.salivationno.alsfrs.score.datalessthan2.alsfrs.scorenum.alsfrs.score.visitsStudy.Arm.ACTIVENeurological.Disease.STROKE.HEMORRHAGICNeurological.Disease.STROKE.ISCHEMICNeurological.Disease.BRAIN.TUMORNeurological.Disease.ALSNeurological.Disease.DATNeurological.Disease.PARKINSON.S.DISEASENeurological.Disease.DEMENTIA.NOSNeurological.Disease.STROKE.NOSNeurological.Disease.OTHERFamilyBrotherSisterDaughterSonUncle..Paternal.Uncle..Maternal.UncleGrandmother..Maternal.GrandmotherGrandfather..Maternal.FatherCousinAunt..Maternal.AuntRace...OtherRace...CaucasianRace...Black.African.AmericanRace...AsianSite.of.Onset.Onset..Limb.and.BulbarSite.of.Onset.Onset..LimbSymptom..Symptom.StiffnessSymptom.SENSORY_CHANGESSymptom.FasciculationsSymptom.CrampsSymptom.GAIT_CHANGESSymptom.OTHERSymptom.Speech020406080100Variable−Importance Plot for Boosting on the  ALS Data17.2 Boosting with Squared-Error Loss

337

Figure 17.8 ALS test error for boosted models with different
depth parameters d, and all using the same shrinkage parameter
(cid:15) D 0:02. It appears that d D 1 is inferior to the rest, with d D 4
about the best. With d D 7, overﬁtting begins around 200 trees,
with d D 4 around 300, while neither of the other two show
evidence of overﬁtting by 500 trees.

Hence boosted stumps ﬁts an additive model, but in a fully adaptive way.
It selects variables, and also selects how much action to devote to each
variable. We return to additive models in Section 17.5. Figure 17.9 shows
the three functions with highest relative importance. The ﬁrst function con-
ﬁrms that a longer time since diagnosis (more negative Onset.Delta)
predicts a slower decline. last.slope.weight is the difference in
body weight at the last two visits—again positive is good. Likewise for
alsfrs.score.slope, which measures the local slope of the FRS
score after the ﬁrst two visits.
In a similar way, boosting with d D 2 ﬁts a two-way interaction model;
each tree involves at most two variables. In general, boosting with d D k
leads to a .k (cid:0) 1/th-order interaction model. Interaction order is perhaps a
more natural way to think of model complexity.

0.250.260.270.280.290.30Number of TreesMean−squared Error1100200300400500Depth1247338

Random Forests and Boosting

Figure 17.9 Three of the ﬁtted functions (17.7) for the ALS data,
in a boosted stumps model (d D 1), each centered to average zero
over the training data. In terms of the outcome, bigger is better
(slower decline in FRS). The ﬁrst function conﬁrms that a longer
time since diagnosis (more negative value of Onset.Delta)
predicts a slower decline. The variable last.slope.weight
is the difference in body weight at the last two visits—again
positive is good. Likewise for alsfrs.score.slope, which
measures the local slope of the FRS score after the ﬁrst two visits.

Shrinkage

The shrinkage parameter (cid:15) controls the rate at which boosting ﬁts—and
hence overﬁts—the data. Figure 17.10 demonstrates the effect of shrink-
age on the ALS data. The under-shrunk ensemble (red) quickly overﬁts the
data, leading to poor validation error. The blue ensemble uses a shrink-
age parameter 20 times smaller, and reaches a lower validation error. The
downside of a very small shrinkage parameter is that it can take many trees
to adequately ﬁt the data. On the other hand, the shrunken ﬁts are smoother,
take much longer to overﬁt, and hence are less sensitive to the stopping
point B.

17.3 Gradient Boosting

We now turn our attention to boosting models using other than square-error
loss. We focus on the family of generalized models generated by the expo-
nential family of response distributions (see Chapter 8). The most popular
and relevant in this class is logistic regression, where we are interested in
modeling (cid:22).x/ D Pr.Y D 1jX D x/ for a Bernoulli response variable.

−2000−10000−0.3−0.2−0.10.00.10.2Onset.DeltaFitted Function−20−1001020−0.3−0.2−0.10.00.10.2last.slope.weightFitted Function−10−6−4−2024−0.3−0.2−0.10.00.10.2alsfrs.score.slopeFitted Function17.3 Gradient Boosting

339

Figure 17.10 Boosted d D 3 models with different shrinkage
parameters, ﬁt to a subset of the ALS data. The solid curves are
validation errors, the dashed curves training errors, with red for
(cid:15) D 0:5 and blue for (cid:15) D 0:02. With (cid:15) D 0:5, the training error
drops rapidly with the number of trees, but the validation error
starts to increase rapidly after an initial decrease. With (cid:15) D 0:02
(25 times smaller), the training error drops more slowly. The
validation error also drops more slowly, but reaches a lower
minimum (the horizontal dotted line) than the (cid:15) D 0:5 case. In
this case, the slower learning has paid off.

The idea is to ﬁt a model of the form

(cid:21).x/ D GB .x/ D BX

bD1

gb.xI (cid:13)b/;

(17.8)

where (cid:21).x/ is the natural parameter in the conditional distribution of Y jX D
x, and the gb.xI (cid:13)b/ are simple functions such as shallow trees. Here we
have indexed each function by a parameter vector (cid:13)b; for trees these would
capture the identity of the split variables, their split values, and the con-
stants in the terminal nodes. In the case of the Bernoulli response, we have

Pr.Y D 1jX D x/

Pr.Y D 0jX D x/



(cid:21).x/ D log

;

(17.9)

0.00.10.20.30.4Number of TreesMean−squared Error150100150200250ǫ=0.02ǫ=0.50TrainValidate340

Random Forests and Boosting

the logit link function that relates the mean to the natural parameter. In gen-
eral, if (cid:22).x/ D E.Y jX D x/ is the conditional mean, we have Œ(cid:22).x/ D
(cid:21).x/, where  is the monotone link function.

Algorithm 17.3 outlines a general strategy for building a model by for-
ward stagewise ﬁtting. L is the loss function, such as the negative log-
likelihood for Bernoulli responses, or squared-error for Gaussian responses.
Although we are thinking of trees for the simple functions g.xI (cid:13) /, the
ideas generalize. This algorithm is easier to state than to implement. For

Algorithm 17.3 GENERALIZED BOOSTING BY FORWARD-STAGEWISE
FITTING
1 Deﬁne the class of functions g.xI (cid:13) /. Start with OG0.x/ D 0, and set B
and the shrinkage parameter (cid:15) > 0.
2 For b D 1; : : : ; B repeat the following steps.
(a) Solve

nX

(cid:16)
yi ;bGb(cid:0)1.xi / C g.xiI (cid:13) /



L

(b) UpdatebGb.x/ DbGb(cid:0)1.x/ C Ogb.x/; with Ogb.x/ D (cid:15) (cid:1) g.xI O(cid:13)b/.
3 Return the sequencebGb.x/; b D 1; : : : ; B.

iD1

(cid:13)

O(cid:13)b D arg min

squared-error loss, at each step we need to solve

nX

minimize

.ri (cid:0) g.xiI (cid:13) //2 ;

(17.10)

(cid:13)

iD1

with ri D yi (cid:0)bGb(cid:0)1.xi /; i D 1; : : : ; n. If g.(cid:1)I (cid:13) / represents a depth-d tree,

(17.10) is still difﬁcult to solve. But here we can resort to the usual greedy
heuristic, and grow a depth-d tree to the residuals by the usual top-down
splitting, as in step 2(a) of Algorithm 17.2. Hence in this case, we have
exactly the squared-error boosting Algorithm 17.2. For more general loss
functions, we rely on one more heuristic for solving step 2(a), inspired by
gradient descent. Algorithm 17.4 gives the details. The idea is to perform
functional gradient descent on the loss function, in the n-dimensional space
of the ﬁtted vector. However, we want to be able to evaluate our new func-
tion everywhere, not just at the n original values xi. Hence once the (neg-
ative) gradient vector has been computed, it is approximated by a depth-d
tree (which can be evaluated everywhere). Taking a step of length (cid:15) down

17.4 Adaboost: the Original Boosting Algorithm

341

the gradient amounts to adding (cid:15) times the tree to the current function. 
Gradient boosting is quite general, and can be used with any differentiable

4

Algorithm 17.4 GRADIENT BOOSTING
1 Start with OG0.x/ D 0, and set B and the shrinkage parameter (cid:15) > 0.
2 For b D 1; : : : ; B repeat the following steps.
(a) Compute the pointwise negative gradient of the loss function at the

current ﬁt:

ˇˇˇˇ(cid:21)iDbGb(cid:0)1.xi /
ri D (cid:0) @L.yi ; (cid:21)i /
nX

@(cid:21)i

minimize

(cid:13)

.ri (cid:0) g.xiI (cid:13) //2 :

iD1

; i D 1; : : : ; n:

(b) Approximate the negative gradient by a depth-d tree by solving

(c) Update OGb.x/ D OGb(cid:0)1.x/ C Ogb.x/; with Ogb.x/ D (cid:15) (cid:1) g.xI O(cid:13)b/.
3 Return the sequence OGb.x/; b D 1; : : : ; B.

loss function. The R package gbm implements Algorithm 17.4 for a variety
of loss functions, including squared-error, binomial (Bernoulli), Laplace
(`1 loss), multinomial, and others. Included as well is the partial likelihood
for the Cox proportional hazards model (Chapter 9). Figure 17.11 com-
pares the misclassiﬁcation error of boosting on the spam data, with that of
random forests and bagging. Since boosting has more tuning parameters, a
careful comparison must take these into account. Using the McNemar test
we would conclude that boosting and random forest are not signiﬁcantly
different from each other, but both outperform bagging.

17.4 Adaboost: the Original Boosting Algorithm

The original proposal for boosting looked quite different from what we
have presented so far. Adaboost was developed for the two-class classiﬁ-
cation problem, where the response is coded as -1/1. The idea was to ﬁt a
sequence of classiﬁers to modiﬁed versions of the training data, where the
modiﬁcations give more weight to misclassiﬁed points. The ﬁnal classiﬁ-
cation is by weighted majority vote. The details are rather speciﬁc, and are
given in Algorithm 17.5. Here we distinguish a classiﬁer C.x/ 2 f(cid:0)1; 1g,
which returns a class label, rather than a probability. Algorithm 17.5 gives

342

Random Forests and Boosting

Figure 17.11 Test misclassiﬁcation for gradient boosting on the
spam data, compared with a random forest and bagging.
Although boosting appears to be better, it requires crossvaldiation
or some other means to estimate its tuning parameters, while the
random forest is essentially automatic.

the Adaboost.M1 algorithm. Although the classiﬁer in step 2(a) can be ar-
bitrary, it was intended for weak learners such as shallow trees. Steps 2(c)–
(d) look mysterious. Its easy to check that, with the reweighted points, the
classiﬁer Ocb just learned would have weighted error 0.5, that of a coin ﬂip.
ues ˙1, the ensemblebGb.x/ takes values in R.
We also notice that, although the individual classiﬁers Ocb.x/ produce val-
ponential loss function. The functions bGb.x/ output in step 3 of Algo-

It turns out that the Adaboost Algorithm 17.5 ﬁts a logistic regression
model via a version of the general boosting Algorithm 17.3, using an ex-

rithm 17.5 are estimates of (half) the logit function (cid:21).x/.

To show this, we ﬁrst motivate the exponential loss, a somewhat unusual
choice, and show how it is linked to logistic regression. For a -1/1 response
y and function f .x/, the exponential loss is deﬁned as LE .y; f .x// D
expŒ(cid:0)yf .x/. A simple calculation shows that the solution to the (condi-

0.000.020.040.060.08Gradient Boosting on the Spam DataNumber of TreesTest Error15001000150020002500BaggingRandom ForestBoosting (depth 4)17.4 Adaboost: the Original Boosting Algorithm

343

wi.

Algorithm 17.5 Adaboost
1 Initialize the observation weights wi D 1=n; i D 1; : : : ; n.
2 For b D 1; : : : ; B repeat the following steps.
(a) Fit a classiﬁer Ocb.x/ to the training data, using observation weights
(b) Compute the weighted misclassiﬁcation error for Ocb:
Pn
iD1 wi I Œyi ¤ Ocb.xi /
iD1 wi
(c) Compute ˛b D logŒ.1 (cid:0) errb/=errb.
(cid:1) exp .˛b (cid:1) I Œyi ¤ cb.xi // ;
(d) Update the weights wi   wi
3 Output the sequence of functions bGb.x/ D Pb
i
sponding classiﬁersbCb.x/ D sign
, b D 1; : : : ; B.

i D
`D1 ˛m Oc`.x/ and corre-

errb DPn

hbGb.x/

1; : : : ; n.

:

tional) population minimization problem

(cid:0)yf .x/ j x

EŒe

minimize

f .x/

Pr.y D C1jx/

Pr.y D (cid:0)1jx/



:

f .x/ D 1

2

log

(17.11)

(17.12)

is given by

Inverting, we get

Pr.y D C1jx/ D

ef .x/

(cid:0)f .x/

e

and Pr.y D (cid:0)1jx/ D

e

e

(cid:0)f .x/ C ef .x/

(cid:0)f .x/ C ef .x/
;
(17.13)
a perfectly reasonable (and symmetric) model for a probability. The quan-
tity yf .x/ is known as the margin (see also Chapter 19); if the margin
is positive, the classiﬁcation using Cf .x/ D sign.f .x// is correct for y,
else it is incorrect if the margin is negative. The magnitude of yf .x/ is
proportional to the (signed) distance of x from the classiﬁcation boundary
(exactly for linear models, approximately otherwise). For -1/1 data, we can
also write the (negative) binomial log-likelihood in terms of the margin.

344

Random Forests and Boosting

Using (17.13) we have

LB .y; f .x// D (cid:0)fI.y D (cid:0)1/ log Pr.y D (cid:0)1jx/

(cid:16)
C I.y D C1/ log Pr.y D C1jx/g
1 C e

(cid:0)2yf .x/

:

D log

(17.14)

E(cid:2)log(cid:0)1 C e

(cid:0)2yf .x/(cid:1) j x(cid:3) also has population minimizer f .x/ equal to

half the logit (17.12).3 Figure 17.12 compares the exponential loss function
with this binomial loss. They both asymptote to zero in the right tail—the
area of correct classiﬁcation. In the left tail, the binomial loss asymptotes
to a linear function, much less severe than the exponential loss.

Figure 17.12 Exponential loss used in Adaboost, versus the
binomial loss used in the usual logistic regression. Both estimate
the logit function. The exponential left tail, which punishes
misclassiﬁcations, is much more severe than the asymptotically
linear tail of the binomial.

The exponential loss simpliﬁes step 2(a) in the gradient boosting Algo-

3 The half comes from the symmetric representation we use.

−3−2−101230123456yf(x)LossBinomialExponentialrithm 17.3.

(cid:16)
yi ;bGb(cid:0)1.xi / C g.xiI (cid:13) /

LE

nX

iD1

17.5 Connections and Extensions

345

iD1

 D nX
D nX
D nX

iD1

expŒ(cid:0)yi .bGb(cid:0)1.xi / C g.xiI (cid:13) //
wi expŒ(cid:0)yi g.xiI (cid:13) /

(17.15)

wi LE .yi ; g.xiI (cid:13) // ;

5

with wi D expŒ(cid:0)yibGb(cid:0)1.xi /. This is just a weighted exponential loss with

the past history encapsulated in the observation weight wi (see step 2(a) in
Algorithm 17.5). We give some more details in the chapter endnotes on
how this reduces to the Adaboost algorithm.

iD1

The Adaboost algorithm achieves an error rate on the spam data com-

parable to binomial gradient boosting.

17.5 Connections and Extensions

Boosting is a general nonparametric function-ﬁtting algorithm, and shares
attributes with a variety of existing methods. Here we relate boosting to two
different approaches: generalized additive models and the lasso of Chap-
ter 16.

Generalized Additive Models

Boosting ﬁts additive, low-order interaction models by a forward stage-
wise strategy. Generalized additive models (GAMs) are a predecessor, a
semi-parametric approach toward nonlinear function ﬁtting. A GAM has
the form

fj .xj /;

(17.16)
where again (cid:21).x/ D Œ(cid:22).x/ is the natural parameter in an exponential
family. The attraction of a GAM is that the components are interpretable
and can be visualized, and they can move us a big step up from a linear
model.

jD1

There are many ways to specify and ﬁt additive models. For the fj , we
could use parametric functions (e.g. polynomials), ﬁxed-knot regression
splines, or even linear functions for some terms. Less parametric options

(cid:21).x/ D pX

346

Random Forests and Boosting

are smoothing splines and local regression (see Section 19.8). In the case of
squared-error loss (the Gaussian case), there is a natural set of backﬁtting
equations for ﬁtting a GAM:

O
fj  

Of`/; j D 1; : : : ; p:

Sj .y (cid:0)X

O
f`.xn`/

(17.17)

O
f`.x1`/; : : : ; .

`¤j
Here Of` D Œ
0 is the n-vector of ﬁtted values for the
current estimate of function f`. Hence the term in parentheses is a partial
residual, removing all the current function ﬁts from y except the one about
to be updated. Sj is a smoothing operator derived from variable xj that
gets applied to this residual and delivers the next estimate for function f`.
Backﬁtting starts with all the functions zero, and then cycles through these
equations for j D 1; 2; : : : ; p; 1; 2; : : : in a block-coordinate fashion, until
all the functions stabilize.

The ﬁrst pass through all the variables is similar to the regression boost-
ing Algorithm 17.2, where each new function takes the residuals from the
past ﬁts, and models them using a tree (for Sj ). The difference is that
boosting never goes back and ﬁxes up past functions, but ﬁts in a forward-
stagewise fashion, leaving all past functions alone. Of course, with its adap-
tive ﬁtting mechanism, boosting can select the same variables as used be-
fore, and thereby update that component of the ﬁt. Boosting with stumps
(single-split trees, see the discussion on tree depth on 335 in Section 17.2)
can hence be seen as an adaptive way for ﬁtting an additive model, that si-
multaneously performs variable selection and allows for different amounts
of smoothing for different variables.

Boosting and the Lasso

In Section 16.7 we drew attention to the close connection between the
forward-stagewise ﬁtting of boosting (with shrinkage ) and the lasso, via
inﬁnitesimal forward-stagewise regression. Here we take this a step further,
by using the lasso as a post-processor for boosting (or random forests).

Boosting with shrinkage does a good job in building a prediction model,
but at the end of the day can involve a lot of trees. Because of the shrink-
age, many of these trees could be similar to each other. The idea here is
to use the lasso to select a subset of these trees, reweight them, and hence
produce a prediction model with far fewer trees and, one hopes, compa-
rable accuracy. Suppose boosting has produced a sequence of ﬁtted trees
Ogb.x/; b D 1; : : : ; B. We then solve the lasso problem

17.6 Notes and Details

347

Figure 17.13 Post-processing of the trees produced by boosting
on the ALS data. Shown is the test prediction error as a function
of the number of trees selected by the (nonnegative) lasso. We see
that the lasso can do as good a job with one-third the number of
trees, although selecting the correct number is critical.

"

nX

iD1

BX

bD1

#

BX

bD1

minimize

fˇbgB

1

L

yi ;

Ogb.xi /ˇb

C (cid:21)

jˇbj

(17.18)

for different values of (cid:21). This model selects some of the trees, and as-
signs differential weights to them. A reasonable variant is to insist that the
weights are nonnegative. Figure 17.13 illustrates this approach on the ALS
data. Here we could use one-third of the trees. Often the savings are much
more dramatic.

17.6 Notes and Details

Random forests and boosting live at the cutting edge of modern predic-
tion methodology. They ﬁt models of breathtaking complexity compared
with classical linear regression, or even with standard GLM modeling as
practiced in the late twentieth century (Chapter 8). They are routinely used
as prediction engines in a wide variety of industrial and scientiﬁc appli-
cations. For the more cautious, they provide a terriﬁc benchmark for how
well a traditional parametrized model is performing: if the random forests

0.250.260.270.280.290.30Number of TreesMean−squared Error1100200300400500Depth 2 BoostLasso Post Fit348

Random Forests and Boosting

does much better, you probably have some work to do, by including some
important interactions and the like.

The regression and classiﬁcation trees discussed in Chapter 8 (Breiman
et al., 1984) took traditional models to a new level, with their ability to
adapt to the data, select variables, and so on. But their prediction per-
formance is somewhat lacking, and so they stood the risk of falling by
the wayside. With their new use as building blocks in random forests and
boosting, they have reasserted themselves as critical elements in the mod-
ern toolbox.

Random forests and bagging were introduced by Breiman (2001), and
boosting by Schapire (1990) and Freund and Schapire (1996). There has
been much discussion on why boosting works (Breiman, 1998; Friedman
et al., 2000; Schapire and Freund, 2012); the statistical interpretation given
here can also be found in Hastie et al. (2009), and led to the gradient boost-
ing algorithm (Friedman, 2001). Adaboost was ﬁrst described in Freund
and Schapire (1997). Hastie et al. (2009, Chapter 15) is devoted to random
forests. For the examples in this chapter we used the randomForest
package in R (Liaw and Wiener, 2002), and for boosting the gbm (Ridge-
way, 2005) package. The lasso post-processing idea is due to Friedman and
Popescu (2005), which we implemented using glmnet (Friedman et al.,
2009). Generalized additive models are described in Hastie and Tibshirani
(1990).

We now give some particular technical details on topics covered in the

chapter.

1 [p. 327] Averaging trees. A maximal-depth tree splits every node until it is
pure, meaning all the responses are the same. For very large n this might
be unreasonable; in practice, one can put a lower bound on the minimum
count in a terminal node. We are deliberately vague about the response type
in Algorithm 17.1. If it is quantitative, we would ﬁt a regression tree. If it
is binary or multilevel qualitative, we would ﬁt a classiﬁcation tree. In this
case at the averaging stage, there are at least two strategies. The original
random-forest paper (Breiman, 2001) proposed that each tree should make
a classiﬁcation, and then the ensemble uses a plurality vote. An alterna-
tive reasonable strategy is to average the class probabilities produced by
the trees; these procedures are identical if the trees are grown to maximal
depth.

2 [p. 330] Jackknife variance estimate. The jackknife estimate of variance
for a random forest, and the bias-corrected version, is described in Wager
et al. (2014). The jackknife formula (17.3) is applied to the B D 1 ver-

17.6 Notes and Details

349

sion of the random forest, but of course is estimated by plugging in ﬁnite
B versions of the quantities involved. Replacing Or .(cid:1)/
rf .x0/ by its expectation
Orrf.x0/ is not the problem; its that each of the Or .i /
rf .x0/ vary about their boot-
strap expectations, compounded by the square in expression (17.4). Calcu-
lating the bias requires some technical derivations, which can be found in
that reference.

They also describe the inﬁnitesimal jackknife estimate of variance, given

by

withdcovi Ddcov.w

(cid:3)

bVIJ.Orrf.x0// D nX

dcov2

i ;

iD1

BX

(17.19)

(17.20)

as discussed in Chapter 20. It too has a bias-corrected version, given by

(cid:0) 1/.Orb.x0/ (cid:0) Orrf.x0//;

B

.w

(cid:3)
b i

bD1

; Or(cid:3).x0// D 1
bVu
IJ.Orrf.x0// DbVIJ.Orrf.x0// (cid:0) n

B

Ov.x0/;

(17.21)

similar to (17.5).

3 [p. 334] The ALS data. These data were kindly provided by Lester Mackey
and Lilly Fang, who won the DREAM challenge prediction prize in 2012
(Kuffner et al., 2015). It includes some additional variables created by
them. Their winning entry used Bayesian trees, not too different from ran-
dom forests.

4 [p. 341] Gradient-boosting details. In Friedman’s gradient-boosting algo-
rithm (Hastie et al., 2009, Chapter 10, for example), a further reﬁnement
is implemented. The tree in step 2(b) of Algorithm 17.4 is used to de-
ﬁne the structure (split variables and splits), but the values in the terminal
nodes are left to be updated. We can think of partitioning the parameters
(cid:13) D .(cid:13)s; (cid:13)t /, and then represent the tree as g.xI (cid:13) / D T .xI (cid:13)s/
(cid:13)t. Here
T .xI (cid:13)s/ is a vector of d C 1 binary basis functions that indicate the termi-
nal node reached by input x, and (cid:13)t are the d C 1 values of the terminal
nodes of the tree. We learn O(cid:13)s by approximating the gradient in step 2(b)
by a tree, and then (re-)learn the terminal-node parameters O(cid:13)t by solving
the optimization problem

0

(cid:16)
yi ; OGb(cid:0)1.xi / C T .xiI O(cid:13)s/

0

(cid:13)t



:

(17.22)

nX

iD1

minimize

(cid:13)t

L

Solving (17.22) amounts to ﬁtting a simple GLM with an offset.

350

Random Forests and Boosting

5 [p. 345] Adaboost and gradient boosting. Hastie et al. (2009, Chapter
10) derive Adaboost as an instance of Algorithm 17.3. One detail is that
the trees g.xI (cid:13) / are replaced by a simpliﬁed scaled classiﬁer ˛ (cid:1) c.xI (cid:13)
/.
Hence, from (17.15), in step 2(a) of Algorithm 17.3 we need to solve

0

wi expŒ(cid:0)yi ˛c.xiI (cid:13)

0

/:

(17.23)

nX

iD1

minimize

0

˛;(cid:13)

The derivation goes on to show that
(cid:15) minimizing (17.23) for any value of ˛ > 0 can be achieved by ﬁtting
/ to minimize the weighted misclassiﬁcation

0

a classiﬁcation tree c.xI O(cid:13)
nX
error

wi I Œyi ¤ c.xi ; (cid:13)

0

/I

iD1

(cid:15) given c.xI O(cid:13)
0
/, ˛ is estimated as in step 2(c) of Algorithm 17.5 (and is
non-negative);
(cid:15) the weight-update scheme in step 2(d) of Algorithm 17.5 corresponds

exactly to the weights as computed in (17.15).

18

Neural Networks and Deep Learning

Something happened in the mid 1980s that shook up the applied statistics
community. Neural networks (NNs) were introduced, and they marked a
shift of predictive modeling towards computer science and machine learn-
ing. A neural network is a highly parametrized model, inspired by the ar-
chitecture of the human brain, that was widely promoted as a universal
approximator—a machine that with enough data could learn any smooth
predictive relationship.

Figure 18.1 Neural network diagram with a single hidden layer.
The hidden layer derives transformations of the inputs—nonlinear
transformations of linear combinations—which are then used to
model the output.

CP4

CP5

0

Figure 18.1 shows a simple example of a feed-forward neural network
diagram. There are four predictors or inputs xj , ﬁve hidden units a` D
`D1 w.2/
g.w.1/
` a`/.
`0
The language associated with NNs is colorful: memory units or neurons
automatically learn new features from the data through a process called

`j xj /, and a single output unit o D h.w.2/

jD1 w.1/

351

x1x2x3x4f(x)HiddenlayerL2InputlayerL1OutputlayerL3352

Neural Networks

`j

supervised learning. Each neuron al is connected to the input layer via a
gp
vector of parameters or weights fw.1/
1 (the .1/ refers to the ﬁrst layer
and `j refers to the j th variable and `th unit). The intercept terms w.1/
`0
are called a bias, and the function g is a nonlinearity, such as the sigmoid
function g.t / D 1=.1 C e
(cid:0)t /. The idea was that each neuron will learn a
simple binary on/off function; the sigmoid function is a smooth and dif-
ferentiable compromise. The ﬁnal or output layer also has weights, and
an output function h. For quantitative regression h is typically the identity
function, and for a binary response it is once again the sigmoid. Note that
without the nonlinearity in the hidden layer, the neural network would re-
duce to a generalized linear model (Chapter 8). Typically neural networks
are ﬁt by maximum likelihood, usually with a variety of forms of regular-
ization.

The knee-jerk response from statisticians was “What’s the big deal? A
neural network is just a nonlinear model, not too different from many other
generalizations of linear models.”

While this may be true, neural networks brought a new energy to the
ﬁeld. They could be scaled up and generalized in a variety of ways: many
hidden units in a layer, multiple hidden layers, weight sharing, a variety
of colorful forms of regularization, and innovative learning algorithms for
massive data sets. And most importantly, they were able to solve problems
on a scale far exceeding what the statistics community was used to. This
was part computing scale and expertise, part liberated thinking and cre-
ativity on the part of this computer science community. New journals were
devoted to the ﬁeld,  and several popular annual conferences (initially at
ski resorts) attracted their denizens, and drew in members of the statistics
community.

After enjoying considerable popularity for a number of years, neural
networks were somewhat sidelined by new inventions in the mid 1990s,
such as boosting (Chapter 17) and SVMs (Chapter 19). Neural networks
were pass´e. But then they re-emerged with a vengeance after 2010—the
reincarnation now being called deep learning. This renewed enthusiasm is
a result of massive improvements in computer resources, some innovations,
and the ideal niche learning tasks such as image and video classiﬁcation,
and speech and text processing.

1

18.1 Neural Networks and the Handwritten Digit Problem

353

18.1 Neural Networks and the Handwritten Digit Problem

Neural networks really cut their baby teeth on an optical character recogni-
tion (OCR) task: automatic reading of handwritten digits, as in a zipcode.
Figure 18.2 shows some examples, taken from the MNIST corpus.  The 2
idea is to build a classiﬁer C.x/ 2 f0; 1; : : : ; 9g based on the input image
x 2 R28(cid:2)28, a 28 (cid:2) 28 grid of image intensities. In fact, as is often the
case, it is more useful to learn the probability function Pr.y D jjx/; j D
0; 1; 2; : : : ; 9; this is indeed the target for our neural network. Figure 18.3

Figure 18.2 Examples of handwritten digits from the MNIST
corpus. Each digit is represented by a 28 (cid:2) 28 grayscale image,
derived from normalized binary images of different shapes and
sizes. The value stored for each pixel in an image is a nonnegative
eight-bit representation of the amount of gray present at that
location. The 784 pixels for each image are the predictors, and the
0–9 class labels the response. There are 60,000 training images in
the full data set, and 10,000 in the test set.

shows a neural network with three hidden layers, a successful conﬁgura-
tion for this digit classiﬁcation problem. In this case the output layer has
10 nodes, one for each of the possible class labels. We use this example
to walk the reader through some of the aspects of the conﬁguration of a
network, and ﬁtting it to training data. Since all of the layers are functions
of their previous layers, and ﬁnally functions of the input vector x, the net-
work represents a somewhat complex function f .xI
W/, where W repre-
sents the entire collection of weights. Armed with a suitable loss function,
we could simply barge right in and throw it at our favorite optimizer. In the
early days this was not computationally feasible, especially when special

354

Neural Networks

Figure 18.3 Neural network diagram with three hidden layers
and multiple outputs, suitable for the MNIST handwritten-digit
problem. The input layer has p D 784 units. Such a network with
hidden layer sizes .1024; 1024; 2048/, and particular choices of
tuning parameters, achieves the state-of-the art error rate of
0:93% on the “ofﬁcial” test data set. This network has close to
four million weights, and hence needs to be heavily regularized.

structure is imposed on the weight vectors. Today there are fairly automatic
systems for setting up and ﬁtting neural networks, and this view is not too
far from reality. They mostly use some form of gradient descent, and rely
on an organization of parameters that leads to a manageable calculation of
the gradient.

The network in Figure 18.3 is complex, so it is essential to establish a
convenient notation for referencing the different sets of parameters. We
continue with the notation established for the single-layer network, but
with some additional annotations to distinguish aspects of different layers.
From the ﬁrst to the second layer we have

w.1/

`j xj ;

(18.1)

(18.2)

C pX

jD1

`0

D w.1/
D g.2/.z.2/
` /:

z.2/
`

a.2/
`

x1x2x3...xpy0y1...y9HiddenlayerL4HiddenlayerL3HiddenlayerL2InputlayerL1OutputlayerL5W(1)a(2)W(2)a(3)W(3)a(4)W(4)a(5)18.1 Neural Networks and the Handwritten Digit Problem

355

We have separated the linear transformations z.2/
` of the xj from the nonlin-
ear transformation of these, and we allow for layer-speciﬁc nonlinear trans-
formations g.k/. More generally we have the transition from layer k (cid:0) 1 to
layer k:

C pk(cid:0)1X

jD1

`0

D w.k(cid:0)1/
D g.k/.z.k/
` /:

z.k/
`

a.k/
`

w.k(cid:0)1/

`j

a.k(cid:0)1/

j

;

(18.3)

(18.4)

In fact (18.3)–(18.4) can serve for the input layer (18.1)–(18.2) if we adopt
 x` and p1 D p, the number of input variables.
the notation that a.1/
Hence each of the arrows in Figure 18.3 is associated with a weight param-
eter.

`

It is simpler to adopt a vector notation

z.k/ D W .k(cid:0)1/a.k(cid:0)1/
a.k/ D g.k/.z.k//;

(18.5)
(18.6)
where W .k(cid:0)1/ represents the matrix of weights that go from layer Lk(cid:0)1
to layer Lk, a.k/ is the entire vector of activations at layer Lk, and our
notation assumes that g.k/ operates elementwise on its vector argument.
We have also absorbed the bias parameters w.k(cid:0)1/
into the matrix W .k(cid:0)1/,
which assumes that we have augmented each of the activation vectors a.k/
with a constant element 1.

`0

Sometimes the nonlinearities g.k/ at the inner layers are the same func-
tion, such as the function (cid:27) deﬁned earlier. In Section 18.5 we present a
network for natural color image classiﬁcation, where a number of different
activation functions are used.
Depending on the response, the ﬁnal transformation g.K/ is usually spe-
cial. For M -class classiﬁcation, such as here with M D 10, one typically
uses the softmax function

g.K/.z.K/

m

I z.K// D

;

(18.7)

mPM

ez.K/
`D1 ez.K/

`

which computes a number (probability) between zero and one, and all M
of them sum to one.1

1 This is a symmetric version of the inverse link function used for multiclass logistic

regression.

356

Neural Networks

18.2 Fitting a Neural Network

As we have seen, a neural network model is a complex, hierarchical func-
tion f .xI
W/ of the the feature vector x, and the collection of weights W.
For typical choices for the g.k/, this function will be differentiable. Given
a training set fxi ; yign
1 and a loss function LŒy; f .x/, along familiar lines
we might seek to solve

)

LŒyi ; f .xiI

W/ C (cid:21)J.W/

;

(18.8)

(

nX

iD1

1

n

minimize

W

where J.W/ is a nonnegative regularization term on the elements of W,
and (cid:21) (cid:21) 0 is a tuning parameter. (In practice there may be multiple reg-
ularization terms, each with their own (cid:21).) For example an early popular
penalty is the quadratic

n

K(cid:0)1X

pkX

pkC1X

kD1

jD1

`D1

o2

J.W/ D 1

2

w.k/
`j

;

(18.9)

as in ridge regression (7.41). Also known as the weight-decay penalty, it
pulls the weights toward zero (typically the biases are not penalized). Lasso
penalties (Chapter 16) are also popular, as are mixtures of these (an elastic
net).

For binary classiﬁcation we could take L to be binomial deviance (8.14),
in which case the neural network amounts to a penalized logistic regres-
sion, Section 8.1, albeit a highly parametrized and penalized one. Loss
functions are usually convex in f , but not in the elements of W, so solving
(18.8) is difﬁcult, and at best we seek good local optima. Most methods
are based on some form of gradient descent, with many associated bells
and whistles. We brieﬂy discuss some elements of the current practice in
ﬁnding good solutions to (18.8).

Computing the Gradient: Backpropagation

The elements of W occur in layers, since f .xI

W/ is deﬁned as a series
of compositions, starting from the input layer. Computing the gradient is
also done most naturally in layers (the chain rule for differentiation; see
for example (18.10) in Algorithm 18.1 below), and our notation makes this
easier to describe in a recursive fashion. We will consider computing the
derivative of LŒy; f .xI
W with respect to any of the elements of W, for a
generic input–output pair x; y; since the loss part of the objective is a sum,

18.2 Fitting a Neural Network

357

`

the overall gradient will be the sum of these individual gradient elements
over the training pairs .xi ; yi /.

The intuition is as follows. Given a training generic pair .x; y/, we ﬁrst
make a forward pass through the network, which creates activations at each
of the nodes a.k/
in each of the layers, including the ﬁnal output layer. We
would then like to compute an error term ı.k/
that measures the responsibil-
ity of each node for the error in predicting the true output y. For the output
activations a.K/
these errors are easy: either residuals or generalized resid-
uals, depending on the loss function. For activations at inner layers, ı.k/
will be a weighted sum of the errors terms of nodes that use a.k/
as inputs.
The backpropagation Algorithm 18.1 gives the details for computing the
gradient for a single input–output pair x; y. We leave it to the reader to
verify that this indeed implements the chain rule for differentiation.

`

`

`

`

Algorithm 18.1 BACKPROPAGATION

at each of the layers L2; L3; : : : ; LK; i.e. compute f .xI

1 Given a pair x; y, perform a “feedforward pass,” computing the activa-
tions a.k/
W/ at
x using the current W, saving each of the intermediary quantities along
the way.

`

2 For each output unit ` in layer LK, compute
D @LŒy; f .x;W/
D @LŒy; f .xI

@z.K/

ı.K/
`

`

W/

Pg.K/.z.K/

(18.10)
where Pg denotes the derivative of g.z/ wrt z. For example for L.y; f / D
ky (cid:0) f k2
3 For layers k D K (cid:0) 1; K (cid:0) 2; : : : ; 2, and for each node ` in layer k, set

2, (18.10) becomes (cid:0).y` (cid:0) f`/ (cid:1) Pg.K/.z.K/

@a.K/

/.

/;

1
2

`

`

0@pkC1X

jD1

`

1A Pg.k/.z.k/

` /:

D

ı.k/
`

j ` ı.kC1/
w.k/

j

(18.11)

4 The partial derivatives are given by
W/

@LŒy; f .xI
@w.k/
`j

D a.k/

j ı.kC1/

`

:

(18.12)

One again matrix–vector notation simpliﬁes these expressions a bit:

358

Neural Networks

(18.10) becomes (for squared-error loss)

ı.K/ D (cid:0).y (cid:0) a.K// ı Pg.K/.z.K//;
where ı denotes the Hadamard (elementwise) product;
(18.11) becomes

ı.kC1/ ı Pg.k/.z.k//I

0

ı.k/ D(cid:16)

W .k/

(18.12) becomes

@LŒy; f .xI

@W .k/

W/

D ı.kC1/a.k/

0

:

(18.13)

(18.14)

(18.15)

Backpropagation was considered a breakthrough in the early days of
neural networks, since it made ﬁtting a complex model computationally
manageable.

Gradient Descent

Algorithm 18.1 computes the gradient of the loss function at a single generic
pair .x; y/; with n training pairs the gradient of the ﬁrst part of (18.8) is
given by

W

:

(18.16)

nX

iD1

@LŒyi ; f .xiI

@W .k/

n

W .k/ D 1
(cid:16)

W .k/ C (cid:21)W .k/

With the quadratic form (18.9) for the penalty, a gradient-descent update is

W .k/   W .k/ (cid:0) ˛

; k D 1; : : : ; K (cid:0) 1;

(18.17)

where ˛ 2 .0; 1 is the learning rate.

Gradient descent requires starting values for all the weights W. Zero is
not an option, because each layer is symmetric in the weights ﬂowing to the
different neurons, hence we rely on starting values to break the symmetries.
Typically one would use random starting weights, close to zero; random
uniform or Gaussian weights are common.

There are a multitude of “tricks of the trade” in ﬁtting or “learning” a
neural network, and many of them are connected with gradient descent.
Here we list some of these, without going into great detail.

Stochastic Gradient Descent

Rather than process all the observations before making a gradient step, it
can be more efﬁcient to process smaller batches at a time—even batches

18.2 Fitting a Neural Network

359

Figure 18.4 Training and test misclassiﬁcation error as a
function of the number of epochs of training, for the MNIST digit
classiﬁcation problem. The architecture for the network is shown
in Figure 18.3. The network was ﬁt using accelerated gradient
descent with adaptive rate control, a rectiﬁed linear activation
function, and dropout regularization (Section 18.5). The
horizontal broken line shows the error rate of a random forest
(Section 17.1). A logistic regression model (Section 8.1) achieves
only 0.072 (off the scale).

of size one! These batches can be sampled at random, or systematically
processed. For large data sets distributed on multiple computer cores, this
can be essential for reasons of efﬁciency. An epoch of training means that
all n training samples have been used in gradient steps, irrespective of how
they have been grouped (and hence how many gradient steps have been
made).

Accelerated Gradient Methods

The idea here is to allow previous iterations to build up momentum and
inﬂuence the current iterations. The iterations have the form

VtC1 D (cid:22)Vt (cid:0) ˛.Wt C (cid:21)Wt /;
WtC1 D

Wt C

VtC1;

(18.18)
(18.19)

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll020040060080010000.000.010.020.030.04EpochsMisclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllTrainTestTest − RFNeural Networks

360
using Wt to represent the entire collection of weights at iteration t. Vt is
a velocity vector that accumulates gradient information from previous iter-
ations, and is controlled by an additional momentum parameter (cid:22). When
correctly tuned, accelerated gradient descent can achieve much faster con-
vergence rates; however, tuning tends to be a difﬁcult process, and is typi-
cally done adaptively.

Rate Annealing

3

A variety of creative methods have been proposed to adapt the learning rate
to avoid jumping across good local minima. These tend to be a mixture of
principled approaches combined with ad-hoc adaptations that tend to work
well in practice.  Figure 18.4 shows the performance of our neural net
on the MNIST digit data. This achieves state-of-the art misclassiﬁcation
error rates on these data (just under 0.93% errors), and outperforms random
forests (2.8%) and a generalized linear model (7.2%). Figure 18.5 shows
the 93 misclassiﬁed digits.

Figure 18.5 All 93 misclassiﬁed digits in the MNIST test set.
The true digit class is labeled in blue, the predicted in red.

42536082589789657246729449957157837987469306379323945320374961909194246153956165329535971249603791645085724613460397273287896180947270495338383989977107958505398549727272089727476356425018.2 Fitting a Neural Network

361

Other Tuning Parameters

Apart from the many details associated with gradient descent, there are sev-
eral other important structural and operational aspects of neural networks
that have to be speciﬁed.

Number of Hidden Layers, and Their Sizes

With a single hidden layer, the number of hidden units determines the num-
ber of parameters. In principle, one could treat this number as a tuning
parameter, which could be adjusted to avoid overﬁtting. The current col-
lective wisdom suggests it is better to have an abundant number of hidden
units, and control the model complexity instead by weight regularization.
Having deeper networks (more hidden layers) increases the complexity as
well. The correct number tends to be task speciﬁc; having two hidden lay-
ers with the digit recognition problem leads to competitive performance.

Choice of Nonlinearities

There are a number of activation functions g.k/ in current use. Apart from
the sigmoid function, which transforms its input to a values in .0; 1/, other
popular choices are

Figure 18.6 Activation functions. ReLU is a rectiﬁed linear
(unit).

tanh:

g.z/ D ez (cid:0) e
(cid:0)z
ez C e
(cid:0)z

;

−2−1012−1.0−0.50.00.51.0zg(z)sigmoidtanhReLUleaky ReLU362
which delivers values in .(cid:0)1; 1/.

Neural Networks

rectiﬁed linear:

g.z/ D zC;

or the positive-part function. This has the advantage of making the gra-
dient computations cheaper to compute.

leaky rectiﬁed linear:

g˛.z/ D zC (cid:0) ˛z(cid:0);

for ˛ nonnegative and close to zero. The rectiﬁed linear tends to have ﬂat
spots, because of the many zero activations; this is an attempt to avoid
these and the accompanying zero gradients.

Choice of Regularization

Typically this is a mixture of `2 and `1 regularization, each of which re-
quires a tuning parameter. As in lasso and regression applications, the bias
terms (intercepts) are usually not regularized. The weight regularization
is typically light, and serves several roles. The `2 reduces problems with
collinearity, the `1 can ignore irrelevant features, and both slow the rate of
overﬁtting, especially with deep (over-parametrized) networks.

Early Stopping

Neural nets are typically over-parametrized, and hence are prone to overﬁt-
ting. Originally early stopping was set up as the primary tuning parameter,
and the stopping time was determined using a held-out set of validation
data. In modern networks the regularization is tuned adaptively to avoid
overﬁtting, and hence it is less of a problem. For example, in Figure 18.4
we see that the test misclassiﬁcation error has ﬂattened out, and does not
rise again with increasing number of epochs.

18.3 Autoencoders

An autoencoder is a special neural network for computing a type of non-
linear principal-component decomposition.

The linear principal component decomposition is a popular and effective
linear method for reducing a large set of correlated variables to a typically
smaller number of linear combinations that capture most of the variance in
the original set. Hence, given a collection of n vectors xi 2 Rp (assumed to
have mean zero), we produce a derived set of uncorrelated features zi 2 Rq

18.3 Autoencoders

363

0

(q  p, and typically smaller) via zi D V
xi. The columns of V are or-
thonormal, and are derived such that the ﬁrst component of zi has maximal
variance, the second has the next largest variance and is uncorrelated with
the ﬁrst, and so on. It is easy to show that the columns of V are the leading
q eigenvectors of the sample covariance matrix S D 1
n X

Principal components can also be derived in terms of a best-approx-
imating linear subspace, and it is this version that leads to the nonlinear
generalization presented here. Consider the optimization problem

X.

0

minimize
A2Rp(cid:2)q ; f(cid:13)ign

2Rq(cid:2)n

1

kxi (cid:0) A(cid:13)ik2
2;

(18.20)

nX

iD1

for q < p. The subspace is deﬁned by the column space of A, and for
each point xi we wish to locate its best approximation in the subspace (in
terms of Euclidean distance). Without loss of generality, we can assume A
has orthonormal columns, in which case O(cid:13)i D A
0
xi for each i (n separate
linear regressions). Plugging in, (18.20) reduces to
kxi (cid:0) AA

(18.21)
A solution is given by OA D V , the matrix above of the ﬁrst q principal-
component direction vectors computed from the xi. By analogy, a single-
layer autoencoder solves a nonlinear version of this problem:

nX

A2Rp(cid:2)q ; A

xik2
2:

minimize

ADIq

iD1

0

minimize
W 2Rq(cid:2)p

kxi (cid:0) W

0

g.W xi /k2
2;

(18.22)

0

nX

iD1

for some nonlinear activation function g; see Figure 18.7 (left panel). If g
is the identity function, these solutions coincide (with W D V

0).

Figure 18.7 (right panel) represents the learned row of W as images,
when the autoencoder is ﬁt to the MNIST digit database. Since autoen-
coders do not require a response (the class labels in this case), this decom-
position is unsupervised. It is often expensive to label images, for example,
while unlabeled images are abundant. Autoencoders provide a means for
extracting potentially useful features from such data, which can then be
used with labeled data to train a classiﬁer. In fact, they are often used as
warm starts for the weights when ﬁtting a supervised neural network.

Once again there are a number of bells and whistles that make autoen-

coders more effective.

364

Neural Networks

Figure 18.7 Left: Network representation of an autoencoder used
for unsupervised learning of nonlinear principal components. The
middle layer of hidden units creates a bottleneck, and learns
nonlinear representations of the inputs. The output layer is the
transpose of the input layer, so the network tries to reproduce the
input data using this restrictive representation. Right: Images
representing the estimated rows of W using the MNIST database;
the images can be seen as ﬁlters that detect local gradients in the
image pixels. In each image, most of the weights are zero, and the
nonzero weights are localized in the two-dimensional image
space.

(cid:15) `1 regularization applied to the rows of W lead to sparse weight vectors,
and hence local features, as was the case in our example.
(cid:15) Denoising is a process where noise is added to the input layer (but not the
output), resulting in features that do not focus on isolated values, such
as pixels, but instead have some volume. We discuss denoising further in
Section 18.5.
(cid:15) With regularization, the bottleneck is not necessary, as in the ﬁgure or in
principal components. In fact we can learn many more than p compo-
nents.
(cid:15) Autoencoders can also have multiple layers, which are typically learned
sequentially. The activations learned in the ﬁrst layer are treated as the
input (and output) features, and a model like (18.22) is ﬁt to them.

18.4 Deep Learning

Neural networks were reincarnated around 2010 with “deep learning” as a
ﬂashier name, largely a result of much faster and larger computing systems,
plus a few new ideas. They have been shown to be particularly successful

x1x2x3x4x5ˆx1ˆx2ˆx3ˆx4ˆx5InputlayerHiddenlayerOutputlayerWg(Wx)W018.4 Deep Learning

365

in the difﬁcult task of classifying natural images, using what is known as a
convolutional architecture. Initially autoencoders were considered a crucial
aspect of deep learning, since unlabeled images are abundant. However,
as labeled corpora become more available, the word on the street is that
supervised learning is sufﬁcient.

Figure 18.8 shows examples of natural images, each with a class label
such as beaver, sunflower, trout etc. There are 100 class labels in

Figure 18.8 Examples of natural images. The CIFAR-100
database consists of 100 color image classes, with 600 examples
in each class (500 train, 100 test). Each image is 32 (cid:2) 32 (cid:2) 3 (red,
green, blue). Here we display a randomly chosen image from each
class. The classes are organized by hierarchical structure, with 20
coarse levels and ﬁve subclasses within each. So, for example, the
ﬁrst ﬁve images in the ﬁrst column are aquatic mammals,
namely beaver, dolphin, otter, seal and whale.

366

Neural Networks

all, and 500 training images and 100 test images per class. The goal is to
build a classiﬁer to assign a label to an image. We present the essential
details of a deep-learning network for this task—one that achieves a re-
spectable classiﬁcation performance of 35% errors on the designated test
set.2 Figure 18.9 shows a typical deep-learning architecture, with many

Figure 18.9 Architecture of a deep-learning network for the
CIFAR-100 image classiﬁcation task. The input layer and
hidden layers are all represented as images, except for the last
hidden layer, which is “ﬂattened” (vectorized). The input layer
consists of the p1 D 3 color (red, green, and blue) versions of an
input image (unlike earlier, here we use the pk to refer to the
number of images rather than the totality of pixels). Each of these
color panes is 32 (cid:2) 32 pixels in dimension. The ﬁrst hidden layer
computes a convolution using a bank of p2 distinct q (cid:2) q (cid:2) p1
learned ﬁlters, producing an array of images of dimension
p2 (cid:2) 32 (cid:2) 32. The next pool layer reduces each non-overlapping
block of ` (cid:2) ` numbers in each pane of the ﬁrst hidden layer to a
single number using a “max” operation. Both q and ` are
typically small; each was 2 for us. These convolve and pool layers
are repeated here three times, with changing dimensions (in our
actual implementation, there are 13 layers in total). Finally the
500 derived features are ﬂattened, and a fully connected layer
maps them to the 100 classes via a “softmax” activation.

hidden layers. These consist of two special types of layers: “convolve” and
“pool.” We describe each in turn.

Convolve Layer

Figure 18.10 illustrates a convolution layer, and some details are given in

2 Classiﬁcation becomes increasingly difﬁcult as the number of classes grows. With equal
representation in each class, the NULL or random error rate for K classes is
.K (cid:0) 1/=K; 50% for two classes, 99% for 100.

100323221650048convolvepoolconvolvepoolconvolvepoolconnect fully367
the caption. If an image x is represented by a k (cid:2) k matrix, and a ﬁlter f

18.4 Deep Learning

Figure 18.10 Convolution layer for the input images. The input
image is split into its three color components. A single ﬁlter is a
q (cid:2) q (cid:2) p1 array (here one q (cid:2) q for each of the p1 D 3 color
panes), and is used to compute an inner product with a
correspondingly sized subimage in each pane, and summed across
the p1 panes. We used q D 2, and small values are typical. This is
repeated over all (overlapping) q (cid:2) q subimages (with boundary
padding), and hence produces an image of the same dimension as
one of the input panes. This is the convolution operation. There
are p2 different versions of this ﬁlter, and hence p2 new panes are
produced. Each of the p2 ﬁlters has p1q2 weights, which are
learned via backpropagation.

0D1 xiC`; jC`

`

0 f`; `

`D1

Pq

Qx with elements Qxi; j DPq

is a q (cid:2) q matrix with q (cid:28) k, the convolved image is another k (cid:2) k matrix
0 (with edge padding to
achieve a full-sized k (cid:2) k output image). In our application we used 2 (cid:2) 2,
but other sizes such as 3 (cid:2) 3 are popular. It is most natural to represent
the structure in terms of these images as in Figure 18.9, but they could all
be vectorized into a massive network diagram as in Figures 18.1 and 18.3.
However, the weights would have special sparse structure, with most being
zero, and the nonzero values repeated (“weight sharing”).

++.........368

Neural Networks

Pool Layer

The pool layer corresponds to a kind of nonlinear activation. It reduces
each nonoverlapping block of r(cid:2)r pixels (r D 2 for us) to a single number
by computing their maximum. Why maximum? The convolution ﬁlters are
themselves small image patches, and are looking to identify similar patches
in the target image (in which case the inner product will be high). The max
operation introduces an element of local translation invariance. The pool
operation reduces the size of each image by a factor r in each dimension.
To compensate, the number of tiles in the next convolution layer is typically
increased accordingly. Also, as these tiles get smaller, the effective weights
resulting from the convolution operator become denser. Eventually the tiles
are the same size as the convolution ﬁlter, and the layer becomes fully
connected.

18.5 Learning a Deep Network

Despite the additional structure imposed by the convolution layers, deep
networks are learned by gradient descent. The gradients are computed by
backpropagation as before, but with special care taken to accommodate the
tied weights in the convolution ﬁlters. However, a number of additional
tricks have been introduced that appear to improve the performance of
modern deep learning networks. These are mostly aimed at regularization;
indeed, our 100-class image network has around 50 million parameters, so
regularization is essential to avoid overﬁtting. We brieﬂy discuss some of
these.

Dropout

This is a form of regularization that is performed when learning a network,
typically at different rates at the different layers. It applies to all networks,
not just convolutional; in fact, it appears to work better when applied at the
deeper, denser layers. Consider computing the activation z.k/
in layer k as
in (18.3) for a single observation during the feed-forward stage. The idea
is to randomly set each of the pk(cid:0)1 nodes a.k(cid:0)1/
to zero with probability
(cid:30), and inﬂate the remaining ones by a factor 1=.1(cid:0) (cid:30)/. Hence, for this ob-
servation, those nodes that survive have to stand in for those omitted. This
can be shown to be a form of ridge regularization, and when done correctly
improves performance.  The fraction (cid:30) omitted is a tuning parameter, and
for convolutional networks it appears to be better to use different values at

`

j

4

18.5 Learning a Deep Network

369

different layers. In particular, as the layers become denser, (cid:30) is increased:
from 0 in the input layer to 0:5 in the ﬁnal, fully connected layer.

Input Distortion

This is another form of regularization that is particularly suitable for tasks
like image classiﬁcation. The idea is to augment the training set with many
distorted copies of an input image (but of course the same label). These
distortions can be location shifts and other small afﬁne transformations, but
also color and shading shifts that might appear in natural images. We show

Figure 18.11 Each column represents distorted versions of an
input image, including afﬁne and color distortions. The input
images are padded on the boundary to increase the size, and
hence allow space for some of the distortions.

some distorted versions of input images in Figure 18.11. The distortions are
such that a human would have no trouble identifying any of the distorted
images if they could identify the original.  This both enriches the training 5
data with hints, and also prevents overﬁtting to the original image. One
could also apply distortions to a test image, and then “poll” the results to
produce a ﬁnal classiﬁcation.

Conﬁguration

Designing the correct architecture for a deep-learning network, along with
the various choices at each layer, appears to require experience and trial

370

Neural Networks

6

and error. We summarize the third and ﬁnal architecture which we built
for classifying the CIFAR-100 data set in Algorithm 18.2.In addition to
these size parameters for each layer, we must select the activation functions
and additional regularization. In this case we used the leaky rectiﬁed linear
functions g˛.z/ (Section 18.2), with ˛ increasing from 0:05 in layer 5 up to
0:5 in layer 13. In addition a type of `2 regularization was imposed on the
weights, restricting all incoming weight vectors to a node to have `2 norm
bounded by one. Figure 18.12 shows both the progress of the optimization
objective (red) and the test misclassiﬁcation error (blue) as the gradient-
descent algorithm proceeds. The accelerated gradient method maintains a
memory, which we can see was restarted twice to get out of local minima.
Our network achieved a test error rate of 35% on the 10,000 test images
(100 images per class). The best reported error rate we have seen is 25%,
so apparently we have some way to go!

Figure 18.12 Progress of the algorithm as a function of the
number of epochs. The accelerated gradient algorithm is
“restarted” every 100 epochs, meaning the long-term memory is
forgotten, and a new trail is begun, starting at the current solution.
The red curve shows the objective (negative penalized
log-likelihood on the training data). The blue curve shows test-set
misclassiﬁcation error. The vertical axis is on the log scale, so
zero cannot be included.

050100150200250300405060708090EpochTest Misclassification Error3001900390063009200ObjectiveObjective CostMisclassification Error18.6 Notes and Details

371

Algorithm 18.2 CONFIGURATION PARAMETERS FOR DEEP-LEARNING
NETWORK USED ON THE CIFAR-100 DATA.
Layer 1: 100 convolution maps each with 2 (cid:2) 2 (cid:2) 3 kernel (the 3 for three
colors). The input image is padded from 32 (cid:2) 32 to 40 (cid:2) 40 to accom-
modate input distortions.
Layers 2 and 3: 100 convolution maps each 2 (cid:2) 2 (cid:2) 100. Compositions of
convolutions are roughly equivalent to convolutions with a bigger band-
width, and the smaller ones have fewer parameters.
Layer 4: Max pool 2 (cid:2) 2 layer, pooling nonoverlapping 2 (cid:2) 2 blocks of
pixels, and hence reducing the images to size 20 (cid:2) 20.
Layer 5: 300 convolution maps each 2 (cid:2) 2 (cid:2) 100, with dropout learning
with rate (cid:30)5 D 0:05.
Layer 6: Repeat of Layer 5.
Layer 7: Max pool 2 (cid:2) 2 layer (down to 10 (cid:2) 10 images).
Layer 8: 600 convolution maps each 2 (cid:2) 2 (cid:2) 300, with dropout rate (cid:30)8 D
0:10.
Layer 9: 800 convolution maps each 2 (cid:2) 2 (cid:2) 600, with dropout rate (cid:30)9 D
0:10.
Layer 10: Max pool 2 (cid:2) 2 layer (down to 5 (cid:2) 5 images).
Layer 11: 1600 convolution maps, each 1 (cid:2) 1 (cid:2) 800. This is a pixelwise
weighted sum across the 800 images from the previous layer.
Layer 12: 2000 fully connected units, with dropout rate (cid:30)12 D 0:25.
Layer 13: Final 100 output units, with softmax activation, and dropout rate
(cid:30)13 D 0:5.

18.6 Notes and Details

The reader will notice that probability models have disappeared from the
development here. Neural nets are elaborate regression methods aimed
solely at prediction—not estimation or explanation in the language of Sec-
tion 8.4. In place of parametric optimality criteria, the machine learning
community has focused on a set of speciﬁc prediction data sets, like the
digits MNIST corpus and CIFAR-100, as benchmarks for measuring per-
formance.

There is a vast literature on neural networks, with hundreds of books and
thousands of papers. With the resurgence of deep learning, this literature is
again growing. Two early statistical references on neural networks are Rip-
ley (1996) and Bishop (1995), and Hastie et al. (2009) devote one chapter
to the topic. Part of our description of backpropagation in Section 18.2 was

372

Neural Networks

guided by Andrew Ng’s online Stanford lecture notes (Ng, 2015). Bengio
et al. (2013) provide a useful review of autoencoders. LeCun et al. (2015)
give a brief overview of deep learning, written by three pioneers of this
ﬁeld: Yann LeCun, Yoshua Bengio and Geoffrey Hinton; we also bene-
ﬁted from reading Ngiam et al. (2010). Dropout learning (Srivastava et al.,
2014) is a relatively new idea, and its connections with ridge regression
were most usefully described in Wager et al. (2013). The most popular
version of accelerated gradient descent is due to Nesterov (2013). Learn-
ing with hints is due to Abu-Mostafa (1995). The material in Sections 18.4
and 18.5 beneﬁted greatly from discussions with Rakesh Achanta (Achanta
and Hastie, 2015), who produced some of the color images and diagrams,
and designed and ﬁt the deep-learning network to the CIFAR-100 data.

1 [p. 352] The Neural Information Processing Systems (NIPS) conferences
started in late Fall 1987 in Denver, Colorado, and post-conference work-
shops were held at the nearby ski resort at Vail. These are still very popular
today, although the venue has changed over the years. The NIPS proceed-
ings are refereed, and NIPS papers count as publications in most ﬁelds,
especially Computer Science and Engineering. Although neural networks
were initially the main topic of the conferences, a modern NIPS conference
covers all the latest ideas in machine learning.

2 [p. 353] MNIST is a curated database of images of handwritten digits
(LeCun and Cortes, 2010). There are 60,000 training images, and 10,000
test images, each a 28 (cid:2) 28 grayscale image. These data have been used as
a testbed for many different learning algorithms, so the reported best error
rates might be optimistic.

3 [p. 360] Tuning parameters. Typical neural network implementations have
dozens of tuning parameters, and many of these are associated with the ﬁne
tuning of the descent algorithm. We used the h2o.deepLearning func-
tion in the R package h2o to ﬁt our model for the MNIST data set. It has
around 20 such parameters, although most default to factory-tuned con-
stants that have been found to work well on many examples. Arno Candel
was very helpful in assisting us with the software.

4 [p. 368] Dropout and ridge regression. Dropout was originally proposed in
Srivastava et al. (2014), and reinterpreted in Wager et al. (2013). Dropout
was inspired by the random selection of variables at each tree split in a
random forest (Section 17.1). Consider a simple version of dropout for
the linear regression problem with squared-error loss. We have an n (cid:2) p
regression matrix X, and a response n-vector y. For simplicity we assume
all variables have mean zero, so we can ignore intercepts. Consider the

following random least-squares criterion:

18.6 Notes and Details

0@yi (cid:0) pX

jD1

nX

iD1

373

1A2

:

xij Iij ˇj

LI .ˇ/ D 1

2

Iij D(cid:26)
 @LI .ˇ/
(cid:21) D (cid:0)X
ˇ D

O

Here the Iij are i.i.d variables 8i; j with

0 with probability (cid:30);

1=.1 (cid:0) (cid:30)/ with probability 1 (cid:0) (cid:30);

(this particular form is used so that EŒIij  D 1). Using simple probability
it can be shown that the expected score equations can be written

E

(18.23)
with D D diagfkx1k2;kx2k2; : : : ;kxpk2g. Hence the solution is given by

@ˇ

0

y C X

0

Dˇ D 0;

X ˇ C (cid:30)
1 (cid:0) (cid:30)
(cid:0)1

0

X C (cid:30)
1 (cid:0) (cid:30)

X

D

X

0

y;

(18.24)

a generalized ridge regression. If the variables are standardized, the term
D becomes a scalar, and the solution is identical to ridge regression. With
a nonlinear activation function, the interpretation changes slightly; see Wa-
ger et al. (2013) for details.

5 [p. 369] Distortion and ridge regression. We again show in a simple ex-
ample that input distortion is similar to ridge regression. Assume the same
setup as in the previous example, except a different randomized version of
the criterion:

LN .ˇ/ D 1

2

.xij C nij /ˇj

1A2

:

iD1

nX

0@yi (cid:0) pX
(cid:21) D (cid:0)X

jD1

0

 @LN .ˇ/

Here we have added random noise to the prediction variables, and we as-
sume this noise is i.i.d .0; (cid:21)/. Once again the expected score equations can
be written

y C X

0

X ˇ C (cid:21)ˇ D 0;

E

@ˇ

(18.25)
ij / D (cid:21). Once again
because of the independence of all the nij and E.n2
this leads to a ridge regression. So replacing each observation pair xi ; yi
by the collection fx
is a noisy version of xi, is
approximately equivalent to a ridge regression on the original data.

bD1, where each x

; yigB

(cid:3)b

(cid:3)b

i

i

374

Neural Networks

6 [p. 370] Software for deep learning. Our deep learning convolutional net-
work for the CIFAR-100 data was constructed and run by Rakesh Achanta
in Theano, a Python-based system (Bastien et al., 2012; Bergstra et al.,
2010). Theano has a user-friendly language for specifying the host of
parameters for a deep-learning network, and uses symbolic differentiation
for computing the gradients needed in stochastic gradient descent. In 2015
Google announced an open-source version of their TensorFlow software
for ﬁtting deep networks.

19

Support-Vector Machines and Kernel

Methods

While linear logistic regression has been the mainstay in biostatistics and
epidemiology, it has had a mixed reception in the machine-learning com-
munity. There the goal is often classiﬁcation accuracy, rather than statistical
one ifbPr.Y D 1jX D x/ (cid:21) 0:5. SVMs bypass the ﬁrst step, and build a
inference. Logistic regression builds a classiﬁer in two steps: ﬁt a condi-
tional probability model for Pr.Y D 1jX D x/, and then classify as a

classiﬁer directly.

Another rather awkward issue with logistic regression is that it fails if
the training data are linearly separable! What this means is that, in the
feature space, one can separate the two classes by a linear boundary. In
cases such as this, maximum likelihood fails and some parameters march
off to inﬁnity. While this might have seemed an unlikely scenario to the
early users of logistic regression, it becomes almost a certainty with mod-
ern wide genomics data. When p (cid:29) n (more features than observations),
we can typically always ﬁnd a separating hyperplane. Finding an optimal
separating hyperplane was in fact the launching point for SVMs. As we
will see, they have more than this to offer, and in fact live comfortably
alongside logistic regression.

SVMs pursued an age-old approach in statistics, of enriching the feature
space through nonlinear transformations and basis expansions; a classical
example being augmenting a linear regression with interaction terms. A
linear model in the enlarged space leads to a nonlinear model in the ambient
space. This is typically achieved via the “kernel trick,” which allows the
computations to be performed in the n-dimensional space for an arbitrary
number of predictors p. As the ﬁeld matured, it became clear that in fact
this kernel trick amounted to estimation in a reproducing-kernel Hilbert
space.

Finally, we contrast the kernel approach in SVMs with the nonparame-

teric regression techniques known as kernel smoothing.

375

376

SVMs and Kernel Methods

19.1 Optimal Separating Hyperplane

Figure 19.1 shows a small sample of points in R2, each belonging to one of
two classes (blue or orange). Numerically we would score these classes as
y D C1 for say blue, and y D (cid:0)1 for orange.1 We deﬁne a two-class lin-
ear classiﬁer via a function f .x/ D ˇ0 C x
ˇ, with the convention that we
classify a point x0 as +1 if f .x0/ > 0, and as -1 if f .x0/ < 0 (on the fence
we ﬂip a coin). Hence the classiﬁer itself is C.x/ D signŒf .x/. The deci-

0

Figure 19.1 Left panel: data in two classes in R2. Three potential
decision boundaries are shown; each separate the data perfectly.
Right panel: the optimal separating hyperplane (a line in R2)
creates the biggest margin between the two classes.

sion boundary is the set fx j f .x/ D 0g. We see three different classiﬁers
in the left panel of Figure 19.1, and they all classify the points perfectly.
The optimal separating hyperplane is the linear classiﬁer that creates the
largest margin between the two classes, and is shown in the right panel
(it is also known as an optimal-margin classiﬁer). The underlying hope is
that, by making a big margin on the training data, it will also classify future
observations well.

Some elementary geometry  shows that the (signed) Euclidean distance

from a point x0 to the linear decision boundary deﬁned by f is given by

1

1kˇk2

f .x0/:

With this in mind, for a separating hyperplane the quantity 1kˇk2
1 In this chapter, the ˙1 scoring leads to convenient notation.

(19.1)

yi f .xi / is

−10123−10123X1X2llllllllllllllllllllllll−10123−10123X1X2llllllllllllllllllllllllllllllllllllllllllllllll19.1 Optimal Separating Hyperplane

377

the distance of xi from the decision boundary.2 This leads to an optimiza-
tion problem for creating the optimal margin classiﬁer:

maximize

ˇ0; ˇ

M

subject to

1kˇk2

yi .ˇ0 C x

0

ˇ/ (cid:21) M; i D 1; : : : ; n:

A rescaling argument reduces this to the simpler form

kˇk2

minimize
subject to yi .ˇ0 C x

ˇ0; ˇ

0

ˇ/ (cid:21) 1; i D 1; : : : ; n:

(19.2)

(19.3)

(19.4)

This is a quadratic program, which can be solved by standard techniques
in convex optimization. One noteworthy property of the solution is that

2

ˇ DX

O

i2
S

O˛i xi ;

S

where S is the support set. We can see in Figure 19.1 that the margin
touches three points (vectors); in this case there are j
j D 3 support vec-
tors, and clearly the orientation of O
ˇ is determined by them. However, we
still have to solve the optimization problem to identify the three points
i 2
in S, and their coefﬁcients ˛i ;
S. Figure 19.2 shows an optimal-
margin classiﬁer ﬁt to wide data, that is data where p (cid:29) n. These are
gene-expression measurements on p D 3571 genes measured on blood
samples from n D 72 leukemia patients (ﬁrst seen in Chapter 1). They
were classiﬁed into two classes, 47 acute lymphoblastic leukemia (ALL)
and 25 myeloid leukemia (AML). In cases like this, we are typically guar-
anteed a separating hyperplane3. In this case 42 of the 72 points are support
points. One might be justiﬁed in thinking that this solution is overﬁt to this
small amount of data. Indeed, when broken into a training and test set, we
see that the test data encroaches well into the margin region, but in this
case none are misclassiﬁed. Such classiﬁers are very popular in the wide-
data world of genomics, largely because they seem to work very well. They
offer a simple alternative to logistic regression, in a situation where the lat-
ter fails. However, sometimes the solution is overﬁt, and a modiﬁcation is
called for. This same modiﬁcation takes care of nonseparable situations as
well.
2 Since all the points are correctly classiﬁed, the sign of f .xi / agrees with yi , hence this
quantity is always positive.
3 If n  p C 1 we can always ﬁnd a separating hyperplane, unless there are exact feature
ties across the class barrier!

378

SVMs and Kernel Methods

Figure 19.2 Left panel: optimal margin classiﬁer ﬁt to
leukemia data. There are 72 observations from two
classes—47 ALL and 25 AML—and 3571 gene-expression
variables. Of the 72 observations, 42 are support vectors, sitting
on the margin. The points are plotted against their ﬁtted classiﬁer
function O
component of the data (chosen for display purposes, since it has
low correlation with the former). Right panel: here the optimal
margin classiﬁer was ﬁt to a random subset of 50 of the 72
observations, and then used to classify the remaining 22 (shown
in color). Although these points fall on the wrong sides of their
respective margins, they are all correctly classiﬁed.

f .x/, labeled SVM projection, and the ﬁfth principal

19.2 Soft-Margin Classiﬁer

Figure 19.3 shows data in R2 that are not separable. The generalization to
a soft margin allows points to violate their margin. Each of the violators
has a line segment connecting it to its margin, showing the extent of the
violation. The soft-margin classiﬁer solves

kˇk2

ˇ0; ˇ

minimize
subject to yi .ˇ0 C x
(cid:15)i (cid:21) 0; i D 1; : : : ; n; and

nX
i ˇ/ (cid:21) 1 (cid:0) (cid:15)i ;
0

(19.5)

(cid:15)i  B:

iD1

Here B is the budget for the total amount of overlap. Once again, the solu-
tion has the form (19.4), except now the support set S includes any vectors
on the margin as well as those that violate the margin. The bigger B, the

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1.5−1.0−0.50.00.51.01.5−0.2−0.10.00.10.20.3Leukemia: All DataSVM ProjectionPCA 5 Projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1.5−1.0−0.50.00.51.0−0.2−0.10.00.10.20.3Leukemia: Train and TestSVM ProjectionPCA 5 Projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllll19.3 SVM Criterion as Loss Plus Penalty

379

Figure 19.3 For data that are not separable, such as here, the
soft-margin classiﬁer allows margin violations. The budget B for
the total measure of violation becomes a tuning parameter. The
bigger the budget, the wider the soft margin and the more support
points there are involved in the ﬁt.

bigger the support set, and hence the more points that have a say in the
solution. Hence bigger B means more stability and lower variance. In fact,
even for separable data, allowing margin violations via B lets us regularize
the solution by tuning B.

19.3 SVM Criterion as Loss Plus Penalty

It turns out that one can reformulate (19.5) and (19.3) in more traditional
terms as the minimization of a loss plus a penalty:

nX
Œ1 (cid:0) yi .ˇ0 C x

ˇ0; ˇ

iD1

i ˇ/C C (cid:21)kˇk2
0
2:

minimize

(19.6)
Here the hinge loss LH .y; f .x// D Œ1 (cid:0) yf .x/C operates on the margin
quantity yf .x/, and is piecewise linear as in Figure 19.4.The same margin 3
quantity came up in boosting in Section 17.4. The quantity Œ1 (cid:0) yi .ˇ0 C
0
i ˇ/C is the cost for xi being on the wrong side of its margin (the cost
x
is zero if it’s on the correct side). The correspondence between (19.6) and
(19.5) is exact; large (cid:21) corresponds to large B, and this formulation makes
explicit the form of regularization. For separable data, the optimal separat-
ing hyperplane solution (19.3) corresponds to the limiting minimum-norm
solution as (cid:21) # 0. One can show that the population minimizer of the

−2−101234−101234X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2−101234−101234X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll380

SVMs and Kernel Methods

Figure 19.4 The hinge loss penalizes observation margins yf .x/
less than C1 linearly, and is indifferent to margins greater than
C1. The negative binomial log-likelihood (deviance) has the
same asymptotes, but operates in a smoother fashion near the
elbow at yf .x/ D 1.

4

hinge loss is in fact the Bayes classiﬁer.4 This shows that the SVM is in
fact directly estimating the classiﬁer C.x/ 2 f(cid:0)1;C1g.
The red curve in Figure 19.4 is (half) the binomial deviance for logistic
regression (i.e. f .x/ D ˇ0 C x
ˇ is now modeling logit Pr.Y D C1jX D
x/). With Y D ˙1, the deviance can also be written in terms of the margin,
nX
and the ridged logistic regression corresponding to (19.6) has the form

(cid:0)yi .ˇ0Cx

0

logŒ1 C e

0

i ˇ / C (cid:21)kˇk2
2:

minimize

ˇ0; ˇ

iD1

(19.7)

Logistic regression is discussed in Section 8.1, as well as Sections 16.5 and
17.4. This form of the binomial deviance is derived in (17.13) on page 343.
These loss functions have some features in common, as can be seen in the
ﬁgure. The binomial loss asymptotes to zero for large positive margins, and
to a linear loss for large negative margins, matching the hinge loss in this
regard. The main difference is that the hinge has a sharp elbow at +1, while
the binomial bends smoothly. A consequence of this is that the binomial
solution involves all the data, via weights pi .1 (cid:0) pi / that fade smoothly
with distance from the decision boundary, as apposed to the binary nature

4 The Bayes classiﬁer C.x/ for a two-class problem using equal costs for
misclassiﬁcation errors assigns x to the class for which Pr.yjx/ is largest.

−3−2−101230.00.51.01.52.02.53.0yf(x)LossBinomialSVM19.4 Computations and the Kernel Trick

381

Pr.y D C1jx/



of support points. Also, as seen in Section 17.4 as well, the population
minimizer of the binomial deviance is the logit of the class probability

(cid:21).x/ D log

;

Pr.y D (cid:0)1jx/

(19.8)
while that of the hinge loss is its sign C.x/ D signŒ(cid:21).x/. Interestingly,
as (cid:21) # 0 the solution direction O
ˇ to the ridged logistic regression prob-
lem (19.7) converges to that of the SVM.
These forms immediately suggest other generalizations of the linear
SVM. In particular, we can replace the ridge penalty kˇk2
2 by the sparsity-
inducing lasso penalty kˇk1, which will set some coefﬁcients to zero and
hence perform feature selection. Publicly available software (e.g. package
liblineaR in R) is available for ﬁtting such lasso-regularized support-
vector classiﬁers.

5

19.4 Computations and the Kernel Trick

The form of the solution O
O˛i xi for the optimal- and soft-margin
classiﬁer has some important consequences. For starters, we can write the
ﬁtted function evaluated at a point x as
ˇ0 C x

i2
S

O˛ihx; xii;

(19.9)

ˇ DP
f .x/ D O
O
D O

0 O
ˇ

ˇ0 CX

i2
S

where we have deliberately replaced the transpose notation with the more
suggestive inner product. Furthermore, we show in (19.23) in Section 19.9
that the Lagrange dual involves the data only through the n2 pairwise inner
products hxi ; xji (the elements of the n(cid:2) n gram matrix XX
0). This means
that the computations for computing the SVM solution scale linearly with
p, although potentially cubic5 in n. With very large p (in the tens of thou-
sands and even millions as we will see), this can be convenient.

It turns out that all ridge-regularized linear models with wide data can

be reparametrized in this way. Take ridge regression, for example:

ˇ

minimize
ˇ D .X

ky (cid:0) X ˇk2
X C (cid:21)Ip/
(cid:0)1X

2

C (cid:21)kˇk2
2:
0

This has solution O
y, and with p large requires
inversion of a p (cid:2) p matrix. However, it can be shown that O
0 O˛ D
5 In practice O.n2jSj/, and, with modern approximate solutions, much faster than that.

ˇ D X

0

(19.10)

6

7

382

0 C (cid:21)In/

SVMs and Kernel Methods

ˇ has the same form as for the SVM.

Pn
O˛i xi, with O˛ D .XX
(cid:0)1y, which means the solution can
iD1
be obtained in O.n2p/ rather than O.np2/ computations. Again the gram
matrix has played a role, and O
We now imagine expanding the p-dimensional feature vector x into a
potentially much larger set h.x/ D Œh1.x/; h2.x/; : : : ; hm.x/; for an ex-
ample to latch onto, think polynomial basis of total degree d. As long as we
have an efﬁcient way to compute the inner products hh.x/; h.xj /i for any
x, we can compute the SVM solution in this enlarged space just as easily
as in the original. It turns out that convenient kernel functions exist that do
just that. For example Kd .x; z/ D .1 C hx; zi/d creates a basis expansion
hd of polynomials of total degree d, and Kd .x; z/ D hhd .x/; hd .z/i.
The polynomial kernels are mainly useful as existence proofs; in practice
other more useful kernels are used. Probably the most popular is the radial
kernel

K.x; z/ D e

(cid:0)(cid:13)kx(cid:0)zk2
2 :

(19.11)

This is a positive deﬁnite function, and can be thought of as computing an
inner product in some feature space. Here the feature space is in principle
inﬁnite-dimensional, but of course effectively ﬁnite.6 Now one can think
of the representation (19.9) in a different light;

O˛i K.x; xi /;

(19.12)

f .x/ D O˛0 CX

O

i2
S

an expansion of radial basis functions, each centered on one of the train-
ing examples. Figure 19.5 illustrates such an expansion in R1. Using such
nonlinear kernels expands the scope of SVMs considerably, allowing one
to ﬁt classiﬁers with nonlinear decision boundaries.

One may ask what objective is being optimized when we move to this
kernel representation. This is covered in the next section, but as a sneak
preview we present the criterion

"

nX

1 (cid:0) yj

 
˛0 C nX

!#

C (cid:21)˛

0

C

˛0; ˛

K ˛;

iD1

jD1

(19.13)

˛i K.xj ; xi /

minimize
where the n (cid:2) n matrix K has entries K.xj ; xi /.
As an illustrative example in R2 (so we can visualize the nonlinear
boundaries), we generated the data in Figure 19.6. We show two SVM
6 A bivariate function K.x; z/ (Rp (cid:2) Rp 7! R1) is positive-deﬁnite if, for every q,
every q (cid:2) q matrix K D fK.xi ; xj /g formed using distinct entries x1; x2; : : : ; xq is
positive deﬁnite. The feature space is deﬁned in terms of the eigen-functions of the
kernel.

19.4 Computations and the Kernel Trick

383

Figure 19.5 Radial basis functions in R1. The left panel shows a
collection of radial basis functions, each centered on one of the
seven observations. The right panel shows a function obtained
from a particular linear expansion of these basis functions.

solutions, both using a radial kernel. In the left panel, some margin errors
are committed, but the solution looks reasonable. However, with the ﬂex-
ibility of the enlarged feature space, by decreasing the budget B we can
typically overﬁt the training data, as is the case in the right panel. A sepa-
rate little blue island was created to accommodate the one blue point in a
sea of brown.

Figure 19.6 Simulated data in two classes in R2, with SVM
classiﬁers computed using the radial kernel (19.11). The left
panel uses a larger value of B than the right. The solid lines are
the decision boundaries in the original space (linear boundaries in
the expanded feature space). The dashed lines are the projected
margins in both cases.

−2−10120.00.51.01.5−2−1012−0.40.00.20.4RadialBasisFunctionsf(x)K(x,xj)f(x)=α0+PjαjK(x,xj)xx−4−2024−4−2024X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll        llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−2024−4−2024X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                    lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll384

SVMs and Kernel Methods

19.5 Function Fitting Using Kernels

The analysis in the previous section is heuristic—replacing inner products
by kernels that compute inner products in some (implicit) feature space.
Indeed, this is how kernels were ﬁrst introduced in the SVM world. There
is however a rich literature behind such approaches, which goes by the
name function ﬁtting in reproducing-kernel Hilbert spaces (RKHSs). We
give a very brief overview here. One starts with a bivariate positive-deﬁnite
kernel K W Rp (cid:2) Rp ! R1, and we consider a space HK of functions
f W Rp ! R1 generated by the kernel: f 2 spanfK.(cid:1); z/; z 2 Rpg7 The
kernel also induces a norm on the space kf k
HK , which can be thought of
as a roughness measure.
We can now state a very general optimization problem for ﬁtting a func-

tion to data, when restricted to this class;

8

)

L.yi ; ˛0 C f .xi // C (cid:21)kf k2
HK

;

(19.14)

( nX

iD1

minimize

f 2

HK

a search over a possibly inﬁnite-dimensional function space. Here L is an
arbitrary loss function. The “magic” of these spaces in the context of this
problem is that one can show that the solution is ﬁnite-dimensional:

f .x/ D nX

O

O˛i K.x; xi /;

iD1

(19.15)
a linear basis expansion with basis functions ki .x/ D K.x; xi / anchored
at each of the observed “vectors” xi in the training data. Moreover, using
the “reproducing” property of the kernel in this space, one can show that
the penalty reduces to
k O
f k2
HK

(19.16)
Here K is the n(cid:2) n gram matrix of evaluations of the kernel, equivalent to
the XX

0 matrix for the linear case.

O˛i O˛j K.xi ; xj / D O˛

D nX

nX

iD1

jD1

0

K O˛:

Hence the abstract problem (19.14) reduces to the generalized ridge

problem

minimize

˛2Rn

8<: nX

iD1

0@yi ; ˛0 C nX

jD1

L

1A C (cid:21)˛

0

K ˛

9=; :

˛i K.xi ; xj /

(19.17)

7 Here kz D K.(cid:1); z/ is considered a function of the ﬁrst argument, and the second
argument is a parameter.

19.6 Example: String Kernels for Protein Classiﬁcation

385

Indeed, if L is the hinge loss as in (19.6), this is the equivalent “loss plus
penalty” criterion being ﬁt by the kernel SVM. Alternatively, if L is the bi-
nomial deviance loss as in (19.7), this would ﬁt a kernel version of logistic
regression. Hence most ﬁtting methods can be generalized to accommodate
kernels.

This formalization opens the door to a wide variety of applications, de-
pending on the kernel function used. Alternatively, as long as we can com-
pute suitable similarities between objects, we can build sophisticated clas-
siﬁers and other models for making predictions about other attributes of
the objects.8 In the next section we consider a particular example.

19.6 Example: String Kernels for Protein Classiﬁcation

One of the important problems in computational biology is to classify pro-
teins into functional and structural classes based on their sequence simi-
larities. Protein molecules can be thought of as strings of amino acids, and
differ in terms of length and composition. In the example we consider, the
lengths vary between 75 and 160 amino-acid molecules, each of which can
be one of 20 different types, labeled using the letters of the alphabet.

Here follow two protein examples x1 and x2, of length 110 and 153

respectively:

IPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV
ERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDYLQEFLGVMNTEWI

PHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAERLQENLQAYRTFHVLLA
RLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK
LWGLKVLQELSQWTVRSIHDLRFISSHQTGIP

We treat the proteins x as documents consisting of letters, with a dictio-
nary of size 20. Our feature vector hm.x/ will consist of the counts for all
m-grams in the protein—that is, distinct sequences of consecutive letters of
length m. As an illustration, we use m D 3, which results in 203 D8,000
possible sub-sequences; hence h3.x/ will be a vector of length 8,000, with
each element the number of times that particular sub-sequence occurs in
the protein x. In our example, the sub-sequence LQE occurs once in the
LQE.x2/ D 2.
ﬁrst, and twice in the second protein, so h3
The number of possible sequences of length m is 20m, which can be very

LQE.x1/ D 1 and h3

8 As long as the similarities behave like inner products; i.e. they form positive

semi-deﬁnite matrices.

386

SVMs and Kernel Methods

large for moderate m. Also the vast majority of the sub-sequences do not
match the strings in our training set, which means hm.x/ will be sparse. It
turns out that we can compute the n(cid:2)n inner product matrix or string kernel
Km.x1; x2/ D hhm.x1/; hm.x2/i efﬁciently using tree structures, without
actually computing the individual vectors.  Armed with the kernel, we

9

Figure 19.7 ROC curves for two classiﬁers ﬁt to the protein data.
The ROC curves were computed using 10-fold cross-validation,
and trace the trade-off between false-positive and true-positive
error rates as the classiﬁer threshold is varied. The area under the
curve (AUC) summarizes the overall performance of each
classiﬁer. Here the SVM is slightly superior to kernel logistic
regression.

can now use it to ﬁt a regularized SVM or logistic regression model, as
outlined in the previous section. The data consist of 1708 proteins in two
classes—negative (1663) and positive (45). We ﬁt both the kernel SVM
and kernel logistic regression models. For both methods, cross-validation
suggested a very small value for (cid:21). Figure 19.7 shows the ROC trade-off
curve for each, using 10-fold cross-validation. Here the SVM outperforms
logistic regression.

Protein ClassificationFalse-positive rateTrue-positive rate0.00.20.40.60.81.00.00.20.40.60.81.0AUCSVM  0.84KLR  0.7819.7 SVMs: Concluding Remarks

387

19.7 SVMs: Concluding Remarks

SVMs have been wildly successful, and are one of the “must have” tools in
any machine-learning toolbox. They have been extended to cover many dif-
ferent scenarios, other than two-class classiﬁcation, with some awkward-
ness in cases. The extension to nonlinear function-ﬁtting via kernels (in-
spiring the “machine” in the name) generated a mini industry. Kernels are
parametrized, learned from data, with special problem-speciﬁc structure,
and so on.

On the other hand, we know that ﬁtting high-dimensional nonlinear
functions is intrinsically difﬁcult (the “curse of dimensionality”), and SVMs
are not immune. The quadratic penalty implicit in kernel methodology
means all features are included in the model, and hence sparsity is gen-
erally not an option. Why then this unbridled enthusiasm? Classiﬁers are
far less sensitive to bias–variance tradeoffs, and SVMs are mostly popular
for their classiﬁcation performance. The ability to deﬁne a kernel for mea-
suring similarities between abstract objects, and then train a classiﬁer, is a
novelty added by these approaches that was missed in the past.

19.8 Kernel Smoothing and Local Regression

The phrase “kernel methodology” might mean something a little different
to statisticians trained in the 1970–90 period. Kernel smoothing represents
a broad range of tools for performing non- and semi-parametric regres-
sion. Figure 19.8 shows a Gaussian kernel smooth ﬁt to some artiﬁcial data
fxi ; yign
1. It computes at each point x0 a weighted average of the y-values
of neighboring points, with weights given by the height of the kernel. In its
simplest form, this estimate can be written as

f .x0/ D nX

O

yi K(cid:13) .x0; xi /;

(19.18)

iD1

where K(cid:13) .x0; xi / represents the radial kernel with width parameter (cid:13).9
Notice the similarity to (19.15); here the O˛i D yi, and the complexity of
the model is controlled by (cid:13). Despite this similarity, and the use of the
same kernel, these methodologies are rather different.

The focus here is on local estimation, and the kernel does the local-
izing. Expression (19.18) is almost a weighted average—almost because

9 Here K(cid:13) .x; (cid:22)/ is the normalized Gaussian density with mean (cid:22) and variance 1=(cid:13).

388

SVMs and Kernel Methods

Figure 19.8 A Gaussian kernel smooth of simulated data. The
points come from the blue curve with added random errors. The
kernel smoother ﬁts a weighted mean of the observations, with
the weighting kernel centered at the target point, x0 in this case.
The points shaded orange contribute to the ﬁt at x0. As x0 moves
across the domain, the smoother traces out the green curve. The
width of the kernel is a tuning parameter. We have depicted the
Gaussian weighting kernel in this ﬁgure for illustration; in fact its
vertical coordinates are all positive and integrate to one.

Pn
iD1 K(cid:13) .x0; xi / (cid:25) 1. In fact, the Nadaraya–Watson estimator is more ex-
plicit:

fN W .x0/ DPn
Pn

O

iD1 yi K(cid:13) .x0; xi /
iD1 K(cid:13) .x0; xi /

:

(19.19)

Although Figure 19.8 is one-dimensional, the same formulation applies to
x in higher dimensions.

Weighting kernels other than the Gaussian are typically favored; in par-

ticular, near-neighbor kernels with compact support. For example, the tricube
kernel used by the lowess smoother in R is deﬁned as follows:
1 Deﬁne di D kx0 (cid:0) xik2; i D 1; : : : ; n, and let d.m/ be the mth smallest
(the distance of the mth nearest neighbor to x0). Let ui D di =d.m/; i D
1; : : : ; n.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0−1.0−0.50.00.51.01.5Gaussian KernelXYlllllllllllllllllllllllllllllllllllllllllllllllllx019.8 Kernel Smoothing and Local Regression

2 The tricube kernel is given by

Ks.x0; xi / D

( (cid:0)1 (cid:0) u3

i

(cid:1)3

0

if ui  1;
otherwise,

389

(19.20)

where s D m=n, the span of the kernel. Near-neighbor kernels such as
this adapt naturally to the local density of the xi; wider in low-density
regions, narrower in high-density regions. A tricube kernel is illustrated
in Figure 19.9.

Figure 19.9 Local regression ﬁt to the simulated data. At each
point x0, we ﬁt a locally weighted linear least-squares model, and
use the ﬁtted value to estimate O
fLR.x0/. Here we use the tricube
kernel (19.20), with a span of 25%. The orange points are in the
weighting neighborhood, and we see the orange linear ﬁt
computed by kernel weighted least squares. The green dot is the
ﬁtted value at x0 from this local linear ﬁt.

Weighted means suffer from boundary bias—we can see in Figure 19.8 that
the estimate appears biased upwards at both boundaries. The reason is that,
for example on the left, the estimate for the function on the boundary aver-
ages points always to the right, and since the function is locally increasing,
there is an upward bias. Local linear regression is a natural generalization
that ﬁxes such problems. At each point x0 we solve the following weighted

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0−1.0−0.50.00.51.01.5Local Regression (tricube)XYlllllllllllllllllllllllllllx0390

SVMs and Kernel Methods

least-squares problem

O
ˇ0.x0/;

.

nX
O
ˇ.x0// D arg min
iD1
O
ˇ0.x0/ C x0
ˇ.x0/. One can show that, to ﬁrst order,

Ks.x0; xi /.yi (cid:0) ˇ0 (cid:0) xi ˇ/2:

(19.21)

ˇ0;ˇ

fLR.x0/ D O

Then O
O
fLR.x0/ removes the boundary bias exactly.

10

Figure 19.9 illustrates the procedure on our simulated data, using the
tricube kernel with a span of 25% of the data. In practice, the width of the
kernel (the span here) has to be selected by some means; typically we use
cross-validation.

Local regression works in any dimension; that is, we can ﬁt two- or
higher-dimensional surfaces using exactly the same technique. Here the
ability to remove boundary bias really pays off, since the boundaries can
be complex. These are referred to as memory-based methods, since there
is no ﬁtted model. We have to save all the training data, and recompute the
local ﬁt every time we make a prediction.

Like kernel SVMs and their relatives, kernel smoothing and local regres-
sion break down in high dimensions. Here the near neighborhoods become
so wide that they are no longer local.

19.9 Notes and Details

In the late 1980s and early 1990s, machine-learning research was largely
driven by prediction problems, and the neural-network community at AT&T
Bell laboratories was amongst the leaders. The problem of the day was
the US Post-Ofﬁce handwritten zip-code OCR challenge—a 10-class im-
age classiﬁcation problem. Vladimir Vapnik was part of this team, and
along with colleagues invented a more direct approach to classiﬁcation, the
support-vector machine. This started with the seminal paper by Boser et al.
(1992), which introduced the optimal margin classiﬁer (optimal separating
hyperplane); see also Vapnik (1996). The ideas took off quite rapidly, at-
tracting a large cohort of researchers, and evolved into the more general
class of “kernel” methods—that is, models framed in reproducing-kernel
Hilbert spaces. A good general reference is Sch¨olkopf and Smola (2001).
x C ˇ0 de-
ﬁne a linear decision boundary fx j f .x/ D 0g in Rp (an afﬁne set of co-
dimension one). The unit vector normal to the boundary is ˇ=kˇk2, where
k (cid:1) k2 denotes the `2 or Euclidean norm. How should one compute the dis-
tance from a point x to this boundary? If x0 is any point on the boundary

1 [p. 376] Geometry of separating hyperplanes. Let f .x/ D ˇ

0

19.9 Notes and Details

391

(i.e. f .x0/ D 0), we can project x (cid:0) x0 onto the normal, giving us

ˇ

0

.x (cid:0) x0/
kˇk2

D 1kˇk2

f .x/;

)

ing to (19.3) can be written as

ˇ C nX

as claimed in (19.1). Note that this is the signed distance, since f .x/ will
be positive or negative depending on what side of the boundary it lies on.
2 [p. 377] The “support” in SVM. The Lagrange primal problem correspond-

0

1

ˇ0; ˇ

minimize

(
iD1 (cid:13)i yi xi andPn
( nX

(cid:13)i Œ1 (cid:0) yi .ˇ0 C x

0
i ˇ/

;

2

ˇ

iD1

ˇ DPn

(19.22)
where (cid:13)i (cid:21) 0 are the Lagrange multipliers. On differentiating we ﬁnd that
iD1 yi (cid:13)i D 0. With ˛i D yi (cid:13)i, we get (19.4), and
note that the positivity constraint on (cid:13)i will lead to some of the ˛i being
nX
zero. Plugging into (19.22) we obtain the Lagrange dual problem

nX

maximize

f(cid:13)ign

1

(cid:13)i (cid:0) 1

iD1

2

iD1

jD1

(cid:13)i (cid:13)j yi yj x

(19.23)

subject to (cid:13)i (cid:21) 0;

yi (cid:13)i D 0:

9=;

0
i xj

nX

iD1

3 [p. 379] The SVM loss function. The constraint in (19.5) can be succinctly

captured via the expression

nX
Œ1 (cid:0) yi .ˇ0 C x

iD1

i ˇ/C  B:
0

(19.24)

We only require a (positive) (cid:15)i if our margin is less than 1, and we get
charged for the sum of these (cid:15)i. We now use a Lagrange multiplier to en-
force the constraint, leading to
C (cid:13)

nX
Œ1 (cid:0) yi .ˇ0 C x

minimize

kˇk2

(19.25)

0
i ˇ/C:

2

ˇ0; ˇ

iD1

Multiplying by (cid:21) D 1=(cid:13) gives us (19.6).

4 [p. 380] The SVM estimates a classiﬁer. The following derivation is due to

Wahba et al. (2000). Consider

EY jXDx fŒ1 (cid:0) Yf .x/Cg :

minimize

f .x/

(19.26)

SVMs and Kernel Methods

392
Dropping the dependence on x, the objective can be written as PCŒ1 (cid:0)
f CC P(cid:0)Œ1C f C, where PC D Pr.Y D C1jX D x/, and P(cid:0) D Pr.Y D
(cid:0)1jX D x/ D 1 (cid:0) PC. From this we see that

f D(cid:26) C1 if PC > 1

(cid:0)1 if P(cid:0) < 1
2
2 :

(19.27)

ˇ=k O

5 [p. 381] SVM and ridged logistic regression. Rosset et al. (2004) show
that the limiting solution as (cid:21) # 0 to (19.7) for separable data coincides
with that of the SVM, in the sense that O
ˇk2 converges to the same
quantity for the SVM. However, because of the required normalization
for logistic regression, the SVM solution is preferable. On the other hand,
for overlapped situations, the logistic-regression solution has some advan-
tages, since its target is the logit of the class probabilities.
6 [p. 382] The kernel trick. The trick here is to observe that from the score
.y (cid:0) X ˇ/ C (cid:21)ˇ D 0, which means we can write
equations we have (cid:0)X
O
ˇ D X
˛ for some ˛. We now plug this into the score equations, and
some simple manipulation gives the result. A similar result holds for ridged
logistic regression, and in fact any linear model with a ridge penalty on the
coefﬁcients (Hastie and Tibshirani, 2004).
7 [p. 382] Polynomial kernels. Consider K2.x; z/ D .1 C hx; zi/2, for x

0

0

(and z) in R2. Expanding we get

K2.x; z/ D 1 C 2x1z1 C 2x2z2 C 2x1x2z1z2 C x2
1 z2

1

C x2
2 z2
2 :

This corresponds to hh2.x/; h2.z/i with

p

p

p

h2.x/ D .1;

2x1;

2x2;

2x1x2; x2

1 ; x2

2 /:

The same is true for p > 2 and for degree d > 2.

8 [p. 384] Reproducing kernel Hilbert spaces. Suppose K has eigen expan-
iD1 (cid:13)i < 1. Then

sion K.x; z/ DP1

iD1 (cid:13)i (cid:30)i .x/(cid:30)i .z/, with (cid:13)i (cid:21) 0 andP1
HK if f .x/ DP1

we say f 2

1X

iD1 ci (cid:30)i .x/, with
< 1:



c2
i
(cid:13)i

iD1

kf k2
HK

(19.28)

Often kf k
HK behaves like a roughness penalty, in that it penalizes unlikely
members in the span of K.(cid:1); z/ (assuming that these correspond to “rough”
functions). If f has some high loadings cj on functions (cid:30)j with small
eigenvalues (cid:13)j (i.e. not prominent members of the span), the norm becomes
large. Smoothing splines and their generalizations correspond to function
ﬁtting in a RKHS (Wahba, 1990).

19.9 Notes and Details

393

9 [p. 386] This methodology and the data we use in our example come from

Leslie et al. (2003).

10 [p. 390] Local regression and bias reduction. By expanding the unknown
true f .x/ in a ﬁrst-order Taylor expansion about the target point x0, one
can show that E

O
fLR.x0/ (cid:25) f .x0/ (Hastie and Loader, 1993).

20

Inference After Model Selection

The classical theory of model selection focused on “F tests” performed
within Gaussian regression models. Inference after model selection (for in-
stance, assessing the accuracy of a ﬁtted regression curve) was typically
done ignoring the model selection process. This was a matter of neces-
sity: the combination of discrete model selection and continuous regression
analysis was too awkward for simple mathematical description. Electronic
computation has opened the door to a more honest analysis of estimation
accuracy, one that takes account of the variability induced by data-based
model selection.

Figure 20.1 displays the cholesterol data, an example we will use
for illustration in what follows: cholestyramine, a proposed cholesterol-
lowering drug, was administered to n D 164 men for an average of seven
years each. The response variable di was the ith man’s decrease in choles-
terol level over the course of the experiment. Also measured was ci, his
compliance or the proportion of the intended dose actually taken, ranging
from 1 for perfect compliers to zero for the four men who took none at all.
Here the 164 ci values have been transformed to approximately follow a
standard normal distribution,

ci P(cid:24)

N .0; 1/:

(20.1)

We wish to predict cholesterol decrease from compliance. Polynomial
regression models, with di a J th-order polynomial in ci, were considered,
for degrees J D 1; 2; 3; 4; 5, or 6. The Cp criterion (12.51) was applied
and selected a cubic model, J D 3, as best. The curve in Figure 20.1 is the
OLS (ordinary least squares) cubic regression curve ﬁt to the cholesterol
data set

f.ci ; di /; i D 1; 2; : : : ; 164g :

(20.2)

We are interested in answering the following question: how accurate is the

394

20.1 Simultaneous Conﬁdence Intervals

395

Figure 20.1 Cholesterol data: cholesterol decrease plotted
versus adjusted compliance for 164 men taking
cholestyramine. The green curve is OLS cubic regression,
with “cubic” selected by the Cp criterion. How accurate is the
ﬁtted curve?

ﬁtted curve, taking account of Cp selection as well as OLS estimation?
(See Section 20.2 for an answer.)

Currently, there is no overarching theory for inference after model selec-
tion. This chapter, more modestly, presents a short series of vignettes that
illustrate promising analyses of individual situations. See also Section 16.6
for a brief report on progress in post-selection inference for the lasso.

20.1 Simultaneous Conﬁdence Intervals

In the early 1950s, just before the beginnings of the computer revolution,
substantial progress was made on the problem of setting simultaneous con-
ﬁdence intervals. “Simultaneous” here means that there exists a catalog of
parameters of possible interest,

C D f1; 2; : : : ; Jg;

(20.3)

and we wish to set a conﬁdence interval for each of them with some ﬁxed
probability, typically 0.95, that all of the intervals will contain their respec-
tive parameters.

********************************************************************************************************************************************************************−2−1012−20020406080100Adjusted complianceCholesterol decreaseInference After Model Selection

396
As a ﬁrst example, we return to the diabetes data of Section 7.3: n D
442 diabetes patients each have had p D 10 medical variables measured at
baseline, with the goal of predicting prog, disease progression one year
later. Let X be the 442 (cid:2) 10 matrix with ith row x
0
i the 10 measurements
for patient i; X has been standardized so that each of its columns has mean
0 and sum of squares 1. Also let y be the 442-vector of centered prog
measurements (that is, subtracting off the mean of the prog values).

Ordinary least squares applied to the normal linear model,

y (cid:24)

Nn.X ˇ; (cid:27) 2I/;

O
ˇ D .X

0

(cid:0)1X

0

y;

X /

O
ˇ (cid:24)

Np.ˇ; (cid:27) 2V /;

V D .X

0

(cid:0)1;

X /

yields MLE

satisfying

as at (7.34).

nent of ˇ, is

(20.4)

(20.5)

(20.6)

(20.7)

The 95% Student-t conﬁdence interval (11.49) for ˇj , the j th compo-

O
ˇj ˙ O(cid:27) V 1=2

jj

t :975
q

;

where O(cid:27) D 54:2 is the usual unbiased estimate of (cid:27),

O(cid:27) 2 D ky (cid:0) X

O
ˇk2=q;

q D n (cid:0) p D 432;

q

(20.8)
D 1:97 is the 0.975 quantile of a Student-t distribution with q
and t :975
degrees of freedom.
The catalog C in (20.3) is now fˇ1; ˇ2; : : : ; ˇ10g. The individual inter-
vals (20.7), shown in Table 20.1, each have 95% coverage, but they are not
simultaneous: there is a greater than 5% chance that at least one of the ˇj
values lies outside its claimed interval.

Valid 95% simultaneous intervals for the 10 parameters appear on the

right side of Table 20.1. These are the Scheff´e intervals

O
ˇj ˙ O(cid:27) V 1=2
(20.9)
jj k.˛/
p;q;
p;q equals 4.30 for p D 10, q D
discussed next. The crucial constant k.˛/
432, and ˛ D 0:95. That makes the Scheff´e intervals wider than the t
intervals (20.7) by a factor of 2.19. One expects to pay an extra price for
simultaneous coverage, but a factor greater than two induces sticker shock.

Scheff´e’s method depends on the pivotal quantity

Q D(cid:16) O

0

(cid:0)1(cid:16) O

ˇ (cid:0) ˇ

V

ˇ (cid:0) ˇ

.O(cid:27) 2;

(20.10)

20.1 Simultaneous Conﬁdence Intervals

397

Table 20.1 Maximum likelihood estimates O
variables (20.6); separate 95% Student-t conﬁdence limits, also
simultaneous 95% Scheff´e intervals. The Scheff´e intervals are wider by a
factor of 2.19.

ˇ for 10 diabetes predictor

Student-t

Scheff´e

O
ˇ
(cid:0)0.5

Lower Upper
Lower Upper
(cid:0)6.1
(cid:0)12.7
age
5.1
(cid:0)5.7
sex (cid:0)11.4 (cid:0)17.1
(cid:0)24.0
bmi
31.0
24.8
18.5
11.1
map
15.4
9.3
21.6
2.1
(cid:0)37.7 (cid:0)76.7
1.2 (cid:0)123.0
tc
(cid:0)9.0
(cid:0)46.7
ldl
54.4
22.7
(cid:0)38.7
4.8 (cid:0)15.1
hdl
24.7
(cid:0)24.6
(cid:0)6.7
tch
23.5
8.4
ltg
51.9
35.8
0.6
19.7
(cid:0)3.0
(cid:0)10.3
glu
3.2
9.4

11.8
1.1
38.4
28.8
47.6
92.1
48.3
41.5
71.0
16.7

which under model (20.4) has a scaled “F distribution,”1

p;q is the ˛th quantile of a pFp;q distribution then PrfQ  k.˛/2

If k.˛/2
yields

Q (cid:24) pFp;q:


(cid:0)1(cid:16)
ˇ (cid:0) O

ˇ

V

O(cid:27) 2

0

ˇ (cid:0) O

ˇ

(cid:16)

8ˆ<ˆ:

Pr

9>=>; D ˛

 k.˛/2

p;q

(20.11)
p;q g D ˛

(20.12)

for any choice of ˇ and (cid:27) in model (20.4). Having observed O
ˇ and O(cid:27),
(20.12) deﬁnes an elliptical conﬁdence region E for the parameter vector
ˇ.
Suppose we are interested in a particular linear combination of the coor-

dinates of ˇ, say

ˇc D c

0

ˇ;

(20.13)

1 Fp;q is distributed as .(cid:31)2

p=p/=.(cid:31)2

q=q/, the two chi-squared variates being

independent. Calculating the percentiles of Fp;q was a major project of the pre-war
period.

398

Inference After Model Selection

Figure 20.2 Ellipsoid of possible vectors ˇ deﬁned by (20.12)
determines conﬁdence intervals for ˇc D c
ˇ according to the
“bounding hyperplane” construction illustrated. The red line
shows the conﬁdence interval for ˇc if c is a unit vector,
V c D 1.
0
c

0

where c is a ﬁxed p-dimensional vector. If ˇ exists in E then we must have
(20.14)

0
which turns out to be the interval centered at O

ˇ/; max
ˇ2
E

min
ˇ2
E

.c

0

1

0 O
ˇ,

(20.15)

ˇc 2

ˇc 2 O

ˇc ˙ O(cid:27) .c

0

(cid:21)
ˇc D c
V c/1=2k.˛/
p;q:

ˇ/

.c

;

(This agrees with (20.9) where c is the j th coordinate vector .0; : : : ; 0; 1; 0,
: : : ; 0/

0.) The construction is illustrated in Figure 20.2.

Np.ˇ; (cid:27) 2V / independently of O(cid:27) 2 (cid:24) (cid:27) 2(cid:31)2
Theorem (Scheff´e)
then with probability ˛ the conﬁdence statement (20.15) for ˇc D c
be simultaneously true for all choices of the vector c.

0

q=q,
ˇ will

If O
ˇ (cid:24)

0

Here we can think of “model selection” as the choice of the linear com-
bination of interest c D c
ˇ. Scheff´e’s theorem allows “data snooping”:
the statistician can examine the data and then choose which c (or many
c’s) to estimate, without invalidating the resulting conﬁdence intervals.
An important application has the O
ˇj ’s as independent estimates of efﬁ-
cacy for competing treatments—perhaps different experimental drugs for
the same target disease:

O
ˇj

ind(cid:24)

N .ˇj ; (cid:27) 2=nj /;

for j D 1; 2; : : : ; J;

(20.16)

 lcb^20.1 Simultaneous Conﬁdence Intervals

399

the nj being known sample sizes. In this case the catalog C might comprise
all pairwise differences ˇi (cid:0) ˇj , as the statistician tries to determine which
treatments are better or worse than the others.
The fact that Scheff´e’s limits apply to all possible linear combinations
0
ˇ is a blessing and a curse, the curse being their very large width, as seen
c
in Table 20.1. Narrower simultaneous limits are possible if we restrict the 2
catalog C , for instance to just the pairwise differences ˇi (cid:0) ˇj .
A serious objection, along Fisherian lines, is that the Scheff´e conﬁdence
limits are accurate without being correct. That is, the intervals have the
claimed overall frequentist coverage probability, but may be misleading
when applied to individual cases. Suppose for instance that (cid:27) 2=nj D 1 for
j D 1; 2; : : : ; J in (20.16) and that we observe O
ˇ1 D 10, with j O
ˇjj < 2 for
all the others. Even if we looked at the data before singling out O
ˇ1 for at-
tention, the usual Student-t interval (20.7) seems more appropriate than its
much longer Scheff´e version (20.9). This point is made more convincingly
in our next vignette.

(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)

A familiar but pernicious abuse of model selection concerns multiple
hypothesis testing. Suppose we observe N independent normal variates zi,
each with its own effect size (cid:22)i,

for i D 1; 2; : : : ; N;
and, as in Section 15.1, we wish to test the null hypotheses

N .(cid:22)i ; 1/

ind(cid:24)

zi

H0i W (cid:22)i D 0:

(20.17)

(20.18)

Being alert to the pitfalls of simultaneous testing, we employ a false-discov-
ery rate control algorithm (15.14), which rejects R of the N null hypothe-
ses, say for cases i1; i2; : : : ; iR. (R equaled 28 in the example of Fig-
ure 15.3.)

So far so good. The “familiar abuse” comes in then setting the usual

conﬁdence intervals

(cid:22)i 2 O(cid:22)i ˙ 1:96

(20.19)

(95% coverage) for the R selected cases. This ignores the model selection
process: the data-based selection of the R cases must be taken into account
in making legitimate inferences, even if R is only 1 so multiplicity is not a
concern.

This problem is addressed by the theory of false-coverage control. Sup-
pose algorithm A sets conﬁdence intervals for R of the N cases, of which

400

Inference After Model Selection

r are actually false coverages, i.e., ones not containing the true effect size
(cid:22)i. The false-coverage rate (FCR) of A is the expected proportion of non-
coverages
(20.20)

FCR.A/ D Efr=Rg;

the expectation being with respect to model (20.17). The goal, as with the
FDR theory of Section 15.2, is to construct algorithm A to control FCR
below some ﬁxed value q.
The BYq algorithm2 controls FCR below level q in three easy steps,

beginning with model (20.17).

1 Let pi be the p-value corresponding to zi,
pi D ˆ.zi /

(20.21)

for left-sided signiﬁcance testing, and order the p.i / values in ascending
order,

p.1/  p.2/  p.3/  : : :  p.N /:

(20.22)
2 Calculate R D maxfi W p.i /  i (cid:1) q=Ng, and (as in the BHq algorithm
(15.14)–(15.15)) declare the R corresponding null hypotheses false.

(cid:22)i 2 zi ˙ z.˛R/;
(cid:0)1.˛/).

3 For each of the R cases, construct the conﬁdence interval
where ˛R D 1 (cid:0) Rq=N

(z.˛/ D ˆ
Theorem 20.1 Under model (20.17), BYq has FCR  q; moreover, none
of the intervals (20.23) contain (cid:22)i D 0.
A simulated example of BYq was run according to these speciﬁcations:

N D 10,000;
(cid:22)i D 0
N .(cid:0)3; 1/

q D 0:05;

zi (cid:24)
for i D 1; 2; : : : ; 9000;
for i D 9001; : : : ; 10,000:

N .(cid:22)i ; 1/

(cid:22)i (cid:24)

(20.23)

(20.24)

In this situation we have 9000 null cases and 1000 non-null cases (all but 2
of which had (cid:22)i < 0).

Because this is a simulation, we can plot the pairs .zi ; (cid:22)i / to assess the
BYq algorithm’s performance. This is done in Figure 20.3 for the 1000
non-null cases (the green points). BYq declared R D 565 cases non-null,
those having zi  (cid:0)2:77 (the circled points); 14 of the 565 declarations

2 Short for “Benjamini–Yekutieli;” see the chapter endnotes.

20.1 Simultaneous Conﬁdence Intervals

401

Figure 20.3 Simulation experiment (20.24) with N D10,000
cases, of which 1000 are non-null; the green points .zi ; (cid:22)i / are
these non-null cases. The FDR control algorithm BHq (q D 0:05)
declared the 565 circled cases having zi  (cid:0)2:77 to be non-null,
of which the 14 red points were actually null. The heavy black
lines show BYq 95% conﬁdence intervals for the 565 cases, only
17 of which failed to contain (cid:22)i. Actual Bayes posterior 95%
intervals for non-null cases (20.26), dotted lines, have half the
width and slope of BYq limits.

were actually null cases (the red circled points), giving false-discovery pro-
portion 14=565 D 0:025. The heavy black lines trace the BYq conﬁdence
limits (20.23) as a function of z  (cid:0)2:77.

The ﬁrst thing to notice is that FCR control has indeed been achieved:
only 17 of the declared cases lie outside their limits (the 14 nulls and 3
non-nulls), for a false-coverage rate of 17=565 D 0:030, safely less than
q D 0:05. The second thing, however, is that the BYq limits provide a
misleading idea of the location of (cid:22)i given zi: they are much too wide and
slope too low, especially for more negative zi values.

In this situation we can describe precisely the posterior distribution of



;

(20.25)

(cid:22)i given zi for the non-null cases,
(cid:22)ijzi (cid:24)

N

 zi (cid:0) 3

;

1

2

2

−8−6−4−20−10−8−6−4−20Observed zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllBY.loBY.up−2.77llllllllllllllBayes.loBayes.upInference After Model Selection

402
this following from (cid:22)i (cid:24)
(5.20)–(5.21). The Bayes credible 95% limits

N .(cid:22)i ; 1/, and Bayes’ rule

N .(cid:0)3; 1/, zij(cid:22)i (cid:24)
(cid:22)i 2 zi (cid:0) 3
˙ 1p
2

2

1:96

(20.26)

are indicated by the dotted lines in Figure 20.3. They are half as wide as
the BYq limits, and have slope 1=2 rather than 1.

In practice, of course, we would only see the zi, not the (cid:22)i, making
(20.26) unavailable to us. We return to this example in Chapter 21, where
empirical Bayes methods will be seen to provide a good approximation to
the Bayes limits. (See Figure 31.5.)

As with Scheff´e’s method, the BYq intervals can be accused of being
accurate but not correct. “Correct” here has a Bayesian/Fisherian ﬂavor
that is hard to pin down, except perhaps in large-scale applications, where
empirical Bayes analyses can suggest appropriate inferences.

20.2 Accuracy After Model Selection

The cubic regression curve for the cholesterol data seen in Figure 20.1
was selected according to the Cp criterion of Section 12.3. Polynomial
regression models, predicting cholesterol decrease di in terms of powers
(“degrees”) of adjusted compliance ci, were ﬁt by ordinary least squares
for degrees 0; 1; 2; : : : ; 6. Table 20.2 shows Cp estimates (12.51) being
minimized at degree 3.

Table 20.2 Cp table for cholesterol data of Figure 20.1, comparing OLS
polynomial models of degrees 0 through 6. The cubic model, degree D 3,
is the minimizer (80,000 subtracted from the Cp values for easier
comparison; assumes (cid:27) D 22:0).

Degree

0
1
2
3
4
5
6

Cp
71887
1132
1412
667
1591
1811
2758

We wish to assess the accuracy of the ﬁtted curve, taking account of both
the Cp model selection method and the OLS ﬁtting process. The bootstrap

20.2 Accuracy After Model Selection

403

is a natural candidate for the job. Here we will employ the nonparamet-
ric bootstrap of Section 10.2 (rather than the parametric bootstrap of Sec-
tion 10.4, though this would be no more difﬁcult to carry out).
The cholesterol data set (20.2) comprises n D 164 pairs xi D
(cid:3) (10.13) consists of 164 pairs
.ci ; di /; a nonparametric bootstrap sample x
chosen at random and with replacement from the original 164. Let t .x/ be
the curve obtained by applying the Cp/OLS algorithm to the original data
(cid:3); and for a given
set x and likewise t .x
point c on the compliance scale let

/ for the algorithm applied to x

(cid:3)

(cid:3)

O

c

D t .c; x

(cid:3)

/

(20.27)

be the value of t .x

(cid:3)

/ evaluated at compliance D c.

Figure 20.4 A histogram of 4000 nonparametric bootstrap
replications for polynomial regression estimates of cholesterol
decreases d at adjusted compliance c D (cid:0)2. Blue histogram,
adaptive estimator O
(cid:3)
c (20.27), using full Cp/OLS algorithm for

each bootstrap data set; line histogram, using OLS only with
degree 3 for each bootstrap data set. Bootstrap standard errors are
5.98 and 3.97.

 Cholesterol decrease q^−2*Frequency−20−10010200100200300400fixed degree (3)adaptive degree1.27Inference After Model Selection
404
B D 4000 nonparametric bootstrap replications t .x
/ were generated.3
Figure 20.4 shows the histogram of the 4000 O
c replications for c D (cid:0)2:0.
(cid:3)
It is labeled “adaptive” to indicate that Cp model selection, as well as OLS
(cid:3). This is as opposed to the “ﬁxed”
ﬁtting, was carred out anew for each x
histogram, where there was no Cp selection, cubic OLS regression always
being used.

(cid:3)



Figure 20.5 Bootstrap standard-error estimates of O
(cid:0)2:2  c  2. Solid black curve, adaptive estimator (20.27)
using full Cp/OLS model selection estimate; red dashed curve,
using OLS only with polynomial degree ﬁxed at 3;
blue dotted curve, “bagged estimator” using bootstrap smoothing
(20.28). Average standard-error ratios: adaptive/ﬁxed D 1:43,
adaptive/smoothed D 1:14.

c, for

The bootstrap estimate of standard error (10.16) obtained from the adap-
tive values O
(cid:3)
c was 5.98, compared with 3.97 for the ﬁxed values.4 In this

case, accounting for model selection (“adaptation”) adds more than 50% to
the standard error estimates. The same comparison was made at all values

3 Ten times more than needed for assessing standard errors, but helpful for the

comparisons that follow.

4 The latter is not the usual OLS assessment, following (8.30), that would be appropriate

for a parametric bootstrap comparison. Rather, it’s the nonparametric one-sample
bootstrap assessment, resampling pairs .xi ; yi / as individual sample points.

−2−101201234567Adjusted compliance cStandard errors of q^cfixedadaptivesmoothed20.2 Accuracy After Model Selection

405

of the adjusted compliance c. Figure 20.5 graphs the results: the adaptive
standard errors averaged 43% greater than the ﬁxed values. The standard
we ignored model selection in assessingbse.
95% conﬁdence intervals O

c ˙bse (cid:1) 1:96 would be roughly 43% too short if

Figure 20.6 “Adaptive” histogram of Figure 20.4 now split into
19% of 4000 bootstrap replications where Cp selected linear
(cid:3) D 1
(cid:3) D 1) as best, versus 81% having m
(cid:3)
> 1. m
regression (m
(cid:3)
cases are shifted about 10 units downward. (The m
> 1 cases
resemble the “ﬁxed” histogram in Figure 20.4.) Histograms are
scaled to have equal areas.

Having an honest assessment of standard error doesn’t mean that t .c; x/
(20.27) is a good estimator. Model selection can induce an unpleasant
“jumpiness” in an estimator, as the original data vector x crosses deﬁni-
tional boundaries. This happened in our example: for 19% of the 4000
(cid:3) D 1,
(cid:3), the Cp algorithm selected linear regression, m
bootstrap samples x
as best, and in these cases O
(cid:3)(cid:0)2:0 tended toward smaller values. Figure 20.6
(cid:3) D 1 histogram shifted about 10 units down from the m
(cid:3)
shows the m
histogram (which now resembles the “ﬁxed” histogram in Figure 20.4).

> 1



Discontinuous estimators such as t .c; x/ can’t be Bayesian, Bayes pos-
terior expectations being continuous. They can also suffer frequentist difﬁ-
culties,  including excess variability and overly long conﬁdence intervals. 3

 Cholesterol decrease q^−2*Frequency−20−10010200204060801.27adaptivem* = 1adaptivem* > 1fixed m = 3406

Inference After Model Selection

Bagging, or bootstrap smoothing, is a tactic for improving a discontinuous
estimation rule by averaging (as in (12.80) and Chapter 17).
replications ft .x
average

Suppose t .x/ is any estimator for which we have obtained bootstrap
(cid:3)b/, b D 1; 2; : : : ; Bg. The bagged version of t .x/ is the

BX

bD1

s.x/ D 1

B

(cid:3)b/:

t .x

(20.28)

The letter s here stands for “smooth.” Small changes in x, even ones that
move across a model selection deﬁnitional boundary, produce only small
changes in the bootstrap average s.x/.

Averaging over the 4000 bootstrap replications of t .c; x

/ (20.27) gave
a bagged estimate sc.x/ for each value of c. Bagging reduced the standard
errors of the Cp/OLS estimates t .c; x/ by about 12%, as indicated by the
green dotted curve in Figure 20.5.

t .c; x

Where did the green dotted curve come from? All 4000 bootstrap values
(cid:3)b/ were needed to produce the single value sc.x/. It seems as if

we would need to bootstrap the bootstrap in order to computebseŒsc.x/.

Fortunately, a more economical calculation is possible, one that requires
only the original B bootstrap computations for t .c; x/.

(cid:3)

Deﬁne

Nbj D #ftimes xj occurs in x

(20.29)
for b D 1; 2; : : : ; B and j D 1; 2; : : : ; n. For instance N4000;7 D 3 says
that data point x7 occurred three times in nonparametric bootstrap sample
x4000. The B by n matrix fNbjg completely describes the B bootstrap
samples. Also denote

(cid:3)bg;

where dots denote averaging over B: N(cid:1)j D 1
Theorem 20.2  The inﬁnitesimal jackknife estimate of standard error

b Nbj and t

(cid:3)(cid:1) D 1

b t

(cid:3)b.

B

B

4

and let covj indicate the covariance in the bootstrap sample between Nbj
and t

(cid:3)b,

t

(cid:3)b/

(cid:3)b D t .x
BX
.Nbj (cid:0) N(cid:1)j /.t
(cid:3)b (cid:0) t
P

bD1

(cid:3)(cid:1)

/;

covj D 1

B

(20.30)

(20.31)

P

(10.41) for the bagged estimate (20.28) is

bseIJ Œsc.x/ D

0@ nX

jD1

1A1=2

cov2
j

:

(20.32)

20.2 Accuracy After Model Selection

407

effort.

Keeping track of Nbj as we generate the bootstrap replications t

us to compute covj and bseŒsc.x/ without any additional computational
Figure 20.5, the ratio ofbseIJŒsc.x/=bsebootŒt .c; x/ averaging 0.88. In fact,
Corollary The ratiobseIJŒsc.x/=bsebootŒt .c; x/ is always  1.

We expect averaging to reduce variability, and this is seen to hold true in

we have the following general result.

(cid:3)b allows

The savings due to bagging increase with the nonlinearity of t .x

/ as
a function of the counts Nbj (or, in the language of Section 10.3, in the
nonlinearity of S.P/ as a function of P). Model-selection estimators such
as the Cp/OLS rule tend toward greater nonlinearity and bigger savings.

(cid:3)

Table 20.3 Proportion of 4000 nonparametric bootstrap replications of
Cp/OLS algorithm that selected degrees m D 1; 2; : : : ; 6; also
inﬁnitesimal jackknife standard deviations for proportions (20.32), which
mostly exceed the estimates themselves.

proportion

bsdIJ

m D 1
.19
.24

2

.12
.20

3

.35
.24

4

.07
.13

5

.20
.26

6

.06
.06

The ﬁrst line of Table 20.3 shows the proportions in which the various
degrees were selected in the 4000 cholesterol bootstrap replications, 19%
for linear, 12% for quadratic, 35% for cubic, etc. With B D 4000, the
proportions seem very accurate, the binomial standard error for 0.19 being
just .0:19 (cid:1) 0:81=4000/1=2 D 0:006, for instance.
(cid:3)b (20.30) indicate whether
Theorem 20.2 suggests otherwise. Now let t

the bth bootstrap sample x

(

(cid:3) made the Cp choice m
(cid:3)b D

(cid:3)b D 1
(cid:3)b > 1:

1 if m
0 if m

(cid:3) D 1,

t

(20.33)
(cid:3)b; b D 1; 2; : : : ; Bg is the observed proportion

The bagged value of ft

408

0.19. Applying the bagging theorem yieldedbseIJ D 0:24, as seen in the

Inference After Model Selection

second line of the table, with similarly huge standard errors for the other
proportions.
The binomial standard errors are internal, saying how quickly the boot-
strap resampling process is converging to its ultimate value as B ! 1.
The inﬁnitesimal jackknife estimates are external: if we collected a new
set of 164 data pairs .ci ; di / (20.2) the new proportion table might look
completely different than the top line of Table 20.3.

Frequentist statistics has the advantage of being applicable to any algo-
rithmic procedure, for instance to our Cp/OLS estimator. This has great
appeal in an era of enormous data sets and fast computation. The draw-
back, compared with Bayesian statistics, is that we have no guarantee that
our chosen algorithm is best in any way. Classical statistics developed a
theory of best for a catalog of comparatively simple estimation and testing
problems. In this sense, modern inferential theory has not yet caught up
with modern problems such as data-based model selection, though tech-
niques such as model averaging (e.g., bagging) suggest promising steps
forward.

20.3 Selection Bias

Many a sports fan has been victimized by selection bias. Your team does
wonderfully well and tops the league standings. But the next year, with
the same players and the same opponents, you’re back in the pack. This
is the winner’s curse, a more picturesque name for selection bias, the ten-
dency of unusually good (or bad) comparative performances not to repeat
themselves.

Modern scientiﬁc technology allows the simultaneous investigation of
hundreds or thousands of candidate situations, with the goal of choosing
the top performers for subsequent study. This is a setup for the heartbreak
of selection bias. An apt example is offered by the prostate study data of
Section 15.1, where we observe statistics zi measuring patient–control dif-
ferences for N D 6033 genes,

zi (cid:24)

N .(cid:22)i ; 1/;

i D 1; 2; : : : ; N:

(20.34)

Here (cid:22)i is the effect size for gene i, the true difference between the patient
and control populations.
Genes with large positive or negative values of (cid:22)i would be promising
targets for further investigation. Gene number 610, with z610 D 5:29, at-

20.3 Selection Bias

409

tained the biggest z-value; (20.34) says that z610 is unbiased for (cid:22)610. Can
we believe the obvious estimate O(cid:22)610 D 5:29?
“No” is the correct selection bias answer. Gene 610 has won a contest for
bigness among 6033 contenders. In addition to being good (having a large
value of (cid:22)) it has almost certainly been lucky, with the noise in (20.34)
pushing z610 in the positive direction—or else it would not have won the
contest. This is the essence of selection bias.

False-discovery rate theory, Chapter 15, provided a way to correct for
selection bias in simultaneous hypothesis testing. This was extended to
false-coverage rates in Section 20.1. Our next vignette concerns the re-
alistic estimation of effect sizes (cid:22)i in the face of selection bias.
We begin by assuming that an effect size (cid:22) has been obtained from a
prior density g.(cid:22)/ (which might include discrete atoms) and then z (cid:24)
N .(cid:22); (cid:27) 2/ observed,

(cid:22) (cid:24) g.(cid:1)/

and zj(cid:22) (cid:24)

N .(cid:22); (cid:27) 2/

(20.35)

((cid:27) 2 is assumed known for this discussion). The marginal density of z is

f .z/ DZ 1


(cid:0) 1
(cid:0)1 g.(cid:22)/(cid:30)(cid:27) .z (cid:0) (cid:22)/ d(cid:22);

(cid:0)1=2 exp

z2
(cid:27) 2

2

where (cid:30)(cid:27) .z/ D .2(cid:25)(cid:27) 2/

(20.36)

:

Tweedie’s formulais an intriguing expression for the Bayes expectation 5

of (cid:22) given z.

Theorem 20.3
observed z is

In model (20.35), the posterior expectation of (cid:22) having

0

.z/ D d

Ef(cid:22)jzg D z C (cid:27) 2l

0

dz

.z/

with l

log f .z/:

(20.37)
The especially convenient feature of Tweedie’s formula is that Ef(cid:22)jzg
is expressed directly in terms of the marginal density f .z/. This is a setup
for empirical Bayes estimation. We don’t know g.(cid:22)/, but in large-scale
situations we can estimate the marginal density f .z/ from the observa-
tions z D .z1; z2; : : : ; zN /, perhaps by Poisson regression as in Table 15.1,
yielding

OEf(cid:22)ijzig D zi C (cid:27) 2O
0
(20.38)
The solid curve in Figure 20.7 shows OEf(cid:22)jzg for the prostate study data,

.z/ D d

with O
0

log O

f .z/:

.zi /

dz

l

l

410

Inference After Model Selection

Figure 20.7 The solid curve is Tweedie’s estimate OEf(cid:22)jzg
local false-discovery ratecfdr.z/ from Figure 15.5 (red scale on
right). At z D 3:5, OEf(cid:22)jzg D 1:96 andcfdr.z/ D 0:15. For gene
(20.38) for the prostate study data. The dashed line shows the
610, with z610 D 5:29, Tweedie’s estimate is 4.03.

with (cid:27) 2 D 1 and O
jzij  2, agreeing with the local false-discovery rate curvecfdr.z/ of Fig-
f .z/ obtained using fourth-degree log polynomial re-
gression as in Section 15.4. The curve has Ef(cid:22)jzg hovering near zero for
cfdr.z/ D 0:15. So even though zi D 3:5 has a one-sided p-value of 0.0002,
ure 15.5 that says these are mostly null genes.
OEf(cid:22)jzg increases for z > 2, equaling 1.96 for z D 3:5. At that point
with 6033 genes to consider at once, it still is not a sure thing that gene i
is non-null. About 85% of the genes with zi near 3.5 will be non-null,
and these will have effect sizes averaging about 2.31 (D 1:96=0:85). All of
this nicely illustrates the combination of frequentist and Bayesian inference
possible in large-scale studies, and also the combination of estimation and
hypothesis-testing ideas in play.

If the prior density g.(cid:22)/ in (20.35) is assumed to be normal, Tweedie’s
formula (20.38) gives (almost) the James–Stein estimator (7.13). The cor-
responding curve in Figure 20.7 in that case would be a straight line pass-
ing through the origin at slope 0.22. Like the James–Stein estimator, ridge
regression, and the lasso of Chapter 16, Tweedie’s formula is a shrink-
age estimator. For z610 D 5:29, the most extreme observation, it gave

−4−2024−2024z−valueE^( m | z)lllz = 3.51.96.1500.20.40.60.81 fdrTweediefdr411
O(cid:22)629 D 4:03, shrinking the maximum likelihood estimate more than one (cid:27)
unit toward the origin.

20.3 Selection Bias

Bayes estimators are immune to selection bias, as discussed in Sections
3.3 and 3.4. This offers some hope that Tweedie’s empirical Bayes esti-
mates might be a realistic cure for the winners’ curse. A small simulation
experiment was run as a test.
(cid:15) A hundred data sets z, each of length N D 1000, were generated accord-

ing to a combination of exponential and normal sampling,

ind(cid:24) e

(cid:0)(cid:22)

(cid:22)i

.(cid:22) > 0/

and

zij(cid:22)i

ind(cid:24)

N .(cid:22)i ; 1/;

(20.39)

l.z/ was computed as in Section 15.4, now using a natural

for i D 1; 2; : : : ; 1000.
(cid:15) For each z, O
spline model with ﬁve degrees of freedom.
(cid:15) This gave Tweedie’s estimates
O(cid:22)i D zi C O
0

.zi /;

l

i D 1; 2; : : : ; 1000;

(20.40)

for that data set z.
(cid:15) For each data set z, the 20 largest zi values and the corresponding O(cid:22)i and
(cid:22)i values were recorded, yielding the

uncorrected differences
and corrected differences

zi (cid:0) (cid:22)i
O(cid:22)i (cid:0) (cid:22)i ;

(20.41)

the hope being that empirical Bayes shrinkage would correct the selection
bias in the zi values.
(cid:15) Figure 20.8 shows the 2000 (100 data sets, 20 top cases each) uncorrected
and corrected differences. Selection bias is quite obvious, with the uncor-
rected differences shifted one unit to the right of zero. In this case at least,
the empirical Bayes corrections have worked well, the corrected differ-
ences being nicely centered at zero. Bias correction often adds variance,
but in this case it hasn’t.

Finally, it is worth saying that the “empirical” part of empirical Bayes is
less the estimation of Bayesian rules from the aggregate data than the appli-
cation of such rules to individual cases. For the prostate data we began with
no deﬁnite prior opinions but arrived at strong (i.e., not “uninformative”)
Bayesian conclusions for, say, (cid:22)610 in the prostate study.

412

Inference After Model Selection

Figure 20.8 Corrected and uncorrected differences for 20 top
cases in each of 100 simulations (20.39)–(20.41). Tweedie
corrections effectively counteracted selection bias.

20.4 Combined Bayes–Frequentist Estimation

As mentioned previously, Bayes estimates are, at least theoretically, im-
mune from selection bias. Let z D .z1; z2; : : : , zN / represent the prostate
study data of the previous section, with parameter vector (cid:22) D .(cid:22)1; (cid:22)2; : : : ;
(cid:22)N /. Bayes’ rule (3.5)

g.(cid:22)jz/ D g.(cid:22)/f(cid:22).z/=f .z/

(20.42)

yields the posterior density of (cid:22) given z. A data-based model selection
rule such as “estimate the (cid:22)i corresponding to the largest observation zi”
has no effect on the likelihood function f(cid:22).z/ (with z ﬁxed) or on g.(cid:22)jz/.
Having chosen a prior g.(cid:22)/, our posterior estimate of (cid:22)610 is unaffected
by the fact that z610 D 5:29 happens to be largest.
This same argument applies just as well to any data-based model selec-
tion procedure, for instance a preliminary screening of possible variables
to include in a regression analysis—the Cp choice of a cubic regression in
Figure 20.1 having no effect on its Bayes posterior accuracy.

There is a catch: the chosen prior g.(cid:22)/ must apply to the entire param-
eter vector (cid:22) and not just the part we are interested in (e.g., (cid:22)610). This is

 DifferencesFrequency−4−2024020406080100120140uncorrecteddifferencescorrecteddifferences20.4 Combined Bayes–Frequentist Estimation

413

feasible in one-parameter situations like the stopping rule example of Fig-
ure 3.3. It becomes difﬁcult and possibly dangerous in higher dimensions.
Empirical Bayes methods such as Tweedie’s rule can be thought of as al-
lowing the data vector z to assist in the choice of a high-dimensional prior,
an effective collaboration between Bayesian and frequentist methodology.
Our chapter’s ﬁnal vignette concerns another Bayes–frequentist estima-
D ff˛.x/g
tion technique. Dropping the boldface notation, suppose that F
is a multi-dimensional family of densities (5.1) (now with ˛ playing the
role of (cid:22)), and that we are interested in estimating a particular parameter
 D t .˛/. A prior g.˛/ has been chosen, yielding posterior expectation

O
 D E ft .˛/jxg :

(20.43)

How accurate is O
? The usual answer would be calculated from the pos-
terior distribution of  given x. This is obviously the correct answer if g.˛/
is based on genuine prior experience. Most often though, and especially in
high-dimensional problems, the prior reﬂects mathematical convenience
and a desire to be uninformative, as in Chapter 13. There is a danger of
circular reasoning in using a self-selected prior distribution to calculate the
accuracy of its own estimator.
An alternate approach, discussed next, is to calculate the frequentist ac-
curacy of O
; that is, even though (20.43) is a Bayes estimate, we consider
O
 simply as a function of x, and compute its frequentist variability. The
next theorem leads to a computationally efﬁcient way of doing so. (The
Bayes and frequentist standard errors for O
 operate in conceptually orthog-
onal directions as pictured in Figure 3.5. Here we are supposing that the
prior g.(cid:1)/ is unavailable or uncertain, forcing more attention on frequentist
calculations.)

For convenience, we will take the family F to be a p-parameter expo-

nential family (5.50),

f˛.x/ D e˛

0

x(cid:0) .˛/f0.x/;

(20.44)
now with ˛ being the parameter vector called (cid:22) above. The p (cid:2) p covari-
ance matrix of x (5.59) is denoted

V˛ D cov˛.x/:

(20.45)
Let Covx indicate the posterior covariance given x between  D t .˛/, the
parameter of interest, and ˛,

Covx D covf˛; t .˛/jxg ;

(20.46)

6

n O

Inference After Model Selection

o D(cid:0)Cov

414
a p (cid:2) 1 vector. Covx leads directly to a frequentist estimate of accuracy for
O
.
Theorem 20.4  The delta method estimate of standard error for O
 D
Eft .˛/jxg (10.41) isbsedelta
0
x VO˛ Covx
where VO˛ is V˛ evaluated at the MLE O˛.
bsedeltaf O
The theorem allows us to calculate the frequentist accuracy estimate
g with hardly any additional computational effort beyond that re-
quired for O
 itself. Suppose we have used an MCMC or Gibbs sampling
algorithm, Section 13.4, to generate a sample from the Bayes posterior dis-
tribution of ˛ given x,

(cid:1)1=2

(20.47)



;

t

:

˛.1/; ˛.2/; : : : ; ˛.B/:
(cid:16)
˛.b/
These yield the usual estimate for Eft .˛/jxg,
BX
O
 D 1
t .b/ (cid:0) t .(cid:1)/
(cid:16)
˛.b/ (cid:0) ˛.(cid:1)/(cid:16)
BX
b t .b/=B, and ˛.(cid:1)/ DP

They also give a similar expression for covf˛; t .˛/jxg,

t .b/ D t .˛.b//, t .(cid:1)/ DP
can calculate5bsedelta.

O
 / (20.47).

Covx D 1

bD1

bD1

B

B

(20.48)

(20.49)

;

(20.50)

b ˛.b/=B, from which we

For an example of Theorem 20.4 in action we consider the diabetes
i the ith row of X, the 442 (cid:2) 10 matrix of
0
data of Section 20.1, with x
prediction, so xi is the vector of 10 predictors for patient i. The response
vector y of progression scores has now been rescaled to have (cid:27) 2 D 1 in
the normal regression model,6

y (cid:24)

Nn.X ˇ; I/:
The prior distribution g.ˇ/ was taken to be
(cid:0)(cid:21)kˇk1 ;

g.ˇ/ D ce

(20.51)

(20.52)

obtained from parametric bootstrap resampling—taking the empirical covariance matrix

5 VO˛ may be known theoretically, calculated by numerical differentiation in (5.57), or
of bootstrap replications Oˇ
(cid:3)
i .
6 By dividing the original data vector y by its estimated standard error from the linear
model Efyg D X ˇ.

20.4 Combined Bayes–Frequentist Estimation

415
with (cid:21) D 0:37 and c the constant that makes g.ˇ/ integrate to 1. This is
the “Bayesian lasso prior,”so called because of its connection to the lasso, 7
(7.42) and (16.1). (The lasso plays no part in what follows).
posterior distribution g.ˇjy/. Let

An MCMC algorithm generated B D10,000 samples (20.48) from the

B

bD1

0
i ˇ;

(20.54)

(20.53)

0
x
i ˇ:

(cid:16) O

It has Bayes posterior standard error

the (unknown) expectation of the ith patient’s response yi. The Bayes pos-
terior expectation of i is

i D x
BX
O
i D 1
"
(cid:16)
 D
BX
which we can compare withbsedelta.
bD1
O
i /, the frequentist standard error (20.47).
Figure 20.9 shows the 10,000 MCMC replications O
0
 .b/
i ˇ for pa-
tient i D 322. The point estimate O
i
i equaled 2.41, with Bayes and frequen-
bseBayes D 0:203 and bsedelta D 0:186:
tist standard error estimates
The frequentist standard error is 9% smaller in this case;bsedelta was less
thanbseBayes for all 442 patients, the difference averaging a modest 5%.

bseBayes

i ˇ.b/ (cid:0) O
0

Things can work out differently. Suppose we are interested in the poste-

#1=2

2

D x

(20.55)

(20.56)

1

B

i

x

i

;

(

(cid:16)
c; ˇ.b/ D

rior cdf of 332 given y. For any given value of c let
322ˇ.b/  c
0
0
332ˇ.b/ > c;
(cid:16)
c; ˇ.b/

cdf.c/ D 1

BX

if x
if x

so

1

0

t

(20.58)
is our MCMC assessment of Prf322  cjyg. The solid curve in Fig-
ure 20.10 graphs cdf.c/.

bD1

B

t

If we believe prior (20.52) then the curve exactly represents the posterior
distribution of 322 given y (except for the simulation error due to stopping
at B D10,000 replications). Whether or not we believe the prior we can use

(20.57)

416

Inference After Model Selection

Figure 20.9 A histogram of 10,000 MCMC replications for
posterior distribution of 322, expected progression for patient
322 in the diabetes study; model (20.51) and prior (20.52).
The Bayes posterior expectation is 2.41. Frequentist standard
error (20.47) for O
posterior standard error (20.55).

322 D 2:41 was 9% smaller than Bayes

Theorem 20.4, with t .b/ D t .c; ˇ.b// in (20.50), to evaluate the frequentist
The dashed vertical red lines show cdf.c/ plus or minus onebsedelta unit.
accuracy of the curve.
The standard errors are disturbingly large, for instance 0:687 ˙ 0:325 at
c D 2:5. The central 90% credible interval for 322 (the c-values between
cdf.c/ 0.05 and 0.95),

.2:08; 2:73/

(20.59)

has frequentist standard errors about 0.185 for each endpoint—28% of the
interval’s length.

If we believe prior (20.52) then .2:08; 2:73/ is an (almost) exact 90%
credible interval for 322, and moreover is immune to any selection bias
involved in our focus on 322. If not, the large frequentist standard errors
are a reminder that calculation (20.59) might turn out much differently in
a new version of the diabetes study, even ignoring selection bias.

To return to our main theme, Bayesian calculations encourage a disre-
gard for model selection effects. This can be dangerous in objective Bayes

 MCMC q322 valuesFrequency2.02.53.001002003004005006002.41Standard ErrorsBayes Posterior .205Frequentist .18620.5 Notes and Details

417

Figure 20.10 The solid curve is the posterior cdf of 322. Vertical
red bars indicate ˙ one frequentist standard error, as obtained
from Theorem 20.4. Black triangles are endpoints of the 0.90
central credible interval.

settings where one can’t rely on genuine prior experience. Theorem 20.4
serves as a frequentist checkpoint, offering some reassurance as in Fig-
ure 20.9, or sounding a warning as in Figure 20.10.

20.5 Notes and Details

Optimality theories—statements of best possible results—are marks of ma-
turity in applied mathematics. Classical statistics achieved two such theo-
ries: for unbiased or asymptotically unbiased estimation, and for hypothe-
sis testing. Most of this book and all of this chapter venture beyond these
safe havens. How far from best are the Cp/OLS bootstrap smoothed esti-
mates of Section 20.2? At this time we can’t answer such questions, though
we can offer appealing methodologies in their pursuit, a few of which have
been highlighted here.

The cholestyramine example comes from Efron and Feldman (1991)
where it is discussed at length. Data for a control group is also analyzed
there.

1 [p. 398] Scheff´e intervals. Scheff´e’s 1953 paper came at the beginning

2.02.22.42.62.80.00.20.40.60.81.0c−valuePr(q322 < c)418

Inference After Model Selection

of a period of healthy development in simultaneous inference techniques,
mostly in classical normal theory frameworks. Miller (1981) gives a clear
and thorough summary. The 1980s followed with a more computer-intensive
approach, nicely developed in Westfall and Young’s 1993 book, leading up
to Benjamini and Hochberg’s 1995 false-discovery rate paper (Chapter 15
here), and Benjamini and Yekutieli’s (2005) false-coverage rate algorithm.
Scheff´e’s construction (20.15) is derived by transforming (20.6) to the
case V D I using the inverse square root of matrix V ,
(cid:0)1=2ˇ

(20.60)
(cid:0)1), which makes the ellipsoid of Figure 20.2 into a circle.
(.V
Then Q D kO(cid:13) (cid:0) (cid:13)k2=O(cid:27) 2 in (20.10), and for a linear combination (cid:13)d D d
0
it is straightforward to see that PrfQ  k.˛/2

p;q g D ˛ amounts to

ˇ and (cid:13) D V

(cid:0)1=2/2 D V

O(cid:13) D V

(cid:0)1=2 O

(cid:13)

(cid:13)d 2 O(cid:13)d ˙ O(cid:27) kdk k.˛/

p;q

(20.61)

for all choices of d, the geometry of Figure 20.2 now being transparent.
Changing coordinates back to O
ˇ D V 1=2 O(cid:13), ˇ D V 1=2(cid:13), and c D V 1=2d
yields (20.15).

2 [p. 399] Restricting the catalog C . Suppose that all the sample sizes nj in
(20.16) take the same value n, and that we wish to set simultaneous con-
ﬁdence intervals for all pairwise differences ˇi (cid:0) ˇj . Tukey’s studentized
range pivotal quantity (1952, unpublished)

ˇˇˇ(cid:16) O

ˇi (cid:0) O

ˇj

 (cid:0) .ˇi (cid:0) ˇj /

ˇˇˇ

O(cid:27)

T D max
i¤j

(20.62)

has a distribution not depending on (cid:27) or ˇ. This implies that

ˇi (cid:0) ˇj 2 O

ˇi (cid:0) O

ˇj ˙ O(cid:27)p

(20.63)
p
is a set of simultaneous level-˛ conﬁdence intervals for all pairwise dif-
ferences ˇi (cid:0) ˇj , where T .˛/ is the ˛th quantile of T . (The factor 1=
n
comes from O

ˇj (cid:24)

T .˛/

n

N .ˇj ; (cid:27) 2=n/ in (20.16).)

p
Table 20.4 Half-width of Tukey studentized range simultaneous 95%
conﬁdence intervals for pairwise differences ˇi (cid:0) ˇj (in units of O(cid:27) =
n)
for p D 2; 3; : : : ; 6 and n D 20; compared with Scheff´e intervals (20.15).

p

2

Tukey
Scheff´e

2.95
3.74

3

3.58
4.31

4

3.96
4.79

5

4.23
5.21

6

4.44
5.58

20.5 Notes and Details

419

Reducing the catalog C from all linear combinations c

ˇ to only pair-
wise differences shortens the simultaneous intervals. Table 20.4 shows the
comparison between the Tukey and Scheff´e 95% intervals for p D 2; 3;
: : : ; 6 and n D 20.

0

Calculating T .˛/ was a substantial project in the early 1980s. Berk et al.
(2013) now carry out the analogous computations for general catalogs of
linear constraints. They discuss at length the inferential basis of such pro-
cedures.

3 [p. 405] Discontinuous estimators. Looking at Figure 20.6 suggests that a
conﬁdence interval for (cid:0)2:0 t .c; x/ will move far left for data sets x where
Cp selects linear regression (m D 1) as best. This kind of “jumpy” behav-
ior lengthens the intervals needed to attain a desired coverage level. More
seriously, intervals for m D 1 may give misleading inferences, another ex-
ample of “accurate but incorrect” behavior. Bagging (20.28), in addition to
reducing interval length, improves inferential correctness, as discussed in
Efron (2014a).

4 [p. 406] Theorem 20.2 and its corollary. Theorem 20.2 is proved in Sec-
tion 3 of Efron (2014a), with a parametric bootstrap version appearing in
Section 4. The corollary is a projection result illustrated in Figure 4 of
that paper: let L.N / be the n-dimensional subspace of B-dimensional Eu-
clidean space spanned by the columns of the B (cid:2) n matrix .Nbj / (20.29)
and t

(cid:3) the B-vector with components t

(cid:3)b (cid:0) t

bseIJ.s/ıbseboot.t / D(cid:13)(cid:13)Ot

(cid:3)(cid:13)(cid:13)ıkt

(cid:3)(cid:1); then
(cid:3)k;

(cid:3) is the projection of t

where Ot
if O
will be substantially less than 1.

(cid:3) into L.N /. In the language of Section 10.3,
(cid:3) D S.P/ is very nonlinear as a function of P, then the ratio in (20.64)
5 [p. 409] Tweedie’s formula. For convenience, take (cid:27) 2 D 1 in (20.35).



Bayes’ rule (3.5) can then be arranged to give
g.(cid:22)jz/ D e(cid:22)z(cid:0) .z/g.(cid:22)/e
(cid:0) 1

2 (cid:22)2ıp

2(cid:25)

(20.64)

(20.65)

(20.66)

with

 .z/ D 1

2

z C log f .z/:

This is a one-parameter exponential family (5.46) having natural parameter
˛ equal to z. Differentiating   as in (5.55) gives

Ef(cid:22)jzg D d 

dz

D z C d

dz

log f .z/;

(20.67)

Inference After Model Selection

420
which is Tweedie’s formula (20.37) when (cid:27) 2 D 1. The formula ﬁrst ap-
pears in Robbins (1956), who credits it to a personal communication from
M. K. Tweedie. Efron (2011) discusses general exponential family versions
of Tweedie’s formula, and its application to selection bias situations.

6 [p. 414] Theorem 20.4. The delta method standard error approximation for

a statistic T .x/ is bsedelta Dh

.rT .x//

0 OV .rT .x//

;

(20.68)
where rT .x/ is the gradient vector .@T =@xj / and OV is an estimate of the
covariance matrix of x. Other names include the “Taylor series method,”
as in (2.10), and “propagation of errors” in the physical sciences literature.
The proof of Theorem 20.4 in Section 2 of Efron (2015) consists of show-
ing that Covx D rT .x/ when T .x/ D Eft .˛/jxg. Standard deviations are
only a ﬁrst step in assessing the frequentist accuracy of T .x/. The paper
goes on to show how Theorem 20.4 can be improved to give conﬁdence
intervals, correcting the impression in Figure 20.10 that cdf.c/ can range
outside Œ0; 1.

i1=2

and prior (20.52) gives

7 [p. 415] Bayesian lasso. Applying Bayes’ rule (3.5) with density (20.51)

log g.ˇjy/ D (cid:0)(cid:26)ky (cid:0) X ˇk2

2

(cid:27)

C (cid:21)kˇk1

;

(20.69)

as discussed in Tibshirani (2006). Comparison with (7.42) shows that the
maximizing value of ˇ (the “MAP” estimate) agrees with the lasso esti-
mate. Park and Casella (2008) named the “Bayesian lasso” and suggested
an appropriate MCMC algorithm. Their choice (cid:21) D 0:37 was based on
marginal maximum likelihood calculations, giving their analysis an empir-
ical Bayes aspect ignored in their and our analyses.

21

Empirical Bayes Estimation Strategies

Classic statistical inference was focused on the analysis of individual cases:
a single estimate, a single hypothesis test. The interpretation of direct evi-
dence bearing on the case of interest—the number of successes and failures
of a new drug in a clinical trial as a familiar example—dominated statistical
practice.

The story of modern statistics very much involves indirect evidence,
“learning from the experience of others” in the language of Sections 7.4
and 15.3, carried out in both frequentist and Bayesian settings. The computer-
intensive prediction algorithms described in Chapters 16–19 use regression
theory, the frequentist’s favored technique, to mine indirect evidence on a
massive scale. False-discovery rate theory, Chapter 15, collects indirect ev-
idence for hypothesis testing by means of Bayes’ theorem as implemented
through empirical Bayes estimation.

Empirical Bayes methodology has been less studied than Bayesian or
frequentist theory. As with the James–Stein estimator (7.13), it can seem to
be little more than plugging obvious frequentist estimates into Bayes esti-
mation rules. This conceals a subtle and difﬁcult task: learning the equiva-
lent of a Bayesian prior distribution from ongoing statistical observations.
Our ﬁnal chapter concerns the empirical Bayes learning process, both as an
exercise in applied deconvolution and as a relatively new form of statistical
inference. This puts us back where we began in Chapter 1, examining the
two faces of statistical analysis, the algorithmic and the inferential.

21.1 Bayes Deconvolution

A familiar formulation of empirical Bayes inference begins by assuming
that an unknown prior density g. /, our object of interest, has produced a
random sample of real-valued variates ‚1; ‚2; : : : ; ‚N ,
i D 1; 2; : : : ; N:

iid(cid:24) g. /;

‚i

(21.1)

421

Empirical Bayes Estimation Strategies

422
(The “density” g.(cid:1)/ may include discrete atoms of probability.) The ‚i are
unobservable, but each yields an observable random variable Xi according
to a known family of density functions

ind(cid:24) pi .Xij‚i /:

Xi

(21.2)
From the observed sample X1; X2; : : : ; XN we wish to estimate the prior
density g. /.
A famous example has pi .Xij‚i / the Poisson family,

Xi (cid:24) Poi.‚i /;

Xi (cid:24)

(21.3)
as in Robbins’ formula, Section 6.1. Still more familiar is the normal model
(3.28),

(21.4)
often with (cid:27) 2 D 1. A binomial model was used in the medical example of
Section 6.3,

N .‚i ; (cid:27) 2/;

Xi (cid:24) Bi.ni ; ‚i /:

(21.5)
There the ni differ from case to case, accounting for the need for the ﬁrst
subscript i in pi .Xij‚i / (21.2).
Let fi .Xi / denote the marginal density of Xi obtained from (21.1)–

(21.2),

fi .Xi / DZ

T

pi .Xiji /g.i / di ;

(21.6)

the integral being over the space T of possible ‚ values. The statistician
has only the marginal observations available,

ind(cid:24) fi .(cid:1)/;

Xi

i D 1; 2; : : : ; N;

(21.7)

from which he or she wishes to estimate the density g.(cid:1)/ in (21.6).

In the normal model (21.4), fi is the convolution of the unknown g. /

with a known normal density, denoted
f D g (cid:3)

(21.8)
(now fi not depending on i). Estimating g using a sample X1; X2; : : : ; XN
from f is a problem in deconvolution. In general we might call the estima-
tion of g in model (21.1)–(21.2) the “Bayes deconvolution problem.”

N .0; (cid:27) 2/

An artiﬁcial example appears in Figure 21.1, where g. / is a mixture
distribution: seven-eighths N .0; 0:52/ and one-eighth uniform over the in-
terval Œ(cid:0)3; 3. A normal sampling model Xi
N .‚i ; 1/ is assumed, yield-
ing f by convolution as in (21.8). The convolution process makes f wider

ind(cid:24)

21.1 Bayes Deconvolution

423

Figure 21.1 An artiﬁcial example of the Bayes deconvolution
problem. The solid curve is g. /, the prior density of ‚ (21.1);
the dashed curve is the density of an observation X from marginal
distribution f D g (cid:3)
N .0; 1/ (21.8). We wish to estimate g. / on
the basis of a random sample X1; X2; : : : ; XN from f .x/.

and smoother than g, as illustrated in the ﬁgure. Having observed a ran-
dom sample from f , we wish to estimate the deconvolute g, which begins
to look difﬁcult in the ﬁgure’s example.

Deconvolution has a well-deserved reputation for difﬁculty. It is the
classic ill-posed problem: because of the convolution process (21.6), large
changes in g. / are smoothed out, often yielding only small changes in
f .x/. Deconvolution operates in the other direction, with small changes in
the estimation of f disturbingly magniﬁed on the g scale. Nevertheless,
modern computation, modern theory, and most of all modern sample sizes,
together can make empirical deconvolution a practical reality.
Why would we want to estimate g. /? In the prostate data example
(3.28) (where ‚ is called (cid:22)) we might wish to know Prf‚ D 0g, the proba-
bility of a null gene, ones whose effect size is zero; or perhaps Prfj‚j (cid:21) 2g,
the proportion of genes that are substantially non-null. Or we might want to
estimate Bayesian posterior expectations like Ef‚jX D xg in Figure 20.7,
or posterior densities as in Figure 6.5.

Two main strategies have developed for carrying out empirical Bayes
estimation: modeling on the  scale, called g-modeling here, and modeling

−4−20240.000.050.100.15q and xg(q) and f(x)f(x)g(q)424

Empirical Bayes Estimation Strategies

on the x scale, called f -modeling. We begin in the next section with g-
modeling.

21.2 g-Modeling and Estimation

There has been a substantial amount of work on the asymptotic accuracy
of estimates Og. / in the empirical Bayes model (21.1)–(21.2), most often
in the normal sampling framework (21.4). The results are discouraging,
with the rate of convergence of Og. / to g. / as slow as .log N /
(cid:0)1. In our
terminology, much of this work has been carried out in a nonparametric g-
modeling framework, allowing the unknown prior density g. / to be virtu-
ally anything at all. More optimistic results are possible if the g-modeling
is pursued parametrically, that is, by restricting g. / to lie within some
parametric family of possibilities.

We assume, for the sake of simpler exposition, that the space T of pos-

sible ‚ values is ﬁnite and discrete, say

(21.9)
The prior density g. / is now represented by a vector g D .g1; g2; : : : ; gm/
with components

T

0,

gj D Pr˚‚ D .j /

(cid:9) :

D˚.1/; .2/; : : : ; .m/
(cid:9)

for j D 1; 2; : : : ; m:

A p-parameter exponential family (5.50) for g can be written as

g D g.˛/ D eQ˛(cid:0) .˛/;

(21.11)
where the p-vector ˛ is the natural parameter and Q is a known m (cid:2) p
structure matrix. Notation (21.11) means that the j th component of g.˛/
is

0
j the j th row of Q; the function  .˛/ is the normalizer that makes

with Q
g.˛/ sum to 1,

gj .˛/ D eQ

j ˛(cid:0) .˛/;
0

0@ mX

jD1

1A :

 .˛/ D log

0
j ˛

eQ

(21.10)

(21.12)

(21.13)

In the nodes example of Figure 6.4, the set of possible ‚ values was
D f0:01; 0:02; : : : ; 0:99g, and Q was a ﬁfth-degree polynomial matrix,
T
(21.14)

Q D poly(T ,5)

21.2 g-Modeling and Estimation

425

in R notation, indicating a ﬁve-parameter exponential family for g, (6.38)–
(6.39).
In the development that follows we will assume that the kernel pi .(cid:1)j(cid:1)/ in
(21.2) does not depend on i, i.e., that Xi has the same family of conditional
distributions p.Xij‚i / for all i, as in the Poisson and normal situations
(21.3) and (21.4), but not the binomial case (21.5). And moreover we as-
sume that the sample space X for the Xi observations is ﬁnite and discrete,
say
(21.15)

X

D˚x.1/; x.2/; : : : ; x.n/
(cid:9) :
pkj D Pr˚Xi D x.k/j‚i D .j /

(cid:9) ;

None of this is necessary, but it simpliﬁes the exposition.

Deﬁne

(21.16)
for k D 1; 2; : : : ; n and j D 1; 2; : : : ; m, and the corresponding n (cid:2) m
matrix

P D .pkj /;
(21.17)
having kth row Pk D .pk1; pk2; : : : ; pkm/
0. The convolution-type for-
mula (21.6) for the marginal density f .x/ now reduces to an inner product,

˚Xi D x.k/

(cid:9) DPm

jD1 pkj gj .˛/

(21.18)
In fact we can write the entire marginal density f .˛/ D .f1.˛/; f2.˛/; : : : ,

0 in terms of matrix multiplication,

fn.˛//

fk.˛/ D Pr˛
D P
0
kg.˛/:

The vector of counts y D .y1; y2; : : : ; yn/, with

f .˛/ D Pg.˛/:

yk D #˚Xi D x.k/

(cid:9) ;

(21.19)

(21.20)

(21.21)

is a sufﬁcient statistic in the iid situation. It has a multinomial distribution
(5.38),

y (cid:24) Multn.N; f .˛//;

indicating N independent draws for a density f .˛/ on n categories.

All of this provides a concise description of the g-modeling probability

model:
˛ ! g.˛/ D eQ˛(cid:0) .˛/ ! f .˛/ D Pg.˛/ ! y (cid:24) Multn.N; f .˛//:
(21.22)

426

Empirical Bayes Estimation Strategies

The inferential task goes in the reverse direction,

y ! O˛ ! f .O˛/ ! g.O˛/ D eQO˛(cid:0) .O˛/:

(21.23)

Figure 21.2 A schematic diagram of empirical Bayes estimation,
as explained in the text. Sn is the n-dimensional simplex,
containing the p-parameter family F of allowable probability
distributions f .˛/. The vector of observed proportions y=N
yields MLE f .O˛/, which is then deconvolved to obtain estimate
g.O˛/.

A schematic diagram of the estimation process appears in Figure 21.2.

(cid:15) The vector of observed proportions y=N is a point in Sn, the simplex
(cid:15) The parametric family of allowable f vectors (21.19)

(5.39) of all possible probability vectors f on n categories; y=N is the
usual nonparametric estimate of f .

D ff .˛/; ˛ 2 Ag;

F

(21.24)
indicated by the red curve, is a curved p-dimensional surface in Sn. Here
A is the space of allowable vectors ˛ in family (21.11).
(cid:15) The nonparametric estimate y=N is “projected” down to the parametric
estimate f .O˛/; if we are using MLE estimation, f .O˛/ will be the closest
point in F to y=N measured according to a deviance metric, as in (8.35).
(cid:15) Finally, f .O˛/ is mapped back to the estimate g.O˛/, by inverting map-
ping (21.19). (Inversion is not actually necessary with g-modeling since,
having found O˛, g.O˛/ is obtained directly from (21.11); the inversion
step is more difﬁcult for f -modeling, Section 21.6.)

21.3 Likelihood, Regularization, and Accuracy

427

The maximum likelihood estimation process for g-modeling is discussed
in more detail in the next section, where formulas for its accuracy will be
developed.

21.3 Likelihood, Regularization, and Accuracy1

Parametric g-modeling, as in (21.11), allows us to work in low-dimensional
parametric families—just ﬁve parameters for the nodes example (21.14)—
where classic maximum likelihood methods can be more conﬁdently ap-
plied. Even here though, some regularization will be necessary for stable
estimation, as discussed in what follows.

The g-model probability mechanism (21.22) yields a log likelihood for

the multinomial vector y of counts as a function of ˛, say ly .˛/;

fk.˛/yk

yk log fk.˛/:

ly .˛/ D log
Its score function P
h D 1; 2; : : : ; p, determines the MLE O˛ according to P
p (cid:2) p matrix of second derivatives R
Fisher information matrix (5.26)

ly .˛/, the vector of partial derivatives @ly .˛/=@˛h for
ly .O˛/ D 0. The
ly .˛/ D .@2ly .˛/=@˛h@˛l / gives the

(21.25)

  nY

kD1

!

D nX

kD1

The exponential family model (21.11) yields simple expressions for P
and I.˛/. Deﬁne

ly .˛/

(21.26)

ly .˛/g:

I.˛/ D Ef(cid:0)R
 pkj

wkj D gj .˛/



(cid:0) 1

fk.˛/

and the corresponding m-vector

Wk.˛/ D .wk1.˛/; wk2.˛/; : : : ; wkm.˛//
0

Lemma 21.1 The score function P

ly .˛/ under model (21.22) is

(21.27)

:

(21.28)

where WC.˛/ D nX

kD1

P
ly .˛/ D QWC.˛/;

Wk.˛/yk

(21.29)

and Q is the m (cid:2) p structure matrix in (21.11).

1 The technical lemmas in this section are not essential to following the subsequent

discussion.

Empirical Bayes Estimation Strategies

428
Lemma 21.2 The Fisher information matrix I.˛/, evaluated at ˛ D O˛,
is

I.O˛/ D Q
0

Wk.O˛/Nfk.O˛/Wk.O˛/
0

Q;

(21.30)

)

( nX

kD1

1 yk is the sample size in the empirical Bayes model (21.1)–

where N DPn

(21.2).

See the chapter endnotes  for a brief discussion of Lemmas 21.1 and
21.2. I.O˛/
(cid:0)1 is the usual maximum likelihood estimate of the covariance
matrix of O˛, but we will use a regularized version of the MLE that is less
variable.
In the examples that follow, O˛ was found by numerical maximization.2
Even though g.˛/ is an exponential family, the marginal density f .˛/ in
(21.22) is not. As a result, some care is needed in avoiding local maxima of
ly .˛/. These tend to occur at “corner” values of ˛, where one of its compo-
nents goes to inﬁnity. A small amount of regularization pulls O˛ away from
the corners, decreasing its variance at the possible expense of increased
bias.

Instead of maximizing ly .˛/ we maximize a penalized likelihood

m.˛/ D ly .˛/ (cid:0) s.˛/;

(21.31)

where s.˛/ is a positive penalty function. Our examples use

s.˛/ D c0k˛k D c0

(21.32)
(with c0 equal 1), which prevents the maximizer O˛ of m.˛/ from venturing
too far into corners.

hD1

˛2
h

The following lemma is discussed in the chapter endnotes.

Lemma 21.3 The maximizer O˛ of m.˛/ has approximate bias vector and
covariance matrix

Bias.O˛/ D (cid:0) .I.O˛/ C Rs.O˛//
and Var.O˛/ D .I.O˛/ C Rs.O˛//

(cid:0)1 Ps.O˛/
(cid:0)1 I.O˛/ .I.O˛/ C Rs.O˛//

(cid:0)1 ;

(21.33)

  pX

!1=2

where I.O˛/ is given in (21.30).

With s.˛/  0 (no regularization) the bias is zero and Var.O˛/ D I.O˛/

(cid:0)1,

2 Using the nonlinear maximizer nlm in R.

1

2

21.3 Likelihood, Regularization, and Accuracy

429

the usual MLE approximations: including s.˛/ reduces variance while in-
troducing bias.
For s.˛/ D c0k˛k we calculate





Rs.˛/ D c0k˛k

0
I (cid:0) ˛˛
k˛k2

Ps.˛/ D c0˛=k˛k

and

(21.34)
with I the p (cid:2) p identity matrix. Adding the penalty s.˛/ in (21.31) pulls
the MLE of ˛ toward zero and the MLE of g.˛/ toward a ﬂat distribution
over T . Looking at Var.O˛/ in (21.33), a measure of the regularization effect

;

is

tr.Rs.O˛//= tr.I.O˛//;

(21.35)

Most often we will be more interested in the accuracy of Og D g.O˛/ than

which was never more than a few percent in our examples.
in that of O˛ itself. Letting

D.O˛/ D diag.g.O˛// (cid:0) g.O˛/g.O˛/
0

;

the m (cid:2) p derivative matrix .@gj =@˛h/ is

@g=@˛ D D.˛/Q;

(21.36)

(21.37)

with Q the structure matrix in (21.11). The usual ﬁrst-order delta-method
calculations then give the following theorem.
Theorem 21.4 The penalized maximum likelihood estimate Og D g.O˛/
has estimated bias vector and covariance matrix
Bias. Og/ D D.O˛/QBias.O˛/
and Var. Og/ D D.O˛/QVar.O˛/Q
0

D.O˛/

(21.38)

with Bias.O˛/ and Var.O˛/ as in (21.33).3

The many approximations going into Theorem 21.4 can be short-circuited
by means of the parametric bootstrap, Section 10.4. Starting from O˛ and
f .O˛/ D Pg.O˛/, we resample the count vector
(cid:3) (cid:24) Multn.N; f .O˛//;
(cid:3) based on y

and calculate4 the penalized MLE O˛

(cid:3), yielding Og

(cid:3) D g.O˛

(21.39)

y

(cid:3)

/.

3 Note that the bias treats model (21.11) as the true prior, and arises as a result of the
penalization.
4 Convergence of the nlm search process is speeded up by starting from O˛.

(cid:3)B gives bias and covariance estimates

430
B replications Og

Empirical Bayes Estimation Strategies
(cid:3)1; Og
dBias D Og
and cVar D BX
(cid:3)(cid:1) DPB

(cid:3)2; : : : ; Og
(cid:3)(cid:1) (cid:0) Og
. Og

/ı.B (cid:0) 1/;

(cid:3)b (cid:0) Og

bD1
(cid:3)b=B.

Og

(cid:3)(cid:1)

/. Og

(cid:3)b (cid:0) Og

(cid:3)(cid:1)

1

and Og

(21.40)

Table 21.1 Comparison of delta method (21.38) and bootstrap (21.40)
standard errors and biases for the nodes study estimate of g in
Figure 6.4. All columns except the ﬁrst multiplied by 100.

Standard Error

Bias



.01
.12
.23
.34
.45
.56
.67
.78
.89
.99

g. / Delta

12.048
1.045
.381
.779
1.119
.534
.264
.224
.321
.576

.887
.131
.058
.096
.121
.102
.047
.056
.054
.164

Delta

.056
.025

Boot
Boot
.967 (cid:0).518 (cid:0).592
.071
.139
.065
.033
.095 (cid:0).011 (cid:0).013
.117 (cid:0).040 (cid:0).049
.027
.100
.019
.051
.027
.023
.053
.020
.018
.009
.048
.013
.169 (cid:0).008
.008

Table 21.1 compares the delta method of Theorem 20.4 with the para-
metric bootstrap (B D 1000 replications) for the surgical nodes example
of Section 6.3. Both the standard errors—square roots of the diagonal el-
ements of Var. Og/—and biases are well approximated by the delta method
formulas (21.38). The delta method also performed reasonably well on the
two examples of the next section.

It did less well on the artiﬁcial example of Figure 21.1, where

g. / D 1

IŒ(cid:0)3;3. /

8

6

C 7
8

1p
2(cid:25)(cid:27) 2

(cid:0) 1

2

 2
(cid:27) 2

e

.(cid:27) D 0:5/

(21.41)

(1/8 uniform on Œ(cid:0)3; 3 and 7/8 N .0; 0:52/). The vertical bars in Fig-
ure 21.3 indicate ˙ one standard error obtained from the parametric boot-
D f(cid:0)3;(cid:0)2:8; : : : ; 3g for the sample space of ‚, and as-
strap, taking T
suming a natural spline model in (21.11) with ﬁve degrees of freedom,

g.˛/ D eQ˛(cid:0) .˛/;

Q D ns(T ,df=5):

(21.42)

21.3 Likelihood, Regularization, and Accuracy

431

Figure 21.3 The red curve is g. / for the artiﬁcial example of
Figure 21.1. Vertical bars are ˙ one standard error for g-model
estimate g.O˛/; speciﬁcations (21.41)–(21.42), sample size
N D 1000 observations Xi (cid:24)
N .‚i ; 1/, using parametric
bootstrap (21.40), B D 500. The light dashed line follows
bootstrap means Og

(cid:3)
j . Some deﬁnitional bias is apparent.

The sampling model was Xi (cid:24)
N .‚i ; 1/ for i D 1; 2; : : : ; N D 1000. In
this case the delta method standard errors were about 25% too small.
The light dashed curve in Figure 21.3 traces Ng. /, the average of the
B D 500 bootstrap replications g
(cid:3)b. There is noticeable bias, compared
with g. /. The reason is simple: the exponential family (21.42) for g.˛/
does not include g. / (21.41). In fact, Ng. / is (nearly) the closest mem-
ber of the exponential family to g. /. This kind of deﬁnitional bias is a
disadvantage of parametric g-modeling.

(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)——(cid:1)(cid:1)

Our g-modeling examples, and those of the next section, bring together
a variety of themes from modern statistical practice: classical maximum
likelihood theory, exponential family modeling, regularization, bootstrap
methods, large data sets of parallel structure, indirect evidence, and a com-
bination of Bayesian and frequentist thinking, all of this enabled by mas-
sive computer power. Taken together they paint an attractive picture of the
range of inferential methodology in the twenty-ﬁrst century.

−3−2−101230.000.050.100.15qg(q)432

Empirical Bayes Estimation Strategies

21.4 Two Examples

We now reconsider two previous data sets from a g-modeling point of
view. the ﬁrst is the artiﬁcial microarray-type example (20.24) comprising
N D10,000 independent observations

(21.43)

ind(cid:24)

zi

with

(cid:22)i (cid:24)

0

N .(cid:22)i ; 1/;

i D 1; 2; : : : ; N D 10,000;

for i D 1; 2; : : : ; 9000
for i D 9001; : : : ; 10,000:

(
N .(cid:0)3; 1/
(cid:22)i 2 .zi (cid:0) 3/=2 ˙ 1:96ıp

(21.44)
Figure 20.3 displays the points .zi ; (cid:22)i / for i D 9001; : : : ; 10; 000, illus-
trating the Bayes posterior 95% conditional intervals (20.26),

These required knowing the Bayes prior distribution (cid:22)i (cid:24)
N .(cid:0)3; 1/. We
would like to recover intervals (21.45) using just the observed data zi, i D
1; 2; : : : ; 10; 000, without knowledge of the prior.

2:

(21.45)

Figure 21.4 Histogram of observed sample of N D 10,000
values zi from simulations (21.43)–(21.44).

A histogram of the 10,000 z-values is shown in Figure 21.4; g-modeling
(21.9)–(21.11) was applied to them (now with (cid:22) playing the role of “‚”

 z-valuesFrequency−8−6−4−20240200400600800||^^21.4 Two Examples
433
D .(cid:0)6;(cid:0)5:75; : : : ; 3/. Q was composed of a
and z being “x”), taking T
delta function at (cid:22) D 0 and a ﬁfth-degree polynomial basis for the nonzero
(cid:22), again a family of spike-and-slab priors. The penalized MLE Og (21.31),
(21.32), c0 D 1, estimated the probability of (cid:22) D 0 as

Og.0/ D 0:891 ˙ 0:006

(21.46)

(using (21.38), which also provided bias estimate 0.001).

Figure 21.5 Purple curves show g-modeling estimates of
conditional 95% credible intervals for (cid:22) given z in artiﬁcial
microarray example (21.43)–(21.44). They are a close match to
the actual Bayes intervals, dotted lines; cf. Figure 20.3.

The estimated posterior density of (cid:22) given z is
Og.(cid:22)jz/ D cz Og.(cid:22)/(cid:30).z (cid:0) (cid:22)/;

(cid:16)

(21.47)
(cid:30).(cid:1)/ the standard normal density and cz the constant required for Og.(cid:22)jz/
to integrate to 1. Let q.˛/.z/ denote the ˛th quantile of Og.(cid:22)jz/. The purple
curves in Figure 21.5 trace the estimated 95% credible intervals

q.:025/.z/; q.:975/.z/

:

(21.48)

They are a close match to the actual credible intervals (21.45).

The solid black curve in Figure 21.6 shows Og.(cid:22)/ for (cid:22) ¤ 0 (the “slab”
portion of the estimated prior). As an estimate of the actual slab density

−8−6−4−20−10−8−6−4−20Observed zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllBY.loBY.up−2.77llllllllllllllBayes.loBayes.upEB.loEB.up434

Empirical Bayes Estimation Strategies

Figure 21.6 The heavy black curve is the g-modeling estimate of
g.(cid:22)/ for (cid:22) ¤ 0 in the artiﬁcial microarray example, suppressing
the atom at zero, Og.0/ D 0:891. It is only a rough estimate of the
actual nonzero density N .(cid:0)3; 1/.
N .(cid:0)3; 1/ it is only roughly accurate, but apparently still accurate

(cid:22) (cid:24)
enough to yield the reasonably good posterior intervals seen in Figure 21.5.
The fundamental impediment to deconvolution—that large changes in g. /
produce only small changes in f .x/—can sometimes operate in the statis-
tician’s favor, when only a rough knowledge of g sufﬁces for applied pur-
poses.
Our second example concerns the prostate study data, last seen in
Figure 15.1: n D 102 men, 52 cancer patients and 50 normal controls, each
have had their genetic activities measured on a microarray of N D 6033
genes; genei yields a test statistic zi comparing patients with controls,

zi (cid:24)

N .(cid:22)i ; (cid:27) 2
0 /;

(21.49)

with (cid:22)i the gene’s effect size. (Here we will take the variance (cid:27) 2
parameter to be estimated, rather than assuming (cid:27) 2
0
density g.(cid:22)/ for the effects?

0 as a
D 1.) What is the prior

The local false-discovery rate program locfdr, Section 15.5, was ap-
plied to the 6033 zi values, as shown in Figure 21.7. Locfdr is an “f -
modeling” method, where probability models are proposed directly for

−6−4−20240.000.020.040.060.080.10qDensitylatom.891N(−3,1)21.4 Two Examples

435

Figure 21.7 The green curve is a six-parameter Poisson
regression estimate ﬁt to counts of the observed zi values for the
prostate data. The dashed curve is the empirical null (15.48),
zi (cid:24)
N .0:00; 1:062/. The f -modeling program locfdr
estimated null probability Prf(cid:22) D 0g D 0:984. Genes with
z-values lying beyond the red triangles have estimated fdr values
less than 0.20.

the marginal density f .(cid:1)/ rather than for the prior density g.(cid:1)/; see Sec-
tion (21.6). Here we can compare locfdr’s results with those from g-
modeling. The former gave5
ı0; O(cid:27)0; O(cid:25)0

 D .0:00; 1:06; 0:984/

(cid:16)O

(21.50)
in the notation of (15.50); that is, it estimated the null distribution as (cid:22) (cid:24)
N .0; 1:062/, with probability O(cid:25)0 D 0:984 of a gene being null ((cid:22) D 0).
Only 22 genes were estimated to have local fdr values less than 0.20, the
9 with zi  (cid:0)3:71 and the 12 with zi (cid:21) 3:81. (These are more pessimistic
results than in Figure 15.5, where we used the theoretical null N .0; 1/
rather than the empirical null N .0; 1:062/.)
The g-modeling approach (21.11) was applied to the prostate study
data, assuming zi (cid:24)
0 /, (cid:27)0 D 1:06 as suggested by (21.50). The

N .(cid:22)i ; (cid:27) 2

5 Using a six-parameter Poisson regression ﬁt to the zi values, of the type employed in

Section 10.4.

−4−20240100200300400 z-valuesCountsEmpirical Bayes Estimation Strategies

436
structure matrix Q in (21.11) had a delta function at (cid:22) D 0 and a ﬁve-
parameter natural spline basis for (cid:22) ¤ 0; T
D .(cid:0)3:6;(cid:0)3:4; : : : ; 3:6/ for
the discretized ‚ space (21.9). This gave a penalized MLE Og having null
probability

Og.0/ D 0:946 ˙ 0:011:

(21.51)

Figure 21.8 The g-modeling estimate for the non-null density
Og.(cid:22)/, (cid:22) ¤ 0, for the prostate study data, also indicating the
null atom Og.0/ D 0:946. About 2% of the genes are estimated to
have effect sizes j(cid:22)ij (cid:21) 2. The red bars show ˙ one standard
error as computed from Theorem 21.4 (page 429).

The non-null distribution, Og.(cid:22)/ for (cid:22) ¤ 0, appears in Figure 21.8, where
it is seen to be modestly unimodal around (cid:22) D 0. Dashed red bars indicate
˙ one standard error for the Og..j // estimates obtained from Theorem 21.4
(page 429). The accuracy is not very good. It is better for larger regions of
the ‚ space, for examplebPrfjj (cid:21) 2g D 0:020 ˙ 0:0014:

(21.52)

Here g-modeling estimated less prior null probability, 0.946 compared
with 0.984 from f -modeling, but then attributed much of the non-null
probability to small values of j(cid:22)ij.
Taking (21.52) literally suggests 121 (D 0:020 (cid:1) 6033) genes with true

−4−20240.00000.00050.00100.00150.00200.00250.0030qg(q)||lnull atom0.94621.5 Generalized Linear Mixed Models

437

false-discovery ratebPrf(cid:22) D 0jzg from g-modeling. For large
Figure 21.9 The black curve is the empirical Bayes estimated
values of jzj it nearly matches the locfdr f -modeling estimate
fdr.z/, red curve.

effect sizes j(cid:22)ij (cid:21) 2. That doesn’t mean we can say with certainty which
121. Figure 21.9 compares the g-modeling empirical Bayes false-discovery
rate

as in (21.47), with the f -modeling estimatecfdr.z/ produced by locfdr.

(21.53)

;

Where it counts, in the tails, they are nearly the same.

bPrf(cid:22) D 0jzg D cz Og.0/(cid:30)



 z (cid:0) (cid:22)O(cid:27)0

21.5 Generalized Linear Mixed Models

The g-modeling theory can be extended to the situation where each ob-
servation Xi is accompanied by an observed vector of covariates ci, say
of dimension d. We return to the generalized linear model setup of Sec-
tion 8.2, where each Xi has a one-parameter exponential family density
indexed by its own natural parameter (cid:21)i,

f(cid:21)i .Xi / D expf(cid:21)i Xi (cid:0) (cid:13).(cid:21)i /gf0.Xi /

(21.54)

in notation (8.20).

−4−20240.00.20.40.60.81.0z-valuefdr(z)438

Empirical Bayes Estimation Strategies

Our key assumption is that each (cid:21)i is the sum of a deterministic compo-

nent, depending on the covariates ci, and a random term ‚i,

(cid:21)i D ‚i C c

0
i ˇ:

(21.55)
Here ‚i is an unobserved realization from g.˛/ D expfQ˛ (cid:0)  .˛/g
(21.11) and ˇ is an unknown d-dimensional parameter. If ˇ D 0 then
(21.55) is a g-model as before,6 while if all the ‚i D 0 then it is a stan-
dard GLM (8.20)–(8.22). Taken together, (21.55) represents a generalized
linear mixed model (GLMM). The likelihood and accuracy calculations of
Section 21.3 extend to GLMMs, as referenced in the endnotes, but here we
will only discuss a GLMM analysis of the nodes study of Section 6.3.

In addition to ni the number of nodes removed and Xi the number

found positive (6.33), a vector of four covariates

ci D .agei, sexi, smokei, progi /

(21.56)
was observed for each patient: a standardized version of age in years; sex
being 0 for female or 1 for male; smoke being 0 for no or 1 for yes to long-
term smoking; and prog being a post-operative prognosis score with large
values more favorable.
GLMM model (21.55) was applied to the nodes data. Now (cid:21)i was the
logit logŒ(cid:25)i =.1 (cid:0) (cid:25)i /, where

Xi (cid:24) Bi.ni ; (cid:25)i /

(21.57)
as in Table 8.4, i.e., (cid:25)i is the probability that any one node from patient
i is positive. To make the correspondence with the analysis in Section 6.3
exact, we used a variant of (21.55)

(cid:21)i D logit.‚i / C c

0
i ˇ:

(21.58)
Now with ˇ D 0, ‚i is exactly the binomial probability (cid:25)i for the ith
case. Maximum likelihood estimates were calculated for ˛ in (21.11)—
D .0:01; 0:02; : : : ; 0:99/ and Q D poly(T ,5) (21.14)—and
with T
ˇ in (21.58). The MLE prior g.O˛/ was almost the same as that estimated
without covariates in Figure 6.4.
ˇk=bsek. Sex
Table 21.2 shows the MLE values .
(from a parametric bootstrap simulation), and the z-values O
looks like it has a signiﬁcant effect, with males tending toward larger values
of (cid:25)i, that is, a greater number of positive nodes. The big effect though is
prog, larger values of prog indicating smaller values of (cid:25)i.
6 Here the setup is more speciﬁc; f is exponential family, and ‚i is on the

O
ˇ4/, their standard errors

O
ˇ1;

O
ˇ2;

O
ˇ3;

natural-parameter scale.

21.5 Generalized Linear Mixed Models
O
ˇ4/ for GLMM

Table 21.2 Maximum likelihood estimates .
analysis of the nodes data, and standard errors from a parametric
bootstrap simulation; large values of progi predict low values of (cid:25)i.

O
ˇ1;

O
ˇ2;

O
ˇ3;

439

age
(cid:0).078
.066
(cid:0)1.18

sex

.192
.070
2.74

smoke

prog
.089 (cid:0).698
.077
.063
1.41
9.07

MLE
Boot st err
z-value

Figure 21.10 Distribution of (cid:25)i, individual probabilities of a
positive node, for best and worst levels of factor prog; from
GLMM analysis of nodes data.

Figure 21.10 displays the distribution of (cid:25)i D 1=Œ1Cexp.(cid:0)(cid:21)i / implied
by the GLMM model for the best and worst values of prog (setting age,
sex, and smoke to their average values and letting ‚ have distribution
g.O˛/). The implied distribution is concentrated near (cid:25) D 0 for the best-
level prog, while it is roughly uniform over Œ0; 1 for the worst level.

The random effects we have called ‚i are sometimes called frailties: a
composite of unmeasured individual factors lumped together as an index
of disease susceptibility. Taken together, Figures 6.4 and 21.10 show sub-
stantial frailty and covariate effects both at work in the nodes data. In

0.00.20.40.60.81.00.000.050.100.15Probability positive nodeDensityworst prognosisbest prognosis440

Empirical Bayes Estimation Strategies

the language of Section 6.1, we have amassed “indirect evidence” for each
patient, using both Bayesian and frequentist methods.

21.6 Deconvolution and f -Modeling

Empirical Bayes applications have traditionally been dominated by f -
modeling—not the g-modeling approach of the previous sections—where
probability models for the marginal density f .x/, usually exponential fam-
ilies, are ﬁt directly to the observed sample X1; X2; : : : ; XN . We have seen
several examples: Robbins’ estimator in Table 6.1 (particularly the bottom
line), locfdr’s Poisson regression estimates in Figures 15.6 and 21.7, and
Tweedie’s estimate in Figure 20.7.

Both the advantages and the disadvantages of f -modeling can be seen in
the inferential diagram of Figure 21.2. For f -modeling the red curve now
can represent an exponential family ff .˛/g, whose concave log likelihood
function greatly simpliﬁes the calculation of f .O˛/ from y=N . This comes
at a price: the deconvolution step, from f .O˛/ to a prior distribution g.O˛/,
is problematical, as discussed below.

This is only a problem if we want to know g. The traditional applications
of f -modeling apply to problems where the desired answer can be phrased
directly in terms of f . This was the case for Robbins’ formula (6.5), the
local false-discovery rate (15.38), and Tweedie’s formula (20.37).

Nevertheless, f -modeling methodology for the estimation of the prior
g. / does exist, an elegant example being the Fourier method described
next. A function f .x/ and its Fourier transform (cid:30).t / are related by

(cid:30).t / DZ 1

(cid:0)1 f .x/ei tx dx and f .x/ D 1

Z 1
For the normal case where Xi D ‚i C Zi with Zi (cid:24)
transform of f .x/ is a multiple of that for g. /,
(cid:0)t 2=2;

(cid:30)f .t / D (cid:30)g .t /e

(21.60)

(cid:0)1 (cid:30).t /e

2(cid:25)

(cid:0)i tx dt:

(21.59)
N .0; 1/, the Fourier

so, on the transform scale, estimating g from f amounts to removing the
factor exp.t 2=2/.
The Fourier method begins with the empirical density N
f .x/ that puts
probability 1=N on each observed value Xi, and then proceeds in three
steps.
1 N
f .x/ is smoothed using the “sinc” kernel,

3

expressed directly as a kernel estimate,

NX
where the kernel k(cid:21).(cid:1)/ is

Og. / D 1

iD1

N

k(cid:21).Xi (cid:0)  / DZ 1
Z 1=(cid:21)

(cid:0)1 k(cid:21).x (cid:0)  /

N
f .x/ dx;

(21.62)

Q
f .x/ D 1

N (cid:21)

21.6 Deconvolution and f -Modeling

NX

iD1

sinc



 Xi (cid:0) x
Q
f , say Q(cid:30).t /, is calculated.

(cid:21)

;

441

:

(21.61)

sinc.x/ D sin.x/

x

2 The Fourier transform of
3 Finally, Og. / is taken to be the inverse Fourier transform of Q(cid:30).t /et 2=2,
this last step eliminating the unwanted factor e
A pleasantly surprising aspect of the Fourier method is that Og. / can be

(cid:0)t 2=2 in (21.60).

0

(cid:25)

et 2=2 cos.tx/ dt:

k(cid:21).x/ D 1
Large values of (cid:21) smooth N
Og. / at the expense of increased bias.
Despite its compelling rationale, there are two drawbacks to the Fourier
method. First of all, it applies only to situations Xi D ‚i C Zi where Xi is
‚i plus iid noise. More seriously, the bias/variance trade-off in the choice
of (cid:21) can be quite unfavorable.

f .x/ more in (21.61), reducing the variance of

(21.63)

This is illustrated in Figure 21.11 for the artiﬁcial example of Figure 21.1.
The black curve is the standard deviation of the g-modeling estimate of
g. / for  in Œ(cid:0)3; 3, under speciﬁcations (21.41)–(21.42). The red curve
graphs the standard deviation of the f -modeling estimate (21.62), with
(cid:21) D 1=3, a value that produced roughly the same amount of bias as the g-
modeling estimate (seen in Figure 21.3). The ratio of red to black standard
deviations averages more than 20 over the range of .

This comparison is at least partly unfair: g-modeling is parametric while
the Fourier method is almost nonparametric in its assumptions about f .x/
or g. /. It can be greatly improved by beginning the three-step algorithm
with a parametric estimate O
f .x/ rather than N
f .x/. The blue dotted curve in
Figure 21.11 does this with O
f .x/ a Poisson regression on the data X1; X2;
: : : ; XN —as in Figure 10.5 but here using a natural spline basis ns(df=5)
—giving the estimate

(cid:0)1 k(cid:21).x (cid:0)  /

O
f .x/ dx:

(21.64)

Og. / DZ 1

442

Empirical Bayes Estimation Strategies

Figure 21.11 Standard deviations of estimated prior density Og. /
for the artiﬁcial example of Figure 21.1, based on N D 1000
observations Xi (cid:24)
N .‚i ; 1/; black curve using g-modeling
under speciﬁcations (21.41)–(21.42); red curve nonparametric
f -modeling (21.62), (cid:21) D 1=3; blue curve parametric f -modeling
(21.64), with O
structure matrix having ﬁve degrees of freedom.

f .x/ estimated from Poisson regression with a

We see a substantial decrease in standard deviation, though still not attain-
ing g-modeling rates.

As commented before, the great majority of empirical Bayes applica-
tions have been of the Robbins/fdr/Tweedie variety, where f -modeling
is the natural choice. g-modeling comes into its own for situations like
the nodes data analysis of Figures 6.4 and 6.5, where we really want
an estimate of the prior g. /. Twenty-ﬁrst-century science is producing
more such data sets, an impetus for the further development of g-modeling
strategies.
Table 21.3 concerns the g-modeling estimation of Ex D Ef‚jX D xg,

Ex DZ

T

(cid:30)Z

T

g. /f .x/ d

g. /f .x/ d

(21.65)

for the artiﬁcial example, under the same speciﬁcations as in Figure 21.11.
Samples of size N D 1000 of Xi (cid:24)
N .‚i ; 1/ were drawn from model
(21.41)–(21.42), yielding MLE Og. / and estimates OEx for x between (cid:0)4

−3−2−101230.000.010.020.030.040.05qsd g^(q)g−modelparametricf−modelnon−parametricf−model21.6 Deconvolution and f -Modeling

443

Table 21.3 Standard deviation of OEf‚jxg computed from parametric
bootstrap simulations of Og. /. The g-modeling is as in Figure 21.11, with
N D 1000 observations Xi (cid:24)
each simulation. The column “info” is the implied empirical Bayes
information for estimating Ef‚jxg obtained from one “other”
observation Xi.

N .‚i ; 1/ from the artiﬁcial example for

x Ef‚jxg
(cid:0)2:00
(cid:0)1:06
(cid:0):44
(cid:0):13
.13
.44
1.06
2.00

(cid:0)3:5
(cid:0)2:5
(cid:0)1:5
(cid:0):5
.5
1.5
2.5
3.5

sd. OE/
.10
.10
.05
.03
.04
.05
.10
.16

info

.11
.11
.47
.89
.80
.44
.10
.04

and 4. One thousand such estimates OEx were generated, averaging almost
exactly Ex, with standard deviations as shown. Accuracy is reasonably
good, the coefﬁcient of variation sd. OEx/=Ex being about 0.05 for large
values of jxj. (Estimate (21.65) is a favorable case: results are worse for
other conditional estimates such as Ef‚2jX D xg.)
Theorem 21.4 (page 429) implies that, for large values of the sample
size N , the variance of OEx decreases as 1=N , say

4

(21.66)

var

n OEx
.(cid:16)
N (cid:1) var
ix D 1

o :D cx=N:
n OEx

o

By analogy with the Fisher information bound (5.27), we can deﬁne the
empirical Bayes information for estimating Ex in one observation to be

;

(21.67)

(cid:0)1
x =N .

so that varf OExg :D i
Empirical Bayes inference leads us directly into the world of indirect
evidence, learning from the experience of others as in Sections 6.4 and
7.4. So, if Xi D 2:5, each “ other” observation Xj provides 0.10 units of
information for learning Ef‚jXi D 2:5g (compared with the usual Fisher
D 1 for the direct estimation of ‚i from Xi). This
information value I
is a favorable case, as mentioned, and ix is often much smaller. The main
point, perhaps, is that assuming a Bayes prior is not a casual matter, and

444

Empirical Bayes Estimation Strategies

can amount to the assumption of an enormous amount of relevant other
information.

21.7 Notes and Details

Empirical Bayes and James–Stein estimation, Chapters 6 and 7, exploded
onto the statistics scene almost simultaneously in the 1950s. They repre-
sented a genuinely new branch of statistical inference, unlike the computer-
based extensions of classical methodology reviewed in previous chapters.
Their development as practical tools has been comparatively slow. The
pace has quickened in the twenty-ﬁrst century, with false-discovery rates,
Chapter 15, as a major step forward. A practical empirical Bayes method-
ology for use beyond traditional f -modeling venues such as fdr is the goal
of the g-modeling approach.

1 [p. 428] Lemmas 21.1 and 21.2. The derivations of Lemmas 21.1 and 21.2
are straightforward but somewhat involved exercises in differential calcu-
lus, carried out in Remark B of Efron (2016). Here we will present just
P
fk.˛/ D
a sample of the calculations. From (21.18), the gradient vector
.@fk.˛/=@˛l / with respect to ˛ is

P
fk.˛/ D Pg.˛/
0
where Pg.˛/ is the m (cid:2) p derivative matrix

Pk;

Pg.˛/ D .@gj .˛/=@˛l / D DQ;

(21.68)

(21.69)

with D as in (21.36), the last equality following, after some work, by dif-
ferentiation of log g.˛/ D Q˛ (cid:0) (cid:30).˛/.
Let lk D log fk (now suppressing ˛ from the notation). The gradient
with respect to ˛ of lk is then
P
lk D P

fk=fk D Q
0

DPk=fk:

(21.70)

The vector DPk=fk has components

.gj pkj (cid:0) gj fk/=fk D wkj

(21.71)

Pk D fk. This gives P
0Pn
0
(21.27), using g
the independent score functions P
score P
ly .˛/ D Q
O D Pm.O˛/

2 [p. 428] Lemma 2. The penalized MLE O˛ satisﬁes

1 ykWk.˛/, which is Lemma 21.1.

:D Pm.˛0/ C Rm.˛0/.O˛ (cid:0) ˛0/;

lk D Q
0

Wk.˛/ (21.28). Adding up
lk over the full sample yields the overall

(21.72)

21.7 Notes and Details

(cid:0)1 Pm.˛0/

(cid:16)(cid:0)R

:D .(cid:0) Rm.˛0//

(cid:0)1(cid:16)P
where ˛0 is the true value of ˛, or
O˛ (cid:0) ˛0
Standard MLE theory shows that the random variable P
and covariance Fisher information matrix I.˛0/, while (cid:0)R
ically approximates I.˛0/. Substituting in (21.73),
(cid:0)1Z;

:D .I.˛0/ C Rs.˛0//

ly .˛0/ C Rs.˛0/

O˛ (cid:0) ˛0

445



ly .˛0/ (cid:0) Ps.˛0/

:

(21.73)
ly .˛0/ has mean 0
ly .˛0/ asymptot-

(21.74)
where Z has mean (cid:0)Ps.˛0/ and covariance I.˛0/. This gives Bias.O˛/ and
Var.O˛/ as in Lemma 2. Note that the bias is with respect to a true parametric
model (21.11), and is a consequence of the penalization.
3 [p. 440] The sinc kernel. The Fourier transform (cid:30)s.t / of the scaled sinc
function s.x/ D sin.x=(cid:21)/=.(cid:25)x/ is the indicator of the interval Œ(cid:0)1=(cid:21); 1=(cid:21),
while that of
1 exp.i tXj /. Formula (21.61) is the convo-
lution N

f (cid:3) s, so Q

f .x/ is .1=N /PN
24 1
NX
f has the product transform
(cid:30) Qf .t / D

35 IŒ(cid:0)1=(cid:21);1=(cid:21).t /:

(21.75)

ei tXj

N

N

jD1

The effect of the sinc convolution is to censor the high-frequency (large t)
N
f or (cid:30) Nf . Larger (cid:21) yields more censoring. Formula (21.63)
components of
has upper limits 1=(cid:21) because of (cid:30)s.t /. All of this is due to Stefanski and
Carroll (1990). Smoothers other than the sinc kernel have been suggested
in the literature, but without substantial improvements on deconvolution
performance.
4 [p. 443] Conditional expectation (21.65). Efron (2014b) considers estimat-
ing Ef‚2jX D xg and other such conditional expectations, both for f -
modeling and for g-modeling. Ef‚jX D xg is by far the easiest case, as
might be expected from the simple form of Tweedie’s estimate (20.37).

Epilogue

Something important changed in the world of statistics in the new millen-
nium. Twentieth-century statistics, even after the heated expansion of its
late period, could still be contained within the classic Bayesian–frequentist–
Fisherian inferential triangle (Figure 14.1). This is not so in the twenty-ﬁrst
century. Some of the topics discussed in Part III—false-discovery rates,
post-selection inference, empirical Bayes modeling, the lasso—ﬁt within
the triangle but others seem to have escaped, heading south from the fre-
quentist corner, perhaps in the direction of computer science.

The escapees were the large-scale prediction algorithms of Chapters 17–
19: neural nets, deep learning, boosting, random forests, and support-vector
machines. Notably missing from their development were parametric prob-
ability models, the building blocks of classical inference. Prediction algo-
rithms are the media stars of the big-data era. It is worth asking why they
have taken center stage and what it means for the future of the statistics
discipline.

The why is easy enough: prediction is commercially valuable. Modern
equipment has enabled the collection of mountainous data troves, which
the “data miners” can then burrow into, extracting valuable information.
Moreover, prediction is the simplest use of regression theory (Section 8.4).
It can be carried out successfully without probability models, perhaps with
the assistance of nonparametric analysis tools such as cross-validation, per-
mutations, and the bootstrap.

A great amount of ingenuity and experimentation has gone into the
development of modern prediction algorithms, with statisticians playing
an important but not dominant role.1 There is no shortage of impressive
success stories. In the absence of optimality criteria, either frequentist or
Bayesian, the prediction community grades algorithmic excellence on per-

1 All papers mentioned in this section have their complete references in the bibliography.

Footnotes will identify papers not fully speciﬁed in the text.

446

Epilogue

447

formance within a catalog of often-visited examples such as the spam and
digits data sets of Chapters 17 and 18.2 Meanwhile, “traditional statistics”
—probability models, optimality criteria, Bayes priors, asymptotics—has
continued successfully along on a parallel track. Pessimistically or opti-
mistically, one can consider this as a bipolar disorder of the ﬁeld or as a
healthy duality that is bound to improve both branches. There are histori-
cal and intellectual arguments favoring the optimists’ side of the story.

The ﬁrst thing to say is that the current situation is not entirely unprece-
dented. By the end of the nineteenth century there was available an im-
pressive inventory of statistical methods—Bayes’ theorem, least squares,
correlation, regression, the multivariate normal distribution—but these ex-
isted more as individual algorithms than as a uniﬁed discipline. Statistics
as a distinct intellectual enterprise was not yet well-formed.

A small but crucial step forward was taken in 1914 when the astrophysi-
cist Arthur Eddington3 claimed that mean absolute deviation was superior
to the familiar root mean square estimate for the standard deviation from a
normal sample. Fisher in 1919 showed that this was wrong, and moreover,
in a clear mathematical sense, the root mean square was the best possible
estimate. Eddington conceded the point while Fisher went on to develop
the theory of sufﬁciency and optimal estimation.4

“Optimal” is the key word here. Before Fisher, statisticians didn’t really
understand estimation. The same can be said now about prediction. Despite
their impressive performance on a raft of test problems, it might still be
possible to do much better than neural nets, deep learning, random forests,
and boosting—or perhaps they are coming close to some as-yet unknown
theoretical minimum.

It is the job of statistical inference to connect “dangling algorithms” to
the central core of well-understood methodology. The connection process
is already underway. Section 17.4 showed how Adaboost, the original
machine learning algorithm, could be restated as a close cousin of logis-
tic regression. Purely empirical approaches like the Common Task Frame-
work are ultimately unsatisfying without some form of principled justi-
ﬁcation. Our optimistic scenario has the big-data/data-science prediction
world rejoining the mainstream of statistical inference, to the beneﬁt of
both branches.

2 This empirical approach to optimality is sometimes codiﬁed as the Common Task

Framework (Liberman, 2015 and Donoho, 2015).

3 Eddington became world-famous for his 1919 empirical veriﬁcation of Einstein’s

relativity theory.

4 See Stigler (2006) for the full story.

448

Epilogue

Development of the statistics discipline since the end of the nine-
teenth century, as discussed in the text.

Whether or not we can predict the future of statistics, we can at least
examine the past to see how we’ve gotten where we are. The next ﬁgure
does so in terms of a new triangle diagram, this time with the poles la-
beled Applications, Mathematics, and Computation. “Mathematics” here
is shorthand for the mathematical/logical justiﬁcation of statistical meth-
ods. “Computation” stands for the empirical/numerical approach.

Statistics is a branch of applied mathematics, and is ultimately judged
by how well it serves the world of applications. Mathematical logic, `a
la Fisher, has been the traditional vehicle for the development and under-
standing of statistical methods. Computation, slow and difﬁcult before the
1950s, was only a bottleneck, but now has emerged as a competitor to (or
perhaps a seven-league boots enabler of) mathematical analysis. At any
one time the discipline’s energy and excitement is directed unequally to-
ward the three poles. The ﬁgure attempts, in admittedly crude fashion, to
track the changes in direction over the past 100C years.

 lllMathematicsComputationApplicationsl19th Century1900190819251933193719501962197219791995200020012016b2016aEpilogue

449

The tour begins at the end of the nineteenth century. Mathematicians of
the caliber of Gauss and Laplace had contributed to the available method-
ology, but the subsequent development was almost entirely applications-
driven. Quetelet5 was especially inﬂuential, applying the Gauss–Laplace
formulation to census data and his “Average Man.” A modern reader will
search almost in vain for any mathematical symbology in nineteenth-century
statistics journals.

1900

Karl Pearson’s chi-square paper was a bold step into the new century, ap-
plying a new mathematical tool, matrix theory, in the service of statisti-
cal methodology. He and Weldon went on to found Biometrika in 1901,
the ﬁrst recognizably modern statistics journal. Pearson’s paper, and Bio-
metrika, launched the statistics discipline on a ﬁfty-year march toward the
mathematics pole of the triangle.

Student’s t statistic was a crucial ﬁrst result in small-sample “exact” infer-
ence, and a major inﬂuence on Fisher’s thinking.

1908

1925

Fisher’s great estimation paper—a more coherent version of its 1922 pre-
decessor. It introduced a host of fundamental ideas, including sufﬁciency,
efﬁciency, Fisher information, maximum likelihood theory, and the notion
of optimal estimation. Optimality is a mark of maturity in mathematics,
making 1925 the year statistical inference went from a collection of inge-
nious techniques to a coherent discipline.

1933

This represents Neyman and Pearson’s paper on optimal hypothesis test-
ing. A logical completion of Fisher’s program, it nevertheless aroused his
strong antipathy. This was partly personal, but also reﬂected Fisher’s con-
cern that mathematization was squeezing intuitive correctness out of statis-
tical thinking (Section 4.2).

1937

Neyman’s seminal paper on conﬁdence intervals. His sophisticated mathe-
matical treatment of statistical inference was a harbinger of decision theory.

5 Adolphe Quetelet was a tireless organizer, helping found the Royal Statistical Society in

1834, with the American Statistical Association following in 1839.

450

Epilogue

1950

The publication of Wald’s Statistical Decision Functions. Decision theory
completed the full mathematization of statistical inference. This date can
also stand for Savage’s and de Finetti’s decision-theoretic formulation of
Bayesian inference. We are as far as possible from the Applications corner
of the triangle now, and it is fair to describe the 1950s as a nadir of the
inﬂuence of the statistics discipline on scientiﬁc applications.

1962

The arrival of electronic computation in the mid 1950s began the process
of stirring statistics out of its inward-gazing preoccupation with mathe-
matical structure. Tukey’s paper “The future of data analysis” argued for
a more application- and computation-oriented discipline. Mosteller and
Tukey later suggested changing the ﬁeld’s name to data analysis, a pre-
scient hint of today’s data science.

1972

Cox’s proportional hazards paper. Immensely useful in its own right, it sig-
naled a growing interest in biostatistical applications and particularly sur-
vival analysis, which was to assert its scientiﬁc importance in the analysis
of AIDS epidemic data.

The bootstrap, and later the widespread use of MCMC: electronic compu-
tation used for the extension of classic statistical inference.

1979

1995

This stands for false-discovery rates and, a year later, the lasso.6 Both are
computer-intensive algorithms, ﬁrmly rooted in the ethos of statistical in-
ference. They lead, however, in different directions, as indicated by the
split in the diagram.

Microarray technology inspires enormous interest in large-scale inference,
both in theory and as applied to the analysis of microbiological data.

2000

6 Benjamini and Hochberg (1995) and Tibshirani (1996).

Epilogue

2001

451

Random forests; it joins boosting7 and the resurgence of neural nets in the
ranks of machine learning prediction algorithms.

2016a

Data science: a more popular successor to Tukey and Mosteller’s “data
analysis,” at one extreme it seems to represent a statistics discipline with-
out parametric probability models or formal inference. The Data Science
Association deﬁnes a practitioner as one who “. . . uses scientiﬁc methods
to liberate and create meaning from raw data.” In practice the emphasis is
on the algorithmic processing of large data sets for the extraction of useful
information, with the prediction algorithms as exemplars.

2016b

This represents the traditional line of statistical thinking, of the kind that
could be located within Figure 14.1, but now energized with a renewed
focus on applications. Of particular applied interest are biology and ge-
netics. Genome-wide association studies (GWAS) show a different face of
big data. Prediction is important here,8 but not sufﬁcient for the scientiﬁc
understanding of disease.

A cohesive inferential theory was forged in the ﬁrst half of the twenti-
eth century, but unity came at the price of an inwardly focused discipline,
of reduced practical utility. In the century’s second half, electronic com-
putation unleashed a vast expansion of useful—and much used—statistical
methodology. Expansion accelerated at the turn of the millennium, further
increasing the reach of statistical thinking, but now at the price of intellec-
tual cohesion.

It is tempting but risky to speculate on the future of statistics. What
will the Mathematics–Applications–Computation diagram look like, say
25 years from now? The appetite for statistical analysis seems to be always
increasing, both from science and from society in general. Data science
has blossomed in response, but so has the traditional wing of the ﬁeld. The
data-analytic initiatives represented in the diagram by 2016a and 2016b are
in actuality not isolated points but the centers of overlapping distributions.

7 Breiman (1996) for random forests, Freund and Schapire (1997) for boosting.
8 “Personalized medicine” in which an individual’s genome predicts his or her optimal

treatment has attracted grail-like attention.

452

Epilogue

A hopeful scenario for the future is one of an increasing overlap that puts
data science on a solid footing while leading to a broader general formula-
tion of statistical inference.

References

Abu-Mostafa, Y. 1995. Hints. Neural Computation, 7, 639–671.
Achanta, R., and Hastie, T. 2015. Telugu OCR Framework using Deep Learning. Tech.

rept. Statistics Department, Stanford University.

Akaike, H. 1973. Information theory and an extension of the maximum likelihood prin-
ciple. Pages 267–281 of: Second International Symposium on Information Theory
(Tsahkadsor, 1971). Akad´emiai Kiad´o, Budapest.

Anderson, T. W. 2003. An Introduction to Multivariate Statistical Analysis. Third edn.

Wiley Series in Probability and Statistics. Wiley-Interscience.

Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A.,
Bouchard, N., and Bengio, Y. 2012. Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.

Becker, R., Chambers, J., and Wilks, A. 1988. The New S Language: A Programming
Environment for Data Analysis and Graphics. Paciﬁc Grove, CA: Wadsworth and
Brooks/Cole.

Bellhouse, D. R. 2004. The Reverend Thomas Bayes, FRS: A biography to celebrate the
tercentenary of his birth. Statist. Sci., 19(1), 3–43. With comments and a rejoinder
by the author.

Bengio, Y., Courville, A., and Vincent, P. 2013. Representation learning: a review and
new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence,
35(8), 1798–1828.

Benjamini, Y., and Hochberg, Y. 1995. Controlling the false discovery rate: A practical
and powerful approach to multiple testing. J. Roy. Statist. Soc. Ser. B, 57(1), 289–
300.

Benjamini, Y., and Yekutieli, D. 2005. False discovery rate-adjusted multiple conﬁ-

dence intervals for selected parameters. J. Amer. Statist. Assoc., 100(469), 71–93.

Berger, J. O. 2006. The case for objective Bayesian analysis. Bayesian Anal., 1(3),

385–402 (electronic).

Berger, J. O., and Pericchi, L. R. 1996. The intrinsic Bayes factor for model selection

and prediction. J. Amer. Statist. Assoc., 91(433), 109–122.

Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,
J., Warde-Farley, D., and Bengio, Y. 2010 (June). Theano: a CPU and GPU math
expression compiler. In: Proceedings of the Python for Scientiﬁc Computing Con-
ference (SciPy).

Berk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. 2013. Valid post-selection

inference. Ann. Statist., 41(2), 802–837.

453

454

References

Berkson, J. 1944. Application of the logistic function to bio-assay. J. Amer. Statist.

Assoc., 39(227), 357–365.

Bernardo, J. M. 1979. Reference posterior distributions for Bayesian inference. J. Roy.
Birch, M. W. 1964. The detection of partial association. I. The 2(cid:2)2 case. J. Roy. Statist.

Statist. Soc. Ser. B, 41(2), 113–147. With discussion.

Soc. Ser. B, 26(2), 313–324.

Bishop, C. 1995. Neural Networks for Pattern Recognition. Clarendon Press, Oxford.
Boos, D. D., and Serﬂing, R. J. 1980. A note on differentials and the CLT and LIL for
statistical functions, with application to M -estimates. Ann. Statist., 8(3), 618–624.
Boser, B., Guyon, I., and Vapnik, V. 1992. A training algorithm for optimal margin

classiﬁers. In: Proceedings of COLT II.

Breiman, L. 1996. Bagging predictors. Mach. Learn., 24(2), 123–140.
Breiman, L. 1998. Arcing classiﬁers (with discussion). Annals of Statistics, 26, 801–

849.

Breiman, L. 2001. Random forests. Machine Learning, 45, 5–32.
Breiman, L., Friedman, J., Olshen, R. A., and Stone, C. J. 1984. Classiﬁcation and
Regression Trees. Wadsworth Statistics/Probability Series. Wadsworth Advanced
Books and Software.

Carlin, B. P., and Louis, T. A. 1996. Bayes and Empirical Bayes Methods for Data
Analysis. Monographs on Statistics and Applied Probability, vol. 69. Chapman &
Hall.

Carlin, B. P., and Louis, T. A. 2000. Bayes and Empirical Bayes Methods for Data

Analysis. 2 edn. Texts in Statistical Science. Chapman & Hall/CRC.

Chambers, J. M., and Hastie, T. J. (eds). 1993. Statistical Models in S. Chapman &

Hall Computer Science Series. Chapman & Hall.

Cleveland, W. S. 1981. LOWESS: A program for smoothing scatterplots by robust

locally weighted regression. Amer. Statist., 35(1), 54.

Cox, D. R. 1958. The regression analysis of binary sequences. J. Roy. Statist. Soc. Ser.

B, 20, 215–242.

Cox, D. R. 1970. The Analysis of Binary Data. Methuen’s Monographs on Applied

Probability and Statistics. Methuen & Co.

Cox, D. R. 1972. Regression models and life-tables. J. Roy. Statist. Soc. Ser. B, 34(2),

187–220.

Cox, D. R. 1975. Partial likelihood. Biometrika, 62(2), 269–276.
Cox, D. R., and Hinkley, D. V. 1974. Theoretical Statistics. Chapman & Hall.
Cox, D. R., and Reid, N. 1987. Parameter orthogonality and approximate conditional

inference. J. Roy. Statist. Soc. Ser. B, 49(1), 1–39. With a discussion.

Crowley, J. 1974. Asymptotic normality of a new nonparametric statistic for use in

organ transplant studies. J. Amer. Statist. Assoc., 69(348), 1006–1011.

de Finetti, B. 1972. Probability, Induction and Statistics. The Art of Guessing. John

Wiley & Sons, London-New York-Sydney.

Dembo, A., Cover, T. M., and Thomas, J. A. 1991. Information-theoretic inequalities.

IEEE Trans. Inform. Theory, 37(6), 1501–1518.

Dempster, A. P., Laird, N. M., and Rubin, D. B. 1977. Maximum likelihood from

incomplete data via the EM algorithm. J. Roy. Statist. Soc. Ser. B, 39(1), 1–38.

Diaconis, P., and Ylvisaker, D. 1979. Conjugate priors for exponential families. Ann.

Statist., 7(2), 269–281.

References

455

DiCiccio, T., and Efron, B. 1992. More accurate conﬁdence intervals in exponential

families. Biometrika, 79(2), 231–245.

Donoho, D. L. 2015. 50 years of data science. R-bloggers. www.r-bloggers.

com/50-years-of-data-science-by-david-donoho/.

Edwards, A. W. F. 1992. Likelihood. Expanded edn. Johns Hopkins University Press.

Revised reprint of the 1972 original.

Efron, B. 1967. The two sample problem with censored data. Pages 831–853 of: Proc.
5th Berkeley Symp. Math. Statist. and Prob., Vol. 4. University of California Press.
Efron, B. 1975. Deﬁning the curvature of a statistical problem (with applications to
second order efﬁciency). Ann. Statist., 3(6), 1189–1242. With discussion and a
reply by the author.

Efron, B. 1977. The efﬁciency of Cox’s likelihood function for censored data. J. Amer.

Statist. Assoc., 72(359), 557–565.

Efron, B. 1979. Bootstrap methods: Another look at the jackknife. Ann. Statist., 7(1),

1–26.

Efron, B. 1982. The Jackknife, the Bootstrap and Other Resampling Plans. CBMS-NSF
Regional Conference Series in Applied Mathematics, vol. 38. Society for Industrial
and Applied Mathematics (SIAM).

Efron, B. 1983. Estimating the error rate of a prediction rule: Improvement on cross-

validation. J. Amer. Statist. Assoc., 78(382), 316–331.

Efron, B. 1985. Bootstrap conﬁdence intervals for a class of parametric problems.

Biometrika, 72(1), 45–58.

Efron, B. 1986. How biased is the apparent error rate of a prediction rule? J. Amer.

Statist. Assoc., 81(394), 461–470.

Efron, B. 1987. Better bootstrap conﬁdence intervals. J. Amer. Statist. Assoc., 82(397),

171–200. With comments and a rejoinder by the author.

Efron, B. 1988. Logistic regression, survival analysis, and the Kaplan–Meier curve. J.

Amer. Statist. Assoc., 83(402), 414–425.

Efron, B. 1993.

Bayes and likelihood calculations from conﬁdence intervals.

Biometrika, 80(1), 3–26.

Efron, B. 1998. R. A. Fisher in the 21st Century (invited paper presented at the 1996
R. A. Fisher Lecture). Statist. Sci., 13(2), 95–122. With comments and a rejoinder
by the author.

Efron, B. 2004. The estimation of prediction error: Covariance penalties and cross-
validation. J. Amer. Statist. Assoc., 99(467), 619–642. With comments and a rejoin-
der by the author.

Efron, B. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Test-
ing, and Prediction. Institute of Mathematical Statistics Monographs, vol. 1. Cam-
bridge University Press.

Efron, B. 2011. Tweedie’s formula and selection bias. J. Amer. Statist. Assoc., 106(496),

1602–1614.

Efron, B. 2014a. Estimation and accuracy after model selection. J. Amer. Statist. Assoc.,

109(507), 991–1007.

Efron, B. 2014b. Two modeling strategies for empirical Bayes estimation. Statist. Sci.,

29(2), 285–301.

Efron, B. 2015. Frequentist accuracy of Bayesian estimates. J. Roy. Statist. Soc. Ser. B,

77(3), 617–646.

456

References

Efron, B. 2016. Empirical Bayes deconvolution estimates. Biometrika, 103(1), 1–20.
Efron, B., and Feldman, D. 1991. Compliance as an explanatory variable in clinical

trials. J. Amer. Statist. Assoc., 86(413), 9–17.

Efron, B., and Gous, A. 2001. Scales of evidence for model selection: Fisher versus
Jeffreys. Pages 208–256 of: Model Selection. IMS Lecture Notes Monograph Series,
vol. 38. Beachwood, OH: Institute of Mathematics and Statististics. With discussion
and a rejoinder by the authors.

Efron, B., and Hinkley, D. V. 1978. Assessing the accuracy of the maximum likelihood
estimator: Observed versus expected Fisher information. Biometrika, 65(3), 457–
487. With comments and a reply by the authors.

Efron, B., and Morris, C. 1972. Limiting the risk of Bayes and empirical Bayes estima-

tors. II. The empirical Bayes case. J. Amer. Statist. Assoc., 67, 130–139.

Efron, B., and Morris, C. 1977. Stein’s paradox in statistics. Scientiﬁc American,

236(5), 119–127.

Efron, B., and Petrosian, V. 1992. A simple test of independence for truncated data

with applications to redshift surveys. Astrophys. J., 399(Nov), 345–352.

Efron, B., and Stein, C. 1981. The jackknife estimate of variance. Ann. Statist., 9(3),

586–596.

Efron, B., and Thisted, R. 1976. Estimating the number of unseen species: How many

words did Shakespeare know? Biometrika, 63(3), 435–447.

Efron, B., and Tibshirani, R. 1993. An Introduction to the Bootstrap. Monographs on

Statistics and Applied Probability, vol. 57. Chapman & Hall.

Efron, B., and Tibshirani, R. 1997. Improvements on cross-validation: The .632+ boot-

strap method. J. Amer. Statist. Assoc., 92(438), 548–560.

Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. 2004. Least angle regression. An-
nals of Statistics, 32(2), 407–499. (with discussion, and a rejoinder by the authors).
Finney, D. J. 1947. The estimation from individual records of the relationship between

dose and quantal response. Biometrika, 34(3/4), 320–334.

Fisher, R. A. 1915. Frequency distribution of the values of the correlation coefﬁcient in

samples from an indeﬁnitely large population. Biometrika, 10(4), 507–521.

Fisher, R. A. 1925. Theory of statistical estimation. Math. Proc. Cambridge Phil. Soc.,

22(7), 700–725.
Fisher, R. A. 1930.

Inverse probability. Math. Proc. Cambridge Phil. Soc., 26(10),

528–535.

Fisher, R. A., Corbet, A., and Williams, C. 1943. The relation between the number of
species and the number of individuals in a random sample of an animal population.
J. Anim. Ecol., 12, 42–58.

Fithian, W., Sun, D., and Taylor, J. 2014. Optimal inference after model selection.

ArXiv e-prints, Oct.

Freund, Y., and Schapire, R. 1996. Experiments with a new boosting algorithm. Pages
148–156 of: Machine Learning: Proceedings of the Thirteenth International Con-
ference. Morgan Kauffman, San Francisco.

Freund, Y., and Schapire, R. 1997. A decision-theoretic generalization of online learn-
ing and an application to boosting. Journal of Computer and System Sciences, 55,
119–139.

Friedman, J. 2001. Greedy function approximation: a gradient boosting machine. An-

nals of Statistics, 29(5), 1189–1232.

References

457

Friedman, J., and Popescu, B. 2005. Predictive Learning via Rule Ensembles. Tech.

rept. Stanford University.

Friedman, J., Hastie, T., and Tibshirani, R. 2000. Additive logistic regression: a statis-

tical view of boosting (with discussion). Annals of Statistics, 28, 337–307.

Friedman, J., Hastie, T., and Tibshirani, R. 2009. glmnet: Lasso and elastic-net regu-

larized generalized linear models. R package version 1.1-4.

Friedman, J., Hastie, T., and Tibshirani, R. 2010. Regularization paths for generalized

linear models via coordinate descent. Journal of Statistical Software, 33(1), 1–22.

Geisser, S. 1974. A predictive approach to the random effect model. Biometrika, 61,

101–107.

Gerber, M., and Chopin, N. 2015. Sequential quasi Monte Carlo. J. Roy. Statist. Soc.

B, 77(3), 509–580. with discussion, doi: 10.1111/rssb.12104.

Gholami, S., Janson, L., Worhunsky, D. J., Tran, T. B., Squires, Malcolm, I., Jin, L. X.,
Spolverato, G., Votanopoulos, K. I., Schmidt, C., Weber, S. M., Bloomston, M., Cho,
C. S., Levine, E. A., Fields, R. C., Pawlik, T. M., Maithel, S. K., Efron, B., Norton,
J. A., and Poultsides, G. A. 2015. Number of lymph nodes removed and survival after
gastric cancer resection: An analysis from the US Gastric Cancer Collaborative. J.
Amer. Coll. Surg., 221(2), 291–299.

Good, I., and Toulmin, G. 1956. The number of new species, and the increase in popu-

lation coverage, when a sample is increased. Biometrika, 43, 45–63.

Hall, P. 1988. Theoretical comparison of bootstrap conﬁdence intervals. Ann. Statist.,

16(3), 927–985. with discussion and a reply by the author.

Hampel, F. R. 1974. The inﬂuence curve and its role in robust estimation. J. Amer.

Statist. Assoc., 69, 383–393.

Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. 1986. Robust
Statistics: The approach based on inﬂuence functions. Wiley Series in Probability
and Mathematical Statistics. John Wiley & Sons.

Harford, T. 2014. Big data: A big mistake? Signiﬁcance, 11(5), 14–19.
Hastie, T., and Loader, C. 1993. Local regression: automatic kernel carpentry (with

discussion). Statistical Science, 8, 120–143.

Hastie, T., and Tibshirani, R. 1990. Generalized Additive Models. Chapman and Hall.
Hastie, T., and Tibshirani, R. 2004. Efﬁcient quadratic regularization for expression

arrays. Biostatistics, 5(3), 329–340.

Hastie, T., Tibshirani, R., and Friedman, J. 2009. The Elements of Statistical Learning.
Data mining, Inference, and Prediction. Second edn. Springer Series in Statistics.
Springer.

Hastie, T., Tibshirani, R., and Wainwright, M. 2015. Statistical Learning with Sparsity:

the Lasso and Generalizations. Chapman and Hall, CRC Press.

Hoeffding, W. 1952. The large-sample power of tests based on permutations of obser-

vations. Ann. Math. Statist., 23, 169–192.

Hoeffding, W. 1965. Asymptotically optimal tests for multinomial distributions. Ann.

Math. Statist., 36(2), 369–408.

Hoerl, A. E., and Kennard, R. W. 1970. Ridge regression: Biased estimation for nonor-

thogonal problems. Technometrics, 12(1), 55–67.

Huber, P. J. 1964. Robust estimation of a location parameter. Ann. Math. Statist., 35,

73–101.

458

References

Jaeckel, L. A. 1972. Estimating regression coefﬁcients by minimizing the dispersion of

the residuals. Ann. Math. Statist., 43, 1449–1458.

James, W., and Stein, C. 1961. Estimation with quadratic loss. Pages 361–379 of:
Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability, vol. I.
University of California Press.

Jansen, L., Fithian, W., and Hastie, T. 2015. Effective degrees of freedom: a ﬂawed

metaphor. Biometrika, 102(2), 479–485.

Javanmard, A., and Montanari, A. 2014. Conﬁdence intervals and hypothesis testing

for high-dimensional regression. J. of Machine Learning Res., 15, 2869–2909.

Jaynes, E. 1968. Prior probabilities. IEEE Trans. Syst. Sci. Cybernet., 4(3), 227–241.
Jeffreys, H. 1961. Theory of Probability. Third ed. Clarendon Press.
Johnson, N. L., and Kotz, S. 1969. Distributions in Statistics: Discrete Distributions.

Houghton Mifﬂin Co.

Johnson, N. L., and Kotz, S. 1970a. Distributions in Statistics. Continuous Univariate

Distributions. 1. Houghton Mifﬂin Co.

Johnson, N. L., and Kotz, S. 1970b. Distributions in Statistics. Continuous Univariate

Distributions. 2. Houghton Mifﬂin Co.

Johnson, N. L., and Kotz, S. 1972. Distributions in Statistics: Continuous Multivariate

Distributions. John Wiley & Sons.

Kaplan, E. L., and Meier, P. 1958. Nonparametric estimation from incomplete obser-

vations. J. Amer. Statist. Assoc., 53(282), 457–481.

Kass, R. E., and Raftery, A. E. 1995. Bayes factors. J. Amer. Statist. Assoc., 90(430),

773–795.

Kass, R. E., and Wasserman, L. 1996. The selection of prior distributions by formal

rules. J. Amer. Statist. Assoc., 91(435), 1343–1370.

Kuffner, R., Zach, N., Norel, R., Hawe, J., Schoenfeld, D., Wang, L., Li, G., Fang,
L., Mackey, L., Hardiman, O., Cudkowicz, M., Sherman, A., Ertaylan, G., Grosse-
Wentrup, M., Hothorn, T., van Ligtenberg, J., Macke, J. H., Meyer, T., Scholkopf,
B., Tran, L., Vaughan, R., Stolovitzky, G., and Leitner, M. L. 2015. Crowdsourced
analysis of clinical trial data to predict amyotrophic lateral sclerosis progression. Nat
Biotech, 33(1), 51–57.

LeCun, Y., and Cortes, C. 2010.

MNIST Handwritten Digit Database.

http://yann.lecun.com/exdb/mnist/.

LeCun, Y., Bengio, Y., and Hinton, G. 2015. Deep learning. Nature, 521(7553), 436–

444.

Lee, J., Sun, D., Sun, Y., and Taylor, J. 2016. Exact post-selection inference, with

application to the Lasso. Annals of Statistics, 44(3), 907–927.

Lehmann, E. L. 1983. Theory of Point Estimation. Wiley Series in Probability and

Mathematical Statistics. John Wiley & Sons.

Leslie, C., Eskin, E., Cohen, A., Weston, J., and Noble, W. S. 2003. Mismatch string

kernels for discriminative pretein classiﬁcation. Bioinformatics, 1, 1–10.

Liaw, A., and Wiener, M. 2002. Classiﬁcation and regression by randomForest. R

News, 2(3), 18–22.

Liberman, M. 2015 (April). “Reproducible Research and the Common Task Method”.
Simons Foundation Frontiers of Data Science Lecture, April 1, 2015; video avail-
able.

References

459

Lockhart, R., Taylor, J., Tibshirani, R., and Tibshirani, R. 2014. A signiﬁcance test for
the lasso. Annals of Statistics, 42(2), 413–468. With discussion and a rejoinder by
the authors.

Lynden-Bell, D. 1971. A method for allowing for known observational selection in
small samples applied to 3CR quasars. Mon. Not. Roy. Astron. Soc., 155(1), 95–18.

Mallows, C. L. 1973. Some comments on Cp. Technometrics, 15(4), 661–675.
Mantel, N., and Haenszel, W. 1959. Statistical aspects of the analysis of data from

retrospective studies of disease. J. Natl. Cancer Inst., 22(4), 719–748.

Mardia, K. V., Kent, J. T., and Bibby, J. M. 1979. Multivariate Analysis. Academic

Press.

McCullagh, P., and Nelder, J. 1983. Generalized Linear Models. Monographs on Statis-

tics and Applied Probability. Chapman & Hall.

McCullagh, P., and Nelder, J. 1989. Generalized Linear Models. Second edn. Mono-

graphs on Statistics and Applied Probability. Chapman & Hall.

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.
1953. Equation of state calculations by fast computing machines. J. Chem. Phys.,
21(6), 1087–1092.

Miller, Jr, R. G. 1964. A trustworthy jackknife. Ann. Math. Statist, 35, 1594–1605.
Miller, Jr, R. G. 1981. Simultaneous Statistical Inference. Second edn. Springer Series

in Statistics. New York: Springer-Verlag.

Nesterov, Y. 2013. Gradient methods for minimizing composite functions. Mathemati-

cal Programming, 140(1), 125–161.

Neyman, J. 1937. Outline of a theory of statistical estimation based on the classical

theory of probability. Phil. Trans. Roy. Soc., 236(767), 333–380.

Neyman, J. 1977. Frequentist probability and frequentist statistics. Synthese, 36(1),

97–131.

Neyman, J., and Pearson, E. S. 1933. On the problem of the most efﬁcient tests of

statistical hypotheses. Phil. Trans. Roy. Soc. A, 231(694-706), 289–337.

Ng, A. 2015. Neural Networks. http://deeplearning.stanford.edu/

wiki/index.php/Neural_Networks. Lecture notes.

Ngiam, J., Chen, Z., Chia, D., Koh, P. W., Le, Q. V., and Ng, A. 2010. Tiled convo-
lutional neural networks. Pages 1279–1287 of: Lafferty, J., Williams, C., Shawe-
Taylor, J., Zemel, R., and Culotta, A. (eds), Advances in Neural Information Pro-
cessing Systems 23. Curran Associates, Inc.

O’Hagan, A. 1995. Fractional Bayes factors for model comparison. J. Roy. Statist. Soc.

Ser. B, 57(1), 99–138. With discussion and a reply by the author.

Park, T., and Casella, G. 2008. The Bayesian lasso. J. Amer. Statist. Assoc., 103(482),

681–686.

Pearson, K. 1900. On the criterion that a given system of deviations from the probable in
the case of a correlated system of variables is such that it can be reasonably supposed
to have arisen from random sampling. Phil. Mag., 50(302), 157–175.

Pritchard, J., Stephens, M., and Donnelly, P. 2000. Inference of Population Structure

using Multilocus Genotype Data. Genetics, 155(June), 945–959.

Quenouille, M. H. 1956. Notes on bias in estimation. Biometrika, 43, 353–360.
R Core Team. 2015. R: A Language and Environment for Statistical Computing. R

Foundation for Statistical Computing, Vienna, Austria.

460

References

Ridgeway, G. 2005. Generalized boosted models: A guide to the gbm package. Avail-

able online.

Ridgeway, G., and MacDonald, J. M. 2009. Doubly robust internal benchmarking and
false discovery rates for detecting racial bias in police stops. J. Amer. Statist. Assoc.,
104(486), 661–668.

Ripley, B. D. 1996. Pattern Recognition and Neural Networks. Cambridge University

Press.

Robbins, H. 1956. An empirical Bayes approach to statistics. Pages 157–163 of: Proc.
3rd Berkeley Symposium on Mathematical Statistics and Probability, vol. I. Univer-
sity of California Press.

Rosset, S., Zhu, J., and Hastie, T. 2004. Margin maximizing loss functions. In: Thrun,
S., Saul, L., and Sch¨olkopf, B. (eds), Advances in Neural Information Processing
Systems 16. MIT Press.

Rubin, D. B. 1981. The Bayesian bootstrap. Ann. Statist., 9(1), 130–134.
Savage, L. J. 1954. The Foundations of Statistics. John Wiley & Sons; Chapman &

Hill.

Schapire, R. 1990. The strength of weak learnability. Machine Learning, 5(2), 197–

227.

Schapire, R., and Freund, Y. 2012. Boosting: Foundations and Algorithms. MIT Press.
Scheff´e, H. 1953. A method for judging all contrasts in the analysis of variance.

Biometrika, 40(1-2), 87–110.

Sch¨olkopf, B., and Smola, A. 2001. Learning with Kernels: Support Vector Ma-
chines, Regularization, Optimization, and Beyond (Adaptive Computation and Ma-
chine Learning). MIT Press.

Schwarz, G. 1978. Estimating the dimension of a model. Ann. Statist., 6(2), 461–464.
Senn, S. 2008. A note concerning a selection “paradox” of Dawid’s. Amer. Statist.,

62(3), 206–210.

Soric, B. 1989. Statistical “discoveries” and effect-size estimation. J. Amer. Statist.

Assoc., 84(406), 608–610.

Spevack, M. 1968. A Complete and Systematic Concordance to the Works of Shake-

speare. Vol. 1–6. Georg Olms Verlag.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. 2014.
Dropout: a simple way to prevent neural networks from overﬁtting. J. of Machine
Learning Res., 15, 1929–1958.

Stefanski, L., and Carroll, R. J. 1990. Deconvoluting kernel density estimators. Statis-

tics, 21(2), 169–184.

Stein, C. 1956. Inadmissibility of the usual estimator for the mean of a multivariate nor-
mal distribution. Pages 197–206 of: Proc. 3rd Berkeley Symposium on Mathematical
Statististics and Probability, vol. I. University of California Press.

Stein, C. 1981. Estimation of the mean of a multivariate normal distribution. Ann.

Statist., 9(6), 1135–1151.

Stein, C. 1985. On the coverage probability of conﬁdence sets based on a prior distribu-
tion. Pages 485–514 of: Sequential Methods in Statistics. Banach Center Publication,
vol. 16. PWN, Warsaw.

Stigler, S. M. 2006. How Ronald Fisher became a mathematical statistician. Math. Sci.

Hum. Math. Soc. Sci., 176(176), 23–30.

References

461

Stone, M. 1974. Cross-validatory choice and assessment of statistical predictions. J.

Roy. Statist. Soc. B, 36, 111–147. With discussion and a reply by the author.

Storey, J. D., Taylor, J., and Siegmund, D. 2004. Strong control, conservative point
estimation and simultaneous conservative consistency of false discovery rates: A
uniﬁed approach. J. Roy. Statist. Soc. B, 66(1), 187–205.

Tanner, M. A., and Wong, W. H. 1987. The calculation of posterior distributions by
data augmentation. J. Amer. Statist. Assoc., 82(398), 528–550. With discussion and
a reply by the authors.

Taylor, J., Loftus, J., and Tibshirani, R. 2015. Tests in adaptive regression via the Kac-

Rice formula. Annals of Statistics, 44(2), 743–770.

Thisted, R., and Efron, B. 1987. Did Shakespeare write a newly-discovered poem?

Biometrika, 74(3), 445–455.

Tibshirani, R. 1989. Noninformative priors for one parameter of many. Biometrika,

76(3), 604–608.

Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. J. Roy. Statist.

Soc. B, 58(1), 267–288.

Tibshirani, R. 2006. A simple method for assessing sample sizes in microarray experi-

ments. BMC Bioinformatics, 7(Mar), 106.

Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., and Tibshirani, R.
2012. Strong rules for discarding predictors in lasso-type problems. J. Roy. Statist.
Soc. B, 74.

Tibshirani, R., Tibshirani, R., Taylor, J., Loftus, J., and Reid, S. 2016. selectiveInfer-

ence: Tools for Post-Selection Inference. R package version 1.1.3.

Tukey, J. W. 1958. “Bias and conﬁdence in not-quite large samples” in Abstracts of

Papers. Ann. Math. Statist., 29(2), 614.

Tukey, J. W. 1960. A survey of sampling from contaminated distributions. Pages
448–485 of: Contributions to Probability and Statistics: Essays in Honor of Harold
Hotelling (I. Olkin, et. al, ed.). Stanford University Press.

Tukey, J. W. 1962. The future of data analysis. Ann. Math. Statist., 33, 1–67.
Tukey, J. W. 1977. Exploratory Data Analysis. Behavioral Science Series. Addison-

Wesley.

van de Geer, S., B¨uhlmann, P., Ritov, Y., and Dezeure, R. 2014. On asymptotically op-
timal conﬁdence regions and tests for high-dimensional models. Annals of Statistics,
42(3), 1166–1202.

Vapnik, V. 1996. The Nature of Statistical Learning Theory. Springer.
Wager, S., Wang, S., and Liang, P. S. 2013. Dropout training as adaptive regularization.
Pages 351–359 of: Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Wein-
berger, K. (eds), Advances in Neural Information Processing Systems 26. Curran
Associates, Inc.

Wager, S., Hastie, T., and Efron, B. 2014. Conﬁdence intervals for random forests: the
jacknife and the inﬁntesimal jacknife. J. of Machine Learning Res., 15, 1625–1651.

Wahba, G. 1990. Spline Models for Observational Data. SIAM.
Wahba, G., Lin, Y., and Zhang, H. 2000. GACV for support vector machines. Pages
297–311 of: Smola, A., Bartlett, P., Sch¨olkopf, B., and Schuurmans, D. (eds), Ad-
vances in Large Margin Classiﬁers. MIT Press.

Wald, A. 1950. Statistical Decision Functions. John Wiley & Sons; Chapman & Hall.

462

References

Wedderburn, R. W. M. 1974. Quasi-likelihood functions, generalized linear models,

and the Gauss–Newton method. Biometrika, 61(3), 439–447.

Welch, B. L., and Peers, H. W. 1963. On formulae for conﬁdence points based on

integrals of weighted likelihoods. J. Roy. Statist. Soc. B, 25, 318–329.

Westfall, P., and Young, S. 1993. Resampling-based Multiple Testing: Examples and
Methods for p-Value Adjustment. Wiley Series in Probability and Statistics. Wiley-
Interscience.

Xie, M., and Singh, K. 2013. Conﬁdence distribution, the frequentist distribution esti-

mator of a parameter: A review. Int. Statist. Rev., 81(1), 3–39. with discussion.

Ye, J. 1998. On measuring and correcting the effects of data mining and model selec-

tion. J. Amer. Statist. Assoc., 93(441), 120–131.

Zhang, C.-H., and Zhang, S. 2014. Conﬁdence intervals for low-dimensional parame-

ters with high-dimensional data. J. Roy. Statist. Soc. B, 76(1), 217–242.

Zou, H., Hastie, T., and Tibshirani, R. 2007. On the “degrees of freedom” of the lasso.

Ann. Statist., 35(5), 2173–2192.

Author Index

Abu-Mostafa, Y. 372, 453
Achanta, R. 372, 453
Akaike, H. 231, 453
Anderson, T. W. 69, 453
Bastien, F. 374, 453
Becker, R. 128, 453
Bellhouse, D. R. 36, 453
Bengio, Y. 372, 374, 453, 458
Benjamini, Y. 294, 418, 450, 453
Berger, J. O. 36, 261, 453
Bergeron, A. 374, 453
Bergstra, J. 374, 453
Berk, R. 323, 419, 453
Berkson, J. 128, 454
Bernardo, J. M. 261, 454
Bibby, J. M. 37, 69, 459
Bien, J. 322, 461
Birch, M. W. 128, 454
Bishop, C. 371, 454
Bloomston, M. 89, 457
Boos, D. D. 180, 454
Boser, B. 390, 454
Bouchard, N. 374, 453
Breiman, L. 129, 348, 451, 454
Breuleux, O. 374, 453
Brown, L. 323, 419, 453
B¨uhlmann, P. 323, 461
Buja, A. 323, 419, 453
Carlin, B. P. 89, 261, 454
Carroll, R. J. 445, 460
Casella, G. 420, 459
Chambers, J. 128, 453
Chen, Z. 372, 459
Chia, D. 372, 459
Cho, C. S. 89, 457
Chopin, N. 261, 457
Cleveland, W. S. 11, 454
Cohen, A. 393, 458

Corbet, A. 456
Cortes, C. 372, 458
Courville, A. 372, 453
Cover, T. M. 52, 454
Cox, D. R. 52, 128, 152, 153, 262, 454
Crowley, J. 153, 454
Cudkowicz, M. 349, 458
de Finetti, B. 261, 454
Dembo, A. 52, 454
Dempster, A. P. 152, 454
Desjardins, G. 374, 453
Dezeure, R. 323, 461
Diaconis, P. 262, 454
DiCiccio, T. 204, 455
Donnelly, P. 261, 459
Donoho, D. L. 447, 455
Edwards, A. W. F. 37, 455
Efron, B. 11, 20, 37, 51, 52, 69, 89, 90,

105, 106, 130, 152, 154, 177–179,
204, 206, 207, 231, 232, 262, 263,
267, 294–297, 322, 323, 348, 417,
419, 420, 444, 445, 455–457, 461

Ertaylan, G. 349, 458
Eskin, E. 393, 458
Fang, L. 349, 458
Feldman, D. 417, 456
Fields, R. C. 89, 457
Finney, D. J. 262, 456
Fisher, R. A. 184, 204, 449, 456
Fithian, W. 323, 456, 458
Freund, Y. 348, 451, 456, 460
Friedman, J. 128, 129, 231, 321, 322,
348–350, 371, 454, 456, 457, 461

Geisser, S. 231, 457
Gerber, M. 261, 457
Gholami, S. 89, 457
Good, I. 88, 457

463

464

Author Index

Goodfellow, I. J. 374, 453
Gous, A. 262, 263, 456
Grosse-Wentrup, M. 349, 458
Guyon, I. 390, 454
Haenszel, W. 152, 459
Hall, P. 204, 457
Hampel, F. R. 179, 457
Hardiman, O. 349, 458
Harford, T. 232, 457
Hastie, T. 128, 231, 321–323, 348–350,

371, 372, 392, 393, 453, 456–458,
460–462

Hawe, J. 349, 458
Hinkley, D. V. 52, 69, 454, 456
Hinton, G. 372, 458, 460
Hochberg, Y. 294, 418, 450, 453
Hoeffding, W. 129, 296, 457
Hoerl, A. E. 105, 457
Hothorn, T. 349, 458
Huber, P. J. 179, 457
Jaeckel, L. A. 178, 458
James, W. 104, 458
Jansen, L. 323, 458
Janson, L. 89, 457
Javanmard, A. 323, 458
Jaynes, E. 261, 458
Jeffreys, H. 261, 458
Jin, L. X. 89, 457
Johnson, N. L. 36, 458
Johnstone, I. 231, 322, 323, 456
Kaplan, E. L. 152, 458
Kass, R. E. 261–263, 458
Kennard, R. W. 105, 457
Kent, J. T. 37, 69, 459
Koh, P. W. 372, 459
Kotz, S. 36, 458
Krizhevsky, A. 372, 460
Kuffner, R. 349, 458
Laird, N. M. 152, 454
Lamblin, P. 374, 453
Le, Q. V. 372, 459
LeCun, Y. 372, 458
Lee, J. 323, 458
Lehmann, E. L. 52, 458
Leitner, M. L. 349, 458
Leslie, C. 393, 458
Levine, E. A. 89, 457
Li, G. 349, 458
Liang, P. S. 372, 373, 461
Liaw, A. 348, 458

Liberman, M. 447, 458
Lin, Y. 391, 461
Loader, C. 393, 457
Lockhart, R. 323, 459
Loftus, J. 323, 461
Louis, T. A. 89, 261, 454
Lynden-Bell, D. 150, 459
MacDonald, J. M. 294, 460
Macke, J. H. 349, 458
Mackey, L. 349, 458
Maithel, S. K. 89, 457
Mallows, C. L. 231, 459
Mantel, N. 152, 459
Mardia, K. V. 37, 69, 459
McCullagh, P. 128, 322, 459
Meier, P. 152, 458
Metropolis, N. 261, 459
Meyer, T. 349, 458
Miller, R. G., Jr 177, 294, 418, 459
Montanari, A. 323, 458
Morris, C. 105, 456
Nelder, J. 128, 322, 459
Nesterov, Y. 372, 459
Neyman, J. 20, 204, 449, 459
Ng, A. 372, 459
Ngiam, J. 372, 459
Noble, W. S. 393, 458
Norel, R. 349, 458
Norton, J. A. 89, 457
O’Hagan, A. 261, 459
Olshen, R. A. 129, 348, 454
Park, T. 420, 459
Pascanu, R. 374, 453
Pawlik, T. M. 89, 457
Pearson, E. S. 449, 459
Pearson, K. 449, 459
Peers, H. W. 37, 207, 261, 462
Pericchi, L. R. 261, 453
Petrosian, V. 130, 456
Popescu, B. 348, 457
Poultsides, G. A. 89, 457
Pritchard, J. 261, 459
Quenouille, M. H. 177, 459
R Core Team 128, 459
Raftery, A. E. 262, 458
Reid, N. 262, 454
Reid, S. 323, 461
Ridgeway, G. 294, 348, 460
Ripley, B. D. 371, 460
Ritov, Y. 323, 461

Author Index

465

Robbins, H. 88, 104, 420, 460
Ronchetti, E. M. 179, 457
Rosenbluth, A. W. 261, 459
Rosenbluth, M. N. 261, 459
Rosset, S. 392, 460
Rousseeuw, P. J. 179, 457
Rubin, D. B. 152, 179, 454, 460
Salakhutdinov, R. 372, 460
Savage, L. J. 261, 460
Schapire, R. 348, 451, 456, 460
Scheff´e, H. 417, 460
Schmidt, C. 89, 457
Schoenfeld, D. 349, 458
Sch¨olkopf, B. 390, 460
Schwarz, G. 263, 460
Senn, S. 37, 460
Serﬂing, R. J. 180, 454
Sherman, A. 349, 458
Siegmund, D. 294, 461
Simon, N. 322, 461
Singh, K. 51, 207, 462
Smola, A. 390, 460
Soric, B. 294, 460
Spevack, M. 89, 460
Spolverato, G. 89, 457
Squires, I., Malcolm 89, 457
Srivastava, N. 372, 460
Stahel, W. A. 179, 457
Stefanski, L. 445, 460
Stein, C. 104, 106, 178, 232, 261, 456,

458, 460

Stephens, M. 261, 459
Stigler, S. M. 447, 460
Stolovitzky, G. 349, 458
Stone, C. J. 129, 348, 454
Stone, M. 231, 461
Storey, J. D. 294, 461
Sun, D. 323, 456, 458
Sun, Y. 323, 458
Sutskever, I. 372, 460
Tanner, M. A. 263, 461
Taylor, J. 294, 322, 323, 456, 458, 459,

461

Teller, A. H. 261, 459
Teller, E. 261, 459
Thisted, R. 89, 456, 461
Thomas, J. A. 52, 454

Tibshirani, R. 128, 179, 207, 231, 232,

261, 321–323, 348–350, 371, 392,
420, 450, 456, 457, 459, 461, 462

Toulmin, G. 88, 457
Tran, L. 349, 458
Tran, T. B. 89, 457
Tukey, J. W. 11, 177, 179, 450, 461
Turian, J. 374, 453
van de Geer, S. 323, 461
van Ligtenberg, J. 349, 458
Vapnik, V. 390, 454, 461
Vaughan, R. 349, 458
Vincent, P. 372, 453
Votanopoulos, K. I. 89, 457
Wager, S. 348, 372, 373, 461
Wahba, G. 391, 392, 461
Wainwright, M. 321–323, 457
Wald, A. 450, 461
Wang, L. 349, 458
Wang, S. 372, 373, 461
Warde-Farley, D. 374, 453
Wasserman, L. 261, 263, 458
Weber, S. M. 89, 457
Wedderburn, R. W. M. 128, 462
Welch, B. L. 37, 207, 261, 462
Westfall, P. 294, 418, 462
Weston, J. 393, 458
Wiener, M. 348, 458
Wilks, A. 128, 453
Williams, C. 456
Wong, W. H. 263, 461
Worhunsky, D. J. 89, 457
Xie, M. 51, 207, 462
Ye, J. 231, 462
Yekutieli, D. 418, 453
Ylvisaker, D. 262, 454
Young, S. 294, 418, 462
Zach, N. 349, 458
Zhang, C.-H. 323, 462
Zhang, H. 391, 461
Zhang, K. 323, 419, 453
Zhang, S. 323, 462
Zhao, L. 323, 419, 453
Zhu, J. 392, 460
Zou, H. 231, 322, 462

Subject Index

abc method, 194, 204
Accelerated gradient descent, 359
Acceleration, 192, 206
Accuracy, 14

after model selection, 402–408

Accurate but not correct, 402
Activation function, 355, 361
leaky rectiﬁed linear, 362
rectiﬁed linear, 362
ReLU, 362
tanh, 362

Active set, 302, 308
adaboost algorithm, 341–345, 447
Adaboost.M1, 342
Adaptation, 404
Adaptive estimator, 404
Adaptive rate control, 359
Additive model, 324

adaptive, 346

Adjusted compliance, 404
Admixture modeling, 256–260
AIC, see Akaike information criterion
Akaike information criterion, 208, 218,

226, 231, 246, 267

Allele frequency, 257
American Statistical Association, 449
Ancillary, 44, 46, 139
Apparent error, 211, 213, 219
arcsin transformation, 95
Arthur Eddington, 447
Asymptotics, xvi, 119, 120
Autoencoder, 362–364
Backﬁtting, 346
Backpropagation, 356–358
Bagged estimate, 404, 406
Bagging, 226, 327, 406, 408, 419
Balance equations, 256
Barycentric plot, 259

Basis expansion, 375
Bayes

deconvolution, 421–424
factor, 244, 285
false-discovery rate, 279
posterior distribution, 254
posterior probability, 280
shrinkage, 212
t-statistic, 255
theorem, 22

inference, 22–37
information criterion, 246
lasso, 420
lasso prior, 415
model selection, 244
trees, 349

Bayes–frequentist estimation, 412–417
Bayesian

Bayesian information criterion, 267
Bayesianism, 3
BCa

accuracy and correctness, 205
conﬁdence density, 202, 207, 237, 242,

243

interval, 202
method, 192

Benjamini and Hochberg, 276
Benjamini–Yekutieli, 400
Bernoulli, 338
Best-approximating linear subspace, 363
Best-subset selection, 299
Beta

distribution, 54, 239

BHq, 276
Bias, 14, 352
Bias-corrected, 330

and accelerated, see BCa method
conﬁdence intervals, 190–191
percentile method, 190

467

468

Subject Index

Bias-correction value, 191
Biased estimation, 321
BIC, see Bayesian information criterion
Big-data era, xv, 446
Binomial, 109, 117

Coherent behavior, 261
Common task framework, 447
Compliance, 394
Computational bottleneck, 128
Computer age, xv
Computer-intensive, 127

Conditional inference, 45–48, 139, 142

inference, 189, 267
statistics, 159
Conditional, 58
Conditional distribution

full, 253

lasso, 318

Conditionality, 44
Conﬁdence

density, 200, 201, 235
distribution, 198–203
interval, 17
region, 397

Conjugate, 253, 259

prior, 238
priors, 237

377

ﬁlters, 368
layer, 367

Convolution, 422, 445

distribution, 54, 117, 239
log-likelihood, 380
standard deviation, 111

Bioassay, 109
Biometrika, 449
Bivariate normal, 182
Bonferroni bound, 273
Boole’s inequality, 274
Boosting, 320, 324, 333–350
Bootstrap, 7, 155–180, 266, 327

Baron Munchausen, 177
Bayesian, 168, 179
cdf, 187
conﬁdence intervals, 181–207
ideal estimate, 160, 179
jackknife after, 179
moving blocks, 168
multisample, 167
nonparametric, 159–162, 217
out of bootstrap, 232
packages, 178
parametric, 169–173, 223, 312, 429
probabilities, 164
replication, 159
sample, 159
sample size, 179, 205
smoothing, 226, 404, 406
t, 196
t intervals, 195–198

Bound form, 305
Bounding hyperplane, 398
Burn-in, 260
BYq algorithm, 400
Causal inference, xvi
Censored

data, 134–139
not truncated, 150

Centering, 107
Central limit theorem, 119
Chain rule for differentiation, 356
Classic statistical inference, 3–73
Classiﬁcation, 124, 209
Classiﬁcation accuracy, 375
Classiﬁcation error, 209
Classiﬁcation tree, 348
Cochran–Mantel–Haenszel test, 131

Convex optimization, 304, 308, 321, 323,

Corrected differences, 411
Correlation effects, 295
Covariance

formula, 312
penalty, 218–226

Coverage, 181
Coverage level, 274
Coverage matching prior, 236–237
Cox model, see proportional hazards

model

395, 403

Cp, 217, 218, 221, 231, 267, 300, 394,

Cram´er–Rao lower bound, 44
Credible interval, 198, 417
Cross-validation, 208–232, 267, 335

10-fold, 326
estimate, 214
K-fold, 300
leave one out, 214, 231

Cumulant generating function, 67
Curse of dimensionality, 387
Dark energy, 210, 231
Data analysis, 450
Data science, xvii, 450, 451

Subject Index

469

Data sets

binomial, 54, 117, 239
gamma, 54, 117, 239
Gaussian, 54
normal, 54, 117, 239
Poisson, 54, 117, 239

Divide-and-conquer algorithm, 325
Document retrieval, 298
Dose–response, 109
Dropout learning, 368, 372
DTI, see diffusion tensor imaging
Early computer-age, xvi, 75–268
Early stopping, 362
Effect size, 272, 288, 399, 408
Efﬁciency, 44, 120
Eigenratio, 162, 173, 194
Elastic net, 316, 356
Ellipsoid, 398
EM algorithm, 146–150

missing data, 266

Empirical Bayes, 75–90, 93, 264
estimation strategies, 421–445
information, 443
large-scale testing, 278–282

Empirical null, 286

estimation, 289–290
maximum-likelihood estimation, 296
Empirical probability distribution, 160
Ensemble, 324, 334
Ephemeral predictors, 227
Epoch, 359
Equilibrium distribution, 256
Equivariant, 106
Exact inferences, 119
Expectation parameter, 118
Experimental design, xvi
Exponential family, 53–72, 225

p-parameter, 117, 413, 424
curved, 69
one-parameter, 116

F distribution, 397
F tests, 394
f -modeling, 424, 434, 440–444
Fake-data principle, 148, 154, 266
False coverage
control, 399

False discovery, 275

control, 399
control theorem, 294
proportion, 275
rate, 271–297
False-discovery

ALS, 334
AML, see leukemia
baseball, 94
butterfly, 78
cell infusion, 112
cholesterol, 395, 402, 403
CIFAR-100, 365
diabetes, 98, 209, 396, 414, 416
dose-response, 109
galaxy, 120
handwritten digits

(MNIST), 353

head/neck cancer, 135
human ancestry, 257
insurance, 131
kidney function, 157, 222
leukemia, 176, 196, 377
NCOG, 134
nodes, 424, 427, 430, 438, 439, 442
pediatric cancer, 143
police, 287
prostate, 249, 272, 289, 408, 410,

423, 434–436

protein classification, 385
shakespear, 81
spam, 113, 127, 209, 215, 300–302,

student score, 173, 181, 186,

supernova, 210, 212, 217, 221, 224
vasoconstriction, 240, 241,

325

202, 203

246, 252

Data snooping, 398
De Finetti, B., 35, 36, 251, 450
De Finetti–Savage school, 251
Debias, 318
Decision rule, 275
Decision theory, xvi
Deconvolution, 422
Deep learning, 351–374
Deﬁnitional bias, 431
Degrees of freedom, 221, 231, 312–313
Delta method, 15, 414, 420
Deviance, 112, 118, 119, 301
Deviance residual, 123
Diffusion tensor imaging, 291
Direct evidence, 105, 109, 421
Directional derivatives, 158
Distribution

beta, 54, 239

470

Subject Index

rate, 9

Family of probability densities, 64
Family-wise error rate, 274
FDR, see false-discovery rate
Feed-forward, 351
Fiducial, 267

constructions, 199
density, 200
inference, 51

Fisher, 79
Fisher information, 29, 41, 59

bound, 41
matrix, 236, 427

Fisherian correctness, 205
Fisherian inference, 38–52, 235
Fixed-knot regression splines, 345
Flat prior, 235
Forward pass, 357
Forward-stagewise, 346

ﬁtting, 320

Forward-stepwise, 298–303

computations, 322
logistic regression, 322
regression, 300

Fully connected layer, 368
Functional gradient descent, 340
FWER, see family-wise error rate
g-modeling, 423
Gamma, 117

distribution, 54, 117, 239

General estimating equations, xvi
General information criterion, 248
Generalized

linear mixed model, 437–440
linear model, 108–123, 266
ridge problem, 384

Genome, 257
Genome-wide association studies, 451
Gibbs sampling, 251–260, 267, 414
GLM, see generalized linear model
GLMM, see generalized linear mixed

model

Frailties, 439
Frequentism, 3, 12–22, 30, 35, 51, 146,

Fourier

method, 440
transform, 440

267

Frequentist, 413

inference, 12–21
strongly, 218

Google ﬂu trends, 230, 232
Gradient boosting, 338–341
Gradient descent, 354, 356
Gram matrix, 381
Gram-Schmidt orthogonalization, 322
Graphical lasso, 321
Graphical models, xvi
Greenwood’s formula, 137, 151
Group lasso, 321
Hadamard product, 358
Handwritten digits, 353
Haplotype estimation, 261
Hazard rate, 131–134

parametric estimate, 138
Hidden layer, 351, 352, 354
High-order interaction, 325
Hinge loss, 380
Hints

learning with, 369

Hoeffding’s lemma, 118
Holm’s procedure, 274, 294
Homotopy path, 306
Hypergeometric distribution, 141, 152
Imputation, 149
Inadmissible, 93
Indirect evidence, 102, 109, 266, 290,

421, 440, 443

Inductive inference, 120
Inference, 3
Inference after model selection, 394–420
Inferential triangle, 446
Inﬁnitesimal forward stagewise, 320
Inﬁnitesimal jackknife, 167

estimate, 406
standard deviations, 407
Inﬂuence function, 174–177

empirical, 175

Inﬂuenza outbreaks, 230
Input distortion, 369, 373
Input layer, 355
Insample error, 219
Inverse chi-squared, 262
Inverse gamma, 239, 262
IRLS, see iteratively reweighted least

Iteratively reweighted least squares, 301,

squares

322

Jackknife, 155–180, 266, 330

estimate of standard error, 156
standard error, 178

Subject Index

471

estimation, 91–107, 282, 305, 410
ridge regression, 265

James–Stein

Jeffreys

prior, 237

Jeffreys’

prior, 28–30, 36, 198, 203, 236
prior, multiparameter, 242
scale, 285

Jumpiness of estimator, 405
Kaplan–Meier, 131, 134, 136, 137

estimate, 134–139, 266

Karush–Kuhn–Tucker optimality

conditions, 308

Kernel

function, 382
logistic regression, 386
method, 375–393
SVM, 386
trick, 375, 381–383, 392

Kernel smoothing, 375, 387–390
Knots, 309
Kullback–Leibler distance, 112
`1 regularization, 321
Lagrange

dual, 381
form, 305, 308
multiplier, 391

Large-scale

hypothesis testing, 271–297
testing, 272–275

Large-scale prediction algorithms, 446
Lasso, 101, 210, 217, 222, 231, 298–323

modiﬁcation, 312
path, 312
penalty, 356

Learning from the experience of others,

104, 280, 290, 421, 443

Learning rate, 358
Least squares, 98, 112, 299
Least-angle regression, 309–313, 321
Least-favorable family, 262
Left-truncated, 150
Lehmann alternative, 294
Life table, 131–134
Likelihood function, 38

concavity, 118

Limited-translation rule, 293
Lindsey’s method, 68, 171
Linearly separable, 375
Link function, 237, 340

Local false-discovery rate, 280, 282–286
Local regression, 387–390, 393
Local translation invariance, 368
Log polynomial regression, 410
Log-rank statistic, 152
Log-rank test, 131, 139–142, 152, 266
Logic of inductive inference, 185, 205
Logistic regression, 109–115, 139, 214,

299, 375

multiclass, 355

Logit, 109
Loss plus penalty, 385
Machine learning, 208, 267, 375
Mallows’ Cp, see Cp
Mantel–Haenzel test, 131
MAP, 101
MAP estimate, 420
Margin, 376
Marginal density, 409, 422
Markov chain Monte Carlo, see MCMC
Markov chain theory, 256
Martingale theory, 294
Matching prior, 198, 200
Matlab, 271
Matrix completion, 321
Max pool layer, 366
Maximized a-posteriori probability, see

MAP

Maximum likelihood, 299
Maximum likelihood estimation, 38–52
MCMC, 234, 251–260, 267, 414
McNemar test, 341
Mean absolute deviation, 447
Median unbiased, 190
Memory-based methods, 390
Meter reader, 30
Meter-reader, 37
Microarrays, 227, 271
Minitab, 271
Misclassiﬁcation error, 302
Missing data, 146–150, 325

EM algorithm, 266

Missing-species problem, 78–84
Mixed features, 325
Mixture density, 279
Model averaging, 408
Model selection, 243–250, 398

criteria, 250

Monotone lasso, 320
Monotonic increasing function, 184
Multinomial

472

Subject Index

distribution, 61–64, 425
from Poisson, 63
Multiple testing, 272
Multivariate

analysis, 119
normal, 55–59

n-gram, 385
N-P complete, 299
Nadaraya–Watson estimator, 388
Natural parameter, 116
Natural spline model, 430
NCOG, see Northern California

Oncology Group

Nested models, 299
Neural Information Processing Systems,

Neural network, 351–374

adaptive tuning, 360
number of hidden layers, 361

Neurons, 351
Neyman’s construction, 181, 183, 193,

372

204

predictor, 221

One-sample nonparametric bootstrap,

161

One-sample problems, 156
OOB, see out-of-bag error
Optical character recognition, 353
Optimal separating hyperplane, 375–377
Optimal-margin classiﬁer, 376
Optimality, 18
Oracle, 275
Orthogonal parameters, 262
Out-of-bag error, 232, 327, 329–330
Out-the-box learning algorithm, 324
Output layer, 352
Outsample error, 219
Over parametrized, 298
Overﬁtting, 304
Overshrinks, 97
p-value, 9, 282
Package/program
gbm, 335, 348
glmnet, 214, 315, 322, 348
h2o, 372
lars, 312, 320
liblineaR, 381
locfdr, 289–291, 296, 437
lowess, 6, 222, 388
nlm, 428
randomForest, 327, 348
selectiveInference, 323

Pairwise inner products, 381
Parameter space, 22, 29, 54, 62, 66
Parametric bootstrap, 242
Parametric family, 169
Parametric models, 53–72
Partial likelihood, 142, 145, 151, 153,

266, 341

Partial logistic regression, 152
Partial residual, 346
Path-wise coordinate descent, 314
Penalized

least squares, 101
likelihood, 101, 428
logistic regression, 356
maximum likelihood, 226, 307

Percentile method, 185–190

central interval, 187

Permutation null, 289, 296
Permutation test, 49–51
Phylogenetic tree, 261
Piecewise

Neyman–Pearson, 18, 19, 293
Non-null, 272
Noncentral chi-square variable, 207
Nonlinear transformations, 375
Nonlinearity, 361
Nonparameteric

regression, 375

Nonparametric, 53, 127

MLE, 150, 160
percentile interval, 187

Normal

correlation coefﬁcient, 182
distribution, 54, 117, 239
multivariate, 55–59
regression model, 414
theory, 119

134

Nuclear norm, 321
Nuisance parameters, 142, 199
Objective Bayes, 36, 267

inference, 233–263
intervals, 198–203
prior distribution, 234–237

Northern California Oncology Group,

OCR, see optical character recognition
Offset, 349
OLS

algorithm, 403
estimation, 395

Subject Index

473

linear, 313
nonlinear, 314

Pivotal

argument, 183
quantity, 196, 198
statistic, 16
.632 rule, 232
Poisson, 117, 193

distribution, 54, 117, 239
regression, 120–123, 249, 284, 295,

435

Poisson regression, 171
Polynomial kernel, 382, 392
Positive-deﬁnite function, 382
Post-selection inference, 317, 394–420
Posterior density, 235, 238
Posterior distribution, 416
Postwar era, 264
Prediction

errors, 216
rule, 208–213

Predictors, 124, 208
Principal components, 362
Prior distribution, 234–243

beta, 239
conjugate, 237–243
coverage matching, 236–237
gamma, 239
normal, 239
objective Bayes, 234
proper, 239

Probit analysis, 112, 120, 128
Propagation of errors, 420
Proper prior, 239
Proportional hazards model, 131,

142–146, 266

Proximal-Newton, 315
q-value, 280
QQ plot, 287
QR decomposition, 311, 322
Quadratic program, 377
Quasilikelihood, 266
Quetelet, Adolphe, 449
R, 178, 271
Random forest, 209, 229, 324–332,

adaptive nearest-neighbor estimator,

leave-one-out cross-validated error,

347–350

328

329

Monte Carlo variance, 330

interval, 396, 397, 417
theorem, 398

Score function, 42
Score tests, 301
Second-order accuracy, 192–195

sampling variance, 330
standard error, 330–331

Randomization, 49–51
Rao–Blackwell, 227, 231
Rate annealing, 360
Rectiﬁed linear, 359
Regression, 109
Regression rule, 219
Regression to the mean, 33
Regression tree, 124–128, 266, 348
Regularization, 101, 173, 298, 379, 428

path, 306

Relevance, 290–293
Relevance function, 293
Relevance theory, 297
Reproducing kernel Hilbert space, 375,

384, 392

Resampling, 162
plans, 162–169
simplex, 164, 169
vector, 163

Residual deviance, 283
Response, 124, 208
Ridge regression, 97–102, 209, 304, 327,

332, 372, 381
James–Stein, 265

Ridge regularization, 368
logistic regression, 392

Right-censored, 150
Risk set, 144
RKHS, see reproducing-kernel Hilbert

space

Robbins’ formula, 75, 77, 422, 440
Robust estimation, 174–177
Royal Statistical Society, 449
S language, 271
Sample correlation coefﬁcient, 182
Sample size coherency, 248
Sampling distribution, 312
SAS, 271
Savage, L. J., 35, 36, 51, 199, 233, 251,

450

Scale of evidence

Fisher, 245
Jeffreys, 245

Scheff´e

474

Subject Index

Selection bias, 33, 408–411
Self-consistent, 149
Separating hyperplane, 375

geometry, 390

Seven-league boots, 448
Shrinkage, 115, 316, 338

estimator, 59, 91, 94, 96, 410

Sigmoid function, 352
Signiﬁcance level, 274
Simulation, 155–207
Simultaneous conﬁdence intervals,

395–399

Simultaneous inference, 294, 418
Sinc kernel, 440, 445
Single-nucleotide polymorphism, see

SNP

Smoothing operator, 346
SNP, 257
Soft margin classiﬁer, 378–379
Soft-threshold, 315
Softmax, 355
Spam ﬁlter, 115
Sparse

models, 298–323
principal components, 321

Sparse matrix, 316
Sparsity, 321
Split-variable randomization, 327, 332
SPSS, 271
Squared error, 209
Standard candles, 210, 231
Standard error, 155

external, 408
internal, 408

Standard interval, 181
Stein’s

paradox, 105
unbiased risk estimate, 218, 231

Stepwise selection, 299
Stochastic gradient descent, 358
Stopping rule, 32, 413
Stopping rules, 243
String kernel, 385, 386
Strong rules, 316, 322
Structure, 261
Structure matrix, 97, 424
Student t

conﬁdence interval, 396
distribution, 196, 272
statistic, 449

two-sample, 8, 272

Studentized range, 418
Subgradient

condition, 308
equation, 312, 315

Subjective prior distribution, 233
Subjective probability, 233
Subjectivism, 35, 233, 243, 261
Sufﬁciency, 44
Sufﬁcient

statistic, 66, 112, 116
vector, 66

Supervised learning, 352
Support

set, 377, 378
vector, 377
vector classiﬁers, 381
vector machine, 319, 375–393

SURE, see Stein’s unbiased risk estimate
Survival analysis, 131–154, 266
Survival curve, 137, 279
SVM

Lagrange dual, 391
Lagrange primal, 391
loss function, 391

Taylor series, 157, 420
Theoretical null, 286
Tied weights, 368
Time series, xvi
Training set, 208
Transformation invariance, 183–185, 236
Transient episodes, 228
Trees

averaging, 348
best-ﬁrst, 333
depth, 335
terminal node, 126

Tricube kernel, 388, 389
Trimmed mean, 175
Triple-point, xv
True error rate, 210
True-discovery rates, 286
Tukey, J. W., 418, 450
Tukey, J. W., 418
Tweedie’s formula, 409, 419, 440
Twenty-ﬁrst-century methods, xvi,

271–446

Two-groups model, 278
Uncorrected differences, 411
Uninformative prior, 28, 169, 233, 261
Universal approximator, 351
Unlabeled images, 365

Subject Index

475

Unobserved covariates, 288
Validation set, 213
Vapnik, V., 390
Variable-importance plot, 331–332, 336
Variance, 14
Variance reduction, 324
Velocity vector, 360
Voting, 333
Warm starts, 314, 363
Weak learner, 333, 342
Weight

decay, 356
regularization, 361, 362
sharing, 352, 367

Weighted exponential loss, 345
Weighted least squares, 315
Weighted majority vote, 341
Weights, 352
Wide data, 298, 321
Wilks’ likelihood ratio statistic, 246
Winner’s curse, 33, 408
Winsorized mean, 175
Working response, 315, 322
z.˛/, 188
Zero set, 296

