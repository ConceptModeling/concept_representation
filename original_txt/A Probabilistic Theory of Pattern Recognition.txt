Stochastic Mechanics  Applications of 

Random Media  Mathematics 

Signal Processing  Stochastic Modelling 

and Image Synthesis  and Applied Probability 

Mathematical Economics 
Stochastic Optimization 
Stochastic Control 

31 

Edited by 

I. Karatzas 
M. Yor 

Advisory Board 

P.  Bremaud 
E. Carlen 
R. Dobrushin 
W.  Fleming 
D. Geman 
G.  Grimmett 
G.  Papanicolaou 
J. Scheinkman 

Springer 
New York 
Berlin 
Heidelberg 
Barcelona 
Budapest 
Hong Kong 
London 
Milan 
Paris 
Santa Clara 
Singapore 
Tokyo 

Applications  of Mathematics 

1  Fleming/Rishel, Deterministic and Stochastic Optimal Control (1975) 
2  Marchuk, Methods of Numerical Mathematics, Second Ed.  (1982) 
3  Balakrishnan, Applied Functional Analysis, Second Ed.  (1981) 
4  Borovkov,  Stochastic Processes in Queueing Theory (1976) 
5  LiptserlShiryayev,  Statistics of Random Processes I: General Theory (1977) 
6  LiptserlShiryayev,  Statistics of Random Processes II:  Applications (1978) 
7  Vorob'ev,  Game Theory: Lectures for  Economists and Systems Scientists 

(1977) 

8  Shiryayev, Optimal Stopping Rules (1978) 
9  Ibragimov/Rozanov,  Gaussian Random Processes (1978) 
10  Wonham,  Linear Multivariable Control:  A Geometric Approach, Third Ed. 

(1985) 

11  Hida,  Brownian Motion (1980) 
12  Hestenes,  Conjugate Direction Methods in Optimization (1980) 
13  Kallianpur,  Stochastic Filtering Theory (1980) 
14  Krylov,  Controlled Diffusion Processes (1980) 
15  Prabhu,  Stochastic Storage Processes:  Queues, Insurance Risk, and Dams 

(1980) 

16  Ibragimov/Has'minskii, Statistical Estimation:  Asymptotic Theory (1981) 
17  Cesari,  Optimization: Theory and Applications (1982) 
18  Elliott, Stochastic Calculus and Applications (1982) 
19  Marchuk/Shaidourov,  Difference Methods and Their Extrapolations (1983) 
20  Hijab,  Stabilization of Control Systems (1986) 
21  Protter, Stochastic Integration and Differential Equations (1990) 
22  Benveniste/Metivier/Priouret, Adaptive Algorithms and Stochastic 

Approximations (1990) 

23  KloedeniPlaten,  Numerical Solution of Stochastic Differential Equations 

(1992) 

24  Kushner/Dupuis,  Numerical Methods for  Stochastic Control Problems 

in Continuous Time (1992) 

25  Fleming/Soner,  Controlled Markov Processes and Viscosity Solutions 

(1993) 

26  BaccellilBremaud, Elements of Queueing Theory (1994) 
27  Winkler,  Image Analysis, Random Fields, and Dynamic Monte Carlo 

Methods:  An Introduction to Mathematical Aspects  (1994) 

28  Kalpazidou,  Cycle Representations of Markov Processes (1995) 
29  Elliott! AggouniMoore,  Hidden Markov Models:  Estimation and Control 

(1995) 

30  Hernandez-LermaiLasserre,  Discrete-Time Markov Control Processes: 

Basic Optimality Criteria (1996) 

31  Devroye/Gyorfl/Lugosi, A Probabilistic Theory of Pattern Recognition (1996) 
32  MaitraiSudderth,  Discrete Gambling and Stochastic Games (1996) 

Luc Devroye  Laszlo Gyorfi 
Gabor Lugosi 

A Probabilistic Theory 
of Pattern Recognition 

With 99 Figures 

Springer 

Lasz16 Gyorfi 
Gabor Lugosi 
Department of Mathematics and 

Computer Science 

Technical University of Budapest 
Budapest 
Hungary 

Luc Devroye 
School of Computer Science 
McGill University 
Montreal, Quebec, H3A 2A 7 
Canada 

Managing Editors 

I. Karatzas 
Department of Statistics 
Columbia University 
New York, NY 10027, USA 

M. Yor 
CNRS, Laboratoire de Probabilites 
Universite Pierre et Marie Curie 
4, Place Jussieu, Tour 56 
F-75252 Paris Cedex OS,  France 

Mathematics Subject Classification (1991): 68T1O,  68T05, 62G07, 62H30 

Library of Congress Cataloging-in-Publication Data 
Devroye, Luc. 

A probabilistic theory of pattern recognition/Luc Devroye, 

Laszlo Gyorfi, Gabor Lugosi. 

p. 

cm. 

Includes bibliographical references and index. 
ISBN 0-387-94618-7 (hardcover) 
1.  Pattern perception.  2.  Probabilities.  1.  Gyorfi, Laszlo. 

II. Lugosi, Gabor. 
Q327.D5  1996 
003' .52'015192-dc20 

III.  Title. 

95-44633 

Printed on acid-free paper. 

©  1996 Springer-Verlag New York, Inc. 
All rights reserved. This work may not be translated or copied in whole or in part without the 
written permission of the publisher (Springer-Verlag New York, Inc.,  175 Fifth Avenue, New 
York,  NY  10010,  USA),  except  for  brief excerpts  in  connection  with  reviews  or  scholarly 
analysis.  Use  in  connection  with  any  form  of information  storage  and  retrieval,  electronic 
adaptation, computer software, or by similar or dissimilar methodology now known or here(cid:173)
after developed is forbidden. 
The use of general descriptive names, trade names, trademarks, etc., in this publication, even 
if the  former  are not  especially  identified,  is  not  to be  taken as  a  sign  that such  names,  as 
understood by the Trade Marks and Merchandise Marks Act, may accordingly be used freely 
by anyone. 

Production managed by Francine McNeill; manufacturing supervised by Jeffrey Taub. 
Photocomposed copy prepared using Springer's svsing.sty macro. 
Printed and bound by Braun-Brumfield, Inc., Ann Arbor, MI. 
Printed in the United States of America. 

9 8 7 6 5 4 3 2 

(Corrected second printing. 1997) 

ISBN 0-387-94618-7  Springer-Verlag  New York  Berlin  Heidelberg  SPIN 10565785 

Preface 

Life is  just a  long  random walk.  Things  are  created because the  circumstances 
happen to  be right.  More  often than  not,  creations,  such  as  this  book,  are  acci(cid:173)
dental. Nonparametric estimation came to life in the fifties and sixties and started 
developing at a frenzied pace in the late sixties, engulfing pattern recognition in 
its growth. In the mid-sixties, two young men, Tom Cover and Peter Hart, showed 
the world that the nearest neighbor rule in all its simplicity was guaranteed to err at 
most twice as often as the best possible discrimination method. Tom's results had 
a profound influence on Terry Wagner, who became a Professor at the University 
of Texas  at Austin and brought probabilistic rigor to the young field  of nonpara(cid:173)
metric  estimation.  Around  1971,  Vapnik  and Chervonenkis  started publishing a 
revolutionary  series  of papers with deep implications in pattern recognition, but 
their work was not well known at the time. However, Tom and Terry had noticed 
the potential of the work, and Terry asked Luc Devroye to read that work in prepa(cid:173)
ration for his Ph.D. dissertation at the University of Texas. The year was 1974. Luc 
ended up in Texas quite by accident thanks to a tip by his friend and fellow Belgian 
Willy Wouters,  who matched him up  with Terry.  By the time Luc's dissertation 
was published in 1976, pattern recognition had taken off in earnest. On the theoret(cid:173)
ical side, important properties were still being discovered. In 1977, Stone stunned 
the nonparametric community by showing that there are Iionparametric rules that 
are convergent for  all distributions  of the data.  This is called distribution-free or 
universal consistency, and it is what makes nonparametric methods so attractive. 
Yet, very few researchers were concerned with universal consistency-one notable 
exception was Laci Gyorfi, who at that time worked in Budapest amid an energetic 
group of nonparametric specialists that included Sandor Csibi, J6zsef Fritz,  and 
Pal Revesz. 

VI 

Preface 

So, linked by a common vision, Luc and Laci decided to join forces in the early 
eighties.  In  1982, they wrote six chapters of a book on nonparametric regression 
function  estimation, but these were never published. In fact,  the notes are still in 
drawers  in their offices  today.  They felt  that the  subject had not matured yet.  A 
book on nonparametric density estimation saw the light in  1985. Unfortunately, 
as  true  baby-boomers,  neither Luc  nor Laci  had  the  time  after  1985  to  write  a 
text on nonpararnetric pattern recognition. Enter Gabor Lugosi, who obtained his 
doctoral  degree  under Laci's  supervision in  1991.  Gabor had  prepared  a set of 
rough  course  notes  on  the  subject around  1992  and  proposed to  coordinate the 
project-this book-in 1993. With renewed energy,  we  set out to  write the book 
that we should have written at least ten years ago. Discussions and work sessions 
were  held  in  Budapest,  Montreal,  Leuven,  and  Louvain-La-Neuve.  In  Leuven, 
our gracious  hosts  were  Ed  van  der Meulen  and  Jan Beirlant,  and  in Louvain(cid:173)
La-Neuve, we  were gastronomically and spiritually supported by Leopold Simar 
and  Irene Gijbels.  We  thank  all  of them.  New  results  accumulated,  and we had 
to  resist  the  temptation  to  publish  these  in journals.  Finally,  in  May  1995,  the 
manuscript had bloated to  such extent that it had to be sent to the publisher, for 
otherwise it would have  become  an  encyclopedia.  Some important unanswered 
questions were quickly turned into masochistic exercises or wild conjectures. We 
will explain subject selection,  classroom use,  chapter dependence,  and personal 
viewpoints  in  the  Introduction.  We  do  apologize,  of course,  for  all  remaining 
errors. 

We  were touched, influenced, guided,  and taught by many people. Terry Wag(cid:173)

ner's  rigor  and  taste  for  beautiful  nonparametric  problems  have  infected  us  for 
life.  We  thank  our  past  and  present  coauthors  on  nonpararnetric  papers,  Alain 
Berlinet,  Michel  Broniatowski,  Ricardo  Cao,  Paul  Deheuvels,  Andras  Farago, 
Adam Krzyzak,  Tamas Linder, Andrew Nobel, Mirek Pawlak, Igor Vajda,  Harro 
Walk,  and Ken Zeger.  Tamas Linder read most of the book and provided invalu(cid:173)
able feedback. His help is especially appreciated. Several chapters were critically 
read  by  students  in  Budapest.  We  thank  all  of them,  especially  Andras  Antos, 
Miklos Csuros, Balazs Kegl,  Istvan Pali,  and Marti Pinter.  Finally, here is an  al(cid:173)
phabetically ordered list of friends  who  directly  or indirectly contributed to  our 
knowledge and love of nonparametrics: Andrew and Roger Barron, Denis Bosq, 
Prabhir Burman, Tom Cover, Antonio Cuevas, Pierre Devijver, Ricardo Fraiman, 
Ned  Glick,  Wenceslao  Gonzalez-Manteiga,  Peter Hall,  Eiichi Isogai,  Ed Mack, 
Arthur  Nadas,  Georg  Pflug,  George  Roussas,  Winfried  Stute,  Tamas  Szabados, 
Godfried Toussaint, Sid Yakowitz, and Yannis Yatracos. 

Gabor diligently typed the entire manuscript and coordinated all contributions. 
He became quite a TEXpert in the process.  Several figures  were made by  idraw 
and  xi ig by  Gabor  and Luc.  Most of the  drawings  were  directly  programmed 
in PostScript by Luc and an undergraduate student at McGill University, Hisham 
Petry, to whom we are grateful. For Gabor, this book comes at the beginning of his 
career. Unfortunately, the other two authors are not so lucky. As both Luc and Laci 
felt  that they  would  probably  not  write  another  book on  nonparametric  pattern 
recognition-the  random  walk  must  go  on-they decided  to  put  their  general 

Preface 

vii 

view of the subject area on paper while trying to separate the important from the 
irrelevant. Surely, this has contributed to the length of the text. 

So  far,  our random excursions  have  been happy  ones.  Coincidentally,  Luc is 
married to Bea, the most understanding woman in the world, and happens to have 
two great daughters, Natasha and Birgit, who do not stray off their random courses. 
Similarly, Laci has an equally wonderful wife, Kati, and two children with steady 
compasses,  Kati  and Janos.  During  the  preparations  of this  book,  Gabor  met a 
wonderful girl, Arrate. They have recently decided to tie their lives together. 

On the less amorous and glamorous side, we gratefully acknowledge the research 
support of NSERC CANADA, FCAR QUEBEC, OTKA HUNGARY, and the exchange program 
between the Hungarian Academy of Sciences and the Royal Belgian Academy of 
Sciences. Early versions of this text were tried out in some classes at the Technical 
University of Budapest, Katholieke Universiteit Leuven, Universitat Stuttgart, and 
Universite Montpellier II.  We would like to thank those students for their help in 
making this a better book. 

Montreal, Quebec, Canada 
Budapest, Hungary 
Budapest, Hungary 

Luc Devroye 
Laci Gyorfi 
Gabor Lugosi 

Contents 

Preface 

1  Introduction 

2  The Bayes Error 

Another Simple Example 

2.1 
The Bayes Problem 
2.2  A Simple Example 
2.3 
2.4  Other Formulas for the Bayes Risk 
2.5 
2.6 
Problems and Exercises 

Plug-In Decisions 
Bayes Error Versus Dimension 

3  Inequalities and Alternate Distance Measures 

3.1  Measuring Discriminatory Information 
3.2 
The Kolmogorov Variational Distance 
3.3 
The Nearest Neighbor Error 
3.4 
The Bhattacharyya Affinity 
3.5 
Entropy 
3.6 
Jeffreys'Divergence 
3.7 
F-Errors 
3.8 
The Mahalanobis Distance 
3.9 
i-Divergences 
Problems and Exercises 

v 

1 

9 
9 
11 
12 
14 
15 
17 
18 

21 
21 
22 
22 
23 
25 
27 
28 
30 
31 
35 

x 

Contents 

4  Linear Discrimination 

Univariate Discrimination and Stoller Splits 
Linear Discriminants 
The Fisher Linear Discriminant 
The Normal Distribution 
Empirical Risk Minimization 

4.1 
4.2 
4.3 
4.4 
4.5 
4.6  Minimizing Other Criteria 
Problems and Exercises 

5  Nearest Neighbor Rules 

Introduction 

5.1 
5.2  Notation and Simple Asymptotics 
5.3 
5.4 
5.5 

Proof of Stone's Lemma 
The Asymptotic Probability of Error 
The  Asymptotic  Error  Probability  of 
Weighted Nearest Neighbor Rules 
k-Nearest Neighbor Rules: Even k 
Inequalities for the Probability of Error 
Behavior When L * Is  Small 

5.6 
5.7 
5.8 
5.9  Nearest Neighbor Rules When L * = 0 
5.10  Admissibility of the Nearest Neighbor Rule 
5.11  The (k, I)-Nearest Neighbor Rule 
Problems and Exercises 

6  Consistency 

Universal Consistency 
Classification and Regression Estimation 
Partitioning Rules 
The Histogram Rule 
Stone's Theorem 
The k-Nearest Neighbor Rule 
Classification Is Easier Than Regression Function Estimation 
Smart Rules 

6.1 
6.2 
6.3 
6.4 
6.5 
6.6 
6.7 
6.8 
Problems and Exercises 

7  Slow Rates of Convergence 

Finite Training Sequence 
Slow Rates 

7.1 
7.2 
Problems and Exercises 

8  Error Estimation 

Error Counting 

8.1 
8.2  Hoeffding's Inequality 
8.3 
8.4 

Error Estimation Without Testing Data 
Selecting Classifiers 

39 
40 
44 
46 
47 
49 
54 
56 

61 
61 
63 
66 
69 

71 
74 
75 
78 
80 
81 
81 
83 

91 
91 
92 
94 
95 
97 
100 
101 
106 
107 

111 
111 
113 
118 

121 
121 
122 
124 
125 

Contents 

xi 

Estimating the Bayes Error 

8.5 
Problems and Exercises 

9  The Regular Histogram Rule 

The Method of Bounded Differences 
Strong Universal Consistency 

9.1 
9.2 
Problems and Exercises 

10  Kernel Rules 

10.1  Consistency 
10.2  Proof of the Consistency Theorem 
10.3  Potential Function Rules 
Problems and Exercises 

11  Consistency of the k-Nearest Neighbor Rule 

11.1  Strong Consistency 
11.2  Breaking Distance Ties 
11.3  Recursive Methods 
11.4  Scale-Invariant Rules 
11.5  Weighted Nearest Neighbor Rules 
11.6  Rotation-Invariant Rules 
11.7  Relabeling Rules 
Problems and Exercises 

12  Vapnik-Chervonenkis Theory 

12.1  Empirical Error Minimization 
12.2  Fingering 
12.3  The Glivenko-Cantelli Theorem 
12.4  Uniform Deviations of Relative Frequencies from Probabilities 
12.5  Classifier Selection 
12.6  Sample Complexity 
12.7  The Zero-Error Case 
12.8  Extensions 
Problems and Exercises 

13  Combinatorial Aspects of Vapnik-Chervonenkis Theory 

13.1  Shatter Coefficients and VC Dimension 
13.2  Shatter Coefficients of Some Classes 
13.3  Linear and Generalized Linear Discrimination Rules 
13.4  Convex Sets and Monotone Layers 
Problems and Exercises 

14  Lower Bounds for Empirical Classifier Selection 

14.1  Minimax Lower Bounds 
14.2  The Case Lc = 0 
14.3  Classes with Infinite VC Dimension 

128 
129 

133 
133 
138 
142 

147 
149 
153 
159 
161 

169 
170 
174 
176 
177 
178 
179 
180 
182 

187 
187 
191 
192 
196 
199 
201 
202 
206 
208 

215 
215 
219 
224 
226 
229 

233 
234 
234 
238 

xii 

Contents 

14.4  The Case Lc  >  0 
14.5  Sample Complexity 
Problems and Exercises 

15  The Maximum Likelihood Principle 

15.1  Maximum Likelihood:  The Formats 
15.2  The Maximum Likelihood Method: Regression Format 
15.3  Consistency 
15.4  Examples 
15.5  Classical Maximum Likelihood: Distribution Format 
Problems and Exercises 

16  Parametric Classification 

16.1  Example: Exponential Families 
16.2  Standard Plug-In Rules 
16.3  Minimum Distance Estimates 
16.4  Empirical Error Minimization 
Problems and Exercises 

17  Generalized Linear Discrimination 

17.1  Fourier Series Classification 
17.2  Generalized Linear Classification 
Problems and Exercises 

18  Complexity Regularization 

18.1  Structural Risk Minimization 
18.2  Poor Approximation Properties of VC Classes 
18.3  Simple Empirical Covering 
Problems and Exercises 

19  Condensed and Edited Nearest Neighbor Rules 

19.1  Condensed Nearest Neighbor Rules 
19.2  Edited Nearest Neighbor Rules 
19.3  Sieves and Prototypes 
Problems and Exercises 

20  Tree Classifiers 
Invariance 

20.1 
20.2  Trees with the X-Property 
20.3  Balanced Search Trees 
20.4  Binary Search Trees 
20.5  The Chronological k-d Tree 
20.6  The Deep k-d Tree 
20.7  Quadtrees 
20.8  Best Possible Perpendicular Splits 
20.9  Splitting Criteria Based on Impurity Functions 

239 
245 
247 

249 
249 
250 
253 
256 
260 
261 

263 
266 
267 
270 
275 
276 

279 
280 
285 
287 

289 
290 
297 
297 
300 

303 
303 
309 
309 
312 

315 
318 
319 
322 
326 
328 
332 
333 
334 
336 

20.10  A Consistent Splitting Criterion 
20.11  BSP Trees 
20.12  Primitive Selection 
20.13  Constructing Consistent Tree Classifiers 
20.14  A Greedy Classifier 
Problems and Exercises 

21  Data-Dependent Partitioning 

Introduction 

21.1 
21.2  A Vapnik-Chervonenkis Inequality for Partitions 
21.3  Consistency 
21.4  Statistically Equivalent Blocks 
21.5  Partitioning Rules Based on Clustering 
21.6  Data-Based Scaling 
21.7  Classification Trees 
Problems and Exercises 

22  Splitting the Data 

22.1  The Holdout Estimate 
22.2  Consistency and Asymptotic Optimality 
22.3  Nearest Neighbor Rules with Automatic Scaling 
22.4  Classification Based on Clustering 
22.5  Statistically Equivalent Blocks 
22.6  Binary Tree Classifiers 
Problems and Exercises 

23  The Resubstitution Estimate 

23.1  The Resubstitution Estimate 
23.2  Histogram Rules 
23.3  Data-Based Histograms and Rule Selection 
Problems and Exercises 

24  Deleted Estimates of the Error Probability 

24.1  A General Lower Bound 
24.2  A General Upper Bound for Deleted Estimates 
24.3  Nearest Neighbor Rules 
24.4  Kernel Rules 
24.5  Histogram Rules 
Problems and Exercises 

25  Automatic Kernel Rules 

25.1  Consistency 
25.2  Data Splitting 
25.3  Kernel Complexity 
25.4  Multiparameter Kernel Rules 

Contents 

xiii 

340 
341 
343 
346 
348 
357 

363 
363 
364 
368 
372 
377 
381 
383 
383 

387 
387 
389 
391 
392 
393 
394 
395 

397 
397 
399 
403 
405 

407 
408 
411 
413 
415 
417 
419 

423 
424 
428 
431 
435 

xiv 

Contents 

25.5  Kernels of Infinite Complexity 
25.6  On Minimizing the Apparent Error Rate 
25.7  Minimizing the Deleted Estimate 
25.8  Sieve Methods 
25.9  Squared Error Minimization 
Problems and Exercises 

26  Automatic Nearest Neighbor Rules 

26.1  Consistency 
26.2  Data Splitting 
26.3  Data Splitting for Weighted NN Rules 
26.4  Reference Data and Data Splitting 
26.5  Variable Metric NN Rules 
26.6  Selection of k Based on the Deleted Estimate 
Problems and Exercises 

Independent Components 

27  Hypercubes and Discrete Spaces 
27.1  Multinomial Discrimination 
27.2  Quantization 
27.3 
27.4  Boolean Classifiers 
27.5  Series Methods for the Hypercube 
27.6  Maximum Likelihood 
27.7  Kernel Methods 
Problems and Exercises 

28  Epsilon Entropy and Totally Bounded Sets 

28.1  Definitions 
28.2  Examples: Totally Bounded Classes 
28.3  Skeleton Estimates 
28.4  Rate of Convergence 
Problems and Exercises 

29  Uniform Laws of Large Numbers 

29.1  Minimizing the Empirical Squared Error 
29.2  Uniform Deviations of Averages from Expectations 
29.3  Empirical Squared Error Minimization 
29.4  Proof of Theorem 29.1 
29.5  Covering Numbers and Shatter Coefficients 
29.6  Generalized Linear Classification 
Problems and Exercises 

30  Neural Networks 

30.1  Multilayer Perceptrons 
30.2  Arrangements 

436 
439 
441 
444 
445 
446 

451 
451 
452 
453 
454 
455 
457 
458 

461 
461 
464 
466 
468 
470 
472 
474 
474 

479 
479 
480 
482 
485 
486 

489 
489 
490 
493 
494 
496 
501 
505 

507 
507 
511 

30.3  Approximation by Neural Networks 
30.4  VC Dimension 
30.5  L1  Error Minimization 
30.6  The Adaline and Padaline 
30.7  Polynomial Networks 
30.8  Kolmogorov-Lorentz Networks and Additive Models 
30.9  Projection Pursuit 
30.10  Radial Basis Function Networks 
Problems and Exercises 

31  Other Error Estimates 

31.1  Smoothing the Error Count 
31.2  Posterior Probability Estimates 
31.3  Rotation Estimate 
31.4  Bootstrap 
Problems and Exercises 

32  Feature Extraction 

32.1  Dimensionality Reduction 
32.2  Transformations with Small Distortion 
32.3  Admissible and Sufficient Transformations 
Problems and Exercises 

Appendix 

Probability 
Inequalities 

A.l  Basics of Measure Theory 
A.2  The Lebesgue Integral 
A.3  Denseness Results 
A.4 
A.5 
A.6  Convergence of Random Variables 
A.7  Conditional Expectation 
A.8  The Binomial Distribution 
A.9  The Hypergeometric Distribution 
A.I0  The Multinomial Distribution 
A.ll  The Exponential and Gamma Distributions 
A.12  The Multivariate Normal Distribution 

Notation 

References 

Author Index 

Subject Index 

Contents 

xv 

517 
521 
526 
531 
532 
534 
538 
540 
542 

549 
549 
554 
556 
556 
559 

561 
561 
567 
569 
572 

575 
575 
576 
579 
581 
582 
584 
585 
586 
589 
589 
590 
590 

591 

593 

619 

627 

1 
Introduction 

Pattern recognition or discrimination is about guessing or predicting the unknown 
nature of an observation, a discrete quantity such as black or white, one or zero, sick 
or healthy, real or fake. An observation is a collection of numerical measurements 
such as  an image (which is a sequence of bits, one per pixel), a vector of weather 
data,  an  electrocardiogram,  or a  signature  on  a  check  suitably  digitized.  More 
formally,  an  observation  is  a  d-dimensional  vector  x.  The  unknown  nature  of 
the  observation is  called a  class.  It is  denoted by  y  and takes  values  in  a finite 
set  {I, 2, ... , M}.  In  pattern recognition,  one  creates  a function  g(x)  :  nd  -* 
{I, ... , M} which represents one's guess of y  given x. The mapping g is called a 
classifier. Our classifier errs on x  if g(x) i  y. 

How  one creates  a rule  g  depends  upon the  problem at hand.  Experts  can be 
called for medical diagnoses or earthquake predictions-they try to mold g to their 
own knowledge and experience, often by trial and error. Theoretically, each expert 
operates with a built-in classifier g, but describing this g explicitly in mathematical 
form is  not a sinecure.  The sheer magnitude and richness of the space of x  may 
defeat even the best expert-it is  simply impossible to  specify g  for all possible 
x's one is likely to see in the future. We have to be prepared to live with imperfect 
classifiers.  In fact,  how  should we measure  the quality  of a classifier? We  can't 
just dismiss a classifier just because it misclassifies a particular x. For one thing, 
if the observation does not fully describe the underlying process (that is, if y is not 
a deterministic function of x), it is possible that the same x  may give rise to two 
different y's on different occasions. For example, if we just measure water content 
of a person's body, and we find that the person is dehydrated, then the cause (the 
class) may range from a low water intake in hot weather to severe diarrhea. Thus, 
we introduce a probabilistic setting, and let (X, Y) be an n d  x  {I, ... , M}-valued 

2 

1.  Introduction 

random pair.  The distribution of (X, Y) describes the frequency  of encountering 
particular pairs in practice.  An error occurs if g(X) i  Y,  and the probability of 
error for a classifier g is 

L(g) = P{g(X) i  Y} . 

There is a best possible classifier, g*, which is defined by 

g*  = 

arg min  P{g(X) i  Y}  . 

g:Rd-+{I, ... ,M} 

Note that g*  depends upon the distribution of (X, Y). If this distribution is known, 
g*  may be computed. The problem of finding  g*  is Bayes J  problem, and the clas(cid:173)
sifier g*  is called the Bayes classifier (or the Bayes rule). The minimal probability 
of error is  called  the  Bayes  error  and  is  denoted  by  L *  =  L(g*).  Mostly,  the 
distribution of (X, Y) is unknown, so that g*  is unknown too. 

We do not consult an expert to try to reconstruct g*, but have access to a good 
database of pairs (Xi, Yi ),  1 ::::  i  :::;  n, observed in the past. This database may be 
the result of experimental observation (as for meteorological data, fingerprint data, 
ECG data,  or handwritten characters). It could also be obtained through an expert 
or a teacher who filled in the  Yi's  after having seen the  X/so To  find  a classifier 
g with a small probability of error is hopeless unless there is some assurance that 
the (Xi, Yi)'sjointly are somehow representative of the unknown distribution. We 
shall assume in this book that (Xl, Y1 ),  .•. ,  (Xn, Yn),  the data,  is  a sequence of 
independent identically distributed (i.i.d.) random pairs with the same distribution 
as  that of (X, Y).  This is  a very strong assumption indeed.  However,  some theo(cid:173)
retical results are emerging that show that classifiers based on slightly dependent 
data pairs and on i.i.d. data pairs behave roughly the same.  Also,  simple models 
are easier to understand are more amenable to interpretation. 

A classifier is constructed on the basis of Xl, Y1,  ••• ,  X n , Yn  and is denoted by 
gn~ Y  is guessed by gn(X~ Xl, YI ,  ., ., Xn, Yn).  The process of constructing gn  is 
called learning, supervised learning, or learning with a teacher. The performance 
of gn  is measured by the conditional probability of error 

This is a random variable because it depends upon the data. So,  Ln  averages over 
the distribution of (X, Y), but the data is held fixed. Averaging over the data as well 
would be unnatural, because in a given application, one has to  live with the data 
at hand. It would be marginally useful to know the number ELn  as  this  number 
would indicate the quality of an average data sequence,  not your data sequence. 
This text is thus about L n ,  the conditional probability of error. 

An individual mapping gn  : nd x  {nd x  {l, ... , M}}n  ---+  {I, ... , M} is still 
called a classifier. A sequence {gn,  n  ::::  I}  is called a (discrimination)  rule. Thus, 
classifiers are functions, and rules are sequences of functions. 

A novice might ask simple questions like this:  How does one construct a good 
classifier? How good can a classifier be? Is classifier A better than classifier B? Can 
we estimate how good a classifier is? What is the best classifier? This book partially 

1.  Introduction 

3 

answers such simple questions. A good deal of energy is spent on the mathematical 
formulations of the novice's questions. For us, a rule-not a classifier-is good if 
it is consistent, that is, if 

lim  ELn  =  L* 
n-+oo 

or  equivalently,  if Ln  -7  L *  in  probability  as  n  -7  00.  We assume  that  the 
reader has  a good grasp  of the  basic  elements  of probability,  including  notions 
such as convergence in probability, strong laws of large numbers for averages, and 
conditional probability.  A  selection of results  and definitions  that may be useful 
for this text is given in the Appendix. A consistent rule guarantees us that taking 
more samples essentially suffices to roughly reconstruct the unknown distribution 
of (X,  Y)  because  Ln  can be pushed as  close  as  desired  to  L *.  In  other words, 
infinite amounts of information can be gleaned from finite  samples. Without this 
guarantee, we would not be motivated to take more samples. We should be careful 
and not impose conditions on (X, Y) for the consistency of a rule, because such 
conditions  may  not  be  verifiable.  If a  rule  is  consistent  for  all  distributions  of 
(X, Y), itis said to be universally consistent. 

Interestingly, until 1977 , it was not known if a universally consistent rule existed. 
All pre-1977 consistency results came with restrictions on (X, Y). In 1977, Stone 
showed that one could just take any k-nearest neighbor rule with k = k(n)  -7  00 
and kin  -7 0.  The k-nearest neighbor classifier gn(x) takes a majority vote over 
the Y/s in the subset of k pairs (Xi, Yi) from (Xl, YI ),  ... ,  (Xn,  Yn) that have the 
smallest values for II Xi - X II  (i.e., for which Xi is closest to x). Since Stone's proof 
of the universal consistency of the k-nearest neighbor rule, several other rules have 
been shown to  be universally consistent as  well.  This book stresses universality 
and hopefully gives a reasonable account of the developments in this direction. 

Probabilists may wonder why we did not use convergence with probability one 
in our definition of consistency. Indeed, strong consistency-convergence of Ln to 
L * with probability one-implies convergence for almost every sample as it grows. 
Fortunately, for most well-behaved rules, consistency and strong consistency are 

equivalent.  For example, for the k-nearest neighbor rule,  k  -7  00  and kin  -7 ° 

together imply Ln  -7 L * with probability one. The equivalence will be dealt with, 
but it will not be a major focus of attention. Most, if not all, equivalence results are 
based upon some powerful concentration inequalities such as  McDiarmid's. For 
example, we will be able to show that for the k-nearest neighbor rule, there exists 

a number c  >  0,  such that for all E  >  0, there exists  N (E)  > ° depending upon 

the distribution of (X, Y), such that 

P{Ln  - L * >  E}  :::::  e-CnE2 

,  n  ~ N(E)  . 

This illustrates yet another focus of the book-inequalities. Whenever possible, we 
make a case or conclude a proof via explicit inequalities. Various parameters can 
be substituted in these inequalities to allow the user to draw conclusions regarding 
sample size or to permit identification of the most important parameters. 

The material in the book is often technical and dry.  So,  to  stay focused on the 

main issues, we keep the problem simple: 

4 

1.  Introduction 

A.  We only deal with binary classification (M ==  2). The class Y takes values 
in {a,  I}, and a classifier gn  is a mapping: Rd x {Rd X  {a,  l}}n  ~ {a,  I}. 
B.  We only consider i.i.d. data sequences. We also disallow active learning, 

a set-up in which the user can select the Xi'S deterministically. 

C.  We do not consider infinite spaces. For example,  X cannot be a random 
function  such as  a cardiogram.  X  must be  a Rd-valued random vector. 
The reader should be aware that many results given here may be painlessly 
extended to certain metric spaces of infinite dimension. 

Let us return to our novice's questions. We know that there are good rules, but 
just how  good  can  a  classifier be?  Obviously,  Ln  ::::  L * in  all  cases.  It is  thus 
important to know L * or to estimate it, for if L * is large, any classifier, including 
yours, will perform poorly. But even if L * were zero, Ln could still be large. Thus, 
it would be nice to have explicit inequalities for probabilities such as 

P{Ln::::  L* +E}. 

However,  such  inequalities  must  necessarily  depend  upon  the  distribution  of 
(X, Y). That is, for any rule, 

lim inf 
n-+oo  all distributions of (X,y)  with L*+E<1/2 

sup 

P{Ln  ::::  L * + E}  > ° . 

Universal rate of convergence guarantees do not exist. Rate of convergence studies 
must involve certain subclasses of distributions of (X, Y). For this reason, with few 
exceptions, we will steer clear of the rate of convergence quicksand. 

Even if there are no universal performance guarantees, we might still be able to 
satisfy our novice's curiosity if we could satisfactorily estimate Ln  for the rule at 
hand by a function Ln  of the data.  Such functions  are called error estimates. For 
example, for the k-nearest neighbor classifier, we could use the deleted estimate 

where gni(Xi) classifies Xi by the k-nearest neighbor method based upon the data 
(X 1, Yd, ... , (Xn,  Yn) with (Xi, Yi) deleted. If this is done, we have a distribution(cid:173)
free inequality 

---

6k + I 
P{ILn - Ln I >  E}  :S  - - 2 -
nf 

(the Rogers-Wagner inequality), provided that distance ties are broken in an ap(cid:173)
propriate manner. In other words, without knowing the distribution of (X, Y), we 
can state with a certain confidence that Ln is contained in [Ln  - f, ~1 + f]. Thus, 
for many  classifiers,  it is  indeed possible to  estimate  Ln  from  the data at hand. 
However, it is impossible to  estimate L * universally well:  for any n, and any es(cid:173)
timate of L * based upon the data sequence, there  always exists  a distribution of 
(X, Y) for which the estimate is arbitrarily poor. 

Can we compare rules {gn}  and {g~}? Again, the answer is negative: there exists 

1.  Introduction 

5 

no  "best" classifier (or superclassifier), as  for any rule {gn},  there exists  a distri(cid:173)
bution of (X, Y) and another rule  {g~} such that for all n, E{L(g~)} <  E{L(gn)}. 
If there had been a universally best classifier, this book would have been unnec(cid:173)
essary: we would all have to use it all the time. This nonexistence implies that the 
debate between practicing pattern recognizers will never end and that simulations 
on particular examples should never be used to compare classifiers. As an exam(cid:173)
ple,  consider the  I-nearest neighbor rule,  a simple but not universally consistent 
rule. Yet, among all k-nearest neighbor classifiers, the I-nearest neighbor classifier 
is  admissible-there are distributions for  which its  expected probability of error 
is  better than for  any k-nearest neighbor classifier with  k  >  1.  So,  it can never 
be  totally  dismissed.  Thus,  we must study all simple rules,  and we will reserve 
many pages for the nearest neighbor rule and its derivatives. We will for example 
prove the Cover-Hart inequality (Cover and Hart,  1967) which states that for all 
distributions of (X, Y), 

lim sup ELn :s 2L * 

n-+oo 

where  Ln  is  the  probability of error with  the  I-nearest neighbor rule.  As  L * is 
usually  small  (for otherwise,  you  would not want to  do  discrimination),  2L * is 
small too, and the I-nearest neighbor rule will do just fine. 

The nonexistence of a best classifier may disappoint our novice.  However, we 
may change the setting somewhat and limit the classifiers to a certain class C, such 
as  all k-nearest neighbor classifiers with all possible values for k.  Is it possible to 
select the best classifier from this class? Phrased in this manner, we cannot possibly 
do better than 

def  . 

L  =  mf  P{gn(X) -r  Y  . 

..../ 

} 

gn EC 

Typically, L  >  L * . Interestingly, there is a general paradigm for picking classifiers 
from C and to obtain universal performance guarantees. It uses empirical risk min(cid:173)
imization, a method studied in great detail in the work of Vapnik and Chervonenkis 
(1971). For example, if we select gn  from C by minimizing 

then the corresponding probability of error Ln  satisfies the following  inequality 
for all E  >  0: 

P{Ln  >  L+E}::S 8(n V  +I)e-nE 2

/128. 

Here V  >  0 is an integer depending upon the massiveness of C only. V is called the 
vc dimension of C and may be infinite for large classes C. For sufficiently restricted 
classes C,  V  is finite and the explicit universal bound given above can be used to 
obtain performance guarantees for the selected g n  (relative to L, not L *). The bound 
above  is  only  valid if C is  independent of the data pairs  (Xl, YI ), ... , (Xn,  Yn). 
Fixed  classes  such  as  all  classifiers  that  decide  1  on  a  halfspace  and  0  on  its 
complement are fine. We may also sample m more pairs (in addition to the n pairs 

6 

1.  Introduction 

already present),  and use the n  pairs as  above to  select the best k  for  use in the 
k-nearest neighbor classifier based  on the  m  pairs.  As  we  will  see,  the  selected 
rule is universally consistent if both m and n diverge and n flog m  --+  00. And we 
have automatically solved the problem of picking k.  Recall that Stone's universal 
consistency  theorem only  told  us  to  pick  k  = o(m)  and  to  let  k  --+  00,  but it 
does  not tell us  whether k  ~ m 0.01  is  preferable over k  ~ m 0.99.  Empirical risk 
minimization produces a random data-dependent k that is not even guaranteed to 
tend to infinity or to be oem), yet the selected rule is universally consistent. 

We offer virtually no help with algorithms as in standard texts, with two notable 

exceptions.  Ease of computation,  storage, and interpretation has  spurred the de(cid:173)
velopment of certain rules. For example, tree classifiers construct a tree for storing 
the data,  and partition R/ by certain cuts that are typically perpendicular to a co(cid:173)
ordinate axis. We say that a coordinate axis is "cut." Such classifiers have obvious 
computational advantages, and are amenable to interpretation-the components of 
the vector X that are cut at the early stages of the tree are most crucial in reaching a 
decision. Expert systems, automated medical diagnosis, and a host of other recog(cid:173)
nition rules use tree classification. For example, in automated medical diagnosis, 
one may first  check a patient's pulse (component #1). If this  is  zero,  the patient 
is dead. If it is below 40, the patient is weak. The first component is cut twice. In 
each case, we may then consider another component, and continue the breakdown 
into more and more specific cases.  Several interesting new universally consistent 
tree classifiers are described in Chapter 20. 

The second group  of classifiers whose development was  partially based upon 
easy implementations  is  the  class  of neural network classifiers,  descendants  of 
Rosenblatt's perceptron  (Rosenblatt,  1956).  These classifiers have unknown pa(cid:173)
rameters that must be trained or selected by the data, in the way we let the data pick 
k in the k-nearest neighbor classifier. Most research papers on neural networks deal 
with the  training  aspect,  but we  will not.  When we  say  "pick the parameters by 
empirical risk minimization," we will leave the important algorithmic complexity 
questions unanswered. Perceptrons divide the space by one hyperplane and attach 
decisions  1 and 0 to the two halfspaces. Such simple classifiers are not consistent 
except for  a few  distributions.  This is  the case,  for  example,  when  X  takes  val(cid:173)
ues on {O,  1}d  (the hypercube) and the components of X are independent. Neural 
networks  with  one hidden  layer are  universally  consistent if the  parameters  are 
well-chosen.  We  will see that there is  also  some gain in considering two hidden 
layers, but that it is not really necessary to go beyond two. 

Complexity  of the  training  algorithm-the phase  in  which  a  classifier  gn  is 
selected from  C-is of course important.  Sometimes,  one  would  like  to  obtain 
classifiers  that  are  invariant  under  certain  transformations.  For example,  the  k(cid:173)
nearest neighbor classifier is not invariant under nonlinear transformations of the 
coordinate  axes.  This  is  a drawback as  components  are  often  measurements  in 
an  arbitrary  scale.  Switching to a logarithmic  scale or stretching a  scale out by 
using  Fahrenheit instead of Celsius  should not affect good discrimination rules. 
There exist variants of the k-nearest neighbor rule that have the given invariance. 
In character recognition, sometimes all components of a vector X that represents 

1.  Introduction 

7 

a character are true measurements involving only vector differences between se(cid:173)
lected points such as  the leftmost and rightmost points, the geometric center, the 
weighted center of all black pixels, the topmost and bottommost points. In this case, 
the scale has essential information, and invariance with respect to changes of scale 
would be detrimental. Here however, some invariance with respect to orthonormal 
rotations is healthy. 

We  follow  the standard notation from textbooks on probability.  Thus, random 
variables are uppercase characters such as  X, Y,  and Z. Probability measures are 
denoted by greek letters  such as  JL  and  1).  Numbers  and vectors  are  denoted by 
lowercase letters such as a, b, c, x, and y. Sets are also denoted by roman capitals, 
but there are obvious mnemonics:  S denotes a sphere,  B denotes a Borel set, and 
so forth.  If we need many kinds of sets, we will typically use the beginning of the 
alphabet (A,  B, C). Most functions  are denoted by  j, g, ¢, and 0/.  Calligraphic 
letters such as A, C,  and:F are used to denote classes of functions or sets. A short 
list of frequently used symbols is found at the end of the book. 

At the end of this chapter, you will find a directed acyclic graph that describes 
the dependence between chapters. Clearly, prospective teachers will have to select 
small subsets of chapters. All chapters, without exception, are unashamedly theo(cid:173)
retical. We did not scar the pages with backbreaking simulations or quick -and-dirty 
engineering solutions. The methods gleaned from this text must be supplemented 
with  a healthy  dose  of engineering  savvy.  Ideally,  students  should have  a  com(cid:173)
panion text filled with beautiful applications such as  automated virus recognition, 
telephone eavesdropping language recognition, voice recognition in security sys(cid:173)
tems,  fingerprint recognition, or handwritten character recognition.  To run a real 
pattern recognition project from  scratch,  several classical texts on statistical pat(cid:173)
tern recognition could and should be consulted, as  our work is limited to general 
probability-theoretical aspects of pattern recognition. We have over 430 exercises 
to help the scholars. These include skill honing exercises, brainteasers, cute puz(cid:173)
zles,  open problems,  and  serious  mathematical challenges.  There is  no  solution 
manual.  This book is only a start.  Use it as a toy-read some proofs, enjoy some 
inequalities,  learn new  tricks,  and study the art of camouflaging one problem to 
look like another. Learn for the sake of learning. 

8 

1.  Introduction 

Introduction 

Inequalities and alternate distance measures 

1 
2  The Bayes error 
3 
4  Linear discrimination 
5  Nearest neighbor rules 
6  Consistency 
7  Slow rates of convergence 
8  Error estimation 
9  The regular histogram rule 
10  Kernel rules 
11  Consistency of the k-nearest neighbor rule 
12  Vapnik-Chervonenkis theoI)' 
13  Combinatorial aspects of V apnik -Chervonenkis theory 
14  Lower bounds for empirical classifier selection 
15  The maximum likelihood principle 
16  Parametric classification 
17  Generalized linear discrimination 
18  Complexity regularization 
19  Condensed and edited nearest neighbor rules 
20  Tree classifiers 
21  Data-dependent partitioning 
22  Splitting the data 
23  The resubstitution estimate 
24  Deleted estimates of the error probability 
25  Automatic kernel rules 
26  Automatic nearest neighbor rules 
27  Hypercubes and discrete spaces 
28  Epsilon entropy and totally bounded sets 
29  Uniform laws of large numbers 
30  Neural networks 
31  Other error estimates 
32  Feature extraction 

FIGURE 1.1. 

2 
The Bayes Error 

2.1  The Bayes Problem 

In this  section, we define the mathematical model and introduce the notation we 
will use for the entire book. Let (X, Y) be a pair of random variables taking their 
respective values from Rd and {O,  I}.  The random pair (X, Y)  may be described 
in a variety of ways:  for example, it is  defined by the pair (IL,  1]),  where IL  is the 
probability measure for X  and 1]  is the regression of Y on X. More precisely, for 
a Borel-measurable set A  S;  R d , 

IL(A) = P{X E  A}, 

and for any x  E  R d

, 

1](x) = PlY = llX = x} = E{YIX = x}. 

Thus,  1] (x ) is the conditional probability that Y  is  1 given X  = x. To  see that this 
suffices to describe the distribution of (X, Y), observe that for any C  S;  Rd X  {O,  I}, 
we have 

C  =  (C n (Rd  x  {OJ))  U (C n (Rd  x  {IJ)) d;t Co  x  {OJ  U C1 x  {I}, 

and 

P{(X, Y)  E  C} 

P{X  E  Co,  Y  = O} + P{X  E  C1,  Y = I} 

[  (1  - 1](x))JL(dx) +  [  1](x)JL(dx). 
lco 

lCI 

10 

2.  The Bayes Error 

As this is valid for any Borel-measurable set C, the distribution of (X, Y) is deter(cid:173)
mined by (Il,  ry).  The function ry  is sometimes called the a posteriori probability. 
Any function  g  : n d  -+  {a,  1}  defines a classifier or a decision function.  The 
error probability of g  is  L(g) = P{g(X) =I  Y}. Of particular interest is the Bayes 
decision function 

*(x) =  {  1 
g 

if ry(x)  :- 1/2 

° otherwIse. 

This decision function minimizes the error probability. 
Theorem 2.1.  For any decision function g  : nd  -+  {a,  1}, 

P{g*(X) =I  Y}  :::  P{g(X) =I Y}, 

that is,  g* is the optimal decision. 

PROOF.  Given X  =  x, the conditional error probability of any decision g may be 
expressed as 

P{g(X) =I YIX = x} 

=  1 - P{Y = g(X)IX = x} 

=  1 -

=  1 -

=  1 -

(P{Y =  1, g(X) =  11X = x} +P{Y = 0, g(X) = 0IX = xD 
(!{g(x)=l}P{Y  =  IIX = x} + I{g(x)=O}P{Y  = OIX = x}) 
(!{g(x)=l}ry(X) + f{g(x)=O} (1  - ry(x») , 

where  fA  denotes the indicator of the set A. Thus, for every x  E  n d , 

P{g(X) =I YIX = x} - P{g*(X) =I  YIX = x} 

= 

ry(x)  (!{g*(x)=l)  -

f{gex)=l})  + (l - ry(x))  (I{g*ex)=o)  -

I{g(x)=O}) 

(2ry(x)  - 1) (I{g*ex)=l)  -

f{g(x)=1l) 

= 

>  ° 

by the definition of g*. The statement now follows by integrating both sides with 
respect to Il-(dx).  0 

decide class 0 

112 ________________ _ 

FIGURE 2.1.  The  Bayes  decision  in  the 
example  on  the  left  is  1 if x  >  a,  and 

° otherwise. 

2.2 A Simple Example 

11 

REMARK. g* is called the Bayes decision and L * = P{g*(X) =I Y} is referred to as 
the Bayes probability of error, Bayes error,  or Bayes risk.  The proof given above 
reveals that 

L(g) = 1 - E {I{g(X)=I}ry(X) + I{g(x)=O}(l  -

ry(X»} , 

and in particular, 

L * = 1 - E {I{1J(X»1/2}ry(X) + I{1J(X):S1/2}(l  -

ry(X»}.  0 

We observe that the a posteriori probability 

ry(x) = P{Y = l/X = x} = E{Y/X = x} 

minimizes the squared error when Y is to be predicted by f(X) for some function 
f  : Rd  --+  R: 

E {(ry(X) - y)2}  :::  E {(f(X) _  y)2}. 

To see why the above inequality is true, observe that for each x  E  R d , 

E {(f(X) - y)21X = x} 

=  E {(f(x) -
=  (f(x) -

ry(x»2 + 2(f(x) -
+E {(ry(X)  - y)21X = x} 

ry(x) + ry(x)  - y)21X = x} 

ry(x»E{ry(x) - YIX = x} 

= 

(f(x) -

ry(x»2 + E {(ry(X) - y)21X = x} . 

The conditional median, i.e., the function minimizing the absolute error E{ I f (X) -
Y I}  is even more closely related to the Bayes rule (see Problem 2.12). 

2.2  A Simple Example 

Let us  consider the prediction of a  student's performance in a  course (pass/fail) 
when given a number of important factors.  First, let Y  =  1 denote a pass and let 

Y  = ° stand for failure.  The sole observation X  is  the number of hours of study 

per week. This, in itself, is not a foolproof predictor of a student's performance, 
because for that we would need more information about the student's quickness of 
mind, health, and social habits. The regression function ry(x) = P{Y = 11X = x} is 
probably monotonically increasing in x. If it were known to be ry(x)  = x / (c + x), 
c >  0,  say, our problem would be solved because the Bayes decision is 

g*(x) = {01 

if ry(x)  >  1/2 (i.e., x  >  c) 
otherwise. 

12 

2.  The Bayes Error 

The corresponding Bayes error is 

L * = L(g*) = E{min(71(X),  1 - 71(X»}  = E  {min(e, X)}  . 

e+X 

While we could deduce the Bayes decision from 71  alone, the same cannot be said 
for the Bayes error L * -it requires knowledge of the distribution of X. If X  =  e 
with probability one (as in an army school, where all students are forced to study 
e  hours per week), then L * =  1/2. If we have a population that is  nicely spread 
out, say,  X is uniform on [0, 4e], then the situation improves: 

1 14c  minCe, x) 

4e  0 

e + x 

L*  =  -

dx =  -log - ~ 0.305785. 

1 
4 

5e 
4 

Far away from x  = e, discrimination is really simple. In general, discrimination is 
much easier than estimation because of this phenomenon. 

2.3  Another Simple Example 
Let us work out a second simple example in which Y  = ° or Y  = 1 according to 

whether a student fails or passes a course. X represents one or more observations 
regarding the student.  The components of X  in our example will be denoted by 
T,  B, and  E  respectively,  where  T  is  the average number of hours the students 
watches  TV,  B  is  the  average  number  of beers  downed  each  day,  and  E  is  an 
intangible quantity measuring extra negative factors such as laziness and learning 
difficulties. In our cooked-up example, we have 

y={l  ifT+B+E<7 

° otherwise. 

Thus, if T, B, and E  are known,  Y is known as well. The Bayes classifier decides 

1 if T  + B  + E  <  7  and ° otherwise.  The corresponding Bayes  probability  of 
error is zero. Unfortunately, E  is intangible, and not available to the observer. We 
only have access to T  and B. Given T  and B, when should we guess that Y = I? 
To  answer this  question,  one  must know  the joint distribution  of (T, B, E),  or, 
equivalently, the joint distribution of (T, B, Y). So, let us assume that T, B, and E 
are i.i.d. exponential random variables (thus, they have density e-u on [0,(0». The 
Bayes rule compares P{Y  =  liT, B} with P{Y = 0IT, B} and makes a  decision 
consistent with the maximum of these two values. A simple calculation shows that 

P{Y =  liT, B}  =  P{T + B + E  <  71T,  B} 
=  prE <  7 - T  - BIT, B} 
max (0,  1 - e-(7-T-B))  . 

The crossover between two decisions occurs when this value equals 1 /2. Thus, the 
Bayes classifier is as follows: 

g*(T, B) = {I 

if T + ~ <  7 -log2 = 6.306852819 ... 

o  otherWIse. 

2.3  Another Simple Example 

13 

Of course, this classifier is not perfect. The probability of error is 

P{g*(T,  B) =I Y} 

=  P{T +B  <  7 -log2,T +B + E  2:  7} 

+P{T + B  2:  7 -log2, T  + B + E  <  7} 

=  E {e-(7-T-B) I{T+B<7-10g2)} 

P {(I 
+ 

- e 

-(7-T-B)) I 

{7>T+Bo::7-log2) 

} 

= 

xe-x (1  - e-(7-x))  dx 

/,7 

xe-x e-(7-x) dx + 

17-10g2 
o 
(since the density of T  + B  is ue-u  on [0, (0» 
-7 ((7 -log2)2 
72 
+ 2(8 - log 2) - 8 - 2  + 

7-log2 

2 

=  e 

(7  -lOg2)2) 

2 

(as Jxoo  ue-udu = (1  + x)e- X

) 

=  0.0199611  e 

_  • •  

If we  have only  access  to  T,  then the Bayes classifier is  allowed to use  T  only. 
First, we find 

P{Y=IIT}  =  P{E+B<7-TIT} 

max (0,  1 -

(1  + 7 - T)e-(7-T)) 

The  crossover  at  1/2 occurs  at  T  = c  =  5.321653009 ... ,  so  that  the  Bayes 
classifier is given by 

def 

g*(T) = {I 

if T  <  .c 

° otherwIse. 

The probability of error is 

P{g*(T) =I Y} 

P{T  <  c,  T  + B + E  ::::  7} + P{T ::::  c,  T  + B + E  <  7} 

=  E {(l + 7 - T)e-(7-T) I{T<cd 

+ P {(1  - (1  + 7 - T)e-(7-T)) I{7>To::cd 

1c 
e-x(l + 7 - x)e-(7-x) dx + 17 e-X  (1  - (1 + 7 - x)e-(7-x»)  dx 
-7 (82 

(8 - c)2 

-(c-7) 

+e 

1-

(8 - C)2  1) 
+-
2 

2 

=  e  - -

2 

2 

=  0.02235309002 .... 

The Bayes error has increased slightly, but not by much. Finally, if we do not have 
access to any of the three variables, T, B, and E, the best we can do is see which 

14 

2.  The Bayes Error 

class is most likely. To this end, we compute 

pry = O}  = P{T + B + E  ~ 7}  = (1 + 7 + 72/2)e-7  = .02963616388 ... 

If we set g  ==  1 all the time, we make an error with probability 0.02963616388 .... 
In  practice,  Bayes  classifiers  are unknown  simply because the distribution of 

(X, Y) is unknown. Consider a classifier based upon (T, B). Rosenblatt's percep(cid:173)
tron (see Chapter 4) looks for the best linear classifier based upon the data. That 
is, the decision is of the form 

I 
geT,  B) =  0 

{

if aT + bB  <  c 
otherwise 

for some data-based choices for a, band c. If we have lots of data at our disposal, 
then it is possible to pick out a linear classifier that is nearly optimal. As we have 
seen above, the Bayes classifier happens to be linear. That is a sheer coincidence, 
of course.  If the  Bayes  classifier  had not  been  linear-for example,  if we  had 
Y  = I{T+B2+E<7}-then even the best perceptron would be suboptimal, regardless 
of how  many  data pairs  one  would have.  If we use  the  3-nearest neighbor rule 
(Chapter 5), the asymptotic probability of error is not more than 1.3155 times the 
Bayes error,  which in our example is  about 0.02625882705. The example above 
also shows the need to look at individual components, and to evaluate how many 
and which components  would be most useful for discrimination.  This subject is 
covered in the chapter on feature extraction (Chapter 32). 

2.4  Other Formulas for the Bayes Risk 

The following forms of the Bayes error are often convenient: 

L *  = 

inf  P{g(X) =I  Y} 

g:RC~{O,l} 

=  E  {min{1](X),  1 - 1](X)}} 

1 

1 
2:  - 2: E  {121](X)  -

= 

II}· 

FIGURE 2.2.  The Bayes decision when class(cid:173)
conditional densities exist. In the figure on 
the  left,  the  decision  is  0  on  [a,  b]  and  1 
elsewhere. 

pi, 

Class  I 

Class 0 

Class 1 

2.5 Plug-In Decisions 

15 

In special cases,  we may obtain other helpful forms.  For example, if X  has  a 

density  f, then 

L * 

f min(1](x),  1 - 1](x))f(x)dx 
f min((l - p)fo(x), plI(x))dx, 

where p  = P{Y = I}, and fi(X)  is the density of X  given that Y  = i.  p  and 1 - p 
are called the class probabilities, and fa,  fl  are the class-conditional densities. If 
fa  and II  are nonoverlapping, that is, f  fofl  = 0, then obviously L * = 0. Assume 
moreover that p  =  1/2. Then 

L*  =  ~ f min (fo(x),  fl(X))dx 

~ f II (x) - (II (x)  -
=  ~ - ~ f Ifl(X)  -

fo(x)ldx. 

fo(x))+ dx 

Here g+  denotes the positive part of a function g.  Thus, the Bayes error is directly 
related to the L 1  distance between the class densities. 

FIGURE 2.3.  The  shaded area  is 
the  L 1  distance  between  the 
class-conditional densities. 

2.5  Plug-In Decisions 

The best guess of Y  from the observation X  is the Bayes decision 

*(x) = {o  if 1](x)  ::::;  1/2  = {o  if 1](x)  ::::;  1 - 1](x) 

1  otherwise. 

g 

1  otherwise 

The function 1]  is typically unknown. Assume that we have access to nonnegative 
functions  i](x), 1 -
1] (x ) respectively. In this 
case it seems natural to use the plug-in decision function 

i](x) that approximate 1] (x ) and 1 -

g(x) =  {01 

if i](x)  ::::;  1/2 
otherwise, 

16 

2.  The Bayes Error 

to approximate the Bayes decision. The next well-known theorem (see, e.g., Van 
Ryzin (1966), Wolverton and Wagner (1969a), Glick (1973), Csibi (1971), Gyorfi 
(1975), (1978), Devroye and Wagner (1976b), Devroye (1982b), and Devroye and 
Gyorfi  (1985))  states  that if i1(x)  is  close  to  the  real  a  posteriori  probability  in 
L 1-sense, then the error probability of decision g is near the optimal decision g*. 
Theorem 2.2.  For the error probability of the plug-in decision  g defined above, 
we have 

P{g(X) =I Y}  - L * = 2  [  11J(x)  - 1/2II {g(x):fg*(x)}{L(dx), 

lRd 

and 

P{g(X) =I Y} - L* S  2  r  11J(x)  -

lRd 

i1(x)IJL(dx) = 2EI1J(X) -

i1(X)I. 

PROOF.  If for some x  E  nd , g(x) = g*(x), then clearly the difference between the 
conditional error probabilities of g  and g* is zero: 

P{g(X) =I  YIX = x} - P{g*(X) =I  YIX = x} = O. 

Otherwise, if g(x) =I g*(x), then as seen in the proof of Theorem 2.1, the difference 
may be written as 

P{g(X) =I YIX = x} 

P{g*(X) =I YIX = x} 

= 

= 

(21J(x)  - 1) (I{g*(x)::::l}  -

I{g(x)=l}) 

12 17(X)  -

lII{g(x)¥g*(x)}. 

Thus, 

P{g(X) =I Y}  - L *  = 

[211J(x) - 1/2II{g(x):fg*(x)}{L(dx) 
JRd 

< 

since g(x) =I g*(x) implies 11J(x)  -

[211J(x) -17(x)IJL(dx), 
JRf 
i1(x)1  ::::  11J(x)  - 1/21.  0 

When the classifier g(x) can be put in the form 

(x) = {O  ifi11(X~ S  i1o(x) 
g 

1  otherWIse, 

where  i11(X),  i1o(x)  are  some approximations  of 1J(x)  and  1 - 7J(X),  respectively, 
the situation differs from that discussed in Theorem 2.2 if i1o(x) + 171 (x) does not 
necessarily equal to one. However, an inequality analogous to that of Theorem 2.2 
remains true: 

2.6 Bayes Error Versus Dimension 

17 

Theorem 2.3.  The error probability of the decision defined above is bounded from 
above by 

P{g(X) =I Y}  - L *:s  [  10 - lJ(x»  -

JRd 

ilo(x)ll-L(dx) +  [ 
. .  JRd 

IlJ(x) -

ill (x)II-L(dx). 

The proof is left to the reader (Problem 2.9). 
REMARK. Assume that the class-conditional densities fo, II exist and are approxi(cid:173)
mated by the densities 10 (x ),  ir (x). Assume furthermore that the class probabilities 
p ==  pry ==  1}  and 1 - P ==  P{Y = O}  are approximated by PI  and Po, respectively. 
Then for the error probability of the plug-in decision function 

g(x) = {o  if PIJI(X) :s  poJo(x) 

1  otherwise, 

P{g(X) =I Y}  - L * 

:s 

[10 - p)foex) - poJoex)ldx +  [ 
JRd 
JRd 

IpfIex) - PIirex)ldx. 

See Problem 2.10.0 

2.6  Bayes Error Versus Dimension 

The components of X  that matter in the Bayes classifier are those that explicitly 
appear in lJ(X). In fact, then, all discrimination problems are one-dimensional, as 
we could equally well replace X by lJeX)  or by any strictly monotone function of 
1](X), such as  lJ 7 eX) + 5lJ3(X) + lJ(X). Unfortunately, lJ  is unknown in general. In 
the example in Section 2.3, we had in one case 

lJ(T,  B) = max (0,  1 - e-C7 - T- B») 

and in another case 

lJeT)  = max (0,  1 - 0 + 7 - T)e-(7-T») . 

The former format suggests that we could base all decisions on T + B. This means 
that if we had no access to T  and B individually, but to T + B jointly, we would be 
able to achieve the same results! Since lJ  is unknown, all of this is really irrelevant. 

In  general,  the  Bayes  risk  increases  if we  replace  X  by  TeX)  for  any  trans(cid:173)
formation  T  (see Problem 2.1),  as  this  destroys information.  On the other hand, 
there exist transformations  (such as  lJ(X»  that leave the Bayes error untouched. 
For more on the relationship between the Bayes error and the dimension, refer to 
Chapter 32. 

18 

2.  The Bayes Error 

Problems and Exercises 

PROBLEM  2.1.  Let  T  :  X  -+  X'  be  an  arbitrary  measurable  function.  If L ~ and  L~(X) 
denote the Bayes error probabilities for (X, Y) and (T(X), y), respectively, then prove that 

(This  shows  that transformations  of X  destroy  information,  because  the  Bayes  risk  in(cid:173)
creases.) 

PROBLEM 2.2.  Let X' be independent of (X, Y). Prove that 

Lex,Xf) = L~. 

PROBLEM 2.3.  Show that L*  ::::  min(p, 1 - p), where p, 1 - P are the class probabilities. 
Show that equality holds if X  and Y  are independent. Exhibit a distribution where X is not 
independent of Y, but L * = rnin(p, 1 - p). 

PROBLEM 2.4.  NEYMAN-PEARSON LEMMA.  Consider again the decision problem, but with 
a decision g, we now assign two error probabilities, 

L(O)(g) = P{g(X) = 11Y = O}  and  L(1)(g) = P{g(X) = OIY =  I}. 

Assume that the class-conditional densities  fo,  fl  exist. For c  >  0, define the decision 

if cfl(x) >  !o(x) 
otherwise. 

Prove that for any decision g, if L(O)(g)  :::::  L(O)(gc), then L(1\g) ::::  L(l)(gc)' In other words, 
if L (0)  is required to be kept under a certain level, then the decision minimizing L (1)  has the 
form of gc  for some c.  Note that g*  is like that. 

PROBLEM 2.5.  DECISIONS WITH REJECTION.  Sometimes in decision problems, one is allowed 
to say "I don't know," if this does not happen frequently. These decisions are called decisions 
with a reject option (see, e.g., Forney (1968), Chow (1970». Formally, a decision g(x) can 
have three values: 0,  1, and "reject." There are two performance measures:  the probability 
of rejection P{g(X) = "reject"}, and the error probability P{g(X) i  Ylg(X) i  "reject"}. 
For a ° <  c  <  1/2, define the decision 
gc(x) = { ~ 

if ry(x)  >  1/2 + c 
ifry(x)::::  1/2 - c 
otherwise. 

"reject" 

Show that for any decision g, if 

P{g(X) = "reject"}  ::::  P{gc(X) = "reject"}, 

then 

P{g(X) i  Y!g(X) i  "reject"}  ::::  P{gc(X) i  Ylgc(X) i  "reject"}. 

Thus, to keep the probability of rejection under a certain level, decisions of the form of gc 
are optimal (Gyorfi, Gyorfi, and Vajda (1978». 

Problems and Exercises 

19 

PROBLEM 2.6.  Consider the prediction of a student's failure based upon variables T  and B, 
where Y  = I{T+B+E<7}  and E  is an inaccessible variable (see Section 2.3). 

(1)  Let T, B, and E be independent. Merely by changing the distribution of E, show 
that the Bayes error for classification based upon (T, B) can be made as close as 
desired to  1/2. 

(2)  Let T  and B be independent and exponentially distributed. Find ajoint distribution 

of (T, B, E) such that the Bayes classifier is not a linear classifier. 

(3)  Let T and B be independent and exponentially distributed. Find a joint distribution 

of (T, B, E) such that the Bayes classifier is given by 

g*(T, B) = {I 

if T2  +.B2  <  10, 

° otherwIse. 

(4)  Find the Bayes classifier and Bayes error for classification based on (T, B) (with 

Y as above) if (T, B, E) is uniformly distributed on [0, 4J3. 

PROBLEM 2.7.  Assume that T,  B, and E  are independent uniform [0,4] random variables 
with interpretations as in Section 2.3. Let Y  = 1 (0) denote whether a student passes (fails) 
a course. Assume that Y  = 1 if and only if T B E  :s  8. 

(1)  Find the Bayes decision if no variable is available, if only T  is  available, and if 

only T  and B  are available. 

(2)  Determine in all three cases the Bayes error. 
(3)  Determine the best linear classifier based upon T  and B  only. 

PROBLEM 2.8.  Let 1]', 1]"  : nd  -+  [0,  1] be arbitrary measurable functions,  and define the 
corresponding decisions by g'(x) = I{T/'(x»1/2j  and g"(x) = I(T/I/(x»1/2j. Prove that 

/L(g') - L(gff)/ :s P{g'(X) i  gff(X)} 

and 

/L(g') - L(gff)1  :s E {121](X)  - 1II{gl(X)igl/(x)d  . 

PROBLEM 2.9.  Prove Theorem 2.3. 

PROBLEM 2.10.  Assume  that  the  class-conditional  densities  fo  and  fi  exist  and  are  ap(cid:173)
proximated  by  the  densities  fo  and A,  respectively.  Assume  furthermore  that  the  class 
probabilities  P = pry = I}  and  1 - P = pry = o} are approximated by PI  and Po.  Prove 
that for the error probability of the plug-in decision function 
if PI A (x) :s  Po 10 ex ) 
otherwise, 

g(x) = { ~ 

we have 

P{g(X) i  Y}  - L * :s  llPfl (x) - PIA ex)ldx + 1 10  - p)fo(X) - polo(x)ldx. 

nd 

PROBLEM 2.11.  Using the notation of Problem 2.10, show that if for a sequence of fm,n (x) 
and Pm,n  (m = 0,  1), 

nd 

20 

2.  The Bayes Error 

then  for  the  corresponding  sequence  of plug-in  decisions  limn---+oo P{gn(X)  =I  YJ  =  L * 
(Wolverton and Wagner (l969a». HINT:  According to Problem 2.10, it suffices to show that 
if we are given a deterministic sequence of density functions  f, !I, 12, h, ... , then 

implies 

n---+oo 

lim f (fn(x) -
lim f Ifn(x) -

11---+00 

f(X»2 dx = 0 

f(x)ldx = O. 

(A function  f  is  called a density function if it is  nonnegative and f  f(x)dx  =  1.) To  see 
this, observe that 

where AI, A 2 ,  •.. is a partition of n d  into unit cubes, and f+  denotes the positive part of a 
function f. The key observation is that convergence to zero of each term of the infinite sum 
implies  convergence of the whole integral by the dominated convergence theorem,  since 
f (fn(x) -
f(x»+ dx :s f  fn(x)dx = 1. Handle the right-hand side by the Cauchy-Schwarz 
inequality. 
PROBLEM 2.12.  Define the Ll error of a function  f  : n d 
--'J>  n by  J(f) = E{lf(X) - YI}. 
Show that a function minimizing J (f) is the Bayes rule g*, that is,  J*  = inf f  J (f) = J (g*). 
Thus,  J*  = L * . Define a decision by 

g(x) = 

{ 

if  f(x):s  1/2 

0 
1  otherwise, 

Prove that its error probability L(g) = P{g(X) =I Y}  satisfies the inequality 

L(g) - L*  :s  J(f) - J*. 

3 
Inequalities and Alternate 
Distance Measures 

3.1  Measuring Discriminatory Information 

In our two-class discrimination problem, the best rule has (Bayes) probability of 
error 

L * = E {min(1J(X),  1 - 1J(X))} . 

This quantity measures how difficult the discrimination problem is. It also serves 
as  a gauge of the quality of the distribution of (X, Y) for pattern recognition. Put 
differently,  if 1/11  and  1/12  are certain many-to-one mappings,  L * may be used to 
compare discrimination based on (1/11 (X), Y) with that based on (1/12 (X), Y). When 
1/11  projects n d  to n d
]  by taking the first d 1  coordinates, and 1/12  takes the last d2 
coordinates, the corresponding Bayes errors will help us decide which projection 
is better. In this sense, L * is the fundamental quantity in feature extraction. 

Other quantities have been suggested over the years that measure the discrimi(cid:173)

natory power hidden in the distribution of (X, Y). These may be helpful in some 
settings. For example, in theoretical studies or in certain proofs, the relationship be(cid:173)
tween L * and the distribution of (X, Y) may become clearer via certain inequalities 
that link L * with other functionals of the distribution. We all understand moments 
and variances, but how do these simple functionals relate to L *? Perhaps we may 
even learn a thing or two about what it is that makes L * small. In feature selection, 
some explicit inequalities involving L * may provide just the kind of numerical in(cid:173)
formation that will allow one to make certain judgements on what kind of feature 
is preferable in practice. In short, we will obtain more information about L * with 
a variety of uses in pattern recognition. 

22 

3.  Inequalities and Alternate Distance Measures 

In the next few  sections, we avoid putting any conditions on the distribution of 

(X, Y). 

3.2  The Kolmogorov Variational Distance 

Inspired  by  the  total  variation  distance  between  distributions,  the  Kolmogorov 
variational distance 

bKO  =  ~E {IP{Y = IIX} - pry = DIX}I} 

2 

1 =  "2 E{ /21](X)  - 11} 

captures the distance between the two classes. We will not need anything special 
to deal with bKO  as 

L*  =  E g -~12~(X) - II} 

1 

I 
"2  - "2E {12ry(X) - 11} 

I 
"2  - OKQ' 

= 

= 

3.3  The Nearest Neighbor Error 

The asymptotic error of the nearest neighbor rule is 

LNN  = E{2ry(X)(1  -

ry(X))} 

(see Chapter 5). Clearly, 2ryO  -ry) 2:  min(1],  1 -ry) as 2 max(1] , 1 -ry) 2:  1. Also, 
using the notation A = min(1](X),  1 -

ry(X)), we have 

L * :::  LNN  =  2E{AO - A)} 

<  2E{A}· E{1  - A} 

(by the second association inequality of Theorem A.19) 

=  2L*(1-L*):::;2L*, 

(3.1) 

which are well-known inequalities of Cover and Hart (1967). LNN provides us with 
quite a bit of information about L * . 

The measure LNN has been rediscovered under other guises: Devijver and Kittler 
(1982, p.263) and Vajda (1968) call it the quadratic entropy, and Mathai and Rathie 
(975) refer to it as the harmonic mean coefficient. 

3.4 The Bhattacharyya Affinity 

23 

FIGURE 3.1.  Relationship between the 
Bayes error and the  asymptotic near(cid:173)
est neighbor error.  Every point in  the 
unshaded region is possible. 

112 

L*~NN 

~N~2L*(1-L*) 

o 

112 

3.4  The Bhattacharyya Affinity 

The Bhattacharyya measure of affinity (Bhattacharyya, (1946)) is  - log p, where 

p = E  {)7J(X)(1 - 7J(X))} 

will be referred to as the Matushita error. It does not occur naturally as the limit of 
any standard discrimination rule (see, however, Problem 6.11). p was suggested as 
a distance measure for pattern recognition by Matushita (1956). It also occurs under 
other guises in mathematical statistics-see, for example, the Hellinger distance 
literature (Le Cam (1970), Beran (1977)). 

Clearly,  p  =  0  if and  only  if 7J(X)  E  {O,  I}  with  probability  one,  that  is,  if 
L * = O.  Furthermore, p takes its maximal value 1/2 if and only if 7J(X) = 1/2 with 
probability one. The relationship between p  and L * is not linear though. We will 
show that for all distributions,  LNN  is more useful than p  if it is to be used as  an 
approximation of L * . 

Theorem 3.1.  For all distributions,  we have 

1 
- - -J1- 4p2  < 
2 

1 
2 

1 
- - -)1- 2L  T 
2 

1 
2 

NN 

<  L* 

<  p. 

PROOF.  First of all, 

p2 

E2 {J 7J(X)(1  - 7J(X))} 

<  E {7J(X)(l  - 7J(X))}  (by Jensen's inequality) 

24 

3.  Inequalities and Alternate Distance Measures 

= 

<  L *(1  - L *)  (by the Cover-Hart inequality (3.1». 

Second, as  J11(I  -
Finally, by the Cover-Hart inequality, 

11)  ~ 211(1  -17) for all 11  E  [0,1], we  see that p  ~ LNN  ~ L*. 

)1- 2LNN  ~ )1- 4L*(1- L*) = 1 - 2L*. 

Putting all these things together establishes the chain of inequalities.  0 

112 

FIGURE 3.2.  The inequalities linking p 
to  L * are illustrated. Note  that the re(cid:173)
gion  is  larger than  that cut out in the 
(L *, LNN) plane in Figure 3.1. 

4p2~1-(1-2L*)2 

p 

112 

The  inequality  LNN  :s  p  is  due  to  Ito  (1972).  The  inequality  LNN  ~ 2p2  is 
due  to  Horibe  (1970).  The  inequality  1/2 - )1 - 4p2/2  :s  L *  :s  p  can  be 
found  in Kailath  (1967).  The left-hand side  of the  last inequality was  shown by 
Hudimoto  (1957).  All these inequalities are tight (see Problem 3.2). The appeal 
of quantities like  LNN  and p is  that they involve polynomials of 11,  whereas  L * = 
E{min(11(X),  1 - 11(X»}  is  nonpolynomial.  For certain discrimination problems 
in which  X  has  a distribution that is  known  up  to  certain parameters,  one may 
be  able  to  compute  LNN  and  p  explicitly as  a function  of these  parameters.  Via 
inequalities, this may then be used to obtain performance guarantees for parametric 
discrimination rules of the plug-in type (see Chapter 16). 

For completeness, we mention a generalization of Bhattacharyya's measure of 

affinity, first suggested by Chernoff (1952): 

where ex  E  (0,  1) is fixed.  For ex  =  1/2, be  =  -log p. The asymmetry introduced 
by taking ex  =/1/2 has no practical interpretation, however. 

3.5  Entropy 

The entropy of a discrete probability distribution (PI, P2,  ... ) is defined by 

3.5 Entropy 

25 

1{ = 1{(PI, P2,  ... ) = - LPi log Pi, 

00 

i=1 

where, by definition, 0 log 0 = 0 (Shannon (1948)). The key quantity in information 
theory  (see  Cover  and  Thomas  (1991)),  it  has  countless  applications  in  many 
branches of computer science, mathematical statistics, and physics. The entropy's 
main properties may be summarized as follows. 

A.  1{ ::::  0 with equality if and only if Pi  =  1 for some i. Proof: log Pi  ::::  0 for 
all i with equality if and only if Pi  = 1 for some i. Thus, entropy is minimal 
for  a  degenerate  distribution,  i.e.,  a  distribution  with the  least  amount of 
"spread." 

B.  1{(PI, ... , Pk)  ::::  log k  with equality if and only if PI  = P2  = ... = Pk  = 

1/ k.  In other words, the entropy is maximal when the distribution is maxi(cid:173)
mally smeared out. Proof: 

by the inequality logx  ::::  x 

1, x  >  o. 

C.  For a Bernoulli distribution (p,  1 - p), the binary entropy 1{(p, 1 - p) = 

- P log P  -

(1  - p) log(1  - p) is concave in p. 

Assume that  X  is  a  discrete random variable that must be guessed by asking 
questions of the type "is X  E  A?," for some sets A. Let N be the minimum expected 
number of questions required to determine X  with certainty. It is well known that 

1{ 

--<N<--+1 
log2  -

1{ 
log 2 

(e.g., Cover and Thomas (1991)). Thus, 1{ not only measures how spread out the 
mass of X  is, but also provides us with concrete computational bounds for certain 
algorithms. In the simple example above, 1{ is in fact proportional to the expected 
computational time of the best algorithm. 

We  are not interested in information theory per se, but rather in its usefulness 
in pattern recognition.  For our discussion,  if we fix  X  = x, then  Y  is  Bernoulli 
(rJ(x)).  Hence, the conditional entropy of Y given X  = x  is 

1{(r](X), 1- r](x)) = -rJ(x)logrJ(x) - (1- rJ(x))log(1- r](x)). 

It measures the amount of uncertainty or chaos in Y  given X  = x. As we know, it 
takes values between 0 (when rJ(x)  E  {O,  I}) and log 2 (when r](x) =  1/2), and is 

26 

3.  Inequalities and Alternate Distance Measures 

concave in 1J(x). We define the expected conditional entropy by 

E  =  E {11(1J(X),  1 - 1J(X))} 

=  -E {1J(X) log 1J(X) + (1  - 1l(X)  log(1  - 1J(X»} . 

For brevity,  we will refer to E as  the entropy. As pointed out above, E = 0 if and 
only if 1J(X)  E  {O,  l}  with probability one.  Thus,  E and  L * are  related to  each 
other. 

Theorem 3.2. 

A.  E::::  11(L*, 1 - L*) =  -L*logL* -

(1  - L*)log(1  - L*) (Fano's  in(cid:173)

equality; Fano (1952),  see  Cover and Thomas (1991, p.  39)). 

B.  E:::  -log(1 - L NN )  :::  -log(1 - L *). 
C.  E::::  log 2 - ~(l - 2L NN ) 

::::  log 2 - ~(l- 2L*)2. 

L* 

1/2 

FIGURE 3.3.  Inequalities (A) and 
(B)  of  Theorem  3.2  are  illus(cid:173)
trated here. 

£"? -log(I-L*) 

£ ~ 11(L*,I-L*) 

o 

o 

log  2 

PROOF. 
PART A.  Define A  = min(1J(X),  1 - 1J(X». Then 

E  =  E{11(A,  1 - A)} 

::::  11(EA, 1 - EA) 

(because 11 is concave, by Jensen's inequality) 

=  11(L *,  1 - L *). 

PARTB. 

E  = 

::: 

-E{A log A + (l - A) log(l - A)} 
-E {log (A2  + (l - A)2)} 
-E{log(l - 2A(l - A»)} 

(by Jensen's inequality) 

> 

-log(1 - E{2A(1 - A)}) 

(by Jensen's inequality) 

3.6 Jeffreys' Divergence 

27 

= 

> 

-log(1 - L NN ) 

-log(1 - L *). 

PART C.  By the concavity of H(A, 1 - A) as  a function of A, and Taylor series 
expansion, 

H(A,1  A):::; log 2 - 2(2A - 1)2. 

1 

Therefore, by Part A, 

= 

log2 

1 
-(1 
2 

< 

= 

* 
log 2 - 2  + 2L  (1  - L  ) 

* 

1 

(by the Cover-Hart inequality (3.1)) 

1 

,  2 

log 2 - 2(1 - 2L'1')  .  0 

REMARK.  The  nearly  monotone  relationship  between £  and  L *  will  see  lots  of 
uses. We warn the reader that near the origin, L * may decrease linearly in £, but 
it may also decrease much faster than £209. Such wide variation was not observed 
in the relationship between L * and LNN  (where it is linear) or L * and p  (where it 
is between linear and quadratic).  0 

3.6  Jeffreys'  Divergence 

Jeffreys'  divergence (1948) is  a symmetric form of the  Kullback-Leibler (1951) 
divergence 

OKL  = E 

{ 

rJ(X) log 

rJ(X) 

1 -

rJ(X) 

} 

. 

It will be denoted by 

J  = E 

(2rJ(X) - 1) log 
{ 

rJ(X) 

1 -

rJ(X) 

} 

. 

To understand J, note that the function (2rJ  - 1) log  1~T/  is symmetric about 1/2, 
convex, and has minimum (0) at rJ  =  1/2. As  rJ  + 0, rJ  t  1, the function becomes 
unbounded.  Therefore,  J  = 00  if P{rJ(X)  E  {O,  I}}  >  0.  For  this  reason,  its 

28 

3.  Inequalities and Alternate Distance Measures 

use in discrimination is  necessarily limited.  For generalizations of J, see Renyi 
(1961), Burbea and Rao (1982), Taneja (1983; 1987), and Burbea (1984). It is thus 
impossible to bound J  from above by a function of LNN and/or L * . However, lower 
bounds are easy to obtain. As x log((1 + x)/(1  - x)) is convex in x  and 

(21]  - 1) log - - = 121]  -

1] 

1-1] 

11  log 

(  1 + 121]  -

11  ) 
1-121]-11 

, 

we note that by Jensen's inequality, 

J 

2:  E{121](X)  -

11} log  1 _  E{121](X)  _  11} 

( 

1 + E{121](X)  -

II} ) 

*  ( 1 + (1  - 2L *)) 

(1  - 2L  )log  1 _  (1 

2L*) 

= 

(1- 2L*) log  ~ 

1 - L*) 

(

>  2(1  - 2L *)2. 

The first bound cannot be universally bettered (it is achieved when 1] (x ) is constant 
over  the  space).  Also,  for  fixed  L *,  any  value  of J  above  the  lower  bound is 
possible for  some  distribution  of (X, Y).  From the  definition  of J, we see that 

J  = ° if and only if 1]  ==  1/2 with probability one, or L * = 1/2. 

FIGURE 3.4.  This  figure 
illustrates 
the  above  lower  bound on  Jeffreys' 
divergence  in  terms of the Bayes er(cid:173)
ror. 

~-- J?:.  (l-2L*) log((l-C)/L*) 

L* 

112 

Related bounds were obtained by Toussaint (1974b): 

(  1 + Jl - 2LNN  ) 
J  2:  v 1 - 2LNN  log  1 _  Jl _ 2LNN 

/ 

2:  2(1  - 2LNN). 

The last bound is strictly better than our L * bound given above. See Problem 3.7. 

3.7  F-Errors 

The error measures discussed so far are all related to expected values of concave 
functions of 1](X) = P{Y = lIX}. In general, if F  is a concave function on [0,  1], 

3.7 F-Errors 

29 

we define the  F -error corresponding to (X,  Y) by 

dF(X, Y) = E {F(1](X»}. 

Examples of F -errors are 

(a)  the Bayes error L *:  F(x) = min(x,  1 - x), 

(b)  the asymptotic nearest neighbor error LNN:  F(x) = 2x(1  - x), 

(c)  the Matushita error p:  F(x) = y'x(1  - x), 

(d)  the expected conditional entropy [: F (x) =  -x log x  -

(1  - x) log(1  - x), 

(e)  the negated Jeffreys'  divergence -J: F(x) = -(2x - 1) log  l~x' 

Hashlamoun,  Varshney,  and  Samarasooriya  (1994)  point  out  that  if F(x)  > 
min(x,l  - x) for  each x  E  [0,1], then  the  corresponding  F-error is  an  upper 
bound on the Bayes error. The closer F (x) is to min(x, 1 - x), the tighter the upper 
bound is. For example,  F(x) = (1/2) sin(JTx)  :::  2x(1 
x) yields an upper bound 
tighter than L NN •  All these errors share the property that the error increases if X is 
transformed by an arbitrary function. 
Theorem 3.3.  Let t  : nd  -+ nk be an arbitrary measurable function.  Thenfor 
any distribution of (X, Y), 

PROOF.  Define 1][  : nk -+  [0,  1] by 1]t(x) = P{Y = 1It(X) = x}, and observe that 

1]t(t(X»  = E {1](X)lt(X)} . 

Thus, 

E {F(1]t(t(X»)} 

E {F(E {1](X)lt(X)})} 

>  E {E {F(1](X»lt(X)}} 

(by Jensen's inequality) 

E{F(1](X»} = dF(X, Y).  0 

REMARK.  We  also  see from the proof that the  F -error remains  unchanged if the 
transformation t is invertible. Theorem 3.3 states that F -errors are a bit like Bayes 
errors-when information is lost (by replacing X by t(X», F -errors increase.  0 

30 

3.  Inequalities and Alternate Distance Measures 

3.8  The Mahalanobis Distance 

Two conditional distributions with about the same covariance matrices and means 
that are far away from each other are probably so well separated that L * is small. 
An interesting measure of the visual distance between two random variables  Xo 
and Xl is the so-called Mahalanobis distance (Mahalanobis, (1936)) given by 

whereml  = EX1, mo  = EXo, are the means, ~l = E {(Xl  - md(Xl - mIl} and 
~o = E {(Xo  - mo)(Xo - mol} are  the  covariance matrices,  ~ = P ~l + (1  -
P ) ~o, (·l is the transpose of a vector,  and  p  =  1 - p  is  a mixture parameter. If 
~l =  ~o = (J2 I, where I  is the identity matrix, then 

~ = _II m_l _-_m_o_11 

is  a scaled version of the distance between the means.  If ~l = (Jf I,  ~o =  (J5 I, 
then 

IIml  - moll 

~ =  ---;====== 

J P(Jf + (1  - P)(J5 

varies between Ilml -moll/(Jl and Ilml -moll/(Jo as P changes from 1 toO. Assume 
that we have a discrimination problem in which given  Y  = 1,  X is  distributed as 
Xl, given  Y  =  0,  X is distributed as  Xo,  and  p  =  P{Y  =  1}, 1 - p  are the class 
probabilities. Then, interestingly, ~ is related to the Bayes error in a general sense. 
If the Mahalanobis distance between the class-conditional distributions is  large, 
then L * is small. 

Theorem 3.4.  (DEVIJVER AND  KITTLER  (1982, p.  166)).  For all distributions of 
(X, Y)for which E {IIXI12}  <  00,  we have 

L*<L  < 

-

NN  -

2p(1 

p) 

1 + p(1  _  p)~2 

REMARK. For a distribution with mean m and covariance matrix ~, the Mahalanobis 
distance from a point x  E n d  to m is 

In one dimension, this is simply interpreted as distance from the mean as measured 
in units of standard deviation. The use of Mahalanobis distance in discrimination 
is based upon the intuitive notion that we should classify according to the class for 
which we are within the least units of standard deviations. At least, for distributions 
that look like nice globular clouds, such a recommendation may make sense.  0 

3.9 f-Divergences 

31 

PROOF. First assume that d = 1, that is, X is real valued. Let u and c be real numbers, 
and consider the quantity E  {(u(X - c) -
(217(X)  - 1))2}. We will show that if u 
and c are chosen to minimize this number, then it satisfies 

o :s  E  {(u(X - c) -

(217(X)  - 1))2}  = 2 

2p(1-p) 
1 + p(1 - p).6. 

) 
2  - LNN 

( 

, 

(3.2) 

which  proves  the  theorem  for  d  =  1.  To  see  this,  note  that  the  expression 
(217(X)  - 1))2}  is  minimized  for  c  =  EX  - E{217(X)  - 1}/u. 
E {(u(X - c) 
Then 

E {(u(X - c) -

(217(X)  - 1))2} 

=  Var{217(X) -

I} + u2 Var{X}  - 2u Cov{X, 217(X)  -

I}, 

-where Cov{X, Z}  = E{(X - EX)(Z - EZ)}-which is, in turn, minimized for 
u  =  Cov{X, 217(X)  - 1}/Var{X}.  Straightforward calculation shows  that (3.2) 
indeed holds. 

To  extend  the  inequality  (3.2)  to  multidimensional  problems,  apply  it  to  the 
one-dimensional decision problem (Z, Y),  where  Z  = XTb-l(ml  mo).  Then 
the theorem follows by noting that by Theorem 3.3, 

where LNN(X,  Y) denotes the nearest-neighbor error corresponding to (X, Y).  0 

In case X I  and Xo  are both normal with the same covariance matrices, we have 

Theorem 3.5.  (MATusHITA  (1973);  SEE  PROBLEM  3.11).  When  Xl  and Xo  are 
multivariate normal random variables with  bl = bO  = b, then 

If the  class-conditional  densities  !l and  10  may  be  written  as  functions  of 
(x - ml)Tb11(x - md and (x  molbol(X - mo) respectively, then.6. remains 
relatively tightly linked with L * (Mitchell and Krzanowski (1985)), but such dis(cid:173)
tributions are the exception rather than the rule.  In general, when .6.  is  small, it is 
impossible to deduce whether L * is small or not (see Problem 3.12). 

3.9  f-Divergences 

We have defined error measures as the expected value of a concave function of 17(X). 
This makes it easier to relate these measures to the Bayes error L * and other error 
probabilities. In this section we briefly make the connection to the more classical 
statistical theory of distances between probability measures. A general concept of 
these distance measures, called I-divergences, was introduced by Csiszar (1967). 

32 

3.  Inequalities and Alternate Distance Measures 

The corresponding theory is summarized in Vajda (1989). F -errors defined earlier 
may be calculated if one knows the class probabilities p, 1 - p, and the conditional 
distributions fLo,  fL 1 of X, given {Y ::: o} and {Y :::  I}, that is, 

fLi(A) ::: P{X E  AIY ::: i}  i::: 0,  1. 

For fixed class probabilities, an F -error is small if the two conditional distributions 
are "far away" from each other. A metric quantifying this distance may be defined 
as follows. Let f  : [0, (0) ~ R  U {-oo, oo}  be a convex function with  f(l) :::  0. 
The f  -divergence between two probability measures fL  and v on Rd is defined by 

Di(fL, v):::  sup  L v(Aj)f (fL(A

j

)) , 

A={A j } 

j 

v(Aj) 

where the supremum is taken over all finite measurable partitions A of Rd. If A is a 
measure dominating fL  and v-that is, both fL  and v are absolutely continuous with 
respect to  A-and p  :::  dfL/dA  and q  :::  dv/dA are the  corresponding densities, 
then the f -divergence may be put in the form 

Di(fL, v)::: f q(x)f (P(X))  A(dx). 

q(x) 

Clearly,  this  quantity  is  independent  of the  choice  of A.  For example,  we  may 
take A :::  fL + v. If fL  and v are absolutely continuous with respect to the Lebesgue 
measure, then A may be chosen to be the Lebesgue measure. By Jensen's inequality, 
Di(fL, v)  ?:  0, and Di(fL, fL)::: 0. 

An important example  of  f -divergences  is  the  total  variation,  or variational 

distance obtained by choosing f  (x) :::  Ix  - 11, yielding 

V(fL, v):::  sup  L IfL(A j ) - v(Aj)l· 

A={A j } 

j 

For this divergence, the equivalence of the two definitions is  stated by Scheffe's 
theorem (see Problem 12.13). 

Theorem 3.6.  (SCHEFFE  (1947)). 

V(fL,  v) :::  2 s~p IfL(A) - v(A)I::: f Ip(x) - q(x)IA(dx), 

where the supremum is taken over all Borel subsets of Rd. 

Another important example  is  the  Hellinger distance,  given by  f(x)  :::  (1  -

,JX)2: 

H 2(fL,  v) 

::: 

sup  2  (1 - L J fL(A j )v(A j )) 

A={A j } 

j 

=  2 (1 -f v' P(X)q(X)A(dX») 

3.9 i-Divergences 

33 

The quantity 12(J.L,  v) = f ,Jp(x)q(x)J...(dx) is often called the Hellinger integral. 
We mention two useful inequalities in this respect. For the sake of simplicity, we 
state their discrete form.  (The integral forms  are analogous, see Problem 3.21.) 

Lemma 3.1.  (LECAM (1973». For positive sequences ai  and bi, both summing to 
one, 

PROOF.  By the Cauchy-Schwarz inequality, 

L..t  YUiUi_ 
" 
r;;E; < 
i:ai<bi 

This, together with the inequality (x + y)2  :::  2X2 + 2 y2, and symmetry, implies 

(~Ja;b)2 

C~b,Jaibi+;~,Jaibi)2 

<  2C~/a;by +2C~/aibir 
<  2 C~b' ai  + ;~, bi) 

2 Lmin(ai' bi).  0 

Lemma 3.2.  (DEVROYE  AND  GYORFI  (1985, p.  225»).  Let al, ... , ak>  bt, ... bk 
be nonnegative numbers such that L:=I ai  = L:=I bi = 1.  Then 

PROOF. 

(by the Cauchy-Schwarz inequality) 

34 

3.  Inequalities and Alternate Distance Measures 

k 

2 

i=l 

<  4 L ( y'ai - ~) 
=  S(l-tJa'h} 

which proves the lemma.  0 

Information divergence is obtained by taking  f  (x) = x log x: 

I (IL,  v) is also called the Kullback-Leibler number. 

Our last example is the x2-divergence, defined by  f(x) = (x  - 1)2: 

' "  (IL(A·) - v(A .))2 

} 

sup  ~  } 
A={A j } 

j 

v(Aj) 

f p2(X) A(dx) - 1. 

q(x) 

Next,  we  highlight  the  connection between  F -errors  and  f -divergences.  Let 
ILo  and ILl  denote the conditional distributions of X  given {Y  = O}  and {Y  = I}. 
Assume that the class probabilities are equal: p  =  1/2. If F  is a concave function, 
then the F -error dF(X, Y) may be written as 

where 

f(x) = -~F (_X_) (1  +x) + F  (~), 

2 

l+x 

2 

and  D f  is  the  corresponding  f -divergence.  It is  easy  to  see  that  f  is  convex, 
whenever F  is concave. A special case of this correspondence is 

) 
*  1 (1 
L  ="2  1 - 2 V (ILo,  IL 1) 
if p  = 1/2. Also, it is easy to verify, that 

, 

where p is the Matushita error. For further connections, we refer the reader to the 
exercises. 

Problems and Exercises 

35 

Problems and Exercises 

PROBLEM3.1.  Showthatforevery(I 1,I*)withO:s I*:s I 1 :s  21*(1-1*):s  1/2, there exists 
a distribution of (X, Y)  with LNN  = II andL * = [*.  Therefore, the Cover-Hart inequalities 
are not universally improvable. 

PROBLEM  3.2.  TIGHTNESS  OF THE BOUNDS.  Theorem 3.1  cannot be improved. 

(1)  Show  that for  all  a  E  [0,  1/2],  there  exists  a  distribution  of (X, Y)  such  that 

LNN  = L * = a. 

(2)  Show  that  for  all  a  E  [0,  1/2],  there  exists  a  distribution  of (X, Y)  such  that 

LNN  = a, L* = ~ - ~,JI - 2a. 

(3)  Show  that for  all  a  E  [0,  1/2],  there  exists  a  distribution  of (X, Y)  such  that 

L*=p=a. 

(4)  Show  that for  all  a  E  [0,  1/2],  there  exists  a  distribution  of (X, Y)  such  that 

L 

NN-a, 

-

L*- 1 

-2"-2"Y.L-'TP-· 

1  ~4 2 

PROBLEM  3.3.  Show that E :::  L *. 

PROBLEM 3.4.  For any a  :s  1, find a sequence of distributions of (Xn'  Yn) having expected 

conditional entropies En  and Bayes errors L~ such that L~ ~ ° as n  ~ 00, and En  decreases 
to zero at the same rate as  (L ~) a . 

PROBLEM  3.5 .  CONCAVITY OF ERROR MEASURES.  Let Y  denote the mixture random variable 
taking the value Y1 with probability p and the value Y2 with probability 1-p. Let X be a fixed 
Rd-valuedrandom variable, and define 1]1(X)  = P{Y1 =  llX = x}, 1]2(X)  = P{Y2 =  11X  = x}, 
where Y1, Y2 are Bernoulli random variables. Clearly, 1] (x ) = PrJl (x) + (1- P )rJ2(X). Which 
of the error measures L *,  p, LNN , E are concave in P for fixed joint distribution of X, Y1,  Y2? 
Can every discrimination problem (X, Y) be decomposed this way for some Y1,  p, Y2 , where 
171 (x),  rJ2(X)  E  {O,  I}  for all x? If not, will the condition 1]1 (x), rJ2(X)  E  {O,  1/2, I}  for all x 
do? 

PROBLEM 3.6.  Show that for every 1*  E  [0, 1/2], there exists a distribution of (X, Y) with 
L * = l* and E = H(L *,  1 - L *). Thus, Fano's inequality is tight. 

PROBLEM 3.7.  TOUSSAINT'S  INEQUALITIES  (1974B).  Mimic a proof in the text to show that 

J>  1 - 2LNN log 

J 

-

( 1 + ,Jl - 2LNN  ) 
,J 1 - 2LNN 
I -

>  2(1  - 2LNN). 
-

PROBLEM  3.8.  Show  that  L *  :s  e-oc ,  where  DC  is  Chernoff's  measure  of affinity  with 
parameter a  E  (0,  1). 

PROBLEM  3.9.  Prove that  L*  =  p  - E{(2rJ(X) - 1)+},  where  p  =  pry =  I},  and (x)+  = 
max(x, 0). 

PROBLEM  3.10.  Show that J  :::  -210gp - 2H(p, 1 - p), where p  = pry = I}  (Toussaint 
(1974b». 

PROBLEM  3 .11.  Let fl  and fo  be two multivariate normal densities with means rno, rn 1  and 
common covariance matrix L  IfP{Y = I}  = p, and iI, fo  are the conditional densities of 

36 

3.  Inequalities and Alternate Distance Measures 

X given Y = 1 and Y = ° respectively, then show that 

p =  Jp(1- p)e-6.2

/8, 

where p  is the Matushita error and  ~ is the Mahalanobis distance. 

PROBLEM 3.12.  For every 0 E  [0,(0) and [*  E  [0,  1/2] with [*  ::::  2/(4 + 02
), find distribu(cid:173)
tions  ILo  and ILl  for X  given Y  = 1, Y  = 0,  such that the Maha1anobis distance tl = 0,  yet 
L * = [*. Therefore, the Maha1anobis distance is not universally related to the Bayes risk. 

PROBLEM 3.13.  Show that the Mahalanobis distance  ~ is invariant under linear invertible 
transformations of X. 

PROBLEM 3.14.  Lissack and Fu (1976) have suggested the measures 

For ex  = 1, this is twice the Ko1mogorov distance 0KO.  Show the following: 

(1) 
(2) 

IfO  <  ex  ::::  1, then ~(1 - OLF)  ::::  L* ::::  (1  - o~~a). 
If 1 ::::  ex  <  00, then  ~(1 - J~~a) ::::  L * ::::  (1  - OLF)' 

PROBLEM 3.15.  Hashlamoun,  Varshney,  and Samarasooriya (1994)  suggest using  the  F(cid:173)
error with the function 

)  -1.8063(x-i)2 

F()  1. ( 

X  = 2"  SIll  TrX  e 

to  obtain tight upper bounds  on  L *.  Show that  F (x)  2:  min(x, 1 - x), so  that the corre(cid:173)
sponding F -error is indeed an upper bound on the Bayes risk. 
PROBLEM 3.16.  Prove that L* ::::  max(p(1- p»  (1  -

~ V(ILo,  ILd). 

PROBLEM 3.17.  Prove that L*  ::::  Jp(1- P)h(ILolLl). HINT:  min(a, b)::::.JQb. 
PROBLEM 3.18.  Assume  that  the  components  of X  = (X(1),  ... , Xed)~ are  conditionally 
independent (given  y), and identically distributed, that is, P{XU)  E  AIY = j} = vj(A) for 
i = 1,  ... , d and j  = 0,  1.  Use the previous exercise to show that 

PROBLEM 3.19.  Show that X2(ILI' IL2)  2:  f(ILI, IL2)' HINT:  x-I 2:  10gx. 
PROBLEM 3.20.  Show the following  analog of Theorem 3.3. Let t  : n d  --+  n k  be a mea(cid:173)
surable function,  and IL,  v  probability measures on nd. Define the measures ILt  and Vt  on 
n k  by  ILt(A)  =  IL(t-\A»  and  vt(A)  =  vet-leA»~. Show that for  any convex function  j, 
Dj(IL, v) 2:  Dj(ILt, vt). 

PROBLEM 3.21.  Prove  the  following  connections  between  the  Hellinger integral  and  the 
total variation: 

and 

HINT:  Proceed analogously to Lemmas 3.1  and 3.2. 

(V(IL, V»2  ::::  8(1  - h(IL, v». 

Problems and Exercises 

37 

PROBLEM  3.22.  PINSKER'S INEQUALITY.  Show that 

(V (/1, V))2  .::::  21 (/1, v) 

(csiszar (1967), Kullback (1967), and Kemperman (1969)). HINT: First prove the inequality 
if J1  and v are concentrated on the same two atoms. Then define A = {x  : p(x) 2::  q (x)}, and 
v*(1) = 
the measures /1*,  v* on the set {O,  I} by /1*(0)  =  1 -
v(A), and apply the previous result. Conclude by pointing out that Scheffe's theorem states 
V(J1*,  v*) = V(/1, v), and that 1(J1*, v*)  .::::  1(/1, v). 

/1*(1)  = /1(A) and v*(O) =  1 

4 
Linear Discrimination 

In this chapter, we split the space by a hyperplane and assign a different class to each 
halfspace.  Such rules offer tremendous advantages-they are easy to interpret as 
each decision is based upon the sign of L~=l aix(i) + ao, where x  = (x(1),  ... , xed)) 
and the ai's are weights. The weight vector determines the relative importance of 
the components. The decision is also easily implemented-in a standard software 
solution, the time of a decision is proportional to d-and the prospect that a small 
chip can be built to make a virtually instantaneous decision is particularly exciting. 
Rosenblatt  (1962)  realized  the  tremendous  potential  of such  linear rules  and 
called themperceptrons. Changing one or more weights as new data arrive allows 
us  to  quickly  and easily  adapt the  weights  to new  situations.  Training  or learn(cid:173)
ing patterned after the  human brain thus  became a reality.  This  chapter merely 
looks at some theoretical properties of perceptrons. We begin with the simple one(cid:173)
dimensional situation, and deal with the choice of weights in nd further on. Unless 
one is terribly lucky, linear discrimination rules cannot provide error probabilities 
close  to  the  Bayes  risk,  but  that  should  not  diminish  the  value  of this  chapter. 
Linear discrimination is at the heart of nearly every successful pattern recognition 
method, including tree classifiers (Chapters 20 and 21), generalized linear classi(cid:173)
fiers  (Chapter 17),  and neural networks  (Chapter 30).  We  also encounter for the 
first time rules in which the parameters (weights) are dependent upon the data. 

40 

4.  Linear Discrimination 

input 

• 

Oarl 

weights 

FIGURE 4.1.  Rosenblatt's perceptron.  The decision is based upon 
a linear combination of the components of the input vector. 

4.1  Univariate Discrimination and Stoller Splits 

As an introductory example, let X be univariate. The crudest and simplest possible 
rule is the linear discrimination rule 

g(x) = 
{ 

y' 
1 - y'  otherwise, 

if x  ~ x' 

where x' is a split point and y' E  {O,  1} is aclass. Ingeneral,x' and y' are measurable 
functions of the data Dn. Within this class of simple rules, there is of course a best 
possible  rule  that  can  be  determined  if we  know  the  distribution.  Assume  for 
example that (X, Y) is described in the standard manner: let P{Y = 1}  = p. Given 
Y  = 1,  X has a distribution function  Fl(X) = P{X  ~ xlY = 1},  and given Y  = 0, 
X  has  a  distribution function  Fo(x)  = P{X  ~ xlY  = O},  where  Fo  and  Fl  are 
the  class-conditional distribution functions.  Then a theoretically  optimal rule is 
determined by the split point x* and class y* given by 

(x* , y*) = arg min P{g(X) =I Y} 

(x',y') 

(the minimum is  always reached if we allow the values  x' =  00 and x' =  -(0). 
We call the corresponding minimal probability of error L  and note that 

L  = 

inf  {I{y'=o}  (p Fl (x') + (1  - p)(1 
(x',y') 

Fo(x'))) 

+ IV=l} (p(1  - Fl (x')) + (1  - p )Fo(x')) } . 

A split defined by (x*, y*) will be called a theoretical Stoller split (Stoller (1954)). 

4.1  Univariate Discrimination and Stoller Splits 

41 

Lemma 4.1.  L  ::::;  1/2 with equality if and only if L * = 1/2. 

PROOF. Take (x', y') = (-00,0). Then the probability of erroris 1- p  = P{Y = OJ. 
Take (x', y') = (-00, 1). Then the probability of error is p. Clearly, 

L * ::::;  L  ::::;  min(p, 1 - p). 

This  proves  the  first  part  of the  lemma.  For the  second  part,  if L  =  1/2,  then 
p  = 1/2, and for every x, pFl(X) + (1- p)(1  - Fo(x))  ::::  1/2 and p(1  FI(X)) + 
(1- P )Fo(x) ::::  1/2. The first inequality implies p FI (x) - (1- p )Fo(x) ::::  p  1/2, 
(1  - p) Fo(x)  ::::;  p  - 1/2. Therefore, L  = 1/2 
while the second implies p FI (x) -
(1  - p)Fo(x)  = p  - 1/2.  Thus,  for  all  x, 
means  that  for  every  x,  pFI(x)  -
FI (x) = Fo(x), and therefore L * =  1/2. D 

Lemma 4.2. 

L  = ~ - sup IPFI(X) -

2 

x 

(1  - p)Fo(x) - p  + ~I. 
2 

In particular,  if p  = 1/2, then 
1 
L  =  - -
2 

1 
- sup IFI(X) - Fo(x)l. 
2  x 

PROOF.  Set p(x) = P FI (x) - (l - p )Fo(x). Then, by definition, 

L  = 

inf min {p(x) + 1 - p, p  - p(x)} 
x 

~ - sup I p(x) - p  + ~ I 
2 
2 
(since min{a, b}  = (a + b -

x 

la  - bl)/2).  D 

The  last  property  relates  the  quality  of theoretical  Stoller  splits  to  the  Kol(cid:173)

mogorov-Smirnov  distance  sUPx  I FI (x)  - Fo(x) I between  the  class-conditional 
distribution functions.  As  a fun  exercise,  consider two classes  with means mo  = 
E{XIY  =  OJ,  ml  =  E{XIY  =  l},  and variances a5  =  Var{XIY  =  O}  and af  = 
Var{ X I Y  =  I}.  Then the following inequality holds. 
Theorem 4.1. 

REMARK. When p  =  1/2, Chernoff (1971) proved 

1 

L< 

- 2 + 2 (mO-ml)2 
(aO+al)2 

. 

42 

4.  Linear Discrimination 

Moreover,  Becker  (1968)  pointed  out  that  this  is  the  best  possible  bound  (see 
Problem 4.2).  0 

PROOF.  Assume without loss  of generality  that mo  <  mI. Clearly,  L  is  smaller 
than the probability of error for the rule that decides 0 when x  :::  mo + .6.0, and 1 
otherwise, where ml  - mo  = .6.0  + .6. 1, .6.0,  .6. 1 >  o. 

decide class 0 

decide class  1 

FIGURE 4.2.  The split providing the bound 
of Theorem 4.1. 

6. 1 

The probability of error of the latter rule is 

= 

< 

pP{X :::  ml  -

.6. 11Y = I} + (1  - p)P{X >  mo + .6.olY = o} 

0- 2 

(J2 

P  21.6.2 +(1-p)  20.6.2 
0 

0'1  + 

1 

0'0  + 

(by the Chebyshev-Cantelli inequality; see Appendix, Theorem A.17) 

p 

1- P 
- -+ - -
1 + Lli 
1 + Ll5 
(JJ 
a} 
(take .6. 1 = (ar/o-o).6. o, and .6.0  =  Im1  - molao/(ao + 0-1)  ) 

1 

We have yet another example of the principle that well-separated classes yield 
small values for L and thus L *. Separation is now measured in terms of the largeness 
of 1m 1  - mo I with respect to 0-0 + 0-1.  Another inequality in the same spirit is given 
in Problem 4.1. 

The limitations of theoretical Stoller splits are best shown in a simple example. 

Consider a uniform [0,  1] random variable X, and define 

Y= 

{ 

if 0 :::  X  :::  ~ + E 

1 
0  ifi+E<X:::~-E 
1 

if 3 -E:::X:::l 

for some small E  >  O.  As Y  is a function of X, we have L * = O. If we are forced to 
make a trivial X -independent decision, then the best we can do is to set g(x) ==  1. 

4.1  Univariate Discrimination and Stoller Splits 

43 

The probability of error is P{ 1 /3 + E <  X  <  2/3 - E}  =  1/3 - 2E.  Consider next 

a theoretical Stoller split.  One sees quickly that the best split occurs at x' = ° or 
x' = 1,  and thus that L  = 1/3 - 2E.  In other words, even the best theoretical split 
is  superfluous.  Note also that in the above example, mo  = ml  = 1/2 so that the 
inequality of Theorem 4.1  says L  :::::  I-it degenerates. 

We  now  consider what to do when a  split must be data-based.  Stoller (l954) 
suggests taking· (x', y') such that the empirical error is minimal. He finds  (x', y') 
such that 

, 

. 
(x  ,y) =  argrrun 

, 

1 ~( 
) 
- L...  I{x;SX,YdY} + I{x;>x,Ydl-y} 

. 

(x,Y)ERx{O,I}  n  i=l 

(x' and y' are now random variables, but in spite of our convention, we keep the 
lowercase notation for now.) We will call this Stoller's rule. The split is referred to 
as an empirical Stoller split. Denote the set {( -00, x] x {y}} U {(x, (0) x {I - y}} 
by C(x, y). Then 

(x', y') = argmin  vn(C(x, y), 

(x,y) 

where Vn  is the empirical measure for the data Dn  =  (Xl, YI ),  ... , (Xn, Yn), that 
is,  for  every  measurable  set  A  E  R  x  {O,  I},  vn(A)  = (l/n) 'L7=1  I{(xi,Yi)EA}. 
Denoting the measure of (X, Y)  in R  x  {O,  I}  by  v,  it is  clear that E{vn(C)}  = 
v(C) = P{X :::::  x, Y =I y} + P{X >  x, Y =11  - y}. Let Ln  = P{gn(X) =I YIDn} be 
the error probability of the splitting rule gn  with the data-dependent choice (x', y') 
given above, conditioned on the data. Then 

Ln 

v(C(x', y') 

=  v(C(x', y'»)  - vn(C(x', y')  + vn(C(x', y') 

< 

sup (v(C(x, y»  - vn(C(x, y») + vn(C(x*, y*» 
(x,y) 

(where (x*, y*) minimizes v(C(x, y)) over all (x, y) 

<  2 sup Iv(C(x, y»)  - vn(C(x, y))1  + v(C(x*, y*) 

(x,y) 

2 sup Iv(C(x, y»)  - vn(C(x, y»)1  + L. 

(x,y) 

From the next theorem we see that the supremum above is  small even for moder(cid:173)
ately large n, and therefore, Stoller's rule performs closely to the best split regard(cid:173)
less of the distribution of (X, Y). 

Theorem 4.2.  For Stoller's rule,  and E  >  0, 

and 

P{Ln - L  :::  E}  :::::  4e-nE2/2 , 

E{Ln - L} ::::: 

2Iog(4e) 

n 

44 

4.  Linear Discrimination 

PROOF.  By the inequality given just above the theorem, 

(x.y) 

<  P  {sup Iv(C(x, y»  - Vn(C(x,  y»1  :::  ~} 
<  p {S~p [v(C(x, 0)) - vn(C(x, 0»[  '" ~ I 
+ p  {S~p [v(C(x, 1»  - vn(C(x, 1»[  '" H 

2 

<  4e-2n(E/2)2 

by a double application of Massart's (1990) tightened version of the Dvoretzky(cid:173)
Kiefer-Wolfowitz inequality (1956) (Theorem 12.9). See Problem 4.5. We do not 
prove this inequality here, but we will thoroughly discuss several such inequalities 
in Chapter 12 in a greater generality. The second inequality follows from the first 
via Problem 12.1.  0 

The probability of error of Stoller's rule is uniformly close to L over all possible 
distributions. This is just a preview of things to come, as we may be able to obtain 
good performance guarantees within a limited class of rules. 

4.2  Linear Discriminants 

Rosenblatt's perceptron (Rosenblatt (1962); see Nilsson (1965) for a good discus(cid:173)
sion) is based upon a dichotomy of Rd into two parts by a hyperplane. The linear 
discrimination rule with weights ao, aI, ... , ad  is given by 

FIGURE 4.3.  A linear discriminant in R2 that cor(cid:173)
rectly classifies all but four data points . 

if Laix(i) +ao  >  0 

i=l 

otherwise, 

g(x) = L d 
where x  = (x O),  ... , xed)~. 
• 
• • • • • 
.e 
•  •  0 

0 

0 

. . .  

0 

4.2 Linear Discriminants 

45 

Its  probability of error is  for now  denoted by  L(a, ao),  where a  = (aI, ... , ad). 
Again, we set 

L  = 

inf 

aERd,aoER 

L(a, ao) 

for the best possible probability of error within this class. Let the class-conditional 
distribution functions of a I x(l) + .. '+ad Xed) be denoted by Fo,a and FI,a, depending 
upon whether Y  = 0 or Y  = 1.  For L(a, ao),  we may use the bounds of Lemma 
4.2, and apply them to  Fo,a  and FI,a.  Thus, 

L  = ~ - s~ps~p IPFI,a(X) - (l - p)Fo,a(x) - P + ~I, 

which, for p = 1/2, reduces to 
1 
2  a 

1 
2 

L = - - - sup sup I F1,a(x)  Fo,a(X)I. 

x 

Therefore,  L  = 1/2 if and only if p  = 1/2 and for all a,  FI,a  ==  Fo,a.  Then apply 
the following simple lemma. 

Lemma 4.3.  (CRAMER AND WOLD (1936)).  Xl and X 2,  random variables taking 
values in  R d, are  identically distributed if and only if aT Xl  and aT X2  have the 
same distribution for all vectors a  E  Rd. 

PROOF.  Two random variables have identical distributions if and only if they have 
the same characteristic function-see, for example, Lukacs and Laha (1964) . Now, 
the characteristic function of X I  = (X~l), ... , xid»)  is 

=  E {ei(aIX\l)+ .. +adX\d)} 

=  E { ei(aIX~I)+ .. +adX~d)  } 

(by assumption) 

the characteristic function of X 2 •  0 

Thus, we have proved the following: 
Theorem 4.3.  L  :::;  1/2 with equality if and only if L * = 1/2. 

Thus, as in the one-dimensional case, whenever L * <  1/2, a meaningful (L  < 
1/2) cut by  a  hyperplane  is  possible.  There  are  also  examples  in which  no  cut 
improves  over a rule in which g(x)  ==  y  for some  y  and all  x, yet  L *  =  0  and 
L  >  1/4 (say). To generalize Theorem 4.1, we offer the following result. A related 
inequality is  shown in Problem 4.7.  The idea of using Chebyshev's inequality to 
obtain  such bounds is  due to  Yau  and Lin (1968)  (see also Devijver and Kittler 
(1982, p.162)). 

46 

4.  Linear Discrimination 

Theorem 4.4.  Let Xo and X I  be random variables distributed as X given Y  = 0, 
and Y  = 1 respectively.  Set mo = E{Xo},  ml = E{Xd. Define also the covariance 
matrices ~l = E {(Xl  - ml)(X I  - mIl} and ~o = E {(Xo  - mo)(Xo - mol}. 
Then 

L*<L<  inf 

-

-

aE'Rd  1 

1 

+ 

(aT(ml-mo)i 

2 
(aT :Eoa)1/2+(aT :Ela)I/2) 

PROOF.  For any a  End we may apply Theorem 4.1 to aT Xo  and aT Xl. Theorem 
4.4 follows by noting that 

E {aT Xo}  = aTE{Xo} = aT mo, 
E{aTXI} =aTml, 

and that 

Var {aT Xo}  = E {aT (Xo  - mo)(Xo - mo)T a}  = aT~oa, 
Var{aTXI}  =aT~la. 0 

We  may  obtain explicit inequalities  by different  choices  of a.  a  = ml  - mo 
yields a convenient formula.  We  see from the next section that a  =  ~(ml - mo) 
with :E  = P:E1  + (1  - p )~o is also a meaningful choice (see also Problem 4.7). 

4.3  The Fisher Linear Discriminant 

Data-based values for a may be found by various criteria. One of the first methods 
was suggested by Fisher (1936). Let ml  and mo  be the sample means for the two 
classes (e.g., ml  =  Li:Yi=1  Xdl{i  : Yi  =  1}1-)  Picture projecting Xl, ... , Xn  to a 
line in the direction of a.  Note that this is perpendicular to the hyperplane given 
by a T x + ao  = O.  The projected values are a T Xl, ... , a T X n. These are all equal to 
o for those Xi  on the hyperplane aT x  = 0 through the origin, and grow in absolute 
value as we flee  that hyperplane. Let ~2 and 85 be the sample scatters for classes 
1 and 0, respectively, that is, 

---2  "( TX 
0'1  =  ~ a 

T--- )2 

i-a  ml  = aSIa 

T 

and similarly for ag,  where 

i:Yi=l 

Sl  = L (Xi  - md(Xi - mdT 

i:Yi=1 

is the scatter matrix for class 1. 

The Fisher linear discriminant is that linear function a T x for which the criterion 

4.4 The Normal Distribution 

47 

J(a) = 

(  T --
a  ml - a  mo 

T -- )2 
--2 
--2 
()l  + (70 

__)2 

(  T(--
a  ml  - mo) 
. aT(Sl + So)a 

is  maximum.  This corresponds to  finding  a direction a  that best separates aT rn 1 
from aT rno relative to the sample scatter. Luckily, to find that a, we need not resort 
to numerical iteration-the solution is given by 

Fisher's  suggestion  is  to  replace  (Xl, Y1), .•• , (Xn, Yn)  by  (aT Xl, Y1),  .•• , 
(aT X n ,  Yn )  and to perform one-dimensional discrimination. Usually, the rule uses 
a simple split 

(4.1) 

gao (x) = 

{

if aT x  + ao  >  0 

I 
0  otherwise 

for  some constant ao.  Unfortunately, Fisher discriminants can be arbitrarily bad: 
there are distributions such that even though the two classes are linearly separable 
(i.e., L = 0), the Fisher linear discriminant has an error probability close to 1 (see 
Problem 4.9). 

4.4  The Normal Distribution 

There  are  a few  situations  in  which,  by  sheer accident,  the Bayes rule  is  a  lin(cid:173)
ear discriminant.  While this  is  not a major issue, it is  interesting to  identify the 
most important case, i.e., that of the multivariate normal distribution. The general 
multivariate normal density is written as 

where  m  is  the mean (both x  and m  are d-component column vectors),  L;  is  the 
d  x  d  covariance matrix,  L;-l is the inverse of L;, and det(L;) is its determinant. 
'"  N(m,  L;).  Clearly,  if X  has  density  j, then  m  =  EX  and  L;  = 
We  write  j 
E{(X - m)(X - ml}. 

The multivariate normal density is  completely specified by d  + e) formal pa(cid:173)

rameters (m  and L;). A sample from the density is clustered in an elliptical cloud. 
The loci of points of constant density are ellipsoids described by 

for some constant r  ~ O.  The number" r  is the Mahalanobis distance from x  to m, 
and is in fact useful even when the underlying distribution is not normal. It takes 
into account the directional stretch of the space determined by L;. 

48 

4.  Linear Discrimination 

FIGURE 4.4.  Points  at  equal  Mahalanobis 
distance from m. 

Given a two-class problem in which X has a density (1  - p) !o(x) + p!I (x) and 
!o and !I are both multivariate normal with parameters mi, l:i, i  = 0,  1, the Bayes 
rule is easily described by 

g*(x) = {01 if P!l(X) >  (1  - p)!o(x) 

otherwise. 

Take logarithms and note that g*(x) =  1 if and only if 

(x  - mdTl:11(x  - ml) - 2logp +log(det(l:l» 

< 

(x  - mo)Tl:ol(x - mo) - 2log(1 - p) + log(det(l:o». 

In practice, one might wish to  estimate m 1, mo,  l: 1,  l:o  and P from the data and 
use these estimates in the formula for g*. Interestingly, as (x  mi f  l:i 1 (x - mi) 
is the squared Mahalanobis distance from x  to mi  in class i  (called rl), the Bayes 
rule is simply 

*(x) = {  1 
g 

0  otherwIse. 

ifrl <.r5 - 2log((1- p)/p)+log(det(l:o)/det(l:d) 

In particular, when p  = 1/2, l:o =  l:l  = l:, we have 

* X  = 
{ 
g  (  ) 

·f  2 
2 
1 
1  r 1  <  ro 
0  otherwise; 

just classify  according  to  the  class  whose  mean  is  at  the  nearest  Mahalanobis 
distance from x. When l:o = l: 1  = l:, the Bayes rule becomes linear: 

* (x) = {I  if a T x  :- ao  >  0 

0  otherwIse, 

g 

where a =  (ml  - mo)l:-l, and ao  = 2log(p /(1 - p» + mT; l:-lmo  mfl:-lml. 
Thus, linear discrimination rules  occur as  special cases of Bayes rules for multi(cid:173)
variate normal distributions. 

Our intuition  that  a  should be in the  direction  m 1  - mo  to  best separate  the 
classes is almost right.  Note nevertheless that a is not perpendicular in general to 
the hyperplane of loci at equal distance from mo  and mI. When l: is replaced by 

4.5 Empirical Risk Minimization 

49 

the standard data-based estimate, we obtain in fact the Fisher linear discriminant. 
Furthermore,  when  ~l  i  ~o, the  decision  boundary  is  usually  not  linear,  and 
Fisher's linear discriminant must therefore be suboptimal. 

In  early. statistical  work  on  discrimination,  the  normal 
HISTORICAL  REMARKS. 
distribution plays a central role (Anderson (1958». For a simple introduction, we 
refer to  Duda and Hart (1973).  McLachlan (1992) has more details,  and Raudys 
(1972;  1976)  relates  the  error,  dimensionality,  and  sample  size  for  normal  and 
nearly normal models. See also Raudys and Pikelis (1980;  1982).  0 

4.5  Empirical Risk Minimization 

In this section we present an algorithm that yields a classifier whose error probabil(cid:173)
ity is very close to the minimal error probability L  achievable by linear classifiers, 
provided that X has a density. The algorithm selects a classifier by minimizing the 
empirical error over finitely many-2C)-linear classifiers. For a rule 

</>(x) = { ~ 

if aT x  + ao  >  0 
otherwise, 

the probability of error is 

L(¢) = P{¢(X) i  Y}. 

L( ¢) may be estimated by the empirical risk 

that is, the number of errors made by the classifier ¢  is counted and normalized. 
Assume that X has a density, and consider d  arbitrary data points  XiI' X i2 ,  ... , 
Xid  among  {Xl,""  Xn},  and let aT x  + ao  =  0 be a hyperplane containing these 
points. Because of the density assumption, the d points are in general position with 
probability  one,  and this  hyperplane is  unique.  This  hyperplane  determines  two 
classifiers: 

if aT x  + ao  >  0 

0  otherwise, 

and 

</>1 (x) = { 1 
</>,(x) = I 1 

if aT x  + ao  <  0 

0  otherwise, 

whose  empirical  errors  Ln(¢l)  and  L n(¢2)  may  be  calculated.  To  each  d-tuple 
XiI' Xi2 ,  ... ,  Xid  of data  points,  we  may  assign  two  classifiers  in  this  manner, 
yielding altogether 2C) classifiers. Denote these classifiers by ¢l, ... , ¢2(1l).  Let 
~ 
¢  be a linear classifier that minimizes Ln(¢i) over all i  = 1, ... , 2(~). 

~ 

d 

50 

4.  Linear Discrimination 

We denote the best possible error probability by 

L  = inf L(fjJ) 

¢ 

over the class of all linear rules, and define fjJ*  = arg min¢ L( fjJ)  as  the best linear 
rule. If there are several classifiers with L(fjJ)  =  L, then we choose fjJ*  among these 
in an arbitrary fixed  manner. Next we show that the classifier corresponding to 1> 
is really very good. 

• 

0 

•  0 

• 

0 

0 

• 

• 
• 

• 
• 

• 
• 

FIGURE 4.5.  If the data points are in general 
position, thenfor each linear rule there exists 
a linear split defined by a hyperplane crossing 
d points such that the difference between the 
empirical errors is at most din . 

First note  that  there  is  no  linear classifier  fjJ  whose  empirical  error [;n (fjJ)  is 
smaller than [;(1)) - din. This follows from the fact that since the data points are 
in general position (recall the density assumption), then for each linear classifier 
we  may find  one whose defining hyperplane contains exactly d  data points such 
that the two decisions agree on all data points except possibly for these d points(cid:173)
see Figure 4.5. Thus, we  may view minimization of the empirical error over the 
finite set {fjJl,  ... , fjJ2C)}  as approximate minimization over the infinite set of linear 
classifiers. In Chapters 12 and 13 we will develop the full theory for rules that are 
found by empirical risk minimization. Theorem 4.5 just gives you a taste of things 
to come. Other-more involved, but also more general-proofs go back to Vapnik 
and Chervonenkis (1971;  197 4c). 

Theorem 4.5.  Assume that X has a density. Ij1> isfound by empirical error mini(cid:173)
mization as described above, then,jor all possible distributions oj(X, Y), ifn  ~ d 
and 2dln  ~ E  ~ 1, we have 

Moreover,  ifn  ~ d, then 

E {L(¢) - L} ::: 

2 
-(Cd + 1) log n + (2d + 2». 
n 

REMARK. With some care Theorem 4.5 and Theorem 4.6 below can be extended so 
that the density assumption may be dropped. One needs to ensure that the selected 

4.5 Empirical Risk Minimization 

51 

linear rule has empirical error close to  that of the best possible linear rule.  With 
the classifier suggested above this property may fail to hold if the data points are 
not  necessarily  of general position.  The  ideas  presented here  are generalized in 
Chapter 12 (see Theorem 12.2).  D 

PROOF.  We begin with the following simple inequality: 

L(J;) - L  =  L(J;)  Ln(¢) + Ln($) - L(¢*) 

(since Ln(¢) :::  Ln(¢) + din for any ¢) 

< 

max 

i=1, ... ,2G) 

(L(¢i)  Ln(¢J) + Ln(¢*) - L(¢*) + -. 

---

---

d 
n 

Therefore, by the union-of-events bound, we have 

P { L(¢) - L  >  E} 
:::  LP  L(¢i)-Ln(¢J>- +P  Ln(¢*)-L(¢*)+->-
dE} 
. 
n 
2 

E} 
2 

{ 

i=l 

2G) 

{ 

To  bound  the  second  term  on  the  right-hand  side,  observe  that  nLn(¢*)  is  bi(cid:173)
nomially distributed with parameters nand L( ¢*). By an inequality due to Chernoff 
(1952) and Okamoto (1958) for the tail of the binomial distribution, 

---

P  Ln(¢*) - L(¢*) >  2: 
E 

{

We  prove  this  inequality  later  (see  Theorem  8.1).  Next  we  bound  one  term  of 
the  sum on the right-hand side.  Note that by symmetry all 2(~) terms  are equal. 
Assume that the classifier ¢1  is determined by the d-tuple of the first d data points 
Xl,.'"  Xd. We write 

P {L(¢l)  L n(¢l)  >  ~} = E {p {L(¢l)  Ln(¢d >  ~ I Xl, ... , Xd} } , 

and bound the conditional probability inside. Let (X~, Y{'),  ... , (XJ, YJ) be inde-
pendent of the data and be distributed as the data (Xl, Yd, ... , (Xd, Yd). Define 

(X~  Y!) = {  (X;', Y/,) 
(Xi, Yi ) 

l'  I 

if i  :::  d 
ifi  >  d. 

Then 

P { L(¢l) - L n(¢l) >  ~ I Xl, ... , Xd} 
:::  P  {L(¢d - ~ t  I{¢l(Xi)=/yd  >  :.1  Xl, ... , Xd} 

n i=d+l 

2 

52 

4.  Linear Discrimination 

<  P  L(¢l) -

{ 

= 

P  L(¢l) -

{ 

- L I{(/JI(x')=jYI}  + - >  - Xl,· .. , Xd 

} 

1  n 
n  i=l 

EI 
2 

d 
n 

I 

I 

;:; Binomlal(n, L(¢l)) >  '2  -;:;  X I,  ... ,  Xd 
1 .  

E 

dj 

} 

(as L(¢l) depends upon Xl, ... , Xd  only and 

(X~, Y{) . .. , (X~, Y~) are independent of Xl, ... , Xd) 
_2n("-_!l)2 
e 

2 

II 

< 

(by Theorem 8.1; use the fact that E  2:  2d/n) 

The inequality for the expected value follows  from the probability inequality by 
the following simple argument: by the Cauchy-Schwarz inequality, 

(E {L(¢) - L})2 :::  E { (L(¢) _  L)2} . 

Denoting Z = (L(¢) - L)2, for any u  >  0, 

E{Z} 

E{ZIZ> u}P{Z >  u} +E{ZIZ::: u}P{Z::: u} 

<  P{Z >  u} + u 

<  e2d  (2nd  + 1) e-nu

/2 + u 

if  u  2: 

(

2d)2 
-;; 

by  the probability  inequality,  and since G)  :::  n d
.  Choosing  u  to  minimize the 
obtained expression  yields  the  desired inequality:  first  verify  that the minimum 
occurs for 

ne 
u =  -log-
2  ' 

2 
n 

where e = e2d (2nd + 1). Check that if n  2:  d, then u  ?:  (2d / n )2. Then note that the 
bound ee-nu / 2 + u equals 

2 
-log -
n 

nee 
2 

2 
n 

2 
n 

:::  -log (e2d+2nd+l)  =  -(Cd + 1) log n + (2d + 2)).  0 

Observe for now that the bound on P  {L(¢)  >  L  + E}  decreases rapidly with n. 
To have an impact, it must become less than 0 for smallo. This happens, roughly 
speaking, when 

n  ?:  e . ~ (lOg ~ + log ~) 

E2 

E2 

0 

for some constant e.  Doubling d,  the dimension, causes this minimal sample size 
to roughly double as well. 

4.5 EmpiIical Risk Minimization 

53 

An important special case is when the distribution is linearly separable, that is, 
L  = O.  In  such cases the empirical risk minimization above performs even better 
as  the size of the error improves to  0 (d log n In) from  0  ( J d log n In). Clearly, 
the data points are linearly separable as  well, that is,  L17(¢*)  = 0 with probability 
one, and therefore L17(;;;)  :::  din with probability one. 
Theorem 4.6.  Assume  that  X  has  a  density,  and that the  best linear  classifier 
has zero probability of error (L  =  0).  Then for the  empirical  risk minimization 
algorithm of Theorem 4.5, for all n  >  d and E  :::  1, 

P (L(~) >  < J ::0 2(: )e-I'-d)', 

and 

.-...} 

E  L(¢)  ::: 

{ 

dlogn + 2 
. 

n-d 

PROOF.  By the union bound, 

P{L(¢) >  <} 

::0  P t=1'2".,mff"(¢;)~~ L(</>i)  >  <} 
<  ~? {Ln(</>i)  ::0  ;;' L(</>i)  > <} , 

2G).-... 

d 

By symmetry, this sum equals 

2(:)P {£n(</>I)  ::0  ~, L(</>I)  >  <} 
=  2(:)E {p { £n(</>')  ::0  ~,L(</>I) >  <I XI""  Xd}} , 

where, as in Theorem 4.5, ¢l is determined by the data points Xl, ... , X d .  How(cid:173)
ever, 

P  {£n(</>,)::o  ~,L(</>I) >  <I XI,""  Xd} 

<  P {¢I(Xd+l ) = Yd+l , ... , ¢1(X17 ) = Y17 , L(¢l) >  EI  Xl, ... , Xd} 

(since all of the (at most d) errors committed by ¢l 
occur for (Xl, YI ),  ... , (Xd, Yd» 
(1  - E)17-d, 

< 

since the probability that no (Xi, Yi ),  pair i  = d + 1,  ... , n  falls in the set {(x, y) : 
<PI (x) =I y} is less than (1  - E )17-d  if the probability of the set is larger than E.  The 
x::: e-x . 
proof of the probability inequality may be completed by noting that 1 

54 

4.  Linear Discrimination 

For the expected error probability, note that for any u  >  0, 

P{L(¢) >  t}dt 

E{L(¢)}  100 
<  u + rx
u + 2n d 100 
(by the probability inequality and G)  ::::::  nd
u + _e-(n-d)u 

, P{L(¢) >  t}dt 

) 

< 

e-(n-d)t dt 

2nd 
n 

We choose u to minimize the obtained bound, which yields the desired inequality. 
o 

4.6  Minimizing Other Criteria 

Empirical risk minimization uses  extensive  computations,  because Ln (4))  is  not 
a unimodal function in general (see Problems 4.10 and 4.11).  Also,  gradient op(cid:173)
timization is  difficult because the gradients  are zero almost everywhere.  In fact, 
given n  labeled points  in n d ,  finding  the  best linear dichotomy  is  NP  hard  (see 
Johnson and Preparata (1978)). To  aid in the optimization,  some have suggested 
minimizing a modified empirical error, such as 

or 

--
L n (4))  = - ~ \II  (2Yi  - 1) - a  Xi  - ao 

T )  
, 

1~  ( 
n  i=l 

where  \II  is a positive convex function.  Of particular importance here is the mean 
square  error criterion  \II(u)  =  u2  (see,  e.g.,  Widrow  and Hoff (1960)).  One can 
easily  verify that Ln (4))  has  a gradient (with respect to  (a, ao))  that may  aid  in 
locating a local minimum. Let -;;; denote the linear discrimination rule minimizing 

over all a  and ao.  A description of the solution is given in Problem 4.14. 

Even in a one-dimensional situation, the mean square error criterion muddles 

the issue and does not give any performance guarantees: 

4.6 Minimizing Other Criteria 

55 

Theorem 4.7.  If sup(X,y)  denotes the  supremum with respect to all distributions 
on R  x  {O,  I},  then 

sup  (L(¢) - L) = 1, 

where 1) is a linear discriminant obtained by minimizing 

(X,Y) 

over all al and ao. 

REMARK. This theorem establishes the existence of distributions of (X, Y) for which 
L(1)  >  1 - E and L  <  E simultaneously for arbitrarily small E  >  0.  Therefore, 
minimizing the mean square error criterion is  not recommended unless  one has 
additional information regarding the distribution of (X, Y).  0 

PROOF. Let E  > ° and 8  >  0. Consider a triatomic distribution of (X, Y): 

P{(X, Y) = (-8, I)} = P{(X, Y) = (1,  I)} = E/2, 

P{(X, Y) = (0, O)}  = 1 - E. 

-8 
~.~--------------40 

0 

1 
• 

FIGURE 4.6.  A distribution for which 
squared error minimization fails. 

probability £/2 

probability 1-£  probability £/2 

For E  <  1/2, the best linear rule decides class ° on [-8/2, (0) and  1 elsewhere, 
for  a probability of error L  = E /2. The mean square error criterion asks that we 
mInImIZe 

----

L(¢)=  (l-E)(-l-v)  +"2(1-u-v)  +"2(1+u8-v) 

2  E2  E 

{ 

2} 

with respect to ao  =  v and a 1  =  u.  Setting the derivatives with respect to u  and v 
equal to zero yields 

(v  - 1)8  - v 

U= 

and 

V  = 2E  - 1 + "2 u (8  - 1), 

E 

for 

~8(8 - 1) 
v = - - - - - - - - -= - - - - - -

1 +82 - ~(1 

8)2 

If we let E  -!- ° and let 8  t  00,  then  v  '""  3E /2. Thus, for E small enough and 8 
large enough, considering the decision at ° only,  L(1)  2:::  1 - E, because at x  = 0, 
ux + v  = v  >  0.  Thus,  L(1)  - L  2:::  1 - 3E /2 for  E small enough and 8  large 
enough.  0 

56 

4.  Linear Discrimination 

Others have suggested minimizing 

n L (a(a T Xi + aO)  - Yi)2, 

i==l 

where a(u) is a sigmoid, that is, an increasing function from 0 to  1 such as  1/(1 + 
),  see, for example, Wassel and Sklansky (1972), Do Tu  and Installe (1975), 
e- U
Fritz and Gyorfi (1976), and Sklansky and Wassel (1979). Clearly, a(u) =  I{uc::o} 
provides the empirical en-or probability. However, the point here is to use smooth 
sigmoids so that gradient algorithms may be used to find the optimum. This may 
be viewed as a compromise between the mean squared en-or criteria and empirical 
en-or minimization. Here, too, anomalies can occur, and the en-or space is not well 
behaved, displaying many local minima (Hertz, Krogh, and Palmer (1991, p.1 08». 
See, however, Problems 4.16 and 4.17. 

Problems and Exercises 

PROBLEM 4.1.  With the  notation  of Theorem 4.1,  show  that the  error probability  L  of a 
one-dimensional theoretical Stoller split satisfies 

4p(1  - p) 

L~ -----------
)  (mo-mj)2 
- p  (l-p)(J6+p°-f 

1 +  (1 

p 

(Gyorfi  and Vajda (1980».  Is  this  bound better than that of Theorem 4.1?  HINT:  For any 
threshold rule gc(x) =  I{x::.c}  and u  >  0, write 

L(gJ 

PIX - c :::  0,  2Y - 1 = -I} + PIX - c  <  0, 2Y - 1 = I} 

<  P{lu(X - c) -

(2Y  - 1)1  :::  1} 

<  E {(u(X - c) -

(2Y -

I»2} 

by Chebyshev's inequality. Choose u  and c to minimize the upper bound. 
PROBLEM 4.2.  Let p  = 1/2. If L  is the error probability of the one-dimensional theoretical 
Stoller split, show that 

1 

L  <  -----;:(cid:173)
- 2 + 2 (mo-mj)2 
((Jo+(Jj}2 

Show that the bound is achieved for some distribution when the class-conditional distribu(cid:173)

tions  of X  (that is,  given  Y  = ° and  Y  = 1)  are  concentrated on two points each,  one of 

which is shared by both classes (Chernoff (1971), Becker (1968». 

PROBLEM 4.3.  Let X be a univariate random variable. The distribution functions for X given 

Y = 1 and Y = ° are F1  and Fo  respectively. Assume that the moment generating functions 
for  X  exist,  that is,  E {etXIY  = 1}  = o/l(t), E {etXIY = o}  = %(t), tEn, where 0/1,  0/0 
are finite for all t. In the spirit of Theorem 4.1, derive an upper bound for L  in function of 
0/1,  0/0' Apply your bound to the case that F1  and Fo  are both normal with possibly different 
means and variances. 

Problems and Exercises 

57 

PROBLEM 4.4.  SIGNALS  IN  ADDITNE  GAUSSIAN  NOISE.  Let So,  sl  E  Rd  be fixed,  and let N 
be a multivariate gaussian random variable with zero mean and covariance matrix  2:.  Let 
pry = O}  = pry = I} = 1/2, and define 

x = { ..  So  +N 
SI  +N 

if Y  = 0 
if Y = 1. 

Construct the Bayes decision and calculate L *.  Prove that if 2:  is the identity matrix,  and 
So  and SI  have constant components, then L * -+ 0 exponentially rapidly as d  -+  00. 

PROBLEM 4.5.  In the last step of the proof of Theorem 4.2, we used the Dvoretzky-Kiefer(cid:173)
Wolfowitz-Massart inequality (Theorem 12.9). This result states that if ZI, ... , Zn  are i.i.d. 
random variables on the real line with distribution function F(z) = P{ ZI  :::  z} and empirical 
distribution function Fn(z) = (l/n) L:;z=1  I{zi::ozj,  then 

Use this inequality to conclude that 

P {s~p Iv(C(x,  1»  - vn(C(x, 1)1  2: i} :::  2e- 2n

(E/2)2. 

HINT:  Map (X, Y)  on the real line by a one-to-one function  1jf  :  (R x  {O,  I})  -+  R  such 
that Z  = 1jf((X, Y»  <  0 if and only if Y = O.  Use the Dvoretzky-Kiefer-Wolfowitz-Massart 
inequality for Z. 

PROBLEM 4.6.  Let L  be the probability of error for the best sphere rule, that is, for the rule 
that associates a class with the inside of a sphere  Sx,n and the other class with the outside. 
Here the center x, and radius r are both variable. Show that L  = 1/2 if and only if L * = 1/2, 
and that L  :::  1/2. 

PROBLEM 4.7.  With the notation of Theorem 4.4, show that the probability of error L  of the 
best linear discriminant satisfies 

4p(l - p) 

L  :::  ---=------=~­
l+p(1-p)fl2 ' 

where 

fl =  J(rnl  - rno)T2:-I(rnl  - rno), 

is the Mahalanobis distance (Chapter 3) with 2:  = P2:1 +(l- p)2:o (Gyorfi and Vajda (1980». 
Interestingly,  the upper bound is just twice the bound of Theorem 3.4 for the asymptotic 
nearest neighbor error.  Thus,  a  large Mahalanobis  distance  does  not only imply that the 
Bayes error is  small, but also,  small error probabilities may be achieved by simple linear 
classifiers.  HINT:  Apply the inequality of Problem 4.1  for the univariate random variable 
X' = aT X a  = 2:-I(rnl  - rno). 

PROBLEM 4.8.  If rni  and a?  are the mean and variance of aT X, given that Y  = i, i  = 0,  1, 
where a is a column vector of weights, then show that the criterion 

58 

4.  Linear Discrimination 

is minimized for a  =  (~j + ~o)-l(Mj - Mo),  where Mi  and  ~i are the mean vector and 
covariance matrix of X, given Y  = i. Also, show that 

is  minimized  for  a  = (p~j + (1  - p)~o)-j(Mj - Mo),  where  p, 1 - p  are  the  class 
probabilities. This exercise shows that if discrimination is attempted in one dimension, we 
might consider projections  a T X  where  a  maximizes  the  weighted distance  between the 
projected means. 

PROBLEM 4.9.  In the Fisher linear discriminant rule (4.1) with free parameter ao, show that 
for any E  >  0, there exists a distribution for (X,  y), X  E n 2
}  <  00 
such that infao E{L(gao)}  >  1/2 - E.  Moreover,  if ao  is  chosen to minimize the squared 
error 

, with L = ° and E{IIXII 2

then E{L(gao)}  >  1 - E. 
PROBLEM 4.10.  Find a  distribution  of (X, Y)  with  X  E  n 2  such that with probability at 
least one half, Ln (</» 

is not unimodal with respect to the weight vector (a, ao). 

PROBLEM 4.11.  The following observation may help in developing a fast algorithm to find 
the best linear classifier in certain cases. Assume that the Bayes rule is a linear split cutting 
through the origin, that is,  L * = L(a*) for  some coefficient vector a*  E  n d, where L(a) 
denotes the error probability of the classifier 

if 2:.1=j  aix(i)  2: ° 

otherwise, 

and a  = (aj, ... , ad)'  Show that L(a) is unimodal as  a function  of a  End, and  L(a) is 
monotone increasing along rays pointing from a*, that is, for any 'A  E  (0,  1) and a  E  n d, 
L(a) - L('Aa + (1  - 'A)a*)  2: ° (Fritz and Gyorfi (1976». HINT:  Use the expression 

L(a) = 1/2 - J (~(x) - 1/2) sign (t aixU)) I"(dx) 

to show that L(a) - LC'Aa + (1  - 'A)a*) = fA  11](x)  - 1j21,u(dx) for some set A  end. 
PROBLEM 4.12.  Let a = (ao, aj) and 

a= arg min E {C(2Y  - 1) - ajX - ao)2 I(Ydga(Xi)d, 

a 

and ga(x) = I(alx+ao>O}'  Show that for every E  >  0, there exists a distribution of (X,  y) on 
n  x  {O,  I}  such that Leii') - L  2:  1 - E,  where L(a) is the error probability for gao  HINT: 
Argue as in the proof of Theorem 4.7. A distribution with four atoms suffices. 

PROBLEM 4.13.  Repeat the previous exercise for 

a  = argminE {I(2Y  - 1) - ajX - aol}. 

a 

Problems and Exercises 

59 

PROBLEM 4.14.  Let ¢* denote the linear discrimination rule that minimizes the mean square 
error E  {(2Y -
I - aT X - ao)2}  over all a and ao.  As this criterion is quadratic in (a, ao), 
it is unimodal. One usually approximates ¢* by ¢ by minimizing Li (2Yi - I - aT Xi  - ao)2 
over all a and ao.  Show that the minimal column vector (a, ao) is given by 

(~X;X;T) -1 (~(2Yi ~ l)X;), 

where X;  = (Xi,  1) is a (d + I)-dimensional column vector. 

PROBLEM 4.15.  The perceptron criterion is 

J  = 

L 

i:2Yi-1isign(aT Xi+aO) 

laTXi  +aol· 

Find a distribution for which L*  = 0,  L  :::  1/4, yet liminfn---,>ooE{Ln(¢)}  ~ 1/2, where ¢ 
is the linear discrimination rule obtained by using the a and ao  that minimize J. 
PROBLEM 4.16.  Let 0' be a monotone nondecreasing function on n satisfying limu---,>_oo  0' (u) 
= 0 and limu---,>oo  O'(u) = 1.  For h  >  0, define O'h(U)  = O'(hu).  Consider the linear discrimi(cid:173)
nation rule ¢  with a and ao  chosen to minimize 

t (O'h(a T 

i=l 

Xi  + ao)  - Yi)2  . 

For every fixed h  >  0 and 0  <  E  <  1, exhibit a distribution with L  <  E  and 

liminfE{Ln(¢)}  >  1 - E. 
n-+oo 

On the other hand, show that if h depends on the sample size n such that h -+ 00 as n  -+ 00, 
then for all distributions, E{Ln(¢)}  -+  L. 
PROBLEM 4.17.  Given  Y  = i, let  X  be  normal  with  mean  mi  and covariance  matrix  Li, 
i = 0, 1. Consider discrimination based upon the minimization of the criterion 

with respect to A, w, and c, a d  x  d matrix, d  x  1 vector and constant respectively, where 
O'(u)  =  1/(1 + e-U
)  is the standard sigmoid function.  Show that this is minimized for the 
same A, w, and c that minimize the probability of error 

and conclude that in this particular case, the squared error criterion may be used to obtain 
a Bayes-optimal classifier (Horne and Hush (1990)). 

5 
Nearest Neighbor Rules 

5.1 

Introduction 

Simple rules  survive.  The k-nearest neighbor rule,  since its  conception in  1951 
and 1952 (Fix and Hodges (1951;  1952;  1991a; 1991b», has thus attracted many 
followers  and continues to  be  studied by many researchers.  Formally,  we define 
the k -NN rule by 

gn(X) = { ~  if  L7=1  Wni I{Yi=ll  >  L7=1  Wni I{Yi=Ol 

otherwise, 

where  Wni  = 1/ k  if  Xi  is  among  the  k  nearest  neighbors  of x,  and  Wni  = 0 
elsewhere.  Xi  is said to be the k-th nearest neighbor of x  if the distance IIx - Xi II 
is the k-th smallest among Ilx - XIII, ... , Ilx - Xn II. In case of a distance tie, the 
candidate with the smaller index is  said to be closer to x. The decision is  based 
upon a majority vote. It is convenient to let k be odd, to avoid voting ties. Several 
issues are worth considering: 

(A)  Universal consistency. Establish convergence to the Bayes rule if k  -+  00 

and k / n  -+ 0 as n  -+  00. This is dealt with in Chapter 11. 

(B)  Finite  k  performance.  What happens  if we  hold  k  fixed  and let n  tend to 

infinity? 

(C)  The choice of the weight vector (Wn l,  ... ,  wnn ). Are equal weights for the 

k nearest neighbors better than unequal weights in some sense? 

62 

5.  Nearest Neighbor Rules 

(D)  The choice of a distance metric. Achieve invariance with respect to a certain 

family of transformations. 

(E)  The reduction of the data size. Can we obtain good performance when the 

data set is edited and/or reduced in size to lessen the storage load? 

FIGURE 5.1.  At every point the  decision 
is the label of the closest data point.  The 
set of points  whose  nearest neighbor is 
Xi  is  called the  Voronoi  cell of Xi.  The 
partition induced by the  Voronoi  cells is 
a  Varona! partition.  A  Voronoi partition 
of 15 random points is shown here. 

In the  first  couple of sections,  we  will be concerned with convergence issues 
for  k  nearest  neighbor rules  when  k  does  not  change  with  n.  In  particular,  we 
will  see that for  all  distributions,  the  expected error probability  E{L n }  tends  to 
a limit  LkNN  that is  in general close to  but larger than  L *.  The methodology for 
obtaining this result is interesting in its own right. The expression for LkNN  is then 
studied, and several key inequalities such as  LNN  :::::  2L * (Cover and Hart (1967» 
and  LkNN  :::::  L * (1 + .J2J k)  are proved and applied.  The other issues mentioned 
above are dealt with in the remaining sections. For surveys of various aspects of 
the nearest neighbor or related methods,  see Dasarathy (1991), Devijver (1980), 
or Devroye and Wagner (1982). 

REMARK.  COMPUTATIONAL  CONCERNS.  Storing  the  n  data pairs  in an  array  and 
searching for the  k  nearest neighbors may take time proportional to nkd if done 
in a naive  manner-the "d"  accounts  for  the cost of one distance  computation. 
This  complexity  may  be  reduced  in  terms  of one  or  more  of the  three  factors 
involved.  Typically,  with k  and d  fixed,  O(n lid) worst-case time (Papadimitriou 
and Bentley (1980»  and  0 (log n) expected time (Friedman, Bentley, and Finkel 
(1977»  may be achieved.  Multidimensional search trees  that partition the  space 
and guide the search are invaluable-for this approach, see Fukunaga and N arendra 
(1975), Friedman, Bentley, and Finkel (1977), Niemann and Goppert (1988), Kim 
and Park (1986),  and  Broder (1990).  We  refer to  a  survey  in Dasarathy  (1991) 
for more references. Other approaches are described by Yunck (1976), Friedman, 
Baskett, and Shustek (1975), Vidal (1986), Sethi (1981), and Farag6, Linder, and 
Lugosi (1993).  Generally,  with preprocessing,  one may considerably reduce the 
overall complexity in terms of nand d.  0 

5.2 Notation and Simple Asymptotics 

63 

5.2  Notation and Simple Asymptotics 

We fix x  E  Rd,andreorderthedata(X 1,  Yd,  ... , (Xn ,  Yn)accordingtoincreasing 
values of II  Xi  - X II.  The reordered data sequence is denoted by 

if no confusion is possible.  X(k)(X) is the k-th nearest neighbor of x. 

REMARK.  We  note here that,  rather arbitrarily,  we defined neighbors  in terms  of 
y II.  Surprisingly, the asymptotic properties derived in 
the Euclidean distance Ilx 
this chapter remain valid to a wide variety of metrics-the asymptotic probability 
of error is independent of the distance measure (see Problem 5.1).  0 

Denote  the  probability  measure  for  X  by  fJ."  and  let  SX,E  be  the  closed  ball 
centered at x  of radius E >  O. The collection of all x  with fJ.,( Sx ,E)  >  0 for all E >  0 
is called the support of X or fJ.,.  This set plays a key role because of the following 
property. 
Lemma 5.1.  If x  E  support(fJ.,)  and limn-+oo kin  = 0,  then  IIX(k)(x)  - x II  ~ 0 
with probability one.  If X  is independent of the data and has probability measure 
/-L,  then  IIX(k)(X)  - XII  ~ 0 with probability one whenever kin  ~ O. 

PROOF.  Take  E  >  O.  By  definition,  x  E  support(fJ.,)  implies  that  fJ.,(SX,E)  >  O. 
Observe that IIX(k)(x) - x II  >  E if and only if 

1  n 
- LI{xES  } <  -. 
n  i=l 

k 
n 

I 

X,E 

By the strong law of large numbers, the left-hand side converges to  fJ.,(SX,E)  >  0 
with  probability  one,  while,  by  assumption,  the  right-hand  side  tends  to  zero. 
Therefore,  IIX(k)(x)  - xii  ~ 0 with probability one. 

The second statement follows  from  the previous  argument as  well.  First note 
that by Lemma A.1  in the Appendix, P{X  E  support(fJ.,)}  =  1, therefore for every 
E  >  0, 

P {IIX(k)(X) - XII  >  E} 

=  E  {I{XESUPport(iL)}P {IIX(k)(X) - XII> EIX  E  support(fJ.,)}}, 

which converges to zero by the dominated convergence theorem, proving conver(cid:173)
gence in probability. If k  does not change with n, then II  X(k)(X) - X II  is monotone 
decreasing  for  n  ~ k;  therefore,  it  converges  with  probability  one  as  well.  If 
k  =  kn  is  allowed to  grow  with n  such that kin  ~ 0,  then using  the notation 
X(kn,n)(X)  = X(klX), we see by a similar argument that the sequence of monotone 
decreasing random variables 

sup IIX(km,m)(X)  - XII  ~ IIX(kn,n)(X)  - XII 

m~n 

64 

5.  Nearest Neighbor Rules 

converges to zero in probability, and therefore, with probability one as well. This 
completes the proof.  0 

Because  1]  is  measurable  (and  thus  well-behaved  in  a  general  sense)  and 
IIX(k)(X)  - xII  is  small,  the  values  1](X(i)(x»  should  be  close  to  1](x)  for  all  i 
small enough. We  now introduce  a proof method that exploits this  fact,  and will 
make subsequent analyses very simple-it suffices to look at data samples in a new 
way via embedding. The basic idea is  to define  an  auxiliary rule g:l (x) in which 
the  Y(i)(x)'s  are  replaced  by  k  i.i.d.  Bernoulli random  variables  with  parameter 
1](x)-locaUy, the Y(i)(x)'s behave in such a way.  It is easy to show that the error 
probabilities of the two rules are close, and analyzing the behavior of the auxiliary 
rule is  much more convenient. 

To  make  things  more  precise,  we  assume  that  we  are  given  i.i.d.  data  pairs 
(Xl, Ud, ... , (XII' Un),  all  distributed as  (X, U), where  X  is  as  before (and has 
probability measure f.L  on the Borel sets of R d
), and U is uniformly distributed on 
[0,1] and independent of X. If we set Y i  = l{Ui~TJ(xi)}' then (Xl, Y I ),  ••. ,  (Xn ,  Y n ) 
are i.i.d. and distributed as the prototype pair (X, Y). So why bother with the Ui's? 
In  embedding  arguments,  we  will  use  the  same  Ui's to  construct a  second data 
sequence that is heavily correlated (coupled) with the original data sequence, and 
is more convenient to analyze. For example, for fixed x  E  R d

,  we may define 

Y/(x) =  l{ui~ry(x)l' 

We  now  have  an  i.i.d.  sequence  with  i-th vector given by  Xi, Yi ,  Y:(x), Ui. Re(cid:173)
ordering the  data sequence according to  increasing values of IIXi  - x II  yields a 
new sequence with the  i-th vector denoted  by  X(i)(x),  Y(i)(x),  Y(i)(x),  U(i)(x). If 
no confusion is possible, the argument x  will be dropped. A  rule is called k-local 
if for n  ::::  k, gn  is of the form 

x) = {I  if ljr(x, Y(l)(x), ... , Y(k)(X»  >  0, 

° otherwise, 

(
gil 

(5.1) 

for some function 1/1.  For the k-NN rule, we have, for example, 

In  other words,  gn  takes  a  majority  vote  over the  k  nearest neighbors  of x  and 
breaks ties in favor of class 0. 

To  study gn  turns out to be almost equivalent to studying the approximate rule 

g" n' 

ifljr(x, Y(l)(x), ... , Y(k)(X»  > ° 

'(x) = {  I 
gil 

° otherwise. 

The latter rule is of no practical value because it requires the knowledge of 1] (x ). 
Interestingly however, it is easier to study, as Y(l)(x), ... , Y(k)(x) are i.i.d., whereas 
Y(l)(x), ... , Y(k) (x ) are not. Note, in particular, the following: 

Lemma 5.2.  For all x,  n  ~ k, 

5.2 Notation and Simple Asymptotics 

65 

P {ljJ(x,  Y(l)(x),  ... , Y(k) (x )) =/ljJ(x, YCl)(x),  ... , YCk) (x ))} 
:s;  LE {11J(x)  - 1J(X(i)(x))I} 

k 

i=l 

and 

P{gn(x) =/ g~(x)} :s;  LE {11J(x)  -1J(X(i)(x))I}. 

k 

i=l 

PROOF.  Both statements follow directly from the observation that 

{ljJ(X,  YCl)(x)"", Y(k)(X))  =/ljJ(x, YCl)(x)"", YCk)(x))} 

{(Y(1)(x)" 

.. , Y(k) (x )) =/ (YCl) (x ), ... , YCk) (x ))} 

c 
C  U {1J(X(i)(x)):s;  U(i)(x):s;  1J(x)}  U U h(x):s; UCi)(x):s;  1J(X(i)(x))}, 

k 

k 

i=l 

i=l 

and using the union bound and the fact that the U(i)(x)'s are uniform [0,  1].  0 

We need the following result, in which X is distributed as  Xl, but independent 

of the data sequence: 

Lemma 5.3.  (STONE  (1977)).  For any  integrable function  f,  any  n,  and any 
k :s;  n, 

k LE {If(X(i)(X))I}  :s;  kYdE{lf(X)I}' 

i=l 

,J3)d - 1 depends upon the dimension only. 

where  Yd  :s;  (1 + 2//2 -
The proof of this lemma is beautiful but a bit technical-it is given in a separate 
section. Here is how it is applied, and why, for fixed k, we may think of f(X(k)(X)) 
as  f(X) for all practical purposes. 

Lemma 5.4.  For any integrable function f, 

1  k -LE {If(X) -

k  i=l 

f(X(i)(X))I}  -+ 0 

as n  -+  00 whenever kin -+ O. 

PROOF.  Given  E  >  0,  find  a  uniformly  continuous  function  g  vanishing  off a 
boundedsetA,suchthatE{lg(X)- f(X)I}  <  E (see Theorem A.8 in the Appendix). 

66 

5.  Nearest Neighbor Rules 

ThenforeachE  >  O,thereisao  > ° such that Ilx-zll  <  o implies Ig(x)-g(z)1  < 

E.  Thus, 

f(X(i)(X»I} 

1  k -LE {If(X) -
k  i=l 
:::  E {If(X) - g(X)1} + k L E {lg(X) - g(X(i)(X»I } 

1  k 

i=l 

1  k 

k  i=l 

f(X(i)(X»I} 

+ - L E {lg(X(i)(X»  -
(1 + Yd)E {If(X)  g(X)1} + E + IIgiiooP {IIX - X(k)(X) II  >  o} 
(by Lemma 5.3, where 0 depends on E only) 
(2 + Yd)E  + 0(1) 

(by Lemma 5.1).  0 

::: 

::: 

5.3  Proof of Stone's Lemma 

In this section we prove Lemma 5.3. For e E  (0, nI2), a cone C(x, e) is the col(cid:173)
lection of all y  E  nd  for which angle(x, y)  :::  e. Equivalently, in vector notation, 
x T yi/ix/illyi/  2:  cose. The set z + C(x, e) is the translation of C(x, e) by z. 

FIGURE 5.2.  A cone of angle e. 

c(x.e) 

Ify,z E  C(x,nI6),andllyll  <  IIzll,thenIlY-zll  <  Ilzll,aswewillnowshow. 

Indeed, 

::: 

= 

II y 112  + liz 112  -

211 y IlIlz II  cos(n /3) 

112 (1 +  IIyll2  -~) 

II 

Z 

IIzl12 

Ilzll 

(see Figure 5.3). 

5.3 Proof of Stone's Lemma 

67 

FIGURE 5.3.  The  key  geometrical  prop(cid:173)
erty of cones of angle  <  n /2. 

The following covering lemma is needed in what follows: 

Lemma 5.5.  Lete  E  (O,n/2)bejixed. Then there exists a set {Xl,  ... ,xyJ end 
such that 

Yd 

nd = U C(Xi, e). 

i=l 

Furthermore,  it is always possible to take 

l)d 

( 

Yd::::  1 + sinCe /2) 

- 1. 

For e  = n /6,  we have 

Yd  ::::  (1 +  2  )d _ 1. 

)2 - v'3 

PROOF.  We assume without loss of generality that  IIxi II  =  1 for all i. Each Xi  is the 
center of a sphere Si  of radius r  = 2 sinCe /2).  Si  has the property that 

{x:  IIxll  = l}nSi = {x:  IIxll  = l}nC(xi,e). 

Let us only look at Xi 's such that IIxi -x j"  :::  r for all j  i  i. In that case, U C(Xi' e) 
covers nd if and only if U Si  covers {x  : IIx II  = I}. Then the spheres S;  of radius 
r/2 centered at the xi's are disjoint and U S;  S;  SO,1+rj2  -
SO,rj2  (see Figure 5.4). 

68 

5.  Nearest Neighbor Rules 

FIGURE 5.4.  Bounding Yd. 

Thus, if Vd  = volume(SO,l), 

or 

YdVd(~)d ~ Vd(l+~)d _Vd(~)d 
Yd  ~ (1 + ~)d _ 1 = (1 +  .  1 )d _ l. 

sm(e /2) 

r 

The last inequality follows from the fact that 

sin ~ = /1- cos(,,/6) =)2 - v'3.  D 

12  V 

2 

4 

FIGURE 5.5.  Covering the 
space by cones. 

With  the  preliminary  results  out  of the  way,  we  cover nd  by  Yd  cones  X  + 
C(xj,1[/6),  1  ~  j  ~ Yd,  and  mark  in  each  cone  the  Xi  that  is  nearest  to 

5.4 The Asymptotic Probability of Error 

69 

X,  if such  an  Xi  exists.  If Xi  belongs  to  X  + C(Xj, 7Tj6)  and  is  not  marked, 
then X cannot be the nearest neighbor of Xi in {Xl, ... , Xi-I, X,  Xi+l, ... , X n }. 
Similarly,  we  might  mark  all  k  nearest  neighbors  of X  in  each  cone  (if there 
are  less  than  k points  in  a  cone,  mark  all  of them).  By  a  similar  argument,  if 
Xi  E  X + C (x j, 7T j 6) is not marked, then X cannot be among the k nearest neigh(cid:173)
bors of Xi  in {X 1,  ... ,  Xi -1, X,  Xi + 1,  ... ,  X n}.  (The order of this set of points is 
important if distance ties  occur with positive probability,  and they are broken by 
comparing indices.) Therefore, if !  is a nonnegative function, 

k 
LE {!(X(i)(X))} 
i=l 

1=1 

E {t I{xi  is among the k nearest neighbors of  Xin  {Xj, ... ,Xn}}!(Xi)} 
E {f(X) t 
<  E { f(X) t I{K, i, "",ked} } 

I{x is among the knearest neighbors of  Xi  in  {Xj"",Xi-j,X,Xi+j, ... ,Xn}}} 
(by exchanging X and Xi) 

<  kYdE{!(X)}, 

as we can mark at most k nodes in each cone, and the number of cones is at most 
Yd-see Lemma 5.5. This concludes the proof of Stone's lemma.  0 

5.4  The Asymptotic Probability of Error 

We  return  to  k-Iocal  rules  (and  in  particular,  to  k-nearest  neighbor  rules).  Let 
D~ = ((X I,  YI ,  VI), ... , (Xn,  Yn, Vn)) be the i.i.d. data augmented by the uniform 
random variables VI, ... , Vn  as described earlier. For a decision gn  based on Dn, 
we have the probability of error 

Ln  =  P{gn(X) i  YID~} 

=  P {sign (1/I(X,  Y(l)(X),  ... , Y(k)(X)))  i  sign(2Y - 1)ID;I} ' 

where 1/1  is the function whose sign determines gn~ see (5.1). Define the random 
variables  YCl)(X),  ... , YCk)(X)  as we did earlier, and set 

L~ = P {sign (1/I(X,  YCl)(X)"", YCk)(X)))  isign(2Y -1)ID~}. 

70 

5.  Nearest Neighbor Rules 

By Lemmas 5.2 and 5.4, 

E{ILn  - L;zl} 

<  P {1jJ(X, Y(l)(X), ... , Y(k)(X)) f  1jJ(X,  YCl)(X),  ... , YCk ) (X)) } 
<  L E  {11](X)  - 1](X(i)(X))I} 

k 

i=l 

0(1). 

Because limn---+oo(EL;z  - ELn) = 0, we need only study the rule g:z 

'(X)={  1 
gn 

0  otherwise 

if1jJ(x,Zl, ... ,Zk»O 

(Z}, ... , Zk are i.i.d. Bernoulli (1](x))) unless we are concerned with the closeness 
of Ln to ELn  as well. 

We now illustrate this important time-saving device on the  I-nearest neighbor 

rule. Clearly, 1jJ(x,  ZI) = 2Z1  - 1, and therefore 

E{L~} =  P{ZJ  f  Y}  =  E {21](X)(1  - 1](X))} . 

We have, without further work: 

Theorem 5.1.  For the nearest neighbor rule,jor any distribution oj(X, Y), 

lim  E{L n} = E {21](X)(1  - 1](X))} = L NN . 
n---+oo 

Under various continuity conditions (X has a density  j  and both  j  and 1]  are 
almost everywhere  continuous);  this  result is  due to Cover and  Hart (1967).  In 
the  present generality,  it essentially  appears  in  Stone  (1977).  See also  Devroye 
(198Ic). Elsewhere (Chapter 3), we show that 

L* :5  LNN  ::::  2L*(1- L*):::: 2L*. 

Hence, the previous result says that the nearest neighbor rule is asymptotically at 
most twice as bad as the Bayes rule-especially for small L *, this property should 
be useful. 

We formally define the quantity,  when k is odd, 

LkNN 

We have the following result: 

Theorem 5.2.  Let k be odd andfixed. Then,jor the k-NN rule, 

lim  E{L n} =  L kNN . 
n---+oo 

5.5 The Asymptotic Error Probability of Weighted Nearest Neighbor Rules 

71 

PROOF.  We note that it suffices to show that limn~oo E{L~} = LkNN  (in the previ(cid:173)
ously introduced notation). But for every n, 

E{L~} 

~  P {Z1  + ... + Zk  >  ~, y ~ o} + P {Z1  + ... + Zk  <  ~, y ~ I} 
~  P {Z1  + ... + Zk  >  ~, Zo ~ o} + P {Z1  + ... + Zk  <  ~, Zo = I} 

(where Zo,  ... , Zk  are i.i.d. Bernoulli (1J(X»  random variables), 

which leads directly to the sought result.  0 

Several representations of LkNN  will be useful for later analysis. For example, 

we have 

LkNN 
=  E { ry(X)P { Binomial(k, ~(X»  <  ~ I X}} 

+ E {(l - ry(X))P { Binomial(k, ry(X»  >  ~ I X} } 

E {min(1J(X),  1 - 1J(X»} 

+ E  { (1 - 2rnin(~(X), 1 - ry(X»)p { Binomial(k, ry(X»  >  ~ I X} } . 

It should be stressed that the limit result in Theorem 5.2 is distribution-free. The 
limit LkNN  depends  upon  1J(X)  (or min(1J(X),  1 - 1J(X») only.  The continuity or 
lack of smoothness of 1J  is  immaterial-it only matters for the speed with which 
E{L n }  approaches the limit L kNN . 

5.5  The Asymptotic Error Probability of 

Weighted Nearest Neighbor Rules 

Following Royall (1966), a weighted nearest neighbor rule with weights  WI,  ... , 
Wk  makes a decision according to 

gn(x) = 

{ 

if  ~~ 

1 
o  otherWIse. 

L...-l:~Ci)(x)=I 

w·  >  ~~ 

l 

L...-l:YCi)(X)=o 

W· 
l 

In case of a voting tie, this rule is not symnletric. We may modify it so that gn (x) d;t 
-1 if we have a voting tie.  The "-1" should be considered as  an indecision.  By 

72 

5.  Nearest Neighbor Rules 

previous arguments the asymptotic probability of error is a function of WI,  ... ,  Wk 
given by L(Wl, .. "  Wk) = E{a(1J(X»}, where 

a(p)  =  P {t WiY/> t w;(l - Y;)} (1- p) 
+P {t Wi Y/ ::: t w;(l- Y/)}  p, 

where now Y{,  ... , Y~ are i.i.d. Bernoulli (p). Equivalently, with Zi  = 2Y! - 1 E 
{-1,1}, 

a(p) = (1  - p)P {t WiZi  > o}  + pP {t WiZi  ::: o} . 

Assume that P  {L~=l Wi Zi  = o}  = 0 for now.  Then, if p  <  1/2, 
a(p) = p + (l - 2p)P {t WiZi  > o}. 

and an antisymmetric expression is valid when p  >  1/2. Note next the following. 
IfweletNz be the number of vectors Z = (Zl,""  Zk)  E  {-I, l}k with L I{zi=l}  = I 
and L WiZi  >  0, then Nz + Nk-z  = e). Thus, 

k 

=  L Nz/(1 - p)k-Z 
=  L  (k)pk-Z(1_ pi + L Nz (/(1 - pl-l - pk-I(1_ pi) 

z=o 

l<kj2 

I 

l<kj2 

1 (  k)  kj2 

+ 2  k/2  P 
I  + I I  + I I I. 

(1  - p) 

kj2 

I{k even} 

= 

Note that I + I I I  does not depend on the vector of weights, and represents 

P{Binomial(k,l - p)  :'S  k/2} = P{Binomial(k, p) ~ k/2}. 

Finally, since p  :'S  1/2, 

I I  =  L Nz  (/(1 - pl-z - pk-l(1  pi) 
L  Nzpl(1  - pi ((1- pl-21 - pk-21) 

l<kj2 

l<kj2 

~  o. 

5.5 The Asymptotic Error Probability of Weighted Nearest Neighbor Rules 

73 

This term is zero if and only if Nz = 0 for alII  <  k12. In other words, it vanishes 
if and only if no numerical minority of Wi'S  can sum to a majority (as in the case 
(0.7,0.2,0.1), where 0.7 alone, a numerical minority, outweighs the others). But 
such cases are equivalent to ordinaryk-nearest neighbor rules if k is odd. When k 
is even, and we add a tiny weight to one Wi,  as in 

1 + E,  1 - E/(k - 1), ... ,  1 - E/(k - 1») , 

k 

k 

k 

( 

for small E  >  0, then no numerical minority can win either, and we have an optimal 
rule (l I  = 0). We have thus shown the following: 

Theorem 5.3.  (BAILEY AND  JAIN  (1978».  Let L(WI, ... , Wk)  be the asymptotic 
probability  of error of the  weighted k-NN  rule  with weights  WI,  ... ,  Wk.  Let the 
k-NN  rule be defined by (11 k,  II k, ... , II k) if k  is odd,  and by 

(11 k, II k, ... , II k) + E(1,  -1/(k - 1),  -1/(k - 1), ... , -1/(k - 1» 

for 0 <  E  <  1 I k when k  is even.  Denoting the asymptotic probability of error by 
LkNN for the latter rule,  we have 

if P{17(X)  =  1/2}  <  1,  then equality occurs ifand only if every numerical minority 
of the Wi'S  carries less than half of the total weight. 

The result states that standard k-nearest neighbor rules are to be preferred in an 
asymptotic sense. This does not mean that for a particular sample size, one should 
steer clear of nonuniform  weights.  In  fact,  if k  is  allowed  to  vary  with  n,  then 
nonuniform weights are advantageous (Royall (1966». 

Consider the space Wofall weight vectors (WI,  ... ,  Wk) with Wi  ~ 0, L~=l Wi  = 
1. Is it totally ordered with respect to  L(WI, ... , Wk)  or not? To answer this ques(cid:173)
tion,  we must return to  a(p) once again.  The weight vector only influences the 
term I I  given there. Consider, for example, the weight vectors 

(0.3,0.22,0.13,0.12,0.071,0.071,0.071,0.017) 

and 

(0.26,0.26,0.13,0.12,0.071,0.071,0.071,0.017). 

Numerical  minorities  are  made  up  of one,  two,  or three  components.  For both 
weight vectors,  NI  = 0,  N2  = 1.  However,  N3  = 6 + 4 in the former case,  and 
N3  = 6 + 2 in the latter. Thus, the "I I" term is uniformly smaller over all p  <  1/2 
in the latter case, and we see that for all distributions, the second weight vector is 
better. When the Nt's are not strictly nested, such a universal comparison becomes 
impossible, as in the example of Problem 5.8. Hence, W  is only partially ordered. 

Unwittingly, we have also shown the following theorem: 

Theorem 5.4.  For all distributions, 

L * ::::;  ... ::::;  L C2k+I)NN  ::::;  L C2k-l)NN  ::::;  ... ::::;  L3NN  ::::;  LNN  ::::;  2L *. 

74 

5.  Nearest Neighbor Rules 

PROOF.  It suffices  once again to  look at a(p). Consider the  weight vector  WI  == 
... = W2k+l  = 1 (ignoring normalization)  as  for the  (2k + I)-NN rule.  The term 
I I  is  zero,  as  No  = Nl  = .. , = Nk  ==  0.  However,  the  (2k  -
l)-NN rule  with 
vector WI  = ... = W2k-l  = 1,  W2k  = W2k+l  = 0,  has  a nonzero term I I, because 
No  = ... = Nk- 1 = 0, yet Nk = ek;l)  >  0. Hence, L C2k+1)NN  ~ L C2k-I)NN'  0 

REMARK.  We  have  strict  inequality  L(2k+l)NN  <  L(2k-l)NN  whenever  P{1J(X)  rj. 

{a,  1,  1/2}}  >  0. When L * = 0, we have LNN  = L3NN  = LSNN  = ... = ° as well.  0 

5.6  k-Nearest Neighbor Rules: Even k 

Until now we assumed throughout that k was odd, so that voting ties were avoided. 
The tie-breaking procedure we follow forthe 2k-nearest neighbor rule is as follows: 

gn(x) =  ° 

I 

{

YCl)(x) 

if  :L;:l YCi)(x)  >  k 
if  :L;:1 Y(i)(x)  <  k 
if  :L;:1 YCi)(x)  = k. 

Formally,  this  is  equivalent to a  weighted 2k-nearest neighbor rule  with weight 
vector (3, 2, 2, 2,  ... , 2, 2).  It is  easy to check from Theorem 5.3  that this is the 
asymptotically best weight vector. Even values do not decrease the probability of 
error. In particular, we have the following: 

Theorem 5.5.  (DEVIJVER  (1978)).  For all distributions,  and all integers k, 

L(2k-l)NN  = L C2k)NN' 

PROOF.  Recall that LkNN  may be written in the form LkNN  = E{a(1J(X))}, where 

a(1J(x)) =  lim  P  {g~k)(X) =I YIX = x} 

n---+oo 

is the pointwise asymptotic error probability of the k-NN rule g~k) . It is convenient to 
consider Zl, ... , Z2k i.i.d. {-I, l}-valuedrandom variables withP{Zi  =  I} = p  = 
1J(x), and to base the decision upon the sign of :L;:1 Zi. From the general formula 

for weighted nearest neighbor rules, the pointwise asymptotic error probability of 
the (2k )-NN rule is 

n---+oo 

n 

lim  P  {gC2k)(X) =I YIX = x} 
~  PP{tZi<O}+PP{tZi=O,ZI<O} 

+(1- p)P  ~Zi > ° +(1- p)P  ~Zi =0, ZI  > ° 

{2k 

2k 

} 

} 

{

5.7 Inequalities for the Probability of Error 

75 

= pP {t,Zi <O}  +(1- p)P {t,Zi > o} 

lim  P  {g(2k-l)(X) =i YjX= x} . 

n----+oo 

n 

~ 

Therefore,  L(2k)NN  = L(2k-l)NN'  0 

5.7 

Inequalities for the Probability of Error 

We return to the case when k is odd. Recall that 

where 

"'k(P) = rnin(p, 1 - p) + 12p  -

lIP {BinOrnia1(k, rnin(p, 1 - p)) >  ~} , 

Since L * = E {min(1J(X),  1 - 1J(X))}, we may exploit this representation to obtain 
a variety of inequalities on LkNN - L  *. We begin with one that is very easy to prove 
but perhaps not the strongest. 

Theorem 5.6.  For all odd k and all distributions, 

LkNN  :s  L  +  ~. 

* 

1 
'\Ike 

PROOF.  By the above representation, 

< 

:s 

= 

= 

sup  (l - 2 p)P {B  >  ~} 
2 

05p51P 
(B is Binomial (k, p)) 
sup  (l _  2p)P {B - kp  >  ~ _  p} 

05p51/2 

k 

2 

sup  (l - 2p)e-2k(l/2- p )2 

05P5 1/2 
(by the Okamoto-Hoeffding inequality-Theorem 8.1) 
sup  ue- ku2
05u51 

/ 2 

1 

o 
~. 

76 

5.  Nearest Neighbor Rules 

Theorem 5.7.  (GYORFI AND  GYORFI (1978)).  For all distributions and all odd k, 

PROOF.  We note that for p  ::::;  1/2, with B  binomial (k,  p), 

< 

E {IB  - kpl} 
k(1/2 - p) 

JVar{B} 
k(1/2 - p) 

(Markov's inequality) 

(Cauchy-Schwarz inequality) 

= 

2Jp(1- p) 
Jk(1 - 2p)' 

Hence, 

LkNN  - L' 

:::  E {~v'ry(X)(l - ry(X))} 

2 
JkJE{17(X)(1 - 17(X))} 

< 

(Jensen's inequality) 

=  ~JL; 
=  J2L NN 

0 

k 

. 

REMARK. For large k,  B  is  approximately normal (k,  p(1  - p)), and thus E{IB -
kpl}  ~ Jkp(1  - p)J2/n,  as  the  first  absolute  moment  of a  normal  random 
variable is ,J2Tii (see Problem 5.11). Working this through yields an approximate 
bound of J LNN /  (n k). The bound is proportional to -JL;;. This can be improved 
to  L * if,  instead of bounding it from  above by Markov's inequality,  we directly 
approximate P {B - kp  >  k (1/2 - p)} as  shown below.  0 

Theorem 5.8.  (DEVROYE (l981B)).  For all distributions and k  ::::  3 odd, 

where  y  = sUPr>O 2rP{N  >  r}  =  0.33994241 ... ,  N  is normal (0,  1),  and 0(-) 
refers to k  .....-+  00. (Explicit constants are given in the proof.) 

5.7 Inequalities for the Probability of Error 

77 

The constant y  in the proof cannot be improved. A slightly weaker bound was 

obtained by Devijver (1979): 

LkNN 

:::s 

LNN 

(where e =  rk/2l) 

L  * + 22k' 

k' 

1  (2k') 
.j!;, 

nk' 

L'" + LNN  -(1 + 0(1)) 

(as k --+  00, see Lemma A.3). 

See also Devijver and Kittler (1982, p.l02). 

Lemma 5.6.  (DEVROYE (1981B)).  For p  :::s  1/2 and with k  >  3 odd, 

P {Binomial(k, p) >  ~} 

k! 

{P 

(k-l)/2 

(k;l)!(k;l)!Jo  (x(1-x)) 

dx 

<  A  (~  e-z2 / 2dz 

J(l-2p)~ 

, 

PROOF. Consider k i.i.d. uniform random variables on [0,  1]. The number of values 
in [0,  p] is binomial (k,  p). The number exceeds k/2 if and only ifthe (k + l)/2-th 
order statistic of the uniformc1oudis at most p. The latter is beta ((k+ 1)/2, (k+ 1)/2) 
distributed, explaining the first equality (Problem 5.32). Note that we have written a 
discrete sum as an integral-in some cases, such tricks payoff handsome rewards. 
To show the inequality, replace x by ~ (1  - A=r) and use the inequality 1  u:::s 
e-u to obtain a bound as shown with 

Finally, 

A 

P {B 

__  k+l}  k+l 

2  2~ (B is binomial (k,  1/2)) 

< 

< 

k 

k+ 1 

2n k+l k-l  2  'k=l 

y l ( , -1  

2 

2 

(Problem 5.17) 

(Problem 5.18).  0 

78 

5.  Nearest Neighbor Rules 

PROOF OF  THEOREM 5.8.  From earlier remarks, 

LkNN  - L *  =  E {ak(17(X))  - min(17(X),  1 - 17(X))} 

=  E { Cnin(ry~;~~\X~ ry(X))  - 1) min(ry(X), 1 - ry(X))} 

1-2p  { 

k}) 
sup  - -P  B  >  -
2 

( 
O<p<l/2 

P 

L * 

(B is binomial (k,  p)). 

We merely bound the factor in brackets. Clearly, by Lemma 5.6, 

LkNN  - L * :::;  L * (  sup  1 - 2p A I-Jk-l 

O<p<1/2 

P 

(1-2p)-Jk-l 

e-Z2

/ 2dZ) . 

Take  a  <  1 as  the  solution  of (3j(ea2))3/2 ~ =  y, which is  possible  if 
k - 1 >  2~? n) 6 = 2.4886858 .... Setting v = (l - 2 p )vT=1, we have 

1 - 2p I-Jk-l 

sup  - -

O<p<l/2 

P 

2 

e-z2
--dz 

/

:::;  max 

(l-2p)v'k=T  .J2ii 

sup 

2vjvT=1  100  e-z2

2 
--dz , 
.J2ii 
2) 
- -
a-Jk-l Sv <v'k=T  1 - v j vT=1 .J2ii 

O<vsa-Jk-l  1 - v j vT=1  v 
( 
2pvT=1  e- v2

sup 

/

/

(1  - a)vT=1 

:::;  max 

( 

:::;  max 

( 

y 

y 

vT=1  -a 2(k-l)/2) 
,---e 
.J2ii 
(  3  )3/2 

1 
) 
l).J2ii 

(1  - a)vT=1'  ea2 

(k -

(use u3/2e-cu  :::;  (3j(2ce))3/2  for all u  >  0) 

= 

y 

(1- a)vT=1' 

Collect all bounds and note that a = 0  (k-l/6).  0 

5.8  Behavior When L * Is  Small 

In  this  section,  we  look more  closely  at  LkNN  when  L * is  small.  Recalling  that 
LkNN  = E{ak(17(X))}  with 
",,(p) = min(p, 1-p)+11-2min(p, 1-p)IP {Binomial(k, min(p, 1- p)) >  ~} 

for  odd  k,  it is  easily  seen  that  LkNN  = E{~k(min(17(X), 1 
function ~k. Because 

17(X)))}  for  some 

5.8 Behavior When L * Is Small 

79 

. 

mlll(p, 1 - p) =  .... 

1 -

,Jl - 4p(l - p) 
' 

2 

we also have LkNN  = E{ lJ!k(17(X)(l  - 17(X)))} for some other function ljfk. Worked(cid:173)
out forms of L kNN  include 

j>kj2 

+ L  e)~(X)j(l- ~(X»k-j+l} 
L  e)E {(17(X)(l  - 17(X)))j+1  ((1  - 17(X))k-2j -1 + 17(X/-2j-1)} . 

] 

j<kj2 

] 

As pa + (1 - p)a is a function of p( 1 - p) for integer a, this may be further reduced 
to simplified forms such as 

LNN 

E{217(X)(l  - 17(X))}, 

L3NN  =  E{17(X)(1  - 17(X))} + 4E {(17(X)(1  - 17(X)))2}  , 

LSNN  =  E{17(X)(l  - 17(X))} + E  {(17(X)(1  - 17(X)))2} 

+ 12E {(17(X)(l  - 17(X)))3} . 

The behavior of ak  near zero is very informative. As p  + 0,  we have 

a1(p)  =  2p(l - p) '"'-'  2p, 

a3(p) 

as(p) 

p(l - p)(1 + 4p) '"'-'  p + 3p2, 

p+l0p3, 

while for the Bayes error,  L * = E{rnin(17(X),  1 - 17(X))}  = E{aoo (17(X))},  where 
a oo  = rnin(p, 1 - p)  AJ  pas p  + O.  Assume that 17(X) = P at all x. Then, as p  + 0, 

LNN  AJ  2L *  and  L3NN  AJ  L * . 

Moreover, LNN  - L *  AJ  L *, L3NN  - L *  AJ  3L *2. Assume that L * = p  = 0.01. Then 
L1  ~ 0.02, whereas L3NN  - L * ~ 0.0003. For all practical purposes, the 3-NN rule 
is virtually perfect. For this reason, the 3-NN rule is highly recommended. Little is 
gained by considering the 5-NN rule when p  is  small, as  LSNN  - L * ~ 0.00001. 

Let ak be the smallest number such that ak(p) ::s  ak min(p, 1  p) for all p  (the 

tangents in Figure 5.6). Then 

80 

5.  Nearest Neighbor Rules 

This is precisely at the basis of the inequalities of Theorems 5.6 through 5.8, where 
it was shown that ak = I + 0(1/ -Jk). 

FIGURE 5.6.  ak(p) as a function 
ofp· 

1/2 

1/2 

and thus, the classes are separated. This does not imply that the support of X given 

for every fixed k,  the k-nearest neighbor rule is  consistent.  Cover has a beautiful 

5.9  Nearest Neighbor Rules When L * = 0 
From Theorem 5.4 we retain that if L * = 0, then LkNN  = ° for all k.  In fact,  then, 
example to illustrate this remarkable fact. L * = ° implies that 1J(x)  E  {O,  I} for all x, 
Y  = ° is different from the support of X given Y  = 1. Take for example a random 
rational number from [0,  1] (e.g., generate I,  J independently and at random from 
the  geometric  distribution  on  {I, 2, 3, ... },  and  set  X  = min (I , J)/ max(I, J)). 
Every  rational number on  [0,  1]  has positive probability.  Given  Y  =  1,  X  is  as 
above, and given Y  = 0,  X is uniform on [0,  1]. Let P{Y =  I}  = P{Y = O}  =  1/2. 
The support of X is identical in both cases. As 

1J(x) =  ° if x  is irrational, 

if x  is rational 

I 

{

we see that  L *  = ° and that the  nearest neighbor rule is  consistent.  If someone 

shows us  a number X drawn from the same distribution as  the data, then we may 
decide  the  rationality  of X  merely  by  looking  at  the  rationality  of the  nearest 
neighbor of X. Although we did not show this, the same is true if we are given any 
x  E  [0,  1]: 

lim  P{x is rational Y(l)(x)  = ° (X(l)(x) is not rational)} 

n---+oo 

= 

lim  P{x is not rational Y(l)(x) = 1 (X(l)(x) is rational)} 
n---+oo °  (see Problem 5.38). 

5.11 The (k, i)-Nearest Neighbor Rule 

81 

5.10  Admissibility of the Nearest Neighbor Rule 

The consistency theorems of Chapter 11  show us that we should take k  = k n  ~ 00 
in the k-NN rule. The decreasing nature ofL kNN  corroborates this. Yet, there exist 
distributions for which for all n, the  I-NN rule is better than the k-NN rule for any 
k 2:  3. This observation, due to Cover and Hart (1967), rests on the following class 
of examples. Let So  and Sl  be two spheres of radius  1 centered at a and b, where 
Iia  - bll  >  2.  Given Y  =  1,  X  is uniform on Sl, while given Y  = 0,  X  is uniform 
on  So,  whereas P{Y = I}  = P{Y = o}  = 1/2. We note that given n  observations, 
with the  I-NN rule, 

E{Ln} = P{Y = 0,  Y1 = ... = Yn = I} + P{Y = 1, Y1 = ... = Yn = o} = -. 

1 
2n 

For the k-NN rule, k  being odd, we have 

E(L n }  =  P { Y = 0, t '[y,=Il}  :'0  Lk/2J} 
+ P {Y = 1, t '[y,=I}  :'0  Lk/2J} 

P {Binomial(n, 1/2) S  Lk/2J} 

= 

1  Lk/2J  ( )   1 
- ~ n  >  _  when k  >  3. 
2n  ~.  2n 

-

j=O 

] 

Hence, the k-NN rule is worse than the  I-NN rule for every n when the distribution 
is given above. We refer to the exercises regarding some interesting admissibility 
questions for k-NN rules. 

5.11  The (k, I)-Nearest Neighbor Rule 

In 1970, Hellman (1970) proposed the (k, I)-nearest neighbor rule, which is iden(cid:173)
tical to the k-nearest neighbor rule, but refuses to make a decision unless at least 
I  >  k /2 observations are from the same class. Formally, we set 

gn(x) =  °  if L~=l Y(i)(x)  skI 

if L~=l Y(i)(x)  2:  I 

I 

-1  otherwise (no decision). 

{

Define the pseudoprobability of error by 

Ln = P{gn(X)  ~ {-I, Y}ID,J, 

that is, the probability that we reach a decision and correctly classify X. Clearly, 
Ln  S  P{gn(X)  =I  YIDn},  our standard probability of error.  The latter inequality 

82 

5.  Nearest Neighbor Rules 

is  only  superficially interesting,  as  the  probability of not reaching  a  decision is 
not taken into account in Ln. We may extend Theorem 5.2 to show the following 
(Problem 5.35): 

Theorem 5.9.  For the (k, I)-nearest neighbor rule,  the pseudoprobability of error 
Ln satisfies 

n-+oo 

lim  E{L n }  =  E{17(X)P{Binomial(k, 17(X»  :s k -IIX} 
+ (1  - 17(X»P{Binomial(k, 17(X»  ?:  IIX}} 

def 

Lk,l' 

The above result is distribution-free. Note that the k-nearest neighbor rule for 
odd k  corresponds to Lk,(k+l)/2. The limit Lk,l by itself is not interesting, but it was 
shown by Devijver (1979) that  Lk,l  holds information regarding the Bayes error 
L*. 

Theorem 5.10.  (DEVIJVER (1979».  For all distributions and with k odd, 

Also, 

LkNN + L k , fk/21+1  <  L  * <  L 

2 

-

-

kNN' 

This  theorem  (for  which  we refer to  Problem 5.34)  shows  that  L * is  tightly 
sandwiched  between  LkNN,  the  asymptotic  probability  of error  of the  k-nearest 
neighbor rule,  and the  "tennis"  rule  which requires  that the  difference  of votes 
between the  two  classes among the  k  nearest neighbors  be at least two.  If Ln  is 
close to its limit, and if we can estimate Ln  (see the chapters on error estimation), 
then we may be able to use Devijver's inequalities to obtain estimates of the Bayes 
error L *. For additional results, see Loizou and Maybank (1987). 

As a corollary of Devijver's inequalities, we note that 

L 

kNN 

We have 

_  L  * <  LkNN  - Lk,rk/21+1 

-

2 

Lk,l  =  L* +E{l1- 2min(17(X), 1-17(X»1 

x P{Binomial(k, min(17(X),  1 - 17(X»)  ?:  IlX}}, 

and therefore 

E{/1-2min(17(X),1-17(X»I 

x  P{Binomial(k, min(17(X),  1 - 17(X») = l IX}} 

Problems and Exercises 

83 

E{ 11  - 2 min(1](X),  1 - 1](X))1 
x G) min(ry(X), 1 -

ry(X)J'(l  min(ry(X),  1 - ry(X)))k-l} 

< 

(k) ll(k -

l)k-l 

kk 

l 
(because ul(1  - u)k-l reaches its maximum on [0,1] at u = lj k) 

< 

k 

12nl(k -I) 

(use (~)  ::::; 

ll(k'!.z)k-l kJ l(k~l)' by Stirling's formula). 

With I = rkj2l, we thus obtain 

LkNN  - L*  < 

1 

k 

rkj2l Lkj2J 

2V2JT 
~ ~0.398942 
V~ v'k' 

improving on Theorem 5.6. Various other inequalities may be derived in this man(cid:173)
ner as well. 

Problems and Exercises 

PROBLEM 5.1.  Let II  . II  be an arbitrary norm on n d
,  and define the k-nearest neighbor rule 
in terms of the distance p(x, z) = IIx  - z II.  Show that Theorems 5.1  and 5.2 remain valid. 
HINT:  Only Stone's lemma needs adjusting. The role of cones C(x, JT /6) used in the proof 
are now played by sets with the following property: x and z belong to the same set if and 
only if 

PROBLEM 5.2.  Does there exist a distribution for which sUPn::: I  E{ Ln}  >  1/2 for the nearest 
neighbor rule? 

PROBLEM 5.3.  Show that L3NN  :s  1.32L * and that LSNN  :s  1.22L *. 
PROBLEM 5.4.  Show that if C* is  a compact subset of n d  and C is the support set for the 
probability measure fL, 

IIX(l) - xii  -+ 0 

sup 
XEcnc* 

with probability one, where X(l)  is the nearest neighbor of x  among XI, ... , Xn  (Wagner 
(1971)). 

84 

5.  Nearest Neighbor Rules 

PROBLEM 5.5.  Let fL be the probability measure of X given Y  = 0, and let v be the probability 
measure of X given Y = 1.  Assume that X is real-valued and that P{Y = O}  = P{Y = I}  = 
1/2. Find a pair (v, fL)  such that 

suPport(fL) = support( v); 

(1) 
(2)  L* = 0. 

Conclude that L * = ° does not tell us  a lot about the support sets of fL  and v. 

PROBLEM 5.6.  Consider the  (2k  + I)-nearest neighbor rule  for  distributions  (X, Y)  with 
1(X) ==  P constant, and Y independent of X. This exercise explores the behavior of L(2k+l)NN 
as  p  to. 

(1)  For fixed integer 1 >  0, as  p  to, show that 

(2)  Use a convenient representation of L(2k+l)NN  to conclude that as  p  t  0, 

L(2k+l)NN  = P + 

((

2k)  (2k))  k+l 
k  +  k + 1 

p  + o(p 

k+l 

). 

PROBLEM 5.7.  Das Gupta and Lin (1980) proposed the following rule for data with X  E  R. 
Assume X is nonatomic. First, reorder XI, ... , X n ,  X according to increasing values, and 
denote the ordered set by X (1)  <  X(2)  <  ... <  XCi)  <  X  <  X(i+l)  <  '"  <  X(n).TheYi'sare 
permuted so that Y(j)  is the label of X(j). Take votes among {Y(i),  Y(i+l)},  {Y(i-l), Y(i+2)},  ... 
until for the first time there is  agreement (Yu-j)  = Y(i+j+l»),  at which time we decide that 
class, that is, gn (X) = Y(i _ j) = Y(i+ j+ 1). This rule is invariant under monotone transformations 
of the x-axis. 

(1) 

If L  denotes the asymptotic  expected probability of error, show that for all non(cid:173)
atomic X, 

L  = E { 

1(X)(1 - 1(X)  I 

1 - 21(X)(1  - 1(X) 

. 

(2)  Show that L  is the same as for the rule in which X  E  Rd and we consider 2-NN, 
4-NN, 6-NN, etc. rules in turn, stopping at the first 2k-NN rule for which there is no 
voting tie.  Assume for  simplicity that X  has  a density (with a good distance-tie 
breaking rule, this may be dropped). 

(3)  Show that L  - LNN  ~ (l/2)(L3NN  - L NN ), and thus that L  ~ (LNN  + L 3NN)/2. 
(4)  Show that  L  :::::  L NN . Hence,  the rule performs somewhere in between the  I-NN 

and 3-NN rules. 

PROBLEM 5.8.  Let Y  be independent  of X,  and 1(X)  ==  P constant.  Consider a weighted 
(2k + I)-nearest neighbor rule  with weights  (2m  + 1,  1,  1, ... ,1) (there  are  2k  "ones"), 
where k  - 1  ~ m  ~ 0.  For m  = 0,  we  obtain the (2k + 1)-NN  rule.  Let L(k, m) be the 
asymptotic probability of error. 

(1)  Using results from Problem 5.6, show that 

L(k, m) = p + (  2k 

)pk-m+l + ( 

2k 

)pk+m+l  + o(pk-m+l) 

k-m 

k+m+l 

as  p  t  0. Conclude that within this class of rules, for small p, the goodness of a 
rule is measured by k - m. 

(2)  Let 0 > ° be small, and set p  = 1/2 - o.  Show that if X is binomial (2k,  1/2 - 0) 

and Z is binomial (2k,  1/2), then for fixed I, as 8 + 0, 

Problems and Exercises 

85 

PIX 2:  l} = P{Z 2:  l}  - 2koP{Z = l} + 0(02), 

and 

PIX ::s  I}  = P{Z ::s  l} + 2k8P{Z = 1 +  I}  + 0(82). 

(3)  Conclude that for fixed k,  m  as 8  .}  0, 

L(k, m) 

1 "2  - 282(kP{Z = k + m} + kP{Z = k  + m  +  I} 
+ P {Z  ::s  k + m} - P {Z  2:  k + m  +  I}) + 0(0 2

). 

(4)  Take weight vector w  with k  fixed and m  = L 1 0,Jk J , and compare it with weight 
vector w' with k/2 components and m = L.Jkl2J  as  p  .} ° and p  t  1/2. Assume 
that k is very large but fixed.  In particular, show that w is better as  p + 0, and w' 
is better as p  t  1/2. For the last example, note that for fixed c  >  0, 

kP{Z=k+m}+kP{Z=k+m+l}"-'8-Jk  ~e-2c2  as  k......,..oo 

y2JT 

by the central limit theorem. 

(5)  Conclude that there exist different weight vectors  w,  Wi  for which there exists a 
pair of distributions  of (X, Y)  such that their asymptotic  error probabilities  are 
differently ordered. Thus, W is not totally ordered with respect to the probability 
of error. 

PROBLEM 5.9.  Patrick and Fisher (1970) find the k-th nearest neighbor in each of the two 
classes and classify according to  which is  nearest.  Show that their rule is  equivalent to a 
(2k  -

I)-nearest neighbor rule. 

PROBLEM 5.10.  Rabiner et aL  (1979)  generalize the rule of Problem 5.9  so  as  to  classify 
according to  the average distance to  the k-th nearest neighbor within each class. Assume 
that X  has a density. For fixed k, find the asymptotic probability of error. 
PROBLEM 5.11.  If N  is normal (0,  1), then E{INI} = ,J2!ii. Prove this. 

PROBLEM 5.12.  Show that if L9NN  = L(11)NN, then L(99)NN  = L(lll)NN' 

PROBLEM 5.13.  Show that 

L3NN  ::s 

(

7V7 + 17 

27v'3  + 1  L  ~ 1.3155 ... L  * 

) 

* 

for all distributions (Devroye (1981b». HINT: Find the smallest constanta such that L3NN  ::s 
L * (1  + a) using the representation of L3NN in terms of the binomial taiL 

PROBLEM 5.14.  Show that if X has a density  j, then for all u  >  0, 
lim  P  {nl/dIlX(l)(X) - XII  >  u\X} = e-f(X)vu

n--+oo 

d 

with probability one, where v = Is  dx is the volume ofthe unit ball in n d  (Gyorfi (1978». 

0,1 

86 

5.  Nearest Neighbor Rules 

PROBLEM 5.15.  Consider a rule that takes a majority vote over all Yi 's for which IIXi -xII ::s 
(c / v n ) 1 j d, where v = Is  dx is the volume of the unit ball, and c  >  0 is fixed.  In case of a 
tie decide gn(x) = O. 

0,1 

If X has a density  j, show thatliminfn--+ooE{Ln}::::  E {1J(X)e-cf(X)}.  HINT: Use 
the obvious inequality E{L n }  ::::  P{Y = 1,  fJ--n(SX,cjvn)  = OJ. 

(1) 

(2)  If Y  is independent of X and 1J  ==  P  >  1/2, then 

E {  (X)e-Cf(X)} 

1J 

L* 

= E {e-cf(X)}  _P- too 

1- P 

as  P t  1. Show this. 

(3)  Conclude that 

sup 

(X,Y):L*>O 

lim infn--+oo E{L n } 

L* 

= 00, 

and thus that distribution-free bounds of the form limn--+ oo  E{ Ln}  ::::  c' L * obtained 
for  k-nearest  neighbor  estimates  do  not  exist  for  these  simple  rules  (Devroye 
(198Ia». 

PROBLEM 5.16.  Take an example with 1J(X)  ==  1/2 -
I/(2./k), and show that the bound 
LkNN  - L * ::::  I/"Jke cannot be essentially bettered for large values of k, that is, there exists 
a sequence of distributions (indexed by k) for which 

as k  -+  00, where N  is a normal (0,  1) random variable. 

PROBLEM 5.17.  If B is binomial (n, p), then 

sup P{B = i}::::  I 

. n 

V 27il(n - 1) 

p 

.,  0  <  i  <  n. 

PROBLEM 5.18.  Show that for k ::::  3, 

,Jk(k+l) <  (1 + ~) (1 + ~) = 1 + ~ + ~. 
k  - 1 

4k2 

2k 

2k 

k 

-

PROBLEM 5.19.  Show that there exists a sequence of distributions of (X, Y) (indexed by k) 
in which Y  is independent of X and 1J(x)  ==  p  (with p depending on k  only) such that 

lim inf 
n--+oo 

( 

LkNN  - L * ) 

L* 

" 
v k  ::::  y  = 0.339942 ... , 

where  y  is the constant of Theorem 5.8 (Devroye (1981b». HINT: Verify the proof of The(cid:173)
orem 5.8 but bound things from below. Slud's inequality (see Lemma A.6 in the Appendix) 
may be of use here. 

PROBLEM 5.20.  Consider a weighted nearest neighbor rule with weights 1,  p, p2,  p3,  '"  for 
p  <  1. Show that the expected probability of error tends for all distributions to a limit L(p). 
HINT:  Truncate  at k fixed  but large,  and argue  that the  tail  has  asymptotically  negligible 
weight. 

Problems and Exercises 

87 

PROBLEM  5.21.  CONTINUED.  With L(p) as in the previous exercise, show that L(p) = LNN 
whenever p  <  1/2. 

PROBLEM  5.22.  CONTINUED. Prove or disprove: as p increases from 1/2 to 1, L(p) decreases 
monotonically from LNN  to L *.  (This question is difficult.) 

PROBLEM  5.23.  Show that in the weighted NN  rule with weights (1,  p, p2), 0  <  P  <  1, the 
asymptotic probability of error is LNN  if P <  (.J5 - 1)/2 and is L3NN  if P >  (.J5 - 1)/2. 

PROBLEM 5.24.  Is there any k, other than one, for which the k-NN rule is admissible, that is, 
for which there exists a distribution of (X, Y)  such that E{L n }  for the k-NN rule is  smaller 
than E{L n }  for any k'-NN rule with k' =I k,  for all n? HINT:  This is difficult. Note that if this 
is to hold for all n, then it must hold for the limits. From this, deduce that with probability 
one,  7J(x)  E  {O,  1/2, I}  for any such distribution. 

PROBLEM 5.25.  For every fixed n and odd k  with n  >  lOOOk,  find  a distribution of (X, Y) 
such that E{Ln} for the k-NN  rule is smaller than E{Ln} for any k'-NN rule with k' =I k,  k' 
odd. Thus, for a given n, no k  can be a priori discarded from consideration. 
PROBLEM 5.26.  Let X  be uniform on [0,  1], 7J(x)  ==  x, and pry = O}  = pry = I}  = 1/2. 
Show that for the nearest neighbor rule, 

E{Ln} = - + - - - - - -
2(n + 1)(n + 2)(n + 3) 

3n + 5 

1 
3 

(Cover and Hart (1967); Peterson (1970)). 

PROBLEM 5.27.  For the nearest neighbor rule, if X  has a density, then 

IE{Ln} - E{Ln+dl  ::s  -

1 

n+l 

(Cover (1968a)). 

PROBLEM  5.28.  Let X have a density f  ~ c  >  0 on [0,  1], and assume that f~" and f{" exist 
and are uniformly bounded. Show that for the nearest neighbor rule, E{ Ln} = LNN + 0 (1/ n2
) 
(Cover (1968a»). For d-dimensional problems this result was generalized by Psaltis, Snapp, 
and Venkatesh (1994). 
PROBLEM  5.29.  Show that LkNN  ::s  (1  + ,J2Jk)L * is the best possible bound of the form 
LkNN  ::s  (1  + a/ "jk)L * valid simultaneously for all k  ~ 1 (Devroye (1981b)). 
PROBLEM  5.30.  Show that LkNN  ::s  (1  + -Jllk)L* for all k  ~ 3 (Devroye (1981b)). 
PROBLEM  5.31.  Let x  = (x(1), x(2))  E  n2. Consider the nearest neighbor rule based upon 
vectors with components (x 3( 1), x 7 (2), x( 1 )x(2)). Show that this is asymptotically not better 
than if we had used (x(1), x(2»). Show by example that (X2(1),  x 3(2), x 6(1)x(2)) may yield 
a worse asymptotic error probability than (x(1), x(2)). 

PROBLEM 5.32.  UNIFORM  ORDER  STATISTICS.  Let U(1)  <  ...  <  U(n)  be order statistics of n 
i.i.d. uniform [0,  1]  random variables. Show the following: 
(1)  U(k)  is beta (k, n + 1 - k), that is,  U(k)  has density 

f( x) -
-

n! 

X k- 1(1  - x)n-k  0  _<  x  _<  1. 

(k - 1)!(n - k)! 

' 

88 

5.  Nearest Neighbor Rules 

(2) 

(3) 

a 

E {U(k)}  = 

r(k + a)r(n + 1) 
, 
r(k)r(n + 1 + a) 

for any  a  >  0. 

l/!(a) 
1 - -< - - -<1+ - -

a  E{U&)} 
(kln)a  -
n  -

k 

for a  ::::  1, where l/!(a) is a function of a only (Royall (1966)). 

PROBLEM 5.33.  DUDANI'S RULE. Dudani (1976) proposes a weighted k-NN rule where Y(i)(x) 
receives weight 

IIX(k)(X)  - xII  -

IIX(i)(x) - xii,  1:::;  i  :::;  k. 

Why is this roughly speaking equivalent to attaching weight 1 - (i I k)l/d to the i-th nearest 
neighbor if X has a density? HINT:  If ~ is the probability measure of X, then 

are distributed like  U(1),  ... ,  U(k)  where U(l)  <  ... <  U(n)  are the order statistics of n i.i.d. 
uniform [0,  1]  random variables. Replace ~ by a good local approximation, and use results 
from the previous exercise. 

PROBLEM 5.34.  Show Devijver's theorem (Theorem 5.10) in two parts:  first establish the 
inequality L * ::::  Lk,lk/21-1  for the tennis rule, and then establish the monotonicity. 

PROBLEM 5.35.  Show Theorem 5.9 for the (k, I) nearest neighbor rule. 

PROBLEM 5.36.  Let  R  be  the  asymptotic  error probability  of the  (2, 2)-nearest neighbor 
rule. Prove that R = E{21J(X)(1  - 1J(X»)} = L NN • 

PROBLEM 5.37.  For the nearest neighbor rule, show that for all distributions, 

lim  P{gn(X) = 0, Y = I} 

n-+oo 

lim  P{gn(X) = 1, Y  = O} 

n-+oo 
E{1J(X)(1  - 1J(X»)} 

(Devijver and Kittler (1982»). Thus, errors of both kinds are equally likely. 

PROBLEM 5.38.  Let P{Y =  I}  = P{Y = O}  =  1/2 and let X be a random rational if Y  =  1 (as 
defined in Section 5.9) such that every rational number has positive probability, and let X 

be uniform [0,  1]  if Y  = 0.  Show that for every x  E  [0,  1]  not rational, P{Y(l)(x) = I}  ---+ ° 
as n  ---+  00, while for every x  E  [0,1] rational, P{Y(l)(x) = o}  ---+ ° as  n  ---+  00. 

PROBLEM 5.39.  Let Xl, ... , Xn  be i.i.d.  and have a common density.  Show that for fixed 
k  >  0, 

nP {X3  is among the k nearest neighbors of  Xl  and  X2  in  {X3, ... , Xn}}  ---+  0. 

Show that the same result remains valid whenever k varies with n such that kl In ---+  0. 

Problems and Exercises 

89. 

PROBLEM 5.40.  IMPERFECT TRAINING. Let (X, Y,  Z), (Xl, YI ,  Zd, .. ·, (Xn'  ~1' Zn)bease(cid:173)
quence of i.i.d.  triples in Rd  x  {a,  I}  x  {a,  I}  with pry = llX = x}  = 17(x)  and P{Z  = 
llX = x} = 17' (x). Let Z(l)(X) be Z j if X j is the nearest neighbor of X  among X I,  ... , X n • 
Show that 

lim  P{Z(l)(X) =I y} = E  {17(X) + 17'(X) - 217(X)17'(X)} 
n-+oo 

(Lugosi (1992». 

PROBLEM 5.41.  Improve the bound in Lemma 5.3 to Yd  :s  3d  - 1. 

,  then Yd  2:  2d. 

PROBLEM 5.42.  Show that if {C(XI' ][/6), ... , C(xyd ,  ][ /6)} is a collection of cones cover(cid:173)
ing R d
PROBLEM 5.43.  Recalling that LkNN  = E  {ak(17(X»)},  where 

ak(p) = min(p, 1 - p) + 11  - 2min(p, 1 - p)IP {Binomial(k, min(p, 1 - p»  >  ~} , 
show that for every fixed  p, P {Binomial(k, min(p, 1 - p»  >  ~} ,} ° as k  ~ 00 (it is the 
mono tonicity that is harder to show). How would you then prove that limk---+oo  LkNN  = L *? 

PROBLEM 5.44.  Show that the asymptotic error probability of the rule that decides gn(x) = 
Y(8)(X)  is identical to that of the rule in which gn(x) = Y(3)(X). 
PROBLEM 5.45.  Show that for all distributions LSNN  = E{ Vrs(17(X)(I-17(X»}, where Vrs(u)  = 
u + u2  + 12u3

. 

PROBLEM 5.46.  Show that for all distributions, 

and that 

PROBLEM 5.47.  Let  X(l)  be  the  nearest  neighbor  of x  among  Xl, ... , X n .  Construct  an 
example for which E{ II X(l)  - x II}  =  00 for all x  E  Rd. (Therefore, we have to steer clear 
of convergence in the mean in Lemma 5.1.) Let X (1)  be the nearest neighbor of X I among 
X 2 ,  •.. ,  X n • Construct a distribution such that E{IIX(l)  - XIII}  = 00 for all n. 

PROBLEM 5.48.  Consider the  weighted  nearest neighbor rule  with  weights  (WI,  ... ,  Wk). 
Define a new weight vector (WI,  W2,  ... ,  Wk-l,  VI,  ... , vz),  where 2:::;=1  Vi  = Wk. Thus, the 
weight vectors are partially ordered by the operation "partition." Assume that all weights 
are nonnegative. Let the asymptotic expected probability of errors be Land L', respectively. 
True or false:  for all distributions of (X, Y),  L' :s  L. 

PROBLEM 5.49.  GABRIEL  NEIGHBORS.  Given Xl,'."  Xn  E  R d ,  we say that Xi  and Xj  are 
Gabriel neighbors if the ball centered at (Xi + X j )/2 of radius IIXi - Xj 11/2 contains no X k , 
k =I i, j  (Gabriel and Sokal (1969); Matula and Sokal (1980». Clearly, if Xj is the nearest 
neighbor of Xi,  then  Xi  and  Xj  are Gabriel neighbors.  Show that if X  has  a density and 
X I,  ... , Xn  are i.i.d.  and drawn from  the distribution of X, then the expected number of 
Gabriel neighbors of Xl tends to  2d  as n  ~ 00 (Devroye (1988c». 

90 

5.  Nearest Neighbor Rules 

FIGURE 5.7.  The  Gabriel  graph  of 
20  points  on  the  plane  is  shown: 
Gabriel neighbors are connected by 
an edge.  Note  that all circles strad(cid:173)
dling these edges have no data point 
in their interior. 

PROBLEM 5.50.  GABRIEL  NEIGHBOR  RULE.  Define the Gabriel neighbor rule  simply as  the 
rule that takes a majority vote over all Yi 's for the Gabriel neighbors of X among Xl, ... , X n • 
Ties  are  broken by flipping  a coin.  Let Ln  be the  conditional probability of error for  the 
Gabriel rule.  Using the result of the previous exercise, show that if L * is  the Bayes error, 
then 

For (3),  determine the best possible value of c.  HINT:  Use Theorem 5.8  and try obtaining, 
for d  = 2,  a lower bound for P {N x  2:  3},  where N x  is the number of Gabriel neighbors of 
X  among Xl, ... , Xn • 

(1) 
(2) 

(3) 

lim  E{L n }  = 0 if L* = 0; 
n---+oo 
limsupE{L n }  <  LNN  if L*  >  0, d  >  1; 
n--+oo 
lim sup E{Ln }  .::::  cL * for some c  <  2, if d  >  1. 
n--+oo 

6 
Consistency 

6.1  Universal Consistency 

If we are given a sequence Dn  = ((Xl, YI ), ... , (Xn,  Yn)) of training data, the best 
we can expect from a classification function is to achieve the Bayes error probability 
L * . Generally, we cannot hope to obtain a function that exactly achieves the Bayes 
error probability, but it is possible to construct a sequence of classification functions 
{gn},  that is, a classification rule, such that the error probability 

gets arbitrarily close to L * with large probability (that is, for "most" Dn). This idea 
is formulated in the definitions of consistency: 

DEFINITION 6.1.  (WEAK AND  STRONG  CONSISTENCY).  A classification rule is con(cid:173)
sistent (or asymptotically Bayes-risk efficient) for a certain distribution of (X,  Y) 
if 

and strongly consistent if 

lim  Ln  = L *  with probability 1. 
n-+oo 

REMARK. Consistency is defined as the convergence of the expected value of Ln  to 
L *.  Since Ln  is  a random variable bounded between L * and 1,  this convergence 

92 

6.  Consistency 

every E  > ° 

is  equivalent to the convergence of Ln to L * in probability, which means that for 

lim  P {Ln  - L * >  E}  = 0. 
n---+oo 

Obviously,  since almost sure convergence always implies convergence in proba(cid:173)
bility, strong consistency implies consistency.  D 

A consistent rule guarantees that by increasing the amount of data the probability 
that the error probability is within a very small distance of the optimal achievable 
gets arbitrarily close to  one.  Intuitively, the rule can eventually learn the optimal 
decision from a large amount of training data with high probability.  Strong con(cid:173)
sistency means that by using more data the error probability gets arbitrarily close 
to the optimum for every training sequence except for a set of sequences that has 
zero probability altogether. 

A decision rule can be consistent for a certain class of distributions of (X, Y), 
but may not be consistent for others.  It is  clearly desirable to  have a rule that is 
consistent for a large class of distributions. Since in many situations we do not have 
any prior information about the distribution, it is essential to have a rule that gives 
good performance for all distributions. This very strong requirement of universal 
goodness is formulated as follows: 

DEFINITION 6.2.  (UNIVERSAL CONSISTENCY).  A sequence of decision rules is called 
universally (strongly) consistent if it is (strongly) consistent for any distribution of 
the pair (X, Y). 

In this chapter we show that such universally consistent classification rules exist. 
At first,  this may seem very surprising, for some distributions are very "strange," 
and seem hard to learn. For example, let X be uniformly distributed on [0,  1] with 
probability  1/2, and let  X  be  atomic  on the  rationals  with probability  1/2. For 
example, if the rationals  are enumerated rl, r2,  r3,  ... , then P{X  = rd = 1/2i+l. 
Let  Y  = 1 if X  is  rational  and  Y  = ° if X  is  irrational.  Obviously,  L *  = 0.  If 

a classification rule gn  is  consistent,  then the probability of incorrectly guessing 
the rationality  of X  tends  to  zero.  Note here that we cannot "check" whether X 
is rational or not, but we should base our decision solely on the data Dn  given to 
us.  One consistent rule is the following:  gn(x,  Dn)  = Yk  if X k  is the closest point 
to x  among Xl,""  X n •  The fact that the rationals are dense in [0,  1]  makes the 
statement even more surprising. See Problem 6.3. 

6.2  Classification and Regression Estimation 

In this  section  we  show  how  consistency of classification rules  can be deduced 
from consistent regression estimation. In many cases the a posteriori probability 
rJ(x) is estimated from the training data Dn  by some function rJn(x)  = rJn(x,  Dn). 

6.2 Classification and Regression Estimation 

93 

In this case, the error probability L(gn) = P{gn(X) =I YIDn} of the plug-in rule 

0 

if rJn(x)  :s  1/2 

gn(x) =  1  otherwise 

{ 

is a random variable. Then a simple corollary of Theorem 2.2 is as follows: 

COROLLARY 6.1.  The error probability of the classifier gn (x) defined above satis(cid:173)
fies the inequality 

The next corollary follows from the Cauchy-Schwarz inequality. 

COROLLARY 6.2.  If 

{ ° if rJn(x)  :s  1/2 

h ot  erwise, 

gn(x) = 

then its error probability satisfies 

Clearly,  rJ(x)  = P{Y = IIX = x} = E{YIX = x} is just the regression function of 
Y on X.  Therefore,  the most interesting consequence of Theorem 2.2 is that the 
mere existence of a regression function estimate rJn (x) for which 

in probability or with probability one implies that the plug-in decision rule gn  is 
consistent or strongly consistent, respectively. 

Clearly, from Theorem 2.3, one can arrive at a conclusion analogous to Corollary 
6.1  when the probabilities rJo(x)  = P{Y = 0IX = x} and rJl(X) = P{Y = IIX = x} 
are estimated from data separately by some rJO,n  and rJl,n,  respectively. Usually, a 
key part of proving consistency of classification rules is  writing the rules in one 
of the plug-in forms, and showing L1-convergence of the approximating functions 
to the a posteriori probabilities. Here we have  some freedom,  as  for any positive 
function  in (x), we have, for example, 

g  (x) _ {O 

n 

-

if rJl,n(X)  :s  rJO,n(x)  =  {  0 

1  otherwise, 

if l)l,n(X)  <  r]O,n(X) 
Tn(X) 

Tn(X) 

-

otherwise. 

94 

6.  Consistency 

6.3  Partitioning Rules 

Many important classification rules partition nd into disjoint cells AI, A 2 ,  ... and 
classify in each cell according to the majority vote  among the labels of the  Xi'S 
falling in the same cell. More precisely, 

(x) = {O 
gn 

1  otherwise, 

if :L7=1  I{Y;=l}I{x;EA(x)}  ~ :L7=1  I{y;=o}I{x;EA(x)} 

where  A(x) denotes the cell containing x. The decision is zero if the  number of 
ones does not exceed the number of zeros in the cell where X falls, and vice versa. 
The partitions we consider in this section may change with n, and they may also 
depend on the points XI, ... , X n, but we assume that the labels do not playa role 
in constructing the partition. The next theorem is a general consistency result for 
such partitioning rules. It requires two properties of the partition: first, cells should 
be small enough so that local changes of the distribution can be detected. On the 
other hand, cells should be large enough to contain a large number of points  so 
that averaging among the labels is effective. diam(A) denotes the diameter of a set 
A, that is, 

diam(A) =  sup  IIx - YII. 

x,yEA 

Let 

N(x) = n/Ln(A(x»  = L I{x;EA(x)} 

n 

i=1 

denote  the  number of Xi'S  falling  in the  same  cell  as  x. The  conditions  of the 
theorem below require that a random cell-selected according to the distribution 
of X -has a small diameter, and contains many points with large probability. 

Theorem 6.1.  Consider a partitioning classification rule as defined above. Then 
E{L n }  -+  L * if 

(1)  diam(A(X»  -+ 0 in probability, 
(2)  N (X)  -+  00 in probability. 

PROOF. Define 1J(x)  = P{Y =  llX = x}. From Corollary 6.1  we recall that we need 
only show EH1Jn(X)  - 1J(X)1}  -+ 0, where 

__ 
1Jn(x)  =  -

1 

'" 
L-
N(x)  . eX;EA(x) 

Yi • 

Introduce ij(x) = E{1J(X)IX  E  A(x)}. By the triangle inequality, 

E{lryn(X) - 1J(X)1}  ~ E{lryn(X) -

ij(X)1} + E{lij(X) - 1J(X)I}· 

By conditioning on the random variable N(x), it is easy to  see that N(x)ryn(x) is 
distributed as  B(N(x), ij(x», a binomial random variable with parameters N(x) 

and ij(x). Thus, 

6.4 The Histogram Rule 

95 

E{I71n(X)  -

ij(X)IIX, I{xl EA(X)},  ••• ,  I{XIlEA(X)}} 

I 

} 

::::  E 

{ I B(N(X), ij(X» 

N(X) 

+ I{N(x)=O} 

I 

-

- 11(X)  I{N(x»O}  X,  I{x 1EA(X)},  ••• ,  I{XIlEA(X)} 

ij(X)(l -

ij(X» 

N(X) 

I 

I{N(x»O}  X,  I{x]EA(X)} • .•• ,  I{XIlEA(X)} 

} 

::::  E 

{ 

+ I{N(x)=o} 

by the Cauchy-Schwarz inequality. Taking expectations, we see that 

E{I71n(X) -

ij(X)1} 

::::  E {  ~IIN(X»OI} + P{N(X) = O} 

2  N(X) 

1 
iP{N(X):::: k} + 2.jk + P{N(X) = O} 

1 

:::: 

for any k,  and this can be made small, first by choosing k large enough and then 
by using condition (2). 

For E  >  0, find a uniformly continuous [0,  I]-valued function 11E  on a bounded 
set C  and vanishing off C  so that E {111E(X)  - 11(X)1}  <  E. Next, we employ the 
triangle inequality: 

E {lij(X) - 11(X)l} 

::::  E {lij(X) -

ijE(X)1} 

+E {lijE(X) -11E(X)1} 
+ E {111E(X)  - 11(X)\} 
I  + I  I  + I  I  I, 

where ijE(X)  = E{11AX)IX  E  A(x)}. Clearly,  I  I  I  <  E  by choice of 11E'  Since 11E  is 

uniformly continuous, we can find a 8 = 8(E)  > ° such that 

I I  ::::  E  + P{diam(A(X» >  8}. 

Therefore,  I  I  <  2E  for  n  large enough, by condition (l). Finally,  I 
Taken together these steps prove the theorem.  0 

::::  I  I  I  <  E. 

6.4  The Histogram Rule 

In this  section we describe the cubic histogram rule  and show its universal con(cid:173)
sistency by checking the conditions of Theorem 6.1. The rule partitions Rd  into 

96 

6.  Consistency 

cubes of the same size, and makes a decision according to the majority vote among 
the Y/s such that the corresponding Xi  falls in the same cube as  X. Formally, let 
P n  =  {AnI,  A n2 ,  ... } beapartitionofRd into cubes ofsizeh n  >  0, that is, into sets 
of the type nf=1 [kih n ,  (ki  + 1)hn ),  where the k/s are integers. For every x  E  Rd 
let An (x) =  Ani  if X  E  Ani. The histogram rule is defined by 

()  { 
gn  x  = 

0  if I:7=1  I{Yi=l}I{XiEAn(X)}  ::::;  I:7=1  I{yi=o}I{XiEAn(x)} 
1  otherwise. 

FIGURE 6.1.  A cubic histogram rule: 
The  decision is  1 in the shaded area. 

oe 

0 

e 

o 

Consistency of the histogram rule was established under some additional con(cid:173)

ditions by Glick (1973). Universal consistency follows from the results of Gordon 
and Olshen (1978), (1980). A direct proof of strong universal consistency is given 
in Chapter 9. 

The next theorem establishes universal consistency of certain cubic histogram 

rules. 
Theorem 6.2.  If hn 
rule is universally consistent. 

---7>- 0 and nh~ ---7>- 00 as n  ---7>- 00,  then  the  cubic histogram 

PROOF. We check the two simple conditions of Theorem 6.1. Clearly, the diameter 
of each cell is -Y'dhd. Therefore condition (1) follows trivially. To show condition 
(2),  we  need to  prove that for  any  M  <  00,  P{N(X)  :::  M}  ---7>- O.  Let S be an 
arbitrary ball centered at the origin. Then the number of cells intersecting S is not 
more than CI  + c21 hd  for some positive constants CI, C2.  Then 

P{N(X)::::;  M} 

<  L  P{X E A nj ,  N(X) ::::;  M} + P{X ESC} 

j:A nj nS=j0 

< 

j:A nj nS10 

fJ.,(A nj ):s2Mjn 

j:A nj nS10 

fJ.,(A nj »2Mjn 

6.5 Stone's Theorem 

97 

j:A,,/lSi 0 

fl,(A nj »2M/n 

(by Chebyshev's inequality) 

j:A nj nSi 0 

fl,(A nj »2M/n 

C2) 

2M + 4  ( 
--n- Cl  + hd  + !L(S  ) 
!L(SC), 

C 

< 

< 

because nh d  --+  00. Since S is arbitrary, the proof of the theorem is complete.  0 

6.5  Stone's Theorem 

A general theorem by Stone (1977) allows  us  to deduce universal consistency of 
several classification rules. Consider a rule based on an estimate of the a posteriori 
probability 1]  of the form 

n 

1]n(x) = L I{Yi=l} Wni(x) = LYi Wni(x), 

n 

where the weights Wni(x)  = Wni(x,  Xl, ... , Xn) are nonnegative and sum to one: 

i=l 

i=l 

i=l 
The classification rule is defined as 

gn  X 

(  ) 

0 

{ 

= 
=  {O 

if L7::::1  I{Yi=l} Wni(x)  :::  L7=1  I{yi=o} Wni(x) 
otherwise, 
if L7=1  Yi Wni(x)  :::  1/2 
otherwise. 

98 

6.  Consistency 

17n  is  a weighted average estimator of 17.  It is  intuitively clear that pairs  (Xi,  Yi ) 
such that Xi is close to x should provide more information about 17 (x ) than those far 
from x. Thus, the weights are typically much larger in the neighborhood of X, so 
17n  is roughly a (weighted) relative frequency of the Xi'S that have label 1 among 
points  in the  neighborhood of X.  Thus,  17n  might be  viewed as  a  local average 
estimator, and gn  a local (weighted) majority vote. Examples of such rules include 
the histogram, kernel,  and nearest neighbor rules.  These rules  will be studied in 
depth later. 

Theorem 6.3. 
satisfy the following three conditions: 

(STONE (1977».  Assume that for any distribution of X, the weights 

(i)  There is a constant c such that,for every nonnegative measurable function 

f  satisfying Ef(X) <  00, 

E {t Wni(X)f(X,)}  S cEf(X). 

(ii)  For all a  >  0, 

(iii) 

lim  E I m~x Wni(X)}  = 0. 

1:9:::n 

n---+oo 

Then gn  is universally consistent. 

REMARK.  Condition  (ii)  requires  that  the  overall  weight  of X/s  outside  of any 
ball of a fixed radius centered at X  must go  to zero.  In other words,  only points 
in a shrinking neighborhood of X  should be taken into account in the averaging. 
Condition (iii) requires that no single Xi has too large a contribution to the estimate. 
Hence,  the number of points encountered in the averaging must tend to infinity. 
Condition (i) is technical.  0 

PROOF.  By Corollary 6.2 it suffices to show that for every distribution of (X, Y) 

Introduce the notation 

lim  E {(17(X)  - 17n(X»2}  = 0. 
n---+oo 

n 

r]n(x) = L 17(Xi )Wni (X). 

i=l 

6.5 Stone's Theorem 

99 

Then by the simple inequality (a + b)2  :::  2(a2 + b2) we have 

E {(ry(X)  - ryn(X»2} 

=  E {((ry(X)  - iln(X»  + (li'n(X)  - ryn(X»)2} 

:::  2 (E {(ry(X)  - Tfn(X»2}  + E {(Tfn(X)  - ryn(X»2}) . 

(6.1) 

Therefore, it is enough to show that both terms on the right-hand side tend to zero. 
Since the  Wni'S  are nonnegative and sum to one, by Jensen's inequality, the first 
term is 

E { (t Wni(X)(~(X) - ~(Xi)) r } 
:s:  E { t Wni(X)(~(X) - ~(Xi)l" }  . 

If the function 0  :::  ry*  :::  1 is continuous with bounded support, then it is uniformly 
continuous as well: for every E  >  0, there is an a  >  0 such that for IlxI - x II  <  a, 
Iry*(xd _ry*(x)1 2 <  E. Recall here that Ilxli denotes the Euc1idean norm ofa vector 
x  E  Rd. Thus, since Iry*(xd - ry*(x)/  :::  1, 

E { t Wni(X)(~'(X) - ry'(Xi»' } 
:s:  E {t Wn;(X)J(IIX-X;lI~a) }  + E {t Wni(X)E}  --+  E, 

by  (ii).  Since the  set of continuous  functions  with bounded support is  dense  in 
L2(/L), for every E  >  0 we can choose ry*  such that 

E {(ry(X)  - ry*(X»2}  <  E. 

By this choice, using the inequality (a + b + c? :::  3(a2 + b2 + c2) (which follows 
from the Cauchy-Schwarz inequality), 

E {(ry(X)  - Tfn(X)/} 

:s:  E { t Wni(X)(ry(X) -
:s:  3E {t Wni(X) (~(X) - ~'(X»2 + (ry'(X) - ~'(Xi»2 

ry(Xi ))2 } 

+ (ry*(Xi)  - ry(Xd)2)} 

:::  3E {(ry(X)  - ry*(X»2} 

+3E {t Wni(X)(~'(X) - ry'(Xi»2} + 3cE {(~(X) - ~*(X»2}. 

100 

6.  Consistency 

where we used (i). Therefore, 

lim sup E {(1J(X)  -1h7(X))2}  ::::  3E(1 + 1 + c). 
n-+oo 

To handle the second term of the right -hand side of (6.1), observe that 

by independence. Therefore, 

E {(1h7(X)  - 1Jn(X))2} 

=  E  { (t Wn;(X)(ry(Xi) - Yi)r} 
n L LE {Wni (X)(1J(Xi ) - Yi )Wnj (X)(1J(X j ) - Yj )} 

n 

i=l  j=l 

n 

i=l 

=  LE {W;i(X)(1J(Xi ) - Yi)2} 
<  E {t w.;,eX)}  ::0  E {1~i':;, Wni(X) ~ Wnj(X)} 
E {1~~;" Wn,(X)}  -+ 0 

by (iii), and the theorem is proved.  0 

6.6  The k-Nearest Neighbor Rule 

In  Chapter 5  we  discussed  asymptotic  properties  of the  k-nearest neighbor rule 
when  k  remains  fixed  as  the  sample  size  n  grows.  In  such  cases  the  expected 
probability of error converges  to  a number between  L * and 2L *.  In this  section 
we show that if k is allowed to grow with n such that kin --+  0, the rule is weakly 
universally consistent. The proof is a very simple application of Stone's theorem. 
This result, appearing in Stone's paper (1977), was the first universal consistency 
result  for  any  rule.  Strong  consistency,  and  many  other different  aspects  of the 
k-NN rule are studied in Chapters 11  and 26. 

Recall the  definition of the  k-nearest neighbor rule:  first  the  data are  ordered 

according to increasing Euclidean distances of the X j  , s to x: 

6.7 Classification Is Easier Than Regression Function Estimation 

101. 

that  is,  X(i)(x)  is  the  i-th nearest neighbor  of x  among  the  points  Xl, ... , X n . 
Distance  ties  are  broken by  comparing  indices,  that is,  in  case  of II Xi  - x II 
"X j  - x II,  Xi  is considered to be "closer" to x  if i  <  j. 

The k-NN classification rule is defined as 

gn (x) = {O 

if L~=l. I{Y(i)(x)=l}  :::;  L~=l I{Y(i)(x)=O} 

1  otherwIse. 

In other words, gn (x) is a majority vote among the labels of the k nearest neighbors 
of x. 

Theorem 6.4.  (STONE (1977».  Ifk -+  00 and k/n -+ 0, thenforall distributions 
ELn-+ L*. 

PROOF. We proceed by checking the conditions of Stone's weak convergence theo(cid:173)
rem (Theorem 6.3). The weight Wni(X) in Theorem 6.3 equals 1/ k iff Xi  is among 

the k nearest neighbors of X, and equals ° otherwise. 

Condition (iii) is obvious since k -+  00. For condition (ii) observe that 

holds whenever 

p  {IIX(k)(X) - XII  >  E}  -+ 0, 

where X(k)(X)  denotes the k-th nearest neighbor of x  among Xl, ... , X n • But we 

know from Lemma 5.1  that this is true for all E  > ° whenever k/n -+ 0. 

Finally, we  consider condition (i).  We  have  to  show  that for  any  nonnegative 

measurable function  f  with E{f(X)}  <  00, 

E {t ~I(Xi is among the k nearest neighbors of  X}f(Xd}  :::;  E {cf(X)} 

1=1 

for some constant c. But we have shown in Lemma 5.3 that this inequality always 
holds with c = Yd.  Thus, condition (i) is verified.  0 

6.7  Classification Is Easier Than Regression 

Function Estimation 

Once again assume that our decision is based on some estimate 17n  of the a posteriori 
probability function 17,  that is, 

(X)={  ° if17n(x):::;1/2 

1  otherwise. 

gn 

102 

6.  Consistency 

The  bounds  of Theorems  2.2,  2.3,  and  Corollary  6.2  point  out  that  if 1}n  is  a 
consistent estimate of 1},  then the  resulting rule is  also consistent.  For example, 
writing Ln  = P{gn(X) i  YIDn},  we have 

that is,  L 2-consistent estimation of the regression function  1}  leads  to  consistent 
classification, and in fact,  this is the main tool used in the proof of Theorem 6.3. 
While the said bounds are useful for proving consistency, they are almost useless 
when it comes to studying rates of convergence. As Theorem 6.5 below shows, for 
consistent rules rates of convergence of P{gn (X) i  Y} to  L * are always orders of 
magnitude better than rates of convergence of JE {(1}(X)  - 1}n(X»2}  to zero. 

112 ___________________ ... 

o========~ __________________________________________ _ 
x 

FIGURE 6.2.  The difference between the error probabilities grows roughly 
in  proportion  to  the  shaded area.  Elsewhere  1}n(x)  does  not need to  be 
close 1}(x). 

Pattern recognition is thus easier than regression function estimation. This will 
be a recurring theme-to achieve acceptable results in pattern recognition, we can 
do more with smaller sample sizes than in regression function estimation. This is 
really just a consequence of the fact  that less is  required in pattern recognition. 
It also  corroborates  our  belief that  pattern  recognition  is  dramatically  different 
from regression function estimation, and that it deserves separate treatment in the 
statistical community. 

Theorem 6.5.  Let 1}n  be a weakly consistent regression estimate,  that is, 

Define 

lim  E {(1Jn(X) 
n-+oo 

1}(X»2}  = O. 

gn(x) = 

{ 

0 

if1}n(x)::::1/2 
otherwise. 

6.7 Classification Is Easier Than Regression Function Estimation 

103 

Then 

EL  -L* 

lim 
n-+oo )E {(fln(X) -

n 

= 0, 

fl(X)?} 

that  is,  ELn  - L * converges  to  zero faster  than  the  L 2-error of the  regression 
estimate. 

PROOF.  We start with the equality of Theorem 2.2: 

ELn - L * = 2E {lfl(X) - 1/2II{gn(X)¥g*(X)}}  . 

Fix E  >  O.  We may bound the last factor by 

E {lfl(X) - 1/2I I {gn(X)¥g*(x)d 

:::  E {I{1J(X)i1/2}lfl(X) -

fln(X)II{g,,(x)¥g*(X)}} 

E {lfl(X) -
+ E {lfl(X) -
<  )E {(rJn(X)  -

fln(X)1 I{gn(X)¥g*(X)} I{I1J(X)-1/21::::E}I{1J(X)il/2} } 

fln(X)II{g,,(x)¥g*(x)}I{I1J(X)-1/21>Ed 

fl(X)?} 

x  ( Jp{lfl(X) - 1/21  :::  E,  fleX) =jl/2} 

+ Jp{gn(X) =j g*(X), Ifl(X) -1/21> E}) 

(by the Cauchy-Schwarz inequality). 

Since  gn(X)  =j  g*(X)  and  Ifl(X) - 1/21  >  E  imply  that  IrJn(X)  -
consistency of the regression estimate implies that for any fixed  E  >  0, 

fl(X)1  >  E, 

lim  P{gn(X) =j g*(X), Ifl(X) - 1/21  >  E}  = O. 
n-+oo 

On the other hand, 

P{lfl(X) - 1/21  :::  E,  fleX)  =j1/2}  ---+  0  as  E  ---+  0, 

which completes the proof.  0 

The actual value of the ratio 

ELn - L* 

Pn  =  --,======== 
fl(X)?} 

)E {(rJn(X)  -

cannot  be  universally  bounded.  In fact,  Pn  may  tend  to  zero  arbitrarily  slowly 
(see Problem 6.5).  On the other hand,  Pn  may tend to  zero extremely quickly.  In 
Problems 6.6 and 6.7 and in the theorem below, upper bounds on Pn  are given that 

104 

6.  Consistency 

may be used in deducing rate-of-convergence results. Theorem 6.6, in particular, 
states  that ELn  - L * tends  to  zero  as  fast  as  the  square  of the  L2  error of the 
regression estimate, i.e., E {(1]n(X)  - 1](X))2}, whenever L* = O.  Just how slowly 
Pn  tends to  zero depends upon two things, basically:  (1) the rate of convergence 
of 1]n  to 1],  and (2) the behavior of P{I1](X) - 1/21  ::::  E, 1](X) ::jl/2} as a function 
of E  when E  {- 0 (i.e., the behavior of 1](x) at those x's where 1](x)  ~ 1/2). 

Theorem 6.6.  Assume that L * = 0,  and consider the decision 

(x) = {O 
gn 

if1]n(x)::::  1/2 

1  otherwise. 

Then 

P{gn(X) ::j Y}  ::::  4 E  {(1]n(X)  - 1](X))2} . 

PROOF. By Theorem 2.2, 

P{gn(X)::j Y}  =  2E {11](X) 

1/2II{gn(X)=tg*(x)d 

=  2E {11](X)  - 1/2/I{gn(X)=tY}} 

(since g*(X) = Y by the assumption L * = 0) 

<  2JE {(1]n(X)  - 1](X))2 h/P{gn(X) ::j Y} 

(by the Cauchy-Schwarz inequality). 

Dividing both sides by ,jP{gn(X) ::j Y} yields the result.  0 

The results above show that the bounds of Theorems 2.2, 2.3, and Corollary 6.2 
may be arbitrarily loose, and the error probability converges to L * faster than the 
L 2-error of the regression estimate converges to zero. In some cases, consistency 
may  even occur without convergence of EI1]n(X)  - 1](X)1  to  zero.  Consider for 
example a strictly separable distribution, that is, a distribution such that there exist 
two sets A, Bend with 

inf 

xEA,yEB 

/Ix 

y II  ~ 8 >  0 

for some 8 >  0, and having the property that 

P{X  E  AIY =  I}  =P{X E  BIY =O}  = 1. 

In such cases, there is  a version of 1]  that has  1] (x )  =  1 on A  and 1] (x )  = 0 on B. 
We say version because 1]  is not defined on sets of measure zero. For such strictly 
separable distributions,  L * = O.  Let ij be 1/2 - E  on Band 1/2 + E  on A. Then, 
with 

g(x) = 

{ 

if ij(x)  ::::  1/2 

0 
1  otherwise, 

{O 
1 

= 

if x  E  B 
if x  E  A, 

6.7 Classification Is Easier Than Regression Function Estimation 

105 

we have P{g(X) -=I  Y} = L * = O.  Yet, 

2Elry(X) -ry(X)1 =  1 - 2E 

is  arbitrarily close to one. 

In a more realistic example, we consider the kernel rule (see Chapter 10), 

gn (x) = 

{ 

ifryn(x)::::  1/2 

0 
1  otherwise, 

in which 

where K  is the standard normal density in nd

: 

K(u) =  __ I_ e- IIU \\2/2. 

(2n)d/2 

Assume that A and B consist of one point each, at distance 0 from each other-that 
is, the distribution of X is concentrated on two points. If P{Y = O}  = P{Y = I}  = 
1/2, we see that 

lim  - L K(x - Xi) = 

1  n 
11--+00  n  i:::1 

K(O) + K(o) 

2 

with probability one 

at x  E  A U B, by the law of large numbers. Also, 

K(0)/2 
K(o)/2 

if x  E  A 
if x  E  B 

with probability one. 

Thus, 

lim  ryn(x)  = 

if x  E  A 
if x  E  B 
Hence, as  ry(x)  =  Ion A and ry(x) = 0 on B, 

n--+oo 

K(O) 

K(<jt(8~(8) 
K(O)+K(8) 

{ 

with probability one. 

lim  2Elry(X) -
n--+oo 

ryn(X)1 

= 

K(O) + K(8) 

Yet,  L * = 0 and P{gn(X) -=I Y}  ~ O.  In fact,  if Dn  denotes the training data, 

lim  P{gn(X) -=I  YIDn} = L *  with probability one, 

n--+oo 

106 

6.  Consistency 

and 

}l~ 2E  IlJn(X) -

{ 

lJ(X)1  Dn  =  K(O) + K(8)  with probability one. 

I}  2K(8) 

This shows very strongly that for any 8 >  0, for many practical classification rules, 
we do not need convergence of lJn  to lJ  at all! 

As all the consistency proofs in Chapters 6 through 11  rely on the convergence 
of lJn  to lJ, we will create unnecessary conditions for some distributions, although it 
will always be possible to find distributions of (X, Y) for which the conditions are 
needed-in the latter sense, the conditions of these universal consistency results 
are not improvable. 

6.8  Smart Rules 
A rule is a sequence of mappings gn  : Rd  x  (Rd  X  to, l}f -+ to,  I}. Most rules 
are expected to perform better when n increases. So, we say that a rule is smart if 
for all distributions of (X, Y), E{L(gn)} is nonincreasing, where 

Some dumb  rules  are  smart,  such  as  the  (useless)  rule  that,  for  each n,  takes  a 
majority over all Yi 's, ignoring the Xi'S. This follows from the fact that 

p  { t (2Yi - 1)  >  0, Y  = 0 or  t(2Yi - 1)  :'0  0, Y =  1 } 

is  monotone  in n.  This  is  a  property  of the  binomial  distribution  (see  Problem 
6.12).  A  histogram  rule  with  a  fixed  partition  is  smart  (Problem  6.13).  The  1-
nearest neighbor rule is not smart. To see this, let (X, Y) be (0,  1) and (Z, 0) with 
probabilities  p  and  1 - p, respectively,  where  Z  is  uniform on [-1000, 1000]. 
Verify that for n = 1, ELn = 2p(1  - p), while for n = 2, 

2p(1- p)2 (~+ EIZI) + p2(1- p)+(1- p)2p 

2 

4000 

=  2p(1  - p) 

(

5(1  - p)  1) 
8  +"2' 

which  is  larger than 2p(l  - p) whenever  p  E  (0,  1/5).  This  shows  that  in all 
these cases it is  better to  have n  =  1 than n  =  2.  Similarly,  the  standard kernel 
rule-discussed in  Chapter  10-with fixed  h  is  not  smart  (see  Problems  6.14, 
6.15). 

The  error  probabilities  of the  above  examples  of smart  rules  do  not  change 

dramatically with n. However, change is necessary to guarantee Bayes risk consis(cid:173)
tency. At the places of change-for example when hn jumps to a new value in the 
histogram rule-the monotonicity may be lost.  This leads  to  the conjecture that 
no universally consistent rule can be smart. 

Problems and Exercises 

107 

Problems and Exercises 

PROBLEM 6.1.  Let the i.i.d.  random variables  Xl, ... , Xn  be distributed on Rd  according 
to  the  density  f.  Estimate  f  by  fn'  a· function  of x  and  Xl, ... , X n,  and  assume  that 
f I fn (x) - f  (x) Idx  -+ 0 in probability (or with probability one). Then show that there exists 
a consistent (or strongly consistent) classification rule whenever the conditional densities 
fa  and  fl  exist. 

PROBLEM 6.2.  HISTOGRAM DENSITY ESTIMATION.  Let Xl, ... , Xn  be i.i.d. random variables 
in  Rd  with  density  f. Let  P n  be a partition of Rd  into  cubes  of size  hn'  and define  the 
histogram density estimate by 

where An (x) is the set in P n that contains x. Prove that the estimate is universally consistent 
in Ll if hn  -+ 0 and nh~ -+  00 as n  -+  00, that is, for any f  the Ll error of the estimate 
f Ifn(x) -

f(x)ldx converges to zero in probability, or equivalently, 

!l~ E {f Ifn(x) -

f(X)ldX}  = O. 

HINT:  The following suggestions may be helpful 

fl}  :s E {f Ifn  - Efnl} + f  IEfn  -

fl. 

(1)  E {f Ifn  -
(2)  E {f Itl - Efn I} = Lj 1f-i(Anj ) -
(3)  First show f IEfn  -
arbitrary densities. 

f-in(Anj)l. 

fl  -+  0 for uniformly continuous  f, and then extend it to 

PROBLEM 6.3.  Let  X  be  uniformly  distributed  on  [0,  1]  with probability  1/2,  and  let  X 
be  atomic  on  the  rationals  with  probability  1/2  (e.g.,  if  the  rationals  are  enumerated 
rl, r2, r3,""  then P{X  = rd  = I/2i +1
).  Let  Y  = 1 if X  is  rational  and  Y  = 0  if X  is 
irrational.  Give a direct proof of consistency of the  I-nearest neighbor rule.  HINT:  Given 
Y  = 1,  the  conditional distribution  of X  is  discrete.  Thus,  for  every  E  >  0,  there is  an 
integer k such that given Y = 1, X equals one of k rationals with probability at least I-E. 
Now,  if n  is  large  enough,  every  point in this  set captures  data points  with label  I  with 
large probability. Also, for large n, the space between these points is filled with data points 
labeled with zeros. 

PROBLEM 6.4.  Prove the consistency of the cubic histogram rule by checking the conditions 
of Stone's theorem. HINT:  To  check (i), first bound Wni(x) by 

Since 

n 

j=l 

I{XiEAn(X)}/ L I{xjEAn(x)} + lin. 
E {t ~f(Xi)} ~ Ef(X), 

it suffices to show that there is a constant c'  >  0 such that for any nonnegative function f 
with Ef(X) <  00, 

108 

6.  Consistency 

In doing so, you may need to use Lemma A.2 (i). To prove that condition (iii) holds, write 

and use Lemma A.2 (ii). 

PROBLEM 6.S.  Let {an} be a sequence of positive numbers converging to zero. Give an exam(cid:173)
ple of an a posteriori probability function 1], and a sequence of functions {1]n}  approximating 
1]  such that 

P{gn(X) =I y} - L* 

.  f 
· 
1m In 
1
11-+00  anJE {(1]n(X)  -

0 
, 

> 

1] (X))2 } 

where 

gn(X) = { ~  if 1]n(X)  ::::  1/2 

otherwise. 

Thus,  the rate of convergence in Theorem 6.S  may be arbitrarily  slow.  HINT:  Define  1]  = 
1/2 + hex), where hex) is a very slowly increasing nonnegative function. 

PROBLEM 6.6.  Let 0 >  0, and assume that 11](x) - 1/21  :::  0 for all x. Consider the decision 

if 1]n(x)  ::::  1/2 
otherwise. 

Prove that 

P{gn(X) =I y} _  L * ::::  2E {(1]n(X~ - 1](X))2} . 

This shows that the rate of convergence implied by the inequality of Theorem 6.6 may be 
preserved for very general classes of distributions. 
PROBLEM 6.7.  Assume that L * = 0, and consider the decision 

Show that for all 1 ::::  p  <  00, 

if 11n(X)  ::::  1/2 
otherwise. 

HINT:  Proceed as in the proof of Theorem 6.6, but use Holder's inequality. 

PROBLEM 6.8.  Theorem 6.S  cannot be generalized to  the  Ll  error.  In particular,  show by 
example that it is not always true that 

EL  -L* 

lim 
n---+oo  E {111n(X)  - 11(X)I} 

n 

= 0 

when E {/1]n(X)  - 1](X)j}  -+ 0 as 11  -+  00 for some regression function estimate 11n.  Thus, 
the inequality (Corollary 6.2) 

ELn  - L * ::::  2E {11]n(X)  - 11(X)I} 

cannot be universally improved. 

Problems and Exercises 

109 

PROBLEM  6.9.  Let 1]'  : Rd  -+  [0,  1], and define g(x) = I{rl'(x»1/2j. Assume that the random 
that E{j1l(X) - 1]n(X)I}  -+ ° as n  -+ 00. Prove that L(gn)  -+  L(g) for all distributions of 
variable X satisfies that P{1]'(X) = 1/2} = 0. Let 1]1,  1]2,  ... be a sequence of functions such 
(X, y) satisfying the condition on X above,where gn(x) = I{lln(X»lj2}  (Lugosi (1992)). 

PROBLEM  6.10.  A LYING TEACHER.  Sometimes the training labels  Yj, ... , Yn  are not avail(cid:173)
able, but can only be observed through a noisy binary channel. Still, we want to decide on 
Y.  Consider the following model. Assume that the Yi ' s in the training data are replaced by 
the i.i.d. binary-valued random variables Zi, whose distribution is given by 
P{Zi  =  llYi  = O}  = p  <  1/2, 
P{Zi  = 0IYi  = I}  = q  <  1/2. 

P{Zi  = llYi = 0,  Xi  = x} 
P{Zi = OIYi  = 1, Xi  = x} 

Consider the decision 

g(x) = { ~ 
where 1]'(x) = P{ZI = I\XI = x}. Show that 
P{g(X) =I Y} :s  L * (1 + 

if 1]'(x) :s  1/2 
otherwise, 

21p - q I 

)  . 

1 - 2max(p, q) 

Use Problem 6.9 to conclude that if the binary channel is symmetric (i.e.,  p  = q  <  1/2), 
and P {1]' (X) = 1/2} = 0,  then L I-consistent estimation leads to a consistent rule, in spite 
of the fact that the labels Yi  were not available in the training sequence (Lugosi (1992)). 

PROBLEM 6.11.  Develop a discrimination rule which has the property 

lim  ELn = p = E  {J1](X)(1  - 1](X))} , 

n-+oo 

for all distributions such that X  has a density. Note: clearly,  since p  :::::  L *,  this rule is not 
universally consistent, but it will aid you in "visualizing" the Matushita error! 

PROBLEM  6.12.  If Zn  is  binomial (n, p) and  Z  is  Bernoulli (p), independent of Zn,  then 
show that P{Zn  >  n12,  Z = O} + P{Zn  :s n12,  Z  = 1} is nonincreasing in n. 
PROBLEM  6.13.  Let gn  be the histogram rule based on a fixed partition P.  Show that gn  is 
smart. 
PROBLEM 6.14.  Show that the kernel rule  with gaussian kernel  and h  =  1,  d  = 1,  is  not 
smart (kernel rules are discussed in Chapter 10). HINT:  Consider n = 1 and n = 2 only. 
PROBLEM  6.15.  Show that the kernel rule on R, with K(x) = I[-l,lj(X),  and h  t  0,  such 
that nh -+  00, is not smart. 

PROBLEM 6.16.  Conjecture: no universally consistent rule is smart. 

:  Rd  x  (Rd  X  {O,  l} r is  called  symmetric  if gn (x,  Dn) 

PROBLEM  6.17.  A  rule  gn 
gn (x,  D~) for every x, and every training sequence Dn , where D~ is an arbitrary permutation 
of the pairs (Xi, Yi ) in Dn. Any nonsymmetric rule gn  may be symmetrized by taking a ma(cid:173)
jority vote at every x  E  Rd over all gn(x,  D~), obtained by then! permutations of Dn. It may 
intuitively be expected that symmetrized rules perform better. Prove that this is false, that is, 
exhibit a distribution and a nonsymmetric classifier gn  such that its expected probability of 
error is smaller than that of the symmetrized version of gn' HINT:  Take g3(X,  D3) = 1 - Y1. 

7 
Slow Rates of Convergence 

In  this  chapter  we  consider the  general  pattern recognition problem:  Given  the 

observation  X  and the  training  data  Dn  =  ((Xl, YI ),  ... , (Xn, Yn» of indepen(cid:173)
dent identically distributed random variable pairs, we estimate the label Y by the 
decision 

The error probability is 

Obviously,  the  average  error  probability  ELn  =  P{Y  =I  gn(X)}  is  completely 
determined by the  distribution of the pair (X, Y),  and the classifier gn'  We  have 
seen in Chapter 6 that there exist classification rules such as  the cubic histogram 
rule with properly chosen cube sizes such that limn---+ oo  ELn  = L * for all possible 
distributions. The next question is whether there are classification rules with ELn 
tending to the Bayes risk at a specified rate for all distributions. Disappointingly, 
such rules do not exist. 

7.1  Finite Training Sequence 

The first  negative result shows that for any classification rule and for any fixed n, 
there exists a distribution such that the difference between the error probability of 
the rule and L * is  larger than  1/4. To  explain this,  note  that for fixed  n,  we can 
find a sufficiently complex distribution for which the sample size n is hopelessly 
small. 

112 

7.  Slow Rates of Convergence 

Theorem 7.1.  (DEVROYE  (1982B)).  Let E  > ° be  an arbitrarily  small number. 
with Bayes risk L * = ° such that 

For any integer n and classification  rule  gn,  there  exists a distribution  of (X, Y) 

ELn::::  1/2 - E. 

PROOF.  First we construct a family of distributions of (X, Y). Then we show that 
the error probability of any classifier is large for at least one member of the family. 
For every member of the family, X is uniformly distributed on the set {I, 2,  ... , K} 
of positive integers 

. = P{X = i} = {  1/ K 
PI 

°  otherwIse, 

if i  E  {.1,  ... , K} 

where K  is a large integer specified later. Now, the family of distributions of (X, Y) 
is parameterized by a number b  E [0,  1), that is, every b determines a distribution 
as  follows.  Let  b  E  [0,  1) have  binary expansion  b  = 0.bob1b2 ... , and  define 
Y = bx . As the label Y is a function of X, there exists a perfect decision, and thus 
L * = 0. We show that for any decision rule gn  there is a b such that if Y  = bx , then 
gn  has very poor performance. Denote the average error probability corresponding 
to the distribution determined by b,  by  Rn(b) = ELn. 

The proof of the existence of a bad distribution is based on the so-called prob(cid:173)

abilistic method.  Here the key trick is  the randomization of b.  Define a random 
variable  B  which  is  uniformly  distributed  in  [0,  1)  and  independent  of X  and 
Xl, ... , X n .  Then we may  compute the  expected value  of the  random  variable 
Rn(B). Since for any decision rule gn, 

sup  Rn(b)::::  E{Rn(B)}, 
bE[O,I) 

a  lower bound for  E{Rn(B)}  proves  the  existence of abE  [0,  1)  whose corre(cid:173)
sponding error probability exceeds the lower bound. 

Since B  is  uniformly distributed in [0,  1), its binary extension  B  = 0.BIB2'" 

is  a sequence of independent binary random variables with P{Bi  = O}  = P{Bi = 
I}  = 1/2. But 

E{Rn(B)} 

=  P {gn(X,  Dn) =I Bx} 

P {gn(X,  Xl, BXl ,  .. ,  ,  Xn, Bx,J =I Bx} 

=  E {p {gn(X, Xl, Bx !'  •.. ,  X n ,  Bx,J =I Bx \ X,  Xl,""  Xn}} 
>  2P {X =I Xl, X =I X 2,.·., X =I  Xn}, 

1 

since  if X  =I  Xi  for  all  i  =  1,2, ... , n,  then  given  X,  Xl, ""  X n ,  Y  =  Bx  is 

conditionally independent of gn(X,  Dn) and Y takes values ° and 1 with probability 

7.2 Slow Rates 

113 

1/2. But clearly, 

P{X i  Xl, X  i  X 2 ,  .. ·, X  i  Xnl  X} = P{X i  XIIX}n = (1- I/Kt. 

In summary, 

sup  Rn(b)::::  -(1 - 1/ Kt. 
hE[O,l) 

1 
2 

The lower bound tends to  1/2 as  K  -+  00.  D 

Theorem 7.1 states that even though we have rules that are universally consistent, 
that is, they asymptotically provide the optimal performance for any distribution, 
their finite  sample  performance is  always  extremely bad for  some distributions. 
This means that no classifier guarantees that with a sample size of (say) n  =  108 
we get within 1/4 of the Bayes error probability for all distributions. However, as 
the bad distribution depends upon n, Theorem 7.1  does not allow us to conclude 
that there is one distribution for which the error probability is more than L * + 1/4 
for all n. Indeed, that would contradict the very existence of universally consistent 
rules. 

7.2  Slow Rates 

The  next  question  is  whether  a  certain  universal  rate  of convergence  to  L * is 
achievable  for  some classifier.  For example,  Theorem 7.1  does  not exclude  the 
existence of a classifier such that for every n, ELn - L * ::::  c / n for all distributions, 
for some constant c depending upon the actual distribution. The next negative result 
is that this cannot be the case. Theorem 7.2 below states that the error probability 
ELn  of any  classifier is  larger than  (say)  L * + c/(log log log n) for  every  n  for 
some  distribution,  even  if c  depends  on the  distribution.  (This  can be  seen by 
considering  that by Theorem 7.2,  there  exists  a distribution of (X, Y)  such  that 
ELn  ::::  L * + 1/ -/log log log n  for  every  n.)  Moreover,  there  is  no  sequence  of 
numbers an  converging to  zero  such that there is  a classification rule with error 
probability below L * plus an  for all distributions. 

Thus,  in practice,  no  classifier assures  us  that its  error probability is  close  to 
L *, unless the actual distribution is known to be a member of a restricted class of 
distributions. Now, it is easily seen that in the proof of both theorems we could take 
X  to  have uniform distribution on [0,  1], or any other density (see Problem 7.2). 
Therefore,  putting restrictions  on the distribution of X  alone does  not suffice to 
obtain rate-of-convergence results. For such results, one needs conditions on the a 
posteriori probability 17 (x ) as well. However, if only training data give information 
about the joint distribution, then theorems with extra conditions on the distribution 
have little practical value,  as  it is  impossible to  detect whether,  for example, the 
a posteriori probability 1J(x) is twice differentiable or not. 

Now, the situation may look hopeless, but this is not so.  Simply put, the Bayes 

error is too difficult a target to shoot at. 

114 

7.  Slow Rates of Convergence 

Weaker versions of Theorem 7.2 appeared earlier in the literature. First Cover 
(1968b)  showed that for any sequence of classification rules, for  sequences  {an} 
converging to  zero at arbitrarily slow algebraic rates  (i.e.,  as  11no for  arbitrarily 
small 8  >  0),  there exists a distribution such that ELn  :::  L * + an  infinitely often. 
Devroye (1982b) strengthened Cover's result allowing sequences tending to zero 
arbitrarily slowly. The next result asserts that ELn  >  L * + an  for every n. 

Theorem 7.2.  Let {an} be a sequence of positive numbers converging to zero with 
1 116  :::  a I  :::  a2  :::  .... For every sequence of classification rules,  there  exists a 
distribution of (X, Y) with L * = 0,  such that 

for all n. 

This result shows  that universally  good classification rules  do not exist.  Rate 
of convergence studies for particular rules  must necessarily be accompanied by 
conditions on (X, Y). That these conditions too are necessarily restrictive follows 
from examples suggested in Problem 7.2. Under certain regularity conditions it is 
possible to obtain upper bounds for the rates of convergence for the probability of 
error of certain rules to L *. Then it is natural to ask what the fastest achievable rate 
is for the given class of distributions. A theory for regression function estimation 
was worked out by Stone (1982). Related results for classification were obtained 
by Marron (1983). In the proof of Theorem 7.2 we will need the following simple 
lemma: 

Lemma 7.1.  For  any  monotone  decreasing  sequence  {an}  of positive  numbers 
converging to zero with al  ~ 1/16, a probability distribution (PI, P2,  ... ) may be 
found such that PI  :::  P2  :::  ... , and for all n 

00 L Pi  :::  max (San,  32npn+I). 

i=n+1 

PROOF. It suffices to look for Pi's such that 

00 L  Pi  :::  max (8an, 32npn) . 

i=n+1 

These conditions are easily satisfied. For positive integers u  <  v, define the func(cid:173)
tion H ( v, u) = L~:l 1 Ii. First we find a sequence 1 = n 1  <  n2  <  ... of integers 
with the following properties: 

(a) 

(b) 

(c) 

H(nk+l, nk) 

is monotonically increasing, 

H(n2, nr)  :::  32, 
8ank  ~ 1/2k  for all k  :::  1. 

Note that (c) may only be satisfied if anl  = al  ::s  1/16. To this end, define constants 
Cl, C2,  ... by 

7.2 Slow Rates 

115 

so that the Ck' S  are decreasing in k, and 

For n  E  [nk, nk+l), we define Pn  = ck/(32n). We claim that these numbers have 
the required properties. Indeed, {Pn} is decreasing, and 

Finally, if n  E  [nk, nk+l), then 

1 
~ p'  >  ~ ~H(n' 1  n·) = ~ - = -
2k . 
~ I 
i=n+l 

1 
J+'  J  ~ 2j 

00  c. 
- ~ 32 

j=k+l 

00 

00 

j=k+l 

Clearly, on the one hand, by the monotonicity of H(nk+l, nk), 1/2k ::::  Ck  = 32npn. 
On the other hand,  1/2k  ::::  8ank  ::::  8an. This concludes the proof.  0 

PROOF  OF  THEOREM  7.2.  We  introduce  some notation.  Let b  = O.b l b2b3  ..• be 
a real number on [0,  1]  with the shown binary expansion,  and let B  be a random 
variable uniformly distributed on [0,  1]  with expansion B  = O.BI B2B3 .... Let us 
restrict ourselves to a random variable X with 

P{X = i} = Pi, 

i::::  1, 

where PI  ::::  P2  ::::  ... >  0, and L~n+l Pi  ::::  max (8an, 32npn+l) for every n. That 
such  Pi'S  exist follows  from  Lemma 7.1.  Set Y  =  bx . As  Y is  a function of X, 
we see that  L * = 0.  Each b  E  [0,  1)  however describes  a  different distribution. 
With b replaced by B we have a random distribution. Introduce the short notation 
~n = ((Xl, BxJ, ... , (Xn ,  BxJ), and define G ni  = gn(i,  ~n).IfLn(B)denotesthe 
probabilityoferrorP{gn(X, ~n) =I YIB, Xl, ... , Xn}  for the random distribution, 
then we note that we may write 

00 

Ln(B) = L pJ{Gni¥Bd' 

i=l 

116 

7.  Slow Rates of Convergence 

If Ln(b) is the probability of error for a distribution parametrized by b, then 

. 

Ln(b) 
supmfE--
2an 
b 

n 

We consider only the conditional expectation for now.  We have 

E {inf Ln(B) I Xl, X 2 ,  •.• } 

n 

2an 

>  P {o {Ln(B)  ::: 2anll Xl, X2,  ... } 

00 

>  1- LP{Ln(B) <  2anl  Xl, X 2 ,  •.. } 

n=1 

00 

=  1 - LP {Ln(B)  <  2anl  Xl, X 2 ,  •.• ,  Xn} 

n=1 

00 

1 - L  E {P { Ln (B)  <  2an I ~n} I X I, X 2,  ... ,  X n}  . 

n:::1 

We bound the conditional probabilities inside the sum: 

P {Ln(B)  <  2anl  ~n} 
<  P {  L 

i¢{Xl"",Xn) 

pJ{Glti=jB;)  <  2an I ~n } 

(and, noting that G ni ,  Xl, ... , Xn 
are all functions of ~n, we have:) 

PiI{Bi=l}  < 2anl ~n} 

i¢{Xl, ... ,Xn} 

=  P {  L 
<  P {.f Pi I{Bi=l)  <  2an} 
=  P {.f PiBi  <  2an}. 

l=n+1 

l=n+1 

(since the Pi's are decreasing, by stochastic dominance) 

Now everything boils down to bounding these probabilities from above. We pro(cid:173)
ceed by Chernoff's bounding technique. The idea is the following: For any random 
variable X, and s  >  0, by Markov's inequality, 

7.2 Slow Rates 

117 

By cleverly choosing s one can often obtain very sharp bounds. For more discussion 
and examples of Chernoff's method, refer to Chapter 8. In our case, 

(since e-x  :s  1 - x  + x 2/2 for x  2:  0) 

< 

< 

= 

< 

S2 pn+1b ) 

(since 1 - x  :s  e- X
Sb 

) 

( 

exp  2san - 2  + 
4 
(where b  = L~n+l pJ 
1_(4_an_-_b_)2) 
exp  --
4 

bPn+l 

( 

(by taking s =  L: -4~ ,  and the fact that b  >  4an) 
exp (-~~) (since  b  2:  8an ) 

Pn+lL... 

16 Pn+l 

<  e-2n 

(since  b  2:  32pn+ln). 

Thus, we conclude that 

supinfE-- >  1 - Le-2n = - - >  -

e2 - 2 
e2 - 1 

LnCb) 
2an  -

1 
2' 

b  n 

00 
n=l 

so that there exists a b for which ELn(b)  ;:::  an  for all n.  0 

118 

7.  Slow Rates of Convergence 

Problems and Exercises 
PROBLEM 7.1.  Extend Theorem 7.2 for distributions with ° <  L * <  1/2: show that if an  is 

a sequence of positive numbers as in Theorem 7.2, then for any classification rule there is 
a distribution such that ELn  - L * ::::  an  for every n for which L * + an  <  1/2. 

PROBLEM 7.2.  Prove Theorems 7.1  and 7.2, under one of the following additional assump(cid:173)
tions,  which make the case that one  will need very restrictive conditions indeed to  study 
rates of convergence. 

(1)  X has a uniform density on (0, 1). 
(2)  X  has  a  uniform  density  on  [0,  1)  and  7J  is  infinitely  many  times  continuously 

(3) 

(4) 

7J  is  unimodal in x  E  n 2 ,  that is,  7J(AX)  decreases  as  A  >  ° increases  for  any 
differentiable on [0,  1). 
x  E n2. 
7J  is {O,  l}-valued, X is n 2-valued, and the set {x  : 7J(x) = I} is a compact convex 
set containing the origin. 

PROBLEM 7.3.  THERE  IS  NO  SUPER-CLASSIFIER.  Show that for every  sequence of classifica(cid:173)
tion rules  {gn}  there is a universally consistent sequence of rules  {g;l}'  such that for  some 
distribution of (X, Y), 

P{gn(X) =I Y}  >  P{g~(X) =I Y} 

for all n. 

PROBLEM 7.4.  The next two  exercises  are  intended  to  demonstrate  that the  weaponry of 
pattern recognition can often be successfully used for attacking other statistical problems. 
For example, a consequence of Theorem 7.2 is that estimating infinite discrete distributions is 
hard. Consider the problem of estimating a distribution (PI, P2,  ... ) on the positive integers 
{l, 2, 3,  ... } from  a  sample  Xl, ... , Xn  of i.i.d.  random variables  with P{X1  =  i}  =  Pi, 
i  ::::  1. Show that for any decreasing sequence {an}  of positive numbers converging to zero 
with al :s  1/16, and any estimate {Pi,n},  there exists a distribution such that 

E {f IPi  - pi,nl}  ::::  an' 

1=1 

HINT:  Consider a classification problem with L * = 0,  pry = O}  = 1/2, and X concentrated 
on  {I, 2, ... }.  Assume  that  the  class-conditional  probabilities  pia)  = P{X  = ilY  = O} 
and  pil)  = P{X  = ilY  = I}  are  estimated  from  two  i.i.d.  samples  xiOl, ... , X~O)  and 
xill, ... , X~l), distributed according to  {piOl}  and {pil)}, respectively.  Use Theorem 2.3  to 
show that for the classification rule obtained from these estimates in a natural way, 

therefore the lower bound of Theorem 7.2 can be applied. 

PROBLEM 7.5.  A similar slow-rate result appears in density estimation. Consider the prob(cid:173)
lem of estimating a density 1 on n, from  an i.i.d.  sample  X I, ... ,  Xn  having density 1. 
Show that for  any  decreasing sequence {an}  of positive numbers converging to  zero with 
a I  :s  1/16, and any density estimate In, there exists a distribution such that 

Problems and Exercises 

119 

E {f If(x) -

fn(X)ldX}  ::::  an· 

This  result  was  proved by  Birge  (1986)  using  a  different-and in  our view  much  more 
d 
comp lcate  -argument. HINT:  Put  Pi  =Ji 
fn(x)dx  and apply 
Problem 7.4. 

(x)dx and Pi,n  = Ji 

1· 

ri+l f 

r i+1 

8 
Error Estimation 

8.1  Error Counting 

Estimating the error probability Ln  = P{gn (X) =I Y I Dn} of a classification function 
gn is of essential importance. The designer always wants to know what performance 
can be expected from a classifier. As  the designer does not know the distribution 
of the data-otherwise there would not be any need to  design  a classifier-it is 
important to find error estimation methods that work well without any condition 
on  the  distribution  of (X, Y).  This  motivates  us  to  search  for  distribution-free 
performance bounds for error estimation methods. 

Suppose that we want to estimate the error probability of a classifier gn  designed 
from  the  training  sequence  Dn  = ((Xl, Yl ),  ... , (Xn,  Yn»).  Assume  first  that  a 
testing sequence 

Tm  = ((Xn+l,  Yn+l),  ... , (Xn+m,  Yn+m) 

is available, which is  a sequence of i.i.d. pairs that are independent of (X, Y) and 
Dn, and that are distributed as  (X, Y). An obvious way to estimate Ln  is to count 
the number of errors that gn  commits on Tm.  The error-counting estimator Ln,m  is 
defined by the relative frequency 

The estimator is clearly unbiased in the sense that 

122 

8.  Error Estimation 

and the conditional distribution of mLn,m, given the training data Dn, is binomial 
with parameters m and Ln. This makes analysis easy, for properties of the binomial 
distribution are well known. One main tool in the analysis is Hoeffding's inequality, 
which we will use many many times throughout this book. 

8.2  Hoeffding's Inequality 

The following inequality was proved for binomial random variables by Chernoff 
(1952) and Okamoto (1958). The general format is due to Hoeffding (1963): 

Theorem 8.1.  (HOEFFDING  (1963)).  Let  Xl, ... , Xn  be  independent  bounded 
random  variables such that Xi  falls  in  the  interval  [ai,  bd with probability one. 

Denote their sum by Sn  = L7=1  Xi.  Thenfor any E  > ° we have 

P{Sn  - ESn ::::  E}  :s  e-2E2/L7=1(bi-ad 

and 

The proof uses a simple auxiliary inequality: 

Lemma 8.1.  Let X  be  a  random variable  with EX  = 0,  a  :s  X  :s  b.  Then for 
s  >  0, 

PROOF.  Note that by convexity of the exponential function 

eSX  <  __ esb  + __ esa 

x-a 
- b-a 

b-x 
b-a 

for a  :s  x  :s b. 

Exploiting EX = 0, and introducing the notation p = -a/(b - a) we get 

b 

a 

__  esa  _ __  esb 
b-a 
(1  - p + pes(b-a») e-ps(b-a) 

b-a 

= 

def 

e¢(u), 

where u = s(b - a), and ¢(u) = - pu + 10g(1  - p + pe U
calculation it is easy to see that the derivative of ¢  is 

). But by straightforward 

,!.'(u) -
'P 
-

-

+ 

P 

p  + 

(1 

P 
- p  e-

)  u' 

therefore ¢(o) = ¢' (0) = 0. Moreover, 

¢//(u) =  P 

(1 

)  -u 

1 
<  _ 
(p + (1  - p)e-u )2  - 4 

- P  e 

Thus, by Taylor series expansion with remainder, for some e E  [0, u], 

8.2 Hoeffding's Inequality 

123 

PROOF  OF  THEOREM  8.1.  The  proof is  based on  Chernoff's  bounding  method 
(Chernoff (1952)): by Markov's inequality, for any nonnegative random variable 
X, and any E  >  0, 

EX 
P{X::::E}:S-. 
E 

Therefore, if s is an arbitrary positive number, then for any random variable X, 

In Chernoff's method, we find an s  > ° that minimizes the upper bound or makes 

the upper bound small. In our case, we have 

=  e-SE n E {es(Xi-EXi)} 
<  e-SE n es2 (bi -ai )2j8 

i=l 

n 

n 

(by independence) 

(by Lemma 8.1) 

i=l 

= 
= 

The second inequality is proved analogouslo/.  0 

The two inequalities in Theorem 8.1  may be combined to get 

Now, we can apply this inequality to get a distribution-free performance bound for 
the counting error estimate: 

COROLLARY  8.1.  For every E  >  0, 

124 

8.  Error Estimation 

The variance of the estimate can easily be computed using the fact that, condi(cid:173)

tioned on the data Dn, mLn,m  is binomially distributed: 

These are just the types of inequalities we want, for these are valid for any distri(cid:173)
bution and data size, and the bounds do not even depend on gn' 

Consider a special case in which all the  X/s take  values on  [-c, c]  and have 

zero mean. Then Hoeffding's inequality states that 

P {Snl n  >  E}  S  e-nr: 2 

jC2c

2

). 

This  bound,  while  useful for  E  larger than  c 1 -Vii,  ignores  variance information. 
When Var{Xi }  «  c2 ,  it is indeed possible to outperform Hoeffding's inequality. 
In particular, we have: 

Theorem 8.2.  (BENNETT  (1962)  AND  BERNSTEIN  (1946)).  Let  Xl, ... ,  Xn  be 
independent real-valued random variables with zero mean, and assume that Xi  ::;  C 
with probability one.  Let 

(}2 =  - LVar{Xd. 

1  n 

n  i=l 

Then, for any E  >  0, 

(Bennett (1962)),  and 

P {~ t Xi  >  E}  ::;  exp ( __ 2_nE_2 -) 

2(}  + 2CE 13 

n  i:::l 

(Bernstein (1946)). 

The proofs are left as exercises (Problem 8.2). We note that Bernstein's inequality 
kicks in when E is larger than about max (() 1 -Vii, cl -Vii). It is typically better than 
Hoeffding's inequality when ()  «  c. 

8.3  Error Estimation Without Testing Data 

A serious problem concerning the practical applicability of the estimate introduced 
above is  that it requires  a large,  independent testing  sequence.  In practice,  how(cid:173)
ever,  an  additional sample is  rarely  available.  One usually  wants  to  incorporate 
all available (Xi, Yi) pairs in the decision function. In such cases, to estimate L n , 
we have to rely on the training data only_  There are well-known methods that we 

8.4 Selecting Classifiers 

125 

will discuss later that are based on cross-validation (or leave-one-out) (Lunts and 
Brailovsky (1967); Stone (1974)); and holdout, resubstitution, rotation, smoothing, 
and bootstrapping (Efron (1979),  (1983)), which may  be employed to construct 
an empirical risk from the training sequence, thus obviating the need for a testing 
sequence. (See Kanal (1974), Cover and Wagner (1975), Toussaint (1974a), Glick 
(1978),  Hand (1986),  Jain,  Dubes,  and Chen  (1987),  and McLachlan (1992) for 
surveys, discussion, and empirical comparison.) 

Analysis of these methods, in general, is clearly a much harder problem, as ~n 
can depend on Dn  in a rather complicated way. If we construct some estimator Ln 
from D n , then it would be desirable to obtain distribution-free bounds on 

or on 

E {ILn  - Lnl q

} 

for  some q  ::::  1.  Conditional probabilities and expectations given  Dn  are mean(cid:173)
ingless, since everything is a funS,Eion of Dn. Here, however, we have to be much 
more careful as  we do not want Ln  to  be optimistically biased because the same 
data are used both for training and testing. 

Distribution-free bounds for the above quantities would be extremely helpful, 
as  we usually do not know the distribution of (X, Y).  While for  some rules such 
estimates exist-we will exhibit several  avenues in Chapters 22,  23,  24,  25,  26, 
and 3 I-it is disappointing that a single error estimation method cannot possibly 
work for  all  discrimination rules.  It is  therefore important to  point  out that we 
have to consider (gn,  Ln) pairs-for every rule one or more error estimates must 
be found if possible, and vice versa, for every error estimate, its limitations have 
to be stated. Secondly, rules for which no good error estimates are known should 
be avoided. Luckily, most popular rules do not fall into this category. On the other 
hand,  proven distribution-free performance guarantees are rarely  available-see 
Chapters 23  and 24 for examples. 

8.4  Selecting Classifiers 

Probably the most important application of error estimation is in the selection of a 
classification function from a class C of functions. If a class C of classifiers is given, 
then it is tempting to pick the one that minimizes an estimate of the error probability 
over the class.  A good method should pick a classifier with an error probability 
that is  close to  the minimal error probability in the class.  Here we require much 
more than distribution-free performance bounds of the error estimator for each of 
the classifiers in the class. Problem 8.8 demonstrates that it is not sufficient to be 
able  to  estimate the error probability of all  classifiers in the class.  Intuitively,  if 
we can estimate the error probability for the classifiers in C uniformly well,  then 
the classification function that minimizes the estimated error probability is likely 
to  have  an  error probability  that is  close  to  the best in the  class.  To  certify  this 

126 

8.  Error Estimation 

intuition, consider the following situation: Let C be a class of classifiers, that is, a 
class of mappings of the form ¢  : n d  -+ {O,  I}. Assume that the error count 

/'<0-

1  n 
Ln(¢) =  - " 
n~  J 

I{<p(x)¥y} 

J 

j=l 

is  used to  estimate the error probability L(¢) =  P{¢(X) i  Y}  of each classifier 
¢  E  C.  Denote by ¢: the classifier that minimizes the estimated error probability 
over the class: 

Ln(¢~) ::;  Ln(¢) 

for all ¢  E  C. 

Then for the error probability 

of the selected rule we have: 

Lemma 8.2.  (VAPNIK AND  CHERVONENKIS  (1974c); SEE ALSO  DEVROYE (1988B). 

L(¢~) -

inf L(¢) ::;  2 sup ILn(¢) - L(¢)I, 
<pEC 

<pEC 

ILn(¢~) - L(¢:)I  ::;  sup ILn(¢) - L(¢)I· 

<pEC 

PROOF. 

L(¢~) - L n(¢,:) + Ln(¢~) -

inf L(¢) 
<pEC 

< 

L(¢~) - Ln(¢~) + sup ILn(¢) - L(¢)I 

<pEC 

<  2 sup ILn(¢) - L(¢)I. 

<pEC 

The second inequality is trivially true.  0 

We see that upper bounds for SUP<pEC  ILn(¢) - L(¢)I provide us with upper bounds 
for two things simultaneously: 

(1)  An upper bound for the suboptimality of ¢: within C,  that is,  a bound for 

L(¢,~) -

inf<pEc  L(¢). 

(2)  An upper bound for the error ILn(¢:) - L(¢:)I committed when L n(¢:) is 

used to estimate the probability of error L(¢:) of the selected rule. 

In other words, by bounding SUP<pEC  ILn(¢) - L(¢)I, we kill two flies  at once. 
It is particularly useful to know that even though Ln (¢:) is usually optimistically 
biased, it is within given bounds of the unknown probability of error with ¢:' and 
that no other test sample is needed to estimate this probability of error. Whenever 

8.4 Selecting Classifiers 

127. 

our bounds indicate that we are close to  the optimum in C,  we must at the same 
time have a good estimate of the probability of error, and vice versa. 

As  a  simple,  but  interesting  application  of Lemma  8.2  we  consider the  case 

when the class C contains finitely many classifiers. 

Theorem 8.3.  Assume that the  cardinality oj C is bounded by N.  Then  we have 
for all E  >  0, 

PROOF. 

P  {sup ILn(¢) - L(¢)I  >  E} 

<pEC 

<  LP{ILn(¢) - L(¢)I  >  E} 

<pEC 

<  2Ne- 2nE2

, 

where we used Hoeffding' s inequality, and the fact that the random variable nLn (¢) 
is binomially distributed with parameters nand L( ¢). 0 

REMARK.  DISTRIBUTION-FREE  PROPERTIES.  Theorem  8.3  shows  that  the  problem 
studied here is purely combinatorial. The actual distribution of the data does not 
playa role at all in the upper bounds.  0 

REMARK. WITHOUT TESTING DATA. Very often, a class of rules C of the form ¢n (x) = 
¢n(x, Dn) is given, and the same data Dn  are used to select a rule by minimizing 
some estimates  Ln(¢n) of the error probabilities  L(¢n)  =  P{¢n(X)  =I  YIDn}.  A 
similar analysis  can  be  carried  out in  this  case.  In  particular,  if ¢~ denotes  the 
selected rule, then we have similar to Lemma 8.2: 

Theorem 8.4. 

and 

ILn(¢~) - L(¢~)I ~ sup  ILn(¢n) - L(¢n)l. 

<Pn EC 

If C is finite, then again, similar to Theorem 8.3, we have for example 

128 

8.  Error Estimation 

8.5  Estimating the Bayes Error 

It is also important to have a good estimate of the optimal error probability L * . First 
of all, if L * is large, we would know beforehand that any rule is going to perform 
poorly. Then perhaps the information might be used to return to the feature selection 
stage. Also, a comparison of estimates of Ln  and L * gives us  an idea how much 
room is left for improvement. Typically, L * is estimated by an estimate of the error 
probability of some consistent classification rule (see Fukunaga and Kessel (1971), 
Chen and Fu (1973), Fukunaga and Hummels (1987), and Garnett and Yau (1977». 

Clearly, if the estimate t1 we use is consistent in the sense that in - Ln  -+ ° with 

probability one as n  -+  00, and the rule is  strongly consistent, then 

in -+  L* 

with probability one. In other words, we have a consistent estimate of the Bayes 
error probability. There are two problems with this approach. The first problem is 
that if our purpose is comparing L * with L n , then using the same estimate for both 
of them does not ~ive any information. The other problem is that even though for 
many classifiers, Ln - Ln can be guaranteed to converge to zero rapidly, regardless 
what the distribution of (X, Y) is (see Ch~ters 23  and 24), in view of the results 
of Chapter 7,  the  rate  of convergence of Ln  to  L * using  such a method may be 
arbitrarily  slow.  Thus,  we cannot expect good performance for  all  distributions 
from  such  a  method.  The  question is  whether it is  possible to  come  up  with  a 
method of estimating L * such that the difference in - L * converges to zero rapidly 
for  all  distributions.  Unfortunately,  there is  no method that guarantees  a certain 
finite sample performance for all distributions. This disappointing fact is reflected 
in the following negative result: 

Theorem 8.5.  For every n, for any estimate in of the Bayes error probability L *, 
and for every E  >  0,  there exists a distribution of ( X, Y),  such that 

----

E {ILn  - L*I}  :::  4 - E. 

1 

PROOF. For a fixed n, we construct a family F of distributions, and show that for at 
least one member of the family, E { I in - L * I}  :::  ~ - E. The family contains 2m + 1 
distributions,  where m  is  a large integer specified later.  In all cases,  Xl, ... , Xn 
are  drawn independently by  a uniform distribution from  the  set {I, ... , m}.  Let 
Bo,  BI,  B2, ... , Bn  be i.i.d. Bernoulli random variables, independent of the  Xi'S, 
with  P{Bi  =  o}  =  P{Bi  =  I}  =  1/2. For the first  member of the  family  F, let 
Yi  = Bi  for i =  1,  ... , n. Thus, for this distribution, L * = 1/2. The Bayes error for 
the other 2m  members of the family is zero. These distributions are determined by 
m binary parameters aI, a2,  '" 

,am  E  {O,  I}  as follows: 
r;(i) = P{Y = 11X = i} = ai. 

In other words, Yi  = a Xi  for every i  =  1,  ... , n. Clearly, L * = ° for these distribu(cid:173)

tions. Note also that all distributions with X distributed uniformly on {I, ... , m} 

Problems and Exercises 

129 

and L * = 0 are members of the family.  Just as in the proofs of Theorems 7.1  and 
7.2, we randomize over the family F  of distributions. However, the way of random(cid:173)
ization is different here. The trick is to use Bo,  B I ,  B2 ,  ... ,  Bn  in randomly picking 
a distribution. (Recall that these random variables are just the labels YI ,  ... ,  Yn  in 
the training sequence for the first distribution in the family.) We choose a distribu(cid:173)
tion randomly, as follows:  If Bo =  0,  then we choose the first member of F  (the 
one with L * = 1/2). If Bo = 1, then the labels of the training sequence are given 
by 

if Xi  =I Xl, Xi  =I  X 2 ,  ... ,  Xi  =I Xi - l 
if j  <  i is the smallest index such that Xi  = X j  . 

Note  that  in  case  of  Bo  =  1,  for  any  fixed  realization  hI, ... , hn  E  {O,  I}  of 
B I,  ... ,  Bn , the Bayes risk is zero. Therefore, the distribution is in the family F. 
Now, let A be the event that all the Xi'S are different. Observe that under A, Ln 

is a function of Xl, ... , X n, B I ,  ... ,  Bn  only, but not Bo. Therefore, 

sUPE{ILn  - L*I}  >  E{ILn - L*I} 
:F 

(with Bo,  B I ,  ... ,  Bn  random) 

>  E { I A I Ln  - L * I } 

=  E {fA  (f[M) Ii. -~ 1+ I[B,"!) Ii. - 01) } 
E {lAHlin 

~I + lin -Ol)} 

>  E {fA ~} 
~P{A}. 

Now, if we pick m large enough, P{A} can be as close to 1 as desired. Hence, 

1 
sup E { I Ln  - L * I}  ::::  4' 

--

where the supremum is taken over all distributions of (X, Y).  0 

Problems and Exercises 

..PROBLEM 8.1.  Let B be a binomial random variable with parameters nand p. Show that 

and 

P{B  >  E}  :s  eE-np-Elog(Ejnp)  (E  >  np) 

P{B  <  E}  :s  eE-np-Elog(Ejnp)  (E  <  np) . 

(Chernoff (1952).) HINT: Proceed by Chernoff's bounding method. 

130 

8.  Error Estimation 

PROBLEM  8.2.  Prove the inequalities of Bennett and Bernstein given in Theorem 8.2.  To 
help you, we will guide you through different stages: 

(1)  Show that for any s  >  0, and any random variable X  with EX = 0, EX2 = (52, 

X  ::Sc, 

where 

(2)  Show that  ffl(u) ::s ° for u  ~ 0. 

(3)  By Chernoff's bounding method, show that 

feu) = log (_l_ e - csu  + _u_ ecs ) 

. 

l+u 

l+u 

(4)  Show that feu) ::s  f(O) + uf'(O) = (e CS 
(5)  Using the bound of (4), find the optimal value of s and derive Bennett's inequality. 

- 1 - cs) u. 

PROBLEM 8.3.  Use  Bernstein's inequality  to  show  that if B  is  a  binomial  (n,  p) random 
variable, then for E >  0, 

and 

PROBLEM  8.4.  LetX 1,  ••• ,  Xn  be independent binary-valued random variables withP{Xi  = 
I}  =  1 - P{Xi  = O}  =  Pi. Set P = (1ln) L~=l Pi  and Sn  =  L~=l Xi' Prove that 

(Angluin and Valiant (1979), see also Hagerup and RUb  (1990». Compare the results with 
Bernstein's inequality for this case.  HINT:  Put s  = log(1  + E)  and s  = -log(1  - E)  in the 
Chernoff bounding argument. Prove and exploit the elementary inequalities 

and 

E2 -2 ~E-(1+E)log(1+E),  E E(-l,O]. 

PROBLEM  8.5.  Let B  be a Binomial (n,  p) random variable. Show that for P ::s  a  <  1, 

Show that for ° <  a  <  P the same upper bounds hold for P{B  ::s  an} (Karp (1988),  see 
also  Hagerup  and  RUb  (1990»).  HINT:  Use Chernoff's  method  with  parameter s  and  set 
s = log(a(1 

p)/(p(1 - a»). 

PROBLEM  8.6.  Let B be a Binomial (n, p) random variable. Show that if p  2:  1/2, 

Problems and Exercises 

131 

and if p  :s  1/2, 

(Okamoto (1958)). HINT: Use Chernoff's method, and the inequality 

P{B - np :s  -nE} <  e- 2p(l-p) 

n<2 

1 - x 
(x  _  p)2 
X  log - + (1  - x)log - - >  - - - -
1 - p  - 2p(1  - p) 

x 
P 

for 1/2 :s  p :s x  :s  1, and for 0 :s  x  :s  p  :s  1/2. 

PROBLEM  8.7.  Let B be a Binomial (n,  p) random variable. Prove that 

and 

P{-JB - FP 2:  Ey'n}  <  e-2nE2 , 

P{-JB - FP :s  -Ey'n}  <  e-nE2 

(Okamoto (1958)). HINT:  Use Chernoff's method, and the inequalities 

X  log - + (1  - x) log - - 2:  2 (v'x - y'p) 

2 

x  E  [p, 1], 

and 

X  log - + (1  - x) log - - 2:  (v'x - y'p)  x  E  [0,  p], 

2 

x 
p 

x 
p 

1 -x 
1- p 

1- x 
1- p 

PROBLEM  8.8.  Give a class C of decision functions of the form ¢  : n d  --+  {O,  I}  (i.e., the 
training data do not play any role in the decision) such that for every E  >  0 

supP (l"Ln(¢) - L(¢)I  >  E}  :s  2e-

<pEC 

2nE2 

for every  distribution,  where Ln (¢) is  the error-counting estimate of the error probability 
L(¢) = P{¢(X) =I Y}  of decision ¢, and at the same time, if Fn  is the class of mappings ¢~ 
minimizing the error count Ln(¢) over the class C,  then there exists one distribution such 
that 

P  { sup  L(¢) -

<PEFn 

for all n. 

inf L(¢) = 1}  = 1 

<pEC 

PROBLEM  8.9.  Let C be  a  class  of classifiers,  that  is,  a  class  of mappings  of the  form 
¢n(x, Dn)  = ¢n(x).  Assume  that  an  independent testing  sequence  Tm  is  given,  and  that 
the error count 

____ 

1  m 

Ln,m(¢n) =  - L I(¢n(Xn+j)=!YI1+j} 

m  j=! 

is used to estimate the error probability L(¢n) = P{¢n(X) =I YIDn} of each classifier ¢n  E  C. 
Denote by ¢~,m the classifier that minimizes the estimated error probability over the class. 
Prove that for the error probability 

132 

8.  Error ~stimation 

of the selected rule we have 

Also, if C is of finite cardinality with lei = N, then 

PROBLEM 8.10.  Show that if a rule gn  is consistent, then we can always find an estimate of 
the error such that E{lin - Ln Iq}  -+ 0 for all q  >  0.  HINT:  Split the data sequence Dn  and 
use the second half to estimate the error probability of grn/21' 

PROBLEM 8.11.  OPEN-ENDED PROBLEM. Is there a rule for which no error estimate works for 
all distributions? More specifically, is  there a sequence of classification rules  gn  such that 
for all n large enough, 

infsupE{(in - Lni}::::  c 
in  X,Y 

for some constant c  >  0, where the infimum is taken over all possible error estimates? Are 
such rules necessarily inconsistent? 

PROBLEM 8.12.  Consider the problem of estimating the asymptotic probability of error of 
the nearest neighbor rule LNN  = 2E{ 1]( X)( 1 - 1]( X»}. Show that for every n, for any estimate 
Ln  of L NN ,  and for every E  >  0, there exists a distribution of eX,  y), such that 

9 
The Regular Histogram Rule 

In this chapter we study the cubic histogram rule.  Recall that this rule partitions 
Rd into cubes of the same size, and gives the decision according to the number of 
zeros and ones among the Yi's such that the corresponding Xi falls in the same cube 
as  X.  Pn = {AnI, A n2 , ... } denotes  a partition of Rd into cubes  of size h n  >  0, 
that is, into sets of the type n1=1 [kihn, (ki + l)hn), where the ki's are integers, and 
the histogram rule is defined by 

where for every x  E  Rd, An(x) = Ani  if X  E  Ani. That is, the decision is zero if 
the  number of ones  does  not exceed the  number of zeros  in the  cell in which x 
falls.  Weak universal consistency of this rule was  shown in Chapter 6 under the 
conditions hn  --+  0 and nh~ --+  00 as  n  --+  00. The purpose of this chapter is to 
introduce some techniques  by  proving  strong universal consistency of this  rule. 
These techniques will prove very useful in handling other problems as  well. First 
we introduce the method of bounded differences. 

9.1  The Method of Bounded Differences 

In  this  section  we  present  a  generalization  of  Hoeffding's  inequality,  due  to 
McDiarmid (1989). The result will equip us with a powerful tool to handle com(cid:173)
plicated functions  of independent random variables.  This  inequality follows  by 
results of Hoeffding  (1963)  and Azuma (1967)  who observed that Theorem 8.1 

134 

9.  The Regular Histogram Rule 

can be generalized to bounded martingale difference sequences. The inequality has 
found many applications in combinatorics,  as  well as in nonparametric statistics 
(see McDiarmid (1989) and Devroye (1991a) for surveys). 

Let us first recall the notion of martingales. Consider a probability space (Q, F, P). 

DEFINITION 9.1.  A sequence of random variables Z 1,  Z2,  ... is called a martingale 
if 

E {Zi+I1ZI, ... , Zd = Zi  with probability one 

for each i  >  0. 

Let Xl, X 2, ... be an  arbitrary  sequence  of random  variables.  Zl, Z2,  ... is 
called a martingale with respect to the sequence Xl, X 2 ,  ... iffor every i  >  0,  Zi 
is afunction of Xl, ... , Xi,  and 

E{Zi+IIXI,""  Xd = Zi  with probability one. 

Obviously, if Zl, Z2,  ... is  a martingale with respect to  Xl, X2, ... , then  Zl, 

Z2,  ... is a martingale, since 

E {Zi+lIZ1, ... , Zd  =  E {E {Zi+lIXl' ... , Xd ZI,""  Zd 

=  E{ZiIZl, ... ,Zi} 

The  most important examples  of martingales  are  sums  of independent  zero(cid:173)
mean  random  variables.  Let  U1,  U2,  ... be  independent random  variables  with 
zero mean. Then the random variables 

i 

Si  = L Uj , 

j=l 

i  >  0, 

form a martingale (see Problem 9.1). Martingales share many properties of sums 
of independent variables.  Our purpose here  is  to  extend Hoeffding's  inequality 
to martingales. The role of the independent random variables is played here by a 
so-called martingale difference sequence. 

DEFINITION  9.2.  A  sequence of random variables  VI,  V2 ,  .•. is  a  martingale dif(cid:173)
ference sequence if 

E {Vi+II VI, ... , Vi} = ° with probability one 

for every i  >  0. 

A  sequence  of random  variables  VI,  V2 ,  .  • .  is  called a  martingale  difference 
sequence with respect to a sequence of random variables Xl, X 2 ,  ... iffor every 

i  > ° Vi  is a function of Xl, ... , Xi,  and 

E {Vi+I IX 1,  ••• ,  Xd = ° with probability one. 

9.1 The Method of Bounded Differences 

135 

Again, it is easily seen that if VI,  V2 ,  ... is a martingale difference sequence with 

respect to a sequence Xl, X 2 ,  ••. of random variables, then it is a martingale dif(cid:173)
ference sequence. Also, any martingale Zl, Z2,  ... leads naturally to a martingale 
difference sequence by defining 

for i  >  O. 

The key result in the method of bounded differences is the following inequality 
that relaxes  the  independence  assumption  in Theorem 8.1,  allowing  martingale 
difference sequences: 

Theorem 9.1.  (HOEFFDING (1963), AZUMA (1967».  Let Xl, X 2 ,  .•• be a sequence 
of random  variables,  and assume  that  VI,  V2 ,  .  • .  is  a  martingale  difference  se(cid:173)
quence with  respect to  X I, X 2 ,  .... Assume furthermore  that there  exist random 
variables Z 1,  Z2,  ... and nonnegative constants C1,  C2,  ... such that for every i  >  0 
Zi  is afunction of Xl, ... , Xi-I, and 

Then for any E  >  0 and n 

and 

The proof is a rather straightforward extension of that of Hoeffding' s inequality. 

First we need an analog of Lemma 8.1: 

Lemma 9.1.  Assume that the  random variables V  and Z  satisfy with probability 
one that E{ V I Z}  = 0,  and for some function  f  and constant c ::::  0 

feZ) ::s  V  ::s  feZ) + c. 

Then for every s  >  0 

The proof of the lemma is left as an exercise (Problem 9.2). 

PROOF OF THEOREM 9.1.  As in the proof of Hoeffding' s inequality, we proceed by 
Chernoff's bounding method. Set Sk  = I::=1  ~. Then for any s  >  0 

P{S  > 

n_E 

}  < 

e-sEE {e SSn } 

136 

9.  The Rygular Histogram Rule 

< 

= 

e-SE es2 L:7=1  ct /8 
-2  2/ "n 
e 

E  L-i=l  Ci 

2 

(iterate previous argument) 

2 
(choose s  = 4E/ '\'.  c.) 
Lz=l  z' 

n 

The second inequality is proved analogously.  0 

Now,  we  are  ready  to  state  the  main  inequality  of this  section.  It is  a  large 
deviation-type inequality for functions of independent random variables such that 
the function is relatively robust to individual changes in the values of the random 
variables. The condition of the function requires that by changing the value of its 
i -th variable, the value of the function cannot change by more than a constant Ci. 

Theorem 9.2.  (McDIARMID (1989». Let Xl,""  Xn  be independent random va(cid:173)
riables taking values in a set A, and assume that f  : An  -+  R  satisfies 

sup 

If(Xl, ... , xn) -

f(XI,  ., ., Xi-I, X;,  Xi+1,  ... , xn)1  :s  Ci  ,  1 :s  i  :s  n  . 

Xl,· .. ,Xn, 

x;EA 

Then for all E  >  0 

P{f(X1 ,  .•. ,  Xn) - Ef(X 1,  .•. ,  Xn)::::  E}  :s  e- 2E2/L:7=lCr  , 

and 

P {Ef(X 

1,  ... , 

X) 
-

n 

f(X 

1,  ... , 

X  )  > 

n  _  E  _  e 

}  < 

-2E

2

/  L:7=1 cf 

. 

PROOF. Define V  = f(X 1,  ... ,  Xn)-Ef(XI, ... , Xn). Introduce VI  = E{VIXd(cid:173)
EV, and for k >  1, 

so that  V  =  L~=l Vk. Clearly,  VI,  ... , Vn  form a martingale difference sequence 
with respect to Xl, ... , X n • Define the random variables 

and 

Vk  =  Hk(X I, ... , X k )  - f Hk(X 1,.·., X k- I, x)Fk(dx), 

where the integration is with respect to Fk , the probability measure of X k. Introduce 
the random variables 

Wk ~ s~p ( Hk(X I, •.. ,  Xk-I, u) - f Hk(X 1 ,  ... ,  Xk-J, X)Fk(dX»)  , 

and 

9.1  The Method of Bounded Differences 

137 

Clearly,  Zk  ::s  Vk ::s  Wk  with probability one.  Since for every k  Zk  is  a function 
of Xl, ... , Xk-l, we can apply Theorem 9.1  directly to  V  =  L:~=1 Vk, if we can 
show that Wk - Zk  ::s  Ck.  But this follows from 

Wk  - Zk  = 

sup sup (Hk(XI , ... , Xk-I,U) - Hk(X I ,.··, Xk-l, v)) 
u 

by the condition of the theorem.  0 

Clearly, if the Xi'S are bounded, then the choice I(xl, ... , xn) = L:7=1  Xi  yields 
Hoeffding's  inequality.  Many  times  the  inequality  can  be  used  to  handle  very 
complicated functions  of independent random variables with great elegance. For 
examples in nonparametric statistics, see Problems 9.3, 9.6, 10.3. 

Similar methods to those used in the proof of Theorem 9.2 may be used to bound 
the variance Var{f(X1,  ... ,  Xn)}.  Other inequalities for the  variance of general 
functions of independent random variables were derived by Efron and Stein (1981) 
and Steele (1986). 

Theorem 9.3.  (DEVROYE  (1991A)).  Assume  that  the  conditions  of Theorem  9.2 
hold.  Then 

PROOF.  Using the notations of the proof of Theorem 9.2, we have to show that 

Var{V}  ::s  - z= cf-

1  n 

4  i=l 

Observe that 

Var{V}  =  EV2 

=  E {tV?}' 

1=1 

where in the last step we used the martingale property in the following  way:  for 
i  <  j  we have 

=  0  with probability one. 

138 

9.  The RE(gular Histogram Rule 

Thus, the theorem follows if we can show that 

Introducing  Wi  and  Zi  as  in the proof of Theorem 9.2,  we see that with proba(cid:173)
bility one  Zi  S  Vi  S  Zi + Ci.  Since Zi is  a function of Xl, ... , Xi -1, therefore, 
conditioned on Xl, ... , Xi-I,  ~ is a zero mean random variable taking values in 
the interval  [Zi, Zi + cd. But an arbitrary random variable  U  taking values in an 
interval [a, b] has variance not exceeding 

so that 

cf 
E{Vi  IXI ,· .. , Xi-d S  4' 

2 

which concludes the proof.  0 

9.2  Strong Universal Consistency 

The purpose of this section is to prove strong universal consistency of the histogram 
rule.  This is  the first  such result that we mention.  Later we will prove the  same 
property  for  other  rules,  too.  The  theorem,  stated  here  for  cubic  partitions,  is 
essentially  due  to  Devroye  and  Gyorfi  (1983).  For  more  general  sequences  of 
partitions,  see  Problem  9.7.  An  alternative  proof of the  theorem  based  on  the 
Vapnik-Chervonenkis  inequality  will  be  given  later-see the  remark  following 
Theorem 17.2. 

Theorem 9.4.  Assume that the sequence of partitions {Pn } satisfies the following 
two conditions as n  ---+  00: 

and 

nh~ ---+  00. 

For any distribution of (X, Y),  andfor every E  >  0 there is an integer no such that 
for n  >  no, for the error probability Ln  of the histogram rule 

P{Ln  - L * >  E}  S  2e-nE2 /32. 

Thus,  the cubic histogram rule is strongly universally consistent. 

PROOF.  Define 

,,~  y.] 

*()  Lc=l  c  {XiEAn(X)} 
ry  X  = 
n 

nfL(An(x» 

. 

Clearly, the decision based on 1J~, 

9.2 Strong Universal Consistency 

139 

is just the histogram rule. Therefore, by Theorem 2.3, it suffices to prove that for 
n large enough, 

p {f Iry(x) -

ry:(x)IJL(dx)  >  ~} :s e-n"132 , 

Decompose the difference as 

11J(x) -1J~(x)1 = EI1J(x)  -

1J~(x)1 + (11J(X)  -

1J~(x)1 - EI1J(x)  -

1J~(x)I).  (9.1) 

The convergence of the first term on the right-hand side implies weak consistency 
of the histogram rule.  The technique we use to bound this term is similar to that 
which we  already  saw in the  proof of Theorem 6.3.  For completeness,  we  give 
the details here. However, new ideas have to appear in our handling of the second 
term. 

We begin with the first term. Since the set of continuous functions with bounded 
support is dense in L 1 (f.L),  it is possible to find a continuous function of bounded 

support r(x) such that  f 11J(x)  -

r(x)If.L(dx)  <  E/16. 

Note that r(x) is uniformly continuous. Introduce the function 

r*(x) = E {r(X)I{xEAn(X)}} . 
n 

f.L(An(x» 

Then we can further decompose the first term on the right-hand side of (9.1) as 

EI1J(x)  -

1J~(x)1 

< 

r(x)1 + Ir(x) -

r,;(x) I 
11J(x)  -
+ Ir;(x) - E1J~(x)1 + EIE1J~(x) -

1J~(x)l. 

(9.2) 

We proceed term by term: 

FIRST TERM:  The integral of 11J(x) -
by the definition of r(x). 

r(x)1 (with respect to f.L)  is smaller than E/16 

SECOND TERM:  Using Fubini's theorem we have 

[Ir(x) - E {r(X)~{XEAjd I  f.L(dx) 

f.L(  J) 

r;(x)If.L(dx) 

f Ir(x) -
=  L 

AHi(Aj)¥O  J Aj 

140 

9.  The Regular Histogram Rule 

As rex) is uniformly continuous, if hn  is small enough, then !r(x) - r(y)!  <  E/16 
for every  x, YEA for  any  cell  A  E  Pn .  Then the double integral in the  above 
expression can be bounded from above as follows: 

L. L Ir(x) -

} 

} 

r(y)llL(dx)lL(dy) :::  EIL2(A j )/16. 

Note that we used the condition hn  -----+  0 here. Summing over the cells we get 

f Ir(x) -

rZ(x)llL(dx)  :::  E/16. 

THIRD TERM:  We have 

f !rZ(x) - E1]~(x)llL(dx)  =  L  IE {r(X)I{xEA j }  - Y I{xEA j }} I 

AjEPn 

A~" ILi r(x)J1Jdx) - Li 1)(x)/L(dx)I 

:::  f Ir(x) - 1](x)llL(dx)  <  E/16. 

FOURTH TERM:  Our aim is to show that for n large enough, 

E f IE1]~(x) - 1]~(x)llL(dx) <  E/16. 

To  this  end, let S be an arbitrary large ball centered at the origin. Denote by mn 
the number of cells of the partition Pn that intersect S. Clearly, mn is proportional 
to 1/ h~ as hn -----+  O.  Using the notation vn(A) =  ~ L7==1  I{Yi=l,X;EA}, it is clear that 
vn(A) = fA  1]~(x)lL(dx). Now,  we can write 

E f IE1]~(x) - 1]~(x)llL(dx) 

E ~ L, IE1)Z(x) - 1);(x)I/L(dx) 
=  E L IEVn(An,j) - vn(An,j)1 

j 

9.2 Strong Universal Consistency 

141 

<  E  L  IEvn(An,j) - vn(An,j)1  + 2tl(SC) 

j:A n,jnS-:j0 

(where SC  denotes the complement of S) 

<  L  JE IEvn(An,j) - Vn(An,j)1

j:A n,jns-:j0 

2  + 2tl(SC) 

(by the Cauchy-Schwarz inequality) 

<  mn _1  L  J /L(An,j)  + 2/L(S') 

mn  j:A n ,jns-:j0 

<  mn 

..1..  L" 

mn 

J.A n,jns-:j0 

n 

n 

tl(An  .) 

,J  + 2tl(SC) 

(by Jensen's inequality) 

<  /'!f- + 2/L(S') 

<  E/l6 

if n  and the radius  of S  are large enough,  since mn/n converges  to  zero by  the 
condition nh~ -+  00, and tl(SC) can be made arbitrarily small by choice of S. 

We have proved for the first term on the right-hand side of (9.1) that for n large 

enough, 

E f 11](x)  - 1]~(x)ltl(dx) <  E/4. 

Finally, we handle the second term on the right-hand side of (9.1) by obtaining 

an exponential bound for 

f 11](x)  - 1]~(x)ltl(dx) - E f 11](x)  - 1]~(x)ltl(dx) 

using Theorem 9.2. Fix the training data (Xl, Yl), ... , (Xn,  Yn)  E  Rd  X  {a,  1}, and 
replace (Xi, Yi) by (X;, Yi) changing the value of 1]~(x) to 1]~i(X), Then 1J~(X)-1J~i(X) 
differs from zero only on An(Xi) and An(X;), and thus 

f 11](x)  - 1]~(x)ltl(dx) - f 11](x)  - 1]~i(x)ltl(dx) 
<  f 11]~(x) - l]~/x)ltl(dx) 

< 

142 

9.  The Regular Histogram Rule 

So by Theorem 9.2, for sufficiently large n, 

p {f Iry(x) - ry~(x)IJL(dx) >  ~ } 
<  P {f Iry(x) -

ry:(x)IJL(dx) - E f Iry(x) -

<  e-n £2 /32.  0 

ry:(x)IJL(dx)  >  ~ } 

REMARK.  Strong  universal  consistency  follows  from  the  exponential  bound  on 
the probability P{Ln  - L * >  E}  via the Borel-Cantelli lemma. The inequality in 
Theorem 9.4 may seem universal in nature. However, it is distribution-dependent 
in a surreptitious  way because its range of validity,  n  :::::  no,  depends  heavily on 
E, h n , and the distribution. We know that distribution-free upper bounds could not 
exist anyway, in view of Theorem 7.2.  0 

Problems and Exercises 

PROBLEM 9.1.  Let VI, V2 ,  ••• be independent random variables with zero mean. Show that 
the random variables Si  = 2::~=1 Vj 

i  >  0 form a martingale. 

PROBLEM 9.2.  Prove Lemma 9.1. 

PROBLEM 9.3.  Let Xl, ... , X n  be real valued i.i.d. random variables with distribution func(cid:173)
tion F (x), and corresponding empirical distribution function Fn (x) = ~ 2:::1  f{X;:9)' Denote 
the Kolmogorov-Smirnov statistic by 

Vn  = sup IFn(x)  - F(x)l· 

xER 

Use Theorem 9.2 to show that 

Compare this result with Theorem 12.9. (None of them implies the other.) 

Also, consider a class A of subsets of nd. Let Zl, ... , Zn  be i.i.d. random variables in 

n d  with common distribution P{Zl  E A} = v(A), and consider the random variable 

Wn  = sup IVn(A)  - v(A)I, 

AEA 

where vnCA) = ~ 2::7=1  f{2;EA)  denotes the standard empirical measure of A. Prove that 

Compare this result with Theorem 12.5, and note that this result is true even if seA, n) = 211 
for all n. 

Problems and Exercises 

143 

PROBLEM 9.4.  THE LAZY HISTOGRAM RULE.  Let Pn = {AnI,  An2'  ... } be a sequence of parti(cid:173)
tions satisfying the conditions of the convergence Theorem 9.4. Define the lazy histogram 
rule as follows: 

gn(x) = Yj ,  X E  Ani, 

where  Xj  is the minimum-index point among  Xl, ... , Xn  for  which  Xj  E  Ani.  In other 
words,  we ignore  all  but  one  point in  each  set of the  partition.  If Ln  is  the  conditional 
probability  of error  for  the  lazy  histogram  rule,  then  show  that  for  any  distribution  of 
(X, Y), 

lim sup ELn  :s  2L *. 
n-+oo 

PROBLEM 9.5.  Assume that Pn  = P  = {AI, ... , A k } is a fixed partition into k sets. Consider 
the lazy histogram rule defined in Problem 9.4 based on P. Show that for all distributions 
of (X, Y), limn-+ oo ELn exists and satisfies 

lim  ELn = ~ 2Pi(1  - Pi)p,(AJ, 

n-+oo 

k 

L.... 
i=I 

where fL  is the probability measure for X, and Pi  = fA;  'f/(X)fL(dx)/fL(Ai).  Show that the 
limit of the probability of error EL~ for the ordinary histogram rule is 

lim  EL~ = ~ min(pi,  I - Pi )fL(Ai), 

n-+oo 

k 

L.... 
i=I 

and show that 

lim  ELn :s  2  lim  EL~. 
n-+oo 

n-+oo 

PROBLEM 9.6.  HISTOGRAM DENSITY ESTIMATION.  Let Xl, ... , Xn  be i.i.d. random variables 
in n d  with density f. Let P be a partition of n d
,  and define the histogram density estimate 
by 

1 

n 

fn(x) = nA(A(x»  ~ I{X;EA(x») , 

where  A(x) is  the  set in P  that contains x  and A is  the Lebesgue measure.  Prove for  the 
L 1 -error of the estimate that 

p  {If If,l(X)  -

f(x)ldx  - E f 

Ifn(x) -

f(X)'dXI  >  E}  :s  2e-

nE2

/2. 

(Devroye (1991a).) Conclude that weak L1-consistency of the estimate implies strong con(cid:173)
sistency (Abou-Jaoude (1976a;  1976c), see also Problem 6.2). 

PROBLEM  9.7.  GENERAL  PARTITIONS.  Extend the consistency result of Theorem 9.4 for se(cid:173)
quences of general, not necessarily cubic partitions. Actually, cells of the partitions need not 
even be hyperrectangles. Assume that the sequence of partitions {Pn } satisfies the following 
two conditions. For every ball S centered at the origin 

and 

lim.  max  (sup  Ilx - YII)  = 0 
n-+oo I:An;nSi0 

x,yEAn; 

lim  ~I{i : Ani  n S =/0}1  = O. 

n-+oo  n 

Prove that the corresponding histogram classification rule is strongly universally consistent. 

144 

9.  The Regular Histogram Rule 

PROBLEM  9.8.  Show that for cubic histograms the conditions of Problem 9.7 on the partition 

are equivalent to  the conditions hn  ---+ ° and nh~ ---+  00, respectively. 
PROBLEM 9.9.  LINEAR SCALING.  Partition n d  into congruent rectangles of the form 
where  kl' ... ,kd  are  integers,  and  hI, ... ,hd  >  ° denote  the  size  of the  edges  of the 
if hi  ---+  ° for  every  i  = 1,  ... , d,  and nh 1h2  ••• hd  ---+  00  as  n  ---+  00.  HINT:  This is  a 
Corollary of Problem 9.7. 
PROBLEM  9.10.  NONLINEAR SCALING. Let FI , ... , Fd  : n ---+  n be invertible, strictly mono(cid:173)
tone increasing functions.  Consider the partition of n d ,  whose cells are rectangles of the 
form 

rectangles.  Prove that the corresponding histogram rule is  strongly universally consistent 

[FI-I(klhd, FI-I((kl  + 1)h l )  x  ... x  [Fd-l(kdhd),  F;;\(kd + l)hd». 

(See Problem 9.9.) Prove that the histogram rule corresponding to this partition is strongly 
universally consistent under the conditions of Problem 9.9. HINT:  Use Problem 9.7. 

PROBLEM 9.11.  NECESSARY  AND  SUFFICIENT  CONDITIONS  FOR THE  BIAS.  A  sequence of par(cid:173)
titions  {Pn }  is  called  {-[-approximating  if for  every  measurable  set  A,  for  every  E  >  0, 
and for  all  sufficiently  large  n  there  is  a  set  An  E  a(Pn),  (a(P)  denotes  the  a-algebra 
generated by  cells  of the partition P)  such that {-[(AnL:.A)  <  E.  Prove that the bias term 
f 17J(x)  - E7J~(x)I{-[(dx) converges to zero for all distributions of (X, Y) if and only if the 
sequence  of partitions  {Pn }  is  {-[-approximating  for  every  probability  measure  {-[  on n d 
(Abou-Jaoude (l976a». Conclude that the first condition of Problem 9.7 implies that {Pn } 
is {-[-approximating for every probability measure {-[  (Csiszar (1973». 

PROBLEM 9.12.  NECESSARY  AND  SUFFICIENT  CONDITIONS  FOR  THE  VARIATION.  Assume that 
for every probability measure fJ- on n d
,  every measurable set A, every  c  >  0,  and every 
E >  0, there is an N(E, c,  A, {-[),  such that for all n  >  N(E, c, A, {-[), 
L 

{-[(An,}  n A)  <  E. 

} :p.,(An,j nA):Sc / n 

Prove that the variation term f IE7J~(x) -7J~(x)I{-[(dx) converges to zero for all distributions 
of eX,  Y) if and only if the sequence of partitions {Pn } satisfies the condition above (Abou(cid:173)
Jaoude (l976a». 

PROBLEM 9.13.  The E-effective cardinality m(P, Ilv,  A, E)  of a partition with respect to the 
probability measure {-[,  restricted to  a set A is  the minimum number of sets in P  such that 
the union of the remaining sets intersected with A has {-[-measure less than E. Prove that the 
sequence of partitions {Pn }  satisfies the condition of Problem 9.12 if and only if for every 
E  >  0, 

1·  m(Pn, {-[,  A, E)  ° 

= 

1m 
}1--HXJ 

n 
(Barron, Gyorfi, and van der Meulen (1992». 
PROBLEM 9.14.  In n 2 ,  partition the plane by taking three fixed  points not on a line,  x, Y 
and z.  At each of these points, partition n 2  by considering k equal sectors of angle 2]'[/ k 
each. Sets in the histogram partition are obtained as intersections of cones. Is  the induced 
histogram rule strongly universally consistent? If yes, state the conditions on k,  and if no, 
provide a counterexample. 

Problems and Exercises 

145 

PROBLEM 9.15.  Partition n 2  into shells of size h each. The i-th shell contains all points at 
l)h, ih) from the origin. Let h  -+ 0 and nh -+  00 as n  -+  00. Consider 
distance d  E  [(i -
the histogram rule. As n  -+  00, to what does ELn  converge? 

10 
Kernel Rules 

Histogram rules have the somewhat undesirable property that the rule is less ac(cid:173)
curate at borders  of cells  of the partition than in the middle of cells.  Looked at 
intuitively, this is because points near the border of a cell should have less weight 
in a decision regarding the cell's center.  To  remedy this problem, one might in(cid:173)
troduce the moving window rule, which is smoother than the histogram rule. This 
classifier simply takes the data points within a certain distance of the point to be 
classified,  and decides  according to  majority  vote.  Working formally,  let h  be a 
positive number. Then the moving window rule is defined as 

gn(X) = {  0 

if 2:7=1  I{yi=o,xiESx,h}  ~ 2:7=1  I{Yi =l,Xi ESx,h} 
otherwise, 

where Sx,h  denotes the closed ball of radius h centered at x. 

It is  possible  to  make  the  decision  even  smoother by  giving  more  weight to 
closer points than to  more distant ones. Let K  : Rd  ~ R  be a  kernel junction, 
which is usually nonnegative and monotone decreasing along rays  starting from 
the origin. The kernel classification rule is given by 

gn(X) = { 0 

'f~n  I 
1  L..,i=l 
otherwise. 

{Yi=O} 

K  (X-Xi)  ~n  I 

-h- ~ L..,i=l  {Yi=l} 

K  (X-Xi) 

-h-

148 

10.  Kernel Rules 

FIGURE  10.1.  The  moving  win(cid:173)
dow  rule  in n2.  The  decision  is 
1 in the shaded area. 

G o  Class 0 

•  Class  1 

The number h is called the smoothing factor, or bandwidth. It provides some form 
of distance weighting. 

FIGURE 10.2.  Kernel 
rule  on 
the  real  line.  The  figure  shows 
:L7=1(2Yi -l)K((x - Xi)! h)for 
n  = 20,  K(u) = (1  - u 2 )I{lul::::l} 
(the  Epanechnikov  kernel),  and 
three  smoothing factors  h.  One 
definitely undersmooths and one 
oversmooths.  We  took  p  = 1/2, 
and the  class-cconditional densi(cid:173)
ties  are  fo(x)  =  2(1  - x)  and 
flex) = 2x on [0,1]. 

Clearly, the kernel rule is a generalization of the moving window rule, since taking 
the  special  kernel  K (x)  =  I{xESo,d  yields  the  moving  window  rule.  This  kernel 
is  sometimes  called  the  naive  kernel.  Other  popular kernels  include  the  Gaus(cid:173)
sian kernel,  K(x)  =  e- lIxIl2 ; the Cauchy kernel,  K(x)  =  1/(1 + IIxll d+l );  and the 

Epanechnikov kernel  K(x)  =  (1  -
distance. 

IlxI12)I{lIxll:::1}.  where  II  . II  denotes Euclidean 

10.1  Consistency 

149 

gaussian kernel 

Cauchy kernel 

Epanechnikov kernel 

uniform kernel 

/\0' /\,xLD 

1 

1 

-1 

0 

-1 

0 

o 

0 

FIGURE  10.3.  Various kernels on R. 

Kernel-based rules  are  derived from the kernel estimate in density estimation 

originally studied by Parzen (1962),  Rosenblatt (1956), Akaike (1954),  and Ca(cid:173)
coullos  (1965)  (see  Problems  10.2  and  10.3);  and  in  regression  estimation,  in(cid:173)
troduced by Nadaraya (1964;  1970),  and Watson  (1964).  For particular choices 
of K,  rules  of this  sort  have  been  proposed  by  Fix  and  Hodges  (1951;  1952), 
Sebestyen  (1962),  Van  Ryzin  (1966),  and Meisel  (1969).  Statistical  analysis  of 
these rules and/or the corresponding regression function estimate can be found in 
Nadaraya (1964;  1970), Rejto and Revesz (1973), Devroye and Wagner (1976b; 
1980a;  1980b),  Greblicki  (1974;  1978b;  1978a),  Krzyzak  and Pawlak (1984b), 
and Devroye and Krzyzak (1989).  Usage of Cauchy kernels in discrimination is 
investigated by Arkadjew and Braverman (1966), Hand (1981), and Coomans and 
Broeckaert (1986). 

10.1  Consistency 

In this section we demonstrate strong universal consistency of kernel-based rules 
under general conditions on hand K. Let h  >  0 be a smoothing factor depending 
only on n, and let K  be a kernel function. If the conditional densities  fo, !I exist, 
then weak and  strong  consistency follow  from  Problems  10.2 and  10.3,  respec(cid:173)
tively,  via Problem 2.11.  We  state the universal consistency theorem for  a large 
class of kernel functions, namely, for all regular kernels. 

DEFINITION 10.1.  The  kernel K  is called regular if it is nonnegative,  and there is 
a ball SO,r  of radius r  >  0 centered at the origin,  and constant b  >  0 such that 
K(x)  ~ bIsO,r  and f SUPyEX+SO,r  K(y)dx  <  00. 

We  provide  three  informative  exercises  on regular  kernels  (Problems  10.18, 
10.19,  10.20).  In all cases,  regular kernels  are bounded and integrable.  The last 
condition holds  whenever  K  is  integrable  and uniformly  continuous.  Introduce 
the short notation Kh(X) = kK(v. The next theorem states strong universal con(cid:173)
sistency of kernel rules.  The theorem is essentially due to Devroye and Krzyzak 

150 

10.  Kernel Rules 

(1989). Under the assumption that X has a density, it was proven by Devroye and 
Gyorfi (1985) and Zhao (1989). 

Theorem 10.1.  (DEVROYE  AND  KRzYZAK  (1989)).  Assume that  K  is a  regular 
kernel.lf 

h  ~ 0  and  nhd  ~ 00  as n  ~ 00, 

thenfor any distribution of (X, Y),  andfor every E  >  0 there is an integer no such 
that for n  >  no for the error probability Ln  of the kernel rule 

P{Ln  - L * >  E}  .::s  2e-nE2/(32p2), 

where the constant p depends on the kernel K  and the dimension only.  Thus,  the 
kernel rule is strongly universally consistent. 

Clearly, naive kernels are regular,  and moving window rules are thus  strongly 
universally consistent. For the sake of readability, we give the proof for this special 
case only, and leave the extension to regular kernels to the reader-see Problems 
10.14,  10.15, and  10.16. Before we embark on the proof in the next section, we 
should warn the reader that Theorem 10.1  is of no help whatsoever regarding the 
choice of K  or h.  One possible solution is to derive explicit upper bounds for the 
probability of error as a function of descriptors of the distribution of (X, Y), and 
of K, nand h. Minimizing such bounds with respect to K  and h will lead to some 
expedient choices. Typically, such bounds would be based upon the inequality 

ELn  - L* 

:"  E {f 1(1  - p)!o(x) - PnO!no(x)ldx + f Ip!l(x) - PnJinl(x)ldx } 

(see Chapter 6), where fo,  f1  are the class densities,  fno,  fn1  are their kernel esti(cid:173)
mates (see Problem 10.2) (1  - p) and p  are the class probabilities, and PnO,  Pnl 
are their relative-frequency estimates.  Bounds for the expected L1-error in den(cid:173)
sity estimation may be found in Devroye (1987)  for d  = 1 and Holmstrom and 
KlemeHi  (1992)  for d  >  1.  Under regularity  conditions  on the  distribution,  the 
choice h  =  cn-d /(d+4)  for some constant is  asymptotically optimal in density es(cid:173)
timation.  However,  c  depends  upon unknown  distributional  parameters.  Rather 
than following  this  roundabout process,  we  ask the  reader to  be patient and to 
wait until Chapter 25, where we study automatic kernel rules, i.e., rules in which 
h, and sometimes K  as  well, is picked by the data without intervention from the 
statistician. 

It is  still too early to say meaningful things  about the choice of a kernel.  The 

kernel density estimate 

fn(x) = n~d ~? C ~ Xi) 

based upon  an  i.i.d.  sample  Xl, ... ,Xn  drawn from  an  unknown  density  f  is 
clearly a density in its  own right if K  ::::  0  and J K  = 1.  Also,  there  are certain 

10.1  Consistency 

151 

popular choices of K  that are  based upon various optimality criteria.  In pattern 
recognition, the story is  much more confused,  as  there is  no compelling a priori 
reason to pick a function  K  that is nonnegative or integrable.  Let us  make a few 
points with the trivial case n  =  1. T$ing h  = 1, the kernel rule is given by 

(X)={  0 

ifYI =O,K(x-XI )::::OorifYI =I,K(x-Xd::::;O 

gi 

1  otherwise. 

If K  ::::  0, then gn(x) = 0 if YI = 0, or if YI = 1 and K(x - Xd = O.  As we would 
obviously like  gn(x)  =  0  if and only if Y1  =  0,  it seems  necessary  to  insist on 
K  >  0 everywhere. However, this restriction makes the kernel estimate nonlocal 
in nature. 

For n = 1 and d = 1, consider next a negative-valued kernel such as the Hermite 

kernel 

FIGURE  10.4.  Hermite kernel. 

It is easy to verify that K (x) ::::  0 if and only if I x I ::::;  1. Also, J K = O. Nevertheless, 
we note that it yields a simple rule: 

(x) = {O 

gl 

1  otherwise. 

if YI  = 0, Ix - XII  ::::;  1 or if YI  = I, Ix  - XII  ::::  1 

If we have a biatomic distribution for X, with equally likely atoms at 0 and 2, and 
1](0)  = 0  and 1](2)  = 1 (i.e.,  Y  = 0  if X  = 0  and  Y  = 1 if X  = 2),  then  L * = 0 
and the probability of error for this kernel rule (L I )  is  0  as  well.  Note also  that 
for all n, gn  = gi if we keep the same K. Consider now any positive kernel in the 
same example. If Xl, ... , Xn  are all zero, then the decision is gn(X) = 0 for all x. 
Hence Ln  ::::  ip{Xl = ... = Xn} = Ij2n+l  >  O.  Our negative zero-integral kernel 
is  strictly  better for all  n  than  any  positive  kernel!  Such kernels  should  not be 
discarded without further thought. In density estimation, negative-valued kernels 
are  used to  reduce the bias  under some  smoothness  conditions.  Here,  as  shown 
above,  there is  an  additional reason-negative weights  given to  points far  away 
from the  Xi'S may actually be beneficial. 

Staying with the same example, if K  >  0 everywhere, then 

ELI = P{Y1 = 0, Y = I} + P{Y1 = 1,  Y = O}  = 2E1](X)E{1  - 1](X)}, 

which maybe Ij2(ifE1](X) = Ij2)evenifL* = o (which happens when 1]  E  {O,  I} 
everywhere). For this particular example, we would have obtained the same result 

152 

10.  Kernel Rules 

even if K  ==  1 everywhere. With  K  ==  1,  we  simply ignore the  Xi'S  and take a 
majority vote among the  Yi'S (with K  ==  -1, it would be a minority vote!): 

(x) = {o  if L:7==1  I{yi==o}  ~ L:7==1  I{Yi==l} 

1  otherwise. 

gn 

Let Nn  be the number of Yi'S  equal to  zero.  As  Nn  is  binomial  (n,  1 - p) with 
p = E17(X) = P{Y = I}, we see that 

ELn  =  pP {Nn ~ ~} + (1  - p)P {Nn  <  ~} -+ min(p, 1 - p), 

simply by invoking the law of large numbers.  Thus,  ELn  -+  min(p, 1 - p). As 

in the case with n  =  1,  the limit is  1/2 when p  =  1/2, even though L * = ° when 

1]  E  {a,  I}  everywhere. It is interesting to note the following though: 

ELI  =  2p(1  - p) 

=  2 min(p, 1 - p) (1  - min(p, 1 - p» 

<  2min(p, 1 - p) 

=  2  lim  ELno 

n-+oo 

The expected error with one observation is  at most twice as  bad as  the expected 
error with an infinite sequence. We have seen various versions of this  inequality 
at work in many instances such as the nearest neighbor rule. 

Let us apply the inequality for EL 1 to each part in a fixed partition P of Rd. On 
each of the k sets AI, ... , Ak ofP, we apply a simple majority vote among the Yi's, 
as in the histogram rule.  If we define the lazy histogram rule as the one in which 
in each set Ai, we assign the class according to the Yj  for which X j  E  Ai  and j  is 
the lowest such index ("the first point to fall in A/'). It is clear (see Problems 9.4 
and 9.5) that 

lim  ELLAZy,n  <  2  lim  ELn 
n-+oo 

n-+oo 

=  2 t /L(~i) L ry(x)/L(dx) 1,0 -ry(x))/L(dx), 

where  Ln  is  the  probability of error for  the  ordinary histogram rule.  Again,  the 
vast majority of observations is barely needed to reach a good decision. 

Just for fun,  let us  return to  a majority vote rule,  now  applied to the first three 

observations only. With p  = P {Y  = I}, we see that 

EL3  =  p ((1- p)3  + 3(1- pfp) + (1- p) (3(1- p)p2 + p3) 

by just writing down binomial probabilities. Observe that 

EL3  =  p(1  - p) (1  + 4p(1 - p» 

<  min(p, 1- p)(l +4min(p, 1 
lim  ELn (1  + 4  lim  4ELn) . 

= 

n-+oo 

n-+oo 

p» 

10.2 Proof of the Consistency Theorem 

153 

If limn--+oo ELn  is  small  to  start  with,  e.g.,  limn--+oo ELn  =  0.01,  then  EL3  :::: 
0.01  x  1.04 = 0.0104. In such cases, it just does not pay to take more than three 
observations. 

Kernels  with fixed  smoothing factors have no local  sensitivity  and,  except in 

some circumstances, have probabilities of error that do not converge to L *. The uni(cid:173)
versal consistency theorem makes a strong case for decreasing smoothing factors(cid:173)
there is no hope in general of approaching L * unless decisions are asymptotically 
local. 

The consistency theorem describes kernel rules with h  -+ 0: these rules become 

more and more local in nature as n  -+  00. The necessity oflocal rules is not appar(cid:173)
ent from the previous biatomic example. However, it is clear that if we consider a 
distribution in which given Y = 0, X is uniform on {8,  38,  ... , (2k + 1)8}, and given 
Y =  1,  X is uniform on {28, 48,  ... , 2k8}, that is, with the two classes intimately 
interwoven, a kernel rule with K  :::  0 of compact support [ -1, 1], and h  <  8  <  1 
will have 

Ln  ::::  I {U~!dNi=O} } , 

where Ni  is  the number of X/s at the i-th atom. Hence ELn  goes to  zero expo(cid:173)
nentially fast.  If in the above example we assign X by a geometric distribution on 
8,83 ,85 ,  ... when Y  = 0,  and by a geometric distribution on 82 ,84 ,86 ,  ... when 
Y  =  1, then to obtain ELn  -+ 0, it is necessary that h  -+ 0 (see Problem 10.1). 

REMARK.  It is worthwhile to investigate what happens for negative-valued kernels 
K  when h  -+ 0, nhd  -+  00 and K  has compact support. Every decision becomes 
an average over many local decisions. If fJ has a density f, then at almost all points 
x,  f  may be approximated very nicely by f Sx8  f/'A(Sx,o)  for small 8  >  0,  where 
S x,o  is the closed ball of radius 8 about x. This implies, roughly speaking, that the 
number of weighted votes from class 0 observations in a neighborhood of x is about 
1](x»f(x)nhd f K, while for class  1 the weight is  about 1](x)f(x)nhd f K. 
(1 
The correct decision is  nearly  always  made for  nh d  large enough provided that 
J K  >  O.  See Problem 10.4 on why kernels with J K  <  0 should be avoided.  0 

10.2  Proof of the Consistency Theorem 

In the proof we can proceed as for the histogram. The crucial difference is captured 
in the following covering lemmas. Let f3d  denote the minimum number of balls of 
radius 1/2 that cover the ball So, 1. If K is the naIve kernel, then p = f3d  in Theorem 
10.1. 
Lemma 10.1.  (COVERING LEMMA)./f K(x) = I{xESo,Il,  thenforany y  End, h  >  0, 
and probability measure fJ, 

f  Kh(X  - y) 

f Kh(x - z)fJ(dz) fJ(dx)  ::::  f3d. 

154 

10.  Kernel Rules 

PROOF.  Cover  the  ball  Sy,h  by  fJd  balls  of radius  h12.  Denote  their  centers  by 
Xl,  ... ,  Xf3d'  Then X  E  SXi,h/2  implies SXi,h/2  C  Sx,h  and thus 

We may write 

JL(Sx,h) 

f I{xEsy,h} JL(dx) 
<  t f I{xE;xi,hI2}  JL(dx) 
<  t f I{xEsxi ,hf2}  JL(dx) 

JL(  x,h) 

i==l 

i==l 

JL(Sxi,hj2) 

= 

f3d.  0 

Lemma 10.2.  Let 0  <  h  ::::  R  <  00,  and let S  C  Rd be a ball of radius R.  Then 
for every probability measure JL, 

where Cd  depends upon the dimension d  only. 

PROOF.  Cover  S  with  balls  of radius  h 12,  centered at center points  of a regular 
grid of dimension hl(2-J(i) x  ... x  hl(2-J(i). Denote these centers by Xl,  ..• ,Xm , 
where m is the number of balls that cover S. Clearly, 

m  < 

= 

volume(So,R+h) 
volume(grid cell) 
Vd(R + h)d 

(Vd  is the volume of the unit ball in R d) 

(hl(2-J(i)) d 

<  (l+~r c~, 

where the constant c~ depends upon the dimension only.  Every X  gets covered at 
most kl times where kl  depends upon d  only. Then we have 

10.2 Proof of the Consistency Theorem 

155 

m  f  I 

<  L 

i:::l 

{XESxi ,hj2} 
J tt(Sxi,h/2) 

tt(dx) 

(by the same argument as in Lemma 10.1) 
m 

<  L J tt(Sxi,h/2) 

i:::l 

m 

<  m L tt(Sxi,h/2) 

i:::l 

(by the Cauchy-Schwarz inequality) 

<  Jklm 

<  He d ' 

where Cd  depends upon the dimension only.  0 

PROOF OF THEOREM  10.1.  Define 

2:}:::1  YjKh(x - X j ) 
. 

nEKh(X  - X) 

l1n(X) = 

Since the decision rule can be written as 

gn(x) = 
{ 

0 

if 2:}:::1  YjKh(x  - X j )  <  2:}:::1(1- Yj)Kh(x  - X j ) 

nEKh(X - X) 

-

nEKh(X - X) 

1  otherwise, 

by Theorem 2.3, what we have to prove is that for n large enough 

p {f Iry(x) -

ryn(X)IIL(dx)  >  ~} :" e-ne'/(32p') 

We  use  a  decomposition as  in the  proof of strong  consistency of the  histogram 
rule: 

111(X)  -

l1n(x)1 

=  Ell1(X) -

l1n(x)1  + (ll1(X) -

l1n(x)1  - Ell1(X) -

l1n(x)I). 

(10.1) 

To handle the first term on the right-hand side, fix  E/  >  0,  and let r  : Rd  -+ R  be 
a continuous function of bounded support satisfying 

f 111(X)  - r(x)ltt(dx)  <  E/. 

156 

10.  Kernel Rules 

Obviously, we can choose the function r  such that 0  S  r(x)  S  1 for all x  E  Rd. 
Then we have the following simple upper bound: 

EI1](x) - 1]n(x)! 

< 

11](x)  -

E {r(X)Kh(X - X)} I 

r(x)1 +  rex) - - - - - - (cid:173)
EKh(x - X) 

I

+ 

E {r(X)Kh(X  - X)} 
I

EKh(x - X) 

I 

- E1]n(x)  + EIE1]n(x) - 1]n(x)l· 

(10.2) 

Next we bound the integral of each term on the right-hand side of the inequality 
above. 

FIRST TERM:  By the definition of r, 

f 11](x)  -

r(x)ltL(dx)  <  fl. 

SECOND  TERM:  Since r(x) is continuous and zero outside of a bounded set, it is 
also  uniformly continuous,  that is,  there  exists  a 8  >  0  such that  Ilx  - y \I  <  8 
implies  Ir(x) -

r(y)1  <  ft. Also, rex) is bounded. Thus, 

rex) -

tL(dx) 

EKh(x _  X) 

E {r(X)Kh(x - X)} I 

f I 
=  f Ir(x) - f r(y)Kh(X - y)tL(dy) I JL(dx) 
s  f (  Kh(X  - Y) 

EKh(X - X) 

]sx,s  EKh(x - X) 

!r(x) -

r(y)!tL(dy)tL(dx) 

r(y)1  s  1.  Clearly,  we have 
In the last step  we  used the fact that  SUPx,y  !r(x) -
fsx,Ii  E~h~:?l)tL(dy) s  1,  and by the uniform continuity of rex),  SUPZESx,o  Ir(x) -
r(z)1  <  ft.  Thus,  the  first  term at  the  end  of the  chain  of inequalities  above is 
bounded by  ft.  The  second term converges  to  zero  since  h  <  8 for  all  n  large 
enough,  which in tum implies  fs~ /j  E~,(Z¥?l) tL(dy)  =  O.  (This  is  obvious for the 
naIve kernel. For regular kernels, convergence to zero follows from Problem 10.15.) 
The convergence of the integral (with respect to tL(dx»  follows from the dominated 

10.2 Proof of the Consistency Theorem 

157 

convergence theorem. In summary, we have shown that 
E {r(X)Kh(x - X)} I 

lim sup 
n--+oo 

r(x) -

.. 
EKh(x - X) 

I I

p,(dx) :s  E'. 

THIRD TERM: 

E {r(X)Kh(x - X)} 

EKh(x  _  X) 

I 

- El]n(x)  p,(dx) 

I I

Ir(y) -

l](Y)I! Kh(x  _  z)p,(dz)p,(dy)p,(dx) 

p,(dx) 

:s 

!(r(y) -

Kh(X  - y) 

l](y))Kh(x  - y)p,(dy) I 
EKh(x - X) 

I I 
I I 
= I (f f K~:(~ ~)~(dz/"(dX)) Ir(y) -
:s  I plr(y) -

l](y)Ip,(dy) :s  PE', 

(by Fubini's theorem) 

ry(y)ll"(dy) 

where in the last two steps we used the covering lemma (see Lemma 10.1  for the 
naIve kernel, and Problem 10.14 for general kernels), and the definition of rex). P 
is the constant appearing in the covering lemma: 

FOURTH TERM:  We show that 

E {I IEryn(x) -

For the naIve kernel, we have 

ryn (x) I I"(dx) }  -+ O. 

E {IEl]n(x) -

l]n(x)1} 
:s  JE {IEl]n(x) -

l]n(x)1 2} 

E  { (L:j., (YjKh(X  - X j ) - ElY Kh(x  _  Xl})) 

2

} 

n 2(EKh(X  - X))2 

E{(YKh(x - X) - E{YKh(x - X)})2} 

n(EKh(x - X))2 

158 

10.  Kernel Rules 

where we used the Cauchy-Schwarz inequality, and properties of the naIve kernel. 
Extension to regular kernels is straightforward. 

Next we use the inequality above to  show that the integral converges to  zero. 
Divide the integral over nd into two terms, namely an integral over a large ball S 
centered at the origin, of radius  R  >  0,  and an integral over SC.  For the integral 
outside of the ball we have 

1 E {IE1]n(X)  - 1]n(x)1} fL(dx)  :::; 21  E1]n(X)fL(dx)  --+  211](X)fL(dX) 

with probability one as n  --+  00, which can be shown in the same way we proved 

~ 

~ 

~ 

[  E1]n(X)fL(dx)  --+ 
JRd 

[  1](X)fL(dx) 
JRd 

(see the first,  second, and third terms of (10.2)). Clearly, the radius R of the ball S 
can be chosen such that 2Isc 1](x)fL(dx)  <  E/4. To bound the integral over S we 
employ Lemma 10.2: 

f. E {IE1]n(x)  - 1]n(x)1} fL(dx) 

< 

-1-1  1 fL(dx) 
..;n  s J fL(Sx,h) 
(by the inequality obtained above) 

R)d/2 
1+ -
h 

Cd 

1  ( 
-
..;n 

--+  0 

(since by assumption nhd  --+  (0). 

Therefore, if n is sufficiently large, then for the first term on the right-hand side of 
(10.1) we have 

E {I Iry(x) -
if we take E!  = E/(4p + 12). 

ryn (x) II-'(dx) }  <  E'(p + 3) = E/4 

It remains to show that the second term on the right-hand side of (10.1) is small 
with large probability. To  do this, we use McDiarmid's inequality (Theorem 9.2) 
for 

I Iry(x)  -

ryn(x)ll-'(dx)  E {I Iry(x) -

ryn(X)II-'(dX)}  . 

Fix the training data at ((Xl, YI),  ... , (Xn, Yn))  and replace the i-th pair (Xi, Yi) by 
(Xi,  )it),  changing  the  value  of 1]n(x)  to  1]~i(x),  Clearly,  by  the  covering  lemma 

(Lemma 10.1), I i1](x)  -1]n(x)ifL(dx) - I i1](x)  - 1]~i(x)lfL(dx) 

<  I i1]n(x)  - 1]~Jx)ifL(dx) :::  sup I 2Kh(X  - Y)  fL(dx) 

YERd 

nEKh(X - X) 

< 

2p 
n 

10.3 Potential Function Rules 

159 

So by Theorem 9.2, 

p {f I~(x) - ~n(x)I",(dx) >  i} 
<  P {f I~(x) -

ryn(x)I",(dx) - E {f Iry(x) - ~n(X)I"'(dX)} >  :} 

The proof is now completed.  0 

10.3  Potential Function Rules 

Kernel  classification rules  may be formulated in terms  of the  so-called potential 
function  rules.  These rules were originally introduced and studied by Bashkirov, 
Braverman  and  Muchnik  (1964),  Aizerman,  Braverman  and  Rozonoer  (1964c; 
1964b;  1964a;  1970), Braverman (1965),  and Braverman and Pyatniskii (1966). 
The  original  idea  was  the  following:  put a  unit  of positive  electrical  charge  at 
every data point Xi, where Yi  = 1, and a unit of negative charge, at data points Xi 
where  Yi  =  O.  The resulting potential field defines  an intuitively appealing rule: 
the decision at a point x  is one if the potential at that point is positive, and zero if 
it is  negative.  This idea leads to  a rule that can be generalized to obtain rules of 
the form 

gn(x) = 
{ 

if !n(x) :::;  0 

0 
I  otherwise, 

where 

n 

fn(x) = L rn,i(Dn)Kn,i(X,  Xi), 

i=l 

where  the  Kn/s describe  the  potential  field  around  Xi,  and  the  rn/s are  their 
weights.  Rules  that can be put into this  form  are  often called potential function 
rules. Here we give a brief survey of these rules. 

KERNEL RULES.  Clearly, kernel rules studied in the previous section are potential 
function rules with 

Kn,i(X, y) = K  T 

(

X  - Y) 

' 

Here K is a fixed kernel function, and hI, h2, ... is a sequence of positive numbers. 

HISTOGRAM RULES.  Similarly, histogram rules (see Chapters 6 and 9) can be put 
in this form, by choosing 

160 

10.  Kernel Rules 

and  rn,i(Dn) = 2Yi  - 1. 
Recall that An (x) denotes the cell of the partition in which x  falls. 

Kn,Jx, y) = I{YEAn(x)}, 

POLYNOMIAL DISCRIMINANT FUNCTIONS.  Specht (1967) suggested applying a poly(cid:173)
nomial expansion to the kernel K  C~Y). This led to the choice 

Kn,i(x,  y) = L V-r/X)'ljfj(Y), 

k 

j=l 

and  rn,i(Dn) = 2Yi  - 1, 

where V-r1,  ... , V-rk  are fixed real-valued functions on nd. When these functions are 
polynomials, the corresponding classifier gn  is  called a polynomial discriminant 
function. The potential function rule obtained this way is a generalized linear rule 
(see Chapter 17) with 

k 

fn(x) = L an,j'ljlj(X), 

j=l 

where the coefficients an,}  depend on the data Dn  only, through 

n 

an,j  = I)2Yi  -

i=l 

l)V-rj(Xi). 

This choice of the coefficients does not necessarily lead to a consistent rule, unless 
the functions V-r1,  ... , V-rk  are allowed to change with n, or k is allowed to vary with 
n.  Nevertheless,  the rule has  some  computational  advantages  over kernel rules. 
In  many practical situations there is  enough time to preprocess the data  Dn ,  but 
once the observation X becomes known, the decision has to be made very quickly. 
Clearly, the coefficients an, 1,  ... ,  an,n  can be computed by knowing the training 
data  Dn  only,  and if the  values  V-r1 (X), ... , V-rk(X)  are  easily  computable,  then 
fn(X) can be computed much more quickly than in a kernel-based decision, where 
all n terms of the sum have to be computed in real time, if no preprocessing is done. 
However, using preprocessing of the data may also help with kernel rules, espe(cid:173)
cially when d  =  1.  For a survey of computational speed-up with kernel methods, 
see Devroye and Machell (1985). 

RECURSIVE KERNEL RULES.  Consider the choice 

Kn,i(x,y)=K  ~ , 

(

X  - Y) 

(10.3) 

Observe that the only difference between this and the ordinary kernel rule is that 
in the expression of Kn,i,  the  smoothing parameter hn is  replaced with hi. With 
this change, we can compute the rule recursively by observing that 

fn+l(X)  =  fn(x) + (2Yn+1 -

l)K 

(

X  - Xn+l) 
hn+l 

. 

Problems and Exercises 

161 

The computational  advantage  of this  rule is  that if one  collects  additional  data, 
then the rule does not have to be entirely recomputed. It can be adjusted using the 
formula above.  Consistency properties of this rule were studied by Devroye and 
Wagner  (1980b),  Krzyzak  and  Pawlak  (1984a),  Krzyzak  (1986),  and  Greblicki 
and  Pawlak  (1987).  Several  similar recursive  kernel rules  have  been  studied in 
the literature. Wolverton and Wagner (1969b), Greblicki (1974), and Krzyzak and 
Pawlak (1983), studied the situation when 
(X - Y) 

Kn,i(x, y) = h1 K  T 

(10.4) 

1 

The corresponding rule can be computed recursively by 

fn+l(X) = fn(x) + (2Yn+l  - 1)-d- K 

1 
hn+l 

(X - Xn+l) 

hn+l 

. 

Motivated by stochastic approximation methods (see Chapter 17), Revesz (1973) 
suggested and studied the rule obtained from 

fn+l(X)  =  fn(x) + --(2Yn+l - 1 -

1 

n + 1 

fn(x))-d-K 

1 
hn+l 

(X - Xn+l) 

hn+l 

. 

A similar rule was studied by Gyorfi (1981): 

fn+l(X)  =  fn(x) + --(2Yn+l - 1 -

1 

n + 1 

fn(x))K 

(X - Xn+l) 

hn+l 

. 

PROBLEM  10.1.  Let  K  be  a  nonnegative  kernel  with  compact  support  on  [-1, 1].  Show 

Problems and Exercises 
that for  some distribution,  h  -+ ° is  necessary for consistency of the kernel rule.  To  this 
end,  consider  the  following  example.  Given  Y  = 0,  X  has  a  geometric  distribution  on 
8,83,8 5 ,  ... ,  and given Y  = 1,  X has a geometric distribution on 82,84, 86 ,  .... Then show 
that to  obtain ELn  -+  L * = 0, it is necessary that h  -+ 0. 
nd  with  density  f. Let K  be a kernel  function  integrating  to  one,  and  let hn  >  ° be a 

PROBLEM  10.2.  KERNEL DENSITY ESTIMATION.  Let Xl, ... , Xn  be i.i.d. random variables in 

smoothing factor.  The kernel density estimate is defined by 

1  ~ (x-X.) 

fn(x)  = nh~ f:t K  T 

(Rosenblatt (1956), Parzen (1962)). Prove that the estimate is weakly universally consistent 

in Ll if h n  -+ ° and nh~ -+ 00 as  n  -+  00. HINT:  Proceed as in Problem 6.2. 

162 

10.  Kernel Rules 

PROBLEM  10.3.  STRONG  CONSISTENCY  OF  KERNEL  DENSITY  ESTIMATION.  Let Xl, ... , Xn  be 
i.i.d. random variables in nd with density f. Let K  be a nonnegative function integrating to 
one (a kernel) and h  >  0 a smoothing factor. As in the previous exercise, the kernel density 
estimate is defined by 

fn(x) = /;d ~ K(-h _ I  ). 

1  ~  x-X(cid:173)

n 

i=! 

Prove for the Ll-error of the estimate that 

p  {If l.hl(X)  -

f(x)ldx  - E f 

Ifn(x) -

f(X)'dXI  >  E}  S  2e-nE2j2 

(Devroye (1991a)). Conclude that weak Ll-consistency of the estimate implies strong con(cid:173)
sistency (see Problem 10.2). This is a  way to show that weak and strong Ll-consistencies 
of the kernel density estimate are equivalent (Devroye (1983).) Also, for d = 1,  since if K 
is nonnegative, then E f Ifn(x) -
f(x)ldx cannot converge to zero faster than n-2(5  for any 
density  (see Devroye and Gyorfi  (1985)), therefore, the inequality above implies that for 
any density 

lim  f ifn(x) -
n-+co  E f  Ifn(x) -

f(x)ldx  = 0 
f(x)ldx 

with probability one (Devroye (1988d)). This property is called the relative stability of the 
L I  error.  It means that the  asymptotic behavior of the  L I -error is  the  same as  that of its 
expected value. HINT:  Use McDiarmid's inequality. 
PROBLEM  10.4.  If f K  <  0,  show that under the assumption that fJ.,  has a density f, and 
that h  ---:>- 0, nhd  ~ 00, the kernel rule has 

lim  ELn = E {max(1J(X),  I -1J(X))} = 1 - L *. 
n-+co 

Thus, the rule makes the wrong decisions, and such kernels should be avoided.  HINT:  You 
may use the fact that for the kernel density estimate with kernel L  satisfying f L = 1, 

f 

Ifn(x) -

f(x)ldx  ---:>- 0 

with probability one, if h  ~ 0 and nhd  ~ 00 (see Problems 10.2 and 10.3). 

PROBLEM  10.5.  Consider a  devilish kernel that attaches  counterproductive  weight to  the 
origin: 

K(x) = 
{

-I 
~ 

if IIxll  S  1/3 
if 1/3  <  Ilxll  s  1 
if Ilxll  >  1. 

---:>- 00.  Assume  that  L *  = O.  Show  that  Ln  ---:>- 0  with 
Assume  that  h  ---:>- 0,  yet nhd 
probability  one.  CONCESSION:  if you  find  that  you  can't handle  the  universality,  try  first 
proving the statement for strictly separable distributions. 

PROBLEM  10.6.  Show that for the distribution depicted in Figure 10.2, the kernel rule with 
kernel K(u) = (1- u2)I{lul~l) is consistent whenever h, the smoothing factor, remains fixed 
and 0  <  h  S  1/2. 

PROBLEM  10.7.  THE LIMIT FOR  FIXED h.  Consider a kernel rule with fixed h ==  1,  and fixed 
kernel K. Find a simple argument that proves 

Problems and Exercises 

163 

lim  ELn = Looi 
n-'>oo 

where Loo  is the probability of error for the decision goo  defined by 

if E{K(x - X)(21](X) -
if E{K(x - X)(21](X) -

l)} :s ° 

I)}  >  0. 

Find a distribution such that for the window kernel,  Loo  =  1/2, yet L * = 0. Is there such a 
distribution for any kernel? HINT:  Try proving a convergence result at each x  by invoking 
the law of large numbers, and then replace x  by X. 

PROBLEM  10.8.  Show that the conditions h n  -+ ° and nhd  -+  00 of Theorem 10.1  are not 
necessary for consistency, that is, exhibit a distribution such that the kernel rule is consistent 
with hn  = 1,  and exhibit another distribution for which the kernel rule is  consistent with 
hn  rv  l/nlJd. 

PROBLEM  10.9.  Prove  that the  conditions  hn  -+  0  and nhd  -+  00  of Theorem  10.1  are 
necessary for universal consistency, that is, show that if one of these conditions are violated 
then there is a distribution for which the kernel rule is not consistent (Krzyzak (1991). 

PROBLEM  10.10.  This exercise provides an argument in favor of monotonicity of the kernel 
K. In 'R..2,  find  a  nonatornic distribution for (X, y), and a  positive kernel with f K  >  0, 
K  vanishing  off So.o  for  some 8  >  0,  such  that for all h  >  0,  and all n, the kernel rule 
b > ° in the universal consistency theorem cannot be abolished altogether. 
has ELn  = 1/2, while L*  = O.  This result says that the condition K(x)  ~ bI{so,ol  for some 
PROBLEM  10.11.  With K  as in the previous problem, and taking h = 1,  show that 

lim  ELn  = L * 
n-'>oo 

under the following conditions: 

(1)  K  has  compact support vanishing  off SO,o,  K  ~ 0,  and  K  ~ bI{so"l  for  some 

E  >  O. 

(2)  We say that we have agreement on Sx,o  when for all Z  E  Sx,o,  either 1J(z)  :s  1/2, 

or 1](z)  ~ 1/2. We ask that P{Agreement on Sx,o}  = 1. 

PROBLEM  10.12.  The previous exercise shows that at points where there is agreement, we 
make asymptotically the correct decision with kernels with fixed  smoothing factor.  Let D 
be the  set  {x  :  1J(x)  =  1/2},  and let the 8-neighborhood of D  be defined by  Do  =  {y  : 
II y - x II  :s 8 for some xED}. Let fL  be the probability measure for X. Take K, h as in the 
previous exercise. Noting that x  f{,  Do  means that we have agreement on Sx,o,  show that for 
all distributions of (X, Y), 

lim sup ELn  :s  L * + fL(Do). 
n-+oo 

164 

10.  Kernel Rules 

FIGURE  10.5.  8-neighborhood  of a 
set D. 

PROBLEM  10.13.  CONTINUATION. Clearly, fJ.,(D8) ~ 0 as 8 ~ 0 when fJ.,(D) = O. Convince 
yourself that fJ.,(D)  = 0  for  most problems.  If you knew  how  fast  fJ.,(D8)  tended to  zero, 
then the previous exercise would enable you to pick h as  a function of n  such that h  ~ 0 
and such  that the upper bound for  ELn  obtained by analogy  from  the  previous  exercise 
is  approximately  minimal.  If in n d ,  D  is  the  surface of the  unit ball,  X has  a  bounded 
density  f, and 'f}  is Lipschitz, determine a bound for fJ.,(D8).  By considering the proof ofthe 
universal consistency theorem, show how to  choose h such that 

PROBLEM  10.14.  EXTENSION OF THE COVERING LEMMA (LEMMA 10.1) TO REGULAR KERNELS. 
Let K  be a regular kernel,  and let fJ.,  be an arbitrary probability measure. Prove that there 
exists a finite constant p = p(K) only depending upon K  such that for any y and h 

(Devroye and Krzyzak (1989)). HINT:  Prove this by checking the following: 

(1)  First take  a bounded overlap cover of nd  with translates of SO,rI2,  where r  >  0 
is  the constant appearing in the definition of a regular kernel.  This cover has an 
infinite number of member balls, but every x  gets covered at most k 1  times where 
k[  depends upon d only. 

(2)  The centers of the balls are called Xi,  i  = 1, 2,  .... The integral condition on K 

implies that 

L  sup  K(x):s 

00 

i=l  XEXi+SO,rj2 

for another finite constant k2 • 

(3)  Show that 

k  J 
Iso,r/2  dx 

1 

sup  K(y)dx:s k2 

YEX+SO,r/2 

Kh(x - y) :s L 

00 

sup 

Kh(x - y)I[xEy+hXi+ So,rh/2] , 

i=l  xEy+hxi+ SO,rh/2 

and 

From (c) conclude 

Problems and Exercises 

165 

<  L. 

SUPZEhxi+SO rh/2  Kh(z) 
i=l  XEy+hxi+SO,rh/2  bfL(y + hXi + SO,rh/2) 

~ 1 
f  fL(Y  + hXi  + SO,rh/2) SUPZEhXi+SO,rh/2  Kh(z) 

' fL (dx )  

bfL(y + hXi + SO,rhj2) 

i=l 

1  00 

-L 

sup 

b  i=l  zEhxi+SO,rh/2 

~ 
Kh(z)  ::::  -, 

b 

where k2  depends on K  and d only. 

PROBLEM  10.15.  Let K  be a regular kernel, and let fL  be an arbitrary probability measure. 
Prove that for any 8  >  0 
r 
h~ s~p 

f Kh(x - y)I{llx-yll>o)  (d)  0 

f Kh(x  _  Z)fL(d;)  fL  x  =  . 

HINT:  Substitute Kh(z) in the proof of Problem 10.15 by Kh(z)I(llzil  2:  8) and notice that 

f Kh(X  - y)I{llx-yll>o) 

Kh(x  - z)fL(dz) 

f 

sup 
y 

~ 
- fL(dx)::::  L. 
i=l  zEhxi+SO,rh/2 

sup 

Kh(z)I(lIzll  2:  8)  -+ 0 

as h  -+ O. 

PROBLEM  10.16.  Use Problems  10.14 and  10.15  to extend the proof of Theorem  10.1  for 
arbitrary regular kernels. 

PROBLEM  10.17.  Show that the constant f3d  in Lemma 10.1 is never more than 4d • 

PROBLEM  10.18.  Show that if L  2:  0 is a bounded function that is monotonically decreasing 
on [0, (0) with the property that f U
- 1 L(u)du  <  00, and if K  : n d  -+  [0, (0) is a function 
with K(x) ::::  L(lIx II), then K  is regular. 

d

PROBLEM  10.19.  Find a kernel  K  2:  0  that is  monotonically  decreasing  along rays  (i.e., 
K(rx)  ::::  K(x) for all x  E  n d  and all r  2:  1)  such that K  is not regular.  (This exercise is 
intended to convince you that it is  very difficult to find  well-behaved kernels that are not 
regular.) 

PROBLEM  10.20.  Let K(x)  =  L(llxll) for  some bounded function  L  2:  O.  Show that K  is 
regular if L  is decreasing on [0, (0) and f K(x)dx  <  00. Conclude that the Gaussian and 
Cauchy kernels are regular. 

PROBLEM  10.21.  Regularity of the kernel is not necessary for universal consistency. Investi(cid:173)
gate universal consistency with a nonintegrable kernel-that is, for which f K (x )dx = 00-
such as  K(x) = 1/(1 + Ixl). Greblicki, Krzyzak, and Pawlak (1984) proved consistency of 
the  kernel  rule  with  smoothing factor  hn  satisfying  hn  -+  0  and nh~ -+  00  if the  ker(cid:173)
nel  K  satisfies the following  conditions:  K(x)  2:  cI{lIxlI::olJ  for  some C  >  0  and for  some 
Cl, C2  >  0, c1H(llx II)  ::::  K(x) ::::  c2H(llxll), where H  is a nonincreasing function on [0, (0) 
with ud H(u) -+ 0 as  u -+  00. 

166 

10.  Kernel Rules 

PROBLEM  10.22.  Consider the kernel rule with kernel K(x) =  l/llx Ilr, r  >  0. Such kernels 
are useless for atomic distributions unless we take limits and define gn  as usual when x  tj.  S, 
the  collection of points z with  Xi  =  X)  =  z for  some pair (i  =I  j). For XES, we  take 
a majority  vote  over the  Yi's  for  which  Xi  =  x.  Discuss  the  weak  universal  consistency 
of this  rule,  which  has  the  curious  property  that  gn  is  invariant  to  the  smoothing factor 
h-so, we might as  well  set h  =  1 without loss  of generality.  Note  also  that for  r  ::::  d, 
Is  K(x)dx  = 00,  and for r :::;  d, Isc  K(x)dx  = 00,  where  SO,I  is  the  unit ball of nd 
centered at the origin. In particular, if r  :::;  d, by considering X  uniform on So, I  if Y =  1 and 
X  uniform on the surface of So, I  if Y  =  0, show that even though L * = 0, the probability of 
error of the rule may tend to a nonzero limit for certain values of P {Y = I}. Hence, the rule 
is not universally consistent. For r  ::::  d, prove or disprove the weak universal consistency, 
noting that the rules' decisions are by-and-Iarge based on the few nearest neighbors. Prove 
the rule is weakly consistent for all r  :::;  d  whenever X  has a density. 

O,! 

0,1 

PROBLEM  10.23.  Assume that the class densities coincide, that is, 1o (x )  =  II (x) for every 
x  E n, and assume p  = p {Y  =  1}  >  1/2. Show that the expected probability of error of the 
kernel rule with  K  ==  1 is  smaller than that with any unimodal regular kernel for every n 
and h small enough. Exhibit a distribution such that the kernel rule with a symmetric kernel 
such that  K (Ix I)  is  monotone increasing has  smaller expected error probability than that 
with any unimodal regular kernel. 

PROBLEM  10.24.  SCALING.  Assume  that  the  kernel  K  can  be  written  into  the  following 
product form of one-dimensional kernels: 

K(x) =  K(x(J), ... , xed»)  = n Ki(x(i»). 

d 

i-I 

Assume also that K  is regular. One can use different smoothing factors along the different 
coordinate axes to define a kernel rule by 

n 

if L (2 Yi  - 1) T1 K) 

i-I 

d 

)-1 

(X(j)  - X(j)) 

,I:::; ° 

h In 

otherwise, 

where X~j) denotes the j -th component of Xi' Prove that gn is strongly universally consistent 

if hin  -+ ° for all i  =  1,  ... , d, and nhlnh2n'"  hdn  -+ 00. 
PROBLEM  10.25.  Let  K  :  (0,  (0)  ---+  [0, (0) be  a  function,  and  L  a  symmetric  positive 
definite d  x  d  matrix. For x  E  n d  define K'(x) =  K  (XTLX). Find conditions on K  such 
that the kernel rule with kernel  K' is universally consistent. 

PROBLEM  10.26.  Prove that the recursive kernel rule defined by (10.4) is strongly universally 
consistent if K  is a regular kernel, hn ---+  0, and nh~ ---+  00 as n  ---+  00 (Krzyzak and Pawlak 
(1983». 

PROBLEM  10.27.  Show that the recursive kernel rule of (10.3) is  strongly universally con(cid:173)
sistent whenever K  is a regular kernel. limn - HXl hn  =  0, and L::l h~ =  00 (Greblicki and 
weaker assumptions on the kernel. They assume that K(x) :::  c!lllxll::::lj  for some C > ° and 
Pawlak (1987». Note: Greblicki and Pawlak (1987) showed convergence under significantly 
tion on [0, (0) with ud H(u) ---+ ° as  u  ---+  00. They also showed that under the additional 

that for some CI, C2  >  0, CI H(lIxll) :::;  K(x) :::;  C2H(llxll), where H  is a nonincreasing func(cid:173)

assumption f K (x )dx  <  00 the following conditions on hn  are necessary and sufficient for 
universal consistency: 

Problems and Exercises 

167 

PROBLEM  10.28.  OPEN-ENDED  PROBLEM.  Let P{Y = I}  =  1/2. Given Y  =  1,  let X  be uni(cid:173)
formly distributed on [0, 1]. Given Y  = 0, let X be atomic on the rationals with the following 
distribution:  let X  = V / W,  where  V  and  Ware independent identically distributed,  and 
P{V  = i}  = 1/2i, i  ::::  1.  Consider the kernel rule  with the  window  kernel.  What is  the 
behavior of the smoothing factor h~ that minimizes the expected probability of error ELn? 

11 
Consistency of the k-Nearest 
Neighbor Rule 

In Chapter 5 we discuss results about the asymptotic behavior of k-nearest neighbor 
classification rules,  where  the  value  of k-the number  of neighbors  taken  into 
account at the decision-is kept at a fixed number as the size of the training data n 
increases. This choice leads to asymptotic error probabilities smaller than 2L * , but 
no universal consistency. In Chapter 6 we showed that if we let k grow to infinity 
as  n  --+  00 such that k / n  --+  0, then the resulting rule is  weakly consistent. The 
main purpose of this chapter is to demonstrate strong consistency, and to discuss 
various versions of the rule. 

We are not concerned here with the data-based choice of k-that subject deserves 
a  chapter of its  own  (Chapter 26).  We  are  also  not tackling  the  problem of the 
selection of a suitable-even data-based-metric. At the end of this chapter and 
in  the  exercises,  we  draw  the  attention  to  I-nearest  neighbor  relabeling  rules, 
which combine the computational comfort of the  I-nearest neighbor rule with the 
asymptotic performance of variable-k nearest neighbor rules. 

Consistency of k-nearest neighbor classification, and corresponding regression 
and density estimation has been studied by many researchers. See Fix and Hodges 
(1951; 1952), Cover (1968a), Stone (1977), Beck (1979), Gyorfi and Gyorfi (1975), 
Devroye (1981a; 1982b), Collomb (1979; 1980; 1981), Bickel and Breiman (1983), 
Mack (1981),  Stute (1984), Devroye and Gyorfi (1985), Bhattacharya and Mack 
(1987), Zhao (1987), and Devroye, Gyorfi, Krzyzak, and Lugosi (1994). 

Recall the definition of the k-nearest neighbor rule: first reorder the data 

(X(l)(X), Y(l)(x», ... , (X(n)(x), Y('!)(x») 

according to increasing Euclidean distances of the X j' s to x. In other words, X CO (x) 
is the i -th nearest neighbor of x among the points Xl, ... , X n' If distance ties occur, 

170 

11.  Cons~stency of the k-Nearest Neighbor Rule 

a tie-breaking strategy must be defined. If fJv  is absolutely continuous with respect 
to the Lebesgue measure, that is, it has a density, then no ties occur with probability 
one, so formally we break ties by comparing indices. However, for general fJv,  the 
problem of distance ties turns out to be important, and its solution is messy.  The 
issue of tie breaking becomes important when one is concerned with convergence 
of Ln  with probability one. For weak universal consistency, it suffices to break ties 
by comparing indices. 

The k-NN classification rule is defined as 

gn(x) =  {O 

if 2:7=1. I{YU)(x)=l}  :::.:  2:7=1  I{YCi)(x)=O} 

1  otherWIse. 

In other words, gn (x) is a majority vote among the labels of the k nearest neighbors 
ofx. 

11.1  Strong Consistency 

In this  section we prove Theorem 11.1. We assume the existence of a density for 
fJv,  so that we can avoid messy technicalities necessary to handle distance ties. We 
discuss this issue briefly in the next section. 

The following result implies strong consistency whenever X has an absolutely 
continuous  distribution.  The  result  was  proved  by  Devroye  and  Gyorfi  (1985), 
and Zhao (1987). The proof presented here basically appears in Devroye, Gyorfi, 
Krzyzak, and Lugosi (1994), where strong universal consistency is proved under 
an appropriate tie-breaking strategy (see discussion later). Some of the main ideas 
appeared in the proof of the strong universal consistency of the regular histogram 
rule (Theorem 9.4). 

Theorem 11.1.  (DEVROYE AND  GYORFI (1985), ZHAO (1987)). Assume that fJv  has 
a density.  If k  --+  00 and k / n  --+  0  then for every E  >  0 there  is an no  such that 
forn  >  no 

P{Ln  - L*  >  E}:::.:  2e-m:2 /(72yl), 

where Yd  is the minimal number of cones centered at the origin of angle Jr /6 that 
cover Rd.  (For  the  definition  of a  cone,  see  Chapter 5.)  Thus,  the  k-NN  rule  is 
strongly consistent. 

REMARK.  At first glance the upper bound in the theorem does not seem to depend 
on k.  It is  no  that depends  on the  sequence  of k's.  What we really prove is  the 
following: for every E >  0 there exists a /30  E  (0,  1) such that for any /3  <  /30  there 
is  an no  such that if n  >  no,  k  >  1//3, and kin  <  /3,  the exponential inequality 
holds.  0 

For the proof we need a generalization of Lemma 5.3. The role of this covering 
lemma is  analogous to that of Lemma 10.1  in the proof of consistency of kernel 
rules. 

11.1  Strong Consistency 

171 

Lemma 11.1.  (DEVROYE AND  GYORFI (1985)). Let 

Ba(x') = {x  : tl(Sx,l!x-x'll)  :::;  a} . 

Then for all x'  E  Rd 

PROOF.  For x  E  Rd  let C(x, s)  C  Rd be a cone of angle Jr /6 centered at x. The 
cone consists of all y with the property that either y  = x  or angle(y - x, s) :::;  Jr / 6, 
where s  is  a  fixed  direction.  If y, y'  E  C(x, s),  and  IIx  - yll  <  IIx  - y'lI,  then 
\I y - y' \I  <  \Ix - y' II· This follows from a simple geometric argument in the vector 
space spanned by x, y and y' (see the proof of Lemma 5.3). 

Now, let C I, ... ,  CYd  be a collection of cones centered at x with different central 

direction covering Rd. Then 

Yd 
tl(Ba(x')  :::;  L 

tl(Ci  n Ba(x')). 

i=l 

Let x*  E  Ci n Ba (x'). Then by the property of the cones mentioned above we have 

fJ..,(Ci  n Sx l ,IIx'-x* II  n Ba(x'» 

:::;  /-t(Sx*,lIx' -x*lI)  :::;  a, 

where we use the fact that x*  E  Ba(x'). Since x* is arbitrary, 

tl(Ci  n Ba(x')  :s a, 

which completes the proof of the lemma. 0 

An immediate consequence of the lemma is that the number of points among 
X I, ... ,  Xn  such that  X  is  one  of their  k  nearest neighbors  is  not more than  a 
constant times k. 
COROLLARY  11.1. 
n 
L1{XisamongthekNN'sof  Xi  in  {X1, ... ,Xn,X}-{Xd}:::; kYd. 
i=l 

PROOF. Apply Lemma 11.1 with a = k / n and let tl be the empirical measure tln of 
Xl, ... , Xn, that is, for each Borel set A  ~ R d

,  tln(A) = (l/n) L7=1  I{xi EA }.  0 

PROOF OF THEOREM  11.1.  Since the decision rule gn  may be rewritten as 

(x) = {O 
gn 

if1Jn(x):::;  1/2 

1  otherwise, 

where 1Jn  is the corresponding regression function estimate 

1  k 

1]n(x) = k LY(i)(x), 

i=l 

172 

11.  Consistency of the k-Nearest Neighbor Rule 

the statement follows from Theorem 2.2 if we show that for sufficiently large n 

p {f I~(x) - ~n(x)lfL(dx) >  ~} S  2e-n"/(72yJ) 

Define Pn (x) as the solution of the equation 

Note that the absolute continuity of J-L  implies that the solution always exists. (This 
is the only point in the proof where we use this assumption.) Also define 

The basis of the proof is the following decomposition: 

11J(x)  - 1Jn(x)1  :::;  IrJn(x)  -

1J~(x)1 + IrJ~(x) -

rJ(x)l· 

For the first term on the right-hand side, observe that denoting Rn (x) = II X(k) (x ) 
xII, 

where fi;; is defined as  1J~ with Y  replaced by the constant random variable Y =  1, 
and 17 ==  1 is the corresponding regression function. Thus, 

11J(x)  - 1Jn(x)1  :::;  r~(x) -17(x)1 + 11J~(x) - 1J(x)l· 

(11.1) 

First we show that the expected values of the integrals of both terms on the right(cid:173)
hand side converge  to  zero.  Then  we use McDiarmid's  inequality  to  prove  that 
both terms are very close to their expected values with large probability. 

For the expected value of the first term on the right -hand side of (11.1), using 

the Cauchy-Schwarz inequality, we have 

E f 11J~(x) - 1Jn(x)IJ-L(dx)  <  E f Iry;:(x)  -17(x)IJ-L(dx) 

<  f JE {117~(X) -17(x)1 2 }J-L(dx) 

11.1  Strong Consistency 

173 

"2 n Var{l{XES 
k 

X,Pn  x 

()}}fl(dx) 

k2 n/-i(SX,Pn(x)/-i(dx) 

= 

I  1 
S  I  1 
=  I / n ~fL(dx) 
=  ~' 

k2  n 

1 

which converges to zero. 

For the expected value of the second term on the right-hand side of (11.1), note 

that in the proof of Theorems 6.3 and 6.4 we already showed that 

lim EI 17J(x)  - 7Jn(X)lfl(dx) = O. 

n-+oo 

Therefore, 

E I 17J~(x) - 7J(x)I/-i(dx) 
S  E I 17J~(x) - 7Jn(x)lfl(dx) + E I 17J(x)  - 77n(x)I/-i(dx)  -+ O. 

Assume now that n is so large that 

E I I~(x) -17(x)I/-i(dx) + E I 17J~(x) - 7J(X)lfl(dx)  <  ~. 

Then, by (11.1), we have 

(11.2) 

::s  p {I Iry;(x) -
+P U I~(x) -

ry(X)lfL(dx) - E I I~~(x) -
i)(X)lfL(dx) - E I 1Ti;:(x) -

ry(X)lfL(dx)  >  ~ } 

i)(X)lfL(dx)  >  ~} . 

Next we get an exponential bound for the first probability on the right-hand side 
of (11.2) by McDiarmid's inequality (Theorem 9.2). Fix an arbitrary realization of 
the data Dn  = (Xl, YI),  ... , (xn, Yn),  and replace (Xi,  yJ by (Xi, Yi),  changing the 
value of 7J~(x) to  7J~i(x), Then 

174 

11.  Consistency of the k-Nearest Neighbor Rule 

But  11J~(x) -
1J~/x)1 is  bounded by 2/ k  and can differ from  zero  only if IIx  -
Xi II  <  Pn(x)  or IIx  - Xi II  <  Pn(x).  Observe that  IIx  - xiii  <  Pn(x) if and only 
if f.L(Sx,lIx-x;lj)  <  kin.  But the  measure  of such  x's  is  bounded  by  Ydkln  by 
Lemma 11.1. Therefore 

and by Theorem 9.2 

Finally, we need a bound for the second term on the right-hand side of (11.2). This 
probability may be bounded by McDiarmid's inequality exactly the same way as 
for the first term, obtaining 

and the proof is completed.  0 

REMARK. The conditions k  -+  00 and kin -+ 0 are optimal in the sense that they 
are also necessary for consistency for some distributions with a density. However, 
for some distributions they are not necessary for consistency, and in fact, keeping 
k =  1 for all n may be a better choice. This latter property, dealt with in Problem 
11.1, shows that the  I-nearest neighbor rule is admissible.  0 

11.2  Breaking Distance Ties 

Theorem  11.1  provides  strong  consistency  under  the  assumption  that  X  has  a 
density. This assumption was needed to avoid problems caused by equal distances. 
Turning to the general case, we see that if f.L does not have a density then distance ties 
can occur with nonzero probability, so we have to deal with the problem of breaking 
them. To see that the density assumption cannot be relaxed to the condition that f.L 
is merely nonatomic without facing frequent distance ties, consider the following 
distribution on n d  x n d

'  with d, d'  ::::  2: 

f.L  =  - (Td  X  ad') + - (ad  x  Td')  , 

1 
2 

1 
2 

where  Td  denotes  the  uniform  distribution  on  the  surface  of the  unit  sphere  of 
n d  and  ad  denotes  the  unit  point mass  at  the  origin  of nd.  Observe  that  if X 
has distribution Td  x  ad'  and X'  has distribution ad  x  Td',  then IlX  X'II  = Ji. 

11.2 Breaking Distance Ties 

175 

Hence, if Xl, X 2 ,  X 3 ,  X4 are independent with distribution J-l, thenP{IIX I - X2 11  = 
IIX3  - X311}  = 1/4. 

Next we list some methods of breaking distance ties. 

•  nE-BREAKING  BY  INDICES:  If Xi  and  Xj  are equidistant from x, then  Xi  is 
declared closer if i  <  j. This method has some undesirable properties. For 
example, if X is monoatomic, with 'f}  <  1/2, then X 1 is the nearest neighbor 
of all  X j 's,  j  >  1,  but  X j  is  only  the  j  -
I-st nearest neighbor of X 1. 
The influence  of X 1  in such a  situation is  too  large,  making the  estimate 
very  unstable  and  thus  undesirable.  In  fact,  in  this  monoatomic  case,  if 
L* + E  <  1/2, 

P {Ln  - L * >  E}  ~ e -ck 

for some c  >  0 (see Problem 11.2). Thus, we cannot expect a distribution(cid:173)
free version of Theorem 11.1  with this tie-breaking method. 

•  STONE'S  TIE-BREAKING:  Stone  (1977)  introduced  a  version  of the  nearest 
neighbor rule, where the labels of the points having the same distance from 
x  as the k-th nearest neighbor are averaged. If we denote the distance of the 
k-th nearest neighbor to x  by Rn (x), then Stone's rule is the following: 

O 

'f 
l

I{i  : lix  - Xi II  <  Rn(x)}1 
' "  
~  I{yi=o}  + --. - - - - - - - - -
I{l  :  Ilx  - Xi II  = Rn(x)}1 
"11  -
·11  R  (  ) 
1.  x  Xl  <  n  X 

k -

gn(x) = 

k 
x  ~  {Yi=O}  ~ 2 

' "   I 

i:llx-xdl=R,,(x) 

1  otherwise. 

This is not a k-nearest neighbor rule in a strict sense, since this estimate, in 
general,  uses more than k  neighbors.  Stone (1977) proved weak universal 
consistency of this rule. 

•  ADDING A RANDOM COMPONENT:  To circumvent the aforementioned difficul(cid:173)

ties, we may artificially increase the dimension of the feature vector by one. 
Define the the d + I-dimensional random vectors 

where the randomizing  variables  V, VI,  ... , Vn  are real-valued i.i.d.  ran(cid:173)
dom variables independent of X, Y, and Dn, and their common distribution 
has  a density.  Clearly,  because of the independence of V, the Bayes error 
corresponding to the pair (X', Y) is the same as that of (X, Y). The algorithm 
performs the k-nearest neighbor rule on the modified data set 

D~ = «X~, Yd, ... , (X~, Yn)). 

It finds  the k nearest neighbors of X', and uses a majority vote among these 
labels to guess Y.  Since V  has a density and is independent of X, distance 

176 

11.  Consistency of the k-Nearest Neighbor Rule 

ties occur with zero probability. Strong universal consistency of this rule can 
be seen by observing that the proof of Theorem 11.1  used the existence of 
the density in the definition of Pn(x) only. With our randomization, Pn(x) is 
well-defined, and the same proof yields strong universal consistency. Inter(cid:173)
estingly, this rule is consistent whenever U has a density and is independent 
of (X, Y).  If,  for  example,  the  magnitude  of U  is  much  larger  than  that 
of II X II,  then the rule defined this  way will significantly differ from the k(cid:173)
nearest neighbor rule,  though it still preserves  universal consistency.  One 
should expect however a dramatic decrease in the performance. Of course, 
if U is very small, then the rule remains intuitively appealing . 

•  TIE-BREAKING  BY  RANDOMIZATION:  There is  another,  perhaps more natural 
way of breaking ties via randomization. We assume that (X,  U) is a random 
vector independent of the data, where U is independent of X  and uniformly 
distributed  on  [0,  1].  We  also  artificially  enlarge  the  data  by introducing 
U1 ,  U2,.'"  Un,  where the U/s are i.i.d. uniform [0,1] as  well. Thus, each 
(Xi, Ui ) is distributed as  (X, U). Let 

(X(1)(X,  u),  YCl)(x, u)), "  ., (XCn)(x,  u), YCn)(x,  u)) 

be a reordering of the data according to increasing values of Ilx - Xi II. In case 
of distance ties, we declare (Xi, Ui )  closer to (x, u) than (X j ,  Uj )  provided 
that 

lUi  - ul  :::  JUj  - uJ. 

Define the k-NN classification rule as 

gn(x) = {O 

if L~=l. I{YCi)Cx,u)=l}  :::;  L~=l I{Y(i)Cx,u)=O} 

1  otherwIse, 

(11.3) 

and denote the error probability of gn  by 

Devroye, Gyorfi,  Krzyzak,  and Lugosi (1994)  proved that  Ln  ---+  L * with 
probability one for  all  distributions  if k  ---+  00  and  k / n  ---+  0.  The basic 
argument  in  (1994)  is  the  same  as  that  of Theorem  11.1,  except that  the 
covering lemma (Lemma 11.1) has to be appropriately modified. 

It should be stressed again that if fL  has a density, or just has an absolutely con(cid:173)

tinuous component, then tie-breaking is needed with zero probability, and becomes 
therefore irrelevant. 

11.3  Recursive Methods 

To find  the nearest neighbor of a point x  among Xl, ... , X n ,  we may preprocess 
the data in  0 (n log n) time,  such that each query  may be answered in  0 (log n) 

11.4 Scale-Invariant Rules 

177 

worst-case  time-see, for  example,  Preparata and  Shamos  (1985).  Other recent 
developments  in computational geometry have  made  the  nearest neighbor rules 
computationally feasible even when n is formidable. Without preprocessing how(cid:173)
ever, one must resort to slow methods. If we need to find a decision at x  and want 
to  process  the data file  once when doing so,  a  simple rule was proposed by De(cid:173)
vroye and Wise  (1980).  It is  a fully  recursive  rule that may be  updated as  more 
observations become available. 

Split  the  data  sequence  Dn  into  disjoint  blocks  of length  II, ... , IN,  where 
II, ... , IN are positive integers satisfying L~1 Ii  = n. In each block find the nearest 
neighbor of x, and denote the nearest neighbor of x  from the i-th block by X7(x). 
Let yt(x) be the corresponding label. Ties are broken by comparing indices. The 
classification rule is defined as a majority vote among the nearest neighbors from 
each block: 

Note that we have only defined the rule gn  for n  satisfying L~l Ii  = n for some 
N.  A  possible extension for  all  n's is  given  by  gn(x)  = gm(x),  where  m  is  the 
largest integer not exceeding n that can be written as  L~l Ii for some N. The rule 
is weakly universally consistent if 

lim  IN  = 00 

N---+oo 

(Devroye and Wise (1980), see Problem 11.3). 

11.4  Scale-Invariant Rules 

A scale-invariant rule is a rule that is invariant under rescalings of the components. 
It is  motivated by  the lack of a universal  yardstick when  components  of a vec(cid:173)
tor represent physically different quantities, such as  temperature, blood pressure, 
alcohol,  and the  number of lost teeth.  More formally,  let x(I), ... , xed)  be the d 
components of a vector x. If 1/1 1,  ... ,  1/1 d  are strictly monotone mappings: n  -+ n, 
and if we define 

1/1 (x ) =  (1/11 (x(l), ... , 1/1 d(x(d)) , 

then gn  is scale-invariant if 

gn(x, Dn) = gn(1/I(x),  D;l)' 

where D:1  = ((1/I(X 1), Y1),  ... , (1J;'(Xn), Yn».  In other words, if all the Xi'S and x 
are transformed in the same manner, the decision does not change. 

Some rules based on statistically equivalent blocks  (discussed in Chapters  21 
and 22) are scale-invariant, while the k-nearest neighbor rule clearly is not. Here we 
describe a scale-invariant modification of the  k-nearest neighbor rule,  suggested 
by Olshen (1977) and Devroye (1978). 

178 

11.  Consistency of the k-Nearest Neighbor Rule 

The scale-invariant k-nearest neighbor rule is  based upon empirical distances 
that are defined in terms of the order statistics along the d  coordinate axes.  First 
order the points x, Xl, ... , Xn  according to increasing values of their first  com(cid:173)
ponents X(I), xiI), ... , X~l), breaking ties via randomization.  Denote the rank of 
xiI) by r?), and the rank of x(l) by r(l). Repeating the same procedure for the other 
coordinates, we obtain the ranks 

ri  ,r  ,  ]  -
(j) 

(j) 

.  - 1  d' - 1 

, ... ,  , l  -

,  ... , n. 

Define the empirical distance between x  and Xi  by 

p(x, Xi) =  max  Ir~j) -

I~j~d  I 

/j)I. 

A k-NN rule can be defined based on these distances, by a majority vote among the 
Yi's with the corresponding Xi'S whose empirical distance from x  are among the 
k smallest.  Since these distances are integer-valued, ties frequently  occur.  These 
ties  should  be  broken  by  randomization.  Devroye  (1978)  proved  that  this  rule 
(with randomized tie-breaking)  is  weakly  universally  consistent when k  ---+  (Xl 
and kj n  ---+  0 (see Problem 11.5).  For another consistent scale-invariant nearest 
neighbor rule we refer to Problem 11.6. 

12 

13 

Ii 

JI 10 

8 

9 

5 

7 

7 

3 

2 

5 

9 

10 

1 

FIGURE  11.1.  Scale-invariant distances of 15 points from  a fixed 
point are shown here. 

11.5  Weighted Nearest Neighbor Rules 

In the k-NN rule, each of the k nearest neighbors of a point x plays an equally impor(cid:173)
tant role in the decision.  However,  intuitively speaking, nearer neighbors  should 

11.6 Rotation-Invariant Rules 

179 

provide more information than more distant ones.  Royall  (1966)  first  suggested 
using rules in which the labels Yi  are given unequal voting powers in the decision 
according to the distances of the Xi'S from x: the i -th nearest neighbor receives 

weight Wni,  where usually  Wnl  :::::  W n2  :::::  ...  :::::  Whn  :::::  ° and 'L7:::1  Wni  =  1.  The 

rule is defined as 

(x) =- {o  if'L7=l wnJ{Y(i)(x)=l}  .:::::  'L7=1  Wni I{Y(i)(x)=Ol 

gn 

1  otherwise. 

We get the ordinary k-nearest neighbor rule back by the choice 

Wni  =  °  otherwise. 

if i  .:::::  k 

1/ k 

{ 

The following conditions for consistency were established by Stone (1977): 

and 

lim  max  Wni  = 0, 
n--HX)  l:::i:::n 
'"""  Wni  = ° 

lim 
n-+oo  6 

for some k  with kin -+ ° (see Problem 11.7). Weighted versions of the recursive 

and scale-invariant methods described above can also be defined similarly. 

k:::i:::n 

11.6  Rotation-Invariant Rules 

Assume that an affine transformation T  is applied to x  and Xl, ... , Xn  (i.e., any 
number of combinations of rotations, translations, and linear rescalings), and that 
for any such linear transformation T, 

gn(X,  Dn) = gn(T(x), D~), 

where  D~ =  «T(Xr), Yr),  ... , (T(Xn ),  ~1))' Then we  call  gn  rotation-invariant. 
Rotation-invariance is indeed a very strong property. In R d , in the context of k-NN 
estimates,  it suffices  to  be  able  to  define  a  rotation-invariant  distance  measure. 
These are necessarily data-dependent.  An example of this  goes  as  follows.  Any 
collection of d  points in general position defines a polyhedron in a hyperplane of 
Rd. For points (Xii' ... , XiJ, we denote this polyhedron by PUl, ... , id). Then 
we define the distance 

p(Xi , x) =  L  I{segment(Xi.x)intersectsP(il •... ,id)}· 

il, ... ,id, 
i~Hil ..... idl 

Near points have few intersections. Using p(., .) in a k-NN rule with k -+  00 and 
k / n  -+ 0, we expect weak universal consistency under an appropriate scheme of 
tie-breaking. The answer to this is left as an open problem for the scholars. 

180 

11.  Consistency of the k-Nearest Neighbor Rule 

3 

FIGURE  11.2.  Rotation-invariant dis-

4  tancesfrom x. 

2 

11.7  Relabeling Rules 

The  I-NN  rule  appeals  to  the  masses  who  crave  simplicity  and  attracts  the  pro(cid:173)
grammers  who  want to  write  short understandable code. Nearest neighbors may 
be found efficiently if the data are preprocessed (see Chapter 5 for references). Can 
we make the  I-NN rule universally consistent as well? In this section we introduce 
a tool called relabeling, which works as follows. Assume that we have a classifica(cid:173)
tion rule {gn(x,  Dn), x  E  n d
, n  ~ I}, where Dn  is the data (Xl, Yl ), ... ,  (Xn, Yn). 
This rule will be called the ancestral rule. Define the labels 

These are the decisions for the Xi'S themselves obtained by mere resubstitution. 
In  the  relabeling method,  we  apply  the  I-NN  rule  to  the new data (Xl, Zl), ... , 
(Xn,  Zn). If all  goes well, when the  ancestral rule gn  is universally consistent, so 
should the relabeling rule. We will show this by example, starting from a consistent 
k-NN rule as ancestral rule (with k --+  00, kin --+  0). 

Unfortunately,  relabeling  rules  do  not  always  inherit  consistency  from  their 
ancestral rules, so that a more general theorem is more difficult to  obtain, unless 
one adds in a lot of regularity conditions-this does not seem to be the right time 
for that sort of effort. To  see that universal consistency of the ancestral rule does 
not imply consistency of the relabeling rule, consider the following rule hn : 

if x  = Xi  and x  =I X j, all  j  =I i 
otherwise, 

where gn  is  a weakly universally consistent rule.  It is easy to show (see Problem 
11.15) that hn  is universally consistent as well. Changing a rule on a set of measure 
zero indeed does not affect Ln. Also, if x  = Xi  is at an atom of the distribution of 
X, we only change gn  to 1 - Yi  if Xi  is the sole occurrence of that atom in the data. 
This has  asymptotically no impact on Ln. However, if hn is used as  an ancestral 
rule,  and  X  is  nonatomic, then  hn(Xi ,  Dn)  =  1 
Yi  for  all  i, and therefore, the 
relabeling rule is a I-NN rule based on the data (X 1,  1 - Yl ),  ••• , (Xn,  1 - Yn).  If 
L *  = 0 for the  distribution of (X, Y), then the relabeling rule has  probability of 
error converging to one! 

For most nonpathological ancestral rules, relabeling does indeed preserve uni(cid:173)

versal consistency. We offer a prototype proof for the k-NN rule. 

11.7 Relabeling Rules 

181 

Theorem 11.2.  Let gn  be the k-NN rule in which tie-breaking is done by random(cid:173)

ization as  in  (11.3).  Assume that k  -+  00  and k / n  -+ ° (so  that  gn  is  weakly 

universally consistent).  Then the  relabeling rule based upon gn  is weakly univer(cid:173)
sally consistent as well. 

PROOF. We verify the conditions of Stone's weak convergence theorem (see Theo(cid:173)
rem 6.3). To keep things simple, we assume that the distribution of X has a density 
so that distance ties happen with probability zero. 

In our case, the weight Wni(X) of Theorem 6.3 equals 1/ k iff Xi  is among the 
k nearest neighbors of X(l)(X), where X(l)(X) is the nearest neighbor of X. It is 
zero otherwise. 

Condition 6.3 (iii) is trivially satisfied since k  -+  00. For condition 6.3  (ii), we 
note that if X(i)(x) denotes the i-th nearest neighbor of X among Xl, ... , X n , then 

Just note that  SX(1)(x).2I1x-X(k)1I  ;2  Sx.llx-Xck)11  and that the latter sphere contains k 
data points. But we already know from the proof of weak consistency of the k-NN 
rule that if k / n  -+ 0,  then for all E  >  0, 

p  {IIX(k)(X) - XII  >  E}  -+ 0. 

Finally,  we  consider  condition  6.3  (i).  Here  we  have,  arguing  partially  as  in 

Stone (1977), 

=  E {~t I{XisamongthekNN'sofXcJ)(Xi)in  (Xl, ... ,Xn.X}-IXd}!(X)} 

1=1 

(reverse the roles of Xi  and X) 

1  n 

n 

=  E{k LLI{Xj is the NN of Xi  in  {Xl ..... Xn.X}-{Xd} 

i=l 

j=l 

X I{X is among the k NN's of Xj  in  {Xl, ... ,Xn,X}-{Xd}!(X)}, 

However, by Lemma 11.1, 

n 
L 
i=l 

I{Xj  is theNN of Xi  in  {Xl .... ,X".X}-{XdJ  :::  Yd· 

182 

11.  Consistency of the k-Nearest Neighbor Rule 

Also, 

n L I{x is among thek NN's of Xj  in  {Xl"",Xn,X}-{Xd}  :::s  kYd' 

j=l 

Therefore, by a double application of Lemma 11.1, 

E { t ~ Irx,  j,,""ong thok NN', of XO)(XJJ!(X,)}  :'0  yJ Ef(X), 

and condition 6.3  (i) is verified.  0 

Problems and Exercises 
PROBLEM  11.1.  Show that the conditions k -+  00 and k / n  -+ ° are necessary for universal 
bounded, lim inf n-HX) ELn  >  L *.  Exhibit a second distribution such that if k / n  ::::  E  > ° 

consistency of the k-nearest neighbor rule. That is, exhibit a distribution such that if k remains 

for all n, and some E,  then lim infn- Hx) ELn  >  L *. 

PROBLEM  11.2.  Let X be monoatomic,  with 1]  <  1/2. Show that for E  <  1/2 -1], 

for  some c  >  0,  where  Ln  is  the error probability of the  k-NN rule  with  tie-breaking by 
indices. 

PROBLEM  11.3.  Prove that the recursive nearest neighbor rule is universally consistent pro(cid:173)
vided that limN-+oo IN  = 00  (Devroye  and Wise  (1980)).  HINT:  Check the  conditions  of 
Theorem 6.3. 

PROBLEM  11.4.  Prove that the nearest neighbor rule defined by  any  L p-distance measure 

° <  p  :::::  00  is  universally  consistent under the usual conditions on k.  The  L p -distance 

between x, y  End is defined by (E1=1Ix(i) - y(i)IP riP forO  <  p  <  oo,andbysuPi Ix(i)(cid:173)
y(i) I for p  = 00, where x  = (x(1),  ... , xed»).  HINT: Check the conditions of Theorem 6.3. 
PROBLEM  11.5.  Let o-(x, z,  Xi, Zd = p(x, Xi)+ IZi  - zl  be a generalized distance between 
(x, z) and (Xi, Zi), where x, Xl, ... , Xn  are as in the description of the scale-invariantk-NN 
rule,  p(x, Xi)  is  the empirical distance defined there,  and z,  Zi  E  [0,1] are real numbers 
added  to  break  ties  at  random.  The  sequence  Zl, ... , Zn  is  i.i.d.  uniform  [0,  1]  and  is 
independent of the data  Dn.  With the  k-NN rule based on the artificial distances  0-,  show 
that the  rule is  universally  consistent by verifying  the conditions  of Theorem  6.3,  when 
k  -+  00 and kin -+ 0. In particular, show first that if Z is uniform [0,  1]  and independent 
of X, Y,  Dn  and  ZI, ... , Zn,  and if Wni(X,  Z) is  the  weight of (Xi, Zi) in  this  k-NN rule 
(i.e., it is 1/ k iff (Xi,  Zi) is among the k  nearest neighbors of (X, Z) according to 0-), then 

(1) 

E {I:7=1  Wni(X,  Z)f(Xd}  :::::  2d Ef(X) 

for all nonnegative measurable  f  with Ef(X) <  00. 

Problems and Exercises 

183 

(2) 

If kin -+ 0, then 

for all a  >  O. 

HINT: Check the conditions of Theorem 6.3 (Devroye (1978)). 

PROBLEM  11.6.  The layered nearest neighbor rule  partitions the space at x  into 2d  quad(cid:173)
rants.  In  each quadrant, the outer-layer points are marked, that is,  those  Xi  for which the 
hyperrectangle defined by x  and Xi  contains no other data point. Then it takes a majority 
vote over the Yi 's for the marked points. Observe that this rule is scale-invariant. Show that 
whenever X has nonatornic marginals (to avoid ties), E{Ln }  -+  L * in probability. 

• 

•  • @  i@  ~  • 

FIGURE  11.3.  The  layered  nearest  neighbor  rule 
takes a majority vote over the marked points. 

-----------_.---------.----------.--------------------------

@ 

@ 

xl 
i empty  i 
•  ®  '---:@xi 

@ 

• 

HINT: It suffices to show that the number of marked points increases unboundedly in prob(cid:173)
ability, and that its proportion to unmarked points tends to zero in probability. 

PROBLEM  11.7.  Prove weak universal consistency of the weighted nearest neighbor rule if 
the weights satisfy 

lim  max  Wni  = 0, 
n--')-oo  lsiSn 

and 

lim  " 
n--')-oo  ~ 

ksisn 

Wni  = 0 

for some k with kin -+ 0 (Stone (1977). HINT: Check the conditions of Theorem 6.3. 
PROBLEM  11.8.  If (Wnl'  ... ,  wnn )  is  a probability vector,  then limn--,)-oo  Li>no Wni  = 0  for 
all  8  >  0  if and  only if there  exists  a  sequence  of integers  k  = kn  such  that k  =  o(n), 
k  -+  00, and  Li>k Wni  =  0(1).  Show this.  Conclude that the conditions of Problem  11.7 
are equivalent to 

(i) 

(ii) 

lim  max Wni  = 0, 
n--')-oo  lSisn 
lim  ~ Wni  = 0  for all 8  >  O. 
n--')-oo~ 
i>no 

PROBLEM  11.9.  Verify the conditions of Problems  11.7 and 11.8 for weight vectors of the 
form Wni  =  Cn I i Ci
,  where a  >  0 is a constant and Cn  is a normalizing constant. In particular, 
check that they do not hold for a  >  1 but that they do hold for 0  <  a  :s  1. 

PROBLEM  l1.lO.  Consider 

as  weight vector. Show that the conditions of Problem 11.8 hold if Pn  -+ ° and npn  -+  00. 

III  -

1 :s  i :s n, 

w·-

Pn 

(1  + PnY 

1 

x - - - - -
1 - (1  + Pn)-n ' 

184 

11.  Consistency of the k-Nearest Neighbor Rule 

PROBLEM  11.11.  Let Wni  = P{Z  = i}/P{Z  :s  n},  1 :s  i  :s  n,  where  Z  is  a Poisson ()"n) 
random  variable.  Show  that  there  is  no  choice  of An  such  that  {Wni,  1  :s  i  :s  n}  is  a 
consistent weight sequence following the conditions of Problem 11.8. 
PROBLEM  11.12.  Let Wni  = P{Binomial(n, Pn) = i} = G)P;7(1- Pn)n-i. Derive conditions 
on Pn  for this choice of weight sequence to be consistent in the sense of Problem 11.8. 

PROBLEM  11.13.  k-NN DENSITY  ESTIMATION.  We  recall from Problem 2.11  that if the con(cid:173)
ditional densities  /0, /1  exist,  then  L 1-consistent density estimation leads  to  a consistent 
classification  rule.  Consider now  the  k-nearest  neighbor  density  estimate  introduced by 
Lofts gaarden  and  Quesenberry  C 1965).  Let  XI, ... , X n  be  independent,  identically  dis(cid:173)
tributed random variables in R d
by 

, with common density /. The k-NN estimate of /  is defined 

fnCx)  =  ( 

A  Sx,lIx- XCk)(xlll 

k 

)' 

where  X(klCx)  is  the  k-th nearest neighbor of x  among  Xl, ... , X n .  Show  that for  every 
n, f  If(x) -
fn(x)ldx  = 00, so that the  density estimate is  never consistent in L 1•  On the 
other hand, according to Problem 11.14, the corresponding classification rule is consistent, 
so read on. 

PROBLEM  11.14.  Assume that the conditional densities  fo  and f1  exist. Then we can use a 
rule suggested by Patrick and Fischer (1970): let No  = L7=1  I{Yi=o}  and N1  = n - No  be the 
number of zeros and ones in the training data. Denote by X6k\x) the k-th nearest neighbor 
of x  among the X/s with Yi  = 0. Define  XiklCx)  similarly. If ACA) denotes the volume of a 
set A  C  Rd, then the rule is defined as 

if NO/A  (Sx,IIx-X6k)(Xlll)  2:  NdA (Sx,llx-xik)(XllI) 
otherwise. 

This estimate is based on the k-nearest neighbor density estimate introduced by Loftsgaarden 
and Quesenberry (1965). Their estimate of fo  is 

Then the rule gn  can be re-written as 

if po,nlo,n  2:  Pl,nlr,n 
otherwise, 

where PO,n  = No/ nand Pl,n  = Nd n  are the obvious estimates of the class probabilities. 
Show  that  gn  is  weakly  consistent if k  --+  00  and  kin  --+  0,  whenever the  conditional 
densities exist. 

PROBLEM  11.15.  Consider a weakly universally consistent rule gn,  and define the rule 

if x  = Xi  and x  =I X j, all  j  =I i 
otherwise. 

Show that h n  too is weakly universally consistent. Note: it is the atomic Cor partially atomic) 
distributions of X that make this exercise interesting. HINT:  The next exercise may help. 

Problems and Exercises 

185 

PROBLEM  11.16.  Let X have an atomic distribution which puts probability Pi  at atom i. Let 
Xl, ... , Xn  be an U.d. sample drawn from this distribution. Then show the following. 

(1)  P{\N - EN\  >  E}  S  2e-2E2In, where E  >  0,  and N  is the number of "occupied" 

atoms (the number of different values in the data sequence). 

(2)  EN In  -+ O. 
(3)  N I n  -+ 0 almost surely. 
(4)  Li:X rli for all  j Sn  Pi  -+ 0 almost surely. 

PROBLEM  11.17.  ROYALL'S  RULE.  Royall (1966) proposes the regression function estimate 

1  LnhnJ  (i  ) 

l]n(X) = -,- L 1  -h  Y(i)(X), 

n 111 

i=I 

n  n 

where leu) is a smooth kernel-like function on [0,  1]  with f~ l(u)du =  1 and h n  >  0 is a 
smoothing factor.  Suggestions included 

(i) 

(ii) 

leu) ==  1; 

(d + 2?  ( 

l(u)=-4- 1- d+2u 

d + 4  21d) 

Define 

(note that this function becomes negative). 

if l]11(X)  >  1/2 
otherwise. 

Assume that hn  -+  0,  nhn  -+  00.  Derive  sufficient  conditions  on  1  that guarantee the 
weak universal consistency of Royall's rule. In particular, insure that choice (ii) is weakly 
universally consistent. HINT: Try adding an appropriate smoothness condition to  l. 

PROBLEM  11.18.  Let K be a kernel and let Rn(x) denote the distance between x  and its k-th 
nearest neighbor,  X (k) (x), among X I, ... ,  X n .  The discrimination rule that corresponds to 
a kernel-type nearest neighbor regression function estimate of Mack (1981) is 

if  "'~l  (2Y.  -
otherwise. 

L.a=1 

I 

I)K (X-Xi)  <  0 

Rn(x) 

-

(The idea of replacing the  smoothing factor in  the kernel estimate by  a local rank-based 
value such as Rn(x) is due to Breiman, Meisel, and Purcell (1977).) For the kernel K  =  Iso.!, 
this rule coincides with the k-NN rule.  For regular kernels  (see Chapter 10),  show that the 
rule remains weakly universally consistent whenever k  -+  00 and kin  -+  0 by verifying 
Stone's conditions of Theorem 6.3. 

PROBLEM  11.19.  Let {gn}  be a weakly universally  consistent sequence of classifiers. Split 
the data sequence Dn  into two parts:  Dm  = «X1' YI ),  ... ,  (Xnn  Ym»  and Tn- m = «Xm+I , 
Ym+1),  ••• , (Xn' Yn». Use gn-m  and the second part Tn - m to relabel the first part, i.e., define 
Y:  = gn-m(Xi ,  Tn- m) for i  = 1,  ... , m. Prove that the I-NN rule based on the data (Xl, Y{), 
... , (Xm,  Y~) is weakly universally consistent whenever m  -+  00 and n - m  -+ 00. HINT: 
Use Problem 5.40. 

PROBLEM  11.20.  Consider the k-NN rule with a fixed k  as the ancestral rule, and apply the 
1-NN rule using the relabeled data. Investigate the convergence of ELn. Is the limit LkNN  or 
something else? 

12 
Vapnik-Chervonenkis Theory 

12.1  Empirical Error Minimization 

In this chapter we select a decision rule from a class of rules with the help of training 
data. Working formally, let C be a class of functions ¢  : nd  -+  {a,  1}. One wishes 
to  select a function from C with small error probability. Assume that the training 
data Dn  = ((Xl, Y1),  ... , (X n,  Yn))  are given to pick one of the functions from C 
to  be used as  a classifier. Perhaps the most natural way of selecting a function is 
to minimize the empirical error probability 

1  n 

Ln(¢) =  - L I{</l(xi)=!Yd 

..-... 

n  i=l 

over the class C.  Denote the empirically optimal rule by ¢~: 

¢~ = argmin Ln(¢). 

</lEC 

Thus,  ¢~ is  the classifier that,  according to the data D n ,  "looks best" among the 
classifiers in C.  This idea of minimizing the empirical risk in the construction of 
a rule was developed to great extent by Vapnik and Chervonenkis (1971;  1974c; 
1974a; 1974b). 

Intuitively,  the  selected classifier ¢~ should be good in the  sense that its  true 
error probability L(¢~) = P{¢~(X) =I YIDn} is expected to be close to the optimal 
error probability within the  class.  Their difference is  the quantity that primarily 
interests us  in this chapter: 

L(¢~) -

inf L(¢). 
</lEC 

188 

12.  Vapnik-Chervonenkis Theory 

The latter difference may be bounded in a distribution-free manner, and a rate of 
convergence  results  that  only  depends  on  the  structure  of C.  While  this  is  very 
exciting, we must add that L(¢~) may be far away from the Bayes error L *. Note 
that 

L(¢~) - L * = (L(¢~) -

inf L(¢») + (inf L(¢) - L *) . 

¢EC 

¢EC 

The  size  of C is  a  compromise:  when  C is  large,  inf¢Ec L(¢)  may  be  close  to 
L *,  but the  former  error,  the  estimation  error,  is  probably large  as  well.  If C is 
too  small,  there  is  no  hope  to  make  the  approximation  error  inf¢Ec L(¢) - L * 
small. For example, if C is  the class of all  (measurable) decision functions,  then 
we  can  always  find  a  classifier in C with  zero  empirical error,  but it may  have 
arbitrary  values  outside  of the  points  Xl, ... , X n'  For example,  an  empirically 
optimal classifier is 

if x  = Xi, i = 1,  ... ,n 
otherwise. 

This is clearly not what we are looking for.  This phenomenon is called overjitting, 
as  the overly large class C overfits the data.  We  will give precise conditions on C 
that allow us to avoid this anomaly. The choice of C such that inf¢Ec L(¢) is close 
to  L * has  been the  subject of various  chapters on consistency-just assume that 
C is allowed to grow with n in some manner.  Here we take the point of view that 
C is fixed,  and that we have to live with the functions in C.  The best we may then 
hope for is to minimize L(¢~)  inf¢Ec L(¢). A typical situation is shown in Figure 
12.1. 

picked rule 

best rule 
in class 
inf 
<pEe 

stimation error 

(can be controlled) 
(small) 

Approximation error 
(not controllable) 
(usually larger than estimation error) 

FIGURE  12.1.  Various errors in empirical classifier selection. 

12.1  Empirical Error Minimization 

189 

Consider first a finite  collection C,  and assume that one of the classifiers in C 
has zero error probability, that is, min¢Ec L( ¢) = O.  Then clearly, Ln (¢~) = 0 with 
probability one. We then have the following performance bound: 

Theorem 12.1.  (VAPNIK  AND  CHERVONENKIS  (l974c».  Assume  ICI  <  00  and 
min¢Ec L( ¢) = O.  Then for every nand E  >  0, 

and 

PROOF.  Clearly, 

E{L(¢~)} :::;  1 + log ICI. 

n 

P{L(¢~) >  E}  <  P { 

Illax 

¢EC:L,,(¢)=O 

L(¢) >  E I 

E { I{max1>Ec:Ln(4));{) L(¢»E} } 

~  E {<g:t 1{i;"(¢)=fJ)I{l>(¢»,) I 

¢EC:L(¢»E 

< 

L  P{Ln(¢)=O} 

since the probability that no (Xi, Yi ) pair falls in the set {(x, y) : ¢(x) =I y} is less 
than (1  - E)n if the probability of the set is larger than E. The probability inequality 
of the theorem follows from the simple inequality 1 - x  :::;  e-x . 

To bound the expected error probability, note that for any u  >  0, 

E{L(¢I~)}  =  f" P{L(</>~) >  t}dt 
<  u + 1"0 P{L(</>~) >  t}dt 

< 

u + ICI/.

oo 

e- nt dt 

= 

u+-e 

ICI  -nu 
. 
n 

Since u  was  arbitrary,  we may choose it to minimize the obtained upper bound. 
The optimal choice is u  = log ICI/n, which yields the desired inequality.  0 

Theorem 12.1 shows that empirical selection works very well if the sample size 
n is much larger than the logarithm of the size of the family C.  Unfortunately, the 

190 

12.  Vapnik-Chervonenkis Theory 

assumption on the  distribution of (X, Y),  that is,  that min¢Ec L( ¢)  =  0,  is  very 
restrictive. In the sequel we drop this assumption, and deal with the distribution(cid:173)
free problem. 

One of our main tools is taken from Lemma 8.2: 

This  leads  to  the  study  of uniform deviations  of relative  frequencies  from  their 
probabilities by the following simple observation:  let v be a probability measure 
of (X, Y) on nd x {O,  I}, and let Vn  be the empirical measure based upon Dn. That 
is,  for  any fixed  measurable  set  A  C  nd  x  {O,  I},  v(A)  =  P{(X, Y)  E  A}, and 
vn(A) = ~ L7=1  I{(xi,Yi)EAj.  Then 

L(¢) = v({(x, y) : ¢(x) i  y}) 

is  just the  v-measure of the  set of pairs  (x, y)  E  n d  x  {O,  I},  where ¢(x) i  y. 
Formally, L(¢) is the v-measure of the set 

{{x: ¢(x) = I}  x  {O}} U {{x: ¢(x) = O}  x  {I}}. 

Similarly, Ln(¢) = vn({(x, y) : ¢(x) i  y}). Thus, 

sup ILn(¢) - L(¢)I =  sup Ivn(A) - v(A)I, 
¢EC 

AEA 

where A is the collection of all sets 

{{x: ¢(x) =  I}  x  {O}} U {{x: ¢(x) = O}  x  {I}},  ¢  E  C. 

For a fixed set A, for any probability measure  v,  by the law of large numbers 

vn(A) - v(A) ---+ ° almost surely as n  ---+  00. Moreover, by Hoeffding's inequality 

(Theorem 8.1), 

P{lvn(A) - v(A)1  >  E}  :::::  2e-2nE2

. 

However,  it is a much harder problem to obtain such results for SUPAEA  Ivn(A) -
v(A)I. If the class of sets A (or, analogously, in the pattern recognition context, C) 
is of finite cardinality, then the union bound trivially gives 

However, if A contains infinitely many sets (as in many of the interesting cases) 
then the  problem becomes nontrivial,  spawning a vast literature.  The most pow(cid:173)
erful weapons to attack these problems are distribution-free large deviation-type 
inequalities first  proved by Vapnik and Chervonenkis  (1971)  in their piGneering 
work. However, in some situations, we can handle the problem in a much simpler 
way. We have already seen such an example in Section 4.5. 

12.2 Fingering 

191 

12.2  Fingering 

Recall that in Section 4.5 we studied a specific rule that selects a linear classifier by 
minimizing the empirical error. The performance bounds provided by Theorems 
4.5 and 4.6 show that the selected rule performs very closely to the best possible 
linear rule. These bounds apply only to the specific algorithm used to find the em(cid:173)
pirical minima-we have not showed that any classifier minimizing the empirical 
error performs well. This matter will be dealt with in later sections. In this section 
we extend Theorems 4.5 and 4.6 to classes other than linear classifiers. 

Let C be the class of classifiers assigning 1 to those x  contained in a closed hy(cid:173)

perrectangle, and 0 to all other points. Then a classifier minimizing the empirical 
error Ln (¢) over all ¢  E  C may be obtained by the following algorithm:  to each 
2d-tuple (Xii' ... , X i2d )  of points from Xl, ... , X n , assign the smallest hyperrect(cid:173)
angle containing these points. If we assume that X  has a density, then the points 
X 1,  ... ,  Xn  are in  general position with probability one.  This way we obtain at 
most (;d)  sets. Let ¢i be the classifier corresponding to the i-th such hyperrectan(cid:173)
gle, that is, the one assigning 1 to those x  contained in the hyperrectangle, and 0 
to other points. Clearly, for each ¢  E C, there exists a ¢i, i  =  1, ... , (2~)' such that 

for all X j, except possibly for those on the boundary of the hyperrectangle. Since 
the  points are  in  general  position,  there  are  at most 2d such exceptional points. 
Therefore, if we select a classifier;;; among ¢1, ... , ¢G~) to minimize the empirical 
error, then it approximately minimizes the empirical error over the whole class C 
as  well.  A  quick scan through the  proof of Theorem 4.5  reveals that by similar 
arguments we may obtain the performance bound 

for n  :::  2d and E  :::  4d/n. 

The idea may be generalized. It always works, if for some k, k-tuples of points 
determine classifiers from C such that no matter where the other data points fall, 
the  minimal empirical error over these sets coincides with the overall minimum. 
Then we may fix these sets-"put our finger on them"-and look for the empirical 
minima over this finite collection. The next theorems, whose proofs are left as an 
exercise (Problem 12.2), show that if C has this property, then "fingering" works 
extremely well whenever n » k. 

Theorem 12.2.  Assume that the class C of classifiers has the following property: 
:  {Rd)k  ~ C such  that for  all 
for  some  integer  k  there  exists  a function  \11 
Xl,  ••• , xn  E  Rd  and all ¢  E  C,  there  exists a k-tuple  iI, ... , ik  E  {I, ... ,n} of 
different indices such that 

W(Xi"  ... , Xh)(X) = ¢(Xj) 

for all j  = 1, ... , n with j  =! ii, 1= 1, ... , k 

192 

12.  Vapnik-Chervonenkis Theory 

with probability one. Let ¢ be found by fingering,  that is,  by empirical error mini(cid:173)
mization over the collection of n! / (n  - k)!  classifiers of the form 

iI, ... , ik  E  {1,  ... , n}, 

different. 

Then for n  2:  k and 2k / n  .:s  E  .:s  1, 

P  {L(4J)  -

inf L(¢)  >  E}  .:s  e2kE  (n k + 1) e-
epEC 

nE2

/2. 

Moreover,  if n  2:  k,  then 

E {L(¢) -

inf L(¢)}  .:s 
epEC 

2(k + 1) log n + (2k + 2) 

n 

The smallest k for  which C has  the property described in the theorem may be 
called the fingering dimension of C.  In most interesting cases, it is independent of 
n. Problem 12.3 offers a few such classes. We will see later in this chapter that the 
fingering  dimension is  closely related the  so-called vc dimension of C (see also 
Problem 12.4). 

Again,  we  get  much  smaller  errors  if infepEc  L(¢)  =  O.  The  next  inequality 

generalizes Theorem 4.6. 

Theorem 12.3.  Assume that C has the property described in  Theorem 12.2 with 
fingering  dimension k.  Assume,  in addition,  that infepEc  L(¢) = O.  Then for all n 
and E, 

and 

----} 
E  L(¢) 

{ 

.:s 

k logn + 2 
. 

n-k 

REMARK.  Even though the results in the  next few  sections based on the Vapnik(cid:173)
Chervonenkis  theory  supersede those of this  section (by requiring less  from the 
class C and being able to bound the error of any classifier minimizing the empirical 
risk), we must remark that the exponents in the above probability inequalities are 
the best possible, and bounds of the same type for  the general case can only be 
obtained with significantly more effort.  D 

12.3  The Glivenko-Cantelli Theorem 

In the next two sections, we prove the Vapnik-Chervonenkis inequality, a powerful 
generalization of the classical Glivenko-Cantelli theorem. It provides upper bounds 
on random variables of the type 

sup  Ivn(A)  - v(A)I. 
AEA 

12.3 The Glivenko-Cantelli Theorem 

193 

As  we  noted  in  Section  12.1,  such  bounds  yield  performance  bounds  for  any 
classifier selected by minimizing the empirical error. To  make the material more 
digestible, we first present the main ideas in a simple one-dimensional setting, and 
then prove the general theorem in the next section. 

We drop the pattern recognition setting momentarily, and return to probability 
theory. The following theorem is sometimes referred to as the fundamental theorem 
of mathematical statistics, stating uniform almost sure convergence of the empirical 
distribution function to the true one: 

Theorem 12.4.  (GLIVENKO-CANTELLI  THEOREM).  Let  Zl, ... , Zn  be  i.i.d.  real(cid:173)
valued random variables  with distribution function  F(z)  =  P{Zl  ::::::  z}.  Denote 
the standard empirical distribution function by 

Then 

P {sup IF(z) - FnCz)1  >  E)  ::::::  8Cn + 1)e-nE2/32, 

ZER 

and,  in particular,  by the Borel-Cantelli lemma, 

lim  sup IF(z) - Fn(z)1  = 0  with probability one. 
n-fOO ZER 

PROOF.  The proof presented here is not the  simplest possible, but it contains  the 
main ideas  leading  to  a powerful generalization.  Introduce  the  notation  v(A)  = 
P{Zl  E A} and vnCA) = (lIn) 2:;:::1  I{ZjEA}  for all measurable sets A  c  R. Let A 
denote the class of sets of form (-00, z] for z E R. With these notations, 

sup IF(z) - Fn(z)1  =  sup  Ivn(A)  - v(A)I. 
ZER 

AEA 

We prove the theorem in several steps, following symmetrization ideas of Dudley 
(1978),  and Pollard (l984). We  assume that nE 2  :::  2,  since otherwise the bound 
is trivial. In the first step we introduce a symmetrization. 

STEP 1. FIRST SYMMETRIZATION BY A GHOST SAMPLE.  Define the random variables 
Zi, ... , Z~ E  R  such that Zl,""  Zn' Zi, ... , Z:1  are all independent and iden(cid:173)
tically distributed. Denote by v~ the empirical measure corresponding to the new 
sample: 

Then for nE 2  :::  2 we have 

P {sup Ivn(A)  - v(A)1  >  E)  ::::::  2P {sup Ivn(A)  - v;/A) I >  ~) . 

AEA 

AEA 

2 

194 

12.  Vapnik-Chervonenkis Theory 

To see this, let A * E  A be a set for which IVn (A *) - v (A *) 1 >  E if such a set exists, 
and let A * be a fixed set in A otherwise. Then 

P {sup Ivn(A) - v~(A)1 >  E 12} 

AEA 
>  P {lvn(A *) - v~(A *)1  >  E 12} 
>  P  {lvn(A *) - v(A *)1  >  E,  Iv~(A *) - v(A *)1  <  ~} 

=  E { I{lvn(A*)-v(A*)I>E}P  { Iv~ (A *) 

v(A *)1  <  ~ J  Zl, ... , Zn} }  . 

The conditional probability inside may be bounded by Chebyshev's inequality as 
follows: 

P 

{ 

IVn(A) - v(A  )1  <  2  Zl, ... , Zn 
} 

EJ 

* 

* 

f 

>  1 _  v(A *)(1  - v(A *)) 

nE2/4 

1 
>  1 - -> -
nE 2  - 2 

1 

whenever nE 2  ::::  2.  In summary, 

P {sup Ivn(A) - v~(A)1 >  E 12} 

AEA 

1 

::::  2 P{lvn(A *) - v(A *)1  >  E} 

> 

~ P {sup IVn(A)  - v(A)1  >  E} . 
2 

AEA 

STEP 2.  SECOND  SYMMETRIZATION BY RANDOM SIGNS.  Let ()1,  ... ,  ()n  be i.i.d. sign 
variables, independent of Zl, ... , Zn  and Z~, ... , Z~, with P{()i  = -I} = P{()i = 
I} = 1/2. Clearly, because Zl, Zi, ... , Zn,  Z~ are all independent and identically 
distributed, the distribution of 

is the same as the distribution of 

!~~ ItUA(Zi) - iA(Z;))I 
sup It ()iUA(Zi) -

IA(Z;))I· 

AEA 

i=l 

Thus, by Step 1, 

P {sup Ivn(A) - v(A)1  >  E} 

AEA 

:s  2P {sup  1  ItUA(Zi) -

AEA n 

i=l 

IA(Z))I >  :.} 

2 

12.3 The Glivenko-Cantelli Theorem 

195 

Simply applying the union bound, we can remove the auxiliary random variables 
Zi, ... , Z~: 

STEP 3.  CONDITIONING.  To bound the probability 

we  condition  on  Z 1,  ... ,  Zn.  Fix  Z 1,  ... ,Zn  E  n d,  and  note  that  as  Z ranges 
over n, the  number  of different  vectors  (I{zl :::z},  ... , I{zn :::Z})  is  at  most  n  + 1. 
Thus, conditional on Zl, ... , Zn, the supremum in the probability above is just a 
maximum taken over at most n + 1 random variables.  Thus, applying the union 
bound gives 

With the supremum now outside the probability, it suffices to find an exponential 
bound on the conditional probability 

STEP  4.  HOEFFDINO'S  INEQUALITY.  With  Zl,  ... , Zn  fixed,  L7=1  aJA(Zi)  is  the 
sum of n  independent zero mean random variables bounded between -1 and  1. 
Therefore, Theorem 8.1  applies in a straightforward manner: 

Thus, 

P 

{ 

1 I ~  I 

E I 
4 

sup - ~aJA(Zi)  >  - Zl, ... , Zn 
AEA n 

i=l 

} 

:s  2(n + l)e 

-nE

2

. 
/32 

196 

12.  Vapnik-Chervonenkis Theory 

Taking the expected value on both sides we have 

In summary, 

P {sup Ivn(A)  - v(A)1  >  E}  :::::  8(n + l)e-nE2/32.  D 

AEA 

12.4  Uniform Deviations of Relative Frequencies 

from Probabilities 

In this  section we prove the  Vapnik-Chervonenkis inequality,  a mighty  general(cid:173)
ization  of Theorem  12.4.  In  the  proof we  need  only  a  slight  adjustment  of the 
proof above.  In  the  general  setting,  let  the  independent  identically  distributed 
random variables  Zl, ... , Zn  take  their values  from  Rd. Again,  we use  the  no(cid:173)
tation v(A) = P{ZI  E  A} and vn(A) = (lIn) L~=l I{zjEA}  for all measurable sets 
A  C  Rd. The Vapnik-Chervonenkis  theory  begins  with the concepts of shatter 
coefficient and Vapnik-Chervonenkis (or vc) dimension: 

DEFINITION  12.1.  Let A  be  a  collection  of measurable  sets.  For  (Zl,  ... ,Zn)  E 
{Rd}n,  let N A(Zl, ... , Zn)  be the number of different sets in 

{ {Z 1,  ... , Zn}  n A; A  E  A}. 

The  n-th shatter coefficient of A  is 

That is, the shatter coefficient is the maximal number of different subsets of n points 
that can be picked out by the class of sets A. 

The shatter coefficients measure the richness of the class A. Clearly, seA, n) ::::: 
2n, as there are 2n subsets of a set with n elements. If N A(Zl, ... , Zn)  = 2n for some 
(Zl,  ... , Zn), then we say that A  shatters {Zl,  ... , Zn}. If seA, n)  <  2n, then any set 
of n points has a subset such that there is no set in A that contains exactly that subset 
of the n points. Clearly, if seA, k)  <  2k  for some integer k, then seA, n)  <  2n for 
all n  >  k.  The first time when this happens is important: 

DEFINITION  12.2.  Let A  be a  collection of sets with  IAI  :::::  2.  The  largest integer 
k  :::::  1 for  which  seA, k)  =  2k  is  denoted  by  VA,  and  it  is  called the  Vapnik(cid:173)
Chervonenkis dimension (or vc dimension) of the class A. If seA, n) = 2n for all 
n,  then by definition,  VA  = 00. 

12.4 Uniform Deviations of Relative Frequencies from Probabilities 

197 

For example, if A contains allhalfiines of form (-00, x],  x  E R, thens(A, 2) = 
3  <  22 ,  and  VA  = 1.  This is  easily seen by observing that for any two different 
points Zl  <  Z2  there is no set of the form (-00, x] that contains Z2,  but not Zl.  A 
class of sets A for which VA  <  00 is called a Vapnik-Chervonenkis (orve) class. In 
a sense, V A  may be considered as the complexity, or size, of A. Several properties 
of the shatter coefficients and the ve dimension will be shown in Chapter 13. The 
main purpose of this section is to prove the following important result by Vapnik 
and Chervonenkis (1971): 

Theorem 12.5.  (VAPNIK AND CHERVONENKIS (1971)). For any probability measure 
v and class of sets A, and for any nand E  >  0, 

P  {sup Ivn(A) - v(A)1  >  E}  :::::  8s(A, n)e-nE2

AEA 

32

/

. 

PROOF.  The  proof parallels  that  of Theorem  12.4.  We  may  again  assume  that 
nE2  ::::  2.  In the first two steps we prove that 

This may be done exactly the same way as in Theorem 12.4; we do not repeat the 
argument. The only difference appears in Step 3: 

STEP 3. CONDITIONING.  To bound the probability 

again we  condition on  Z 1,  ... ,  Zn.  Fix  Z 1,  ... ,Zn  E  R d,  and observe that as  A 
ranges  over  A,  the  number  of different  vectors  (lA(Zl),  ... , IA(zn))  is  just the 
number of different subsets of {Z 1,  ... ,  Zn}  produced by intersecting it with sets in 
A, which, by definition, cannot exceed seA, n). Therefore, with Zl, ... , Zn  fixed, 
the supremum in the above probability is a maximum of at most N A (Z 1,  ... ,  Zn) 
random variables. This number, by definition, is bounded from above by seA, n). 
By the union bound we get 

I 

Therefore, as before, it suffices to bound the conditional probability 

198 

12.  Vapnik-Chervonenkis Theory 

This may be done by Hoeffding's inequality exactly as  in Step 4 of the proof of 
Theorem 12.4. Finally, we obtain 

p  {sup Ivn(A) - v(A)1  >  E}  S  8s(A, n)e-nE2/32.  0 

AEA 

The bound of Theorem 12.5 is useful when the shatter coefficients do not increase 
too quickly with n. For example, if A contains all Borel sets of nd ,  then we can 
shatter any collection of n  different points at will,  and obtain seA, n) =  2n.  This 
would be  useless,  of course.  The  smaller A, the  smaller the  shatter coefficient 
is.  To  apply  the  vc bound,  it suffices  to  compute shatter coefficients for  certain 
families of sets. Examples may be found in Cover (1965), Vapnik and Chervonenkis 
(1971), Devroye and Wagner (1979a), Feinholz (1979), Devroye (1982a), Massart 
(1983), Dudley (1984), Simon (1991), and Stengle and Yukich (1989). This list of 
references is  far from exhaustive.  More information about shatter coefficients is 
given in Chapter 13. 

REMARK.  MEASURABILITY.  The supremum in Theorem 12.5  is  not always  mea(cid:173)
surable. Measurability must be verified for every family A. For all our examples, 
the quantities are indeed measurable. For more on the measurability question, see 
Dudley (1978; 1984), Massart (1983), and Gaenssler (1983). Gine andZinn (1984) 
and Yukich (1985) provide further work on suprema of the type shown in Theorem 
12.5.0 

REMARK.  OPTIMAL  EXPONENT.  For the  sake of readability we followed  the line 
of Pollard's  proof (1984)  instead  of the  original  by  Vapnik  and  Chervonenkis. 
In particular, the exponent  -nE 2/32 in Theorem 12.5  is  worse than the  -nE2/8 
established in the original paper.  The best known exponents together with some 
other related results are mentioned in Section 12.8. The basic ideas of the original 
proof by Vapnik and Chervonenkis appear in the proof of Theorem 12.7 below. 0 

REMARK.  NECESSARY  AND  SUFFICIENT  CONDITIONS. 
the theorem that it can be strengthened to 

It is  clear from the proof of 

p  {sup IVn(A)  - v(A)1  >  E}  S  8E {N A(Zl, ... , Zn)} e-

AEA 

nE2 /32, 

where Z 1,  ... ,  Zn are i.i.d. random variables with probability measure v. Although 
this  upper bound is  tighter than  that in the  stated inequality,  it is  usually  more 
difficult to handle, since the coefficient in front of the exponential term depends on 
the distribution of Zl, while seA, n) is purely combinatorial in nature.  However, 
this form is  important in a different setting:  we say that the uniform law of large 
numbers holds if 

sup  Ivn(A) 
AEA 

v(A)1  -+ 0 

in probability. 

12.5 Classifier Selection 

199 

It follows from this form of Theorem 12.5 that the uniform law of large numbers 
holds if 

E {log (NA(Zl, ... , Zn))} 
- - - - - - - - - -+ O. 

n 

Vapnik and Chervonenkis showed (1971; 1981) that this condition is also necessary 
for the uniform law of large numbers. Another characterization of the uniform law 
of large numbers is given by Talagrand (1987), who showed that the uniform law 
of large  numbers  holds  if and  only  if there  does  not exist  a  set  A  C  Rd  with 
v(A) >  0 such that, with probability one, the set {Zl, ... , Zn} n A is shattered by 
A.D 

12.5  Classifier Selection 

The following  theorem  relates  the  results  of the  previous  sections  to  empirical 
classifier selection, that is, when the empirical error probability Ln (cj> ) is minimized 
over a  class  of classifiers  C.  We  emphasize that unlike in Section  12.2, here we 
allow any classifier with the property that has minimal empirical error L n (cj> ) in C. 
First introduce the shatter coefficients and vc dimension of C: 

DEFINITION  12.3.  Let C be  a  class  of decision functions  of the form  cj>  :  Rd  -+ 
{O,  I}.  Define A as the collection of all sets 

{{x: cj>(x)  =  I}  x  {On U {{x: cj>(x)  = O}  x  {In,  cj>  E  C. 

Define the n-th shatter coefficient S(C, n) of the class of classifiers Cas 

Furthermore,  define the vc dimension  Vc  of C as 

S(C, n) = seA, n). 

For the performance of the empirically selected decision cj>,~,  we have the fol(cid:173)

lowing: 

Theorem 12.6.  LetC be a class of decision functions oftheformcj>  : Rd  -+ {O,  I}. 
Then  using the notation Ln(cj»  = ~ 'L7=1I{¢(x i )=/yd  and L(cj»  = P{cj>(X)  =I  Y},  we 
have 

and therefore 

P {L(cj>~) -

inf L(cj»  >  E}  ::::;  8S(C, n)e-nE2/128, 

¢EC 

where cj>~ denotes the classifier minimizing Ln (cj»  over the class C. 

200 

12.  Vapnik-Chervonenkis Theory 

PROOF.  The statements are immediate consequences of Theorem 12.5 and Lemma 
8.2.0 

The next corollary,  an easy  application  of Theorem  12.6  (see  Problem  12.1), 

makes things a little more transparent. 

COROLLARY  12.1.  In the notation of Theorem 12.6, 

E  {L(¢*)}  -

n 

inf L(¢) :s  16 
epEC 

log(8eS(C, n)) . 

2n 

If S (C,  n) increases polynomially with n, then the average error probability of the 
selected classifier is within 0 ( Jlog n / n )  of the error of the best rule in the class. 
We point out that this result is completely distribution-free. Furthermore, note the 
nonasymptotic nature of these inequalities:  they hold for every n.  From here on 
the problem is purely combinatorial-one has to estimate the shatter coefficients. 
Many properties are given in Chapter 13. In particular, if Vc  >  2, then S(C, n) :s 
n vc,  that  is,  if the  class  C has  finite  vc dimension,  then  S(C, n)  increases  at  a 
polynomial rate, and 

E {L(¢~)} -

inf L(¢) :s  16 
epEC 

Vc logn + 4 

2n 

REMARK.  Theorem 12.6 provides a bound for the behavior of the empirically opti(cid:173)
mal classifier. In practice, finding an empirically optimal classifier is often compu(cid:173)
tationally very expensive. In such cases, the designer is often forced to put up with 
algorithms yielding suboptimal classifiers. Assume for example, that we have an 
algorithm which selects  a classifier gn  such that its  empirical error is  not too far 
from the optimum with large probability: 

where {En}  and {6n }  are sequences of positive numbers converging to zero. Then 
it is easy to see (see Problem 12.6) that 

P  {L(gn) -

inf L(¢) >  E}  :s  6n  +P {2SUP ILn(¢)  - L(¢)I  >  E - En}' 

epEC 

epEC 

and Theorem 12.6 may be used to obtain bounds for the error probability of gn.  0 

Interestingly, the empirical error probability of the empirically optimal classifier 
is always close to its expected value, as may be seen from the following example: 

COROLLARY  12.2.  Let C be an arbitrary class of classification rules (that is, func(cid:173)
tions  of the form  ¢  : n d  -+  {a,  I}).  Let ¢:  E  C  be  the  rule  that minimizes the 

number of errors committed on the training sequence Dn ,  among the classifiers in 
C.  In  other words, 

12.6 Sample Complexity 

201 

where Ln(¢) =  n Li=:l  I{¢(xi)=/Yd'  Thenfor every nand E  >  0, 

--

n 

1 

The corollary follows immediately from Theorem 9.1 by observing that chang(cid:173)

ing the value of one (Xi, Yi )  pair in the training sequence results in a change of at 
most lin in the value of Ln(¢~)' The corollary is true even if the vc dimension of 
C is infinite (I). The result shows that Ln(¢~) is always very close to its expected 
value with large probability, even if E {Ln(¢~)} is far from inf¢Ec L(¢) (see also 
Problem 12.12). 

12.6  Sample Complexity 

In his theory of learning, Valiant (1984) rephrases the empirical classifier selection 
problem as follows. For E,  8 >  0, define an (E,  8) learning algorithm as a method 
that selects a classifier gn  from C using the data Dn  such that for the selected rule 

sup P  {L(gn) -
(X,Y) 

inf L(¢) >  E}  ~ 8, 
¢EC 

whenever n  ~ N(E, 8).  Here  N(E, 8)  is  the  sample  complexity of the algorithm, 
defined  as  the  smallest integer  with the  above  property.  Since the  supremum is 
taken over all possible distributions of (X, Y), the integer N(E, 8) is the number of 
data pairs that guarantees E  accuracy with 8 confidence for any distribution. Note 
that we use  the notation  gn  and not ¢I~ in the definition above,  as  the definition 
does not force us to take the empirical risk minimizer ¢I~ . 

We may use Theorem 12.6 to get an upper bound on the sample complexity of 
the selection algorithm based on empirical error minimization (i.e.,  the classifier 
¢~). 

COROLLARY  12.3.  The sample complexity of the method based on empirical error 
minimization is bounded from above by 

8) 

. 

N(E, 8)  ~ max  --2 -log --2-' -2  log -
8 

E 

E 

E 

(

512VC 

256Vc  256 

The corollary is a direct consequence of Theorem 12.6. The details are left as an 
exercise (Problem 12.5). The constants may be improved by using refined versions 
of Theorem  12.5  (see,  e.g., Theorem  12.8).  The sample size that guarantees the 
prescribed accuracy and confidence is proportional to the maximum of 

Vc 
-
E2 

log -

Vc 
E2 

and 

1 
-
E2 

1 
log -. 
8 

202 

12.  Vapnik-Chervonenkis Theory 

Here we have our first practical interpretation of the vc dimension. Doubling the 
vc dimension requires that we basically double the sample size to obtain the same 
accuracy and confidence. Doubling the accuracy however, forces us  to quadruple 
the sample size. On the other hand, the confidence level has little influence on the 
sample size,  as  it is hidden behind a logarithmic term,  thanks to the exponential 
nature of the Vapnik-Chervonenkis inequality. 

The Vapnik-Chervonenkis bound and the sample complexity N(E, 8) also allow 
us  to  compare different classes in a unified manner.  For example, if we pick ¢,~ 
by minimizing the empirical error over all hyperrectangles of n 18, will we need a 
sample size that exceeds that of the rule that minimizes the empirical error over all 
linear halfspaces of n29 ? With the sample complexity in hand, it is just a matter 
of comparing vc dimensions. 

As a function of E, the above bound grows as 0  ((1/E2) log(1/E2»). Itis possible, 
interestingly, to get rid of the "log" term, at the expense of increasing the linearity 
in the vc dimension (see Problem 12.11). 

12.7  The Zero-Error Case 

Theorem 12.5 is completely general, as it applies to any class of classifiers and all 
distributions. In some cases, however, when we have some additional information 
about the distribution, it is possible to obtain even better bounds. For example, in 
the theory of concept learning one commonly assumes that L * =  0,  and that the 
Bayes decision is  contained in C (see, e.g., Valiant (1984), Blumer, Ehrenfeucht, 
Haussler, and Warmuth (1989), Natarajan (1991». The following theorem provides 
significant  improvement.  Its  various  forms  have  been  proved  by  Devroye  and 
Wagner (1976b), Vapnik (1982), and Blumer, Ehrenfeucht, Haussler, and Warmuth 
(1989). For a sharper result, see Problem 12.9. 

Theorem 12.7.  Let C be a class of decision functions mapping n d  to  {O,  I},  and 
let ¢I~ be a function  in C that minimizes the empirical error based on the training 
sample Dn.  Suppose that inf<jJEc  L(¢) = 0,  i.e.,  the Bayes decision is contained in 
C,  and L*  = 0.  Then 

To contrast this with Theorem 12.6, observe that the exponent in the upper bound 
for the empirically optimal rule is proportional to -nE instead of -nE2. To see the 
significance of this difference, note that Theorem 12.7 implies that the error proba(cid:173)
bility of the selected classifier is within 0 (log n / n) of the optimal rule in the class 
(which equals zero in this case, see Problem 12.8), as  opposed to o( y'logn/n) 

from Theorem 12.6. We show in Chapter 14 that this is not a technical coincidence, 
but since both bounds are essentially tight, it is a mathematical witness to the fact 
that it is remarkably easier to  select a good classifier when L * = 0.  The proof is 

12.7 The Zero-Error Case 

203 

based on the random permutation argument developed in the original proof of the 
Vapnik -Chervonenkis inequality (1971). 
PROOF. For nE :s 2, the inequality is clearly true. So, we assume that nE  >  2. First 
observe that since infq'>Ec  L(¢) = O,Ln(¢~) = o with probability one. It is  easily 
seen that 

L(¢I:):S 

sup 

IL(¢) - Ln(¢ )1. 

q'>:Ln(q'»=O 

Now,  we return  to  the  notation of the previous  sections,  that is,  Zi  denotes  the 
pair (Xi, Yi ),  v  denotes  its  measure,  and  Vn  is  the  empirical  measure based on 
Zl, ... , Zn.  Also,  A consists of all sets of the form  A  = {(x, y)  :  ¢(x) =I  y} for 
¢  E  C. With these notations, 

sup 

q'>:Ln(q'»=O 

IL(¢) - Ln(¢)1 =  sup 

IVn(A) - v(A)I. 

A:vn(A)=O 

STEP 1. SYMMETRIZATION BY A GHOST SAMPLE.  The first step ofthe proof is similar 
to that of Theorem 12.5. Introduce the auxiliary sample Z~, ... , Z~ such that the 
random  variables  Z 1,  ... ,  Zn, Zi, ... , Z~ are  i.i.d.,  and let  v~ be the  empirical 
measure for  Zi, ... , Z~. Then for nE  >  2 

P {A:'~£)dllvn(Al  v(All  >  E}  :s 2P { A:'~£)dllvn(Al - v~(All >  ~ }  . 

The proof of this inequality parallels that of the corresponding one in the proof 
of Theorem 12.5. However, an important difference is that the condition nE2  >  2 
there is  more restrictive than the condition nE  >  2 here.  The details of the proof 
are left to the reader (see Problem 12.7). 

STEP 2.  SYMMETRIZATION BY  PERMUTING.  Note that the distribution of 

I 

A.vn(A)-O 

is the same as the distribution of 

1  n 

. sup _  Ivn(A) - v~(A)1 =  .  n sup  _  ;;:?= IA(Zi) -

;; L IA(Z;) 
11  sup  _I ~ t I A(I1(Zi)) - ~ t IA(I1(Z;))1 ' 

,B(I1) d;f 

. 

A·Li=IIA(Zi)-O 

1

1=1 

1  n 

1=1 

A·Li=IIA(I1(Zi»-O 

1=1 

1=1 

where  I1(Zd, ... , I1(Zn), I1(ZD, ... , I1(Z~) is  an  arbitrary  permutation  of the 
random  variables  Zl, ... , Zn, Zi, ... , Z~. The  (2n)!  possible  permutations  are 
denoted by 

Therefore, 

p  {  sup 

A:vll(A)=O 

Ivn(A) - v~(A)1 >  ~} 

2 

=  E 

{

(2n)! 

I 
(2n)!  ~ I{,B(I1 j»H 

} 

204 

12.  Vapnik-Chervonenkis Theory 

I 

=  E-L 

(2n)! 

(2n)!  j=l 

{

sup 

A:2::;~1 IA([1j(Zd)=O 

I{11 ""  I  ([1  (Z·»_l ""  I  ([1  (ZI»I>S}} . 

"L..,=1  A  J ,  

"L..,=l  A 

2 

) 

, 

STEP  3.  CONDITIONING .  Next we fix  Z 1,  ... ,  Zn, z~, ... ,  z~ and bound the value 
of the random variable  above.  Let A  c  A be a collection of sets  such that any 
two sets in A pick different subsets of {z 1,  ... ,  Zn, z~, ... , z~}, and its cardinality 
is  N A(ZI, ... , Zn,  z~, ... , z~), that is,  all possible subsets are represented exactly 
once. Then it suffices to take the supremum over A instead of A: 

(2n)! 

1 
(2n)!  ~ .  "sup 

]=1  A·2::i=IIA(I1/zJ)-O 

_  I{I~ 2::;~1 IA(TIj(zd)-~ 2::7=JA(I1j(z;»I> 0 

= 

< 

= 

(2n)! 

1 
(2n)! ~  -." sup 

J=1  AEA2::i=lIA (TI j (Zi))-O 

_  Iu 2::;'=1  IA(I1j(Z;»> 0 

1 

(2n)! 

(2n)!  ~ 2:. I{2::;'=1  IA(I1j(zi))=o}IU  2::;'=J  IA(TIj(z;»> 0 
2:. (2n)!  ~ I{2::;'=J  IA([1j(zi))=o}IU 2::;~1 IA(I1j(z;))>O' 

]=1  AEA 
1 

(2n)! 

AEA 

]=1 

STEP 4. COUNTING.  Clearly, the expression behind the first summation sign is just 
the number of permutations of the 2n points Z 1 , z~, ... ,  zn, z~ with the property 

divided by (2n)!, the total number of permutations. Observe that if l  = L:7=1 (l A (Zi)+ 
IA(zD) is  the total number of points in A  among ZI, z~, ... , Zn, z~ for a fixed set 
A, then the number of permutations such that 

is zero if l  .:s  nE /2. If l  >  nE /2, then the fraction  of the number of permutations 
with the above property and the number of all permutations can not exceed 

To  see  this,  note  that for  the  above  product of indicators  to  be  1,  all  the points 
falling in A have to be in the second half of the permuted sample. Now clearly, 

12.7 The Zero-Error Case 

205 

_n_(_n_-_1_) _ . . ...=.<'-(n--'--_l_+_l_)_.  <  2-l  <  2-nE /2. 
2n(2n - 1) ... (2n  - l + 1)  -

-

Summarizing, we have 

p {A.:'~~=oI~n(A) - v~(A)1 >  ~ } 
<  E{~~)! ~ 

n  sup 

• 

A·2:i=lIA(IT/Zi»-O 

_  I{I~ 2:;'=lIA(ITj(Zi»-~ 2:7=1  IA(ITj(Zml> 0 } 

<  E{2:2-nE

/2

} 

AEA 

=  E {NA(Zl, ... , Zn,  Z~, ... , Z~)2-nE/2} 

< 

seA, 2n )2-nE/2

, 

and the theorem is proved.  0 

Again,  we can bound the sample complexity N (E, 8)  restricted to the class of 
distributions with inf¢Ec L(¢) = O.  Just as Theorem 12.6 implies Corollary  12.3, 
Theorem 12.7 yields 

COROLLARY  12.4.  The  sample complexity N(E, 8) that guarantees 
inf L(¢) >  E}  :s  8 

P  {L(gn) -

sup 

(X,y):inf</JEc  L(¢)=O 

¢EC 

for n  2:  N(E, 8),  is bounded by 

13  4  2) 
N(E, 8) :s  max  -log2 - ,  -log2 -
8 

8Vc 
E 

E 

E 

(

. 

A quick comparison with Corollary 12.3 shows that the E2  factors in the denom(cid:173)

inators there are now replaced by E.  For the same accuracy, much smaller samples 
suffice if we know that inf¢Ec L(¢) =  O.  Interestingly, the sample complexity is 
still roughly linear in the vc dimension. 

REMARK.  We also note that under the condition of Theorem 12.7, 

P {L(¢~) >  E}  <  2E {NA(Zl, ... , Zn,  Z~, ... , Z~)} 2-nE

/ 2 

206 

12.  Vapnik-Chervonenkis Theory 

where the class A of sets is defined in the proof of Theorem 12.7. 0 

REMARK.  As  Theorem  12.7 shows,  the  difference  between the  error probability 
of an  empirically optimal classifier and that of the optimal in the class is  much 
smaller if the latter quantity is known to be zero than if no restriction is imposed 
on the distribution. To  bridge the gap between the  two  bounds, one may put the 
restriction  inicPEc L(t/»  ~ L  on the  distribution,  where  L  E  (0,  1/2) is  a  fixed 
number. Devroye and Wagner (1976b) and Vapnik (1982) obtained such bounds. 
For example, it follows from a result by Vapnik (1982) that 

As expected, the bound becomes smaller as  L  decreases. We face the same phe(cid:173)
nomenon in Chapter 14, where lower bounds are obtained for the probability above. 
o 

12.8  Extensions 

We  mentioned earlier that the  constant in the exponent in Theorem  12.5  can be 
improved at the expense of a more complicated argument. The best possible ex(cid:173)
ponent appears in the following result, whose proof is left as an exercise (Problem 
12.15): 

Theorem 12.8.  (DEVROYE  (1982A). 

P {sup IVn(A)  - v(A)1  >  E}  ~ cs(A, n2)e-2nE2, 

AEA 

where the constant c does not exceed 4e4E+

4E2  :5  4e8 ,  E  ~ 1. 

Even though the coefficient in front is larger than in Theorem 12.5, it becomes 
very  quickly  absorbed  by  the  exponential term.  We  will  see  in  Chapter  13  that 
for  VA  >  2,  seA, n)  ~ n VA,  so  seA, n2)  ~ n2VA.  This  difference  is  negligible 
compared to  the  difference  between the exponential  terms, even for moderately 
large values of nE2. 

Both Theorem 12.5 and Theorem 12.8 imply that 

E {sup Ivn(A)  - V(A)I}  = o( y'logn/n) 

AEA 

12.8 Extensions 

207 

(see Problem  12.1).  However,  it is  possible to  get rid of the logarithmic term to 
obtain 0(1/ Fn). For example, for the Kolmogorov-Srnirnov statistic we have the 
following result by Dvoretzky, Kiefer and Wolfowitz (1956), sharpened by Massart 
(1990): 

Theorem 12.9.  (DVORETZKY,  KIEFER, AND WOLFOWITZ (1956); MASSART (1990)). 
Using the notation of Theorem 12.4, we have for every nand E  >  0, 

P {sup IF(z) - Fn(z)1  >  E}  ::s  2e-2nE2

ZER 

. 

For the general case, we also have Alexander's bound: 

Theorem 12.10.  (ALEXANDER (1984)).  For nE 2  ::::  64, 

P 

{ 

sup Ivn(A) - v(A)1  >  E 
AEA 

} 

::s  16  v nE 

( 

r::  )4096VA 

2 

2 
e- nE  • 

The theorem implies the following (Problem 12.10): 

COROLLARY  12.5. 

E  sup Ivn(A) - v(A)1 

{ 

AEA 

::s 

8+J2048VAlog(4096VA) 
. 

Fn 
n 

} 

The bound in Theorem  12.10 is  theoretically interesting,  since it implies  (see 
Problem 12.10) that for fixed V A  the expected value of the supremum decreases as 
a / Fn instead of Jlog n / n. However, a quick comparison reveals that Alexander's 
bound is  larger than that of Theorem 12.8, unless  n  >  26144
,  an astronomically 
large value.  Recently, Talagrand (1994) obtained a very strong result.  He proved 
that there exists a universal constant c such that 

For more information about these inequalities, see also Vapnik (1982), Gaenssler 
(1983), Gaenssler and Stute (1979), and Massart (1983). 

It is only natural to ask whether the uniform law of large numbers 

sup Ivn(A) - v(A)1  -+ 0 
AEA 

holds if we allow A to be the class of all measurable subsets of nd. In this case 
the supremum above is called the total variation between the measures  Vn  and v. 
The convergence clearly can not hold if Vn  is the standard empirical measure 

208 

12.  Vapnik-Chevonenkis Theory 

But is  there  another  empirical  measure  such  that  the  convergence  holds?  The 
somewhat amusing answer is no.  As Devroye and Gyorfi  (1992) proved, for any 
empirical measure  vn-that is,  a function depending on  Zl, ... , Zn  assigning  a 
nonnegative number to  any measurable set-there exists a distribution of Z  such 
that for all n 

sup  Ivn(A) - v(A)1  >  1/4 
AEA 

almost surely.  Thus,  in this  generality,  the  problem is  hopeless.  For meaningful 
results,  either  A  or  v  must  be  restricted.  For  example,  if we  assume  that  v  is 
absolutely  continuous  with  density  f,  and  that  vn  is  absolutely  continuous  too 
(with density  fn), then by Scheffe's theorem (1947), 

-

11 

2  Rd 

sup  Ivn(A) - v(A)1  = -
AEA 

I fn(x) -

f(x )Idx 

(see Problem 12.13). But as  we see from Problems 6.2,  10.2,9.6, and 10.3, there 
exist density estimators (such as histogram and kernel estimates) such that the L 1-
error converges to zero almost surely for all possible densities. Therefore, the total 
variation  between  the  empirical  measures  derived  from  these  density  estimates 
and the true measure converges to zero  almost surely for all distributions  with a 
density. For other large classes of distributions that can be estimated consistently 
in total variation, we refer to Barron, Gyorfi, and van der Meulen (1992). 

Problems and Exercises 
for all t  > ° and some c  >  0, then, 

PROBLEM  12.1.  Prove that if a nonnegative random variable Z satisfies P{Z >  t}  :s  ce-2nt2 

Furthermore, 

EZ :s  JE{Z2} :s jlo~~e). 

HINT:  Use  the  identity  E{Z2}  = 1000  P{Z2  >  t}dt,  and  set 1000  = IoU  + luoo. Bound  the 

first  integral by u,  and the second by the exponential inequality.  Find the  value of u  that 
minimizes the obtained upper bound. 

PROBLEM  12.2.  Generalize the arguments of Theorems 4.5 and 4.6 to prove Theorems 12.2 
and  12.3. 
PROBLEM  12.3.  Determine the fingering dimension of classes of classifiers C = {¢ : ¢(x) = 
I{xEA);A  E  A} if the class Ais 

(1) 
(2) 
(3) 
( 4) 

the class of all closed intervals in n, 
the class of all sets obtained as the union of m closed intervals in n, 
the class of balls in n d  centered at the origin, 
the class of all balls in n d , 

Problems and Exercises 

209 

(5) 
(6) 

the class of sets of the form (-00, xd x  ... x  (-00, Xd]  in n d ,  and 
the class of all convex polygons in n2. 

PROBLEM  12.4.  Let C be a class of classifiers with fingering dimension k  >  4 (independently 
of n). Show that Vc  .:;  k log~ k. 

PROBLEM  12.5.  Prove that Theorem 12.6 implies Corollary  12.3.  HINT:  Find N(E, 0)  such 
that 8n Vc e-nE2 /128.:;  0 whenever n  >  N (E,  0).  To  see this, first show that n Vc  .:;  enE2/256  is 
satisfied for n  ::::  51:2Vc  log 25:2Vc  , which follows from the fact that 2 log x  .:;  x  if x  ::::  e2. But 
in this case 8n Vc e-nE2 /128  <  8e-nE2 /256. The upper bound does not exceed 0 if n  >  256  log £. 
8 
PROBLEM  12.6.  Let C be a  class  of classifiers  ¢  : n d  ---+  {O,  1},  and let ¢,:  be  a  classi(cid:173)
fier minimizing the empirical error probability measured on Dn.  Assume that we have an 
algorithm which selects a classifier gn  such that 

E2 

-

-

where {En}  and {On}  are sequences of positive numbers converging to zero. Show that 

P  {L(gn) -

inf L(¢) >  E}  .:;  On  +P !2SUP ILn(¢) - L(¢)I  >  E - En}. 
¢EC 

¢EC 

Find conditions on {En}  and {On}  so that 

EL(gn) -

inf L(¢) = 0  - ,  
¢EC 

(fi¥ogn) 

n 

that is, EL(gn) converges to the optimum at the same order as the error probability of ¢,: . 

PROBLEM  12.7.  Prove that 

P {  sup 

A:vn(A)=O 

Ivn(A) - v(A)1  >  E}  .:;  2P {  sup 

A:vn(A)=O 

IVn(A) - v~(A)1 >  ~} 
2 

holds if nE  >  2.  This inequality is  needed to  complete the proof of Theorem  12.7.  HINT: 

Proceed as  in the  proof of Theorem  12.5.  Introduce  A *  with  V/1 (A *)  = ° and justify the 

validity of the steps of the following chain of inequalities: 

P L::',~f)=o Ivn(A) - v~(A)1 >  E/2} 

>  E  {I{v(A*»EjP {  v~(A*) ::::  ~ I ZI, ... , Zn}} 

>  P  {B(n, E)  >  n2E}  P  {lvn(A*) - v(A*)1  >  E}, 

where  B(n, E)  is a binomial random variable with parameters nand E.  Finish the proof by 
showing that the probability on the right-hand side is greater than or equal to  1/2 if nE  >  2. 
(Under  the  slightly  more  restrictive  condition  nE  >  8,  this  follows  from  Chebyshev's 
inequality.) 

210 

12.  VapQ.ik-Chervonenkis Theory 

PROBLEM  12.8.  Prove that Theorem 12.7 implies that if inf</.>Ec  L(4)) =  0, then 

* 

EL(4)n) :s: 

2Vc 10g(2n) + 4 
. 

I  2 

n  og 

We  note  here  that Haussler,  Littlestone, and Warmuth (1988) demonstrated the existence 
of a  classifier 4>;  with  EL(4);)  :s:  2Vcln  when  inf</.>Ec  L(4))  =  O.  HINT:  Use  the  identity 
fooo P{X  >  t}dt  for  nonnegative  random  variables  X,  and  employ  the  fact  that 
EX  = 
S(C, n) :s:  nVc  (see Theorem 13.3). 
PROBLEM  12.9.  Prove  the following  version of Theorem  12.7.  Let 4>;  be  a  function  that 
minimizes the empirical error over a class in C.  Assume that inf¢Ec L(4)) =  O.  Then 

P {L(4)J~)  >  E}  :s:  2S(C, n2)e-nE 

(Shawe-Taylor, Anthony, and Biggs (1993». HINT:  Modify the proof of Theorem 12.7 by 
introducing a ghost sample Z;, ... , Z:J1  with size m (to be specified after optimization). Only 
the first symmetrization step (Problem 12.7) needs adjusting: show that for any a  E  (0,  1), 

P { 

sup  v(A)  >  E}:S:  _ I_metE P {  SUp  v:n(A)  >  (1  - a)E} , 

A:I'I/(A)-O 

1 

e 

A:vl/(A)-O 

where V~I is the empirical measure based on the ghost sample (use Bernstein's inequality(cid:173)
Theorem 8.2). The rest of the proof is similar to that of Theorem 12.7. Choose m  =  n2  - n 
and a  =  n/(n + m). 

PROBLEM  12.10.  Prove  that  Alexander's  bound  (Theorem  12.10)  implies  that  if A  is  a 
Vapnik-Chervonenkis class with vc dimension V =  VA,  then 

E  sup IVn(A) - v(A)1 

{ 

AEA 

:s: 

8 + )2048 V loge 4096 V) 
. 

r.; 
v n 

} 

HINT:  Justify the following steps: 

(1) 

If 1/1  is a negative decreasing concave function,  then 

e"'(II) 
-1/1  (u) 
HINT:  Bound l/I  by using its Taylor series expansion. 

e"'(t)dt  :s:  --,(cid:173)

f oo 

II 

(2)  Let b  >  0 be fixed.  Then for u  2:  .Jb72, 

HINT:  Use the previous step. 

(3)  Let X be a positive random variable for which 
P{X  >  u}  :s:  au h e- 2112

,  u  2:  ..jC, 

where a, b, c are positive constants. Then, if b 2:  2c  2:  e, 

EX <  -+  - - .  

JblOgb 

a 
- 2 

2 

HINT:  Use EX =  fooo  P{X >  u }du, and bound the probability either by one, or by 
the bound of the previous step. 

PROBLEM  12.11.  Use Alexander's inequality to obtain the following sample size bound for 
empirical error minimization: 

Problems and Exercises 

211 

N(E, 8)  S  max 

(

214VCIOg (214VC)  4 
. 

'  zlog -

2 
E 

E 

16) 
8 

. 

For what values of E does this bound beat Corollary 12.3? 

PROBLEM  12.12.  Let  ZI, ... , Zn  be  i.i.d.  random  variables  in  Rd,  with  measure  v,  and 
standard empirical measure vn • Let A be an arbitrary class of subsets of Rd. Show that as 
n  ~ 00, 

if and only if 

sup IVn(A)  - v(A)1  ~ 0 
AEA 

in probability 

sup Ivn(A) - v(A)1  ~ 0  with probability one. 
AEA 

HINT:  Use McDiarmid's inequality. 

PROBLEM  12.13.  Prove Scheffe's theorem (1947): let p, and v be absolute continuous prob(cid:173)
ability measures on nd  with densities I  and g, respectively. Prove that 

sup 1p,(A) - v(A)1  =  ~ f I/(x) - g(x)ldx, 

AEA 

2 

where A is the class of all Borel-measurable sets. HINT: Show that the supremum is achieved 
for the set {x  : I(x) >  g(x)}. 

PROBLEM  12.14.  LEARNING BASED ON  EMPIRICAL COVERING.  This problem demonstrates an 
alternative method of picking a classifier which works as well as empirical error minimiza(cid:173)
tion. The method, based on empirical covering of the class of classifiers, was introduced by 
Buescher and Kumar (1996a). The idea of covering the class goes back to Vapnik (1982). 
See also Benedek and Itai (1988), Kulkarni (1991), and Dudley, Kulkarni, Richardson, and 
Zeitouni (1994). LetC be a class ofclassifierscp : Rd  ~ {O,  I}. ThedatasetDn  is split into 
two  parts,  Dm  =  «XI, YI),  ... , (Xm' Ym»,  and  Tt  =  «Xm+I'  Ym+I),  ... , (Xn'  Yn»,  where 
n  =  m + I. We use the first part Dm  to cover C as follows. Define the random variable N  as 
the number of different values the binary vector bm(cp)  = (cp(X I),  ... , cp(Xm» takes as cp  is 
varied over C.  Clearly,  N  S  S(C, m). Take N  classifiers from C,  such that all  N  possible 
values  of the  binary  vector b m (cp)  are  represented exactly  once.  Denote  these  classifiers 
by 4>1,  ... , 4> N. Among these functions, pick one that minimizes the empirical error on the 
second part of the data set Tt: 

Denote the selected classifier by ;p,l'  Show  that for every n, m  and E  >  0,  the difference 
between the error probability L(;p,l) =  P{¢,;(X) "I YIDn }  and the minimal error probability 
in the class satisfies 

212 

12.  Vapnik-Chervonenkis Theory 

(Buescher and Kumar (1996a». For example, by taking m  '" ,Jii., we get 

{ ...... 

E  L(</Jn)  -

}  F¥ClOgn 

inf L(</J)  ~ c  - - ,  
~EC 

n 

where c is a universal constant. The fact that the number of samples m used for covering C 
is very small compared to n, may make the algorithm computationally more attractive than 
the method of empirical error minimization. HINT:  Use the decomposition 

P  {L(¢n) -

inf L(</J)  >  E} 

~EC 

Bound the first term on the right-hand side by using Lemma 8.2 and Hoeffding's inequality: 

To bound the second term of the decomposition, observe that 

inf  L(¢i) -

i-I, ... ,N 

inf L(</J) 
~EC 

< 

< 

sup 

IL(</J)  - L(¢')I 

~,¢>/EC:bm(~)-bmW) 

sup 

p{</J(X)i¢'(X)} 

~,~/EC:bm(~)-bm(~/) 

sup 

v(A), 

AEA:vm(A)=O 

where 

A  = {{x  : </J(x)  = I}  : ¢(x) = 1¢I(x) - ¢2(x)l, ¢I, ¢2  E C}, 

and vm(A)  =  ~ .E:I l{x;EA).  Bound the latter quantity by  applying Theorem  12.7. To do 
this, you will need to bound the shatter coefficients seA, 2m). In Chapter 13  we introduce 
simple  tools  for  this.  For example,  it  is  easy  to  deduce  from  parts  (ii),  (iii),  and  (iv)  of 
Theorem 13.5, that seA, 2m) ~ S4(C, 2m). 

PROBLEM  12.15.  Prove that for all E  E  (0,  1), 

P  {sup /vn(A) - v(A)/  >  E}  ~ cs(A, n2)e-2nE2 , 

AEA 

where c ~ 4e4€+4€2 

(1) 

(Devroye (1982a». HINT: Proceed as indicated by the following steps: 
Introduce  an  i.i.d.  ghost sample  Zi, ... , Z~ of size  m  =  n2 
- n,  where  Zi  is 
distributed as  Z 1. Denote the  corresponding empirical measure by  v~. As in the 
proof of the first step of Theorem 12.4, prove that for a, E  E  (0, 1), 

P {sup IVn(A)  - v~(A)1 >  (1  - a)E} 

AEA 

( 1 - +,) P {sup IVn(A)  - v(A)/  >  E}. 

4a  E  m 

AEA 

(2) 

Introduce  n2 ! permutations  IT r, ... ,  ITn+m  of the  n + m  random  variables  as  in 
Step 2 of the proof of Theorem 12.5. Show that 

Problems and Exercises 

213 

/1

2 ! 

1 
n2 ! ~ ~~~ I{I* L::'=l  IA(TIjCZi))-;), L:r=l  JACTIjCZ;))I>CI-a)E} 

(3)  Show that for each A  E  A, 

by using Hoeffding's inequality for sampling without replacement from n2 binary(cid:173)
valued elements (see Theorem A.25). Choose a  = I/(nE). 

13 
Combinatorial Aspects of 
Vapnik -Chervonenkis Theory 

13.1  Shatter Coefficients and VC Dimension 

In this  section we  list a few  interesting properties of shatter coefficients seA, n) 
and of the vc dimension  V A  of a class of sets  A. We  begin with a property that 
makes things easier. In Chapter 12 we noted the importance of classes of the form 

A = {A  x  {OJ  U A C  x  {l}; A  E  A} . 

(The sets A are of the form {x  : ¢(x) =  I}, and the sets in A are sets of pairs (x, y) 
for  which ¢(x)  =I  y.)  Recall that if C is  a class  of classifiers ¢  :  Rd  ---+  {O,  I}, 
then  by  definition,  S(C, n)  = seA, n)  and  Vc  = VA.  The  first  result  states  that 
S(C, n) = seA, n), so it suffices to  investigate properties of A, a class of subsets 
of Rd. 

Theorem 13.1.  For every n we have seA, n) = seA, n),  and therefore  VA  = VA. 

PROOF.  Let  N  be  a  positive  integer.  We  show  that  for  any  n  pairs  from  Rd  x 
{O,  I},  if N  sets  from  A pick  N  different  subsets  of the  n  pairs,  then there  are 
N  corresponding  sets in A that pick N  different subsets  of n  points  in R d ,  and 
vice versa.  Fix n  pairs  (Xl, 0),  ... , (xm,  0), (Xm+l,  1), ... , (xn,  1).  Note that since 
ordering does not matter, we may arrange any n pairs in this manner. Assume that 
for a certain set A  E  A, the corresponding set A = A  x  {OJ U AC  x  {I}  E  A picks 
out the  pairs  (Xl, 0),  ... , (Xk,  0), (Xm+l,  1), ... ,  (xm+Z,  1),  that is,  the  set of these 
pairs is the intersection of A and the n pairs. Again, we can assume without loss of 
generality that the pairs are ordered in this way. This means that A picks from the set 

216 

13.  Combinatorial Aspects of Vapnik-Chervonenkis Theory 

{Xl,  ... ,xn }  the subset {Xl,  ... ,Xb Xm+l+l,  ... ,X,,}, and the two subsets uniquely 
determine each other. This proves seA, n)  ::::  seA, n). To prove the other direction, 
notice that if A  picks a subset of k  points Xl,  ... , Xb  then the corresponding set 
A E  A picks the pairs with the same indices from {(Xl, 0),  ""  (Xb O)}.  Equality 
of the vc dimensions follows from the equality of the shatter coefficients.  0 

The following theorem, attributed to Vapnik and Chervonenkis (1971) and Sauer 

(1972),  describes  the  relationship between the  vc dimension  and  shatter coeffi(cid:173)
cients of a class of sets. This is the most important tool for obtaining useful upper 
bounds on the shatter coefficients in terms of the vc dimension. 

Theorem 13.2.  If A is a class of sets with vc dimension  V A,  then for every n 

seA, n)  :::: t (~). 

i=O 

1 

PROOF. Recall the definition of the shatter coefficients 

seA, n) =  max  NA(Xl,""  Xn ), 

(Xj, ... ,xn ) 

where 

NA(XI, ... ,Xn) = I{{XI, ... ,xn}nA;A E All· 

Clearly, it suffices to prove that for every Xl,  ... , X n , 

But since N A(XI, ... , xn) is just the shatter coefficient of the class of finite sets 

we  need  only  to  prove  the  theorem  for  finite  sets.  We  assume  without  loss  of 
generality that A is a class of subsets of {Xl,  ... , xn} with vc dimension VA.  Note 
that in this case seA, n) =  IAI. 

We prove the theorem by induction with respect to n and VA.  The statement is 
obviously true for n  = 1 for any class with V A  ~ 1. It is also true for any n  ~ 1 if 
VA  = 0, since seA, n) = 1 for all n in this case. Thus, we assume VA  2::  1. Assume 
that the  statement is  true for  all  k  <  n  for  all classes of subsets of {Xl,  ... , xd 
with vc dimension not exceeding VA,  and for n for all classes with vc dimension 
smaller than VA.  Define the following two classes of subsets of {Xl,  ... , xn}: 

A' = {A -

{xn };  A  E  A}, 

and 

13.1  Shatter Coefficients and VC Dimension 

217 

Note that both A' and A contain subsets of {Xl,  ... , xn-d. A contains all sets A 
that are members of A such that AU {xn}  is also in A, but Xn  tJ.  A. 

Then  IAI  =  IA'I + IAI. To see this, write 
A' =  {A - {xn}  : Xn  E  A, A  E ArU {A - {xn}  : Xn  tJ.  A, A  E  A} = 8 1 U 8 2. 
Thus, 

IA'I 

1811 + 182 1 - 181 n 8 2 1 
I{A - {xn}  : Xn  E  A, A  E  A}I  + I{A - {xn}  : Xn  tJ.  A, A  E  A}I -
I{A: Xn  E  A, A  E  A}I  + I{A  : Xn  tJ.  A, A  E  A}I -

IAI 

= 

IAI 

IAI-IAI. 

Since  IA'I  :s  IAI,  and  A' is  a  class  of subsets  of {Xl,  ... ,xn-d, the  induction 
hypothesis implies that 

IA'I  = seA', n 

~ (n -1) 
. 

1) .:s  ~  . 

i=O 

1 

Next we show that VA.:s  VA  - 1, which will imply 

by the induction hypothesis. To  see this, consider a set S  C  {Xl,  ... , xn-d that is 
shattered by A. Then S U {xn}  is  shattered by A. To  prove this  we have to  show 
that any  set  S'  C  Sand S' U {xn}  is  the intersection of S U  {xn}  and a set from 
A.  Since S is  shattered by A, if S'  C  S,  then there exists  a set A E  A such that 
S' = S n A. But since by definition Xn  tJ.  A, we must have 

S' = (S U {xnD n A 

and 

Sf U {xn}  = (S U {xnD n (AU {xn})  . 

Since by the definition of A both A and AU {Xn}  are in A, we see that S U {xn} 
is indeed shattered by A. But any set that is shattered by A must have cardinality 
not exceeding VA,  therefore lSI  .:s  VA  -
l. But S was an arbitrary set shattered by 
A, which means  VA .:s  VA  - 1.  Thus, we have shown that 
1)  VA-l  ( 

1) 
seA, n) =  IAI  =  IA'I  + IAI  :s L  n ~  + L  n  ~  . 

VA 

( 

i=O 

1 

i=O 

1 

Straightforward application of the identity (7)  = C~l) + (7=:)  shows that 

218 

13.  Comhinatorial Aspects of Vapnik-Chervonenkis Theory 

Theorem 13.2 has  some very  surprising implications.  For example, it follows 
immediately from the binomial theorem that seA, n) :::  (n + 1) VA.  This means that 
a shatter coefficient falls in one of two categories: either seA, n) = 2n  for all n, or 
seA, n) :::  (n + l)VA, which happens if the vc dimension of A  is finite.  We cannot 
have seA, n) ~ 2Jn, for example. If VA  <  00, the upper bound in Theorem 12.5 
decreases exponentially quickly with n. Other sharper bounds are given below. 

Theorem 13.3.  For all n  >  2 V, 

seA, n) ::: L  .  :::  -

VA  (n) 

(  en )  VA 

i=O 

l 

VA 

Theorem  13.3  follows  from  Theorem  13.4 below.  We  leave the details  as  an 

exercise (see Problem 13.2). 

Theorem 13.4.  For all n  2:  1 and VA  <  n /2, 

H(~) 
seA, n) :::  en  n, 

where H(x) = -x logx - 0  - x) logO - x)for x  E  (0,  1),  and H(O) = HO) = 0 
is the binary entropy function. 

Theorem 13.4 is  a consequence of Theorem 13.2, and the inequality below.  A 
different, probabilistic proof is sketched in Problem 13.3 (see also Problem 13.4). 

Lemma 13.1.  For k  <  n /2, 

PROOF.  Introduce). = kin :::  1/2. By the binomial theorem, 

(since )./0 - ).) :::  1) 

e -nH("A) t (~), 

i==O 

l 

= 

the desired inequality.  0 

13.2 Shatter Coefficients of Some Classes 

219 

REMARK.  The binary  entropy  function  H(x) plays  a  central role in information 
theory (see, e.g., Csiszar and Korner (1981), Cover and Thomas (1991». Its main 
properties  are  the  following:  H(x) is  symmetric  around  1/2,  where  it takes  its 
maximum.  It  is  continuous,  concave,  strictly monotone  increasing  in  [0,  1/2], 
decreasing in [1/2, 1], and equals zero for x  = 0, and x  =  1.  0 

Next we present some simple results  about shatter coefficients of classes  that 

are obtained by combinations of classes of sets. 

Theorem 13.5. 

(i)  If A  = Al U A 2,  then seA, n) ::s  s(AI , n) + s(A2 , n). 
(ii)  Given a class A  define Ac = {A C
(iii)  For A  = {A n A; A E  A, A E  A} seA, n) ::s  seA, n)s(A, n). 
(iv)  For A  = {AU A; A  E  A, A E  A}, seA, n) ::s  seA, n)s(A, n). 
(v)  For A  = {A x  A; A E  A, A E  A}, seA, n) ::s  seA, n)s(A, n). 

;  A  E  A}. Then s(Ac, n) = seA, n). 

PROOF.  (i),  (ii),  and  (v)  are  trivial.  To  prove  (iii),  fix  n  points  Xl,  ... ,  x n ,  and 
assume that A picks N  ::s  seA, n) subsets C I ,  ... , CN. Then A picks from C  at 
most seA, ICil) subsets. Therefore, sets of the form AnA pick at most 

N L seA, I Ci I) ::s  seA, n )s(A, n) 

i=l 

subsets. Here we used the obvious monotonicity property seA, n) ::s  seA, n + m). 
To prove (iv), observe that 

{ A U A; A E  A, A E A} = { (AC  n A c )  c  ; A E  A, A E  A} . 

The statement now follows from (ii) and (iii).  0 

13.2  Shatter Coefficients of Some Classes 

Here we calculate shatter coefficients of some simple but important examples of 
classes of subsets of nd. We begin with a simple observation. 

Theorem 13.6.  If A  contains  finitely  many  sets,  then  VA  < 
seA, n) ::s  IAI for every n. 

log2 lAI,  and 

PROOF. The first inequality follows from the fact that at least 2n  sets are necessary 
to  shatter n points. The second inequality is trivial.  0 

220 

13.  Combinatorial Aspects of Vapnik -Chervonenkis Theory 

In the next example, it is interesting to observe that the bound of Theorem 13.2 

is tight. 

Theorem 13.7. 

(i)  If A  is  the  class of all half lines:  A  ==  {( -00, x]; x  E  R},  then  VA  ==  1, 

and 

s(A, n) = n + 1 = (~) + (:), 

(ii) 

If A  is the class of all intervals in R, then  VA  ==  2,  and 

seA, n) == 

n(n + 1) 

2 

+ 1 ==  ° +  1  +  2 

(n) 

(n) 

(n) 
. 

PROOF.  (i) is easy. To  see that VA  ==  2 in (ii), observe that if we fix three different 
points in R, then there is  no  interval that does  not contain the middle point, but 
does contain the other two.  The shatter coefficient can be calculated by counting 
that  there  are  at  most  n  - k  + 1 sets  in  {A  n {Xl,  ... , x n }; A  E  A}  such  that 
IA n {Xl,  ... , xn}1  ==  k for k  ==  1,  ... , n,  and one set (namely 0) such that  IA n 
{x 1,  ... ,  Xn} I ==  0. This gives altogether 

o 

1 + L..-(n - k + 1) = 

k=l 

n(n + 1) 

2 

+ 1.  0 

Now we can generalize the result above for classes of intervals and rectangles 

in Rd: 

Theorem 13.8. 

If A = {( -00, xd x  ... x  (-00, Xd]},  then  VA  = d. 

(i) 
(ii)  If A is the  class of all rectangles in R d

,  then  VA  = 2d. 

PROOF.  We prove (ii). The first part is left as an exercise (Problem 13.5). We have 
to show that there are 2d points that can be shattered by A, but for any set of 2d + 1 
points there is a subset of it that can not be picked by sets in A. To see the first part 
just consider the following 2d points: 

(1,0,0, ... ,0), (0,  1,0,0, ... ,0), ... , (0,0, ... ,0, 1), 

(-1,0,0, ... ,0), (0,  -1,0,0, ... ,0), ... , (0,0, ... ,0, -1), 

(see Figure 13.1). 

13.2 Shatter Coefficients of Some Classes 

221 

FIGURE 13.1.  24  =  16  rectangles 
shatter 4 points in n2. 

[!] 

[!] 

0 

[!] 

GJ 

On the other hand, for any given set of 2d + 1 points we can choose a subset of at 
most 2d points with the property that it contains a point with largest first coordinate, 
a point with smallest first coordinate, a point with largest second coordinate, and 
so forth. Clearly, there is no set in A that contains these points, but does not contain 
the others (Figure 13.2).  D 

max  Y 

FIGURE  13.2.  No  5  points  can  be 
shattered by rectangles in n2. 

minX 

o 
Z 

max X 

minY 

Theorem 13.9.  (STEELE (1975), DUDLEY (1978)).  Let 9 be afinite-dimensional 
vector space of real functions on nd.  The  class of sets 
A = {{x  : g(x) ::::  O}  : g  E  g} 

has vc dimension  VA  :::  r,  where r = dimension(g). 

PROOF. It suffices to show that no set of size m  =  1 + r can be shattered by sets of 
the form  {x  : g(x) ::::  O}.  Fix m  arbitrary points  Xl,  ... , x m ,  and define the linear 
mapping L  : 9 -+ nm  as 

222 

13.  Combinatorial Aspects of Vapnik -Chervonenkis Theory 

Then  the  image  of Q,  L(Q),  is  a  linear  subspace  of nm  of dimension  not  ex(cid:173)
ceeding  the dimension  of Q,  that is,  m  - 1.  Then there exists  a nonzero  vector 
Y  = (Yl,  ... , Ym)  E n m, that is orthogonal to L(Q), that is, for every g  E Q 

We can assume that at least one of the Yi 's is negative.  Rearrange this equality so 
that terms with nonnegative Yi  stay on the left-hand side: 

L  Yig(Xi) =  L  -Yig(Xi)' 

i:Yi:::O 

i:Yi<O 

Now,  suppose that there exists  agE Q such that the  set  {x  :  g(x)  ~ O}  picks 
exactly  the  xi's  on  the  left-hand  side.  Then  all  terms  on  the  left-hand  side  are 
nonnegative, while the terms on the right-hand side must be negative, which is  a 
contradiction, so Xl,  ... , Xm  cannot be shattered, and the proof is completed.  0 

REMARK. Theorem 13.2 implies that the shatter coefficients of the class of sets in 
Theorem 13.9 are bounded as follows: 

seA, n) :s t (~). 

i=O 

1 

In many cases it is possible to get sharper estimates. Let 

be the  linear  space  of functions  spanned by  some  fixed  functions  1/fI,  ... , 1jf r 
n d  -+  n,  and  define  \lI(x)  = (o/l(X),  ... , o/r(x».  Cover  (1965)  showed  that 
if for  some  Xl,  ... ,Xn  E  nd ,  every  r-element  subset  of \lI(Xl), ... , \lI(xn)  is 
linearly  independent,  then  the  n-th  shatter  coefficient  of the  class  of sets  A  = 
{{x  : g(x) ~ O}  : g  E Q}  actually equals 

1) 
seA, n) = 2 L  n ~ 

1'-1  ( 

i=O 

1 

(see Problem 13.6). By using the last identity in the proof of Theorem 13.2, it is 
easily  seen that the difference  between the  bound obtained from  Theorem  13.9 
and the true value is (n~l). Using Cover's result for the shatter coefficients we can 
actually improve Theorem 13.9, in that the vc dimension of the class A equals r. 
To see this, note that 

13.2 Shatter Coefficients of Some Classes 

223 

while 

seA, r + 1) = 2 ~ (~) = 2 t (~) -2(r) = 2· 2r - 2  <  2r+l. 

i=O 

l i=O   l 

.  r 

Therefore no r + 1 points are shattered. It is interesting that Theorem 13.9 above 
combined with Theorem 13.2 and Theorem 13.3 gives the bound 

s(A,n) s nr + 1 

when r  >  2.  Cover's result, however, improves it to 

seA, n) S  2(n  - 1y-1 + 2.  0 

Perhaps the most important class of sets is the class of halfspaces in Rd, that is, 
sets containing points falling on one side of a hyperplane. The shatter coefficients 
of this class can be obtained from the results above: 
COROLLARY  13.1.  Let A  be the  class of half spaces,  that is,  subsets ofRd of the 
form {x  : ax  ::::  b}, where a  E  Rd, bE Rtakeallpossiblevalues. Then VA  = d+1, 
and 

PROOF.  This is an immediate consequence of the remark above if we take 9 to be 
the linear space spanned by the functions 

(h(x) = x(l),  (h(x) = X(2),  ... ,  <Pd(X)  = xed),  and <Pd+I(X)  = 1, 

where x(l), ... , xed)  denote the d  components of the vector x.  0 

It is equally simple now to obtain an upper bound on the vc dimension of the 
class of all closed balls in Rd (see Cover (1965), Devroye (1978), or Dudley (1979) 
for more information). 
COROLLARY  13.2.  Let A  be the  class of all closed balls in R d, that is,  subsets of 
Rd of the form 

where aI, ... , ad, b  E  R  take all possible values.  Then  VA  S  d + 2. 

PROOF. If we write 

d 
L 
i=l 

Ix(i) - ail 2 

d 
- b = L 
i=l 

IX(i)1 2 

d 

d 

- 2 LX(i)ai + Lal- b, 

i=1 

i=l 

224 

13.  Combinatorial Aspects of Vapnik-Chervonenkis Theory 

then it is  clear that Theorem  13.9 yields the result by  setting 9 to  be the linear 
space spanned by 

d 

(h(x) = L IxU)I Z,  (h(x) = x(l),  ... ,  ¢d+l(X) = xed),  and ¢d+Z(X)  =  1.  0 

i=l 

It follows from Theorems 13.9 and 13.5 (iii) that the class of all polytopes with 
a bounded number of faces  has  finite  vc dimension.  The next negative example 
demonstrates that this boundedness is necessary. 
Theorem 13.10.  If A is the class of all convex polygons in R Z

,  then VA  = 00. 

FIGURE  13.3.  Any subset of n  points 
on the  unit circle can be picked by a 
convex polygon. 

PROOF.  Let Xl,  ... ,Xn  E  R2 lie on the unit circle.  Then it is  easy to  see that for 
any subset of these (different) points there is a polygon that picks that subset.  0 

13.3  Linear and Generalized Linear Discrimination 

Rules 

Recall from Chapter 4 that a linear classification rule classifies X  into one of the 
two classes according to whether 

d 

ao  + Laix(i) 

i=l 

is  positive  or negative,  where  xCI),  ... ,x(d)  denote  the  components  of x  E  Rd. 
The coefficients ai  are determined by the training sequence.  These decisions di(cid:173)
chotomize the space R d by virtue of a halfspace, and assign class 1 to one halfspace, 
and class 0 to the other.  Points on the border are treated as  belonging to  class O. 
Consider  a  classifier that  adjusts  the  coefficients  by  minimizing  the  number of 

13.3 Linear and Generalized Linear Discrimination Rules 

225 

errors committed on Dn.  In  the terminology of Chapter 12, C is the collection of 
all linear classifiers. 

o 
X7 

FIGURE  13.4.  An  empirically  opti(cid:173)
mal linear classifier. 

o 
x, 

. Xs 

o 
X6  decide class 0 

o 
X8 

decide class  1 

. XII 

Glick (1976) pointed out that for the error probability  L(¢l~) of this classifier, 

L(¢l~) - inf¢Ec L(¢) -+ ° almost surely. However, from Theorems 12.6, 13.1 and 

Corollary 13.1, we can now provide more details: 

Theorem 13.11.  For all nand E  >  0, the error probability L( ¢~) of the empirically 
optimal linear classifier satisfies 

P {L(¢*) -

n 

inf L(¢) >  E}  :s  8nd+le-nE2/128. 
¢EC 

Comparing the above inequality with Theorem 4.5, note that there we selected 
¢~ by a  specific algorithm, while this result holds for any linear classifier whose 
empirical error is minimal. 

Generalized linear classification rules  (see Duda and Hart (1973»  are defined 

by 

where d*  is  a positive integer, the functions  1/II,  ... , 0/ d*  are fixed,  and the coef(cid:173)
ficients  ao,  ... ,ad*  are  functions  of the  data  Dn.  These include for  example all 
quadratic discrimination rules in n d  when we choose all functions that are either 
components of x, or squares of components of x, or products of two components 
of x.  That is,  the functions  o/i (x)  are  of the form  either x(j),  or x(j) x(k).  In  all, 
d*  = 2d + d(d - 1)/2. The argument used for linear discriminants remain valid, 
and we obtain 

Theorem 13.12.  Let C be the  class of generalized linear discriminants (i.e.,  the 
coefficients vary,  the basis functions o/i  are fixed).  For the error probability L( ¢l~) 
of the empirically optimal classifier, for all d*  >  1,n and E  >  0,  we have 

226 

13.  Combinatorial Aspects of Vapnik -Chervonenkis Theory 

Also, for n  >  2d* + 1, 

The second inequality is obtained by using the bound of Theorem 13.4 for the 
shatter coefficients. Note nevertheless that unless d*  (and therefore C) is allowed to 
increase with n, there is no hope of obtaining universal consistency. The question 
of universal consistency will be addressed in Chapters 17 and 18. 

13.4  Convex Sets and Monotone Layers 

Classes of infinite vc dimension are not hopeless by any  means.  In this  section, 
we offer examples that will show how they may be useful in pattern recognition. 
The classes of interest to us for now are 

C  = 

{ all convex sets of n 2 

} 

C  = 

{ all monotone layers of n 2

,  i.e., all sets of the form 

{(Xl, X2)  : X2  ~ 1jf(Xl)} for some nonincreasing function  1jf }. 

In discrimination, this corresponds to making decisions of the form ¢(x) =  I{xEC}, 
C  E  C,  or ¢(x)  =  I{x1-c},  C  E  C,  and  similarly for  C.  Decisions of these forms 
are important in many situations. For example, if 1J(x) is monotone decreasing in 
both components of X  E  n2
,  then the Bayes rule is of the form g*(x) = hX:EL}  for 
some L  E  C. We have pointed out elsewhere (Theorem 13.10, and Problem 13.19) 
that  Vc  = V.c  = ex).  To  see this, note that any set of n points on the  unit circle is 
shattered by C, while any set of n points on the antidiagonal X2  = -Xl is shattered 
by C.  Nevertheless, shattering becomes unlikely if X has a density.  Our starting 
point here is the bound obtained while proving Theorem 12.5: 

where NA(X 1,  ••• ,  Xn)is the number of sets in {A n {Xl, ... , Xn}  : A  E  A}. The 
following theorem is essential: 
Theorem 13.13.  If X  has a density  f  on n 2,  then 

when A is either C or £. 

This theorem, a proof of which is a must for the reader, implies the following: 

13.4 Convex Sets and Monotone Layers 

227 

COROLLARY  13.3.  Let X have a density  f  on n2. Let ¢~ be picked by minimizing 
the empirical error over all classifiers of the form 

¢(x) ={  01 

if x  E  A 
if x  tf:  A, 

where  A  or A C  is in C (or £). Then 

L(¢~) ---+ 

inf 

L(¢) 

¢=Ic for C or CC  in C 

with probability one (and similarly for £). 

PROOF.  This follows  from the inequality of Lemma 8.2,  Theorem  13.13, and the 
Borel-Cantelli lemma.  D 

REMARK.  Theorem  13 .13  and the corollary may be extended to n d, but this gen(cid:173)
eralization holds nothing new and will only result in tedious notations.  D 

PROOF  OF  THEOREM  13.13.  We  show the  theorem for  £, and indicate the proof 
for C.  Take two sequences of integers, m  and r, where m  ""  ..jii and r  ""  m 1/3, so 
that m  ---+  00, yet r21m  ---+  0, as n  ---+  00. Consider the set [-r, r]2  and partition 
each  side  into  m  equal  intervals,  thus  obtaining  an  m  x  m  grid  of square  cells 
Cl, ... , Cm 2.  Denote Co  = n2 - [-r, r]2.  Let No,  Nl, ... ,Nm 2  be the number of 
points among Xl, ... , Xn  that belong to these cells. The vector (No,  N l , ... , N m2) 
is clearly multinomially distributed. 
Let 1/1  be a nonincreasing function n ---+  n defining a set in £ by L  = {(Xl, X2)  : 
X2  :::;  1/I(Xl)}. Let C(1/I)  be the collection of all cell sets cut by 1/1,  that is, all cells 
with a nonempty intersection with both Land L c. The collection C ( 1/1)  is shaded 
in Figure 13.5. 

FIGURE  13.5.  A  monotone layer and 
bordering cells. 

228 

13.  Combinatorial Aspects of Vapnik-Chervonenkis Theory 

We bound N dX 1,  ... ,  Xn) from above, conservatively, as follows: 

(13.1) 

The number of different collections  C ( 1/1)  cannot exceed  22m  because each cell 
in C ( 1/1)  may be obtained from its predecessor cell by either moving right on the 
same row or moving down one cell in the same column. For a particular collection, 
denoting Pi  = P{X E  C}, we have 

=  (~ 2 Pi  + 2 Po  + 1 - ~ Pi  _  po) n 

Ci EC(1/!) 

Ci EC(1/!) 

(by applying Lemma A.7) 

< 

exp (n C~o/) p;  + po)) 
<  exp C en sets A wit~Uf(A) :s:  8r'Im 1 f + L f) ) 
(since A (,'  U  Ci)  S  2m(2rlm)2 = 8r21m) 

t. Ci EC(1/!) 

= 

because r2 I m  -+  0 and r  -+  00  and by  the  absolute  continuity of X.  As  this 
estimate is  uniform over all collections  C (1/1),  we  see that the expected value of 
(13.1)  is  not  more  than  22m eo(n)  =  eo(n).  The  argument for  the  collection C  of 
convex sets is analogous.  0 

REMARK.  The  theorem  implies  that  if A  is  the  class  of all  convex  sets,  then 
fL(A)1  -+ 0 with probability one whenever fL  has a density. This 
SUPAEA IfLn(A) -
is  a special case of a result of Ranga Rao (1962). Assume now that the density f 
of X is bounded and of bounded support. Then in the proof above we may take 
r  fixed so that [ -r, r]2  contains the support of I. Then the estimate of Theorem 
13.13 is 

E {N  (X 

AI, .. ·,  n 

X  )}  < 

22m. e8nllfllex,r2/m 

(where 11/1100  denotes the bound on I) 

Problems and Exercises 

229 

=  24r .Jn II flloo e4r.Jnllflloo 

(if we take m = 2rJnllflloo) 

for a constant a. This implies by Corollary 12.1  that 

E {L(¢~) -

inf 

r/J=lc  for Coree in C 

L(¢)} = 0  (n-I/4) . 

This latter inequality was proved by Steele (1975). To see that it cannot be extended 
to  arbitrary  densities,  observe that the  data points falling  on the convex hull  (or 
upper layer) of the points X I, ... ,  X n  can always be shattered by convex sets (or 
monotone layers, respectively). Thus,  NA(X I ,  ... ,  Xn)  is at least 2M", where Mn 
is the  number of points among  Xl, ... , X n ,  falling  on the convex hull (or upper 
layer) of X I, ... ,  X n . Thus, 

But it follows  from  results  of Carnal (1970)  and Devroye (1991b)  that for each 
a  <  1, there exists a density such that 

E{Mn } 

. 
hmsup - - - >  1.  0 
n-+oo 

na 

The  important  point  of this  interlude  is  that  with  infinite  vc dimension,  we 
may  under  some  circumstances  get expected error rates  that are  0(1) but larger 
than  1/,Jri.  However,  the  bounds  are  sometimes rather loose.  The reason is  the 
looseness of the Vapnik -Chervonenkis inequality when the collections A become 
very big. To get such results for classes with infinite vc dimension it is necessary 
to impose some conditions on the distribution. We will prove this in Chapter 14. 

Problems and Exercises 

PROBLEM  13.1.  Show that the inequality of Theorem 13.2 is tight, that is, exhibit a class A 
of sets such that for each n, seA, n) = 'L:1 G). 

PROBLEM  13.2.  Show that for all n  >  2VA 

and that 

230 

13.  Combinatorial Aspects of Vapnik -Chervonenkis Theory 

if VA  >  2.  HINT:  There  are  several  ways  to  prove the  statements.  One can  proceed  di(cid:173)
rectly  by  using  the  recurrence  L~ (;)  =  L~fo e~l) + L~fo-l e~} A  simpler way  to 
prove L~1 G)  ::::  ( ~:) VA  is by using Theorem 13.4. The third inequality is an immediate 
consequence of the first two. 

PROBLEM  13.3.  Give  an  alternative  proof of Lemma  13.1  by  completing  the  following 
probabilistic argument. Observe that for k'  = n  - k, 

where  Bn  is  a  binomial  random  variable  with  parameters  nand  1/2.  Then  Chernoff's 
bounding technique (see the proof of Theorem 8.1) may be used to bound this probability: 
for all s  >  0, 

P{Bn  ::::  k'}  ::::  e-sk'E {e SEll

}  = exp ( -n (Sk'in -log (e

S

;  1))) . 

Take the derivative of the exponent with respect to s to minimize the upper bound. Substitute 
the obtained value into the bound to get the desired inequality. 

PROBLEM  13.4.  Let B  be a binomial random variable with parameters nand p. Prove that 
for k >  np 

P{B::::  k}::::  exp (-n (H(kln) + ~logp+ n :k 10g(1- P))). 

HINT:  Use Chernoff's bounding technique as in the previous problem. 

PROBLEM  13.5.  Prove part (i) of Theorem 13.8. 

PROBLEM  13.6.  Prove that for  the class  of sets  defined in the  remark following  Theorem 
13.9, 

1) 
seA, n) = 2 L  n ~ 

r-1  ( 

i=O 

l 

(Cover (1965». HINT: Proceed by induction with respect to nand r. In particular, show that 
the recurrence sCAr, n) = sCAn n  - 1) + s(Ar- 1 , n - 1) holds, where Ar denotes the class 
of sets defined as  {x  :  g(x)  ::::  O}  where g  runs through a vector space spanned by the first 
r  of the sequence of functions  1h, 7f2,  .... 
PROBLEM  13.7.  Let A and B be two families of subsets of nd 

• Assume that for some r  ::::  2, 

seA, n)  ::::  nrs(B, n) 

for  all  n  ::::  1.  Show  that  if VB  >  2,  then  VA  ::::  2 (VB + r  - 1) log (VB + r  - 1).  HINT: 
By  Theorem  13.3,  seA, n)  ::::  n Vs+r-l.  Clearly,  VA  is  not  larger  than  any  k  for  which 
kVs+r-l  <  2k. 

PROBLEM  13.8.  Determine the vc dimension of the class of subsets of the real line such that 
each set in the class can be written as a union of k intervals. 

Problems and Exercises 

231 

PROBLEM  13.9.  Determine the vc dimension of the collection of all polygons with k vertices 
in the plane. 

PROBLEM  13.10.  What is the vc dimension of the collection of all ellipsoids ofnd ? 

PROBLEM  13 .11.  Determine the vc dimension of the collection of all subsets of {1,  ... , k}d , 
where k  and d  are fixed.  How does the answer change if we restrict the subsets to those of 
cardinality l  :'S  kd? 

PROBLEM  13.12.  Let A consist of all simplices ofnd ,  that is, all sets of the form 

where Xl,  ... ,Xd+l  are fixed points of nd. Determine the vc dimension of A. 

PROBLEM  13.13.  Let A(XI' ... ,Xk) be the set of all X En that are of the form 

where Xl, ... , Xk  are fixed numbers and 'tfI, ... , 'tfk  are fixed functions on the integers. Let 
A = {A(XI, ... ,xd : Xl,  ... , Xk  En}. Determine the vc dimension of A. 

PROBLEM  13.14.  In some sense, vc dimension measures the "size" of a class of sets. How(cid:173)
ever it has  little to  do  with cardinalities, as  this  exercise demonstrates.  Exhibit a class  of 
subsets of the integers with uncountably many sets, yet vc dimension 1. (This property was 
pointed out to  us by Andras Farago.) Note:  On the other hand, the class  of all  subsets  of 
integers cannot be written as  a countable union of classes with finite vc dimension (Theo(cid:173)
rem  18.6). HINT:  Find a class of subsets of the reals with the desired properties, and make 
a proper correspondence between sets of integers and sets in the class. 

PROBLEM  13.15.  Show that if a class of sets A is linearly ordered by inclusion, that is, for 
any pair of sets A, B  E  A either A  c  B  or B  c  A  and IAI  2::  2, then  VA  = 1.  Conversely, 
assume that VA  = 1 and for every set B  with I B I = 2, 

0,  B  E  {A n B  : A  E  A}. 

Prove that then A is linearly ordered by inclusion (Dudley (1984)). 

PROBLEM  13.16.  We  say that four sets  A, B, C, D  form a diamond if A  c  B  C  C,  A  C 
DeC, but B  ct  D  and D  ct  C. Let A be a class of sets. Show that VA  2::  2 if and only if 
for some set R, the class {A n R  : A  E  A} includes a diamond (Dudley (1984)). 

PROBLEM  13 .17.  Let A be a class of sets, and define its density by 

DA = inf {r  >  0:  sup seA, n)  <  oo} . 

n:::::l 

nr 

Verify the following properties: 

(1)  DA:'S  VA; 
(2)  For each positive integer k,  there exists  a class  A of sets  such that  VA  = k,  yet 

DA = 0; and 

232 

13.  Combinatorial Aspects of Vapnik-Chervonenkis Theory 

(3)  DA  <  00 if and only if VA  <  00 

(Assouad (1983a)). 
PROBLEM  13.18.  CONTINUED. Let A and AI be classes of sets, and define B = A u A'. Show 
that 

(1)  DB = max (DA' DA'); 
(2)  VB::S  VA + VA'  + 1;  and 
(3)  For every  pair of positive  integers  k, m  there  exist classes  A  and  AI  such  that 

VA  =k, VA'  =m, and VB  =k+m+ 1 

(Assouad (l983a)). 

PROBLEM  13.19.  A set A  C  n d  is called a monotone layer if x  E  A implies that yEA for 
all y  ::s  x  (i.e., each component of x  is not larger than the corresponding component of y). 
Show that the class of all monotone layers has infinite vc dimension. 
PROBLEM  13.20.  Let(X, Y)beapairofrandomvariablesinn2 x{0,  l}suchthatY = I{xEA}, 
where A  is a convex set.  Let Dn  = «Xl, Yd, ... , (Xn'  Yn))  be an i.i.d.  training sequence, 
and consider the classifier 

if x  is in the convex hull of the Xi'S with Y = 1 
otherwise. 

Find a distribution for which gil is not consistent, and find conditions for consistency. HINT: 
Recall Theorem 13.10 and its proof. 

14 
Lower Bounds for Empirical 
Classifier Selection 

In Chapter  12  a classifier was  selected by minimizing the empirical error over a 
class of classifiers C.  With the help of the Vapnik -Chervonenkis theory  we have 
been able to obtain distribution-free performance guarantees for the selected rule. 
For example, it was  shown that the difference between the expected error proba(cid:173)
bility of the selected rule and the best error probability in the class behaves at least 
as  well  as  o (.jVc logn/n), where  Vc  is  the Vapnik-Chervonenkis dimension of 
C,  and n  is  the size of the training data  Dn.  (This upper bound is  obtained from 
Theorem 12.5. Corollary 12.5 may be used to replace the log n term with log Vc.) 
Two questions arise immediately: Are these upper bounds (at least up to the order 
of magnitude) tight? Is there a much better way of selecting a classifier than mini(cid:173)
mizing the empirical error? This chapter attempts to answer these questions. As it 
turns out, the answer is essentially affirmative for the first question, and negative 
for the second. 

These questions were also asked in the learning theory setup, where it is usually 
assumed  that the  error probability of the  best classifier in the  class  is  zero  (see 
Blumer,  Ehrenfeucht,  Haussler,  and Warmuth  (1989),  Haussler,  Littlestone,  and 
Warmuth (1988), and Ehrenfeucht, Haussler, Kearns, and Valiant (1989)). In this 
case,  as  the  bound  of Theorem  12.7  implies,  the  error  of the  rule  selected  by 
minimizing the empirical error is  within  0 (Vc log n / n) of that of the best in the 
class (which equals zero, by assumption). We will see that essentially there is no 
way to beat this upper bound either. 

234 

14.  Lo~er Bounds for Empirical Classifier Selection 

14.1  Minimax Lower Bounds 

Let us  formulate  exactly what we are interested in.  Let C be  a class  of decision 
functions ¢  : nd ---+  {O,  I}.  The training sequence  Dn  =  ((Xl, YI ),  ... ,  (Xn,  Yn)) 
is  used to  select the classifier gn(X)  =  gn(X, Dn)  from C,  where the selection is 
based on the data Dn.  We emphasize here that gn  can be an arbitrary function of 
the data,  we  do  not restrict our attention to empirical error minimization, where 
gn  is a classifier in C that minimizes the number errors committed on the data Dn. 
As  before,  we  measure  the  performance  of the  selected classifier by  the  dif(cid:173)
ference between the error probability L(gn)  =  P{gn(X)  i  YIDn} of the selected 
classifier and that of the best in the class.  To  save  space  further  on,  denote this 
optimum by 

Ij>EC 
In particular, we seek lower bounds for 

Lc d;f inf P{¢(X) i  Y}. 

and 

supP{L(gn) - Lc  >  E}, 

supEL(gn) - Lc, 

where the supremum is  taken over all possible distributions of the pair (X, Y). A 
lower bound for one of these quantities means that no matter what our method of 
picking a rule from C is, we may face a distribution such that our method performs 
worse  than the bound.  This  view may be criticized as  too  pessimistic.  However, 
it is  clearly  a perfectly  meaningful  question  to  pursue,  as  typically  we  have  no 
other information available than the training data,  so we have to be prepared for 
the worst situation. 

Actually, we investigate a stronger problem, in that the supremum is taken over 
all distributions with Lc kept at a fixed value between zero and  1/2. We will see 
that  the bounds  depend  on n,  Vc,  and  Lc jointly.  As  it turns  out,  the  situations 
for  Lc  >  0  and  Lc  =  0  are  quite  different.  Because  of its  relative  simplicity, 
we  first  treat  the  case  Lc  =  O.  All  the  proofs  are  based  on  a  technique  called 
"the probabilistic  method."  The basic  idea here  is  that the  existence of a  "bad" 
distribution is proved by considering a large class of distributions, and bounding 
the average behavior over the class. 

Lower bounds on the probabilities P{L(gn) - Lc  >  E}  may be translated into 
lower bounds on the sample complexity N(E, 8).  We obtain lower bounds for the 
size ofthe training sequence such that for any classifier, P{L(gn) - Lc  >  E} cannot 
be smaller than 8 for all distributions if n is  smaller than this bound. 

14.2  The Case Lc = 0 

In this section we obtain lower bounds under the assumption that the best classifier 
in the  class has  zero error probability.  In view  of Theorem  12.7 we  see that the 

14.2 The Case Lc = 0 

235 

situation here is  different from when  Lc  >  0;  there exist methods of picking  a 
classifier  from  C (e.g.,  minimization  of the  empirical  error)  such that  the  error 
probability decreases to zero at a rate of 0 (Vc log n In). We obtain minimax lower 
bounds close to the upper bounds obtained for empirical error minimization. For 
example, Theorem 14.1  shows that if Lc = 0, then the expected error probability 
cannot decrease faster than a sequence proportional to Vc I n for some distributions. 

Theorem 14.1.  (VAPNIK  AND  CHERVONENKIS  (1974c);  HAUSSLER,  LITTLESTONE, 
AND  WARMUTH  (1988». Let C be a  class of discrimination functions  with vc di(cid:173)
mension V. Let X  be the set of all random variables (X, Y) for which Lc = 0.  Then, 
for every discrimination rule gn  based upon Xl, YI ,  ... ,  X n, Yn, and n  ::::  V  - 1, 

sup  ELn:::  V  - 1 (1 _~) . 

(X,y)EX 

2en 

n 

PROOF.  The idea is to construct a family F  of 2 V-I  distributions within the distri(cid:173)

butions with Lc = ° as follows: first find points x I,  ... ,  Xv  that are shattered by C. 

Each distribution in F  is concentrated on the set of these points. A  member in F 
is described by V  - 1 bits, bI ,  ... , bv -1. For convenience, this is represented as a 
bit vector b. Assume V - 1 ~ n. For a particular bit vector, we let X  = Xi  (i  <  V) 
with probability  lin each, while X  = Xv  with probability  1 -
l)/n. Then 
set Y  =  fb(X), where fb  is defined as follows: 

(V  -

-r 

J h(X) =  °  if X = Xv. 

if X = Xi,  i  <  V 

{bi 

Note that since Y  is a function of X, we must have L * =  0.  Also,  Lc = 0,  as  the 
set {Xl,  ... , xv} is  shattered by C,  i.e.,  there is  agE C with g(Xi)  = fb(Xi)  for 
1 ~ i  ~ V. Clearly, 

sup  E{L n  - Lc} 

(X,Y):Lc=O 
> 

sup  E{L n  - Lc} 

(X,Y)EF 
supE{Ln  - Lc} 
b 

>  E{L n  - Lc} 

(where b is replaced by B, uniformly distributed over {a,  1} V-I) 

E{L n }  , 
P{gn(X, Xl, YI ,  ... ,  X n, Yn) =I  fB(X)}  . 

The last probability may be viewed as the error probability of the decision function 
gn  : n d  x  (nd  x  {a,  l})n  ---+  {a,  1}  in predicting the value of the random variable 
fB(X)  based  on  the  observation  Zn  = (X, Xl, YI ,  ... ,  X n, Yn).  Naturally,  this 
probability is bounded from below by the Bayes probability of error 

L *(Zn,  fB(X»  = inf P{gn(Zn) =I  fB(X)} 

gil 

236 

14.  Lm.yer Bounds for Empirical Classifier Selection 

corresponding to the decision problem (Zn,  fB(X», By the results of Chapter 2, 

where 17*(Zn)  = P{fB(X) =  lIZn}. Observe that 

* Z  ) _1112 
17  (  n 

-

0  or  1  otherwise. 

if X =I Xl, ... , X =I Xn ,  X =I Xv 

Thus, we see that 

sup  E{L n  - Lc}  >  L *(Z11 , fB(X» 

(X,y):Lc=O 

1 2: P {X  =I Xl .... , X =I X n ,  X =I Xv} 
1 V-I 
-I: PIX =  xd(1  - PIX =  Xd)11 

2  i=l 
V-I 
--(1-1Int 

2n 

>  V-I (1 __ 1) 

2en 

n 

(since (1  - 1In)n-l  -l-

lie). 

This concludes the proof.  0 

Minimax lower bounds on the probability P{Ln  ::::  E} can also be obtained. These 
bounds have  evolved through  several papers:  see Ehrenfeucht,  Haussler,  Kearns 
and Valiant (1989); and Blumer, Ehrenfeucht, Haussler and Warmuth (1989). The 
tightest bounds we are aware of thus far are given by the next theorem. 

Theorem 14.2.  (DEVROYE AND LUGOSI (1995».  Let C be a class of discrimination 
functions  with  vc dimension  V  ::::  2.  Let X  be  the  set of all random  variables 
(X, Y)for which  Lc  =  O.  Assume E  ::::  1/4. Assume n  ::::  V-I. Then for every 
discrimination rule gn  based upon Xl, YI ,  ••• ,  X n' Yn , 

sup  P{Ln::::  E}  ::::  - - - -
e-JrrV  V-I 

(X,y)EX 

1 

(  2neE  )(V-I)!2 

e-4nE !(l-4E)  • 

If on the other hand n ::::  15  and n  ::::  (V - 1)/(12E), then 

sup  P{Ln::::  E}  ::::  -

. 

(X,Y)EX 

1 
10 

PROOF. We randomize as in the proof of Theorem 14.1. The difference now is that 
we pick XI ,  ""XV-l with probability peach. Thus,P{X =xv} = 1- p(V -1). 

We inherit the notation from the proof of Theorem 14.1. For a fixed b, denote the 
error probability by 

14.2 The Case Lc =  0 

237 

We now randomize and replace b by B. Clearly, 

sup  P{Ln:::: E} 

:::: 

(X,Y):Lc=O 

sup P{Ln(b) ::::  E} 
b 

::::  E {P{Ln(B)  ::::  EIB)} 

P{Ln(B) ::::  E}. 

As  in the proof of Theorem 14.1, observe that Ln(B) cannot be smaller than the 
Bayes risk corresponding to the decision problem (Zn, iBeX)), where 

Thus, 

Ln(B):::: E {min(1]*(Zn),  1 -1]*(Zn))IX I ,  ... ,  Xn}. 

As in the proof of Theorem 14.1, we see that 

E {min(1]*(Zn),  1 - 1]*(Zn))IX I, ... ,  Xn} 

1 2P {X f  X I. ···, X f  Xn, X f  xvIX I ,···, Xn} 
1  V-I 

:::::  2 p L I{x;=jx1, ... ,xifXIl }, 

i=I 

For fixed Xl, ... , Xn, we denote by 1 the collection {j : 1 S  j  S  V-I, n7=1 {Xi =I 
X j  }}. This is the collection of empty cells Xi. We summarize: 

We consider two choices for p: 

CHOICE A.  Take  p  =  lin, and assume  12nE  S  V-I, E  <  1/2. Note that for 
n  :::::  15, Eill = (V  - 1)(1  - p)n  ::::  (V - 1)/3. Also,  since 0  Sill s  V-I, 
we have Var III S  (V - 1)2/4. By the Chebyshev-Cantelli inequality (Theorem 
A.l7), 

P{lll :::::  2nE} 

1 - P{lll <  2nE} 

:::: 

1 - P{lll <  (V - 1)/6} 
1 - P{lll - Eill s -(V - 1)/6} 

238 

14.  Low~r Bounds for Empirical Classifier Selection 

> 

1 ____ V:_a_r_l_l_1 __  
Var III + (V - 1)2/36 

(V - 1)2/4 

(V  - 1)2/4 + (V - 1)2/36 

1 _ 

1 
10 

This proves the second inequality for supP{LIl  2:  E}. 
CHOICE B.  Assume that E :5  1/4. By the pigeonhole principle,  III  ~ 2E / P  if 
the number of points  Xi,  1  :5  i  :5  n,  that are  not equal  to Xv  does not exceed 
V-I - 2E / p. Therefore, we have a further lower bound: 

P{lll  ~ 2E/p}  ~ P{Binomial(n, (V -1)p) :5  V-I - 2E/p}. 

Define  v  =  r(V  - 1)/21. Take  p  =  2E/V. By assumption, n  ~ 2v - 1. Then the 
lower bound is 

P{Binomial(n, 4E)  :5  v} 

> 

(:)<4<)"(1 _ 4<YH 

>  _1_ (4eE(n - v + 1»)V ('I  _  4E)1l 

v(1  - 4E) 

e-J2n v 
(since (:)  2:  (n-~+l)e) v ek by Stirling's formula) 

1 

(4eE(n - v + 1»)V 

11 
(1-4E) 

>  - -
e-J2nv 

v 

>  _1_ (4enE)V (1- 4E)1l (1- v -

e-J2nv 

v 

l)V 

n 

>  __ 1_ (2enE) v e-4m=/(l-4€) 

e-J2nv 
(use  1 - X  2:  exp(-x/(1- x»). 

v 

(since n  2:  2(v _  1» 

This concludes the proof.  0 

14.3  Classes with Infinite VC Dimension 

The results presented in the previous section may also be applied to classes with 
infinite vc dimension.  For example,  it is  not hard to  derive  the  following  result 
from Theorem 14.1: 

Theorem 14.3.  Assume that Vc  = 00.  For every n,  8  >  0 and classification rule 
gn,  there is a distribution with Lc = 0 such that 

14.4 The Case Lc  >  0 

239 

EL(gn) >  -

1 
2e 

J. 

For the proof, see Problem 14.2. Thus, when Vc  = 00, distribution-free nontrivial 
performance guarantees for L(g n) - Lc do not exist. This generalizes Theorem 7.1, 
where a similar result is  shown if C is the class of all measurable discrimination 
functions. We have also seen in Theorem 7.2, that if C is the class of all measurable 
classifiers, then no  universal rate of convergence exists.  However,  we  will see in 
Chapter 18 that for some classes with infinite vc dimension, it is possible to find 
a classification rule such that L(gn) - L * :::  cJlog n/n for any distribution such 
that the Bayes classifier is in C.  The constant c, however, necessarily depends on 
the distribution, as is apparent from Theorem 14.3. 

Infinite  vc dimension  means  that  the  class  C shatters  finite  sets  of any  size. 
On the  other hand,  if C shatters  infinitely many points,  then no  universal rate of 
convergence exists. This may be seen by an appropriate modification of Theorem 
7.2, as  follows.  See Problem 14.3 for the proof. 

Theorem 14.4.  Let {an}  be  a  sequence  of positive  numbers  converging  to  zero 
with  1/16  ~ al  ~ a2  ~ .... Let C be a class of classifiers with the property that 
there  exists a set A  c  nd of infinite cardinality such that for any subset B  of A, 
there  exists ¢  E  C such that ¢(x) = 1 if x  E  Band ¢(x) = 0 if x  E  A-B. Then 
for every sequence of classification rules,  there exists a distribution of (X, Y) with 
Le = 0,  such that 

for all n. 

Note that the basic difference between this result and all others in this chapter 
is that in Theorem 14.4 the "bad" distribution does not vary with n. This theorem 
shows  that selecting a classifier from  a class  shattering infinitely many points  is 
essentially as hard as  selecting one from the class of all classifiers. 

14.4  The Case Lc  >  0 

In the more general case, when the best decision in the class C has positive error 
probability, the upper bounds derived in Chapter 12 for the expected error proba(cid:173)
bility of the classifier obtained by minimizing the empirical risk are much larger 
than when Le = O.  In this section we show that these upper bounds are necessarily 
large,  and they may be  tight for some distributions.  Moreover,  there is  no  other 
classifier that performs significantly better than empirical error minimization. 

Theorem 14.5 below gives  a lower bound for sUPeX,Y):Lc  fixed EL(gn) - Le. As 
a function of nand Vc,  the bound decreases  basically as  in the upper bound ob(cid:173)
tained from Theorem 12.6. Interestingly, the lower bound becomes smaller as  Le 

240 

14.  Lower Bounds for Empirical Classifier Selection 

decreases,  as  should be expected.  The bound is  largest when  Lc is  close to  1/2. 
The constants in the bound may be tightened at the expense of more complicated 
expressions. The theorem is essentially due to Devroye and Lugosi (1995), though 
the proof given here is different. Similar bounds-without making the dependence 
on Lc explicit-have been proved by Vapnik and Chervonenkis (197 4c ) and Simon 
(1993). 

Theorem 14.5.  Let C be a  class of discrimination functions  with  vc dimension 
::::  2.  Let  X  be  the  set  of all  random  variables  (X, Y) for  which for fixed 
V 
L  E  (0,  1/2), 

L  = inf P{g(X) i  Y}  . 

gEe 

Then, for every discrimination rule gn  based upon  Xl, Y1,  .•. ,  X n, Yn, 

sup  E(L n  - L) :::: 

(X,Y)EA' 

/L(V -1) 

24n 

e- 8 

ifn ::::  i~l max(9,  1/(1 - 2L)2). 

(V  -

PROOF.  Again  we  consider  the  finite  family  F  from  the  previous  section.  The 
notation band B  is  also  as  above.  X  now  puts  mass  p  at Xi,  i  <  V,  and mass 
l)p  :s  1,  which will be 
1 -
satisfied.  Next introduce  the  constant  c  E  (0,  1/2).  We  no  longer  have  Y  as  a 
function of X. Instead, we have a uniform [0,  1]  random variable U  independent 
of X  and define 

l)p  at Xv.  This  imposes the condition (V  -

Y  = {I  if U  :s  4 - c + 2cbi , X =  Xi,  i  <  V 

° otherwise. 

Thus,  when X  = Xi,  i  <  V,  Y  is  1 with probability 1/2 - cor 1/2 + c.  A  simple 
argument shows that the best rule for b is the one which sets 

f, (x) = {I  if x  = ~i  ,  i  <  V, bi  = 1 

° otherwIse. 

b 

Also, observe that 

Noting that 127J(xJ  -
2.2, we may write 

L  = (V  -

l)p(1j2 - c) . 

11  =  c for  i  <  V,  for fixed  b,  by the equality in Theorem 

V-I 

Ln  - L  ::::  L 2pc!(gll(Xi,X 1,Y1, ••• ,XIl ,l';,)=1- fb(xdl  . 

i=l 

It is  sometimes convenient to make the dependence of gn  upon b explicit by con(cid:173)
sidering  gn(Xi)  as  a  function  of Xi,  Xl, ... , X n,  U 1,  ... ,  Un  (an  i.i.d.  sequence 
of uniform  [0,  1]  random  variables),  and  bi •  The  proof given  here  is  based  on 

14.4 The Case Lc  >  0 

241 

the  ideas  used in the proofs of Theorems  14.1  and  14.2.  We  replace b by  a uni(cid:173)
formly  distributed  random  B  over  {O,  I} V-I.  After  this  randomization,  denote 
Zn  = (X, Xl, YI ,· .. ,  X n, Yn).  Thus, 

sup  E{Ln - L} 

(X,Y)EF 

supE{Ln  - L} 
b 

V-I 

i=I 

(with random B) 

>  E{Ln - L} 
>  L 2pcEI{gn(x;,X J , ••• ,Yn)=I- IBeX;)} 
=  2cP{gn(Zn) =I IBeX)} 
>  2cL *(Zn,  IBeX)), 

where,  as  before,  L *(Zn, IBeX))  denotes  the  Bayes  probability of error of pre(cid:173)
dicting the value of IBeX) based on observing  Zn'  All we have to  do is  to find  a 
suitable lower bound for 

where 1]*(Zn) = P{iB(X) =  lIZn}. Observe that 

Next  we  compute  P{Bi  =  llYiJ  = YI,  ... , Yik  = Yk}  for  YI,  ... , Yk  E  {O,  I}. 
Denoting the numbers of zeros and ones by ko  = I {j s k  : Y j  = O} I and k 1  = I {j s 
k : Yj  = 1}1,  we see that 

P{Bi  = llYiJ  = YI,  ... , Yik  = yd 

= 

(1  - 2c )kJ (1  + 2c )ko 

(1  - 2c )kl (1  + 2c )ko  + (1 + 2c )kl (1  - 2c )ko  . 

Therefore, if X = XiJ  = ... = X ik  = Xi,  i  <  V, then 

min(17*(Zn),  1 - 1]*(Z/I)) 

min ((1  - 2c )kJ (1  + 2c )ko , (1  + 2c )kJ (1  - 2c )ko ) 

(1  - 2c )kJ (1  + 2c )ko  + (1 + 2c )kJ (1  - 2c )ko 

min ( 1, (f*) kJ -ko ) 

1 + ( I+2c )k1-kO 

I-2c 

1 + (I+2c)lkJ-kOI ' 

I-2c 

242 

14.  Lower Bounds for Empirical Classifier Selection 

In summary, denoting a  = (1  + 2c)/(l - 2c), we have 

L *(Zn'  IBeX)) 

E  { 

1 

1 + a/Lj:Xj=x(2Yj-1)/ 

} 

>  E  { 2alL:jXj~x(2Yj-I)1 } 

> 

> 

1 I: P{X = xdE {a -ILj:Xj=Xi(2Yj-l)I} 

2  i=l 
~(V -
2 
(by Jensen's inequality). 

l)pa -E{]Lj:xrXi(2Yj -l)1} 

Next we bound E {I Lj:Xj=Xi (2Yj  - 1)\}. Clearly,  if B(k, q) denotes  a binomial 
random variable with parameters k and q, 
E {Ij~; (2Yj -lJl} = ~ G)pk(l- p)n-kE {I2B(k, 1/2 - c) - klj· 

However, by straightforward calculation we see that 

E{12B(k, 1/2 - c) - kl}  <  JE {(2B(k, 1/2 - c) - k)2} 

Jk(l - 4c2 ) + 4k2c2 

<  2kc +,Jk. 

Therefore, applying Jensen's inequality once again, we get 

t (n)pk(1 - pt-kE {12B(k,  1/2 - c) - kl} :::  2npc +-JrlP. 

k=O  k 

Summarizing what we have obtained so far,  we have 

supE{Ln - L}  > 
b 

2cL *(Zn,  IBeX)) 

>  2c-(V -

l)pa-2npc -y?£P 

> 

l)pe-2npc(a-l)-(a-1)y?£P 

1 
2 
c(V -

(by the inequality 1 + x  :::  eX) 
c(V -

l)pe-8npc2/(l-2c)-4cy?£P/(l-2c). 

A rough asymptotic analysis shows that the best asymptotic choice for c is given 
by 

1 

C= - -

,J4np' 

14.4 The Case Lc  >  0 

243 

Then the constraint L  = (V - 1) p( 1 /2 - c) leaves us with a quadratic equation in c. 
Instead of solving this equation, it is more convenient to take c = J (V - 1)/ (8nL). 
If2nL/(V-1)::::: 9,thenc::::  1/6. With this choiceforc, using L  = (V-1)p(1/2-
c), straightforward calculation provides 

sup  E(L n  - L) ::::: 

(X,y)EF 

J(V - l)L 

24n 

e-8 • 

The condition p( V-I) ::::  1 implies that we need to ask that n  :::::  (V - 1)/ (2L( 1 
2L)2). This concludes the proof of Theorem 14.5.  D 

Next we obtain a probabilistic bound. Its proof below is based upon Hellinger 

distances, and its methodology is essentially due to Assouad (1983b). For refine(cid:173)
ments and applications, we refer to Birge (1983;  1986) and Devroye (1987). 

Theorem 14.6.  (DEVROYE AND LUGOSI (1995)).  Let C be a class of discrimination 
functions with vc dimension V  :::::  2. Let X  be the set of all random variables (X, Y) 
for which for fixed L  E  (0,  1/4], 

L  = inf P{g(X) =I  Y}  . 

gEe 

Then,  for every discrimination  rule  gn  based upon  Xl, Y1,  ••• ,  X n,  Yn,  and any 
E  ~ L, 

sup  P{Ln 

(X,Y)EX 

PROOF. The method of randomization here is similar to that in the proof of Theorem 
14.5. Using the same notation as there, it is clear that 

sup  P{Ln 

L:::::  E} 

(X,y)EX 

>  EI "V-l 

{L.,i=l  2pcI{gn(xi,X I , .. "Yn)=l- fB(Xi)}:::E} 

=  2-(V-l) 

(xi , .. "xi"Yl " .. ,yn) 
E({Xl"",XV}x{O,I})" 

First observe that if 

then 

E/(2pc)  ~ (V - 1)/2, 

(14.1) 

244 

14.  Lower Bounds for Empirical Classifier Selection 

where be denotes the binary vector (1 - h, ... , 1 - bv -1), that is, the complement 
of b.  Therefore, for E  ::s  pc(V - 1), the last expression in the lower bound above 
is bounded from below by 

(by LeCam's inequality, Lemma 3.1) 
2n 

2;+1  L  L J Pb(X, Y)Pbc(X,  y) 

)

( 
(x,y) 

b 

It is easy to  see that for x  = xv, 

Pb(X,Y)=Pbc(X,y)= 

1 - (V - 1)p 

2 

' 

and for x  = Xi,  i  <  V, 

Pb(X, Y)Pbc(X,  y) = P  4 - c 

2 (1 

2) 

Thus, we have the equality 

Summarizing, since L  = p(V - 1)(1/2 - c), we have 

sup  P{Ln  - L  ::::  E}  > 

(X,y)EX 

> 

> 

~ (1 _ ~4c2)2n 
4 
"2- c 
~ exp (_ 16nLc
1 - 2c 
4 

2 

/  (1 _ 8Lc

2 

1 - 2c 

).)  , 

where we used the inequality 1 - x  ::::  e-x/(l-x) again. We may choose c as  2L:2E' 
It is  easy  to  verify  that condition  (14.1)  holds.  Also,  p(V - 1)  ::s  1.  From the 

condition L  2:  E  we deduce that c  :'S  1/4. The exponent in the expression above 
may be bounded as 

14.5 Sample Complexity 

245 

1 - 2c - 8Lc2 

(by substituting c = E / (2L + 2E» 

16nLc2 
1"=2C 
1 _  8Lc2 
1-2c 

= 

= 

< 

Thus, 

(X,Y)EX 

as desired.  0 

sup  P{Ln  - L  2:  E}  2:  - exp (-4nE2 / L)  , 

1 
4 

14.5  Sample Complexity 

We may rephrase the probability bounds above in terms of the sample complexity 
of algorithms for selecting a classifier from a class. Recall that for given E, 8 >  0, 
the sample complexity of a selection rule  gn  is the smallest integer N(E, 8)  such 
that 

sup P{L(gn) - Lc  2:  E}  :'S  8 

for all n  2:  N(E, 8). The supremum is taken over a class of distributions of (X, Y). 
Here we are interested in distributions such that Lc is fixed. 

We start with the case Lc = 0, by checking the implications of Theorem 14.2 for 
the sample complexity N (E, 8). First Blumer, Ehrenfeucht, Haussler, and Warmuth 
(1989) showed that for any algorithm, 

N(E,o),,=CGIOgG)+Vc) 

, 

where  C  is  a  universal  constant.  In Ehrenfeucht,  Haussler,  Kearns,  and  Valiant 
(1989), the lower bound was partially improved to 

Vc  -1 

N(E,8) 2: ~ 

:'S  1/8 and  8  :'S  1/100.  It may  be  combined  with  the  previous  bound. 

when  E 
Theorem 14.2 provides the following bounds: 

COROLLARY  14.1.  Let C be a class of discrimination functions with vc dimension 
V  2:  2.  Let X  be the set of all random variables (X, Y)for which Lc = 0. Assume 

246 

14.  Lower Bounds for Empirical Classifier Selection 

E  :'S  1/4, and denote v =  rev -l)/2l Thenfor every discrimination rule gn  based 
upon Xl, f l ,  ... ,  X n ,  fn, when E  :::  1/8 and 

then 

1
N(., 8) ~ 8

•  log G) 

Finally,jor 8 :'S  1/10, and E  <  1/2, 

V-I 
N(E, 8)  :::  --u;- . 

PROOF. The second bound follows trivially from the second inequality of Theorem 
14.2. By the first inequality there, 

sup  P{Ln::: E}  > 

1 

(  2neE  )(V-I)/2  e-4nE/(l-4E) 

(X,y)EX 

ev'2nv  V-I 

> 

> 

__ 1_ (2enE)V e-81lE 
ev'2nv 
(SE)V 

v  -8nE 

v 

- - - n e  
logv(1/8) 

(sinceE:::  l/S) 

(since we assume log G) :::  (~) (ev'27r v y/v ). 

The function nV e- SIlE  varies unimodally in n, and achieves a peak at n = v I(SE). 
For n below this threshold, by monotonicity, we apply the bound at n = VI(SE). It 
is easy to verify that the value of the bound at v / (SE) is always at least 8. If on the 
other hand,  (1 /SE) 10g(1 /8) :::  n  ::::  v / (SE),  the lower bound achieves its minimal 
value at (1 ISE) 10g(1 18), and the value there is 8. This proves the first bound.  0 

Corollary 14.1  shows that for any classifier, at least 

( 1  (1)  Vc  - 1) 

"8  '~ 

max  SE  log 

training  samples  are  necessary  to  achieve  E accuracy  with  8  confidence  for  all 
distributions. Apart from a log (~ ) factor, the order of magnitude of this expression 
is the same as that of the upper bound for empirical error minimization, obtained in 
Corollary 12.4. That the upper and lower bounds are very close, has two important 
messages. On the one hand it gives a very good estimate for the number of training 
samples  needed  to  achieve  a  certain  performance.  On  the  other  hand  it  shows 
that  there  is  essentially  no  better  method  than  minimizing  the  empirical  error 
probability. 

In the case  Lc  >  0,  we may derive lower bounds for  N(E, 8)  from Theorems 

14.5 and 14.6: 

Problems and Exercises 

247 

COROLLARY  14.2.  Let 9 be a class of discrimination functions with vc dimension 
V  2:  2.  Let  X  be  the  set  of all  random  variables  (X, Y)  for  which for fixed 
L  E  (0,  1/2), 

L  = infP{g(X);lY} . 

gEe;; 

Then, for every discrimination rule gn  based upon Xl, Y1 ,  ••. ,  X n, Yn, 

N(E,8) 2: 

L(V -

l)e- lO 

32 

Also,  and in particular, for E  ::::;  L  ::::;  1 14, 

(1  1) 

xmin  82 'E 2 

. 

L 

1 

N(E, 8)  2:  4E2  log 48  . 

PROOF.  The  first  bound  may  be  obtained  easily  from  the  expectation-bound  of 
Theorem 14.5 (see Problem 14.1). Setting the bound of Theorem 14.6 equal to 8 
provides the second bound on N (E,  8).  0 

These bounds may of course be combined. They show that N (E, 8) is bounded 
from  below by terms  like  (l/E2)log(1/8)  (independent of Vc)  and (Ve  -
l)/E2, 
as  I)  is typically much smaller than E.  By comparing these bounds with the upper 
bounds  of Corollary  12.3, we  see that the only difference between the  orders of 

magnitude is  a log (1 IE )-factor,  so  all remarks made for the case Le  = ° remain 

valid. 

Interestingly, all bounds depend on the class C only through its vc dimension. 
This fact suggests that when studying distribution-free properties of L(gn) - Le, 
the vc dimension is the most important characteristic of the class. Also, all bounds 
are linear in the vc dimension, which links it conveniently to sample size. 

REMARK. It is easy to see from the proofs that all results remain valid if we allow 
randomization in the rules gn.  0 

Problems and Exercises 

PROBLEM  14.1.  Show that Theorem 14.5 implies that for every discrimination rule gn  based 
upon D", 

L(V -

l)e- IO 

N(E,  8)  :::: 

32 

•  (1  1) 

x  mIll  82'  E2 

. 

HINT:  Assume that P{Ln  - L  >  E}  <  8. Then clearly, E{Ln - L}  ::::  E  + 8. 

PROBLEM  14.2.  Prove Theorem 14.3: First apply the proof method of Theorem 14.1 to show 
that for every n, andg", there is a distribution with Lc = 0 such thatEL(gn) ::::  (1-11 n)/(2e). 
Use a monotonicity argument to finish the proof. 

PROBLEM  14.3.  Prove Theorem 14.4 by modifying the proof of Theorem 7.2. 

15 
The Maximum Likelihood Principle 

In this chapter we explore the various uses of the maximum likelihood principle 
in discrimination.  In general,  the principle is  only applicable if we have  some a 
priori knowledge of the problem at hand. We offer definitions, consistency results, 
and examples that highlight the advantages and shortcomings. 

15.1  Maximum Likelihood: The Formats 

Sometimes, advance information takes a very specific form (e.g.,  "if Y  =  1,  X is 
normal (fL, a 2 )"). Often, it is rather vague (e.g., "we believe that X has a density," 
or "17 (x ) is thought to be a monotone function of x  E  R"). 

If we have information in set format,  the maximum likelihood principle is less 
appropriate. Here we know that the Bayes rule g*(x) is of the form g(x) = I{xEA} ' 
where A  E  A and A is a class of sets of Rd. We refer to the chapters on empirical 
risk minimization (see Chapter 12 and also Chapter 18) for this situation. 

If we know that the true (unknown) 17  belongs to a class :.F of functions that map 
n d  to [0,  1], then we say that we are given information in regressionformat. With 
each 17'  E  :.F  we associate a set A  =  {x  :  17' (x)  >  1/2} and a discrimination rule 
g(x) = I{xEA}.  The class of these rules is denoted by C.  Assume that we somehow 
could estimate  17  by  17n.  Then it makes  sense  to  use  the  associated rule  gn(x)  = 
I/2}. The maximum likelihood method suggests a way of picking the 17n  from 
I{1)Il(x» 
:.F that in some sense is most likely given the data. It is fully automatic-the user 
does not have to pick any parameters-but it does require a serious implementation 
effort in many cases.  In a sense, the regression format is more powerful than the 

250 

15.  The Maximum Likelihood Principle 

set format, as there is more information in knowing a function 1]  than in knowing 
the indicator function  I{7]>1/2}.  Still, no structure is assumed on the part of X, and 
none is needed to obtain consistency results. 

A third format, even more detailed, is that in which we know that the distribution 
of (X, Y) belongs to a class V  of distributions on nd x {O,  I}. For a given distribu(cid:173)
tion, we know 1], so we may once again deduce a rule g by setting g(x) = I{17(X»1/2}. 
This distributionformat is even more powerful, as the positions X I, ... ,  Xn  alone 
in some cases may determine the unknown parameters in the model.  This  situa(cid:173)
tion fits in squarely with classical parameter estimation in mathematical statistics. 
Once again, we may apply the maximum likelihood principle to select a distribu(cid:173)
tion from V. Unfortunately, as we move to more restrictive and stronger formats, 
the number of conditions under which the maximum likelihood principle is consis(cid:173)
tent increases as well. We will only superficially deal with the distribution format 
(see Chapter 16 for more detail). 

15.2  The Maximum Likelihood Method: 

Regression Format 

Given Xl, ... , X n, the probability of observing Y1 = YI,  ... , Yn = Yn  is 

n 

Il1](Xi )Yi (l - 1](Xi »l- yi . 
i=l 

If 1]  is  unknown but belongs  to  a  family  F  of functions,  we  may wish to  select 
that  1]'  from  F  (if it exists)  for which that likelihood product is  maximal.  More 
formally,  we select 1]'  so that the logarithm 

is  maximal.  If the  family  F  is  too  rich,  this  will  overfit,  and  consequently,  the 
selected function 1]n  has a probability of error 

that  does  not tend to  L *.  For convenience,  we  assume  here  that  there  exists  an 
element of F  maximizing .en. 

We do not assume here that the class F is very small. Classes in which each 1]' in 
F is known up to one or a few parameters are loosely called parametric. Sometimes 
F  is defined via a generic description such as: F  is the class of all 1]'  : nd  -+  [0,  1] 
that are Lipschitz with constant c. Such classes are called nonparametric. In certain 
cases,  the  boundary  between  parametric  and nonparametric  is  unclear.  We  will 
be  occupied  with  the  consistency  question:  does  L(1]n)  -+  inf17'EF L(1]')  with 
probability one for all distribution of (X, Y)? (Here L(1]') = P  {I{17'(X»1/2}  i  Y} is 

15.2 The Maximum Likelihood Method: Regression Format 

251 

the probability of error of the natural rule that corresponds to rJ'.) If, additionally, F 
is rich enough or our prior information is good enough, we may have inf 11' EF L (rJ')  = 
L *, but that is not our concern here, as F  is given to us. 

We will not be concerned with the computational problems related to the maxi(cid:173)

mization of Ln(rJ') over F. Gradient methods or variations of them are sometimes 
used-refer to McLachlan (1992) for a bibliography. It suffices to say that in simple 
cases, an explicit form for rJn  may be available. An example follows. 

Our first lemma illustrates that the maximum likelihood method should only be 
used  when the true  (but unknown)  rJ  indeed belongs to F. Recall that the  same 
was not true for empirical risk minimization over vc classes (see Chapter 12). 

Lemma 15.1.  Consider the  class  F  with  two functions  rJA  ==  0.45, rJB  ==  0.95. 
Let rJn  be  the function picked by the maximum likelihood method.  There  exists a 
distribution for (X, Y) with rJ 

tJ.  F  such that with probability one,  as n  ---+  00, 

L(rJn)  ---+  max L(rJ')  >  min L(rJ'). 

l1'EF 

l1'EF 

Thus,  maximum likelihood picks the wrong classifier. 

PROOF.  Define the distribution of (X, Y)  on {O,  1}  x  {O,  1}  by P{X = 0,  Y = O}  = 
p {X  = 1, Y  = O}  = 2/9, P {X  = 0, Y  = 1}  = 1/9, and P {X  = 1,  Y  = 1}  = 4/9. 
Then one may quickly verify that 

(Note that L * = 1/3, but this is irrelevant here.) Within F, rJB  is the better for our 
distribution. By the strong law of large numbers, we have 

with  probability  one  (and  similarly  for  rJB).  If one  works  out  the  values,  it  is 
seen that with probability one,  rJn  ==  rJA  for all n  large enough. Hence,  L(rJn)  ---+ 
maxl1'EF L(rJ') with probability one.  0 

REMARK. Besides the clear theoretical hazard of not capturing rJ  in F, the maximum 
likelihood method runs into a practical problem with "infinity." For example, take 
:F  =  {rJA  ==  0,  rJB  ==  1},  and  assume  rJ  ==  1/3.  For  all  n  large  enough,  both 
classes are represented in the data sample with probability one. This implies that 
Ln(rJA)  = Ln(rJB)  = -00. The  maximum  likelihood  estimate  rJn  is  ill-defined, 
while any reasonable rule should quickly be able to pick rJA  over rJB.  0 

The  lemma shows  that when  rJ 

tJ.  F,  the  maximum likelihood method is  not 
even capable of selecting one of two choices. The situation changes dramatically 
when  rJ  E  F. For finite  classes F, nothing can go wrong.  Noting that whenever 
rJ  E  F,  we have  L *  =  infl1'EF L(rJ'),  we  may  now  expect that  L(rJn)  ---+  L * in 
probability, or with probability one. 

252 

15.  The Maximum Likelihood Principle 

Theorem 15.1.  lflFI = k  <  00, and 1]  E  F, then the maximum likelihood method 
is consistent,  that is, 

L(1]n)  -+  L * 

in probability. 

PROOF. For a fixed distribution of (X, Y), we rank the members 1](1),  ... , 1](k)  of F 
by increasing values of L(1](i»). Put 1](1)  ==  1].  Let io  be the largest index for which 
L(1](i»)  = L *.  Let 1]n  be the maximum likelihood choice from F. For any  a, we 
have 

P{L(1]n) =I L *} 

<  P  {Ln(1](l»)  ::: rp.~x Ln(1J(i»)} 

1>10 

<  P{Ln(1](l»):::  a} + LP{Ln(1](i»)  ~ a}. 

i>io 

Define the entropy of (X, Y) by 

£  = £(1]) = -E {1](X) log 1](X) + (1  - 1](X)) log(1  - 1](X))} 

(see Chapter 3). Recall that ° :::  £  :::  log 2. We also need the negative divergences 

Di = E 

'fj(X) log - - + (1  -
{ 

1](i)(X) 
1](X) 

'fj(X)) log 

1 - 1](i)(X) I 

, 

1 - 1](X) 

which are easily seen to be nonpositive for all i  (by Jensen's inequality). Further(cid:173)

more,  Di  = ° if and only if 1](i)(X) = 1](X) with probability one. Observe that for 
i  >  io,  we cannot have this.  Let e =  maxi>io  Di .  (If io  =  k, this set is empty, but 
then the theorem is trivially true.) 

It is  advantageous to take a  = -£ + e /2. Observe that 

E  Ln(1] 

{

(i)  }  _ 

)  - -£ + Di 

{  = -£ 

c  e  ·f· 

if i  = 1 
. 
1  1  >  lO. 

:::  -0  + 

Thus, 

P{L(ryn)  i  L *)  :'0  P {Ln(ry(I))  :'0  -E + ~} + L P {Ln(ry(i))  :::  -E + n . 

1>10 

By the law  of large numbers,  we  see that both terms  converge to  zero.  Note,  in 
particular, that it is true even if for some i  >  io, Di  = -00. D 

For infinite classes, many things can go wrong.  Assume that F  is  the class of 
all 1]'  : R2 -+  [0,  1] with 11'  = fA  and A  is a convex set containing the origin. Pick 
X  uniformly  on the perimeter of the  unit circle.  Then the  maximum likelihood 
estimate 'fjn  matches the data,  as  we may  always  find  a closed polygon  Pn with 

15.3 Consistency 

253 

vertices  at the  Xi'S  with Yi  = 1.  For r;n  = lPn'  we have Ln(r;n) = ° (its  maximal 
value), yet L(r;n) = P{Y = I}  = p  and L* = 0. The class F  is plainly too rich. 

For distributions  of X  on the positive integers,  maximum likelihood does  not 
behave  as  poorly,  even though it must pick among infinitely many possibilities. 
Assume F is the class of all r;', but we know that X puts all its mass on the positive 
integers. Then maximum likelihood tries to maximize 

(X) n (r;' (i))Nl,i (1  -

i=l 

r;' (i))NO,i 

over  r;'  E  F, where  Nl,i  = L~=l I{Xj=i,Yj=l)  and  No,i  = L~=l I{xj=i'yj=O}.  The 
maximization is to be done over all (r;'(I), r;'(2),  ... ) from ®~l [0,  1]. Fortunately, 
Thus, if NO,i  + N1,i  = 0, we set r;n(i) = ° (arbitrarily), while if No,i  + Nl,i  >  0, we 
this is turned into a maximization for each individual i -usually we are not so lucky. 

pick r;n(i) as the value u that maximizes 

Nl,i log u + NO,i 10g(1  - u). 

Setting the derivative with respect to u equal to zero shows that 

( .) 

Nl,i 

r;n  l  = - - - -
Nl,i + NO,i 

In other words, r;n  is the familiar histogram estimate with bin width less than one 
half.  It is known to be universally consistent (see Theorem 6.2). Thus, maximum 
likelihood may work for large F  if we restrict the distribution of (X, Y)  a bit. 

15.3  Consistency 

Finally,  we  are ready  for  the  main consistency result for  r;n  when F  may have 
infinitely  many elements.  The conditions  of the theorem involve  the  bracketing 
metric entropy of F, defined as  follows:  for every distribution of X,  and E  >  0, 
let FX,E  be  a  set  of functions  such  that for  each  r;'  E  F, there  exist functions 
r;~, r;~  E  FX,E  such that for all x  End 

and 

r;~(x) ::::  ,l(x) ::::  r;~(x), 

E{r;~(X) - r;~(X)} :::  E. 

That is, every r;'  E  F  is "bracketed" between two members of FX,E  whose L1 (IL) 
distance is  not larger than E.  Let N (X, E)  denote the cardinality of the smallest 
such FX,E' If N(X, E)  <  00, 

log N(X, E) 

is called the bracketing E -entropy of F, corresponding to X. 

254 

15.  The Maximum Likelihood Principle 

Theorem 15.2.  Let F  be a  class of regression functions  n d  ~ [0,  1].  Assume 
that for every distribution  of X  and E  >  0,  N(X, E)  <  00.  Then  the  maximum 
likelihood choice rJn  satisfies 

lim  L(1]n)  = L * 
n--+oo 

in probability 

for all distributions of (X, Y) with rJ  E  F. 

Thus, consistency is guaranteed if F  has a finite bracketing E -entropy for every 
X  and E.  We provide examples in the next section. For the proof, first we need a 
simple corollary of Lemma 3.2: 

COROLLARY  15.1.  LetrJ, rJ':  nd  ~ [0,  1], and let (X,  Y)beanndx{O, 1}-valued 
random variable pair with P{Y =  llX = x} = rJ(x).  Define 

L * = E{min(rJ(X),  1 - 1](X))}, 

L(rJ') = P {I{ryl(X»1/2l  =I  Y} . 

Then 

E {)rJ'(X)rJ(X) + )(1 - rJ'(X))(1  - rJ(X))}  < 

(2ElrJ(X) - rJ'(X)I)2 
1 - --'----------'---

8 

< 

1 - ~(L(rJ') - L *)2 
. 

8 

PROOF.  The first  inequality follows  by Lemma 3.2,  and the  second by Theorem 
2.2.0 

Now, we are ready to prove the theorem. Some of the ideas used here appear in 

Wong and Shen (1992). 
PROOF OF THEOREM  15.2.  Look again at the proof for the case I FI  <  00 (Theorem 
15.1). Define E as  there. Let F(E)  be the collection of those rJ'  E  F  with L(rJ')  > 
L * + E, recalling that L(rJ') = P {I{ryl(X»1/2l  =I Y}. For every a, 

For reasons that will be obvious later, we take a  = -E - E2/ 16. Thus, 

15.3 Consistency 

255 

Noting that E {Ln(ry)}  = -£, the law of large numbers implies that the first term 
on the right-hand side converges to zero for every E.  (See Problem 15.4 for more 
information. ) 

Next we bound the second term. For a fixed distribution, let FX,8  be the smallest 

set of functions such that for each ry'  E  F, there exist ry~, ry~  E  FX,8  with 

ry~(x) ::::  ry'(x)  ::::  ry~(x),  x  End 

and 

E{ry~(X) - ry~(X)} ::::  8, 

where 8  >  0 will be specified later.  By assumption,  N(X, 8)  = IFx,81  <  00. We 
have, 

ry~ (Xi))l-Y; 
, 

l-Y  ~ e 

ry(Xi )  , (1  - ry(Xi)) 

-nE2/8} 

Y 

:::: 

P 

sup 
ry'EF(E)  i=l 

{  nn  ry~(XJY; (1 
::::  N 2(X,8)  sup  P  n ryu 

ry'EF(E) 

{ 

i=l 

(where ry~, ry~  E  F X ,8,  ry~  ::::  ry'  ::::  ry~ and E{ry~(X) -

ry~ (X)}  <  8) 

n 

'(X )Y(1 

i  Y' 

-

'  (X ))l-Y 
ryL 

} 
i l-Y  ,  ~ e-nE2/8 

ry(Xi)I(1-ry(Xi)) 

I 

(by the union bound) 

N'(X, 8) "~u:') P {O 
x  sup  (E {Jry~(X)ry(X) + J(1  - ry~(X))(1 _  ry(X))})n enE2/16, 

::::  N2(X, 8) 

ry'EF(E) 

256 

15.  The Maximum Likelihood Principle 

where in the last step  we used Markov's  inequality  and independence. We now 
find a good bound for 

E {J ry~(X)ry(X) + J(l - ry~(X»(1 - ry(X»} 

for each 1]'  E  FeE)  as follows: 

E {J ry;/X)ry(X) + J(l - ry~ (X»(l - ry(X»} 
:s  E  {J1]'(X)1](X) + .j(l- 1]'(X»(l - 1](X» 

+ J ry(X) (ry~(X) - ry'(X») + J(1 - ry(X»  (ry'(X)  - ry~ (X») } 
(here we used ra+b :s  Fa + Jb) 

:s  E {J 1]'(X)1](X) + .j (l - 1]'(X»(l - 1](X»} 

+ .jE{1](X)}JE {1]~(X) - 1]'(X)} 
+.jE{1 -1](X)}JE {1]'(X) -1]~(X)} 
(by the Cauchy-Schwarz inequality) 

:s  E {J1]'(X)1](X) + .j(l - 1]'(X»(l - 1](X»} + 2.Ji 

:s  1 -

E2 
- + 2.Ji 
8 

(by Corollary 15.1). 

Summarizing, we obtain 

ry'EF(E) 

8 

p  {  sup  Ln(1]') - Ln(1])  ::::  _  E2  } 

:s:  N'(X, 0) (1 - ~ + 2J8 r e""/16 

By taking 8  <  (~)2, we see that the probability converges to zero exponentially 
rapidly, which concludes the proof.  0 

15.4  Examples 

In this section we show by example how to apply the previous consistency results. 
In all cases, we assume that 1]  E  F and we are concerned with the weak convergence 

15.4 Examples 

257 

of LCr},J to L * for all such distributions of (X, Y). The classes are as  follows: 

{ry  =  /[a,b],  -00:::: a:::: b::::  oo} 
{ry  = C/[a,b]' C  E  [0,  1], -00 ~ a  ::::  b ::::  oo} 
{ry  = /[aj,bIlx",x[ad,bd],  -00 ::::  ai  ::::  bi  ::::  00,  1 ::::  i  ::::  d} 

Fl  = 
F{ 
F2 
F3  =  {ry:  ry(x) = cx/(l +cx): x  ::::  0, c::::  O} 

{ry  is monotone decreasing on [0,  I]} 

F5  = 

{ ry:  ry(x) = 
{ry  : ry(x) = sin2(ex), e E  R} 

1 + Ilx - m 

1 

,m E  R d

} 

ry  : ry(x) = 

eao+o? x 
T 

1 + eao+a  x 

{ 

} 
,ao E  R, x, a  E  Rd 

. 

These are all rather simple, yet they will illustrate various points. Of these classes, 
F4  is  nonparametric;  yet,  it  behaves  "better"  than  the  one-parameter  class  F6 , 
for  example.  We  emphasize  again  that  we  are  interested  in  consistency  for  all 
distributions of X. 

For F 1, ryn  will agree with the samples, that is, ryn(X i )  = Yi for all i, and therefore, 

ryn  E Fl is any function of the form  /[a,b]  with 

X(l-I)  <  a::::  X(l),  X(r)::::  b  <  X(r+l), 

where  X(1)  ::::  ...  ::::  x(n)  are  the  order  statistics  for  XI, ... ,Xn  (with  x(O)  = 
-00, x(n+l)  = (0), x(l)  is  the  smallest data point with  y(l)  = 1,  and  x(r)  is  the 
largest data point with y(r) = 1. As L * = 0, we claim that 

E{L(ryn)}  <  p{X(l-l) <  X  <  X(l)} +p{x(r) <  X  <  x(r+l)} 

< 

4 

n+1 

The rule is simply excellent, and has universal performance guarantees. 

REMARK. Note that in this case, maximum likelihood minimizes the empirical risk 
over the class of classifiers C = {¢ = /[a,b] , a  ::::  b}. As C has vc dimension Vc  = 2 
(Theorem 13.7), and inf¢Ec L(¢) = 0, Theorem 12.7 implies that for all E  >  0, 

P{L(ryn)  >  E}  ::::  8n 22-nE/2

, 

and that 

E{L(ryn)}  ::::  - - (cid:173)

4logn +4 

nlog2 

(see Problem 12.8). With the analysis given here, we have gotten rid of the log n 
factor.  0 

258 

15.  The Maximum Likelihood Principle 

The class F{  is much more interesting. Here you  will observe a dramatic dif(cid:173)

ference with empirical risk minimization, as the parameter c plays a key role that 
will aid a lot in the selection. Note that the likelihood product is zero if NI  = 0 or 
if Yi  =  1 for some i  with Xi  tj  [a, b], and it is 

cN1(1  - c)NO  otherwise, 

where  No  is  the  number of (Xi, Yi )  pairs  with  a  ::::  Xi  ::::  b,  Yi  = 0,  and  NI  is 
the number of (Xi, Yi )  pairs with a  ::::  Xi  ~ b,  Yi  =  1.  For fixed  No,  N I, this is 
maximal when 

c=---No+NI 

Resubstitution yields that the likelihood product is zero if NI  = 0 or if Yi  = 1 for 
some i with Xi  tj  [a,  b], and equals 

exp {NIIOg  NI 

No  + Nl 

+ No log 

No 

No  + NI 

}  if  otherwise. 

Thus, we should pick 'fln  = cI[a,b] such thatc = NI/(No+NI)' and [a,  b] maximizes 
the divergence 

Nilog 

See Problem 15.5. 

NI 

No+NI 

No 

+ No log - - -
No+NI 

For .1'2,  by a similar argument as  for .1'1,  let  [AI, Bd  x  "  .  X  [Ad,  Bd] be the 
smallest rectangle of R d  that encloses all  Xi'S for which Yi  =  1.  Then, we know 
that 'fln  = I[Al,BI1x ... x[Ad,Bd]  agrees with the data. Furthermore, L * = 0, and 

E{L(7]n)}  ~ -

4d 
n+1 

(see Problem 15.6). 

The logarithm of the likelihood product for .1'3  is 

L  log(cXi) - Llog(1 + cXi )· 

n 

i:Yi=l 

i=l 

Setting the derivative with respect to  c equal to zero in the hope of obtaining an 
equation that must be satisfied by the maximum, we see that c must satisfy 

NI=t~=t  Xi 

i=l  1 + cXi 

i=1  1/c + Xi 

unless  Xl  = ... = Xn  = 0,  where Nl  = L7=1  I{Yi=I}'  This equation has  a unique 
solution for c. The rule corresponding to 'fln  is of the form g(x) = I{cx>I}  = I{x>l/c}' 
Equivalently, 

(x) = {  1 
g 

if  Nl  >  L7=1  x:xi 

0  otherwise. 

15.4 Examples 

259 

This surprising example shows that we do not even have to know the parameter c in 
order to describe the maximum likelihood rule. In quite a few cases, this shortcut 
makes such rules very appealing indeed. Furthermore, as the condition of Theorem 
15.2 is fulfilled,  1]  E  :F3  implies that the rule is consistent as well. 

For :F4, it is  convenient to  order the  Xi'S  from  small  to  large  and to  identify 
k consecutive groups, each group consisting of any  number of Xi'S with Yi  = 0, 
and one  Xi  with Yi  = 1.  Thus, k  = L:7=1  I{Yi=I}' Then, a moment's thought shows 
that l]n  must be piecewise constant, taking values  1 ::::  al  ::::  ... ::::  ak  :::: ° on the 

consecutive groups, and the value zero past the k-th group (which can only consist 
of Xi'S with Yi  = 0.  The likelihood product thus is of the form 

al(1 - adn1a2(1  - a2)n2  ••• ak(1 

ak)nk , 

where n I,  ... , n k  are the cardinalities of the groups minus one (i.e., the number of 
Yj  = O-elements in the i -th group). Finally, we have 

(aI, ... , ak) =  argmax  IT aj(1 - aj)n j

k 

l::::al::::···::::ak::::O j=l 

• 

To check consistency, we see that for every X and E  >  0, 

N(X, E)  ::::  41~1 

(see Problem 15.7). Thus, the condition of Theorem 15.2 is satisfied, and L(l]n) ---+ 
L * in probability, whenever 1]  E  :F4 . 

In :Fs, the maximum likelihood method will attempt to place m at the center of 
the highest concentration in n d  of Xi'S with Yi  = 1, while staying away from Xi'S 
with Yi  = 0. Certainly, there are computational problems, but it takes little thought 
to verify that the conditions of Theorem 15.2 are satisfied. Explicit description of 
the rule is not necessary for some theoretical analysis! 

Class :F6  is  a simple one-parameter class that does not satisfy the condition of 
Theorem  15.2.  In  fact,  maximum likelihood fails  here for the following  reason: 
Let X be uniform on [0,  1]. Then the likelihood product is 

IT  sin2

(8Xi) x  IT  cos2(8Xi ). 

i:Yi=O 

This product reaches a degenerate global maximum (1) as 8  ---+  00, regardless of 
the true (unknown) value of 8 that gave rise to the data. See Problem 15.9. 

Class :F7  is used in the popular logistic discrimination problem, reviewed and 

studied by Anderson (1982), see also McLachlan (1992, Chapter 8). It is particu(cid:173)
larly important to observe that with this model, 

260 

15.  The Maximum Likelihood Principle 

where  f3  = -Cio  -
log 2.  Thus,  :F7  subsumes  linear discriminants.  It does  also 
force a bit more structure on the problem, making  rJ  approach zero or one as  we 
move  away  from  the  separating hyperplane.  Day  and Kerridge  (1967) point out 
that model :F7  is appropriate if the class-conditional densities take the form 

cf(x) exp {-~(x - m)TE-'(x - m)} , 

where c is a normalizing constant, f  is a density, m is a vector, and b  is a positive 
definite matrix; f  and  :E  must be the same for the two densities, but c and m may 
be different.  Unfortunately,  obtaining the best values for Cio  and Ci  by maximum 
likelihood takes a serious computational effort. Had we tried to estimate f, m, and 
:E  in the last example,  we  would have  done more than what is  needed,  as  both 
f  and  b  drop  out of the  picture.  In  this  respect,  the  regression  format  is  both 
parsimonious and lightweight. 

15.5  Classical Maximum Likelihood: Distribution 

Format 

In a more classical approach, we assume that given Y = 1, X has a density iI, and 
given Y  = 0, X has a density  fo,  where both fo  and iI belong to a given family :F 
of densities. A similar setup may be used for atomic distributions, but this will not 
add anything new here, and is rather routine. The likelihood product for the data 
(Xl, Yd,  ... , (Xn,  Yn) is 

n n (pfl(Xi))Y; ((1  - p)fO(Xi))l-Yi , 

i=l 

where p  = P {Y  =  I} is assumed to be unknown as well. The maximum likelihood 
choices for  p,  fo,  fl  are given by 

(p*,  f;,  ft) = 

arg max  Ln(p,  fo,  iI), 

pE[O, 1], fo, II EF 

where 

Having determined p*,  fo*'  ft (recall that the solution is not necessarily unique), 
the maximum likelihood rule is 

(x) = I ° if (1  - p*)fo*(x)  2:  p* ft(x) 

1  otherwise. 

gn 

Generally speaking, the distribution format is  more sensitive than the regression 
format.  It may  work better under  the  correct  circumstances.  However,  we  give 

Problems and Exercises 

261 

up  our  universality,  as  the  nature  of the  distribution  of X  must be  known.  For 
example, if X is distributed on a lower dimensional nonlinear manifold of n d , the 
distribution format is particularly inconvenient. Consistency results and examples 
are provided in a few exercises. 

Problems and Exercises 

PROBLEM  15.1.  Show that if IFI = k  <  00 and 1]  E  F, then L(1]n)  -+  L * with probability 
one, where 1]n  is obtained by the maximum likelihood method. Also, prove that convergence 
with probability one holds in Theorem 15.2. 

PROBLEM  15.2.  Prove that if 1],  1]' are [0,  1]-valued regression functions, then the divergence 

1 - 1]'(X)} 
D(1] ) = E  1](X) log - - + (1  - 1](X)  log - - -
1 - 1](X) 

' {   1]'(X) 
1](X) 

is  nonpositive,  and that  D  =  0  if and only if 1](X)  =  1]'(X)  with probability  one.  In this 
sense,  D  measures the distance between 1]  and 1]'. 

PROBLEM  15.3.  Let 

L * = E{min(1](X),  1 - 1](X»}, 

L(1]') = P  {I{i)'(X»lj2j =I Y} , 

and 

D(1]') = E  {1](X) log 1]'(X)  + (1-1](X))log 1 -1]'(X)} , 

1](X) 

1 - 1](X) 

where 1](x) = P{Y = llX = x}, and 1]': nd  -+  [0,1] is arbitrary. Prove that 

(see also Problem 3.22). HINT:  First prove that for p, q  E  [0,  1], 

plog- +(1- p)log 

q 

p 

1 -
1- p 

(p _  q)2 
::::  - - - -

2 

(use Taylor's series with remainder term for h(·)). 

PROBLEM  15.4.  Show that for each 1]  there exists an Eo  >  0 such that for all E  E  (0,  EO), 

HINT:  Proceed by Chernoff's bounding technique. 

PROBLEM  15.5.  Consider 1]  E  F{  and let 1]n  be a maximum likelihood estimate over F{. Let 
p = P{Y = I}.  Show that 

1 - c) 

L* = pmin  1,  -c- . 

( 

Derive an upper bound for 

E {L(17n)  - L *} , 

262 

15.  T,he Maximum Likelihood Principle 

where in case of multiple choices for 1]n,  you take the smallest 1]n  in the equivalence class. 
This is an important problem, as F{  picks a histogram cell in a data-based manner. F{  may 
be generalized to the automatic selection of the best k-cell histogram: let F  be the collection 
of all1]'s that are constant on the k intervals determined by breakpoints -00 <  al  <  ... < 
ak-l  <  00. 

PROBLEM  15.6.  Show that for the class F2 , 

E{L(1],J} s (cid:173)

4d 
n+l 

when Yj  E  F2  and Yjn  is the maximum likelihood estimate. 

PROBLEM  15.7.  Show that for the class F4 , 

holds  for all  X  and E  >  0,  that is,  the bracketing E-entropy  of F4  is  bounded by  f3/El. 
HINT:  Cover the class F4  by a class of monotone decreasing, piecewise constant functions, 
whose values are multiples of E /3, and whose breakpoints are at the kE /3-quantiles of the 
distribution of X (k = 1, 2,  ... ,  f3 / E l). 

PROBLEM  15.8.  Discuss the maximum likelihood method for the class 

if  aT x  >  f3 
otherwise; 

e,e'  E  [0,  1], a  E Rd ,j3  E R}. 

What  do  the  discrimination  rules  look like?  If Yj  E  F,  is  the  rule  consistent?  Can  you 
guarantee a certain rate of convergence for E{L(Yjn)}? If Yj  ¢.  F, can you prove that L(1]n) 
does not converge in probability to inf1J'EF L(Yj') for some distribution of (X, Y) with 1](x) = 
P {Y = 11 X = x}? How would you obtain the values of e, e', a, j3 for the maximum likelihood 
choice 1]/1? 

PROBLEM  15.9.  Let Xl, ... , X/1  be i.i.d. uniform [0,  1] random variables. Let YI,  ... , Yn  be 
arbitrary {O,  1}-valued numbers. Show that with probability one, 

lim sup n sin2

(eXi) x  n cOS

e-+oo 

i:Yi=l 

i:Yi=O 

2(eXi) = 1, 

while for any t  <  00, with probability one, 

sup  n sin\eXi )  x  n cOS

o:ses:t i:Yi=] 

i:Yi=O 

2(eXi)  <  l. 

16 
Parametric Classification 

What do you do if you believe (or someone tells you) that the conditional distribu(cid:173)
tions of X given Y = 0 and Y = 1 are members of a given family of distributions, 
described by finitely  many  real-valued parameters? Of course,  it does  not make 
sense to say that there are,  say,  six parameters. By interleaving the bits of binary 
expansions, we can always make one parameter out of six, and by splitting binary 
expressions, we may make a countable number of parameters out of one parameter 
(by writing the bits down in triangular fashion as shown below). 

b7 

b4 
bs 

b2 
bs 
b9 

bI 
b3 
b6 
ho 

Thus, we must proceed with care. The number of parameters of a family really 

is measured more by the sheer size or vastness of the family than by mere repre(cid:173)
sentation of numbers.  If the family  is  relatively small,  we will call it parametric 
but we  will not give you a formal  definition of "parametric." For now,  we let 8, 
the set of all possible values of the parameter e, be a subset of a finite-dimensional 
Euclidean space. Formally, let 

Pe = {Pe : e E  8}, 

be a class of probability distributions on the Borel sets of Rd. Typically, the family 
Pe  is parametrized in a smooth way.  That is,  two distributions, corresponding to 
two parameter vectors close to each other are in some sense close to each other, 

264 

16.  Parametric Classification 

as well. Assume that the class-conditional densities  fo  and fi  exist, and that both 
belong to the class of densities 

Fe = {fe  : e E  e}. 

Discrete examples may be handled similarly. Take for example all gaussian distri(cid:173)
butions on n d

,  in which 

where mEnd is the vector of means, and L:  is the covariance matrix. Recall that 
x T  denotes the transposition ofthe column vector x, and det(L:) is the determinant 
of L:.  This class is  conveniently parametrized bye = (m,  L:),  that is,  a vector of 
d + d(d + 1)/2 real numbers. 

Knowing that the class-conditional distributions are in Pe makes discrimination 
so much easier-rates of convergence to L * are excellent. Take Fe as the class of 
uniform densities  on hyperrectangles of n d
:  this  has  2d natural parameters, the 
coordinates of the lower left and upper right vertices. 

FIGURE 16.1.  The class-conditional densities are uni(cid:173)
form on hyperrectangles. 

class 0 

Given  (Xl, Yd, ... , (Xn,  Yn),  a  child  could  not  do  things  wrong-for class  1, 
estimate the upper right vertex by 

and similarly for the upper right vertex of the class 0 density. Lower left vertices are 
estimated by considering minima. If AD,  A 1 are the two unknown hyperrectangles 
and p  = P{Y = 1}, the Bayes rule is simply 

1 
if x  E  Al - AD 
o  if x  E  AD  - Al 
1 

1- p 
if x  E  Al n AD,  - - >  - -
A(Ao) 

A(AI) 

P 

g*(x) = 

o  if x  E  Al nAo  - - <  - - .  

1- p 
,  A(Ad  - A(Ao) 

p 

16. Parametric Classification 

265 

In reality, replace Ao,  AI, and p by the sample estimates Ao,  Al (described above) 
and p = (1/ n) 2:7=1  f{Yi=lJ.  This way of doing things works very well, and we will 
pick up the example a bit further on. However, it is a bit ad hoc. There are indeed a 
few main principles that may be used in the design of classifiers under the additional 
information given here. In no particular order, here are a few methodologies: 

(A)  As the Bayes classifiers belong to the class 

C = {¢ = f{p!el >(l-p)!eo)  :  P  E  [0,  1],80,81  E  e} , 

it  suffices  to  consider  classifiers  in  C.  For  example,  if Fe is  the  normal 
family, then C coincides with indicators of functions in the set 

{{x: x T Ax + bT 

X  + c  >  0:  A is a d  x  d matrix, b E  R d

,  C  E  R}} , 

that is,  the family  of quadratic decisions.  In the hyperrectangular example 
above, every ¢ is of the form f Al - A2  where Al and A2 are hyperrectangles of 
Rd. Finding the best classifier ofthe form ¢  =  fA  where A  E  A is something 
we can do in a variety of ways: one such way, empirical risk minimization, 
is dealt with in Chapter 12 for example. 

(B)  Plug-in rules estimate (80, 8d by (80, Ih)  and p  = pry = I}  by  pfrom the 

data, and form the rule 

gn (x) =  ° otherwise. 

I 

if PfeJx) >  (1  - [i)f?o(x) 

{

The rule here is within the class C described in the previous paragraph. We 
are hopeful that the performance with gn  is  close to the performance with 
the Bayes rule  g*  when (p, 8o,~) is  close to  (p, 80, 8d. For this  strategy 
to  work it is  absolutely essential that L(p, 80, 81)  (the probability of error 
when p, 80, 81  are the parameters) be continuous in (p, 80, 81).  Robustness 
is  a key ingredient. If the cOJ.?tinuity  can be captured in an inequality, then 
we may get performance guarantees for E{ Ln} - L * in terms of the distance 
between  (ii, 8o,~) and  (p, 80, 81).  Methods  of estimating the  parameters 
include  maximum likelihood.  This  methodology  is  dealt  with  in  Chapter 
15. This method is rather sensitive to incorrect hypotheses (what if we were 
wrong about our assumption that the class-conditional distributions were in 
P e ?).  Another strategy,  minimum distance estimation, picks that member 
from  P e  that  is  closest in  some  sense  to  the  raw  empirical measure  that 
puts mass  1/ n  at each if the n data points. See Section 16.3 This approach 
does not care about continuity of L(p, 80, 81), as it judges members of Pe 
by  closeness  in  some  space  under  a  metric  that  is  directly  related  to  the 
probability of error. Robustness will drop out naturally. 

A general approach should not have to know whether e can be described by a fi(cid:173)
nite number of parameters. For example, it should equally well handle descriptions 

266 

16.  Parametric Classification 

as Pe is the class of all distributions of (X, Y)  in which 1J(x) = P{Y = llX = x} 
is monotonically increasing in all the components of x. Universal paradigms such 
as maximum likelihood, minimum distance estimation, and empirical error min(cid:173)
imization are all applicable here.  This particular Pe  is  dealt with in Chapter 15, 
just to  show you that the description of the class  does invalidate the underlying 
principles. 

16.1  Example: Exponential Families 

A class Pe  is exponential, if every class-conditional density  fe  can be written in 
the form 

where /3,  0/1,  ... , o/k  : Rd  -+ R, /3  ::::  0, a, JT1,  ... , JTk  : e  -+ R  are fixed func(cid:173)
tions,  and c is  a normalizing constant. Examples of exponential families include 
the gaussian, gamma, beta, Rayleigh, and Maxwell densities (see Problem 16.4). 
The Bayes-rule can be rewritten as 

g*(x) ~ {  ~ 

This is equivalent to 

P!e*(X) 

) 

(l-p)l!eo(x)  >  0 

( 

if log 
otherwise. 

g*(X) = {01 

ifL7=10/i(X)(JTi(e;) - JTi(e:))  <  log (l~~~~~~o)) 
otherwise. 

The Bayes-rule is  a so-called generalized linear rule with  1, 0/1,  ... , o/k  as  basis 
functions.  Such  rules  are  easily  dealt  with  by  empirical  risk  minimization  and 
related methods such as complexity regularization (Chapters  12,  17,  18,22). 

Another important point is  that g* does not involve the function /3.  For all  we 
know,  f3  may be some esoteric ill-behaved function  that would make estimating 
fe(x)  all  but  impossible  if /3  were  unknown.  Even if Pe  is  the  huge family  in 
which f3  ::::  0 is left undetermined, but it is known to be identical for the two class(cid:173)
conditional densities (and aCe) is just a normalization factor), we would still only 
have to look at the same small class of generalized linear discrimination rules! So, 
densities do not matter-ratios of densities do. Pattern recognition should be, and 
is, easier than density estimation. Those who first estimate (fo, !I) by (10, it) and 
then construct rules based on the sign of fij-; (x) 
fj) 1o(x) do themselves a 
disservice. 

(1  -

16.2 Standard Plug-In Rules 

267 

16.2  Standard Plug-In Rules 
In standard plug-in methods, we construct estimates 80, Bt  and pfrom the data and 
use them to form a classifier 

(x) = {  1 

ifPf8t (x)  >  (1- fi)f80 (x) 

gn 

0  otherwise. 

It is  generally  not true  that if p -+  p, 80  -+  80 ,  and Bt  -+  81  in  probability, 
then  L(gn)  -+  L * in probability, where L(gn) is the probability of error with gn' 
Consider the following simple example: 

EXAMPLE.  Let:Fe  be the class of all uniform densities on [-8,0] if 8 =/1  and on 
[0,8] if 8 = 1. Let 80  = 1,81 = 2, P = 1/2. Then a reasonable estimate of 8i  would 
be fh  =  maxi:Yi=l  IXi I·  Clearly, fh  -+  8i  in probability.  However,  as fh  =/1  with 
probability one, we note that gn (x)  = 0 for x  >  0 and thus, even though L * = 0 
(as the supports of feD  and fel  are not overlapping), L(gn)  ~ P{Y = I}  = p = 1/2. 
The problem with this is that there is no continuity with respect to 8 in :Fe.  0 

Basic consistency based upon continuity considerations is indeed easy to estab(cid:173)

lish. As ratios of densities matter, it helps to introduce 

l7e(x)  = 

pfel (x) 

(1  - p) feD (x) + p fel (x) 

= P{Y = llX = x}, 

where  8  = (p, 80 , 8d or (80 , 8d  as  the  case may  be.  We  recall  that  if gn(x)  = 
I(1)(f(x»1/2}  where Ois an estimate of 8, then 

L(gn) - L * :::  2E { 117e(X) -

l7e(X)11  Dn}  , 

where Dn  is the data sequence. Thus, 

E  {L(gn)  - L*}  :::  2E {ll7e(X) -l7e(X)I}. 

By the Lebesgue dominated convergence theorem, we have, without further ado: 
Theorem 16.1.  If l7e  is continuous in 8 in the L 1 (M) sense, where M is the measure 
of X,  and 0 -+  8  in probability,  then  E{L(gn)}  -+  L * for the  standard plug-in 
rule. 

In some cases, we can do better and derive rates of convergence by examining 

the local behavior of l7e(x).  For example, if l7e(x) = e-e/x/, x  E  R, then 

117e(x) 

l7B'(x) 1 :::  Ixl18  - 8'1, 

and 

E{L(gn)}  - L*  <  2E {IX118  - 8]} 

<  2/E {X 2 )JE {(8 -en 

268 

16.  Parametric Classification 

yielding an explicit bound. In general, 8 is multivariate, consisting at least of the 
triple (p, 80 ,81), but the above example shows the way to happy analysis. For the 
simple example given here,  E{ L(gn)}  -+  L * if E { (8  - e?}  -+  O.  In fact,  this 
seems to suggest that for this family, e should be found to minimize E { (8  - 8) 2 

}. 
This  is  false.  One is  always  best off minimizing  the  probability  of error.  Other 
criteria may be relevant via continuity, but should be considered with care. 

How certain parameters are estimated for given families of distributions is what 
mathematical statistics is all about. The maximum likelihood principle looms large: 
80 is estimated for densities Ie  by 

80  = argmax n le(XJ 

e 

i:Yi=O 

for example. If you work out this (likelihood) product, you will often discover a 
simple form  for  the  estimate  of the  data.  In discrimination,  only  1]  matters,  not 
the  class-conditional  densities.  Maximum likelihood in function  of the  1]'S  was 
studied in Chapter 15. We saw there that this is often consistent, but that maximum 
likelihood behaves poorly when the true distribution is  not in Pe.  We  will work 
out two simple examples: 

As an example of the maximum likelihood method in discrimination, we assume 

that 

Fe =  {/e(x) =  x

a

l

x

f3 

e-

/
-
f3af(a) 

I{x>o}  :  a,  f3  > oJ 

is  the class of all gamma densities with 8  =  (a,  f3).  The likelihood product given 
(X 1,  Yl ), ... , (Xn ,  Yn ), is, if 80,81 are the unknown parameters, 

n n (plej(Xi)(i  ((1  - p)/eo(Xi))l-Yi 

• 

i=I 

This  is  the  probability  of observing  the  data  sequence  if the  Ie's  were  in  fact 
discrete probabilities. This product is  simply 

where 80 = (ao,  f30),  81 = (aI, f31)'  The first thing we notice is that this expression 
depends  only  on  certain  functions  of the  data,  notably  L Yi Xi,  L(l - Yi )Xi' 
L Yi log Xi, L(l - Yi) log Xi, and L Yi· These are called the sufficient statistics 
for  the problem at hand.  We  may in fact  throwaway the  data and just store the 
sufficient statistics. The likelihood product has to be maximized. Even in this rather 
simple univariate example, this is a nontrivial task. Luckily, we immediately note 
that p  occurs in the factor  pN (1  - p )n-N, where  N  = L Yi • This is  maximal at 
p =  N In,  a well-known result.  For fixed  ao, aI, we  can also get  f30,  f3l;  but for 

16.2 Standard Plug-In Rules 

269 

variable ao, aI, f30,  f31,  the optimization is difficult.  In d-dimensional cases, one 
has nearly always to resort to specialized algorithms. 

As a last example, we return to Fe = {unifonn densities on rectangles of Rd}. 
Here the likelihood product once again has pN (l-=-- P )n-N as a factor, leading to p = 
N In.  The other factor  (if (aI, bl), (ao, bo) are the lower left, upper right vertices 
of the rectangles for 81,(0 )  is zero if for some i, Yi =  1,  Xi  tf:- rectangle(al, bd, or 
Yi  = 0,  Xi  tf:- rectangle(ao, bo)·  Otherwise, it is 

1 

where Ilbl - al II  = TI~=l (bij) - aij)) denotes the volume of the rectangle (aI, h), 
and similarly for IIbo -ao II. This is maximal if IIbl -al II  and IIbo -ao II  are minimal. 
Thus, the maximum likelihood estimates are 

~k)  =  min  X~k),  1 :::;  k:::;  d,  i  = 0,1, 
ai 

j:Yj=i 

1 

j}k) 

1 

max X~k), 
j:YFi 

1 

1 :::;  k:::; d,  i  = 0,1, 

where ai = (ai 

--

~l) 

, ... , ai 

~d)  --

), bi  = (bi  , ... , bi 

'2(1) 

'2(d) 

), and Xi = (Xi  , ... , Xi  ). 

(d) 

(1) 

Rates of convergence may be obtained via some of the (in)equalities of Chapter 

6,  such as 

(1)  E{L(gn)} - L *  <  2E{I17n(X) - 17(X)I} 

(where 1717  = pf~1 (pf~ + (1  - P>feo )), 

(2)  E{L(gn)} - L *  <  2JE {(17n(X)  - 17(X))2}, 

(3)  E{L(gn)}  - L * 

2E { I ry(X) - ~ I l(g"(X)1g'(Xll }  . 

The rate with which e approaches 8  in e  (measured with some metric) may be 
very different from that with which L(gn) approaches  L *.  As shown in Theorem 
6.5,  the inequality (2)  is  always  loose,  yet it is  this  inequality that is  often used 
to  derive  rates  of convergence by  authors  and researchers.  Let us  take  a  simple 
example to  illustrate this point.  Assume Fe is  the family  of nonnal densities on 
the real line. If 80  = (mo, ao), 81  = (ml' al) are the unknown means and standard 
deviations of the class-conditional densities, and p  = P{Y = 1}  is also unknown, 
then we may estimate p  by P = L]=l Yj In,  and mi and ai  by 

respectively,  when denominators  are positive.  If a denominator is  0,  set mi  = 0, 
~2 =  1.  From Chebyshev's  inequality,  we can verify  that  for  fixed  p  E  (0,  1), 

i  = 0,1, 

270 

16.  Parametric Classification 

E{lp - pH  = 0  (l/-fo), E{lmi  - mil} = 0  (l/-fo), E{lai  - ~I} = 0  (l/-fo) 
(Problem 16.3). 

If we  compute  E {117n(X)  - 17(X)I}'  we  will  discover  that  E{L(gn)}  - L *  = 
o (l/-fo).  However,  if we  compute  E {\17(X)  -
~I I(gn(X):;t'g*(x)d,  we  will  find 
that  E{L(gn)}  - L *  =  0  (lIn).  Thus,  while  the  parameters  converge  at  a  rate 
o ( 1 I -fo)  dictated by the  central limit theorem,  and while  1711  converges  to  17  in 
L 1 (fL)  with the  same  rate,  the error rate  in discrimination is  much  smaller.  See 
Problems 16.7 to 16.9 for some practice in this respect. 

BIBLIOGRAPHIC  REMARKS.  McLachlan (1992) has a comprehensive treatment on 
parametric  classification.  Duda  and  Hart  (1973)  have  many  good  introductory 
examples and a nice discussion on sufficient statistics, a topic we do not deal with 
in this text. For maximum likelihood estimation, see Hjort (1986a;  1986b).  0 

16.3  Minimum Distance Estimates 

Here we describe a general parameter estimation principle that appears to be more 
suitable for plug-in classification rules than the maximum likelihood method. The 
estimated parameter is obtained by the projection of the empirical measure on the 
parametric family. 

The principle of minimum distance estimation may be described as follows. Let 
'Pe  = {Pe  : ()  E  8} be a parametric family of distributions, and assume that Pe*  is 
the unknown distribution of the  i.i.d.  observations  Zl, ... , Zn.  Denote by  Vn  the 
empirical measure 

Let D(·, .) be a metric on the set of all probability distributions on nd . The minimum 
distance estimate of ()*  is defined as 

()n  = argmin D(vn ,  Pe), 

eEe 

if it exists  and  is  unique.  If it is  not unique,  select one candidate for  which the 
minimum is attained. 

Consider for example the Kolmogorov-Smirnov distance 

DKs(P,  Q) = sup  IF(z) - G(z)!, 

ZEnd 

16.3 Minimum Distance Estimates 

271 

Pen 

-- - -- - -- - -- "". Vn 

FIGURE 16.2.  The  member oiPs closest 
to the empirical measure Vn  is chosen by 
the minimum distance estimate. 

where  F  and  G  are the distribution functions  of the measures  P  and  Q  on n d , 
respectively. It is easy to see that D KS  is a metric. Note that 

sup  IF(z) - G(z)1  = sup lP(A) - Q(A)I, 

ZERd 

AEA 

where  A  is  the  class  of sets  of  the  form  (-00, z(1»  x  ...  x  (-00, zed»~,  for 
z  =  (zO),  ... , zed»~  E  nd.  For the  Kolmogorov-Smirnov  distance  between  the 
estimated and the true distributions, by the triangle inequality, we have 

DKS(Pen , Pe*)  <  DKS(Pell , vn ) + DKs(vn ,  Pe*) 

<  2DKs (vn ,  Pe*), 

where in the second inequality we used the definition of en.  Now notice that the 
upper bound is just twice the Kolmogorov-Smirnov distance between an empirical 
distribution and the true distribution. By a straightforward application of Theorem 
12.5, for every nand E  >  0, 

since the n-th shatter coefficient seA, n) of the class of sets 

cannot exceed (ne / d)d. This can easily be seen by an argument similar to that in the 
proof of Theorem 13.8. From the inequalities above, we see that the Kolmogorov(cid:173)
Smirnov  distance  between  the  estimated  and  the  true  distributions  is  always 
o ( Jlog n / n ). The only condition we require is that en  be well defined. 

272 

16.  Parametric Classification 

Of course, rather than the Kolmogorov-Smirnov distance,  it is  the error prob(cid:173)
ability  of the plug-in classification rule in which we  are primarily interested.  In 
order to make the connection, we adapt the notions of minimum distance estima(cid:173)
tion  and  Kolmogorov-Smirnov  distance to  better suit the classification problem 
we are after. Every parametric family of distributions defines a class of sets in nd 
as  the collection A of sets of the form {x  : ¢(x) = I},  where the classifiers ¢  are 
the  possible  plug-in rules  defined  by  the  parametric  family.  The  idea here  is  to 
perform minimum distance estimation with the generalized Kolmogorov-Smirnov 
distance with respect to a class of sets closely related to A. 

Assume that both class-conditional distributions Pea'  Pel  belong to a parametric 
family Pe, and the class-conditional densities  feo,  fe]  exist. Then the Bayes rule 
may be written as 

*(x) = {I  ifae(x! > 0 
g 
where  ae(x)  = pfel(x) -
(1  - p)fea(x)  and  p  = P{Y  = I}.  We  use  the  short 
notation 8 = (p, 80, 8d. The function ae may be thought of as the Radon-Nikodym 
derivative of the signed measure  Qe  = PPe1  - (1  - p)Peo ' In other words, to each 
Borel set A  C  n d
,  Qe  assigns the real number Qe(A) = PPe1 (A) - (1- p)Peo(A). 
Given the data Dn  = «Xl, Yd, ... , (Xn, Yn)),  we define the empirical counterpart 
of Qe  by 

0  otherwIse, 

The minimum distance classification rule we propose projects the empirical signed 
measure 1}n  on the set of measures  Qe. The metric we use is also specifically fitted 
to the given pattern recognition problem: define the class of sets 

A = {{x  E  nd :  ae(x)  >  O}  : p  E  [0,  1],80,81  E  e} . 

A  is  just the  class  of sets  A  C  nd  such  that  i{XEA}  is  the  Bayes  rule  for  some 
8 = (p, 80,81). Also introduce 

13  =  {A n Be  : A, B  E  A} . 

Given  two  signed  measures  Q, Q',  we  define  their  generalized  Kolmogorov(cid:173)
Smirnov distance by 

DB(Q,  Q') = sup IQ(A) - Q'(A)I, 

AEB 

that  is,  instead  of the  class  of half-infinite  intervals  as  in  the  definition  of the 
ordinary  Kolmogorov-Smirnov  distance,  here  we  take  the  supremum  over  13,  a 
class  tailored  to  our  discrimination  problem.  Now,  we  are  ready  to  define  our 
minimum distance estimate 

e = argmin DB(Qe, 1}n), 

e 

16.3 Minimum Distance Estimates 

273 

where  the  minimum  is  taken  over  all  triples  e = (p, eo, (1)  with  p  E  [0,  1], 
()o,  ()l  E  e. The corresponding classification rule is 

if ae( x) >  0 
gg(x) =.  ·0  otherwise. 

I 

{

The next theorem shows that if the parametric assumption is valid, then ge performs 
extremely well. The theorem shows that if A has finite vc dimension V A, then with(cid:173)
out any additional conditions imposed on the parametric class, the corresponding 
error probability L(ge) is not large:  L(ge) - L * = 0 (IVA log n/n). 
Theorem 16.2.  Assume that both conditional densities  feo,  fe l  are  in  the para(cid:173)
metric class Fe.  Then for the classification rule defined above,  we have for every 
nand E  >  0 that 

P {L(ge) - L* >  E}  ::s  8s(8, n)e-nE2jS12, 
where s(8, n) is the n-th shatter coefficient of 8. Furthermore, 

E {L(ge) - L *}  ::s  32  109(8e;~8, n)). 

REMARK.  Recalling from Chapter  13  that s(8, n)  :::::  s2(A, n) and that seA, n)  ::s 
(n + l)vA, where VA  is the vc dimension of A, we obtain the bound 

P {L(ge) - L* >  E}  :::::  8(n + 1)2VAe-nE2jS12.  0 

The proof of the theorem is based upon two key observations. The first lemma 
provides a bound on L(ge) - L * in terms of the generalized Kolmogorov-Smirnov 
distance between the estimated and the  true parameters.  The second lemma is  a 
straightforward extension of the Vapnik-Chervonenkis inequality to  signed mea(cid:173)
sures. 
Lemma 16.1. 

PROOF.  Denote 

Ae  = {x  : ae(x) >  O} 

and  Ae = {x  : ae(x) >  A}, 

that is,  gg(x) = I{ag(x»O},  g*(x) = I{ae(x»O},  and Ae, Ae E  A. At the first  crucial 
step, we use the equality of Theorem 2.2: 

L(ge) - L * 

f I{ge(x);,fge(x)} lae(x)ldx 
- r  c  ae(x)dx +  [c 
J AenAe 

J AenAe 

ae(x)dx 

274 

16.  Parametric Classification 

<  r  ae(x)dx - r  ae(x)dx 
J AenA~ 
+  r  ae(x)dx - r  ae(x)dx 
JA~nAe 

JA~nAe 

J AenA~ 

Qe(Ae n A~) - Qe(Ae n A~) + Qe(A~ n Ae) - Qe(A~ n Ae) 

<  2Ds (Qe,  Qe), 

since both Ae n A~ and A~ n Ae  are members of Be.  0 

The next lemma is  an  extension of the  Vapnik -Chervonenkis  inequality.  The 

proof is left to the reader (Problem 16.11). 

Lemma 16.2.  For every nand E  >  0, 

P {Ds(Qe, 1)n)  >  E}  ::s  8s(B, n)e-nE2/32. 

The rest of the proof of Theorem 16.2 shows that the generalized Kolmogorov(cid:173)
Smimov distance with respect to B between the estimated and true distributions is 
small with large probability. This can be done as  we proved for the Kolmogorov(cid:173)
Smimov distance at the beginning of the section. 

PROOF OF THEOREM  16.2.  By Lemma 16.1, 

L(ge) - L *  <  2Ds(Qe,  Qe) 

<  2Ds(Qe, 1)n) + 2Ds (Qe, 1)n) 
<  4Ds(Qe,1)n) 

(by the definition of e). 

(by the triangle inequality) 

The theorem now follows by Lemma 16.2.  0 

Finally, we examine robustness of the minimum distance rule against modeling 
errors, that is, what happens if the distributions are not in Pe. A good rule should 
still work reasonably well if the distributions are in some sense close to the modeled 
parametric class  Pe. Observe that if for some e = (p, eo, (1)  the Bayes rule can 
be written as 

{1  if ae (x)  > ° 

g  (x) =  ° otherwise, 

* 

then Lemma 16.1  remains valid even when the class-conditional distributions are 
not  in  P e.  Denote  the  true  class-conditional  distributions  by  P;, Pt,  let  p *  = 
pry = 1},  and introduce Q* = p* Pt - (l - p)P;. Thus, 

L(ge) - L * 

(by Lemma 16.1) 

<  2Ds(Q*,  Qe) 
<  2Ds(Q*, 1)n) + 2Ds(1)n,  Qe) 
<  2Ds(Q*, 1)n) + 2Ds(1)n,  Qe) 
<  4Ds(Q*, vn) + 2Ds(Q*,  Qe) 

(by the triangle inequality) 

(by the definition of Qe) 

(again by the triangle inequality). 

16.4 Empirical Error Minimization 

275 

Lemma 16.2 now applies to the first term on the right-hand side. Thus, we conclude 
that if the Bayes rule is in A, then for all E  ::::  4 inf Qe  Ds( Q*,  Qe), 

P {L(ge) - L * >  EJ::s  8(n +1)2VAe-nE2j211. 

The constant in the exponent may be improved significantly by more careful anal(cid:173)
ysis.  In other words, if the Bayes rule is in A and the true distribution is  close to 
the parametric family in the generalized Kolmogorov-Smimov distance specified 
above,  then  the  minimum  distance  rule  still  performs  close  to  the  Bayes  error. 
Unfortunately, we cannot say the same if A does not contain the Bayes rule. Em(cid:173)
pirical error minimization, discussed in the next section, is however very robust in 
all situations. 

16.4  Empirical Error Minimization 

In this  section we explore  the  connection between parametric classification and 
rule selection by minimizing the empirical error, studied in Chapter 12. 

Consider the class C of classifiers of the form 

¢(x) = {O 

if (1  - .p)feo(x) ::::  pfe/x ) 

1  otherwIse, 

where p  E  [0,  1], and eo, el  E  8. The parametric assumption means that the Bayes 
rule is contained in C. Then it is a very natural approach to minimize the empirical 
error probability 

measured  on  the  training  data  Dn  over  classifiers  ¢  in the  class  C.  Denote  the 
empirically selected rule (i.e., the one minimizing Ln(¢)) by ¢~. For most typical 
parametric classes 8, the vc dimension Vc  is finite. Therefore, as a straightforward 
consequence of Theorem 12.6, we have 

COROLLARY  16.1.  If both conditional distributions are contained in the paramet(cid:173)
ric family  Pes,  then for the  error probability  L(¢~) = P{¢~(X) =I  YIDn} of the 
empirically optimal rule ¢~, we have for every nand E  > ° 
P {L(¢~) - L*  >  E}  ::s  8S(C, n)e-nE2j128. 

The result above means that  0  ( -/log n / n )  rate of convergence to  the  Bayes 
rule  is  guaranteed for  the  empirically optimal rule,  whenever the vc dimension 
Vc  is  finite.  This  is  the  case,  for example,  for exponential families.  If Pes  is  an 
exponential family,  with densities of the form 

Je(x) ~ ca(e)j:J(x)exp {tJri(e)</!i(X)} , 

276 

16.  Parametric Classification 

then by results from Chapter 13, S(C, n) :::  (n+ 1t+l. Observe that in this approach, 
nothing but properties of the class C are used to derive the a ( Jlog n In) rate of 
convergence. 

REMARK. ROBUSTNESS. The method of empirical minimization is clearly extremely 
robust against errors  in the  parametric model.  Obviously,  (see Theorem 12.6) if 
the  true conditional distributions  are not contained in the  class P e, then L * can 
be replaced in Corollary 16.1 by inf¢>Ec P{¢(X) ¥ Y}. If the model is fairly good, 
then this number should be very close to the Bayes risk L *.  D 

REMARK.  LOWER  BOUNDS.  The results  of Chapter  14  imply that for  some distri(cid:173)
butions  the  error probability  of the  selected rule  is  about  a (1 I vfn,)  away  from 
the Bayes risk.  In the parametric case, however, since the class of distributions is 
restricted by the parametric model,  this  is  not necessarily true.  In some cases,  a 
much faster rate of convergence is possible than the a ( Jlog n In) rate guaranteed 
by Corollary 16.1. See, for example, Problem 16.3, where an example is given in 
which the error rate is  a(1In). D 

Problems and Exercises 

PROBLEM  16.1.  Show that if both conditional distributions of X,  given  Y  = 0 and Y = 1, 
are gaussian, then the Bayes decision is quadratic, that is, it can be written as 

g*(x) =  {  ~  if'L.~:(d+I)/2 a(t/Ji(x) + ao  2:  0 

otherwise, 

where the functions  l/Ji(X)  are either of the form x(i)  (the i-th component of the vector x), 
or x (i)x(J) , and ao,  ... , ad+d(d+J)/2  E  R. 
PROBLEM  16.2.  Let 1 be the  normal  density  on the  real  line  with mean  m  and  standard 
deviation 0-

,  and we draw an i.i.d.  sample X I, ... ,  Xn  from 1, and set 

2

and 

---2 

0- = - L(Xi  - m)  . 

' "  2 

1 

11 

n  i=1 

Show that E {1m  - mil  = 0  (1/ In) and E {Io- - CTI}  = 0  (1/ In) by using Chebyshev's 
inequality.  Show that this  rate is  in fact tight.  Prove  also  that the result remains true if n 
is  replaced by  N,  a binomial  (n,  p) random variable  independent of XI, ... , X n ,  where 
p  E  (0,  1). That is, m becomes 0/ N) 2=::1  Xi  if N  >  0 and 0 otherwise, and similarly for 
CT. 
PROBLEM  16.3.  Assume that p  =  1/2, and that both class-conditional densities 10  and 11 
are gaussian on R  with unit variance, but different means. We use the maximum likelihood 
estimates  mo, iii]  of the  conditional means  mo  = E{XIY = O}  and m]  = E{XIY  = I}  to 
obtain the plug-in classifier ge.  Show that E {(mo  - mo)2}  = 00/ n). Then go on to show 
thatE {L(ge)}  - L* :::  OO/n). 

Problems and Exercises 

277 

PROBLEM  16.4.  Show  that the following  classes of densities  on R  constitute exponential 
families: 

(1)  gaussian family: 

(2)  gamma family: 

(3)  beta family: 

{ 

(4)  Rayleigh family: 

rea + f3) 
r(a)r(f3) x 

0'-1 

tJ-I 

. 

(1  - x) 

I(xEO.I).  a, f3  >  0 

}  . 
, 

E;Af3 1- Ae-tJ  /(2e)x"e-ex  /2,,"", 

2 

2 

{ 

1 

00 
1:tJ!r(j+,A)  2 

(X)2J+A-I 
- :  e f3  >  0  ,A  >  0 

} 
. 

" 

-

PROBLEM  16.5.  This exercise shows that one-parameter classes may be incredibly rich. Let 
C be the class of rules of the form 

ge(x) =  {  ~  if x  E  A+e 
if x  '/:  A+e, 

where  x  E  R,  8  E  R  is  a  parameter,  and  A  is  a  union  of intervals  on  the  real  line. 
Equivalently, ge  =  IA+e.  Let 

A = U [i - ~, i + ~) , 

i:h;=1 

2 

2 

where b I ,  b2 ,  .•• are bits, obtained as follows:  first list all sequences of length l, then those 
of length 2, et cetera, so that (b I ,  b2 ,  ..• )  is a concatenation of (1,0), (1,  1,  1,0,0, 1,0,0), 
et cetera. 

(1)  Show that for all n, there exists a set {x 1,  ... ,  X n }  C  R  that can be shattered by a 

set from  {A + e : e En}. Conclude that the vc dimension of C is infinite. 
If we use C for empirical error minimization and L * = 0, what can you say about 
E{L n }, the probability of error of the selected rule? 

(2) 

PROBLEM  16.6.  CONTINUATION.  Let X be uniform on 8 + [i  - 1/2, i + 1/2) with probability 
1/2i, i  2:  1. Set Y  = bi  if X  E  8 + [i  -1/2, i + 1/2), so that L*  = o. 

(1)  Derive the class of Bayes rules. 
(2)  Work out the details of the generalized Kolmogorov-Smirnov distance minimizer 

based on the class of (1). 

(3)  Provide the best upper bound on E{L n }  you can get with any method. 
(4)  Regardless of discrimination, how would you estimate e? Derive the asymptotic 

behavior of E{ Ln} for the plug-in rule based on your estimate of e. 

PROBLEM  16.7.  If Fe  = {all  uniform  densities  on rectangles  of Rd}  and  if we  use  the 
maximum  likelihood  estimates  of p, 80 , (h  derived in  the  text,  show  that E{ Ln}  - L *  = 
O(1/n). 

278 

16.  Parametric Classification 

PROBLEM  16.8.  CONTINUED.  Assume  that  the  true  class-conditional  densities  are  fa,  f1, 
and fa,  II ¢.  :Fe· With the same maximum likelihood estimates given above, find  fa,  f1  for 
which E{L n }  ~ 1, yet L * = O. 

PROBLEM  16.9.  CONTINUED.  Show that the  O(1/n) rate above can, in general, not be im(cid:173)
proved. 

PROBLEM  16.10.  Show that 

by noting that the supremum in the definition of D KS  may be replaced by a maximum over 
nd  carefully selected points of nd. HINT:  Use the idea of fingering from Chapters 4 and 12. 

PROBLEM  16.11.  Prove  Lemma  16.2  by  following  the  line  of the  proof of the  Vapnik(cid:173)
Chervonenkis inequality (Theorem 12.5). HINT:  In the second symmetrization step observe 
that 

has the same distribution as that of 

sup 1 ~ t  ai I{xi EA} I· 

AEB  n  i=l 

PROBLEM  16.12.  MINIMUM  DISTANCE  DENSITY  ESTIMATION.  Let:Fe  =  {fe  : e E  8} be  a 
parametric class of densities on Rd,  and assume that the i.i.d. sample Xl, ... , Xn  is drawn 
from the density  fe  E  :Fe. Define the class of sets 

and define the minimum distance estimate of e by 

(j = arg min D A(Pe"  fin), 

e'Ee 

where  Pe, is the distribution belonging to the density  fe',  fin  is  the empirical distribution 
defined by Xl, ... , X n ,  and  DAis the generalized Kolmogorov-Smimov distance defined 
by 

AEA 
Prove that if A has finite vc dimension, then 

DA(P,  Q) = sup IP(A) - Q(A)I. 

E {! Ifg-(x)  -

fe(X)ldX}  =  0  (l/Jn). 

For more information on minimum distance density  estimation,  see Yatracos  (1985)  and 
Devroye (1987). HINT:  Follow the steps listed below: 

(1)  f Ifg- -
fe I :s  2DA(Pg-,  Pe) (use Scheff6's theorem). 
(2)  DA(Pg-,  Pe) :s  2DA(Pe, fin) (proceed as in the text). 
(3)  Finish the proof by applying Alexander's improvement of the Vapnik-Chervonen(cid:173)

kis inequality (Theorem 12.10). 

17 
Generalized Linear Discrimination 

The classifiers we study here have their roots in the Fourier series estimate or other 
series estimates of an unknown density, potential function methods (see Chapter 
10),  and generalized  linear classifiers.  All  these  estimators  can  be  put into  the 
following form:  classify x  as belonging to class 0 if 

k L an,jo/j(x )  ::::  0, 

j=l 

where the 0/ j'S are fixed functions, forming a base for the series estimate, an,j is a 
fixed function of the training data, and k controls the amount of smoothing. When 
the  o//s are  the  usual  trigonometric  basis,  then  this  leads  to  the  Fourier  series 
classifier  studied  by  Greblicki  and  Pawlak  (1981;  1982).  When  the  o//s form 
an orthonormal system based upon Hermite polynomials, we obtain the classifiers 
studied by Greblicki (1981), and Greblicki and Pawlak (1983; 1985). When {o/ j (x)} 
is the collection of all products of components of x  (such as  1,  (x(i)k, (x(i)k(x(j)y, 
etcetera), we obtain the polynomial method of Specht (1971). 

We  start  with  classification  based on Fourier  series  expansion,  which has  its 
origins  in  Fourier  series  density  estimation,  which,  in  tum,  was  developed  by 
Cencov (1962), Schwartz (1967), Kronmal and Tarter (1968), Tarter and Kronmal 
(1970),  and Specht (1971).  Its  use in classification was  considered by Greblicki 
(1981), Specht (1971), Greblicki and Pawlak (1981;  1982;  1983), and others. 

280 

17.  Generalized Linear Discrimination 

17.1  Fourier Series Classification 
Let f  be the density of X, and assume that f  E  L2(A), that is,  f  ::::  0, f  f(x)dx = 1 
and f  f2(x )dx  <  00, and recall that A denotes the Lebesgue measure on nd. Let 
,,/II,  0/2, ... be  a  complete  orthonormal  set  of bounded  functions  in  L 2(A)  with 
sup j,x  100j(x)1  ::::  B  <  00. An orthonormal set of functions  {o/j}  in L 2(tL) is  such 
that f l/Jjo/i  =  IU==j}  for  all  i, j. It is  complete if f al/Ji  =  0 for  all  i  for  some 
function a  E  L 2(A)  implies that a  '=  0 almost everywhere (with respect to A). If 
f  E  L 2(A),  then the class-conditional densities  fo  and  fl  exist, and are in L2(A). 
Then the function 

a(x) = p/1(x) -

(1  - p)fo(x) = (217(X)  -

l)f(x) 

is in L 2(A)  as well. Recall that the Bayes decision is given by 

* 
g  (x) = 

{O 

if a(x) ::::  0 
1  otherwise. 

Let the bounded functions  l/Jl,  l/J2,  ... form a complete orthonormal basis, and let 
the Fourier representation of a  be given by 

00 

a  = Lajl/Jj. 

j=l 

The above convergence is understood in L 2(A), that is, the infinite sum means that 

The coefficients are given by 

We approximate a(x) by a truncation of its Fourier representation to finitely many 
terms,  and  use  the  data  Dn  to  estimate  the  coefficients  appearing  in  this  sum. 
Formally, consider the classification rule 

(17.1) 

where the estimates an,j  of a j  are very easy to compute: 

Before discussing the properties of these rules, we list a few examples of complete 
orthonormal  systems  on  the  real  line.  Some of these  systems  contain functions 

17.1  Fourier Series Classification 

281 

on a bounded interval. These, of course, can only be used if the distribution of X 
is  concentrated on an interval. The completeness of these systems may typically 
be  checked  via  the  Stone-Weierstrass  theorem  (Theorem  A.9).  A  general  way 
of producing  complete  orthonormal systems  on n d  is  taking  products  of one(cid:173)
dimensional  functions  of the  d  components,  as  sketched  in  Problem  17.1.  For 
more information on orthogonal series, we refer to Sansone (1969), Szeg6 (1959), 
and Zygmund (1959). 

(1)  The standard trigonometric basis on the interval [-Jr, Jr]  is  formed by the 

functions 

%(x) =  ~' o/2i-l(X) = 

1 

v~ 

cos(ix) 

r:;;'  o/2i(X) = 
vJr 

r:;;'  i  = 1,2, .... 
vJr 

sin(ix) 

This is a complete orthonormal system in L 2([ -Jr, n]). 

(2)  The Legendre polynomials form a complete orthonormal basis over the in(cid:173)

terval [-1, 1]: 

i) 
·x  =  - - - - x - I  
) ,   i  =0, 1,2, .... 

f!{Ii  + 1  1  d

i  (2 

2  2ii!dxi 

( 

o/l(  ) 

(3)  The set of Hermite functions  is  complete and orthonormal over the whole 

real line: 

(4)  Functions of the Laguerre basis are defined on [0, (0) by 

rei + 1) 
o/i(X) = 
. 
( 
r(z +a + 1) 

-ex  x)1/2  1  d i 
-:---.  x 
x  e 
z!  dx 1 

( 

i+ex  -X) 

e 

. 

,  z=0,1,2, ... , 

where a  >  -1 is a parameter of the system. A complete orthonormal system 
over the whole real line may be obtained by 

(5)  The Haar basis contains functions on [0,  1] that take three different values. 
It is  orthonormal and complete.  Functions with finitely  many values have 
computational advantages in pattern recognition. Write the integer i  ::::  0 as 
i  = 2k + j, where k = Llog2 i J (i.e., 0 ::::  j  <  2k). Then 

o/i(X) = 
{ 

2k/2 
- k/2 
2
0

if  x  E  ((j - 1)j2k, (j -
if  x  E  ((j -
otherwise. 

Ij2)j2k, jj2k) 

Ij2)j2k) 

282 

17.  Generalized Linear Discrimination 

(6)  Functions on [0,  1] of the Rademacher basis take only two values,  -1 and 

1. It is an orthonormal system, but it is not complete: 

1/Io(x)  ==  1,  1/Ii(X) = (_l) LixJ,  i = 1,2, .... 

The basis may be completed as  follows:  write i  = L~=1 2ij  such that 0  :.s 
il  <  i2  <  ... <  ik. This form is unique. Define 

where the 1/1 j  's are the Rademacher functions. The resulting basis 1/1~, 1/1{,  ... 
is orthonormal and complete, and is called the Walsh basis. 

There is no system of basis functions that is better than another for all distribu(cid:173)

tions. In selecting the basis of a Fourier series rule, the designer must use a mixture 
of intuition, error estimation, and computational concerns. We have the following 
consistency theorem: 

Theorem 17.1.  Let {1/Io,  1/11,  ... } be a complete orthonormal basis on Rd such that 
for some B  <  00,  !1/Ii (x)!  ::s  B for each i and x. Assume that the class-conditional 
densities  fo  and fl  exist and are in L2CA).  If 

kn  ~ 00  and  - ~ 0  as  n  ~ 00, 

kn 
n 

then the Fourier series classification rule defined in (17.1) is  consistent: 

lim  EL(gn) = L *. 
n---+oo 

If 

then 

kn ~ 00  and  - - - ~ 0  as  n  ~ 00, 

kn logn 

n 

lim  L(gn) = L * 

n---+oo 

with probability one,  that is,  the  rule is strongly consistent. 

PROOF. First observe that the an,) 's are unbiased estimates of the a j 's: 

E{an,j} 

E {(2Y - 1)1/Ij(X)}  = E {E {(2Y  - 1)1/Ij(X)!X}} 

E {1/Ij(X)E {(2Y  -

l)IX}} = E {(217(X)  - 1)1/Ij(X)} 

= 

j(217(X)  - 1)1/Ij(x)f(x)dx = j  1/I/x)a(x)dx 

and that their variance can be bounded from above as follows: 

17.1 Fourier Series Classification 

283 

n 

!1/I](x)(2ry(x) - 1)2 f(x)dx - Ct; 

n 

< 

J  <_, 

B2 

n 

B2  - Ct~ 

n 

where we used the boundedness of the l/J/s. By Parseval's identity, 

Therefore, exploiting orthonormality of the 1/1 j  's, we have 

J (a(x) - t a n,j1jfM) 2 dx 
=  J a

(x)dx+ J (tanAMr dx 

2

Thus, the expected L 2-error is bounded as follows: 

k" 
LVar{Ctn,j} +  L 
j=l 

(X) 

j=kn+l 

CtJ 

< 

284 

17.  Generalized Linear Discrimination 

Since L~l a;  <  00, the second term tends to zero if kn  ---+  00. If at the same time 

kn /  n  ---+  0,  then the expected L2-error converges to zero, that is,  the estimate is 
consistent in L 2• Now, convergence of the error probability follows from Problem 
2.11. 

To prove strong consistency (i.e., convergence with probability one), fix  E  >  0, 

and assume that n is so large that 

00 L a;  <  E/2. 

j=kn+l 

Then 

kn 

<  L P {(an,j  - aj)2  >  E/(2kn )} 

j=l 

(by the union bound) 

< 

where we  used Hoeffding's inequality (Theorem 8.1).  Because k n log n / n  ---+  0, 
the  upper bound is  eventually  smaller than  e-21ogn  =  n-2,  which is  summable. 
The Borel-Cantelli lemma yields strong L2 consistency. Strong consistency of the 
classifier then follows from Problem 2.11.  0 

REMARK. It is clear from the inequality 

that  the  rate  of convergence  is  determined  by  the  choice  of kn .  If kn  is  small, 
then  the  first  term,  which corresponds  to  the  estimation  error,  is  small,  but the 
approximation  error,  expressed  by  the  second term,  is  larger.  For large  kn'  the 

17.2 Generalized Linear Classification 

285 

situation is  reversed.  The  optimal choice of kn  depends  on how  fast  the  second 
term goes to zero as  kn  grows. This depends on other properties of f, such as the 
size of its tail and its smoothness.  0 

REMARK.  Consistency in Theorem  17.1  is  not  universal,  since  we  needed to  as(cid:173)
sume the existence of square integrable conditional densities. This, however, is not 
a restrictive assumption in some practical situations. For example, if the observa(cid:173)
tions are corrupted by additive gaussian noise, then the conditions of the theorem 
hold (Problem 17.2). However, if f1,  does  not have a density,  the method may be 
inconsistent (see Problem 17.4).  0 

Fourier series classifiers have two rather unattractive features in general: 

(i)  They are not invariant under translations of the coordinate space. 

(ii)  Most series classifiers are not local in nature-points at arbitrary distances 
from  x  affect  the  decision  at x.  In kernel  and nearest neighbor rules,  the 
locality is easily controlled. 

17.2  Generalized Linear Classification 

When X is purely atomic or singular continuous, Theorem 17.1  is not applicable. 
A theme of this book is that pattern recognition can be developed in a distribution(cid:173)
free manner since, after all, the distribution of (X, Y) is not known. Besides, even 
if we had an i.i.d. sample (X 1,  Y1),  .•• ,  (Xn ,  Yn )  at our disposal, we do not know of 
any test for verifying whether X has a square integrable density. So, we proceed a 
bit differently to develop universally consistent rules. To generalize Fourier series 
classifiers,  let  1/11,  1/12,  ... be bounded functions  on nd.  These  functions  do  not 
necessarily form an orthonormal basis of L2.  Consider the classifier 

where the coefficients an,) are some functions of the data Dn. This may be viewed 
as  a generalization of Fourier series rules,  where the coefficients were unbiased 
estimates ofthe Fourier coefficients of a(x) = (2rJ(x) -1)f(x). Here we will con(cid:173)
sider some other choices of the coefficients. Observe that gn is a generalized linear 
classifier, as defined in Chapter 13. An intuitively appealing way to determine the 
coefficients is  to pick them to minimize the empirical error committed on Dn,  as 
in  empirical  risk minimization.  The  critical parameter here is  kn ,  the  number of 
basis  functions  used  in  the  linear combination.  If we  keep  k n  fixed,  then  as  we 
saw in Chapter 13, the error probability of the selected rule converges quickly to 
that of the best classifier of the above form.  However, for some distributions, this 
infimum may be far from the Bayes risk, so it is useful to let kn  grow as n becomes 

286 

17.  Generalized Linear Discrimination 

larger.  However,  choosing kn  too  large may result in overfitting the  data.  Using 
the terminology introduced in Chapter 12, let C(kn )  be the class of classifiers of the 
form 

where the a j  's range through n. Choose the coefficients to minimize the empirical 
error 

Let gn  = ¢: be the corresponding classifier.  We  recall from Chapter  13  that the 
)  is not more than kn .  Therefore, by Theorem 13.12, for every 
vc dimension of C(kll
n  >  2kn  + 1, 

where H is the binary entropy function. The right-hand side is e-nE2 
j128+o(n) if kn  = 
o(n). However, to obtain consistency, we need to know how close inf</>Ec(knJ  L(¢) 
is  to  L *.  This  obviously  depends  on  the  choice  of the  1/1/ s,  as  well  as  on  the 
distribution. If kn is not allowed to grow with n, and is bounded by a number K, then 
It follows from Theorem 2.2 that for every B  > ° 
universal consistency eludes us, as for some distribution inf</>Ec(K)  L(¢) - L * >  0. 

inf  L(¢) - L * 
</>EC(kn ) 

< 

inf 

[ 

al,···,akn  1I!x,,:SB 

1(21J(X)  - 1) -

taj1/lj(X)1 p.,(dx) +  [ 
j=l 

1I!X">B 

p.,(dx). 

The limit supremum of the right-hand side can be arbitrarily close to zero for all 
distributions if kn  ~ 00 as n  ~ 00 and the set of functions 

is  dense in Ll (p.,)  on balls of the form  {x  :  Ilx II  :::  B}  for all p.,.  This means that 
given any  probability measure p."  and function  f  with J Ifldp.,  <  00, for every 
E  >  0, there exists an integer k and coefficients aI, ... ,  ak  such that 

Thus, we have obtained the following consistency result: 

Problems and Exercises 

287 

Theorem 17.2.  Let 0/1, 0/2, ... be  a  sequence  of functions  such  that  the  set  of 
all finite  linear combinations of the  0/ j  's is  dense  in  L 1 (jL)  on  balls  of the form 
{x  : Ilx II  s B} for any probability measure jL.  Then then gn  is strongly universally 
consistent when 

kn  ---+  00  and  -

---+ ° as  n  ---+  00. 

kn 
n 

REMARK. To see that the statement of Theorem 17.2 is not vacuous, note that dense(cid:173)
ness in L 1 (jL) on balls follows from denseness with respect to the supremum norm 
on balls. Denseness in the Loo  sense may be checked by the Stone-Weierstrass the(cid:173)
orem (Theorem A.9). For example, the class of all polynomial classifiers satisfies 
the conditions of the theorem.  This  class  can be obtained by choosing the func(cid:173)
tions  0/1,  0/2,  ... as  the simple polynomials x(l), ... , x Cd ), xCI) x(2),  .... Similarly, 
the functions  o/j  can be chosen as bases of a trigonometric series.  0 

REMARK. THE HISTOGRAM RULE. It is clear that Theorem 17.2 can be modified in the 
following way: let 0/ j,b j  = 1,  ... , k; k = 1, 2,  ... , be functions such that for every 
fELl (f-L)  (with f-L  concentrated on a bounded ball) and E  > ° there is a ko  such 

that for all k  >  ko  there is a function of the form L~=l a j 0/ j,k whose distance from 
f  in L  1 (f-L)  is smaller than E. Let C(kl1
)  be the class of generalized linear classifiers 
based  on the  functions  o/l,kn ,  ••• ,  o/k",kn •  If k n 
---+  00  and  kn /  n  ---+  0,  then  the 
classifier gn  that minimizes  the empirical error over C(kn )  is  strongly universally 
consistent. This modification has an interesting implication. Consider for example 
functions  o/l,kn ,  ••• ,  o/k" ,k"  that are indicators of sets of a partition of nd. Then it is 
easy to see that the classifier that minimizes the empirical error is just the histogram 
classifier based on the same partition. The denseness assumption requires that the 
partition becomes  infinitesimally  fine  as  n  ---+  00.  In fact,  we have  obtained an 
alternative proof for the strong universal consistency of the regular histogram rule 
(Theorem 9.4). The details are left as an exercise (Problem 17.3).  0 

PROBLEM  17.l.  Let Yrl,  Yr2,  ... be a sequence of real-valued functions  on  a bounded in(cid:173)

Problems and Exercises 
terval  [a, b]  such that f: Yri(x)Yr}(x)dx  =  I{i=}},  and the  set of finite  linear combinations 
2:=;=1  ai Yri (x)  is  dense  in  the  class  of continuous  functions  on  [a, b]  with  respect  to  the 
supremum norm. Define the functions  \{Iil, .. "id  :  [a, b]d  -+ n by 

Show that these functions form a complete orthonormal set of functions on  [a, b]d. 

PROBLEM  17.2.  Let  Z  be  an  arbitrary  random  variable  on n,  and  V  be  a  real  random 
variable, independent of Z, that has a density  h  E  L2('A).  Show that the density  f  of the 

288 

17.  Generalized Linear Discrimination 

random variable X  =  Z + V  exists,  and f f2(X)dx  <  00. HINT:  Use Jensen's inequality to 
prove that f  f2(X)dx  :s f h2(x)dx. 

PROBLEM 17.3.  Derive the strong universal consistency of the regular histogram rule from 
Theorem 17.2, as  indicated in the remark following it. 

PROBLEM 17.4.  Let {1fo,  1f1'  1f2' ... } be the standard trigonometric basis, and consider the 
classification rule defined in  (17.1).  Show that the rule is not consistent for  the following 
distribution:  given  Y = l, let X be  0,  and given Y = 0,  let X be uniformly distributed on 
[-JT, JT].  Assume furthermore that pry = 1}  = 1/2. 

PROBLEM 17.5.  The Haar basis  is  not bounded.  Determine  whether or not  the  Laguerre, 
Hermite, and Legendre bases are bounded. 

18 
Complexity Regularization 

This  chapter offers  key  theoretical  results  that  confirm  the  existence  of certain 
"good" rules. Although the proofs are constructive-we do tell you how you may 
design such rules-the computational requirements are often prohibitive. Many of 
these rules are thus not likely to filter down to the software packages and pattern 
recognition implementations. An attempt at reducing the computational complex(cid:173)
ity  somewhat  is  described  in  the  section  entitled  "Simple  empirical  covering." 
Nevertheless, we feel that much more serious work on discovering practical algo(cid:173)
rithms for empirical risk minimization is  sorely needed. 

In  Chapter  12,  the  empirical  error  probability  was  minimized  over  a  class e 
of decision rules  ¢  : n d  ~ {O,  I}.  We  saw  that if the  vc dimension  Vc  of the 
class  is  finite,  then  the  error probability  of the  selected  rule  is  within  constant 
times  -/Vc log n / n  of that of the best rule in e.  Unfortunately, classes with finite 
vc dimension are  nearly  always  too  small,  and thus  the  error probability  of the 
best rule  in e is  typically  far  from  the  Bayes risk  L * for  some distribution  (see 
Theorem  18.4).  In Chapter  17  we investigated the  special classes  of generalized 
linear rules. Theorem 17.2, for example, shows that if we increase the size of the 
class in a controlled fashion  as  the sample size n  increases, the error probability 
of the selected rule approaches L * for any distribution. Thus,  Vc  increases with n! 
Theorem 17.2 may be generalized straightforwardly for other types of classifiers. 
Consider  a  sequence  of classes  e(l), e(2),  ... ,  containing  classifiers  of the  form 
¢  : n d  ~ {O,  I}.  The  training  data  Dn  = ((Xl, Yd, ... ,(Xn, Yn))  are  used to 
select a classifier ¢~ by minimizing the empirical error probability Ln (¢) over the 
class  e(kn ),  where the integer kn  depends on n  in a specified way.  The following 
generalization is based on the proof of Theorem 17.2 (see Problem 18.1): 

290 

18.  Complexity Regularization 

Theorem 18.1.  Assume thate(l), e(2),  ... is a sequence of classes of decision rules 
such that for any distribution of (X, Y), 

lim  inf  L(¢) = L *, 
i--+oo <jJECCi) 

and that the vc dimensions  VCC]),  VC(2)  ,  ••• are all finite.  If 

kn  -+  00  and 

VC(kn )  log n 
- - - - -+ 0  as n  -+  00, 

n 

then  the  classifier ¢~ that minimizes  the  empirical error  over the  class  e Ckn )  is 
strongly universally consistent. 

Theorem 18.1 is the missing link-we are now ready to apply the rich theory of 
Vapnik -Chervonenkis classes in the construction of universally consistent rules. 
The  theorem  does  not  help  us,  however,  with  the  choice  of the  classes eCO ,  or 
the  choice  of the  sequence  {kn }.  Examples  for  sequences  of classes  satisfying 
the condition of Theorem 18.1 include generalized linear decisions with properly 
chosen basis functions  (Chapter 17), or neural networks (Chapter 30). 

A word of warning here. The universally consistent rules obtained via Theorem 
18.1  may come at a tremendous  computational price.  As we will  see further on, 
we will often have exponential complexities in n instead of polynomial time that 
would be obtained if we just minimized the empirical risk over a fixed vc class. 
The computational complexity of these rules are often incomparably larger than 
that of some simple universally consistent rules as  k-nearest neighbor, kernel, or 
partitioning rules. 

18.1  Structural Risk Minimization 

Let eCl), e(2),  •.. be  a sequence  of classes of classifiers, from which we wish to 
select a sequence of classifiers with the help of the training data Dn. Previously, we 
picked ¢l: from the kn -th class eCkn ), where the integer kn  is some prespecified func(cid:173)
tion of the sample size n only. The integer kn  basically determines the complexity 
of the  class  from  which the  decision rule  is  selected.  Theorem  18.1  shows  that 
under mild conditions on the sequence of classes, it is possible to find sequences 
{kn } such that the rule is universally consistent. Typically, kn  should grow with n in 
order to assure convergence of the approximation error inf<jJEc(knJ  L(¢) - L *, but it 
cannot grow too rapidly, for otherwise the estimation error L (¢~) - inf <jJEC(kn J L (¢ ) 
might fail  to converge to zero.  Ideally, to get best performance,  the two types of 
error should be about the same order of magnitude. Clearly, a pre specified choice 
of the complexity kn  cannot balance the two sides of the trade-off for all distribu(cid:173)
tions. Therefore, it is important to find methods such that the classifier is selected 
from a class whose index is automatically determined by the data Dn. This section 
deals with such methods. 

18.1  Structural Risk Minimization 

291 

The most obvious method would be based on selecting a candidate decision rule 
¢n,)  from  every class C(j)  (for example,  by minimizing the empirical error over 
C(j)), and then minimizing the empirical error over these rules. However, typically, 
the vc dimension of 

00 

C*  = UC(J) 

equals infinity, which, in view of results in Chapter 14, implies that this approach 
will not work. In fact,  in order to guarantee 

)=1 

it is necessary that 

inf  inf  L(¢) =  L*, 
)~1 <jJEC(j) 

VC*  = 00 

(see Theorems  18.4 and 18.5). 

A possible solution for the problem is a method proposed by Vapnik and Cher(cid:173)

vonenkis (1974c) and Vapnik (1982), called structural risk minimization. First we 
select a classifier 1n,)  from every class C(j)  which minimizes the empirical error 
over the class. Then we know from Theorem 12.5 that for every j, with very large 
probability, 

vcU)~ogn) . 

Now, pick a classifier that minimizes the upper bound over j  ::::  1. To make things 
more precise, for every nand j, we introduce the complexity penalty 

r(j, n) = 

32 
-
n 

VC(j)  log(en). 

Let  ¢n,l, 1n,2,  .,. be  classifiers  minimizing  the  empirical  error  Ln(¢)  over  the 
classes C(l), C(2),  ... , respectively. For ¢  E  C(j),  define the complexity-penalized 
error estimate 

-
Ln(¢) = Ln(¢) + r(j, n). 

----

Finally, select the classifier ¢I~ minimizing the complexity penalized error estimate 
in(¢n,)) over j  ::::  1. The nexttheorem states thatthis method avoids overfitting the 
data. The only condition is that each class in the sequence has finite vc dimension. 

Theorem 18.2.  Let C(l), C(2),  ... be a sequence of classes of classifiers such that 
for any distribution of (X, Y), 

lim 
)---'?OO <jJEC(j) 

inf  L (¢) =  L * . 

Assume also that the vc dimensions  VC(!),  VC(2)  ,  ••• are finite and satisfy 

00 

.6.  = L e-vc(j)  <  00. 

)=1 

292 

18.  ~omplexity Regularization 

Then  the  classification rule  cp~ based on structural risk minimization, as defined 
above, is strongly universally consistent. 

REMARK.  Note that the condition on the vc dimensions is satisfied if we insist that 
VC(j+I)  >  VC(j)  for all  j, a natural assumption. See also Problem 18.3.  0 

n 

Instead of minimizing the empirical error Ln (cp) over the set of candidates C* , the 
method of structural risk minimization minimizes in (cp),  the sum of the empirical 
error,  and  a term  ~ VC(j)  log(en),  which  increases  as  the  vc dimension of the 
class C(j)  containing cp  increases.  Since classes with larger vc dimension can be 
considered as more complex than those with smaller vc dimension, the term added 
to the empirical error may be considered as a penalty for complexity. The idea of 
minimizing the sum of the empirical error and a term penalizing the complexity has 
been investigated in various statistical problems by, for example, Rissanen (1983), 
Akaike  (1974),  Barron (1985;  1991), Barron and Cover (1991),  and Lugosi and 
Zeger (1996). Barron (1991) minimizes the penalized empirical risk over a suitably 
chosen countably infinite list of candidates. This approach is close in spirit to the 
skeleton  estimates  discussed  in  Chapter 28.  He  makes  the  connection  between 
structural  risk  minimization  and  the minimum  description  length  principle,  and 
obtains results similar to those discussed in this section. The theorems presented 
here were essentially developed in Lugosi and Zeger (1996). 

PROOF OF THEOREM  18.2.  We show that both terms on the right-hand side of the 
following decomposition converge to zero with probability one: 

For the first term we have 

P {L(¢Z) - %\ i.(<P",j)  >  E } 
P {L(CP~) - Ln(CP,~) >  E} 

<  P {~up (L(¢n,j) - in(¢ll.j»)  >  E} 

}~l 

00 

<  L P {IL(¢n,j) - Ln(¢n.j)1  >  E + r(j, n)} 
<  L 8n Vc(j) e-n(Hru.1l)f/32 

(by Theorem 12.5) 

j=l 

00 

j=l 

18.1  Structural Risk Minimization 

293 

<  L 8n vc(j) e-nr2(j,n)/32e-nE2/32 

00 

j=l 

00 

2 /32 L e -Vc(j)  = f::..e ~nE2 /32 

8e -nE

j=l 

using  the  defining  expression  for  r(j, n),  where  f::..  = 8 L~l e- vC(j)  <  00,  by 
assumption. Thus, it follows from the Borel-Cantelli lemma that 

L(</>~) - ~nfl in(¢n,j) ~ ° 

J?:. 

with probability one as n  ~ 00. Now, we can tum to investigating the second term 
infF:::l  inC¢n,j) - L *. By our assumptions, for every E  >  0, there exists an integer 
k such that 

inf  LC </»  - L * ::s  E. 
<pEC(k) 

Fix such a k. Thus, it suffices to prove that 

lim sup inf inC¢n,j) -
n-+oo 

j?:.l 

<pEC(k) 

inf  LC</»::s ° with probability one. 

Clearly, for any fixed k, if n is large enough, then 

rCk,  n) = 

32 
E 
-;; VC(k)  logC en) ::s  2' 

Thus, for such large n, 

P  {i.nf inC¢n,j) -

J?:.I 

inf  LC</»  >  E} 

<pECW 

inf  LC</»  >  E} 

<  P  {inC¢n,k) -
=  P {L n C ¢n, k) + r C k, n) -

<pEC(k) 

inf  L C</»  >  E} 

<pEC(k) 

<  P  {LnC¢n,k)  -

inf  LC</»  >  ~} 
2 
<  p{  sup  ILn(¢)-LC¢)1  >~} 

<pEC(k) 

<pEC(k) 

2 

< 

8n VC(k) e- nE2 /128 

by Theorem 12.5. Therefore, since  VC(k)  <  00, the proof is completed.  0 

Theorem 18.2 shows that the method of structural risk minimization is univer(cid:173)

sally consistent under very mild conditions on the sequence of classes e(l), e(2),  .... 

294 

18.  Complexity Regularization 

This property, however, is shared with the minimizer of the empirical error over the 
), where kn  is a properly chosen function of the sample size n  (Theorem 
class C(kn
18.1). The real strength, then, of structural risk minimization is seen from the next 
result. 

Theorem 18.3.  Let C(l), C(2),  ... be a sequence of classes of classifiers such that 
the vc dimensions  VC(l),  V C(2)  ,  ••• are all finite.  Assume further that the Bayes rule 

00 

g*  E  C*  = UC(j), 

j=l 

that is,  a Bayes rule is contained in one of the classes. Let k be the smallest integer 

such that g*  E  C(k).  Thenfor every nand E  > ° satisfying 

the error probability of the classification rule based on structural risk minimization 
¢~ satisfies 

PROOF. Theorem 18.3 follows by examining the proof of Theorem 18.2. The only 
difference is that by assumption, there is an integer k such that inf rjJEC(k)  L( ¢) = L *. 
Therefore, for this k, 

L(¢~) - L*  = (L(¢~) - ~nf in(¢n,j)) + (~nf in(¢n,j) - inf  L(¢)) , 

12.1 

12.1 

rjJEC(k) 

and the two terms on the right-hand side may be bounded as in the proof of Theorem 
18.2.0 

Theorem 18.3 implies that if g*  E  C*, there is a universal constant Co  and a finite 

k such that 

VC(k)  logn 

n 

that is, the rate of convergence is always of the order of Jlog n / n, and the constant 
factor  VC(k)  depends  on the distribution.  The number  VC(k)  may be viewed as  the 
inherent complexity of the Bayes rule for the distribution. The intuition is that the 
simplest rules are contained in C(l), and more and more complex rules are added to 
the class as the index of the class increases. The size of the error is about the same 
as if we had known k  beforehand, and minimized the empirical error over C(k). In 
view of the results of Chapter 14 it is clear that the classifier described in Theorem 
18.1 does not share this property, since if L * >  0, then the error probability of the 

IS.1  Structural Risk Minimization 

295 

rule  selected from  C(kn )  is  larger than a constant times  JVC(kn )  log n/ n for  some 
distribution-even if g*  E  C(k)  for  some fixed  k.  Just as  in designs  based upon 
minimum  description  length,  automatic  model  selection,  and  other  complexity 
regularization  methods  (Rissanen (1983),  Akaike  (1974),  Barron (1985;  1991), 
and  Barron  and Cover  (1991)),  structural risk minimization automatically  finds 
where to look for the optimal classifier. The constants appearing in Theorem 18.3 
may be improved by using refined versions of the Vapnik -Chervonenkis inequality. 
The condition g*  E  C*  in Theorem 18.3 is not very severe, as C*  can be a large 
class with infinite vc dimension. The only requirement is that it should be written 
as a countable union of classes of finite vc dimension. Note however that the class 
of all decision rules can not be decomposed as  such (see Theorem 18.6). We also 
emphasize that in order to achieve the  0  ( Jlog n / n )  rate of convergence, we do 
not have to assume that the distribution is a member of a known finite-dimensional 
parametric family  (see Chapter 16). The condition is imposed solely on the form 
of the Bayes classifier g*. 

By inspecting the proof of Theorem 18.2 we see that for every k, 

EL(¢~) - L* ::;  Co 

VC(k)  log n  + (  inf  L( ¢) _  L *) . 

n 

¢EC(k) 

In  fact,  Theorem  18.3  is  the  consequence  of this  inequality.  The  first  term  on 
the right-hand side, which may be called estimation error, usually increases with 
growing  k,  while the second, the approximation error,  usually  decreases with it. 
Importantly, the above inequality is true for every k,  so that 

VC(k)  logn  + (  inf  L(¢) - L *)) . 

¢EC(k) 

n 

Thus structural risk minimization finds  a nearly optimal balance between the two 
terms.  See also Problem 18.6. 

REMARK.  It is worthwhile mentioning that under the conditions of Theorem 18.3, 
an even faster,  0  ( -JI711), rate of convergence is achievable at the expense of mag(cid:173)
nifying the constant factor.  More precisely, it is possible to define the complexity 
penalties r(j, n) such that the resulting classifier satisfies 

EL(A.*) - L* <  ~ 
-.jn' 

'f'n 

where the constant Cl  depends on the distribution. These penalties may be defined 
by exploiting Alexander's inequality (Theorem  12.10),  and the inequality above 
can be proved by using the bound in Problem 12.10, see Problem 18.6.  0 

REMARK.  We see from the proof of Theorem 18.2 that 

p  {L(¢~) >  in(¢~) + E}  ::::  6.e-nE2 

/32. 

296 

18.  Complexity Regularization 

This means that in(¢~) does not underestimate the true error probability of ¢~ by 
much. This is  a very attractive feature of in as  an error estimate, as  the designer 
can be confident about the performance of the rule ¢~. 0 

The initial  excitement over a  consistent rule  with  a guaranteed  0 (.jlog n / n) 

rate of convergence to the Bayes error is tempered by a few  sobering facts: 

(1)  The  user  needs  to  choose  the  sequence  Cm and  has  to  know  Vcw  (see, 
however, Problems 18.3, 18.4, and the method of simple empirical covering 
below). 

(2)  It is difficult to find the structural risk minimizer ¢/: efficiently. After all, the 

minimization is done over an infinite sequence of infinite sets. 

The  second  concern  above  deserves  more  attention.  Consider  the  following 

simple example: let d  = 1, and let C(j) be the class of classifiers ¢  for which 

{x:  ¢(x) = 1} = UA i , 

j 

i=l 

where each Ai  is an interval oCR. Then, from Theorem l3.7, Vcw  =  2j, and we 
may take 

r(j, n) = 

64j 
-log(en) . 
n 

In  structural risk minimization,  we  find  those  j  (possibly  empty)  intervals  that 
minimize 

Ln(¢) + r(j, n), 

and call the corresponding classifier ¢n,j' For j  = 1 we have r(j, n) =  ~ loge en). 
As r(n, n)  >  1 + r(l, n), and r(j, n) is monotone in j, it is easily seen that to pick 
the  best  j  as  well, we  need only consider  1  ::::  j  ::::  n.  Still,  this  is  a formidable 
exercise. For fixed j, the best j  intervals may be found by considering all possible 
insertions  of 2j interval boundaries  among the n  X/so  This brute force  method 
takes computation time bounded from below by (n;~j). If we let j  run up to n, then 
we have as a lower bound 

Just the last term alone,  G~), grows as a Ul r, and is prohibitively large for 

n  ::::  20.  Fortunately,  in this  particular case,  there is  an algorithm which  finds  a 
classifier minimizing Ln (¢) + r(j, n) over C*  =  U~l C(j)  in computational time 
polynomial  in  n,  see  Problem  18.5.  Another  example  when  the  structural  risk 
minimizer  ¢~ is  easy  to  find  is  described  in  Problem  18.6.  However,  such  fast 
algorithms  are  not  always  available,  and exponential running time prohibits the 
use of structural risk minimization even for relatively small values of n. 

t (n +~j). 

j=l 

2] 

18.3 Simple Empirical Covering 

297 

18.2  Poor Approximation Properties of VC Classes 

We  pause  here  for  a  moment  to  summarize  some  interesting  by-products  that 
readily follow from Theorem 18.3 and the slow convergence results of Chapter 7. 
Theorem 18.3 states that for a large class of distributions an 0  (/log n / n )  rate of 
convergence to the Bayes error L * is achievable. On the other hand, Theorem 7.2 
asserts that no universal rates of convergence to  L * exist. Therefore, the class of 
distributions for which Theorem 18.3 is  valid cannot be that large,  after all.  The 
combination of these facts results in the following three theorems, which say that vc 
classes-classes of subsets of n d  with finite vc dimension-have necessarily very 
poor approximation properties. The proofs are left to the reader as easy exercises 
(see Problems 18.7 to  18.9). For direct proofs, see, for example, Benedek and Itai 
(1994). 

Theorem 18.4.  LetC be any class of classifiers of the fo rm ¢  : nd --+  {a,  I},  with 

vc dimension  Vc  <  00.  Then for every E  > ° there exists a distribution such that 

. 
1 
mf L( ¢) - L  >  -
2 
¢EC 

* 

- E. 

Theorem 18.5.  Let C(1),  C(2),  ... be  a  sequence  of classifiers  such  that  the  vc 
dimensions  VC(l) ,  VC(2) ,  ... are  all finite.  Then for  any  sequence  {ak}  of positive 
numbers converging to zero arbitrarily slowly, there exists a distribution such that 
for every k large enough, 

inf  L (¢) - L * >  ak. 

¢EC(k) 

Theorem 18.6.  The  class  C*  of all  (Borel  measurable)  decision  rules  of form 
¢  : Rd  --+  {a,  I}  cannot be written as 

00 

C*  = U cen, 

j=l 

where the vc dimension of each class C(j)  is finite.  In  other words,  the class B  of 
all Borel subsets ofnd cannot be written as a union of countably many vc classes. 
In fact,  the same is true for the class of all subsets of the set of positive integers. 

18.3  Simple Empirical Covering 

As Theorem 18.2 shows, the method of structural risk minimization provides au(cid:173)
tomatic protection against the danger of overfitting the data, by penalizing com(cid:173)
plex  candidate  classifiers.  One  of the  disadvantages  of the  method  is  that  the 
penalty terms  r(j, n) require knowledge of the vc dimensions of the classes C(j) 
or  upper  bounds  of these  dimensions.  Next  we  discuss  a  method  proposed  by 

298 

18.  Complexity Regularization 

Buescher and  Kumar  (1996b)  which is  applicable  even if the  vc dimensions  of 
the  classes  are  completely unknown.  The  method,  called simple  empirical cov~ 
ering,  is  closely  related  to  the  method  based  on  empirical  covering  studied  in 
Problem  12.14. In simple empirical covering, we first split the data sequence  Dn 
into two parts. The first part is  Dm  =  ((Xl, Yd, ... , (Xm,  Ym»  and the second part 
is  'It  =  (Xm+l' Ym+1), ••• ,  (Xn,  Yn».  The positive integers m  and I = n  - m  will 
be specified later. The first part Dm  is used to cover C*  = U7=1 Cen as follows. For 
every ¢  E  C*, define the binary m-vector bm (¢) by 

There are N  ::::  2m  different values of bm (¢ ). Usually, as  Ve*  = 00, N  = 2m
, that is, 
all possible values of bm (¢) occur as ¢  is varied over C*, but of course, N  depends 
on the values of X I,  ... ,  X m. We call a classifier ¢  simpler than another classifier 
¢/, if the smallest index i such that ¢  E  C(i) is smaller than or equal to the -smallest 
index  j  such that ¢'  E  C(j).  For every binary m-vector b  E  {O,  l}m  that can be 
written as  b  =  bm(¢) for  some  ¢  E  C*,  we pick a candidate classifier ;Pm.k  with 
k  E  {l, ... , N} such that bm(;Pm,k)  =  b, and it is the simplest such classifier, that 
is, there is no ¢  E  C*  such that simultaneously bm(;Pm,k)  =  b(¢) and ¢  is  simpler 
than ¢m.k. This yields N  ::::  2m candidates ;Pm,l,  ... , ;Pm.N' Among these, we select 
one that minimizes the empirical error 

measured on the independent testing sequence 'It. Denote the selected classifier by 
¢:. The next theorem asserts that the method works under circumstances similar 
to structural risk minimization. 

Theorem 18.7.  (BUESCHER  AND  KUMAR  (l996B».  Let C(l)  ~ C(2)  ~ '"  be a 
nested sequence of classes of classifiers such that for any distribution of (X , Y), 

lim 
j~OO¢Ee(j) 

inf  L (¢) = L * . 

Assume also that the vc dimensions Ve(l) , Ve(2),  ... are all finite.  If m I log n  -+  00 
and min -+ 0, then the classification rule ¢: based on simple empirical covering 

as defined above is strongly universally consistent. 

PROOF. We decompose the difference between the error probability of the selected 
rule ¢,~ and the Bayes risk as follows: 

The first term can be handled by Lemma 8.2 and Hoeffding's inequality: 

18.3 Simple Empirical Covering 

299 

P {L(ep:)  -

inf  L(¢m,k) >  E} 

l-Sk-SN 

<  P {2  sup  IL(¢m,k) 
E {p {2 1~~;JL(1m'k) - [,(1m,k)i  > ,I Dm}} 

LZ(¢m,k)1  >  E} 

l-Sk-SN 

<  E { 2N e -ZE2 /2 } 

<  2m+le- ZE2 / 2  = 2m+1e-(n-m)E 2 /2. 

Because m  =  o(n), the latter expression converges to zero exponentially rapidly. 
Thus, it remains to show that 

inf  L(¢m,k) - L * -+ ° 

l-Sk-sN 

with probability one.  By our assumptions, for every  E  >  0,  there is  an integer k 
such that 

inf  L(ep)  - L*  <  E. 
¢EC(k) 

Then there exists a classifier epeE)  E  e(k)  with L(ep(E))  - L * :::s  E. Therefore, we are 
done if we prove that 

lim sup  inf  L(¢m,k) - L(ep(E))  :::: ° with probability one. 

n-HX! 

l-Sk-SN 

Clearly, there is a classifier ¢m,j  among the candidates ¢m,l, ... , ¢m,N, such that 

Since  by  definition,  ¢m,j  is  simpler  than  ep(E),  and  the  classes  e(l), e(2),  ... are 
nested, it follows that ¢m,j  E  e(k). Therefore, 

< 

sup 

IL(ep) 

L(ep') I, 

¢,¢'EC(k):bm(¢)=bm(¢') 

where the last supremum is taken over all pairs of classifiers such that their corre(cid:173)
sponding binary vectors bm (ep ) and bm (ep')  are equal. But from Problem 12.14, 

300 

18.  G:omplexity Regularization 

which is summable if m/ log n  -+ 00.0 

REMARK. As in Theorem 18.3, we may assume again that there is an integer k such 
that inf</>Eok)  L(¢) = L *. Then from the proof of the theorem above we see that the 
error probability of the classifier ¢~, obtained by the method of simple empirical 
covering, satisfies 

P {L(¢/:) - L *  >  E}  ::s  2 m+1 e-(n-m)E 2

/8  + 2S4 (C(k) , 2m)e-mdog2/8. 

Unfortunately, for any choice of m, this bound is much larger than the analogous 
bound obtained for  structural risk minimization.  In  particular,  it does  not yield 
an  OCjlogn/n) rate of convergence. Thus, it appears that the price paid for the 
advantages  of simple  empirical  covering-no knowledge  of the  vc dimensions 
is required, and the implementation may require significantly less computational 
time in general-is a slower rate of convergence. See Problem 18.10.  0 

Problems and Exercises 

PROBLEM  18.1.  Prove Theorem 18.1. 

PROBLEM  18.2.  Define  the  complexity  penalties  r(j, n)  so  that  under  the  conditions  of 
Theorem 18.3, the classification rule ¢1~ based upon structural risk minimization satisfies 

where the constant Cl  depends on the distribution. HINT: Use Alexander's bound (Theorem 
12.10), and the inequality of Problem 12.10. 

PROBLEM  18.3.  Let C(l), C(2),  ... be  a sequence of classes of decision rules  with finite vc 
:::  VeUJ  on the vc dimensions  are known. 
dimensions. Assume that only upper bounds a j 
Define the complexity penalties by 

r(j, n) =  32 
-(Xj log(en). 
n 

Show that if L~I e-CXj  <  00, then Theorems 18.2 and 18.3 carryover to the classifier based 
on structural risk minimization defined by these penalties. This points out that knowledge 
of relatively rough estimates of the vc dimensions suffice. 

PROBLEM  18.4.  Let CCI),  C(2),  ... be  a  sequence  of classes  of classifiers  such  that the vc 
dimensions  Ve(l),  Ve(2),  •••  are all  finite.  Assume furthermore  that  the  Bayes  rule is  con(cid:173)
tained in one of the classes and that L * = O.  Let ¢1:  be the rule obtained by structural risk 
minimization using the positive penalties r(j, n), satisfying 

Problems and Exercises 

301 

(l)  For each n, r(j, n) is strictly monotone increasing in j. 
(2)  For each j, limn-+oo r(j, n) =  0. 

Show  that  E{L(¢:)}  =  O(logn/n)  (Lugosi  and  Zeger  (1996».  For  related  work,  see 
Benedek and Itai (1994). 
PROBLEM  18.5.  Let C(j) be the class of classifiers ¢  : n d  ~ to,  I}  satisfying 

{x  : ¢(x) =  I}  = U Ai, 

j 

i=l 

where the Ai'S are bounded intervals in n. The purpose of this exercise is to point out that 
there is a fast algorithm to find the structural risk minimizer ¢: over C*  =  U~l Cen, that is, 
which minimizes in  =  Ln  + r(j, n) over C*,  where the penalties r(j, n) are defined as in 
Theorems  18.2 and 18.3. The property below was pointed out to us by Miklos Csuros and 
R6ka Szabo. 

(1)  Let AL, ... , Aj,}  be the  j  intervals  defining the classifier (fin,)  minimizing Ln 
over C(J).  Show that the optimal intervals for  C(J+l),  Ar,}+l'  ... ,  A j+l,}+l  satisfy 
the following property:  either j  of the intervals coincide with A L, ... , A j,}, or 
1  of them  are  among  A r,}, ... , A j,},  and  the  remaining  two  intervals  are 
j 
subsets of the j -th interval. 
(2)  Use property (1) to define an algorithm that finds ¢: in running time polynomial 

in the sample size n. 

PROBLEM  18.6.  Assume that the  distribution  of X  is  concentrated on the  unit cube,  that 
IS,  P{X  E  [0,  l]d}  =  1.  Let p} be a partition of [0,  l]d  into cubes of size  1/ j, that is,  p} 
contains /  cubic cells. Let C(j) be the class of all histogram classifiers ¢  : [0,  l]d  ~ to,  I} 
based on p}. In other words, p} contains all 2}d  classifiers which assign the same label to 
points falling in the same cell of p}. What is the vc dimension Ve(j)  of Cen? Point out that 
the classifier minimizing Ln  over C(j)  is just the regular histogram rule based on p}. (See 
Chapter 9.) Thus, we have another example in which the empirically optimal classifier is 
computationally inexpensive. The structural risk minimizer ¢; based on C*  =  U~l C(j)  is 
also easy to find. Assume that the a posteriori probability 'fl(x) is uniformly Lipschitz, that 
is, for any x, y  E  [0,  l]d, 

!'fl(x) -

'fl(y)!  ::::  cllx - y!1, 

where c is some constant. Find upper bounds for the rate of convergence of EL(¢;) to L *. 

PROBLEM  18.7.  Prove Theorem 18.4. HINT:  Use Theorem 7.1. 

PROBLEM  18.8.  Prove Theorem 18.5. HINT:  Use Theorem 7.2. 

PROBLEM  18.9.  Prove Theorem 18.6. HINT: Use Theorems 7.2 and 18.3. 
PROBLEM  18.10.  Assume  that the  Bayes  rule  g*  is  contained  in C*  =  U~l C(J).  Let ¢; 
be the classifier obtained by simple empirical covering. Determine the value of the design 
parameter m  that minimizes the bounds obtained in the proof of Theorem  18.7. Obtain a 
tight upper bound for EL(¢:) - L *. Compare your results with Theorem 18.3. 

19 
Condensed and Edited 
Nearest Neighbor Rules 

19.1  Condensed Nearest Neighbor Rules 

Condensing is the process by which we eliminate data points, yet keep the same 
behavior.  For  example,  in  the  nearest  neighbor  rule,  by  condensing  we  might 
mean the reduction of (X 1, Y1),  ••• , (Xn,  Yn) to (X~, Y{),  ... , (X~, Y~l) such that 
for  all  x  E  n d ,  the  l-NN  rule  is  identical based on the  two  samples.  This  will 
be  called  pure  condensing.  This  operation  has  no  effect  on  L n ,  and  therefore 
is  recommended whenever space is  at a  premium.  The  space  savings  should be 
substantial whenever the classes are separated. Unfortunately, pure condensing is 
computationally expensive, and offers no hope of improving upon the performance 
of the ordinary  l-NN rule. 

o 

• 

o 

• 

class  1 
under I-NN rule 

FIGURE  19.1.  Pure  condensing: 
Eliminating  the  marked  points 
does not change the decision . 

class 0 
under 1-NN rule 

• 

• 

o 

304 

19.  G:ondensed and Edited Nearest Neighbor Rules 

Hart (1968) has the first simple algorithm for condensing. He picks a subset that 
correctly classifies the remaining data by the  I-NN  rule.  Finding a minimal such 
subset is computationally difficult, but heuristics may do the job. Hart's heuristic is 
also discussed in Devijver and Kittler (1982, p.120). For probabilistic analysis, we 
take a more abstract setting. Let (X~, Y{),  ... , (X:n ,  Y~1) be a sequence that depends 
in an arbitrary fashion  on the data Dn, and let gn  be the  I-nearest neighbor rule 
with (X~, Y{),  ... , (X;n'  Y';I)' where for simplicity, m is fixed beforehand. The data 
might, for example, be obtained by finding the subset of the data of size m for which 
the error with the  I-NN  rule committed on the remaining n  - m  data is  minimal 
(this  will  be called Hart's  rule).  Regardless,  if in  = (1/ n) L~1=1 I{gn(X,)/Y;}  and 
Ln  = P{gn(X) =I YIDnL then we have the following: 

Theorem 19.1.  (DEVROYE  AND  WAGNER  (I979c».  For all E  >  0 and all distri(cid:173)
butions, 

____ 

P{ILn - Lnl:::: E}:S  8  d+ 1 

(  ne  )(d+l)m<m-l) 

0 

e-ncj32. 

REMARK.  The estimate in  is  called the  resubstitution estimate of the error prob(cid:173)
ability. It is  thoroughly studied in Chapter 23,  where several results of the afore(cid:173)
mentioned kind are stated.  0 

PROOF.  Observe that 

where  Bi  is  the  Voronoi  cell  of X;  in  the  Voronoi  partition  corresponding  to 
Xi, ... , X~l' that is,  Bi  is the set of points of nd closer to X;  than to any other X~ 
(with appropriate distance-tie breaking). Similarly, 

We use the simple upper bound 

ILn  - inl :s  sup  IVn(A)  - v(A)I, 

AEAm 

where v denotes the measure of (X, Y), Vn  is the corresponding empirical measure, 
and Am is the family of all subsets ofnd  x {O,  I} of the form U;:l Bi  x {yd, where 
B l ,  .•• ,  Bm  are Voronoi cells corresponding to Xl,  ... ,  X m, Xi  End, Yi -'E  {O,  I}. 
We  use the  Vapnik-Chervonenkis  inequality  to  bound the  above  supremum.  By 
Theorem 13.5 (iv), 

19.1  Condensed Nearest Neighbor Rules 

305 

where A is the class of sets  B1  x  {yd. But each set in A  is  an intersection of at 
most m - 1 hyperplanes. Therefore, by Theorems  13.5 (iii),  13.9, and 13.3, 

seA, n) :::; 

sup 

110.111:110+111=11 

(01 

- -
)=0  d + 1 

.. (  n).e  )(d+1)(k-1)) 

(ne  )(d+1)(k-1) 
:::;  - -
d + 1 

, 

where n) denotes the number of points in R d  X  {j}. The result now follows from 
Theorem 12.5.0 

REMARK.  With Hart's  rule,  at least m  data points  are  correctly  classified by the 
1-NN rule (if we handle distance ties satisfactorily). Therefore, Ln  :::;  1 - min. 0 

The following is  a special case: Let m  <  n be fixed and let Dm  be an arbitrary 
(possibly random) subsetofm pairs from (Xl, Y1),  ... , (Xn ,  Yn ), usedingn • Let the 
remaining n - m points be denoted by Tn- m . We write Ln(Dm) for the probability 
of error with the  1-NN based upon Dm,  and we define 

In Hart's rule, Ln .m  would be zero, for example. Then we have: 

Theorem 19.2.  For all E  >  0, 

where  Ln  is the probability of error with gn  (note that Dm  depends in an arbitrary 
fashion  upon Dn),  and L n.m is  Ln.m(Dm,  Tn- m) with the data set Dm. 

PROOF.  List the m-element  subsets  {i 1,  ... , im }  of {I, 2,  ... , n},  and define  D~~) 
as  the  sequence of m  pairs  from  Dn  indexed by i  =  Ii 1,  ... ,  i In},  1 ::s  i  ::s  (,~). 
A 
ccor  mg y,  enote  n-m  -

d·  1  d 

en 

D(i)  Th 

In' 

T(i) 

- D 

n -

306 

19.  C::ondensed and Edited Nearest Neighbor Rules 

(by Hoeffding's inequality, because given D~), 

(n  - m)Ln,m(D~i), T~~m) is binomial ((n  - m), Ln(D~)))) 

~  2(:)e~2(n~m)c'.  0 

By checking the proof, we also see that if Dm  is selected to minimize the error 
estimate Ln,m (Dm, T,~-m), then the error probability Ln of the obtained rule satisfies 

p  {ILn  - inf Ln(Dm)1  >  E} 

D111 

<  P {2 sup ILn(DnJ  - Ln,m(Dm, T,1-m)1  >  E} 

D", 

(by Theorem 8.4) 

<  2(: )e~("~m)"/2  (as in the proof of Theorem 19.2). 

(19.1) 

Thus, for the particular rule that mimics Hart's rule (with the exception that m is 
fixed),  if m is not too large-it must be much smaller than n / log n-Ln is likely 
to  be  close to  the  best possible  we  can hope  for  with a  1-NN rule based upon a 
subsample of size m. With some work (see Problem 12.1), we see that 

By Theorem 5.1, 

where Lm  is the probability of error with the  1-NN rule based upon a sample of m 
data pairs. Hence, if m = o(n/ log n), m  -+  00, 

lim sup E{L n }  ::::  LNN 

n--7OO 

for the I-NN rule based on m data pairs Dm  selected to minimize the error estimate 
Ln,m(Dm, Tn-m). However, this is very pessimistic indeed. It reassures us that with 
only a small fraction of the original data, we obtain at least as good a performance 
as with the full data set-so, this method of condensing is worthwhile. This is not 
very surprising. Interestingly, however, the following much stronger result is true. 

19.1  Condensed Nearest Neighbor Rules 

307 

Theorem 19.3.  If m  = o(n /  log n)  and m  -+  00,  and if Ln}s the  probability 
of error for  the  condensed  nearest  neighbor  rule  in  which  Ln,m(Dm, Tn- m)  is 
minimized over all data sets Dm, then 

lim  E{Ln} = L*. 
n--*oo 

PROOF.  By (19.1), it suffices to establish that 

as  m  -+  00  such that m  = o(n), where  Ln(Dm) is the probability of error of the 
I-NN rule with Dm. As this is one of the fundamental properties of nearest neighbor 
rules not previously found in texts, we offer a thorough analysis and proof of this 
result in the remainder of this section, culminating in Theorem 19.4.  0 

The distribution-free result of Theorem 19.3 sets the stage for many consistency 
proofs for rules that use condensing (or editing, or proto typing, as  defined in the 
next two  sections).  It states that inherently, partitions of the space by  I-NN rules 
are rich. 

HISTORICAL  REMARK.  Other condensed nearest neighbor rules  are presented by 
Gates  (1972),  Ullmann  (1974),  Ritter,  Woodruff,  Lowry,  and  Isenhour  (1975), 
Tomek (1976b), Swonger (1972), Gowda and Krishna (1979), and Fukunaga and 
Mantock (1984).  0 

Define  Z  = I{1)(X»1/2)'  Let (X;,  Yi ,  Zi),  i  =  1,2, ... , n, be i.i.d.  tuples,  inde(cid:173)
pendent of (X, Y,  Z), where X;  may have a distribution different from X, but the 
support  set of the  distribution  J-['  of X;  is  identical to  that of J-[,  the  distribution 
of X.  Furthermore, P{Yi  = l1X;  = x} = 1J(x),  and  Zi  = I{1)(XD>1/2)  is the Bayes 
decision at X;. 

Lemma 19.1.  Let Ln  =  P { Z(l)(X) =I ZIXi, ZI, ... , X~, Zn}  be the probability 
of error for the J-NN rule based on (X;,  Zi),  1 :::  i  :::  n,  that is,  Z(l)(X) = Zi  if X; 
is the nearest neighbor of X among Xi, ... , X~. Then 

lim  E{L n } = O. 
n--*oo 

PROOF.  Denote by  X(l)(X) the nearest neighbor of X  among Xi, ... , X~. Notice 
that the proof of Lemma 5.1  may be extended in a straightforward way to  show 
that  IIX(l)(X)  - XII  -+  0 with probability one. Since this is the only property of 
the nearest neighbor of X that we used in deriving the asymptotic formula for the 
ordinary  I-NN error, limn--*oo E{Ln} equals  LNN  corresponding to  the pair (X,  Z). 
But we have P {Z = 11 X = x} = I{1)(X»  1/2). Thus, the Bayes probability of error L * 

308 

19.  Condensed and Edited Nearest Neighbor Rules 

for the pattern recognition problem with (X, Z) is zero. Hence, for this distribution, 
the  I-NN rule is consistent as  LNN  = L * = O.  0 

Lemma 19.2.  Let Z(I)(X) be as in the previous lemma.  Let 

Ln  = P {Z(l)(X) i  YIX~, Y1,  ••• ,  X~, Yn} 

be the probability of error for the discrimination problem for (X, Y) (not (X,  Z)). 
Then 

lim  E{ Ln} = L * , 

11-+00 

where L * is the Bayes error corresponding to (X,  Y). 

PROOF. 

P {Z(l)(X) i  Y} 

<  P {Z(l)(X) i  Z} +P{Y i  Z} 

0(1) + L * 

by Lemma 19.1.  0 

Theorem 19.4.  Let Dm  be a  subset of size m  drawn from  Dn.  If m  -+  00 and 
min -+ 0 as n  -+  00,  then 

lim  p{infLn(Dm) >  L*+E} =0 
n-+oo 

DI11 

for all E  >  0,  where  Ln (Dm)  denotes  the  conditional probability of error of the 
nearest neighbor rule with Dnu  and the infimum ranges over all (;J  subsets. 

PROOF.  Let  D  be  the  subset  of Dn  consisting  of those  pairs  (Xi, Yi )  for  which 
Yi  =  I{rJ(Xi »lj2}  =  Zi. If jDj  ~ m, let D* be the first m pairs of D, and if JDI  <  m, 
let D* = {(Xl, YI ),  ... ,  (Xm ,  Ym)}.  Then 

If N  =  JDI  ~ m, then we know that the pairs in D*  are i.i.d.  and drawn from the 
distribution of (X', Z), where X' has the same support set as  X; see Problem 19.2 
for properties of X'.  In particular, 

S  P{N  <  m} +P {N ~ m,  Ln(D*)  >  L* +E} 

19.3 Sieves and Prototypes 

309 

<  P{N<m}+P{Ln(D*»L*+E} 

<  P{Binomial(n, p) <  m} + __ n ___  _ 

E{L  (D*)}  - L * 

(where p = P{Y = Z}  = 1 - L *  ~ 1/2, and by Markov's inequality) 

E 

=  0(1), 

by the law of large numbers (here we use m/n  ~ 0), and by Lemma 19.2 (here 
we use m  ~ (0). 0 

19.2  Edited Nearest Neighbor Rules 

Edited nearest neighbor rules are  I-NN rules that are based upon carefully selected 
subsets (X~, Y{),  ... , (X~, YI~)' This situation is partially dealt with in the previous 
section, as the frontier between condensed and edited nearest neighbor rules is ill(cid:173)
defined. The idea of editing based upon the k-NN rule was first suggested by Wilson 
(1972) and later studied by Wagner (1973) and Penrod and Wagner (1977). Wilson 
suggests the following scheme: compute (Xi, Yi ,  Zi), where Zi is the k-NN decision 
at Xi based on the full data set with (Xi, Yi ) deleted. Then eliminate all data pairs for 
which Yi =I Zi. The remaining data pairs are used with the  I-NN rule (not the k-NN 
rule). Another rule, based upon data splitting is dealt with by Devijver and Kittler 
(1982). A survey is given by Dasarathy (1991), Devijver (1980), and Devijver and 
Kittler (1980). Repeated editing was investigated by Tomek (1976a). Devijver and 
Kittler (1982) propose a modification of Wilson's leave-one-out method of editing 
based upon data splitting. 

19.3  Sieves and Prototypes 

Let gn  be a rule that uses the  I-NN classification based upon prototype data pairs 
(X~, Y{),  ... , (X~, Y~)thatdependinsomefashionontheoriginaldata.Ifthepairs 
form  a subset of the data pairs (and thus,  m  ::::;  n), we have edited or condensed 
nearest  neighbor rules.  However,  the  (X;,  Yf)  pairs  may  be  strategically  picked 
outside the original data set. For example, in relabeling (see Section 11.7), m  = n, 
X;  = Xi  and Y/  = g~(Xi)' where g~(Xi) is the  k-NN decision at Xi.  Under some 
conditions, the relabeling rule is consistent (see Theorem 11.2). The true objective 
of proto typing is to extract information from the data by insisting that m  be much 
smaller than n. 

310 

19.  Condensed and Edited Nearest Neighbor Rules 

....  .. 
• 

• 

FIGURE  19.2.  A  1-NN  rule  based 
upon  4 prototypes.  In  this  example, 
all  the  data  points  are  correctly 
classified based  upon  the  prototype 
1-NN rule . 

: 

Chang  (1974)  describes  a  rule  in  which  we  iterate  the  following  step  until a 
given stopping rule is satisfied: merge the two closest nearest neighbors of the same 
class and replace both pairs by a new average prototype pair. Kohonen (1988; 1990) 
recognizes the advantages of such prototyping in general as a device for partitioning 
Rd-he calls this  learning vector quantization. This theme was picked up  again 
by Geva and Sitte (1991), who pick X~, ... , X~ as a random subset of X I,  ... ,  Xn 
and allow Y( to be different from Yi . Diverging a bit from Geva and Sitte, we might 
minimize the empirical error with the prototyped  1-NN rule over all  Y{,  ... , Y~, 
where the empirical error is that committed on the remaining data. We  show that 
this simple strategy leads to a Bayes-risk consistent rule whenever m  ---+  00 and 
min  ---+  O.  Note, in particular, that we may take (X~, ... , X~) = (Xl, ... , Xm), 
and that we "throwaway" YI ,  ... ,  Y m, as  these are not used. We may, in fact, use 
additional data with missing Yi-values for this purpose-the unclassified data are 
thus efficiently used to partition the space. 

Let 

be the empirical risk on the remaining data, where gn  is the  1-NN rule based upon 
(X 1, YD,  ... , (Xm,  Y~). Let g~ be the  I-NN rule with the choice of Y{,  ... , Y~ that 
minimizes Ln(gn). Let L(g~) denote its probability of error. 

Theorem 19.5.  L(g,:)  ---+  L * in probability for all distributions  whenever m  ---+ 
00 and min ---+  O. 

PROOF.  There are 2m  different possible functions  gn'  Thus, 

p { s~"p ILn(gn)  - L(gn)1  >  €  I Xl, ... , Xn} 

<  2m supP {ILn(gn) - L(gn)1  >  EI  Xl, ... , Xm} 

gn 

by Hoeffding's inequality. Also, 

P {L(g]:)  >  L * + 3E } 

<  p  {L(g~) - Ln (g~) >  E} 

+ P  {Ln(gn) - L(gn)  >  E} 
+ P  {L(gn)  >  L * + E} 

19.3 Sieves and Prototypes 

311 

(gn  minimizes L(gn» 

<  2E {p { s~"p ILn(gn) - L(gn)1  > ,I Xl, ... , Xn}} 

(here we used Ln(gn) ~ Ln(g]~» 

+ P {L(g;)  >  L * + E} 
(where g; is the  1-NN rule based on (Xl, Zl), ... , (Xm,  Zm) 
with Zi  =  I(Y/(X;»lj2}  as in Lemma 19.1) 
2m+2e-(n-m)E

2 + 0(1) 

< 

(if m  -+  00, by Lemma 19.1) 

=  0(1) 

if m -+  00 and m / n  -+ 0.  0 

If we  let Xi, ... , X;n  have  arbitrary  values-not only  among those  taken by 
Xl, ... , Xn-then we get a much larger, more flexible class of classifiers. For ex(cid:173)
ample, every linear discriminant is nothing but a prototype I-NN rule with m = 2-
just take  (X~, 1), (X;, 0)  and  place  X~ and X; in the right places.  In this  sense, 
prototype 1-NN rules generalize a vast class of rules. The most promising strategy 
of choosing prototypes is to minimize the empirical error committed on the train(cid:173)
ing  sequence  Dn. Finding this optimum may be computationally very expensive. 
Nevertheless,  the  theoretical  properties  provided in the  next result may provide 
useful guidance. 

Theorem 19.6.  Let C be the  class of nearest neighbor rules based on prototype 
pairs (Xl,  yd, ... , (Xm, Ym),  m  ~ 3, where the (Xi, Yi) 's range through nd  x {O,  I}. 
Given the training data Dn  = (X I,  Yd, ... , (Xn,  Yn),  let gn  be the nearest neighbor 
rule from C minimizing the error estimate 

Then for each E  >  0, 

The  rule is consistent if m  -+  00 such that m 2 log m  = o(n).  For d  = 1 and d  = 2 

312 

19.  Condensed and Edited Nearest Neighbor Rules 

the probability bound may be improved significantly.  For d  =  1, 

andford = 2, 

In  both cases,  the rule is consistent if m  ---+  00 and m log m = o(n). 

PROOF. It follows from Theorem 12.6 that 

where S(C, n) is  the  n-th shatter coefficient of the class  of sets  {x  :  ¢(x)  =  I}, 
¢  E  C. All we need is to find suitable upper bounds for S (C, n). Each classifier ¢ is 
a partitioning rule based on the m  Voronoi cells defined by Xl,  ... ,  X m . Therefore, 
S(C, n)  is  not more  than 2m  times  the  number of different ways  n  points  in nd 
can  be partitioned by  Voronoi  partitions  defined by m  points.  In  each partition, 
there are at most m(m - 1)/2 cell boundaries that are subsets of d - I-dimensional 
hyperplanes. Thus, the sought number is not greater than the number of different 
ways  m(m  - 1)/2 hyperplanes  can partition n  points.  By results  of Chapter  13, 
this is at most n(d+l)m(m-l)/2, proving the first inequality. 

The  other  two  inequalities  follow  by  sharper  bounds  on  the  number  of cell 
boundaries. For d  =  1, this is clearly at most m. To prove the third inequality, for 
each Voronoi partition construct a graph whose vertices are  Xl,  .•. ,  X m ,  and two 
vertices are connected with an edge if and only if their corresponding Voronoi cells 
are connected. It is  easy to  see that this graph is planar. But the number of edges 
of a planar graph with m vertices cannot exceed 3m - 6 (see Nishizeki and Chiba 
(1988, p.10)) which proves the inequality. 

The consistency results follow from the stated inequalities and from the fact that 

inf¢Ec L(¢) tends to  L * as m  ---+  00 (check the proof of Theorem 5.15 again).  0 

Problems and Exercises 
lim infn-+CXJ  E{N}/n  >  ° whenever  L *  >  0.  True or false:  if L * = 0,  then E{N}  = o(n). 

PROBLEM  19.1.  Let  N  :S  n  be the  size of the data  set after pure  condensing.  Show  that 

HINT:  Consider the real line, and note that all points whose right and left neighbors are of 
the same class are eliminated. 

PROBLEM  19.2.  Let (Xl, Yd, (X2'  Y2),  ... be an i.i.d. sequence of pairs of random variables 
in  n d  x  {a,  l} with  pry]  = llX]  = x}  = ry(x).  Let  (X', Z) be  the  first  pair (Xi, Yi )  in 

Problems and Exercises 

313 

the  sequence such that Yi  =  I{I)(x;»Jj2}.  Show that the distribution ~' of X'  is  absolutely 
continuous with respect to the common distribution ~ of the Xi'S, with density (i.e., Radon(cid:173)
Nikodym derivative) 

d~' 
d ~ (x) = 

1 - min(1J(x),  1 - 17(X)) 

1 - L * 

' 

where  L* is  the Bayes error corresponding to  (Xl, Yl).  Let Y be a  {a,  l}-valued random 
variable  with  P{Y  =  11X'  =  x}  =  1J(x).  If L' denotes  the Bayes  error corresponding  to 
(X',  y), then show that L' ::::  L * . 

PROBLEM  19.3.  Consider the following edited NN rule. The pair (Xi, Yz)  is eliminated from 
the  training  sequence  if the  k-NN  rule  (based  on  the  remaining  n  - 1 pairs)  incorrectly 
classifies  Xi' The  I-NN rule is  used with the edited data.  Show that this rule is consistent 
whenever X has a density if k  ---+  00 and k / n  ---+  O.  Related papers: Wilson (1972), Wagner 
(1973), Penrod and Wagner (1977), and Devijver and Kittler (1980). 

20 
Tree Classifiers 

Classification trees partition nd into regions, often hyperrectangles parallel to the 
axes.  Among these,  the most important are the binary classification trees,  since 
they have just two children per node and are thus easiest to manipulate and update. 
We recall the simple terminology of books on data structures. The top of a binary 
tree  is  called the  root.  Each node  has  either no child  (in  that case it is  called a 
terminal node or leaf),  a left child,  a right child, or a left child and a right child. 
Each node is the root of a tree itself. The trees rooted at the children of a node are 
called the left and right subtrees of that node. The depth of a node is the length of 
the path from the node to the root.  The height of a tree is the maximal depth of 
any node. 

Trees with more than two children per node can be reduced to binary trees by 

root 

FIGURE 20.1.  A binary tree. 

right child ....... . 

leaf 

~ .......... ,,, 

316 

20.  Tree Classifiers 

a simple device-just associate a left child with each node by selecting the oldest 
child in the list of children. Call the right child of a node its next sibling (see Figures 
20.2 and 20.3). The new binary tree is called the oldest-childlnext-sibling binary 
tree (see,  e.g.,  Cormen, Leiserson,  and Rivest (1990) for a general introduction). 
We  only mention this  particular mapping because it enables  us  to only consider 
binary trees for simplicity. 

A 

A 

B 

F 

G 

H 

H 

FIGURE 20.2.  Ordered tree: The chil(cid:173)
dren  are  ordered  from  oldest  to 
youngest. 

FIGURE 20.3.  The  corresponding bi(cid:173)
nary tree. 

In a classification tree,  each node represents a set in the space nd. Also, each 
node has  exactly  two  or zero  children.  If a node  u  represents  the  set  A  and its 
children u', u" represent A' and A", then we require that A  = A' U A" and A' n A" = 
0. The root represents n d, and the leaves, taken together, form a partition of nd. 
Assume that we know x  E  A. Then the question "is x  E  A?" should be answered 
in  a  computationally  simple  manner  so  as  to  conserve  time.  Therefore,  if x  = 
(x(l), ... , xed»~, we may just limit ourselves to questions of the following forms: 

(i)  Is xU)  :::  ex? This leads to ordinary binary classification trees with partitions 

into hyperrectangles. 

(ii)  Is alx(l) + ... + adx(d)  :::  ex? This leads to BSP trees (binary space partition 
trees). Each decision is more time consuming, but the space is more flexibly 
cut up into convex polyhedral cells. 

(iii)  Is  Ilx - z \I  :::  ex?  (Here z is a point of n d ,  to be picked for each node.) This 
induces a partition into pieces of spheres. Such trees are called sphere trees. 

(iv)  Is ljJ(x)  2:  O? Here, ljJ  is a nonlinear function, different for each node. Every 
classifier can be thought of as being described in this format-decide class 
one if ljJ(x)  2:  O.  However,  this misses the point,  as  tree classifiers should 
really be built up from fundamental atomic operations and queries  such as 
those listed in (i)-(iii). We will not consider such trees any further. 

20.  Tree Classifiers 

317 

-

I - - -

I---

FIGURE 20.4.  Partition  induced  by 
an ordinary binary tree. 

FIGURE 20.5.  Corresponding tree. 

FIGURE 20.6.  Partition induced by a 
BSP  tree. 

FIGURE 20.7.  Partition induced by a 
sphere tree. 

We associate a class in some manner with each leaf in a classification tree. The 
tree structure is usually data dependent, as well, and indeed, it is in the construction 
itself where  methods  differ.  If a  leaf represents  region  A,  then  we  say  that the 
classifier gn  is natural if 

if  L  Yi >  L (1  - Yi ), x  E  A 

i:XiEA 

i:XiEA 

otherwise. 

That is,  in every leaf region,  we take a majority vote over all  (Xi, Yi)'s  with  Xi 
in  the  same region.  Ties  are broken,  as  usual,  in favor  of class  O.  In  this  set-up, 
natural tree classifiers are but special cases of data-dependent partitioning rules. 
The latter are further described in detail in Chapter 21. 

318 

20.  Tree Classifiers 

FIGURE 20.8.  A natural classifier based 
on an ordinary binary tree.  The deci(cid:173)
sion is 1 in regions where points with 
label 1 form a majority.  These areas 
are shaded. 

Regular histograms can also be thought of as natural binary tree classifiers-the 
construction and relationship is obvious. However, as n  ---+  00, histograms change 
size,  and usually, histogram partitions are not nested as  n  grows.  Trees  offer the 
exciting perspective of fully  dynamic classification-as data are  added,  we may 
update the tree slightly, say, by splitting a leaf or so, to obtain an updated classifier. 
The most compelling reason for using binary tree classifiers is to explain com(cid:173)

plicated data and  to  have  a  classifier that is  easy  to  analyze  and  understand.  In 
fact, expert system design is based nearly exclusively upon decisions obtained by 
going down a binary classification tree. Some argue that binary classification trees 
are preferable over BSP trees for this simple reason. As argued in Breiman, Fried(cid:173)
man, Olshen, and Stone (1984),  trees allow mixing component variables that are 
heterogeneous-some components may be of a nonnumerical nature, others may 
represent integers, and still others may be real numbers. 

20.1 

Invariance 

Nearly  all  rules  in  this  chapter  and  in  Chapters  21  and  30  show  some  sort  of 
invariance  with  respect  to  certain  transformations  of the  input.  This  is  often  a 
major asset in pattern recognition methods.  We  say a rule  gn  is  invariant  under 
transformation T  if 

for all values of the arguments. In this sense, we may require translation invariance, 
rotation  invariance,  linear translation  invariance,  and monotone transformation 
invariance (T(·) maps each coordinate separately by a strictly increasing but pos(cid:173)
sibly nonlinear function). 

Monotone  transformation  invariance  frees  us  from  worries  about  the  kind  of 

measuring unit. For example, it would not matter whether earthquakes were mea(cid:173)
sured on a logarithmic (Richter) scale or a linear scale. Rotation invariance matters 
of course in situations in which input data have no natural coordinate axis system. 
In many cases, data are of the ordinal form-colors and names spring to mind-and 

20.2 Trees with the X-Property 

319 

ordinal values may be translated into numeric values by creating bit vectors. Here, 
distance loses  its  physical meaning,  and any rule that uses  ordinal data perhaps 
mixed in with numerical data should be monotone transformation invariant. 

Tree methods that are based upon perpendicular splits are usually  (but not al(cid:173)

ways) monotone transformation invariant and translation invariant. Tree methods 
based upon linear hyperplane splits are sometimes linear transformation invariant. 
The  partitions  of space  cause  some  problems  if the  data  points  can  line  up 
along hyperplanes.  This is just a matter of housekeeping,  of course, but the fact 
that  some projections of X  to  a line have atoms  or some components of X  have 
atoms  will  make  the  proofs  heavier  to  digest.  For this  reason  only,  we  assume 
throughout this chapter that X has a density  f. As typically no conditions are put 
on f  in our consistency theorems, it will be relatively easy to generalize them to 
all distributions. The density assumption affords us the luxury of being able to say 
that,  with  probability one,  no  d  + 1 points fall  in a hyperplane,  no  d  points  fall 
III a hyperplane perpendicular to  one axis,  no  d  - 1 points fall  in a hyperplane 
perpendicular to two axes, etcetera. If a rule is monotone transformation invariant, 
we can without harm transform all the data as follows for the purpose of analysis 
only.  Let  iI, ... , fd  be  the  marginal  densities  of X  (see  Problem  20.1),  with 
corresponding  distribution  functions  F I ,  ... ,  Fd.  Then replace  in  the  data  each 
Xi  by  T(Xi ), where 

Each  component of T(Xi )  is  now  uniformly  distributed on  [0,  1].  Of course,  as 
we do not know T  beforehand, this device could only be used in the analysis. The 
transformation T  will be called the uniform marginal transformation. Observe that 
the original density is now transformed into another density. 

20.2  Trees with the X -Property 

It is possible to prove the convergence of many tree classifiers all at once. What is 
needed,  clearly, is  a partition into  small regions, yet, most majority votes  should 
be  over  sufficiently  large  sample.  In  many  of the  cases  considered,  the  form  of 
the  tree  is  determined by  the  X/s only,  that  is,  the  labels  Yi  do  not playa role 
in constructing the partition, but they are used in voting.  This is of course rather 
simplistic, but as  a start, it is very  convenient. We  will say that the classification 
tree  has  the  X -property,  for lack of a  better mnemonic.  Let the leaf regions  be 
{AI, ... , AN} (with N  possibly random). Define N j  as the number of Xi'S falling 
in A j. As the leaf regions form a partition, we have L~=I N j  = n. By diam(A j) we 
mean the diameter of the cell A j, that is, the maximal distance between two points 
of A j. Finally, decisions are taken by majority vote,  so for x  E  A j, 1 S  j  S  N, 

320 

20.  Tree Classifiers 

the rule is 

I 
0 

gn(x) = 

{

if  L  Yi  >  L  (1  - Yi ), x  E  A j 

i:X;EAj 

i:X;EAj 

otherwise. 

A(x) denotes the set ofthe partition {AI, ... , AN} into which x falls, and N(x) is the 
number of data points falling in this set. Recall that the general consistency result 
given in Theorem 6.1  is applicable in such cases. Consider a natural classification 
tree  as  defined  above  and  assume  the  X -property.  Theorem 6.1  states  that then 
E{L n }  ~ L*  if 

(1)  diam(A(X»  ~ 0 in probability, 
(2)  N(X) ~ 00 in probability. 

A more general, but also more complicated consistency theorem is proved in Chap(cid:173)
ter 21. Let us start with the simplest possible example. We verify the conditions of 
Theorem 6.1  for the k-spacing rule in one dimension. This rule partitions the real 
line by using the k-th, 2k-th (and so on) order statistics (Mahalanobis, (1961); see 
also Parthasarathy and Bhattacharya (1961». 

o 

0 

FIGURE 20.9.  A 3-spacing classifier. 

Formally,  let k  <  n  be  a  positive  integer,  and let  X(l),  X(2),  .•. ,  X(n)  be  the 
order statistics of the data points. Recall that X(l),  X(2),  ... ,  X(n)  are obtained by 
permuting Xl, ... , Xn  in such a way that X(l)  :s  X(2)  :s  ... :s  X(n). Note that this 
ordering is  unique  with probability one as  X  has  a density.  We  partition R  into 
N  intervals  AI, ... , AN, where N  =  r n I k l, such that for  j  =  1, ... , N  - 1,  A j 
satisfies 

X(kU-l)+I),.'.' X(kj)  E  A j , 

and the rightmost cell AN  satisfies 

We  have not specified the endpoints of each cell  of the partition. For simplicity, 
let the borders between A j  and  A j+I  be put halfway between the rightmost data 
point in A j  and leftmost data point in A j+I, j  = 1, ... , N  - 1. 

The classification rule gn  is defined in the usual way: 

Theorem 20.1.  Let gn  be  the k-spacing  classifier given  above.  Assume that the 
distribution of X has a density f  on R. Then the classification rule gn  is consistent, 
that is,  limn-,>-oo E{L n }  = L *, if k  ~ 00 and kin  ~ 0 as n tends to  infinity. 

20.2 Trees with the X -Property 

321 

REMARK.  ¥le discuss various generalizations of this rule in Chapter 21.  0 

PROOF. We check the conditions of Theorem 6.1, as the partition has the X -property. 
Condition (2) is obvious from k -+  00. 

To  establish condition  (1),  fix  E  >  O.  Note that by the  invariance  of the  rule 
under monotone transformations, we may assume without loss of generality that 
f  is  the uniform density on [0,1]. Among the intervals  Ai, ... , AN, there are at 
most  liE disjoint intervals of length greater than E  in [0,  1]. Thus, 

P{diam(A(X)) >  E} 

<  ~E {  max  fL(A j )} 

E 

l',,::j~N 

E 

<  ~E {( m~x fLn(A j) +  m.ax  IfL(A j) -
<  H~ + E hp IfL(A) -

fLn(A)I}) , 

l~J~N 

l~J~N 

fLn(A j)I)} 

where  the  supremum is  taken  over all  intervals  in  R. The  first  term  within  the 
parentheses converges to  zero by the second condition of the theorem, while the 
second term goes to zero by an obvious extension of the classical Glivenko-Cantelli 
theorem (Theorem 12.4). This completes the proof.  0 

We will encounter several trees in which the partition is determined by a small 
fraction of the data, such as binary k -d trees and quadtrees. In these cases, condition 
(2) of Theorem 6.1  may be verified with the help of the following lemma: 

Lemma 20.1.  Let Pi, ... , Pk  be a probability vector. Let N l ,  ... ,  Nk  be multino(cid:173)
mially  distributed  random  variables  with parameters n  and Pi, ... , Pk·  Then  if 
the random variable X is independent oJ N l ,  ... ,  Nb andP{X = i} = Pi,  we have 
for any M, 

P{Nx :s  M}  :s 

(2M + 4)k 
. 

n 

(Note:  this probability goes to 0 if kin -+ 0 uniformly over all probability vectors 
with k components!) 

PROOF.  Let Zi be binomial with parameters n and Pi. Then 

P{Nx :s  M}  < 

i:npi~2M 

i:npi>2M 

< 

< 

2Mk 
--+ 

n 

i:npi>2M 

L  PiP{Zi  - EZi  :s  M  - npi} 

2Mk 
- - + 

n 

" 

.  ~  C 
l:npi>2M 

E~} 
p.p  z·  -EZ· < - -
2 

l_  

{ 

I 

322 

20.  Tree Classifiers 

< 

< 

< 

_2M_k  +  '"  4 p- Var{ Zd 
i:n~M  I  (E{Zd)2 

n 

(by Chebyshev's inequality) 
2Mk 

-+  L  4Pi-

1 

n 

i:npi >2M 

npi 

(2M +4)k  o 

n 

The  previous lemma implies that for  any binary tree classifier constructed on 
the basis of Xl, ... , X k  with k + 1 regions,  N(X)  -+  00 in probability whenever 
kl(n - k) -+ 0 (i.e., kin -+ 0). It suffices to note that we may take M  arbitrarily 
large  but  fixed  in  Lemma 20.1.  This  remark  saves  us  the  trouble  of having  to 
verify just how large or small the probability mass of the region is. In fact, it also 
implies  that  we  should  not  worry  so  much  about regions  with  few  data points. 
What matters  more than  anything  else  is  the  number of regions.  Stopping rules 
based upon cardinalities of regions can effectively be dropped in many cases! 

20.3  Balanced Search Trees 

Balanced  multidimensional  search  trees  are  computationally  attractive.  Binary 
trees with n leaves have  0 (log n) height, for example, when at each node, the size 
of every subtree is at least ex  times the size of the other subtree rooted at the parent, 
for some constant ex  >  O. It is thus important to verify the consistency of balanced 
search trees  used in classification.  We  again consider binary classification trees 
with the X -property and majority votes over the leaf regions. Take for example a 
tree in which we split every node perfectly, that is, if there are n points, we find the 
median according to one coordinate, and create two subtrees of sizes  L(n  - 1)/2J 
and r(n -1)/2l. The median itself stays behind and is not sent down to the subtrees. 
Repeat this for k levels of nodes, at each level cutting along the next coordinate axe 
in a rotational manner. This leads to 2k  leaf regions, each having at least n 12k - k 
points and at most nl2k points. Such a tree will be called a median tree. 

FIGURE 20.10.  Median tree withfour leaf regions in 1?}. 

25% 

25% 

25% 

25% 

Setting up such a tree is very easy, and hence such trees may appeal to certain pro(cid:173)
grammers. In hypothesis testing, median trees were studied by Anderson (1966). 

Theorem 20.2.  Natural classifiers based upon median trees with k levels (2k leaf 
regions) are consistent (E{ Ln}  -+  L * ) whenever X  has a density,  if 

20.3 Balanced Search Trees 

323 

n 
-k -+  00  and  k-+oo. 
k2 

.... 

. 

(Note:  the conditions of k are fulfilled if k  :::s  log2 n  - 2log2log2 n,  k  -+  00.) 

We may prove the theorem by checking the conditions of Theorem 6.1. Condition 
(2) follows trivially by the fact that each leaf region contains at least n 12k - k points 
and the condition nl(k2k) -+  00. Thus, we need only verify the first condition of 
Theorem 6.1. To make the proof more transparent, we first analyze a closely related 
hypothetical  tree,  the  theoretical  median  tree.  Also,  we  restrict  the  analysis  to 
d ==  2. The multidimensional extension is straightforward. The theoretical median 
tree rotates through the coordinates and cuts each hyperrectangle precisely so that 
the two new hyperrectangles have equal jL-measure. 

FIGURE 20.11.  Theoretical median tree with three 
levels of cuts. 

118 

1/8 

118 

! - - -

118 

~ 118 

118 

118 

I 

Observe that the rule is invariant under monotone transformations of the coordinate 
axes.  Recall  that  in  such cases  there  is  no  harm in  assuming  that  the  marginal 
dIstributions  are all uniform on [0,  1].  We let  {Hi,  Vd  denote the horizontal and 
vertical  sizes  of the  rectangles  after k  levels  of cuts.  Of course,  we  begin  with 
HI  = VI  = 1 when k  = O.  We  now  show  that,  for  the  theoretical  median  tree, 
diam(A(X))  -+  0 in probability,  as  k  -+  00.  Note that diam(A(X))  :::s  H(X) + 
VeX), where H(X) and VeX) are the horizontal and vertical sizes of the rectangle 
A(X). We show that if k is even, 

E{H(X) + VeX)} = 2k / 2 ' 

2 

from which the claim follows. After the k-th round of splits, since a1l2k rectangles 
have equal probability measure, we have 

Apply  another round of splits,  all  vertical.  Then each term  -1F(Hi  + VJ  spawns, 
so  to  speak,  two  new  rectangles  with horizontal  and vertical  sizes  (H(, Vi)  and 

324 

20.  Tree Classifiers 

(H(",  Vi)  with Hi  = H( + H("  that contribute 

The next round yields  horizontal  splits,  with  total  contribution  now (see  Figure 
20.12) 

_1_(H: + V.' + H: + V."  + H'" + v.'" + H:" + V."") 
2k+2 

I 

I 

I 

I 

I 

I 

I 

I 

1 

2k+2 (2Hi + 2 Vi) 

1 

2k+1  (Hi  + ~). 

Thus, over two iterations of splits, we see that E{ H (X) + V (X)} is halved, and the 
claim follows  by simple induction. 

We show now what happens in the real median tree when cuts are based upon 
a  random  sample.  We  deviate  of course  from  the  theoretical  median  tree,  but 
consistency  is  preserved.  The  reason,  seen  intuitively,  is  that  if the  number of 
points  in  a cell is  large,  then  the  sample median  will  be  close to  the  theoretical 
median,  so that the  shrinking-diameter property  is  preserved.  The methodology 
followed here shows how one may approach the analysis in general by separating 
the theoretical model from the sample-based model. 

PROOF  OF  THEOREM  20.2.  As  we  noted  before,  all  we  have  to  show  is  that 
diam(A(X» 
--+  0  in  probability.  Again,  we  assume  without loss  of generality 
that the  marginals of X  are uniform [0,  1],  and that d  =  2.  Again, we  show that 
E{H(X) + VeX)}  --+  O. 

FIGURE 20.12.  A rectangle after two rounds 
a/splits. 

H· I 

p/!/ 
I 

v/!/ 

I 

H!" I 

p"" 
i 

Hi'" 

tvt/ 
, 

v· I 

I 

P:/ 
tvy 
I 
H('  + 
p~ 
V'· 
I 
I 
H( 
I 

If a rectangle of probability mass Pi  and sizes Hi,  Vi  is split into four rectangles 
as in Figure 20.12, with probability masses p;, p;',  p;",  pt', then the contribution 
PiCHi  + Vi)  to E{H(X) + VeX)} becomes 

after two levels of cuts. This does not exceed 

if 

and 

20.3 Balanced Search Trees 

325 

1;-:;-:-1 

::::"2 v  1  + E Pi, 

(  I 

max  Pi  + Pi  ,Pi  + Pi 

II 

III 

1111) 

(  I 

max  Pi' Pi 

/1)  <  ~;-:;-:-1 (I 

- 2 v  1 + E  Pi  + Pi 

") 
' 

max(p:"  p:llI)  <  _~( :" +  :111) 
' 

z'  z 

Pz 

Pz 

1 
- 2 

that is, when all three cuts are within (lj2)~ of the true median. We call such 
"(lj2)~" cuts good.  If all cuts are good, we thus  note that in two levels of 
cuts, E{H(X)+ VeX)} is reduced by (1 +E)j2. Also, all Pi'S decrease at a controlled 
rate. Let G be the event that all 1 + 2 + ... + 2k- 1 cuts in a median tree with k levels 
are good. Then, at level k, all p/s are at most (~j2 )k. Thus, 

2k 
LPi(Hi  + Vi)  < 
i=l 

since L~~l (Hi  + VJ  ::::  4 + 2 + 4 + 8 + ... + 2k/2  ::::  2k/2+1  + 3 if k is even. Hence, 
after k levels of cuts, 

The  last term tends  to  zero  if E  is  small enough.  We  bound P{ G e }  by  2k  times 
the probability that one cut is bad. Let us cut a cell with N  points and probability 
content P in a given direction. A quick check of the median tree shows that given 
the position and size of the cell, the N  points inside the cell are distributed in an 
i.i.d.  manner according to the restriction of fL  to  the  cell.  After the cut,  we have 
l)j2J  and  r(N - 1)j2l  points in the  new cells,  and probability contents 
L(N  -
pi  and  p". It is  clear that we may assume without loss of generality that P  = 1. 
Thus,  if all points  are projected down in the direction of the  cut,  and  F  and  F N 
denote the distribution function and empirical distribution function of the obtained 
one-dimensional data, then 

P {cut is not goodl  N} 

<  P  P  >  --2-orP  >  --2- N 

I  ~ II  ~I} 

{

326 

20.  Tree Classifiers 

<  php(F(X)-FN(X))>~(~-l)IN} 
<  2eXp(-~N(~-ln 

(by Theorem 12.9) 

<  2eXp(-(2:+1  -D(~-ln 

(as N  ::::  nj(2k) - k). 

Hence, for n large enough, 

20.4  Binary Search Trees 

The simplest trees to analyze are those whose structure depends in a straightforward 
way on the data. To make this point, we begin with the binary search tree and its 
multivariate extension, the k-d tree (see Cormen, Leiserson, and Rivest (1990) for 
the binary search tree; for multivariate binary trees,  we refer to  Samet (1990b)). 
A full-fledged k-d tree is  defined as  follows:  we promote the first  data point Xl 
to the root and partition {X2,  ... , Xn}  into two sets:  those whose first coordinate 
exceeds that of Xl, and the remaining points. Within each set, points are ordered 
by original index. The former set is  used to  build the right subtree of Xl  and the 
latter to construct the left subtree of Xl' For each subtree, the same construction is 
applied recursively with only one variant: at depth l in the tree, the (l  mod d + l)-st 
coordinate is used to split the data. In this manner, we rotate through the coordinate 
axes periodically. 

Attach  to  each  leaf two  new  nodes,  and  to  each  node  with  one  child  a  sec(cid:173)
ond child. Call these new nodes external nodes. Each of the n + 1 external nodes 
correspond to  a region of R d ,  and collectively the external nodes define a parti(cid:173)
tion ofRd  (if we define exactly what happens on the boundaries between regions). 

20.4 Binary Search Trees 

327 

FIGURE 20.13.  A k-d tree oi15 random points on the plane and the induced 
partition. 

Put  differently,  we  may  look  at  the  external  nodes  as  the  leaves  of a  new  tree 
with  2n  + 1 nodes  and declare this  new  tree to be our new  binary classification 
tree.  As  there  are  n  + 1 leaf regions  and  n  data  points,  the  natural  binary  tree 
classifier it induces is  degenerate-indeed, all  external regions  contain very few 
points.  Clearly,  we must have a mechanism for trimming the tree to insure better 
populated leaves.  Let us  look at just three naive strategies.  For convenience,  we 
assume  that  the  data  points  determining  the  tree  are  not  counted  when  taking 
a majority  vote  over the  cells.  As  the  number of these  points  is  typically much 
smaller than n, this restriction does not make a significant difference. 

(1)  Fix k  <  n, and construct a k -d tree with k internal nodes and k + 1 external 
nodes,  based  on  the  first  k  data  points  Xl, ... , Xk.  Classify  by  majority 
vote  over all k + 1 regions  as in natural  classification tees  (taking the data 
pairs  (Xk+l,  Yk+d,  ... , (Xn, Yn) into  account).  Call this  the  chronological 
k-d tree. 

(2)  Fix k  and truncate the k-d tree to  k  levels. All nodes at level k  are declared 
leaves and classification is again by majority vote over the leaf regions. Call 
this the deep k-d tree. 

(3)  Fix  k  and  trim  the  tree  so  that  each  node  represents  at  least  k  points  in 
the  original construction.  Consider the maximal such tree.  The number of 
regions here is random, with between 1 and nj k regions. Call this the well(cid:173)
populated k-d tree. 

328 

20.  Tree Classifiers 

Let the leaf regions be {AI, ... , AN} (with N  possibly random), and denote the 
leaf nodes  by  UI,  ... ,  UN.  The  strict descendants  of Ui  in the  full  k-d tree have 
indices that we will collect in an index set Ii. Define I Ii I = Ni. As the leaf regions 
form a partition, we have 

N 
LNi=n-N, 
i=l 

because the leaf nodes themselves are not counted in Ii. Voting is by majority vote, 
so the rule is 

20.5  The Chronological k-d Tree 

Here we have  N  =  k + 1.  Also,  (/L(A l ),  ... , /L(Ak+I )) are distributed as  uniform 
spacings.  That is,  if U I ,  ... ,  Uk  are i.i.d.  uniform  [0,  1]  random variables defin(cid:173)
ing  k + 1  spacings  U(l),  U(2)  - U(l),  ... ,  U(k)  - U(k-I), 1 - U(k)  by  their  order 
statistics  U(l)  .:::  U(2)  .:::  ...  .:::  U(k), then these spacings are jointly distributed as 
({L(Ad, ... , /L(Ak+d). This can be shown by induction. When Uk+1 is added, Uk+1 
first picks a spacing with probability equal to the size of the spacing. Then it cuts 
that spacing in a uniform manner. As the same is true when the chronological k-d 
tree grows by one leaf, the property follows by induction on k. 

Theorem 20.3.  We  have E{ Ln}  ---+  L * for all distributions of X with a density for 
the chronological k-d tree classifier whenever 

k ---+  00, 

and 

kin ---+  O. 

PROOF. We verify the conditions of Theorem 6.1. As the number of regions is k+ 1, 
and the partition is determined by the first k data points, condition (2) immediately 
follows from Lemma 20.1  and the remark following it. 

Condition (1) of Theorem 6.1  requires significantly more work. We verify con(cid:173)
dition (1) for d  =  2,  leaving the  straightforward extension to  R d,  d  >  2,  to  the 
reader. Throughout, we assume without loss of generality that the marginal distri(cid:173)
butions  are uniform [0,  1].  We  may do  so by the invariance properties discussed 
earlier.  Fix a point x  E  Rd. We  insert points  Xl, ... ,  Xk  into an initially empty 
k -d tree and let R 1,  ... ,  Rk  be the rectangles containing x  just after these points 
were inserted.  Note that  Rl  ;2  R2  ;2  ...  ;2  Rk .  Assume for  simplicity that the 
integer k  is a perfect cube and set l = k 1
/ 3 .  Define the distances from x 
to the sides of Ri  by Hi,  H:,  Vi,  V/ (see Figure 20.14). 

,  m  = k 2

3

/

20.5 The Chronological k-d Tree 

329 

FIGURE 20.14.  A  rectangle  Ri  containing  x 
with its distances to  the sides. 

Hi 

V.' 
I 

x 

H; 

Vi 

We  construct a sequence of events  that forces  the diameter of Rk  to  be small 

with high probability. Let E  be a small positive number to be specified later. 

Denote the four squares with opposite vertices x, x + (±E, ±E) by C1, C2 ,  C3 , 

C4' Then define the following events: 

El  n {Ci  n {Xl, ... , Xd 7'0}, 

4 

i=l 

E2 

E3 

E4 

E5 

E6 

{max(~, ~', H/,  H()  :s E} , 
{min (max(V/, V/), max(H/, H(»)  :s  E  <  max(V/, V/'  H/,  H()}  , 
{at least three of  Vm ,  V~, Hm ,  H~l are  :s E} , 
{max(Vm ,  V~, Hm ,  H~):s E}, 
{max(Vk,  V;, Hk,  HD  ~ E} . 

If E2,  E5,  or E6  hold, then diam(R k )  :s  E.J8. Assume that we find a set Bend 
such  that  P{X  E  B}  =  1,  and  for  all  x  E  B,  and  sufficiently  small  E  >  0, 
P{E2 U E5  U E6}  -+  1 as k  -+  00. Then, by the Lebesgue dominated convergence 
theorem, diam(A(X»  -+ 0 in probability, and condition (1) of Theorem 6.1 would 
follow.  In the remaining part of the proof we define such a set B, and show that 
P{E2  U E5  U E6} -+  1. This will require some work. 

We define the set B in terms of the density f  of X. x  E  B if and only if 
(1)  minl~i:~4 fe i 

f(z)dz  >  0 for all  E  >  0 small enough; 

f  f(z)dz 

R 

A(R) 

f(x) 

>  __ for all E  >  0 small enough; 
-

2 

(2) 

inf 

rectangles  R containing x, 

of diameter ~ E.j8 

(3)  f(x)  ::::  O. 

That  P{X  E  B}  =  1 follows  from  a  property  of the  support  (Problem  20.6), 
a corollary of the  lessen-Marcinkiewicz-Zygmund theorem  (Corollary A.2;  this 
implies (2) for almost all x), and the fact that for j.i-almost all x, f (x)  >  O. 

It is easy to verify the following: 

P{E~}  =  P{ED 

+P{E1 n E~ n E~} 

330 

20.  Tree Classifiers 

+P{E1 n E~ n E3  n E~} 
+P{E1 n E~ n E3  n E4  n E~ n E6}. 

We show that each term tends to 0 at x  E  B. 

TERM  1.  By the union bound, 

4 

P{Ef}  <  ~P{XI tI:  Ci ,  ... ,XI tI:  Cd 

i=l 

4 

<  ~ (1  -

i=l 

/L(Ci)i 

< 

exp ( -l min  /l(ei») 

l::si:::A 

-+  0, 

by part (1) of the definition of B. 
TERM 2.  E1  ~ E3  by a simple geometric argument. Hence P{EI n En = o. 
TERM  3.  To  show  that P{E1  n E~ n E3  n E~}  -+  0,  we  assume  without loss 
of generality  that max(Vz,  V/)  ::::  E  while max(Hz, HI)  =  a  >  E.  Let {XD be a 
subset of {Xi, I  <  i  ::::  m}  consisting of those  Xi'S  that fall  in  RE•  We  introduce 
three notions in this  sequence:  first,  Zi  is  the absolute value of the difference of 
the x(2) -coordinates of x  and X;. Let Wi  be the absolute value of the difference of 
the x(l)-coordinates of x  and X;. We re-index the sequence X;  (and Wi  and Zi) so 
that i  runs from 1 to N, where 

N  = ~ I{x;ERtl. 

m 

i=Z+l 

To avoid trivialities, assume that N  ~ 1 (this will be shown to happen with prob(cid:173)
ability tending to one). Call X;  a record if Zi = min(Zl, ... , Zi). Call X;  a good 
point if Wi  ::::  E.  An X;  causes min(Hm,  H'~l)  ::::  E  if that X;  is a good point and a 
record, and if it defines a vertical cut. The alternating nature of the cuts makes our 
analysis a bit heavier than needed. We show here what happens when all directions 
are picked independently of each other, leaving the rotating-cuts case to the reader 
(Problem 20.8). Thus, if we set 

Si  = I {x;  is a record,  X;  defines a vertical cut} , 

we have, 

20.5 The Chronological k-d Tree 

331 

Re-index  again  and  let  X~, X~, ... all  be  records.  Note  that  given  X;,  X;+l  is 
distributed according to f  restricted to the rectangle R' of 

height  min(V[,  Zi)above x, 
height  min ("l',  Z i) below x, 
width Hz  to the left of x, 
width H! to the right of x. 

Call these four quantities v, v', h, h', respectively. Then 

P{W.  <  EIX~} >  (v + v')Ef(x)/2  =  Ef(x) 

1+1  -

1 

-

V + v' 

2 

because the marginal distribution of an independent X 1 is uniform and thus, P {X 1  E 
R'IR'}  :::  v + v',  while,  by  property (2)  of B, P{X1  E  R', WI  :::  EIR'}  2::  (v  + 
v')Ef(x)/2. 

Recall the re-indexing. Let M  be the number of records (thus,  M  is the length 

of our sequence X). Then 

E!  n E2 n E3  n E~ n (N  >  OJ ~ {t I{w,s,)I{x;  ""erticrumt) = o} . 

But as  cuts have independently picked directions,  and  since P{Wi+1  :::  EIX;J  2:: 
Ef(x)/2, we see that 

P{E1  n E2  n E3  n E4 ,  N  >  O}  :::  E 

c 

c 

Ef(x) 
1 - -4-
)

{ (  

M 

f{N>o}, 

} 

We  rewrite  M  =  L~1 fiX;  is a record}  and recall that the indicator variables in this 
sum are independent and are of mean 1 I i  (Problem 20.9). Hence, for c  >  0, 

<  E {e-(l-C) L~l Iii} 

(use 1 - x  ::s  e-X) 

<  E { 

1 

(N + 1)I-c 

} 

(use "1:1 II i  2::  log(N + 1)). 

L 

The latter formula remains valid even if N  = O.  Thus, with c = 1 - Ef(x)/4, 

PIE! n E2  n E3 n E~J  <  E {(I - Ef~X)r} 

<  E{(N+1l)!_e}. 

332 

20.  Tree Classifiers 

l, fl(Rz)).  We  know  from the introduction 
N  is  binomial with parameters  (m  -
of this  section that fl(Rt) is distributed as the minimum of l  i.i.d.  uniform [0,  1] 
random variables. Thus, for 8  >  0, E{fl(Rz)} = 1/(l + 1), and P{fL(Rz)  <  81 l} ::::  8. 
Hence, 

E {(N +\)1-' } 

<  P{fl(Rz)  <  81 I}  + E  { 

} 

(Binomial(m -1,81 I) + l)l-c 

1 

< 

8 + 

( 

2(m  -1)8 

l 

)  l-c 

{ 

+ P  Binomial(m -

l, 8 I l) < 

(m  -1)8} 
. 

2l 

The first term is small by choice of 8.  The second one is  0  (k-(l-c)/3). The third 
one is bounded from above, by Chebyshev's inequality, by 

(m  -l)(81 l)(1  - 81 I) 
- - - - - - - - < 
-

((m  -l)812l)2 

41 

(m  -1)8 

-'7  0. 

TERM 4.  This term is handled exactly as Term 3. Note, however, that I and m now 

become m and k respectively. The convergence to ° requires now m I (k - m)  -'7  0, 

which is still the case. 

This concludes the proof of Theorem 20.3.  0 

20.6  The Deep k-d Tree 

Theorem 20.4.  The  deep  k-d tree classifier is  consistent (i.e.,  E{L n }  -'7  L *) for 
all distributions such that X  has a density,  whenever 

lim  k  = 00  and 
n----+oo 

k 

. 
hmsup-- <  2. 
n----+oo 

log n 

PROOF.  In Problem 20.10, you are asked to show that k -'7  00 implies diam(A(X)) 

-'7 ° in probability.  Theorem 20.3  may be invoked here. We now show that the 

assumption lim sUPn----+oo kl log n  <  2 implies that N(X) -'7  00 in probability. Let 
D  be the depth (distance from the root)  of X  when X  is  inserted into a k-d tree 
having n elements. Clearly, N(X) ~ D - k, so it suffices to show that D - k  -'7  00 
in probability.  We know that D 1(2 log n)  -'7  1 in probability (see, e.g.,  Devroye 
(1988a) and Problem 20.10). This concludes the proof of the theorem.  0 

20.7 Quadtrees 

333 

20.7  Quadtrees 

Quadtrees or hyperquadtrees are unquestionably the most prominent trees in com(cid:173)
puter  graphics.  Easy to  manipulate and compact to  store,  they  have found  their 
way into mainstream computer science. Discovered by Finkel and Bentley (1974) 
and surveyed by Samet (1984), (1990a), they take several forms. We are given d(cid:173)
dimensional data X I, ... ,  X n . The tree is constructed as the k -d tree. In particular, 
Xl  becomes the root of the tree. It partitions X 2 ,  ..• ,  Xn  into 2d  (possibly empty) 
sets according to membership in one of the 2d  (hyper-) quadrants centered at Xl 
(see Figure 20.15). 

r--- 1 
.---

1 
I 

:---11---

~ 

~ 

f-------1 

[  1 T 

1 

FIGURE 20.15.  Quadtree  and  the  induced partition  of R2,  The 
points on the right are shown in the position in space.  The  root is 
specially marked. 

The partitioning process is repeated at the 2d  child nodes until a certain stopping 
rule  is  satisfied.  In analogy  with  the k-d tree,  we  may define  the  chronological 
quadtree (only k splits are allowed, defined by the first k points X I, ... ,  X k ), and 
the deep quadtree (k  levels of splits are allowed).  Other, more balanced versions 
may also be introduced. Classification is by majority vote over all (Xi, Yi ),s-with 
k <  i  :::  n in the chronological quadtree-that fall in the  same region as  x. Ties 
are  broken in favor  of class O.  We  will refer to  this  as  the  (chronological,  deep) 
quadtree classifier. 

Theorem 20.5.  Whenever X  has a density,  the chronological quadtree classifier 
is consistent (E{ Ln}  ---+  L * ) provided that k  ---+  00 and kin ---+  O. 

PROOF.  Assume without loss of generality that all marginal distributions are uni(cid:173)
form [0,1]. As the X-property holds, Theorem 6.1 applies. By Lemma 20.1, since 
we have k(2d  - 1) + 1 external regions, 

P{N(X) :::  M} :s 

(2M + 4)(k(2d  - 1) + 1) 

n  _  k 

---+  0 

for all M  >  0, provided that kin  ---+  O.  Hence, we need only verify the condition 

334 

20.  Tree Classifiers 

diam(A(X)  ~ 0 in probability. This is a bit easier than in the proof of Theorem 
20.3 for the k-d tree and is thus left to the reader (Problem 20.18).  0 

REMARK.  Full-fledged  random  quadtrees  with  n  nodes  have  expected  height 
o (log n) whenever X has a density (see, e.g., Devroye and Laforest (1990)). With 
k nodes, every region is thus reached in only  o (log k)  steps on the average. Fur(cid:173)
thermore, quadtrees enjoy the  same monotone transformation invariance that we 
observed for k-d trees and median trees.  0 

20.8  Best Possible Perpendicular Splits 

For computational reasons, classification trees are most often produced by deter(cid:173)
mining the splits recursively. At a given stage of the tree-growing algorithm, some 
criterion is used to determine which node of the tree should be split next, and where 
the split should be made. As these criteria typically use all the data, the resulting 
trees no longer have the X -property. In this section we examine perhaps the most 
natural criterion. In the following sections we introduce some alternative splitting 
criteria. 

A binary  classification tree  can be obtained  by  associating  with  each  node a 
splitting function ¢(x) obtained in a top-down fashion from the data. For example, 
at the root, we may select the function 

¢(x) = sx(i) - ex, 

where  i,  the  component  cut,  ex  E  n,  the  threshold,  and  s  E  {-I, + I},  a  po(cid:173)
larization,  are  all  dependent  upon  the  data.  The  root  then  splits  the  data  Dn  = 
{(Xl, Yd, ... , (Xn,  Yn)} into two sets, D~, D~, withD;1UD~ = Dn, ID~I+ID~I = n, 
such that 

D~  = 

{(x, Y)  E  Dn: ¢(x) ~ OJ, 

D;: 

{(x, y)  E  Dn  : ¢(x) <  O}. 

A  decision is made whether to  split a node or not,  and the procedure is  applied 
recursively to  the  subtrees.  Natural majority vote  decisions  are taken  at the leaf 
level. All such trees will be called perpendicular splitting trees. 

In Chapter 4, we introduced univariate Stoller splits, that is, splits that minimize 
the empirical error. This could be at the basis of a perpendicular splitting tree. One 
realizes  immediately that the  number of possibilities for  stopping is  endless. To 
name two,  we could stop  after k splitting nodes have been defined,  or we could 
make a tree with k  full levels of splits (so that all leaves are at distance k  from the 
root). We first show that for d  >  1,  any such strategy is virtually doomed to  fail. 
To  make this case, we will argue on the basis of distribution functions  only. For 
convenience, we consider a two-dimensional problem. Given a rectangle  R noW 

20.8 Best Possible Perpendicular Splits 

335 

assigned to one class y  E  {O,  I}, we see that the current probability of error in R, 
before splitting is P {X  E  R, Y  =I y}. Let R' range over all rectangles of the form 
R n((-oo, a] x  R), R n ([a, (0) x  R), R n (R x  (-00, aD, or R n (R x  [a, (0)), 
and  let  R" = R  - R'.  Then after asplit based upon (R',  R"),  the probability of 
error over the rectangle R  is 

P{X E  R', Y  =  I} + P{X E  R", Y = O} 

as we assign class ° to R' and class 1 to R". Call1::J.(R) the decrease in probability 

of error if we minimize over all R'. Computel::J.(R) for all leaf rectangles and then 
proceed to split that rectangle (or leaf) for which I::J.(R) is maximal. The data-based 
rule based upon this would proceed similarly, if P{A} is replaced everywhere by 
the  empirical estimate  (l/n) L.:7=1  I{(xi'yi)EAj,  where  A  is  of the  form  R'  x  {I}, 
R"  x  {O},  or R  x  {I  - y}, as the case may be. 

Let us denote by L o, L 1, L 2 ,  •.. the sequence of the overall probabilities of error 
for the theoretically optimal sequence of cuts described above. Here we start with 
R =  R2  and  y  =  0,  for  example.  For fixed  E  >  0,  we now  construct a  simple 
example in which 

L* = 0, 

and 

Lk  ,J..- - - as k  ---+  00. 

l-E 

2 

Thus,  applying the best split incrementally, even if we use the true probability of 
error as our criterion for splitting, is not advisable. 

The example is  very simple:  X  has uniform distribution on [0,  1]2  with proba(cid:173)

bility E  and on [1,2]2 with probability  1 - E.  Also,  Y  is a deterministic function 
of X, so that L * = 0. 

FIGURE 20.16.  Repeated  Stoller  splits  are  not 
consistent  in  this  two-dimensional  example. 
Cuts will always be made in the leftmost square. 

probability 

E 

o 

The way Y depends on X is shown in Figure 20.16:  Y =  1 if 

X  E  [1,3/2]2 U [3/2,2]2 U (A2  U A4 U A6 "  .)  x  [0,  1], 

336 

20.  Tree Classifiers 

where 

Al  = 

A2  = 

[0,  1/4), 
[1/4, 1/4 + 3/8), 

A3  = 

[1/4 + 3/8, 1/4 + 3/8 + 3/16), 

Ak 

[1/4 + 3/8 + ... + 3/2k

,  1/4 + 3/8 + ... + 3/2k+I

), 

and so  forth.  We  verify  easily that P{Y  =  I}  =  1/2. Also,  the  error probability 
before any  cut is  made is  Lo  =  1/2. The best split has  R'  =  (-00, 1/4) x  R  so 
that Al is  cut off. Therefore, LI = E/4 + (l - E)/2. We continue and split off A2 
and so forth, leading to the tree of Figure 20.17. 

FIGURE 20.17.  The  tree  obtained by 
repeated Stoller splits. 

yes 

o 

Verify that L2  = E/8 + (l - E)/2 and in general that 

1-E  1-E 
Lk - -+ - - 1_ -
' 

2"0/  2 

E 
2k+1 

-

as claimed. 

20.9  Splitting Criteria Based on Impurity Functions 

In 1984, Breiman, Friedman, Olshen, and Stone presented their CART program for 
constructing classification trees with perpendicular splits. One of the key ideas in 
their approach is the notion that trees  should be constructed from the bottom up, 
by combining small subtrees.  The starting point is  a tree with n + 1 leaf regions c 
defined by a partition of the space based on the n data points. Such a tree is much' 
too  large  and  is  pruned by  some methods  that will not be  explored here.  When 

20.9 Splitting Criteria Based on Impurity Functions 

337 

constructing a starting tree, a certain splitting criterion is applied recursively. The 
criterion  determines  which  rectangle  should  be  split,  and  where  the  cut  should 
be made.  To  keep  the  classifier invariant under monotone transformation of the 
coordinate axes, the criterion should only depend on the coordinatewise ranks of 
the points, and their labels. Typically the criterion is a function of the numbers of 

points labeled by ° and 1 in the rectangles after the cut is made. One such class of 
Let a  E n, and let i be a given coordinate (1  ::S  i  ::S  d). Let R be a hyperrectangle 
to be cut. Define the following quantities for a split at a, perpendicular to the i -th 
coordinate: 

criteria is described here. 

Xt(R, a) = {(X j , Yj ) : Xj  E  R,  XJi)  >  a} 

are the sets of pairs falling to the left and to the right of the cut, respectively. 

are the numbers of such pairs. Finally, the numbers of points with label ° and label 

1 in these sets are denoted, respectively, by 

Ni,oCR, a)  =  IXi(R, a) n {CX j , Yj ): Yj  = O}I, 
IXiCR, a) n {CX j , Yj ): Yj  = 1}1, 
Ni,lCR, a) 
!XicCR,a)n{CXj,Yj):Yj=O}!, 

N:'oCR,a)  = 

N(,l(R,a)  = 

I Xt(R,a)n{(xj ,Yj ):Yj =l}l· 

Following Breiman, Friedman, Olshen,  and Stone (1984), we  define an  impurity 
junction for a possible split (i, a) by 

Ii (R, a) = ljJ 

( 

NiO 

, 

Nil) 

Ni + ljJ 

NiO  + Nil  NiO + Nil 

(N!o  N:l)  I 

I '   I 

I  Ni ' 

I 

NiO  + Nil  NiO  + Nil 

where we dropped the argument (R, a) throughout. Here ljJ  is a nonnegative func(cid:173)
tion with the following properties: 

(1)  ljJ G, D 2':  ljJ(p,  1 - p) for any p  E  [0,  1]; 

(2)  ljJ(O,  1) = ljJ(1,  0) = 0; 

(3)  1f;'(p,  1 - p) increases in p on [0,  1/2] and decreases in  p on [1/2,1]. 

A rectangle  R  is  split  at  a  along  the  i-th  coordinate  if Ii(R, a) is  minimal.  Ii 
penalizes  splits in which the subregions have about equal proportions from both 
classes. Examples of such functions  ljJ  include 

(1)  The entropy junction 1f;'(p,  1- p) = - p log p - (1- p) 10g(1- p) (Breiman 

et al.  (1984, pp.25,103)). 

338 

20.  Tree Classifiers 

(2)  The Gini function  ljf(p,  1 - p)  =  2p(1  - p), leading to  the Gini index of 

diversity f i • (Breiman et al. (1984, pp.38,l03». 

(3)  The probability ofmisclassification t(p, 1 - p)  =  min(p, 1 - p). In this 
case the splitting criterion leads to the empirical Stoller splits studied in the 
previous section. 

We have two kinds of splits: 

(A)  Theforced split: force a split along the i-th coordinate, but minimize fiCa,  R) 

over all a  and rectangles  R. 

(B)  The free  split:  choose the most advantageous coordinate for  splitting, that 

is, minimize fiCa,  R) over all a, i, and R. 

Unfortunately, regardless of which kind of policy we choose, there are distributions 
for which no splitting based on an impurity function leads to a consistent classifier. 
To see this, note that the two-dimensional example of the previous section applies 
to all impurity functions.  Assume that we had infinite sample size. Then fiCa,  R) 
would approach aljf(p, 1- p)+ bt(q, 1- q), where p is the probability content of 
R' (one of the rectangles obtained after the cut is made), b is that of R" (the other 
rectangle),  p  = P{Y = 11X  E  R'} and q  = P{Y = llX  E  R"}. If X is  uniformly 
distributed on the checkerboard shown in Figure 20.18, regardless where we try 
to cut,  p = q = 1/2, and every cut seems to be undesirable. 

FIGURE 20.18.  No  cut  decreases  the  value  of the  impurity 
function. 

This simple example may be made more interesting by mixing it with a distribu(cid:173)

tion with much less weight in which x(l)- and x (2)-direction splits are alternatingly 
encouraged all the time.  Therefore, impurity functions should be avoided in their 
raw form for splitting. This may explain partially why in CART, the original tree is 
undesirable and must be pruned from the bottom up.  See Problems 20.22 to 20.24 
for more information. In the next section and in the last section of this chapter we 
propose other ways of growing trees with much more desirable properties. 

The derivation shown above does not indicate that the empirical version will not 

work properly, but simple versions of it will certainly not.  See Problem 20.22. 

REMARK.  MALICIOUS  SPLITTING.  The impurity functions suggested above all avoid 
leaving the proportions of zeros and ones intact through splitting. They push to(cid:173)
wards more homogeneous regions. Assume now that we do the opposite. Through 
such splits,  we can in fact create hyperplane classification trees  that are globally 
poor,  that is,  that are  such that every trimmed version of the  tree is  also  a poor 
classifier. Such splitting methods must of course use the Yi 'so  Our example shows 

20.9 Splitting Criteria Based on Impurity Functions 

339 

that any general consistency theorem for hyperplane classification trees must come 
with certain restrictions on the splitting process-the X  property is  good;  some(cid:173)
times it is necessary to force cells to shrink; sometimes the position of the split is 
restricted by empirical error minimization or some other criterion. 
The ham-sandwich theorem (see Edelsbrunner (1987»  states that given 2n class-
o points  and 2m  class-l  points in nd ,  d  >  1,  there exists a hyperplane cut that 
leaves n  class-O points and m  class-l points in each halfspace. So, assume that X 
has  a density and that  p  =  P{Y  =  I}  =  1/2. In a  sample of size n, let  y  be  the 
majority class (ties are broken in favor of class 0). 

FIGURE 20.19.  Ham-sandwich  cut:  Each  halfspace 
contains exactly half the points from each class . 

o 

• 
o 
• 

Regardless of the sample make-up, if y  = 0, we may construct a hyperplane-tree 
classifier in which, during the construction, every node represents a region in which 
the majority vote would be O. This property has nothing to do with the distribution 
of (X, Y), and therefore, for any trimmed version of the tree classifier, 

and P{Ln  ::::  p}  ::::  P{y  = O}  ::::  1/2 if p  =  1/2. Obviously, as we may take L * = 0, 
these classifiers are hopeless.  0 

BIBLIOGRAPHIC REMARKS. Empirical Stoller splits without forced rotation were rec(cid:173)
ommended by Payne  and Meisel (1977)  and  Rounds  (1980),  but their failure  to 
be  universally  good was  noted by Gordon  and  Olshen  (1978).  The last two  au(cid:173)
thors  recommended a splitting scheme that combined several  ideas, but roughly 
speaking, they perform empirical Stoller splits with forced rotation through the co(cid:173)
ordinate axes (Gordon and Olshen (1978), (1980». Other splitting criteria include 
the AID criterion of Morgan and Sonquist (1963), which is a predecessor of the Gini 
index of diversity used in CART (Breiman, Friedman, Olshen, and Stone (1984), see 
also Gelfand and Delp (1991), Guo and Gelfand (1992) Gelfand, Ravishankar, and 
Delp (1989), (1991), and Ciampi (1991». Michel-Briand and Milhaud (1994) also 
observed the failure of multivariate classification trees based on the AID criterion. 
The Shannon entropy or modifications of it are recommended by Talmon (1986), 
Sethi  and  Sarvarayudu  (1982),  Wang  and  Suen  (1984),  Goodman  and  Smyth 
(1988), and Chou (1991). Permutation statistics are used in Li and Dubes (1986), 
still without forced rotations  through the  coordinate axes.  Quinlan (1993) has a 
more  involved  splitting criterion.  A  general  discussion  on tree  splitting  may  be 

340 

20.  Tree Classifiers 

found  in  the  paper by  Sethi  (1991).  A  class  of impurity  functions  is  studied in 
Burshtein, Della Pietra, Kanevsky, and Nadas (1992). Among the pioneers of tree 
splitting (with perpendicular cuts) are Sebestyen  (1962),  and Henrichon and Fu 
(1969). For related work, we refer to Stoffel (1974), Sethi and Chatterjee (1977), 
Argentiero, Chin and Beaudet (1982), You andFu (1976), Anderson andFu (1979), 
Qing-Yun and Fu (1983), Hartmann, Varshney,  Mehrotra, and Gerberich (1982), 
and Casey  and Nagy  (1984).  References  on nonperpendicular splitting methods 
are given below in the section on ESP trees. 

20.10  A Consistent Splitting Criterion 

There is no  reason for pessimism after the previous sections. Rest assured,  there 
are several consistent splitting strategies that are fully automatic and depend only 
upon the populations of the regions.  In this  section we provide a solution for the 
simple case when X is  univariate and nonatomic.  It is possible to  generalize the 
method for d  >  1 if we force cuts to alternate directions. We omit here the detailed 
analysis for multidimensional cases for two reasons. First, it is significantly heavier 
than for d  = 1.  Secondly,  in the last section of this  chapter we introduce a fully 
automatic way of building up consistent trees, that is, without forcing the directions 
of the splits. 

To a partition AI, ... , AN of R, we assign the quantity 

N 

Q = L No(Ai )NI (Ai), 

i=1 

where 

n 

No(A) = L I{XjEA,Yj=Oj, 

j=l 

n 

NI (A) = L I{XjEA,Yj=l} 

j=l 

are the respective numbers of points labeled with 0 and 1 falling in the region A. 
The tree-growing algorithm starts with the trivial partition {R}, and at each step it 
makes  a cut that yields the minimal value of Q. It proceeds recursively until the 
improvement in the value of Q falls below a threshold ~n. 
REMARK.  Notice that this criterion always splits a cell that has many points from 
both classes  (see the proof of the theorem below). Thus, it avoids the anomalies 
of impurity-function criteria described in the previous section. On the other hand, 
it  does  not  necessarily  split large  cells,  if they  are  almost homogeneous.  For a 
comparison, recall that the Gini criterion minimizes the quantity 

,  ~ No(Ai )Nl (Ai) 
o  =~ 

i=1  No(Ai) + NI (Ai)' 

-

thus favoring  cutting cells  with very  few  points.  We  realize that the criterion  Q 
introduced  here  is  just one  of many  with  similarly  good  properties,  and  albeit 
probably imperfect, it is certainly one of the simplest.  0 

20.11 BSP Trees 

341 

Theorem 20.6.  Let X have a nonatomic distribution on the real line, and consider 
the  tree  classifier  obtained by  the  algorithm  described  above.  If the  threshold 
satisfies 

-+ 0, 

~n 
-
n 

~n 
-+  (Xl  and  2 
n 

then the classification rule is strongly consistent. 

PROOF.  There are two key properties of the algorithm that we exploit: 

PROPERTY  1.  If min(No(A), NI (A))  >  V2~n for a cell A, then A gets cut by the 
algorithm.  To see this,  observe that (dropping the argument A  from the notation) 
if No  :::  N 1,  and we cut A so that the number of O-labeled points in the two child 
regions differ by at most one, then the contribution of these two new regions to  Q 
is 

r No/2l N{  + LNo/2J N~' :s  r NoNd2l, 

where N{  and N{'  are the numbers of class-1 points in the two child regions. Thus, 
the decrease of Q if A is split is at least  LNoNI/2J. If min(No, Nd >  J2~n' then 
D.n  :s  LNoNI/2J, and a cut is made. 
PROPERTY  2.  No leaf node has less than  ~nl n  points. Assume that after a region 
is cut, in one of the child regions, the total number of points is  N~ + N{  :s  k.  Then 
the improvement in Q caused by the split is bounded by 

NoNl  - (NbN~ + N~ N~') S  NoNl  - (No  - k)(Nl  - k)  S (No + N1)k  S nk. 

Therefore, if k  <  ~n In, then the improvement is smaller than ~n' Thus, no cut is 
made that leaves behind a child region with fewer than ~n I n points. 

It follows  from  Property  1 that if a leaf region has  more than 4V2~n points, 
then  since  the  class  in  minority has less  than  J2~n points  in it,  it may be  cut 
into intervals containing between 2J2~n and 4J2~n points without altering the 
decision, since the majority vote within each region remains the same. 

Summarizing,  we  see  that  the  tree  classifier is  equivalent  to  a  classifier  that 
partitions the real line into intervals, each containing at least  ~nln, and at most 
4J2~n data points.  Thus, in this partition, each interval has a number of points 
growing to infinity as o(n). We emphasize that the number of points in a region of 
the studied tree classifier may be large, but such regions are almost homogeneous, 
and therefore the classifier is equivalent to another classifier which has o(n) points 
in each region.  Consistency of such partitioning classifiers is  proved in the next 
chapter-see Theorem 21.3.  D 

20.11  BSP Trees 

Binary  space partition trees  (or BSP  trees)  partition Rd by hyperplanes.  Trees of 
this nature have evolved in the computer graphics literature via the work of Fuchs, 

342 

20.  Tree Classifiers 

Kedem, and Naylor (1980) and Fuchs, Abram, and Grant (1983) (see also Samet 
(1990b), Kaplan (1985), and Sung and Shirley (1992». In discrimination they are 
at  the  same  time  generalizations  of linear  discriminants,  of histograms,  and  of 
binary tree classifiers.  BSP trees  were recommended for use in discrimination by 
Henrichon and Fu (1969), Mizoguchi, Kizawa, and Shimura (1977) and Friedman 
(1977). Further studies include Sklansky and Michelotti (1980), Argentiero, Chin, 
and Beaudet (1982),  Qing-Yun  and Fu (1983),  Breiman, Friedman,  Olshen,  and 
Stone (1984), Loh and Vanichsetakul (1988), and Park and Sklansky (1990). 

There  are  numerous  ways  of constructing  BSP  trees.  Most methods  of course 
use the  Y -values to  determine good splits.  Nevertheless, we should mention first 
simple splits with the X -property, if only to better understand the BSP trees. 

1,2 

FIGURE 20.20.  A  raw  BSP  tree  and its  induced partition  in  the plane.  Every 
region is split by a line determined by the two data points with smallest index 
in the region. 

We call the raw BSP tree the tree obtained by letting Xl, ... ,  Xd  determine the 
first hyperplane. The d  data points remain associated with the root, and the others 
(Xd+l, ... , X k) are sent down to the subtrees, where the process is repeated as far 
as possible. The remaining points Xk+l, ... , Xn  are used in a majority vote in the 
external regions. Note that the number of regions is not more than kid. Thus, by 
Lemma 20.1, if kin  -+  0,  we  have  N(X)  -+  00 in probability.  Combining this 
with Problem 20.25, we have our first result: 

Theorem 20.7.  The natural binary tree classifier based upon a raw BSP tree with 
k  -+  00 and kin -+ 0 is consistent whenever X  has a density. 

Hyperplanes may also be selected by optimization of a criterion. Typically, this 
would involve  separating the classes in some way.  All that was  said for  perpen~ 
dicular splitting remains valid here. It is worthwhile recalling therefore that there 

20.12 Primitive Selection 

343 

are  distributions  for  which the best empirical Stoller split does  not improve the 
probabilit~ of error.  Take for example the uniform distribution in the unit ball of 
nd in whIch 

y  = { 1  if IIXII  2: 1/2 
o  if IIXII  <  1/2. 

FIGURE 20.21.  No single split improves on the error prob(cid:173)
ability for this distribution. 

Here,  no  linear  split would be helpful  as  the  l' s  would always  hold a  strong 
majority.  Minimizing other impurity functions  such as  the Gini criterion may be 
helpful, however (Problem 20.27). 

BIBLIOGRAPHIC REMARKS. Hyperplane splits may be generalized to include quadratic 
splIts (Henrichon and Fu (1969)). For example, Mui and Fu (1980) suggest taking 
d'  <  d and forming quadratic classifiers as in normal discrimination (see Chapter 
4)  based upon vectors in R d'.  The cuts are thus perpendicular to d  - d' axes but 
quadratic in the subspace R d
tance or 2-means clustering for determining splits. As a novelty, within each leaf 
region, the decision is not by majority vote, but rather by a slightly more sophisti(cid:173)
cated rule such as the k-NN rule or linear discrimination. Here, no optimization is 
required along the way.  Loh and Vanichsetakul (1988) allow linear splits but use 
F ratios to select desirable hyperplanes. 

'.  Lin and Fu (1983) employ the Bhattacharyya dis(cid:173)

20.12  Primitive Selection 

There are two reasons for optimizing a tree configuration. First of all, it just does 
not make  sense to  ignore the  class  labels  when constructing a  tree  classifier,  so 
the  Yi'S  must be used to help in the selection of a best tree.  Secondly, some trees 
may not be consistent (or provably consistent), yet when optimized over a family 
of trees,  consistency drops out.  We  take the following example: let (;h  be a class 
of binary tree classifiers with the X -property, with the space partitioned into k + 1 
regions determined by Xl, ... , Xk  only.  Examples include the chronological k-d 
tree and some kinds of BSP trees. We estimate the error for g  E  9k  by 

realizing the danger of using the same data that were used to obtain majority votes 
to  estimate  the  error.  An optimistic  bias  will  be introduced.  (For  more  on such 
error estimates, see Chapter 23.) Let g~ be the classifier (or one of the classifiers) 
in (h  for which Ln  is  minimum. Assume that 19k1  <  oo-for example, 9k  could 

344 

20.  Tree Classifiers 

be all k!  chronological k-d trees  obtained by permuting  Xl, ... , Xk.  We  call g~ 
then the permutation-optimized chronological k-d classifier. When k =  L jIOgnJ, 
one can verify that k!  = o(nE) for any E  >  0, so that the computational burden-at 
least on paper-is not out of sight. We  assume that 9k  has a consistent classifier 
sequence, that is,  as n  -+  00, k  usually grows unbounded, and E{Ln(gk)}  -+  L* 
for a sequence gk  E  9k. 
EXAMPLE.  Among the chronological k-d trees modulo permutations, the first one 
(i.e., the one corresponding to the identity permutation) was shown to be consistent 
in Theorem 20.3, if X has a density, k  -+  00, and kin -+ o.  0 
EXAMPLE. Let 9k be the class of BSP trees in which we take as possible hyperplanes 
for splitting the root: 

(1) 
(2) 
(3) 

the hyperplane through X I,  ... ,  X d; 
the d hyperplanes through X I,  ... ,  X d - l  that are parallel to one axis; 
the  (~) hyperplanes through X I,  ... ,  Xd-2 that are parallel to two axes; 

(d) 

the  C~l) = d hyperplanes through Xl that are parallel to d  - 1 axes. 

Thus, conservatively estimated, 19k I :::  k !2d because there are at most 2d  possible 
choices  at each node  and there are k!  permutations of X I,  ... ,  X k.  Granted, the 
number of external regions  is  very  variable, but it remains bounded by k + 1 in 
any case. As 9k  contains the chronological k-d tree, it has  a consistent sequence 
of classifiers when k  -+  00 and nl(k log k)  -+  00.  0 
Theorem 20.8.  Let gl~ = argmingE9k  Ln(g).  Then,  if9k has a consistent sequence 
of classifiers,  if the number of regions in the partitions for all g  E  9k are at most 
k + 1,  andifk -+  00 and nl log 19k1-+  00,  then E{Ln(g~)} -+  L*,  where Ln(g~) 
is the conditional probability of error of g~. 

In the examples cited above, we must take k  -+  00, n I k  -+  00. Furthermore, 
log 19k I = o (log kl) = O(k log k) in both cases.  Thus,  gl~ is consistent whenever 
X has a density, k -+  00 and k  = o(nl log n). This is a simple way of constructing 
a basically universally consistent BSP tree. 
PROOF. Let g+  = arg mingE9k  Ln (g). 

Ln(g/:) - L *  =  Ln(g~) - Ln(g~) 

+ Ln(g~) - ~l(g+) 
+ Ln(g+) - Ln(g+) 

+Ln(g+) - L*. 

Clearly, 

def 

I  + I I. 

Obviously,  I  -+ 0 in the mean by our assumption, and for E  >  0, 

20.12 Primitive Selection 

345 

Next we bound the probabilities on the right-hand side. Let Pw,  Pil denote P{X E 
region i, Y = O}  and P{X E  region i, Y =  1} respectively, with regions determined 
by g,  1 ::::  i  ::::  k + 1. Let 

n 

Nw  = L  I{XjEfegion i,Yj=O} 

j=k+l 

and  Nil  = L  I{XjEregion  i,Yj=I}' 

n 

j=k+l 

Then 

and 

Thus, 

Ln(g) = L PilI{Nio~Nid + L PWI{Nio<Nid, 

k+l 

k+1 

i=1 

i=1 

ILn(g) - Ln(g)1  ::::  L  Pil  - _i_l  + L  PiG  __  iO_ 

I 

I 
. 

N 
n - k 

k+1  I 
i=l 

k+1  I 
i=l 

N 
n  - k 

Introduce  the  notation  Z  for  the random variable  on the  right-hand  side  of the 
above inequality. By the Cauchy-Schwarz inequality, 

k+l 

<  L 

i=1 

k+1 +L 

i=l 

E {(PiO - n~Okr IX I , . ,  xc} 

(as given Xl,""  Xb Nil  is binomial (n  - k,  Pil)) 

<  ~ (J nP~\ + J nP~Ok) 

< 

2k+2 
rn=k 
(by another use of the Cauchy-Schwarz inequality) 

J2k+2 
n-k 

346 

20.  Tree Classifiers 

Thus, E{ZIXl, ... , X k }  -+ 0 as kin -+ O.  Note that 

P{Z >  EIX1,  ... ,  Xd 

<  P { Z  - E{ZIXI, ... ,Xk} >  ~ I Xl, ... , Xk} 

E 
(if E{ZIXI' ... , Xk }  <  E/2, which happens when  - - : : :  -) 
2 

J2k+2 
n -k 

< 

exp (- (~)2 /2(n _  k) 

2 

4  2) 
(n  -k) 

(by McDiarmid's inequality, as changing the value of an X j, j  :::::  k, 

changes  Z by at most 2/(n - k);  see Theorem 9.2) 

= 

exp 

( 

(n  - k)E2) 

32 

. 

Thus, taking expected values, we see that 

if  J(2k + 2)/(n - k)  <  E/2.  This  tends  to  0  for  all  E  >  0  if kin  -+  0  and 
nllog 19k1  -+  00.0 

20.13  Constructing Consistent Tree Classifiers 

Thus far, we have taken you through a forest of beautiful trees and we have shown 
you a few  tricks of the trade.  When you read (or write)  a research paper on tree 
classifiers, and try to directly apply a consistency theorem, you will get frustrated 
however-most real-life  tree  classifiers  use  the  data  in  intricate  ways  to  suit a 
certain  application.  It really  helps  to  have  a  few  truly  general  results  that  have 
universal  impact.  In  this  section  we  will  point  you  to  three  different  places  in 
the book where you may find  useful results in this respect.  First of all,  there is a 
consistency theorem-Theorem 21.2-that applies to rules that partition the space 
and decide by majority vote. The partition is arbitrary and may thus be generated by 
using some or all of Xl, Y1,  ••• ,  X n ,  Yn . If a rule satisfies the two (weak) conditions 
of Theorem 21.2, it must be universally consistent. To  put it differently,  even the 
worst rule within the boundaries of the theorem's conditions  must perform well 
asymptotically. 

Second, we will briefly discuss the design of tree classifiers obtained by mini(cid:173)
mizing the empirical error estimate (in) over possibly infinite classes of classifiers. 
Such classifiers, however hard to find by an algorithm, have asymptotic properties 
that are related to the vc dimension of the class of rules. Consistency follows al(cid:173)
most without work if one can calculate or bound the vc dimension appropriately. 

20.13 Constructing Consistent Tree Classifiers 

347 

While Chapters 13 and 21 deal in more detail with the vc dimension, it is necessary 
to give a few examples here. 

Third,  we point the reader to Chapter 22 on data splitting,  where the previous 
approach is applied to the minimization. of the holdout estimate, obtained by trees 
based upon part of the sample, and using another part to select the best tree in the 
bunch. Here too the vc dimension plays a crucial role. 

Theorem 21.2 allows space partitions that depend quite arbitrarily on all the data 
and  extends  earlier universally  applicable results  of Gordon and Olshen (1978), 
(1980), (1984), and Breiman, Friedman, Olshen, and Stone (1984). A particularly 
useful format is given in Theorem 21.8. If the partition is by recursive hyperplane 
splits  as  in BSP  trees and the number of splits  is  at most m n, if mn log n I n  -+  0, 
and if 

diam(A(X) n SB)  -+ 0 

with probability one for  all  SB  (where SB  is  the ball of radius  B  centered at the 
origin), then the classification rule is strongly consistent. The last condition forces 
a randomly picked region in the partition to be small. However, mn log n I n  -+  0 
guarantees  that no  devilish partition can be inconsistent.  The  latter condition is 
certainly satisfied if each region contains at least kn points, where knl log n  -+  00. 
Next  we  take  a  look  at  full-fledged  minimization  of Ln ,  the  empirical  error, 
over certain classes of tree classifiers. Here we are not concerned with the (often 
unacceptable) computational effort.  For example, let Qk  be the class of all binary 
tree classifiers based upon a tree consisting of k internal nodes, each representing 
a hyperplane cut (as  in the BSP tree),  and all  possible 2k+l  labelings of the  k + 1 
leaf regions. Pick such a classifier for which 

is minimal. Observe that the chosen tree is always natural; that is, it takes majority 
votes over the leaf regions. Thus, the minimization is equivalent to the minimization 
of the resubstitution error estimate (defined in Chapter 23) over the corresponding 
class  of natural tree classifiers.  We  say that a  sequence  {Qk}  of classes is  rich if 
we  can  find  a  sequence  gk  E  Qk  such that  L(gk)  -+  L *.  For hyperplanes,  this 
is the case if k  -+  00 as n  -+  oo-just make the hyperplane cuts form a regular 
histogram grid and recall Theorem 9.4. Let S(Qb n) be the shatter coefficient of Qk 
(for a definition, see Chapter 12, Definition 12.3). For example, for the hyperplane 
family, 

S(Qk, n) .:s  nk(d+l). 

Then  by  Corollary  12.1  we have E{Ln(g~)} -+  L * for  the  selected classifier g~ 
When Qk  is rich (k  -+  00 here) and log S(Qk, n) = o(n), that is, 

k=o  - -
) 
logn 

n 

( 

. 

348 

20.  Tree Classifiers 

Observe that no conditions are placed on the distribution of (X, Y) here!  Consis(cid:173)
tency follows from basic notions-one combinatorial to keep us from overfitting, 
and one approximation-theoretical (the richness). 

The above result remains valid under the same conditions on k in the following 

classes: 

(1)  All trees based upon k internal nodes each representing a perpendicular split. 

(Note:  S(~h, n)  ::::  ((n + l)d)k.) 

(2)  All trees  based upon k internal nodes,  each representing  a  quadtree  split. 

(Note: S((;h, n)  ::::  (n d  + ll.) 

20.14  A Greedy Classifier 

In this section, we define simply a binary tree classifier that is grown via optimiza(cid:173)
tion of a simple criterion. It has the remarkable property that it does  not require 
a forced  rotation through the coordinate axes or special safeguards against small 
or large regions or the like. It remains entirely parameter-free (nothing is picked 
by the user), is monotone transformation invariant, and fully automatic. We show 
that in nd it is always consistent. It serves as a prototype for teaching about such 
rules and should not be considered as more than that. For fully practical methods, 
we believe, one will have to tinker with the approach. 

The space is partitioned into rectangles as shown below: 

-

CJ 

3 6rn  5 

FIGURE 20.22.  A  tree  based  on  partitioning  the  plane  into  rectangles.  The 
right subtree of each internal node belongs to  the  inside of a  rectangle,  and 
the  left subtree belongs to  the  complement of the same rectangle  (ic  denotes 
the complement of i). Rectangles are not allowed to overlap. 

A hyperrectangle  defines  a  split in  a  natural  way.  The  theory  presented here 
applies for many other types of cuts. These will be discussed after the main con(cid:173)
sistency theorem is stated. 

20.14 A Greedy Classifier 

349 

A partition is denoted by p, and a decision on a set A  E  P  is by majority vote. 

We write gp for such a rule: 

gp(x) = 
{ 

1 
0 

if  L  Yi  >L (1  - Yi ), x  E  A 

i:X;EA 

otherwise. 

i:X;EA 

Given a partition p, a legal rectangle A  is one for which A n B  = 0 or A  S;  B  for 
all sets  B  E  P. If we refine P  by adding a legal rectangle T  somewhere, then we 
obtain the partition T. The decision gy agrees with gp  except on the set B  E  P 
that contains T. 

We introduce the convenient notation 

=  P{X  E  A, Y =  }},  j  E  {a,  I}, 

An estimate of the quality of gp is 

where 

1  n - L I{X;ER,gp(XJ=!Y;} 
=  min(vo,n(R), vI,n(R». 

n  i=I 

Here we use two different arguments for Ln  (R and P), but the distinction should 
be clear. We may similarly define Ln(T). Given a partition p, the greedy classifier 
selects that legal rectangle T  for  which Ln(T) is  minimal (with any  appropriate 
policy  for  breaking  ties).  Let  R  be the  set of P  containing  T.  Then the  greedy 
classifier picks that T  for which 

is  minimal.  Starting with the trivial partition Po  = {n d
},  we repeat the previous 
step k times, leading thus to k + 1 regions. The sequence of partitions is denoted 
by Po, PI, ... , Pk. 

We put no safeguards in place-the rectangles are not forced to shrink. And in 
fact,  it is easy to construct examples in which most rectangles do not shrink. The 
main result of the section, and indeed of this chapter, is that the obtained classifier 
is consistent: 
Theorem 20.9.  For  the  greedy  classifier with k  -+  00  and k  = 0  ( J n /  log n ), 
assuming  that X  has nonatomic marginals,  we have  Ln  -+  L * with probability 
one. 

350 

20.  Tree Classifiers 

REMARK.  We note that with techniques presented in the next chapter, it is possible 
to improve the second condition on k to k ==  0  ( vi n / log n )  (see Problem 20.31). 
o 

Before proving the theorem,  we mention that the same argument may be used 
to establish consistency of greedily grown trees with many other types of cuts. We 
have seen in Section 20.8 that repeated Stoller splits do not result in good classifiers. 
The reason  is  that  optimization  is  over  a  collection  of sets  (halfspaces)  that is 
not guaranteed to improve matters-witness the examples provided in previous 
sections. A  good cutting method is one that includes somehow many (but not too 
many) small sets. For example, let us split at the root making d + 1 hyperplane cuts 
at once, that is, by finding the d + 1 cuts that together produce the largest decrease 
in the empirical error probability. Then repeat this step recursively in each region 
k times. The procedure is consistent under the same conditions on k as in Theorem 
6.1,  whenever  X  has  a  density  (see Problem 20.30).  The d  + 1 hyperplane cuts 
may be considered as  an  elementary cut which is  repeated in a  greedy manner. 
In Figures 20.23  to  20.25  we show a  few  elementary cuts  that may be repeated 
greedily for a consistent classifier. The straightforward proofs of consistency are 
left to the reader in Problem 20.30. 

PROOF  OF  THEOREM  20.9.  We restrict  ourselves  to  n 2
,  but  the  proof remains 
similar in n d  (Problem 20.29). The notation Ln (·)  was introduced above,  where 
the argument is allowed to be a partition or a set in a partition. We similarly define 

L(R) 

min  P{X  E  R, Y  iy} 
YE{O.I} 

L(P) 

=  min(vo(R), vI(R)), 

L L(R). 

REP 

FIGURE 20.23.  An elementary  cut here 
is  composed of d  + 1 hyperplane cuts. 
They are jointly optimized. 

FIGURE 20.24.  2d rectangular cuts de-
termine an elementary cut.  All 2d cuts, 
have arbitrary directions;  there are no 
forced directions. 

20.14 A Greedy Classifier 

351 

2 

1 

FIGURE 20.25.  Simplex  cuts.  A  cut  is 
determined by a polyhedron with d + 1 
vertices.  The simplices are not allowed 
to  overlap, just as for rectangular cuts 
in the greedy classifier.  Three consecu-
tive simplex cuts in R2 are shown here.  mal.  Two  consecutive rectangular grid 

FIGURE 20.26.  At every iteration we re(cid:173)
fine  the partition by selecting that rect(cid:173)
angle  R  in  the partition  and that 3  x 
3 x  ... x  3 rectangular grid cut of R 
for which  the  empirical error  is  mini(cid:173)

cuts are shown. 

For example, let (;h  denote a partition of R 2  into a rectangular l  x  I grid. 

FIGURE 20.27.  91: An I x I grid (with I = 7 here). 

It is clear that for all E >  0,  there exists an I = I (E)  and an I  x  I  grid 91  such that 

If Q is another finer partition into rectangles (i.e., each set of Q is a rectangle and 
intersects at most one rectangle of 91), then necessarily 

L(Q)::::  L(91):::: L* +E. 

We will call Q a refinement of 91. The next lemma is a key property of partitioning 
classifiers. In our eyes, it is the main technical property of this entire chapter. We 
say that the partition T  is an extension of P  by a set  Q-where Q  ~ REP-if 
T  contains all cells of P  other than R, plus  Q  and R - Q. 

352 

20..  Tree Classifiers 

Lemma 20.2.  Let Oz  be a finite partition with  L (Oz)  ::::  L * + E.  Let P  be a finite 
partition oiRd ,  and let  Q  be a  refinement oiboth P  and Oz.  Then  there  exists a 
set Q  E  Q (note:  Q is contained in one set oiP only) and an extension oiP by Q 
to TQ  such that,  if L(P) :::  L * + E, 

PROOF. First fix  REP and let Q 1,  ... ,  Q N  be the sets of Q contained in R. Define 

i=l 

L(R) = min(p, q), 

First we show that there exists an integer i  such that 

L(R) - L(Q,) - L(R - Q,) ":  L(R) -"7;':[ L(Q,), 

or equivalently, 

where Il i  = min(p, q) - min(pi, qi) - min(p - Pi, q  - qi). To see this, assume 
without loss of generality that P  ::::  q. If Pi  ::::  qi  for all i, then 

min(p, q) - I: min(Pi , qi) = P - I: Pi  = 0, 

N 

N 

l=l 

i=l 

so we  are  done.  Assume therefore  that Pi  >  qi  for  i  E  A,  where  A  is  a  set of 
indices with I A I :::  1.  For such i, 

and thus, 

I: Ili  = I:(pi - qJ = P - I: Pi  - I:qi = P - Lmin(pi' qJ. 

N 

iEA 

lEA 

i¢.A 

iEA 

i=l 

But then, 

AIL A 
IAI  iEA 

I 

max  u' >  -
lSiSN 

-

u' > 

I 

-

P - L~l min(pi, qi) 

IAI 

P - L~l min(pi' qi) 
>  ----'-...:...-----
-
' 

N 

and the  claim follows.  To  prove the lemma,  notice that since L(Q)  :::  L * + E,  it 
suffices to show that 

20.14 A Greedy Classifier 

353 

max(L(P) - L(TQ» ::: 

QEQ 

L(P) - L(Q) 
' 

Q 
I  I 

or equivalently, that 

max(L(R Q)  - L(Q) - L(R Q  - Q» 
QEQ 

::: 

L(P) - L(Q) 
' 

Q 
I  I 

where RQ  is the unique cell of P containing  Q. However, 

max(L(R Q )  - L(Q) - L(R Q  - Q» 
QEQ 
> 

max  (L(R) - L(Q) - L(R - Q» 

REP,Q<:;R 

> 

> 

= 

L(R) - LQCR L(Q) 

-

IRI 

max 
REP 
(by the inequality shown above, where I R I denotes 
the number of sets of Q contained in R) 

LREP (L(R) - LQ<:;R L(Q») 

LREP IRI 

LREP L(R) - LQEQ L(Q) 

IQI 
L(P) - L(Q) 

IQI 

and the lemma is proved.  0 

Let us return to the proof of Theorem 20.9. The previous lemma applied to our 
situation (P = Pi) shows that we may extend Pi by a rectangle  Q  E  Q (as  Q will 
(L * + E) is 
be a collection of rectangles refining both Qz  and Pi) such that L(TQ)  -
smaller by a guaranteed amount than L(Pi ) - (L * + E). (TQ is the partition obtained 
by extending Pi.) To  describe  Q, we do the following  (note that  Q  must consist 
entirely  of rectangles for otherwise the lemma is useless):  take the rectangles of 
Pi  and extend all four sides (in the order of birth of the rectangles) until they hit a 
side of another rectangle or an extended border of a previous rectangle if they hit 
anything at all. Figure 20.28 illustrates the partition into rectangles. Note that this 
partition consists of 4i line segments, and at most 9i rectangles (this can be seen 
by noting that we can write in each non original rectangle of Pi the number of an 
original neighboring rectangle, each number appearing at most 8 times). 

354 

20.  Tree Classifiers 

FIGURE 20.28.  Extensions  of rectangles 
to get a rectangular grid. 

The rectangular grid partition thus obtained is  intersected with 91  to yield  Q. To 
apply Lemma 20.2, we only need a bound on I QI. To the rectangular partition just 
created,  we add each of the 2l - 2 lines of the grid 91  one by one.  Start with the 
horizontal lines. Each time one is added, it creates at most 9i new rectangles. Then, 
each vertical line adds  at most 9i + 1 new rectangles for a total of not more than 
9i + 9i(l- 1) + (l  - 1)(9i + l) :s l2  + 18li (assuming 1 ::::  1). Hence, 

I QI  :s  l2  + 18li. 

Apply  the  lemma,  and  observe  that  there  is  a  rectangle  Q  E  Q  such  that  the 
extension of Pi by  Q to P; would yield 

At this  point in the proof, the reader can safely forget  Q. It has done what it was 
supposed to do. 

Of course, we cannot hope to find P[ because we do not know the distribution. 
Let us denote the actual new partition by Pi+1 and let Ri+l be the selected rectangle 
by the empirical minimization. Observe that 

L(Pi+d - L(P;) 

(L(Pi+d - Ln(Pi+d) + (Ln(Pi+1)  - Ln(P;)) + (Ln(P;) - L(P;)) 
(L(Pi+l) - Ln(Pi+1)) + (Ln(P[) - L(P;)) . 

< 

As  Pi +1  and n;  have  most sets  in  comm~, many  terms  cancel in the  last dou(cid:173)
ble  sum.  We  are  left  with  just  L(·)  and  Ln (.)  terms  for  sets  of the  form  R  -
Ri+1,  Ri+l, R - Q,  Q with R  E  Pi. Thus, 

L(Pi+1)  - L(P;) :s  2 sup ILn(R') - L(R')I, 

R' 

where  the  supremum  is  taken  over  all  rectangles  of the  above  described  form. 
These are  sets  of the  form  obtainable in Pi +1•  Every  such  set  can be written  as 
the difference of a (possibly infinite) rectangle and at most i + 1 nonoverlapping 
contained rectangles. As i  + 1 :s k in our analysis, we call Zk the class of all sets 

20.14 A Greedy Classifier 

355 

... - Zk,  where  Zo,  Zl, ... , Zk 
that may be described in this  form:  Zo  - Zl  -
are rectangles, each Zi  ~ Zo,  Zl, ... , Zk  are mutually disjoint. Rectangles may 
be infinite or half infinite. Hence, for i  <  k, 

L(Pi+1) - L(F) :::;  2  sup  ILn(Z) - L(Z)I. 

ZEZk 

FOf a fixed  Z  E  Zk, 

l£n(Z) - L(Z)I 

1 min(vo,n(Z), V1,n(Z» 

min(vo(Z), v1(Z»1 

< 

IVo,n(Z)  - vo(Z)1  + IV1,n(Z)  - v1(Z)I· 

Thus, 

L(Pi+1)  - L(  J:::;  Vn  = 2  sup 

P' 

def 

( 

IVo,n(Z)  - vo(Z)1  + IV1,n(Z) - v1(Z)1 
) 

. 

Define the (good) event 

ZEZk 

G  = {Vn  <  8}, 

where 8  >  0 will be picked later.  On G, we see that for all i  <  k, 

L(Pi+1)  - (L * + E) 

<  L(P;) - (L * + E) + 8 

(L(Pi) - (L * + E») (1 

< 

2  1 ) + 8 

l  + 18h 

(Lemma 20.2). 

We now introduce a convenient lemma. 
Lemma 20.3.  Let an, bn be sequences of positive numbers with bn t  0,  bo <  1, 
and let 8  >  0 be fixed.  If an+1  :::;  an (1  - bn) + 8,  then 

PROOF. We have 

an+! 

:'0 

an+1  :::;  aoe-EJ=ob j  +8(n + 1). 

ao 0(1- bj )  + 8 (~D(l -bj )) + 8 

I  + I I  + I I I. 

Clearly,  I  :::;  ao exp ( - L~=o b j ). A trivial bound for I I  + I I I  is 8(n + 1).  0 

Conclude that on G, 

e 

1/(l2+1Slu)du  + 8k 

< 
<  e- fs71og(l2+1Sl(k-1»/(l2»  + 8k 

1 

------:-:-:-:-::-:-:- + 8k. 
1+-l-

lS(k-1») l/(lSl) 

( 

356 

20.  Tree Classifiers 

Thus, 

P{L(Pk) >  L * + 2E}  :::s  P{ Ge

}  ==  P{V,1  :::  o} 

when ok  <  E/2 and 

1 

E 
-------:-:-::-:-:::-::-<-. 
2 

l8(k_l))1/(l8/) 

1+-/-

( 

Finally, introduce the notation 

Ln(R) 

Ln(P) 

==  P{X E  R, Y  i  gp(X)}, 

L  Ln(R). 
REP 

As  L(Pk) takes the partition into account, but not the majority vote,  and Ln(Pk) 
does, we need a small additional argument: 

Ln(Pk) 

(Ln(Pk) - Ln(Pk»)  + (Ln(Pk) - L(Pk») + L(Pk) 

< 

2  L 

(Ivo(R) 

vO,n(R)1  + IVI (R) - Vl,n(R)I)  + L(Pk) 

REPk 

Putting things together, 

P {Ln(Pk)  >  L * + 4E} 

<  P {Wn >  E}  + P {L(Pk)  >  L * + 2E} 
<  P {Wn  >  E} + P {Vn  :::  o} 

under the given conditions on 0 and k.  Observe that if for a set Z, 

def 

Un(Z)  ==  Ivo(Z)  - vO,n(Z)1  + IVl(Z) - vl,n(Z)I, 

then 

Vn  < 

sup  Un(Z) + 

sup 

rectangles  Z 

disjoint sets of k 
rectangles  ZI, ... ,Zk 

k 
LUn(Zi) 
i=l 

< 

(k + 1) 

sup  U/1(Z), 

rectangles  Z 

Wn  < 

(k + 1) 

sup  Un(Z). 

rectangles  Z 

We may now use the Vapnik-Chervonenkis inequality (Theorems  12.5 and 13.8) 
to conclude that for all E  >  0, 

P { 

sup  Un(Z)  >  E}  :::s  16n2de-nE2/32. 

rectangles  Z 

Problems and Exercises 

357 

Armed with this, we have 

P{Wn  >  E}  < 

16n2de -n(2(k~1)//32, 

P{Vn  ::::  o}  < 

16n2d e --n(2(LI) /  /32. 

Both  terms  tend  to  0  with  n  if nl(k2log n)  ~ 00  and  no2 l(k2log n)  ~ 00. 
Take  0  =  E I (3k)  to  satisfy  our  earlier  side  condition,  and  note  that  we  need 
L * by 
n/(k3 log n) ~ 00. We, in fact, have strong convergence to 0 for Ln(Pk) 
the Borel-Cantelli lemma.  0 

The  crucial element in the proof of Theorem 20.9 is  the fact  that the number 
of sets in the partition  Q grows at most linearly in i, the iteration number. Had it 
grown quadratically, say, it would not have been good enough-we would not have 
had guaranteed improvements of large enough sizes to push the error probability 
towards L * . In the multidimensional version and in extension to other types of cuts 
(Problems 20.29, 20.30) this is virtually the only thing that must be verified. For 
hyperplane cuts, an additional inequality of the vc type is  needed, extending that 
for classes of hyperplanes. 

Our  proof is  entirely  combinatorial  and  geometric  and  comes  with  explicit 
bounds. The only reference to the Bayes error we need is the quantity I(E), which 
is the smallest value l for which 

inf 

alllxl grids 9z 

L(Yl)  ::::;  L * + E. 

It depends heavily on the distribution of course. CallI (E) the grid complexity of the 
distribution, for lack of a better term. For example, if X is discrete and takes values 
on the hypercube {O,  l}d, then I(E)  ::::;  2d. If X takes values on {aI, ... , aM}d, then 
I(E)  ::::;  (M + l)d.  If X  has  an  arbitrary  distribution  on the  real line and  1](x)  is 
monotone, then l ( E)  ::::;  2.  However, if 1]  is unimodal, l ( E)  ::::;  3.  In one dimension, 
I(E) is sensitive to the number of places where the Bayes decision changes. In two 
dimensions, however, l (E) measures the complexity of the distribution, especially 
near regions  with  1](x)  ~ 1/2. When  X  is  uniform on  [0,  1]2  and  Y  =  1 if and 
only if the components of X sum to less than one, then I (E)  =  8( 1 I -JE) as  E {- 0 
(Problem 20.32),  for example.  The grid complexity is  eminently suited to  study 
rates  of convergence,  as  it is  explicitly featured  in the  inequalities of our proof. 
There is no room in this book for following this thread, however. 

Problems and Exercises 

PROBLEM  20.1.  Let X be a random vector with density f  on nd. Show that each component 
has a density as well. 

PROBLEM 20.2.  Show that both condition (1) and condition (2) of Theorem 6.1 are necessary 
for consistency of trees with the X -property. 

358 

20.  Tree Classifiers 

PROBLEM 20.3.  For a theoretical median tree in n d  with uniform marginals and k levels of 
splitting, show that E{H(X) + VeX)} = 1/(2k

)  when k is a multiple of d. 

d

/

PROBLEM  20.4.  a-BALANCED  TREES.  Consider the following  generalization of the median 
tree: at a node in the tree that represents n points, if a split occurs, both subtrees must be of 
size at least a(n - 1)  for  some fixed  a  >  O.  Repeat the splits for k levels, resulting in 2k 
leaf regions. The tree has the X -property and the classifier is natural. However, the points 
at which cuts are made are picked among the data points in an arbitrary fashion.  The splits 
rotate through the coordinate axes.  Generalize the consistency theorem for median trees. 

PROBLEM  20.5.  Consider the following tree classifier.  First we find  the median according 
to  one coordinate, and create two  subtrees of sizes  Len  - 1)/2J  and  r(n  - 1)/21- Repeat 
this at each level, cutting the next coordinate axis in a rotational manner. Do not cut a node 
any further if either all data points in the corresponding region have identical Yi  values or 
the region contains less than k points. Prove that the obtained natural classifier is consistent 
whenever X has a density if k  -+  00 and log n/ k  -+ O. 
PROBLEM 20.6.  Let M be a probability measure with a density f  on n d

,  and define 

C = {x = (xCI),  ... , x Cd»: M ([xCI), xCI) + E]  x  ... x  [X(d) , xed)  + En  >  0 all E >  O}. 

Show that M( C) = 1. HINT:  Proceed as in the proof of Lemma A.l in the Appendix. 

PROBLEM 20.7.  Let U(l),  ... ,  U Cn ) be uniform order statistics defining spacings Sl, ... , Sn+l 
with Si  = U(i)  - UU-l), if U(n+l) = 1 and U(O)  = O.  Show that 

(1)  Sl, ... , Sn+l  are identically distributed; 
(2)  P{Sl  >  x} = (1  - xY\ x  E  [0,  1]; 
(3)  E{SIl = l/(n + 1). 

PROBLEM 20.8.  In the proof of Theorem 20.3, we assumed in the second part that horizontal 
and vertical cuts were meted out independently. Return to the proof and see how you can 
modify it to take care of the forced alternating cuts. 

PROBLEM  20.9.  Let Xl, X 2 ,  •.. ,  Xn  be i.i.d. with common density f  on the real line. Call 
Xi  a record if Xi  = min(X I ,  ... ,  XJ. Let K  = f{x;  is a record)·  Show that R l ,  ..• ,  Rn  are 
independent and that P {Ri  = I}  = 11 i.  Conclude that the expected number of records IS 
"-'  logn. 

PROBLEM  20.10.  THE DEEP  K-D TREE.  Assume that X has a density f  and that all marginal 
densities are uniform. 

(1)  Show  that  k  -+  00  implies  that  diam(A(X» 

-+  0  in  probability.  Do  this  by 
arguing that diam(A(X»  -+  0 in probability for the chronological k-d tree WIth 
the same parameter k. 
In a random k -d tree with n elements, show that the depth D of the last inserted node 
satisfies D I (2 log n) -+  I in probability. Argue first that you may restrict yourself 
to d = 1. Then write D  as  a sum of indicators of records. Conclude by computing 
E{D} and Var{D} or at least bounding these quantities in an appropriate manner. 
Improve the second condition of Theorem 20.4 to (k - 2 log n)IJlog n  -+  -00. 
(Note  that it is  possible  that  lim sUPn-Hxl k I log n  =  2.)  HINT:  Show  that  I D  -
2 log n I =  0  ( Jlog n)  in  probability  by  referring  to  the  previous  part  of this 

(2) 

(3) 

exercise. 

Problems and Exercises 

359 

PROBLEM  20.11.  Consider a full-fledged k-d tree with n+ 1 external node regions. Let Ln be 
the probability of error if classification is based upon this tree and if external node regions 
are assigned the labels (classes) of their immediate parents (see Figure 20.29). 

FIGURE 20.29.  The  external  nodes  are  as(cid:173)
signed the classes of their parents. 

leaf region 

Show that whenever X has a density, E{ Ln}  ~ LNN  :::  2L * just as for the I-nearest neighbor 
rule. 

PROBLEM  20.12.  CONTINUATION.  Show that Ln  ~ LNN  in probability. 

PROBLEM  20.13.  Meisel  and  Micha1opoulos  (1973)  propose  a  binary  tree  classifier with 
perpendicular cuts in which all leaf regions are homogeneous, that is,  all  Yi's  for the Xi'S 
in the same region are identical. 

• •  •  • 

• 

0 

0 

0 -.-

0 

0 

•  •  0 

0 

0 

0 

FIGURE 20.30.  Example  of a  tree partition  into  homogeneous 

regions . 

(1) 

(2) 

(3) 

(4) 

If L * = 0, give a stopping rule for constructing a consistent rule whenever X  has 
a density. 
If L *  = 0,  show  that there  exists  a  consistent homogeneous  partition  classifier 
with o(n) expected number of cells. 
If L *  >  0,  show a (more complicated) way of constructing a homogeneous par(cid:173)
tition that yields  a  tree classifier with E{ Ln}  ~ L * whenever X  has  a  density. 
HINT:  First make a consistent nonhomogeneous binary tree classifier,  and refine 
it to make it homogeneous. 
If L *  >  0,  then  show  that  the  expected number of homogeneous  regions  is  at 
least en  for  some e  >  0.  Such rules  are therefore not practical,  unless  L *  = 0. 
Overfitting will occur. 

PROBLEM 20.14.  Let X  be uniform on [0,  1]  and  let Y  be independent of X  with P{Y  = 
I}  = p. Find a tree classifier based upon simple interval splitting for which each region has 
one data point, and 

. 
. 
hmmf 
p-+O 

lim infn-+oo ELn 

p 

2:  3. 

360 

20.  Tree Classifiers 

We know that for the nearest neighbor rule, 

. 
hm 
p-+O 

lim sUPn-+CXl  ELn 

p 

=2, 

so that the given interval splitting method is worse than the nearest neighbor method. (This 
provides a counterexample to a conjecture of Breiman, Friedman, Olshen, and Stone (1984, 
pp.89-90).) HINT:  Sort the points,  and adjust the sizes of the intervals given to the class 0 
and class  1 points in favor of the  1 'so  By doing so, the asymptotic probability of error can 
be made as close as desired to (p2 + 3 p(1  - p))(1  - p). 

PROBLEM  20.15.  CONTINUATION.  Let X be uniform on [0,  1)2  and let Y  be independent of 
X  with P {Y  = I}  = p. Find a data-based tree classifier based upon perpendicular cuts for 
which each region  has  one data point, diam(A(X))  -+  0 in probability (recall Theorems 
6.1  and 21.2), and 

. 
. 
hmmf 
p-+O 

lim infn-+ CXl  ELn 

p 

=  00. 

Conclude that in n d
,  we can construct tree classifiers with one point per cell that are much 
worse than the nearest neighbor rule. HINT:  The next two problems may help you with the 
construction and analysis. 

PROBLEM  20.16.  CONTINUATION. Draw ani.i.d. sample Xl, ... , Xn  from the uniformdistn(cid:173)
butionon [0,1)2, and let YI ,  ••• ,  Yn  bei.i.d. and independent of the Xi'S withP{Y =  I}  = p. 
Construct a binary tree partition with perpendicular cuts for {Xi:  Yi = I}  such that every 
leaf region has one and only one point and diam(A(X)) -+ 0 in probability. 

(1)  How would you proceed, avoiding putting Xi'S on borders of regions? 
(2)  Prove diam(A(X)) -+ 0 in probability. 
(3)  Add the (Xi, Yi ) pairs  with  Yi  =  0  to  the leaf regions,  so that every region has 
one class-1  observation and zero or more class-O observations. Give the class-1 
observation the largest area containing no class-O points, as shown in Figure 20.31. 
Show that this  can always be done by adding perpendicular cuts and keeping at 
least one observation per region. 

·0 

0 

FIGURE 20.31.  Cutting a rectangle by giving 

a large area to the single class-1 point. 

... 0 

•  1 

0 

.0 

·0 

(4)  Partition all rectangles  with more than one point further to  finally  obtain a one(cid:173)

point-per-leaf-region partition. If there are N  points in a region of the tree before 
the class-l  and class-O points are separated (thus,  N  - 1 class-O points and one 
class-1 point), then show that the expected proportion of the region's area given to 
class 1,  given N, times N  tends to  00. (An explicit lower bound will be helpful.) 
HINT:  Use the next problem. 

(5)  Write the probability of error for the rule in terms of areas of rectangles, and use 

part (4) to get an asymptotic lower bound. 

Problems and Exercises 

361 

(6)  Now let p  tend to zero and get an asymptotic expression for your lower bound in 
terms of p. Compare this with 2 p(1  - p), the asymptotic error probability of the 
I-nearest neighbor rule. 

PROBLEM 20.17.  CONTINUATION.  Draw a  sample  Xl, ... , Xn  of n  i.i.d.  observations  uni(cid:173)
formly  distributed on  [0,  I]d.  The rectangles defined by the origin and X I,  ... ,  Xn  as  op(cid:173)
posite  vertices  are  denoted by  R I,  ... ,  Rn ,  respectively.  The probability content of Ri  is 
clearly fL(Ri ) = n:=l xij)· Study 

Mn  = max{fL(Ri) :  1 :s  i :s  n, Ri  does not contain R j  for any  j  =I i}, 

the probability content of the largest empty rectangle with a vertex at the origin. For d  =  1, 
Mil  is  just the  minimum  of the  X/s,  and  thus  nMn  ~ E,  where  E  is  the  exponential 
distribution.  Also E{Mn} = l/(n + 1).  For d  >  1,  Mn  is larger.  Show that nMn  -+  00 in 
probability and try obtaining the first term in the rate of increase. 

PROBLEM 20.18.  Show that diam(A(X»  -+  0 in probability for the chronological quadtree 
whenever k -+ 0 and X has a density. HINT: Mimic the proof for the chronological quadtree. 

PROBLEM 20.19.  Show that the deep quadtree is consistent if X  has a density and k  levels 
of splits are applied, where k  -+  00 and kl log n  -+ O. 

- 1) + 1 leaf 
PROBLEM 20.20.  Consider a full-fledged quadtree with n nodes (and thus n(2d 
regions).  Assign to  each region the  Y-Iabel  (class)  of its parent in the quadtree.  With this 
simple classifier, show that whenever X has a density, E{Ln} -+  2E{1](X)(1-1](X»} =  L NN • 
In particular, lim supn-;. DO  E{ Ln}  :s 2L * and the classifier is consistent when L * = O. 

PROBLEM 20.21.  In R}, partition the space as follows: Xl, X2 define nine regions by vertical 
and horizontal lines  through them.  X 3 ,  •.. ,  X k  are  sent down  to  the appropriate  subtrees 
in the 9-ary tree,  and within each subtree with at least two points, the process is repeated 
recursively. A decision at x  is by a majority vote (among (Xk+l ,  Yk+I ),  .•• ,  (Xn' Yn» among 
those  Xi'S in the same rectangle of the partition as  x  Show that if k  -+  00, kin  -+ 0, the 
rule is consistent whenever X has a density. 

PROBLEM 20.22.  On the two-dimensional counterexample shown in the text for multivariate 
Stoller splits, prove that if splits are performed based upon a sample drawn from the distri(cid:173)
bution, and if we stop after k splits with k depending on n in such a way that k I log n  -+ 0, 
then L n , the conditional probability of error, satisfies lim inf'HDO E{L,J 2:  (1- E)/2. HINT: 
Bound the probability of ever splitting [1,  2f anywhere by noting that the maximal differ(cid:173)
ence between the empirical distribution functions for the first coordinate of X given Y  = 0 
and Y  = 1 is  0  (1 I In) when restricted to  [1, 2f. 
PROBLEM 20.23.  Let  X  have  the  uniform  distribution  on  [0,5]  and  let  Y  = I{2<xdj,  so 
that  L *  = O.  Construct a binary  classification tree by  selecting  at  each iteration  the  split 
that  minimizes  the  impurity function  I,  where  1jJ  is  the  Gini  criterion.  Consider just the 
first three splits made in this manner. Let Ln  be the probability of error with the given rule 
(use a majority vote over the leaf regions).  Show that Ln  -+ 0 in probability. Analyze the 
algorithm when the Gini criterion is replaced by the probability-of-error criterion. 

PROBLEM 20.24.  Let X  be uniform on  [0,  1]  and let Y  be independent of X,  with  PlY = 
I}  = 2/3. Draw  a sample of size  n  from  this  distribution.  Investigate  where the  first  cut 

362 

20.  Tree Classifiers 

might take place, based upon minimizing the impurity function with 'I/f(p,  1- p) (the Gini 
criterion). (Once this is established, you will have discovered the nature of the classification 
tree, roughly speaking.) 

PROBLEM  20.25.  Complete the consistency proof of Theorem 20.7 for the raw BSP tree for 
n 2  by showing that diam(A(X)) ~ 0 in probability. 

PROBLEM 20.26.  BALANCED  BSP  TREES.  We  generalize median trees  to  allow  splitting the 
space along hyperplanes. 

50%/ 
/50% 

FIGURE 20.32.  A  balanced BSP  tree:  Each hyperplane 

cut splits a region into two cells of the same cardinality. 

Assume that X has a density and that the tree possesses the X -property. Keep splitting until 
there are 2k  leaf regions, as  with median trees. Call the trees balanced BSP trees. 

(1)  Show that there are ways  of splitting in n 2  that lead to  nonconsistent rules, re(cid:173)

(2) 

gardless how k varies with n. 
If every splitting hyperplane is forced  to contain d  data points (in nd
)  and these 
data points stay with the splitting node (they are not sent down to subtrees), then 
show that once again,  there exists  a splitting method that leads to  nonconsistent 
rules, regardless of how k varies with n. 

PROBLEM 20.27.  LetX have a uniform distribution on the unit ball of Rd. Let Y = I{IlXIl~1/2)' 
so that L * = O.  Assume that we split the space by a hyperplane by minimizing an impurity 
function based upon the Gini criterion.  If 11  is  very large,  where approximately would the 
cut take place (modulo a rotation, of course)? 

PROBLEM 20.28.  There  exist  singular  continuous  distributions  that  admit  uniform  [0,  1] 
marginals in n d.  Show, for example, that if X is uniformly distributed on the surface of the 
unit sphere of n3 ,  then its three components are all uniformly distributed on [ - 1,  1]. 

PROBLEM 20.29.  Verify that Theorem 20.9 remains valid in Rd. 

PROBLEM 20.30.  Prove that Theorem 20.9 remains valid if rectangular cuts are replaced by 
any of the elementary cuts shown on Figures 20.23 to  20.25, and such cuts are performed 
recursively k times, always by maximizing the decrease of the empirical error. 
PROBLEM  20.31.  Show that Theorem 20.11 remains valid if k  -+  ex) and k  = 0  ( J n / log n). 
HINT:  In the  proof of the theorem,  the bound on  '01  and  W"  is  loose.  You  may get more 
efficient bounds by applying Theorem 21.1  from the next chapter. 

PROBLEM  20.32.  Study  the  behavior  of the  grid  complexity  as  E  t  0  for  the  following 
cases: 

(1)  X is uniform on the perimeter of the unit circle of R 2  with probability 1/2 and X 
is uniform on [0,  1]2  otherwise. Let Y  =  1 if and only if X is on the perimeter of 
that circle (so that L * = 0). 

(2)  X = (X(l), X (2) ) is uniform on (0,  1 f  and Y  = 1 if and only if X(l) + X(2)  ::::  1. 

21 
Data-Dependent Partitioning 

21.1 

Introduction 

In Chapter 9 we investigated properties of the regular histogram rule.  Histogram 
classifiers partition the observation space nd and classify the input vector X  ac(cid:173)
cording  to  a majority  vote among the  Y/s  whose corresponding  X/s fall  in the 
same  cell  of the  partition as  X.  Partitions  discussed in  Chapter 9  could depend 
on the  sample size n, but were not allowed to depend on the data  Dn  itself.  We 
dealt mostly with grid partitions, but will now allow other partitions as  well. Just 
consider clustered training observations Xl, ... , X n . Near the cluster's center finer 
partitions  are called for.  Similarly,  when the  components have different physical 
dimensions, the scale of one coordinate axis is not related at all to the other scales, 
and  some data-adaptive  stretching is necessary.  Sometimes the data are concen(cid:173)
trated on or around a hyperplane. In all these cases, although consistent, the regular 
histogram method behaves rather poorly, especially if the dimension of the space 
is large. Therefore, it is useful to allow data-dependent partitions, while keeping a 
majority voting scheme within each cell. 

The  simplest  data-dependent  partitioning  methods  are  based  on  statistically 

equivalent blocks in which each cell contains the same number of points. In one(cid:173)
dimensional problems statistically equivalent blocks reduce to k-spacing estimates 
where the k-th, 2k-th, etc. order statistics determine the partition of the real line. 
Sometimes, it makes sense to cluster the data points into groups such that points 
in a group are close to each other, and define the partition so that each group is in 
a different cell. 

Many other data-dependent partitioning schemes have been introduced in the lit-

364 

21:  Data-Dependent Partitioning 

erature. In most of these algorithms the cells of the partition correspond to the leaves 
of a binary tree, which makes computation of the corresponding classification rule 
fast and convenient. Tree classifiers were dealt with in Chapter 20. Analysis of uni(cid:173)
versal consistency for these algorithms and corresponding density estimates was 
begun by Abou-Jaoude (1976b)  and Gordon and Olshen (1984),  (1978),  (1980) 
in a general framework,  and was extended, for example, by Breiman, Friedman, 
Olshen,  and Stone  (1984),  Chen and Zhao  (1987),  Zhao, Krishnaiah,  and Chen 
(1990), Lugosi and Nobel (1996), and Nobel (1994). 

This chapter is more general than the chapter on tree classifiers, as every partition 
induced by a tree classifier is  a  valid partition of space,  but not vice  versa.  The 
example below shows a rectangular partition of the plane that cannot be obtained 
by consecutive perpendicular cuts in a binary classification tree. 

FIGURE 21.1.  A  rectangular partition  of [0, 1 J2 
that cannot be achieved by a tree of consecutive 
cuts. 

In this chapter we first establish general sufficient conditions for the consistency 
of data-dependent histogram classifiers. Because of the complicated dependence 
of the partition on the data, methods useful for handling regular histograms have to 
be significantly refined. The main tool is a large deviation inequality for families 
of partitions that is related to the Vapnik -Chervonenkis inequality for families of 
sets.  The reader is  asked  for forgiveness-we  want to present a  very  generally 
applicable theorem and have to sacrifice (temporarily) by increasing the density 
of the text. However, as you will discover, the rewards will be sweet. 

21.2  A Vapnik -Chervonenkis Inequality for Partitions 

This is  a technical section.  We  will use its results in the next section to establish 
a general consistency theorem for histogram rules with data-dependent partitions. 
The main goal of this section is to extend the basic Vapnik-Chervonenkis inequality 
(Theorem 12.5) to families of partitions from families of sets. The line of thought 
followed here essentially appears in Zhao, Krishnaiah, and Chen (1990) for rect(cid:173)
angular partitions, and more generally in Lugosi and Nobel (1996). A substantial 
simplification in the proof was pointed out to us by Andrew Barron. 

By  a  partition  of Rd  we  mean  a  countable  collection  P  =  {AI, A2, .. '}  of 
subsets of Rd  such that Uj:l A j  = Rd  and Ai n A j  = 0  if i  =I  j. Each set A j  is 
called a cell of the partition P. 

21.2 A Vapnik-Chervonenkis Inequality for Partitions 

365 

Let M be a positive number. For each partition p, we define p(M) as the restric(cid:173)
tion ofP to the ball SM  (recall that SM  C  Rd denotes the closed ball of radius M 
centered at the origin). In other words, p(M) is a partition of SM, whose cells are 
obtained by intersecting the cells oiP with SM. We assume throughout that Pis 
such that IP(M) I <  00 for each M  <  00. We denote by B(p(M)  the collection of 
all 2IP(M)1  sets obtained by unions of cells of p(M). 

Just  as  we  dealt  with  classes  of sets  in  Chapter  12,  here  we  introduce fami(cid:173)

lies  of partitions.  Let F  be  a  (possibly  infinite)  collection  of partitions  of Rd. 
F(M)  =  {p(M)  :  P  E  F} denotes the family  of partitions of SM  obtained by re(cid:173)
stricting members of F  to SM.  For each M. we will measure the complexity of a 
family of partitions F(M) by the shatter coefficients of the class of sets obtained as 
unions of cells of partitions taken from the family pM). Formally, we define the 
combinatorial quantity  fl.n (F(M)  as  follows:  introduce the class A (M)  of subsets 
ofRd  by 

A(M) =  {A  E  B(p(M)  for some  p(M)  E  pM)} , 

and define 

fl.n(pM)  = s(A(M) , n), 

the shatter coefficient of A (M) • A (M) is thus the class of all sets that can be obtained 
as unions of cells of some partition of S M in the collection pM). For example, if 
all members of F(M) partition SM  into two sets, then  fl.n(PM)  is just the shatter 
coefficient of all sets in these partitions (with 0 and SM  included in the collection 
of sets). 

Let JL be a probability measure on Rd and let Xl, X 2, ... be i.i.d. random vectors 
in Rd with distribution JL. For n  =  1, 2, ... let JLn  denote the empirical distribution 
of Xl,""  X n ,  which places  mass  lin at each of Xl,""  X n •  To  establish the 
consistency of data-driven histogram methods, we require information about the 
large deviations of random variables of the form 

/LCA)\, 

sup  L  \JLn(A) -

P(M)E:F(M)  AEP(M) 

where F  is an appropriate family of partitions. 
REMARK.  Just  as  in  Chapter  12,  the  supremum  above  is  not  guaranteed  to  be 
measurable.  In order to insure measurability, it is necessary to impose regularity 
conditions on uncountable collections of partitions. It suffices to mention that in all 
our applications, the measurability can be verified by checking conditions given, 
e.g., in Dudley (1978), or Pollard (1984).  0 

The following theorem is a consequence of the Vapnik -Chervonenkis inequality: 

Theorem 21.1.  (LUGOSI  AND  NOBEL  (1996».  Let  Xl,""  Xn  be  i.i.d.  random 
vectors in Rd with measure JL  and empirical measure JLn. Let F  be a collection 0/ 
partitions a/Rd. Then/or each M  <  00 and E  >  0, 

p { 

sup  L  IJLn(A)  - JL(A)I  > E}  ::::  8fl.nCPM)e-nE2/512 + e-nE

2
/2. 

P(M)E:PM )  AEP(M) 

366 

21.  Data-Dependent Partitioning 

PROOF.  For a fixed partition P, define A' as the set 

A'=  u 

A. 

Then 

L  IfLn(A) -

AEP(M) 

fL(A)1 

= 

(fLn(A) -

fL(A)) 

+ 

=  2 (fLn(A')  -

fL(A')) + fL(SM) 

fLn(SM) 

<  2 

sup 

AEB(P(M) 

IfLn(A) -

fL(A)1  + fL(SM)  -

fLn(SM)· 

Recall that the class of sets B(p(M») contains all2IP(M)1  sets obtained by unions of 
cells of p(M). Therefore, 

< 

2 

sup 

p(M) E:fW) AEB(P(M)U{SM} 

sup 

IfLn(A) -

fL(A)1  + fL(SM)  -

fLn(SM)· 

Observe that the  first  term on the right-hand  side  of the inequality is  a  uniform 
deviation of the empirical measure  fLn  from  fL  over a  specific  class of sets.  The 
class  contains  all  sets  that can be  written  as  unions  of cells  of a partition p(M) 
in the class of partitions :F(M).  This class of sets is just A (M),  defined above. The 
theorem now follows  from the Vapnik-Chervonenkis inequality (Theorem 12.5), 
the definition of /:}.n(:F(M»),  and Hoeffding's inequality (Theorem 8.1).  0 

We will use a special application of Theorem 21.1, summarized in the following 

corollary: 

COROLLARY  21.1.  Let (Xl, Yl ), (X 2 ,  Y2 )  ... be  a sequence ofi.i.d.  random pairs 
in Rd  x  {O,  I} and let :Fl , :F2 ,  .•• be a sequence offamilies of partitions of Rd. If 
forM  <  00 

lim  log (/:}.n(:F~M»))  = 0, 

n-i>OO 

n 

then 

sup  L  l~tYJ{XiEA}-E{YI{XEA}}I~O 

P(M)EF,;M)  AEP(M)  n  i=l 

with probability one as n tends to infinity. 

21.2 A Vapnik-Chervonenkis Inequality for Partitions 

367 

PROOF. Let v be the measure of (X, Y) on Rd  x  {O,  I}, and let Vn  be the empirical 
measure  corresponding  to  the  sequence  (Xl, Yd, ... , (Xn, Yn).  Using  F~M) we 
define a family g~M) of partitions ofRd  x  {O,  I}  by 

g~M) = {p(M)  x  {OJ  : p(M)E F~M)} U {p(M)  x  {I}  : p(M)  E  F~M)}, 

where  P  x  {O}=  {AI, A 2, ... } x  {OJ  =  {AI  X  {O},  A2  X  {O},  ... }.  We  apply 
Theorem 21.1  for these families of partitions. 

def 

sup  L  I~ tYJ{Xi EA} -E{YI{XEAdl 
sup  L  Ivn(A  x  {I}) - v(A  x  {I})I 
sup  L  IVn(A) - v(A)I· 

PCM)E:FJ,M)  AEPCM) 

PCM)E:FJ,M)  AEPCM)  n  i=l 

< 

PCM)Ey,;M)  AEPCM) 

It  is  easy  to  see  that  ~n(g~M») = ~n(F~M»). Therefore  the  stated  convergence 
follows  from Theorem 21.1  and the Borel-Cantelli lemma.D 

Lemma 21.1.  Assume that the family F(M) is such that the number of cells of the 
partitions in the family are  uniformly bounded, that is,  there is a constant N  such 
that  IP(M)I  :::;  N for each p(M)  E  F(M).  Then 

~n(F(M») :::;  2N ~~(F(M»), 

where  ~~(F(M») is maximal number of different ways n points can be partitioned 
by members of F(M). 

EXAMPLE. FLEXIBLE GRIDS. As a first simple example, let us take in Fn all partitions 
into d-dimensional grids (called flexible grids as they may be visualized as chicken(cid:173)
wire fences with unequally spaced vertical and horizontal wires) in which cells are 
made up as Cartesian products of d  intervals, and each coordinate axis contributes 
one of mn intervals to these products. Clearly, if P is a member partition of Fn , then 
IPI  = m~. Nevertheless, there are uncountably many P's, as there are uncountably 
many  intervals  of the  real  line.  This  is  why  the  finite  quantity  ~n comes  in so 
handy.  We will verify later that for each M, 

!>'n(pN»)  S  2m~ (n +nmn) d, 

so that the condition of Corollary 21.1  is fulfilled when 

m d 
lim  --...!!:.  = O.  0 
n-+oo  n 

368 

21.  Data-Dependent Partitioning 

FIGURE 21.2.  Afiexible grid partition. 

21.3  Consistency 

In  this  section  we  establish  a  general  consistency  theorem  for  a  large  class  of 
data-based partitioning rules.  Using the training data  Dn  we produce a partition 
Pn =  Jrn(X r , Yr,  ... , X n, Yn) according to  a prescribed rule  Jrn. We  then use the 
partition  P n  in  conjunction  with  Dn  to  produce  a  classification  rule  based  on 
a  majority  vote  within  the  cells  of the  (random)  partition.  That is,  the  training 
set is  used twice and it is  this  feature  of data-dependent histogram methods that 
distinguishes them from regular histogram methods. 
Formally, an n-sample partitioning rule for nd  is  a function Jrn  that associates 
every  n-tuple  of pairs  (Xl, YI),  ... , (Xn,  Yn)  E  n d  x  {a,  I}  with  a  measurable 
partition of nd. Associated with every partitioning rule Jrn  there is  a fixed,  non(cid:173)
random family of partitions 

;::;1  =  {Jrn(XI,  YI,···, Xn , Yn)  : (Xl, YI) ... , (xn, Yn)  E  nd  x  {a,  I}}. 

;::;1  is the family of partitions produced by the partitioning rule Jrn  for all possible 
realizations  of the  training  sequence  Dn.  When a partitioning rule  Jrn is  applied 
to the sequence Dn  = (Xl, Yr), ••• , (Xn, Yn ), it produces a random partition Pn = 
JrnCDn)  E  ;::;1'  In  what  follows  we  suppress  the  dependence  of Pn  on  Dn  for 
notational simplicity.  For every  X  E  nd  let  An(x) be the  unique cell of Pn  that 
contains the point x. 

Now let {JrI, Jr2,  ... } be a fixed sequence of partitioning rules. The classification 
rule  gn(')  = gnc,  Dn)  is  defined  by  taking  a  majority  vote  among  the  classes 
appearing in a given cell of Pn ,  that is, 

We  emphasize here  that  the  partition Pn  can depend  on the  vectors  Xi  and  the 
labels  Yi •  First  we  establish  the  strong consistency of the  rules  {gn}  for  a  wide 
class  of partitioning rules.  As  always,  diam(A)  denotes  the diameter of a set A, 
that is, the maximum Euclidean distance between any two points of A: 

diam(A) = sup  /Ix  - y/I. 

x,yEA 

21.3 Consistency 

369 

Theorem 21.2.  (LUGOSI AND NOBEL (1996».  Let {7rl' 7r2,  ••• } be afixed sequence 
of partitioning rules, and for each n let Fn  be the collection of partitions associated 
with the n-sample partitioning rule 7rn· If 

(i) 

for each M  <  00 

. 
hm 
n--+oo 

log (~n (~M)) 

=0, 

n 

(ii) 

and 
for all balls SB  and all y  >  0, 

lim  JJ.- ({x: diam(An(x) n SM)  >  y}) = ° 

n--+oo 

with probability one, 

then the classification rule {gn}  corresponding to 7r1l  satisfies 

with probability one. In other words, the rule {gil}  is strongly consistent. 

REMARK.  In  some applications we need a weaker condition to replace  (ii)  in the 

theorem. The following condition will do:  for every y  > ° and 8  E  (0,  1) 

JJ.-({x  : diam(AIl(x) n T) >  y}) = ° with probability one. 

lim 
n-+OO  TCR,d:JL(T)~l-o 

inf 

The verification of this is left to the reader (Problem 21.2).  0 

The proof, given below, requires quite some effort. The utility of the theorem is 
not immediately apparent. The length of the  proof is indicative of the generality 
of the  conditions  in  the  theorem.  Given  a  data-dependent partitioning rule,  we 
must  verify  two  things:  condition  (i)  merely relates  to  the richness  of the  class 
of partitions of Rd  that may possibly occur, such as  flexible grids. Condition (ii) 
tells  us  that the  rule  should eventually  make  local  decisions.  From examples in 
earlier chapters,  it should  be  obvious  that  (ii)  is  not necessary.  Finite partitions 
of Rd  necessarily have  component sets  with infinite  diameter,  hence we  need a 
condition that states that such sets have small JJ.--measure.  Condition (ii) requires 
that  a randomly  chosen  cell  have  a  small  diameter.  Thus,  it  may  be  viewed  as 
the "with-probability-one" version of condition (1) of Theorem 6.1. However, the 
weaker  version  of condition  (ii)  stated  in  the  above  remark  is  more  subtle.  By 
considering examples in which JJ.- has bounded support,  more than just balls  S M 
are  needed,  as  the  sets of the partition near the boundary of the  support may all 
have  infinite diameter as  well.  Hence we introduced an  infimum with respect to 
sets T  over all T  with JJ.-(T)  :::  1 - O. 

It  suffices  to  mention that  (ii)  is  satisfied for  all  distributions  in  some  of the 
examples  that follow.  Theorem 21.2  then  allows  us  to  conclude that such rules 
are strongly universally consistent. The theorem has done most of the digestion of 

370 

21.  Data-Dependent Partitioning 

such proofs, so we are left with virtually no work at all.  Consistency results win 
follow like dominos falling. 

PROOF OF THEOREM 21.2.  Observe that the partitioning classifier gn can be rewritten 
in the form 

if L:'~l I{Yi~l)I{X;EAn(x)}  <  L:'~l I{y;~o}I{xiEA!l(x)} 

fL(An(x» 

otherwise. 

-

fL(An(x» 

Introduce the notation 

17n  x  = 

( )  

,,~  YI 
L....-l=l 

I 

{X;EAn(X») 

nM(An(x)) 

. 

For any  E  >  0, there is  an M  E  (0, (0) such that P{X  tf:.  SM}  <  E.  Thus, by an 
application of Theorem 2.3  we see that 

L(gn) - L* 

<  P{gn(X) =I  Y,  X  E  SMIDn}  - P{g*(X) =I  Y,  X  E  SM} + E 

< 

[117(x) - 17n(x)IM(dx) +  [ 
JSM 
JSM 

\(1  - 17(X))  - 17~O\x)IM(dx) + E, 

where 

17(O)(X)  = L7=1(l - YdI{x;EAn(x)}  • 
n 

nM(An(x)) 

By symmetry, since E  > ° is  arbitrary, it suffices to show that for each M  <  00, 

117(X)  - 17n(x)IM(dx) -+ ° with probability one. 

[ 
JS M 

Fix  E  > ° and let r  :  Rd  -+  R  be a continuous function  with bounded support 
such that iSM  117(X)  - r(x)IM(dx)  <  E. Such r  exists by Theorem A.8. Now define 
the auxiliary functions 

and 

Note that both ryn  and fn  are piecewise constant on the cells of the random partition 
Pn .  We may decompose the error as follows: 

I 17(X)  - 17n(x)1 

(21.1) 

21.3 Consistency 

371 

The integral of the first term on the right -hand side is smaller than E by the definition 
of r. For the third term we have 

r IFn(x)  - 17n(x)IP,(dx) 
1sM 

L  IE { Y I{xEA} I Dn}  - E { r(X)I{xEA} I Dn} I 

A EP,;M) 

::::  r 11](x)  -

1sM 

r(x)Ip,(dx)  <  E. 

Consider the fourth term on the right-hand side of (21.1). Clearly, 

It follows  from the first condition of the theorem and Corollary 21.1  of Theorem 
21.1  that 

r l17n(x)  - 17n(x)IP,(dx)  ---?>  0  with probability one. 
1sM 

Finally,  we  consider the  second term on the right-hand side  of (21.1).  Using 

Fubini's theorem we have 

372 

21.  Data-Dependent Partitioning 

Fix 6  E  (0,  1).  As  r  is  uniformly continuous,  there exists  a number  y  >  Osuch 
that if diam(A)  <  y, then  Ir(x) -
r(y)1  <  6 for every x, yEA. In addition, there 
is  a constant M  <  00  such that  I r(x) I ~ M  for every x  E  Rd.  Fix  n  now and 
consider the integrals 

_1_  ( 
fL(A)  JA  JA 

[Ir(x) -

r(y)IM(dx)M(dy) 

appearing in the sum above. We always have the upper bound 

_1_  [ 
M(A)JAJA 

[Ir(x) -

r(y)IM(dx)M(dy) ~ 2MfL(A). 

Assume now that A  E  p~M) has diam(A)  <  y. Then we can write 

Summing over the cells A  E  PI~M) with M(A)  >  0, these bounds give 

Ir(x) -

( 
JSM 
< 

rn(x)IM(dx) 

L 

2MM(A) + 

<  2MM({X: diam(An(x)})  2:  y) + 6. 

Letting n tend to infinity gives 

lim sup  [ 
n--+oo  1sM 

!r(x) -

rn(x)IM(dx) ~ 6  with probability one 

by the second condition of the theorem. Summarizing, 

lim sup  [  11](x)  - 17n(x)IM(dx) ~ 2E  + 6  with probability one. 
11--+00  J SM 

Since E  and 6 are arbitrary, the theorem is proved.  0 

21.4  Statistically Equivalent Blocks 

In this section we apply Theorem 21.2 both to classifiers based on uniform spacings 
in one dimension, and to their extension to multidimensional problems. We refer 
to these as rules based on statistically equivalent blocks. The order statistics of the 
components of the training data are used to construct a partition into rectangles. All 

21.4 Statistically Equivalent Blocks 

373 

such classifiers are invariant with respect to all strictly monotone transformations of 
the coordinate axes. The simplest such rule is the k-spacing rule studied in Chapter 
20 (see Theorem 20.1). Generalizations are possible in several ways. Theorem 21.2 
allows us to handle partitions depending on the whole data sequence-and not only 
on the Xi'S. The next simple result is sometimes useful. 

Theorem 21.3.  Consider a data-dependent partitioning classifier on the real line 
that partitions R  into  intervals in such a way that each interval contains at least 
an  and at most bn points. Assume that X  has a nonatomic distribution.  Then  the 
classifier is strongly consistent whenever an  -+  00 and bnln -+ 0 as n  -+  00. 

PROOF.  We check the conditions of Theorem 21.2. Let :01  contain all partitions of 
n into m = r n I an l intervals. Since for each M, all partitions in F~M) have at most 
m cells,  we  can bound  .6.n(F~M)) according to the Lemma 21.1.  By the  lemma, 
D.n(:F~M)) does not exceed 2m times the number of different ways n points can be 
partitioned into m  intervals. A little thought confirms that this number is 

and therefore, 

Let'H denote the binary entropy function, H(x) = -x log(x) -
(1  - x) loge 1 - x) 
for x  E  (0,  1).  Note that H is  symmetric about  1/2 and that H is  increasing for 
0< x  ::::  1/2. By the inequality of Theorem  13.4, log G)  ::::  sH(tls). Therefore, 
it is easy to see that 

<  m+(n+m)H(~) 
n+m 
<  nlan  + 2nH(1lan )  + 1. 

As 'H(x)  ---+  0 when x  ---+  0, the condition an  -+  00 implies that 

which establishes condition (i). 

To  establish condition (ii)  of Theorem 21.2, we proceed similarly to the proof 
of Theorem  20.1.  Fix  y, E  >  O.  There  exists  an  interval  [-M, M]  such  that 
JL([ - M, MY) <  E, and consequently 

JL({x  : diam(An(x))  >  y})::::  E + fl,({x: diam(An(x))  >  y} n [-M, MD, 

where An(x) denotes the cell of the partition P n containing x. Among the intervals 
of Pn , there can be at most 2 + 2M I y  disjoint intervals of length greater than y  in 

374 

21.  Data-Dependent Partitioning 

[-M, MJ.  Thus we may bound the second term on the right-hand side above by 

fL({X  : diam(An(x))  >  y} n [-M, M]) 

< 

< 

< 

AEPn 

(2 + 2M)  max fL(A) 
(2 + 2M)  (max fLn(A) + max IfL(A) -
(2 + 2M) (bn + sup IfL(A) -

AEPI1 

AEPI1 

y 

y 

y 

fLn(A)I) 

n 

AEA 

fLn(A)I)  , 

where A is the set of all intervals in R. The first term in the parenthesis converges 
to zero by the second condition of the theorem, while the second term goes to zero 
with probability one by an  obvious  extension of the classical Glivenko-Cantelli 
theorem (Theorem 12.4). Summarizing, we have shown that for any y, E  >  0 

lim sup fL( {x  : diam( An (X))  >  y}) :::;  E  with probability one. 

n---+CXJ 

This completes the proof.  0 

The d-dimensional generalizations of k-spacing rules include rules based upon 
statistically  equivalent  blocks,  that  is,  partitions  with  sets  that  contain  k  points 
each. It is obvious that one can proceed in many ways, see, for example, Anderson 
(1966),  Patrick  (1966),  Patrick  and  Fisher  (1967),  Quesenberry  and  Gessaman 
(1968) and Gessaman and Gessaman (1972). 

As  a  first  example,  consider  the  following  algorithm:  the  k-th  smallest x(lt 
coordinate among  the  training  data defines  the first  cut.  The  (infinite)  rectangle 
with n - k points is cut according to the x(2)-axis, isolating another k points. This 
can be repeated on a rotational basis for  all  coordinate axes.  Unfortunately,  the 
classifier obtained this way is not consistent. To see this, observe that if k is much 
smaller than n-a clearly necessary requirement for consistency-then almost all 
cells produced by the cuts are long and thin. We sketched a distribution in Figure 
21.3 for which the error probability of this classifier fails  to converge to L *. The 
details are left to the reader (Problem 21.3). This example highlights the importance 
of condition  (ii)  of Theorem 21.2,  that is,  that the  diameters  of the cells  should 
shrink in some sense as n ---+  00. 

21.4 Statistically Equivalent Blocks 

375 

FIGURE 21.3.  A  nonconsistent k-block algo(cid:173)
rithm  (with  k  = 2  in  the picture).  1J(x)  = 1 
in the shaded area and 1J(x) = 0 in the white 
area. 

o 

o 

o 

Rules have been developed in which the rectangular partition depends not only 
upon the Xi'S in the training sequence, but also upon the Yi's (see, e.g., Henrichon 
and Fu (1969), Meisel and Michalopoulos (1973) and Friedman (1977». For ex(cid:173)
ample, Friedman cuts the axes at the places where the absolute differences between 
the marginal empirical distribution functions are largest, to insure minimal empir(cid:173)
ical error after the cut. His procedure is based upon Stoller splits (see Chapters 4 
and 20). 

Rules depending on the coordinatewise ranks of data points are interesting be(cid:173)
cause they are invariant under monotone transformations of the coordinate axes. 
This is particularly important in practice when the components are not physically 
comparable.  "Distribution-free" is  an adjective often used to point out a property 
that  is  universally  valid.  For  such  methods  in  statistics,  see  the  survey  of Das 
Gupta  (1964).  Statistically  equivalent  sets  in partitions  are  called  "distribution(cid:173)
free" because the measure M(A) of a set in the partition does not depend upon the 
distribution of X. We already noted a similar distribution-free behavior for k -d trees 
and median trees (Chapter 20). There is no reason to stay with rectangular-shaped 
sets  (Anderson  and  Benning  (1970),  Beakley  and  Tuteur  (1972»  but  doing  so 
greatly simplifies the interpretation of a classifier. In this book, to avoid confusion, 
we reserve the term "distribution-free" for consistency results or other theoretical 
properties that hold for all distributions of (X, Y). 

It is possible to define consistent partitions that have statistically equivalent sets. 
To fix  the ideas, we take Gessaman's rule (1970) as our prototype rule for further 
study  (note:  for hypothesis testing,  this partition was already noted by Anderson 
(1966».  For each n,  let m  =  I(njkn)l/dl  Project the  vectors  Xl, ... , Xn  onto 
the first coordinate axis, and then partition the data into m  sets using hyperplanes 

FIGURE 21.4.  Gessaman's  partition 
with m = 3. 

376 

21.  Data-Dependent Partitioning 

o  0 
• 

o 

• 

• 
•  • 

o 

o 

°l------~·~--=·-----­

• 

0 

•  0 

0 

0 

0 

Po 
• 0 

0 

perpendicular to that axis, in such a way that each set contains an equal number of 
points (except, possibly, the rightmost set, where fewer points may fall if n is not a 
mUltiple of m). We obtain m cylindrical sets. In the same fashion, cut each of these 
cylindrical sets, along the second axis, into m  boxes such that each box contains 
the same number of data points. Continuing in the same way along the remaining 
coordinate axes, we obtain m d  rectangular cells, each of which (with the exception 
ofthose on the boundary) contains about kn points. The classification rule gn  uses 
a majority vote among those Yi 's for which Xi lies within a given cell. Consistency 
of this  classification rule can be established by an  argument similar to that used 
for the kn -spacing rule above. One needs to check that the conditions of Theorem 
21.2  are  satisfied.  The only minor difference  appears  in the  computation of Lln, 
which in this  case is  bounded from above by 2md (n:m/.  The following  theorem 
summarizes the result: 

Theorem 21.4.  Assume that the marginal distributions of X in nd are nonatomic. 
Then  the  partitioning  classification  rule  based on  Gessaman's  rule  is  strongly 
consistent if kn  -»- (X)  and kn /  n  -»- 0 as n tends to infinity. 

To consider distributions with possibly atomic marginals, the partitioning algo(cid:173)

rithm must be modified, since every atom has more than kn  points falling on it for· 
large n. With a proper modification,  a strongly universally consistent rule can be 
obtained. We leave the details to the reader (Problem 21.4). 

REMARK.  Consistency  of Gessaman's  classification  scheme  can  also  be derived 
from  the  results  of Gordon  and  Olshen  (1978)  under  the  additional  condition 
kn / fo -»- 00.  Results  of Breiman, Friedman,  Olshen,  and Stone (1984)  can be 

21.5 Partitioning Rules Based on Clustering 

377 

used to improve this condition to kn /  log n  -+  00. Theorem 21.4 guarantees con(cid:173)
sistency under the weakest possible condition kn  -+  00.  0 

21.5  Partitioning Rules Based on Clustering 

Clustering  is  one  of the  most  widely  used  methods  in  statistical  data  analysis. 
Typical clustering schemes divide the data into a finite number of disjoint groups 
by minimizing some empirical error measure, such as the average squared distance 
from cluster centers (see Hartigan (1975». In this section we outline the application 
of our  results  to  classification  rules  based  on  k-means  clustering  of unlabeled 
observations. 

As  a  first  step,  we  divide  Xl, ... , Xn  into  kn disjoint  groups  having  cluster 
centers  aI, ... , akn  E  Rd.  The  vectors  aI, ... , akn  are  chosen to  minimize  the 
empirical squared Euclidean distance error, 

over  all  the  nearest-neighbor clustering  rules  having  kn  representatives  bI ,  ... , 
E  Rd, where  II  . II  denotes the usual Euclidean norm.  Note that the choice of 
bkn 
cluster centers depends only on the vectors Xi, not on their labels. For the behavior 
of en(aI,  ... , akJ, see Problem 29.4. 

The vectors aI, ... , akll  give rise to a Voronoi partition P n = {A 1,  ... , Akn }  in a 

natural way:  for each j  E  {I, ... , kn }, let 

Ties  are  broken by assigning points on the  boundaries  to  the  vector that has  the 
smallest index. 

The  classification rule gn  is  defined in the usual way:  gl1 (x) is  a majority vote 
among those Yj's such that X j  falls in An (x). If the measure fL  of X has a bounded 
support, Theorem 21.2 shows that the classification rule {gl1}  is strongly consistent 
if kn  grows with n at an appropriate rate. Note that this rule is just another of the 
prototype nearest neighbor rules that we discussed in Chapter 19. 

378 

21.  Data-Dependent Partitioning 

FIGURE 21.5.  Example  of partition_ 
ing based on clustering with kn ==  3. 
The criterion we minimize is the sum 
of the squares of the distances of the 
X/s to  the a/so 

Theorem 21.5.  (LUGOSI AND NOBEL (1996)). Assume that there is a bounded set 
A  C  Rd such that PIX E  A} =  1. Let {kn } be a sequence of integers for which 

kn -+ 00  and  - - - -+ 0  as n  -+  00. 

k~ logn 

n 

Let gn(',  Dn)  be the histogram classification rule based on the Voronoi partition 
of kn cluster centers minimizing the empirical squared Euclidean distance error. 
Then 

with probability one as n tends to  infinity. If d  = 1 or 2, then the second condition 
on kn can be relaxed to 

kn logn 
- - - -+0. 

n 

PROOF.  Again,  we  check the  conditions  of Theorem 21.2.  Let Fn  consist of aU 
Voronoi partitions of kn  points in Rd.  As  each partition consists of kn  cells, we 
may use Lemma 21.1  to bound  /)..n(:P:zM »).  Clearly, boundaries between cells are 
subsets of hyperplanes. Since there are at most knCkn - 1)/2 boundaries between 
the  kn Voronoi  cells,  each  cell  of a  partition  in  Fn  is  a  polytope  with  at  most 
knCkn - 1)/2  <  k~ faces.  By  Theorem  13.9,  n  fixed  points  in  R d ,  d  ::::  2,  can 
be  split by  hyperplanes  in  at  most nd+1  different  ways.  It follows  that for each 

21.5 Partitioning Rules Based on Clustering 

379 

M <  00,  f::j.n(F~M)  :::::  2klln(d+l)k~, and consequently 

1 
-log f::j.n(Fn 
n 

(M) 

):::::  - + 

kn 
n 

(d + l)k~ log n 

n 

~ 0 

by the second condition on the sequence {kn }.  Thus condition (i) of Theorem 21.2 
is satisfied. 

It remains  to  establish condition (ii)  of Theorem 21.2.  This time we  need the 
weaker condition mentioned in the remark after the theorem, that is, that for every 
y >  0 and £5  E  (0,  1) 

inf 

T:M(T):::.1-8 

p,( {x  : diam( An (x) n T)  >  y}) ~ 0  with probability one. 

Clearly, we are done if we can prove that there is a sequence of subsets  Tn  of nd 
(possibly depending on the data Dn)  such that fL(Tn)  ~ 1 with probability one, 
and 

fL({X:  diam(An(x) n Tn)  >  y}) = O. 

To this end, let a I,  ... , akn  denote the optimal cluster centers corresponding to D n , 
and define 

kn 

'Lz  = U Saj,y/2 n A j , 

j=l 

'Lz  implies 
where  Sx,r  is  the  ball of radius  r  around the  vector  x.  Clearly,  x  E 
that  Ilx  - a(x) II  <  y /2, where a(x) denotes the closest cluster center to x  among 
al,""  akn •  But since 

it follows  that 

and 

fL ({x: diam(An(x) n Tn)  >  y}) = O. 

It  remains  to  show  that  fL(Tn)  ~ 1  with  probability  one  as  n  ~ 00.  Using 
Markov's inequality, we may write 

< 

E {minl::J::kn  IIX - ajl121  Dn} 

(y /2)2 

Using  a  large-deviation  inequality  for  the  empirical  squared  error  of nearest(cid:173)
neighbor  clustering  schemes,  it can be  shown  (see  Problem 29.4)  that if X  has 
bounded support, then 

E {  ~in  IIX - ajfl Dn}  -

I::;:J::;:kn 

min 

b1, ... ,bkn ERd 

E  {  ~in  IIX - bj112 } ~ 0 

l::;:J::;:kn 

380 

21.  Data-Dependent Partitioning 

with probability one if kl1  log n/ n  -+ 0 as n  -+  00. Moreover, it is easy to see that 

b"~~~ER' E {I~~~" IIX - bj  112}  -> 0 

as kn  -+  00. It follows that fL(Tn)  -+  1, as desired. 

If d  =  1, then the cells of the Voronoi partition are intervals on the real line, and 
therefore, ~n (~~M») ::::  2kl1 nkn . Similarly, if d  = 2, then the number of hyperplanes 
defining the Voronoi partition increases linearly with kn . To see this, observe that 
if we  connect centers of neighboring clusters by  edges, then we obtain a planar 
graph. From Euler's theorem (see, e.g., Edelsbrunner (1987, p.242)), the number 
of edges in a planar graph is bounded by 3N - 6,  N  ::::  3, where N  is the number 
of vertices. Thus, in order to  satisfy condition (i), it suffices that kn  log n / n  --+  0 
in both cases.  0 

REMARK.  In the theorem above we assume that the cluster centers a], ... ,ak are 
empirically  optimal in the  sense that they  are chosen to  minimize the empirical 
squared Euclidean distance error 

Practically speaking, it is hard to determine minimum. To get around this difficulty, 
several  fast  algorithms  have  been proposed that approximate  the  optimum (see 
Hartigan (1975) for a survey). Perhaps the most popular algorithm is the so-called 
k-means clustering method, also known in the theory of quantization as the Lloyd(cid:173)
Max algorithm (Lloyd (1982), Max (1960),  Linde, Buzo, and Gray (1980)). The 
iterative method works as follows: 

STEP  1. 
S 
TEP 2. 

(0) 
X 

(0) 
.  X 

T  k  k ·  ..  1 
-nd 
llutla  centers a 1  ,  ... , ak  E  1'-' 
a  e 
(i) 
Cl 
h  d 
uster t  e  ata pomts  1,' .. , 
,  ... ,  ak 
into k sets such that the m-th set CI~) contains the X j'S that are closer 
to  a~) than  to  any  other center.  Ties  are broken in  favor  of smaller 
indices. 
. 

d 
.  0 
,  an  set l  =  . 
d  h 

(i) 
t  e  centers a 1 

n  aroun 

h 

as  t  e  averages  0 

f  h 
t  e 

(i+l) 

'  ... , ak 

(i+l) 

STEP 3.  Determme  t  e  new  centers  a 1 
data points within the clusters: 

h 

a(i+l) = 
m 

L" 

(i)X, 
J 

J.XjEC,n 
ICI~)I 

STEP 4. 

Increase i by one, and repeat Steps 1 and 2 until there are no changes 
in the cluster centers. 

It is easy to see that each step of the algorithm decreases the empirical squared 
Euclidean distance error. On the other hand,  the empirical squared error can take 
finitely  many  different  values  during  the  execution of the  algorithm.  Therefore 

21.6 Data-Based Scaling 

381 

the  algorithm halts in finite  time.  By inspecting the proof of Theorem 21.5,  it is 
not  hard  to  see  that  consistency  can  also  be proved for  partitions  given  by  the 
suboptimal  cluster centers  obtained by the  k-means  method,  provided  that it is 
initialized appropriately (see Problem 21.6). 0 

21.6  Data-Based Scaling 

We now choose the grid size h in a cubic histogram rule in a data-dependent manner 
and denote its value by  Hn.  Theorem 21.2 implies the following general result: 

Theorem 21.6.  Let gn  be the cubic histogram classifier based on a partition into 
cubes of size Hn.  If 

(a) 

(b) 

lim  Hn  = 0 and 

/1--fCXJ 

lim  nH~ =  00 with probability one, 

/1--fCXJ 

then  the partitioning rule is strongly universally consistent. 

To prove the theorem, we need the following auxiliary result: 

Lemma 21.2.  Let  Z 1,  Z2,  ... be  a  sequence  of nonnegative  random  variables. 
If limn--fCXJ  Zn  =  0  with probability  one,  then  there  exists  a  sequence  an  .}  0  of 
positive numbers such that limn--fCXJ  I{Zn?:.an} = 0 with probability one. 

PROOF.  Define  Vn  =  sUPm?:.n  Zm.  Then clearly,  Vn 
can find a subsequence nl, n2,  ... of positive integers such that for each k, 

.}  0  with probability one.  We 

Then the Borel-Cantelli lemma implies that 

lim  I{Vllk ?:.lj k}  = 0  with probability one. 
k

--f CXJ 

(21.2) 

The fact that Vn  2:  Zn  and (21.2) imply the statement.  0 

PROOF  OF THEOREM  21.6.  Let  {an}  and  {bn}  be  sequences  of positive  numbers 
with  an  <  bn . Then 

382 

21.  Data-Dependent Partitioning 

It follows  from Lemma 21.2 that there exist sequences of positive numbers  {an} 
and {bn} satisfying an  <  bn, bn ~ 0, and na~ ~ 00 as  n  ~ 00 such that 

lim  I{HIl~[an,b,zl}  = 0  with probability one. 
n-+oo 

Therefore we may assume that for each n, P{Hn  E  [an,  bn]}  = 1.  Since bn  ~ 0, 
condition  (ii)  of Theorem  21.2  holds  trivially,  as  all  diameters  of all  cells  are 
inferior to  bn,J(i.  It remains  to  check condition  (i).  Clearly,  for  each  M  <  00 
each partition in :F~M) contains  less  than  co(1/an )d  cells,  where the  constant c~ 
depends on M and d only. On the other hand, it is easy to see that n points can not 
be partitioned more than c1n(1/an)d different ways by cubic-grid partitions with 
cube size h  2:  an  for some other constant C1.  Therefore, for each M  <  00, 

.6.  (:F(M))  <  2co(l/an)d C1 n 
a~ , 

n 

-

and condition (i) is satisfied.  0 

In many applications, different components of the feature vector X correspond 
to  different  physical  measurements.  For example,  in  a  medical  application,  the 
first coordinate could represent blood pressure, the second cholesterol level, and 
the third the weight of the patient.  In such cases there is  no  reason to  use cubic 
histograms, because the resolution of the partition hn along the coordinate axes de(cid:173)
pends on the apparent scaling of the measurements, which is rather arbitrary. Then 
one can use scale-independent partitions such as methods based on order statistics 
described  earlier.  Alternatively,  one  might  use  rectangular  partitions  instead of 
cubic ones, and let the data decide the scaling along the different coordinate axes. 
Again, Theorem 21.2 can be used to establish conditions of universal consistency 
of the classification rule corresponding to data-based rectangular partitions: 

Theorem 21.7.  Consider a  data-dependent histogram rule when the  cells of the 
partition are all rectangles of the form 

[k 1Hn1 , (k1  + 1)Hn1)  x  .. , x  [kdHnd,  (kd  + l)Hnd ), 

where k 1,  ... ,  kd  run through the set of integers, and the edge sizes of the rectangles 
Hn1,  ... , Hnd  are determinedfrom the data  Dn.1fas n  ~ 00 
Hni  ~ 0  for each 1 ::;  i  ::;  d,  and  nHnl  ... Hnd  ~ 00  with probability one, 
then the data-dependent rectangular partitioning rule is strongly universally con(cid:173)
sistent. 

To  prove this, just check the conditions of Theorem 21.2 (Problem 21.7). We 

may pick, for example,  Hn1 , ... , Hnd  to minimize the resubstitution estimate 

n L I{gn(Xi )::jYi ) 

i==l 

subject of course to certain conditions, so that 'L.1=1  Hni  ~ 0 with probability one, 
yet n [11=:1  Hni  ~ 00 with probability one. See Problem 21.8. 

Problems and Exercises 

383 

21.7  Classification Trees 

consider a partition of the space obtained by a binary classification tree in which 
each node dichotomizes its set by ahyperplane (see Chapter 20 for more on classifi(cid:173)
cation trees). The construction of the tree is stopped according to some unspecified 
rule, and classification is by majority vote over the convex polytopes of the parti-
tion. 

The  following  corollary of Theorem 21.2  generalizes  (somewhat)  the consis-

tency results in the book by Breiman, Friedman, Olshen, and Stone (1984): 

Theorem 21.8.  (LUGOSI  AND  NOBEL  (1996».  Let gn  be a  binary tree  classifier 
based  upon  at most mn  - 1 hyperplane  splits,  where  mn  =  o(n/logn). If,  in 
addition, condition (ii) of Theorem 21.2 is satisfied, then gn  is strongly consistent. 
In particular, the rule is strongly consistent if condition (ii) o/Theorem 21.2 holds 
and every cell of the partition contains at least kn points, where kn /  log n  -+  00. 

PROOF. To check condition (i) of Theorem 21.2, recall Theorem 13.9 which implies 
that  n  ::::  2  points  in  a d-dimensional  Euclidean  space can be dichotomized by 
hyperplanes in at most nd+1 different ways. From this, we see that the number of 
different ways n points of R,d  can be partitioned by the rule gil  can be bounded by 

as there are not more than mn  cells in the partition. Thus, 

By the assumption that mn log n / n  -+ ° we have 

11 
- og~n(Jn) ~ -
n 

'L 

mn  mn(d + 1)logn 
n 

+ 

n 

-+ 0, 

so that condition (i) of Theorem 21.2 is satisfied. 

For the second part of the statement, observe that there are no more than n / k n 
cells in any partition,  and that the tree-structured nature of the partitions assures 
that gn  is based on at most n / kn  hyperplane splits. This completes the proof.  D 

Problems and Exercises 

PROBLEM  21.1.  Let P be a partition of n d. Prove that 

L IILn(A) -

AEP 

IL(A)I  = 2  sup  IILn(B) -

IL(B)I , 

BES(P) 

where  the  class  of sets  B(P) contains  all  sets  obtained  by  unions  of cells  of P. This  is 
Scheffe's (1947) theorem for partitions. See also Problem 12.13. 

384 

21.  Data-Dependent Partitioning 

PROBLEM  21.2.  Show that condition (ii) of Theorem 21.2 may be replaced by the following: 

for every y  > ° and 8  E  (0,  1) 

lim 
/1-HX) TcRd :/1-(T)?:: 1-0 

inf 

(t({x  : diam(A I1 (x) n T)  >  y}) = ° with probability one. 

PROBLEM  21.3.  Let X be uniformly distributed on the unit square [0,  1]2.  Let 77(X)  = 1 if 
x(l)  :s  2/3, and 77(X)  = ° otherwise (see Figure 21.3).  Consider the algorithm when first 

the x(l)-coordinate is  cut at the k-th smallest x(l)-value among the  training data.  Next the 
rectangle with n  - k points is cut according to the x(2l-axis, isolating another k  points. This 
is repeated on a rotational basis for the two coordinate axes. Show that the error probability 
of the  obtained  partitioning  classifier  does  not  tend  to  L *  =  0.  Can  you  determine the 
asymptotic error probability? 

PROBLEM 21.4.  Modify Gessaman's rule based on statistically equivalent blocks so that the 
rule is strongly universally consistent. 

PROBLEM  21.5.  Cut each axis independently into intervals containing exactly k of the (pro(cid:173)
jected)  data points.  The i -th  axis  has  intervals  A l,i, A2,i,  .... Form a histogram rule that 
takes a majority vote over the product sets A il ,1  x  ... X  Aid,d. 

FIGURE 21.6.  A  partition  based  upon  the 
method obtained above with k = 6. 

0 

• 
• 

0 

\I 

\I 

\I 

CII 

\I 

\I 

0 

• 

0 

0 

0 

\I 

0 

\I 

This  rule  does  not  guarantee  a  minimal  number  of points  in  every  cell.  Nevertheless, if 
k d  = o(n),  k  -+  00,  show that this  decision rule is consistent,  i.e.,  that E{ La}  -+  L * in 
probability. 

PROBLEM  2l.6.  Show  that  each  step  of the  k-means  clustering  algorithm  decreases  the 
empirical squared error.  Conclude that Theorem 21.5 is  also true if the clusters  are given 
by the k-means algorithm. HINT:  Observe that the only property of the clusters used in the 
proof of Theorem 21.5 is  that 

E {1;n~~11 IIX - aJI121  Dn}  -+ ° 

with probability one.  This can be proven for clusters given by the k-means algorithm if it 
is appropriately initialized. To this end, use the techniques of Problem 29.4. 

PROBLEM 21.7.  Prove Theorem 21.7. 

PROBLEM 21.8.  Consider the Hni 's in Theorem 21.7, the interval sizes for cubic histograms. 
Let the  Hni 's be found by minimizing 

n L I{gn(X;ljYil 

i=] 

Problems and Exercises 

385 

subject to the condition that each marginal interval contain at least k  data points (that is, at 
least k data points have that one coordinate in the interval). Under which condition on k  is 
the rule consistent? 

PROBLEM 21.9.  Take a histogram rule with data-dependent sizes Hnl ,  ..• ,  Hnd  as in Theo(cid:173)
rem 21.7, defined as follows: 

ni  - max 
l~J~n 

(1) 
(2)  Hni  = (Wni  - ~li)/nl/(2d), 1 :s  i  :s  d, where ~li and Wni  are 25 and 75 percentiles 

- mm 
l~J~n 

j '  ... , 

y  n, 

:s  l:S  were 

d (h  X  - (X(1) 

j  -

. 

.  X(i)) /  r::  1 

X(d)). 
, 

j 

j 

H 

-

( 

XU) 
j 

of xii), ... , X~i). 

Assume for convenience that X  has nonatomic marginals. Show that (1) leads sometimes to 
an inconsistent rule, even if d = 1. Show that (2) always yields a scale-invariant consistent 
rule. 

22 
Splitting the Data 

22.1  The Holdout Estimate 

Universal consistency gives us a partial satisfaction-without knowing the under(cid:173)
lying distribution, taking more samples is guaranteed to push us close to the Bayes 
rule in the long run. Unfortunately, we will never know just how close we are to the 
Bayes rule unless we are given more information about the unknown distribution 
(see Chapter 7). A more modest goal is to do as well as possible within a given class 
of rules.  To  fix  the ideas, consider all nearest neighbor rules based upon metrics 
of the form 

d 

IIxl12  = Laix(i)2, 

i=l 

where ai  ::::  0 for all i and x  = (x(l),  ... , x(d»). Here the ai's are variable scale fac(cid:173)
tors. Let <Pn  be a particular nearest neighbor rule for a given choice of (al, ... , ad), 
and let gn  be a data-based rule chosen from this class.  The best we can hope for 
now is  something like 

L(gn) 

----+  1 

inf<pn  L(<Pn) 

in probability 

for all distributions, where L(gn) = P{gn(X) =I YIDn} is the conditional probabil(cid:173)
ity of error for gn.  This sort of optimality-within-a-class is definitely achievable. 
However, proving such optimality is generally not easy as gn  depends on the data. 
In this chapter we present one possible methodology for selecting provably good 
rules  from restricted classes. This is  achieved by splitting the data into a training 

388 

22.  Splitting the Data 

sequence and a testing sequence. This idea was explored and analyzed in depth in 
Devroye (1988b) and is now formalized. 

The  data  sequence  Dn  is  split  into  a  training  sequence  Dm  =  (Xl, Yd, ... 
(Xm,  Ym)  and a testing sequence Tt  = (XnHl,  Ym+r),  ... , (Xm+l,  Ym+l),  where l ~ 
m  =  n.  The sequence  Dm  defines  a  class  of classifiers en  whose members  are 
denoted by ¢m(') = ¢m(', Dm).  The testing  sequence is used to  select a classifier 
from Cm  that minimizes the error count 

This error estimate is called the holdout estimate, as the testing sequence is "held 
out" of the design of ¢m.  Thus, the selected classifier gn  E  Cm  satisfies 

Lm,l(gn)  :::;  Lm,l(¢m) 

for all ¢m  E  Cm . The subscript n in gn  may be a little confusing, since gn  is in Cm,· 
a class of classifiers depending on the first m pairs Dm  only. However, gil  depends 
on the entire data  Dn ,  as  the rest of the data is used for  testing the  classifiers in 
Cm . We are interested in the difference between the error probability 

and that of the best classifier in Cm , inf <Pill ECn  L (¢m). Note that L (¢m) = P { ¢m (X) =J 
Y I Dm}  denotes  the error probability conditioned on  Dm.  The conditional proba(cid:173)
bility 

is  small when most testing sequences Tt  pick a rule gn  whose error probability is 
within E  of the best classifier in Cm . We have already addressed similar questions 
in Chapters 8 and 12. There we have seen (Lemma 8.2), that 

L(gn) -

inf  L(¢m):::;  2  sup  ILm,l(¢m)  - L(¢m)I. 
<PmECm 

<PmECm 

If Cm  contains finitely many rules, then the bound of Theorem 8.3  may be useful: 

If we take m  = 1 = n12, then Theorem 8.3  shows (see Problem 12.1) that on the 
average we are within -/log(2e ICm I) I n of the best possible error rate, whatever it 
is. 

If Cm  is of infinite cardinality, then we can use the Vapnik-Chervonenkis theory 

to get similar inequalities. For example, from Theorem 12.8 we get 

22.2 Consistency and Asymptotic Optimality 

389 

and consequently, 

P  {L(gn) -

inf  L(¢m)  >  EI  Dm}  S  4e8S(Cm, l2)e-lE2j2, 

<PmECm 

_ 

(22.2) 

where S(Cm, I) is the l-th shatter coefficient corresponding to the class of classifiers 
em  (see Theorem 12.6 for the definition).  Since Cm  depends on the training data 
Dm, the shatter coefficients S(Cm, l) may depend on D m , too. However, usually itis 
possible to find upper bounds on the random variable S(Cm, I) that depend on m and 
I only,  but not on the actual values of the random variables  Xl, Y1,  ..• ,  X m,  Y m' 
Both  upper  bounds  above  are  distribution-free,  and  the  problem  now  is  purely 
combinatorial: count ICm I (this is usually trivial), or compute S(Cm ,  l). 

REMARK.  With much more effort it is  possible to  obtain performance bounds  of 
the holdout estimate in the form of bounds for 

for some special rules where ¢m and ¢n are carefully defined. For example, Devroye 
and Wagner (1979a) give upper bounds when both ¢m and ¢n are k-nearest neighbor 
classifiers with the same k (but working with different sample size).  D 

REMARK.  Minimizing the holdout estimate is  not the only possibility. Other error 
estimates that do not split the data may be used in classifier selection as well. Such 
estimates  are discussed in Chapters 23, 24, and 31. However,  these estimates are 
usually  tailored to work well for specific discrimination rules.  The most general 
and robust method is certainly the data splitting described here.  D 

22.2  Consistency and Asymptotic Optimality 

Typically,  Cm  becomes  richer  as  m  grows,  and it  is  natural  to  ask whether  the 
empirically best classifier in Cm  is consistent. 

Theorem 22.1.  Assume  that from  each  Cm  we  can  pick one  ¢m  such  that  the 
sequence  of ¢m 's is consistent for a certain class of distributions.  Then  the auto(cid:173)
matic rule  gn  defined above  is consistent for the same class of distributions (i.e., 
EL(gn)  -+  L * as n  -+  (0) if 

. 
!~~ 

10g(E{S(Cm ,  l)}) 

l 

= O. 

PROOF.  Decompose  the  difference  between  the  actual  error  probability  and the 
Bayes error as 

390 

22.  Splitting the Data 

The convergence of the first  term on the right-hand side is  a direct corollary of 
(22.2). The second term converges to zero by assumption.  0 

Theorem 22.1  shows  that  a  consistent rule  is  picked  if the  sequence  of Cm'S 
contains a consistent rule, even if we do not know which functions from em lead to 
consistency. If we are just worried about consistency, Theorem 22.1  reassures Us 
that nothing is lost as long as we take l much larger than log(E{S(Cm ,  l)}). Often 
this reduces to a very weak condition on the size l  of the testing set. 
' 
Let us now introduce the notion of asymptotic optimality. A sequence of rules 

gn  is  said to be asymptotically optimal for a given distribution of (X, Y) when 

E{L(gn)} - L * 

. 
hm 
17-+00  E {inf¢mEcm  L(¢m)}  - L* 

=  l. 

Our definition  is  not entirely  fair,  because  gn  uses  n  observations,  whereas  the 
family  of rules in the denominator is restricted to  using m  observations. If gn  is 
not taken from the same em, then it is possible to have a ratio which is smaller than 
one. But if gn  E  Cm, then the ratio always is at least one. That is why the definition 
makes sense in our setup. 

When our selected rule is asymptotically optimal, we have achieved something 
very strong: we have in effect picked a rule (or better, a sequence of rules) which has 
a probability of error converging at the optimal rate attainable within the sequence 
of em's. And we do not even have to know what the optimal rate of convergence 
is.  This  is  especially important in nonparametric  rules,  where  some researchers 
choose smoothing factors based on theoretical results about the optimal attainable 
rate of convergence for certain classes of distributions. 

We are constantly faced with the problem of choosing between parametric and~ 

nonparametric discriminators. Parametric discriminators are based upon an under(cid:173)
lying model in which a finite  number of unknown parameters is  estimated from 
the  data.  A  case in point is  the  multivariate  normal  distribution,  which leads  to 
linear or quadratic discriminators. If the model is wrong, parametric methods can 
perform very  poorly;  when  the  model  is  right,  their performance is  difficult to 
beat. The method based on splitting the data chooses among the best discriminator 
depending upon which happens to be best for the given data. We can throw in em 
a variety of rules, including nearest neighbor rules,  a few  linear discriminators, a 
couple of tree classifiers and perhaps a kernel rule. The probability bounds above 
can be used when the complexity of em  (measured by its shatter coefficients) does 
not get out of hand. 

The notion of asymptotic optimality can be too strong in many cases. The reason 
for this is that in some rare lucky situations E {inf¢mEcm  L(¢m)} - L * may be very 
small. In these cases it is impossible to achieve asymptotic optimality. We can fix 
this  problem by introducing  the notion of Em -optimality,  where  Em  is  a  positive 
sequence decreasing to 0 with m. A rule is  said to be Em-optimal when 

E{L(gn)}  - L * 

. 
hm 
n-+oo  E {inf¢mEcm  L(¢m)}  - L* 

= l. 

22.3 Nearest Neighbor Rules with Automatic Scaling 

391 

for all distributions of (X, Y) for which 

lim  E {infrPmEcm  L(¢m)}  - L * = 00. 

m--7CXJ 

In what follows  we apply the idea of data splitting to  scaled nearest neighbor 
rules  and  to  rules  closely  related  to  the  data-dependent  partitioning  classifiers 
studied in Chapter 21. Many more examples are presented in Chapters 25 and 26. 

22.3  Nearest Neighbor Rules with Automatic Scaling 

Let us work out the simple example introduced above. The (aI, ... ,ad)-NN rule is 
the nearest neighbor rule for the metric 

Ilxll  = 

where  x  =  (x(l), ... , xed»).  The class em  is the class  of all  (al, ... , ad)-NN rules 
for  Dm  = ((Xl, YI ), ... ,(Xm, Ym)).  The  testing  sequence  Tz  is  used  to  choose 
(aI, ... , ad) so as  to minimize the holdout error estimate. In order to have 

it suffices that 

log E {S(em , l)} 

. 
}~~ 

l 

=0. 

This puts a lower bound on l. To get this lower bound, one must compute S (em,  1). 
Clearly, seem, l) is bounded by the number of ways of classifying Xm+l,  ... , Xm+Z 
using rules picked from em, that is, the total number of different values for 

We now show that regardless of Dm  and Xm+l, ... , Xm+z,  for n  ::::  4 we have 

This  sort of result is  typical-the bound does  not depend upon  Dm  or Tz.  When 
plugged back into the condition of convergence, it yields the simple condition 

logm 
- - -+ o. 

l 

In fact,  we may thus take 1 slightly larger than log m. It would plainly be silly.to 
take 1 = n12, as we would thereby in fact throwaway most of the data. 

392 

22.  Splitting the Data 

Set 

Am  = {(¢m(Xm+d,  "  . ,¢m(Xm+Z», ¢m  E  Cm}. 

To count the number of values in Am, note that all squared distances can be written 
as 

"Xi  - X j /l

2 = LaiPk,i,j, 

d 

k=l 

where  Pk,i,j  is  a  nonnegative  number  only  depending  upon  Xi  and  X j.  Note 
that  each  squared distance  is  linear in  (ar,  ... , aJ).  Now  consider the  space of 
(ar,  ... , a;). Observe that, in this space, ¢m(Xm+d is constant within each cell of 
the partition determined by the (;) hyperplanes 

LaiPkJ,m+l = LaiPk,il,m+l,  1 S  i  <  if Sm. 

d 

k=l 

d 

k=l 

To  see this,  note  that within each set in the partition,  ¢m(Xm+1)  keeps  the  same 
nearest neighbor among  Xl, ... , X m'  It is known that k  >  2 hyperplanes in Rd 
create partitions of cardinality not exceeding k d  (see Problem 22.1). Now, overlay 
the I  partitions obtained for ¢m(Xm+d, ... , ¢m(XnHz)  respectively. This yields at 
most 

sets,  as  the overlays are determined by I G)  hyperplanes. But clearly, on each of 
these sets, (¢m(Xm+1),  ... , ¢m(Xm+Z»  is constant. Therefore, 

S(Cm ,  /) ::0  IAml  ::0  «;)) d 

22.4  Classification Based on Clustering 

Recall the classification rule based on clustering that was introduced in Chapter 21. 
The data points Xl, ... , Xn  are grouped into k clusters, where k is a predetermined 
integer,  and  a  majority  vote  decides  within  the  k  clusters.  If k  is  chosen  such 
that k  --+  00  and k 2 log n / n  --+  0,  then the rule is  consistent.  For a given finite 
n,  however,  these  conditions  give  little  guidance.  Also,  the  choice  of k  could 
dramatically affect the performance of the rule, as there may be a mismatch between 
k and some unknown natural number of clusters. For example, one may construct 
distributions in which the optimal number of clusters does not increase with n. 
Let us  split the data and let the testing  sequence decide the value of k.  In the 
framework  of this  chapter, em  contains  the  classifiers based on  the  first  m  pairs 
Dm  of the data with  all  possible values  of k.  Clearly, en  is  a finite  family  with 
ICml  = m. In this case, by Problem 12.1, we have 

2Iog(2m) 

I 

The consistency result Theorem 21.5 implies that 

22.5 Statistically Equivalent Blocks 

393 

for all distributions, ifthe Xi'S have bounded support. Thus, we see that our strategy 
leads to a universally consistent rule whenever (log m)j 1 ~ O. This is a very mild 
condition,  since we can take l  equal to  a small fraction of n, without sacrificing 
consistency. If l  is  small compared to  n, then m  is close to n, so  inf¢mEcm  L(4)m) 
is  likely  to  be close to inf¢II EclI  L(4)n).  Thus, we do not lose much by sacrificing 
a part of the data for testing purposes, but the gain can be tremendous, as  we are 
guaranteed to be within -! (log m ) j 1 of the optimum in the class em. 

That we cannot take I = 1 and hope to obtain consistency should be obvious. It 
should also be noted that for I  =  m, we are roughly within -!log(m)jm of the best 
possible probability of error within the family. Also the empirical selection rule is 
jfog(m) j I-optimal. 

22.5  Statistically Equivalent Blocks 

Recall  from  Chapter  21  that  classifiers  based  on  statistically  equivalent  blocks 
typically  partition the  feature  space R,d  into  rectangles  such  that each rectangle 
contains k points, where k is a certain positive integer, the parameter of the rule. 
This  may  be  done  in  several  different  ways.  One  of many  such rules-the rule 
introduced by Gessaman (1970)- is consistent if k  ~ 00 and k j n  ~ 0 (Theorem 
21.4).  Again,  we  can let the data pick the  value of k  by minimizing the holdout 
estimate. Just as in the previous section,  lem I =  m, and every remark mentioned 
there  about consistency and  asymptotic  optimality remains  true  for  this  case  as 
well. 

We  can  enlarge  the  family  em  by  allowing  partitions  without restrictions  on 
cardinalities of cells.  This leads very quickly to  oversized families  of rules,  and 
we have to impose reasonable restrictions. Consider cuts into at most k rectangles, 
where k is a number picked beforehand. Recall that for a fixed partition, the class 
assigned to every rectangle is decided upon by a majority vote among the training 
points.  On the  real  line,  choosing a partition into  at most k  sets is  equivalent to 
choosing k - 1 cut positions from m + I + 1 =  n + 1 spacings between all test and 
training points. Hence, 

S(em,l):::;  k-1  :s(n+1)k-l. 

n + 1) 

(

For consistency, k has to grow as n grows. It is easily seen that 

394 

22.  Splitting the Data 

if k -+  00 and m  -+  00. To achieve consistency of the selected rule, however, We 
also need 

log S(Cm ,  l) 
- - - -<  

(k -

l 

-

l)log(n + 1) 

1 

-+0. 

Now  consider d-dimensional partitions defined by at most k - 1 consecutive 
orthogonal cuts, that is, the first cut divides Rd into two halfspaces along a hyper~ 
plane perpendicular to one of the coordinate axes. The second cut splits one of the 
halfspaces into two parts along another orthogonal hyperplane, and so forth. This 
procedure yields a partition of the space into k rectangles. We see that for the first 
cut, there are at most 1 + dn possible combinations to choose from. This yields the 
loose upper bound 

This bound is  also valid for all grids  defined by at most k  - 1 cuts.  The main 
difference here is that every cut defines two halfspaces of R d ,  and not two hyper(cid:173)
rectangles of a cells, so that we usually end up with 2k  rectangles in the partition. 
Assume that Cm  contains all histograms with partitions into at most k  (possibly 
infinite) rectangles. Then, considering that a rectangle in Rd requires choosing 2d. 
spacings between all test and training points, two per coordinate axis, 

See Feinholz (1979) for more work on such partitions. 

22.6  Binary Tree Classifiers 

We  can analyze binary tree classifiers from the same viewpoint. Recall that such 
classifiers are represented by binary trees,  where each internal node corresponds 
to  a  split of a cell by a hyperplane,  and the terminal nodes represent the cells of 
the partition. 

Assume that there are k cells (and therefore k - 1 splits leading to the partition). 
If every split is perpendicular to one of the axes, then, the situation is the same as 
in the previous section, 

For smaller families of rules whose cuts depend upon the training sequence only" 
the  bound is  pessimistic.  Others  have  proposed generalizing orthogonal  cuts by 
using general hyperplane cuts. Recall that there are at most 

ways of dichotomizing n points in nd  by hyperplanes (see Theorem 13.9). Thus, 
if we allow up to k  - 1 internal nodes (or hyperplane cuts), 

Problems and Exercises 

395 

S(em , l) ::::;  (1  + nd+1 )k-l . 

The number of internal nodes has  to  be restricted in order to  obtain consistency 
from this bound: Refer to Chapter 20 for more details. 

Problems and Exercises 

PROBLEM 22.1.  Prove  that  n  hyperplanes  partition n d  into  at  most  2..:.1=0  (7)  contiguous 
regions when d  :::  n  (Schlaffii (1950), Cover (1965)). HINT:  Proceed by induction. 

PROBLEM  22.2.  Assume that  gn  is  selected  so  as  to  minimize the  holdout error estimate 
Lt,m(¢m) over Cm,  the class of rules based upon the first m data points. Assume furthermore 
that we vary lover [10g2 n, nI2], and that we pick the best I (and m = n -I) by minimizing 
the holdout error estimate again.  Show that if S(Cm,  I)  = O(nY) for some y  >  0, then the 
obtained rule is strongly universally consistent. 

PROBLEM 22.3.  Let Cm  be the class of (aI, ... , ad)-NN rules based upon Dm.  Show that if 
min --+  1, then 

inf  L(¢m) -
<PmECm 

inf  L(¢n) --+  0 
<PnECn 

in probability 

for  all  distributions of (X, Y) for which X  has a density.  Conclude that if ljlogn  --+  00, 
m = n  -I, 1 = o(n), then 

L(gn) -

inf  L(¢n) --+  0 
<PnECn 

in probability. 

PROBLEM 22.4.  FINDING  THE  BEST  SPLIT.  This  exercise  is  concerned  with  the  automatic 
selection of m  and I = n  - m. If gn  is the selected rule minimizing the holdout estimate, 
then 

2log(4S(Cm ,[2))+16  (. 
--'------'--- +  mf  L(¢m) - L 

. 

(22.3) 

*) 

I 

<PmECm 

Since in most interesting cases S(Cm, I) is bounded from above as a polynomial ofm and I, 
the estimation error typically decreases as I increases. On the other hand, the approximation 
error inf<pmEcm  L(¢m) - L * typically decreases as m  increases,  as the class Cm gets richer. 
Some kind of balance between the two terms is required to get optimum performance. We 
may use the empirical estimates Lm,l (¢m) again to decide which value of m we wish to choose 
(Problem 22.2). However, as m gets large-and therefore I small-the class Cm  will tend to 
overfit  the data Tz,  providing strongly optimistically biased estimates for inf<pmEcm  L(¢m). 
To prevent overfitting, we may apply the method of complexity regularization (Chapter 18). 
By (22.1), we may define the penalty term by 

rem, I) = 

2log (4e 8E{S(Cm, [2)l) + logn 

I 

396 

22.  Splitting the Data 

and minimize the penalized error estimate Lm,l(¢) = Lm,l(¢) + rem, l) over all ¢  E U~l=lCm. 
Denote the selected rule by ¢:. Prove that for every n and all distributions of (X,  y), 

. 

( 

mm  3 
m,! 

21og(4E{S(CI111 F)})+logn+16 

l 

+  (E {  inf  L( ¢m) - L * })) +  ~ . 
y  n 

<PmECm 

HINT:  Proceed similarly to the proof of Theorem 18.2. 

23 
The Resubstitution Estimate 

Estimating the error probability is of primordial importance for classifier selection. 
The  method explored in the previous chapter attempts  to  solve this  problem by 
using  a testing sequence to obtain a reliable holdout estimate. The independence 
of testing  and  training  sequences  leads  to  a rather straightforward analysis.  For 
a good  performance,  the  testing  sequence  has  to  be  sufficiently  large  (although 
we often get away with testing sequences as  small as about log n). When data are 
expensive, this constitutes a waste. Assume that we do not split the data and use the 
same sequence for testing and training. Often dangerous, this strategy nevertheless 
works if the class of rules from which we select is sufficiently restricted. The error 
estimate in this case is appropriately called the resubstitution estimate and it will 
be  denoted  by  L~R). This  chapter explores  its  virtues  and pitfalls.  A  third  error 
estimate,  the  deleted estimate,  is  discussed in the next chapter.  Estimates based 
upon other paradigms are treated briefly in Chapter 31. 

23.1  The Resubstitution Estimate 

The  resubstitution  estimate  L~R) counts  the  number of errors  committed on  the 
training sequence by the classification rule. Expressed formally, 

Sometimes L~R) is called the apparent error rate. It is usually strongly optimisti-

398 

23.  The Resubstitution Estimate 

cally biased.  Since the classifier gn  is tuned by  Dn, it is  intuitively clear that gn 
may behave better on Dn  than on independent data. 

The best way to demonstrate this biasedness is to consider the  I-nearest neigh_ 
bor rule. If X  has  a  density,  then the nearest neighbor of Xi  among Xl, ... , Xn 
is  Xi  itself with probability one. Therefore,  L~R) ==  0,  regardless of the value of 
Ln  = L(gn). In this case the resubstitution estimate is useless. For k-nearest neigh_ 
bor rules with large k,  L~R) is close to Ln. This was demonstrated by Devroye and 
Wagner (1979a), who obtained upper bounds on the performance of the resubsti_ 
tution estimate for the k-nearest neighbor rule without posing any assumption on 
the distribution. 

Also, if the classifier whose error probability is to be estimated is a member of a 
class of classifiers with finite Vapnik-Chervonenkis dimension (see the definitions 
in Chapter 12), then we can get good performance bounds for the resubstitution 
estimate.  To  see  this,  consider any generalized linear classification rule,  that is, 
any rule that can be put into the following form: 

n(X) = {O 

g 

if aO.n -: L1~1 ai,n1/!i(X)  ~ ° 

1  otherwIse, 

where  the  1/!i'S  are  fixed  functions,  and the coefficients  ai,n  depend on the data 
Dn  in  an  arbitrary  but measurable way.  We  have the following  estimate for the 
performance of the resubstitution estimate L~). 

Theorem 23.1.  (DEVROYE  AND  WAGNER  (1976A)).  For  all nand E  >  0,  the  re(cid:173)
substitution estimate L~) of the error probability Ln  of a generalized linear rule 
satisfies 

PROOF. Define the set An  C  Rd X  {O,  I} as the set of all pairs (X,  y)  E  Rd X  {O,  1}, 
on which gn  errs: 

An  = {(x, y)  : gn(x) =I y}. 

Observe that 

and 

or denoting the measure of (X, Y) by 1), and the corresponding empirical measure 
by l)n, 

and 

23.2 Histogram Rules 

399 

The set An  depends on the data Dn, so that, for example, E{ vn(An)}  =t E{ v(An)}. 
Fortunately, the powerful Vapnik-Chervonenkis theory comes to the rescue via the 
inequality 

ILn  - L~R)I:::Ssup Ivn(C) 

v(C)I, 

CEC 

where C is the family of all sets of the form {(x, y)  : ¢(x) =t y}, where ¢  : Rd  ---+ 
{a,  1}  is  a  generalized linear  classifier based  on  the  functions  0/1,  ... , 0/ d*.  By 
Theorems  12.6,  13.1, and 13.7 we have 

P {sup IVn(C)  - v(C)1  ~ E}  :s;  8nd * e-nE2j3Z.  0 

CEC 

Similar inequalities can be obtained for other classifiers. For example, for par(cid:173)

titioning rules with fixed partitions we have the following: 

Theorem 23.2.  Let gn  be a classifier whose value is constant over cells of a fixed 
partition ofRd  into k  cells.  Then 

P {IL~R) - Lnl  ~ E}  :s;  8. 2ke-nt2j3Z. 

The proof is left as  an exercise (Problem 23.1). From Theorems 23.1  and 23.2 
we get bounds for the expected difference between the resubstitution estimate and 
the actual error probability Ln. For example, Theorem 23.1  implies (via Problem 
12.1) that 

( ~) 
E {IL~R) - Lnl} = 0  V -;;- . 

In  some  special  cases  the  expected  behavior  of the  resubstitution  estimate  can 
be  analyzed  in  more  detail.  For example,  McLachlan  (1976)  proved  that  if the 
conditional  distributions  of X  given  Y  = 0  and  Y  =  1 are  both  normal  with 
the  same covariance matrices,  and  the  rule  is  linear and based on the  estimated 
parameters, then the bias of the estimate is of the order 1/ n: 

McLachlan also showed for this case that for large n the expected value of the re(cid:173)
substitution estimate is smaller than that of L n ,  that is, the estimate is optimistically 
biased, as expected. 

23.2  Histogram Rules 

In  this  section,  we  explore  the  properties  of the resubstitution  estimate for  his(cid:173)
togram rules.  Let P  = {AI, Az, ... } be a fixed partition of Rd, and let gn  be the 

400 

23.  The Resubstitution Estimate 

corresponding histogram classifier (see Chapters 6 and 9). Introduce the notation 

The  analysis  is  simplified if we rewrite the  estimate  L~R) in the following  form 
(see Problem 23.2): 

(23.1) 

It is also interesting to observe that L~R) can be put in the following form: 

L~R) = f (I{1Jl,n(X)::::1Jo.n(X)} 171,11 (X) + I{1Jl,n(X»1JO,n(x)} 170,11 (X»)  f.1.,(dx), 

where 

170,I1(X)  =  ----.::.------(cid:173)

11  LJ=l  {Yj-O,XjEA(x)} 

.1  "'1: 

I  _ 
f.1.,(A(x » 

and 

71I,n(X)  = - - - - - - - (cid:173)

.1  "'~  I  _ 
11  Lj=l  {Yj-l,XjEA(x)} 
f.1.,(A(x» 

We can compare this form with the following expression of Ln: 

Ln  = f (I{1J l,n (X)::::1JO.n (x)} 11(X) + I{1Jl,n(X»1JO,n(X)}(l  - 71(X»)  f.1.,(dx) 

(see  Problem  23.3).  We  begin  the  analysis  of performance  of the  estimate by 
showing that its mean squared error is not larger than a constant times the number. 
of cells in the partition over n. 

Theorem 23.3.  For any distribution of (X, Y) andfor all n,  the estimate L~R) of 
the error probability of any histogram rule satisfies 

Also, the  estimate is optimistically biased,  that is, 

If,  in  addition,  the  histogram  rule  is based on a partition P  ofRd  into at most k 
cells,  then 

PROOF.  The  first  inequality  is  an  immediate  consequence  of Theorem  9.3  and 
(23.1). Introduce the auxiliary quantity 

R* = Lmin{vo(AJ, VI (Ai)}, 

where  vo(A)  =  P{Y  =  0, X  E  A},  and  VI (A)  =  P{Y  =  1,  X  E  A}.  We use the 
decomposition 

23.2 Histogram Rules 

401 

(23.2) 

First we bound the second term on the right-hand side of (23.2). Observe that R* 
is just the Bayes error corresponding to the pair of random variables  (T(X), Y), 
where  the function  T  transforms  X  according  to  T (x)  = i  if x  E  Ai.  Since the 
histogram classification rule gn(x) can be written as  a function of T(x), its error 
probability  Ln  cannot be  smaller than  R*.  Furthermore,  by  the  first  identity  of 
Theorem 2.2, we have 

0::::;  Ln  - R* 

=  L I{sign(vl,n(Ai)-vo,n(Ai»¥sign(vl(Ai)-vo(Ai»}lvl(Ai)  - VO(Ai)1 

If the partition has at most k cells, then by the Cauchy-Schwarz inequality, 

E {cLn  - R*)2} 

<  E { (~ IV1 (A;) - vo(A;)  _  (vI,n(A,) _  vo,n(A,)) I) 2} 

<  k LVar {Vl,n(Ai) -vo,n(Ai )} 

We bound the first term on the right-hand side of (23.2): 

As we have seen earlier, Var{L~)} ::::;  lin, so it suffices to bound IR*  - E{L~R)}I. 
By Jensen's inequality, 

<  I: min (E{vo,n(A i )}, E{Vl.n(Ai )}) 

i 

=  R*. 

402 

23.  The Resubstitution Estimate 

So we bound R* - E{L~R)} from above. By the inequality 

Imin(a, b) - minCe, d)1  :::  la  - el  + Ib  - dl, 

we have 

R*  - E{L~)}  <  LE {lvo(Ad - vO,n(Ai)1  + IVl(A i )  - vI,n(Adl} 

<  L ()Var{vo,n(Ai )} + JVar{v1,n(AJ}) 

i 

(by the Cauchy-Schwarz inequality) 

=  ~ (  Vo(Ai )(I: VO(Ai))  +  VI(A,)(I: VI (A,)) ) 

<  ~ (  2(vo(A,)(I- vo(Ai))n+ vI(A,)(I- VI(A,)))) 

<  I:.tIL:A,) 

I 

where we used the elementary inequality fa + -Jb :::  J2(a + b). Therefore, 

(R'  _ E{L~R»))2 :"  (~tIL:Ai)) 2 

To  complete the  proof of the  third inequality,  observe that if there are at most k 
cells, then 

< 

k  ~ ' "  2fh(Ai) 

~ 

k. 

l 

n 

(by Jensen's inequality) 

Therefore, (R*  - E{L;:)})2 :::  2k/n. Finally, 

E{Ln }  - E{L~R)} = (E{L n }  - R*) + (R*  - E {L~R)}) ::::  O.  0 

We  see that if the partition contains a small number of cells, then the resubsti(cid:173)

tution estimate performs very nicely. However, if the partition has a large number 
of cells, then the resubstitution estimate of Ln  can be very misleading, as the next 
result indicates: 

23.3 Data-Based Histograms and Rule Selection 

403 

Theorem 23.4.  (DEVROYE AND  WAGNER (1976B)). For every n there exists a par(cid:173)
titioning rule and a distribution such that 

PROOF.  Let  AI, ... , A2n  be  2n  cells  of the  partition  such  that  fl,(AJ  = 1/(2n), 
i  ==  1,  ... , 2n.  Assume further  that  1J(x)  = 1 for  every  x  E  n d
,  that is,  Y  = 1 
with probability one.  Then clearly L~R) = O.  On the other hand, if a cell does not 
contain any of the data points Xl, ... , X n, then gn(x)  =  0 in that cell.  But since 
the number of points is  only half of the number of cells,  at least half of the cells 
are empty. Therefore Ln  ::::  1/2, and IL~R) - Ln I ::::  1/2. This concludes the proof. 
o 

23.3  Data-Based Histograms and Rule Selection 

Theorem 23.3 demonstrates the usefulness of L~R) for histogram rules with a fixed 
partition, provided that the number of cells in the partition is not too large. If we 
want  to  use  L~R) to  select a good classifier,  the  estimate  should work uniformly 
well over the class from which we  select a rule.  In this  section we explore such 
data-based histogram rules. 
Let F  be a class of partitions of nd. We will assume that each member of F  par(cid:173)
titions n d  into at most k cells. For each partition P  E  F, define the corresponding 
histogram rule by 

if 2::7=1  I{Y;=l,X;EA(x)}  :::;  2::7=1  I{y;=o,x;EA(x)} 
otherwise, 

where A(x) is the cell of P that contains x. Denote the error probability of g~P)(x) 
by 

The corresponding error estimate is denoted by 

L?\P) = L min{vo,n(A), vl,n(A)}. 

AEP 

By  analogy  with  Theorems  23.1  and  23.2,  we  can  derive  the  following  result, 
which  gives  a  useful  bound for  the largest difference between  the  estimate  and 
the  error probability within the class of histogram classifiers  defined by F. The 
combinatorial  coefficient  /)'~(F) defined  in  Chapter 21  appears  as  a  coefficient 
in  the  upper bound.  The  computation  of /)'~(F) for  several  different classes  of 
partitions is illustrated in Chapter 21. 

404 

23.  The Resubstitution Estimate 

Theorem 23.5.  (FEINHOLZ  (1979)).  Assume that each member of F  partitions 
nd into at most k cells.  For every nand E  >  0, 

p{sup IL~R)(p) - Ln(P)1  >  E}  :::  8. 2k~~(F)e-nE2/32. 

PEF 

PROOF.  We  can proceed as  in the proof of Theorem 23.1.  The shatter coefficient 
S(C, n) corresponding to the class of histogram classifiers defined by partitions in 
F  can clearly be bounded from  above by the number of different ways in which 
n  points  can be partitioned by  members of F,  times  2k,  as  there  are  at most 2k 
different ways to assign labels to cells of a partition of at most k cells.  0 

The theorem has two interesting implications. The error estimate L~R) can also 
be used to  estimate the perfOlmance of histogram rules based on data-dependent 
partitions (see Chapter 21). The argument ofthe proof of Theorem 23.3 is not valid 
for these rules. However, Theorem 23.5 provides performance guarantees for these 
rules in the following corollaries: 

COROLLARY 23.1.  Let gn(x) be a histogram classifier based on a random partition 
Pn  into at most k cells,  which is determined by the  data  Dn.  Assume that for any 
possible realization ot the training data Dn, the partition P n is a member of a class 
of partitions F. If Ln  is the error probability of gn,  then 

p  { I L (R)  - L  I >  E}  <  8 . 2k tJ,,* (F)e -nE 2

n 

n 

-

n 

/32 
' 

and 

(R)  _ 

E  Ln 

{( 

)2}  <  32k + 3210g(~~(F)) + 128 
. 

_ 

Ln 

n 

PROOF.  The first inequality follows from Theorem 23.5 by the obvious inequality 

p {IL~R) - Ln I >  E}  :::  P 1 sup IL~R)(p) - Ln(P)1  >  E}  . 

PEF 

The second inequality follows from the first one via Problem 12.1.  0 

Perhaps the most important application of Theorem 23.5 is in classifier selection. 
Let Cn  be a class of (possibly data-dependent) histogram rules. We may use the error 
estimate L~R) to select a classifier that minimizes the estimated error probability. 
Denote the selected histogram rule by ¢/:' that is, 

L~R)(¢,:) :::  L~R)(¢n)  for all  ¢n  E  Cn. 

Here L~R)(¢n) denotes the estimated error probability of the classification rule ¢In. 
The question is how well the selection method works,  in other words, how close 
the error probability of the selected classifier L(¢,:) is to  the error probability of 
the best rule in the class, infcPnEcn  L(¢n). It turns out that if the possible partitions 
are not too complex, then the method works very well: 

Problems and Exercises 

405 

COROLLARY 23.2.  Assume  that jor any  realization  oj the  data  Dn ,  the  possible 
partitions that define the histogram classifiers in en  belong to a class ojpartitions 
F,  whose members partition nd into at most k cells.  Then 

P  { L(¢Z) - ¢:~t L(¢n) > E}  ::s  8 . 2' /',,~(:F)e-n"/128 

PROOF.  By Lemma 8.2, 

L(¢,:) -

inf  L(¢n) ~ 2  sup  IL~R)(¢n) - L(¢n)1  ~ 2 sup  IL~R)(p) - L(P)I. 
~~ 

~~ 

P6 

Therefore, the statement follows from Theorem 23.5.  0 

Problems and Exercises 

PROBLEM 23.1.  Prove Theorem 23.2. HINT: Proceed as in the proof of Theorem 23.1. 

PROBLEM 23.2.  Show that for  histogram rules,  the resubstitution estimate may be written 
as 

PROBLEM 23.3.  Consider the resubstitution estimate  L~R) of the error probability  Ln  of a 
histogram  rule  based  on  a  fixed  sequence  of partitions  Pn .  Show  that  if the  regression 
function estimate 

7Jn(x) = ~ L;=~~~j(:);jEA(X)} 

is consistent, that is, it satisfies E {J 17J(x)  - 7Jn(X)lfL(dx)}  ~ ° as n  ~ 00, then 

lim  E {IL;:) - Lnll = 0, 

n-+oo 

andalsoE{L~R)} ~ L*. 

PROBLEM 23.4.  Let (X~, Y{),  ... , (X~, Y~l) be a sequence that depends in an arbitrary fash(cid:173)
ion  on  the data  Din  and let gn  be the  nearest neighbor rule  with  (X~ , Y{),  ... , (X~l' Y/;) , 
where m is fixed.  Let L~R) denote the resubstitution estimate of Ln  = L(gn). Show that for 
all E  > ° and all distributions, 

p  {ILn  - L;:)I  ::::  E}  ::::  8ndm(m-l)e-nE2/32. 

PROBLEM 23.5.  Find a rule for (X, Y)  E  R  x {O,  I} such that for all nonatomic distributions 

with L * = ° we have E{Ln} ~ 0, yetE {L~R)} ::::  E{Ln}. (Thus, L~R) maybe pessimistically 

biased even for a consistent rule.) 

PROBLEM 23.6.  For histogram rules on fixed partitions (pattitions that do not change with 
n and are independent of the data), show that E  {L~R)} is monotonically nonincreasing. 

PROBLEM 23.7.  Assume that X has a density, and investigate the resubstitution estimate of 
the 3-NN rule. What is the limit of E {L~R)} as n  ~ oo? 

24 
Deleted Estimates of 
the Error Probability 

The  deleted estimate  (also  called cross-validation,  leave-one-out,  or  U-method) 
attempts  to  avoid  the  bias  present in  the  resubstitution  estimate.  Proposed  and 
developed  by Lunts  and Brailovsky (1967), Lachenbruch (1967),  Cover (1969), 
and Stone (1974), the method deletes the first pair (Xl, Yl) from the training data 
and makes a decision gn-l using the remaining n - 1 pairs. It tests for an an error 
on (Xl, Yl), and repeats this procedure for all n pairs of the training data Dn. The 
estimate L~D) is the average the number of errors. 

We formally denote the training set with (Xi, YJ deleted by 

Then we define 

Clearly, the deleted estimate is almost unbiased in the sense that 

Thus,  L~D) should be viewed as  an estimator of Ln- l , rather than of Ln.  In most 
of the  interesting cases  Ln  converges with probability one so  that the difference 
between Ln- l  and Ln  becomes negligible for large n. 

The  designer has  the  lUXury  of being  able  to  pick the  most convenient  gn-l. 
In  some  cases  the  choice  is  very  natural,  in  other cases  it is  not.  For example, 
if gn  is  the  I-nearest neighbor rule,  then  letting  gn-l  be the  I-nearest neighbor 
rule based on n  - 1 data pairs  seems to  be an obvious choice.  We  will  see later 

408 

24.  Deleted Estimates of the Error Probability 

that this  indeed yields  an extremely good estimator.  But what should gn-l  be if 
gn  is, for example, a k-NN rule? Should it be the k-nearest neighbor classifier, the 
I-nearest neighbor classifier,  or maybe  something else? Well,  the  choice is 
k  -
typically nontrivial and needs careful attention if the designer wants a distribution_ 
free performance guarantee for the resulting estimate.  Because of the variety of 
choices  for  gn-l,  we  should  not  speak  of the  deleted  estimate,  but rather of a 
deleted estimate. 

In this chapter we analyze the performance of deleted estimates for a few proto(cid:173)

type classifiers, such as the kernel, nearest neighbor, and histogram rules. In most 
cases studied here, deleted estimates have good distribution-free properties. 

24.1  A General Lower Bound 
nonparametric rules. An error estimate Ln is merely a function (Rd  x  {O, l}r -+ 

We  begin by exploring general limitations of error estimates for some important 

[0,  1], which is applied to the data Dn  = ((X I, YI),  ... , (Xn, Yn». 

Theorem 24.1.  Let gn  be one of the following classifiers: 

(a)  The kernel rule 

gn(X)  =  {  ~  if "n  I 

I  L.d=l  {Yi=O} 
otherwise 

K  (X-Xi)  ~n  I 
-h-:::: L....i=l 

{Yi=l} 

K  (X-Xi) 
-h-

with  a  nonnegative  kernel  K  of compact support and smoothing factor 
h  >  0. 

(b)  The histogram rule 
(x) = {O 

if"L7=1  I{Yi=l}I{xiEA(x)}  :s  L:7=1  I{yi=o}I{xiEA(x)} 

gil 

1  otherwise 

based on a fixed partition P  = {AI, A 2 ,  ... } containing at least n  cells. 

(c)  The  lazy histogram rule 

where Xj is the minimum-index point among Xl, ... ,  Xnforwhich Xj  E 
Ai, whereP = {AI, A 2 ,  ... } isajixedpartition containing atleastn cells. 
Denote the probability of error for gil  by Ln  = P{gn(X,  Dn)  =I  Y/Dn}.  Then/or 
with L * = ° such that 
every n  ::::  10, and/or every error estimatorLn, there exists a distribution o/(X, y) 

The theorem says that for any estimate Ln, there exists a distribution with the 
property thatE {ILn(Dn) - Ln I} ::::  1/ ,J32n. For the rules gn given in the theorem, 

24.1  A General Lower Bound 

409 

no error estimate can possibly give better distribution-free performance guarantees. 
Error  estimates  are  necessarily  going  to  perform  at least  this  poorly  for  some 
distributions. 
PROOF  OF THEOREM  24.1.  Let Ln  be an arbitrary fixed error estimate. The proof 
relies on randomization ideas similar to those of the proofs of Theorems 7.1 and 7.2. 
We construct a family of distributions forCX,  Y).Forb  E  [0,  1)letb = 0.b 1b2b3 •.. 
be its binary expansion.  In all cases the distribution of X  is uniform on n  points 
{Xl,  ... , x n }.  For the histogram and lazy histogram rules choose Xl,  ... , Xn  such 
that they fall into different cells. For the kernel rule choose Xl,  ... , Xn  so that they 

are isolated from each other, that is,  K CXi~Xj) = ° for all  i, j  E  {1,  ... , n}, i  =I  j. 
To  simplify the notation we will refer to these points by their indices, that is, we 
will write X = i  instead of X = Xi. For a fixed b define 

Y  =bx . 

We may create infinitely many samples Cone for each b  E  [0,  1»  drawn from the 
distribution of CX,  Y) as follows. Let X I,  ... ,  Xn be i.i.d. and uniformly distributed 
on {1,  ... , n}. All the samples share the same Xl, ... , X n ,  but differ in their Y/s. 
For given  Xi, define  Yi  = b Xi • Write  Zn  = CX 1, ... , Xn)  and Ni  = :L):=l  I{xj:=i}' 
Observe that Dn  is a function of Zn  and b. It is clear that for all classifiers covered 
by our assumptions, for a fixed b, 

where  S  = {i  :  1  :::;  i  :::;  n, Ni  = O}  is  the  set of empty bins.  We randomize the 
distribution  as  follows.  Let B  = 0.BIB2 ... be a uniform  [0,  1]  random variable 
independent of CX,  Y).  Then clearly  B I ,  B 2 ,  •.. are  independent uniform binary 
random variables. Note that 

sUPE{ILnCDn)-Lnl}  >  E{ILnCDn)-Lnl} 
b 

Cwhere b is replaced by B) 
E {E { ILnCDn)  - Ln II  Zn} } . 

In what follows we bound the conditional expectation within the brackets. To make 
the dependence upon  B  explicit we write LnCZn,  B) = LnCDn)  and LnCB) = Ln. 
Thus, 

E { ILn(Zn,  B) - Ln(B)11  Zn} 

= 

--

1 
2E {ILn(Zn, B) - LnCB)1  + ILnCZn,  B*)  - LnCB*)11  Zn} 
(where B; = Bi  for i  with Ni  > ° and B; = 1 - Bi  for i with Ni  = 0) 

--

:::  ~E {lLnCB) - Ln(B*)11  Zn} 
(since Ln(Zn, B*) = LnCZn,  B» 

410 

2:4.  Deleted Estimates of the Error Probability 

(recall the expression for Ln  given above) 

=  ~E{12B(ISI, 1/2) -111 lSI} 

2n 
(where B(ISI, 1/2) is a binomial (lSI, 1/2) random variable) 

::::  ~ fIST 
2nVT 

(by Khintchine's inequality, see Lemma A.S). 

In summary, we have 

We have only to bound the right-hand side. We apply Lemma AA to the random 
variable JIST = JJ:.7==11{Ni=Oj.  Clearly, EISI  = nO  - l/n)n, and 

}  =  E {t l{Ni==oJ  + L l{Ni==O,Ni==OJ} 

.  i==l 

i=/j 

=  EISI + n(n - 1)  1 -
( 

2)11 
-;; 

E {ISI 2

so that 

In (1- ~r + n(n - 1) (1- ~r 

(1- ~r 
::::  v'n----;::===== 
I  + (1-2/n)" 
11 
(1-l/n)" 

(use  1 -"ii 

( 

1)11 

1 
::::;) 

The proof is now complete.  0 

24.2 A General Upper Bound for Deleted Estimates 

411 

24.2  A General Upper Bound for Deleted Estimates 

The  following  inequality  is  a  general  tool  for  obtaining  distribution-free  upper 
bounds for the difference between the deleted estimate and the true error probability 
Ln: 
Theorem 24.2.  (ROGERS  AND  WAGNER (1978); DEVROYE AND  WAGNER (1976B)). 
Assume that gn  is a symmetric classifier,  that is,  gn(x,  Dn)  ==  gn(x,  D~), where D~ 
is obtained by permuting the pairs of Dn  arbitrarily.  Then 

PROOF.  First we express the three terms on the right-hand side of 

The first term can be bounded, by using symmetry of gn,  by 

E {L~D)2} 

=  E  { (~ ~ J{g"  ,(X,D",)¥y;) r } 

E {-;. t  I{gn_l(Xi,Dn,i)¥Yd} 

n 

i=l 

The second term is written as 

E {L~D) Ln} 

E {Ln~ t  I{gn-l(Xi,DIl,i)¥Yd} 

n  i=l 

412 

24.  Deleted Estimates of the Error Probability 

1  n 
- LE {P{gn(X, Dn) ¥ Y,  gn-l(Xi , Dn,J ¥ YiIDn}} 
n  i=l 
P{gn(X, Dn) ¥ Y,  gn-I(Xl, Dn,l) ¥ Yd. 

For the third term,  we introduce the pair (XI,  yl), independent of X,  Y,  and  D n, 
having the same distribution as (X, Y).  Then 

E{L~}  =  E{P{gn(X,Dn)¥YIDnf} 

=  E {P{gn(X, Dn) ¥ YIDn}P{gn(XI, Dn) ¥ yIIDn}} 
=  E {P{gn(X, Dn) ¥ Y,  gn(XI, Dn) ¥ yIIDn }} 

where we used independence of (XI, yl). 

We introduce the notation 

D~  = 

Ak,i,j 

(X3, Y3),  ... , (Xn,  Yn), 
{gn(Xk; (Xi, Yi ), (Xj , Yj ), D~) ¥ Yd ' 

Bk,i  =  {gn-l(Xk;(Xi'Yi),D~)¥Yd, 

and we formally replace (X, Y) and (XI, yl) by (Xa, Ya) and (Xf3 , Y(3)  so that we 
may work with the  indices a  and f3.  With this notation, we have shown thus far 
the following: 

E {(L(D) - L  )2}  <  2.  +  P{Aa,1,2,  Af3,1,2}  - P{Aa,1,2,  BI,2} 

n 

n 

- n 

Note that 

Also, 

P{Aa,1,2,  A,B,1,2}  - P{Aa,1,2,  B1,2} 

=  P{A a ,1,2,  Af3 ,1,2}  - P{A a ,t3,2,  B{3,2} 

(by symmetry) 

P{Aa,l,2, A,B,1,2}  - P{Aa,,B,2,  A,B,1,2} 
+ P{Aa,,B,2,  Af3,1,2}  - P{Aa,,B,2,  B,B,2} 
I  + II. 

= 

P{B1,2,  B2,d - P{Aa,1,2,  Bl,2} 

=  P{Ba ,{3,  B{3,a}  - P{Aa,,B,2,  B{3,2} 

(by symmetry) 

24.3 Nearest Neighbor Rules 

413 

P{Ba,tl,  Btl.a} - P{Aa,tl,2,  Btl,a} 
+ P{Aa,tl,2,  Btl,a}  - P{Aa,tl,2,  Btl,2} 
III+IV. 

Using  the  fact  that for  events  {C}'  IP{Ci ,  C j }  - P{Ci ,  Ck}1  ::::  P{Cj 6Ck},  we 
bound 

I 

<  P{Aa,I,26Aa,tl,2}' 
def 

II  <  P{Atl,I,26Btl,2}  = V, 
III  <  P{Ba,tl6Aa,tl,2} = V, 
IV  <  P{Btl ,a 6Btl,2}' 

The upper bounds for II and III are identical by symmetry. Also, 

and 

IV ::::  P{Btl,a6Atl,a,2} + P{Atl,a,26Btl,2}  = 2V, 

for a grand total of 6 V. This concludes the proof.  D 

24.3  Nearest Neighbor Rules 

Theorem  24.2  can be  used to  obtain distribution-free upper bounds  for  specific 
rules.  Here is the most important example. 

Theorem 24.3.  (ROGERS AND  WAGNER (1978». Let gn  be the k-nearest neighbor 
rule with randomized tie-breaking. If L~D) is the deleted estimate with gn-l chosen 
as the k-NN rule (with the same k and with the same randomizing random variables), 
then 

E  Ln  -Ln 

{( 

(D) 

)2} 

6k+l 

::::  - - .  

n 

PROOF.  Because of the randomized tie-breaking, the k-NN  rule is  symmetric, and 
Theorem 24.2 is applicable. We only have to show that 

Clearly, gn(X,  Dn) =I gn-l (X,  Dn- l ) can happen only if Xn  is among the k nearest 
neighbors  of X. But the probability of this event is just kin, since by symmetry, 
all points are equally likely to be among the k  nearest neighbors.  D 

414 

24.  Deleted Estimates ofthe Error Probability 

REMARK.  If gn  is  the  k-NN  rule  such that distance ties  are broken by compaling 
indices, then gn  is not symmetric, and Theorem 24.2 is no longer applicable (unless 
e.g., X has a density). Another non symmetric classifier is the lazy histogram rule: 
o 

REMARK. Applying Theorem 24.3 to the I-NN rule, the Cauchy-Schwarz inequality 
implies E I L~D) - Ln I ::;  ,J7/ n for all distributions.  0 

REMARK.  Clearly, the inequality of Theorem 24.3  holds for any rule that is some 
function of the k nearest points. For the k-nearest neighbor rule, with a more careful 
analysis, Devroye and Wagner (1979a) improved Theorem 24.3 to 

(D) 
E  L 
{( 
n 

(see Problem 24.8).  0 

)2} 

n 

24,J1( 
-L  < -+ - -
- n  nhii 

1 

Probability inequalities for IL~D) - Ln I can also be obtained with further work. 

By Chebyshev's inequality we immediately get 

so  that  the  above  bounds  on  the  expected  squared  error  can  be  used.  Sharper 
distribution-free inequalities were obtained by Devroye and Wagner (1979a; 1979b) 
for several nonparametric rules. Here we present a result that follows immediately 
from what we have already seen: 

Theorem 24.4.  Consider the k-nearest neighbor rule with randomized tie-break(cid:173)
ing. If L~D) is the deleted estimate with gn-l chosen as the k-NN rule with the same 
tie-breaking,  then 

PROOF.  The result follows  immediately from McDiarmid's inequality by the fol(cid:173)
lowing argument: from Lemma 11.1, given n points in nd ,  a particular point can 
be among the k nearest neighbors of at most kYd points. To see this, just set fJv  equal 
to  the empirical measure of the n points in Lemma 11.1. Therefore, changing the 
value of one pair from the training data can change the value of the estimate by at 
most 2kYd. Now,  since EL~D) = ELn-l, Theorem 9.1  yields the result.  0 

Exponential upper bounds for the probability P{IL~D) - Ln I >  E}  are typically 

much harder to obtain. We mention one result without proof. 

24.4 Kernel Rules 

415 

Theorem 24.5.  (DEVROYE  AND  WAGNER  (1979A».  For  the  k-nearest  neighbor 
rule, 

P{IL~D) - Lnl  >  E}  :s  2e-nE2

/ 18 + 6e-nE3 /(108k(Yd+2». 

One  of the  drawbacks  of the deleted  estimate  is  that it  requires  much  more 
computation than the resubstitution estimate. If conditional on  Y  = 0 and Y  = 1, 
X is gaussian, and the classification rule is the appropriate parametric rule, then the 
estimate can be computed quickly. See Lachenbruch and Mickey (1968), Fukunaga 
and Kessel (1971), and McLachlan (1992) for further references. 

Another, and probably more serious, disadvantage of the deleted estimate is its 
large variance. This fact can be illustrated by the following example from Devroye 
and Wagner (1979b): let n be even, and let the distribution of (X, Y) be such that 
Y is  independent of X  with P{Y = O}  = P{Y = I}  = 1/2. Consider the k-nearest 
neighbor rule with k = n 
1.  Then obviously,  Ln  = 1/2. Clearly, if the number 
of zeros and ones among the labels Y1,  ••• ,  Yn  are equal, then L~D) =  1.  Thus, for 
0< E  <  1/2, 

P{IL~{)) - Lnl  >  E}  2:  P {t I{Y;=ll}  = ~} = ;n C~2). 

By Stirling's formula (Lemma A.3), we have 

(D) 
P{ILn 

- Lnl  >  E}  ~  ~ 1/12. 

1 

1 

V 2nn e 

Therefore, for this simple rule and certain distributions, the probability above can 
not decrease to zero faster than  1/ -JIi.  Note that in the  example above,  EL~D) = 
ELn-1  = 1/2, so the lower bound holds for P{IL~D)  EL~D)I >  E}  as well. Also, 
in this example, we have 

E {(L~D) - EL~»)2} ~ ~p {t l(y;=o}  = !2.}  2:  ~ 1:12. 

2 

4  2nn e 

4 

i=l 

In Chapter 31  we describe other estimates with much smaller variances. 

24.4  Kernel Rules 

Theorem 24.2 may also be used to obtain tight distribution-free upper bounds for 
the performance of the deleted estimate of the error probability of kernel rules. We 
have the following bound: 

Theorem 24.6.  Assume that K  ~ 0 is a regular kernel of bounded support,  that 
is,  it is a function satisfying 

(i) 

(ii) 

(iii) 

K(x)  ~ [3, 
K(x) :s  B, 
K(x) = 0, 

IIxll:s p, 

Ilxll  >  R, 

416 

24,  Deleted Estimates of the Error Probability 

for some positive finite constants f3,  p, Band R. Let the kernel rule be defined by 

(x) = {O  ifL~1=1 I{Yi=o}K (x  - Xi) :: L~1=1 I{Yi=l}K (x  - Xi) 

gn 

1  otherwise, 

and define gn-1  similarly.  Then there exist constants C1 (d) depending upon d  only 
and C2(K) depending upon K  only such that for all n, 

One may take C2 (K) = 6(1 + Rlp)d /2 min(2, B I f3). 

REMARK.  Since C2(K) is  a scale-invariant factor,  the theorem applies to the rule 
with K(u) replaced by Kh(U)  =  tK (*) for any smoothing factor.  As itis, C2(K) 
is  minimal  and  equal  to  12  if we  let  K  be the  uniform  kernel  on  the  unit ball 
(R = p, B  =  f3).  The assumptions of the theorem require that gn-l is defined with 
the same kernel and smoothing factor as gn'  0 

REMARK. The theorem applies to virtually any kernel of compact support that is of 
interest to the practitioners. Note, however, that the gaussian kernel is not covered 
by the  result.  The theorem generalizes an earlier result of Devroye  and Wagner 
(1979b), in which a more restricted class of kernels was considered. They showed 
that if K  is the uniform kernel then 

CD) 

E  Ln 

{  (

)2} 

- Ln 

1 

24 
:::  2n  + Jfi' 

See Problem 24.4.  0 

We need the following auxiliary inequality, which we quote without proof: 

Lemma 24.1.  (PETROV (1975), p.44).  Let Zl, ... , Zn  be real-valuedi.i.d. random 
variables.  For E  >  0, 

where C is a universal constant. 

COROLLARY 24.1.  Let Zl, ... , Zn  be real-valued i.i.d.  random variables. ForE  2: 
A> 0, 

I  E}  CE 

Z·  <  - < - -

P 

{I n 
~ I 

2 

- Jfi A -JP{I Z 11  :: A/2}' 

1 

where C  is a universal constant. 

PROOF OF THEOREM 24.6.  We apply Theorem 24.2, by finding an upper bound for 

24.5 Histogram Rules 

417 

For the kernel rule with kernel K  :::0, in whichh is absorbed, 

P {gn(X,  Dn) =I gn-l (X,  Dn- l )} 

:S  p {I ~(2Yi -

l)K(X - Xill  :S  K(X - Xn),  K(X - Xn)  > o} . 

Define B' = max(2fJ, B). We have 

p {1~(2Yi - l)K(X - X;) I :S  K(X - Xn),  K(X - Xn)  > o} 

<  p {1~(2Yi - I)K(X - Xi)1  :S  B', Xn  E  SX.R} 

(where SX,R  is the ball of radius R  centered at X) 

=  E 

{ 
I{xnESx,R} 2fJ.Jn",JP{I(2YI  -

2CB' 
l)K(X - Xdl  ~ fJIX} 

} 

(by Corollary 24.1, since 2fJ  ::::  B') 

<  E 

{ 

I{xnESx,R} fJJnfL(Sx,p) 

CB' 

J 

(recall that K(u)  ~ fJ  for  lIull  ::::  p, 
so that P{I(2YI -

I)K(x - XI)I  ~ fJ}  ~ P{X I  ~ Sx,p}) 

CB' f Is 

-
fJ.Jn" 

= 

fL(dy) 

fL(dx) 

Sx,R  J  fL(Sx,p) 

CCd B '  ( 
R)d/2 
- - 1+-
p 
fJ.Jn" 

where  we used Lemma 10.2. The constant Cd  depends upon the dimension only. 
o 

24.5  Histogram Rules 

In this section we discuss properties of the deleted estimate of the error probability 
of histogram rules.  Let P  =  {AI, A 2 ,  ... }  be a partition of R d
,  and let gn  be the 
corresponding histogram classifier (see Chapters 6 and 9). To  get a performance 
bound for the deleted estimate, we can simply apply Theorem 24.2. 

418 

24.  Deleted Estimates of the Error Probability 

Theorem 24.7.  For the histogram rule gn  corresponding to any partition p, and 
for all n, 

E {(L(D)  _ L )2}  <  _  + 6"  f-L 

1 

n 

n 

-

L  J  (  _ 1)  Lr  I 

+ 6" 1/2(A.)e-n/L(A i ) 

, 

(A  )3/2 

i 
7T  n 

n 

i 

i 

and in particular, 

D) 

E  Ln 

{  (

)2} 

- Ln 

1 + 6/ e 

<  - - + 
-

n 

6 

. 
J7T(n  - 1) 

PROOF.  The first  inequality follows  from  Theorem 24.2  if we can find  an  upper 
bound for P{gn(X) =I gn-l (X)}. We introduce the notation 

Clearly, gn-l (X) can differ from gn(X) only if both Xn  and X fall in the same cell 
of the partition, and if the number of zeros in the cell is either equal, or less by one 
than the number of ones. Therefore, by independence, we have 

P{gn(X) =I gn-l(X)} 

L P {gn(X) =I gn-l(X)1 X  E  Ai, Xn  E  Ad f-L(Ai)2 
"  { 

If-Ln-I(AJJI 

<  7 P  vO,n-l(Ai )= 

XEAi,XnEAi 

2 

} 

f-L(AJ. 
2 

The terms in the sum above may be bounded as follows: 

P  vO,n-1(A)= 

{ 

I

f-Ln-1(A)JI 

2 

XEA,XnEA 

} 

<  P{f-Ln-1 (A) = O} 

2 

P  VO,n-1 (A) = 

(by independence) 

If-Ln-1(A)J} 

{ 
+ E {p { VO,"-I (Al = If.'n-~ (A) J I XI, ", , Xn }  J[~"_,(A»O) } 
(l - f.'( A l)" + E { J2rr (n  _ l1)f.'n_1 (A 1 I{~"  ,(A»O) } 

:0: 

(by Lemma A.3) 

::: 

(1  -

f-L(A)r +  E  { 

I{/Ln_l(A»O} 

} 

(by Jensen's inequality) 

27T(n  - 1)Mn-1 (A) 

< 

e-n/L(A) + 

Problems and Exercises 

419 

where  in  the last step we use Lemma A.2.  This concludes  the proof of the  first 
inequality.  The second one follows  trivially by noting that xe-x 
::::;  I/e for all x. 
o 

REMARK. It is  easy to  see that the inequalities of Theorem 24.7  are tight,  up to  a 
constant factor, in the sense that for any partition p, there exists a distribution such 
that 

D) 

E  Ln 

{  (

- Ln 

)2} 

1 

1 

~  ~ 1/12 

4  2nn e 

(see Problem 24.5).  0 

REMARK. The second inequality in Theorem 23.3 points out an important difference 
between the behavior of the resubstitution and the deleted estimates for histogram 
rules.  As mentioned above, for some distributions the variance of L~) can be of 
the order I/.Jli. This should be contrasted with the much smaller variance of the 
resubstitution estimate. The small variance of L~R) comes often with a larger bias. 
Other types of error estimates with small variance are discussed in Chapter 31.  0 

REMARK. Theorem 24.3  shows that for any partition, 

sup E {(L~D) - Ln)2}  =  0  (  ~). 

~n 

(X,Y) 

On the other hand, if k =  o( .Jli), where k is  the number of cells in the partition, 
then for the resubstitution estimate we have a better guaranteed distribution-free 
performance: 

sup E {(L~R) - Ln)2}  = 0  (  ~). 

~n 

(X,Y) 

At first sight, the resubstitution estimate seems preferable to the deleted estimate. 
However,  if the partition has  a large number of cells,  L~R) may be off the mark; 
see Theorem 23.4.  0 

Problems and Exercises 

PROBLEM 24.1.  Show the following variant of Theorem 24.2: For all symmetric classifiers 

E { (L:fl  - Ln)2}  :S  ~  +  2P{gn(X,  Dn) =I gn-l (X, Dn- 1)} 

+  P{gn(X,  Dn) =I gn(X,  D~)} 
+  P{gn-l(X, Dn- 1)  =I gn-l(X, D~_J}, 

420 

24.  Deleted Estimates of the Error Probability 

where  D,:  and DZ- I  are just Dn  and Dn- l  with (Xl, Yj )  replaced by an independent copy 
(Xo, Yo). 

PROBLEM  24.2.  Let gn  be the relabeling NN  rule with the k-NN classifier as  ancestral rule 
as  defined in Chapter 11.  Provide an upper bound for the squared error E { (L~D) - Ln)2} 
of the deleted estimate. 

PROBLEM 24.3.  Let gil  be the rule obtained by choosing the best k  ::::  ko in the k-NN rule (ko 
is a constant) by minimizing the standard deleted estimate L ~D) with respect to k. How would 
you estimate the probability of error for this rule? Give the best possible distribution-free 
performance guarantees you can find. 

PROBLEM  24.4.  Consider the kernel rule with the window kernel K  = SO, I. Show that 

E{(L(D)-L  )2}  <  __  + 

n 

n 

-

1 + 6je 

n 

6 

. 
In(n - 1) 

HINT:  Follow the line of the proof of Theorem 24.7. 

PROBLEM  24.5.  Show that for any partition P, there exists a distribution such that for the 
deleted estimate of the error probability of the corresponding histogram rule, 

E 

{  (

(D) 

Ln 

)2} 

- Ln 

1 

1 

2:  4J2nn e l / 12  . 

HINT:  Proceed as in the proof of the similar inequality for k-nearest neighbor rules. 

PROBLEM  24.6.  Consider the k-spacings method (see Chapter 21). We estimate the proba~ 
bility of error (Ln) by a modified deleted estimate Ln as follows: 

where gn,i  is  a histogram rule based upon the same k-spacings partition used for gn-that 
is,  the partition determined by k-spacings  of the data points  X I,  ... , Xn-but in which a 
majority vote is based upon the Yj's in the same cell of the partition with Yi  deleted. Show 
that 

E{(Ln  -Ln)}::::  6k+l. 

n 

HINT:  Condition on the  Xi'S, and verify that the inequality of Theorem 24.2 remains valid. 

PROBLEM  24.7.  Consider a rule in which we rank the real-valued observations XI, ... , Xn 
from  small  to  large,  to  obtain  XC!),  ... ,  X(n)'  Assume  that  XI  has  a  density.  Derive an 
inequality for the error IL~D) - LIlI for some deleted estimate L~D) (of your choice), when 
the rule is defined by a majority vote over the data-dependent partition 

defined by 1,2,3,4, ... points, respectively. 

Problems and Exercises 

421 

PROBLEM  24.8.  Prove that for the k-nearest neighbor rule, 

E {(L~D) - Ln)2}  :s  .!.  +  24-Jk 
n.  n-Jiii 

(Devroye and Wagner (1979a)). HINT:  Obtain a refined upper bound for 

using techniques not unlike those of the proof of Theorem 24.7. 

PROBLEM 24.9.  OPEN-ENDED PROBLEM. Investigate if Theorem 24.6 can be extended to ker(cid:173)
nels with unbounded support such as the gaussian kernel. 

25 
Automatic Kernel Rules 

We saw in Chapter 10 that for a large class of kernels, if the smoothing parameter h 
converges to zero such that nh d  goes to infinity as n  -+  00, then the kernel classifi(cid:173)
cation rule is universally consistent. For a particular n, asymptotic results provide 
little guidance in the selection of h. On the other hand, selecting the wrong value 
of h may lead to catastrophic error rates-in fact, the crux of every nonparametric 
estimation  problem is  the  choice  of an  appropriate  smoothing factor.  It tells  us 
how far we generalize each data point Xi in the space. Purely atomic distributions 
require  little  smoothing  (h  = 0  will  generally be fine),  while  distributions  with 
densities  require  a  lot  of smoothing.  As  there  are  no  simple  tests  for  verifying 
whether the data are drawn from an absolutely continuous distribution-let alone 
a distribution  with a Lipschitz density-it is  important to let the  data  Dn  deter(cid:173)
mine  h.  A  data-dependent  smoothing  factor  is  merely  a  mathematical  function 
Hn:  (Rd  x  {O,  l}f -+  [0,  00). For brevity, we will simply write Hn  to denote the 
random  variable  Hn(Dn).  This chapter develops results regarding such functions 
Hn. 

This  chapter is not a lUXury  but a necessity.  Anybody developing software for 
pattern  recognition  must  necessarily  let  the  data  do  the  talking-in fact,  good 
universally applicable programs can have only data-dependent parameters. 

Consider the family of kernel decision rules gn  and let the smoothing factor h 
play the role of parameter. The best parameter (HoPT )  is the one that minimizes Ln. 
Unfortunately,  it is  unknown,  as  is  L OPT ,  the  corresponding minimal  probability 
of error.  The first  goal of any  data-dependent smoothing factor  Hn  should be to 
approach  the performance of HoPT '  We  are  careful here to  avoid saying that  Hn 
should  be  close  to  HOPT>  as  closeness  of smoothing factors  does  not necessarily 
imply closeness of error probabilities and vice versa.  Guarantees one might want 

424 

2~.  Automatic Kernel Rules 

in this respect are 

E{Ln - L oPT } ~ an 
for some suitable sequence an  ~ 0, or better still, 

E{Ln - L *}  ~ (1 + .Bn)E{LoPT  - L *}, 

for another sequence.Bn  ~ O.  But before one even attempts to develop such data(cid:173)
dependent smoothing factors,  one's first concern should be with consistency: is it 
true that with the given Hn,  Ln  ~ L * in probability or with probability one? This 
question is dealt with in the next section. In subsequent sections, we give various 
examples of data-dependent smoothing factors. 

25.1  Consistency 

We  start with  consistency  results  that generalize  Theorem  10.1.  The  first  result 
assumes that the value of the smoothing parameter is picked from a discrete set. 

Theorem 25.1.  Assume that the random variable Hn  takes its values from the set 
of real numbers of the form  (1+Lk'  where k  is a nonnegative integer and 8n >  O. 
Let K  be a  regular  kernel function.  (Recall  Definition  10.1.)  Define  the  kernel 
classification rule corresponding to the random smoothing parameter Hn  by 

gn(X)  = 

K  (X-Xi) 

~:::: Li=l  {Yi=l}  ~ 

",n 

I 

K  (X-Xi) 

{ ° if",n 

1  otherwise. 

I 

1  Li=l  {Yi=O} 

If 

and 

Hn  ~ ° and  nHI~ ~ 00  with probability one as n  ~ 00, 

then L(gn)  ~ L * with probability one, that is,  gn  is strongly universally consistent. 

PROOF.  The  theorem  is  a  straightforward  extension  of Theorem  10.1.  Clearly, 
L(gn) ~ L* with probability one if and only if for every E  >  0,  I{L(gn)-L*>E}  -+ 0 
with probability one. Now, for any .B  >  0, 

I{L(gn)-L*>E}  ~ I{l/Hn>f3,nH,~>f3,L(gn)-L*>E} + I{l/Hnsf3} + I{nH,~Sf3}' 

We have to show that the random variables on the right-hand side converge to zero 
with probability one. The convergence of the second and third terms follows from 
10.1, since it states that for any E  >  0, there exist.B  > ° and no  such that for the 
the  conditions  on  Hn.  The  convergence of the  first  term  follows  from  Theorem 
error probability Ln,k of the kernel rule with smoothing parameter h  =  (l +t )k  , 

P{L 

n,k 

- L * >  E}  <  4e-CnE2 

_ 

25.1  Consistency 

425 

for  some  constant  c  depending  on  the  dimension  only,  provided  that  n  >  no, 
h  <  1/ fJ  and nh d  >  fJ·  Now clearly, 

P {Ln  - L*  >  E,l/Hn >  fJ,  nH: >  fJ} 

<  P { 

sup 

k:(l +8n)k > jJ,n/(l +8,z)kd > jJ 

Ln,k  - L * >  E} 

<  Cn 

sup 

P{Ln,k  - L*  >  E}, 

k:(l +8)k > {J,n/(l +8)kd > {J 

by the union bound, where Cn is the number of possible values of Hn  in the given 
range. As 

we note that 

Cn  <  2 + 

-

~ log (~) - log fJ 

log(l + on) 

=  0  (logn/8 2)  = eo(n) 

n 

by  the  condition  on  the  sequence  {on}.  Combining  this  with Theorem  10.1,  for 
n >  no, we get 

P {Ln  - L * >  E,  1/ Hn  >  fJ,  n H~ >  fJ}  :::;  4Cn e -cnE

2 

, 

which is summable in n. The Borel-Cantelli lemma implies that 

with probability one, and the theorem is proved.  0 

For  weak  consistency,  it  suffices  to  require  convergence  of  HI1  and  nH:  in 

probability (Problem 25.1): 

Theorem 25.2.  Assume that the random variable Hn  takes its values from the set 
of real numbers of the form  (l+~n)k'  where k  is a nonnegative integer and On  >  O. 
Let K  be a regular kernel.  If 

and 

HI1  ----+  0  and  nHl~ ----+  00 

in probability as n  ----+  00, 

then the kernel classification rule corresponding to the random smoothing param(cid:173)
eter Hn  is universally consistent,  that is,  L(gn) ----+  L * in probability. 

We are now prepared to prove a result similar to Theorem 25.1 without restricting 
the possible values of the random smoothing parameter Hn.  For technical reasons, 

426 

25.  Automatic Kernel Rules 

we need to assume some additional regularity conditions on the kernel function: K 
must be decreasing along rays starting from the origin,. but it should not decrease 
too rapidly. Rapidly decreasing functions such as the Gaussian kernel, or functions 
of bounded support, such as the window kernel are excluded. 

Theorem 25.3.  Let K  be a regular kernel that is monotone decreasing along rays, 
that is, for any x  E  nd and a  >  1,  K(ax) ::::  K(x). Assume in addition that there 
exists a constant c  >  0 such that for every sufficiently small 0  >  0,  and x  E  Rd, 
K((l +o)x)  2:  (l-co)K(x). Let {Hn} be a sequence of random variables satisfying 

Hn  -7 0  and  nH: -7  00  with probability one,  as n  -7  00. 

Then the error probability L(gn) of the kernel classification rule with kernel K  and 
smoothing parameter Hn  converges to L * with probability one,  that is,  the rule is 
strongly universally consistent. 

REMARK.  The technical condition on K  is  needed to ensure that small changes in 
h  do  not cause dramatic changes in  L(gn).  We  expect some smooth behavior of 
L(gn) as a function of h. The conditions are rather restrictive, as the kernels must 
have infinite support and decrease slower than at a polynomial rate.  An example 
satisfying the conditions is 

K(x) = { ;/IIXII' 

if IIxll  :::;  1 
otherwise, 

where r  >  0 (see Problem 25.2). The conditions on Hn  are by no means necessary. 
We have already seen that consistency occurs for atomic distributions if K (0)  > 0 
and Hn  ==  0, or for distributions with L * = 1/2 when Hn  takes any value. However, 
Theorem 25.3 provides us with a simple collection of sufficient conditions.  0 

PROOF  OF  THEOREM  25.3.  First we  discretize  Hn.  Define  a  sequence  On  -+  0 
satisfying the condition in Theorem 25.1, and introduce the random variables H n 
and  Hn  as  follows:  H n  =  (l+8~,)Kll'  where  Kn  is  the  smallest  integer  such  that 
Hn  >  (l+Ol1)KI1 '  and let H n = (l + on)H n' Thus, H n  <  Hn  :::;  H n. Note that bothH.n 
and H n  satisfy the conditions of Theorem 25.1. As usual, the consistency proof is 
based on Theorem 2.3.  Here, however, we need a somewhat tricky choice of the 
denominator of the functions that approximate 1J(x).  Introduce 

1 

-

-

1717,Hll (X)  = 

1,,17 
~ L..,i=l 

J K 

K  (X-Xi) 

I 
(Yi=l}  ~ 
( )  

H,~  ~(dz) 

Clearly, the value of the classification rule gn(x) equals one if and only if17n,Hl1(XJ 
is greater than the function defined similarly, with the I{Yi=ll 's replaced with I{Yi=OI> 
Then by Theorem 2.3 it suffices to show that 

f rJ7;t,H,JX)  - 1J(x )I~(dx) -7 0 

25.1 Consistency 

427. 

with probability one. We use the following decomposition: 

f l17n,Hn (X)  - 1](x)IJL(dx) 
:::  f l1];l,H" (x) -ifn,81l (x)lfL(dx) + f l17n,8n (x) - 1](X)lfL(dx).  (25.1) 

The  second term on the right-hand side converges  to  zero with probability one, 
which can be seen by repeating the argument of the proof of Theorem 25.1, using 
the  observation that in the proof of Theorem  10.1  we proved consistency via an 
exponential probability inequality for 

f l1]n,h ll (X)  -1](X)lfL(dx). 

The first term may be bounded as the following simple chain of inequalities indi(cid:173)
cates: 

(from H n  = (1 + 8n )H n' and the condition on K, if n is large enough) 

c8n f  () " J1(dx), 

~ L:7=1  K (Xj/i) 
J K  ~z  fL(dz) 

H" 

= 

Since H n  satisfies the conditions of Theorem 25.1, the integral on the right-hand 
side converges to one with probability one, just as we argued for the second term 
on the right-hand side of (25.1). But 8n  converges to zero. Therefore, the first term 
on the right-hand side of (25.1) tends to zero with probability one.  0 

REMARK.  A quick inspection of the proof above  shows  that if {an}  and  {bn }  are 
deterministic sequences with the property that an  <  bn, bn  -+ 0, and na~ -+  00, 

428 

25.  Automatic Kernel Rules 

then for the kernel estimate with kernel as in Theorem 25.3, we have 

sup  Ln(h) ~ L *  with probability one 

allshsbll 

for  all  distributions.  One  would never  use  the  worst  smoothing  factor  over  the 
range [an,  bn],  but this corollary points out just how powerful Theorem 25.3 is.  0 

25.2  Data Splitting 

Our first  example  of a  data-dependent  Hn  is  based upon  the  minimization of a 
suitable error estimate. You  should have read Chapter 22 on data splitting if you 
want to understand the remainder of this section. 

The data sequence  Dn  =  (Xl, Y1),  ••• , (Xn'  Yn) is divided into two parts.  The 
first part Dm  = (Xl, Yl),  ... , (X m,  YnJ  is  used for  training,  while the remaining 
l  = n  - m pairs constitute the testing sequence: 

Tz  = (Xm+l,  ~n+d, ... , (Xm+Z,  Ym+Z). 

The training sequence Dm  is used to design a class of classifiers em, which, in our 
case is  the class of kernel rules  based on Dm ,  with all possible values of h  >  0, 
for fixed  kernel  K. Note that the value of the kernel rule gm(x)  with smoothing 
parameter h  is zero if and only if 

(x - X.) 

h 

Jmex)  = L(Yi  - 1/2)K  __  I  SO. 

m 

i=l 

Classifiers in em are denoted by 1m. A classifier is selected from em that minimizes 
the holdout estimate of the error probability: 

____ 

1 

Lm,l(¢m) = 7 L I{rpm(Xm+I)=!Ym+d' 

l 

i=l 

The particular rule selected in this manner is called gil' The question is how far the 
error probability L(gn) of the obtained rule is from that of the optimal rule in Cm. 

FINITE COLLECTIONS.  It is computationally attractive to restrict the possible values 
of h to a finite set of real numbers. For example, em could consist of all kernel rules 
with h  E  {2-km , 2-km+l ,  ... ,  1/2, 1,2, ... , 2km}, for some positive integer km' The 
advantage of this choice of Cm  is that the best h in this class is  within a factor of 
two  of the best h  among all possible real  smoothing factors,  unless the best h is 
smaller than 2-kl11 - 1 or larger than 2km+l.  Clearly,  lem I = 2km + 1,  and as pointed 
out in Chapter 22,  for the  selected rule  gn  Hoeffding's inequality and the union 
bound imply that 

P {L(gn) -

inf  L(¢m)  >  Ej  Dm}  :s  (4km + 2)e- ZE2
rpmECm 

2

/

. 

If km  = eo(l), then the upper bound decreases exponentially in I, and, in fact, 

25.2 Data Splitting 

429 

{ 

E  L(gn) -

inf  L(CPm)  = 0  - - .  
¢mECm 

}  (fiflOg(I») 

I 

By Theorem 10.1, em  contains a subsequence of consistent rules if m  ----+  00, and 
km ----+  00  as  n  ----+  00.  To  make sure that gn  is  strongly universally consistent as 
well, we only need that limn--+oo I = 00, and km  = eo(l)  (see Theorem 22.1). Under 
these conditions, the rule is -/log(km )/ I-optimal (see Chapter 22). 

The discussion above does little to help us  with the selection of m, I,  and km . 
Safe, but possibly suboptimal, choices might be I = n /10, m  = n -I, km  = 210g2 n. 
Note that the argument above is valid for any regular kernel K. 

INFINITE  COLLECTIONS.  If we do not want to exclude any value of the smoothing 
parameter,  and  pick h  from  [0,  00),  then em  is  of infinite  cardinality.  Here,  we 
need something stronger, like the Vapnik -Chervonenkis theory. For example, from 
Chapter 22, we have 

where S(em , I) is the l-th shatter coefficient corresponding to the class of classifiers 
em.  We now obtain upper bounds for seem, I) for different choices of K. 

Define the function 

Recall that for the kernel rule based on D m , 

(x) = {o  if fm(x,  Dm)  :::: ° 

1  otherwise. 

gm 

We introduce the kernel complexity Km: 

Km 

sup 

{Number of sign changes of 

j;n(X,  (Xl, YI),  ... , (Xm'  Ym»  as h varies from ° to infinity}. 

Suppose we have a kernel with kernel complexity Km.  Then, as h  varies from ° to 

infinity,  the binary I-vector 

changes at most IKm  times. It can thus take at most IKm + 1 different values. There(cid:173)
fore, 

430 

25.  Automatic Kernel Rules 

We  postpone the  issue  of computing kernel  complexities  until  the  next section. 
It suffices to note that if gn  is obtained by minimizing the holdout error estimate 
Lm,l(¢m) by varying h, then 

< 

< 

16 

log(8eS(Cm, I)) 

2l 

16 

log(8e(IKm + 1)) 

21 

(Corollary 12.1) 

(25.2) 

(25.3) 

Various  probability bounds  may also  be  derived from  the  results  of Chapter 12. 
For example, we have 

<  4e 8

(12Km + l)e-lE2

/

2

. 

(25.4) 

Theorem 25.4.  Assume that gn  minimizes the holdout estimate Lm,l(¢m) over all 
kernel rules withfixed kernel K  of kernel complexity Km, and overall (unrestricted) 
smoothing factors h  >  O.  Then  gn  is strongly universally consistent if 

(0 
(U) 

(iii) 

(iv) 

limn-+oo m = 00; 
lim  log Km  = 0; 
n-+oo 
. 
hm  - - =00; 
n-+oo  log n 
K  is a  regular kernel. 

1 
I 

For weak universal consistency,  (iii) may be replaced by (v):  limn-+oo I = 00. 

PROOF.  Note  that Cm  contains  a  strongly  universally  consistent  subsequence(cid:173)
take h  = m- 1/(2d)  for example, and apply Theorem  10.1, noting that h  ---+  0, yet 
mhd  ---+  00. Thus, 

lim 
n-+oo ¢mECm 

inf  L(¢m) = L *  with probability one. 

It suffices to apply Theorem 22.1  and to note that the bound in (25.4) is summable 
in n  when  lilog n  ---+  00  and  log Km  =  o(l).  For weak universal  consistency, a 
simple application of (25.2) suffices to note that we only need 1 ---+  00 instead of 
lilogn ---+  00.0 

Approximation errors decrease with m. For example, if class densities exist, we 
may combine the inequality of Problem 2.10 with bounds from Devroye and Gyorfi 
(1985) and Holmstrom and KlemeHi (1992) to conclude thatE {inf¢mEcm  L(¢m)}(cid:173)
L * is of the order of m-2a/(4+d),  with ex  E  [1,2], under suitable conditions on K 

25.3 Kernel Complexity 

431 

and the  densities.  By  (25.2),  the estimation error is  0  ( Jlog(IKm )/ I), requiring 
instead large values for I. Clearly, some sort of balance is called for.  Ignoring the 
logarithmic  term for now,  we  see  that I  should be roughly m 4a /(4+d)  if we are to 
balance errors of both kinds.  Unfortunately,  all of this  is  ad hoc and based upon 
unverifiable distributional conditions. Ideally, one should let the data select I and 
m. See Problem 25.3, and Problem 22.4 on optimal data splitting. 

For some distributions,  the estimation error is just too large to  obtain asymp(cid:173)

totic  optimality  as  defined  in  Chapter  22.  For  example,  the  best bound  on  the 
estimation error is  0  ( Jlog n / n ), attained when  Km  = I, I  = n. If the  distribu(cid:173)
tion  of X  is  atomic  with  finitely  many  atoms,  then  the  expected approximation 
error is  0(1/ JIii). Hence the error introduced by the selection process smothers 
the  approximation error when m  is  linear in n.  Similar conclusions may even be 
with  1J(x)  = 1 if x  E  [0,1] and  1J(x)  = ° if x  E  [3,4]. For the kernel rule  with 
drawn  when X has a density:  consider the uniform distribution on [0,  1]  U [3,4] 
h = 1, K  = 1[-1,1], the expected approximation error tends to L * = ° exponentially 
quickly in m, and this is always far better than the estimation error which at best 
is  0  ( Jlog n / n) . 

25.3  Kernel Complexity 

We now tum to the kernel complexity Km.  The following lemmas are useful in our 
computations. If l/ log n  ---+  00, we note that for strong consistency it suffices that 
Km  =  0  (mY)  for some finite  y  Gust verify the proof again). This, as it turns  out, 
is satisfied for nearly all practical kernels. 

Lemma 25.1.  Let ° :s  b 1  <  ... <  bm be fixed numbers,  and let ai  E  R,  1 :s  i  :s 
m,  befixed, with the restriction that am  i  0.  Then the function  f(x) = L~:1 aixh; 
has at most m  - 1 nonzero positive roots. 

PROOF.  Note  first  that  f  cannot  be  identically  zero  on  any  interval  of nonzero 
length. Let Z (g) denote the number of nonzero positive roots of a function g. We 
have 

Z  (taixhi) 

1=1 

Z (t aixCi) 

1=1 

(where Ci  = bi  - bI ,  all i; thus,  Cl  = 0) 

432 

25.  Automatic Kernel Rules 

(for a continuously differentiable g, we have Z(g) :::;  1 + Z(gl)) 

=  Z (ta;xb;) + 1 

l=2 

Note that the b;  are increasing, b~ = 0,  and a;  =I 0.  As  Z (amx bm
we derive our claim by simple induction on m.  0 

)  = ° for am  -:j 0, 

Lemma 25.2.  Let a I,  ... , am be fixed real numbers, and let bI ,  ... , bm be different 
nonnegative reals.  Then if ex  =I 0,  the function 
f(x) = L ai e- biX

",  x?: 0, 

m 

i=1 

is either identically zero,  or takes the value 0 at most m times. 

PROOF.  Define y  = e-xCt . If ex  =I 0,  y ranges from 0 to  1.  By Lemma 25.1,  g(y) = 
L;:1 ai ybi  takes the value 0 at at most m -1 positive y-values, unless it is identically 
zero everywhere. This concludes the proof of the lemma.  0 

A star-shaped kernel is one of the form K(x) = l{xEA} , where A is a star-shaped 
set of unit Lebesgue measure,  that is,  x  f:  A  implies cx  f:  A  for all  c  ?:  1. It is 
clear that  Km  =  m  - 1 by a simple thresholding  argument.  On the real line,  the 
kernel  K(x) = L~-oo aJ{xE[2i,2i+lJ}  for ai  >  0 oscillates infinitely often, and has 
Km  = 00 for all values of m  ?:  2. We must therefore disallow such kernels. For the 
same reason, kernels such as  K (x) = (sin x/x Y on the real line are not good (see 
Problem 25.4). 

If K  = L~=l ai hi for some finite k, some numbers ai and some star-shaped sets 

Ai, then Km  :::;  k(m  - 1). 

Consider next kernels of the form 

where  A  is  star-shaped, and r  ?:  0 is  a constant (see Sebestyen (1962)).  We  se~ 
that 

(x -x.) 

h 

1 
2 

1 

m 
i=1 

m 

i=1 

=  L(Yi  - -)K  __  I 

=  hr  I)Yi  - 2)!!x  - Xill- r 

1{(x-Xi)/hEA} , 

25.3  Kernel Complexity 

433 

which changes sign at most as  often as  f m (x) /  hr. From our earlier remarks, it is 
easy to  see that Km  =  m  - 1,  as  Km  is  the same as  for the kernel  K  =  I A. If A  is 
replaced by nd
, then the kernel is not integrable, but clearly, Km  = O.  Assume next 
that we have 

K(x) =  {  ;/IIXII' 

if Ilx II  :::  1 
otherwise, 

where r  >  O.  For r  >  d, these kernels are integrable and thus regular. Note that 

X) 

hr 
K  h  = I{llxIISh} + I{llxll>h} IIxll r

(

' 

As h increases, f m (x), which is of the form 

" 

~  I 

i:llx-Xilish 

(y.  _~) +hf 

2 

(y.  _~) _1_  
Ilx - Xl·llr ' 

2 

" 
~  I 

i:llx-Xill>h 

transfers an Xi  from one sum to the other at most m times. On an interval on which 
no  such transfer occurs,  fm  varies  as  a  + j3h r  and has  at most one sign change. 
Therefore, Km  cannot be more than m + 1 (one for each h-interval) plus m (one for 
each transfer), so that Km  :::  2m + l. For more practice with such computations, we 
refer to the exercises. We now continue with a few important classes of kernels. 

Consider next exponential kernels such as 

for some a  >  0, where II  . II  is any norm on nd. These kernels include the popular 
gaussian kernels. As the decision rule based on Dm  is of the form 

a simple application of Lemma 25.2 shows that Km  :::  m. The entire class of kernels 
behaves nicely. 

Among compact support kernels, kernels of the form 

K(x) = (t ai IIXll b

,) I{ljxjjSlI 

for  real  numbers ai, and hi  :::::  1,  are important.  A particularly popular kernel in 
d-dimensional density estimation is Deheuvels' (1977) kernel 

If the  kernel  was  K(x)  =  (I:7=1 ai Ilx Ilhi),  without the  indicator function,  then 
Lemma  25.1  would immediately yield the estimate Km  :::  k,  uniformly  over all 
m.  Such  kernels  would  be  particularly  interesting.  With  the  indicator  function 

434 

25.  Automatic Kernel Rules 

multiplied in, we have Km  ::::;  km, simply because  fm(x) at each h is based upon a 
subset of the Xj'S, 1 ::::;  j  ::::;  m, with the subset growing monotonically with h. For 
each subset, the function fm (x) is a polynomial in IIx II  with powers bI ,  ... , bk, and 
changes sign at most k times.  Therefore, polynomial kernels of compact suPpOrt 
also have small complexities.  Observe that the "k" in the bound Km 
::::;  km refers 
to the number of terms in the polynomial and not the maximal power. 

A  large class of kernels of finite  complexity may be obtained by applying the 
rich theory of total positivity. See Karlin (1968) for a thorough treatment. A real(cid:173)
valued function L  of two real variables is said to be totally positive on A  x B  c  'R} 
if for all n, and all S1  <  ... <  Sn,  Si  E  A, tl  <  ... <  tn, ti  E  B, the determinant of 
the matrix with elements L(Si, tj) is nonnegative. A key property of such functions 
is the following result, which we cite without proof: 

Theorem 25.5.  (SCHOENBERG  (1950».  Let L  be a  totally positive function on 
A  x  B  E  R 2,  and let a  : B  --+  R  be a bounded function.  Define the function 

f3(s)  = 1. L(s, t)a(t)a(dt), 

on A,  where a  is a finite measure on B.  Then  f3(s)  changes sign at most as many 
times as a(s) does.  (The number of sign changes of a function  f3  is defined as the 
supremum of sign changes of sequences of the form  f3(Sl),  ... , f3(sn),  where n is 
arbitrary,  and S1  <  ... <  sn.) 

COROLLARY 25.1.  Assume  that the  kernel  K  is  such  that the function  L(s, t)  = 

K(st)  is  totally positive for s  > ° and t  E  Rd.  Then  the kernel complexity of K 

satisfies Km  ::::;  m  - 1. 

PROOF. We  apply Theorem 25.5. We are interested in the number of sign changes 
of the function 

m 

f3(s)  =  L(2Yi  -

I)K((Xi  - x)s) 

i=l 

on s  E  (0, (0) (s plays the role of 1/ h). But f3(s)  may be written as 

f3(s)  = 1. L(s, t)a(t)a(dt), 

where L(s, t) = K(st), the measure a  puts mass Ion each t  = Xi -x, i  = 1,  ... ,m, 
and aCt) is defined at these points as 

a (t) = 2 Yi  - 1  if  t  =  Xi  - x. 

Other  values  of aCt)  are  irrelevant for  the  integral  above.  Clearly,  aCt)  can be 
defined such that it changes sign at most m - 1 times. Then Theorem 25.5 implies 
that  f3(s)  changes sign at most m  - 1 times, as desired.  0 

This corollary equips us  with a whole army of kernels with small complexity. 

For examples, refer to the monograph of Karlin (1968). 

25.4  Multiparameter Kernel Rules 

25.4 Multiparameter Kernel Rules 

435 

Assume that in the kernel rules considered in em, we perform an optimization with 
respect to more than one parameter. Collect these parameters in e,  and write the 
discrimination function as 

fm(x) = t (Yi - ~) Ke(x  Xi)' 

i=l 

2 

EXAMPLES: 

(i)  Product kernels. Take 

Ke(x)=TIK(  ~j))' 

h 

d 

j=l 

where e = (h(l),  ... , h(d)) is a vector of smoothing factors-one per dimen(cid:173)
sion-and K  is a fixed one-dimensional kernel. 

(ii)  Kernels of variable form. Define 

Ke(x) = e-Uxll"lh", 

where a  >  0 is  a  shape parameter,  and h  >  0 is  the  standard smoothing 
parameter. Here e = (a, h) is two-dimensional. 

(iii)  Define 

I 

1 

Ke(x) =  1 +xT RRTx 

1 + IIRxII 2 ' 

where x T  is the transpose of x, and R is an orthogonal transformation matrix, 
all of whose free components taken together are collected in e.  Kernels of 
this kind may be used to adjust automatically to a certain variance-covariance 
structure in the data. 

We will not spend a lot of time on these cases. Clearly, one route is to properly 
generalize the definition of kernel complexity. In some cases, it is more convenient 
to  directly  find  upper bounds for Seem, l).  In  the product kernel case,  with one(cid:173)
dimensional kernel 1[-1,1], we claim for example that 

Seem, I)  :s  (lm)d + 1. 

The corresponding rule takes a majority vote over centered rectangles with sides 
equal to 2h(l), 2h(2),  ... , 2h(d).  To  see why the inequality is true,  consider the d(cid:173)
dimensional quadrant of 1m  points obtained by taking the absolute values of the 
vectors  Xj  - Xi, m  <  j  :s  m + I  =  n,  1 :s  i  :s  m, where the absolute value of 
a vector is a vector whose components are the absolute values of the components 
ofthe vector. To compute Seem, l), it suffices to count how many different subsets 
can be obtained from these lm points by considering all possible rectangles with 
one vertex at the origin, and the diagonally opposite vertex in the quadrant. This is 
1 + (lm)d. The strong universal consistency of the latter family em  is insured when 
m -+  00 and I! log n ~ 00. 

436 

25.  Automatic Kernel Rules 

25.5  Kernels of Infinite Complexity 

In this section we demonstrate that not every kernel function supports smoothing 
factor selection based on data  splitting.  These kernels have infinite kernel com(cid:173)
plexity, or even worse, infinite vc dimension.  Some of the examples may appear 
rather artificial, but some "nice" kernels will surprise us by misbehaving. 

We begin with a kernel K  having the property that the class of sets 

has vc dimension VA::::  00 for any fixed value of Xl (Le., using the notation of the 
previous sections, Vel  ::::  (0). Hence, with one sample point, all hope is lost to use 
the  Vapnik-Chervonenkis  inequality  in  any  meaningful  way.  Unfortunately,  the 
kernel K  takes alternately positive and negative values. In the second part of this 
section, a kernel is constructed that is unimodal and symmetric and has  Vem  ::::  00 
for m  ::::  4 when D4  ::::  «Xl, Y1), (X2, Y2),  (X3,  Y3),  (X4,  Y4»  takes certain values. 
Finally, in the last part, we construct a positive kernel with the property that for 
any m and any nondegenerate non atomic distribution, limm~oo P  {Vem  ::::  oo}  ::::  1. 

We return to A. Our function is picked as follows: 

K(x):::: a(x)g(i),  2i  S  x  <  i+l,  i::: 1, 

where  g(i)  E  {-I, I} for  all  i,  a(x)  >  ° for  all  x,  a(x)  ,}  ° as  x  t  00, 
x  >  0,  and  a(x)  ,}  ° as  x 
,}  -00, x  <  ° (the  monotonicity  is  not essential 

and  may  be  dropped  but  the  resulting  class  of kernels  will  be  even  less  inter(cid:173)
esting).  We  enumerate all binary strings  in lexicographical order,  replace all O's 
by  -1 's,  and  map  the  bit  sequence  to  g(l), g(2), .... Hence,  the  bit  sequence 
0,1,00,01,10,11,000,001,010, ... becomes 

-1,1, -1, -1, -1,1,1, -1, 1,  1,  -1, -1, -1, -1, -1, 1,  -1,1, -1, .... 

Call this sequence S. For every n, we can find a set {x I,  ... , x n } that can be shattered 
by sets from  A. Take  {Xl,  .. , ,xn }  ::::  Xl + {2I, ... , 2n}.  A subset of this may be 
characterized by  a string  Sn  from  {-I, 1}n,  1 's  denoting  membership,  and -l's 
denoting absence. We find the first occurrence of Sn  in S and let the starting point 
be g(k). Take h  ::::  21-k. Observe that 

( K  (Xl  ~ X I  ) 

,  ... ,  K  ( Xn  ~ X I  )  ) 

:::: 

:::: 

(K(2k), ... ,K(2k+n-l)) 
(a(2k)g(k), a(2k+I)g(k + 1),  ... , a(2k+n-l)g(k + n  - 1)) , 

which agrees in sign with Sn  as desired. Hence, the vc dimension is infinite. 

The next kernel is symmetric, unimodal, and piecewise quadratic. The intervals 
into  which  [0,(0) is  divided  are  denoted by  Ao, AI, A2, ... ,  from  left  to  right~ 

25.5  Kernels of Infinite Complexity 

437 

where 

i  ::::  1 
i  = O. 

On each Ai, K  is of the form ax 2  + bx + c.  Observe that Kf!  = 2a  takes the sign 
of a.  Also, any finite symmetric difference of order 2 has the sign of a, as 

K (x + 8) + K (x  - 8)  - 2K (x) = 2a8 2

,  x  - 8, x, x  + 8 E  Ai. 

We take four points and construct the class 

A. = { {x : t K  ( x ~ X, )  (2Y,  - 1)  > o} : h  > o} , 

where  Xl  = -8, X2  = 8,  X3  = X4  = 0,  YI  = Y2  = 0,  Y3  = Y4 = 1 are fixed  for 
now. On Ai, we let the "a" coefficient have the same sign as g(i), known from the 
previous  example. All three quadratic coefficients are picked so that K  ::::  0 and 
K is unimodal. For each n,  we show that the set {21,  ... ,  2n}  can be shattered by 
intersecting with sets from  ~. A  subset is again identified by a {-I, l}n-valued 
string Sn,  and its first match in Sis g(k), ... , g(k + n - 1). We take h = 21-k. Note 
that for  1 .:::  i  .:::  n, 

sign ( K (2' ~ 8) + K (2' : 8) _ 2K (~) ) 

sign (K (2k+i - 1 

- 82k

- 1)  + K  (2k+i - 1 + 82k - l ) 

2K (2k+i - 1)) 

sign (Kf! (2k+i-l)) 

(if 8 .:::  1/4, by the finite-difference property of quadratics) 

g(k+i -1), 

as desired. Hence, any subset of {2 1 ,  ... ,  2n}  can be picked out. 

The previous example works  whenever 8  .:::  1/4. It takes just a  little thought 
to  see  that if (X 1, Yr),  ... , (X4, Y4) are i.i.d.  and drawn from  the  distribution of 
(X,  Y),  then 

P  IVAi = oo}  ::::  P{XI = -x2 ,  X3  = X4  = 0, Yl  = Yz  = 0, Y3 = Y4  = I}, 

and this is positive if given Y  = 0,  X has atoms at 8 and -8 for some 8  >  0;  and 
given Y  = 1, X has an atom at O.  However, with some work, we may even remove 
these restrictions (see Problem 25.12). 

We also draw the reader's attention to an 8-point example in which K  is sym(cid:173)

metric,  unimodal, and convex on [0,  (0), yet Ves  = 00. (See Problem 25.11.) 

Next we turn to our general m-point example. Let the class of rules be given by 

g 

m,h 

(x) -
-

{

if  "~1~ (2Y.  -

I 
0  otherwise, 

.L....l-l 

I 

l)K (X-Xi)  >  0 

h 

438 

25 ..  Automatic Kernel Rules 

h  >  0,  where  the  data  (Xl, YI), ... , (Xm, Ym)  are  fixed  for  now.  We  exhibit a 
nonnegative kernel  such that if the  Xi'S  are  different and not all  the  Yi's  are the 
same,  VCm  =  00.  This  situation occurs with probability tending to one whenever 
X is nonatomic and P{Y = I}  E  (0,  1).  It is stressed here that the same kernel K 
is used regardless of the data. 

The kernel is of the form K(x) = Ko(x)IA(x), where Ko(x) = e-x2

,  and A C  R 
will  be  specially  picked.  Order  X I,  ... ,  Xm  into  X(1)  < 
'"  <  X Cm)  and  find 
the first pair X(i),  XU+I)  with opposite values for  Y(i).  Without loss of generality, 
assume that XCi)  = -1, XU+1)  =  1,  2Y(i)  - 1 =  -1, and 2YU+I) - 1 = 1 (YCi)  is the 
Y-value that corresponds to  XCi)' Let the smallest value of IX(j)1,  j  =I i, j  =I i + 1 
be denoted by 8  >  1.  The contribution of all  such X(j)'s for x  E  [-8/3,8/3] is 
not more than (m  - 2)Ko((l + 28/3)/ h)  ~ mKo((l + 28/3)/ h). The contribution 
of either XCi)  or X(i+l) is at least Ko((1 +8/3)/ h). For fixed 8 and m (after all, they 
are given), we first find h* such that h  ~ h* implies 

1 + 8/3) 

Ko  - - - >mKo 

(

h 

(  1 + 28/3) 

h 

. 

For h  ~ h*, the rule, for x  E  [-8/3,8/3], is equivalent to a 2-point rule based on 

if  - K  eZI) + K  e-;;I)  >  0 

gm,h (x) =  0  otherwise. 

{

We define the set A by 

00  2k-1 

A=U U 3

k==l  1==0 

2k

1A k,l, 
+

where the  sets  Ak,l  are  specified later.  (For c  =I  0  and  B  C  R, c B  =  {x  E  R  : 
x / C  E  B}.) Here k represents the length of a bit string, and I  cycles through all 2k 
bit strings of length k.  For a particular such bit string, bI, ... , bk, represented by 
l, we define Ak,l as follows. First of all,  Ak,l  ~ [1/2, 3/2], so that all sets 32k
+1 Ak,l 
are nonoverlapping.  Ak,l consists of the sets 

(~ [ 1 -

(i + l1)2k'  1 -

(i  + ~)2k )  ) 

U (~1 (1 + (i + ~)2k' 1 + (i + 11)2k J) 

Ak,l is completed by symmetry. 

We now exhibit for each integer k a set of that size that can be shattered. These 
sets  will  be  positioned  in  [-8/3,0]  at  coordinate  values  -1/(2· 2P ), -1/(3· 
2 P),  ••• ,  -1/((k + 1) . 2P), where p is a suitable large integer such that 1/(2p+l )  < 
8/3. Assume that we wish to extract the set indexed by the bit vector (bl ,  ... , bk) 
(bi  = 1 means that -1/(i + 1)2P  must be extracted). To do so, we are only allowed 
to  vary h. First, we find  a pair (k', I), where k' is  at least k  and  1/(2k'+I)  <  8/3. 

25.6 On Minimizing the Apparent Error Rate 

439 

3 _2k'  S  h * is needed too. Also, 1 is such that it matches bI ,  ... , h  in its first k S  k' 
bits. Take h  = 3-2k' -l and p  = k' and observe that 

gn,h  ( 

= 

0 

>  K 

(i+l)~kl 
32k  +1 

if  K 

1 

(i+l)~kl 
32k  +1 

(i + 11 )2k'  ) 

0  otherwise 

{  c-' -')  C-' +1) 
{ 1 
{ 1 

if  1 + (i+i)2k'  E  Ak,l,  1 -
if  1 + (i+ i)2k'  1.  Ak,l,  1 -

0 
unimportant  otherwise 

if  bi  = 1 
if  bi  = 0, 

(i+i)2k'  ¢  Ak,l 
(i+i)2k'  E  Ak,l 

1 .:::;  i  S  k.  Thus,  we pick the desired set.  This construction may be repeated for 
all values of 1 of course. We have shown the following: 
Theorem 25.6.  If X  is  nonatomic,  P{Y  = I}  E  (0,  1),  and if VCm  denotes  the 
vc dimension  of the  class of kernel  rules based on  m  i.i.d.  data  drawn from  the 
distribution of (X , Y),  and with the kernel specified above,  then 

lim  P{Vcm  =  oo}  =  1. 
m-+oo 

25.6  On Minimizing the Apparent Error Rate 

In this section, we look more closely at kernel rules that are picked by minimizing 
the resubstitution estimate L~R) over kernel rules with smoothing factor h  >  O.  We 
make two remarks in this respect: 

(i)  The procedure is generally inconsistent if X is nonatomic. 
(ii)  The method is consistent if X is purely atomic. 

To  see  (i),  take  K  = l{so,d'  We  note that  L~R) = 0  if h  <  miniij IIXi  - Xj II  as 
gn(Xi ) = Yi  for such h. In fact, if Hn  is the minimizing h, it may take any value on 

If X is independent of Y,  P{Y = I}  = 2/3, and X has a density, then, 
lim  lim inf P {.  '. ~in _  II Xi - X j  II d.:::;  c2 }  =  1 

c--+oo  n--+oo 

l,j.Y;-l,Yj-O 

n 

(Problem  25.13).  Thus,  nHd  -+  0  in  probability,  and  therefore,  in  this  case, 
E{L(g,J}  -+  2/3  (since  ties  are  broken  by  favoring  the  "0"  class).  Hence  the 
Inconsistency. 

440 

25.  Automatic Kernel Rules 

Consider next X purely atomic.  Fix (X 1, Yd, ... , (Xn,  Yn),  the data;  and con(cid:173)
sider the class en  of kernel rules in which h  >  0 is  a free parameter. If a typical 
rule is  gn,h,  let An be the class  of sets  {x  : gn,h(X)  = I},  h  >  O.  If gn  is  the rule 
that minimizes L~R)(gn,h)' we have 

<  2  sup  'L~R\gn,h) - L(gn,h)1 

(Theorem 8.4) 

g",IzEC" 

<  2  sup  !!tn(A) -

!t(A)! + 2  sup  !!tn(A) -

!t(A)! 

AEAn 
(A~ denotes the collection of sets {x  : gn,h(X)  = OD 

AEA~ 

<  4 sup !!tn(B) -

BEB 

!t(B)!, 

where  B is  the collection of all  Borel sets.  However,  the latter quantity tends to 
zero with probability one-as denoting the set of the atoms of X by T, we have 

sup !fln(B) -
BEB 

/J,(B)! 

1 2 L l!tn({x D - !t({X D! 

xET 

1 

<  2 L !fln({xD -

XEA 

fl({xDI + !tn(A

C
) + fl(AC) 

(where A is an arbitrary finite  subset of T) 

<  2!t(AC) + 2 L !!tn({xD -

1 

XEA 

!t({xDI + !!tn(AC) -

!t(AC)I. 

The first term is small by choice of A and the last two terms are small by applying 
Hoeffding's inequality to  each of the  !AI + 1 terms.  For yet a different proof, the 
reader is referred to Problems 25.16 and 25.17. 

Note next that 

But if K(O)  >  0  and  K(x)  -+  0  as  IIxll  -+  00  along  any  ray,  then  we  have 
infgn,hEcn  L(gn,h)  :::  L(g;l)' where g;z  is the fundamental rule discussed in Chapter 
27 (in which h  = 0).  Theorem 27.1  shows that L(g:1)  -+  L * with probability one, 
and therefore, L(gn) --+  0 with probability one, proving 

Theorem 25.7.  Take  any kernel K  with  K(O)  >  0 and K(x)  -+  0 as  I!xll  --+  00 
along any  ray,  and let gn  be selected by minimizing  L~R) over all h  ::::  O.  Then 
L(gn)  --+  L * almost surely whenever the distribution of X  is purely atomic. 

25.7 Minimizing the Deleted Estimate 

441 

REMARK. The above theorem is all the more surprising since we may take just about 
any kernel, such as K(x) = e- lIx1e , K(x) = sin(IIx 1I)/IIx II,  or K(x) = (1 + IIxll)-1/3. 
Also,  if X  puts its mass on a dense subset of n d
,  the data will look and feel like 
data from an absolutely continuous distribution (well, very roughly speaking); yet 
there  is  a  dramatic  difference  with  rules  that  minimize  L~R) when the  X/s are 
indeed drawn from a distribution with a density!  0 

25.7  Minimizing the Deleted Estimate 

We have seen in Chapter 24 that the deleted estimate of the error probability of a 
kernel rule is generally very reliable. This suggests using the estimate as  a basis 
of selecting a  smoothing factor for the kernel rule.  Let L~D)(gn,h) be the deleted 
estimate of L(gn,h), obtained by using in gn-l,h the same kernel K  and smoothing 
factor h. Define the set of h' s for which 

L (D)( 

)  -
n  gn,h  -

h'E[O,OO) 

.  f  L(D)( 
In 

11 

) 
gn,h' 

by  A.  Set 

Hn  = inf {h  : h  E  A}. 

Two fundamental questions regarding Hn  must be asked: 

(a)  If we use Hn  in the kernel estimate, is the rule universally consistent? Note 
in this respect that Theorem 25.3 cannot be used because it is not true that 
Hn  ~ 0 in probability in all cases. 

(b)  If Hn  is  used  as  smoothing  factor,  how  does  E{L(gn)}  - L * compare  to 

E {infh L(gn,lJ}  - L *? 

To  our knowledge,  both questions have been unanswered so far.  We believe that 
this  way  of selecting  h  is  very  effective.  Below,  generalizing  a  result  by  Tutz 
(1986)  (Theorem 27.6), we show that the kernel rule obtained by minimizing the 
deleted estimate is  consistent when the  distribution  of X  is  atomic  with finitely 
many atoms. 
REMARK. If 1]  ==  I  everywhere, so that Yi  ==  I  for all i, and if K  has support equal 
to the unit ball in n d ,  and K  >  0 on this ball, then L~D)(gn,h) is minimal and zero 
if 

where  X f N  denotes the  nearest neighbor of X j  among the  data points  Xl,  ... , 
Xj-l, Xj+l' ... ,  X n .  But this shows that 

Hn  = m~x IIXj - XfNII. 

l-:s;-:sn 

442 

25.  Automatic Kernel Rules 

We leave it as an exercise to show that for any nonatomic distribution of X, n Hl~ ~ 
00 with probability one. However, it is also true that for some distributions, Hn  ~ 
00  with  probability  one  (Problem  25.19).  Nevertheless,  for  1]  ==  1,  the  rule  is 
consistent! 

If 1]  ==  0 everywhere, then Yi  = 0 for all  i. Under the same condition on K  as 
above, with tie breaking in favor of class "0" (as in the entire book), it is clear that 
Hn  ==  O.  Interestingly, here too the rule is  consistent despite the strange value of 
Hn. O 

We state the following theorem for window kernels, though it can be extended 

to include kernels taking finitely many different values (see Problem 25.18): 

Theorem 25.8.  Let gn  be the kernel rule with kernel K  =  IAfor some bounded set 
A  C  nd  containing the origin,  and with smoothing factor chosen by minimizing 
the deleted estimate as described above.  Then E{ L(gn)}  -+  L * whenever X  has a 
discrete distribution with finitely many atoms. 

PROOF. Let UI, ... , Un  be independent, uniform [0,  1] random variables, indepen(cid:173)
dent of the data D n ,  and consider the augmented sample 

where X;  = (Xi, Vi). For each h  ~ 0, introduce the rule 

g~,h(X!) 

= 

{ 

if  L~I=l K  (X~;i) (2Yi  - 1) - L~I=l K(0)(2Yi  -

o  otherwise, 

l)I{u=ui} >  0 

where  Xl  = (x, u)  E  nd  x  [0,1].  Observe  that  minimizing  the  resubstitution 
estimate  L~R)(g~,h) over these rules  yields  the  same h  as  minimizing  L~D)(gn,h) 
over the original kernel rules. Furthermore, E{ L(gn)} = E{ L(g~)} if g~ is obtained 
by minimizing L~R)(g;l,h)' It follows from the universal consistency of kernel rules 
(Theorem 1 0.1) that 

lim  inf L(g~ h)  -+  L *  with probability one. 
n--+oo  h 

' 

Thus,  it  suffices  to  investigate  L(g~) -
subsets A;l  C  nd x  [0,  1] by 

infh L(g~,h)' For a  given  D~, define  the 

A~ =  {Xl  : g~,h(XI) =  I},  h  E  [0, (0). 

Arguing as in the previous section, we see that 

25.7 Minimizing the Deleted Estimate 

443 

where  v  is  the  measure  of X'  on  nd  x  [0,  1],  and  Vn  is  the  empirical  measure 
determined by D~. Observe that each A~ can be written as 

Ah  = {x  E  Rd : t K e ~ Xi )  (2Yi - 1)  > o} , 

{(X, u)  End x  {UI , ... , Un}: 

t K e ~ Xi) (2Yi - 1) - K(O) t(2Yi -

l)I{u~U;}  > 0). 

where 

and 

B;1 

Clearly, as 

it  suffices  to  prove that sUPh  Ivn(B!1)  - M(Ah)1  --+  ° in probability,  as  n  --+  00. 

Then 

sup IVn(B~) -
h 

{l(Ah)1 

< 

< 

sup IVn(B~) - v(B~)1 + sup IvCB~) - M(Ah)1 
h 

h 

sup 

IVnCB')  - vCB')1  + sup Iv(B~) - M(Ah)l. 

B'cRdx{U1, ... ,Un } 

h 

The  first  term on  the right-hand side tends  to  zero in probability,  which may be 
seen  by  Problem  25.17  and  the  independence  of the  Ui ' s of Dn.  To  bound  the 
second term, observe that for each h, 

Iv(B;) -

fL(Ah)1  < 

fL ({x: It, K e ~ Xi) (2Y,  - o!-<:  K(O)}) 
=  t; P{X = Xj} I{!I:~=1 K( Xj~Xi )C2Yi-l)!:::K(O)} ' 

N 

where x I,  ... ,  X N  are the atoms of X.  Thus, the proof is finished if we show that 
for each Xj, 

444 

25.  Automatic Kernel Rules 

Now, we exploit the special form K  =  fA  of the kernel. Observe that 

= 

< 

Ii 

I 

-

h 

L..l~l  A 

su.p f{I",n  I  (Xj-Xi )C2Y-l)I<I} 
N L f{IVI+,,+Vk I:SI}, 

where 

k=l 

Vk  =  L  (2Yi  - 1) 

i:Xi=X(k) 

and X(l), X(2),  ... ,XCN)  is  an  ordering of the atoms  of X  such that x(1)  =  x j  and 
(Xj  - X(k))/ h  E  A  implies  (Xj  - X(k-l))/ h  E  A  for 1  <  k  ~ N. By properties of 
the binomial distribution, as n  ~ 00, 

This completes the proof.  0 

HISTORICAL REMARK.  In an early paper by Habbema, Hermans, and van den Broek 
(1974), the deleted estimate is used to select an appropriate subspace for the kernel 
rule. The kernel rules in tum have smoothing factors that are selected by maximum 
likelihood.  0 

25.8  Sieve Methods 

Sieve methods pick a best estimate or rule from a limited class of rules. For example, 
our sieve Ck  might consist of rules of the form 

where the ai 's are real numbers, the Xi 's are points from n d , and the hi'S are positive 
numbers. There are formally (d + 2)k free scalar parameters. If we pick a rule ¢~ 
that minimizes the empirical error on (Xl, YI ), ... ,(Xn ,  Yn ), we are governed by 
the theorems  of Chapters  12  and  18,  and we will  need to  find  the vc dimension 
of Ck . For this, conditions on K  will be needed. We will return to this question in 
Chapter 30 on neural networks, as they are closely related to the sieves described 
here. 

25.9 Squared ElTor Minimization 

445 

25.9  Squared Error Minimization 

Many researchers have considered the problem of selecting h in order to minimize 
the  L2  error (integrated squared error) of the kernel estimate 

---
1]11,h  = 

1.  ~~l  Y. K  (Xi-X) 
n  L..l=l 
1.  ~~  K  (Xi-X) 
n  L..l=l 

h 

1 

h 

of the regression function 1] (x ) = p {Y = 11 X = x}, 

For example, HardIe and Marron (1985) proposed and studied a cross-validation 
method for choosing the optimal h for the kernel regression estimate. They obtain 
asymptotic optimality for the integrated squared error. Although their method gives 
us a choice for h if we consider 1](x) = P{Y = llX = x} as the regression function, 
it is not clear that the h thus obtained is optimal for the probability of error. In fact, as 
the following theorem illustrates, for some distributions, the smoothing parameter 
that minimizes the L2 error yields a rather poor error probability compared to that 
corresponding to the optimal h. 

Theorem 25.9.  Let d  =  l. Consider the kernel classification rule with the window 
kernel  K  =  /[-1,1],  and smoothing parameter h  >  O.  Denote its error probability 
by  Ln(h).  Let h*  be the smoothing parameter that minimizes the mean integrated 
squared error 

Then for some distributions 

E{Ln(h*)} 

. 
hm 
n-+oo  infh E{Ln(h)} 

= 00, 

and the convergence is  exponentially fast. 

We  leave the details  of the proof to  the reader (Problem 25.20).  Only a rough 

sketch is given here. Consider X  uniform on [0,  1]  U [3,4], and define 

if x  E  [0,  1] 
1] (x ) =  2 - x /2  otherwise. 

I  - x /2 

{

The optimal value of h  (i.e., the value minimizing the error probability) is one. It 
is constant, independent of n. This shows that we should not a priori exclude any 
values of h, as is commonly done in studies on regression and density estimation. 
The  minimal  error probability  can  be  bounded  from  above  (using  Hoeffding's 
inequality) by e- C1n  for some constant Cl  >  0.  On the other hand, straightforward 
calculations show that the smoothing factor h*  that minimizes the mean integrated 

446 

25. , Automatic Kernel Rules 

squared  error goes  to  zero  as  n  -+  (Xl  as  0  (n- I
/ 4 ).  The  corresponding  error 
probability  is  larger than  e-c2h*n  for  some  constant  C2.  The  order-of-magnitude 
difference between the exponents explains the exponential' speed of convergence 
to infinity. 

Problems and Exercises 

PROBLEM  25.1.  Prove Theorem 25.2. 

PROBLEM  25.2.  Show that for any r  >  0, the kernel 

K(x) = { ~/IIXI( 

if Ilxll s  1 
otherwise, 

satisfies the conditions of Theorem 25.3. 

PROBLEM 25.3.  Assume that K is a regular kernel with kernel complexity Km  S  mY for some 
constant y  >  0.  Let gn  be selected so as  to minimize the holdout error estimate L1,m(¢>m) 
over Cm ,  the class of kernel rules based upon the first m  data points, with smoothing factor 
h  >  0.  Assume furthermore that we vary lover [log2 n, n12],  and that we pick the best l 
(and m  = n  - 1)  by minimizing the holdout error estimate again.  Show that the obtained 
rule is  strongly universally consistent. 

PROBLEM  25.4.  Prove  that  the kernel  complexity  Km  of the  de  la Vallee-Poussin  kernel 
K(x) = (sinxlxf, x  E  R, is infinite when m  ::::  2. 

PROBLEM  25.5.  Show that Km  = 00 for m  ::::  2 when K(x) = cos2(x), x  E  R. 

PROBLEM  25.6.  Compute an upper bound for the kernel complexity Km  for the following 
kernels, where x  = (x(1),  ... , xed)~ E  Rd: 

A. 

B. 

c. 

D. 

K(x) = 

d TI ( 

i=l 

I-x 

(i)2) 

IUx(i)I:::l}' 

1 

d 

K(x) = (1  + Ilxl1 2t' ex  >  0. 
K(x) = TI -1  (i)2' 
K(x) = TI cos(x(i)IUx(i)I:::n/2j' 

i=l  +x 

1 

d 

i=l 

PROBLEM 25.7.  Can  you  construct  a  kernel  on  R  with  the  property  that  its  complexity 
satisfies 2m  S  Km  <  00 for all m? Prove your claim. 

PROBLEM 25.8.  Show that for kernel classes Cm  with kernel rules having  a fixed training 
sequence  Dm  but variable h  >  0, we have VCm  S 10g2 Km· 

Problems and Exercises 

447 

PROBLEM  25.9.  Calculate upper bounds for S(em, /) when em  is  the class of kernel rules 
based  on  fixed  training  data  but  with  variable  parameter e in  the  following  cases  (for 
definitions, see Section 25.4): 

(1)  Kg  is  a  product  kernel  of R d

,  where  the  unidimensional  kernel  K  has  kernel 

complexity Km. 

(2)  Kg  =  l+lIxl:a/ha , where e = (h, a), h  >  0, a  > ° are two parameters. 
(3)  Kg(x) = l{xEA}, where A is any ellipsoid ofRd  centered at the origin. 

PROBLEM 25.lO.  Prove  or disprove:  if Dm  is  fixed  and em  is  the class  of all  kernel rules 
based on Dm  with K  = lA, A being any convex set ofRd  containing the origin, is it possible 
that VCm  = 00, or is VCm  <  00 for all possible configurations of Dm? 

PROBLEM 25.11.  Let 

As ~ { {x . t K  (x ~ X, )  (2Y,  -

I) > o} , h  > o} 

with Xl  =  -38, X2  = X3  = X 4  =  -8, Xs  =  X6  =  X7  = 8,  X8  = 38,  YI  =  1,  Y2  =  Y3  =  Y4 = 
-1, Ys  =  Y6  =  Y7  =  1,  Y8  =  -1, and let  K  be piecewise cubic.  Extending the quadratic 
example in the text,  show that the vc dimension of A8 is infinite for some K  in this class 
that is  symmetric, unimodal, positive, and convex on [0,  (0). 

PROBLEM 25.12.  Draw (Xl, Yd, ... , (X4' Y4)from the distribution of (X, Y) on Rd x{O,  I}, 
where X  has a density and 1J(x)  E  (0,  1) at all x. Find a symmetric unimodal K  such that 

As ~ { { x  • t K  (  x ~ X, )  (2Y,  -

I) >  0 } , h  >  0 } 

has vc dimension satisfying P{ V ~ = oo}  >  0. Can you find such a kernel K  with a bounded 
support? 
PROBLEM 25.13.  Let X have a density f  on Rd and let Xl, ... , Xn  bei.i.d., drawn from f. 
Show that 

lim  liminfP {min IIXi  - Xjlld:s  c2
n 

C--700  n--7OO 

11] 

}  = 1. 

Apply this result in the following situation:  define Hn  = mini,j:Y;=l,Yj'=O IIXi  - Xi II,  where 
(Xl, Yd, ... , (Xn,  Yn) are i.i.d. Rd  x  {O,  1}-valued random variables distributed as (X,  Y), 
with X absolutely continuous, Y independent of X, and P{Y = I}  E  (0, 1). Show that 

lim lim inf P {H::S  C
n 
2

}  = 1. 

Conclude that nHd  ---+  ° in probability. If you have a kernel rule with kernel  SO,1  on R d
and if the smoothing factor Hn  is random but satisfies nH~ ---+ ° in probability, then 

C--700  n--7OO 

, 

lim  E{L n }  = pry = 1} 

n--7OO 

whenever X  has a density. Show this. 

PROBLEM  25.14.  Consider the variable kernel rule based upon the variable kernel density 
estimate of Breiman, Meisel, and Purcell (1977) and studied by Krzyzak (1983) 

gn(x) = {I 

° otherwIse. 

if Li:Y;.=1  KH;(X  - Xi)  >  Li:Y;=O KH;(x  - Xi) 

448 

25.  Automatic Kernel Rules 

Here K  is  a positive-valued kernel,  Ku  = (l/ud)K(x/u) for u  >  0, and Hi  is the distance 
between Xi  and the k-th nearest neighbor of Xi  among  Xj,  j  ii, 1 ::::  j  ::::  n.  Investigate 
the consistency of this rule when k / n  --+  0 and k  --+  00 and X 1 has a density. 
PROBLEM 25.15.  CONTINUATION. Fix k  = 1, and let K be the normal density in nd. If Xl has 
a density, what can you say about the asymptotic probability of error of the variable kernel 
rule? Is  the inequality of Cover and  Hart still valid? Repeat the  exercise for  the  uniform 
kernel on the unit ball of nd. 

PROBLEM 25.16.  If X 1,  ... ,  Xl)  are discrete i.i.d. random variables and N denotes the num(cid:173)
ber of different values taken by X I, ... ,  X n , then 

.  E{N} 
hm  - - =0, 

17--+00 

n 

and N / n  --+  0 with probability one. HINT: For the weak convergence, assume without loss of 
generality that the probabilities are monotone. For the strong convergence, use McDiarmid's 
inequality. 
PROBLEM  25.17.  Let B be the class of all Borel subsets of nd. Using the previous exercise, 
show that for any discrete distribution, 

sup IJLn(A)  - JL(A)I  --+  0  with probability one. 
AEB 

HINT:  Recall the necessary and sufficient condition E {log N A (X 1,  ... ,  Xn)} / n  --+  0 from 
Chapter 12. 

PROBLEM  25.18.  Prove Theorem 25.8  allowing kernel functions  taking finitely many dif(cid:173)
ferent values. 

PROBLEM 25.19.  Let X I, ... ,  Xn  be an Li.d.  sample drawn from the distribution of X. Let 
XfN denote the nearest neighbor of Xj  among Xl, ... , X j - 1,  X j +1,  ••• ,  X n .  Define 

Bn  = max IIX} - XfNII· 

J 

. 

(1)  Show that for all nonatomic distributions of X, nB~ --+  00 with probability one. 
Is  it true that for every X  with a density,  there exists a constant c  >  0  such that 
(2) 
with probability one, nB,~ :::  clog n for all n large enough? 

(3)  Exhibit distributions on the real line for which Bn  --+  00 with probability one. 

HINT:  Look at the difference between the first and second order statistics. 

PROBLEM  25.20.  Prove  Theorem  25.9.  HINT:  Use  the  example  given  in the  text.  Get an 
upper bound for  the error probability corresponding to  h  =  1 by Hoeffding's inequality. 
The mean integrated squared error can be computed for every h  in a straightforward way 
by observing that 

Split the integral between 0 and 1 in three parts, 0 to h, h to 1 - h, and 1 - h to 1. Setting the 
derivative of the obtained expression with respect to h  equal to zero leads to a third-order 

Problems and Exercises 

449 

4

/

). To get a lower bound for the corresponding error 

equation in h, whose roots are  O(n- I
probability, use the crude bound 

III 

I-h 

E{Ln(h)}  2:  -
2 

P{g/1(X) i  YIX = x }dx. 

Now, estimate the tail of a binomial distribution from below; and use Stirling's formula to 
show that, modulo polynomial factors,  the error probability is larger than 2-/13/4. 

26 
Automatic Nearest Neighbor Rules 

The error probability of the k-nearest neighbor rule converges to  the Bayes risk 
for  all  distributions when k  ----'7  00,  and kin  ---+  0 as n  ----'7  00.  The convergence 
result is extended here to include data-dependent choices of k. We also look at the 
data-based selection of a metric and of weights in weighted nearest neighbor rules. 
To keep the notation consistent with that of earlier chapters, random (data-based) 
values of k  are denoted by Kn.  In most instances,  Kn  is merely a function of Dn, 
the data sequence (Xl, Yd, ... , (Xn, Yn).  The reader should not confuse Kn  with 
the kernel K  in other chapters. 

26.1  Consistency 

We  start  with  a  general  theorem  assessing  strong  consistency  of the  k-nearest 
neighbor rule  with  data-dependent choices  of k.  For the  sake  of simplicity,  we 
assume the existence of the density of X. The general case can be taken care of by 
introducing an appropriate tie-breaking method as in Chapter 11. 

Theorem 26.1.  Let K 1,  K 2,  ... be integer valued random variables, and let gn  be 
the  Kn-nearest neighbor rule. If X  has a density,  and 

Kn  ----'7  00  and  Kn/n  ----'7  0  with probability one as  n  ----'7  00, 

then L(gn)  ----'7  L * with probability one. 

452 

26.  Automatic Nearest Neighbor Rules 

PROOF. L(g,J ~ L * with probability one if and only if for every E  >  O,I{L(gn)-U>E) 
~ 0 with probability one. Clearly, for any f3  >  0, 

We are done if both random variables on the right-hand side converge to zero with 
probability one. The convergence of the second term for all f3  >  0 follows trivially 
from the conditions of the theorem. The convergence of the first term follows from 
the remark following Theorem  11.1, which states that for any  E  >  0,  there exist 
f3  >  0  and no  such that for  the  error probability  Ln,k  of the k-nearest neighbor 
rule, 

P{L 

n,k 

- L * >  E}  <  4e-CnE2 

_ 

for  some  constant  c  depending  on  the  dimension  only,  provided  that  n  >  no, 
k  >  I/f3  and kin  <  f3.  Now clearly, 

P{L(gn) - L *  >  E,  II Kn  + Knln  <  2f3}  <  P{  sup  Ln,k  - L * >  E} 

l/f3~k~nf3 

<  n 

sup  P{Ln,k  - L*  >  E}, 

l/f3~k~nf3 

by the union bound. Combining this with Theorem 11.1, we get 

P{L(gn) - L * >  E,  II Kn  + Knln  <  2f3}  ::::  4ne-CnE2 ,  n  ~ no. 

The Borel-Cantelli lemma implies that 

with probability one, and the theorem is proved.  0 

Sometimes we only know that Kn  ~ 00 and Knln  ~ 0 in probability. In such 
cases weak consistency is  guaranteed.  The proof is  left as  an  exercise  (Problem 
26.1). 

Theorem 26.2.  Let KI, K2,  ... be integer valued random variables, and let gn  be 
the  Kn -nearest neighbor rule.  If X  has a density,  and 

Kn  ~ 00  and  Knln  ~ 0 

in probability as  n  ~ 00, 

then limn--+ oo EL(gn) = L *,  that is,  gn  is weakly consistent. 

26.2  Data Splitting 

Consistency by itself may be obtained by choosing k =  L Fn J, but few-if any(cid:173)
users  will  want  to  blindly  use  such  recipes.  Instead,  a  healthy  dose  of feed(cid:173)
back from  the  data is preferable.  If we proceed as  in  Chapter 22,  we  may  split 

26.3 Data Splitting for Weighted NN Rules 

453 

the  data  sequence  Dn  =  (Xl, YI ), ... ,  (Xn, Yn) into  a  training  sequence  Dm  = 
(Xl, Yd, ... , (Xm, Ym),  and a testing sequence Tz  =  (Xm+l , Ym+l ), ... ,  (Xn, Yn), 
where m + I = n. The training sequence Dm  is used to design a class of classifiers 
em.  The testing sequence is used to select a classifier from em  that minimizes the 
holdout estimate of the error probability, 

If em  contains  all  k-nearest  neighbor  rules  with  I  ~ k  ~ m,  then  ,em'  =  m. 
Therefore, we have 

where  gn  is  the  selected k-nearest neighbor rule.  By combining  Theorems  11.1 
and 22.1, we immediately deduce that gn  is universally consistent if 

lim  m  = 00, 
n---+oo 

I 

. 
1
1m  - - =00. 
n---+oo  log m 

It is strongly universally consistent if limn---+oo 1/ log n = 00 also. Note too that 

E {IL(gn) -

inf  L(¢m)11  ~ 2 
<p",ECm 

log(2m) + 1 

21 

(by Problem 12.1), so that it is indeed important to pick I much larger than log m. 

26.3  Data Splitting for Weighted NN Rules 

Royall (1966) introduced the weighted NN rule in which the i-th nearest neighbor 
receives weight Wi, where WI  ~ W2  ~ .•.  ~ Wk  ~ 0 and the Wi'S sum to one. We 
assume that  Wk+l  =  ... =  Wn  =  0 if there are n  data points.  Besides the natural 
appeal of attaching more weight to nearer neighbors, there is  also a practical by(cid:173)
product:  if the  Wi'S  are all of the form  1/ Zi,  where  Zi  is  a prime integer, then no 
two  subsums of Wi'S are equal, and therefore voting ties are avoided altogether. 
Consider now  data splitting  in which em  consists  of all  weighted  k-NN  rules 
as  described above-clearly, k  ~ m  now.  As  lem I =  00, we compute the shatter 
coefficients seem, I). We claim that 

if I  :::  k 
if I  <  k. 

(26.1) 

This result is true even if we do not insist that WI  ~ W2  ~ ... ~ Wk  ~ o. 
PROOF  OF  (26.1).  Each  Xj  in the  testing  sequence is  classified based upon  the 
sign of L~=l aij Wi, where aij  E  {-I, I} depends upon the class of the i -th nearest 

454 

26. , Automatic Nearest Neighbor Rules 

neighbor of Xj among Xl, ... , Xm  (and does not depend upon the Wi'S). Consider 
the I-vector of signs of 2:=:=1  aijWi,  m  <  j  S  n. In the computation of Seem, 1), 
we consider the aij'S as fixed numbers, and vary the Wi'S  subject to the condition 
laid out above. Here is the crucial step in the argument: the collection of all vectors 
(WI, ... , Wk)  for  which  Xj  is  assigned  to  class  1 is  a  linear  halfspace  of Rk. 
Therefore, Seem, l) is bounded from above by the number of cells in the partition 
of Rk defined by llinear halfspaces.  This  is  bounded by 2:=:=1  G)  (see Problem 
22.1) if k  s l.  0 

Let gn  be the rule in  lem I that minimizes the empirical error committed on the 
test sequence (Xm+1,  Ym+1),  ... , (Xn, Yn).  Then by (22.1), if [  =  n  - m  2:  k, we 
have 

---+ 

(X)  (which  implies  m 

---+  (0),  we  have  universal  consistency  when 
If  k 
k log(l)/ l  ---+  O.  The estimation error is of the order of Jk log 1/ [-in the termi(cid:173)
nology of Chapter 22,  the rule is J k log l/ I-optimal. This error must be weighed 
against the unknown approximation error.  Let us present a quick heuristic argu(cid:173)
ment. On the real line, there is compelling evidence to suggest that when X has a 
smooth density, k = cm4 / 5 is nearly optimal. With this choice, if both 1 and m grow 
linearly in n, the estimation error is of the order of Jlog n/ n 1/1O.  This is painfully 
large-to reduce this error by a factor of two, sample sizes must rise by a factor of 
about 1000. The reason for this disappointing result is that em  is just too rich for 
the values of k that interest us.  Automatic selection may lead to rules that overfit 
the data. 

If we restrict em  by making m  very small, the following rough argument may 
be used to  glean information  about the  size  of m  and I  =  n  - m.  We  will take 
k  = m  «  n.  For  smooth  regression  function  1],  the  estimation  error  may  be 
anywhere between m -2/5  and m -4/5  on the real line.  As  the  estimation error is 
of the  order  of Jmlogn/n,  equating  the  errors  leads  to  the  rough  recipe  that 
m  ~ (n/logn)5/9,  and  m  ~ (n/logn)5/13,  respectively.  Both  errors  are  then 
about (n/logn)-2/9  and  (n/logn)-4/9, respectively.  This is better than  with the 
previous example with m  linear in n.  Unfortunately, it is  difficult to test whether 
the conditions on 1]  that guarantee certain errors are satisfied. The above procedure 
is thus doomed to remain heuristic. 

26.4  Reference Data and Data Splitting 

Split the data into  Dm  and Tz  as is  done in  the previous  section.  Let em  contain 
allI-NN rules that are based upon the data (Xl, YI),  ... , (Xb Yk),  where k  S  m is 
to be picked,  {Xl,  ... , Xk}  C  {Xl,'"  Xm},  and {YI,  ... , Yk}  E  {O,  I}k.  Note that 
because the  y/s are free  parameters,  {(Xl, YI),  ... , (Xb Yk)}  is  not necessarily a 
subset of {(X I, YI ),  ... ,  (Xm,  Ym)}-this allows us to flip certain y-values at some 

data points. Trivially, lem I = G)2k. Hence, 

26.5 Variable Metric NN Rules 

455 

where I = n - m, and gn  E em  minimizes the empirical error on the test sequence 
Tz.  The best rule in em is universally consistent when k -+  00 (see Theorem 19.4). 
Therefore,  gn  is universally consistent when the above bound converges to  zero. 
Sufficient conditions are 

11-+00 

lim  I = 00; 
klogm 
lim  - - = O. 

n-+oo 

I 

(i) 

(ii) 

As the estimation error is  0  ( J k log m / I), it is important to make I large, while 
keeping k small. 

The  selected  sequence  (Xl, YI),  ... , (Xb Yk)  may  be  called reference  data,  as 
it  captures  the  information  in  the  larger  data  set.  If k  is  sufficiently  small,  the 
computation  of gn (x)  is  extremely fast.  The  idea  of selecting  reference  data or 
throwing out useless or "bad" data points has been proposed and studied by many 
researchers under names such as condensed NN rules, edited NN rules, and selective 
NN rules. See Hart (1968), Gates (1972), Wilson (1972), Wagner (1973), Ullmann 
(1974), Ritter et al.  (1975), Tomek (1976b), and Devijver and Kittler (1980).  See 
also Section 19.1. 

26.5  Variable Metric NN Rules 

The  data may also be used to  select a suitable metric for use with the  k-NN rule. 
The metric adapts itself for certain scale information gleaned from  the data. For 
example, we may compute the distance between Xl  and X2  by the formula 

"AT (Xl  -

x2)11 

((Xl  - x2l AAT (Xl  - X2) )1/2 

= 

((Xl  - X2)T L:(XI  - X2») 1/2 , 

where  (Xl  - X2)  is  a  column  vector,  (·l denotes  its  transpose,  A  is  a  d  x  d 
transformation matrix,  and  L:  =  AA T  is  a positive definite matrix.  The elements 
of A or L:  may be determined from the data according to some heuristic formulas. 
We refer to Fukunaga and Hostetler (1973), Short and Fukunaga (1981), Fukunaga 
and Flick (1984), and Myles and Hand (1990) for more information. 

For example, the object of principal component analysis is to find a transforma(cid:173)

tion matrix A such that the components of the vector A T X have unit variance and 
are uncorrelated. These methods are typically based on estimating the eigenvalues 
of the covariance matrix of X. For such situations, we prove the following general 
consistency result: 

456 

26.  Automatic Nearest Neighbor Rules 

Theorem 26.3.  Let the  random metric Pn  be of the form 

Pn(X,  y) = IIA~ (x  - y)lI, 

where the matrix An  is a function of Xl, ... , X n .  Assume that distance ties occur 
with zero probability, and there are two sequences of nonnegative random variables 
{mn} and {Mn}  such thatfor any n and x, y  End, 

and 

If 

P  {liminf mn  = o}  = O. 

n-+oo  Mn 

lim  kn  = 00  and 
n-+oo 

kn 
. 
hm  - =0, 
n-+oo  n 

then the kn -nearest neighbor rule based on the metric Pn  is consistent. 

PROOF. We verify the three conditions of Theorem 6.3. In this case, WnJX) = 1/ kn 
if Xi  is one of the kn nearest neighbors of X (according to Pn), and zero otherwise. 
Condition (iii) holds trivially. Just as in the proof of consistency of the ordinary kn-
nearest neighbor rule, for condition (i) we need the property that the number of data 
points that can be among the kn  nearest neighbors of a particular point is at most 
kn Yd,  where the constant Yd  depends on the dimension only. This is a deterministic 
property,  and it can be proven exactly  the  same way  as  for  the  standard nearest 
neighbor rule. The only condition of Theorem 6.3  whose justification needs extra 
work is condition (ii):  we need to show that for any a  >  0, 

lim  E {~Wni(X)!{IIX-Xill>a}} = o. 

n-+oo  ~ 

i=l 

Denote the k-th nearest neighbor of X according to Pn  by X(k), and the k-th nearest 
neighbor of X according to the Euclidean metric by X(k).  Then 

E {t Wni(X)I{IIX-Xdl>aJ } 

(since mn IIx - yll  :::  Pn(x,  y)) 

26.6 Selection of k  Based on the Deleted Estimate 

457 

<  P {mna  :::;  Pn(X,  X(kn ) }  

P {t I{Pn(X,X;):::'mna}  <  kn} 

l=1 

(since Mnllx  - Y II  ::::  Pn(x, y» 

P{IIX-X(dl >a::}. 

But we know from the consistency proof of the ordinary kl1 -nearest neighbor rule 
in Chapter 11  that for each a  >  0, 

lim  P {IIX - X(kn ) II  >  a}  = O. 

l1--HX) 

It follows  from the condition on mn/ Mn  that the probability above converges to 
zero as well.  0 

The conditions of the theorem hold if, for example, the elements of the matrix 
An  converge to the elements of an invertible matrix A in probability. In that case, 
we may take mn  as the smallest, and Ml1  as the largest eigenvalues of A~ An. Then 
mn/ Ml1  converges to the ratio of the smallest and largest eigenvalues of AT A, a 
positive number. 

If we pick the elements of A by minimizing the empirical error of a test sequence 
Tz  over  c'n,  where  Cm contains  all  k-NN  rules  based  upon  a  training  sequence 
Dm  (thus,  the  elements  of A  are  the  free  parameters),  the  value  of S(Cm, l)  is 
too large to be useful-see Problem 26.3. Furthermore, such minimization is not 
computationally feasible. 

26.6  Selection of k  Based on the Deleted Estimate 

If you wish to use all the available data in the training sequence, without splitting, 
then  empirical  selection  based  on  minimization  of other  estimates  of the  error 
probability may be your solution. Unfortunately, performance guarantees for the 
selected rule are rarely available. If the class of rules is finite, as when k is selected 
from {I, ... , n}, there are useful inequalities.  We will show you how this works. 

458 

26.  Automatic Nearest Neighbor Rules 

Let  en  be  the  class  of all  k-nearest  neighbor rules  based  on  a  fixed  training 
sequence  Dn,  but with  k  variable.  Clearly,  I en I =  n.  Assume  that  the  deleted 
estimate is used to pick a classifier gn  from en: 

We  can  derive  performance bounds for  gn  from  Theorem 24.5.  Since the  result 
gives poor bounds for large values of k,  the range of k's has to be restricted-see 
the discussion following  Theorem 24.5.  Let ko  denote  the  value of the  largest k 
allowed, that is, en  now contains all k-nearest neighbor rules with k ranging from 
1 to ko. 

Theorem 26.4.  Let gn  be the classijierminimizing the deleted estimate of the error 
probability over en,  the class of k-nearest neighbor rules with 1 ::::  k  ::::  ko.  Then 

where  c  is  a  constant  depending  on  the  dimension  only.  If ko 
ko log nj n  ---7>- 0,  then gn  is strongly universally consistent. 

---7>- 00  and 

PROOF.  From Theorem 8.4 we recall 

The inequality now  follows  from  Theorem 24.5  via the union bound.  Universal 
consistency follows from the previous inequality,  and the fact that the kO-NN rule 
is strongly universally consistent (see Theorem 11.1).  0 

Problems and Exercises 

PROBLEM 26.1.  Let K I,  K 2,  ... be integer valued random variables, and let gn  be the  Kn(cid:173)
nearest  neighbor rule.  Show  that  if X  has  a  density,  and  Kn  -+  00  and  Kill n  -+  0 in 
probability as n  -+  00, then E{L(gn)}  -+  L*. 

PROBLEM 26.2.  Let C be the class of all  l-NN rules based upon pairs (XI, YI), ... , (Xb Yk), 
where k is a fixed parameter (possibly varying with n), and the (Xi,  Yi)'S are variable pairs 
from R d  X {O,  l}. Let gn be the rule that minimizes the empirical error over C, or, equivalently, 
let gn  be the rule that minimizes the resubstitution estimate L~R) over C. 

(1)  Compute a suitable upper bound for S(C, n). 
(2)  Compute a good upper bound for  Vc  as a function of k and d. 
(3) 

If k  -+  00 with n,  show that the  sequence of classes C contains a strongly um., 
versally consistent subsequence of rules (you may assume for convenience that X: 
has a density to avoid distance ties). 

(4)  Under what condition on k can you guarantee strong universal consistency of gn! 

(5)  Give an upper bound for 

Problems and Exercises 

459 

PROBLEM  26.3.  Let Cm contain all k-NN rules based upon data pairs (Xl, Yl ),  ..• ,  (Xm'  Ym). 
The metric used in computing the neighbors is derived from the norm 

d 

/Ix II ~ = L L (Jij X (i) xU),  x  = (x O),  ... , xed»), 

d 

i=l 

j=l 

where  {(Jij} forms  a positive definite matrix  2:.  The elements of 2:  are the free parameters 
in Cm.  Compute upper and lower bounds for Seem, I) as a function of m, I, k, and d. 

PROBLEM 26.4.  Let gn  be the  rule  obtained by minimizing  L~D) over all  k-NN rules  with 
1 ::::  k  ::::  n.  Prove or disprove:  gn  is  strongly universally consistent.  Note that in view of 
Theorem 26.4, it suffices to consider En/ log n  ::::  k  ::::  n - 1 for all E  >  O. 

27 
Hypercubes and Discrete Spaces 

In many situations, the pair (X, Y) is purely binary, taking values in {a,  I}d x {a,  I}. 
Examples include boolean settings (each component of X represents "on" or "off"), 
representations of continuous variables through quantization (continuous variables 
are always represented by bit strings in computers), and ordinal data (a component 
of X is  I if and only if a certain item is present). The components of X are denoted 
by  X (1) ,  .•. ,  X(d).  In  this  chapter,  we  review  pattern recognition  briefly  in  this 
setup. 

Without  any  particular  structure  in  the  distribution  of (X, Y)  or the  function 
1J(x) = P{Y = IIX = x}, x  E  {a,  I}d, the pattern recognition problem might as well 
be cast in the space of the first 2d positive integers: (X, Y)  E  {I, ... , 2d}  x  {a,  I}. 
This  is  dealt with in  the first  section.  However,  things  become more interesting 
under certain structural assumptions, such as the assumption that the components 
of X be independent. This is dealt with in the third section. General discrimination 
rules on hypercubes are treated in the rest of the chapter. 

27.1  Multinomial Discrimination 

At first sight, discrimination on a finite set {I, ... ,k}-called multinomial discri(cid:173)
mination-may seem utterly trivial. Let us call the following rule the fundamental 
rule,  as  it captures  what most of us  would  do in  the  absence  of any  additional 
information. 

462 

27., Hypercubes and Discrete Spaces 

The fundamental rule coincides with the standard kernel and histogram rules if the 
smoothing factor or bin width are taken small enough. If Pi  = P{X = i} and YJ  is 
as usual, then it takes just a second to see that 

YJ(x)px 

and 

ELn  ;:::  L  YJ(x)pxCl  - px)n, 

x 

where Ln  = L(g~). Assume YJ(x)  = 1 at all x. Then L * = 0 and 

ELn  ;:::  L  Px(l  - px)n. 

x 

If Px  = II k  for  all  x,  we  have  ELn  ;:::  (l -
;:::  1/2  if k  ;:::  2n.  This 
simple calculation shows  that we cannot say anything useful about fundamental 
rules unless k  ::s  2n at the very least. On the positive side, the following universal 
bound is useful. 

II k)n 

Theorem 27.1.  For the fundamental rule,  we have Ln  ---+  L * with probability one 
as n  ---+  00,  and,  in fact, for all distributions, 

ELn  <  L* + 

-

~ k 

2(n + 1) 

+-
en 

and 

ELn  :0:  L * + 1.07S!';;. 

PROOF.  The first statement follows trivially from the strong universal consistency 
of histogram rules (see Theorem 9.4). It is the universal inequality that is of interest 
here. If B(n, p) denotes a binomial random variable with parameters nand p, then 
we have 

k 

ELn  =  LPx( YJ(x)  + (l - 2YJ(x»P{B(N(x), YJ(x»  >  N(X)/2}) 

x=l 
(here N(x) is a binomial (n, p(x»  random variable) 

k 

=  LPx(I-YJ(x)+(2YJ(x)-I)P{B(N(x),YJ(x»::s N(x)/2}). 

x::: 1 

From this, if sex) = min(17(x),  1 - 17(X)), 

27.1  Multinomial Discrimination 

463 

N(x)s(x)(1 - sex)) 

N(x)s(x)(l - 2s(x)) + G - sex))  N(x)2 

2 

} 

k 

x=l 

x=l 

(since s(x)(1 - sex)) :s  1/4) 

(by the Chebyshev-Cantelli inequality-Theorem A.17) 

<  L Px (s(X) + (1  - 2s(x))P{B(N(x), sex))  ~ N(x)/2}) 
<  L * + t Px(1  - 2s(x))E { 
:0:  L' + t pAl - 2~(x))E L + (1  - 2~(X))2 N(x) } 
:s  L * + t PxE  {  ~I{N(x»O) + (1  - ~(X))I{N(x)",,)} 
:0:  L' + t Px(1  - px)n + ~ t Px  E {N;x/{N(X)=O)} 
:s  L * + (1 -~)n + ~ t P  {2  

(since the function u / (1 + N u2

(by Jensen's inequality) 

2  N(x) 

k 

2x=1  xV~ 

x=l 

)  :s  1/ (2.jN) for 0 :s  u  :s  1) 

(by Lemma A.2 and the fact that the worst distribution 

has  Px  =  1/ k for all x) 

:s  L* +e-n1k + 

1 

k 

LJP; 

.j2(n + 1)  x=l 

:s  L * + ~ +  fk (since e-u  :s  1/(eu) for u ~ 0, 

en  V 2(;+1) 

and by the Cauchy-Schwarz inequality). 

This concludes the proof, as .jIJ2 + 1/ e :s  1.075.  0 

For several key properties of the fundamental rule, the reader is referred to Glick 

(1973)  and to Problem 27.1.  Other references include Krzanowski (1987), Gold(cid:173)
stein (1977), and Goldstein and Dillon (1978). Note also the following extension 
of Theorem 27.1,  which shows that the  fundamental  rule can handle all  discrete 
distributions. 

464 

27.  Hypercubes and Discrete Spaces 

Theorem 27.2.  If X  is purely atomic (with possibly infinitely many atoms),  then 
Ln  ~ L * with probability one for the fundamental rule. 

PROOF. Number the atoms 1,2, .... Define XI = min(X, k) and replace (Xi,  yt) by 
(X;, Yi ), where X;  = min(Xi, k). Apply the fundamental rule to the new problem 
and note that by Theorem 27.1,  if k  is  fixed,  Ln  ~ L * with probability one for 
the new rule (and new  distribution).  However,  the difference in  Ln's and in L *'s 
between the truncated and nontruncated versions cannot be more than P {X  ~ k}, 
which may be made as small as desired by choice of k.  0 

27.2  Quantization 

Consider a fixed partition of space nd into k sets {AI, ... , Ad, and let gn  be the 
standard partitioning rule based upon majority votes  in the  Ai'S (ties are broken 
by favoring the response "0," as elsewhere in the book). We consider two rules: 

(1)  The  rule  gn  considered  above;  the  data  are  (Xl, YI),  ... , (Xn,  Yn),  with 
(X, Y)  E  nd  x  to,  I}.  The probability of error is  denoted by  L n ,  and the 
Bayes  probability  of error is  L*  = E{min(1J(X),  1 - 1J(X))}  with  1J(x)  = 
P{Y =  llX =x},x End. 

(2)  The  fundamental  rule  g:1  operating  on  the  quantized  data  (X~, Y1),  ••• , 
(X;!' Yn ), with (XI, Y)  E  {l, ... , k}  x  to,  I},  X;  = j  if X  E  A j . The Bayes 
probability of error is L'* = E{min(1J'(X'),  1 -1J'(X'))} with 1J'(x') = P{Y = 
11 XI = Xl}, Xl  E  {I, ... , k}.  The probability of error is denoted by L;1' 

Clearly, g~ is nothing but the fundamental rule for the quantized data. As gn(x) = 
g:1 (Xl), where Xl  = j  if x  E  A j' we see that 

However, the Bayes error probabilities are different in the two situations. We claim, 
however, the following: 

Theorem 27.3.  For the standard partitioning rule, 

E{Ln }  :s  L* + l.075~ + 8, 

where 0 = E {11J(X)  - 1J'(X')I}.  Furthermore, 

L * :.:::  L'* :.:::  L * + O. 

27.2 Quantization 

465 

PROOF.  Clearly, L * ::s  L'* (see Problem 2.1). Also, 

L'*  =  E{min(n'(X'), 1 - n'(X'))} 

<  E{min(1](X) + 11](X) - 1]' (X')I ' 1 - 1](X) + 11](X) - 1]' (X')I)} 
<  8 + E{min(n(X), 1 - n(X))} 

=  8+L*. 

Furthermore, 

E(Ln)  S  L" + 1.075/f; s  L* + S + 1.075/f; 

by Theorem 27.1.0 

As  an  immediate corollary,  we  show  how  to  use  the last bound to  get useful 

distribution-free performance bounds. 

Theorem 27.4.  Let F  be a class of distributions of (X, Y)for which X  E  [0,  l]d 
with probability one,  and for some constants c  >  0,  1 ::::  Ci  >  0, 

11](x)  - 1](z) I ::s  cllx - zW\  x, z  E  Rd. 

Then,  ifwe consider all cubic histogram rules gn  (see  Chapter 6 for a definition), 
we have 

inf 

cubic histogram rule gn 

E{L(gn) - L  }:s -

* 

a 
Fn 

b 
+ -01  , 
n d+201 

where  a  and b  are  constants depending upon c, Ci,  and d  only (see  the prooffor 
explicit expressions). 

Theorem 27.4 establishes the existence of rules that perform uniformly at rate 
o (n- a /(d+2a))  over  F.  Results  like  this  have  an  impact on the  number of data 
points required to guarantee a given performance for any (X, Y)  E  F. 
PROOF.  Consider  a  cubic  grid  with  cells  of volume  hd
covering [0,  l]d does not exceed (1/ h + 2)d,  we apply Theorem 27.3 to  obtain 

•  As  the  number of cells 

E(L(gn)) S  L * + 1.075 (2 + * )"/2 + S, 

where 

8 ::s  sup 11](x)  -1]'(x')1  ::s  c  sup  sup  liz - xlla ::s  c (hFd)a 

x 

Ai  x,ZEAi 

The right-hand side is approximately maximal when 

1.075d 

) 

h =  2" (c.Jd)" In 

( 

def  C' 
-1 - '  
n d+201 

Resubstitution yields the result.  0 

466 

27.  Hypercubes and Discrete Spaces 

27.3 

Independent Components 

Let  X  =  (x(l), ... , XCd))  have  components  that  are  conditionally  independent, 
given {Y = 1},  and also, given  {Y = OJ.  Introduce the notation 

P{XCi )  =  11Y  =  1}, 
p(i) 
q(i)  =  P{X(i) =  11Y  = O}, 

p  =  P{Y =  1}. 
With x  = (x(l), ... , xed))  E  {O,  l}d, we see that 

1J(x)  = 

P{Y=1,X=x} 

P{X = x} 

= - - - - - - - - - - - - - -
pP{X = xlY =  1} + (1  - p)P{X = xlY = O}' 

pP{X=xIY=1} 

and 

P{X = xlY =  1}  =  IT p(it(i) (1  - p(i))l-x(i) , 

d 

i=l 

P{X = xlY = O}  =  IT q(it(i) (1  - q(i))l-xU

d 

). 

Simple consideration shows that the Bayes rule is given by 

i=l 

1 

if  P n~=l p(i)X(i) (1  - p(i))l-x(i) 

g*(x)  = 

{ 

>  (1  - p) n~=l q(iy(i)(1  - q(i))l-x(i) 

o  otherwise. 

Taking logarithms, it is easy to see that this is equivalent to the following rule: 

*(x) = 
{ 
g 

1 'f 
,\,d 
1 
0  otherwise, 

(Yo  + L....,i=l  (Yi X 

0 

Ci) 

> 

where 

(Yo  = 

log (_P_) + ~ log (1 - P(i)) , 

f:t 

1 - p 

1 - q(i) 

log  - .  

P(i)  1 - q(i)) 
q(i)  1 - p(i) 

(

. 
l  =  1,  ... , d. 

In  other  words,  the  Bayes  classifier is  linear!  This  beautiful  fact  was  noted by 
Minsky (1961), Winder (1963),  and Chow (1965). 

Having identified the Bayes rule as  a linear discrimination rule,  we may apply 
the full force of the Vapnik-Chervonenkis theory. Let gn  be the rule that minimizes 
the empirical  error over the  class  of all  linear discrimination rules.  As  the class 

27.3 Independent Components 

467. 

of linear halfspaces of Rd has vc dimension d + 1 (see Corollary 13.1), we recall 
from Theorem 13.11  that for gn, 

P{L(gn) - L*  >  E}  < 

8nd+1 e-nE2 /128, 

E{L(gn) - L *}  < 

16 

(d+1)logn+4 

2n 

(by Corollary 12.1), 

P{L(gn) - L *  >  E}  < 

16 (JilE )4096(d+1) e-nE2 /2,  nE 2 

::::  64, 

and 

where 

E{L(gn) - L*}:s  yfi' 

c 

c =  16 + -/I013(d + 1) log(l012(d + 1)) 

(Problem 12.10). 

These  bounds  are  useful  (i.e.,  they  tend  to  zero  with  n)  if d  =  o(n/logn).  In 
contrast, without the independence assumption, we have pointed out that no non(cid:173)
trivial guarantee can be given about E{L(gn)}  unless 2d  <  n. The independence 
assumption has led us out of the high-dimensional quagmire. 

One may wish to attempt to estimate the p(i)'s and q(i)'s by p(i) and q(i) and 

to use these in the plug-in rule gn (x) that decides 1 if 

d 

P n putU) (l - p(i))l-x(i)  >  (l -

fi) n q(i)x(i) (l - qU))l-x(i), 

d 

i=l 

i=l 

where  p is  the  standard  sample-based estimate  of p.  The maximum-likelihood 
estimate of p(i) is given by 

while 

q-(i) = 

L~=l I{X(i)=l  Y=O} 

n 

} 

Lj=l l {YFO} 

' J  

, 

with  %  equal to  0  (see Problem 27.6).  Note  that the plug-in rule  too  is  linear, 
with 

n(X) = 
g 

{ 

'f 
1  ao +. L...i=l ai X 

l
,\:,d 
0  otherWIse, 

0 

(i) 

> 

where 

ao  = 

p  )  ~  ( 

log  - - + ~ log 

( 
1 - P 

i=l 

NO! (i) 

NO! (i) + Nll (i) 

Noo(i) + NlO(i)) 

. - - - - -

Noo(i) 

, 

ai  = 

log 

Nll (i)  .  NOO(i)) 
NOl (i)  NlO(i) 

( 

._ 
l  - 1, ... , d, 

468 

27.  Hypercubes and Discrete Spaces 

and 

n 

Noo(i)  = L I{xj)=o,yj=o}' 

j=l 

n 

N lO(i) = L I{Xj)=l,Yj=O}' 

j=l 

n 

NOl(i) = L I{xj)=O,Yj=l} , 

j=l 

n 

Nll(i) = L I{Xji)=l,Yj=l} , 

j=l 

d 

P = L (NOl (i) + Nll (i)) . 

i=l 

For all this, we refer to Warner, Toronto, Veasey, and Stephenson (1961) (see also 
McLachlan (1992)). 
Use the inequality 

E{L(gn) - L *} ::s  2E {117n(X)  - 17(X))}  , 

where 17  is  as  given in the text,  and 17n  is  as  17  but with p, p(i), q(i) replaced by 
p, p(i), q(i), to establish consistency: 

Theorem 27.5.  For  the  plug-in  rule,  L(gn)  -+  L *  with  probability  one  and 
E{L(gn)} - L * -+ 0,  whenever the components are independent. 

We refer to the Problem section for an evaluation of an upper bound for E{ L(gn)(cid:173)

L *} (see Problem 27.7). 

Linear discrimination on the hypercube has of course limited value. The world 
is  full  of examples  that  are  not  linearly  separable.  For  example,  on  {O,  1}2,  if 
Y  = XCl)(1  - X(2)) + X(2)(1  - XO))  (so that Y implements the boolean "xor" or 
"exclusive or" function), the problem is not linearly separable if all four possible 
values of X have positive probability. However, the exclusive or function may be 
dealt with very nicely if one considers quadratic discriminants dealt with in a later 
section (27.5) on series methods. 

27.4  Boolean Classifiers 

By a boolean classification problem,  we mean a pattern recognition problem on 
the hypercube for which L * = 0.  This setup relates to the fact that if we consider 
Y  =  I as a circuit failure and Y  = ° as an operable circuit, then Y is a deterministic 

function of the X(i)'s, which may be considered as gates or switches. In that case, 
Y  may be written as a boolean function  of X (1) ,  .•. ,  XCd).  We may limit boolean 
classifiers in various ways by partially specifying this function. For example, fol(cid:173)
lowing  Natarajan  (1991),  we  first  consider  all  monomials,  that is,  all  functions 
g  : {O,  l}d  -+  {O,  I}  of the form 

27.4 Boolean Classifiers 

469 

for some k  .:s  d  and some indices 1 .:s  il  <  ... <  ik  .:s  d  (note that algebraic mul(cid:173)
tiplication corresponds to a boolean "and"). In such situations, one might attempt 
to minimize the empirical error. As we know that g* is also a monomial, it is clear 
that the minimal empirical error is zero. One such minimizing monomial is given 
by 

where 

d 

an  mm 
l::J::;on 

.  XU)  0 

i  = 

.  d  {' 

.  } 
,  ]  'F  ll,···, lk  . 

Thus,  gn  picks  those components for  which every data point has  a  "1". Clearly, 
the empirical error is zero. The number of possible functions is 2d.  Therefore, by 
Theorem  12.1, 

and 

E{L(gn)} .:s  - .  

d+1 

n 

Here again, we have avoided the curse of dimensionality. For good performance, 
it suffices that n be a bit larger than d, regardless of the distribution of the data! 

Assume that we limit the complexity of a boolean classifier g by requiring that g 
must be written as an expression having at most k operations "not," "and," or "or," 
with the X(i),S as inputs. To avoid problems with precedence rules, we assume that 
any  number of parentheses is  allowed in the expression.  One may visualize each 
expression as  an  expression tree,  that is,  a tree in which internal nodes represent 
operations and leaves represent operands (inputs). The number of such binary trees 
with k  internal nodes (and thus k + 1 leaves) is given by the Catalan number 

1  (2k) 
k + 1  k 

(see, e.g., Kemp (1984)). Furthermore, we may associate any of the X(i),S with the 
leaves  (possibly preceded by "not") and "and" or "or" with each binary internal 
node, thus obtaining a total of not more than 

2k (2d)k+ 1 _1_ (2k) 
k 

k+1 

possible boolean functions of this kind. As k  -?  00, this bound is not more than 

for k large enough. Again, for k large enough, 

470 

27.  Hypercubes and Discrete Spaces 

and 

E{L(gn)}  ::::  k 10g(16d) + 1 . 

n 

Note that k is  much more important than d in determining the  sample  size.  For 
historic reasons, we mention that if g  is  any boolean expression consisting of at 
most k  "not," "and,"  or "or" operations, then the number of such functions  was 
shown by Pippenger (1977) not to exceed 

C6

(d
k

+kf Y 

Pearl (1979) used Pippenger's estimate to obtain performance bounds such as the 
ones given above. 

27.5  Series Methods for the Hypercube 

It is  interesting to  note  that we may write  any function  rt  on the hypercube as  a 
linear combination of Rademacher-Walsh polynomials 

2x(l) - 1 

2X(d)  - 1 
(2x(l)  - 1)(2x(2)  - 1) 

i=O 
i =  1 

i=d 
i=d+1 

(2X(d-l)  - 1)(2x(d)  - 1) 

(2x(l)  - 1)(2x(2)  - 1)(2x(3)  - 1) 

i  = d + 1 + (~) 
i  = d + 2 + (~) 

(2x(l)  - 1) ... (2x(d)  - 1) 

i  = 2d  - 1. 

We verify easily that 

so that the 1/Ji 's form an orthogonal system. Therefore, we may write 

fl-({x})rt(x) = L ai 1/Ji(x), 

2d -l 

i=O 

where 

27.5 Series Methods for the Hypercube 

471-

and f1({x}) = P{X = x}. Also, 

f1({x})(l  - 17(X)) = L bil/Ji(X), 

2d -1 

i=O 

with 

bi  =  ;d ~ l/rt(x)(l - ry(X»Jl({X)) = E {1/f~SX) lIY~OI}' 

Sample-based estimates of ai  and bi  are 

= 

= 

The Bayes rule is given by 

Replacing  ai  - bi  formally  by ~ - b;  yields  the plug-in rule.  Observe that this 
is just a discrete version of the Fourier series rules discussed in Chapter 17. This 
rule requires the estimation of 2d  differences ai  - bi . Therefore, we might as well 
have used the fundamental rule. 

When our hand is forced by the dimension, we may wish to consider only rules 

in the class C given by 

g(x) = 

{

I 
0 

if  "~~)+(~)+"+(:)(a~ - b(»lr·(x)  >  0 
otherwise, 

.L...t1=O 

'f'l 

I 

I 

where  k  :::  d  is  a positive integer,  and ab, bb, ai ' b~, ... are  arbitrary  constants. 
We  have  seen  that  the  vc dimension  of this  class  is  not  more  than  (~) + (~) + 
... + (~)  :::  d k  +  1 (Theorem  13.9).  Within  this  class,  estimation  errors  of the 
order of 0  ( .J dk log n / n)  are  thus  possible if we  minimize  the  empirical error 
(Theorem 13.12). This, in effect, forces us to take k «  logd n. For larger k, pattern 
recognition is all but impossible. 

As  an interesting side note, observe that for a given parameter k,  each member 

of C is a k-th degree polynomial in x. 

REMARK.  PERFORMANCE OF THE PLUG-IN RULE.  Define the plug-in rule by 

gn(x) = 

{

I 
o  otherwise. 

.L...t1=O 

if  ,,~~)+(~)+···+(t)(a. - T;.»lr·(x)  >  0 

I 

I 

'f'l 

472 

27.  Hypercubes and Discrete Spaces 

As n  ----+  00, by the law oflarge numbers,  gn  approaches goo: 

goo(x) = 

{ 

'f  ~(~)+(~)+ ... +(~)( 
1 

ai  -

1 
o  otherWIse, 

Li==,O 

b  ),/r  ()  0 
i 

'Pi  X  > 

where ai, bi  are as  defined above. Interestingly,  goo  may be much worse than the 
best rule in C!  Consider the simple example for d = 2, k = 1: 
II  x(l) = 0 

x(l) = 1 

Table of 17 (x ): 

X(2;  =  1 
X(2  = 0 

118 
5/8 

6/8 
1/8 
II 
II  x(l) = 0  X(I) = 1 

Table of I'l({x}) = P{X = x}: 

x(2)  =  1 
x(2)  = 0 

P 
4  8p 
-9-

II 

s-lOp 
~ 

P 

A  simple calculation shows that for  any  choice  p  E  [0,  1/2], either goo  ==  1 or 
goo  ==  O.  We  have  goo  ==  1 if  p  <  10/47,  and  goo  ==  0  otherwise.  However, 
in  obvious  notation,  L(g  ==  1)  =  (41p  + 11)/36,  L(g  ==  0)  =  (50  - 82p)/72. 
The  best constant  rule  is  g  ==  1  when  p  <  7/41  and  g  ==  0  otherwise.  For 
7/41  <  p  <  10/47, the plug-in rule does not even pick the best constant rule, let 
alone the best rule in C with k =  1,  which it was intended to pick. 

This example highlights  the danger of parametric rules or plug-in rules  when 

applied to incorrect or incomplete models.  0 

REMARK. HISTORICAL NOTES. The Rademacher-Walsh expansion occurs frequently 
in switching theory, and was given in Duda and Hart (1973). The Bahadur-Lazars(cid:173)
feid  expansion  (Bahadur  (1961»  is  similar  in  nature.  Ito  (1969)  presents  error 
bounds for discrimination based upon  a k-term truncation  of the series.  Ott and 
Kronmal (1976) provide further statistical properties. The rules described here with 
k defining the number of interactions are also obtained if we model P {X = x I Y = I} 
and P {X = x I Y  = 1}  by functions of the form 

) 
and  exp  ~  bi o/i (x) 

((~)+(~)+ ... +(~) 

. 

The  latter  model  is  called the  log-linear  model  (see  McLachlan  (1992,  section 
7.3».0 

27.6  Maximum Likelihood 

The maximum likelihood method (see Chapter 15) should not be used for picking 
the best rule from a class C that is not guaranteed to include the Bayes rule. Perhaps 

a simple example will suffice to make the point. Consider the following hypercube 
setting with d  = 2: 

27.6 Maximum Likelihood 

473  . 

1] (x )  : 

p,({x})  : 

II  x(l)= 0  I x(l) = 1 

II  x(l) = 0 

p 
r 

q 
s 

In this example, L * = o. Apply maximum likelihood to a class F  with two members 
{1]A,  1]B},  where 

1]A(X) 

1] B(X) 

= 

{ 4/5 
liS 
{ 0 

1 

if x(l)  = 0 
if x(l) =  1, 

if x(l) = 0 
if x(l) =  1. 

Then maximum likelihood won't even pick the best member from F! To verify 
this,  with  gA(X)  =  1{1JA(x»lj2j,  gB(X)  =  1{1JB(x»lj2j.  we see that L(gA)  =  P + q, 
L(g B)  = r + s. However, if 1]ML  is given by 

and if we write  Nij  for the number of data pairs (X k,  Yk)  having X k = i, Yk = j, 
then  1]ML  = 1] A  if 

o  if NOl  + NlO  >  0 
if N 01  + N 10  = 0 
1 

and  1]ML  = 1]B  otherwise.  Equivalently,  1]ML  = 1]A  if and only if N01  + NlO  >  O. 
Apply  the  strong  law  of large numbers  to  note that  NOlin  --+  r,  NlOln  --+  s, 
Nooln  --+  p, and Nll/n --+  q  with probability one, as n  --+  00. Thus, 

lim  P{ 
n-+oo 

1]ML 

= 

} = {I  if r  + s  >  0 

0  otherwise. 

1]A 

Take r + s  = E  very small, p + q  = 1 - E. Then, for the maximum likelihood rule, 

However, when F contains the Bayes rule, maximum likelihood is consistent (see 
Theorem 15.1). 

474 

27.  Hypercubes and Discrete Spaces 

27.7  Kernel Methods 

Sometimes,  d  is  so  large  with respect to  n  that the  atoms  in the  hypercube  are 
sparsely  populated.  Some  amount of smoothing  may  help  under  some  circum(cid:173)
stances. Consider a kernel K, and define the kernel rule 

gn(x) = 

{

if  1 "'~1_ (2Yi  -l)K (lIx - Xill)  >  0 

h 

I 
o  otherwise, 

n  L....-I-l 

2 

where  Ilx  - z II  is just the  Hamming distance  (i.e.,  the  number of disagreements 
between components of x and z). With K (u) = e _u
,  the rule above reduces to a rule 
given in Aitchison and Aitken (1976). In their paper, different h ' s are considered for 
the two classes, but we won't consider that distinction here. Observe that at h = 0, 
we  obtain the  fundamental  rule.  As  h  ~ 00,  we  obtain a (degenerate) majority 
rule  over  the  entire  sample.  The  weight  given  to  an  observation  Xi  decreases 
exponentially in Ilx - Xi 112.  For consistency, we merely need h  ~ 0 (the condition 
nh d  ~ 00  of Theorem  10.1  is  no  longer needed).  And  in  fact,  we  even  have 
consistency with h ==  0 as this yields the fundamental rule. 

The  data-based  choice  of h  has  been  the  object  of several  papers,  including 
Hall (1981) and Hall and Wand  (1988).  In the latter paper,  a mean squared error 
criterion is  minimized.  We  only  mention  the  work of Tutz  (1986;  1988;  1989), 
who picks h so as to minimize the deleted estimate L~D). 

Theorem 27.6.  (TuTZ (1986».  Let Hn  be the smoothing factor in the Aitchison(cid:173)
Aitken rule that minimizes L~). Then  the rule  is weakly consistent for all distri(cid:173)
butions of (X, Y) on the hypercube. 

PROOF.  See Theorem 25.8.  0 

Problems and Exercises 

PROBLEM  27.1.  THE  FUNDAMENTAL  RULE.  Let g,:  be the  fundamental  rule  on  a  finite  set 
{I, ... , k}, and define L/1  = L(g,:). Let g* be the Bayes rule (with error probability L *), and 
let 

~ =  inf  (~- min(r](x), 1 -

X:1)(x}fIJ2  2 

r](X») . 

Let L~R) be the resubstitution error estimate (or apparent error rate). Show the following: 

(1)  E {L~R)! :::  ELn  (the apparent error rate is always optimistic; Hills (1966». 
(2)  E {L~R)  :::  L * (the apparent error rate is in fact very optimistic; Glick (1973». 
(3)  ELn:::  L * + e-2n /::,2  (for a  similar inequality related to Hellinger distances,  see 

Glick (1973». This is an exponential but distribution-dependent error rate. 

PROBLEM 27.2.  DISCRETE  LIPSCHITZ  CLASSES.  Consider the  class  of regression  functions 
r]  E  [0,  1] with I r](x) -
r](z) I :::  cp(x, zt, where x, z  E  {O,  l}d, p(., .) denotes the Hamming 

distance, and c  > ° and a  > ° are constants (note that a  is not bounded from above). The 
purpose  is  to  design  a  discrimination  rule  for  which uniformly  over  all  distributions  of 
(X, y) on {O,  1}d  x  {O,  I}  with such 1](x) = pry = 11X = x}, we have 

Problems and Exercises 

475 

E{L  _  L *}  <  \II(e, a, d) 
' 

In 

n 

-

where the function \II(e, a, d) is as  small as possible. Note: for e  =  1, the class contains all 
regression functions on the hypercube, and thus \11(1,  a, d) = 1.075 . 2d/2  (Theorem 27.1). 
How small should e be to make \II polynomial in d? 

PROBLEM  27.3.  With a cubic histogram partition of [0,  1Jd  into kd cells (of volume  1/ kd 
each), we have, for the Lipschitz (e, a) class:F of Theorem 27.4, 

sup  8 = e 

CX,y)EF 

( ~)a 

k 

-

This grows as d a
HINT:  Consult Conway and Sloane (1993). 

2

/

. Can you define a partition into k d  cells for which sUPcx, y)EF 8 is smaller? 

PROBLEM  27.4.  Consider the following  randomized histogram rule:  Xl, ... , X k  partition 
[0,  1]d  into polyhedra based on the nearest neighbor rule.  Within each cell, we employ  a 
majority rule based upon X k+l ,  •.• ,  X n .  If X is uniform on [0,  1]d  and 1]  is Lipschitz (e, a) 
(as in Theorem 27.4), then can you derive an upper bound for E{L n - L *} as  a function of 
k, n, e, a, and d? How does your bound compare with the cubic histogram rule that uses 
the same number (k) of cells? 
PROBLEM  27.5.  Let:F be the class of all Lipschitz (e,  a) functions  1]'  E  nd  -+  [0,  1J.  Let 
(X,  Y)  E  :F denote the fact that (X, Y) has regression function 1](x) = P{Y = 11X  = x} in 
F. Then, for any cubic histogram rule, show that 

sup 

E{ 

CX,Y)EF 

* 

1 
Ln  - L  } 2:  -
2 

* 
- L  . 

Thus,  the  compactness  condition  on the  space  is  essential  for  the distribution-free  error 
bound given in Theorem 27.4. 

PROBLEM 27.6.  INDEPENDENT  MODEL.  Show that in the independent model, the maximum 
likelihood estimate p(i) of p(i) is given by 

r:~=l I{xjil=I,Yj=l} 

r:~=l I{Yj=l} 

PROBLEM  27.7.  INDEPENDENT  MODEL.  For the plug-in rule in the independent model,  is it 
true that E{Ln - L *} = 0(1/ In) uniformly over all pairs (X, Y) on {O,  I}d  x  {O,  I}? If so, 
find  a constant e depending upon d  only, such that E{L n  - L *} :s  e/ -Jri. If not, provide a 
counterexample. 

PROBLEM 27.8.  Consider a  hypercube  problem  in  which  X  =  (X(1),  ... , XCd)  and each 
XCi)  E  {-1, 0,  1}  (a ternary generalization). Assume that the XU)'s are independent but not 

476 

27.  Hypercubes and Discrete Spaces 

necessarily identically distributed. Show that there exists a quadratic Bayes rule, i.e., g*(x) 
is 1 on the set 

d 

ao + L aix(i) + L aijx(i) x(j)  >  0, 

d 

i=] 

i,j=l 

where ao, {ai}, and {aij} are some weights (Kazmierczak and Steinbuch (1963». 

PROBLEM 27.9.  Let A be the class of all sets on the hypercube {a,  1}d  of the form xCi!)  ... 
X(ik)  = 1,  where (x(l),  ... , xed»~  E  {a,  l}d,  1  :::  i1  <  '"  <  ik  :::  d.  (Thus,  A  is  the class 
of all sets carved out by the monomials.) Show that the vc dimension of Cis d.  HINT:  The 
set {CO,  1,  1,  ... ,1), (1, 0,1, ... ,1), ... , (1,1,1, ... , a)} is shattered by A. No set of size 
d + 1 can be shattered by A by the pigeonhole principle. 

PROBLEM  27.10.  Show that the Catalan number 

1  (2n)  4n 

- -
n+ I  n  ~. 

" ' - ' - -

(See, e.g., Kemp (1984).) 

PROBLEM  27.11.  Provide an argument to show that the number of boolean functions with 
at most k operations "and" or "or" and d  operands of the form xU)  or I - x(i), xCi)  E  {a,  I}, 
is not more than 

k+ 1  k 
(this is 2k  times less than the bound given in the text). 

2(2di+1 _1_ (2k) 

PROBLEM  27.12.  Provide upper and lower bounds on the vc dimension of the class of sets 
A on the hypercube {a,  l}d  that can be described by a boolean expression with the x(i)'s or 
1 - x(i)'s as operands and with at most k  operations "and" or "or." 

PROBLEM 27.13.  LINEAR DISCRIMINATION  ON  THE  HYPERCUBE.  Let gn  be the rule that mini(cid:173)
mizes the empirical error Ln(¢) over all linear rules ¢, when the data are drawn from any 
distribution on {a,  l}d  x  {a,  I}. Let Ln be its probability of error, and let L be the minimum 
error probability over all linear rules.  Show that for E  >  0, 

Deduce that 

E{L n  - L} :::  2 

J 1 + log4(~) 

ffn 

d + 1 
:::  2--. 
ffn 

Compare this result with the general Vapnik-Chervonenkis bound for linear rules (Theorem 
13.11) and deduce when the bound given above is better. HINT: Count the number of possible 
linear rules. 
PROBLEM  27.14.  On the hypercube to,  l}d, show that the kernel rule of Aitchison and Aitken 
(1976) is strongly consistent when limn-+oo h = 0. 

PROBLEM  27 .15.  Pick h  in the kernel estimate by minimizing the resubstitution estimate 
L~R), and call it HY). For L~D), we call it H~D). Assume that the kernel function is  of the 

form  K(II  .  III h) with  K  ~ 0,  K(u)  -t ° as  u  t  00.  Let Ln  be the error estimate for the 

kernel rule with one of these two choices. Is it possible to find a constant c, depending upon 
d and K  only,  such that 

Problems and Exercises 

477 

E {Ln  - inf Ln(h)}:S  ~? 

h:::.O 

yn 

If so,  give  a  proof.  If not,  provide  a  counterexample.  Note:  if the  answer  is  positive,  a 
minor corollary of this result is Tutz's theorem. However, an explicit constant c may aid in 
determining appropriate sample sizes. It may also be minimized with respect to  K. 

PROBLEM  27.16.  (Simon (1991).) Construct a partition of the hypercube {a,  l}d  in the fol(cid:173)
lowing  manner,  based  upon  a binary  classification  tree  with  perpendicular  splits:  Every 

node at level i splits the subset according to x(i) = ° or x(i) = 1,  so that there are at most d 

levels of nodes.  (In practice, the most important component should be x(l).) For example, 
all possible partitions of {a,  1}2  obtainable with 2 cuts are 

..  . 

•  • 

..  .  :~  ~:  • I • 

~ 

Assign to each internal node (each region) a class. Define the Horton-Strahler number ~ of 
a tree as follows:  if a tree has one node, then ~ = 0.  If the root of the tree has left and right 
subtrees with Horton-Strahler numbers ~1  and ~2' then set 

Let C be the class of classifiers g described above with Horton-Strahler number :s  ~. 

(1)  Let S = {x  E  {a,  1}d  : Ilx II  :s n, where II  . II  denotes Hamming distance from the 

all-zero vector. Show that S is shattered by the class of sets  {g = 1 : g  E  C}. 

(2)  Show that lSI  = Ito e)· 
(3)  Conclude that the vc dimension of C is at least 2:;==0  (~). (Simon has shown that 

the vc dimension of C is exactly this, but that proof is more involved.) 

(4)  Assuming L * = 0, obtain an upper bound for E{L n } as afunctionof~ andd, where 
Ln  is the probability of elTor for the rule picked by minimizing the empirical elTor 
over C. 
Interpret ~ as the height of the largest complete binary tree that can be embedded 
in the classification tree. 

(5) 

28 
Epsilon Entropy and Totally 
Bounded Sets 

28.1  Definitions 

This  chapter deals  with discrimination rules that are picked from  a certain class 
of classifiers by minimizing the empirical probability of error over a finite  set of 
carefully  selected rules.  We  begin with a class F  of regression functions  (i.e.,  a 
posteriori probability functions)  TJ  : nd  -+  [0,  1]  from which  TJn  will be picked 
by  the  data.  The  massiveness  of F  can  be  measured  in  many  ways-the route 
followed  here is  suggested in  the  work of Kolmogorov  and Tikhomirov  (1961). 
We will depart from their work only in details. We suggest comparing the results 
here with those from Chapters 12 and 15. 

Let FE  = {TJ(l),  ... , TJ(N)}  be a finite  collection of functions nd  -+  [0,1] such 

that 

where  ST)I,E  is the ball of all functions ~ : nd  -+  [0,  1]  with 

II~  - TJ'IIoo  = sup I~(x) - TJ'(x)1  <  E. 

x 

In  other  words,  for  each  TJ'  E  F,  there  exists  an  TJU)  E  FE  with  sUPx  ITJ'(x)  -
TJ(i)(x) I <  E.  The  fewer  TJ(i),S  needed to  cover F, the  smaller F  is,  in  a certain 
sense.  FE  is  called an  E -cove r of F. The minimal value of I FE lover all  E -covers 
is  called  the  E-covering  number (NE ).  Following  Kolmogorov  and  Tikhomirov 
(1961), 

480 

28.  Epsilon Entropy and Totally Bounded Sets 

is called the E -entropy of F. We will also call it the metric entropy. A collection 
F  is  totally bounded if NE  <  00 for all E >  O.  It is with such classes that we are 
concerned in this chapter. The next section gives a few examples. In the following 
section,  we define the skeleton  estimate based upon picking the empirically best 
member from FE' 

FIGURE 28.1.  An  E -cover  of the 
unit square. 

28.2  Examples: Totally Bounded Classes 

The  simple  scalar  parametric  class  F  =  {e-e1x 1, x  E  R; 8  >  O}  is  not  totally 
bounded  (Problem  28.1).  This  is  due  simply  to  the  presence  of an  unrestricted 
scale factor.  It would still fail to be totally bounded if we restricted 8 to [1, (0) or 
[0,  1].  However, if we force 8  E  [0,  1]  and change the class F  to have functions 

, 
1]  (x) =  0 

{e-B1X1 

if Ixl  ::::  1 
otherwise, 

then the class is totally bounded. While it is usually difficult to compute N: exactly, 
it is  often  simple to  obtain matching  upper and lower bounds.  Here  is  a  simple 
argument.  Take  8i  = 2E i, 0  ::::  i  ::::  L I j (2E) J  and  define  8 *  = 1.  Each of these 
8-values defines  a function  1]'.  Collect these in FE'  Note that with  1]'  E  F,  with 
parameter 8, if e is  the nearest value among  {8i , 0  ::::  i  ::::  L 1 j (2E) J}  U {8 *},  then 
Ie - 81  ::::  E.  But then 

sup  je-e1xl  - e-e1xlj  ::::  18  - el  ::::  E. 

Ixisl 

Hence FE  is  an  E-cover of F  of cardinality  L1j(2E)J  + 2.  We  conclude that F  is 
totally bounded, and that 

(See Problem 28.2 for a d-dimensional generalization.) 

NE  ::::  2 + L1j(2E)J. 

28.2 Examples: Totally Bounded Classes 

481 

For a lower bound, we use once again an idea from Kolmogorov and Tikhomirov 
(1961). Let OE  =  {1}(l),  ... ,  1}(M)}  c  F  be a subset with the property that for every 
i  -::j  j, supx  11}(i)(x)  - 1}(j)(x)I :::::  E.  The set OE  is thus E-separated.  The maximal 
cardinality E -separated set is called the E -packing number (or E -separation number) 
ME. It is easy to see that 

M2E  :::::NE:::::  ME 

(Problem 28.3). With this in hand, we see that OE  may be constructed as  follows 
for our example: Begin with 80  = 0. Then define 81 by e-B1  = 1 - E,  e-B2  = 1 - 2E, 
etcetera,  until  8i  >  1.  It  is  clear  that  this  wayan  E -separated  set  OE  may  be 
constructed with IOE I =  L (1  - 1/ e) / E J + 1.  Thus, 

Ne  :::::  M2E  ::::: 

l1-1/eJ 

2E 

+ 1. 

The E-entropy of F  grows as 10g2(1/E) as E t 0. 

Consider  next  a  larger  class,  not of a  parametric  nature:  let F  be  a  class  of 

functions  1}  on [0,  Ll]  satisfying the Lipschitz condition 

11}(x)  - 1}(XI) I :::::  clx - xII 

and taking values on [0,  1]. Kolmogorov and Tikhomirov (1961) showed that if 

E  <  min (~ _1_) 

4'  16Llc 

-

' 

then 

< 

< 

10g2 N E 
Llc 
-

E 

+log2 - +3 

I 

E 

(see Problem 28.5).  Observe that the metric entropy is exponentially larger than 
for the parametric class considered above. This has a major impact on sample sizes 
needed for similar performances (see the next sections). 

Another class of functions of interest is that containing functions  1}  :  [0,  1]  -+ 
[0,  1]  that are s-times differentiable and for which the s-th derivative 1}(s)  satisfies 
a Holder condition of order ex , 

11}(S)(x)  -n(s)(xl)1  :::::  clx - xll a , 

x, Xl  E  [0,  1], 

where c is a constant. In that case, 10g2 ME  and log2Ne  are both 8  (E-1/(s+a»)  as 
E  t  0,  where an  = 8(bn)  means  that an  = O(bn)  and bn  =  O(an).  This result, 
also due to Kolmogorov and Tikhomirov (1961), establishes a continuum of rates 
of increase  of the  E-entropy.  In n d, with functions  n : [0,  l]d  -+  [0,  1],  if the 
Holder condition holds for all derivatives of order s, then log2 NE  = 8  (E-d/(s+a»). 

482 

28.  Epsilon Entropy and Totally Bounded Sets 

Here we have an interesting interpretation of dimension: doubling the dimension 
roughly  offsets  the  effect  of doubling  the  number  of derivatives  (or the  degree 
of smoothness)  of the  1]'s.  Working  with  Lipschitz functions  on Rl  is  roughly 
equivalent to working with functions on R 25  for which all 24-th order derivatives 
are Lipschitz! As there are 2524  such derivatives, we note immediately how much 
we must pay for certain performances in high dimensions. 

Let :F be the class  of all entire analytic functions  1]  :  [0, 2;r]  ----+  [0,  1]  whose 

periodic continuation satisfies 

for some constants c, a  (z  is  a complex variable,  ~(z) is its imaginary part). For 
this class, we know that 

1 
10g2)\('"'-'(4LaJ+2)log-
E 

as  E+O 

(Kolmogorov  and  Tikhomirov  (1961)).  The class  appears  to  be as  small  as  our 
parametric class. See also Vitushkin (1961). 

28.3  Skeleton Estimates 

The members of :FE  form a representative skeleton of F. We assume that FE  c F 
(this condition was not imposed in the definition of an E-cover). For each 1]'  E F, 
we define its discrimination rule as 

(x) = {  1 
g 

if 1]'(x). >  1/2 

0  otherwIse. 

Thus,  we will  take  the liberty  of referring  to  1]'  as  a  rule.  For each such  1]',  we 
define the probability of error as usual: 

L(1]') = P{g(X) =I  Y}. 

The empirical probability of error of 1]'  is denoted by 

We define the skeleton estimate 1]n  by 

1]n  = argminLn (l'7'). 

r/'EFE 

One of the best rules in :F is denoted by 1]*: 

L(1]*)  2:  L(1]'),  1]'  E  F. 

28.3 Skeleton Estimates 

483 

The  first  objective,  as  in  standard empirical risk minimization,  is  to  ensure  that 
L(1]n) is close to L(rJ*). If the true a posteriori probability function rJ  is in F  (recall 
that  1] (x )  = P{Y  = 11X  = x n, then  it is  clear that  L * = L(rJ*).  It will be  seen 
from the theorem below that under this condition, the skeleton estimate has nice 
consistency and rate-of-convergence properties. The result is distribution-free in 
the sense that no structure on the distribution of X is assumed. Problems 6.9 and 
28.9  show  that convergence  of L(rJn)  to  L(rJ*)  for  all  rJ's-that is,  not only for 
those in F-holds if X has a density.  In any  case, because the skeleton estimate 
is selected from a finite deterministic set (that may be constructed before data are 
collected!), probability bounding is trivial: for all E  >  0,8 >  0, we have 

P {L(rJn)  -

inf  L(rJ!)  >  <5} 
r/,EFE 

< 

< 

IFE I sup  P  {ILn(rJ')  - L(rJ')1  >  8/2} 

TJ'EFE 

(by Lemma 8.2) 

21FE le- no2 /2 

(Hoeffding's inequality). 

Theorem 28.1.  Let F  be a  totally bounded class offunctions rJ'  : nd  -+  [0,  1]. 
There is a sequence {En> O}  and a sequence of skeletons FEn  C  F  such that if rJn 
is the skeleton estimate drawn from FEn'  then 

L(rJn)  -+ L *  with probability one, 

whenever the true regression function 1] (x ) = P{Y = 11X = x} is in F. 

It  suffices  to  take  FE  as  an  E -cover of F  (note  that  IFE I need not equal  the 
E -covering number NE ), and to define En  as the smallest positive number for which 

Finally,  with En  picked in this manner, 

E {L(ryn)  - L '} :s  (2 + v's)€n  + tf;. 

PROOF.  We note that infTJ'EFE L(rJ')  S  L * + 2E, because if rJ'  E  STJ,E'  then 

E{lrJ'(X) - 1](X)I}  s sup Ir/(x) - 1](x)1  S  E 

x 

and thus, by Theorem 2.2,  L( 1]')  - L * S  2E. Then for any <5  :::  2En , 

P{L(1]n)  - L * >  8}  <  P{L(rJn)-

inf  L(1]!»8-2En} 
1]'EFEn 

< 

21FEn le- n(o-2Eni /2 

(see above) 

484 

28.  Epsilon Entropy and Totally Bounded Sets 

which is summable in n, as En  = 00). This shows the first part of the theorem. For 
the second part, we have 

E{L(1'}n)}  - L * 

< 

< 

< 

< 

= 

2En  + E{L(1'}n)}  -

inf  L(1'}') 
r( EFEn 

2En  + 100 
min (2e 2",;-",'/2, I) dt 
(2 + v's)En + 21 00  e2nE~-nt2!2dt 
(2 + Js)En + 100 
(2+ Js)En + t. 

e-",' /4dt 

,J8"En 

(since E~ -

t 2/4  :::  -t2/8 for t  ::::  ~En) 

The proof is completed.  0 

Observe that the estimate 1'}n  is picked from a deterministic class. This, of course, 
requires quite a bit of preparation and knowledge on behalf of the user. Knowledge 
of the E-entropy (or at least an upper bound) is absolutely essential. Furthermore, 
one must be able  to  construct FE'  This  is  certainly not computationally simple. 
Skeleton  estimates  should  therefore  be  mainly  of theoretical  importance.  They 
may be used, for example, to establish the existence of estimates with a guaranteed 
error bound as  given in Theorem 28.1.  A  similar idea in  nonparametric  density 
estimation was proposed and worked out in Devroye (987). 

REMARK. In the first step of the proof, we used the inequality 

E{I1'}'(X) - 17(X)I}  :::;  sup I 77'(X)  - 1'}(X) I <  E. 

x 

It is clear from this that what we really need is not an E -cover of F  with respect to 
the supremum norm, but rather,  with respect to the L 1 (/-l)  norm.  In other words, 
the  skeleton estimate works equally well if the skeleton is  an  E -cover of F  with 
respect to the  Ll (M)  norm, that is, it is a list of finitely many candidates with the 
property that for each 1'}'  E  F  there exists an  lJ(i)  in the list such that E{l1'}' (X) -
77(i) (X) I}  <  E. It follows from the inequality above that the smallest such covering 
has fewer elements than that of any  E -cover of F  with respect to the  supremum 
norm.  Therefore,  estimates based on  such  skeletons  perform  better.  In  fact,  the 
difference may be essential. As an example, consider the class F  of all regression 
functions  on  [0,  1]  that are  monotone  increasing.  For E  <  1/2, this  class  does 
not have a finite  E -cover with respect to the  supremum norm.  However,  for any 
M it  is  possible  to  find  an  E-cover of F,  with  respect  to  L1(/-l),  with  not more 

28.4 Rate of Convergence 

485 

than  4fl/El  elements  (Problem  28.6).  Unfortunately,  an  E-cover  with  respect  to 
Ll(lL) depends  on  IL,  the  distribution  of X.  Since  IL  is  not  known  in  advance, 
we  cannot construct this  better skeleton.  However,  in some cases, we may have 
some a priori information about J.L.Forexample, if we know that J.L  is a member 
of a known  class  of distributions,  then  we  may be  able  to  construct a  skeleton 
that is an E -cover for all measures in the class. In the above example, if we know 
that IL  has  a density bounded by a known number, then there is  a finite  skeleton 
with this property (see Problem 28.7). We note here that the basic idea behind the 
empirical covering method of Buescher and Kumar (l996a), described in Problem 
12.14,  is  to  find  a  good  skeleton  based  on  a  fraction  of the  data.  Investigating 
this question further, we notice that even covering in L I (J.L)  is more than what we 
really need. From the proof of Theorem 28.1, we see that all we need is a skeleton 
FE  such  that  infrylEFE  L(1]')  :::  L * + E for  all  1]  E  F. Staying  with  the  example 
of the class of monotonically increasing  1] 's,  we see that we  may take  in FE  the 
functions  1](i)(x)  =  I{x?:.q;}'  where qi  is  the  i-th E-quantile of J.L,  that is,  qi  is the 
smallest number z such that P {X  :::  z}  2:  i / E.  This collection of functions forms 
a skeleton in the required sense with  about (l/E) elements, instead of the 4fl/El 
obtained by covering in L I (J.L), a significant improvement. Problem 28.8 illustrates 
another application of this idea. For more work on this we refer to Vapnik (1982), 
Benedek  and  Itai  (1988),  Kulkarni  (1991),  Dudley,  Kulkarni,  Richardson,  and 
Zeitouni (1994), and Buescher and Kumar (1996a).  0 

28.4  Rate of Convergence 

In this section, we take a closer look at the distribution-free upper bound 

E IL(~n) - L *} :s  (2 + vIs)cn + tf· 

For typical parametric classes  (such as  the one discussed in a Section 28.2),  we 
have 

;v,-e(D 

If we take IFE I close enough to Ne, then Ell  is the solution of 

10gNe  "-
- - 2 - = n, 

E 

or En  =  G(log n / ~), and we achieve a guaranteed rate of 0 (log n / ~). The same 
is true for the example of the class of analytic functions discussed earlier. 

The situation is different for massive classes such as the Lipschitz functions on 
[0,  l]d. Recalling that 10gNe  = e(1/Ed )  as E ~ 0, we note that Ell  = G(n- 1/(2+d). 
For this class, we have 

486 

28.  Epsilon Entropy and Totally Bounded Sets 

Here, once again, we encounter the phenomenon called the "curse of dimension(cid:173)
ality." In order to achieve the performance E {L(1']n)  - L *}  S  E, we need a sample 
of size n  2:  (l/E?+d,  exponentially large in d.  Note that the class  of classifiers 
defined by this class of functions has infinite vc dimension. The skeleton estimates 
thus provide a vehicle for dealing with very large classes. 

Finally, if we take all 1']  on [0,  1] forwhichs derivatives exist, and 1'](s)  is Lipschitz 
with a given constant c,  similar considerations show that the rate of convergence 
is  0  (n-(s+1)/(2s+3»),  which ranges from  0  (n-l/3)  (at s  = 0) to  0  (n-l/2)  (as s  ---'» 
(0). As the class becomes smaller, we can guarantee better rates of convergence. 
Of course, this requires more a priori knowledge about the true 1']. 

We also note that if log ~ / E2  = 2n in Theorem 28.1, then the bound is 

~ + (2 + v's)En =  ~ + (2 + v's)  log~ll. 
V -;; 

V -;; 

2n 

The error grows only sub-logarithmically in the size of the skeleton set. It grows as 
the square root of the E -entropy. Roughly speaking (and ignoring the dependence 
of En  upon n),  we may  say  that for  the  same performance guarantees,  doubling 
the  E -entropy implies  that we should double the  sample  size  (to  keep log ~ / n 
constant).  When referring  to  E-entropy,  it is  important to  keep  this  sample  size 
interpretation in mind. 

Problems and Exercises 

PROBLEM 28.1.  Show  that  the  class  of functions  e-8lxl  on  the  real  line,  with  parameter 
()  >  0, is not totally bounded. 

PROBLEM 28.2.  Compute a good upper bound for N"  as  a function of d and E for the class 
F  of all functions  on n d  given by 

1]'(x) =  ° 

{ 

e-8l1xll 

if Ilxll  :s  1 
otherwise, 

where ()  E  [0,  1]  is a parameter. Repeat this question if ()j,  ... ,  ()d  are in [0,  1]  and 

HINT:  Both answers are polynomial in liE as  E ,J,  0. 
PROBLEM 28.3.  Show that M2E  :s ~ :s  ME  for any totally bounded set F  (Kolmogorov 
and Tikhomirov (1961». 

PROBLEM 28.4.  Find a class F  of functions  1]'  :  Rd  ~ [0,  1]  such that 
(a)  for every E  >  0, it has a finite  E-cover; 
(b) 

the vc dimension of A = {{x  : 7J'(x)  >  I/2}; 1]'  E  F} is infinite. 

Problems and Exercises 

487 

PROBLEM 28.5.  Show that if F  =  {lJl  :  [0,6.]  ---+  [0,  1],  lJl  is Lipschitz with constant c}, 
then for E small enough, 10g2 Ne  S  AlE, where A  is a constant depending upon 6.  and c. 
(This is not as precise as the statement in the text obtained by Kolmogorov and Tikhomirov, 
but it will give you excellent practice.) 

PROBLEM  28.6.  Obtain an estimate for the cardinality of the smallest E-cover with respect 
to the L 1(/.1)  norm for the class of l}'S  on (0,1] that are increasing. In particular, show that 
for any f.l it is possible to find an E-cover with 4fl/El elements. Can you do something similar 
for lJ'S  on [0,  1]d  that are increasing in each coordinate? 

PROBLEM 28.7.  Consider the class of l}'S  on [0,  1]  that are increasing. Show that for every 
E  >  0, there is a finite list l}(l),  ... lJ(N)  such that for all  lJ  in the class, 

whenever X  has a density bounded by B  <  00. Estimate the smallest such N. 

PROBLEM  28.8.  Assume that X has a bounded density on [0,  1]2, and that lJ is monotonically 
increasing in both coordinates. (This is a reasonable assumption in many applications.) Then 
the set {x  : g*(x) = O}  is a monotone layer. Consider the following classification rule: take 
a k  x  k  grid in  (0,  1]2,  and minimize the  empirical error over all  classifiers ¢  such that 
{x  :  ¢(x)  = O}  is  a  monotone  layer,  and  it is  a  union  of cells  in  the  k  x  k  grid.  What 
is  the  optimal choice of k? Obtain an upper bound for  L(gn) - L *.  Compare your result 
with that obtained for empirical error minimization over the class of all monotone layers in 
Section 13.4. HINT:  Count the number of different classifiers in the class. Use Hoeffding's 
inequality and the union-of-events bound for the estimation error. Bound the approximation 
error using the bounded density assumption. 

PROBLEM 28.9.  Apply  Problem 6.9  to  extend the  consistency  result in Theorem 28.1  as 
follows.  Let F  be a totally bounded class  of functions  l}'  :  R/  ---+  [0, 1]  such that J..({x  : 

lJl(X)  = 1/2}) = ° for each  lJl  E  F  (J..  is  the Lebesgue measure on  R d).  Show that there 

is  a  sequence  {En  >  o}  and  a  sequence  of skeletons  FEn  such that if l}n  is  the  skeleton 
estimate drawn from FEll'  then L(lJ,J  ---+  inf1JIEF  L(l}') with probability one, whenever  X 
has  a density.  In particular,  L(lJn)  ---+  L * with probability one if the Bayes rule takes the 
I/2} for some l}'  E  F. Note: the true regression function l} is not required 
form g*(x) =  I{1)I(x» 
to be in F. 

29 
Uniform Laws of Large Numbers 

29.1  Minimizing the Empirical Squared Error 

In  Chapter 28  the  data  Dn  were used to  select a function  1]n  from  a  class  F  of 
candidate regression functions 1]'  :  nd  -7  [0,  1]. The corresponding classification 
rule gn is I{1Jn>1/2}' Selecting 1]n  was done in two steps: a skeleton-an E-covering(cid:173)
of F  was formed, and the empirical error count was minimized over the skeleton. 
This  method  is  computationally  cumbersome.  It is  tempting  to use  some  other 
empirical  quantity  to  select  a classifier.  Perhaps  the  most popular among  these 
measures is the empirical squared error: 

Assume now that the function 1]n  is selected by minimizing the empirical squared 
error over F, that is, 

As always, we are interested in the error probability, 

of the resulting classifier. If the true regression function 1] (x ) = P{ Y = 11 X = x} is 
not in the class F, then it is easy to see that empirical squared error minimization 
may fail miserably (see Problem 29.1). However, if 1]  E  F, then for every 1]'  E  F 

490 

29. \ Uniform Laws of Large Numbers 

we have 

L(r!') - L * 

:::  2JE {(l}/(X) -
=  2JE {(l}/(X) - y)2}  - E {(l}(X) - y)2} 

l}(X»2} 

(by Corollary 6.2) 

=  2  E {(l}/(X) - Y)2}  -

!nf E {O](X) - y)2}, 
I)EF 

where the two equalities follow from the fact that l}(X) = E{YIX}. Thus, we have 

L(l}n) - L * 

<  2  E {(l}n(X) - Y)2IDn}  -

inf E {(l}/(X) - Y)2} 
I)'EF 

by an argument as  in the proof of Lemma 8.2.  Thus, the method is  consistent if 
the  supremum above converges  to  zero.  If we  define  Zi  =  (Xi, Yd  and  I(Zd = 
(l}/(Xi) - Yd2, then we see that we need only to bound 

where F  is a class of bounded functions. In the next four sections we develop upper 
bounds for such uniform deviations of averages from their expectations. Then we 
apply  these  techniques  to  establish  consistency  of generalized linear classifiers 
obtained by minimization of the empirical squared error. 

29.2  Uniform Deviations of Averages from 

Expectations 

Let F  be a class of real-valued functions defined on n d , and let Z 1,  ... ,  Zn be i.i.d. 
nd-valued random variables. We assume that for each I  E  F, 0 :::  I(x) :::  M for 
all x  E nd and some M  <  00. By Hoeffding's inequality, 

for  any I  E  F. However,  it is  much less  trivial  to  obtain information about the 
probabilities 

p  {sup I~ t I(ZJ - E{/(Zl)}1  >  E}  . 

IEF  n  i=l 

29.2 Uniform Deviations of Averages from Expectations 

491 

Vapnik and Chervonenkis (1981) were the first to obtain bounds for the probability 
above. For example, the following simple observation makes Theorems 12.5, 12.8, 
and 12.10 easy to apply in the new situation: 

Lemma 29.1. 

sup  - L f(Zi) - E{f(Z)}  .-:::  M 

I 

1  n 
fE:F  n  i=l 
1

- L IU(z;»t}  - P{f(Z) >  t}  . 
I 
11 

sup 

n 
fE:F,t>O  n  i==l 

PROOF.  Exploiting  the  identity  fooo P{X  >  t }dt  =  EX for  nonnegative  random 
variables, we have 

fE:F  n  i==l 

sup I ~ t  f(Zi) - E{f(Z)}1 
=  sup I [00 (~ t IU(z;»t}  - P{f(Z) >  t}) dtl 
fE:F  10 
sup  I~ t IU(Z;»t}  - P{f(Z) >  tll·  0 

<  M 

n  i==l 

fE:F,t>O  n  i=l 

For example, from Theorem 12.5 and Lemma 29.1  we get 

COROLLARY 29.1.  Define the collection of sets 

F = {A f,t:  f  E  F, t  E  [0, M]}  , 

where for every  f  E  F  and t  E  [0, M] the set A f,t  E  nd  is defined as 

A f,t  =  {z  :  fez)  >  t}. 

Then 

EXAMPLE.  Consider the empirical  squared error minimization problem  sketched 
in the previous section. Let F  be the class of monotone increasing functions  1]'  : 
R  -+  [0,  1],  and  let  1]n  be  the  function  selected by  minimizing  the  empirical 
squared error.  By (29.1),  if 1](x)  =  pry =  llX =  x} is also monotone increasing, 
then 

492 

29.  Uniform Laws of Large Numbers 

If T contains all subsets of R  x  {O,  I}  of the form 

A 1)',t  =  {(x, y)  : (ry'(x)  - y)2  >  t} , 

ry'  E  F, t E  [0,  1], 

then it is easy to see that its n-th shatter coefficient satisfies seT, n)  ::;  (n12 + 1)2. 
Thus, Corollary 29.1 can be applied, and the empirical squared error minimization 
is consistent.  0 

In many cases, Corollary 29.1 does not provide the best possible bound. To state 
a  similar,  but sometimes  more useful result,  we  introduce  II-covering  numbers. 
The notion is very  similar to that of covering numbers discussed in Chapter 28. 
The main difference is  that here the balls  are  defined in terms  of an  II -distance, 
rather than the supremum norm. 

DEFINITION 29.1.  Let  A  be  a  bounded subset of Rd.  For  every  E  >  0,  the  h(cid:173)
covering number, denoted by N(E, A), is defined as the cardinality of the smallest 
finite  set in  Rd  such  that for every  Z  E  A  there  is  a point t  E  Rd  in  the finite 
till  <  E.  (lIxllI  =  L~=l IxU)1  denotes  the  II-norm of the 
set such that (lld)llz  -
vector x  = (x(l), ... , xed)) in Rd.) In other words, N(E, A) is the smallest number 
of II-balls of radius Ed,  whose union contains A.  logN(E, A) is often called the 
metric entropy of A. 

We  will  mainly  be  interested  in  covering  numbers  of special  sets.  Let zl  = 

(Zl,  ... , Zn) be n fixed points in R d ,  and define the following set: 

The h -covering number of F(zl) is N(E, F(zl))' 

If Z?  = (Zl, ... , Zn) is a sequence ofi.i.d. random variables, thenN(E, FCZ!)) 

is a random variable, whose expected value plays a central role in our problem: 

Theorem 29.1.  (POLLARD (1984)).  For any nand E  >  0, 

The proof of the theorem is given in Section 29.4. 

REMARK.  Theorem 29.1  is a generalization of the basic Vapnik -Chervonenkis in(cid:173)
equality.  To  see  this,  define  loo-covering  numbers  based on  the maximum norm 
(Vapnik  and  Chervonenkis  (1981)):  Noo(E,  A) is  the  cardinality of the  smallest 
finite  set in Rd such that for every Z  E  A  there is  a point t  E  Rd in the set such 
that maXI<i<d IZ(i) 
t(i)1  <  E.  If the functions  in F  are indicators of sets from a 
class A  of s~bsets of R d, then it is easy to see that for every E E  (0,  1/2), 

29.3 Empirical Squared Error Minimization 

493  . 

where  N A(Zl, ... , Zn)  is  the  combinatorial  quantity  that  was  used in Definition 
12.1  of shatter coefficients. Since 

Theorem 29.1  remains true with loo-covering numbers,  therefore, it is  a general(cid:173)
ization of Theorem 12.5. To see this, notice that if F  contains indicators of sets of 
the class A, then 

sup I~ t f(Zi) - E{f(Zl)}1  =  sup I~ t I{ZiEA}  - P{ZI  E A}I·  0 

AEA  n  i=l 

IEF  n  i=l 

For inequalities sharper and more general than Theorem 29.1 we refer to Vapnik 
(1982),  Pollard  (1984;  1986),  Haussler (1992),  and Anthony  and Shawe-Taylor 
(1990). 

29.3  Empirical Squared Error Minimization 

We  return  to  the  minimization  of the  empirical  squared error.  Let F  be  a  class 
of functions  r!,  : nd  -+  [0, 1],  containing  the true  a posteriori function  'Y}.  The 
empirical squared error 

is  minimized over 'Y}'  E  F, to  obtain the  estimate  'Y}n.  The next result shows that 
empirical squared error minimization is consistent under general conditions. Ob(cid:173)
serve that these are the same conditions that we assumed in Theorem 28.1 to prove 
consistency of skeleton estimates. 

Theorem 29.2.  Assume that F  is  a totally bounded class of functions.  (For  the 
definition  see  Chapter  28.) If 'Y}  E  F,  then  the  classification  rule  obtained by 
minimizing the empirical squared error over F  is strongly consistent,  that is, 

lim  L(17I1)  = L *  with probability one. 
n--+oo 

PROOF.  Recall that by (29.1), 

We apply Theorem 29.1 to show that for every E  >  0, the probability on the right(cid:173)
hand  side  converges  to  zero  exponentially  as  n  -+  00.  To  this  end,  we  need  to 
find a suitable upper bound on E{N(E, J(Z7))}, where J  is the class of functions 

494 

29.  Uniform Laws of Large Numbers 

I'(x, Y)  = (7J'(x)- y? froEU nd x {O,  I} to [0,1], where 7J'  E  F, and Zi  = (Xi, Yi). 
Observe that for any I', I  E  .:J, 

<  2 sup 17J'(x)  - ~(x)l. 

x 

This inequality implies that for every f  >  0, 

E{N(f, .:J(Z~»} ::s  ~/2' 

where ~ is the covering number of F  defined in Chapter 28. By the assumption 
of total boundedness, for every E  >  0, NE/2  <  00. Since ~/2 does not depend on 
n, the theorem is proved.  0 

REMARK. The nonasymptotic exponential nature of the inequality in Theorem 29.1 
makes it possible to  obtain upper bounds for the rate of convergence of L(7Jn)  to 
L * in terms of the covering numbers ~ of the class F. However, since we started 
our analysis by the loose inequality  L(7J')  - L *  ::s  2)E {(7J'(X)  - 7J(X))2},  the 
resulting rates are likely to be suboptimal (see Theorem 6.5). Also, the inequality 
of Theorem 29.1 may be loose in this case. In a somewhat different setup, Barron 
(1991) developed a proof method based on Bernstein's inequality that is useful for 
obtaining tighter upper bounds for L(7Jn)  - L * in certain cases.  0 

29.4  Proof of Theorem 29.1 

The main tricks in the proof resemble those of Theorem  12.5. We can show that 
for nf 2  ~ 2M2

, 

P  {sup j~ t 

fEY::  n  i=l 

I(ZJ - E{I(ZI)}j  >  f}  ::s  4P {sup j~ taiI(Zi)j >  :.}, 

fEY::  n  i=l 

4 

where  0'1,  ... , an  are i.i.d.  {-I, l}-valued random variables,  independent of the 
Zi'S,  with  P{ai  =  I}  =  P{ai  =  -I}  =  1/2.  The  only  minor  difference  with 
Theorem  12.5  appears  when Chebyshev's inequality is  applied.  We  use the fact 
that by boundedness, Var(I(ZI»  ::s  M2/4 for every I  E  F. 

Now,  take  a  minimal  f/8-covering  of F(Z~), that  is,  M  = N(f/8, F(Z'!») 
functions gl, ... , gM  such that for every I  E  F there is a g*  E  {gl, ... , gM} with 

1  n 
;; ~ II(Zi) - g*(Zi)1  ::s  "8. 

f 

29.4 Proof of Theorem 29.1 

495" 

For any function  f, we have 

and thus 

I~ ta;JCZi)1  ~  I~ taig'CZi)H~ t<Y;(fCZi) ~ g'CZi»1 
1 n i l  n 
- L O'ig*(Zi)  + - L If(Zi) - g*(Zi)l· 
n  i=l 
n  i=l 

S 

1

fEF  n  i=l 

As (lIn) L7=1  Ig*(Zi)  - !(Zi)1  S  E/8, 
P {sup I~ t  O'i!CZi)1  >  :.1  Zl, ... , Zn} 
S  P  { sup I ~ t  O'ig*(Zi)1  + ~ t 
S  P  {max I~ to'igj(ZJI  >  :.1  Zl, ... , Zn}. 

fEF  n  i=l 

n  i=l 

4 

8 

g; 

n  i=l 

IfCZJ - g*(Zi)1  >  :.1  Zl, ... , Zn} 

4 

Now  that we have been able to  convert the  "sup" into a  "max," we can use the 
union bound: 

We need only find a uniform bound for the probability following the "max." This, 
however, is easy, since after conditioning on Zl, ... , Zn,  L~1=1 O'ig/Zi) is the sum 
of independent bounded random variables whose expected value is zero. Therefore, 
Hoeffding's inequality gives 

1 ~  I 
;; f:: O'igj(Zi)  >  '8  Zl, ... , Zn  S  2e 

E  I 

} 

-nE

2

/(128M

2

) 

. 

P 

{ 

1

In summary, 

496 

29.  Uniform Laws of Large Numbers 

~  2E {N (i, F(Zn) } e-nE2

/(l28M

2

). 

The theorem is proved.  0 

Properties of N(E, F(Z7)) will be studied in the next section. 

29.5  Covering Numbers and Shatter Coefficients 

In this section we study covering numbers, and relate them to shatter coefficients 
of certain classes of sets. As in Chapter 28, we introduce II-packing numbers. Let 
Fbe a class of functions on n d
, taking their values in [0, MJ. Let /.1  be an arbitrary 
probability measure on nd. Let gl, ... , gm  be a finite collection of functions from 
F  with the property that for any two of them 

The largest m  for which such a collection exists is  called the packing number of 
F  (relative to /.1),  and is denoted by M(E, F). If t1 places probability lin on each 
of Zl, ... , Zn,  then  by  definition  M(E, F)  =  M(E, F(ZI{)),  and it is  easy  to  see 
(Problem 28.3) that 

M(2E, F(zID)  ~ N(E, F(z~)) ~ M(E, F(z7)). 

An important feature of a class of functions F  is the vc dimension V.r+  of 

F+ = {{ (x, t) : t  ~ f (x )} ; f  E  F} . 

This is clarified by the following theorem, which is a slight refinement of a result by 
Pollard (1984), which is based on Dudley's (1978) work. It connects the packing 
number of F  with the  shatter coefficients of F+. See also Haussler (1992) for a 
somewhat different argument. 
Theorem 29.3.  Let F  be  a  class  of [0,  M]-valued functions  on nd.  For  every 
E  > ° and probability measure j.L, 
eEM2(E, F)l 

M(E, F) ~ s (F+, k), 

where k =  -log 

fM 

E 

. 

2M 

PROOF.  Let {gl, g2,  ... ,gm} be an arbitrary E-packing of F  of size m  ~ M(E, F). 
The proof is  in the spirit of the probabilistic method of combinatorics (see, e.g., 
Spencer (1987)). To prove the inequality, we create k random points on nd x [0, M] 
in  the  following  way,  where  k  is  a  positive  integer  specified  later.  We  gener(cid:173)
ate k  independent random variables  Sl,  ... , Sk  on nd  with common distribution 

29.5 Covering Numbers and Shatter Coefficients 

497 . 

jk,  and  independently  of this,  we  generate  another  k  independent random vari(cid:173)
ables  Tl, .. "  Tb  uniformly  distributed  on  [0, M].  This  yields  k  random  pairs 
(S1,  Tl), ... , (Sk,  Tk). For any two functions gi and gj in an E-packing, the proba(cid:173)
bility that the sets Gi = {(x, t) : t  :::gi(X)} and G j  = {(x, t) : t  ::::  gj(x)} pick the 
same points from our random set of k points is bounded as follows: 

P { G i  and G j  pick the same points} 

k 

=  rI (I - P { (Sf,  Tr)  E  GiL. G j  }) 

1=1 

= 

(1  - E { P { (S 1, T1)  E  GiL. G j  } I S d / 
(1 - ~E{lg;(SJ) - gj(SJlIl)' 

(1  - E/Mi 

< 
<  e-kE / M , 

where we used the definition of the functions gl, , .. , gm' Observe that the expected 
number of pairs (gi, gj) of these functions, such that the corresponding sets G i  = 
{(x,t): t::::  gi(x)}andG j  =  {(x,t): t::::  gj(x)} pick the same points, is bounded 
by 

E {I {(gi' gj); G i  and G j  pick the same points} I} 

~  (~)P{ G;  and  G j  pick the same point')  <S  (~)e-k,/M 

Since  for  k  randomly  chosen  points  the  average  number  of pairs  that pick the 
same points is  bounded by G)e-kE/ M,  there exist k points in Rd  x  [0,  M], such 
that the number of pairs (gi, gj) that pick the same points is actually bounded by 
C;)e-kE/ M .  For each such pair we can add one more point in Rd  x  [0, M] such 
that the point is  contained in  GiL. G j. Thus,  we have  obtained a  set of no more 
than k + (~)e-kE/M points such that the sets  G I ,  ... ,  G m  pick different subsets of 
it. Since k was arbitrary, we can choose it to minimize this expression. This yields 
l ~ log (eE G) / M) J points, so the shatter coefficient of F+ corresponding to this 
number must be greater than m, which proves the statement.  0 

The meaning of Theorem 29.3 is best seen from the following simple corollary: 

COROLLARY 29.2.  Let F  be  a class 0/[0, M]-valued/unctions on Rd.  For every 
E >  0 and probability measure  jk, 

M(E, F)::::  -E- 1og  -E-

(

4eM 

2eM)VF+ 

498 

29.  Uniform Laws of Large Numbers 

PROOF.  Recall that Theorem 13.2 implies 

The inequality  follows  from  Theorem 29.3  by  straightforward calculation.  The 
details are left as an exercise (Problem 29.2).  D 

Recently Haussler (1991)  was  able to  get rid of the "log" factor in the above 

upper bound. He proved that if E  = kin for an integer k, then 

M(E, F) :s  e(d + 1) 

2  )  V.F+ 
e 
E

(

The quantity  V.:F+  is  sometimes  called the pseudo dimension  of F  (see Problem 
29.3).  It follows  immediately from  Theorem  13.9  that  if F  is  a  linear space of 
functions of dimension r, then its pseudo dimension is at most r + 1.  A few more 
properties are worth mentioning: 

Theorem 29.4.  (WENOCUR AND DUDLEY, (1981)). Let g  : R d  ~ R  be an arbitrary 
junction,  and consider the class ojjunctions 9 = {g + j; j  E  F}.  Then 

Vg+  = VF+. 

PROOF.  If the points (Sl' tl), ... , (Sk,  tk)  E  Rd  x  R  are shattered by F+, then the 
points (Sl' tl + g(Sl)), ... , (Sk,  tk  + g(Sk)) are shattered by g+. This proves 

The proof of the other inequality is similar.  D 

Theorem 29.5.  (NOLAN AND POLLARD (1987); DUDLEY,  (1987)). Let g: [0,  M]  ~ 
R  be  a fixed nondecreasing junction,  and define  the  class 9  =  {g  0  j; j  E  F}. 
Then 

Vg+  :s  Vp. 

PROOF.  Assume that n  :s  Vg+,  and let the functions  fr,  ... ,12"  E  F  be such that 
the binary vector 

takes all 2n values if j  = 1, ... , 2n.  For all  1 ::;  i  ::;  n define the numbers 

(I{g(/j(sl»c::td'  ... ,  I{g(/j(sn»?:.tn }) 

and 

29.5 Covering Numbers and Shatter Coefficients 

499 

By the monotonicity of g, Ui  >  li. Then the binary vector 

takes the same value as 

for every j  ::::  2n.  Therefore, the pairs 

( ~)  (un +In) 

' ... ,  Sn, 

Sl, 

2 

2 

are shattered by f+, which proves the theorem.  0 

Next we present a few  results about covering numbers of classes of functions 
whose  members  are  sums  or products  of functions  from  other classes.  Similar 
results can be found in Nobel (1992), Nolan and Pollard (1987), and Pollard (1990). 

Theorem 29.6.  Let fl, ... , fk be classes of real functions on Rd. For n arbitrary, 
fixed points z'{  = (Zl,  ... , Zn)  in R d, define the sets fl (z7),  ... ,  fk(z7) in Rn by 

j  = 1,  ... , k.  Also,  introduce 

f(Z7) = {(f(zl), ... , f(zn»; f  E  f} 

for the class offunctions 

Thenfor every E  >  0 and z7 

N(E, f(z7))  :::: IT N(E/ k,  fj(z7))· 

k 

j=l 

PROOF. Let Sl, ... , Sk  C  Rn be minimal E /  k-coverings of fl (z'j),  ... , fk(Z'j), re(cid:173)
spectively. This implies that for any /j  E  fj there is a vector s j  = (sY),  ... , sjn)  E 
Sj  such that 

for every  j  = 1,  ... , k.  Moreover,  ISj I = N(E / k,  fj(Z'j). We show that 

S={Sl+ ... +Sk;Sj  ESj,j=I, ... ,k} 

500 

29.  Uniform Laws of Large Numbers 

is an E-covering of :F(z7). This follows immediately from the triangle inequality, 
since for any  fl,  ... , !k there is SI,  ... , Sk  such that 

< 

1 ~ I 
- L....- h(zl) - sl  + ... + - L....- !k(Zi) - Sk 
n  i=1 

1 ~ I 
n  i=1 

Ci) I 

Ci) I 

<  k-.  0 

E 
k 

Theorem 29.7.  (POLLARD  (1990)).  Let:F and 9 be classes of real functions on 
R d ,  bounded by Ml  and M 2,  respectively.  (That is,  e.g.,  If(x)1  S  Ml for every 
x  E  Rd and f  E  :F.) Forarbitraryjixedpoints z7  = (ZI,  ... , Zn)  in Rd dejine the 
sets :F(z7) and g(z~) in RH  as in Theorem 29.6.  Introduce 

for the class of functions 

.:J = {fg; f  E  :F,  g  E  g}. 

Then for every E  >  0 and z~ 

PROOF.  Let S  C  [-MI, Md n  be an E/(2M2)-covering of :F(z7),  that is,  for  any 
f  E  :F there is a vector s  = (s(1),  ... , sen»)  E  S such that 

n "\:' I 
I
- L....-
n  i=I 

<  -

E 

. 

2M2 

f(Zt)  - s 

Ci) I 

It is easy to see that S can be chosen such that lSI  = N(E/(2M2), :F(z'D). Similarly, 
let T  C  [-M2, M 2] be  an  E/(2Md-covering of g(z7)  with  ITI  =  N(E/(2Ml ), 
g(z'D) such that for any g  E  9 there is  at = (t(1),  ... , ten»~ E  T  with 

We show that the set 

U  = {st; s  E  S, t  E  T} 

is an E-covering of .:J(zV. Let f  E  :F and g  E  9 be arbitrary and s  E  Sand t  E  T 
the corresponding vectors such that 

Then 

29.6 Generalized Linear Classification 

501 

29.6  Generalized Linear Classification 

In this section we use the uniform laws of large numbers discussed in this chapter 
to  prove  that  squared error minimization  over  an  appropriately  chosen  class  of 
generalized linear classifiers yields a universally consistent rule. Consider the class 
C(kn )  of generalized linear classifiers,  whose  members  are  functions  ¢  : nd  ---7>(cid:173)
{O,  1}  of the form 

where 0/1, ... , 1fkn  are fixed  basis functions,  and the coefficients aI, ... ,akn  are 
arbitrary real numbers. The training sequence Dn  is used to determine the coeffi(cid:173)
cients ai . In Chapter 17 we studied the behavior of the classifier whose coefficients 
are picked to minimize the empirical error probability 

Instead of minimizing the empirical error probability Ln (¢), several authors sug(cid:173)
gested minimizing the empirical squared error 

(see,  e.g.,  Duda and  Hart (1973),  Vapnik  (1982)).  This  is  rather dangerous.  For 
example, for k = 1 and d = 1 it is easy to find a distribution such that the error prob(cid:173)
ability of the linear classifier that minimizes the empirical squared error converges 
to  1 - E,  while the error probability of the best linear classifier is  E,  where  E  is 
an arbitrarily small positive number (Theorem 4.7). Clearly, similar examples can 

502 

29.  Uniform Laws of Large Numbers 

be found for any fixed k. This demonstrates powerfully the danger of minimizing 
squared error instead of error count. Minimizing the latter yields a classifier whose 
average error probability is  always  within  0  ( Jlog n / n)  of the optimum in the 
class, for fixed k. We note here that in some special cases minimization of the two 
types of error are equivalent (see Problem 29.5). Interestingly though, if kn  -+  00 
as  n  increases, we can obtain universal consistency by minimizing the empirical 
squared error. 
Theorem 29.8.  Let Vrl , '1f2,  .. , be a sequence of bounded functions with I Vrj(x)1  :s 
1 such that the set of all finite linear combinations of the Vr j 's 

Q {t, aj1fr/x);aj, a2,· .. E R} 

is  dense  in  L 2(/.t)  on  all  balls of the form  {x  :  Ilx II  :s  M} for  any probability 
measure /.t.  Let the coefficients at, ... , at minimize the empirical squared error 

under the constraint L~'~l la j I :s  bit>  bn  2::  1.  Define the generalized linear clas(cid:173)
sifier gn  by 

If kn and bn satisfy 

then E{ L(gn)}  -+  L * for all distributions of (X , Y), that is, the rule gn  is universally 
consistent.  If we  assume  additionally  that  b~ log n  =  o(n),  then  gn  is  strongly 
universally consistent. 

PROOF. Let 0 >  0 be arbitrary. Then there exists a constant M  such that P{IIXII  > 
M}  <  o.  Thus, 

L(gn) - L * :s  0 + P{gn(X) i  Y,  IIXII  :s  MIDn}  - P{g*(X) i  Y,  IIXII  :s  M}. 

It  suffices  to  show  that  P{gn(X)  i  Y,  IIXII  :s  MIDn}  - P{g*(X)  i  Y,  IIXII  ::: 
M}  -+  0 in the required sense for every M  >  O.  Introduce the notation  fn*(x) = 
L~:l ajVrj(x). By Corollary 6.2, we see that 

P{gn(X) i  Y, IIXII  :s  MIDn}  - P{g*(X) i  Y,  IIXII  :s  M} 

< 

( 
J11xll-::::M 

(f,:(x) -

(21J(x)  - 1))2/.t(dx). 

29.6 Generalized Linear Classification 

503· 

We prove that the right-hand side converges  to  zero in probability.  Observe that 
since E{2Y -

llX = x} = 217(X)  - 1, for any function hex), 

(h(x) -

(217(x)  - 1»2 

=  E{(h(X) -

(2Y  - 1»21X = x} - E{(2Y - 1)  - (217(X)  - 1)2IX = x} 

(see Chapter 2), therefore, denoting the class of functions over which we minimize 
by 

we have 

[ 
JIIXIIS:M 

(fn*(x) -

(217(X)  - 1»)2 fJ.,(dx) 

=  E { (fn*(X) -

(2Y  - 1»)2 1{IIXIIS:M} I Dn} 

- E {((2Y - 1)  - (217(X)  - 1»2 1{IIXIIS:M}} 
(E { (t~(X) - (2Y  - 1»)2 1{IIXIIS:M} I Dn} 

= 

-

inf  E {(f(X) -
fE:!;, 

(2Y  - 1»2 1{IIXIIS:M}}) 

+  inf  E {(f(X) -

fE:Fn 

(2Y - 1»2 I{jlx II s:M} } 

- E {((2Y - 1) - (217(X)  - 1»2 1{IIXIIS:M}}  . 

The last two terms may be combined to yield 

inf  [ 
fE:Fn  JIIXIIS:M 

(f(x) -

(217(X)  - 1)i fJ.,(dx), 

which converges to zero by the denseness assumption. To prove that the first term 
converges  to  zero  in  probability,  observe  that  we  may  assume  without  loss  of 
generality that P{ II X II  >  M} = O.  As in the proof of Lemma 8.2, it is easy to show 
that 

E { (t:(X) - (2Y  - 1))2 1{lIXIIS:M) I Dn} 

-

inf  E {(f(X) -
fE:!;, 

(2Y  - 1)2 1{IIXIIS:M}} 

=  E {(fn*(X) -

(2Y  _1))21 Dn}  -

inf  E {(f(X) -
fE:!;, 

(2Y  _l)2} 

:s  2 ;~~,  ;; ~ (f(Xi )  -

1  n 

(2Yi  - 1»)2  - E {(f(X) -

(2Y - 1»2} 

I 

1

2 sup I~ th(Xi ,  Yi )  - E{h(X, y)}I, 

hEJ  n  i=l 

504 

29.  Uniform Laws of Large Numbers 

where the class of functions J  is defined by 

J  =  {hex, y) = (f(x) -

(2y  - 1))2; f  E  Fn}  . 

Observe that since 12y  -

11  =  1 and I 1/I;(x) I ::::  1, we have 

Therefore, Theorem 29.1  asserts that 

P {E { (f,;(X) - (2Y - 1))21  Dn} 

-

inf  E  {(f(X) - (2Y  - 1))2}  >  E} 

fE:Fn 

::::  p  {sup I~ th(Xi ,  Yi )  - E{h(X, y)}1  >  E/2} 

hEJ  n  i==l 

where  Z7  =  (Xl, Y1),.·., (Xn, Yn).  Next,  for  fixed  z7,  we  estimate  the  cover(cid:173)
ing number N (E 116, J (z7) ).  For arbitrary  fl' 12  E  :01'  consider the  functions 
hI(x, y) = (fl(X) -
(2y  - 1))2. Then for any 
probability measure v on nd  x  {O,  I}, 

(2y  - 1)? and h 2(x, y) = (h(x) -

f Ih 1(x,  y) - h2(X, y)lv(d(x, y) 
=  f I(fl(X) -
::::  f 21!I(x) - h(x)l(bn  + l)v(d(x, y)) 
::::  4bn f I fi (x) - hex )1/J,(dx), 

(2y - 1))2  - (h(x) -

(2y - 1))21  v(d(x, y)) 

where  fJ.,  is  the  marginal  measure  for  v on nd.  Thus,  for any  z7  =  (Xl, yd, ... , 
(xn, Yn)  and E, 

N(E, J(Zl»  :5:  N (4:n ' F,,(X~») . 

Therefore, it suffices to estimate the covering number corresponding to Fn.  Since 
::::  kn + 1 (Theorem 13.9). 
Fn  is a subset of a linear space of functions, we have V.1;;-
By Corollary 29.2, 

N 

( 

E 
4bn

:F,  (  ll) 
) 
n  xl 

' 

8  b 
e  n  1 
::::  E/(4bn

4  b 
e  n 
)  og  E/(4bn

( 

)kn+l 

) 

(32  b2 )2(kll+l) 

e  11 
::::  -E -

Problems and Exercises 

505 

Summarizing, we have 

P {E { (fn*(X)  - (2Y  - 1»)21  Dn} 

-

inf  E  {(f(X) - (2Y - 1»2}  >  E} 

fEFI1 

which goes to zero if knb~ log(bn)/n  ~ 0. The proof of the theorem is completed. 
It is easy to see that if we assume additionally that b~ log n / n  ~ 0, then strong 
universal  consistency  follows  by  applying  the  Borel-Cantelli lemma to  the  last 
probability.  0 

REMARK. Minimization of the squared error is attractive because there are efficient 
algorithms to find  the minimizing coefficients,  while minimizing the number of 
errors committed on the training sequence is computationally more difficult. If the 
dimension k  of the generalized linear classifier is  fixed,  then stochastic approxi(cid:173)
mation asymptotically provides the minimizing coefficients. For more information 
about this we refer to Robbins and Monro (1951), Kiefer and Wolfowitz (1952), 
Dvoretzky  (1956),  Fabian  (1971),  Tsypkin  (1971),  Nevelson  and  Khasminskii 
(1973), Kushner (1984), Ruppert (1991), and Ljung, Pflug, and Walk (1992). For 
example, Gyorfi (1984) proved that if (Ul ,  VI), (U2 ,  V2 ),  ... form a stationary and 
ergodic sequence, in which each pair is distributed as the bounded random variable 
pair (U,  V)  E  nk  x  n, and the vector of coefficients a  = (aI, ... , ak) minimizes 

and a(O)  E  nk is  arbitrary, then the sequence of coefficient vectors defined by 

a(n+1)  - a(n)  __  1_ (a(n)T U 

-

n + 1 

- v.  ) U 

n+1 

n+1 

n+1 

satisfies 

lim  R(a(n»  = R(a)  a.s.  0 
n-+oo 

Problems and Exercises 

PROBLEM 29.1.  Find a class :F containing two functions  1]1,  1]2  : n  -+  [0,  1]  and a distri(cid:173)
bution of (X, Y) such that min(L(1]I),  L(1]2)) = L *,  but as n  -+  00, the probability 

converges to one, where 1]n  is selected from :F by minimizing the empirical squared error. 

506 

29.  Uniform Laws of Large Numbers 

PROBLEM  29.2.  Prove Corollary 29.2. 
PROBLEM 29.3.  Let Fbe a class of functions on n d
, taking their values in [0,  M]. Haussler 
(1992) defines the pseudo dimension of F  as  the largest integer n for which there exist n 
points in n d, Zj, ... ,  Zn, and a vector v  = (v(l),  ... ,  v(n))  E  nn such that the binary n-vector 

takes a1l2n  possible values as  f  ranges through F. Prove that the pseudo dimension of F 
equals the quantity V F+  defined in the text. 

PROBLEM 29.4.  CONSISTENCY OF CLUSTERING.  Let X, XI, ... , Xn  be i.i.d. random variables 
in nd, and assume that there is a number 0  <  M  <  00 such that P{X  E  [-M, M]d} = l. 
Take  the empirically optimal clustering of Xl, ... , X n ,  that is,  find  the points aI, ... , ak 
that minimize the empirical squared error: 

The error of the clustering is defined by the mean squared error 

Prove that if aj, ... , ak  denote the empirically optimal cluster centers, then 

and that for every E  >  0 

P 

{  S P 

u 

bJ , ... ,bk ERd 

Ie  (b 
II  1,···,  k  -e  I,··" 

b  ) 

(b 

b  )1  >  E}  <_  4e8n2k(d+l)e-nE2/(32M4). 

k 

Conclude that the error of the empirically optimal clustering converges to that of the truly 
optimal one as  n  -+  00.  (Pollard (1981;  1982), Linder, Lugosi,  and Zeger (1994»).  HINT: 
For the first part proceed as in the proof of Lemma 8.2. For the second part use the technique 
shown in Corollary 29.1. To compute the vc dimension, exploit Corollary 13.2. 

PROBLEM 29.5.  Let V!'1,  ... ,  V!'k  be indicator functions of cells of a k-way partition of nd. 
Consider generalized linear classifiers  based on these functions.  Show  that the  classifier 
obtained by minimizing the number of errors made on the training sequence is the same as 
for the classifier obtained by minimizing the empirical squared error. Point out that this is 
just the histogram classifier based on the partition defined by the  V!'i'S  (Csibi (1975». 

30 
Neural Networks 

30.1  Multilayer Perceptrons 

The linear discriminant or perceptron (see Chapter 4) makes a decision 

</J(x) = {O 

if  1/r(x~ :::  1/2 

1  otherwlse, 

based upon a linear combination 1/r(x) of the inputs, 

1/r(x) = Co  + L CiX(i)  = Co  + cT X, 

d 

i=l 

(30.1) 

where the Ci'S  are weights,  x  = (x(I), ... , x(d)l, and  C  = (Cl'  ... ,  Cd)T.  This is 
called a neural network without hidden layers (see Figure 4.1). 

In a (feed-forward) neural network with one hidden layer, one takes 

1/r(x) = Co  + L Ci cr (1/ri(X)), 

k 

i=l 

(30.2) 

where the c/s are  as  before,  and each 1/ri  is  of the form given in (30.1):  1/ri(X)  = 
hi + L~=l aijx(j) for some constants hi  and aij. The function cr  is called a sigmoid. 
We define sigmoids to be nondecreasing functions with cr(x)  -+  -1 as x  {..  -00 
and cr(x) -+  1 as x  t  00. Examples include: 

(1)  the threshold sigmoid 

-I 

cr(x) =  1 

{

if x  :::  0 
if x  >  0; 

508 

30.  Neural Networks 

(2)  the standard, or logistic,  sigmoid 

o-(x) = - - -
1+ e- X 

' 

(3)  the arctan sigmoid 

o-(x) =  - arctan(x); 

2 

TC 

(4)  the gaussian sigmoid 

o-(x)  = 2 

1 

__ e- u  /2du  - 1. 

2 

j x 

-oo~ 

OOT 1 

FIGURE 30.1.  A neural network with one hidden layer.  The hidden 
neurons are those within the frame. 

~~~~ 

a(x) = ~ f e-v212 dv - 1 

I-e-x 
a(x) = 1 +e-x 

a(x) = ~ arctan(x) 

-I 

x 

2 

FIGURE 30.2.  The  threshold,  standard,  arctan,  and gaussian sig(cid:173)
moids. 

30.1 Multilayer Perceptrons 

509 

For early discussion of multilayer perceptrons,  see Rosenblatt (1962),  Barron 
(1975), Nilsson (1965), and Minsky and Papert (1969).  Surveys may be found in 
Barron and Barron (1988), Ripley (1993; 1994), Hertz, Krogh, and Palmer (1991), 
and Weiss and Kulikowski (1991). 

In the perceptron with one hidden layer, we say that there are k hidden neurons(cid:173)

the output of the i-th hidden neuron is Ui  = CJ( o/i(X)). Thus, (30.2) may be rewritten 
as 

1jf(x) = Co  + L CiUi, 

k 

i=l 

which is similar in form to (30.1). We may continue this process and create multi(cid:173)
layer feed-forward neural networks. For example, a two-hidden-Iayer perceptron 
uses 

1jf(x) = Co  + L CiZi, 

I 

i=l 

where 

and 

u . = CJ  (b.  + ~ a .. X(i)) 

} 

1 <  J'  <  k 

, 

}  ~}l  ,_   _ 

and  the  dij's,  b/s, and  aj/s are  constants.  The first  hidden  layer has  k  hidden 
neurons, while the second hidden layer has I hidden neurons. 

i=l 

FIGURE 30.3.  Afeed-forward neural network with two hidden lay-
ers. 

510 

30.  Neural Networks 

The  step  from  perceptron  to  a  one-hidden-Iayer neural  network is  nontrivial. 
We know that linear discriminants cannot possibly lead to universally consistent 
rules.  Fortunately,  one-hidden-Iayer neural networks yield universally consistent 
discriminants provided that we  allow  k,  the number of hidden neurons,  to  grow 
unboundedly  with  n.  The interest in  neural  networks  is  undoubtedly  due  to  the 
possibility of implementing them directly via processors and circuits. As the hard(cid:173)
ware is fixed beforehand, one does not have the luxury to let k become a function 
of n,  and thus,  the  claimed universal  consistency is  a moot point.  We  will  deal 
with both fixed  architectures  and variable-sized neural networks.  Because of the 
universal consistency of one-hidden-Iayer neural networks, there is little theoret(cid:173)
ical gain in considering neural networks with more than one hidden layer.  There 
may,  however, be an information-theoretic gain as  the number of hidden neurons 
needed to achieve the same performance may be substantially reduced. In fact, we 
will make a case for two hidden layers, and show that after two hidden layers, little 
is gained for classification. 

For theoretical analysis,  the neural networks are rooted in a classical theorem 
by  Kolmogorov  (1957)  and Lorentz  (1976)  which  states  that every  continuous 
function f  on [0,  l]d can be written as 

where the G ij , s and the Fi ' s are continuous functions whose form depends on f. We 
will see that neural networks approximate any measurable function with arbitrary 
precision, despite the fact that the form of the sigmoids is fixed beforehand. 

As an example, consider d  = 2.  The function x(1) x(2)  is rewritten as 

which is  in  the  desired form.  However,  it is  much  less  obvious  how  one would 
rewrite more general continuous functions. In fact, in neural networks, we approx(cid:173)
imate the Gij's and Fi's by functions of the form a(b+a T x) and allow the number 
of tunable coefficients to  be high enough such that any continuous function may 
be represented-though no  longer rewritten  exactly in  the form  of Kolmogorov 
and Lorentz. We discuss other examples of approximations based upon such rep(cid:173)
resentations in a later section. 

30.2 Arrangements 

511 . 

input 

• 
• 
• 

FIGURE 30.4.  The general Kolmogorov-Lorentz representation of 
a continuous function. 

30.2  Arrangements 
A finite  set  A  of hyperplanes  in n d  partitions the  space into  connected convex 
polyhedral pieces of various dimensions.  Such a partition P =  peA) is called an 
arrangement. An arrangement is called simple if any d hyperplanes of A  have  a 
unique point in common and if d + 1 hyperplanes have no point in common. 

FIGURE 30.5.  An arrangement of five 
lines in the plane. 

FIGURE 30.6.  An  arrangement  clas(cid:173)
sifier. 

A simple arrangement creates polyhedral cells. Interestingly, the number of these 
cells is independent of the actual configuration of the hyperplanes. In particular, 

512 

30.  Neural Networks 

the number of cells is exactly 2k  if d  ::::  k,  and 

where IAI  = k. For a proof of this, see Problem 22.1, or Lemma 1.2 of Edelsbrunner 
(1987). For general arrangements, this is merely an upper bound. 

We may of course use arrangements for designing classifiers. We let gA be the 
natural classifier obtained by taking majority votes over all Yi's for which Xi  is in 
the same cell of the arrangement P  = peA) as x. 

All classifiers discussed in this section possess the property that they are invariant 
under linear transformations and universally consistent (in some cases, we assume 
that X has a density, but that is only done to avoid messy technicalities). 
If we fix  k  and find that A with  IAI  = k  for which the empirical error 

is  minimal,  we  obtain-perhaps at great computational  expense-the empirical 
risk optimized classifier. There is a general theorem for such classifiers-see, for 
example, Corollary 23.2-the conditions of which are as follows: 

(1)  It must  be  possible  to  select  a  given  sequence  of A's  for  which  Ln(gA) 
(the  conditional  probability  of error  with  gA)  tends  to  L * in  probability. 
But if k  -+  00, we may  align the hyperplanes with the axes,  and create a 
cubic histogram, for which, by Theorem 6.2, we have consistency if the grid 
expands to  00  and the cell sizes in the grid shrink to  O.  Thus,  as  k  -+  00, 
this condition holds trivially. 

(2)  The  collection  9  =  {gAl  is  not too  rich,  in  the  sense  that  njlogS(9, n) 
-+  00, where S(9, n) denotes the shatter coefficient of 9, that is, the maximal 
number of ways (Xl,  YI),  ... , (xn, Yn)  can be split by sets of the form 

( U  A  x {O})  U  (  U  A  x {I}) . 

AEP(A) 

A EP(A) 

If IAI  = 1, we know that S(9, n) :::  2(n d  + 1) (see Chapter 13). For IAI  = k, 
a trivial upper bound is 

(2(n d + 1))k . 

The consistency condition is fulfilled if k = o(nj log n). 

We have 

Theorem 30.1.  The  empirical-risk-optimized arrangement classifier based upon 
arrangements with  IAI  :::  k has E{L n }  -+  L * for all distributions if k  -+  00 and 
k  = o(nj log n). 

30.2 Anangements 

513 

Arrangements  can also  be  made from  the  data  at hand in  a  simpler way.  Fix 

k points  Xl, ... , X k  in general position and look at all possible e)  hyperplanes 

you  can form  with  these  points.  These  form  your  collection  A,  which  defines 
your  arrangement.  No optimization of any kind is  performed.  We  take  the  nat(cid:173)
ural  classifier obtained by  a  majority  vote within the  cells  of the partition over 
(Xk+l, Yk+d,  ... , (Xn, Yn). 

FIGURE 30.7.  Arrangement  determined  by 
k  = 4 data points on the plane. 

Here we cannot apply the powerful consistency theorem mentioned above. Also, 
the arrangement is no longer simple. Nevertheless, the partition of space depends 
on the Xi'S only, and thus Theorem 6.1  (together with Lemma 20.1) is useful. The 
rule thus obtained is consistent if diam(A(X))  -+ 0 in probability and the number 
of cells is o(n), where A(X) is the cell to which X belongs in the arrangement. As 
the number of cells is certainly not more than 

d  (k') L· , 

i=O 

l 

where k' = e), we see that the number of cells divided by n tends to zero if 

This puts a severe restriction on the growth of k.  However, it is easy to prove the 
following: 

Lemma 30.1.  If k  -+  00,  then diam(A(X))  -+  0 in probability whenever X  has 
a density. 

PROOF.  As noted in Chapter 20 (see Problem 20.6), the set of all x  for which for 
all  E  >  0,  we have  JL(x  + E Qi)  >  0  for  all  quadrants  Ql, ... ,  Q2d  having  one 
vertex at (0, 0,  ... , 0)  and sides of length one, has JL-measure one. For such x, if 
at  least one of the  X/s (i  ~ k)  falls  in each of the 2d  quadrants  x  + EQi,  then 
diam(A(x))  ~ 2dE  (see Figure 30.8). 

514 

30.  Neural Networks 

FIGURE 30.8.  The  diameter  of the  cell con(cid:173)
taining  x  is  less  than  4E  if there  is  a  data 
point in  each of the four quadrants of size  E 
around x. 

Therefore, for arbitrary E  >  0, 

P{diam(A(x»  >  2dE}  :::;  2d (1 - min  M(X  + EQd)k -+ O. 

I~i~2d 

Thus, by the Lebesgue dominated convergence theorem, 

P{diam(A(X»  >  2dE}  -+ O.  0 

Theorem 30.2.  The arrangement classifier defined above is consistent whenever 
X  has a density and 

The theorem points  out that empirical error minimization over a  finite  set of 
arrangements can also be consistent.  Such a set may be formed as  the collection 
of arrangements  consisting  of hyperplanes  through d  points  of Xl, ... , X k.  As 
nothing new is added here to the discussion, we refer the reader to Problem 30.1. 
So how do we deal with arrangements in a computer? Clearly, to reach a cell, we 
find for each hyperplane A  E  A the side to which x  belongs. If f(x) = aT x  + ao, 
then  f(x)  >  0  in  one  halfplane,  f(x)  =  0  on  the  hyperplane,  and  f(x)  <  0 
in the other halfplane. If A  =  {AI, ... , Ad, the vector (/{Hl(X»O},  ••• ,  I{Hk(x»O}) 
describes  the  cell  to  which  x  belongs,  where  Hi (x)  is  a  linear function  that is 
positive if x  is  on  one  side  of the  hyperplane  Ai,  negative  if x  is  on  the  other 
side  of Ai,  and  0  if x  E  Ai. A  decision  is  thus  reached  in  time  O(kd).  More 
importantly,  the  whole process is  easily parallelizable and  can  be pictured  as  a 
battery of perceptrons. It is easy to see that the classifier depicted in Figure 30.9 
is identical to the arrangement classifier. In neural network terminology,  the first 
hidden layer of neurons corresponds to just k perceptrons (and has k( d + 1) weights 
or parameters, if you wish). The first layer outputs a k-vector of bits that pinpoints 
the precise location of x  in the cells of the  arrangement.  The  second layer only 
assigns a class (decision) to each cell of the arrangement by firing up one neuron. 
It has 2k neurons (for class assignments), but of course, in natural classifiers, these 
neurons do not require training or learning-the majority vote takes care of that. 

30.2 Arrangements 

515 

~ • •  I-~O~orlJl~ 

,-/ 

± 

x 

o or 1 

Oor 1  ± 

FIGURE 30.9.  Arrangement classifier  realized  by  a  two-hidden(cid:173)
layer neural network.  Each of the  2k  cells  in  the  second hidden 
layer peiforms an "and" operation: the output of node "101" is 1 
ifits three inputs are 1,0, and 1,  respectively.  Otherwise its output 
is O.  Thus,  one and only one of the 2k  outputs is 1. 

If a  more  classical  second layer is  needed-without boolean  operations-let 
b = (bI ,  ... , bk )  be the k-vector of bits seen at the output of the first layer. Assign 
a perceptron  in  the  second layer to  each region  of the  arrangement  and  define 
the  output z  E  {-1, 1}  to  be  (j  (L~=l C j b j  - k + 1/2 ), where  c j  E  {-1, 1}  are 
weights.  For each region of the  arrangement,  we have  a description in terms  of 
C = (Cl'  ... , Ck).  The argument of the sigmoid function is  1/2 if 2b j  - 1 = C j  for 
all j  and is negative otherwise. Hence z = 1 if and only if 2b - 1 = c. Assume we 
now take a decision based upon the sign of 

LWZZZ + Wo, 
z 

where the wz's are weights and the zz 's are the outputs of the second hidden layer. 
Assume that we wish to assign class  1 to s regions in the arrangement and class 0 
to  t  other regions.  For a class  1 region l, set Wz  =  1,  and for a class 0 region,  set 
Wz  = -1. Define Wo  = 1 + S  -

t. Then, if Zj  =  1, Zi  =  -1, i =I j, 

L  Wzzz  + Wo  = W j  + Wo  - L  Wi  =  {  ~ 1 

if Wj  =  1 
if Wj  = -1. 

z 

i=/j 

516 

30.  Neural Networks 

...  - ......... -_ ................... -_ ...... ... 

second hidden layer 

Oor 1 

l ....... ................. __  .......... .............. " 

FIGURE 30.10.  The  second  hidden  layer  of a  two-hidden-layer 
neural network with threshold sigmoids in the first layer.  For each 
k-vector of bits  b  = (bt, ... , bk )  at the  output of the first  layer, 
we  may find  a  decision  g(b)  E  {O,  I}.  Now  return  to  the  two(cid:173)
hidden-layer network of Figure  30.9 and assign  the  values g(b) 
to the neurons in the second hidden layer to obtain an equivalent 
network. 

Thus,  every  arrangement classifier corresponds  to  a neural network with two 
hidden layers, and threshold units. The correspondence is also reciprocal. Assume 
someone shows a two-hidden-Iayer neural network with the first hidden layer as 
above-thus,  outputs consist of a  vector of k bits-and with the  second hidden 
layer consisting once again a battery of perceptrons (see Figure 30.10). Whatever 
happens in  the second hidden layer,  the decision is just a function  of the config(cid:173)
uration of k  input bits.  The output of the first hidden layer is constant over each 
region of the arrangement defined by the hyperplanes given by the input weights of 
the units of the first layer. Thus, the neural network classifier with threshold units 
in the first hidden layer is  equivalent to  an  arrangement classifier with the same 
number of hyperplanes as units in the first hidden layer. The equivalence with tree 
classifiers is described in Problem 30.2. 

Of course, equivalence is only valid up to a certain point. If the number of neurons 
in the second layer is small, then neural networks are more restricted. This could be 
an advantage in training. However,  the majority vote in an arrangement classifier 
avoids  training  of the  second layer's  neurons  altogether,  and  offers  at the  same 
time an easier interpretation of the classifier. Conditions on consistency of general 
two-hidden-layer neural networks will be given in Section 30.4. 

30.3 Approximation by Neural Networks 

517 

30.3  Approximation by Neural Networks 

Consider first  the class  C(k)  of classifiers  (30.2) that contains all neural network 
classifiers  with the  threshold  sigmoid and k  hidden nodes  in two  hidden layers. 
The training data Dn are used to select a classifier from C(k). For good performance 
of the selected rule, it is necessary that the best rule in C(k) has probability of error 
close to  L *, that is, that 

inf  L(¢) - L * 
¢EC(k) 

is  small.  We  call  this  quantity  the  approximation  error.  Naturally,  for  fixed  k, 
the approximation error is  positive for  most distributions.  However,  for  large k, 
it is  expected to  be small.  The question is  whether the  last statement is  true for 
all  distributions  of (X, Y).  We  showed  in  the  section  on  arrangements  that  the 
class of two-hid den-layer neural network classifiers with m nodes in the first layer 
and  2m  nodes  in  the  second  layer  contains  all  arrangement  classifiers  with  m 
hyperplanes.  Therefore,  for k  =  m + 2m ,  the class  of all  arrangement classifiers 
with m hyperplanes is a subclass of C(k) . From this, we easily obtain the following 
approximation result: 

Theorem 30.3.  IjC(k) is the class of all neural network classifiers with the thresh(cid:173)
old sigmoid and k neurons in two hidden layers,  then 

inf  L( ¢) - L * = 0 

lim 
k-+oo ¢EC(k) 

for all distributions of (X, Y). 

It is  more  surprising  that the  same  property holds  if C(k)  is  the  class  of one(cid:173)
hidden-layer neural  networks  with  k  hidden  neurons,  and  an  arbitrary  sigmoid. 
More precisely, C(k)  is the class of classifiers 

¢(x) = {O 

if  1/I(X!  :::  1/2 

1  otherwIse, 

where 1/1  is as in (30.2). 

By Theorem 2.2, we have 

L(¢) - L*  ~ 2E{11/I(X) -1](X)I}' 

where YJ(x)  = P{Y  =  IIX = x}. Thus, inf¢Ec(k)  L(¢) - L* -+ 0 as k  -+  00 if 

for  some  sequence  {1/Ik}  with  ¢k  E  C(k)  for  ¢k(X)  =  h!/Ik(x» 
I/2}'  For universal 
consistency,  we need only assure that the  family  of 1/1 's  can approximate  any  1] 
in  the  LI(M) sense.  In  other words,  the  approximation error infc/JEc(k)  L(¢) - L* 
converges to zero if the class of functions 1/1  is dense in LI (M) for every M. Another 
sufficient  condition  for  this-but of course  much  too  severe-is  that  the  class 

518 

30.  Neural Networks 

:F of functions  1/r  becomes  dense  in the  Loo  (supremum-)  norm in the  space of 
continuous functions C[a, b]d on [a,  b]d, where [a,  b]d denotes the hyperrectangle 
of Rd defined by its opposite vertices a  and b, for any a  and b. 

Lemma 30.2.  Assume that a sequence of classes of functions :Fk  becomes dense 
in  the  Loo  norm  in  the  space of continuous functions  C[a, b]d  (where  [a,  b]d  is 
the hyperrectangle ofRd defined by a, b).  In  other words,  assume that for every 
a, b  E  R d, and every bounded function g, 

lim  inf 
k-+oo JEFk xE[a,b]d 

sup  If(x) - g(x)1  = 0. 

Then for any distribution of (X , Y), 

lim 
k-+oo rjJEC(k) 

inf  L(¢) - L* = 0, 

where C(k)  is the class of classifiers ¢(x) = !r1/J(x» 

Ij2} for 1/r  E  :Fk. 

PROOF.  For fixed E  >  0, find a, b  such that fL([a,  b]d)  ~ 1 - E/3, where  fL  is the 
probability measure of X.  Choose a continuous function 1}vanishing off [a,  b]d 
such that 

E{lry(X)  -

Find k and f  E  :Fk  such that 

____ 
ry(X)1}  :::  6 . 

E 

sup 
xE[a,bJd 

___ 

I/(x) - 1](x) I :::  - . 

E 

6 

For ¢(x) =  IU(x»lj2}, we have, by Theorem 2.2, 

L(¢)-L* 

< 

E 
2E {If(X) - ry(X)II{XE[a,b]dd  + 3" 

<  2E {If(X) -1}(X)II{XE[a,b]dd +2E{I1}(X) -

ry(X)I}  + ~ 

< 

2  sup  If(x) -

xE[a,b]d 

<  E.O 

____ 

___ 
E 
ry(x)1  + +2E {lry(X)  - ry(X) I} + -
3 

This text is basically about all such good families, such as families that are ob(cid:173)

tainable by summing kernel functions, and histogram families. The first results for 
approximation with neural networks with one hidden layer appeared in 1989, when 
Cybenko (1989), Hornik, Stinchcombe, and White (1989), and Funahashi (1989) 
proved independently that feedforward neural networks with one hidden layer are 
dense with respect to the supremum norm on bounded sets in the set of continuous 
functions.  In  other words,  by  taking k  large  enough,  every  continuous  function 

30.3 Approximation by Neural Networks 

519 

on nd  can be  approximated arbitrarily closely,  uniformly  over any  bounded set 
by functions  realized by neural networks with one hidden layer.  For a survey of 
various denseness results we refer to Barron (1989) and Hornik (1993). The proof 
given here uses ideas of Chen, Chen, and Liu (1990). It uses the denseness of the 
class of trigonometric polynomials in the Loo  sense for C[O,  l]d  (thisds a special 
case of the Stone-Weierstrass theorem; see Theorem A.9 in the Appendix), that is, 
by functions of the form 

L (ai cos(2n aT x) + f3i  sin(2n bT x)) , 

i 

where ai, bi  are integer-valued vectors of Rd. 

Theorem 30.4.  For  every  continuous function  f 
:  [a,  b]d  ~ nand for every 
E  >  0,  there exists a neural network with one hidden layer and function  t(x) as 
in (30.2) such that 

sup  If(x) -ljr(x)1  <  E. 

xE[a,b]d 

PROOF.  We prove the theorem for the threshold sigmoid 

a(x) = 

{

-I 
1 

if x  :::;  0 
if x  >  O. 

The  extension  to  general  non decreasing  sigmoids  is  left  as  an  exercise  (Prob(cid:173)
lem  30.3).  Fix  E  >  O.  We  take  the  Fourier  series  approximation  of  f(x).  By 
the  Stone-Weierstrass theorem (Theorem A.9),  there exists a large positive inte(cid:173)
ger  M, nonzero  real  coefficients  al,.'"  aM, fh, ... , 13M,  and integers  mi,}  for 
i  = 1, ... , M,  j  =  1, ... , d, such that 

sup  It (a i cos (~mT x) + f3i  sin (~mT x)) - f(X)1  <  ~, 

xE[a,b]d 

i=l 

2 

a 

a 

where mi = (mi,l, ... ,mi,d), i = 1,  ... , M. It is clear that every continuous func(cid:173)
tion  on the real line that is zero outside some bounded interval can be arbitrarily 
closely  approximated  uniformly  on  the  interval by one-dimensional  neural  net(cid:173)
works, that is, by functions of the form 

k L cia(ai x  + hi) + co· 

i==l 

Just  observe  that  the  indicator function  of an  interval  [b,  c]  may  be  written  as 
a(x - b) + a( -x + c). This implies that bounded functions such as sin and cos can 
be approximated arbitrarily closely by neural networks.  In particular,  there exist 
neural  networks  uiCx), Vi(X)  with i  =  1,  ... , M,  (i.e.,  mappings from  nd to  n) 
such that 

sup 
xE[a,b]d 

jUi(X) - cos (~mT x) j  <  __ E_ 
4Ml a i I 

a 

520 

30.  Neural Networks 

and 

sup 
xE[a,bJd 

!Vi(X)  - sin (~mT X)!  <  __ E_. 
4MI,Bi I 

a 

Therefore, applying the triangle inequality we get 

Since the Ui'S  and Vi'S are neural networks, their linear combination 

M 

1/I(x) = L (aiUi(X) + ,BiVi(X)) 

i=l 

is a neural network too and, in fact, 

sup  If(x) -1/I(x)1 

xE[a,b]d 

< 

sup. 

xE[a,b]d 

If(X) - t (a i cos (~mT X)  +,Bi  sin (~mT x )) I 
+  sup  It (ai cos (~mT x) +,Bi  sin (~mT X))  -1/I(X)1 

xE[a,b]d 

i==l 

i=l 

a 

a 

a 

a 

2E 
"2  =E.  0 

< 

The convergence may be arbitrarily slow for some  f. By restricting the class of 
functions, it is possible to obtain upper bounds for the rate of convergence. For an 
example, see Barron (1993). The following corollary of Theorem 30.4 is obtained 
via Lemma 30.2: 

COROLLARY 30.1.  Let C(k)  contain  all neural network classifiers defined by  net(cid:173)
works of one hidden layer with k hidden nodes,  and an arbitrary sigmoid (J.  Then 
for any distribution of (X, Y), 

lim 
k-+oo cpEC(k) 

inf  L(¢) - L * = O. 

The  above  convergence  also  holds  if the  range  of the  parameters  aij, bi ,  Ci  is 
restricted to an interval [-fh, ,Bk],  where limk-+oo,Bk  = 00. 

REMARK. It is also true that the class of one-hidden-Iayer neural networks with k 
hidden neurons becomes dense in  L 1 (J-L)  for every probability measure  J-L  on nd 
as  k  -?- 00 (see Problem 30.4). Then Theorem 2.2 may be used directly to prove 
Corollary 30.l. 0 

30.4 VC Dimension 

521  . 

In practice, the network architecture (i.e., k in our case) is given to the designer, 
who can only adjust the parameters aij, bi ,  and Ci,  depending on the data  Dn.  In 
this respect, the above results are only of theoretical interest. It is more interesting 
to  find  out how far the error probability of the chosen rule is  from inf c/>ECk  L (<P ). 
We discuss this problem in the next few  sections. 

30.4  VC Dimension 

Assume  now  that  the  data  Dn  =  ((Xl, Yd,  ... , (Xn, Yn»  are  used  to  tune  the 
parameters of the network. To choose a classifier from C(k),  we focus  on the dif(cid:173)
ference between the probability of error of the selected rule and that of the best 
classifier in C(k). Recall from Chapters 12 and 14 that the vc dimension VC(k)  of the 
class C(k)  determines the performance of some learning algorithms. Theorem 14.5 
tells us  that no method of picking a classifier from C(k)  can guarantee better than 
Q  (,JVC(k) / n)  performance uniformly for  all  distributions.  Thus, for meaningful 
distribution-free performance guarantees, the sample size n has to be significantly 
larger than the vc dimension. On the other hand, by Corollary 12.1, there exists a 
way of choosing the parameters of the network-namely, by minimization of the 
empirical error probability-such that the obtained classifier <PI~  satisfies 

E {L(<P~)} -

inf  L(<P):::  16 
c/>EC(k) 

VC(k)  log n + 4 

2n 

for all distributions.  On the other hand, if VC(k)  = 00, then for any n  and any rule, 
some bad distributions exist that induce very large error probabilities (see Theorem 
14.3). 

We  start with  a universal lower bound on the vc dimension  of networks  with 

one hidden layer. 

Theorem 30.5.  (BAUM (1988».  Let a  be an arbitrary sigmoid and consider the 
class C(k)  of neural net classifiers with k nodes in one hidden layer.  Then 

PROOF.  We prove the statement for the threshold sigmoid, and leave the extension 
as an exercise (Problem 30.7). We need to show that there is a set of n = 2 Lk/2Jd 
points in Rd  that can be shattered by sets of the form {x  : ljf(x)  >  1/2}, where ljf 
is  a one-layer neural network of k hidden nodes.  Clearly, it suffices to prove this 
for even k. In fact,  we prove more: if k is even, any set of n = kd points in general 
position can be shattered (points are in general position if no d + 1 points fall  on 
I-dimensional hyperplane). Let {x 1,  ... , xn } be a set of n = kd such 
the same d -
points. For each subset of this set, we construct a neural network ljf with k  hidden 
nodes such that 1jJ(Xi)  >  1/2 if and only if Xi  is a member of this subset. We may 

522 

30.  Neural Networks 

assume without loss  of generality that the cardinality of the  subset to  be picked 
out is at most n/2, since otherwise we can use 1/2 - Vr(x), where Vr  picks out the 
complement of the subset. Partition the subset into at most n / (2d) = k /2 groups, 
each containing at most d points. For each such group, there exists a hyperplane 
a T x  + b  =  0  that  contains  these  points,  but  no  other point  from  {Xl,  ... , xn}. 
Moreover,  there exists  a  small positive number h  such that aT Xi  + b  E  [-h, h] 
if and only if Xi  is  among this  group of at most d  points.  Therefore,  the  simple 
network 

a(aT X  + b + h) + a(-a T X  - b + h) 

is larger than 0 on Xi  for exactly these Xi'S.  Denote the vectors a, and parameters 
b, h  obtained for the k/2 groups by aI, ... , ak/2,  bt, ... , bk/2, and hI,.'"  hk/2. 
Let h = min j -s.k/2 h j. It is easy to see that 

kj2 

Vr(x)  = L (a(aJx +bj  +h)+a(-aJx - b j  +h)) +-
2 

j==l 

1 

is larger than 1/2 for exactly the desired Xi'S. This network has k hidden nodes.  0 

Theorem 30.5  implies that there  is  no  hope for good performance guarantees 
unless the sample size is much larger than kd. Recall Chapter 14, where we showed 
that n  »  VC(k)  is necessary for a guaranteed small elTor probability, regardless of 
the method of tuning the parameters. Bartlett (1993) improved Theorem 30.5 in 
several ways. For example, he proved that 

VC(k)  >  d min (k, 

-

2d 

2 
d  /2 + d + 1 

)  + 1. 

Bartlett also obtained similar lower bounds for not fully connected networks-see 
Problem 30.9-and for two-hidden-layer networks. 

Next  we  show  that for  the  threshold  sigmoid,  the  bound  of Theorem  30.5  is 
tight up to a logarithmic factor, that is, the vc dimension is at most of the order of 
kdlogk. 

Theorem 30.6.  (BAUM  AND  HAUSSLER  (1989)).  Let (5  be the threshold sigmoid 
and let C(k)  be the class of neural net classifiers with k  nodes in the hidden layer. 
Then the shatter coefficients satisfy 

< 

( ~)k(d+l) (~)k+l <  ne  kd+2k+l 
, 

- ( )  

k+l 

d+l 

which implies that for all k, d  ::::  1, 

VC(k)  :::  (2kd + 4k + 2) 10g2(e(kd + 2k + 1)). 

30.4 VC Dimension 

523. 

PROOF.  Fix n  points Xl,  ... ,Xn  E  nd. We bound the number of different values 
of the vector (¢(XI), ... , ¢(xn»  as ¢  ranges through C(k). A node j  in the hidden 
layer realizes a dichotomy of the n points by a hyperplane split. By Theorems 13.9 
and 13.3, this can be done at mostEf:J (7)  ::::  (ne/(d + 1»d+1  different ways. The 
different splittings obtained at the k  nodes determine the k-dimensional input of 
the  output node.  Different choices of the parameters  Co,  CI,  ... ,  Ck  of the output 
node determine different k-dimensionallinear splits of the  n  input vectors.  This 
cannot be done in more than L~:J (7)  ::::  (ne/(k + 1»k+1  different ways for a fixed 
setting of the au  and bi  parameters. This altogether yields at most 

different dichotomies of the n points X I, ... ,  X n ,  as desired. The bound on the vc 
dimension follows from the fact that  VC(k) 

::::  n  if S(C(k), n) 2:  2n.  0 

For threshold sigmoids, the gap between the lower and upper bounds above is 
logarithmic in kd. Notice that the vc dimension is about the number of weights (or 
tunable parameters) w  = kd + 2k + 1 of the network. Surprisingly, Maass (1994) 
proved  that  for  networks  with  at  least two  hidden  layers,  the  upper bound  has 
the right order of magnitude,  that is,  the vc dimension is  Q( w log w).  A  simple 
application of Theorems 30.4 and 30.6 provides the next consistency result that 
was pointed out in Farago and Lugosi (1993): 

Theorem 30.7.  Let (J  be the  threshold sigmoid.  Let gn  be a  classifier from  C(k) 
that minimizes the empirical error 

over ¢  E  C(k). If k  ---+  00 such that k log n/ n  ---+  0 as n  ---+  00,  then gn  is strongly 
universally consistent,  that is, 

with probability one for all distributions of (X, Y). 

lim  L(gn) =  L * 
n-+oo 

PROOF.  By the usual decomposition into approximation and estimation errors, 

L(gn) - L*  =  (L(gn) -

inf  L(¢)) + (inf  L(¢) - L*). 

fjJEC(k) 

fjJEC(k) 

The second term on the right-hand side tends to zero by Corollary 30.1. For the 
estimation error, by Theorems 12.6 and 30.6, 

P  {L(gn) -

inf  L(¢) >  E} 

fjJEC(k) 

< 

8S(C(k) , n)e-nE2/128 

524 

30.  Neural Networks 

which is summable if k = o(nj log n). 0 

The theorem assures  us  that if a  is the threshold sigmoid,  then a sequence of 
properly  sized networks  may be trained to  asymptotically  achieve  the  optimum 
probability of error, regardless of what the distribution of (X, Y) is. For example, 
k  ~ Fn  will  do  the job.  However,  this  is  clearly not the  optimal choice  in  the 
majority of cases. Since Theorem 30.6 provides suitable upper bounds on the vc 
dimension of each class C(k), one may use complexity regularization as described 
in Chapter 18 to find a near-optimum size network. 

Unfortunately,  the  situation  is  much less  clear for  more  general,  continuous 
sigmoids. The vc dimension then depends on the specific sigmoid. It is not hard 
to  see that the vc dimension  of C(k)  with  an  arbitrary nondecreasing  sigmoid is 
always  larger than  or equal  to  that  with  the  threshold  sigmoid  (Problem  30.8). 
Typically,  the  vc  dimension  of a  class  of such  networks  is  significantly  larger 
than that for the threshold sigmoid. In fact, it can even be infinite! Macintyre and 
Sontag  (1993)  demonstrated the  existence  of continuous,  infinitely  many  times 
differentiable monotone increasing sigmoids such that the vc dimension of C(k) is 
infinite if k  ::::  2. Their sigmoids have little squiggles, creating the large variability. 
It is even more surprising that infinite vc dimension may occur for even smoother 
sigmoids, whose second derivative is negative for x  >  0 and positive for x  <  O. 
In Chapter 25  (see Problem 25.11) we basically proved the following result. The 
details are left to the reader (Problem 30.13). 

Theorem 30.8.  There exists a sigmoid a  that is monotone increasing, continuous, 
concave on (0, (0), and convex on (-00,0), such that  VC(k)  = 00 for each k  ::::  8. 

We recall once again that infinite vc dimension implies that there is no hope of 

obtaining nontrivial distribution-free upper bounds on 

no  matter how  the training  sequence  Dn  is  used to  select the  parameters  of the 
neural network. However, as we will see later, it is still possible to obtain universal 
consistency.  Finiteness of the vc dimension has been proved for  many  types  of 
sigmoids.  Maass  (1993)  and  Goldberg  and  Jerrum  (1993)  obtain  upper bounds 
for piecewise polynomial  sigmoids.  The results  of Goldberg and Jerrum  (1993) 
apply for general classes parametrized by real numbers, e.g., for classes of neural 
networks with the sigmoid 

a(x) = {  1 -

Ij(2x + 2) 

Ij(2 - 2x) 

if x  ::::  0 
if x  <  O. 

Macintyre and Sontag (1993) prove VC(k)  <  00 for a large class of sigmoids, which 
includes the standard, arctan, and gaussian sigmoids. While finiteness is useful, the 

30.4 VC Dimension 

525 

lack of an explicit tight upper bound on  VC(k)  prevents us from getting meaningful 
upper  bounds  on  the  performance  of gn,  and  also  from  applying  the  structural 
risk minimization of Chapter 18. For the standard sigmoid, and for networks with 
k hidden nodes and  w tunable  weights Karpinski and Macintyre (1994) recently 
reported the upper bound 

V  :::: 

kw(kw - 1) 

2 

+ w(1 + 2k) + w(l + 3k) 10g(3w + 6kw + 3). 

See also Shawe-Taylor (1994). 

Unfortunately,  the  consistency  result  of Theorem  30.7  is  only  of theoretical 
interest,  as  there is  no efficient algorithm to  find  a classifier that minimizes  the 
empirical  error  probability.  Relatively  little  effort  has  been  made  to  solve  this 
important problem. Farago and Lugosi (1993) exhibit an  algorithm that finds  the 
empirically optimal network. However, their method takes time exponential in kd, 
which is intractable even for the smallest toy problems. Much more effort has been 
invested in the tuning of networks by minimizing the empirical squared error, or 
the empirical L 1 error.  These problems are also computationally demanding, but 
numerous suboptimal hill-climbing algorithms have been used with some success. 
Most famous among these is the back propagation algorithm of Rumelhart, Hinton, 
and Williams (1986). Nearly all known algorithms that run in reasonable time may 
get stuck at local  optima,  which results  in  classifiers whose probability  of error 
is hard to  predict. In the next section we study the error probability of neural net 
classifiers obtained by minimizing empirical L p  errors. 

We end this section with a very simple kind of one-layer network. The committee 

machine (see, e.g., Nilsson (1965) and Schmidt (1994)) is a special case of a one(cid:173)
hidden-layer neural network of the form (30.2) with Co  = 0,  Cl  = ... = Ck  = 1, and 
the threshold sigmoid. 

o or 1 

FIGURE 30.11.  The  committee  machine  has fixed  weights  at the 
output of the hidden layer. 

Committee machines  thus  use  a majority  vote over the  outcomes  of the  hidden 
neurons.  It is  interesting  that  the  lower  bound  of Theorem  30.5  remains  valid 
when  C(k)  is  the  class  of all  committee machines  with  k  neurons  in the  hidden 

526 

30.  Neural "Networks 

layer.  It is less  obvious,  however,  that the  class  of committee machines  is  large 
enough for the asymptotic property 

lim 
k-Hx) </JEC(k) 

inf  L(rp) = L * 

for all distributions of (X, Y) (see Problem 30.6). 

FIGURE 30.12.  A partition of the plane 
determined by a committee machine 
with 5 hidden neurons. The total vote 
is shown  in  each  region.  The  region 
in which we decide "0" is shaded. 

30.5  Ll Error Minimization 

In the previous section, we obtained consistency for the standard threshold sigmoid 
networks by empirical risk minimization. We could not apply the same methodol(cid:173)
ogy for general sigmoids simply because the vc dimension for general sigmoidal 
networks  is  not bounded.  It is  bounded for certain classes  of sigmoids,  and for 
those, empirical risk minimization yields universally consistent classifiers. Even if 
the vc dimension is infinite, we may get consistency, but this must then be proved 
by other methods, such as methods based upon metric entropy and covering num(cid:173)
bers (see Chapters 28 and 29, as well as the survey by Haussler (1992)). One could 
also train the classifier by minimizing another empirical criterion, which is exactly 
what we  will do in this section.  We  will be rewarded with a general consistency 
theorem for all sigmoids. 

For 1 ::;  p  <  00, the empirical L p  error of a neural network l/f is defined by 

The most interesting cases are p  = 1 and p  = 2. For P = 2 this is just the empirical 
squared  error,  while  p  =  1 yields  the  empirical  absolute  error.  Often  it makes 
sense to attempt to choose the parameters of the network l/f  such that J~p\ l/f) is 

30.5 Ll Error Minimization 

527 

minimized. In situations where one is not only interested in the number of errors, 
but also how  robust the  decision is,  such error measures may be meaningful.  In 
other words, these error measures penalize even good decisions if 'ljf  is close to the 
threshold value O.  Minimizing J}~P)is like finding a good regression function esti(cid:173)
mate. Our concern is primarily with the error probability. In Chapter 4 we already 
highlighted the dangers of squared error minimization and L p  errors in  general. 
Here we will concentrate on the consistency properties. We minimize the empirical 
error over a class of functions, which should not be too large to avoid overfitting. 
However, the class should be large enough to contain a good approximation of the 
target function. Thus, we let the class of candidate functions grow with the sample 
size n, as in Grenander's "method of sieves" (Grenander (1981)). Its consistency 
and  rates  of convergence  have  been  widely  studied  primarily  for  least  squares 
regression  function  estimation  and  nonparametric  maximum  likelihood  density 
estimation-see Geman and Hwang (1982), Gallant (1987), and Wong and Shen 
(1992). 
REMARK.  REGRESSION  FUNCTION  ESTIMATION.  In  the  regression  function  estima(cid:173)
tion setup, White (1990) proved consistency of neural network estimates based on 
squared error minimization.  Barron (1991;  1994) used a complexity-regularized 
modification  of these  error  measures  to  obtain the  fastest  possible  rate  of con(cid:173)
vergence for nonparametric neural network estimates. Haussler (1992) provides a 
general framework for empirical error minimization, and provides useful tools for 
handling neural networks.  Various  consistency properties of nonparametric neu(cid:173)
ral network estimates have been proved by White (1991), Mielniczuk and Tyrcha 
(1993), and Lugosi and Zeger (1995).  0 

We  only consider the  p  =  1 case,  as  the generalization to  other values of p  is 

straightforward. Define the Ll error of a function 'ljf  : Rd  -+ R  by 

J('ljf) = E{I'ljf(X) - YI}. 

We pointed out in Problem 2.12 that one of the functions minimizing  J ('ljf) is the 
Bayes rule g* whose error is denoted by 

Then clearly,  J* = L *. We have also seen that if we define a decision by 

J* = inf J('ljf) =  J(g*). 

1jJ 

g(x) = 

{ 

if  'ljf(x)  ::::  1/2 

0 
1  otherwise, 

then its error probability L(g) = P{g(X) =I  Y} satisfies the inequality 

L(g) - L* ::::  J('ljf) - J*. 

Our approach is to select a neural network from a suitably chosen class of networks 
by minimizing the empirical error 

528 

30.  Neural Networks 

Denoting this function by o/n,  according to the inequality above, the classifier 

gn(x) = 

{ 

0 

if  o/n(x)  :::  1/2 
h ·  
ot  erWlse, 

is consistent if the LIenor 

converges to  J* in probability. Convergence with probability one provides strong 
consistency. For universal convergence, the class over which the minimization is 
performed has to be defined carefully. The following theorem shows that this may 
be achieved by neural  networks  with k  nodes,  in which the range of the  output 
weights Co,  CI,  ... ,  Ck  is restricted. 

Theorem 30.9.  (LUGOSI  AND  ZEGER  (1995)).  Let  (j  be  an  arbitrary  sigmoid. 
Define the class :01  of neural networks by 

and let o/n  be afunction that minimizes the empirical Ll error 

over 0/  E  :01· If kn  and f3n  satisfy 

lim  k n  = 00, 
n---+oo 

lim  f3n  =  00,  and 
n---+oo 

. 
lUll 
n---+oo 

knf3~ log(kn f3n) 

n 

= 0, 

then the classification rule 

gn (x) = 

{ 

if o/n(x)  :::  1/2 

0 
1  otherwise 

is universally consistent. 

REMARK.  Strong universal  consistency may  also  be  shown by imposing  slightly 
more restrictive conditions on k n  and f3n  (see Problem 30.16).0 

PROOF.  By the argument preceding the theorem, it suffices to prove that J('t/ln )  -
J*  -+ 0 in probability. Write 

J(ljIn)  - J* = (J(ljIn) -

inf  J(ljI)) + (inf  J(o/) - J*) . 

1jJE:F" 

1jJE:F" 

30.5  LJ  Error Minimization 

529 

To handle the approximation error-the second term on the right-hand side-let 
t' E  :01  be a function such that 

E{lt'(X) - g*(X)1}  ::; E{lt(X) - g*(X)1} 

for  each t  E  :07.  The existence of such  a function  may  be  seen by  noting that 
E{lt(X) - g*(X)1}  is  a  continuous  function  of the  parameters  ai, hi, Ci  of the 
neural network t. Clearly, 

inf  J(t) - J*  <  J(t') - J* 
1jJEFI1 

=  E{lt'(X) - YI}  - E{lg*(X) - YI} 

<  E{lt'(X) - g*(X)I}' 

which converges to zero as n  -+  00, by Problem 30.4. We start the analysis of the 
estimation error by noting that as in Lemma 8.2, we have 

J(tn) -

inf  J(t) 
1jJE:F;l 

::;  2  sup  IJ(t) -

1jJEF" 

In(t)1 

=  2  sup  E{lt(X) - YI}  -

1jJEFI1 

l

- L It(Xd - Yil  . 
I 
In 
n  i=l 

Define the class Mn  of functions on nd  x  {O,  I}  by 

Mil 

={m(x, Y) = ItciG(arx +bi) +CO  - yl : ai  E  Rd, hi  E  R, ~ [Cd:s: fJn}. 

Then the previous bound becomes 

2  sup  E{m(X, Y)}  -

mEM" 

l

I 
In 
- Lm(Xi ,  Yi ) 
n  i=l 

. 

Such quantities may be handled by the uniform law of large numbers of Theorem 
29.1,  which applies  to  classes of uniformly bounded functions.  Indeed,  for each 
m  EMn 

) 
<  2max  ~ lcd, 1 

kl1 

( 

< 

2{3n, 

530 

30.  Neural Networks 

if n is so large that f3n  2:  1. For such n's, Theorem 29.1  states that 

p  {  sup  iE{m(X, Y)}  - ~ t111(Xi , Yi)i  >  E} 

mEMn 

1=1 

where N(E, Mn(Dn»  denotes the h -covering number of the random set 

:::::  8E {N(E/8, Mn(Dn»} e-ne2/(512f3,7)  , 

defined in Chapter 29.  All  we need now is  to  estimate these covering numbers. 
Observe  that for  1111,1112  E  Mn  with 1111(X,  y)  = 11/fI(X)  - yl  and 1112(X,  y)  = 
11/f2(X)  - yl, for any probability measure v on n d  x  {O,  I}, 

f 1111I(X,  y) -

1112 (X , y)lv(d(x, y»)  ::::: f 11/fI(X)  -1/f2(x)lfL(dx), 

where fL  is  the marginal of von nd. Therefore, it follows thatN(E, Mn(Dn» 
::::: 
N(E, :Fn(X7»,  where  X7  =  (Xl, ... , Xn).  It means  that an  upper bound on the 
covering number of the class of neural networks :Fn  is also an upper bound on the 
quantity that interests us.  This bounding may be done by applying lemmas from 
Chapters 13  and 30. Define the following three collections of functions: 

QI 

Q2 

Q3 

{aT x + b;  a  End, bEn} , 

{ 0- (a T x  + b);  a  End, bEn} , 

{co-CaT x + b);  a  End, bEn, c  E  [-f3n,  f3nJ}. 

By Theorem 13.9, the vc dimension of the class of sets 
Qt = {(x, t) : t  :::::  1/f(x), 1/f  E Qd 

is  Vgt  :::::  d + 2.  This implies by Lemma 29.5  that Vg;  :::::  d + 2,  so by Corollary 
29.2, for any xl1  = (Xl,  ... , xn ), 

N(E, Q2(X~» :::::  2 

(

4  )2(d+2) 
e 
E

, 

where Q2(X?)  =  {z  E  nn  : z  =  (g(xd, ... , g(xn», g  E  Q2}.  Now,  using  similar 
notations, Theorem 29.7 allows us to estimate covering numbers of Q3(Xn: 

N(E, Q3(X?) 

:::::  ;N(E/(2f3n), Q2(X?»)  ::::: 

4 

(8  f3  )2d+S 

:  n 

if fin  >  2/e. Finally, we can apply Lemma 29.6 to obtain 

< 

30.6 The Adaline and Padaline 

531 . 

Thus,  substituting this  bound into the probability inequality above,  we  get for n 
large enough, 

which tends to zero if 

concluding the proof.  0 

knf3~ 10g(knf3n) 
-....:.:....---- -+0, 

n 

There are yet other ways to obtain consistency for general sigmoidal networks. 
We may restrict the network by discretizing the values of the coefficients in some 
way-thus creating a sieve with a number of members that is easy to enumerate(cid:173)
and applying complexity regularization (Chapter 18). This is the method followed 
by Barron (1988;  1991). 

30.6  The Adaline and Padaline 

Widrow (1959) and Widrow and Hoff (1960) introduced the Adaline, and Specht 
(1967;  1990)  studied  polynomial  discriminant  functions  such  as  the  Padaline. 
Looked  at  formally,  the  discriminant  function  1/1  used  in  the  decision  g(x)  = 
I{1/J(x»O}  is of a polynomial nature, with 1/1  consisting of sums of monomials like 

(  (l»)il 
a  x 

(d»)id 

'"  x 

, 

where iI, ... , id  ::::  0 are integers, and a is a coefficient. The order of a monomial 
is i 1 + ... + id • Usually, all terms up to, and including those of order r are included. 
Widrow's Adaline (1959) has r  = 1. The total number of monomials of order r or 
less does not exceed (r + 1)d. The motivation for developing these discriminants is 
that only up to  (r + l)d coefficients need to be trained and stored. In applications 
in  which  data  continuously  arrive,  the  coefficients  may  be  updated  on-line  and 
the data can be discarded. This property is,  of course,  shared with standard neu(cid:173)
ral networks. In most cases, order r polynomial discriminants are not translation 
invariant.  Minimizing a given  criterion on-line is  a phenomenal  task,  so  Specht 
noted that training is not necessary if the a's are chosen so as to give decisions that 
are close to those of the kernel method with normal kernels. 

For example,  if K(u)  =  e- u2

/

2 ,  the kernel method picks a smoothing factor h 

532 

30.  Neural Networks 

(such that h  -+ 0 and nh d  -+ 00; see Chapter 10) and uses 

1/r(x) 

~ t(2Yi  -1)K (IIX - Xiii) 

n  i=l 

h 

The same decision is obtained if we use 

Now,  approximate  this  by  using Taylor's  series  expansion and  truncating  to  the 
order r  terms. For example, the coefficient of (X(l))i!  ... (x(d))i d  in the expansion 
of 1/r(x)  would be, if L~=l i j  = i, 

These sums are easy to update on-line, and decisions are based on the sign of the 
order r truncation 1/r r  of 1/r. The classifier is called the Padaline. Specht notes that 
overfitting in 1/rr  does not occur due to the fact that overfitting does not occur for 
the kemel method based on 1/r. His method interpolates between the latter method, 
generalized linear discrimination, and generalizations of the perceptron. 

For fixed r, the Pad aline defined above is not universally consistent (for the same 
reason linear discriminants  are not universally  consistent),  but if r  is allowed to 
grow with n, the decision based on the sign of 1/rr  becomes universally consistent 
(Problem 30.14). Recall, however, that Padaline was not designed with a variable 
r  in mind. 

30.7  Polynomial Networks 

Besides  Adaline  and  Padaline,  there  are  several  ways  of constructing  polyno(cid:173)
mial  many-layered  networks  in  which  basic  units  are  of the  form  (xCl)t  ... 
(XCk))ik  for  inputs  xCI),  ... , x(k)  to  that level.  Pioneers  in  this  respect are  Gabor 
(1961),  Ivakhnenko  (1968;  1971)  who  invented  the  GMDH  method-the  group 
method of data handling-and Barron (1975). See also Ivakhnenko, Konovalenko, 
Tulupchuk,  and Tymchenko  (1968)  and Ivakhnenko,  Petrache,  and  Krasyts'kyy 
(1968). These networks can be visualized in the following way: 

30.7 Polynomial Networks 

533. 

FIGURE 30.13.  Simple polynomial network: Each gi  represents a 
simple polynomialfunction of its input. In Barron's work (Barron 
(1975), Barron and Barron (1988)), the gi 's are sometimes 2-input 
elements of the form gi(X,  y) = ai  + bix + CiY + dixy. 

If the gi 's are Barron's quadratic elements, then the network shown in Figure 30.13 
represents a particular polynomial of order 8 in which the largest degree of any xCi) 
is  at most 4.  The number of unknown coefficients is  36 in the example shown in 
the figure,  while in a full-fledged order-8 polynomial network, it would be much 
larger. 

For training these networks, many strategies have been proposed by Barron and 
his associates. Ivakhnenko (1971), for example, trains one layer at a time and lets 
only the best neurons in each layer survive for use as input in the next layer. 

It is easy to see that polynomial networks, even with only two inputs per node, 
and with  degree in each cell restricted to  two,  but with  an  unrestricted number 
of layers,  can implement any polynomial in d  variables.  As the polynomials are 
dense in the Loo  sense on C[a, b]d for all a, bEnd, we note by Lemma 30.2 that 
such networks include a sequence of classifiers approaching the Bayes error for 
any distribution. Consider several classes of polynomials: 

where 0/1,  ... , o/k  are fixed monomials, but the ai's are free coefficients. 

('h  = {o/  E ('h  : 0/1,  ... , o/k  are monomials of order  .::;  r}. 

(The order of a monomial  (X(l»)i 1 

••• (X(d»)i d  is il + ... + id.) 

93  = {o/  E  91  : 0/1,  ... , o/k  are monomials of order  ~ r,  but k  is not fixed}. 

As  k  ~ 00,  91  does  not generally  become  dense  in  C[a, b]d  in  the  Loo  sense 
unless  we  add  o/k+l, 0/k+2,  ... in  a  special  way.  However,  93  becomes dense  as 
r  ~ 00 by Theorem A.9  and 92  becomes dense  as  both k  ~ 00 and r  ~ 00 
(as  92  contains  a  subclass  of 93  for  a smaller r  depending  upon  k  obtained by 
including in l/II, ... , o/k  all monomials in increasing order). 

The vc dimension of the class of classifiers based on 91  is not more than k (see 
Theorem  13.9).  The vc dimension  of classifiers based upon 92  does  not exceed 

534 

30.  Neural Networks 

those  of ~h, which in tum is  nothing but the  number of possible monomials  of 
order:::;  r. A simple counting argument shows that this is bounded by (r + 1)d . See 
also Anthony and Holden (1993). 

These simple bounds may be used to  study the consistency of polynomial net(cid:173)

works. 

Let us  take  a fixed structure network in which all  nodes  are fixed-they have 
at most k inputs with k  ::::  2 fixed  and represent polynomials of order :::;  r  with 
r  ::::  2 fixed.  For example,  with r  =  2,  each cell with input Z I, ... ,  Zk  computes 
Li ai o/i (Zl,  ... , Zk), where the ai's are coefficients and the o/i 's are fixed monomi(cid:173)
als of order r or less, and all such monomials are included. Assume that the layout 
is  fixed  and is  such that it can realize all  polynomials of order:::;  s  on the input 
X(l),  ... ,  x(d).  One way of doing this is to realize all polynomials of order:::;  r  by 
taking all  (~) possible input combinations, and to repeat at the second level with 
(cp)  cells of neurons, and so forth for a total of s / r layers of cells. This construc(cid:173)
tion is obviously redundant but it will do for now. Then note that the vc dimension 
is not more than (s + 1)d, as noted above. If we choose the best coefficients in the 
cells by empirical risk minimization, then the method is consistent: 
Theorem 30.10.  In the fixed-structure polynomial network described above, if Ln 
is  the  probability of error of the  empirical risk minimizer,  then  E{ Ln}  -+  L * if 
s  -+  (X)  and s  = o(n lid). 

PROOF.  Apply Lemma 30.2 and Theorem 12.6.  0 

Assume a fixed-structure network as  above  such that all polynomials of order 
:::;  s  are realized plus some other ones,  while the number of layers of cells is not 
more than I. Then the vc dimension is not more than (r I + l)d because the maximal 
order is not more than Ir. Hence, we have consistency under the same conditions 
as above, that is, s  -+  00, and I = o(n lid). Similar considerations can now be used 
in a variety of situations. 

30.8  Kolmogorov-Lorentz Networks 

and Additive Models 

Answering one of Hilbert's famous  questions,  Kolmogorov  (1957)  and Lorentz 
(1976) (see also Sprecher (1965) and Hecht-Nielsen (1987)) obtained the following 
interesting representation of any continuous function on [0,  1]d. 

Theorem 30.11.  (KOLMOGOROV  (1957);  LORENTZ  (1976)).  Let  f  be  continuous 
on [0,  1]d.  Then  f  can be rewritten asfollows: let <5  >  0 be an arbitrary constant, 
and choose 0  <  E  :::;  <5  rational.  Then 

2d+1 

f  = L g(Zk), 

k=l 

30.8 Kolmogorov-Lorentz Networks and Additive Models 

535 

where g  : R  ----'»- R  is a continuous function (depending upon  f  and E),  and each 
Zk  is rewritten as 

d 

Zk  = LAk 1fr  (xU) + Ek)  + k. 

j=l 

Here A is real and 1jf  is monotonic and increasing in its argument. Also, both A and 
1jf  are universal (independent of f), and 1jf  is Lipschitz:  11jf(x) -1jf(y)1  :s  clx - y 1 
for some c  >  0. 

The Kolmogorov-Lorentz theorem states that f  may be represented by a very 
simple network that we will call the Kolmogorov-Lorentz network. What is amaz(cid:173)
ing  is  that  the  first  layer  is  fixed  and  known  beforehand.  Only  the  mapping  g 
depends on f. This representation immediately opens up new revenues of pursuit(cid:173)
we need not mix the input variables. Simple additive functions of the input variables 
suffice to represent all continuous functions. 

input 

FIGURE 30.14.  The  Kolmogorov-Lorentz  network  of  Theorem 
30.11. 

To explain what is happening here, we look at the interleaving of bits to make 
one-dimensional numbers out of d-dimensional vectors. For the sake of simplicity, 
let f  : [0, 1]2  ----'»- R. Let 

be the binary expansions of x  and y, and consider a representation for the function 
f(x, y). The bit-interleaved number Z  E  [0,  1]  has binary expansion 

and may thus be written as  z = ¢(x) + (lj2)¢(y), where 

536 

30.  Neural Networks 

and thus 

1 
2CP(Y)  = O.Oy10Y20Y3  .... 

We can also retrieve x  and y from z by noting that 

x  = 0/1 (Z), 

0/1 (Z)  = O.Z1Z3ZSZ7  ... , 

y  =  0/2(Z), 

0/2(Z)  = O.Z2Z4Z6ZS  .... 

and 

Therefore, 

J(x, y)  = 

J(0/1(Z),0/2(Z)) 

def 

g(z) 

(a one-dimensional function of z) 

=  g (1)(X) + ~1>(Y») 

The function cP  is  strictly monotone increasing. Unfortunately, cP,  0/1,  and 0/2  are 
not continuous. Kolmogorov's theorem for this special case is as follows: 

Theorem 30.12.  (KOLMOGOROV  (1957)).  There  exist jive  monotone  Junctions 
CPI,  ... , CPs  : [0,  1]  -+ Rsatisfying Icpi(xI)-CPi(x2)1  ::;  IXI  -x21, withtheJollowing 
property: Jor every J  E  e[O,  1]2 (the continuous Junctions on [0,  1]2),  there exists 
a  continuous Junction g  such thatJor all (Xl, X2)  E  [0,1]2, 

The difference with pure bit-interleaving is that now the CPi 's are continuous and 
g  is  continuous whenever J is.  Also, just as  in our simle example,  Kolmogorov 
gives an explicit construction for CPl,  ... , CPs. 

Kolmogorov's theorem may be used to  show  the  denseness of certain classes 
of functions that may be described by networks. There is one pitfall however: any 
such result must involve at least one neuron or cell that has a general function in 
it,  and we  are  back at square one,  because a general function,  even on only  one 
input, may be arbitrarily complicated and wild. 

Additive models include, for example, models such as 

d 

ex  + L o/i(x(i)), 

i=l 

where  the  o/i'S  are  unspecified  univariate  functions  (Friedman  and  Silverman 
(1989), Hastie and Tibshirani (1990)). These are not powerful enough to approxi(cid:173)
mate all functions. A generalized additive model is 

30.8 Kolmogorov-Lorentz Networks and Additive Models 

537 

(Hastie and  Tibshirani  (1990»,  where (5  is  now a given or unspecified function. 
From Kolmogorov's theorem, we know that the model 

with  l/fi,k,  l/f  unspecified functions,  includes all continuous functions  on compact 
sets  and  is  thus  ideally  suited for  constructing  networks.  In  fact,  we  may  take 
ctk  = k  and take alll/fi,k's as  specified in Kolmogorov's theorem. This leaves only 
l/f  as the unknown. 

Now,  consider the following:  any continuous univariate function  f  can be ap(cid:173)
proximated  on  bounded  sets  to  within  E  by  simple  combinations  of threshold 
sigmoids (5  of the form 

k L ai(5(x  - Ci), 

i=l 

where ai, Ci, k are variable. This leads to a two-hidden-layer neural network repre(cid:173)
sentation related to that of Kurkova (1992), where only the last layer has unknown 
coefficients for a total of 2k. 

Theorem 30.13.  Consider a  network  classifier of the form  described  above  in 
which (5(.)  is the threshold sigmoid,  and the ai's and Ci 's are found by empirical 
error minimization.  Then  E{L n }  -7>  L * for all distributions of (X, Y),  if k  -7>  00 
and klognjn  -7>  O. 

PROOF.  We  will  only  outline the  proof.  First observe that we  may  approximate 
all functions  on  C[a, b]d  by selecting k  large enough. By Lemma 30.2 and The(cid:173)
orem  12.6,  it  suffices  to  show  that  the  vc dimension  of our class  of classifiers 
is o(n j  log n). Considering Cik  + 2:.1=1  l/fi,k(XCi»  as  new input elements, called Yb 
1  S  k  s  2d + 1,  we  note  that  the  vc  dimension  is  not  more  than  that of the 
classifiers based on 

2d+l 

k L L ai(5(Yj  - Ci), 

k==l 

i=l 

which in turn is not more than that of the classifiers given by 

k(2d+1) L  bz(5(zz  - dz), 

i=l 

where {bt},  {dz}  are parameters, and {zz}  is  an input sequence. By Theorem 13.9, 
the vc dimension is not more than k(2d + 1). This concludes the proof.  0 

For more results along these lines, we refer to Kurkova (1992). 

538 

30.  Neural Networks 

30.9  Projection Pursuit 

In projection pursuit (Friedman and Tukey (1974), Friedman and Stuetzle (1981), 
Friedman, Stuetzle, and Schroeder (1984), Huber (1985), Hall (1989), Flick, Jones, 
Priest, and Herman (1990», one considers functions of the form 

k 

o/(x) = L o/j(bj  + aJ x), 

j=l 

(30.3) 

where b j  E  R, a j  E  Rd  are constants and  0/1,  ... , o/k  are  fixed  functions.  This 
is  related to,  but not a special case of,  one-hidden-Iayer neural networks.  Based 
upon the Kolmogorov-Lorentz representation theorem, we may also consider 

d 

o/(x) =  L o/j(x(j», 

j=l 

(30.4) 

for  fixed  functions  o/j  (Friedman  and  Silverman  (1989),  Hastie  and  Tibshirani 
(1990».  In  (30.4)  and  (30.3),  the  o/j'S  may  be  approximated  in  tum  by  spline 
functions or other nonparametric constructs. This approach is covered in the liter(cid:173)
ature on generalized additive models (Stone (1985), Hastie and Tibshirani (1990». 

The  class  of functions  eaT x,  a  E  R d

,  satisfies  the  conditions  of the  Stone(cid:173)

Weierstrass  theorem  (Theorem  A.9)  and  is  therefore  dense  in  the  Loo  norm  on 
C[a, b]d for any a, b  E  Rd (see, e.g., Diaconis and Shahshahani (1984». 

As a corollary, we note that the same denseness result applies to the family 

k L o/i(aT x), 

i=l 

(30.5) 

where k  2:  1 is  arbitrary and 0/1,  0/2,  ... are general functions.  The latter result is 
at the basis of projection pursuit methods for approximating functions, where one 
tries  to  find  vectors  ai  and functions  o/i  that approximate  a given  function  very 
well. 

REMARK.  In some cases, approximations by functions  as  in (30.5)  may be exact. 
For example, 

and 

Theorem 30.14.  (DIACONIS  AND  SHAHSHAHANI  (1984». Let m  be a positive in(cid:173)
teger.  There  are  (m+~-l) distinct  vectors  a j  E  Rd  such  that any  homogeneous 
polynomial f  of order m  can be written as 

30.9 Projection Pursuit 

539 

C+!-l) 

f(x) =  L  cj(aJ x)m 

j=l 

for some real numbers C j. 

Every polynomial of order mover n d  is a homogeneous polynomial of order m 
over n d+ 1 by replacing the constant 1 by a component x (d+ 1) raised to an appropriate 
power. Thus, any polynomial of order mover nd may be decomposed exactly by 

(m+d) 

f (x) = t  C j  (a J x  + b j  ) m 

j=l 

for some real numbers b j, C j  and vectors a j  E  nd. 

Polynomials may  thus  be represented exactly in the form  ~~=l cfJi(aF x) with 
k  =  (m~d). As  the polynomials are dense in C[a, b]d, we have yet another proof 
that {~~=l cfJi(aF x)} is dense in C[a, b]d. See Logan and Shepp (1975) or Logan 
(1975) for other proofs. 

The previous  discussion suggests at least two families  from  which to  select a 
discriminant function. As usual, we let g(x) =  I{1/f(x»O} for a discriminant function 
1jJ.  Here 1jJ  could be picked from 

or 

where  m  is  sufficiently  large.  If we  draw  1jJ  by  minimizing  the  empirical  error 
(admittedly at a tremendous computational cost), then convergence may result if 
m is not too large. We need to know the vc dimension of the classes of classifiers 
corresponding to:;::;n  and F~. Note that Fm  coincides with all polynomials of order 
m and each such polynomial is the sum of at most (m + l)d monomials. If we invoke 
Lemma 30.2 and Theorem 12.6, then we get 

Theorem 30.15.  Empirical risk minimization to determine {a j  ,  b j, C j  }  in Fm leads 
to a universally consistent classifier provided that m  -+  00 and m  = o(n lid /  log n). 

Projection pursuit is very powerful and not at all confined to our limited discus(cid:173)

sion above. In particular, there are many other ways of constructing good consistent 
classifiers that do not require extensive computations such as empirical error min(cid:173)
imization. 

540 

30.  Neural Networks 

30.10  Radial Basis Function Networks 

We may perform discrimination based upon networks with functions of the form 

(and decision  g(x)  =  I{1/f(x»O}),  where k  is  an  integer, aI, ... , ak  hI, ... , hk  are 
constants, xl, ... , Xk  E  Rd, and K  is a kernel function (such as  K(u) = e-lIul12  or 
K(u) =  I/O + IluI1 2)). In this form,  1fJ  covers several well-known methodologies: 
0)  The kernel rule (Chapter 10): Take k = n, ai  = 2Yi -1, hi  = h, xi  = Xi. With 
this  choice, for  a large class  of kernels,  we  are  guaranteed convergence if 
h  ~ 0 and nh d  ~ 00. This approach is attractive as no difficult optimization 
problem needs to be solved. 

(2)  The  potential function  method.  In  Bashkirov,  Braverman,  and  Muchnik 
(964),  the  parameters  are  k  = n,  hi  = h,  Xi  = Xi.  The  weights  ai  are 
picked to minimize the empirical error on the data, and h is held fixed.  The 
original kernel suggested there is  K(u) = I/O + lIuI1 2). 

(3)  Linear discrimination.  For k  = 2,  K(u) = e- lIuIl2 , hi  ==  h, al  = 1, a2  = -1, 
the  set  {x  :  1fJ(x)  >  O}  is  a  linear halfspace.  This,  of course,  is  not uni(cid:173)
versally consistent. Observe that the separating hyperplane is the collection 
of all points x  at equal distance from  Xl  and X2.  By varying  Xl  and X2,  all 
hyperplanes may be obtained. 

(4)  Radial basis function  (RBF)  neural networks (e.g.,  Powell (987), Broom(cid:173)

head  and  Lowe  (988),  Moody  and  Darken  (989),  Poggio  and  Girosi 
(990), Xu,  Krzyzak, and Oja (993), Xu, Krzyzak, and Yuille (994), and 
Krzyzak, Linder, and Lugosi  (1993)).  An even more general function 1fJ  is 
usually employed here: 

k 

1fJ(x) = L Ci K  ((x  - xd T Ai(X - Xi))  + co, 

i=l 

where the A/s are tunable d  x  d matrices. 

(5)  Sieve methods.  Grenander (981) and Geman and Hwang (982) advocate 
the use of maximum likelihood methods to find suitable values for the tunable 
parameters in 1fJ  (for k,  K  fixed beforehand) subject to certain compactness 
constraints  on  these  parameters  to  control  the  abundance  of choices  one 
may have. If we were to use empirical error minimization, we would find, 
if k  :::  n, that all data points can be correctly classified (take the hi'S small 
enough, setai = 2Yi -1, Xi  = Xi, k = n), causing overfitting. Hence, k must 
be smaller than  n  if parameters  are picked in  this  manner.  Practical ways 
of choosing the parameters  are discussed by  Kraaijveld and Duin (1991), 

30.10 Radial Basis Function Networks 

541 

and Chou and Chen (1992). In both (4) and (5), the Xi'S  may be thought of 
as representative prototypes, the ai's as weights, and the hi's as the radii of 
influence. As a rule, k is much smaller than n  as Xl,  ... , Xk  summarizes the 
information present at the data. 

To design a consistent RBF neural network classifier, we may proceed as in (1). 
We  may  also  take  k  --+  00 but k  =  o(n).  Just let (Xl,  ... , xd  ==  (Xl, ... , Xk), 
(aI, ... , ak  ==  2Yl  - 1, ... , 2Yk  - 1),  and choose  Ai  or hi  to  minimize a  given 
error criterion based upon X k+ I,  ... , X n, such as 

A 

Ln (g) =  --k L  I{g(x; )iY;} , 

n 

1 
n-

i=k+l 

where g(x) =  I{1f;(x»O}, and 1/1  is as in (1). This is nothing but data splitting (Chapter 
22). Convergence conditions are described in Theorem 22.1. 

A more ambitious person might try empirical risk minimization to find the best 

Xl,  ... , Xb aI,""  ab AI, ... ,  Ak (d  x  d matrices as in (4)) based upon 

1  n -L I{g(x;)=!Yd' 

n  i=I 

If k  --+  00, the class of rules contains a consistent subsequence, and therefore, it 
suffices only to show that the vc dimension is o(n /  log n). This is a difficult task 
and some kernels  yield infinite vc dimension, even if d  =  I  and k  is  very  small 
(see Chapter 25). However, there is a simple argument if K  = IR  for a simple set 
R. Let 

A  =  {{x: X  = a + Ay, y  E  R}  : a  End, A  a d  x d matrix} . 

If R  is  a  sphere,  then  A  is  the  class  of all  ellipsoids.  The  number  of ways  of 
shattering  a  set  {x 1,  ... ,  xn }  by  intersecting  with members  from  A  is  not more 
than 

nd(d+ l)/2+ 1 

(see  Theorem  13.9,  Problem  13.10).  The  number  of ways  of shattering  a  set 
{Xl,  ... , x l1 }  by intersecting with sets of the form 

is  not more than  the product of all  ways  of shattering by intersections  with  RI, 
with R2,  and so forth,  that is, 

nk(d(d+1)/2+1) . 

The logarithm of the shatter coefficient is o(n) if k = o(n/ log n). Thus, by Corollary 
12.1, we have 

Theorem 30.16.  (KRzYZAK,  LINDER,  AND  LUGOSI  (1993)).  lfwe take  k  --+  00, 
k = o(n/ log n) in the  RBF classifier (4)  in which K  = IR ,  R  being the  unit ball of 

542 

30.  Neural Networks 

R d ,  and in which all the parameters are  chosen by empirical risk minimization, 
then E{L n }  -+  L * for all distributions of (X, Y).  Furthermore,  ijQk is the class of 
all RBF classifiers with k prototypes, 

E{L n }  -

inf  L(g) .:::;  16 
gEQk 

k(d(d + 1)/2 + 1) log n + log(8e) 
. 

2n 

The theorem remains valid (with modified constants in the error estimate) when 
R  is a hyperrectangle or polytope with a bounded number of faces. However, for 
more general K, the vc dimension is more difficult to evaluate. 

For general kernels, consistent RBF classifiers can be obtained by empirical L I or 
L2 error minimization (Problem 30.32). However, no efficient practical algorithms 
are known to compute the minima. 

Finally, as suggested by Chou and Chen (1992) and Kraaijveld and Duin (1991), 
it is  a good idea to place Xl,  ... ,Xk  by k-means clustering or another clustering 
method and to build an RBF classifier with those values or by optimization started 
at the given cluster centers. 

Problems and Exercises 

PROBLEM 30.1.  Let k, I be integers, with d  :::  k  <  n, and I  :::  (~). Assume X has a density. 

Let A be a collection of hyperplanes drawn from the e) possible hyperplanes through d 

points of {XI, ... , X k },  and let gA  be the corresponding natural classifier based upon the 
arrangement peA). Take I  such collections  A  at random and with replacement,  and pick 
the best A by minimizing Ln(gA), where 

Show that the selected classifier is consistent if I  --+  00, ld  = o(n), n / (llog k)  --+  00. (Note: 
this is applicable with k  =  L n /2 J, that is, half the sample is used to define A, and the other 
half is used to pick a classifier empilically.) 

PROBLEM 30.2.  We  are  given  a tree  classifier  with  k  internal linear  splits  and  k  + I  leaf 
regions  (a BSP  tree).  Show how to  combine the neurons in a two-hidden-Iayer perceptron 
with k  and  k  + I  hidden neurons in the  two  hidden layers  so  as  to  obtain a decision that 
is  identical  to  the  tree-based  classifier (Brent  (1991),  Sethi  (1990;  1991)).  For more  on 
the  equivalence of decision  trees  and neural  networks,  see Meisel  (1990),  Koutsougeras 
and  Papachristou (1989),  or Golea and Marchand (1990).  HINT:  Mimic the  argument for 
arrangements in text. 

PROBLEM 30.3.  Extend  the  proof of Theorem 30.4  so  that it includes  any  nondecreasing 
sigmoid with limx~_oo o-(x) = -1 and limx~oo o-(x) = 1.  HINT: If t is large, o-(tx) approx(cid:173)
imates the threshold sigmoid. 

PROBLEM  30.4.  This  exercise  states  denseness  in  L I (fJ.,)  for  any  probability  measure  fJ.,. 
Show that for every probability measure fJ.,  on R d, every measurable function f  : Rd  --+  R 

Problems and Exercises 

543 

with j  1!(x)I,u(dx)  <  00, and every E >  0, there exists a neural network with one hidden 
layer and function 1/J(x) as in (30.2) such that 

f I!(x} -1/f(x)I,u(dx) <  E 

(Hornik (1991). HINT:  Proceed as in Theorem 30.4, considering the following: 

(1)  Approximate! in L 1 (,u) by a continuous function g(x) that is zero outside some 
bounded set  B  C  1?f Since g(x) is bounded, its maximum f3  = maxxERd  Ig(x)1 
is finite. 

(2)  Now,  choose a  to be a positive number large enough so that both B  C  [-a, a]d 

and,u ([ -a, a]d)  is large. 

(3)  Extend the restriction of g(x) to  [-a, a)d  periodically by tiling  over  all  of nd. 

The obtained function, i(x) is a good approximation of g(x) in L J (,u). 

(4)  Take  the  Fourier  series  approximation  of i(x),  and  use  the  Stone-Weierstrass 

theorem as in Theorem 30.4. 

(5)  Observe that every continuous function on the real line that is zero outside some 
bounded interval can be arbitrarily closely approximated uniformly over the whole 
real line by  one-dimensional neural networks.  Thus, bounded functions  such as 
the  sine  and cosine functions  can be approximated arbitrarily closely by neural 
networks in L 1 (j)) for any probability measure  j)  on n. 

(6)  Apply the triangle inequality to finish the proof. 

PROBLEM 30.5.  Generalize the previous exercise for denseness in Lp(,u). More precisely, 
let  1  ::::  p  <  00.  Show  that  for  every  probability  measure  ,u  on nd
,  every  measurable 
function!: nd  -+  nwithjl!(x)IP,u(dx) <  oo,andeverYE  >  O,thereexistsaneural 
network with one hidden layer h(x) such that 

(1 I/(x) - hex)!, /L(dX») lip  <  E 

(Hornik (1991)). 

PROBLEM 30.6.  COMMITTEE  MACHINES.  Let  C(k)  be  the  class  of all  committee  machines. 
Prove that for all distributions of (X, Y), 

lim  inf  L(¢) = L *. 

k---'>oo  <jJEC(k) 

HINT: For a one-hidden-layer neural network with coefficients Ci  in (30.2), approximate the 
Ci'S  by discretization  (truncation to  a grid of values),  and note that  Ci 1/Ji (x )  may  thus be 
approximated in a committee machine by a sufficient number of identical copies of 1/Ji(X). 
This only forces the number of neurons to be a bit larger. 

PROBLEM  30.7.  Prove Theorem 30.5 for arbitrary sigmoids. HINT: Approximate the thresh(cid:173)
old sigmoid by a (t x) for a sufficiently large t. 

PROBLEM 30.8.  Let a  be  a  nondecreasing  sigmoid  with  a(x)  -+  -1 if x  -+  -00 and 
a(x) -+  1 if x  -+  00. Denote by C~k) the class of corresponding neural network classifiers 
with k hidden layers.  Show that  VC~k)  ~ VC(k),  where C(k)  is the class corresponding to the 
threshold sigmoid. 

544 

30.  Neural Networks 

PROBLEM 30.9.  This exercise generalizes Theorem 30.5 for not fully connected neural net(cid:173)
works  with one hidden layer.  Consider the class C(k)  of one-hidden-Iayer neural networks 
with the threshold sigmoid such that each of the k nodes in the hidden layer are connected to 
d], d2 ,  ••• , dk  inputs, where 1 ::::  di  ::::  d. More precisely, C(k)  contains all classifiers based 
on functions of the form 

1jf(x) = Co  + L Cio-(1jfi(X»,  where  1jfi(X) = L ajx(I1l;,j) , 

d; 

k 

i=] 

j=1 

where for each i, (mi,], ... , mi,d) is a vector of distinct positive integers not exceeding d. 
Show that 

if the a/s, c/s and mi,/s are the tunable parameters (Bartlett (1993». 

i=] 

PROBLEM 30.10.  Let 0- be a sigmoid that takes  m  different values.  Find upper bounds on 
the vc dimension of the class C~k). 

PROBLEM 30.11.  Consider a one-hidden-Iayer neural network 1jf. If (X], Y]),  ... , (Xn,  Yn) 
are  fixed  and all  Xi'S are  different,  show that with n  hidden neurons,  we  are always  able 
to tune the weights  such that Yi  =  1jf(Xi ) for all  i. (This remains true if YEn instead of 
Y  E  {O,  I}.)  The property above  describes  a situation of overfitting that occurs when the 
neural network becomes  too  "rich"-recall also  that  the vc dimension,  which is  at  least 
d  times  the  number  of hidden  neurons,  must remain  smaller than  n  for  any  meaningful 
training. 

PROBLEM  30.12.  THE BERNSTEIN  PERCEPTRON.  Consider the following perceptron for one(cid:173)
dimensional data: 

¢(x) =  {  ~  if L7=]  aixi(1  - X)k-i  >  1/2 

otherwise. 

Let us call this the Bernstein perceptron since it involves Bernstein polynomials. If n  data 
points  are  collected,  how  would  you  choose  k  (as  a  function  of n)  and  how  would  you 
adjust the weights (the ai's) to make sure that the Bernstein perceptron is consistent for all 
distributions  of (X, y) with PiX  E  [0,  In  =  I? Can you make the  Bernstein perceptron 
consistent for all distributions of (X, Y) on n x  {O,  I}? 

FIGURE 30.15.  The  Bernstein  per(cid:173)
ceptron. 

PROBLEM  30.13.  Use the ideas of Section 25.5 and Problem 25.11  to prove Theorem 30.8. 

545 
PROBLEM  30.14.  Consider  Specht's  Padaline  with  r  = rn  t  00.  Let  h  = hll  + 0,  and 
nhd  --+  00. Show that for any distribution of (X, Y), E{L,J  --+  L *. 

Problems and Exercises 

PROBLEM  30.15.  BOUNDED FIRST LAYERS.  Consider a feed-forward neural network with any 
number of layers, with only one restriction, that is, the first layer has at most k  <  d outputs 
Z1,  ... , Zk, where 

x  =  (x(l),  ... , X(d)) is the input, and (J'  is an arbitrary function n --+  n (not just a sigmoid). 
The integer k remains fixed.  Let A denote the k  x  (d + 1) matrix of weights aji, bj . 

(1) 

If L *(A) is  the  Bayes error for  a  recognition problem based upon (Z, Y),  with 
Z  = (Z1' ... , Zk) and 

then show that for some distribution of (X,  y), inf A  L * (A)  >  L *, where L * is the 
Bayes probability of error for (X, Y). 
If k  ~ d  however, show that for any strictly monotonically increasing sigmoid (J', 
inf A  L * (A) = L * . 

(2) 

(3)  Use (1) to conclude that any neural network based upon a first layer with k  <  d 
outputs is not consistent for some distribution, regardless of how many layers it 
has (note however, that the inputs of each layer are restricted to be the outputs of 
the previous layer). 

FIGURE 30.16.  The first  layer is  re(cid:173)

stricted  to  have  k  outputs.  It  has 
ked + 1) tunable parameters. 

Zj 

PROBLEM  30.16.  Find conditions on kll  and (311  that guarantee strong universal consistency 
in Theorem 30.9. 

PROBLEM  30.17.  BARRON  NETWORKS.  Call a Barron network a network of any number of 
layers  (as  in  Figure  30.13)  with  2  inputs  per  cell  and  cells  that  perform  the  operation 
a + (3x  + yy + 8xy on inputs x, yEn, with trainable weights a, (3,  y, 8. If 1jJ  is the output 

546 

30.  Neural Networks 

of a network with d inputs and k cells (arranged in any way), compute an upper bound for 
the vc dimension of the classifier g(x)  =  I{1/f(x»O},  as a function of d  and k.  Note that the 
structure of the network (i.e., the positions of the cells and the connections) is variable. 

PROBLEM  30.18.  CONTINUED.  Restrict the BalTon network to llayers and k  cells per layer 
(for kl cells total), and repeat the previous exercise. 

PROBLEM 30.19.  CONTINUED.  Find  conditions  on  k  and  l  in  the  previous  exercises  that 
would guarantee the universal consistency of the Barron network, if we train to minimize 
the empirical elTor. 

PROBLEM  30.20.  Consider the family of functions :fit of the form 
(il))] 

I L L Wij  (1h(x))j , 

.1,  ( 
'f'i  X 

)  _  " "  I 

- ~~Wii'J'  X 

k 

d 

I 

./ 

1 :s  i  :s  k, 

i=1 

j=1 

i'=1  J'=! 

where all the wi} 's and W~il J' 's are tunable parameters. Show that for every I  E  C[O,  l]d and 
E  >  0, there exist k,  llarge enough so that for some g  E  :Fk,/, sUPxE[O,ljd  I/(x) - g(x)1  :s  E. 
PROBLEM 30.21.  CONTINUED.  Obtain an upper bound  on the  vc dimension  of the  above 
two-hidden-Iayer network. HINT:  The vc dimension is  usually about equal to the number 
of degrees of freedom (which is (1  + d)lk here). 

PROBLEM  30.22.  CONTINUED. If gn is the rule obtained by empirical error minimization over 
:Fk,z, then show that L(gn)  -»  L * in probability if k  -»  00, l  -»  00, and kl = o(nj log n). 

PROBLEM 30.23.  How many different monomials of order r  in x(l), ... , xed)  are there? How 
does this grow with r when d  is held fixed? 

PROBLEM  30.24.  Show that 1/11,1/12,  and ¢ in the bit-interleaving example in the section on 
Kolmogorov-Lorentz representations are not continuous. Which are the points of disconti(cid:173)
nuity? 

PROBLEM  30.25.  Let 

Pick  {ai, C j}  by empirical risk minimization for  the classifier g(x) = I{1/f(x»O},  1/1  E  :F~l' 
ShowthatE{Ln} -»  L* for all distributions of (X,  Y)whenm -» ooandm = o(n 1
/ d jlogn). 
PROBLEM 30.26.  Write  (X(l)X(2))2  as  2:,;=1  (ailx(l) + ai2x(2))2  and identify the coefficients 
{ail, ai2}.  Show  that there is  an  entire  subspace  of solutions  (Diaconis  and  Shahshahani 
(1984)). 
PROBLEM 30.27.  Show  that  ex (1)x(2)  and  sin(x(1)x(2)  cannot be  written  in  the  form  2:,:=1 
1/Ii(a! x) for  any  finite  k,  where x  = (x(l), X(2))  and ai  E  R2,  1  :s  i  :s  k  (Diaconis  and 
Shahshahani (1984)).  Thus, the projection pursuit representation of functions can only at 
best approximate all continuous functions on bounded sets. 
PROBLEM 30.28.  Let:F be the  class  of classifiers  of the form  g  = I{1/f>o},  where  1/I(x) = 
al II (x(l)) + a2 12 (X(2»), for arbitrary functions 11,  12, and coefficients a1, a2  E R. Show that 
for some distribution of (X, Y) on R2  x  {O,  1}, infgE.F L(g)  >  L *, so that there is no hope 
of meaningful distribution-free classification based on additive functions only. 

Problems and Exercises 

547 

PROBLEM  30.29.  CONTINUED. Repeat the previous exercise for functions of the form 1jr (x) = 
ai 11 (x (2) ,X(3») + a2fz(x(1) , X(3») +a3h(x(l), x(2») and distributions of (X, Y) on R3 x {O,  l}. 
Thus, additive functions of pairs do not suffice either. 
PROBLEM 30.30.  Let  K  :  R  --+  R  be a nonnegative bounded kernel with f K (x )dx  < 
00.  Show  that  for  any  E  >  0,  any  measurable  function  I  : Rd  --+  R  and probability 
measure  fJ- such that f II(x)IfJ-(dx)  <  00,  there  exists  a function  1jr  of the form  1jr(x)  = 
:L;=I Ci K  (x - bi)T Ai(X  - bi)) + Co  such that f II(x) - 1jr(x)fJ-(dx)  <  E  (see Poggio and 
Girosi  (1990),  Park and Sandberg  (1991;  1993),  Darken,  Donahue,  Gurvits,  and  Sontag 
(1993),  Krzyzak,  Linder,  and Lugosi (1993) for such denseness results).  HINT:  Relate the 
problem to  a similar result for kernel estimates. 

PROBLEM 30.31.  Let C(k)  be the class of classifiers defined by the functions 

k L Ci K  (x - bil Ai(X - bi)) + Co· 

i=l 

Find upper bounds on its vc dimension when K  is an indicator of an interval containing the 
origin. 

PROBLEM 30.32.  Consider the class of radial basis function networks 

where  K  is  nonnegative,  unimodal, bounded,  and continuous.  Let Vr n  be  a  function  that 
minimizes I n(1jr)  = n- I :L7=1  11jr(Xi )  - Yi lover 1jr  E  F n, and define gn  as the corresponding 
classifier.  Prove that if kn  --+  00,  fJn  --+  00,  and knfJ;; 10g(knfJn)  =  o(n) as  n  --+  00, then 
gn  is universally consistent (Krzyzak, Linder, and Lugosi (1993)). HINT:  Proceed as in the 
proof of Theorem 30.9.  Use Problem 30.30 to handle the approximation error.  Bounding 
the covering numbers needs a little additional work. 

31 
Other Error Estimates 

In  this  chapter  we  discuss  some  alternative  error  estimates  that  have  been  in(cid:173)
troduced to improve on the performance of the  standard estimates-holdout, re(cid:173)
substitution,  and  deleted-we have  encountered so  far.  The first  group  of these 
estimates-smoothed and posterior probability estimates-are used for their small 
variance. However, we will give examples that show that classifier selection based 
on the  minimization of these estimates  may fail  even in  the  simplest situations. 
Among other alternatives, we deal briefly with the rich class of bootstrap estimates. 

31.1  Smoothing the Error Count 

The  resubstitution,  deleted,  and  holdout  estimates  of the  error probability  (see 
Chapters  22  and  23)  are  all  based on  counting the  number of errors  committed 
by the  classifier to be tested.  This  is  the  reason for the relatively large  variance 
inherently present in these estimates. This intuition is based on the following. Most 
classification rules can be written into the form 

(x) = {O 

gn 

if7Jn(x,  Dn)::::  1/2 

1  otherwise, 

where  7Jn (x, Dn)  is  either an  estimate  of the  a posteriori probability  7J(x),  as  in 
the case of histogram, kernel, or nearest neighbor rules; or something else, as for 
generalized linear, or neural network classifiers. In any case, if 7Jn (x,  Dn) is close 
to  1/2, then we feel  that the  decision is  less robust compared to  when the  value 
of 7Jn(x,  Dn) is far from  1/2. In other words, intuitively, inverting the value of the 

550 

31.  Other Error Estimates 

decision gn (x) at a point x, where 17n (x,  Dn) is close to  1/2 makes less difference 
in  the  elTor  probability,  than if l'7n(x,  Dn)  - 1/21  is  large.  The  elTor  estimators 
based on counting the number of elTors do not take the value of '7n (x) into account: 
they  "penalize" elTOfS  with the  same amount,  no matter what the value of '7n  is. 
For example, in the case of the resubstitution estimate 

each elTor contributes with 1/ n to the overall count. Now, if '7n (Xi,  Dn) is close to 
1/2, a small perturbation of Xi  can flip the decision gn(Xi ), and therefore change 
the  value  of the  estimate  by  1/ n,  although  the  elTor  probability  of the  rule  gn 
probably  does  not  change  by  this  much.  This  phenomenon  is  what  causes  the 
relatively large variance of elTor counting estimators. 

Glick (978) proposed a modification of the counting elTor estimates. The gen(cid:173)

eral form of his "smoothed" estimate is 

where r  is a monotone increasing function satisfying r( 1 /2 - u) = 1 - r( 1 /2 + u). 
Possible choices of the  smoothing function  r(u) are  r(u)  = u,  or r(u)  = I/O + 
e1/ 2- cu ),  where  the  parameter c  >  0  may  be  adjusted  to  improve  the  behavior 
of the  estimate  (see  also  Glick (978), Knoke  (1986),  or  Tutz  (1985)).  Both of 
these estimates give less penalty to elTOfS  close to the decision boundary, that is, 
to elTors where '7n  is close to  1/2. Note that taking r(u) =  I{u2::1/2}  cOlTesponds to 
the resubstitution estimate. We will see in Theorem 31.2 below that if r  is smooth, 
then L~S) indeed has a very small variance in many situations. 

Just  like  the  resubstitution  estimate,  the  estimate  L~S) may  be  strongly  opti(cid:173)

mistically biased.  Just consider the  I-nearest neighbor rule,  when  L~S) =  0 with 
probability one, whenever X has a density. To combat this defect, one may define 
the deleted version of the smoothed estimate, 

where  Dn,i  is  the  training  sequence with the  i-th pair (Xi, Yi )  deleted.  The first 
thing we notice is that this estimate is still biased, even asymptotically. To illustrate 
this point, consider r(u) = u. In this case, 

E {L~SD)} 

E {I{Y=O}'7n-l (X,  Dn-l) + I{Y=l}(1  -

'7n-1 (X, D n - 1))} 

=  E {(I  - 1J(X))17n-l(X,  Dn- 1)  + 1J(X)(I  - 1}n-l(X,  Dn- 1))}. 

If the estimate '7n-! (x,  Dn-l) was perfect, that is,  equal to  '7 (x ) for every x, then 
the expected value above would be 2E{n(X)(l - n(X))}, which is  the asymptotic 

error probability LNN  of the  I-NN rule. In fact, 

31.1 Smoothing the Error Count 

551 

IE {L~SD)} - LNNI 

IE {I{Y=O}1]n-1 (X, D n- 1) + I{Y=l}(1  - 1]n-1 (X,  Dn- 1)) 
-

(I{y=o}1](X) + I{Y=l}(1  - 1](X)))} \ 

<  21E {1]n-1 (X, Dn- 1) - 1](X)}I. 

This means that when the estimate 1]n (x) of the a posteriori probability of 1] (x ) is 
consistent in L1 (/-L),  then L~SD) converges to  L NN ,  and not to  L *! 

Biasedness of an error estimate is not necessarily a bad property.  In most ap(cid:173)

plications, all we care about is how the classification rule selected by minimizing 
the error estimate works.  Unfortunately,  in this respect,  smoothed estimates per(cid:173)
form poorly,  even compared to other strongly biased error estimates  such as  the 
empirical squared error (see Problem 31.4). The next example illustrates our point. 

Theorem 31.1.  Let the distribution of X  be concentrated on two values such that 
P{X = a} = P{X = b} = 1/2, and let 1](a) = 3/8 and 1](b) = 5/8. Assume that the 
smoothed error estimate 

is minimized over 1]'  E  F,  to select a classifier from F,  where the class F contains 
two functions,  the true a posteriori probability function  1],  and ij,  where 

ij(a) =0, 

3 
_ 
1](b) = 8' 

Then the probability that ij  is selected converges to one as n  ~ 00. 

PROOF.  Straightforward calculation shows that 

The statement follows from the law of large numbers.  0 

REMARK. The theorem shows that even if the true a posteriori probability function 
is contained in a finite  class  of candidates, the smoothed estimate with r(u) = u 
is  unable  to  select  a  good  discrimination  rule.  The  result  may  be  extended  to 
general  smooth  r's.  As  Theorems  15.1  and  29.2  show,  empirical  squared  error 
minimization or maximum likelihood never fail in this situation.  0 

Finally we demonstrate that if r  is smooth, then the variance of L~S) is  indeed 
small. Our analysis is based on the work of Lugosi and Pawlak (1994). The bounds 

552 

31.  Other Error Estimates 

for the variance of L~S) remain valid for L~SD). Consider classification rules of the 
form 

(x) = {O 

ifrJn(x, Dn)::::  1/2 

gn 

1  otherwise, 

where  1711 (x, Dn)  is  an  estimate  of the  a  posteriori  probability  17 (x ).  Examples 
include the histogram rule, where 

(see Chapters 6 and 9), the k-nearest neighbor rule, where 

(Chapters 5 and 11), or the kernel rule, where 

(Chapter 10).  In the sequel, we  concentrate on the performance of the  smoothed 
estimate of the error probability of these nonparametric rules.  The next theorem 
shows that for these rules, the variance of the smoothed error estimate is 0 (1/ n), no 
matter what the distribution is. This is a significant improvement over the variance 
of the deleted estimate,  which,  as  pointed out in Chapter 23,  can be larger than 
1/J2nn. 

Theorem 31.2.  Assume that the  smoothing function  r(u) satisfies ° ::::  r(u)  ::::  1 

for u  E  [0,  1],  and is uniformly Lipschitz continuous,  that is, 

Ir(u)  - r(v)1  ::::  clu - vi 

for  all  u,  v,  and for  some  constant  c.  Then  the  smoothed  estimate  L~S)  of the 
histogram,  k-nearest neighbor,  and moving window rules (with kernel  K  =  Iso,) 
satisfies 

and 

Var{L(S)}  <  -

n 

c 
- 4n' 

where C is a constant depending on the rule only.  In the case of the histogram rule 
the value of C  is  C  = (1  + 4c)2, for the k-nearest neighbor rule  C  = (1  + 2CYd)2, 
and for the  moving window  rule  C  = (1  + 2cf3d)2.  Here  c  is  the  constant in  the 
Lipschitz condition,  Yd  is  the  minimal number of cones centered at the  origin of 
angle n /6 that coverRd, and f3d  is the minimal number of balls of radius 1/2 that 
cover the unit ball in Rd. 

31.1  Smoothing the Error Count 

553 

REMARK.  Notice that the inequalities of Theorem 31.2 are  valid for all n, E,  and 
h  >  0 for the histogram and moving window rules, and k for the nearest neighbor 
rules.  Interestingly,  the  constant  C  does  not  change  with  the  dimension  in  the 
histogram  case,  but  grows  exponentially with d  for  the  k-nearest neighbor and 
moving window rules.  0 

PROOF.  The probability inequalities follow from  appropriate applications of Mc(cid:173)
Diarmid's  inequality.  The  upper  bound  on  the  variance  follows  similarly  from 
Theorem 9.3. We consider each of the three rules in tum. 
THE  HISTOGRAM  RULE.  Let (Xl, yd, ... , (xn, Yn)  be a fixed training sequence.  If 
we can show that by replacing the value of a pair (Xi,  Yi)  in the training sequence 
by some (x;, Y;)  the value of the estimate L~S) can change by at most (l + 2c)ln, 
then the inequality follows by applying Theorem 9.2 with Ci  = Sin. 

The i -th term of the sum in L~S) can change by one, causing 1 In  change in the 
average.  Obviously,  all  the  other terms  in  the  sum  that can change are  the ones 
corresponding to  the  Xj'S  that are  in the  same  set of the partition as  either Xi  or 
x;.  Denoting  the  number  of points  in  the  same  set  with  Xi  and xI  by  k  and  k' 
respectively,  it is  easy to  see that the estimate of the a posteriori probabilities in 
these points can change by at most 21 k  and 21 k', respectively.  It means that the 
overall change in the value of the sum can not exceed (1 +k ¥- +k' f,) In  = (1 +4c) In. 
THE K-NEAREST NEIGHBOR RULE.  To avoid difficulties caused by breaking distance 
ties, assume that X has a density. Then recall that the application of Lemma 11.1 for 
the empirical distribution implies that no X j  can be one of the k nearest neighbors 
of more  than  kYd  points  from  Dn.  Thus,  changing  the  value  of one  pair in  the 
training  sequence can change at most  1 + 2kYd  terms in the  expression of L~D), 
one of them by  at most 1,  and all the others by at most c I k.  Theorem 9.2 yields 
the result. 

THE MOVING WINDOW RULE.  Again, we only have to check the condition of Theorem 
9.2  with  Ci  =  (1  + 2c{3d)/n.  Fix  a  training  sequence  (Xl, Yl),  '" 
, (xn, Yn)  and 
replace the pair (Xi,  Yi) by (x;, Y;>'  Then the i -th term in the sum of the expression 
of L~S) can change by  at most one.  Clearly,  the  j-th term,  for  which  Xj  fj  SXi. h 
and Xj  fj Sx;.h'  keeps its value. It is easy to see that all the other terms can change 
by  at most c  . max {I I k j, 1 I kj}, where  k j  and  kj  are  the  numbers  of points  Xk, 
k  =I i,  j, from the training sequence that fall in SXi.h and Sx;.h'  respectively. Thus, 
the overall change in the sum does not exceed 
1 +  "  ~ +  " 
L 

~. 
k'. 
] 

L..  k· 
] 
XjESXi.h 

XjESX;.h 

It suffices  to  show  by  symmetry  that  LXES 
i,  j  : Xk  E  SXi,h  n SXj,h}l.  Then clearly, 

) 

Xi,h 

llkj  S  (3d.  Let  nj  =  I{Xb  k  =I 

1 Lk-SLn. 

XjESxi,h 

] 

XjESxj.h 

] 

554 

31.  Other Error Estimates 

To  bound the right-hand side from  above,  cover  SXj,h  by  fJd  balls  Sl, ... , Sf3d  of 
radius hj2. Denote the number of points falling in them by lm  (m  =  1,  ... , fJd): 

lm  =  I{Xk,  k i  i  : Xk  E  SXj,h  n Sm}l· 

Then 

and the theorem is proved.  0 

31.2  Posterior Probability Estimates 

The error estimates discussed in this section improve on the biasedness of smoothed 
estimates, while preserving their small variance. Still, these estimates are of ques(cid:173)
tionable utility in classifier selection. Considering the formula 

for the error probability of a classification rule gn(x) =  1{1Jn(x,Dn»1/2} , it is plausible 
to introduce the estimate 

L(P)  = 

n 

1  n - L (I{1Jn(X; ,Dn)sl/2}17n (Xi , Dn) 

n  i=l 
+ 1{1Jn(X;,Dn»1/2}(1  - 17n(Xi ,  Dn») , 

that is,  the expected value is  estimated by  a  sample average,  and instead of the 
(unknown) a posteriori probability YJ(X),  its estimate 17n (x,  Dn) is plugged into the 
formula of Ln. The estimate L?) is usually called the posterior probability error 
estimate. In the case of nonparametric rules such as  histogram, kernel,  and k-NN 
rules it is natural to use the corresponding nonparametric estimates of the a poste(cid:173)
riori probabilities for plugging in the expression of the error probability. This, and 
similar estimates of Ln  have been introduced and studied by Fukunaga and Kessel 
(1973), Rora and Wilcox (1982), Fitzmaurice and Rand (1987), Ganesalingam and 
McLachlan (1980), Kittler and Devijver (1981), Matloff and Pruitt (1984), Moore, 
Whitsitt, and Landgrebe (1976), Pawlak (1988), Schwemer and Dunn (1980), and 
Lugosi  and  Pawlak  (1994).  It is  interesting to  notice  the  similarity between  the 
estimates L~S) and L~P), although they were developed from different scenarios. 

To  reduce  the  bias,  we  can  use  the  leave-one-out,  (or deleted)  version  of the 

estimate, 

L(PD)  = 

n 

1  n - L (I{1Jn-l(X;,D/l,i)Sl/2}17n-1 (Xi,  Dn,J) 

n  i==l 
+ I{1Jn-l (Xi ,Dn,;»1/2}(1  - 17n-1(Xi , Dn,i»)' 

The deleted version L~P D),  has a much better bias than L~SD). We have the bound 

31.2 Posterior Probability Estimates 

555. 

IE{L~PD)} -ELn-l! 

IE {I{gn(X)=O}1]n-l (X,  Dn-1) + I{gn(X)=l}(1  - 1]n-1 (X,  Dn- 1)) 
-

(I{gll(X)=O}1](X) + I{g,,(x)=l}(1  - 17(X)))}! 

<  21E {1]n-l (X, Dn-d - 1](X)}I· 

This means that if the estimate 1]n (x) of the a posteriori probability of 1] (x ) is con(cid:173)
sistent in L 1 (/1), then EL~P D) converges to L *. This is the case for all distributions 
for  the  histogram  and moving window rules  if h  -+  0  and  nh d  -+  00,  and  the 
k-nearest neighbor rule if k  -+  00 and k / n  -+ 0, as it is seen in Chapters 9, 10, and 
11. For specific cases it is possible to obtain sharper bounds on the bias of L~P D). 
For the histogram rule, Lugosi and Pawlak (1994) carried out such analysis. They 
showed for example that the estimate L~PD) is optimistically biased (see Problem 
31.2). 

Posterior  probability  estimates  of  Ln  share  the  good  stability  properties  of 

smoothed estimates (Problem 31.1). 

Finally, let us  select a function  1]'  :  nd  -+  [0, 1]-and a  corresponding rule 
g(x)  =  Ih '(x»lj2}-from a  class  F  based on the  minimization of the posterior 
probability error estimate 

Observe that L~P)(1]') = 0 when 1]'(x)  E  {O,  I} for all x, that is, rule selection based 
on this estimate just does not make sense. The reason is that L~P) ignores the Yi's 
of the data sequence! 

Fukunaga and Kessel (1973) argued that efficient posterior probability estima(cid:173)

tors  can  be  obtained  if additional  unclassified  observations  are  available.  Very 
often in practice, in addition to the training sequence  Dn,  further feature  vectors 
Xn+1 ,  ... ,  Xn+Z  are given without their labels  Yn+1,  ••• ,  Yn+Z, where the  Xn+i  are 
i.i.d., independent from X and  Dn, and they have the same distribution as that of 
X.  This  situation is  typical  in medical  applications,  when large  sets  of medical 
records are available, but it is usually very expensive to get their correct diagnosis. 
These unclassified samples can be efficiently used for testing the performance of 
a classifier designed from  Dn  by using the estimate 

= 

1  Z 

I L (I{7Jn(Xn+i,Dn)~lj2}1]n(Xn+i' Dn) 

i=l 

+ I{7Jn(Xn+i,Dn»lj2}(1  - 1]n(Xn+i ,  Dn))) . 

Again, using L~~) for rule selection is meaningless. 

556 

31.  Other Error Estimates 

31.3  Rotation Estimate 

This method,  suggested by Toussaint and Donaldson (1970), is  a combination of 
the holdout and deleted estimates. It is sometimes called the n -estimate. Let s  <  n 
be a positive integer (typically much smaller than n), and assume, for the sake of 
simplicity,  that  q  = n / s  is  an  integer.  The  rotation  method  forms  the  holdout 
estimate,  by holding  the  first  s  pairs  of the  training data out,  then  the  second s 
pairs,  and so forth.  The estimate is  defined by averaging the q  numbers obtained 
this way. To formalize, denote by D~~j the training data, with the J-th s-block held 
out (j = 1,  ... , q): 

D,~~j = ((Xl, Yd, ... , (Xs(j-l), YS(j-l), (Xs)+l,  Ys)+l),  ... , (Xn,  Yn». 

The estimate is defined by 

1  q  1 

L (D)  - L  L  I 

s) 

n s  -
, 

q 

{gll-s(X"DIl  )=tYi} 

• 

. 

(s) 
' 

-
S 

)=1 

i=s(j-l)+l 

s  = 1 yields the deleted estimate. If s  >  1, then the estimate is usually more biased 
than the deleted estimate, as 

but usually exhibits smaller variance. 

EL~~J = ELn- s, 

31.4  Bootstrap 

Bootstrap methods for estimating the misc1assification error became popular fol(cid:173)
lowing  the  revolutionary  work  of Efron  (1979;  1983).  All  bootstrap  estimates 
introduce artificial randomization. The bootstrap sample 

D (b)  = ((X(b)  y(b) 

In 

1 

1 

' 

,  ... , 

(X(b)  yCb)) 

In' 

In 

is a sequence of random variable pairs drawn randomly with replacement from the 
set {(Xl, Yd, ... , (Xn,  Yn)}.  In other words, conditionally on the training sample 
Dn  = ((Xl, YI ), ... , (Xn, Yn», the pairs (xib
) ,  Y?)  are drawn independently from 
Vn , the empirical distribution based on Dn  in n d  x  {O,  I}. 

One of the standard bootstrap estimates aims to  compensate the (usually opti(cid:173)

mistic) bias 

B(gn) = E{L(gn)}  - L?)(gn) 

of the resubstitution estimate L~R). To  estimate B(gn), a bootstrap sample of size 
m = n may be used: 

Often, bootstrap sampling is  repeated several times  to  average out effects of the 
additional randomization. In our case, 

31.4 Bootstrap 

557 

yields the estimator 

L~B)(gn) = L~R)(gn) - Bn(gn). 

Another instance of a bootstrap sample of size n, the so-called EO estimator, uses 
resubstitution  on  the  training  pairs  not  appearing  in  the  bootstrap  sample.  The 
estimator is defined by 

Here,  too,  averages  may be  taken  after generating  the  bootstrap  sample  several 
times. 

Many  other  versions  of bootstrap  estimates  have  been  reported,  such  as  the 
"0.632  estimate,"  "double  bootstrap,"  and  "randomized  bootstrap"  (see  Hand 
(1986), Jain, Dubes, and Chen (1987), and McLachlan (1992) for surveys and ad(cid:173)
ditional references). Clearly, none of these estimates provides a universal remedy, 
but for several specific classification rules, bootstrap estimates have been experi(cid:173)
mentally found to outperform the deleted and resubstitution estimates. However, 
one  point has  to  be  made clear:  we  always  lose information with the  additional 
randomization. We summarize this in the following  simple general result: 

Theorem 31.3.  Let X I,  ... ,  Xn  be  drawn from  an  unknown distribution  JL,  and 
let a(fL) be afunctional to be estimated.  Let r(·) be a convex risk function  (such 
as r(u)  =  u2 or r(u)  =  luI}.  Let Xib),  ... XI~) be a bootstrap sample drawn from 
Xl, ... , X n .  Then 

The theorem  states that no matter how  large  m  is,  the  class  of estimators that 
are  functions  of the  original  sample  is  always  at  least  as  good  as  the  class  of 
all estimators that are based upon bootstrap samples. In our case, a(JL)  plays the 
role  of the  expected error probability ELn  =  P{gn(X)  =I  Y}.  If we  take  r(u)  = 
u2 ,  then it follows  from the theorem that there is  no  estimator Ln  based on the 
bootstrap sample D,~) whose squared error E  {(ELn  - Lm)2}  is smaller than that 
of some nonbootstrap estimate.  In the proof of the theorem we  construct such a 
non-bootstrap estimator. It is clear, however, that, in general, the latter estimator is 
too complex to have any practical value. The randomization of bootstrap methods 
may  provide  a useful  tool  to  overcome  the  computational difficulties  in  finding 
good estimators. 

558 

31.  Other Error Estimates 

PROOF. Let 1jr  be any mapping taking m  arguments. Then 

E {r (1jr(xib

),  ... X}~»  - a(JL»)  lXI, ... , Xn} 

= 

1 
m 
n 

I: 

(iJ, .... im)c{I, ... ,n}m 

r(1jr(XiJ , ... , Xi,,)  - a(JL» 

(by Jensen's inequality and the convexity of r) 

r (~  I: 
r (1jr*(X1,  ... ,  Xn)  - a(JL») 

(iJ, ... ,im)c{I, ... ,n}m 

n 

1jr(XiJ , ... , XiJ - a(JL») 

= 

where 

Now,  after taking expectations with respect to  Xl, ... , X n ,  we see that for every 
1jr  we start out with, there is a 1jr*  that is at least as good.  0 

I 

' 

1 

,  ... ,  Xm  ' Ym' 

If m =  0 (n), then the bootstrap has an additional problem related to the coupon 
collector problem. Let N  be the number of different pairs in the bootstrap sample 
(X (b)  y(b» 
en, 1  m  "'-'  en lor some constant c,  t  en N  n  --+ 
1 - e-c  with probability one. To see this, note that 
l)Cn 
;; 

(b»  Th'f 

""  ne-c

(b) 

h 

( 
E{n - N} =  n  1 -

, 

~ 

/ 

so E{ N / n}  --+  1 - e -c. Furthermore, if one of the m  drawings is varied, N changes 
by at most one. Hence, by McDiarmid's inequality, for E  >  0, 

from which we conclude that N / n  --+  1- e-c with probability one. As n (1  - e-c ) 
of the  original  data pairs  do  not appear in the  bootstrap  sample,  a considerable 
loss  of information  takes  place  that  will  be  reflected  in  the  performance.  This 
phenomenon is  well-known,  and motivated several modifications of the simplest 
bootstrap estimate.  For more information,  see the  surveys by Hand (1986), Jain, 
Dubes, and Chen (1987), and McLachlan (1992). 

Problems and Exercises 

559 

Problems and Exercises 

PROBLEM  31.1.  Show that the posterior probability estimate L~P) of the histogram, k-nearest 
neighbor, and moving window rules satisfies 

and 

Var{L (P)}  <  .£ 
- 4n' 

n 

where C is a constant, depending on the rule. In the case of the histogram rule the value of 
C  is  C  = 25, for the k-nearest neighbor rule C  = (1  + 2Yd)2,  and for the moving window 
rule C  = (1  + 2f3d)2.  Also show that the deleted version L~P D)  of the estimate satisfies the 
same inequalities (Lugosi and Pawlak (1994». HINT:  Proceed as  in the proof of Theorem 
31.2. 

PROBLEM  31.2.  Show that the deleted posterior probability estimate of the error probability 
of a histogram rule is always optimistically biased, that is,  for all n,  and all  distributions, 
E {L~PD)} :s  EL n- l . 
PROBLEM  31.3.  Show that for any classification rule and any estimate 0  :s  TJn(x,  Dn)  :s  1 
of the a posteriori probabilities, for all distributions of (X,  Y) for alli, n, and E  >  0 

p  { IL~~) - E {L~~)} I :::  E I Dn}  :s  2e-2lE2

, 

and Var {L~~)IDn} :::;  1/ I.  Further, show that for alII, E  {L~~)} = E  {L~~~)}. 

PROBLEM  31.4.  EMPIRICAL  SQUARED ERROR.  Consider the deleted empirical squared error 

Show that 

2} 
E{en} = 2  + E  (TJ(X)  - 17n-l(X,  Dn-d) 

LNN 

{ 

, 

where  LNN  is  the  asymptotic  error probability  on  the  I-nearest neighbor rule.  Show that 
if TJn-J  is the histogram, kernel, or k-NN  estimate,  then Var{en}  :s  c/n for some constant 
depending on the dimension only. We see that en  is an asymptotically optimistically biased 
estimate  of L(gn)  when  TJn-J  is  an  L 2(p,)-consistent  estimate  of TJ.  Still,  this  estimate is 
useful in classifier selection (see Theorem 29.2). 

32 
Feature Extraction 

32.1  Dimensionality Reduction 

So far,  we have not addressed the question of how the components of the feature 
vector X are obtained. In general, these components are based on d measurements 
of the object to  be classified.  How  many measurements  should be made? What 
should these measurements be? We study these questions in this chapter. General 
recipes are hard to give as the answers depend on the specific problem. However, 
there are some rules of thumb that should be followed. One such rule is that noisy 
measurements, that is, components that are independent of Y,  should be avoided. 
Also, adding a component that is a function of other components is useless. A nec(cid:173)
essary and sufficient condition for measurements providing additional information 
is given in Problem 32.1. 

Our goal, of course, is to make the error probability L(gn) as small as possible. 
This depends on many things, such as the joint distribution of the selected compo(cid:173)
nents and the label Y, the sample size, and the classification rule gn' To make things 
a bit simpler,  we  first  investigate the  Bayes errors corresponding to  the  selected 
components. This approach makes sense,  since the Bayes error is the theoretical 
limit of the  performance  of any  classifier.  As  Problem  2.1  indicates,  collecting 
more measurements cannot increase the Bayes error.  On the  other hand,  having 
too many components is not desirable. Just recall the curse of dimensionality that 
we often faced:  to get good error rates, the number of training samples should be 
exponentially large in the number of components. Also, computational and storage 
limitations may prohibit us from working with many components. 

We may formulate the feature selection problem as follows:  Let X(l), ... , Xed) 

562 

32.  Feature Extraction 

be random variables representing d  measurements.  For a  set A  ~ {I, ... ,d} of 
indices, let X A  denote the I A I-dimensional random vector, whose components are 
the X(i)'s with i  E  A (in the order of increasing indices). Define 

L *(A) = 

inf 

g:RJAI--+{O, I} 

P{g(XA )  =I Y} 

as the Bayes error corresponding to the pair (XA' Y). 

L*({3}) 

FIGURE 32.1.  A  possible ar(cid:173)
rangement  of  L*(A)'s  for 
d  = 3. 

L*({1,3}) 

112 

L*(0) 

L* ({I}) 

o 

Obviously,  L *(A)  ::s  L *(B)  whenever  B  C  A,  and  L *(0)  =  min(P{Y  = 
OJ, P{Y  = I}).  The problem is to find  an efficient way of selecting an index set 
A with  IAI  =  k,  whose corresponding Bayes error is  the  smallest.  Here  k  <  d 
is  a fixed  integer.  Exhaustive  search through the  (~) possibilities  is  often unde(cid:173)
sirable because of the imposed computational burden.  Many attempts have been 
made to find fast algorithms to obtain the best subset of features. See Fu, Min, and 
Li (1970), Kanal (1974), Ben-Bassat (1982), and Devijver and Kittler (1982) for 
surveys. It is  easy to see that the best k  individual features-that is,  components 
corresponding to the k smallest values of L * ({ i} )-do not necessarily constitute the 
best k-dimensional vector: just consider a case in which X(l)  ==  X(2)  ==  ... ==  X(k). 
Cover and Van Campenhout (1977) showed that any ordering of the 2d  subsets of 
{l, ... , d}  consistent with the obvious requirement L *(A)  :::  L *(B) if B  ~ A  is 
possible. More precisely, they proved the following surprising result: 

Theorem 32.1.  (COVER AND  VAN CAMPENHOUT (1977)).  Let AI, A 2, ... ,  A2d  be 
an  ordering  of the  2d  subsets  of {I, ... ,d},  satisfying  the  consistency property 
i  <  j  if Ai  C  A j •  (Therefore,  Al  =  0 and A2d  =  {I, ... , d}.)  Then  there exists a 
distribution of the random variables (X, Y) =  (X(!), ... , Xed),  Y) such that 

The  theorem  shows  that every  feature  selection algorithm  that finds  the  best k(cid:173)
element subset has to search exhaustively through all k-element subsets for some 
distributions. Any other method is doomed to failure for some distribution. Many 
suboptimal, heuristic algorithms have been introduced trying to avoid the compu(cid:173)
tational demand of exhaustive search (see, e.g., Sebestyen (1962), Meisel (1972), 

32.1  Dimensionality Reduction 

563. 

Chang (1973), Vilmansen (1973), and Devijver and Kittler (1982». Narendra and 
Fukunaga (1977) introduced an efficient branch-and-bound method that finds the 
optimal set of features. Their method avoids searching through all subsets in many 
cases  by making use of the  monotonicity of the Bayes error with respect to  the 
partial ordering of the subsets. The key of our proof ofthe theorem is the following 
simple lemma: 

Lemma 32.1.  Let AI,""  A2d  be as in Theorem 32.1.  Let 1  <  i  <  2d.  Assume 
that the distribution of (X, Y) on nd  x  {O,  I}  is such that the distribution of X is 
concentrated on a finite  set,  L *(A2d)  = L *({l, ... , 2d})  = 0,  and L *(A j)  <  1/2 
for each i  <  }  :::  2d.  Then there exists another finite distribution such that L * (A j  ) 
remains unchanged for each}  >  i,  and 

* 

L(A j )<L(Ai )<2  foreach}>i. 

* 

1 

PROOF. We denote the original distribution of X by {vi and the a posteriori probability 
function by  fJ.  We  may assume without loss  of generality that every atom of the 
distribution  of X  is  in  [0,  M)d  for  some  M  >  0.  Since  L *({l, ... , d})  =  0,  the 
value of fJ(x)  is either zero or one at each atom. We construct the new distribution 
by duplicating  each atom in a special way.  We describe the new  distribution by 
defining a measure {vi'  on nd and an a posteriori function  fJ'  : nd  ~ {O,  I}. 
Define the vector VAi  E  nd  such that its m-th component equals  M  if m  1:.  Ai, 
and  zero  if m  E  Ai.  The  new  measure  {vi'  has  twice  as  many  atoms  as  {vi.  For 
each atom x  E  nd  of {vi,  the new measure  {vi!  has two atoms, namely, Xl  = x  and 
X2  = X  + VA i •  The new distribution is specified by 

{vi' (xd = q {vi (x ), 

{vi' (X2)  = (1  - q ) {vi (x ), 

fJ' (xd = fJ(x),  and  fJ' (X2)  = 1 -

fJ(x), 

where q  E  (0,  1/2) is specified later. It remains for us to verify that this distribution 
satisfies the requirements of the lemma for some q. First observe that the values 
L *(A j)  remain  unchanged  for  all  }  >  i.  This  follows  from  the  fact  that  there 
is  at  least  one  component  in  A j  along  which  the  new  set  of atoms  is  strictly 
separated from  the old one,  leaving the  corresponding contribution to  the Bayes 
error unchanged. On the other hand, as we vary q from zero to  1/2, the new value 
of L *(Ai)  grows  continuously  from  the  old  value  of L *(AJ to  1/2. Therefore, 
since by assumption max j>i L *(A j)  <  1/2, there exists a value of q  such that the 
new  L *(Ai) satisfies maXj>i  L *(A j) <  L *(Ai)  <  1/2 as desired.  0 

PROOF OF  THEOREM  32.1.  We construct the  desired distribution in 2d  - 2 steps, 
applying Lemma 32.1 in each step. The procedure for d = 3 is illustrated in Figure 
32.2. 

564 

32.  Feature Extraction 

CD  Ag ={ 1,2,3} 

CD 

CD 

CD 

CD 

CD  As={2} 

!---<;> 

I 
I 

I 
I 

CD 

FIGURE 32.2.  Construction  of a prespecijied ordering.  In  this  three-dimensional 
example, the first four steps of the procedure are shown, when the desired ordering 
is  L*({1, 2,  3})  ~ L*({2, 3})  ~ L*({1, 2})  ~ L*({2})  ~ L*({1, 3})  ~ .... Black 
circles represent atoms with 11  = 1 and white circles are those with 11  = O. 

We start with a monoatomic distribution concentrated at the origin, with 11(0)  = 
1. Then clearly, L *(A2d) = O. By Lemma 32.1, we construct a distribution such that 
L*(A2d) =OandO= L*(A2d)  <  L*(A2d_d <  1/2. By applying the lemma again, 
we can construct a distribution  with 0  =  L *(A2d)  <  L *(A2d-l)  <  L *(A2d_2)  < 
1/2. After i  steps,  we have a distribution satisfying the last i  inequalities of the 
desired ordering. The construction is finished after 2d  - 2 steps.  0 

REMARK.  The  original example  of Cover and  Van  Campenhout (1977)  uses  the 
multidimensional  gaussian  distribution.  Van  Campenhout (1980)  developed  the 
idea further by showing that not only all possible orderings, but all possible values 
of the L *(Ai)'S can be achieved by some distributions. The distribution constructed 
in the above proof is discrete. It has 22d -2 atoms.  0 

One  may  suspect  that feature  extraction is  much easier if given  Y,  the  com(cid:173)
ponents  X(1),  ... ,  X(d)  are  conditionally independent.  However,  three  and four(cid:173)
dimensional examples given by Elashoff, Elashoff, and Goldman (1967), Toussaint 
(1971),  and Cover (1974)  show  that even the individually best two  independent 
components are not the best pair of components. We do not know if Theorem 32.1 

32.1  Dimensionality Reduction 

565 

generalizes to the case when the components are conditionally independent. In the 
next example, the pair of components consisting of the two worst single features 
is the best pair, and vice versa. 

Theorem 32.2.  (TOUSSAINT (1971)).  There exist binary-valued random variables 
Xl, X 2 ,  X3, Y  E  {O,  I}  such  that Xl, X 2,  and X3  are conditionally  independent 
(given Y),  and 

L *({1})  <  L *({2})  <  L *({3}), 

but 

L *({1, 2})  >  L *({1, 3})  >  L *({2, 3}). 

PROOF.  Let P{Y =  I}  =  1/2. Then the joint distribution of Xl, X2,  X3,  Y is spec(cid:173)
ified  by the  conditional  probabilities  P{Xi  = 11Y  = O}  and P{Xi  = llY  = 1}, 
i  =  1, 2, 3.  Straightforward calculation shows that the values 

P{XI = llY = O}  = 0.1, 
P{X2 = IIY = O}  = 0.05, 
P{X3 = IIY = O}  = 0.01, 

P{XI = IIY = I} = 0.9, 
P{X2 = IIY = 1} = 0.8, 
P{X3 = IIY = I} = 0.71 

satisfy the stated inequalities.  0 

As our ultimate goal is to minimize the error probability, finding the feature set 
minimizing the Bayes error is not the best we can do. For example, ,if we know that 
we will use the 3-nearest neighbor rule, then it makes more sense to select the set 
of features that minimizes the asymptotic error probability L3NN  of the 3-nearest 
neighbor rule. Recall from Chapter 5 that 

L3NN  = E {17(X)(1  - 17(X))(1 + 417(X)(1  - 17(X))) }  . 

The situation here is even messier than for the Bayes error.  As the next example 
shows, it is not even true that A  C  B  implies L3NN(A)  ::::  L3NN(B), where A, B  ~ 
{I, ... ,d} are  two  subsets  of components,  and  L3NN(A)  denotes  the  asymptotic 
error probability of the 3-nearest neighbor rule for (XA' Y). In other words, adding 
components may increase L3NN!  This can never happen to the Bayes error-and in 
fact, to any f -error (Theorem 3.3). The anomaly is due to the fact that the function 
x(1  - x)(1 + 4x(1  - x)) is convex near zero and one. 

EXAMPLE.  Let the joint distribution of X  =  (Xl, X2)  be uniform on [0,2]2. The 
joint distribution of (X, Y) is defined by the a posteriori probabilities given by 

17(x) = 
[ 

0.1 
~ 
0.9 

if x  E  [0,  1/2) x  [0,  1/2) 
if x  E  [1/2, 1]  x  [0,  1/2) 
if x  E  [0,  1/2) x  [1/2, 1] 
if x  E  [1/2 x  1]  x  [1/2, 1] 

566 

32.  Feature Extraction 

(Figure 32.3). Straightforward calculations show that L3NN({I,  2})  = 0.0612, while 
L3NN({2}) = 0.056525, a smaller value!  0 

FIGURE 32.3.  Anexamplewhen 
an additionalfeature increases 
the error probability of the 3-
NN rule. 

T\=0.9 

L3NN  ({ 1,2))= 0.0612 
L'NN  ((2})  = 0.056525 

~~--------~------~ 

r1'=O.l 

Of course,  the  real  measure  of the  goodness of the  selected feature  set is  the 
error probability L(gn) of the classifier designed by using training data Dn. If the 
classification rule gn is not known at the stage of feature selection, then the best one 
can do is to estimate the Bayes errors L *(A) for each set A of features, and select a 
feature set by minimizing the estimate. Unfortunately, as Theorem 8.5 shows, no 
method of estimating the Bayes errors can guarantee good performance. If we know 
what classifier will be used after feature selection, then the best strategy is to select 
a set of measurements based on comparing estimates of the error probabilities. We 
do not pursue this question further. 

For special cases, we do not need to mount a big search for the best features. Here 
is  a simple example:  given  Y  =  i,  let X  =  (X(1),  ... , Xed))  have d  independent 
components,  where  given  Y  =  i,  X(J)  is  normal  (m ji, a}).  It is  easy to  verify 
(Problem 32.2) that if P {Y = I}  = 1/2, then 

L*=P{N>~}, 

where N  is a standard normal random variable, and 

~(m.l-m.o)2 

] 

r2  = L 
j=l 

] 

CYj 

is  the  square  of the  Mahalanobis  distance  (see  also  Duda  and  Hart  (1973,  pp. 
66-67)). For this case, the quality of the j-th feature is measured by 

We  may as  well rank these values,  and given that we need only d'  <  d  features, 
we are best off taking the d' features with the highest quality index. 

32.2 Transformations with Small Distortion 

567 

It is possible to come up with analytic solutions in other special cases as  well. 
For example, Raudys (1976) and Raudys and Pikelis (1980) investigated the de(cid:173)
pendence of E{ L(gn)} on the dimension of the feature space in the case of certain 
linear classifiers  and  normal  distributions.  They  point out that for  a fixed  n,  by 
increasing  the  number of features,  the  expected error probability  E{L(gn)}  first 
decreases, and then, after reaching an optimum, grows again. 

32.2  Transformations with Small Distortion 

One may view the problem of feature extraction in general as the problem of find(cid:173)
ing a transformation (i.e.,  a function)  T  : nd  -+ nd  so that the Bayes error L~ 
corresponding to the pair (T(X), Y) is close to the Bayes error L * corresponding to 
the pair (X, Y).  One typical example of such transformations is fine quantization 
(i.e., discretization) of X, when T  maps the observed values into a set of finitely, 
or countably infinitely many values. Reduction of dimensionality of the observa(cid:173)
tions  can be put in this framework as  well.  In the  following  result we show that 
small distortion of the observation cannot cause large increase in the Bayes error 
probability. 

Theorem 32.3.  (FARAG6  AND  GYORFI  (1975)).  Assume  that for  a  sequence  of 
transformations Tn,  n =  1,2, ... 

in probability,  where  II  . II  denotes  the Euclidean norm in nd.  Then,  if L * is the 
Bayes error for (X, Y) and L~n is the Bayes error for (Tn (X), Y), 

L*  -+ L*. 

T" 

PROOF. For arbitrarily small E  >  0 we can choose a uniformly continuous function 
o :::;  ij(x) :::;  1 such that 

2E {lry(X) -

ij(X)I} <  E. 

For any transformation T, consider now the decision problem of (T(X), Y), where 
the random variable Y satisfies P{Y = 11 X = x} = ij(x) for every x  E  nd. Denote 
the  corresponding Bayes error by  i~, and the Bayes error corresponding to  the 
pair (X, Y)  by i *. Obviously 

0:::;£*  -£*:::;1£*  -i*I+li*  -i*I+li*-L*I. 

(32.1) 

Tn 

T" 

Tn 

Tn 

To  bound the  first  and  third  terms  on  the  right-hand  side,  observe  that  for  any 
transformation T, 

P{Y = 11 T(X)} = E {P{Y = l1X}1 T(X)} = E {ry(X)1 T(X)} . 

568 

32.  Feature Extraction 

Therefore, the Bayes error corresponding to the pair (T(X), Y) equals 

L~ = E {min (E {ry(X)IT(X)}, 1 - E {ry(X)IT(X)})}. 

Thus, 

IL~ - i~1 

IE {min (E {ry(X)1 T(X)} , 1 - E {ry(X)1 T(X)})} 

- E {min (E {ry(X)IT(X)}, 1 - E {ry(X)IT(X)})}1 

<  E {IE {ry(X)IT(X)} - E {ry(X)IT(X)}1} 

<  E {lry(X) -

ry(X)I} 

(by Jensen's inequality) 

< 

E, 

so  the  first  and third terms of (32.1)  are less than  E.  For the second term,  define 
the decision function 

gn (x) = 

{ 

if ry(x)  :::;  1/2 

0 
1  otherwise, 

which has  error probability i(gn) = P{gn(Tn(X))  =I  Y}.  Then i(gn)  ~ ii"  and 
we have 

by Theorem 2.2. All we have to show now is that the limit supremum of the above 
quantity does not exceed E.  Let o( E) be the inverse modulus of continuity of the a 
posteriori probability ry,  that is, 

O(E)= sup {llx 

yll  :2Iry(x)-ry(y)1  <E}. 

For every E >  0, we have O(E)  >  0 by the uniform continuity of ry.  Now, we have 

2E {lry(7;1(X)) -

ry(X)1} 

:::;  2E {I{IIX-I;,(X)II>8(E)}} + 2E {I{IIX-I;,(X)II:::8(E)}  Iry(Tn(X))  -

ry(X)I} . 

Clearly,  the  first  term  on  the  right-hand  side  converges  to  zero  by  assumption, 
while the second term does not exceed E by the definition of O(E).  0 

REMARK. It is clear from the proof of Theorem 32.3 that everything remains true 
if the observation X takes its values from a separable metric space with metric p, 
and the condition of the theorem is modified to  p(T(X), X)  --+  0 in probability. 
This generalization has significance in curve recognition, when X is a stochastic 
process. Then Theorem 32.3  asserts that one does not lose much information by 
using usual discretization methods, such as, for example, Karhunen-Loeve series 
expansion (see Problems 32.3 to 32.5).  0 

32.3 Admissible and Sufficient Transformations 

569 

32.3  Admissible and Sufficient Transformations 

Sometimes the cost of guessing zero while the true value of Y is one is different 
from the cost of guessing one, while  Y  = O.  These situations may be handled as 
follows.  Define the costs 

C(rn, i), 

rn, l  = 0,  1. 

Here C(Y, g(X»  is the cost of deciding on g(X) when the true label is Y. The risk 
of a decision function g is defined as the expected value of the cost: 

Note that if 

Rg  = E{C(Y, g(X»}. 
l) = {I  if rn  =Ii 

0 

otherwise, 

C( 

rn, 

then the risk is just the probability of error. Introduce the notation 

Qm(X) = 7](x)C(1, rn) + (1  - 7](x»C(O, rn), 

rn  = 0,  1. 

Then we have the following extension of Theorem 2.1: 

Theorem 32.4.  Define 

g(X) = {01 if Ql (x)  >  Qo(x) 

otherwise. 

Then for all decision functions g we have 

Rg::::  Rg . 

Rg is called the Bayes risk. The proof is left as an exercise (see Problem 32.7). 
Which transformations preserve all the necessary information in the sense that the 
Bayes error probability corresponding to the pair (T (X), Y) equals that of (X, Y)? 
Clearly,  every  invertible  mapping  T  has  this  property.  However,  the  practically 
interesting  transformations  are  the  ones  that  provide  some  compression  of the 
data.  The most efficient of such  transformations is  the  Bayes  decision  g*  itself: 
g*  is  specifically  designed  to  minimize  the  error  probability.  If the  goal  is  to 
minimize the Bayes risk with respect to  some other cost function  than the error 
probability, then g*  generally fails  to preserve the Bayes risk.  It is  natural to ask 
what transformations preserve the Bayes risk for all possible cost functions. This 
question  has  a  practical  significance,  when  collecting  data  and  construction  of 
the  decision  are  separated in  space  or in  time.  In  such  cases  the  data  should be 
transmitted via a communication channel (or should be stored). In both cases there 
is a need for an efficient data compression rule. In this problem formulation, when 
getting the data,  one may not know the final  cost function.  Therefore a desirable 
data compression (transformation)  does  not increase the Bayes risk for any cost 
function  C(-,  .).  Here  T  is  a  measurable function  mapping from  Rd  to  Rd'  for 
some positive integer d'. 

570 

32.  Feature Extraction 

DEFINITION 32.1.  Let R~ T  denote the Bayes riskfor the costfunction C and trans(cid:173)
formed observation T(X). A  transformation T  is called admissible iffor any cost 
function  C, 

R~,T = R~, 

where  R~ denotes the Bayes risk for the original observation. 

Obviously each invertible transformation T  is admissible. A nontrivial example 

of an admissible transformation is 

T*(X) = 1](X), 

since according to Theorem 32.4, the Bayes decision for any cost function can be 
constructed by the a posteriori probability 17 (x ) and by the cost function.  Surpris(cid:173)
ingly, this is basically the only such transformation in the following sense: 

Theorem 32.5.  A  transformation T  is admissible if and only if there is a mapping 
G  such that 

G(T(X)) = 1](X)  with probability one. 

PROOF.  The converse is easy since for such G 

R~ :::  R~,T(x) :::  R~,G(T(X» = R~. 

Assume now that T  is admissible but such function  G does not exist. Then there 
is  a  set  A  C  nd  such that fL(A)  >  0,  T(x)  is  constant on  A,  while  all  values 
Then there are real numbers ° <  c  <  1 and E  >  0,  and sets B, DcA such that 
of 1] (x )  are different on A, that is,  if x, yEA, then x  =I  y  implies  1] (x )  =l1](Y). 

fL(B), fL(D)  >  0, and 

1  -

1] (x )  > 

c + E  if  x  E  B, 

l-1](x)  <  C-E  if  xED. 

Now, choose a cost function with the following values: 

CO,O  =  CI,I  =  0, 

CO,I  =  1, 

and 

Then, 

CI,O=  - - .  

1 - C 

C 

Qo(x)  = 

1] (x ), 

QI(X) 

CI,O(1  - 1](x)), 

and the Bayes decision on BUD is given by 

* (x) = {o  if C  <  ~  -

1  otherwIse. 

g 

1] (x) 

32.3 Admissible and Sufficient Transformations 

571 

Now, let geT (x»  be an arbitrary decision. Without loss of generality we can assume 
that g(T(x»  =  0  if x  E  A. Then the difference between the risk of g(T(x» and 
the Bayes risk is 

JRd 

=  r (Qg(T(x»(x)  - Qg*(x)(x»M(dx) 
>  i (Qo(x)  - Ql(x»M(dx) 
=  i (7J(x)  - cl,O(1  - 7J(X»M(dx) 
= i (1 - 1 - c~(X)) JL(dx)  2:  ~JL(D) >  O.  D 

We can give another characterization of admissible transformations by virtue of 

a well-known concept of mathematical statistics: 

DEFINITION 32.2.  T(X) is  called a sufficient statistic if the  random variables X, 
T(X),  and Y form a Markov chain  in  this order.  That  is, for any set  A,  P{Y  E 
AIT(X), X}  = pry E  AIT(X)}. 

Theorem 32.6.  A transformation T  is admissible if and only ifT (X) is a sufficient 
statistic. 

PROOF.  Assume that T  is admissible. Then according to Theorem 32.5  there is  a 
mapping G  such that 

Then 

and 

G(T(X»  = 7J(X)  with probability one. 

P{Y = IIT(X), X}  = P{Y = llX} = l1(X) = G(T(X», 

P{Y = IIT(X)}  =  E{P{Y = lIT(X), X}IT(X)} 

=  E {G(T(X»IT(X)} 

=  G(T(X», 

thus,  P{Y = IIT(X), X}  = P{Y = lIT(X)}, therefore  T(X) is  sufficient.  On the 
other hand, if T(X) is sufficient, then 

P{Y = llX} = P{Y = IIT(X), X} = P{Y = IIT(X)}, 

572 

32.  Feature Extraction 

so for the choice 

G(T(X)) = P{Y = IIT(X)} 

we have the desired function G(·), and therefore T  is admissible.  0 

Theorem  32.6  states  that we  may  replace  X  by  any  sufficient statistic  T(X) 
without altering the Bayes error. The problem with this, in practice, is that we do 
not know the sufficient statistics because we do  not know  the distribution. If the 
distribution of (X, Y) is known to some extent, then Theorem 32.6 may be useful. 
EXAMPLE.  Assume that it is known that 1] (x )  = e-cllxll  for some unknown c  >  o. 
Then  II X II  is  a  sufficient statistic.  Thus,  for  discrimination,  we  may replace  the 
d-dimensional vector X by the  I-dimensional random variable  IIXII  without loss 
of discrimination power.  0 
EXAMPLE. If 1]  (x(1) , X(2) , x(3»)  = 1]0  (x(1)x(2) , x(2)x(3»)  for  some function  1]0,  then 
(X (1) X(2),  X(2) X(3») is a 2-dimensional sufficient statistic. For discrimination, there 
is no need to deal with X(l),  X(2),  X(3).  It suffices to extract the features  X (1) X (2) 
and X(2) X (3) •  0 

EXAMPLE.  If given  Y  =  i,  X  is  normal  with  unknown  mean  mi  and  diagonal 
covariance matrix a 2 I  (for unknown a), then 1](x) is a function of IIx  - m111 2 
II x  - mo 112  only for unknown mo,  mI. Here we have no obvious sufficient statistic. 
However, if mo and m 1 are both known, then a quick inspection shows that x T (m 1 -
mo) is a I-dimensional sufficient statistic. Again, it suffices to look for the simplest 
possible argument for 1].  If mo  = m 1 = 0 but the covariance matrices are ag I  and 
al I  given that Y  = 0 or Y  = 1, then  IIXII 2  is a sufficient statistic.  0 

-

In summary, the results of this  section are useful for picking out features when 

some theoretical information is available regarding the distribution of (X, Y). 

Problems and Exercises 

PROBLEM32.1.  Consider the pair (X, y) E  Rdx{O, l}ofrandomvariables,andletXCd+l)  E 
R  be  an  additional  component.  Define  the  augmented  random  vector  X'  =  (X, X Cd+l). 
Denote  the  Bayes  errors  corresponding  to  the  pairs  (X, Y)  and  (X', Y)  by  L~ and  L~, 
respectively. Clearly, L ~ :::  L ~/. Prove that equality holds if and only if 

P {I{ri'CX/»1/2}  =I I{T/CX»1/2}}  = 0, 

where the a posteriori probability functions  r;  :  Rd  -+  {O,  I}  and  r;'  :  R d+1  -+  {O,  I}  are 
defined as  r;(x) = P{Y = llX = x} and  r;'(x') = P{Y = llX' = x'). HINT:  Consult with the 
proof of Theorem 32.5. 
PROBLEM  32.2.  LetP{Y = I}  = 1/2,andgivenY = i,i = 0,  1,letX = (XCl),  ... , XCd»)  have 
d independent components, where X(J) is normal (m ji, a}). Prove that L * = p{ N  >  r/2}, 

where  N  is  a standard normal random variable,  and r2  is  the  square of the  Mahalanobis 
distance:  r2  = L~=l ((m jl  - m jo)/aj)2 (Dud a and Hart (1973, pp. 66-67». 

Problems and Exercises 

573 

PROBLEM  32.3.  SAMPLING  OF  A STOCHASTIC  PROCESS.  Let  X(t),  t  E  [0,  1],  be a  stochas(cid:173)
tic  process  (i.e.,  a  collection  of real-valued  random  variables  indexed  by  t),  and  let  Y 
be a  binary  random variable.  For integer  N  >  0,  define xW  = X (i / N),  i  :::  1,  ... , N. 
Find sufficient conditions on the function met)  =E{X(t)} and on the covariance function 
K(t, s)::: E{(X(t) - E{X(t)})(X(s) - E{X(s)})} such that 
lim  L *(XCj) = L *(X(·», 

N--+oo 

where  L*(X~) is the Bayes error corresponding to (X~), ... , xf/\ and L*(X(·)  is  the 
infimum  of the  error  probabilities  of decision  functions  that map  measurable  functions 
into  {O,  I}.  HINT:  Introduce  the  stochastic  process  XN(t)  as  the  linear  interpolation  of 
X~l), ... , X(N).  Find conditions under which 

lim  E {ll (XN(t) - X(t))2dt} = 0, 

N--+oo 

0 

and use Theorem 32.3, and the remark following it. 

PROBLEM  32.4.  EXPANSION OF  A STOCHASTIC  PROCESS.  Let X(t), met), and K(t, s) be as in 
the previous problem.  Let 0/1,  0/2,  ... be a complete orthonormal system of functions  on 
[0,  1]. Define 

Find conditions under which 

lim  L *(x(1), ... , X(N)) = L *(X(·). 
N--+oo 

PROBLEM  32.5.  Extend Theorem 32.3 such that the transformations Tn(X,  D,J are allowed 
to depend on the training data. This extension has significance, because feature extraction 
algorithms use the training data. 

PROBLEM  32.6.  For discrete X prove that T(X) is sufficient iff Y,  T(X), X form a Markov 
chain (in this order). 

PROBLEM  32.7.  Prove Theorem 32.4. 

PROBLEM  32.8.  Recall the definition of F -errors from Chapter 3.  Let F be a strictly con(cid:173)
cave function.  Show that dF(X, Y)  = dF(T(X), Y) if and only if the transformation  T  is 
admissible.  Conclude that LNN(X, Y) = LNN(T(X), Y) Construct a  T  and a distribution of 
(X, Y) such that L *(X, Y) = L *(T(X), y), but T  is not admissible. 

PROBLEM  32.9.  Find sufficient statistics of minimal dimension for the following discrimi(cid:173)
nation problems: 

(1) 

It is known that for two given sets  A  and B  with A  n B  = 0,  if Y  = 1,  we have 
X  E  A  and if Y = 0, then X  E  B, or vice versa. 

(2)  Given  Y,  X is  a  vector of independent gamma random variables  with common 
unknown shape parameter a  and common unknown scale parameter b  (i.e.,  the 
marginal  density  of each  component  of X  is  of the  form  xa-Je-xjb /(r(a)b a ), 
x  >  0). The parameters a  and b depend upon Y. 

PROBLEM  32.10.  Let X have support on the surface of a ball of n d  centered at the origin of 
unknown radius. Find a sufficient statistic for discrimination of dimension smaller than d. 

574 

32.  Feature Extraction 

PROBLEM 32.11.  Assume that the  distribution of X  =  (X(l),  X(2),  X(3),  X(4))  is  such that 
X (2) X(4)  =  1 and  X(l)  + X(2)  + X(3)  =  0  with  probability  one.  Find  a  simple  sufficient 
statistic. 

Appendix 

In this appendix we summarize some basic definitions and results from the theory 
of probability. Most proofs are omitted as they may be found in standard textbooks 
on probability, such as Ash (1972), Shiryayev (1984), Chow and Teicher (1978), 
Durrett  (1991),  Grimmett and  Stirzaker  (1992),  and  Zygmund (1959).  We  also 
give a list of useful inequalities that are used in the text. 

A.I  Basics of Measure Theory 

DEFINITION A.l.  Let S be a set,  and let F  be a family of subsets of S.  F  is called 
a a -algebra if 
(i)  0  E  F, 
(iO  A  E  F 
(iii)  AI, A2, ... E  F 

implies  U~l Ai  E  F. 

implies  A C  E  F, 

A a -algebra is closed under complement and union of countably infinitely many 
sets. Conditions (i) and (ii) imply that S  E  F. Moreover, (ii) and (iii) imply that a 
a-algebra is closed under countably infinite intersections. 

DEFINITION A.2.  Let S  be a set,  and let F  be a a -algebra of subsets of S.  Then 
(S,:F)  is  called a  measurable  space.  The  elements of F  are  called measurable 
sets. 
DEFINITION A.3.  If S  = nd  and!3 is  the  smallest a-algebra containing all rect(cid:173)
angles,  then!3 is called the Borel a-algebra.  The  elements of!3 are  called Borel 
sets. 

576 

Appendix 

DEFINITION AA.  Let (S, F)  be  a  measurable  space  and let  f 
function.  f  is called measurable if for all B  E  B 

S  ~ n  be  a 

f-l(B) = {s  :  f(s)  E  B}  E  F, 

that is,  the inverse image of any Borel set B  is in F. 

Obviously, if A  is a measurable set, then the indicator variable IA  is a measur(cid:173)

able function. Moreover, finite linear combinations of indicators of measurable sets 
(called simple functions)  are also measurable functions. It can be shown that the 
set of measurable functions is  closed under addition,  subtraction, multiplication, 
and division. Moreover, the supremum and infimum of a sequence of measurable 
functions, as well as its pointwise limit supremum and limit infimum are measur(cid:173)
able. 

DEFINITION A.5.  Let (S, F) be a measurable space and let v  : F  ~ [0, (0) be a 
function.  v  is a measure on F  if 

(i)  v(0) = 0, 
(ii)  v  is (j-additive,  that is,  AI, A2, ...  E  F,  and Ai n Aj  = 0,  i  =I  j  imply 

that v(U~lAi) = 2:~1 v(Ai). 

In other words, a measure is a nonnegative, (j -additive set function. 

DEFINITION A.6.  v  is  a finite  measure  if v(S)  <  00.  v  is  a  (j -finite  measure  if 
there are countably many measurable sets AI, A 2, ... such that U~l Ai  =  Sand 
V(Ai)  <  00, i =  1,2, .... 

DEFINITION A.7.  The triple (S, F, v) is a measure space if(S, F) is a measurable 
space and v  is a measure on F. 

DEFINITION A.8.  The Lebesgue measure A on nd is a measure on the Borel (j -algebra 
ofnd such that the A measure of each rectangle equals to its volume. 

A.2  The Lebesgue Integral 

DEFINITION A.9.  Let (S, F, v) be a  measure space and  f  =  2:7=1  XJAi  a  simple 
function such that the measurable sets AI, ... , An are disjoint. Then the (Lebesgue) 
integral of f  with respect to  v  is defined by 

f fdv = is f(s)v(ds) = tXiV(Ai). 

If f  is  a  nonnegative-valued measurable function,  then  introduce a  sequence of 
simple functions as follows: 

Us) =  {  ~~ -

l)/n 

(k - 1)ln :::  f(s)  <  kin,  k = 1,2, ... n2n 

if 
if  f(s)  2:  2n. 

A.2 The Lebesgue Integral 

577 

Then  the  fn's  are  simple  functions,  and  fn(s)  -+  f(s)  in  a  monotone  non(cid:173)
decreasing fashion.  Therefore,  the sequence of integrals f  fndv is monotone non(cid:173)
decreasing,  with a limit.  The  integral f  is then defined by 

f fdv =  [  f(s)v(ds) =  lim f fn dv . 

J s 

n---+oo 

If f  is an arbitrary measurable function,  then decompose  it as a difference of its 
positive and negative parts, 

f(s) = fest -

f(s)- = fest - (- f(s)t, 

where  f+  and f- are both nonnegative functions.  Define the integral of f  by 

if at least one term on the  right-hand side is finite.  Then we say that the  integral 
exists. If the integral is finite then  f  is integrable. 

DEFINITION A.IO.  Iff Jdv exists and A  is a measurablefun{Ji~'n,~then fA  fdv is 
defined by 

C"',  .J"7 

i fdv = f fJAdv. 

DEFINITION A.II.  We  say that fn  -+  f  (mod v) if 

v ({s: 2i~fn(s) i  f(s)})  = o. 

Theorem A.I.  (BEPPo-LEVY THEOREM).  If fn(s)  -+  f(s) in a monotone increas(cid:173)
ing way for some nonnegative integrable function  f,  then 

f lim  fn dv = lim f fn dv . 

17---+00 

IZ---+OO 

Theorem A.2.  (LEBESGUE'S  DOMINATED  CONVERGENCE  THEOREM).  Assume that 
(mod  v)andlh7(s)1  ::::  g(s)fors  E  S,  n = 1,2, ... , where f  gdv  <  00. 
fn  -+  f 
Then 

f lim  h 1dv =  lim f h7 dv . 

17---+00 

11---+00 

Theorem A.3.  (FATOU'S LEMMA).  Let fl' 12, ... be measurable functions. 

(i)  If there exists a measurable function  g  with f gdv  >  -00 such thatJor 

every n,  hI (s)  2:  g(s),  then 

lim inf f hI d v  2: f lim inf h1 d v . 

n---+oo 

IZ---+OO 

578 

Appendix 

(ii)  If there is a a measurable function g with J gdv  <  00,  such that  fn(s)  ::::: 

g(s) for every n,  then 

lim sup f fn dv  ::::: f lim sup fn dv . 

n---+oo 

n---+oo 

DEFINITION A.12.  Let VI  and V2  be measures on a measurable space (S, F).  We 
say  that  VI  is  absolutely continuous  with  respect to  V2  if and only  if v2(A)  =  0 
implies VI (A) = 0 (A  E  F).  We  denote this relation by VI  «  V2. 

Theorem A.4.  (RADON-NIKODYM THEOREM).  Let VI  and V2  be measures on the 
measurable space (S,  F) such that VI  «  V2  and V2  is a-finite.  Then  there exists a 
measurable function  f  such that for all A  E  F 

vI(A) = 1 f dv2. 

f  is unique (mod V2).  If VI  is finite,  then  f  has a finite  integral. 

DEFINITION A.13.  f  is called the density,  or Radon-Nikodym derivative of VI  with 
respect to V2.  We  use the notation  f  = dvIidv2. 
DEFINITION A.14.  Let  VI  and V2  be  measures  on  a  measurable  space  (S, F).  If 
)  = 0 and v2(A)  = 0,  then  VI  is  singular 
there exists a set A  E  F  such that vI(A C
with respect to V2  (and vice versa). 

Theorem A.S.  (LEBESGUE  DECOMPOSITION  THEOREM).  If fJ.,  is a  a-finite measure 
on a measurable space (S,  F),  then there  exist two  unique measures  VI,  V2  such 
that fJv  =  VI  + V2,  where  VI  « fJ.,  and V2  is singular with respect to  fJ.,. 

DEFINITION A.IS.  Let (S,  F, v)  be a  measure  space  and let  f  be  a  measurable 
function.  Then  f  induces a measure fJ.,  on the Borel a -algebra as follows: 

fJ.,(B)  = v(f-I(B)),  B  E  B. 

Theorem A.6.  Let V be a measure on the Borel a -algebra B ofR, and let f  and 
g be measurable functions.  Then for all B  E  B, 

r g(x)fJ.,(dx) = r 

iB 

if-ICB) 

g(f(s))v(ds), 

where  fJv  is induced by f. 

DEFINITION A.16.  Let VI  and V2  be  measures on the measurable spaces (Sl, Fd 
and (S2,  F 2),  respectively. Let (S, F) be a measurable space such that S = Sl  X  S2, 
and FI  x  F2  E  F  whenever FI  E  FI and F2  E  F 2. V is called the product measure 
of VI  and V2  on F  iffor FI  E  F1  and F2  E  F2,  V(F1  x  F2)  =  VI (FI)V2(F2).  The 
product of more than two measures can be defined similarly. 

A.3 Denseness Results 

579 

Theorem A. 7.  (FUBINI'S THEOREM).  Let h be a measurable function on the product 
space (S, F). Then 

is h(u, v)v(d(u, v))  =  is, (is, h(u, V)V2(dV)) vl(du) 
is, (is, h(u, v )v.(dU)) V2 (d v), 

assuming that one of the three integrals is finite. 

A.3  Denseness Results 
Lemma A.t.  (COVER AND HART (1967)).  Let fL  be a probability measure on nd
and define its support set by 

, 

A  = support(fL) = {x  :  for all r  >  0,  fL(Sx,r)  >  o} . 

Then  fL(A)  = 1. 

PROOF. By the definition of A, 

A C =  {x  : fL(Sx,rJ  = 0 for some rx  >  o}. 

Let Q denote the set of vectors in nd with rational components (or any countable 
dense set).  Then for each x  E  AC,  there is  a Yx  E  Q  with Ilx 
Yxll:::s  rx/3. This 
implies SYx,rx/2  C  Sx,rx' Therefore,  fL(SYx,rx /2)  = 0, and 

C 

C  U SYx,rx/2. 

A

XEAC 

The right-hand side is a union of countably many sets of zero measure, and therefore 
fL(A·) = 1.  0 

DEFINITION A.17.  Let (S, F, v) be a  measure space.  For a fixed number p  ~ 1, 
L p (v) denotes the set of all measurable functions satisfying f I f  I P d v  <  00. 
Theorem A.S.  For every probability measure v on n d ,  the set of continuousfunc(cid:173)
tions with bounded support is dense in L p (v).  In other words, for every E  >  0 and 
f  E  L p  there is a continuous function with bounded support gEL p  such that 

The following theorem is a rich source of denseness results: 

580 

Appendix 

Theorem A.9.  (STONE-WEIERSTRASS THEOREM).  Let F  be afamily of real-valued 
continuous functions  on a  closed bounded subset B  of Rd. Assume that F  is an 
algebra,  that is,  for any II, h  E  F  and a, b  E  R,  we have alI + bh  E  F,  and 
fl h  E  F.  Assume furthermore  that if x  =I  y  then  there  is an  f  E  F  such  that 
f  (x) =I  f  (y),  and that for each x  E  B  there exists an  f  E  F  with  f  (x) =I O.  Then 
for every E  >  0 and continuous function g  : B  -+  R, there exists an  f  E  F  such 
that 

sup Ig(x)  -
xEB 

f(x)1  <  E. 

The following two theorems concern differentiation of integrals. Good general 

references are Whee den and Zygmund (1977) and de Guzman (1975): 

Theorem A.IO.  (THE LEBESGUE  DENSITY  THEOREM).  Let f  be a  density on Rd. 
Let {Qk(X)}  be a  sequence  of closed cubes centered at x  and contracting to x. 
Then 

. 
hm 
k--+oo 

fQk(x)  If(x) -

f(y)ldy 

A(Qk(X)) 

=0 

at almost all x,  where A denotes the Lebesgue measure. Note that this implies 

at almost all x. 

COROLLARY A.I.  Let A  be a  collection of subsets of So, 1  with  the property that 
for all  A  E  A,  A(A)  :::  CA(SO,l) for some fixed c  >  O.  Then for almost all x,  if 
x  + r A  = {y  : (y  - x) / rEA}, 

1· 
1m  sup 
r-+O AEA  A(X + r A) 

I  fx+rA  f(y)dy 

-

f()i  0 
. 

x  = 

The Lebesgue density theorem also holds if {Qk(X)}  is replaced by a sequence 

of contracting balls centered at x, or indeed by any sequence of sets that satisfy 

x  + brkS S;  Qk(X)  S;  x  + arkS, 

where  S is the unit ball of Rd, rk  + 0,  and 0  <  b  ::::  a  <  00 are fixed  constants. 
This follows from the Lebesgue density theorem. It does not hold in general when 
{Qk(X)}  is  a sequence of hyperrectangles containing x  and contracting to  x. For 
that, an additional restriction is needed: 

Theorem A.I1.  (THE JESSEN-MARCINKIEWICZ-ZYGMUND THEOREM).  Let f  be a 
density  on  Rd  with f  f  logd-l (1  +  f)dx  <  00.  Let {Qk(X)}  be  a  sequence  of 
hyperrectangles containing x  andfor which diam(Qk(x)) -+ O.  Then 

at almost all x. 

A.4 Probability 

581 

COROLLARY A.2.  Iff is a density and {Qk(X)}  is as in TheoremA.ll, then 

~  1  f(y)dy ~ f(x) 

lim inf 
k-+oo  A(Qk(X»Qk(X) 

I 

at almost all x. 
To see this, take g ::::  min(f, M) for large fixed M. As f g logd-l(l + g)  <  00, by 
the lessen-Marcinkiewicz-Zygmund theorem, 

I  1 g(y)dy ~ g(x) 

lim inf 
k-+oo  A(Qk(X»  Qk(X) 

at almost all x. Conclude by letting M  ---?>- 00 along the integers. 

A.4  Probability 

DEFINITION A.I8.  A  measure  space  (Q, F, P)  is  called  a  probability  space  if 
P{Q}  ::::  1.  Q  is  the  sample  space  or sure  event,  the  measurable sets are  called 
events,  and the measurable functions are called random variables. If Xl, ... , Xn 
are random variables then X  ::::  (Xl, ... , Xn)  is a vector-valued random variable. 

DEFINITION A.19.  Let X  be a random variable,  then X  induces the measure fL  on 
the Borel 0' -algebra ofR by 

fL(B)  ::::  P {{w : X(w)  E  Bll ::::  P{X E  B},  B  E  B. 

The probability measure fL  is called the distribution of the random variable X. 

DEFINITION A.20.  Let X be a random variable. The expectation of X is the integral 
of x  with respect to the distribution  fL  of X: 

E{X}:::: L XfL(dx) 

if it exists. 

DEFINITION A.21.  Let X  be a random variable.  The variance of X  is 

Var{X}  ::::  E  {(X - E{X))2} 

ifE{X} isjinite, and 00 ifE{X} is notjinite or does not exist. 

DEFINITION A.22.  Let Xl, ... , Xn  be random variables.  They induce the measure 
{L(n)  on the Borel O'-algebra ofRn with the property 

582 

Appendix 

p}n)  is  called the joint distribution  of the  random variables  X I,  ... ,  X n. Let fJ,i 
be  the  distribution  of Xi  (i  =  1, ... , n).  The  random variables  Xl, ... , Xn  are 
independent if their joint distribution  fJ, (n)  is  the product measure of fJ,1,  ... ,  fJ,n. 
The events AI, ... , An  E  :F are independent if the random variables IAJ , ... , IAn 
are independent. 

Fubini's theorem implies the following: 

Theorem A.12.  If the  random  variables  Xl, ... , Xn  are  independent and have 
finite  expectations then 

A.S 

Inequalities 

Theorem A.13.  (CAUCHy-SCHWARZ INEQUALITY).  If the random variables X and 
Y  have finite second moments (E{X2}  <  00 andE{y2}  <  (0), then 

Theorem A.14.  (HOLDER'S  INEQUALITY).  Let p, q  E  (1, (0) such  that (lip) + 
(1lq)  = 1.  Let X and Y  be  random variables such  that (E{IXPI})ljp  <  00 and 
(E{lyql})ljq  <  00.  Then 

Theorem A.IS.  (MARKOV'S INEQUALITY).  Let X be a nonnegative-valued random 
variable.  Then for each t  >  0, 

P{X  :::  t}  s  E{X} . 

t 

Theorem A.16.  (CHEBYSHEV'S  INEQUALITY).  Let X be a random variable.  Then 
for each t  >  0, 

P{IX - E{X}I  :::  t}  s 

Var{X} 
. 

t 2 

Theorem A.17.  (CHEBYSHEV-CANTELLI INEQUALITY).  Let t  :::  0.  Then 

P{X - EX  >  t}  < 

Var{X} 
2 
- Var{X} + t 

PROOF. We may assume without loss of generality that EX = 0.  Then for all t 

t = E{t - X}  s  E{(t - X)I{x::st}}. 

A.5  Inequalities 

583 

Thus for t  ~ 0 from the Cauchy-Schwarz inequality, 

t 2 

::::;  E{(t - Xf}E{IlxSt}} 

E{(t - X)2}p{X  ::::;  t} 

(Var{X} + t 2 )P{X ::::;  t}, 

that is, 

and the claim follows.  0 

. 

t 2 

P{X <  t}  >  ------,,(cid:173)
- Var{X} + 

-

Theorem A.lS.  (JENSEN'S INEQUALITY).  If f  is a real-valued convex function on 
afinite or infinite interval ofR, and X is a random variable withfinite expectation, 
taking its values in this interval,  then 

f(E{X})  :::;  E{f(X)}. 

Theorem A.19.  (ASSOCIATION  INEQUALITIES).  Let X  be  a  real-valued random 
variable and let f(x) and g(x) be monotone nondecreasing real-valuedfunctions. 
Then 

E{f(X)g(X)}  ~ E{f(X)}E{g(X)}, 

provided that all expectations exist and are finite.  If f  is monotone increasing and 
g  is monotone decreasing,  then 

E{f(X)g(X)} :::;  E{f(X)}E{g(X)}. 

PROOF. We prove the first inequality. The second follows by symmetry. Let X have 
distribution  fL.  Then we write 

E{f(X)g(X)} - E{f(X)}E{g(X)} 

f 

f(x)g(x) fL(dx)  -

f 

fey) fL(dy)  f  g(x) fL(dx) 

=  f  (flf(X) -

f(y)]g(x) {'(dX»)  {'(dy) 

f  ( f  hex, y)g(x) {'(dX»)  {'(dy), 

where hex, y) =  f(x) -

fey). By Fubini's theorem the last integral equals 

r hex, y)g(x) fL2(dxdy) 
IR} 

l>y hex, y)g(x) {'2(dxdy) + l<y hex, y)g(x) {'2(dxdy), 

= 

584 

Appendix 

since h(x, x) = 0 for all x. Here J1- 2(dxdy) = J1-(dx)  . I~(dy). The second integral 
on the right-hand side is just 

Thus, we have 

E{f(X)g(X)} - E{f(X)}E{g(X)} 

L2 hex, y)g(x) J1- 2(dxdy) 
=  1 (1  [hex,  y)g(x) + hey, x)g(y)] J1-(dX»)  J1-(dy) 
1 (1  hex, y)[g(x) - g(y)] J1-(dX»)  J1-(dy) 

x>y 

x>y 

y 

y 
::::  0, 

since hey, x) = -hex, y), and by the fact that hex, y) ::::  0 and g(x) - g(y)  ::::  0 if 
x> y.  0 

A.6  Convergence of Random Variables 

DEFINITION A.23.  Let {Xn},  n  =  1,2, ... , be a sequence of random variables.  We 
say that 

lim  Xn  = X 

n-+oo 

in probability 

iffor each E  >  0 

We  say that 

lim  P{IXn  - XI  ::::  E}  = O. 
n-+oo 

lim  Xn  = X  with probability one (or almost surely), 
n-+oo 

if Xn  --+  X  (mod P),  that is, 

P {w:  lim  Xn(w)  =  X(w)}  =  1. 

11-+00 

For a fixed number p  ::::  1 we say that 

if 

lim  Xn  = X 

n-+oo 

in  L p , 

lim  E {IXn  - XI P }  = O. 

11-+00 

A.7 Conditional Expectation 

585 

Theorem A.20.  Convergence in L p  implies convergence in probability. 

Theorem A.21.  limn~oo Xn  = X with probability one if and only if 

lim  sup IXm  - XI  = 0 
n~oo nS;m 

in  probability.  Thus,  convergence  with  probability  one  implies  convergence  in 
probability. 

Theorem A.22.  (BOREL-CANTELLI LEMMA).  Let An, n =  1, 2,  ... , be a sequence 
of events.  Introduce the notation 

[An  i.o.]  = lim sup An  =  n~l U~=n Am. 

n~CX) 

("i.o." stands for  "infinitely often.") If 

then 

00 

LP{An} <  00 
n=l 

P{[An  i.o.]}  = O. 

By Theorems A.21  and A.22, we have 

Theorem A.23.  If for each E  >  0 

CX) 
LP{IXn  - XI::::  E}  <  00, 
n=l 

then limn~CX) Xn  = X with probability one. 

A.7  Conditional Expectation 

If Y  is  a random variable with finite  expectation and A  is  an event with positive 
probability, then the conditional expectation of Y  given A  is defined by 

E{YIA} = E{Y IA} . 

P{A} 

The conditional probability of an event B  given A  is 

P{B IA} = E{lB IA} =  P{A} 

P{A n B} 
. 

586 

Appendix 

DEFINITION A.24.  Let Y  be a random variable with finite  expectation and X  be a 
d -dimensional vector-valued random variable. Let F x be the a -algebra generated 
by X: 

Fx = {X-l(B); B  E  sn}. 

The  conditional expectation E{ Y I X} of Y  given X  is a  random variable with the 

property that for all A  E  Fx i Y dP = i E{YIX}dP. 

The existence and uniqueness (with probability one) of E{ Y I X} is a consequence 

of the Radon-Nikodym theorem if we apply it to the measures 

such that 

and 

DEFINITION A.2S.  Let  C  be  an  event  and  X  be  a  d-dimensional  vector-valued 
random  variable.  Then  the  conditional probability  of C  given  X  is  P{CIX}  = 
E{1cI X }. 

Theorem A.24.  Let Y  be a random variable with finite  expectation.  Let C be an 
event,  and let X  and Z  be vector-valued random variables.  Then 

(i)  There  is a measurable function  g  on nd  such that E{YIX}  =  g(X) with 

probability one. 

(ii)  E{Y} = E{E{YIX}},  P{C} = E{P{CIX}}. 
(iii)  E{YIX} = E{E{YIX, Z}IX},  P{CIX} = E{P{CIX, Y}IX}. 
(iv)  IfY is a function of X  thenE{YIX} = Y. 
(v) 
(vi)  IfY = f(X, Z)forameasurablefunction f, and X and Z are independent, 

If(Y, X) and Z  are independent,  then E{YIX, Z} = E{YIX}. 

then E{YIX} = g(X), where g(x) = E{f(x, Z)}. 

A.8  The Binomial Distribution 

An  integer-valued  random  variable  X  is  said  to  be binomially  distributed  with 
parameters nand p  if 

P{X = k} =  k  P  (1  - p) 

n-k 

,  k = 0,1, ... , n. 

(n)  k 

If AI, ... , An  are  independent  events  with  P{Ad  =  p,  then  X  =  2::7=1IA;  is 
binomial (n,  p). IAi  is called a Bernoulli random variable with parameter p. 

A.8 The Binomial Distribution 

587 

Lemma A.2.  Let the random variable B(n, p) be binomially distributed with pa(cid:173)
rameters nand p.  Then 

(i) 

and 

( ii) 

PROOF.  (i) follows from the following simple calculation: 

k=ok+1  k 

t  _1_ (n) pk (1  _  p )n-k 
--- L  n 
(n  + 1) p  k=O 

1  n 
1  ~ (n + l)pk(1 _  p)n-k+l  = 

pk+l(1_ pt-k 

(  + 1) 

k + 1 

< 

(n  + 1) P  k=O 

k 

1 

. 
(n  + 1) P 

For (ii) we have 

E {B(:, p/(B(n,p»o}}  S  E L + B~n, p)}  S  (n :l)P 

by (i).  0 

Lemma A.3.  Let  B  be  a  binomial  random  variable  with parameters  nand p. 
Then for every 0  ~ p  ~ 1, 

andfor p  = 1/2 

P!B: l~J) ~J2~rreJ~J2' 

PROOF. The lemma follows from Stirling's formula: 

J2nn  ;; 

(n)n 

e1/(12n)  ~ n!  ~ -J2nn 

(n)n 
;; 

e 1/(l2n+l) 

(see, e.g., Feller (1968».  0 

588 

Appendix 

Lemma A.4.  (DEVROYE  AND  GYORFI  (1985), P.  194).  For any random variable 
X  with finite fourth moment, 

PROOF. Fix a  >  O. The function l/x +ax2 is minimal on (0, (0) whenx 3  =  1/(2a). 
Thus, 

x  + ax4 
___ >  (2a)1/3  + __  =  _(2a)1/3. 

a 

x 2 

-

(2a )2/3 

3 
2 

Replace x by IXI  and take expectations: 

The lower bound,  considered  as  a  function  of a,  is  maximized  if we  take  a  = 
~ (E{X2}/E{X4})3/2. Resubstitution yields the given inequality.  0 

Lemma A.S.  Let B  be a binomial (n,  1/2) random variable.  Then 

PROOF. This bound is a special case of Khintchine's inequality (see Szarek (1976), 
Haagerup (1978), and also Devroye and Gyorfi (1985), p. 139). Rather than proving 
the given inequality, we will show how to apply the previous lemma to get (without 
further work) the inequality 

Indeed, E {(B  - n/2)2}  = n/4 and E {(B  - n/2)4}  = 3n2/16 - n/8  :::;  3n2/16. 
Thus, 

n I} 
2" 

In 
~ (3n2/16)1/2  = V 12·  0 

(n/4)3/2 

E  B 

{ 

I

Lemma A.6.  (SLUD  (1977)).  Let B  be a  binomial (n,  p) random variable with 
p  ::::  1/2. Thenfor n(1  - p)  ~ k  ~ np, 

P{B  >  k}  >  P {N  > 

-

-

k - np 

} , 

- y'np(1-p) 

where N  is normal (0,  1). 

A.10 The Multinomial Distribution 

589 

A.9  The Hypergeometric Distribution 

Let N, b, and n be positive integers with N  >  n and N  >  b. A random variable X 
taking values on the integers 0, 1,  ... , b  is hypergeometric with parameters N, b 
and n, if 

k  =  1,  ... , b. 

X models the number of blue balls in a sample of n balls drawn without replacement 
from an urn containing b blue and N  - b red balls. 

Theorem A.2S.  (HOEFFDING (1963». Let the set A  consist of N  numbers aj, ... , 
aN.  Let ZI, ... , Zn  denote a random sample taken without replacement from  A, 
where n  :::  N. Denote 

Then for any E  >  0 we have 

Specifically,  if X  is hypergeometrically distributed with parameters N, b, and n, 
then 

For more inequalities of this type, see Hoeffding (1963) and Serfling (1974). 

A.I0  The Multinomial Distribution 

A  vector (NI ,  ••• ,  Nk )  of integer-valued random variables  is  multinomially dis(cid:173)
tributed with parameters (n,  PI, ... , Pk)  if 

if  L~=l i j  = k, 
otherwise. 

i j 

::::  0 

Lemma A.7.  The  moment-generating function  of a  multinomial (k,  PI, ... ,  Pk) 
vector is 

590 

Appendix 

A.II  The Exponential and Gamma Distributions 

A nonnegative random variable has exponential distribution with parameter 'A  >  0 
if it has a density 

f(x) = 'Ae- AX

,  x::: o. 

A nonnegative-valued random variable has the gamma distribution with parameters 
a, b  :::  0 if it has density 

The sum of n  i.i.d. exponential()') random variables has gamma distribution with 
parameters nand 1/).. 

A.I2  The Multivariate Normal Distribution 

A d-dimensional random variable X  =  (X(l), ... , Xed))  has the multivariate nor(cid:173)
mal distribution if it has a density 

where mEnd, 'E is a positive definite symmetric d x d matrix with entries (5ij, and 
det('E) denotes the determinant of 'E.  Then EX = m, and for all i, j  = 1, ... , d, 

'E  is called the covariance matrix of X. 

Notation 

indicator of an event A. 

indicator function of a set B. 

•  I A 
•  I B (x) =  I{xEB} 
• IAI  cardinality of a finite set A. 
•  A C  complement of a set A. 
•  ALB  symmetric difference of sets A, B. 
•  jog  composition of functions  j, g. 
•  log  natural logarithm (base e). 
•  Lx J  integer part of the real number x. 
• I x l  upper integer part of the real number x. 
•  X ~ Z 
•  x(I), ... ,x(d)  components of the d-dimensional column vector x. 
•  Ilx II  = JLf=l (x(i))2  L 2-norm of x  E  nd. 
•  X  E  Rd  observation, vector-valued random variable. 
•  Y  E  {a,  I} 
•  Dn  = ((Xl, Yl),  ... , (Xn'  Yn»  training data, sequence of i.i.d. pairs that are 
independent of (X, Y), and have the same distribution as  that of (X, Y) . 

if X and Z have the same distribution. 

label, binary random variable. 

•  17(X)  =  P{Y  =  IIX  =  x}, 1 - 17(X)  =  P{Y  =  0IX  =  x} 

a  posteriori 

probabilities. 

•  p  = P{Y =  I},  1 - p  = P{Y = O}  class probabilities. 
:  Rd  x  {Rd  X  {a,  l} r ---+  {a,  l}  classification 
•  g*  : Rd  ---+  {a,  I}  Bayes decision function. 
•  ¢  :  Rd  ---+  {a,  I},  gn 
functions.  The short notation gn(x) = gn(x, Dn) is also used. 
•  L * = P{g*(X) =I Y}  Bayes risk, the error probability of the Bayes decision. 

592 

Notation 

•  Ln  = L(gn)  = P{g,JX, Dn)  =I  YIDn}  error probability of a classification 
• in (¢) = * L~I=1 I{¢(xi )¥Yi }  empirical error probability of a classifier ¢. 
function gn' 
•  ~n(A) = * L~I=1 I{xiEA}  empirical measure corresponding to Xl, ... , X n. 
•  A  Lebesgue measure on nd. 
• I (x)  density of X, Radon-Nikodym derivative of ~ with respect to A (if it 

•  ~(A) = P{X  E  A}  probability measure of X. 

exists). 

•  10 (x ), 11 (x)  conditional densities of X given Y  = 0 and Y  =  1, respectively 

(if they exist). 

• P  partition of nd. 
•  X(klx), X(k)  k-th nearest neighbor of x  among Xl, ... , X n • 
•  K  : nd -+ n  kernel function. 
•  h, hn  >  0  smoothing factor for a kernel rule. 
•  Kh(X) = (11 h)K(xl h)  scaled kernel function. 
•  Tm  = ((Xn+l, Yn+l),  ... , (Xn+m'  Yn+m» 

testing data,  sequence of i.i.d. pairs 
that are independent of (X, Y) and D n , and have the same distribution as that 
of (X, Y). 

• A  class of sets. 
• C, Cn  classes of classification functions. 
• seA, n)  n-th shatter coefficient of the class of sets A. 
•  VA  Vapnik-Chervonenkis dimension of the class of sets A. 
•  S(C, n)  n-th shatter coefficient of the class of classifiers C. 
•  Vc  Vapnik-Chervonenkis dimension of the class of classifiers C. 
•  Sx,r  = {y  E  n d  :  II y  - x II  ::;  r}  closed Euclidean ball in nd centered at 
x  E  n d

,  with radius r  >  O. 

References 

Abou-Jaoude, S. (1976a). Conditions necessaires et suffisantes de convergence L1  en prob(cid:173)
abilite de l'histogramme pour une densite. Annales de l' Institut Henri Poincare, 12:213-
231. 

Abou-Jaoude, S. (1976b). La convergence LI  et Loo de l'estimateur de la partition aleatoire 

pour une densite. Annales de l'Institut Henri Poincare, 12:299-317. 

Abou-Jaoude,  S.  (1976c).  Sur une  condition  necessaire  et suffisante  de  LI-convergence 
presque complete de l' estimateur de la partition fixe pour une densite. Comptes Rendus 
de l' Academie des Sciences de Paris, 283: 1107-1110. 

Aitchison, J. and Aitken, C. (1976). Multivariate binary discrimination by the kernel method. 

Biometrika,63:413-420. 

Aizerman, M., Braverman, E., and Rozonoer, L. (1964a). The method of potential functions 
for  the problem of restoring the characteristic of a function converter from  randomly 
observed points. Automation and Remote Control, 25: 1546-1556. 

Aizerman, M., Braverman, E., and Rozonoer, L. (1964b). The probability problem of pattern 
recognition  learning  and  the  method  of potential  functions.  Automation and Remote 
Control,25:1307-1323. 

Aizerman, M., Braverman, E., and Rozonoer, L. (1964c). Theoretical foundations of the po(cid:173)
tential function method in pattern recognition learning. Automation and Remote Control, 
25:917-936. 

Aizerman, M., Braverman, E., and Rozonoer, L. (1970).  Extrapolative problems in auto(cid:173)

matic control and the  method of potential functions.  American Mathematical  Society 
Translations, 87:281-303. 

Akaike,  H.  (1954).  An  approximation  to  the  density  function.  Annals of the  Institute  of 

Statistical Mathematics, 6:127-132. 

Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on 

Automatic Control,  19:716-723. 

594 

References 

Alexander,  K.  (1984).  Probability  inequalities  for  empirical  processes  and  a  law  of the 

iterated logarithm. Annals of Probability, 4: 1041-1067. 

Anderson, A. and Fu, K. (1979). Design and development of a linear binary tree classifier 

for leukocytes. Technical Report TR-EE-79-31, Purdue University, Lafayette, IN. 

Anderson, J. (1982). Logistic discrimination. In Handbook of Statistics, Krishnaiah, P. and 

Kanal, L., editors, volume 2, pages 169-191. North-Holland, Amsterdam. 

Anderson, M. and Benning, R. (1970). A distribution-free discrimination procedure based 

on clustering. IEEE Transactions on Information Theory,  16:541-548. 

Anderson, T. (1958). An Introduction to Multivariate Statistical Analysis. John Wiley, New 

York. 

Anderson,  T.  (1966).  Some nonparametric  multivariate  procedures  based on statistically 
equivalent blocks. In Multivariate Analysis, Krishnaiah, P., editor, pages 5-27. Academic 
Press, New York. 

Angluin, D.  and Valiant, L.  (1979). Fast probabilistic algorithms for Hamiltonian circuits 

and matchings. Journal of Computing System Science,  18:155-193. 

Anthony, M. and Holden, S. (1993). On the power of polynomial discriminators and radial 

basis function networks. In Proceedings of the Sixth Annual ACM Conference on Com(cid:173)
putational Learning  Theory,  pages  158-164.  Association for  Computing Machinery, 
New York. 

Anthony, M. and Shawe-Taylor, J. (1990). A result ofVapnik with applications. Technical 

Report CSD-TR-628, University of London, Surrey. 

Argentiero, P., Chin, R., and Beaudet, P. (1982). An automated approach to the design of de(cid:173)

cision tree classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 
4:51-57. 

Arkadjew, A. and Braverman, E. (1966). Zeichenerkennung und Maschinelles Lernen. Old-

enburg Verlag, Miinchen, Wien. 

Ash, R.  (1972). Real Analysis and Probability. Academic Press, New York. 
Assouad, P.  (1983a). Densite et dimension. Annales de  l'Institut Fourier, 33:233-282. 
Assouad, P. (1983b). Deux remarques sur I 'estimation. Comptes Rendus de l'Academie des 

Sciences de Paris, 296: 1021-1024. 

Azuma, K. (1967). Weighted sums of certain dependent random variables. Tohoku Mathe(cid:173)

matical Journal, 68:357-367. 

Bahadur, R. (1961). A representation of the joint distribution of responses to n dichotomous 
items. In Studies in Item Analysis and Prediction, Solomon, H., editor, pages 158-168. 
Stanford University Press, Stanford, CA. 

Bailey, T. and Jain, A. (1978). A note on distance-weighted k-nearest neighbor rules. IEEE 

Transactions on Systems, Man,  and Cybernetics, 8:311-313. 

Barron, A. (1985). Logically smooth density estimation. Technical Report TR 56, Depart(cid:173)

ment of Statistics, Stanford University, Stanford, CA. 

Barron, A. (1989). Statistical properties of artificial neural networks. In Proceedings of the 

28th Conference on Decision and Control, pages 280-285. Tampa, FL. 

Barron, A. (1991). Complexity regularization with application to artificial neural networks. 
In Nonparametric Functional Estimation and Related Topics, Roussas, G., editor, pages 
561-576. NATO ASI Series, Kluwer Academic Publishers, Dordrecht. 

Barron, A. (1993). Universal approximation bounds for superpositions of a sigmoidal func(cid:173)

tion. IEEE Transactions on Information Theory, 39:930-944. 

Barron,  A.  (1994).  Approximation and estimation bounds  for  artificial  neural  networks. 

Machine Learning,  14:115-133. 

References 

595 

Barron, A.  and Barron, R.  (1988).  Statistical learning networks:  a unifying view.  In Pro(cid:173)

ceedings  of the 20th  Symposium  on  the  Interface:  Computing  Science  and Statistics, 
Wegman, E., Gantz, D., and Miller, J., editors, pages 192-203. AMS, Alexandria, VA. 
Barron, A. and Cover, T.  (1991). Minimum complexity density estimation. IEEE Transac(cid:173)

tions on Information Theory, 37: 1034-1054. 

Barron, A., Gyorfi, L., and van der Meulen, E.  (1992). Distribution estimation consistent 
in  total  variation  and  in  two  types  of information divergence.  IEEE  Transactions  on 
Information Theory,  38: 1437-1454. 

Barron, R. (1975). Learning networks improve computer-aided prediction and control. Com(cid:173)

puter Design, 75:65-70. 

Bartlett, P.  (1993).  Lower bounds  on the  Vapnik-Chervonenkis dimension of multi-layer 

threshold networks. In Proceedings of the Sixth annual ACM Conference on Computa(cid:173)
tional Learning Theory,  pages  144-150. Association for Computing Machinery, New 
York. 

Bashkirov,  0., Braverman, E.,  and Muchnik, I. (1964).  Potential function  algorithms  for 

pattern recognition learning machines. Automation and Remote Control, 25:692-695. 
Baum,  E.  (1988).  On  the  capabilities  of multilayer perceptrons.  Journal  of Complexity, 

4:193-215. 

Baum, E.  and Haussler, D.  (1989). What size net gives valid generalization? Neural Com(cid:173)

putation,  1:151-160. 

Beakley, G. and Tuteur, F.  (1972). Distribution-free pattern verification using statistically 

equivalent blocks. IEEE Transactions on Computers, 21:l337-l347. 

Beck,  J.  (1979).  The  exponential  rate  of convergence  of error for  kn-NN  nonparametric 

regression and decision. Problems of Control and Information Theory, 8:303-311. 

Becker, P.  (1968). Recognition of Patterns. Polyteknisk Forlag, Copenhagen. 
Ben-Bassat, M.  (1982). Use of distance measures, information measures and error bounds 
in feature evaluation. In Handbook of Statistics,  Krishnaiah, P.  and Kanal,  L., editors, 
volume 2, pages 773-792. North-Holland, Amsterdam. 

Benedek,  G.  and  Itai,  A.  (1988).  Learnability  by fixed  distributions.  In  Computational 
Learning Theory: Proceedings of the 1988 Workshop, pages 80-90. Morgan Kaufman, 
San Mateo, CA. 

Benedek, G. and Itai, A. (1994). Nonuniform learnability. Journal of Computer and Systems 

Sciences, 48:311-323. 

Bennett, G. (1962). Probability inequalities for the sum of independent random variables. 

Journal of the American Statistical Association, 57:33-45. 

Beran, R. (1977). Minimum Hellinger distance estimates for parametric models. Annals of 

Statistics, 5:445-463. 

Beran, R.  (1988). Comments on "a new theoretical and algorithmical basis for estimation, 

identification and control" by P. Kovanic. Automatica, 24:283-287. 

Bernstein, S. (1946). The Theory of Probabilities. Gastehizdat Publishing House, Moscow. 
Bhattacharya, P.  and Mack, Y.  (1987). Weak convergence of k-NN density and regression 

estimators with varying k and applications. Annals of Statistics,  15:976-994. 

Bhattacharyya, A.  (1946). On a measure of divergence between two multinomial popula(cid:173)

tions. Sankhya, Series A, 7:401-406. 

Bickel, P. and Breiman, L. (1983). Sums of functions of nearest neighbor distances, moment 

bounds, limit theorems and a goodness of fit test. Annals of Probability, 11:185-214. 

Birge,  L.  (1983).  Approximation  dans  les  espaces  metriques  et theorie  de  l'estimation. 

ZeitschriJt for Wahrscheinlichkeitstheorie und verwandte Gebiete, 65: 181-237. 

596 

References 

Birge, L. (1986). On estimating a density using Hellinger distance and some other strange 

facts. Probability Theory and Related Fields, 71:271-291. 

Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. (1989). Learnability and the 

Vapnik-Chervonenkis dimension. Journal of the ACM, 36:929-965. 

Braverman, E. (1965). The method of potential functions. Automation and Remote Control, 

26:2130-2138. 

Braverman, E. and Pyatniskii, E. (1966). Estimation of the rate of convergence of algorithms 

based on the potential function method. Automation and Remote Control, 27:80-100. 

Breiman, L., Friedman, J., Olshen, R., and Stone, C. (1984). Classification and Regression 

Trees.  Wadsworth International, Belmont, CA. 

Breiman, L., Meisel, W.,  and Purcell, E. (1977). Variable kernel estimates of multivariate 

densities. Technometrics,  19: 135-144. 

Brent, R. (1991). Fast training algorithms for multilayer neural nets. IEEE Transactions on 

Neural Networks, 2:346-354. 

Broder,  A.  (1990).  Strategies  for  efficient  incremental  nearest  neighbor  search.  Pattern 

Recognition, 23:171-178. 

Broornhead, D.  and Lowe, D.  (1988).  Multivariable functional  interpolation and adaptive 

networks. Complex Systems, 2:321-323. 

Buescher,  K.  and  Kumar,  P.  (1996a).  Learning  by  canonical  smooth  estimation,  Part  I: 

Simultaneous estimation. IEEE Transactions on Automatic Control,  41 :545-556. 

Buescher, K.  and  Kumar,  P.  (1996b).  Learning  by canonical  smooth estimation,  Part II: 
Learning and choice of model complexity. IEEE Transactions  on Automatic Control, 
41:557-569. 

Burbea, J.  (1984). The convexity with respect to gaussian distributions of divergences of 

order ex.  Utilitas Mathematica, 26: 171-192. 

Burbea, J.  and Rao, C.  (1982).  On the convexity of some divergence measures  based on 

entropy functions. IEEE Transactions on Information Theory, 28:48-495. 

Burshtein,  D.,  Della Pietra,  V.,  Kanevsky,  D.,  and Nadas,  A.  (1992).  Minimum impurity 

partitions. Annals of Statistics, 20: 1637-1646. 

Cacoullos, T.  (1965). Estimation of a multivariate density. Annals of the Institute of Statis(cid:173)

tical Mathematics,  18: 179-190. 

Carnal,  H.  (1970).  Die  konvexe  Hiille  von  n  rotationssymmetrisch  verteilten  Punkten. 

Zeitschrijt for Wahrscheinlichkeitstheorie und verwandte Gebiete,  15: 168-176. 

Casey,  R.  and  Nagy,  G.  (1984).  Decision tree  design  using  a  probabilistic  model.  IEEE 

Transactions on Information Theory,  30:93-99. 

Cencov, N. (1962). Evaluation of an unknown distribution density from observations. Soviet 

Math.  Doklady, 3:1559-1562. 

Chang, C.  (1974). Finding prototypes for nearest neighbor classifiers. IEEE Transactions 

on Computers, 26:1179-1184. 

Chang, C. (1973). Dynamic programming as applied to feature selection in pattern recog(cid:173)

nition systems. IEEE Transactions on Systems, Man, and Cybernetics, 3:166-171. 

Chen, T., Chen, H., and Liu, R. (1990). A constructive proof and an extension of Cybenko's 

approximation theorem. In Proceedings of the 22nd Symposium of the Interface: Com(cid:173)
puting Science and Statistics, pages 163-168. American Statistical Association, Alexan(cid:173)
dria, VA. 

Chen, X. and Zhao, L. (1987). Almost sure L I-norm convergence for data-based histogram 

density estimates. Journal of Multivariate Analysis, 21: 179-188. 

References 

597 

Chen, Z. and Fu, K. (1973). Nonparametric Bayes risk estimation for pattern classification. 
In Proceedings of the IEEE Conference on Systems, Man, and Cybernetics. Boston, MA. 
Chernoff, H. (1952). A measure of asymptotic efficiency of tests of a hypothesis based on 

the sum of observations. Annals of Mathematical Statistics, 23:493-507. 

Chernoff, H.  (1971). A bound on the classification error for discriminating between popu(cid:173)
lations with specified means and variances. In Studi di probabilita, statistica e ricerca 
operativa in onare di Giuseppe Pompilj, pages 203-211. Oderisi, Gubbio. 

Chou, P.  (1991). Optimal partitioning for classification and regression trees. IEEE Trans(cid:173)

actions on Pattern Analysis and Machine Intelligence,  13:340-354. 

Chou, W. and Chen, Y.  (1992). A new fast algorithm for effective training of neural classi(cid:173)

fiers. Pattern Recognition, 25:423-429. 

Chow, C. (1965). Statistical independence and threshold functions. IEEE Transactions on 

Computers, E-14:66-68. 

Chow, C. (1970). On optimum recognition error and rejection tradeoff. IEEE Transactions 

on Information Theory,  16:41-46. 

Chow,  Y.  and  Teicher,  H.  (1978).  Probability  Theory,  Independence,  Interchangeability, 

Martingales. Springer-Verlag, New York. 

Ciampi, A. (1991). Generalized regression trees. Computational Statistics and Data Anal(cid:173)

ysis,  12:57-78. 

Collomb, G. (1979). Estimation de la regression par la methode des k points les plus proches: 
proprietes de convergence ponctuelle. Comptes Rendus de l' Academie des Sciences de 
Paris, 289:245-247. 

Collomb, G. (1980). Estimation de la regression par la methode des k points les plus proches 

avec noyau. Lecture Notes in Mathematics #821, Springer-Verlag, Berlin.  159-175. 

Collomb, G.  (1981). Estimation non parametrique de la regression: revue bibliographique. 

International Statistical Review, 49:75-93. 

Conway, J. and Sloane, N. (1993). Sphere-Packings, Lattices and Groups. Springer-Verlag, 

Berlin. 

Coomans,  D.  and Broeckaert,  I.  (1986).  Potential Pattern Recognition  in  Chemical and 
Medical Decision Making. Research Studies Press, Letchworth, Hertfordshire, England. 
Cormen, T.,  Leiserson, c., and Rivest, R.  (1990). Introduction to Algorithms. MIT Press, 

Boston, MA. 

Cover,  T.  (1965).  Geometrical  and  statistical  properties  of systems of linear inequalities 
with applications in pattern recognition. IEEE Transactions on Electronic Computers, 
14:326-334. 

Cover, T. (1968a). Estimation by the nearest neighbor rule. IEEE Transactions on Informa(cid:173)

tion Theory,  14:50-55. 

Cover, T. (1968b). Rates of convergence for nearest neighbor procedures. In Proceedings of 

the Hawaii International Conference on Systems Sciences, pages 413-415. Honolulu. 
Cover, T. (1969). Learning in pattern recognition. In Methodologies of Pattern Recognition, 

Watanabe, S., editor, pages 111-132. Academic Press, New York. 

Cover,  T.  (1974).  The  best  two  independent  measurements  are  not  the  two  best.  IEEE 

Transactions on Systems, Man, and Cybernetics, 4:116-117. 

Cover, T.  and Hart, P.  (1967).  Nearest neighbor pattern classification. IEEE Transactions 

on Information Theory,  13:21-27. 

Cover, T.  and Thomas, J. (1991). Elements of Information Theory. John Wiley, New York. 
Cover, T.  and Van  Campenhout, J.  (1977). On the possible orderings in the measurement 
selection problem. IEEE Transactions on Systems, Man, and Cybernetics, 7:657-661. 

598 

References 

Cover, T.  and Wagner, T.  (1975). Topics in statistical pattern recognition. Communication 

and Cybernetics,  10: 15-46. 

Cramer, H. and Wold, H. (1936).  Some theorems on distribution functions. Journal of the 

London Mathematical Society,  11:290-294. 

Csibi, S. (1971). Simple and compound processes in iterative machine learning. Technical 

Report, CISM Summer Course, Udine, Italy. 

Csibi, S.  (1975). Using indicators as  a base for estimating optimal decision functions.  In 
Colloquia Mathematica  Societatis Janos Botyai:  Topics  in Information  Theory,  pages 
143-153. Keszthely, Hungary. 

Csiszar, I.  (1967). Information-type measures of difference of probability distributions and 

indirect observations. Studia Scientiarium Mathematicarum Hungarica, 2:299-318. 

Csiszar, I. (1973). Generalized entropy and quantization problems. In Transactions of the 

Sixth Prague Conference on Information Theory,  Statistical Decision Functions, Ran(cid:173)
dom Processes, pages 159-174. Academia, Prague. 

Csiszar, I. and Korner, J. (1981). Information Theory: Coding Theoremsfor Discrete Mem(cid:173)

oryless Systems. Academic Press, New York. 

Cybenko, G. (1989). Approximations by superpositions of sigmoidal functions. Math. Con(cid:173)

trol, Signals, Systems, 2:303-314. 

Darken, C., Donahue, M., Gurvits, L., and Sontag, E. (1993). Rate of approximation results 

motivated by robust neural network learning. In Proceedings of the  Sixth ACM Work(cid:173)
shop on Computational Learning Theory, pages 303-309. Association for Computing 
Machinery, New York. 

Das Gupta, S. (1964). Nonparametric classification rules. Sankhya Series A, 26:25-30. 
Das Gupta, S.  and Lin, H.  (1980). Nearest neighbor rules of statistical classification based 

on ranks. Sankhya Series A, 42:419-430. 

Dasarathy, B. (1991). Nearest Neighbor Pattern Classification Techniques. IEEE Computer 

Society Press, Los Alamitos, CA. 

Day, N. and Kerridge, D. (1967). A general maximum likelihood discriminant. Biometrics, 

23:313-324. 

de Guzman, M.  (1975). Differentiation of Integrals  in  Rn.  Lecture Notes in Mathematics 

#481, Springer-Verlag, Berlin. 

Deheuvels, P.  (1977). Estimation nonparametrique de la densite par histogrammes gener(cid:173)

alises. Publications de l'Institut de  Statistique de  l' Universite de Paris, 22: 1-23. 

Devijver, P. (1978). A note on ties in voting with the k-NN rule. Pattern Recognition, 10:297-

298. 

Devijver, P.  (1979). New error bounds with the nearest neighbor rule. IEEE Transactions 

on Information Theory, 25:749-753. 

Devijver,  P.  (1980).  An overview  of asymptotic  properties  of nearest neighbor rules.  In 
Pattern Recognition  in Practice,  Gelsema,  E.  and  Kanal,  L.,  editors,  pages  343-350. 
Elsevier Science Publishers, Amsterdam. 

Devijver,  P.  and  Kittler,  J.  (1980).  On  the  edited nearest neighbor rule.  In  Proceedings 
of the  Fifth  International  Conference  on  Pattern  Recognition,  pages  72-80.  Pattern 
Recognition Society, Los Alamitos, CA. 

Devijver, P.  and Kittler, J.  (1982). Pattern Recognition: A  Statistical Approach. Prentice(cid:173)

Hall, Englewood Cliffs, NJ. 

Devroye,  L.  (1978).  A  universal k-nearest neighbor procedure in discrimination.  In Pro(cid:173)

ceedings of the 1978 IEEE Computer Society Conference on Pattern Recognition and 
Image Processing, pages 142-147. IEEE Computer Society, Long Beach, CA. 

References 

599 

Devroye, L. (1981a). On the almost everywhere convergence of nonparametric regression 

function estimates. Annals of Statistics, 9: 1310-1309. 

Devroye, L. (1981 b). On the asymptotic probability of error in nonparametric discrimina(cid:173)

tion. Annals of Statistics, 9: 1320-1327. 

Devroye, L. (1981 c). On the inequality of Cover and Hart in nearest neighbor discrimination. 

IEEE Transactions on Pattern Analysis and Machine Intelligence, 3:75-78. 

Devroye, L. (1982a). Bounds for the uniform deviation of empirical measures. Journal of 

Multivariate Analysis,  12:72-79. 

Devroye, L. (1982b). Necessary and sufficient conditions for the almost everywhere con(cid:173)
vergence of nearest neighbor regression function estimates. ZeitschriJt for Wahrschein(cid:173)
lichkeitstheorie und verwandte Gebiete, 61:467-481. 

Devroye, L. (1983). The equivalence of weak, strong and complete convergence in L1  for 

kernel density estimates. Annals of Statistics,  11:896-904. 

Devroye, L. (1987). A Course in Density Estimation. Birkhauser, Boston, MA. 
Devroye, L. (1988a).  Applications  of the theory  of records  in the  study of random trees. 

Acta Informatica, 26: 123-130. 

Devroye, L. (1988b).  Automatic  pattern recognition:  A  study of the probability of error. 

IEEE Transactions on Pattern Analysis and Machine Intelligence,  10:530-543. 

Devroye, L. (1988c). The expected size of some graphs in computational geometry. Com(cid:173)

puters and Mathematics with Applications, 15:53-64. 

Devroye, L. (1988d). The kernel estimate is relatively stable. Probability Theory and Related 

Fields, 77:521-536. 

Devroye, L. (1991 a). Exponential inequalities in nonparametric estimation. In N onparamet(cid:173)
ric Functional Estimation and Related Topics, Roussas, G., editor, pages 31-44. NATO 
ASI Series, Kluwer Academic Publishers, Dordrecht. 

Devroye,  L. (1991b).  On the  oscillation of the  expected number of points  on  a  random 

convex hull. Statistics and Probability Letters,  11 :281-286. 

Devroye, L. and Gyorfi, L.  (1983). Distribution-free exponential bound on the L 1 error of 
partitioning estimates of a regression function.  In Proceedings of the Fourth Pannonian 
Symposium  on  Mathematical  Statistics,  Konecny,  E,  Mogyor6di,  J.,  and  Wertz,  W., 
editors, pages 67-76. Akademiai Kiad6, Budapest, Hungary. 

Devroye, L. and Gyorfi, L.  (1985). Nonparametric Density Estimation: The  L1  View. John 

Wiley, New York. 

Devroye, L. and Gyorfi, L. (1992). No empirical probability measure can converge in the 

total variation sense for all distributions. Annals of Statistics,  18:1496-1499. 

Devroye, L., Gyorfi, L., Krzyzak, A., and Lugosi, G. (1994). On the strong universal consis(cid:173)
tency of nearest neighbor regression function estimates. Annals of Statistics, 22: 1371-
1385. 

Devroye, L.  and Krzyzak, A.  (1989).  An equivalence theorem for  L1  convergence of the 

kernel regression estimate. Journal of Statistical Planning and Inference, 23:71-82. 

Devroye, L. and Laforest, L. (1990). An analysis of random d-dimensional quadtrees. SIAM 

Journal on Computing,  19:821-832. 

Devroye,  L.  and  Lugosi,  G.  (1995).  Lower  bounds  in  pattern  recognition  and  learning. 

Pattern Recognition, 28: 1011-1018. 

Devroye,  L.  and  Machell,  E  (1985).  Data  structures  in  kernel  density  estimation.  IEEE 

Transactions on Pattern Analysis and Machine Intelligence, 7:360-366. 

Devroye, L. and Wagner, T.  (1976a). A distribution-free performance bound in error esti(cid:173)

mation. IEEE Transactions Information Theory, 22:586-587. 

600 

References 

Devroye, L. and Wagner, T.  (1976b). Nonparametric discrimination and density estimation. 

Technical Report 183, Electronics Research Center, University of Texas. 

Devroye, L. and Wagner, T. (1979a). Distribution-free inequalities for the deleted and hold(cid:173)

out error estimates. IEEE Transactions on Information Theory, 25:202-207. 

Devroye, L.  and Wagner,  T.  (1979b).  Distribution-free performance bounds for  potential 

function rules. IEEE Transactions on Information Theory, 25:601-604. 

Devroye, L. and Wagner, T.  (1979c). Distribution-free performance bounds with the resub(cid:173)

stitution error estimate. IEEE Transactions on Information Theory, 25:208-210. 

Devroye, L. and Wagner, T. (1980a). Distribution-free consistency results in nonparametric 

discrimination and regression function estimation. Annals of Statistics, 8:231-239. 

Devroye, L. and Wagner, T. (1980b). On the Ll convergence ofkemel estimators of reg res(cid:173)
sion functions  with applications in discrimination. Zeitschriji for Wahrscheinlichkeits(cid:173)
theorie und verwandte Gebiete, 51:15-21. 

Devroye, L. and Wagner, T. (1982). Nearest neighbor methods in discrimination. In Hand(cid:173)

book of Statistics, Krishnaiah, P. and Kanal, L., editors, volume 2, pages 193-197. North 
Holland, Amsterdam. 

Devroye, L.  and Wise, G.  (1980).  Consistency of a recursive nearest neighbor regression 

function estimate. Journal of Multivariate Analysis, 10:539-550. 

Diaconis, P.  and Shahshahani, M.  (1984). On nonlinear functions of linear combinations. 

SIAM Journal on Scientific and Statistical Computing, 5: 175-191. 

Do-Tu,  H.  and Installe,  M.  (1975).  On adaptive solution of piecewise linear approxima(cid:173)

tion problem-application to modeling and identification. In Milwaukee Symposium on 
Automatic Computation and Control, pages 1-6. Milwaukee, WI. 

Duda, R. and Hart, P.  (1973). Pattern Classification and Scene Analysis. John Wiley, New 

York. 

Dudani, S.  (1976). The distance-weighted k-nearest-neighbor rule. IEEE Transactions on 

Systems, Man, and Cybernetics, 6:325-327. 

Dudley, R.  (1978). Central limit theorems for empirical measures. Annals of Probability, 

6:899-929. 

Dudley,R. (1979). Balls in Rk do not cut all subsets of k+2 points. Advances in Mathematics, 

31  (3):306-308. 

Dudley, R. (1984). Empirical processes. In Ecole de Probabilite de St. Flour 1982. Lecture 

Notes in Mathematics #1097, Springer-Verlag, New York. 

Dudley, R.  (1987).  Universal Donsker classes and metric entropy. Annals of Probability, 

15: 1306-1326. 

Dudley, R., Kulkarni, S., Richardson, T.,  and Zeitouni, O. (1994). A metric entropy bound 
is not sufficient for learnability. IEEE Transactions on Information Theory, 40:883-885. 
Durrett, R. (1991). Probability: Theory and Examples. Wadsworth and Brooks/Cole, Pacific 

Grove, CA. 

Dvoretzky, A.  (1956).  On stochastic approximation. In Proceedings of the First Berkeley 
Symposium on Mathematical Statistics and Probability, Neyman, J., editor, pages 39-55. 
University of California Press, Berkeley, Los Angeles. 

Dvoretzky,  A.,  Kiefer,  J.,  and Wolfowitz,  J.  (1956).  Asymptotic  minimax  character of a 

sample distribution function and of the classical multinomial estimator. Annals of Math(cid:173)
ematical Statistics, 33:642-669. 

Edelsbrunner, H. (1987). Algorithmsfor Computational Geometry. Springer-Verlag, Berlin. 
Efron,  B.  (1979).  Bootstrap methods:  another look at the jackknife. Annals of Statistics, 

7:1-26. 

References 

601 

Efron,  B.  (1983).  Estimating  the  error rate  of a  prediction  rule:  improvement  on  cross 

validation. Journal of the American Statistical Association, 78: 316-331. 

Efron,  B.  and Stein,  C.  (1981).  The jackknife estimate of variance.  Annals of Statistics, 

9:586-596. 

Ehrenfeucht, A., Haussler, D., Kearns, M., and Valiant, L.  (1989). A general lower bound 
on the number of examples needed for learning. Information and Computation, 82:247-
261. 

Elashoff, J., Elashoff, R., and Goldmann, G.  (1967). On the choice of variables in classifi(cid:173)

cation problems with dichotomous variables. Biometrika, 54:668-670. 

Fabian, V. (1971). Stochastic approximation. In Optimizing Methods in Statistics, Rustagi, 

J., editor, pages 439-470. Academic Press, New York, London. 

Fano, R.  (1952). Class Notes for Transmission  of Information. Course 6.574. MIT, Cam(cid:173)

bridge, MA. 

Farago, A., Linder, T., and Lugosi, G. (1993). Fast nearest neighbor search in dissimilarity 
spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15:957-962. 
Farago, A. and Lugosi, G. (1993). Strong universal consistency of neural network classifiers. 

IEEE Transactions on Information Theory, 39: 1146-1151. 

Farago,  T.  and  Gyorfi,  L.  (1975).  On  the  continuity  of the  error  distortion  function  for 
multiple hypothesis decisions. IEEE Transactions on Information Theory, 21: 458-460. 
Feinholz, L.  (1979).  Estimation of the performance of partitioning algorithms  in pattern 

classification. Master's Thesis, Department of Mathematics, McGill University, Mon(cid:173)
treal. 

Feller, W.  (1968). An Introduction to Probability Theory and its Applications, YoU. John 

Wiley, New York. 

Finkel, R.  and Bentley, J.  (1974). Quad trees:  A data structure for retrieval  on composite 

keys. Acta Informatica, 4: 1-9. 

Fisher, R.  (1936). The case of multiple measurements in taxonomic problems. Annals of 

Eugenics, 7, part II: 179-188. 

Fitzmaurice, G. and Hand, D.  (1987). A comparison of two average conditional error rate 

estimators. Pattern Recognition Letters, 6:221-224. 

Fix,  E.  and  Hodges,  J.  (1951).  Discriminatory  analysis.  Nonparametric  discrimination: 
Consistency properties. Technical Report 4, Project Number 21-49-004, USAF School 
of Aviation Medicine, Randolph Field, TX. 

Fix, E.  and Hodges, J.  (1952). Discriminatory analysis:  small sample performance. Tech(cid:173)

nical Report 21-49-004, USAF School of Aviation Medicine, Randolph Field, TX. 

Fix, E. and Hodges, J. (1991 a). Discriminatory analysis, nonparametric discrimination, con(cid:173)

sistency properties. In Nearest Neighbor Pattern Classification Techniques, Dasarathy, 
B., editor, pages 32-39. IEEE Computer Society Press, Los Alamitos, CA. 

Fix,  E.  and Hodges,  J.  (1991b).  Discriminatory  analysis:  small  sample  performance.  In 
Nearest Neighbor Pattern  Classification  Techniques,  Dasarathy,  B., editor,  pages  40-
56. IEEE Computer Society Press, Los Alamitos, CA. 

Flick, T., Jones, L., Priest, R., and Herman, C. (1990). Pattern classification using protection 

pursuit. Pattern Recognition, 23: 1367-1376. 

Forney,  G.  (1968).  Exponential  error  bounds  for  erasure,  list,  and  decision  feedback 

schemes. IEEE Transactions on Information Theory, 14:41-46. 

Friedman, J. (1977). A recursive partitioning decision rule for nonparametric classification. 

IEEE Transactions on Computers, 26:404-408. 

602 

References 

Friedman, J., Baskett, F., and Shustek, L. (1975). An algorithm for finding nearest neighbor. 

IEEE Transactions on Computers, 24: 100~ 1006. 

Friedman, J., Bentley, J., and Finkel, R.  (1977). An algorithm for finding  best matches in 
logarithmic expected time. ACM Transactions on Mathematical Software, 3:209-226. 
Friedman, J. and Silverman, B. (1989). Flexible parsimonious smoothing and additive mod(cid:173)

eling. Technometrics, 31:3-39. 

Friedman, J. and Stuetzle, W. (1981). Projection pursuit regression. Journal of the American 

Statistical Association, 76:817-823. 

Friedman, J., Stuetzle, W., and Schroeder, A. (1984). Projection pursuit density estimation. 

Journal of the American Statistical Association, 79:599-608. 

Friedman,  1.  and  Tukey,  1.  (1974).  A  projection  pursuit  algorithm  for  exploratory  data 

analysis. IEEE Transactions on Computers, 23:881-889. 

Fritz, 1.  and Gyorfi, L. (1976).  On the minimization of classification error probability in 
statistical pattern recognition. Problems of Control and Information Theory, 5:371-382. 
Fu, K., Min, P., and Li, T. (1970). Feature selection in pattern recognition. IEEE Transactions 

on Systems Science and Cybernetics, 6:33-39. 

Fuchs, H., Abram, G., and Grant, E. (1983). Near real-time shaded display of rigid objects. 

Computer Graphics,  17:65-72. 

Fuchs,  H.,  Kedem,  Z.,  and Naylor,  B.  (1980).  On visible  surface generation by  a  priori 
tree structures. In Proceedings SIGGRAPH' 80, pages 124-133. Published as Computer 
Graphics, volume 14. 

Fukunaga, K. and Flick, T. (1984). An optimal global nearest neighbor metric. IEEE Trans(cid:173)

actions on Pattern Analysis and Machine Intelligence, 6: 314-318. 

Fukunaga, K. and Hostetler, L. (1973). Optimization of k-nearest neighbor density estimates. 

IEEE Transactions on Information Theory,  19:32~326. 

Fukunaga,  K.  and  Hummels,  D.  (1987).  Bayes  error estimation  using  Parzen  and  k-NN 
procedures. IEEE Transactions  on Pattern Analysis and Machine Intelligence,  9:634-
643. 

Fukunaga, K.  and Kessel, D. (1971). Estimation of classification error. IEEE Transactions 

on Computers, 20:1521-1527. 

Fukunaga, K.  and Kessel, D. (1973). Nonparametric Bayes error estimation using unclas(cid:173)

sified samples. IEEE Transactions Information Theory,  19:434-440. 

FUkunaga, K.  and Mantock, J.  (1984).  Nonparametric data reduction. IEEE Transactions 

on Pattern Analysis and Machine Intelligence, 6:115-118. 

Fukunaga,  K.  and  Narendra, P.  (1975).  A  branch and bound algorithm for computing k(cid:173)

nearest neighbors. IEEE Transactions on Computers, 24:750-753. 

Funahashi,  K.  (1989).  On the approximate realization of continuous mappings  by neural 

networks. Neural Networks, 2:183-192. 

Gabor,  D.  (1961).  A  universal  nonlinear filter,  predictor,  and simulator  which optimizes 

itself. Proceedings of the Institute of Electrical Engineers, 108B:422-438. 

Gabriel, K. and Sokal, R. (1969). A new statistical approach to geographic variation analysis. 

Systematic Zoology,  18:259-278. 

Gaenssler, P.  (1983). Empirical processes.  Lecture Notes-Monograph Series, Institute of 

Mathematical Statistics, Hayward, CA. 

Gaenssler, P. and Stute, W. (1979). Empirical processes: A survey of results for independent 

identically distributed random variables. Annals of Probability, 7: 193-243. 

Gallant, A. (1987). Nonlinear Statistical Models. John Wiley, New York. 

References 

603 

Ganesalingam, S. and McLachlan, G. (1980). Error rate estimation on the basis of posterior 

probabilities. Pattern Recognition,  12:405-413. 

Garnett, J. and Yau, S. (1977). Nonparametric estimation of the Bayes error offeature extrac(cid:173)

tors using ordered nearest neighbour sets. IEEE Transactions on Computers, 26:46-54. 
Gates,  G.  (1972). The reduced nearest neighbor rule.  IEEE Transactions  on Information 

Theory,  18:431-433. 

Gelfand, S. and Delp, E. (1991). On tree structured classifiers. In Artificial Neural Networks 
and Statistical Pattern Recognition,  Old and New  Connections,  Sethi, I. and Jain, A., 
editors, pages 71-88. Elsevier Science Publishers, Amsterdam. 

Gelfand,  S.,  Ravishankar, c.,  and Delp,  E.  (1989).  An  iterative  growing  and pruning al(cid:173)
gorithm for classification tree  design. In Proceedings of the 1989 IEEE International 
Conference on Systems, Man, and Cybernetics, pages 818-823. IEEE Press, Piscataway, 
NJ. 

Gelfand, S., Ravishankar, c., and Delp, E. (1991). An iterative growing and pruning algo(cid:173)
rithm for classification tree design. IEEE Transactions on Pattern Analysis and Machine 
Intelligence,  13:163-174. 

Geman, S.  and Hwang, C.  (1982). Nonparametric maximum likelihood estimation by the 

method of sieves. Annals of Statistics,  10:401-414. 

Gessaman, M. (1970). A consistent nonparametric multivariate density estimator based on 

statistically equivalent blocks. Annals of Mathematical Statistics, 41: 1344-1346. 

Gessaman, M. and Gessaman, P. (1972). A comparison of some multivariate discrimination 

procedures. Journal of the American Statistical Association, 67:468-472. 

Geva, S. and Sitte, J. (1991). Adaptive nearest neighbor pattern classification. IEEE Trans(cid:173)

actions on Neural Networks, 2:318-322. 

Gine, E. and Zinn, J. (1984). Some limit theorems for empirical processes. Annals of Prob(cid:173)

ability,  12:929-989. 

Glick, N. (1973). Sample-based multinomial classification. Biometrics, 29:241-256. 
Glick, N.  (1976). Sample-based classification procedures related to empiric distributions. 

IEEE Transactions on Information Theory, 22:454-461. 

Glick,  N.  (1978).  Additive  estimators  for  probabilities  of correct  classification.  Pattern 

Recognition, 10:211-222. 

Goldberg,  P.  and  Jerrum,  M.  (1993).  Bounding  the  Vapnik-Chervonenkis  dimension  of 

concept classes parametrized by real numbers. In Proceedings of the Sixth ACM Work(cid:173)
shop on Computational Learning Theory, pages 361-369. Association for Computing 
Machinery, New York. 

Goldstein, M.  (1977).  A two-group classification procedure for multivariate dichotomous 

responses. Multivariate Behavorial Research,  12:335-346. 

Goldstein,  M.  and Dillon,  W.  (1978).  Discrete  Discriminant Analysis.  John Wiley,  New 

York. 

Golea, M. and Marchand, M .. (1990). A growth algorithm for neural network decision trees. 

Europhysics Letters,  12:205-210. 

Goodman,  R.  and  Smyth,  P.  (1988).  Decision tree  design  from  a  communication theory 

viewpoint. IEEE Transactions on Information Theory, 34:979-994. 

Gordon, L. and Olshen, R. (1984). Almost surely consistent nonparametric regression from 

recursive partitioning schemes. ] ournal of Multivariate Analysis, 15: 147-163. 

Gordon, L. and Olshen, R.  (1978).  Asymptotically efficient solutions to the classification 

problem. Annals of Statistics, 6:515-533. 

604 

References 

Gordon,  L.  and  Olshen,  R.  (1980).  Consistent nonparametric  regression  from  recursive 

partitioning schemes. Journal of Multivariate Analysis, 10:611-627. 

Gowda, K. and Krishna, G. (1979). The condensed nearest neighbor rule using the concept of 
mutual nearest neighborhood. IEEE Transactions on Information Theory, 25:488-490. 

Greblicki, W.  (1974). Asymptotically optimal probabilistic algorithms for pattern recogni(cid:173)

tion and identification. Technical Report, Monografie No.3, Prace Naukowe Instytutu 
Cybernetyki Technicznej Politechniki Wroclawsjiej No. 18, Wroclaw, Poland. 

Greblicki, W. (1978a). Asymptotically optimal pattern recognition procedures with density 

estimates. IEEE Transactions on Information Theory, 24:250-251. 

Greblicki, W. (1978b). Pattern recognition procedures with nonparametric density estimates. 

IEEE Transactions on Systems, Man and Cybernetics, 8:809-812. 

Greblicki, W.  (1981).  Asymptotic efficiency of classifying procedures  using  the Hermite 
series estimate of multivariate probability densities. IEEE Transactions on Information 
Theory, 27:364-366. 

Greblicki, W., Krzyzak, A., and Pawlak, M. (1984). Distribution-free pointwise consistency 

of kernel regression estimate. Annals of Statistics, 12: 1570-1575. 

Greblicki, W.  and Pawlak,  M.  (1981).  Classification using the Fourier series  estimate of 
multivariate density  functions.  IEEE  Transactions  on Systems,  Man  and Cybernetics, 
11:726-730. 

Greblicki, W. and Pawlak, M. (1982). A classification procedure using the mUltiple Fourier 

series. Information Sciences, 26:115-126. 

Greblicki, W. and Pawlak, M. (1983). Almost sure convergence of classification procedures 

using Hermite series density estimates. Pattern Recognition Letters, 2: 13-17. 

Greblicki, W.  and Pawlak, M.  (1985). Pointwise consistency of the Hermite series density 

estimate. Statistics and Probability Letters, 3:65-69. 

Greblicki,  W.  and Pawlak, M.  (1987).  Necessary and sufficient conditions for Bayes risk 
consistency of a recursive kernel classification rule. IEEE Transactions on Information 
Theory, 33:408-412. 

Grenander, U.  (1981). Abstract Inference. John Wiley, New York. 
Grimmett, G. and Stirzaker, D. (1992). Probability and Random Processes. Oxford Univer(cid:173)

sity Press, Oxford. 

Guo, H. and Gelfand, S. (1992). Classification trees with neural network feature extraction. 

IEEE Transactions on Neural Networks, 3:923-933. 

Gyorfi,  L. (1975).  An  upper bound of error probabilities for  multihypothesis  testing  and 
its  application  in  adaptive  pattern  recognition.  Problems  of Control  and Information 
Theory, 5:449-457. 

Gyorfi, L. (1978). On the rate of convergence of nearest neighbor rules. IEEE Transactions 

on Information Theory, 29:509-512. 

Gyorfi, L. (1981). Recent results on nonparametric regression estimate and multiple clas(cid:173)

sification. Problems of Control and Information Theory,  10:43-52. 

Gyorfi, L. (1984). Adaptive linear procedures under general conditions. IEEE Transactions 

on Information Theory, 30:262-267. 

Gyorfi, L. and Gyorfi, Z. (1975). On the nonparametric estimate of a posteriori probabilities 
of simple  statistical  hypotheses.  In  Colloquia  Mathematica  Societatis  Janos  Bolyai: 
Topics in Information Theory, pages 299-308. Keszthely, Hungary. 

Gyorfi, L. and Gyorfi, Z. (1978). An upper bound on the asymptotic error probability of the 
k-nearest neighbor rule for multiple classes. IEEE Transactions on Information Theory, 
24:512-514. 

References 

605 

Gyorfi, L., Gyorfi, Z., and Vajda, I. (1978). Bayesian decision with rejection. Problems of 

Control and Information Theory,  8:445-452. 

Gyorfi, L. and Vajda, I.  (1980). Upper bound on the error probability of detection in non(cid:173)

gaussian noise. Problems of Control and Information Theory, 9:215-224. 

Haagerup,  U.  (1978).  Les  meilleures  constantes  de  l'inegalite  de  Khintchine.  Comptes 

Rendus de  [' Academie des  Sciences de Paris A, 286:259-262. 

Habbema, J., Hermans, J., and van den Broek, K. (1974). A stepwise discriminant analysis 
program using density estimation. In COMPSTAT 1974, Bruckmann, G., editor, pages 
101-110. Physica Verlag, Wien. 

Hagerup, T. and Riib, C. (1990). A guided tour of Chernoff bounds. Information Processing 

Letters, 33:305-308. 

Hall, P.  (1981). On nonparametric multivariate binary discrimination. Biometrika, 68:287-

.294. 

Hall, P.  (1989). On projection pursuit regression. Annals of Statistics,  17:573-588. 
Hall, P.  and Wand, M. (1988). On nonparametric discrimination using density differences. 

Biometrika, 75:541-547. 

Hand, D. (1981). Discrimination and Classification. John Wiley, Chichester, U.K. 
Hand,  D.  (1986).  Recent advances  in error rate  estimation.  Pattern Recognition Letters, 

4:335-346. 

HardIe, W. and Marron, J. (1985). Optimal bandwidth selection in nonparametric regression 

function estimation. Annals of Statistics,  13: 1465-1481. 

Hart, P.  (1968). The condensed nearest neighbor rule. IEEE Transactions  on Information 

Theory,  14:515-516. 

Hartigan, J.  (1975). Clustering Algorithms. John Wiley, New York. 
Hartmann,  C.,  Varshney,  P.,  Mehrotra,  K.,  and  Gerberich,  C.  (1982).  Application  of in(cid:173)

formation  theory  to  the construction of efficient decision trees. IEEE Transactions  on 
Information Theory, 28:565-577. 

Hashlamoun,  W.,  Varshney,  P.,  and  Samarasooriya,  V.  (1994).  A  tight  upper  bound  on 
the Bayesian probability of error. IEEE Transactions on Pattern Analysis and Machine 
Intelligence,  16:220-224. 

Hastie,  T.  and  Tibshirani,  R.  (1990).  Generalized Additive Models.  Chapman  and  Hall, 

London, U.K. 

Haussler,  D.  (1991).  Sphere  packing  numbers  for  subsets  of the  boolean  n-cube  with 

bounded V apnik -Chervonenkis dimension. Technical Report, Computer Research Lab(cid:173)
oratory, University of California, Santa Cruz. 

Haussler, D. (1992). Decision theoretic generalizations of the PAC model for neural net and 

other learning applications. Information and Computation,  100:78-150. 

Haussler,  D.,  Littlestone,  N.,  and  Warmuth,  M.  (1988).  Predicting  {O,  1}  functions  from 
randomly drawn points. In Proceedings of the 29th IEEE Symposium on the Foundations 
of Computer Science, pages 100-109. IEEE Computer Society Press, Los Alamitos, CA. 
Hecht-Nielsen,  R.  (1987).  Kolmogorov's  mapping  network  existence  theorem.  In  IEEE 
First  International  Conference  on  Neural  Networks,  volume  3,  pages  11-l3.  IEEE, 
Piscataway, NJ. 

Hellman,  M.  (1970).  The nearest neighbor classification  rule  with  a  reject option.  IEEE 

Transactions on Systems, Man and Cybernetics, 2: 179-185. 

Henrichon, E.  and Fu, K.  (1969). A nonparametric partitioning procedure for pattern clas(cid:173)

sification.IEEE Transactions on Computers,  18:614--624. 

606 

References 

Hertz, J., Krogh, A., and Palmer, R. (1991). Introduction to the Theory of Neural Compu(cid:173)

tation. Addison-Wesley, Redwood City, CA. 

Hills,  M.  (1966).  Allocation  rules  and  their error  rates.  Journal  of the  Royal  Statistical 

Society, B28:1-31. 

Hjort, N.  (1986a). Contribution to the discussion of a paper by P. Diaconis and Freeman. 

Annals of Statistics,  14:49-55. 

Hjort, N.  (1986b). Notes on the theory of statistical symbol recognition. Technical Report 

778, Norwegian Computing Centre, Oslo. 

Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Jour(cid:173)

nal of the American Statistical Association, 58:13-30. 

Holmstrom, L.  and KlemeUi,  J.  (1992).  Asymptotic bounds for the expected 11  error of a 

multivariate kernel density estimator. Journal of Multivariate Analysis, 40:245-255. 

Hora, S. and Wilcox, J. (1982). Estimation of error rates in several-population discriminant 

analysis. Journal of Marketing Research,  19:57-61. 

Horibe,  Y.  (1970).  On  zero error probability of binary  decisions.  IEEE  Transactions  on 

Information Theory,  16:347-348. 

Home, B. and Hush, D. (1990). On the optimality of the sigmoid perceptron. In International 
Joint Conference  on Neural Networks,  volume  1, pages  269-272. Lawrence Erlbaum 
Associates, Hillsdale, NJ. 

Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural 

Networks, 4:251-257. 

Hornik, K.  (1993). Some new results on neural network approximation. Neural Networks, 

6:1069-1072. 

Hornik, K., Stinchcombe, M., and White, H. (1989). Multi-layer feedforward networks are 

universal approximators. Neural Networks, 2:359-366. 

Huber, P.  (1985). Projection pursuit. Annals of Statistics,  13:435-525. 
Hudimoto,  H.  (1957).  A  note  on  the  probability  of the  correct  classification  when  the 
distributions are not specified. Annals of the Institute of Statistical Mathematics, 9:31-
36. 

Ito, T.  (1969).  Note on a class of statistical recognition functions.  IEEE Transactions  on 

Computers, C-18:76-79. 

Ito, T.  (1972).  Approximate error bounds in pattern recognition. In Machine Intelligence, 

Meltzer, B. and Mitchie, D., editors, pages 369-376. Edinburgh University Press, Edin(cid:173)
burgh. 

Ivakhnenko,  A.  (1968).  The  group  method  of data  handling-a rival  of the  method  of 

stochastic approximation. Soviet Automatic Control,  1:43-55. 

Ivakhnenko,  A.  (1971).  Polynomial  theory  of complex  systems.  IEEE  Transactions  on 

Systems, Man, and Cybernetics,  1:364-378. 

Ivakhnenko,  A.,  Konovalenko,  V.,  Tulupchuk,  Y.,  and Tymchenko, I. (1968).  The group 
method of data handling in pattern recognition and decision problems. Soviet Automatic 
Control,  1:31-41. 

Ivakhnenko, A., Petrache, G., and Krasyts'kyy, M. (1968). A GMDH algorithm with random 

selection of pairs. Soviet Automatic Control, 5:23-30. 

Jain, A., Dubes, R., and Chen, C.  (1987). Bootstrap techniques for error estimation. IEEE 

Transactions on Pattern Analysis and Machine Intelligence, 9:628-633. 

Jeffreys, H.  (1948). Theory of Probability. Clarendon Press, Oxford. 
Johnson, D. and Preparata, F.  (1978). The densest hemisphere problem. Theoretical Com(cid:173)

puter Science, 6:93-107. 

References 

607 

Kurkova,  V.  (1992).  Kolmogorov's theorem and multilayer neural networks. Neural Net(cid:173)

works, 5:501-506. 

Kailath, T. (1967). The divergence and Bhattacharyya distance measures in signal detection. 

IEEE Transactions on Communication Technology,  15:52-60. 

Kanal, L. (1974). Patterns in pattern recognition. IEEE Transactions on Information Theory, 

20:697-722. 

Kaplan, M.  (1985). The uses  of spatial coherence in ray tracing.  SIGGRAPH '85 Course 

Notes,  11:22-26. 

Karlin, A. (1968). Total Positivity, Volume 1. Stanford University Press, Stanford, CA. 
Karp, R. (1988). Probabilistic Analysis of Algorithms. Class Notes, University of California, 

Berkeley. 

Karpinski, M. and Macintyre, A. (1994). Quadratic bounds for vc dimension of sigmoidal 

neural networks. Submitted to the ACM Symposium on Theory of Computing. 

Kazmierczak, H. and Steinbuch, K.  (1963). Adaptive systems in pattern recognition. IEEE 

Transactions on Electronic Computers,  12:822-835. 

Kemp, R.  (1984).  Fundamentals of the Average  Case Analysis of Particular Algorithms. 

B.G. Teubner, Stuttgart. 

Kemperman, J. (1969). On the optimum rate of transmitting information. In Probability and 

Information Theory, pages 126-169. Springer Lecture Notes in Mathematics, Springer(cid:173)
Verlag, Berlin. 

Kiefer, J.  and Wolfowitz, J.  (1952). Stochastic estimation of the maximum of a regression 

function. Annals of Mathematical Statistics, 23:462-466. 

Kim,  B.  and  Park,  S.  (1986).  A  fast  k-nearest  neighbor finding  algorithm  based on the 
ordered  partition.  IEEE  Transactions  on  Pattern Analysis  and Machine  Intelligence, 
8:761-766. 

Kittler, J. and Devijver, P. (1981). An efficient estimator of pattern recognition system error 

probability. Pattern Recognition,  13:245-249. 

Knoke, J. (1986). The robust estimation of classification error rates. Computers and Math(cid:173)

ematics with Applications, 12A:253-260. 

Kohonen, T.  (1988). Self-Organization and Associative Memory. Springer-Verlag, Berlin. 
Kohonen, T.  (1990). Statistical pattern recognition revisited. In Advanced Neural Comput(cid:173)

ers, Eckmiller, R., editor, pages 137-144. North-Holland, Amsterdam. 

Kolmogorov, A. (1957). On the representation of continuous functions of many variables by 
superposition of continuous functions  of one variable and addition. Doklady Akademii 
Nauk USSR,  114:953-956. 

Kolmogorov,  A.  and Tikhomirov,  V.  (1961).  E-entropy  and E-capacity of sets in function 

spaces. Translations of the American Mathematical Society,  17:277-364. 

Koutsougeras,  C.  and  Papachristou,  C.  (1989).  Training  of a  neural  network for  pattern 

classification based on an entropy measure. In Proceedings of IEEE International Con(cid:173)
ference  on Neural Networks, volume  1, pages 247-254. IEEE San Diego Section, San 
Diego, CA. 

Kraaijveld,  M.  and Duin, R.  (1991).  Generalization capabilities of minimal kernel-based 
methods. In International Joint Conference on Neural Networks, volume 1, pages 843-
848. Piscataway, NJ. 

Kronmal, R. and Tarter, M. (1968). The estimation of probability densities and cumulatives 
by Fourier series methods. Journal of the American Statistical Association, 63:925-952. 
Krzanowski, W. (1987). A comparison between two distance-based discriminant principles. 

Journal of Classification, 4:73-84. 

608 

References 

Krzyzak,  A.  (1983).  Classification procedures  using  multivariate  variable kernel  density 

estimate. Pattern Recognition Letters, 1:293-298. 

Krzyzak, A. (1986). The rates of convergence of kernel regression estimates and classifica(cid:173)

tion rules. IEEE Transactions on Information Theory, 32:668-679. 

Krzyzak, A.  (1991).  On exponential bounds on the bayes risk of the kernel classification 

rule. IEEE Transactions on Information Theory, 37:490-499. 

Krzyzak, A., Linder, T., and Lugosi, G. (1993). Nonparametric estimation and classification 
using radial basis function nets and empirical risk minimization. IEEE Transactions on 
Neural Networks. To appear. 

Krzyzak, A. and Pawlak, M.  (1983). Universal consistency results for Wolverton-Wagner 
regression function estimate with application in discrimination. Problems of Control and 
Information Theory,  12:33-42. 

Krzyzak, A.  and Pawlak, M. (1984a). Almost everywhere convergence of recursive kernel 

regression function estimates. IEEE Transactions on Information Theory, 31:91-93. 

Krzyzak,  A.  and  Pawlak,  M.  (1984b).  Distribution-free  consistency  of a  nonparametric 
kernel regression estimate and classification. IEEE Transactions on Information Theory, 
30:78-81. 

Kulkarni,  S.  (1991).  Problems  of computational and information  complexity in machine 
vision and learning. PhD Thesis, Department of Electrical Engineering and Computer 
Science, MIT, Cambridge, MA. 

Kullback,  S.  (1967). A lower bound for discrimination information in terms of variation. 

IEEE Transactions on Information Theory,  13:126-127. 

Kullback, S. and Leibler, A. (1951). On information and sufficiency. Annals of Mathematical 

Statistics, 22:79-86. 

Kushner, H. (1984). Approximation and Weak Convergence Methods for Random Processes 

with Applications to Stochastic Systems Theory.  MIT Press, Cambridge, MA. 

Lachenbruch, P.  (1967). An almost unbiased method of obtaining confidence intervals for 

the probability of misclassification in discriminant analysis. Biometrics, 23:639-645. 

Lachenbruch, P.  and Mickey, M. (1968). Estimation of error rates in discriminant analysis. 

Technometrics,  10: 1-11. 

LeCam, L. (1970). On the  assumptions  used to prove asymptotic normality of maximum 

likelihood estimates. Annals of Mathematical Statistics, 41:802-828. 

LeCam, L. (1973).  Convergence of estimates under dimensionality restrictions. Annals of 

Statistics,  1 :38-53. 

Li,  X.  and Dubes,  R.  (1986).  Tree  classifier design  with  a permutation  statistic.  Pattern 

Recognition,  19:229-235. 

Lin,  Y.  and Fu,  K.  (1983).  Automatic  classification  of cervical cells  using  a  binary tree 

classifier. Pattern Recognition, 16:69-80. 

Linde, Y.,  Buzo, A., and Gray, R.  (1980). An algorithm for vector quantizer design. IEEE 

Transactions on Communications, 28:84-95. 

Linder, T., Lugosi, G., and Zeger, K. (1994). Rates of convergence in the source coding theo(cid:173)

rem, empirical quantizer design, and universal lossy source coding. IEEE Transactions 
on Information Theory, 40:1728-1740. 

Lissack, T. and Fu, K. (1976). Error estimation in pattern recognition via za  distance between 

posterior density functions. IEEE Transactions on Information Theory, 22:34-45. 

Ljung, L., Pflug, G., and Walk,  H.  (1992).  Stochastic Approximation and Optimization of 

Random Systems. Birkhauser, Basel, Boston, Berlin. 

References 

609 

Lloyd,  S.  (1982).  Least squares  quantization  in PCM.  IEEE  Transactions  on Information 

Theory, 28:129-137. 

Loftsgaarden, D. and Quesenberry, C.  (1965). A nonparametric estimate of a multivariate 

density function. Annals of Mathematical Statistics,. 36: 1049-1051. 

Logan, B.  (1975). The uncertainty principle in reconstructing functions from projections. 

Duke Mathematical Journal,  42:661-706. 

Logan, B. and Shepp, L. (1975). Optimal reconstruction of a function from its projections. 

Duke Mathematical Journal, 42:645-660. 

Loh, W.  and Vanichsetakul,  N.  (1988).  Tree-structured classification via generalized dis(cid:173)

criminant analysis. Journal of the American Statistical Association, 83:715-728. 

Loizou, G. and Maybank, S. (1987). The nearest neighbor and the Bayes error rates. IEEE 

Transactions on Pattern Analysis and Machine Intelligence, 9:254-262. 

Lorentz, G. (1976). The thirteenth problem of Hilbert. In Proceedings of Symposia in Pure 

Mathematics, volume 28, pages 419-430. Providence, RI. 

Lugosi, G.  (1992). Learning with an unreliable teacher. Pattern Recognition, 25:79-87. 
Lugosi, G. and Nobel, A. (1996). Consistency of data-driven histogram methods for density 

estimation and classification. Annals of Statistics, 24:687-706. 

Lugosi, G. and Pawlak, M. (1994). On the posterior-probability estimate of the error rate of 
nonparametric classification rules. IEEE Transactions on Information Theory, 40:475-
481. 

Lugosi, G. and Zeger, K. (1995). Nonparametric estimation via empirical risk minimization. 

IEEE Transactions on Information Theory, 41:677-678. 

Lugosi, G.  and Zeger, K.  (1996). Concept learning using complexity regularization. IEEE 

Transactions on Information Theory, 42:48-54. 

Lukacs,  E.  and Laha,  R.  (1964). Applications of Characteristic Functions  in Probability 

Theory. Griffin, London. 

Lunts, A. and Brailovsky, V. (1967). Evaluation of attributes obtained in statistical decision 

rules. Engineering Cybernetics, 3:98-109. 

Maass, W.  (1993).  Bounds for  the  computational power and learning complexity of ana(cid:173)

log neural nets.  In Proceedings of the 25th Annual ACM Symposium on the Theory of 
Computing, pages 335-344. Association of Computing Machinery, New York. 

Maass, W. (1994). Neural nets with superlinear vc-dimension. Neural Computation, 6:875-

882. 

Macintyre, A.  and Sontag, E.  (1993). Finiteness results for sigmoidal "neural" networks. 
In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, pages 
325-334. Association of Computing Machinery, New York. 

Mack, Y. (1981). Local properties of k -nearest neighbor regression estimates. SIAM Journal 

on Algebraic and Discrete Methods, 2:311-323. 

Mahalanobis, P. (1936). On the generalized distance in statistics. Proceedings of the National 

Institute of Sciences of India, 2:49-55. 

Mahalanobis, P.  (1961). A method offractile graphical analysis. Sankhya Series A, 23:41-

64. 

Marron, J.  (1983). Optimal rates of convergence to Bayes risk in nonparametric discrimi(cid:173)

nation. Annals of Statistics,  11:1142-1155. 

Massart, P.  (1983).  Vitesse  de  convergence dans le  theoreme de  la  limite centrale pour le 

processus empirique. PhD Thesis, Universite Paris-Sud, Orsay, France. 

Massart, P. (1990). The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. Annals 

of Probability,  18: 1269-1283. 

610 

References 

Mathai,  A.  and  Rathie,  P.  (1975).  Basic  Concepts  in  Information  Theory  and Statistics. 

Wiley Eastern Ltd., New Delhi. 

Matloff, N. and Pruitt, R.  (1984). The asymptotic distribution of an estimator of the bayes 

error rate. Pattern Recognition Letters, 2:271-274. 

Matula, D. and Sokal, R. (1980). Properties of Gabriel graphs relevant to geographic varia(cid:173)
tion research and the clustering of points in the plane. Geographical Analysis, 12:205-
222. 

Matushita, K. (1956). Decision rule, based on distance, for the classification problem. Annals 

of the Institute of Statistical Mathematics, 8:67-77. 

Matushita, K. (1973). Discrimination and the affinity of distributions. In Discriminant Anal(cid:173)

ysis and Applications, Cacoullos, T., editor, pages 213-223. Academic Press, New York. 
Max,  J.  (1960).  Quantizing  for  minimum distortion.  IEEE  Transactions  on Information 

Theory, 6:7-12. 

McDiarmid, C. (1989). On the method of bounded differences. In Surveys in Combinatorics 

1989, pages  148-188. Cambridge University Press, Cambridge. 

McLachlan,  G.  (1976).  The  bias  of  the  apparent  error  rate  in  discriminant  analysis. 

Biometrika, 63:239-244. 

McLachlan,  G.  (1992).  Discriminant Analysis and Statistical Pattern Recognition.  John 

Wiley, New York. 

Meisel, W.  (1969). Potential functions in mathematical pattern recognition. IEEE Transac(cid:173)

tions on Computers,  18:911-918. 

Meisel, W. (1972). Computer Oriented Approaches to Pattern Recognition. Academic Press, 

New York. 

Meisel, W.  (1990). Parsimony in neural networks. In International Conference on Neural 

Networks, pages 443-446. Lawrence Erlbaum Associates, Hillsdale, NJ. 

Meisel,  W.  and  Michalopoulos,  D.  (1973).  A  partitioning  algorithm  with  application  in 

pattern classification and the optimization of decision tree. IEEE Transactions on Com(cid:173)
puters, 22:93-103. 

Michel-Briand, C. and Milhaud, X.  (1994). Asymptotic behavior of the AID method. Tech(cid:173)

nical Report, Universite Montpellier 2, Montpellier. 

Mielniczuk, J. and Tyrcha, J.  (1993). Consistency of multilayer perceptron regression esti(cid:173)

mators. Neural Networks, To appear. 

Minsky,  M.  (1961).  Steps  towards  artificial  intelligence. In Proceedings of the IRE,  vol(cid:173)

ume 49, pages 8-30. 

Minsky, M. and Papert, S. (1969). Perceptrons,' An Introduction to Computational Geometry. 

MIT Press, Cambridge, MA. 

Mitchell, A.  and Krzanowski,  W.  (1985). The Mahalanobis distance and elliptic distribu(cid:173)

tions. Biometrika, 72:464-467. 

Mizoguchi, R., Kizawa, M.,  and Shimura, M.  (1977). Piecewise linear discriminant func(cid:173)

tions in pattern recognition. Systems-Computers-Controls, 8: 114-121. 

Moody,  J.  and  Darken,  J.  (1989).  Fast learning  in  networks  of locally-tuned  processing 

units. Neural Computation,  1 :281-294. 

Moore,  D.,  Whitsitt,  S.,  and  Landgrebe,  D.  (1976).  Variance  comparisons  for  unbiased 
estimators  of probability  of correct classification.  IEEE  Transactions  on Information 
Theory, 22: 102-105. 

Morgan, J. and Sonquist, J. (1963). Problems in the analysis of survey data, and a proposal. 

Journal of the American Statistical Association, 58:415-434. 

References 

611 

Mui, 1. and Fu, K. (1980). Automated classification of nucleated blood cells using a binary 
tree classifier. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2:429-
443. 

Myles, 1.  and Hand, D.  (1990). The multi-class metric problem in nearest neighbour dis(cid:173)

crimination rules. Pattern Recognition, 23: 1291-1297. 

Nadaraya, E. (1964). On estimating regression. Theory of Probability and its Applications, 

9: 141-142. 

Nadaraya, E. (1970). Remarks on nonparametric estimates for density functions and regres(cid:173)

sion curves. Theory of Probability and its Applications, 15: 134-137. 

Narendra,  P.  and Fukunaga,  K.  (1977).  A  branch  and bound algorithm for feature  subset 

selection. IEEE Transactions on Computers, 26:917-922. 

Natarajan, B. (1991). Machine Learning: A Theoretical Approach. Morgan Kaufmann, San 

Mateo, CA. 

Nevelson,  M.  and Khasminskii,  R.  (1973).  Stochastic Approximation and Recursive Es(cid:173)

timation. Translations of Mathematical Monographs, Vol.  47. American Mathematical 
Society, Providence, RI. 

Niemann,  H.  and Goppert,  R.  (1988).  An efficient branch-and-bound nearest neighbour 

classifier. Pattern Recognition Letters, 7:67-72. 

Nilsson, N. (1965). Learning Machines: Foundations of Trainable Pattern Classifying Sys(cid:173)

tems. McGraw-Hill, New York. 

Nishizeki, T. and Chiba, N. (1988). Planar Graphs: Theory and Algorithms. North Holland, 

Amsterdam. 

Nobel, A. (1994). Histogram regression estimates using data-dependent partitions. Techni(cid:173)

cal Report, Beckman Institute, University of Illinois, Urbana-Champaign. 

Nobel,  A.  (1992).  On  uniform  laws  of averages.  PhD  Thesis,  Department  of Statistics, 

Stanford University, Stanford, CA. 

Nolan, D. and Pollard, D. (1987). U-processes: Rates of convergence. Annals of Statistics, 

15:780-799. 

Okamoto, M. (1958). Some inequalities relating to the partial sum of binomial probabilities. 

Annals of the Institute of Statistical Mathematics,  10:29-35. 

Olshen, R. (1977). Comments on a paper by C.l. Stone. Annals of Statistics, 5:632-633. 
Ott, 1. and Kronmal, R. (1976). Some classification procedures for multivariate binary data 
using orthogonal functions. Journal of the American Statistical Association, 71:391-
399. 

Papadimitriou, C. and Bentley, 1. (1980). A worst-case analysis of nearest neighbor search(cid:173)

ing  by projection.  In Automata,  Languages and Programming  1980,  pages  470-482. 
Lecture Notes in Computer Science #85, Springer-Verlag, Berlin. 

Park, 1. and Sandberg, I. (1991). Universal approximation using radial-basis-function net(cid:173)

works. Neural Computation, 3:246-257. 

Park, 1. and Sandberg, I. (1993). Approximation and radial-basis-function networks. Neural 

Computation, 5:305-316. 

Park, Y. and Sklansky, 1. (1990). Automated design of linear tree classifiers. Pattern Recog(cid:173)

nition, 23:1393-1412. 

Parthasarathy, K.  and Bhattacharya, P.  (1961).  Some limit theorems  in regression theory. 

Sankhya Series A, 23:91-102. 

Parzen, E. (1962). On the estimation of a probability density function and the mode. Annals 

of Mathematical Statistics, 33: 1065-1076. 

612 

References 

Patrick, E. (1966). Distribution-free minimum conditional risk learning systems. Technical 

Report TR-EE-66-18, Purdue University, Lafayette, IN. 

Patrick, E. and Fischer, F.  (1970). A generalized k-nearest neighbor rule. Information and 

Control,16:128-152. 

Patrick,  E.  and  Fisher,  F.  (1967).  Introduction  to  the  performance  of distribution-free 
conditional risk learning systems. Technical Report TR-EE-67-12, Purdue University, 
Lafayette, IN. 

Pawlak, M. (1988). On the asymptotic properties of smoothed estimators of the classification 

error rate. Pattern Recognition, 21:515-524. 

Payne,  H.  and Meisel,  W.  (1977).  An  algorithm for constructing optimal binary decision 

trees. IEEE Transactions on Computers, 26:905-916. 

Pearl, J. (1979). Capacity and error estimates for boolean classifiers with limited complexity. 

IEEE Transactions on Pattern Analysis and Machine Intelligence,  1:350-355. 

Penrod, C. and Wagner, T.  (1977). Another look at the edited nearest neighbor rule. IEEE 

Transactions on Systems, Man and Cybernetics, 7:92-94. 

Peterson, D. (1970). Some convergence properties of a nearest neighbor decision rule. IEEE 

Transactions on Information Theory,  16:26-31. 

Petrov, V.  (1975). Sums of Independent Random Variables.  Springer-Verlag, Berlin. 
Pippenger, N.  (1977). Information theory and the complexity of boolean functions. Math(cid:173)

ematical Systems Theory,  10: 124-162. 

Poggio,  T.  and Girosi,  F.  (1990).  A  theory  of networks  for  approximation  and learning. 

Proceedings of the IEEE, 78: 1481-1497. 

Pollard, D.  (1981).  Strong consistency of k-means clustering. Annals of Statistics, 9: 135-

140. 

Pollard, D. (1982). Quantization and the method of k-means. IEEE Transactions on Infor(cid:173)

mation Theory, 28: 199-205. 

Pollard, D. (1984). Convergence of Stochastic Processes. Springer-Verlag, New York. 
Pollard, D.  (1986).  Rates of uniform almost sure convergence for empirical processes in(cid:173)

dexed by unbounded classes of functions. Manuscript. 

Pollard, D.  (1990). Empirical Processes:  Theory and Applications. NSF-CBMS Regional 
Conference  Series  in  Probability  and  Statistics,  Institute  of Mathematical  Statistics, 
Hayward, CA. 

Powell, M.  (1987).  Radial basis functions for multivariable interpolation:  a review. Algo(cid:173)

rithms for Approximation. Clarendon Press, Oxford. 

Prep arata, F. and Shamos, M. (1985). Computational Geometry-An Introduction. Springer(cid:173)

Verlag, New York. 

Psaltis, D., Snapp, R.,  and Venkatesh,  S.  (1994). On the finite sample performance of the 

nearest neighbor classifier. IEEE Transactions on Information Theory, 40:820-837. 

Qing-Yun, S. and Fu, K. (1983). A method for the design of binary tree classifiers. Pattern 

Recognition, 16:593-603. 

Quesenberry, c. and Gessaman, M. (1968). Nonparametric discrimination using tolerance 

regions. Annals of Mathematical Statistics, 39:664-673. 

Quinlan, J. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo. 
Rabiner, L., Levinson, S., Rosenberg, A., and Wilson, J. (1979). Speaker-independentrecog(cid:173)

nition of isolated words using clustering techniques. IEEE Transactions  on Acoustics, 
Speech, and Signal Processing, 27:339-349. 

References 

613 

Rao, R. (1962). Relations between weak and uniform convergence of measures with appli(cid:173)

cations. Annals of Mathematical Statistics, 33:659-680. 

Raudys,  S.  (1972).  On the  amount of a  priori information in designing the classification 

algorithm. Technical Cybernetics, 4: 168-174. 

Raudys, S. (1976). On dimensionality, learning sample size and complexity of classification 
algorithms. In Proceedings of the 3rd International Conference on Pattern Recognition, 
pages 166-169. IEEE Computer Society, Long Beach, CA. 

Raudys, S. and Pikelis, V.  (1980). On dimensionality, sample size, classification error, and 
complexity  of classification  algorithm  in  pattern  recognition.  IEEE  Transactions  on 
Pattern Analysis and Machine Intelligence, 2:242-252. 

Raudys,  S.  and  Pikelis,  V.  (1982).  Collective  selection  of the  best  version  of a  pattern 

recognition system. Pattern Recognition Letters, 1 :7-13. 

Rejto, L. and Revesz, P.  (1973). Density estimation and pattern classification. Problems of 

Control and Information Theory, 2:67-80. 

Renyi, A. (1961). On measures of entropy and information. In Proceedings of the Fourth 

Berkeley Symposium, pages 547-561. University of California Press, Berkeley. 

Revesz, P. (1973). Robbins-Monroe procedures in a Hilbert space and its application in the 
theory of learning processes. Studia Scientiarium Mathematicarum Hungarica, 8:391-
398. 

Ripley, B. (1993). Statistical aspects of neural networks. In Networks and Chaos--Statistical 
and Probabilistic Aspects, Barndorff-Nielsen, 0., Jensen, J.,  and Kendall, W.,  editors, 
pages 40--123. Chapman and Hall, London, U.K. 

Ripley, B.  (1994).  Neural networks  and related methods for classification. Journal of the 

Royal Statistical Society, 56:409-456. 

Rissanen, l. (1983). A universal prior for integers and estimation by minimum description 

length. Annals of Statistics, 11:416-43l. 

Ritter, G., Woodruff, H., Lowry, S., and Isenhour, T.  (1975). An algorithm for a  selective 
nearest neighbor decision rule. IEEE Transactions on Information Theory, 21: 665-669. 

Robbins, H.  and Monro, S.  (1951). A stochastic approximation method. Annals of Mathe(cid:173)

matical Statistics, 22: 400-407. 

Rogers, W. and Wagner, T. (1978). A finite sample distribution-free performance bound for 

local discrimination rules. Annals of Statistics, 6:506-514. 

Rosenblatt, F.  (1962). Principles ofNeurodynamics: Perceptrons and the Theory  of Brain 

Mechanisms.  Spartan Books, Washington, DC. 

Rosenblatt, M.  (1956). Remarks on some nonparametric estimates of a  density function. 

Annals of Mathematical Statistics, 27:832-837. 

Rounds,  E.  (1980).  A  combined nonparametric  approach to feature  selection and binary 

decision tree design. Pattern Recognition,  12:313-317. 

Royall, R. (1966). A Class of N onparametric Estimators of a Smooth Regression Function. 

PhD Thesis, Stanford University, Stanford, CA. 

Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning internal representations by er(cid:173)

ror propagation. In Parallel Distributed Processing Vol. I, Rumelhart, D., J .L.McCelland, 
and the PDP Research Group, editors. MIT Press, Cambridge, MA. Reprinted in:  l.A. 
Anderson and E. Rosenfeld, Neurocomputing--Foundations  of Research,  MIT Press, 
Cambridge, MA., pp. 673-695,1988. 

Ruppert, D. (1991). Stochastic approximation. In Handbook of Sequential Analysis, Ghosh, 

B. and Sen, P., editors, pages 503-529. Marcel Dekker, New York. 

614 

References 

Samet, H. (1984). The quadtree and related hierarchical data structures. Computing Surveys, 

16:187-260. 

Samet, H. (1990a). Applications of Spatial Data Structures. Addison-Wesley, Reading, MA. 
Samet, H.  (1990b).  The Design and Analysis of Spatial Data Structures. Addison-Wesley, 

Reading, MA. 

Sansone, G. (1969). Orthogonal Functions. Interscience, New York. 
Sauer, N. (1972). On the density of families of sets. Journal of Combinatorial Theory Series 

A,13:145-147. 

Scheff6, H.  (1947). A useful convergence theorem for probability distributions. Annals of 

Mathematical Statistics,  18:434-458. 

SchIaffli, L. (1950). Gesammelte Mathematische Abhandlungen. Birkhauser-Verlag, Basel. 
Schmidt, W. (1994). Neural Pattern Classifying Systems. PhD Thesis, Technical University, 

Delft, The Netherlands. 

Schoenberg,  I.  (1950).  On P6lya  frequency  functions,  II:  variation-diminishing  integral 
operators of the convolution type. Acta Scientiarium Mathematicarum  Szeged,  12:97-
106. 

Schwartz, S.  (1967). Estimation of probability density by an orthogonal series. Annals of 

Mathematical Statistics, 38:1261-1265. 

Schwemer, G.  and Dunn, O. (1980). Posterior probability estimators in classification sim(cid:173)

ulations. Communications in Statistics--Simulation, B9: 133-140. 

Sebestyen, G. (1962). Decision-Making Processes in Pattern Recognition. Macmillan, New 

York. 

Serfiing, R.  (1974). Probability inequalities for the sum in sampling without replacement. 

Annals of Statistics, 2:39-48. 

Sethi, I. (1981). A fast algorithm for recognizing nearest neighbors. IEEE Transactions on 

Systems, Man and Cybernetics,  11:245-248. 

Sethi, I. (1990). Entropy nets: from decision trees to neural nets. Proceedings of the IEEE, 

78:1605-1613. 

Sethi, I. (1991). Decision tree performance enhancement using an artificial neural network 
interpretation.  In Artificial Neural Networks  and Statistical Pattern Recognition,  Old 
and New  Connections,  Sethi,  I. and Jain,  A.,  editors,  pages  71-88.  Elsevier  Science 
Publishers, Amsterdam. 

Sethi, I. and Chatterjee, B. (1977). Efficient decision tree design for discrete variable pattern 

recognition problems. Pattern Recognition, 9: 197-206. 

Sethi, I. and Sarvarayudu, G.  (1982). Hierarchical classifier design using mutual informa(cid:173)

tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:441-445. 

Shannon,  C.  (1948).  A  mathematical  theory  of communication.  Bell  Systems  Technical 

Journal,27:379-423. 

Shawe-Taylor, J.  (1994).  Sample sizes for  sigmoidal  neural networks.  Technical Report, 

Department of Computer Science, Royal Holloway, University of London, Egham, Eng(cid:173)
land. 

Shawe-Taylor,  J.,  Anthony,  M.,  and Biggs,  N.L.  (1993).  Bounding sample  size  with the 

Vapnik-Chervonenkis dimension. Discrete Applied Mathematics, 42:65-73. 

Shiryayev, A. (1984). Probability. Springer-Verlag, New York. 
Short, R.  and Fukunaga,  K.  (1981).  The  optimal  distance  measure  for  nearest  neighbor 

classification. IEEE Transactions on Information Theory, 27:622-627. 

References 

615 

Simon,  H.  (1991).  The  Vapnik-Chervonenkis  dimension of decision  trees  with bounded 

rank. Information Processing Letters, 39:137-141. 

Simon, H.  (1993). General lower bounds on the number of examples needed for learning 

probabilistic concepts. In Proceedings of the Sixth Annual ACM Conference on Compu(cid:173)
tational Learning Theory, pages 402-412. Association for Computing Machinery, New 
York. 

Sklansky, J. and Michelotti (1980). Locally trained piecewise linear classifiers. IEEE Trans(cid:173)

actions on Pattern Analysis and Machine Intelligence, 2: 101-111. 

Sklansky, J. and Wassel, G.  (1979). Pattern Classifiers and Trainable Machines. Springer(cid:173)

Verlag, New York. 

Slud, E. (1977). Distribution inequalities for the binomial law. Annals of Probability, 5: 404-

412. 

Specht, D. (1967). Generation of polynomial discriminant functions for pattern classifica(cid:173)

tion. IEEE Transactions on Electronic Computers,  15:308-319. 

Specht,  D.  (1971).  Series  estimation  of  a  probability  density  function.  Technometrics, 

13:409-424. 

Specht, D. (1990). Probabilistic neural networks and the polynomial Adaline as complemen(cid:173)
tary techniques for classification. IEEE Transactions on Neural Networks,  1: 111-121. 

Spencer, J.  (1987). Ten Lectures on the Probabilistic Method. SIAM, Philadelphia, PA. 
Sprecher, D.  (1965). On the structure of continuous functions of several variables. Trans(cid:173)

actions of the American Mathematical Society,  115:340-355. 

Steele,  J.  (1975).  Combinatorial  entropy  and uniform  limit  laws.  PhD  Thesis,  Stanford 

University, Stanford, CA. 

Steele, J. (1986). An Efron-Stein inequality for nonsymmetric statistics. Annals of Statistics, 

14:753-758. 

Stengle,  G.  and  Yukich,  J.  (1989).  Some  new  Vapnik-Chervonenkis  classes.  Annals  of 

Statistics,  17: 1441-1446. 

Stoffel,  J.  (1974).  A  classifier design  technique  for  discrete  variable  pattern  recognition 

problems. IEEE Transactions on Computers, 23:428-441. 

Stoller, D.  (1954).  Univariate  two-population distribution-free  discrimination. Journal of 

the American Statistical Association, 49:770-777. 

Stone, C. (1977). Consistent nonparametric regression. Annals of Statistics, 5:595-645. 
Stone, C. (1982). Optimal global rates of convergence for nonparametric regression. Annals 

of Statistics,  10: 1040-1053. 

Stone, C. (1985). Additive regression and other nonparametric models. Annals of Statistics, 

13:689-705. 

Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. Journal 

of the Royal Statistical Society, 36: 111-147. 

Stute, W.  (1984). Asymptotic normality of nearest neighbor regression function estimates. 

Annals of Statistics,  12:917-926. 

Sung, K. and Shirley, P. (1992). Ray tracing with the BSP tree. In Graphics Gems III, Kirk, 

D., editor, pages 271-274. Academic Press, Boston, MA. 

Swonger,  C.  (1972).  Sample set condensation for a  condensed nearest neighbor decision 
rule for pattern recognition.  In Frontiers of Pattern Recognition, Watanabe,  S., editor, 
pages 511-519. Academic Press, New York. 

Szarek, S. (1976). On the best constants in the Khintchine inequality. Studia Mathematica, 

63:197-208. 

616 

References 

Szego, G.  (1959).  Orthogonal Polynomials, volume 32.  American Mathematical Society, 

Providence, RI. 

Talagrand, M. (1987). The Glivenko-Cantelli problem. Annals of Probability,  15:837-870. 
Talagrand,  M.  (1994).  Sharper  bounds  for  Gaussian  and  empirical  processes.  Annals  of 

Probability, 22:28-76. 

Talmon,  J.  (1986).  A multiclass nonparametric partitioning algorithm.  In Pattern Recog(cid:173)

nition in Practice II,  Gelsema, E.  and Kanal, L., editors. Elsevier Science Publishers, 
Amsterdam. 

Taneja, 1.  (1983).  On characterization of J-divergence  and its  generalizations. Journal of 

Combinatorics, Information and System Sciences, 8:206-212. 

Taneja,1. (1987). Statistical aspects of divergence measures. Journal of Statistical Planning 

and Inference,  16: 137-145. 

Tarter, M. and Kronmal, R.  (1970). On multivariate density estimates based on orthogonal 

expansions. Annals of Mathematical Statistics, 41:718-722. 

Tomek, 1.  (1976a). A generalization of the k-NN rule. IEEE Transactions on Systems, Man 

and Cybernetics, 6:121-126. 

Tomek,  1.  (1976b).  Two  modifications  of CNN.  IEEE  Transactions  on  Systems,  Man  and 

Cybernetics, 6:769-772. 

Toussaint, G.  (1971). Note on optimal selection of independent binary-valued features for 

pattern recognition. IEEE Transactions on Information Theory,  17:618. 

Toussaint, G. (1974a). Bibliography on estimation ofmisclassification. IEEE Transactions 

on Information Theory, 20:472-479. 

Toussaint, G.  (1974b). On the divergence between two distributions and the probability of 
misclassification of several decision rules. In Proceedings of the Second International 
Joint Conference on Pattern Recognition, pages 27-34. Copenhagen. 

Toussaint, G.  and Donaldson, R.  (1970). Algorithms for recognizing contour-traced hand(cid:173)

printed characters. IEEE Transactions on Computers,  19:541-546. 

Tsypkin, Y.  (1971). Adaptation and Learning in Automatic Systems. Academic Press, New 

York. 

Tutz, G.  (1985). Smoothed additive estimators for non-error rates in multiple discriminant 

analysis. Pattern Recognition, 18:151-159. 

Tutz, G. (1986). An alternative choice of smoothing for kernel-based density estimates in 

discrete discriminant analysis. Biometrika, 73:405-411. 

Tutz,  G.  (1988).  Smoothing  for  discrete  kernels  in  discrimination.  Biometrics Journal, 

6:729-739. 

Tutz,  G.  (1989).  On cross-validation for discrete kernel estimates in discrimination. Com(cid:173)

munications in Statistics-Theory and Methods,  18:4145-4162. 

Ullmann,  J.  (1974).  Automatic  selection of reference  data for  use  in  a  nearest-neighbor 
method of pattern  classification.  IEEE  Transactions  on Information  Theory,  20:541-
543. 

Vajda, I. (1968). The estimation of minimal error probability for testing finite or countable 

number of hypotheses. Problemy Peredaci Informacii, 4:6-14. 

Vajda, 1.  (1989).  Theory of Statistical Inference and Information. Kluwer Academic Pub(cid:173)

lishers, Dordrecht. 

Valiant, L. (1984). A theory of the learnable. Communications of the ACM, 27:1134-1142. 
Van Campenhout, J.  (1980).  The arbitrary relation between probability of error and mea(cid:173)

surement subset. Journal of the American Statistical Association, 75: 104-109. 

References 

617 

Van  Ryzin,  J.  (1966).  Bayes  risk  consistency  of classification  procedures  using  density 

estimation. Sankhya Series A, 28:161-170. 

Vapnik, V. (1982). Estimation of Dependencies Based on Empirical Data. Springer-Verlag, 

New York. 

Vapnik, V. and Chervonenkis, A. (1971). On the uniform convergence of relative frequencies 
of events to their probabilities. Theory of Probability and its Applications, 16:264-280. 
Vapnik,  V.  and Chervonenkis, A.  (1974a).  Ordered risk minimization. I. Automation and 

Remote Control, 35:1226-1235. 

Vapnik,  V.  and Chervonenkis, A.  (197 4b). Ordered risk minimization. II. Automation and 

Remote Control, 35: 1403-1412. 

Vapnik, V. and Chervonenkis, A. (197 4c). Theory of Pattern Recognition. Nauka, Moscow. 
(in  Russian);  German  translation:  Theorie  der  Zeichenerkennung,  Akademie  Verlag, 
Berlin, 1979. 

Vapnik, V. and Chervonenkis, A. (1981). Necessary and sufficient conditions for the uniform 
convergence of means to their expectations. Theory of Probability and its Applications, 
26:821-832. 

Vidal,  E.  (1986).  An algorithm for finding  nearest neighbors  in (approximately) constant 

average time. Pattern Recognition Letters, 4: 145-157. 

Vilmansen, T. (1973). Feature evaluation with measures of probabilistic dependence. IEEE 

Transactions on Computers, 22:381-388. 

Vitushkin, A. (1961). The absolute E-entropy of metric spaces. Translations of the American 

Mathematical Society,  17:365-367. 

Wagner, T.  (1971). Convergence of the nearest neighbor rule. IEEE Transactions on Infor(cid:173)

mation Theory,  17:566-571. 

Wagner,  T.  (1973).  Convergence  of the  edited  nearest  neighbor.  IEEE  Transactions  on 

Information Theory,  19:696-699. 

Wang,  Q.  and  Suen,  C.  (1984).  Analysis  and  design  of decision  tree  based  on  entropy 
reduction and its  application to large character set recognition. IEEE Transactions  on 
Pattern Analysis and Machine Intelligence, 6:406-417. 

Warner, H., Toronto, A., Veasey, L., and Stephenson, R.  (1961). A mathematical approach 

to medical diagnosis. Journal of Jhe American Medical Association, 177: 177-183. 

Wassel, G.  and Sklansky, J.  (1972). Training a one-dimensional classifier to minimize the 
probability of error. IEEE Transactions on Systems, Man, and Cybernetics, 2:533-541. 

Watson, G. (1964). Smooth regression analysis. Sankhya Series A, 26:359-372. 
Weiss,  S.  and Kulikowski,  C.  (1991).  Computer Systems  that Learn.  Morgan Kaufmann, 

San Mateo, CA. 

Wenocur, R.  and Dudley, R.  (1981). Some special Vapnik-Chervonenkis classes. Discrete 

Mathematics, 33:313-318. 

Wheeden, R. and Zygmund, A. (1977). Measure and Integral. Marcel Dekker, New York. 
White,  H.  (1990).  Connectionist  nonparametric  regression:  multilayer  feedforward  net(cid:173)

works can learn arbitrary mappings. Neural Networks, 3:535-549. 

White, H. (1991). Nonparametric estimation of conditional quantiles using neural networks. 

In Proceedings of the 23rd Symposium of the Interface:  Computing Science and Statis(cid:173)
tics, pages  190-199. American Statistical Association, Alexandria, VA. 

Widrow,  B. (1959). Adaptive sampled-data systems-a statistical theory of adaptation. In 

IRE WESCON Convention Record, volume part 4, pages 74-85. 

618 

References 

Widrow, B. and Hoff, M. (1960). Adaptive switching circuits. In IRE WESCON Convention 
Record,  volume  part 4,  pages  96-104. Reprinted in  J .A.  Anderson and E.  Rosenfeld, 
Neurocomputing: Foundations of Research, MIT Press, Cambridge, MA., 1988. 

Wilson, D. (1972). Asymptotic properties of nearest neighbor rules using edited data. IEEE 

Transactions on Systems, Man and Cybernetics, 2:408-42l. 

Winder, R. (1963). Threshold logic in artificial intelligence. In Artificial Intelligence, pages 

107-128. IEEE Special Publication S-142. 

Wolverton, C.  and Wagner, T.  (1969a). Asymptotically optimal discriminant functions for 
pattern classification. IEEE Transactions on Systems, Science and Cybernetics, 15:258-
265. 

Wolverton, C.  and Wagner, T.  (1969b). Recursive estimates of probability densities. IEEE 

Transactions on Systems,  Science and Cybernetics, 5:307. 

Wong, W. and Shen, X. (1992). Probability inequalities for likelihood ratios and convergence 
rates  of sieve  MLE's.  Technical  Report  346,  Department  of Statistics,  University  of 
Chicago, Chicago, IL. 

Xu, L., Krzyzak, A., and Oja, E. (1993). Rival penalized competitive learning for clustering 
analysis, RBF net and curve detection. IEEE Transactions on Neural Networks, 4:636-
649. 

Xu, L., Krzyzak, A., and Yuille, A. (1994). On radial basis function nets and kernel regres(cid:173)
sion: Approximation ability, convergence rate and receptive field size. Neural Networks. 
To appear. 

Yatracos,  Y.  (1985).  Rates  of  convergence  of  minimum  distance  estimators  and  kol(cid:173)

mogorov's entropy. Annals of Statistics,  13:768-774. 

Yau, S. and Lin, T. (1968). On the upper bound of the probability of error of a linear pattern 

classifier for probabilistic pattern classes. Proceedings of the IEEE, 56:321-322. 

You, K.  and Fu, K.  (1976). An approach to the design of a linear binary tree classifier. In 
Proceedings of the Symposium of Machine Processing of Remotely Sensed Data, pages 
3A-1O. Purdue University, Lafayette, IN. 

Yukich, J.  (1985). Laws of large numbers for classes of functions. Journal of Multivariate 

Analysis,  17:245-260. 

Yunck, T. (1976). A technique to identify nearest neighbors. IEEE Transactions on Systems, 

Man,  and Cybernetics, 6:678-683. 

Zhao, L.  (1987).  Exponential bounds of mean error for the nearest neighbor estimates of 

regression functions. Journal of Multivariate Analysis, 21: 168-178. 

Zhao, L.  (1989).  Exponential bounds of mean error for the kernel estimates of regression 

functions. Journal of Multivariate Analysis, 29:260-273. 

Zhao, L., Krishnaiah, P., and Chen, X.  (1990). Almost sure Lr-norm convergence for data(cid:173)

based histogram estimates. Theory of Probability and its Applications, 35:396-403. 

Zygmund, A.  (1959). Trigonometric Series I. University Press, Cambridge. 

Author Index 

Abou-Jaoude, S.,  143, 144,364 
Abram, G.D., 342 
Aitchison, 1., 474, 476 
Aitken, CG.G., 474, 476 
Aizerman, M.A.,  159 
Akaike, H.,  149,292,295 
Alexander, K.,  207 
Anderson, A.C., 340 
Anderson, J.A., 259 
Anderson, M.W., 375 
Anderson, T.W., 49,322,374,375 
Angluin, D.,  130 
Anthony, M., 493, 534 
Antos, A.,  vi 
Argentiero, P.,  340, 342 
Arkadjew, A.G.,  149 
Ash, RB., 575 
Assouad, P.,  231, 232, 243 
Azuma, K.,  133,  135 

Bahadur, RR, 472 
Bailey, T.,  73 
Barron, A.R, vi,  144,208,292,295, 364, 
494,509,518,520,527,531,533 

Barron, RL., vi, 509, 532, 533 
Bartlett, P., 522, 544 
Bashkirov, 0., 159, 540 

Baskett, F., 62 
Baum, E.B., 521, 522 
Beakley, G.W., 375 
Beaudet, P.,  340, 342 
Beck, J.,  169 
Becker, P.,  42, 56 
Beiriant, J.,  vi 
Ben-Bassat, M., 562 
Benedek, G.M., 211, 297, 301,485 
Bennett, 6., 124 
Benning, RD., 375 
Bentley, J.L., 62, 333 
Beran, RH., 23 
Beriinet, A., vi 
Bernstein, S.N.,  124 
Bhattacharya, P.K.,  169,320 
Bhattacharyya, A., 23 
Bickel, P.J.,  169 
Birge, L.,  118,243 
Blumer, A., 202, 233, 236, 245 
Bosq, D., vi 
Brailovsky, v.L.,  125 
Braverman, E.M., 149,  159,540 
Breiman, L.,  169,  185,318,336,337,339, 

342,347,360,364,376,383,447 

Brent, RP., 542 
Broder, A.J., 62 

620 

Author Index 

Broeckaert, I., 149 
Broniatowski, M., vi 
Broomhead, D.S., 540 
Buescher, K.L., 211, 212, 297, 485 
Burbea, J., 28 
Burman, P.,  vi 
Burshtein, D., 340 
Buzo, A, 380 

Cacoullos, T, 149 
Cao, R, vi 
Carnal, H., 229 
Casey, RG., 340 
Cencov, N.N., 279 
Chang, e.L., 310 
Chang, C.Y, 563 
Chatterjee, B., 340 
Chen, C.,  125,557,558 
Chen, H., 518 
Chen, T, 518 
Chen, X.R, 364 
Chen, Ye., 541, 542 
Chen, Z.,  128 
Chernoff, H., 24, 41, 51, 56,  122,  123,  129 
Chervonenkis, AY, v, 5, 50, 126,  187, 189, 
190,197,198,199,203,216,235, 
240,291,491,492 

Chin, R, 340, 342 
Chou, P.A, 339 
Chou, W.S., 541, 542 
Chow, C.K.,  18,466 
Chow, YS., 575 
Ciampi, A., 339 
Collomb, G.,  169 
Conway, J.H., 475 
Coomans, D.,  149 
Cormen, TH., 316, 326 
Cover, TM., v,  vi, 5, 22, 25, 26, 62, 70,80, 

81,87,  114,  125,  169,  198,219, 
222,223,230,292,295,395,562, 
564,579 

Cramer, H., 45 
Csibi, S., v,  16,506 
Csiszar, 1.,31,37, 144,219 
Csuros, M., vi, 301 
Cuevas, A, vi 
Cybenko, G., 518 

Darken, e., 540, 547 

Dasarathy, B.V, 62, 309 
Das Gupta, S., 84,  375 
Day, N.E., 260 
de Guzman, M., 580 
Deheuvels, P, vi, 433 
Della Pietra, VD., 340 
Delp, E.J., 339 
Devijver, P.A, vi,  22,  30, 45,  62, 74, 77, 
82,88,304,309,313,455,554, 
562,563 

Devroye,L., 16,33,62,70,76,77,86,87, 

88,  90,  112,  114,  126,  134,  137, 
138,  143,  149,  150,  160,  161,  162, 
164,  169,  170,  171, 176,  177,  178, 
182,183,198,202,206,208,212, 
223,229,236,240,243,278,304, 
333,334,388,389,398,403,411, 
414,415,41~421,430,484,588 

Diaconis, P, 538, 546 
Dillon, W.R., 463 
Donahue, M., 547 
Donaldson, RW., 556 
Do-Tu, H., 56 
Dubes, RC., 125,339,557,558 
Duda, RO., 49,225,270,472,501,566, 

572 

Dudani, S.A., 88 
Dudley, R.M.,  193,  198,  211,  221,  223, 

231,365,485,496,498 

Duin, RP.W., 540, 542 
Dunn, OJ., 554 
Durrett, R., 575 
Dvoretzky, A., 207, 505 

Edelsbrunner, H., 339, 380, 512 
Efron, B.,  125, 137,556 
Ehrenfeucht, A, 202, 233, 236, 245 
Elashoff, J.D., 564 
Elashoff, RM., 564 

Fabian, V, 505 
Fano, RM., 26 
Farago, A.,  vi, 62, 231, 523, 525 
Farago, T, 567 
Feinholz, L.,  198, 394 
Feller, W.,  587 
Finkel, RA., 62, 333 
Fisher, F.P.,  85,  184,374 
Fisher, RA, 46 

Fitzmaurice, G.M., 554 
Fix, E., 61,  149, 169 
Flick, T.E., 455, 538 
Forney, G.D.,  18 
Fraiman, R, vi 
Friedman, J.H.,  62,  318,  336,  337,  339, 

342, 347,  360,  364, 375, 376, 383, 
535,538 
Fritz, J., v,  56, 58 
Fu,  KS., 36,  128, 340, 342, 343, 375, 562 
Fuchs, H., 341, 342 
Fukunaga,K., 62,  128,307,415,455,554, 

555,563 

Funahashi, K, 518 

Gabor, D., 532 
Gabriel, KR, 90 
Gaenssler, P.,  198,207 
Gallant, AR, 527 
Ganesalingam, S., 554 
Garnett, J.M.,  128 
Gates, G.W., 307,455 
Gelfand, S.B., 339 
Geman, S., 527, 540 
Gerberich, CL., 340 
Gessaman, M.P.,  374, 375, 393 
Gessaman, P.H.,  374 
Geva, S., 310 
Gijbels, I., vi 
Gine, E.,  198 
Girosi, E, 540, 547 
Glick, N., vi,  16,96, 125,225,463,474, 

550 

Goldberg, P.,  524 
Goldmann, G.E., 564 
Goldstein, M., 463 
Golea, M., 542 
Gonzalez-Manteiga, W.,  vi 
Goodman, RM., 339 
Goppert, R, 62 
Gordon, L., 96,339,347,364,376 
Gowda,  KC., 307 
Grant, E.D., 342 
Gray, RM., 380 
Greblicki, W.,  149,  161, 165,  166,279 
Grenander, U., 527, 540 
Grimmett, G.R, 575 
Guo, H.,  339 
Gurvits, L., 547 

Author Index 

621 

Gyorfi, L.,  16,  18, 33, 56, 57, 58,  76,  86, 
138,  144,  150,  161,  162,  169,  170, 
171,176,208,430,505,567,588 

Gyorfi~ Z.,  18, 76,  169 

Haagerup, u., 588 
Habbema, J.D.E, 444 
Hagerup, T.,  130 
Hall, P.,  vi, 474, 538 
Hand, D.J.,  149,455,554,557,558 
HardIe, W., 445 
Halt, P.E.,  v,  5, 22,49,62, 70,  81,  87,225, 
270,304,455,472,501,566,572, 
579 

Hartigan, J.A., 377 
Hartmann, CRP., 340 
Hashlamoun, W.A, 29, 36 
Hastie, T.,  356, 537, 538 
Haussler, D., 202, 210, 233, 235, 236, 245, 

493,496,498,506,522,526,527 

Hecht-Nielsen, R, 534 
Hellman, M.E., 81 
Henrichon, E.G., 340, 342, 343, 375 
Herman, C., 538 
Hermans, J., 444 
Hertz, J., 56, 509 
Hills, M., 474 
Hinton, G.E., 525 
Hjort, N., 270 
Hodges, J.L., 61,  149,  169 
Hoeffding, W.,  122,  133, 135, 589 
Hoff, M.E., 54, 531 
Holden, S.B., 534 
Holmstrom, L.,  150,430 
Hora, S.C., 554 
Horibe, Y, 24 
Home, B., 59 
Hornik, K, 518, 543 
Hostetler, L.D., 455 
Huber, P.J., 538 
Hudimoto, H., 24 
Hummels, D.M.,  128 
Hush, D., 59 
Hwang, CR, 527, 540 

Installe, M., 56 
Isenhour, T.L., 307, 455 
Isogai, E., vi 
Itai, A, 211, 297, 485 

622 

Author Index 

Ito, T.,  24,472 
Ivakhnenko, A.G., 532, 533 

Jain, A.K., 73,  125,557,558 
Jeffreys, H., 27 
Jerrum, M., 524 
Johnson, D.S., 54 
Jones, L.K., 538 

Kailath, T., 24 
Kanal, L.N.,  125, 562 
Kanevsky, D., 340 
Kaplan, M.R, 342 
Karlin, A.S., 434 
Karp, RM., 130 
Karpinski, M., 525 
Kazmierczak, H., 476 
Kearns, M., 233, 236, 245 
Kedem, Z.M., 342 
Kegl, B., vi 
Kemp, R, 469, 476 
Kemperman, J.H.B., 37 
Kerridge, N.E., 260 
Kessel, D.L.,  128,415,554,555 
Khasminskii, RZ., 505 
Kiefer, J., 207, 505 
Kim, B.S., 62 
Kittler, J., 22, 30,45, 77,88,304,309,313, 

455,554,562,563 

Kizawa, M., 342 
KlemeHi, J.,  150, 430 
Knoke, J.D., 550 
Kohonen, T.,  310 
Kolmogorov, A.N.,  22,479,481,482,486, 

487,510,534,536,537 

Konovalenko, v.v., 532 
Komer, J., 219 
Koutsougeras, C., 542 
Kraaijveld, M.A., 540, 542 
Krasits'kyy, M.S., 532 
Krishna, G., 307 
Krishnaiah, P.R, 364 
Krogh,A.,56, 509 
Kronmal, RA., 279,473 
Krzanowski, W.J., 31, 463 
Krzyzak, A.,  vi,  149,  150,  161,  163,  164, 
165,166,169,170,176,447,540, 
541,547 

Kulikowski, C.A., 509 

Kulkarni, S.R, 211, 485 
Kullback, S., 27, 37 
Kumar, P.R, 211, 212, 297 
Kurkova, v., 537 
Kushner, H.J., 505 

Lachenbruch, P.A., 415 
Laforest, L., 334 
Laha, RG., 45 
Landgrebe, D.A., 554 
LeCam, L., 23, 33 
Leibler, A., 27 
Leiserson, C.E., 316, 326 
Levinson, S.E., 85 
Li, T.L., 562 
Li, X., 339 
Lin, H.E., 84 
Lin, T.T., 45 
Lin, Y.K.,  343 
Linde, Y.,  380 
Linder, T., vi, 62, 506, 540, 541, 547 
Lissack, T.,  36 
Littlestone, N.,  210, 233, 235 
Liu, R, 518 
Ljung, L., 505 
Lloyd, S.P., 380 
Loftsgaarden, D.O.,  184 
Logan, B.E, 539 
Loh, W.Y.,  342, 343 
Loizou, G.,  84 
Lorentz, G.G., 510, 534 
Lowe, D., 540 
Lowry, S.L., 307, 455 
Lugosi, G., 62, 89,  109,  169, 170, 176,210, 
236,240,243,292,301,364,365, 
369,378,383,506,523,525,527, 
528,540,541,547,551,554,555, 
559 

Lukacs, E., 45 
Lunts, A.L.,  125 

Maass, W.,  523, 524 
Machell, E, 160 
Macintyre, A., 524, 525 
Mack, Y.P.,  vi,  169, 185 
Mahalanobis, P.c., 30,  320 
Mantock, J.M., 307 
Marchand, M., 542 
Marron, J.S.,  114,445 

Massart, P, 44, 198,207 
Mathai, A.M., 22 
Matloff, N., 554 
Matula, D.W., 90 
Matushita, K, 23, 31 
Max, J., 380 
Maybank, S.J.,  84 
McDiarmid, c., 3,  133,  134,  136 
McLachlan, G.J., 49,  125,251,259,270, 
399,415,468,472,554,557,558 

Mehrotra, KG., 340 
Meisel, W.,  149,  185,339,359,365,447, 

542,562 

Michalopoulos, W.S., 359, 365 
Michel-Briand, c., 339 
Michelotti, 342 
Mickey, P.A., 415 
Mielniczuk, J., 527 
Milhaud, X., 339 
Min, P.J., 562 
Minsky, M.L., 466, 509 
Mitchell, A.F.S., 31 
Mizoguchi, R, 342 
Monro, S., 505 
Moody, J., 540 
Moore, D.S., 554 
Morgan, J.N., 339 
Muchnik, I.E.,  159, 540 
Mui, J.K, 343 
Myles, J.P., 455 

Nadaraya, E.A.,  149 
Nadas, A., vi, 340 
Nagy, G., 340 
Narendra, P.M., 62, 563 
Natarajan, B.K, 202,468 
Naylor, B., 342 
Neve1son,  M.B., 505 
Niemann, H., 62 
Nilsson, N.J., 44, 509, 525 
Nobel, A.B., vi,  364,  365, 369, 378, 383, 

499 

Nolan, D., 498, 499 

Oja, E., 540 
Okamoto, M., 51, 75,  122,  131 
Olshen, RA., 96,178,318,336,337,339, 

342,347,360,364,376,383 

Ott, l, 472 

Author Index 

623 

PaIi, I., vi 
Palmer, RG., 56, 509 
Papachristou, C.A., 542 
Papadimitriou, c.H., 62 
Papert, S., 509 
Park, J., 547 
Park, S.B., 62 
Park, Y.,  342 
Parthasarathy, KR, 320 
Pm'zen, E.,  149,  161 
Patrick, E.A., 85,  184,374 
Pawlak, M., vi,  149,  161,  165,  166,279, 

551,554,555,559 

Payne, H.J., 339 
Pearl, J., 470 
Penrod, C.S., 309, 313 
Peterson, D.W., 87 
Petrache, G., 532 
Petrov, v.v., 416 
Petry, H., vi 
Pflug, G.,  vi,  505 
Pikelis, v., 49, 567 
Pinsker, M., 37 
Pinter, M., vi 
Pippinger, N., 470 
Poggio, T.,  540, 547 
Pollard, D.,  193,  198,365,492,493,496, 

498,499,500,506 

Powell, M.J.D., 540 
Prep arata, P.P.,  54,  177 
Priest, RG., 538 
Pruitt, R, 554 
Psaltis, D., 87 
Purcell, E.,  185,447 
Pyatniskii, E.S.,  159 

Qing-Yun,  S., 340, 342 
Quesenberry, C.P.,  184,374 
Quinlan, J.R, 339 

Rabiner, L.R, 85 
Rao, C.R, 28 
Rao, RR, 228 
Rathie, P.N., 22 
Raudys, S., 49, 567 
Ravishankar, C.S., 339 
Rejto, L.,  149 
Renyi, A., 28 
Revesz, P, v,  149, 161 

624 

Author Index 

Richardson, T., 211, 485 
Ripley, B.D., 509 
Rissanen, J., 292, 295 
Ritter, G.L., 307,455 
Rivest, RL., 316, 326 
Robbins, H., 505 
Rogers, W.H., 4, 411, 413 
Rosenberg, A.E., 85 
Rosenblatt, E, 5,  14, 39,44,509 
Rosenblatt, M.,  149,  161 
Rounds, E.M., 339 
Roussas, G., vi 
Royall, RM., 71, 73,  179,  185,453 
Rozonoer, L.I.,  159 
Rub, C.,  130 
Rumelhart, D.E., 525 
Ruppert, D., 505 

Samarasooriya, VN.S., 29, 36 
Samet, H., 326, 333, 342 
Sandberg, I.W., 547 
Sansone, G., 281 
Sarvarayudu, G.P.R, 339 
Sauer, N., 216 
Scheffe, H., 32, 208, 211,383 
SchHiffli, L., 395 
Schmidt, W.E, 525 
Schoenberg, LJ., 434 
Schroeder, A., 538 
Schwartz, S.c., 279 
Schwemer, G.T., 554 
Sebestyen, G.,  149,340,432,562 
Serfling, R.I., 589 
Sethi, I.K., 62, 339, 340, 542 
Shahshahani, M., 538, 546 
Shamos, M.I.,  177 
Shannon, C.E., 25 
Shawe-Taylor, J., 493, 525 
Shen, X., 254, 527 
Shepp, L., 539 
Shimura, M., 342 
Shirley, P.,  342 
Shiryayev, A.N., 575 
Short, RD., 455 
Shustek, L.J., 62 
Silverman, B.W., 536, 538 
Simar, L., vi 
Simon, H.D., 198,240,477 
Sitte, J., 310 

Sklansky, 1, 56, 342 
Sloane, N.J.A., 475 
Slud, E.V, 87, 588 
Smyth, P.,  339 
Snapp, RR, 87 
Sokal, RR, 90 
Sonquist, J.A., 339 
Sontag, E., 524, 547 
Specht, D.E, 160,279,531,532 
Spencer, J., 496 
Sprecher, D.A., 534 
Steele, J.M.,  137,221,229 
Stein, c., 137 
Steinbuch, K., 476 
Stengle, G.,  198 
Stephenson, R., 468 
Stinchcombe, M., 518 
Stirzaker, D.R, 575 
Stoffel, J.C., 340 
Stoller, D.S., 40, 43 
Stone, C.I., v,  3, 6, 65, 70, 97, 98,  100, 101, 
114,  169,  175,  179,  181,  183,318, 
336,337,339,342,347,360,364, 
376,383,538 

Stone, M.,  125 
Stuetzle, W., 538 
Stute, W.,  vi,  169,207 
Suen, c.Y., 339 
Sung, K., 342 
Swonger, C.W., 307 
Szabados, T.,  vi 
Szab6, R, 301 
Szarek, S.I., 588 
Szego, G., 289 

Talagrand, M., 199,207 
Talmon, 1L., 339 
Taneja, I.J., 28 
Tarter, M.E., 279 
Thomas, J.A., 25, 26,  219 
Tibshirani, R.I., 536, 537, 538 
Tikhomirov, VM., 479, 481, 482, 486, 487 
Tomek, I., 307, 309, 455 
Toronto, A.E, 468 
Toussaint, G.T., vi, 28,  35,  125, 556, 564, 

565 

Tsypkin, Y.Z., 505 
Tukey, J., 538 
Tulupchuk, Y.M., 532 

Author Index 

625 

Tuteur, F.B., 375 
Tutz, G.E., 441, 474, 477,550 
Tymchenko, LK, 532 
Tyrcha, J., 527 

Ullmann, J.R, 307, 455 

Vajda, Igor, vi, 22, 32 
Vajda, Istvan,  18,56,57 
Valiant, L.G.,  130,202,233,236,245 
Van Campenhout, J.M., 562, 564 
van den Broek, K, 444 
van der Meulen, E.C., vi,  144,208 
Vanichsetakul, N., 342, 343 
Van Ryzin, J.,  16,  149 
Vapnik, V.N., v,  5, 50,  126,  187,  189,  190, 
197,198,199,202,203,206,207, 
211,216,235,240,291,485,491, 
492,493,501 

Varshney, P.K, 29, 36, 340 
Veasey, L.G., 468 
Venkatesh, S.S.,  87 
Vidal, E., 62 
Vilmansen, T.E., 563 
Vitushkin, AG., 482 

Wagner, TJ., v,  vi, 4,  16,20,62,84, 125, 
149,161,198,202,206,304,309, 
313,389,398,403,411,413,414, 
415,416,421,455 

Walk, H., vi, 505 
Wand, M.P., 474 
Wang, Q.R, 339 
Warmuth, M.K, 202, 210, 233, 235, 236, 

245 

Warner, H.R, 468 

Wassel, G.N., 56 
Watson, G.S.,  149 
Weiss, S.M., 509 
Wenocur, RS., 498 
Wheeden,RL.,580 
White, H., 518, 527 
Whitsitt, S.J., 554 
Widrow, B., 54, 531 
Wilcox, J.B., 554 
Williams, RJ., 525 
Wilson, D.L., 309, 313,455 
Wilson, J.G.,  85 
Winder, RO., 466 
Wise, G.L.,  177,  182 
Wold, H., 45 
Wolfowitz, J., 207, 505 
Wolverton, c.T., 16,20, 161 
Wong, W.H., 254, 527 
Woodruff, H.B., 307, 455 
Wouters, W.,  v 

Xu, L., 540 

Yakowitz, S., vi 
Yatracos, YG., vi, 278 
Yau, S.S., 45,  128 
You,  KC., 340 
Yuille, AL., 540 
Yukich, J.E.,  198 
Yunck, T.P., 62 

Zeitouni, 0., 211, 485 
Zhao,L.C., 150,  169,  170,364 
Zeger, K, vi, 292, 301, 506, 527, 528 
Zinn, J.,  198 
Zygmund, A, 281, 575, 580 

Subject Index 

absolute error,  11 
accuracy, 201, 202, 206,246 
Adaline, 531 
additive model, 536, 538, 546 
admissible transformation, 570, 571, 572, 

573 

agreement,  163 
AID criterion, 339 
Alexander's inequality, 207, 210, 211, 278, 

295,300 

ancestral rule,  180,  181, 185,420 
a posteriori probability,  10,  11,  16,92,97, 
102,108,113,301,479,483,493, 
549,551,552,554,555,563,568, 
570,572 

apparent error rate,  see error estimation, 

resubstitution 

approximation error,  188,284,290,295, 
297,395,430,431,454,487,517, 
523,529,547 

arrangement, 511, 512, 513, 514, 515, 516, 

517,542 

arrangement classifier, 511, 512, 514, 515, 

516,517 

association inequality, 22, 583 
asymptotic optimality, 390, 393, 431, 445 

back propagation, 525 
Bahadur-Lazarsfeld expansion, 472 
bandwidth, see smoothing factor 
Barron network, 545, 546 
basis function,  225, 266, 279, 285,  290, 

398,501,506 

complete orthonormal, 280,  281, 282, 

287 

Haar, 281, 288 
Hermite, 279, 281, 288 
Laguerre, 281, 288 
Legendre, 281, 288 
Rademacher, 282 
Rademacher-Walsh, 470, 472 
trigonometric, 279, 287,288 
Walsh, 282 

Bayes classifier (or rule), 2,  10,  11,  12,  13, 

14,  15,  16,  17,  19,20,47,48,57, 
58,59,61,69,202,226,239,249, 
264,265,266,271,272,274,275, 
276,277,280,294,295,300,301, 
307,357,387,464,466,467,471, 
472,473,474,487,527,569,570 

Bayes decision, see Bayes classifier 
Bayes error, 2,  11,  12,  13,  14,  15,  17,  18, 
19,21,23,28,29,30,31,35,36, 
39,57, 79,  82,90, 91,  111,  112, 

628 

Subject Index 

113,  128,  129,  176,  188, 236,  237, 
241,276,285,289,295,297,298, 
307,308,313,357,389,401,451, 
533,545,561,562,563,565,567, 
568,569,570,571,572,573 

estimation of, 4,  128, 566 

Bayes problem, 2, 9 
Bayes risk, see Bayes error 
Bennett's inequality,  124,  130 
Beppo-Levy theorem, 577 
Bernstein perceptron, 544 
Bernstein polynomial, 544 
Bernstein's inequality, 124,  130, 210, 494 
beta distribution, 77, 88,266,276 
Bhattacharyya affinity, 23, 24, 343 
bias,  121,  125,  126,  144,  151,395,555 
binary space partition tree, 316, 317, 318, 
338,340,341,342,343,344,347, 
362,383,542 

balanced, 362 
raw, 342, 362 

binomial distribution, 51, 71,  72, 75, 76, 
77, 78,  79,  81,  82,  83,  84,  85,  86, 
89,94,  106,  109,  122,  124,  127, 
129,130,131,152,184,230,238, 
242,276,306,309,321,332,345, 
410,444,449,462,586,587,588 

binomial theorem, 218 
boolean classification, 461, 468 
Borel-Cantelli lemma,  142,  193,227,284, 
293,357,367,381,425,452,505, 
585 

bracketing metric entropy, 253, 254, 262 
branch-and-bound method, 563 
ESP tree, see binary space partition tree 

CART,  336,338, 339 
Catalan number, 469, 476 
Cauchy-Schwarz inequality, 20, 33, 52, 76, 
93,95,99,103,104,141,155,158, 
173,256,345,401,402,414,463, 
582,583 

central limit theorem, 85,270 
channel,  109 
characteristic function, 45 
Chebyshev-Cantelli inequality, 42, 237, 

463,582 

Chebyshev'sinequality, 45, 56, 97,  194, 

210,269,276,322,332,414, 

494,582 

Chernoff's affinity, 24, 35 
Chernoff's bounding method,  117,  123, 

129,  130,  131,  135,230,261 

x2-divergence, 34, 36 
class-conditional density,  14,  15,  17,  19, 
31,148,149,150,166,184,260, 
263,264,266,268,269,271,272, 
276,277,280,282,285 

class-conditional distribution, 56,  118,263, 

264,265,271,274,275,276 

classifier selection,  125,  127,  132,  187, 

188,189,193,199,201,233,234, 
245,250,404,549,554,555,559 
lower bounds for,  206,  233,  234,  239, 

245,246,275 

class probability,  15,  17,  18,  19,32,34,58, 

150 

clustering, 343,  363, 377, 378, 379, 380, 

384,392,506,542 

committee machine, 525, 526, 543 
complexity penalty, 291,  292,  297,  300, 

301,395 

complexity regularization, 266, 289, 290, 
291,292,293,2~4,295,296,298, 
300,301,395,524,525,527,531 

concept learning, see learning 
confidence, 201, 202,246 
consistency, 92, 93,  104,  106,  109,  128, 

132,184,188,232,268,296,389, 
390, 468, 513, 555 

definition of, 3, 91 
of Fourier series rule, 282, 284 
of generalized linear rules, 286 
of kernel rules,  160, 161, 163,  171,439, 

441,442,446,448,474 

of maximum likelihood, 249, 250, 252, 
253,254,257,259,262,268,473 
of nearest neighbor rules,  80,  174,  176, 
307,308,309,310,311,312,313, 
457 

of neural network classifiers, 514, 523, 
525,527,531,534,542,544,545 
of partitioning rules, 94, 96, 319,  320, 
322,323,324,332,333,335,338, 
339,340,341,342,343,344,346, 
348,349,350,357,358,359,361, 
362,364,368,370,374,376,377, 
381,384,385,392,394,395 

Subject Index 

629 

of skeleton estimates, 483, 487 
of squared error minimization, 490, 493 
strong, see strong consistency 
strong universal, see strong universal 

tie breaking, 4, 61, 69,  101,  170,  174, 

175,  176,  177,  178,  180,  181,  182, 
304,305,377,413,414,451,456, 
458,553 

consistency 

distribution 

universal, see universal consistency 
weak,91, 100,  103,  133,  139,  149,  162, 

169,184,425,430,452 
consistent rule, see consistency 
convex hull, 229, 232 
Cover-Hart inequality, 5, 22, 24, 27, 35,62, 

70,448 

covering 

of a class of classifiers, 211, 212 
of a class of functions,  479, 480, 483, 
484,485,486,487,489,499,500 

empirical, 211, 298, 485 
simple empirical, 289, 296, 297,  298, 

300,301 

covering lemma, 67,  153,  157,  158,  164, 

165,171,176, 181, 182 

covering number, 479,483,492,493,494, 

496,499,504,526,530,547 

cross-validation,  125,445 
curse of dimensionality, 469, 486, 561 

decision with rejection,  18 
denseness 

in L 1,  286, 287, 517, 520, 542 
in L 2 ,  502, 503 
in Lp, 543, 579 
with respect to the supremum norm, 287, 

517,518,533,539 

density estimation,  107,  118,  143,  149, 

150,  151,  161,  162,  169,  184,208, 
266,278,279,364,433,445,447, 
484,527 

density of a class of sets, 231 
diamond, 231 
dimension,  17,49,52,222,363,482,567 

reduction of, 561, 567 

distance 

data-based,  169,391,451,456,459 
empirical,  178, 182 
Euclidean, 63,  101,  149,  169,368,370, 

378,456 

generalized, 182 
Lp,  182 
rotation-invariant,  179,  180 

beta, 77, 88, 266, 276 
binomial, 51, 71, 72,  75, 76, 77, 78, 79, 
81,82,83,84,85,86,89,94,106, 
109,  122,  124,  127, 129, 130,  131, 
152,184,230,238,242,276,306, 
309,321,332,345,410,444,449, 
462,586,587,588 

class-conditional, 56,  118,  263, 264, 

265,271,274,275,276 

exponential, 361, 590 
gamma,266,268,276,573,590 
geometric, 80,  153,  161, 167 
hypergeometric, 589 
Maxwell, 266 
multinomial, 227,321,589 
normal,  31, 35, 47, 48, 49, 56, 57, 59, 

76,85,86,249,264,265,266,276, 
343,390,399,415,564,566,567, 
572,588,590 
Rayleigh, 266, 277 
strictly separable, 104, 162 

distribution function,  142 

class-conditional, 40, 41, 45 
empirical, 142, 193,325,375 

divergence,  see  Kullback-Leibler 

divergence 

dominated convergence theorem, 63,  156, 

267,329,514,577 

Dvoretzky-Kiefer-Wolfowitz-Massart 

inequality, 44, 57, 207 

elementary cut, 350 
embedding, 64 
empirical classifier selection, see classifier 

selection 

empirical covering, see covering 
empirical error, 43, 49, 54, 56,  125,  187, 

188,191,200,310,335,351,362, 
482 

empirically optimal classifier,  187,  188, 

200,206,225,275 

empirical measure, 43,  142, 190,  193,203, 
208,210,211,213,265,270,271, 

630 

Subject Index 

272,278,304,365,366,367,398, 
414,443,553,556 

empirical risk, see empirical error 
empirical risk minimization, 5, 6,  49, 50, 

53,54,56,125,126,131,187,191, 
192,193,199,201,202,209,210, 
211,212,225,227,232,234,235, 
239,246,249,251,257,258,265, 
266,268,274,275,277,285,286, 
287,289,290,291,292,294,306, 
310,311,334,339,346,347,354, 
375,388,404,420,428,441,444, 
454,455,457,458,467,469,476, 
477,479,483,487,489,491,493, 
501,502,506,512,514,521,523, 
525,526,527,534,537,539,540, 
541,542,546,549,551 

empirical squared Euclidean distance error, 

550,557 

bias of, 343, 397, 398, 399,400,405, 

419,550 

rotation,  125, 556 
smoothed,  125,549,550,551,552,554, 

555 

U -method, see leave-one-out estimate 
estimation error,  188,284,290,295, 395, 
431,454,455,471,487,523,529 

Euclidean norm, 99,  370, 567 
Euler's theorem, 380 
exponential distribution, 361, 590 
exponential family,  266, 275, 276 

Fano's inequality, 26, 35 
Fatou's lemma, 577 
f-divergence, 31, 32, 34, 36 
feature extraction,  14,21, 128,561, 562, 

377,378,380,384 

567,572,573 

entropy, 25, 26, 29, 35, 218, 219, 252, 286, 

337,339,373 

E -covering number, see covering number 
E -entropy, see metric entropy 
Em-optimality, 390, 393, 454 
E -packing number, see packing number 
error estimation, 4,  121,  124,  125,  127, 

132,343,397,408,409,549 

0.632 estimator, 557 
bootstrap,  125,549,556,557,558 
complexity penalized, 291, 297, 396 
cross  validated,  see leave-one-out 

estimate 

deleted, see leave-one-out estimate 
double bootstrap, 557 
EO estimator, 557 
error counting,  121,  123,  126,  131,549, 

550 

holdout,  125,347,388, 389, 391, 393, 
395,397,428,430,453,549,556 

leave-one-out,4,  125,309, 397,407, 

408,410,413,414,415,417,419, 
420,441,444,446,458,459,474, 
549,550,554,555,556,557 
variance of, 415, 419,552,556 

posterior probability, 549, 554, 555, 559 
randomized bootstrap, 557 
resubstitution,  125, 304, 343, 347, 382, 
397,398,399,400,402,405,407, 
415,419,439,458,474,476,549, 

F -error, 29, 32, 34, 36, 573 
fingering,  191,  192, 277 
fingering dimension,  192, 209 
Fisher linear discriminant, 47, 49, 58 
flexible grid, 367, 368, 369 
Fourier series classifier, 279, 280, 282, 285, 

471 

Fourier series density estimation, 279 
Fubini's theorem,  139,  157,371,579,582, 

583 

fundamental rule, 440, 461, 462, 463, 471, 

474 

fundamental theorem of mathematical 
statistics, see Glivenko-Cantelli 
theorem 

Gabriel graph, 90 
Gabriel neighbor, 90 
gamma distribution, 266, 268, 276, 573, 

590 

gaussian distribution,  see  normal 

distribution 

gaussian noise, 57, 285 
geometric distribution, 80,  153,  161, 167 
Gessaman's rule, 375, 376, 384, 393 
ghost sample,  193,203,210,213 
Giniindex, 338, 339, 340,343, 361, 362 
Glivenko-Cantelli theorem,  192, 193,321, 

374 

gradient optimization, 54, 56, 251, 525 

Grenander's method of sieves, 527, 540 
grid complexity, 357 
group method of data handling, 532 

Hamming distance, 474, 477 
ham-sandwich theorem, 339 
Hart's rule, 304, 305, 306 
Hellinger distance, 23, 32, 243, 474 
Hellinger integral, 32, 36 
histogram density estimation,  107,  143, 

208 

histogram rule,  94,  95, 96, 98,  106,  109, 

133,  138,  139,  143,  144,  145,  147, 
152,153,155,159,170,253,261, 
287,288,290,301,318,342,347, 
351,363,368,370,378,384,394, 
399,400,401,404,405,408,409, 
417,418,419,420,462,463,506, 
518,549,552,553,555,559 

cubic, 95,96, 107,  133,  138,  144,381, 

382,384,465,475,512 

data-dependent, 317, 365, 368, 369, 373, 

382,385,391,403,404 
lazy,  143,  152,408,409,414 
randomized, 475 

Hoeffding's inequality, 51, 52, 75,  122, 

124,  127,  133,  134,  135,  137,  190, 
193,198,212,213,284,299,306, 
311,366,428,440,445,448,483, 
487,490,495 

HOlder's inequality,  108, 582 
hypergeometric distribution, 589 

imperfect training, 89,  109 
impurity function,  336, 337, 338, 340, 343, 

361,362 

inequality 

Subject Index 

631 

Cover-Hart, 5, 22, 24, 27, 35, 62, 70, 448 
Dvoretzky-Kiefer-Wolfowitz-Massart, 

44,57,207 

Fano's, 26, 35 
Hoeffding's, 51, 52, 75,  122,  124,  127, 
133,  134,  135,  137,  190,  193,  198, 
212,213,284,299,306,311,366, 
428,440,445,448,483,487,490, 
495 

HOlder's,  108,582 
Jensen's,  23,  26,  29,  32,  76, 99,  141, 

242,252,288,401,402,418,463, 
558,568,583 

Khinchine's, 410, 588 
large deviation,  136,  190,364,365,379 
LeCam's, 33, 244 
Markov's, 76,  117,  123,256, 309, 379, 

582 

McDiarmid's, 3,  136,  142,  158,  162, 

172,174,201,211,346,414,448, 
553,558 
Pinsker's, 37 
Rogers-Wagner, 4, 411, 413, 417, 418 
Vapnik-Chervonenkis, 5,  138,  192,  196, 
197,198,201,202,203,206,225, 
226,229,233,271,273,275,278, 
295,304,312,356,357,364,365, 
366,388,399,429,436,467,476, 
492,494 

information divergence,  see Kullback(cid:173)

Leibler divergence 

Jeffreys' divergence, 27, 28, 29 
Jensen's inequality, 23,  26,  29, 32, 76, 99, 
141,242,252,288,401,402,418, 
463,558,568,583 

J essen-Marcinkiewitz-Z ygmund theorem, 

Alexander's,  207,  210, 211, 278,  295, 

329,580,581 

300 

association, 22, 583 
Bennett's, 124, 130 
Bernstein's, 124, 130,210,494 
Cauchy-Schwarz, 20, 33, 52, 76, 93, 95, 

99,  103,  104,  141,  155,  158,  173, 
256,345,401,402,414,463,582, 
583 

Chebyshev-Cantelli, 42, 237, 463, 582 
Chebyshev's, 45, 56, 97,  194,210,269, 

276,322,332,414,494,582 

Karhunen-Loeve expansion, 568 
k-d tree,  321, 326, 327, 328, 333, 334, 358, 

359,375 

chronological, 327, 328, 343, 344, 358 
deep,  327,333,358 
permutation-optimized chronological, 

344 

well-populated, 327 

kernel,  147,  148,  149,  151,  153,  159,  160, 
161,  162,  163,  165,  166,  185,416, 

632 

Subject Index 

420,421,423,428,430,432,438, 
439,440,441,446,447,448,474, 
476,518,540,541,542,547,549 

Cauchy,  148,  149,  165 
Deheuvels',433 
de la Vallee-Poussin, 446 
devilish,  162 
Epanechniko~ 148, 149, 162 
exponential, 433 
gaussian,  105,  109,  148,  149,  165,416, 

421,426,433,448,531 

Hermite,  151 
multiparameter, 435, 447 
naive,  148, 149, 150, 153, 156,  157, 158, 

163,167,416,420,426,442,445 

negative valued,  151, 153 
polynomial, 434 
product, 435, 447 
regular,  149,  150,  156,  158,  164,  165, 

166,185,415,424,425,426,433, 
446 

star-shaped, 432 
uniform, see naive kernel 
window, see naive kernel 

kernel complexity, 429, 430, 431, 432, 433, 

434,435,436,446,447 

kernel density estimation,  149,  150,  151, 

161,162,208,433,447 

kernel rule, 98,  105,  109,  147,  148,  149, 

150,  151,  153,  159,  160,  161,  162, 
163,166,167,171,185,285,290, 
390,408,409,415,416,417,423, 
428,429,430,439,440,442,444, 
445,446,447,462,474,476,477, 
531,532,540,547,552 

automatic,  150,423,424,425,426 
variable, 447, 448 

Khinchine's inequality, 410, 588 
k-local rule, 64, 69, 414 
k-means clustering, 380, 381, 384, 542 
Kolmogorov-Lorentz network, 535 
Kolmogorov-Lorentz representation, 511, 

534,535,538,546 

Kolmogorov-Smirnov distance, 41,  271, 

272,274 

generalized, 271, 272, 273, 274, 277, 

278 

Kolmogorov-Smirnov statistic, 142,207 
Kolmogorov variational distance, 22,  36 

k-spacing rule, 320, 363, 373,  374,  376, 

420 

Kullback-Leibler divergence, 27,  34,  36, 

252,258,261 

L]  consistency,  109,  143,  161,  162,  184 
L] convergence, 93, 270 
L] distance,  15,  16,492 
L] error, 20,  107,  108,  150,  162,208,525, 

526,527,528,542 

L2  consistency,  102,284,559 
L2erro~ 104,283,284,445,526,542 
large deviation inequality,  136,  190, 364, 

365,379 

learning, 2, 201, 202, 211, 233 

algorithm, 201 
supervised, 2 
with a teacher, 2 

learning vector quantization, 310 
Lebesgue's density theorem, 580 
LeCam's inequality, 33, 244 
likelihood product, 250, 258, 259, 260, 268 
linear classifier,  14,  19, 39,40,44,47,48, 
49,50,51,53,54,55,57,58,59, 
191,224,225,260,311,342,343, 
390,399,466,467,468,476,490, 
501,507,510,540,567 

generalized, 39, 160,224,225,266,279, 
285,287,289,290,398,399,501, 
505,506,532,549 

linear discriminant, see linear classifier 
linear independence, 222 
linear ordering by inclusion, 231 
Lloyd-Max algorithm, 380 
local average estimator, 98 
logistic discrimination, 259 
log-linear model, 472 
L p  error, 525, 526, 527 
lying teacher, see imperfect training 

Mahalanobis distance, 30, 36, 47, 48,57, 

566,572 

majority vote, 61,  84,  86,94, 96, 98,  101, 
106,  109,  147,  152,  166,  170,  176, 
177,178,183,317,319,322,327, 
328,333,334,339,341,342,343, 
346,347,349,356,361,363,368, 
376,377,383,384,392,420,435, 

463,474,475,512,513,514,516, 
525 

Markov's inequality, 76,  117,  123,256, 

309,379,582 

martingale,  134,  135,  142 
martingale difference sequence,  133,  134, 

135,136 

Matushita error, 23, 29, 34, 36,  109 
maximum likelihood, 249, 250, 251, 252, 
253,254,257,259,260,261,262, 
265,266,268,269,270,276,277, 
444,467,472,473,475,527,540, 
551 

distribution format,  250, 260 
regression format,  249, 250, 260 
set format,  249 

Maxwell distribution, 266 
McDiarmid's inequality, 3,  136,  142,  158, 
162,172,174,201,211,346,414, 
448,553,558 

median tree,  322, 323, 324, 325, 334, 358, 

362,375 

theoretical, 323, 324, 358 

method of bounded differences,  133,  135 
metric, see distance 
metric entropy, 480, 481, 484, 486, 492, 

526 

minimax lower bound, 234, 235, 236 
minimum description length principle, 292, 

295 

minimum distance estimation, 265, 266, 

270,271,272,274,277,278 
moment generating function,  56, 589 
monomial, 468, 469, 476, 531, 533, 534 
monotone layer,  183,226,227,229,232, 

487 

moving window rule,  147,  148,  150,  552, 

553,555,559 

multinomial discrimination, 461 
multinomial distribution, 227, 321, 589 
multivariate normal distribution,  see 

normal distribution 

Subject Index 

633 

307,310,392,413,414,441,448, 
453,456,553 

nearest neighbor clustering, 377, 379 
nearest neighbor density estimation,  184 
nearest neighbor error, 22, 23, 29,  31, 57, 

63,69,132,307,308,361,551,559 

nearest neighbor rule 

I-NN,5, 70,  80,  84,  87,  88,106,107, 

152,  169,  180,  181,  185,285, 303, 
304,305,306,307,308,309,310, 
311,313,359,360,387,390,391, 
395,398,405,407,414,454,458, 
475,550 

3-NN,14,80,84,86,405,565,566 
admissibility of the  1-NN rule,  5,  81,  87, 

174 

asymptotic error probability of the  1-NN 

rule, see nearest neighbor error 

asymptotic error probability of the k-NN 

rule, 70, 71, 73, 75, 79,82,169 

automatic,  169,387,451 
based on reference data, see prototype 

NN rule 

condensed, 303, 304, 307, 309,455 
data-based,  see automatic nearest 

neighbor rule 

edited, 62, 307, 309, 313,455 
k-NN, 3,4,5, 6, 61,62,64,69, 70, 73, 74, 
80,81,82,83,85,86,87,98,100, 
101,169,170,175,176,177,179, 
180,181, 182,  185,290,309,313, 
343,389,397,408,413,414,415, 
420,421,451,452,453,455,456, 
457,458,459,549,552,553,555 

(k, l)-NN, 81, 82,  88 
layered,  183 
prototype, 307,309,310, 311,377,455 
recursive,  177, 179,  182 
relabeling,  169,  180,  181, 185,309,420 
selective, 455 
variable metric, 455 
weighted, 61, 71, 73, 74,  85,  87,  88,  90, 

179,  183,451,453 

neural network, 444, 507,510,514,520, 

natural classifier, 317,  318, 320, 323, 327, 

527,528,530,531 

342,347,358,513,514,542 

nearest neighbor, 62, 63, 69, 80,  84, 85,  89, 

90,  101,  166,  169,  170,  171,  175, 
177,  179,  180,  181,  182,  184,  185, 

classifier, 5, 39, 290, 516, 517, 543, 549 
multilayer, 509, 523, 545 
with one hidden layer, 507, 508, 509, 

510,517,518,519,520,521,525, 

634 

Subject Index 

538,543,544 

with two hidden layers, 509, 510, 514, 
515,516,517,522,537,542,546 

Neyman-Pearson lemma,  18 
normal distribution, 31, 35,47,48,49,56, 

57,59,76,85,86,249,264,265, 
266,276,343,390,399,415,564, 
566,567,572,588,590 

order statistics, 88,  178,257,320,328,358, 

363,372,382 

outer layer, see monotone layer 
overfitting,  188,286,291,297,348,395, 

454,527,532,544 

packing number, 481, 496 
Padaline,531,532,545 
parameter estimation, 250, 268, 270, 277, 

399 

parametric classification,  24,  250,  263, 

270,275,390,415,472 

Parseval's identity, 283 
partition, 94,  106,  109,  140, 143,  144,  147, 
152,287,301,307,310,312,315, 
317,318,319,320,321,327,328, 
333,343,346,347,349,350,351, 
352,353,354,356,357,360,363, 
364,365,366,368,372,373,374, 
378,381,382,383,392,394,395, 
399,400,402,403,404,417,418, 
419,420,454,477,506,511,513, 
553 

complexity of a family of, 365, 367, 369, 

403,404 

cubic, 95, 96,  107,  133,  138,  143,382, 

475 

data-dependent, 94, 363, 364, 370, 404, 

420 

E -effective cardinality of,  144 
family of, 365, 367, 368, 369,403,404, 

405 

~-approximating, 144 

partitioning rule, see histogram rule 
perceptron, 6,14,39,40,44,507,509,510, 

514,515,516,532,544 

perceptron criterion, 59 
pigeonhole principle, 238, 476 
Pinsker's inequality, 37 
planar graph, 312, 380 

plug-in decision,  15,  16,  17,  19,20,24,93, 
265,266,267,270,271,276,277, 
467,468,471,472,475 

polynomial discriminant function,  160, 

279,287,531 

polynomial network, 532, 533, 534 
potential function rule,  159,  160,279,540 
principal component analysis, 455 
probabilistic method,  112,234,496 
projection pursuit, 538, 539, 546 
pseudo dimension, 498, 506 
pseudo probability of error, 82 

quadratic discrimination rule,  225, 265, 

276,343,390,468,476 

quadtree, 321, 333, 334, 348, 361 

chronological, 333, 361 
deep, 333, 361 

quantization, 380,461,463,567 

radial basis function, 540, 541, 542, 547 
Radon-Nikodym derivative, 272, 313 
Radon-Nikodym theorem, 578, 586 
rate of convergence, 4,  102,  104,  108,  111, 
113,114,118,128,239,262,264, 
267,269,275,276,284,294,295, 
296,297,300,357,390,483,485, 
486,494,527 

Rayleigh distribution, 266, 277 
record, 330, 331, 358 
recursive kernel rule,  160,  161,  166 
regression function,  9,  11, 93,  254, 261, 

445,454,474,475,479,483,484, 
487,489 

estimation, 92, 93,  102,  103,  104,  108, 
114,149,169,172,185,405,445, 
527 

relative stability,  162 
Rogers-Wagner inequality, 4,  411,  413, 

417,418 

rotation invariance, see transformation 

invariance 
Royall's rule,  185 

sample complexity, 201, 202, 205, 206, 

211,234,245 
sample scatter, 46, 47 
sampling without replacement, 213 

Subject Index 

635 

scale invariance,  see transformation 

559 

invariance 

standard empirical measure, see empirical 

sc~ing, 144,  166,  177, 179,381,385,387, 

measure 

391 

scatter matrix, 46 
Scheffe's theorem, 32, 36, 208, 211, 278, 

383 

search tree, 62, 326 

balanced, 322 

shatter coefficient, 196,  197, 198,200,212, 
215,216,219,220,222,223,226, 
271,273,312,347,365,389,390, 
404,429,453,492,493,496,497, 
512,522,541 

shattering,  196,  198,  199, 217, 218, 219, 

220,221,222,223,226,229,235, 
239,277,436,437,438,476,477, 
498,499,521,541 

sieve method, 444 
sigmoid, 56, 59, 507, 510, 515, 517, 519, 
520,521,524,526,528,542,543, 
544,545 

arctan, 508, 524 
gaussian, 508, 524 
logistic, see standard sigmoid 
piecewise polynomial, 524 
standard, 508, 524, 525 
threshold,  507, 508, 516,  517,  519, 

521,522,523,524,525,526,537, 
543,544 

signed measure, 272, 273 
skeleton estimate, 292, 480, 482, 483, 484, 

486,487,489,493 

smart rule,  106, 109 
smoothing factor,  148, 149, 153,  160, 162, 
163,165,166,185,390,416,423, 
424,428,429,435,436,439,441, 
442,444,446,462,474,531 

data-dependent, 423, 424, 425, 426, 474 

spacing, 328, 358, 372 
splitting criterion, 334, 335, 336, 337, 338, 

339,348 

splitting function,  334 
splitting the data,  132,  185, 211,  298, 309, 
347,387,388,389,390,391,392, 
397,428,436,452,453,454,541 
squared error,  11,54, 55, 56, 58, 59,445, 
448,489,490,491,492,493,501, 
502,505,506,525,526,527,551, 

statistically equivalent blocks,  178,  363, 

372,374,375,384,393 

Stirling's formula, 83,238,415,449,587 
stochastic approximation,  161, 505 
stochastic process 

expansion of, 573 
sampling of, 573 

Stoller split, 40, 334, 335, 336, 338, 350, 

361,375 

empirical, 43, 339, 343 
theoretical, 40, 41, 42, 43, 56 

Stoller's rule, 43, 44 
Stone's lemma, 65, 66, 69, 83 
Stone's theorem, 97,  100,  101,  107,  181, 

182,  183,  185,456 

Stone-Weierstrass theorem, 281, 287,519, 

538,543,580 

strictly separable distribution,  104, 162 
strong consistency, 92, 93,  128 

definition of,  3, 91 
of Fourier series rule, 282, 284 
of kernel rules,  149, 161, 162,431 
of nearest neighbor rules,  100, 169,  174, 

451 

of neural network classifiers, 528 
of partitioning rules,  143,  155,341,347, 

368,369,370,376,378,383 

strong universal consistency, 395 

of complexity regularization, 290, 292, 

298 

definition of, 92 
of generalized linear rules, 287, 502, 505 
of kernel rules,  149,  150,  166,424,426, 

429,430,435 

of nearest neighbor rules,  170,  176, 453, 

458,459 

of neural network classifiers, 523, 528, 

545 

of partitioning rules, 96,  133,  138,  142, 
143,144,170,288,369,376,381, 
382,384,462 

structural risk minimization, see complexity 

regularization 

sufficient statistics,  268, 270,  571, 572, 

573,574 

superclassifier, 5,  118 

636 

Subject Index 

support, 63, 80, 84, 579 
symmetric rule,  109,411,413,414,419 
symmetrization,  193,  194, 203, 278 

tennis rule, 82, 88 
testing sequence,  121,  124,  125,  126,  127, 
131,388,390,392,393,397,428, 
453,455,457 

total boundedness, 480, 483, 486, 487, 493, 

494 

total positivity, 434 
total variation, 22, 32, 36, 208 
transformation invariance, 6, 62,  84,  177, 
178,179,182,183,285,318,319, 
320,328,334,337,348,373,375, 
385,512,531 

tree 

a-balanced, 358 
binary, 315, 316, 364 
BSP,  see binary space partition tree 
classifier, 6,  39, 316, 319, 322, 341, 343, 
347,364,383,390,394,477,516, 
542 

depth of, 315, 333, 358 
expression, 469 
greedy, 348, 349,350 
height of, 315, 334, 474 
Horton-Strahler number of, 477 
hyperplane,  see binary space partition 

tree 

k-d, see k-d tree 
median, see median tree 
ordered, 316 
ordinary, 316, 317, 318 
perpendicular splitting, 334, 336, 348 
pruning of, see trimming of a tree 
sphere, 316, 317 
trimming of, 327, 336, 338, 339 

unbiased estimator, 121,285 
uniform deviations, see uniform laws of 

large numbers 

uniform marginal transformation, 319 
universal consistency, 97, 98,  106,  109, 

113,118,226,285,387,510 
of complexity regularization, 293 
definition of, 3, 92 
of generalized linear rules, 286, 501, 502 
ofkemel rules,  149,  153,  163,  164,  165, 

166,167,423,425,441,442 

of nearest neighbor rules, 61,  100,  169, 
170,175,177,178,180,181,182, 
183,184,185,453,454,455 

of neural network classifiers, 512, 524, 

526,528,532,539,540,547 

of partitioning rules,  95, 96,  107,  133, 

253,344,346,364,382,393 

Vapnik-Chervonenkis dimension, see vc 

dimension 

Vapnik-Chervonenkis inequality, 5,  138, 

192,196,197,198,201,202,203, 
206,225,226,229,233,271,273, 
275,278,295,304,312,356,357, 
364,365,366,388,399,429,436, 
467,476,492,494 

variation,  144 
vc class,  197,210,251,290,297 
vc dimension, 5,  192,  196,  197,200,201, 
202,206,210,215,216,218,219, 
220,221,223,224,226,229,230, 
231,232,233,235,236,238,239, 
240,243,245,247,257,272,273, 
275,277,278,286,289,290,291, 
292,294,295,297,298,300,301, 
346,347,398,436,439,444,447, 
467,471,476,477,486,496,521, 
522,523,524,526,530,533,534, 
537,538,541,542,544,546,547 

Voronoi cell, 62, 304, 312, 378 
Voronoi partition, 62, 304, 312, 377, 378, 

380 

weighted average estimator, 98 

uniform laws of large numbers,  190,  196, 
198,199,207,228,366,490,529 

X-property, 319, 320, 322,333,334, 339, 

342,343,357,358,362,513 

Pattern  recognition  presents  one  of the  most significant challenges 
for scientists  and  engineers,  and  many  different approaches  have 
been  proposed.  The  aim  of  this  book  is  to  provide  a self-contained 
account of  probabilistic analysis  of these  approaches.  The  book 
includes a discussion of distance measures, nonparametric methods 
based on  kernels or nearest neighbors, Vapnik-Chervonenkis theo(cid:173)
ry,  epsilon  entropy,  parametric classification,  error estimation,  tree 
classifiers,  and neural networks. 
Wherever  possible,  distribution-free properties  and  inequalities are 
derived.  A substantial  portion  of the  results  or the  analysis  is  new. 
Over 430 problems and exercises complement the  material. 

ISBN  0-387-94618-7 

ISB!\,)  0-387-94618-7 

