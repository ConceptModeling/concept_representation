               A Brief Introduction to Neural Networks    David Kriesel  dkriesel.com Download location:http://www.dkriesel.com/en/science/neural_networksNEW – for the programmers: Scalable and efficient NN framework, written in JAVA http://www.dkriesel.com/en/tech/snipedkriesel.com

In remembrance of
Dr. Peter Kemp, Notary (ret.), Bonn, Germany.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

iii

A small preface

"Originally, this work has been prepared in the framework of a seminar of the
University of Bonn in Germany, but it has been and will be extended (after
being presented and published online under www.dkriesel.com on
5/27/2005). First and foremost, to provide a comprehensive overview of the
subject of neural networks and, second, just to acquire more and more
knowledge about LATEX . And who knows – maybe one day this summary will
become a real preface!"
Abstract of this work, end of 2005

The above abstract has not yet become a
preface but at least a little preface, ever
since the extended text (then 40 pages
long) has turned out to be a download
hit.

Ambition and intention of this
manuscript

The entire text is written and laid out
more eﬀectively and with more illustra-
tions than before.
I did all the illustra-
tions myself, most of them directly in
LATEX by using XYpic. They reﬂect what
I would have liked to see when becoming
acquainted with the subject: Text and il-
lustrations should be memorable and easy
to understand to oﬀer as many people as
possible access to the ﬁeld of neural net-
works.
Nevertheless, the mathematically and for-
mally skilled readers will be able to under-

stand the deﬁnitions without reading the
running text, while the opposite holds for
readers only interested in the subject mat-
ter; everything is explained in both collo-
quial and formal language. Please let me
know if you ﬁnd out that I have violated
this principle.

The sections of this text are mostly
independent from each other

The document itself is divided into diﬀer-
ent parts, which are again divided into
chapters. Although the chapters contain
cross-references, they are also individually
accessible to readers with little previous
knowledge. There are larger and smaller
chapters: While the larger chapters should
provide profound insight into a paradigm
of neural networks (e.g. the classic neural
network structure: the perceptron and its
learning procedures), the smaller chapters
give a short overview – but this is also ex-

v

dkriesel.com

the original high-performance simulation
design goal. Those of you who are up for
learning by doing and/or have to use a
fast and stable neural networks implemen-
tation for some reasons, should deﬁnetely
have a look at Snipe.

However, the aspects covered by Snipe are
not entirely congruent with those covered
by this manuscript. Some of the kinds
of neural networks are not supported by
Snipe, while when it comes to other kinds
of neural networks, Snipe may have lots
and lots more capabilities than may ever
be covered in the manuscript in the form
of practical hints. Anyway, in my experi-
ence almost all of the implementation re-
quirements of my readers are covered well.
On the Snipe download page, look for the
section "Getting started with Snipe" – you
will ﬁnd an easy step-by-step guide con-
cerning Snipe and its documentation, as
well as some examples.

SNIPE: This manuscript frequently incor-
porates Snipe. Shaded Snipe-paragraphs
like this one are scattered among large
parts of the manuscript, providing infor-
mation on how to implement their con-
text in Snipe. This also implies that
those who do not want to use Snipe,
just have to skip the shaded Snipe-
paragraphs! The Snipe-paragraphs as-
sume the reader has had a close look at
the "Getting started with Snipe" section.
Often, class names are used. As Snipe con-
sists of only a few diﬀerent packages, I omit-
ted the package names within the qualiﬁed
class names for the sake of readability.

plained in the introduction of each chapter.
In addition to all the deﬁnitions and expla-
nations I have included some excursuses
to provide interesting information not di-
rectly related to the subject.
Unfortunately, I was not able to ﬁnd free
German sources that are multi-faceted
in respect of content (concerning the
paradigms of neural networks) and, nev-
ertheless, written in coherent style. The
aim of this work is (even if it could not
be fulﬁlled at ﬁrst go) to close this gap bit
by bit and to provide easy access to the
subject.

Want to learn not only by
reading, but also by coding?
Use SNIPE!

SNIPE1 is a well-documented JAVA li-
brary that implements a framework for
neural networks in a speedy, feature-rich
and usable way.
It is available at no
cost for non-commercial purposes. It was
originally designed for high performance
simulations with lots and lots of neural
networks (even large ones) being trained
simultaneously. Recently, I decided to
give it away as a professional reference im-
plementation that covers network aspects
handled within this work, while at the
same time being faster and more eﬃcient
than lots of other implementations due to
1 Scalable and Generalized Neural Information Pro-
cessing Engine, downloadable at http://www.
dkriesel.com/tech/snipe, online JavaDoc at
http://snipe.dkriesel.com

vi

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

It’s easy to print this
manuscript

This text
is completely illustrated in
color, but it can also be printed as is in
monochrome: The colors of ﬁgures, tables
and text are well-chosen so that in addi-
tion to an appealing design the colors are
still easy to distinguish when printed in
monochrome.

There are many tools directly
integrated into the text

Diﬀerent aids are directly integrated in the
document to make reading more ﬂexible:
However, anyone (like me) who prefers
reading words on paper rather than on
screen can also enjoy some features.

In the table of contents, diﬀerent
types of chapters are marked

Diﬀerent types of chapters are directly
marked within the table of contents. Chap-
ters, that are marked as "fundamental"
are deﬁnitely ones to read because almost
all subsequent chapters heavily depend on
them. Other chapters additionally depend
on information given in other (preceding)
chapters, which then is marked in the ta-
ble of contents, too.

Speaking headlines throughout the
text, short ones in the table of
contents

The whole manuscript is now pervaded by
such headlines. Speaking headlines are
not just title-like ("Reinforcement Learn-
ing"), but centralize the information given
in the associated section to a single sen-
tence.
In the named instance, an appro-
priate headline would be "Reinforcement
learning methods provide feedback to the
network, whether it behaves good or bad".
However, such long headlines would bloat
the table of contents in an unacceptable
way. So I used short titles like the ﬁrst one
in the table of contents, and speaking ones,
like the latter, throughout the text.

Marginal notes are a navigational
aid

The entire document contains marginal
notes in colloquial language (see the exam- Hypertext
ple in the margin), allowing you to "scan"
on paper
:-)
the document quickly to ﬁnd a certain pas-
sage in the text (including the titles).
New mathematical symbols are marked by
speciﬁc marginal notes for easy ﬁnding (cid:74)x
(see the example for x in the margin).

There are several kinds of indexing

This document contains diﬀerent types of
indexing:
If you have found a word in
the index and opened the corresponding
page, you can easily ﬁnd it by searching

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

vii

dkriesel.com

for highlighted text – all indexed words
are highlighted like this.

Mathematical symbols appearing in sev-
eral chapters of this document (e.g. Ω for
an output neuron; I tried to maintain a
consistent nomenclature for regularly re-
curring elements) are separately indexed
under "Mathematical Symbols", so they
can easily be assigned to the correspond-
ing term.
Names of persons written in small caps
are indexed in the category "Persons" and
ordered by the last names.

3. You must maintain the author’s attri-
bution of the document at all times.
4. You may not use the attribution to
imply that the author endorses you
or your document use.

For I’m no lawyer, the above bullet-point
summary is just informational: if there is
any conﬂict in interpretation between the
summary and the actual license, the actual
license always takes precedence. Note that
this license does not extend to the source
ﬁles used to produce the document. Those
are still mine.

Terms of use and license

Beginning with the epsilon edition, the
text is licensed under the Creative Com-
mons Attribution-No Derivative Works
3.0 Unported License2, except for some
little portions of the work licensed under
more liberal licenses as mentioned (mainly
some ﬁgures from Wikimedia Commons).
A quick license summary:

1. You are free to redistribute this docu-
ment (even though it is a much better
idea to just distribute the URL of my
homepage, for it always contains the
most recent version of the text).

2. You may not modify, transform, or
build upon the document except for
personal use.

2 http://creativecommons.org/licenses/

by-nd/3.0/

How to cite this manuscript

There’s no oﬃcial publisher, so you need
to be careful with your citation. Please
ﬁnd more information in English and
German language on my homepage, re-
spectively the subpage concerning the
manuscript3.

Acknowledgement

Now I would like to express my grati-
tude to all the people who contributed, in
whatever manner, to the success of this
work, since a work like this needs many
helpers. First of all, I want to thank
the proofreaders of this text, who helped
me and my readers very much.
In al-
phabetical order: Wolfgang Apolinarski,
Kathrin Gräve, Paul
Imhoﬀ, Thomas
3 http://www.dkriesel.com/en/science/

neural_networks

viii

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Kühn, Christoph Kunze, Malte Lohmeyer,
Joachim Nock, Daniel Plohmann, Daniel
Rosenthal, Christian Schulz and Tobias
Wilken.
Additionally, I want to thank the readers
Dietmar Berger, Igor Buchmüller, Marie
Christ, Julia Damaschek, Jochen Döll,
Maximilian Ernestus, Hardy Falk, Anne
Feldmeier, Sascha Fink, Andreas Fried-
mann, Jan Gassen, Markus Gerhards, Se-
bastian Hirsch, Andreas Hochrath, Nico
Höft, Thomas Ihme, Boris Jentsch, Tim
Hussein, Thilo Keller, Mario Krenn, Mirko
Kunze, Maikel Linke, Adam Maciak,
Benjamin Meier, David Möller, Andreas
Müller, Rainer Penninger, Lena Reichel,
Alexander Schier, Matthias Siegmund,
Mathias Tirtasana, Oliver Tischler, Max-
imilian Voit,
Igor Wall, Achim Weber,
Frank Weinreis, Gideon Maillette de Buij
Wenniger, Philipp Woock and many oth-
ers for their feedback, suggestions and re-
marks.
Additionally, I’d like to thank Sebastian
Merzbach, who examined this work in a
very conscientious way ﬁnding inconsisten-
cies and errors. In particular, he cleared
lots and lots of language clumsiness from
the English version.
Especially, I would like to thank Beate
Kuhl for translating the entire text from
German to English, and for her questions
which made me think of changing the
phrasing of some paragraphs.
I would particularly like to thank Prof.
Rolf Eckmiller and Dr. Nils Goerke as
well as the entire Division of Neuroinfor-
matics, Department of Computer Science

of the University of Bonn – they all made
sure that I always learned (and also had
to learn) something new about neural net-
works and related subjects. Especially Dr.
Goerke has always been willing to respond
to any questions I was not able to answer
myself during the writing process. Conver-
sations with Prof. Eckmiller made me step
back from the whiteboard to get a better
overall view on what I was doing and what
I should do next.

Globally, and not only in the context of
this work, I want to thank my parents who
never get tired to buy me specialized and
therefore expensive books and who have
always supported me in my studies.

For many "remarks" and the very special
and cordial atmosphere ;-) I want to thank
Andreas Huber and Tobias Treutler. Since
our ﬁrst semester it has rarely been boring
with you!

Now I would like to think back to my
school days and cordially thank some
teachers who (in my opinion) had im-
parted some scientiﬁc knowledge to me –
although my class participation had not
always been wholehearted: Mr. Wilfried
Hartmann, Mr. Hubert Peters and Mr.
Frank Nökel.

Furthermore I would like to thank the
whole team at the notary’s oﬃce of Dr.
Kemp and Dr. Kolb in Bonn, where I have
always felt to be in good hands and who
have helped me to keep my printing costs
low - in particular Christiane Flamme and
Dr. Kemp!

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

ix

dkriesel.com

Thanks go also to the Wikimedia Com-
mons, where I took some (few) images and
altered them to suit this text.
Last but not least I want to thank two
people who made outstanding contribu-
tions to this work who occupy, so to speak,
a place of honor: My girlfriend Verena
Thomas, who found many mathematical
and logical errors in my text and dis-
cussed them with me, although she has
lots of other things to do, and Chris-
tiane Schultze, who carefully reviewed the
text for spelling mistakes and inconsisten-
cies.

David Kriesel

x

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Contents

A small preface

I

From biology to formalization – motivation, philosophy, history and
realization of neural models

v

1

1 Introduction, motivation and history

3
3
1.1 Why neural networks? . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.1 The 100-step rule . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.2
Simple application examples . . . . . . . . . . . . . . . . . . . . .
8
1.2 History of neural networks . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.2.1 The beginning . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.2 Golden age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.2.3 Long silence and slow reconstruction . . . . . . . . . . . . . . . . 11
1.2.4 Renaissance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

Exercises

2 Biological neural networks

13
2.1 The vertebrate nervous system . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.1 Peripheral and central nervous system . . . . . . . . . . . . . . . 13
2.1.2 Cerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.3 Cerebellum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1.4 Diencephalon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1.5 Brainstem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.2 The neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.2.1 Components
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.2.2 Electrochemical processes in the neuron . . . . . . . . . . . . . . 19
2.3 Receptor cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.3.1 Various types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.3.2
Information processing within the nervous system . . . . . . . . 25
2.3.3 Light sensing organs . . . . . . . . . . . . . . . . . . . . . . . . . 26
. . . . . . . . . . . . . . . . 28

2.4 The amount of neurons in living organisms

xi

Contents

dkriesel.com

2.5 Technical neurons as caricature of biology . . . . . . . . . . . . . . . . . 30
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Exercises

3 Components of artiﬁcial neural networks (fundamental)

33
. . . . . . . . . . . . . . . . . . 33
3.1 The concept of time in neural networks
3.2 Components of neural networks . . . . . . . . . . . . . . . . . . . . . . . 33
3.2.1 Connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2.2 Propagation function and network input . . . . . . . . . . . . . . 34
3.2.3 Activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.2.4 Threshold value . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.2.5 Activation function . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.2.6 Common activation functions . . . . . . . . . . . . . . . . . . . . 37
3.2.7 Output function . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.8 Learning strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.3 Network topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.3.1 Feedforward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.3.2 Recurrent networks . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.3.3 Completely linked networks . . . . . . . . . . . . . . . . . . . . . 42
3.4 The bias neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.5 Representing neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.6 Orders of activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.6.1
Synchronous activation . . . . . . . . . . . . . . . . . . . . . . . 45
3.6.2 Asynchronous activation . . . . . . . . . . . . . . . . . . . . . . . 46
Input and output of data . . . . . . . . . . . . . . . . . . . . . . . . . . 48
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

3.7
Exercises

Supervised learning

4 Fundamentals on learning and training samples (fundamental)

51
4.1 Paradigms of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.1.1 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . 52
4.1.2 Reinforcement learning
. . . . . . . . . . . . . . . . . . . . . . . 53
4.1.3
. . . . . . . . . . . . . . . . . . . . . . . . . 53
4.1.4 Oﬄine or online learning? . . . . . . . . . . . . . . . . . . . . . . 54
4.1.5 Questions in advance . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.2 Training patterns and teaching input . . . . . . . . . . . . . . . . . . . . 54
4.3 Using training samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.3.1 Division of the training set
. . . . . . . . . . . . . . . . . . . . . 57
4.3.2 Order of pattern representation . . . . . . . . . . . . . . . . . . . 57
4.4 Learning curve and error measurement . . . . . . . . . . . . . . . . . . . 58
4.4.1 When do we stop learning? . . . . . . . . . . . . . . . . . . . . . 59

xii

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Contents

4.5.1 Problems of gradient procedures

4.5 Gradient optimization procedures . . . . . . . . . . . . . . . . . . . . . . 61
. . . . . . . . . . . . . . . . . . 62
4.6 Exemplary problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.6.1 Boolean functions
. . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.6.2 The parity function . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.6.3 The 2-spiral problem . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.6.4 The checkerboard problem . . . . . . . . . . . . . . . . . . . . . . 65
4.6.5 The identity function . . . . . . . . . . . . . . . . . . . . . . . . 65
4.6.6 Other exemplary problems
. . . . . . . . . . . . . . . . . . . . . 66
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.7.1 Original rule
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.7.2 Generalized form . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

4.7 Hebbian rule

Exercises

II Supervised learning network paradigms

69

5 The perceptron, backpropagation and its variants

71
5.1 The singlelayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.1.1 Perceptron learning algorithm and convergence theorem . . . . . 75
5.1.2 Delta rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
5.2 Linear separability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.3 The multilayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.4 Backpropagation of error . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
5.4.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.4.2 Boiling backpropagation down to the delta rule . . . . . . . . . . 91
5.4.3
Selecting a learning rate . . . . . . . . . . . . . . . . . . . . . . . 92
5.5 Resilient backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . 93
. . . . . . . . . . . . . . . . . . . . . . . . . 94
5.5.1 Adaption of weights
5.5.2 Dynamic learning rate adjustment
. . . . . . . . . . . . . . . . . 94
5.5.3 Rprop in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.6 Further variations and extensions to backpropagation . . . . . . . . . . 96
5.6.1 Momentum term . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.6.2 Flat spot elimination . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.6.3
Second order backpropagation . . . . . . . . . . . . . . . . . . . 98
5.6.4 Weight decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
5.6.5 Pruning and Optimal Brain Damage . . . . . . . . . . . . . . . . 98
Initial conﬁguration of a multilayer perceptron . . . . . . . . . . . . . . 99
5.7.1 Number of layers . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.7.2 The number of neurons
. . . . . . . . . . . . . . . . . . . . . . . 100

5.7

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

xiii

Contents

dkriesel.com

5.7.3
5.7.4

Selecting an activation function . . . . . . . . . . . . . . . . . . . 100
Initializing weights . . . . . . . . . . . . . . . . . . . . . . . . . . 101
. . . . . . . . . . . . 101
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

5.8 The 8-3-8 encoding problem and related problems
Exercises

6 Radial basis functions

Information processing in RBF neurons

105
6.1 Components and structure . . . . . . . . . . . . . . . . . . . . . . . . . . 105
Information processing of an RBF network . . . . . . . . . . . . . . . . 106
6.2
6.2.1
. . . . . . . . . . . . . . 108
6.2.2 Analytical thoughts prior to the training . . . . . . . . . . . . . . 111
6.3 Training of RBF networks . . . . . . . . . . . . . . . . . . . . . . . . . . 114
. . . . . . . . . . . . . . . . 115
. . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6.4.1 Adding neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6.4.2 Limiting the number of neurons . . . . . . . . . . . . . . . . . . . 119
6.4.3 Deleting neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
. . . . . . . . . . 119
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

6.5 Comparing RBF networks and multilayer perceptrons
Exercises

6.3.1 Centers and widths of RBF neurons

6.4 Growing RBF networks

7 Recurrent perceptron-like networks (depends on chapter 5)

121
7.1 Jordan networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.2 Elman networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
7.3 Training recurrent networks . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.3.1 Unfolding in time . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.3.2 Teacher forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
7.3.3 Recurrent backpropagation . . . . . . . . . . . . . . . . . . . . . 127
7.3.4 Training with evolution . . . . . . . . . . . . . . . . . . . . . . . 127

8 Hopﬁeld networks

129
8.1
Inspired by magnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
8.2 Structure and functionality . . . . . . . . . . . . . . . . . . . . . . . . . 129
Input and output of a Hopﬁeld network . . . . . . . . . . . . . . 130
8.2.1
8.2.2
Signiﬁcance of weights . . . . . . . . . . . . . . . . . . . . . . . . 131
8.2.3 Change in the state of neurons . . . . . . . . . . . . . . . . . . . 131
8.3 Generating the weight matrix . . . . . . . . . . . . . . . . . . . . . . . . 132
8.4 Autoassociation and traditional application . . . . . . . . . . . . . . . . 133
8.5 Heteroassociation and analogies to neural data storage . . . . . . . . . . 134
8.5.1 Generating the heteroassociative matrix . . . . . . . . . . . . . . 135
8.5.2
Stabilizing the heteroassociations . . . . . . . . . . . . . . . . . . 135
8.5.3 Biological motivation of heterassociation . . . . . . . . . . . . . . 136

xiv

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Contents

8.6 Continuous Hopﬁeld networks . . . . . . . . . . . . . . . . . . . . . . . . 136
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

9 Learning vector quantization

139
9.1 About quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
9.2 Purpose of LVQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.3 Using codebook vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.4 Adjusting codebook vectors . . . . . . . . . . . . . . . . . . . . . . . . . 141
9.4.1 The procedure of learning . . . . . . . . . . . . . . . . . . . . . . 141
. . . . . . . . . . . . . . . . . . . . . . . 143
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

9.5 Connection to neural networks
Exercises

III Unsupervised learning network paradigms

145

10 Self-organizing feature maps

10.4 Examples

147
10.1 Structure
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
10.2 Functionality and output interpretation . . . . . . . . . . . . . . . . . . 149
10.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
10.3.1 The topology function . . . . . . . . . . . . . . . . . . . . . . . . 150
10.3.2 Monotonically decreasing learning rate and neighborhood . . . . 152
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
10.4.1 Topological defects . . . . . . . . . . . . . . . . . . . . . . . . . . 156
10.5 Adjustment of resolution and position-dependent learning rate
. . . . . 156
10.6 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
10.6.1 Interaction with RBF networks . . . . . . . . . . . . . . . . . . . 161
10.7 Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
10.7.1 Neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
10.7.2 Multi-SOMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
10.7.3 Multi-neural gas
. . . . . . . . . . . . . . . . . . . . . . . . . . . 163
10.7.4 Growing neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . 164
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

Exercises

11 Adaptive resonance theory

165
11.1 Task and structure of an ART network . . . . . . . . . . . . . . . . . . . 165
11.1.1 Resonance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
11.2 Learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.2.1 Pattern input and top-down learning . . . . . . . . . . . . . . . . 167
11.2.2 Resonance and bottom-up learning . . . . . . . . . . . . . . . . . 167
11.2.3 Adding an output neuron . . . . . . . . . . . . . . . . . . . . . . 167

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

xv

Contents

dkriesel.com

11.3 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

IV Excursi, appendices and registers

169

A Excursus: Cluster analysis and regional and online learnable ﬁelds

171
A.1 k-means clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
A.2 k-nearest neighboring
A.3 ε-nearest neighboring
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
A.4 The silhouette coeﬃcient . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
A.5 Regional and online learnable ﬁelds . . . . . . . . . . . . . . . . . . . . . 175
A.5.1 Structure of a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . 176
A.5.2 Training a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
A.5.3 Evaluating a ROLF . . . . . . . . . . . . . . . . . . . . . . . . . 178
A.5.4 Comparison with popular clustering methods . . . . . . . . . . . 179
A.5.5 Initializing radii, learning rates and multiplier . . . . . . . . . . . 180
. . . . . . . . . . . . . . . . . . . . . . . . 180
A.5.6 Application examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180

Exercises

B Excursus: neural networks used for prediction

181
B.1 About time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
B.2 One-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 183
B.3 Two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 185
B.3.1 Recursive two-step-ahead prediction . . . . . . . . . . . . . . . . 185
B.3.2 Direct two-step-ahead prediction . . . . . . . . . . . . . . . . . . 185
B.4 Additional optimization approaches for prediction . . . . . . . . . . . . . 185
B.4.1 Changing temporal parameters . . . . . . . . . . . . . . . . . . . 185
B.4.2 Heterogeneous prediction . . . . . . . . . . . . . . . . . . . . . . 187
B.5 Remarks on the prediction of share prices . . . . . . . . . . . . . . . . . 187

C Excursus: reinforcement learning

191
C.1 System structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
C.1.1 The gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
C.1.2 Agent und environment . . . . . . . . . . . . . . . . . . . . . . . 193
C.1.3 States, situations and actions . . . . . . . . . . . . . . . . . . . . 194
C.1.4 Reward and return . . . . . . . . . . . . . . . . . . . . . . . . . . 195
C.1.5 The policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
C.2 Learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
C.2.1 Rewarding strategies . . . . . . . . . . . . . . . . . . . . . . . . . 198
C.2.2 The state-value function . . . . . . . . . . . . . . . . . . . . . . . 199

xvi

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Contents

C.2.3 Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . 201
C.2.4 Temporal diﬀerence learning . . . . . . . . . . . . . . . . . . . . 202
C.2.5 The action-value function . . . . . . . . . . . . . . . . . . . . . . 203
C.2.6 Q learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
C.3 Example applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
C.3.1 TD gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
. . . . . . . . . . . . . . . . . . . . . . . . . . 205
C.3.2 The car in the pit
. . . . . . . . . . . . . . . . . . . . . . . . . . 206
C.3.3 The pole balancer
. . . . . . . 207
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207

C.4 Reinforcement learning in connection with neural networks
Exercises

Bibliography

List of Figures

Index

209

215

219

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

xvii

Part I

From biology to formalization –

motivation, philosophy, history and

realization of neural models

1

Chapter 1
Introduction, motivation and history

How to teach a computer? You can either write a ﬁxed program – or you can
enable the computer to learn on its own. Living beings do not have any
programmer writing a program for developing their skills, which then only has
to be executed. They learn by themselves – without the previous knowledge
from external impressions – and thus can solve problems better than any
computer today. What qualities are needed to achieve such a behavior for
devices like computers? Can such cognition be adapted from biology? History,
development, decline and resurgence of a wide approach to solve problems.

1.1 Why neural networks?

There are problem categories that cannot
be formulated as an algorithm. Problems
that depend on many subtle factors, for ex-
ample the purchase price of a real estate
which our brain can (approximately) cal-
culate. Without an algorithm a computer
cannot do the same. Therefore the ques-
tion to be asked is: How do we learn to
explore such problems?

Exactly – we learn; a capability comput-
ers obviously do not have. Humans have
a brain that can learn. Computers have
some processing units and memory. They
allow the computer to perform the most
complex numerical calculations in a very
short time, but they are not adaptive.

If we compare computer and brain1, we
will note that, theoretically, the computer
should be more powerful than our brain:
It comprises 109 transistors with a switch-
ing time of 10−9 seconds. The brain con-
tains 1011 neurons, but these only have a
switching time of about 10−3 seconds.
The largest part of the brain is work-
ing continuously, while the largest part of
the computer is only passive data storage.
Thus, the brain is parallel and therefore
performing close to its theoretical maxi-
1 Of course, this comparison is - for obvious rea-
sons - controversially discussed by biologists and
computer scientists, since response time and quan-
tity do not tell anything about quality and perfor-
mance of the processing units as well as neurons
and transistors cannot be compared directly. Nev-
ertheless, the comparison serves its purpose and
indicates the advantage of parallelism by means
of processing time.

parallelism

Computers
cannot
learn

3

Chapter 1 Introduction, motivation and history

dkriesel.com

No. of processing units
Type of processing units
Type of calculation
Data storage
Switching time
Possible switching operations
Actual switching operations

Brain
≈ 1011
Neurons
massively parallel
associative
≈ 10−3s
≈ 1013 1
≈ 1012 1

s

s

Computer
≈ 109
Transistors
usually serial
address-based
≈ 10−9s
≈ 1018 1
≈ 1010 1

s

s

Table 1.1: The (ﬂawed) comparison between brain and computer at a glance. Inspired by: [Zel94]

mum, from which the computer is orders
of magnitude away (Table 1.1). Addition-
ally, a computer is static - the brain as
a biological neural network can reorganize
itself during its "lifespan" and therefore is
able to learn, to compensate errors and so
forth.
Within this text I want to outline how
we can use the said characteristics of our
brain for a computer system.
So the study of artiﬁcial neural networks
is motivated by their similarity to success-
fully working biological systems, which - in
comparison to the overall system - consist
of very simple but numerous nerve cells
that work massively in parallel and (which
is probably one of the most signiﬁcant
aspects) have the capability to learn.
There is no need to explicitly program a
neural network. For instance, it can learn
from training samples or by means of en-
couragement - with a carrot and a stick,
so to speak (reinforcement learning).
One result from this learning procedure is
the capability of neural networks to gen-

eralize and associate data: After suc-
cessful training a neural network can ﬁnd
reasonable solutions for similar problems
of the same class that were not explicitly
trained. This in turn results in a high de-
gree of fault tolerance against noisy in-
put data.
Fault tolerance is closely related to biolog-
ical neural networks, in which this charac-
teristic is very distinct: As previously men-
tioned, a human has about 1011 neurons
that continuously reorganize themselves
or are reorganized by external inﬂuences
(about 105 neurons can be destroyed while
in a drunken stupor, some types of food
or environmental inﬂuences can also de-
stroy brain cells). Nevertheless, our cogni-
tive abilities are not signiﬁcantly aﬀected.
Thus, the brain is tolerant against internal
errors – and also against external errors,
for we can often read a really "dreadful
scrawl" although the individual letters are
nearly impossible to read.
Our modern technology, however, is not
automatically fault-tolerant. I have never
heard that someone forgot to install the

4

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

simple
but many
processing
units

n. network
capable
to learn

n. network
fault
tolerant

dkriesel.com

1.1 Why neural networks?

i.e.

hard disk controller into a computer and
therefore the graphics card automatically
took over its tasks,
removed con-
ductors and developed communication, so
that the system as a whole was aﬀected
by the missing component, but not com-
pletely destroyed.
A disadvantage of this distributed fault-
tolerant storage is certainly the fact that
we cannot realize at ﬁrst sight what a neu-
ral neutwork knows and performs or where
its faults lie. Usually, it is easier to per-
form such analyses for conventional algo-
rithms. Most often we can only trans-
fer knowledge into our neural network by
means of a learning procedure, which can
cause several errors and is not always easy
to manage.
Fault tolerance of data, on the other hand,
is already more sophisticated in state-of-
the-art technology: Let us compare a
record and a CD. If there is a scratch on a
record, the audio information on this spot
will be completely lost (you will hear a
pop) and then the music goes on. On a CD
the audio data are distributedly stored: A
scratch causes a blurry sound in its vicin-
ity, but the data stream remains largely
unaﬀected. The listener won’t notice any-
thing.
So let us summarize the main characteris-
tics we try to adapt from biology:

. Self-organization and learning capa-

bility,

. Generalization capability and
. Fault tolerance.

What types of neural networks particu-
larly develop what kinds of abilities and
can be used for what problem classes will
be discussed in the course of this work.

In the introductory chapter I want to
"The neural net-
clarify the following:
work" does not exist. There are diﬀer-
ent paradigms for neural networks, how
they are trained and where they are used.
My goal
is to introduce some of these
paradigms and supplement some remarks
for practical application.

We have already mentioned that our brain
works massively in parallel, in contrast to
the functioning of a computer, i.e. every
component is active at any time.
If we
want to state an argument for massive par-
allel processing, then the 100-step rule
can be cited.

1.1.1 The 100-step rule

Experiments showed that a human can
recognize the picture of a familiar object
or person in ≈ 0.1 seconds, which cor-
responds to a neuron switching time of
≈ 10−3 seconds in ≈ 100 discrete time
steps of parallel processing.

A computer following the von Neumann
architecture, however, can do practically
nothing in 100 time steps of sequential pro-
cessing, which are 100 assembler steps or
cycle steps.

Now we want to look at a simple applica-
tion example for a neural network.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

5

Important!

parallel
processing

Chapter 1 Introduction, motivation and history

dkriesel.com

put is called H for "halt signal"). There-
fore we need a mapping

f : R8 → B1,

that applies the input signals to a robot
activity.

1.1.2.1 The classical way

There are two ways of realizing this map-
ping. On the one hand, there is the clas-
sical way: We sit down and think for a
while, and ﬁnally the result is a circuit or
a small computer program which realizes
the mapping (this is easily possible, since
the example is very simple). After that
we refer to the technical reference of the
sensors, study their characteristic curve in
order to learn the values for the diﬀerent
obstacle distances, and embed these values
into the aforementioned set of rules. Such
procedures are applied in the classic artiﬁ-
cial intelligence, and if you know the exact
rules of a mapping algorithm, you are al-
ways well advised to follow this scheme.

1.1.2.2 The way of learning

On the other hand, more interesting and
more successful for many mappings and
problems that are hard to comprehend
straightaway is the way of learning: We
show diﬀerent possible situations to the
robot (ﬁg. 1.2 on page 8), – and the robot
shall learn on its own what to do in the
course of its robot life.
In this example the robot shall simply
learn when to stop. We ﬁrst treat the

Figure 1.1: A small robot with eight sensors
and two motors. The arrow indicates the driv-
ing direction.

1.1.2 Simple application examples

Let us assume that we have a small robot
as shown in ﬁg. 1.1. This robot has eight
distance sensors from which it extracts in-
put data: Three sensors are placed on the
front right, three on the front left, and two
on the back. Each sensor provides a real
numeric value at any time, that means we
are always receiving an input I ∈ R8.
Despite its two motors (which will be
needed later) the robot in our simple ex-
ample is not capable to do much: It shall
only drive on but stop when it might col-
lide with an obstacle. Thus, our output
is binary: H = 0 for "Everything is okay,
drive on" and H = 1 for "Stop" (The out-

6

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

1.1 Why neural networks?

Our example can be optionally expanded.
For the purpose of direction control
it
would be possible to control the motors
of our robot separately2, with the sensor
layout being the same. In this case we are
looking for a mapping

f : R8 → R2,

which gradually controls the two motors
by means of the sensor inputs and thus
cannot only, for example, stop the robot
but also lets it avoid obstacles. Here it
is more diﬃcult to analytically derive the
rules, and de facto a neural network would
be more appropriate.

Our goal is not to learn the samples by
heart, but to realize the principle behind
them: Ideally, the robot should apply the
neural network in any situation and be
able to avoid obstacles. In particular, the
robot should query the network continu-
ously and repeatedly while driving in order
to continously avoid obstacles. The result
is a constant cycle: The robot queries the
network. As a consequence, it will drive
in one direction, which changes the sen-
sors values. Again the robot queries the
network and changes its position, the sen-
sor values are changed once again, and so
on. It is obvious that this system can also
be adapted to dynamic, i.e changing, en-
vironments (e.g. the moving obstacles in
our example).

2 There is a robot called Khepera with more or less
similar characteristics. It is round-shaped, approx.
7 cm in diameter, has two motors with wheels
and various sensors. For more information I rec-
ommend to refer to the internet.

Figure 1.3: Initially, we regard the robot control
as a black box whose inner life is unknown. The
black box receives eight real sensor values and
maps these values to a binary output value.

neural network as a kind of black box
(ﬁg. 1.3). This means we do not know its
structure but just regard its behavior in
practice.

The situations in form of simply mea-
sured sensor values (e.g. placing the robot
in front of an obstacle, see illustration),
which we show to the robot and for which
we specify whether to drive on or to stop,
are called training samples. Thus, a train-
ing sample consists of an exemplary input
and a corresponding desired output. Now
the question is how to transfer this knowl-
edge, the information, into the neural net-
work.

The samples can be taught to a neural
network by using a simple learning pro-
cedure (a learning procedure is a simple
algorithm or a mathematical formula. If
we have done everything right and chosen
good samples, the neural network will gen-
eralize from these samples and ﬁnd a uni-
versal rule when it has to stop.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

7

Chapter 1 Introduction, motivation and history

dkriesel.com

Figure 1.2: The robot is positioned in a landscape that provides sensor values for diﬀerent situa-
tions. We add the desired output values H and so receive our learning samples. The directions in
which the sensors are oriented are exemplarily applied to two robots.

1.2 A brief history of neural

networks

The ﬁeld of neural networks has, like any
other ﬁeld of science, a long history of
development with many ups and downs,
as we will see soon. To continue the style
of my work I will not represent this history
in text form but more compact in form of a
timeline. Citations and bibliographical ref-
erences are added mainly for those topics
that will not be further discussed in this
text. Citations for keywords that will be
explained later are mentioned in the corre-
sponding chapters.
The history of neural networks begins in
the early 1940’s and thus nearly simulta-

neously with the history of programmable
electronic computers. The youth of this
ﬁeld of research, as with the ﬁeld of com-
puter science itself, can be easily recog-
nized due to the fact that many of the
cited persons are still with us.

1.2.1 The beginning

As soon as 1943 Warren McCulloch
and Walter Pitts introduced mod-
els of neurological networks, recre-
ated threshold switches based on neu-
rons and showed that even simple
networks of this kind are able to
calculate nearly any logic or arith-
metic function [MP43].
Further-

8

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

1.2 History of neural networks

Figure 1.4: Some institutions of the ﬁeld of neural networks. From left to right: John von Neu-
mann, Donald O. Hebb, Marvin Minsky, Bernard Widrow, Seymour Papert, Teuvo Kohonen, John
Hopﬁeld, "in the order of appearance" as far as possible.

more,
the ﬁrst computer precur-
sors ("electronic brains")were de-
veloped, among others supported by
Konrad Zuse, who was tired of cal-
culating ballistic trajectories by hand.
1947: Walter Pitts and Warren Mc-
Culloch indicated a practical ﬁeld
of application (which was not men-
tioned in their work from 1943),
namely the recognition of spacial pat-
terns by neural networks [PM47].

1949: Donald O. Hebb formulated the
classical Hebbian rule [Heb49] which
represents in its more generalized
form the basis of nearly all neural
learning procedures. The rule im-
plies that the connection between two
neurons is strengthened when both
neurons are active at the same time.
This change in strength is propor-
tional to the product of the two activ-
ities. Hebb could postulate this rule,
but due to the absence of neurological
research he was not able to verify it.
neuropsychologist Karl
Lashley defended the thesis that

1950: The

brain information storage is realized
as a distributed system. His thesis
was based on experiments on rats,
where only the extent but not the
location of the destroyed nerve tissue
inﬂuences the rats’ performance to
ﬁnd their way out of a labyrinth.

1.2.2 Golden age

1951: For his dissertation Marvin Min-
sky developed the neurocomputer
Snark, which has already been capa-
ble to adjust its weights3 automati-
cally. But it has never been practi-
cally implemented, since it is capable
to busily calculate, but nobody really
knows what it calculates.

1956: Well-known scientists and ambi-
tious students met at the Dart-
mouth Summer Research Project
and discussed, to put it crudely, how
to simulate a brain. Diﬀerences be-
tween top-down and bottom-up re-
search developed. While the early

3 We will learn soon what weights are.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

9

Chapter 1 Introduction, motivation and history

dkriesel.com

supporters of artiﬁcial intelligence
wanted to simulate capabilities by
means of software, supporters of neu-
ral networks wanted to achieve sys-
tem behavior by imitating the small-
est parts of the system – the neurons.
1957-1958: At the MIT, Frank Rosen-
blatt, Charles Wightman and
their coworkers developed the ﬁrst
successful neurocomputer, the Mark
I perceptron, which was capable to
recognize simple numerics by means
of a 20 × 20 pixel image sensor and
electromechanically worked with 512
motor driven potentiometers - each
potentiometer representing one vari-
able weight.

1959: Frank Rosenblatt described dif-
ferent versions of the perceptron, for-
mulated and veriﬁed his perceptron
convergence theorem. He described
neuron layers mimicking the retina,
threshold switches, and a learning
rule adjusting the connecting weights.
1960: Bernard Widrow and Mar-
cian E. Hoff introduced the ADA-
LINE (ADAptive LInear NEu-
ron) [WH60], a fast and precise
adaptive learning system being the
ﬁrst widely commercially used neu-
ral network:
It could be found in
nearly every analog telephone for real-
time adaptive echo ﬁltering and was
trained by menas of the Widrow-Hoﬀ
rule or delta rule. At that time Hoﬀ,
later co-founder of Intel Corporation,
was a PhD student of Widrow, who
himself is known as the inventor of

modern microprocessors. One advan-
tage the delta rule had over the origi-
nal perceptron learning algorithm was
its adaptivity:
If the diﬀerence be-
tween the actual output and the cor-
rect solution was large, the connect-
ing weights also changed in larger
steps – the smaller the steps, the
closer the target was. Disadvantage:
missapplication led to inﬁnitesimal
small steps close to the target. In the
following stagnation and out of fear
of scientiﬁc unpopularity of the neu-
ral networks ADALINE was renamed
in adaptive linear element – which
was undone again later on.

1961: Karl Steinbuch introduced tech-
nical realizations of associative mem-
ory, which can be seen as predecessors
of today’s neural associative mem-
ories [Ste61]. Additionally, he de-
scribed concepts for neural techniques
and analyzed their possibilities and
limits.

1965: In his book Learning Machines,
Nils Nilsson gave an overview of
the progress and works of this period
of neural network research.
It was
assumed that the basic principles of
self-learning and therefore, generally
speaking, "intelligent" systems had al-
ready been discovered. Today this as-
sumption seems to be an exorbitant
overestimation, but at that time it
provided for high popularity and suf-
ﬁcient research funds.

1969: Marvin Minsky and Seymour
Papert published a precise mathe-

10

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

development
accelerates

ﬁrst
spread
use

backprop
developed

dkriesel.com

1.2 History of neural networks

research
funds were
stopped

matical analysis of the perceptron
[MP69] to show that the perceptron
model was not capable of representing
many important problems (keywords:
XOR problem and linear separability),
and so put an end to overestimation,
popularity and research funds. The
implication that more powerful mod-
els would show exactly the same prob-
lems and the forecast that the entire
ﬁeld would be a research dead end re-
sulted in a nearly complete decline in
research funds for the next 15 years
– no matter how incorrect these fore-
casts were from today’s point of view.

1.2.3 Long silence and slow

reconstruction

The research funds were, as previously-
mentioned, extremely short. Everywhere
research went on, but there were neither
conferences nor other events and therefore
only few publications. This isolation of
individual researchers provided for many
independently developed neural network
paradigms: They researched, but there
was no discourse among them.
In spite of the poor appreciation the ﬁeld
received, the basic theories for the still
continuing renaissance were laid at that
time:
1972: Teuvo Kohonen introduced a
the linear associator,
model of
a model of an associative memory
[Koh72].
In the same year, such a
model was presented independently
and from a neurophysiologist’s point

of view by James A. Anderson
[And72].

1973: Christoph von der Malsburg
used a neuron model that was non-
linear and biologically more moti-
vated [vdM73].

1974: For his dissertation in Harvard
Paul Werbos developed a learning
procedure called backpropagation of
error [Wer74], but it was not until
one decade later that this procedure
reached today’s importance.

1976-1980 and thereafter: Stephen

instance

[Gro76])

Grossberg presented many papers
(for
in which
numerous neural models are analyzed
mathematically.
Furthermore, he
dedicated himself to the problem of
keeping a neural network capable
of
destroying
already learned associations. Under
cooperation of Gail Carpenter
led to models of adaptive
this
resonance theory (ART).

learning without

–

[Koh82, Koh98]

1982: Teuvo Kohonen described the
feature maps
self-organizing
(SOM)
also
known as Kohonen maps. He was
looking for the mechanisms involving
self-organization in the brain (He
knew that the information about the
creation of a being is stored in the
genome, which has, however, not
enough memory for a structure like
the brain. As a consequence, the
brain has to organize and create
itself for the most part).

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

11

Chapter 1 Introduction, motivation and history

dkriesel.com

John Hopfield also invented the
so-called Hopﬁeld networks [Hop82]
which are inspired by the laws of mag-
netism in physics. They were not
widely used in technical applications,
but the ﬁeld of neural networks slowly
regained importance.

1983: Fukushima, Miyake and Ito in-
troduced the neural model of the
Neocognitron which could recognize
handwritten characters [FMI83] and
was an extension of the Cognitron net-
work already developed in 1975.

1.2.4 Renaissance

Through the inﬂuence of John Hopfield,
who had personally convinced many re-
searchers of the importance of the ﬁeld,
and the wide publication of backpro-
pagation by Rumelhart, Hinton and
Williams, the ﬁeld of neural networks
slowly showed signs of upswing.
1985: John Hopfield published an arti-
cle describing a way of ﬁnding accept-
able solutions for the Travelling Sales-
man problem by using Hopﬁeld nets.
1986: The backpropagation of error learn-
ing procedure as a generalization of
the delta rule was separately devel-
oped and widely published by the Par-
allel Distributed Processing Group
[RHW86a]:
Non-linearly-separable
problems could be solved by multi-
layer perceptrons, and Marvin Min-
sky’s negative evaluations were dis-
proven at a single blow. At the same

time a certain kind of fatigue spread
in the ﬁeld of artiﬁcial
intelligence,
caused by a series of failures and un-
fulﬁlled hopes.

From this time on, the development of
the ﬁeld of research has almost been
explosive.
It can no longer be item-
ized, but some of its results will be
seen in the following.

Exercises

Exercise 1. Give one example for each
of the following topics:

. A book on neural networks or neuroin-

formatics,

. A collaborative group of a university

working with neural networks,

. A software tool realizing neural net-

works ("simulator"),

. A company using neural networks,

and

. A product or service being realized by

means of neural networks.

Exercise 2. Show at least four applica-
tions of technical neural networks:
two
from the ﬁeld of pattern recognition and
two from the ﬁeld of function approxima-
tion.
Exercise 3. Brieﬂy characterize the four
development phases of neural networks
and give expressive examples for each
phase.

12

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Renaissance

Chapter 2
Biological neural networks

How do biological systems solve problems? How does a system of neurons
work? How can we understand its functionality? What are diﬀerent quantities
of neurons able to do? Where in the nervous system does information
processing occur? A short biological overview of the complexity of simple
elements of neural information processing followed by some thoughts about
their simpliﬁcation in order to technically adapt them.

Before we begin to describe the technical
side of neural networks, it would be use-
ful to brieﬂy discuss the biology of neu-
ral networks and the cognition of living
organisms – the reader may skip the fol-
lowing chapter without missing any tech-
nical information. On the other hand I
recommend to read the said excursus if
you want to learn something about the
underlying neurophysiology and see that
our small approaches, the technical neural
networks, are only caricatures of nature
– and how powerful their natural counter-
parts must be when our small approaches
are already that eﬀective. Now we want
to take a brief look at the nervous system
of vertebrates: We will start with a very
rough granularity and then proceed with
the brain and up to the neural level. For
further reading I want to recommend the
books [CR00, KSJ00], which helped me a
lot during this chapter.

2.1 The vertebrate nervous

system

The entire information processing system,
i.e. the vertebrate nervous system, con-
sists of the central nervous system and the
peripheral nervous system, which is only
a ﬁrst and simple subdivision.
In real-
ity, such a rigid subdivision does not make
sense, but here it is helpful to outline the
information processing in a body.

2.1.1 Peripheral and central

nervous system

The peripheral nervous system (PNS)
comprises the nerves that are situated out-
side of the brain or the spinal cord. These
nerves form a branched and very dense net-
work throughout the whole body. The pe-

13

Chapter 2 Biological neural networks

dkriesel.com

ripheral nervous system includes, for ex-
ample, the spinal nerves which pass out
of the spinal cord (two within the level of
each vertebra of the spine) and supply ex-
tremities, neck and trunk, but also the cra-
nial nerves directly leading to the brain.
The central nervous system (CNS),
however, is the "main-frame" within the
vertebrate.
It is the place where infor-
mation received by the sense organs are
stored and managed. Furthermore, it con-
trols the inner processes in the body and,
last but not least, coordinates the mo-
tor functions of the organism. The ver-
tebrate central nervous system consists of
the brain and the spinal cord (Fig. 2.1).
However, we want to focus on the brain,
which can - for the purpose of simpliﬁca-
tion - be divided into four areas (Fig. 2.2
on the next page) to be discussed here.

2.1.2 The cerebrum is responsible

for abstract thinking
processes.

The cerebrum (telencephalon) is one of
the areas of the brain that changed most
during evolution. Along an axis, running
from the lateral face to the back of the
head, this area is divided into two hemi-
spheres, which are organized in a folded
structure. These cerebral hemispheres
are connected by one strong nerve cord
("bar") and several small ones. A large
number of neurons are located in the cere-
bral cortex (cortex) which is approx. 2-
4 cm thick and divided into diﬀerent cor-
tical ﬁelds, each having a speciﬁc task to Figure 2.1: Illustration of the central nervous

system with spinal cord and brain.

14

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.1 The vertebrate nervous system

and errors are continually corrected. For
this purpose, the cerebellum has direct
sensory information about muscle lengths
as well as acoustic and visual
informa-
tion. Furthermore, it also receives mes-
sages about more abstract motor signals
coming from the cerebrum.

In the human brain the cerebellum is con-
siderably smaller than the cerebrum, but
this is rather an exception. In many ver-
tebrates this ratio is less pronounced.
If
we take a look at vertebrate evolution, we
will notice that the cerebellum is not "too
small" but the cerebum is "too large" (at
least, it is the most highly developed struc-
ture in the vertebrate brain). The two re-
maining brain areas should also be brieﬂy
discussed: the diencephalon and the brain-
stem.

2.1.4 The diencephalon controls
fundamental physiological
processes

The interbrain (diencephalon) includes
parts of which only the thalamus will
be brieﬂy discussed: This part of the di-
encephalon mediates between sensory and
motor signals and the cerebrum. Particu-
larly, the thalamus decides which part of
the information is transferred to the cere-
brum, so that especially less important
sensory perceptions can be suppressed at
short notice to avoid overloads. Another
part of the diencephalon is the hypotha-
lamus, which controls a number of pro-
cesses within the body. The diencephalon

thalamus
ﬁlters
incoming
data

Figure 2.2: Illustration of the brain. The col-
ored areas of the brain are discussed in the text.
The more we turn from abstract information pro-
cessing to direct reﬂexive processing, the darker
the areas of the brain are colored.

fulﬁll. Primary cortical ﬁelds are re-
sponsible for processing qualitative infor-
mation, such as the management of diﬀer-
the visual cortex
ent perceptions (e.g.
is responsible for the management of vi-
sion). Association cortical ﬁelds, how-
ever, perform more abstract association
and thinking processes; they also contain
our memory.

2.1.3 The cerebellum controls and
coordinates motor functions

The cerebellum is located below the cere-
brum, therefore it is closer to the spinal
cord. Accordingly, it serves less abstract
functions with higher priority: Here, large
parts of motor coordination are performed,
i.e., balance and movements are controlled

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

15

Chapter 2 Biological neural networks

dkriesel.com

is also heavily involved in the human cir-
cadian rhythm ("internal clock") and the
sensation of pain.

2.1.5 The brainstem connects the

brain with the spinal cord and
controls reﬂexes.

In comparison with the diencephalon the
brainstem or the (truncus cerebri) re-
spectively is phylogenetically much older.
Roughly speaking,
it is the "extended
spinal cord" and thus the connection be-
tween brain and spinal cord. The brain-
stem can also be divided into diﬀerent ar-
eas, some of which will be exemplarily in-
troduced in this chapter. The functions
will be discussed from abstract functions
towards more fundamental ones. One im-
portant component is the pons (=bridge),
a kind of transit station for many nerve sig-
nals from brain to body and vice versa.

If the pons is damaged (e.g. by a cere-
bral infarct), then the result could be the
locked-in syndrome – a condition in
which a patient is "walled-in" within his
own body. He is conscious and aware
with no loss of cognitive function, but can-
not move or communicate by any means.
Only his senses of sight, hearing, smell and
taste are generally working perfectly nor-
mal. Locked-in patients may often be able
to communicate with others by blinking or
moving their eyes.

Furthermore, the brainstem is responsible
for many fundamental reﬂexes, such as the
blinking reﬂex or coughing.

All parts of the nervous system have one
thing in common: information processing.
This is accomplished by huge accumula-
tions of billions of very similar cells, whose
structure is very simple but which com-
municate continuously. Large groups of
these cells send coordinated signals and
thus reach the enormous information pro-
cessing capacity we are familiar with from
our brain. We will now leave the level of
brain areas and continue with the cellular
level of the body - the level of neurons.

2.2 Neurons are information

processing cells

Before specifying the functions and pro-
cesses within a neuron, we will give a
rough description of neuron functions: A
neuron is nothing more than a switch with
information input and output. The switch
will be activated if there are enough stim-
uli of other neurons hitting the informa-
tion input. Then, at the information out-
put, a pulse is sent to, for example, other
neurons.

2.2.1 Components of a neuron

Now we want to take a look at the com-
ponents of a neuron (Fig. 2.3 on the fac-
ing page). In doing so, we will follow the
way the electrical information takes within
the neuron. The dendrites of a neuron
receive the information by special connec-
tions, the synapses.

16

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.2 The neuron

Figure 2.3: Illustration of a biological neuron with the components discussed in this text.

2.2.1.1 Synapses weight the individual

parts of information

Incoming signals from other neurons or
cells are transferred to a neuron by special
connections, the synapses. Such connec-
tions can usually be found at the dendrites
of a neuron, sometimes also directly at the
soma. We distinguish between electrical
and chemical synapses.

The electrical synapse is the simpler
variant. An electrical signal received by
the synapse, i.e. coming from the presy-
naptic side, is directly transferred to the
postsynaptic nucleus of the cell. Thus,
there is a direct, strong, unadjustable
connection between the signal transmitter
and the signal receiver, which is, for exam-
ple, relevant to shortening reactions that
must be "hard coded" within a living or-
ganism.

The chemical synapse is the more dis-
tinctive variant. Here, the electrical cou-
pling of source and target does not take
place, the coupling is interrupted by the
synaptic cleft. This cleft electrically sep-
arates the presynaptic side from the post-
synaptic one. You might think that, never-
theless, the information has to ﬂow, so we
will discuss how this happens: It is not an
electrical, but a chemical process. On the
presynaptic side of the synaptic cleft the
electrical signal is converted into a chemi-
cal signal, a process induced by chemical
cues released there (the so-called neuro-
transmitters). These neurotransmitters
cross the synaptic cleft and transfer the
information into the nucleus of the cell
(this is a very simple explanation, but later
on we will see how this exactly works),
where it is reconverted into electrical in-
formation. The neurotransmitters are de-
graded very fast, so that it is possible to re-

electrical
synapse:
simple

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

17

cemical
synapse
is more
complex
but also
more
powerful

Chapter 2 Biological neural networks

dkriesel.com

lease very precise information pulses here,
too.
In spite of the more complex function-
ing, the chemical synapse has - compared
with the electrical synapse - utmost advan-
tages:
One-way connection: A

chemical
synapse is a one-way connection.
Due to the fact that there is no direct
electrical
connection between the
pre- and postsynaptic area, electrical
pulses
area
cannot ﬂash over to the presynaptic
area.

in the postsynaptic

Adjustability: There is a large number of
diﬀerent neurotransmitters that can
also be released in various quantities
in a synaptic cleft. There are neuro-
transmitters that stimulate the post-
synaptic cell nucleus, and others that
slow down such stimulation. Some
synapses transfer a strongly stimulat-
ing signal, some only weakly stimu-
lating ones. The adjustability varies
a lot, and one of the central points
in the examination of the learning
ability of the brain is, that here the
synapses are variable, too. That is,
over time they can form a stronger or
weaker connection.

2.2.1.2 Dendrites collect all parts of

information

Dendrites branch like trees from the cell
nucleus of the neuron (which is called
soma) and receive electrical signals from

many diﬀerent sources, which are then
transferred into the nucleus of the cell.
The amount of branching dendrites is also
called dendrite tree.

2.2.1.3 In the soma the weighted
information is accumulated

After the cell nucleus (soma) has re-
ceived a plenty of activating (=stimulat-
ing) and inhibiting (=diminishing) signals
by synapses or dendrites, the soma accu-
mulates these signals. As soon as the ac-
cumulated signal exceeds a certain value
(called threshold value), the cell nucleus
of the neuron activates an electrical pulse
which then is transmitted to the neurons
connected to the current one.

2.2.1.4 The axon transfers outgoing

pulses

The pulse is transferred to other neurons
by means of the axon. The axon is a
long, slender extension of the soma.
In
an extreme case, an axon can stretch up
to one meter (e.g. within the spinal cord).
The axon is electrically isolated in order
to achieve a better conduction of the elec-
trical signal (we will return to this point
later on) and it leads to dendrites, which
transfer the information to, for example,
other neurons. So now we are back at the
beginning of our description of the neuron
elements. An axon can, however, transfer
information to other kinds of cells in order
to control them.

18

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.2 The neuron

2.2.2 Electrochemical processes in

the neuron and its
components

After having pursued the path of an elec-
trical signal from the dendrites via the
synapses to the nucleus of the cell and
from there via the axon into other den-
drites, we now want to take a small step
from biology towards technology. In doing
so, a simpliﬁed introduction of the electro-
chemical information processing should be
provided.

2.2.2.1 Neurons maintain electrical

membrane potential

One fundamental aspect is the fact that
compared to their environment the neu-
rons show a diﬀerence in electrical charge,
a potential.
In the membrane (=enve-
lope) of the neuron the charge is diﬀerent
from the charge on the outside. This dif-
ference in charge is a central concept that
is important to understand the processes
within the neuron. The diﬀerence is called
membrane potential. The membrane
potential, i.e., the diﬀerence in charge, is
created by several kinds of charged atoms
(ions), whose concentration varies within
and outside of the neuron. If we penetrate
the membrane from the inside outwards,
we will ﬁnd certain kinds of ions more of-
ten or less often than on the inside. This
descent or ascent of concentration is called
a concentration gradient.
Let us ﬁrst take a look at the membrane
potential in the resting state of the neu-

ron, i.e., we assume that no electrical sig-
nals are received from the outside. In this
case, the membrane potential is −70 mV.
Since we have learned that this potential
depends on the concentration gradients of
various ions, there is of course the central
question of how to maintain these concen-
tration gradients: Normally, diﬀusion pre-
dominates and therefore each ion is eager
to decrease concentration gradients and
to spread out evenly.
If this happens,
the membrane potential will move towards
0 mV, so ﬁnally there would be no mem-
brane potential anymore. Thus, the neu-
ron actively maintains its membrane po-
tential to be able to process information.
How does this work?
The secret is the membrane itself, which is
permeable to some ions, but not for others.
To maintain the potential, various mecha-
nisms are in progress at the same time:
Concentration gradient: As

If

the

described
above the ions try to be as uniformly
distributed as possible.
the
concentration of an ion is higher on
the inside of the neuron than on
it will try to diﬀuse
the outside,
and vice versa.
to
outside
charged ion K+
The positively
(potassium) occurs very frequently
within the neuron but less frequently
outside of the neuron, and therefore
it slowly diﬀuses out through the
neuron’s membrane.
But another
group of negative ions, collectively
called A−, remains within the neuron
since the membrane is not permeable
to them. Thus, the inside of the
neuron becomes negatively charged.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

19

Chapter 2 Biological neural networks

dkriesel.com

Negative A ions remain, positive K
ions disappear, and so the inside of
the cell becomes more negative. The
result is another gradient.

Electrical Gradient: The electrical gradi-
ent acts contrary to the concentration
gradient. The intracellular charge is
now very strong, therefore it attracts
positive ions: K+ wants to get back
into the cell.

If these two gradients were now left alone,
they would eventually balance out, reach
a steady state, and a membrane poten-
tial of −85 mV would develop. But we
want to achieve a resting membrane po-
tential of −70 mV, thus there seem to ex-
ist some disturbances which prevent this.
Furthermore, there is another important
ion, Na+ (sodium), for which the mem-
brane is not very permeable but which,
however, slowly pours through the mem-
brane into the cell. As a result, the sodium
is driven into the cell all the more: On the
one hand, there is less sodium within the
neuron than outside the neuron. On the
other hand, sodium is positively charged
but the interior of the cell has negative
charge, which is a second reason for the
sodium wanting to get into the cell.
Due to the low diﬀusion of sodium into the
cell the intracellular sodium concentration
increases. But at the same time the inside
of the cell becomes less negative, so that
K+ pours in more slowly (we can see that
this is a complex mechanism where every-
thing is inﬂuenced by everything). The
sodium shifts the intracellular equilibrium
from negative to less negative, compared

with its environment. But even with these
two ions a standstill with all gradients be-
ing balanced out could still be achieved.
Now the last piece of the puzzle gets into
the game: a "pump" (or rather, the protein
ATP) actively transports ions against the
direction they actually want to take!
Sodium is actively pumped out of the cell,
although it tries to get into the cell
along the concentration gradient and
the electrical gradient.

Potassium, however, diﬀuses strongly out
of the cell, but is actively pumped
back into it.

For this reason the pump is also called
sodium-potassium pump. The pump
maintains the concentration gradient for
the sodium as well as for the potassium,
so that some sort of steady state equilib-
rium is created and ﬁnally the resting po-
tential is −70 mV as observed. All in all
the membrane potential is maintained by
the fact that the membrane is imperme-
able to some ions and other ions are ac-
tively pumped against the concentration
and electrical gradients. Now that we
know that each neuron has a membrane
potential we want to observe how a neu-
ron receives and transmits signals.

2.2.2.2 The neuron is activated by
changes in the membrane
potential

Above we have learned that sodium and
potassium can diﬀuse through the mem-
brane - sodium slowly, potassium faster.

20

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.2 The neuron

They move through channels within the
membrane,
the sodium and potassium
channels.
In addition to these per-
manently open channels responsible for
diﬀusion and balanced by the sodium-
potassium pump, there also exist channels
that are not always open but which only
response "if required". Since the opening
of these channels changes the concentra-
tion of ions within and outside of the mem-
brane, it also changes the membrane po-
tential.
These controllable channels are opened as
soon as the accumulated received stimulus
exceeds a certain threshold. For example,
stimuli can be received from other neurons
or have other causes. There exist, for ex-
ample, specialized forms of neurons, the
sensory cells, for which a light incidence
could be such a stimulus.
If the incom-
ing amount of light exceeds the threshold,
controllable channels are opened.
The said threshold (the threshold poten-
tial) lies at about −55 mV. As soon as the
received stimuli reach this value, the neu-
ron is activated and an electrical signal,
an action potential, is initiated. Then
this signal is transmitted to the cells con-
nected to the observed neuron, i.e.
the
cells "listen" to the neuron. Now we want
to take a closer look at the diﬀerent stages
of the action potential (Fig. 2.4 on the next
page):
Resting state: Only

permanently
open sodium and potassium channels
are permeable.
The membrane
potential is at −70 mV and actively
kept there by the neuron.

the

Stimulus up to the threshold: A stimu-
lus opens channels so that sodium
can pour in. The intracellular charge
becomes more positive. As soon as
the membrane potential exceeds the
threshold of −55 mV, the action po-
tential is initiated by the opening of
many sodium channels.

Depolarization: Sodium is pouring in. Re-
member: Sodium wants to pour into
the cell because there is a lower in-
tracellular than extracellular concen-
tration of sodium. Additionally, the
cell is dominated by a negative en-
vironment which attracts the posi-
tive sodium ions. This massive in-
ﬂux of sodium drastically increases
the membrane potential - up to ap-
prox. +30 mV - which is the electrical
pulse, i.e., the action potential.

Repolarization: Now the sodium channels
are closed and the potassium channels
are opened. The positively charged
ions want to leave the positive inte-
rior of the cell. Additionally, the intra-
cellular concentration is much higher
than the extracellular one, which in-
creases the eﬄux of ions even more.
The interior of the cell is once again
more negatively charged than the ex-
terior.

Hyperpolarization: Sodium as well as
potassium channels are closed again.
At ﬁrst the membrane potential
is
slightly more negative than the rest-
ing potential. This is due to the
fact that the potassium channels close
more slowly. As a result, (positively

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

21

Chapter 2 Biological neural networks

dkriesel.com

Figure 2.4: Initiation of action potential over time.

22

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.2 The neuron

charged) potassium eﬀuses because of
its lower extracellular concentration.
After a refractory period of 1 − 2
ms the resting state is re-established
so that the neuron can react to newly
applied stimuli with an action poten-
tial.
In simple terms, the refractory
period is a mandatory break a neu-
ron has to take in order to regenerate.
The shorter this break is, the more
often a neuron can ﬁre per time.

Then the resulting pulse is transmitted by
the axon.

2.2.2.3 In the axon a pulse is

conducted in a saltatory way

We have already learned that the axon
is used to transmit the action potential
across long distances (remember: You will
ﬁnd an illustration of a neuron including
an axon in Fig. 2.3 on page 17). The axon
is a long, slender extension of the soma.
In vertebrates it is normally coated by a
myelin sheath that consists of Schwann
cells (in the PNS) or oligodendrocytes
(in the CNS) 1, which insulate the axon
very well from electrical activity. At a dis-
tance of 0.1−2mm there are gaps between
these cells, the so-called nodes of Ran-
vier. The said gaps appear where one in-
sulate cell ends and the next one begins.
It is obvious that at such a node the axon
is less insulated.
1 Schwann cells as well as oligodendrocytes are vari-
eties of the glial cells. There are about 50 times
more glial cells than neurons: They surround the
neurons (glia = glue), insulate them from each
other, provide energy, etc.

Now you may assume that these less in-
sulated nodes are a disadvantage of the
axon - however, they are not. At the
nodes, mass can be transferred between
the intracellular and extracellular area, a
transfer that is impossible at those parts
of the axon which are situated between
two nodes (internodes) and therefore in-
sulated by the myelin sheath. This mass
transfer permits the generation of signals
similar to the generation of the action po-
tential within the soma. The action po-
tential is transferred as follows:
It does
not continuously travel along the axon but
jumps from node to node. Thus, a series
of depolarization travels along the nodes of
Ranvier. One action potential initiates the
next one, and mostly even several nodes
are active at the same time here. The
pulse "jumping" from node to node is re-
sponsible for the name of this pulse con-
ductor: saltatory conductor.
Obviously, the pulse will move faster if its
jumps are larger. Axons with large in-
ternodes (2 mm) achieve a signal disper-
sion of approx.
180 meters per second.
However, the internodes cannot grow in-
deﬁnitely, since the action potential to be
transferred would fade too much until it
reaches the next node. So the nodes have
a task, too: to constantly amplify the sig-
nal. The cells receiving the action poten-
tial are attached to the end of the axon –
often connected by dendrites and synapses.
As already indicated above, the action po-
tentials are not only generated by informa-
tion received by the dendrites from other
neurons.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

23

Chapter 2 Biological neural networks

dkriesel.com

2.3 Receptor cells are
modiﬁed neurons

2.3.1 There are diﬀerent receptor

cells for various types of
perceptions

Action potentials can also be generated by
sensory information an organism receives
from its environment through its sensory
cells. Specialized receptor cells are able
to perceive speciﬁc stimulus energies such
as light, temperature and sound or the ex-
istence of certain molecules (like, for exam-
ple, the sense of smell). This is working
because of the fact that these sensory cells
are actually modiﬁed neurons. They do
not receive electrical signals via dendrites
but the existence of the stimulus being
speciﬁc for the receptor cell ensures that
the ion channels open and an action po-
tential is developed. This process of trans-
forming stimulus energy into changes in
the membrane potential is called sensory
transduction. Usually, the stimulus en-
ergy itself is too weak to directly cause
nerve signals. Therefore, the signals are
ampliﬁed either during transduction or by
means of the stimulus-conducting ap-
paratus. The resulting action potential
can be processed by other neurons and is
then transmitted into the thalamus, which
is, as we have already learned, a gateway
to the cerebral cortex and therefore can re-
ject sensory impressions according to cur-
rent relevance and thus prevent an abun-
dance of information to be managed.

Primary receptors transmit their pulses
directly to the nervous system. A good
example for this is the sense of pain.
Here, the stimulus intensity is propor-
tional to the amplitude of the action po-
tential. Technically, this is an amplitude
modulation.

Secondary receptors, however, continu-
ously transmit pulses. These pulses con-
trol the amount of the related neurotrans-
mitter, which is responsible for transfer-
ring the stimulus. The stimulus in turn
controls the frequency of the action poten-
tial of the receiving neuron. This process
is a frequency modulation, an encoding of
the stimulus, which allows to better per-
ceive the increase and decrease of a stimu-
lus.

There can be individual receptor cells or
cells forming complex sensory organs (e.g.
eyes or ears). They can receive stimuli
within the body (by means of the intero-
ceptors) as well as stimuli outside of the
body (by means of the exteroceptors).

After having outlined how information is
received from the environment, it will be
interesting to look at how the information
is processed.

24

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.3 Receptor cells

2.3.2 Information is processed on

every level of the nervous
system

There is no reason to believe that all re-
ceived information is transmitted to the
brain and processed there, and that the
brain ensures that it is "output" in the
form of motor pulses (the only thing an
organism can actually do within its envi-
ronment is to move). The information pro-
cessing is entirely decentralized. In order
to illustrate this principle, we want to take
a look at some examples, which leads us
again from the abstract to the fundamen-
tal in our hierarchy of information process-
ing.

. It is certain that information is pro-
cessed in the cerebrum, which is the
most developed natural information
processing structure.

. The midbrain and the thalamus,
which serves – as we have already
learned – as a gateway to the cere-
bral cortex, are situated much lower
in the hierarchy. The ﬁltering of in-
formation with respect to the current
relevance executed by the midbrain
is a very important method of infor-
mation processing, too. But even the
thalamus does not receive any prepro-
cessed stimuli from the outside. Now
let us continue with the lowest level,
the sensory cells.

. On the lowest level, i.e. at the recep-
tor cells, the information is not only
received and transferred but directly
processed. One of the main aspects of

this subject is to prevent the transmis-
sion of "continuous stimuli" to the cen-
tral nervous system because of sen-
sory adaptation: Due to continu-
ous stimulation many receptor cells
automatically become insensitive to
stimuli. Thus, receptor cells are not
a direct mapping of speciﬁc stimu-
lus energy onto action potentials but
depend on the past. Other sensors
change their sensitivity according to
the situation: There are taste recep-
tors which respond more or less to the
same stimulus according to the nutri-
tional condition of the organism.

. Even before a stimulus reaches the
receptor cells, information processing
can already be executed by a preced-
ing signal carrying apparatus, for ex-
ample in the form of ampliﬁcation:
The external and the internal ear
have a speciﬁc shape to amplify the
sound, which also allows – in asso-
ciation with the sensory cells of the
sense of hearing – the sensory stim-
ulus only to increase logarithmically
with the intensity of the heard sig-
nal. On closer examination, this is
necessary, since the sound pressure of
the signals for which the ear is con-
structed can vary over a wide expo-
nential range. Here, a logarithmic
measurement is an advantage. Firstly,
an overload is prevented and secondly,
the fact that the intensity measure-
ment of intensive signals will be less
precise, doesn’t matter as well. If a jet
ﬁghter is starting next to you, small

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

25

Chapter 2 Biological neural networks

dkriesel.com

changes in the noise level can be ig-
nored.

Just to get a feeling for sensory organs
and information processing in the organ-
ism, we will brieﬂy describe "usual" light
sensing organs, i.e. organs often found in
nature. For the third light sensing organ
described below, the single lens eye, we
will discuss the information processing in
the eye.

2.3.3 An outline of common light

sensing organs

For many organisms it turned out to be ex-
tremely useful to be able to perceive elec-
tromagnetic radiation in certain regions of
the spectrum. Consequently, sensory or-
gans have been developed which can de-
tect such electromagnetic radiation and
the wavelength range of the radiation per-
ceivable by the human eye is called visible
range or simply light. The diﬀerent wave-
lengths of this electromagnetic radiation
are perceived by the human eye as diﬀer-
ent colors. The visible range of the elec-
tromagnetic radiation is diﬀerent for each
organism. Some organisms cannot see the
colors (=wavelength ranges) we can see,
others can even perceive additional wave-
length ranges (e.g. in the UV range). Be-
fore we begin with the human being – in
order to get a broader knowledge of the
sense of sight– we brieﬂy want to look at
two organs of sight which, from an evolu-
tionary point of view, exist much longer
than the human.

2.3.3.1 Compound eyes and pinhole

eyes only provide high temporal
or spatial resolution

Let us ﬁrst take a look at the so-called
compound eye (Fig. 2.5 on the next
page), which is, for example, common in
insects and crustaceans. The compound
eye consists of a great number of small,
individual eyes. If we look at the com-
pound eye from the outside, the individ-
ual eyes are clearly visible and arranged
in a hexagonal pattern. Each individual
eye has its own nerve ﬁber which is con-
nected to the insect brain. Since the indi-
vidual eyes can be distinguished, it is ob-
vious that the number of pixels, i.e. the
spatial resolution, of compound eyes must
be very low and the image is blurred. But
compound eyes have advantages, too, espe-
cially for fast-ﬂying insects. Certain com-
pound eyes process more than 300 images
per second (to the human eye, however,
movies with 25 images per second appear
as a ﬂuent motion).

Compound eye:
high temp.,
low
spatial
resolution

Pinhole eyes are, for example, found in
octopus species and work – as you can
guess – similar to a pinhole camera. A pinhole
pinhole eye has a very small opening for
camera:
high spat.,
light entry, which projects a sharp image
low
onto the sensory cells behind. Thus, the
temporal
spatial resolution is much higher than in
resolution
the compound eye. But due to the very
small opening for light entry the resulting
image is less bright.

26

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.3 Receptor cells

2.3.3.3 The retina does not only

receive information but is also
responsible for information
processing

The light signals falling on the eye are
received by the retina and directly pre-
processed by several layers of information-
processing cells. We want to brieﬂy dis-
cuss the diﬀerent steps of this informa-
tion processing and in doing so, we follow
the way of the information carried by the
light:
Photoreceptors receive the light signal
und cause action potentials (there
are diﬀerent receptors for diﬀerent
color components and light intensi-
ties). These receptors are the real
light-receiving part of the retina and
they are sensitive to such an extent
that only one single photon falling
on the retina can cause an action po-
tential. Then several photoreceptors
transmit their signals to one single

bipolar cell. This means that here the in-
formation has already been summa-
rized. Finally, the now transformed
light signal travels from several bipo-
lar cells 2 into

ganglion cells. Various bipolar cells can
transmit their information to one gan-
glion cell. The higher the number
of photoreceptors that aﬀect the gan-
glion cell, the larger the ﬁeld of per-
ception, the receptive ﬁeld, which
covers the ganglions – and the less
2 There are diﬀerent kinds of bipolar cells, as well,

but to discuss all of them would go too far.

Figure 2.5: Compound eye of a robber ﬂy

2.3.3.2 Single lens eyes combine the
advantages of the other two
eye types, but they are more
complex

The light sensing organ common in verte-
brates is the single lense eye. The result-
ing image is a sharp, high-resolution image
of the environment at high or variable light
intensity. On the other hand it is more
complex. Similar to the pinhole eye the
light enters through an opening (pupil)
and is projected onto a layer of sensory
cells in the eye. (retina). But in contrast
to the pinhole eye, the size of the pupil can
be adapted to the lighting conditions (by
means of the iris muscle, which expands
or contracts the pupil). These diﬀerences
in pupil dilation require to actively focus
the image. Therefore, the single lens eye
contains an additional adjustable lens.

Single
lense eye:
high temp.
and spat.
resolution

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

27

Chapter 2 Biological neural networks

dkriesel.com

sharp is the image in the area of this
ganglion cell. So the information is
already reduced directly in the retina
and the overall image is, for exam-
ple, blurred in the peripheral ﬁeld
of vision. So far, we have learned
about the information processing in
the retina only as a top-down struc-
ture. Now we want to take a look at
the

horizontal and amacrine cells. These

and

compressing

cells are not connected from the
front backwards but laterally. They
allow the light signals to inﬂuence
themselves laterally directly during
the information processing in the
retina – a much more powerful
information processing
method of
than
blurring.
When the horizontal cells are excited
by a photoreceptor, they are able to
excite other nearby photoreceptors
and at the same time inhibit more
distant bipolar cells and receptors.
This ensures the clear perception of
outlines and bright points. Amacrine
cells can further
intensify certain
stimuli by distributing information
from bipolar cells to several ganglion
cells or by inhibiting ganglions.

These ﬁrst steps of transmitting visual in-
formation to the brain show that informa-
tion is processed from the ﬁrst moment the
information is received and, on the other
hand, is processed in parallel within mil-
lions of information-processing cells. The
system’s power and resistance to errors
is based upon this massive division of
work.

2.4 The amount of neurons in

living organisms at
diﬀerent stages of
development

An overview of diﬀerent organisms and
their neural capacity (in large part from
[RD05]):
302 neurons are required by the nervous
system of a nematode worm, which
serves as a popular model organism
in biology. Nematodes live in the soil
and feed on bacteria.

104 neurons make an ant (To simplify
matters we neglect the fact that some
ant species also can have more or less
eﬃcient nervous systems). Due to the
use of diﬀerent attractants and odors,
ants are able to engage in complex
social behavior and form huge states
with millions of individuals. If you re-
gard such an ant state as an individ-
ual, it has a cognitive capacity similar
to a chimpanzee or even a human.

With 105 neurons the nervous system of
a ﬂy can be constructed. A ﬂy can
evade an object in real-time in three-
dimensional space, it can land upon
the ceiling upside down, has a consid-
erable sensory system because of com-
pound eyes, vibrissae, nerves at the
end of its legs and much more. Thus,
a ﬂy has considerable diﬀerential and
integral calculus in high dimensions
implemented "in hardware". We all
know that a ﬂy is not easy to catch.
Of course, the bodily functions are

28

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.4 The amount of neurons in living organisms

also controlled by neurons, but these
should be ignored here.

With 0.8 · 106 neurons we have enough
cerebral matter to create a honeybee.
Honeybees build colonies and have
amazing capabilities in the ﬁeld of
aerial reconnaissance and navigation.
4 · 106 neurons result in a mouse, and
here the world of vertebrates already
begins.

1.5 · 107 neurons are suﬃcient for a rat,
an animal which is denounced as be-
ing extremely intelligent and are of-
ten used to participate in a variety
of intelligence tests representative for
the animal world. Rats have an ex-
traordinary sense of smell and orien-
tation, and they also show social be-
havior. The brain of a frog can be
positioned within the same dimension.
The frog has a complex build with
many functions, it can swim and has
evolved complex behavior. A frog
can continuously target the said ﬂy
by means of his eyes while jumping
in three-dimensional space and and
catch it with its tongue with consid-
erable probability.

5 · 107 neurons make a bat. The bat can
navigate in total darkness through a
room, exact up to several centime-
ters, by only using their sense of hear-
ing. It uses acoustic signals to localize
self-camouﬂaging insects (e.g.
some
moths have a certain wing structure
that reﬂects less sound waves and the
echo will be small) and also eats its
prey while ﬂying.

1.6 · 108 neurons are required by the
brain of a dog, companion of man for
ages. Now take a look at another pop-
ular companion of man:

3 · 108 neurons can be found in a cat,
which is about twice as much as in
a dog. We know that cats are very
elegant, patient carnivores that can
show a variety of behaviors. By the
way, an octopus can be positioned
within the same magnitude. Only
very few people know that, for exam-
ple, in labyrinth orientation the octo-
pus is vastly superior to the rat.

For 6 · 109 neurons you already get a
chimpanzee, one of the animals being
very similar to the human.

1011 neurons make a human. Usually,
the human has considerable cognitive
capabilities, is able to speak, to ab-
stract, to remember and to use tools
as well as the knowledge of other hu-
mans to develop advanced technolo-
gies and manifold social structures.
With 2 · 1011 neurons there are nervous
systems having more neurons than
the human nervous system. Here we
should mention elephants and certain
whale species.

Our state-of-the-art computers are not
able to keep up with the aforementioned
processing power of a ﬂy. Recent research
results suggest that the processes in ner-
vous systems might be vastly more pow-
erful than people thought until not long
ago: Michaeva et al. describe a separate,

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

29

Chapter 2 Biological neural networks

dkriesel.com

synapse-integrated information way of in-
formation processing [MBW+10]. Poster-
ity will show if they are right.

therefore it is a vector.
In nature a
neuron receives pulses of 103 to 104
other neurons on average.

2.5 Transition to technical

neurons: neural networks
are a caricature of biology

How do we change from biological neural
networks to the technical ones? Through
radical simpliﬁcation.
I want to brieﬂy
summarize the conclusions relevant for the
technical part:
We have learned that the biological neu-
rons are linked to each other in a weighted
way and when stimulated they electrically
transmit their signal via the axon. From
the axon they are not directly transferred
to the succeeding neurons, but they ﬁrst
have to cross the synaptic cleft where the
signal is changed again by variable chem-
ical processes.
In the receiving neuron
the various inputs that have been post-
processed in the synaptic cleft are summa-
rized or accumulated to one single pulse.
Depending on how the neuron is stimu-
lated by the cumulated input, the neuron
itself emits a pulse or not – thus, the out-
put is non-linear and not proportional to
the cumulated input. Our brief summary
corresponds exactly with the few elements
of biological neural networks we want to
take over into the technical approxima-
tion:

Scalar output: The output of a neuron is
a scalar, which means that the neu-
ron only consists of one component.
Several scalar outputs in turn form
the vectorial input of another neuron.
This particularly means that some-
where in the neuron the various input
components have to be summarized in
such a way that only one component
remains.

Synapses change input: In technical neu-
ral networks the inputs are prepro-
cessed, too. They are multiplied by
a number (the weight) – they are
weighted. The set of such weights rep-
resents the information storage of a
neural network – in both biological
original and technical adaptation.

Accumulating the inputs: In biology, the
inputs are summarized to a pulse ac-
cording to the chemical change, i.e.,
they are accumulated – on the techni-
cal side this is often realized by the
weighted sum, which we will get to
know later on. This means that after
accumulation we continue with only
one value, a scalar, instead of a vec-
tor.

Non-linear characteristic: The input of
our technical neurons is also not pro-
portional to the output.

Vectorial input: The input of technical
neurons consists of many components,

Adjustable weights: The weights weight-
ing the inputs are variable, similar to

30

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

2.5 Technical neurons as caricature of biology

bits of information. Naïvely calculated:
How much storage capacity does the brain
have? Note: The information which neu-
ron is connected to which other neuron is
also important.

the chemical processes at the synap-
tic cleft. This adds a great dynamic
to the network because a large part of
the "knowledge" of a neural network is
saved in the weights and in the form
and power of the chemical processes
in a synaptic cleft.

So our current, only casually formulated
and very simple neuron model receives a
vectorial input

~x,

with components xi. These are multiplied
by the appropriate weights wi and accumu-
lated:

X

i

wixi.

aforementioned

term is

The
weighted sum.
mapping f deﬁnes the scalar output y:

called
Then the nonlinear

 X

!

y = f

wixi

.

i

After this transition we now want to spec-
ify more precisely our neuron model and
add some odds and ends. Afterwards we
will take a look at how the weights can be
adjusted.

Exercises

Exercise 4.
It is estimated that a hu-
man brain consists of approx. 1011 nerve
cells, each of which has about 103 to 104
synapses. For this exercise we assume 103
synapses per neuron. Let us further as-
sume that a single synapse could save 4

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

31

Chapter 3
Components of artiﬁcial neural networks

Formal deﬁnitions and colloquial explanations of the components that realize
the technical adaptations of biological neural networks. Initial descriptions of
how to combine these components into a neural network.

This chapter contains the formal deﬁni-
tions for most of the neural network com-
ponents used later in the text. After this
chapter you will be able to read the indi-
vidual chapters of this work without hav-
ing to know the preceding ones (although
this would be useful).

3.1 The concept of time in

neural networks

certain point in time, the notation will be,
for example, netj(t − 1) or oi(t).

From a biological point of view this is, of
course, not very plausible (in the human
brain a neuron does not wait for another
one), but it signiﬁcantly simpliﬁes the im-
plementation.

In some deﬁnitions of this text we use the
term time or the number of cycles of the
neural network, respectively. Time is di-
vided into discrete time steps:

discrete
time steps Deﬁnition 3.1 (The concept of time).
The current time (present time) is referred
to as (t), the next time step as (t + 1),
the preceding one as (t − 1). All other
time steps are referred to analogously. If in
the following chapters several mathemati-
cal variables (e.g. netj or oi) refer to a

(t)(cid:73)

3.2 Components of neural

networks

A technical neural network consists of sim-
ple processing units, the neurons, and
directed, weighted connections between
those neurons. Here, the strength of a
connection (or the connecting weight) be-

33

Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

n. network
= neurons
+ weighted
connection

wi,j(cid:73)

1.

tween two neurons i and j is referred to as
wi,j
Deﬁnition 3.2 (Neural network). A
neural network is a sorted triple
(N, V, w) with two sets N, V and a func-
tion w, where N is the set of neurons and
V a set {(i, j)|i, j ∈ N} whose elements are
called connections between neuron i and
neuron j. The function w : V → R deﬁnes
the weights, where w((i, j)), the weight of
the connection between neuron i and neu-
ron j, is shortened to wi,j . Depending on
the point of view it is either undeﬁned or
0 for connections that do not exist in the
network.

SNIPE: In Snipe, an instance of the class
NeuralNetworkDescriptor is created in
the ﬁrst place.
The descriptor object
roughly outlines a class of neural networks,
e.g.
it deﬁnes the number of neuron lay-
ers in a neural network. In a second step,
the descriptor object is used to instantiate
an arbitrary number of NeuralNetwork ob-
jects. To get started with Snipe program-
ming, the documentations of exactly these
two classes are – in that order – the right
thing to read. The presented layout involv-
ing descriptor and dependent neural net-
works is very reasonable from the imple-
mentation point of view, because it is en-
ables to create and maintain general param-
eters of even very large sets of similar (but
not neccessarily equal) networks.

W(cid:73)

So the weights can be implemented in a
square weight matrix W or, optionally,
in a weight vector W with the row num-
1 Note: In some of the cited literature i and j could
be interchanged in wi,j. Here, a consistent stan-
dard does not exist. But in this text I try to use
the notation I found more frequently and in the
more signiﬁcant citations.

ber of the matrix indicating where the con-
nection begins, and the column number of
the matrix indicating, which neuron is the
target. Indeed, in this case the numeric
0 marks a non-existing connection. This
matrix representation is also called Hin-
ton diagram2.
The neurons and connections comprise the
following components and variables (I’m
following the path of the data within a
neuron, which is according to ﬁg. 3.1 on
the facing page in top-down direction):

3.2.1 Connections carry information

that is processed by neurons

Data are transferred between neurons via
connections with the connecting weight be-
ing either excitatory or inhibitory. The
deﬁnition of connections has already been
included in the deﬁnition of the neural net-
work.

be

set

weights
the method

Connection

SNIPE:
can
NeuralNetwork.setSynapse.

using

3.2.2 The propagation function
converts vector inputs to
scalar network inputs

Looking at a neuron j, we will usually ﬁnd
a lot of neurons with a connection to j, i.e.
which transfer their output to j.
2 Note that, here again, in some of the cited liter-
ature axes and rows could be interchanged. The
published literature is not consistent here, as well.

34

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

3.2 Components of neural networks

inputs

For a neuron j the propagation func-
tion receives the outputs oi1, . . . , oin of
other neurons i1, i2, . . . , in (which are con-
nected to j), and transforms them in con- manages
sideration of the connecting weights wi,j
into the network input netj that can be fur-
ther processed by the activation function.
Thus, the network input is the result of
the propagation function.
Deﬁnition
func-
tion
and
Let
I = {i1, i2, . . . , in} be the set of neurons,
such that ∀z ∈ {1, . . . , n} : ∃wiz,j. Then
the network input of j, called netj,
is
calculated by the propagation function
fprop as follows:

3.3
network

(Propagation

input).

netj = fprop(oi1, . . . , oin, wi1,j, . . . , win,j)
(3.1)

Here the weighted sum is very popular:
The multiplication of the output of each
neuron i by wi,j, and the summation of
the results:

netj =X

i∈I

(oi · wi,j)

(3.2)

Figure 3.1: Data processing of a neuron. The
activation function of a neuron implies the
threshold value.

SNIPE: The propagation function in
Snipe was implemented using the weighted
sum.

3.2.3 The activation is the

"switching status" of a
neuron

Based on the model of nature every neuron
is, to a certain extent, at all times active,
excited or whatever you will call it. The

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

35

 Propagierungsfunktion (oft gewichtete Summe, verarbeitet Eingaben zur Netzeingabe) Ausgabefunktion (Erzeugt aus Aktivierung die Ausgabe, ist oft Identität) Aktivierungsfunktion (Erzeugt aus Netzeingabe und alter Aktivierung die neue Aktivierung)Eingaben anderer Neuronen NetzeingabeAktivierungAusgabe zu anderen Neuronen Propagation function (often weighted sum, transforms outputs of other neurons to net input) Output function (often identity function, transforms activation to output for other neurons) Activation function (Transforms net input and sometimes old activation to new activation)Data Input of other Neurons Network InputActivationData Output to other Neurons Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

How active
is a
neuron?

reactions of the neurons to the input val-
ues depend on this activation state. The
activation state indicates the extent of a
neuron’s activation and is often shortly re-
ferred to as activation. Its formal deﬁni-
tion is included in the following deﬁnition
of the activation function. But generally,
it can be deﬁned as follows:
Deﬁnition 3.4 (Activation state / activa-
tion in general). Let j be a neuron. The
activation state aj, in short activation, is
explicitly assigned to j, indicates the ex-
tent of the neuron’s activity and results
from the activation function.

SNIPE: It is possible to get and set activa-
tion states of neurons by using the meth-
ods getActivation or setActivation in
the class NeuralNetwork.

3.2.5 The activation function

determines the activation of a
neuron dependent on network
input and treshold value

At a certain time – as we have already
learned – the activation aj of a neuron j
depends on the previous3 activation state
of the neuron and the external input.
Deﬁnition 3.6 (Activation function and
Activation). Let j be a neuron. The ac-
tivation function is deﬁned as
aj(t) = fact(netj(t), aj(t − 1), Θj).
It transforms the network input netj, (cid:74)fact
as well as the previous activation state
aj(t − 1) into a new activation state aj(t),
with the threshold value Θ playing an im-
portant role, as already mentioned.

(3.3)

calculates
activation

3.2.4 Neurons get activated if the
network input exceeds their
treshold value

Near the threshold value, the activation
function of a neuron reacts particularly
sensitive. From the biological point of
view the threshold value represents the
threshold at which a neuron starts ﬁr-
ing. The threshold value is also mostly
included in the deﬁnition of the activation
function, but generally the deﬁnition is the
following:
Deﬁnition 3.5 (Threshold value in gen-
eral). Let j be a neuron. The threshold
value Θj is uniquely assigned to j and
marks the position of the maximum gradi-
ent value of the activation function.

Unlike the other variables within the neu-
ral network (particularly unlike the ones
deﬁned so far) the activation function is
often deﬁned globally for all neurons or
at least for a set of neurons and only the
threshold values are diﬀerent for each neu-
ron. We should also keep in mind that
the threshold values can be changed, for
example by a learning procedure. So it
can in particular become necessary to re-
late the threshold value to the time and to
write, for instance Θj as Θj(t) (but for rea-
sons of clarity, I omitted this here). The
activation function is also called transfer
function.
3 The previous activation is not always relevant for
the current – we will see examples for both vari-
ants.

highest
point of
sensation

Θ(cid:73)

36

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

3.2 Components of neural networks

SNIPE: In Snipe, activation functions are
generalized to neuron behaviors.
Such
behaviors can represent just normal acti-
vation functions, or even incorporate in-
ternal states and dynamics. Correspond-
ing parts of Snipe can be found in the
package neuronbehavior, which also con-
tains some of the activation functions in-
troduced in the next section. The inter-
face NeuronBehavior allows for implemen-
tation of custom behaviors. Objects that
inherit from this interface can be passed to
a NeuralNetworkDescriptor instance.
It
is possible to deﬁne individual behaviors
per neuron layer.

3.2.6 Common activation functions

The simplest activation function is the bi-
nary threshold function (ﬁg. 3.2 on the
next page), which can only take on two val-
ues (also referred to as Heaviside func-
tion).
If the input is above a certain
threshold, the function changes from one
value to another, but otherwise remains
constant. This implies that the function
is not diﬀerentiable at the threshold and
for the rest the derivative is 0. Due to
this fact, backpropagation learning, for ex-
ample, is impossible (as we will see later).
Also very popular is the Fermi function
or logistic function (ﬁg. 3.2)

1

1 + e−x ,

(3.4)

which maps to the range of values of (0, 1)
and the hyperbolic tangent (ﬁg. 3.2)
which maps to (−1, 1). Both functions are
diﬀerentiable. The Fermi function can be
expanded by a temperature parameter
T into the form

T(cid:73)

1

1 + e −x

T

.

(3.5)

The smaller this parameter, the more does
it compress the function on the x axis.
Thus, one can arbitrarily approximate the
Heaviside function. Incidentally, there ex-
ist activation functions which are not ex-
plicitly deﬁned but depend on the input ac-
cording to a random distribution (stochas-
tic activation function).
A alternative to the hypberbolic tangent
that is really worth mentioning was sug-
gested by Anguita et al. [APZ93], who
have been tired of the slowness of the work-
stations back in 1993. Thinking about
how to make neural network propagations
faster, they quickly identiﬁed the approx-
imation of the e-function used in the hy-
perbolic tangent as one of the causes of
slowness. Consequently, they "engineered"
an approximation to the hyperbolic tan-
gent, just using two parabola pieces and
two half-lines. At the price of delivering
a slightly smaller range of values than the
hyperbolic tangent ([−0.96016; 0.96016] in-
stead of [−1; 1]), dependent on what CPU
one uses, it can be calculated 200 times
faster because it just needs two multipli-
cations and one addition. What’s more,
it has some other advantages that will be
mentioned later.

SNIPE: The activation functions intro-
duced here are implemented within the
classes Fermi and TangensHyperbolicus,
both of which are located in the package
neuronbehavior. The fast hyperbolic tan-
gent approximation is located within the
class TangensHyperbolicusAnguita.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

37

Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

3.2.7 An output function may be

used to process the activation
once again

The output function of a neuron j cal-
culates the values which are transferred to
the other neurons connected to j. More
formally:
Deﬁnition 3.7 (Output function). Let j
be a neuron. The output function

fout(aj) = oj

(3.6)

informs
other
neurons

calculates the output value oj of the neu- (cid:74)fout
ron j from its activation state aj.

Generally, the output function is deﬁned
globally, too. Often this function is the
identity, i.e. the activation aj is directly
output4:

fout(aj) = aj, so oj = aj

(3.7)

Unless explicitly speciﬁed diﬀerently, we
will use the identity as output function
within this text.

3.2.8 Learning strategies adjust a

network to ﬁt our needs

Since we will address this subject later in
detail and at ﬁrst want to get to know the
principles of neural network structures, I
will only provide a brief and general deﬁ-
nition here:
4 Other deﬁnitions of output functions may be use-
ful if the range of values of the activation function
is not suﬃcient.

Figure 3.2: Various popular activation func-
tions, from top to bottom: Heaviside or binary
threshold function, Fermi function, hyperbolic
tangent. The Fermi function was expanded by
a temperature parameter. The original Fermi
function is represented by dark colors, the tem-
perature parameters of the modiﬁed Fermi func-
2, 1
tions are, ordered ascending by steepness, 1
5,
10 und 1
1
25.

38

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

−1−0.5 0 0.5 1−4−2 0 2 4f(x)xHeaviside Function 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4f(x)xFermi Function with Temperature Parameter−1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4tanh(x)xHyperbolic Tangentnetwork of
layers

dkriesel.com

3.3 Network topologies

Deﬁnition 3.8 (General learning rule).
The learning strategy is an algorithm
that can be used to change and thereby
train the neural network, so that the net-
work produces a desired output for a given
input.

3.3 Network topologies

After we have become acquainted with the
composition of the elements of a neural
network, I want to give an overview of
the usual topologies (= designs) of neural
networks, i.e. to construct networks con-
sisting of these elements. Every topology
described in this text is illustrated by a
map and its Hinton diagram so that the
reader can immediately see the character-
istics and apply them to other networks.

In the Hinton diagram the dotted weights
are represented by light grey ﬁelds, the
solid ones by dark grey ﬁelds. The input
and output arrows, which were added for
reasons of clarity, cannot be found in the
Hinton diagram. In order to clarify that
the connections are between the line neu-
rons and the column neurons, I have in-
serted the small arrow (cid:31) in the upper-left
cell.

SNIPE: Snipe is designed for realization
of arbitrary network topologies.
In this
respect, Snipe deﬁnes diﬀerent kinds of
synapses depending on their source and
their target. Any kind of synapse can sep-
arately be allowed or forbidden for a set of
networks using the setAllowed methods in
a NeuralNetworkDescriptor instance.

3.3.1 Feedforward networks consist

of layers and connections
towards each following layer

Feedforward In this text feedforward net-
works (ﬁg. 3.3 on the following page) are
the networks we will ﬁrst explore (even if
we will use diﬀerent topologies later). The
neurons are grouped in the following lay-
ers: One input layer, n hidden pro-
cessing layers (invisible from the out-
side, that’s why the neurons are also re-
ferred to as hidden neurons) and one out-
put layer. In a feedforward network each
neuron in one layer has only directed con-
nections to the neurons of the next layer
(towards the output layer). In ﬁg. 3.3 on
the next page the connections permitted
for a feedforward network are represented
by solid lines. We will often be confronted
with feedforward networks in which every
neuron i is connected to all neurons of the
next layer (these layers are called com-
pletely linked). To prevent naming con-
ﬂicts the output neurons are often referred
to as Ω.

Deﬁnition 3.9 (Feedforward network).
The neuron layers of a feedforward net-
work (ﬁg. 3.3 on the following page) are
clearly separated: One input layer, one
output layer and one or more processing
layers which are invisible from the outside
(also called hidden layers). Connections
are only permitted to neurons of the fol-
lowing layer.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

39

Shortcuts
skip
layers

Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

3.3.1.1 Shortcut connections skip layers

i1

i2

GFED
@ABC
GFED
@ABC
*UUUUUUUUUUUUUUUUUUUUUUUUUU
tiiiiiiiiiiiiiiiiiiiiiiiiii
 AAAAAAAAA
 AAAAAAAAA
~}}}}}}}}}
~}}}}}}}}}
GFED
@ABCh1
GFED
@ABCh2
GFED
@ABCh3
*UUUUUUUUUUUUUUUUUUUUUUUUUU
tiiiiiiiiiiiiiiiiiiiiiiiiii
 AAAAAAAAA
 AAAAAAAAA
~}}}}}}}}}
~}}}}}}}}}
GFED
@ABCΩ2
GFED
@ABCΩ1

Some feedforward networks permit the so-
called shortcut connections (ﬁg. 3.4 on
the next page): connections that skip one
or more levels. These connections may
only be directed towards the output layer,
too.
Deﬁnition 3.10 (Feedforward network
with shortcut connections). Similar to the
feedforward network, but the connections
may not only be directed towards the next
layer but also towards any other subse-
quent layer.

3.3.2 Recurrent networks have
inﬂuence on themselves

i1 i2 h1 h2 h3 Ω1 Ω2

(cid:31)
i1
i2
h1
h2
h3
Ω1
Ω2

Figure 3.3: A feedforward network with three
layers: two input neurons, three hidden neurons
and two output neurons. Characteristic for the
Hinton diagram of completely linked feedforward
networks is the formation of blocks above the
diagonal.

Recurrence is deﬁned as the process of a
neuron inﬂuencing itself by any means or
by any connection. Recurrent networks do
not always have explicitly deﬁned input or
output neurons. Therefore in the ﬁgures
I omitted all markings that concern this
matter and only numbered the neurons.

3.3.2.1 Direct recurrences start and

end at the same neuron

Some networks allow for neurons to be
connected to themselves, which is called
direct recurrence (or sometimes self-
recurrence (ﬁg. 3.5 on the facing page).
As a result, neurons inhibit and therefore
strengthen themselves in order to reach
their activation limits.

40

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)





~
 
*
t
~
 
 
*
~
 
~
t




dkriesel.com

3.3 Network topologies

?>=<
89:;1
?>=<
89:;6

1

?>=<
89:;4

2

3

4

5

?>=<
89:;2
?>=<
89:;7

6

?>=<
89:;5

7

?>=<
89:;3

(cid:31)
1
2
3
4
5
6
7

GFED
@ABCh3

Figure 3.5: A network similar to a feedforward
network with directly recurrent neurons. The di-
rect recurrences are represented by solid lines and
exactly correspond to the diagonal in the Hinton
diagram matrix.

i1

GFED
@ABC
GFED
@ABCΩ1

GFED
@ABCh2

i2

GFED
@ABC
GFED
@ABCΩ2

i1 i2 h1 h2 h3 Ω1 Ω2

GFED
@ABCh1

(cid:31)
i1
i2
h1
h2
h3
Ω1
Ω2

Figure 3.4: A feedforward network with short-
cut connections, which are represented by solid
lines. On the right side of the feedforward blocks
new connections have been added to the Hinton
diagram.

Deﬁnition 3.11 (Direct recurrence).
Now we expand the feedforward network
by connecting a neuron j to itself, with the
weights of these connections being referred
to as wj,j. In other words: the diagonal
of the weight matrix W may be diﬀerent
from 0.

neurons
inﬂuence
themselves

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

41







+
+
~
~
 
 
*
*
s
s


t
t
~
~
 
 
 
 
*
*
~
~
 
 
~
~
t
t




v
v
 
 


)
)
v
v
u
u
 
 


v
v


)
)
v
v
 
 


v
v
 
 
u
u
v
v
v
v
Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

3.3.2.2 Indirect recurrences can

inﬂuence their starting neuron
only by making detours

If connections are allowed towards the in-
put layer, they will be called indirect re-
currences. Then a neuron j can use in-
direct forwards connections to inﬂuence it-
self, for example, by inﬂuencing the neu-
rons of the next layer and the neurons of
this next layer inﬂuencing j (ﬁg. 3.6).
Deﬁnition 3.12 (Indirect recurrence).
Again our network is based on a feedfor-
ward network, now with additional connec-
tions between neurons and their preceding
layer being allowed. Therefore, below the
diagonal of W is diﬀerent from 0.

3.3.2.3 Lateral recurrences connect

neurons within one layer

Connections between neurons within one
layer are called lateral recurrences
(ﬁg. 3.7 on the facing page). Here, each
neuron often inhibits the other neurons of
the layer and strengthens itself. As a re-
sult only the strongest neuron becomes ac-
tive (winner-takes-all scheme).
Deﬁnition 3.13 (Lateral recurrence). A
laterally recurrent network permits con-
nections within one layer.

3.3.3 Completely linked networks

allow any possible connection

Completely linked networks permit connec-
tions between all neurons, except for direct

?>=<
89:;1
?>=<
89:;6

1

?>=<
89:;4

2

3

4

5

?>=<
89:;2
?>=<
89:;7

6

?>=<
89:;5

7

?>=<
89:;3

(cid:31)
1
2
3
4
5
6
7

Figure 3.6: A network similar to a feedforward
network with indirectly recurrent neurons. The
indirect recurrences are represented by solid lines.
As we can see, connections to the preceding lay-
ers can exist here, too. The ﬁelds that are sym-
metric to the feedforward blocks in the Hinton
diagram are now occupied.

42

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

 
 


)
)
u
u
 
 


8
8
2
2


)
)
X
X
8
8
 
 


X
X
g
g
 
 
u
u
X
X
8
8
2
2
g
g
X
X
8
8
dkriesel.com

3.4 The bias neuron

?>=<
89:;1
?>=<
89:;6

1

?>=<
89:;4

2

3

4

5

?>=<
89:;2
?>=<
89:;7

6

?>=<
89:;5

7

?>=<
89:;3

(cid:31)
1
2
3
4
5
6
7

Figure 3.7: A network similar to a feedforward
network with laterally recurrent neurons. The
direct recurrences are represented by solid lines.
Here, recurrences only exist within the layer.
In the Hinton diagram, ﬁlled squares are con-
centrated around the diagonal in the height of
the feedforward blocks, but the diagonal is left
uncovered.

recurrences. Furthermore, the connections
must be symmetric (ﬁg. 3.8 on the next
page). A popular example are the self-
organizing maps, which will be introduced
in chapter 10.
Deﬁnition 3.14 (Complete interconnec-
tion). In this case, every neuron is always
allowed to be connected to every other neu-
ron – but as a result every neuron can
become an input neuron. Therefore, di-
rect recurrences normally cannot be ap-
plied here and clearly deﬁned layers do not
longer exist. Thus, the matrix W may be
unequal to 0 everywhere, except along its
diagonal.

3.4 The bias neuron is a

technical trick to consider
threshold values as
connection weights

By now we know that in many network
paradigms neurons have a threshold value
that indicates when a neuron becomes ac-
tive. Thus, the threshold value is an
activation function parameter of a neu-
ron. From the biological point of view
this sounds most plausible, but it is com-
plicated to access the activation function
at runtime in order to train the threshold
value.
But threshold values Θj1, . . . , Θjn for neu-
rons j1, j2, . . . , jn can also be realized as
connecting weight of a continuously ﬁr-
ing neuron: For this purpose an addi-
tional bias neuron whose output value

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

43

+
+
k
k
 
 


)
)
u
u
 
 


+
+
k
k
*
*
j
j


)
)
+
+
k
k
 
 


 
 
u
u
+
+
k
k
Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

?>=<
89:;1 i
?>=<
89:;25
)TTTTTTTTTTTTTTTTTTTTTTT
ujjjjjjjjjjjjjjjjjjjjjjj O
@         
>>>>>>>>>
>>>>>>>>>
          
?>=<
89:;3 i
89:;54
?>=<
89:;4
?>=<
)TTTTTTTTTTTTTTTTTTTTTTTo
ujjjjjjjjjjjjjjjjjjjjjjj/
@         
^>>>>>>>>>
^>>>>>>>>>
          
?>=<
89:;6
?>=<
89:;7/

7

1

4

2

3

5

6

(cid:31)
1
2
3
4
5
6
7

Figure 3.8: A completely linked network with
symmetric connections and without direct recur-
rences. In the Hinton diagram only the diagonal
is left blank.

is always 1 is integrated in the network
and connected to the neurons j1, j2, . . . , jn.
These new connections get the weights
−Θj1, . . . ,−Θjn, i.e. they get the negative
threshold values.
Deﬁnition 3.15. A bias neuron is a
neuron whose output value is always 1 and
which is represented by

It is used to represent neuron biases as con-
nection weights, which enables any weight-
training algorithm to train the biases at
the same time.

GFED
@ABC

BIAS .

Then the threshold value of the neurons
j1, j2, . . . , jn is set to 0. Now the thresh-
old values are implemented as connection
weights (ﬁg. 3.9 on page 46) and can di-
rectly be trained together with the con-
nection weights, which considerably facil-
itates the learning process.
In other words: Instead of including the
threshold value in the activation function,
it is now included in the propagation func-
tion. Or even shorter: The threshold value
is subtracted from the network input, i.e.
it is part of the network input. More for-
mally:
Let j1, j2, . . . , jn be neurons with thresh-
old values Θj1, . . . , Θjn. By inserting a
bias neuron whose output value is always
1, generating connections between the said
bias neuron and the neurons j1, j2, . . . , jn
and
connections
wBIAS,j1, . . . , wBIAS,jnwith −Θj1, . . . ,−Θjn,
we can set Θj1 = . . . = Θjn = 0 and

weighting

these

bias neuron
replaces
thresh. value
with weights

44

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

i


i
i
)
O
O


o
o
/
/
^
^

5
u
O


@
@
 
^
^

i
)
o
/
/
 
 
@
4
j
j
5
5
u
/
o
o
@
@
 




5
5
 
 
@


^
/
o
o


^
dkriesel.com

3.6 Orders of activation

receive an equivalent neural network
whose threshold values are realized by
connection weights.

Undoubtedly, the advantage of the bias
neuron is the fact that it is much easier
to implement it in the network. One dis-
advantage is that the representation of the
network already becomes quite ugly with
only a few neurons, let alone with a great
number of them. By the way, a bias neu-
ron is often referred to as on neuron.

From now on, the bias neuron is omit-
ted for clarity in the following illustrations,
but we know that it exists and that the
threshold values can simply be treated as
weights because of it.

SNIPE: In Snipe, a bias neuron was imple-
mented instead of neuron-individual biases.
The neuron index of the bias neuron is 0.

||c,x||
Gauß

WVUT
PQRS
WVUT
PQRSΣ

Tanh

GFED
@ABC(cid:30)
WVUT
PQRSΣ

Fermi

(cid:30)

ONML
HIJKΣ
ONML
HIJKΣ

fact

L|H

WVUT
PQRSΣ
GFED
@ABC

BIAS

Figure 3.10: Diﬀerent types of neurons that will
appear in the following text.

3.6 Take care of the order in
which neuron activations
are calculated

For a neural network it is very important
in which order the individual neurons re-
ceive and process the input and output the
results. Here, we distinguish two model
classes:

3.5 Representing neurons

3.6.1 Synchronous activation

We have already seen that we can either
write its name or its threshold value into
a neuron. Another useful representation,
which we will use several times in the
following, is to illustrate neurons accord-
ing to their type of data processing. See
ﬁg. 3.10 for some examples without fur-
ther explanation – the diﬀerent types of
neurons are explained as soon as we need
them.

All neurons change their values syn-
chronously, i.e. they simultaneously cal-
culate network inputs, activation and out-
put, and pass them on. Synchronous ac-
tivation corresponds closest to its biolog-
ical counterpart, but it is – if to be im-
plemented in hardware – only useful on
certain parallel computers and especially
not for feedforward networks. This order
of activation is the most generic and can
be used with networks of arbitrary topol-
ogy.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

45

Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

GFED
@ABCΘ1
 BBBBBBBBB
~|||||||||
GFED
@ABCΘ2
GFED
@ABCΘ3

BIAS

GFED
@ABC

−Θ1
TTTTTTTTTT
AAAA
−Θ2
 AAAA

−Θ3

*TTTTTTTTTT?>=<
89:;0

?>=<
89:;0

?>=<
89:;0

Figure 3.9: Two equivalent neural networks, one without bias neuron on the left, one with bias
neuron on the right. The neuron threshold values can be found in the neurons, the connecting
weights at the connections. Furthermore, I omitted the weights of the already existing connections
(represented by dotted lines on the right side).

biologically
plausible

Deﬁnition 3.16 (Synchronous activa-
tion). All neurons of a network calculate
network inputs at the same time by means
of the propagation function, activation by
means of the activation function and out-
put by means of the output function. Af-
ter that the activation cycle is complete.

SNIPE: When implementing in software,
one could model this very general activa-
tion order by every time step calculating
and caching every single network input,
and after that calculating all activations.
This is exactly how it is done in Snipe, be-
cause Snipe has to be able to realize arbi-
trary network topologies.

3.6.2 Asynchronous activation

of time. For this, there exist diﬀerent or-
ders, some of which I want to introduce in
the following:

easier to
implement

3.6.2.1 Random order

Deﬁnition 3.17 (Random order of acti-
vation). With random order of acti-
vation a neuron i is randomly chosen and
its neti, ai and oi are updated. For n neu-
rons a cycle is the n-fold execution of this
step. Obviously, some neurons are repeat-
edly updated during one cycle, and others,
however, not at all.

Here, the neurons do not change their val-
ues simultaneously but at diﬀerent points

Apparently, this order of activation is not
always useful.

46

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)



 
~






/
/
 
*


 
 




dkriesel.com

3.6 Orders of activation

3.6.2.2 Random permutation

With random permutation each neuron
is chosen exactly once, but in random or-
der, during one cycle.
Deﬁnition 3.18 (Random permutation).
Initially, a permutation of the neurons is
calculated randomly and therefore deﬁnes
the order of activation. Then the neurons
are successively processed in this order.

This order of activation is as well used
rarely because ﬁrstly, the order is gener-
ally useless and, secondly, it is very time-
consuming to compute a new permutation
for every cycle. A Hopﬁeld network (chap-
ter 8) is a topology nominally having a
random or a randomly permuted order of
activation. But note that in practice, for
the previously mentioned reasons, a ﬁxed
order of activation is preferred.
For all orders either the previous neuron
activations at time t or, if already existing,
the neuron activations at time t + 1, for
which we are calculating the activations,
can be taken as a starting point.

3.6.2.3 Topological order

often very
useful

Deﬁnition 3.19 (Topological activation).
With topological order of activation
the neurons are updated during one cycle
and according to a ﬁxed order. The order
is deﬁned by the network topology.

since otherwise there is no order of activa-
tion. Thus, in feedforward networks (for
which the procedure is very reasonable)
the input neurons would be updated ﬁrst,
then the inner neurons and ﬁnally the out-
put neurons. This may save us a lot of
time: Given a synchronous activation or-
der, a feedforward network with n layers
of neurons would need n full propagation
cycles in order to enable input data to
have inﬂuence on the output of the net-
work. Given the topological activation or-
der, we just need one single propagation.
However, not every network topology al-
lows for ﬁnding a special activation order
that enables saving time.

feature

SNIPE: Those who want to use Snipe
for
implementing feedforward networks
may save some calculation time by us-
ing the
fastprop (mentioned
within the documentation of the class
NeuralNetworkDescriptor. Once fastprop
is enabled, it will cause the data propaga-
tion to be carried out in a slightly diﬀerent
way. In the standard mode, all net inputs
are calculated ﬁrst, followed by all activa-
tions. In the fastprop mode, for every neu-
ron, the activation is calculated right after
the net input. The neuron values are calcu-
lated in ascending neuron index order. The
neuron numbers are ascending from input
to output layer, which provides us with the
perfect topological activation order for feed-
forward networks.

3.6.2.4 Fixed orders of activation

during implementation

This procedure can only be considered for
non-cyclic,
i.e. non-recurrent, networks,

Obviously, ﬁxed orders of activation
can be deﬁned as well. Therefore, when
implementing,
feedforward

for instance,

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

47

Chapter 3 Components of artiﬁcial neural networks (fundamental)

dkriesel.com

networks it is very popular to determine
the order of activation once according to
the topology and to use this order without
further veriﬁcation at runtime. But this is
not necessarily useful for networks that are
capable to change their topology.

outputs y1, y2, . . . , ym. They are regarded
as output vector y = (y1, y2, . . . , ym).
Thus, the output dimension is referred
to as m. Data is output by a neural net- (cid:74)m
work by the output neurons adopting the
components of the output vector in their
output values.

3.7 Communication with the
outside world: input and
output of data in and
from neural networks

Finally, let us take a look at the fact that,
of course, many types of neural networks
permit the input of data. Then these data
are processed and can produce output.
Let us, for example, regard the feedfor-
ward network shown in ﬁg. 3.3 on page 40:
It has two input neurons and two output
neurons, which means that it also has two
numerical inputs x1, x2 and outputs y1, y2.
As a simpliﬁcation we summarize the in-
put and output components for n input
or output neurons within the vectors x =
(x1, x2, . . . , xn) and y = (y1, y2, . . . , yn).
Deﬁnition 3.20 (Input vector). A net-
work with n input neurons needs n inputs
x1, x2, . . . , xn. They are considered as in-
put vector x = (x1, x2, . . . , xn). As a
consequence, the input dimension is re-
ferred to as n. Data is put into a neural
network by using the components of the in-
put vector as network inputs of the input
neurons.
Deﬁnition 3.21 (Output vector). A net-
work with m output neurons provides m

SNIPE: In order to propagate data through
a NeuralNetwork-instance, the propagate
method is used. It receives the input vector
as array of doubles, and returns the output
vector in the same way.

Now we have deﬁned and closely examined
the basic components of neural networks –
without having seen a network in action.
But ﬁrst we will continue with theoretical
explanations and generally describe how a
neural network could learn.

Exercises

Exercise 5. Would it be useful (from
your point of view) to insert one bias neu-
ron in each layer of a layer-based network,
such as a feedforward network? Discuss
this in relation to the representation and
implementation of the network. Will the
result of the network change?
Exercise 6. Show for the Fermi function
f(x) as well as for the hyperbolic tangent
tanh(x), that their derivatives can be ex-
pressed by the respective functions them-
selves so that the two statements
1. f0(x) = f(x) · (1 − f(x)) and
2. tanh0(x) = 1 − tanh2(x)

x(cid:73)

n(cid:73)

y(cid:73)

48

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

are true.

3.7 Input and output of data

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

49

Chapter 4
Fundamentals on learning and training
samples

Approaches and thoughts of how to teach machines. Should neural networks
be corrected? Should they only be encouraged? Or should they even learn
without any help? Thoughts about what we want to change during the
learning procedure and how we will change it, about the measurement of
errors and when we have learned enough.

As written above, the most interesting
characteristic of neural networks is their
capability to familiarize with problems
by means of training and, after suﬃcient
training, to be able to solve unknown prob-
lems of the same class. This approach is re-
ferred to as generalization. Before intro-
ducing speciﬁc learning procedures, I want
to propose some basic principles about the
learning procedure in this chapter.

4.1 There are diﬀerent

paradigms of learning

Learning is a comprehensive term. A
learning system changes itself in order to
adapt to e.g. environmental changes. A
neural network could learn from many
things but, of course, there will always be

In
the question of how to implement it.
principle, a neural network changes when
its components are changing, as we have
learned above. Theoretically, a neural net-
work could learn by

1. developing new connections,
2. deleting existing connections,
3. changing connecting weights,
4. changing the threshold values of neu-

rons,

5. varying one or more of the three neu-
ron functions (remember: activation
function, propagation function and
output function),

6. developing new neurons, or
7. deleting existing neurons (and so, of

course, existing connections).

51

From what
do we learn?

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

As mentioned above, we assume the
change in weight to be the most common
procedure. Furthermore, deletion of con-
nections can be realized by additionally
taking care that a connection is no longer
trained when it is set to 0. Moreover, we
can develop further connections by setting
a non-existing connection (with the value
0 in the connection matrix) to a value dif-
ferent from 0. As for the modiﬁcation of
threshold values I refer to the possibility
of implementing them as weights (section
3.4). Thus, we perform any of the ﬁrst four
of the learning paradigms by just training
synaptic weights.
The change of neuron functions is diﬃcult
to implement, not very intuitive and not
exactly biologically motivated. Therefore
it is not very popular and I will omit this
topic here. The possibilities to develop or
delete neurons do not only provide well
adjusted weights during the training of a
neural network, but also optimize the net-
work topology. Thus, they attract a grow-
ing interest and are often realized by using
evolutionary procedures. But, since we ac-
cept that a large part of learning possibil-
ities can already be covered by changes in
weight, they are also not the subject mat-
ter of this text (however, it is planned to
extend the text towards those aspects of
training).

of

allow for

the
changes

class
SNIPE: Methods
in
NeuralNetwork
connection weights, and addition and
removal of both connections and neurons.
Methods
in NeuralNetworkDescriptor
enable the change of neuron behaviors,
respectively
per
layer.

activation

functions

Learning
by changes
in weight

Thus, we let our neural network learn by
modifying the connecting weights accord-
ing to rules that can be formulated as al-
gorithms. Therefore a learning procedure
is always an algorithm that can easily be
implemented by means of a programming
language. Later in the text I will assume
that the deﬁnition of the term desired out-
put which is worth learning is known (and
I will deﬁne formally what a training pat-
tern is) and that we have a training set
of learning samples. Let a training set be
deﬁned as follows:
Deﬁnition 4.1 (Training set). A train- (cid:74)P
ing set (named P) is a set of training
patterns, which we use to train our neu-
ral net.

I will now introduce the three essential
paradigms of learning by presenting the
diﬀerences between their regarding train-
ing sets.

4.1.1 Unsupervised learning

provides input patterns to the
network, but no learning aides

Unsupervised learning is the biologi-
cally most plausible method, but is not
suitable for all problems. Only the in-
put patterns are given; the network tries
to identify similar patterns and to classify
them into similar categories.
Deﬁnition 4.2 (Unsupervised learning).
The training set only consists of input
patterns, the network tries by itself to de-
tect similarities and to generate pattern
classes.

52

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

4.1 Paradigms of learning

Here I want to refer again to the popu-
lar example of Kohonen’s self-organising
maps (chapter 10).

4.1.2 Reinforcement learning

methods provide feedback to
the network, whether it
behaves well or bad

In reinforcement learning the network
receives a logical or a real value after
completion of a sequence, which deﬁnes
whether the result is right or wrong. Intu-
itively it is clear that this procedure should
be more eﬀective than unsupervised learn-
ing since the network receives speciﬁc crit-
era for problem-solving.
Deﬁnition 4.3 (Reinforcement learning).
The training set consists of input patterns,
after completion of a sequence a value is re-
turned to the network indicating whether
the result was right or wrong and, possibly,
how right or wrong it was.

4.1.3 Supervised learning methods

provide training patterns
together with appropriate
desired outputs

In supervised learning the training set
consists of input patterns as well as their
correct results in the form of the precise ac-
tivation of all output neurons. Thus, for
each training set that is fed into the net-
work the output, for instance, can directly
be compared with the correct solution and
and the network weights can be changed

according to their diﬀerence. The objec-
tive is to change the weights to the eﬀect
that the network cannot only associate in-
put and output patterns independently af-
ter the training, but can provide plausible
results to unknown, similar input patterns,
i.e. it generalises.
Deﬁnition 4.4 (Supervised learning).
The training set consists of input patterns
with correct results so that the network can
receive a precise error vector1 can be re-
turned.

This learning procedure is not always bio-
logically plausible, but it is extremely ef-
fective and therefore very practicable.

At ﬁrst we want to look at the the su-
pervised learning procedures in general,
which - in this text - are corresponding
to the following steps:

Entering the input pattern (activation of

input neurons),

Forward propagation of the input by the

network, generation of the output,

Comparing the output with the desired
output (teaching input), provides er-
ror vector (diﬀerence vector),

Corrections of the network are

calculated based on the error vector,

Corrections are applied.

1 The term error vector will be deﬁned in section
4.2, where mathematical formalisation of learning
is discussed.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

53

network
receives
reward or
punishment

network
receives
correct
results for
samples

learning
scheme

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

4.1.4 Oﬄine or online learning?

It must be noted that learning can be
oﬄine (a set of training samples is pre-
sented, then the weights are changed, the
total error is calculated by means of a error
function operation or simply accumulated -
see also section 4.4) or online (after every
sample presented the weights are changed).
Both procedures have advantages and dis-
advantages, which will be discussed in the
learning procedures section if necessary.
Oﬄine training procedures are also called
batch training procedures since a batch
of results is corrected all at once. Such a
training section of a whole batch of train-
ing samples including the related change
in weight values is called epoch.
Deﬁnition 4.5 (Oﬄine learning). Sev-
eral training patterns are entered into the
network at once, the errors are accumu-
lated and it learns for all patterns at the
same time.
Deﬁnition 4.6 (Online learning). The
network learns directly from the errors of
each training sample.

. How must the weights be modiﬁed to

allow fast and reliable learning?

. How can the success of a learning pro-
cess be measured in an objective way?
. Is it possible to determine the "best"

learning procedure?

. Is it possible to predict if a learning
procedure terminates, i.e. whether it
will reach an optimal state after a ﬁ-
nite time or if it, for example, will os-
cillate between diﬀerent states?

. How can the learned patterns be

stored in the network?

. Is it possible to avoid that newly
learned patterns destroy previously
learned associations (the so-called sta-
bility/plasticity dilemma)?

We will see that all these questions cannot
be generally answered but that they have (cid:74)(cid:74)(cid:74)
to be discussed for each learning procedure
no easy
and each network topology individually.
answers!

4.2 Training patterns and

teaching input

4.1.5 Questions you should answer

before learning

The application of such schemes certainly
requires preliminary thoughts about some
questions, which I want to introduce now
as a check list and,
if possible, answer
them in the course of this text:

. Where does the learning input come

from and in what form?

Before we get to know our ﬁrst learning
rule, we need to introduce the teaching
input.
In (this) case of supervised learn-
ing we assume a training set consisting
of training patterns and the correspond-
ing correct output values we want to see
at the output neurons after the training.
While the network has not ﬁnished train-
ing, i.e. as long as it is generating wrong
outputs, these output values are referred

desired
output

54

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

4.2 Training patterns and teaching input

p(cid:73)

t(cid:73)

desired
output

to as teaching input, and that for each neu-
ron individually. Thus, for a neuron j with
the incorrect output oj, tj is the teaching
input, which means it is the correct or de-
sired output for a training pattern p.
Deﬁnition 4.7 (Training patterns). A
training pattern is an input vector p
with the components p1, p2, . . . , pn whose
desired output is known. By entering the
training pattern into the network we re-
ceive an output that can be compared with
the teaching input, which is the desired
output. The set of training patterns is
called P. It contains a ﬁnite number of or-
dered pairs(p, t) of training patterns with
corresponding desired output.

Training patterns are often simply called
patterns, that is why they are referred
to as p.
In the literature as well as in
this text they are called synonymously pat-
terns, training samples etc.
Deﬁnition 4.8 (Teaching input). Let j
be an output neuron. The teaching in-
put tj is the desired and correct value j
should output after the input of a certain
training pattern. Analogously to the vec-
tor p the teaching inputs t1, t2, . . . , tn of
the neurons can also be combined into a
vector t. t always refers to a speciﬁc train-
ing pattern p and is, as already mentioned,
contained in the set P of the training pat-
terns.

are

that

relevant
SNIPE: Classes
in
for
training
the package
class
TrainingSampleLesson allows for storage
of training patterns and teaching inputs,

data
are
training.

located
The

as well as simple preprocessing of the
training data.

Deﬁnition 4.9 (Error vector). For sev- (cid:74)Ep
eral output neurons Ω1, Ω2, . . . , Ωn the dif-
ference between output vector and teach-
ing input under a training input p

 t1 − y1

...

tn − yn



Ep =

is referred to as error vector, sometimes
it is also called diﬀerence vector. De-
pending on whether you are learning of-
ﬂine or online, the diﬀerence vector refers
to a speciﬁc training pattern, or to the er-
ror of a set of training patterns which is
normalized in a certain way.

Now I want to brieﬂy summarize the vec-
tors we have yet deﬁned. There is the
input vector x, which can be entered into
the neural network. Depending on
the type of network being used the
neural network will output an

output vector y. Basically, the
training sample p is nothing more than
an input vector. We only use it for
training purposes because we know
the corresponding

teaching input t which is nothing more
than the desired output vector to the
training sample. The

error vector Ep is the diﬀerence between
the teaching input t and the actural
output y.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

55

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

Important!

So, what x and y are for the general net-
work operation are p and t for the network
training - and during training we try to
bring y as close to t as possible. One ad-
vice concerning notation: We referred to
the output values of a neuron i as oi. Thus,
the output of an output neuron Ω is called
oΩ. But the output values of a network are
referred to as yΩ. Certainly, these network
outputs are only neuron outputs, too, but
they are outputs of output neurons.
In
this respect

yΩ = oΩ

is true.

4.3 Using training samples

We have seen how we can learn in prin-
ciple and which steps are required to do
so. Now we should take a look at the se-
lection of training data and the learning
curve. After successful learning it is par-
ticularly interesting whether the network
has only memorized – i.e. whether it can
use our training samples to quite exactly
produce the right output but to provide
wrong answers for all other problems of
the same class.
Suppose that we want the network to train
a mapping R2 → B1 and therefor use the
training samples from ﬁg. 4.1: Then there
could be a chance that, ﬁnally, the net-
work will exactly mark the colored areas
around the training samples with the out-
put 1 (ﬁg. 4.1, top), and otherwise will
output 0 . Thus, it has suﬃcient storage
capacity to concentrate on the six training

Figure 4.1: Visualization of training results of
the same training set on networks with a capacity
being too high (top), correct (middle) or too low
(bottom).

56

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

4.3 Using training samples

samples with the output 1. This implies
an oversized network with too much free
storage capacity.
On the other hand a network could have
insuﬃcient capacity (ﬁg. 4.1, bottom) –
this rough presentation of input data does
not correspond to the good generalization
performance we desire. Thus, we have to
ﬁnd the balance (ﬁg. 4.1, middle).

4.3.1 It is useful to divide the set of

training samples

An often proposed solution for these prob-
lems is to divide, the training set into

. one training set really used to train ,
. and one veriﬁcation set to test our

progress

– provided that there are enough train-
ing samples. The usual division relations
are, for instance, 70% for training data
and 30% for veriﬁcation data (randomly
chosen). We can ﬁnish the training when
the network provides good results on the
training data as well as on the veriﬁcation
data.

SNIPE: The method splitLesson within
the class TrainingSampleLesson allows for
splitting a TrainingSampleLesson with re-
spect to a given ratio.

But note: If the veriﬁcation data provide
poor results, do not modify the network
structure until these data provide good re-
sults – otherwise you run the risk of tai-
loring the network to the veriﬁcation data.

This means, that these data are included
in the training, even if they are not used
explicitly for the training. The solution
is a third set of validation data used only
for validation after a supposably success-
ful training.

By training less patterns, we obviously
withhold information from the network
and risk to worsen the learning perfor-
mance. But this text is not about 100%
exact reproduction of given samples but
about successful generalization and ap-
proximation of a whole function – for
which it can deﬁnitely be useful to train
less information into the network.

4.3.2 Order of pattern

representation

You can ﬁnd diﬀerent strategies to choose
the order of pattern presentation: If pat-
terns are presented in random sequence,
there is no guarantee that the patterns
are learned equally well (however, this is
the standard method). Always the same
sequence of patterns, on the other hand,
provokes that the patterns will be memo-
rized when using recurrent networks (later,
we will learn more about this type of net-
works). A random permutation would
solve both problems, but it is – as already
mentioned – very time-consuming to cal-
culate such a permutation.

SNIPE: The method shuffleSamples lo-
cated in the class TrainingSampleLesson
permutes a lesson.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

57

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

norm
to
compare

4.4 Learning curve and error

measurement

The learning curve indicates the progress
of the error, which can be determined in
various ways. The motivation to create a
learning curve is that such a curve can in-
dicate whether the network is progressing
or not. For this, the error should be nor-
malized, i.e. represent a distance measure
between the correct and the current out-
put of the network. For example, we can
take the same pattern-speciﬁc, squared er-
ror with a prefactor, which we are also go-
ing to use to derive the backpropagation
of error (let Ω be output neurons and O
the set of output neurons):

X

Ω∈O

Errp = 1
2

(tΩ − yΩ)2

(4.1)

Errp(cid:73)

Deﬁnition 4.10 (Speciﬁc error). The
speciﬁc error Errp is based on a single
training sample, which means it is gener-
ated online.

Additionally, the root mean square (ab-
breviated: RMS) and the Euclidean
distance are often used.
The Euclidean distance (generalization of
the theorem of Pythagoras) is useful for
lower dimensions where we can still visual-
ize its usefulness.
Deﬁnition 4.11 (Euclidean distance).
The Euclidean distance between two vec-
tors t and y is deﬁned as

sX

Ω∈O

Errp =

Generally, the root mean square is com-
monly used since it considers extreme out-
liers to a greater extent.
Deﬁnition 4.12 (Root mean square).
The root mean square of two vectors t and
y is deﬁned as

sP

Errp =

Ω∈O(tΩ − yΩ)2

|O|

.

(4.3)

As for oﬄine learning, the total error in
the course of one training epoch is inter-
esting and useful, too:

Err = X

p∈P

Errp

(4.4)

Deﬁnition 4.13 (Total error). The total
error Err is based on all training samples, (cid:74)Err
that means it is generated oﬄine.

Analogously we can generate a total RMS
and a total Euclidean distance in the
course of a whole epoch. Of course, it is
possible to use other types of error mea-
surement. To get used to further error
measurement methods, I suggest to have a
look into the technical report of Prechelt
[Pre94].
In this report, both error mea-
surement methods and sample problems
are discussed (this is why there will be a
simmilar suggestion during the discussion
of exemplary problems).

SNIPE: There are several static meth-
ods representing diﬀerent methods of er-
ror measurement implemented in the class
ErrorMeasurement.

(tΩ − yΩ)2.

(4.2)

Depending on our method of error mea-
surement our
learning curve certainly

58

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

objectivity

dkriesel.com

4.4 Learning curve and error measurement

changes, too. A perfect learning curve
looks like a negative exponential
func-
tion, that means it is proportional to e−t
(ﬁg. 4.2 on the following page). Thus, the
representation of the learning curve can be
illustrated by means of a logarithmic scale
(ﬁg. 4.2, second diagram from the bot-
tom) – with the said scaling combination
a descending line implies an exponential
descent of the error.

With the network doing a good job, the
problems being not too diﬃcult and the
logarithmic representation of Err you can
see - metaphorically speaking - a descend-
ing line that often forms "spikes" at the
bottom – here, we reach the limit of the
64-bit resolution of our computer and our
network has actually learned the optimum
of what it is capable of learning.

i.e.

Typical learning curves can show a few ﬂat
areas as well,
they can show some
steps, which is no sign of a malfunctioning
learning process. As we can also see in ﬁg.
4.2, a well-suited representation can make
any slightly decreasing learning curve look
good – so just be cautious when reading
the literature.

4.4.1 When do we stop learning?

Now, the big question is: When do we
stop learning? Generally, the training is
stopped when the user in front of the learn-
ing computer "thinks" the error was small
enough.
Indeed, there is no easy answer
and thus I can once again only give you
something to think about, which, however,

depends on a more objective view on the
comparison of several learning curves.

Conﬁdence in the results, for example, is
boosted, when the network always reaches
nearly the same ﬁnal error-rate for diﬀer-
ent random initializations – so repeated
initialization and training will provide a
more objective result.

On the other hand, it can be possible that
a curve descending fast in the beginning
can, after a longer time of learning, be
overtaken by another curve: This can indi-
cate that either the learning rate of the
worse curve was too high or the worse
curve itself simply got stuck in a local min-
imum, but was the ﬁrst to ﬁnd it.

Remember: Larger error values are worse
than the small ones.

But, in any case, note: Many people only
generate a learning curve in respect of the
training data (and then they are surprised
that only a few things will work) – but for
reasons of objectivity and clarity it should
not be forgotten to plot the veriﬁcation
data on a second learning curve, which
generally provides values that are slightly
worse and with stronger oscillation. But
with good generalization the curve can de-
crease, too.

When the network eventually begins to
memorize the samples, the shape of the
learning curve can provide an indication:
If the learning curve of the veriﬁcation
samples is suddenly and rapidly rising
while the learning curve of the veriﬁcation

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

59

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

Figure 4.2: All four illustrations show the same (idealized, because very smooth) learning curve.
Note the alternating logarithmic and linear scalings! Also note the small "inaccurate spikes" visible
in the sharp bend of the curve in the ﬁrst and second diagram from bottom.

60

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

 0 5e−005 0.0001 0.00015 0.0002 0.00025 0 100 200 300 400 500 600 700 800 900 1000FehlerEpoche 0 2e−005 4e−005 6e−005 8e−005 0.0001 0.00012 0.00014 0.00016 0.00018 0.0002 1 10 100 1000FehlerEpoche 1e−035 1e−030 1e−025 1e−020 1e−015 1e−010 1e−005 1 0 100 200 300 400 500 600 700 800 900 1000FehlerEpoche 1e−035 1e−030 1e−025 1e−020 1e−015 1e−010 1e−005 1 1 10 100 1000FehlerEpochedkriesel.com

4.5 Gradient optimization procedures

data is continuously falling, this could indi-
cate memorizing and a generalization get-
ting poorer and poorer. At this point it
could be decided whether the network has
already learned well enough at the next
point of the two curves, and maybe the
ﬁnal point of learning is to be applied
here (this procedure is called early stop-
ping).
Once again I want to remind you that they
are all acting as indicators and not to draw
If-Then conclusions.

4.5 Gradient optimization

procedures

In order to establish the mathematical ba-
sis for some of the following learning pro-
cedures I want to explain brieﬂy what is
meant by gradient descent: the backpro-
pagation of error learning procedure, for
example, involves this mathematical basis
and thus inherits the advantages and dis-
advantages of the gradient descent.
Gradient descent procedures are generally
used where we want to maximize or mini-
mize n-dimensional functions. Due to clar-
ity the illustration (ﬁg. 4.3 on the next
page) shows only two dimensions, but prin-
cipally there is no limit to the number of
dimensions.
The gradient
is a vector g that is de-
ﬁned for any diﬀerentiable point of a func-
tion, that points from this point exactly
towards the steepest ascent and indicates
the gradient in this direction by means

gradient is
multi-dim.
derivative

of its norm |g|. Thus, the gradient is a
generalization of the derivative for multi-
dimensional functions. Accordingly, the
negative gradient −g exactly points to-
wards the steepest descent. The gradient
operator ∇ is referred to as nabla op- (cid:74)∇
erator, the overall notation of the the
gradient g of the point (x, y) of a two-
dimensional function f being g(x, y) =
∇f(x, y).
Deﬁnition 4.14 (Gradient). Let g be
a gradient. Then g is a vector with n
components that is deﬁned for any point
of a (diﬀerential) n-dimensional function
f(x1, x2, . . . , xn). The gradient operator
notation is deﬁned as
g(x1, x2, . . . , xn) = ∇f(x1, x2, . . . , xn).
g directs from any point of f towards
the steepest ascent from this point, with
|g| corresponding to the degree of this as-
cent.

Gradient descent means to going downhill
in small steps from any starting point of
our function towards the gradient g (which
means, vividly speaking, the direction to
which a ball would roll from the starting
point), with the size of the steps being pro-
portional to |g| (the steeper the descent,
the longer the steps). Therefore, we move
slowly on a ﬂat plateau, and on a steep as-
cent we run downhill rapidly. If we came
into a valley, we would - depending on the
size of our steps - jump over it or we would
return into the valley across the opposite
hillside in order to come closer and closer
to the deepest point of the valley by walk-
ing back and forth, similar to our ball mov-
ing within a round bowl.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

61

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

Figure 4.3: Visualization of the gradient descent on a two-dimensional error function. We
move forward in the opposite direction of g, i.e. with the steepest descent towards the lowest
point, with the step width being proportional to |g| (the steeper the descent, the faster the
steps). On the left the area is shown in 3D, on the right the steps over the contour lines are
shown in 2D. Here it is obvious how a movement is made in the opposite direction of g towards
the minimum of the function and continuously slows down proportionally to |g|.
Source:
http://webster.fhs-hagenberg.ac.at/staff/sdreisei/Teaching/WS2001-2002/
PatternClassification/graddescent.pdf

We go
towards the
gradient

Deﬁnition 4.15 (Gradient descent).
Let f be an n-dimensional function and
s = (s1, s2, . . . , sn) the given starting
point. Gradient descent means going
from f(s) against the direction of g, i.e.
towards −g with steps of the size of |g|
towards smaller and smaller values of f.

Gradient descent procedures are not an er-
rorless optimization procedure at all (as
we will see in the following sections) – how-
ever, they work still well on many prob-
lems, which makes them an optimization
paradigm that is frequently used. Anyway,
let us have a look on their potential disad-
vantages so we can keep them in mind a
bit.

4.5.1 Gradient procedures

incorporate several problems

As already implied in section 4.5, the gra-
dient descent (and therefore the backpro-
pagation) is promising but not foolproof.
One problem, is that the result does not
always reveal if an error has occurred.

4.5.1.1 Often, gradient descents

converge against suboptimal
minima

Every gradient descent procedure can, for
example, get stuck within a local mini-
mum (part a of ﬁg. 4.4 on the facing page).

gradient
descent
with errors

62

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

4.5 Gradient optimization procedures

Figure 4.4: Possible errors during a gradient descent: a) Detecting bad minima, b) Quasi-standstill
with small gradient, c) Oscillation in canyons, d) Leaving good minima.

This problem is increasing proportionally
to the size of the error surface, and there
is no universal solution.
In reality, one
cannot know if the optimal minimum is
reached and considers a training success-
ful, if an acceptable minimum is found.

4.5.1.2 Flat plataeus on the error
surface may cause training
slowness

When passing a ﬂat plateau, for instance,
the gradient also becomes negligibly small
because there is hardly a descent (part b
of ﬁg. 4.4), which requires many further
steps. A hypothetically possible gradient
of 0 would completely stop the descent.

4.5.1.3 Even if good minima are
reached, they may be left
afterwards

On the other hand the gradient is very
large at a steep slope so that large steps
can be made and a good minimum can pos-
sibly be missed (part d of ﬁg. 4.4).

4.5.1.4 Steep canyons in the error

surface may cause oscillations

A sudden alternation from one very strong
negative gradient to a very strong positive
one can even result in oscillation (part c
of ﬁg. 4.4). In nature, such an error does
not occur very often so that we can think
about the possibilities b and d.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

63

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

4.6 Exemplary problems allow

for testing self-coded
learning strategies

We looked at learning from the formal
point of view – not much yet but a little.
Now it is time to look at a few exemplary
problem you can later use to test imple-
mented networks and learning rules.

4.6.1 Boolean functions

i1
0
0
0
0
1
1
1
1

i2
0
0
1
1
0
0
1
1

i3 Ω
0
1
0
1
0
0
1
1
0
0
1
1
0
1
1
0

Table 4.1: Illustration of the parity function
with three inputs.

A popular example is the one that did
not work in the nineteen-sixties: the XOR
function (B2 → B1). We need a hidden
neuron layer, which we have discussed in
detail. Thus, we need at least two neu-
rons in the inner layer. Let the activation
function in all layers (except in the input
layer, of course) be the hyperbolic tangent.
Trivially, we now expect the outputs 1.0
or −1.0, depending on whether the func-
tion XOR outputs 1 or 0 - and exactly
here is where the ﬁrst beginner’s mistake
occurs.

For outputs close to 1 or -1, i.e. close to
the limits of the hyperbolic tangent (or
in case of the Fermi function 0 or 1), we
need very large network inputs. The only
chance to reach these network inputs are
large weights, which have to be learned:
The learning process is largely extended.
Therefore it is wiser to enter the teaching
inputs 0.9 or −0.9 as desired outputs or
to be satisﬁed when the network outputs
those values instead of 1 and −1.

Another favourite example for singlelayer
perceptrons are the boolean functions
AND and OR.

4.6.2 The parity function

The parity function maps a set of bits to 1
or 0, depending on whether an even num-
ber of input bits is set to 1 or not. Ba-
sically, this is the function Bn → B1.
It
is characterized by easy learnability up to
approx. n = 3 (shown in table 4.1), but
the learning eﬀort rapidly increases from
n = 4. The reader may create a score ta-
ble for the 2-bit parity function. What is
conspicuous?

4.6.3 The 2-spiral problem

As a training sample for a function let
us take two spirals coiled into each other
(ﬁg. 4.5 on the facing page) with the
function certainly representing a mapping

64

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

4.6 Exemplary problems

Figure 4.5: Illustration of the training samples
of the 2-spiral problem

Figure 4.6: Illustration of training samples for
the checkerboard problem

R2 → B1. One of the spirals is assigned
to the output value 1, the other spiral to
0. Here, memorizing does not help. The
network has to understand the mapping it-
self. This example can be solved by means
of an MLP, too.

4.6.4 The checkerboard problem

We again create a two-dimensional func-
tion of the form R2 → B1 and specify
checkered training samples (ﬁg. 4.6) with
one colored ﬁeld representing 1 and all the
rest of them representing 0. The diﬃculty
increases proportionally to the size of the
function: While a 3×3 ﬁeld is easy to learn,
the larger ﬁelds are more diﬃcult (here
we eventually use methods that are more

suitable for this kind of problems than the
MLP).
The 2-spiral problem is very similar to the
checkerboard problem, only that, mathe-
matically speaking, the ﬁrst problem is us-
ing polar coordinates instead of Cartesian
coordinates.
I just want to introduce as
an example one last trivial case: the iden-
tity.

4.6.5 The identity function

By using linear activation functions the
identity mapping from R1 to R1 (of course
only within the parameters of the used ac-
tivation function) is no problem for the
network, but we put some obstacles in its
way by using our sigmoid functions so that

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

65

Chapter 4 Fundamentals on learning and training samples (fundamental)

dkriesel.com

it would be diﬃcult for the network to
learn the identity. Just try it for the fun
of it.
Now, it is time to hava a look at our ﬁrst
mathematical learning rule.

4.6.6 There are lots of other

exemplary problems

For lots and lots of further exemplary prob-
lems, I want to recommend the technical
report written by prechelt [Pre94] which
also has been named in the sections about
error measurement procedures..

4.7 The Hebbian learning rule

is the basis for most
other learning rules

In 1949, Donald O. Hebb formulated
the Hebbian rule [Heb49] which is the ba-
sis for most of the more complicated learn-
ing rules we will discuss in this text. We
distinguish between the original form and
the more general form, which is a kind of
principle for other learning rules.

4.7.1 Original rule

Deﬁnition 4.16 (Hebbian rule).
"If neu-
ron j receives an input from neuron i and
if both neurons are strongly active at the
same time, then increase the weight wi,j
(i.e.
the strength of the connection be-
tween i and j)." Mathematically speaking,
the rule is:

∆wi,j ∼ ηoiaj

(4.5)

with ∆wi,j being the change in weight
from i to j , which is proportional to the (cid:74)∆wi,j
following factors:
. the output oi of the predecessor neu-

ron i, as well as,

. the activation aj of the successor neu-

ron j,

. a constant η, i.e.

the learning rate,
which will be discussed in section
5.4.3.

The changes in weight ∆wi,j are simply
added to the weight wi,j.
Why am I speaking twice about activation,
but in the formula I am using oi and aj, i.e.
the output of neuron of neuron i and the ac-
tivation of neuron j? Remember that the
identity is often used as output function
and therefore ai and oi of a neuron are of-
ten the same. Besides, Hebb postulated
his rule long before the speciﬁcation of
technical neurons. Considering that this
learning rule was preferred in binary acti-
vations, it is clear that with the possible
activations (1, 0) the weights will either in-
crease or remain constant. Sooner or later
they would go ad inﬁnitum, since they can
only be corrected "upwards" when an error
occurs. This can be compensated by using
the activations (-1,1)2. Thus, the weights
are decreased when the activation of the
predecessor neuron dissents from the one
of the successor neuron, otherwise they are
increased.
2 But that is no longer the "original version" of the

Hebbian rule.

weights
go ad
inﬁnitum

66

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

early
form of
the rule

dkriesel.com

4.7 Hebbian rule

4.7.2 Generalized form

Exercises

Exercise 7. Calculate the average value
µ and the standard deviation σ for the fol-
lowing data points.

p1 = (2, 2, 2)
p2 = (3, 3, 3)
p3 = (4, 4, 4)
p4 = (6, 0, 0)
p5 = (0, 6, 0)
p6 = (0, 0, 6)

Most of the learning rules discussed before
are a specialization of the mathematically
more general form [MR86] of the Hebbian
rule.
Deﬁnition 4.17 (Hebbian rule, more gen-
eral). The generalized form of the
Hebbian Rule only speciﬁes the propor-
tionality of the change in weight to the
product of two undeﬁned functions, but
with deﬁned input values.

∆wi,j = η · h(oi, wi,j) · g(aj, tj)

(4.6)

Thus, the product of the functions

. g(aj, tj) and
. h(oi, wi,j)
. as well as the constant learning rate

η

results in the change in weight. As you
can see, h receives the output of the pre-
decessor cell oi as well as the weight from
predecessor to successor wi,j while g ex-
pects the actual and desired activation of
the successor aj and tj (here t stands for
the aforementioned teaching input). As al-
ready mentioned g and h are not speciﬁed
in this general deﬁnition. Therefore, we
will now return to the path of specializa-
tion we discussed before equation 4.6. Af-
ter we have had a short picture of what
a learning rule could look like and of our
thoughts about learning itself, we will be
introduced to our ﬁrst network paradigm
including the learning procedure.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

67

Part II

Supervised learning network

paradigms

69

Chapter 5
The perceptron, backpropagation and its
variants

A classic among the neural networks. If we talk about a neural network, then
in the majority of cases we speak about a percepton or a variation of it.
Perceptrons are multilayer networks without recurrence and with ﬁxed input
and output layers. Description of a perceptron, its limits and extensions that
should avoid the limitations. Derivation of learning procedures and discussion
of their problems.

As already mentioned in the history of neu-
ral networks, the perceptron was described
by Frank Rosenblatt in 1958 [Ros58].
Initially, Rosenblatt deﬁned the already
discussed weighted sum and a non-linear
activation function as components of the
perceptron.

There is no established deﬁnition for a per-
ceptron, but most of the time the term
is used to describe a feedforward network
with shortcut connections. This network
has a layer of scanner neurons (retina)
with statically weighted connections to
the following layer and is called input
layer (ﬁg. 5.1 on the next page); but the
weights of all other layers are allowed to be
changed. All neurons subordinate to the
retina are pattern detectors. Here we ini-
tially use a binary perceptron with every
output neuron having exactly two possi-

ble output values (e.g. {0, 1} or {−1, 1}).
Thus, a binary threshold function is used
as activation function, depending on the
threshold value Θ of the output neuron.

In a way, the binary activation function
represents an IF query which can also
be negated by means of negative weights.
The perceptron can thus be used to ac-
complish true logical information process-
ing.

Whether this method is reasonable is an-
other matter – of course, this is not the
easiest way to achieve Boolean logic. I just
want to illustrate that perceptrons can
be used as simple logical components and
that, theoretically speaking, any Boolean
function can be realized by means of per-
ceptrons being connected in series or in-
terconnected in a sophisticated way. But

71

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

Figure 5.1: Architecture of a perceptron with one layer of variable connections in diﬀerent views.
The solid-drawn weight layer in the two illustrations on the bottom can be trained.
Left side: Example of scanning information in the eye.
Right side, upper part: Drawing of the same example with indicated ﬁxed-weight layer using the
deﬁned designs of the functional descriptions for neurons.
Right side, lower part: Without indicated ﬁxed-weight layer, with the name of each neuron
corresponding to our convention. The ﬁxed-weight layer will no longer be taken into account in the
course of this work.

72

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Kapitel5DasPerceptrondkriesel.com""))++,,##))++||##)){{uu""{{uuss||uussrrGFED@ABC(cid:30)''OOOOOOOOOOOOOOOOOGFED@ABC(cid:30)@@@@@@@@@GFED@ABC(cid:30)GFED@ABC(cid:30)~~~~~~~~~GFED@ABC(cid:30)wwoooooooooooooooooWVUTPQRSΣL|HGFED@ABCi1((PPPPPPPPPPPPPPPPPPGFED@ABCi2!!CCCCCCCCCCGFED@ABCi3GFED@ABCi4}}{{{{{{{{{{GFED@ABCi5vvnnnnnnnnnnnnnnnnnn?>=<89:;ΩAbbildung5.1:AufbaueinesPerceptronsmiteinerSchichtvariablerVerbindungeninverschiede-nenAnsichten.DiedurchgezogeneGewichtsschichtindenunterenbeidenAbbildungenisttrainier-bar.Oben:AmBeispielderInformationsabtastungimAuge.Mitte:SkizzedesselbenmiteingezeichneterfesterGewichtsschichtunterVerwendungderdeﬁnier-tenfunktionsbeschreibendenDesignsf¨urNeurone.Unten:OhneeingezeichnetefesteGewichtsschicht,mitBenennungdereinzelnenNeuronennachunsererKonvention.WirwerdendiefesteGewichtschichtimweiterenVerlaufderArbeitnichtmehrbetrachten.70D.Kriesel–Einkleiner¨Uberblick¨uberNeuronaleNetze(EPSILON-DE)input neuron
only forwards
data

dkriesel.com

we will see that this is not possible without
connecting them serially. Before providing
the deﬁnition of the perceptron, I want to
deﬁne some types of neurons used in this
chapter.

Deﬁnition 5.1 (Input neuron). An in-
put neuron is an identity neuron.
It
exactly forwards the information received.
Thus, it represents the identity function,
which should be indicated by the symbol
(cid:30). Therefore the input neuron is repre-

sented by the symbolGFED
@ABC(cid:30) .

Deﬁnition 5.2 (Information process-
ing neuron).
Information processing
neurons somehow process the input infor-
mation, i.e. do not represent the identity
function. A binary neuron sums up all
inputs by using the weighted sum as prop-
agation function, which we want to illus-
trate by the sign Σ. Then the activation
function of the neuron is the binary thresh-
old function, which can be illustrated by
L|H. This leads us to the complete de-
piction of information processing neurons,
L|H . Other neurons that use
the weighted sum as propagation function
but the activation functions hyperbolic tan-
gent or Fermi function, or with a sepa-
rately deﬁned activation function fact, are
similarly represented by

namely WVUT
PQRSΣ

These neurons are also referred to as
Fermi neurons or Tanh neuron.

WVUT
PQRSΣ

Tanh

WVUT
PQRSΣ

Fermi

ONML
HIJKΣ

fact

.

Now that we know the components of a
perceptron we should be able to deﬁne
it.

Deﬁnition 5.3 (Perceptron). The per-
ceptron (ﬁg. 5.1 on the facing page) is1 a
feedforward network containing a retina
that is used only for data acquisition and
which has ﬁxed-weighted connections with
the ﬁrst neuron layer (input layer). The
ﬁxed-weight layer is followed by at least
one trainable weight layer. One neuron
layer is completely linked with the follow-
ing layer. The ﬁrst layer of the percep-
tron consists of the input neurons deﬁned
above.

A feedforward network often contains
shortcuts which does not exactly corre-
spond to the original description and there-
fore is not included in the deﬁnition. We
can see that the retina is not included in
the lower part of ﬁg. 5.1. As a matter
of fact the ﬁrst neuron layer is often un-
derstood (simpliﬁed and suﬃcient for this
method) as input layer, because this layer
only forwards the input values. The retina
itself and the static weights behind it are
no longer mentioned or displayed, since
they do not process information in any
case. So, the depiction of a perceptron
starts with the input neurons.

1 It may confuse some readers that I claim that there
is no deﬁnition of a perceptron but then deﬁne the
perceptron in the following section.
I therefore
suggest keeping my deﬁnition in the back of your
mind and just take it for granted in the course of
this work.

retina is
unconsidered

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

73

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

The

methods

SNIPE:
setSettingsTopologyFeedForward
and the variation -WithShortcuts in
NeuralNetworkDescriptor-Instance
a
apply settings
to a descriptor, which
are appropriate for feedforward networks
or feedforward networks with shortcuts.
The respective kinds of connections are
allowed, all others are not, and fastprop is
activated.

5.1 The singlelayer

perceptron provides only
one trainable weight layer

1 trainable

Here, connections with trainable weights
go from the input layer to an output
neuron Ω, which returns the information
layer whether the pattern entered at the input
neurons was recognized or not. Thus, a
singlelayer perception (abbreviated SLP)
has only one level of trainable weights
(ﬁg. 5.1 on page 72).
Deﬁnition 5.4 (Singlelayer perceptron).
A singlelayer perceptron (SLP) is a
perceptron having only one layer of vari-
able weights and one layer of output neu-
rons Ω. The technical view of an SLP is
shown in ﬁg. 5.2.

Important!

Certainly, the existence of several output
neurons Ω1, Ω2, . . . , Ωn does not consider-
ably change the concept of the perceptron
(ﬁg. 5.3): A perceptron with several out-
put neurons can also be regarded as sev-
eral diﬀerent perceptrons with the same
input.

BIAS

GFED
@ABC

wBIAS,Ω

GFED
@ABC

i2


wi2,Ω


i1

wi1,Ω

GFED
@ABC
?>=<
89:;Ω

Figure 5.2: A singlelayer perceptron with two in-
put neurons and one output neuron. The net-
work returns the output by means of the ar-
row leaving the network. The trainable layer of
weights is situated in the center (labeled). As a
reminder, the bias neuron is again included here.
Although the weight wBIAS,Ω is a normal weight
and also treated like this, I have represented it
by a dotted line – which signiﬁcantly increases
the clarity of larger networks. In future, the bias
neuron will no longer be included.

i2

i3

i1

GFED
@ABC
GFED
@ABC
GFED
@ABC
GFED
@ABC
GFED
@ABC
*UUUUUUUUUUUUUUUUUUUUUUUUUU
(PPPPPPPPPPPPPPPPPP
'PPPPPPPPPPPPPPPPP
tiiiiiiiiiiiiiiiiiiiiiiiiii
vnnnnnnnnnnnnnnnnnn
wnnnnnnnnnnnnnnnnn
 AAAAAAAAA
 AAAAAAAAA
 @@@@@@@@@
~}}}}}}}}}
~}}}}}}}}}
~~~~~~~~~~
GFED
@ABCΩ1
GFED
GFED
@ABCΩ3
@ABCΩ2

i4

i5

Figure 5.3: Singlelayer perceptron with several
output neurons

74

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

 
 





 
*
'


(
 
~
 


v


~
~
t
w






dkriesel.com

5.1 The singlelayer perceptron

GFED
@ABC(cid:30)

GFED
@ABC(cid:30)

}}}}
1
~}}}}

}}}}
1
~}}}}

GFED
@ABC(cid:30)

AAAA
1

GFED
@ABC(cid:30)

AAAA
1

 AAAA

GFED
@ABC1.5

 AAAA

GFED
@ABC0.5

Figure 5.4: Two singlelayer perceptrons
for
Boolean functions. The upper singlelayer per-
ceptron realizes an AND, the lower one realizes
an OR. The activation function of the informa-
tion processing neuron is the binary threshold
function. Where available, the threshold values
are written into the neurons.

The Boolean functions AND and OR shown
in ﬁg. 5.4 are trivial examples that can eas-
ily be composed.

Now we want to know how to train a single-
layer perceptron. We will therefore at ﬁrst
take a look at the perceptron learning al-
gorithm and then we will look at the delta
rule.

5.1.1 Perceptron learning algorithm

and convergence theorem

The original perceptron learning algo-
rithm with binary neuron activation func-
tion is described in alg. 1.
It has been
proven that the algorithm converges in
ﬁnite time – so in ﬁnite time the per-
ceptron can learn anything it can repre-
sent (perceptron convergence theorem,
[Ros62]). But please do not get your hopes
up too soon! What the perceptron is capa-
ble to represent will be explored later.

During the exploration of linear separabil-
ity of problems we will cover the fact that
at least the singlelayer perceptron unfor-
tunately cannot represent a lot of prob-
lems.

5.1.2 The delta rule as a gradient
based learning strategy for
SLPs

In the following we deviate from our bi-
nary threshold value as activation function
because at least for backpropagation of er-
ror we need, as you will see, a diﬀeren-
tiable or even a semi-linear activation func-
tion. For the now following delta rule (like
backpropagation derived in [MR86]) it is
not always necessary but useful. This fact,
however, will also be pointed out in the
appropriate part of this work. Compared
with the aforementioned perceptron learn-
ing algorithm, the delta rule has the ad-
vantage to be suitable for non-binary acti-
vation functions and, being far away from

fact now diﬀer-
entiable

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

75

 
~


 
~


Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

if yΩ = tΩ then

else

Output is okay, no correction of weights

end for

end if

if yΩ = 0 then

for all input neurons i do

wi,Ω := wi,Ω + oi {...increase weight towards Ω by oi}

Input p into the network, calculate output y {P set of training patterns}
for all output neurons Ω do

1: while ∃p ∈ P and error too large do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
end if
17:
end for
18:
19: end while
Algorithm 1: Perceptron learning algorithm. The perceptron learning algorithm
reduces the weights to output neurons that return 1 instead of 0, and in the inverse
case increases weights.

wi,Ω := wi,Ω − oi {...decrease weight towards Ω by oi}

end for

end if
if yΩ = 1 then

for all input neurons i do

76

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.1 The singlelayer perceptron

the learning target, to automatically learn
faster.
Suppose that we have a singlelayer percep-
tron with randomly set weights which we
want to teach a function by means of train-
ing samples. The set of these training sam-
ples is called P. It contains, as already de-
ﬁned, the pairs (p, t) of the training sam-
ples p and the associated teaching input t.
I also want to remind you that
. x is the input vector and
. y is the output vector of a neural net-

work,

. output neurons are referred to as

Ω1, Ω2, . . . , Ω|O|,
. i is the input and
. o is the output of a neuron.

Additionally, we deﬁned that

. the error vector Ep represents the dif-
ference (t−y) under a certain training
sample p.

. Furthermore, let O be the set of out-

put neurons and

. I be the set of input neurons.

Another naming convention shall be that,
for example, for an output o and a teach-
ing input t an additional index p may be
set in order to indicate that these values
are pattern-speciﬁc. Sometimes this will
considerably enhance clarity.
Now our learning target will certainly be,
that for all training samples the output y

of the network is approximately the de-
sired output t,
formally it is true
that

i.e.

∀p : y ≈ t

or

∀p : Ep ≈ 0.

This means we ﬁrst have to understand the
total error Err as a function of the weights:
The total error increases or decreases de-
pending on how we change the weights.
Deﬁnition 5.5 (Error function). The er-
ror function

Err : W → R

(cid:74)Err(W)

regards the set2 of weights W as a vector
and maps the values onto the normalized
output error (normalized because other-
wise not all errors can be mapped onto
one single e ∈ R to perform a gradient de-
scent). It is obvious that a speciﬁc error
function can analogously be generated (cid:74)Errp(W)
for a single pattern p.

error as
function

As already shown in section 4.5, gradient
descent procedures calculate the gradient
of an arbitrary but ﬁnite-dimensional func-
tion (here: of the error function Err(W))
and move down against the direction of
the gradient until a minimum is reached.
Err(W) is deﬁned on the set of all weights
which we here regard as the vector W.
So we try to decrease or to minimize the
error by simply tweaking the weights –
thus one receives information about how
to change the weights (the change in all
2 Following the tradition of the literature, I previ-
ously deﬁned W as a weight matrix.
I am aware
of this conﬂict but it should not bother us here.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

77

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

Thus, we tweak every single weight and ob-
serve how the error function changes, i.e.
we derive the error function according to
a weight wi,Ω and obtain the value ∆wi,Ω
of how to change this weight.

∆wi,Ω = −η

∂Err(W)

∂wi,Ω

.

(5.3)

Now the following question arises: How
is our error function deﬁned exactly? It
is not good if many results are far away
from the desired ones; the error function
should then provide large values – on the
other hand,
it is similarly bad if many
results are close to the desired ones but
there exists an extremely far outlying re-
sult. The squared distance between the
output vector y and the teaching input t
appears adequate to our needs. It provides
the error Errp that is speciﬁc for a train-
ing sample p over the output of all output
neurons Ω:

Errp(W) = 1
2

X

Ω∈O

(tp,Ω − yp,Ω)2.

(5.4)

Figure 5.5: Exemplary error surface of a neural
network with two trainable connections w1 und
w2. Generally, neural networks have more than
two connections, but this would have made the
illustration too complex. And most of the time
the error surface is too craggy, which complicates
the search for the minimum.

weights is referred to as ∆W) by calcu-
lating the gradient ∇Err(W) of the error
function Err(W):

∆W ∼ −∇Err(W).

(5.1)

Due to this relation there is a proportional-
ity constant η for which equality holds (η
will soon get another meaning and a real
practical use beyond the mere meaning of
a proportionality constant. I just ask the
reader to be patient for a while.):

∆W = −η∇Err(W).

(5.2)

To simplify further analysis, we now
rewrite the gradient of the error-function
according to all weights as an usual par-
tial derivative according to a single weight
wi,Ω (the only variable weights exists be-
tween the hidden and the output layer Ω).

Thus, we calculate the squared diﬀerence
of the components of the vectors t and
y, given the pattern p, and sum up these
squares. The summation of the speciﬁc er-
rors Errp(W) of all patterns p then yields
the deﬁnition of the error Err and there-

78

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

−2−1 0 1 2w1−2−1 0 1 2w2 0 1 2 3 4 5dkriesel.com

5.1 The singlelayer perceptron

fore the deﬁnition of the error function
Err(W):

results from the sum of the speciﬁc er-
rors):

Errp(W)

(5.5)

sum over all p

X
|

Ω∈O

}|
(tp,Ω − yp,Ω)2
{z

sum over all Ω

{

}

.

(5.6)

Err(W) = X
z
X

p∈P

= 1
2

p∈P

The observant reader will certainly wonder
where the factor 1
2 in equation 5.4 on the
preceding page suddenly came from and
why there is no root in the equation, as
this formula looks very similar to the Eu-
clidean distance. Both facts result from
simple pragmatics: Our intention is to
minimize the error. Because the root func-
tion decreases with its argument, we can
simply omit it for reasons of calculation
and implementation eﬀorts, since we do
not need it for minimization. Similarly, it
does not matter if the term to be mini-
mized is divided by 2: Therefore I am al-
lowed to multiply by 1
2. This is just done
so that it cancels with a 2 in the course of
our calculation.
Now we want to continue deriving the
delta rule for linear activation functions.
We have already discussed that we tweak
the individual weights wi,Ω a bit and see
how the error Err(W) is changing – which
corresponds to the derivative of the er-
ror function Err(W) according to the very
same weight wi,Ω. This derivative cor-
responds to the sum of the derivatives
of all speciﬁc errors Errp according to
this weight (since the total error Err(W)

∆wi,Ω = −η

= X

p∈P

∂Err(W)

∂wi,Ω
−η

∂Errp(W)

∂wi,Ω

(5.7)

(5.8)

.

Once again I want to think about the ques-
tion of how a neural network processes
data. Basically, the data is only trans-
ferred through a function, the result of the
function is sent through another one, and
so on.
If we ignore the output function,
the path of the neuron outputs oi1 and oi2,
which the neurons i1 and i2 entered into a
neuron Ω, initially is the propagation func-
tion (here weighted sum), from which the
network input is going to be received. This
is then sent through the activation func-
tion of the neuron Ω so that we receive
the output of this neuron which is at the
same time a component of the output vec-
tor y:

netΩ → fact

= fact(netΩ)
= oΩ
= yΩ.

As we can see, this output results from
many nested functions:

oΩ = fact(netΩ)

= fact(oi1 · wi1,Ω + oi2 · wi2,Ω).

(5.9)
(5.10)

It is clear that we could break down the
output into the single input neurons (this
is unnecessary here, since they do not

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

79

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

process information in an SLP). Thus,
we want to calculate the derivatives of
equation 5.8 on the preceding page and
due to the nested functions we can apply
the chain rule to factorize the derivative
∂Errp(W )
in equation 5.8 on the previous
∂wi,Ω
page.

∂Errp(W)

∂wi,Ω

= ∂Errp(W)

∂op,Ω

· ∂op,Ω
∂wi,Ω

.

(5.11)

i.e.

Let us take a look at the ﬁrst multiplica-
tive factor of the above equation 5.11
which represents the derivative of the spe-
ciﬁc error Errp(W) according to the out-
put,
the change of the error Errp
with an output op,Ω: The examination
of Errp (equation 5.4 on page 78) clearly
shows that this change is exactly the dif-
ference between teaching input and out-
put (tp,Ω − op,Ω) (remember: Since Ω is an
output neuron, op,Ω = yp,Ω). The closer
the output is to the teaching input, the
smaller is the speciﬁc error. Thus we can
replace one by the other. This diﬀerence
is also called δp,Ω (which is the reason for
the name delta rule):

∂Errp(W)

∂wi,Ω

= −(tp,Ω − op,Ω) · ∂op,Ω
∂wi,Ω

(5.12)

= −δp,Ω · ∂op,Ω
∂wi,Ω

(5.13)

The second multiplicative factor of equa-
tion 5.11 and of the following one is the
derivative of the output speciﬁc to the pat-
tern p of the neuron Ω according to the
weight wi,Ω.
So how does op,Ω change
when the weight from i to Ω is changed?

∂Errp(W)

∂wi,Ω

i∈I(op,iwi,Ω)
∂wi,Ω

.

Due to the requirement at the beginning of
the derivation, we only have a linear acti-
vation function fact, therefore we can just
as well look at the change of the network
input when wi,Ω is changing:

= −δp,Ω · ∂P
The resulting derivative ∂P
P
∂P

i∈I(op,iwi,Ω)
∂wi,Ω
can now be simpliﬁed: The function
i∈I(op,iwi,Ω) to be derived consists of
many summands, and only the sum-
mand op,iwi,Ω contains the variable wi,Ω,
according to which we derive.
Thus,

(5.14)

i∈I(op,iwi,Ω)
∂wi,Ω

= op,i and therefore:

∂Errp(W)

∂wi,Ω

= −δp,Ω · op,i
= −op,i · δp,Ω.

(5.15)

(5.16)

We insert this in equation 5.8 on the previ-
ous page, which results in our modiﬁcation
rule for a weight wi,Ω:

op,i · δp,Ω.

(5.17)

∆wi,Ω = η ·X

p∈P

However: From the very beginning the
derivation has been intended as an oﬄine
rule by means of the question of how to
add the errors of all patterns and how to
learn them after all patterns have been
represented. Although this approach is
mathematically correct, the implementa-
tion is far more time-consuming and, as
we will see later in this chapter, partially

80

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.2 Linear separability

needs a lot of compuational eﬀort during
training.
The "online-learning version" of the delta
rule simply omits the summation and
learning is realized immediately after the
presentation of each pattern, this also sim-
pliﬁes the notation (which is no longer nec-
essarily related to a pattern p):
∆wi,Ω = η · oi · δΩ.

(5.18)

This version of the delta rule shall be used
for the following deﬁnition:
Deﬁnition 5.6 (Delta rule).
If we deter-
mine, analogously to the aforementioned
derivation, that the function h of the Heb-
bian theory (equation 4.6 on page 67) only
provides the output oi of the predecessor
neuron i and if the function g is the diﬀer-
ence between the desired activation tΩ and
the actual activation aΩ, we will receive
the delta rule, also known as Widrow-
Hoﬀ rule:
∆wi,Ω = η · oi · (tΩ − aΩ) = ηoiδΩ (5.19)
If we use the desired output (instead of the
activation) as teaching input, and there-
fore the output function of the output neu-
rons does not represent an identity, we ob-
tain
∆wi,Ω = η · oi · (tΩ − oΩ) = ηoiδΩ (5.20)
and δΩ then corresponds to the diﬀerence
between tΩ and oΩ.

In the case of the delta rule, the change
of all weights to an output neuron Ω is
proportional

In. 1
0
0
1
1

In. 2
0
1
0
1

Output
0
1
1
0

Table 5.1: Deﬁnition of the logical XOR. The
input values are shown of the left, the output
values on the right.

. to the diﬀerence between the current
activation or output aΩ or oΩ and the
corresponding teaching input tΩ. We
want to refer to this factor as δΩ , (cid:74)δ
which is also referred to as "Delta".

Apparently the delta rule only applies for
SLPs, since the formula is always related
to the teaching input, and there is no
teaching input for the inner processing lay-
ers of neurons.

delta rule
only for SLP

5.2 A SLP is only capable of

representing linearly
separable data

Let f be the XOR function which expects
two binary inputs and generates a binary
output (for the precise deﬁnition see ta-
ble 5.1).

Let us try to represent the XOR func-
tion by means of an SLP with two input
neurons i1, i2 and one output neuron Ω
(ﬁg. 5.6 on the following page).

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

81

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

GFED
@ABC

i1

GFED
@ABC

i2

BBBB
wi1,Ω

 BBBB

||||
wi2,Ω
~||||

?>=<
89:;Ω

XOR?

Figure 5.6: Sketch of a singlelayer perceptron
that shall represent the XOR function - which is
impossible.

Here we use the weighted sum as propaga-
tion function, a binary activation function
with the threshold value Θ and the iden-
tity as output function. Depending on i1
and i2, Ω has to output the value 1 if the
following holds:

netΩ = oi1wi1,Ω + oi2wi2,Ω ≥ ΘΩ (5.21)
We assume a positive weight wi2,Ω, the in-
equality 5.21 is then equivalent to

oi1 ≥ 1
wi1,Ω

(ΘΩ − oi2wi2,Ω)

(5.22)

With a constant threshold value ΘΩ, the
right part of inequation 5.22 is a straight
line through a coordinate system deﬁned
by the possible outputs oi1 und oi2 of the
input neurons i1 and i2 (ﬁg. 5.7).
For a (as required for inequation 5.22) pos-
itive wi2,Ω the output neuron Ω ﬁres for

Figure 5.7: Linear separation of n = 2 inputs of
the input neurons i1 and i2 by a 1-dimensional
straight line. A and B show the corners belong-
ing to the sets of the XOR function that are to
be separated.

82

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)





 
~


dkriesel.com

5.2 Linear separability

n number of

share

lin.
separable
ones
4
14
104
1, 772
94, 572
5, 028, 134

binary
functions
4
16
256
65, 536
4.3 · 109
1.8 · 1019

1
2
3
4
5
6

100%
87.5%
40.6%
2.7%
0.002%
≈ 0%
Table 5.2: Number of functions concerning n bi-
nary inputs, and number and proportion of the
functions thereof which can be linearly separated.
In accordance with [Zel94,Wid89,Was89].

input combinations lying above the gener-
ated straight line. For a negative wi2,Ω it
would ﬁre for all input combinations lying
below the straight line. Note that only the
four corners of the unit square are possi-
ble inputs because the XOR function only
knows binary inputs.

In order to solve the XOR problem, we
have to turn and move the straight line so
that input set A = {(0, 0), (1, 1)} is sepa-
rated from input set B = {(0, 1), (1, 0)} –
this is, obviously, impossible.

Generally, the input parameters of n many
input neurons can be represented in an n-
dimensional cube which is separated by an
SLP through an (n−1)-dimensional hyper-
plane (ﬁg. 5.8). Only sets that can be sep-
arated by such a hyperplane, i.e. which
are linearly separable, can be classiﬁed
by an SLP.

SLP cannot
do everything

Figure 5.8: Linear separation of n = 3 inputs
from input neurons i1, i2 and i3 by 2-dimensional
plane.

Unfortunately, it seems that the percent-
age of the linearly separable problems
rapidly decreases with increasing n (see
table 5.2), which limits the functionality
of the SLP. Additionally, tests for linear
separability are diﬃcult. Thus, for more
diﬃcult tasks with more inputs we need
something more powerful than SLP. The
XOR problem itself is one of these tasks,
since a perceptron that is supposed to rep-
resent the XOR function already needs a
hidden layer (ﬁg. 5.9 on the next page).

few tasks
are linearly
separable

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

83

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

GFED
@ABC(cid:30)
AAAA
11111111
1

1

GFED
@ABC(cid:30)
}}}}

GFED
@ABC1.5
 AAAA
1
~}}}}
11111111
1

GFED
@ABC0.5

−2

XOR

Figure 5.9: Neural network realizing the XOR
function. Threshold values (as far as they are
existing) are located within the neurons.

5.3 A multilayer perceptron
contains more trainable
weight layers

A perceptron with two or more trainable
weight layers (called multilayer perceptron
or MLP) is more powerful than an SLP. As
we know, a singlelayer perceptron can di-
vide the input space by means of a hyper-
plane (in a two-dimensional input space
by means of a straight line). A two-
stage perceptron (two trainable weight lay-
ers, three neuron layers) can classify con-
vex polygons by further processing these
straight lines, e.g.
in the form "recognize
patterns lying above straight line 1, be-
low straight line 2 and below straight line
3". Thus, we – metaphorically speaking
- took an SLP with several output neu-
rons and "attached" another SLP (upper

more planes

it

part of ﬁg. 5.10 on the facing page). A
multilayer perceptron represents an uni-
versal function approximator, which
is proven by the Theorem of Cybenko
[Cyb89].
Another trainable weight layer proceeds
analogously, now with the convex poly-
gons. Those can be added, subtracted or
somehow processed with other operations
(lower part of ﬁg. 5.10 on the next page).
Generally,
can be mathematically
proven that even a multilayer perceptron
with one layer of hidden neurons can ar-
bitrarily precisely approximate functions
with only ﬁnitely many discontinuities as
well as their ﬁrst derivatives. Unfortu-
nately, this proof is not constructive and
therefore it is left to us to ﬁnd the correct
number of neurons and weights.
In the following we want
to use a
widespread abbreviated form for diﬀerent
multilayer perceptrons: We denote a two-
stage perceptron with 5 neurons in the in-
put layer, 3 neurons in the hidden layer
and 4 neurons in the output layer as a 5-
3-4-MLP.
Deﬁnition 5.7 (Multilayer perceptron).
Perceptrons with more than one layer of
variably weighted connections are referred
to as multilayer perceptrons (MLP).
An n-layer or n-stage perceptron has
thereby exactly n variable weight layers
and n + 1 neuron layers (the retina is dis-
regarded here) with neuron layer 1 being
the input layer.

Since three-stage perceptrons can classify
sets of any form by combining and sepa-

3-stage
MLP is
suﬃcient

84

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

 

~





dkriesel.com

5.3 The multilayer perceptron

i2

i1

GFED
@ABC
GFED
@ABC
*UUUUUUUUUUUUUUUUUUUUUUUUU
tjjjjjjjjjjjjjjjjjjjjjjjjj
@@@@@@@@@
@@@@@@@@@


GFED
@ABCh3
GFED
@ABCh2
GFED
@ABCh1
'PPPPPPPPPPPPPPPPP
wooooooooooooooooo
?>=<
89:;Ω

i1

i2

GFED
@ABC
GFED
@ABC
 @@@@@@@@@
 @@@@@@@@@
~~~~~~~~~~
~~~~~~~~~~
GFED
@ABCh1
GFED
@ABCh2
GFED
@ABCh3
GFED
GFED
@ABCh6
GFED
@ABCh4
@ABCh5
'PPPPPPPPPPPPPPPPP
wnnnnnnnnnnnnnnnnn
 @@@@@@@@@
~~~~~~~~~~
GFED
@ABCh7
GFED
@ABCh8
@@@@@@@@@
~~~~~~~~~
?>=<
89:;Ω

Figure 5.10: We know that an SLP represents a straight line. With 2 trainable weight layers,
several straight lines can be combined to form convex polygons (above). By using 3 trainable
weight layers several polygons can be formed into arbitrary sets (below).

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

85



*


t
'


w


~


 
'
'
)
)
*
*
t
t
u
u
w
w
~


 
'
-
-
 
,
,


*
*


t
t
~
r
r
w
q
q




Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

n
1
2
3
4

classiﬁable sets
hyperplane
convex polygon
any set
any set as well, i.e. no
advantage

Table 5.3: Representation of which perceptron
can classify which types of sets with n being the
number of trainable weight layers.

rating arbitrarily many convex polygons,
another step will not be advantageous
with respect to function representations.
Be cautious when reading the literature:
There are many diﬀerent deﬁnitions of
what is counted as a layer. Some sources
count the neuron layers, some count the
weight layers. Some sources include the
retina, some the trainable weight layers.
Some exclude (for some reason) the out-
put neuron layer.
In this work, I chose
the deﬁnition that provides, in my opinion,
the most information about the learning
capabilities – and I will use it cosistently.
Remember: An n-stage perceptron has ex-
actly n trainable weight layers. You can
ﬁnd a summary of which perceptrons can
classify which types of sets in table 5.3.
We now want to face the challenge of train-
ing perceptrons with more than one weight
layer.

5.4 Backpropagation of error
generalizes the delta rule
to allow for MLP training

Next, I want to derive and explain the
backpropagation of error learning rule
(abbreviated: backpropagation, backprop
or BP), which can be used to train multi-
stage perceptrons with semi-linear3 activa-
tion functions. Binary threshold functions
and other non-diﬀerentiable functions are
no longer supported, but that doesn’t mat-
ter: We have seen that the Fermi func-
tion or the hyperbolic tangent can arbi-
trarily approximate the binary threshold
function by means of a temperature pa-
rameter T. To a large extent I will fol-
low the derivation according to [Zel94] and
[MR86]. Once again I want to point out
that this procedure had previously been
published by Paul Werbos in [Wer74]
but had consideraby less readers than in
[MR86].

Backpropagation is a gradient descent pro-
cedure (including all strengths and weak-
nesses of the gradient descent) with the
error function Err(W) receiving all n
weights as arguments (ﬁg. 5.5 on page 78)
and assigning them to the output error, i.e.
being n-dimensional. On Err(W) a point
of small error or even a point of the small-
est error is sought by means of the gradi-
ent descent. Thus, in analogy to the delta
rule, backpropagation trains the weights
of the neural network. And it is exactly

3 Semilinear functions are monotonous and diﬀeren-

tiable – but generally they are not linear.

86

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.4 Backpropagation of error

the delta rule or its variable δi for a neu-
ron i which is expanded from one trainable
weight layer to several ones by backpropa-
gation.

5.4.1 The derivation is similar to

the one of the delta rule, but
with a generalized delta

Let us deﬁne in advance that the network
input of the individual neurons i results
from the weighted sum. Furthermore, as
with the derivation of the delta rule, let
op,i, netp,i etc. be deﬁned as the already
familiar oi, neti, etc. under the input pat-
tern p we used for the training. Let the
output function be the identity again, thus
oi = fact(netp,i) holds for any neuron i.
Since this is a generalization of the delta
rule, we use the same formula framework
as with the delta rule (equation 5.20 on
page 81). As already indicated, we have
to generalize the variable δ for every neu-
ron.
First of all: Where is the neuron for which
we want to calculate δ? It is obvious to
select an arbitrary inner neuron h having
a set K of predecessor neurons k as well
as a set of L successor neurons l, which
are also inner neurons (see ﬁg. 5.11). It
is therefore irrelevant whether the prede-
cessor neurons are already the input neu-
rons.
Now we perform the same derivation as
for the delta rule and split functions by
means the chain rule.
I will not discuss
this derivation in great detail, but the prin-
cipal is similar to that of the delta rule (the

general-
ization
of δ

/.-,
()*+
/.-,
()*+
/.-,
()*+
&LLLLLLLLLLLLLLL
==========
ONML
HIJKΣ
xrrrrrrrrrrrrrrr
 
/.-,
/.-,
/.-,
()*+
()*+
()*+

fact

wppppppp
NNNNNNN

. . .

pppppppp

wk,h

h
'NNNNNNNN

wh,l

. . .

?>=<
89:;k
?>=<
89:;l

K

H

L

Figure 5.11: Illustration of the position of our
neuron h within the neural network. It is lying in
layer H, the preceding layer is K, the subsequent
layer is L.

diﬀerences are, as already mentioned, in
the generalized δ). We initially derive the
error function Err according to a weight
wk,h.

∂Err(wk,h)

∂wk,h

· ∂neth
∂wk,h

= ∂Err
| {z }
∂neth
=−δh

(5.23)

The ﬁrst factor of equation 5.23 is −δh,
which we will deal with later in this text.
The numerator of the second factor of the
equation includes the network input, i.e.
the weighted sum is included in the numer-
ator so that we can immediately derive it.
Again, all summands of the sum drop out
apart from the summand containing wk,h.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

87

&



w
x
 


'
Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

This summand is referred to as wk,h· ok. If
we calculate the derivative, the output of
neuron k becomes:

According to the deﬁnition of the multi-
dimensional chain rule, we immediately ob-
tain equation 5.31:

∂neth
∂wk,h

k∈K wk,hok
∂wk,h

= ∂P

= ok

(cid:18)

=X

l∈L

− ∂Err
∂oh

(5.24)

(5.25)

(cid:19)

− ∂Err
∂netl

· ∂netl
∂oh

(5.31)

As promised, we will now discuss the −δh
of equation 5.23 on the previous page,
which is split up again according of the
chain rule:

The sum in equation 5.31 contains two fac-
tors. Now we want to discuss these factors
being added over the subsequent layer L.
We simply calculate the second factor in
the following equation 5.33:

δh = − ∂Err
∂neth
= − ∂Err
∂oh

· ∂oh
∂neth

(5.26)

(5.27)

∂netl
∂oh

= ∂P

= wh,l

h∈H wh,l · oh

∂oh

(5.32)

(5.33)

The derivation of the output according to
the network input (the second factor in
equation 5.27) clearly equals the deriva-
tion of the activation function according
to the network input:

The same applies for the ﬁrst factor accord-
ing to the deﬁnition of our δ:

− ∂Err
∂netl

= δl

(5.34)

∂oh
∂neth

= ∂fact(neth)
∂neth
0(neth)
= fact

(5.28)

(5.29)

Now we insert:

⇒ − ∂Err
∂oh

=X

l∈L

δlwh,l

(5.35)

Consider this an important passage! We
now analogously derive the ﬁrst factor in
equation 5.27. Therefore, we have to point
out that the derivation of the error func-
tion according to the output of an inner
neuron layer depends on the vector of all
network inputs of the next following layer.
This is reﬂected in equation 5.30:
= − ∂Err(netl1, . . . , netl|L|)
− ∂Err
∂oh

(5.30)

∂oh

You can ﬁnd a graphic version of the δ
generalization including all splittings in
ﬁg. 5.12 on the facing page.

The reader might already have noticed
that some intermediate results were shown
in frames. Exactly those intermediate re-
sults were highlighted in that way, which
are a factor in the change in weight of
wk,h. If the aforementioned equations are

88

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.4 Backpropagation of error

δh

− ∂Err
∂neth

∂oh
∂neth

− ∂Err

∂oh

f0
act(neth)

− ∂Err
∂netl

δl

l∈L

∂netl
∂oh

P
∂P

wh,l·oh

h∈H
∂oh

wh,l

Figure 5.12: Graphical representation of the equations (by equal signs) and chain rule splittings
(by arrows) in the framework of the backpropagation derivation. The leaves of the tree reﬂect the
ﬁnal results from the generalization of δ, which are framed in the derivation.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

89









Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

combined with the highlighted intermedi-
ate results, the outcome of this will be the
wanted change in weight ∆wk,h to

act(neth) ·X

0

l∈L

δh = f

∆wk,h = ηokδh with

(5.36)

(δlwh,l)

– of course only in case of h being an inner
neuron (otherweise there would not be a
subsequent layer L).
The case of h being an output neuron has
already been discussed during the deriva-
tion of the delta rule. All in all, the re-
sult is the generalization of the delta rule,
called backpropagation of error:
∆wk,h = ηokδh with
δh =

act(neth) · (th − yh) (h outside)
f0
f0

l∈L(δlwh,l) (h inside)
(5.37)
In contrast to the delta rule, δ is treated
diﬀerently depending on whether h is an
output or an inner (i.e. hidden) neuron:

act(neth) ·P

(

1. If h is an output neuron, then

δp,h = f

act(netp,h) · (tp,h − yp,h)
0

(5.38)
Thus, under our training pattern p
the weight wk,h from k to h is changed
proportionally according to

. the learning rate η,
. the output op,k of the predeces-

sor neuron k,

. the gradient of the activation
function at the position of the
network input of the successor
neuron f0

act(netp,h) and

Teach. Input
changed for
the outer
weight layer

back-
propagation
for inner
layers

. the diﬀerence between teaching
input tp,h and output yp,h of the
successor neuron h.

In this case, backpropagation is work-
ing on two neuron layers, the output
layer with the successor neuron h and
the preceding layer with the predeces-
sor neuron k.

2. If h is an inner, hidden neuron, then
(δp,l · wh,l)

act(netp,h) ·X

δp,h = f

0

l∈L

(5.39)
holds.
I want to explicitly mention
that backpropagation is now working
on three layers. Here, neuron k is
the predecessor of the connection to
be changed with the weight wk,h, the
neuron h is the successor of the con-
nection to be changed and the neu-
rons l are lying in the layer follow-
ing the successor neuron. Thus, ac-
cording to our training pattern p, the
weight wk,h from k to h is proportion-
ally changed according to
. the learning rate η,
. the output of the predecessor

neuron op,k,

. the gradient of the activation
function at the position of the
network input of the successor
neuron f0

act(netp,h),

. as well as, and this
according

the
diﬀerence,
the
weighted sum of the changes in
weight to all neurons following h,

P
l∈L(δp,l · wh,l).

is
to

90

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

backprop
expands
delta rule

dkriesel.com

5.4 Backpropagation of error

Deﬁnition 5.8 (Backpropagation). If we
summarize formulas 5.38 on the preceding
page and 5.39 on the facing page, we re-
ceive the following ﬁnal formula for back-
propagation (the identiﬁers p are om-
mited for reasons of clarity):

(

∆wk,h = ηokδh with
δh =

act(neth) ·P

act(neth) · (th − yh) (h outside)
f0
f0

l∈L(δlwh,l) (h inside)
(5.40)

SNIPE: An online variant of backpro-
pagation is implemented in the method
trainBackpropagationOfError within the
class NeuralNetwork.

5.4.2 Heading back: Boiling

backpropagation down to
delta rule

As explained above, the delta rule is a
special case of backpropagation for one-
stage perceptrons and linear activation
functions – I want to brieﬂy explain this
circumstance and develop the delta rule
out of backpropagation in order to aug-
ment the understanding of both rules. We
have seen that backpropagation is deﬁned
by

(

∆wk,h = ηokδh with
δh =

act(neth) ·P

act(neth) · (th − yh) (h outside)
f0
f0

l∈L(δlwh,l) (h inside)
(5.41)

It is obvious that backpropagation ini-
tially processes the last weight layer di-
rectly by means of the teaching input and
then works backwards from layer to layer
while considering each preceding change in
weights. Thus, the teaching input leaves
traces in all weight layers. Here I describe
the ﬁrst (delta rule) and the second part
of backpropagation (generalized delta rule
on more layers) in one go, which may meet
the requirements of the matter but not
of the research. The ﬁrst part is obvious,
which you will soon see in the framework
of a mathematical gimmick. Decades of
development time and work lie between the
ﬁrst and the second, recursive part. Like
many groundbreaking inventions,
it was
not until its development that it was recog-
nized how plausible this invention was.

Since we only use it for one-stage percep-
trons, the second part of backpropagation
(light-colored) is omitted without substitu-
tion. The result is:

∆wk,h = ηokδh with
δh = f0

act(neth) · (th − oh)

(5.42)

Furthermore, we only want to use linear
activation functions so that f0
act (light-
colored) is constant.
As is generally
known, constants can be combined, and
therefore we directly merge the constant
derivative f0
act and (being constant for at
least one lerning cycle) the learning rate η
(also light-colored) in η. Thus, the result
is:

∆wk,h = ηokδh = ηok · (th − oh)

(5.43)

This exactly corresponds to the delta rule
deﬁnition.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

91

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

5.4.3 The selection of the learning
rate has heavy inﬂuence on
the learning process

5.4.3.1 Variation of the learning rate

over time

In the meantime we have often seen that
the change in weight is, in any case, pro-
portional to the learning rate η. Thus, the
selection of η is crucial for the behaviour
of backpropagation and for learning proce-
dures in general.

how fast
will be
learned? Deﬁnition 5.9 (Learning rate). Speed
and accuracy of a learning procedure can
always be controlled by and are always pro-
portional to a learning rate which is writ-
ten as η.

η(cid:73)

If the value of the chosen η is too large,
the jumps on the error surface are also
too large and, for example, narrow valleys
could simply be jumped over. Addition-
ally, the movements across the error sur-
face would be very uncontrolled. Thus, a
small η is the desired input, which, how-
ever, can cost a huge, often unacceptable
amount of time. Experience shows that
good learning rate values are in the range
of

0.01 ≤ η ≤ 0.9.

The selection of η signiﬁcantly depends on
the problem, the network and the training
data, so that it is barely possible to give
practical advise. But for instance it is pop-
ular to start with a relatively large η, e.g.
0.9, and to slowly decrease it down to 0.1.
For simpler problems η can often be kept
constant.

During training, another stylistic device
can be a variable learning rate: In the
beginning, a large learning rate leads to
good results, but later it results in inac-
curate learning. A smaller learning rate
is more time-consuming, but the result is
more precise. Thus, during the learning
process the learning rate needs to be de-
creased by one order of magnitude once or
repeatedly.

A common error (which also seems to be a
very neat solution at ﬁrst glance) is to con-
tinually decrease the learning rate. Here
it quickly happens that the descent of the
learning rate is larger than the ascent of
a hill of the error function we are climb-
ing. The result is that we simply get stuck
at this ascent. Solution: Rather reduce
the learning rate gradually as mentioned
above.

5.4.3.2 Diﬀerent layers – Diﬀerent

learning rates

The farer we move away from the out-
put layer during the learning process, the
slower backpropagation is learning. Thus,
it is a good idea to select a larger learning
rate for the weight layers close to the in-
put layer than for the weight layers close
to the output layer.

92

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.5 Resilient backpropagation

5.5 Resilient backpropagation

is an extension to
backpropagation of error

We have just raised two backpropagation-
speciﬁc properties that can occasionally be
a problem (in addition to those which are
already caused by gradient descent itself):
On the one hand, users of backpropaga-
tion can choose a bad learning rate. On
the other hand, the further the weights are
from the output layer, the slower backpro-
pagation learns. For this reason, Mar-
tin Riedmiller et al.
enhanced back-
propagation and called their version re-
silient backpropagation (short Rprop)
[RB93, Rie94].
I want to compare back-
propagation and Rprop, without explic-
itly declaring one version superior to the
other. Before actually dealing with formu-
las, let us informally compare the two pri-
mary ideas behind Rprop (and their con-
sequences) to the already familiar backpro-
pagation.

Learning rates: Backpropagation uses by
default a learning rate η, which is se-
lected by the user, and applies to the
entire network. It remains static un-
til it is manually changed. We have
already explored the disadvantages of
this approach. Here, Rprop pursues a
completely diﬀerent approach: there
is no global learning rate. First, each
weight wi,j has its own learning rate
ηi,j, and second, these learning rates
are not chosen by the user, but are au-
tomatically set by Rprop itself. Third,
the weight changes are not static but

are adapted for each time step of
Rprop. To account for the temporal
change, we have to correctly call it
ηi,j(t). This not only enables more
focused learning, also the problem of
an increasingly slowed down learning
throughout the layers is solved in an
elegant way.

Weight change: When using backpropa-
gation, weights are changed propor-
tionally to the gradient of the error
function. At ﬁrst glance, this is really
intuitive. However, we incorporate ev-
ery jagged feature of the error surface
into the weight changes. It is at least
questionable, whether this is always
useful. Here, Rprop takes other ways
as well: the amount of weight change
∆wi,j simply directly corresponds to
the automatically adjusted learning
rate ηi,j. Thus the change in weight is
not proportional to the gradient, it is
only inﬂuenced by the sign of the gra-
dient. Until now we still do not know
how exactly the ηi,j are adapted at
run time, but let me anticipate that
the resulting process looks consider- Much
ably less rugged than an error func-
tion.

smoother learning

In contrast to backprop the weight update
step is replaced and an additional step
for the adjustment of the learning rate is
added. Now how exactly are these ideas
being implemented?

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

93

One learning-
rate per
weight

ηi,j(cid:73)

automatic
learning rate
adjustment

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

change

in

Deﬁnition 5.10 (Weight
Rprop).

5.5.1 Weight changes are not

proportional to the gradient

Let us ﬁrst consider the change in weight.
We have already noticed that the weight-
speciﬁc learning rates directly serve as ab-
solute values for the changes of the re-
spective weights. There remains the ques-
tion of where the sign comes from – this
is a point at which the gradient comes
into play. As with the derivation of back-
propagation, we derive the error function
Err(W) by the individual weights wi,j and
obtain gradients ∂Err(W )
. Now, the big
∂wi,j
diﬀerence:
rather than multiplicatively
incorporating the absolute value of the
gradient into the weight change, we con-
sider only the sign of the gradient. The
gradient hence no longer determines the
strength, but only the direction of the
weight change.

If the sign of the gradient ∂Err(W )
is pos-
∂wi,j
itive, we must decrease the weight wi,j.
So the weight is reduced by ηi,j.
If the
sign of the gradient is negative, the weight
needs to be increased. So ηi,j is added to
it.
If the gradient is exactly 0, nothing
happens at all. Let us now create a for-
mula from this colloquial description. The
corresponding terms are aﬃxed with a (t)
to show that everything happens at the
same time step. This might decrease clar-
ity at ﬁrst glance, but is nevertheless im-
portant because we will soon look at an-
other formula that operates on diﬀerent
time steps.
Instead, we shorten the gra-
dient to: g = ∂Err(W )
∂wi,j

.

gradient
determines only
direction of the
updates



∆wi,j(t) =

−ηi,j(t),
+ηi,j(t),
0

if g(t) > 0
if g(t) < 0
otherwise.

(5.44)

We now know how the weights are changed
– now remains the question how the learn-
ing rates are adjusted. Finally, once we
have understood the overall system, we
will deal with the remaining details like ini-
tialization and some speciﬁc constants.

5.5.2 Many dynamically adjusted
learning rates instead of one
static

To adjust the learning rate ηi,j, we again
have to consider the associated gradients
g of two time steps: the gradient that has
just passed (t − 1) and the current one
(t). Again, only the sign of the gradient
matters, and we now must ask ourselves:
What can happen to the sign over two time
steps? It can stay the same, and it can
ﬂip.
If the sign changes from g(t − 1) to g(t),
we have skipped a local minimum in the
gradient. Hence, the last update was too
large and ηi,j(t) has to be reduced as com-
pared to the previous ηi,j(t − 1). One can
say, that the search needs to be more accu-
rate. In mathematical terms, we obtain a
new ηi,j(t) by multiplying the old ηi,j(t−1)
with a constant η↓, which is between 1 and (cid:74)η↓
0.
In this case we know that in the last
time step (t − 1) something went wrong –

94

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.5 Resilient backpropagation

hence we additionally reset the weight up-
date for the weight wi,j at time step (t) to
0, so that it not applied at all (not shown
in the following formula).

However, if the sign remains the same, one
can perform a (careful!) increase of ηi,j to
get past shallow areas of the error function.
Here we obtain our new ηi,j(t) by multiply-
ing the old ηi,j(t − 1) with a constant η↑

η↑(cid:73) which is greater than 1.

Deﬁnition 5.11 (Adaptation of learning
rates in Rprop).

g(t − 1)g(t) > 0
g(t − 1)g(t) < 0
otherwise.

(5.45)

η↑ηi,j(t − 1),

η↓ηi,j(t − 1),
ηi,j(t − 1)

ηi,j(t) =

Rprop only
learns
oﬄine

Caution: This also implies that Rprop is
exclusively designed for oﬄine. If the gra-
dients do not have a certain continuity, the
learning process slows down to the lowest
rates (and remains there). When learning
online, one changes – loosely speaking –
the error function with each new epoch,
since it is based on only one training pat-
tern. This may be often well applicable
in backpropagation and it is very often
even faster than the oﬄine version, which
is why it is used there frequently. It lacks,
however, a clear mathematical motivation,
and that is exactly what we need here.

(cid:74)ηmin(cid:74)ηmax

5.5.3 We are still missing a few

details to use Rprop in
practice

A few minor issues remain unanswered,
namely
1. How large are η↑ and η↓ (i.e. how
much are learning rates reinforced or
weakened)?

2. How to choose ηi,j(0) (i.e. how are
the weight-speciﬁc learning rates ini-
tialized)?4

3. What are the upper and lower bounds

ηmin and ηmax for ηi,j set?

We now answer these questions with a
quick motivation. The initial value for the
learning rates should be somewhere in the
order of the initialization of the weights.
ηi,j(0) = 0.1 has proven to be a good
choice. The authors of the Rprop paper
explain in an obvious way that this value
– as long as it is positive and without an ex-
orbitantly high absolute value – does not
need to be dealt with very critically, as
it will be quickly overridden by the auto-
matic adaptation anyway.
Equally uncritical is ηmax, for which they
recommend, without further mathemati-
cal justiﬁcation, a value of 50 which is used
throughout most of the literature. One
can set this parameter to lower values in
order to allow only very cautious updates.
Small update steps should be allowed in
any case, so we set ηmin = 10−6.
4 Protipp: since the ηi,j can be changed only by
multiplication, 0 would be a rather suboptimal ini-
tialization :-)

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

95

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

Now we have left only the parameters η↑
and η↓. Let us start with η↓: If this value
is used, we have skipped a minimum, from
which we do not know where exactly it lies
on the skipped track. Analogous to the
procedure of binary search, where the tar-
get object is often skipped as well, we as-
sume it was in the middle of the skipped
track. So we need to halve the learning
rate, which is why the canonical choice
η↓ = 0.5 is being selected.
If the value
of η↑ is used, learning rates shall be in-
creased with caution. Here we cannot gen-
eralize the principle of binary search and
simply use the value 2.0, otherwise the
learning rate update will end up consist-
ing almost exclusively of changes in direc-
tion. Independent of the particular prob-
lems, a value of η↑ = 1.2 has proven to
be promising. Slight changes of this value
have not signiﬁcantly aﬀected the rate of
convergence. This fact allowed for setting
this value as a constant as well.

With advancing computational capabili-
ties of computers one can observe a more
and more widespread distribution of net-
works that consist of a big number of lay-
ers, i.e. deep networks. For such net-
works it is crucial to prefer Rprop over the
original backpropagation, because back-
prop, as already indicated,
learns very
slowly at weights wich are far from the
output layer. For problems with a smaller
number of layers, I would recommend test-
ing the more widespread backpropagation
(with both oﬄine and online learning) and
the less common Rprop equivalently.

In Snipe

resilient backpropa-
SNIPE:
gation is
supported via the method
trainResilientBackpropagation of the
class NeuralNetwork. Furthermore, you
can also use an additional
improvement
to resilient propagation, which is, however,
not dealt with in this work. There are get-
ters and setters for the diﬀerent parameters
of Rprop.

5.6 Backpropagation has

often been extended and
altered besides Rprop

Backpropagation has often been extended.
Many of these extensions can simply be im-
plemented as optional features of backpro-
pagation in order to have a larger scope for
testing. In the following I want to brieﬂy
describe some of them.

5.6.1 Adding momentum to

learning

Let us assume to descent a steep slope
on skis - what prevents us from immedi-
ately stopping at the edge of the slope
to the plateau? Exactly - our momen-
tum. With backpropagation the momen-
tum term [RHW86b] is responsible for the
fact that a kind of moment of
inertia
(momentum) is added to every step size
(ﬁg. 5.13 on the next page), by always
adding a fraction of the previous change
to every new change in weight:
(∆pwi,j)now = ηop,iδp,j+α·(∆pwi,j)previous.

96

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Rprop is very
good for
deep networks

dkriesel.com

5.6 Further variations and extensions to backpropagation

moment of
inertia

α(cid:73)

Of course, this notation is only used for
a better understanding. Generally, as al-
ready deﬁned by the concept of time, when
referring to the current cycle as (t), then
the previous cycle is identiﬁed by (t − 1),
which is continued successively. And now
we come to the formal deﬁnition of the mo-
mentum term:
Deﬁnition 5.12 (Momentum term). The
variation of backpropagation by means of
the momentum term is deﬁned as fol-
lows:
∆wi,j(t) = ηoiδj + α · ∆wi,j(t − 1) (5.46)

We accelerate on plateaus (avoiding quasi-
standstill on plateaus) and slow down on
craggy surfaces (preventing oscillations).
Moreover, the eﬀect of inertia can be var-
ied via the prefactor α,
common val-
ues are between 0.6 und 0.9. Addition-
ally, the momentum enables the positive
eﬀect that our skier swings back and
forth several times in a minimum, and ﬁ-
nally lands in the minimum. Despite its
nice one-dimensional appearance, the oth-
erwise very rare error of leaving good min-
ima unfortunately occurs more frequently
because of the momentum term – which
means that this is again no optimal solu-
tion (but we are by now accustomed to
this condition).

5.6.2 Flat spot elimination prevents

neurons from getting stuck

It must be pointed out that with the hy-
perbolic tangent as well as with the Fermi

Figure 5.13: We want to execute the gradient
descent like a skier crossing a slope, who would
hardly stop immediately at the edge to the
plateau.

neurons
get stuck

function the derivative outside of the close
proximity of Θ is nearly 0. This results
in the fact that it becomes very diﬃcult
to move neurons away from the limits of
the activation (ﬂat spots), which could ex-
tremely extend the learning time. This
problem can be dealt with by modifying
the derivative, for example by adding a
constant (e.g. 0.1), which is called ﬂat
spot elimination or – more colloquial –
fudging.

It is an interesting observation, that suc-
cess has also been achieved by using deriva-
tives deﬁned as constants [Fah88]. A nice
example making use of this eﬀect is the
fast hyperbolic tangent approximation by
Anguita et al. introduced in section 3.2.6
on page 37. In the outer regions of it’s (as

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

97

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

well approximated and accelerated) deriva-
tive, it makes use of a small constant.

5.6.4 Weight decay: Punishment of

large weights

5.6.3 The second derivative can be

used, too

According to David Parker [Par87],
Second order backpropagation also us-
ese the second gradient, i.e.
the second
multi-dimensional derivative of the error
function, to obtain more precise estimates
of the correct ∆wi,j. Even higher deriva-
tives only rarely improve the estimations.
Thus, less training cycles are needed but
those require much more computational ef-
fort.
In general, we use further derivatives (i.e.
Hessian matrices, since the functions are
multidimensional) for higher order meth-
ods. As expected, the procedures reduce
the number of learning epochs, but signiﬁ-
cantly increase the computational eﬀort of
the individual epochs. So in the end these
procedures often need more learning time
than backpropagation.
The quickpropagation learning proce-
dure [Fah88] uses the second derivative of
the error propagation and locally under-
stands the error function to be a parabola.
We analytically determine the vertex (i.e.
the lowest point) of the said parabola and
directly jump to this point. Thus, this
learning procedure is a second-order proce-
dure. Of course, this does not work with
error surfaces that cannot locally be ap-
proximated by a parabola (certainly it is
not always possible to directly say whether
this is the case).

The weight decay according to Paul
Werbos [Wer88] is a modiﬁcation that ex-
tends the error by a term punishing large
weights. So the error under weight de-
cay

ErrWD

does not only increase proportionally to (cid:74)ErrWD
the actual error but also proportionally to
the square of the weights. As a result the
network is keeping the weights small dur-
ing learning.

ErrWD = Err + β · 1
|
2

X
{z

w∈W

punishment

(5.47)

(w)2

}

This approach is inspired by nature where
synaptic weights cannot become inﬁnitely
strong as well. Additionally, due to these
small weights, the error function often
shows weaker ﬂuctuations, allowing easier
and more controlled learning.
The prefactor 1
2 again resulted from sim-
ple pragmatics. The factor β controls the (cid:74)β
strength of punishment: Values from 0.001
to 0.02 are often used here.

keep weights
small

5.6.5 Cutting networks down:

Pruning and Optimal Brain
Damage

If we have executed the weight decay long
enough and notice that for a neuron in
the input layer all successor weights are
0 or close to 0, we can remove the neuron,

prune the
network

98

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.7 Initial conﬁguration of a multilayer perceptron

hence losing this neuron and some weights
and thereby reduce the possibility that the
network will memorize. This procedure is
called pruning.

Such a method to detect and delete un-
necessary weights and neurons is referred
to as optimal brain damage [lCDS90].
I only want to describe it brieﬂy: The
mean error per output neuron is composed
of two competing terms. While one term,
as usual, considers the diﬀerence between
output and teaching input, the other one
tries to "press" a weight towards 0.
If a
weight is strongly needed to minimize the
error, the ﬁrst term will win. If this is not
the case, the second term will win. Neu-
rons which only have zero weights can be
pruned again in the end.

There are many other variations of back-
prop and whole books only about this
subject, but since my aim is to oﬀer an
overview of neural networks, I just want
to mention the variations above as a moti-
vation to read on.

For some of these extensions it is obvi-
ous that they cannot only be applied to
feedforward networks with backpropaga-
tion learning procedures.

We have gotten to know backpropagation
and feedforward topology – now we have
to learn how to build a neural network. It
is of course impossible to fully communi-
cate this experience in the framework of
this work. To obtain at least some of
this knowledge, I now advise you to deal
with some of the exemplary problems from
4.6.

5.7 Getting started – Initial

conﬁguration of a
multilayer perceptron

After having discussed the backpropaga-
tion of error learning procedure and know-
ing how to train an existing network, it
would be useful to consider how to imple-
ment such a network.

5.7.1 Number of layers: Two or

three may often do the job,
but more are also used

Let us begin with the trivial circumstance
that a network should have one layer of in-
put neurons and one layer of output neu-
rons, which results in at least two layers.
Additionally, we need – as we have already
learned during the examination of linear
separability – at least one hidden layer of
neurons, if our problem is not linearly sep-
arable (which is, as we have seen, very
likely).
It is possible, as already mentioned, to
mathematically prove that this MLP with
one hidden neuron layer is already capable
of approximating arbitrary functions with
any accuracy 5 – but it is necessary not
only to discuss the representability of a
problem by means of a perceptron but also
the learnability. Representability means
that a perceptron can, in principle, realize

5 Note: We have not indicated the number of neu-
rons in the hidden layer, we only mentioned the
hypothetical possibility.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

99

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

a mapping - but learnability means that
we are also able to teach it.
In this respect, experience shows that two
hidden neuron layers (or three trainable
weight layers) can be very useful to solve
a problem, since many problems can be
represented by a hidden layer but are very
diﬃcult to learn.
One should keep in mind that any ad-
ditional
layer generates additional sub-
minima of the error function in which we
can get stuck. All these things consid-
ered, a promising way is to try it with
one hidden layer at ﬁrst and if that fails,
retry with two layers. Only if that fails,
one should consider more layers. However,
given the increasing calculation power of
current computers, deep networks with
a lot of layers are also used with success.

5.7.2 The number of neurons has

to be tested

The number of neurons (apart from input
and output layer, where the number of in-
put and output neurons is already deﬁned
by the problem statement) principally cor-
responds to the number of free parameters
of the problem to be represented.
Since we have already discussed the net-
work capacity with respect to memorizing
or a too imprecise problem representation,
it is clear that our goal is to have as few
free parameters as possible but as many as
necessary.
But we also know that there is no stan-
dard solution for the question of how many

neurons should be used. Thus, the most
useful approach is to initially train with
only a few neurons and to repeatedly train
new networks with more neurons until the
result signiﬁcantly improves and, particu-
larly, the generalization performance is not
aﬀected (bottom-up approach).

5.7.3 Selecting an activation

function

Another very important parameter for the
way of information processing of a neural
network is the selection of an activa-
tion function. The activation function
for input neurons is ﬁxed to the identity
function, since they do not process infor-
mation.

The ﬁrst question to be asked is whether
we actually want to use the same acti-
vation function in the hidden layer and
in the ouput layer – no one prevents us
from choosing diﬀerent functions. Gener-
ally, the activation function is the same for
all hidden neurons as well as for the output
neurons respectively.

For tasks of function approximation it
has been found reasonable to use the hy-
perbolic tangent (left part of ﬁg. 5.14 on
page 102) as activation function of the hid-
den neurons, while a linear activation func-
tion is used in the output. The latter is
absolutely necessary so that we do not gen-
erate a limited output intervall. Contrary
to the input layer which uses linear acti-
vation functions as well, the output layer
still processes information, because it has

100

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

5.8 The 8-3-8 encoding problem and related problems

threshold values. However, linear activa-
tion functions in the output can also cause
huge learning steps and jumping over good
minima in the error surface. This can be
avoided by setting the learning rate to very
small values in the output layer.
An unlimited output interval is not essen-
tial for pattern recognition tasks6.
If
the hyperbolic tangent is used in any case,
the output interval will be a bit larger. Un-
like with the hyperbolic tangent, with the
Fermi function (right part of ﬁg. 5.14 on
the following page) it is diﬃcult to learn
something far from the threshold value
(where its result is close to 0). However,
here a lot of freedom is given for selecting
an activation function. But generally, the
disadvantage of sigmoid functions is the
fact that they hardly learn something for
values far from thei threshold value, unless
the network is modiﬁed.

range of random values could be the in-
terval [−0.5; 0.5] not including 0 or values
very close to 0. This random initialization
has a nice side eﬀect: Chances are that
the average of network inputs is close to 0,
a value that hits (in most activation func-
tions) the region of the greatest derivative,
allowing for strong learning impulses right
from the start of learning.

a

In Snipe, weights are initial-
SNIPE:
synapse
initial-
ized randomly (if
The maximum
ization is wanted).
synapse
of
absolute weight value
initialized at
set
in
a NeuralNetworkDescriptor using the
method setSynapseInitialRange.

a
random can be

5.8 The 8-3-8 encoding
problem and related
problems

5.7.4 Weights should be initialized
with small, randomly chosen
values

The initialization of weights is not as triv-
ial as one might think. If they are simply
initialized with 0, there will be no change
in weights at all. If they are all initialized
by the same value, they will all change
equally during training. The simple so-
lution of this problem is called symme-
try breaking, which is the initialization
of weights with small random values. The
6 Generally, pattern recognition is understood as a
special case of function approximation with a few
discrete output possibilities.

The 8-3-8 encoding problem is a clas-
sic among the multilayer perceptron test
training problems.
In our MLP we
have an input layer with eight neurons
i1, i2, . . . , i8, an output layer with eight
neurons Ω1, Ω2, . . . , Ω8 and one hidden
layer with three neurons. Thus, this net-
work represents a function B8 → B8. Now
the training task is that an input of a value
1 into the neuron ij should lead to an out-
put of a value 1 from the neuron Ωj (only
one neuron should be activated, which re-
sults in 8 training samples.

During the analysis of the trained network
we will see that the network with the 3

random
initial
weights

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

101

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

Figure 5.14: As a reminder the illustration of the hyperbolic tangent (left) and the Fermi function
(right). The Fermi function was expanded by a temperature parameter. The original Fermi function
is thereby represented by dark colors, the temperature parameter of the modiﬁed Fermi functions
are, ordered ascending by steepness, 1

2, 1

5, 1

10 and 1
25.

hidden neurons represents some kind of bi-
nary encoding and that the above map-
ping is possible (assumed training time:
≈ 104 epochs). Thus, our network is a ma-
chine in which the input is ﬁrst encoded
and afterwards decoded again.
Analogously, we can train a 1024-10-1024
encoding problem. But is it possible to
improve the eﬃciency of this procedure?
Could there be,
for example, a 1024-9-
1024- or an 8-2-8-encoding network?
Yes, even that is possible, since the net-
work does not depend on binary encodings:
Thus, an 8-2-8 network is suﬃcient for our
problem. But the encoding of the network
is far more diﬃcult to understand (ﬁg. 5.15
on the next page) and the training of the
networks requires a lot more time.

The

static

method
SNIPE:
getEncoderSampleLesson in the
class
TrainingSampleLesson allows for creating
simple training sample lessons of arbitrary

dimensionality for encoder problems like
the above.

An 8-1-8 network, however, does not work,
since the possibility that the output of one
neuron is compensated by another one is
essential, and if there is only one hidden
neuron, there is certainly no compensatory
neuron.

Exercises

Exercise 8. Fig. 5.4 on page 75 shows
a small network for the boolean functions
AND and OR. Write tables with all computa-
tional parameters of neural networks (e.g.
network input, activation etc.). Perform
the calculations for the four possible in-
puts of the networks and write down the
values of these variables for each input. Do
the same for the XOR network (ﬁg. 5.9 on
page 84).

102

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

−1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4tanh(x)xHyperbolic Tangent 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4f(x)xFermi Function with Temperature Parameterdkriesel.com

5.8 The 8-3-8 encoding problem and related problems

Exercise 9.
1. List all boolean functions B3 → B1,
that are linearly separable and char-
acterize them exactly.

2. List those that are not linearly sepa-
rable and characterize them exactly,
too.

Exercise 10. A simple 2-1 network shall
be trained with one single pattern by
means of backpropagation of error and
η = 0.1. Verify if the error
Err = Errp = 1

2(t − y)2

converges and if so, at what value. How
does the error curve look like? Let the
pattern (p, t) be deﬁned by p = (p1, p2) =
(0.3, 0.7) and tΩ = 0.4. Randomly initalize
the weights in the interval [1;−1].
Exercise 11. A one-stage perceptron
with two input neurons, bias neuron
and binary threshold function as activa-
tion function divides the two-dimensional
space into two regions by means of a
straight line g. Analytically calculate a
set of weight values for such a perceptron
so that the following set P of the 6 pat-
terns of the form (p1, p2, tΩ) with ε (cid:28) 1 is
correctly classiﬁed.

P ={(0, 0,−1);
(2,−1, 1);
(7 + ε, 3 − ε, 1);
(7 − ε, 3 + ε,−1);
(0,−2 − ε, 1);
(0 − ε,−2,−1)}

Figure 5.15: Illustration of the functionality of
8-2-8 network encoding. The marked points rep-
resent the vectors of the inner neuron activation
associated to the samples. As you can see, it
is possible to ﬁnd inner activation formations so
that each point can be separated from the rest
of the points by a straight line. The illustration
shows an exemplary separation of one point.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

103

Chapter 5 The perceptron, backpropagation and its variants

dkriesel.com

Exercise 12. Calculate in a comprehen-
sible way one vector ∆W of all changes in
weight by means of the backpropagation of
error procedure with η = 1. Let a 2-2-1
MLP with bias neuron be given and let the
pattern be deﬁned by

p = (p1, p2, tΩ) = (2, 0, 0.1).

For all weights with the target Ω the ini-
tial value of the weights should be 1. For
all other weights the initial value should
be 0.5. What is conspicuous about the
changes?

104

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Chapter 6
Radial basis functions

RBF networks approximate functions by stretching and compressing Gaussian
bells and then summing them spatially shifted. Description of their functions
and their learning process. Comparison with multilayer perceptrons.

According to Poggio and Girosi [PG89]
radial basis function networks (RBF net-
works) are a paradigm of neural networks,
which was developed considerably later
than that of perceptrons. Like percep-
trons, the RBF networks are built in layers.
But in this case, they have exactly three
layers, i.e. only one single layer of hidden
neurons.
Like perceptrons, the networks have a
feedforward structure and their layers are
completely linked. Here, the input layer
again does not participate in information
processing.
The RBF networks are -
like MLPs - universal function approxima-
tors.
Despite all things in common: What is the
diﬀerence between RBF networks and per-
ceptrons? The diﬀerence lies in the infor-
mation processing itself and in the compu-
tational rules within the neurons outside
of the input layer. So, in a moment we
will deﬁne a so far unknown type of neu-
rons.

6.1 Components and

structure of an RBF
network

Initially, we want to discuss colloquially
and then deﬁne some concepts concerning
RBF networks.

Output neurons: In an RBF network the
output neurons only contain the iden-
tity as activation function and one
weighted sum as propagation func-
tion. Thus, they do little more than
adding all input values and returning
the sum.

Hidden neurons are also called RBF neu-
rons (as well as the layer in which
they are located is referred to as RBF
layer). As propagation function, each
hidden neuron calculates a norm that
represents the distance between the
input to the network and the so-called
position of the neuron (center). This
is inserted into a radial activation

105

input
is linear
again

c(cid:73)

Position
in the input
space

Important!

only sums
up

Chapter 6 Radial basis functions

dkriesel.com

function which calculates and outputs
the activation of the neuron.

Deﬁnition 6.1 (RBF input neuron). Def-
inition and representation is identical to
the deﬁnition 5.1 on page 73 of the input
neuron.
Deﬁnition 6.2 (Center of an RBF neu-
ron). The center ch of an RBF neuron
h is the point in the input space where
the RBF neuron is located .
In general,
the closer the input vector is to the center
vector of an RBF neuron, the higher is its
activation.
Deﬁnition 6.3 (RBF neuron). The so-
called RBF neurons h have a propaga-
tion function fprop that determines the dis-
tance between the center ch of a neuron
and the input vector y. This distance rep-
resents the network input. Then the net-
work input is sent through a radial basis
function fact which returns the activation
or the output of the neuron. RBF neurons

are represented by the symbol WVUT
PQRS

Deﬁnition 6.4 (RBF output neuron).
RBF output neurons Ω use the
weighted sum as propagation function
fprop, and the identity as activation func-
tion fact. They are represented by the sym-

||c,x||
Gauß

.

bolONML
HIJKΣ

(cid:30) .

Deﬁnition 6.5 (RBF network). An
RBF network has exactly three layers in
the following order: The input layer con-
sisting of input neurons, the hidden layer
(also called RBF layer) consisting of RBF
neurons and the output layer consisting of

3 layers,
feedforward

RBF output neurons. Each layer is com-
pletely linked with the following one, short-
cuts do not exist (ﬁg. 6.1 on the next page)
– it is a feedforward topology. The connec-
tions between input layer and RBF layer
are unweighted, i.e.
they only transmit
the input. The connections between RBF
layer and output layer are weighted. The
original deﬁnition of an RBF network only
referred to an output neuron, but – in anal-
ogy to the perceptrons – it is apparent that
such a deﬁnition can be generalized. A
bias neuron is not used in RBF networks.
The set of input neurons shall be repre-
sented by I, the set of hidden neurons by (cid:74)I, H, O
H and the set of output neurons by O.

Therefore, the inner neurons are called ra-
dial basis neurons because from their def-
inition follows directly that all input vec-
tors with the same distance from the cen-
ter of a neuron also produce the same out-
put value (ﬁg. 6.2 on page 108).

6.2 Information processing of

an RBF network

Now the question is, what can be realized
by such a network and what is its purpose.
Let us go over the RBF network from top
to bottom: An RBF network receives the
input by means of the unweighted con-
nections. Then the input vector is sent
through a norm so that the result is a
scalar. This scalar (which, by the way, can
only be positive due to the norm) is pro-
cessed by a radial basis function, for exam-

106

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

6.2 Information processing of an RBF network

||c,x||
Gauß

GFED
@ABC(cid:30)
GFED
@ABC(cid:30)
+VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV
(RRRRRRRRRRRRRRRRRRRR
shhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
vllllllllllllllllllll
"EEEEEEEEEE
"EEEEEEEEEE
|yyyyyyyyyy
|yyyyyyyyyy
WVUT
PQRS
WVUT
PQRS
WVUT
PQRS
WVUT
PQRS
WVUT
PQRS
*VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV
thhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
(QQQQQQQQQQQQQQQQQQQQ
(QQQQQQQQQQQQQQQQQQQQ
vmmmmmmmmmmmmmmmmmmmm
vmmmmmmmmmmmmmmmmmmmm
!CCCCCCCCCC
!CCCCCCCCCC
!CCCCCCCCCC
}{{{{{{{{{{
}{{{{{{{{{{
}{{{{{{{{{{
ONML
HIJKΣ
ONML
HIJKΣ
ONML
HIJKΣ

||c,x||
Gauß

||c,x||
Gauß

(cid:30)

(cid:30)

(cid:30)

||c,x||
Gauß

||c,x||
Gauß

i1, i2, . . . , i|I|

h1, h2, . . . , h|H|

Ω1, Ω2, . . . , Ω|O|

Figure 6.1: An exemplary RBF network with two input neurons, ﬁve hidden neurons and three
output neurons. The connections to the hidden neurons are not weighted, they only transmit the
input. Right of the illustration you can ﬁnd the names of the neurons, which coincide with the
names of the MLP neurons:
Input neurons are called i, hidden neurons are called h and output
neurons are called Ω. The associated sets are referred to as I, H and O.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

107





|


"
(
+
"


|
v
s
!
(
*


!
(
}


!
v
}


t
v
}






Chapter 6 Radial basis functions

dkriesel.com

ging, compressing and removing Gaussian
bells and subsequently accumulating them.
Here, the parameters for the superposition
of the Gaussian bells are in the weights
of the connections between the RBF layer
and the output layer.

Furthermore, the network architecture of-
fers the possibility to freely deﬁne or train
height and width of the Gaussian bells –
due to which the network paradigm be-
comes even more versatile. We will get
to know methods and approches for this
later.

input
→ distance
→ Gaussian bell
→ sum
→ output

Figure 6.2: Let ch be the center of an RBF neu-
ron h. Then the activation function facth is ra-
dially symmetric around ch.

6.2.1 Information processing in

RBF neurons

ple by a Gaussian bell (ﬁg. 6.3 on the next
page) .
The output values of the diﬀerent neurons
of the RBF layer or of the diﬀerent Gaus-
sian bells are added within the third layer:
basically, in relation to the whole input
space, Gaussian bells are added here.
Suppose that we have a second, a third
and a fourth RBF neuron and therefore
four diﬀerently located centers. Each of
these neurons now measures another dis-
tance from the input to its own center
and de facto provides diﬀerent values, even
if the Gaussian bell is the same. Since
these values are ﬁnally simply accumu-
lated in the output layer, one can easily
see that any surface can be shaped by drag-

RBF neurons process information by using
norms and radial basis functions

At ﬁrst, let us take as an example a sim-
ple 1-4-1 RBF network.
It is apparent
that we will receive a one-dimensional out-
put which can be represented as a func-
tion (ﬁg. 6.4 on the facing page). Ad-
ditionally, the network includes the cen-
ters c1, c2, . . . , c4 of the four inner neurons
h1, h2, . . . , h4, and therefore it has Gaus-
sian bells which are ﬁnally added within
the output neuron Ω. The network also
possesses four values σ1, σ2, . . . , σ4 which
inﬂuence the width of the Gaussian bells.
On the contrary, the height of the Gaus-
sian bell is inﬂuenced by the subsequent
weights, since the individual output val-
ues of the bells are multiplied by those
weights.

108

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

6.2 Information processing of an RBF network

Figure 6.3: Two individual one- or two-dimensional Gaussian bells.
In both cases σ = 0.4 holds
and the centers of the Gaussian bells lie in the coordinate origin. The distance r to the center (0, 0)

is simply calculated according to the Pythagorean theorem: r =p

x2 + y2.

Figure 6.4: Four diﬀerent Gaussian bells in one-dimensional space generated by means of RBF
neurons are added by an output neuron of the RBF network. The Gaussian bells have diﬀerent
heights, widths and positions. Their centers c1, c2, . . . , c4 are located at 0, 1, 3, 4, the widths
σ1, σ2, . . . , σ4 at 0.4, 1, 0.2, 0.8. You can see a two-dimensional example in ﬁg. 6.5 on the following
page.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

109

 0 0.2 0.4 0.6 0.8 1−2−1.5−1−0.5 0 0.5 1 1.5 2h(r)rGaussian in 1DGaussian in 2D−2−1 0 1x−2−1 0 1 2y 0 0.2 0.4 0.6 0.8 1h(r)−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4−2 0 2 4 6 8yxChapter 6 Radial basis functions

dkriesel.com

||c,x||
Gauß

WVUT
PQRS

||c,x||
Gauß

||c,x||
Gauß

WVUT
PQRS

||c,x||
Gauß

WVUT
PQRS
WVUT
PQRS
(QQQQQQQQQQQQQQQQQQQQ
vmmmmmmmmmmmmmmmmmmm
 AAAAAAAAAA
~}}}}}}}}}}
ONML
HIJKΣ

(cid:30)

neurons are added by an output neuron of the RBF network. Once again r =p

Figure 6.5: Four diﬀerent Gaussian bells in two-dimensional space generated by means of RBF
x2 + y2 applies for
the distance. The heights w, widths σ and centers c = (x, y) are: w1 = 1, σ1 = 0.4, c1 = (0.5, 0.5),
w2 = −1, σ2 = 0.6, c2 = (1.15,−1.15), w3 = 1.5, σ3 = 0.2, c3 = (−0.5,−1), w4 = 0.8, σ4 =
1.4, c4 = (−2, 0).

110

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Gaussian 1−2−1 0 1x−2−1 0 1 2y−1−0.5 0 0.5 1 1.5 2h(r)Gaussian 2−2−1 0 1x−2−1 0 1 2y−1−0.5 0 0.5 1 1.5 2h(r)Gaussian 3−2−1 0 1x−2−1 0 1 2y−1−0.5 0 0.5 1 1.5 2h(r)Gaussian 4−2−1 0 1x−2−1 0 1 2y−1−0.5 0 0.5 1 1.5 2h(r)Sum of the 4 Gaussians−2−1.5−1−0.5 0 0.5 1 1.5 2x−2−1.5−1−0.5 0 0.5 1 1.5 2y−1−0.75−0.5−0.25 0 0.25 0.5 0.75 1 1.25 1.5 1.75 2(
 
~
v


dkriesel.com

6.2 Information processing of an RBF network

Since we use a norm to calculate the dis-
tance between the input vector and the
center of a neuron h, we have diﬀerent
choices: Often the Euclidian norm is cho-
sen to calculate the distance:

rh = ||x − ch||

sX

i∈I

=

(xi − ch,i)2

(6.1)

(6.2)

Remember: The input vector was referred
to as x. Here, the index i runs through
the input neurons and thereby through the
input vector components and the neuron
center components. As we can see, the
Euclidean distance generates the squared
diﬀerences of all vector components, adds
them and extracts the root of the sum.
In two-dimensional space this corresponds
to the Pythagorean theorem. From the
deﬁnition of a norm directly follows that
the distance can only be positive. Strictly
speaking, we hence only use the positive
part of the activation function. By the
way, activation functions other than the
Gaussian bell are possible. Normally, func-
tions that are monotonically decreasing
over the interval [0;∞] are chosen.
Now that we know the distance rh be-
tween the input vector x and the center
ch of the RBF neuron h, this distance has
to be passed through the activation func-
tion. Here we use, as already mentioned,
a Gaussian bell:

(cid:18) −r2

(cid:19)

h
2σ2
h

fact(rh) = e

(6.3)

activation function fact, and hence the ac-
tivation functions should not be referred
to as fact simultaneously. One solution
would be to number the activation func-
tions like fact1, fact2, . . . , fact|H| with H be-
ing the set of hidden neurons. But as a
result the explanation would be very con-
fusing. So I simply use the name fact for
all activation functions and regard σ and
c as variables that are deﬁned for individ-
ual neurons but no directly included in the
activation function.
The reader will certainly notice that in the
literature the Gaussian bell is often nor-
malized by a multiplicative factor. We
can, however, avoid this factor because
we are multiplying anyway with the subse-
quent weights and consecutive multiplica-
tions, ﬁrst by a normalization factor and
then by the connections’ weights, would
only yield diﬀerent factors there. We do
not need this factor (especially because for
our purpose the integral of the Gaussian
bell must not always be 1) and therefore
simply leave it out.

6.2.2 Some analytical thoughts

prior to the training

The output yΩ of an RBF output neuron
Ω results from combining the functions of
an RBF neuron to

wh,Ω · fact (||x − ch||) .

(6.4)

yΩ = X

h∈H

It is obvious that both the center ch and
the width σh can be seen as part of the

Suppose that similar to the multilayer per-
ceptron we have a set P, that contains |P|

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

111

rh(cid:73)

Chapter 6 Radial basis functions

dkriesel.com

training samples (p, t). Then we obtain
|P| functions of the form

wh,Ω · fact (||p − ch||) ,

(6.5)

yΩ = X

h∈H

i.e. one function for each training sam-
ple.

Of course, with this eﬀort we are aiming
at letting the output y for all training
patterns p converge to the corresponding
teaching input t.

6.2.2.1 Weights can simply be

computed as solution of a
system of equations

Thus, we have |P| equations. Now let us
assume that the widths σ1, σ2, . . . , σk, the
centers c1, c2, . . . , ck and the training sam-
ples p including the teaching input t are
given. We are looking for the weights wh,Ω
with |H| weights for one output neuron
Ω. Thus, our problem can be seen as a
system of equations since the only thing
we want to change at the moment are the
weights.

This demands a distinction of cases con-
cerning the number of training samples |P|
and the number of RBF neurons |H|:

|P| = |H|: If the number of RBF neurons
equals the number of patterns,
i.e.
|P| = |H|, the equation can be re-
duced to a matrix multiplication

simply
calculate
weights

T = M · G

−1 · T = M
−1 · T = E · G
−1 · T = G,

(6.6)
−1 · M · G (6.7)
(6.8)
(6.9)

⇔ M
⇔ M
⇔ M

where

. T is the vector of the teaching (cid:74)T
inputs for all training samples,
. M is the |P| × |H| matrix of (cid:74)M
the outputs of all |H| RBF neu-
rons to |P| samples (remember:
|P| = |H|, the matrix is squared
and we can therefore attempt to
invert it),
. G is the vector of the desired (cid:74)G
weights and
. E is a unit matrix with the same (cid:74)E
size as G.

Mathematically speaking, we can sim-
ply calculate the weights: In the case
of |P| = |H| there is exactly one RBF
neuron available per training sample.
This means, that the network exactly
meets the |P| existing nodes after hav-
ing calculated the weights, i.e. it per-
forms a precise interpolation. To
calculate such an equation we cer-
tainly do not need an RBF network,
and therefore we can proceed to the
next case.
Exact interpolation must not be mis-
taken for the memorizing ability men-
tioned with the MLPs: First, we are
not talking about the training of RBF

112

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

6.2 Information processing of an RBF network

networks at the moment.
Second,
it could be advantageous for us and
might in fact be intended if the net-
work exactly interpolates between the
nodes.

|P| < |H|: The system of equations is
under-determined,
there are more
RBF neurons than training samples,
|P| < |H|. Certainly, this case
i.e.
normally does not occur very often.
In this case, there is a huge variety
of solutions which we do not need in
such detail. We can select one set of
weights out of many obviously possi-
ble ones.

|P| > |H|: But most interesting for fur-
ther discussion is the case if there
are signiﬁcantly more training sam-
ples than RBF neurons, that means
|P| > |H|. Thus, we again want
to use the generalization capability of
the neural network.

If we have more training samples than
RBF neurons, we cannot assume that
every training sample is exactly hit.
So, if we cannot exactly hit the points
and therefore cannot just interpolate
as in the aforementioned ideal case
with |P| = |H|, we must try to ﬁnd
a function that approximates our
training set P as closely as possible:
As with the MLP we try to reduce
the sum of the squared error to a min-
imum.

How do we continue the calculation
in the case of |P| > |H|? As above,
to solve the system of equations, we

have to ﬁnd the solution M of a ma-
trix multiplication

T = M · G.

(6.10)

The problem is that this time we can-
not invert the |P|×|H| matrix M be-
cause it is not a square matrix (here,
|P|
6= |H| is true). Here, we have
to use the Moore-Penrose pseudo
inverse M + which is deﬁned by

M + = (M T · M)−1 · M T

(6.11)

(cid:74)M +

Although the Moore-Penrose pseudo
inverse is not the inverse of a matrix,
it can be used similarly in this case1.
We get equations that are very similar
to those in the case of |P| = |H|:

T = M · G

(6.12)
⇔ M + · T = M + · M · G (6.13)
⇔ M + · T = E · G
(6.14)
⇔ M + · T = G
(6.15)
Another reason for the use of the
Moore-Penrose pseudo inverse is the
fact that it minimizes the squared
error (which is our goal): The esti-
mate of the vector G in equation 6.15
corresponds to the Gauss-Markov
model known from statistics, which
is used to minimize the squared error.
In the aforementioned equations 6.11
and the following ones please do not
mistake the T in M T (of the trans-
pose of the matrix M) for the T of
the vector of all teaching inputs.

+ = M

1 Particularly, M

−1 is true if M is invertible.
I do not want to go into detail of the reasons for
+ - they
these circumstances and applications of M
can easily be found in literature for linear algebra.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

113

Chapter 6 Radial basis functions

dkriesel.com

6.2.2.2 The generalization on several
outputs is trivial and not quite
computationally expensive

We have found a mathematically exact
way to directly calculate the weights.
What will happen if there are several out-
put neurons, i.e. |O| > 1, with O being, as
usual, the set of the output neurons Ω? In
this case, as we have already indicated, it
does not change much: The additional out-
put neurons have their own set of weights
while we do not change the σ and c of the
RBF layer. Thus, in an RBF network it is
easy for given σ and c to realize a lot of
output neurons since we only have to cal-
culate the individual vector of weights

GΩ = M + · TΩ

(6.16)

for every new output neuron Ω, whereas
the matrix M +, which generally requires
a lot of computational eﬀort, always stays
the same: So it is quite inexpensive – at
least concerning the computational com-
plexity – to add more output neurons.

inexpensive
output
dimension

6.2.2.3 Computational eﬀort and

accuracy

For realistic problems it normally applies
that there are considerably more training
|P| (cid:29)
samples than RBF neurons, i.e.
|H|: You can, without any diﬃculty, use
106 training samples, if you like. Theoreti-
cally, we could ﬁnd the terms for the math-
ematically correct solution on the black-
board (after a very long time), but such
calculations often seem to be imprecise

M+ complex
and imprecise

and very time-consuming (matrix inver-
sions require a lot of computational ef-
fort).
Furthermore, our Moore-Penrose pseudo-
inverse is,
in spite of numeric stabil-
ity, no guarantee that the output vector
corresponds to the teaching vector, be-
cause such extensive computations can be
prone to many inaccuracies, even though
the calculation is mathematically correct:
Our computers can only provide us with
(nonetheless very good) approximations of
the pseudo-inverse matrices. This means
that we also get only approximations of
the correct weights (maybe with a lot of
accumulated numerical errors) and there-
fore only an approximation (maybe very
rough or even unrecognizable) of the de-
sired output.
If we have enough computing power to an-
alytically determine a weight vector, we
should use it nevertheless only as an initial
value for our learning process, which leads
us to the real training methods – but oth-
erwise it would be boring, wouldn’t it?

6.3 Combinations of equation

system and gradient
strategies are useful for
training

Analogous to the MLP we perform a gra-
dient descent to ﬁnd the suitable weights
by means of the already well known delta
rule. Here, backpropagation is unneces-
sary since we only have to train one single

retraining
delta rule

114

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

6.3 Training of RBF networks

weight layer – which requires less comput-
ing time.

We know that the delta rule is
∆wh,Ω = η · δΩ · oh,

(6.17)

in which we now insert as follows:

∆wh,Ω = η · (tΩ − yΩ) · fact(||p − ch||)

(6.18)

Here again I explicitly want to mention
that it is very popular to divide the train-
ing into two phases by analytically com-
puting a set of weights and then reﬁning
it by training with the delta rule.

There is still the question whether to learn
oﬄine or online. Here, the answer is sim-
ilar to the answer for the multilayer per-
ceptron: Initially, one often trains online
(faster movement across the error surface).
Then, after having approximated the so-
lution, the errors are once again accumu-
lated and, for a more precise approxima-
tion, one trains oﬄine in a third learn-
ing phase. However, similar to the MLPs,
you can be successful by using many meth-
ods.

As already indicated, in an RBF network
not only the weights between the hidden
and the output layer can be optimized. So
let us now take a look at the possibility to
vary σ and c.

training
in phases

6.3.1 It is not always trivial to

determine centers and widths
of RBF neurons

It is obvious that the approximation accu-
racy of RBF networks can be increased by
adapting the widths and positions of the
Gaussian bells in the input space to the
problem that needs to be approximated.
There are several methods to deal with the
centers c and the widths σ of the Gaussian
bells:

Fixed selection: The centers and widths
can be selected in a ﬁxed manner and
regardless of the training samples –
this is what we have assumed until
now.

Conditional, ﬁxed selection: Again cen-
ters and widths are selected ﬁxedly,
but we have previous knowledge
about the functions to be approxi-
mated and comply with it.

vary
σ and c

Adaptive to the learning process: This

too.

A realization of

is deﬁnitely the most elegant variant,
but certainly the most challenging
one,
this
approach will not be discussed in
this chapter but it can be found in
connection with another network
topology (section 10.6.1).

6.3.1.1 Fixed selection

In any case, the goal is to cover the in-
put space as evenly as possible. Here,
widths of 2
3 of the distance between the

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

115

Chapter 6 Radial basis functions

dkriesel.com

responsible for the fact that six- to ten-
dimensional problems in RBF networks
are already called "high-dimensional" (an
MLP, for example, does not cause any
problems here).

6.3.1.2 Conditional, ﬁxed selection

Suppose that our training samples are not
evenly distributed across the input space.
It then seems obvious to arrange the cen-
ters and sigmas of the RBF neurons by
means of the pattern distribution. So the
training patterns can be analyzed by statis-
tical techniques such as a cluster analysis,
and so it can be determined whether there
are statistical factors according to which
we should distribute the centers and sig-
mas (ﬁg. 6.7 on the facing page).

A more trivial alternative would be to
set |H| centers on positions randomly se-
lected from the set of patterns. So this
method would allow for every training pat-
tern p to be directly in the center of a neu-
ron (ﬁg. 6.8 on the next page). This is
not yet very elegant but a good solution
when time is an issue. Generally, for this
method the widths are ﬁxedly selected.

If we have reason to believe that the set
of training samples is clustered, we can
use clustering methods to determine them.
There are diﬀerent methods to determine
clusters in an arbitrarily dimensional set
of points. We will be introduced to some
of them in excursus A. One neural cluster-
ing method are the so-called ROLFs (sec-
tion A.5), and self-organizing maps are

Figure 6.6: Example for an even coverage of a
two-dimensional input space by applying radial
basis functions.

centers can be selected so that the Gaus-
sian bells overlap by approx. "one third"2
(ﬁg. 6.6). The closer the bells are set the
more precise but the more time-consuming
the whole thing becomes.
This may seem to be very inelegant, but
in the ﬁeld of function approximation we
cannot avoid even coverage. Here it is
useless if the function to be approximated
is precisely represented at some positions
but at other positions the return value is
only 0. However, the high input dimen-
sion requires a great many RBF neurons,
which increases the computational eﬀort
exponentially with the dimension – and is
2 It is apparent that a Gaussian bell is mathemati-
cally inﬁnitely wide, therefore I ask the reader to
apologize this sloppy formulation.

input
dimension
very expensive

116

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

6.3 Training of RBF networks

Figure 6.7: Example of an uneven coverage of
a two-dimensional
input space, of which we
have previous knowledge, by applying radial ba-
sis functions.

also useful in connection with determin-
ing the position of RBF neurons (section
10.6.1). Using ROLFs, one can also receive
indicators for useful radii of the RBF neu-
rons. Learning vector quantisation (chap-
ter 9) has also provided good results. All
these methods have nothing to do with
the RBF networks themselves but are only
used to generate some previous knowledge.
Therefore we will not discuss them in this
chapter but independently in the indicated
chapters.
Another approach is to use the approved
methods: We could slightly move the po-
sitions of the centers and observe how our
error function Err is changing – a gradient
descent, as already known from the MLPs.

Figure 6.8: Example of an uneven coverage of
a two-dimensional input space by applying radial
basis functions. The widths were ﬁxedly selected,
the centers of the neurons were randomly dis-
tributed throughout the training patterns. This
distribution can certainly lead to slightly unrepre-
sentative results, which can be seen at the single
data point down to the left.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

117

Chapter 6 Radial basis functions

dkriesel.com

In a similar manner we could look how the
error depends on the values σ. Analogous
to the derivation of backpropagation we
derive

In the following text, only simple mecha-
nisms are sketched. For more information,
I refer to [Fri94].

∂Err(σhch)

∂σh

and ∂Err(σhch)

∂ch

.

6.4.1 Neurons are added to places

with large error values

Since the derivation of these terms corre-
sponds to the derivation of backpropaga-
tion we do not want to discuss it here.

But experience shows that no convincing
results are obtained by regarding how the
error behaves depending on the centers
and sigmas. Even if mathematics claim
that such methods are promising, the gra-
dient descent, as we already know, leads
to problems with very craggy error sur-
faces.

And that is the crucial point: Naturally,
RBF networks generate very craggy er-
ror surfaces because,
if we considerably
change a c or a σ, we will signiﬁcantly
change the appearance of the error func-
tion.

6.4 Growing RBF networks
automatically adjust the
neuron density

In growing RBF networks, the number
|H| of RBF neurons is not constant. A
certain number |H| of neurons as well as
their centers ch and widths σh are previ-
ously selected (e.g. by means of a cluster-
ing method) and then extended or reduced.

After generating this initial conﬁguration
the vector of the weights G is analytically
calculated. Then all speciﬁc errors Errp
concerning the set P of the training sam-
ples are calculated and the maximum spe-
ciﬁc error

max
P

(Errp)

is sought.
The extension of the network is simple:
We replace this maximum error with a new replace
RBF neuron. Of course, we have to exer-
error with
neuron
cise care in doing this: IF the σ are small,
the neurons will only inﬂuence each other
if the distance between them is short. But
if the σ are large, the already exisiting
neurons are considerably inﬂuenced by the
new neuron because of the overlapping of
the Gaussian bells.
So it is obvious that we will adjust the al-
ready existing RBF neurons when adding
the new neuron.
To put it simply, this adjustment is made
by moving the centers c of the other neu-
rons away from the new neuron and re-
ducing their width σ a bit. Then the
current output vector y of the network is
compared to the teaching input t and the
weight vector G is improved by means of
training. Subsequently, a new neuron can
be inserted if necessary. This method is

118

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

6.5 Comparing RBF networks and multilayer perceptrons

particularly suited for function approxima-
tions.

two paradigms and look at their advan-
tages and disadvantages.

6.4.2 Limiting the number of

neurons

Here it is mandatory to see that the net-
work will not grow ad inﬁnitum, which can
happen very fast. Thus, it is very useful
to previously deﬁne a maximum number
for neurons |H|max.

6.4.3 Less important neurons are

deleted

Which leads to the question whether it
is possible to continue learning when this
limit |H|max is reached. The answer is:
this would not stop learning. We only have
to look for the "most unimportant" neuron
and delete it. A neuron is, for example,
unimportant for the network if there is an-
other neuron that has a similar function:
It often occurs that two Gaussian bells ex-
actly overlap and at such a position, for
instance, one single neuron with a higher
Gaussian bell would be appropriate.
But to develop automated procedures in
order to ﬁnd less relevant neurons is highly
problem dependent and we want to leave
this to the programmer.
With RBF networks and multilayer per-
ceptrons we have already become ac-
quainted with and extensivley discussed
two network paradigms for similar prob-
lems. Therefore we want to compare these

delete
unimportant
neurons

6.5 Comparing RBF networks

and multilayer
perceptrons

networks

We will compare multilayer perceptrons
and RBF networks with respect to diﬀer-
ent aspects.
Input dimension: We must be careful
high-
with RBF
dimensional
functional spaces since
the network could very quickly
require huge memory storage and
computational
a
multilayer perceptron would cause
less problems because its number of
neuons does not grow exponentially
with the input dimension.

in

eﬀort.

Here,

Center selection: However, selecting the
centers c for RBF networks is (despite
the introduced approaches) still a ma-
jor problem. Please use any previous
knowledge you have when applying
them. Such problems do not occur
with the MLP.

Output dimension: The

advantage

of
RBF networks is that the training is
not much inﬂuenced when the output
dimension of the network is high.
For an MLP, a learning procedure
such as backpropagation thereby will
be very time-consuming.

Extrapolation: Advantage as well as dis-
advantage of RBF networks is the lack

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

119

Chapter 6 Radial basis functions

dkriesel.com

Exercises
Exercise 13. An |I|-|H|-|O| RBF net-
work with ﬁxed widths and centers of the
neurons should approximate a target func-
tion u. For this, |P| training samples of
the form (p, t) of the function u are given.
Let |P| > |H| be true. The weights should
be analytically determined by means of
the Moore-Penrose pseudo inverse.
Indi-
cate the running time behavior regarding
|P| and |O| as precisely as possible.
Note: There are methods for matrix mul-
tiplications and matrix inversions that are
more eﬃcient than the canonical methods.
For better estimations, I recommend to
look for such methods (and their complex-
ity). In addition to your complexity calcu-
lations, please indicate the used methods
together with their complexity.

Important!

of extrapolation capability: An RBF
network returns the result 0 far away
from the centers of the RBF layer. On
the one hand it does not extrapolate,
unlike the MLP it cannot be used
for extrapolation (whereby we could
never know if the extrapolated values
of the MLP are reasonable, but expe-
rience shows that MLPs are suitable
for that matter). On the other hand,
unlike the MLP the network is capa-
ble to use this 0 to tell us "I don’t
know", which could be an advantage.

Lesion tolerance: For the output of an
MLP, it is no so important if a weight
It will only
or a neuron is missing.
worsen a little in total.
If a weight
or a neuron is missing in an RBF net-
work then large parts of the output
remain practically uninﬂuenced. But
one part of the output is heavily af-
fected because a Gaussian bell is di-
rectly missing. Thus, we can choose
between a strong local error for lesion
and a weak but global error.

Spread: Here the MLP is "advantaged"
since RBF networks are used consid-
erably less often – which is not always
understood by professionals (at least
as far as low-dimensional input spaces
are concerned). The MLPs seem to
have a considerably longer tradition
and they are working too good to take
the eﬀort to read some pages of this
work about RBF networks) :-).

120

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Chapter 7
Recurrent perceptron-like networks

Some thoughts about networks with internal states.

Generally,
recurrent networks are net-
works that are capable of inﬂuencing them-
selves by means of recurrences, e.g. by
including the network output in the follow-
ing computation steps. There are many
types of recurrent networks of nearly arbi-
trary form, and nearly all of them are re-
ferred to as recurrent neural networks.
As a result,
for the few paradigms in-
troduced here I use the name recurrent
multilayer perceptrons.

Apparently, such a recurrent network is ca-
pable to compute more than the ordinary
MLP: If the recurrent weights are set to 0,
the recurrent network will be reduced to
an ordinary MLP. Additionally, the recur-
rence generates diﬀerent network-internal
states so that diﬀerent inputs can produce
diﬀerent outputs in the context of the net-
work state.

Recurrent networks in themselves have a
great dynamic that is mathematically dif-
ﬁcult to conceive and has to be discussed
extensively. The aim of this chapter is
only to brieﬂy discuss how recurrences can

be structured and how network-internal
states can be generated. Thus, I will
brieﬂy introduce two paradigms of recur-
rent networks and afterwards roughly out-
line their training.

With a recurrent network an input x that
is constant over time may lead to diﬀer-
ent results: On the one hand, the network
could converge, i.e.
it could transform it-
self into a ﬁxed state and at some time re-
turn a ﬁxed output value y. On the other
hand, it could never converge, or at least
not until a long time later, so that it can
no longer be recognized, and as a conse-
quence, y constantly changes.

If the network does not converge, it is, for
example, possible to check if periodicals
or attractors (ﬁg. 7.1 on the following
page) are returned. Here, we can expect
the complete variety of dynamical sys-
tems. That is the reason why I particu-
larly want to refer to the literature con-
cerning dynamical systems.

state
dynamics

more capable
than MLP

121

Chapter 7 Recurrent perceptron-like networks (depends on chapter 5)

dkriesel.com

Further discussions could reveal what will
happen if the input of recurrent networks
is changed.

In this chapter the related paradigms of
recurrent networks according to Jordan
and Elman will be introduced.

7.1 Jordan networks

A Jordan network [Jor86]
is a multi-
layer perceptron with a set K of so-called
context neurons k1, k2, . . . , k|K|. There
is one context neuron per output neuron
(ﬁg. 7.2 on the next page). In principle, a
context neuron just memorizes an output
until it can be processed in the next time
step. Therefore, there are weighted con-
nections between each output neuron and
one context neuron. The stored values are
returned to the actual network by means
of complete links between the context neu-
rons and the input layer.

In the originial deﬁnition of a Jordan net-
work the context neurons are also recur-
rent to themselves via a connecting weight
λ. But most applications omit this recur-
rence since the Jordan network is already
very dynamic and diﬃcult to analyze, even
without these additional recurrences.
Deﬁnition 7.1 (Context neuron). A con-
text neuron k receives the output value of
another neuron i at a time t and then reen-
ters it into the network at a time (t + 1).
Deﬁnition 7.2 (Jordan network). A Jor-
dan network is a multilayer perceptron

output
neurons
are buﬀered

Figure 7.1: The Roessler attractor

122

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

7.2 Elman networks

i1

i2

GFED
@ABC
GFED
@ABC
*UUUUUUUUUUUUUUUUUUUUUUUUUU
tiiiiiiiiiiiiiiiiiiiiiiiiii
 AAAAAAAAA
 AAAAAAAAA
~}}}}}}}}}
~}}}}}}}}}
GFED
@ABCh2
GFED
@ABCh1
GFED
@ABCh3
*UUUUUUUUUUUUUUUUUUUUUUUUUU
tiiiiiiiiiiiiiiiiiiiiiiiiii
 AAAAAAAAA
 AAAAAAAAA
~}}}}}}}}}
~}}}}}}}}}
GFED
@ABCΩ2
GFED
@ABCΩ1
@A


GFED
@ABCk2



GFED
@ABCk1

BC

Figure 7.2: Illustration of a Jordan network. The network output is buﬀered in the context neurons
and with the next time step it is entered into the network together with the new input.

with one context neuron per output neu-
ron. The set of context neurons is called
K. The context neurons are completely
linked toward the input layer of the net-
work.

7.2 Elman networks

The Elman networks (a variation of
the Jordan networks) [Elm90] have con-
text neurons, too, but one layer of context
neurons per information processing neu-
ron layer (ﬁg. 7.3 on the following page).
Thus, the outputs of each hidden neuron
or output neuron are led into the associ-
ated context layer (again exactly one con-
text neuron per neuron) and from there it
is reentered into the complete neuron layer

during the next time step (i.e. again a com-
plete link on the way back). So the com-
plete information processing part1 of the
MLP exists a second time as a "context
version" – which once again considerably
increases dynamics and state variety.
Compared with Jordan networks the El-
man networks often have the advantage to
act more purposeful since every layer can
access its own context.
Deﬁnition 7.3 (Elman network). An El-
man network is an MLP with one con-
text neuron per information processing
neuron. The set of context neurons is
called K. This means that there exists one
context layer per information processing
1 Remember: The input layer does not process in-

formation.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

123

nearly every-
thing is
buﬀered





~
 
*
t
~
 




x
x


{
{
v
v
 
*
~
 
~
t

O
O

O
O
Chapter 7 Recurrent perceptron-like networks (depends on chapter 5)

dkriesel.com

i2

i1

GFED
@ABC
GFED
@ABC
*UUUUUUUUUUUUUUUUUUUUUUUUUU
tiiiiiiiiiiiiiiiiiiiiiiiiii
 @@@@@@@@@@
 @@@@@@@@@@
~~~~~~~~~~~
~~~~~~~~~~~
GFED
@ABCh3
GFED
@ABCh2
GFED
@ABCh1
*UUUUUUUUUUUUUUUUUUUUUUUUUU
tiiiiiiiiiiiiiiiiiiiiiiiiii
@@@@@@@@@
@@@@@@@@@
~~~~~~~~~
~~~~~~~~~
GFED
GFED
@ABCΩ1
@ABCΩ2

ONML
HIJKkh3

ONML
HIJKkh1
ONML
HIJKkΩ1

ONML
HIJKkh2
ONML
HIJKkΩ2

Figure 7.3: Illustration of an Elman network. The entire information processing part of the network
exists, in a way, twice. The output of each neuron (except for the output of the input neurons)
is buﬀered and reentered into the associated layer. For the reason of clarity I named the context
neurons on the basis of their models in the actual network, but it is not mandatory to do so.

neuron layer with exactly the same num-
ber of context neurons. Every neuron has
a weighted connection to exactly one con-
text neuron while the context layer is com-
pletely linked towards its original layer.

Now it is interesting to take a look at the
training of recurrent networks since, for in-
stance, ordinary backpropagation of error
cannot work on recurrent networks. Once
again, the style of the following part is
rather informal, which means that I will
not use any formal deﬁnitions.

7.3 Training recurrent

networks

In order to explain the training as compre-
hensible as possible, we have to agree on
some simpliﬁcations that do not aﬀect the
learning principle itself.

So for the training let us assume that in
the beginning the context neurons are ini-
tiated with an input, since otherwise they
would have an undeﬁned input (this is no
simpliﬁcation but reality).

Furthermore, we use a Jordan network
without a hidden neuron layer for our
training attempts so that the output neu-

124

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)





~
 
*
t
~
 

*
4
4


5
5

t
5
5
u
u
z
z
v
v
w
w
u
u
t
t
v
v
u
u
t
t


5
5
5
5


u
u
w
w
u
u
v
v
attach
the same
network
to each
context
layer

dkriesel.com

7.3 Training recurrent networks

rons can directly provide input. This ap-
proach is a strong simpliﬁcation because
generally more complicated networks are
used. But this does not change the learn-
ing principle.

7.3.1 Unfolding in time

Remember our actual learning procedure
for MLPs, the backpropagation of error,
which backpropagates the delta values.
So,
in case of recurrent networks the
delta values would backpropagate cycli-
cally through the network again and again,
which makes the training more diﬃcult.
On the one hand we cannot know which
of the many generated delta values for a
weight should be selected for training, i.e.
which values are useful. On the other hand
we cannot deﬁnitely know when learning
should be stopped. The advantage of re-
current networks are great state dynamics
within the network; the disadvantage of
recurrent networks is that these dynamics
are also granted to the training and there-
fore make it diﬃcult.

One learning approach would be the at-
tempt to unfold the temporal states of
the network (ﬁg. 7.4 on the next page):
Recursions are deleted by putting a sim-
ilar network above the context neurons,
i.e.
the context neurons are, as a man-
ner of speaking, the output neurons of
the attached network. More generally spo-
ken, we have to backtrack the recurrences
and place "‘earlier"’ instances of neurons
in the network – thus creating a larger,

but forward-oriented network without re-
currences. This enables training a recur-
rent network with any training strategy
developed for non-recurrent ones. Here,
the input is entered as teaching input into
every "copy" of the input neurons. This
can be done for a discrete number of time
steps. These training paradigms are called
unfolding in time [MP69]. After the un-
folding a training by means of backpropa-
gation of error is possible.
But obviously,
for one weight wi,j sev-
eral changing values ∆wi,j are received,
which can be treated diﬀerently: accumu-
lation, averaging etc. A simple accumu-
lation could possibly result in enormous
changes per weight if all changes have the
same sign. Hence, also the average is not
to be underestimated. We could also intro-
duce a discounting factor, which weakens
the inﬂuence of ∆wi,j of the past.
Unfolding in time is particularly useful if
we receive the impression that the closer
past is more important for the network
than the one being further away. The
reason for this is that backpropagation
has only little inﬂuence in the layers far-
ther away from the output (remember:
the farther we are from the output layer,
the smaller the inﬂuence of backpropaga-
tion).
Disadvantages: the training of such an un-
folded network will take a long time since
a large number of layers could possibly be
produced. A problem that is no longer
negligible is the limited computational ac-
curacy of ordinary computers, which is
exhausted very fast because of so many

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

125

Chapter 7 Recurrent perceptron-like networks (depends on chapter 5)

dkriesel.com

i3

i1

i2

GFED
@ABC
GFED
@ABCk1
GFED
@ABC
GFED
@ABCk2
GFED
@ABC
*UUUUUUUUUUUUUUUUUUUUUUUUU
'PPPPPPPPPPPPPPPPP
'OOOOOOOOOOOOOOOO
tiiiiiiiiiiiiiiiiiiiiiiiiii
wnnnnnnnnnnnnnnnnn
wnnnnnnnnnnnnnnnnn
 AAAAAAAAA
@@@@@@@@@
~}}}}}}}}}
GFED
@ABCΩ1@A
GFED
@ABCΩ2

BC

...

...

...

...

...

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
*VVVVVVVVVVVVVVVVVVVVVVVV
(RRRRRRRRRRRRRRRRR
(PPPPPPPPPPPPPPP
tjjjjjjjjjjjjjjjjjjjjj
!CCCCCCCCC
????????
woooooooooooooo
woooooooooooooo

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
*VVVVVVVVVVVVVVVVVVVVVVVVVVV
(RRRRRRRRRRRRRRRRRR
(QQQQQQQQQQQQQQQQQQ
tjjjjjjjjjjjjjjjjjjjjjjj
!DDDDDDDDDD
!CCCCCCCCC
vnnnnnnnnnnnnnnnn
wppppppppppppppp

GFED
@ABC
GFED
@ABC
GFED
@ABC
GFED
@ABCk1
GFED
@ABCk2
*UUUUUUUUUUUUUUUUUUUUUUUUU
'PPPPPPPPPPPPPPPPP
'OOOOOOOOOOOOOOOO
tiiiiiiiiiiiiiiiiiiiiiiiiii
wnnnnnnnnnnnnnnnnn
wnnnnnnnnnnnnnnnnn
 AAAAAAAAA
@@@@@@@@@
~}}}}}}}}}
GFED
@ABCΩ2
GFED
@ABCΩ1

i3

i2

i1

Figure 7.4: Illustration of the unfolding in time with a small exemplary recurrent MLP. Top: The
recurrent MLP. Bottom: The unfolded network. For reasons of clarity, I only added names to
the lowest part of the unfolded network. Dotted arrows leading into the network mark the inputs.
Dotted arrows leading out of the network mark the outputs. Each "network copy" represents a time
step of the network with the most recent time step being at the bottom.

126

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)







'
*

'


 
w
~
t
w
O
O


O
O














(
*
!
(



w

t
w





(
*
!
(


!
v

t
w
'
*

'


 
w
~
t
w




dkriesel.com

7.3 Training recurrent networks

are chosen suitably: So, for example, neu-
rons and weights can be adjusted and
the network topology can be optimized
(of course the result of learning is not
necessarily a Jordan or Elman network).
With ordinary MLPs, however, evolution-
ary strategies are less popular since they
certainly need a lot more time than a di-
rected learning procedure such as backpro-
pagation.

teaching
input
applied at
context
neurons

nested computations (the farther we are
from the output layer, the smaller the in-
ﬂuence of backpropagation, so that this
limit is reached). Furthermore, with sev-
eral levels of context neurons this proce-
dure could produce very large networks to
be trained.

7.3.2 Teacher forcing

the

Other procedures are
equivalent
teacher forcing and open loop learn-
ing. They detach the recurrence during
the learning process: We simply pretend
that the recurrence does not exist and ap-
ply the teaching input to the context neu-
rons during the training. So, backpropaga-
tion becomes possible, too. Disadvantage:
with Elman networks a teaching input for
non-output-neurons is not given.

7.3.3 Recurrent backpropagation

Another popular procedure without lim-
ited time horizon is the recurrent back-
propagation using methods of diﬀer-
ential calculus
to solve the problem
[Pin87].

7.3.4 Training with evolution

Due to the already long lasting train-
ing time, evolutionary algorithms have
proved to be of value, especially with recur-
rent networks. One reason for this is that
they are not only unrestricted with respect
to recurrences but they also have other ad-
vantages when the mutation mechanisms

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

127

Chapter 8
Hopﬁeld networks

In a magnetic ﬁeld, each particle applies a force to any other particle so that
all particles adjust their movements in the energetically most favorable way.
This natural mechanism is copied to adjust noisy inputs in order to match
their real models.

Another supervised learning example of
the wide range of neural networks was
developed by John Hopfield:
the so-
called Hopﬁeld networks [Hop82]. Hop-
ﬁeld and his physically motivated net-
works have contributed a lot to the renais-
sance of neural networks.

8.1 Hopﬁeld networks are

inspired by particles in a
magnetic ﬁeld

The idea for the Hopﬁeld networks origi-
nated from the behavior of particles in a
magnetic ﬁeld: Every particle "communi-
cates" (by means of magnetic forces) with
every other particle (completely linked)
with each particle trying to reach an ener-
getically favorable state (i.e. a minimum
of the energy function). As for the neurons
this state is known as activation. Thus,
all particles or neurons rotate and thereby

encourage each other to continue this rota-
tion. As a manner of speaking, our neural
network is a cloud of particles
Based on the fact that the particles auto-
matically detect the minima of the energy
function, Hopﬁeld had the idea to use the
"spin" of the particles to process informa-
tion: Why not letting the particles search
minima on arbitrary functions? Even if we
only use two of those spins, i.e. a binary
activation, we will recognize that the devel-
oped Hopﬁeld network shows considerable
dynamics.

8.2 In a hopﬁeld network, all

neurons inﬂuence each
other symmetrically

Brieﬂy speaking, a Hopﬁeld network con-
sists of a set K of completely linked neu- (cid:74)K
rons with binary activation (since we only

129

Chapter 8 Hopﬁeld networks

?>=<
89:;↓5
?>=<
89:;↑ i
)SSSSSSSSSSSSSSSSSSSSSSSS
ukkkkkkkkkkkkkkkkkkkkkkkk
@
<<<<<<<<<
<<<<<<<<<
 
?>=<
89:;↑4
?>=<
89:;↓
?>=<
89:;↑ i
)SSSSSSSSSSSSSSSSSSSSSSSSo
ukkkkkkkkkkkkkkkkkkkkkkkk/
@
^<<<<<<<<<
^<<<<<<<<<
 
?>=<
?>=<
89:;↓
89:;↑/

Figure 8.1: Illustration of an exemplary Hop-
ﬁeld network. The arrows ↑ and ↓ mark the
binary "spin". Due to the completely linked neu-
rons the layers cannot be separated, which means
that a Hopﬁeld network simply includes a set of
neurons.

dkriesel.com

Deﬁnition 8.1 (Hopﬁeld network). A
Hopﬁeld network consists of a set K of
completely linked neurons without direct
recurrences. The activation function of
the neurons is the binary threshold func-
tion with outputs ∈ {1,−1}.
Deﬁnition 8.2 (State of a Hopﬁeld net-
work). The state of the network con-
sists of the activation states of all neu-
rons. Thus, the state of the network can
be understood as a binary string z ∈
{−1, 1}|K|.

8.2.1 Input and output of a
Hopﬁeld network are
represented by neuron states

use two spins), with the weights being
symmetric between the individual neurons
and without any neuron being directly con-
nected to itself (ﬁg. 8.1). Thus, the state
of |K| neurons with two possible states
∈ {−1, 1} can be described by a string
x ∈ {−1, 1}|K|.
The complete link provides a full square
matrix of weights between the neurons.
The meaning of the weights will be dis-
cussed in the following. Furthermore, we
will soon recognize according to which
rules the neurons are spinning, i.e. are
changing their state.
Additionally, the complete link leads to
the fact that we do not know any input,
output or hidden neurons. Thus, we have
to think about how we can input some-
thing into the |K| neurons.

We have learned that a network, i.e. a
set of |K| particles, that is in a state
is automatically looking for a minimum.
An input pattern of a Hopﬁeld network
is exactly such a state: A binary string
x ∈ {−1, 1}|K| that initializes the neurons.
Then the network is looking for the min-
imum to be taken (which we have previ-
ously deﬁned by the input of training sam-
ples) on its energy surface.

But when do we know that the minimum
has been found? This is simple, too: when
the network stops. It can be proven that a
Hopﬁeld network with a symmetric weight
matrix that has zeros on its diagonal al-
ways converges [CG88], i.e. at some point
it will stand still. Then the output is a
binary string y ∈ {−1, 1}|K|, namely the
state string of the network that has found
a minimum.

130

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

input and
output =
network
states

always
converges

completely
linked
set of
neurons

i


i
i
)
O
O


o
o
/
/
^
^

5
u
O
O


@
@
 
^
^

i
)
o
/
/
 
 
@
4
j
j
5
5
u
/
o
o
@
@
 




6
6
 
 
@


^
/
o
o


^
dkriesel.com

8.2 Structure and functionality

Now let us take a closer look at the con-
tents of the weight matrix and the rules
for the state change of the neurons.
Deﬁnition 8.3 (Input and output of
a Hopﬁeld network). The input of a
Hopﬁeld network is binary string x ∈
{−1, 1}|K| that initializes the state of the
network. After the convergence of the
network, the output is the binary string
y ∈ {−1, 1}|K| generated from the new net-
work state.

8.2.2 Signiﬁcance of weights

i.e.

We have already said that the neurons
change their states,
their direction,
from −1 to 1 or vice versa. These spins oc-
cur dependent on the current states of the
other neurons and the associated weights.
Thus, the weights are capable to control
the complete change of the network. The
weights can be positive, negative, or 0.
Colloquially speaking, for a weight wi,j be-
tween two neurons i and j the following
holds:
If wi,j is positive, it will try to force the
two neurons to become equal – the
larger they are, the harder the net-
work will try.
If the neuron i is in
state 1 and the neuron j is in state
−1, a high positive weight will advise
the two neurons that it is energeti-
cally more favorable to be equal.

If wi,j is negative, its behavior will be
analoguous only that i and j are
urged to be diﬀerent. A neuron i in
state −1 would try to urge a neuron
j into state 1.

X

j∈K

 (8.1)

Zero weights lead to the two involved

neurons not inﬂuencing each other.

The weights as a whole apparently take
the way from the current state of the net-
work towards the next minimum of the en-
ergy function. We now want to discuss
how the neurons follow this way.

8.2.3 A neuron changes its state

according to the inﬂuence of
the other neurons

Once a network has been trained and
initialized with some starting state, the
change of state xk of the individual neu-
rons k occurs according to the scheme

xk(t) = fact

wj,k · xj(t − 1)

in each time step, where the function fact
generally is the binary threshold function
(ﬁg. 8.2 on the next page) with threshold
0. Colloquially speaking: a neuron k cal-
culates the sum of wj,k · xj(t − 1), which
indicates how strong and into which direc-
tion the neuron k is forced by the other
neurons j. Thus, the new state of the net-
work (time t) results from the state of the
network at the previous time t − 1. This
sum is the direction into which the neuron
k is pushed. Depending on the sign of the
sum the neuron takes state 1 or −1.
Another diﬀerence between Hopﬁeld net-
works and other already known network
topologies is the asynchronous update: A
neuron k is randomly chosen every time,
which then recalculates the activation.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

131

Chapter 8 Hopﬁeld networks

dkriesel.com

a minimum, then there is the question of
how to teach the weights to force the net-
work towards a certain minimum.

8.3 The weight matrix is

generated directly out of
the training patterns

Figure 8.2: Illustration of the binary threshold
function.

Thus, the new activation of the previously
changed neurons immediately inﬂuences
the network, i.e. one time step indicates
the change of a single neuron.
Regardless of the aforementioned random
selection of the neuron, a Hopﬁeld net-
work is often much easier to implement:
The neurons are simply processed one af-
ter the other and their activations are re-
calculated until no more changes occur.
Deﬁnition 8.4 (Change in the state of
a Hopﬁeld network). The change of state
of the neurons occurs asynchronously with
the neuron to be updated being randomly
chosen and the new state being generated
by means of this rule:

xk(t) = fact

wj,k · xj(t − 1)

X

j∈J

 .

The aim is to generate minima on the
mentioned energy surface, so that at an
input the network can converge to them.
As with many other network paradigms,
we use a set P of training patterns p ∈
{1,−1}|K|, representing the minima of our
energy surface.
Unlike many other network paradigms, we
do not look for the minima of an unknown
error function but deﬁne minima on such a
function. The purpose is that the network
shall automatically take the closest min-
imum when the input is presented. For
now this seems unusual, but we will un-
derstand the whole purpose later.
Roughly speaking, the training of a Hop-
ﬁeld network is done by training each train-
ing pattern exactly once using the rule
described in the following (Single Shot
Learning), where pi and pj are the states
of the neurons i and j under p ∈ P:

wi,j = X

p∈P

pi · pj

(8.2)

random
neuron
calculates
new
activation

Now that we know how the weights inﬂu-
ence the changes in the states of the neu-
rons and force the entire network towards

This results in the weight matrix W. Col-
loquially speaking: We initialize the net-
work by means of a training pattern and
then process weights wi,j one after another.

132

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

−1−0.5 0 0.5 1−4−2 0 2 4f(x)xHeaviside Functiondkriesel.com

8.4 Autoassociation and traditional application

For each of these weights we verify: Are
the neurons i, j n the same state or do the
states vary? In the ﬁrst case we add 1
to the weight, in the second case we add
−1.
This we repeat for each training pattern
p ∈ P. Finally, the values of the weights
wi,j are high when i and j corresponded
with many training patterns. Colloquially
speaking, this high value tells the neurons:
"Often, it is energetically favorable to hold
the same state". The same applies to neg-
ative weights.
Due to this training we can store a certain
ﬁxed number of patterns p in the weight
matrix. At an input x the network will
converge to the stored pattern that is clos-
est to the input p.
Unfortunately, the number of the maxi-
mum storable and reconstructible patterns
p is limited to

|P|MAX ≈ 0.139 · |K|,

(8.3)
which in turn only applies to orthogo-
nal patterns. This was shown by precise
(and time-consuming) mathematical anal-
yses, which we do not want to specify
now. If more patterns are entered, already
stored information will be destroyed.
Deﬁnition 8.5 (Learning rule for Hop-
ﬁeld networks). The individual elements
of the weight matrix W are deﬁned by a
single processing of the learning rule

wi,j = X

p∈P

pi · pj,

where the diagonal of the matrix is covered
with zeros. Here, no more than |P|MAX ≈

0.139·|K| training samples can be trained
and at the same time maintain their func-
tion.

Now we know the functionality of Hopﬁeld
networks but nothing about their practical
use.

8.4 Autoassociation and

traditional application

Hopﬁeld networks,
like those mentioned
above, are called autoassociators. An
autoassociator a exactly shows the afore- (cid:74)a
mentioned behavior:
Firstly, when a
known pattern p is entered, exactly this
known pattern is returned. Thus,

a(p) = p,

with a being the associative mapping. Sec-
ondly, and that is the practical use, this
also works with inputs that are close to a
pattern:

a(p + ε) = p.

Afterwards, the autoassociator is, in any
case, in a stable state, namely in the state
p.
If the set of patterns P consists of, for ex-
ample, letters or other characters in the
form of pixels, the network will be able to
correctly recognize deformed or noisy let-
ters with high probability (ﬁg. 8.3 on the
following page).
The primary ﬁelds of application of Hop-
ﬁeld networks are pattern recognition
and pattern completion, such as the zip

network
restores
damaged
inputs

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

133

Chapter 8 Hopﬁeld networks

dkriesel.com

code recognition on letters in the eighties.
But soon the Hopﬁeld networks were re-
placed by other systems in most of their
ﬁelds of application, for example by OCR
systems in the ﬁeld of letter recognition.
Today Hopﬁeld networks are virtually no
longer used, they have not become estab-
lished in practice.

8.5 Heteroassociation and

analogies to neural data
storage

So far we have been introduced to Hopﬁeld
networks that converge from an arbitrary
input into the closest minimum of a static
energy surface.

Another variant is a dynamic energy sur-
face: Here, the appearance of the energy
surface depends on the current state and
we receive a heteroassociator instead of
an autoassociator. For a heteroassocia-
tor

a(p + ε) = p

is no longer true, but rather

h(p + ε) = q,

which means that a pattern is mapped
onto another one. h is the heteroasso- (cid:74)h
ciative mapping. Such heteroassociations
are achieved by means of an asymmetric
weight matrix V .

Figure 8.3: Illustration of the convergence of an
exemplary Hopﬁeld network. Each of the pic-
tures has 10 × 12 = 120 binary pixels.
In the
Hopﬁeld network each pixel corresponds to one
neuron. The upper illustration shows the train-
ing samples, the lower shows the convergence of
a heavily noisy 3 to the corresponding training
sample.

134

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

8.5 Heteroassociation and analogies to neural data storage

Heteroassociations connected in series of
the form

h(p + ε) = q
h(q + ε) = r
h(r + ε) = s

...

h(z + ε) = p

can provoke a fast cycle of states

p → q → r → s → . . . → z → p,

whereby a single pattern is never com-
pletely accepted: Before a pattern is en-
tirely completed, the heteroassociation al-
ready tries to generate the successor of this
pattern. Additionally, the network would
never stop, since after having reached the
last state z, it would proceed to the ﬁrst
state p again.

8.5.1 Generating the

heteroassociative matrix

V (cid:73)
v(cid:73)

q(cid:73)

We generate the matrix V by means of el-
ements v very similar to the autoassocia-
tive matrix with p being (per transition)
the training sample before the transition
and q being the training sample to be gen-
erated from p:

vi,j = X

p,q∈P,p6=q

piqj

(8.4)

netword
is instable
while
changing
states

The diagonal of the matrix is again ﬁlled
with zeros. The neuron states are, as al-
ways, adapted during operation. Several
transitions can be introduced into the ma-
trix by a simple addition, whereby the said
limitation exists here, too.

Deﬁnition 8.6 (Learning rule for the het-
eroassociative matrix). For two training
samples p being predecessor and q being
successor of a heteroassociative transition
the weights of the heteroassociative matrix
V result from the learning rule

vi,j = X

piqj,

p,q∈P,p6=q

with several heteroassociations being intro-
duced into the network by a simple addi-
tion.

8.5.2 Stabilizing the

heteroassociations

We have already mentioned the problem
that the patterns are not completely gen-
erated but that the next pattern is already
beginning before the generation of the pre-
vious pattern is ﬁnished.

This problem can be avoided by not only
inﬂuencing the network by means of the
heteroassociative matrix V but also by
the already known autoassociative matrix
W.

Additionally, the neuron adaptation rule
is changed so that competing terms are
generated: One term autoassociating an
existing pattern and one term trying to
convert the very same pattern into its suc-
cessor. The associative rule provokes that
the network stabilizes a pattern, remains

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

135

Chapter 8 Hopﬁeld networks

dkriesel.com

there for a while, goes on to the next pat-
tern, and so on.
xi(t + 1) =

(8.5)



X
|

fact

wi,jxj(t)

j∈K
autoassociation

{z



+X
|

k∈K

}

vi,kxk(t − ∆t)
{z
}

heteroassociation

∆t(cid:73)

stable change
in states

Here, the value ∆t causes, descriptively
speaking, the inﬂuence of the matrix V
to be delayed, since it only refers to a
network being ∆t versions behind. The
result is a change in state, during which
the individual states are stable for a short
while. If ∆t is set to, for example, twenty
steps, then the asymmetric weight matrix
will realize any change in the network only
twenty steps later so that it initially works
with the autoassociative matrix (since it
still perceives the predecessor pattern of
the current one), and only after that it will
work against it.

8.5.3 Biological motivation of

heterassociation

From a biological point of view the transi-
tion of stable states into other stable states
is highly motivated: At least in the begin-
ning of the nineties it was assumed that
the Hopﬁeld modell will achieve an ap-
proximation of the state dynamics in the
brain, which realizes much by means of
state chains: When I would ask you, dear
reader, to recite the alphabet, you gener-
ally will manage this better than (please
try it immediately) to answer the follow-
ing question:

Which letter in the alphabet follows the

letter P ?

Another example is the phenomenon that
one cannot remember a situation, but the
place at which one memorized it the last
time is perfectly known.
If one returns
to this place, the forgotten situation often
comes back to mind.

8.6 Continuous Hopﬁeld

networks

So far, we only have discussed Hopﬁeld net-
works with binary activations. But Hop-
ﬁeld also described a version of his net-
works with continuous activations [Hop84],
which we want to cover at least brieﬂy:
continuous Hopﬁeld networks. Here,
the activation is no longer calculated by
the binary threshold function but by the
Fermi function with temperature parame-
ters (ﬁg. 8.4 on the next page).

Here, the network is stable for symmetric
weight matrices with zeros on the diagonal,
too.

Hopﬁeld also stated, that continuous Hop-
ﬁeld networks can be applied to ﬁnd ac-
ceptable solutions for the NP-hard trav-
elling salesman problem [HT85]. Accord-
ing to some veriﬁcation trials [Zel94] this
statement can’t be kept up any more. But
today there are faster algorithms for han-
dling this problem and therefore the Hop-
ﬁeld network is no longer used here.

136

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

8.6 Continuous Hopﬁeld networks

Figure 8.4: The already known Fermi function
with diﬀerent temperature parameter variations.

Exercises

Exercise 14.
Indicate the storage re-
quirements for a Hopﬁeld network with
|K| = 1000 neurons when the weights wi,j
shall be stored as integers. Is it possible
to limit the value range of the weights in
order to save storage space?
Exercise 15. Compute the weights wi,j
for a Hopﬁeld network using the training
set

P ={(−1,−1,−1,−1,−1, 1);

(−1, 1, 1,−1,−1,−1);
(1,−1,−1, 1,−1, 1)}.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

137

 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4f(x)xFermi Function with Temperature ParameterChapter 9
Learning vector quantization

Learning Vector Quantization is a learning procedure with the aim to represent
the vector training sets divided into predeﬁned classes as well as possible by
using a few representative vectors. If this has been managed, vectors which
were unkown until then could easily be assigned to one of these classes.

Slowly, part II of this text is nearing its
end – and therefore I want to write a last
chapter for this part that will be a smooth
transition into the next one: A chapter
about the learning vector quantization
(abbreviated LVQ) [Koh89] described by
Teuvo Kohonen, which can be charac-
terized as being related to the self orga-
nizing feature maps. These SOMs are de-
scribed in the next chapter that already
belongs to part III of this text, since SOMs
learn unsupervised. Thus, after the explo-
ration of LVQ I want to bid farewell to
supervised learning.

Previously, I want to announce that there
are diﬀerent variations of LVQ, which will
be mentioned but not exactly represented.
The goal of this chapter is rather to ana-
lyze the underlying principle.

9.1 About quantization

In order to explore the learning vec-
tor quantization we should at ﬁrst get
a clearer picture of what quantization
(which can also be referred to as dis-
cretization) is.
Everybody knows the sequence of discrete
numbers

N = {1, 2, 3, . . .},

which contains the natural numbers. Dis-
crete means, that this sequence consists of
separated elements that are not intercon-
nected. The elements of our example are
exactly such numbers, because the natural
numbers do not include, for example, num-
bers between 1 and 2. On the other hand,
the sequence of real numbers R, for in-
stance, is continuous: It does not matter
how close two selected numbers are, there
will always be a number between them.

139

discrete
= separated

Chapter 9 Learning vector quantization

dkriesel.com

Quantization means that a continuous
space is divided into discrete sections: By
deleting, for example, all decimal places
of the real number 2.71828, it could be
assigned to the natural number 2. Here
it is obvious that any other number hav-
ing a 2 in front of the comma would also
be assigned to the natural number 2, i.e.
2 would be some kind of representative
for all real numbers within the interval
[2; 3).
It must be noted that a sequence can be ir-
regularly quantized, too: For instance, the
timeline for a week could be quantized into
working days and weekend.
A special case of quantization is digiti-
zation: In case of digitization we always
talk about regular quantization of a con-
tinuous space into a number system with
respect to a certain basis. If we enter, for
example, some numbers into the computer,
these numbers will be digitized into the bi-
nary system (basis 2).
Deﬁnition 9.1 (Quantization). Separa-
tion of a continuous space into discrete sec-
tions.
Deﬁnition 9.2 (Digitization). Regular
quantization.

9.2 LVQ divides the input

space into separate areas

Now it is almost possible to describe by
means of its name what LVQ should en-
able us to do: A set of representatives
should be used to divide an input space

input space
reduced to
vector repre-
sentatives

into classes that reﬂect the input space
as well as possible (ﬁg. 9.1 on the facing
page). Thus, each element of the input
space should be assigned to a vector as a
representative, i.e. to a class, where the
set of these representatives should repre-
sent the entire input space as precisely as
possible. Such a vector is called codebook
vector. A codebook vector is the represen-
tative of exactly those input space vectors
lying closest to it, which divides the input
space into the said discrete areas.
It is to be emphasized that we have to
know in advance how many classes we
have and which training sample belongs
to which class. Furthermore, it is impor-
tant that the classes must not be disjoint,
which means they may overlap.
Such separation of data into classes is in-
teresting for many problems for which it
is useful to explore only some characteris-
tic representatives instead of the possibly
huge set of all vectors – be it because it is
less time-consuming or because it is suﬃ-
ciently precise.

9.3 Using codebook vectors:

the nearest one is the
winner

The use of a prepared set of codebook vec-
tors is very simple: For an input vector y
the class association is easily decided by
considering which codebook vector is the
closest – so, the codebook vectors build a
voronoi diagram out of the set. Since

closest
vector
wins

140

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

9.4 Adjusting codebook vectors

Figure 9.1: BExamples for quantization of a two-dimensional input space. DThe lines represent
the class limit, the × mark the codebook vectors.

each codebook vector can clearly be asso-
ciated to a class, each input vector is asso-
ciated to a class, too.

are used to cause a previously deﬁned num-
ber of randomly initialized codebook vec-
tors to reﬂect the training data as precisely
as possible.

9.4 Adjusting codebook

vectors

As we have already indicated, the LVQ is
a supervised learning procedure. Thus, we
have a teaching input that tells the learn-
ing procedure whether the classiﬁcation of
the input pattern is right or wrong:
In
other words, we have to know in advance
the number of classes to be represented or
the number of codebook vectors.
Roughly speaking,
it is the aim of the
learning procedure that training samples

9.4.1 The procedure of learning

Learning works according to a simple
scheme. We have (since learning is su-
pervised) a set P of |P| training samples.
Additionally, we already know that classes
are predeﬁned, too, i.e. we also have a set
of classes C. A codebook vector is clearly
assigned to each class. Thus, we can say
that the set of classes |C| contains many
codebook vectors C1, C2, . . . , C|C|.
This leads to the structure of the training
samples: They are of the form (p, c) and

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

141

Chapter 9 Learning vector quantization

dkriesel.com

therefore contain the training input vector
p and its class aﬃliation c. For the class
aﬃliation

c ∈ {1, 2, . . . ,|C|}

holds, which means that it clearly assigns
the training sample to a class or a code-
book vector.

Intuitively, we could say about learning:
"Why a learning procedure? We calculate
the average of all class members and place
their codebook vectors there – and that’s
it." But we will see soon that our learning
procedure can do a lot more.

I only want to brieﬂy discuss the steps
of the fundamental LVQ learning proce-
dure:

Initialization: We place our set of code-
book vectors on random positions in
the input space.

Training sample: A training sample p of
our training set P is selected and pre-
sented.

Distance measurement: We measure the
distance ||p − C|| between all code-
book vectors C1, C2, . . . , C|C| and our
input p.

Winner: The

wins, i.e. the one with

closest

codebook vector

||p − Ci||.

min
Ci∈C

Learning process: The learning process

takes place according to the rule

∆Ci = η(t) · h(p, Ci) · (p − Ci)
(9.1)
(9.2)

Ci(t + 1) = Ci(t) + ∆Ci,

which we now want to break down.

. We have already seen that the ﬁrst
factor η(t) is a time-dependent learn-
ing rate allowing us to diﬀerentiate
between large learning steps and ﬁne
tuning.

. The last factor (p − Ci) is obviously
the direction toward which the code-
book vector is moved.

. But the function h(p, Ci) is the core of
the rule: It implements a distinction
of cases.

Assignment is correct: The winner
vector is the codebook vector of
the class that includes p. In this
case, the function provides posi-
tive values and the codebook vec-
tor moves towards p.

Assignment is wrong: The winner
vector does not represent the
class that includes p. Therefore
it moves away from p.

We can see that our deﬁnition of the func-
tion h was not precise enough. With good
reason: From here on, the LVQ is divided
into diﬀerent nuances, dependent of how
exactly h and the learning rate should
be deﬁned (called LVQ1, LVQ2, LVQ3,

Important!

142

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

9.5 Connection to neural networks

OLVQ, etc). The diﬀerences are, for in-
stance, in the strength of the codebook vec-
tor movements. They are not all based on
the same principle described here, and as
announced I don’t want to discuss them
any further. Therefore I don’t give any
formal deﬁnition regarding the aforemen-
tioned learning rule and LVQ.

Exercises

Exercise 16.
Indicate a quantization
which equally distributes all vectors H ∈
H in the ﬁve-dimensional unit cube H into
one of 1024 classes.

vectors
= neurons?

9.5 Connection to neural

networks

Until now, in spite of the learning process,
the question was what LVQ has to do with
neural networks. The codebook vectors
can be understood as neurons with a ﬁxed
position within the input space, similar to
RBF networks. Additionally, in nature it
often occurs that in a group one neuron
may ﬁre (a winner neuron, here: a code-
book vector) and, in return, inhibits all
other neurons.

I decided to place this brief chapter about
learning vector quantization here so that
this approach can be continued in the fol-
lowing chapter about self-organizing maps:
We will classify further inputs by means of
neurons distributed throughout the input
space, only that this time, we do not know
which input belongs to which class.

Now let us take a look at the unsupervised
learning networks!

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

143

Part III

Unsupervised learning network

paradigms

145

Chapter 10
Self-organizing feature maps

A paradigm of unsupervised learning neural networks, which maps an input
space by its ﬁxed topology and thus independently looks for simililarities.
Function, learning procedure, variations and neural gas.

no output,
but active
neuron

If you take a look at the concepts of biologi-
cal neural networks mentioned in the intro-
duction, one question will arise: How does
our brain store and recall the impressions
it receives every day. Let me point out
that the brain does not have any training
samples and therefore no "desired output".
And while already considering this subject
we realize that there is no output in this
sense at all, too. Our brain responds to
external input by changes in state. These
are, so to speak, its output.

How are
data stored
in the
brain?

Based on this principle and exploring
the question of how biological neural net-
works organize themselves, Teuvo Ko-
honen developed in the Eighties his self-
organizing feature maps [Koh82, Koh98],
shortly referred to as self-organizing
maps or SOMs. A paradigm of neural
networks where the output is the state of
the network, which learns completely un-
supervised, i.e. without a teacher.

Unlike the other network paradigms we
have already got to know, for SOMs it is
unnecessary to ask what the neurons calcu-
late. We only ask which neuron is active at
the moment. Biologically, this is very mo-
tivated: If in biology the neurons are con-
nected to certain muscles, it will be less
interesting to know how strong a certain
muscle is contracted but which muscle is
activated. In other words: We are not in-
terested in the exact output of the neuron
but in knowing which neuron provides out-
put. Thus, SOMs are considerably more
related to biology than, for example, the
feedforward networks, which are increas-
ingly used for calculations.

10.1 Structure of a

self-organizing map

Typically, SOMs have – like our brain –
the task to map a high-dimensional
in-
put (N dimensions) onto areas in a low-

147

Chapter 10 Self-organizing feature maps

dkriesel.com

dimensional grid of cells (G dimensions)
to draw a map of the high-dimensional
space, so to speak. To generate this map,
the SOM simply obtains arbitrary many
points of the input space. During the in-
put of the points the SOM will try to cover
as good as possible the positions on which
the points appear by its neurons. This par-
ticularly means, that every neuron can be
assigned to a certain position in the input
space.

At ﬁrst, these facts seem to be a bit con-
fusing, and it is recommended to brieﬂy
reﬂect about them. There are two spaces
in which SOMs are working:

. The N-dimensional input space and

. the G-dimensional grid on which the
neurons are lying and which indi-
cates the neighborhood relationships
between the neurons and therefore
the network topology.

In a one-dimensional grid, the neurons
could be, for instance,
like pearls on a
string. Every neuron would have exactly
two neighbors (except for the two end neu-
rons). A two-dimensional grid could be a
square array of neurons (ﬁg. 10.1). An-
other possible array in two-dimensional
space would be some kind of honeycomb
shape.
Irregular topologies are possible,
too, but not very often. Topolgies with
more dimensions and considerably more
neighborhood relationships would also be
possible, but due to their lack of visualiza-
tion capability they are not employed very
often.

high-dim.
input
↓
low-dim.
map

input space
and topology

Important!

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

topologies of a self-
Figure 10.1: Example
organizing map.
Above we can see a one-
dimensional topology, below a two-dimensional
one.

Even if N = G is true, the two spaces are
not equal and have to be distinguished. In
this special case they only have the same
dimension.
Initially, we will brieﬂy and formally re-
gard the functionality of a self-organizing
map and then make it clear by means of
some examples.
Deﬁnition 10.1 (SOM neuron). Similar
to the neurons in an RBF network a SOM
neuron k does not occupy a ﬁxed position
ck (a center) in the input space.
Deﬁnition 10.2 (Self-organizing map).
A self-organizing map is a set K of SOM
neurons. If an input vector is entered, ex- (cid:74)K
actly that neuron k ∈ K is activated which

(cid:74)c

148

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

input
↓

winner

N(cid:73)

i(cid:73)
k(cid:73)

G(cid:73)

dkriesel.com

10.3 Training

is closest to the input pattern in the input
space. The dimension of the input space
is referred to as N.
Deﬁnition 10.3 (Topology). The neu-
rons are interconnected by neighborhood
relationships. These neighborhood rela-
tionships are called topology. The train-
ing of a SOM is highly inﬂuenced by the
It is deﬁned by the topology
topology.
function h(i, k, t), where i is the winner
neuron1 ist, k the neuron to be adapted
(which will be discussed later) and t the
timestep. The dimension of the topology
is referred to as G.

10.2 SOMs always activate

the neuron with the
least distance to an
input pattern

Like many other neural networks, the
SOM has to be trained before it can be
used. But let us regard the very simple
functionality of a complete self-organizing
map before training, since there are many
analogies to the training. Functionality
consists of the following steps:
Input of an arbitrary value p of the input

space RN.

Calculation of the distance between ev-
ery neuron k and p by means of a
norm, i.e. calculation of ||p − ck||.
One neuron becomes active, namely

such neuron i with the shortest

1 We will learn soon what a winner neuron is.

calculated distance to the input. All
other neurons remain inactive.This
paradigm of activity is also called
winner-takes-all scheme. The output
we expect due to the input of a SOM
shows which neuron becomes active.
In many literature citations, the descrip-
tion of SOMs is more formal: Often an
input layer is described that is completely
linked towards an SOM layer. Then the in-
put layer (N neurons) forwards all inputs
to the SOM layer. The SOM layer is later-
ally linked in itself so that a winner neuron
can be established and inhibit the other
neurons. I think that this explanation of
a SOM is not very descriptive and there-
fore I tried to provide a clearer description
of the network structure.
Now the question is which neuron is ac-
tivated by which input – and the answer
is given by the network itself during train-
ing.

10.3 Training

[Training makes the SOM topology cover
the input space] The training of a SOM
is nearly as straightforward as the func-
tionality described above. Basically, it is
structured into ﬁve steps, which partially
correspond to those of functionality.
Initialization: The network starts with
random neuron centers ck ∈ RN from
the input space.

Creating an input pattern: A stimulus,
i.e. a point p, is selected from the

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

149

Chapter 10 Self-organizing feature maps

dkriesel.com

training:
input,
→ winner i,
change in
position
i and
neighbors

input space RN. Now this stimulus is
entered into the network.

Distance measurement: Then the dis-
tance ||p− ck|| is determined for every
neuron k in the network.

Winner takes all: The winner neuron i
is determined, which has the smallest
distance to p, i.e. which fulﬁlls the
condition

||p − ci|| ≤ ||p − ck|| ∀ k 6= i

. You can see that from several win-
ner neurons one can be selected at
will.

Adapting the centers: The neuron cen-
ters are moved within the input space
according to the rule2

∆ck = η(t) · h(i, k, t) · (p − ck),

where the values ∆ck are simply
added to the existing centers. The
last factor shows that the change in
position of the neurons k is propor-
tional to the distance to the input
pattern p and, as usual, to a time-
dependent learning rate η(t). The
above-mentioned network topology ex-
erts its inﬂuence by means of the func-
tion h(i, k, t), which will be discussed
in the following.

2 Note: In many sources this rule is written ηh(p −
ck), which wrongly leads the reader to believe that
h is a constant. This problem can easily be solved
by not omitting the multiplication dots ·.

Deﬁnition 10.4 (SOM learning rule). A
SOM is trained by presenting an input pat-
tern and determining the associated win-
ner neuron. The winner neuron and its
neighbor neurons, which are deﬁned by the
topology function, then adapt their cen-
ters according to the rule

∆ck = η(t) · h(i, k, t) · (p − ck),

(10.1)
(10.2)

ck(t + 1) = ck(t) + ∆ck(t).

10.3.1 The topology function
deﬁnes, how a learning
neuron inﬂuences its
neighbors

The topology function h is not deﬁned
on the input space but on the grid and rep-
resents the neighborhood relationships be-
tween the neurons, i.e. the topology of the
network. It can be time-dependent (which
it often is) – which explains the parameter
t. The parameter k is the index running
through all neurons, and the parameter i
is the index of the winner neuron.
In principle, the function shall take a large
value if k is the neighbor of the winner neu-
ron or even the winner neuron itself, and
small values if not. SMore precise deﬁni-
tion: The topology function must be uni-
modal, i.e. it must have exactly one maxi-
mum. This maximum must be next to the
winner neuron i, for which the distance to
itself certainly is 0.
Additionally, the time-dependence enables
us, for example, to reduce the neighbor-
hood in the course of time.

deﬁned on
the grid

only 1 maximum
for the winner

150

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

10.3 Training

In order to be able to output large values
for the neighbors of i and small values for
non-neighbors, the function h needs some
kind of distance notion on the grid because
from somewhere it has to know how far i
and k are apart from each other on the
grid. There are diﬀerent methods to cal-
culate this distance.

On a two-dimensional grid we could apply,
for instance, the Euclidean distance (lower
part of ﬁg. 10.2) or on a one-dimensional
grid we could simply use the number of the
connections between the neurons i and k
(upper part of the same ﬁgure).

Deﬁnition 10.5 (Topology function).
The topology function h(i, k, t) describes
the neighborhood relationships
in the
topology.
It can be any unimodal func-
tion that reaches its maximum when i = k
gilt. Time-dependence is optional, but of-
ten used.

10.3.1.1 Introduction of common

distance and topology
functions

A common distance function would be, for
example, the already known Gaussian
bell (see ﬁg. 10.3 on page 153). It is uni-
modal with a maximum close to 0. Addi-
tionally, its width can be changed by ap-
plying its parameter σ , which can be used
to realize the neighborhood being reduced
in the course of time: We simply relate the
time-dependence to the σ and the result is

σ(cid:73)

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

?>=<
89:;i
/.-,
()*+
/.-,
()*+
?>=<
89:;i
/.-,
()*+

1

/?>=<
89:;k
/.-,
()*+
/.-,
()*+
//.-,
()*+o
/.-,
()*+

8qqqqqq

2.23qqqqqqq

/.-,
()*+
/.-,
()*+
?>=<
89:;kO
//.-,
()*+
/.-,
()*+

/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+
/.-,
()*+

Figure 10.2: Example distances of
a one-
dimensional SOM topology (above) and a two-
dimensional SOM topology (below) between two
neurons i and k. In the lower case the Euclidean
distance is determined (in two-dimensional space
equivalent to the Pythagoream theorem). In the
upper case we simply count the discrete path
length between i and k. To simplify matters I
required a ﬁxed grid edge length of 1 in both
cases.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

151

o
o
/
O


x
x
8
o
o
/
o
/
Chapter 10 Self-organizing feature maps

dkriesel.com

a monotonically decreasing σ(t). Then our
topology function could look like this:

Typical sizes of the target value of a learn-
ing rate are two sizes smaller than the ini-
tial value, e.g

(cid:16)− ||gi−ck||2

2·σ(t)2

(cid:17)

h(i, k, t) = e

,

(10.3)

0.01 < η < 0.6

where gi and gk represent the neuron po-
sitions on the grid, not the neuron posi-
tions in the input space, which would be
referred to as ci and ck.
Other functions that can be used in-
stead of the Gaussian function are,
for
instance, the cone function, the cylin-
der function or the Mexican hat func-
tion (ﬁg. 10.3 on the facing page). Here,
the Mexican hat function oﬀers a particu-
lar biological motivation: Due to its neg-
ative digits it rejects some neurons close
to the winner neuron, a behavior that has
already been observed in nature. This can
cause sharply separated map areas – and
that is exactly why the Mexican hat func-
tion has been suggested by Teuvo Koho-
nen himself. But this adjustment charac-
teristic is not necessary for the functional-
ity of the map, it could even be possible
that the map would diverge, i.e.
it could
virtually explode.

10.3.2 Learning rates and

neighborhoods can decrease
monotonically over time

To avoid that the later training phases
forcefully pull the entire map towards
a new pattern, the SOMs often work
with temporally monotonically decreasing
learning rates and neighborhood sizes. At
ﬁrst, let us talk about the learning rate:

could be true. But this size must also de-
pend on the network topology or the size
of the neighborhood.

As we have already seen, a decreasing
neighborhood size can be realized, for ex-
ample, by means of a time-dependent,
monotonically decreasing σ with the
Gaussin bell being used in the topology
function.

The advantage of a decreasing neighbor-
hood size is that in the beginning a moving
neuron "pulls along" many neurons in its
vicinity, i.e. the randomly initialized net-
work can unfold fast and properly in the
beginning. In the end of the learning pro-
cess, only a few neurons are inﬂuenced at
the same time which stiﬀens the network
as a whole but enables a good "ﬁne tuning"
of the individual neurons.

It must be noted that

h · η ≤ 1

must always be true, since otherwise the
neurons would constantly miss the current
training sample.

But enough of theory – let us take a look
at a SOM in action!

152

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

10.3 Training

Figure 10.3: Gaussian bell, cone function, cylinder function and the Mexican hat function sug-
gested by Kohonen as examples for topology functions of a SOM..

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

153

 0 0.2 0.4 0.6 0.8 1−2−1.5−1−0.5 0 0.5 1 1.5 2h(r)rGaussian in 1D 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4f(x)xCone Function 0 0.2 0.4 0.6 0.8 1−4−2 0 2 4f(x)xCylinder Funktion−1.5−1−0.5 0 0.5 1 1.5 2 2.5 3 3.5−3−2−1 0 1 2 3f(x)xMexican Hat FunctionChapter 10 Self-organizing feature maps

dkriesel.com

?>=<
89:;1

?>=<
89:;4
>>>>>>>>
?>=<
89:;3

?>=<
89:;7

p

?>=<
89:;2

?>=<
89:;6
?>=<
89:;5

?>=<
89:;1
?>=<
89:;2
?>=<
89:;3
?>=<
89:;4
?>=<
89:;5
?>=<
89:;6
?>=<
89:;7

Figure 10.4: Illustration of the two-dimensional input space (left) and the one-dimensional topolgy
space (right) of a self-organizing map. Neuron 3 is the winner neuron since it is closest to p.
In
the topology, the neurons 2 and 4 are the neighbors of 3. The arrows mark the movement of the
winner neuron and its neighbors towards the training sample p.
To illustrate the one-dimensional topology of the network, it is plotted into the input space by the
dotted line. The arrows mark the movement of the winner neuron and its neighbors towards the
pattern.

154

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)







/
/
/
/
dkriesel.com

10.4 Examples

10.4 Examples for the

functionality of SOMs

Let us begin with a simple, mentally com-
prehensible example.
In this example, we use a two-dimensional
input space, i.e. N = 2 is true. Let the
grid structure be one-dimensional (G = 1).
Furthermore, our example SOM should
consist of 7 neurons and the learning rate
should be η = 0.5.
The neighborhood function is also kept
simple so that we will be able to mentally
comprehend the network:



h(i, k, t) =

1 k direct neighbor of i,
1 k = i,
0

otherw.

(10.4)

Now let us take a look at the above-
mentioned network with random initializa-
tion of the centers (ﬁg. 10.4 on the preced-
ing page) and enter a training sample p.
Obviously, in our example the input pat-
tern is closest to neuron 3, i.e. this is the
winning neuron.
We
remember
SOMs

learning rule

the

for

∆ck = η(t) · h(i, k, t) · (p − ck)

Thus, the factor (p− ck) indicates the
vector of the neuron k to the pattern
p. This is now multiplied by diﬀerent
scalars:

Our topology function h indicates that
only the winner neuron and its two
closest neighbors (here: 2 and 4) are
allowed to learn by returning 0 for
all other neurons. A time-dependence
is not speciﬁed. Thus, our vector
(p − ck) is multiplied by either 1 or
0.

The learning rate indicates, as always,
the strength of learning. As already
mentioned, η = 0.5, i. e. all in all, the
result is that the winner neuron and
its neighbors (here: 2, 3 and 4) ap-
proximate the pattern p half the way
(in the ﬁgure marked by arrows).

Although the center of neuron 7 – seen
from the input space – is considerably
closer to the input pattern p than neuron
2, neuron 2 is learning and neuron 7 is
not.
I want to remind that the network
topology speciﬁes which neuron is allowed
to learn and not its position in the input
space. This is exactly the mechanism by
which a topology can signiﬁcantly cover an
input space without having to be related
to it by any sort.

topology
speciﬁes,
who will learn

and process the three factors from the
back:
Learning direction: Remember that the
neuron centers ck are vectors in the
input space, as well as the pattern p.

After the adaptation of the neurons 2, 3
and 4 the next pattern is applied, and so
on. Another example of how such a one-
dimensional SOM can develop in a two-
dimensional
input space with uniformly
distributed input patterns in the course of

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

155

Chapter 10 Self-organizing feature maps

dkriesel.com

time can be seen in ﬁgure 10.5 on the fac-
ing page.

End states of one- and two-dimensional
SOMs with diﬀerently shaped input spaces
can be seen in ﬁgure 10.6 on page 158.
As we can see, not every input space can
be neatly covered by every network topol-
ogy. There are so called exposed neurons
– neurons which are located in an area
where no input pattern has ever been oc-
curred. A one-dimensional topology gen-
erally produces less exposed neurons than
a two-dimensional one: For instance, dur-
ing training on circularly arranged input
patterns it is nearly impossible with a two-
dimensional squared topology to avoid the
exposed neurons in the center of the cir-
cle. These are pulled in every direction
during the training so that they ﬁnally
remain in the center. But this does not
make the one-dimensional topology an op-
timal topology since it can only ﬁnd less
complex neighborhood relationships than
a multi-dimensional one.

10.4.1 Topological defects are

failures in SOM unfolding

"knot"
in map

During the unfolding of a SOM it
could happen that a topological defect
(ﬁg. 10.7) occurs, i.e. the SOM does not
unfold correctly. A topological defect can
be described at best by means of the word
"knotting".

Figure 10.7: A topological defect in a two-
dimensional SOM.

neighborhood size, because the more com-
plex the topology is (or the more neigh-
bors each neuron has, respectively, since a
three-dimensional or a honeycombed two-
dimensional topology could also be gener-
ated) the more diﬃcult it is for a randomly
initialized map to unfold.

10.5 It is possible to adjust

the resolution of certain
areas in a SOM

A remedy for topological defects could
be to increase the initial values for the

We have seen that a SOM is trained by
entering input patterns of the input space

156

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

10.5 Adjustment of resolution and position-dependent learning rate

Figure 10.5: Behavior of a SOM with one-dimensional topology (G = 1) after the input of 0, 100,
300, 500, 5000, 50000, 70000 and 80000 randomly distributed input patterns p ∈ R2. During the
training η decreased from 1.0 to 0.1, the σ parameter of the Gauss function decreased from 10.0
to 0.2.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

157

Chapter 10 Self-organizing feature maps

dkriesel.com

Figure 10.6: End states of one-dimensional (left column) and two-dimensional (right column)
SOMs on diﬀerent input spaces. 200 neurons were used for the one-dimensional topology, 10 × 10
neurons for the two-dimensionsal topology and 80.000 input patterns for all maps.

158

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

more
patterns
↓
higher
resolution

dkriesel.com

10.6 Application

RN one after another, again and again so
that the SOM will be aligned with these
patterns and map them. It could happen
that we want a certain subset U of the in-
put space to be mapped more precise than
the other ones.
This problem can easily be solved by
means of SOMs: During the training dis-
proportionally many input patterns of the
area U are presented to the SOM. If the
number of training patterns of U ⊂ RN
presented to the SOM exceeds the number
of those patterns of the remaining RN \ U,
then more neurons will group there while
the remaining neurons are sparsely dis-
tributed on RN \ U (ﬁg. 10.8 on the next
page).
As you can see in the illustration, the edge
of the SOM could be deformed. This can
be compensated by assigning to the edge
of the input space a slightly higher proba-
bility of being hit by training patterns (an
often applied approach for reaching every
corner with the SOMs).
Also, a higher learning rate is often used
for edge and corner neurons, since they are
only pulled into the center by the topol-
ogy. This also results in a signiﬁcantly im-
proved corner coverage.

10.6 Application of SOMs

Regarding the biologically inspired asso-
ciative data storage, there are many
ﬁelds of application for self-organizing
maps and their variations.

For example, the diﬀerent phonemes of
the ﬁnnish language have successfully been
mapped onto a SOM with a two dimen-
sional discrete grid topology and therefore
neighborhoods have been found (a SOM
does nothing else than ﬁnding neighbor-
hood relationships).
So one tries once
more to break down a high-dimensional
space into a low-dimensional space (the
topology), looks if some structures have
been developed – et voilà: clearly deﬁned
areas for the individual phenomenons are
formed.
Teuvo Kohonen himself made the ef-
fort to search many papers mentioning his
SOMs in their keywords. In this large in-
put space the individual papers now indi-
vidual positions, depending on the occur-
rence of keywords. Then Kohonen created
a SOM with G = 2 and used it to map the
high-dimensional "paper space" developed
by him.

Thus,
it is possible to enter any paper
into the completely trained SOM and look
which neuron in the SOM is activated. It
will be likely to discover that the neigh-
bored papers in the topology are interest-
ing, too. This type of brain-like context-
based search also works with many other
input spaces.

It is to be noted that the system itself
similar,
deﬁnes what is neighbored, i.e.
within the topology – and that’s why it
is so interesting.

This example shows that the position c of
the neurons in the input space is not signif-
icant. It is rather interesting to see which

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

159

SOM ﬁnds
similarities

Chapter 10 Self-organizing feature maps

dkriesel.com

Figure 10.8: Training of a SOM with G = 2 on a two-dimensional input space. On the left side,
the chance to become a training pattern was equal for each coordinate of the input space. On the
right side, for the central circle in the input space, this chance is more than ten times larger than
for the remaining input space (visible in the larger pattern density in the background). In this circle
the neurons are obviously more crowded and the remaining area is covered less dense but in both
cases the neurons are still evenly distributed. The two SOMS were trained by means of 80.000
training samples and decreasing η (1 → 0.2) as well as decreasing σ (5 → 0.5).

160

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

10.7 Variations

neuron is activated when an unknown in-
put pattern is entered. Next, we can look
at which of the previous inputs this neu-
ron was also activated – and will imme-
diately discover a group of very similar
inputs. The more the inputs within the
topology are diverging, the less things they
have in common. Virtually, the topology
generates a map of the input characteris-
tics – reduced to descriptively few dimen-
sions in relation to the input dimension.

Therefore, the topology of a SOM often
is two-dimensional so that it can be easily
visualized, while the input space can be
very high-dimensional.

10.6.1 SOMs can be used to

determine centers for RBF
neurons

SOMs arrange themselves exactly towards
the positions of the outgoing inputs. As a
result they are used, for example, to select
the centers of an RBF network. We have
already been introduced to the paradigm
of the RBF network in chapter 6.

As we have already seen,
it is possible
to control which areas of the input space
should be covered with higher resolution
- or,
in connection with RBF networks,
on which areas of our function should the
RBF network work with more neurons, i.e.
work more exactly. As a further useful fea-
ture of the combination of RBF networks
with SOMs one can use the topology ob-
tained through the SOM: During the ﬁnal
training of a RBF neuron it can be used

to inﬂuence neighboring RBF neurons in
diﬀerent ways.
For this, many neural network simulators
oﬀer an additional so-called SOM layer
in connection with the simulation of RBF
networks.

10.7 Variations of SOMs

There are diﬀerent variations of SOMs
for diﬀerent variations of representation
tasks:

10.7.1 A neural gas is a SOM

without a static topology

The neural gas is a variation of the self-
organizing maps of Thomas Martinetz
[MBS93], which has been developed from
the diﬃculty of mapping complex input
information that partially only occur in
the subspaces of the input space or even
change the subspaces (ﬁg. 10.9 on the fol-
lowing page).
The idea of a neural gas is, roughly speak-
ing, to realize a SOM without a grid struc-
ture. Due to the fact that they are de-
rived from the SOMs the learning steps
are very similar to the SOM learning steps,
but they include an additional intermedi-
ate step:

. again, random initialization of ck ∈

Rn

. selection and presentation of a pat-

tern of the input space p ∈ Rn

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

161

Chapter 10 Self-organizing feature maps

dkriesel.com

dynamic
neighborhood

Figure 10.9: A ﬁgure ﬁlling diﬀerent subspaces of the actual input space of diﬀerent positions
therefore can hardly be ﬁlled by a SOM.

. neuron distance measurement

. identiﬁcation of the winner neuron i
. Intermediate step: generation of a list
L of neurons sorted in ascending order
by their distance to the winner neu-
ron. Thus, the ﬁrst neuron in the list
L is the neuron that is closest to the
winner neuron.

. changing the centers by means of the
known rule but with the slightly mod-
iﬁed topology function

hL(i, k, t).

The function hL(i, k, t), which is slightly
modiﬁed compared with the original func-
tion h(i, k, t), now regards the ﬁrst el-
ements of the list as the neighborhood

of the winner neuron i. The direct re-
sult is that – similar to the free-ﬂoating
molecules in a gas – the neighborhood rela-
tionships between the neurons can change
anytime, and the number of neighbors is
almost arbitrary, too. The distance within
the neighborhood is now represented by
the distance within the input space.

The bulk of neurons can become as stiﬀ-
ened as a SOM by means of a constantly
decreasing neighborhood size. It does not
have a ﬁxed dimension but it can take the
dimension that is locally needed at the mo-
ment, which can be very advantageous.

A disadvantage could be that there is
no ﬁxed grid forcing the input space to
become regularly covered, and therefore
wholes can occur in the cover or neurons
can be isolated.

162

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

can classify
complex
ﬁgure

dkriesel.com

10.7 Variations

In spite of all practical hints, it is as al-
ways the user’s responsibility not to un-
derstand this text as a catalog for easy an-
swers but to explore all advantages and
disadvantages himself.

Unlike a SOM, the neighborhood of a neu-
ral gas must initially refer to all neurons
since otherwise some outliers of the ran-
dom initialization may never reach the re-
maining group. To forget this is a popular
error during the implementation of a neu-
ral gas.

With a neural gas it is possible to learn a
kind of complex input such as in ﬁg. 10.9
on the preceding page since we are not
bound to a ﬁxed-dimensional grid. But
some computational eﬀort could be neces-
sary for the permanent sorting of the list
(here, it could be eﬀective to store the list
in an ordered data structure right from the
start).
Deﬁnition 10.6 (Neural gas). A neural
gas diﬀers from a SOM by a completely dy-
namic neighborhood function. With every
learning cycle it is decided anew which neu-
rons are the neigborhood neurons of the
winner neuron. Generally, the criterion
for this decision is the distance between
the neurosn and the winner neuron in the
input space.

10.7.2 A Multi-SOM consists of

several separate SOMs

In order to present another variant of the
SOMs, I want to formulate an extended

problem: What do we do with input pat-
terns from which we know that they are
conﬁned in diﬀerent (maybe disjoint) ar-
eas?
Here, the idea is to use not only one
SOM but several ones: A multi-self-
organizing map, shortly referred to as
M-SOM [GKE01b,GKE01a,GS06]. It is
unnecessary that the SOMs have the same
topology or size, an M-SOM is just a com-
bination of M SOMs.
This learning process is analog to that of
the SOMs. However, only the neurons be-
longing to the winner SOM of each train-
ing step are adapted. Thus, it is easy to
represent two disjoint clusters of data by
means of two SOMs, even if one of the
clusters is not represented in every dimen-
sion of the input space RN. Actually, the
individual SOMs exactly reﬂect these clus-
ters.
Deﬁnition 10.7 (Multi-SOM). A multi-
SOM is nothing more than the simultane-
ous use of M SOMs.

10.7.3 A multi-neural gas consists
of several separate neural
gases

Analogous to the multi-SOM, we also have
a set of M neural gases: a multi-neural
gas [GS06, SG06]. This construct be-
haves analogous to neural gas and M-SOM:
Again, only the neurons of the winner gas
are adapted.
The reader certainly wonders what advan-
tage is there to use a multi-neural gas since

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

163

several SOMs

several gases

less computa-
tional eﬀort

Chapter 10 Self-organizing feature maps

dkriesel.com

an individual neural gas is already capa-
ble to divide into clusters and to work on
complex input patterns with changing di-
mensions. Basically, this is correct, but
a multi-neural gas has two serious advan-
tages over a simple neural gas.

1. With several gases, we can directly
tell which neuron belongs to which
gas. This is particularly important
for clustering tasks, for which multi-
neural gases have been used recently.
Simple neural gases can also ﬁnd and
cover clusters, but now we cannot rec-
ognize which neuron belongs to which
cluster.

2. A lot of computational eﬀort is saved
when large original gases are divided
into several smaller ones since (as al-
ready mentioned) the sorting of the
list L could use a lot of computa-
tional eﬀort while the sorting of sev-
eral smaller lists L1, L2, . . . , LM is less
time-consuming – even if these lists in
total contain the same number of neu-
rons.

As a result we will only obtain local in-
stead of global sortings, but in most cases
these local sortings are suﬃcient.
Now we can choose between two extreme
cases of multi-neural gases: One extreme
case is the ordinary neural gas M = 1, i.e.
we only use one single neural gas. Interest-
ing enough, the other extreme case (very
large M, a few or only one neuron per gas)
behaves analogously to the K-means clus-
tering (for more information on clustering
procedures see excursus A).

Deﬁnition 10.8 (Multi-neural gas). A
multi-neural gas is nothing more than the
simultaneous use of M neural gases.

10.7.4 Growing neural gases can
add neurons to themselves

A growing neural gas is a variation of
the aforementioned neural gas to which
more and more neurons are added accord-
ing to certain rules. Thus, this is an at-
tempt to work against the isolation of neu-
rons or the generation of larger wholes in
the cover.
Here, this subject should only be men-
tioned but not discussed.
To build a growing SOM is more diﬃcult
because new neurons have to be integrated
in the neighborhood.

Exercises

Exercise 17. A regular, two-dimensional
grid shall cover a two-dimensional surface
as "well" as possible.

1. Which grid structure would suit best

for this purpose?

2. Which criteria did you use for "well"

and "best"?

The very imprecise formulation of this ex-
ercise is intentional.

164

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Chapter 11
Adaptive resonance theory

An ART network in its original form shall classify binary input vectors, i.e. to
assign them to a 1-out-of-n output. Simultaneously, the so far unclassiﬁed
patterns shall be recognized and assigned to a new class.

As in the other smaller chapters, we want
to try to ﬁgure out the basic idea of
the adaptive resonance theory (abbre-
viated: ART) without discussing its the-
ory profoundly.

In several sections we have already men-
tioned that it is diﬃcult to use neural
networks for the learning of new informa-
tion in addition to but without destroying
the already existing information. This cir-
cumstance is called stability / plasticity
dilemma.
In 1987, Stephen Grossberg and Gail
Carpenter published the ﬁrst version of
their ART network [Gro76] in order to al-
leviate this problem. This was followed
by a whole family of ART improvements
(which we want to discuss brieﬂy, too).

It is the idea of unsupervised learning,
whose aim is the (initially binary) pattern
recognition, or more precisely the catego-
rization of patterns into classes. But addi-

tionally an ART network shall be capable
to ﬁnd new classes.

11.1 Task and structure of an

ART network

An ART network comprises exactly two
layers: the input layer I and the recog-
nition layer O with the input layer be-
ing completely linked towards the recog-
nition layer. This complete link induces
a top-down weight matrix W that con-
tains the weight values of the connections
between each neuron in the input layer
and each neuron in the recognition layer
(ﬁg. 11.1 on the following page).

Simple binary patterns are entered into
the input layer and transferred to the
recognition layer while the recognition
layer shall return a 1-out-of-|O| encoding,
i.e.
it should follow the winner-takes-all

pattern
recognition

165

Chapter 11 Adaptive resonance theory

dkriesel.com

i2

i3

i4

i1

GFED
@ABC
GFED
@ABC
GFED
@ABC
GFED
@ABC
5kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk
)SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
7ooooooooooooooooooooooooooooo
7ooooooooooooooooooooooooooooo
'OOOOOOOOOOOOOOOOOOOOOOOOOOOOO
'OOOOOOOOOOOOOOOOOOOOOOOOOOOOO
iSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
;xxxxxxxxxxxxxxxxxxxxx
;xxxxxxxxxxxxxxxxxxxxx
;xxxxxxxxxxxxxxxxxxxxx
#FFFFFFFFFFFFFFFFFFFFF
#FFFFFFFFFFFFFFFFFFFFF
#FFFFFFFFFFFFFFFFFFFFF
gOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
gOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
ukkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk
cFFFFFFFFFFFFFFFFFFFFF
cFFFFFFFFFFFFFFFFFFFFF
cFFFFFFFFFFFFFFFFFFFFF
E














E














E














E














44444444444444
44444444444444
44444444444444
44444444444444
wooooooooooooooooooooooooooooo
wooooooooooooooooooooooooooooo
Y44444444444444
Y44444444444444
Y44444444444444
Y44444444444444
{xxxxxxxxxxxxxxxxxxxxx
{xxxxxxxxxxxxxxxxxxxxx
{xxxxxxxxxxxxxxxxxxxxx




























































GFED
@ABCΩ1
GFED
GFED
@ABCΩ2
GFED
@ABCΩ3
GFED
@ABCΩ4
GFED
@ABCΩ5
@ABCΩ6

Figure 11.1: Simpliﬁed illustration of the ART network structure. Top: the input layer, bottom:
the recognition layer. In this illustration the lateral inhibition of the recognition layer and the control
neurons are omitted.

scheme. For instance, to realize this 1-
out-of-|O| encoding the principle of lateral
inhibition can be used – or in the imple-
mentation the most activated neuron can
be searched. For practical reasons an IF
query would suit this task best.

put layer causes an activity within the
recognition layer while in turn in the recog-
nition layer every activity causes an activ-
ity within the input layer.

layers
activate
one
another

11.1.1 Resonance takes place by
activities being tossed and
turned

V (cid:73)

But there also exists a bottom-up weight
matrix V , which propagates the activi-
ties within the recognition layer back into
the input layer. Now it is obvious that
these activities are bounced forth and back
again and again, a fact that leads us to
resonance. Every activity within the in-

In addition to the two mentioned layers,
in an ART network also exist a few neu-
rons that exercise control functions such as
signal enhancement. But we do not want
to discuss this theory further since here
only the basic principle of the ART net-
work should become explicit. I have only
mentioned it to explain that in spite of the
recurrences, the ART network will achieve
a stable state after an input.

166

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)













#
'
)
{




#
'
w
{




#
u
w
{




E
;
7
5


O
O
E
;
7


Y
O
O
E
;


c
Y
O
O
E


g
c
Y
O
O


i
g
c
Y


dkriesel.com

11.3 Extensions

11.2 The learning process of

an ART network is
divided to top-down and
bottom-up learning

The trick of adaptive resonance theory is
not only the conﬁguration of the ART net-
work but also the two-piece learning pro-
cedure of the theory: On the one hand
we train the top-down matrix W, on the
other hand we train the bottom-up matrix
V (ﬁg. 11.2 on the next page).

11.2.1 Pattern input and top-down

learning

When a pattern is entered into the net-
work it causes - as already mentioned - an
activation at the output neurons and the
strongest neuron wins. Then the weights
of the matrix W going towards the output
neuron are changed such that the output
of the strongest neuron Ω is still enhanced,
i.e. the class aﬃliation of the input vector
to the class of the output neuron Ω be-
comes enhanced.

11.2.2 Resonance and bottom-up

learning

The training of the backward weights of
the matrix V is a bit tricky: Only the
weights of the respective winner neuron
are trained towards the input layer and
our current input pattern is used as teach-
ing input. Thus, the network is trained to
enhance input vectors.

11.2.3 Adding an output neuron

Of course, it could happen that the neu-
rons are nearly equally activated or that
several neurons are activated, i.e. that the
network is indecisive.
In this case, the
mechanisms of the control neurons acti-
vate a signal that adds a new output neu-
ron. Then the current pattern is assigned
to this output neuron and the weight sets
of the new neuron are trained as usual.
Thus, the advantage of this system is not
only to divide inputs into classes and to
ﬁnd new classes, it can also tell us after
the activation of an output neuron what a
typical representative of a class looks like
- which is a signiﬁcant feature.
Often, however, the system can only mod-
erately distinguish the patterns. The ques-
tion is when a new neuron is permitted to
become active and when it should learn.
In an ART network there are diﬀerent ad-
ditional control neurons which answer this
question according to diﬀerent mathemat-
ical rules and which are responsible for in-
tercepting special cases.
At the same time, one of the largest ob-
jections to an ART is the fact that an
ART network uses a special distinction of
cases, similar to an IF query, that has been
forced into the mechanism of a neural net-
work.

11.3 Extensions

As already mentioned above, the ART net-
works have often been extended.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

167

winner
neuron
is
ampliﬁed

input is
teach. inp.
for backward
weights

Chapter 11 Adaptive resonance theory

dkriesel.com

ART-2 [CG87] is extended to continuous
inputs and additionally oﬀers (in an ex-
tension called ART-2A) enhancements of
the learning speed which results in addi-
tional control neurons and layers.
ART-3 [CG90] 3 improves the learning
ability of ART-2 by adapting additional
biological processes such as the chemical
processes within the synapses1.
Apart from the described ones there exist
many other extensions.

Figure 11.2: Simpliﬁed illustration of the two-
piece training of an ART network: The trained
weights are represented by solid lines. Let us as-
sume that a pattern has been entered into the
network and that the numbers mark the outputs.
Top: We can see that Ω2 is the winner neu-
ron. Middle: So the weights are trained towards
the winner neuron and (below) the weights of
the winner neuron are trained towards the input
layer.

1 Because of the frequent extensions of the adap-
tive resonance theory wagging tongues already call
them "ART-n networks".

168

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Kapitel11AdaptiveResonanceTheorydkriesel.comGFED@ABCi1""GFED@ABCi2GFED@ABCi3GFED@ABCi4||GFED@ABCΩ1YYOOEE<<GFED@ABCΩ2bbYYOOEE01GFED@ABCi1""FFFFFFFFFFFFFFFFFFFFGFED@ABCi244444444444444GFED@ABCi3GFED@ABCi4||GFED@ABCΩ1YYOOEE<<GFED@ABCΩ2bbYYOOEE01GFED@ABCi1""GFED@ABCi2GFED@ABCi3GFED@ABCi4||GFED@ABCΩ1YYOOEE<<GFED@ABCΩ2bbFFFFFFFFFFFFFFFFFFFFYY44444444444444OOEE01Abbildung11.2:VereinfachteDarstellungdeszweigeteiltenTrainingseinesART-Netzes:DiejeweilstrainiertenGewichtesinddurchgezogendargestellt.Nehmenwiran,einMusterwurdeindasNetzeingegebenunddieZahlenmarkierenAusgaben.Oben:Wirwirsehen,istΩ2dasGe-winnerneuron.Mitte:AlsowerdendieGewichtezumGewinnerneuronhintrainiertund(unten)dieGewichtevomGewinnerneuronzurEingangs-schichttrainiert.einerIF-Abfrage,diemanindenMecha-nismuseinesNeuronalenNetzesgepressthat.11.3ErweiterungenWieschoneingangserw¨ahnt,wurdendieART-Netzevielfacherweitert.ART-2[CG87]isteineErweiterungaufkontinuierlicheEingabenundbietetzus¨atzlich(ineinerART-2AgenanntenErweiterung)VerbesserungenderLernge-schwindigkeit,waszus¨atzlicheKontroll-neuroneundSchichtenzurFolgehat.ART-3[CG90]verbessertdieLernf¨ahig-keitvonART-2,indemzus¨atzlichebiolo-gischeVorg¨angewiez.B.diechemischenVorg¨angeinnerhalbderSynapsenadap-tiertwerden1.Zus¨atzlichzudenbeschriebenenErweite-rungenexistierennochvielemehr.1Durchdieh¨auﬁgenErweiterungenderAdaptiveResonanceTheorysprechenb¨oseZungenbereitsvon”ART-n-Netzen“.168D.Kriesel–Einkleiner¨Uberblick¨uberNeuronaleNetze(EPSILON-DE)Part IV

Excursi, appendices and registers

169

Appendix A
Excursus: Cluster analysis and regional and
online learnable ﬁelds

In Grimm’s dictionary the extinct German word "Kluster" is described by "was
dicht und dick zusammensitzet (a thick and dense group of sth.)". In static
cluster analysis, the formation of groups within point clouds is explored.
Introduction of some procedures, comparison of their advantages and
disadvantages. Discussion of an adaptive clustering method based on neural
networks. A regional and online learnable ﬁeld models from a point cloud,
possibly with a lot of points, a comparatively small set of neurons being
representative for the point cloud.

As already mentioned, many problems can
be traced back to problems in cluster
analysis. Therefore, it is necessary to re-
search procedures that examine whether
groups (so-called clusters) exist within
point clouds.
Since cluster analysis procedures need a
notion of distance between two points, a
metric must be deﬁned on the space
where these points are situated.
We brieﬂy want to specify what a metric
is.
Deﬁnition A.1 (Metric). A relation
dist(x1, x2) deﬁned for two objects x1, x2
is referred to as metric if each of the fol-
lowing criteria applies:
1. dist(x1, x2) = 0 if and only if x1 = x2,

2. dist(x1, x2) = dist(x2, x1), i.e.

metry,

sym-

≤
3. dist(x1, x3)
dist(x2, x3),
i.e.
inequality holds.

dist(x1, x2) +
the
triangle

Colloquially speaking, a metric is a tool
for determining distances between points
in any space. Here, the distances have
to be symmetrical, and the distance be-
tween to points may only be 0 if the two
points are equal. Additionally, the trian-
gle inequality must apply.

Metrics are provided by, for example, the
squared distance and the Euclidean
distance, which have already been intro-
duced. Based on such metrics we can de-

171

number of
cluster
must be
known
previously

Appendix A Excursus: Cluster analysis and regional and online learnable ﬁeldsdkriesel.com

ﬁne a clustering procedure that uses a met-
ric as distance measure.

7. Continue with 4 until the assignments

are no longer changed.

Now we want to introduce and brieﬂy dis-
cuss diﬀerent clustering procedures.

A.1 k-means clustering
allocates data to a
predeﬁned number of
clusters

k-means clustering according to J.
MacQueen [Mac67] is an algorithm that
is often used because of its low computa-
tion and storage complexity and which is
regarded as "inexpensive and good". The
operation sequence of the k-means cluster-
ing algorithm is the following:

1. Provide data to be examined.

2. Deﬁne k, which is the number of clus-

ter centers.

3. Select k random vectors for the clus-
ter centers (also referred to as code-
book vectors).

4. Assign each data point to the next

codebook vector1

5. Compute cluster centers for all clus-

ters.

6. Set codebook vectors to new cluster

centers.

1 The name codebook vector was created because
the often used name cluster vector was too un-
clear.

Step 2 already shows one of the great ques-
tions of the k-means algorithm: The num-
ber k of the cluster centers has to be de-
termined in advance. This cannot be done
by the algorithm. The problem is that it
is not necessarily known in advance how k
can be determined best. Another problem
is that the procedure can become quite in-
stable if the codebook vectors are badly
initialized. But since this is random, it
is often useful to restart the procedure.
This has the advantage of not requiring
much computational eﬀort. If you are fully
aware of those weaknesses, you will receive
quite good results.
However, complex structures such as "clus-
ters in clusters" cannot be recognized. If k
is high, the outer ring of the construction
in the following illustration will be recog-
nized as many single clusters. If k is low,
the ring with the small inner clusters will
be recognized as one cluster.
For an illustration see the upper right part
of ﬁg. A.1 on page 174.

A.2 k-nearest neighboring
looks for the k nearest
neighbors of each data
point

The k-nearest neighboring procedure
[CH67] connects each data point to the k
closest neighbors, which often results in a
division of the groups. Then such a group

172

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

clustering
radii around
points

dkriesel.com

A.4 The silhouette coeﬃcient

builds a cluster. The advantage is that
the number of clusters occurs all by it-
self. The disadvantage is that a large stor-
age and computational eﬀort is required to
ﬁnd the next neighbor (the distances be-
tween all data points must be computed
and stored).

clustering
next
points There are some special cases in which the
procedure combines data points belonging
to diﬀerent clusters, if k is too high. (see
the two small clusters in the upper right
of the illustration). Clusters consisting of
only one single data point are basically
conncted to another cluster, which is not
always intentional.

Furthermore, it is not mandatory that the
links between the points are symmetric.

But this procedure allows a recognition of
rings and therefore of "clusters in clusters",
which is a clear advantage. Another ad-
vantage is that the procedure adaptively
responds to the distances in and between
the clusters.

For an illustration see the lower left part
of ﬁg. A.1.

which is the reason for the name epsilon-
nearest neighboring. Points are neig-
bors if they are at most ε apart from each
other. Here, the storage and computa-
tional eﬀort is obviously very high, which
is a disadvantage.
But note that there are some special cases:
Two separate clusters can easily be con-
nected due to the unfavorable situation of
a single data point. This can also happen
with k-nearest neighboring, but it would
be more diﬃcult since in this case the num-
ber of neighbors per point is limited.
An advantage is the symmetric nature of
the neighborhood relationships. Another
advantage is that the combination of min-
imal clusters due to a ﬁxed number of
neighbors is avoided.
On the other hand, it is necessary to skill-
fully initialize ε in order to be successful,
i.e. smaller than half the smallest distance
between two clusters. With variable clus-
ter and point distances within clusters this
can possibly be a problem.
For an illustration see the lower right part
of ﬁg. A.1.

A.3 ε-nearest neighboring

looks for neighbors within
the radius ε for each
data point

A.4 The silhouette coeﬃcient
determines how accurate
a given clustering is

Another approach of neighboring: here,
the neighborhood detection does not use a
ﬁxed number k of neighbors but a radius ε,

As we can see above, there is no easy an-
swer for clustering problems. Each proce-
dure described has very speciﬁc disadvan-
tages. In this respect it is useful to have

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

173

Appendix A Excursus: Cluster analysis and regional and online learnable ﬁeldsdkriesel.com

Figure A.1: Top left: our set of points. We will use this set to explore the diﬀerent clustering
methods. Top right: k-means clustering. Using this procedure we chose k = 6. As we can
see, the procedure is not capable to recognize "clusters in clusters" (bottom left of the illustration).
Long "lines" of points are a problem, too: They would be recognized as many small clusters (if k
is suﬃciently large). Bottom left: k-nearest neighboring. If k is selected too high (higher than
the number of points in the smallest cluster), this will result in cluster combinations shown in the
upper right of the illustration. Bottom right: ε-nearest neighboring. This procedure will cause
diﬃculties if ε is selected larger than the minimum distance between two clusters (see upper left of
the illustration), which will then be combined.

174

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

A.5 Regional and online learnable ﬁelds

a criterion to decide how good our clus-
ter division is. This possibility is oﬀered
by the silhouette coeﬃcient according
to [Kau90]. This coeﬃcient measures how
well the clusters are delimited from each
other and indicates if points may be as-
signed to the wrong clusters.

Apparently, the whole term s(p) can only
be within the interval [−1; 1]. A value
close to -1 indicates a bad classiﬁcation of
p.

The silhouette coeﬃcient S(P) results
from the average of all values s(p):

S(P) = 1
|P|

X

p∈P

s(p).

(A.4)

As above the total quality of the clus-
ter division is expressed by the interval
[−1; 1].

As diﬀerent clustering strategies with dif-
ferent characteristics have been presented
now (lots of further material is presented
in [DHS01]), as well as a measure to in-
dicate the quality of an existing arrange-
ment of given data into clusters, I want
to introduce a clustering method based
on an unsupervised learning neural net-
work [SGE05] which was published in 2005.
Like all the other methods this one may
not be perfect but it eliminates large stan-
dard weaknesses of the known clustering
methods

A.5 Regional and online
learnable ﬁelds are a
neural clustering strategy

The paradigm of neural networks, which I
want to introduce now, are the regional
and online learnable ﬁelds, shortly re-
ferred to as ROLFs.

clustering
quality is

measureable Let P be a point cloud and p a point in
P. Let c ⊆ P be a cluster within the
point cloud and p be part of this cluster,
i.e. p ∈ c. The set of clusters is called C.
Summary:

p ∈ c ⊆ P

applies.
To calculate the silhouette coeﬃcient, we
initially need the average distance between
point p and all its cluster neighbors. This
variable is referred to as a(p) and deﬁned
as follows:

a(p) = 1

|c| − 1

X

q∈c,q6=p

dist(p, q)

(A.1)

Furthermore, let b(p) be the average dis-
tance between our point p and all points
of the next cluster (g represents all clusters
except for c):

b(p) = min
g∈C,g6=c

1
|g|

dist(p, q)

(A.2)

X

q∈g

The point p is classiﬁed well if the distance
to the center of the own cluster is minimal
and the distance to the centers of the other
clusters is maximal. In this case, the fol-
lowing term provides a value close to 1:

s(p) = b(p) − a(p)
max{a(p), b(p)}

(A.3)

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

175

Appendix A Excursus: Cluster analysis and regional and online learnable ﬁeldsdkriesel.com

K(cid:73)

network
covers
point cloud

A.5.1 ROLFs try to cover data with

neurons

Roughly speaking, the regional and online
learnable ﬁelds are a set K of neurons
which try to cover a set of points as well
as possible by means of their distribution
in the input space. For this, neurons are
added, moved or changed in their size dur-
ing training if necessary. The parameters
of the individual neurons will be discussed
later.
Deﬁnition A.2 (Regional and online
learnable ﬁeld).
A regional and on-
line learnable ﬁeld (abbreviated ROLF or
ROLF network) is a set K of neurons that
are trained to cover a certain set in the
input space as well as possible.

Figure A.2: Structure of a ROLF neuron.

A.5.1.1 ROLF neurons feature a

position and a radius in the
input space

c(cid:73)

σ(cid:73)

Here, a ROLF neuron k ∈ K has two
parameters: Similar to the RBF networks,
it has a center ck, i.e. a position in the
input space.

But it has yet another parameter: The ra-
dius σ, which deﬁnes the radius of the per-
ceptive surface surrounding the neuron2.
A neuron covers the part of the input space
that is situated within this radius.

neuron
represents
surface

ck and σk are locally deﬁned for each neu-

2 I write "deﬁnes" and not "is" because the actual
radius is speciﬁed by σ · ρ.

ron. This particularly means that the neu-
rons are capable to cover surfaces of diﬀer-
ent sizes.
The radius of the perceptive surface is
speciﬁed by r = ρ · σ (ﬁg. A.2) with
the multiplier ρ being globally deﬁned and
previously speciﬁed for all neurons. Intu-
itively, the reader will wonder what this
multiplicator is used for. Its signiﬁcance
will be discussed later. Furthermore, the
following has to be observed: It is not nec-
essary for the perceptive surface of the dif-
ferent neurons to be of the same size.
Deﬁnition A.3 (ROLF neuron). The pa-
rameters of a ROLF neuron k are a center
ck and a radius σk.
Deﬁnition A.4 (Perceptive surface).
The perceptive surface of a ROLF neuron

176

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

A.5 Regional and online learnable ﬁelds

k consists of all points within the radius
ρ · σ in the input space.

A.5.2 A ROLF learns unsupervised

by presenting training
samples online

Like many other paradigms of neural net-
works our ROLF network learns by receiv-
ing many training samples p of a training
set P. The learning is unsupervised. For
each training sample p entered into the net-
work two cases can occur:

1. There is one accepting neuron k for p

or

2. there is no accepting neuron at all.

If in the ﬁrst case several neurons are suit-
able, then there will be exactly one ac-
cepting neuron insofar as the closest neu-
ron is the accepting one. For the accepting
neuron k ck and σk are adapted.
Deﬁnition A.5 (Accepting neuron). The
criterion for a ROLF neuron k to be an
accepting neuron of a point p is that the
point p must be located within the percep-
tive surface of k. If p is located in the per-
ceptive surfaces of several neurons, then
the closest neuron will be the accepting
one.
If there are several closest neurons,
one can be chosen randomly.

A.5.2.1 Both positions and radii are
adapted throughout learning

Adapting
existing
neurons Let us assume that we entered a training
sample p into the network and that there

is an accepting neuron k. Then the radius
moves towards ||p − ck|| (i.e. towards the
distance between p and ck) and the center
ck towards p. Additionally, let us deﬁne
the two learning rates ησ and ηc for radii (cid:74)ησ, ηc
and centers.
ck(t + 1) = ck(t) + ηc(p − ck(t))
σk(t + 1) = σk(t) + ησ(||p − ck(t)|| − σk(t))

Note that here σk is a scalar while ck is a
vector in the input space.
Deﬁnition A.6 (Adapting a ROLF neu-
ron). A neuron k accepted by a point p is
adapted according to the following rules:
ck(t + 1) = ck(t) + ηc(p − ck(t))
(A.5)
σk(t + 1) = σk(t) + ησ(||p − ck(t)|| − σk(t))
(A.6)

A.5.2.2 The radius multiplier allows

neurons to be able not only to
shrink

Now we can understand the function of the
multiplier ρ: Due to this multiplier the per- (cid:74)ρ
ceptive surface of a neuron includes more
than only all points surrounding the neu-
ron in the radius σ. This means that due
to the aforementioned learning rule σ can-
not only decrease but also increase.
Deﬁnition A.7 (Radius multiplier). The
radius multiplier ρ > 1 is globally deﬁned
and expands the perceptive surface of a
neuron k to a multiple of σk. So it is en-
sured that the radius σk cannot only de-
crease but also increase.

so the
neurons
can grow

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

177

Appendix A Excursus: Cluster analysis and regional and online learnable ﬁeldsdkriesel.com

Generally, the radius multiplier is set to
values in the lower one-digit range, such
as 2 or 3.
So far we only have discussed the case in
the ROLF training that there is an accept-
ing neuron for the training sample p.

A.5.2.3 As required, new neurons are

generated

This suggests to discuss the approach for
the case that there is no accepting neu-
ron.
In this case a new accepting neuron k is
generated for our training sample. The re-
sult is of course that ck and σk have to be
initialized.
The initialization of ck can be understood
intuitively: The center of the new neuron
is simply set on the training sample, i.e.

ck = p.

We generate a new neuron because there
is no neuron close to p – for logical reasons,
we place the neuron exactly on p.
But how to set a σ when a new neuron
is generated? For this purpose there exist
diﬀerent options:
Init-σ: We always

select a predeﬁned

static σ.

Minimum σ: We take a look at the σ of
each neuron and select the minimum.
Maximum σ: We take a look at the σ of
each neuron and select the maximum.

Mean σ: We select the mean σ of all neu-

rons.

Currently, the mean-σ variant is the fa-
vorite one although the learning procedure
also works with the other ones.
In the
minimum-σ variant the neurons tend to
cover less of the surface, in the maximum-
σ variant they tend to cover more of the
surface.
Deﬁnition A.8 (Generating a ROLF neu-
ron).
If a new ROLF neuron k is gener-
ated by entering a training sample p, then
ck is intialized with p and σk according to
one of the aforementioned strategies (init-
σ, minimum-σ, maximum-σ, mean-σ).

The training is complete when after re-
peated randomly permuted pattern presen-
tation no new neuron has been generated
in an epoch and the positions of the neu-
rons barely change.

A.5.3 Evaluating a ROLF

The result of the training algorithm is that
the training set is gradually covered well
and precisely by the ROLF neurons and
that a high concentration of points on a
spot of the input space does not automati-
cally generate more neurons. Thus, a pos-
sibly very large point cloud is reduced to
very few representatives (based on the in-
put set).
Then it is very easy to deﬁne the num-
ber of clusters: Two neurons are (accord-
ing to the deﬁnition of the ROLF) con-
nected when their perceptive surfaces over-

initialization
of a
neurons

cluster =
connected
neurons

178

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

A.5 Regional and online learnable ﬁelds

some kind of nearest neighbor-
lap (i.e.
ing is executed with the variable percep-
tive surfaces). A cluster is a group of
connected neurons or a group of points of
the input space covered by these neurons
(ﬁg. A.3).
Of course, the complete ROLF network
can be evaluated by means of other clus-
tering methods, i.e.
the neurons can be
searched for clusters. Particularly with
clustering methods whose storage eﬀort
grows quadratic to |P| the storage eﬀort
can be reduced dramatically since gener-
ally there are considerably less ROLF neu-
rons than original data points, but the
neurons represent the data points quite
well.

A.5.4 Comparison with popular

clustering methods

It is obvious, that storing the neurons
rather than storing the input points takes
the biggest part of the storage eﬀort of the
ROLFs. This is a great advantage for huge
point clouds with a lot of points.
Since it is unnecessary to store the en-
tire point cloud, our ROLF, as a neural
clustering method, has the capability to
learn online, which is deﬁnitely a great ad-
vantage. Furthermore, it can (similar to
ε nearest neighboring or k nearest neigh-
boring) distinguish clusters from enclosed
clusters – but due to the online presenta-
tion of the data without a quadratically
growing storage eﬀort, which is by far the
greatest disadvantage of the two neighbor-
ing methods.

less
storage
eﬀort!

recognize
"cluster in
clusters"

Figure A.3: The clustering process. Top: the
input set, middle: the input space covered by
ROLF neurons, bottom: the input space only
covered by the neurons (representatives).

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

179

Appendix A Excursus: Cluster analysis and regional and online learnable ﬁeldsdkriesel.com

Additionally, the issue of the size of the in-
dividual clusters proportional to their dis-
tance from each other is addressed by us-
ing variable perceptive surfaces - which is
also not always the case for the two men-
tioned methods.
The ROLF compares favorably with k-
means clustering, as well: Firstly, it is un-
necessary to previously know the number
of clusters and, secondly, k-means cluster-
ing recognizes clusters enclosed by other
clusters as separate clusters.

A.5.5 Initializing radii, learning
rates and multiplier is not
trivial

Certainly, the disadvantages of the ROLF
shall not be concealed:
It is not always
easy to select the appropriate initial value
for σ and ρ. The previous knowledge
about the data set can so to say be in-
cluded in ρ and the initial value of σ of the
ROLF: Fine-grained data clusters should
use a small ρ and a small σ initial value.
But the smaller the ρ the smaller, the
chance that the neurons will grow if neces-
sary. Here again, there is no easy answer,
just like for the learning rates ηc and ησ.
For ρ the multipliers in the lower single-
digit range such as 2 or 3 are very popu-
lar. ηc and ησ successfully work with val-
ues about 0.005 to 0.1, variations during
run-time are also imaginable for this type
of network. Initial values for σ generally
depend on the cluster and data distribu-
tion (i.e.
they often have to be tested).
But compared to wrong initializations –

at least with the mean-σ strategy – they
are relatively robust after some training
time.
As a whole, the ROLF is on a par with
the other clustering methods and is par-
ticularly very interesting for systems with
low storage capacity or huge data sets.

A.5.6 Application examples

A ﬁrst application example could be ﬁnd-
ing color clusters in RGB images. Another
ﬁeld of application directly described in
the ROLF publication is the recognition of
words transferred into a 720-dimensional
feature space. Thus, we can see that
ROLFs are relatively robust against higher
dimensions. Further applications can be
found in the ﬁeld of analysis of attacks on
network systems and their classiﬁcation.

Exercises

Exercise 18. Determine at least four
adaptation steps for one single ROLF neu-
ron k if the four patterns stated below
are presented one after another in the in-
dicated order. Let the initial values for
the ROLF neuron be ck = (0.1, 0.1) and
σk = 1. Furthermore, let ηc = 0.5 and
ησ = 0. Let ρ = 3.

P = {(0.1, 0.1);
= (0.9, 0.1);
= (0.1, 0.9);
= (0.9, 0.9)}.

180

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

Appendix B
Excursus: neural networks used for
prediction

Discussion of an application of neural networks: a look ahead into the future
of time series.

After discussing the diﬀerent paradigms of
neural networks it is now useful to take a
look at an application of neural networks
which is brought up often and (as we will
see) is also used for fraud: The applica-
tion of time series prediction. This ex-
cursus is structured into the description of
time series and estimations about the re-
quirements that are actually needed to pre-
dict the values of a time series. Finally, I
will say something about the range of soft-
ware which should predict share prices or
other economic characteristics by means of
neural networks or other procedures.

B.1 About time series

A time series is a series of values dis-
cretized in time. For example, daily mea-
sured temperature values or other meteo-
rological data of a speciﬁc site could be
represented by a time series. Share price
values also represent a time series. Often
the measurement of time series is timely
equidistant, and in many time series the
future development of their values is very
interesting, e.g.
the daily weather fore-
cast.

This chapter should not be a detailed
description but rather indicate some ap-
proaches for time series prediction. In this
respect I will again try to avoid formal def-
initions.

time
series of
values

Time series can also be values of an actu-
ally continuous function read in a certain
distance of time ∆t (ﬁg. B.1 on the next (cid:74)∆t
page).
If we want to predict a time series, we will
look for a neural network that maps the
previous series values to future develop-
ments of the time series, i.e.
if we know
longer sections of the time series, we will

181

Appendix B Excursus: neural networks used for prediction

dkriesel.com

have enough training samples. Of course,
these are not examples for the future to be
predicted but it is tried to generalize and
to extrapolate the past by means of the
said samples.
But before we begin to predict a time
series we have to answer some questions
about this time series we are dealing with
and ensure that it fulﬁlls some require-
ments.

1. Do we have any evidence which sug-
gests that future values depend in any
way on the past values of the time se-
ries? Does the past of a time series
include information about its future?
2. Do we have enough past values of the
time series that can be used as train-
ing patterns?

3. In case of a prediction of a continuous
function: What must a useful ∆t look
like?

Now these questions shall be explored in
detail.
How much information about the future
is included in the past values of a time se-
ries? This is the most important question
to be answered for any time series that
should be mapped into the future. If the
future values of a time series, for instance,
do not depend on the past values, then a
time series prediction based on them will
be impossible.
In this chapter, we assume systems whose
future values can be deduced from their
states – the deterministic systems. This

Figure B.1: A function x that depends on the
time is sampled at discrete time steps (time dis-
cretized), this means that the result is a time
series. The sampled values are entered into a
neural network (in this example an SLP) which
shall learn to predict the future values of the time
series.

182

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

B.2 One-step-ahead prediction

leads us to the question of what a system
state is.

B.2 One-step-ahead

prediction

A system state completely describes a sys-
tem for a certain point of time. The future
of a deterministic system would be clearly
deﬁned by means of the complete descrip-
tion of its current state.

The problem in the real world is that such
a state concept includes all things that in-
ﬂuence our system by any means.

In case of our weather forecast for a spe-
ciﬁc site we could deﬁnitely determine
the temperature, the atmospheric pres-
sure and the cloud density as the mete-
orological state of the place at a time t.
But the whole state would include signiﬁ-
cantly more information. Here, the world-
wide phenomena that control the weather
would be interesting as well as small local
pheonomena such as the cooling system of
the local power plant.

So we shall note that the system state is de-
sirable for prediction but not always possi-
ble to obtain. Often only fragments of the
current states can be acquired, e.g. for a
weather forecast these fragments are the
said weather data.

However, we can partially overcome these
weaknesses by using not only one single
state (the last one) for the prediction, but
by using several past states. From this
we want to derive our ﬁrst prediction sys-
tem:

predict
the next
value

The ﬁrst attempt to predict the next fu-
ture value of a time series out of past val-
ues is called one-step-ahead prediction
(ﬁg. B.2 on the following page).

Such a predictor system receives the last
n observed state parts of the system as
input and outputs the prediction for the
next state (or state part). The idea of
a state space with predictable states is
called state space forecasting.

The aim of the predictor is to realize a
function

f(xt−n+1, . . . , xt−1, xt) = ˜xt+1,

(B.1)

which receives exactly n past values in or-
der to predict the future value. Predicted
values shall be headed by a tilde (e.g. ˜x) (cid:74)˜x
to distinguish them from the actual future
values.

The most intuitive and simplest approach
would be to ﬁnd a linear combination

˜xi+1 = a0xi + a1xi−1 + . . . + ajxi−j

(B.2)

that approximately fulﬁlls our
tions.

condi-

Such a construction is called digital ﬁl-
ter. Here we use the fact that time series

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

183

Appendix B Excursus: neural networks used for prediction

dkriesel.com

xt−3

xt−2

xt−1

xt

˜xt+1

predictor

Figure B.2: Representation of the one-step-ahead prediction.
It is tried to calculate the future
value from a series of past values. The predicting element (in this case a neural network) is referred
to as predictor.

usually have a lot of past values so that we
can set up a series of equations1:

means of the delta rule provides results
very close to the analytical solution.

xt = a0xt−1 + . . . + ajxt−1−(n−1)
xt−1 = a0xt−2 + . . . + ajxt−2−(n−1)

...

(B.3)

xt−n = a0xt−n + . . . + ajxt−n−(n−1)

Thus, n equations could be found for n un-
known coeﬃcients and solve them (if pos-
sible). Or another, better approach: we
could use m > n equations for n unknowns
in such a way that the sum of the mean
squared errors of the already known pre-
diction is minimized. This is called mov-
ing average procedure.
But this linear structure corresponds to a
singlelayer perceptron with a linear activa-
tion function which has been trained by
means of data from the past (The experi-
mental setup would comply with ﬁg. B.1
on page 182).
In fact, the training by
1 Without going into detail, I want to remark that
the prediction becomes easier the more past values
of the time series are available.
I would like to
ask the reader to read up on the Nyquist-Shannon
sampling theorem

Even if this approach often provides satis-
fying results, we have seen that many prob-
lems cannot be solved by using a single-
layer perceptron. Additional layers with
linear activation function are useless, as
well, since a multilayer perceptron with
only linear activation functions can be re-
duced to a singlelayer perceptron. Such
considerations lead to a non-linear ap-
proach.

The multilayer perceptron and non-linear
activation functions provide a universal
non-linear function approximator, i.e. we
can use an n-|H|-1-MLP for n n inputs out
of the past. An RBF network could also be
used. But remember that here the number
n has to remain low since in RBF networks
high input dimensions are very complex to
realize. So if we want to include many past
values, a multilayer perceptron will require
considerably less computational eﬀort.

184

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

.
.
.
.
-
-
+
+
K
K
dkriesel.com

B.4 Additional optimization approaches for prediction

B.3 Two-step-ahead

prediction

B.4 Additional optimization

approaches for prediction

What approaches can we use to to see far-
ther into the future?

B.3.1 Recursive two-step-ahead

prediction

In order to extend the prediction to, for in-
stance, two time steps into the future, we
could perform two one-step-ahead predic-
tions in a row (ﬁg. B.3 on the following
i.e. a recursive two-step-ahead
page),
prediction. Unfortunately, the value de-
termined by means of a one-step-ahead
prediction is generally imprecise so that
errors can be built up, and the more pre-
dictions are performed in a row the more
imprecise becomes the result.

B.3.2 Direct two-step-ahead

prediction

We have already guessed that there exists
a better approach: Just like the system
can be trained to predict the next value,
we can certainly train it to predict the
next but one value. This means we di-
rectly train, for example, a neural network
to look two time steps ahead into the fu-
ture, which is referred to as direct two-
step-ahead prediction (ﬁg. B.4 on the
next page). Obviously, the direct two-step-
ahead prediction is technically identical to
the one-step-ahead prediction. The only
diﬀerence is the training.

The possibility to predict values far away
in the future is not only important because
we try to look farther ahead into the fu-
ture. There can also be periodic time se-
ries where other approaches are hardly pos-
sible: If a lecture begins at 9 a.m. every
Thursday, it is not very useful to know how
many people sat in the lecture room on
Monday to predict the number of lecture
participants. The same applies, for ex-
ample, to periodically occurring commuter
jams.

B.4.1 Changing temporal

parameters

Thus, it can be useful to intentionally leave
gaps in the future values as well as in the
past values of the time series, i.e. to in-
troduce the parameter ∆t which indicates
which past value is used for prediction.
Technically speaking, we still use a one-
step-ahead prediction only that we extend
the input space or train the system to pre-
dict values lying farther away.
It is also possible to combine diﬀerent ∆t:
In case of the traﬃc jam prediction for a
Monday the values of the last few days
could be used as data input in addition to
the values of the previous Mondays. Thus,
we use the last values of several periods,
in this case the values of a weekly and a
daily period. We could also include an an-
nual period in the form of the beginning of
the holidays (for sure, everyone of us has

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

185

predict
future
values

direct
prediction
is better

extent
input
period

Appendix B Excursus: neural networks used for prediction

dkriesel.com

predictor

xt−3

xt−2

xt−1

xt

˜xt+1

˜xt+2

predictor

Figure B.3: Representation of the two-step-ahead prediction. Attempt to predict the second future
value out of a past value series by means of a second predictor and the involvement of an already
predicted value.

xt−3

xt−2

xt−1

xt

˜xt+1

˜xt+2

predictor

Figure B.4: Representation of the direct two-step-ahead prediction. Here, the second time step is
predicted directly, the ﬁrst one is omitted. Technically, it does not diﬀer from a one-step-ahead
prediction.

186

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)



.
.
0
0
.
.
0
0
-
-
+
+
0
0
O
O
J
J
.
.
.
.
-
-
+
+
E
E
use
information
outside of
time series

dkriesel.com

B.5 Remarks on the prediction of share prices

already spent a lot of time on the highway
because he forgot the beginning of the hol-
idays).

B.4.2 Heterogeneous prediction

Another prediction approach would be to
predict the future values of a single time
series out of several time series,
if it is
assumed that the additional time series
is related to the future of the ﬁrst one
(heterogeneous one-step-ahead pre-
diction, ﬁg. B.5 on the following page).
If we want to predict two outputs of two
related time series, it is certainly possible
to perform two parallel one-step-ahead pre-
dictions (analytically this is done very of-
ten because otherwise the equations would
become very confusing); or in case of
the neural networks an additional output
neuron is attached and the knowledge of
both time series is used for both outputs
(ﬁg. B.6 on the next page).
You’ll ﬁnd more and more general material
on time series in [WG94].

B.5 Remarks on the

prediction of share prices

Many people observe the changes of a
share price in the past and try to con-
clude the future from those values in or-
der to beneﬁt from this knowledge. Share
prices are discontinuous and therefore they
are principally diﬃcult functions. Further-
more, the functions can only be used for

discrete values – often, for example, in a
daily rhythm (including the maximum and
minimum values per day, if we are lucky)
with the daily variations certainly being
eliminated. But this makes the whole
thing even more diﬃcult.
There are chartists, i.e. people who look
at many diagrams and decide by means
of a lot of background knowledge and
decade-long experience whether the equi-
ties should be bought or not (and often
they are very successful).

Apart from the share prices it is very in-
teresting to predict the exchange rates of
currencies: If we exchange 100 Euros into
Dollars, the Dollars into Pounds and the
Pounds back into Euros it could be pos-
sible that we will ﬁnally receive 110 Eu-
ros. But once found out, we would do this
more often and thus we would change the
exchange rates into a state in which such
an increasing circulation would no longer
be possible (otherwise we could produce
money by generating, so to speak, a ﬁnan-
cial perpetual motion machine.

At the stock exchange, successful stock
and currency brokers raise or lower their
thumbs – and thereby indicate whether in
their opinion a share price or an exchange
rate will increase or decrease. Mathemat-
ically speaking, they indicate the ﬁrst bit
(sign) of the ﬁrst derivative of the ex-
change rate. In that way excellent world-
class brokers obtain success rates of about
70%.

In Great Britain, the heterogeneous one-
step-ahead prediction was
successfully

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

187

Appendix B Excursus: neural networks used for prediction

dkriesel.com

xt−3

xt−2

xt−1

xt

˜xt+1

predictor

yt−3

yt−2

yt−1

yt

Figure B.5: Representation of the heterogeneous one-step-ahead prediction. Prediction of a time
series under consideration of a second one.

xt−3

xt−2

xt−1

xt

yt−3

yt−2

yt−1

yt

predictor

˜xt+1

˜yt+1

Figure B.6: Heterogeneous one-step-ahead prediction of two time series at the same time.

188

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

.
.
.
.
-
-
+
+
K
K
0
0
0
0
1
1
3
3
.
.
.
.
-
-
+
+
K
K


0
0
0
0
1
1
3
3
dkriesel.com

B.5 Remarks on the prediction of share prices

Again and again some software appears
which uses scientiﬁc key words such as
”neural networks” to purport that it is ca-
pable to predict where share prices are go-
ing. Do not buy such software!
In addi-
tion to the aforementioned scientiﬁc exclu-
sions there is one simple reason for this:
If these tools work – why should the man-
ufacturer sell them? Normally, useful eco-
nomic knowledge is kept secret. If we knew
a way to deﬁnitely gain wealth by means
of shares, we would earn our millions by
using this knowledge instead of selling it
for 30 euros, wouldn’t we?

used to increase the accuracy of such pre-
dictions to 76%: In addition to the time
series of the values indicators such as the
oil price in Rotterdam or the US national
debt were included.

This is just an example to show the mag-
nitude of the accuracy of stock-exchange
evaluations, since we are still talking only
about the ﬁrst bit of the ﬁrst derivation!
We still do not know how strong the ex-
pected increase or decrease will be and
also whether the eﬀort will pay oﬀ: Prob-
ably, one wrong prediction could nullify
the proﬁt of one hundred correct predic-
tions.

How can neural networks be used to pre-
dict share prices? Intuitively, we assume
that future share prices are a function of
the previous share values.

But this assumption is wrong:
Share
prices are no function of their past val-
ues, but a function of their assumed fu-
ture value. We do not buy shares be-
cause their values have been increased
during the last days, but because we be-
lieve that they will futher increase tomor-
row.
If, as a consequence, many people
buy a share, they will boost the price.
Therefore their assumption was right – a
self-fulﬁlling prophecy has been gener-
ated, a phenomenon long known in eco-
nomics.

The same applies the other way around:
We sell shares because we believe that to-
morrow the prices will decrease. This will
beat down the prices the next day and gen-
erally even more the day after the next.

share price
function of
assumed
future
value!

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

189

Appendix C
Excursus: reinforcement learning

What if there were no training samples but it would nevertheless be possible
to evaluate how well we have learned to solve a problem? Let us examine a
learning paradigm that is situated between supervised and unsupervised
learning.

I now want to introduce a more exotic ap-
proach of learning – just to leave the usual
paths. We know learning procedures in
which the network is exactly told what to
do, i.e. we provide exemplary output val-
ues. We also know learning procedures
like those of the self-organizing maps, into
which only input values are entered.
Now we want to explore something in-
between: The learning paradigm of rein-
forcement learning – reinforcement learn-
ing according to Sutton and Barto
[SB98].
Reinforcement learning in itself is no neu-
ral network but only one of the three learn-
ing paradigms already mentioned in chap-
ter 4. In some sources it is counted among
the supervised learning procedures since a
feedback is given. Due to its very rudimen-
tary feedback it is reasonable to separate
it from the supervised learning procedures
– apart from the fact that there are no
training samples at all.

no
samples
but
feedback

from cognitive

science

i.e.

term reinforcement

While it is generally known that pro-
cedures such as backpropagation cannot
work in the human brain itself, reinforce-
ment learning is usually considered as be-
ing biologically more motivated.
learning
The
and
comes
psychology and it describes the learning
system of carrot and stick, which occurs
everywhere in nature,
learning by
means of good or bad experience, reward
and punishment. But there is no learning
aid that exactly explains what we have
to do: We only receive a total result
for a process (Did we win the game of
chess or not? And how sure was this
victory?), but no results for the individual
intermediate steps.
For example, if we ride our bike with worn
tires and at a speed of exactly 21, 5 km
h
through a turn over some sand with a
grain size of 0.1mm, on the average, then
nobody could tell us exactly which han-

191

Appendix C Excursus: reinforcement learning

dkriesel.com

dlebar angle we have to adjust or, even
worse, how strong the great number of
muscle parts in our arms or legs have to
contract for this. Depending on whether
we reach the end of the curve unharmed or
not, we soon have to face the learning expe-
rience, a feedback or a reward, be it good
or bad. Thus, the reward is very simple
- but on the other hand it is considerably
easier to obtain. If we now have tested dif-
ferent velocities and turning angles often
enough and received some rewards, we will
get a feel for what works and what does
not. The aim of reinforcement learning is
to maintain exactly this feeling.
Another
the
quasi-
impossibility to achieve a sort of cost or
utility function is a tennis player who
tries to maximize his athletic success
on the long term by means of complex
movements and ballistic trajectories in
the three-dimensional space including the
wind direction,
the importance of the
tournament, private factors and many
more.
To get straight to the point: Since we
receive only little feedback, reinforcement
learning often means trial and error – and
therefore it is very slow.

example

for

C.1 System structure

Now we want to brieﬂy discuss diﬀerent
sizes and components of the system. We
will deﬁne them more precisely in the fol-
lowing sections. Broadly speaking, rein-
forcement learning represents the mutual

interaction between an agent and an envi-
ronmental system (ﬁg. C.2).
The agent shall solve some problem. He
could,
for instance, be an autonomous
robot that shall avoid obstacles. The
agent performs some actions within the
environment and in return receives a feed-
back from the environment, which in the
following is called reward. This cycle of ac-
tion and reward is characteristic for rein-
forcement learning. The agent inﬂuences
the system, the system provides a reward
and then changes.
The reward is a real or discrete scalar
which describes, as mentioned above, how
well we achieve our aim, but it does not
give any guidance how we can achieve it.
The aim is always to make the sum of
rewards as high as possible on the long
term.

C.1.1 The gridworld

As a learning example for reinforcement
learning I would like to use the so-called
gridworld. We will see that its struc-
ture is very simple and easy to ﬁgure out
and therefore reinforcement is actually not
necessary. However,
it is very suitable
for representing the approach of reinforce-
ment learning. Now let us exemplary de-
ﬁne the individual components of the re-
inforcement system by means of the grid-
world. Later, each of these components
will be examined more exactly.

Environment: The gridworld (ﬁg. C.1 on
the facing page) is a simple, discrete

simple
examplary
world

192

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.1 System structure

world in two dimensions which in the
following we want to use as environ-
mental system.

Agent: As an Agent we use a simple robot

being situated in our gridworld.

State space: As we can see, our gridworld
has 5 × 7 ﬁelds with 6 ﬁelds being un-
accessible. Therefore, our agent can
occupy 29 positions in the grid world.
These positions are regarded as states
for the agent.

Action space: The actions are still miss-
ing. We simply deﬁne that the robot
could move one ﬁeld up or down, to
the right or to the left (as long as
there is no obstacle or the edge of our
gridworld).

Task: Our agent’s task is to leave the grid-
world. The exit is located on the right
of the light-colored ﬁeld.

Non-determinism: The two obstacles can
be connected by a "door". When the
door is closed (lower part of the illus-
tration), the corresponding ﬁeld is in-
accessible. The position of the door
cannot change during a cycle but only
between the cycles.

We now have created a small world that
will accompany us through the following
learning strategies and illustrate them.

C.1.2 Agent und environment

Our aim is that the agent learns what hap-
pens by means of the reward. Thus, it

×

×

Figure C.1: A graphical representation of our
gridworld. Dark-colored cells are obstacles and
therefore inaccessible. The exit is located on the
right side of the light-colored ﬁeld. The symbol
× marks the starting position of our agent.
In
the upper part of our ﬁgure the door is open, in
the lower part it is closed.

Agent

reward / new situation

action

environment

Figure C.2: The agent performs some actions
within the environment and in return receives a
reward.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

193

_
_
?
?
agent
acts in
environment

Appendix C Excursus: reinforcement learning

dkriesel.com

is trained over, of and by means of a dy-
namic system, the environment, in order
to reach an aim. But what does learning
mean in this context?
The agent shall learn a mapping of sit-
uations to actions (called policy), i.e.
it
shall learn what to do in which situation
to achieve a certain (given) aim. The aim
is simply shown to the agent by giving an
award for the achievement.
Such an award must not be mistaken for
the reward – on the agent’s way to the
solution it may sometimes be useful to
receive a smaller award or a punishment
when in return the longterm result is max-
imum (similar to the situation when an
investor just sits out the downturn of the
share price or to a pawn sacriﬁce in a chess
game). So, if the agent is heading into
the right direction towards the target, it
receives a positive reward, and if not it re-
ceives no reward at all or even a negative
reward (punishment). The award is, so to
speak, the ﬁnal sum of all rewards – which
is also called return.
After having colloquially named all the ba-
sic components, we want to discuss more
precisely which components can be used to
make up our abstract reinforcement learn-
ing system.
In the gridworld:
In the gridworld, the
agent is a simple robot that should ﬁnd the
exit of the gridworld. The environment
is the gridworld itself, which is a discrete
gridworld.
Deﬁnition C.1 (Agent).
In reinforce-
ment learning the agent can be formally

described as a mapping of the situation
space S into the action space A(st). The
meaning of situations st will be deﬁned
later and should only indicate that the ac-
tion space depends on the current situa-
tion.

Agent: S → A(st)

(C.1)

Deﬁnition C.2 (Environment). The en-
vironment represents a stochastic map-
ping of an action A in the current situa-
tion st to a reward rt and a new situation
st+1.
Environment: S × A → P(S × rt)

(C.2)

C.1.3 States, situations and actions

As already mentioned, an agent can be in
diﬀerent states: In case of the gridworld,
for example, it can be in diﬀerent positions
(here we get a two-dimensional state vec-
tor).
For an agent is ist not always possible to
realize all information about its current
state so that we have to introduce the term
situation. A situation is a state from the
agent’s point of view, i.e. only a more or
less precise approximation of a state.
Therefore, situations generally do not al-
low to clearly "predict" successor situa-
tions – even with a completely determin-
istic system this may not be applicable.
If we knew all states and the transitions
between them exactly (thus, the complete
system), it would be possible to plan op-
timally and also easy to ﬁnd an optimal

194

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.1 System structure

policy (methods are provided, for example,
by dynamic programming).
Now we know that reinforcement learning
is an interaction between the agent and
the system including actions at and sit-
uations st. The agent cannot determine
by itself whether the current situation is
good or bad: This is exactly the reason
why it receives the said reward from the
environment.
In the gridworld: States are positions
where the agent can be situated. Sim-
ply said, the situations equal the states
in the gridworld. Possible actions would
be to move towards north, south, east or
west.
Situation and action can be vectorial, the
reward is always a scalar (in an extreme
case even only a binary value) since the
aim of reinforcement learning is to get
along with little feedback. A complex vec-
torial reward would equal a real teaching
input.
By the way, the cost function should be
minimized, which would not be possible,
however, with a vectorial reward since we
do not have any intuitive order relations
in multi-dimensional space, i.e. we do not
directly know what is better or worse.
Deﬁnition C.3 (State). Within its en-
vironment the agent is in a state. States
contain any information about the agent
within the environmental system. Thus,
it is theoretically possible to clearly pre-
dict a successor state to a performed ac-
tion within a deterministic system out of
this godlike state knowledge.

Deﬁnition C.4 (Situation). Situations
st (here at time t) of a situation space (cid:74)st
S are the agent’s limited, approximate (cid:74)S
knowledge about its state. This approx-
imation (about which the agent cannot
even know how good it is) makes clear pre-
dictions impossible.
Deﬁnition C.5 (Action). Actions at can (cid:74)at
be performed by the agent (whereupon it
could be possible that depending on the
situation another action space A(S) ex- (cid:74)A(S)
ists). They cause state transitions and
therefore a new situation from the agent’s
point of view.

C.1.4 Reward and return

As in real
life it is our aim to receive
an award that is as high as possible, i.e.
to maximize the sum of the expected re-
wards r, called return R, on the long
term. For ﬁnitely many time steps1 the
rewards can simply be added:

Rt = rt+1 + rt+2 + . . .

∞X

x=1

=

rt+x

(C.3)

(C.4)

Certainly, the return is only estimated
here (if we knew all rewards and therefore
the return completely, it would no longer
be necessary to learn).
Deﬁnition C.6 (Reward). A reward rt is (cid:74)rt
a scalar, real or discrete (even sometimes
only binary) reward or punishment which
1 In practice, only ﬁnitely many time steps will be
possible, even though the formulas are stated with
an inﬁnite sum in the ﬁrst place

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

195

Appendix C Excursus: reinforcement learning

dkriesel.com

the environmental system returns to the
agent as reaction to an action.
Deﬁnition C.7 (Return). The return Rt
is the accumulation of all received rewards
until time t.

Rt(cid:73)

C.1.4.1 Dealing with long periods of

time

However, not every problem has an ex-
plicit target and therefore a ﬁnite sum (e.g.
our agent can be a robot having the task
to drive around again and again and to
avoid obstacles). In order not to receive a
diverging sum in case of an inﬁnite series
of reward estimations a weakening factor
0 < γ < 1 is used, which weakens the in-
ﬂuence of future rewards. This is not only
useful if there exists no target but also if
the target is very far away:

γ(cid:73)

Rt = rt+1 + γ1rt+2 + γ2rt+3 + . . .

∞X

x=1

=

γx−1rt+x

(C.5)

(C.6)

The farther the reward is away, the smaller
is the inﬂuence it has in the agent’s deci-
sions.

Another possibility to handle the return
sum would be a limited time horizon
τ so that only τ many following rewards
rt+1, . . . , rt+τ are regarded:

τ(cid:73)

Rt = rt+1 + . . . + γτ−1rt+τ

τX

x=1

=

γx−1rt+x

(C.7)

(C.8)

the

Thus, we divide
into
episodes. Usually, one of the two meth-
ods is used to limit the sum, if not both
methods together.

timeline

As in daily living we try to approximate
our current situation to a desired state.
Since it is not mandatory that only the
next expected reward but the expected to-
tal sum decides what the agent will do, it
is also possible to perform actions that, on
short notice, result in a negative reward
(e.g. the pawn sacriﬁce in a chess game)
but will pay oﬀ later.

C.1.5 The policy

After having considered and formalized
some system components of reinforcement
learning the actual aim is still to be dis-
cussed:

During reinforcement learning the agent
learns a policy

Π : S → P(A),

Thus, it continuously adjusts a mapping
of the situations to the probabilities P(A),
with which any action A is performed in
any situation S. A policy can be deﬁned
as a strategy to select actions that would
maximize the reward in the long term.
In the gridworld: In the gridworld the pol-
icy is the strategy according to which the
agent tries to exit the gridworld.
Deﬁnition C.8 (Policy). The policy Π
s a mapping of situations to probabilities

(cid:74)Π

196

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.1 System structure

to perform every action out of the action
space. So it can be formalized as

Π : S → P(A).

(C.9)

Basically, we distinguish between two pol-
icy paradigms: An open loop policy rep-
resents an open control chain and creates
out of an initial situation s0 a sequence of
actions a0, a1, . . . with ai 6= ai(si); i > 0.
Thus, in the beginning the agent develops
a plan and consecutively executes it to the
end without considering the intermediate
situations (therefore ai 6= ai(si), actions af-
ter a0 do not depend on the situations).
In the gridworld:
In the gridworld, an
open-loop policy would provide a precise
direction towards the exit, such as the way
from the given starting position to (in ab-
breviations of the directions) EEEEN.
So an open-loop policy is a sequence of
actions without interim feedback. A se-
quence of actions is generated out of a
starting situation. If the system is known
well and truly, such an open-loop policy
can be used successfully and lead to use-
ful results. But, for example, to know the
chess game well and truly it would be nec-
essary to try every possible move, which
would be very time-consuming. Thus, for
such problems we have to ﬁnd an alterna-
tive to the open-loop policy, which incorpo-
rates the current situations into the action
plan:
A closed loop policy is a closed loop, a
function

Π : si → ai with ai = ai(si),

in a manner of speaking. Here, the envi-
ronment inﬂuences our action or the agent
responds to the input of the environment,
respectively, as already illustrated in ﬁg.
C.2. A closed-loop policy, so to speak, is
a reactive plan to map current situations
to actions to be performed.
In the gridworld: A closed-loop policy
would be responsive to the current posi-
tion and choose the direction according to
the action. In particular, when an obsta-
cle appears dynamically, such a policy is
the better choice.
When selecting the actions to be per-
formed, again two basic strategies can be
examined.

C.1.5.1 Exploitation vs. exploration

As in real life, during reinforcement learn-
ing often the question arises whether the
exisiting knowledge is only willfully ex-
ploited or new ways are also explored.
Initially, we want to discuss the two ex-
tremes:
A greedy policy always chooses the way
of the highest reward that can be deter-
mined in advance, i.e. the way of the high-
est known reward. This policy represents
the exploitation approach and is very
promising when the used system is already
known.
In contrast to the exploitation approach it
is the aim of the exploration approach
to explore a system as detailed as possible
so that also such paths leading to the tar-
get can be found which may be not very

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

197

research
or safety?

Appendix C Excursus: reinforcement learning

dkriesel.com

promising at ﬁrst glance but are in fact
very successful.

Let us assume that we are looking for the
way to a restaurant, a safe policy would
be to always take the way we already
know, not matter how unoptimal and long
it may be, and not to try to explore bet-
ter ways. Another approach would be to
explore shorter ways every now and then,
even at the risk of taking a long time and
being unsuccessful, and therefore ﬁnally
having to take the original way and arrive
too late at the restaurant.

In reality, often a combination of both
methods is applied: In the beginning of
the learning process it is researched with
a higher probability while at the end more
existing knowledge is exploited. Here, a
static probability distribution is also pos-
sible and often applied.

In the gridworld: For ﬁnding the way in
the gridworld, the restaurant example ap-
plies equally.

C.2 Learning process

Let us again take a look at daily life. Ac-
tions can lead us from one situation into
diﬀerent subsituations, from each subsit-
uation into further sub-subsituations. In
a sense, we get a situation tree where
links between the nodes must be consid-
ered (often there are several ways to reach
a situation – so the tree could more accu-
rately be referred to as a situation graph).

he leaves of such a tree are the end situ-
ations of the system. The exploration ap-
proach would search the tree as thoroughly
as possible and become acquainted with all
leaves. The exploitation approach would
unerringly go to the best known leave.

Analogous to the situation tree, we also
can create an action tree. Here, the re-
wards for the actions are within the nodes.
Now we have to adapt from daily life how
we learn exactly.

C.2.1 Rewarding strategies

Interesting and very important is the ques-
tion for what a reward and what kind of
reward is awarded since the design of the
reward signiﬁcantly controls system behav-
ior. As we have seen above, there gener-
ally are (again as in daily life) various ac-
tions that can be performed in any situa-
tion. There are diﬀerent strategies to eval-
uate the selected situations and to learn
which series of actions would lead to the
target. First of all, this principle should
be explained in the following.

We now want to indicate some extreme
cases as design examples for the reward:

A rewarding similar to the rewarding in a
chess game is referred to as pure delayed
reward: We only receive the reward at
the end of and not during the game. This
method is always advantageous when we
ﬁnally can say whether we were succesful
or not, but the interim steps do not allow

198

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.2 Learning process

an estimation of our situation. If we win,
then

rt = 0 ∀t < τ

(C.10)
as well as rτ = 1. If we lose, then rτ = −1.
With this rewarding strategy a reward is
only returned by the leaves of the situation
tree.
Pure negative reward: Here,

rt = −1 ∀t < τ.

(C.11)

This system ﬁnds the most rapid way to
reach the target because this way is auto-
matically the most favorable one in respect
of the reward. The agent receives punish-
ment for anything it does – even if it does
nothing. As a result it is the most inex-
pensive method for the agent to reach the
target fast.
Another strategy is the avoidance strat-
egy: Harmful
situations are avoided.
Here,

rt ∈ {0,−1},

(C.12)

Most situations do not receive any reward,
only a few of them receive a negative re-
ward. The agent agent will avoid getting
too close to such negative situations
Warning: Rewarding strategies can have
unexpected consequences. A robot that is
told "have it your own way but if you touch
an obstacle you will be punished" will sim-
ply stand still. If standing still is also pun-
ished, it will drive in small circles. Recon-
sidering this, we will understand that this
behavior optimally fulﬁlls the return of the

robot but unfortunately was not intended
to do so.
Furthermore, we can show that especially
small tasks can be solved better by means
of negative rewards while positive, more
diﬀerentiated rewards are useful for large,
complex tasks.
For our gridworld we want to apply the
pure negative reward strategy: The robot
shall ﬁnd the exit as fast as possible.

C.2.2 The state-value function

evaluation

Unlike our agent we have a godlike view state
of our gridworld so that we can swiftly de-
termine which robot starting position can
provide which optimal return.
In ﬁgure C.3 on the next page these opti-
mal returns are applied per ﬁeld.
In the gridworld: The state-value function
for our gridworld exactly represents such
a function per situation (= position) with
the diﬀerence being that here the function
is unknown and has to be learned.
Thus, we can see that it would be more
practical for the robot to be capable to
evaluate the current and future situations.
So let us take a look at another system
component of reinforcement learning: the
state-value function V (s), which with
regard to a policy Π is often called VΠ(s).
Because whether a situation is bad often
depends on the general behavior Π of the
agent.
A situation being bad under a policy that
is searching risks and checking out limits

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

199

Appendix C Excursus: reinforcement learning

dkriesel.com

-6
-7
-6
-7
-8
-9
-10

-6
-7
-8
-9
-10
-11
-10

-5

-5
-6
-7
-8
-9

-5

-9
-10
-11
-10
-9

-4

-4
-5
-6
-7
-8

-3

-3

-7

-4

-3

-10
-11
-10
-9
-8

-7

-2
-1
-2
-3
-4
-5
-6

-2
-1
-2
-3
-4
-5
-6

Figure C.3: Representation of each optimal re-
turn per ﬁeld in our gridworld by means of pure
negative reward awarding, at the top with an
open and at the bottom with a closed door.

would be, for instance, if an agent on a bi-
cycle turns a corner and the front wheel
begins to slide out. And due to its dare-
devil policy the agent would not brake in
this situation. With a risk-aware policy
the same situations would look much bet-
ter, thus it would be evaluated higher by
a good state-value function

VΠ(s) simply returns the value the current
situation s has for the agent under policy
Π. Abstractly speaking, according to the
above deﬁnitions, the value of the state-
value function corresponds to the return
Rt (the expected value) of a situation st.

VΠ(s)(cid:73)

(cid:74)V ∗

Π(s)

EΠ denotes the set of the expected returns
under Π and the current situation st.

VΠ(s) = EΠ{Rt|s = st}

Deﬁnition C.9 (State-value function).
The state-value function VΠ(s) has the
task of determining the value of situations
under a policy, i.e. to answer the agent’s
question of whether a situation s is good
or bad or how good or bad it is. For this
purpose it returns the expectation of the
return under the situation:

VΠ(s) = EΠ{Rt|s = st}

(C.13)

The optimal state-value function is called
V ∗
Π(s).

Unfortunaely, unlike us our robot does not
have a godlike view of its environment. It
does not have a table with optimal returns
like the one shown above to orient itself.
The aim of reinforcement learning is that
the robot generates its state-value func-
tion bit by bit on the basis of the returns of
many trials and approximates the optimal
state-value function V ∗ (if there is one).
In this context I want to introduce two
terms closely related to the cycle between
state-value function and policy:

C.2.2.1 Policy evaluation

Policy evaluation is the approach to try
a policy a few times, to provide many re-
wards that way and to gradually accumu-
late a state-value function by means of
these rewards.

200

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.2 Learning process

V

V ∗

Π

Π∗

Figure C.4: The cycle of reinforcement learning
which ideally leads to optimal Π∗ and V ∗.

C.2.2.2 Policy improvement

Policy improvement means to improve
a policy itself, i.e. to turn it into a new and
better one. In order to improve the policy
we have to aim at the return ﬁnally having
a larger value than before, i.e. until we
have found a shorter way to the restaurant
and have walked it successfully

The principle of reinforcement learning is
to realize an interaction. It is tried to eval-
uate how good a policy is in individual
situations. The changed state-value func-
tion provides information about the sys-
tem with which we again improve our pol-
icy. These two values lift each other, which
can mathematically be proved, so that the
ﬁnal result is an optimal policy Π∗ and an
optimal state-value function V ∗ (ﬁg. C.4).
This cycle sounds simple but is very time-
consuming.

At ﬁrst, let us regard a simple, random pol-
icy by which our robot could slowly fulﬁll
and improve its state-value function with-
out any previous knowledge.

C.2.3 Monte Carlo method

The easiest approach to accumulate a
state-value function is mere trial and er-
ror. Thus, we select a randomly behaving
policy which does not consider the accumu-
lated state-value function for its random
decisions. It can be proved that at some
point we will ﬁnd the exit of our gridworld
by chance.
Inspired by random-based games of chance
this approach is called Monte Carlo
method.
If we additionally assume a pure negative
reward, it is obvious that we can receive
an optimum value of −6 for our starting
ﬁeld in the state-value function. Depend-
ing on the random way the random policy
takes values other (smaller) than −6 can
occur for the starting ﬁeld. Intuitively, we
want to memorize only the better value for
one state (i.e. one ﬁeld). But here caution
is advised: In this way, the learning proce-
dure would work only with deterministic
systems. Our door, which can be open or
closed during a cycle, would produce oscil-
lations for all ﬁelds and such oscillations
would inﬂuence their shortest way to the
target.
With the Monte Carlo method we prefer
to use the learning rule2
V (st)new = V (st)alt + α(Rt − V (st)alt),
in which the update of the state-value func-
tion is obviously inﬂuenced by both the
2 The learning rule is, among others, derived by
means of the Bellman equation, but this deriva-
tion is not discussed in this chapter.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

201

)
)


i
i


Appendix C Excursus: reinforcement learning

dkriesel.com

α(cid:73)

old state value and the received return (α
is the learning rate). Thus, the agent gets
some kind of memory, new ﬁndings always
change the situation value just a little bit.
An exemplary learning step is shown in
ﬁg. C.5.
In this example, the computation of the
state value was applied for only one single
state (our initial state). It should be ob-
vious that it is possible (and often done)
to train the values for the states visited in-
between (in case of the gridworld our ways
to the target) at the same time. The result
of such a calculation related to our exam-
ple is illustrated in ﬁg. C.6 on the facing
page.
The Monte Carlo method seems to be
suboptimal and usually it is signiﬁcantly
slower than the following methods of re-
inforcement learning. But this method is
the only one for which it can be mathemat-
ically proved that it works and therefore
it is very useful for theoretical considera-
tions.
Deﬁnition C.10 (Monte Carlo learning).
Actions are randomly performed regard-
less of the state-value function and in the
long term an expressive state-value func-
tion is accumulated by means of the fol-
lowing learning rule.
V (st)new = V (st)alt + α(Rt − V (st)alt),

C.2.4 Temporal diﬀerence learning

Most of the learning is the result of ex-
periences; e.g. walking or riding a bicycle

-6

-5

-4

-3

-1
-2

-14

-13

-12
-11
-10
-9
-8

-7

-1
-2
-3
-4
-5
-6

-10

Figure C.5: Application of the Monte Carlo
learning rule with a learning rate of α = 0.5.
Top: two exemplary ways the agent randomly
selects are applied (one with an open and one
with a closed door). Bottom: The result of the
learning rule for the value of the initial state con-
sidering both ways. Due to the fact that in the
course of time many diﬀerent ways are walked
given a random policy, a very expressive state-
value function is obtained.

202

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.2 Learning process

-10

-9

-8
-11
-10
-9
-8

-3

-7

-1
-2
-3
-4
-5
-6

Figure C.6: Extension of the learning example
in ﬁg. C.5 in which the returns for intermedi-
ate states are also used to accumulate the state-
value function. Here, the low value on the door
ﬁeld can be seen very well: If this state is possi-
ble, it must be very positive. If the door is closed,
this state is impossible.

Evaluation

Π

Q

policy improvement

Figure C.7: We try diﬀerent actions within the
environment and as a result we learn and improve
the policy.

without getting injured (or not), even men-
tal skills like mathematical problem solv-
ing beneﬁt a lot from experience and sim-
ple trial and error. Thus, we initialize our
policy with arbitrary values – we try, learn
and improve the policy due to experience
(ﬁg. C.7). In contrast to the Monte Carlo
method we want to do this in a more di-
rected manner.

Just as we learn from experience to re-
act on diﬀerent situations in diﬀerent ways

the temporal diﬀerence learning (abbre-
viated: TD learning), does the same by
training VΠ(s) (i.e. the agent learns to esti-
mate which situations are worth a lot and
which are not). Again the current situa-
tion is identiﬁed with st, the following sit-
uations with st+1 and so on. Thus, the
learning formula for the state-value func-
tion VΠ(st) is

V (st)new =V (st)

|
}
+ α(rt+1 + γV (st+1) − V (st))

{z

change of previous value

We can see that the change in value of the
current situation st, which is proportional
to the learning rate α, is inﬂuenced by

. the received reward rt+1,
. the previous return weighted with a
factor γ of the following situation
V (st+1),

. the previous value of the situation

V (st).

Unlike

Deﬁnition C.11 (Temporal diﬀerence
learning).
the Monte Carlo
method, TD learning looks ahead by re-
garding the following situation st+1. Thus,
the learning rule is given by

V (st)new =V (st)

(C.14)
|
}
+ α(rt+1 + γV (st+1) − V (st))
.

{z

change of previous value

C.2.5 The action-value function

Analogous
VΠ(s),

to the state-value function

the action-value

function action

evaluation

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

203

!
!
a
a
Appendix C Excursus: reinforcement learning

dkriesel.com

0
× +1
-1

C.2.6 Q learning

This implies QΠ(s, a) as learning fomula
for the action-value function, and – analo-
gously to TD learning – its application is
called Q learning:

Figure C.8: Exemplary values of an action-
value function for the position ×. Moving right,
one remains on the fastest way towards the tar-
get, moving up is still a quite fast way, moving
down is not a good way at all (provided that the
door is open for all cases).

QΠ(s, a) is another system component of
QΠ(s, a)(cid:73) reinforcement learning, which evaluates a
certain action a under a certain situation
s and the policy Π.
In the gridworld:
In the gridworld, the
action-value function tells us how good it
is to move from a certain ﬁeld into a cer-
tain direction (ﬁg. C.8).
Deﬁnition C.12 (Action-value function).
Like the state-value function, the action-
value function QΠ(st, a) evaluates certain
actions on the basis of certain situations
under a policy. The optimal action-value
function is called Q∗

Π(st, a).

Π(s, a)(cid:73)
Q∗

As shown in ﬁg. C.9, the actions are per-
formed until a target situation (here re-
ferred to as sτ) is achieved (if there exists a
target situation, otherwise the actions are
simply performed again and again).

Q(st, a)new =Q(st, a)

+ α(rt+1 + γ max

Q(st+1, a)

−Q(st, a))

.

}

}

|

|

a

{z
{z

greedy strategy

change of previous value

Again we break down the change of the
current action value (proportional to the
learning rate α) under the current situa-
tion. It is inﬂuenced by

. the received reward rt+1,

. the maximum action over the follow-
ing actions weighted with γ (Here, a
greedy strategy is applied since it can
be assumed that the best known ac-
tion is selected. With TD learning,
on the other hand, we do not mind to
always get into the best known next
situation.),

. the previous value of the action under
our situation st known as Q(st, a) (re-
member that this is also weighted by
means of α).

Usually, the action-value function learns
considerably faster than the state-value
function. But we must not disregard that
reinforcement learning is generally quite
slow: The system has to ﬁnd out itself
what is good. But the advantage of Q

204

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.3 Example applications

GFED
@ABCs0

a0

r1

GFED
@ABCs1

direction of actions

···

GFED
@ABC

a1

aτ−2/
rτ−1
r2
direction of reward

sτ−1

ONML
HIJK

aτ−1 /
rτ

GFED
@ABCsτ

Figure C.9: Actions are performed until the desired target situation is achieved. Attention should
be paid to numbering: Rewards are numbered beginning with 1, actions and situations beginning
with 0 (This has simply been adopted as a convention).

Q(st, a)new =Q(st, a)

+ α(rt+1 + γ max

a

(C.15)

Q(st+1, a) − Q(st, a)).

played backgammon knows that the situ-
ation space is huge (approx. 1020 situa-
tions). As a result, the state-value func-
tions cannot be computed explicitly (par-
ticularly in the late eighties when TD gam-
mon was introduced). The selected re-
warding strategy was the pure delayed re-
ward, i.e. the system receives the reward
not before the end of the game and at the
same time the reward is the return. Then
the system was allowed to practice itself
(initially against a backgammon program,
then against an entity of itself). The result
was that it achieved the highest ranking in
a computer-backgammon league and strik-
ingly disproved the theory that a computer
programm is not capable to master a task
better than its programmer.

learning is: Π can be initialized arbitrar-
ily, and by means of Q learning the result
is always Q∗.
Deﬁnition C.13 (Q learning). Q learn-
ing trains the action-value function by
means of the learning rule

and thus ﬁnds Q∗ in any case.

C.3 Example applications

C.3.1 TD gammon

C.3.2 The car in the pit

TD gammon is a very successful
backgammon game based on TD learn-
ing invented by Gerald Tesauro. The
situation here is the current conﬁgura-
tion of the board. Anyone who has ever

Let us take a look at a car parking on a
one-dimensional road at the bottom of a
deep pit without being able to get over
the slope on both sides straight away by
means of its engine power in order to leave

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

205

/
/
(
(
/
/
k
k
/
k
k
/
k
k
l
l
h
h
Appendix C Excursus: reinforcement learning

dkriesel.com

the pit. Trivially, the executable actions
here are the possibilities to drive forwards
and backwards. The intuitive solution we
think of immediately is to move backwards,
to gain momentum at the opposite slope
and oscillate in this way several times to
dash out of the pit.
The actions of a reinforcement learning
system would be "full throttle forward",
"full reverse" and "doing nothing".
Here, "everything costs" would be a good
choice for awarding the reward so that the
system learns fast how to leave the pit and
realizes that our problem cannot be solved
by means of mere forward directed engine
power. So the system will slowly build up
the movement.
The policy can no longer be stored as a
table since the state space is hard to dis-
cretize. As policy a function has to be
generated.

C.3.3 The pole balancer

The pole balancer was developed by
Barto, Sutton and Anderson.
Let be given a situation including a vehicle
that is capable to move either to the right
at full throttle or to the left at full throt-
tle (bang bang control). Only these two
actions can be performed, standing still
is impossible. On the top of this car is
hinged an upright pole that could tip over
to both sides. The pole is built in such a
way that it always tips over to one side so
it never stands still (let us assume that the
pole is rounded at the lower end).

The angle of the pole relative to the verti-
cal line is referred to as α. Furthermore,
the vehicle always has a ﬁxed position x an
our one-dimensional world and a velocity
of
˙x. Our one-dimensional world is lim-
ited, i.e. there are maximum values and
minimum values x can adopt.

The aim of our system is to learn to steer
the car in such a way that it can balance
the pole, to prevent the pole from tipping
over. This is achieved best by an avoid-
ance strategy: As long as the pole is bal-
anced the reward is 0. If the pole tips over,
the reward is -1.

Interestingly, the system is soon capable
to keep the pole balanced by tilting it suf-
ﬁciently fast and with small movements.
At this the system mostly is in the cen-
ter of the space since this is farthest from
the walls which it understands as negative
(if it touches the wall, the pole will tip
over).

C.3.3.1 Swinging up an inverted

pendulum

More diﬃcult for the system is the fol-
lowing initial situation: the pole initially
hangs down, has to be swung up over the
vehicle and ﬁnally has to be stabilized. In
the literature this task is called swing up
an inverted pendulum.

206

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

C.4 Reinforcement learning in connection with neural networks

ment learning to ﬁnd a strategy in order
to exit a maze as fast as possible.

. What could an appropriate state-

value function look like?

. How would you generate an appropri-

ate reward?

Assume that the robot is capable to avoid
obstacles and at any time knows its posi-
tion (x, y) and orientation φ.
Exercise 20. Describe the function of
the two components ASE and ACE as
they have been proposed by Barto, Sut-
ton and Anderson to control the pole
balancer.
Bibliography: [BSA83].
Exercise 21.
Indicate several "classical"
problems of informatics which could be
solved eﬃciently by means of reinforce-
ment learning. Please give reasons for
your answers.

C.4 Reinforcement learning in

connection with neural
networks

Finally, the reader would like to ask why a
text on "neural networks" includes a chap-
ter about reinforcement learning.
The answer is very simple. We have al-
ready been introduced to supervised and
unsupervised learning procedures.
Al-
though we do not always have an om-
niscient teacher who makes unsupervised
learning possible, this does not mean that
we do not receive any feedback at all.
There is often something in between, some
kind of criticism or school mark. Problems
like this can be solved by means of rein-
forcement learning.
But not every problem is that easily solved
like our gridworld: In our backgammon ex-
ample we have approx. 1020 situations and
the situation tree has a large branching fac-
tor, let alone other games. Here, the tables
used in the gridworld can no longer be re-
alized as state- and action-value functions.
Thus, we have to ﬁnd approximators for
these functions.
And which learning approximators for
these reinforcement learning components
come immediately into our mind? Exactly:
neural networks.

Exercises

Exercise 19. A robot control system
shall be persuaded by means of reinforce-

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

207

Bibliography

[And72]

[APZ93]

[BSA83]

[CG87]

[CG88]

[CG90]

[CH67]

[CR00]

[Cyb89]

[DHS01]

James A. Anderson. A simple neural network generating an interactive
memory. Mathematical Biosciences, 14:197–220, 1972.
D. Anguita, G. Parodi, and R. Zunino. Speed improvement of the back-
propagation on current-generation workstations. In WCNN’93, Portland:
World Congress on Neural Networks, July 11-15, 1993, Oregon Convention
Center, Portland, Oregon, volume 1. Lawrence Erlbaum, 1993.
A. Barto, R. Sutton, and C. Anderson. Neuron-like adaptive elements
that can solve diﬃcult learning control problems. IEEE Transactions on
Systems, Man, and Cybernetics, 13(5):834–846, September 1983.
G. A. Carpenter and S. Grossberg. ART2: Self-organization of stable cate-
gory recognition codes for analog input patterns. Applied Optics, 26:4919–
4930, 1987.
M.A. Cohen and S. Grossberg. Absolute stability of global pattern forma-
tion and parallel memory storage by competitive neural networks. Com-
puter Society Press Technology Series Neural Networks, pages 70–81, 1988.
G. A. Carpenter and S. Grossberg. ART 3: Hierarchical search using
chemical transmitters in self-organising pattern recognition architectures.
Neural Networks, 3(2):129–152, 1990.
T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation.
Transactions on Information Theory, 13(1):21–27, 1967.
N.A. Campbell and JB Reece. Biologie. Spektrum. Akademischer Verlag,
2000.
G. Cybenko. Approximation by superpositions of a sigmoidal function.
Mathematics of Control, Signals, and Systems (MCSS), 2(4):303–314,
1989.
R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classiﬁcation. Wiley New
York, 2001.

IEEE

209

Bibliography

dkriesel.com

[Elm90]

[Fah88]

[FMI83]

[Fri94]

Jeﬀrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–
211, April 1990.
S. E. Fahlman. An empirical sudy of learning speed in back-propagation
networks. Technical Report CMU-CS-88-162, CMU, 1988.
K. Fukushima, S. Miyake, and T. Ito. Neocognitron: A neural network
model for a mechanism of visual pattern recognition. IEEE Transactions
on Systems, Man, and Cybernetics, 13(5):826–834, September/October
1983.
B. Fritzke. Fast learning with incremental RBF networks. Neural Process-
ing Letters, 1(1):2–5, 1994.

[Gro76]

[GS06]

[Heb49]

[GKE01a] N. Goerke, F. Kintzler, and R. Eckmiller. Self organized classiﬁcation of
chaotic domains from a nonlinearattractor. In Neural Networks, 2001. Pro-
ceedings. IJCNN’01. International Joint Conference on, volume 3, 2001.
[GKE01b] N. Goerke, F. Kintzler, and R. Eckmiller. Self organized partitioning of
chaotic attractors for control. Lecture notes in computer science, pages
851–856, 2001.
S. Grossberg. Adaptive pattern classiﬁcation and universal recoding, I:
Parallel development and coding of neural feature detectors. Biological
Cybernetics, 23:121–134, 1976.
Nils Goerke and Alexandra Scherbart. Classiﬁcation using multi-soms and
multi-neural gas. In IJCNN, pages 3895–3902, 2006.
Donald O. Hebb. The Organization of Behavior: A Neuropsychological
Theory. Wiley, New York, 1949.
John J. Hopﬁeld. Neural networks and physical systems with emergent col-
lective computational abilities. Proc. of the National Academy of Science,
USA, 79:2554–2558, 1982.
JJ Hopﬁeld. Neurons with graded response have collective computational
properties like those of two-state neurons. Proceedings of the National
Academy of Sciences, 81(10):3088–3092, 1984.
JJ Hopﬁeld and DW Tank. Neural computation of decisions in optimiza-
tion problems. Biological cybernetics, 52(3):141–152, 1985.
M. I. Jordan. Attractor dynamics and parallelism in a connectionist se-
quential machine. In Proceedings of the Eighth Conference of the Cognitive
Science Society, pages 531–546. Erlbaum, 1986.

[Hop82]

[Hop84]

[HT85]

[Jor86]

210

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Bibliography

[Kau90]

[Koh72]

[Koh82]

[Koh89]

[Koh98]

[KSJ00]

L. Kaufman. Finding groups in data: an introduction to cluster analysis.
In Finding Groups in Data: An Introduction to Cluster Analysis. Wiley,
New York, 1990.

T. Kohonen. Correlation matrix memories. IEEEtC, C-21:353–359, 1972.

Teuvo Kohonen. Self-organized formation of topologically correct feature
maps. Biological Cybernetics, 43:59–69, 1982.

Teuvo Kohonen. Self-Organization and Associative Memory. Springer-
Verlag, Berlin, third edition, 1989.

T. Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1–6, 1998.

E.R. Kandel, J.H. Schwartz, and T.M. Jessell. Principles of neural science.
Appleton & Lange, 2000.

[lCDS90] Y. le Cun, J. S. Denker, and S. A. Solla. Optimal brain damage.

In
D. Touretzky, editor, Advances in Neural Information Processing Systems
2, pages 598–605. Morgan Kaufmann, 1990.

[Mac67]

[MBS93]

J. MacQueen. Some methods for classiﬁcation and analysis of multivariate
observations. In Proceedings of the Fifth Berkeley Symposium on Mathe-
matics, Statistics and Probability, Vol. 1, pages 281–296, 1967.

Thomas M. Martinetz, Stanislav G. Berkovich, and Klaus J. Schulten.
’Neural-gas’ network for vector quantization and its application to time-
series prediction. IEEE Trans. on Neural Networks, 4(4):558–569, 1993.

[MBW+10] K.D. Micheva, B. Busse, N.C. Weiler, N. O’Rourke, and S.J. Smith. Single-
synapse analysis of a diverse synapse population: proteomic imaging meth-
ods and markers. Neuron, 68(4):639–653, 2010.

[MP43]

[MP69]

[MR86]

W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent
in nervous activity. Bulletin of Mathematical Biology, 5(4):115–133, 1943.

M. Minsky and S. Papert. Perceptrons. MIT Press, Cambridge, Mass,
1969.

J. L. McClelland and D. E. Rumelhart. Parallel Distributed Processing:
Explorations in the Microstructure of Cognition, volume 2. MIT Press,
Cambridge, 1986.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

211

Bibliography

[Par87]

[PG89]

[Pin87]

[PM47]

[Pre94]

[RB93]

[RD05]

dkriesel.com

David R. Parker. Optimal algorithms for adaptive networks: Second or-
der back propagation, second order direct propagation, and second order
hebbian learning. In Maureen Caudill and Charles Butler, editors, IEEE
First International Conference on Neural Networks (ICNN’87), volume II,
pages II–593–II–600, San Diego, CA, June 1987. IEEE.
T. Poggio and F. Girosi. A theory of networks for approximation and
learning. MIT Press, Cambridge Mass., 1989.
F. J. Pineda. Generalization of back-propagation to recurrent neural net-
works. Physical Review Letters, 59:2229–2232, 1987.
W. Pitts and W.S. McCulloch. How we know universals the perception of
auditory and visual forms. Bulletin of Mathematical Biology, 9(3):127–147,
1947.
L. Prechelt. Proben1: A set of neural network benchmark problems and
benchmarking rules. Technical Report, 21:94, 1994.
M. Riedmiller and H. Braun. A direct adaptive method for faster back-
propagation learning: The rprop algorithm. In Neural Networks, 1993.,
IEEE International Conference on, pages 586–591. IEEE, 1993.
G. Roth and U. Dicke. Evolution of the brain and intelligence. Trends in
Cognitive Sciences, 9(5):250–257, 2005.

[RHW86a] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by

back-propagating errors. Nature, 323:533–536, October 1986.

[Rie94]

[Ros58]

[RHW86b] David E. Rumelhart, Geoﬀrey E. Hinton, and R. J. Williams. Learning
internal representations by error propagation. In D. E. Rumelhart, J. L.
McClelland, and the PDP research group., editors, Parallel distributed pro-
cessing: Explorations in the microstructure of cognition, Volume 1: Foun-
dations. MIT Press, 1986.
M. Riedmiller. Rprop - description and implementation details. Technical
report, University of Karlsruhe, 1994.
F. Rosenblatt. The perceptron: a probabilistic model for information
storage and organization in the brain. Psychological Review, 65:386–408,
1958.
F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, 1962.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.
MIT Press, Cambridge, MA, 1998.

[Ros62]
[SB98]

212

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Bibliography

[SG06]

[SGE05]

[Ste61]

[vdM73]

[Was89]

[Wer74]

[Wer88]

[WG94]

[WH60]

[Wid89]

[Zel94]

A. Scherbart and N. Goerke. Unsupervised system for discovering patterns
in time-series, 2006.
Rolf Schatten, Nils Goerke, and Rolf Eckmiller. Regional and online learn-
able ﬁelds. In Sameer Singh, Maneesha Singh, Chidanand Apté, and Petra
Perner, editors, ICAPR (2), volume 3687 of Lecture Notes in Computer
Science, pages 74–83. Springer, 2005.
K. Steinbuch. Die lernmatrix. Kybernetik (Biological Cybernetics), 1:36–45,
1961.
C. von der Malsburg. Self-organizing of orientation sensitive cells in striate
cortex. Kybernetik, 14:85–100, 1973.
P. D. Wasserman. Neural Computing Theory and Practice. New York :
Van Nostrand Reinhold, 1989.
P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis
in the Behavioral Sciences. PhD thesis, Harvard University, 1974.
P. J. Werbos. Backpropagation: Past and future. In Proceedings ICNN-88,
San Diego, pages 343–353, 1988.
A.S. Weigend and N.A. Gershenfeld. Time series prediction. Addison-
Wesley, 1994.
B. Widrow and M. E. Hoﬀ. Adaptive switching circuits. In Proceedings
WESCON, pages 96–104, 1960.
R. Widner. Single-stage logic. AIEE Fall General Meeting, 1960. Wasser-
man, P. Neural Computing, Theory and Practice, Van Nostrand Reinhold,
1989.
Andreas Zell. Simulation Neuronaler Netze. Addison-Wesley, 1994. Ger-
man.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

213

List of Figures

1.1 Robot with 8 sensors and 2 motors . . . . . . . . . . . . . . . . . . . . .
1.3 Black box with eight inputs and two outputs
. . . . . . . . . . . . . . .
1.2 Learning samples for the example robot . . . . . . . . . . . . . . . . . .
1.4
Institutions of the ﬁeld of neural networks . . . . . . . . . . . . . . . . .

6
7
8
9

2.1 Central nervous system . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2 Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3 Biological neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.4 Action potential
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.5 Compound eye . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

3.1 Data processing of a neuron . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.2 Various popular activation functions . . . . . . . . . . . . . . . . . . . . 38
3.3 Feedforward network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.4 Feedforward network with shortcuts
. . . . . . . . . . . . . . . . . . . . 41
3.5 Directly recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.6
Indirectly recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.7 Laterally recurrent network . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.8 Completely linked network . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.10 Examples for diﬀerent types of neurons
. . . . . . . . . . . . . . . . . . 45
3.9 Example network with and without bias neuron . . . . . . . . . . . . . . 46

4.1 Training samples and network capacities . . . . . . . . . . . . . . . . . . 56
4.2 Learning curve with diﬀerent scalings
. . . . . . . . . . . . . . . . . . . 60
4.3 Gradient descent, 2D visualization . . . . . . . . . . . . . . . . . . . . . 62
4.4 Possible errors during a gradient descent . . . . . . . . . . . . . . . . . . 63
4.5 The 2-spiral problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.6 Checkerboard problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

5.1 The perceptron in three diﬀerent views . . . . . . . . . . . . . . . . . . . 72
5.2 Singlelayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.3 Singlelayer perceptron with several output neurons . . . . . . . . . . . . 74
5.4 AND and OR singlelayer perceptron . . . . . . . . . . . . . . . . . . . . 75

215

List of Figures

dkriesel.com

5.5 Error surface of a network with 2 connections . . . . . . . . . . . . . . . 78
5.6 Sketch of a XOR-SLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.7 Two-dimensional linear separation . . . . . . . . . . . . . . . . . . . . . 82
5.8 Three-dimensional linear separation . . . . . . . . . . . . . . . . . . . . 83
5.9 The XOR network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.10 Multilayer perceptrons and output sets . . . . . . . . . . . . . . . . . . . 85
5.11 Position of an inner neuron for derivation of backpropagation . . . . . . 87
5.12 Illustration of the backpropagation derivation . . . . . . . . . . . . . . . 89
5.13 Momentum term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.14 Fermi function and hyperbolic tangent . . . . . . . . . . . . . . . . . . . 102
5.15 Functionality of 8-2-8 encoding . . . . . . . . . . . . . . . . . . . . . . . 103

6.1 RBF network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.2 Distance function in the RBF network . . . . . . . . . . . . . . . . . . . 108
6.3
Individual Gaussian bells in one- and two-dimensional space . . . . . . . 109
6.4 Accumulating Gaussian bells in one-dimensional space . . . . . . . . . . 109
6.5 Accumulating Gaussian bells in two-dimensional space . . . . . . . . . . 110
6.6 Even coverage of an input space with radial basis functions
. . . . . . . 116
6.7 Uneven coverage of an input space with radial basis functions . . . . . . 117
6.8 Random, uneven coverage of an input space with radial basis functions . 117

7.1 Roessler attractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.2 Jordan network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
7.3 Elman network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.4 Unfolding in time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

8.1 Hopﬁeld network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
8.2 Binary threshold function . . . . . . . . . . . . . . . . . . . . . . . . . . 132
8.3 Convergence of a Hopﬁeld network . . . . . . . . . . . . . . . . . . . . . 134
8.4 Fermi function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

9.1 Examples for quantization . . . . . . . . . . . . . . . . . . . . . . . . . . 141

10.1 Example topologies of a SOM . . . . . . . . . . . . . . . . . . . . . . . . 148
10.2 Example distances of SOM topologies
. . . . . . . . . . . . . . . . . . . 151
10.3 SOM topology functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
10.4 First example of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
10.7 Topological defect of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . 156
10.5 Training a SOM with one-dimensional topology . . . . . . . . . . . . . . 157
10.6 SOMs with one- and two-dimensional topologies and diﬀerent input spaces158
10.8 Resolution optimization of a SOM to certain areas . . . . . . . . . . . . 160

216

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

List of Figures

10.9 Shape to be classiﬁed by neural gas . . . . . . . . . . . . . . . . . . . . . 162

11.1 Structure of an ART network . . . . . . . . . . . . . . . . . . . . . . . . 166
11.2 Learning process of an ART network . . . . . . . . . . . . . . . . . . . . 168

A.1 Comparing cluster analysis methods
. . . . . . . . . . . . . . . . . . . . 174
A.2 ROLF neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
A.3 Clustering by means of a ROLF . . . . . . . . . . . . . . . . . . . . . . . 179

B.1 Neural network reading time series . . . . . . . . . . . . . . . . . . . . . 182
B.2 One-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 184
B.3 Two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 186
B.4 Direct two-step-ahead prediction . . . . . . . . . . . . . . . . . . . . . . 186
B.5 Heterogeneous one-step-ahead prediction . . . . . . . . . . . . . . . . . . 188
B.6 Heterogeneous one-step-ahead prediction with two outputs . . . . . . . . 188

C.1 Gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
C.2 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
C.3 Gridworld with optimal returns . . . . . . . . . . . . . . . . . . . . . . . 200
C.4 Reinforcement learning cycle
. . . . . . . . . . . . . . . . . . . . . . . . 201
C.5 The Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . . 202
C.6 Extended Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . 203
C.7 Improving the policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
C.8 Action-value function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
C.9 Reinforcement learning timeline . . . . . . . . . . . . . . . . . . . . . . . 205

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

217

Index

*
100-step rule . . . . . . . . . . . . . . . . . . . . . . . . 5

ATP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
attractor . . . . . . . . . . . . . . . . . . . . . . . . . . 119
autoassociator. . . . . . . . . . . . . . . . . . . . .131
axon. . . . . . . . . . . . . . . . . . . . . . . . . . . .18, 23

A
Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
action potential . . . . . . . . . . . . . . . . . . . . 21
action space . . . . . . . . . . . . . . . . . . . . . . . 195
action-value function . . . . . . . . . . . . . . 203
activation . . . . . . . . . . . . . . . . . . . . . . . . . . 36
activation function . . . . . . . . . . . . . . . . . 36
selection of . . . . . . . . . . . . . . . . . . . . . 98
ADALINE . . see adaptive linear neuron
adaptive linear element . . . see adaptive

linear neuron

adaptive linear neuron. . . . . . . . . . . . . .10
adaptive resonance theory . . . . . 11, 165
agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .194
algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . .50
amacrine cell . . . . . . . . . . . . . . . . . . . . . . . 28
approximation. . . . . . . . . . . . . . . . . . . . .110
ART . . . . see adaptive resonance theory
ART-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
ART-2A. . . . . . . . . . . . . . . . . . . . . . . . . . .168
ART-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
artiﬁcial intelligence . . . . . . . . . . . . . . . . 10
associative data storage . . . . . . . . . . . 157

B
backpropagation. . . . . . . . . . . . . . . . . . . .88
second order . . . . . . . . . . . . . . . . . . . 95
backpropagation of error. . . . . . . . . . . .84
recurrent . . . . . . . . . . . . . . . . . . . . . . 125
bar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
bias neuron. . . . . . . . . . . . . . . . . . . . . . . . .44
binary threshold function . . . . . . . . . . 37
bipolar cell . . . . . . . . . . . . . . . . . . . . . . . . . 27
black box. . . . . . . . . . . . . . . . . . . . . . . . . . . .7
brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
brainstem . . . . . . . . . . . . . . . . . . . . . . . . . . 16

C
capability to learn . . . . . . . . . . . . . . . . . . . 4
center

of a ROLF neuron . . . . . . . . . . . . 176
of a SOM neuron. . . . . . . . . . . . . .146

219

Index

dkriesel.com

of an RBF neuron. . . . . . . . . . . . .104
distance to the . . . . . . . . . . . . . . 107
central nervous system . . . . . . . . . . . . . 14
cerebellum . . . . . . . . . . . . . . . . . . . . . . . . . 15
cerebral cortex . . . . . . . . . . . . . . . . . . . . . 14
cerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
change in weight. . . . . . . . . . . . . . . . . . . .64
cluster analysis . . . . . . . . . . . . . . . . . . . . 171
clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
CNS . . . . . . . see central nervous system
codebook vector . . . . . . . . . . . . . . 138, 172
complete linkage. . . . . . . . . . . . . . . . . . . .39
compound eye . . . . . . . . . . . . . . . . . . . . . . 26
concentration gradient. . . . . . . . . . . . . .19
cone function. . . . . . . . . . . . . . . . . . . . . .150
connection. . . . . . . . . . . . . . . . . . . . . . . . . .34
context-based search . . . . . . . . . . . . . . 157
continuous . . . . . . . . . . . . . . . . . . . . . . . . 137
cortex . . . . . . . . . . . . . . see cerebral cortex
visual . . . . . . . . . . . . . . . . . . . . . . . . . . 15
cortical ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . 14
association . . . . . . . . . . . . . . . . . . . . . 15
primary . . . . . . . . . . . . . . . . . . . . . . . . 15
cylinder function . . . . . . . . . . . . . . . . . . 150

D
Dartmouth Summer Research Project9
deep networks . . . . . . . . . . . . . . . . . . 93, 97
Delta. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79
delta rule. . . . . . . . . . . . . . . . . . . . . . . . . . .79
dendrite . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
depolarization . . . . . . . . . . . . . . . . . . . . . . 21
diencephalon . . . . . . . . . . . . see interbrain
diﬀerence vector . . . . . . . see error vector
digital ﬁlter . . . . . . . . . . . . . . . . . . . . . . . 183

digitization. . . . . . . . . . . . . . . . . . . . . . . .138
discrete . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
discretization . . . . . . . . . see quantization
distance

Euclidean . . . . . . . . . . . . . . . . . 56, 171
squared. . . . . . . . . . . . . . . . . . . .76, 171
dynamical system . . . . . . . . . . . . . . . . . 119

E
early stopping . . . . . . . . . . . . . . . . . . . . . . 59
electronic brain . . . . . . . . . . . . . . . . . . . . . . 9
Elman network . . . . . . . . . . . . . . . . . . . . 121
environment. . . . . . . . . . . . . . . . . . . . . . .193
episode. . . . . . . . . . . . . . . . . . . . . . . . . . . .196
epoch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
epsilon-nearest neighboring . . . . . . . . 173
error

speciﬁc . . . . . . . . . . . . . . . . . . . . . . . . . 56
total . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
error function . . . . . . . . . . . . . . . . . . . . . . 75
speciﬁc . . . . . . . . . . . . . . . . . . . . . . . . . 75
error vector . . . . . . . . . . . . . . . . . . . . . . . . 53
evolutionary algorithms . . . . . . . . . . . 125
exploitation approach . . . . . . . . . . . . . 197
exploration approach . . . . . . . . . . . . . . 197
exteroceptor. . . . . . . . . . . . . . . . . . . . . . . .24

F
fastprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
fault tolerance . . . . . . . . . . . . . . . . . . . . . . . 4
feedforward. . . . . . . . . . . . . . . . . . . . . . . . .39
Fermi function . . . . . . . . . . . . . . . . . . . . . 37
ﬂat spot elimination . . . . . . . . . . . . . . . . 95

220

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Index

fudging . . . . . . . see ﬂat spot elimination
function approximation . . . . . . . . . . . . . 98
function approximator

universal . . . . . . . . . . . . . . . . . . . . . . . 82

G
ganglion cell. . . . . . . . . . . . . . . . . . . . . . . .27
Gauss-Markov model . . . . . . . . . . . . . . 111
Gaussian bell. . . . . . . . . . . . . . . . . . . . . .149
generalization . . . . . . . . . . . . . . . . . . . . 4, 49
glial cell . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
gradient descent . . . . . . . . . . . . . . . . . . . . 59
problems . . . . . . . . . . . . . . . . . . . . . . . 60
grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
gridworld. . . . . . . . . . . . . . . . . . . . . . . . . .192

H
Heaviside function see binary threshold

function

Hebbian rule . . . . . . . . . . . . . . . . . . . . . . . 64
generalized form. . . . . . . . . . . . . . . .65
heteroassociator . . . . . . . . . . . . . . . . . . . 132
Hinton diagram . . . . . . . . . . . . . . . . . . . . 34
history of development. . . . . . . . . . . . . . .8
Hopﬁeld networks . . . . . . . . . . . . . . . . . 127
continuous . . . . . . . . . . . . . . . . . . . . 134
horizontal cell . . . . . . . . . . . . . . . . . . . . . . 28
hyperbolic tangent . . . . . . . . . . . . . . . . . 37
hyperpolarization. . . . . . . . . . . . . . . . . . .21
hypothalamus . . . . . . . . . . . . . . . . . . . . . . 15

I
individual eye . . . . . . . . see ommatidium
input dimension . . . . . . . . . . . . . . . . . . . . 48
input patterns . . . . . . . . . . . . . . . . . . . . . . 50
input vector . . . . . . . . . . . . . . . . . . . . . . . . 48
interbrain . . . . . . . . . . . . . . . . . . . . . . . . . . 15
internodes . . . . . . . . . . . . . . . . . . . . . . . . . . 23
interoceptor . . . . . . . . . . . . . . . . . . . . . . . . 24
interpolation

precise . . . . . . . . . . . . . . . . . . . . . . . . 110
ion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
iris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

J
Jordan network. . . . . . . . . . . . . . . . . . . .120

K
k-means clustering . . . . . . . . . . . . . . . . 172
k-nearest neighboring. . . . . . . . . . . . . .172

L
layer

hidden . . . . . . . . . . . . . . . . . . . . . . . . . 39
input. . . . . . . . . . . . . . . . . . . . . . . . . . .39
output . . . . . . . . . . . . . . . . . . . . . . . . . 39
learnability . . . . . . . . . . . . . . . . . . . . . . . . . 97
learning

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

221

Index

dkriesel.com

batch. . . . . . . . . .see learning, oﬄine
oﬄine . . . . . . . . . . . . . . . . . . . . . . . . . . 52
online . . . . . . . . . . . . . . . . . . . . . . . . . . 52
reinforcement . . . . . . . . . . . . . . . . . . 51
supervised. . . . . . . . . . . . . . . . . . . . . .51
unsupervised . . . . . . . . . . . . . . . . . . . 50
learning rate . . . . . . . . . . . . . . . . . . . . . . . 89
variable . . . . . . . . . . . . . . . . . . . . . . . . 90
learning strategy . . . . . . . . . . . . . . . . . . . 39
learning vector quantization . . . . . . . 137
lens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
linear separability . . . . . . . . . . . . . . . . . . 81
linearer associator . . . . . . . . . . . . . . . . . . 11
locked-in syndrome . . . . . . . . . . . . . . . . . 16
logistic function . . . . see Fermi function
temperature parameter . . . . . . . . . 37
LVQ . . see learning vector quantization
LVQ1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
LVQ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
LVQ3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140

M
M-SOM . see self-organizing map, multi
Mark I perceptron . . . . . . . . . . . . . . . . . . 10
Mathematical Symbols

(t) . . . . . . . . . . . . . . . see time concept
A(S) . . . . . . . . . . . . . see action space
Ep . . . . . . . . . . . . . . . . see error vector
G . . . . . . . . . . . . . . . . . . . . see topology
N . . see self-organizing map, input

dimension

P . . . . . . . . . . . . . . . . . see training set
Q∗
Π(s, a) . see action-value function,
QΠ(s, a) . see action-value function
Rt . . . . . . . . . . . . . . . . . . . . . . see return

optimal

optimal

S . . . . . . . . . . . . . . see situation space
T . . . . . . see temperature parameter
V ∗
Π(s) . . . . . see state-value function,
VΠ(s) . . . . . see state-value function
W . . . . . . . . . . . . . . see weight matrix
∆wi,j . . . . . . . . see change in weight
Π . . . . . . . . . . . . . . . . . . . . . . . see policy
Θ. . . . . . . . . . . . . .see threshold value
α . . . . . . . . . . . . . . . . . . see momentum
β . . . . . . . . . . . . . . . . see weight decay
δ . . . . . . . . . . . . . . . . . . . . . . . . see Delta
η . . . . . . . . . . . . . . . . .see learning rate
η↑ . . . . . . . . . . . . . . . . . . . . . . see Rprop
η↓ . . . . . . . . . . . . . . . . . . . . . . see Rprop
ηmax . . . . . . . . . . . . . . . . . . . .see Rprop
ηmin . . . . . . . . . . . . . . . . . . . . see Rprop
ηi,j . . . . . . . . . . . . . . . . . . . . . see Rprop
∇ . . . . . . . . . . . . . . see nabla operator
ρ . . . . . . . . . . . . . see radius multiplier
Err . . . . . . . . . . . . . . . . see error, total
Err(W) . . . . . . . . . see error function
Errp . . . . . . . . . . . . . see error, speciﬁc
Errp(W)see error function, speciﬁc
ErrWD . . . . . . . . . . . see weight decay
at . . . . . . . . . . . . . . . . . . . . . . see action
c. . . . . . . . . . . . . . . . . . . . . . . .see center
of an RBF neuron, see neuron,
self-organizing map, center

m . . . . . . . . . . . see output dimension
n . . . . . . . . . . . . . see input dimension
p . . . . . . . . . . . . . see training pattern
rh . . . see center of an RBF neuron,

distance to the

rt . . . . . . . . . . . . . . . . . . . . . . see reward
st . . . . . . . . . . . . . . . . . . . . see situation
t . . . . . . . . . . . . . . . see teaching input
wi,j . . . . . . . . . . . . . . . . . . . . see weight
x . . . . . . . . . . . . . . . . . see input vector
y . . . . . . . . . . . . . . . . see output vector

222

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Index

fact . . . . . . . . see activation function
fout . . . . . . . . . . . see output function
membrane . . . . . . . . . . . . . . . . . . . . . . . . . . 19
-potential . . . . . . . . . . . . . . . . . . . . . . 19
memorized . . . . . . . . . . . . . . . . . . . . . . . . . 54
metric. . . . . . . . . . . . . . . . . . . . . . . . . . . . .171
Mexican hat function. . . . . . . . . . . . . .150
MLP. . . . . . . .see perceptron, multilayer
momentum . . . . . . . . . . . . . . . . . . . . . . . . . 94
momentum term. . . . . . . . . . . . . . . . . . . .94
Monte Carlo method . . . . . . . . . . . . . . 201
Moore-Penrose pseudo inverse . . . . . 110
moving average procedure . . . . . . . . . 184
myelin sheath . . . . . . . . . . . . . . . . . . . . . . 23

N
nabla operator. . . . . . . . . . . . . . . . . . . . . .59
Neocognitron . . . . . . . . . . . . . . . . . . . . . . . 12
nervous system . . . . . . . . . . . . . . . . . . . . . 13
network input . . . . . . . . . . . . . . . . . . . . . . 35
neural gas . . . . . . . . . . . . . . . . . . . . . . . . . 159
growing . . . . . . . . . . . . . . . . . . . . . . . 162
multi- . . . . . . . . . . . . . . . . . . . . . . . . . 161
neural network . . . . . . . . . . . . . . . . . . . . . 34
recurrent . . . . . . . . . . . . . . . . . . . . . . 119
neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
accepting . . . . . . . . . . . . . . . . . . . . . 177
binary. . . . . . . . . . . . . . . . . . . . . . . . . .71
context. . . . . . . . . . . . . . . . . . . . . . . .120
Fermi . . . . . . . . . . . . . . . . . . . . . . . . . . 71
identity . . . . . . . . . . . . . . . . . . . . . . . . 71
information processing . . . . . . . . . 71
input. . . . . . . . . . . . . . . . . . . . . . . . . . .71
RBF . . . . . . . . . . . . . . . . . . . . . . . . . . 104
output . . . . . . . . . . . . . . . . . . . . . . 104
ROLF. . . . . . . . . . . . . . . . . . . . . . . . .176

self-organizing map. . . . . . . . . . . .146
tanh . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
winner . . . . . . . . . . . . . . . . . . . . . . . . 148
neuron layers. . . . . . . . . . . . . . . . .see layer
neurotransmitters . . . . . . . . . . . . . . . . . . 17
nodes of Ranvier . . . . . . . . . . . . . . . . . . . 23

O
oligodendrocytes . . . . . . . . . . . . . . . . . . . 23
OLVQ. . . . . . . . . . . . . . . . . . . . . . . . . . . . .141
on-neuron . . . . . . . . . . . . . see bias neuron
one-step-ahead prediction . . . . . . . . . 183
heterogeneous . . . . . . . . . . . . . . . . . 187
open loop learning. . . . . . . . . . . . . . . . .125
optimal brain damage . . . . . . . . . . . . . . 96
order of activation . . . . . . . . . . . . . . . . . . 45

asynchronous

ﬁxed order . . . . . . . . . . . . . . . . . . . 47
random order . . . . . . . . . . . . . . . . 46
randomly permuted order . . . . 46
topological order . . . . . . . . . . . . . 47
synchronous . . . . . . . . . . . . . . . . . . . . 46
output dimension. . . . . . . . . . . . . . . . . . .48
output function. . . . . . . . . . . . . . . . . . . . .38
output vector. . . . . . . . . . . . . . . . . . . . . . .48

P
parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . 5
pattern . . . . . . . . . . . see training pattern
pattern recognition . . . . . . . . . . . . 98, 131
perceptron . . . . . . . . . . . . . . . . . . . . . . . . . 71
multilayer . . . . . . . . . . . . . . . . . . . . . . 82
recurrent. . . . . . . . . . . . . . . . . . . .119

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

223

Index

dkriesel.com

singlelayer. . . . . . . . . . . . . . . . . . . . . .72
perceptron convergence theorem . . . . 73
perceptron learning algorithm . . . . . . 73
period. . . . . . . . . . . . . . . . . . . . . . . . . . . . .119
peripheral nervous system . . . . . . . . . . 13
Persons

Anderson . . . . . . . . . . . . . . . . . . . . 206 f.
Anderson, James A. . . . . . . . . . . . . 11
Anguita . . . . . . . . . . . . . . . . . . . . . . . . 37
Barto . . . . . . . . . . . . . . . . . . . 191, 206 f.
Carpenter, Gail. . . . . . . . . . . .11, 165
Elman . . . . . . . . . . . . . . . . . . . . . . . . 120
Fukushima . . . . . . . . . . . . . . . . . . . . . 12
Girosi . . . . . . . . . . . . . . . . . . . . . . . . . 103
Grossberg, Stephen . . . . . . . . 11, 165
Hebb, Donald O. . . . . . . . . . . . . 9, 64
Hinton . . . . . . . . . . . . . . . . . . . . . . . . . 12
Hoﬀ, Marcian E. . . . . . . . . . . . . . . . 10
Hopﬁeld, John . . . . . . . . . . . 11 f., 127
Ito . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Jordan . . . . . . . . . . . . . . . . . . . . . . . . 120
Kohonen, Teuvo . 11, 137, 145, 157
Lashley, Karl . . . . . . . . . . . . . . . . . . . . 9
MacQueen, J. . . . . . . . . . . . . . . . . . 172
Martinetz, Thomas. . . . . . . . . . . .159
McCulloch, Warren . . . . . . . . . . . . 8 f.
Minsky, Marvin . . . . . . . . . . . . . . . . 9 f.
Miyake . . . . . . . . . . . . . . . . . . . . . . . . . 12
Nilsson, Nils. . . . . . . . . . . . . . . . . . . .10
Papert, Seymour . . . . . . . . . . . . . . . 10
Parker, David . . . . . . . . . . . . . . . . . . 95
Pitts, Walter . . . . . . . . . . . . . . . . . . . 8 f.
Poggio . . . . . . . . . . . . . . . . . . . . . . . . 103
Pythagoras. . . . . . . . . . . . . . . . . . . . .56
Riedmiller, Martin . . . . . . . . . . . . . 90
Rosenblatt, Frank . . . . . . . . . . 10, 69
Rumelhart . . . . . . . . . . . . . . . . . . . . . 12
Steinbuch, Karl . . . . . . . . . . . . . . . . 10
Sutton . . . . . . . . . . . . . . . . . . 191, 206 f.
Tesauro, Gerald . . . . . . . . . . . . . . . 205

von der Malsburg, Christoph . . . 11
Werbos, Paul . . . . . . . . . . . 11, 84, 96
Widrow, Bernard. . . . . . . . . . . . . . .10
Wightman, Charles. . . . . . . . . . . . .10
Williams . . . . . . . . . . . . . . . . . . . . . . . 12
Zuse, Konrad. . . . . . . . . . . . . . . . . . . .9
pinhole eye . . . . . . . . . . . . . . . . . . . . . . . . . 26
PNS . . . . see peripheral nervous system
pole balancer . . . . . . . . . . . . . . . . . . . . . . 206
policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
closed loop . . . . . . . . . . . . . . . . . . . . 197
evaluation. . . . . . . . . . . . . . . . . . . . .200
greedy . . . . . . . . . . . . . . . . . . . . . . . . 197
improvement . . . . . . . . . . . . . . . . . . 200
open loop . . . . . . . . . . . . . . . . . . . . . 197
pons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
propagation function . . . . . . . . . . . . . . . 35
pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
pupil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

Q
Q learning . . . . . . . . . . . . . . . . . . . . . . . . 204
quantization. . . . . . . . . . . . . . . . . . . . . . .137
quickpropagation . . . . . . . . . . . . . . . . . . . 95

R
RBF network. . . . . . . . . . . . . . . . . . . . . .104
growing . . . . . . . . . . . . . . . . . . . . . . . 115
receptive ﬁeld . . . . . . . . . . . . . . . . . . . . . . 27
receptor cell . . . . . . . . . . . . . . . . . . . . . . . . 24
photo-. . . . . . . . . . . . . . . . . . . . . . . . . .27
primary . . . . . . . . . . . . . . . . . . . . . . . . 24
secondary . . . . . . . . . . . . . . . . . . . . . . 24

224

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

dkriesel.com

Index

recurrence . . . . . . . . . . . . . . . . . . . . . 40, 119
direct . . . . . . . . . . . . . . . . . . . . . . . . . . 40
indirect . . . . . . . . . . . . . . . . . . . . . . . . 41
lateral. . . . . . . . . . . . . . . . . . . . . . . . . .42
refractory period . . . . . . . . . . . . . . . . . . . 23
regional and online learnable ﬁelds 175
reinforcement learning. . . . . . . . . . . . .191
repolarization . . . . . . . . . . . . . . . . . . . . . . 21
representability. . . . . . . . . . . . . . . . . . . . .97
resilient backpropagation . . . . . . . . . . . 90
resonance . . . . . . . . . . . . . . . . . . . . . . . . . 166
retina. . . . . . . . . . . . . . . . . . . . . . . . . . .27, 71
return . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
avoidance strategy . . . . . . . . . . . . 199
pure delayed . . . . . . . . . . . . . . . . . . 198
pure negative . . . . . . . . . . . . . . . . . 198
RMS . . . . . . . . . . . . see root mean square
ROLFs. . . . . . . . .see regional and online

learnable ﬁelds

root mean square . . . . . . . . . . . . . . . . . . . 56
Rprop . . . see resilient backpropagation

S
saltatory conductor. . . . . . . . . . . . . . . . .23
Schwann cell . . . . . . . . . . . . . . . . . . . . . . . 23
self-fulﬁlling prophecy . . . . . . . . . . . . . 189
self-organizing feature maps . . . . . . . . 11
self-organizing map. . . . . . . . . . . . . . . .145
multi- . . . . . . . . . . . . . . . . . . . . . . . . . 161
sensory adaptation . . . . . . . . . . . . . . . . . 25
sensory transduction. . . . . . . . . . . . . . . .24
shortcut connections. . . . . . . . . . . . . . . .39
silhouette coeﬃcient. . . . . . . . . . . . . . .175
single lense eye . . . . . . . . . . . . . . . . . . . . . 27
Single Shot Learning . . . . . . . . . . . . . . 130

situation . . . . . . . . . . . . . . . . . . . . . . . . . . 194
situation space . . . . . . . . . . . . . . . . . . . . 195
situation tree. . . . . . . . . . . . . . . . . . . . . .198
SLP . . . . . . . . see perceptron, singlelayer
Snark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
SNIPE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vi
sodium-potassium pump. . . . . . . . . . . .20
SOM . . . . . . . . . . see self-organizing map
soma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
spin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
spinal cord . . . . . . . . . . . . . . . . . . . . . . . . . 14
stability / plasticity dilemma . . . . . . 165
state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
state space forecasting. . . . . . . . . . . . .183
state-value function . . . . . . . . . . . . . . . 200
stimulus . . . . . . . . . . . . . . . . . . . . . . . 21, 147
stimulus-conducting apparatus. . . . . .24
surface, perceptive. . . . . . . . . . . . . . . . .176
swing up an inverted pendulum. . . .206
symmetry breaking . . . . . . . . . . . . . . . . . 98
synapse

chemical . . . . . . . . . . . . . . . . . . . . . . . 17
electrical . . . . . . . . . . . . . . . . . . . . . . . 17
synapses. . . . . . . . . . . . . . . . . . . . . . . . . . . .17
synaptic cleft . . . . . . . . . . . . . . . . . . . . . . . 17

T
target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
TD gammon . . . . . . . . . . . . . . . . . . . . . . 205
TD learning. . . .see temporal diﬀerence

learning

teacher forcing . . . . . . . . . . . . . . . . . . . . 125
teaching input . . . . . . . . . . . . . . . . . . . . . . 53
telencephalon . . . . . . . . . . . . see cerebrum
temporal diﬀerence learning . . . . . . . 202
thalamus . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

225

Index

dkriesel.com

weighted sum. . . . . . . . . . . . . . . . . . . . . . .35
Widrow-Hoﬀ rule . . . . . . . . see delta rule
winner-takes-all scheme. . . . . . . . . . . . .42

threshold potential . . . . . . . . . . . . . . . . . 21
threshold value . . . . . . . . . . . . . . . . . . . . . 36
time concept . . . . . . . . . . . . . . . . . . . . . . . 33
time horizon . . . . . . . . . . . . . . . . . . . . . . 196
time series . . . . . . . . . . . . . . . . . . . . . . . . 181
time series prediction. . . . . . . . . . . . . .181
topological defect. . . . . . . . . . . . . . . . . .154
topology . . . . . . . . . . . . . . . . . . . . . . . . . . 147
topology function . . . . . . . . . . . . . . . . . 148
training pattern . . . . . . . . . . . . . . . . . . . . 53
set of. . . . . . . . . . . . . . . . . . . . . . . . . . .53
training set. . . . . . . . . . . . . . . . . . . . . . . . .50
transfer functionsee activation function
truncus cerebri . . . . . . . . . . see brainstem
two-step-ahead prediction . . . . . . . . . 185
direct . . . . . . . . . . . . . . . . . . . . . . . . . 185

U
unfolding in time . . . . . . . . . . . . . . . . . . 123

V
voronoi diagram . . . . . . . . . . . . . . . . . . . 138

W
weight. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34
weight matrix . . . . . . . . . . . . . . . . . . . . . . 34
bottom-up . . . . . . . . . . . . . . . . . . . . 166
top-down. . . . . . . . . . . . . . . . . . . . . .165
weight vector . . . . . . . . . . . . . . . . . . . . . . . 34

226

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)

