think	O
bayes	O
bayesian	O
statistics	O
made	O
simple	O
version	O
1.0.9	O
think	O
bayes	O
bayesian	O
statistics	O
made	O
simple	O
version	O
1.0.9	O
allen	O
b.	O
downey	O
green	O
tea	O
press	O
needham	O
,	O
massachusetts	O
copyright	O
©	O
2012	O
allen	O
b.	O
downey	O
.	O
green	O
tea	O
press	O
9	O
washburn	O
ave	O
needham	O
ma	O
02492	O
permission	O
is	O
granted	O
to	O
copy	O
,	O
distribute	O
,	O
and/or	O
modify	O
this	O
document	O
under	O
the	O
terms	O
of	O
the	O
creative	O
commons	O
attribution-noncommercial	O
3.0	O
unported	O
license	O
,	O
which	O
is	O
available	O
at	O
http	O
:	O
//creativecommons.org/	O
licenses/by-nc/3.0/	O
.	O
preface	O
0.1	O
my	O
theory	O
,	O
which	O
is	O
mine	O
the	O
premise	O
of	O
this	O
book	O
,	O
and	O
the	O
other	O
books	O
in	O
the	O
think	O
x	O
series	O
,	O
is	O
that	O
if	O
you	O
know	O
how	O
to	O
program	O
,	O
you	O
can	O
use	O
that	O
skill	O
to	O
learn	O
other	O
topics	O
.	O
most	O
books	O
on	O
bayesian	O
statistics	O
use	O
mathematical	O
notation	O
and	O
present	O
ideas	O
in	O
terms	O
of	O
mathematical	O
concepts	O
like	O
calculus	O
.	O
this	O
book	O
uses	O
python	O
code	O
instead	O
of	O
math	O
,	O
and	O
discrete	O
approximations	O
instead	O
of	O
con-	O
tinuous	O
mathematics	O
.	O
as	O
a	O
result	O
,	O
what	O
would	O
be	O
an	O
integral	O
in	O
a	O
math	O
book	O
becomes	O
a	O
summation	O
,	O
and	O
most	O
operations	B
on	O
probability	B
distributions	O
are	O
simple	O
loops	O
.	O
i	O
think	O
this	O
presentation	O
is	O
easier	O
to	O
understand	O
,	O
at	O
least	O
for	O
people	O
with	O
pro-	O
gramming	O
skills	O
.	O
it	O
is	O
also	O
more	O
general	O
,	O
because	O
when	O
we	O
make	O
modeling	B
decisions	O
,	O
we	O
can	O
choose	O
the	O
most	O
appropriate	O
model	O
without	O
worrying	O
too	O
much	O
about	O
whether	O
the	O
model	O
lends	O
itself	O
to	O
conventional	O
analysis	O
.	O
also	O
,	O
it	O
provides	O
a	O
smooth	O
development	O
path	O
from	O
simple	O
examples	O
to	O
real-	O
world	O
problems	O
.	O
chapter	O
3	O
is	O
a	O
good	O
example	O
.	O
it	O
starts	O
with	O
a	O
simple	O
ex-	O
ample	O
involving	O
dice	B
,	O
one	O
of	O
the	O
staples	O
of	O
basic	O
probability	B
.	O
from	O
there	O
it	O
proceeds	O
in	O
small	O
steps	O
to	O
the	O
locomotive	B
problem	I
,	O
which	O
i	O
borrowed	O
from	O
mosteller	O
’	O
s	O
fifty	O
challenging	O
problems	O
in	O
probability	B
with	O
solutions	O
,	O
and	O
from	O
there	O
to	O
the	O
german	O
tank	O
problem	O
,	O
a	O
famously	O
successful	O
application	O
of	O
bayesian	O
methods	O
during	O
world	O
war	O
ii	O
.	O
0.2	O
modeling	B
and	O
approximation	O
most	O
chapters	O
in	O
this	O
book	O
are	O
motivated	O
by	O
a	O
real-world	O
problem	O
,	O
so	O
they	O
involve	O
some	O
degree	O
of	O
modeling	O
.	O
before	O
we	O
can	O
apply	O
bayesian	O
methods	O
(	O
or	O
any	O
other	O
analysis	O
)	O
,	O
we	O
have	O
to	O
make	O
decisions	O
about	O
which	O
parts	O
of	O
the	O
vi	O
chapter	O
0.	O
preface	O
real-world	O
system	O
to	O
include	O
in	O
the	O
model	O
and	O
which	O
details	O
we	O
can	O
abstract	O
away	O
.	O
for	O
example	O
,	O
in	O
chapter	O
7	O
,	O
the	O
motivating	O
problem	O
is	O
to	O
predict	O
the	O
winner	O
of	O
a	O
hockey	B
game	O
.	O
i	O
model	O
goal-scoring	O
as	O
a	O
poisson	O
process	B
,	O
which	O
implies	O
that	O
a	O
goal	O
is	O
equally	O
likely	O
at	O
any	O
point	O
in	O
the	O
game	O
.	O
that	O
is	O
not	O
exactly	O
true	O
,	O
but	O
it	O
is	O
probably	O
a	O
good	O
enough	O
model	O
for	O
most	O
purposes	O
.	O
in	O
chapter	O
12	O
the	O
motivating	O
problem	O
is	O
interpreting	O
sat	O
scores	O
(	O
the	O
sat	O
is	O
a	O
standardized	B
test	I
used	O
for	O
college	O
admissions	O
in	O
the	O
united	O
states	O
)	O
.	O
i	O
start	O
with	O
a	O
simple	O
model	O
that	O
assumes	O
that	O
all	O
sat	O
questions	O
are	O
equally	O
difﬁ-	O
cult	O
,	O
but	O
in	O
fact	O
the	O
designers	O
of	O
the	O
sat	O
deliberately	O
include	O
some	O
questions	O
that	O
are	O
relatively	O
easy	O
and	O
some	O
that	O
are	O
relatively	O
hard	O
.	O
i	O
present	O
a	O
second	O
model	O
that	O
accounts	O
for	O
this	O
aspect	O
of	O
the	O
design	O
,	O
and	O
show	O
that	O
it	O
doesn	O
’	O
t	O
have	O
a	O
big	O
effect	O
on	O
the	O
results	O
after	O
all	O
.	O
i	O
think	O
it	O
is	O
important	O
to	O
include	O
modeling	B
as	O
an	O
explicit	O
part	O
of	O
problem	O
solving	O
because	O
it	O
reminds	O
us	O
to	O
think	O
about	O
modeling	B
errors	O
(	O
that	O
is	O
,	O
errors	O
due	O
to	O
simpliﬁcations	O
and	O
assumptions	O
of	O
the	O
model	O
)	O
.	O
many	O
of	O
the	O
methods	O
in	O
this	O
book	O
are	O
based	O
on	O
discrete	O
distributions	O
,	O
which	O
makes	O
some	O
people	O
worry	O
about	O
numerical	O
errors	O
.	O
but	O
for	O
real-world	O
prob-	O
lems	O
,	O
numerical	O
errors	O
are	O
almost	O
always	O
smaller	O
than	O
modeling	B
errors	O
.	O
furthermore	O
,	O
the	O
discrete	O
approach	O
often	O
allows	O
better	O
modeling	B
decisions	O
,	O
and	O
i	O
would	O
rather	O
have	O
an	O
approximate	O
solution	O
to	O
a	O
good	O
model	O
than	O
an	O
exact	O
solution	O
to	O
a	O
bad	O
model	O
.	O
on	O
the	O
other	O
hand	O
,	O
continuous	O
methods	O
sometimes	O
yield	O
performance	O
advantages—for	O
example	O
by	O
replacing	O
a	O
linear-	O
or	O
quadratic-time	O
compu-	O
tation	O
with	O
a	O
constant-time	O
solution	O
.	O
so	O
i	O
recommend	O
a	O
general	O
process	B
with	O
these	O
steps	O
:	O
1.	O
while	O
you	O
are	O
exploring	O
a	O
problem	O
,	O
start	O
with	O
simple	O
models	O
and	O
im-	O
plement	O
them	O
in	O
code	O
that	O
is	O
clear	O
,	O
readable	O
,	O
and	O
demonstrably	O
correct	O
.	O
focus	O
your	O
attention	O
on	O
good	O
modeling	B
decisions	O
,	O
not	O
optimization	B
.	O
2.	O
once	O
you	O
have	O
a	O
simple	O
model	O
working	O
,	O
identify	O
the	O
biggest	O
sources	O
of	O
error	B
.	O
you	O
might	O
need	O
to	O
increase	O
the	O
number	O
of	O
values	O
in	O
a	O
discrete	O
approximation	O
,	O
or	O
increase	O
the	O
number	O
of	O
iterations	O
in	O
a	O
monte	O
carlo	O
simulation	B
,	O
or	O
add	O
details	O
to	O
the	O
model	O
.	O
3.	O
if	O
the	O
performance	O
of	O
your	O
solution	O
is	O
good	O
enough	O
for	O
your	O
applica-	O
tion	O
,	O
you	O
might	O
not	O
have	O
to	O
do	O
any	O
optimization	B
.	O
but	O
if	O
you	O
do	O
,	O
there	O
are	O
two	O
approaches	O
to	O
consider	O
.	O
you	O
can	O
review	O
your	O
code	O
and	O
look	O
0.3.	O
working	O
with	O
the	O
code	O
vii	O
for	O
optimizations	O
;	O
for	O
example	O
,	O
if	O
you	O
cache	B
previously	O
computed	O
re-	O
sults	O
you	O
might	O
be	O
able	O
to	O
avoid	O
redundant	O
computation	O
.	O
or	O
you	O
can	O
look	O
for	O
analytic	O
methods	O
that	O
yield	O
computational	O
shortcuts	O
.	O
one	O
beneﬁt	O
of	O
this	O
process	B
is	O
that	O
steps	O
1	O
and	O
2	O
tend	O
to	O
be	O
fast	O
,	O
so	O
you	O
can	O
explore	O
several	O
alternative	O
models	O
before	O
investing	O
heavily	O
in	O
any	O
of	O
them	O
.	O
another	O
beneﬁt	O
is	O
that	O
if	O
you	O
get	O
to	O
step	O
3	O
,	O
you	O
will	O
be	O
starting	O
with	O
a	O
ref-	O
erence	O
implementation	B
that	O
is	O
likely	O
to	O
be	O
correct	O
,	O
which	O
you	O
can	O
use	O
for	O
regression	B
testing	I
(	O
that	O
is	O
,	O
checking	O
that	O
the	O
optimized	O
code	O
yields	O
the	O
same	O
results	O
,	O
at	O
least	O
approximately	O
)	O
.	O
0.3	O
working	O
with	O
the	O
code	O
the	O
code	O
and	O
sound	O
samples	O
used	O
in	O
this	O
book	O
are	O
available	O
from	O
https	O
:	O
//	O
github.com/allendowney/thinkbayes	O
.	O
git	O
is	O
a	O
version	O
control	O
system	O
that	O
allows	O
you	O
to	O
keep	O
track	O
of	O
the	O
ﬁles	O
that	O
make	O
up	O
a	O
project	O
.	O
a	O
collection	O
of	O
ﬁles	O
under	O
git	O
’	O
s	O
control	O
is	O
called	O
a	O
“	O
repository	B
”	O
.	O
github	O
is	O
a	O
hosting	O
service	O
that	O
provides	O
storage	O
for	O
git	O
repositories	O
and	O
a	O
convenient	O
web	O
interface	B
.	O
the	O
github	O
homepage	O
for	O
my	O
repository	B
provides	O
several	O
ways	O
to	O
work	O
with	O
the	O
code	O
:	O
•	O
you	O
can	O
create	O
a	O
copy	O
of	O
my	O
repository	B
on	O
github	O
by	O
pressing	O
the	O
fork	B
button	O
.	O
if	O
you	O
don	O
’	O
t	O
already	O
have	O
a	O
github	O
account	O
,	O
you	O
’	O
ll	O
need	O
to	O
create	O
one	O
.	O
after	O
forking	O
,	O
you	O
’	O
ll	O
have	O
your	O
own	O
repository	B
on	O
github	O
that	O
you	O
can	O
use	O
to	O
keep	O
track	O
of	O
code	O
you	O
write	O
while	O
working	O
on	O
this	O
book	O
.	O
then	O
you	O
can	O
clone	B
the	O
repo	O
,	O
which	O
means	O
that	O
you	O
copy	O
the	O
ﬁles	O
to	O
your	O
computer	O
.	O
•	O
or	O
you	O
could	O
clone	B
my	O
repository	B
.	O
you	O
don	O
’	O
t	O
need	O
a	O
github	O
account	O
to	O
do	O
this	O
,	O
but	O
you	O
won	O
’	O
t	O
be	O
able	O
to	O
write	O
your	O
changes	O
back	O
to	O
github	O
.	O
•	O
if	O
you	O
don	O
’	O
t	O
want	O
to	O
use	O
git	O
at	O
all	O
,	O
you	O
can	O
download	O
the	O
ﬁles	O
in	O
a	O
zip	O
ﬁle	O
using	O
the	O
button	O
in	O
the	O
lower-right	O
corner	O
of	O
the	O
github	O
page	O
.	O
the	O
code	O
for	O
the	O
ﬁrst	O
edition	O
of	O
the	O
book	O
works	O
with	O
python	O
2.	O
if	O
you	O
are	O
using	O
python	O
3	O
,	O
you	O
might	O
want	O
to	O
use	O
the	O
updated	O
code	O
in	O
https	O
:	O
//github.com/allendowney/thinkbayes2	O
instead	O
.	O
i	O
developed	O
this	O
book	O
using	O
anaconda	O
from	O
continuum	O
analytics	O
,	O
which	O
is	O
a	O
free	O
python	O
distribution	B
that	O
includes	O
all	O
the	O
packages	O
you	O
’	O
ll	O
need	O
to	O
viii	O
chapter	O
0.	O
preface	O
run	O
the	O
code	O
(	O
and	O
lots	O
more	O
)	O
.	O
i	O
found	O
anaconda	O
easy	O
to	O
install	O
.	O
by	O
default	O
it	O
does	O
a	O
user-level	O
installation	B
,	O
not	O
system-level	O
,	O
so	O
you	O
don	O
’	O
t	O
need	O
admin-	O
istrative	O
privileges	O
.	O
you	O
can	O
download	O
anaconda	O
from	O
http	O
:	O
//continuum	O
.	O
io/downloads	O
.	O
if	O
you	O
don	O
’	O
t	O
want	O
to	O
use	O
anaconda	O
,	O
you	O
will	O
need	O
the	O
following	O
packages	O
:	O
•	O
numpy	B
for	O
basic	O
numerical	O
computation	O
,	O
http	O
:	O
//www.numpy.org/	O
;	O
•	O
scipy	B
for	O
scientiﬁc	O
computation	O
,	O
http	O
:	O
//www.scipy.org/	O
;	O
•	O
matplotlib	B
for	O
visualization	O
,	O
http	O
:	O
//matplotlib.org/	O
.	O
although	O
these	O
are	O
commonly	O
used	O
packages	O
,	O
they	O
are	O
not	O
included	O
with	O
all	O
python	O
installations	O
,	O
and	O
they	O
can	O
be	O
hard	O
to	O
install	O
in	O
some	O
environments	O
.	O
if	O
you	O
have	O
trouble	O
installing	O
them	O
,	O
i	O
recommend	O
using	O
anaconda	O
or	O
one	O
of	O
the	O
other	O
python	O
distributions	O
that	O
include	O
these	O
packages	O
.	O
many	O
of	O
the	O
examples	O
in	O
this	O
book	O
use	O
classes	O
and	O
functions	O
deﬁned	O
in	O
thinkbayes.py	O
.	O
some	O
of	O
them	O
also	O
use	O
thinkplot.py	O
,	O
which	O
provides	O
wrappers	O
for	O
some	O
of	O
the	O
functions	O
in	O
pyplot	O
,	O
which	O
is	O
part	O
of	O
matplotlib	B
.	O
0.4	O
code	O
style	O
experienced	O
python	O
programmers	O
will	O
notice	O
that	O
the	O
code	O
in	O
this	O
book	O
does	O
not	O
comply	O
with	O
pep	O
8	O
,	O
which	O
is	O
the	O
most	O
common	O
style	O
guide	O
for	O
python	O
(	O
http	O
:	O
//www.python.org/dev/peps/pep-0008/	O
)	O
.	O
speciﬁcally	O
,	O
pep	O
8	O
calls	O
for	O
lowercase	O
function	O
names	O
with	O
underscores	O
be-	O
tween	O
words	O
,	O
like_this	O
.	O
in	O
this	O
book	O
and	O
the	O
accompanying	O
code	O
,	O
function	O
and	O
method	O
names	O
begin	O
with	O
a	O
capital	O
letter	O
and	O
use	O
camel	O
case	O
,	O
likethis	O
.	O
i	O
broke	O
this	O
rule	O
because	O
i	O
developed	O
some	O
of	O
the	O
code	O
while	O
i	O
was	O
a	O
visiting	O
scientist	O
at	O
google	O
,	O
so	O
i	O
followed	O
the	O
google	O
style	O
guide	O
,	O
which	O
deviates	O
from	O
pep	O
8	O
in	O
a	O
few	O
places	O
.	O
once	O
i	O
got	O
used	O
to	O
google	O
style	O
,	O
i	O
found	O
that	O
i	O
liked	O
it	O
.	O
and	O
at	O
this	O
point	O
,	O
it	O
would	O
be	O
too	O
much	O
trouble	O
to	O
change	O
.	O
also	O
on	O
the	O
topic	O
of	O
style	O
,	O
i	O
write	O
“	O
bayes	O
’	O
s	O
theorem	O
”	O
with	O
an	O
s	O
after	O
the	O
apos-	O
trophe	O
,	O
which	O
is	O
preferred	O
in	O
some	O
style	O
guides	O
and	O
deprecated	O
in	O
others	O
.	O
i	O
don	O
’	O
t	O
have	O
a	O
strong	O
preference	O
.	O
i	O
had	O
to	O
choose	O
one	O
,	O
and	O
this	O
is	O
the	O
one	O
i	O
chose	O
.	O
and	O
ﬁnally	O
one	O
typographical	O
note	O
:	O
throughout	O
the	O
book	O
,	O
i	O
use	O
pmf	O
and	O
cdf	O
for	O
the	O
mathematical	O
concept	O
of	O
a	O
probability	B
mass	I
function	I
or	O
cumu-	O
lative	O
distribution	B
function	O
,	O
and	O
pmf	O
and	O
cdf	O
to	O
refer	O
to	O
the	O
python	O
objects	O
i	O
use	O
to	O
represent	O
them	O
.	O
0.5.	O
prerequisites	O
0.5	O
prerequisites	O
ix	O
there	O
are	O
several	O
excellent	O
modules	O
for	O
doing	O
bayesian	O
statistics	O
in	O
python	O
,	O
including	O
pymc	O
and	O
openbugs	O
.	O
i	O
chose	O
not	O
to	O
use	O
them	O
for	O
this	O
book	O
be-	O
cause	O
you	O
need	O
a	O
fair	O
amount	O
of	O
background	O
knowledge	O
to	O
get	O
started	O
with	O
these	O
modules	O
,	O
and	O
i	O
want	O
to	O
keep	O
the	O
prerequisites	O
minimal	O
.	O
if	O
you	O
know	O
python	O
and	O
a	O
little	O
bit	O
about	O
probability	B
,	O
you	O
are	O
ready	O
to	O
start	O
this	O
book	O
.	O
chapter	O
1	O
is	O
about	O
probability	B
and	O
bayes	O
’	O
s	O
theorem	O
;	O
it	O
has	O
no	O
code	O
.	O
chap-	O
ter	O
2	O
introduces	O
pmf	O
,	O
a	O
thinly	O
disguised	O
python	O
dictionary	O
i	O
use	O
to	O
represent	O
a	O
probability	B
mass	I
function	I
(	O
pmf	O
)	O
.	O
then	O
chapter	O
3	O
introduces	O
suite	B
,	O
a	O
kind	O
of	O
pmf	O
that	O
provides	O
a	O
framework	O
for	O
doing	O
bayesian	O
updates	O
.	O
in	O
some	O
of	O
the	O
later	O
chapters	O
,	O
i	O
use	O
analytic	O
distributions	O
including	O
the	O
gaus-	O
sian	O
(	O
normal	O
)	O
distribution	B
,	O
the	O
exponential	O
and	O
poisson	O
distributions	O
,	O
and	O
the	O
beta	B
distribution	I
.	O
in	O
chapter	O
15	O
i	O
break	O
out	O
the	O
less-common	O
dirichlet	O
distribution	B
,	O
but	O
i	O
explain	O
it	O
as	O
i	O
go	O
along	O
.	O
if	O
you	O
are	O
not	O
familiar	O
with	O
these	O
distributions	O
,	O
you	O
can	O
read	O
about	O
them	O
on	O
wikipedia	O
.	O
you	O
could	O
also	O
read	O
the	O
companion	O
to	O
this	O
book	O
,	O
think	O
stats	O
,	O
or	O
an	O
introductory	O
statistics	O
book	O
(	O
although	O
i	O
’	O
m	O
afraid	O
most	O
of	O
them	O
take	O
a	O
mathematical	O
approach	O
that	O
is	O
not	O
particularly	O
helpful	O
for	O
practical	O
purposes	O
)	O
.	O
contributor	O
list	O
if	O
you	O
have	O
a	O
suggestion	O
or	O
downey	O
@	O
allendowney.com	O
.	O
i	O
will	O
add	O
you	O
to	O
the	O
contributor	O
list	O
(	O
unless	O
you	O
ask	O
to	O
be	O
omitted	O
)	O
.	O
to	O
if	O
i	O
make	O
a	O
change	O
based	O
on	O
your	O
feedback	O
,	O
correction	O
,	O
please	O
send	O
email	O
if	O
you	O
include	O
at	O
least	O
part	O
of	O
the	O
sentence	O
the	O
error	B
appears	O
in	O
,	O
that	O
makes	O
it	O
easy	O
for	O
me	O
to	O
search	O
.	O
page	O
and	O
section	O
numbers	O
are	O
ﬁne	O
,	O
too	O
,	O
but	O
not	O
as	O
easy	O
to	O
work	O
with	O
.	O
thanks	O
!	O
•	O
first	O
,	O
i	O
have	O
to	O
acknowledge	O
david	O
mackay	O
’	O
s	O
excellent	O
book	O
,	O
information	O
the-	O
ory	O
,	O
inference	O
,	O
and	O
learning	O
algorithms	O
,	O
which	O
is	O
where	O
i	O
ﬁrst	O
came	O
to	O
under-	O
stand	O
bayesian	O
methods	O
.	O
with	O
his	O
permission	O
,	O
i	O
use	O
several	O
problems	O
from	O
his	O
book	O
as	O
examples	O
.	O
•	O
this	O
book	O
also	O
beneﬁted	O
from	O
my	O
interactions	O
with	O
sanjoy	O
mahajan	O
,	O
espe-	O
cially	O
in	O
fall	O
2012	O
,	O
when	O
i	O
audited	O
his	O
class	O
on	O
bayesian	O
inference	O
at	O
olin	O
college	O
.	O
•	O
i	O
wrote	O
parts	O
of	O
this	O
book	O
during	O
project	O
nights	O
with	O
the	O
boston	O
python	O
user	O
group	O
,	O
so	O
i	O
would	O
like	O
to	O
thank	O
them	O
for	O
their	O
company	O
and	O
pizza	O
.	O
x	O
chapter	O
0.	O
preface	O
•	O
olivier	O
yiptong	O
sent	O
several	O
helpful	O
suggestions	O
.	O
•	O
yuriy	O
pasichnyk	O
found	O
several	O
errors	O
.	O
•	O
kristopher	O
overholt	O
sent	O
a	O
long	O
list	O
of	O
corrections	O
and	O
suggestions	O
.	O
•	O
max	O
hailperin	O
suggested	O
a	O
clariﬁcation	O
in	O
chapter	O
1	O
.	O
•	O
markus	O
dobler	O
pointed	O
out	O
that	O
drawing	O
cookies	O
from	O
a	O
bowl	O
with	O
replace-	O
ment	O
is	O
an	O
unrealistic	O
scenario	O
.	O
•	O
in	O
spring	O
2013	O
,	O
students	O
in	O
my	O
class	O
,	O
computational	O
bayesian	O
statistics	O
,	O
made	O
many	O
helpful	O
corrections	O
and	O
suggestions	O
:	O
kai	O
austin	O
,	O
claire	O
barnes	O
,	O
kari	O
bender	O
,	O
rachel	O
boy	O
,	O
kat	O
mendoza	O
,	O
arjun	O
iyer	O
,	O
ben	O
kroop	O
,	O
nathan	O
lintz	O
,	O
kyle	O
mcconnaughay	O
,	O
alec	O
radford	O
,	O
brendan	O
ritter	O
,	O
and	O
evan	O
simpson	O
.	O
•	O
greg	O
marra	O
and	O
matt	O
aasted	O
helped	O
me	O
clarify	O
the	O
discussion	O
of	O
the	O
price	O
is	O
right	O
problem	O
.	O
•	O
marcus	O
ogren	O
pointed	O
out	O
that	O
the	O
original	O
statement	O
of	O
the	O
locomotive	O
prob-	O
lem	O
was	O
ambiguous	O
.	O
•	O
jasmine	O
kwityn	O
and	O
dan	O
fauxsmith	O
at	O
o	O
’	O
reilly	O
media	O
proofread	O
the	O
book	O
and	O
found	O
many	O
opportunities	O
for	O
improvement	O
.	O
•	O
linda	O
pescatore	O
found	O
a	O
typo	O
and	O
made	O
some	O
helpful	O
suggestions	O
.	O
•	O
tomasz	O
mi	O
˛asko	O
sent	O
many	O
excellent	O
corrections	O
and	O
suggestions	O
.	O
other	O
people	O
who	O
spotted	O
typos	O
and	O
small	O
errors	O
include	O
tom	O
pollard	O
,	O
paul	O
a.	O
giannaros	O
,	O
jonathan	O
edwards	O
,	O
george	O
purkins	O
,	O
robert	O
marcus	O
,	O
ram	O
limbu	O
,	O
james	O
lawry	O
,	O
ben	O
kahle	O
,	O
jeffrey	O
law	O
,	O
and	O
alvaro	O
sanchez	O
.	O
contents	O
preface	O
0.1	O
my	O
theory	O
,	O
which	O
is	O
mine	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
0.2	O
modeling	B
and	O
approximation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
v	O
v	O
v	O
0.3	O
working	O
with	O
the	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
vii	O
0.4	O
0.5	O
code	O
style	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
viii	O
prerequisites	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
ix	O
1	O
bayes	O
’	O
s	O
theorem	O
1	O
1	O
2	O
3	O
3	O
5	O
6	O
8	O
1.1	O
1.2	O
1.3	O
1.4	O
1.5	O
1.6	O
1.7	O
1.8	O
conditional	B
probability	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
conjoint	B
probability	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
the	O
cookie	B
problem	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
bayes	O
’	O
s	O
theorem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
the	O
diachronic	B
interpretation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
the	O
m	O
&	O
m	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
the	O
monty	O
hall	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10	O
2	O
computational	O
statistics	O
11	O
2.1	O
2.2	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11	O
the	O
cookie	B
problem	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12	O
xii	O
2.3	O
2.4	O
2.5	O
2.6	O
2.7	O
2.8	O
contents	O
the	O
bayesian	O
framework	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
13	O
the	O
monty	O
hall	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
15	O
encapsulating	O
the	O
framework	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
16	O
the	O
m	O
&	O
m	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
17	O
discussion	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
18	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
19	O
3	O
estimation	O
21	O
3.1	O
3.2	O
the	O
dice	B
problem	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
21	O
the	O
locomotive	B
problem	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
22	O
3.3	O
what	O
about	O
that	O
prior	B
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
25	O
3.4	O
3.5	O
3.6	O
3.7	O
3.8	O
3.9	O
an	O
alternative	O
prior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
25	O
credible	O
intervals	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
27	O
cumulative	O
distribution	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
28	O
the	O
german	O
tank	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
29	O
discussion	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
30	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
30	O
4	O
more	O
estimation	O
33	O
4.1	O
4.2	O
4.3	O
4.4	O
4.5	O
4.6	O
4.7	O
the	O
euro	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
33	O
summarizing	O
the	O
posterior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
35	O
swamping	B
the	I
priors	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
36	O
optimization	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
37	O
the	O
beta	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
38	O
discussion	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
40	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
41	O
contents	O
5	O
odds	B
and	O
addends	O
xiii	O
43	O
5.1	O
5.2	O
5.3	O
5.4	O
odds	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
43	O
the	O
odds	B
form	I
of	O
bayes	O
’	O
s	O
theorem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
oliver	O
’	O
s	O
blood	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
45	O
addends	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
46	O
5.5	O
maxima	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
49	O
5.6	O
mixtures	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
51	O
5.7	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
54	O
6	O
decision	B
analysis	I
55	O
the	O
price	O
is	O
right	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
55	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
56	O
the	O
prior	B
.	O
6.1	O
6.2	O
6.3	O
6.4	O
probability	B
density	I
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
57	O
representing	O
pdfs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
57	O
6.5	O
modeling	B
the	O
contestants	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
60	O
6.6	O
6.7	O
6.8	O
6.9	O
likelihood	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
62	O
update	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
63	O
optimal	O
bidding	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
64	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
67	O
7	O
prediction	O
69	O
7.1	O
7.2	O
7.3	O
7.4	O
7.5	O
7.6	O
7.7	O
7.8	O
the	O
boston	O
bruins	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
69	O
poisson	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
71	O
the	O
posteriors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
71	O
the	O
distribution	B
of	O
goals	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
72	O
the	O
probability	B
of	O
winning	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
74	O
sudden	B
death	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
75	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
76	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
78	O
xiv	O
8	O
observer	B
bias	I
contents	O
81	O
the	O
red	O
line	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
81	O
the	O
model	O
.	O
.	O
8.3	O
wait	O
times	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
82	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
84	O
predicting	O
wait	O
times	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
86	O
estimating	O
the	O
arrival	B
rate	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
89	O
incorporating	O
uncertainty	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
91	O
8.1	O
8.2	O
8.4	O
8.5	O
8.6	O
8.7	O
8.8	O
8.9	O
9.1	O
9.2	O
9.3	O
9.4	O
9.5	O
9.6	O
9.7	O
9.8	O
9.9	O
9	O
two	O
dimensions	O
decision	B
analysis	I
discussion	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
paintball	O
.	O
.	O
.	O
the	O
suite	B
.	O
.	O
.	O
.	O
.	O
trigonometry	B
.	O
likelihood	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
92	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
95	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
95	O
97	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
97	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
98	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
99	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
101	O
joint	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
102	O
conditional	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
103	O
credible	O
intervals	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
104	O
discussion	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
106	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
107	O
10	O
approximate	O
bayesian	O
computation	O
109	O
10.1	O
the	O
variability	O
hypothesis	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
109	O
10.2	O
mean	O
and	O
standard	O
deviation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
110	O
10.3	O
update	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
112	O
10.4	O
the	O
posterior	B
distribution	I
of	O
cv	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
113	O
contents	O
xv	O
10.5	O
underﬂow	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
114	O
10.6	O
log-likelihood	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
115	O
10.7	O
a	O
little	O
optimization	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
116	O
10.8	O
abc	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
118	O
10.9	O
robust	B
estimation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
119	O
10.10	O
who	O
is	O
more	O
variable	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
122	O
10.11	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
123	O
10.12	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
124	O
11	O
hypothesis	B
testing	I
125	O
11.1	O
back	O
to	O
the	O
euro	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
125	O
11.2	O
making	O
a	O
fair	O
comparison	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
126	O
11.3	O
the	O
triangle	O
prior	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
128	O
11.4	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
129	O
11.5	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
129	O
12	O
evidence	B
131	O
12.1	O
12.2	O
12.3	O
12.4	O
interpreting	O
sat	O
scores	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
the	O
scale	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
132	O
the	O
prior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
132	O
posterior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
134	O
12.5	O
a	O
better	O
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
136	O
12.6	O
calibration	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
138	O
12.7	O
12.8	O
posterior	B
distribution	I
of	O
efﬁcacy	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
139	O
predictive	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
141	O
12.9	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
142	O
xvi	O
13	O
simulation	B
contents	O
145	O
13.1	O
the	O
kidney	O
tumor	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
145	O
13.2	O
a	O
simple	O
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
146	O
13.3	O
a	O
more	O
general	O
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
148	O
13.4	O
implementation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
150	O
13.5	O
caching	O
the	O
joint	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
151	O
13.6	O
conditional	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
152	O
13.7	O
serial	B
correlation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
154	O
13.8	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
157	O
14	O
a	O
hierarchical	B
model	I
159	O
14.1	O
14.2	O
the	O
geiger	O
counter	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
159	O
start	O
simple	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
160	O
14.3	O
make	O
it	O
hierarchical	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
161	O
14.4	O
a	O
little	O
optimization	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
163	O
14.5	O
extracting	O
the	O
posteriors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
163	O
14.6	O
discussion	O
.	O
.	O
.	O
14.7	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
164	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
165	O
15	O
dealing	O
with	O
dimensions	O
167	O
15.1	O
15.2	O
15.3	O
belly	B
button	I
bacteria	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
167	O
lions	B
and	I
tigers	I
and	I
bears	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
168	O
the	O
hierarchical	O
version	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
170	O
15.4	O
random	O
sampling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
172	O
15.5	O
optimization	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
174	O
15.6	O
collapsing	O
the	O
hierarchy	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
175	O
15.7	O
one	O
more	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
177	O
contents	O
xvii	O
15.8	O
we	O
’	O
re	O
not	O
done	O
yet	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
179	O
15.9	O
the	O
belly	B
button	I
data	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
181	O
15.10	O
predictive	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
184	O
15.11	O
joint	O
posterior	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
187	O
15.12	O
coverage	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
188	O
15.13	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
190	O
xviii	O
contents	O
chapter	O
1	O
bayes	O
’	O
s	O
theorem	O
1.1	O
conditional	B
probability	I
the	O
fundamental	O
idea	O
behind	O
all	O
bayesian	O
statistics	O
is	O
bayes	O
’	O
s	O
theorem	O
,	O
which	O
is	O
surprisingly	O
easy	O
to	O
derive	O
,	O
provided	O
that	O
you	O
understand	O
con-	O
ditional	O
probability	B
.	O
so	O
we	O
’	O
ll	O
start	O
with	O
probability	B
,	O
then	O
conditional	B
proba-	O
bility	O
,	O
then	O
bayes	O
’	O
s	O
theorem	O
,	O
and	O
on	O
to	O
bayesian	O
statistics	O
.	O
a	O
probability	B
is	O
a	O
number	O
between	O
0	O
and	O
1	O
(	O
including	O
both	O
)	O
that	O
represents	O
a	O
degree	B
of	I
belief	I
in	O
a	O
fact	O
or	O
prediction	O
.	O
the	O
value	O
1	O
represents	O
certainty	O
that	O
a	O
fact	O
is	O
true	O
,	O
or	O
that	O
a	O
prediction	O
will	O
come	O
true	O
.	O
the	O
value	O
0	O
represents	O
certainty	O
that	O
the	O
fact	O
is	O
false	O
.	O
intermediate	O
values	O
represent	O
degrees	O
of	O
certainty	O
.	O
the	O
value	O
0.5	O
,	O
often	O
writ-	O
ten	O
as	O
50	O
%	O
,	O
means	O
that	O
a	O
predicted	O
outcome	O
is	O
as	O
likely	O
to	O
happen	O
as	O
not	O
.	O
for	O
example	O
,	O
the	O
probability	B
that	O
a	O
tossed	O
coin	O
lands	O
face	O
up	O
is	O
very	O
close	O
to	O
50	O
%	O
.	O
a	O
conditional	B
probability	I
is	O
a	O
probability	B
based	O
on	O
some	O
background	O
in-	O
formation	O
.	O
for	O
example	O
,	O
i	O
want	O
to	O
know	O
the	O
probability	B
that	O
i	O
will	O
have	O
a	O
heart	B
attack	I
in	O
the	O
next	O
year	O
.	O
according	O
to	O
the	O
cdc	O
,	O
“	O
every	O
year	O
about	O
785,000	O
americans	O
have	O
a	O
ﬁrst	O
coronary	O
attack	O
.	O
(	O
http	O
:	O
//www.cdc.gov/	O
heartdisease/facts.htm	O
)	O
”	O
the	O
u.s.	O
population	O
is	O
about	O
311	O
million	O
,	O
so	O
the	O
probability	B
that	O
a	O
randomly	O
chosen	O
american	O
will	O
have	O
a	O
heart	B
attack	I
in	O
the	O
next	O
year	O
is	O
roughly	O
0.3	O
%	O
.	O
but	O
i	O
am	O
not	O
a	O
randomly	O
chosen	O
american	O
.	O
epidemiologists	O
have	O
identiﬁed	O
many	O
factors	O
that	O
affect	O
the	O
risk	O
of	O
heart	O
attacks	O
;	O
depending	O
on	O
those	O
factors	O
,	O
my	O
risk	O
might	O
be	O
higher	O
or	O
lower	O
than	O
average	O
.	O
2	O
chapter	O
1.	O
bayes	O
’	O
s	O
theorem	O
i	O
am	O
male	O
,	O
45	O
years	O
old	O
,	O
and	O
i	O
have	O
borderline	O
high	O
cholesterol	O
.	O
those	O
fac-	O
tors	O
increase	O
my	O
chances	O
.	O
however	O
,	O
i	O
have	O
low	O
blood	O
pressure	O
and	O
i	O
don	O
’	O
t	O
smoke	O
,	O
and	O
those	O
factors	O
decrease	O
my	O
chances	O
.	O
plugging	O
everything	O
into	O
the	O
online	O
calculator	O
at	O
http	O
:	O
//cvdrisk.nhlbi	O
.	O
nih.gov/calculator.asp	O
,	O
i	O
ﬁnd	O
that	O
my	O
risk	O
of	O
a	O
heart	B
attack	I
in	O
the	O
next	O
year	O
is	O
about	O
0.2	O
%	O
,	O
less	O
than	O
the	O
national	O
average	O
.	O
that	O
value	O
is	O
a	O
conditional	B
probability	I
,	O
because	O
it	O
is	O
based	O
on	O
a	O
number	O
of	O
factors	O
that	O
make	O
up	O
my	O
“	O
condition.	O
”	O
the	O
usual	O
notation	O
for	O
conditional	B
probability	I
is	O
p	O
(	O
a|b	O
)	O
,	O
which	O
is	O
the	O
prob-	O
ability	O
of	O
a	O
given	O
that	O
b	O
is	O
true	O
.	O
in	O
this	O
example	O
,	O
a	O
represents	O
the	O
prediction	O
that	O
i	O
will	O
have	O
a	O
heart	B
attack	I
in	O
the	O
next	O
year	O
,	O
and	O
b	O
is	O
the	O
set	O
of	O
conditions	O
i	O
listed	O
.	O
1.2	O
conjoint	B
probability	I
conjoint	O
probability	B
is	O
a	O
fancy	O
way	O
to	O
say	O
the	O
probability	B
that	O
two	O
things	O
are	O
true	O
.	O
i	O
write	O
p	O
(	O
a	O
and	O
b	O
)	O
to	O
mean	O
the	O
probability	B
that	O
a	O
and	O
b	O
are	O
both	O
true	O
.	O
if	O
you	O
learned	O
about	O
probability	B
in	O
the	O
context	O
of	O
coin	O
tosses	O
and	O
dice	B
,	O
you	O
might	O
have	O
learned	O
the	O
following	O
formula	O
:	O
p	O
(	O
a	O
and	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
warning	O
:	O
not	O
always	O
true	O
for	O
example	O
,	O
if	O
i	O
toss	O
two	O
coins	O
,	O
and	O
a	O
means	O
the	O
ﬁrst	O
coin	O
lands	O
face	O
up	O
,	O
and	O
b	O
means	O
the	O
second	O
coin	O
lands	O
face	O
up	O
,	O
then	O
p	O
(	O
a	O
)	O
=	O
p	O
(	O
b	O
)	O
=	O
0.5	O
,	O
and	O
sure	O
enough	O
,	O
p	O
(	O
a	O
and	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
=	O
0.25.	O
but	O
this	O
formula	O
only	O
works	O
because	O
in	O
this	O
case	O
a	O
and	O
b	O
are	O
independent	O
;	O
that	O
is	O
,	O
knowing	O
the	O
outcome	O
of	O
the	O
ﬁrst	O
event	O
does	O
not	O
change	O
the	O
proba-	O
bility	O
of	O
the	O
second	O
.	O
or	O
,	O
more	O
formally	O
,	O
p	O
(	O
b|a	O
)	O
=	O
p	O
(	O
b	O
)	O
.	O
here	O
is	O
a	O
different	O
example	O
where	O
the	O
events	O
are	O
not	O
independent	O
.	O
suppose	O
that	O
a	O
means	O
that	O
it	O
rains	O
today	O
and	O
b	O
means	O
that	O
it	O
rains	O
tomorrow	O
.	O
if	O
i	O
know	O
that	O
it	O
rained	O
today	O
,	O
it	O
is	O
more	O
likely	O
that	O
it	O
will	O
rain	O
tomorrow	O
,	O
so	O
p	O
(	O
b|a	O
)	O
>	O
p	O
(	O
b	O
)	O
.	O
in	O
general	O
,	O
the	O
probability	B
of	O
a	O
conjunction	B
is	O
p	O
(	O
a	O
and	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
1.3.	O
the	O
cookie	B
problem	I
3	O
for	O
any	O
a	O
and	O
b.	O
so	O
if	O
the	O
chance	O
of	O
rain	O
on	O
any	O
given	O
day	O
is	O
0.5	O
,	O
the	O
chance	O
of	O
rain	O
on	O
two	O
consecutive	O
days	O
is	O
not	O
0.25	O
,	O
but	O
probably	O
a	O
bit	O
higher	O
.	O
1.3	O
the	O
cookie	B
problem	I
we	O
’	O
ll	O
get	O
to	O
bayes	O
’	O
s	O
theorem	O
soon	O
,	O
but	O
i	O
want	O
to	O
motivate	O
it	O
with	O
an	O
example	O
called	O
the	O
cookie	O
problem.1	O
suppose	O
there	O
are	O
two	O
bowls	O
of	O
cookies	O
.	O
bowl	O
1	O
contains	O
30	O
vanilla	O
cookies	O
and	O
10	O
chocolate	O
cookies	O
.	O
bowl	O
2	O
contains	O
20	O
of	O
each	O
.	O
now	O
suppose	O
you	O
choose	O
one	O
of	O
the	O
bowls	O
at	O
random	O
and	O
,	O
without	O
looking	O
,	O
select	O
a	O
cookie	O
at	O
random	O
.	O
the	O
cookie	O
is	O
vanilla	O
.	O
what	O
is	O
the	O
probability	B
that	O
it	O
came	O
from	O
bowl	O
1	O
?	O
this	O
is	O
a	O
conditional	B
probability	I
;	O
we	O
want	O
p	O
(	O
bowl	O
1|vanilla	O
)	O
,	O
but	O
it	O
is	O
not	O
obvious	O
how	O
to	O
compute	O
it	O
.	O
if	O
i	O
asked	O
a	O
different	O
question—the	O
probability	B
of	O
a	O
vanilla	O
cookie	O
given	O
bowl	O
1—it	O
would	O
be	O
easy	O
:	O
p	O
(	O
vanilla|bowl	O
1	O
)	O
=	O
3/4	O
sadly	O
,	O
p	O
(	O
a|b	O
)	O
is	O
not	O
the	O
same	O
as	O
p	O
(	O
b|a	O
)	O
,	O
but	O
there	O
is	O
a	O
way	O
to	O
get	O
from	O
one	O
to	O
the	O
other	O
:	O
bayes	O
’	O
s	O
theorem	O
.	O
1.4	O
bayes	O
’	O
s	O
theorem	O
at	O
this	O
point	O
we	O
have	O
everything	O
we	O
need	O
to	O
derive	O
bayes	O
’	O
s	O
theorem	O
.	O
we	O
’	O
ll	O
start	O
with	O
the	O
observation	O
that	O
conjunction	B
is	O
commutative	O
;	O
that	O
is	O
p	O
(	O
a	O
and	O
b	O
)	O
=	O
p	O
(	O
b	O
and	O
a	O
)	O
for	O
any	O
events	O
a	O
and	O
b.	O
next	O
,	O
we	O
write	O
the	O
probability	B
of	O
a	O
conjunction	B
:	O
p	O
(	O
a	O
and	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
since	O
we	O
have	O
not	O
said	O
anything	O
about	O
what	O
a	O
and	O
b	O
mean	O
,	O
they	O
are	O
inter-	O
changeable	O
.	O
interchanging	O
them	O
yields	O
p	O
(	O
b	O
and	O
a	O
)	O
=	O
p	O
(	O
b	O
)	O
p	O
(	O
a|b	O
)	O
1based	O
on	O
an	O
example	O
from	O
http	O
:	O
//en.wikipedia.org/wiki/bayes'_theorem	O
that	O
is	O
no	O
longer	O
there	O
.	O
4	O
chapter	O
1.	O
bayes	O
’	O
s	O
theorem	O
that	O
’	O
s	O
all	O
we	O
need	O
.	O
pulling	O
those	O
pieces	O
together	O
,	O
we	O
get	O
p	O
(	O
b	O
)	O
p	O
(	O
a|b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
which	O
means	O
there	O
are	O
two	O
ways	O
to	O
compute	O
the	O
conjunction	B
.	O
if	O
you	O
have	O
p	O
(	O
a	O
)	O
,	O
you	O
multiply	O
by	O
the	O
conditional	B
probability	I
p	O
(	O
b|a	O
)	O
.	O
or	O
you	O
can	O
do	O
it	O
the	O
other	O
way	O
around	O
;	O
if	O
you	O
know	O
p	O
(	O
b	O
)	O
,	O
you	O
multiply	O
by	O
p	O
(	O
a|b	O
)	O
.	O
either	O
way	O
you	O
should	O
get	O
the	O
same	O
thing	O
.	O
finally	O
we	O
can	O
divide	O
through	O
by	O
p	O
(	O
b	O
)	O
:	O
p	O
(	O
a|b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
b	O
)	O
and	O
that	O
’	O
s	O
bayes	O
’	O
s	O
theorem	O
!	O
it	O
might	O
not	O
look	O
like	O
much	O
,	O
but	O
it	O
turns	O
out	O
to	O
be	O
surprisingly	O
powerful	O
.	O
for	O
example	O
,	O
we	O
can	O
use	O
it	O
to	O
solve	O
the	O
cookie	B
problem	I
.	O
i	O
’	O
ll	O
write	O
b1	O
for	O
the	O
hypothesis	O
that	O
the	O
cookie	O
came	O
from	O
bowl	O
1	O
and	O
v	O
for	O
the	O
vanilla	O
cookie	O
.	O
plugging	O
in	O
bayes	O
’	O
s	O
theorem	O
we	O
get	O
p	O
(	O
b1|v	O
)	O
=	O
p	O
(	O
b1	O
)	O
p	O
(	O
v|b1	O
)	O
p	O
(	O
v	O
)	O
the	O
term	O
on	O
the	O
left	O
is	O
what	O
we	O
want	O
:	O
the	O
probability	B
of	O
bowl	O
1	O
,	O
given	O
that	O
we	O
chose	O
a	O
vanilla	O
cookie	O
.	O
the	O
terms	O
on	O
the	O
right	O
are	O
:	O
•	O
p	O
(	O
b1	O
)	O
:	O
this	O
is	O
the	O
probability	B
that	O
we	O
chose	O
bowl	O
1	O
,	O
unconditioned	O
by	O
what	O
kind	O
of	O
cookie	O
we	O
got	O
.	O
since	O
the	O
problem	O
says	O
we	O
chose	O
a	O
bowl	O
at	O
random	O
,	O
we	O
can	O
assume	O
p	O
(	O
b1	O
)	O
=	O
1/2	O
.	O
•	O
p	O
(	O
v|b1	O
)	O
:	O
this	O
is	O
the	O
probability	B
of	O
getting	O
a	O
vanilla	O
cookie	O
from	O
bowl	O
1	O
,	O
which	O
is	O
3/4	O
.	O
•	O
p	O
(	O
v	O
)	O
:	O
this	O
is	O
the	O
probability	B
of	O
drawing	O
a	O
vanilla	O
cookie	O
from	O
either	O
bowl	O
.	O
since	O
we	O
had	O
an	O
equal	O
chance	O
of	O
choosing	O
either	O
bowl	O
and	O
the	O
bowls	O
contain	O
the	O
same	O
number	O
of	O
cookies	O
,	O
we	O
had	O
the	O
same	O
chance	O
of	O
choosing	O
any	O
cookie	O
.	O
between	O
the	O
two	O
bowls	O
there	O
are	O
50	O
vanilla	O
and	O
30	O
chocolate	O
cookies	O
,	O
so	O
p	O
(	O
v	O
)	O
=	O
5/8	O
.	O
putting	O
it	O
together	O
,	O
we	O
have	O
p	O
(	O
b1|v	O
)	O
=	O
(	O
1/2	O
)	O
(	O
3/4	O
)	O
5/8	O
1.5.	O
the	O
diachronic	B
interpretation	I
5	O
which	O
reduces	O
to	O
3/5	O
.	O
so	O
the	O
vanilla	O
cookie	O
is	O
evidence	B
in	O
favor	O
of	O
the	O
hy-	O
pothesis	O
that	O
we	O
chose	O
bowl	O
1	O
,	O
because	O
vanilla	O
cookies	O
are	O
more	O
likely	O
to	O
come	O
from	O
bowl	O
1.	O
this	O
example	O
demonstrates	O
one	O
use	O
of	O
bayes	O
’	O
s	O
theorem	O
:	O
it	O
provides	O
a	O
strat-	O
egy	O
to	O
get	O
from	O
p	O
(	O
b|a	O
)	O
to	O
p	O
(	O
a|b	O
)	O
.	O
this	O
strategy	O
is	O
useful	O
in	O
cases	O
,	O
like	O
the	O
cookie	B
problem	I
,	O
where	O
it	O
is	O
easier	O
to	O
compute	O
the	O
terms	O
on	O
the	O
right	O
side	O
of	O
bayes	O
’	O
s	O
theorem	O
than	O
the	O
term	O
on	O
the	O
left	O
.	O
1.5	O
the	O
diachronic	B
interpretation	I
there	O
is	O
another	O
way	O
to	O
think	O
of	O
bayes	O
’	O
s	O
theorem	O
:	O
it	O
gives	O
us	O
a	O
way	O
to	O
update	O
the	O
probability	B
of	O
a	O
hypothesis	O
,	O
h	O
,	O
in	O
light	O
of	O
some	O
body	O
of	O
data	O
,	O
d.	O
this	O
way	O
of	O
thinking	O
about	O
bayes	O
’	O
s	O
theorem	O
is	O
called	O
the	O
diachronic	O
inter-	O
pretation	O
.	O
“	O
diachronic	O
”	O
means	O
that	O
something	O
is	O
happening	O
over	O
time	O
;	O
in	O
this	O
case	O
the	O
probability	B
of	O
the	O
hypotheses	O
changes	O
,	O
over	O
time	O
,	O
as	O
we	O
see	O
new	O
data	O
.	O
rewriting	O
bayes	O
’	O
s	O
theorem	O
with	O
h	O
and	O
d	O
yields	O
:	O
p	O
(	O
h	O
)	O
p	O
(	O
d|h	O
)	O
p	O
(	O
h|d	O
)	O
=	O
p	O
(	O
d	O
)	O
in	O
this	O
interpretation	O
,	O
each	O
term	O
has	O
a	O
name	O
:	O
•	O
p	O
(	O
h	O
)	O
is	O
the	O
probability	B
of	O
the	O
hypothesis	O
before	O
we	O
see	O
the	O
data	O
,	O
called	O
the	O
prior	B
probability	O
,	O
or	O
just	O
prior	B
.	O
•	O
p	O
(	O
h|d	O
)	O
is	O
what	O
we	O
want	O
to	O
compute	O
,	O
the	O
probability	B
of	O
the	O
hypothesis	O
after	O
we	O
see	O
the	O
data	O
,	O
called	O
the	O
posterior	B
.	O
•	O
p	O
(	O
d|h	O
)	O
is	O
the	O
probability	B
of	O
the	O
data	O
under	O
the	O
hypothesis	O
,	O
called	O
the	O
likelihood	B
.	O
•	O
p	O
(	O
d	O
)	O
is	O
the	O
probability	B
of	O
the	O
data	O
under	O
any	O
hypothesis	O
,	O
called	O
the	O
normalizing	B
constant	I
.	O
sometimes	O
we	O
can	O
compute	O
the	O
prior	B
based	O
on	O
background	O
information	O
.	O
for	O
example	O
,	O
the	O
cookie	B
problem	I
speciﬁes	O
that	O
we	O
choose	O
a	O
bowl	O
at	O
random	O
with	O
equal	O
probability	B
.	O
6	O
chapter	O
1.	O
bayes	O
’	O
s	O
theorem	O
in	O
other	O
cases	O
the	O
prior	B
is	O
subjective	O
;	O
that	O
is	O
,	O
reasonable	O
people	O
might	O
dis-	O
agree	O
,	O
either	O
because	O
they	O
use	O
different	O
background	O
information	O
or	O
because	O
they	O
interpret	O
the	O
same	O
information	O
differently	O
.	O
the	O
likelihood	B
is	O
usually	O
the	O
easiest	O
part	O
to	O
compute	O
.	O
in	O
the	O
cookie	B
problem	I
,	O
if	O
we	O
know	O
which	O
bowl	O
the	O
cookie	O
came	O
from	O
,	O
we	O
ﬁnd	O
the	O
probability	B
of	O
a	O
vanilla	O
cookie	O
by	O
counting	O
.	O
the	O
normalizing	B
constant	I
can	O
be	O
tricky	O
.	O
it	O
is	O
supposed	O
to	O
be	O
the	O
probability	B
of	O
seeing	O
the	O
data	O
under	O
any	O
hypothesis	O
at	O
all	O
,	O
but	O
in	O
the	O
most	O
general	O
case	O
it	O
is	O
hard	O
to	O
nail	O
down	O
what	O
that	O
means	O
.	O
most	O
often	O
we	O
simplify	O
things	O
by	O
specifying	O
a	O
set	O
of	O
hypotheses	O
that	O
are	O
mutually	B
exclusive	I
:	O
at	O
most	O
one	O
hypothesis	O
in	O
the	O
set	O
can	O
be	O
true	O
,	O
and	O
collectively	B
exhaustive	I
:	O
there	O
are	O
no	O
other	O
possibilities	O
;	O
at	O
least	O
one	O
of	O
the	O
hypotheses	O
has	O
to	O
be	O
true	O
.	O
i	O
use	O
the	O
word	O
suite	O
for	O
a	O
set	O
of	O
hypotheses	O
that	O
has	O
these	O
properties	O
.	O
in	O
the	O
cookie	B
problem	I
,	O
there	O
are	O
only	O
two	O
hypotheses—the	O
cookie	O
came	O
from	O
bowl	O
1	O
or	O
bowl	O
2—and	O
they	O
are	O
mutually	B
exclusive	I
and	O
collectively	B
exhaustive	I
.	O
in	O
that	O
case	O
we	O
can	O
compute	O
p	O
(	O
d	O
)	O
using	O
the	O
law	O
of	O
total	B
probability	I
,	O
which	O
says	O
that	O
if	O
there	O
are	O
two	O
exclusive	O
ways	O
that	O
something	O
might	O
happen	O
,	O
you	O
can	O
add	O
up	O
the	O
probabilities	O
like	O
this	O
:	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
b1	O
)	O
p	O
(	O
d|b1	O
)	O
+	O
p	O
(	O
b2	O
)	O
p	O
(	O
d|b2	O
)	O
plugging	O
in	O
the	O
values	O
from	O
the	O
cookie	B
problem	I
,	O
we	O
have	O
p	O
(	O
d	O
)	O
=	O
(	O
1/2	O
)	O
(	O
3/4	O
)	O
+	O
(	O
1/2	O
)	O
(	O
1/2	O
)	O
=	O
5/8	O
which	O
is	O
what	O
we	O
computed	O
earlier	O
by	O
mentally	O
combining	O
the	O
two	O
bowls	O
.	O
1.6	O
the	O
m	O
&	O
m	O
problem	O
m	O
&	O
m	O
’	O
s	O
are	O
small	O
candy-coated	O
chocolates	O
that	O
come	O
in	O
a	O
variety	O
of	O
colors	O
.	O
mars	O
,	O
inc.	O
,	O
which	O
makes	O
m	O
&	O
m	O
’	O
s	O
,	O
changes	O
the	O
mixture	B
of	O
colors	O
from	O
time	O
to	O
time	O
.	O
1.6.	O
the	O
m	O
&	O
m	O
problem	O
7	O
in	O
1995	O
,	O
they	O
introduced	O
blue	O
m	O
&	O
m	O
’	O
s	O
.	O
before	O
then	O
,	O
the	O
color	O
mix	O
in	O
a	O
bag	O
of	O
plain	O
m	O
&	O
m	O
’	O
s	O
was	O
30	O
%	O
brown	O
,	O
20	O
%	O
yellow	O
,	O
20	O
%	O
red	O
,	O
10	O
%	O
green	O
,	O
10	O
%	O
orange	O
,	O
10	O
%	O
tan	O
.	O
afterward	O
it	O
was	O
24	O
%	O
blue	O
,	O
20	O
%	O
green	O
,	O
16	O
%	O
orange	O
,	O
14	O
%	O
yellow	O
,	O
13	O
%	O
red	O
,	O
13	O
%	O
brown	O
.	O
suppose	O
a	O
friend	O
of	O
mine	O
has	O
two	O
bags	O
of	O
m	O
&	O
m	O
’	O
s	O
,	O
and	O
he	O
tells	O
me	O
that	O
one	O
is	O
from	O
1994	O
and	O
one	O
from	O
1996.	O
he	O
won	O
’	O
t	O
tell	O
me	O
which	O
is	O
which	O
,	O
but	O
he	O
gives	O
me	O
one	O
m	O
&	O
m	O
from	O
each	O
bag	O
.	O
one	O
is	O
yellow	O
and	O
one	O
is	O
green	O
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
yellow	O
one	O
came	O
from	O
the	O
1994	O
bag	O
?	O
this	O
problem	O
is	O
similar	O
to	O
the	O
cookie	B
problem	I
,	O
with	O
the	O
twist	O
that	O
i	O
draw	O
one	O
sample	O
from	O
each	O
bowl/bag	O
.	O
this	O
problem	O
also	O
gives	O
me	O
a	O
chance	O
to	O
demonstrate	O
the	O
table	B
method	I
,	O
which	O
is	O
useful	O
for	O
solving	O
problems	O
like	O
this	O
on	O
paper	O
.	O
in	O
the	O
next	O
chapter	O
we	O
will	O
solve	O
them	O
computationally	O
.	O
the	O
ﬁrst	O
step	O
is	O
to	O
enumerate	O
the	O
hypotheses	O
.	O
the	O
bag	O
the	O
yellow	O
m	O
&	O
m	O
came	O
from	O
i	O
’	O
ll	O
call	O
bag	O
1	O
;	O
i	O
’	O
ll	O
call	O
the	O
other	O
bag	O
2.	O
so	O
the	O
hypothe-	O
ses	O
are	O
:	O
•	O
a	O
:	O
bag	O
1	O
is	O
from	O
1994	O
,	O
which	O
implies	O
that	O
bag	O
2	O
is	O
from	O
1996	O
.	O
•	O
b	O
:	O
bag	O
1	O
is	O
from	O
1996	O
and	O
bag	O
2	O
from	O
1994.	O
now	O
we	O
construct	O
a	O
table	O
with	O
a	O
row	O
for	O
each	O
hypothesis	O
and	O
a	O
column	O
for	O
each	O
term	O
in	O
bayes	O
’	O
s	O
theorem	O
:	O
prior	B
likelihood	O
p	O
(	O
d|h	O
)	O
p	O
(	O
h	O
)	O
(	O
20	O
)	O
(	O
20	O
)	O
a	O
1/2	O
b	O
1/2	O
(	O
14	O
)	O
(	O
10	O
)	O
p	O
(	O
h	O
)	O
p	O
(	O
d|h	O
)	O
200	O
70	O
posterior	B
p	O
(	O
h|d	O
)	O
20/27	O
7/27	O
the	O
ﬁrst	O
column	O
has	O
the	O
priors	O
.	O
based	O
on	O
the	O
statement	O
of	O
the	O
problem	O
,	O
it	O
is	O
reasonable	O
to	O
choose	O
p	O
(	O
a	O
)	O
=	O
p	O
(	O
b	O
)	O
=	O
1/2	O
.	O
the	O
second	O
column	O
has	O
the	O
likelihoods	O
,	O
which	O
follow	O
from	O
the	O
information	O
in	O
the	O
problem	O
.	O
for	O
example	O
,	O
if	O
a	O
is	O
true	O
,	O
the	O
yellow	O
m	O
&	O
m	O
came	O
from	O
the	O
1994	O
bag	O
with	O
probability	B
20	O
%	O
,	O
and	O
the	O
green	O
came	O
from	O
the	O
1996	O
bag	O
with	O
probability	B
20	O
%	O
.	O
if	O
b	O
is	O
true	O
,	O
the	O
yellow	O
m	O
&	O
m	O
came	O
from	O
the	O
1996	O
bag	O
with	O
probability	B
14	O
%	O
,	O
and	O
the	O
green	O
came	O
from	O
the	O
1994	O
bag	O
with	O
probability	B
10	O
%	O
.	O
because	O
the	O
selections	O
are	O
independent	O
,	O
we	O
get	O
the	O
conjoint	B
probability	I
by	O
multiplying	O
.	O
the	O
third	O
column	O
is	O
just	O
the	O
product	O
of	O
the	O
previous	O
two	O
.	O
the	O
sum	O
of	O
this	O
column	O
,	O
270	O
,	O
is	O
the	O
normalizing	B
constant	I
.	O
to	O
get	O
the	O
last	O
column	O
,	O
which	O
8	O
chapter	O
1.	O
bayes	O
’	O
s	O
theorem	O
contains	O
the	O
posteriors	O
,	O
we	O
divide	O
the	O
third	O
column	O
by	O
the	O
normalizing	O
con-	O
stant	O
.	O
that	O
’	O
s	O
it	O
.	O
simple	O
,	O
right	O
?	O
well	O
,	O
you	O
might	O
be	O
bothered	O
by	O
one	O
detail	O
.	O
i	O
write	O
p	O
(	O
d|h	O
)	O
in	O
terms	O
of	O
per-	O
centages	O
,	O
not	O
probabilities	O
,	O
which	O
means	O
it	O
is	O
off	O
by	O
a	O
factor	O
of	O
10,000.	O
but	O
that	O
cancels	O
out	O
when	O
we	O
divide	O
through	O
by	O
the	O
normalizing	B
constant	I
,	O
so	O
it	O
doesn	O
’	O
t	O
affect	O
the	O
result	O
.	O
when	O
the	O
set	O
of	O
hypotheses	O
is	O
mutually	B
exclusive	I
and	O
collectively	O
exhaus-	O
tive	O
,	O
you	O
can	O
multiply	O
the	O
likelihoods	O
by	O
any	O
factor	O
,	O
if	O
it	O
is	O
convenient	O
,	O
as	O
long	O
as	O
you	O
apply	O
the	O
same	O
factor	O
to	O
the	O
entire	O
column	O
.	O
1.7	O
the	O
monty	O
hall	O
problem	O
the	O
monty	O
hall	O
problem	O
might	O
be	O
the	O
most	O
contentious	O
question	O
in	O
the	O
his-	O
tory	O
of	O
probability	B
.	O
the	O
scenario	O
is	O
simple	O
,	O
but	O
the	O
correct	O
answer	O
is	O
so	O
coun-	O
terintuitive	O
that	O
many	O
people	O
just	O
can	O
’	O
t	O
accept	O
it	O
,	O
and	O
many	O
smart	O
people	O
have	O
embarrassed	O
themselves	O
not	O
just	O
by	O
getting	O
it	O
wrong	O
but	O
by	O
arguing	O
the	O
wrong	O
side	O
,	O
aggressively	O
,	O
in	O
public	O
.	O
monty	O
hall	O
was	O
the	O
original	O
host	O
of	O
the	O
game	O
show	O
let	O
’	O
s	O
make	O
a	O
deal	O
.	O
the	O
monty	O
hall	O
problem	O
is	O
based	O
on	O
one	O
of	O
the	O
regular	O
games	O
on	O
the	O
show	O
.	O
if	O
you	O
are	O
on	O
the	O
show	O
,	O
here	O
’	O
s	O
what	O
happens	O
:	O
•	O
monty	O
shows	O
you	O
three	O
closed	O
doors	O
and	O
tells	O
you	O
that	O
there	O
is	O
a	O
prize	O
behind	O
each	O
door	O
:	O
one	O
prize	O
is	O
a	O
car	O
,	O
the	O
other	O
two	O
are	O
less	O
valuable	O
prizes	O
like	O
peanut	O
butter	O
and	O
fake	O
ﬁnger	O
nails	O
.	O
the	O
prizes	O
are	O
arranged	O
at	O
random	O
.	O
•	O
the	O
object	O
of	O
the	O
game	O
is	O
to	O
guess	O
which	O
door	O
has	O
the	O
car	O
.	O
if	O
you	O
guess	O
right	O
,	O
you	O
get	O
to	O
keep	O
the	O
car	O
.	O
•	O
you	O
pick	O
a	O
door	O
,	O
which	O
we	O
will	O
call	O
door	O
a.	O
we	O
’	O
ll	O
call	O
the	O
other	O
doors	O
b	O
and	O
c.	O
•	O
before	O
opening	O
the	O
door	O
you	O
chose	O
,	O
monty	O
increases	O
the	O
suspense	O
by	O
opening	O
either	O
door	O
b	O
or	O
c	O
,	O
whichever	O
does	O
not	O
have	O
the	O
car	O
.	O
(	O
if	O
the	O
car	O
is	O
actually	O
behind	O
door	O
a	O
,	O
monty	O
can	O
safely	O
open	O
b	O
or	O
c	O
,	O
so	O
he	O
chooses	O
one	O
at	O
random	O
.	O
)	O
•	O
then	O
monty	O
offers	O
you	O
the	O
option	O
to	O
stick	B
with	O
your	O
original	O
choice	O
or	O
switch	B
to	O
the	O
one	O
remaining	O
unopened	O
door	O
.	O
1.7.	O
the	O
monty	O
hall	O
problem	O
9	O
the	O
question	O
is	O
,	O
should	O
you	O
“	O
stick	B
”	O
or	O
“	O
switch	B
”	O
or	O
does	O
it	O
make	O
no	O
differ-	O
ence	O
?	O
most	O
people	O
have	O
the	O
strong	O
intuition	B
that	O
it	O
makes	O
no	O
difference	O
.	O
there	O
are	O
two	O
doors	O
left	O
,	O
they	O
reason	O
,	O
so	O
the	O
chance	O
that	O
the	O
car	O
is	O
behind	O
door	O
a	O
is	O
50	O
%	O
.	O
but	O
that	O
is	O
wrong	O
.	O
in	O
fact	O
,	O
the	O
chance	O
of	O
winning	O
if	O
you	O
stick	B
with	O
door	O
a	O
is	O
only	O
1/3	O
;	O
if	O
you	O
switch	B
,	O
your	O
chances	O
are	O
2/3	O
.	O
by	O
applying	O
bayes	O
’	O
s	O
theorem	O
,	O
we	O
can	O
break	O
this	O
problem	O
into	O
simple	O
pieces	O
,	O
and	O
maybe	O
convince	O
ourselves	O
that	O
the	O
correct	O
answer	O
is	O
,	O
in	O
fact	O
,	O
correct	O
.	O
to	O
start	O
,	O
we	O
should	O
make	O
a	O
careful	O
statement	O
of	O
the	O
data	O
.	O
in	O
this	O
case	O
d	O
consists	O
of	O
two	O
parts	O
:	O
monty	O
chooses	O
door	O
b	O
and	O
there	O
is	O
no	O
car	O
there	O
.	O
next	O
we	O
deﬁne	O
three	O
hypotheses	O
:	O
a	O
,	O
b	O
,	O
and	O
c	O
represent	O
the	O
hypothesis	O
that	O
the	O
car	O
is	O
behind	O
door	O
a	O
,	O
door	O
b	O
,	O
or	O
door	O
c.	O
again	O
,	O
let	O
’	O
s	O
apply	O
the	O
table	B
method	I
:	O
prior	B
likelihood	O
p	O
(	O
d|h	O
)	O
p	O
(	O
h	O
)	O
1/2	O
a	O
1/3	O
0	O
b	O
1/3	O
c	O
1/3	O
1	O
p	O
(	O
h	O
)	O
p	O
(	O
d|h	O
)	O
1/6	O
0	O
1/3	O
posterior	B
p	O
(	O
h|d	O
)	O
1/3	O
0	O
2/3	O
filling	O
in	O
the	O
priors	O
is	O
easy	O
because	O
we	O
are	O
told	O
that	O
the	O
prizes	O
are	O
arranged	O
at	O
random	O
,	O
which	O
suggests	O
that	O
the	O
car	O
is	O
equally	O
likely	O
to	O
be	O
behind	O
any	O
door	O
.	O
figuring	O
out	O
the	O
likelihoods	O
takes	O
some	O
thought	O
,	O
but	O
with	O
reasonable	O
care	O
we	O
can	O
be	O
conﬁdent	O
that	O
we	O
have	O
it	O
right	O
:	O
•	O
if	O
the	O
car	O
is	O
actually	O
behind	O
a	O
,	O
monty	O
could	O
safely	O
open	O
doors	O
b	O
or	O
c.	O
so	O
the	O
probability	B
that	O
he	O
chooses	O
b	O
is	O
1/2	O
.	O
and	O
since	O
the	O
car	O
is	O
actually	O
behind	O
a	O
,	O
the	O
probability	B
that	O
the	O
car	O
is	O
not	O
behind	O
b	O
is	O
1	O
.	O
•	O
if	O
the	O
car	O
is	O
actually	O
behind	O
b	O
,	O
monty	O
has	O
to	O
open	O
door	O
c	O
,	O
so	O
the	O
prob-	O
ability	O
that	O
he	O
opens	O
door	O
b	O
is	O
0	O
.	O
•	O
finally	O
,	O
if	O
the	O
car	O
is	O
behind	O
door	O
c	O
,	O
monty	O
opens	O
b	O
with	O
probability	B
1	O
and	O
ﬁnds	O
no	O
car	O
there	O
with	O
probability	B
1.	O
now	O
the	O
hard	O
part	O
is	O
over	O
;	O
the	O
rest	O
is	O
just	O
arithmetic	O
.	O
the	O
sum	O
of	O
the	O
third	O
column	O
is	O
1/2	O
.	O
dividing	O
through	O
yields	O
p	O
(	O
a|d	O
)	O
=	O
1/3	O
and	O
p	O
(	O
c|d	O
)	O
=	O
2/3	O
.	O
so	O
you	O
are	O
better	O
off	O
switching	O
.	O
10	O
chapter	O
1.	O
bayes	O
’	O
s	O
theorem	O
there	O
are	O
many	O
variations	O
of	O
the	O
monty	O
hall	O
problem	O
.	O
one	O
of	O
the	O
strengths	O
of	O
the	O
bayesian	O
approach	O
is	O
that	O
it	O
generalizes	O
to	O
handle	O
these	O
variations	O
.	O
for	O
example	O
,	O
suppose	O
that	O
monty	O
always	O
chooses	O
b	O
if	O
he	O
can	O
,	O
and	O
only	O
chooses	O
c	O
if	O
he	O
has	O
to	O
(	O
because	O
the	O
car	O
is	O
behind	O
b	O
)	O
.	O
in	O
that	O
case	O
the	O
revised	O
table	O
is	O
:	O
p	O
(	O
h	O
)	O
p	O
(	O
d|h	O
)	O
1/3	O
0	O
1/3	O
1	O
0	O
1	O
posterior	B
p	O
(	O
h|d	O
)	O
1/2	O
0	O
1/2	O
prior	B
likelihood	O
p	O
(	O
d|h	O
)	O
p	O
(	O
h	O
)	O
a	O
1/3	O
b	O
1/3	O
c	O
1/3	O
the	O
only	O
change	O
is	O
p	O
(	O
d|a	O
)	O
.	O
if	O
the	O
car	O
is	O
behind	O
a	O
,	O
monty	O
can	O
choose	O
to	O
open	O
b	O
or	O
c.	O
but	O
in	O
this	O
variation	O
he	O
always	O
chooses	O
b	O
,	O
so	O
p	O
(	O
d|a	O
)	O
=	O
1.	O
as	O
a	O
result	O
,	O
the	O
likelihoods	O
are	O
the	O
same	O
for	O
a	O
and	O
c	O
,	O
and	O
the	O
posteriors	O
are	O
the	O
same	O
:	O
p	O
(	O
a|d	O
)	O
=	O
p	O
(	O
c|d	O
)	O
=	O
1/2	O
.	O
in	O
this	O
case	O
,	O
the	O
fact	O
that	O
monty	O
chose	O
b	O
reveals	O
no	O
information	O
about	O
the	O
location	O
of	O
the	O
car	O
,	O
so	O
it	O
doesn	O
’	O
t	O
matter	O
whether	O
the	O
contestant	O
sticks	O
or	O
switches	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
he	O
had	O
opened	O
c	O
,	O
we	O
would	O
know	O
p	O
(	O
b|d	O
)	O
=	O
1.	O
i	O
included	O
the	O
monty	O
hall	O
problem	O
in	O
this	O
chapter	O
because	O
i	O
think	O
it	O
is	O
fun	O
,	O
and	O
because	O
bayes	O
’	O
s	O
theorem	O
makes	O
the	O
complexity	O
of	O
the	O
problem	O
a	O
little	O
more	O
manageable	O
.	O
but	O
it	O
is	O
not	O
a	O
typical	O
use	O
of	O
bayes	O
’	O
s	O
theorem	O
,	O
so	O
if	O
you	O
found	O
it	O
confusing	O
,	O
don	O
’	O
t	O
worry	O
!	O
1.8	O
discussion	O
for	O
many	O
problems	O
involving	O
conditional	B
probability	I
,	O
bayes	O
’	O
s	O
theorem	O
pro-	O
vides	O
a	O
divide-and-conquer	B
strategy	O
.	O
if	O
p	O
(	O
a|b	O
)	O
is	O
hard	O
to	O
compute	O
,	O
or	O
hard	O
to	O
measure	O
experimentally	O
,	O
check	O
whether	O
it	O
might	O
be	O
easier	O
to	O
compute	O
the	O
other	O
terms	O
in	O
bayes	O
’	O
s	O
theorem	O
,	O
p	O
(	O
b|a	O
)	O
,	O
p	O
(	O
a	O
)	O
and	O
p	O
(	O
b	O
)	O
.	O
if	O
the	O
monty	O
hall	O
problem	O
is	O
your	O
idea	O
of	O
fun	O
,	O
i	O
have	O
collected	O
a	O
num-	O
ber	O
of	O
similar	O
problems	O
in	O
an	O
article	O
called	O
“	O
all	O
your	O
bayes	O
are	O
belong	O
to	O
us	O
,	O
”	O
which	O
you	O
can	O
read	O
at	O
http	O
:	O
//allendowney.blogspot.com/2011/10/	O
all-your-bayes-are-belong-to-us.html	O
.	O
chapter	O
2	O
computational	O
statistics	O
2.1	O
distributions	O
in	O
statistics	O
a	O
distribution	B
is	O
a	O
set	O
of	O
values	O
and	O
their	O
corresponding	O
proba-	O
bilities	O
.	O
for	O
example	O
,	O
if	O
you	O
roll	O
a	O
six-sided	O
die	O
,	O
the	O
set	O
of	O
possible	O
values	O
is	O
the	O
numbers	O
1	O
to	O
6	O
,	O
and	O
the	O
probability	B
associated	O
with	O
each	O
value	O
is	O
1/6	O
.	O
as	O
another	O
example	O
,	O
you	O
might	O
be	O
interested	O
in	O
how	O
many	O
times	O
each	O
word	O
appears	O
in	O
common	O
english	O
usage	O
.	O
you	O
could	O
build	O
a	O
distribution	B
that	O
in-	O
cludes	O
each	O
word	O
and	O
how	O
many	O
times	O
it	O
appears	O
.	O
to	O
represent	O
a	O
distribution	B
in	O
python	O
,	O
you	O
could	O
use	O
a	O
dictionary	O
that	O
maps	O
from	O
each	O
value	O
to	O
its	O
probability	B
.	O
i	O
have	O
written	O
a	O
class	O
called	O
pmf	O
that	O
uses	O
a	O
python	O
dictionary	O
in	O
exactly	O
that	O
way	O
,	O
and	O
provides	O
a	O
number	O
of	O
useful	O
methods	O
.	O
i	O
called	O
the	O
class	O
pmf	O
in	O
reference	O
to	O
a	O
probability	B
mass	I
function	I
,	O
which	O
is	O
a	O
way	O
to	O
represent	O
a	O
distribution	B
mathematically	O
.	O
pmf	O
is	O
deﬁned	O
in	O
a	O
python	O
module	O
i	O
wrote	O
to	O
accompany	O
this	O
book	O
,	O
thinkbayes.py	O
.	O
you	O
can	O
download	O
it	O
from	O
http	O
:	O
//thinkbayes.com/	O
thinkbayes.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3.	O
to	O
use	O
pmf	O
you	O
can	O
import	O
it	O
like	O
this	O
:	O
from	O
thinkbayes	O
import	O
pmf	O
the	O
following	O
code	O
builds	O
a	O
pmf	O
to	O
represent	O
the	O
distribution	B
of	O
outcomes	O
for	O
a	O
six-sided	O
die	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
x	O
in	O
[	O
1,2,3,4,5,6	O
]	O
:	O
pmf.set	O
(	O
x	O
,	O
1/6.0	O
)	O
12	O
chapter	O
2.	O
computational	O
statistics	O
pmf	O
creates	O
an	O
empty	O
pmf	O
with	O
no	O
values	O
.	O
the	O
set	O
method	O
sets	O
the	O
proba-	O
bility	O
associated	O
with	O
each	O
value	O
to	O
1/6	O
.	O
here	O
’	O
s	O
another	O
example	O
that	O
counts	O
the	O
number	O
of	O
times	O
each	O
word	O
appears	O
in	O
a	O
sequence	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
word	O
in	O
word_list	O
:	O
pmf.incr	O
(	O
word	O
,	O
1	O
)	O
incr	O
increases	O
the	O
“	O
probability	B
”	O
associated	O
with	O
each	O
word	O
by	O
1.	O
if	O
a	O
word	O
is	O
not	O
already	O
in	O
the	O
pmf	O
,	O
it	O
is	O
added	O
.	O
i	O
put	O
“	O
probability	B
”	O
in	O
quotes	O
because	O
in	O
this	O
example	O
,	O
the	O
probabilities	O
are	O
not	O
normalized	O
;	O
that	O
is	O
,	O
they	O
do	O
not	O
add	O
up	O
to	O
1.	O
so	O
they	O
are	O
not	O
true	O
proba-	O
bilities	O
.	O
but	O
in	O
this	O
example	O
the	O
word	O
counts	O
are	O
proportional	O
to	O
the	O
probabilities	O
.	O
so	O
after	O
we	O
count	O
all	O
the	O
words	O
,	O
we	O
can	O
compute	O
probabilities	O
by	O
dividing	O
through	O
by	O
the	O
total	O
number	O
of	O
words	O
.	O
pmf	O
provides	O
a	O
method	O
,	O
normalize	B
,	O
that	O
does	O
exactly	O
that	O
:	O
pmf.normalize	O
(	O
)	O
once	O
you	O
have	O
a	O
pmf	O
object	O
,	O
you	O
can	O
ask	O
for	O
the	O
probability	B
associated	O
with	O
any	O
value	O
:	O
print	O
pmf.prob	O
(	O
'the	O
'	O
)	O
and	O
that	O
would	O
print	O
the	O
frequency	O
of	O
the	O
word	O
“	O
the	O
”	O
as	O
a	O
fraction	O
of	O
the	O
words	O
in	O
the	O
list	O
.	O
pmf	O
uses	O
a	O
python	O
dictionary	O
to	O
store	O
the	O
values	O
and	O
their	O
probabilities	O
,	O
so	O
the	O
values	O
in	O
the	O
pmf	O
can	O
be	O
any	O
hashable	O
type	O
.	O
the	O
probabilities	O
can	O
be	O
any	O
numerical	O
type	O
,	O
but	O
they	O
are	O
usually	O
ﬂoating-point	O
numbers	O
(	O
type	O
float	O
)	O
.	O
2.2	O
the	O
cookie	B
problem	I
in	O
the	O
context	O
of	O
bayes	O
’	O
s	O
theorem	O
,	O
it	O
is	O
natural	O
to	O
use	O
a	O
pmf	O
to	O
map	O
from	O
each	O
hypothesis	O
to	O
its	O
probability	B
.	O
in	O
the	O
cookie	B
problem	I
,	O
the	O
hypotheses	O
are	O
b1	O
and	O
b2	O
.	O
in	O
python	O
,	O
i	O
represent	O
them	O
with	O
strings	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
pmf.set	O
(	O
'bowl	O
1	O
'	O
,	O
0.5	O
)	O
pmf.set	O
(	O
'bowl	O
2	O
'	O
,	O
0.5	O
)	O
2.3.	O
the	O
bayesian	O
framework	O
13	O
this	O
distribution	B
,	O
which	O
contains	O
the	O
priors	O
for	O
each	O
hypothesis	O
,	O
is	O
called	O
(	O
wait	O
for	O
it	O
)	O
the	O
prior	B
distribution	I
.	O
to	O
update	O
the	O
distribution	B
based	O
on	O
new	O
data	O
(	O
the	O
vanilla	O
cookie	O
)	O
,	O
we	O
mul-	O
tiply	O
each	O
prior	B
by	O
the	O
corresponding	O
likelihood	B
.	O
the	O
likelihood	B
of	O
drawing	O
a	O
vanilla	O
cookie	O
from	O
bowl	O
1	O
is	O
3/4	O
.	O
the	O
likelihood	B
for	O
bowl	O
2	O
is	O
1/2	O
.	O
pmf.mult	O
(	O
'bowl	O
1	O
'	O
,	O
0.75	O
)	O
pmf.mult	O
(	O
'bowl	O
2	O
'	O
,	O
0.5	O
)	O
mult	O
does	O
what	O
you	O
would	O
expect	O
.	O
it	O
gets	O
the	O
probability	B
for	O
the	O
given	O
hy-	O
pothesis	O
and	O
multiplies	O
by	O
the	O
given	O
likelihood	B
.	O
after	O
this	O
update	O
,	O
the	O
distribution	B
is	O
no	O
longer	O
normalized	O
,	O
but	O
because	O
these	O
hypotheses	O
are	O
mutually	B
exclusive	I
and	O
collectively	B
exhaustive	I
,	O
we	O
can	O
renormalize	B
:	O
pmf.normalize	O
(	O
)	O
the	O
result	O
is	O
a	O
distribution	B
that	O
contains	O
the	O
posterior	B
probability	O
for	O
each	O
hypothesis	O
,	O
which	O
is	O
called	O
(	O
wait	O
now	O
)	O
the	O
posterior	B
distribution	I
.	O
finally	O
,	O
we	O
can	O
get	O
the	O
posterior	B
probability	O
for	O
bowl	O
1	O
:	O
print	O
pmf.prob	O
(	O
'bowl	O
1	O
'	O
)	O
and	O
the	O
answer	O
is	O
0.6.	O
you	O
can	O
download	O
this	O
example	O
from	O
http	O
:	O
//	O
thinkbayes.com/cookie.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
2.3	O
the	O
bayesian	O
framework	O
before	O
we	O
go	O
on	O
to	O
other	O
problems	O
,	O
i	O
want	O
to	O
rewrite	O
the	O
code	O
from	O
the	O
pre-	O
vious	O
section	O
to	O
make	O
it	O
more	O
general	O
.	O
first	O
i	O
’	O
ll	O
deﬁne	O
a	O
class	O
to	O
encapsulate	O
the	O
code	O
related	O
to	O
this	O
problem	O
:	O
class	O
cookie	O
(	O
pmf	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
hypos	O
)	O
:	O
pmf.__init__	O
(	O
self	O
)	O
for	O
hypo	O
in	O
hypos	O
:	O
self.set	O
(	O
hypo	O
,	O
1	O
)	O
self.normalize	O
(	O
)	O
a	O
cookie	O
object	O
is	O
a	O
pmf	O
that	O
maps	O
from	O
hypotheses	O
to	O
their	O
probabilities	O
.	O
the	O
__init__	O
method	O
gives	O
each	O
hypothesis	O
the	O
same	O
prior	B
probability	O
.	O
as	O
in	O
the	O
previous	O
section	O
,	O
there	O
are	O
two	O
hypotheses	O
:	O
14	O
chapter	O
2.	O
computational	O
statistics	O
hypos	O
=	O
[	O
'bowl	O
1	O
'	O
,	O
'bowl	O
2	O
'	O
]	O
pmf	O
=	O
cookie	O
(	O
hypos	O
)	O
cookie	O
provides	O
an	O
update	O
method	O
that	O
takes	O
data	O
as	O
a	O
parameter	B
and	O
up-	O
dates	O
the	O
probabilities	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
like	O
=	O
self.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
self.mult	O
(	O
hypo	O
,	O
like	O
)	O
self.normalize	O
(	O
)	O
update	O
loops	O
through	O
each	O
hypothesis	O
in	O
the	O
suite	B
and	O
multiplies	O
its	O
proba-	O
bility	O
by	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
the	O
hypothesis	O
,	O
which	O
is	O
computed	O
by	O
likelihood	B
:	O
mixes	O
=	O
{	O
'bowl	O
1	O
'	O
:	O
dict	O
(	O
vanilla=0.75	O
,	O
chocolate=0.25	O
)	O
,	O
'bowl	O
2	O
'	O
:	O
dict	O
(	O
vanilla=0.5	O
,	O
chocolate=0.5	O
)	O
,	O
}	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
mix	O
=	O
self.mixes	O
[	O
hypo	O
]	O
like	O
=	O
mix	O
[	O
data	O
]	O
return	O
like	O
likelihood	B
uses	O
mixes	O
,	O
which	O
is	O
a	O
dictionary	O
that	O
maps	O
from	O
the	O
name	O
of	O
a	O
bowl	O
to	O
the	O
mix	O
of	O
cookies	O
in	O
the	O
bowl	O
.	O
here	O
’	O
s	O
what	O
the	O
update	O
looks	O
like	O
:	O
pmf.update	O
(	O
'vanilla	O
'	O
)	O
and	O
then	O
we	O
can	O
print	O
the	O
posterior	B
probability	O
of	O
each	O
hypothesis	O
:	O
for	O
hypo	O
,	O
prob	O
in	O
pmf.items	O
(	O
)	O
:	O
print	O
hypo	O
,	O
prob	O
the	O
result	O
is	O
bowl	O
1	O
0.6	O
bowl	O
2	O
0.4	O
which	O
is	O
the	O
same	O
as	O
what	O
we	O
got	O
before	O
.	O
this	O
code	O
is	O
more	O
complicated	O
than	O
what	O
we	O
saw	O
in	O
the	O
previous	O
section	O
.	O
one	O
advantage	O
is	O
that	O
it	O
general-	O
izes	O
to	O
the	O
case	O
where	O
we	O
draw	O
more	O
than	O
one	O
cookie	O
from	O
the	O
same	O
bowl	O
(	O
with	O
replacement	O
)	O
:	O
dataset	O
=	O
[	O
'vanilla	O
'	O
,	O
'chocolate	O
'	O
,	O
'vanilla	O
'	O
]	O
for	O
data	O
in	O
dataset	O
:	O
pmf.update	O
(	O
data	O
)	O
2.4.	O
the	O
monty	O
hall	O
problem	O
15	O
the	O
other	O
advantage	O
is	O
that	O
it	O
provides	O
a	O
framework	O
for	O
solving	O
many	O
sim-	O
ilar	O
problems	O
.	O
in	O
the	O
next	O
section	O
we	O
’	O
ll	O
solve	O
the	O
monty	O
hall	O
problem	O
com-	O
putationally	O
and	O
then	O
see	O
what	O
parts	O
of	O
the	O
framework	O
are	O
the	O
same	O
.	O
the	O
code	O
in	O
this	O
section	O
is	O
available	O
from	O
http	O
:	O
//thinkbayes.com/cookie2	O
.	O
py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
2.4	O
the	O
monty	O
hall	O
problem	O
to	O
solve	O
the	O
monty	O
hall	O
problem	O
,	O
i	O
’	O
ll	O
deﬁne	O
a	O
new	O
class	O
:	O
class	O
monty	O
(	O
pmf	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
hypos	O
)	O
:	O
pmf.__init__	O
(	O
self	O
)	O
for	O
hypo	O
in	O
hypos	O
:	O
self.set	O
(	O
hypo	O
,	O
1	O
)	O
self.normalize	O
(	O
)	O
so	O
far	O
monty	O
and	O
cookie	O
are	O
exactly	O
the	O
same	O
.	O
and	O
the	O
code	O
that	O
creates	O
the	O
pmf	O
is	O
the	O
same	O
,	O
too	O
,	O
except	O
for	O
the	O
names	O
of	O
the	O
hypotheses	O
:	O
hypos	O
=	O
'abc'	O
pmf	O
=	O
monty	O
(	O
hypos	O
)	O
calling	O
update	O
is	O
pretty	O
much	O
the	O
same	O
:	O
data	O
=	O
'b'	O
pmf.update	O
(	O
data	O
)	O
and	O
the	O
implementation	B
of	O
update	O
is	O
exactly	O
the	O
same	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
like	O
=	O
self.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
self.mult	O
(	O
hypo	O
,	O
like	O
)	O
self.normalize	O
(	O
)	O
the	O
only	O
part	O
that	O
requires	O
some	O
work	O
is	O
likelihood	B
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
if	O
hypo	O
==	O
data	O
:	O
return	O
0	O
elif	O
hypo	O
==	O
'a	O
'	O
:	O
return	O
0.5	O
else	O
:	O
return	O
1	O
16	O
chapter	O
2.	O
computational	O
statistics	O
finally	O
,	O
printing	O
the	O
results	O
is	O
the	O
same	O
:	O
for	O
hypo	O
,	O
prob	O
in	O
pmf.items	O
(	O
)	O
:	O
print	O
hypo	O
,	O
prob	O
and	O
the	O
answer	O
is	O
a	O
0.333333333333	O
b	O
0.0	O
c	O
0.666666666667	O
in	O
this	O
example	O
,	O
writing	O
likelihood	B
is	O
a	O
little	O
complicated	O
,	O
but	O
the	O
frame-	O
work	O
of	O
the	O
bayesian	O
update	O
is	O
simple	O
.	O
the	O
code	O
in	O
this	O
section	O
is	O
avail-	O
able	O
from	O
http	O
:	O
//thinkbayes.com/monty.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
2.5	O
encapsulating	O
the	O
framework	O
now	O
that	O
we	O
see	O
what	O
elements	O
of	O
the	O
framework	O
are	O
the	O
same	O
,	O
we	O
can	O
encapsulate	O
them	O
in	O
an	O
object—a	O
suite	B
is	O
a	O
pmf	O
that	O
provides	O
__init__	O
,	O
update	O
,	O
and	O
print	O
:	O
class	O
suite	B
(	O
pmf	O
)	O
:	O
''	O
''	O
''	O
represents	O
a	O
suite	B
of	O
hypotheses	O
and	O
their	O
probabilities	O
.	O
''	O
''	O
''	O
def	O
__init__	O
(	O
self	O
,	O
hypo=tuple	O
(	O
)	O
)	O
:	O
''	O
''	O
''	O
initializes	O
the	O
distribution	B
.	O
''	O
''	O
''	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
''	O
''	O
''	O
updates	O
each	O
hypothesis	O
based	O
on	O
the	O
data	O
.	O
''	O
''	O
''	O
def	O
print	O
(	O
self	O
)	O
:	O
''	O
''	O
''	O
prints	O
the	O
hypotheses	O
and	O
their	O
probabilities	O
.	O
''	O
''	O
''	O
the	O
implementation	B
of	O
suite	B
is	O
in	O
thinkbayes.py	O
.	O
to	O
use	O
suite	B
,	O
you	O
should	O
write	O
a	O
class	O
that	O
inherits	O
from	O
it	O
and	O
provides	O
likelihood	B
.	O
for	O
example	O
,	O
here	O
is	O
the	O
solution	O
to	O
the	O
monty	O
hall	O
problem	O
rewritten	O
to	O
use	O
suite	B
:	O
from	O
thinkbayes	O
import	O
suite	B
class	O
monty	O
(	O
suite	B
)	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
if	O
hypo	O
==	O
data	O
:	O
return	O
0	O
2.6.	O
the	O
m	O
&	O
m	O
problem	O
17	O
elif	O
hypo	O
==	O
'a	O
'	O
:	O
return	O
0.5	O
else	O
:	O
return	O
1	O
and	O
here	O
’	O
s	O
the	O
code	O
that	O
uses	O
this	O
class	O
:	O
suite	B
=	O
monty	O
(	O
'abc	O
'	O
)	O
suite.update	O
(	O
'b	O
'	O
)	O
suite.print	O
(	O
)	O
you	O
can	O
download	O
this	O
example	O
from	O
http	O
:	O
//thinkbayes.com/monty2.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
2.6	O
the	O
m	O
&	O
m	O
problem	O
we	O
can	O
use	O
the	O
suite	B
framework	O
to	O
solve	O
the	O
m	O
&	O
m	O
problem	O
.	O
writing	O
the	O
likelihood	B
function	I
is	O
tricky	O
,	O
but	O
everything	O
else	O
is	O
straightforward	O
.	O
first	O
i	O
need	O
to	O
encode	O
the	O
color	O
mixes	O
from	O
before	O
and	O
after	O
1995	O
:	O
mix94	O
=	O
dict	O
(	O
brown=30	O
,	O
yellow=20	O
,	O
red=20	O
,	O
green=10	O
,	O
orange=10	O
,	O
tan=10	O
)	O
mix96	O
=	O
dict	O
(	O
blue=24	O
,	O
green=20	O
,	O
orange=16	O
,	O
yellow=14	O
,	O
red=13	O
,	O
brown=13	O
)	O
then	O
i	O
have	O
to	O
encode	O
the	O
hypotheses	O
:	O
hypoa	O
=	O
dict	O
(	O
bag1=mix94	O
,	O
bag2=mix96	O
)	O
hypob	O
=	O
dict	O
(	O
bag1=mix96	O
,	O
bag2=mix94	O
)	O
hypoa	O
represents	O
the	O
hypothesis	O
that	O
bag	O
1	O
is	O
from	O
1994	O
and	O
bag	O
2	O
from	O
1996.	O
hypob	O
is	O
the	O
other	O
way	O
around	O
.	O
next	O
i	O
map	O
from	O
the	O
name	O
of	O
the	O
hypothesis	O
to	O
the	O
representation	O
:	O
hypotheses	O
=	O
dict	O
(	O
a=hypoa	O
,	O
b=hypob	O
)	O
18	O
chapter	O
2.	O
computational	O
statistics	O
and	O
ﬁnally	O
i	O
can	O
write	O
likelihood	B
.	O
in	O
this	O
case	O
the	O
hypothesis	O
,	O
hypo	O
,	O
is	O
a	O
string	O
,	O
either	O
a	O
or	O
b.	O
the	O
data	O
is	O
a	O
tuple	B
that	O
speciﬁes	O
a	O
bag	O
and	O
a	O
color	O
.	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
bag	O
,	O
color	O
=	O
data	O
mix	O
=	O
self.hypotheses	O
[	O
hypo	O
]	O
[	O
bag	O
]	O
like	O
=	O
mix	O
[	O
color	O
]	O
return	O
like	O
here	O
’	O
s	O
the	O
code	O
that	O
creates	O
the	O
suite	B
and	O
updates	O
it	O
:	O
suite	B
=	O
m_and_m	O
(	O
'ab	O
'	O
)	O
suite.update	O
(	O
(	O
'bag1	O
'	O
,	O
'yellow	O
'	O
)	O
)	O
suite.update	O
(	O
(	O
'bag2	O
'	O
,	O
'green	O
'	O
)	O
)	O
suite.print	O
(	O
)	O
and	O
here	O
’	O
s	O
the	O
result	O
:	O
a	O
0.740740740741	O
b	O
0.259259259259	O
the	O
posterior	B
probability	O
of	O
a	O
is	O
approximately	O
20/27	O
,	O
which	O
is	O
what	O
we	O
got	O
before	O
.	O
the	O
code	O
in	O
this	O
section	O
is	O
available	O
from	O
http	O
:	O
//thinkbayes.com/m_and_	O
m.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
2.7	O
discussion	O
this	O
chapter	O
presents	O
the	O
suite	B
class	O
,	O
which	O
encapsulates	O
the	O
bayesian	O
up-	O
date	O
framework	O
.	O
suite	B
is	O
an	O
abstract	B
type	I
,	O
which	O
means	O
that	O
it	O
deﬁnes	O
the	O
interface	B
a	O
suite	B
is	O
supposed	O
to	O
have	O
,	O
but	O
does	O
not	O
provide	O
a	O
complete	O
implementation	B
.	O
the	O
suite	B
interface	O
includes	O
update	O
and	O
likelihood	B
,	O
but	O
the	O
suite	B
class	O
only	O
provides	O
an	O
implementation	B
of	O
update	O
,	O
not	O
likelihood	B
.	O
a	O
concrete	B
type	I
is	O
a	O
class	O
that	O
extends	O
an	O
abstract	O
parent	O
class	O
and	O
provides	O
an	O
implementation	B
of	O
the	O
missing	O
methods	O
.	O
for	O
example	O
,	O
monty	O
extends	O
suite	B
,	O
so	O
it	O
inherits	O
update	O
and	O
provides	O
likelihood	B
.	O
if	O
you	O
are	O
familiar	O
with	O
design	O
patterns	O
,	O
you	O
might	O
recognize	O
this	O
as	O
an	O
example	O
of	O
the	O
template	B
method	I
pattern	I
.	O
you	O
can	O
read	O
about	O
this	O
pattern	O
at	O
http	O
:	O
//en.wikipedia.org/wiki/template_method_pattern	O
.	O
2.8.	O
exercises	O
19	O
most	O
of	O
the	O
examples	O
in	O
the	O
following	O
chapters	O
follow	O
the	O
same	O
pattern	O
;	O
for	O
each	O
problem	O
we	O
deﬁne	O
a	O
new	O
class	O
that	O
extends	O
suite	B
,	O
inherits	O
update	O
,	O
and	O
provides	O
likelihood	B
.	O
in	O
a	O
few	O
cases	O
we	O
override	O
update	O
,	O
usually	O
to	O
improve	O
performance	O
.	O
2.8	O
exercises	O
exercise	O
2.1.	O
in	O
section	O
2.3	O
i	O
said	O
that	O
the	O
solution	O
to	O
the	O
cookie	B
problem	I
general-	O
izes	O
to	O
the	O
case	O
where	O
we	O
draw	O
multiple	O
cookies	O
with	O
replacement	O
.	O
but	O
in	O
the	O
more	O
likely	O
scenario	O
where	O
we	O
eat	O
the	O
cookies	O
we	O
draw	O
,	O
the	O
likelihood	B
of	O
each	O
draw	O
depends	O
on	O
the	O
previous	O
draws	O
.	O
modify	O
the	O
solution	O
in	O
this	O
chapter	O
to	O
handle	O
selection	O
without	O
replacement	O
.	O
hint	O
:	O
add	O
instance	O
variables	O
to	O
cookie	O
to	O
represent	O
the	O
hypothetical	O
state	O
of	O
the	O
bowls	O
,	O
and	O
modify	O
likelihood	B
accordingly	O
.	O
you	O
might	O
want	O
to	O
deﬁne	O
a	O
bowl	O
object	O
.	O
20	O
chapter	O
2.	O
computational	O
statistics	O
chapter	O
3	O
estimation	O
3.1	O
the	O
dice	B
problem	I
suppose	O
i	O
have	O
a	O
box	O
of	O
dice	B
that	O
contains	O
a	O
4-sided	O
die	O
,	O
a	O
6-sided	O
die	O
,	O
an	O
8-sided	O
die	O
,	O
a	O
12-sided	O
die	O
,	O
and	O
a	O
20-sided	O
die	O
.	O
if	O
you	O
have	O
ever	O
played	O
dungeons	O
&	O
dragons	O
,	O
you	O
know	O
what	O
i	O
am	O
talking	O
about	O
.	O
suppose	O
i	O
select	O
a	O
die	O
from	O
the	O
box	O
at	O
random	O
,	O
roll	O
it	O
,	O
and	O
get	O
a	O
6.	O
what	O
is	O
the	O
probability	B
that	O
i	O
rolled	O
each	O
die	O
?	O
let	O
me	O
suggest	O
a	O
three-step	O
strategy	O
for	O
approaching	O
a	O
problem	O
like	O
this	O
.	O
1.	O
choose	O
a	O
representation	O
for	O
the	O
hypotheses	O
.	O
2.	O
choose	O
a	O
representation	O
for	O
the	O
data	O
.	O
3.	O
write	O
the	O
likelihood	B
function	I
.	O
in	O
previous	O
examples	O
i	O
used	O
strings	O
to	O
represent	O
hypotheses	O
and	O
data	O
,	O
but	O
for	O
the	O
die	O
problem	O
i	O
’	O
ll	O
use	O
numbers	O
.	O
speciﬁcally	O
,	O
i	O
’	O
ll	O
use	O
the	O
integers	O
4	O
,	O
6	O
,	O
8	O
,	O
12	O
,	O
and	O
20	O
to	O
represent	O
hypotheses	O
:	O
suite	B
=	O
dice	B
(	O
[	O
4	O
,	O
6	O
,	O
8	O
,	O
12	O
,	O
20	O
]	O
)	O
and	O
integers	O
from	O
1	O
to	O
20	O
for	O
the	O
data	O
.	O
these	O
representations	O
make	O
it	O
easy	O
to	O
write	O
the	O
likelihood	B
function	I
:	O
class	O
dice	B
(	O
suite	B
)	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
if	O
hypo	O
<	O
data	O
:	O
return	O
0	O
else	O
:	O
return	O
1.0/hypo	O
22	O
chapter	O
3.	O
estimation	O
here	O
’	O
s	O
how	O
likelihood	B
works	O
.	O
if	O
hypo	O
<	O
data	O
,	O
that	O
means	O
the	O
roll	O
is	O
greater	O
than	O
the	O
number	O
of	O
sides	O
on	O
the	O
die	O
.	O
that	O
can	O
’	O
t	O
happen	O
,	O
so	O
the	O
likelihood	B
is	O
0.	O
otherwise	O
the	O
question	O
is	O
,	O
“	O
given	O
that	O
there	O
are	O
hypo	O
sides	O
,	O
what	O
is	O
the	O
chance	O
of	O
rolling	O
data	O
?	O
”	O
the	O
answer	O
is	O
1/hypo	O
,	O
regardless	O
of	O
data	O
.	O
here	O
is	O
the	O
statement	O
that	O
does	O
the	O
update	O
(	O
if	O
i	O
roll	O
a	O
6	O
)	O
:	O
suite.update	O
(	O
6	O
)	O
and	O
here	O
is	O
the	O
posterior	B
distribution	I
:	O
4	O
0.0	O
6	O
0.392156862745	O
8	O
0.294117647059	O
12	O
0.196078431373	O
20	O
0.117647058824	O
after	O
we	O
roll	O
a	O
6	O
,	O
the	O
probability	B
for	O
the	O
4-sided	O
die	O
is	O
0.	O
the	O
most	O
likely	O
alternative	O
is	O
the	O
6-sided	O
die	O
,	O
but	O
there	O
is	O
still	O
almost	O
a	O
12	O
%	O
chance	O
for	O
the	O
20-sided	O
die	O
.	O
what	O
if	O
we	O
roll	O
a	O
few	O
more	O
times	O
and	O
get	O
6	O
,	O
8	O
,	O
7	O
,	O
7	O
,	O
5	O
,	O
and	O
4	O
?	O
for	O
roll	O
in	O
[	O
6	O
,	O
8	O
,	O
7	O
,	O
7	O
,	O
5	O
,	O
4	O
]	O
:	O
suite.update	O
(	O
roll	O
)	O
with	O
this	O
data	O
the	O
6-sided	O
die	O
is	O
eliminated	O
,	O
and	O
the	O
8-sided	O
die	O
seems	O
quite	O
likely	O
.	O
here	O
are	O
the	O
results	O
:	O
4	O
0.0	O
6	O
0.0	O
8	O
0.943248453672	O
12	O
0.0552061280613	O
20	O
0.0015454182665	O
now	O
the	O
probability	B
is	O
94	O
%	O
that	O
we	O
are	O
rolling	O
the	O
8-sided	O
die	O
,	O
and	O
less	O
than	O
1	O
%	O
for	O
the	O
20-sided	O
die	O
.	O
the	O
dice	B
problem	I
is	O
based	O
on	O
an	O
example	O
i	O
saw	O
in	O
sanjoy	O
mahajan	O
’	O
s	O
class	O
on	O
bayesian	O
inference	O
.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
section	O
from	O
http	O
:	O
//thinkbayes.com/dice.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
3.2	O
the	O
locomotive	B
problem	I
i	O
found	O
the	O
locomotive	B
problem	I
in	O
frederick	O
mosteller	O
’	O
s	O
,	O
fifty	O
challenging	O
problems	O
in	O
probability	B
with	O
solutions	O
(	O
dover	O
,	O
1987	O
)	O
:	O
3.2.	O
the	O
locomotive	B
problem	I
23	O
figure	O
3.1	O
:	O
posterior	B
distribution	I
for	O
the	O
locomotive	B
problem	I
,	O
based	O
on	O
a	O
uniform	O
prior	O
.	O
“	O
a	O
railroad	O
numbers	O
its	O
locomotives	O
in	O
order	O
1..n.	O
one	O
day	O
you	O
see	O
a	O
locomotive	O
with	O
the	O
number	O
60.	O
estimate	O
how	O
many	O
loco-	O
motives	O
the	O
railroad	O
has.	O
”	O
based	O
on	O
this	O
observation	O
,	O
we	O
know	O
the	O
railroad	O
has	O
60	O
or	O
more	O
locomo-	O
tives	O
.	O
but	O
how	O
many	O
more	O
?	O
to	O
apply	O
bayesian	O
reasoning	O
,	O
we	O
can	O
break	O
this	O
problem	O
into	O
two	O
steps	O
:	O
1.	O
what	O
did	O
we	O
know	O
about	O
n	O
before	O
we	O
saw	O
the	O
data	O
?	O
2.	O
for	O
any	O
given	O
value	O
of	O
n	O
,	O
what	O
is	O
the	O
likelihood	B
of	O
seeing	O
the	O
data	O
(	O
a	O
locomotive	O
with	O
number	O
60	O
)	O
?	O
the	O
answer	O
to	O
the	O
ﬁrst	O
question	O
is	O
the	O
prior	B
.	O
the	O
answer	O
to	O
the	O
second	O
is	O
the	O
likelihood	B
.	O
we	O
don	O
’	O
t	O
have	O
much	O
basis	O
to	O
choose	O
a	O
prior	B
,	O
but	O
we	O
can	O
start	O
with	O
something	O
simple	O
and	O
then	O
consider	O
alternatives	O
.	O
let	O
’	O
s	O
assume	O
that	O
n	O
is	O
equally	O
likely	O
to	O
be	O
any	O
value	O
from	O
1	O
to	O
1000.	O
hypos	O
=	O
xrange	O
(	O
1	O
,	O
1001	O
)	O
now	O
all	O
we	O
need	O
is	O
a	O
likelihood	B
function	I
.	O
in	O
a	O
hypothetical	O
ﬂeet	O
of	O
n	O
lo-	O
comotives	O
,	O
what	O
is	O
the	O
probability	B
that	O
we	O
would	O
see	O
number	O
60	O
?	O
if	O
we	O
assume	O
that	O
there	O
is	O
only	O
one	O
train-operating	O
company	O
(	O
or	O
only	O
one	O
we	O
care	O
about	O
)	O
and	O
that	O
we	O
are	O
equally	O
likely	O
to	O
see	O
any	O
of	O
its	O
locomotives	O
,	O
then	O
the	O
chance	O
of	O
seeing	O
any	O
particular	O
locomotive	O
is	O
1/n	O
.	O
here	O
’	O
s	O
the	O
likelihood	B
function	I
:	O
02004006008001000number	O
of	O
trains0.0000.0010.0020.0030.0040.0050.006probability	O
24	O
chapter	O
3.	O
estimation	O
class	O
train	O
(	O
suite	B
)	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
if	O
hypo	O
<	O
data	O
:	O
return	O
0	O
else	O
:	O
return	O
1.0/hypo	O
this	O
might	O
look	O
familiar	O
;	O
the	O
likelihood	B
functions	O
for	O
the	O
locomotive	O
prob-	O
lem	O
and	O
the	O
dice	B
problem	I
are	O
identical	O
.	O
here	O
’	O
s	O
the	O
update	O
:	O
suite	B
=	O
train	O
(	O
hypos	O
)	O
suite.update	O
(	O
60	O
)	O
there	O
are	O
too	O
many	O
hypotheses	O
to	O
print	O
,	O
so	O
i	O
plotted	O
the	O
results	O
in	O
figure	O
3.1.	O
not	O
surprisingly	O
,	O
all	O
values	O
of	O
n	O
below	O
60	O
have	O
been	O
eliminated	O
.	O
the	O
most	O
likely	O
value	O
,	O
if	O
you	O
had	O
to	O
guess	O
,	O
is	O
60.	O
that	O
might	O
not	O
seem	O
like	O
a	O
very	O
good	O
guess	O
;	O
after	O
all	O
,	O
what	O
are	O
the	O
chances	O
that	O
you	O
just	O
happened	O
to	O
see	O
the	O
train	O
with	O
the	O
highest	O
number	O
?	O
nevertheless	O
,	O
if	O
you	O
want	O
to	O
maxi-	O
mize	O
the	O
chance	O
of	O
getting	O
the	O
answer	O
exactly	O
right	O
,	O
you	O
should	O
guess	O
60.	O
but	O
maybe	O
that	O
’	O
s	O
not	O
the	O
right	O
goal	O
.	O
an	O
alternative	O
is	O
to	O
compute	O
the	O
mean	O
of	O
the	O
posterior	B
distribution	I
:	O
def	O
mean	O
(	O
suite	B
)	O
:	O
total	O
=	O
0	O
for	O
hypo	O
,	O
prob	O
in	O
suite.items	O
(	O
)	O
:	O
total	O
+=	O
hypo	O
*	O
prob	O
return	O
total	O
print	O
mean	O
(	O
suite	B
)	O
or	O
you	O
could	O
use	O
the	O
very	O
similar	O
method	O
provided	O
by	O
pmf	O
:	O
print	O
suite.mean	O
(	O
)	O
the	O
mean	O
of	O
the	O
posterior	B
is	O
333	O
,	O
so	O
that	O
might	O
be	O
a	O
good	O
guess	O
if	O
you	O
wanted	O
to	O
minimize	O
error	B
.	O
if	O
you	O
played	O
this	O
guessing	O
game	O
over	O
and	O
over	O
,	O
using	O
the	O
mean	O
of	O
the	O
posterior	B
as	O
your	O
estimate	O
would	O
minimize	O
the	O
mean	B
squared	I
error	I
over	O
the	O
long	O
run	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/	O
wiki/minimum_mean_square_error	O
)	O
.	O
you	O
can	O
download	O
this	O
example	O
from	O
http	O
:	O
//thinkbayes.com/train.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
3.3.	O
what	O
about	O
that	O
prior	B
?	O
25	O
3.3	O
what	O
about	O
that	O
prior	B
?	O
to	O
make	O
any	O
progress	O
on	O
the	O
locomotive	B
problem	I
we	O
had	O
to	O
make	O
assump-	O
tions	O
,	O
and	O
some	O
of	O
them	O
were	O
pretty	O
arbitrary	O
.	O
in	O
particular	O
,	O
we	O
chose	O
a	O
uniform	O
prior	O
from	O
1	O
to	O
1000	O
,	O
without	O
much	O
justiﬁcation	O
for	O
choosing	O
1000	O
,	O
or	O
for	O
choosing	O
a	O
uniform	B
distribution	I
.	O
it	O
is	O
not	O
crazy	O
to	O
believe	O
that	O
a	O
railroad	O
company	O
might	O
operate	O
1000	O
loco-	O
motives	O
,	O
but	O
a	O
reasonable	O
person	O
might	O
guess	O
more	O
or	O
fewer	O
.	O
so	O
we	O
might	O
wonder	O
whether	O
the	O
posterior	B
distribution	I
is	O
sensitive	O
to	O
these	O
assumptions	O
.	O
with	O
so	O
little	O
data—only	O
one	O
observation—it	O
probably	O
is	O
.	O
recall	O
that	O
with	O
a	O
uniform	O
prior	O
from	O
1	O
to	O
1000	O
,	O
the	O
mean	O
of	O
the	O
posterior	B
is	O
333.	O
with	O
an	O
upper	O
bound	O
of	O
500	O
,	O
we	O
get	O
a	O
posterior	B
mean	O
of	O
207	O
,	O
and	O
with	O
an	O
upper	O
bound	O
of	O
2000	O
,	O
the	O
posterior	B
mean	O
is	O
552.	O
so	O
that	O
’	O
s	O
bad	O
.	O
there	O
are	O
two	O
ways	O
to	O
proceed	O
:	O
•	O
get	O
more	O
data	O
.	O
•	O
get	O
more	O
background	O
information	O
.	O
with	O
more	O
data	O
,	O
posterior	B
distributions	O
based	O
on	O
different	O
priors	O
tend	O
to	O
converge	O
.	O
for	O
example	O
,	O
suppose	O
that	O
in	O
addition	O
to	O
train	O
60	O
we	O
also	O
see	O
trains	O
30	O
and	O
90.	O
we	O
can	O
update	O
the	O
distribution	B
like	O
this	O
:	O
for	O
data	O
in	O
[	O
60	O
,	O
30	O
,	O
90	O
]	O
:	O
suite.update	O
(	O
data	O
)	O
with	O
these	O
data	O
,	O
the	O
means	O
of	O
the	O
posteriors	O
are	O
upper	O
posterior	B
bound	O
mean	O
500	O
1000	O
2000	O
152	O
164	O
171	O
so	O
the	O
differences	O
are	O
smaller	O
.	O
3.4	O
an	O
alternative	O
prior	B
if	O
more	O
data	O
are	O
not	O
available	O
,	O
another	O
option	O
is	O
to	O
improve	O
the	O
priors	O
by	O
gathering	O
more	O
background	O
information	O
.	O
it	O
is	O
probably	O
not	O
reasonable	O
to	O
as-	O
sume	O
that	O
a	O
train-operating	O
company	O
with	O
1000	O
locomotives	O
is	O
just	O
as	O
likely	O
as	O
a	O
company	O
with	O
only	O
1	O
.	O
26	O
chapter	O
3.	O
estimation	O
figure	O
3.2	O
:	O
posterior	B
distribution	I
based	O
on	O
a	O
power	B
law	I
prior	O
,	O
compared	O
to	O
a	O
uniform	O
prior	O
.	O
with	O
some	O
effort	O
,	O
we	O
could	O
probably	O
ﬁnd	O
a	O
list	O
of	O
companies	O
that	O
operate	O
locomotives	O
in	O
the	O
area	O
of	O
observation	O
.	O
or	O
we	O
could	O
interview	O
an	O
expert	O
in	O
rail	O
shipping	O
to	O
gather	O
information	O
about	O
the	O
typical	O
size	O
of	O
companies	O
.	O
but	O
even	O
without	O
getting	O
into	O
the	O
speciﬁcs	O
of	O
railroad	O
economics	O
,	O
we	O
can	O
make	O
some	O
educated	O
guesses	O
.	O
in	O
most	O
ﬁelds	O
,	O
there	O
are	O
many	O
small	O
compa-	O
nies	O
,	O
fewer	O
medium-sized	O
companies	O
,	O
and	O
only	O
one	O
or	O
two	O
very	O
large	O
com-	O
panies	O
.	O
in	O
fact	O
,	O
the	O
distribution	B
of	O
company	O
sizes	O
tends	O
to	O
follow	O
a	O
power	B
law	I
,	O
as	O
robert	O
axtell	O
reports	O
in	O
science	O
(	O
see	O
http	O
:	O
//www.sciencemag.org/	O
content/293/5536/1818.full.pdf	O
)	O
.	O
this	O
law	O
suggests	O
that	O
if	O
there	O
are	O
1000	O
companies	O
with	O
fewer	O
than	O
10	O
loco-	O
motives	O
,	O
there	O
might	O
be	O
100	O
companies	O
with	O
100	O
locomotives	O
,	O
10	O
companies	O
with	O
1000	O
,	O
and	O
possibly	O
one	O
company	O
with	O
10,000	O
locomotives	O
.	O
mathematically	O
,	O
a	O
power	B
law	I
means	O
that	O
the	O
number	O
of	O
companies	O
with	O
a	O
given	O
size	O
is	O
inversely	O
proportional	O
to	O
size	O
,	O
or	O
(	O
cid:18	O
)	O
1	O
(	O
cid:19	O
)	O
α	O
pmf	O
(	O
x	O
)	O
∝	O
x	O
where	O
pmf	O
(	O
x	O
)	O
is	O
the	O
probability	B
mass	I
function	I
of	O
x	O
and	O
α	O
is	O
a	O
parameter	B
that	O
is	O
often	O
near	O
1.	O
we	O
can	O
construct	O
a	O
power	B
law	I
prior	O
like	O
this	O
:	O
class	O
train	O
(	O
dice	B
)	O
:	O
02004006008001000number	O
of	O
trains0.0000.0020.0040.0060.0080.0100.0120.0140.0160.018probabilityuniformpower	O
law	O
3.5.	O
credible	O
intervals	O
27	O
def	O
__init__	O
(	O
self	O
,	O
hypos	O
,	O
alpha=1.0	O
)	O
:	O
pmf.__init__	O
(	O
self	O
)	O
for	O
hypo	O
in	O
hypos	O
:	O
self.set	O
(	O
hypo	O
,	O
hypo**	O
(	O
-alpha	O
)	O
)	O
self.normalize	O
(	O
)	O
and	O
here	O
’	O
s	O
the	O
code	O
that	O
constructs	O
the	O
prior	B
:	O
hypos	O
=	O
range	O
(	O
1	O
,	O
1001	O
)	O
suite	B
=	O
train	O
(	O
hypos	O
)	O
again	O
,	O
the	O
upper	O
bound	O
is	O
arbitrary	O
,	O
but	O
with	O
a	O
power	B
law	I
prior	O
,	O
the	O
poste-	O
rior	O
is	O
less	O
sensitive	O
to	O
this	O
choice	O
.	O
figure	O
3.2	O
shows	O
the	O
new	O
posterior	B
based	O
on	O
the	O
power	B
law	I
,	O
compared	O
to	O
the	O
posterior	B
based	O
on	O
the	O
uniform	O
prior	O
.	O
using	O
the	O
background	O
information	O
represented	O
in	O
the	O
power	B
law	I
prior	O
,	O
we	O
can	O
all	O
but	O
eliminate	O
values	O
of	O
n	O
greater	O
than	O
700.	O
if	O
we	O
start	O
with	O
this	O
prior	B
and	O
observe	O
trains	O
30	O
,	O
60	O
,	O
and	O
90	O
,	O
the	O
means	O
of	O
the	O
posteriors	O
are	O
upper	O
posterior	B
bound	O
mean	O
500	O
1000	O
2000	O
131	O
133	O
134	O
now	O
the	O
differences	O
are	O
much	O
smaller	O
.	O
in	O
fact	O
,	O
with	O
an	O
arbitrarily	O
large	O
up-	O
per	O
bound	O
,	O
the	O
mean	O
converges	O
on	O
134.	O
so	O
the	O
power	B
law	I
prior	O
is	O
more	O
realistic	O
,	O
because	O
it	O
is	O
based	O
on	O
general	O
in-	O
formation	O
about	O
the	O
size	O
of	O
companies	O
,	O
and	O
it	O
behaves	O
better	O
in	O
practice	O
.	O
you	O
can	O
download	O
the	O
examples	O
in	O
this	O
section	O
from	O
http	O
:	O
//thinkbayes	O
.	O
com/train3.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
3.5	O
credible	O
intervals	O
once	O
you	O
have	O
computed	O
a	O
posterior	B
distribution	I
,	O
it	O
is	O
often	O
useful	O
to	O
sum-	O
marize	O
the	O
results	O
with	O
a	O
single	O
point	O
estimate	O
or	O
an	O
interval	O
.	O
for	O
point	O
es-	O
timates	O
it	O
is	O
common	O
to	O
use	O
the	O
mean	O
,	O
median	B
,	O
or	O
the	O
value	O
with	O
maximum	B
likelihood	I
.	O
28	O
chapter	O
3.	O
estimation	O
for	O
intervals	O
we	O
usually	O
report	O
two	O
values	O
computed	O
so	O
that	O
there	O
is	O
a	O
90	O
%	O
chance	O
that	O
the	O
unknown	O
value	O
falls	O
between	O
them	O
(	O
or	O
any	O
other	O
probabil-	O
ity	O
)	O
.	O
these	O
values	O
deﬁne	O
a	O
credible	B
interval	I
.	O
a	O
simple	O
way	O
to	O
compute	O
a	O
credible	B
interval	I
is	O
to	O
add	O
up	O
the	O
probabilities	O
in	O
the	O
posterior	B
distribution	I
and	O
record	O
the	O
values	O
that	O
correspond	O
to	O
prob-	O
abilities	O
5	O
%	O
and	O
95	O
%	O
.	O
in	O
other	O
words	O
,	O
the	O
5th	O
and	O
95th	O
percentiles	O
.	O
thinkbayes	O
provides	O
a	O
function	O
that	O
computes	O
percentiles	O
:	O
def	O
percentile	B
(	O
pmf	O
,	O
percentage	O
)	O
:	O
p	O
=	O
percentage	O
/	O
100.0	O
total	O
=	O
0	O
for	O
val	O
,	O
prob	O
in	O
pmf.items	O
(	O
)	O
:	O
total	O
+=	O
prob	O
if	O
total	O
>	O
=	O
p	O
:	O
return	O
val	O
and	O
here	O
’	O
s	O
the	O
code	O
that	O
uses	O
it	O
:	O
interval	O
=	O
percentile	B
(	O
suite	B
,	O
5	O
)	O
,	O
percentile	B
(	O
suite	B
,	O
95	O
)	O
print	O
interval	O
for	O
the	O
previous	O
example—the	O
locomotive	B
problem	I
with	O
a	O
power	B
law	I
prior	O
and	O
three	O
trains—the	O
90	O
%	O
credible	B
interval	I
is	O
(	O
91	O
,	O
243	O
)	O
.	O
the	O
width	O
of	O
this	O
range	O
suggests	O
,	O
correctly	O
,	O
that	O
we	O
are	O
still	O
quite	O
uncertain	O
about	O
how	O
many	O
locomotives	O
there	O
are	O
.	O
3.6	O
cumulative	O
distribution	O
functions	O
in	O
the	O
previous	O
section	O
we	O
computed	O
percentiles	O
by	O
iterating	O
through	O
the	O
values	O
and	O
probabilities	O
in	O
a	O
pmf	O
.	O
if	O
we	O
need	O
to	O
compute	O
more	O
than	O
a	O
few	O
percentiles	O
,	O
it	O
is	O
more	O
efﬁcient	O
to	O
use	O
a	O
cumulative	B
distribution	I
function	I
,	O
or	O
cdf	O
.	O
cdfs	O
and	O
pmfs	O
are	O
equivalent	O
in	O
the	O
sense	O
that	O
they	O
contain	O
the	O
same	O
infor-	O
mation	O
about	O
the	O
distribution	B
,	O
and	O
you	O
can	O
always	O
convert	O
from	O
one	O
to	O
the	O
other	O
.	O
the	O
advantage	O
of	O
the	O
cdf	O
is	O
that	O
you	O
can	O
compute	O
percentiles	O
more	O
efﬁciently	O
.	O
thinkbayes	O
provides	O
a	O
cdf	O
class	O
that	O
represents	O
a	O
cumulative	B
distribution	I
function	I
.	O
pmf	O
provides	O
a	O
method	O
that	O
makes	O
the	O
corresponding	O
cdf	O
:	O
cdf	O
=	O
suite.makecdf	O
(	O
)	O
and	O
cdf	O
provides	O
a	O
function	O
named	O
percentile	B
3.7.	O
the	O
german	O
tank	O
problem	O
29	O
interval	O
=	O
cdf.percentile	O
(	O
5	O
)	O
,	O
cdf.percentile	O
(	O
95	O
)	O
converting	O
from	O
a	O
pmf	O
to	O
a	O
cdf	O
takes	O
time	O
proportional	O
to	O
the	O
number	O
of	O
values	O
,	O
len	O
(	O
pmf	O
)	O
.	O
the	O
cdf	O
stores	O
the	O
values	O
and	O
probabilities	O
in	O
sorted	O
lists	O
,	O
so	O
looking	O
up	O
a	O
probability	B
to	O
get	O
the	O
corresponding	O
value	O
takes	O
“	O
log	O
time	O
”	O
:	O
that	O
is	O
,	O
time	O
proportional	O
to	O
the	O
logarithm	B
of	O
the	O
number	O
of	O
values	O
.	O
looking	O
up	O
a	O
value	O
to	O
get	O
the	O
corresponding	O
probability	B
is	O
also	O
logarithmic	O
,	O
so	O
cdfs	O
are	O
efﬁcient	O
for	O
many	O
calculations	O
.	O
the	O
examples	O
in	O
this	O
section	O
are	O
in	O
http	O
:	O
//thinkbayes.com/train3.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
3.7	O
the	O
german	O
tank	O
problem	O
during	O
world	O
war	O
ii	O
,	O
the	O
economic	O
warfare	O
division	O
of	O
the	O
american	O
em-	O
bassy	O
in	O
london	O
used	O
statistical	O
analysis	O
to	O
estimate	O
german	O
production	O
of	O
tanks	O
and	O
other	O
equipment.1	O
the	O
western	O
allies	O
had	O
captured	O
log	O
books	O
,	O
inventories	O
,	O
and	O
repair	O
records	O
that	O
included	O
chassis	O
and	O
engine	O
serial	O
numbers	O
for	O
individual	O
tanks	O
.	O
analysis	O
of	O
these	O
records	O
indicated	O
that	O
serial	O
numbers	O
were	O
allocated	O
by	O
manufacturer	O
and	O
tank	O
type	O
in	O
blocks	O
of	O
100	O
numbers	O
,	O
that	O
numbers	O
in	O
each	O
block	O
were	O
used	O
sequentially	O
,	O
and	O
that	O
not	O
all	O
numbers	O
in	O
each	O
block	O
were	O
used	O
.	O
so	O
the	O
problem	O
of	O
estimating	O
german	O
tank	O
production	O
could	O
be	O
re-	O
duced	O
,	O
within	O
each	O
block	O
of	O
100	O
numbers	O
,	O
to	O
a	O
form	O
of	O
the	O
locomotive	O
prob-	O
lem	O
.	O
based	O
on	O
this	O
insight	O
,	O
american	O
and	O
british	O
analysts	O
produced	O
estimates	O
substantially	O
lower	O
than	O
estimates	O
from	O
other	O
forms	O
of	O
intelligence	O
.	O
and	O
after	O
the	O
war	O
,	O
records	O
indicated	O
that	O
they	O
were	O
substantially	O
more	O
accurate	O
.	O
they	O
performed	O
similar	O
analyses	O
for	O
tires	O
,	O
trucks	O
,	O
rockets	O
,	O
and	O
other	O
equip-	O
ment	O
,	O
yielding	O
accurate	O
and	O
actionable	O
economic	O
intelligence	O
.	O
the	O
german	O
tank	O
problem	O
is	O
historically	O
interesting	O
;	O
it	O
is	O
also	O
a	O
nice	O
example	O
of	O
real-world	O
application	O
of	O
statistical	O
estimation	O
.	O
so	O
far	O
many	O
of	O
the	O
exam-	O
ples	O
in	O
this	O
book	O
have	O
been	O
toy	O
problems	O
,	O
but	O
it	O
will	O
not	O
be	O
long	O
before	O
we	O
start	O
solving	O
real	O
problems	O
.	O
i	O
think	O
it	O
is	O
an	O
advantage	O
of	O
bayesian	O
analysis	O
,	O
especially	O
with	O
the	O
computational	O
approach	O
we	O
are	O
taking	O
,	O
that	O
it	O
provides	O
such	O
a	O
short	O
path	O
from	O
a	O
basic	O
introduction	O
to	O
the	O
research	O
frontier	O
.	O
1ruggles	O
and	O
brodie	O
,	O
“	O
an	O
empirical	O
approach	O
to	O
economic	O
intelligence	O
in	O
world	O
war	O
ii	O
,	O
”	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
,	O
vol	O
.	O
42	O
,	O
no	O
.	O
237	O
(	O
march	O
1947	O
)	O
.	O
30	O
3.8	O
discussion	O
chapter	O
3.	O
estimation	O
among	O
bayesians	O
,	O
there	O
are	O
two	O
approaches	O
to	O
choosing	O
prior	B
distributions	O
.	O
some	O
recommend	O
choosing	O
the	O
prior	B
that	O
best	O
represents	O
background	O
infor-	O
mation	O
about	O
the	O
problem	O
;	O
in	O
that	O
case	O
the	O
prior	B
is	O
said	O
to	O
be	O
informative	O
.	O
the	O
problem	O
with	O
using	O
an	O
informative	B
prior	I
is	O
that	O
people	O
might	O
use	O
dif-	O
ferent	O
background	O
information	O
(	O
or	O
interpret	O
it	O
differently	O
)	O
.	O
so	O
informative	O
priors	O
often	O
seem	O
subjective	O
.	O
the	O
alternative	O
is	O
a	O
so-called	O
uninformative	B
prior	I
,	O
which	O
is	O
intended	O
to	O
be	O
as	O
unrestricted	O
as	O
possible	O
,	O
in	O
order	O
to	O
let	O
the	O
data	O
speak	O
for	O
themselves	O
.	O
in	O
some	O
cases	O
you	O
can	O
identify	O
a	O
unique	O
prior	B
that	O
has	O
some	O
desirable	O
property	O
,	O
like	O
representing	O
minimal	O
prior	B
information	O
about	O
the	O
estimated	O
quantity	O
.	O
uninformative	O
priors	O
are	O
appealing	O
because	O
they	O
seem	O
more	O
objective	O
.	O
but	O
i	O
am	O
generally	O
in	O
favor	O
of	O
using	O
informative	O
priors	O
.	O
why	O
?	O
first	O
,	O
bayesian	O
analysis	O
is	O
always	O
based	O
on	O
modeling	B
decisions	O
.	O
choosing	O
the	O
prior	B
is	O
one	O
of	O
those	O
decisions	O
,	O
but	O
it	O
is	O
not	O
the	O
only	O
one	O
,	O
and	O
it	O
might	O
not	O
even	O
be	O
the	O
most	O
subjective	O
.	O
so	O
even	O
if	O
an	O
uninformative	B
prior	I
is	O
more	O
objective	O
,	O
the	O
entire	O
analysis	O
is	O
still	O
subjective	O
.	O
also	O
,	O
for	O
most	O
practical	O
problems	O
,	O
you	O
are	O
likely	O
to	O
be	O
in	O
one	O
of	O
two	O
regimes	O
:	O
either	O
you	O
have	O
a	O
lot	O
of	O
data	O
or	O
not	O
very	O
much	O
.	O
if	O
you	O
have	O
a	O
lot	O
of	O
data	O
,	O
the	O
choice	O
of	O
the	O
prior	B
doesn	O
’	O
t	O
matter	O
very	O
much	O
;	O
informative	O
and	O
uninforma-	O
tive	O
priors	O
yield	O
almost	O
the	O
same	O
results	O
.	O
we	O
’	O
ll	O
see	O
an	O
example	O
like	O
this	O
in	O
the	O
next	O
chapter	O
.	O
but	O
if	O
,	O
as	O
in	O
the	O
locomotive	B
problem	I
,	O
you	O
don	O
’	O
t	O
have	O
much	O
data	O
,	O
using	O
rele-	O
vant	O
background	O
information	O
(	O
like	O
the	O
power	B
law	I
distribution	O
)	O
makes	O
a	O
big	O
difference	O
.	O
and	O
if	O
,	O
as	O
in	O
the	O
german	O
tank	O
problem	O
,	O
you	O
have	O
to	O
make	O
life-and-death	O
decisions	O
based	O
on	O
your	O
results	O
,	O
you	O
should	O
probably	O
use	O
all	O
of	O
the	O
infor-	O
mation	O
at	O
your	O
disposal	O
,	O
rather	O
than	O
maintaining	O
the	O
illusion	O
of	O
objectivity	B
by	O
pretending	O
to	O
know	O
less	O
than	O
you	O
do	O
.	O
3.9	O
exercises	O
exercise	O
3.1.	O
to	O
write	O
a	O
likelihood	B
function	I
for	O
the	O
locomotive	B
problem	I
,	O
we	O
had	O
to	O
answer	O
this	O
question	O
:	O
“	O
if	O
the	O
railroad	O
has	O
n	O
locomotives	O
,	O
what	O
is	O
the	O
probability	B
that	O
we	O
see	O
number	O
60	O
?	O
”	O
3.9.	O
exercises	O
31	O
the	O
answer	O
depends	O
on	O
what	O
sampling	O
process	B
we	O
use	O
when	O
we	O
observe	O
the	O
loco-	O
motive	O
.	O
in	O
this	O
chapter	O
,	O
i	O
resolved	O
the	O
ambiguity	O
by	O
specifying	O
that	O
there	O
is	O
only	O
one	O
train-operating	O
company	O
(	O
or	O
only	O
one	O
that	O
we	O
care	O
about	O
)	O
.	O
but	O
suppose	O
instead	O
that	O
there	O
are	O
many	O
companies	O
with	O
different	O
numbers	O
of	O
trains	O
.	O
and	O
suppose	O
that	O
you	O
are	O
equally	O
likely	O
to	O
see	O
any	O
train	O
operated	O
by	O
any	O
company	O
.	O
in	O
that	O
case	O
,	O
the	O
likelihood	B
function	I
is	O
different	O
because	O
you	O
are	O
more	O
likely	O
to	O
see	O
a	O
train	O
operated	O
by	O
a	O
large	O
company	O
.	O
as	O
an	O
exercise	O
,	O
implement	O
the	O
likelihood	B
function	I
for	O
this	O
variation	O
of	O
the	O
locomotive	B
problem	I
,	O
and	O
compare	O
the	O
results	O
.	O
32	O
chapter	O
3.	O
estimation	O
chapter	O
4	O
more	O
estimation	O
4.1	O
the	O
euro	O
problem	O
in	O
information	O
theory	O
,	O
inference	O
,	O
and	O
learning	O
algorithms	O
,	O
david	O
mackay	O
poses	O
this	O
problem	O
:	O
a	O
statistical	O
statement	O
appeared	O
in	O
“	O
the	O
guardian	O
''	O
on	O
friday	O
january	O
4	O
,	O
2002	O
:	O
when	O
spun	O
on	O
edge	O
250	O
times	O
,	O
a	O
belgian	O
one-euro	O
coin	O
came	O
up	O
heads	O
140	O
times	O
and	O
tails	O
110	O
.	O
‘	O
it	O
looks	O
very	O
suspicious	O
to	O
me	O
,	O
’	O
said	O
barry	O
blight	O
,	O
a	O
statistics	O
lecturer	O
at	O
the	O
london	O
school	O
of	O
economics	O
.	O
‘	O
if	O
the	O
coin	O
were	O
unbiased	O
,	O
the	O
chance	O
of	O
getting	O
a	O
result	O
as	O
extreme	O
as	O
that	O
would	O
be	O
less	O
than	O
7	O
%	O
.	O
’	O
but	O
do	O
these	O
data	O
give	O
evidence	B
that	O
the	O
coin	O
is	O
biased	O
rather	O
than	O
fair	O
?	O
to	O
answer	O
that	O
question	O
,	O
we	O
’	O
ll	O
proceed	O
in	O
two	O
steps	O
.	O
the	O
ﬁrst	O
is	O
to	O
esti-	O
mate	O
the	O
probability	B
that	O
the	O
coin	O
lands	O
face	O
up	O
.	O
the	O
second	O
is	O
to	O
evaluate	O
whether	O
the	O
data	O
support	O
the	O
hypothesis	O
that	O
the	O
coin	O
is	O
biased	O
.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
section	O
from	O
http	O
:	O
//thinkbayes.com/	O
euro.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3.	O
any	O
given	O
coin	O
has	O
some	O
probability	B
,	O
x	O
,	O
of	O
landing	O
heads	O
up	O
when	O
spun	O
on	O
edge	O
.	O
it	O
seems	O
reasonable	O
to	O
believe	O
that	O
the	O
value	O
of	O
x	O
depends	O
on	O
some	O
physical	O
characteristics	O
of	O
the	O
coin	O
,	O
primarily	O
the	O
distribution	B
of	O
weight	O
.	O
34	O
chapter	O
4.	O
more	O
estimation	O
figure	O
4.1	O
:	O
posterior	B
distribution	I
for	O
the	O
euro	O
problem	O
on	O
a	O
uniform	O
prior	O
.	O
if	O
a	O
coin	O
is	O
perfectly	O
balanced	O
,	O
we	O
expect	O
x	O
to	O
be	O
close	O
to	O
50	O
%	O
,	O
but	O
for	O
a	O
lop-	O
sided	O
coin	O
,	O
x	O
might	O
be	O
substantially	O
different	O
.	O
we	O
can	O
use	O
bayes	O
’	O
s	O
theorem	O
and	O
the	O
observed	O
data	O
to	O
estimate	O
x.	O
let	O
’	O
s	O
deﬁne	O
101	O
hypotheses	O
,	O
where	O
hx	O
is	O
the	O
hypothesis	O
that	O
the	O
probability	B
of	O
heads	O
is	O
x	O
%	O
,	O
for	O
values	O
from	O
0	O
to	O
100.	O
i	O
’	O
ll	O
start	O
with	O
a	O
uniform	O
prior	O
where	O
the	O
probability	B
of	O
hx	O
is	O
the	O
same	O
for	O
all	O
x.	O
we	O
’	O
ll	O
come	O
back	O
later	O
to	O
consider	O
other	O
priors	O
.	O
the	O
likelihood	B
function	I
is	O
relatively	O
easy	O
:	O
if	O
hx	O
is	O
true	O
,	O
the	O
probability	B
of	O
heads	O
is	O
x/100	O
and	O
the	O
probability	B
of	O
tails	O
is	O
1	O
−	O
x/100	O
.	O
class	O
euro	O
(	O
suite	B
)	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
hypo	O
if	O
data	O
==	O
'h	O
'	O
:	O
return	O
x/100.0	O
else	O
:	O
return	O
1	O
-	O
x/100.0	O
here	O
’	O
s	O
the	O
code	O
that	O
makes	O
the	O
suite	B
and	O
updates	O
it	O
:	O
suite	B
=	O
euro	O
(	O
xrange	O
(	O
0	O
,	O
101	O
)	O
)	O
dataset	O
=	O
'h	O
'	O
*	O
140	O
+	O
't	O
'	O
*	O
110	O
for	O
data	O
in	O
dataset	O
:	O
suite.update	O
(	O
data	O
)	O
the	O
result	O
is	O
in	O
figure	O
4.1	O
.	O
020406080100x0.000.020.040.060.080.100.120.14probabilityuniform	O
4.2.	O
summarizing	O
the	O
posterior	B
35	O
4.2	O
summarizing	O
the	O
posterior	B
again	O
,	O
there	O
are	O
several	O
ways	O
to	O
summarize	O
the	O
posterior	B
distribution	I
.	O
one	O
option	O
is	O
to	O
ﬁnd	O
the	O
most	O
likely	O
value	O
in	O
the	O
posterior	B
distribution	I
.	O
thinkbayes	O
provides	O
a	O
function	O
that	O
does	O
that	O
:	O
def	O
maximumlikelihood	O
(	O
pmf	O
)	O
:	O
''	O
''	O
''	O
returns	O
the	O
value	O
with	O
the	O
highest	O
probability	B
.	O
''	O
''	O
''	O
prob	O
,	O
val	O
=	O
max	O
(	O
(	O
prob	O
,	O
val	O
)	O
for	O
val	O
,	O
prob	O
in	O
pmf.items	O
(	O
)	O
)	O
return	O
val	O
in	O
this	O
case	O
the	O
result	O
is	O
56	O
,	O
which	O
is	O
also	O
the	O
observed	O
percentage	O
of	O
heads	O
,	O
140/250	O
=	O
56	O
%	O
.	O
so	O
that	O
suggests	O
(	O
correctly	O
)	O
that	O
the	O
observed	O
percentage	O
is	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
the	O
population	O
.	O
we	O
might	O
also	O
summarize	O
the	O
posterior	B
by	O
computing	O
the	O
mean	O
and	O
me-	O
dian	O
:	O
print	O
'mean	O
'	O
,	O
suite.mean	O
(	O
)	O
print	O
'median	O
'	O
,	O
thinkbayes.percentile	O
(	O
suite	B
,	O
50	O
)	O
the	O
mean	O
is	O
55.95	O
;	O
the	O
median	B
is	O
56.	O
finally	O
,	O
we	O
can	O
compute	O
a	O
credible	B
interval	I
:	O
print	O
'ci	O
'	O
,	O
thinkbayes.credibleinterval	O
(	O
suite	B
,	O
90	O
)	O
the	O
result	O
is	O
(	O
51	O
,	O
61	O
)	O
.	O
now	O
,	O
getting	O
back	O
to	O
the	O
original	O
question	O
,	O
we	O
would	O
like	O
to	O
know	O
whether	O
the	O
coin	O
is	O
fair	O
.	O
we	O
observe	O
that	O
the	O
posterior	B
credible	O
interval	O
does	O
not	O
include	O
50	O
%	O
,	O
which	O
suggests	O
that	O
the	O
coin	O
is	O
not	O
fair	O
.	O
but	O
that	O
is	O
not	O
exactly	O
the	O
question	O
we	O
started	O
with	O
.	O
mackay	O
asked	O
,	O
“	O
do	O
these	O
data	O
give	O
evidence	B
that	O
the	O
coin	O
is	O
biased	O
rather	O
than	O
fair	O
?	O
”	O
to	O
answer	O
that	O
question	O
,	O
we	O
will	O
have	O
to	O
be	O
more	O
precise	O
about	O
what	O
it	O
means	O
to	O
say	O
that	O
data	O
constitute	O
evidence	B
for	O
a	O
hypothesis	O
.	O
and	O
that	O
is	O
the	O
subject	O
of	O
the	O
next	O
chapter	O
.	O
but	O
before	O
we	O
go	O
on	O
,	O
i	O
want	O
to	O
address	O
one	O
possible	O
source	O
of	O
confusion	O
.	O
since	O
we	O
want	O
to	O
know	O
whether	O
the	O
coin	O
is	O
fair	O
,	O
it	O
might	O
be	O
tempting	O
to	O
ask	O
for	O
the	O
probability	B
that	O
x	O
is	O
50	O
%	O
:	O
print	O
suite.prob	O
(	O
50	O
)	O
the	O
result	O
is	O
0.021	O
,	O
but	O
that	O
value	O
is	O
almost	O
meaningless	O
.	O
the	O
decision	O
to	O
evaluate	O
101	O
hypotheses	O
was	O
arbitrary	O
;	O
we	O
could	O
have	O
divided	O
the	O
range	O
into	O
more	O
or	O
fewer	O
pieces	O
,	O
and	O
if	O
we	O
had	O
,	O
the	O
probability	B
for	O
any	O
given	O
hy-	O
pothesis	O
would	O
be	O
greater	O
or	O
less	O
.	O
36	O
chapter	O
4.	O
more	O
estimation	O
figure	O
4.2	O
:	O
uniform	O
and	O
triangular	O
priors	O
for	O
the	O
euro	O
problem	O
.	O
4.3	O
swamping	B
the	I
priors	I
we	O
started	O
with	O
a	O
uniform	O
prior	O
,	O
but	O
that	O
might	O
not	O
be	O
a	O
good	O
choice	O
.	O
i	O
can	O
believe	O
that	O
if	O
a	O
coin	O
is	O
lopsided	O
,	O
x	O
might	O
deviate	O
substantially	O
from	O
50	O
%	O
,	O
but	O
it	O
seems	O
unlikely	O
that	O
the	O
belgian	O
euro	O
coin	O
is	O
so	O
imbalanced	O
that	O
x	O
is	O
10	O
%	O
or	O
90	O
%	O
.	O
it	O
might	O
be	O
more	O
reasonable	O
to	O
choose	O
a	O
prior	B
that	O
gives	O
higher	O
probability	B
to	O
values	O
of	O
x	O
near	O
50	O
%	O
and	O
lower	O
probability	B
to	O
extreme	O
values	O
.	O
as	O
an	O
example	O
,	O
i	O
constructed	O
a	O
triangular	O
prior	B
,	O
shown	O
in	O
figure	O
4.2.	O
here	O
’	O
s	O
the	O
code	O
that	O
constructs	O
the	O
prior	B
:	O
def	O
triangleprior	O
(	O
)	O
:	O
suite	B
=	O
euro	O
(	O
)	O
for	O
x	O
in	O
range	O
(	O
0	O
,	O
51	O
)	O
:	O
suite.set	O
(	O
x	O
,	O
x	O
)	O
for	O
x	O
in	O
range	O
(	O
51	O
,	O
101	O
)	O
:	O
suite.set	O
(	O
x	O
,	O
100-x	O
)	O
suite.normalize	O
(	O
)	O
figure	O
4.2	O
shows	O
the	O
result	O
(	O
and	O
the	O
uniform	O
prior	O
for	O
comparison	O
)	O
.	O
updat-	O
ing	O
this	O
prior	B
with	O
the	O
same	O
dataset	O
yields	O
the	O
posterior	B
distribution	I
shown	O
in	O
figure	O
4.3.	O
even	O
with	O
substantially	O
different	O
priors	O
,	O
the	O
posterior	B
distribu-	O
tions	O
are	O
very	O
similar	O
.	O
the	O
medians	O
and	O
the	O
credible	O
intervals	O
are	O
identical	O
;	O
the	O
means	O
differ	O
by	O
less	O
than	O
0.5	O
%	O
.	O
this	O
is	O
an	O
example	O
of	O
swamping	B
the	I
priors	I
:	O
with	O
enough	O
data	O
,	O
people	O
who	O
start	O
with	O
different	O
priors	O
will	O
tend	O
to	O
converge	O
on	O
the	O
same	O
posterior	B
.	O
020406080100x0.0000.0050.0100.0150.0200.025probabilityuniformtriangle	O
4.4.	O
optimization	B
37	O
figure	O
4.3	O
:	O
posterior	B
distributions	O
for	O
the	O
euro	O
problem	O
.	O
4.4	O
optimization	B
the	O
code	O
i	O
have	O
shown	O
so	O
far	O
is	O
meant	O
to	O
be	O
easy	O
to	O
read	O
,	O
but	O
it	O
is	O
not	O
very	O
efﬁcient	O
.	O
in	O
general	O
,	O
i	O
like	O
to	O
develop	O
code	O
that	O
is	O
demonstrably	O
correct	O
,	O
then	O
check	O
whether	O
it	O
is	O
fast	O
enough	O
for	O
my	O
purposes	O
.	O
if	O
so	O
,	O
there	O
is	O
no	O
need	O
to	O
optimize	O
.	O
for	O
this	O
example	O
,	O
if	O
we	O
care	O
about	O
run	O
time	O
,	O
there	O
are	O
several	O
ways	O
we	O
can	O
speed	O
it	O
up	O
.	O
the	O
ﬁrst	O
opportunity	O
is	O
to	O
reduce	O
the	O
number	O
of	O
times	O
we	O
normalize	B
the	O
suite	B
.	O
in	O
the	O
original	O
code	O
,	O
we	O
call	O
update	O
once	O
for	O
each	O
spin	O
.	O
dataset	O
=	O
'h	O
'	O
*	O
heads	O
+	O
't	O
'	O
*	O
tails	O
for	O
data	O
in	O
dataset	O
:	O
suite.update	O
(	O
data	O
)	O
and	O
here	O
’	O
s	O
what	O
update	O
looks	O
like	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
like	O
=	O
self.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
self.mult	O
(	O
hypo	O
,	O
like	O
)	O
return	O
self.normalize	O
(	O
)	O
each	O
update	O
iterates	O
through	O
the	O
hypotheses	O
,	O
then	O
calls	O
normalize	B
,	O
which	O
iterates	O
through	O
the	O
hypotheses	O
again	O
.	O
we	O
can	O
save	O
some	O
time	O
by	O
doing	O
all	O
of	O
the	O
updates	O
before	O
normalizing	O
.	O
suite	B
provides	O
a	O
method	O
called	O
updateset	O
that	O
does	O
exactly	O
that	O
.	O
here	O
it	O
is	O
:	O
020406080100x0.000.020.040.060.080.100.120.14probabilityuniformtriangle	O
38	O
chapter	O
4.	O
more	O
estimation	O
def	O
updateset	O
(	O
self	O
,	O
dataset	O
)	O
:	O
for	O
data	O
in	O
dataset	O
:	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
like	O
=	O
self.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
self.mult	O
(	O
hypo	O
,	O
like	O
)	O
return	O
self.normalize	O
(	O
)	O
and	O
here	O
’	O
s	O
how	O
we	O
can	O
invoke	O
it	O
:	O
dataset	O
=	O
'h	O
'	O
*	O
heads	O
+	O
't	O
'	O
*	O
tails	O
suite.updateset	O
(	O
dataset	O
)	O
this	O
optimization	B
speeds	O
things	O
up	O
,	O
but	O
the	O
run	O
time	O
is	O
still	O
proportional	O
to	O
the	O
amount	O
of	O
data	O
.	O
we	O
can	O
speed	O
things	O
up	O
even	O
more	O
by	O
rewriting	O
likelihood	B
to	O
process	B
the	O
entire	O
dataset	O
,	O
rather	O
than	O
one	O
spin	O
at	O
a	O
time	O
.	O
in	O
the	O
original	O
version	O
,	O
data	O
is	O
a	O
string	O
that	O
encodes	O
either	O
heads	O
or	O
tails	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
hypo	O
/	O
100.0	O
if	O
data	O
==	O
'h	O
'	O
:	O
return	O
x	O
else	O
:	O
return	O
1-x	O
as	O
an	O
alternative	O
,	O
we	O
could	O
encode	O
the	O
dataset	O
as	O
a	O
tuple	B
of	O
two	O
integers	O
:	O
the	O
number	O
of	O
heads	O
and	O
tails	O
.	O
in	O
that	O
case	O
likelihood	B
looks	O
like	O
this	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
hypo	O
/	O
100.0	O
heads	O
,	O
tails	O
=	O
data	O
like	O
=	O
x**heads	O
*	O
(	O
1-x	O
)	O
**tails	O
return	O
like	O
and	O
then	O
we	O
can	O
call	O
update	O
like	O
this	O
:	O
heads	O
,	O
tails	O
=	O
140	O
,	O
110	O
suite.update	O
(	O
(	O
heads	O
,	O
tails	O
)	O
)	O
since	O
we	O
have	O
replaced	O
repeated	O
multiplication	O
with	O
exponentiation	B
,	O
this	O
version	O
takes	O
the	O
same	O
time	O
for	O
any	O
number	O
of	O
spins	O
.	O
4.5	O
the	O
beta	B
distribution	I
there	O
is	O
one	O
more	O
optimization	B
that	O
solves	O
this	O
problem	O
even	O
faster	O
.	O
4.5.	O
the	O
beta	B
distribution	I
39	O
so	O
far	O
we	O
have	O
used	O
a	O
pmf	O
object	O
to	O
represent	O
a	O
discrete	O
set	O
of	O
values	O
for	O
x.	O
now	O
we	O
will	O
use	O
a	O
continuous	B
distribution	I
,	O
speciﬁcally	O
the	O
beta	B
distribution	I
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/beta_distribution	O
)	O
.	O
the	O
beta	B
distribution	I
is	O
deﬁned	O
on	O
the	O
interval	O
from	O
0	O
to	O
1	O
(	O
including	O
both	O
)	O
,	O
so	O
it	O
is	O
a	O
natural	O
choice	O
for	O
describing	O
proportions	O
and	O
probabilities	O
.	O
but	O
wait	O
,	O
it	O
gets	O
better	O
.	O
it	O
turns	O
out	O
that	O
if	O
you	O
do	O
a	O
bayesian	O
update	O
with	O
a	O
binomial	B
likelihood	I
function	I
,	O
which	O
is	O
what	O
we	O
did	O
in	O
the	O
previous	O
section	O
,	O
the	O
beta	B
distribution	I
is	O
a	O
conjugate	B
prior	I
.	O
that	O
means	O
that	O
if	O
the	O
prior	B
distribution	I
for	O
x	O
is	O
a	O
beta	B
distribution	I
,	O
the	O
posterior	B
is	O
also	O
a	O
beta	B
distribution	I
.	O
but	O
wait	O
,	O
it	O
gets	O
even	O
better	O
.	O
the	O
shape	O
of	O
the	O
beta	B
distribution	I
depends	O
on	O
two	O
parameters	O
,	O
written	O
α	O
and	O
β	O
,	O
or	O
alpha	O
and	O
beta	O
.	O
if	O
the	O
prior	B
is	O
a	O
beta	B
distribution	I
with	O
parameters	O
alpha	O
and	O
beta	O
,	O
and	O
we	O
see	O
data	O
with	O
h	O
heads	O
and	O
t	O
tails	O
,	O
the	O
posterior	B
is	O
a	O
beta	B
distribution	I
with	O
parameters	O
alpha+h	O
and	O
beta+t	O
.	O
in	O
other	O
words	O
,	O
we	O
can	O
do	O
an	O
update	O
with	O
two	O
additions	O
.	O
so	O
that	O
’	O
s	O
great	O
,	O
but	O
it	O
only	O
works	O
if	O
we	O
can	O
ﬁnd	O
a	O
beta	B
distribution	I
that	O
is	O
a	O
good	O
choice	O
for	O
a	O
prior	B
.	O
fortunately	O
,	O
for	O
many	O
realistic	O
priors	O
there	O
is	O
a	O
beta	B
distribution	I
that	O
is	O
at	O
least	O
a	O
good	O
approximation	O
,	O
and	O
for	O
a	O
uniform	O
prior	O
there	O
is	O
a	O
perfect	O
match	O
.	O
the	O
beta	B
distribution	I
with	O
alpha=1	O
and	O
beta=1	O
is	O
uniform	O
from	O
0	O
to	O
1.	O
let	O
’	O
s	O
see	O
how	O
we	O
can	O
take	O
advantage	O
of	O
all	O
this	O
.	O
thinkbayes.py	O
provides	O
a	O
class	O
that	O
represents	O
a	O
beta	B
distribution	I
:	O
class	O
beta	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
alpha=1	O
,	O
beta=1	O
)	O
:	O
self.alpha	O
=	O
alpha	O
self.beta	O
=	O
beta	O
by	O
default	O
__init__	O
makes	O
a	O
uniform	B
distribution	I
.	O
update	O
performs	O
a	O
bayesian	O
update	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
heads	O
,	O
tails	O
=	O
data	O
self.alpha	O
+=	O
heads	O
self.beta	O
+=	O
tails	O
data	O
is	O
a	O
pair	O
of	O
integers	O
representing	O
the	O
number	O
of	O
heads	O
and	O
tails	O
.	O
so	O
we	O
have	O
yet	O
another	O
way	O
to	O
solve	O
the	O
euro	O
problem	O
:	O
40	O
chapter	O
4.	O
more	O
estimation	O
beta	O
=	O
thinkbayes.beta	O
(	O
)	O
beta.update	O
(	O
(	O
140	O
,	O
110	O
)	O
)	O
print	O
beta.mean	O
(	O
)	O
beta	O
provides	O
mean	O
,	O
which	O
computes	O
a	O
simple	O
function	O
of	O
alpha	O
and	O
beta	O
:	O
def	O
mean	O
(	O
self	O
)	O
:	O
return	O
float	O
(	O
self.alpha	O
)	O
/	O
(	O
self.alpha	O
+	O
self.beta	O
)	O
for	O
the	O
euro	O
problem	O
the	O
posterior	B
mean	O
is	O
56	O
%	O
,	O
which	O
is	O
the	O
same	O
result	O
we	O
got	O
using	O
pmfs	O
.	O
beta	O
also	O
provides	O
evalpdf	O
,	O
which	O
evaluates	O
the	O
probability	B
density	I
func-	O
tion	O
(	O
pdf	O
)	O
of	O
the	O
beta	B
distribution	I
:	O
def	O
evalpdf	O
(	O
self	O
,	O
x	O
)	O
:	O
return	O
x**	O
(	O
self.alpha-1	O
)	O
*	O
(	O
1-x	O
)	O
**	O
(	O
self.beta-1	O
)	O
finally	O
,	O
beta	O
provides	O
makepmf	O
,	O
which	O
uses	O
evalpdf	O
to	O
generate	O
a	O
discrete	O
approximation	O
of	O
the	O
beta	B
distribution	I
.	O
4.6	O
discussion	O
in	O
this	O
chapter	O
we	O
solved	O
the	O
same	O
problem	O
with	O
two	O
different	O
priors	O
and	O
found	O
that	O
with	O
a	O
large	O
dataset	O
,	O
the	O
priors	O
get	O
swamped	O
.	O
if	O
two	O
people	O
start	O
with	O
different	O
prior	B
beliefs	O
,	O
they	O
generally	O
ﬁnd	O
,	O
as	O
they	O
see	O
more	O
data	O
,	O
that	O
their	O
posterior	B
distributions	O
converge	O
.	O
at	O
some	O
point	O
the	O
difference	O
between	O
their	O
distributions	O
is	O
small	O
enough	O
that	O
it	O
has	O
no	O
practical	O
effect	O
.	O
when	O
this	O
happens	O
,	O
it	O
relieves	O
some	O
of	O
the	O
worry	O
about	O
objectivity	B
that	O
i	O
discussed	O
in	O
the	O
previous	O
chapter	O
.	O
and	O
for	O
many	O
real-world	O
problems	O
even	O
stark	O
prior	B
beliefs	O
can	O
eventually	O
be	O
reconciled	O
by	O
data	O
.	O
but	O
that	O
is	O
not	O
always	O
the	O
case	O
.	O
first	O
,	O
remember	O
that	O
all	O
bayesian	O
analysis	O
is	O
based	O
on	O
modeling	B
decisions	O
.	O
if	O
you	O
and	O
i	O
do	O
not	O
choose	O
the	O
same	O
model	O
,	O
we	O
might	O
interpret	O
data	O
differently	O
.	O
so	O
even	O
with	O
the	O
same	O
data	O
,	O
we	O
would	O
compute	O
different	O
likelihoods	O
,	O
and	O
our	O
posterior	B
beliefs	O
might	O
not	O
converge	O
.	O
also	O
,	O
notice	O
that	O
in	O
a	O
bayesian	O
update	O
,	O
we	O
multiply	O
each	O
prior	B
probability	O
by	O
a	O
likelihood	B
,	O
so	O
if	O
p	O
(	O
h	O
)	O
is	O
0	O
,	O
p	O
(	O
h|d	O
)	O
is	O
also	O
0	O
,	O
regardless	O
of	O
d.	O
in	O
the	O
euro	O
problem	O
,	O
if	O
you	O
are	O
convinced	O
that	O
x	O
is	O
less	O
than	O
50	O
%	O
,	O
and	O
you	O
assign	O
probability	B
0	O
to	O
all	O
other	O
hypotheses	O
,	O
no	O
amount	O
of	O
data	O
will	O
convince	O
you	O
otherwise	O
.	O
4.7.	O
exercises	O
41	O
this	O
observation	O
is	O
the	O
basis	O
of	O
cromwell	O
’	O
s	O
rule	O
,	O
which	O
is	O
the	O
recommen-	O
dation	O
that	O
you	O
should	O
avoid	O
giving	O
a	O
prior	B
probability	O
of	O
0	O
to	O
any	O
hypoth-	O
esis	O
that	O
is	O
even	O
remotely	O
possible	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/	O
cromwell's_rule	O
)	O
.	O
cromwell	O
’	O
s	O
rule	O
is	O
named	O
after	O
oliver	O
cromwell	O
,	O
who	O
wrote	O
,	O
“	O
i	O
beseech	O
you	O
,	O
in	O
the	O
bowels	O
of	O
christ	O
,	O
think	O
it	O
possible	O
that	O
you	O
may	O
be	O
mistaken.	O
”	O
for	O
bayesians	O
,	O
this	O
turns	O
out	O
to	O
be	O
good	O
advice	O
(	O
even	O
if	O
it	O
’	O
s	O
a	O
little	O
over-	O
wrought	O
)	O
.	O
4.7	O
exercises	O
exercise	O
4.1.	O
suppose	O
that	O
instead	O
of	O
observing	O
coin	O
tosses	O
directly	O
,	O
you	O
measure	O
the	O
outcome	O
using	O
an	O
instrument	O
that	O
is	O
not	O
always	O
correct	O
.	O
speciﬁcally	O
,	O
suppose	O
there	O
is	O
a	O
probability	B
y	O
that	O
an	O
actual	O
heads	O
is	O
reported	O
as	O
tails	O
,	O
or	O
actual	O
tails	O
re-	O
ported	O
as	O
heads	O
.	O
write	O
a	O
class	O
that	O
estimates	O
the	O
bias	O
of	O
a	O
coin	O
given	O
a	O
series	O
of	O
outcomes	O
and	O
the	O
value	O
of	O
y.	O
how	O
does	O
the	O
spread	O
of	O
the	O
posterior	B
distribution	I
depend	O
on	O
y	O
?	O
exercise	O
4.2.	O
this	O
exercise	O
is	O
inspired	O
by	O
a	O
question	O
posted	O
by	O
a	O
“	O
redditor	O
”	O
named	O
dominosci	O
on	O
reddit	O
’	O
s	O
statistics	O
“	O
subreddit	O
”	O
at	O
http	O
:	O
//	O
reddit	O
.	O
com/	O
r/	O
statistics	O
.	O
reddit	O
is	O
an	O
online	O
forum	O
with	O
many	O
interest	O
groups	O
called	O
subreddits	O
.	O
users	O
,	O
called	O
redditors	O
,	O
post	O
links	O
to	O
online	O
content	O
and	O
other	O
web	O
pages	O
.	O
other	O
redditors	O
vote	O
on	O
the	O
links	O
,	O
giving	O
an	O
“	O
upvote	O
”	O
to	O
high-quality	O
links	O
and	O
a	O
“	O
downvote	O
”	O
to	O
links	O
that	O
are	O
bad	O
or	O
irrelevant	O
.	O
a	O
problem	O
,	O
identiﬁed	O
by	O
dominosci	O
,	O
is	O
that	O
some	O
redditors	O
are	O
more	O
reliable	O
than	O
others	O
,	O
and	O
reddit	O
does	O
not	O
take	O
this	O
into	O
account	O
.	O
the	O
challenge	O
is	O
to	O
devise	O
a	O
system	O
so	O
that	O
when	O
a	O
redditor	O
casts	O
a	O
vote	O
,	O
the	O
estimated	O
quality	O
of	O
the	O
link	O
is	O
updated	O
in	O
accordance	O
with	O
the	O
reliability	O
of	O
the	O
redditor	O
,	O
and	O
the	O
estimated	O
reliability	O
of	O
the	O
redditor	O
is	O
updated	O
in	O
accordance	O
with	O
the	O
quality	O
of	O
the	O
link	O
.	O
one	O
approach	O
is	O
to	O
model	O
the	O
quality	O
of	O
the	O
link	O
as	O
the	O
probability	B
of	O
garnering	O
an	O
upvote	O
,	O
and	O
to	O
model	O
the	O
reliability	O
of	O
the	O
redditor	O
as	O
the	O
probability	B
of	O
correctly	O
giving	O
an	O
upvote	O
to	O
a	O
high-quality	O
item	O
.	O
write	O
class	O
deﬁnitions	O
for	O
redditors	O
and	O
links	O
and	O
an	O
update	O
function	O
that	O
updates	O
both	O
objects	O
whenever	O
a	O
redditor	O
casts	O
a	O
vote	O
.	O
42	O
chapter	O
4.	O
more	O
estimation	O
chapter	O
5	O
odds	B
and	O
addends	O
5.1	O
odds	B
one	O
way	O
to	O
represent	O
a	O
probability	B
is	O
with	O
a	O
number	O
between	O
0	O
and	O
1	O
,	O
but	O
that	O
’	O
s	O
not	O
the	O
only	O
way	O
.	O
if	O
you	O
have	O
ever	O
bet	O
on	O
a	O
football	O
game	O
or	O
a	O
horse	O
race	O
,	O
you	O
have	O
probably	O
encountered	O
another	O
representation	O
of	O
probability	B
,	O
called	O
odds	B
.	O
you	O
might	O
have	O
heard	O
expressions	O
like	O
“	O
the	O
odds	B
are	O
three	O
to	O
one	O
,	O
”	O
but	O
you	O
might	O
not	O
know	O
what	O
that	O
means	O
.	O
the	O
odds	B
in	O
favor	O
of	O
an	O
event	O
are	O
the	O
ratio	O
of	O
the	O
probability	B
it	O
will	O
occur	O
to	O
the	O
probability	B
that	O
it	O
will	O
not	O
.	O
so	O
if	O
i	O
think	O
my	O
team	O
has	O
a	O
75	O
%	O
chance	O
of	O
winning	O
,	O
i	O
would	O
say	O
that	O
the	O
odds	B
in	O
their	O
favor	O
are	O
three	O
to	O
one	O
,	O
because	O
the	O
chance	O
of	O
winning	O
is	O
three	O
times	O
the	O
chance	O
of	O
losing	O
.	O
you	O
can	O
write	O
odds	B
in	O
decimal	O
form	O
,	O
but	O
it	O
is	O
most	O
common	O
to	O
write	O
them	O
as	O
a	O
ratio	O
of	O
integers	O
.	O
so	O
“	O
three	O
to	O
one	O
”	O
is	O
written	O
3	O
:	O
1.	O
when	O
probabilities	O
are	O
low	O
,	O
it	O
is	O
more	O
common	O
to	O
report	O
the	O
odds	B
against	O
rather	O
than	O
the	O
odds	B
in	O
favor	O
.	O
for	O
example	O
,	O
if	O
i	O
think	O
my	O
horse	O
has	O
a	O
10	O
%	O
chance	O
of	O
winning	O
,	O
i	O
would	O
say	O
that	O
the	O
odds	B
against	O
are	O
9	O
:	O
1.	O
probabilities	O
and	O
odds	B
are	O
different	O
representations	O
of	O
the	O
same	O
informa-	O
tion	O
.	O
given	O
a	O
probability	B
,	O
you	O
can	O
compute	O
the	O
odds	B
like	O
this	O
:	O
def	O
odds	B
(	O
p	O
)	O
:	O
return	O
p	O
/	O
(	O
1-p	O
)	O
given	O
the	O
odds	B
in	O
favor	O
,	O
in	O
decimal	O
form	O
,	O
you	O
can	O
convert	O
to	O
probability	B
like	O
this	O
:	O
44	O
chapter	O
5.	O
odds	B
and	O
addends	O
def	O
probability	B
(	O
o	O
)	O
:	O
return	O
o	O
/	O
(	O
o+1	O
)	O
if	O
you	O
represent	O
odds	B
with	O
a	O
numerator	O
and	O
denominator	O
,	O
you	O
can	O
convert	O
to	O
probability	B
like	O
this	O
:	O
def	O
probability2	O
(	O
yes	O
,	O
no	O
)	O
:	O
return	O
yes	O
/	O
(	O
yes	O
+	O
no	O
)	O
when	O
i	O
work	O
with	O
odds	B
in	O
my	O
head	O
,	O
i	O
ﬁnd	O
it	O
helpful	O
to	O
picture	O
people	O
at	O
the	O
track	O
.	O
if	O
20	O
%	O
of	O
them	O
think	O
my	O
horse	O
will	O
win	O
,	O
then	O
80	O
%	O
of	O
them	O
don	O
’	O
t	O
,	O
so	O
the	O
odds	B
in	O
favor	O
are	O
20	O
:	O
80	O
or	O
1	O
:	O
4.	O
if	O
the	O
odds	B
are	O
5	O
:	O
1	O
against	O
my	O
horse	O
,	O
then	O
ﬁve	O
out	O
of	O
six	O
people	O
think	O
she	O
will	O
lose	O
,	O
so	O
the	O
probability	B
of	O
winning	O
is	O
1/6	O
.	O
5.2	O
the	O
odds	B
form	I
of	O
bayes	O
’	O
s	O
theorem	O
in	O
chapter	O
1	O
i	O
wrote	O
bayes	O
’	O
s	O
theorem	O
in	O
the	O
probability	B
form	O
:	O
p	O
(	O
h|d	O
)	O
=	O
p	O
(	O
h	O
)	O
p	O
(	O
d|h	O
)	O
p	O
(	O
d	O
)	O
if	O
we	O
have	O
two	O
hypotheses	O
,	O
a	O
and	O
b	O
,	O
we	O
can	O
write	O
the	O
ratio	O
of	O
posterior	B
probabilities	O
like	O
this	O
:	O
p	O
(	O
a|d	O
)	O
p	O
(	O
b|d	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
d|a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
d|b	O
)	O
=	O
notice	O
that	O
the	O
normalizing	B
constant	I
,	O
p	O
(	O
d	O
)	O
,	O
drops	O
out	O
of	O
this	O
equation	O
.	O
if	O
a	O
and	O
b	O
are	O
mutually	B
exclusive	I
and	O
collectively	B
exhaustive	I
,	O
that	O
means	O
p	O
(	O
b	O
)	O
=	O
1	O
−	O
p	O
(	O
a	O
)	O
,	O
so	O
we	O
can	O
rewrite	O
the	O
ratio	O
of	O
the	O
priors	O
,	O
and	O
the	O
ratio	O
of	O
the	O
posteriors	O
,	O
as	O
odds	B
.	O
writing	O
o	O
(	O
a	O
)	O
for	O
odds	B
in	O
favor	O
of	O
a	O
,	O
we	O
get	O
:	O
o	O
(	O
a|d	O
)	O
=	O
o	O
(	O
a	O
)	O
p	O
(	O
d|a	O
)	O
p	O
(	O
d|b	O
)	O
in	O
words	O
,	O
this	O
says	O
that	O
the	O
posterior	B
odds	O
are	O
the	O
prior	B
odds	O
times	O
the	O
like-	O
lihood	O
ratio	O
.	O
this	O
is	O
the	O
odds	B
form	I
of	O
bayes	O
’	O
s	O
theorem	O
.	O
this	O
form	O
is	O
most	O
convenient	O
for	O
computing	O
a	O
bayesian	O
update	O
on	O
paper	O
or	O
in	O
your	O
head	O
.	O
for	O
example	O
,	O
let	O
’	O
s	O
go	O
back	O
to	O
the	O
cookie	B
problem	I
:	O
5.3.	O
oliver	O
’	O
s	O
blood	O
45	O
suppose	O
there	O
are	O
two	O
bowls	O
of	O
cookies	O
.	O
bowl	O
1	O
contains	O
30	O
vanilla	O
cookies	O
and	O
10	O
chocolate	O
cookies	O
.	O
bowl	O
2	O
contains	O
20	O
of	O
each	O
.	O
now	O
suppose	O
you	O
choose	O
one	O
of	O
the	O
bowls	O
at	O
random	O
and	O
,	O
with-	O
out	O
looking	O
,	O
select	O
a	O
cookie	O
at	O
random	O
.	O
the	O
cookie	O
is	O
vanilla	O
.	O
what	O
is	O
the	O
probability	B
that	O
it	O
came	O
from	O
bowl	O
1	O
?	O
the	O
prior	B
probability	O
is	O
50	O
%	O
,	O
so	O
the	O
prior	B
odds	O
are	O
1	O
:	O
1	O
,	O
or	O
just	O
1.	O
the	O
likeli-	O
4/	O
1	O
hood	O
ratio	O
is	O
3	O
2	O
,	O
or	O
3/2	O
.	O
so	O
the	O
posterior	B
odds	O
are	O
3	O
:	O
2	O
,	O
which	O
corresponds	O
to	O
probability	B
3/5	O
.	O
5.3	O
oliver	O
’	O
s	O
blood	O
here	O
is	O
another	O
problem	O
from	O
mackay	O
’	O
s	O
information	O
theory	O
,	O
inference	O
,	O
and	O
learning	O
algorithms	O
:	O
two	O
people	O
have	O
left	O
traces	O
of	O
their	O
own	O
blood	O
at	O
the	O
scene	O
of	O
a	O
crime	O
.	O
a	O
suspect	O
,	O
oliver	O
,	O
is	O
tested	O
and	O
found	O
to	O
have	O
type	O
‘	O
o	O
’	O
blood	O
.	O
the	O
blood	O
groups	O
of	O
the	O
two	O
traces	O
are	O
found	O
to	O
be	O
of	O
type	O
‘	O
o	O
’	O
(	O
a	O
common	O
type	O
in	O
the	O
local	O
population	O
,	O
having	O
frequency	O
60	O
%	O
)	O
and	O
of	O
type	O
‘	O
ab	O
’	O
(	O
a	O
rare	O
type	O
,	O
with	O
frequency	O
1	O
%	O
)	O
.	O
do	O
these	O
data	O
[	O
the	O
traces	O
found	O
at	O
the	O
scene	O
]	O
give	O
evidence	B
in	O
favor	O
of	O
the	O
proposition	O
that	O
oliver	O
was	O
one	O
of	O
the	O
people	O
[	O
who	O
left	O
blood	O
at	O
the	O
scene	O
]	O
?	O
to	O
answer	O
this	O
question	O
,	O
we	O
need	O
to	O
think	O
about	O
what	O
it	O
means	O
for	O
data	O
to	O
give	O
evidence	B
in	O
favor	O
of	O
(	O
or	O
against	O
)	O
a	O
hypothesis	O
.	O
intuitively	O
,	O
we	O
might	O
say	O
that	O
data	O
favor	O
a	O
hypothesis	O
if	O
the	O
hypothesis	O
is	O
more	O
likely	O
in	O
light	O
of	O
the	O
data	O
than	O
it	O
was	O
before	O
.	O
in	O
the	O
cookie	B
problem	I
,	O
the	O
prior	B
odds	O
are	O
1	O
:	O
1	O
,	O
or	O
probability	B
50	O
%	O
.	O
the	O
posterior	B
odds	O
are	O
3	O
:	O
2	O
,	O
or	O
probability	B
60	O
%	O
.	O
so	O
we	O
could	O
say	O
that	O
the	O
vanilla	O
cookie	O
is	O
evidence	B
in	O
favor	O
of	O
bowl	O
1.	O
the	O
odds	B
form	I
of	O
bayes	O
’	O
s	O
theorem	O
provides	O
a	O
way	O
to	O
make	O
this	O
intuition	B
more	O
precise	O
.	O
again	O
o	O
(	O
a|d	O
)	O
=	O
o	O
(	O
a	O
)	O
p	O
(	O
d|a	O
)	O
p	O
(	O
d|b	O
)	O
or	O
dividing	O
through	O
by	O
o	O
(	O
a	O
)	O
:	O
o	O
(	O
a|d	O
)	O
o	O
(	O
a	O
)	O
p	O
(	O
d|a	O
)	O
p	O
(	O
d|b	O
)	O
=	O
46	O
chapter	O
5.	O
odds	B
and	O
addends	O
the	O
term	O
on	O
the	O
left	O
is	O
the	O
ratio	O
of	O
the	O
posterior	B
and	O
prior	B
odds	O
.	O
the	O
term	O
on	O
the	O
right	O
is	O
the	O
likelihood	B
ratio	I
,	O
also	O
called	O
the	O
bayes	O
factor	O
.	O
if	O
the	O
bayes	O
factor	O
value	O
is	O
greater	O
than	O
1	O
,	O
that	O
means	O
that	O
the	O
data	O
were	O
more	O
likely	O
under	O
a	O
than	O
under	O
b.	O
and	O
since	O
the	O
odds	B
ratio	O
is	O
also	O
greater	O
than	O
1	O
,	O
that	O
means	O
that	O
the	O
odds	B
are	O
greater	O
,	O
in	O
light	O
of	O
the	O
data	O
,	O
than	O
they	O
were	O
before	O
.	O
if	O
the	O
bayes	O
factor	O
is	O
less	O
than	O
1	O
,	O
that	O
means	O
the	O
data	O
were	O
less	O
likely	O
under	O
a	O
than	O
under	O
b	O
,	O
so	O
the	O
odds	B
in	O
favor	O
of	O
a	O
go	O
down	O
.	O
finally	O
,	O
if	O
the	O
bayes	O
factor	O
is	O
exactly	O
1	O
,	O
the	O
data	O
are	O
equally	O
likely	O
under	O
either	O
hypothesis	O
,	O
so	O
the	O
odds	B
do	O
not	O
change	O
.	O
if	O
oliver	O
is	O
one	O
of	O
now	O
we	O
can	O
get	O
back	O
to	O
the	O
oliver	O
’	O
s	O
blood	O
problem	O
.	O
the	O
people	O
who	O
left	O
blood	O
at	O
the	O
crime	O
scene	O
,	O
then	O
he	O
accounts	O
for	O
the	O
‘	O
o	O
’	O
sample	O
,	O
so	O
the	O
probability	B
of	O
the	O
data	O
is	O
just	O
the	O
probability	B
that	O
a	O
random	O
member	O
of	O
the	O
population	O
has	O
type	O
‘	O
ab	O
’	O
blood	O
,	O
which	O
is	O
1	O
%	O
.	O
if	O
oliver	O
did	O
not	O
leave	O
blood	O
at	O
the	O
scene	O
,	O
then	O
we	O
have	O
two	O
samples	O
to	O
account	O
for	O
.	O
if	O
we	O
choose	O
two	O
random	O
people	O
from	O
the	O
population	O
,	O
what	O
is	O
the	O
chance	O
of	O
ﬁnding	O
one	O
with	O
type	O
‘	O
o	O
’	O
and	O
one	O
with	O
type	O
‘	O
ab	O
’	O
?	O
well	O
,	O
there	O
are	O
two	O
ways	O
it	O
might	O
happen	O
:	O
the	O
ﬁrst	O
person	O
we	O
choose	O
might	O
have	O
type	O
‘	O
o	O
’	O
and	O
the	O
second	O
‘	O
ab	O
’	O
,	O
or	O
the	O
other	O
way	O
around	O
.	O
so	O
the	O
total	B
probability	I
is	O
2	O
(	O
0.6	O
)	O
(	O
0.01	O
)	O
=	O
1.2	O
%	O
.	O
the	O
likelihood	B
of	O
the	O
data	O
is	O
slightly	O
higher	O
if	O
oliver	O
is	O
not	O
one	O
of	O
the	O
people	O
who	O
left	O
blood	O
at	O
the	O
scene	O
,	O
so	O
the	O
blood	O
data	O
is	O
actually	O
evidence	B
against	O
oliver	O
’	O
s	O
guilt	O
.	O
this	O
example	O
is	O
a	O
little	O
contrived	O
,	O
but	O
it	O
is	O
an	O
example	O
of	O
the	O
counterintuitive	O
result	O
that	O
data	O
consistent	O
with	O
a	O
hypothesis	O
are	O
not	O
necessarily	O
in	O
favor	O
of	O
the	O
hypothesis	O
.	O
if	O
this	O
result	O
is	O
so	O
counterintuitive	O
that	O
it	O
bothers	O
you	O
,	O
this	O
way	O
of	O
think-	O
ing	O
might	O
help	O
:	O
the	O
data	O
consist	O
of	O
a	O
common	O
event	O
,	O
type	O
‘	O
o	O
’	O
blood	O
,	O
and	O
a	O
rare	O
event	O
,	O
type	O
‘	O
ab	O
’	O
blood	O
.	O
if	O
oliver	O
accounts	O
for	O
the	O
common	O
event	O
,	O
that	O
leaves	O
the	O
rare	O
event	O
still	O
unexplained	O
.	O
if	O
oliver	O
doesn	O
’	O
t	O
account	O
for	O
the	O
‘	O
o	O
’	O
blood	O
,	O
then	O
we	O
have	O
two	O
chances	O
to	O
ﬁnd	O
someone	O
in	O
the	O
population	O
with	O
‘	O
ab	O
’	O
blood	O
.	O
and	O
that	O
factor	O
of	O
two	O
makes	O
the	O
difference	O
.	O
5.4	O
addends	O
the	O
fundamental	O
operation	O
of	O
bayesian	O
statistics	O
is	O
update	O
,	O
which	O
takes	O
a	O
prior	B
distribution	I
and	O
a	O
set	O
of	O
data	O
,	O
and	O
produces	O
a	O
posterior	B
distribution	I
.	O
5.4.	O
addends	O
47	O
but	O
solving	O
real	O
problems	O
usually	O
involves	O
a	O
number	O
of	O
other	O
operations	B
,	O
including	O
scaling	O
,	O
addition	O
and	O
other	O
arithmetic	O
operations	B
,	O
max	O
and	O
min	O
,	O
and	O
mixtures	O
.	O
this	O
chapter	O
presents	O
addition	O
and	O
max	O
;	O
i	O
will	O
present	O
other	O
operations	B
as	O
we	O
need	O
them	O
.	O
the	O
ﬁrst	O
example	O
is	O
based	O
on	O
dungeons	O
&	O
dragons	O
,	O
a	O
role-playing	O
game	O
where	O
the	O
results	O
of	O
players	O
’	O
decisions	O
are	O
usually	O
determined	O
by	O
rolling	O
dice	B
.	O
in	O
fact	O
,	O
before	O
game	O
play	O
starts	O
,	O
players	O
generate	O
each	O
attribute	O
of	O
their	O
characters—strength	O
,	O
intelligence	O
,	O
wisdom	O
,	O
dexterity	O
,	O
constitution	O
,	O
and	O
charisma—by	O
rolling	O
three	O
6-sided	O
dice	B
and	O
adding	O
them	O
up	O
.	O
so	O
you	O
might	O
be	O
curious	O
to	O
know	O
the	O
distribution	B
of	O
this	O
sum	O
.	O
there	O
are	O
two	O
ways	O
you	O
might	O
compute	O
it	O
:	O
simulation	B
:	O
given	O
a	O
pmf	O
that	O
represents	O
the	O
distribution	B
for	O
a	O
single	O
die	O
,	O
you	O
can	O
draw	O
random	O
samples	O
,	O
add	O
them	O
up	O
,	O
and	O
accumulate	O
the	O
dis-	O
tribution	O
of	O
simulated	O
sums	O
.	O
enumeration	B
:	O
given	O
two	O
pmfs	O
,	O
you	O
can	O
enumerate	O
all	O
possible	O
pairs	O
of	O
val-	O
ues	O
and	O
compute	O
the	O
distribution	B
of	O
the	O
sums	O
.	O
thinkbayes	O
provides	O
functions	O
for	O
both	O
.	O
here	O
’	O
s	O
an	O
example	O
of	O
the	O
ﬁrst	O
ap-	O
proach	O
.	O
first	O
,	O
i	O
’	O
ll	O
deﬁne	O
a	O
class	O
to	O
represent	O
a	O
single	O
die	O
as	O
a	O
pmf	O
:	O
class	O
die	O
(	O
thinkbayes.pmf	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
sides	O
)	O
:	O
thinkbayes.pmf.__init__	O
(	O
self	O
)	O
for	O
x	O
in	O
xrange	O
(	O
1	O
,	O
sides+1	O
)	O
:	O
self.set	O
(	O
x	O
,	O
1	O
)	O
self.normalize	O
(	O
)	O
now	O
i	O
can	O
create	O
a	O
6-sided	O
die	O
:	O
d6	O
=	O
die	O
(	O
6	O
)	O
and	O
use	O
thinkbayes.samplesum	O
to	O
generate	O
a	O
sample	O
of	O
1000	O
rolls	O
.	O
dice	B
=	O
[	O
d6	O
]	O
*	O
3	O
three	O
=	O
thinkbayes.samplesum	O
(	O
dice	B
,	O
1000	O
)	O
samplesum	O
takes	O
list	O
of	O
distributions	O
(	O
either	O
pmf	O
or	O
cdf	O
objects	O
)	O
and	O
the	O
sam-	O
ple	O
size	O
,	O
n.	O
it	O
generates	O
n	O
random	O
sums	O
and	O
returns	O
their	O
distribution	B
as	O
a	O
pmf	O
object	O
.	O
48	O
chapter	O
5.	O
odds	B
and	O
addends	O
def	O
samplesum	O
(	O
dists	O
,	O
n	O
)	O
:	O
pmf	O
=	O
makepmffromlist	O
(	O
randomsum	O
(	O
dists	O
)	O
for	O
i	O
in	O
xrange	O
(	O
n	O
)	O
)	O
return	O
pmf	O
samplesum	O
uses	O
randomsum	O
,	O
also	O
in	O
thinkbayes.py	O
:	O
def	O
randomsum	O
(	O
dists	O
)	O
:	O
total	O
=	O
sum	O
(	O
dist.random	O
(	O
)	O
for	O
dist	O
in	O
dists	O
)	O
return	O
total	O
randomsum	O
invokes	O
random	O
on	O
each	O
distribution	B
and	O
adds	O
up	O
the	O
results	O
.	O
the	O
drawback	O
of	O
simulation	B
is	O
that	O
the	O
result	O
is	O
only	O
approximately	O
correct	O
.	O
as	O
n	O
gets	O
larger	O
,	O
it	O
gets	O
more	O
accurate	O
,	O
but	O
of	O
course	O
the	O
run	O
time	O
increases	O
as	O
well	O
.	O
the	O
other	O
approach	O
is	O
to	O
enumerate	O
all	O
pairs	O
of	O
values	O
and	O
compute	O
the	O
sum	O
and	O
probability	B
of	O
each	O
pair	O
.	O
this	O
is	O
implemented	O
in	O
pmf.__add__	O
:	O
#	O
class	O
pmf	O
def	O
__add__	O
(	O
self	O
,	O
other	O
)	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
v1	O
,	O
p1	O
in	O
self.items	O
(	O
)	O
:	O
for	O
v2	O
,	O
p2	O
in	O
other.items	O
(	O
)	O
:	O
pmf.incr	O
(	O
v1+v2	O
,	O
p1*p2	O
)	O
return	O
pmf	O
self	O
is	O
a	O
pmf	O
,	O
of	O
course	O
;	O
other	O
can	O
be	O
a	O
pmf	O
or	O
anything	O
else	O
that	O
pro-	O
vides	O
items	O
.	O
the	O
result	O
is	O
a	O
new	O
pmf	O
.	O
the	O
time	O
to	O
run	O
__add__	O
depends	O
on	O
the	O
number	O
of	O
items	O
in	O
self	O
and	O
other	O
;	O
it	O
is	O
proportional	O
to	O
len	O
(	O
self	O
)	O
*	O
len	O
(	O
other	O
)	O
.	O
and	O
here	O
’	O
s	O
how	O
it	O
’	O
s	O
used	O
:	O
three_exact	O
=	O
d6	O
+	O
d6	O
+	O
d6	O
when	O
you	O
apply	O
the	O
+	O
operator	O
to	O
a	O
pmf	O
,	O
python	O
invokes	O
__add__	O
.	O
in	O
this	O
example	O
,	O
__add__	O
is	O
invoked	O
twice	O
.	O
figure	O
5.1	O
shows	O
an	O
approximate	O
result	O
generated	O
by	O
simulation	B
and	O
the	O
exact	O
result	O
computed	O
by	O
enumeration	B
.	O
pmf.__add__	O
is	O
based	O
on	O
the	O
assumption	O
that	O
the	O
random	O
selections	O
from	O
each	O
pmf	O
are	O
independent	O
.	O
in	O
the	O
example	O
of	O
rolling	O
several	O
dice	B
,	O
this	O
as-	O
sumption	O
is	O
pretty	O
good	O
.	O
in	O
other	O
cases	O
,	O
we	O
would	O
have	O
to	O
extend	O
this	O
method	O
to	O
use	O
conditional	B
probabilities	O
.	O
the	O
code	O
from	O
this	O
section	O
is	O
available	O
from	O
http	O
:	O
//thinkbayes.com/	O
dungeons.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
5.5.	O
maxima	O
49	O
figure	O
5.1	O
:	O
approximate	O
and	O
exact	O
distributions	O
for	O
the	O
sum	O
of	O
three	O
6-sided	O
dice	B
.	O
5.5	O
maxima	O
when	O
you	O
generate	O
a	O
dungeons	O
&	O
dragons	O
character	O
,	O
you	O
are	O
particularly	O
interested	O
in	O
the	O
character	O
’	O
s	O
best	O
attributes	O
,	O
so	O
you	O
might	O
like	O
to	O
know	O
the	O
distribution	B
of	O
the	O
maximum	B
attribute	O
.	O
there	O
are	O
three	O
ways	O
to	O
compute	O
the	O
distribution	B
of	O
a	O
maximum	B
:	O
simulation	B
:	O
given	O
a	O
pmf	O
that	O
represents	O
the	O
distribution	B
for	O
a	O
single	O
se-	O
lection	O
,	O
you	O
can	O
generate	O
random	O
samples	O
,	O
ﬁnd	O
the	O
maximum	B
,	O
and	O
accumulate	O
the	O
distribution	B
of	O
simulated	O
maxima	O
.	O
enumeration	B
:	O
given	O
two	O
pmfs	O
,	O
you	O
can	O
enumerate	O
all	O
possible	O
pairs	O
of	O
val-	O
ues	O
and	O
compute	O
the	O
distribution	B
of	O
the	O
maximum	B
.	O
exponentiation	B
:	O
if	O
we	O
convert	O
a	O
pmf	O
to	O
a	O
cdf	O
,	O
there	O
is	O
a	O
simple	O
and	O
efﬁcient	O
algorithm	O
for	O
ﬁnding	O
the	O
cdf	O
of	O
the	O
maximum	B
.	O
the	O
code	O
to	O
simulate	O
maxima	O
is	O
almost	O
identical	O
to	O
the	O
code	O
for	O
simulating	O
sums	O
:	O
def	O
randommax	O
(	O
dists	O
)	O
:	O
total	O
=	O
max	O
(	O
dist.random	O
(	O
)	O
for	O
dist	O
in	O
dists	O
)	O
return	O
total	O
def	O
samplemax	O
(	O
dists	O
,	O
n	O
)	O
:	O
pmf	O
=	O
makepmffromlist	O
(	O
randommax	O
(	O
dists	O
)	O
for	O
i	O
in	O
xrange	O
(	O
n	O
)	O
)	O
return	O
pmf	O
24681012141618sum	O
of	O
three	O
d60.000.020.040.060.080.100.120.14probabilitysampleexact	O
50	O
chapter	O
5.	O
odds	B
and	O
addends	O
figure	O
5.2	O
:	O
distribution	B
of	O
the	O
maximum	B
of	O
six	O
rolls	O
of	O
three	O
dice	B
.	O
all	O
i	O
did	O
was	O
replace	O
“	O
sum	O
”	O
with	O
“	O
max	O
”	O
.	O
and	O
the	O
code	O
for	O
enumeration	B
is	O
almost	O
identical	O
,	O
too	O
:	O
def	O
pmfmax	O
(	O
pmf1	O
,	O
pmf2	O
)	O
:	O
res	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
v1	O
,	O
p1	O
in	O
pmf1.items	O
(	O
)	O
:	O
for	O
v2	O
,	O
p2	O
in	O
pmf2.items	O
(	O
)	O
:	O
res.incr	O
(	O
max	O
(	O
v1	O
,	O
v2	O
)	O
,	O
p1*p2	O
)	O
return	O
res	O
in	O
fact	O
,	O
you	O
could	O
generalize	O
this	O
function	O
by	O
taking	O
the	O
appropriate	O
opera-	O
tor	O
as	O
a	O
parameter	B
.	O
the	O
only	O
problem	O
with	O
this	O
algorithm	O
is	O
that	O
if	O
each	O
pmf	O
has	O
m	O
values	O
,	O
the	O
run	O
time	O
is	O
proportional	O
to	O
m2	O
.	O
and	O
if	O
we	O
want	O
the	O
maximum	B
of	O
k	O
selections	O
,	O
it	O
takes	O
time	O
proportional	O
to	O
km2	O
.	O
if	O
we	O
convert	O
the	O
pmfs	O
to	O
cdfs	O
,	O
we	O
can	O
do	O
the	O
same	O
calculation	O
much	O
faster	O
!	O
the	O
key	O
is	O
to	O
remember	O
the	O
deﬁnition	O
of	O
the	O
cumulative	O
distribution	O
func-	O
tion	O
:	O
cdf	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
≤	O
x	O
)	O
where	O
x	O
is	O
a	O
random	O
variable	O
that	O
means	O
“	O
a	O
value	O
chosen	O
randomly	O
from	O
this	O
distribution.	O
”	O
so	O
,	O
for	O
example	O
,	O
cdf	O
(	O
5	O
)	O
is	O
the	O
probability	B
that	O
a	O
value	O
from	O
this	O
distribution	B
is	O
less	O
than	O
or	O
equal	O
to	O
5.	O
if	O
i	O
draw	O
x	O
from	O
cdf1	O
and	O
y	O
from	O
cdf2	O
,	O
and	O
compute	O
the	O
maximum	B
z	O
=	O
max	O
(	O
x	O
,	O
y	O
)	O
,	O
what	O
is	O
the	O
chance	O
that	O
z	O
is	O
less	O
than	O
or	O
equal	O
to	O
5	O
?	O
well	O
,	O
in	O
that	O
case	O
both	O
x	O
and	O
y	O
must	O
be	O
less	O
than	O
or	O
equal	O
to	O
5	O
.	O
24681012141618sum	O
of	O
three	O
d60.000.050.100.150.20probability	O
5.6.	O
mixtures	O
51	O
if	O
the	O
selections	O
of	O
x	O
and	O
y	O
are	O
independent	O
,	O
cdf3	O
(	O
5	O
)	O
=	O
cdf1	O
(	O
5	O
)	O
cdf2	O
(	O
5	O
)	O
where	O
cdf3	O
is	O
the	O
distribution	B
of	O
z.	O
i	O
chose	O
the	O
value	O
5	O
because	O
i	O
think	O
it	O
makes	O
the	O
formulas	O
easy	O
to	O
read	O
,	O
but	O
we	O
can	O
generalize	O
for	O
any	O
value	O
of	O
z	O
:	O
cdf3	O
(	O
z	O
)	O
=	O
cdf1	O
(	O
z	O
)	O
cdf2	O
(	O
z	O
)	O
in	O
the	O
special	O
case	O
where	O
we	O
draw	O
k	O
values	O
from	O
the	O
same	O
distribution	B
,	O
cdfk	O
(	O
z	O
)	O
=	O
cdf1	O
(	O
z	O
)	O
k	O
so	O
to	O
ﬁnd	O
the	O
distribution	B
of	O
the	O
maximum	B
of	O
k	O
values	O
,	O
we	O
can	O
enumerate	O
the	O
probabilities	O
in	O
the	O
given	O
cdf	O
and	O
raise	O
them	O
to	O
the	O
kth	O
power	O
.	O
cdf	O
provides	O
a	O
method	O
that	O
does	O
just	O
that	O
:	O
#	O
class	O
cdf	O
def	O
max	O
(	O
self	O
,	O
k	O
)	O
:	O
cdf	O
=	O
self.copy	O
(	O
)	O
cdf.ps	O
=	O
[	O
p**k	O
for	O
p	O
in	O
cdf.ps	O
]	O
return	O
cdf	O
max	O
takes	O
the	O
number	O
of	O
selections	O
,	O
k	O
,	O
and	O
returns	O
a	O
new	O
cdf	O
that	O
repre-	O
sents	O
the	O
distribution	B
of	O
the	O
maximum	B
of	O
k	O
selections	O
.	O
the	O
run	O
time	O
for	O
this	O
method	O
is	O
proportional	O
to	O
m	O
,	O
the	O
number	O
of	O
items	O
in	O
the	O
cdf	O
.	O
pmf.max	O
does	O
the	O
same	O
thing	O
for	O
pmfs	O
.	O
it	O
has	O
to	O
do	O
a	O
little	O
more	O
work	O
to	O
convert	O
the	O
pmf	O
to	O
a	O
cdf	O
,	O
so	O
the	O
run	O
time	O
is	O
proportional	O
to	O
m	O
log	O
m	O
,	O
but	O
that	O
’	O
s	O
still	O
better	O
than	O
quadratic	O
.	O
finally	O
,	O
here	O
’	O
s	O
an	O
example	O
that	O
computes	O
the	O
distribution	B
of	O
a	O
character	O
’	O
s	O
best	O
attribute	O
:	O
best_attr_cdf	O
=	O
three_exact.max	O
(	O
6	O
)	O
best_attr_pmf	O
=	O
best_attr_cdf.makepmf	O
(	O
)	O
where	O
three_exact	O
is	O
deﬁned	O
in	O
the	O
previous	O
section	O
.	O
if	O
we	O
print	O
the	O
re-	O
sults	O
,	O
we	O
see	O
that	O
the	O
chance	O
of	O
generating	O
a	O
character	O
with	O
at	O
least	O
one	O
at-	O
tribute	O
of	O
18	O
is	O
about	O
3	O
%	O
.	O
figure	O
5.2	O
shows	O
the	O
distribution	B
.	O
5.6	O
mixtures	O
let	O
’	O
s	O
do	O
one	O
more	O
example	O
from	O
dungeons	O
&	O
dragons	O
.	O
suppose	O
i	O
have	O
a	O
box	O
of	O
dice	B
with	O
the	O
following	O
inventory	O
:	O
52	O
chapter	O
5.	O
odds	B
and	O
addends	O
figure	O
5.3	O
:	O
distribution	B
outcome	O
for	O
random	O
die	O
from	O
a	O
box	O
.	O
4-sided	O
dice	B
6-sided	O
dice	B
8-sided	O
dice	B
12-sided	O
dice	B
20-sided	O
die	O
5	O
4	O
3	O
2	O
1	O
i	O
choose	O
a	O
die	O
from	O
the	O
box	O
and	O
roll	O
it	O
.	O
what	O
is	O
the	O
distribution	B
of	O
the	O
out-	O
come	O
?	O
if	O
you	O
know	O
which	O
die	O
it	O
is	O
,	O
the	O
answer	O
is	O
easy	O
.	O
a	O
die	O
with	O
n	O
sides	O
yields	O
a	O
uniform	B
distribution	I
from	O
1	O
to	O
n	O
,	O
including	O
both	O
.	O
but	O
if	O
we	O
don	O
’	O
t	O
know	O
which	O
die	O
it	O
is	O
,	O
the	O
resulting	O
distribution	B
is	O
a	O
mixture	B
of	O
uniform	O
distributions	O
with	O
different	O
bounds	O
.	O
in	O
general	O
,	O
this	O
kind	O
of	O
mix-	O
ture	O
does	O
not	O
ﬁt	O
any	O
simple	O
mathematical	O
model	O
,	O
but	O
it	O
is	O
straightforward	O
to	O
compute	O
the	O
distribution	B
in	O
the	O
form	O
of	O
a	O
pmf	O
.	O
as	O
always	O
,	O
one	O
option	O
is	O
to	O
simulate	O
the	O
scenario	O
,	O
generate	O
a	O
random	O
sam-	O
ple	O
,	O
and	O
compute	O
the	O
pmf	O
of	O
the	O
sample	O
.	O
this	O
approach	O
is	O
simple	O
and	O
it	O
generates	O
an	O
approximate	O
solution	O
quickly	O
.	O
but	O
if	O
we	O
want	O
an	O
exact	O
solu-	O
tion	O
,	O
we	O
need	O
a	O
different	O
approach	O
.	O
let	O
’	O
s	O
start	O
with	O
a	O
simple	O
version	O
of	O
the	O
problem	O
where	O
there	O
are	O
only	O
two	O
dice	B
,	O
one	O
with	O
6	O
sides	O
and	O
one	O
with	O
8.	O
we	O
can	O
make	O
a	O
pmf	O
to	O
represent	O
each	O
die	O
:	O
d6	O
=	O
die	O
(	O
6	O
)	O
d8	O
=	O
die	O
(	O
8	O
)	O
then	O
we	O
create	O
a	O
pmf	O
to	O
represent	O
the	O
mixture	B
:	O
0510152025outcome0.000.020.040.060.080.100.120.140.160.18probabilitymix	O
5.6.	O
mixtures	O
53	O
mix	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
die	O
in	O
[	O
d6	O
,	O
d8	O
]	O
:	O
for	O
outcome	O
,	O
prob	O
in	O
die.items	O
(	O
)	O
:	O
mix.incr	O
(	O
outcome	O
,	O
prob	O
)	O
mix.normalize	O
(	O
)	O
the	O
ﬁrst	O
loop	O
enumerates	O
the	O
dice	B
;	O
the	O
second	O
enumerates	O
the	O
outcomes	O
and	O
their	O
probabilities	O
.	O
inside	O
the	O
loop	O
,	O
pmf.incr	O
adds	O
up	O
the	O
contributions	O
from	O
the	O
two	O
distributions	O
.	O
this	O
code	O
assumes	O
that	O
the	O
two	O
dice	B
are	O
equally	O
likely	O
.	O
more	O
generally	O
,	O
we	O
need	O
to	O
know	O
the	O
probability	B
of	O
each	O
die	O
so	O
we	O
can	O
weight	O
the	O
outcomes	O
accordingly	O
.	O
first	O
we	O
create	O
a	O
pmf	O
that	O
maps	O
from	O
each	O
die	O
to	O
the	O
probability	B
it	O
is	O
selected	O
:	O
pmf_dice	O
=	O
thinkbayes.pmf	O
(	O
)	O
pmf_dice.set	O
(	O
die	O
(	O
4	O
)	O
,	O
5	O
)	O
pmf_dice.set	O
(	O
die	O
(	O
6	O
)	O
,	O
4	O
)	O
pmf_dice.set	O
(	O
die	O
(	O
8	O
)	O
,	O
3	O
)	O
pmf_dice.set	O
(	O
die	O
(	O
12	O
)	O
,	O
2	O
)	O
pmf_dice.set	O
(	O
die	O
(	O
20	O
)	O
,	O
1	O
)	O
pmf_dice.normalize	O
(	O
)	O
next	O
we	O
need	O
a	O
more	O
general	O
version	O
of	O
the	O
mixture	B
algorithm	O
:	O
mix	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
die	O
,	O
weight	O
in	O
pmf_dice.items	O
(	O
)	O
:	O
for	O
outcome	O
,	O
prob	O
in	O
die.items	O
(	O
)	O
:	O
mix.incr	O
(	O
outcome	O
,	O
weight*prob	O
)	O
now	O
each	O
die	O
has	O
a	O
weight	O
associated	O
with	O
it	O
(	O
which	O
makes	O
it	O
a	O
weighted	O
die	O
,	O
i	O
suppose	O
)	O
.	O
when	O
we	O
add	O
each	O
outcome	O
to	O
the	O
mixture	B
,	O
its	O
probability	B
is	O
multiplied	O
by	O
weight	O
.	O
figure	O
5.3	O
shows	O
the	O
result	O
.	O
as	O
expected	O
,	O
values	O
1	O
through	O
4	O
are	O
the	O
most	O
likely	O
because	O
any	O
die	O
can	O
produce	O
them	O
.	O
values	O
above	O
12	O
are	O
unlikely	O
be-	O
cause	O
there	O
is	O
only	O
one	O
die	O
in	O
the	O
box	O
that	O
can	O
produce	O
them	O
(	O
and	O
it	O
does	O
so	O
less	O
than	O
half	O
the	O
time	O
)	O
.	O
thinkbayes	O
provides	O
a	O
function	O
named	O
makemixture	O
that	O
encapsulates	O
this	O
algorithm	O
,	O
so	O
we	O
could	O
have	O
written	O
:	O
mix	O
=	O
thinkbayes.makemixture	O
(	O
pmf_dice	O
)	O
we	O
’	O
ll	O
use	O
makemixture	O
again	O
in	O
chapters	O
7	O
and	O
8	O
.	O
54	O
chapter	O
5.	O
odds	B
and	O
addends	O
5.7	O
discussion	O
other	O
than	O
the	O
odds	B
form	I
of	O
bayes	O
’	O
s	O
theorem	O
,	O
this	O
chapter	O
is	O
not	O
speciﬁcally	O
bayesian	O
.	O
but	O
bayesian	O
analysis	O
is	O
all	O
about	O
distributions	O
,	O
so	O
it	O
is	O
important	O
to	O
understand	O
the	O
concept	O
of	O
a	O
distribution	B
well	O
.	O
from	O
a	O
computational	O
point	O
of	O
view	O
,	O
a	O
distribution	B
is	O
any	O
data	O
structure	O
that	O
represents	O
a	O
set	O
of	O
values	O
(	O
possible	O
outcomes	O
of	O
a	O
random	O
process	O
)	O
and	O
their	O
probabilities	O
.	O
we	O
have	O
seen	O
two	O
representations	O
of	O
distributions	O
:	O
pmfs	O
and	O
cdfs	O
.	O
these	O
representations	O
are	O
equivalent	O
in	O
the	O
sense	O
that	O
they	O
contain	O
the	O
same	O
infor-	O
mation	O
,	O
so	O
you	O
can	O
convert	O
from	O
one	O
to	O
the	O
other	O
.	O
the	O
primary	O
difference	O
between	O
them	O
is	O
performance	O
:	O
some	O
operations	B
are	O
faster	O
and	O
easier	O
with	O
a	O
pmf	O
;	O
others	O
are	O
faster	O
with	O
a	O
cdf	O
.	O
the	O
other	O
goal	O
of	O
this	O
chapter	O
is	O
to	O
introduce	O
operations	B
that	O
act	O
on	O
distri-	O
butions	O
,	O
like	O
pmf.__add__	O
,	O
cdf.max	O
,	O
and	O
thinkbayes.makemixture	O
.	O
we	O
will	O
use	O
these	O
operations	B
later	O
,	O
but	O
i	O
introduce	O
them	O
now	O
to	O
encourage	O
you	O
to	O
think	O
of	O
a	O
distribution	B
as	O
a	O
fundamental	O
unit	O
of	O
computation	O
,	O
not	O
just	O
a	O
con-	O
tainer	O
for	O
values	O
and	O
probabilities	O
.	O
chapter	O
6	O
decision	B
analysis	I
6.1	O
the	O
price	O
is	O
right	O
problem	O
on	O
november	O
1	O
,	O
2007	O
,	O
contestants	O
named	O
letia	O
and	O
nathaniel	O
appeared	O
on	O
the	O
price	O
is	O
right	O
,	O
an	O
american	O
game	O
show	O
.	O
they	O
competed	O
in	O
a	O
game	O
called	O
the	O
showcase	O
,	O
where	O
the	O
objective	O
is	O
to	O
guess	O
the	O
price	O
of	O
a	O
showcase	O
of	O
prizes	O
.	O
the	O
contestant	O
who	O
comes	O
closest	O
to	O
the	O
actual	O
price	O
of	O
the	O
showcase	O
,	O
without	O
going	O
over	O
,	O
wins	O
the	O
prizes	O
.	O
nathaniel	O
went	O
ﬁrst	O
.	O
his	O
showcase	O
included	O
a	O
dishwasher	O
,	O
a	O
wine	O
cabinet	O
,	O
a	O
laptop	O
computer	O
,	O
and	O
a	O
car	O
.	O
he	O
bid	O
$	O
26,000	O
.	O
letia	O
’	O
s	O
showcase	O
included	O
a	O
pinball	O
machine	O
,	O
a	O
video	O
arcade	O
game	O
,	O
a	O
pool	O
table	O
,	O
and	O
a	O
cruise	O
of	O
the	O
bahamas	O
.	O
she	O
bid	O
$	O
21,500	O
.	O
the	O
actual	O
price	O
of	O
nathaniel	O
’	O
s	O
showcase	O
was	O
$	O
25,347	O
.	O
his	O
bid	O
was	O
too	O
high	O
,	O
so	O
he	O
lost	O
.	O
the	O
actual	O
price	O
of	O
letia	O
’	O
s	O
showcase	O
was	O
$	O
21,578	O
.	O
she	O
was	O
only	O
off	O
by	O
$	O
78	O
,	O
so	O
she	O
won	O
her	O
showcase	O
and	O
,	O
because	O
her	O
bid	O
was	O
off	O
by	O
less	O
than	O
$	O
250	O
,	O
she	O
also	O
won	O
nathaniel	O
’	O
s	O
showcase	O
.	O
for	O
a	O
bayesian	O
thinker	O
,	O
this	O
scenario	O
suggests	O
several	O
questions	O
:	O
1.	O
before	O
seeing	O
the	O
prizes	O
,	O
what	O
prior	B
beliefs	O
should	O
the	O
contestant	O
have	O
about	O
the	O
price	O
of	O
the	O
showcase	O
?	O
2.	O
after	O
seeing	O
the	O
prizes	O
,	O
how	O
should	O
the	O
contestant	O
update	O
those	O
be-	O
liefs	O
?	O
3.	O
based	O
on	O
the	O
posterior	B
distribution	I
,	O
what	O
should	O
the	O
contestant	O
bid	O
?	O
56	O
chapter	O
6.	O
decision	B
analysis	I
figure	O
6.1	O
:	O
distribution	B
of	O
prices	O
for	O
showcases	O
on	O
the	O
price	O
is	O
right	O
,	O
2011-12.	O
the	O
third	O
question	O
demonstrates	O
a	O
common	O
use	O
of	O
bayesian	O
analysis	O
:	O
de-	O
cision	O
analysis	O
.	O
given	O
a	O
posterior	B
distribution	I
,	O
we	O
can	O
choose	O
the	O
bid	O
that	O
maximizes	O
the	O
contestant	O
’	O
s	O
expected	O
return	O
.	O
this	O
problem	O
is	O
inspired	O
by	O
an	O
example	O
in	O
cameron	O
davidson-pilon	O
’	O
s	O
book	O
,	O
bayesian	O
methods	O
for	O
hackers	O
.	O
the	O
code	O
i	O
wrote	O
for	O
this	O
chapter	O
is	O
avail-	O
able	O
from	O
http	O
:	O
//thinkbayes.com/price.py	O
;	O
it	O
reads	O
data	O
ﬁles	O
you	O
can	O
download	O
from	O
http	O
:	O
//thinkbayes.com/showcases.2011.csv	O
and	O
http	O
:	O
//thinkbayes.com/showcases.2012.csv	O
.	O
for	O
more	O
information	O
see	O
sec-	O
tion	O
0.3	O
.	O
6.2	O
the	O
prior	B
to	O
choose	O
a	O
prior	B
distribution	I
of	O
prices	O
,	O
we	O
can	O
take	O
advantage	O
of	O
data	O
from	O
previous	O
episodes	O
.	O
fortunately	O
,	O
fans	O
of	O
the	O
show	O
keep	O
detailed	O
records	O
.	O
when	O
i	O
corresponded	O
with	O
mr.	O
davidson-pilon	O
about	O
his	O
book	O
,	O
he	O
sent	O
me	O
data	O
collected	O
by	O
steve	O
gee	O
at	O
http	O
:	O
//tpirsummaries.8m.com	O
.	O
it	O
includes	O
the	O
price	O
of	O
each	O
showcase	O
from	O
the	O
2011	O
and	O
2012	O
seasons	O
and	O
the	O
bids	O
offered	O
by	O
the	O
contestants	O
.	O
figure	O
6.1	O
shows	O
the	O
distribution	B
of	O
prices	O
for	O
these	O
showcases	O
.	O
the	O
most	O
common	O
value	O
for	O
both	O
showcases	O
is	O
around	O
$	O
28,000	O
,	O
but	O
the	O
ﬁrst	O
showcase	O
has	O
a	O
second	O
mode	O
near	O
$	O
50,000	O
,	O
and	O
the	O
second	O
showcase	O
is	O
occasionally	O
worth	O
more	O
than	O
$	O
70,000	O
.	O
these	O
distributions	O
are	O
based	O
on	O
actual	O
data	O
,	O
but	O
they	O
have	O
been	O
smoothed	O
01000020000300004000050000600007000080000price	O
(	O
$	O
)	O
0.000.010.020.030.040.05pdfshowcase	O
1showcase	O
2	O
6.3.	O
probability	B
density	I
functions	O
57	O
by	O
gaussian	O
kernel	B
density	I
estimation	I
(	O
kde	O
)	O
.	O
before	O
we	O
go	O
on	O
,	O
i	O
want	O
to	O
take	O
a	O
detour	O
to	O
talk	O
about	O
probability	B
density	I
functions	O
and	O
kde	O
.	O
6.3	O
probability	B
density	I
functions	O
so	O
far	O
we	O
have	O
been	O
working	O
with	O
probability	O
mass	O
functions	O
,	O
or	O
pmfs	O
.	O
a	O
pmf	O
is	O
a	O
map	O
from	O
each	O
possible	O
value	O
to	O
its	O
probability	B
.	O
in	O
my	O
implemen-	O
tation	O
,	O
a	O
pmf	O
object	O
provides	O
a	O
method	O
named	O
prob	O
that	O
takes	O
a	O
value	O
and	O
returns	O
a	O
probability	B
,	O
also	O
known	O
as	O
a	O
probability	O
mass	O
.	O
a	O
probability	B
density	I
function	I
,	O
or	O
pdf	O
,	O
is	O
the	O
continuous	O
version	O
of	O
a	O
pmf	O
,	O
where	O
the	O
possible	O
values	O
make	O
up	O
a	O
continuous	O
range	O
rather	O
than	O
a	O
discrete	O
set	O
.	O
in	O
mathematical	O
notation	O
,	O
pdfs	O
are	O
usually	O
written	O
as	O
functions	O
;	O
for	O
exam-	O
ple	O
,	O
here	O
is	O
the	O
pdf	O
of	O
a	O
gaussian	O
distribution	B
with	O
mean	O
0	O
and	O
standard	O
deviation	O
1	O
:	O
f	O
(	O
x	O
)	O
=	O
exp	O
(	O
−x2/2	O
)	O
1√	O
2π	O
for	O
a	O
given	O
value	O
of	O
x	O
,	O
this	O
function	O
computes	O
a	O
probability	B
density	I
.	O
a	O
den-	O
sity	O
is	O
similar	O
to	O
a	O
probability	O
mass	O
in	O
the	O
sense	O
that	O
a	O
higher	O
density	B
indi-	O
cates	O
that	O
a	O
value	O
is	O
more	O
likely	O
.	O
but	O
a	O
density	B
is	O
not	O
a	O
probability	B
.	O
a	O
density	B
can	O
be	O
0	O
or	O
any	O
positive	O
value	O
;	O
it	O
is	O
not	O
bounded	O
,	O
like	O
a	O
probability	B
,	O
between	O
0	O
and	O
1.	O
if	O
you	O
integrate	O
a	O
density	B
over	O
a	O
continuous	O
range	O
,	O
the	O
result	O
is	O
a	O
probability	B
.	O
but	O
for	O
the	O
applications	O
in	O
this	O
book	O
we	O
seldom	O
have	O
to	O
do	O
that	O
.	O
instead	O
we	O
primarily	O
use	O
probability	B
densities	O
as	O
part	O
of	O
a	O
likelihood	B
func-	O
tion	O
.	O
we	O
will	O
see	O
an	O
example	O
soon	O
.	O
6.4	O
representing	O
pdfs	O
to	O
represent	O
pdfs	O
in	O
python	O
,	O
thinkbayes.py	O
provides	O
a	O
class	O
named	O
pdf	O
.	O
pdf	O
is	O
an	O
abstract	B
type	I
,	O
which	O
means	O
that	O
it	O
deﬁnes	O
the	O
interface	B
a	O
pdf	O
is	O
supposed	O
to	O
have	O
,	O
but	O
does	O
not	O
provide	O
a	O
complete	O
implementation	B
.	O
the	O
pdf	O
interface	B
includes	O
two	O
methods	O
,	O
density	B
and	O
makepmf	O
:	O
class	O
pdf	O
(	O
object	O
)	O
:	O
58	O
chapter	O
6.	O
decision	B
analysis	I
def	O
density	B
(	O
self	O
,	O
x	O
)	O
:	O
raise	O
unimplementedmethodexception	O
(	O
)	O
def	O
makepmf	O
(	O
self	O
,	O
xs	O
)	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
x	O
in	O
xs	O
:	O
pmf.set	O
(	O
x	O
,	O
self.density	O
(	O
x	O
)	O
)	O
pmf.normalize	O
(	O
)	O
return	O
pmf	O
density	B
takes	O
a	O
value	O
,	O
x	O
,	O
and	O
returns	O
the	O
corresponding	O
density	B
.	O
makepmf	O
makes	O
a	O
discrete	O
approximation	O
to	O
the	O
pdf	O
.	O
pdf	O
provides	O
an	O
implementation	B
of	O
makepmf	O
,	O
but	O
not	O
density	B
,	O
which	O
has	O
to	O
be	O
provided	O
by	O
a	O
child	O
class	O
.	O
a	O
concrete	B
type	I
is	O
a	O
child	O
class	O
that	O
extends	O
an	O
abstract	B
type	I
and	O
provides	O
an	O
implementation	B
of	O
the	O
missing	O
methods	O
.	O
for	O
example	O
,	O
gaussianpdf	O
extends	O
pdf	O
and	O
provides	O
density	B
:	O
class	O
gaussianpdf	O
(	O
pdf	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
mu	O
,	O
sigma	O
)	O
:	O
self.mu	O
=	O
mu	O
self.sigma	O
=	O
sigma	O
def	O
density	B
(	O
self	O
,	O
x	O
)	O
:	O
return	O
scipy.stats.norm.pdf	O
(	O
x	O
,	O
self.mu	O
,	O
self.sigma	O
)	O
__init__	O
takes	O
mu	O
and	O
sigma	O
,	O
which	O
are	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
distribution	B
,	O
and	O
stores	O
them	O
as	O
attributes	O
.	O
density	B
uses	O
a	O
function	O
from	O
scipy.stats	O
to	O
evaluate	O
the	O
gaussian	O
pdf	O
.	O
the	O
function	O
is	O
called	O
norm.pdf	O
because	O
the	O
gaussian	O
distribution	B
is	O
also	O
called	O
the	O
“	O
normal	O
”	O
distribution	B
.	O
the	O
gaussian	O
pdf	O
is	O
deﬁned	O
by	O
a	O
simple	O
mathematical	O
function	O
,	O
so	O
it	O
is	O
easy	O
to	O
evaluate	O
.	O
and	O
it	O
is	O
useful	O
because	O
many	O
quantities	O
in	O
the	O
real	O
world	O
have	O
distributions	O
that	O
are	O
approximately	O
gaussian	O
.	O
but	O
with	O
real	O
data	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
distribution	B
is	O
gaussian	O
or	O
any	O
other	O
simple	O
mathematical	O
function	O
.	O
in	O
that	O
case	O
we	O
can	O
use	O
a	O
sample	O
to	O
estimate	O
the	O
pdf	O
of	O
the	O
whole	O
population	O
.	O
for	O
example	O
,	O
in	O
the	O
price	O
is	O
right	O
data	O
,	O
we	O
have	O
313	O
prices	O
for	O
the	O
ﬁrst	O
show-	O
case	O
.	O
we	O
can	O
think	O
of	O
these	O
values	O
as	O
a	O
sample	O
from	O
the	O
population	O
of	O
all	O
possible	O
showcase	O
prices	O
.	O
6.4.	O
representing	O
pdfs	O
59	O
this	O
sample	O
includes	O
the	O
following	O
values	O
(	O
in	O
order	O
)	O
:	O
28800	O
,	O
28868	O
,	O
28941	O
,	O
28957	O
,	O
28958	O
in	O
the	O
sample	O
,	O
no	O
values	O
appear	O
between	O
28801	O
and	O
28867	O
,	O
but	O
there	O
is	O
no	O
reason	O
to	O
think	O
that	O
these	O
values	O
are	O
impossible	O
.	O
based	O
on	O
our	O
background	O
information	O
,	O
we	O
expect	O
all	O
values	O
in	O
this	O
range	O
to	O
be	O
equally	O
likely	O
.	O
in	O
other	O
words	O
,	O
we	O
expect	O
the	O
pdf	O
to	O
be	O
fairly	O
smooth	O
.	O
kernel	B
density	I
estimation	I
(	O
kde	O
)	O
is	O
an	O
algorithm	O
that	O
takes	O
a	O
sample	O
and	O
ﬁnds	O
an	O
appropriately	O
smooth	O
pdf	O
that	O
ﬁts	O
the	O
data	O
.	O
you	O
can	O
read	O
details	O
at	O
http	O
:	O
//en.wikipedia.org/wiki/kernel_density_estimation	O
.	O
scipy	B
provides	O
an	O
implementation	B
of	O
kde	O
and	O
thinkbayes	O
provides	O
a	O
class	O
called	O
estimatedpdf	O
that	O
uses	O
it	O
:	O
class	O
estimatedpdf	O
(	O
pdf	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
sample	O
)	O
:	O
self.kde	O
=	O
scipy.stats.gaussian_kde	O
(	O
sample	O
)	O
def	O
density	B
(	O
self	O
,	O
x	O
)	O
:	O
return	O
self.kde.evaluate	O
(	O
x	O
)	O
__init__	O
takes	O
a	O
sample	O
and	O
computes	O
a	O
kernel	O
density	O
estimate	O
.	O
the	O
result	O
is	O
a	O
gaussian_kde	O
object	O
that	O
provides	O
an	O
evaluate	O
method	O
.	O
density	B
takes	O
a	O
value	O
,	O
calls	O
gaussian_kde.evaluate	O
,	O
and	O
returns	O
the	O
result-	O
ing	O
density	B
.	O
finally	O
,	O
here	O
’	O
s	O
an	O
outline	O
of	O
the	O
code	O
i	O
used	O
to	O
generate	O
figure	O
6.1	O
:	O
prices	O
=	O
readdata	O
(	O
)	O
pdf	O
=	O
thinkbayes.estimatedpdf	O
(	O
prices	O
)	O
low	O
,	O
high	O
=	O
0	O
,	O
75000	O
n	O
=	O
101	O
xs	O
=	O
numpy.linspace	O
(	O
low	O
,	O
high	O
,	O
n	O
)	O
pmf	O
=	O
pdf.makepmf	O
(	O
xs	O
)	O
pdf	O
is	O
a	O
pdf	O
object	O
,	O
estimated	O
by	O
kde	O
.	O
pmf	O
is	O
a	O
pmf	O
object	O
that	O
approximates	O
the	O
pdf	O
by	O
evaluating	O
the	O
density	B
at	O
a	O
sequence	O
of	O
equally	O
spaced	O
values	O
.	O
linspace	B
stands	O
for	O
“	O
linear	O
space.	O
”	O
it	O
takes	O
a	O
range	O
,	O
low	O
and	O
high	O
,	O
and	O
the	O
number	O
of	O
points	O
,	O
n	O
,	O
and	O
returns	O
a	O
new	O
numpy	B
array	O
with	O
n	O
elements	O
equally	O
spaced	O
between	O
low	O
and	O
high	O
,	O
including	O
both	O
.	O
and	O
now	O
back	O
to	O
the	O
price	O
is	O
right	O
.	O
60	O
chapter	O
6.	O
decision	B
analysis	I
figure	O
6.2	O
:	O
cumulative	O
distribution	O
(	O
cdf	O
)	O
of	O
the	O
difference	O
between	O
the	O
contestant	O
’	O
s	O
bid	O
and	O
the	O
actual	O
price	O
.	O
6.5	O
modeling	B
the	O
contestants	O
the	O
pdfs	O
in	O
figure	O
6.1	O
estimate	O
the	O
distribution	B
of	O
possible	O
prices	O
.	O
if	O
you	O
were	O
a	O
contestant	O
on	O
the	O
show	O
,	O
you	O
could	O
use	O
this	O
distribution	B
to	O
quan-	O
tify	O
your	O
prior	B
belief	O
about	O
the	O
price	O
of	O
each	O
showcase	O
(	O
before	O
you	O
see	O
the	O
prizes	O
)	O
.	O
to	O
update	O
these	O
priors	O
,	O
we	O
have	O
to	O
answer	O
these	O
questions	O
:	O
1.	O
what	O
data	O
should	O
we	O
consider	O
and	O
how	O
should	O
we	O
quantify	O
it	O
?	O
2.	O
can	O
we	O
compute	O
a	O
likelihood	B
function	I
;	O
that	O
is	O
,	O
for	O
each	O
hypothetical	O
value	O
of	O
price	O
,	O
can	O
we	O
compute	O
the	O
conditional	B
likelihood	O
of	O
the	O
data	O
?	O
to	O
answer	O
these	O
questions	O
,	O
i	O
am	O
going	O
to	O
model	O
the	O
contestant	O
as	O
a	O
price-	O
guessing	O
instrument	O
with	O
known	O
error	B
characteristics	O
.	O
in	O
other	O
words	O
,	O
when	O
the	O
contestant	O
sees	O
the	O
prizes	O
,	O
he	O
or	O
she	O
guesses	O
the	O
price	O
of	O
each	O
prize—ideally	O
without	O
taking	O
into	O
consideration	O
the	O
fact	O
that	O
the	O
prize	O
is	O
part	O
of	O
a	O
showcase—and	O
adds	O
up	O
the	O
prices	O
.	O
let	O
’	O
s	O
call	O
this	O
total	O
guess	O
.	O
under	O
this	O
model	O
,	O
the	O
question	O
we	O
have	O
to	O
answer	O
is	O
,	O
“	O
if	O
the	O
actual	O
price	O
is	O
price	O
,	O
what	O
is	O
the	O
likelihood	B
that	O
the	O
contestant	O
’	O
s	O
estimate	O
would	O
be	O
guess	O
?	O
”	O
or	O
if	O
we	O
deﬁne	O
error	B
=	O
price	O
-	O
guess	O
30000200001000001000020000300004000050000diff	O
(	O
$	O
)	O
0.00.20.40.60.81.0cdfplayer	O
1player	O
2	O
6.5.	O
modeling	B
the	O
contestants	O
61	O
then	O
we	O
could	O
ask	O
,	O
“	O
what	O
is	O
the	O
likelihood	B
that	O
the	O
contestant	O
’	O
s	O
estimate	O
is	O
off	O
by	O
error	B
?	O
”	O
to	O
answer	O
this	O
question	O
,	O
we	O
can	O
use	O
the	O
historical	O
data	O
again	O
.	O
figure	O
6.2	O
shows	O
the	O
cumulative	O
distribution	O
of	O
diff	O
,	O
the	O
difference	O
between	O
the	O
con-	O
testant	O
’	O
s	O
bid	O
and	O
the	O
actual	O
price	O
of	O
the	O
showcase	O
.	O
the	O
deﬁnition	O
of	O
diff	O
is	O
diff	O
=	O
price	O
-	O
bid	O
when	O
diff	O
is	O
negative	O
,	O
the	O
bid	O
is	O
too	O
high	O
.	O
as	O
an	O
aside	O
,	O
we	O
can	O
use	O
this	O
distribution	B
to	O
compute	O
the	O
probability	B
that	O
the	O
contestants	O
overbid	O
:	O
the	O
ﬁrst	O
contestant	O
overbids	O
25	O
%	O
of	O
the	O
time	O
;	O
the	O
second	O
contestant	O
overbids	O
29	O
%	O
of	O
the	O
time	O
.	O
we	O
can	O
also	O
see	O
that	O
the	O
bids	O
are	O
biased	O
;	O
that	O
is	O
,	O
they	O
are	O
more	O
likely	O
to	O
be	O
too	O
low	O
than	O
too	O
high	O
.	O
and	O
that	O
makes	O
sense	O
,	O
given	O
the	O
rules	O
of	O
the	O
game	O
.	O
finally	O
,	O
we	O
can	O
use	O
this	O
distribution	B
to	O
estimate	O
the	O
reliability	O
of	O
the	O
contes-	O
tants	O
’	O
guesses	O
.	O
this	O
step	O
is	O
a	O
little	O
tricky	O
because	O
we	O
don	O
’	O
t	O
actually	O
know	O
the	O
contestant	O
’	O
s	O
guesses	O
;	O
we	O
only	O
know	O
what	O
they	O
bid	O
.	O
so	O
we	O
’	O
ll	O
have	O
to	O
make	O
some	O
assumptions	O
.	O
speciﬁcally	O
,	O
i	O
assume	O
that	O
the	O
distribution	B
of	O
error	B
is	O
gaussian	O
with	O
mean	O
0	O
and	O
the	O
same	O
variance	O
as	O
diff	O
.	O
the	O
player	O
class	O
implements	O
this	O
model	O
:	O
class	O
player	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
prices	O
,	O
bids	O
,	O
diffs	O
)	O
:	O
self.pdf_price	O
=	O
thinkbayes.estimatedpdf	O
(	O
prices	O
)	O
self.cdf_diff	O
=	O
thinkbayes.makecdffromlist	O
(	O
diffs	O
)	O
mu	O
=	O
0	O
sigma	O
=	O
numpy.std	O
(	O
diffs	O
)	O
self.pdf_error	O
=	O
thinkbayes.gaussianpdf	O
(	O
mu	O
,	O
sigma	O
)	O
prices	O
is	O
a	O
sequence	O
of	O
showcase	O
prices	O
,	O
bids	O
is	O
a	O
sequence	O
of	O
bids	O
,	O
and	O
diffs	O
is	O
a	O
sequence	O
of	O
diffs	O
,	O
where	O
again	O
diff	O
=	O
price	O
-	O
bid	O
.	O
pdf_price	O
is	O
the	O
smoothed	O
pdf	O
of	O
prices	O
,	O
estimated	O
by	O
kde	O
.	O
cdf_diff	O
is	O
the	O
cumulative	O
distribution	O
of	O
diff	O
,	O
which	O
we	O
saw	O
in	O
figure	O
6.2.	O
and	O
pdf_error	O
is	O
the	O
pdf	O
that	O
characterizes	O
the	O
distribution	B
of	O
errors	O
;	O
where	O
error	B
=	O
price	O
-	O
guess	O
.	O
62	O
chapter	O
6.	O
decision	B
analysis	I
again	O
,	O
we	O
use	O
the	O
variance	O
of	O
diff	O
to	O
estimate	O
the	O
variance	O
of	O
error	B
.	O
this	O
estimate	O
is	O
not	O
perfect	O
because	O
contestants	O
’	O
bids	O
are	O
sometimes	O
strategic	O
;	O
for	O
example	O
,	O
if	O
player	O
2	O
thinks	O
that	O
player	O
1	O
has	O
overbid	O
,	O
player	O
2	O
might	O
make	O
a	O
very	O
low	O
bid	O
.	O
in	O
that	O
case	O
diff	O
does	O
not	O
reﬂect	O
error	B
.	O
if	O
this	O
happens	O
a	O
lot	O
,	O
the	O
observed	O
variance	O
in	O
diff	O
might	O
overestimate	O
the	O
variance	O
in	O
error	B
.	O
nevertheless	O
,	O
i	O
think	O
it	O
is	O
a	O
reasonable	O
modeling	B
decision	O
.	O
as	O
an	O
alternative	O
,	O
someone	O
preparing	O
to	O
appear	O
on	O
the	O
show	O
could	O
estimate	O
their	O
own	O
distribution	B
of	O
error	B
by	O
watching	O
previous	O
shows	O
and	O
recording	O
their	O
guesses	O
and	O
the	O
actual	O
prices	O
.	O
6.6	O
likelihood	B
now	O
we	O
are	O
ready	O
to	O
write	O
the	O
likelihood	B
function	I
.	O
as	O
usual	O
,	O
i	O
deﬁne	O
a	O
new	O
class	O
that	O
extends	O
thinkbayes.suite	O
:	O
class	O
price	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
pmf	O
,	O
player	O
)	O
:	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
pmf	O
)	O
self.player	O
=	O
player	O
pmf	O
represents	O
the	O
prior	B
distribution	I
and	O
player	O
is	O
a	O
player	O
object	O
as	O
de-	O
scribed	O
in	O
the	O
previous	O
section	O
.	O
here	O
’	O
s	O
likelihood	B
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
price	O
=	O
hypo	O
guess	O
=	O
data	O
error	B
=	O
price	O
-	O
guess	O
like	O
=	O
self.player.errordensity	O
(	O
error	B
)	O
return	O
like	O
hypo	O
is	O
the	O
hypothetical	O
price	O
of	O
the	O
showcase	O
.	O
data	O
is	O
the	O
contestant	O
’	O
s	O
best	O
guess	O
at	O
the	O
price	O
.	O
error	B
is	O
the	O
difference	O
,	O
and	O
like	O
is	O
the	O
likelihood	B
of	O
the	O
data	O
,	O
given	O
the	O
hypothesis	O
.	O
errordensity	O
is	O
deﬁned	O
in	O
player	O
:	O
#	O
class	O
player	O
:	O
def	O
errordensity	O
(	O
self	O
,	O
error	B
)	O
:	O
return	O
self.pdf_error.density	O
(	O
error	B
)	O
6.7.	O
update	O
63	O
figure	O
6.3	O
:	O
prior	B
and	O
posterior	B
distributions	O
for	O
player	O
1	O
,	O
based	O
on	O
a	O
best	O
guess	O
of	O
$	O
20,000	O
.	O
errordensity	O
works	O
by	O
evaluating	O
pdf_error	O
at	O
the	O
given	O
value	O
of	O
error	B
.	O
the	O
result	O
is	O
a	O
probability	B
density	I
,	O
so	O
it	O
is	O
not	O
really	O
a	O
probability	B
.	O
but	O
re-	O
member	O
that	O
likelihood	B
doesn	O
’	O
t	O
need	O
to	O
compute	O
a	O
probability	B
;	O
it	O
only	O
has	O
to	O
compute	O
something	O
proportional	O
to	O
a	O
probability	B
.	O
as	O
long	O
as	O
the	O
constant	O
of	O
proportionality	O
is	O
the	O
same	O
for	O
all	O
likelihoods	O
,	O
it	O
gets	O
canceled	O
out	O
when	O
we	O
normalize	B
the	O
posterior	B
distribution	I
.	O
and	O
therefore	O
,	O
a	O
probability	B
density	I
is	O
a	O
perfectly	O
good	O
likelihood	B
.	O
6.7	O
update	O
player	O
provides	O
a	O
method	O
that	O
takes	O
the	O
contestant	O
’	O
s	O
guess	O
and	O
computes	O
the	O
posterior	B
distribution	I
:	O
#	O
class	O
player	O
def	O
makebeliefs	O
(	O
self	O
,	O
guess	O
)	O
:	O
pmf	O
=	O
self.pmfprice	O
(	O
)	O
self.prior	O
=	O
price	O
(	O
pmf	O
,	O
self	O
)	O
self.posterior	O
=	O
self.prior.copy	O
(	O
)	O
self.posterior.update	O
(	O
guess	O
)	O
pmfprice	O
generates	O
a	O
discrete	O
approximation	O
to	O
the	O
pdf	O
of	O
price	O
,	O
which	O
we	O
use	O
to	O
construct	O
the	O
prior	B
.	O
pmfprice	O
uses	O
makepmf	O
,	O
which	O
evaluates	O
pdf_price	O
at	O
a	O
sequence	O
of	O
values	O
:	O
01000020000300004000050000600007000080000price	O
(	O
$	O
)	O
0.000.010.020.030.040.050.060.070.08pmfpriorposterior	O
64	O
#	O
class	O
player	O
chapter	O
6.	O
decision	B
analysis	I
n	O
=	O
101	O
price_xs	O
=	O
numpy.linspace	O
(	O
0	O
,	O
75000	O
,	O
n	O
)	O
def	O
pmfprice	O
(	O
self	O
)	O
:	O
return	O
self.pdf_price.makepmf	O
(	O
self.price_xs	O
)	O
to	O
construct	O
the	O
posterior	B
,	O
we	O
make	O
a	O
copy	O
of	O
the	O
prior	B
and	O
then	O
invoke	O
update	O
,	O
which	O
invokes	O
likelihood	B
for	O
each	O
hypothesis	O
,	O
multiplies	O
the	O
pri-	O
ors	O
by	O
the	O
likelihoods	O
,	O
and	O
renormalizes	O
.	O
so	O
let	O
’	O
s	O
get	O
back	O
to	O
the	O
original	O
scenario	O
.	O
suppose	O
you	O
are	O
player	O
1	O
and	O
when	O
you	O
see	O
your	O
showcase	O
,	O
your	O
best	O
guess	O
is	O
that	O
the	O
total	O
price	O
of	O
the	O
prizes	O
is	O
$	O
20,000	O
.	O
figure	O
6.3	O
shows	O
prior	B
and	O
posterior	B
beliefs	O
about	O
the	O
actual	O
price	O
.	O
the	O
posterior	B
is	O
shifted	O
to	O
the	O
left	O
because	O
your	O
guess	O
is	O
on	O
the	O
low	O
end	O
of	O
the	O
prior	B
range	O
.	O
on	O
one	O
level	O
,	O
this	O
result	O
makes	O
sense	O
.	O
the	O
most	O
likely	O
value	O
in	O
the	O
prior	B
is	O
$	O
27,750	O
,	O
your	O
best	O
guess	O
is	O
$	O
20,000	O
,	O
and	O
the	O
mean	O
of	O
the	O
posterior	B
is	O
some-	O
where	O
in	O
between	O
:	O
$	O
25,096	O
.	O
on	O
another	O
level	O
,	O
you	O
might	O
ﬁnd	O
this	O
result	O
bizarre	O
,	O
because	O
it	O
suggests	O
that	O
if	O
you	O
think	O
the	O
price	O
is	O
$	O
20,000	O
,	O
then	O
you	O
should	O
believe	O
the	O
price	O
is	O
$	O
24,000	O
.	O
to	O
resolve	O
this	O
apparent	O
paradox	O
,	O
remember	O
that	O
you	O
are	O
combining	O
two	O
sources	O
of	O
information	O
,	O
historical	O
data	O
about	O
past	O
showcases	O
and	O
guesses	O
about	O
the	O
prizes	O
you	O
see	O
.	O
we	O
are	O
treating	O
the	O
historical	O
data	O
as	O
the	O
prior	B
and	O
updating	O
it	O
based	O
on	O
your	O
guesses	O
,	O
but	O
we	O
could	O
equivalently	O
use	O
your	O
guess	O
as	O
a	O
prior	B
and	O
update	O
it	O
based	O
on	O
historical	O
data	O
.	O
if	O
you	O
think	O
of	O
it	O
that	O
way	O
,	O
maybe	O
it	O
is	O
less	O
surprising	O
that	O
the	O
most	O
likely	O
value	O
in	O
the	O
posterior	B
is	O
not	O
your	O
original	O
guess	O
.	O
6.8	O
optimal	O
bidding	O
now	O
that	O
we	O
have	O
a	O
posterior	B
distribution	I
,	O
we	O
can	O
use	O
it	O
to	O
compute	O
the	O
optimal	O
bid	O
,	O
which	O
i	O
deﬁne	O
as	O
the	O
bid	O
that	O
maximizes	O
expected	O
return	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/expected_return	O
)	O
.	O
6.8.	O
optimal	O
bidding	O
65	O
i	O
’	O
m	O
going	O
to	O
present	O
the	O
methods	O
in	O
this	O
section	O
top-down	O
,	O
which	O
means	O
i	O
will	O
show	O
you	O
how	O
they	O
are	O
used	O
before	O
i	O
show	O
you	O
how	O
they	O
work	O
.	O
if	O
you	O
see	O
an	O
unfamiliar	O
method	O
,	O
don	O
’	O
t	O
worry	O
;	O
the	O
deﬁnition	O
will	O
be	O
along	O
shortly	O
.	O
to	O
compute	O
optimal	O
bids	O
,	O
i	O
wrote	O
a	O
class	O
called	O
gaincalculator	O
:	O
class	O
gaincalculator	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
player	O
,	O
opponent	O
)	O
:	O
self.player	O
=	O
player	O
self.opponent	O
=	O
opponent	O
player	O
and	O
opponent	O
are	O
player	O
objects	O
.	O
gaincalculator	O
provides	O
expectedgains	O
,	O
which	O
computes	O
a	O
sequence	O
of	O
bids	O
and	O
the	O
expected	O
gain	O
for	O
each	O
bid	O
:	O
def	O
expectedgains	O
(	O
self	O
,	O
low=0	O
,	O
high=75000	O
,	O
n=101	O
)	O
:	O
bids	O
=	O
numpy.linspace	O
(	O
low	O
,	O
high	O
,	O
n	O
)	O
gains	O
=	O
[	O
self.expectedgain	O
(	O
bid	O
)	O
for	O
bid	O
in	O
bids	O
]	O
return	O
bids	O
,	O
gains	O
low	O
and	O
high	O
specify	O
the	O
range	O
of	O
possible	O
bids	O
;	O
n	O
is	O
the	O
number	O
of	O
bids	O
to	O
try	O
.	O
expectedgains	O
calls	O
expectedgain	O
,	O
which	O
computes	O
expected	O
gain	O
for	O
a	O
given	O
bid	O
:	O
def	O
expectedgain	O
(	O
self	O
,	O
bid	O
)	O
:	O
suite	B
=	O
self.player.posterior	O
total	O
=	O
0	O
for	O
price	O
,	O
prob	O
in	O
sorted	O
(	O
suite.items	O
(	O
)	O
)	O
:	O
gain	O
=	O
self.gain	O
(	O
bid	O
,	O
price	O
)	O
total	O
+=	O
prob	O
*	O
gain	O
return	O
total	O
expectedgain	O
loops	O
through	O
the	O
values	O
in	O
the	O
posterior	B
and	O
computes	O
the	O
gain	O
for	O
each	O
bid	O
,	O
given	O
the	O
actual	O
prices	O
of	O
the	O
showcase	O
.	O
it	O
weights	O
each	O
gain	O
with	O
the	O
corresponding	O
probability	B
and	O
returns	O
the	O
total	O
.	O
expectedgain	O
invokes	O
gain	O
,	O
which	O
takes	O
a	O
bid	O
and	O
an	O
actual	O
price	O
and	O
re-	O
turns	O
the	O
expected	O
gain	O
:	O
def	O
gain	O
(	O
self	O
,	O
bid	O
,	O
price	O
)	O
:	O
if	O
bid	O
>	O
price	O
:	O
return	O
0	O
66	O
chapter	O
6.	O
decision	B
analysis	I
figure	O
6.4	O
:	O
expected	O
gain	O
versus	O
bid	O
in	O
a	O
scenario	O
where	O
player	O
1	O
’	O
s	O
best	O
guess	O
is	O
$	O
20,000	O
and	O
player	O
2	O
’	O
s	O
best	O
guess	O
is	O
$	O
40,000	O
.	O
diff	O
=	O
price	O
-	O
bid	O
prob	O
=	O
self.probwin	O
(	O
diff	O
)	O
if	O
diff	O
<	O
=	O
250	O
:	O
return	O
2	O
*	O
price	O
*	O
prob	O
else	O
:	O
return	O
price	O
*	O
prob	O
if	O
you	O
overbid	O
,	O
you	O
get	O
nothing	O
.	O
otherwise	O
we	O
compute	O
the	O
difference	O
be-	O
tween	O
your	O
bid	O
and	O
the	O
price	O
,	O
which	O
determines	O
your	O
probability	B
of	O
win-	O
ning	O
.	O
if	O
diff	O
is	O
less	O
than	O
$	O
250	O
,	O
you	O
win	O
both	O
showcases	O
.	O
for	O
simplicity	O
,	O
i	O
assume	O
that	O
both	O
showcases	O
have	O
the	O
same	O
price	O
.	O
since	O
this	O
outcome	O
is	O
rare	O
,	O
it	O
doesn	O
’	O
t	O
make	O
much	O
difference	O
.	O
finally	O
,	O
we	O
have	O
to	O
compute	O
the	O
probability	B
of	O
winning	O
based	O
on	O
diff	O
:	O
def	O
probwin	O
(	O
self	O
,	O
diff	O
)	O
:	O
prob	O
=	O
(	O
self.opponent.proboverbid	O
(	O
)	O
+	O
self.opponent.probworsethan	O
(	O
diff	O
)	O
)	O
return	O
prob	O
if	O
your	O
opponent	O
overbids	O
,	O
you	O
win	O
.	O
otherwise	O
,	O
you	O
have	O
to	O
hope	O
that	O
your	O
opponent	O
is	O
off	O
by	O
more	O
than	O
diff	O
.	O
player	O
provides	O
methods	O
to	O
compute	O
both	O
probabilities	O
:	O
#	O
class	O
player	O
:	O
01000020000300004000050000600007000080000bid	O
(	O
$	O
)	O
05000100001500020000expected	O
gain	O
(	O
$	O
)	O
player	O
1player	O
2	O
6.9.	O
discussion	O
67	O
def	O
proboverbid	O
(	O
self	O
)	O
:	O
return	O
self.cdf_diff.prob	O
(	O
-1	O
)	O
def	O
probworsethan	O
(	O
self	O
,	O
diff	O
)	O
:	O
return	O
1	O
-	O
self.cdf_diff.prob	O
(	O
diff	O
)	O
this	O
code	O
might	O
be	O
confusing	O
because	O
the	O
computation	O
is	O
now	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
opponent	O
,	O
who	O
is	O
computing	O
,	O
“	O
what	O
is	O
the	O
probability	B
that	O
i	O
overbid	O
?	O
”	O
and	O
“	O
what	O
is	O
the	O
probability	B
that	O
my	O
bid	O
is	O
off	O
by	O
more	O
than	O
diff	O
?	O
”	O
both	O
answers	O
are	O
based	O
on	O
the	O
cdf	O
of	O
diff	O
.	O
if	O
the	O
opponent	O
’	O
s	O
diff	O
is	O
less	O
than	O
or	O
equal	O
to	O
-1	O
,	O
you	O
win	O
.	O
if	O
the	O
opponent	O
’	O
s	O
diff	O
is	O
worse	O
than	O
yours	O
,	O
you	O
win	O
.	O
otherwise	O
you	O
lose	O
.	O
finally	O
,	O
here	O
’	O
s	O
the	O
code	O
that	O
computes	O
optimal	O
bids	O
:	O
#	O
class	O
player	O
:	O
def	O
optimalbid	O
(	O
self	O
,	O
guess	O
,	O
opponent	O
)	O
:	O
self.makebeliefs	O
(	O
guess	O
)	O
calc	O
=	O
gaincalculator	O
(	O
self	O
,	O
opponent	O
)	O
bids	O
,	O
gains	O
=	O
calc.expectedgains	O
(	O
)	O
gain	O
,	O
bid	O
=	O
max	O
(	O
zip	O
(	O
gains	O
,	O
bids	O
)	O
)	O
return	O
bid	O
,	O
gain	O
given	O
a	O
guess	O
and	O
an	O
opponent	O
,	O
optimalbid	O
computes	O
the	O
posterior	B
distri-	O
bution	O
,	O
instantiates	O
a	O
gaincalculator	O
,	O
computes	O
expected	O
gains	O
for	O
a	O
range	O
of	O
bids	O
and	O
returns	O
the	O
optimal	O
bid	O
and	O
expected	O
gain	O
.	O
whew	O
!	O
figure	O
6.4	O
shows	O
the	O
results	O
for	O
both	O
players	O
,	O
based	O
on	O
a	O
scenario	O
where	O
player	O
1	O
’	O
s	O
best	O
guess	O
is	O
$	O
20,000	O
and	O
player	O
2	O
’	O
s	O
best	O
guess	O
is	O
$	O
40,000	O
.	O
for	O
player	O
1	O
the	O
optimal	O
bid	O
is	O
$	O
21,000	O
,	O
yielding	O
an	O
expected	O
return	O
of	O
almost	O
$	O
16,700	O
.	O
this	O
is	O
a	O
case	O
(	O
which	O
turns	O
out	O
to	O
be	O
unusual	O
)	O
where	O
the	O
optimal	O
bid	O
is	O
actually	O
higher	O
than	O
the	O
contestant	O
’	O
s	O
best	O
guess	O
.	O
for	O
player	O
2	O
the	O
optimal	O
bid	O
is	O
$	O
31,500	O
,	O
yielding	O
an	O
expected	O
return	O
of	O
almost	O
$	O
19,400	O
.	O
this	O
is	O
the	O
more	O
typical	O
case	O
where	O
the	O
optimal	O
bid	O
is	O
less	O
than	O
the	O
best	O
guess	O
.	O
6.9	O
discussion	O
one	O
of	O
the	O
features	O
of	O
bayesian	O
estimation	O
is	O
that	O
the	O
result	O
comes	O
in	O
the	O
form	O
of	O
a	O
posterior	B
distribution	I
.	O
classical	B
estimation	I
usually	O
generates	O
a	O
68	O
chapter	O
6.	O
decision	B
analysis	I
single	O
point	O
estimate	O
or	O
a	O
conﬁdence	O
interval	O
,	O
which	O
is	O
sufﬁcient	O
if	O
estima-	O
tion	O
is	O
the	O
last	O
step	O
in	O
the	O
process	B
,	O
but	O
if	O
you	O
want	O
to	O
use	O
an	O
estimate	O
as	O
an	O
input	O
to	O
a	O
subsequent	O
analysis	O
,	O
point	O
estimates	O
and	O
intervals	O
are	O
often	O
not	O
much	O
help	O
.	O
in	O
this	O
example	O
,	O
we	O
use	O
the	O
posterior	B
distribution	I
to	O
compute	O
an	O
optimal	O
bid	O
.	O
the	O
return	O
on	O
a	O
given	O
bid	O
is	O
asymmetric	O
and	O
discontinuous	O
(	O
if	O
you	O
overbid	O
,	O
you	O
lose	O
)	O
,	O
so	O
it	O
would	O
be	O
hard	O
to	O
solve	O
this	O
problem	O
analytically	O
.	O
but	O
it	O
is	O
relatively	O
simple	O
to	O
do	O
computationally	O
.	O
newcomers	O
to	O
bayesian	O
thinking	O
are	O
often	O
tempted	O
to	O
summarize	O
the	O
pos-	O
terior	O
distribution	B
by	O
computing	O
the	O
mean	O
or	O
the	O
maximum	B
likelihood	I
es-	O
timate	O
.	O
these	O
summaries	O
can	O
be	O
useful	O
,	O
but	O
if	O
that	O
’	O
s	O
all	O
you	O
need	O
,	O
then	O
you	O
probably	O
don	O
’	O
t	O
need	O
bayesian	O
methods	O
in	O
the	O
ﬁrst	O
place	O
.	O
bayesian	O
methods	O
are	O
most	O
useful	O
when	O
you	O
can	O
carry	O
the	O
posterior	B
distri-	O
bution	O
into	O
the	O
next	O
step	O
of	O
the	O
analysis	O
to	O
perform	O
some	O
kind	O
of	O
decision	B
analysis	I
,	O
as	O
we	O
did	O
in	O
this	O
chapter	O
,	O
or	O
some	O
kind	O
of	O
prediction	O
,	O
as	O
we	O
see	O
in	O
the	O
next	O
chapter	O
.	O
chapter	O
7	O
prediction	O
7.1	O
the	O
boston	O
bruins	O
problem	O
in	O
the	O
2010-11	O
national	O
hockey	B
league	O
(	O
nhl	O
)	O
finals	O
,	O
my	O
beloved	O
boston	O
bruins	O
played	O
a	O
best-of-seven	O
championship	O
series	O
against	O
the	O
despised	O
vancouver	O
canucks	O
.	O
boston	O
lost	O
the	O
ﬁrst	O
two	O
games	O
0-1	O
and	O
2-3	O
,	O
then	O
won	O
the	O
next	O
two	O
games	O
8-1	O
and	O
4-0.	O
at	O
this	O
point	O
in	O
the	O
series	O
,	O
what	O
is	O
the	O
prob-	O
ability	O
that	O
boston	O
will	O
win	O
the	O
next	O
game	O
,	O
and	O
what	O
is	O
their	O
probability	B
of	O
winning	O
the	O
championship	O
?	O
as	O
always	O
,	O
to	O
answer	O
a	O
question	O
like	O
this	O
,	O
we	O
need	O
to	O
make	O
some	O
assump-	O
tions	O
.	O
first	O
,	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
goal	O
scoring	O
in	O
hockey	B
is	O
at	O
least	O
approximately	O
a	O
poisson	O
process	B
,	O
which	O
means	O
that	O
it	O
is	O
equally	O
likely	O
for	O
a	O
goal	O
to	O
be	O
scored	O
at	O
any	O
time	O
during	O
a	O
game	O
.	O
second	O
,	O
we	O
can	O
assume	O
that	O
against	O
a	O
particular	O
opponent	O
,	O
each	O
team	O
has	O
some	O
long-term	O
average	O
goals	O
per	O
game	O
,	O
denoted	O
λ.	O
given	O
these	O
assumptions	O
,	O
my	O
strategy	O
for	O
answering	O
this	O
question	O
is	O
1.	O
use	O
statistics	O
from	O
previous	O
games	O
to	O
choose	O
a	O
prior	B
distribution	I
for	O
λ	O
.	O
2.	O
use	O
the	O
score	O
from	O
the	O
ﬁrst	O
four	O
games	O
to	O
estimate	O
λ	O
for	O
each	O
team	O
.	O
3.	O
use	O
the	O
posterior	B
distributions	O
of	O
λ	O
to	O
compute	O
distribution	B
of	O
goals	O
for	O
each	O
team	O
,	O
the	O
distribution	B
of	O
the	O
goal	O
differential	O
,	O
and	O
the	O
proba-	O
bility	O
that	O
each	O
team	O
wins	O
the	O
next	O
game	O
.	O
4.	O
compute	O
the	O
probability	B
that	O
each	O
team	O
wins	O
the	O
series	O
.	O
70	O
chapter	O
7.	O
prediction	O
to	O
choose	O
a	O
prior	B
distribution	I
,	O
i	O
got	O
some	O
statistics	O
from	O
http	O
:	O
//www.nhl	O
.	O
com	O
,	O
speciﬁcally	O
the	O
average	O
goals	O
per	O
game	O
for	O
each	O
team	O
in	O
the	O
2010-11	O
season	O
.	O
the	O
distribution	B
is	O
roughly	O
gaussian	O
with	O
mean	O
2.8	O
and	O
standard	O
deviation	O
0.3.	O
the	O
gaussian	O
distribution	B
is	O
continuous	O
,	O
but	O
we	O
’	O
ll	O
approximate	O
it	O
with	O
a	O
discrete	O
pmf	O
.	O
thinkbayes	O
provides	O
makegaussianpmf	O
to	O
do	O
exactly	O
that	O
:	O
def	O
makegaussianpmf	O
(	O
mu	O
,	O
sigma	O
,	O
num_sigmas	O
,	O
n=101	O
)	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
low	O
=	O
mu	O
-	O
num_sigmas*sigma	O
high	O
=	O
mu	O
+	O
num_sigmas*sigma	O
for	O
x	O
in	O
numpy.linspace	O
(	O
low	O
,	O
high	O
,	O
n	O
)	O
:	O
p	O
=	O
scipy.stats.norm.pdf	O
(	O
x	O
,	O
mu	O
,	O
sigma	O
)	O
pmf.set	O
(	O
x	O
,	O
p	O
)	O
pmf.normalize	O
(	O
)	O
return	O
pmf	O
mu	O
and	O
sigma	O
are	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
gaussian	O
distri-	O
bution	O
.	O
num_sigmas	O
is	O
the	O
number	O
of	O
standard	O
deviations	O
above	O
and	O
below	O
the	O
mean	O
that	O
the	O
pmf	O
will	O
span	O
,	O
and	O
n	O
is	O
the	O
number	O
of	O
values	O
in	O
the	O
pmf	O
.	O
again	O
we	O
use	O
numpy.linspace	O
to	O
make	O
an	O
array	O
of	O
n	O
equally	O
spaced	O
values	O
between	O
low	O
and	O
high	O
,	O
including	O
both	O
.	O
norm.pdf	O
evaluates	O
the	O
gaussian	O
probability	B
density	I
function	I
(	O
pdf	O
)	O
.	O
getting	O
back	O
to	O
the	O
hockey	B
problem	O
,	O
here	O
’	O
s	O
the	O
deﬁnition	O
for	O
a	O
suite	B
of	O
hy-	O
potheses	O
about	O
the	O
value	O
of	O
λ.	O
class	O
hockey	B
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
)	O
:	O
pmf	O
=	O
thinkbayes.makegaussianpmf	O
(	O
2.7	O
,	O
0.3	O
,	O
4	O
)	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
pmf	O
)	O
so	O
the	O
prior	B
distribution	I
is	O
gaussian	O
with	O
mean	O
2.7	O
,	O
standard	O
deviation	O
0.3	O
,	O
and	O
it	O
spans	O
4	O
sigmas	O
above	O
and	O
below	O
the	O
mean	O
.	O
as	O
always	O
,	O
we	O
have	O
to	O
decide	O
how	O
to	O
represent	O
each	O
hypothesis	O
;	O
in	O
this	O
case	O
i	O
represent	O
the	O
hypothesis	O
that	O
λ	O
=	O
x	O
with	O
the	O
ﬂoating-point	O
value	O
x	O
.	O
7.2.	O
poisson	O
processes	O
71	O
7.2	O
poisson	O
processes	O
in	O
mathematical	O
statistics	O
,	O
a	O
process	B
is	O
a	O
stochastic	O
model	O
of	O
a	O
physical	O
sys-	O
tem	O
(	O
“	O
stochastic	O
”	O
means	O
that	O
the	O
model	O
has	O
some	O
kind	O
of	O
randomness	O
in	O
it	O
)	O
.	O
for	O
example	O
,	O
a	O
bernoulli	O
process	B
is	O
a	O
model	O
of	O
a	O
sequence	O
of	O
events	O
,	O
called	O
trials	O
,	O
in	O
which	O
each	O
trial	O
has	O
two	O
possible	O
outcomes	O
,	O
like	O
success	O
and	O
fail-	O
ure	O
.	O
so	O
a	O
bernoulli	O
process	B
is	O
a	O
natural	O
model	O
for	O
a	O
series	O
of	O
coin	O
ﬂips	O
,	O
or	O
a	O
series	O
of	O
shots	O
on	O
goal	O
.	O
a	O
poisson	O
process	B
is	O
the	O
continuous	O
version	O
of	O
a	O
bernoulli	O
process	B
,	O
where	O
an	O
event	O
can	O
occur	O
at	O
any	O
point	O
in	O
time	O
with	O
equal	O
probability	B
.	O
poisson	O
processes	O
can	O
be	O
used	O
to	O
model	O
customers	O
arriving	O
in	O
a	O
store	O
,	O
buses	O
arriving	O
at	O
a	O
bus	O
stop	O
,	O
or	O
goals	O
scored	O
in	O
a	O
hockey	B
game	O
.	O
in	O
many	O
real	O
systems	O
the	O
probability	B
of	O
an	O
event	O
changes	O
over	O
time	O
.	O
cus-	O
tomers	O
are	O
more	O
likely	O
to	O
go	O
to	O
a	O
store	O
at	O
certain	O
times	O
of	O
day	O
,	O
buses	O
are	O
supposed	O
to	O
arrive	O
at	O
ﬁxed	O
intervals	O
,	O
and	O
goals	O
are	O
more	O
or	O
less	O
likely	O
at	O
different	O
times	O
during	O
a	O
game	O
.	O
but	O
all	O
models	O
are	O
based	O
on	O
simpliﬁcations	O
,	O
and	O
in	O
this	O
case	O
modeling	B
a	O
hockey	B
game	O
with	O
a	O
poisson	O
process	B
is	O
a	O
reasonable	O
choice	O
.	O
heuer	O
,	O
müller	O
and	O
rubner	O
(	O
2010	O
)	O
analyze	O
scoring	O
in	O
a	O
german	O
soccer	O
league	O
and	O
come	O
to	O
the	O
same	O
conclusion	O
;	O
see	O
http	O
:	O
//www.cimat.mx/eventos/vpec10/img/	O
poisson.pdf	O
.	O
the	O
beneﬁt	O
of	O
using	O
this	O
model	O
is	O
that	O
we	O
can	O
compute	O
the	O
distribution	B
of	O
goals	O
per	O
game	O
efﬁciently	O
,	O
as	O
well	O
as	O
the	O
distribution	B
of	O
time	O
between	O
goals	O
.	O
speciﬁcally	O
,	O
if	O
the	O
average	O
number	O
of	O
goals	O
in	O
a	O
game	O
is	O
lam	O
,	O
the	O
distribution	B
of	O
goals	O
per	O
game	O
is	O
given	O
by	O
the	O
poisson	O
pmf	O
:	O
def	O
evalpoissonpmf	O
(	O
k	O
,	O
lam	O
)	O
:	O
return	O
(	O
lam	O
)	O
**k	O
*	O
math.exp	O
(	O
-lam	O
)	O
/	O
math.factorial	O
(	O
k	O
)	O
and	O
the	O
distribution	B
of	O
time	O
between	O
goals	O
is	O
given	O
by	O
the	O
exponential	O
pdf	O
:	O
def	O
evalexponentialpdf	O
(	O
x	O
,	O
lam	O
)	O
:	O
return	O
lam	O
*	O
math.exp	O
(	O
-lam	O
*	O
x	O
)	O
i	O
use	O
the	O
variable	O
lam	O
because	O
lambda	O
is	O
a	O
reserved	O
keyword	O
in	O
python	O
.	O
both	O
of	O
these	O
functions	O
are	O
in	O
thinkbayes.py	O
.	O
7.3	O
the	O
posteriors	O
now	O
we	O
can	O
compute	O
the	O
likelihood	B
that	O
a	O
team	O
with	O
a	O
hypothetical	O
value	O
of	O
lam	O
scores	O
k	O
goals	O
in	O
a	O
game	O
:	O
72	O
chapter	O
7.	O
prediction	O
figure	O
7.1	O
:	O
posterior	B
distribution	I
of	O
the	O
number	O
of	O
goals	O
per	O
game	O
.	O
#	O
class	O
hockey	B
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
lam	O
=	O
hypo	O
k	O
=	O
data	O
like	O
=	O
thinkbayes.evalpoissonpmf	O
(	O
k	O
,	O
lam	O
)	O
return	O
like	O
each	O
hypothesis	O
is	O
a	O
possible	O
value	O
of	O
λ	O
;	O
data	O
is	O
the	O
observed	O
number	O
of	O
goals	O
,	O
k.	O
with	O
the	O
likelihood	B
function	I
in	O
place	O
,	O
we	O
can	O
make	O
a	O
suite	B
for	O
each	O
team	O
and	O
update	O
them	O
with	O
the	O
scores	O
from	O
the	O
ﬁrst	O
four	O
games	O
.	O
suite1	O
=	O
hockey	B
(	O
'bruins	O
'	O
)	O
suite1.updateset	O
(	O
[	O
0	O
,	O
2	O
,	O
8	O
,	O
4	O
]	O
)	O
suite2	O
=	O
hockey	B
(	O
'canucks	O
'	O
)	O
suite2.updateset	O
(	O
[	O
1	O
,	O
3	O
,	O
1	O
,	O
0	O
]	O
)	O
figure	O
7.1	O
shows	O
the	O
resulting	O
posterior	B
distributions	O
for	O
lam	O
.	O
based	O
on	O
the	O
ﬁrst	O
four	O
games	O
,	O
the	O
most	O
likely	O
values	O
for	O
lam	O
are	O
2.6	O
for	O
the	O
canucks	O
and	O
2.9	O
for	O
the	O
bruins	O
.	O
7.4	O
the	O
distribution	B
of	O
goals	O
to	O
compute	O
the	O
probability	B
that	O
each	O
team	O
wins	O
the	O
next	O
game	O
,	O
we	O
need	O
to	O
compute	O
the	O
distribution	B
of	O
goals	O
for	O
each	O
team	O
.	O
1.52.02.53.03.54.0goals	O
per	O
game0.0000.0020.0040.0060.0080.0100.0120.0140.0160.018probabilitybruinscanucks	O
7.4.	O
the	O
distribution	B
of	O
goals	O
73	O
figure	O
7.2	O
:	O
distribution	B
of	O
goals	O
in	O
a	O
single	O
game	O
.	O
if	O
we	O
knew	O
the	O
value	O
of	O
lam	O
exactly	O
,	O
we	O
could	O
use	O
the	O
poisson	O
distribution	B
again	O
.	O
thinkbayes	O
provides	O
a	O
method	O
that	O
computes	O
a	O
truncated	O
approxi-	O
mation	O
of	O
a	O
poisson	O
distribution	B
:	O
def	O
makepoissonpmf	O
(	O
lam	O
,	O
high	O
)	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
k	O
in	O
xrange	O
(	O
0	O
,	O
high+1	O
)	O
:	O
p	O
=	O
evalpoissonpmf	O
(	O
k	O
,	O
lam	O
)	O
pmf.set	O
(	O
k	O
,	O
p	O
)	O
pmf.normalize	O
(	O
)	O
return	O
pmf	O
the	O
range	O
of	O
values	O
in	O
the	O
computed	O
pmf	O
is	O
from	O
0	O
to	O
high	O
.	O
so	O
if	O
the	O
value	O
of	O
lam	O
were	O
exactly	O
3.4	O
,	O
we	O
would	O
compute	O
:	O
lam	O
=	O
3.4	O
goal_dist	O
=	O
thinkbayes.makepoissonpmf	O
(	O
lam	O
,	O
10	O
)	O
i	O
chose	O
the	O
upper	O
bound	O
,	O
10	O
,	O
because	O
the	O
probability	B
of	O
scoring	O
more	O
than	O
10	O
goals	O
in	O
a	O
game	O
is	O
quite	O
low	O
.	O
that	O
’	O
s	O
simple	O
enough	O
so	O
far	O
;	O
the	O
problem	O
is	O
that	O
we	O
don	O
’	O
t	O
know	O
the	O
value	O
of	O
lam	O
exactly	O
.	O
instead	O
,	O
we	O
have	O
a	O
distribution	B
of	O
possible	O
values	O
for	O
lam	O
.	O
for	O
each	O
value	O
of	O
lam	O
,	O
the	O
distribution	B
of	O
goals	O
is	O
poisson	O
.	O
so	O
the	O
overall	O
distribution	B
of	O
goals	O
is	O
a	O
mixture	B
of	O
these	O
poisson	O
distributions	O
,	O
weighted	O
according	O
to	O
the	O
probabilities	O
in	O
the	O
distribution	B
of	O
lam	O
.	O
given	O
the	O
posterior	B
distribution	I
of	O
lam	O
,	O
here	O
’	O
s	O
the	O
code	O
that	O
makes	O
the	O
dis-	O
tribution	O
of	O
goals	O
:	O
0246810goals0.000.050.100.150.200.25probabilitybruinscanucks	O
74	O
chapter	O
7.	O
prediction	O
figure	O
7.3	O
:	O
distribution	B
of	O
time	O
between	O
goals	O
.	O
def	O
makegoalpmf	O
(	O
suite	B
)	O
:	O
metapmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
lam	O
,	O
prob	O
in	O
suite.items	O
(	O
)	O
:	O
pmf	O
=	O
thinkbayes.makepoissonpmf	O
(	O
lam	O
,	O
10	O
)	O
metapmf.set	O
(	O
pmf	O
,	O
prob	O
)	O
mix	O
=	O
thinkbayes.makemixture	O
(	O
metapmf	O
)	O
return	O
mix	O
for	O
each	O
value	O
of	O
lam	O
we	O
make	O
a	O
poisson	O
pmf	O
and	O
add	O
it	O
to	O
the	O
meta-pmf	O
.	O
i	O
call	O
it	O
a	O
meta-pmf	O
because	O
it	O
is	O
a	O
pmf	O
that	O
contains	O
pmfs	O
as	O
its	O
values	O
.	O
then	O
we	O
use	O
makemixture	O
to	O
compute	O
the	O
mixture	B
(	O
we	O
saw	O
makemixture	O
in	O
section	O
5.6	O
)	O
.	O
figure	O
7.2	O
shows	O
the	O
resulting	O
distribution	B
of	O
goals	O
for	O
the	O
bruins	O
and	O
canucks	O
.	O
the	O
bruins	O
are	O
less	O
likely	O
to	O
score	O
3	O
goals	O
or	O
fewer	O
in	O
the	O
next	O
game	O
,	O
and	O
more	O
likely	O
to	O
score	O
4	O
or	O
more	O
.	O
7.5	O
the	O
probability	B
of	O
winning	O
to	O
get	O
the	O
probability	B
of	O
winning	O
,	O
ﬁrst	O
we	O
compute	O
the	O
distribution	B
of	O
the	O
goal	O
differential	O
:	O
goal_dist1	O
=	O
makegoalpmf	O
(	O
suite1	O
)	O
goal_dist2	O
=	O
makegoalpmf	O
(	O
suite2	O
)	O
diff	O
=	O
goal_dist1	O
-	O
goal_dist2	O
0.00.51.01.52.0games	O
until	O
goal0.00000.00050.00100.00150.00200.00250.0030probabilitybruinscanucks	O
7.6.	O
sudden	B
death	I
75	O
the	O
subtraction	O
operator	O
invokes	O
pmf.__sub__	O
,	O
which	O
enumerates	O
pairs	O
of	O
values	O
and	O
computes	O
the	O
difference	O
.	O
subtracting	O
two	O
distributions	O
is	O
almost	O
the	O
same	O
as	O
adding	O
,	O
which	O
we	O
saw	O
in	O
section	O
5.4.	O
if	O
the	O
goal	O
differential	O
is	O
positive	O
,	O
the	O
bruins	O
win	O
;	O
if	O
negative	O
,	O
the	O
canucks	O
win	O
;	O
if	O
0	O
,	O
it	O
’	O
s	O
a	O
tie	O
:	O
p_win	O
=	O
diff.probgreater	O
(	O
0	O
)	O
p_loss	O
=	O
diff.probless	O
(	O
0	O
)	O
p_tie	O
=	O
diff.prob	O
(	O
0	O
)	O
with	O
the	O
distributions	O
from	O
the	O
previous	O
section	O
,	O
p_win	O
is	O
46	O
%	O
,	O
p_loss	O
is	O
37	O
%	O
,	O
and	O
p_tie	O
is	O
17	O
%	O
.	O
in	O
the	O
event	O
of	O
a	O
tie	O
at	O
the	O
end	O
of	O
“	O
regulation	O
play	O
,	O
”	O
the	O
teams	O
play	O
overtime	B
periods	O
until	O
one	O
team	O
scores	O
.	O
since	O
the	O
game	O
ends	O
immediately	O
when	O
the	O
ﬁrst	O
goal	O
is	O
scored	O
,	O
this	O
overtime	B
format	O
is	O
known	O
as	O
“	O
sudden	O
death.	O
”	O
7.6	O
sudden	B
death	I
to	O
compute	O
the	O
probability	B
of	O
winning	O
in	O
a	O
sudden	B
death	I
overtime	O
,	O
the	O
im-	O
portant	O
statistic	O
is	O
not	O
goals	O
per	O
game	O
,	O
but	O
time	O
until	O
the	O
ﬁrst	O
goal	O
.	O
the	O
assumption	O
that	O
goal-scoring	O
is	O
a	O
poisson	O
process	B
implies	O
that	O
the	O
time	O
be-	O
tween	O
goals	O
is	O
exponentially	O
distributed	O
.	O
given	O
lam	O
,	O
we	O
can	O
compute	O
the	O
time	O
between	O
goals	O
like	O
this	O
:	O
lam	O
=	O
3.4	O
time_dist	O
=	O
thinkbayes.makeexponentialpmf	O
(	O
lam	O
,	O
high=2	O
,	O
n=101	O
)	O
high	O
is	O
the	O
upper	O
bound	O
of	O
the	O
distribution	B
.	O
in	O
this	O
case	O
i	O
chose	O
2	O
,	O
because	O
the	O
probability	B
of	O
going	O
more	O
than	O
two	O
games	O
without	O
scoring	O
is	O
small	O
.	O
n	O
is	O
the	O
number	O
of	O
values	O
in	O
the	O
pmf	O
.	O
if	O
we	O
know	O
lam	O
exactly	O
,	O
that	O
’	O
s	O
all	O
there	O
is	O
to	O
it	O
.	O
but	O
we	O
don	O
’	O
t	O
;	O
instead	O
we	O
have	O
a	O
posterior	B
distribution	I
of	O
possible	O
values	O
.	O
so	O
as	O
we	O
did	O
with	O
the	O
dis-	O
tribution	O
of	O
goals	O
,	O
we	O
make	O
a	O
meta-pmf	O
and	O
compute	O
a	O
mixture	B
of	O
pmfs	O
.	O
def	O
makegoaltimepmf	O
(	O
suite	B
)	O
:	O
metapmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
lam	O
,	O
prob	O
in	O
suite.items	O
(	O
)	O
:	O
pmf	O
=	O
thinkbayes.makeexponentialpmf	O
(	O
lam	O
,	O
high=2	O
,	O
n=2001	O
)	O
metapmf.set	O
(	O
pmf	O
,	O
prob	O
)	O
76	O
chapter	O
7.	O
prediction	O
mix	O
=	O
thinkbayes.makemixture	O
(	O
metapmf	O
)	O
return	O
mix	O
figure	O
7.3	O
shows	O
the	O
resulting	O
distributions	O
.	O
for	O
time	O
values	O
less	O
than	O
one	O
period	O
(	O
one	O
third	O
of	O
a	O
game	O
)	O
,	O
the	O
bruins	O
are	O
more	O
likely	O
to	O
score	O
.	O
the	O
time	O
until	O
the	O
canucks	O
score	O
is	O
more	O
likely	O
to	O
be	O
longer	O
.	O
i	O
set	O
the	O
number	O
of	O
values	O
,	O
n	O
,	O
fairly	O
high	O
in	O
order	O
to	O
minimize	O
the	O
number	O
of	O
ties	O
,	O
since	O
it	O
is	O
not	O
possible	O
for	O
both	O
teams	O
to	O
score	O
simultaneously	O
.	O
now	O
we	O
compute	O
the	O
probability	B
that	O
the	O
bruins	O
score	O
ﬁrst	O
:	O
time_dist1	O
=	O
makegoaltimepmf	O
(	O
suite1	O
)	O
time_dist2	O
=	O
makegoaltimepmf	O
(	O
suite2	O
)	O
p_overtime	O
=	O
thinkbayes.pmfprobless	O
(	O
time_dist1	O
,	O
time_dist2	O
)	O
for	O
the	O
bruins	O
,	O
the	O
probability	B
of	O
winning	O
in	O
overtime	B
is	O
52	O
%	O
.	O
finally	O
,	O
the	O
total	B
probability	I
of	O
winning	O
is	O
the	O
chance	O
of	O
winning	O
at	O
the	O
end	O
of	O
regulation	O
play	O
plus	O
the	O
probability	B
of	O
winning	O
in	O
overtime	B
.	O
p_tie	O
=	O
diff.prob	O
(	O
0	O
)	O
p_overtime	O
=	O
thinkbayes.pmfprobless	O
(	O
time_dist1	O
,	O
time_dist2	O
)	O
p_win	O
=	O
diff.probgreater	O
(	O
0	O
)	O
+	O
p_tie	O
*	O
p_overtime	O
for	O
the	O
bruins	O
,	O
the	O
overall	O
chance	O
of	O
winning	O
the	O
next	O
game	O
is	O
55	O
%	O
.	O
to	O
win	O
the	O
series	O
,	O
the	O
bruins	O
can	O
either	O
win	O
the	O
next	O
two	O
games	O
or	O
split	O
the	O
next	O
two	O
and	O
win	O
the	O
third	O
.	O
again	O
,	O
we	O
can	O
compute	O
the	O
total	B
probability	I
:	O
#	O
win	O
the	O
next	O
two	O
p_series	O
=	O
p_win**2	O
#	O
split	O
the	O
next	O
two	O
,	O
win	O
the	O
third	O
p_series	O
+=	O
2	O
*	O
p_win	O
*	O
(	O
1-p_win	O
)	O
*	O
p_win	O
the	O
bruins	O
chance	O
of	O
winning	O
the	O
series	O
is	O
57	O
%	O
.	O
and	O
in	O
2011	O
,	O
they	O
did	O
.	O
7.7	O
discussion	O
as	O
always	O
,	O
the	O
analysis	O
in	O
this	O
chapter	O
is	O
based	O
on	O
modeling	B
decisions	O
,	O
and	O
modeling	B
is	O
almost	O
always	O
an	O
iterative	O
process	O
.	O
in	O
general	O
,	O
you	O
want	O
to	O
start	O
with	O
something	O
simple	O
that	O
yields	O
an	O
approximate	O
answer	O
,	O
identify	O
likely	O
sources	O
of	O
error	B
,	O
and	O
look	O
for	O
opportunities	O
for	O
improvement	O
.	O
in	O
this	O
example	O
,	O
i	O
would	O
consider	O
these	O
options	O
:	O
7.7.	O
discussion	O
77	O
•	O
i	O
chose	O
a	O
prior	B
based	O
on	O
the	O
average	O
goals	O
per	O
game	O
for	O
each	O
team	O
.	O
but	O
this	O
statistic	O
is	O
averaged	O
across	O
all	O
opponents	O
.	O
against	O
a	O
particu-	O
lar	O
opponent	O
,	O
we	O
might	O
expect	O
more	O
variability	O
.	O
for	O
example	O
,	O
if	O
the	O
team	O
with	O
the	O
best	O
offense	O
plays	O
the	O
team	O
with	O
the	O
worst	O
defense	O
,	O
the	O
expected	O
goals	O
per	O
game	O
might	O
be	O
several	O
standard	O
deviations	O
above	O
the	O
mean	O
.	O
•	O
for	O
data	O
i	O
used	O
only	O
the	O
ﬁrst	O
four	O
games	O
of	O
the	O
championship	O
series	O
.	O
if	O
the	O
same	O
teams	O
played	O
each	O
other	O
during	O
the	O
regular	O
season	O
,	O
i	O
could	O
use	O
the	O
results	O
from	O
those	O
games	O
as	O
well	O
.	O
one	O
complication	O
is	O
that	O
the	O
composition	O
of	O
teams	O
changes	O
during	O
the	O
season	O
due	O
to	O
trades	O
and	O
injuries	O
.	O
so	O
it	O
might	O
be	O
best	O
to	O
give	O
more	O
weight	O
to	O
recent	O
games	O
.	O
•	O
to	O
take	O
advantage	O
of	O
all	O
available	O
information	O
,	O
we	O
could	O
use	O
results	O
from	O
all	O
regular	O
season	O
games	O
to	O
estimate	O
each	O
team	O
’	O
s	O
goal	O
scoring	O
rate	O
,	O
possibly	O
adjusted	O
by	O
estimating	O
an	O
additional	O
factor	O
for	O
each	O
pair-	O
wise	O
match-up	O
.	O
this	O
approach	O
would	O
be	O
more	O
complicated	O
,	O
but	O
it	O
is	O
still	O
feasible	O
.	O
for	O
the	O
ﬁrst	O
option	O
,	O
we	O
could	O
use	O
the	O
results	O
from	O
the	O
regular	O
season	O
to	O
esti-	O
mate	O
the	O
variability	O
across	O
all	O
pairwise	O
match-ups	O
.	O
thanks	O
to	O
dirk	O
hoag	O
at	O
http	O
:	O
//forechecker.blogspot.com/	O
,	O
i	O
was	O
able	O
to	O
get	O
the	O
number	O
of	O
goals	O
scored	O
during	O
regulation	O
play	O
(	O
not	O
overtime	B
)	O
for	O
each	O
game	O
in	O
the	O
regular	O
season	O
.	O
teams	O
in	O
different	O
conferences	O
only	O
play	O
each	O
other	O
one	O
or	O
two	O
times	O
in	O
the	O
regular	O
season	O
,	O
so	O
i	O
focused	O
on	O
pairs	O
that	O
played	O
each	O
other	O
4–6	O
times	O
.	O
for	O
each	O
pair	O
,	O
i	O
computed	O
the	O
average	O
goals	O
per	O
game	O
,	O
which	O
is	O
an	O
estimate	O
of	O
λ	O
,	O
then	O
plotted	O
the	O
distribution	B
of	O
these	O
estimates	O
.	O
the	O
mean	O
of	O
these	O
estimates	O
is	O
2.8	O
,	O
again	O
,	O
but	O
the	O
standard	O
deviation	O
is	O
0.85	O
,	O
substantially	O
higher	O
than	O
what	O
we	O
got	O
computing	O
one	O
estimate	O
for	O
each	O
team	O
.	O
if	O
we	O
run	O
the	O
analysis	O
again	O
with	O
the	O
higher-variance	O
prior	B
,	O
the	O
probability	B
that	O
the	O
bruins	O
win	O
the	O
series	O
is	O
80	O
%	O
,	O
substantially	O
higher	O
than	O
the	O
result	O
with	O
the	O
low-variance	O
prior	B
,	O
57	O
%	O
.	O
so	O
it	O
turns	O
out	O
that	O
the	O
results	O
are	O
sensitive	O
to	O
the	O
prior	B
,	O
which	O
makes	O
sense	O
considering	O
how	O
little	O
data	O
we	O
have	O
to	O
work	O
with	O
.	O
based	O
on	O
the	O
differ-	O
ence	O
between	O
the	O
low-variance	O
model	O
and	O
the	O
high-variable	O
model	O
,	O
it	O
seems	O
worthwhile	O
to	O
put	O
some	O
effort	O
into	O
getting	O
the	O
prior	B
right	O
.	O
78	O
chapter	O
7.	O
prediction	O
the	O
code	O
and	O
data	O
for	O
this	O
chapter	O
are	O
available	O
from	O
http	O
:	O
//thinkbayes	O
.	O
com/hockey.py	O
and	O
http	O
:	O
//thinkbayes.com/hockey_data.csv	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
7.8	O
exercises	O
exercise	O
7.1.	O
if	O
buses	O
arrive	O
at	O
a	O
bus	O
stop	O
every	O
20	O
minutes	O
,	O
and	O
you	O
arrive	O
at	O
the	O
bus	O
stop	O
at	O
a	O
random	O
time	O
,	O
your	O
wait	O
time	O
until	O
the	O
bus	O
arrives	O
is	O
uniformly	O
distributed	O
from	O
0	O
to	O
20	O
minutes	O
.	O
but	O
in	O
reality	O
,	O
there	O
is	O
variability	O
in	O
the	O
time	O
between	O
buses	O
.	O
suppose	O
you	O
are	O
waiting	O
for	O
a	O
bus	O
,	O
and	O
you	O
know	O
the	O
historical	O
distribution	B
of	O
time	O
between	O
buses	O
.	O
compute	O
your	O
distribution	B
of	O
wait	O
times	O
.	O
hint	O
:	O
suppose	O
that	O
the	O
time	O
between	O
buses	O
is	O
either	O
5	O
or	O
10	O
minutes	O
with	O
equal	O
probability	B
.	O
what	O
is	O
the	O
probability	B
that	O
you	O
arrive	O
during	O
one	O
of	O
the	O
10	O
minute	O
intervals	O
?	O
i	O
solve	O
a	O
version	O
of	O
this	O
problem	O
in	O
the	O
next	O
chapter	O
.	O
exercise	O
7.2.	O
suppose	O
that	O
passengers	O
arriving	O
at	O
the	O
bus	O
stop	O
are	O
well-modeled	O
by	O
a	O
poisson	O
process	B
with	O
parameter	B
λ.	O
if	O
you	O
arrive	O
at	O
the	O
stop	O
and	O
ﬁnd	O
3	O
people	O
waiting	O
,	O
what	O
is	O
your	O
posterior	B
distribution	I
for	O
the	O
time	O
since	O
the	O
last	O
bus	O
arrived	O
.	O
i	O
solve	O
a	O
version	O
of	O
this	O
problem	O
in	O
the	O
next	O
chapter	O
.	O
exercise	O
7.3.	O
suppose	O
that	O
you	O
are	O
an	O
ecologist	O
sampling	O
the	O
insect	O
population	O
in	O
a	O
new	O
environment	O
.	O
you	O
deploy	O
100	O
traps	O
in	O
a	O
test	O
area	O
and	O
come	O
back	O
the	O
next	O
day	O
to	O
check	O
on	O
them	O
.	O
you	O
ﬁnd	O
that	O
37	O
traps	O
have	O
been	O
triggered	O
,	O
trapping	O
an	O
insect	O
inside	O
.	O
once	O
a	O
trap	O
triggers	O
,	O
it	O
can	O
not	O
trap	O
another	O
insect	O
until	O
it	O
has	O
been	O
reset	O
.	O
if	O
you	O
reset	O
the	O
traps	O
and	O
come	O
back	O
in	O
two	O
days	O
,	O
how	O
many	O
traps	O
do	O
you	O
expect	O
to	O
ﬁnd	O
triggered	O
?	O
compute	O
a	O
posterior	B
predictive	O
distribution	B
for	O
the	O
number	O
of	O
traps	O
.	O
exercise	O
7.4.	O
suppose	O
you	O
are	O
the	O
manager	O
of	O
an	O
apartment	O
building	O
with	O
100	O
light	O
bulbs	O
in	O
common	O
areas	O
.	O
it	O
is	O
your	O
responsibility	O
to	O
replace	O
light	O
bulbs	O
when	O
they	O
break	O
.	O
on	O
january	O
1	O
,	O
all	O
100	O
bulbs	O
are	O
working	O
.	O
when	O
you	O
inspect	O
them	O
on	O
february	O
1	O
,	O
you	O
ﬁnd	O
3	O
light	O
bulbs	O
out	O
.	O
if	O
you	O
come	O
back	O
on	O
april	O
1	O
,	O
how	O
many	O
light	O
bulbs	O
do	O
you	O
expect	O
to	O
ﬁnd	O
broken	O
?	O
in	O
the	O
previous	O
exercise	O
,	O
you	O
could	O
reasonably	O
assume	O
that	O
an	O
event	O
is	O
equally	O
likely	O
at	O
any	O
time	O
.	O
for	O
light	O
bulbs	O
,	O
the	O
likelihood	B
of	O
failure	O
depends	O
on	O
the	O
age	O
of	O
the	O
7.8.	O
exercises	O
79	O
bulb	O
.	O
speciﬁcally	O
,	O
old	O
bulbs	O
have	O
an	O
increasing	O
failure	O
rate	O
due	O
to	O
evaporation	O
of	O
the	O
ﬁlament	O
.	O
this	O
problem	O
is	O
more	O
open-ended	O
than	O
some	O
;	O
you	O
will	O
have	O
to	O
make	O
modeling	B
de-	O
cisions	O
.	O
you	O
might	O
want	O
to	O
read	O
about	O
the	O
weibull	O
distribution	B
(	O
http	O
:	O
//	O
en	O
.	O
wikipedia	O
.	O
org/	O
wiki/	O
weibull_	O
distribution	B
)	O
.	O
or	O
you	O
might	O
want	O
to	O
look	O
around	O
for	O
information	O
about	O
light	O
bulb	O
survival	O
curves	O
.	O
80	O
chapter	O
7.	O
prediction	O
chapter	O
8	O
observer	B
bias	I
8.1	O
the	O
red	O
line	O
problem	O
in	O
massachusetts	O
,	O
the	O
red	O
line	O
is	O
a	O
subway	O
that	O
connects	O
cambridge	O
and	O
boston	O
.	O
when	O
i	O
was	O
working	O
in	O
cambridge	O
i	O
took	O
the	O
red	O
line	O
from	O
kendall	O
square	O
to	O
south	O
station	O
and	O
caught	O
the	O
commuter	O
rail	O
to	O
needham	O
.	O
during	O
rush	O
hour	O
red	O
line	O
trains	O
run	O
every	O
7–8	O
minutes	O
,	O
on	O
average	O
.	O
when	O
i	O
arrived	O
at	O
the	O
station	O
,	O
i	O
could	O
estimate	O
the	O
time	O
until	O
the	O
next	O
train	O
based	O
on	O
the	O
number	O
of	O
passengers	O
on	O
the	O
platform	O
.	O
if	O
there	O
were	O
only	O
a	O
few	O
people	O
,	O
i	O
inferred	O
that	O
i	O
just	O
missed	O
a	O
train	O
and	O
expected	O
to	O
wait	O
about	O
7	O
minutes	O
.	O
if	O
there	O
were	O
more	O
passengers	O
,	O
i	O
expected	O
the	O
train	O
to	O
arrive	O
sooner	O
.	O
but	O
if	O
there	O
were	O
a	O
large	O
number	O
of	O
passengers	O
,	O
i	O
suspected	O
that	O
trains	O
were	O
not	O
running	O
on	O
schedule	O
,	O
so	O
i	O
would	O
go	O
back	O
to	O
the	O
street	O
level	O
and	O
get	O
a	O
taxi	O
.	O
while	O
i	O
was	O
waiting	O
for	O
trains	O
,	O
i	O
thought	O
about	O
how	O
bayesian	O
estimation	O
could	O
help	O
predict	O
my	O
wait	O
time	O
and	O
decide	O
when	O
i	O
should	O
give	O
up	O
and	O
take	O
a	O
taxi	O
.	O
this	O
chapter	O
presents	O
the	O
analysis	O
i	O
came	O
up	O
with	O
.	O
this	O
chapter	O
is	O
based	O
on	O
a	O
project	O
by	O
brendan	O
ritter	O
and	O
kai	O
austin	O
,	O
who	O
took	O
a	O
class	O
with	O
me	O
at	O
olin	O
college	O
.	O
the	O
code	O
in	O
this	O
chapter	O
is	O
available	O
from	O
http	O
:	O
//thinkbayes.com/redline.py	O
.	O
the	O
code	O
i	O
used	O
to	O
collect	O
data	O
is	O
in	O
http	O
:	O
//thinkbayes.com/redline_data.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
82	O
chapter	O
8.	O
observer	B
bias	I
figure	O
8.1	O
:	O
pmf	O
of	O
gaps	O
between	O
trains	O
,	O
based	O
on	O
collected	O
data	O
,	O
smoothed	O
by	O
kde	O
.	O
z	O
is	O
the	O
actual	O
distribution	B
;	O
zb	O
is	O
the	O
biased	O
distribution	O
seen	O
by	O
passengers	O
.	O
8.2	O
the	O
model	O
before	O
we	O
get	O
to	O
the	O
analysis	O
,	O
we	O
have	O
to	O
make	O
some	O
modeling	B
decisions	O
.	O
first	O
,	O
i	O
will	O
treat	O
passenger	O
arrivals	O
as	O
a	O
poisson	O
process	B
,	O
which	O
means	O
i	O
assume	O
that	O
passengers	O
are	O
equally	O
likely	O
to	O
arrive	O
at	O
any	O
time	O
,	O
and	O
that	O
they	O
arrive	O
at	O
an	O
unknown	O
rate	O
,	O
λ	O
,	O
measured	O
in	O
passengers	O
per	O
minute	O
.	O
since	O
i	O
observe	O
passengers	O
during	O
a	O
short	O
period	O
of	O
time	O
,	O
and	O
at	O
the	O
same	O
time	O
every	O
day	O
,	O
i	O
assume	O
that	O
λ	O
is	O
constant	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
arrival	O
process	O
for	O
trains	O
is	O
not	O
poisson	O
.	O
trains	O
to	O
boston	O
are	O
supposed	O
to	O
leave	O
from	O
the	O
end	O
of	O
the	O
line	O
(	O
alewife	O
station	O
)	O
every	O
7–8	O
minutes	O
during	O
peak	O
times	O
,	O
but	O
by	O
the	O
time	O
they	O
get	O
to	O
kendall	O
square	O
,	O
the	O
time	O
between	O
trains	O
varies	O
between	O
3	O
and	O
12	O
minutes	O
.	O
to	O
gather	O
data	O
on	O
the	O
time	O
between	O
trains	O
,	O
i	O
wrote	O
a	O
script	O
that	O
downloads	O
real-time	O
data	O
from	O
http	O
:	O
//www.mbta.com/rider_tools/developers/	O
,	O
se-	O
lects	O
south-bound	O
trains	O
arriving	O
at	O
kendall	O
square	O
,	O
and	O
records	O
their	O
ar-	O
rival	O
times	O
in	O
a	O
database	O
.	O
i	O
ran	O
the	O
script	O
from	O
4pm	O
to	O
6pm	O
every	O
weekday	O
for	O
5	O
days	O
,	O
and	O
recorded	O
about	O
15	O
arrivals	O
per	O
day	O
.	O
then	O
i	O
computed	O
the	O
time	O
between	O
consecutive	O
arrivals	O
;	O
the	O
distribution	B
of	O
these	O
gaps	O
is	O
shown	O
in	O
figure	O
8.1	O
,	O
labeled	O
z.	O
if	O
you	O
stood	O
on	O
the	O
platform	O
from	O
4pm	O
to	O
6pm	O
and	O
recorded	O
the	O
time	O
be-	O
tween	O
trains	O
,	O
this	O
is	O
the	O
distribution	B
you	O
would	O
see	O
.	O
but	O
if	O
you	O
arrive	O
at	O
some	O
random	O
time	O
(	O
without	O
regard	O
to	O
the	O
train	O
schedule	O
)	O
you	O
would	O
see	O
a	O
05101520time	O
(	O
min	O
)	O
0.0000.0050.0100.0150.0200.025cdfzzb	O
8.3.	O
wait	O
times	O
83	O
different	O
distribution	B
.	O
the	O
average	O
time	O
between	O
trains	O
,	O
as	O
seen	O
by	O
a	O
ran-	O
dom	O
passenger	O
,	O
is	O
substantially	O
higher	O
than	O
the	O
true	O
average	O
.	O
why	O
?	O
because	O
a	O
passenger	O
is	O
more	O
like	O
to	O
arrive	O
during	O
a	O
large	O
interval	O
than	O
a	O
small	O
one	O
.	O
consider	O
a	O
simple	O
example	O
:	O
suppose	O
that	O
the	O
time	O
between	O
trains	O
is	O
either	O
5	O
minutes	O
or	O
10	O
minutes	O
with	O
equal	O
probability	B
.	O
in	O
that	O
case	O
the	O
average	O
time	O
between	O
trains	O
is	O
7.5	O
minutes	O
.	O
but	O
a	O
passenger	O
is	O
more	O
likely	O
to	O
arrive	O
during	O
a	O
10	O
minute	O
gap	O
than	O
a	O
5	O
minute	O
gap	O
;	O
in	O
fact	O
,	O
twice	O
as	O
likely	O
.	O
if	O
we	O
surveyed	O
arriving	O
passengers	O
,	O
we	O
would	O
ﬁnd	O
that	O
2/3	O
of	O
them	O
arrived	O
during	O
a	O
10	O
minute	O
gap	O
,	O
and	O
only	O
1/3	O
during	O
a	O
5	O
minute	O
gap	O
.	O
so	O
the	O
average	O
time	O
between	O
trains	O
,	O
as	O
seen	O
by	O
an	O
arriving	O
passenger	O
,	O
is	O
8.33	O
minutes	O
.	O
this	O
kind	O
of	O
observer	B
bias	I
appears	O
in	O
many	O
contexts	O
.	O
students	O
think	O
that	O
classes	O
are	O
bigger	O
than	O
they	O
are	O
because	O
more	O
of	O
them	O
are	O
in	O
the	O
big	O
classes	O
.	O
airline	O
passengers	O
think	O
that	O
planes	O
are	O
fuller	O
than	O
they	O
are	O
because	O
more	O
of	O
them	O
are	O
on	O
full	O
ﬂights	O
.	O
in	O
each	O
case	O
,	O
values	O
from	O
the	O
actual	O
distribution	B
are	O
oversampled	O
in	O
propor-	O
tion	O
to	O
their	O
value	O
.	O
in	O
the	O
red	O
line	O
example	O
,	O
a	O
gap	O
that	O
is	O
twice	O
as	O
big	O
is	O
twice	O
as	O
likely	O
to	O
be	O
observed	O
.	O
so	O
given	O
the	O
actual	O
distribution	B
of	O
gaps	O
,	O
we	O
can	O
compute	O
the	O
distribution	B
of	O
gaps	O
as	O
seen	O
by	O
passengers	O
.	O
biaspmf	O
does	O
this	O
computation	O
:	O
def	O
biaspmf	O
(	O
pmf	O
)	O
:	O
new_pmf	O
=	O
pmf.copy	O
(	O
)	O
for	O
x	O
,	O
p	O
in	O
pmf.items	O
(	O
)	O
:	O
new_pmf.mult	O
(	O
x	O
,	O
x	O
)	O
new_pmf.normalize	O
(	O
)	O
return	O
new_pmf	O
pmf	O
is	O
the	O
actual	O
distribution	B
;	O
new_pmf	O
is	O
the	O
biased	O
distribution	O
.	O
inside	O
the	O
loop	O
,	O
we	O
multiply	O
the	O
probability	B
of	O
each	O
value	O
,	O
x	O
,	O
by	O
the	O
likelihood	B
it	O
will	O
be	O
observed	O
,	O
which	O
is	O
proportional	O
to	O
x.	O
then	O
we	O
normalize	B
the	O
result	O
.	O
figure	O
8.1	O
shows	O
the	O
actual	O
distribution	B
of	O
gaps	O
,	O
labeled	O
z	O
,	O
and	O
the	O
distribu-	O
tion	O
of	O
gaps	O
seen	O
by	O
passengers	O
,	O
labeled	O
zb	O
for	O
“	O
z	O
biased	O
”	O
.	O
84	O
chapter	O
8.	O
observer	B
bias	I
figure	O
8.2	O
:	O
cdf	O
of	O
z	O
,	O
zb	O
,	O
and	O
the	O
wait	O
time	O
seen	O
by	O
passengers	O
,	O
y	O
.	O
8.3	O
wait	O
times	O
wait	O
time	O
,	O
which	O
i	O
call	O
y	O
,	O
is	O
the	O
time	O
between	O
the	O
arrival	O
of	O
a	O
passenger	O
and	O
the	O
next	O
arrival	O
of	O
a	O
train	O
.	O
elapsed	O
time	O
,	O
which	O
i	O
call	O
x	O
,	O
is	O
the	O
time	O
between	O
the	O
arrival	O
of	O
the	O
previous	O
train	O
and	O
the	O
arrival	O
of	O
a	O
passenger	O
.	O
i	O
chose	O
these	O
deﬁnitions	O
so	O
that	O
zb	O
=	O
x	O
+	O
y.	O
given	O
the	O
distribution	B
of	O
zb	O
,	O
we	O
can	O
compute	O
the	O
distribution	B
of	O
y.	O
i	O
’	O
ll	O
start	O
with	O
a	O
simple	O
case	O
and	O
then	O
generalize	O
.	O
suppose	O
,	O
as	O
in	O
the	O
previous	O
ex-	O
ample	O
,	O
that	O
zb	O
is	O
either	O
5	O
minutes	O
with	O
probability	B
1/3	O
,	O
or	O
10	O
minutes	O
with	O
probability	B
2/3	O
.	O
if	O
we	O
arrive	O
at	O
a	O
random	O
time	O
during	O
a	O
5	O
minute	O
gap	O
,	O
y	O
is	O
uniform	O
from	O
0	O
to	O
5	O
minutes	O
.	O
if	O
we	O
arrive	O
during	O
a	O
10	O
minute	O
gap	O
,	O
y	O
is	O
uniform	O
from	O
0	O
to	O
10.	O
so	O
the	O
overall	O
distribution	B
is	O
a	O
mixture	B
of	O
uniform	O
distributions	O
weighted	O
according	O
to	O
the	O
probability	B
of	O
each	O
gap	O
.	O
the	O
following	O
function	O
takes	O
the	O
distribution	B
of	O
zb	O
and	O
computes	O
the	O
distri-	O
bution	O
of	O
y	O
:	O
def	O
pmfofwaittime	O
(	O
pmf_zb	O
)	O
:	O
metapmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
gap	O
,	O
prob	O
in	O
pmf_zb.items	O
(	O
)	O
:	O
uniform	O
=	O
makeuniformpmf	O
(	O
0	O
,	O
gap	O
)	O
metapmf.set	O
(	O
uniform	O
,	O
prob	O
)	O
pmf_y	O
=	O
thinkbayes.makemixture	O
(	O
metapmf	O
)	O
return	O
pmf_y	O
05101520time	O
(	O
min	O
)	O
0.00.20.40.60.81.0cdfzzby	O
8.3.	O
wait	O
times	O
85	O
pmfofwaittime	O
makes	O
a	O
meta-pmf	O
that	O
maps	O
from	O
each	O
uniform	O
distribu-	O
tion	O
to	O
its	O
probability	B
.	O
then	O
it	O
uses	O
makemixture	O
,	O
which	O
we	O
saw	O
in	O
sec-	O
tion	O
5.6	O
,	O
to	O
compute	O
the	O
mixture	B
.	O
pmfofwaittime	O
also	O
uses	O
makeuniformpmf	O
,	O
deﬁned	O
here	O
:	O
def	O
makeuniformpmf	O
(	O
low	O
,	O
high	O
)	O
:	O
pmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
x	O
in	O
makerange	O
(	O
low=low	O
,	O
high=high	O
)	O
:	O
pmf.set	O
(	O
x	O
,	O
1	O
)	O
pmf.normalize	O
(	O
)	O
return	O
pmf	O
low	O
and	O
high	O
are	O
the	O
range	O
of	O
the	O
uniform	B
distribution	I
,	O
(	O
both	O
ends	O
in-	O
cluded	O
)	O
.	O
finally	O
,	O
makeuniformpmf	O
uses	O
makerange	O
,	O
deﬁned	O
here	O
:	O
def	O
makerange	O
(	O
low	O
,	O
high	O
,	O
skip=10	O
)	O
:	O
return	O
range	O
(	O
low	O
,	O
high+skip	O
,	O
skip	O
)	O
makerange	O
deﬁnes	O
a	O
set	O
of	O
possible	O
values	O
for	O
wait	O
time	O
(	O
expressed	O
in	O
sec-	O
onds	O
)	O
.	O
by	O
default	O
it	O
divides	O
the	O
range	O
into	O
10	O
second	O
intervals	O
.	O
to	O
encapsulate	O
the	O
process	B
of	O
computing	O
these	O
distributions	O
,	O
i	O
created	O
a	O
class	O
called	O
waittimecalculator	O
:	O
class	O
waittimecalculator	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
pmf_z	O
)	O
:	O
self.pmf_z	O
=	O
pmf_z	O
self.pmf_zb	O
=	O
biaspmf	O
(	O
pmf	O
)	O
self.pmf_y	O
=	O
self.pmfofwaittime	O
(	O
self.pmf_zb	O
)	O
self.pmf_x	O
=	O
self.pmf_y	O
the	O
parameter	B
,	O
pmf_z	O
,	O
is	O
the	O
unbiased	O
distribution	B
of	O
z.	O
pmf_zb	O
is	O
the	O
biased	O
distribution	O
of	O
gap	O
time	O
,	O
as	O
seen	O
by	O
passengers	O
.	O
pmf_y	O
is	O
the	O
distribution	B
of	O
wait	O
time	O
.	O
pmf_x	O
is	O
the	O
distribution	B
of	O
elapsed	O
time	O
,	O
which	O
is	O
the	O
same	O
as	O
the	O
distribution	B
of	O
wait	O
time	O
.	O
to	O
see	O
why	O
,	O
remem-	O
ber	O
that	O
for	O
a	O
particular	O
value	O
of	O
zp	O
,	O
the	O
distribution	B
of	O
y	O
is	O
uniform	O
from	O
0	O
to	O
zp	O
.	O
also	O
x	O
=	O
zp	O
-	O
y	O
so	O
the	O
distribution	B
of	O
x	O
is	O
also	O
uniform	O
from	O
0	O
to	O
zp	O
.	O
figure	O
8.2	O
shows	O
the	O
distribution	B
of	O
z	O
,	O
zb	O
,	O
and	O
y	O
based	O
on	O
the	O
data	O
i	O
collected	O
from	O
the	O
red	O
line	O
web	O
site	O
.	O
86	O
chapter	O
8.	O
observer	B
bias	I
figure	O
8.3	O
:	O
prior	B
and	O
posterior	B
of	O
x	O
and	O
predicted	O
y.	O
to	O
present	O
these	O
distributions	O
,	O
i	O
am	O
switching	O
from	O
pmfs	O
to	O
cdfs	O
.	O
most	O
people	O
are	O
more	O
familiar	O
with	O
pmfs	O
,	O
but	O
i	O
think	O
cdfs	O
are	O
easier	O
to	O
interpret	O
,	O
once	O
you	O
get	O
used	O
to	O
them	O
.	O
and	O
if	O
you	O
want	O
to	O
plot	O
several	O
distributions	O
on	O
the	O
same	O
axes	O
,	O
cdfs	O
are	O
the	O
way	O
to	O
go	O
.	O
the	O
mean	O
of	O
z	O
is	O
7.8	O
minutes	O
.	O
the	O
mean	O
of	O
zb	O
is	O
8.8	O
minutes	O
,	O
about	O
13	O
%	O
higher	O
.	O
the	O
mean	O
of	O
y	O
is	O
4.4	O
,	O
half	O
the	O
mean	O
of	O
zb	O
.	O
as	O
an	O
aside	O
,	O
the	O
red	O
line	O
schedule	O
reports	O
that	O
trains	O
run	O
every	O
9	O
minutes	O
during	O
peak	O
times	O
.	O
this	O
is	O
close	O
to	O
the	O
average	O
of	O
zb	O
,	O
but	O
higher	O
than	O
the	O
average	O
of	O
z.	O
i	O
exchanged	O
email	O
with	O
a	O
representative	O
of	O
the	O
mbta	O
,	O
who	O
conﬁrmed	O
that	O
the	O
reported	O
time	O
between	O
trains	O
is	O
deliberately	O
conservative	O
in	O
order	O
to	O
account	O
for	O
variability	O
.	O
8.4	O
predicting	O
wait	O
times	O
let	O
’	O
s	O
get	O
back	O
to	O
the	O
motivating	O
question	O
:	O
suppose	O
that	O
when	O
i	O
arrive	O
at	O
the	O
platform	O
i	O
see	O
10	O
people	O
waiting	O
.	O
how	O
long	O
should	O
i	O
expect	O
to	O
wait	O
until	O
the	O
next	O
train	O
arrives	O
?	O
as	O
always	O
,	O
let	O
’	O
s	O
start	O
with	O
the	O
easiest	O
version	O
of	O
the	O
problem	O
and	O
work	O
our	O
way	O
up	O
.	O
suppose	O
we	O
are	O
given	O
the	O
actual	O
distribution	B
of	O
z	O
,	O
and	O
we	O
know	O
that	O
the	O
passenger	O
arrival	B
rate	I
,	O
λ	O
,	O
is	O
2	O
passengers	O
per	O
minute	O
.	O
in	O
that	O
case	O
we	O
can	O
:	O
1.	O
use	O
the	O
distribution	B
of	O
z	O
to	O
compute	O
the	O
prior	B
distribution	I
of	O
zp	O
,	O
the	O
time	O
between	O
trains	O
as	O
seen	O
by	O
a	O
passenger	O
.	O
05101520time	O
(	O
min	O
)	O
0.00.20.40.60.81.0cdfprior	O
xposterior	O
xpred	O
y	O
8.4.	O
predicting	O
wait	O
times	O
87	O
2.	O
then	O
we	O
can	O
use	O
the	O
number	O
of	O
passengers	O
to	O
estimate	O
the	O
distribution	B
of	O
x	O
,	O
the	O
elapsed	O
time	O
since	O
the	O
last	O
train	O
.	O
3.	O
finally	O
,	O
we	O
use	O
the	O
relation	O
y	O
=	O
zp	O
-	O
x	O
to	O
get	O
the	O
distribution	B
of	O
y.	O
the	O
ﬁrst	O
step	O
is	O
to	O
create	O
a	O
waittimecalculator	O
that	O
encapsulates	O
the	O
distri-	O
butions	O
of	O
zp	O
,	O
x	O
,	O
and	O
y	O
,	O
prior	B
to	O
taking	O
into	O
account	O
the	O
number	O
of	O
passen-	O
gers	O
.	O
wtc	O
=	O
waittimecalculator	O
(	O
pmf_z	O
)	O
pmf_z	O
is	O
the	O
given	O
distribution	B
of	O
gap	O
times	O
.	O
the	O
next	O
step	O
is	O
to	O
make	O
an	O
elapsedtimeestimator	O
(	O
deﬁned	O
below	O
)	O
,	O
which	O
encapsulates	O
the	O
posterior	B
distribution	I
of	O
x	O
and	O
the	O
predictive	B
distribution	I
of	O
y.	O
ete	O
=	O
elapsedtimeestimator	O
(	O
wtc	O
,	O
lam=2.0/60	O
,	O
num_passengers=15	O
)	O
the	O
parameters	O
are	O
the	O
waittimecalculator	O
,	O
the	O
passenger	O
arrival	B
rate	I
,	O
lam	O
(	O
expressed	O
in	O
passengers	O
per	O
second	O
)	O
,	O
and	O
the	O
observed	O
number	O
of	O
passen-	O
gers	O
,	O
let	O
’	O
s	O
say	O
15.	O
here	O
is	O
the	O
deﬁnition	O
of	O
elapsedtimeestimator	O
:	O
class	O
elapsedtimeestimator	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
wtc	O
,	O
lam	O
,	O
num_passengers	O
)	O
:	O
self.prior_x	O
=	O
elapsed	O
(	O
wtc.pmf_x	O
)	O
self.post_x	O
=	O
self.prior_x.copy	O
(	O
)	O
self.post_x.update	O
(	O
(	O
lam	O
,	O
num_passengers	O
)	O
)	O
self.pmf_y	O
=	O
predictwaittime	O
(	O
wtc.pmf_zb	O
,	O
self.post_x	O
)	O
prior_x	O
and	O
posterior_x	O
are	O
the	O
prior	B
and	O
posterior	B
distributions	O
of	O
elapsed	O
time	O
.	O
pmf_y	O
is	O
the	O
predictive	B
distribution	I
of	O
wait	O
time	O
.	O
elapsedtimeestimator	O
uses	O
elapsed	O
and	O
predictwaittime	O
,	O
deﬁned	O
below	O
.	O
elapsed	O
is	O
a	O
suite	B
that	O
represents	O
the	O
hypothetical	O
distribution	B
of	O
x.	O
the	O
prior	B
distribution	I
of	O
x	O
comes	O
straight	O
from	O
the	O
waittimecalculator	O
.	O
then	O
we	O
use	O
the	O
data	O
,	O
which	O
consists	O
of	O
the	O
arrival	B
rate	I
,	O
lam	O
,	O
and	O
the	O
number	O
of	O
passengers	O
on	O
the	O
platform	O
,	O
to	O
compute	O
the	O
posterior	B
distribution	I
.	O
here	O
’	O
s	O
the	O
deﬁnition	O
of	O
elapsed	O
:	O
88	O
chapter	O
8.	O
observer	B
bias	I
class	O
elapsed	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
hypo	O
lam	O
,	O
k	O
=	O
data	O
like	O
=	O
thinkbayes.evalpoissonpmf	O
(	O
k	O
,	O
lam	O
*	O
x	O
)	O
return	O
like	O
as	O
always	O
,	O
likelihood	B
takes	O
a	O
hypothesis	O
and	O
data	O
,	O
and	O
computes	O
the	O
like-	O
lihood	O
of	O
the	O
data	O
under	O
the	O
hypothesis	O
.	O
in	O
this	O
case	O
hypo	O
is	O
the	O
elapsed	O
time	O
since	O
the	O
last	O
train	O
and	O
data	O
is	O
a	O
tuple	B
of	O
lam	O
and	O
the	O
number	O
of	O
passengers	O
.	O
the	O
likelihood	B
of	O
the	O
data	O
is	O
the	O
probability	B
of	O
getting	O
k	O
arrivals	O
in	O
x	O
time	O
,	O
given	O
arrival	B
rate	I
lam	O
.	O
we	O
compute	O
that	O
using	O
the	O
pmf	O
of	O
the	O
poisson	O
dis-	O
tribution	O
.	O
finally	O
,	O
here	O
’	O
s	O
the	O
deﬁnition	O
of	O
predictwaittime	O
:	O
def	O
predictwaittime	O
(	O
pmf_zb	O
,	O
pmf_x	O
)	O
:	O
pmf_y	O
=	O
pmf_zb	O
-	O
pmf_x	O
removenegatives	O
(	O
pmf_y	O
)	O
return	O
pmf_y	O
pmf_zb	O
is	O
the	O
distribution	B
of	O
gaps	O
between	O
trains	O
;	O
pmf_x	O
is	O
the	O
distribution	B
of	O
elapsed	O
time	O
,	O
based	O
on	O
the	O
observed	O
number	O
of	O
passengers	O
.	O
since	O
y	O
=	O
zb	O
-	O
x	O
,	O
we	O
can	O
compute	O
pmf_y	O
=	O
pmf_zb	O
-	O
pmf_x	O
the	O
subtraction	O
operator	O
invokes	O
pmf.__sub__	O
,	O
which	O
enumerates	O
all	O
pairs	O
of	O
zb	O
and	O
x	O
,	O
computes	O
the	O
differences	O
,	O
and	O
adds	O
the	O
results	O
to	O
pmf_y	O
.	O
the	O
resulting	O
pmf	O
includes	O
some	O
negative	O
values	O
,	O
which	O
we	O
know	O
are	O
im-	O
possible	O
.	O
for	O
example	O
,	O
if	O
you	O
arrive	O
during	O
a	O
gap	O
of	O
5	O
minutes	O
,	O
you	O
can	O
’	O
t	O
wait	O
more	O
than	O
5	O
minutes	O
.	O
removenegatives	O
removes	O
the	O
impossible	O
val-	O
ues	O
from	O
the	O
distribution	B
and	O
renormalizes	O
.	O
def	O
removenegatives	O
(	O
pmf	O
)	O
:	O
for	O
val	O
in	O
pmf.values	O
(	O
)	O
:	O
if	O
val	O
<	O
0	O
:	O
pmf.remove	O
(	O
val	O
)	O
pmf.normalize	O
(	O
)	O
figure	O
8.3	O
shows	O
the	O
results	O
.	O
the	O
prior	B
distribution	I
of	O
x	O
is	O
the	O
same	O
as	O
the	O
distribution	B
of	O
y	O
in	O
figure	O
8.2.	O
the	O
posterior	B
distribution	I
of	O
x	O
shows	O
that	O
,	O
after	O
seeing	O
15	O
passengers	O
on	O
the	O
platform	O
,	O
we	O
believe	O
that	O
the	O
time	O
since	O
8.5.	O
estimating	O
the	O
arrival	B
rate	I
89	O
figure	O
8.4	O
:	O
prior	B
and	O
posterior	B
distributions	O
of	O
lam	O
based	O
on	O
ﬁve	O
days	O
of	O
passenger	O
data	O
.	O
the	O
last	O
train	O
is	O
probably	O
5-10	O
minutes	O
.	O
the	O
predictive	B
distribution	I
of	O
y	O
in-	O
dicates	O
that	O
we	O
expect	O
the	O
next	O
train	O
in	O
less	O
than	O
5	O
minutes	O
,	O
with	O
about	O
80	O
%	O
conﬁdence	O
.	O
8.5	O
estimating	O
the	O
arrival	B
rate	I
the	O
analysis	O
so	O
far	O
has	O
been	O
based	O
on	O
the	O
assumption	O
that	O
we	O
know	O
(	O
1	O
)	O
the	O
distribution	B
of	O
gaps	O
and	O
(	O
2	O
)	O
the	O
passenger	O
arrival	B
rate	I
.	O
now	O
we	O
are	O
ready	O
to	O
relax	O
the	O
second	O
assumption	O
.	O
suppose	O
that	O
you	O
just	O
moved	O
to	O
boston	O
,	O
so	O
you	O
don	O
’	O
t	O
know	O
much	O
about	O
the	O
passenger	O
arrival	B
rate	I
on	O
the	O
red	O
line	O
.	O
after	O
a	O
few	O
days	O
of	O
commuting	O
,	O
you	O
could	O
make	O
a	O
guess	O
,	O
at	O
least	O
qualitatively	O
.	O
with	O
a	O
little	O
more	O
effort	O
,	O
you	O
could	O
estimate	O
λ	O
quantitatively	O
.	O
each	O
day	O
when	O
you	O
arrive	O
at	O
the	O
platform	O
,	O
you	O
should	O
note	O
the	O
time	O
and	O
the	O
number	O
of	O
passengers	O
waiting	O
(	O
if	O
the	O
platform	O
is	O
too	O
big	O
,	O
you	O
could	O
choose	O
a	O
sample	O
area	O
)	O
.	O
then	O
you	O
should	O
record	O
your	O
wait	O
time	O
and	O
the	O
number	O
of	O
new	O
arrivals	O
while	O
you	O
are	O
waiting	O
.	O
after	O
ﬁve	O
days	O
,	O
you	O
might	O
have	O
data	O
like	O
this	O
:	O
k1	O
--	O
17	O
y	O
--	O
-	O
4.6	O
k2	O
--	O
9	O
012345arrival	O
rate	O
(	O
passengers	O
/	O
min	O
)	O
0.00.20.40.60.81.0cdfpriorposterior	O
90	O
22	O
23	O
18	O
4	O
1.0	O
1.4	O
5.4	O
5.8	O
0	O
4	O
12	O
11	O
chapter	O
8.	O
observer	B
bias	I
where	O
k1	O
is	O
the	O
number	O
of	O
passengers	O
waiting	O
when	O
you	O
arrive	O
,	O
y	O
is	O
your	O
wait	O
time	O
in	O
minutes	O
,	O
and	O
k2	O
is	O
the	O
number	O
of	O
passengers	O
who	O
arrive	O
while	O
you	O
are	O
waiting	O
.	O
over	O
the	O
course	O
of	O
one	O
week	O
,	O
you	O
waited	O
18	O
minutes	O
and	O
saw	O
36	O
passen-	O
gers	O
arrive	O
,	O
so	O
you	O
would	O
estimate	O
that	O
the	O
arrival	B
rate	I
is	O
2	O
passengers	O
per	O
minute	O
.	O
for	O
practical	O
purposes	O
that	O
estimate	O
is	O
good	O
enough	O
,	O
but	O
for	O
the	O
sake	O
of	O
completeness	O
i	O
will	O
compute	O
a	O
posterior	B
distribution	I
for	O
λ	O
and	O
show	O
how	O
to	O
use	O
that	O
distribution	B
in	O
the	O
rest	O
of	O
the	O
analysis	O
.	O
arrivalrate	O
is	O
a	O
suite	B
that	O
represents	O
hypotheses	O
about	O
λ.	O
as	O
always	O
,	O
likelihood	B
takes	O
a	O
hypothesis	O
and	O
data	O
,	O
and	O
computes	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
the	O
hypothesis	O
.	O
in	O
this	O
case	O
the	O
hypothesis	O
is	O
a	O
value	O
of	O
λ.	O
the	O
data	O
is	O
a	O
pair	O
,	O
y	O
,	O
k	O
,	O
where	O
y	O
is	O
a	O
wait	O
time	O
and	O
k	O
is	O
the	O
number	O
of	O
passengers	O
that	O
arrived	O
.	O
class	O
arrivalrate	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
lam	O
=	O
hypo	O
y	O
,	O
k	O
=	O
data	O
like	O
=	O
thinkbayes.evalpoissonpmf	O
(	O
k	O
,	O
lam	O
*	O
y	O
)	O
return	O
like	O
look	O
familiar	O
;	O
to	O
identical	O
this	O
likelihood	B
might	O
in	O
that	O
elapsed.likelihood	O
in	O
section	O
8.4.	O
in	O
elapsed	O
time	O
;	O
elapsed.likelihood	O
the	O
hypothesis	O
arrivalrate.likelihood	O
the	O
hypothesis	O
is	O
lam	O
,	O
the	O
arrival	B
rate	I
.	O
but	O
in	O
both	O
cases	O
the	O
likelihood	B
is	O
the	O
probability	B
of	O
seeing	O
k	O
arrivals	O
in	O
some	O
period	O
of	O
time	O
,	O
given	O
lam	O
.	O
it	O
the	O
difference	O
is	O
is	O
almost	O
is	O
x	O
,	O
the	O
arrivalrateestimator	O
encapsulates	O
the	O
process	B
of	O
estimating	O
λ.	O
the	O
pa-	O
rameter	O
,	O
passenger_data	O
,	O
is	O
a	O
list	O
of	O
k1	O
,	O
y	O
,	O
k2	O
tuples	O
,	O
as	O
in	O
the	O
table	O
above	O
.	O
class	O
arrivalrateestimator	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
passenger_data	O
)	O
:	O
low	O
,	O
high	O
=	O
0	O
,	O
5	O
n	O
=	O
51	O
8.6.	O
incorporating	O
uncertainty	B
91	O
figure	O
8.5	O
:	O
predictive	O
distributions	O
of	O
y	O
for	O
possible	O
values	O
of	O
lam	O
.	O
hypos	O
=	O
numpy.linspace	O
(	O
low	O
,	O
high	O
,	O
n	O
)	O
/	O
60	O
self.prior_lam	O
=	O
arrivalrate	O
(	O
hypos	O
)	O
self.post_lam	O
=	O
self.prior_lam.copy	O
(	O
)	O
for	O
k1	O
,	O
y	O
,	O
k2	O
in	O
passenger_data	O
:	O
self.post_lam.update	O
(	O
(	O
y	O
,	O
k2	O
)	O
)	O
__init__	O
builds	O
hypos	O
,	O
which	O
is	O
a	O
sequence	O
of	O
hypothetical	O
values	O
for	O
lam	O
,	O
then	O
builds	O
the	O
prior	B
distribution	I
,	O
prior_lam	O
.	O
the	O
for	O
loop	O
updates	O
the	O
prior	B
with	O
data	O
,	O
yielding	O
the	O
posterior	B
distribution	I
,	O
post_lam	O
.	O
figure	O
8.4	O
shows	O
the	O
prior	B
and	O
posterior	B
distributions	O
.	O
as	O
expected	O
,	O
the	O
mean	O
and	O
median	B
of	O
the	O
posterior	B
are	O
near	O
the	O
observed	O
rate	O
,	O
2	O
passengers	O
per	O
minute	O
.	O
but	O
the	O
spread	O
of	O
the	O
posterior	B
distribution	I
captures	O
our	O
uncer-	O
tainty	O
about	O
λ	O
based	O
on	O
a	O
small	O
sample	O
.	O
8.6	O
incorporating	O
uncertainty	B
whenever	O
there	O
is	O
uncertainty	B
about	O
one	O
of	O
the	O
inputs	O
to	O
an	O
analysis	O
,	O
we	O
can	O
take	O
it	O
into	O
account	O
by	O
a	O
process	B
like	O
this	O
:	O
1.	O
implement	O
the	O
analysis	O
based	O
on	O
a	O
deterministic	O
value	O
of	O
the	O
uncertain	O
parameter	B
(	O
in	O
this	O
case	O
λ	O
)	O
.	O
2.	O
compute	O
the	O
distribution	B
of	O
the	O
uncertain	O
parameter	B
.	O
0246810wait	O
time	O
(	O
min	O
)	O
0.00.20.40.60.81.0cdfmix	O
92	O
chapter	O
8.	O
observer	B
bias	I
3.	O
run	O
the	O
analysis	O
for	O
each	O
value	O
of	O
the	O
parameter	B
,	O
and	O
generate	O
a	O
set	O
of	O
predictive	O
distributions	O
.	O
4.	O
compute	O
a	O
mixture	B
of	O
the	O
predictive	O
distributions	O
,	O
using	O
the	O
weights	O
from	O
the	O
distribution	B
of	O
the	O
parameter	B
.	O
we	O
have	O
already	O
done	O
steps	O
(	O
1	O
)	O
and	O
(	O
2	O
)	O
.	O
waitmixtureestimator	O
to	O
handle	O
steps	O
(	O
3	O
)	O
and	O
(	O
4	O
)	O
.	O
class	O
waitmixtureestimator	O
(	O
object	O
)	O
:	O
i	O
wrote	O
a	O
class	O
called	O
def	O
__init__	O
(	O
self	O
,	O
wtc	O
,	O
are	O
,	O
num_passengers=15	O
)	O
:	O
self.metapmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
lam	O
,	O
prob	O
in	O
sorted	O
(	O
are.post_lam.items	O
(	O
)	O
)	O
:	O
ete	O
=	O
elapsedtimeestimator	O
(	O
wtc	O
,	O
lam	O
,	O
num_passengers	O
)	O
self.metapmf.set	O
(	O
ete.pmf_y	O
,	O
prob	O
)	O
self.mixture	O
=	O
thinkbayes.makemixture	O
(	O
self.metapmf	O
)	O
wtc	O
is	O
the	O
waittimecalculator	O
that	O
contains	O
the	O
distribution	B
of	O
zb	O
.	O
are	O
is	O
the	O
arrivaltimeestimator	O
that	O
contains	O
the	O
distribution	B
of	O
lam	O
.	O
the	O
ﬁrst	O
line	O
makes	O
a	O
meta-pmf	O
that	O
maps	O
from	O
each	O
possible	O
distribution	B
of	O
y	O
to	O
its	O
probability	B
.	O
for	O
each	O
value	O
of	O
lam	O
,	O
we	O
use	O
elapsedtimeestimator	O
to	O
compute	O
the	O
corresponding	O
distribution	B
of	O
y	O
and	O
store	O
it	O
in	O
the	O
meta-pmf	O
.	O
then	O
we	O
use	O
makemixture	O
to	O
compute	O
the	O
mixture	B
.	O
figure	O
8.5	O
shows	O
the	O
results	O
.	O
the	O
shaded	O
lines	O
in	O
the	O
background	O
are	O
the	O
distributions	O
of	O
y	O
for	O
each	O
value	O
of	O
lam	O
,	O
with	O
line	O
thickness	O
that	O
represents	O
likelihood	B
.	O
the	O
dark	O
line	O
is	O
the	O
mixture	B
of	O
these	O
distributions	O
.	O
in	O
this	O
case	O
we	O
could	O
get	O
a	O
very	O
similar	O
result	O
using	O
a	O
single	O
point	O
estimate	O
of	O
lam	O
.	O
so	O
it	O
was	O
not	O
necessary	O
,	O
for	O
practical	O
purposes	O
,	O
to	O
include	O
the	O
uncer-	O
tainty	O
of	O
the	O
estimate	O
.	O
in	O
general	O
,	O
it	O
is	O
important	O
to	O
include	O
variability	O
if	O
the	O
system	O
response	O
is	O
non-linear	B
;	O
that	O
is	O
,	O
if	O
small	O
changes	O
in	O
the	O
input	O
can	O
cause	O
big	O
changes	O
in	O
the	O
output	O
.	O
in	O
this	O
case	O
,	O
posterior	B
variability	O
in	O
lam	O
is	O
small	O
and	O
the	O
system	O
response	O
is	O
approximately	O
linear	O
for	O
small	O
perturbations	O
.	O
8.7	O
decision	B
analysis	I
at	O
this	O
point	O
we	O
can	O
use	O
the	O
number	O
of	O
passengers	O
on	O
the	O
platform	O
to	O
pre-	O
dict	O
the	O
distribution	B
of	O
wait	O
times	O
.	O
now	O
let	O
’	O
s	O
get	O
to	O
the	O
second	O
part	O
of	O
the	O
8.7.	O
decision	B
analysis	I
93	O
figure	O
8.6	O
:	O
probability	B
that	O
wait	O
time	O
exceeds	O
15	O
minutes	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
passengers	O
on	O
the	O
platform	O
.	O
question	O
:	O
when	O
should	O
i	O
stop	O
waiting	O
for	O
the	O
train	O
and	O
go	O
catch	O
a	O
taxi	O
?	O
remember	O
that	O
in	O
the	O
original	O
scenario	O
,	O
i	O
am	O
trying	O
to	O
get	O
to	O
south	O
station	O
to	O
catch	O
the	O
commuter	O
rail	O
.	O
suppose	O
i	O
leave	O
the	O
ofﬁce	O
with	O
enough	O
time	O
that	O
i	O
can	O
wait	O
15	O
minutes	O
and	O
still	O
make	O
my	O
connection	O
at	O
south	O
station	O
.	O
in	O
that	O
case	O
i	O
would	O
like	O
to	O
know	O
the	O
probability	B
that	O
y	O
exceeds	O
15	O
minutes	O
as	O
a	O
function	O
of	O
num_passengers	O
.	O
it	O
is	O
easy	O
enough	O
to	O
use	O
the	O
analysis	O
from	O
section	O
8.4	O
and	O
run	O
it	O
for	O
a	O
range	O
of	O
num_passengers	O
.	O
but	O
there	O
’	O
s	O
a	O
problem	O
.	O
the	O
analysis	O
is	O
sensitive	O
to	O
the	O
frequency	O
of	O
long	O
de-	O
lays	O
,	O
and	O
because	O
long	O
delays	O
are	O
rare	O
,	O
it	O
is	O
hard	O
to	O
estimate	O
their	O
frequency	O
.	O
i	O
only	O
have	O
data	O
from	O
one	O
week	O
,	O
and	O
the	O
longest	O
delay	O
i	O
observed	O
was	O
15	O
minutes	O
.	O
so	O
i	O
can	O
’	O
t	O
estimate	O
the	O
frequency	O
of	O
longer	O
delays	O
accurately	O
.	O
however	O
,	O
i	O
can	O
use	O
previous	O
observations	O
to	O
make	O
at	O
least	O
a	O
coarse	O
estimate	O
.	O
when	O
i	O
commuted	O
by	O
red	O
line	O
for	O
a	O
year	O
,	O
i	O
saw	O
three	O
long	O
delays	O
caused	O
by	O
a	O
signaling	O
problem	O
,	O
a	O
power	O
outage	O
,	O
and	O
“	O
police	O
activity	O
”	O
at	O
another	O
stop	O
.	O
so	O
i	O
estimate	O
that	O
there	O
are	O
about	O
3	O
major	O
delays	O
per	O
year	O
.	O
but	O
remember	O
that	O
my	O
observations	O
are	O
biased	O
.	O
i	O
am	O
more	O
likely	O
to	O
observe	O
long	O
delays	O
because	O
they	O
affect	O
a	O
large	O
number	O
of	O
passengers	O
.	O
so	O
we	O
should	O
treat	O
my	O
observations	O
as	O
a	O
sample	O
of	O
zb	O
rather	O
than	O
z.	O
here	O
’	O
s	O
how	O
we	O
can	O
do	O
that	O
.	O
during	O
my	O
year	O
of	O
commuting	O
,	O
i	O
took	O
the	O
red	O
line	O
home	O
about	O
220	O
times	O
.	O
so	O
i	O
take	O
the	O
observed	O
gap	O
times	O
,	O
gap_times	O
,	O
generate	O
a	O
sample	O
of	O
220	O
gaps	O
,	O
and	O
compute	O
their	O
pmf	O
:	O
05101520253035num	O
passengers0.000.020.040.060.080.100.12p	O
(	O
y	O
>	O
15	O
min	O
)	O
94	O
chapter	O
8.	O
observer	B
bias	I
n	O
=	O
220	O
cdf_z	O
=	O
thinkbayes.makecdffromlist	O
(	O
gap_times	O
)	O
sample_z	O
=	O
cdf_z.sample	O
(	O
n	O
)	O
pmf_z	O
=	O
thinkbayes.makepmffromlist	O
(	O
sample_z	O
)	O
next	O
i	O
bias	O
pmf_z	O
to	O
get	O
the	O
distribution	B
of	O
zb	O
,	O
draw	O
a	O
sample	O
,	O
and	O
then	O
add	O
in	O
delays	O
of	O
30	O
,	O
40	O
,	O
and	O
50	O
minutes	O
(	O
expressed	O
in	O
seconds	O
)	O
:	O
cdf_zp	O
=	O
biaspmf	O
(	O
pmf_z	O
)	O
.makecdf	O
(	O
)	O
sample_zb	O
=	O
cdf_zp.sample	O
(	O
n	O
)	O
+	O
[	O
1800	O
,	O
2400	O
,	O
3000	O
]	O
cdf.sample	O
is	O
more	O
efﬁcient	O
than	O
pmf.sample	O
,	O
so	O
it	O
is	O
usually	O
faster	O
to	O
con-	O
vert	O
a	O
pmf	O
to	O
a	O
cdf	O
before	O
sampling	O
.	O
next	O
i	O
use	O
the	O
sample	O
of	O
zb	O
to	O
estimate	O
a	O
pdf	O
using	O
kde	O
,	O
and	O
then	O
convert	O
the	O
pdf	O
to	O
a	O
pmf	O
:	O
pdf_zb	O
=	O
thinkbayes.estimatedpdf	O
(	O
sample_zb	O
)	O
xs	O
=	O
makerange	O
(	O
low=60	O
)	O
pmf_zb	O
=	O
pdf_zb.makepmf	O
(	O
xs	O
)	O
finally	O
i	O
unbias	O
the	O
distribution	B
of	O
zb	O
to	O
get	O
the	O
distribution	B
of	O
z	O
,	O
which	O
i	O
use	O
to	O
create	O
the	O
waittimecalculator	O
:	O
pmf_z	O
=	O
unbiaspmf	O
(	O
pmf_zb	O
)	O
wtc	O
=	O
waittimecalculator	O
(	O
pmf_z	O
)	O
this	O
process	B
is	O
complicated	O
,	O
but	O
all	O
of	O
the	O
steps	O
are	O
operations	B
we	O
have	O
seen	O
before	O
.	O
now	O
we	O
are	O
ready	O
to	O
compute	O
the	O
probability	B
of	O
a	O
long	O
wait	O
.	O
def	O
problongwait	O
(	O
num_passengers	O
,	O
minutes	O
)	O
:	O
ete	O
=	O
elapsedtimeestimator	O
(	O
wtc	O
,	O
lam	O
,	O
num_passengers	O
)	O
cdf_y	O
=	O
ete.pmf_y.makecdf	O
(	O
)	O
prob	O
=	O
1	O
-	O
cdf_y.prob	O
(	O
minutes	O
*	O
60	O
)	O
given	O
the	O
number	O
of	O
passengers	O
on	O
the	O
platform	O
,	O
problongwait	O
makes	O
an	O
elapsedtimeestimator	O
,	O
extracts	O
the	O
distribution	B
of	O
wait	O
time	O
,	O
and	O
com-	O
putes	O
the	O
probability	B
that	O
wait	O
time	O
exceeds	O
minutes	O
.	O
figure	O
8.6	O
shows	O
the	O
result	O
.	O
when	O
the	O
number	O
of	O
passengers	O
is	O
less	O
than	O
20	O
,	O
we	O
infer	O
that	O
the	O
system	O
is	O
operating	O
normally	O
,	O
so	O
the	O
probability	B
of	O
a	O
long	O
delay	O
is	O
small	O
.	O
if	O
there	O
are	O
30	O
passengers	O
,	O
we	O
estimate	O
that	O
it	O
has	O
been	O
15	O
minutes	O
since	O
the	O
last	O
train	O
;	O
that	O
’	O
s	O
longer	O
than	O
a	O
normal	O
delay	O
,	O
so	O
we	O
infer	O
that	O
something	O
is	O
wrong	O
and	O
expect	O
longer	O
delays	O
.	O
if	O
we	O
are	O
willing	O
to	O
accept	O
a	O
10	O
%	O
chance	O
of	O
missing	O
the	O
connection	O
at	O
south	O
station	O
,	O
we	O
should	O
stay	O
and	O
wait	O
as	O
long	O
as	O
there	O
are	O
fewer	O
than	O
30	O
passen-	O
gers	O
,	O
and	O
take	O
a	O
taxi	O
if	O
there	O
are	O
more	O
.	O
8.8.	O
discussion	O
95	O
or	O
,	O
to	O
take	O
this	O
analysis	O
one	O
step	O
further	O
,	O
we	O
could	O
quantify	O
the	O
cost	O
of	O
miss-	O
ing	O
the	O
connection	O
and	O
the	O
cost	O
of	O
taking	O
a	O
taxi	O
,	O
then	O
choose	O
the	O
threshold	O
that	O
minimizes	O
expected	O
cost	O
.	O
8.8	O
discussion	O
the	O
analysis	O
so	O
far	O
has	O
been	O
based	O
on	O
the	O
assumption	O
that	O
the	O
arrival	B
rate	I
of	O
passengers	O
is	O
the	O
same	O
every	O
day	O
.	O
for	O
a	O
commuter	O
train	O
during	O
rush	O
hour	O
,	O
that	O
might	O
not	O
be	O
a	O
bad	O
assumption	O
,	O
but	O
there	O
are	O
some	O
obvious	O
exceptions	O
.	O
for	O
example	O
,	O
if	O
there	O
is	O
a	O
special	O
event	O
nearby	O
,	O
a	O
large	O
number	O
of	O
people	O
might	O
arrive	O
at	O
the	O
same	O
time	O
.	O
in	O
that	O
case	O
,	O
the	O
estimate	O
of	O
lam	O
would	O
be	O
too	O
low	O
,	O
so	O
the	O
estimates	O
of	O
x	O
and	O
y	O
would	O
be	O
too	O
high	O
.	O
if	O
special	O
events	O
are	O
as	O
common	O
as	O
major	O
delays	O
,	O
it	O
would	O
be	O
important	O
to	O
include	O
them	O
in	O
the	O
model	O
.	O
we	O
could	O
do	O
that	O
by	O
extending	O
the	O
distribution	B
of	O
lam	O
to	O
include	O
occasional	O
large	O
values	O
.	O
we	O
started	O
with	O
the	O
assumption	O
that	O
we	O
know	O
distribution	B
of	O
z.	O
as	O
an	O
alter-	O
native	O
,	O
a	O
passenger	O
could	O
estimate	O
z	O
,	O
but	O
it	O
would	O
not	O
be	O
easy	O
.	O
as	O
a	O
passen-	O
ger	O
,	O
you	O
only	O
observe	O
only	O
your	O
own	O
wait	O
time	O
,	O
y.	O
unless	O
you	O
skip	O
the	O
ﬁrst	O
train	O
and	O
wait	O
for	O
the	O
second	O
,	O
you	O
don	O
’	O
t	O
observe	O
the	O
gap	O
between	O
trains	O
,	O
z.	O
however	O
,	O
we	O
could	O
make	O
some	O
inferences	O
about	O
zb	O
.	O
if	O
we	O
note	O
the	O
number	O
of	O
passengers	O
waiting	O
when	O
we	O
arrive	O
,	O
we	O
can	O
estimate	O
the	O
elapsed	O
time	O
since	O
the	O
last	O
train	O
,	O
x.	O
then	O
we	O
observe	O
y.	O
if	O
we	O
add	O
the	O
posterior	B
dis-	O
tribution	O
of	O
x	O
to	O
the	O
observed	O
y	O
,	O
we	O
get	O
a	O
distribution	B
that	O
represents	O
our	O
posterior	B
belief	O
about	O
the	O
observed	O
value	O
of	O
zb	O
.	O
we	O
can	O
use	O
this	O
distribution	B
to	O
update	O
our	O
beliefs	O
about	O
the	O
distribution	B
of	O
zb	O
.	O
finally	O
,	O
we	O
can	O
compute	O
the	O
inverse	O
of	O
biaspmf	O
to	O
get	O
from	O
the	O
distribu-	O
tion	O
of	O
zb	O
to	O
the	O
distribution	B
of	O
z.	O
i	O
leave	O
this	O
analysis	O
as	O
an	O
exercise	O
for	O
the	O
reader	O
.	O
one	O
suggestion	O
:	O
you	O
should	O
read	O
chapter	O
15	O
ﬁrst	O
.	O
you	O
can	O
ﬁnd	O
the	O
outline	O
of	O
a	O
solution	O
in	O
http	O
:	O
//thinkbayes.com/redline.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
8.9	O
exercises	O
exercise	O
8.1.	O
this	O
exercise	O
is	O
from	O
mackay	O
,	O
information	O
theory	O
,	O
inference	O
,	O
and	O
learning	O
algorithms	O
:	O
96	O
chapter	O
8.	O
observer	B
bias	I
unstable	O
particles	O
are	O
emitted	O
from	O
a	O
source	O
and	O
decay	O
at	O
a	O
distance	O
x	O
,	O
a	O
real	O
number	O
that	O
has	O
an	O
exponential	O
probability	O
distribution	B
with	O
[	O
pa-	O
rameter	O
]	O
λ.	O
decay	O
events	O
can	O
only	O
be	O
observed	O
if	O
they	O
occur	O
in	O
a	O
win-	O
dow	O
extending	O
from	O
x	O
=	O
1	O
cm	O
to	O
x	O
=	O
20	O
cm	O
.	O
n	O
decays	O
are	O
observed	O
at	O
locations	O
{	O
1.5	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
12	O
}	O
cm	O
.	O
what	O
is	O
the	O
posterior	B
distribution	I
of	O
λ	O
?	O
you	O
can	O
download	O
a	O
solution	O
to	O
this	O
exercise	O
from	O
http	O
:	O
//	O
thinkbayes	O
.	O
com/	O
decay	O
.	O
py	O
.	O
chapter	O
9	O
two	O
dimensions	O
9.1	O
paintball	O
paintball	O
is	O
a	O
sport	O
in	O
which	O
competing	O
teams	O
try	O
to	O
shoot	O
each	O
other	O
with	O
guns	O
that	O
ﬁre	O
paint-ﬁlled	O
pellets	O
that	O
break	O
on	O
impact	O
,	O
leaving	O
a	O
colorful	O
mark	O
on	O
the	O
target	O
.	O
it	O
is	O
usually	O
played	O
in	O
an	O
arena	O
decorated	O
with	O
barriers	O
and	O
other	O
objects	O
that	O
can	O
be	O
used	O
as	O
cover	O
.	O
suppose	O
you	O
are	O
playing	O
paintball	O
in	O
an	O
indoor	O
arena	O
30	O
feet	O
wide	O
and	O
50	O
feet	O
long	O
.	O
you	O
are	O
standing	O
near	O
one	O
of	O
the	O
30	O
foot	O
walls	O
,	O
and	O
you	O
suspect	O
that	O
one	O
of	O
your	O
opponents	O
has	O
taken	O
cover	O
nearby	O
.	O
along	O
the	O
wall	O
,	O
you	O
see	O
several	O
paint	O
spatters	O
,	O
all	O
the	O
same	O
color	O
,	O
that	O
you	O
think	O
your	O
opponent	O
ﬁred	O
recently	O
.	O
the	O
spatters	O
are	O
at	O
15	O
,	O
16	O
,	O
18	O
,	O
and	O
21	O
feet	O
,	O
measured	O
from	O
the	O
lower-left	O
corner	O
of	O
the	O
room	O
.	O
based	O
on	O
these	O
data	O
,	O
where	O
do	O
you	O
think	O
your	O
opponent	O
is	O
hiding	O
?	O
figure	O
9.1	O
shows	O
a	O
diagram	O
of	O
the	O
arena	O
.	O
using	O
the	O
lower-left	O
corner	O
of	O
the	O
room	O
as	O
the	O
origin	O
,	O
i	O
denote	O
the	O
unknown	O
location	O
of	O
the	O
shooter	O
with	O
coordinates	O
α	O
and	O
β	O
,	O
or	O
alpha	O
and	O
beta	O
.	O
the	O
location	O
of	O
a	O
spatter	O
is	O
labeled	O
x.	O
the	O
angle	O
the	O
opponent	O
shoots	O
at	O
is	O
θ	O
or	O
theta	O
.	O
the	O
paintball	O
problem	O
is	O
a	O
modiﬁed	O
version	O
of	O
the	O
lighthouse	O
problem	O
,	O
a	O
common	O
example	O
of	O
bayesian	O
analysis	O
.	O
my	O
notation	O
follows	O
the	O
presenta-	O
tion	O
of	O
the	O
problem	O
in	O
d.s	O
.	O
sivia	O
’	O
s	O
,	O
data	O
analysis	O
:	O
a	O
bayesian	O
tutorial	O
,	O
second	O
edition	O
(	O
oxford	O
,	O
2006	O
)	O
.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
paintball.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
98	O
chapter	O
9.	O
two	O
dimensions	O
figure	O
9.1	O
:	O
diagram	O
of	O
the	O
layout	O
for	O
the	O
paintball	O
problem	O
.	O
9.2	O
the	O
suite	B
to	O
get	O
started	O
,	O
we	O
need	O
a	O
suite	B
that	O
represents	O
a	O
set	O
of	O
hypotheses	O
about	O
the	O
location	O
of	O
the	O
opponent	O
.	O
each	O
hypothesis	O
is	O
a	O
pair	O
of	O
coordinates	O
:	O
(	O
alpha	O
,	O
beta	O
)	O
.	O
here	O
is	O
the	O
deﬁnition	O
of	O
the	O
paintball	O
suite	B
:	O
class	O
paintball	O
(	O
thinkbayes.suite	O
,	O
thinkbayes.joint	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
alphas	O
,	O
betas	O
,	O
locations	O
)	O
:	O
self.locations	O
=	O
locations	O
pairs	O
=	O
[	O
(	O
alpha	O
,	O
beta	O
)	O
for	O
alpha	O
in	O
alphas	O
for	O
beta	O
in	O
betas	O
]	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
pairs	O
)	O
paintball	O
inherits	O
from	O
suite	B
,	O
which	O
we	O
have	O
seen	O
before	O
,	O
and	O
joint	O
,	O
which	O
i	O
will	O
explain	O
soon	O
.	O
alphas	O
is	O
the	O
list	O
of	O
possible	O
values	O
for	O
alpha	O
;	O
betas	O
is	O
the	O
list	O
of	O
values	O
for	O
beta	O
.	O
pairs	O
is	O
a	O
list	O
of	O
all	O
(	O
alpha	O
,	O
beta	O
)	O
pairs	O
.	O
locations	O
is	O
a	O
list	O
of	O
possible	O
locations	O
along	O
the	O
wall	O
;	O
it	O
is	O
stored	O
for	O
use	O
in	O
likelihood	B
.	O
the	O
room	O
is	O
30	O
feet	O
wide	O
and	O
50	O
feet	O
long	O
,	O
so	O
here	O
’	O
s	O
the	O
code	O
that	O
creates	O
the	O
suite	B
:	O
alphas	O
=	O
range	O
(	O
0	O
,	O
31	O
)	O
αβθxshooterwall	O
9.3.	O
trigonometry	B
99	O
figure	O
9.2	O
:	O
posterior	B
cdfs	O
for	O
alpha	O
and	O
beta	O
,	O
given	O
the	O
data	O
.	O
betas	O
=	O
range	O
(	O
1	O
,	O
51	O
)	O
locations	O
=	O
range	O
(	O
0	O
,	O
31	O
)	O
suite	B
=	O
paintball	O
(	O
alphas	O
,	O
betas	O
,	O
locations	O
)	O
this	O
prior	B
distribution	I
assumes	O
that	O
all	O
locations	O
in	O
the	O
room	O
are	O
equally	O
likely	O
.	O
given	O
a	O
map	O
of	O
the	O
room	O
,	O
we	O
might	O
choose	O
a	O
more	O
detailed	O
prior	B
,	O
but	O
we	O
’	O
ll	O
start	O
simple	O
.	O
9.3	O
trigonometry	B
now	O
we	O
need	O
a	O
likelihood	B
function	I
,	O
which	O
means	O
we	O
have	O
to	O
ﬁgure	O
out	O
the	O
likelihood	B
of	O
hitting	O
any	O
spot	O
along	O
the	O
wall	O
,	O
given	O
the	O
location	O
of	O
the	O
opponent	O
.	O
as	O
a	O
simple	O
model	O
,	O
imagine	O
that	O
the	O
opponent	O
is	O
like	O
a	O
rotating	O
turret	O
,	O
equally	O
likely	O
to	O
shoot	O
in	O
any	O
direction	O
.	O
in	O
that	O
case	O
,	O
he	O
is	O
most	O
likely	O
to	O
hit	O
the	O
wall	O
at	O
location	O
alpha	O
,	O
and	O
less	O
likely	O
to	O
hit	O
the	O
wall	O
far	O
away	O
from	O
alpha	O
.	O
with	O
a	O
little	O
trigonometry	B
,	O
we	O
can	O
compute	O
the	O
probability	B
of	O
hitting	O
any	O
spot	O
along	O
the	O
wall	O
.	O
imagine	O
that	O
the	O
shooter	O
ﬁres	O
a	O
shot	O
at	O
angle	O
θ	O
;	O
the	O
pellet	O
would	O
hit	O
the	O
wall	O
at	O
location	O
x	O
,	O
where	O
x	O
−	O
α	O
=	O
β	O
tan	O
θ	O
01020304050distance0.00.20.40.60.81.0probalphabeta	O
100	O
chapter	O
9.	O
two	O
dimensions	O
figure	O
9.3	O
:	O
pmf	O
of	O
location	O
given	O
alpha=10	O
,	O
for	O
several	O
values	O
of	O
beta	O
.	O
solving	O
this	O
equation	O
for	O
θ	O
yields	O
θ	O
=	O
tan−1	O
(	O
cid:18	O
)	O
x	O
−	O
α	O
(	O
cid:19	O
)	O
β	O
so	O
given	O
a	O
location	O
on	O
the	O
wall	O
,	O
we	O
can	O
ﬁnd	O
θ.	O
taking	O
the	O
derivative	O
of	O
the	O
ﬁrst	O
equation	O
with	O
respect	O
to	O
θ	O
yields	O
dx	O
dθ	O
=	O
β	O
cos2	O
θ	O
this	O
derivative	O
is	O
what	O
i	O
’	O
ll	O
call	O
the	O
“	O
straﬁng	B
speed	I
”	O
,	O
which	O
is	O
the	O
speed	O
of	O
the	O
target	O
location	O
along	O
the	O
wall	O
as	O
θ	O
increases	O
.	O
the	O
probability	B
of	O
hitting	O
a	O
given	O
point	O
on	O
the	O
wall	O
is	O
inversely	O
related	O
to	O
straﬁng	B
speed	I
.	O
if	O
we	O
know	O
the	O
coordinates	O
of	O
the	O
shooter	O
and	O
a	O
location	O
along	O
the	O
wall	O
,	O
we	O
can	O
compute	O
straﬁng	B
speed	I
:	O
def	O
strafingspeed	O
(	O
alpha	O
,	O
beta	O
,	O
x	O
)	O
:	O
theta	O
=	O
math.atan2	O
(	O
x	O
-	O
alpha	O
,	O
beta	O
)	O
speed	O
=	O
beta	O
/	O
math.cos	O
(	O
theta	O
)	O
**2	O
return	O
speed	O
alpha	O
and	O
beta	O
are	O
the	O
coordinates	O
of	O
the	O
shooter	O
;	O
x	O
is	O
the	O
location	O
of	O
a	O
spatter	O
.	O
the	O
result	O
is	O
the	O
derivative	O
of	O
x	O
with	O
respect	O
to	O
theta	O
.	O
now	O
we	O
can	O
compute	O
a	O
pmf	O
that	O
represents	O
the	O
probability	B
of	O
hitting	O
any	O
lo-	O
cation	O
on	O
the	O
wall	O
.	O
makelocationpmf	O
takes	O
alpha	O
and	O
beta	O
,	O
the	O
coordinates	O
of	O
the	O
shooter	O
,	O
and	O
locations	O
,	O
a	O
list	O
of	O
possible	O
values	O
of	O
x	O
.	O
051015202530distance0.0100.0150.0200.0250.0300.0350.0400.0450.0500.055probbeta	O
=	O
10beta	O
=	O
20beta	O
=	O
40	O
9.4.	O
likelihood	B
101	O
def	O
makelocationpmf	O
(	O
alpha	O
,	O
beta	O
,	O
locations	O
)	O
:	O
pmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
x	O
in	O
locations	O
:	O
prob	O
=	O
1.0	O
/	O
strafingspeed	O
(	O
alpha	O
,	O
beta	O
,	O
x	O
)	O
pmf.set	O
(	O
x	O
,	O
prob	O
)	O
pmf.normalize	O
(	O
)	O
return	O
pmf	O
makelocationpmf	O
computes	O
the	O
probability	B
of	O
hitting	O
each	O
location	O
,	O
which	O
is	O
inversely	O
related	O
to	O
straﬁng	B
speed	I
.	O
the	O
result	O
is	O
a	O
pmf	O
of	O
locations	O
and	O
their	O
probabilities	O
.	O
figure	O
9.3	O
shows	O
the	O
pmf	O
of	O
location	O
with	O
alpha	O
=	O
10	O
and	O
a	O
range	O
of	O
values	O
for	O
beta	O
.	O
for	O
all	O
values	O
of	O
beta	O
the	O
most	O
likely	O
spatter	O
location	O
is	O
x	O
=	O
10	O
;	O
as	O
beta	O
increases	O
,	O
so	O
does	O
the	O
spread	O
of	O
the	O
pmf	O
.	O
9.4	O
likelihood	B
now	O
all	O
we	O
need	O
is	O
a	O
likelihood	B
function	I
.	O
we	O
can	O
use	O
makelocationpmf	O
to	O
compute	O
the	O
likelihood	B
of	O
any	O
value	O
of	O
x	O
,	O
given	O
the	O
coordinates	O
of	O
the	O
opponent	O
.	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
alpha	O
,	O
beta	O
=	O
hypo	O
x	O
=	O
data	O
pmf	O
=	O
makelocationpmf	O
(	O
alpha	O
,	O
beta	O
,	O
self.locations	O
)	O
like	O
=	O
pmf.prob	O
(	O
x	O
)	O
return	O
like	O
again	O
,	O
alpha	O
and	O
beta	O
are	O
the	O
hypothetical	O
coordinates	O
of	O
the	O
shooter	O
,	O
and	O
x	O
is	O
the	O
location	O
of	O
an	O
observed	O
spatter	O
.	O
pmf	O
contains	O
the	O
probability	B
of	O
each	O
location	O
,	O
given	O
the	O
coordinates	O
of	O
the	O
shooter	O
.	O
from	O
this	O
pmf	O
,	O
we	O
select	O
the	O
probability	B
of	O
the	O
observed	O
location	O
.	O
and	O
we	O
’	O
re	O
done	O
.	O
to	O
update	O
the	O
suite	B
,	O
we	O
can	O
use	O
updateset	O
,	O
which	O
is	O
inher-	O
ited	O
from	O
suite	B
.	O
suite.updateset	O
(	O
[	O
15	O
,	O
16	O
,	O
18	O
,	O
21	O
]	O
)	O
the	O
result	O
is	O
a	O
distribution	B
that	O
maps	O
each	O
(	O
alpha	O
,	O
beta	O
)	O
pair	O
to	O
a	O
posterior	B
probability	O
.	O
102	O
9.5	O
joint	O
distributions	O
chapter	O
9.	O
two	O
dimensions	O
when	O
each	O
value	O
in	O
a	O
distribution	B
is	O
a	O
tuple	B
of	O
variables	O
,	O
it	O
is	O
called	O
a	O
joint	B
distribution	I
because	O
it	O
represents	O
the	O
distributions	O
of	O
the	O
variables	O
together	O
,	O
that	O
is	O
“	O
jointly	O
”	O
.	O
a	O
joint	B
distribution	I
contains	O
the	O
distributions	O
of	O
the	O
vari-	O
ables	O
,	O
as	O
well	O
information	O
about	O
the	O
relationships	O
among	O
them	O
.	O
given	O
a	O
joint	B
distribution	I
,	O
we	O
can	O
compute	O
the	O
distributions	O
of	O
each	O
variable	O
independently	O
,	O
which	O
are	O
called	O
the	O
marginal	O
distributions	O
.	O
thinkbayes.joint	O
provides	O
a	O
method	O
that	O
computes	O
marginal	O
distribu-	O
tions	O
:	O
#	O
class	O
joint	O
:	O
def	O
marginal	O
(	O
self	O
,	O
i	O
)	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
vs	O
,	O
prob	O
in	O
self.items	O
(	O
)	O
:	O
pmf.incr	O
(	O
vs	O
[	O
i	O
]	O
,	O
prob	O
)	O
return	O
pmf	O
i	O
is	O
the	O
index	O
of	O
the	O
variable	O
we	O
want	O
;	O
in	O
this	O
example	O
i=0	O
indicates	O
the	O
distribution	B
of	O
alpha	O
,	O
and	O
i=1	O
indicates	O
the	O
distribution	B
of	O
beta	O
.	O
here	O
’	O
s	O
the	O
code	O
that	O
extracts	O
the	O
marginal	O
distributions	O
:	O
marginal_alpha	O
=	O
suite.marginal	O
(	O
0	O
)	O
marginal_beta	O
=	O
suite.marginal	O
(	O
1	O
)	O
figure	O
9.2	O
shows	O
the	O
results	O
(	O
converted	O
to	O
cdfs	O
)	O
.	O
the	O
median	B
value	O
for	O
alpha	O
is	O
18	O
,	O
near	O
the	O
center	O
of	O
mass	O
of	O
the	O
observed	O
spatters	O
.	O
for	O
beta	O
,	O
the	O
most	O
likely	O
values	O
are	O
close	O
to	O
the	O
wall	O
,	O
but	O
beyond	O
10	O
feet	O
the	O
distribution	B
is	O
almost	O
uniform	O
,	O
which	O
indicates	O
that	O
the	O
data	O
do	O
not	O
distinguish	O
strongly	O
between	O
these	O
possible	O
locations	O
.	O
given	O
the	O
posterior	B
marginals	O
,	O
we	O
can	O
compute	O
credible	O
intervals	O
for	O
each	O
coordinate	O
independently	O
:	O
print	O
'alpha	O
ci	O
'	O
,	O
marginal_alpha.credibleinterval	O
(	O
50	O
)	O
print	O
'beta	O
ci	O
'	O
,	O
marginal_beta.credibleinterval	O
(	O
50	O
)	O
the	O
50	O
%	O
credible	O
intervals	O
are	O
(	O
14	O
,	O
21	O
)	O
for	O
alpha	O
and	O
(	O
5	O
,	O
31	O
)	O
for	O
beta	O
.	O
so	O
the	O
data	O
provide	O
evidence	B
that	O
the	O
shooter	O
is	O
in	O
the	O
near	O
side	O
of	O
the	O
room	O
.	O
but	O
it	O
is	O
not	O
strong	O
evidence	B
.	O
the	O
90	O
%	O
credible	O
intervals	O
cover	O
most	O
of	O
the	O
room	O
!	O
9.6.	O
conditional	B
distributions	O
103	O
figure	O
9.4	O
:	O
posterior	B
distributions	O
for	O
alpha	O
conditioned	O
on	O
several	O
values	O
of	O
beta	O
.	O
9.6	O
conditional	B
distributions	O
the	O
marginal	O
distributions	O
contain	O
information	O
about	O
the	O
variables	O
inde-	O
pendently	O
,	O
but	O
they	O
do	O
not	O
capture	O
the	O
dependence	B
between	O
variables	O
,	O
if	O
any	O
.	O
one	O
way	O
to	O
visualize	O
dependence	B
is	O
by	O
computing	O
conditional	B
distribu-	O
tions	O
.	O
thinkbayes.joint	O
provides	O
a	O
method	O
that	O
does	O
that	O
:	O
def	O
conditional	B
(	O
self	O
,	O
i	O
,	O
j	O
,	O
val	O
)	O
:	O
pmf	O
=	O
pmf	O
(	O
)	O
for	O
vs	O
,	O
prob	O
in	O
self.items	O
(	O
)	O
:	O
if	O
vs	O
[	O
j	O
]	O
!	O
=	O
val	O
:	O
continue	O
pmf.incr	O
(	O
vs	O
[	O
i	O
]	O
,	O
prob	O
)	O
pmf.normalize	O
(	O
)	O
return	O
pmf	O
again	O
,	O
i	O
is	O
the	O
index	O
of	O
the	O
variable	O
we	O
want	O
;	O
j	O
is	O
the	O
index	O
of	O
the	O
condi-	O
tioning	O
variable	O
,	O
and	O
val	O
is	O
the	O
conditional	B
value	O
.	O
the	O
result	O
is	O
the	O
distribution	B
of	O
the	O
ith	O
variable	O
under	O
the	O
condition	O
that	O
the	O
jth	O
variable	O
is	O
val	O
.	O
for	O
example	O
,	O
the	O
following	O
code	O
computes	O
the	O
conditional	B
distributions	O
of	O
alpha	O
for	O
a	O
range	O
of	O
values	O
of	O
beta	O
:	O
betas	O
=	O
[	O
10	O
,	O
20	O
,	O
40	O
]	O
051015202530distance0.000.010.020.030.040.050.060.070.080.09probbeta	O
=	O
10beta	O
=	O
20beta	O
=	O
40	O
104	O
chapter	O
9.	O
two	O
dimensions	O
figure	O
9.5	O
:	O
credible	O
intervals	O
for	O
the	O
coordinates	O
of	O
the	O
opponent	O
.	O
for	O
beta	O
in	O
betas	O
:	O
cond	O
=	O
suite.conditional	O
(	O
0	O
,	O
1	O
,	O
beta	O
)	O
figure	O
9.4	O
shows	O
the	O
results	O
,	O
which	O
we	O
could	O
fully	O
describe	O
as	O
“	O
posterior	B
conditional	O
marginal	O
distributions.	O
”	O
whew	O
!	O
if	O
the	O
variables	O
were	O
independent	O
,	O
the	O
conditional	B
distributions	O
would	O
all	O
be	O
the	O
same	O
.	O
since	O
they	O
are	O
all	O
different	O
,	O
we	O
can	O
tell	O
the	O
variables	O
are	O
depen-	O
dent	O
.	O
for	O
example	O
,	O
if	O
we	O
know	O
(	O
somehow	O
)	O
that	O
beta	O
=	O
10	O
,	O
the	O
conditional	B
distribution	I
of	O
alpha	O
is	O
fairly	O
narrow	O
.	O
for	O
larger	O
values	O
of	O
beta	O
,	O
the	O
distri-	O
bution	O
of	O
alpha	O
is	O
wider	O
.	O
9.7	O
credible	O
intervals	O
another	O
way	O
to	O
visualize	O
the	O
posterior	B
joint	O
distribution	B
is	O
to	O
compute	O
cred-	O
ible	O
intervals	O
.	O
when	O
we	O
looked	O
at	O
credible	O
intervals	O
in	O
section	O
3.5	O
,	O
i	O
skipped	O
over	O
a	O
subtle	O
point	O
:	O
for	O
a	O
given	O
distribution	B
,	O
there	O
are	O
many	O
intervals	O
with	O
the	O
same	O
level	O
of	O
credibility	O
.	O
for	O
example	O
,	O
if	O
you	O
want	O
a	O
50	O
%	O
credible	O
inter-	O
val	O
,	O
you	O
could	O
choose	O
any	O
set	O
of	O
values	O
whose	O
probability	B
adds	O
up	O
to	O
50	O
%	O
.	O
when	O
the	O
values	O
are	O
one-dimensional	O
,	O
it	O
is	O
most	O
common	O
to	O
choose	O
the	O
cen-	O
tral	O
credible	B
interval	I
;	O
for	O
example	O
,	O
the	O
central	O
50	O
%	O
credible	B
interval	I
con-	O
tains	O
all	O
values	O
between	O
the	O
25th	O
and	O
75th	O
percentiles	O
.	O
in	O
multiple	O
dimensions	O
it	O
is	O
less	O
obvious	O
what	O
the	O
right	O
credible	B
interval	I
should	O
be	O
.	O
the	O
best	O
choice	O
might	O
depend	O
on	O
context	O
,	O
but	O
one	O
common	O
051015202530alpha01020304050beta255075	O
9.7.	O
credible	O
intervals	O
105	O
choice	O
is	O
the	O
maximum	B
likelihood	I
credible	O
interval	O
,	O
which	O
contains	O
the	O
most	O
likely	O
values	O
that	O
add	O
up	O
to	O
50	O
%	O
(	O
or	O
some	O
other	O
percentage	O
)	O
.	O
thinkbayes.joint	O
provides	O
a	O
method	O
that	O
computes	O
maximum	B
likelihood	I
credible	O
intervals	O
.	O
#	O
class	O
joint	O
:	O
def	O
maxlikeinterval	O
(	O
self	O
,	O
percentage=90	O
)	O
:	O
interval	O
=	O
[	O
]	O
total	O
=	O
0	O
t	O
=	O
[	O
(	O
prob	O
,	O
val	O
)	O
for	O
val	O
,	O
prob	O
in	O
self.items	O
(	O
)	O
]	O
t.sort	O
(	O
reverse=true	O
)	O
for	O
prob	O
,	O
val	O
in	O
t	O
:	O
interval.append	O
(	O
val	O
)	O
total	O
+=	O
prob	O
if	O
total	O
>	O
=	O
percentage/100.0	O
:	O
break	O
return	O
interval	O
the	O
ﬁrst	O
step	O
is	O
to	O
make	O
a	O
list	O
of	O
the	O
values	O
in	O
the	O
suite	B
,	O
sorted	O
in	O
descending	O
order	O
by	O
probability	B
.	O
next	O
we	O
traverse	O
the	O
list	O
,	O
adding	O
each	O
value	O
to	O
the	O
interval	O
,	O
until	O
the	O
total	B
probability	I
exceeds	O
percentage	O
.	O
the	O
result	O
is	O
a	O
list	O
of	O
values	O
from	O
the	O
suite	B
.	O
notice	O
that	O
this	O
set	O
of	O
values	O
is	O
not	O
necessarily	O
contiguous	O
.	O
to	O
visualize	O
the	O
intervals	O
,	O
i	O
wrote	O
a	O
function	O
that	O
“	O
colors	O
”	O
each	O
value	O
ac-	O
cording	O
to	O
how	O
many	O
intervals	O
it	O
appears	O
in	O
:	O
def	O
makecredibleplot	O
(	O
suite	B
)	O
:	O
d	O
=	O
dict	O
(	O
(	O
pair	O
,	O
0	O
)	O
for	O
pair	O
in	O
suite.values	O
(	O
)	O
)	O
percentages	O
=	O
[	O
75	O
,	O
50	O
,	O
25	O
]	O
for	O
p	O
in	O
percentages	O
:	O
interval	O
=	O
suite.maxlikeinterval	O
(	O
p	O
)	O
for	O
pair	O
in	O
interval	O
:	O
d	O
[	O
pair	O
]	O
+=	O
1	O
return	O
d	O
d	O
is	O
a	O
dictionary	O
that	O
maps	O
from	O
each	O
value	O
in	O
the	O
suite	B
to	O
the	O
number	O
of	O
intervals	O
it	O
appears	O
in	O
.	O
the	O
loop	O
computes	O
intervals	O
for	O
several	O
percentages	O
and	O
modiﬁes	O
d.	O
106	O
chapter	O
9.	O
two	O
dimensions	O
figure	O
9.5	O
shows	O
the	O
result	O
.	O
the	O
25	O
%	O
credible	B
interval	I
is	O
the	O
darkest	O
region	O
near	O
the	O
bottom	O
wall	O
.	O
for	O
higher	O
percentages	O
,	O
the	O
credible	B
interval	I
is	O
bigger	O
,	O
of	O
course	O
,	O
and	O
skewed	O
toward	O
the	O
right	O
side	O
of	O
the	O
room	O
.	O
9.8	O
discussion	O
this	O
chapter	O
shows	O
that	O
the	O
bayesian	O
framework	O
from	O
the	O
previous	O
chapters	O
can	O
be	O
extended	O
to	O
handle	O
a	O
two-dimensional	O
parameter	B
space	O
.	O
the	O
only	O
difference	O
is	O
that	O
each	O
hypothesis	O
is	O
represented	O
by	O
a	O
tuple	B
of	O
parameters	O
.	O
i	O
also	O
presented	O
joint	O
,	O
which	O
is	O
a	O
parent	O
class	O
that	O
provides	O
meth-	O
ods	O
that	O
apply	O
to	O
joint	O
distributions	O
:	O
marginal	O
,	O
conditional	B
,	O
and	O
makelikeinterval	O
.	O
in	O
object-oriented	O
terms	O
,	O
joint	O
is	O
a	O
mixin	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/mixin	O
)	O
.	O
there	O
is	O
a	O
lot	O
of	O
new	O
vocabulary	O
in	O
this	O
chapter	O
,	O
so	O
let	O
’	O
s	O
review	O
:	O
joint	B
distribution	I
:	O
a	O
distribution	B
that	O
represents	O
all	O
possible	O
values	O
in	O
a	O
multidimensional	O
space	O
and	O
their	O
probabilities	O
.	O
the	O
example	O
in	O
this	O
chapter	O
is	O
a	O
two-dimensional	O
space	O
made	O
up	O
of	O
the	O
coordinates	O
alpha	O
and	O
beta	O
.	O
the	O
joint	B
distribution	I
represents	O
the	O
probability	B
of	O
each	O
(	O
alpha	O
,	O
beta	O
)	O
pair	O
.	O
marginal	B
distribution	I
:	O
the	O
distribution	B
of	O
one	O
parameter	B
in	O
a	O
joint	O
distri-	O
bution	O
,	O
treating	O
the	O
other	O
parameters	O
as	O
unknown	O
.	O
for	O
example	O
,	O
fig-	O
ure	O
9.2	O
shows	O
the	O
distributions	O
of	O
alpha	O
and	O
beta	O
independently	O
.	O
conditional	B
distribution	I
:	O
the	O
distribution	B
of	O
one	O
parameter	B
in	O
a	O
joint	O
dis-	O
tribution	O
,	O
conditioned	O
on	O
one	O
or	O
more	O
of	O
the	O
other	O
parameters	O
.	O
fig-	O
ure	O
9.4	O
several	O
distributions	O
for	O
alpha	O
,	O
conditioned	O
on	O
different	O
values	O
of	O
beta	O
.	O
given	O
the	O
joint	B
distribution	I
,	O
you	O
can	O
compute	O
marginal	O
and	O
conditional	B
dis-	O
tributions	O
.	O
with	O
enough	O
conditional	B
distributions	O
,	O
you	O
could	O
re-create	O
the	O
joint	B
distribution	I
,	O
at	O
least	O
approximately	O
.	O
but	O
given	O
the	O
marginal	O
distribu-	O
tions	O
you	O
can	O
not	O
re-create	O
the	O
joint	B
distribution	I
because	O
you	O
have	O
lost	O
infor-	O
mation	O
about	O
the	O
dependence	B
between	O
variables	O
.	O
if	O
there	O
are	O
n	O
possible	O
values	O
for	O
each	O
of	O
two	O
parameters	O
,	O
most	O
operations	B
on	O
the	O
joint	B
distribution	I
take	O
time	O
proportional	O
to	O
n2	O
.	O
if	O
there	O
are	O
d	O
parameters	O
,	O
run	O
time	O
is	O
proportional	O
to	O
nd	O
,	O
which	O
quickly	O
becomes	O
impractical	O
as	O
the	O
number	O
of	O
dimensions	O
increases	O
.	O
9.9.	O
exercises	O
107	O
if	O
you	O
can	O
process	B
a	O
million	O
hypotheses	O
in	O
a	O
reasonable	O
amount	O
of	O
time	O
,	O
you	O
could	O
handle	O
two	O
dimensions	O
with	O
1000	O
values	O
for	O
each	O
parameter	B
,	O
or	O
three	O
dimensions	O
with	O
100	O
values	O
each	O
,	O
or	O
six	O
dimensions	O
with	O
10	O
values	O
each	O
.	O
if	O
you	O
need	O
more	O
dimensions	O
,	O
or	O
more	O
values	O
per	O
dimension	O
,	O
there	O
are	O
op-	O
timizations	O
you	O
can	O
try	O
.	O
i	O
present	O
an	O
example	O
in	O
chapter	O
15.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
paintball.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
9.9	O
exercises	O
exercise	O
9.1.	O
in	O
our	O
simple	O
model	O
,	O
the	O
opponent	O
is	O
equally	O
likely	O
to	O
shoot	O
in	O
any	O
direction	O
.	O
as	O
an	O
exercise	O
,	O
let	O
’	O
s	O
consider	O
improvements	O
to	O
this	O
model	O
.	O
the	O
analysis	O
in	O
this	O
chapter	O
suggests	O
that	O
a	O
shooter	O
is	O
most	O
likely	O
to	O
hit	O
the	O
closest	O
wall	O
.	O
but	O
in	O
reality	O
,	O
if	O
the	O
opponent	O
is	O
close	O
to	O
a	O
wall	O
,	O
he	O
is	O
unlikely	O
to	O
shoot	O
at	O
the	O
wall	O
because	O
he	O
is	O
unlikely	O
to	O
see	O
a	O
target	O
between	O
himself	O
and	O
the	O
wall	O
.	O
design	O
an	O
improved	O
model	O
that	O
takes	O
this	O
behavior	O
into	O
account	O
.	O
try	O
to	O
ﬁnd	O
a	O
model	O
that	O
is	O
more	O
realistic	O
,	O
but	O
not	O
too	O
complicated	O
.	O
108	O
chapter	O
9.	O
two	O
dimensions	O
chapter	O
10	O
approximate	O
bayesian	O
computation	O
10.1	O
the	O
variability	O
hypothesis	O
i	O
have	O
a	O
soft	O
spot	O
for	O
crank	B
science	I
.	O
recently	O
i	O
visited	O
norumbega	O
tower	O
,	O
which	O
is	O
an	O
enduring	O
monument	O
to	O
the	O
crackpot	O
theories	O
of	O
eben	O
norton	O
horsford	O
,	O
inventor	O
of	O
double-acting	O
baking	O
powder	O
and	O
fake	O
history	O
.	O
but	O
that	O
’	O
s	O
not	O
what	O
this	O
chapter	O
is	O
about	O
.	O
this	O
chapter	O
is	O
about	O
the	O
variability	O
hypothesis	O
,	O
which	O
''	O
originated	O
in	O
the	O
early	O
nineteenth	O
century	O
with	O
johann	O
meckel	O
,	O
who	O
argued	O
that	O
males	O
have	O
a	O
greater	O
range	O
of	O
ability	O
than	O
fe-	O
males	O
,	O
especially	O
in	O
intelligence	O
.	O
in	O
other	O
words	O
,	O
he	O
believed	O
that	O
most	O
geniuses	O
and	O
most	O
mentally	O
retarded	O
people	O
are	O
men	O
.	O
be-	O
cause	O
he	O
considered	O
males	O
to	O
be	O
the	O
’	O
superior	O
animal	O
,	O
’	O
meckel	O
concluded	O
that	O
females	O
’	O
lack	O
of	O
variation	O
was	O
a	O
sign	O
of	O
inferior-	O
ity	O
.	O
''	O
from	O
hypothesis	O
.	O
http	O
:	O
//en.wikipedia.org/wiki/variability_	O
i	O
particularly	O
like	O
that	O
last	O
part	O
,	O
because	O
i	O
suspect	O
that	O
if	O
it	O
turns	O
out	O
that	O
women	O
are	O
actually	O
more	O
variable	O
,	O
meckel	O
would	O
take	O
that	O
as	O
a	O
sign	O
of	O
in-	O
feriority	O
,	O
too	O
.	O
anyway	O
,	O
you	O
will	O
not	O
be	O
surprised	O
to	O
hear	O
that	O
the	O
evidence	B
for	O
the	O
variability	O
hypothesis	O
is	O
weak	O
.	O
nevertheless	O
,	O
it	O
came	O
up	O
in	O
my	O
class	O
recently	O
when	O
we	O
looked	O
at	O
data	O
from	O
the	O
cdc	O
’	O
s	O
behavioral	O
risk	O
factor	O
surveillance	O
system	O
(	O
brfss	O
)	O
,	O
speciﬁcally	O
110	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
the	O
self-reported	O
heights	O
of	O
adult	O
american	O
men	O
and	O
women	O
.	O
the	O
dataset	O
includes	O
responses	O
from	O
154407	O
men	O
and	O
254722	O
women	O
.	O
here	O
’	O
s	O
what	O
we	O
found	O
:	O
•	O
the	O
average	O
height	B
for	O
men	O
is	O
178	O
cm	O
;	O
the	O
average	O
height	B
for	O
women	O
is	O
163	O
cm	O
.	O
so	O
men	O
are	O
taller	O
,	O
on	O
average	O
.	O
no	O
surprise	O
there	O
.	O
•	O
for	O
men	O
the	O
standard	O
deviation	O
is	O
7.7	O
cm	O
;	O
for	O
women	O
it	O
is	O
7.3	O
cm	O
.	O
so	O
in	O
absolute	O
terms	O
,	O
men	O
’	O
s	O
heights	O
are	O
more	O
variable	O
.	O
•	O
but	O
to	O
compare	O
variability	O
between	O
groups	O
,	O
it	O
is	O
more	O
meaningful	O
to	O
use	O
the	O
coefﬁcient	B
of	I
variation	I
(	O
cv	O
)	O
,	O
which	O
is	O
the	O
standard	O
deviation	O
divided	O
by	O
the	O
mean	O
.	O
it	O
is	O
a	O
dimensionless	O
measure	O
of	O
variability	O
rela-	O
tive	O
to	O
scale	O
.	O
for	O
men	O
cv	O
is	O
0.0433	O
;	O
for	O
women	O
it	O
is	O
0.0444.	O
that	O
’	O
s	O
very	O
close	O
,	O
so	O
we	O
could	O
conclude	O
that	O
this	O
dataset	O
provides	O
weak	O
ev-	O
idence	O
against	O
the	O
variability	O
hypothesis	O
.	O
but	O
we	O
can	O
use	O
bayesian	O
meth-	O
ods	O
to	O
make	O
that	O
conclusion	O
more	O
precise	O
.	O
and	O
answering	O
this	O
question	O
gives	O
me	O
a	O
chance	O
to	O
demonstrate	O
some	O
techniques	O
for	O
working	O
with	O
large	O
datasets	O
.	O
i	O
will	O
proceed	O
in	O
a	O
few	O
steps	O
:	O
1.	O
we	O
’	O
ll	O
start	O
with	O
the	O
simplest	O
implementation	B
,	O
but	O
it	O
only	O
works	O
for	O
datasets	O
smaller	O
than	O
1000	O
values	O
.	O
2.	O
by	O
computing	O
probabilities	O
under	O
a	O
log	B
transform	I
,	O
we	O
can	O
scale	O
up	O
to	O
the	O
full	O
size	O
of	O
the	O
dataset	O
,	O
but	O
the	O
computation	O
gets	O
slow	O
.	O
3.	O
finally	O
,	O
we	O
speed	O
things	O
up	O
substantially	O
with	O
approximate	O
bayesian	O
computation	O
,	O
also	O
known	O
as	O
abc	O
.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
variability.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
10.2	O
mean	O
and	O
standard	O
deviation	O
in	O
chapter	O
9	O
we	O
estimated	O
two	O
parameters	O
simultaneously	O
using	O
a	O
joint	O
dis-	O
tribution	O
.	O
in	O
this	O
chapter	O
we	O
use	O
the	O
same	O
method	O
to	O
estimate	O
the	O
param-	O
eters	O
of	O
a	O
gaussian	O
distribution	B
:	O
the	O
mean	O
,	O
mu	O
,	O
and	O
the	O
standard	O
deviation	O
,	O
sigma	O
.	O
for	O
this	O
problem	O
,	O
i	O
deﬁne	O
a	O
suite	B
called	O
height	B
that	O
represents	O
a	O
map	O
from	O
each	O
mu	O
,	O
sigma	O
pair	O
to	O
its	O
probability	B
:	O
10.2.	O
mean	O
and	O
standard	O
deviation	O
111	O
class	O
height	B
(	O
thinkbayes.suite	O
,	O
thinkbayes.joint	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
mus	O
,	O
sigmas	O
)	O
:	O
pairs	O
=	O
[	O
(	O
mu	O
,	O
sigma	O
)	O
for	O
mu	O
in	O
mus	O
for	O
sigma	O
in	O
sigmas	O
]	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
pairs	O
)	O
mus	O
is	O
a	O
sequence	O
of	O
possible	O
values	O
for	O
mu	O
;	O
sigmas	O
is	O
a	O
sequence	O
of	O
values	O
for	O
sigma	O
.	O
the	O
prior	B
distribution	I
is	O
uniform	O
over	O
all	O
mu	O
,	O
sigma	O
pairs	O
.	O
the	O
likelihood	B
function	I
is	O
easy	O
.	O
given	O
hypothetical	O
values	O
of	O
mu	O
and	O
sigma	O
,	O
we	O
compute	O
the	O
likelihood	B
of	O
a	O
particular	O
value	O
,	O
x.	O
that	O
’	O
s	O
what	O
evalgaussianpdf	O
does	O
,	O
so	O
all	O
we	O
have	O
to	O
do	O
is	O
use	O
it	O
:	O
#	O
class	O
height	B
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
data	O
mu	O
,	O
sigma	O
=	O
hypo	O
like	O
=	O
thinkbayes.evalgaussianpdf	O
(	O
x	O
,	O
mu	O
,	O
sigma	O
)	O
return	O
like	O
if	O
you	O
have	O
studied	O
statistics	O
from	O
a	O
mathematical	O
perspective	O
,	O
you	O
know	O
that	O
when	O
you	O
evaluate	O
a	O
pdf	O
,	O
you	O
get	O
a	O
probability	B
density	I
.	O
in	O
order	O
to	O
get	O
a	O
probability	B
,	O
you	O
have	O
to	O
integrate	O
probability	B
densities	O
over	O
some	O
range	O
.	O
but	O
for	O
our	O
purposes	O
,	O
we	O
don	O
’	O
t	O
need	O
a	O
probability	B
;	O
we	O
just	O
need	O
something	O
proportional	O
to	O
the	O
probability	B
we	O
want	O
.	O
a	O
probability	B
density	I
does	O
that	O
job	O
nicely	O
.	O
the	O
hardest	O
part	O
of	O
this	O
problem	O
turns	O
out	O
to	O
be	O
choosing	O
appropriate	O
ranges	O
for	O
mus	O
and	O
sigmas	O
.	O
if	O
the	O
range	O
is	O
too	O
small	O
,	O
we	O
omit	O
some	O
pos-	O
sibilities	O
with	O
non-negligible	O
probability	B
and	O
get	O
the	O
wrong	O
answer	O
.	O
if	O
the	O
range	O
is	O
too	O
big	O
,	O
we	O
get	O
the	O
right	O
answer	O
,	O
but	O
waste	O
computational	O
power	O
.	O
so	O
this	O
is	O
an	O
opportunity	O
to	O
use	O
classical	B
estimation	I
to	O
make	O
bayesian	O
tech-	O
niques	O
more	O
efﬁcient	O
.	O
speciﬁcally	O
,	O
we	O
can	O
use	O
classical	O
estimators	O
to	O
ﬁnd	O
a	O
likely	O
location	O
for	O
mu	O
and	O
sigma	O
,	O
and	O
use	O
the	O
standard	O
errors	O
of	O
those	O
esti-	O
mates	O
to	O
choose	O
a	O
likely	O
spread	O
.	O
if	O
the	O
true	O
parameters	O
of	O
the	O
distribution	B
are	O
µ	O
and	O
σ	O
,	O
and	O
we	O
take	O
a	O
sample	O
of	O
n	O
values	O
,	O
an	O
estimator	O
of	O
µ	O
is	O
the	O
sample	O
mean	O
,	O
m.	O
and	O
an	O
estimator	O
of	O
σ	O
is	O
the	O
sample	O
standard	O
variance	O
,	O
s.	O
112	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
estimated	O
σ	O
is	O
s/	O
(	O
cid:112	O
)	O
2	O
(	O
n	O
−	O
1	O
)	O
.	O
the	O
standard	O
error	B
of	O
the	O
estimated	O
µ	O
is	O
s/	O
√	O
n	O
and	O
the	O
standard	O
error	B
of	O
the	O
here	O
’	O
s	O
the	O
code	O
to	O
compute	O
all	O
that	O
:	O
def	O
findpriorranges	O
(	O
xs	O
,	O
num_points	O
,	O
num_stderrs=3.0	O
)	O
:	O
#	O
compute	O
m	O
and	O
s	O
n	O
=	O
len	O
(	O
xs	O
)	O
m	O
=	O
numpy.mean	O
(	O
xs	O
)	O
s	O
=	O
numpy.std	O
(	O
xs	O
)	O
#	O
compute	O
ranges	O
for	O
m	O
and	O
s	O
stderr_m	O
=	O
s	O
/	O
math.sqrt	O
(	O
n	O
)	O
mus	O
=	O
makerange	O
(	O
m	O
,	O
stderr_m	O
,	O
num_stderrs	O
)	O
stderr_s	O
=	O
s	O
/	O
math.sqrt	O
(	O
2	O
*	O
(	O
n-1	O
)	O
)	O
sigmas	O
=	O
makerange	O
(	O
s	O
,	O
stderr_s	O
,	O
num_stderrs	O
)	O
return	O
mus	O
,	O
sigmas	O
xs	O
is	O
the	O
dataset	O
.	O
num_points	O
is	O
the	O
desired	O
number	O
of	O
values	O
in	O
the	O
range	O
.	O
num_stderrs	O
is	O
the	O
width	O
of	O
the	O
range	O
on	O
each	O
side	O
of	O
the	O
estimate	O
,	O
in	O
num-	O
ber	O
of	O
standard	O
errors	O
.	O
the	O
return	O
value	O
is	O
a	O
pair	O
of	O
sequences	O
,	O
mus	O
and	O
sigmas	O
.	O
here	O
’	O
s	O
makerange	O
:	O
def	O
makerange	O
(	O
estimate	O
,	O
stderr	O
,	O
num_stderrs	O
)	O
:	O
spread	O
=	O
stderr	O
*	O
num_stderrs	O
array	O
=	O
numpy.linspace	O
(	O
estimate-spread	O
,	O
estimate+spread	O
,	O
num_points	O
)	O
return	O
array	O
numpy.linspace	O
makes	O
an	O
array	O
of	O
equally	O
spaced	O
elements	O
between	O
estimate-spread	O
and	O
estimate+spread	O
,	O
including	O
both	O
.	O
10.3	O
update	O
finally	O
here	O
’	O
s	O
the	O
code	O
to	O
make	O
and	O
update	O
the	O
suite	B
:	O
mus	O
,	O
sigmas	O
=	O
findpriorranges	O
(	O
xs	O
,	O
num_points	O
)	O
suite	B
=	O
height	B
(	O
mus	O
,	O
sigmas	O
)	O
10.4.	O
the	O
posterior	B
distribution	I
of	O
cv	O
113	O
suite.updateset	O
(	O
xs	O
)	O
print	O
suite.maximumlikelihood	O
(	O
)	O
this	O
process	B
might	O
seem	O
bogus	B
,	O
because	O
we	O
use	O
the	O
data	O
to	O
choose	O
the	O
range	O
of	O
the	O
prior	B
distribution	I
,	O
and	O
then	O
use	O
the	O
data	O
again	O
to	O
do	O
the	O
update	O
.	O
in	O
general	O
,	O
using	O
the	O
same	O
data	O
twice	O
is	O
,	O
in	O
fact	O
,	O
bogus	B
.	O
but	O
in	O
this	O
case	O
it	O
is	O
ok.	O
really	O
.	O
we	O
use	O
the	O
data	O
to	O
choose	O
the	O
range	O
for	O
the	O
prior	B
,	O
but	O
only	O
to	O
avoid	O
computing	O
a	O
lot	O
of	O
probabilities	O
that	O
would	O
have	O
been	O
very	O
small	O
anyway	O
.	O
with	O
num_stderrs=4	O
,	O
the	O
range	O
is	O
big	O
enough	O
to	O
cover	O
all	O
values	O
with	O
non-negligible	O
likelihood	B
.	O
after	O
that	O
,	O
making	O
it	O
bigger	O
has	O
no	O
effect	O
on	O
the	O
results	O
.	O
in	O
effect	O
,	O
the	O
prior	B
is	O
uniform	O
over	O
all	O
values	O
of	O
mu	O
and	O
sigma	O
,	O
but	O
for	O
com-	O
putational	O
efﬁciency	O
we	O
ignore	O
all	O
the	O
values	O
that	O
don	O
’	O
t	O
matter	O
.	O
10.4	O
the	O
posterior	B
distribution	I
of	O
cv	O
once	O
we	O
have	O
the	O
posterior	B
joint	O
distribution	B
of	O
mu	O
and	O
sigma	O
,	O
we	O
can	O
com-	O
pute	O
the	O
distribution	B
of	O
cv	O
for	O
men	O
and	O
women	O
,	O
and	O
then	O
the	O
probability	B
that	O
one	O
exceeds	O
the	O
other	O
.	O
to	O
compute	O
the	O
distribution	B
of	O
cv	O
,	O
we	O
enumerate	O
pairs	O
of	O
mu	O
and	O
sigma	O
:	O
def	O
coefvariation	O
(	O
suite	B
)	O
:	O
pmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
(	O
mu	O
,	O
sigma	O
)	O
,	O
p	O
in	O
suite.items	O
(	O
)	O
:	O
pmf.incr	O
(	O
sigma/mu	O
,	O
p	O
)	O
return	O
pmf	O
then	O
we	O
use	O
thinkbayes.pmfprobgreater	O
to	O
compute	O
the	O
probability	B
that	O
men	O
are	O
more	O
variable	O
.	O
the	O
analysis	O
itself	O
is	O
simple	O
,	O
but	O
there	O
are	O
two	O
more	O
issues	O
we	O
have	O
to	O
deal	O
with	O
:	O
1.	O
as	O
the	O
size	O
of	O
the	O
dataset	O
increases	O
,	O
we	O
run	O
into	O
a	O
series	O
of	O
computa-	O
tional	O
problems	O
due	O
to	O
the	O
limitations	O
of	O
ﬂoating-point	O
arithmetic	O
.	O
2.	O
the	O
dataset	O
contains	O
a	O
number	O
of	O
extreme	O
values	O
that	O
are	O
almost	O
cer-	O
tainly	O
errors	O
.	O
we	O
will	O
need	O
to	O
make	O
the	O
estimation	O
process	B
robust	O
in	O
the	O
presence	O
of	O
these	O
outliers	O
.	O
the	O
following	O
sections	O
explain	O
these	O
problems	O
and	O
their	O
solutions	O
.	O
114	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
10.5	O
underﬂow	B
if	O
we	O
select	O
the	O
ﬁrst	O
100	O
values	O
from	O
the	O
brfss	O
dataset	O
and	O
run	O
the	O
analysis	O
i	O
just	O
described	O
,	O
it	O
runs	O
without	O
errors	O
and	O
we	O
get	O
posterior	B
distributions	O
that	O
look	O
reasonable	O
.	O
if	O
we	O
select	O
the	O
ﬁrst	O
1000	O
values	O
and	O
run	O
the	O
program	O
again	O
,	O
we	O
get	O
an	O
error	B
in	O
pmf.normalize	O
:	O
valueerror	O
:	O
total	B
probability	I
is	O
zero	O
.	O
the	O
problem	O
is	O
that	O
we	O
are	O
using	O
probability	B
densities	O
to	O
compute	O
likeli-	O
hoods	O
,	O
and	O
densities	O
from	O
continuous	O
distributions	O
tend	O
to	O
be	O
small	O
.	O
and	O
if	O
you	O
take	O
1000	O
small	O
values	O
and	O
multiply	O
them	O
together	O
,	O
the	O
result	O
is	O
very	O
small	O
.	O
in	O
this	O
case	O
it	O
is	O
so	O
small	O
it	O
can	O
’	O
t	O
be	O
represented	O
by	O
a	O
ﬂoating-point	O
number	O
,	O
so	O
it	O
gets	O
rounded	O
down	O
to	O
zero	O
,	O
which	O
is	O
called	O
underﬂow	B
.	O
and	O
if	O
all	O
probabilities	O
in	O
the	O
distribution	B
are	O
0	O
,	O
it	O
’	O
s	O
not	O
a	O
distribution	B
any	O
more	O
.	O
a	O
possible	O
solution	O
is	O
to	O
renormalize	B
the	O
pmf	O
after	O
each	O
update	O
,	O
or	O
after	O
each	O
batch	O
of	O
100.	O
that	O
would	O
work	O
,	O
but	O
it	O
would	O
be	O
slow	O
.	O
a	O
better	O
alternative	O
is	O
to	O
compute	O
likelihoods	O
under	O
a	O
log	B
transform	I
.	O
that	O
way	O
,	O
instead	O
of	O
multiplying	O
small	O
values	O
,	O
we	O
can	O
add	O
up	O
log	O
likelihoods	O
.	O
pmf	O
provides	O
methods	O
log	O
,	O
logupdateset	O
and	O
exp	O
to	O
make	O
this	O
process	B
easy	O
.	O
log	O
computes	O
the	O
log	O
of	O
the	O
probabilities	O
in	O
a	O
pmf	O
:	O
#	O
class	O
pmf	O
def	O
log	O
(	O
self	O
)	O
:	O
m	O
=	O
self.maxlike	O
(	O
)	O
for	O
x	O
,	O
p	O
in	O
self.d.iteritems	O
(	O
)	O
:	O
if	O
p	O
:	O
self.set	O
(	O
x	O
,	O
math.log	O
(	O
p/m	O
)	O
)	O
else	O
:	O
self.remove	O
(	O
x	O
)	O
before	O
applying	O
the	O
log	B
transform	I
log	O
uses	O
maxlike	O
to	O
ﬁnd	O
m	O
,	O
the	O
highest	O
probability	B
in	O
the	O
pmf	O
.	O
it	O
divide	O
all	O
probabilities	O
by	O
m	O
,	O
so	O
the	O
highest	O
proba-	O
bility	O
gets	O
normalized	O
to	O
1	O
,	O
which	O
yields	O
a	O
log	O
of	O
0.	O
the	O
other	O
log	O
probabil-	O
ities	O
are	O
all	O
negative	O
.	O
if	O
there	O
are	O
any	O
values	O
in	O
the	O
pmf	O
with	O
probability	B
0	O
,	O
they	O
are	O
removed	O
.	O
while	O
the	O
pmf	O
is	O
under	O
a	O
log	B
transform	I
,	O
we	O
can	O
’	O
t	O
use	O
update	O
,	O
updateset	O
,	O
or	O
normalize	B
.	O
the	O
result	O
would	O
be	O
nonsensical	O
;	O
if	O
you	O
try	O
,	O
pmf	O
raises	O
an	O
exception	B
.	O
instead	O
,	O
we	O
have	O
to	O
use	O
logupdate	O
and	O
logupdateset	O
.	O
10.6.	O
log-likelihood	B
115	O
here	O
’	O
s	O
the	O
implementation	B
of	O
logupdateset	O
:	O
#	O
class	O
suite	B
def	O
logupdateset	O
(	O
self	O
,	O
dataset	O
)	O
:	O
for	O
data	O
in	O
dataset	O
:	O
self.logupdate	O
(	O
data	O
)	O
logupdateset	O
loops	O
through	O
the	O
data	O
and	O
calls	O
logupdate	O
:	O
#	O
class	O
suite	B
def	O
logupdate	O
(	O
self	O
,	O
data	O
)	O
:	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
like	O
=	O
self.loglikelihood	O
(	O
data	O
,	O
hypo	O
)	O
self.incr	O
(	O
hypo	O
,	O
like	O
)	O
logupdate	O
is	O
just	O
like	O
update	O
except	O
that	O
it	O
calls	O
loglikelihood	O
instead	O
of	O
likelihood	B
,	O
and	O
incr	O
instead	O
of	O
mult	O
.	O
using	O
log-likelihoods	O
avoids	O
the	O
problem	O
with	O
underﬂow	B
,	O
but	O
while	O
the	O
pmf	O
is	O
under	O
the	O
log	B
transform	I
,	O
there	O
’	O
s	O
not	O
much	O
we	O
can	O
do	O
with	O
it	O
.	O
we	O
have	O
to	O
use	O
exp	O
to	O
invert	O
the	O
transform	O
:	O
#	O
class	O
pmf	O
def	O
exp	O
(	O
self	O
)	O
:	O
m	O
=	O
self.maxlike	O
(	O
)	O
for	O
x	O
,	O
p	O
in	O
self.d.iteritems	O
(	O
)	O
:	O
self.set	O
(	O
x	O
,	O
math.exp	O
(	O
p-m	O
)	O
)	O
if	O
the	O
log-likelihoods	O
are	O
large	O
negative	O
numbers	O
,	O
the	O
resulting	O
likelihoods	O
might	O
underﬂow	B
.	O
so	O
exp	O
ﬁnds	O
the	O
maximum	B
log-likelihood	O
,	O
m	O
,	O
and	O
shifts	O
all	O
the	O
likelihoods	O
up	O
by	O
m.	O
the	O
resulting	O
distribution	B
has	O
a	O
maximum	B
like-	O
lihood	O
of	O
1.	O
this	O
process	B
inverts	O
the	O
log	B
transform	I
with	O
minimal	O
loss	O
of	O
precision	O
.	O
10.6	O
log-likelihood	B
now	O
all	O
we	O
need	O
is	O
loglikelihood	O
.	O
#	O
class	O
height	B
def	O
loglikelihood	O
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
data	O
116	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
mu	O
,	O
sigma	O
=	O
hypo	O
loglike	O
=	O
scipy.stats.norm.logpdf	O
(	O
x	O
,	O
mu	O
,	O
sigma	O
)	O
return	O
loglike	O
norm.logpdf	O
computes	O
the	O
log-likelihood	B
of	O
the	O
gaussian	O
pdf	O
.	O
here	O
’	O
s	O
what	O
the	O
whole	O
update	O
process	B
looks	O
like	O
:	O
suite.log	O
(	O
)	O
suite.logupdateset	O
(	O
xs	O
)	O
suite.exp	O
(	O
)	O
suite.normalize	O
(	O
)	O
to	O
review	O
,	O
log	O
puts	O
the	O
suite	B
under	O
a	O
log	B
transform	I
.	O
logupdateset	O
calls	O
logupdate	O
,	O
which	O
calls	O
loglikelihood	O
.	O
logupdate	O
uses	O
pmf.incr	O
,	O
because	O
adding	O
a	O
log-likelihood	B
is	O
the	O
same	O
as	O
multiplying	O
by	O
a	O
likelihood	B
.	O
after	O
the	O
update	O
,	O
the	O
log-likelihoods	O
are	O
large	O
negative	O
numbers	O
,	O
so	O
exp	O
shifts	O
them	O
up	O
before	O
inverting	O
the	O
transform	O
,	O
which	O
is	O
how	O
we	O
avoid	O
un-	O
derﬂow	O
.	O
once	O
the	O
suite	B
is	O
transformed	O
back	O
,	O
the	O
probabilities	O
are	O
“	O
linear	O
”	O
again	O
,	O
which	O
means	O
“	O
not	O
logarithmic	O
”	O
,	O
so	O
we	O
can	O
use	O
normalize	B
again	O
.	O
using	O
this	O
algorithm	O
,	O
we	O
can	O
process	B
the	O
entire	O
dataset	O
without	O
underﬂow	B
,	O
but	O
it	O
is	O
still	O
slow	O
.	O
on	O
my	O
computer	O
it	O
might	O
take	O
an	O
hour	O
.	O
we	O
can	O
do	O
better	O
.	O
10.7	O
a	O
little	O
optimization	B
this	O
section	O
uses	O
math	O
and	O
computational	O
optimization	B
to	O
speed	O
things	O
up	O
by	O
a	O
factor	O
of	O
100.	O
but	O
the	O
following	O
section	O
presents	O
an	O
algorithm	O
that	O
is	O
even	O
faster	O
.	O
so	O
if	O
you	O
want	O
to	O
get	O
right	O
to	O
the	O
good	O
stuff	O
,	O
feel	O
free	O
to	O
skip	O
this	O
section	O
.	O
suite.logupdateset	O
calls	O
logupdate	O
once	O
for	O
each	O
data	O
point	O
.	O
we	O
can	O
speed	O
it	O
up	O
by	O
computing	O
the	O
log-likelihood	B
of	O
the	O
entire	O
dataset	O
at	O
once	O
.	O
(	O
cid:19	O
)	O
2	O
(	O
cid:35	O
)	O
(	O
cid:18	O
)	O
x	O
−	O
µ	O
(	O
cid:19	O
)	O
2	O
(	O
cid:18	O
)	O
x	O
−	O
µ	O
σ	O
σ	O
−	O
log	O
σ	O
−	O
1	O
2	O
we	O
’	O
ll	O
start	O
with	O
the	O
gaussian	O
pdf	O
:	O
√	O
1	O
2π	O
σ	O
exp	O
(	O
cid:34	O
)	O
−1	O
2	O
and	O
compute	O
the	O
log	O
(	O
dropping	O
the	O
constant	O
term	O
)	O
:	O
10.7.	O
a	O
little	O
optimization	B
117	O
given	O
a	O
sequence	O
of	O
values	O
,	O
xi	O
,	O
the	O
total	O
log-likelihood	O
is	O
(	O
cid:18	O
)	O
xi	O
−	O
µ	O
(	O
cid:19	O
)	O
2	O
∑	O
i	O
−	O
log	O
σ	O
−	O
1	O
2	O
σ	O
pulling	O
out	O
the	O
terms	O
that	O
don	O
’	O
t	O
depend	O
on	O
i	O
,	O
we	O
get	O
(	O
xi	O
−	O
µ	O
)	O
2	O
−n	O
log	O
σ	O
−	O
1	O
2σ2	O
∑	O
i	O
which	O
we	O
can	O
translate	O
into	O
python	O
:	O
#	O
class	O
height	B
def	O
logupdatesetfast	O
(	O
self	O
,	O
data	O
)	O
:	O
xs	O
=	O
tuple	B
(	O
data	O
)	O
n	O
=	O
len	O
(	O
xs	O
)	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
mu	O
,	O
sigma	O
=	O
hypo	O
total	O
=	O
summation	O
(	O
xs	O
,	O
mu	O
)	O
loglike	O
=	O
-n	O
*	O
math.log	O
(	O
sigma	O
)	O
-	O
total	O
/	O
2	O
/	O
sigma**2	O
self.incr	O
(	O
hypo	O
,	O
loglike	O
)	O
by	O
itself	O
,	O
this	O
would	O
be	O
a	O
small	O
improvement	O
,	O
but	O
it	O
creates	O
an	O
opportunity	O
for	O
a	O
bigger	O
one	O
.	O
notice	O
that	O
the	O
summation	O
only	O
depends	O
on	O
mu	O
,	O
not	O
sigma	O
,	O
so	O
we	O
only	O
have	O
to	O
compute	O
it	O
once	O
for	O
each	O
value	O
of	O
mu	O
.	O
to	O
avoid	O
recomputing	O
,	O
i	O
factor	O
out	O
a	O
function	O
that	O
computes	O
the	O
summation	O
,	O
and	O
memoize	O
it	O
so	O
it	O
stores	O
previously	O
computed	O
results	O
in	O
a	O
dictionary	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/memoization	O
)	O
:	O
def	O
summation	O
(	O
xs	O
,	O
mu	O
,	O
cache=	O
{	O
}	O
)	O
:	O
try	O
:	O
return	O
cache	B
[	O
xs	O
,	O
mu	O
]	O
except	O
keyerror	O
:	O
ds	O
=	O
[	O
(	O
x-mu	O
)	O
**2	O
for	O
x	O
in	O
xs	O
]	O
total	O
=	O
sum	O
(	O
ds	O
)	O
cache	B
[	O
xs	O
,	O
mu	O
]	O
=	O
total	O
return	O
total	O
cache	O
stores	O
previously	O
computed	O
sums	O
.	O
the	O
try	O
statement	O
returns	O
a	O
re-	O
sult	O
from	O
the	O
cache	B
if	O
possible	O
;	O
otherwise	O
it	O
computes	O
the	O
summation	O
,	O
then	O
caches	O
and	O
returns	O
the	O
result	O
.	O
the	O
only	O
catch	O
is	O
that	O
we	O
can	O
’	O
t	O
use	O
a	O
list	O
as	O
a	O
key	O
in	O
the	O
cache	B
,	O
because	O
it	O
is	O
118	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
not	O
a	O
hashable	O
type	O
.	O
that	O
’	O
s	O
why	O
logupdatesetfast	O
converts	O
the	O
dataset	O
to	O
a	O
tuple	B
.	O
this	O
optimization	B
speeds	O
up	O
the	O
computation	O
by	O
about	O
a	O
factor	O
of	O
100	O
,	O
pro-	O
cessing	O
the	O
entire	O
dataset	O
(	O
154	O
407	O
men	O
and	O
254	O
722	O
women	O
)	O
in	O
less	O
than	O
a	O
minute	O
on	O
my	O
not-very-fast	O
computer	O
.	O
10.8	O
abc	O
but	O
maybe	O
you	O
don	O
’	O
t	O
have	O
that	O
kind	O
of	O
time	O
.	O
in	O
that	O
case	O
,	O
approximate	O
bayesian	O
computation	O
(	O
abc	O
)	O
might	O
be	O
the	O
way	O
to	O
go	O
.	O
the	O
motivation	O
be-	O
hind	O
abc	O
is	O
that	O
the	O
likelihood	B
of	O
any	O
particular	O
dataset	O
is	O
:	O
1.	O
very	O
small	O
,	O
especially	O
for	O
large	O
datasets	O
,	O
which	O
is	O
why	O
we	O
had	O
to	O
use	O
the	O
log	B
transform	I
,	O
2.	O
expensive	O
to	O
compute	O
,	O
which	O
is	O
why	O
we	O
had	O
to	O
do	O
so	O
much	O
optimiza-	O
tion	O
,	O
and	O
3.	O
not	O
really	O
what	O
we	O
want	O
anyway	O
.	O
we	O
don	O
’	O
t	O
really	O
care	O
about	O
the	O
likelihood	B
of	O
seeing	O
the	O
exact	O
dataset	O
we	O
saw	O
.	O
especially	O
for	O
continuous	O
variables	O
,	O
we	O
care	O
about	O
the	O
likelihood	B
of	O
seeing	O
any	O
dataset	O
like	O
the	O
one	O
we	O
saw	O
.	O
for	O
example	O
,	O
in	O
the	O
euro	O
problem	O
,	O
we	O
don	O
’	O
t	O
care	O
about	O
the	O
order	O
of	O
the	O
coin	O
ﬂips	O
,	O
only	O
the	O
total	O
number	O
of	O
heads	O
and	O
tails	O
.	O
and	O
in	O
the	O
locomotive	B
problem	I
,	O
we	O
don	O
’	O
t	O
care	O
about	O
which	O
particular	O
trains	O
were	O
seen	O
,	O
only	O
the	O
number	O
of	O
trains	O
and	O
the	O
maximum	B
of	O
the	O
serial	O
numbers	O
.	O
similarly	O
,	O
in	O
the	O
brfss	O
sample	O
,	O
we	O
don	O
’	O
t	O
really	O
want	O
to	O
know	O
the	O
probability	B
of	O
seeing	O
one	O
particular	O
set	O
of	O
values	O
(	O
especially	O
since	O
there	O
are	O
hundreds	O
of	O
thousands	O
of	O
them	O
)	O
.	O
it	O
is	O
more	O
relevant	O
to	O
ask	O
,	O
“	O
if	O
we	O
sample	O
100,000	O
people	O
from	O
a	O
population	O
with	O
hypothetical	O
values	O
of	O
µ	O
and	O
σ	O
,	O
what	O
would	O
be	O
the	O
chance	O
of	O
collecting	O
a	O
sample	O
with	O
the	O
observed	O
mean	O
and	O
variance	O
?	O
”	O
for	O
samples	O
from	O
a	O
gaussian	O
distribution	B
,	O
we	O
can	O
answer	O
this	O
question	O
efﬁ-	O
ciently	O
because	O
we	O
can	O
ﬁnd	O
the	O
distribution	B
of	O
the	O
sample	B
statistics	I
analyt-	O
ically	O
.	O
in	O
fact	O
,	O
we	O
already	O
did	O
it	O
when	O
we	O
computed	O
the	O
range	O
of	O
the	O
prior	B
.	O
if	O
you	O
draw	O
n	O
values	O
from	O
a	O
gaussian	O
distribution	B
with	O
parameters	O
µ	O
and	O
σ	O
,	O
and	O
compute	O
the	O
sample	O
mean	O
,	O
m	O
,	O
the	O
distribution	B
of	O
m	O
is	O
gaussian	O
with	O
parameters	O
µ	O
and	O
σ/	O
√	O
n.	O
10.9.	O
robust	B
estimation	I
with	O
parameters	O
σ	O
and	O
σ/	O
(	O
cid:112	O
)	O
2	O
(	O
n	O
−	O
1	O
)	O
.	O
119	O
similarly	O
,	O
the	O
distribution	B
of	O
the	O
sample	O
standard	O
deviation	O
,	O
s	O
,	O
is	O
gaussian	O
we	O
can	O
use	O
these	O
sample	O
distributions	O
to	O
compute	O
the	O
likelihood	B
of	O
the	O
sam-	O
ple	O
statistics	O
,	O
m	O
and	O
s	O
,	O
given	O
hypothetical	O
values	O
for	O
µ	O
and	O
σ.	O
here	O
’	O
s	O
a	O
new	O
version	O
of	O
logupdateset	O
that	O
does	O
it	O
:	O
def	O
logupdatesetabc	O
(	O
self	O
,	O
data	O
)	O
:	O
xs	O
=	O
data	O
n	O
=	O
len	O
(	O
xs	O
)	O
#	O
compute	O
sample	B
statistics	I
m	O
=	O
numpy.mean	O
(	O
xs	O
)	O
s	O
=	O
numpy.std	O
(	O
xs	O
)	O
for	O
hypo	O
in	O
sorted	O
(	O
self.values	O
(	O
)	O
)	O
:	O
mu	O
,	O
sigma	O
=	O
hypo	O
#	O
compute	O
log	O
likelihood	O
of	O
m	O
,	O
given	O
hypo	O
stderr_m	O
=	O
sigma	O
/	O
math.sqrt	O
(	O
n	O
)	O
loglike	O
=	O
evalgaussianlogpdf	O
(	O
m	O
,	O
mu	O
,	O
stderr_m	O
)	O
#	O
compute	O
log	O
likelihood	O
of	O
s	O
,	O
given	O
hypo	O
stderr_s	O
=	O
sigma	O
/	O
math.sqrt	O
(	O
2	O
*	O
(	O
n-1	O
)	O
)	O
loglike	O
+=	O
evalgaussianlogpdf	O
(	O
s	O
,	O
sigma	O
,	O
stderr_s	O
)	O
self.incr	O
(	O
hypo	O
,	O
loglike	O
)	O
on	O
my	O
computer	O
this	O
function	O
processes	O
the	O
entire	O
dataset	O
in	O
about	O
a	O
sec-	O
ond	O
,	O
and	O
the	O
result	O
agrees	O
with	O
the	O
exact	O
result	O
with	O
about	O
5	O
digits	O
of	O
preci-	O
sion	O
.	O
10.9	O
robust	B
estimation	I
we	O
are	O
almost	O
ready	O
to	O
look	O
at	O
results	O
,	O
but	O
we	O
have	O
one	O
more	O
problem	O
to	O
deal	O
with	O
.	O
there	O
are	O
a	O
number	O
of	O
outliers	O
in	O
this	O
dataset	O
that	O
are	O
almost	O
certainly	O
errors	O
.	O
for	O
example	O
,	O
there	O
are	O
three	O
adults	O
with	O
reported	O
height	B
of	O
61	O
cm	O
,	O
which	O
would	O
place	O
them	O
among	O
the	O
shortest	O
living	O
adults	O
in	O
the	O
world	O
.	O
at	O
the	O
other	O
end	O
,	O
there	O
are	O
four	O
women	O
with	O
reported	O
height	B
229	O
cm	O
,	O
just	O
short	O
of	O
the	O
tallest	O
women	O
in	O
the	O
world	O
.	O
it	O
is	O
not	O
impossible	O
that	O
these	O
values	O
are	O
correct	O
,	O
but	O
it	O
is	O
unlikely	O
,	O
which	O
makes	O
it	O
hard	O
to	O
know	O
how	O
to	O
deal	O
with	O
them	O
.	O
and	O
we	O
have	O
to	O
get	O
it	O
120	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
figure	O
10.1	O
:	O
contour	O
plot	O
of	O
the	O
posterior	B
joint	O
distribution	B
of	O
mean	O
and	O
standard	O
deviation	O
of	O
height	B
for	O
men	O
in	O
the	O
u.s.	O
figure	O
10.2	O
:	O
contour	O
plot	O
of	O
the	O
posterior	B
joint	O
distribution	B
of	O
mean	O
and	O
standard	O
deviation	O
of	O
height	B
for	O
women	O
in	O
the	O
u.s.	O
178.46178.48178.50178.52178.54mean	O
height	B
(	O
cm	O
)	O
7.287.297.307.317.327.337.347.35stddev	O
(	O
cm	O
)	O
0.0010.0020.0020.0030.0040.0050.006posterior	O
joint	O
distribution163.46163.47163.48163.49163.50163.51163.52163.53mean	O
height	B
(	O
cm	O
)	O
6.997.007.017.027.037.04stddev	O
(	O
cm	O
)	O
0.0010.0020.0020.0030.0040.0050.006posterior	O
joint	B
distribution	I
10.9.	O
robust	B
estimation	I
121	O
right	O
,	O
because	O
these	O
extreme	O
values	O
have	O
a	O
disproportionate	O
effect	O
on	O
the	O
estimated	O
variability	O
.	O
because	O
abc	O
is	O
based	O
on	O
summary	O
statistics	O
,	O
rather	O
than	O
the	O
entire	O
dataset	O
,	O
we	O
can	O
make	O
it	O
more	O
robust	O
by	O
choosing	O
summary	O
statistics	O
that	O
are	O
robust	O
in	O
the	O
presence	O
of	O
outliers	O
.	O
for	O
example	O
,	O
rather	O
than	O
use	O
the	O
sample	O
mean	O
and	O
standard	O
deviation	O
,	O
we	O
could	O
use	O
the	O
median	B
and	O
inter-quartile	B
range	I
(	O
iqr	O
)	O
,	O
which	O
is	O
the	O
difference	O
between	O
the	O
25th	O
and	O
75th	O
percentiles	O
.	O
more	O
generally	O
,	O
we	O
could	O
compute	O
an	O
inter-percentile	O
range	O
(	O
ipr	O
)	O
that	O
spans	O
any	O
given	O
fraction	O
of	O
the	O
distribution	B
,	O
p	O
:	O
def	O
medianipr	O
(	O
xs	O
,	O
p	O
)	O
:	O
cdf	O
=	O
thinkbayes.makecdffromlist	O
(	O
xs	O
)	O
median	B
=	O
cdf.percentile	O
(	O
50	O
)	O
alpha	O
=	O
(	O
1-p	O
)	O
/	O
2	O
ipr	O
=	O
cdf.value	O
(	O
1-alpha	O
)	O
-	O
cdf.value	O
(	O
alpha	O
)	O
return	O
median	B
,	O
ipr	O
xs	O
is	O
a	O
sequence	O
of	O
values	O
.	O
p	O
is	O
the	O
desired	O
range	O
;	O
for	O
example	O
,	O
p=0.5	O
yields	O
the	O
inter-quartile	B
range	I
.	O
medianipr	O
works	O
by	O
computing	O
the	O
cdf	O
of	O
xs	O
,	O
then	O
extracting	O
the	O
median	B
and	O
the	O
difference	O
between	O
two	O
percentiles	O
.	O
we	O
can	O
convert	O
from	O
ipr	O
to	O
an	O
estimate	O
of	O
sigma	O
using	O
the	O
gaussian	O
cdf	O
to	O
compute	O
the	O
fraction	O
of	O
the	O
distribution	B
covered	O
by	O
a	O
given	O
number	O
of	O
standard	O
deviations	O
.	O
for	O
example	O
,	O
it	O
is	O
a	O
well-known	O
rule	O
of	O
thumb	O
that	O
68	O
%	O
of	O
a	O
gaussian	O
distribution	B
falls	O
within	O
one	O
standard	O
deviation	O
of	O
the	O
mean	O
,	O
which	O
leaves	O
16	O
%	O
in	O
each	O
tail	O
.	O
if	O
we	O
compute	O
the	O
range	O
between	O
the	O
16th	O
and	O
84th	O
percentiles	O
,	O
we	O
expect	O
the	O
result	O
to	O
be	O
2	O
*	O
sigma	O
.	O
so	O
we	O
can	O
estimate	O
sigma	O
by	O
computing	O
the	O
68	O
%	O
ipr	O
and	O
dividing	O
by	O
2.	O
more	O
generally	O
we	O
could	O
use	O
any	O
number	O
of	O
sigmas	O
.	O
medians	O
performs	O
the	O
more	O
general	O
version	O
of	O
this	O
computation	O
:	O
def	O
medians	O
(	O
xs	O
,	O
num_sigmas	O
)	O
:	O
half_p	O
=	O
thinkbayes.standardgaussiancdf	O
(	O
num_sigmas	O
)	O
-	O
0.5	O
median	B
,	O
ipr	O
=	O
medianipr	O
(	O
xs	O
,	O
half_p	O
*	O
2	O
)	O
s	O
=	O
ipr	O
/	O
2	O
/	O
num_sigmas	O
return	O
median	B
,	O
s	O
122	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
figure	O
10.3	O
:	O
posterior	B
distributions	O
of	O
cv	O
for	O
men	O
and	O
women	O
,	O
based	O
on	O
robust	O
estimators	O
.	O
again	O
,	O
xs	O
is	O
the	O
sequence	O
of	O
values	O
;	O
num_sigmas	O
is	O
the	O
number	O
of	O
standard	O
deviations	O
the	O
results	O
should	O
be	O
based	O
on	O
.	O
the	O
result	O
is	O
median	B
,	O
which	O
esti-	O
mates	O
µ	O
,	O
and	O
s	O
,	O
which	O
estimates	O
σ.	O
finally	O
,	O
in	O
logupdatesetabc	O
we	O
can	O
replace	O
the	O
sample	O
mean	O
and	O
standard	O
deviation	O
with	O
median	B
and	O
s.	O
and	O
that	O
pretty	O
much	O
does	O
it	O
.	O
it	O
might	O
seem	O
odd	O
that	O
we	O
are	O
using	O
observed	O
percentiles	O
to	O
estimate	O
µ	O
and	O
σ	O
,	O
but	O
it	O
is	O
an	O
example	O
of	O
the	O
ﬂexibility	O
of	O
the	O
bayesian	O
approach	O
.	O
in	O
effect	O
we	O
are	O
asking	O
,	O
“	O
given	O
hypothetical	O
values	O
for	O
µ	O
and	O
σ	O
,	O
and	O
a	O
sampling	O
pro-	O
cess	O
that	O
has	O
some	O
chance	O
of	O
introducing	O
errors	O
,	O
what	O
is	O
the	O
likelihood	B
of	O
generating	O
a	O
given	O
set	O
of	O
sample	B
statistics	I
?	O
”	O
we	O
are	O
free	O
to	O
choose	O
any	O
sample	B
statistics	I
we	O
like	O
,	O
up	O
to	O
a	O
point	O
:	O
µ	O
and	O
σ	O
determine	O
the	O
location	O
and	O
spread	O
of	O
a	O
distribution	B
,	O
so	O
we	O
need	O
to	O
choose	O
statistics	O
that	O
capture	O
those	O
characteristics	O
.	O
for	O
example	O
,	O
if	O
we	O
chose	O
the	O
49th	O
and	O
51st	O
percentiles	O
,	O
we	O
would	O
get	O
very	O
little	O
information	O
about	O
spread	O
,	O
so	O
it	O
would	O
leave	O
the	O
estimate	O
of	O
σ	O
relatively	O
unconstrained	O
by	O
the	O
data	O
.	O
all	O
values	O
of	O
sigma	O
would	O
have	O
nearly	O
the	O
same	O
likelihood	B
of	O
producing	O
the	O
observed	O
values	O
,	O
so	O
the	O
posterior	B
distribution	I
of	O
sigma	O
would	O
look	O
a	O
lot	O
like	O
the	O
prior	B
.	O
10.10	O
who	O
is	O
more	O
variable	O
?	O
finally	O
we	O
are	O
ready	O
to	O
answer	O
the	O
question	O
we	O
started	O
with	O
:	O
is	O
the	O
coefﬁ-	O
cient	O
of	O
variation	O
greater	O
for	O
men	O
than	O
for	O
women	O
?	O
0.04050.04100.04150.04200.04250.04300.0435coefficient	O
of	O
variation0.00.20.40.60.81.0probabilitymalefemale	O
10.11.	O
discussion	O
123	O
using	O
abc	O
based	O
on	O
the	O
median	B
and	O
ipr	O
with	O
num_sigmas=1	O
,	O
i	O
computed	O
posterior	B
joint	O
distributions	O
for	O
mu	O
and	O
sigma	O
.	O
figures	O
10.1	O
and	O
10.2	O
show	O
the	O
results	O
as	O
a	O
contour	O
plot	O
with	O
mu	O
on	O
the	O
x-axis	O
,	O
sigma	O
on	O
the	O
y-axis	O
,	O
and	O
probability	B
on	O
the	O
z-axis	O
.	O
for	O
each	O
joint	B
distribution	I
,	O
i	O
computed	O
the	O
posterior	B
distribution	I
of	O
cv	O
.	O
fig-	O
ure	O
10.3	O
shows	O
these	O
distributions	O
for	O
men	O
and	O
women	O
.	O
the	O
mean	O
for	O
men	O
is	O
0.0410	O
;	O
for	O
women	O
it	O
is	O
0.0429.	O
since	O
there	O
is	O
no	O
overlap	O
between	O
the	O
dis-	O
tributions	O
,	O
we	O
conclude	O
with	O
near	O
certainty	O
that	O
women	O
are	O
more	O
variable	O
in	O
height	B
than	O
men	O
.	O
so	O
is	O
that	O
the	O
end	O
of	O
the	O
variability	O
hypothesis	O
?	O
it	O
turns	O
out	O
that	O
this	O
result	O
depends	O
on	O
the	O
choice	O
of	O
the	O
inter-percentile	O
range	O
.	O
with	O
num_sigmas=1	O
,	O
we	O
conclude	O
that	O
women	O
are	O
more	O
variable	O
,	O
but	O
with	O
num_sigmas=2	O
we	O
conclude	O
with	O
equal	O
conﬁdence	O
that	O
men	O
are	O
more	O
vari-	O
able	O
.	O
sadly	O
,	O
no	O
.	O
the	O
reason	O
for	O
the	O
difference	O
is	O
that	O
there	O
are	O
more	O
men	O
of	O
short	O
stature	O
,	O
and	O
their	O
distance	O
from	O
the	O
mean	O
is	O
greater	O
.	O
so	O
our	O
evaluation	O
of	O
the	O
variability	O
hypothesis	O
depends	O
on	O
the	O
interpreta-	O
tion	O
of	O
“	O
variability.	O
”	O
with	O
num_sigmas=1	O
we	O
focus	O
on	O
people	O
near	O
the	O
mean	O
.	O
as	O
we	O
increase	O
num_sigmas	O
,	O
we	O
give	O
more	O
weight	O
to	O
the	O
extremes	O
.	O
to	O
decide	O
which	O
emphasis	O
is	O
appropriate	O
,	O
we	O
would	O
need	O
a	O
more	O
precise	O
statement	O
of	O
the	O
hypothesis	O
.	O
as	O
it	O
is	O
,	O
the	O
variability	O
hypothesis	O
may	O
be	O
too	O
vague	O
to	O
evaluate	O
.	O
nevertheless	O
,	O
it	O
helped	O
me	O
demonstrate	O
several	O
new	O
ideas	O
and	O
,	O
i	O
hope	O
you	O
agree	O
,	O
it	O
makes	O
an	O
interesting	O
example	O
.	O
10.11	O
discussion	O
there	O
are	O
two	O
ways	O
you	O
might	O
think	O
of	O
abc	O
.	O
one	O
interpretation	O
is	O
that	O
it	O
is	O
,	O
as	O
the	O
name	O
suggests	O
,	O
an	O
approximation	O
that	O
is	O
faster	O
to	O
compute	O
than	O
the	O
exact	O
value	O
.	O
but	O
remember	O
that	O
bayesian	O
analysis	O
is	O
always	O
based	O
on	O
modeling	B
deci-	O
sions	O
,	O
which	O
implies	O
that	O
there	O
is	O
no	O
“	O
exact	O
”	O
solution	O
.	O
for	O
any	O
interesting	O
physical	O
system	O
there	O
are	O
many	O
possible	O
models	O
,	O
and	O
each	O
model	O
yields	O
different	O
results	O
.	O
to	O
interpret	O
the	O
results	O
,	O
we	O
have	O
to	O
evaluate	O
the	O
models	O
.	O
124	O
chapter	O
10.	O
approximate	O
bayesian	O
computation	O
so	O
another	O
interpretation	O
of	O
abc	O
is	O
that	O
it	O
represents	O
an	O
alternative	O
model	O
of	O
the	O
likelihood	B
.	O
when	O
we	O
compute	O
p	O
(	O
d|h	O
)	O
,	O
we	O
are	O
asking	O
“	O
what	O
is	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
a	O
given	O
hypothesis	O
?	O
”	O
for	O
large	O
datasets	O
,	O
the	O
likelihood	B
of	O
the	O
data	O
is	O
very	O
small	O
,	O
which	O
is	O
a	O
hint	O
that	O
we	O
might	O
not	O
be	O
asking	O
the	O
right	O
question	O
.	O
what	O
we	O
really	O
want	O
to	O
know	O
is	O
the	O
likelihood	B
of	O
any	O
outcome	O
like	O
the	O
data	O
,	O
where	O
the	O
deﬁnition	O
of	O
“	O
like	O
”	O
is	O
yet	O
another	O
modeling	B
decision	O
.	O
the	O
underlying	O
idea	O
of	O
abc	O
is	O
that	O
two	O
datasets	O
are	O
alike	O
if	O
they	O
yield	O
the	O
same	O
summary	O
statistics	O
.	O
but	O
in	O
some	O
cases	O
,	O
like	O
the	O
example	O
in	O
this	O
chapter	O
,	O
it	O
is	O
not	O
obvious	O
which	O
summary	O
statistics	O
to	O
choose	O
.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
variability.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
10.12	O
exercises	O
exercise	O
10.1.	O
an	O
“	O
effect	O
size	O
”	O
is	O
a	O
statistic	O
intended	O
to	O
measure	O
the	O
difference	O
between	O
two	O
groups	O
(	O
see	O
http	O
:	O
//	O
en	O
.	O
wikipedia	O
.	O
org/	O
wiki/	O
effect_	O
size	O
)	O
.	O
for	O
example	O
,	O
we	O
could	O
use	O
data	O
from	O
the	O
brfss	O
to	O
estimate	O
the	O
difference	O
in	O
height	B
between	O
men	O
and	O
women	O
.	O
by	O
sampling	O
values	O
from	O
the	O
posterior	B
distributions	O
of	O
µ	O
and	O
σ	O
,	O
we	O
could	O
generate	O
the	O
posterior	B
distribution	I
of	O
this	O
difference	O
.	O
but	O
it	O
might	O
be	O
better	O
to	O
use	O
a	O
dimensionless	O
measure	O
of	O
effect	O
size	O
,	O
rather	O
than	O
a	O
difference	O
measured	O
in	O
cm	O
.	O
one	O
option	O
is	O
to	O
use	O
divide	O
through	O
by	O
the	O
standard	O
deviation	O
(	O
similar	O
to	O
what	O
we	O
did	O
with	O
the	O
coefﬁcient	B
of	I
variation	I
)	O
.	O
if	O
the	O
parameters	O
for	O
group	O
1	O
are	O
(	O
µ1	O
,	O
σ1	O
)	O
,	O
and	O
the	O
parameters	O
for	O
group	O
2	O
are	O
(	O
µ2	O
,	O
σ2	O
)	O
,	O
the	O
dimensionless	O
effect	O
size	O
is	O
µ1	O
−	O
µ2	O
(	O
σ1	O
+	O
σ2	O
)	O
/2	O
write	O
a	O
function	O
that	O
takes	O
joint	O
distributions	O
of	O
mu	O
and	O
sigma	O
for	O
two	O
groups	O
and	O
returns	O
the	O
posterior	B
distribution	I
of	O
effect	O
size	O
.	O
hint	O
:	O
if	O
enumerating	O
all	O
pairs	O
from	O
the	O
two	O
distributions	O
takes	O
too	O
long	O
,	O
consider	O
random	O
sampling	O
.	O
chapter	O
11	O
hypothesis	B
testing	I
11.1	O
back	O
to	O
the	O
euro	O
problem	O
in	O
section	O
4.1	O
i	O
presented	O
a	O
problem	O
from	O
mackay	O
’	O
s	O
information	O
theory	O
,	O
in-	O
ference	O
,	O
and	O
learning	O
algorithms	O
:	O
a	O
statistical	O
statement	O
appeared	O
in	O
“	O
the	O
guardian	O
''	O
on	O
friday	O
january	O
4	O
,	O
2002	O
:	O
when	O
spun	O
on	O
edge	O
250	O
times	O
,	O
a	O
belgian	O
one-euro	O
coin	O
came	O
up	O
heads	O
140	O
times	O
and	O
tails	O
110	O
.	O
‘	O
it	O
looks	O
very	O
suspicious	O
to	O
me	O
,	O
’	O
said	O
barry	O
blight	O
,	O
a	O
statistics	O
lecturer	O
at	O
the	O
london	O
school	O
of	O
economics	O
.	O
‘	O
if	O
the	O
coin	O
were	O
unbiased	O
,	O
the	O
chance	O
of	O
getting	O
a	O
result	O
as	O
extreme	O
as	O
that	O
would	O
be	O
less	O
than	O
7	O
%	O
.	O
’	O
but	O
do	O
these	O
data	O
give	O
evidence	B
that	O
the	O
coin	O
is	O
biased	O
rather	O
than	O
fair	O
?	O
we	O
estimated	O
the	O
probability	B
that	O
the	O
coin	O
would	O
land	O
face	O
up	O
,	O
but	O
we	O
didn	O
’	O
t	O
really	O
answer	O
mackay	O
’	O
s	O
question	O
:	O
do	O
the	O
data	O
give	O
evidence	B
that	O
the	O
coin	O
is	O
biased	O
?	O
in	O
chapter	O
4	O
i	O
proposed	O
that	O
data	O
are	O
in	O
favor	O
of	O
a	O
hypothesis	O
if	O
the	O
data	O
are	O
more	O
likely	O
under	O
the	O
hypothesis	O
than	O
under	O
the	O
alternative	O
or	O
,	O
equivalently	O
,	O
if	O
the	O
bayes	O
factor	O
is	O
greater	O
than	O
1.	O
in	O
the	O
euro	O
example	O
,	O
we	O
have	O
two	O
hypotheses	O
to	O
consider	O
:	O
i	O
’	O
ll	O
use	O
f	O
for	O
the	O
hypothesis	O
that	O
the	O
coin	O
is	O
fair	O
and	O
b	O
for	O
the	O
hypothesis	O
that	O
it	O
is	O
biased	O
.	O
if	O
the	O
coin	O
is	O
fair	O
,	O
it	O
is	O
easy	O
to	O
compute	O
the	O
likelihood	B
of	O
the	O
data	O
,	O
p	O
(	O
d|f	O
)	O
.	O
in	O
fact	O
,	O
we	O
already	O
wrote	O
the	O
function	O
that	O
does	O
it	O
.	O
126	O
chapter	O
11.	O
hypothesis	B
testing	I
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
x	O
=	O
hypo	O
/	O
100.0	O
head	O
,	O
tails	O
=	O
data	O
like	O
=	O
x**heads	O
*	O
(	O
1-x	O
)	O
**tails	O
return	O
like	O
to	O
use	O
it	O
we	O
can	O
create	O
a	O
euro	O
suite	B
and	O
invoke	O
likelihood	B
:	O
suite	B
=	O
euro	O
(	O
)	O
likelihood	B
=	O
suite.likelihood	O
(	O
data	O
,	O
50	O
)	O
p	O
(	O
d|f	O
)	O
is	O
5.5	O
·	O
10−76	O
,	O
which	O
doesn	O
’	O
t	O
tell	O
us	O
much	O
except	O
that	O
the	O
probability	B
of	O
seeing	O
any	O
particular	O
dataset	O
is	O
very	O
small	O
.	O
it	O
takes	O
two	O
likelihoods	O
to	O
make	O
a	O
ratio	O
,	O
so	O
we	O
also	O
have	O
to	O
compute	O
p	O
(	O
d|b	O
)	O
.	O
it	O
is	O
not	O
obvious	O
how	O
to	O
compute	O
the	O
likelihood	B
of	O
b	O
,	O
because	O
it	O
’	O
s	O
not	O
obvi-	O
ous	O
what	O
“	O
biased	O
”	O
means	O
.	O
one	O
possibility	O
is	O
to	O
cheat	O
and	O
look	O
at	O
the	O
data	O
before	O
we	O
deﬁne	O
the	O
hypoth-	O
esis	O
.	O
in	O
that	O
case	O
we	O
would	O
say	O
that	O
“	O
biased	O
”	O
means	O
that	O
the	O
probability	B
of	O
heads	O
is	O
140/250	O
.	O
actual_percent	O
=	O
100.0	O
*	O
140	O
/	O
250	O
likelihood	B
=	O
suite.likelihood	O
(	O
data	O
,	O
actual_percent	O
)	O
this	O
version	O
of	O
b	O
i	O
call	O
b_cheat	O
;	O
the	O
likelihood	B
of	O
b_cheat	O
is	O
34	O
·	O
10−76	O
and	O
the	O
likelihood	B
ratio	I
is	O
6.1.	O
so	O
we	O
would	O
say	O
that	O
the	O
data	O
are	O
evidence	B
in	O
favor	O
of	O
this	O
version	O
of	O
b.	O
but	O
using	O
the	O
data	O
to	O
formulate	O
the	O
hypothesis	O
is	O
obviously	O
bogus	B
.	O
by	O
that	O
deﬁnition	O
,	O
any	O
dataset	O
would	O
be	O
evidence	B
in	O
favor	O
of	O
b	O
,	O
unless	O
the	O
observed	O
percentage	O
of	O
heads	O
is	O
exactly	O
50	O
%	O
.	O
11.2	O
making	O
a	O
fair	O
comparison	O
to	O
make	O
a	O
legitimate	O
comparison	O
,	O
we	O
have	O
to	O
deﬁne	O
b	O
without	O
looking	O
at	O
the	O
data	O
.	O
so	O
let	O
’	O
s	O
try	O
a	O
different	O
deﬁnition	O
.	O
if	O
you	O
inspect	O
a	O
belgian	O
euro	O
coin	O
,	O
you	O
might	O
notice	O
that	O
the	O
“	O
heads	O
”	O
side	O
is	O
more	O
prominent	O
than	O
the	O
“	O
tails	O
”	O
side	O
.	O
you	O
might	O
expect	O
the	O
shape	O
to	O
have	O
some	O
effect	O
on	O
x	O
,	O
but	O
be	O
unsure	O
whether	O
it	O
makes	O
heads	O
more	O
or	O
less	O
likely	O
.	O
so	O
you	O
might	O
say	O
“	O
i	O
think	O
the	O
coin	O
is	O
biased	O
so	O
that	O
x	O
is	O
either	O
0.6	O
or	O
0.4	O
,	O
but	O
i	O
am	O
not	O
sure	O
which.	O
”	O
we	O
can	O
think	O
of	O
this	O
version	O
,	O
which	O
i	O
’	O
ll	O
call	O
b_two	O
as	O
a	O
hypothesis	O
made	O
up	O
of	O
two	O
sub-hypotheses	O
.	O
we	O
can	O
compute	O
the	O
likelihood	B
for	O
each	O
sub-	O
hypothesis	O
and	O
then	O
compute	O
the	O
average	O
likelihood	B
.	O
11.2.	O
making	O
a	O
fair	O
comparison	O
127	O
like40	O
=	O
suite.likelihood	O
(	O
data	O
,	O
40	O
)	O
like60	O
=	O
suite.likelihood	O
(	O
data	O
,	O
60	O
)	O
likelihood	B
=	O
0.5	O
*	O
like40	O
+	O
0.5	O
*	O
like60	O
the	O
likelihood	B
ratio	I
(	O
or	O
bayes	O
factor	O
)	O
for	O
b_two	O
is	O
1.3	O
,	O
which	O
means	O
the	O
data	O
provide	O
weak	O
evidence	B
in	O
favor	O
of	O
b_two	O
.	O
more	O
generally	O
,	O
suppose	O
you	O
suspect	O
that	O
the	O
coin	O
is	O
biased	O
,	O
but	O
you	O
have	O
no	O
clue	O
about	O
the	O
value	O
of	O
x.	O
in	O
that	O
case	O
you	O
might	O
build	O
a	O
suite	B
,	O
which	O
i	O
call	O
b_uniform	O
,	O
to	O
represent	O
sub-hypotheses	O
from	O
0	O
to	O
100.	O
b_uniform	O
=	O
euro	O
(	O
xrange	O
(	O
0	O
,	O
101	O
)	O
)	O
b_uniform.remove	O
(	O
50	O
)	O
b_uniform.normalize	O
(	O
)	O
i	O
initialize	O
b_uniform	O
with	O
values	O
from	O
0	O
to	O
100.	O
i	O
removed	O
the	O
sub-	O
hypothesis	O
that	O
x	O
is	O
50	O
%	O
,	O
because	O
if	O
x	O
is	O
50	O
%	O
the	O
coin	O
is	O
fair	O
,	O
but	O
it	O
has	O
almost	O
no	O
effect	O
on	O
the	O
result	O
whether	O
you	O
remove	O
it	O
or	O
not	O
.	O
to	O
compute	O
the	O
likelihood	B
of	O
b_uniform	O
we	O
compute	O
the	O
likelihood	B
of	O
each	O
sub-hypothesis	O
and	O
accumulate	O
a	O
weighted	O
average	O
.	O
def	O
suitelikelihood	O
(	O
suite	B
,	O
data	O
)	O
:	O
total	O
=	O
0	O
for	O
hypo	O
,	O
prob	O
in	O
suite.items	O
(	O
)	O
:	O
like	O
=	O
suite.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
total	O
+=	O
prob	O
*	O
like	O
return	O
total	O
the	O
likelihood	B
ratio	I
for	O
b_uniform	O
is	O
0.47	O
,	O
which	O
means	O
that	O
the	O
data	O
are	O
weak	O
evidence	B
against	O
b_uniform	O
,	O
compared	O
to	O
f.	O
if	O
you	O
think	O
about	O
the	O
computation	O
performed	O
by	O
suitelikelihood	O
,	O
you	O
might	O
notice	O
that	O
it	O
is	O
similar	O
to	O
an	O
update	O
.	O
to	O
refresh	O
your	O
memory	O
,	O
here	O
’	O
s	O
the	O
update	O
function	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
like	O
=	O
self.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
self.mult	O
(	O
hypo	O
,	O
like	O
)	O
return	O
self.normalize	O
(	O
)	O
and	O
here	O
’	O
s	O
normalize	B
:	O
def	O
normalize	B
(	O
self	O
)	O
:	O
total	O
=	O
self.total	O
(	O
)	O
factor	O
=	O
1.0	O
/	O
total	O
128	O
chapter	O
11.	O
hypothesis	B
testing	I
for	O
x	O
in	O
self.d	O
:	O
self.d	O
[	O
x	O
]	O
*=	O
factor	O
return	O
total	O
the	O
return	O
value	O
from	O
normalize	B
is	O
the	O
total	O
of	O
the	O
probabilities	O
in	O
the	O
suite	B
,	O
which	O
is	O
the	O
average	O
of	O
the	O
likelihoods	O
for	O
the	O
sub-hypotheses	O
,	O
weighted	O
by	O
the	O
prior	B
probabilities	O
.	O
and	O
update	O
passes	O
this	O
value	O
along	O
,	O
so	O
instead	O
of	O
using	O
suitelikelihood	O
,	O
we	O
could	O
compute	O
the	O
likelihood	B
of	O
b_uniform	O
like	O
this	O
:	O
likelihood	B
=	O
b_uniform.update	O
(	O
data	O
)	O
11.3	O
the	O
triangle	O
prior	O
in	O
chapter	O
4	O
we	O
also	O
considered	O
a	O
triangle-shaped	O
prior	B
that	O
gives	O
higher	O
probability	B
to	O
values	O
of	O
x	O
near	O
50	O
%	O
.	O
if	O
we	O
think	O
of	O
this	O
prior	B
as	O
a	O
suite	B
of	O
sub-hypotheses	O
,	O
we	O
can	O
compute	O
its	O
likelihood	B
like	O
this	O
:	O
b_triangle	O
=	O
triangleprior	O
(	O
)	O
likelihood	B
=	O
b_triangle.update	O
(	O
data	O
)	O
the	O
likelihood	B
ratio	I
for	O
b_triangle	O
is	O
0.84	O
,	O
compared	O
to	O
f	O
,	O
so	O
again	O
we	O
would	O
say	O
that	O
the	O
data	O
are	O
weak	O
evidence	B
against	O
b.	O
the	O
following	O
table	O
shows	O
the	O
priors	O
we	O
have	O
considered	O
,	O
the	O
likelihood	B
of	O
each	O
,	O
and	O
the	O
likelihood	B
ratio	I
(	O
or	O
bayes	O
factor	O
)	O
relative	O
to	O
f.	O
hypothesis	O
likelihood	O
bayes	O
×10−76	O
factor	O
–	O
6.1	O
1.3	O
0.47	O
0.84	O
f	O
b_cheat	O
b_two	O
b_uniform	O
b_triangle	O
5.5	O
34	O
7.4	O
2.6	O
4.6	O
depending	O
on	O
which	O
deﬁnition	O
we	O
choose	O
,	O
the	O
data	O
might	O
provide	O
evidence	B
for	O
or	O
against	O
the	O
hypothesis	O
that	O
the	O
coin	O
is	O
biased	O
,	O
but	O
in	O
either	O
case	O
it	O
is	O
relatively	O
weak	O
evidence	B
.	O
in	O
summary	O
,	O
we	O
can	O
use	O
bayesian	O
hypothesis	B
testing	I
to	O
compare	O
the	O
likeli-	O
hood	O
of	O
f	O
and	O
b	O
,	O
but	O
we	O
have	O
to	O
do	O
some	O
work	O
to	O
specify	O
precisely	O
what	O
b	O
means	O
.	O
this	O
speciﬁcation	O
depends	O
on	O
background	O
information	O
about	O
coins	O
and	O
their	O
behavior	O
when	O
spun	O
,	O
so	O
people	O
could	O
reasonably	O
disagree	O
about	O
the	O
right	O
deﬁnition	O
.	O
11.4.	O
discussion	O
129	O
my	O
presentation	O
of	O
this	O
example	O
follows	O
david	O
mackay	O
’	O
s	O
discussion	O
,	O
and	O
comes	O
to	O
the	O
same	O
conclusion	O
.	O
you	O
can	O
download	O
the	O
code	O
i	O
used	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/euro3.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
11.4	O
discussion	O
the	O
bayes	O
factor	O
for	O
b_uniform	O
is	O
0.47	O
,	O
which	O
means	O
that	O
the	O
data	O
provide	O
evidence	B
against	O
this	O
hypothesis	O
,	O
compared	O
to	O
f.	O
in	O
the	O
previous	O
section	O
i	O
characterized	O
this	O
evidence	B
as	O
“	O
weak	O
,	O
”	O
but	O
didn	O
’	O
t	O
say	O
why	O
.	O
part	O
of	O
the	O
answer	O
is	O
historical	O
.	O
harold	O
jeffreys	O
,	O
an	O
early	O
proponent	O
of	O
bayesian	O
statistics	O
,	O
suggested	O
a	O
scale	O
for	O
interpreting	O
bayes	O
factors	O
:	O
strength	O
bayes	O
factor	O
1	O
–	O
3	O
3	O
–	O
10	O
10	O
–	O
30	O
30	O
–	O
100	O
very	O
strong	O
>	O
100	O
barely	O
worth	O
mentioning	O
substantial	O
strong	O
decisive	O
in	O
the	O
example	O
,	O
the	O
bayes	O
factor	O
is	O
0.47	O
in	O
favor	O
of	O
b_uniform	O
,	O
so	O
it	O
is	O
2.1	O
in	O
favor	O
of	O
f	O
,	O
which	O
jeffreys	O
would	O
consider	O
“	O
barely	O
worth	O
mentioning.	O
”	O
other	O
authors	O
have	O
suggested	O
variations	O
on	O
the	O
wording	O
.	O
to	O
avoid	O
arguing	O
about	O
adjectives	O
,	O
we	O
could	O
think	O
about	O
odds	B
instead	O
.	O
if	O
your	O
prior	B
odds	O
are	O
1:1	O
,	O
and	O
you	O
see	O
evidence	B
with	O
bayes	O
factor	O
2	O
,	O
your	O
posterior	B
odds	O
are	O
2:1.	O
in	O
terms	O
of	O
probability	B
,	O
the	O
data	O
changed	O
your	O
degree	B
of	I
belief	I
from	O
50	O
%	O
to	O
66	O
%	O
.	O
for	O
most	O
real	O
world	O
problems	O
,	O
that	O
change	O
would	O
be	O
small	O
relative	O
to	O
modeling	B
errors	O
and	O
other	O
sources	O
of	O
uncertainty	B
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
you	O
had	O
seen	O
evidence	B
with	O
bayes	O
factor	O
100	O
,	O
your	O
posterior	B
odds	O
would	O
be	O
100:1	O
or	O
more	O
than	O
99	O
%	O
.	O
whether	O
or	O
not	O
you	O
agree	O
that	O
such	O
evidence	B
is	O
“	O
decisive	O
,	O
”	O
it	O
is	O
certainly	O
strong	O
.	O
11.5	O
exercises	O
exercise	O
11.1.	O
some	O
people	O
believe	O
in	O
the	O
existence	O
of	O
extra-sensory	B
perception	I
(	O
esp	O
)	O
;	O
for	O
example	O
,	O
the	O
ability	O
of	O
some	O
people	O
to	O
guess	O
the	O
value	O
of	O
an	O
unseen	O
play-	O
ing	O
card	O
with	O
probability	B
better	O
than	O
chance	O
.	O
130	O
chapter	O
11.	O
hypothesis	B
testing	I
what	O
is	O
your	O
prior	B
degree	O
of	O
belief	O
in	O
this	O
kind	O
of	O
esp	O
?	O
do	O
you	O
think	O
it	O
is	O
as	O
likely	O
to	O
exist	O
as	O
not	O
?	O
or	O
are	O
you	O
more	O
skeptical	O
about	O
it	O
?	O
write	O
down	O
your	O
prior	B
odds	O
.	O
now	O
compute	O
the	O
strength	O
of	O
the	O
evidence	B
it	O
would	O
take	O
to	O
convince	O
you	O
that	O
esp	O
is	O
at	O
least	O
50	O
%	O
likely	O
to	O
exist	O
.	O
what	O
bayes	O
factor	O
would	O
be	O
needed	O
to	O
make	O
you	O
90	O
%	O
sure	O
that	O
esp	O
exists	O
?	O
exercise	O
11.2.	O
suppose	O
that	O
your	O
answer	O
to	O
the	O
previous	O
question	O
is	O
1000	O
;	O
that	O
is	O
,	O
evidence	B
with	O
bayes	O
factor	O
1000	O
in	O
favor	O
of	O
esp	O
would	O
be	O
sufﬁcient	O
to	O
change	O
your	O
mind	O
.	O
now	O
suppose	O
that	O
you	O
read	O
a	O
paper	O
in	O
a	O
respectable	O
peer-reviewed	O
scientiﬁc	O
journal	O
that	O
presents	O
evidence	B
with	O
bayes	O
factor	O
1000	O
in	O
favor	O
of	O
esp	O
.	O
would	O
that	O
change	O
your	O
mind	O
?	O
if	O
not	O
,	O
how	O
do	O
you	O
resolve	O
the	O
apparent	O
contradiction	O
?	O
you	O
might	O
ﬁnd	O
it	O
helpful	O
to	O
read	O
about	O
david	O
hume	O
’	O
s	O
article	O
,	O
“	O
of	O
miracles	O
,	O
”	O
at	O
http	O
:	O
//	O
en	O
.	O
wikipedia	O
.	O
org/	O
wiki/	O
of_	O
miracles	O
.	O
chapter	O
12	O
evidence	B
12.1	O
interpreting	O
sat	O
scores	O
suppose	O
you	O
are	O
the	O
dean	O
of	O
admission	O
at	O
a	O
small	O
engineering	O
college	O
in	O
massachusetts	O
,	O
and	O
you	O
are	O
considering	O
two	O
candidates	O
,	O
alice	O
and	O
bob	O
,	O
whose	O
qualiﬁcations	O
are	O
similar	O
in	O
many	O
ways	O
,	O
with	O
the	O
exception	B
that	O
al-	O
ice	O
got	O
a	O
higher	O
score	O
on	O
the	O
math	O
portion	O
of	O
the	O
sat	O
,	O
a	O
standardized	B
test	I
intended	O
to	O
measure	O
preparation	O
for	O
college-level	O
work	O
in	O
mathematics	O
.	O
if	O
alice	O
got	O
780	O
and	O
bob	O
got	O
a	O
740	O
(	O
out	O
of	O
a	O
possible	O
800	O
)	O
,	O
you	O
might	O
want	O
to	O
know	O
whether	O
that	O
difference	O
is	O
evidence	B
that	O
alice	O
is	O
better	O
prepared	O
than	O
bob	O
,	O
and	O
what	O
the	O
strength	O
of	O
that	O
evidence	B
is	O
.	O
now	O
in	O
reality	O
,	O
both	O
scores	O
are	O
very	O
good	O
,	O
and	O
both	O
candidates	O
are	O
probably	O
well	O
prepared	O
for	O
college	O
math	O
.	O
so	O
the	O
real	O
dean	O
of	O
admission	O
would	O
prob-	O
ably	O
suggest	O
that	O
we	O
choose	O
the	O
candidate	O
who	O
best	O
demonstrates	O
the	O
other	O
skills	O
and	O
attitudes	O
we	O
look	O
for	O
in	O
students	O
.	O
but	O
as	O
an	O
example	O
of	O
bayesian	O
hypothesis	B
testing	I
,	O
let	O
’	O
s	O
stick	B
with	O
a	O
narrower	O
question	O
:	O
“	O
how	O
strong	O
is	O
the	O
evidence	B
that	O
alice	O
is	O
better	O
prepared	O
than	O
bob	O
?	O
”	O
to	O
answer	O
that	O
question	O
,	O
we	O
need	O
to	O
make	O
some	O
modeling	B
decisions	O
.	O
i	O
’	O
ll	O
start	O
with	O
a	O
simpliﬁcation	O
i	O
know	O
is	O
wrong	O
;	O
then	O
we	O
’	O
ll	O
come	O
back	O
and	O
im-	O
prove	O
the	O
model	O
.	O
i	O
pretend	O
,	O
temporarily	O
,	O
that	O
all	O
sat	O
questions	O
are	O
equally	O
difﬁcult	O
.	O
actually	O
,	O
the	O
designers	O
of	O
the	O
sat	O
choose	O
questions	O
with	O
a	O
range	O
of	O
difﬁculty	O
,	O
because	O
that	O
improves	O
the	O
ability	O
to	O
measure	O
statistical	O
differ-	O
ences	O
between	O
test-takers	O
.	O
but	O
if	O
we	O
choose	O
a	O
model	O
where	O
all	O
questions	O
are	O
equally	O
difﬁcult	O
,	O
we	O
can	O
deﬁne	O
a	O
characteristic	O
,	O
p_correct	O
,	O
for	O
each	O
test-taker	O
,	O
which	O
is	O
the	O
probabil-	O
132	O
chapter	O
12.	O
evidence	B
ity	O
of	O
answering	O
any	O
question	O
correctly	O
.	O
this	O
simpliﬁcation	O
makes	O
it	O
easy	O
to	O
compute	O
the	O
likelihood	B
of	O
a	O
given	O
score	O
.	O
12.2	O
the	O
scale	O
in	O
order	O
to	O
understand	O
sat	O
scores	O
,	O
we	O
have	O
to	O
understand	O
the	O
scoring	O
and	O
scaling	O
process	B
.	O
each	O
test-taker	O
gets	O
a	O
raw	B
score	I
based	O
on	O
the	O
number	O
of	O
correct	O
and	O
incorrect	O
questions	O
.	O
the	O
raw	B
score	I
is	O
converted	O
to	O
a	O
scaled	B
score	I
in	O
the	O
range	O
200–800	O
.	O
in	O
2009	O
,	O
there	O
were	O
54	O
questions	O
on	O
the	O
math	O
sat	O
.	O
the	O
raw	B
score	I
for	O
each	O
test-taker	O
is	O
the	O
number	O
of	O
questions	O
answered	O
correctly	O
minus	O
a	O
penalty	O
of	O
1/4	O
point	O
for	O
each	O
question	O
answered	O
incorrectly	O
.	O
the	O
college	O
board	O
,	O
which	O
administers	O
the	O
sat	O
,	O
publishes	O
the	O
map	O
from	O
raw	O
scores	O
to	O
scaled	O
scores	O
.	O
i	O
have	O
downloaded	O
that	O
data	O
and	O
wrapped	O
it	O
in	O
an	O
interpolator	O
object	O
that	O
provides	O
a	O
forward	O
lookup	O
(	O
from	O
raw	B
score	I
to	O
scaled	O
)	O
and	O
a	O
reverse	O
lookup	O
(	O
from	O
scaled	B
score	I
to	O
raw	O
)	O
.	O
you	O
can	O
download	O
the	O
code	O
for	O
this	O
example	O
from	O
http	O
:	O
//thinkbayes.com/	O
sat.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
12.3	O
the	O
prior	B
the	O
college	O
board	O
also	O
publishes	O
the	O
distribution	B
of	O
scaled	O
scores	O
for	O
all	O
test-takers	O
.	O
if	O
we	O
convert	O
each	O
scaled	B
score	I
to	O
a	O
raw	B
score	I
,	O
and	O
divide	O
by	O
the	O
number	O
of	O
questions	O
,	O
the	O
result	O
is	O
an	O
estimate	O
of	O
p_correct	O
.	O
so	O
we	O
can	O
use	O
the	O
distribution	B
of	O
raw	O
scores	O
to	O
model	O
the	O
prior	B
distribution	I
of	O
p_correct	O
.	O
here	O
is	O
the	O
code	O
that	O
reads	O
and	O
processes	O
the	O
data	O
:	O
class	O
exam	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
)	O
:	O
self.scale	O
=	O
readscale	O
(	O
)	O
scores	O
=	O
readranks	O
(	O
)	O
score_pmf	O
=	O
thinkbayes.makepmffromdict	O
(	O
dict	O
(	O
scores	O
)	O
)	O
self.raw	O
=	O
self.reversescale	O
(	O
score_pmf	O
)	O
self.max_score	O
=	O
max	O
(	O
self.raw.values	O
(	O
)	O
)	O
self.prior	O
=	O
dividevalues	O
(	O
self.raw	O
,	O
self.max_score	O
)	O
12.3.	O
the	O
prior	B
133	O
figure	O
12.1	O
:	O
prior	B
distribution	I
of	O
p_correct	O
for	O
sat	O
test-takers	O
.	O
exam	O
encapsulates	O
the	O
information	O
we	O
have	O
about	O
the	O
exam	O
.	O
readscale	O
and	O
readranks	O
read	O
ﬁles	O
and	O
return	O
objects	O
that	O
contain	O
the	O
data	O
:	O
self.scale	O
is	O
the	O
interpolator	O
that	O
converts	O
from	O
raw	O
to	O
scaled	O
scores	O
and	O
back	O
;	O
scores	O
is	O
a	O
list	O
of	O
(	O
score	O
,	O
frequency	O
)	O
pairs	O
.	O
score_pmf	O
is	O
the	O
pmf	O
of	O
scaled	O
scores	O
.	O
self.raw	O
is	O
the	O
pmf	O
of	O
raw	O
scores	O
,	O
and	O
self.prior	O
is	O
the	O
pmf	O
of	O
p_correct	O
.	O
figure	O
12.1	O
shows	O
the	O
prior	B
distribution	I
of	O
p_correct	O
.	O
this	O
distribution	B
is	O
approximately	O
gaussian	O
,	O
but	O
it	O
is	O
compressed	O
at	O
the	O
extremes	O
.	O
by	O
design	O
,	O
the	O
sat	O
has	O
the	O
most	O
power	O
to	O
discriminate	O
between	O
test-takers	O
within	O
two	O
standard	O
deviations	O
of	O
the	O
mean	O
,	O
and	O
less	O
power	O
outside	O
that	O
range	O
.	O
for	O
each	O
test-taker	O
,	O
i	O
deﬁne	O
a	O
suite	B
called	O
sat	O
that	O
represents	O
the	O
distribution	B
of	O
p_correct	O
.	O
here	O
’	O
s	O
the	O
deﬁnition	O
:	O
class	O
sat	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
exam	O
,	O
score	O
)	O
:	O
thinkbayes.suite.__init__	O
(	O
self	O
)	O
self.exam	O
=	O
exam	O
self.score	O
=	O
score	O
#	O
start	O
with	O
the	O
prior	B
distribution	I
for	O
p_correct	O
,	O
prob	O
in	O
exam.prior.items	O
(	O
)	O
:	O
self.set	O
(	O
p_correct	O
,	O
prob	O
)	O
0.00.20.40.60.81.0p_correct0.00.20.40.60.81.0cdfprior	O
134	O
chapter	O
12.	O
evidence	B
figure	O
12.2	O
:	O
posterior	B
distributions	O
of	O
p_correct	O
for	O
alice	O
and	O
bob	O
.	O
#	O
update	O
based	O
on	O
an	O
exam	O
score	O
self.update	O
(	O
score	O
)	O
__init__	O
takes	O
an	O
exam	O
object	O
and	O
a	O
scaled	B
score	I
.	O
it	O
makes	O
a	O
copy	O
of	O
the	O
prior	B
distribution	I
and	O
then	O
updates	O
itself	O
based	O
on	O
the	O
exam	O
score	O
.	O
as	O
usual	O
,	O
we	O
inherit	O
update	O
from	O
suite	B
and	O
provide	O
likelihood	B
:	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
p_correct	O
=	O
hypo	O
score	O
=	O
data	O
k	O
=	O
self.exam.reverse	O
(	O
score	O
)	O
n	O
=	O
self.exam.max_score	O
like	O
=	O
thinkbayes.evalbinomialpmf	O
(	O
k	O
,	O
n	O
,	O
p_correct	O
)	O
return	O
like	O
hypo	O
is	O
a	O
hypothetical	O
value	O
of	O
p_correct	O
,	O
and	O
data	O
is	O
a	O
scaled	B
score	I
.	O
to	O
keep	O
things	O
simple	O
,	O
i	O
interpret	O
the	O
raw	B
score	I
as	O
the	O
number	O
of	O
correct	O
answers	O
,	O
ignoring	O
the	O
penalty	O
for	O
wrong	O
answers	O
.	O
with	O
this	O
simpliﬁcation	O
,	O
the	O
likelihood	B
is	O
given	O
by	O
the	O
binomial	B
distribution	I
,	O
which	O
computes	O
the	O
probability	B
of	O
k	O
correct	O
responses	O
out	O
of	O
n	O
questions	O
.	O
12.4	O
posterior	B
figure	O
12.2	O
shows	O
the	O
posterior	B
distributions	O
of	O
p_correct	O
for	O
alice	O
and	O
bob	O
based	O
on	O
their	O
exam	O
scores	O
.	O
we	O
can	O
see	O
that	O
they	O
overlap	O
,	O
so	O
it	O
is	O
possible	O
that	O
p_correct	O
is	O
actually	O
higher	O
for	O
bob	O
,	O
but	O
it	O
seems	O
unlikely	O
.	O
0.700.750.800.850.900.951.00p_correct0.00.20.40.60.81.0cdfposterior	O
780posterior	O
740	O
12.4.	O
posterior	B
135	O
which	O
brings	O
us	O
back	O
to	O
the	O
original	O
question	O
,	O
“	O
how	O
strong	O
is	O
the	O
evidence	B
that	O
alice	O
is	O
better	O
prepared	O
than	O
bob	O
?	O
”	O
we	O
can	O
use	O
the	O
posterior	B
distribu-	O
tions	O
of	O
p_correct	O
to	O
answer	O
this	O
question	O
.	O
to	O
formulate	O
the	O
question	O
in	O
terms	O
of	O
bayesian	O
hypothesis	B
testing	I
,	O
i	O
deﬁne	O
two	O
hypotheses	O
:	O
•	O
a	O
:	O
p_correct	O
is	O
higher	O
for	O
alice	O
than	O
for	O
bob	O
.	O
•	O
b	O
:	O
p_correct	O
is	O
higher	O
for	O
bob	O
than	O
for	O
alice	O
.	O
to	O
compute	O
the	O
likelihood	B
of	O
a	O
,	O
we	O
can	O
enumerate	O
all	O
pairs	O
of	O
values	O
from	O
the	O
posterior	B
distributions	O
and	O
add	O
up	O
the	O
total	B
probability	I
of	O
the	O
cases	O
where	O
p_correct	O
is	O
higher	O
for	O
alice	O
than	O
for	O
bob	O
.	O
and	O
we	O
already	O
have	O
a	O
function	O
,	O
thinkbayes.pmfprobgreater	O
,	O
that	O
does	O
that	O
.	O
so	O
we	O
can	O
deﬁne	O
a	O
suite	B
that	O
computes	O
the	O
posterior	B
probabilities	O
of	O
a	O
and	O
b	O
:	O
class	O
toplevel	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
a_sat	O
,	O
b_sat	O
=	O
data	O
a_like	O
=	O
thinkbayes.pmfprobgreater	O
(	O
a_sat	O
,	O
b_sat	O
)	O
b_like	O
=	O
thinkbayes.pmfprobless	O
(	O
a_sat	O
,	O
b_sat	O
)	O
c_like	O
=	O
thinkbayes.pmfprobequal	O
(	O
a_sat	O
,	O
b_sat	O
)	O
a_like	O
+=	O
c_like	O
/	O
2	O
b_like	O
+=	O
c_like	O
/	O
2	O
self.mult	O
(	O
'a	O
'	O
,	O
a_like	O
)	O
self.mult	O
(	O
'b	O
'	O
,	O
b_like	O
)	O
self.normalize	O
(	O
)	O
usually	O
when	O
we	O
deﬁne	O
a	O
new	O
suite	B
,	O
we	O
inherit	O
update	O
and	O
provide	O
likelihood	B
.	O
in	O
this	O
case	O
i	O
override	O
update	O
,	O
because	O
it	O
is	O
easier	O
to	O
evaluate	O
the	O
likelihood	B
of	O
both	O
hypotheses	O
at	O
the	O
same	O
time	O
.	O
the	O
data	O
passed	O
to	O
update	O
are	O
sat	O
objects	O
that	O
represent	O
the	O
posterior	B
distri-	O
butions	O
of	O
p_correct	O
.	O
a_like	O
is	O
the	O
total	B
probability	I
that	O
p_correct	O
is	O
higher	O
for	O
alice	O
;	O
b_like	O
is	O
that	O
probability	B
that	O
it	O
is	O
higher	O
for	O
bob	O
.	O
136	O
chapter	O
12.	O
evidence	B
c_like	O
is	O
the	O
probability	B
that	O
they	O
are	O
“	O
equal	O
,	O
”	O
but	O
this	O
equality	O
is	O
an	O
artifact	O
of	O
the	O
decision	O
to	O
model	O
p_correct	O
with	O
a	O
set	O
of	O
discrete	O
values	O
.	O
if	O
we	O
use	O
more	O
values	O
,	O
c_like	O
is	O
smaller	O
,	O
and	O
in	O
the	O
extreme	O
,	O
if	O
p_correct	O
is	O
continu-	O
ous	O
,	O
c_like	O
is	O
zero	O
.	O
so	O
i	O
treat	O
c_like	O
as	O
a	O
kind	O
of	O
round-off	O
error	B
and	O
split	O
it	O
evenly	O
between	O
a_like	O
and	O
b_like	O
.	O
here	O
is	O
the	O
code	O
that	O
creates	O
toplevel	O
and	O
updates	O
it	O
:	O
exam	O
=	O
exam	O
(	O
)	O
a_sat	O
=	O
sat	O
(	O
exam	O
,	O
780	O
)	O
b_sat	O
=	O
sat	O
(	O
exam	O
,	O
740	O
)	O
top	O
=	O
toplevel	O
(	O
'ab	O
'	O
)	O
top.update	O
(	O
(	O
a_sat	O
,	O
b_sat	O
)	O
)	O
top.print	O
(	O
)	O
the	O
likelihood	B
of	O
a	O
is	O
0.79	O
and	O
the	O
likelihood	B
of	O
b	O
is	O
0.21.	O
the	O
likelihood	B
ratio	I
(	O
or	O
bayes	O
factor	O
)	O
is	O
3.8	O
,	O
which	O
means	O
that	O
these	O
test	O
scores	O
are	O
evidence	B
that	O
alice	O
is	O
better	O
than	O
bob	O
at	O
answering	O
sat	O
questions	O
.	O
if	O
we	O
believed	O
,	O
before	O
seeing	O
the	O
test	O
scores	O
,	O
that	O
a	O
and	O
b	O
were	O
equally	O
likely	O
,	O
then	O
after	O
seeing	O
the	O
scores	O
we	O
should	O
believe	O
that	O
the	O
probability	B
of	O
a	O
is	O
79	O
%	O
,	O
which	O
means	O
there	O
is	O
still	O
a	O
21	O
%	O
chance	O
that	O
bob	O
is	O
actually	O
better	O
prepared	O
.	O
12.5	O
a	O
better	O
model	O
remember	O
that	O
the	O
analysis	O
we	O
have	O
done	O
so	O
far	O
is	O
based	O
on	O
the	O
simpliﬁca-	O
tion	O
that	O
all	O
sat	O
questions	O
are	O
equally	O
difﬁcult	O
.	O
in	O
reality	O
,	O
some	O
are	O
easier	O
than	O
others	O
,	O
which	O
means	O
that	O
the	O
difference	O
between	O
alice	O
and	O
bob	O
might	O
be	O
even	O
smaller	O
.	O
but	O
how	O
big	O
is	O
the	O
modeling	B
error	I
?	O
if	O
it	O
is	O
small	O
,	O
we	O
conclude	O
that	O
the	O
ﬁrst	O
model—based	O
on	O
the	O
simpliﬁcation	O
that	O
all	O
questions	O
are	O
equally	O
difﬁcult—	O
is	O
good	O
enough	O
.	O
if	O
it	O
’	O
s	O
large	O
,	O
we	O
need	O
a	O
better	O
model	O
.	O
in	O
the	O
next	O
few	O
sections	O
,	O
i	O
develop	O
a	O
better	O
model	O
and	O
discover	O
(	O
spoiler	O
alert	O
!	O
)	O
that	O
the	O
modeling	B
error	I
is	O
small	O
.	O
so	O
if	O
you	O
are	O
satisﬁed	O
with	O
the	O
simple	O
model	O
,	O
you	O
can	O
skip	O
to	O
the	O
next	O
chapter	O
.	O
if	O
you	O
want	O
to	O
see	O
how	O
the	O
more	O
realistic	O
model	O
works	O
,	O
read	O
on	O
...	O
•	O
assume	O
that	O
each	O
test-taker	O
has	O
some	O
degree	O
of	O
efficacy	O
,	O
which	O
mea-	O
sures	O
their	O
ability	O
to	O
answer	O
sat	O
questions	O
.	O
•	O
assume	O
that	O
each	O
question	O
has	O
some	O
level	O
of	O
difficulty	O
.	O
12.5.	O
a	O
better	O
model	O
137	O
•	O
finally	O
,	O
assume	O
that	O
the	O
chance	O
that	O
a	O
test-taker	O
answers	O
a	O
question	O
correctly	O
is	O
related	O
to	O
efficacy	O
and	O
difficulty	O
according	O
to	O
this	O
func-	O
tion	O
:	O
def	O
probcorrect	O
(	O
efficacy	O
,	O
difficulty	O
,	O
a=1	O
)	O
:	O
return	O
1	O
/	O
(	O
1	O
+	O
math.exp	O
(	O
-a	O
*	O
(	O
efficacy	O
-	O
difficulty	O
)	O
)	O
)	O
this	O
function	O
is	O
a	O
simpliﬁed	O
version	O
of	O
the	O
curve	O
used	O
in	O
item	O
response	O
the-	O
ory	O
,	O
which	O
you	O
can	O
read	O
about	O
at	O
http	O
:	O
//en.wikipedia.org/wiki/item_	O
response_theory	O
.	O
efficacy	O
and	O
difficulty	O
are	O
considered	O
to	O
be	O
on	O
the	O
same	O
scale	O
,	O
and	O
the	O
probability	B
of	O
getting	O
a	O
question	O
right	O
depends	O
only	O
on	O
the	O
difference	O
between	O
them	O
.	O
when	O
efficacy	O
and	O
difficulty	O
are	O
equal	O
,	O
the	O
probability	B
of	O
getting	O
the	O
question	O
right	O
is	O
50	O
%	O
.	O
as	O
efficacy	O
increases	O
,	O
this	O
probability	B
approaches	O
100	O
%	O
.	O
as	O
it	O
decreases	O
(	O
or	O
as	O
difficulty	O
increases	O
)	O
,	O
the	O
probability	B
ap-	O
proaches	O
0	O
%	O
.	O
given	O
the	O
distribution	B
of	O
efficacy	O
across	O
test-takers	O
and	O
the	O
distribution	B
of	O
difficulty	O
across	O
questions	O
,	O
we	O
can	O
compute	O
the	O
expected	O
distribution	B
of	O
raw	O
scores	O
.	O
we	O
’	O
ll	O
do	O
that	O
in	O
two	O
steps	O
.	O
first	O
,	O
for	O
a	O
person	O
with	O
given	O
efficacy	O
,	O
we	O
’	O
ll	O
compute	O
the	O
distribution	B
of	O
raw	O
scores	O
.	O
def	O
pmfcorrect	O
(	O
efficacy	O
,	O
difficulties	O
)	O
:	O
pmf0	O
=	O
thinkbayes.pmf	O
(	O
[	O
0	O
]	O
)	O
ps	O
=	O
[	O
probcorrect	O
(	O
efficacy	O
,	O
diff	O
)	O
for	O
diff	O
in	O
difficulties	O
]	O
pmfs	O
=	O
[	O
binarypmf	O
(	O
p	O
)	O
for	O
p	O
in	O
ps	O
]	O
dist	O
=	O
sum	O
(	O
pmfs	O
,	O
pmf0	O
)	O
return	O
dist	O
difficulties	O
is	O
a	O
list	O
of	O
difﬁculties	O
,	O
one	O
for	O
each	O
question	O
.	O
ps	O
is	O
a	O
list	O
of	O
probabilities	O
,	O
and	O
pmfs	O
is	O
a	O
list	O
of	O
two-valued	O
pmf	O
objects	O
;	O
here	O
’	O
s	O
the	O
func-	O
tion	O
that	O
makes	O
them	O
:	O
def	O
binarypmf	O
(	O
p	O
)	O
:	O
pmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
pmf.set	O
(	O
1	O
,	O
p	O
)	O
pmf.set	O
(	O
0	O
,	O
1-p	O
)	O
return	O
pmf	O
dist	O
is	O
the	O
sum	O
of	O
these	O
pmfs	O
.	O
remember	O
from	O
section	O
5.4	O
that	O
when	O
we	O
add	O
up	O
pmf	O
objects	O
,	O
the	O
result	O
is	O
the	O
distribution	B
of	O
the	O
sums	O
.	O
in	O
order	O
to	O
use	O
python	O
’	O
s	O
sum	O
to	O
add	O
up	O
pmfs	O
,	O
we	O
have	O
to	O
provide	O
pmf0	O
which	O
is	O
the	O
identity	O
for	O
pmfs	O
,	O
so	O
pmf	O
+	O
pmf0	O
is	O
always	O
pmf	O
.	O
138	O
chapter	O
12.	O
evidence	B
if	O
we	O
know	O
a	O
person	O
’	O
s	O
efﬁcacy	B
,	O
we	O
can	O
compute	O
their	O
distribution	B
of	O
raw	O
scores	O
.	O
for	O
a	O
group	O
of	O
people	O
with	O
a	O
different	O
efﬁcacies	O
,	O
the	O
resulting	O
distri-	O
bution	O
of	O
raw	O
scores	O
is	O
a	O
mixture	B
.	O
here	O
’	O
s	O
the	O
code	O
that	O
computes	O
the	O
mix-	O
ture	O
:	O
#	O
class	O
exam	O
:	O
def	O
makerawscoredist	O
(	O
self	O
,	O
efficacies	O
)	O
:	O
pmfs	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
efficacy	O
,	O
prob	O
in	O
efficacies.items	O
(	O
)	O
:	O
scores	O
=	O
pmfcorrect	O
(	O
efficacy	O
,	O
self.difficulties	O
)	O
pmfs.set	O
(	O
scores	O
,	O
prob	O
)	O
mix	O
=	O
thinkbayes.makemixture	O
(	O
pmfs	O
)	O
return	O
mix	O
makerawscoredist	O
takes	O
efficacies	O
,	O
which	O
is	O
a	O
pmf	O
that	O
represents	O
the	O
dis-	O
tribution	O
of	O
efﬁcacy	B
across	O
test-takers	O
.	O
i	O
assume	O
it	O
is	O
gaussian	O
with	O
mean	O
0	O
and	O
standard	O
deviation	O
1.5.	O
this	O
choice	O
is	O
mostly	O
arbitrary	O
.	O
the	O
probability	B
of	O
getting	O
a	O
question	O
correct	O
depends	O
on	O
the	O
difference	O
between	O
efﬁcacy	B
and	O
difﬁculty	O
,	O
so	O
we	O
can	O
choose	O
the	O
units	O
of	O
efﬁcacy	B
and	O
then	O
calibrate	O
the	O
units	O
of	O
difﬁculty	O
accordingly	O
.	O
pmfs	O
is	O
a	O
meta-pmf	O
that	O
contains	O
one	O
pmf	O
for	O
each	O
level	O
of	O
efﬁcacy	B
,	O
and	O
maps	O
to	O
the	O
fraction	O
of	O
test-takers	O
at	O
that	O
level	O
.	O
makemixture	O
takes	O
the	O
meta-pmf	O
and	O
computes	O
the	O
distribution	B
of	O
the	O
mixture	B
(	O
see	O
section	O
5.6	O
)	O
.	O
12.6	O
calibration	B
if	O
we	O
were	O
given	O
the	O
distribution	B
of	O
difﬁculty	O
,	O
we	O
could	O
use	O
makerawscoredist	O
to	O
compute	O
the	O
distribution	B
of	O
raw	O
scores	O
.	O
but	O
for	O
us	O
the	O
problem	O
is	O
the	O
other	O
way	O
around	O
:	O
we	O
are	O
given	O
the	O
distribution	B
of	O
raw	O
scores	O
and	O
we	O
want	O
to	O
infer	O
the	O
distribution	B
of	O
difﬁculty	O
.	O
i	O
assume	O
that	O
the	O
distribution	B
of	O
difﬁculty	O
is	O
uniform	O
with	O
parameters	O
center	O
and	O
width	O
.	O
makedifficulties	O
makes	O
a	O
list	O
of	O
difﬁculties	O
with	O
these	O
parameters	O
.	O
def	O
makedifficulties	O
(	O
center	O
,	O
width	O
,	O
n	O
)	O
:	O
low	O
,	O
high	O
=	O
center-width	O
,	O
center+width	O
return	O
numpy.linspace	O
(	O
low	O
,	O
high	O
,	O
n	O
)	O
by	O
trying	O
out	O
a	O
few	O
combinations	O
,	O
i	O
found	O
that	O
center=-0.05	O
and	O
width=1.8	O
yield	O
a	O
distribution	B
of	O
raw	O
scores	O
similar	O
to	O
the	O
actual	O
data	O
,	O
as	O
shown	O
in	O
figure	O
12.3	O
.	O
12.7.	O
posterior	B
distribution	I
of	O
efﬁcacy	B
139	O
figure	O
12.3	O
:	O
actual	O
distribution	B
of	O
raw	O
scores	O
and	O
a	O
model	O
to	O
ﬁt	O
it	O
.	O
so	O
,	O
assuming	O
that	O
the	O
distribution	B
of	O
difﬁculty	O
is	O
uniform	O
,	O
its	O
range	O
is	O
ap-	O
proximately	O
-1.85	O
to	O
1.75	O
,	O
given	O
that	O
efﬁcacy	B
is	O
gaussian	O
with	O
mean	O
0	O
and	O
standard	O
deviation	O
1.5.	O
the	O
following	O
table	O
shows	O
the	O
range	O
of	O
probcorrect	O
for	O
test-takers	O
at	O
dif-	O
ferent	O
levels	O
of	O
efﬁcacy	B
:	O
difﬁculty	O
efﬁcacy	B
-1.85	O
-0.05	O
0.95	O
0.82	O
0.51	O
0.19	O
0.05	O
3.00	O
1.50	O
0.00	O
-1.50	O
-3.00	O
0.99	O
0.97	O
0.86	O
0.59	O
0.24	O
1.75	O
0.78	O
0.44	O
0.15	O
0.04	O
0.01	O
someone	O
with	O
efﬁcacy	B
3	O
(	O
two	O
standard	O
deviations	O
above	O
the	O
mean	O
)	O
has	O
a	O
99	O
%	O
chance	O
of	O
answering	O
the	O
easiest	O
questions	O
on	O
the	O
exam	O
,	O
and	O
a	O
78	O
%	O
chance	O
of	O
answering	O
the	O
hardest	O
.	O
on	O
the	O
other	O
end	O
of	O
the	O
range	O
,	O
someone	O
two	O
standard	O
deviations	O
below	O
the	O
mean	O
has	O
only	O
a	O
24	O
%	O
chance	O
of	O
answer-	O
ing	O
the	O
easiest	O
questions	O
.	O
12.7	O
posterior	B
distribution	I
of	O
efﬁcacy	B
now	O
that	O
the	O
model	O
is	O
calibrated	O
,	O
we	O
can	O
compute	O
the	O
posterior	B
distribution	I
of	O
efﬁcacy	B
for	O
alice	O
and	O
bob	O
.	O
here	O
is	O
a	O
version	O
of	O
the	O
sat	O
class	O
that	O
uses	O
the	O
new	O
model	O
:	O
0102030405060raw	O
score0.00.20.40.60.81.0cdfdatamodel	O
140	O
chapter	O
12.	O
evidence	B
figure	O
12.4	O
:	O
posterior	B
distributions	O
of	O
efﬁcacy	B
for	O
alice	O
and	O
bob	O
.	O
class	O
sat2	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
exam	O
,	O
score	O
)	O
:	O
self.exam	O
=	O
exam	O
self.score	O
=	O
score	O
#	O
start	O
with	O
the	O
gaussian	O
prior	B
efficacies	O
=	O
thinkbayes.makegaussianpmf	O
(	O
0	O
,	O
1.5	O
,	O
3	O
)	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
efficacies	O
)	O
#	O
update	O
based	O
on	O
an	O
exam	O
score	O
self.update	O
(	O
score	O
)	O
update	O
invokes	O
likelihood	B
,	O
which	O
computes	O
the	O
likelihood	B
of	O
a	O
given	O
test	O
score	O
for	O
a	O
hypothetical	O
level	O
of	O
efﬁcacy	B
.	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
efficacy	O
=	O
hypo	O
score	O
=	O
data	O
raw	O
=	O
self.exam.reverse	O
(	O
score	O
)	O
pmf	O
=	O
self.exam.pmfcorrect	O
(	O
efficacy	O
)	O
like	O
=	O
pmf.prob	O
(	O
raw	O
)	O
return	O
like	O
pmf	O
is	O
the	O
distribution	B
of	O
raw	O
scores	O
for	O
a	O
test-taker	O
with	O
the	O
given	O
efﬁcacy	B
;	O
like	O
is	O
the	O
probability	B
of	O
the	O
observed	O
score	O
.	O
01234efficacy0.00.20.40.60.81.0cdfposterior	O
780posterior	O
740	O
12.8.	O
predictive	B
distribution	I
141	O
figure	O
12.4	O
shows	O
the	O
posterior	B
distributions	O
of	O
efﬁcacy	B
for	O
alice	O
and	O
bob	O
.	O
as	O
expected	O
,	O
the	O
location	O
of	O
alice	O
’	O
s	O
distribution	B
is	O
farther	O
to	O
the	O
right	O
,	O
but	O
again	O
there	O
is	O
some	O
overlap	O
.	O
using	O
toplevel	O
again	O
,	O
we	O
compare	O
a	O
,	O
the	O
hypothesis	O
that	O
alice	O
’	O
s	O
efﬁcacy	B
is	O
higher	O
,	O
and	O
b	O
,	O
the	O
hypothesis	O
that	O
bob	O
’	O
s	O
is	O
higher	O
.	O
the	O
likelihood	B
ratio	I
is	O
3.4	O
,	O
a	O
bit	O
smaller	O
than	O
what	O
we	O
got	O
from	O
the	O
simple	O
model	O
(	O
3.8	O
)	O
.	O
so	O
this	O
model	O
indicates	O
that	O
the	O
data	O
are	O
evidence	B
in	O
favor	O
of	O
a	O
,	O
but	O
a	O
little	O
weaker	O
than	O
the	O
previous	O
estimate	O
.	O
if	O
our	O
prior	B
belief	O
is	O
that	O
a	O
and	O
b	O
are	O
equally	O
likely	O
,	O
then	O
in	O
light	O
of	O
this	O
evidence	B
we	O
would	O
give	O
a	O
a	O
posterior	B
probability	O
of	O
77	O
%	O
,	O
leaving	O
a	O
23	O
%	O
chance	O
that	O
bob	O
’	O
s	O
efﬁcacy	B
is	O
higher	O
.	O
12.8	O
predictive	B
distribution	I
the	O
analysis	O
we	O
have	O
done	O
so	O
far	O
generates	O
estimates	O
for	O
alice	O
and	O
bob	O
’	O
s	O
efﬁcacy	B
,	O
but	O
since	O
efﬁcacy	B
is	O
not	O
directly	O
observable	O
,	O
it	O
is	O
hard	O
to	O
validate	O
the	O
results	O
.	O
to	O
give	O
the	O
model	O
predictive	O
power	O
,	O
we	O
can	O
use	O
it	O
to	O
answer	O
a	O
related	O
ques-	O
tion	O
:	O
“	O
if	O
alice	O
and	O
bob	O
take	O
the	O
math	O
sat	O
again	O
,	O
what	O
is	O
the	O
chance	O
that	O
alice	O
will	O
do	O
better	O
again	O
?	O
”	O
we	O
’	O
ll	O
answer	O
this	O
question	O
in	O
two	O
steps	O
:	O
•	O
we	O
’	O
ll	O
use	O
the	O
posterior	B
distribution	I
of	O
efﬁcacy	B
to	O
generate	O
a	O
predictive	B
distribution	I
of	O
raw	B
score	I
for	O
each	O
test-taker	O
.	O
•	O
we	O
’	O
ll	O
compare	O
the	O
two	O
predictive	O
distributions	O
to	O
compute	O
the	O
proba-	O
bility	O
that	O
alice	O
gets	O
a	O
higher	O
score	O
again	O
.	O
we	O
already	O
have	O
most	O
of	O
the	O
code	O
we	O
need	O
.	O
to	O
compute	O
the	O
predictive	O
dis-	O
tributions	O
,	O
we	O
can	O
use	O
makerawscoredist	O
again	O
:	O
exam	O
=	O
exam	O
(	O
)	O
a_sat	O
=	O
sat	O
(	O
exam	O
,	O
780	O
)	O
b_sat	O
=	O
sat	O
(	O
exam	O
,	O
740	O
)	O
a_pred	O
=	O
exam.makerawscoredist	O
(	O
a_sat	O
)	O
b_pred	O
=	O
exam.makerawscoredist	O
(	O
b_sat	O
)	O
then	O
we	O
can	O
ﬁnd	O
the	O
likelihood	B
that	O
alice	O
does	O
better	O
on	O
the	O
second	O
test	O
,	O
bob	O
does	O
better	O
,	O
or	O
they	O
tie	O
:	O
142	O
chapter	O
12.	O
evidence	B
figure	O
12.5	O
:	O
joint	O
posterior	O
distribution	B
of	O
p_correct	O
for	O
alice	O
and	O
bob	O
.	O
a_like	O
=	O
thinkbayes.pmfprobgreater	O
(	O
a_pred	O
,	O
b_pred	O
)	O
b_like	O
=	O
thinkbayes.pmfprobless	O
(	O
a_pred	O
,	O
b_pred	O
)	O
c_like	O
=	O
thinkbayes.pmfprobequal	O
(	O
a_pred	O
,	O
b_pred	O
)	O
the	O
probability	B
that	O
alice	O
does	O
better	O
on	O
the	O
second	O
exam	O
is	O
63	O
%	O
,	O
which	O
means	O
that	O
bob	O
has	O
a	O
37	O
%	O
chance	O
of	O
doing	O
as	O
well	O
or	O
better	O
.	O
notice	O
that	O
we	O
have	O
more	O
conﬁdence	O
about	O
alice	O
’	O
s	O
efﬁcacy	B
than	O
we	O
do	O
about	O
the	O
outcome	O
of	O
the	O
next	O
test	O
.	O
the	O
posterior	B
odds	O
are	O
3:1	O
that	O
alice	O
’	O
s	O
efﬁcacy	B
is	O
higher	O
,	O
but	O
only	O
2:1	O
that	O
alice	O
will	O
do	O
better	O
on	O
the	O
next	O
exam	O
.	O
12.9	O
discussion	O
we	O
started	O
this	O
chapter	O
with	O
the	O
question	O
,	O
“	O
how	O
strong	O
is	O
the	O
evidence	B
that	O
alice	O
is	O
better	O
prepared	O
than	O
bob	O
?	O
”	O
on	O
the	O
face	O
of	O
it	O
,	O
that	O
sounds	O
like	O
we	O
want	O
to	O
test	O
two	O
hypotheses	O
:	O
either	O
alice	O
is	O
more	O
prepared	O
or	O
bob	O
is	O
.	O
but	O
in	O
order	O
to	O
compute	O
likelihoods	O
for	O
these	O
hypotheses	O
,	O
we	O
have	O
to	O
solve	O
an	O
estimation	O
problem	O
.	O
for	O
each	O
test-taker	O
we	O
have	O
to	O
ﬁnd	O
the	O
posterior	B
distribution	I
of	O
either	O
p_correct	O
or	O
efficacy	O
.	O
values	O
like	O
this	O
are	O
called	O
nuisance	O
parameters	O
because	O
we	O
don	O
’	O
t	O
care	O
what	O
they	O
are	O
,	O
but	O
we	O
have	O
to	O
estimate	O
them	O
to	O
answer	O
the	O
question	O
we	O
care	O
about	O
.	O
one	O
way	O
to	O
visualize	O
the	O
analysis	O
we	O
did	O
in	O
this	O
chapter	O
is	O
to	O
plot	O
the	O
space	O
of	O
these	O
parameters	O
.	O
thinkbayes.makejoint	O
takes	O
two	O
pmfs	O
,	O
com-	O
0.800.850.900.951.00p_correct	O
alice0.800.850.900.951.00p_correct	O
bob	O
12.9.	O
discussion	O
143	O
putes	O
their	O
joint	B
distribution	I
,	O
and	O
returns	O
a	O
joint	O
pmf	O
of	O
each	O
possible	O
pair	O
of	O
values	O
and	O
its	O
probability	B
.	O
def	O
makejoint	O
(	O
pmf1	O
,	O
pmf2	O
)	O
:	O
joint	O
=	O
joint	O
(	O
)	O
for	O
v1	O
,	O
p1	O
in	O
pmf1.items	O
(	O
)	O
:	O
for	O
v2	O
,	O
p2	O
in	O
pmf2.items	O
(	O
)	O
:	O
joint.set	O
(	O
(	O
v1	O
,	O
v2	O
)	O
,	O
p1	O
*	O
p2	O
)	O
return	O
joint	O
this	O
function	O
assumes	O
that	O
the	O
two	O
distributions	O
are	O
independent	O
.	O
figure	O
12.5	O
shows	O
the	O
joint	O
posterior	O
distribution	B
of	O
p_correct	O
for	O
alice	O
and	O
bob	O
.	O
the	O
diagonal	O
line	O
indicates	O
the	O
part	O
of	O
the	O
space	O
where	O
p_correct	O
is	O
the	O
same	O
for	O
alice	O
and	O
bob	O
.	O
to	O
the	O
right	O
of	O
this	O
line	O
,	O
alice	O
is	O
more	O
prepared	O
;	O
to	O
the	O
left	O
,	O
bob	O
is	O
more	O
prepared	O
.	O
in	O
toplevel.update	O
,	O
when	O
we	O
compute	O
the	O
likelihoods	O
of	O
a	O
and	O
b	O
,	O
we	O
add	O
up	O
the	O
probability	O
mass	O
on	O
each	O
side	O
of	O
this	O
line	O
.	O
for	O
the	O
cells	O
that	O
fall	O
on	O
the	O
line	O
,	O
we	O
add	O
up	O
the	O
total	O
mass	O
and	O
split	O
it	O
between	O
a	O
and	O
b.	O
the	O
process	B
we	O
used	O
in	O
this	O
chapter—estimating	O
nuisance	O
parameters	O
in	O
order	O
to	O
evaluate	O
the	O
likelihood	B
of	O
competing	O
hypotheses—is	O
a	O
common	O
bayesian	O
approach	O
to	O
problems	O
like	O
this	O
.	O
144	O
chapter	O
12.	O
evidence	B
chapter	O
13	O
simulation	B
in	O
this	O
chapter	O
i	O
describe	O
my	O
solution	O
to	O
a	O
problem	O
posed	O
by	O
a	O
patient	O
with	O
a	O
kidney	O
tumor	O
.	O
i	O
think	O
the	O
problem	O
is	O
important	O
and	O
relevant	O
to	O
patients	O
with	O
these	O
tumors	O
and	O
doctors	O
treating	O
them	O
.	O
and	O
i	O
think	O
the	O
solution	O
is	O
interesting	O
because	O
,	O
although	O
it	O
is	O
a	O
bayesian	O
approach	O
to	O
the	O
problem	O
,	O
the	O
use	O
of	O
bayes	O
’	O
s	O
theorem	O
is	O
implicit	O
.	O
i	O
present	O
the	O
solution	O
and	O
my	O
code	O
;	O
at	O
the	O
end	O
of	O
the	O
chapter	O
i	O
will	O
explain	O
the	O
bayesian	O
part	O
.	O
if	O
you	O
want	O
more	O
technical	O
detail	O
than	O
i	O
present	O
here	O
,	O
you	O
can	O
read	O
my	O
paper	O
on	O
this	O
work	O
at	O
http	O
:	O
//arxiv.org/abs/1203.6890	O
.	O
13.1	O
the	O
kidney	O
tumor	O
problem	O
i	O
am	O
a	O
frequent	O
reader	O
and	O
occasional	O
contributor	O
to	O
the	O
online	O
statistics	O
forum	O
at	O
http	O
:	O
//reddit.com/r/statistics	O
.	O
in	O
november	O
2011	O
,	O
i	O
read	O
the	O
following	O
message	O
:	O
''	O
i	O
have	O
stage	O
iv	O
kidney	O
cancer	O
and	O
am	O
trying	O
to	O
determine	O
if	O
the	O
cancer	O
formed	O
before	O
i	O
retired	O
from	O
the	O
military	O
.	O
...	O
given	O
the	O
dates	O
of	O
retirement	O
and	O
detection	O
is	O
it	O
possible	O
to	O
determine	O
when	O
there	O
was	O
a	O
50/50	O
chance	O
that	O
i	O
developed	O
the	O
disease	O
?	O
is	O
it	O
possible	O
to	O
determine	O
the	O
probability	B
on	O
the	O
retirement	O
date	O
?	O
my	O
tumor	O
was	O
15.5	O
cm	O
x	O
15	O
cm	O
at	O
detection	O
.	O
grade	O
ii	O
.	O
''	O
i	O
contacted	O
the	O
author	O
of	O
the	O
message	O
and	O
got	O
more	O
information	O
;	O
i	O
learned	O
that	O
veterans	O
get	O
different	O
beneﬁts	O
if	O
it	O
is	O
``	O
more	O
likely	O
than	O
not	O
''	O
that	O
a	O
tumor	O
formed	O
while	O
they	O
were	O
in	O
military	O
service	O
(	O
among	O
other	O
considerations	O
)	O
.	O
146	O
chapter	O
13.	O
simulation	B
figure	O
13.1	O
:	O
cdf	O
of	O
rdt	O
in	O
doublings	O
per	O
year	O
.	O
because	O
renal	O
tumors	O
grow	O
slowly	O
,	O
and	O
often	O
do	O
not	O
cause	O
symptoms	O
,	O
they	O
are	O
sometimes	O
left	O
untreated	O
.	O
as	O
a	O
result	O
,	O
doctors	O
can	O
observe	O
the	O
rate	O
of	O
growth	O
for	O
untreated	O
tumors	O
by	O
comparing	O
scans	O
from	O
the	O
same	O
patient	O
at	O
different	O
times	O
.	O
several	O
papers	O
have	O
reported	O
these	O
growth	O
rates	O
.	O
i	O
collected	O
data	O
from	O
a	O
paper	O
by	O
zhang	O
et	O
al1	O
.	O
i	O
contacted	O
the	O
authors	O
to	O
see	O
if	O
i	O
could	O
get	O
raw	O
data	O
,	O
but	O
they	O
refused	O
on	O
grounds	O
of	O
medical	O
privacy	O
.	O
nevertheless	O
,	O
i	O
was	O
able	O
to	O
extract	O
the	O
data	O
i	O
needed	O
by	O
printing	O
one	O
of	O
their	O
graphs	O
and	O
measuring	O
it	O
with	O
a	O
ruler	O
.	O
they	O
report	O
growth	O
rates	O
in	O
reciprocal	O
doubling	B
time	I
(	O
rdt	O
)	O
,	O
which	O
is	O
in	O
units	O
of	O
doublings	O
per	O
year	O
.	O
so	O
a	O
tumor	O
with	O
rdt	O
=	O
1	O
doubles	O
in	O
volume	B
each	O
year	O
;	O
with	O
rdt	O
=	O
2	O
it	O
quadruples	O
in	O
the	O
same	O
time	O
,	O
and	O
with	O
rdt	O
=	O
−1	O
,	O
it	O
halves	O
.	O
figure	O
13.1	O
shows	O
the	O
distribution	B
of	O
rdt	O
for	O
53	O
patients	O
.	O
the	O
squares	O
are	O
the	O
data	O
points	O
from	O
the	O
paper	O
;	O
the	O
line	O
is	O
a	O
model	O
i	O
ﬁt	O
to	O
the	O
data	O
.	O
the	O
positive	O
tail	O
ﬁts	O
an	O
exponential	B
distribution	I
well	O
,	O
so	O
i	O
used	O
a	O
mixture	B
of	O
two	O
exponentials	O
.	O
13.2	O
a	O
simple	O
model	O
it	O
is	O
usually	O
a	O
good	O
idea	O
to	O
start	O
with	O
a	O
simple	O
model	O
before	O
trying	O
some-	O
thing	O
more	O
challenging	O
.	O
sometimes	O
the	O
simple	O
model	O
is	O
sufﬁcient	O
for	O
the	O
1zhang	O
et	O
al	O
,	O
distribution	B
of	O
renal	O
tumor	O
growth	O
rates	O
determined	O
by	O
using	O
serial	O
volumetric	O
ct	O
measurements	O
,	O
january	O
2009	O
radiology	O
,	O
250	O
,	O
137-144	O
.	O
2101234567rdt	O
(	O
volume	B
doublings	O
per	O
year	O
)	O
0.00.20.40.60.81.0cdfdistribution	O
of	O
rdtmodeldata	O
13.2.	O
a	O
simple	O
model	O
147	O
problem	O
at	O
hand	O
,	O
and	O
if	O
not	O
,	O
you	O
can	O
use	O
it	O
to	O
validate	O
the	O
more	O
complex	O
model	O
.	O
for	O
my	O
simple	O
model	O
,	O
i	O
assume	O
that	O
tumors	O
grow	O
with	O
a	O
constant	O
doubling	B
time	I
,	O
and	O
that	O
they	O
are	O
three-dimensional	O
in	O
the	O
sense	O
that	O
if	O
the	O
maximum	B
linear	O
measurement	O
doubles	O
,	O
the	O
volume	B
is	O
multiplied	O
by	O
eight	O
.	O
i	O
learned	O
from	O
my	O
correspondent	O
that	O
the	O
time	O
between	O
his	O
discharge	O
from	O
the	O
military	O
and	O
his	O
diagnosis	O
was	O
3291	O
days	O
(	O
about	O
9	O
years	O
)	O
.	O
so	O
my	O
ﬁrst	O
calculation	O
was	O
,	O
“	O
if	O
this	O
tumor	O
grew	O
at	O
the	O
median	B
rate	O
,	O
how	O
big	O
would	O
it	O
have	O
been	O
at	O
the	O
date	O
of	O
discharge	O
?	O
”	O
the	O
median	B
volume	O
doubling	B
time	I
reported	O
by	O
zhang	O
et	O
al	O
is	O
811	O
days	O
.	O
assuming	O
3-dimensional	O
geometry	O
,	O
the	O
doubling	B
time	I
for	O
a	O
linear	O
measure	O
is	O
three	O
times	O
longer	O
.	O
#	O
time	O
between	O
discharge	O
and	O
diagnosis	O
,	O
in	O
days	O
interval	O
=	O
3291.0	O
#	O
doubling	B
time	I
in	O
linear	O
measure	O
is	O
doubling	B
time	I
in	O
volume	B
*	O
3	O
dt	O
=	O
811.0	O
*	O
3	O
#	O
number	O
of	O
doublings	O
since	O
discharge	O
doublings	O
=	O
interval	O
/	O
dt	O
#	O
how	O
big	O
was	O
the	O
tumor	O
at	O
time	O
of	O
discharge	O
(	O
diameter	O
in	O
cm	O
)	O
d1	O
=	O
15.5	O
d0	O
=	O
d1	O
/	O
2.0	O
**	O
doublings	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
kidney.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3.	O
the	O
result	O
,	O
d0	O
,	O
is	O
about	O
6	O
cm	O
.	O
so	O
if	O
this	O
tumor	O
formed	O
after	O
the	O
date	O
of	O
dis-	O
charge	O
,	O
it	O
must	O
have	O
grown	O
substantially	O
faster	O
than	O
the	O
median	B
rate	O
.	O
there-	O
fore	O
i	O
concluded	O
that	O
it	O
is	O
“	O
more	O
likely	O
than	O
not	O
”	O
that	O
this	O
tumor	O
formed	O
before	O
the	O
date	O
of	O
discharge	O
.	O
in	O
addition	O
,	O
i	O
computed	O
the	O
growth	B
rate	I
that	O
would	O
be	O
implied	O
if	O
this	O
tumor	O
had	O
formed	O
after	O
the	O
date	O
of	O
discharge	O
.	O
if	O
we	O
assume	O
an	O
initial	O
size	O
of	O
0.1	O
cm	O
,	O
we	O
can	O
compute	O
the	O
number	O
of	O
doublings	O
to	O
get	O
to	O
a	O
ﬁnal	O
size	O
of	O
15.5	O
cm	O
:	O
#	O
assume	O
an	O
initial	O
linear	O
measure	O
of	O
0.1	O
cm	O
d0	O
=	O
0.1	O
d1	O
=	O
15.5	O
148	O
chapter	O
13.	O
simulation	B
#	O
how	O
many	O
doublings	O
would	O
it	O
take	O
to	O
get	O
from	O
d0	O
to	O
d1	O
doublings	O
=	O
log2	O
(	O
d1	O
/	O
d0	O
)	O
#	O
what	O
linear	O
doubling	B
time	I
does	O
that	O
imply	O
?	O
dt	O
=	O
interval	O
/	O
doublings	O
#	O
compute	O
the	O
volumetric	O
doubling	B
time	I
and	O
rdt	O
vdt	O
=	O
dt	O
/	O
3	O
rdt	O
=	O
365	O
/	O
vdt	O
dt	O
is	O
linear	O
doubling	B
time	I
,	O
so	O
vdt	O
is	O
volumetric	O
doubling	B
time	I
,	O
and	O
rdt	O
is	O
reciprocal	O
doubling	B
time	I
.	O
the	O
number	O
of	O
doublings	O
,	O
in	O
linear	O
measure	O
,	O
is	O
7.3	O
,	O
which	O
implies	O
an	O
rdt	O
of	O
2.4.	O
in	O
the	O
data	O
from	O
zhang	O
et	O
al	O
,	O
only	O
20	O
%	O
of	O
tumors	O
grew	O
this	O
fast	O
during	O
a	O
period	O
of	O
observation	O
.	O
so	O
again	O
,	O
i	O
concluded	O
that	O
is	O
“	O
more	O
likely	O
than	O
not	O
”	O
that	O
the	O
tumor	O
formed	O
prior	B
to	O
the	O
date	O
of	O
discharge	O
.	O
these	O
calculations	O
are	O
sufﬁcient	O
to	O
answer	O
the	O
question	O
as	O
posed	O
,	O
and	O
on	O
behalf	O
of	O
my	O
correspondent	O
,	O
i	O
wrote	O
a	O
letter	O
explaining	O
my	O
conclusions	O
to	O
the	O
veterans	O
’	O
beneﬁt	O
administration	O
.	O
later	O
i	O
told	O
a	O
friend	O
,	O
who	O
is	O
an	O
oncologist	O
,	O
about	O
my	O
results	O
.	O
he	O
was	O
sur-	O
prised	O
by	O
the	O
growth	O
rates	O
observed	O
by	O
zhang	O
et	O
al	O
,	O
and	O
by	O
what	O
they	O
imply	O
about	O
the	O
ages	O
of	O
these	O
tumors	O
.	O
he	O
suggested	O
that	O
the	O
results	O
might	O
be	O
in-	O
teresting	O
to	O
researchers	O
and	O
doctors	O
.	O
but	O
in	O
order	O
to	O
make	O
them	O
useful	O
,	O
i	O
wanted	O
a	O
more	O
general	O
model	O
of	O
the	O
relationship	O
between	O
age	O
and	O
size	O
.	O
13.3	O
a	O
more	O
general	O
model	O
given	O
the	O
size	O
of	O
a	O
tumor	O
at	O
time	O
of	O
diagnosis	O
,	O
it	O
would	O
be	O
most	O
useful	O
to	O
know	O
the	O
probability	B
that	O
the	O
tumor	O
formed	O
before	O
any	O
given	O
date	O
;	O
in	O
other	O
words	O
,	O
the	O
distribution	B
of	O
ages	O
.	O
to	O
ﬁnd	O
it	O
,	O
i	O
run	O
simulations	O
of	O
tumor	O
growth	O
to	O
get	O
the	O
distribution	B
of	O
size	O
conditioned	O
on	O
age	O
.	O
then	O
we	O
can	O
use	O
a	O
bayesian	O
approach	O
to	O
get	O
the	O
distri-	O
bution	O
of	O
age	O
conditioned	O
on	O
size	O
.	O
the	O
simulation	B
starts	O
with	O
a	O
small	O
tumor	O
and	O
runs	O
these	O
steps	O
:	O
1.	O
choose	O
a	O
growth	B
rate	I
from	O
the	O
distribution	B
of	O
rdt	O
.	O
13.3.	O
a	O
more	O
general	O
model	O
149	O
figure	O
13.2	O
:	O
simulations	O
of	O
tumor	O
growth	O
,	O
size	O
vs.	O
time	O
.	O
2.	O
compute	O
the	O
size	O
of	O
the	O
tumor	O
at	O
the	O
end	O
of	O
an	O
interval	O
.	O
3.	O
record	O
the	O
size	O
of	O
the	O
tumor	O
at	O
each	O
interval	O
.	O
4.	O
repeat	O
until	O
the	O
tumor	O
exceeds	O
the	O
maximum	B
relevant	O
size	O
.	O
for	O
the	O
initial	O
size	O
i	O
chose	O
0.3	O
cm	O
,	O
because	O
carcinomas	O
smaller	O
than	O
that	O
are	O
less	O
likely	O
to	O
be	O
invasive	O
and	O
less	O
likely	O
to	O
have	O
the	O
blood	O
supply	O
needed	O
for	O
rapid	O
growth	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/carcinoma_in_situ	O
)	O
.	O
i	O
chose	O
an	O
interval	O
of	O
245	O
days	O
(	O
about	O
8	O
months	O
)	O
because	O
that	O
is	O
the	O
median	B
time	O
between	O
measurements	O
in	O
the	O
data	O
source	O
.	O
for	O
the	O
maximum	B
size	O
i	O
chose	O
20	O
cm	O
.	O
in	O
the	O
data	O
source	O
,	O
the	O
range	O
of	O
ob-	O
served	O
sizes	O
is	O
1.0	O
to	O
12.0	O
cm	O
,	O
so	O
we	O
are	O
extrapolating	O
beyond	O
the	O
observed	O
range	O
at	O
each	O
end	O
,	O
but	O
not	O
by	O
far	O
,	O
and	O
not	O
in	O
a	O
way	O
likely	O
to	O
have	O
a	O
strong	O
effect	O
on	O
the	O
results	O
.	O
the	O
simulation	B
is	O
based	O
on	O
one	O
big	O
simpliﬁcation	O
:	O
the	O
growth	B
rate	I
is	O
chosen	O
independently	O
during	O
each	O
interval	O
,	O
so	O
it	O
does	O
not	O
depend	O
on	O
age	O
,	O
size	O
,	O
or	O
growth	B
rate	I
during	O
previous	O
intervals	O
.	O
in	O
section	O
13.7	O
i	O
review	O
these	O
assumptions	O
and	O
consider	O
more	O
detailed	O
mod-	O
els	O
.	O
but	O
ﬁrst	O
let	O
’	O
s	O
look	O
at	O
some	O
examples	O
.	O
figure	O
13.2	O
shows	O
the	O
size	O
of	O
simulated	O
tumors	O
as	O
a	O
function	O
of	O
age	O
.	O
the	O
dashed	O
line	O
at	O
10	O
cm	O
shows	O
the	O
range	O
of	O
ages	O
for	O
tumors	O
at	O
that	O
size	O
:	O
the	O
fastest-growing	O
tumor	O
gets	O
there	O
in	O
8	O
years	O
;	O
the	O
slowest	O
takes	O
more	O
than	O
35	O
.	O
0510152025303540tumor	O
age	O
(	O
years	O
)	O
0.20.51251020diameter	O
(	O
cm	O
,	O
log	B
scale	I
)	O
simulations	O
of	O
tumor	O
growth	O
150	O
chapter	O
13.	O
simulation	B
i	O
am	O
presenting	O
results	O
in	O
terms	O
of	O
linear	O
measurements	O
,	O
but	O
the	O
calculations	O
are	O
in	O
terms	O
of	O
volume	B
.	O
to	O
convert	O
from	O
one	O
to	O
the	O
other	O
,	O
again	O
,	O
i	O
use	O
the	O
volume	B
of	O
a	O
sphere	B
with	O
the	O
given	O
diameter	O
.	O
13.4	O
implementation	B
here	O
is	O
the	O
kernel	O
of	O
the	O
simulation	B
:	O
def	O
makesequence	O
(	O
rdt_seq	O
,	O
v0=0.01	O
,	O
interval=0.67	O
,	O
vmax=volume	O
(	O
20.0	O
)	O
)	O
:	O
seq	O
=	O
v0	O
,	O
age	O
=	O
0	O
for	O
rdt	O
in	O
rdt_seq	O
:	O
age	O
+=	O
interval	O
final	O
,	O
seq	O
=	O
extendsequence	O
(	O
age	O
,	O
seq	O
,	O
rdt	O
,	O
interval	O
)	O
if	O
final	O
>	O
vmax	O
:	O
break	O
return	O
seq	O
rdt_seq	O
is	O
an	O
iterator	B
that	O
yields	O
random	O
values	O
from	O
the	O
cdf	O
of	O
growth	B
rate	I
.	O
v0	O
is	O
the	O
initial	O
volume	B
in	O
ml	O
.	O
interval	O
is	O
the	O
time	O
step	O
in	O
years	O
.	O
vmax	O
is	O
the	O
ﬁnal	O
volume	B
corresponding	O
to	O
a	O
linear	O
measurement	O
of	O
20	O
cm	O
.	O
volume	B
converts	O
from	O
linear	O
measurement	O
in	O
cm	O
to	O
volume	B
in	O
ml	O
,	O
based	O
on	O
the	O
simpliﬁcation	O
that	O
the	O
tumor	O
is	O
a	O
sphere	B
:	O
def	O
volume	B
(	O
diameter	O
,	O
factor=4*math.pi/3	O
)	O
:	O
return	O
factor	O
*	O
(	O
diameter/2.0	O
)	O
**3	O
extendsequence	O
computes	O
the	O
volume	B
of	O
the	O
tumor	O
at	O
the	O
end	O
of	O
the	O
inter-	O
val	O
.	O
def	O
extendsequence	O
(	O
age	O
,	O
seq	O
,	O
rdt	O
,	O
interval	O
)	O
:	O
initial	O
=	O
seq	O
[	O
-1	O
]	O
doublings	O
=	O
rdt	O
*	O
interval	O
final	O
=	O
initial	O
*	O
2**doublings	O
new_seq	O
=	O
seq	O
+	O
(	O
final	O
,	O
)	O
cache.add	O
(	O
age	O
,	O
new_seq	O
,	O
rdt	O
)	O
return	O
final	O
,	O
new_seq	O
age	O
is	O
the	O
age	O
of	O
the	O
tumor	O
at	O
the	O
end	O
of	O
the	O
interval	O
.	O
seq	O
is	O
a	O
tuple	B
that	O
contains	O
the	O
volumes	O
so	O
far	O
.	O
rdt	O
is	O
the	O
growth	B
rate	I
during	O
the	O
interval	O
,	O
in	O
doublings	O
per	O
year	O
.	O
interval	O
is	O
the	O
size	O
of	O
the	O
time	O
step	O
in	O
years	O
.	O
13.5.	O
caching	O
the	O
joint	B
distribution	I
151	O
figure	O
13.3	O
:	O
joint	B
distribution	I
of	O
age	O
and	O
tumor	O
size	O
.	O
the	O
return	O
values	O
are	O
final	O
,	O
the	O
volume	B
of	O
the	O
tumor	O
at	O
the	O
end	O
of	O
the	O
inter-	O
val	O
,	O
and	O
new_seq	O
,	O
a	O
new	O
tuple	B
containing	O
the	O
volumes	O
in	O
seq	O
plus	O
the	O
new	O
volume	B
final	O
.	O
cache.add	O
records	O
the	O
age	O
and	O
size	O
of	O
each	O
tumor	O
at	O
the	O
end	O
of	O
each	O
interval	O
,	O
as	O
explained	O
in	O
the	O
next	O
section	O
.	O
13.5	O
caching	O
the	O
joint	B
distribution	I
here	O
’	O
s	O
how	O
the	O
cache	B
works	O
.	O
class	O
cache	B
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
)	O
:	O
self.joint	O
=	O
thinkbayes.joint	O
(	O
)	O
joint	O
is	O
a	O
joint	O
pmf	O
that	O
records	O
the	O
frequency	O
of	O
each	O
age-size	O
pair	O
,	O
so	O
it	O
approximates	O
the	O
joint	B
distribution	I
of	O
age	O
and	O
size	O
.	O
at	O
the	O
end	O
of	O
each	O
simulated	O
interval	O
,	O
extendsequence	O
calls	O
add	O
:	O
#	O
class	O
cache	B
def	O
add	O
(	O
self	O
,	O
age	O
,	O
seq	O
)	O
:	O
final	O
=	O
seq	O
[	O
-1	O
]	O
cm	O
=	O
diameter	O
(	O
final	O
)	O
bucket	B
=	O
round	O
(	O
cmtobucket	O
(	O
cm	O
)	O
)	O
self.joint.incr	O
(	O
(	O
age	O
,	O
bucket	B
)	O
)	O
0510152025303540ages0.20.51251020diameter	O
(	O
cm	O
,	O
log	B
scale	I
)	O
152	O
chapter	O
13.	O
simulation	B
figure	O
13.4	O
:	O
distributions	O
of	O
age	O
,	O
conditioned	O
on	O
size	O
.	O
again	O
,	O
age	O
is	O
the	O
age	O
of	O
the	O
tumor	O
,	O
and	O
seq	O
is	O
the	O
sequence	O
of	O
volumes	O
so	O
far	O
.	O
before	O
adding	O
the	O
new	O
data	O
to	O
the	O
joint	B
distribution	I
,	O
we	O
use	O
diameter	O
to	O
convert	O
from	O
volume	B
to	O
diameter	O
in	O
centimeters	O
:	O
def	O
diameter	O
(	O
volume	B
,	O
factor=3/math.pi/4	O
,	O
exp=1/3.0	O
)	O
:	O
return	O
2	O
*	O
(	O
factor	O
*	O
volume	B
)	O
**	O
exp	O
and	O
cmtobucket	O
to	O
convert	O
from	O
centimeters	O
to	O
a	O
discrete	O
bucket	B
number	O
:	O
def	O
cmtobucket	O
(	O
x	O
,	O
factor=10	O
)	O
:	O
return	O
factor	O
*	O
math.log	O
(	O
x	O
)	O
the	O
buckets	O
are	O
equally	O
spaced	O
on	O
a	O
log	B
scale	I
.	O
using	O
factor=10	O
yields	O
a	O
reasonable	O
number	O
of	O
buckets	O
;	O
for	O
example	O
,	O
1	O
cm	O
maps	O
to	O
bucket	B
0	O
and	O
10	O
cm	O
maps	O
to	O
bucket	B
23.	O
after	O
running	O
the	O
simulations	O
,	O
we	O
can	O
plot	O
the	O
joint	B
distribution	I
as	O
a	O
pseu-	O
docolor	O
plot	O
,	O
where	O
each	O
cell	O
represents	O
the	O
number	O
of	O
tumors	O
observed	O
at	O
a	O
given	O
size-age	O
pair	O
.	O
figure	O
13.3	O
shows	O
the	O
joint	B
distribution	I
after	O
1000	O
simulations	O
.	O
13.6	O
conditional	B
distributions	O
by	O
taking	O
a	O
vertical	O
slice	O
from	O
the	O
joint	B
distribution	I
,	O
we	O
can	O
get	O
the	O
distribu-	O
tion	O
of	O
sizes	O
for	O
any	O
given	O
age	O
.	O
by	O
taking	O
a	O
horizontal	O
slice	O
,	O
we	O
can	O
get	O
the	O
distribution	B
of	O
ages	O
conditioned	O
on	O
size	O
.	O
01020304050tumor	O
age	O
(	O
years	O
)	O
0.00.20.40.60.81.0cdfdistribution	O
of	O
age	O
for	O
several	O
diameters2	O
cm5	O
cm10	O
cm15	O
cm	O
13.6.	O
conditional	B
distributions	O
153	O
figure	O
13.5	O
:	O
percentiles	O
of	O
tumor	O
age	O
as	O
a	O
function	O
of	O
size	O
.	O
here	O
’	O
s	O
the	O
code	O
that	O
reads	O
the	O
joint	B
distribution	I
and	O
builds	O
the	O
conditional	B
distribution	I
for	O
a	O
given	O
size	O
.	O
#	O
class	O
cache	B
def	O
conditionalcdf	O
(	O
self	O
,	O
bucket	B
)	O
:	O
pmf	O
=	O
self.joint.conditional	O
(	O
0	O
,	O
1	O
,	O
bucket	B
)	O
cdf	O
=	O
pmf.makecdf	O
(	O
)	O
return	O
cdf	O
bucket	B
is	O
the	O
integer	O
bucket	B
number	O
corresponding	O
to	O
tumor	O
size	O
.	O
joint.conditional	O
computes	O
the	O
pmf	O
of	O
age	O
conditioned	O
on	O
bucket	B
.	O
the	O
result	O
is	O
the	O
cdf	O
of	O
age	O
conditioned	O
on	O
bucket	B
.	O
figure	O
13.4	O
shows	O
several	O
of	O
these	O
cdfs	O
,	O
for	O
a	O
range	O
of	O
sizes	O
.	O
to	O
summarize	O
these	O
distributions	O
,	O
we	O
can	O
compute	O
percentiles	O
as	O
a	O
function	O
of	O
size	O
.	O
percentiles	O
=	O
[	O
95	O
,	O
75	O
,	O
50	O
,	O
25	O
,	O
5	O
]	O
for	O
bucket	B
in	O
cache.getbuckets	O
(	O
)	O
:	O
cdf	O
=	O
conditionalcdf	O
(	O
bucket	B
)	O
ps	O
=	O
[	O
cdf.percentile	O
(	O
p	O
)	O
for	O
p	O
in	O
percentiles	O
]	O
figure	O
13.5	O
shows	O
these	O
percentiles	O
for	O
each	O
size	O
bucket	B
.	O
the	O
data	O
points	O
are	O
computed	O
from	O
the	O
estimated	O
joint	B
distribution	I
.	O
in	O
the	O
model	O
,	O
size	O
and	O
time	O
are	O
discrete	O
,	O
which	O
contributes	O
numerical	O
errors	O
,	O
so	O
i	O
also	O
show	O
a	O
least	B
squares	I
ﬁt	I
for	O
each	O
sequence	O
of	O
percentiles	O
.	O
0.51251020diameter	O
(	O
cm	O
,	O
log	B
scale	I
)	O
051015202530354045tumor	O
age	O
(	O
years	O
)	O
95th75th50th25th5thcredible	O
interval	O
for	O
age	O
vs	O
diameter	O
154	O
chapter	O
13.	O
simulation	B
13.7	O
serial	B
correlation	I
the	O
results	O
so	O
far	O
are	O
based	O
on	O
a	O
number	O
of	O
modeling	B
decisions	O
;	O
let	O
’	O
s	O
review	O
them	O
and	O
consider	O
which	O
ones	O
are	O
the	O
most	O
likely	O
sources	O
of	O
error	B
:	O
•	O
to	O
convert	O
from	O
linear	O
measure	O
to	O
volume	B
,	O
we	O
assume	O
that	O
tumors	O
are	O
approximately	O
spherical	O
.	O
this	O
assumption	O
is	O
probably	O
ﬁne	O
for	O
tumors	O
up	O
to	O
a	O
few	O
centimeters	O
,	O
but	O
not	O
for	O
very	O
large	O
tumors	O
.	O
•	O
the	O
distribution	B
of	O
growth	O
rates	O
in	O
the	O
simulations	O
are	O
based	O
on	O
a	O
con-	O
tinuous	O
model	O
we	O
chose	O
to	O
ﬁt	O
the	O
data	O
reported	O
by	O
zhang	O
et	O
al	O
,	O
which	O
is	O
based	O
on	O
53	O
patients	O
.	O
the	O
ﬁt	O
is	O
only	O
approximate	O
and	O
,	O
more	O
impor-	O
tantly	O
,	O
a	O
larger	O
sample	O
would	O
yield	O
a	O
different	O
distribution	B
.	O
•	O
the	O
growth	O
model	O
does	O
not	O
take	O
into	O
account	O
tumor	O
subtype	O
or	O
grade	O
;	O
this	O
assumption	O
is	O
consistent	O
with	O
the	O
conclusion	O
of	O
zhang	O
et	O
al	O
:	O
“	O
growth	O
rates	O
in	O
renal	O
tumors	O
of	O
different	O
sizes	O
,	O
subtypes	O
and	O
grades	O
represent	O
a	O
wide	O
range	O
and	O
overlap	O
substantially.	O
”	O
but	O
with	O
a	O
larger	O
sample	O
,	O
a	O
difference	O
might	O
become	O
apparent	O
.	O
•	O
the	O
distribution	B
of	O
growth	B
rate	I
does	O
not	O
depend	O
on	O
the	O
size	O
of	O
the	O
tumor	O
.	O
this	O
assumption	O
would	O
not	O
be	O
realistic	O
for	O
very	O
small	O
and	O
very	O
large	O
tumors	O
,	O
whose	O
growth	O
is	O
limited	O
by	O
blood	O
supply	O
.	O
but	O
tumors	O
observed	O
by	O
zhang	O
et	O
al	O
ranged	O
from	O
1	O
to	O
12	O
cm	O
,	O
and	O
they	O
found	O
no	O
statistically	O
signiﬁcant	O
relationship	O
between	O
size	O
and	O
growth	B
rate	I
.	O
so	O
if	O
there	O
is	O
a	O
relationship	O
,	O
it	O
is	O
likely	O
to	O
be	O
weak	O
,	O
at	O
least	O
in	O
this	O
size	O
range	O
.	O
•	O
in	O
the	O
simulations	O
,	O
growth	B
rate	I
during	O
each	O
interval	O
is	O
independent	O
of	O
previous	O
growth	O
rates	O
.	O
in	O
reality	O
it	O
is	O
plausible	O
that	O
tumors	O
that	O
have	O
grown	O
quickly	O
in	O
the	O
past	O
are	O
more	O
likely	O
to	O
grow	O
quickly	O
.	O
in	O
other	O
words	O
,	O
there	O
is	O
probably	O
a	O
serial	B
correlation	I
in	O
growth	B
rate	I
.	O
of	O
these	O
,	O
the	O
ﬁrst	O
and	O
last	O
seem	O
the	O
most	O
problematic	O
.	O
i	O
’	O
ll	O
investigate	O
serial	B
correlation	I
ﬁrst	O
,	O
then	O
come	O
back	O
to	O
spherical	O
geometry	O
.	O
to	O
simulate	O
correlated	O
growth	O
,	O
i	O
wrote	O
a	O
generator2	O
that	O
yields	O
a	O
correlated	O
series	O
from	O
a	O
given	O
cdf	O
.	O
here	O
’	O
s	O
how	O
the	O
algorithm	O
works	O
:	O
1.	O
generate	O
correlated	O
values	O
from	O
a	O
gaussian	O
distribution	B
.	O
this	O
is	O
easy	O
to	O
do	O
because	O
we	O
can	O
compute	O
the	O
distribution	B
of	O
the	O
next	O
value	O
con-	O
ditioned	O
on	O
the	O
previous	O
value	O
.	O
2if	O
you	O
are	O
not	O
familiar	O
with	O
python	O
generators	O
,	O
see	O
http	O
:	O
//wiki.python.org/moin/	O
generators	O
.	O
13.7.	O
serial	B
correlation	I
155	O
2.	O
transform	O
each	O
value	O
to	O
its	O
cumulative	B
probability	I
using	O
the	O
gaussian	O
cdf	O
.	O
3.	O
transform	O
each	O
cumulative	B
probability	I
to	O
the	O
corresponding	O
value	O
us-	O
ing	O
the	O
given	O
cdf	O
.	O
here	O
’	O
s	O
what	O
that	O
looks	O
like	O
in	O
code	O
:	O
def	O
correlatedgenerator	O
(	O
cdf	O
,	O
rho	O
)	O
:	O
x	O
=	O
random.gauss	O
(	O
0	O
,	O
1	O
)	O
yield	O
transform	O
(	O
x	O
)	O
sigma	O
=	O
math.sqrt	O
(	O
1	O
-	O
rho**2	O
)	O
;	O
while	O
true	O
:	O
x	O
=	O
random.gauss	O
(	O
x	O
*	O
rho	O
,	O
sigma	O
)	O
yield	O
transform	O
(	O
x	O
)	O
cdf	O
is	O
the	O
desired	O
cdf	O
;	O
rho	O
is	O
the	O
desired	O
correlation	O
.	O
the	O
values	O
of	O
x	O
are	O
gaussian	O
;	O
transform	O
converts	O
them	O
to	O
the	O
desired	O
distribution	B
.	O
the	O
ﬁrst	O
value	O
of	O
x	O
is	O
gaussian	O
with	O
mean	O
0	O
and	O
standard	O
deviation	O
1.	O
for	O
subsequent	O
values	O
,	O
the	O
mean	O
and	O
standard	O
deviation	O
depend	O
on	O
the	O
previ-	O
ous	O
value	O
.	O
given	O
the	O
previous	O
x	O
,	O
the	O
mean	O
of	O
the	O
next	O
value	O
is	O
x	O
*	O
rho	O
,	O
and	O
the	O
variance	O
is	O
1	O
-	O
rho**2.	O
transform	O
maps	O
from	O
each	O
gaussian	O
value	O
,	O
x	O
,	O
to	O
a	O
value	O
from	O
the	O
given	O
cdf	O
,	O
y.	O
def	O
transform	O
(	O
x	O
)	O
:	O
p	O
=	O
thinkbayes.gaussiancdf	O
(	O
x	O
)	O
y	O
=	O
cdf.value	O
(	O
p	O
)	O
return	O
y	O
gaussiancdf	O
computes	O
the	O
cdf	O
of	O
the	O
standard	O
gaussian	O
distribution	B
at	O
x	O
,	O
returning	O
a	O
cumulative	B
probability	I
.	O
cdf.value	O
maps	O
from	O
a	O
cumulative	B
probability	I
to	O
the	O
corresponding	O
value	O
in	O
cdf	O
.	O
depending	O
on	O
the	O
shape	O
of	O
cdf	O
,	O
information	O
can	O
be	O
lost	O
in	O
transformation	O
,	O
so	O
the	O
actual	O
correlation	O
might	O
be	O
lower	O
than	O
rho	O
.	O
for	O
example	O
,	O
when	O
i	O
gen-	O
erate	O
10000	O
values	O
from	O
the	O
distribution	B
of	O
growth	O
rates	O
with	O
rho=0.4	O
,	O
the	O
actual	O
correlation	O
is	O
0.37.	O
but	O
since	O
we	O
are	O
guessing	O
at	O
the	O
right	O
correlation	O
anyway	O
,	O
that	O
’	O
s	O
close	O
enough	O
.	O
remember	O
that	O
makesequence	O
takes	O
an	O
iterator	B
as	O
an	O
argument	O
.	O
that	O
inter-	O
face	O
allows	O
it	O
to	O
work	O
with	O
different	O
generators	O
:	O
156	O
chapter	O
13.	O
simulation	B
serial	O
correlation	O
0.0	O
0.4	O
diameter	O
(	O
cm	O
)	O
6.0	O
6.0	O
percentiles	O
of	O
age	O
5th	O
25th	O
50th	O
75th	O
95th	O
10.7	O
30.2	O
36.9	O
9.4	O
15.4	O
15.4	O
19.5	O
20.8	O
23.5	O
26.2	O
table	O
13.1	O
:	O
percentiles	O
of	O
tumor	O
age	O
conditioned	O
on	O
size	O
.	O
iterator	B
=	O
uncorrelatedgenerator	O
(	O
cdf	O
)	O
seq1	O
=	O
makesequence	O
(	O
iterator	B
)	O
iterator	B
=	O
correlatedgenerator	O
(	O
cdf	O
,	O
rho	O
)	O
seq2	O
=	O
makesequence	O
(	O
iterator	B
)	O
in	O
this	O
example	O
,	O
seq1	O
and	O
seq2	O
are	O
drawn	O
from	O
the	O
same	O
distribution	B
,	O
but	O
the	O
values	O
in	O
seq1	O
are	O
uncorrelated	O
and	O
the	O
values	O
in	O
seq2	O
are	O
correlated	O
with	O
a	O
coefﬁcient	O
of	O
approximately	O
rho	O
.	O
now	O
we	O
can	O
see	O
what	O
effect	O
serial	B
correlation	I
has	O
on	O
the	O
results	O
;	O
the	O
follow-	O
ing	O
table	O
shows	O
percentiles	O
of	O
age	O
for	O
a	O
6	O
cm	O
tumor	O
,	O
using	O
the	O
uncorrelated	O
generator	B
and	O
a	O
correlated	O
generator	O
with	O
target	O
ρ	O
=	O
0.4.	O
correlation	O
makes	O
the	O
fastest	O
growing	O
tumors	O
faster	O
and	O
the	O
slowest	O
slower	O
,	O
so	O
the	O
range	O
of	O
ages	O
is	O
wider	O
.	O
the	O
difference	O
is	O
modest	O
for	O
low	O
percentiles	O
,	O
but	O
for	O
the	O
95th	O
percentile	B
it	O
is	O
more	O
than	O
6	O
years	O
.	O
to	O
compute	O
these	O
per-	O
centiles	O
precisely	O
,	O
we	O
would	O
need	O
a	O
better	O
estimate	O
of	O
the	O
actual	O
serial	O
cor-	O
relation	O
.	O
however	O
,	O
this	O
model	O
is	O
sufﬁcient	O
to	O
answer	O
the	O
question	O
we	O
started	O
with	O
:	O
given	O
a	O
tumor	O
with	O
a	O
linear	O
dimension	O
of	O
15.5	O
cm	O
,	O
what	O
is	O
the	O
probability	B
that	O
it	O
formed	O
more	O
than	O
8	O
years	O
ago	O
?	O
here	O
’	O
s	O
the	O
code	O
:	O
#	O
class	O
cache	B
def	O
probolder	O
(	O
self	O
,	O
cm	O
,	O
age	O
)	O
:	O
bucket	B
=	O
cmtobucket	O
(	O
cm	O
)	O
cdf	O
=	O
self.conditionalcdf	O
(	O
bucket	B
)	O
p	O
=	O
cdf.prob	O
(	O
age	O
)	O
return	O
1-p	O
cm	O
is	O
the	O
size	O
of	O
the	O
tumor	O
;	O
age	O
is	O
the	O
age	O
threshold	O
in	O
years	O
.	O
probolder	O
converts	O
size	O
to	O
a	O
bucket	B
number	O
,	O
gets	O
the	O
cdf	O
of	O
age	O
conditioned	O
on	O
bucket	B
,	O
and	O
computes	O
the	O
probability	B
that	O
age	O
exceeds	O
the	O
given	O
value	O
.	O
13.8.	O
discussion	O
157	O
with	O
no	O
serial	B
correlation	I
,	O
the	O
probability	B
that	O
a	O
15.5	O
cm	O
tumor	O
is	O
older	O
than	O
8	O
years	O
is	O
0.999	O
,	O
or	O
almost	O
certain	O
.	O
with	O
correlation	O
0.4	O
,	O
faster-growing	O
tu-	O
mors	O
are	O
more	O
likely	O
,	O
but	O
the	O
probability	B
is	O
still	O
0.995.	O
even	O
with	O
correlation	O
0.8	O
,	O
the	O
probability	B
is	O
0.978.	O
another	O
likely	O
source	O
of	O
error	B
is	O
the	O
assumption	O
that	O
tumors	O
are	O
approx-	O
imately	O
spherical	O
.	O
for	O
a	O
tumor	O
with	O
linear	O
dimensions	O
15.5	O
x	O
15	O
cm	O
,	O
this	O
assumption	O
is	O
probably	O
not	O
valid	O
.	O
if	O
,	O
as	O
seems	O
likely	O
,	O
a	O
tumor	O
this	O
size	O
is	O
relatively	O
ﬂat	O
,	O
it	O
might	O
have	O
the	O
same	O
volume	B
as	O
a	O
6	O
cm	O
sphere	B
.	O
with	O
this	O
smaller	O
volume	B
and	O
correlation	O
0.8	O
,	O
the	O
probability	B
of	O
age	O
greater	O
than	O
8	O
is	O
still	O
95	O
%	O
.	O
so	O
even	O
taking	O
into	O
account	O
modeling	B
errors	O
,	O
it	O
is	O
unlikely	O
that	O
such	O
a	O
large	O
tumor	O
could	O
have	O
formed	O
less	O
than	O
8	O
years	O
prior	B
to	O
the	O
date	O
of	O
diagnosis	O
.	O
13.8	O
discussion	O
well	O
,	O
we	O
got	O
through	O
a	O
whole	O
chapter	O
without	O
using	O
bayes	O
’	O
s	O
theorem	O
or	O
the	O
suite	B
class	O
that	O
encapsulates	O
bayesian	O
updates	O
.	O
what	O
happened	O
?	O
one	O
way	O
to	O
think	O
about	O
bayes	O
’	O
s	O
theorem	O
is	O
as	O
an	O
algorithm	O
for	O
inverting	O
conditional	B
probabilities	O
.	O
given	O
p	O
(	O
b|a	O
)	O
,	O
we	O
can	O
compute	O
p	O
(	O
a|b	O
)	O
,	O
provided	O
we	O
know	O
p	O
(	O
a	O
)	O
and	O
p	O
(	O
b	O
)	O
.	O
of	O
course	O
this	O
algorithm	O
is	O
only	O
useful	O
if	O
,	O
for	O
some	O
reason	O
,	O
it	O
is	O
easier	O
to	O
compute	O
p	O
(	O
b|a	O
)	O
than	O
p	O
(	O
a|b	O
)	O
.	O
in	O
this	O
example	O
,	O
it	O
is	O
.	O
by	O
running	O
simulations	O
,	O
we	O
can	O
estimate	O
the	O
distri-	O
bution	O
of	O
size	O
conditioned	O
on	O
age	O
,	O
or	O
p	O
(	O
size|age	O
)	O
.	O
but	O
it	O
is	O
harder	O
to	O
get	O
the	O
distribution	B
of	O
age	O
conditioned	O
on	O
size	O
,	O
or	O
p	O
(	O
age|size	O
)	O
.	O
so	O
this	O
seems	O
like	O
a	O
perfect	O
opportunity	O
to	O
use	O
bayes	O
’	O
s	O
theorem	O
.	O
the	O
reason	O
i	O
didn	O
’	O
t	O
is	O
computational	O
efﬁciency	O
.	O
to	O
estimate	O
p	O
(	O
size|age	O
)	O
for	O
any	O
given	O
size	O
,	O
you	O
have	O
to	O
run	O
a	O
lot	O
of	O
simulations	O
.	O
along	O
the	O
way	O
,	O
you	O
end	O
up	O
computing	O
p	O
(	O
size|age	O
)	O
for	O
a	O
lot	O
of	O
sizes	O
.	O
in	O
fact	O
,	O
you	O
end	O
up	O
computing	O
the	O
entire	O
joint	B
distribution	I
of	O
size	O
and	O
age	O
,	O
p	O
(	O
size	O
,	O
age	O
)	O
.	O
and	O
once	O
you	O
have	O
the	O
joint	B
distribution	I
,	O
you	O
don	O
’	O
t	O
really	O
need	O
bayes	O
’	O
s	O
the-	O
orem	O
,	O
you	O
can	O
extract	O
p	O
(	O
age|size	O
)	O
by	O
taking	O
slices	O
from	O
the	O
joint	B
distribution	I
,	O
as	O
demonstrated	O
in	O
conditionalcdf	O
.	O
so	O
we	O
side-stepped	O
bayes	O
,	O
but	O
he	O
was	O
with	O
us	O
in	O
spirit	O
.	O
158	O
chapter	O
13.	O
simulation	B
chapter	O
14	O
a	O
hierarchical	B
model	I
14.1	O
the	O
geiger	O
counter	O
problem	O
i	O
got	O
the	O
idea	O
for	O
the	O
following	O
problem	O
from	O
tom	O
campbell-ricketts	O
,	O
author	O
of	O
the	O
maximum	B
entropy	O
blog	O
at	O
http	O
:	O
//maximum-entropy-blog	O
.	O
blogspot.com	O
.	O
and	O
he	O
got	O
the	O
idea	O
from	O
e.	O
t.	O
jaynes	O
,	O
author	O
of	O
the	O
classic	O
probability	B
theory	O
:	O
the	O
logic	O
of	O
science	O
:	O
suppose	O
that	O
a	O
radioactive	O
source	O
emits	O
particles	O
toward	O
a	O
geiger	O
counter	O
at	O
an	O
average	O
rate	O
of	O
r	O
particles	O
per	O
second	O
,	O
but	O
the	O
counter	O
only	O
registers	O
a	O
fraction	O
,	O
f	O
,	O
of	O
the	O
particles	O
that	O
hit	O
it	O
.	O
if	O
f	O
is	O
10	O
%	O
and	O
the	O
counter	O
registers	O
15	O
particles	O
in	O
a	O
one	O
sec-	O
ond	O
interval	O
,	O
what	O
is	O
the	O
posterior	B
distribution	I
of	O
n	O
,	O
the	O
actual	O
number	O
of	O
particles	O
that	O
hit	O
the	O
counter	O
,	O
and	O
r	O
,	O
the	O
average	O
rate	O
particles	O
are	O
emitted	O
?	O
to	O
get	O
started	O
on	O
a	O
problem	O
like	O
this	O
,	O
think	O
about	O
the	O
chain	O
of	O
causation	B
that	O
starts	O
with	O
the	O
parameters	O
of	O
the	O
system	O
and	O
ends	O
with	O
the	O
observed	O
data	O
:	O
1.	O
the	O
source	O
emits	O
particles	O
at	O
an	O
average	O
rate	O
,	O
r.	O
2.	O
during	O
any	O
given	O
second	O
,	O
the	O
source	O
emits	O
n	O
particles	O
toward	O
the	O
counter	O
.	O
3.	O
out	O
of	O
those	O
n	O
particles	O
,	O
some	O
number	O
,	O
k	O
,	O
get	O
counted	O
.	O
the	O
probability	B
that	O
an	O
atom	O
decays	O
is	O
the	O
same	O
at	O
any	O
point	O
in	O
time	O
,	O
so	O
radioactive	B
decay	I
is	O
well	O
modeled	O
by	O
a	O
poisson	O
process	B
.	O
given	O
r	O
,	O
the	O
distri-	O
bution	O
of	O
n	O
is	O
poisson	O
distribution	B
with	O
parameter	B
r.	O
160	O
chapter	O
14.	O
a	O
hierarchical	B
model	I
figure	O
14.1	O
:	O
posterior	B
distribution	I
of	O
n	O
for	O
three	O
values	O
of	O
r.	O
and	O
if	O
we	O
assume	O
that	O
the	O
probability	B
of	O
detection	O
for	O
each	O
particle	O
is	O
inde-	O
pendent	O
of	O
the	O
others	O
,	O
the	O
distribution	B
of	O
k	O
is	O
the	O
binomial	B
distribution	I
with	O
parameters	O
n	O
and	O
f	O
.	O
given	O
the	O
parameters	O
of	O
the	O
system	O
,	O
we	O
can	O
ﬁnd	O
the	O
distribution	B
of	O
the	O
data	O
.	O
so	O
we	O
can	O
solve	O
what	O
is	O
called	O
the	O
forward	B
problem	I
.	O
now	O
we	O
want	O
to	O
go	O
the	O
other	O
way	O
:	O
given	O
the	O
data	O
,	O
we	O
want	O
the	O
distribution	B
of	O
the	O
parameters	O
.	O
this	O
is	O
called	O
the	O
inverse	B
problem	I
.	O
and	O
if	O
you	O
can	O
solve	O
the	O
forward	B
problem	I
,	O
you	O
can	O
use	O
bayesian	O
methods	O
to	O
solve	O
the	O
inverse	B
problem	I
.	O
14.2	O
start	O
simple	O
let	O
’	O
s	O
start	O
with	O
a	O
simple	O
version	O
of	O
the	O
problem	O
where	O
we	O
know	O
the	O
value	O
of	O
r.	O
we	O
are	O
given	O
the	O
value	O
of	O
f	O
,	O
so	O
all	O
we	O
have	O
to	O
do	O
is	O
estimate	O
n.	O
i	O
deﬁne	O
a	O
suite	B
called	O
detector	O
that	O
models	O
the	O
behavior	O
of	O
the	O
detector	O
and	O
estimates	O
n.	O
class	O
detector	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
r	O
,	O
f	O
,	O
high=500	O
,	O
step=1	O
)	O
:	O
pmf	O
=	O
thinkbayes.makepoissonpmf	O
(	O
r	O
,	O
high	O
,	O
step=step	O
)	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
pmf	O
,	O
name=r	O
)	O
self.r	O
=	O
r	O
self.f	O
=	O
f	O
0100200300400500number	O
of	O
particles	O
(	O
n	O
)	O
0.0000.0050.0100.0150.0200.0250.0300.0350.0400.045pmf100250400	O
14.3.	O
make	O
it	O
hierarchical	O
161	O
if	O
the	O
average	O
emission	O
rate	O
is	O
r	O
particles	O
per	O
second	O
,	O
the	O
distribution	B
of	O
n	O
is	O
poisson	O
with	O
parameter	B
r.	O
high	O
and	O
step	O
determine	O
the	O
upper	O
bound	O
for	O
n	O
and	O
the	O
step	O
size	O
between	O
hypothetical	O
values	O
.	O
now	O
we	O
need	O
a	O
likelihood	B
function	I
:	O
#	O
class	O
detector	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
k	O
=	O
data	O
n	O
=	O
hypo	O
p	O
=	O
self.f	O
return	O
thinkbayes.evalbinomialpmf	O
(	O
k	O
,	O
n	O
,	O
p	O
)	O
data	O
is	O
the	O
number	O
of	O
particles	O
detected	O
,	O
and	O
hypo	O
is	O
the	O
hypothetical	O
num-	O
ber	O
of	O
particles	O
emitted	O
,	O
n.	O
if	O
there	O
are	O
actually	O
n	O
particles	O
,	O
and	O
the	O
probability	B
of	O
detecting	O
any	O
one	O
of	O
them	O
is	O
f	O
,	O
the	O
probability	B
of	O
detecting	O
k	O
particles	O
is	O
given	O
by	O
the	O
binomial	B
distribution	I
.	O
that	O
’	O
s	O
it	O
for	O
the	O
detector	O
.	O
we	O
can	O
try	O
it	O
out	O
for	O
a	O
range	O
of	O
values	O
of	O
r	O
:	O
f	O
=	O
0.1	O
k	O
=	O
15	O
for	O
r	O
in	O
[	O
100	O
,	O
250	O
,	O
400	O
]	O
:	O
suite	B
=	O
detector	O
(	O
r	O
,	O
f	O
,	O
step=1	O
)	O
suite.update	O
(	O
k	O
)	O
print	O
suite.maximumlikelihood	O
(	O
)	O
figure	O
14.1	O
shows	O
the	O
posterior	B
distribution	I
of	O
n	O
for	O
several	O
given	O
values	O
of	O
r.	O
14.3	O
make	O
it	O
hierarchical	O
in	O
the	O
previous	O
section	O
,	O
we	O
assume	O
r	O
is	O
known	O
.	O
now	O
let	O
’	O
s	O
relax	O
that	O
assump-	O
tion	O
.	O
i	O
deﬁne	O
another	O
suite	B
,	O
called	O
emitter	O
,	O
that	O
models	O
the	O
behavior	O
of	O
the	O
emitter	O
and	O
estimates	O
r	O
:	O
class	O
emitter	O
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
rs	O
,	O
f=0.1	O
)	O
:	O
detectors	O
=	O
[	O
detector	O
(	O
r	O
,	O
f	O
)	O
for	O
r	O
in	O
rs	O
]	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
detectors	O
)	O
162	O
chapter	O
14.	O
a	O
hierarchical	B
model	I
rs	O
is	O
a	O
sequence	O
of	O
hypothetical	O
value	O
for	O
r.	O
detectors	O
is	O
a	O
sequence	O
of	O
detector	O
objects	O
,	O
one	O
for	O
each	O
value	O
of	O
r.	O
the	O
values	O
in	O
the	O
suite	B
are	O
detec-	O
tors	O
,	O
so	O
emitter	O
is	O
a	O
meta-suite	O
;	O
that	O
is	O
,	O
a	O
suite	B
that	O
contains	O
other	O
suites	O
as	O
values	O
.	O
to	O
update	O
the	O
emitter	O
,	O
we	O
have	O
to	O
compute	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
each	O
hypothetical	O
value	O
of	O
r.	O
but	O
each	O
value	O
of	O
r	O
is	O
represented	O
by	O
a	O
detector	O
that	O
contains	O
a	O
range	O
of	O
values	O
for	O
n.	O
to	O
compute	O
the	O
likelihood	B
of	O
the	O
data	O
for	O
a	O
given	O
detector	O
,	O
we	O
loop	O
through	O
the	O
values	O
of	O
n	O
and	O
add	O
up	O
the	O
total	B
probability	I
of	O
k.	O
that	O
’	O
s	O
what	O
suitelikelihood	O
does	O
:	O
#	O
class	O
detector	O
def	O
suitelikelihood	O
(	O
self	O
,	O
data	O
)	O
:	O
total	O
=	O
0	O
for	O
hypo	O
,	O
prob	O
in	O
self.items	O
(	O
)	O
:	O
like	O
=	O
self.likelihood	O
(	O
data	O
,	O
hypo	O
)	O
total	O
+=	O
prob	O
*	O
like	O
return	O
total	O
now	O
we	O
can	O
write	O
the	O
likelihood	B
function	I
for	O
the	O
emitter	O
:	O
#	O
class	O
emitter	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
detector	O
=	O
hypo	O
like	O
=	O
detector.suitelikelihood	O
(	O
data	O
)	O
return	O
like	O
each	O
hypo	O
is	O
a	O
detector	O
,	O
so	O
we	O
can	O
invoke	O
suitelikelihood	O
to	O
get	O
the	O
likeli-	O
hood	O
of	O
the	O
data	O
under	O
the	O
hypothesis	O
.	O
after	O
we	O
update	O
the	O
emitter	O
,	O
we	O
have	O
to	O
update	O
each	O
of	O
the	O
detectors	O
,	O
too	O
.	O
#	O
class	O
emitter	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
thinkbayes.suite.update	O
(	O
self	O
,	O
data	O
)	O
for	O
detector	O
in	O
self.values	O
(	O
)	O
:	O
detector.update	O
(	O
)	O
a	O
model	O
like	O
this	O
,	O
with	O
multiple	O
levels	O
of	O
suites	O
,	O
is	O
called	O
hierarchical	O
.	O
14.4.	O
a	O
little	O
optimization	B
163	O
figure	O
14.2	O
:	O
posterior	B
distributions	O
of	O
n	O
and	O
r.	O
14.4	O
a	O
little	O
optimization	B
you	O
might	O
recognize	O
suitelikelihood	O
;	O
we	O
saw	O
it	O
in	O
section	O
11.2.	O
at	O
the	O
time	O
,	O
i	O
pointed	O
out	O
that	O
we	O
didn	O
’	O
t	O
really	O
need	O
it	O
,	O
because	O
the	O
total	O
prob-	O
ability	O
computed	O
by	O
suitelikelihood	O
is	O
exactly	O
the	O
normalizing	B
constant	I
computed	O
and	O
returned	O
by	O
update	O
.	O
so	O
instead	O
of	O
updating	O
the	O
emitter	O
and	O
then	O
updating	O
the	O
detectors	O
,	O
we	O
can	O
do	O
both	O
steps	O
at	O
the	O
same	O
time	O
,	O
using	O
the	O
result	O
from	O
detector.update	O
as	O
the	O
likelihood	B
of	O
emitter	O
.	O
here	O
’	O
s	O
the	O
streamlined	O
version	O
of	O
emitter.likelihood	O
:	O
#	O
class	O
emitter	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
return	O
hypo.update	O
(	O
data	O
)	O
and	O
with	O
this	O
version	O
of	O
likelihood	B
we	O
can	O
use	O
the	O
default	O
version	O
of	O
update	O
.	O
so	O
this	O
version	O
has	O
fewer	O
lines	O
of	O
code	O
,	O
and	O
it	O
runs	O
faster	O
because	O
it	O
does	O
not	O
compute	O
the	O
normalizing	B
constant	I
twice	O
.	O
14.5	O
extracting	O
the	O
posteriors	O
after	O
we	O
update	O
the	O
emitter	O
,	O
we	O
can	O
get	O
the	O
posterior	B
distribution	I
of	O
r	O
by	O
looping	O
through	O
the	O
detectors	O
and	O
their	O
probabilities	O
:	O
0100200300400500emission	O
rate0.000.010.020.030.040.050.06pmfposterior	O
rposterior	O
n	O
164	O
chapter	O
14.	O
a	O
hierarchical	B
model	I
#	O
class	O
emitter	O
def	O
distofr	O
(	O
self	O
)	O
:	O
items	O
=	O
[	O
(	O
detector.r	O
,	O
prob	O
)	O
for	O
detector	O
,	O
prob	O
in	O
self.items	O
(	O
)	O
]	O
return	O
thinkbayes.makepmffromitems	O
(	O
items	O
)	O
items	O
is	O
a	O
list	O
of	O
values	O
of	O
r	O
and	O
their	O
probabilities	O
.	O
the	O
result	O
is	O
the	O
pmf	O
of	O
r.	O
to	O
get	O
the	O
posterior	B
distribution	I
of	O
n	O
,	O
we	O
have	O
to	O
compute	O
the	O
mixture	B
of	O
the	O
detectors	O
.	O
we	O
can	O
use	O
thinkbayes.makemixture	O
,	O
which	O
takes	O
a	O
meta-	O
pmf	O
that	O
maps	O
from	O
each	O
distribution	B
to	O
its	O
probability	B
.	O
and	O
that	O
’	O
s	O
exactly	O
what	O
the	O
emitter	O
is	O
:	O
#	O
class	O
emitter	O
def	O
distofn	O
(	O
self	O
)	O
:	O
return	O
thinkbayes.makemixture	O
(	O
self	O
)	O
figure	O
14.2	O
shows	O
the	O
results	O
.	O
not	O
surprisingly	O
,	O
the	O
most	O
likely	O
value	O
for	O
n	O
is	O
150.	O
given	O
f	O
and	O
n	O
,	O
the	O
expected	O
count	O
is	O
k	O
=	O
f	O
n	O
,	O
so	O
given	O
f	O
and	O
k	O
,	O
the	O
expected	O
value	O
of	O
n	O
is	O
k/	O
f	O
,	O
which	O
is	O
150.	O
and	O
if	O
150	O
particles	O
are	O
emitted	O
in	O
one	O
second	O
,	O
the	O
most	O
likely	O
value	O
of	O
r	O
is	O
150	O
particles	O
per	O
second	O
.	O
so	O
the	O
posterior	B
distribution	I
of	O
r	O
is	O
also	O
centered	O
on	O
150.	O
the	O
posterior	B
distributions	O
of	O
r	O
and	O
n	O
are	O
similar	O
;	O
the	O
only	O
difference	O
is	O
that	O
we	O
are	O
slightly	O
less	O
certain	O
about	O
n.	O
in	O
general	O
,	O
we	O
can	O
be	O
more	O
certain	O
about	O
the	O
long-range	O
emission	O
rate	O
,	O
r	O
,	O
than	O
about	O
the	O
number	O
of	O
particles	O
emitted	O
in	O
any	O
particular	O
second	O
,	O
n.	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
jaynes.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
14.6	O
discussion	O
the	O
geiger	O
counter	O
problem	O
demonstrates	O
the	O
connection	O
between	O
causa-	O
tion	O
and	O
hierarchical	O
modeling	O
.	O
in	O
the	O
example	O
,	O
the	O
emission	O
rate	O
r	O
has	O
a	O
causal	O
effect	O
on	O
the	O
number	O
of	O
particles	O
,	O
n	O
,	O
which	O
has	O
a	O
causal	O
effect	O
on	O
the	O
particle	O
count	O
,	O
k.	O
the	O
hierarchical	B
model	I
reﬂects	O
the	O
structure	O
of	O
the	O
system	O
,	O
with	O
causes	O
at	O
the	O
top	O
and	O
effects	O
at	O
the	O
bottom	O
.	O
14.7.	O
exercises	O
165	O
1.	O
at	O
the	O
top	O
level	O
,	O
we	O
start	O
with	O
a	O
range	O
of	O
hypothetical	O
values	O
for	O
r.	O
2.	O
for	O
each	O
value	O
of	O
r	O
,	O
we	O
have	O
a	O
range	O
of	O
values	O
for	O
n	O
,	O
and	O
the	O
prior	B
distribution	I
of	O
n	O
depends	O
on	O
r.	O
3.	O
when	O
we	O
update	O
the	O
model	O
,	O
we	O
go	O
bottom-up	O
.	O
we	O
compute	O
a	O
poste-	O
rior	O
distribution	B
of	O
n	O
for	O
each	O
value	O
of	O
r	O
,	O
then	O
compute	O
the	O
posterior	B
distribution	I
of	O
r.	O
so	O
causal	O
information	O
ﬂows	O
down	O
the	O
hierarchy	O
,	O
and	O
inference	O
ﬂows	O
up	O
.	O
14.7	O
exercises	O
exercise	O
14.1.	O
this	O
exercise	O
is	O
also	O
inspired	O
by	O
an	O
example	O
in	O
jaynes	O
,	O
probability	B
theory	O
.	O
suppose	O
you	O
buy	O
a	O
mosquito	O
trap	O
that	O
is	O
supposed	O
to	O
reduce	O
the	O
population	O
of	O
mosquitoes	O
near	O
your	O
house	O
.	O
each	O
week	O
,	O
you	O
empty	O
the	O
trap	O
and	O
count	O
the	O
num-	O
ber	O
of	O
mosquitoes	O
captured	O
.	O
after	O
the	O
ﬁrst	O
week	O
,	O
you	O
count	O
30	O
mosquitoes	O
.	O
after	O
the	O
second	O
week	O
,	O
you	O
count	O
20	O
mosquitoes	O
.	O
estimate	O
the	O
percentage	O
change	O
in	O
the	O
number	O
of	O
mosquitoes	O
in	O
your	O
yard	O
.	O
to	O
answer	O
this	O
question	O
,	O
you	O
have	O
to	O
make	O
some	O
modeling	B
decisions	O
.	O
here	O
are	O
some	O
suggestions	O
:	O
•	O
suppose	O
that	O
each	O
week	O
a	O
large	O
number	O
of	O
mosquitoes	O
,	O
n	O
,	O
is	O
bred	O
in	O
a	O
wetland	O
near	O
your	O
home	O
.	O
•	O
during	O
the	O
week	O
,	O
some	O
fraction	O
of	O
them	O
,	O
f1	O
,	O
wander	O
into	O
your	O
yard	O
,	O
and	O
of	O
those	O
some	O
fraction	O
,	O
f2	O
,	O
are	O
caught	O
in	O
the	O
trap	O
.	O
•	O
your	O
solution	O
should	O
take	O
into	O
account	O
your	O
prior	B
belief	O
about	O
how	O
much	O
n	O
is	O
likely	O
to	O
change	O
from	O
one	O
week	O
to	O
the	O
next	O
.	O
you	O
can	O
do	O
that	O
by	O
adding	O
a	O
level	O
to	O
the	O
hierarchy	O
to	O
model	O
the	O
percent	O
change	O
in	O
n.	O
166	O
chapter	O
14.	O
a	O
hierarchical	B
model	I
chapter	O
15	O
dealing	O
with	O
dimensions	O
15.1	O
belly	B
button	I
bacteria	O
belly	B
button	I
biodiversity	O
2.0	O
(	O
bbb2	O
)	O
is	O
a	O
nation-wide	O
citizen	O
science	O
project	O
with	O
the	O
goal	O
of	O
identifying	O
bacterial	O
species	B
that	O
can	O
be	O
found	O
in	O
human	O
navels	O
(	O
http	O
:	O
//bbdata.yourwildlife.org	O
)	O
.	O
the	O
project	O
might	O
seem	O
whim-	O
sical	O
,	O
but	O
it	O
is	O
part	O
of	O
an	O
increasing	O
interest	O
in	O
the	O
human	O
microbiome	B
,	O
the	O
set	O
of	O
microorganisms	O
that	O
live	O
on	O
human	O
skin	O
and	O
parts	O
of	O
the	O
body	O
.	O
in	O
their	O
pilot	O
study	O
,	O
bbb2	O
researchers	O
collected	O
swabs	O
from	O
the	O
navels	O
of	O
60	O
volunteers	O
,	O
used	O
multiplex	O
pyrosequencing	B
to	O
extract	O
and	O
sequence	O
frag-	O
ments	O
of	O
16s	O
rdna	O
,	O
then	O
identiﬁed	O
the	O
species	B
or	O
genus	O
the	O
fragments	O
came	O
from	O
.	O
each	O
identiﬁed	O
fragment	O
is	O
called	O
a	O
“	O
read.	O
”	O
we	O
can	O
use	O
these	O
data	O
to	O
answer	O
several	O
related	O
questions	O
:	O
•	O
based	O
on	O
the	O
number	O
of	O
species	B
observed	O
,	O
can	O
we	O
estimate	O
the	O
total	O
number	O
of	O
species	B
in	O
the	O
environment	O
?	O
•	O
can	O
we	O
estimate	O
the	O
prevalence	B
of	O
each	O
species	B
;	O
that	O
is	O
,	O
the	O
fraction	O
of	O
the	O
total	O
population	O
belonging	O
to	O
each	O
species	B
?	O
•	O
if	O
we	O
are	O
planning	O
to	O
collect	O
additional	O
samples	O
,	O
can	O
we	O
predict	O
how	O
many	O
new	O
species	B
we	O
are	O
likely	O
to	O
discover	O
?	O
•	O
how	O
many	O
additional	O
reads	O
are	O
needed	O
to	O
increase	O
the	O
fraction	O
of	O
ob-	O
served	O
species	B
to	O
a	O
given	O
threshold	O
?	O
these	O
questions	O
make	O
up	O
what	O
is	O
called	O
the	O
unseen	O
species	B
problem	O
.	O
168	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
15.2	O
lions	B
and	I
tigers	I
and	I
bears	I
i	O
’	O
ll	O
start	O
with	O
a	O
simpliﬁed	O
version	O
of	O
the	O
problem	O
where	O
we	O
know	O
that	O
there	O
are	O
exactly	O
three	O
species	B
.	O
let	O
’	O
s	O
call	O
them	O
lions	O
,	O
tigers	O
and	O
bears	O
.	O
suppose	O
we	O
visit	O
a	O
wild	O
animal	O
preserve	O
and	O
see	O
3	O
lions	O
,	O
2	O
tigers	O
and	O
one	O
bear	O
.	O
if	O
we	O
have	O
an	O
equal	O
chance	O
of	O
observing	O
any	O
animal	O
in	O
the	O
preserve	O
,	O
the	O
number	O
of	O
each	O
species	B
we	O
see	O
is	O
governed	O
by	O
the	O
multinomial	B
distribution	I
.	O
if	O
the	O
prevalence	B
of	O
lions	B
and	I
tigers	I
and	I
bears	I
is	O
p_lion	O
and	O
p_tiger	O
and	O
p_bear	O
,	O
the	O
likelihood	B
of	O
seeing	O
3	O
lions	O
,	O
2	O
tigers	O
and	O
one	O
bear	O
is	O
proportional	O
to	O
p_lion**3	O
*	O
p_tiger**2	O
*	O
p_bear**1	O
an	O
approach	O
that	O
is	O
tempting	O
,	O
but	O
not	O
correct	O
,	O
is	O
to	O
use	O
beta	O
distributions	O
,	O
as	O
in	O
section	O
4.5	O
,	O
to	O
describe	O
the	O
prevalence	B
of	O
each	O
species	B
separately	O
.	O
for	O
example	O
,	O
we	O
saw	O
3	O
lions	O
and	O
3	O
non-lions	O
;	O
if	O
we	O
think	O
of	O
that	O
as	O
3	O
“	O
heads	O
”	O
and	O
3	O
“	O
tails	O
,	O
”	O
then	O
the	O
posterior	B
distribution	I
of	O
p_lion	O
is	O
:	O
beta	O
=	O
thinkbayes.beta	O
(	O
)	O
beta.update	O
(	O
(	O
3	O
,	O
3	O
)	O
)	O
print	O
beta.maximumlikelihood	O
(	O
)	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
p_lion	O
is	O
the	O
observed	O
rate	O
,	O
50	O
%	O
.	O
similarly	O
the	O
mles	O
for	O
p_tiger	O
and	O
p_bear	O
are	O
33	O
%	O
and	O
17	O
%	O
.	O
but	O
there	O
are	O
two	O
problems	O
:	O
1.	O
we	O
have	O
implicitly	O
used	O
a	O
prior	B
for	O
each	O
species	B
that	O
is	O
uniform	O
from	O
0	O
to	O
1	O
,	O
but	O
since	O
we	O
know	O
that	O
there	O
are	O
three	O
species	B
,	O
that	O
prior	B
is	O
not	O
correct	O
.	O
the	O
right	O
prior	B
should	O
have	O
a	O
mean	O
of	O
1/3	O
,	O
and	O
there	O
should	O
be	O
zero	O
likelihood	B
that	O
any	O
species	B
has	O
a	O
prevalence	B
of	O
100	O
%	O
.	O
2.	O
the	O
distributions	O
for	O
each	O
species	B
are	O
not	O
independent	O
,	O
because	O
the	O
prevalences	O
have	O
to	O
add	O
up	O
to	O
1.	O
to	O
capture	O
this	O
dependence	B
,	O
we	O
need	O
a	O
joint	B
distribution	I
for	O
the	O
three	O
prevalences	O
.	O
we	O
can	O
use	O
a	O
dirichlet	O
distribution	B
to	O
solve	O
both	O
of	O
these	O
problems	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/dirichlet_distribution	O
)	O
.	O
in	O
the	O
same	O
way	O
we	O
used	O
the	O
beta	B
distribution	I
to	O
describe	O
the	O
distribution	B
of	O
bias	O
for	O
a	O
coin	O
,	O
we	O
can	O
use	O
a	O
dirichlet	O
distribution	B
to	O
describe	O
the	O
joint	B
distribution	I
of	O
p_lion	O
,	O
p_tiger	O
and	O
p_bear	O
.	O
the	O
dirichlet	O
distribution	B
is	O
the	O
multi-dimensional	O
generalization	O
of	O
the	O
beta	B
distribution	I
.	O
instead	O
of	O
two	O
possible	O
outcomes	O
,	O
like	O
heads	O
and	O
tails	O
,	O
15.2.	O
lions	B
and	I
tigers	I
and	I
bears	I
169	O
the	O
dirichlet	O
distribution	B
handles	O
any	O
number	O
of	O
outcomes	O
:	O
in	O
this	O
exam-	O
ple	O
,	O
three	O
species	B
.	O
if	O
there	O
are	O
n	O
outcomes	O
,	O
the	O
dirichlet	O
distribution	B
is	O
described	O
by	O
n	O
parame-	O
ters	O
,	O
written	O
α1	O
through	O
αn	O
.	O
here	O
’	O
s	O
the	O
deﬁnition	O
,	O
from	O
thinkbayes.py	O
,	O
of	O
a	O
class	O
that	O
represents	O
a	O
dirichlet	O
distribution	B
:	O
class	O
dirichlet	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
n	O
)	O
:	O
self.n	O
=	O
n	O
self.params	O
=	O
numpy.ones	O
(	O
n	O
,	O
dtype=numpy.int	O
)	O
n	O
is	O
the	O
number	O
of	O
dimensions	O
;	O
initially	O
the	O
parameters	O
are	O
all	O
1.	O
i	O
use	O
a	O
numpy	B
array	O
to	O
store	O
the	O
parameters	O
so	O
i	O
can	O
take	O
advantage	O
of	O
array	O
opera-	O
tions	O
.	O
given	O
a	O
dirichlet	O
distribution	B
,	O
the	O
marginal	B
distribution	I
for	O
each	O
prevalence	B
is	O
a	O
beta	B
distribution	I
,	O
which	O
we	O
can	O
compute	O
like	O
this	O
:	O
def	O
marginalbeta	O
(	O
self	O
,	O
i	O
)	O
:	O
alpha0	O
=	O
self.params.sum	O
(	O
)	O
alpha	O
=	O
self.params	O
[	O
i	O
]	O
return	O
beta	O
(	O
alpha	O
,	O
alpha0-alpha	O
)	O
i	O
is	O
the	O
index	O
of	O
the	O
marginal	B
distribution	I
we	O
want	O
.	O
alpha0	O
is	O
the	O
sum	O
of	O
the	O
parameters	O
;	O
alpha	O
is	O
the	O
parameter	B
for	O
the	O
given	O
species	B
.	O
in	O
the	O
example	O
,	O
the	O
prior	B
marginal	O
distribution	B
for	O
each	O
species	B
is	O
beta	O
(	O
1	O
,	O
2	O
)	O
.	O
we	O
can	O
compute	O
the	O
prior	B
means	O
like	O
this	O
:	O
dirichlet	O
=	O
thinkbayes.dirichlet	O
(	O
3	O
)	O
for	O
i	O
in	O
range	O
(	O
3	O
)	O
:	O
beta	O
=	O
dirichlet.marginalbeta	O
(	O
i	O
)	O
print	O
beta.mean	O
(	O
)	O
as	O
expected	O
,	O
the	O
prior	B
mean	O
prevalence	B
for	O
each	O
species	B
is	O
1/3	O
.	O
to	O
update	O
the	O
dirichlet	O
distribution	B
,	O
we	O
add	O
the	O
observations	O
to	O
the	O
param-	O
eters	O
like	O
this	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
m	O
=	O
len	O
(	O
data	O
)	O
self.params	O
[	O
:	O
m	O
]	O
+=	O
data	O
here	O
data	O
is	O
a	O
sequence	O
of	O
counts	O
in	O
the	O
same	O
order	O
as	O
params	O
,	O
so	O
in	O
this	O
example	O
,	O
it	O
should	O
be	O
the	O
number	O
of	O
lions	O
,	O
tigers	O
and	O
bears	O
.	O
170	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
figure	O
15.1	O
:	O
distribution	B
of	O
prevalences	O
for	O
three	O
species	B
.	O
data	O
can	O
be	O
shorter	O
than	O
params	O
;	O
in	O
that	O
case	O
there	O
are	O
some	O
species	B
that	O
have	O
not	O
been	O
observed	O
.	O
here	O
’	O
s	O
code	O
that	O
updates	O
dirichlet	O
with	O
the	O
observed	O
data	O
and	O
computes	O
the	O
posterior	B
marginal	O
distributions	O
.	O
data	O
=	O
[	O
3	O
,	O
2	O
,	O
1	O
]	O
dirichlet.update	O
(	O
data	O
)	O
for	O
i	O
in	O
range	O
(	O
3	O
)	O
:	O
beta	O
=	O
dirichlet.marginalbeta	O
(	O
i	O
)	O
pmf	O
=	O
beta.makepmf	O
(	O
)	O
print	O
i	O
,	O
pmf.mean	O
(	O
)	O
figure	O
15.1	O
shows	O
the	O
results	O
.	O
the	O
posterior	B
mean	O
prevalences	O
are	O
44	O
%	O
,	O
33	O
%	O
,	O
and	O
22	O
%	O
.	O
15.3	O
the	O
hierarchical	O
version	O
we	O
have	O
solved	O
a	O
simpliﬁed	O
version	O
of	O
the	O
problem	O
:	O
if	O
we	O
know	O
how	O
many	O
species	B
there	O
are	O
,	O
we	O
can	O
estimate	O
the	O
prevalence	B
of	O
each	O
.	O
now	O
let	O
’	O
s	O
get	O
back	O
to	O
the	O
original	O
problem	O
,	O
estimating	O
the	O
total	O
number	O
of	O
species	B
.	O
to	O
solve	O
this	O
problem	O
i	O
’	O
ll	O
deﬁne	O
a	O
meta-suite	O
,	O
which	O
is	O
a	O
suite	B
that	O
contains	O
other	O
suites	O
as	O
hypotheses	O
.	O
in	O
this	O
case	O
,	O
the	O
top-level	O
suite	B
contains	O
hypotheses	O
about	O
the	O
number	O
of	O
species	B
;	O
the	O
bottom	O
level	O
contains	O
hypothe-	O
ses	O
about	O
prevalences	O
.	O
0.00.20.40.60.81.0prevalence0.0000.0050.0100.0150.0200.0250.0300.035problionstigersbears	O
15.3.	O
the	O
hierarchical	O
version	O
171	O
here	O
’	O
s	O
the	O
class	O
deﬁnition	O
:	O
class	O
species	B
(	O
thinkbayes.suite	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
ns	O
)	O
:	O
hypos	O
=	O
[	O
thinkbayes.dirichlet	O
(	O
n	O
)	O
for	O
n	O
in	O
ns	O
]	O
thinkbayes.suite.__init__	O
(	O
self	O
,	O
hypos	O
)	O
__init__	O
takes	O
a	O
list	O
of	O
possible	O
values	O
for	O
n	O
and	O
makes	O
a	O
list	O
of	O
dirichlet	O
objects	O
.	O
here	O
’	O
s	O
the	O
code	O
that	O
creates	O
the	O
top-level	O
suite	B
:	O
ns	O
=	O
range	O
(	O
3	O
,	O
30	O
)	O
suite	B
=	O
species	B
(	O
ns	O
)	O
ns	O
is	O
the	O
list	O
of	O
possible	O
values	O
for	O
n.	O
we	O
have	O
seen	O
3	O
species	B
,	O
so	O
there	O
have	O
to	O
be	O
at	O
least	O
that	O
many	O
.	O
i	O
chose	O
an	O
upper	O
bound	O
that	O
seems	O
reasonable	O
,	O
but	O
we	O
will	O
check	O
later	O
that	O
the	O
probability	B
of	O
exceeding	O
this	O
bound	O
is	O
low	O
.	O
and	O
at	O
least	O
initially	O
we	O
assume	O
that	O
any	O
value	O
in	O
this	O
range	O
is	O
equally	O
likely	O
.	O
to	O
update	O
a	O
hierarchical	B
model	I
,	O
you	O
have	O
to	O
update	O
all	O
levels	O
.	O
usually	O
you	O
have	O
to	O
update	O
the	O
bottom	O
level	O
ﬁrst	O
and	O
work	O
up	O
,	O
but	O
in	O
this	O
case	O
we	O
can	O
update	O
the	O
top	O
level	O
ﬁrst	O
:	O
#	O
class	O
species	B
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
thinkbayes.suite.update	O
(	O
self	O
,	O
data	O
)	O
for	O
hypo	O
in	O
self.values	O
(	O
)	O
:	O
hypo.update	O
(	O
data	O
)	O
species.update	O
invokes	O
update	O
in	O
the	O
parent	O
class	O
,	O
then	O
loops	O
through	O
the	O
sub-hypotheses	O
and	O
updates	O
them	O
.	O
now	O
all	O
we	O
need	O
is	O
a	O
likelihood	B
function	I
:	O
#	O
class	O
species	B
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
dirichlet	O
=	O
hypo	O
like	O
=	O
0	O
for	O
i	O
in	O
range	O
(	O
1000	O
)	O
:	O
like	O
+=	O
dirichlet.likelihood	O
(	O
data	O
)	O
return	O
like	O
172	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
data	O
is	O
a	O
sequence	O
of	O
observed	O
counts	O
;	O
hypo	O
is	O
a	O
dirichlet	O
object	O
.	O
species.likelihood	O
calls	O
dirichlet.likelihood	O
1000	O
times	O
and	O
returns	O
the	O
total	O
.	O
why	O
call	O
it	O
1000	O
times	O
?	O
because	O
dirichlet.likelihood	O
doesn	O
’	O
t	O
actually	O
compute	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
the	O
whole	O
dirichlet	O
distribution	B
.	O
instead	O
,	O
it	O
draws	O
one	O
sample	O
from	O
the	O
hypothetical	O
distribution	B
and	O
com-	O
putes	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
the	O
sampled	O
set	O
of	O
prevalences	O
.	O
here	O
’	O
s	O
what	O
it	O
looks	O
like	O
:	O
#	O
class	O
dirichlet	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
)	O
:	O
m	O
=	O
len	O
(	O
data	O
)	O
if	O
self.n	O
<	O
m	O
:	O
return	O
0	O
x	O
=	O
data	O
p	O
=	O
self.random	O
(	O
)	O
q	O
=	O
p	O
[	O
:	O
m	O
]	O
**x	O
return	O
q.prod	O
(	O
)	O
the	O
length	O
of	O
data	O
is	O
the	O
number	O
of	O
species	B
observed	O
.	O
if	O
we	O
see	O
more	O
species	B
than	O
we	O
thought	O
existed	O
,	O
the	O
likelihood	B
is	O
0.	O
otherwise	O
we	O
select	O
a	O
random	O
set	O
of	O
prevalences	O
,	O
p	O
,	O
and	O
compute	O
the	O
multi-	O
nomial	O
pmf	O
,	O
which	O
is	O
cx	O
px1	O
1	O
·	O
·	O
·	O
pxn	O
n	O
pi	O
is	O
the	O
prevalence	B
of	O
the	O
ith	O
species	B
,	O
and	O
xi	O
is	O
the	O
observed	O
number	O
.	O
the	O
ﬁrst	O
term	O
,	O
cx	O
,	O
is	O
the	O
multinomial	B
coefﬁcient	I
;	O
i	O
leave	O
it	O
out	O
of	O
the	O
computa-	O
tion	O
because	O
it	O
is	O
a	O
multiplicative	O
factor	O
that	O
depends	O
only	O
on	O
the	O
data	O
,	O
not	O
the	O
hypothesis	O
,	O
so	O
it	O
gets	O
normalized	O
away	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/	O
wiki/multinomial_distribution	O
)	O
.	O
m	O
is	O
the	O
number	O
of	O
observed	O
species	B
.	O
we	O
only	O
need	O
the	O
ﬁrst	O
m	O
elements	O
of	O
p	O
;	O
for	O
the	O
others	O
,	O
xi	O
is	O
0	O
,	O
so	O
pxi	O
is	O
1	O
,	O
and	O
we	O
can	O
leave	O
them	O
out	O
of	O
the	O
product	O
.	O
i	O
15.4	O
random	O
sampling	O
there	O
are	O
two	O
ways	O
to	O
generate	O
a	O
random	B
sample	I
from	O
a	O
dirichlet	O
dis-	O
tribution	O
.	O
one	O
is	O
to	O
use	O
the	O
marginal	O
beta	O
distributions	O
,	O
but	O
in	O
that	O
case	O
you	O
have	O
to	O
select	O
one	O
at	O
a	O
time	O
and	O
scale	O
the	O
rest	O
so	O
they	O
add	O
up	O
to	O
15.4.	O
random	O
sampling	O
173	O
figure	O
15.2	O
:	O
posterior	B
distribution	I
of	O
n.	O
1	O
(	O
see	O
http	O
:	O
//en.wikipedia.org/wiki/dirichlet_distribution	O
#	O
random_	O
number_generation	O
)	O
.	O
a	O
less	O
obvious	O
,	O
but	O
faster	O
,	O
way	O
is	O
to	O
select	O
values	O
from	O
n	O
gamma	O
distribu-	O
tions	O
,	O
then	O
normalize	B
by	O
dividing	O
through	O
by	O
the	O
total	O
.	O
here	O
’	O
s	O
the	O
code	O
:	O
#	O
class	O
dirichlet	O
def	O
random	O
(	O
self	O
)	O
:	O
p	O
=	O
numpy.random.gamma	O
(	O
self.params	O
)	O
return	O
p	O
/	O
p.sum	O
(	O
)	O
now	O
we	O
’	O
re	O
ready	O
to	O
look	O
at	O
some	O
results	O
.	O
here	O
is	O
the	O
code	O
that	O
extracts	O
the	O
posterior	B
distribution	I
of	O
n	O
:	O
def	O
distofn	O
(	O
self	O
)	O
:	O
pmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
for	O
hypo	O
,	O
prob	O
in	O
self.items	O
(	O
)	O
:	O
pmf.set	O
(	O
hypo.n	O
,	O
prob	O
)	O
return	O
pmf	O
distofn	O
iterates	O
through	O
the	O
top-level	O
hypotheses	O
and	O
accumulates	O
the	O
probability	B
of	O
each	O
n.	O
figure	O
15.2	O
shows	O
the	O
result	O
.	O
the	O
most	O
likely	O
value	O
is	O
4.	O
values	O
from	O
3	O
to	O
7	O
are	O
reasonably	O
likely	O
;	O
after	O
that	O
the	O
probabilities	O
drop	O
off	O
quickly	O
.	O
the	O
probability	B
that	O
there	O
are	O
29	O
species	B
is	O
low	O
enough	O
to	O
be	O
negligible	O
;	O
if	O
we	O
chose	O
a	O
higher	O
bound	O
,	O
we	O
would	O
get	O
nearly	O
the	O
same	O
result	O
.	O
051015202530number	O
of	O
species0.000.020.040.060.080.100.12prob	O
174	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
remember	O
that	O
this	O
result	O
is	O
based	O
on	O
a	O
uniform	O
prior	O
for	O
n.	O
if	O
we	O
have	O
background	O
information	O
about	O
the	O
number	O
of	O
species	B
in	O
the	O
environment	O
,	O
we	O
might	O
choose	O
a	O
different	O
prior	B
.	O
15.5	O
optimization	B
i	O
have	O
to	O
admit	O
that	O
i	O
am	O
proud	O
of	O
this	O
example	O
.	O
the	O
unseen	O
species	B
prob-	O
lem	O
is	O
not	O
easy	O
,	O
and	O
i	O
think	O
this	O
solution	O
is	O
simple	O
and	O
clear	O
,	O
and	O
takes	O
sur-	O
prisingly	O
few	O
lines	O
of	O
code	O
(	O
about	O
50	O
so	O
far	O
)	O
.	O
the	O
only	O
problem	O
is	O
that	O
it	O
is	O
slow	O
.	O
it	O
’	O
s	O
good	O
enough	O
for	O
the	O
example	O
with	O
only	O
3	O
observed	O
species	B
,	O
but	O
not	O
good	O
enough	O
for	O
the	O
belly	B
button	I
data	O
,	O
with	O
more	O
than	O
100	O
species	B
in	O
some	O
samples	O
.	O
the	O
next	O
few	O
sections	O
present	O
a	O
series	O
of	O
optimizations	O
we	O
need	O
to	O
make	O
this	O
solution	O
scale	O
.	O
before	O
we	O
get	O
into	O
the	O
details	O
,	O
here	O
’	O
s	O
a	O
road	O
map	O
.	O
•	O
the	O
ﬁrst	O
step	O
is	O
to	O
recognize	O
that	O
if	O
we	O
update	O
the	O
dirichlet	O
distribu-	O
tions	O
with	O
the	O
same	O
data	O
,	O
the	O
ﬁrst	O
m	O
parameters	O
are	O
the	O
same	O
for	O
all	O
of	O
them	O
.	O
the	O
only	O
difference	O
is	O
the	O
number	O
of	O
hypothetical	O
unseen	O
species	B
.	O
so	O
we	O
don	O
’	O
t	O
really	O
need	O
n	O
dirichlet	O
objects	O
;	O
we	O
can	O
store	O
the	O
parameters	O
in	O
the	O
top	O
level	O
of	O
the	O
hierarchy	O
.	O
species2	O
implements	O
this	O
optimization	B
.	O
•	O
species2	O
also	O
uses	O
the	O
same	O
set	O
of	O
random	O
values	O
for	O
all	O
of	O
the	O
hy-	O
potheses	O
.	O
this	O
saves	O
time	O
generating	O
random	O
values	O
,	O
but	O
it	O
has	O
a	O
sec-	O
ond	O
beneﬁt	O
that	O
turns	O
out	O
to	O
be	O
more	O
important	O
:	O
by	O
giving	O
all	O
hypothe-	O
ses	O
the	O
same	O
selection	O
from	O
the	O
sample	O
space	O
,	O
we	O
make	O
the	O
compari-	O
son	O
between	O
the	O
hypotheses	O
more	O
fair	O
,	O
so	O
it	O
takes	O
fewer	O
iterations	O
to	O
converge	O
.	O
•	O
even	O
with	O
these	O
changes	O
there	O
is	O
a	O
major	O
performance	O
problem	O
.	O
as	O
the	O
number	O
of	O
observed	O
species	B
increases	O
,	O
the	O
array	O
of	O
random	O
preva-	O
lences	O
gets	O
bigger	O
,	O
and	O
the	O
chance	O
of	O
choosing	O
one	O
that	O
is	O
approxi-	O
mately	O
right	O
becomes	O
small	O
.	O
so	O
the	O
vast	O
majority	O
of	O
iterations	O
yield	O
small	O
likelihoods	O
that	O
don	O
’	O
t	O
contribute	O
much	O
to	O
the	O
total	O
,	O
and	O
don	O
’	O
t	O
discriminate	O
between	O
hypotheses	O
.	O
the	O
solution	O
is	O
to	O
do	O
the	O
updates	O
one	O
species	B
at	O
a	O
time	O
.	O
species4	O
is	O
a	O
simple	O
implementation	B
of	O
this	O
strategy	O
using	O
dirichlet	O
objects	O
to	O
rep-	O
resent	O
the	O
sub-hypotheses	O
.	O
15.6.	O
collapsing	O
the	O
hierarchy	O
175	O
•	O
finally	O
,	O
species5	O
combines	O
the	O
sub-hypotheses	O
into	O
the	O
top	O
level	O
and	O
uses	O
numpy	B
array	O
operations	B
to	O
speed	O
things	O
up	O
.	O
if	O
you	O
are	O
not	O
interested	O
in	O
the	O
details	O
,	O
feel	O
free	O
to	O
skip	O
to	O
section	O
15.9	O
where	O
we	O
look	O
at	O
results	O
from	O
the	O
belly	B
button	I
data	O
.	O
15.6	O
collapsing	O
the	O
hierarchy	O
all	O
of	O
the	O
bottom-level	O
dirichlet	O
distributions	O
are	O
updated	O
with	O
the	O
same	O
data	O
,	O
so	O
the	O
ﬁrst	O
m	O
parameters	O
are	O
the	O
same	O
for	O
all	O
of	O
them	O
.	O
we	O
can	O
eliminate	O
them	O
and	O
merge	O
the	O
parameters	O
into	O
the	O
top-level	O
suite	B
.	O
species2	O
imple-	O
ments	O
this	O
optimization	B
:	O
class	O
species2	O
(	O
object	O
)	O
:	O
def	O
__init__	O
(	O
self	O
,	O
ns	O
)	O
:	O
self.ns	O
=	O
ns	O
self.probs	O
=	O
numpy.ones	O
(	O
len	O
(	O
ns	O
)	O
,	O
dtype=numpy.double	O
)	O
self.params	O
=	O
numpy.ones	O
(	O
self.high	O
,	O
dtype=numpy.int	O
)	O
ns	O
is	O
the	O
list	O
of	O
hypothetical	O
values	O
for	O
n	O
;	O
probs	O
is	O
the	O
list	O
of	O
corresponding	O
probabilities	O
.	O
and	O
params	O
is	O
the	O
sequence	O
of	O
dirichlet	O
parameters	O
,	O
initially	O
all	O
1.	O
species2.update	O
updates	O
both	O
levels	O
of	O
the	O
hierarchy	O
:	O
ﬁrst	O
the	O
probability	B
for	O
each	O
value	O
of	O
n	O
,	O
then	O
the	O
dirichlet	O
parameters	O
:	O
#	O
class	O
species2	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
like	O
=	O
numpy.zeros	O
(	O
len	O
(	O
self.ns	O
)	O
,	O
dtype=numpy.double	O
)	O
for	O
i	O
in	O
range	O
(	O
1000	O
)	O
:	O
like	O
+=	O
self.samplelikelihood	O
(	O
data	O
)	O
self.probs	O
*=	O
like	O
self.probs	O
/=	O
self.probs.sum	O
(	O
)	O
m	O
=	O
len	O
(	O
data	O
)	O
self.params	O
[	O
:	O
m	O
]	O
+=	O
data	O
samplelikelihood	O
returns	O
an	O
array	O
of	O
likelihoods	O
,	O
one	O
for	O
each	O
value	O
of	O
n.	O
like	O
accumulates	O
the	O
total	O
likelihood	O
for	O
1000	O
samples	O
.	O
self.probs	O
is	O
mul-	O
tiplied	O
by	O
the	O
total	O
likelihood	O
,	O
then	O
normalized	O
.	O
the	O
last	O
two	O
lines	O
,	O
which	O
update	O
the	O
parameters	O
,	O
are	O
the	O
same	O
as	O
in	O
dirichlet.update	O
.	O
176	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
now	O
let	O
’	O
s	O
look	O
at	O
samplelikelihood	O
.	O
there	O
are	O
two	O
opportunities	O
for	O
opti-	O
mization	O
here	O
:	O
•	O
when	O
the	O
hypothetical	O
number	O
of	O
species	B
,	O
n	O
,	O
exceeds	O
the	O
observed	O
number	O
,	O
m	O
,	O
we	O
only	O
need	O
the	O
ﬁrst	O
m	O
terms	O
of	O
the	O
multinomial	O
pmf	O
;	O
the	O
rest	O
are	O
1	O
.	O
•	O
if	O
the	O
number	O
of	O
species	B
is	O
large	O
,	O
the	O
likelihood	B
of	O
the	O
data	O
might	O
be	O
too	O
small	O
for	O
ﬂoating-point	O
(	O
see	O
10.5	O
)	O
.	O
so	O
it	O
is	O
safer	O
to	O
compute	O
log-	O
likelihoods	O
.	O
again	O
,	O
the	O
multinomial	O
pmf	O
is	O
cx	O
px1	O
1	O
·	O
·	O
·	O
pxn	O
n	O
so	O
the	O
log-likelihood	B
is	O
log	O
cx	O
+	O
x1	O
log	O
p1	O
+	O
·	O
·	O
·	O
+	O
xn	O
log	O
pn	O
which	O
is	O
fast	O
and	O
easy	O
to	O
compute	O
.	O
again	O
,	O
cx	O
it	O
is	O
the	O
same	O
for	O
all	O
hypotheses	O
,	O
so	O
we	O
can	O
drop	O
it	O
.	O
here	O
’	O
s	O
the	O
code	O
:	O
#	O
class	O
species2	O
def	O
samplelikelihood	O
(	O
self	O
,	O
data	O
)	O
:	O
gammas	O
=	O
numpy.random.gamma	O
(	O
self.params	O
)	O
m	O
=	O
len	O
(	O
data	O
)	O
row	O
=	O
gammas	O
[	O
:	O
m	O
]	O
col	O
=	O
numpy.cumsum	O
(	O
gammas	O
)	O
log_likes	O
=	O
[	O
]	O
for	O
n	O
in	O
self.ns	O
:	O
ps	O
=	O
row	O
/	O
col	O
[	O
n-1	O
]	O
terms	O
=	O
data	O
*	O
numpy.log	O
(	O
ps	O
)	O
log_like	O
=	O
terms.sum	O
(	O
)	O
log_likes.append	O
(	O
log_like	O
)	O
log_likes	O
-=	O
numpy.max	O
(	O
log_likes	O
)	O
likes	O
=	O
numpy.exp	O
(	O
log_likes	O
)	O
coefs	O
=	O
[	O
thinkbayes.binomialcoef	O
(	O
n	O
,	O
m	O
)	O
for	O
n	O
in	O
self.ns	O
]	O
likes	O
*=	O
coefs	O
15.7.	O
one	O
more	O
problem	O
177	O
return	O
likes	O
gammas	O
is	O
an	O
array	O
of	O
values	O
from	O
a	O
gamma	B
distribution	I
;	O
its	O
length	O
is	O
the	O
largest	O
hypothetical	O
value	O
of	O
n.	O
row	O
is	O
just	O
the	O
ﬁrst	O
m	O
elements	O
of	O
gammas	O
;	O
since	O
these	O
are	O
the	O
only	O
elements	O
that	O
depend	O
on	O
the	O
data	O
,	O
they	O
are	O
the	O
only	O
ones	O
we	O
need	O
.	O
for	O
each	O
value	O
of	O
n	O
we	O
need	O
to	O
divide	O
row	O
by	O
the	O
total	O
of	O
the	O
ﬁrst	O
n	O
values	O
from	O
gamma	O
.	O
cumsum	O
computes	O
these	O
cumulative	O
sums	O
and	O
stores	O
them	O
in	O
col.	O
the	O
loop	O
iterates	O
through	O
the	O
values	O
of	O
n	O
and	O
accumulates	O
a	O
list	O
of	O
log-	O
likelihoods	O
.	O
inside	O
the	O
loop	O
,	O
ps	O
contains	O
the	O
row	O
of	O
probabilities	O
,	O
normalized	O
with	O
the	O
appropriate	O
cumulative	B
sum	I
.	O
terms	O
contains	O
the	O
terms	O
of	O
the	O
summation	O
,	O
xi	O
log	O
pi	O
,	O
and	O
log_like	O
contains	O
their	O
sum	O
.	O
after	O
the	O
loop	O
,	O
we	O
want	O
to	O
convert	O
the	O
log-likelihoods	O
to	O
linear	O
likelihoods	O
,	O
but	O
ﬁrst	O
it	O
’	O
s	O
a	O
good	O
idea	O
to	O
shift	O
them	O
so	O
the	O
largest	O
log-likelihood	B
is	O
0	O
;	O
that	O
way	O
the	O
linear	O
likelihoods	O
are	O
not	O
too	O
small	O
(	O
see	O
10.5	O
)	O
.	O
finally	O
,	O
before	O
we	O
return	O
the	O
likelihood	B
,	O
we	O
have	O
to	O
apply	O
a	O
correction	O
factor	O
,	O
which	O
is	O
the	O
number	O
of	O
ways	O
we	O
could	O
have	O
observed	O
these	O
m	O
species	B
,	O
if	O
the	O
total	O
number	O
of	O
species	B
is	O
n.	O
binomialcoefficient	O
computes	O
“	O
n	O
choose	O
m	O
”	O
,	O
which	O
is	O
written	O
(	O
n	O
m	O
)	O
.	O
as	O
often	O
happens	O
,	O
the	O
optimized	O
version	O
is	O
less	O
readable	O
and	O
more	O
error-	O
prone	O
than	O
the	O
original	O
.	O
but	O
that	O
’	O
s	O
one	O
reason	O
i	O
think	O
it	O
is	O
a	O
good	O
idea	O
to	O
start	O
with	O
the	O
simple	O
version	O
;	O
we	O
can	O
use	O
it	O
for	O
regression	B
testing	I
.	O
i	O
plotted	O
results	O
from	O
both	O
versions	O
and	O
conﬁrmed	O
that	O
they	O
are	O
approximately	O
equal	O
,	O
and	O
that	O
they	O
converge	O
as	O
the	O
number	O
of	O
iterations	O
increases	O
.	O
15.7	O
one	O
more	O
problem	O
there	O
’	O
s	O
more	O
we	O
could	O
do	O
to	O
optimize	O
this	O
code	O
,	O
but	O
there	O
’	O
s	O
another	O
prob-	O
lem	O
we	O
need	O
to	O
ﬁx	O
ﬁrst	O
.	O
as	O
the	O
number	O
of	O
observed	O
species	B
increases	O
,	O
this	O
version	O
gets	O
noisier	O
and	O
takes	O
more	O
iterations	O
to	O
converge	O
on	O
a	O
good	O
answer	O
.	O
the	O
problem	O
is	O
that	O
if	O
the	O
prevalences	O
we	O
choose	O
from	O
the	O
dirichlet	O
distri-	O
bution	O
,	O
the	O
ps	O
,	O
are	O
not	O
at	O
least	O
approximately	O
right	O
,	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
is	O
close	O
to	O
zero	O
and	O
almost	O
equally	O
bad	O
for	O
all	O
values	O
of	O
n.	O
178	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
so	O
most	O
iterations	O
don	O
’	O
t	O
provide	O
any	O
useful	O
contribution	O
to	O
the	O
total	O
likeli-	O
hood	O
.	O
and	O
as	O
the	O
number	O
of	O
observed	O
species	B
,	O
m	O
,	O
gets	O
large	O
,	O
the	O
probability	B
of	O
choosing	O
ps	O
with	O
non-negligible	O
likelihood	B
gets	O
small	O
.	O
really	O
small	O
.	O
fortunately	O
,	O
there	O
is	O
a	O
solution	O
.	O
remember	O
that	O
if	O
you	O
observe	O
a	O
set	O
of	O
data	O
,	O
you	O
can	O
update	O
the	O
prior	B
distribution	I
with	O
the	O
entire	O
dataset	O
,	O
or	O
you	O
can	O
break	O
it	O
up	O
into	O
a	O
series	O
of	O
updates	O
with	O
subsets	O
of	O
the	O
data	O
,	O
and	O
the	O
result	O
is	O
the	O
same	O
either	O
way	O
.	O
for	O
this	O
example	O
,	O
the	O
key	O
is	O
to	O
perform	O
the	O
updates	O
one	O
species	B
at	O
a	O
time	O
.	O
that	O
way	O
when	O
we	O
generate	O
a	O
random	O
set	O
of	O
ps	O
,	O
only	O
one	O
of	O
them	O
affects	O
the	O
computed	O
likelihood	B
,	O
so	O
the	O
chance	O
of	O
choosing	O
a	O
good	O
one	O
is	O
much	O
better	O
.	O
here	O
’	O
s	O
a	O
new	O
version	O
that	O
updates	O
one	O
species	B
at	O
a	O
time	O
:	O
class	O
species4	O
(	O
species	B
)	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
m	O
=	O
len	O
(	O
data	O
)	O
for	O
i	O
in	O
range	O
(	O
m	O
)	O
:	O
one	O
=	O
numpy.zeros	O
(	O
i+1	O
)	O
one	O
[	O
i	O
]	O
=	O
data	O
[	O
i	O
]	O
species.update	O
(	O
self	O
,	O
one	O
)	O
this	O
version	O
inherits	O
__init__	O
from	O
species	B
,	O
so	O
it	O
represents	O
the	O
hypotheses	O
as	O
a	O
list	O
of	O
dirichlet	O
objects	O
(	O
unlike	O
species2	O
)	O
.	O
update	O
loops	O
through	O
the	O
observed	O
species	B
and	O
makes	O
an	O
array	O
,	O
one	O
,	O
with	O
all	O
zeros	O
and	O
one	O
species	B
count	O
.	O
then	O
it	O
calls	O
update	O
in	O
the	O
parent	O
class	O
,	O
which	O
computes	O
the	O
likelihoods	O
and	O
updates	O
the	O
sub-hypotheses	O
.	O
so	O
in	O
the	O
running	O
example	O
,	O
we	O
do	O
three	O
updates	O
.	O
the	O
ﬁrst	O
is	O
something	O
like	O
“	O
i	O
have	O
seen	O
three	O
lions.	O
”	O
the	O
second	O
is	O
“	O
i	O
have	O
seen	O
two	O
tigers	O
and	O
no	O
additional	O
lions.	O
”	O
and	O
the	O
third	O
is	O
“	O
i	O
have	O
seen	O
one	O
bear	O
and	O
no	O
more	O
lions	O
and	O
tigers.	O
”	O
here	O
’	O
s	O
the	O
new	O
version	O
of	O
likelihood	B
:	O
#	O
class	O
species4	O
def	O
likelihood	B
(	O
self	O
,	O
data	O
,	O
hypo	O
)	O
:	O
dirichlet	O
=	O
hypo	O
like	O
=	O
0	O
for	O
i	O
in	O
range	O
(	O
self.iterations	O
)	O
:	O
15.8.	O
we	O
’	O
re	O
not	O
done	O
yet	O
179	O
like	O
+=	O
dirichlet.likelihood	O
(	O
data	O
)	O
#	O
correct	O
for	O
the	O
number	O
of	O
unseen	O
species	B
the	O
new	O
one	O
#	O
could	O
have	O
been	O
m	O
=	O
len	O
(	O
data	O
)	O
num_unseen	O
=	O
dirichlet.n	O
-	O
m	O
+	O
1	O
like	O
*=	O
num_unseen	O
return	O
like	O
this	O
is	O
almost	O
the	O
same	O
as	O
species.likelihood	O
.	O
the	O
difference	O
is	O
the	O
fac-	O
tor	O
,	O
num_unseen	O
.	O
this	O
correction	O
is	O
necessary	O
because	O
each	O
time	O
we	O
see	O
a	O
species	B
for	O
the	O
ﬁrst	O
time	O
,	O
we	O
have	O
to	O
consider	O
that	O
there	O
were	O
some	O
num-	O
ber	O
of	O
other	O
unseen	O
species	B
that	O
we	O
might	O
have	O
seen	O
.	O
for	O
larger	O
values	O
of	O
n	O
there	O
are	O
more	O
unseen	O
species	B
that	O
we	O
could	O
have	O
seen	O
,	O
which	O
increases	O
the	O
likelihood	B
of	O
the	O
data	O
.	O
this	O
is	O
a	O
subtle	O
point	O
and	O
i	O
have	O
to	O
admit	O
that	O
i	O
did	O
not	O
get	O
it	O
right	O
the	O
ﬁrst	O
time	O
.	O
but	O
again	O
i	O
was	O
able	O
to	O
validate	O
this	O
version	O
by	O
comparing	O
it	O
to	O
the	O
previous	O
versions	O
.	O
15.8	O
we	O
’	O
re	O
not	O
done	O
yet	O
performing	O
the	O
updates	O
one	O
species	B
at	O
a	O
time	O
solves	O
one	O
problem	O
,	O
but	O
it	O
creates	O
another	O
.	O
each	O
update	O
takes	O
time	O
proportional	O
to	O
km	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
hypotheses	O
and	O
m	O
is	O
the	O
number	O
of	O
observed	O
species	B
.	O
so	O
if	O
we	O
do	O
m	O
updates	O
,	O
the	O
total	O
run	O
time	O
is	O
proportional	O
to	O
km2	O
.	O
but	O
we	O
can	O
speed	O
things	O
up	O
using	O
the	O
same	O
trick	O
we	O
used	O
in	O
section	O
15.6	O
:	O
we	O
’	O
ll	O
get	O
rid	O
of	O
the	O
dirichlet	O
objects	O
and	O
collapse	O
the	O
two	O
levels	O
of	O
the	O
hier-	O
archy	O
into	O
a	O
single	O
object	O
.	O
so	O
here	O
’	O
s	O
yet	O
another	O
version	O
of	O
species	B
:	O
class	O
species5	O
(	O
species2	O
)	O
:	O
def	O
update	O
(	O
self	O
,	O
data	O
)	O
:	O
m	O
=	O
len	O
(	O
data	O
)	O
for	O
i	O
in	O
range	O
(	O
m	O
)	O
:	O
self.updateone	O
(	O
i+1	O
,	O
data	O
[	O
i	O
]	O
)	O
self.params	O
[	O
i	O
]	O
+=	O
data	O
[	O
i	O
]	O
this	O
version	O
inherits	O
__init__	O
from	O
species2	O
,	O
so	O
it	O
uses	O
ns	O
and	O
probs	O
to	O
represent	O
the	O
distribution	B
of	O
n	O
,	O
and	O
params	O
to	O
represent	O
the	O
parameters	O
of	O
the	O
dirichlet	O
distribution	B
.	O
180	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
update	O
is	O
similar	O
to	O
what	O
we	O
saw	O
in	O
the	O
previous	O
section	O
.	O
it	O
loops	O
through	O
the	O
observed	O
species	B
and	O
calls	O
updateone	O
:	O
#	O
class	O
species5	O
def	O
updateone	O
(	O
self	O
,	O
i	O
,	O
count	O
)	O
:	O
likes	O
=	O
numpy.zeros	O
(	O
len	O
(	O
self.ns	O
)	O
,	O
dtype=numpy.double	O
)	O
for	O
i	O
in	O
range	O
(	O
self.iterations	O
)	O
:	O
likes	O
+=	O
self.samplelikelihood	O
(	O
i	O
,	O
count	O
)	O
unseen_species	O
=	O
[	O
n-i+1	O
for	O
n	O
in	O
self.ns	O
]	O
likes	O
*=	O
unseen_species	O
self.probs	O
*=	O
likes	O
self.probs	O
/=	O
self.probs.sum	O
(	O
)	O
this	O
function	O
is	O
similar	O
to	O
species2.update	O
,	O
with	O
two	O
changes	O
:	O
•	O
the	O
interface	B
is	O
different	O
.	O
instead	O
of	O
the	O
whole	O
dataset	O
,	O
we	O
get	O
i	O
,	O
the	O
index	O
of	O
the	O
observed	O
species	B
,	O
and	O
count	O
,	O
how	O
many	O
of	O
that	O
species	B
we	O
’	O
ve	O
seen	O
.	O
•	O
we	O
have	O
to	O
apply	O
a	O
correction	O
factor	O
for	O
the	O
number	O
of	O
unseen	O
species	B
,	O
as	O
in	O
species4.likelihood	O
.	O
the	O
difference	O
here	O
is	O
that	O
we	O
update	O
all	O
of	O
the	O
likelihoods	O
at	O
once	O
with	O
array	O
multiplication	O
.	O
finally	O
,	O
here	O
’	O
s	O
samplelikelihood	O
:	O
#	O
class	O
species5	O
def	O
samplelikelihood	O
(	O
self	O
,	O
i	O
,	O
count	O
)	O
:	O
gammas	O
=	O
numpy.random.gamma	O
(	O
self.params	O
)	O
sums	O
=	O
numpy.cumsum	O
(	O
gammas	O
)	O
[	O
self.ns	O
[	O
0	O
]	O
-1	O
:	O
]	O
ps	O
=	O
gammas	O
[	O
i-1	O
]	O
/	O
sums	O
log_likes	O
=	O
numpy.log	O
(	O
ps	O
)	O
*	O
count	O
log_likes	O
-=	O
numpy.max	O
(	O
log_likes	O
)	O
likes	O
=	O
numpy.exp	O
(	O
log_likes	O
)	O
return	O
likes	O
this	O
is	O
similar	O
to	O
species2.samplelikelihood	O
;	O
the	O
difference	O
is	O
that	O
each	O
update	O
only	O
includes	O
a	O
single	O
species	B
,	O
so	O
we	O
don	O
’	O
t	O
need	O
a	O
loop	O
.	O
15.9.	O
the	O
belly	B
button	I
data	O
181	O
the	O
runtime	O
of	O
this	O
function	O
is	O
proportional	O
to	O
the	O
number	O
of	O
hypotheses	O
,	O
k.	O
it	O
runs	O
m	O
times	O
,	O
so	O
the	O
run	O
time	O
of	O
the	O
update	O
is	O
proportional	O
to	O
km	O
.	O
and	O
the	O
number	O
of	O
iterations	O
we	O
need	O
to	O
get	O
an	O
accurate	O
result	O
is	O
usually	O
small	O
.	O
15.9	O
the	O
belly	B
button	I
data	O
that	O
’	O
s	O
enough	O
about	O
lions	B
and	I
tigers	I
and	I
bears	I
.	O
let	O
’	O
s	O
get	O
back	O
to	O
belly	O
but-	O
tons	O
.	O
to	O
get	O
a	O
sense	O
of	O
what	O
the	O
data	O
look	O
like	O
,	O
consider	O
subject	O
b1242	O
,	O
whose	O
sample	O
of	O
400	O
reads	O
yielded	O
61	O
species	B
with	O
the	O
following	O
counts	O
:	O
92	O
,	O
53	O
,	O
47	O
,	O
38	O
,	O
15	O
,	O
14	O
,	O
12	O
,	O
10	O
,	O
8	O
,	O
7	O
,	O
7	O
,	O
5	O
,	O
5	O
,	O
4	O
,	O
4	O
,	O
4	O
,	O
4	O
,	O
4	O
,	O
4	O
,	O
4	O
,	O
3	O
,	O
3	O
,	O
3	O
,	O
3	O
,	O
3	O
,	O
3	O
,	O
3	O
,	O
2	O
,	O
2	O
,	O
2	O
,	O
2	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
there	O
are	O
a	O
few	O
dominant	O
species	B
that	O
make	O
up	O
a	O
large	O
fraction	O
of	O
the	O
whole	O
,	O
but	O
many	O
species	B
that	O
yielded	O
only	O
a	O
single	O
read	O
.	O
the	O
number	O
of	O
these	O
“	O
sin-	O
gletons	O
”	O
suggests	O
that	O
there	O
are	O
likely	O
to	O
be	O
at	O
least	O
a	O
few	O
unseen	O
species	B
.	O
in	O
the	O
example	O
with	O
lions	O
and	O
tigers	O
,	O
we	O
assume	O
that	O
each	O
animal	O
in	O
the	O
preserve	O
is	O
equally	O
likely	O
to	O
be	O
observed	O
.	O
similarly	O
,	O
for	O
the	O
belly	B
button	I
data	O
,	O
we	O
assume	O
that	O
each	O
bacterium	O
is	O
equally	O
likely	O
to	O
yield	O
a	O
read	O
.	O
in	O
reality	O
,	O
each	O
step	O
in	O
the	O
data-collection	O
process	B
might	O
introduce	O
biases	O
.	O
some	O
species	B
might	O
be	O
more	O
likely	O
to	O
be	O
picked	O
up	O
by	O
a	O
swab	O
,	O
or	O
to	O
yield	O
identiﬁable	O
amplicons	O
.	O
so	O
when	O
we	O
talk	O
about	O
the	O
prevalence	B
of	O
each	O
species	B
,	O
we	O
should	O
remember	O
this	O
source	O
of	O
error	B
.	O
i	O
should	O
also	O
acknowledge	O
that	O
i	O
am	O
using	O
the	O
term	O
“	O
species	B
”	O
loosely	O
.	O
first	O
,	O
bacterial	O
species	B
are	O
not	O
well	O
deﬁned	O
.	O
second	O
,	O
some	O
reads	O
identify	O
a	O
partic-	O
ular	O
species	B
,	O
others	O
only	O
identify	O
a	O
genus	O
.	O
to	O
be	O
more	O
precise	O
,	O
i	O
should	O
say	O
“	O
operational	B
taxonomic	I
unit	I
”	O
,	O
or	O
otu	O
.	O
now	O
let	O
’	O
s	O
process	B
some	O
of	O
the	O
belly	B
button	I
data	O
.	O
subject	O
to	O
represent	O
information	O
about	O
each	O
subject	O
in	O
the	O
study	O
:	O
class	O
subject	O
(	O
object	O
)	O
:	O
i	O
deﬁne	O
a	O
class	O
called	O
def	O
__init__	O
(	O
self	O
,	O
code	O
)	O
:	O
self.code	O
=	O
code	O
self.species	O
=	O
[	O
]	O
each	O
subject	O
has	O
a	O
string	O
code	O
,	O
like	O
“	O
b1242	O
”	O
,	O
and	O
a	O
list	O
of	O
(	O
count	O
,	O
species	B
name	O
)	O
pairs	O
,	O
sorted	O
in	O
increasing	O
order	O
by	O
count	O
.	O
subject	O
provides	O
several	O
182	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
figure	O
15.3	O
:	O
distribution	B
of	O
n	O
for	O
subject	O
b1242	O
.	O
methods	O
to	O
make	O
it	O
easy	O
to	O
access	O
these	O
counts	O
and	O
species	B
names	O
.	O
you	O
can	O
see	O
the	O
details	O
in	O
http	O
:	O
//thinkbayes.com/species.py	O
.	O
for	O
more	O
informa-	O
tion	O
see	O
section	O
0.3.	O
subject	O
provides	O
a	O
method	O
named	O
process	B
that	O
creates	O
and	O
updates	O
a	O
species5	O
suite	B
,	O
which	O
represents	O
the	O
distributions	O
of	O
n	O
and	O
the	O
prevalences	O
.	O
and	O
suite2	O
provides	O
distofn	O
,	O
which	O
returns	O
the	O
posterior	B
distribution	I
of	O
n.	O
#	O
class	O
suite2	O
def	O
distn	O
(	O
self	O
)	O
:	O
items	O
=	O
zip	O
(	O
self.ns	O
,	O
self.probs	O
)	O
pmf	O
=	O
thinkbayes.makepmffromitems	O
(	O
items	O
)	O
return	O
pmf	O
figure	O
15.3	O
shows	O
the	O
distribution	B
of	O
n	O
for	O
subject	O
b1242	O
.	O
the	O
probability	B
that	O
there	O
are	O
exactly	O
61	O
species	B
,	O
and	O
no	O
unseen	O
species	B
,	O
is	O
nearly	O
zero	O
.	O
the	O
most	O
likely	O
value	O
is	O
72	O
,	O
with	O
90	O
%	O
credible	B
interval	I
66	O
to	O
79.	O
at	O
the	O
high	O
end	O
,	O
it	O
is	O
unlikely	O
that	O
there	O
are	O
as	O
many	O
as	O
87	O
species	B
.	O
next	O
we	O
compute	O
the	O
posterior	B
distribution	I
of	O
prevalence	B
for	O
each	O
species	B
.	O
species2	O
provides	O
distofprevalence	O
:	O
#	O
class	O
species2	O
def	O
distofprevalence	O
(	O
self	O
,	O
index	O
)	O
:	O
metapmf	O
=	O
thinkbayes.pmf	O
(	O
)	O
6065707580859095100number	O
of	O
species0.000.020.040.060.080.100.12probb1242	O
15.9.	O
the	O
belly	B
button	I
data	O
183	O
figure	O
15.4	O
:	O
distribution	B
of	O
prevalences	O
for	O
subject	O
b1242	O
.	O
for	O
n	O
,	O
prob	O
in	O
zip	O
(	O
self.ns	O
,	O
self.probs	O
)	O
:	O
beta	O
=	O
self.marginalbeta	O
(	O
n	O
,	O
index	O
)	O
pmf	O
=	O
beta.makepmf	O
(	O
)	O
metapmf.set	O
(	O
pmf	O
,	O
prob	O
)	O
mix	O
=	O
thinkbayes.makemixture	O
(	O
metapmf	O
)	O
return	O
metapmf	O
,	O
mix	O
index	O
indicates	O
which	O
species	B
we	O
want	O
.	O
for	O
each	O
n	O
,	O
we	O
have	O
a	O
different	O
posterior	B
distribution	I
of	O
prevalence	B
.	O
the	O
loop	O
iterates	O
through	O
the	O
possible	O
values	O
of	O
n	O
and	O
their	O
probabilities	O
.	O
for	O
each	O
value	O
of	O
n	O
it	O
gets	O
a	O
beta	O
object	O
representing	O
the	O
marginal	O
distri-	O
bution	O
for	O
the	O
indicated	O
species	B
.	O
remember	O
that	O
beta	O
objects	O
contain	O
the	O
parameters	O
alpha	O
and	O
beta	O
;	O
they	O
don	O
’	O
t	O
have	O
values	O
and	O
probabilities	O
like	O
a	O
pmf	O
,	O
but	O
they	O
provide	O
makepmf	O
,	O
which	O
generates	O
a	O
discrete	O
approximation	O
to	O
the	O
continuous	O
beta	O
distribution	B
.	O
metapmf	O
is	O
a	O
meta-pmf	O
that	O
contains	O
the	O
distributions	O
of	O
prevalence	B
,	O
condi-	O
tioned	O
on	O
n.	O
makemixture	O
combines	O
the	O
meta-pmf	O
into	O
mix	O
,	O
which	O
combines	O
the	O
conditional	B
distributions	O
into	O
a	O
single	O
distribution	B
of	O
prevalence	B
.	O
figure	O
15.4	O
shows	O
results	O
for	O
the	O
ﬁve	O
species	B
with	O
the	O
most	O
reads	O
.	O
the	O
most	O
prevalent	O
species	B
accounts	O
for	O
23	O
%	O
of	O
the	O
400	O
reads	O
,	O
but	O
since	O
there	O
are	O
al-	O
most	O
certainly	O
unseen	O
species	B
,	O
the	O
most	O
likely	O
estimate	O
for	O
its	O
prevalence	B
is	O
20	O
%	O
,	O
with	O
90	O
%	O
credible	B
interval	I
between	O
17	O
%	O
and	O
23	O
%	O
.	O
0.000.050.100.150.200.25prevalence0.00.20.40.60.81.0prob1	O
(	O
92	O
)	O
2	O
(	O
53	O
)	O
3	O
(	O
47	O
)	O
4	O
(	O
38	O
)	O
5	O
(	O
15	O
)	O
184	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
figure	O
15.5	O
:	O
simulated	O
rarefaction	O
curves	O
for	O
subject	O
b1242	O
.	O
15.10	O
predictive	O
distributions	O
i	O
introduced	O
the	O
hidden	O
species	B
problem	O
in	O
the	O
form	O
of	O
four	O
related	O
ques-	O
tions	O
.	O
we	O
have	O
answered	O
the	O
ﬁrst	O
two	O
by	O
computing	O
the	O
posterior	B
distribu-	O
tion	O
for	O
n	O
and	O
the	O
prevalence	B
of	O
each	O
species	B
.	O
the	O
other	O
two	O
questions	O
are	O
:	O
•	O
if	O
we	O
are	O
planning	O
to	O
collect	O
additional	O
reads	O
,	O
can	O
we	O
predict	O
how	O
many	O
new	O
species	B
we	O
are	O
likely	O
to	O
discover	O
?	O
•	O
how	O
many	O
additional	O
reads	O
are	O
needed	O
to	O
increase	O
the	O
fraction	O
of	O
ob-	O
served	O
species	B
to	O
a	O
given	O
threshold	O
?	O
to	O
answer	O
predictive	O
questions	O
like	O
this	O
we	O
can	O
use	O
the	O
posterior	B
distribu-	O
tions	O
to	O
simulate	O
possible	O
future	O
events	O
and	O
compute	O
predictive	O
distribu-	O
tions	O
for	O
the	O
number	O
of	O
species	B
,	O
and	O
fraction	O
of	O
the	O
total	O
,	O
we	O
are	O
likely	O
to	O
see	O
.	O
the	O
kernel	O
of	O
these	O
simulations	O
looks	O
like	O
this	O
:	O
1.	O
choose	O
n	O
from	O
its	O
posterior	B
distribution	I
.	O
2.	O
choose	O
a	O
prevalence	B
for	O
each	O
species	B
,	O
species	B
,	O
using	O
the	O
dirichlet	O
distribution	B
.	O
including	O
possible	O
unseen	O
3.	O
generate	O
a	O
random	O
sequence	O
of	O
future	O
observations	O
.	O
050100150200250300350400450	O
#	O
samples20246810	O
#	O
species	B
15.10.	O
predictive	O
distributions	O
185	O
4.	O
compute	O
the	O
number	O
of	O
new	O
species	B
,	O
num_new	O
,	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
additional	O
reads	O
,	O
k.	O
5.	O
repeat	O
the	O
previous	O
steps	O
and	O
accumulate	O
the	O
joint	B
distribution	I
of	O
num_new	O
and	O
k.	O
and	O
here	O
’	O
s	O
the	O
code	O
.	O
runsimulation	O
runs	O
a	O
single	O
simulation	B
:	O
#	O
class	O
subject	O
def	O
runsimulation	O
(	O
self	O
,	O
num_reads	O
)	O
:	O
m	O
,	O
seen	O
=	O
self.getseenspecies	O
(	O
)	O
n	O
,	O
observations	O
=	O
self.generateobservations	O
(	O
num_reads	O
)	O
curve	O
=	O
[	O
]	O
for	O
k	O
,	O
obs	O
in	O
enumerate	O
(	O
observations	O
)	O
:	O
seen.add	O
(	O
obs	O
)	O
num_new	O
=	O
len	O
(	O
seen	O
)	O
-	O
m	O
curve.append	O
(	O
(	O
k+1	O
,	O
num_new	O
)	O
)	O
return	O
curve	O
num_reads	O
is	O
the	O
number	O
of	O
additional	O
reads	O
to	O
simulate	O
.	O
m	O
is	O
the	O
number	O
of	O
seen	O
species	B
,	O
and	O
seen	O
is	O
a	O
set	O
of	O
strings	O
with	O
a	O
unique	O
name	O
for	O
each	O
species	B
.	O
n	O
is	O
a	O
random	O
value	O
from	O
the	O
posterior	B
distribution	I
,	O
and	O
observations	O
is	O
a	O
random	O
sequence	O
of	O
species	B
names	O
.	O
each	O
time	O
through	O
the	O
loop	O
,	O
we	O
add	O
the	O
new	O
observation	O
to	O
seen	O
and	O
record	O
the	O
number	O
of	O
reads	O
and	O
the	O
number	O
of	O
new	O
species	B
so	O
far	O
.	O
the	O
result	O
of	O
runsimulation	O
is	O
a	O
rarefaction	B
curve	I
,	O
represented	O
as	O
a	O
list	O
of	O
pairs	O
with	O
the	O
number	O
of	O
reads	O
and	O
the	O
number	O
of	O
new	O
species	B
.	O
the	O
before	O
we	O
generateobservations	O
.	O
see	O
#	O
class	O
subject	O
results	O
,	O
let	O
’	O
s	O
look	O
at	O
getseenspecies	O
and	O
def	O
getseenspecies	O
(	O
self	O
)	O
:	O
names	O
=	O
self.getnames	O
(	O
)	O
m	O
=	O
len	O
(	O
names	O
)	O
seen	O
=	O
set	O
(	O
speciesgenerator	O
(	O
names	O
,	O
m	O
)	O
)	O
return	O
m	O
,	O
seen	O
186	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
getnames	O
returns	O
the	O
list	O
of	O
species	B
names	O
that	O
appear	O
in	O
the	O
data	O
ﬁles	O
,	O
but	O
for	O
many	O
subjects	O
these	O
names	O
are	O
not	O
unique	O
.	O
so	O
i	O
use	O
speciesgenerator	O
to	O
extend	O
each	O
name	O
with	O
a	O
serial	O
number	O
:	O
def	O
speciesgenerator	O
(	O
names	O
,	O
num	O
)	O
:	O
i	O
=	O
0	O
for	O
name	O
in	O
names	O
:	O
yield	O
'	O
%	O
s-	O
%	O
d	O
'	O
%	O
(	O
name	O
,	O
i	O
)	O
i	O
+=	O
1	O
while	O
i	O
<	O
num	O
:	O
yield	O
'unseen-	O
%	O
d	O
'	O
%	O
i	O
i	O
+=	O
1	O
given	O
a	O
name	O
corynebacterium-1	O
.	O
when	O
the	O
list	O
of	O
names	O
is	O
exhausted	O
,	O
names	O
like	O
unseen-62	O
.	O
corynebacterium	O
,	O
like	O
speciesgenerator	O
yields	O
it	O
yields	O
here	O
is	O
generateobservations	O
:	O
#	O
class	O
subject	O
def	O
generateobservations	O
(	O
self	O
,	O
num_reads	O
)	O
:	O
n	O
,	O
prevalences	O
=	O
self.suite.sampleposterior	O
(	O
)	O
names	O
=	O
self.getnames	O
(	O
)	O
name_iter	O
=	O
speciesgenerator	O
(	O
names	O
,	O
n	O
)	O
d	O
=	O
dict	O
(	O
zip	O
(	O
name_iter	O
,	O
prevalences	O
)	O
)	O
cdf	O
=	O
thinkbayes.makecdffromdict	O
(	O
d	O
)	O
observations	O
=	O
cdf.sample	O
(	O
num_reads	O
)	O
return	O
n	O
,	O
observations	O
again	O
,	O
num_reads	O
is	O
the	O
number	O
of	O
additional	O
reads	O
to	O
generate	O
.	O
n	O
and	O
prevalences	O
are	O
samples	O
from	O
the	O
posterior	B
distribution	I
.	O
cdf	O
is	O
a	O
cdf	O
object	O
that	O
maps	O
species	B
names	O
,	O
including	O
the	O
unseen	O
,	O
to	O
cu-	O
mulative	O
probabilities	O
.	O
using	O
a	O
cdf	O
makes	O
it	O
efﬁcient	O
to	O
generate	O
a	O
random	O
sequence	O
of	O
species	B
names	O
.	O
finally	O
,	O
here	O
is	O
species2.sampleposterior	O
:	O
def	O
sampleposterior	O
(	O
self	O
)	O
:	O
pmf	O
=	O
self.distofn	O
(	O
)	O
n	O
=	O
pmf.random	O
(	O
)	O
15.11.	O
joint	O
posterior	O
187	O
figure	O
15.6	O
:	O
distributions	O
of	O
the	O
number	O
of	O
new	O
species	B
conditioned	O
on	O
the	O
number	O
of	O
additional	O
reads	O
.	O
prevalences	O
=	O
self.sampleprevalences	O
(	O
n	O
)	O
return	O
n	O
,	O
prevalences	O
and	O
sampleprevalences	O
,	O
which	O
generates	O
a	O
sample	O
of	O
prevalences	O
condi-	O
tioned	O
on	O
n	O
:	O
#	O
class	O
species2	O
def	O
sampleprevalences	O
(	O
self	O
,	O
n	O
)	O
:	O
params	O
=	O
self.params	O
[	O
:	O
n	O
]	O
gammas	O
=	O
numpy.random.gamma	O
(	O
params	O
)	O
gammas	O
/=	O
gammas.sum	O
(	O
)	O
return	O
gammas	O
we	O
saw	O
this	O
algorithm	O
for	O
generating	O
random	O
values	O
from	O
a	O
dirichlet	O
dis-	O
tribution	O
in	O
section	O
15.4.	O
figure	O
15.5	O
shows	O
100	O
simulated	O
rarefaction	O
curves	O
for	O
subject	O
b1242	O
.	O
the	O
curves	O
are	O
“	O
jittered	O
;	O
”	O
that	O
is	O
,	O
i	O
shifted	O
each	O
curve	O
by	O
a	O
random	O
offset	O
so	O
they	O
would	O
not	O
all	O
overlap	O
.	O
by	O
inspection	O
we	O
can	O
estimate	O
that	O
after	O
400	O
more	O
reads	O
we	O
are	O
likely	O
to	O
ﬁnd	O
2–6	O
new	O
species	B
.	O
15.11	O
joint	O
posterior	O
we	O
can	O
use	O
these	O
simulations	O
to	O
estimate	O
the	O
joint	B
distribution	I
of	O
num_new	O
and	O
k	O
,	O
and	O
from	O
that	O
we	O
can	O
get	O
the	O
distribution	B
of	O
num_new	O
conditioned	O
on	O
any	O
value	O
of	O
k.	O
02468101214	O
#	O
new	O
species0.00.20.40.60.81.0probk=100k=200k=400k=800	O
188	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
def	O
makejointpredictive	O
(	O
curves	O
)	O
:	O
joint	O
=	O
thinkbayes.joint	O
(	O
)	O
for	O
curve	O
in	O
curves	O
:	O
for	O
k	O
,	O
num_new	O
in	O
curve	O
:	O
joint.incr	O
(	O
(	O
k	O
,	O
num_new	O
)	O
)	O
joint.normalize	O
(	O
)	O
return	O
joint	O
makejointpredictive	O
makes	O
a	O
joint	O
object	O
,	O
which	O
is	O
a	O
pmf	O
whose	O
values	O
are	O
tuples	O
.	O
curves	O
is	O
a	O
list	O
of	O
rarefaction	O
curves	O
created	O
by	O
runsimulation	O
.	O
each	O
curve	O
contains	O
a	O
list	O
of	O
pairs	O
of	O
k	O
and	O
num_new	O
.	O
the	O
resulting	O
joint	B
distribution	I
is	O
a	O
map	O
from	O
each	O
pair	O
to	O
its	O
probability	B
of	O
occurring	O
.	O
given	O
the	O
joint	B
distribution	I
,	O
we	O
can	O
use	O
joint.conditional	O
get	O
the	O
distribution	B
of	O
num_new	O
conditioned	O
on	O
k	O
(	O
see	O
section	O
9.6	O
)	O
.	O
subject.makeconditionals	O
takes	O
a	O
list	O
of	O
ks	O
and	O
computes	O
the	O
conditional	B
distribution	I
of	O
num_new	O
for	O
each	O
k.	O
the	O
result	O
is	O
a	O
list	O
of	O
cdf	O
objects	O
.	O
def	O
makeconditionals	O
(	O
curves	O
,	O
ks	O
)	O
:	O
joint	O
=	O
makejointpredictive	O
(	O
curves	O
)	O
cdfs	O
=	O
[	O
]	O
for	O
k	O
in	O
ks	O
:	O
pmf	O
=	O
joint.conditional	O
(	O
1	O
,	O
0	O
,	O
k	O
)	O
pmf.name	O
=	O
'k=	O
%	O
d	O
'	O
%	O
k	O
cdf	O
=	O
pmf.makecdf	O
(	O
)	O
cdfs.append	O
(	O
cdf	O
)	O
return	O
cdfs	O
figure	O
15.6	O
shows	O
the	O
results	O
.	O
after	O
100	O
reads	O
,	O
the	O
median	B
predicted	O
number	O
of	O
new	O
species	B
is	O
2	O
;	O
the	O
90	O
%	O
credible	B
interval	I
is	O
0	O
to	O
5.	O
after	O
800	O
reads	O
,	O
we	O
expect	O
to	O
see	O
3	O
to	O
12	O
new	O
species	B
.	O
15.12	O
coverage	B
the	O
last	O
question	O
we	O
want	O
to	O
answer	O
is	O
,	O
“	O
how	O
many	O
additional	O
reads	O
are	O
needed	O
to	O
increase	O
the	O
fraction	O
of	O
observed	O
species	B
to	O
a	O
given	O
threshold	O
?	O
”	O
to	O
answer	O
this	O
question	O
,	O
we	O
need	O
a	O
version	O
of	O
runsimulation	O
that	O
computes	O
the	O
fraction	O
of	O
observed	O
species	B
rather	O
than	O
the	O
number	O
of	O
new	O
species	B
.	O
15.12.	O
coverage	B
189	O
figure	O
15.7	O
:	O
complementary	O
cdf	O
of	O
coverage	B
for	O
a	O
range	O
of	O
additional	O
reads	O
.	O
#	O
class	O
subject	O
def	O
runsimulation	O
(	O
self	O
,	O
num_reads	O
)	O
:	O
m	O
,	O
seen	O
=	O
self.getseenspecies	O
(	O
)	O
n	O
,	O
observations	O
=	O
self.generateobservations	O
(	O
num_reads	O
)	O
curve	O
=	O
[	O
]	O
for	O
k	O
,	O
obs	O
in	O
enumerate	O
(	O
observations	O
)	O
:	O
seen.add	O
(	O
obs	O
)	O
frac_seen	O
=	O
len	O
(	O
seen	O
)	O
/	O
float	O
(	O
n	O
)	O
curve.append	O
(	O
(	O
k+1	O
,	O
frac_seen	O
)	O
)	O
return	O
curve	O
next	O
we	O
loop	O
through	O
each	O
curve	O
and	O
make	O
a	O
dictionary	O
,	O
d	O
,	O
that	O
maps	O
from	O
the	O
number	O
of	O
additional	O
reads	O
,	O
k	O
,	O
to	O
a	O
list	O
of	O
fracs	O
;	O
that	O
is	O
,	O
a	O
list	O
of	O
values	O
for	O
the	O
coverage	B
achieved	O
after	O
k	O
reads	O
.	O
def	O
makefraccdfs	O
(	O
self	O
,	O
curves	O
)	O
:	O
d	O
=	O
{	O
}	O
for	O
curve	O
in	O
curves	O
:	O
for	O
k	O
,	O
frac	O
in	O
curve	O
:	O
d.setdefault	O
(	O
k	O
,	O
[	O
]	O
)	O
.append	O
(	O
frac	O
)	O
cdfs	O
=	O
{	O
}	O
for	O
k	O
,	O
fracs	O
in	O
d.iteritems	O
(	O
)	O
:	O
0.650.700.750.800.850.900.951.001.05fraction	O
of	O
species	B
seen0.00.20.40.60.81.0probability800100200104001000600	O
190	O
chapter	O
15.	O
dealing	O
with	O
dimensions	O
cdf	O
=	O
thinkbayes.makecdffromlist	O
(	O
fracs	O
)	O
cdfs	O
[	O
k	O
]	O
=	O
cdf	O
return	O
cdfs	O
then	O
for	O
each	O
value	O
of	O
k	O
we	O
make	O
a	O
cdf	O
of	O
fracs	O
;	O
this	O
cdf	O
represents	O
the	O
distribution	B
of	O
coverage	B
after	O
k	O
reads	O
.	O
remember	O
that	O
the	O
cdf	O
tells	O
you	O
the	O
probability	B
of	O
falling	O
below	O
a	O
given	O
threshold	O
,	O
so	O
the	O
complementary	O
cdf	O
tells	O
you	O
the	O
probability	B
of	O
exceeding	O
it	O
.	O
figure	O
15.7	O
shows	O
complementary	O
cdfs	O
for	O
a	O
range	O
of	O
values	O
of	O
k.	O
to	O
read	O
this	O
ﬁgure	O
,	O
select	O
the	O
level	O
of	O
coverage	B
you	O
want	O
to	O
achieve	O
along	O
the	O
x-axis	O
.	O
as	O
an	O
example	O
,	O
choose	O
90	O
%	O
.	O
now	O
you	O
can	O
read	O
up	O
the	O
chart	O
to	O
ﬁnd	O
the	O
probability	B
of	O
achieving	O
90	O
%	O
coverage	B
after	O
k	O
reads	O
.	O
for	O
example	O
,	O
with	O
200	O
reads	O
,	O
you	O
have	O
about	O
a	O
40	O
%	O
chance	O
of	O
getting	O
90	O
%	O
coverage	B
.	O
with	O
1000	O
reads	O
,	O
you	O
have	O
a	O
90	O
%	O
chance	O
of	O
getting	O
90	O
%	O
coverage	B
.	O
with	O
that	O
,	O
we	O
have	O
answered	O
the	O
four	O
questions	O
that	O
make	O
up	O
the	O
unseen	O
species	B
problem	O
.	O
to	O
validate	O
the	O
algorithms	O
in	O
this	O
chapter	O
with	O
real	O
data	O
,	O
i	O
had	O
to	O
deal	O
with	O
a	O
few	O
more	O
details	O
.	O
but	O
this	O
chapter	O
is	O
already	O
too	O
long	O
,	O
so	O
i	O
won	O
’	O
t	O
discuss	O
them	O
here	O
.	O
read	O
about	O
you	O
can	O
them	O
,	O
belly-button-biodiversity-end-game.html	O
.	O
at	O
problems	O
,	O
the	O
addressed	O
http	O
:	O
//allendowney.blogspot.com/2013/05/	O
and	O
how	O
i	O
you	O
can	O
download	O
the	O
code	O
in	O
this	O
chapter	O
from	O
http	O
:	O
//thinkbayes.com/	O
species.py	O
.	O
for	O
more	O
information	O
see	O
section	O
0.3	O
.	O
15.13	O
discussion	O
the	O
unseen	O
species	B
problem	O
is	O
an	O
area	O
of	O
active	O
research	O
,	O
and	O
i	O
believe	O
the	O
algorithm	O
in	O
this	O
chapter	O
is	O
a	O
novel	O
contribution	O
.	O
so	O
in	O
fewer	O
than	O
200	O
pages	O
we	O
have	O
made	O
it	O
from	O
the	O
basics	O
of	O
probability	B
to	O
the	O
research	O
frontier	O
.	O
i	O
’	O
m	O
very	O
happy	O
about	O
that	O
.	O
my	O
goal	O
for	O
this	O
book	O
is	O
to	O
present	O
three	O
related	O
ideas	O
:	O
•	O
bayesian	O
thinking	O
:	O
the	O
foundation	O
of	O
bayesian	O
analysis	O
is	O
the	O
idea	O
of	O
using	O
probability	B
distributions	O
to	O
represent	O
uncertain	O
beliefs	O
,	O
using	O
data	O
to	O
update	O
those	O
distributions	O
,	O
and	O
using	O
the	O
results	O
to	O
make	O
pre-	O
dictions	O
and	O
inform	O
decisions	O
.	O
15.13.	O
discussion	O
191	O
•	O
a	O
computational	O
approach	O
:	O
the	O
premise	O
of	O
this	O
book	O
is	O
that	O
it	O
is	O
easier	O
to	O
understand	O
bayesian	O
analysis	O
using	O
computation	O
rather	O
than	O
math	O
,	O
and	O
easier	O
to	O
implement	O
bayesian	O
methods	O
with	O
reusable	O
building	O
blocks	O
that	O
can	O
be	O
rearranged	O
to	O
solve	O
real-world	O
problems	O
quickly	O
.	O
•	O
iterative	B
modeling	I
:	O
most	O
real-world	O
problems	O
involve	O
modeling	B
de-	O
cisions	O
and	O
trade-offs	O
between	O
realism	O
and	O
complexity	O
.	O
it	O
is	O
often	O
im-	O
possible	O
to	O
know	O
ahead	O
of	O
time	O
what	O
factors	O
should	O
be	O
included	O
in	O
the	O
model	O
and	O
which	O
can	O
be	O
abstracted	O
away	O
.	O
the	O
best	O
approach	O
is	O
to	O
it-	O
erate	O
,	O
starting	O
with	O
simple	O
models	O
and	O
adding	O
complexity	O
gradually	O
,	O
using	O
each	O
model	O
to	O
validate	O
the	O
others	O
.	O
these	O
ideas	O
are	O
versatile	O
and	O
powerful	O
;	O
they	O
are	O
applicable	O
to	O
problems	O
in	O
every	O
area	O
of	O
science	O
and	O
engineering	O
,	O
from	O
simple	O
examples	O
to	O
topics	O
of	O
current	O
research	O
.	O
if	O
you	O
made	O
it	O
this	O
far	O
,	O
you	O
should	O
be	O
prepared	O
to	O
apply	O
these	O
tools	O
to	O
new	O
problems	O
relevant	O
to	O
your	O
work	O
.	O
i	O
hope	O
you	O
ﬁnd	O
them	O
useful	O
;	O
let	O
me	O
know	O
how	O
it	O
goes	O
!	O